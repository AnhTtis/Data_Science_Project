% !TEX root = ./CauchyCombination.tex
\section{Multiple Hypothesis Testing with Correlated Test Statistics} \label{secPrelims}


In this section, we first introduce the notation and terminology used throughout the article regarding multiple hypothesis testing. 
We then review the global Cauchy combination test of \citet{liu2020cauchy}, followed by our sequential version of the Cauchy combination test.  

\subsection{Setting}

Let $H_{i}$ denote the $i^{\text{th}}$ null hypothesis of interest, with $i=1,...,d$. Here, $d$ denotes the total number of individual hypotheses, and $\mathcal{H}_{0}$ denotes the collection of null hypotheses of interest. 
To test the $d$ hypotheses, we can use the associated vector of test statistics $\bm{X}=(X_{1},X_{2},\ldots,X_{d})^{^{\prime }}$, one for each hypothesis being tested, or the corresponding raw $p$-values $p_{1},\ldots ,p_{d}$. The test statistics can be independent or dependent. 
For many popular tests, such as those described in Section \ref{secApplDriftBurst}, the test statistics are constructed from rolling windows and exhibit strong serial correlation.

To ensure the validity of individual hypothesis testing, it is common practice to control the probability of falsely rejecting a single hypothesis that is true (known as a false positive or Type I error) at a pre-specified nominal $\alpha$-level. 
However, when dealing with a large number of hypotheses, the issue of multiplicity arises: 
if the Type I error of each individual test is controlled at the $\alpha$-level, 
the probability of having at least one false positive conclusion rises well above $\alpha$. 

A classical global test circumvents the issue of multiplicity by replacing multiple tests with a single test. The corresponding global null hypothesis, denoted as $\mathcal{H}_0 = \bigcap_{i=1}^{d} H_{i}$, assumes that all elementary hypotheses are true, and the alternative hypothesis posits that at least one  hypothesis is false. 
In the context of monitoring specific events like jumps or drift bursts within a fixed time period, such as a day, the global null hypothesis would reflect the absence of any such event occurring within that given timeframe. Although global tests serve their purpose by aggregating effects, they may not provide the means to differentiate among individual hypotheses. In the field of financial econometrics, we are often interested in precisely timestamping drift bursts or identifying skilled fund managers, which requires a more granular analysis beyond the scope of global tests. 

Let $\mathcal{T}$ denote the set of true hypotheses,  $\mathcal{F}$  denote the set of false hypotheses,  and $\mathcal{R}$ denote the set of rejected hypotheses. 
The set of true and false hypotheses are unknown. 
A statistical test selects hypotheses to reject based on empirical data, and the corresponding set of discoveries in $\mathcal{R}$ 
should coincide with $\mathcal{F}$ as much as possible, while controlling the probability of making false discoveries. 
The objective of many multiple testing corrections is to control the familywise error rate (FWER), which constrains the probability of at least one false rejection within a family, denoted as $P[\mathcal{T} \cap \mathcal{R} \neq \varnothing]$. 
Ideally, the multiple testing correction should ensure that the FWER is not greater than the upper bound $\alpha$, while striving to keep it as close to $\alpha$ as possible. 
We concentrate on strong control of the FWER, allowing for the presence of some false hypotheses ($\mathcal{F} \neq \varnothing$), rather than weak FWER control, which assumes that all hypotheses of interest are true (i.e., $\mathcal{T} = \mathcal{H}_0$).  


\subsection{Global Cauchy combination test}
\label{sec:CC}

The global Cauchy combination (GCC) test examines the global null hypothesis. 
The GCC test statistic is constructed from raw  $p$-values of the individual test statistics $X_i$, which are uniformely distributed between $0$ and $1$ under the  null hypothesis. 
The core idea of this test is first to transform these uniformly distributed $p$-values into standard Cauchy variates using the transformation formula $\tan \{(0.5-p_{i})\pi \}$, and then construct a new test statistic by taking the  weighted sum of these transformed $p$-values. 
The new test statistic is denoted by $\tilde{T}$ and is defined as: 
\begin{equation}
	\label{eqCauchyStatistic}
	{\normalsize \tilde{T}=\sum_{i=1}^{d}w_{i}\tan \{(0.5-p_{i})\pi \},} 
\end{equation}
in which the $w_{i}$'s are non-negative weights that sum up to one. Throughout the paper, the weights $w_{i}$ are set to $1/d$, for $i=1,\ldots,d$, following \citet{liu2020cauchy}. 


When the raw $p$-values are independent or perfectly dependent, the new test statistic \eqref{eqCauchyStatistic} has a standard Cauchy distribution under the null, because the family of Cauchy densities is closed under convolutions. Even in cases of general dependence (whether weak, mild, or strong), the correlation structure has minimal impact on the tail behavior of the test statistics due to the heavy tails of the Cauchy distribution. 
Specifically, \citet{liu2020cauchy} prove that: 
\begin{equation}
	\lim_{h\rightarrow \infty }\frac{\Pr\left( \tilde{T}>h\right) }{\Pr\left(
		C>h\right) }=1,  \label{eq:tail}
\end{equation}
in which $C$ is a standard Cauchy random variable, subject to certain regularity conditions on the test statistic.

The result expressed in  \eqref{eq:tail} suggests that, under the global null hypothesis, the tail of the Cauchy combination test statistic is approximately Cauchy distributed, under arbitrary dependence structures, so that a $p$-value of the Cauchy combination test, denoted 
$\widetilde{p}$, can  be calculated from the standard Cauchy distribution. Suppose that we observe $\tilde{T}=t_{0}$, then: 
\begin{equation}
	\label{eqCauchyPval}
	\widetilde{p}=\frac{1}{2}-\frac{\arctan t_{0}}{\pi }. 
\end{equation}

Using the GCC $p$-values \eqref{eqCauchyPval}, the tail result in \eqref{eq:tail} can be equivalently stated as the actual size converging to the nominal size $\alpha$ as the significance level tends to zero:  % \textit{i.e.}, 
\begin{equation}
	\lim_{\alpha\rightarrow 0 }\frac{\Pr\left( \widetilde{p} \leq \alpha\right) }{\alpha}=1. \label{eq:wFWER}
\end{equation} 
 The approximation is particularly accurate for small $\alpha$'s, which are of particular interest in large-scale testing problems such as Examples 1 and 2 in sections \ref{secApplDriftBurst} and \ref{secApplFan}.
Importantly, the GCC test achieves weak familywise error rate control  regardless of the underlying correlation structure.

Figure \ref{figCauchyPvalsAR} illustrates that while the dependence among individual test statistics may affect the null distribution of the GCC test statistic \eqref{eqCauchyStatistic}, its influence on the tail is minimal. 
To illustrate this point, we simulate a vector of  test statistics  $\bm{X}$ from a $d$-variate normal distribution with correlation matrix $\bm{\Sigma}$, \textit{i.e.}, $N_d(\bm{0}, \bm{\Sigma})$ with $\bm{\Sigma} = (\sigma_{ij})$ and $d = 300$. 
The diagonal elements $\sigma_{ii}=1$ for all $i=1,\ldots,d$ and the off-diagonal elements $\sigma_{ij} = \theta^{\abs{i-j}}$ for $i \neq j$, where $\theta$ takes the values of $0.2, 0.4, 0.8, 0.95$. 
We repeat the simulation $10^7$ times, and calculate two-sided $p$-values, the GCC test \eqref{eqCauchyStatistic} and the GCC $p$-value \eqref{eqCauchyPval} for each draw. 
The  histogram of the $10^7$ GCC $p$-values is plotted in Figure \ref{figCauchyPvalsAR}. 
When the autocorrelation is low (i.e., $\theta=0.2$), the distribution of $p$-values resembles a uniform distribution. As the autocorrelation increases, a pothole in the middle and a bump at the end of the histogram appear. However, regardless of magnitude of the autoregressive parameter, the percentage of $p$-values falling into the first bin remains consistently around $5$\%, as guaranteed by the limit result described in \eqref{eq:wFWER}. 

\begin{figure}[!h]
	\caption{The minimal impact of dependence on the tails of the GCC test statistic}
	\label{figCauchyPvalsAR}
	\centering
	
	\par
	
	\subfloat[${\theta} = 0.2$ ]{{\includegraphics[width=.40\textwidth,angle =
			-90,scale=0.70]{Sample_pval_dim_300_rho_0.2_alpha0.05.eps} }} 
	\subfloat[$\theta = 0.4$ ]{{\includegraphics[width=.40\textwidth,angle =
			-90,scale=0.70]{Sample_pval_dim_300_rho_0.4_alpha0.05.eps} }} 
	
	\vspace{0.4cm}
	
	\subfloat[$\theta = 0.8$ ]{{\includegraphics[width=.40\textwidth,angle =
			-90,scale=0.70]{Sample_pval_dim_300_rho_0.8_alpha0.05.eps} }} 
	\subfloat[$\theta = 0.95$]{{\includegraphics[width=.40\textwidth,angle =
			-90,scale=0.70]{Sample_pval_dim_300_rho_0.95_alpha0.05.eps} }} 
	
	\par
	\begin{minipage}{1.0\linewidth}
		\begin{tablenotes}
			\small
			\item {
				\medskip
				Note: We plot histograms of GCC $p$-values \eqref{eqCauchyPval} for various correlation strengths. The individual test statistics are drawn from a $d$-variate normal distribution $N_d(\bm{0}, \bm{\Sigma})$ with $\bm{\Sigma}= (\sigma_{ij})$ and $d=300$. The diagonal elements of the covariance matrix $\sigma_{ii}=1$ for all $i=1,\ldots,d$ and the off-diagonal elements  $\sigma_{ij} = \theta^{\vert i-j \vert}$ for $i\neq j$, with $\theta = 0.2, 0.4,0.8,0.95$. 
				The simulation is repeated $10^7$ times. The simulated GCC $p$-values are sorted into bins 
				with a width of  0.05. 
				We highlight the first bin in black and 
				add a text note with the probability of $p$-values being in the first bin. }
		\end{tablenotes}
	\end{minipage}
\end{figure}


\subsection{Sequential Cauchy Combination Test}

The main contribution of this paper is the introduction of the sequential Cauchy combination test, which extends the GCC test of \citet{liu2020cauchy} to make statements on the elementary hypotheses. 
To facilitate this, the raw $p$-values are sorted in ascending order, denoting them as $p_{(1)}\leq p_{(2)}\leq \ldots \leq p_{(d)}$, where  $H_{(1)},H_{(2)},\ldots,H_{(d)}$ correspond to their respective null hypotheses.
For the purpose of testing hypothesis $H_{\left( i\right) }$, we calculate a Cauchy combination test statistic, denoted as  ${\normalsize \tilde{T}}_{\left( i\right) }$, using a subset of $p$-values running from $p_{(i)}$ to $p_{(d)}$ as:  
\begin{equation}
{\normalsize \ \tilde{T}%
		_{\left( i\right) }=\sum_{j=i}^{d}w_{j}\tan \{(0.5-p_{(j)})\pi \}
	} \text{ \ with \ } w_j =  \frac{1}{d-i+1},
	\label{eq:CC_mt}
\end{equation}
where $w_j$ represents the weight assigned to each $p$-value in the subset.
% 
The corresponding $p$-value is computed as: 
\begin{equation}\label{eq:SCCp}
\widetilde{p}_{(i)}=\frac{1}{2}-\frac{\arctan \tilde{T}_{\left(i\right) }}{\pi }.
\end{equation} 
We reject the $i$th null hypothesis $H_{(i)}$ if  $\widetilde{p}_{(i)}\leq\alpha$. 
Similar to the step-up procedure introduced by \citet{hommel1988stagewise}, the SCC test leverages power across hypotheses: the test statistic $\tilde{T}_{(i)}$ is computed using the raw $p$-values associated with $\mathcal{H}_0^{(i)}=\bigcap_{j=i}^{d} H_{(j)}$. 

A more prescriptive description of the SCC testing procedure is as follows: 

\medskip

\begin{tcolorbox}
\begin{minipage}{1\linewidth}
	\textbf{SCC algorithm}	
	
	\begin{enumerate}
	
	\item Calculate raw $p$-values $p_1, p_2,\ldots, p_d$ corresponding to the null hypotheses $H_{1}, H_{2},\ldots, H_{d} $.%
	
	\item Order the raw $p$-values in ascending order, 	$p_{(1)},p_{(2)},\ldots,p_{(d)}$, with their  corresponding  ordered null hypotheses $H_{(1)},H_{(2)},\ldots,H_{(d)}$.
	
	\item Calculate the SCC test statistic $\tilde T_{(i)}$ and the transformed Cauchy $p$-values $\widetilde{p}_{(i)}$ from a subset of the ordered $p$-values $\left\{p_{(j)}\right\} _{j=i}^{d}$ using \eqref{eq:CC_mt} and \eqref{eq:SCCp}, respectively, for $i=1,\ldots,d$.
	
	\item Construct the rejection set $\mathcal{R}=\left\{H_{\left(i\right)} : \widetilde{p}_{(i)}\leq \alpha\right\}$. 
\end{enumerate}
\end{minipage}
\end{tcolorbox}

Figure \ref{figSequentialCauchyIllustration} illustrates the mechanics of sequential Cauchy combination procedure using a simulated sequence of test statistics. 
The top row of the figure shows the raw and ordered 
$p$-values. Most of the observations correspond to the null hypothesis (represented by grey dots), while a few observations correspond to the alternative hypothesis (represented by black dots). 
The data-generating process is the same as the one used in Figure \ref{figCauchyPvalsAR}, where $\theta=0.9$ and $d=100$. 
We add constant signals (nonzero mean) for five out of the 100 hypotheses,  with a signal strength of $\pm2.806$. 
The sign of the signal aligns with the sign of the test statistic under the null, such that the signal always amplifies the magnitude of the test statistic. 
The bottom row of the figure plots the sequential Cauchy combination test statistics \eqref{eq:CC_mt} and their corresponding $p$-values \eqref{eq:SCCp}. In particular,  the bottom right panel shows that the SCC $p$-values $\widetilde{p}_{(i)}$ decrease as $i$ decreases from $d$ to $1$. In this example, the SCC test rejects three out of the five alternative hypotheses and does not reject any under the null hypothesis. These rejections correspond to the 4$^\text{th}$, 29$^\text{th}$ and  46$^\text{th}$ hypotheses in the top left panel. 
It is worth noting that the smallest SCC $p$-value corresponds to the $p$-value of the GCC test in \eqref{eqCauchyStatistic}, which performs the test on the largest set of hypotheses. 

\begin{figure}[!h]
	\caption{Mechanics of the sequential Cauchy Combination test}
	\label{figSequentialCauchyIllustration}\centering	
	\par

	\subfloat[Raw
	$p$-values]{{\includegraphics[width=.31\textwidth,angle = -90]{2_pvals_d_100_rho_0.9_signal_5_5} }}
	\subfloat[Ordered raw $p$-values]{{\includegraphics[width=.31\textwidth,angle =
			-90]{3_spvals_d_100_rho_0.9_signal_5_5} }} 
	
	\vspace{0.4cm}	
	
	\subfloat[SCC test statistics
	]{{\includegraphics[width=.31\textwidth,angle = -90]{4_ctstats_d_100_rho_0.9_signal_5_5}}}
	\subfloat[SCC 
	$p$-values]{{\includegraphics[width=.31\textwidth,angle = -90]{4_cpvals_d_100_rho_0.9_signal_5_5}}}
	
	
	\begin{minipage}{1.0\linewidth}
		\begin{tablenotes}
			\small
			\item {
				\medskip
				Note: 
				The test statistics are simulated from $N_{d}(\bm{0},\bm{\Sigma})$ as in Figure \ref{figCauchyPvalsAR}, with $d=100$, $\theta=0.9$ and $5\%$ signals. The strength of the signal is $\pm2.806$, with its sign identical to that of the test statistic under the null.  
				The horizon line in panel (d) is the 5\% significance level.
			}
		\end{tablenotes}
	\end{minipage}
\end{figure}

The sequential Cauchy combination testing procedure requires two assumptions. 
Let $\bm{X}=(X_{1},X_{2},\ldots,X_{d})^{^{\prime }}$ represent the vector of test statistics. 

\begin{assumption}
	\label{ass1} (1) The original test statistics $(X_{i},X_{j})$, for any $1\leq
	i<j\leq d$, follow a bivariate normal distribution; (2) $E\left( \bm{X}%
	\right) =0$. 
\end{assumption}
The requirement of bivariate normality in Assumption \ref{ass1} is a condition weaker than joint normality, enabling the procedure to be applicable for high-dimensional settings. 
When the dimension $d$ increases at a certain rate with the sample size, the test statistics $\bm{X}$ may not jointly converge to a multivariate normal distribution due to a slower rate of convergence \citep[see][and references therein]{liu2020cauchy}, and assuming joint normality becomes unrealistic in such settings. 

%While there are  applications where the test statistics are non-normally distributed,
 \citet[][]{liu2020cauchy} show through simulations that the global Cauchy approximation remains accurate even when the normality assumption is violated, and follows a multivariate Student-$t$ distribution (with four degrees-of-freedom) instead. 
For a showcase example in finance where the test statistics are Student-$t$ distributed, we refer the reader to Section \ref{secApplFan}. 

\begin{assumption}
	\label{ass2} Let $\mathbf{\Sigma }=corr\left( \bm{X}\right) $. 
	(1) The	largest eigenvalue of the correlation matrix $\lambda _{\max}\left( \mathbf{%
		\Sigma }\right) \leq C_0$ for some constant $C_0>0$; 
	(2) $\max_{1\leq i<j\leq
		d}\left\{ \sigma _{i,j}^{2}\right\} \leq \sigma _{\max }^{2}<1$ for some
	constant $0<\sigma _{\max }^{2}<1$, where $\sigma _{i,j}$ is the $\left(
	i,j\right) $ element of $\mathbf{\Sigma }$.
\end{assumption}
Assumption \ref{ass2} on the correlation matrix becomes relevant when the number of hypotheses $d$ diverges to infinity. 
It imposes two conditions: boundedness of the largest eigenvalue of the correlation matrix and the absence of perfectly correlated test statistics. 
The conditions are frequently encountered in high-dimensional settings and general enough to encompass a wide range of tests.\footnote{However, this assumption excludes very strong dependence and latent factors shared by the test statistics in high-dimensional settings. Investigating the relaxation of this assumption is left for future research.}

\begin{theorem}\label{thm}
	Under Assumption \ref{ass1} for a fixed $d$  and Assumptions \ref{ass1} and \ref{ass2} for $d=o(h^{\eta})$ with $0<\eta<1/2$, as $\alpha\rightarrow 0$,  the probability of the SCC testing procedure making at least one false rejection converges to $\alpha$, i.e., 
\begin{equation}\label{eq:conv}
\lim_{\alpha\rightarrow 0}\Pr\left\{\mathcal{R} \cap \mathcal{T}=\emptyset\right\} \rightarrow \alpha.
\end{equation}
\end{theorem}
The proof of Theorem \ref{thm} is provided in Appendix \ref{sec:proof}.  The theoretical result in \eqref{eq:conv} for the SCC testing procedure stands in stark contrast to statistical inequality-based controlling procedures covered in the Online Supplement, which have the property: 
$$\Pr \left\{ \mathcal{R}\cap \mathcal{T\neq \emptyset }\right\} \leq \alpha.$$  See \cite{goeman2010sequential} for a discussion of their theoretical properties. These inequality-based controlling procedures ensure that the likelihood of making at least one false discovery is bounded above by the pre-specified significance level, $\alpha$.  Consequently, the SCC procedure exhibits less conservatism compared to inequality-based controlling procedures. 
