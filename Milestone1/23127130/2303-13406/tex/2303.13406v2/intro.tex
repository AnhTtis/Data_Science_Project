% !TEX root = ./CauchyCombination.tex

% \newpage
\section{Introduction}

There are many needle-in-a-haystack problems in empirical finance. There are tests to detect skilled funds \citep{barras2010false,giglio2021thousands}, nonzero alpha stocks  \citep{fan2015power}, explanatory factors  \citep{harvey2016and}, profitable technical trading rules \citep{bajgrowicz2012technical,sullivan1999data}, jumps and drift bursts in high-frequency asset prices \citep[][]{lee2007jumps, leemykland2012jumps,  bajgrowicz2015jumps,christensen2014fact,christensen2018drift},  to name but a few examples. 
These statistical tests have in common that, in order to detect a signal, they require applying the same  test repeatedly. This repetition arises either from the presence of a large cross-section of units (e.g., different funds, stocks, factors or trading rules) or because the test is required to be applied continuously over time (every minute or every day). 
It is commonly known that the simultaneous testing of multiple hypotheses is prone to a ``false discovery problem". As more and more tests are performed, an increasing number of them will be significant purely due to chance.\footnote{A well-known example of disregarding the multiple testing problem is so-called ``data snooping" or ``\textit{p}-hacking", which is a misuse of statistical testing: one  exhaustively searches for signals without compensating for the number of inferences being made 
	\citep[see e.g.,][and references therein]{giglio2021thousands}.} 
	To  counteract the inflation of false discoveries, researchers typically treat 
	the  hypotheses as a ``family" and set a threshold which controls a combined measure of error across all tests, rather than the Type I error of an individual test. 
	The procedures we focus on in this paper 
	address this issue by 
	imposing an upper bound, denoted as $\alpha$, on the probability of making at least one false discovery, thereby controlling the so-called ``familywise error rate"  \citep[see e.g.,][for reviews]{shaffer1995multiple,goeman2014multiple}.\footnote{Other procedures control, for example, the expected proportion of false discoveries or the so-called ``the false discovery rate" \citep[see e.g.,][]{benjamini1995,barras2010false,giglio2021thousands}.}

The dependence between test statistics plays a crucial role in the effectiveness of multiple testing corrections. 
In scenarios where the test statistics are independent and adhere to a standard normal distribution under the null hypothesis (as observed in certain jump tests, for instance), there exist well-established statistical solutions.\footnote{Popular approaches to address the false discovery problem in jump detection include 
	choosing a critical value corresponding to an extremely high quantile of the normal distribution 
	\citep[as in][]{andersen2007no} or 
	choosing another threshold based on the quantile of the  asymptotic distribution of the maximums of the test statistics \citep[as in][]{lee2007jumps,leemykland2012jumps}.} 
However, in many applications in economics and finance, assuming independence among the test statistics is implausible, because 
many popular test statistics are constructed from  overlapping rolling windows or are computed from stock returns that are likely to be driven by common factors. 
In scenarios with dependent test statistics, popular multiple testing corrections, such as those based the Gumbel distribution \citep[e.g.,][]{lee2007jumps} or statistical inequalities such as the Bonferroni correction and its subsequent improvements \citep{holm1979simple,hommel1988stagewise,hochberg1988sharper} protect against false discoveries, but are known to be overly conservative. The familywise error rate of these methods often turns out to be  much smaller than the desired upper bound $\alpha$. 
Simulation-based methods have also been used to
account for the observed correlation of the test statistics when setting a threshold  \citep[e.g.,][]{white2000,romano2005exact,romano2005stepwise}, but they are not ideal either. 
Aside from being computationally intensive,  they impose a strong parametric assumption on the dependence structure \citep[e.g., a Gaussian AR(1) process as in][]{christensen2018drift}, which could be misspecified. 
%Additionally, they attempt to reproduce the dependence structure under the null hypothesis using possibly contaminated data \citep[as seen in the attenuation bias reported in][]{christensen2018drift},  which in turn can affect the estimation of the critical value.

In this paper, we introduce a simple tool to control for false discoveries, while  being agnostic about the dependence among the test statistics. 
The solution we propose only uses raw $p$-values and
is built upon the Cauchy combination test of \citet{liu2020cauchy}, which tackles the issue of dependence in the test statistics from another perspective. 
Their global Cauchy combination (GCC) test  is grounded on a convenient theoretical property of Cauchy distributions, 
which states that  linear combinations of these variates behave similarly to a standard Cauchy variate at extreme tails, regardless of the dependence structure.  
Drawing from this insight, \citet{liu2020cauchy} propose a transformation of raw $p$-values, such that the transformed $p$-values follow a standard Cauchy distribution under the null hypothesis, and then construct a new test statistic as a linear combination of these transformed $p$-values, with its corresponding critical value derived from a Cauchy distribution. 
In doing so, they prove that the familywise error rate of the GCC test  converges to the nominal size $\alpha$ as the significance level $\alpha$ tends to zero, when all hypotheses are true and test statistics have arbitrary dependency structures. 
This test is well-suited to deal with the challenges posed by correlation,  high-dimensionality, and sparsity, but it is designed for  inferences about a global hypothesis. 
It is not obvious how statements about individual hypotheses are to be made with this procedure. 

We extend the pioneering work of \citet{liu2020cauchy} by introducing a sequential version of the Cauchy combination test to pinpoint the individual hypotheses that trigger the rejection of the global null, enabling the identification of  individual signals, such as, 
skilled  funds,  nonzero alpha stocks, explanatory factors, or timestamps of jumps and flash crashes.
%The sequential test follows a step-up methodology \citep[see e.g.,][]{hommel1988stagewise}: 
We apply the GCC test recursively on expanding subsets of $p$-values, starting from the largest and progressing to the smallest $p$-value. This process generates a sequence of Cauchy combination test statistics. The $p$-values associated with these test statistics are computed based on a standard Cauchy distribution. Individual violations are detected when the corresponding $p$-value is lower than a predefined threshold $\alpha$. 
We refer to this new testing procedure as the sequential Cauchy combination (SCC) test, which inherits all the convenient theoretical properties of the GCC test, including being agnostic about the dependence structure. 
We prove that the SCC test achieves strong familywise error rate control as the significance level $\alpha$ tends to zero, regardless of whether the number of individual hypotheses is fixed or infinite. Moreover, 
compared to the benchmark procedures, the familywise error rate of the SCC test is closer to the theoretical upper bound, which boosts the power and helps to better identify the individual  signals. 
 
To showcase the advantages of the sequential Cauchy combination test, we revisit two 
multiple testing problems in financial econometrics that exhibit  non-trivial correlation structures in the test statistics, high dimensions, and sparse signals, which are common challenges in the field. 


\begin{itemize}
\item In the first example, we revisit the drift burst hypothesis of \cite{christensen2018drift}, which aims to identify explosive trends in  stock prices. 
The drift burst test statistic relies on ultrahigh-frequency data and is applied multiple times within a trading day. 
The test statistics are constructed from overlapping rolling  windows and exhibit serial dependence. Drift bursts are rare events, and the strength of this signal varies over time.  

\item In the second example, we test for multiple nonzero alphas within the \citet{fama2015five} five-factor model framework. If the model fully explains asset returns, the estimated ``alphas" should be statistically indistinguishable from zero.
The presence of unknown common factors generates strong cross-sectional  dependencies among the test statistics \citep[see e.g.,][]{giglio2021thousands}. 
Nonzero alphas are typically rare and weak \citep[see e.g.,][and references therein]{fan2015power}. 
\end{itemize}

To assess 
the robustness of the SCC test against different   forms of dependence,  
we conduct two sets of simulation studies. 
The first set involves directly generating  test statistics with different correlation matrices. 
The second set involves generating data from a specific underlying process and computing a sequence of test statistics from the data, mimicking the situation in real-world empirical applications. 
Specifically, we simulate log prices of financial assets using a continuous-time drift bursting process in Example 1, and excess returns from a factor model in Example 2. 
The main findings from these simulations highlight that the SSC test outperforms other  multiple testing corrections, including statistical inequality-based approaches, methods based on extreme value theory, resampling, and screening approaches.  Despite its simplicity, the SCC test demonstrates superior properties 
in terms of  minimizing conservativeness and maximizing successful detections. 

The rest of the paper is organized as follows.
 Section \ref{secPrelims} introduces the general notation, definitions, and introduces both the global and sequential Cauchy combination tests. 
 Section \ref{secSims} illustrates the finite sample
 performance of the sequential Cauchy combination test in a simulation experiment with different types of correlations, relative to other multiple testing corrections.
 Sections \ref{secApplDriftBurst} and \ref{secApplFan}  revisit the two financial applications. 
 Section \ref{secConc} concludes. 
Appendix \ref{sec:proof} contains the proofs. 
The Online Supplement provides detailed information on the benchmark procedures, along with additional descriptions and simulations of the drift burst test and the nonzero alpha test.




