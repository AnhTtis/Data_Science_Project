\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}

\usepackage{amsfonts}
% \usepackage{graphicx,subfig}
 \usepackage{graphicx} % subcaption <> subfig
\usepackage[authoryear]{natbib}
\usepackage{xcolor}
\usepackage{float}
\usepackage{rotating} % Rotating table
\usepackage[doublespacing]{setspace}
% \usepackage[onehalfspacing]{setspace}
\usepackage[title]{appendix}
% \usepackage{hyperref}
\usepackage[breaklinks=true]{hyperref} % arxiv
\usepackage{breakcites} % arxiv
\usepackage{url}
\usepackage{xr}
\usepackage{bm}
\usepackage{cleveref}
%\usepackage{showkeys}
\RequirePackage{amsmath,amssymb}
\hypersetup{colorlinks=true,citecolor=blue, linkcolor=blue, urlcolor=blue}
\newtheorem{assumption}{Assumption}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{prop}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{case}[theorem]{Case}
\newtheorem{Corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{condition}{Condition}[section]
\numberwithin{equation}{section}

% extra packages
\usepackage{threeparttable} % tablenotes cmd
\usepackage{physics} % \abs cmd
\usepackage{adjustbox} % adjustbox in tables
\hypersetup{colorlinks=true,citecolor=blue, linkcolor=blue, urlcolor=blue}
\usepackage{subcaption} % subfig <> subcaption
\usepackage{ulem}
\usepackage{changes}
%\definechangesauthor[name={Shuping}, color=red]{}
\usepackage{tcolorbox}

\RequirePackage[left=2cm,
right=2cm,
top=1in,
bottom=1in,headheight=12pt]{geometry}

\renewcommand{\thetheorem}{\arabic{theorem}}
\newcommand{\red}{\textcolor{red}}
\newcommand{\blue}{\textcolor{blue}}
\newcommand{\green}{\textcolor{green}}
\newcommand{\Prob}{\ensuremath{\mathbb{P}}} % Drift-burst section
\DeclareMathOperator*{\argmax}{arg\,max}

\allowdisplaybreaks
\graphicspath{{Figures/}}
\externaldocument{CauchyCombination}
\renewcommand{\baselinestretch}{1.5}
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\externaldocument{ms}
\newcommand{\blind}{0}

\begin{document}
		\def\spacingset#1{\renewcommand{\baselinestretch}%
		{#1}\small\normalsize} \spacingset{1}

\if1\blind
{
\title{\bf Online Supplement to ``Sequential Cauchy Combination Test for Multiple Testing Problems with Financial Applications''\thanks{
 		$^{\ddag}$ Nabil Bouamara, Louvain Institute of Data Analysis and Modeling in economics and statistics, Universit\'{e} catholique de Louvain;  	Email: nabil.bouamara@uclouvain.be.\\
 		$^{\ddag\ddag}$ S\'{e}bastien Laurent, Aix-Marseille University (Aix-Marseille School of Economics), CNRS \& EHESS, Aix-Marseille Graduate School of Management -- IAE; Email: sebastien.laurent@univ-amu.fr.\\ 
 		$^{\ddag\ddag\ddag}$ Shuping Shi, Department of Economics, Macquarie University; Email: shuping.shi@mq.edu.au. 
}
\vspace{0.1cm}
}

	
\author{Nabil Bouamara$^{\ddag}$, S\'{e}bastien Laurent$^{\ddag\ddag}$, Shuping Shi$%
^{\ddag\ddag\ddag}$ \\}

\maketitle
} \fi

\if0\blind
{
	\bigskip
	\bigskip
	\bigskip
	\begin{center}
		{\LARGE\bf Online Supplement to ``Sequential Cauchy Combination Test for Multiple Testing Problems with Financial Applications''}
	\end{center}
	\vspace{2cm}
} \fi


 
% \date{March 23, 2023} % arxiv recompiles tex files 
% remove to update file. 


		\begin{abstract}
		This online appendix consists of three sections. Section \ref{ssecBenchmarks} gives an overview of the benchmark procedures, which includes four inequality-based methods and two procedures based on the maximum of the test statistics. 
		Section \ref{AppDB} complements Section % \ref{secApplDriftBurst} 
		4 
		in the main text by providing additional information on the noise-robust estimators used in the drift burst test, the implementation of the resampling procedure, and a simulation study comparing different multiple testing corrections when monitoring drift bursts. 
		Section \ref{AppAlpha} complements Section % \ref{secApplFan} 
		5 
		in the main text by providing additional details on the screening procedure and includes a simulation study focused on the nonzero alpha test.
					
		\end{abstract}
	
\singlespacing % arxiv 
% \spacingset{1.9} % DON'T change the spacing! (JASA)


\section{Benchmarks}
\label{ssecBenchmarks}

The benchmark multiple testing corrections controlling the familywise error rate can be classified into two categories: those based on statistical inequalities and  those based on the maximum of the test statistics. We discuss their theoretical behavior when confronted with correlated test statistics. 

\subsection{Procedures based on statistical inequalities}
\label{ssecOrderdPvals}

We review four popular statistical inequality-based procedures:  the Bonferroni correction and the subsequent improvements by  \citet{holm1979simple}, \citet{hommel1988stagewise} and \citet{hochberg1988sharper}. 
The Bonferroni correction is the most commonly used method, where the null  hypothesis $H_{(i)}$ is rejected if the corresponding $p$-value is smaller than a stringent critical value  $c_i = \alpha/d$  for $i=1,\ldots,d$. The threshold is the same for all $p$-values. 

\citet{holm1979simple} and \citet{hochberg1988sharper} order the $p$-values in an ascending order, i.e.,  $p_{(1)}\leq p_{(2)}\leq \ldots \leq p_{(d)}$ with  $H_{(1)},H_{(2)},\ldots,H_{(d)}$ corresponding to their respective null hypotheses, and set the thresholds based on their ranks, $c_i=\alpha / (d - i + 1)$ for $i=1,2,\cdots,d$. However, they differ in their rejection procedures. 
% 
\citet{holm1979simple}'s method ``steps up" from the smallest $p$-value to the largest $p$-value. It is a pessimistic approach that  scans forward and stops as soon as a $p$-value fails to clear its threshold. In other words,  at any stage $i$, $H_{(i)} $ is rejected only if $p_{(i)}$ $\leq c_i$ and all preceding hypotheses $H_{(j)}$ with $j< i$ have been rejected. 
% 
On the other hand, \citet{hochberg1988sharper}'s method ``steps down" from the largest $p$-value to the smallest $p$-value, comparing $p_{(i)}$ against the critical value $c_i $. It is an optimistic approach that scans backward and stops as soon as a $p$-value succeeds in clearing its threshold, rejecting all remaining hypotheses. 
By design, Hochberg's method will reject as many hypotheses as Holm's method. 

\cite{hommel1988stagewise} proposes a more intricate procedure that applies  \citet{simes1986improved}' global test to subsets of $p$-values  $\left\{ p_{\left(k\right) }\right\} _{ k = i }^{d}$, instead of relying  only on a single $p$-value $p_{\left(i\right)}$, and 
thus borrows power across hypotheses. 
Hommel's method rejects the hypotheses $H_{(i)}$ if $p_{(i)} \leq c_i = \alpha / j$, where 
$$
j=\argmax_{i\in[1,n]} \left\{i: p_{(n-i+k)} >  k \alpha  / i  \text{ for all } k = 1, \ldots, i\right\}.
$$ 
If the maximum does not exist, then all hypotheses are rejected. The procedure uses the closure principle \citep{marcus1976closed} to extend \citet{simes1986improved}'s global test, and makes statements about individual hypotheses. 
It has been shown that Hommel's method has higher power than Hochberg's method \citep{hommel1989}. 

Bonferroni and \citet{holm1979simple}'s methods are based on the first-order Bonferroni inequality, which states that given any set of events, the probability of the union of a set of events is smaller than or equal to the sum of their individual probabilities. 
Under the null hypothesis, 
the probability that at least one hypothesis $H_{(i)}$ has a  $p$-value $p_{(i)} \leq \alpha / d$ 
is bounded by $\alpha$: 
\begin{align}  \label{eqIneqPval}
	\Pr\left( \min_{i} p_{(i)} \leq \frac{\alpha}{d} \right) 
	= \Pr\left(\bigcup_{i =
		1}^{d} \left\{ p_{(i)} \leq \frac{\alpha}{d} \right\} \right) 
	&\leq
	\sum^{d}_{i = 1} \Pr \left( p_{(i)} \leq \frac{\alpha}{d}\right) \leq d
	\frac{\alpha}{d} \leq \alpha.
\end{align}
The Bonferroni \eqref{eqIneqPval} inequality 
does not make any specific assumption about the dependence between the $p$-values, but it provides protection against the ``worst-case" scenario where all events are independent and the rejection regions are disjoint. 
The inequality becomes an equality when all test statistics are independent, and it is a strict inequality when the hypotheses are dependent. 
In other words, the Bonferroni correction is  conservative when the $p$-values are correlated.  

The methods proposed by \cite{hochberg1988sharper} and \cite{hommel1988stagewise} rely on  \cite{simes1986improved}'s inequality.
According to \citeauthor{simes1986improved}' inequality, if a set of hypotheses $H_{(1)}, ...,H_{(d)}$ are all true, the probability of the joint event is: 
\begin{equation}  \label{eqSimes}
	\Pr\left(\bigcap_{i=1}^{d} \left\{p_{(i) }> \frac{i\alpha}{d}\right\}\right) \geq 1-\alpha.
\end{equation}
\citeauthor{simes1986improved}' inequality was initially developed for independent uniform $p$-values but it is applicable to a wide large of multivariate distributions. 
However, simulations conducted by \citet{simes1986improved} show that the  \citet{simes1986improved} test can be overly conservative for highly correlated multivariate normal statistics, although it is generally less conservative than the classical Bonferroni correction. 


\subsection{Procedures based on the maximum of test statistics}
\label{ssecMaxTest}

Another category of controlling procedures involves the use of the maximum of a group of test statistics, denoted as $X_m = \max_{i} \abs{X_{i}}$, where $i = 1, \ldots, d$. This maximum statistic is used to set a stringent critical value that can be applied to each individual hypothesis, ensuring control over the familywise error rate. 
Specifically, when the individual test statistics are independent and follow a standard normal distribution under the null hypothesis, a normalized version of $X_m$ converges to a Gumbel distribution. The distribution of the normalized maximum statistic is given by: 
\begin{eqnarray}
	\frac{X_m - C_d}{S_d} 	\sim	G,
\end{eqnarray}
where $G$ has a cumulative distribution function of 
$\Prob (G \leq x) = \exp (- \exp (-x))$. The constants $C_d$ and $S_d$ are defined as: 
\begin{eqnarray}
	C_d = 
	\sqrt{2 \log d} 
	-
	\dfrac{\log \pi 
		+ 
		\log(\log d)
	}
	{2\sqrt{2 \log d}}
	\, \,  \, \text{and} \, \,  \, 
	S_d = \dfrac{1}{\sqrt{2\log d}}.
\end{eqnarray}
To reject the $i$th null hypothesis $H_{i}$, we compare the absolute value of $X_i$ against the threshold $G^{-1} (1 - \alpha) S_d + C_d$, where $G^{-1} (1 - \alpha)$ represents the $(1-\alpha)$ quantile of the standard Gumbel distribution.

For instance, the Gumbel critical value has been used as a multiple testing correction when testing for  jumps  in high-frequency asset returns \citep[][]{lee2007jumps}.
However,  when the  sequence of test statistics exhibits strong correlation, the number of tests severely overstates the effective number of independent copies within a given sample. The Gumbel critical values tend to be overly conservative in such cases \citep[see e.g.,][]{christensen2018drift}. 

Resampling-based methods loosen the assumption of independence by incorporating the specific dependence structure in the  dataset under consideration, resulting in less conservative testing outcomes compared to the Gumbel method and the inequality-based procedures. 
Depending on the empirical problem of interest, the resampling can be carried out by bootstrap, permutation, simulation, or randomization \citep[see e.g.,][for comprehensive discussions on resampling methods and testing procedures]{white2000,romano2005exact,romano2005stepwise,lehmann2005testing}. 
An example of the resampling-based approach can be found in Section
 \ref{secResampling}. However, it is important to note that resampling methods can  be computationally intensive and require strong parametric assumptions to be imposed on the underlying dependence structure.






\section{Example 1: Monitoring Drift Burst}
\label{AppDB}

In Section \ref{AppDBEstim}, we provide details on the drift and variance estimators used to construct the drift burst test statistic discussed in the main text. 
Section \ref{secResampling} introduces the resampling-based method proposed by \citet[][Appendix B]{christensen2018drift}, designed to control the familywise error rate when monitoring drift bursts. Section \ref{ssecDBSim} outlines a simulation study that compares different multiple testing corrections for the drift burst test.

\subsection{Drift and Variance Estimators}
\label{AppDBEstim}

To reduce the impact of market microstructure noise, \citet{christensen2018drift} use pre-averaged returns. These pre-averaged returns as computed as: 
\begin{eqnarray}
	\label{eqPreavRets}
	\Delta_i^n \bar{P} 
	= \sum^{k_n - 1}_{j = 1} 
	g_j^n 
	\, 
	\Delta^n_{i + j} \widetilde{P},
\end{eqnarray}
where $\Delta_i^n \widetilde{P} = \widetilde{P}_{t_i} - \widetilde{P}_{t_{i-1}}$ is the discretely sampled noise-contaminated log return over $[t_{i-1}, t_i]$,  $k_n$ is the pre-averaging window, and $g_j^n = g(j/k_n)$ is a weight function. 
For the construction of the drift burst statistic, \citet{christensen2018drift} specify the weight function as $g(x) = \min(x, 1-x)$ and set the pre-averaged window $k_n$ to $3$.

The noise-robust estimator for the drift $\mu_t$ is computed from the pre-averaged returns as:
\begin{equation}
	\label{eqHatMu}
	\begin{split}
		\hat{\bar{\mu}}_t^n 
		&= 
		\frac{1}{h_n} 
		\sum^{n-k_n+2}_{i = 1} 
		K \left( \frac{t_{i-1} - t}{h_n} \right) 
		\Delta^n_{i-1} \bar{P} ,
	\end{split}
\end{equation}
where $t \in [0,T]$, $h_n$ is a bandwidth parameter, and $K(.)$ is a kernel function. 

The noise-robust variance estimator $\hat{\bar{\sigma}}_t^{2,n} $ 
is a heteroscedasticity and autocorrelation consistent (HAC)-type statistic, accounting for dependence in the pre-averaged returns. It is computed as: 
\begin{equation}
	\label{eqHatSigma}
	\begin{split}
		\hat{\bar{\sigma}}_t^{2,n} 
		&= 
		\frac{1}{h'_n} 
		\Bigg\{
		%
		\sum^{n-k_n + 2}_{i = 1} 
		%
		\left[
		K 
		\left(
		\frac{t_{i-1} -t}{h'_n} 
		\right)
		\Delta^n_{i-1} \bar{P}
		\right]
		^2
		%
		%
		%%%%
		\\
		&\quad +  
		2 \sum^{L_n}_{L = 1}
		w 
		\left( 
		\frac{L}{L_n}
		\right)		
		\sum^{n-k_n-L+2}_{i = 1}
		K 
		\left(
		\frac{t_{i-1} -t}{h'_n} 
		\right)
		%
		K 
		\left(
		\frac{t_{i+L-1} -t}{h'_n} 
		\right)
		%
		\Delta^n_{i-1} \bar{P}
		\Delta^n_{i-1+L} \bar{P}		
		\Bigg\}
		, 
	\end{split}
\end{equation}
where $h'_n$ is a bandwidth parameter, $\omega(.)$ is a kernel function, and $L_n$ is the maximum autocorrelation lag length. 
The lag length is determined as $L_n = Q^* + 2 (k_n - 1)$, with $Q^*$ computed from the raw returns $(\Delta \widetilde{P})_{i = 0}^n$ using automatic lag selection. 

To avoid look-ahead bias, \citet{christensen2018drift} suggest for $K(.)$ a left-side exponential kernel, $K(x) = \exp (-\abs{x})$ with $x \leq 0$. 
In our implementation, we use a ten-minute 
bandwidth for the mean estimator 
($h_n$ = 600) 
and a  50-minute 
bandwidth ($h'_n = 5h_n$) 
for the volatility estimator. 

The kernel function $w: \mathbb{R}_+ \rightarrow \mathbb{R}$ chosen satisfies the properties $w(0)=0$ and $w(x) \rightarrow 0$ as $x \rightarrow \infty$. We use a Parzen kernel for $w(.)$, defined as: 
\begin{eqnarray*}
	w(x)
	= \left\{
	\begin{array}{ll}
		1 - 6x^2 + 6 \abs{x}^3, 
		& \text{for} \, \, 0 \leq \abs{x} \leq 1/2, \\
		2(1-\abs{x}^3), 
		&  \text{for} \, \, 1/2 < \abs{x} \leq 1, \\
		0, 
		& \text{otherwise.} \\
	\end{array}
	\right.
\end{eqnarray*}


\subsection{Resampling}
\label{secResampling}

Resampling aims to approximate the dependence structure of the test statistic sequence under the null hypothesis and obtain its distribution through simulation. Specifically, the dependence structure is approximated by a stationary Gaussian autoregressive process of order one given by:   
\begin{equation} 
	\label{eqXAR1}
	X_{i}=\theta X_{i-1}+\epsilon _{i}, 
	\, 
	\text{for } i = 1, \ldots, d,
	\text{ with }\abs{\theta}<1\text{ and }%
	\epsilon _{i}\overset{i.i.d.}{\sim }N(0,1-\theta^{2})\text{.}
\end{equation}
Under this specification,  the autocovariance function of $X_i$ is $cov\left( X_{i},X_{j}\right) =\theta^{\left\vert i-j\right\vert }$, exhibiting exponential decay. The autoregressive coefficient $\theta$ in \eqref{eqXAR1} can be replaced with an estimate obtained from the empirical test statistic sequence $\left\{X_{i}\right\} _{i=1}^{d}$ using  conditional maximum likelihood.

The resampling procedure involves 
generating $R$ sequences of test statistics from \eqref{eqXAR1}. 
For each sequence, $r=1,\ldots,R$,  the maximum value  $X_{m}^{r} = \max_{i = 1, \ldots, d} \abs{X^r_i}$, 
is computed, resulting in a collection of simulated maxima $\left\{ X_{m}^{r}\right\} _{r=1}^{R}$. The critical value of the two-sided drift burst test is then determined as the $1-\alpha $ quantile of these simulated maxima. 

Figure \ref{figCOR18_fig8} plots the simulated critical values as a function of the autoregressive coefficient $\theta$ for $d = 2,500$ at three significance levels (1\%, 5\%, and 10\%), as in \citet[][Figure 8]{christensen2018drift}.
Comparing these simulated finite-sample critical values with those derived from the asymptotic Gumbel distribution, some observations can be made. 
When the autocorrelation is weak (i.e., $\theta \approx 0$) and the confidence level is set at $10\%$, the gap between the Gumbel and simulated critical values is relatively small. However, as we move towards extreme tail probabilities (e.g., $\alpha = 1\%$),  even when the test statistics are uncorrelated, the gap becomes more prominent.  
This behavior is expected since the convergence in law to the Gumbel distribution for the maximum term  is known to be slow \citep[as discussed  in][Appendix B, and references therein]{christensen2018drift}. 
The gap starts to widen noticeably in the region where the autoregressive coefficient exceeds $0.7$, a situation frequently encountered in rolling window implementations. 
The resampling-based method is expected to be less conservative than the Gumbel method (and the inequality-based methods) when the autocorrelation structure of $X_i$ follows an AR(1) process and when the estimate is accurate (i.e., $\hat{\theta}$ is close to $\theta$). This is because the resampling method takes into account the data-specific dependence structure. 

\begin{figure}[!ht]
	\centering
	\caption{Simulated critical values for the drift burst test}
	\includegraphics[width=.6\textwidth,angle = 0]{cv_sim_edit}
	\label{figCOR18_fig8}%
	\begin{minipage}{1.0\linewidth}
		\begin{tablenotes}
			\small
			\item {
				\medskip
				Note: 
				This figure plots the (resampling-based critical values of the maxima when $d = 2,500$ and the Gumbel critical values. The figure shows how the varying degree of dependence (captured by $\theta$ ranging from $-0.5$ to $0.99$) affects the critical values at three different confidence levels. It extends \citet[][Figure 8]{christensen2018drift} to allow $\theta$ to take negative values.
				The simulation includes $R = 10^7$ replications with a burn-in of $10^4$ observations.
			}
		\end{tablenotes}
	\end{minipage}
\end{figure}

There are a few limitations associated with using simulated critical values in practice. 
The critical value obtained through resampling is unique for each sequence of test statistics. It is unique for each day in an empirical analysis and for each sample path in a Monte Carlo study. 
Obtaining these critical values is therefore more computationally demanding than other multiple testing corrections. 
To save time, a table can be prepared in advance, containing quantile functions of normalized maxima for different values of the autoregressive coefficient $\theta$ and dimensions $d$. 
However, when the estimated first-order autocorrelation and dimension are not included in the table, an interpolation routine becomes necessary. 

It is also important to note that the use of simulated critical value may be inappropriate to assess the strength of evidence against the null hypothesis.  As documented by \citet[][Appendix B, pp. 494]{christensen2018drift}, ``the estimated ACF [of an AR(1)]  is close to the observed one [for simulated data], although there is a slight attenuation bias for the empirical estimates". 
In practice, the estimation of the parameter $\theta$ in \eqref{eqXAR1} relies on empirical data that can be generated under either the null or alternative hypothesis.
This unavoidable mixture of test statistics under the null and the alternative hypotheses introduces  the possibility of biased autocorrelation estimates and simulated critical values, which can ultimately result in biased power calculations. 
Therefore, caution should be exercised when interpreting the results, and the potential biases associated with the parameter estimation must be considered. 



\subsection{Simulation Study}\label{ssecDBSim}


We evaluate the performance of several multiple testing corrections for monitoring drift bursts in a  simulation setting. 
Specifically, we evaluate the performance of the four inequality-based procedures, the Gumbel method, the resampling approach, and the SCC testing procedure. 
Instead of directly simulating the test statistics as done in Section % \ref{secSims},  
3, 
we generate log prices from % \eqref{eqDGPNull} 
(4.1)
under the null hypothesis and % \eqref{simDB} 
(4.3) 
under the alternative hypothesis, and then compute test statistics based on the simulated prices. 
Under the null hypothesis, we set $\mu_t^b=\sigma_t^b=0$ in Equation % \eqref{eqDGPAlt}, 
(4.2), 
while under the alternative we set $\mu_t^b$ and $\sigma_t^b$ as in % \eqref{simDB}.
(4.3). 

In line with \citet{christensen2018drift}, we use the following (annualized) parameters for the \citet{heston1993closed}-type variance process, namely $(\kappa, \omega, \xi, r) = (5, 0.0225, 0.4, -\sqrt{0.5})$. 
The parameter $\omega$ corresponds to a return volatility of roughly 20\% per annum. 
In each simulation, $\sigma_t^2$ is initiated at a random draw from its stationary law  $\sigma_t \sim \text{Gamma} (2 \kappa \theta \xi^{-2}, 2 \kappa \xi^{-2})$. 

Under the alternative hypothesis, we consider two different drift and volatility bursting patterns: 
\begin{enumerate}
	\item Flash Crash. 
	In this scenario, 
	the price experiences a brief, V-shaped flash crash lasting approximately $20$ minutes ($\Delta t = 0.025$) in the middle of the trading day ($\tau_{\text{b}} = 0.5$), following  \citet[][]{christensen2018drift}. Signals are sparse: the percentage of signals is about 6\% (20 out of 341 time intervals). 
	The strength of the signal is not constant, but  it reaches its peak at the bottom of the ``V" shape.  The drift burst rate is set to $\alpha^\text{b} = 0.65$ and the volatility burst rate is set to $\beta^\text{b} = 0.4$. The parameters $a$ and $b$ are set to $3$ and $0.15$, respectively. 
	The parameter choices generate a mild burst event.
	
	\item Persistent Expansion. 
	In this scenario, the price experiences  a three-day upward trend inspired by  \cite{laurent2020drift} and \cite{laurent2020volatility,laurent2022unit}. 
	The upward trend starts at the beginning of day one and culminates in a peak at the end of the third day. 
	The percentage of signals is 100\%, and the signal strength is progressively intensifies over the three days. 
	We set $\alpha^\text{b} = 0.75$, $\beta^\text{b} = 0.40$, $a = 3$, and $b = 0.15$. 
\end{enumerate}

The log prices are contaminated with a noise term $\epsilon_{t_i} \sim	N(0, n^{-1/2}\gamma\sigma_{t_i})$,  	where the  noise-to-volatility ratio $\gamma$ is fixed at $0.5$. 
The noise term is conditionally heteroskedastic and serially dependent (through $\sigma_{t_i}$). 

We simulate data at the one-second frequency, assuming 6.5 trading hours each day, resulting in a total of $23,401$ observations per day. 
Figure \ref{figSamplePath} plots a typical sample path of the data-generating processes.

\begin{figure}[!h] % htb!
	\centering
	\caption{A typical sample path of the DGPs}
	\label{figSamplePath}
	\subfloat[log Price with Flash Crash]{{\includegraphics[width=.5\textwidth,angle = 0]{price_drift_a065b040} }}
	\subfloat[log Price with three-day rise]{{\includegraphics[width=.5\textwidth,angle = 0]{price_3drise} }}	\\
	\begin{minipage}{1.0\linewidth}
		\begin{tablenotes}
			\small
			\item {
				\medskip
				Note: 
				We plot a typical sample path of the DGPs % \eqref{eqDGPNull}
				(4.1)
				 under the null and % \eqref{eqDGPAlt}  
				 (4.2)
				 under the alternative with Flash Crash and Persistent Expansion patterns} 
		\end{tablenotes}
	\end{minipage}
\end{figure}

We detect drift bursts on a one-minute grid. 
Each day is treated separately, with a burn-in period of 49 minutes, resulting in 341 tests per day. 
These 341 tests are treated as a family, and we  control the familywise error rate at 5\%. 
The simulation is repeated 2,000 times. 
Table \ref{tabSimsDriftBurst} reports the 
familywise error rate, global power, and successful detection rates
of the different controlling procedures, 
under the specified conditions. 
Additionally, the third column reports the average estimated first-order autocorrelation coefficient of the test statistics $\bar{\hat{\rho}}$. 
It is worth noting that there are moderate differences between the null and the alternative hypotheses, with a value of $0.894$ under the null and higher values under the alternative hypothesis. 
The difference becomes more pronounced  when the signal persists, such as $\bar{\hat{\rho}}=0.94 $ on the third day of the persistent expansion process.

\begin{table}[!ht] % htb!
	\centering
	\caption{Finite sample performance of the controlling procedures for the drift burst test}
	\label{tabSimsDriftBurst}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{l l | c | cccccccc}
			\hline
			\vspace{-.4cm} \\
			\multicolumn{2}{c}{DGP}
			&\multicolumn{1}{c}{$\bar{\hat{\rho}}$} 
			&\multicolumn{1}{c}{Bonferroni}
			&\multicolumn{1}{c}{Holm} 
			&\multicolumn{1}{c}{Hommel}  
			&\multicolumn{1}{c}{Hochberg} 
			& \multicolumn{1}{c}{Gumbel} 
			&\multicolumn{1}{c}{Resampling}
			&\multicolumn{1}{c}{SCC} \\
			\hline
			&& \multicolumn{8}{c}{Empirical FWER} \\ 
			\multicolumn{1}{l}{Heston}
			& 
			& 0.89
			& 2.40  &  2.40 &   2.40   & 2.40   & 1.80 &   4.80 &  5.25
			\\\hline
			&& \multicolumn{8}{c}{Global power} \\ 
			\multicolumn{1}{l}{Flash Crash} 
			% bandwidth: hn = 600			
			&
			& 0.90
			& 67.70  & 67.70  & 67.70  & 67.70   & 65.45 &  74.45 & 69.75 
			\\
			%			\multicolumn{1}{l}{\added{2010 Flash Crash}}
			%			% bandwidth: hn = 600			
			%			&
			%			& 0.92 
			%			&  99.40   
			%			& 99.40  
			%			&  99.40  
			%			&  99.40   
			%			& 99.30
			%			& 99.60
			%			& 99.30
			%			\\
			\multicolumn{1}{l}{Persistent Expansion}
			& \text{Day 1}
			& 0.89
			& 5.90
			& 5.90
			& 5.90
			& 5.90
			& 4.75
			& 10.55
			& 15.55
			\\
			& \text{Day 2}
			& 0.89
			& 12.95
			& 12.95
			& 12.95
			& 12.95
			& 10.55
			& 19.85
			& 29.40
			\\
			& \text{Day 3}
			& 0.94
			& 100.00
			& 100.00
			& 100.00
			& 100.00
			& 100.00
			& 100.00
			& 100.00
			\\
			\hline
			&& \multicolumn{8}{c}{Successful detection rates} \\ 
			\multicolumn{1}{l}{Flash Crash} 
			& 
			& 0.90
			& 6.40  &  6.41  &  6.41  &  6.41 &   6.08  &  7.58 & 6.86
			\\ 
			\multicolumn{1}{l}{Persistent Expansion} 
			& \text{Day 1}
			& 0.89
			& 0.05
			& 0.05
			& 0.05
			& 0.05
			& 0.04
			& 0.11
			& 0.41
			\\
			& \text{Day 2}
			& 0.89
			& 0.13
			& 0.14
			& 0.14 
			& 0.14
			& 0.11
			& 0.25
			& 1.36
			\\
			& \text{Day 3}
			& 0.94
			& 11.20
			& 11.61
			& 12.06
			& 11.61
			& 10.73
			& 13.71
			& 23.59
			\\
			\hline
			\hline
	\end{tabular}}
\end{adjustbox}	
\parbox{\textwidth}{\footnotesize%
	\vspace{.1cm} % If wanted space after the bottomrule
	{Note}: 
	We consider different types of signals (Flash Crash and Persistent Expansion) and set the nominal level $\alpha$ to 5\%. The simulation is repeated $2,000$ times.
}
\end{table}

The SCC test and the resampling approach perform well under the null hypothesis, achieving  FWERs of $4.8\%$ and $5.25$\% respectively, at a nominal  level of 5\%. 
In contrast, the inequality-based  approaches and the Gumbel method are more conservative, with  empirical FWERs ranging between $1.8\%$ and $2.4$\% at a nominal level of 5\%.

Under the alternative hypothesis, the SCC test consistently outperforms the inequality-based approaches and the Gumbel method in terms of power and successful detection rate. 
When the signals persists through time, such as in the case of the Persistent Expansion, the SCC test exhibits higher global and local power  compared to the resampling approach. 
For instance, on the third day of the Persistent
 Expansion, the SCC test successfully detects approximately 24\% of intervals under the alternative, on average, while the resampling method only detects 14\% of intervals.
However, there are instances where the resampling approach shows higher power. 
When the duration of the signal is relatively short, such as in the case of the Flash Crash, the resampling approach performs better. 
Nonetheless, it is important to consider that a proper implementation of resampling is computationally demanding, and the critical values derived from it are susceptible to the attenuation bias (as discussed in \ref{secResampling}). 

Any potential contamination needs to be carefully considered when interpreting the results in the simulations (and  in the main text). 
Given the high autocorrelation of the drift burst test statistics, we are operating in the far-right region of Figure \ref{figCOR18_fig8}. 
Even minor downward or upward biased estimates of the autocorrelation under the null hypothesis can result in a loss or spurious power.  
Considering these challenges, we conclude that the SCC test is a more practical choice overall. It is easier to implement and can accommodate arbitrary dependency structures without requiring simulations. Although there might be a small loss in power observed in some specific cases, it is a reasonable trade-off for having a robust approach to monitor drift bursts. 

\section{Example 2: Nonzero Alpha Test}
\label{AppAlpha}

In Section \ref{ssecScreening}, we present the screening approach proposed by \cite{fan2015power}. 
In Section \ref{ssecSim}, we conduct a simulation study to assess the performance of the SCC test in  testing factor pricing models. 

\subsection{Screening}
\label{ssecScreening}

The power enhancement test proposed in \cite{fan2015power} is an important benchmark in testing factor pricing models. 
This global test examines whether there
is any asset with nonzero alpha, while also identifying individual violations as a byproduct. 
The global null hypothesis is that the alphas of the $d$ financial assets are jointly
indistinguishable from zero \citep[see e.g.,][]{fama1996multifactor}: 
$	\mathcal{H}_{0}:\bm{a} =\bm{0} \text{ with } \bm{a}=(a_{1},\ldots
,a_{d})^{\prime }.$

Classical tests for the global null hypothesis rely  on a quadratic statistic, which can present challenges in high-dimensional settings. 
Specifically, these tests typically have low power when faced with sparse alternatives. The accumulation of estimation errors under the null hypothesis causes the quadratic statistic to produce inflated critical values that overpower the signals present in the sparse alternatives \citep[see][for further
elaboration]{fan2015power}. 

The idea behind \citet{fan2015power}'s power
enhancement is to use a composite test statistic given by 
$J=J_{1}+J_{0}$, which combines a classical test statistic 
$J_{1}$ (with correct asymptotic size but low power under sparse alternatives) and a power-enhancing component $J_{0}$ designed to improve the power with minimal size distortion.  
% 
An example of a power enhancement component is a screening statistic: 
 \begin{equation}
 	\label{eqScreeningStat}
	J_{0}=\sqrt{d}\sum_{j\in \widehat{S}}
	\frac{\widehat{a}_{j}^{2}}{\widehat{\nu }%
		_{j}},
\end{equation}%
in which $\widehat{\nu }_{j}$ is  the estimated asymptotic variance of $%
\widehat{a}_{j}$ and $\widehat{S}$ is a screening set containing the indices where the null hypothesis is violated. 

The power enhancing screening statistic \eqref{eqScreeningStat} is not intended to function as a stand-alone test. 
However, the screening set $\widehat{S}$ within  $J_0$ is the only part of the composite test $J$ that can incidentally identify individual violations \citep[as demonstrated in the empirical application of][Section 6.3]{fan2015power} and will therefore be included in our simulations and empirical application.

The screening set $\widehat{S}$ is defined as follows: 
\begin{equation*}
	\widehat{S}=\left\{ j:\frac{\abs{\widehat{a}_j}}{\widehat{\nu }_{j}^{1/2}}%
	>\delta _{d,T},j=1,\ldots ,d\right\}  \text{ with } \delta _{d,T}=C\log (\log (T))\sqrt{\log (d)}. 
\end{equation*}%
where $\delta_{d,T}$ is a threshold  chosen to dominate the maximum noise
level. In our implementation, we set the constant $C$ to $1.06$ as specified in the code shared by \citet{fan2015power} for replicating their simulations. 

The screening set is designed such that it  imposes a 0\% theoretical FWER, as the screening set is asymptotically empty under the null hypothesis. 
Under the alternative hypothesis, the screening test, which is based on the screening set, is expected to have power and enhance the power of a classical test statistic, specifically in the region 
$\left\{\max_{j\leq d}\frac{\abs{a_j}}{\nu_{j}^{1/2}}>3\delta _{d,T}\right\}$ (with $\nu_j$ being the asymptotic variance of $a_j$). 
For more details, we refer the reader to \cite{fan2015power}. 

\subsection{Simulation Study}
\label{ssecSim} 

We evaluate the performance of several multiple testing corrections for testing factor pricing models. 
Specifically, we evaluate the performance of the four inequality-based procedures, the Gumbel method, the screening approach, and the SCC testing procedure. 
Instead of directly simulating the test statistics as done in Section % \ref{secSims},  
3, 
we generate excess returns from % \eqref{eqCrossDecomp}, 
(5.1),
and then compute test statistics based on the simulated returns. 
We exclude the resampling approach discussed in  Section \ref{secResampling}, as it is not suitable for the current cross-sectional context. 

We generate excess returns $y_{it}$ from the 
five-factor model % \eqref{eqCrossDecomp} 
(5.1)
proposed by \citet{fama2015five}. We consider a setting with $d=100$ assets and $%
T=240$ time series observations, inspired by 
\citet[][Section 6.1]{fan2015power} and 
\citet[][Section 4.2]{shi2022relax}. 
Under the null hypothesis, all alphas are zero ($a_i=0$), for $i=1,\ldots,d$. Under the
alternative hypothesis, we introduce multiple  weak signals, where the alphas are defined as: 
\begin{eqnarray}\label{eq:ai}
	a_{i}=\left\{ 
	\begin{array}{cc}
		0.5 & \text{if }i\leq d^{0.4} \\ 
		0 & \text{if }i>d^{0.4}
	\end{array}
	,\right.
\end{eqnarray}
which matches the scenario considered in our empirical application in Section % \ref{sec_KF}.  
5.2.

The factor loadings and factor returns are generated from independent
multivariate distributions:  $%
\mathcal{N}_5 (\bm{\mu_B}, \bm{\Sigma_B})$ for the factor loadings and $\mathcal{N}_5 (\bm{\mu_f}, \bm{\Sigma_f})$ for the factor returns. 
We calibrate the parameters to the empirical Fama-French 100 portfolios that are sorted by Size-BM, Size-INV, and Size-OP, covering the period from January 1998 to December 2017. The choice of this sample period is guided by Figure % \ref{figFMRejections}, 
4, 
where we observe substantial differences between the testing results during this timeframe. 
Table \ref{tabPars} reports the means and covariances for generating factor loadings $\bm{b}_i$ and factor returns $\bm{f}_t$. 
The mean vector ($\bm{\mu_f}$) and covariance matrix  ($\bm{\Sigma_f}$)  of the factor returns $\bm{f}_t$ are reported in the top panel. The mean vectors  ($\bm{\mu_B}$) and covariance matrices ($%
\bm{\Sigma_B}$) of  the factor loadings $\bm{b}_i$ for each of the different portfolio sorts are reported in the bottom panel. 

\begin{table}[H]
	\caption{Means and covariances for generating factor loadings $\bm{b}_i$ and factor returns $\bm{f}_t$}\label{tabPars}
	\centering
	\scalebox{1.}[1.]{
		\begin{tabular}{r | rrrrr }
			\hline
			\multicolumn{6}{c}{Parameters for factor returns} \\
			\hline
			\multicolumn{1}{c}{$\bm{\mu_f}$} 
			& \multicolumn{5}{c}{$\bm{\Sigma_f}$} 
			\\
			\hline
			$0.552$
			& $\phantom{-}19.942$ & $\phantom{-}3.472$ & $-1.742$ & $-6.725$ & $-3.185$
			\\ 
			$0.255$
			& $\phantom{-}3.472$ & $\phantom{-}10.183$ & $-0.613$ & $-4.774$ & $\phantom{-}0.198$ \\ 
			$0.145$
			& $-1.742$ & $-0.613$ & $\phantom{-}10.265$ & $\phantom{-}4.341$ & $\phantom{-}4.317$
			\\ 
			$0.315$
			& 	$-6.725$ & $-4.774$ & $\phantom{-}4.341$ & $\phantom{-}9.312$ & $\phantom{-}1.930$ 
			\\ 
			$0.245$
			& 	$-3.185$ & $\phantom{-}0.198$ & $\phantom{-}4.317$ & $\phantom{-}1.930$ & $\phantom{-}4.641$\\ 
			\hline
			\multicolumn{6}{c}{Parameters for factor loadings} \\
			\hline
			\multicolumn{1}{c}{$\bm{\mu_B$}} 
			& \multicolumn{5}{c}{$\bm{\Sigma_B}$} \\
			\hline
			\multicolumn{6}{c}{Size-BM} \\
			$1.029$ 
			%
			& $\phantom{-}0.015$ & $-0.016$ & $\phantom{-}0.014$ & $\phantom{-}0.018$ & \phantom{-}%
			$0.000$ \\
			$0.578$ 
			&	$-0.016$ & $\phantom{-}0.178$ & $-0.028$ & $-0.067$ & $-0.028$ \\
			$0.201$ 
			&	$\phantom{-}0.014$ & $-0.028$ & $\phantom{-}0.131$ & $\phantom{-}0.056$ & $\phantom{-}0.011$ \\
			$0.047$ 
			& $\phantom{-}0.018$ & $-0.067$ & $\phantom{-}0.056$ & $\phantom{-}0.097$ & $\phantom{-}$%
			$0.024$  
			\\
			$0.050$
			& $\phantom{-}0.000$ & $-0.028$ & $\phantom{-}0.011$ & $\phantom{-}0.024$ & $\phantom{-}0.047$ \\
			%
			%
			\multicolumn{6}{c}{Size-INV} \\
			$\phantom{-}1.015$ & $\phantom{-}0.016$ & $-0.012$  &  $-0.005$  &  $\phantom{-}0.002$  &  $\phantom{-}0.004$ \\
			$\phantom{-}0.564$ &  $-0.012$  &  $\phantom{-}0.182$  &  $\phantom{-}0.012$ &  $-0.044$  & $-0.031$ \\ 
			$\phantom{-}0.109$ &  $-0.005$  &  $\phantom{-}0.012$  &  $\phantom{-}0.021$  &  $\phantom{-}0.021$ &   $\phantom{-}0.004$ \\ 
			$\phantom{-}0.034$ & $\phantom{-}0.002$  & $-0.044$  &  $\phantom{-}0.021$  &  $\phantom{-}0.076$  &  $\phantom{-}0.018$ \\
			$\phantom{-}0.110$ & $\phantom{-}0.004$ &  $-0.031$  &  $\phantom{-}0.004$  &  $\phantom{-}0.018$  &  $\phantom{-}0.136$ \\
			\multicolumn{6}{c}{Size-OP} \\ 
			$\phantom{-}1.028$ & $\phantom{-}0.015$  &  $-0.010$  & $-0.009$ &  $-0.004$ &  $-0.002$ \\ 
			$\phantom{-}0.538$ &  $-0.010$ &   $\phantom{-}0.179$ &   $\phantom{-}0.012$  &  $\phantom{-}0.035$ &   $-0.005$ 
			\\ 
			$\phantom{-}0.159$ &  $-0.009$ &   $\phantom{-}0.012$  &  $\phantom{-}0.045$ &   $\phantom{-}0.037$ &  $-0.013$ \\ 
			$\phantom{-}0.089$ & $-0.004$  &  $\phantom{-}0.035$  &  $\phantom{-}0.037$ &   $\phantom{-}0.188$  &  $\phantom{-}0.006$ \\
			$\phantom{-}0.022$ &  $-0.002$ &  $-0.005$ &  $-0.013$  &  $\phantom{-}0.006$   & $\phantom{-}0.034$ \\
			%
			\hline
			\hline	
	\end{tabular}}
	%		\begin{minipage}{1.0\linewidth}
		%			\begin{tablenotes}
			%				\small
			%				\item {
				%					\medskip
				%					Note: The means and covariances to generate factor loadings $\bm{b}_i$ for the Size-BM portfolios and factor returns $\bm{f}_t$ within the \citet{fama2015five} five-factor model.  
				%						
				%				}
			%			\end{tablenotes}
		%		\end{minipage}
\end{table}

The idiosyncratic noises are generated from a multivariate normal distribution, i.e., $%
\mathcal{N}_d (0, \bm{\Sigma_u})$. To generate cross-sectional dependence in the idiosyncratic noise that resembles the one 
observed in the empirical application, we compute the covariance matrix $\bm{\Sigma_u}$ as the sample covariance matrix of the residuals obtained from the OLS estimation of the five-factor model. These sample covariance matrices have dimensions of $100\times100$ and are available upon request.\footnote{We thank the authors of \citet{shi2022relax} for generously sharing their calibration code.}  

We treat the 100 portfolios within each subgroup (Size-BM, Size-INV, and Size-OP) as a family and control the familywise error rate at the  5\% level. 
The simulation is repeated 2,000 times.
Table \ref{tabCrossSize} reports the familywise error rate, global power, and successful detection rates of the different controlling procedures, under the specified conditions.  
Additionally, the second and third columns show the minimum and maximum off-diagonal elements of the empirical correlation matrix of the test statistics across all replications, indicating strong cross-sectional dependence. 
Under this scenario, all procedures have reasonably good control of the familywise error rate. 
The inequality-based,  Gumbel and screening methods show FWERs below the nominal level of 5\%, with the Gumbel and screening  methods being the most conservatives. 
Note that, in the presence of strong cross-sectional dependence, the screening test is found to have an empirical rejection frequency of approximately $3\%$, which is higher than what is expected asymptotically (i.e., 0\%).  
Interestingly, the SCC testing procedure consistently achieves FWERs very close to the nominal level of 5\% in all three cases, and  outperforms all other procedures in terms of global power and successful detection rate. 

\begin{table}[!ht] 
	\centering
	\caption{Finite sample performance of the controlling procedures for the nonzero alphas test}
	\label{tabCrossSize}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{c | cc | cccc cccccc}
			\hline
			& \multicolumn{1}{c}{$\widehat{\rho} (\min)$}
			& \multicolumn{1}{c|}{$\widehat{\rho} (\max)$}
			&\multicolumn{1}{c}{Bonferroni}
			&\multicolumn{1}{c}{Holm} 
			&\multicolumn{1}{c}{Hommel}  
			&\multicolumn{1}{c}{Hochberg} 
			&\multicolumn{1}{c}{Gumbel} 
			&\multicolumn{1}{c}{Screening}
			&\multicolumn{1}{c}{SCC}	\\
			\hline
			&& \multicolumn{8}{c}{FWER}\\
			Size-BM 
			& --0.41  &   0.71
			& 4.00    &  4.00   &   4.00   &   4.00     & 3.70   &   
			3.00 & 
			5.10
			\\
			Size-INV 
			&  --0.44    &    0.60
			& 4.65   &   4.65    &  4.65   &   4.65     & 4.15      &
			3.40 &
			5.70
			\\ 
			Size-OP 
			&	--0.37      &   0.60
			& 4.30  &    4.30     & 4.30   &   4.30    &  3.80      &
			3.45 &
			5.20
			\\
			&&\multicolumn{8}{c}{Global power}\\
			Size-BM 
			&  --0.41      &      0.71
			& 54.15   &  54.20   &  54.25   &  54.20  &   51.55    & 
			49.15 &
			58.75
			\\
			Size-INV 
			& --0.43      &     0.60
			& 	72.10    & 72.10  &   72.10  &   72.10  &   70.10    
			& 67.80
			& 76.40
			\\
			Size-OP 
			& --0.36      &      0.59
			& 83.40   &  83.40  &   83.40  &   83.40 &     81.75   & 80.25 & 88.05
			\\
			&&\multicolumn{8}{c}{Successful detection rates}\\
			Size-BM  
			&  --0.41      &      0.71
			& 15.49 & 15.56 & 15.59 & 15.57 & 14.56 & 13.62 & 17.67
			\\
			Size-INV 
			& --0.43      &     0.60
			& 	27.80    & 27.92  &   27.99 & 27.92 & 26.45 & 24.73 & 30.88
			\\
			Size-OP 
			& --0.36      &      0.59
			& 41.45   &  41.63  &   41.65  &   41.63 &     39.62   & 37.88 & 45.72
			\\
			\hline
		\end{tabular}
	\end{adjustbox}	
	\parbox{1\textwidth}{\footnotesize%
		\vspace{.1cm} % If wanted space after the bottomrule
		{Note}: The data-generating process is % \eqref{eqCrossDecomp} 
		(5.1)
		with $a_{i}=0$ for all $i=1,...,100$ under the null hypothesis and $a_i$ specified in \eqref{eq:ai} under the alternative hypothesis. 
		The sample size is $T=240$ and the number of assets is $d=100$. 
		The nominal level $\alpha$ of the tests is 5\%. The simulation is repeated 2,000 times. }
\end{table}

\newpage

\bibliographystyle{chicago}
\bibliography{Reference}

\end{document}
