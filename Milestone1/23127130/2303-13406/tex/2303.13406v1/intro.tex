% !TEX root = ./CauchyCombination.tex

\newpage
\section{Introduction}

There are many needle-in-a-haystack problems in empirical finance. There are tests for skilled funds \citep{barras2010false,giglio2021thousands}, nonzero alpha stocks  \citep{fan2015power}, explanatory factors  \citep{harvey2016and}, applicable technical trading rules \citep{bajgrowicz2012technical,sullivan1999data}, jumps and drift bursts in high-frequency asset prices \citep{lee2007jumps, leemykland2012jumps,  bajgrowicz2015jumps,christensen2014fact,christensen2018drift},  to name but a few examples. 
These statistical tests are often required to be applied repeatedly due to either the presence of a large cross-section of units (e.g., funds, stocks, factors or trading rules) or to test a particular hypothesis continuously over time (e.g., every minute or every day). 
It is commonly known that the simultaneous testing of multiple hypotheses is subject to a ``false discovery problem"; as more and more tests are performed, an increasing number of them will be significant purely due to chance.\footnote{A well-known example of disregarding the multiple testing problem is so-called ``data snooping" or ``\textit{p}-hacking", which is a misuse of statistical testing: one  exhaustively searches for signals without compensating for the number of inferences being made 
	\citep[see e.g.,][and references therein]{giglio2021thousands}.} Many statistical procedures have been proposed to  counteract the inflation of false discoveries. Some procedures control the expected proportion of false discoveries or the so-called ``the false discovery rate" \citep[see e.g.,][]{benjamini1995,barras2010false,giglio2021thousands}. Other procedures are designed to control the probability of making at least one false discovery or the so-called ``familywise error rate"  \citep[see e.g.,][for reviews]{shaffer1995multiple,goeman2014multiple}. The procedures considered in this paper fall in the second category. 

An important factor in the performance of multiple testing corrections is the dependence between the test statistics. When the test statistics are independent and follow a standard normal distribution under the null hypothesis, as is the case for some jump tests, some well-known statistical solutions can be imported. 
Specifically, the multiple testing problem can be addressed with one of the following solutions: (1) choose a critical value corresponding to an extremely high quantile (which is similar in spirit to a Bonferroni correction) as in \cite{andersen2007no}; (2) rely on the standard Gumbel distribution to draw inferences \citep{lee2007jumps}; or (3) use a sequential procedure like the \citeauthor{holm1979simple}'s step-up procedure as in \citet{leemykland2012jumps}. 

In many applications in economics and finance the independence assumption is implausible.  Well-known test statistics are computed from stock returns driven by unknown common factors or are constructed from  overlapping rolling windows.
When the test statistics are dependent, the popular controlling procedures that are based on statistical inequalities (such as the Bonferroni correction and the subsequent improvements of \citeauthor{holm1979simple}, \citeyear{holm1979simple};  \citeauthor{hommel1988stagewise}, 
\citeyear{hommel1988stagewise} and   \citeauthor{hochberg1988sharper}, 
\citeyear{hochberg1988sharper}) do protect against false discoveries, but are known to be too conservative:  the familywise error rate is strictly smaller and often much smaller than the nominal level  $\alpha$. Resampling or simulation-based methods have also been used to
account for the observed correlation of the test statistics in setting a data-driven critical value  \citep[e.g.,][]{white2000,romano2005exact,romano2005stepwise} but they are not ideal either. Aside from being computationally intensive,  (1) they impose a strong parametric assumption on the dependence structure \citep[e.g., a Gaussian AR(1) process as in][]{christensen2018drift}, which could be misspecified, and, more importantly, 
(2) they attempt to reproduce the dependence structure under the null hypothesis with possibly contaminated data 
\citep[e.g., the attenuation bias reported in][]{christensen2018drift}, 
which on its turn contaminates the data-driven critical value. 

Meanwhile, \citet{liu2020cauchy} tackle dependence in the test statistics from another perspective by 
proposing a test which is agnostic about the dependence structure. 
Their global Cauchy combination (GCC) test  is drawn on a convenient theoretical property of Cauchy distributions; 
although a linear combination of correlated standard Cauchy variates does not follow a Cauchy distribution, the tails of the distribution are asymptotically equivalent to those of a standard Cauchy. In other words, the dependence between the test statistics does not have significant impact on the very heavy tail of a Cauchy. As such, they propose to transform each individual raw $p$-value (such that the transformed $p$-values follow a standard Cauchy distribution under the null hypothesis) and construct the new test statistic as a linear combination of  transformed $p$-values, with its corresponding critical value taken from a Cauchy distribution. Importantly, under the null hypothesis, the actual size of the GCC test  converges to the nominal size $\alpha$ (instead of being bounded above by $\alpha$ like the workhorse procedures), under arbitrary dependency structures. The GCC test is designed for making inferences about a global null hypothesis, however, 
it is not obvious how statements about individual hypotheses are to be made with this procedure. 

Set against this background, we unravel the GCC test to find out which individual hypotheses trigger the rejection of the global null, which can be used to identify, for example, skilled  funds,  nonzero alpha stocks, explanatory factors, or timestamp jumps and flash crashes. Specifically, we apply the GCC test to subsets of $p$-values, which delivers a sequence of Cauchy combination test statistics. The $p$-values of the test statistics are computed from a standard Cauchy distribution and violations are detected when the corresponding $p$-value is lower than a fixed cut-off $\alpha$. The new procedure is referred to as the sequential Cauchy combination (SCC) test and inherits all the nice theoretical properties of the GCC test; most importantly, it is also agnostic about the dependence structure. 
We prove that the SCC test achieves strong familywise error rate control when $\alpha\rightarrow 0$ with either a fixed or an infinite number of individual hypotheses. The familywise error rate of the SCC is closer to the theoretical upper bound compared to the benchmark procedures, which on its turn boosts the power and helps to better identify the individual  signals. 


To further illustrate the benefits of the SCC test, we revisit two showcase examples of multiple testing problems in finance involving non-trivial correlation structures in the test statistics, high dimensions, and sparse signals. In the first example (Example 1), we revisit the drift burst hypothesis of \cite{christensen2018drift} which seeks to detect explosive trends in the stock price. 
The drift burst test statistic relies on ultrahigh-frequency data, the test is applied many times per day (e.g., for each of the 390 minutes of a 6.5-hour trading day), the statistics are serially dependent because they are constructed from overlapping rolling  windows. The drift bursts are rare and the strength of this signal is not constant.  

In the second example (Example 2), we test for multiple nonzero alpha within the \citet{fama2015five} five-factor model framework, in which portfolio or stock returns are driven by a set of common factors (e.g., market, size, value). If the model fully explains asset returns, the estimated ``alphas" should be indistinguishable from zero. The correlation between the factors as well as the correlation between the idiosyncratic errors of the model (due to, for instance, unknown common factors, see e.g., \citeauthor{giglio2021thousands}, \citeyear{giglio2021thousands}), generate to a strong cross-sectional  dependence between the test statistics on the alphas which complicates the inference. The signals are often rare and weak in this literature \citep[see][and the references therein]{fan2015power}. 

To illustrate the finite sample properties of the SCC test, we conduct two types of simulation studies. We first simulate test statistics with various types of dependence and compare our SCC test with several competitors. We also generate data from an underlying process and compute a sequence of test statistics from the data, mimicking the situation in empirical applications. More specifically, we simulate log prices of financial assets from a continuous time drift bursting process in Example 1 and log returns from a factor model in Example 2. In all cases, test statistics are correlated with different strengths. The main conclusion of these simulations is that despite its simplicity, the SSC test has the best performance in minimizing conservativeness and maximizing successful detections.

The rest of the paper is organized as follows. Section 2 introduces the general notation, definitions and benchmark procedures in the multiple hypothesis testing literature. Section 3 introduces the global and sequential Cauchy combination tests. Section 4 illustrates the statistical performance of the sequential Cauchy combination test in a simulation experiment with different types of correlations. Sections 5 and 6 revisit the two financial applications responding to Examples 1 and 2, respectively. We conclude in Section 7. Appendices A and B provide further details on the benchmark procedures and the drift burst test, respectively.



