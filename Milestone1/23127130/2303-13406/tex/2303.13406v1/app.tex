% !TEX root = ./CauchyCombination.tex
\section{Controlling Procedures for Multiplicity}
\label{AppOrderedPVals}

We discuss methods based on the first-order Bonferroni inequality and the Simes' inequatility in Section \ref{ssecBonferroni} and Section \ref{ssecSimes}, respectively. The Gumbel method is introduced in Section \ref{AppMaxTest}.

\subsection{Methods based on the first-order Bonferroni inequality}
\label{ssecBonferroni}
The Bonferroni and \citeauthor{holm1979simple}'s methods are based on the Bonferroni inequality \eqref{eqIneqPval}. Let $\left\{c_{i}\right\} _{i=1}^{d}$ be a non-decreasing set of dynamic thresholds such that $0<c_{1}\leq \ldots\leq c_{d}<1$. The Bonferroni method suggests to reject the null hypothesis $H_{i}$ if the associated $p$-value $p_{i}$ is not greater than $c_i$. The cut-offs $c_i$ are chosen so that their sum is equal to $\alpha$. It is standard to set $c_i$ to be equal to $\alpha/d$. The larger the number of hypotheses $d$  in the family, the smaller the average power for testing the individual hypotheses. 

\citeauthor{holm1979simple}'s method is a sequential variant of the Bonferroni method that rejects at least as many as Bonferroni. 
% but often a bit more. 
% This method is applied in stages and uses the ordered $p$-values. 
Holm's method is a step-down method. It starts at the smallest $p$-value and compares the $p$-values against progressively less strict criteria. At any stage $i$, $H_{(i)} $ is rejected if and only if all the preceding hypotheses,  $H_{(j)}$ have been rejected with $j< i$ and $p_{(i)}$ $\leq c_i$, with the dynamic threshold equal to $c_i = \alpha / (d - i + 1)$. The procedure stops the first time a $p$-value exceeds the critical value. 

\subsection{Methods based on Simes' inequality}\label{ssecSimes}

Observing that the Simes' inequality \eqref{eqSimes} holds under some very general dependence structure of $p$-values, \citet{simes1986improved} proposes to reject the global null when $p_{(i)} \leq (i \alpha) / d$ for any $i = 1, \ldots, d$. 
 \citet{simes1986improved} also raised the problem of making statements on
 individual hypotheses but did not provide a formal procedure. Formal
 multiple testing procedures are provided by \citet{hommel1988stagewise} and  \citet{hochberg1988sharper}, leading to more powerful procedures than %
 \citet{holm1979simple}'s method.
 
 \citet{hochberg1988sharper}'s method is a step-up version of \citet{holm1979simple}'s method and is also a variant of the Bonferroni method.
 The procedure starts from the largest $p$-value, comparing $p_{(i)}$ against critical value $c_i =\alpha / (d- i + 1)$, and steps down. Once it breaches the threshold,  all remaining hypotheses are rejected. 
 
\citet{hommel1988stagewise}'s method is a slightly more complicated step-up method. 
It rejects the hypotheses $H_{(i)}$ for which $p_{(i)} \leq c_i = \alpha / j$. The rejection criterion $c_i$ is fixed, but it is not a single-stage method, because the index $j$ is the result of a sequential strategy. Let $j$ be the largest integer, with $i = 1, \ldots, n$,  for which $p_{(n-i+k)} >  k \alpha  / i$, for all $k = 1, \ldots, i$. If the maximum does not exist, we reject all hypotheses. The procedure uses the {closure principle}
% \footnote{The closure principle allows the rejection of any of the hypotheses, $H_i$, if all of the possible intersection hypotheses involving $H_i$ can be rejected by using an $\alpha$ test \citep[such as,][]{simes1986improved}.} 
\citep{marcus1976closed} to extend the global \citet{simes1986improved} test, and makes statements about individual hypotheses. For each ordered hypothesis $H_{\left( j\right) }$, the procedure applies a \citet{simes1986improved} global test to the subset of $p$-values $\left\{ p_{\left(k\right) }\right\} _{ k = j }^{d}$ with $p_{\left( j\right) }$ being the first elementary hypothesis in the set and $p_{\left( d\right) }$ being the last, instead of relying on only $p_{\left(j\right)}$ to draw conclusions on $H_{(j)}$.
%  it uses information from subsets of $p$-values.

 
\subsection{Gumbel Method}
\label{AppMaxTest}

Consider the maximum test statistic:  $X_m = \max_{i} \abs{X_{i}}, \, \text{with} \, \, i = 1, \ldots, d.$ When the $d$ test statistics $X_{i}$ are independent and follow the standard normal distribution, a normalized version of $X_m$ has a limiting Gumbel distribution: 
% hat is, as $d \rightarrow \infty$: 
\begin{eqnarray}
	\frac{X_m - C_d}{S_d} 	\sim	G,
	\end{eqnarray}
where $G$ has a cumulative distribution function of 
$\Prob (G \leq x) = \exp (- \exp (-x))$, with: 
\begin{eqnarray}
	C_d = 
	\sqrt{2 \log d} 
	-
	\dfrac{\log \pi 
		+ 
		\log(\log d)
	}
	{2\sqrt{2 \log d}},
	\, \,  \, \text{and} \, \,  \, 
	S_d = \dfrac{1}{\sqrt{2\log d}}.
\end{eqnarray}

We reject the null hypothesis $H_{i}$ if $\abs{X_{i}}$ breaches the threshold $G^{-1} (1 - \alpha) S_d + C_d$, in which $G^{-1} (1 - \alpha)$ is the $(1-\alpha)$ quantile of the standard Gumbel distribution.
%  and takes value $2.970$ when $\alpha=0.05$. d? 


\section{Drift and Variance Estimators}
\label{AppDBEstim}

Let $\Delta_i^n \widetilde{P} = \widetilde{P}_{t_i} - \widetilde{P}_{t_{i-1}}$ be the discretely sampled noise contaminated log return over $[t_{i-1}, t_i]$. To reduce the impact of market microstructure noise, \citet{christensen2018drift} use pre-averaged returns computed as: 
\begin{eqnarray}
	\label{eqPreavRets}
	\Delta_i^n \bar{P} 
	= \sum^{k_n - 1}_{j = 1} 
	g_j^n 
	\, 
	\Delta^n_{i + j} \widetilde{P},
\end{eqnarray}
where $k_n$ is the pre-averaging window, $g_j^n = g(j/k_n)$ is a weight function. 
For the construction of the drift burst statistic, \citet{christensen2018drift} specify the weight function as $g(x) = \min(x, 1-x)$ and set the pre-averaged window $k_n$ to $3$.

The noise-robust estimator for the drift $\mu_t$ is computed from the pre-averaged returns as:
\begin{equation}
	\label{eqHatMu}
	\begin{split}
		\hat{\bar{\mu}}_t^n 
		&= 
		\frac{1}{h_n} 
		\sum^{n-k_n+2}_{i = 1} 
		K \left( \frac{t_{i-1} - t}{h_n} \right) 
		\Delta^n_{i-1} \bar{P} ,
	\end{split}
\end{equation}
in which $t \in [0,T]$, $h_n$ is a bandwidth parameter, and $K(.)$ is a kernel function. The variance estimator $\hat{\bar{\sigma}}_t^{2,n} $ is a heteroscedasticity and autocorrelation consistent (HAC)-type statistic, accounting for dependence in the pre-averaged returns: 
\begin{equation}
	\label{eqHatSigma}
	\begin{split}
		\hat{\bar{\sigma}}_t^{2,n} 
		&= 
		\frac{1}{h'_n} 
		\Bigg\{
		%
		\sum^{n-k_n + 2}_{i = 1} 
		%
		\left[
		K 
		\left(
		\frac{t_{i-1} -t}{h'_n} 
		\right)
		\Delta^n_{i-1} \bar{P}
		\right]
		^2
		%
		%
		%%%%
		\\
		&\quad +  
		2 \sum^{L_n}_{L = 1}
		w 
		\left( 
		\frac{L}{L_n}
		\right)		
		\sum^{n-k_n-L+2}_{i = 1}
		K 
		\left(
		\frac{t_{i-1} -t}{h'_n} 
		\right)
		%
		K 
		\left(
		\frac{t_{i+L-1} -t}{h'_n} 
		\right)
		%
		\Delta^n_{i-1} \bar{P}
		\Delta^n_{i-1+L} \bar{P}		
		\Bigg\}
		, 
	\end{split}
\end{equation}
where $h'_n$ is a bandwidth parameter, $\omega(.)$ is a kernel functions, and $L_n$ is the maximum autocorrelation lag length. We set the lag length $L_n = Q^* + 2 (k_n - 1)$ and $Q^*$ is computed from the raw returns $(\Delta \widetilde{P})_{i = 0}^n$ as a measure of noise dependence based on automatic lag selection. 

\citet{christensen2018drift} suggest for $K(.)$ a left-side exponential kernel, $K(x) = \exp (-\abs{x})$ with $x \leq 0$ to avoid look-ahead bias. We use a ten-minute 
% five-minute 
bandwidth for the mean estimator 
($h_n$ = 600) 
%  ($h_n$ = 300) 
and a  50-minute 
% 25-minute 
bandwidth ($h'_n = 5h_n$) 
for the volatility estimator. % as in \citet{christensen2018drift}. 
The kernel function $w: \mathbb{R}_+ \rightarrow \mathbb{R}$ satisfies the properties $w(0)=0$ and $w(x) \rightarrow 0$ as $x \rightarrow \infty$. A Parzen kernel is selected for $w(.)$: 
\begin{eqnarray*}
	w(x)
	= \left\{
	\begin{array}{ll}
		1 - 6x^2 + 6 \abs{x}^3, 
		& \text{for} \, \, 0 \leq \abs{x} \leq 1/2, \\
		2(1-\abs{x}^3), 
		&  \text{for} \, \, 1/2 < \abs{x} \leq 1, \\
		0, 
		& \text{otherwise.} \\
	\end{array}
	\right.
\end{eqnarray*}



