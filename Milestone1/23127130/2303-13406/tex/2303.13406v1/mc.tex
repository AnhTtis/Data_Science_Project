% !TEX root = ./CauchyCombination.tex
\section{Simulations}
\label{secSims}

In this section, we conduct a horse race of the SCC test against popular controlling procedures under various forms of dependence. The benchmark procedures include four approaches based on statistical inequalities
\citep[the Bonferroni correction and the subsequent improvements by][]{holm1979simple,hommel1988stagewise,hochberg1988sharper} 
and the Gumbel approach. 


\subsection{Under the null hypothesis}
\label{ssecAccuracy}

We gauge the statistical performance of the controlling procedures under the null hypothesis. For each method $m$, we calculate an empirical familywise error rate over $S=10^4$ replications, denoted by $\widehat{FWER}_m$ as follows: 
\[
\widehat{FWER}_m=\frac{1}{S}\sum_{s=1}^{S} I_{s,m},
\]
where  the dummy 
$I_{s,m}$ takes value one if $\min_{i\in\left\{1,2,\cdots,d\right\}} p_{(m,i)}^{(s)}\leq\alpha$ and zero otherwise in which
$p_{(m,i)}^{(s)}$ denotes the $p$-value of the $i$th hypothesis for method $m$ in the $s$th replication. The $\widehat{FWER}$ of the SCC method is expected to be closer to the nominal level $\alpha$ than other procedures when the test statistics are strongly dependent. 

The test statistics under the null hypothesis $\bm{X}$ are generated from a $d$-variate normal distribution with zero mean and covariance matrix $\bm{\Sigma}$, i.e., $N_d(\bm{0}, \bm{\Sigma})$. We set the dimension $d$ to 100. The diagonal elements of the covariance matrix $\sigma_{ii}$ are equal to $1$, for $i=1,\ldots,d$. The off-diagonal elements $\sigma_{ij}$, with $i\neq j$, adhere to the following four specifications.
\begin{itemize}
	
	\item Model 1. Exponential decay: $\sigma_{ij} = 	\theta^{\abs{i-j}}$ with $\theta= 0.2, 0.4, 0.6,$ $0.8, 0.90, 0.95$.
	
	\item Model 2. 	Polynomial decay: $\sigma_{ij} = \frac{1}{0.7 + \abs{i - j}^\theta}$ with $\theta =	1.0, 1.5, 2.0, 2.5$. 
	
	\item Model 3. 	Mixture of exponentially decaying processes:  
\[
\sigma _{ij}=\left\{ 
\begin{array}{cc}
\theta _{1}^{\left\vert i-j\right\vert } & \text{for }i,j\leq \lfloor
d/2\rfloor  \\ 
\theta _{2}^{\left\vert i-j\right\vert } & \text{for } \lfloor d/2 \rfloor <i,j>d
\end{array}%
\right. 
\]
with $\theta_1=0.00, 0.20$ and $\theta_2=0.90,  0.80$ and $\lfloor . \rfloor$ signifies the largest integer of the argument.
		
\item Model 4.	Block-diagonal:  $\bm{\Sigma} = \text{diag}\{A_1,\ldots, A_{d/10}\}$, for which each diagonal block $A_k$ is a $10 \times 10$ equi-correlation matrix with its off-diagonals $\sigma_{ij} = \theta$ and $\theta = 0.1, 0.3, 0.5, 0.7, 0.9$. 
	
\end{itemize}


Models 1 and 2 have been used in \citet{liu2020cauchy}. 
Some examples of exponentially decaying correlation structures can also be found in time series and financial econometrics. 
The sequence of drift burst test statistics in Section \ref{ssecDB}, which is constructed from overlapping rolling windows, is well approximated by an autoregressive process, as shown by \citet{christensen2018drift}. Model 3 emulates a structural break in the correlation structure of test statistics.\footnote{Structural breaks can occur when alternating between backward- and forward-looking test statistics. For example, the drift burst test  typically uses a backward-looking kernel in practice and by treating each day separately it has a blind spot in the burn-in sample.  
	% (e.g., the first 49 minutes of each day). 
	To counter this problem, one could use a forward-looking kernel for those time intervals.}
The block-diagonal structure in Model 4 
% which 
is commonly used % in the simulations for 
% which is % expected when is 
% consistent with the Monte Carlo experiments of 
when testing high-dimensional factor pricing models  \citep[see e.g., the Monte Carlo experiments in][]{fan2015power} and emulates a cross-sectional dependence structure. 
% This used to refer to Section 6 in our paper, but we no longer use this simulation model. Rather we use the empirical covariance matrix of the residuals. 

Table \ref{tabSizeGlobalSequentialOtherMethods} shows the superiority of the SCC test when the test statistics are correlated under the null hypothesis. The last column shows that SCC is robust to various correlation structures, with its empirical familywise error rate being close to the theoretical upper bound $\alpha = 5\%$. The benchmark procedures are more conservative, with their empirical FWER substantially lower than 5\% when the dependence is strong (although there are some slight variations depending on the correlation pattern). This finding is in line with the discussion in Section \ref{ssecOrderdPvals}. The Gumbel method assumes independence of the test statistics and is also conservative when test statistics are correlated. To sum up, the SCC test is the sole controlling procedure with a FWER close to the nominal level (5\%) under all four different types of correlations in the test statistics.   


\begin{table}[htb!] 
	\centering
	\caption{Empirical FWERs (in\%) of the controling procedures}
	\label{tabSizeGlobalSequentialOtherMethods}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{c ccccccc}
			\hline
			\multicolumn{1}{c}{$\theta$}
			&\multicolumn{1}{c}{Bonferroni}
			&\multicolumn{1}{c}{Holm} 
			&\multicolumn{1}{c}{Hommel}  
			&\multicolumn{1}{c}{Hochberg} 
			&\multicolumn{1}{c}{Gumbel}
			&\multicolumn{1}{c}{SCC} 
			\\
			\hline
			\multicolumn{7}{c}{\textbf{Model {1}:} Exponential decay} \\	
			0.2  & 4.68 & 4.68 & 4.68 & 4.68 & $\underline{3.30}$ &  4.94 \\ 
			0.4 & 4.88 & 4.88 & 4.88 & 4.88 &  $\underline{3.44}$ &  5.36 \\ 
			0.6 & 4.96 & 4.96 & 4.96 & 4.96 & $\underline{3.54}$ &  5.78 \\ 
			0.8  & $\underline{3.98}$ & $\underline{3.98}$ & $\underline{4.00}$ & $\underline{3.98}$ & $\underline{2.84}$ & 5.82 \\ 
			0.9 & $\underline{2.40}$ & $\underline{2.40}$ & $\underline{2.42}$ & $\underline{2.40}$ & $\underline{1.70}$ & 5.48 \\ 
			0.95 & $\underline{1.76}$ & $\underline{1.76}$ & $\underline{1.78}$ & $\underline{1.76}$ & $\underline{1.18}$   & 5.38 \\ \hline
			\multicolumn{7}{c}{{\textbf{Model {2}:} Polynomial decay}} \\	
			1.0 & 4.70 & 4.70 & 4.70 & 4.70 & $\underline{3.38}$  &  5.96 \\ 
			1.5  & 4.62 & 4.62 & 4.62 & 4.62 & $\underline{3.62}$ & 5.48 \\ 
			2.0  & 4.44 & 4.44 & 4.44 & 4.44 & $\underline{3.18}$ & 5.38 \\ 
			2.5  & 4.46 & 4.46 & 4.46 & 4.46 & $\underline{3.16}$  & 5.14 \\ \hline
			\multicolumn{7}{c}{\textbf{Model {3}:} Mixture of exponential decay} \\	
			(0.0, 0.9) 	& $\underline{3.39}$     &    $\underline{3.39}$   &      $\underline{3.40}$  &       $\underline{3.39}$   &    $\underline{2.48}$     &          5.29 \\
			(0.2, 0.8) 		& 4.01    &     4.01   &      4.02     &    4.01      &  $\underline{2.95}$   &       5.32 \\\hline
			\multicolumn{7}{c}{\textbf{Model {4}:} Block-diagonal} \\	
			0.1  & 4.56 & 4.56 & 4.58 & 4.56 & $\underline{3.50}$ & 4.92 \\ 
			0.3  & 4.74 & 4.74 & 4.76 & 4.74  &   $\underline{3.74}$ & 5.32\\
			0.5 &  4.54 & 4.54 & 4.54 & 4.54 & $\underline{3.28}$ &  5.70 \\ 
			0.7  & $\underline{3.40}$ & $\underline{3.40}$ & $\underline{3.40}$ & $\underline{3.40}$ &  $\underline{2.46}$ & 5.62\\
			0.9 &  $\underline{1.88}$ & $\underline{1.88}$ & $\underline{1.92}$ & $\underline{1.88}$ &  $\underline{1.30}$ & 5.62\\ 
			\hline
			\hline
			
	\end{tabular}}
\end{adjustbox}	
\parbox{0.76\textwidth}{\footnotesize%
	\vspace{.1cm} % If wanted space after the bottomrule
	{Note}: We 	report the empirical FWERs (frequencies of falsely rejecting at least one hypothesis) of the controlling procedures. 
	The test statistics are generated with different correlation structures. 
	We fix the dimension $d$ to $100$. The nominal significance level $\alpha$ is 5\%. The number of replications is $10^4$. 
	Cases with lower than 4\% FWER are underlined. 
}
\end{table}



\subsection{Under the alternative hypothesis}


In this section, we gauge the statistical performance of the controlling procedures under the alternative hypothesis. The performance is measured by the global power (percentage of replications rejecting at least one hypothesis) and the successful detection rate (i.e., the percentage of overlapping hypotheses between the sets of false hypotheses $\mathcal{F}$ and discoveries $\mathcal{R}$). 
%The improved accuracy of FWER of the SCC procedure under the null (Table \ref{tabSizeGlobalSequentialOtherMethods}) is expected to be translated into a higher power when it is under the alternative.
 
The test statistic vector $\bm{X}$ is generated from a $d$-variate normal distribution with mean vector $\bm{\mu} = (\mu_i)$  and correlation matrix $\bm{\Sigma} = (\sigma_{ij})$, i.e., $N_d (\bm{\mu}, \bm{\Sigma})$. We consider the same correlation matrices $\bm{\Sigma}$ as in Section \ref{ssecAccuracy}.\footnote{\citet{liu2020cauchy} evaluate the power of the Cauchy combination test with a constant correlation matrix. We opt for the  correlation matrices in Section \ref{ssecAccuracy} because they are more realistic.} The percentage of signals (i.e., non-zero $\mu_i$'s in the vector $\bm{\mu}$) is set to be 5\% (5 out of 100 hypotheses are under the alternative). All signals have the same strength $\abs{\mu_i} = \mu_0$ which is chosen to be relatively weak, i.e.,  $\pm2.1737$.\footnote{The test power is shown to converge to unity as $d \rightarrow \infty$ when the signal is sparse and the strength of the signal equals $\sqrt{2r\log d}$ with $r > 0$ \citep[Theorem 3 in][]{liu2020cauchy}. We set the strength of the signal to be $\sqrt{3\log (d)}/s^{1/3}=2.1737$ with $s$ being the number of signals as in \cite{liu2020cauchy}.} The sign of the signal is set to be the same as that of the test statistic under the null so that the signal always amplifies the magnitude of the test statistic.

%In their simulations, the strength of the signal is chosen to be $\sqrt{3\log d}/s^{1/3}$, in which $s$ is the number of signals.


\begin{table}[h] 
	\centering
	\caption{Global power (in\%) in a correlated setting with sparse signals}
	\label{tabPowerGlobalSequentialOtherMethods}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{c cccccc}
			\hline
			\multicolumn{1}{c}{$\theta$}
			&\multicolumn{1}{c}{Bonferroni}
			&\multicolumn{1}{c}{Holm} 
			&\multicolumn{1}{c}{Hommel}  
			&\multicolumn{1}{c}{Hochberg} 
			&\multicolumn{1}{c}{Gumbel}
			&\multicolumn{1}{c}{SCC} \\
			\hline
			\multicolumn{7}{c}{{\textbf{Model 1:} Exponential decay}} \\
			0.2 & 66.50 & 66.50 & 66.62 & 66.50 & $\underline{59.32}$ & 76.20 \\ 
			0.4 & 66.48 & 66.48 & 66.60 & 66.48 & $\underline{59.86}$ & 76.84 \\ 
			0.6 & 65.70 & 65.70 & 65.74 & 65.70 & $\underline{58.58}$ &  75.82 \\ 
			0.8 & $\underline{63.86}$ & $\underline{63.86}$ & $\underline{63.90}$ & $\underline{63.86}$ & $\underline{56.90}$ & 72.74 \\ 
			0.9 & $\underline{60.62}$ & $\underline{60.62}$ & $\underline{60.78}$ & $\underline{60.64}$ & $\underline{53.90}$ & 68.42 \\ 
			0.95 & $\underline{57.24}$ & $\underline{57.24}$ & $\underline{57.30}$ & $\underline{57.24}$ & $\underline{50.58}$ & 63.94 \\ \hline
			\multicolumn{7}{c}{{\textbf{Model 2:} Polynomial decay}} \\
			1.0  & 66.04 & 66.04 & 66.14 & 66.04 & $\underline{58.76}$ &  74.82 \\ 
			1.5  & 65.88 & 65.88 & 66.00 & 65.88 & $\underline{58.46}$  &  75.10 \\ 
			2.0  & 65.88 & 65.88 & 66.00 & 65.88 & $\underline{58.84}$ & 75.20 \\ 
			2.5  & 65.22 & 65.22 & 65.28 & 65.22 & $\underline{58.22}$ & 74.66 \\ \hline
			\multicolumn{7}{c}{\textbf{Model {3}:} Mixture of exponential decay} \\
			(0.0, 0.9) 				& $\underline{63.42}$    &    $\underline{63.42}$    &    $\underline{63.52}$   &     $\underline{63.42}$    &  $\underline{56.69}$ &      72.00 
			\\
			(0.2, 0.8) 		 & 64.86   &     64.86   &     64.96    &    64.86    & $\underline{57.68}$ &     73.47  \\\hline
			\multicolumn{7}{c}{\textbf{Model {4}:} Block-diagonal} \\
			0.1 & 66.44 & 66.44 & 66.50 & 66.44 & $\underline{59.84}$ &   76.50 \\ 
			0.3  & 67.10 & 67.10 & 67.18 & 67.10 & $\underline{60.40}$  & 76.00 \\ 
			0.5  & 65.84 & 65.84 & 65.94 & 65.84 &$\underline{58.48}$ &  74.44 \\ 
			0.7 & $\underline{63.72}$ & $\underline{63.72}$ & $\underline{63.74}$ & $\underline{63.72}$ & $\underline{57.34}$ & 71.62 \\ 
			0.9 & $\underline{60.88}$ & $\underline{60.88}$ & $\underline{61.02}$ & $\underline{60.88}$ & $\underline{54.10}$ & 69.22 \\ 
			\hline
			\hline
	\end{tabular}}
\end{adjustbox}	
\parbox{0.76\textwidth}{\footnotesize%
	\vspace{.1cm} % If wanted space after the bottomrule
	{Note}: 
	We report the global powers (frequencies of rejecting at least one hypothesis). 
	The test statistics are generated with different correlation structures with sparse signals. 
	% under the alternative 
	% with correlated test statistics and sparse signals. 
	We fix the dimension $d$ to $100$ and the percentage of signals to  $5\%$ (or 5 out 100 hypotheses). All the signals have the same strength ($\pm2.1737$) with its sign depending on the sign of the test statistic under the null.	The nominal significance level is 5\%. The number of replications is $10^4$. 
	Cases with lower than 4\% FWERs  are underlined (Table \ref{tabSizeGlobalSequentialOtherMethods}). 
}
\end{table}


Table \ref{tabPowerGlobalSequentialOtherMethods}  shows the superior global power of the SCC test when the test statistics are correlated and the signals are sparse. The SCC test leads to a power enhancement of approximately $10$\% compared to the runner-up. The power of the SCC test varies between  $69$\% and $77$\% when $\alpha=5\%$. The power goes down when the correlation increases, but the relative superiority of the SCC test remains. Even though each  statistical inequality based method overcomes some disadvantage of its predecessor (see Appendix \ref{AppOrderedPVals} for a discussion), we do not observe a markedly different rejection frequency among the four approaches. As expected, the Gumbel approach, which assumes independent test statistics,  is the most conservative test. Table \ref{tabPowerLocalSequentialOtherMethods} offers a similar story with the average numbers of successful detections. The SCC procedure successfully detects about 1.2 out of 5 hypotheses under the alternative even with a sparse and weak signal.\footnote{The average number of false detections (not reported) is slightly higher for the SCC test, but are small in absolute value: it falsely rejects on average $0.1$ (out of 95) true hypotheses.} 
%The average number of successful detection for the  inequality-based procedures are around 0.95, whereas the value for the Gumbel procedure is about 0.81.


\begin{table}[!ht] % htb!
	\centering
	\caption{Successful detection rates (in\%)  in a correlated setting with sparse signals}
	\label{tabPowerLocalSequentialOtherMethods}
	\begin{adjustbox}{max width=\textwidth}
	\begin{tabular}{c cccccc}
	\hline
	\multicolumn{1}{c}{$\theta$}
	&\multicolumn{1}{c}{Bonferroni}
	&\multicolumn{1}{c}{Holm} 
	&\multicolumn{1}{c}{Hommel}  
	&\multicolumn{1}{c}{Hochberg} 
	&\multicolumn{1}{c}{Gumbel}
	&\multicolumn{1}{c}{SCC} 
	\\
	\hline
	\multicolumn{7}{c}{\textbf{Model {1}:} Exponential decay}  \\
	0.2 & 18.80 & 19.00 & 19.00 & 19.00 & $\underline{16.00}$ & 23.20 \\ 
	0.4 & 19.20 & 19.20 & 19.20 & 19.20 & $\underline{16.20}$ & 23.60 \\ 
	0.6 & 18.80 & 19.00 & 19.00 & 19.00 & $\underline{16.00}$ & 23.40 \\ 
	0.8 & $\underline{19.20}$ & $\underline{19.40}$ & $\underline{19.40}$ & $\underline{19.40}$ &  $\underline{16.20}$ & 24.00 \\ 
	0.9 &  $\underline{19.00}$ & $\underline{19.00}$ & $\underline{19.20}$ & $\underline{19.00}$ & $\underline{16.00}$  & 24.00 \\ 
	0.95 &  $\underline{19.20}$ & $\underline{19.20}$ & $\underline{19.20}$ & $\underline{19.20}$ & $\underline{16.20}$ & 24.40 \\ \hline
	\multicolumn{7}{c}{\textbf{Model {2}:} Polynomial decay}  \\
	1.0 & 19.00 & 19.20 & 19.20 & 19.20 & $\underline{16.00}$  & 23.60 \\ 
	1.5 & 19.00 & 19.20 & 19.20 & 19.20 &$\underline{16.00}$  & 23.40 \\ 
	2.0 & 19.00 & 19.20 & 19.20 & 19.20 &$\underline{16.00}$ & 23.60 \\ 
	2.5 & 18.80 & 18.80 & 18.80 & 18.80 & $\underline{16.00}$ & 23.40 \\ \hline
	\multicolumn{7}{c}{
	\textbf{Model {3}:} Mixture of exponential decay 
	} \\
	(0.0, 0.9)
	& $\underline{19.00}$   &      $\underline{19.00}$    &     $\underline{19.20}$   &      $\underline{19.00}$     &   $\underline{16.20}$  &         23.80
	\\
	(0.2, 0.8)
	 & 19.00    &     19.20     &    19.20      &   19.20      &  $\underline{16.00}$    &  23.40 \\\hline
	\multicolumn{7}{c}{
	\textbf{Model {4}:} Block-diagonal
	} \\
	0.1 & 19.40 & 19.40 & 19.40 & 19.40 & $\underline{16.40}$  & 23.60 \\ 
	0.3 & 19.40 & 19.60 & 19.60 & 19.60 & $\underline{16.60}$  & 23.60 \\ 
	0.5 & 19.40 & 19.40 & 19.40 & 19.40 &  $\underline{16.40}$ & 23.80 \\ 
	0.7 & $\underline{19.20}$ & $\underline{19.40}$ & $\underline{19.40}$ & $\underline{19.40}$ & $\underline{16.40}$  & 23.80 \\ 
	0.9 & $\underline{19.00}$ & $\underline{19.00}$ & $\underline{19.00}$ & $\underline{19.00}$ & $\underline{16.20}$  & 23.80 \\ 
	\hline
	\hline
	
	\end{tabular}}
	\end{adjustbox}	
	\parbox{0.76\textwidth}{\footnotesize%
	\vspace{.1cm} % If wanted space after the bottomrule
	{Note}: 
	We report the successful detection rates (overlapping between the sets of alternative hypotheses and discoveries). 
	% with correlated test statistics and and sparse signals. 
	The test statistics are generated with different correlation structures with sparse signals. 
	We fix the dimension $d$ to $100$. The percentage of signals is set to $5\%$ (or 5 out of 100 hypotheses). All the signals have the same strength ($\pm2.1737$) with its sign depending on the sign of the test statistic under the null. The nominal significance level is 5\%. The number of replications is $10^4$. Cases with lower than 4\% FWERs  are underlined (Table \ref{tabSizeGlobalSequentialOtherMethods}).}
\end{table}


%The 7th and 8th column report the empirical rejection frequencies of two methods which use the maximum of the test statistics. The Gumbel distribution (see Appendix \ref{AppMaxTest}) assumes independence between the test statistics and is too conservative when the test statistics are even slightly correlated (see also Figure \ref{figCOR18_fig8}).  The  AR(1) simulation approach (discussed in  Section \ref{ssecMaxTest}) relaxes the independence assumption within the Gumbel distribution by simulating critical values. In practice, the AR(1) simulation approach estimates an autoregressive parameter on each simulated sample path $X_i$, with $i = 1, \ldots, 100$ \eqref{eqXAR1} and retrieves the appropriate simulated critical value from a preprocessed table. The simulation approach is accurate under Model 1, because the parametric assumption on the dependence between the test statistics  \eqref{eqXAR1} matches the  \textit{dgp} in Model 1. Note also that the average estimated first-order autocorrelation $\hat{\rho}_1$ in the second column, which we plug into Equation (\ref{eqXAR1}) to get the simulated critical values, is close to the autoregressive parameter $\theta$ in the first column. Unfortunately, the simulated critical values require a parametric assumption upfront and can be misspecified under other models (e.g., Model 3). This highlights one of the drawbacks of the simulation approach: in practice, we do not know upfront which \textit{dgp} generates the statistics under the null hypothesis. Another problem is that when the \textit{dgp} emulates cross-correlation between the test statistics (Model 4) it is not appropriate to use a time series \textit{dgp} to simulate critical values. 

%With a biased estimate for the autoregressive coefficient, the AR(1) simulation approach  retrieves an incorrect critical value, especially when we cross the threshold of 0.7 when the critical value is more sensitive to small changes in autocorrelation. In the of case Model 1, an underestimation of the true autocorrelation under the null leads to a higher critical value and thus a loss in power as we saw in Figure \ref{figCOR18_fig8}. Indeed, the infeasible version in the 9th column has a higher power than the feasible version, but  in most cases the SCC test has a higher power than both the feasible and infeasible version.  Note that with empirical data, we cannot run such a procedure in which we estimate the autocorrelation on a path without signals to retrieve the appropriate critical value --  hence ``infeasible" -- because with empirical observations nothing prevents the inclusion of statistics under the alternative.  We cannot ``rerun" economic processes without signals. 

%Figure \ref{figACF} shows the potential bias when we approximate the dependence  between the test statistics using a parametric Gaussian AR(1) process. Each panel consists of an autocorrelogram (\textit{acf}) a the sequence of test statistics generated from the first three models. We show the \textit{acf}s before (gray line) and after adding signals (red dash). The gray and red vertical lines show us that in each case, the large-valued signals break the autocorrelation in the sequence of test statistics:    the gray lines (under the null) are under the red dashed lines (under the alternative). The gray and red dots show us that a parametric  Gaussian AR(1) process would not  capture the autocorrelation structure under the null. For example, in Panel (a) the estimated autocorrelation under the null (gray dots) almost fully captures the autocorrelation  under the null (gray lines), but is infeasible in practice. It is only feasible to estimate the autocorrelation on the observed sample path which potentially has  signals (red dots), which is quite far from the autocorrelation structure under the null (gray lines). The other panels show that the assumption of an AR(1) process behaves even worse when the test statistics are drawn from another \textit{dgp}. It, for example, misses significant autocorrelations in Panel  (c).

%\begin{figure}[htb]
%	\caption{Making the wrong assumption on the (auto)correlation structure}
%	\label{figACF}\centering
%	
%	\subfloat[AR(1) with $\theta = 0.95$ ]{{\includegraphics[width=.38\textwidth,angle = -90]{7_acf_both_d_100_rho_0.95_signal_5_3.eps} }}
%	%
%	\subfloat[Polynomial decay with $\theta = 1.5$ ]{{\includegraphics[width=.38\textwidth,angle = -90]{7_acf_both_d_100_rho_1.5_signal_5_3.eps} }}
%	
%	\par
%	
%	\subfloat[Mixture with $(\theta_1, \theta_2) = (0.0, 0.9)$ ]{{\includegraphics[width=.38\textwidth,angle = -90]{7_acf_both_d_100_rho_0_0.9_signal_5_3.eps} }}
%	
%	\begin{minipage}{1.0\linewidth}
%		\begin{tablenotes}
%			\small
%			\item {
%				\medskip
%				Note: We plot the autocorrelograms or autocorrelation functions (\textit{acf}) for a sequence of test statistics   				for three different types of dependence. 				We show the \textit{acf}s before (under the null, see Table \ref{tabSizeGlobalSequentialOtherMethods}) and after adding signals (under the alternative, see Tables \ref{tabPowerGlobalSequentialOtherMethods} and \ref{tabPowerLocalSequentialOtherMethods}). 					We also show the 					implied 					autocorrelations when would impose an AR(1) process. 				We do not show the \textit{acf} for the block-diagonal, because there is no particular ordering in the those test statistics. 
%			}
%		\end{tablenotes}
%	\end{minipage}
%\end{figure}



%The 7th, 8th and 9th column report the empirical rejection frequencies of three methods which use the maximum of the test statistics. As expected, using a Gumbel distribution has the lowest global power due to its stringent independence assumption. Compared to the previous section, we now split the column relating to the AR(1) simulation approach and consider a feasible and an infeasible way of implementing the AR(1) simulation approach. The feasible version estimates the autoregressive coefficient $\hat{\theta}$ in the Gaussian AR(1) model \eqref{eqXAR1} on the observed sequence of test statistics $\bm{X}$ which is contaminated with signals. The infeasible version estimates the autoregressive coefficient $\hat{\theta}$ on the sequence of test statistics before adding signals $\bm{X}_0$ -- under the alternative we generate test statistics as $\bm{X} = \bm{X}_0 + \bm{\mu}$. 

%The reason why we compare the feasible and the infeasible version is because the signals break the persistence structure. The 2nd column  reports the average estimated first-order autocorrelations $\hat{\rho}_1$ on the observed sequences of test statistics. When we compare this to the 2nd column in Table \ref{tabSizeGlobalSequentialOtherMethods}, which reports the average first-order autocorrelations on a sequence of test statistics before adding signals $\bm{X}_0$, we observe that adding signals biases the first-order autocorrelation estimate. For example, when the test statistics are drawn from a Gaussian AR(1) model, the average autocorrelations are far from the values in the 1st column, especially when we compare them to the values in  Table \ref{tabSizeGlobalSequentialOtherMethods}. 