% !TEX root = ./CauchyCombination.tex
\section{Multiple Hypothesis Testing} \label{secPrelims}

This section introduces the notation on multiple hypothesis testing and the benchmark procedures for addressing the multiple testing problem. 

\subsection{Setting}
Let $H_{i}$ denote the $i^{\text{th}}$ null hypothesis of interest, with $i=1,...,d$,  
and $d$ being the total number of individual hypotheses. To test the $d$ hypotheses, we can use the associated vector of test statistics $\bm{X}=(X_{1},X_{2},\ldots,X_{d})^{^{\prime }}$, one for each hypothesis being tested, or the corresponding raw $p$-values $p_{1},\ldots ,p_{d}$. The test statistics can be independent or % corrected.
correlated. 

%In some cases, like in Section \ref{secApplDriftBurst}, the test statistics are constructed from rolling windows and are extremely serially correlated. 

The first task is to test the global null hypothesis. Let $\mathcal{H}_{0}$ be the collection of null hypotheses of interest. 
The strategy of a classical global test is to abandon the multiplicity issue altogether and replace multiple tests with the global null hypothesis that all elementary hypotheses are true.  The alternative is that at least one elementary hypothesis is false. For example, in high-frequency financial econometrics,  we often need to monitor the presence of certain events (e.g., jumps or drift bursts) within a fixed time period (e.g., within a day). The global null is that there is no occurrence of such an event at all (e.g., 
% in Example 2, none of the stocks has a significant alpha
in Example 1, there is no drift burst within the day or in Example 2, none of the stocks has a significant alpha).
The goal is to get $\alpha$-level control under this global null, i.e., $P_{\mathcal{H}_0} [\text{reject}\, \mathcal{H}_0] \leq \alpha$. The test is conservative when $P_{\mathcal{H}_0} [\text{reject}\, \mathcal{H}_0]$ is strictly less than the theoretical upper bound $\alpha$ and ideal when it is equal to  $\alpha$. 

When, by any  test, the global null $\mathcal{H}_0$ is rejected, the second task is to identify which of the elementary hypotheses $H_{i}$ should be rejected. The set of true hypotheses $\mathcal{T}$, the set of false hypotheses $\mathcal{F}$ and the set of rejected hypotheses $\mathcal{R}$ are defined as: 
\begin{align}
	\begin{split}  \label{eqLocalHypothesis}
		\mathcal{T} &= \{ H_{i}\in \mathcal{H}_0: H_{i} \, \text{is true}\}, \\
		\mathcal{F} &= \{ H_{i}\in \mathcal{H}_0: H_{i} \, \text{is false}\}, \text{and} \\
		\mathcal{R} &= \{H_{i}\in \mathcal{H}_0: H_{i} \, \text{is rejected}\}.
	\end{split}%
\end{align}
The set of true and false hypotheses are unknown. We choose a set of hypotheses to reject. 
on the basis of our data. 
The set of discoveries $\mathcal{R}$ 
should coincide with the set of false hypotheses $\mathcal{F}$ as much as possible.

The goal of various multiple testing corrections is to control the familywise error rate (FWER), defined as the probability of at least one false
rejection in the family, $P[\mathcal{T} \cap \mathcal{R} \neq \varnothing]$,
while retaining the reasonable power in detecting false hypotheses. We want procedures for which the FWER is less than or equal to the upper bound $\alpha$ and ideally as close as possible to the upper bound. We focus on strong control of the FWER, meaning that some of the hypotheses we are testing can be false ($\mathcal{F} \neq \varnothing$), as opposed to the weak FWER control where all hypotheses of interest are true, i.e., $\mathcal{H}_0=\mathcal{T}$.

The probability of falsely rejecting a single hypothesis that is true (i.e., false positive or Type I error) is usually controlled at a nominal $\alpha$-level. However, when the number of tested hypotheses is large, the problem of multiplicity arises: the probability of having at least one false positive conclusion rises well above $\alpha$ if the Type I error of each individual test is controlled at the $\alpha$-level. Numerous controlling procedures have been proposed to deal with this problem. 
We review two classes of controlling procedures: one based on statistical inequalities (Section \ref{ssecOrderdPvals}) and one based on the maximum of the test statistics (Section \ref{ssecMaxTest}).


\subsection{Procedures based on statistical inequalities}
\label{ssecOrderdPvals}

Let us denote by $0 < p_{(1)}\leq p_{(2)}\leq \ldots\leq p_{(d)} < 1$ the set of $d$  ordered (in ascending order) raw $p$-values and $H_{(1)},H_{(2)},\ldots,H_{(d)}$ their corresponding null hypotheses. A single-stage method uses the same rejection 
criterion for all individual hypotheses, like the conservative Bonferroni threshold, while a multi-stage method examines the ordered $p$-values sequentially and adjusts the rejection criterion for each of the individual tests  \citep[e.g.,][]{holm1979simple,hochberg1988sharper,hommel1988stagewise}. 

The Bonferroni method rejects the elementary null hypothesis $H_{(i)}$ if 
$p_{(i)}\leq\alpha/d$ 
for $i=1,\ldots,d$. \citet{holm1979simple} and \citet{hochberg1988sharper} use the same critical values $ \alpha / (d - i + 1)$ depending on the rank of the $p$-value, but reject differently depending on whether they ``step up" or ``step down". The terminologies (``step up" or ``step down") were originally formulated in terms of test statistics which can be confusing when discussing $p$-values.  \citet{holm1979simple} proposes a step-down method that ``steps up" from the smallest $p$-value to the largest one. It is a pessimistic approach: it scans forward and stops as soon as a $p$-value fails to clear its threshold. \citet{hochberg1988sharper} suggests a step-up method that ``steps down" from the largest $p$-value to the	smallest one. It is an optimistic approach: it scans backward and stops as soon as a $p$-value succeeds in clearing its threshold. By construction, Hochberg's procedure will reject as many hypotheses as Holm's procedure. 

\cite{hommel1988stagewise} proposes a more complicated procedure which applies  \citet{simes1986improved}' global test to the $p$-value subset $\left\{ p_{\left(k\right) }\right\} _{ k = i }^{d}$, instead of relying  only on $p_{\left(i\right)}$ to draw inference on $H_{(i)}$ and thus borrows power across hypotheses. 
Hommel's procedure is shown to have higher power than Hochberg's method \citep{hommel1989}. We refer to Appendix \ref{AppOrderedPVals} for more details on the practical implementation of these procedures.


Bonferroni and \citet{holm1979simple}'s method are based on the first-order Bonferroni inequality, which states that given any set of events, the probability of their union is smaller than or equal to the sum of their probabilities. 
Under the null hypothesis, 
the probability that there is at least one hypothesis $H_{(i)}$  for which its raw $p$-value $p_{(i)} \leq \alpha / d$ 
% is not greater than $\alpha$ 
is bounded by $\alpha$: 
\begin{align}  \label{eqIneqPval}
	\Pr\left( \min_{i} p_{(i)} \leq \frac{\alpha}{d} \right) 
	= \Pr\left(\bigcup_{i =
		1}^{d} \left\{ p_{(i)} \leq \frac{\alpha}{d} \right\} \right) 
	&\leq
	\sum^{d}_{i = 1} \Pr \left( p_{(i)} \leq \frac{\alpha}{d}\right) \leq d
	\frac{\alpha}{d} \leq \alpha.
\end{align}
The Bonferroni \eqref{eqIneqPval} inequality 
makes no specific assumption on the dependence between the $p$-values, but protects against the so-called ``worst-case", in which all events are independent and the rejection regions are disjoint (the right half of Equation \eqref{eqIneqPval}) . 
The inequality becomes an equality when all test statistics are independent, and a strict inequality when the hypotheses are dependent. 
In other words, the Bonferroni correction is  conservative when the $p$-values are correlated. 

The methods of \cite{hochberg1988sharper} and \cite{hommel1988stagewise} are based on  \cite{simes1986improved}'s inequality. If a set of hypotheses $H_{(1)}, ...,H_{(d)}$ are all true, the probability of the joint event is: 
\begin{equation}  \label{eqSimes}
	\Pr\left( p_{\left( i\right) }> \frac{i\alpha}{d}, \text{ for all } i=1,\ldots
	,d\right) \geq 1-\alpha.
\end{equation}
\citet{simes1986improved}' inequality was developed for independent uniform $p$-values, and it is applicable for a large family of multivariate distributions. The simulations of \citet{simes1986improved} do show, however, that the test is very conservative for highly correlated multivariate normal statistics, but less so than the classical Bonferroni correction. 



\subsection{Procedures based on the maximum of test statistics}
\label{ssecMaxTest}

Another class of controlling procedures uses the maximum in a group of test statistics: $X_m = \max_{i} \abs{X_{i}}$, with $i = 1, \ldots, d$, to set a stringent critical value. The same critical value can be used for each elementary hypothesis and will control the familywise error rate. In particular, when the individual test statistics are independent and follow the standard normal distribution under the null, the maximum of the test statistics follows a Gumbel distribution when $d$ is large. Quantiles of the Gumbel distribution were used as critical values of the individual tests as a multiple testing correction when, for example, conducting jump tests in high-frequency asset returns \citep[][]{lee2007jumps}. Unfortunately, if the sequence of test statistics exhibits strong correlation, the number of tests severely overstates the effective number of independent copies in a given sample, which makes the Gumbel critical values too conservative \citep[see e.g.,][]{christensen2018drift}. We refer to Appendix \ref{AppOrderedPVals} for more details on the Gumbel distribution. 

Resampling-based methods account for the dependence structure that is specific to the considered dataset, leading to less conservative testing outcomes than the Gumbel-based methods and the inequality-based procedures. Depending on the empirical problem of interest, the resampling can be carried out by bootstrap, permutation, simulation, or randomization \citep[see e.g.,][for detailed discussions on resampling methods and testing procedures]{white2000,romano2005exact,romano2005stepwise,lehmann2005testing}. We refer to Section \ref{secApplDriftBurst} for an example of the resampling-based approach for the drift burst test. 


\section{Cauchy Combination Tests}
\label{secSeqCauchy}

In this section, we first review the global Cauchy combination (GCC) test of \citet{liu2020cauchy} and present our sequential Cauchy combination (SCC) test. While the global test tests the global null hypothesis $\mathcal{H}_0 = \bigcap_{i=1}^{d} H_{i}$ against the alternative hypothesis that at least one of the elementary null hypotheses is false, the sequential test aims at identifying the violations of the elementary null hypotheses while controlling the global error rate. 

\subsection{Global Cauchy combination test}
\label{sec:CC}

The GCC test statistic is constructed from the raw $p$-values of the test statistics $X_i$, which follow a uniform distribution between $0$ and $1$ under the  null hypothesis. The idea of this test is first to transform the uniformly distributed $p$-values into standard Cauchy variates using the formula $\tan \{(0.5-p_{i})\pi \}$ and then construct a new test statistic as the weighted sum of these transformed $p$-values. The new test statistic is denoted by $\tilde{T}$ and defined as: 
\begin{equation}
	\label{eqCauchyStatistic}
	{\normalsize \tilde{T}=\sum_{i=1}^{d}w_{i}\tan \{(0.5-p_{i})\pi \},} 
\end{equation}
in which the $w_{i}$'s are non-negative weights summing to 1. Throughout the paper, the weights $w_{i}$ are set to $1/d$ for $i=1,\ldots,d$ as in \citet{liu2020cauchy}. 

When the raw and hence the transformed $p$-values are independent (resp. perfectly correlated), % under the null hypothesis, 
the new test statistic $\tilde{T}$ is a linear combination of independent (resp. perfectly correlated) Cauchy variates and therefore follows a standard Cauchy distribution because the family of Cauchy densities is closed under convolutions. Although the correlation structure can affect the null distribution of $\tilde{T}$ in the case of general dependence, \citet{liu2020cauchy} show that the impact on the tail is very limited because of the heaviness of the Cauchy tail. Specifically, they prove that: 
\begin{equation}
	\lim_{h\rightarrow \infty }\frac{\Pr\left( \tilde{T}>h\right) }{\Pr\left(
		C>h\right) }=1,  \label{eq:tail}
\end{equation}
in which $C$ is a standard Cauchy random variable, under the null hypothesis $\mathcal{H}_0$ and Assumption \ref{ass1} which requires the test statistics to follow a bivariate zero mean normal distribution.
\begin{assumption}
	\label{ass1} (1) The original test statistics $(X_{i},X_{j})$, for any $1\leq
	i<j\leq d$, follow a bivariate normal distribution; (2) $E\left( \bm{X}%
	\right) =0$, with $\bm{X}=(X_{1},X_{2},\ldots,X_{d})^{^{\prime }}$. 
\end{assumption}
The bivariate normal requirement of Assumption \ref{ass1} is a condition weaker than joint normality, making the procedure applicable for high-dimensional settings. When the dimension $d$ increases at a certain rate with the sample size, the test statistics $\bm{X}$ may not jointly converge to a multivariate normal distribution due to its slower rate of convergence \citep[see][and references therein]{liu2020cauchy} and thus a joint normality assumption is not realistic for those settings. In contrast, the bivariate normality assumption is much weaker and more realistic. There are, of course, applications for which the test statistics are not normally distributed. Through simulations, \citet[][]{liu2020cauchy} show the Cauchy approximation is still accurate when the normality assumption is violated and follows a multivariate Student-$t$ distribution (with 4 degrees of freedom) instead. 
We refer to Section \ref{secApplFan} for a showcase example in finance with test statistics being Student-$t$ distributed. 

The result in  \eqref{eq:tail} suggests that, under the null hypothesis $\mathcal{H}_0$, the tail of the Cauchy combination test statistic is approximately Cauchy under arbitrary dependence structures, so that a $p$-value of the Cauchy combination test, denoted 
$\widetilde{p}$, can  be calculated from the standard Cauchy distribution. Suppose that we observe $\tilde{T}=t_{0}$, then: 
\begin{equation}
	\label{eqCauchyPval}
	\widetilde{p}=\frac{1}{2}-\frac{\arctan t_{0}}{\pi }. 
\end{equation}

Using the GCC $p$-values, the tail result in \eqref{eq:tail} can be equivalently stated as the actual size converging to the nominal size $\alpha$ as the significance level tends to zero:  % \textit{i.e.}, 
\begin{equation}
	\lim_{\alpha\rightarrow 0 }\frac{\Pr\left( \widetilde{p} \leq \alpha\right) }{\alpha}=1, \label{eq:wFWER}
\end{equation} 
The approximation should be particularly accurate for small $\alpha$'s, which are of particular interest in large-scale problems as in Examples 1 and 2. The simulations in \citeauthor{liu2020cauchy} show that when the significance level is moderately small ($\alpha = 10^{-1}, 10^{-2},10^{-3},10^{-4},10^{-5}$), the $p$-value calculation is accurate:  the ratio of the empirical size to the significance level is close to 1 for different types of correlations. 
Put differently, the GCC test achieves the weak familywise error rate control as the empirical size is very close to the nominal size $\alpha$ regardless of the correlation structure. 


Figure \ref{figCauchyPvalsAR} illustrates the fact that while the dependence between the individual test statistics $X_i$ can affect the null distribution of the GCC test statistic, the impact of the dependence is marginal on the tail. We simulate a vector of $d$ test statistics  $\bm{X}$ from a $d$-variate normal distribution with correlation matrix $\bm{\Sigma}$, \textit{i.e.}, $N_d(\bm{0}, \bm{\Sigma})$ with $\bm{\Sigma} = (\sigma_{ij})$ and $d = 300$. The diagonal elements $\sigma_{ii}=1$ for all $i=1,\ldots,d$ and the off-diagonal elements $\sigma_{ij} = \theta^{\abs{i-j}}$ for $i \neq j$, with $\theta = 0.2, 0.4, 0.6,$ $0.8, 0.90, 0.95$. The simulation is repeated $10^7$ times. For each draw, we calculate the GCC test statistic \eqref{eqCauchyStatistic} and its corresponding $p$-value \eqref{eqCauchyPval}. The histogram of the $10^7$ GCC $p$-values is displayed in Figure \ref{figCauchyPvalsAR}. For a low level of autocorrelation (i.e., $\theta=0.2$), the distribution of the $p$-values is close to a uniform distribution. When the level of autocorrelation is higher, there is a pothole in the middle and a bump at the end of the histogram, but whatever the strength of the autoregressive parameter, the percentage of the GCC $p$-values in the first bin is always around $5$\% as is ensured by the limit result in \eqref{eq:wFWER}. 


\begin{figure}[p]
	\caption{The impact of dependence on the tail of the GCC test statistic}
	\label{figCauchyPvalsAR}
	\centering
	
	\par
	
	\subfloat[${\theta} = 0.2$ ]{{\includegraphics[width=.40\textwidth,angle =
			-90,scale=0.70]{Sample_pval_dim_300_rho_0.2_alpha0.05.eps} }} 
	\subfloat[$\theta = 0.4$ ]{{\includegraphics[width=.40\textwidth,angle =
			-90,scale=0.70]{Sample_pval_dim_300_rho_0.4_alpha0.05.eps} }} 
	
	\vspace{0.4cm}
	
	\subfloat[$\theta = 0.6$ ]{{\includegraphics[width=.40\textwidth,angle =
			-90,scale=0.70]{Sample_pval_dim_300_rho_0.6_alpha0.05.eps} }} 
	\subfloat[$\theta = 0.8$ ]{{\includegraphics[width=.40\textwidth,angle =
			-90,scale=0.70]{Sample_pval_dim_300_rho_0.8_alpha0.05.eps} }} 
	
	\vspace{0.4cm}	
	
	\subfloat[$\theta = 0.90$]{{\includegraphics[width=.40\textwidth,angle =
			-90,scale=0.70]{Sample_pval_dim_300_rho_0.9_alpha0.05.eps} }} 
	% 
	\subfloat[$\theta = 0.95$]{{\includegraphics[width=.40\textwidth,angle =
			-90,scale=0.70]{Sample_pval_dim_300_rho_0.95_alpha0.05.eps} }} 
	
	\par
	\begin{minipage}{1.0\linewidth}
		\begin{tablenotes}
			\small
			\item {
				\medskip
				Note: We plot histograms of GCC $p$-values \eqref{eqCauchyPval} for various correlation strengths. The individual test statistics are drawn from a $d$-variate normal distribution $N_d(\bm{0}, \bm{\Sigma})$ with $\bm{\Sigma}= (\sigma_{ij})$ and $d=300$. The diagonal elements of the covariance matrix $\sigma_{ii}=1$ for all $i=1,\ldots,d$ and the off-diagonal elements  $\sigma_{ij} = \theta^{\vert i-j \vert}$ for $i\neq j$, with $\theta = 0.2, 0.4, 0.6,$ $0.8, 0.90, 0.95$. We compute the GCC $p$-value from the test statistic sequence. The simulation is repeated $10^7$ times. The simulated GCC $p$-values are sorted into bins with the bin edges being a sequence of edges from 0 to 1 with a width of  0.05. 				Each bin includes the right edge (right-closed) but does not include the left edge (left-open). We highlight the first bin in black and we
				also add a text note with the probability of $p$-values being in the first bin. }
		\end{tablenotes}
	\end{minipage}
\end{figure}

Interestingly,  \citet{liu2020cauchy} show that the tail property \eqref{eq:tail}  also holds when the number of hypotheses $d$ diverges to infinity at a rate of $o\left(h^{\eta }\right) \ $with $0<\eta <1/2$ and the following additional assumption is satisfied.
\begin{assumption}
	\label{ass2} Let $\mathbf{\Sigma }=corr\left( \bm{X}\right) $. 
	(1) The	largest eigenvalue of the correlation matrix $\lambda _{\max}\left( \mathbf{%
		\Sigma }\right) \leq C_0$ for some constant $C_0>0$; 
	(2) $\max_{1\leq i<j\leq
		d}\left\{ \sigma _{i,j}^{2}\right\} \leq \sigma _{\max }^{2}<1$ for some
	constant $0<\sigma _{\max }^{2}<1$, where $\sigma _{i,j}$ is the $\left(
	i,j\right) $ element of $\mathbf{\Sigma }$.
\end{assumption}
The additional assumptions on the correlation matrix are mild and standard in high dimensional settings and are general enough to incorporate a large class of tests. 


\subsection{Sequential Cauchy Combination Test}

The main contribution of this paper is the sequential Cauchy combination (SCC) test, which unravels the GCC test to make statements on the elementary hypotheses. The raw $p$-values are sorted in ascending order so that  $p_{(1)}\leq p_{(2)}\leq \ldots \leq p_{(d)}$, which is standard for step-down and step-up sequential procedures (see Section \ref{ssecOrderdPvals}). For the inference on hypothesis $H_{\left( i\right) }$ we compute a Cauchy combination test statistic ${\normalsize \tilde{T}}_{\left( i\right) }$ from a subset of $p$-values, running from $p_{(i)}$ to $p_{(d)}$ as:  
\begin{equation}
{\normalsize \ \tilde{T}%
		_{\left( i\right) }=\sum_{j=i}^{d}w_{j}\tan \{(0.5-p_{(j)})\pi \}
	}.
	\label{eq:CC_mt}
\end{equation}
The corresponding $p$-value is: $$ \widetilde{p}_{(i)}=\frac{1}{2}-\frac{\arctan \tilde{T}_{\left(i\right) }}{\pi }.$$ We reject the null hypothesis $H_{(i)}$ if  $\widetilde{p}_{(i)}\leq\alpha$. Like the step-up procedure of  \citet{hommel1988stagewise}, the SCC test also borrows power across hypotheses: the test statistic $\tilde{T}_{(i)}$ is computed from the raw $p$-values associated with $\mathcal{H}_0^{(i)}=\bigcap_{j=i}^{d} H_{(j)}$.


\subsubsection*{Theoretical Properties}
The SCC testing procedure can be viewed as a sequential rejection procedure. Let $\mathcal{R}^{(s)}$ be the collection of rejected hypothesis after step $s$, with $s=\left\{1,2,\ldots,d\right\}$. The hypothesis of interest and decision rules in each step  are illustrated in Table \ref{tabDecisionRule}.
\begin{table}[H]
	\caption{Decision rule in the sequential Cauchy combination test}
	\label{tabDecisionRule}
	\centering
	\begin{tabular}{p{1.cm}p{3.8cm}p{10.5cm}}
		\hline
		Step & Hypothesis & Decision\\ 
		$s=1$ & $\mathcal{H}_0^{\left( d\right) }=H_{(d)}$ & 
		If $\widetilde{p}_{(d)}\leq\alpha$ then reject $\mathcal{H}_0^{\left( d\right) }$ and include $H_{(d)}$ in $\mathcal{R}^{(1)}$
		\\
		$s=2$ & $\mathcal{H}_0^{\left( d-1\right) }=\bigcap_{j=d-1}^d H_{(j)}$  
		& 
		If $\widetilde{p}_{(d-1)}\leq\alpha$ then reject $\mathcal{H}_0^{\left( d-1\right) }$  and include $H_{(d-1)}$ in $\mathcal{R}^{(2)}$  
		\\
		$\ldots$  & $\ldots$  & $\ldots$  \\ 
		$s=d$ & $\mathcal{H}_0^{\left( 1\right) }=\bigcap_{j=1}^d H_{(j)}$ & 
		If $\widetilde{p}_{(1)}\leq\alpha$ then reject $\mathcal{H}_0^{\left( 1\right) }$ and include $H_{(1)}$ in $\mathcal{R}^{(d)}$ \\
		\hline
	\end{tabular}
\end{table}
Let $\mathcal{N}\left(\mathcal{R}^{(s)}\right)$ be the successor function, representing hypotheses to be rejected in the next step given that $\mathcal{R}^{(s)}$ has been rejected. For the SCC test, the successor function is defined as: 
\[
\mathcal{N}\left(\mathcal{R}^{(s)}\right)=\left\{H_{(d-s)} :  \widetilde{p}_{(d-s)} \leq \alpha_{\mathcal{R}^{(s)}}=\alpha\right\}.
\]
The cut-off value is fixed (i.e., $\alpha_{\mathcal{R}^{(s)}}=\alpha$) instead of depending on the rejection set $\mathcal{R}^{(s)}$ like in many other sequential procedures. According to the sequential rejection principle of \cite{goeman2010sequential},  the SCC test  achieves a strong family-wise error rate control if the following two conditions are satisfied. 
\begin{condition}[Monotonicity]
	For every $\mathcal{R}^{(s)}\subseteq \mathcal{R}^{(l)} \subset \mathcal{H}_{0}$, 
	\[
	\mathcal{N}(\mathcal{R}^{(s)}) \subseteq \mathcal{N}(\mathcal{R}^{(l)}) \cup \mathcal{R}^{(l)}
	\]
	almost surely. 
\end{condition}
% By construction, 
The transformed $p$-values of the SCC test are monotonic by construction, with $\widetilde{p}_{(d)}$ being the largest for the smallest set of global null hypotheses $\mathcal{H}_0^{(d)} = H_{(d)}$ and $\widetilde{p}_{(1)}$ being the smallest for the largest set of global nulls $\mathcal{H}_0^{(1)} = \bigcap_{j=1}^{d} H_{(j)}$ (see Figure \ref{figSequentialCauchyIllustration}(e) for an illustration of the monotonic $p$-values). Note that the largest set of global null hypotheses has the same null specification as the GCC test \eqref{eqCauchyStatistic}. It follows that that $\widetilde{p}_{(s)}\geq \widetilde{p}_{(l)}$. Since the cut-off value is fixed, the monotonicity condition of the successor function is satisfied.

\begin{condition}[Single-step condition] \label{SS} 
	When $\mathcal{H}_{0}^{(i)} =\mathcal{T}$, 
	$\Pr\left( \widetilde{p}_{(i)} \leq \alpha\right) \leq \alpha. $
\end{condition}
Condition \ref{SS} requires FWER control of the underlying test of SCC (i.e., the Cauchy combination test) at the ``critical case" in which all hypotheses of interest are true: $\mathcal{H}_{0}^{(i)} =\mathcal{T}$. The condition can be rewritten as $\Pr{\mathcal{N}(\mathcal{F})\subseteq \mathcal{F}} \geq 1-\alpha$ and has been shown to be satisfied by \cite{liu2020cauchy}. In fact,  when $\alpha$ is very small, the familywise false rejection probability of the GCC test under the null is not only bounded by $\alpha$ but also approaches the nominal size $\alpha$, as stated in \eqref{eq:wFWER}, which implies that it is less conservative than tests based on statistical inequalities or tests which impose independence in the presence of correlation. 

The theorem below follows directly from \cite[Theorem 1]{goeman2010sequential} for general sequential rejection procedures, so that Type I control in the critical case is sufficient for overall familywise error control of the sequential procedure. 
\begin{theorem}\label{thm}
	The SCC testing procedure satisfies both the monotonicity and the single-step condition and achieves the strong FWER control:  
	\[
	\lim_{\alpha\rightarrow 0}\Pr\left\{\mathcal{R}^{(d)} \subseteq \mathcal{F} \right\} \geq 1-\alpha, 
	\]
	under Assumption \ref{ass1} if $d$ is fixed and under Assumptions \ref{ass1} and \ref{ass2} if $d\rightarrow \infty$.
\end{theorem}


\subsubsection*{An Illustration}

A more prescriptive description of the SCC testing procedure is as follows: 
\begin{enumerate}
	
	\item Obtain raw $p$-values $p_1, p_2,\ldots, p_d$ corresponding to the null hypotheses $H_{1}, H_{2},\ldots, H_{d} $;%
	
	\item Order the raw $p$-values in ascending order, 	$p_{(1)},p_{(2)},\ldots,p_{(d)}$, with corresponding null ordered hypotheses $H_{(1)},H_{(2)},\ldots,H_{(d)}$;
	
	\item Calculate the SCC test statistic $\tilde T_{(i)}$ and the transformed Cauchy $p$-values $\widetilde{p}_{(i)}$ from a subset of the ordered $p$-values $\left\{p_{(j)}\right\} _{j=i}^{d}$ using \eqref{eq:CC_mt} for $i=1,\ldots,d$;
	
	\item Obtain the rejection set $\mathcal{R}=\left\{H_{\left(i\right)} : \widetilde{p}_{(i)}\leq \alpha\right\}$. 
\end{enumerate}

Figure \ref{figSequentialCauchyIllustration} illustrates the sequential Cauchy combination procedure on a simulated sequence of test statistics. The top row shows the simulated test statistics and their corresponding $p$-values, of which some hypotheses are under the null (grey dots) and some are under the alternative (black dots). The data-generating process is the same as that in Figure \ref{figCauchyPvalsAR} with $\theta=0.9$ and $d=100$. We add constant signals for $5$ out of 100 hypotheses,  with a signal strength equal to $\pm2.806$. The sign of the signal is the same as the sign of the test statistic under the null, such that the signal always amplifies the magnitude of the test statistic. 
The GCC test rejects the global null at $\alpha = 5\%$ for this sequence of $p$-values, which tells us there is at least one signal in the sequence.  
%The estimated first-order autocorrelation of the simulated test statistics is equal to $0.7910$ under the null and is equal to $0.4987$ under the alternative. 

\begin{figure}[p]
	\caption{Rejection procedure of the sequential Cauchy Combination test}
	\label{figSequentialCauchyIllustration}\centering

	\par
	
	\subfloat[Raw test statistics]{{\includegraphics[width=.31\textwidth,angle =
			-90]{1_tstat_d_100_rho_0.9_signal_5_5} }} 
	\subfloat[Raw
	$p$-values]{{\includegraphics[width=.31\textwidth,angle = -90]{2_pvals_d_100_rho_0.9_signal_5_5} }}
	
	\vspace{0.4cm}	
	
	\subfloat[Ordered raw $p$-values]{{\includegraphics[width=.31\textwidth,angle =
			-90]{3_spvals_d_100_rho_0.9_signal_5_5} }} 
	
	\vspace{0.4cm}	
	
	\subfloat[SCC test statistics
	]{{\includegraphics[width=.31\textwidth,angle = -90]{4_ctstats_d_100_rho_0.9_signal_5_5}}}
	\subfloat[SCC 
	$p$-values]{{\includegraphics[width=.31\textwidth,angle = -90]{4_cpvals_d_100_rho_0.9_signal_5_5}}}
	
	
	\begin{minipage}{1.0\linewidth}
		\begin{tablenotes}
			\small
			\item {
				\medskip
				Note: We illustrate the mechanics of the SCC procedure on a simulated test statistic sequence with sparse signals. The top row shows raw test statistics and $p$-values of which some hypotheses are under the null and some are under the alternative. The test statistics are simulated from $N_{d}(\bm{0},\bm{\Sigma})$ as in Figure \ref{figCauchyPvalsAR}. We set $d=100$, $\theta=0.9$ and add $5\%$ signals. The strength of the signal is $\pm2.806$, with its sign identical to that of the test statistic under the null. The horizon line in panel (e) is the 5\% significance level.
			}
		\end{tablenotes}
	\end{minipage}
\end{figure}

The SCC test can tell us which individual $p$-values trigger the rejection of the GCC test. The middle row plots the raw $p$-values in ascending order and the bottom row plots its sequential Cauchy combination test statistics and $p$-values. Specifically, the bottom right panel shows that the SCC $p$-values $\widetilde{p}_{(i)}$ decrease as $i$ moves from $d$ to $1$. In this example, the SCC test rejects three out of the five alternative hypotheses and does not reject under the null hypothesis. The rejections correspond to the 4$^\text{th}$, 29$^\text{th}$ and  46$^\text{th}$ hypotheses in the top row. Note that the smallest SCC $p$-value corresponds to the $p$-value of the GCC test of \citet{liu2020cauchy}, which performs the test on the largest set of hypotheses. 