
\section{Example 2: In Search of Nonzero Alpha Assets}

\label{secApplFan}

%  the residual, the part of the expected return not explained by betas. 
The efficient market hypothesis postulates that asset prices incorporate all
available information at all times, hence, there is no way of systematically
beating the market on a risk-adjusted basis. %Because the efficient market hypothesis is postulated on a risk-adjusted basis, it requires a risk model to be tested. The most obvious one is the Capital Asset Pricing Model is a pricing model. 
In response to mounting empirical evidence of patterns in stock
returns (the so-called ``anomalies"), many new risk factors have been
introduced to explain average returns 
\citep[see e.g.,][for a historical perspective]{fama1996multifactor}. These risk factors are said to represent some
dimension of undiversifiable systematic risk which should be compensated
with higher returns. If the factor model fully characterizes expected
returns, the regression intercept (a.k.a, the ``alpha") should be equal to zero. 
 
 We search for
nonzero alpha assets using the \citet{fama2015five} five-factor model framework, in which
returns are characterized by a set of common factors (i.e., market, size, value, profitability and investment). Correctly identifying nonzero alpha assets is a needle-in-a-haystack
problem. There is a general consensus in the empirical literature that markets are efficient and mispriced assets are rare \citep[see e.g.,][]{fan2015power,giglio2021thousands}. The standard approach is to run time series regressions, asset-by-asset, and perform individual tests on the alphas. The number of assets that need to be tested simultaneously is large (e.g., all of the
S\&P 500 stocks) and the test statistics of the
cross-sectional alphas are most likely correlated due to the presence of unknown common factors 
\citep[see e.g.,][]{giglio2021thousands}. 
The key to success is to use a proper  % effective
controlling procedure that achieves the desired
FWER\footnote{An alternative target is the false discovery rate defined as the proportion of false discoveries (see e.g.,\citeauthor{barras2010false}, \citeyear{barras2010false} and \citeauthor{giglio2021thousands}, \citeyear{giglio2021thousands} for two examples).} 
control and is powerful enough to detect rare nonzero alpha assets.

Section \ref{sec_HS} introduces the nonzero alpha hypothesis and the test.
Section \ref{ssecScreening} describes a screening approach used in  \citet{fan2015power} to detect individual violations. 
Section \ref{ssecSim} presents simulation results comparing the finite
sample performance of the controlling procedures in the context of the
nonzero alpha test. Again, instead of simulating the test statistics
directly, we generate returns from a factor model and compute the test
statistics for a cross-section of financial assets. In Section \ref{sec_KF}, we
examine Fama-French portfolios formed on bivariate sorts and search for portfolios with a nonzero alpha.

\subsection{Nonzero Alpha Hypothesis  and Test}
\label{sec_HS}

The multi-factor pricing model, motivated by the Arbitrage Pricing Theory %
\citep{ross1976arbitrage}, postulates how financial returns are related to
market risks. It has enjoyed widespread application in asset pricing and
portfolio management. 
% \red{Returns covary with the factors through time.} 
% 
Let $y_{it}$ be the excess return (i.e., real rate of return minus the
risk-free rate) of the $i$th financial asset at day $t$ and consider the
following linear regression model: % \red{time series regression}
\begin{eqnarray}  \label{eqCrossDecomp}
y_{it} = a_i + \bm{b}^{\prime }_i \bm{f}_t + u_{it}, \quad \text{with } i =
1, \ldots, d, \, t = 1, \ldots, T,
\end{eqnarray}
in which $a_i$ is an intercept (a.k.a the ``alpha"), $\bm{b}_i = (b_{i1},
\ldots, b_{iK})^{\prime }$ is a vector of factor sensitivities or loadings
(a.k.a the ``beta"), $\bm{f}_t = (f_{1t}, \ldots, f_{Kt})^{\prime }$ are
observable factors, and $u_{it}$ is an idiosyncratic error which is
uncorrelated with the factors. The number of factors $K$ is fixed. 
% The number of financial assets $d$ can be larger than the sample size $T$. 
An example of \eqref{eqCrossDecomp} is the three-factor model of \citet{fama1992cross}, 
% with $K = 3$, 
which captures much of the variation in the cross-section of average returns and 
absorbs a lot of the anomalies that have plagued the CAPM 
\citep[see
also][]{fama1996multifactor}.

Our objective is to identify individual assets with a nonzero alpha. The
null hypothesis of each asset is therefore $H_i: a_i=0$ (`there is no
mispricing of asset $i$') and the alternative hypothesis is $a_i\neq 0$
(`asset $i$ is mispriced'), for $i=1,\ldots, d$. The most common way to test
this null hypothesis is to use a simple $t$-statistic for $\alpha_i$, i.e.: 
\begin{eqnarray}  \label{eqTestAlpha}
{X}_i = \frac{\hat{a}_i}{ \hat{\sigma}_{\hat{a}_i}},
\end{eqnarray}
in which, for each asset $i$, $\hat{a}_i$ is the estimated alpha and $\hat{%
\sigma}_{\hat{a}_i}$ is the estimated standard error.
% 
Under the null hypothesis, the test statistic \eqref{eqTestAlpha} follows a
Student-$t$ distribution, i.e., $X_i \sim t(\nu)$, with $\nu$ being the
degrees of freedom. A viable detection strategy is to compute the test
statistic \eqref{eqTestAlpha} for each asset in the cross-section and reject
the null hypothesis when $\abs{X_i}$ is higher than a chosen quantile of the 
$t(\nu)$ distribution.


The multiplicility issue emerges when the number of assets $d$ is large. To
identify the individual violations and control the familywise error rate, one can again use the
inequality-based, Gumbel and SCC approaches. 
The cross-sectional test statistics are  likely to be
correlated \citep[see e.g.,][]{giglio2021thousands}
 and hence the Gumbel method is again expected to be conservative. 
Admittedly, 
a Student-$t$ distribution does
not exactly fulfill the assumptions of the Cauchy combination test, but
through simulations, \citet{liu2020cauchy} show that the Cauchy approximation
is still accurate when the test statistic follows a multivariate multivariate Student-$t$
distribution. 

% Another benchmark for identifying individual violations is the screening test proposed in \cite{fan2015power},  for which the individual violations are treated as a by-product of a power enhancement global test. 

\subsection{Screening}
\label{ssecScreening}

An important benchmark in testing factor pricing models is the power enhanced global test proposed in \cite{fan2015power}, which identifies individual violations as a byproduct. 
The global test examines whether there is \textit{any} asset with nonzero alpha. The global null hypothesis is therefore that the alphas of the $d$ financial
assets are jointly indistinguishable from zero \citep[see e.g.,][]{fama1996multifactor}: 
\begin{eqnarray*} \mathcal{H}_{0}:\bm{a}%
=\bm{0}, \text{with } \bm{a}=(a_{1},\ldots ,a_{d})^{\prime }.
\end{eqnarray*}

Classical tests 
\citep[see \textit{e.g.},][and references
therein]{fan2015power} for the global null hypothesis are based on a
quadratic statistic of the form % such as the Wald statistic:
$\bm{\widehat{a}}^{\prime }\bm{V}\bm{\widehat{a}}$, in which $\hat{\bm{a}}$
is an element-wise consistent estimator (\textit{e.g.}, the OLS estimator)
for the vector of intercepts $\bm{a}$ and $\bm{V}$ is a high-dimensional
positive definite weight matrix, often taken to be the inverse of the
asymptotic covariance matrix of $\bm{\widehat{a}}$ (such as for the Wald
test). Some problems can arise for high-dimensional testing problems when
using a quadratic statistic.  In particular, they typically have low power under sparse alternatives,
because the quadratic statistic accumulates estimation errors under the null hypothesis, 
which results in large critical values that can dominate the signals in the sparse alternatives 
\citep[see][for further elaboration]{fan2015power}.

%\footnote{Some problems can arise for high-dimensional testing problems when
%using a quadratic statistic. 
%For example, a quadratic test requires making assumptions on the joint distribution of the intercepts of the $d$ regressions. 
%When $d>T$, estimating the
%covariance matrix $\bm{V}$ is challenging, as the sample analogue of the
%covariance matrix is singular. Another problem is that tests based on the quadratic form have typically low power under sparse alternatives,
%because the quadratic statistic accumulates % high-dimensional 
%estimation
%errors under the null hypothesis, 
%which results in large critical values that can dominate the signals in the sparse alternatives 
%\citep[see][for further elaboration]{fan2015power}.} 

The idea behind \citet{fan2015power}'s power enhancement is to use the following test statistic: 
\begin{equation}
	\label{eqJ}
J=J_{1}+J_{0},
\end{equation}%
where $J_{1}$ is the classical test statistic with a correct asymptotic size (e.g., Wald) but  may suffer from low power under sparse alternatives, and $J_{0}$ 
% is the power enhancement component, which 
enhances the power with little size distortion.  An example of such a $J_0$ is a screening statistic: 
\begin{equation*}
J_{0}=\sqrt{d}\sum_{j\in \widehat{S}}
\frac{\widehat{a}_{j}^{2}}{\widehat{\nu }%
_{j}},
\, 
\text{with }\widehat{S}=\left\{ j:\frac{\abs{\widehat{a}_j}}{%
\widehat{\nu }_{j}^{1/2}}>\delta _{d,T},j=1,\ldots ,d\right\},
\end{equation*}%
in which $\widehat{\nu }_{j}$ is a data-dependent normalizing threshold
taken by \citet{fan2015power} as the estimated asymptotic variance of $%
\widehat{a}_{j}$. 
The screening set $\widehat{S}$ is
constructed from the standardized  alpha with a \textquotedblleft high
criticism" threshold $\delta _{d,T}$.
Since $\max_{j\leq d}\abs{\widehat{a}_j}%
\widehat{\nu }_{j}^{-1/2}=O_p(\sqrt{\log d})$, the threshold is chosen  to dominate the maximum
noise level and set to $\delta _{d,T}=C\log (\log (T))\sqrt{\log (d)}$.\footnote{We set the constant $C$ to $1.06$ as in the code shared by \cite{fan2015power} replicating their simulations.} 

The power enhancement component $J_0$ is asymptotically zero under the null hypothesis, but diverges quickly under sparse alternatives. By construction, the method imposes a 0\% theoretical FWER, as the screening set $\widehat{S}$ is asymptotic empty under the null. Under the alternative, the screening test is expected to have power
(and hence $J_{0}$ can enhance the power of the global test) in the following region: 
\begin{equation*}
\max_{j\leq d}\frac{\abs{a_j}}{v_{j}^{1/2}}>3\delta _{d,T}, \\
% \max_{j \leq d}
% \frac{T^{1/2} \abs{a_j}}{\text{var}^{1/2} (u_{jt})} > 3 a_f^{-1/2} \delta_{d,T}, 
% Is this correct? 
\end{equation*}%
% n which $a_f$ can be estimated by $1 - \bm{\bar{f}}' \bm{w}$, with $\bm{\bar{f}} = \frac{1}{T} \sum^{T}_{t = 1} \bm{f}_t$. 
where $v_{j}$ is the asymptotic variance of $\widehat{a}_{j}$. 

The power enhancement component in \eqref{eqJ} does not serve as a test statistic on its own, but is meant to be added to a classical global statistic. It is however the only part of the test statistic which identifies individual violations and will serve as a benchmark in the next sections. % as a byproduct. 
% which is the aim of this paper.  
% The screening set $\widehat{S}$ does capture however the indices where the null hypothesis is violated as a byproduct. 
% "Unfortunately, the global test $J$ does not allow the identification of individual violations. 
% Since our aim is to identify individual violations only the part relating to the screening test is relevant. 
	% We compare the finite sample performance of the various controlling procedures in the next subsection.

\subsection{Simulations}
\label{ssecSim} 

We compare the finite sample performance of the SCC
procedure with existing controlling procedures (including the four inequality-based, Gumbel and screening)
for identifying nonzero alpha assets.  The resampling approach
considered for the drift burst test  is not suitable for the current cross-sectional context
and  is therefore not included here.

\subsubsection{Data-generating processes}

We generate excess returns $y_{it}$ from the \citet{fama2015five}
five-factor model \eqref{eqCrossDecomp} % (with $K=5$) 
for $d=100$ assets and $%
T=240$ time series observations. The loadings and factors are generated from independent
multivariate distributions, $\mathcal{N}_5 (\bm{\mu_f}, \bm{\Sigma_f})$ and $%
\mathcal{N}_5 (\bm{\mu_B}, \bm{\Sigma_B})$, respectively. The data-generating process % and parameter settings 
is inspired by 
\citet[][Section 6.1]{fan2015power} and 
\citet[][Section 4.2]{shi2022relax}. We calibrate the parameters to the Fama-French 100 portfolios, bivariately sorted by size and book-to-market  (size-BM), size and investment (Size-INV), and size and operating profitability (Size-OP), for the period spanning from % 2001 to 2020. 
January 1998 to December 2017.\footnote{The choice of the sample period is guided by Figure \ref{figFMRejections}, where we observe substantial deviations between the testing results of SCC and the benchmark procedures over this period.} Table \ref{tabPars} reports the mean ($\bm{\mu_f}$) and covariance matrix ($\bm{\Sigma_f}$) of the five factors $\bm{f}_t$ on the right and the mean ($\bm{\mu_B}$) and covariance matrix ($%
\bm{\Sigma_B}$) of $\bm{b}_i$ for the SIZE-BM portfolios on the left. The parameters for the other two portfolios, i.e., Size-INV and Size-OP, are available upon request.
%\begin{eqnarray}
%\bm{\mu_B} &= 
%\begin{bmatrix}
%1.029 \\ 
%0.578 \\ 
%0.201 \\ 
%0.047 \\ 
%0.050 \\ 
%\end{bmatrix}
%\text{ and } \bm{\Sigma_B} &= 
%\begin{bmatrix}
%\phantom{-}0.015 & -0.016 & \phantom{-}0.014 & \phantom{-}0.018 & \phantom{-}%
%0.000 \\ 
%-0.016 & \phantom{-}0.178 & -0.028 & -0.067 & -0.028 \\ 
%\phantom{-}0.014 & -0.028 & \phantom{-}0.131 & \phantom{-}0.056 & \phantom{-}%
%0.011 \\ 
%\phantom{-}0.018 & -0.067 & \phantom{-}0.056 & \phantom{-}0.097 & \phantom{-}%
%0.024 \\ 
%\phantom{-}0.000 & -0.028 & \phantom{-}0.011 & \phantom{-}0.024 & \phantom{-}%
%0.047%
%\end{bmatrix}%
%.
%\end{eqnarray}
%% We refer to Table 2 in \citet{shi2022relax} for 
%
% are set respectively to 
%\begin{eqnarray}
%\bm{\mu_f} &= 
%\begin{bmatrix}
%0.552 \\ 
%0.255 \\ 
%0.145 \\ 
%0.315 \\ 
%0.245 \\ 
%\end{bmatrix}
%\text{ and } \bm{\Sigma_f} &= 
%\begin{bmatrix}
%\phantom{-}19.942 & \phantom{-}3.472 & -1.742 & -6.725 & -3.185 \\ 
%\phantom{-}3.472 & \phantom{-}10.183 & -0.613 & -4.774 & \phantom{-}0.198 \\ 
%-1.742 & -0.613 & \phantom{-}10.265 & \phantom{-}4.341 & \phantom{-}4.317 \\ 
%-6.725 & -4.774 & \phantom{-}4.341 & \phantom{-}9.312 & \phantom{-}1.930 \\ 
%-3.185 & \phantom{-}0.198 & \phantom{-}4.317 & \phantom{-}1.930 & \phantom{-}%
%4.641%
%\end{bmatrix}%
%.
%\end{eqnarray}
%The parameters are chosen to simulate factor loadings and explanatory variables mimicking the
%market, size,  value, profitability, and investment factors.

\begin{table}[H]
	\caption{Means and covariances for generating factor loadings $\bm{b}_i$ for the Size-BM portfolios and factors $\bm{f}_t$}\label{tabPars}
	\scalebox{0.85}[0.85]{
		\begin{tabular}{r | rrrrr | r  | rrrrr}
			\hline
			\multicolumn{1}{c}{$\bm{\mu_B$}} 
			& \multicolumn{5}{c|}{$\bm{\Sigma_B}$} 
			& \multicolumn{1}{c}{$\bm{\mu_f}$} 
			& \multicolumn{5}{c}{$\bm{\Sigma_f}$} 
			\\
			\hline
			$1.029$ 
			%
			& $\phantom{-}0.015$ & $-0.016$ & $\phantom{-}0.014$ & $\phantom{-}0.018$ & \phantom{-}%
			$0.000$ 
			%
			& $0.552$
			& $\phantom{-}19.942$ & $\phantom{-}3.472$ & $-1.742$ & $-6.725$ & $-3.185$
			\\ 
			$0.578$ 
			&	$-0.016$ & $\phantom{-}0.178$ & $-0.028$ & $-0.067$ & $-0.028$ 
			& $0.255$
			& $\phantom{-}3.472$ & $\phantom{-}10.183$ & $-0.613$ & $-4.774$ & $\phantom{-}0.198$ \\ 
			$0.201$ 
			&	$\phantom{-}0.014$ & $-0.028$ & $\phantom{-}0.131$ & $\phantom{-}0.056$ & $\phantom{-}0.011$ 
			%
			& $0.145$
			& $-1.742$ & $-0.613$ & $\phantom{-}10.265$ & $\phantom{-}4.341$ & $\phantom{-}4.317$
			\\ 
			$0.047$ 
			& $\phantom{-}0.018$ & $-0.067$ & $\phantom{-}0.056$ & $\phantom{-}0.097$ & $\phantom{-}$%
			$0.024$  
			& $0.315$
			& 	$-6.725$ & $-4.774$ & $\phantom{-}4.341$ & $\phantom{-}9.312$ & $\phantom{-}1.930$ 
			\\ 
			$0.050$
			& $\phantom{-}0.000$ & $-0.028$ & $\phantom{-}0.011$ & $\phantom{-}0.024$ & $\phantom{-}0.047$
			%
			& $0.245$
			& 	$-3.185$ & $\phantom{-}0.198$ & $\phantom{-}4.317$ & $\phantom{-}1.930$ & $\phantom{-}4.641$\\ 
			\hline
			\hline	
	\end{tabular}}
%		\begin{minipage}{1.0\linewidth}
%			\begin{tablenotes}
%				\small
%				\item {
%					\medskip
%					Note: The means and covariances to generate factor loadings $\bm{b}_i$ for the Size-BM portfolios and factor returns $\bm{f}_t$ within the \citet{fama2015five} five-factor model.  
%						
%				}
%			\end{tablenotes}
%		\end{minipage}
\end{table}

The idiosyncratic noises are generated from a multivariate normal distribution, i.e., $%
\mathcal{N}_d (0, \bm{\Sigma_u})$. 
To generate cross-sectional dependence in the idiosyncratic noise similar to the one in the empirical application, the covariance matrix $\bm{\Sigma_u}$ is
computed as the sample covariance matrix of the residuals from the OLS
estimation of the five-factor model.\footnote{The coefficients are available upon request.
We thank the authors of \citet{shi2022relax} for sharing their calibration code.
} The alphas are zero ($a_i=0$) for all assets $i=1,\ldots,d$ under the null hypothesis, and under the
alternative, we consider % either a sparse but strong signal or 
many, weak signals: 
\begin{eqnarray}\label{eq:ai}
a_{i}=\left\{ 
\begin{array}{cc}
0.5 & \text{if }i\leq d^{0.4} \\ 
0 & \text{if }i>d^{0.4}
\end{array}
,\right.
\end{eqnarray}
matching the situation in our empirical application in Section \ref{sec_KF}. 



\subsubsection{Simulation results}

We treat all 100 portfolios in each subgroup as a family and control the familywise error rate at  5\%. 
The performance of the tests are measured by the empirical FWERs under
the null hypothesis and 
the global
empirical power and the successful detection rates 
under the alternative hypothesis. 
The simulation is
repeated 2,000 times.


Table \ref{tabCrossSize} presents the FWERs of the controlling procedures for the null of the nonzero alpha test in the presence of strong cross-sectional dependence. We also report the minimum and maximum off-diagonal element of the empirical correlation matrix of the test statistics across all replications. 
All procedures have reasonably good control of the familywise error rate. 
The FWERs of the inequality-based,  Gumbel and screening methods are below the nominal level of 5\%, with the Gumbel and screening  methods being the most conservative ones. 
% Note that the screening test has an empirical rejection frequency higher (i.e., 3\%) than what is expected asymptotically (i.e., 0\%) in the presence of strong cross-sectional dependence. 
Note that, in the presence of strong cross-sectional dependence, the screening test is found to have an empirical rejection frequency higher (i.e., 3\%) than what is expected asymptotically (i.e., 0\%). 
% As expected, the FWERs of the SCC procedure are  very close to upper bound imposed on the familywise error rate. 
Interestingly, the FWERs of the SCC procedure are very close to the nominal level of 5\% in all three cases. 

\begin{table}[!ht] 
	\centering
	\caption{FWERs of the nonzero alphas test with different controlling procedures under the null hypothesis}
	\label{tabCrossSize}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{c | cc | cccc cccccc}
			\hline
			& \multicolumn{1}{c}{$\widehat{\rho} (\min)$}
			& \multicolumn{1}{c|}{$\widehat{\rho} (\max)$}
			&\multicolumn{1}{c}{Bonferroni}
			&\multicolumn{1}{c}{Holm} 
			&\multicolumn{1}{c}{Hommel}  
			&\multicolumn{1}{c}{Hochberg} 
			&\multicolumn{1}{c}{Gumbel} 
			&\multicolumn{1}{c}{Screening}
			&\multicolumn{1}{c}{SCC}	\\
			\\
			\hline
			Size-BM 
			& --0.41  &   0.71
			& 4.00    &  4.00   &   4.00   &   4.00     & 3.70   &   
			3.00 & 
			5.10
			\\
			Size-INV 
			&  --0.44    &    0.60
			& 4.65   &   4.65    &  4.65   &   4.65     & 4.15      &
			3.40 &
			5.70
			\\ 
			Size-OP 
			&	--0.37      &   0.60
			& 4.30  &    4.30     & 4.30   &   4.30    &  3.80      &
			3.45 &
			5.20
			\\
			\hline
	\end{tabular}}
\end{adjustbox}	
\parbox{1\textwidth}{\footnotesize%
	\vspace{.1cm} % If wanted space after the bottomrule
	{Note}: The data generating process is \eqref{eqCrossDecomp} with $a_{i}=0$ for all $i=1,...,100$. 
	The sample size is $T=240$ and the number of assets is $d=100$. 
	The nominal level $\alpha$ of the tests is 5\%. The simulation is repeated 2,000 times. }
\end{table}

Table \ref{tabCrosspower} presents the global power and the successful detection rates for the alternative of the nonzero alpha test 
in the presence of strong cross-sectional dependence and 
 weak signals. 
 %  The SCC test outperforms other procedures with much higher global power and successful detection rates. 
 The SCC test outperforms all the other procedures, i.e., it has a much higher global power and more successful detections.
The power and successful detection rates of the Gumbel and screening methods are the lowest. 
The results are consistent across all three different parameter settings calibrated to the Size-BM, Size-INV and Size-OP portfolios, respectively.

\begin{table}[!ht] 
\centering
\caption{Global powers and successful detection rates (in\%) of the nonzero alpha test with different controlling procedures}
\label{tabCrosspower}
\begin{adjustbox}{max width=\textwidth}
	\begin{tabular}{c | cc | cccc cccccc}
		\hline
		& \multicolumn{1}{c}{$\widehat{\rho} (\min)$}
		& \multicolumn{1}{c|}{$\widehat{\rho} (\max)$}
		&\multicolumn{1}{c}{Bonferroni}
		&\multicolumn{1}{c}{Holm} 
		&\multicolumn{1}{c}{Hommel}  
		&\multicolumn{1}{c}{Hochberg} 
		&\multicolumn{1}{c}{Gumbel} 
		&\multicolumn{1}{c}{Screening}
		&\multicolumn{1}{c}{SCC}	\\
		\hline
		\multicolumn{10}{c}{Global power}\\
		% paper is getting a bit too long, so we removed the sparse strong signals
		%			\multicolumn{10}{c}{Global power: Sparse  signals}\\
		%			Size-BM & 
		%			--0.48      &      0.56
		%			& 90.50  &   90.50  &   90.50  &   90.50  &   89.70 &      
		%			89.15
		%			& 90.65
		%			\\
		%			Size-INV & 
		%			-0.23     &     0.80
		%			& 56.80   &  56.80  &   56.80   &  56.80   &  54.65     
		%			& 52.75
		%			& 56.75
		%			\\
		%			Size-OP & 
		%			-0.27      &      0.73
		%			& 70.10   &  70.10  &   70.10  &   70.10  &   68.60  &  67.00 & 70.40
		%			\\
		% \multicolumn{10}{c}{\red{Update}} \\
		%			Size-BM 
		%			& -0.42      &      0.68
		%			& 85.00   &  85.00  &   85.00   &  85.00  &   83.90   &   82.90 & 85.70
		%			\\
		%			Size-INV 
		%			& --0.45      &     0.60
		%			& 95.10  &   95.10   &  95.10    & 95.10  &   94.70  & 93.95 &  95.30
		%			\\
		%			Size-OP 
		%			& --0.37      &      0.61 
		%			& 99.30  &   99.30   &  99.30    & 99.30  &   99.30  &   99.10 & 99.40
		%			\\
		% \multicolumn{10}{c}{Global power: Weak  signals}\\
		%			Size-BM & 
		%			-0.52      &      0.56
		%			& 54.25   &  54.25  &   54.25   &  54.25 &  52.05     
		%			& 49.95
		%			& 59.00
		%			\\
		%			Size-INV & 
		%			-0.25     &     0.83
		%			& 46.75  &   46.75 &    46.85  &   46.75   &  44.60   &  42.35 & 52.00
		%			\\
		%			Size-OP &
		%			-0.27      &     0.74
		%			& 65.50  &   65.50  &   65.60   &  65.50   & 63.95     
		%			& 61.55
		%			& 70.75 \\
		%			\multicolumn{10}{c}{\red{Update}} 
		%			 \\			
		Size-BM 
		&  --0.41      &      0.71
		& 54.15   &  54.20   &  54.25   &  54.20  &   51.55    & 
		49.15 &
		58.75
		\\
		Size-INV 
		& --0.43      &     0.60
	& 	72.10    & 72.10  &   72.10  &   72.10  &   70.10    
		& 67.80
		 & 76.40
		\\
		Size-OP 
		& --0.36      &      0.59
		& 83.40   &  83.40  &   83.40  &   83.40 &     81.75   & 80.25 & 88.05
		\\
		
		\multicolumn{10}{c}{Successful detection rates}\\
		%\multicolumn{10}{c}{SDR: Sparse signals}\\
		%Size-BM 
		%& -0.48      &      0.56
		%& 89.90 & 89.90 & 89.90 & 89.90  & 89.10 & 88.65 & 89.90\\ 
		%Size-INV & 
		%-0.23      &      0.80
		%& 54.60 & 54.60 &  54.60 & 54.60 & 52.65 & 50.75 & 53.70
		%\\
		%Size-OP & 
		%-0.27      &      0.73
		%& 68.50 & 68.50 & 68.50 & 68.50 & 67.35 & 65.70 & 68.15
		%\\
		%\multicolumn{10}{c}{\red{Update}} \\
		%Size-BM 
		%& --0.43      &    0.67
		%&
		%84.25  & 84.25 & 84.25 & 84.25 & 83.20 & 82.25 & 84.30 \\
		%Size--INV 
		%& --0.45      &      0.60
		%& 94.90 &    94.95   &  94.95  &   94.95    & 94.55 &  93.75  & 94.95
		%\\
		%	Size-OP 
		%& --0.37      &      0.61 
		% & 99.25 & 99.25 & 99.25 & 99.25 & 99.25  & 99.05 & 99.35 
		%\\
		%\multicolumn{10}{c}{SDR: Weak  signals}\\
		%Size-BM 
		%& -0.52      &      0.56
		%& 14.18 & 14.23 & 14.24 &14.23& 13.43 &12.73 & 16.06
		%\\
		%Size-INV & 
		%-0.25      &     0.83
		%& 15.78 & 15.88 & 15.92 & 15.88 & 14.86 & 14.09 & 18.38
		%\\
		%Size-OP & 
		%-0.27      &     0.74
		%& 27.08 & 27.25 & 27.30 & 27.25 & 25.94 & 24.83 & 30.35
		%\\		
		%\multicolumn{10}{c}{\red{Update}} 
		%\\
		Size-BM  
		&  --0.41      &      0.71
		& 15.49 & 15.56 & 15.59 & 15.57 & 14.56 & 13.62 & 17.67
		\\
		Size-INV 
		& --0.43      &     0.60
		& 	27.80    & 27.92  &   27.99 & 27.92 & 26.45 & 24.73 & 30.88
		\\
		Size-OP 
		& --0.36      &      0.59
		& 41.45   &  41.63  &   41.65  &   41.63 &     39.62   & 37.88 & 45.72
		\\
		% \multicolumn{10}{c}{\red{\textit{Sanity check}}}\\
		%\red{$T = 300, d = 500$}
		%& -0.10    &   0.11
		%& 7.24   &  7.25  &  7.25   &  7.25 &    7.20 &    
		%1.90
		%&			 9.12
		%\\
		%\red{$T = 300, d = 800$} & 
		%-0.10 & 0.10 
		%& 6.89   &   6.89  &    6.89  &    6.89  &    6.97  &    1.56
		%& 8.50
		%\\
		\hline
\end{tabular}}
\end{adjustbox}	
\parbox{1\textwidth}{\footnotesize%
\vspace{.1cm} % If wanted space after the bottomrule
{Note}: 
The data generating process is \eqref{eqCrossDecomp} with $a_i$ specified in \eqref{eq:ai}. The sample size is $T=240$ and the number of assets is $d=100$. The nominal level $\alpha$ is 5\%. The simulation is repeated 2,000 times. 
}
\end{table}




\subsection{Empirics: Kenneth French Portfolios} \label{sec_KF}

In the empirical application, we study the portfolios 
available in
Kenneth French's Data Library.\footnote{\url{https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html}} 
We focus on the  $d = 100$  portfolios formed  bivariately ($10\times10$) on Size-BM, Size-INV, and Size-OP. 
The left-hand side of  \eqref{eqCrossDecomp} consists of value-weighted portfolio excess returns and the right-hand side are the five factors in the \citet{fama2015five} five-factor model. 
The sample period is from July 1963 to November 2022 and consists of $713$ monthly observations. 
% We choose the portfolios which are value-weighted. 
We conduct the nonzero alpha test using a rolling window procedure with a window size of $T = 240$ observations. 

Table \ref{tabCrossFFK} reports the average rejection frequencies of zero alpha null (across a  rolling analysis) using the same controlling procedures as in the previous subsection. 
As expected, there are few violations. For example for the Size-BM portfolios, the average rejection frequency is between  1.09\% and 2.60\%. 
% meaning that out of a total of 100  portfolios, 1 or 2 are inconsistent with the model
The rejection frequency is always higher for the SCC test, 
%  higher in the Size-INV portfolios among the three types of sorted portfolios. Importantly, the SCC procedure consistently rejects more than other controlling procedures. Take the Size-BM 100 portfolio as an example. The rejection number of SCC is the highest ($2.60\%$), 
followed by the inequality-based methods, 
% screening procedure (about 1.25\%) and 
the screening procedure, and the Gumbel method, in this order.
% The Gumbel and the screening procedures are the two most % conservative ones. % rejecting only $12\% - 13\%$. 
The empirical results are consistent with our simulations and previous research on mispricing: nonzero alpha assets are rare \citep[see \textit{e.g.},][]{fama1996multifactor,fan2015power,giglio2021thousands}, but the SCC testing procedure can detect more of these rare violations. 

\begin{table}[!ht] 
	\centering
	\caption{Average empirical rejection frequencies (in\%) of zero alpha null across a rolling analysis of bivariate-sorted portfolios}
	\label{tabCrossFFK}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{c | ccccccccc}
			\hline
			&\multicolumn{1}{c}{Bonferroni}
			&\multicolumn{1}{c}{Holm} 
			&\multicolumn{1}{c}{Hommel}  
			&\multicolumn{1}{c}{Hochberg} 
			&\multicolumn{1}{c}{Gumbel}
			&\multicolumn{1}{c}{Screening}
			&\multicolumn{1}{c}{SCC} 
			\\
			\hline
			% \multicolumn{8}{c}{Average rejections (in\%)}  \\
			Size-BM  
			& 1.25 & 1.26 & 1.27 & 1.26 & 1.09 & 1.09 & 2.60 \\
			Size-OP 
			& 0.53 & 0.53 & 0.53 & 0.53 & 0.51 & 0.61 & 0.75
			\\
			Size-INV  & 1.19 & 1.19 & 1.19 & 1.19 & 1.13 & 1.15 & 1.40 
			\\
			\iffalse
			\multicolumn{8}{c}{Maximum rejections (in\#)}  \\
			Size-BM  
			& 4  & 5  & 5 &  5  & 4 &  4 & 13 \\
			Size-OP 
			& 2 & 2 & 2 & 2 & 2 & 3 & 3
			\\
			Size-INV  & 3 & 3 & 3 & 3 & 3 & 3 & 6
			\\
			\fi
			\hline
			\hline
			
	\end{tabular}}
\end{adjustbox}	
\parbox{0.9\textwidth}{\footnotesize%
	\vspace{.1cm} % If wanted space after the bottomrule
	{Note}: 
	We test the zero alpha null hypotheses within the Fama-French five-factor model framework using various controlling procedures on the full sample. The nominal level is 5\% and the rolling window size is $T = 240$. 
}
\end{table}

Figure \ref{figFMRejections} plots the number of rejected size-BM portfolios over time. Evidently, the rejection numbers vary over time: there is almost zero rejection according to all procedures in the first decade of the sample period. The number of nonzero portfolios increased in the late 1990s, peaked when the dot-com bubble crashed in the early 2000s, and declined afterwards. Importantly, the SCC test identifies more violations than other procedures for about $40\%$ of the sample period. For the remaining periods, the rejection numbers of SCC are on par with other procedures. While results from the benchmark procedures are similar, the gap between SCC and other procedures can be very substantial. For example, SCC detected 13 portfolios with nonzero alpha in March 2001, as opposed to 1 or 2 according to the benchmark procedures. The findings above corroborate with our theory and simulations results, suggesting that SCC can significantly improve testing outcomes.


% a result which can then be used to to improve , % 
%The five-factor risk return relationship \citep{fama2015five} is just a model. It does not explain the expected return of all sorted portfolios. But empirically, these violations of the factor model are an important first diagnostic in understanding why the assets are not consistent with the factor model and % which allows us to 
%allows us to subsequently improve the models for returns and average returns. 


\begin{figure}[!ht] % 
	\caption{% Number of rejected Size-BM portfolios: a rolling window analysis
	Number of rejected Size-BM portfolios from 1963 to 2022} 
	\label{figFMRejections}\centering
	
	\includegraphics[width=.6\textwidth,angle = -90]{Size-BM-100} 
	
	% \subfloat[Size-BM]{{\includegraphics[width=.35\textwidth,angle = -90]{Size-BM-100} }}
	% \subfloat[Size-OP]{{\includegraphics[width=.35\textwidth,angle = -90]{Size-OP-100} }}
	
		%	\vspace{.5cm}
		%	
		%	\subfloat[
		%	\if1\edits\red{\fi
		%		Size-INV
		%		\if1\edits}\fi
		%	]{{\includegraphics[width=.35\textwidth,angle = -90]{Size-INV-100} }}
	
	
	\begin{minipage}{0.8\linewidth}
		\begin{tablenotes}
			\small
			\item {
				\medskip
				Note: The nominal level is 5\% and the rolling window size is $T = 240$. 
				% and the maximum number of rejected portfolios (in\#). 
			}
		\end{tablenotes}
	\end{minipage}
\end{figure}

% \clearpage


