\vspace{-0.2cm}
\section{Numerical results}
\vspace{-0.15cm}
We now present experimental results of the proposed Algorithm \ref{alg_modular_sgd} on two exemplar problems in computed tomography (CT). 
The first set of experiments consider a simulated setting for quantitatively comparing the performance of Algorithm 
\ref{alg_modular_sgd} with the corresponding Hilbert and Banach space versions \eqref{eq:sgd_banach}.
%, in a setting where the ground truth data is known. Moreover, the impact of the stochastic setting is shown by comparing CPU times with the corresponding deterministic algorithms.   
In the second set of experiments we consider the dataset of real-world CT scans of a walnut \red{taken from} \url{doi:10.5281/zenodo.4279549}, with a fan beam geometry.
For these data, we utilise the insights from the first set of experiments and apply Algorithm \ref{alg_modular_sgd} in a setting with different noise modalities across the sinogram space.
%which allows investigating the use of variable exponents in the measurement space.
%
The experiments were conducted in \texttt{python}, using the open source  package \cite{CIL2021} for the tomographic backend.

\vspace{-0.25cm}

\paragraph{Hyper-parameter selection.} In the following experiments, we employ a decaying stepsize regime such that it satisfies the conditions of Theorem \ref{thm:as_lin_convergence} for the convergence of Banach space SGD, cf. \cite{Kereta2023}. A need for a decaying stepsize regime is common for stochastic gradient descent to mitigate the effects of inter-iterate variance.
Specifically, we use $\mu_k=\frac{\mu_{0}}{1+c(k/N_s)^\gamma}$, where $\mu_0>0$ is the initial stepsize, and $\gamma>0$ and $c>0$ control the decay speed. For the Hilbert space setting, $\mathbf{SGD}_2$, initial stepsize $\mu_0$ is given by the Lipschitz constant of the gradient of the objective function, namely $\mu_0=0.95/\max_{i}\|A_i\|^2$. For $\mathbf{SGD_p}$ and $\mathbf{SGD_{p_n,q_n}}$ the estimation of the respective H\"older continuity constant is more delicate and $\mu_0$ has to be tuned to guarantee convergence. However, its tuning is rather easy and the employ of a decaying strategy makes the choice of $\mu_0$ less critical.

As far as variable exponents are concerned, it is difficult (and somehow undesirable) to have a unified configuration as their selection is strictly problem-related. Parameters $(q_n)_n$ are related to the regularity of the measured sinograms as well as the different noise distributions considered.
For instance, when impulsive noise is considered, values of $q_-$ and $q_+$ closer to 1 are preferred while and for Gaussian noise values closer to $2$ are more effective. Solution space parameters $p_-$ and $p_+$ relate to the regularity of the solution to retrieve. As a consequence, their choice is intrinsically harder. We refer the reader to \cite{Bonino23}, where a comparison between different choices for $p_-$ and $p_+$ and different interpolation strategies is carried out for image deblurring with gradient descent \eqref{eq:gd_banach} in $\lpn$. 
%It is shown that the interpolation strategy used does not affect the reconstruction quality significantly, but the latter is more sensitive to the choice of a good interval $[p_-,p_+]$. However, the tuning of $p_-$, $p_+$, $q_-$, $q_+$ remains heuristic and specific to the particular problem and image considered. This will be object of further study.



\vspace{-0.25cm}
\paragraph{Simulated data.} We considered \eqref{eq:inv_prob} with $A$ given by the discrete Radon transform. For its definition we use a 2D parallel beam geometry, with 180 projection angles on a 1 angle separation, 256
detector elements, and pixel size of 0.1.
The synthetic phantom was provided by the CIL library, see Figure \ref{fig:phantom15}(b). 
After applying the forward operator, a high level (15\%) of salt-and-pepper noise is applied to the sinogram. The noisy sinogram is shown in Figure \ref{fig:phantom15}(a). 
\begin{figure}[t!]
\centering
\small
\setlength{\tabcolsep}{0pt}
\begin{tabular}{cccc}
% \includegraphics[height=0.22\textwidth]{Figures/Phantom15/sinogram.png}
{
\begin{tikzpicture}
\draw (0,0) node [anchor=south] {\includegraphics[height=0.22\textwidth]{{Figures/Phantom15/sinogram.png}}};%
% \spy on (0.1cm,-1.4cm) in node at (0.61cm, -.6cm);
\end{tikzpicture}
\hspace*{-4mm}
}
&
% \includegraphics[height=0.22\textwidth
\showpic{Figures/Phantom15/phantom15.png}\hspace*{-4mm} &
% \includegraphics[height=0.22\textwidth]
{
\begin{tikzpicture}[spy using outlines={rectangle, magnification=2, width = 0.75cm, height = 0.75cm,
connect spies, red, thick}]%
\draw (0,0) node [anchor=north] {\includegraphics[height=0.22\textwidth]{Figures/Phantom15/phantom15_rec_var_exp_40epochs.png}};%
\spy on (0.1cm,-1.4cm) in node at (0.61cm, -.6cm);
\end{tikzpicture}}
\hspace*{-4mm}
&
% \includegraphics[height=0.22\textwidth]
% \showpic
\begin{tikzpicture}[spy using outlines={rectangle, magnification=2, width = 0.75cm, height = 0.75cm,
connect spies, red, thick}]%
\draw (0,0) node [anchor=north] {\includegraphics[height=0.22\textwidth]{Figures/Phantom15/phantom15_px_stochastic.png} 
};%
\spy on (0.1cm,-1.4cm) in node at (0.54cm, -.61cm);
\end{tikzpicture}\hspace*{-4mm} \\
{\scriptsize{(a) Sinogram}} & {\scriptsize{(b) GT}} & {\scriptsize{(c) $\mathbf{SGD_{p_n,q_n}}$}} & {\scriptsize{(d) $(p_n)$ map}}
\end{tabular}
\caption{\footnotesize In (c) reconstruction of noisy sinogram (a) by $\mathbf{SGD_{p_n,q_n}}$, where $1.05= p_-\le(p_n)\le p_+=1.25$ is shown in (d) and $1.05=q_-\le(q_n)\le q_+=1.25$ is based on the model observation corresponding to $(p_n)$.}
\label{fig:phantom15}
\vspace{-0.25cm}
\end{figure}
\begin{figure}[t!]
\centering
\begin{subfigure}[b]{0.33\textwidth}
\includegraphics[height=3.5cm]{Figures/Phantom15/phantom15_mae.png}
\caption{MAE}
\label{fig:mae}
\end{subfigure}
\begin{subfigure}[b]{0.31\textwidth}
\includegraphics[height=3.5cm]{Figures/Phantom15/phantom15_psnr.png}
\caption{PSNR}
\label{fig:psnr}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
\includegraphics[height=3.5cm]{Figures/Phantom15/phantom15_ssim.png}
\caption{SSIM}
\label{fig:ssim}
\end{subfigure}
\caption{\footnotesize Quality metrics along the first 100 epochs of $\mathbf{\text{SGD}_2}$; $\mathbf{\text{SGD}_{1.1}}$; $\mathbf{\text{SGD}_{p_n,q_n}}$ with and without adapting the exponent maps $(p_n)$. $\mathbf{\text{SGD}_2}$ is omitted from MAE and SSIM to improve the readability of the plots, due to its poor performance.}
\label{fig:phantom15_metrics}
\vspace{-0.15cm}
\end{figure}

\begin{table}[t]
\begin{center}
% \begin{tabular}{@{}cccc@{}}
% \centering
\begin{tabular}{c@{\hskip 0.2in}ccc@{\hskip 0.1in}cccccc}
% \toprule
\cmidrule[\heavyrulewidth]{2-10}
& \multicolumn{2}{c}{Deterministic} &      & \multicolumn{6}{c}{Stochastic ($\cdot=\mathbf{S}$)}                                            \\ \cmidrule{2-3} \cmidrule{5-10} 
Algorithm                   & It. & Tot. & & It.& Epoch & Tot.  & MAE & PSNR & SSIM \\ 
\cmidrule{1-3} \cmidrule{5-10} 
$\cdot\mathbf{GD_2}$                        & 0.44s         & 1324s & &0.02s         & 0.74s    & 74.4 s & 2.582{\rm e-}1   & 57.89  & 0.0304  \\
$\cdot\mathbf{GD_{1.1}}$                    & 0.43s         & 1297s & & 0.03s         & 0.81s    & 81.3s   & 3.671{\rm e-}3 & 82.64 & 0.9897   \\
$\cdot\mathbf{GD_{p_n,q_n}}$                     & 0.47s         & 1403s & & 0.03s         & 0.96s    & 96.5s  & 2.887{\rm e-}3 & 84.05 & 0.9927 \\
$\cdot\mathbf{GD_{p_n,q_n}}$ adapt.        & 0.44s         & 1317s & & 0.03s         & 0.91s    & 91.2s  &  1.777{\rm e-}3 & 88.10 & 0.9965  \\
Compute $(p_n),\ (q_n)$ & 0.45s         & 16s   & & 0.03s         & 0.8s      & 4.0s & - & - & -\\
\bottomrule
\end{tabular}
\end{center}
\caption{\footnotesize Comparison of per iteration cost and total CPU times after 3000 iterations for determistic algorithms and after 100 epochs for stochastic algorithm with $N_s=30$. MAE, PSNR and SSIM values for stochastic algorithms are computed after 40 epochs (before noise overfitting).
% \zeljko{maybe add some further details about how was this computed. Ie were metrics computed or not, what machine, averaging}
}
\label{tab:cputime}
\vspace{-0.7cm}
\end{table}
To compute subset data $A_i$ and $y_i$, the forward operator and the sinogram are pre-binned according to equally spaced views (w.r.t.~the number of subsets) of the scanner geometry. Subsequent subset data are offset from one another by the subset index $i$.
We consider $N_s=30$ batches. We compare results obtained by solving \eqref{eq:min_prob_general} by:
\begin{itemize}
    \item[] $\mathbf{{SGD}_{2}}$: $\mathcal{X}=\mathcal{Y}=\ell^2(\mathbb{R})$, $\mathcal{Y}=\ell^2(\mathbb{R})$, $f(x)=\frac{1}{2}\|Ax-y\|_2^2$ by SGD; 
    \item[] $\mathbf{{SGD}_p}$:  $\mathcal{X}=\mathcal{Y}=\ell^p(\mathbb{R})$, $p=1.1$, $f(x)=\frac{1}{p}\|Ax-y\|_p^p$ by Banach SGD  \eqref{eq:sgd_banach}; 
    \item[] $\mathbf{{SGD}_{p_n,q_n}}$: $\mathcal{X}=\lpn(\mathbb{R})$, $\mathcal{Y}=\ell^{(q_n)}(\R)$ for appropriately chosen exponent maps, $f(x)=\Bar{\rho}_{(q_n)}(Ax-y)$ with modular-based SGD Algorithm \ref{alg_modular_sgd}.
\end{itemize}
We considered step-sizes $\mu_k=\frac{\mu_{0}}{1+0.1(k/N_s)^\gamma}$, with $\mu_{0}$ and $\gamma$ which depend on the algorithm.\footnote{For $\mathbf{{SGD}_2}$ $\mu_0$ is set as $0.95/\max_{i}\|A_i\|^2$ and $\gamma=0.51$. For $\mathbf{{SGD}_p}$ and $\mathbf{{SGD}_{p_n,q_n}}$, we use $\mu_0=0.015$ with $\gamma=(p-1)/p+0.01$ and $\gamma=(p_--1)/p_-+0.01$ respectively.}
Spaces $\lpn(\mathbb{R})$ allow for variable exponent maps sensitive to local assumptions on both the solution and the measured data. 
A possible strategy for informed pixel-wise variable exponents consists in basing them on observed data (for $(q_n)$) and an approximation of the reconstruction (for $(p_n)$), as done in \cite{Bonino23,Lazzaretti_SISC22,Estatico2019}. 
To this end, we first compute an approximate reconstruction $\Tilde{x}\in\lpn(\mathbb{R})$ by running $\mathbf{{SGD}_p}$ in $\ell^{1.1}(\mathbb{R})$ for 5 epochs with a constant stepsize regime. 
The map $(p_n)$ is then computed via a linear interpolation of $\Tilde{x}$ between $p_{-}=1.05$ and $p_{+}=1.25$.  
The map $(q_n)$ is chosen as the linear interpolation between $q_{-}=1.05$ and $q_{+}=1.25$ of $A(p_n)$. 
The bounds $p_-,p_+$ and $q_-,q_+$ are chosen by prior assumptions on $y$ (sparse phantom) and on the noise observed (impulsive). 
% \red{ML: Should I show also the map $(p_n)$ in Figure \ref{fig:phantom15}?}
%
%The variable exponent setting also allows a further modification in the utilised solution space exponents.
%Provided that the iterates are improving the quality of the reconstruction, a sensible strategy would be to correspondingly update the solution space exponents $(p_n)$.
%To this end, we investigate
We also tested an adaptive strategy by updating $(p_n)$ based on the current solution estimate once every $\beta_\text{updates}$ epochs to adapt the exponents along the iterations. 
%We will refer to this strategy as adaptive modular-based SGD.

In Figure \ref{fig:phantom15_metrics}, we report the mean absolute error (MAE), peak signal to noise ratio (PSNR) and structural similarity index (SSIM) of the iterates $\xk$ w.r.t.~the known ground-truth phantom along the first 100 epochs. 
Since PSNR favours smoothness, it is thus beneficial for $\mathbf{{SGD}_2}$, whereas MAE promotes sparsity hence is beneficial for both $\mathbf{{SGD}_p}$ and $\mathbf{{SGD}_{p_n,q_n}}$.
Figure \ref{fig:psnr} shows that  Banach space algorithms provide better performance than $\mathbf{{SGD}_2}$ in all three quality metrics. 
Note that all the results show the well-known semi-convergence behaviour with respect to the metrics considered.
%: after the improvements throughout the first 40 epochs, the reconstructions start to overfit to data noise.
To avoid such behaviour an explicit regulariser or a sound early stopping criterion would be beneficial for reconstruction performance. 
We observe that the use variable exponents does not only improve all quality metrics, but also makes the algorithm more stable: the quality of the reconstructed solutions is significantly less sensitive to the number of epochs,  making possible early stopping strategies more robust. 

In Table \ref{tab:cputime}, the CPU times for deterministic ($\mathbf{GD_2}$, $\mathbf{GD_p}$ and $\mathbf{GD_{p_n,q_n}}$) approaches and stochastic ones ($\mathbf{SGD_2}$, $\mathbf{SGD_p}$ and $\mathbf{SGD_{p_n,q_n}}$) are compared.
%in terms of computational costs per iteration, epoch (for stochastic algorithms) and total time. 
%For the stochastic algorithms the MAE, PSNR and SSIM values after 40 epochs are reported.

\vspace{-0.2cm}
\paragraph{Real CT datasets: walnut.}
 We consider a cone beam CT dataset of a walnut \cite{Meaney2020}, from which we take a 2D fan beam sinograms from the centre plane of the cone. The cone beam data uses $0.5$ angle separation over the range $[0,360]$. The used sinogram is obtained by pre-binning the raw data by a factor of $8$, resulting in $280$ effective detector pixels.
The measurements have been post-processed for dark current and flat-field compensation.
As stepsize we used $\mu_k=\frac{\mu_{0}}{1+0.001(k/N_s)^\gamma}$, with $N_s=10$ subsets, and suitable $\mu_{0}$ and $\gamma$. \footnote{For $\mathbf{SGD}_2$ , $\mu_0=0.95/\max_{i}\|A_i\|^2$, $\gamma=0.51$. For $\mathbf{SGD}_{p_n,q_n}$ we $\mu_0=0.001$, $\gamma=0.58$.} 
Initial images are computed by $5$ epochs of $\mathbf{SGD_{1.4}}$  with a constant stepsize.
\begin{figure}[t!]
\centering
\small
\setlength{\tabcolsep}{0pt}
\begin{tabular}{cccc}
\begin{tikzpicture}[spy using outlines={rectangle, magnification=3, width = 0.75cm, height = 0.75cm,
connect spies, red, thick}]%
\draw (0,0) node [anchor=north] {\includegraphics[height=0.24\textwidth]{Figures/Walnut/Walnut_SNPSPeckle_Sinogram.pdf} 
};%
% \spy on (-0.4cm,-.65cm) in node at (0.75cm, -.55cm);
\end{tikzpicture}\hspace*{-3mm}
% \includegraphics[height=0.24\textwidth]{Figures/Walnut/Walnut_SNPSPeckle_Sinogram.pdf}
&
% \includegraphics[height=0.24\textwidth]
\begin{tikzpicture}[spy using outlines={rectangle, magnification=3, width = 0.75cm, height = 0.75cm,
connect spies, red, thick}]%
\draw (0,0) node [anchor=north] {\includegraphics[height=0.24\textwidth]{Figures/Walnut/Walnut_SNPSPeckle_Hilbert.pdf} 
};%
\spy on (-0.4cm,-.65cm) in node at (0.75cm, -.55cm);
\end{tikzpicture}\hspace*{-3mm}
% {Figures/Walnut/Walnut_SNPSPeckle_Hilbert.pdf} 
&
\begin{tikzpicture}[spy using outlines={rectangle, magnification=3, width = 0.75cm, height = 0.75cm,
connect spies, red, thick}]%
\draw (0,0) node [anchor=north] {\includegraphics[height=0.24\textwidth]{Figures/Walnut/Walnut_SNPSPeckle_NonAcq.pdf} 
};%
\spy on (-0.4cm,-.66cm) in node at (0.75cm, -.56cm);
\end{tikzpicture}\hspace*{-3mm} &
% \includegraphics[height=0.24\textwidth]{Figures/Walnut/Walnut_SNPSPeckle_NonAcq.pdf} &
% &\includegraphics[height=0.24\textwidth]
\begin{tikzpicture}[spy using outlines={rectangle, magnification=3, width = 0.75cm, height = 0.75cm,
connect spies, red, thick}]%
\draw (0,0) node [anchor=north] {\includegraphics[height=0.24\textwidth]{Figures/Walnut/Walnut_SNPSPeckle_Acq.pdf}  
};%
\spy on (-0.37cm,-.66cm) in node at (0.78cm, -.56cm);
\end{tikzpicture}\hspace*{-3mm}
% {Figures/Walnut/Walnut_SNPSPeckle_Acq.pdf} 
\\
{\scriptsize{(a) Sinogram}} & {\scriptsize{(b) SGD}} & {\scriptsize{(c) Constant exponents}} & {\scriptsize{(d) Variable exponents}}
\end{tabular}
\caption{\footnotesize{(a) Noisy sinogram with $10\%$ salt \& pepper (background) and speckle noise with $0$ mean and variance $0.01$ (foreground). (b) $\mathbf{SGD_2}$ result. (c) $\mathbf{SGD_{p_n,1.1}}$ result (d) $\mathbf{SGD_{p_n,q_n}}$ result. $p_-=1.2$, $p_+=1.3$, $q_{-}=1.1$ and $q_{+}=1.9$.}}
\label{fig:walnut_snpbackspecklefore_hilbert}
\vspace{-0.25cm}
\end{figure}
% We consider two reconstruction scenarios.

% \paragraph{Ablative study on impulse noise}
% A $0.10\%$ salt and pepper noise is applied to the sinogram, cf. Figure \ref{fig:walnut_snpnoise}(a).
% The walnut is then reconstructed using algorithm \ref{} \red{non-adaptive} with $p_Y=1.1$ in the acquisition space, and $p_{\max}=1.3,\,p_{\min}=1.1$ in the solution space. 
% The reconstructed image for $N_s=10$ is shown in Figure \ref{fig:walnut_snpnoise}(b). 
% In Figure \ref{fig:walnut_subset_study} we investigate the convergence of the objective value with respect to the number of subsets.
% The algorithms use the stepsize rule $\alpha_k=\frac{\alpha_{0,N_s}}{1+0.005(k/N_s)^\gamma}$, where $\gamma=2p_{\max}^\ast$, $N_s\in\{10,20,40\}$ is the number of used subsets, and $\alpha_{0,N_s}$ is chosen as large as possible to still ensure convergence. 

% \begin{figure}[h!]
% \centering
% \small
% \setlength{\tabcolsep}{0pt}
% \begin{tabular}{ccc}
% \includegraphics[width=0.19\textwidth]{Figures/Walnut/Walnut_Snp_high_sinogramVert.pdf} &
% % \includegraphics[width=0.4\textwidth]{Figures/Walnut/Walnut_Snp_high_hilbert.pdf}
% \includegraphics[width=0.38\textwidth]{Figures/Walnut/Walnut_Snp_high_Banach10.pdf}
% & \includegraphics[width=0.38\textwidth]{Figures/Walnut/Walnut_Snp_high_Banach10.pdf}
% % \includegraphics[width=0.38\textwidth]{Figures/Walnut/Walnut_Snp_high_Objective value.pdf}
% \\
% \scriptsize{(a) Sinogram} & \scriptsize{(b) Reconstruction with $N_s=10$} & {\scriptsize{(c) Objective value convergence}}
% \end{tabular}
% \caption{The sinogram in panel (a) contains $0.1$ salt and pepper noise. Panel (b) shows an exemplar reconstruction with $N_s=10$ subsets. In panel (c) we study the behaviour of the objective value with respect to the number of subsets. }
% \label{fig:walnut_snpnoise}
% \end{figure}
% % \red{maybe remove the hilbert and just have the sinogram, the banach recon and the objective value}


% \paragraph{Mixed sinogram noise}
% In the second set of experiments on the walnut we 
We consider a more delicate noise setting that requires exponential maps which vary in the acquisition domain.
Here, we assume that noise has a different effect on the background (zero entries) and the foreground (non-zero entries) of the clean sinogram.
Namely, we apply $10\%$ salt and pepper noise to the background, and speckle noise with mean $0$ and variance $0.01$ to the foreground, cf. Fig. \ref{fig:walnut_snpbackspecklefore_hilbert}(a) for the resulting noisy sinogram.
Notably, since this noise model has a non-uniform effect across the measurement data, Banach space methods favouring the adjustment of the Lebesgue exponents are expected to perform better than those making use of a constant value. 
Taking as a reference the result obtained by $\mathbf{SGD_2}$ (Fig. \ref{fig:walnut_snpbackspecklefore_hilbert}(b)), we compare here the effect of allowing variable exponents in the solution space only with the effect of allowing both maps $(p_n)$ and $(q_n)$ to be chosen.
By choosing $(p_n)$ based on the initial image and interpolating it between $p_{-}=1.2$ and $p_{+}=1.3$ we then compare $\mathbf{SGD_{p_n,1.1}}$ (i.e., fixed exponent $q=1.1$ in the measurement space), cf.~ Fig. \ref{fig:walnut_snpbackspecklefore_hilbert}(c), with $\mathbf{SGD_{p_n,q_n}}$ where $(p_n)$ is as before while $(q_n)$ is chosen from the sinogram by interpolating between $q_{-}=1.1$ and $q_{+}=1.9$, cf.~Fig. \ref{fig:walnut_snpbackspecklefore_hilbert}(d).
The results show that a flexible framework where both maps $(p_n)$ and $(q_n)$ adapt to local contents are more suited for dealing with this challenging scenario.


