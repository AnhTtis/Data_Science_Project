% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
\usepackage[utf8]{inputenc}
\usepackage{latexsym}
\usepackage{cite}
\usepackage[ruled,vlined]{algorithm2e}
% \usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{amscd}
\usepackage{color}
\usepackage{graphicx}
% \usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs=true}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{url}
\usepackage{hyperref}
\usepackage{mathabx}
\usepackage{enumitem}
\usepackage{float}


%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hy perref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
\renewcommand\UrlFont{\color{blue}\rmfamily}

\newtheorem{Theorem}{Theorem}[section]
\newtheorem{Def}{Definition}[section]
\newtheorem{Cor}{Corollary}[section]
\newtheorem{Lemma}{Lemma}[section]
\newtheorem{Pro}{Proposition}[section]
\newtheorem{assumptions}{Assumption}
\newtheorem{Remark}{Remark}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\diff}{\mathrm{d}}%diff
\newcommand{\sign}{\operatornamewithlimits{sign}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\esssup}{\operatornamewithlimits{ess\ sup}}
\newcommand{\essinf}{\operatornamewithlimits{ess\ inf}}
\newcommand{\prox}{\operatornamewithlimits{prox}}

\newcommand{\xk}{x^{k}}
\newcommand{\xkk}{x^{k+1}}
\newcommand{\JpX}{\mathbf{J}_\mathcal{X}^{p}} 
\newcommand{\JqY}{\mathbf{J}_\mathcal{Y}^{q}}
\newcommand{\JqYi}{\mathbf{J}^{q}_{\mathcal{Y}_i}}
\newcommand{\JpXstar}{\mathbf{J}_\mathcal{X^*}^{{p}^*}}

\newcommand{\lpn}{\ell^{(p_n)}} %Lpvar discreto
\newcommand{\lp}{{L^p(\Omega)}} %Lp
\newcommand{\lpvar}{{L^{p(\cdot)}(\Omega)}} %Lp
\newcommand{\lpvarb}{{L^{p(\cdot)}}} %Lp
\newcommand{\lqvar}{{L^{p^*(\cdot)}(\Omega)}} %Lq
\newcommand{\lqvarb}{{L^{p^*(\cdot)}}} %Lq
\newcommand{\lpvardual}{{{(L^{p(\cdot)}(\Omega))^*}}} %duale di Lp
\newcommand{\lpvarspass}{{{\mathcal{A}(L^{p^*(\cdot)}(\Omega))}}}

\newcommand{\pnorm}{_{\lpvarb}}
\newcommand{\qnorm}{_{\lqvarb}}
\newcommand{\dualnorm}{_{(\lpvarb)^*}}
\newcommand{\spassnorm}{'_{p^*(\cdot)}}

\newcommand{\rhopn}{\rho_{(p_n)}}
\newcommand{\rhobarpn}{\bar{\rho}_{(p_n)}}

\newcommand{\rhopvar}{\rho_{p(\cdot)}}
\newcommand{\rhobarpvar}{\bar{\rho}_{p(\cdot)}}
\newcommand{\rhoqvar}{\rho_{p'(\cdot)}}
\newcommand{\rhobarqvar}{\bar{\rho}_{p'(\cdot)}}
\newcommand{\jrho}{\mathbf{J}_{\rho_{p(\cdot)}}}
\newcommand{\jrhobar}{\mathbf{J}_{\bar{\rho}_{p(\cdot)}}}

\newcommand{\jrhopn}{\mathbf{J}_{\rhopn}}
\newcommand{\jrhobarpn}{\mathbf{J}_{\rhobarpn}}

%\newcommand{\red}[1]{\textcolor{red}{\textsf{#1}}}
\newcommand{\red}[1]{\textcolor{black}{\text{#1}}}

\graphicspath{{Figures/Phantom15/}}


\usepackage{tikz}
\usetikzlibrary{spy,decorations.fractals,shapes.misc}
\newcommand{\showpic}[1]{%
\begin{tikzpicture}[spy using outlines={rectangle, magnification=2, width = 0.75cm, height = 0.75cm,
connect spies, red, thick}]%
\draw (0,0) node [anchor=north] {\includegraphics[height=0.22\textwidth]{#1}};%
\spy on (0.1cm,-1.4cm) in node at (0.62cm, -.58cm);
\end{tikzpicture}%
}
\newcommand{\showpik}[1]{%
\begin{tikzpicture}[spy using outlines={rounded rectangle, magnification=3, width = 2cm, height = 2cm,
connect spies, red, thick}]%
\draw (0,0) node [anchor=north] {\includegraphics[width=.5\textwidth]{#1}};%
\spy on (3.50cm,-0.52cm) in node at (-.2cm, -4cm);
\end{tikzpicture}\hspace*{-2mm}%
}


%
\begin{document}
%
\title{Stochastic gradient descent for linear inverse problems in variable exponent Lebesgue spaces 
%\thanks{Supported by organization x.}
}
%
\titlerunning{Stochastic gradient descent in variable exponent Lebesgue spaces}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%\author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
%Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
%Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
%
% \authorrunning{No author given}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
%\institute{Princeton University, Princeton NJ 08544, USA \and
%Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
%\email{lncs@springer.com}\\
%\url{http://www.springer.com/gp/computer-science/lncs} \and
%ABC Institute, Rupert-Karls-University Heidelberg, %Heidelberg, Germany\\
%\email{\{abc,lncs\}@uni-heidelberg.de}}
% \institute{No Institute given}
%

\author{Marta Lazzaretti\inst{1,3} \and
Zeljko Kereta\inst{2} \and
Claudio Estatico\inst{1} \and 
Luca Calatroni\inst{3}}
%
\authorrunning{M. Lazzaretti, Z. Kereta, C. Estatico, L. Calatroni}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Dip. di Matematica, Universit\`a di Genova, Via Dodecaneso 35, 16146, Italy\\
\email{lazzaretti@dima.unige.it}, \email{estatico@dima.unige.it} \\
\and
Dept. of Computer Science, University College London, UK\\
\email{z.kereta@ucl.ac.uk} 
\and
CNRS, UCA, Inria, Laboratoire I3S, Sophia-Antipolis, 06903,  France \\
\email{calatroni@i3s.unice.fr}
}

\maketitle              % typeset the header of the contribution
%

\begin{abstract}
We consider a stochastic gradient descent (SGD) algorithm for solving linear inverse problems (e.g., CT image reconstruction) in the Banach space framework of variable exponent Lebesgue spaces $\lpn(\mathbb{R})$. Such non-standard spaces have been recently proved to be the appropriate functional framework to enforce pixel-adaptive regularisation in signal and image processing applications. Compared to its use in Hilbert settings, however, the application of SGD in the Banach setting of $\lpn(\mathbb{R})$ is not straightforward, due, in particular to the lack of a closed-form expression and the non-separability property of the underlying norm.  In this manuscript, we show that SGD iterations can effectively be performed using the associated modular function. Numerical validation on both simulated and real CT data show significant improvements in comparison to   SGD solutions both in Hilbert and other Banach settings, in particular when non-Gaussian or mixed noise is observed in the data.

\keywords{Iterative regularisation \and Stochastic gradient descent  \and Inverse problems in Banach spaces \and Computed Tomography.}

\end{abstract}

\vspace{-0.2cm}
\section{Introduction}
% \vspace{-0.15cm}
The literature on iterative regularisation methods for solving ill-posed linear inverse problems in finite/infinite-dimensional Hilbert or Banach settings is very vast, see, e.g.,  \cite{Engl2000,Schuster2012} for surveys. Given two normed vector spaces $(\mathcal{X},\|\cdot\|_{\mathcal{X}})$ and $(\mathcal{Y},\|\cdot\|_{\mathcal{Y}})$, we are interested in the inverse problem
\begin{equation}  \label{eq:inv_prob}
\text{find}\quad x\in\mathcal{X}\quad\text{s.t.}\quad \mathcal{Y}\ni y=Ax+\eta,
\end{equation}
where $A\in\mathcal{L}(\mathcal{X};\mathcal{Y})$ is a bounded linear operator, and $\eta\in\mathcal{Y}$  denotes the (additive) noise perturbation of magnitude $\|\eta\|_{\mathcal{Y}}\le\delta$, $ \delta>0$, corrupting the measurements. %, so that $\| y - Ax\|\leq \delta$. 
Due to the ill-posedness, the standard strategy for solving \eqref{eq:inv_prob} consists in computing  $x^\star\in\argmin_{x\in\mathcal{X}} ~\Psi(x)$, where the functional $\Psi:\mathcal{X}\to \mathbb{R}_+\cup\{+\infty\}$ quantifies the fidelity of a candidate reconstruction to the measurements, possibly combined with a penalty or regularisation term enforcing prior assumptions on the sought quantity $x\in\mathcal{X}$. 
A popular strategy for promoting implicit regularisation through algorithmic optimisation consists in designing iterative schemes solving instances of the minimisation problem 
$\argmin_{x\in\mathcal{X}} ~ \| Ax-y \|_{\mathcal{Y}}~$
% where $\vvvert \cdot \vvvert_{\mathcal{Y}}$ denotes a norm-like fidelity function in the space $\mathcal{Y}$, 
or, more generally
\begin{equation}  \label{eq:min_prob_general}
\tag{P}
\argmin_{x\in\mathcal{X}}~f(x)\qquad \mbox{with} \quad f(x)=\tilde f (Ax-y),
\end{equation}
where, for $y\in\mathcal{Y}$, the function $f(\cdot)=\tilde f(A \cdot - y):\mathcal{X}\to\R_{\ge 0}$ measures the discrepancy between the model observation $Ax$ and $y$. The iterative scheme has to be endowed with a robust criterion for its early stopping in order to avoid that the computed reconstruction overfits the noise \cite{Natterer86}. In this context, the role of the parameter tuning the amount of regularisation is thus played by nothing but the number of performed iterations. One-step gradient descent algorithms, such as the (accelerated) Landweber or the Conjugate Gradient, represent the main class of optimisation methods for the resolution of \eqref{eq:min_prob_general}, see e.g. \cite{Neubauer1988,Eicke1992,Piana1997}. 
%This approach is often referred to as implicit regularisation, since regularisation is not enforced explicitly by a penalty term but it is achieved implicitly by performing a relatively small number of iterations of the iterative scheme used to solve \eqref{eq:P}.
  
The most well-studied cases consider $\mathcal{X}$ and $\mathcal{Y}$ to be Hilbert spaces, e.g., $\mathcal{X}=\mathcal{Y}=\ell^2(\R)$. In this setting, problem \eqref{eq:min_prob_general} takes the form
$
    \argmin_{x\in \ell^2(\R)} ~ \frac{1}{2}\|Ax-y\|^2_{\ell^2(\mathbb{R})}
$
and can be solved by a standard Landweber iterative scheme 
\begin{equation}  \label{eq:landweber}
\xkk=\xk-\mu_{k+1}A^*(A\xk-y),
\end{equation}
for $k\geq 0$, where $\mu_{k+1}>0$ denotes the algorithmic step-sizes. 
%It can be also seen as a gradient descent iteration for the function $f$: $\xkk=\xk-\mu_{k+1}\nabla_1 f(A\xk,y)$, with gradient $\nabla_1 f(A\xk,y)=A^*(A\xk-y)$ computed with respect to the first entry. 
%
%However, if many inverse problems can be effectively described in Hilbert spaces and solved with iterative schemes yielding to smooth solutions, 
However, many inverse problems require a more complex setting to retrieve solutions with specific features, such as sharp edges, piecewise constancy, sparsity patterns and/or to model non-standard (e.g., mixed) noise in the data. Either $\mathcal{X}$ or $\mathcal{Y}$, or both, can thus be modelled as more general Banach spaces. Notable examples are standard Lebesgue spaces $\lp$ and, in discrete settings, sequence spaces $\ell^p(\mathbb{R})$ with $p\in [1,+\infty]\setminus \left\{2\right\}$. While the solution space $\mathcal{X}$ affects the choice of the specific iterative scheme to be used, the measurement (or data) space $\mathcal{Y}$ is naturally connected to the norm appearing in \eqref{eq:min_prob_general}.
%, which in $\ell^p$ spaces is nothing but the $p$-norm of the discrepancy $Ax$ and $y$. 
For example, for Hilbert $\mathcal{X}=\ell^2(\mathbb{R})$ and Banach $\mathcal{Y}=\ell^p(\mathbb{R})$, an instance of \eqref{eq:min_prob_general} reads as
\begin{equation}   \label{eq:prob_banach}
    \argmin_{x\in\ell^2(\mathbb{R})} ~ \frac{1}{q}\|Ax-y\|^q_{\ell^p},\quad \mbox{with} \;\, q>1,
\end{equation}
for which a gradient descent-type scheme can still be used in the form  $\xkk=\xk- A^*\mathbf{J}_{\ell^p}^q(A\xk-y)$, where $\mathbf{J}_{\ell^p}^q:\ell^p(\mathbb{R}) \to \ell^{p^*}(\mathbb{R}) $ is the so-called $q$-duality map of $\ell^p(\mathbb{R})$, defined as $\mathbf{J}_{\ell^p}^q(\cdot)
= \partial\left(\frac{1}{{q}}\|\cdot\|_{\ell^p(\R)}^{{q}}\right)$. 
When both $\mathcal{X}$ and $\mathcal{Y}$ are Banach spaces, a popular algorithm for solving 
$$\argmin_{x\in\mathcal{X}} ~ \frac{1}{q}\|Ax-y\|^q_{\mathcal{Y}},\quad \mbox{with} \;\, q>1$$
% \eqref{eq:prob_banach} 
is the dual Landweber method \cite{Schopfer2006}
\begin{equation}\label{eq:gd_banach}
\xkk=\JpXstar \left( \JpX(\xk)-\mu_{k+1}A^*\JqY(A\xk-y) \right)
\end{equation} 
where 
$\JpX: \mathcal{X} \to {\mathcal{X}^*}$, 
%$\JpX = \partial\Big(\frac{1{{p}}\|\cdot\|_\mathcal{X}^{\texttt{p}}\Big)$, 
is the ${p}$-duality map of $\mathcal{X}$, $\mathbf{J}_\mathcal{X^*}^{p^*}: {\mathcal{X}^*} \to \mathcal{X}\,$ is its inverse with $p^*$ denoting the conjugate exponent of $p$, i.e.~$1/p + 1/p^* =1$. For other references of gradient-descent-type solvers in Banach settings, see, e.g.~\cite{Schopfer2006,Schuster2012,Jin_2012}.
%and $\JqY:\mathcal{Y}\longrightarrow\mathcal{Y}^*$, $\JqY= \partial\Big(\frac{1}{\texttt{p}}\|\cdot\|_\mathcal{X}^{q}\Big)$, is the ${q}$-duality map of $\mathcal{Y}$.
%
%\textcolor{red}{Add some citations with examples of problems solved in Banach settings and iterative schemes developed for this purpose}.
%

A non-standard Banach framework for solving linear inverse problems is the one of variable exponent Lebesgue spaces $\lpvar$ and $\lpn(\mathbb{R})$ \cite{LpvarBOOK}.
These Banach spaces are defined in terms of a Lebesgue measurable function $p(\cdot):\Omega\rightarrow[1,+\infty]$, or a real sequence $(p_n)_n$, respectively, that assigns coordinate-wise exponents to all points in the domain.
Variable exponent Lebesgue spaces have proven useful in the design of adaptive regularisation, 
%models 
suited %, in particular,
to model heterogeneous data  and complex noise settings. Iterative regularisation procedures in this setting have been recently studied \cite{Bonino23} and also extended to composite optimisation problems involving non-smooth penalty terms \cite{Lazzaretti_SISC22}.

While benefiting from several convergence properties, the use of such (deterministic) iterative algorithms may be prohibitively expensive in large-size applications as they require the use of all data at each iteration. In this work, we follow the strategy performed by the seminal work of Robbins and Monro \cite{Robbins1951} and adapt a stochastic gradient descent (SGD) strategy to the non-standard setting of variable exponent Lebesgue space, in order to reduce the per-iteration complexity costs. Roughly speaking, this is done by defining a suitable decomposition of the original problem and implementing an iterative scheme where only a batch of data, typically one, is used to compute the current update. Note that the use of SGD schemes has recently attracted the attention of the mathematical imaging community \cite{Jin2022,Kereta2023} due to its applicability in large-scale applications such as medical imaging \cite{Herman1993,Needell2015,Twyman2023}. However, its extension to variable exponent Lebesgue setting is not trivial due to some structural difficulties (e.g., non-separability of the norm), making the adaptation a challenging task.

\vspace{-0.2cm}
\paragraph{Contribution.} 
We consider an SGD-based iterative regularisation strategy for solving linear inverse problems in the non-standard Banach setting of variable exponent Lebesgue space $\lpn(\mathbb{R})$. To overcome the non-separability of the norm in such space, we consider updates defined in terms of a separable function, the modular function. Numerical investigation of the methodology on CT image reconstruction are reported to show the advantages of considering such non-standard Banach setting in comparison to standard Hilbert scenarios. Comparisons between the modular-based deterministic and stochastic algorithms confirm improvements of the latter w.r.t. CPU times.

\vspace{-0.2cm}
\section{Optimisation in Banach spaces}
\vspace{-0.15cm}
%\section{Mathematical preliminaries and notation}
In this section we revise the main definitions and tools useful for solving a general instance of \eqref{eq:min_prob_general} in the general context of Banach spaces $\mathcal{X}$ and $\mathcal{Y}$.
%\subsection{Optimisation algorithms in Banach spaces}
For a real Banach space $(\mathcal{X},\|\cdot\|_{\mathcal{X}})$, we denote by $(\mathcal{X}^*, \|\cdot\|_\mathcal{X^*})$ its dual space and, for any $x\in\mathcal{X}$ and $x^*\in\mathcal{X}^*$,  by $\langle x^*,x\rangle=x^*(x)\in\R$ its duality pairing.
%In the following, for any scalar $p\in(1,+\infty)$ we denote with $p^*\in\R$ its conjugate exponent, i.e. the value such that $1/p+1/p^*=1$ holds.

The following definition is crucial for the development of algorithms solving \eqref{eq:min_prob_general} in Banach spaces.  We recall that in Hilbert settings $\mathcal{H}\cong\mathcal{H}^*$ holds by the Riesz representation theorem, with $\cong$ denoting an isometric isomorphism. Hence, for $x\in\mathcal{H}$, the element $\nabla f(x) \in \mathcal{H}^*$ can be implicitly identified with a unique element in $\mathcal{H}$ itself, up to
the canonical isometric isomorphism, so that the design of gradient-type schemes is significantly simplified, as in \eqref{eq:landweber}. Since the same identification does not hold, in general, for a Banach space $\mathcal{X}$, we recall the notion of duality maps, which properly associate an element of $\mathcal{X}$ with an element  (or a subset) of $\mathcal{X}^*$ \cite{Cioranescu1990}. 

\begin{definition}
\label{def:map_dual}
Let $\mathcal{X}$ be a Banach space and $p>1$. The duality map $\JpX$ with gauge function $t\mapsto t^{p-1}$ is the operator $\JpX:\mathcal{X}\to 2^{\mathcal{X}^*}$ such that, for all $x\in\mathcal{X}$,
\[
\JpX(x)=\big\{x^{*}\in \mathcal{X}^{*}:
%x^{*}(x)=
\left<x^*,x \right>=\|x\|_\mathcal{X} \|x^{*}\|_{\mathcal{X}^{*}}, \, \|x^{*}\|_{\mathcal{X}^{*}}=\|x\|_\mathcal{X}^{p-1}\big\}.
\]
\end{definition}

Under suitable smoothness assumptions on $\mathcal{X}$ \cite{Schuster2012}, $\JpX(x)$ is single valued at all $x\in\mathcal{X}$.
%, that is $\JpX:\mathcal{X}\rightarrow \mathcal{X}^*$. 
For instance, for $\mathcal{X}=\ell^p(\mathbb{R})$, with $p>1$, %and $\mathcal{X}=\lpn(\mathbb{R})$ for $(p_n) \subset (1,+\infty)$, 
all duality maps are single-valued. 
% In addition, if $\mathcal{X}$ is a Hilbert space $\mathcal{H}$, by virtue of Riesz Theorem, the duality map reduces to the identity operator for $r=2$, that is  $\mathbf{J}_\mathcal{H}^\texttt{2}(x)=x$, where the isometric isomorphism between $\mathcal{H}$ and $\mathcal{H}^*$ has been implicitly considered.
The following Theorem (see  \cite{Cioranescu1990}) provides an operative definition and a more intuitive interpretation of the duality maps.  
% In general, the following result gives a more intuitive interpretation of the important role played by duality maps in the definition of minimization schemes.
\begin{theorem}[Asplund's Theorem]
\label{Theo:Asplund}
The duality map $\JpX$ is the subdifferential of the convex functional $h:x \mapsto \frac{1}{p}\|x\|_\mathcal{X}^p$, that is, $\JpX(\cdot) = \partial (\frac{1}{p}\| \cdot \|_\mathcal{X}^p)$.
%$$ \JpX = \partial h = \partial \left ( \frac{1}{p}\| \cdot \|_\mathcal{X}^p \right )\,.$$
\end{theorem}
% Thanks to this Theorem, the duality maps are thus monotone operators.
%The fundamental role played by duality maps in the definition of minimization scheme \eqref{eq:gd_banach} (and in the gradient of the objective functional) appears now evident. 
% Under suitable (non-restrictive) assumptions on $\mathcal{X}$ (see \cite{Schuster2012} for details), $\JpX$ is invertible and $(\JpX)^{-1}=\JpXstar$. This motivates the role of $\JpXstar$ in iteration \eqref{eq:gd_banach}.
% \vspace{-0.2cm}
\red{The following result is needed for the invertibility of the duality map.}
\begin{proposition}\cite{Schuster2012}
    Under suitable smoothness and convexity conditions on $\mathcal{X}$ and for $p>1$, for all $x\in\mathcal{X}$ and all $x^*\in\mathcal{X}^*$, there holds
    \begin{equation}   \label{pro:inverse_dual_map}
        \JpXstar(\JpX(x))=x\,, \quad \quad\JpX(\JpXstar(x^*))=x^*.
    \end{equation}
\end{proposition}
% \begin{Remark}
% If the gradient term $A^*\JqY(A\xk-y)$ vanishes in iteration \eqref{eq:gd_banach}, then $\xkk=\JpXstar(\JpX(\xk))=\xk$ by Proposition \ref{pro:inverse_dual_map}. 
% %The scheme \eqref{eq:gd_banach} is therefore is a fixed-point iteration scheme
% \end{Remark}
We notice that, if the gradient term $A^*\JqY(A\xk-y)$ vanishes in iteration \eqref{eq:gd_banach}, then $\xkk=\JpXstar(\JpX(\xk))=\xk$ by Proposition \ref{pro:inverse_dual_map}. 
%The scheme \eqref{eq:gd_banach} is therefore is a fixed-point iteration scheme

For any $p,r>1$ and for any $x,h\in\ell^p(\mathbb{R})$, the explicit formula for $\mathbf{J}_{\ell^p}^{r}$ is 
\begin{equation}  \label{eq:duality_map_banach}
\langle\mathbf{J}_{\ell^p}^{r}(x),h\rangle=\|x\|_{p}^{r-p}\sum_{n\in\N} \sign(x_n)|x_n|^{p-1}h_n.
\end{equation}
Moreover, since $\left(\ell^p(\mathbb{R})\right)^* \cong \ell^{p^*}(\mathbb{R})$, then the inverse of the $r$-duality map $\mathbf{J}_{\ell^p}^{r}$ is nothing but $\left(\mathbf{J}_{\ell^p}^{r}\right)^{-1}=\mathbf{J}_{(\ell^p)^*}^{r^*}=\mathbf{J}_{\ell^{p^*}}^{r^*}$. 
% Once the explicit analytical expression of $\mathbf{J}_{\ell^p}^{r}$, then the formula of the inverse is also known, so \eqref{eq:gd_banach} can be effectively used in $\ell^p(\mathbb{R})$ spaces. 
Hence, the explicit analytical expression of its inverse $\left(\mathbf{J}_{\ell^p}^{r}\right)^{-1}=\mathbf{J}_{\ell^{p^*}}^{r^*}$ is also known \cite{Cioranescu1990}.
 % With a slight abuse of notation, we will thus denote in the following $\mathbf{J}_{\ell^p}^{r}(x)=\|x\|_{p}^{r-p}\left(\sign(x_n)|x_n|^{p-1}\right)_{n\in\N}.$ 
\vspace{-0.2cm}
\subsection{Variable exponent Lebesgue spaces $\lpn(\mathbb{R})$}
\vspace{-0.15cm}
 In the following, we will introduce the main concepts and definitions on the variable exponent Lebesgue spaces in the discrete setting of $\lpn(\mathbb{R})$. For surveys, we refer the reader to \cite{LpvarBOOK,CruzUribeFiorenzaBOOK}.  We define a family $\mathcal{P}$ of variable exponents as
\[
\footnotesize{\mathcal{P} :=\left\{ (p_n)_{n\in\N} \subset \R :   1 < p_-:=\inf_{n\in\N} p_n \leq p_+:=\sup_{n\in\N}p_n<+\infty\right\}. }
\]
\vspace{-0.2cm}
\begin{definition}
For $(p_n)_{n\in\N}\in\mathcal{P}$ and any real sequence $x=(x_n)_{n \in \N}$, %the real functions
%\begin{equation}
%\rhopn(x):=\sum_{n\in\N} \vert x_n\vert ^{p_n}
%\label{eq:modular_rho}
%\end{equation}
\vspace{-0.2cm}
\begin{equation}
\rhopn(x):=\sum_{n\in\N} \vert x_n\vert ^{p_n} \;\quad \mbox{and} \qquad \rhobarpn(x):=\sum_{n\in\N} \frac{1}{p_n}\vert x_n\vert ^{p_n}
\label{eq:modular_rho_bar}
\vspace{-0.1cm}
\end{equation}
are called modular functions associated with the exponent map $(p_n)_{n\in\mathbb{N}}$.
\label{def:modular}
\end{definition}
\vspace{-0.2cm}
\begin{definition}
\label{def:lux_norm_lp_discr}
The Banach space $\lpn(\mathbb{R})$ is the set of real sequences $x=(x_n)_{n\in\N}$ such that $\rhopn\left(\frac{x}{\lambda}\right)<1$ for some $\lambda>0$. For any $x=(x_n)_{n\in\N}\in \lpn(\mathbb{R})$, the (Luxemburg) norm on $\lpn(\mathbb{R})$ is defined as 
\begin{equation}  \label{eq:lux_norm}
    \|x\|_{\lpn}:=\inf \left\{\lambda>0:\ \rhopn\left(\frac{x}{\lambda}\right) \le 1\right\}.
\end{equation}
%The space $\left(\lpn(\mathbb{R}), \|\cdot\|_{\lpn} \right)$ is a Banach space. 
\end{definition}
% The space $\lpvar$ is the set of functions $x\in\mathcal{F}(\Omega)$ such that
% $$\rhopvar\Big(\frac{x}{\lambda}\Big)\le 1,$$ 
% for some $\lambda>0$. For any $x\in\lpvar$, we define $\|\cdot\|_{\lpvar}:\lpvar\longrightarrow \mathbb{R}$ as
% \begin{equation}
%     \|x\|_{\lpvar}:=\inf\left\{\lambda>0:\rhopvar\Big(\frac{x}{\lambda}\Big)\le1\right\}.
% \label{eq_Lux_norm}
% \end{equation}
% The function $\|\cdot\|_{\lpvar}$ defined in \eqref{eq_Lux_norm} is a norm on $\lpvar$ and the space $\Big (\lpvar, \|\cdot\|_{\lpvar} \Big )$ is a Banach space.

We now report a result from \cite{Bonino23} where a characterisation of the duality map $ \mathbf{J}^r_{\lpn}$ is given, in relation with \eqref{eq:duality_map_banach}.
\begin{theorem}
Given $(p_n)_{n\in\N}\in\mathcal{P}$,  then %$\left(l^{\left(p_n\right)},\|x\|_{\left(p_n\right)}\right)$ is smooth. F
for each $x=\left(x_n\right)_{n\in\N}\in\lpn(\mathbb{R})$ and for any $r>1$, the duality map $\mathbf{J}^r_{\lpn}(x):\lpn(\mathbb{R})\to(\lpn)^*(\mathbb{R})$ is the linear operator defined, for  all $h=(h_n)_{n\in\N}\in\lpn(\mathbb{R})$ by:
\begin{equation}
    \langle \mathbf{J}^r_{\lpn}(x), h\rangle = \frac{1}{\sum_{n\in\N}\frac{p_n \vert x_n\vert^{p_n}}{\|x\|_{\lpn}^{p_n}}} \sum_{n\in\N}\frac{p_n \sign(x_n)\vert x_n\vert^{p_n-1}}{\|x\|_{\lpn}^{p_n-r}}h_n.
    \label{eq:Jlpvar_succ}
\end{equation}
\end{theorem}

By \eqref{eq:lux_norm}, we note that $\|\cdot\|_{\lpn}$ is not separable as its computation requires the solution of a minimisation problem involving all elements $x_n$ and $p_n$ at the same time. As a consequence, the expression \eqref{eq:Jlpvar_succ} is not suited to be used in a computational optimisation framework. The following result from \cite{Lazzaretti_SISC22} provides more flexible expressions associated to the modular functions \eqref{eq:modular_rho_bar}.
\begin{proposition}
The functions $\rhopn$ and $\rhobarpn$ in \eqref{eq:modular_rho_bar} are Gateaux differentiable at any $x=\left(x_n\right)_{n\in\N}\in\lpn(\mathbb{R})$. For $h=(h_n)_{n\in\N}\in\lpn(\mathbb{R})$ their derivatives read
\begin{equation}
\footnotesize{
% \Big(\jrho(x)\Big)(u)=\Big(\nabla\rhopvar(x)\Big)(u)
\langle\jrhopn(x),h\rangle=\sum_{n\in\N} p_n\sign(x_n)|x_n|^{p_n-1}h_n,\quad 
% \Big(\jrhobar(x)\Big)(u)=\Big(\nabla\rhobarpvar(x)\Big)(u)
\langle\jrhobarpn(x),h\rangle=\sum_{n\in\N} \sign(x_n)|x_n|^{p_n-1}h_n.
\label{eq:jrhobarpn}
}
\end{equation}
\end{proposition}

Notice that, although $\jrhopn$ and $\jrhobarpn$ are formally not duality maps, we adopt the same notation for the sake of consistency with Asplund Theorem \ref{Theo:Asplund}.
%\begin{theorem}\cite[Theorem 3.4.7, Theorem 3.4.9]{LpvarBOOK} \cite[Lemma 1]{DincaMatei2009lungo}
%    Variable exponent Lebesgue spaces $\lpn$ with $(p_n)$ such that $1<p_-\le p_+<+\infty$ are reflexive, uniformly convex and
%smooth.
%\end{theorem}

\vspace{-0.2cm}
\section{Modular-based gradient descent in $\lpn(\mathbb{R})$}
\vspace{-0.15cm}
\label{sec3}
Given $(p_n)_{n\in\mathbb{N}}, (q_n)_{n\in\mathbb{N}}\in\mathcal{P}$, we now discuss how to implement a deterministic gradient-descent (GD) type algorithm for solving an instance of \eqref{eq:min_prob_general} with $\mathcal{X}=\lpn(\mathbb{R})$ and $\mathcal{Y}=\ell^{(q_n)}(\mathbb{R})$. Recalling \eqref{eq:gd_banach}, GD iterations in this setting require knowing the duality map $\mathbf{J}^r_{\lpn}$ and its inverse.
However, as shown in \cite[Corollary 3.2.14]{LpvarBOOK}, such an inverse does not directly relate to the point-wise conjugate exponents of $(p_n)_{n\in\mathbb{N}}$ as the isomorphism between $(\lpn)^*(\mathbb{R})$ and $\ell^{(p_n^*)}(\mathbb{R})$ -differing from the standard $\ell^p$ constant case- is not isometric. As discussed in \cite{Bonino23}, the approximation 
$\left(\mathbf{J}^r_{\lpn}\right)^{-1}=\mathbf{J}^{r^*}_{(\lpn)^*}\approx \mathbf{J}^{r^*}_{\ell^{(p_n^*)}}$
can be used as an inexact (but explicit) formula for computing the duality map of  $(\lpn)^*(\mathbb{R})$. Under this assumption, the dual Landweber method can thus be used to solve the minimisation problem $\argmin_{x\in \lpn(\mathbb{R})}~\frac{1}{q}\|Ax-y\|_{\ell^{(q_n)}}^q, \quad q>1.$ 
% \begin{equation}  \label{eq:lpvar_dualland}
% \argmin_{x\in \lpn(\mathbb{R})}~\frac{1}{q}\|Ax-y\|_{\ell^{(q_n)}}^q, \quad q>0.
% \end{equation}
Note, however, that the computation of the duality map $\mathbf{J}^p_{\lpn}$ requires the computation of $\|x\|_{\lpn}$ which, as previously discussed, makes the iterative scheme rather inefficient in terms of computational time. We thus follow \cite{Lazzaretti_SISC22} and define in Algorithm \ref{alg_modular_gd} a more efficient modular-based gradient descent iteration for the resolution of \eqref{eq:min_prob_general} in the general setting of variable exponent Lebesgue spaces.  The following set of assumptions needs to hold:
\begin{enumerate}[label=\textbf{A.\arabic*}]
    \item \label{assump1} $\nabla f : \lpn(\mathbb{R}) \to(\lpn)^*(\mathbb{R})$ is $(\textit{\texttt{p}} -1)-$H{\"o}lder-continuous with exponent $1<\textit{\texttt{p}}\le2$ and constant $K>0$.
    \item \label{assump2} There exists $c>0$ such that, for all $u,v\in\lpn(\mathbb{R})\,$,
\small{
\[
\langle \jrhobarpn(u)-\jrhobarpn(v), u-v\rangle \ge c \max \left\{ \|u-v\|_{\lpn}^\texttt{p}, \|\jrhobarpn(u)-\jrhobarpn(v)\|_{(\lpn)^*}^{\texttt{p}^*}\right\} .
\label{eq:SGD_mod_based_it}
\]}
\end{enumerate}

% \begin{assumptions}
%     $\nabla f : \lpn(\mathbb{R}) \to(\lpn)^*(\mathbb{R})$ is $(\textit{\texttt{p}} -1)-$H{\"o}lder-continuous with exponent $1<\textit{\texttt{p}}\le2$ and constant $K>0$.
% %     , i.e.:
% % \[
% % \|\nabla f(u_1)-\nabla f(u_2)\|_{(\lpn)^*} \le K\|u_1-u_2\|_{\lpn}^{\textit{\texttt{p}} -1} \quad \forall \ u_1,u_2\in\lpn(\mathbb{R}).
% % \]
% \label{ass2}
% \end{assumptions}
% \begin{assumptions}
% There exists $c>0$ such that for all $u,v\in\lpn(\mathbb{R})$
% \small{
% \[
% \langle \jrhobarpn(u)-\jrhobarpn(v), u-v\rangle \ge c \max \left\{ \|u-v\|_{\lpn}^\texttt{p}, \|\jrhobarpn(u)-\jrhobarpn(v)\|_{(\lpn)^*}^{\texttt{p}^*}\right\} .
% \label{eq:SGD_mod_based_it}
% \]
% }
% \label{ass3}
% \end{assumptions}
The latter bound was previously used in \cite{GuanSong2015,Lazzaretti_SISC22}. It is a compatibility condition between the ambient space $\lpn(\mathbb{R})$ and the H{\"o}lder smoothness properties of the residual function to minimise to achieve algorithmic convergence. 

The minimisation of the specific function $f$ of \eqref{eq:min_prob_general} is achieved solving at each iteration \eqref{eq:GD_var} the following minimisation problem:  
\begin{equation*}
    \xkk = \argmin_{u\in\lpn(\mathbb{R})} \rhobarpn(u)-\langle \jrhobarpn(\xk), u\rangle + \mu_k \langle \nabla f(\xk),u\rangle.
\end{equation*}

\begin{algorithm}[t!]
\caption{Modular-based Gradient Descent in $\lpn(\mathbb{R})$}
\label{alg_modular_gd}
% \textbf{Parameters:} $\{\mu_k\}_k$ s.t. 
% \begin{equation}
% 0<\Bar{\mu}\le \mu_k \le \frac{\textit{\texttt{p}}c(1-\delta)}{K} \qquad \text{with} \ 0<\delta<1,\quad\text{for all }k\geq 0.
% \end{equation}\\
\textbf{Parameters:} $\{\mu_k\}_k$ s.t. $
0<\Bar{\mu}\le \mu_k \le \frac{\textit{\texttt{p}}c(1-\delta)}{K}$ with $0<\delta<1$, for all $k\geq 0.$\\
\textbf{Initialisation:} $x^0\in\lpn(\mathbb{R})$.\\
{
\textbf{repeat}
%\begin{itemize}
%        \item[] Compute the next iterate as: 
\begin{equation}
   \footnotesize{ \xkk = \vert \jrhobarpn(\xk)-\mu_k\nabla f(\xk)\vert ^{\frac{1}{p_n-1}}\sign{(\jrhobarpn(\xk)-\mu_k\nabla f(\xk))}}
    \label{eq:GD_var}
\end{equation}
%where the operation $\vert u \vert ^{\frac{1}{p_n-1}}\cdot \sign(u)$ is intended to be componentwise.
%\end{itemize}
\textbf{until} convergence
}
\end{algorithm}

The following proof shows that the functional $\jrhobarpn$ defined by \eqref{eq:jrhobarpn} is invertible and gives a point-wise characterisation of its inverse.
\begin{proposition}
The functional $\jrhobarpn$ in \eqref{eq:jrhobarpn} is invertible. For all $v\in(\lpn)^*(\mathbb{R})$, %its inverse reads  
\begin{equation}
%    G_v(\cdot)=\langle v,\cdot\rangle \mapsto    
     (\jrhobarpn)^{-1}(v)=
    \Big(|v_n|^{\frac{1}{p_n-1}}\sign(v_n)\Big)_{n\in\N} \in \lpn(\mathbb{R}).
\end{equation}
\end{proposition}
\begin{proof}
    By straightforward componentwise computation, we have 
    \begin{align*}
        &|\jrhobarpn(v_n)|^{\frac{1}{p_n-1}}\sign(\jrhobarpn(v_n))=|\jrhobarpn (v_n)|^{\frac{1}{p_n-1}-1}\jrhobarpn (v_n)\\
        &=|\jrhobarpn (v_n)|^\frac{2-p_n}{p_n-1}\jrhobarpn (v_n)=| \, |v_n|^{p_n-1}\sign(v_n)|^\frac{2-p_n}{p_n-1}|v_n|^{p_n-1}\sign(v_n)=v_n \, .
    \end{align*}
\end{proof}
% \begin{Remark}
% By the Proposition above, it is easy to show that if $\nabla f(x_k)=0$ at some $k\geq 0$ in \eqref{eq:GD_var}, then $\xkk=\xk$.
% \end{Remark}
% By the Proposition above, it is easy to show that if $\nabla f(x_k)=0$ at some $k\geq 0$ in \eqref{eq:GD_var}, then $\xkk=\xk$, as expected for numerical consistence.
By the Proposition above, the update rule \eqref{eq:GD_var} of Algorithm \ref{alg_modular_gd}, can be rewritten as
\[
\xkk = (\jrhobarpn)^{-1}\Big( \jrhobarpn(\xk)-\mu_k\nabla f(\xk)\Big).
\]
As a consequence, whenever $\nabla f(x_k)=0$ at some $k\geq 0$, a stationary point $\xkk=(\jrhobarpn)^{-1}\Big( \jrhobarpn(\xk)\Big)=\xk$ is found, as expected.

%{\textcolor{red}{ESTATICO: QUESTA PARTE SECONDO ME VA ACCORCIATA, TOGLIENDO LA PROPOSIZIONE 4 E RIASSUMENDO IL RISULTATO, FINO A Note that  ESCLUSO}}

The following convergence result is a special case of \cite[Proposition 3.4]{Lazzaretti_SISC22} providing an explicit convergence rate for the iterates of Algorithm \ref{alg_modular_gd}.

\begin{proposition}
\label{prop:convergencerate}
Let $x^* \in  \lpn(\mathbb{R})$ be a minimiser of $f$ and let $(\xk)_k $ be the sequence generated by Algorithm \ref{alg_modular_gd}. If $(\xk)$ is bounded, then:
\begin{equation}
\label{eq:conv_GS}
f(x^k) - f(x^*) \le  \frac{\eta}{k^{\textit{\texttt{p}}-1}}, 
\end{equation}
where $\texttt{p}>1$ is defined in assumption \ref{assump1} and $\eta=\eta(\bar{\mu}, \delta, p_{-}, x^0,x^*)$.
% \[
% \|\nabla f(u_1,v)-\nabla f(u_2,v)\|_{(\lpn)^*} \le K\|u_1-u_2\|_{\lpn}^{\textit{\texttt{p}} -1} \qquad \forall \ u_1,u_2\in\lpn.
% \]
\end{proposition}
%\lc{The following remark is not clear to me, what do we want to say? It's way too technical, I would remove it unless the message can be shortened in few lines}
%\zeljko{is the message that operations that require the computation of the pointwise norm are expensive. this suggests not only to not use the duality maps, but also to change the objective, which we herein adhere to?}
% \zeljko{I think that most of the content of this remark should not be a remark but rather just the main text. This is the objective function we take, and the gradient of that function, and what the algorithm presented is trying to minimise. }

Note that when the measurement space $\mathcal{Y}$ is a variable exponent Lebesgue space $\ell^{(q_n)}(\mathbb{R})$, a more effective and consistent choice for the objective function is the modular of the discrepancy between the model observation and the data, i.e. $f(x)=\Bar{\rho}_{(q_n)}(Ax-y)$. In this way, the heavy computations of the $\|\cdot\|_{\ell^{(q_n)}}$ norm and of its gradient are not required, making the iteration scheme faster. 
% The practical use of applying algorithm \eqref{alg_modular_gd} for solving \eqref{eq:lpvar_dualland} may be not very efficient in practice since
% %    Note that computational time improves by avoiding the use of the duality map $\mathbf{J}_{\lpn}^p$ and of $\mathbf{J}_{\ell^{(p_n^*)}}^{p^*}$, which require the evaluation of a norm in $\lpn$ and $\ell^{(p_n^*)}$ respectively, heavy to compute, at each iteration. However, to get rid of every heavy computation one needs to replace also the norm in 
% to evaluate the objective functional $f$ and its gradient $\nabla f$ the computation of the quantity $\|\cdot\|_{\ell^{(q_n)}}^q$ is required. One practical solution consists in taking as objective function $f(x)=\rhobarpn(Ax-y)$, so that $\nabla f(x)=A^*\mathbf{J}_{\Bar{\rho}_{(q_n)}}(Ax-y)$ which depends only on the derivative of the modular (and not on any $r$-duality map $\mathbf{J}_{\ell^{q_n}}^r$). 

\vspace{-0.2cm}
\section{Stochastic modular-based gradient-descent in $\lpn(\mathbb{R})$}
\vspace{-0.15cm}
The key challenge for the viability of many deterministic iterative methods for real-world image reconstruction problems is their scalability to data-size.
For example, the highest per-iteration cost in emission tomography lies in the application of the entire forward operator at each iteration, whereas each image domain datum in computed tomography often requires several gigabytes of storage space. The same could thus be a bottleneck in the application of Algorithm \ref{alg_modular_gd}.
%These two issues occur, for example, when computing the (sub-)derivative of an objective.
The stochastic gradient descent (SGD) paradigm addresses this issue \cite{Robbins1951}.

We partition the forward operator $A$, and the forward model into a finite number of block-type operators $A_1,\ldots,A_{N_s}$, where $N_s\in\mathbb{N}$ is the number of subsets of data.
The same partition is applied to the observations. 
Classical examples of this methodology include Kaczmarz methods in CT \cite{Herman1993,Needell2015}.
%Though the vast majority of existing stochastic methods operate in Hilbert, and in particular Euclidean spaces, there has recently been a renewed interest in stochastic methods in Banach space.
The SGD version of the iteration \eqref{eq:gd_banach} in Banach spaces 
%minimising \eqref{eq:P} for $\mathcal{X}=\ell^p,\mathcal{Y}=\ell^q$ utilises iterations of the form
takes the form
\begin{equation}\label{eq:sgd_banach}
\xkk=\JpXstar \left( \JpX(\xk)-\mu_{k+1}A_{i_k}^*\JqY(A_{i_k}\xk-y) \right),
\end{equation}
where the indices $i_k\in\left\{1,\ldots,N_s\right\}$ are sampled uniformly at random.
Sampling reduces the per-iteration computational cost in $\mathcal{Y}$ by a factor of $N_s$.
In  \cite{Kereta2023} convergence of the iterates to a minimum norm solution is shown.
\begin{theorem}\label{thm:as_lin_convergence}
Let $\sum_{k=1}^\infty\mu_{k}=+\infty$  and $\sum_{k=1}^\infty \mu_{k}^{p^\ast} <+\infty.$
Then 
\begin{align*}
    \mathbb{P}\Big(\lim_{k\rightarrow\infty} \inf_{\widetilde x\in \mathcal{X}_{\min}}\|{\xkk-\widetilde x}\|_{\mathcal X}=0\Big)=1.
\end{align*}
Let $\JpX(x_0)\in\overline{{\rm range}(A^\ast)}$ and let
$\mu_{k}^{p^\ast-1}\leq \frac{C}{L_{\max}^{p^\ast}}$ for all $k\geq 0$ and some constant $C>0$, where $L_{\max}=\max_i\|A_i\|$.
Then $\lim_{k\rightarrow\infty} \mathbb{E}[\|\xkk-x^\dag\|_{\mathcal{X}}]=0$ 
 $\lim_{k\rightarrow\infty} \mathbb{E}[\|\JpX(\xkk)-\JpX(x^\dag)\|^{p^\ast}]=0$.
\end{theorem}

For noisy measurements, the regularising property of SGD  should be established by defining suitable stopping criteria.
%, e.g. for strongly convex operators the stopping index behaves as $\mathcal{O}(\delta^{-p})$, mirroring the behaviour in Euclidean spaces. 
However, robust stopping strategies are hard to use in practice and having methods that are less sensitive to data overfit is crucial for their practical use.  Note that \eqref{eq:sgd_banach} is the standard form of SGD for separable objectives. Namely, for $f(x)=\|Ax-y\|_q^q$, we can choose $f_i(x;A,y)=\|A_ix-y_i\|_q^q$, so that $f(x)=\sum_{i=1}^{N_s}f_i(x)$. By Theorem \ref{Theo:Asplund}, this decomposition shows that each step of \eqref{eq:sgd_banach} can thus be computed by simply taking a sub-differential of a single sum-function $f_i$.

To define a suitable SGD in variable exponent Lebesgue spaces, we  take as objective function $f(x)=\bar{\rho}_{(q_n)}(Ax-y)$ and split it into $N_s\ge1$ sub-objectives $f_i(x):=\bar{\rho}_{(q_n^i)}(A_ix-y_i)$, so that $\nabla f_i(x)=A_i^\ast\mathbf{J}_{\Bar{\rho}_{(q_n^i)}}(A_ix-y_i)$.  
\red{Exponents} $(q_n^i)_n$ are obtained through the same partition of the exponents $(q_n)_n$ as the one used to split up the data. Then, at iteration $k$ \red{and a} randomly sampled index $1\le i_k\le N_s$, the corresponding stochastic iterates are given by
% \red{write the corresponding equation as in deterministic case but with $i$}.
\begin{equation*}
    \xkk = \argmin_{u\in\lpn(\mathbb{R})} ~\rhobarpn(u)-\langle \jrhobarpn(\xk), u\rangle + \mu_k \langle \nabla f_{i_k}(\xk),u\rangle.
\end{equation*}
The pseudocode of the resulting stochastic modular-based gradient descent in $\lpn(\R)$ is reported in Algorithm \ref{alg_modular_sgd}.
We expect that through minimal modifications an  analogous convegence result as Theorem \ref{thm:as_lin_convergence} can be proved in this setting too. A detailed convergence proof, however, is left for future research.

%The resulting algorithm is provided in \ref{alg_modular_sgd}.
\begin{algorithm}[t!]
\caption{Stochastic Modular-based Gradient Descent in $\lpn(\mathbb{R})$}
\small{
\textbf{Parameters:} $\mu_0$ s.t. $
0<\Bar{\mu}\le \mu_0 \le \frac{\textit{\texttt{p}}c(1-\delta)}{K}$, $0<\delta<1$, $N_s\ge 1$, $\gamma>0$, $\eta>0$.\\
\textbf{Initialisation:}  $x^0\in\lpn(\mathbb{R})$.\\

\textbf{repeat}
\begin{itemize}
        \item[] Select uniformly at random $i_k\in\{1,\cdots,N_s\}$.
        \item[] Set
        $\mu_k=\frac{\mu_0}{1+\eta (k/N_s) ^ \gamma}$
        \item[] Compute
        \begin{equation}
        \footnotesize{
    \xkk = \vert \jrhobarpn(\xk)-\mu_k
    %\nabla f_{i_k}(A_{i_k}\xk,y_{i_k})
    \nabla f_{i_k}(\xk)
    \vert ^{\frac{1}{p_n-1}}\sign{(\jrhobarpn(\xk)-\mu_k 
    %\nabla f_{i_k}(A_{i_k}\xk,y_{i_k}))
    \nabla f_{i_k}(\xk) )}
    }
    \label{eq:SGD_var}
\end{equation}
\end{itemize}
\textbf{until} convergence
}\label{alg_modular_sgd}
\end{algorithm}
% \vspace{-0.15cm}
% where the operation $\vert u \vert ^{\frac{1}{p_n-1}}\cdot \sign(u)$ is intended to be componentwise.


%\lc{it would be nice to have a statement of a convergence theorem in this setting or, if not, saying something on the fact that something analogous has been proved in Banach settings in \cite{Kereta2023} so we expect the same}
%\zeljko{we clearly cannot claim that such a claim has been proven, but can reasonably claim that we expect analogous convergence results}

\input{sec5.tex}
\vspace{-0.25cm}
\section{Conclusions}
\vspace{-0.15cm}
We proposed a stochastic gradient descent algorithm for solving linear inverse problems in 
 $\lpn(\mathbb{R})$. After recalling its deterministic counterpart and the difficulties encountered due to the non-separability of the underlying norm, a modular-based stochastic algorithm enjoying fast scalability properties is proposed. Numerical results show improved performance in comparison to standard $\ell^2(\mathbb{R})$ and $\ell^p(\mathbb{R})$-based algorithms and significant computational gains. Future work should adapt the convergence result (Theorem \ref{alg_modular_sgd}) to this setting and consider proximal extensions for incorporating non-smooth regularisation terms.

 \vspace{-0.25cm}
 {\small \section{Acknowledgements}
 \vspace{-0.15cm}
CE and ML acknowledge the support of the Italian INdAM group on scientific calculus GNCS. LC acknowledges the support received by the ANR projects TASKABILE (ANR-22-CE48-0010) and MICROBLIND (ANR-21-CE48-0008), the H2020 RISE projects NoMADS (GA. 777826) and the GdR ISIS project SPLIN. ZK acknowledges support from EPSRC grants EP/T000864/1 and EP/X010740/1.
}
\vspace{-0.2cm}
\bibliographystyle{plain}
\bibliography{ref.bib}
\end{document}


% $$\xkk=\xk-\mu_{k+1}A^*_i(A_i\xk-y_i)$$

% $$\xkk=\JpXstar\Big(\JpX(\xk)-\mu_{k+1}A^*_i\JqYi(A_i\xk-y_i)\Big)$$


\subsection{Stochastic gradient descent in Banach spaces}
\begin{align}
    & \argmin_{x\in \mathcal{X}} \frac{1}{q}\|Ax-y\|_\mathcal{Y}^q =  \argmin_{x\in \mathcal{X}} \sum_{i=1}^N \frac{1}{q}\|A_i x-y_i\|_{\mathcal{Y}_i}^q
\end{align}
\begin{algorithm}[H]
\caption{Gradient Descent Banach setting}
\label{alg_sgd_b}
\small{
\textbf{Parameters:} Sequence of stepsizes $\{\mu_k\}_k$ s.t. ???\\
\textbf{Initialization:} Start with $x^0\in\mathcal{X}$.\\
{
\textsc{FOR $k=0,1,\ldots$ REPEAT}}
\begin{itemize}
% \item[] Randomly select $i\in\{1,\cdots,N\}$
\item[] $\xkk=\JpXstar\Big(\JpX(\xk)-\mu_{k+1}A^*\JqY(A\xk-y)\Big)$
\end{itemize}
{\textsc{UNTIL}} convergence
}
\end{algorithm}
% \begin{align}
%     0 \in\JpX(\xkk-\xk)+\tau_k \nabla f(\xk)  
%     \quad  \Leftrightarrow \quad
%     \xkk \in\xk-\tau_k \JpXstar(\nabla f(\xk)),   \label{eq:primal_algo}
%     \end{align}
    
% \begin{equation}  \label{eq:dual_algo}
%     \JpX(\xkk) \in \JpX(\xk)-\tau_k \nabla f(\xk) \qquad
%     \xkk=\JpXstar\Big(\JpX(\xkk)\Big),
% \end{equation}
 