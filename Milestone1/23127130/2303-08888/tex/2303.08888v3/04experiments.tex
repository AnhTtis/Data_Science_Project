\section{Experiments}

In all our experiments, we set~$T=250$ and the collection of $\beta_t$ are set following the cosine schedule proposed in \cite{nichol2021improved}. We evaluate our method on two tasks described below.

\begin{table*}[ht]
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|cccccc|cccc}
\toprule
 & \multicolumn{6}{c}{\textbf{LIDCv1}} & \multicolumn{4}{c}{\textbf{LIDCv2}} \\
\textbf{Method} & GED$_{16}$ & GED$_{32}$ & GED$_{50}$ & GED$_{100}$ & HM-IoU$_{16}$ & HM-IoU$_{32}$ & GED$_{16}$ & GED$_{50}$ & GED$_{100}$ & HM-IoU$_{16}$\\
\hline
Prob. Unet~\cite{Kohl2018-hp} & 0.310\tiny{$\pm$0.01}$^-$ & 0.303\tiny{$\pm$0.01}$^+$ & - & 0.252\tiny{$\pm$0.004}$^\dagger$ & 0.552\tiny{$\pm$0.00}$^-$ & 0.548\tiny{$\pm$0.00}$^+$ & 0.320\tiny{$\pm$0.030}$^\ddagger$ & - & 0.252\tiny{$\pm$}$^\ddagger$ & 0.500\tiny{$\pm$0.030}$^\ddagger$\\
HProb. Unet~\cite{kohl2019hierarchical} & 0.270\tiny{$\pm$0.01}$^-$ & - & - & - & 0.530\tiny{$\pm$0.01}$^-$ & - & 0.270\tiny{$\pm$0.010}$^\ddagger$ & - & - & 0.530\tiny{$\pm$0.01}\\
PhiSeg~\cite{baumgartner2019phiseg} & 0.262\tiny{$\pm$0.00}$^-$ & 0.247\tiny{$\pm$0.00}$^+$ & - & 0.224\tiny{$\pm$0.004}$^\dagger$ & 0.586\tiny{$\pm$0.00}$^-$ & 0.595\tiny{$\pm$0.00}$^+$ & - & - & - & - \\
SSN~\cite{monteiro2020stochastic} & 0.259\tiny{$\pm$0.00}$^-$ & 0.243\tiny{$\pm$0.01}$^+$ & - & 0.225\tiny{$\pm$0.002} & 0.558\tiny{$\pm$0.00}$^-$ & 0.555\tiny{$\pm$0.01}$^+$ & - & - & - & - \\
cFlow~\cite{selvan2020uncertainty} & - & 0.225\tiny{$\pm$0.01}$^+$ & - & - & - & 0.584\tiny{$\pm$0.00}$^+$ & - & - & - & - \\
CAR~\cite{kassapis2021calibrated}  & - & - & - & 0.228\tiny{$\pm$0.009} & - & - & 0.264\tiny{$\pm$0.002} & \underline{0.248}\tiny{$\pm$0.004} & \underline{0.243}\tiny{$\pm$0.004} & \underline{0.592}\tiny{$\pm$0.005}\\
JProb. Unet~\cite{zhang2022a-probabilistic} & - & \underline{0.206}\tiny{$\pm$0.00} & - & - & - & \textbf{0.647}\tiny{$\pm$0.01} & 0.262\tiny{$\pm$0.00} & - & - & 0.585\tiny{$\pm$0.00}\\
PixelSeg~\cite{zhang2022b-pixelseg} & 0.243\tiny{$\pm$0.01} & - & - & - & \underline{0.614}\tiny{$\pm$0.00} & - & \underline{0.260}\tiny{$\pm$0.00} & - & - & 0.587\tiny{$\pm$0.01}\\
MoSE~\cite{Gao2022-zt} & \underline{0.218}\tiny{$\pm$0.003} & - & \underline{0.195}\tiny{$\pm$0.002} & \underline{0.189}\tiny{$\pm$0.002} & \textbf{0.624}\tiny{$\pm$0.004} & - & - & - & - & - \\
\hline
CCDM \textit{(ours)} & \textbf{0.212}\tiny{$\pm$0.002} & \textbf{0.194}\tiny{$\pm$0.001} & \textbf{0.187}\tiny{$\pm$0.002} & \textbf{0.183}\tiny{$\pm$0.002} & \textbf{0.623}\tiny{$\pm$0.002} & \underline{0.631}\tiny{$\pm$0.002} & \textbf{0.239}\tiny{$\pm$0.003} & \textbf{0.216}\tiny{$\pm$0.003} & \textbf{0.210}\tiny{$\pm$0.003} & \textbf{0.598}\tiny{$\pm$0.001}\\

\bottomrule
\end{tabular}
%}  
}
\end{center}
\caption{Quantitative results on LIDCv1 and LIDCv2, with the methods ordered by year. \textbf{Bold} and \underline{underlined} indicate best and second best per column, respectively. Our results are over 3 seeds. For GED, lower is better; for HM-IoU, higher is better. All scores taken from their original papers, except ($^+$) from \cite{zhang2022a-probabilistic}, ($^-$) from \cite{zhang2022a-probabilistic}, ($^\dagger$) from \cite{monteiro2020stochastic}, $\ddagger$ from \cite{kassapis2021calibrated}.}
\label{tab:lidc}
\end{table*}


\subsection{Segmentation with multiple annotations}

\paragraph{Dataset} The Lung Image Database Consortium (LIDC)~\cite{lidc-armato2011lung} dataset consists of 1'018~three dimensional chest CT scans of patients with lung cancer. Lung nodules of each volume are annotated by four expert raters from a pool of~$12$, yielding large differences in annotations in some cases. We extract nodule-centered slices from the CT volumes and treat each slice as an independent image.

While LIDC is the standard benchmark of stochastic segmentation methods to date (\eg~\cite{baumgartner2019phiseg,Gao2022-zt,hu2019supervised,kassapis2021calibrated,Kohl2018-hp,kohl2019hierarchical,monteiro2020stochastic,selvan2020uncertainty,zhang2022a-probabilistic,zhang2022semantic}), experimental configurations (pre-processing, training/validation/test splits, metrics) vastly differ across the literature.
We conduct our experiments on the two most prominent LIDC splits and report results on both separately. The first, referred to as LIDCv1, is used in \cite{baumgartner2019phiseg,Gao2022-zt,monteiro2020stochastic,zhang2022a-probabilistic}. LIDCv1 comprises 15'096~slices, divided into training, validation, and testing sets with the ratio $60:20:20$. The second, LIDCv2, is used in~\cite{kassapis2021calibrated,Kohl2018-hp} and contains 12'816 images with the ratio $70:15:15$.

\paragraph{Metrics} We measure the performances with the Generalised Energy Distance (GED) and the Hungarian-Matched Intersection over Union (HM-IoU)~\cite{Gao2022-zt, kassapis2021calibrated, kohl2019hierarchical}. 
Both metrics measure the difference between the distributions of generated and ground-truth label maps. We denote the metrics computed with $n$~samples using a subscript, \ie,~GED\textsubscript{$n$} and HM-IoU\textsubscript{$n$}, and we set $n$ to common values found in the literature. Note that higher number of samples yield more precise estimates.

\paragraph{Baselines} We compare our approach to nine recent stochastic segmentation methods: probabilistic U-Net (Prob.~Unet)~\cite{Kohl2018-hp}, hierarchical probabilistic U-Net (HProb.~Unet)~\cite{kohl2019hierarchical}, PhiSeg~\cite{baumgartner2019phiseg}, stochastic segmentation network (SSN)~\cite{monteiro2020stochastic}, conditional normalizing flow (cFlow)~\cite{selvan2020uncertainty}, calibrated adversarial refinement (CAR)~\cite{kassapis2021calibrated}, joint probabilistic U-Net (JProb. Unet)~\cite{zhang2022a-probabilistic}, PixelSeg~\cite{zhang2022b-pixelseg}, and mixture of stochastic experts (MoSE)~\cite{Gao2022-zt}.

Following standard practice, we use random horizontal and vertical flipping and random rotations of $0^\circ$, $90^\circ$, $180^\circ$ and~$270^\circ$ for data augmentation. The input resolution was set to $128\times128$. We trained our method with the Adam optimizer~\cite{kingma2014adam} until convergence of the GED metric on the validation set, a polynomial learning rate scheduling starting from $1e^{-4}$ and ending with $1e^{-6}$, and batch size of~64. We applied Polyak averaging with $\alpha =  0.99995$.

\subsection{Segmentation with a single annotation}
\label{sec:single_annotation}

We also evaluate our method with Cityscapes, a classical segmentation dataset where each image is annotated with a single label map. It comprises 2'975~RGB images of urban scenes for training and 500~images for validation, with each image labeled using 19~possible classes.

We compare our approach to several established baselines using the validation set: DeepLabv3~\cite{DeepLabv3}, HRNet~\cite{HRNet}, and UPerNet~\cite{UPerNet}, with both ResNet~\cite{ResNet} and Swin~\cite{Swin} backbones. 

Besides our standard method, which performs image conditioning by concatenating the raw pixel values as channels of the input tensor, we also included in our comparison a variant CCDM-Dino which leverages pre-trained Dino-ViT features~\cite{Dino} as additional conditioning concatenated to intermediate feature maps of our model's encoder.

Experiments are conducted separately for two different image resolutions: $128\times256$ and $256\times512$. For all reported methods, we first resize the images to a fixed resolution and then apply color jittering, random flipping, and standard ImageNet intensity normalization as data augmentation. %\LZ{don't see the latter in the tables?}. 
All baselines are trained for $500$~epochs with a batch size of~$32$, with optimizers, learning rate schedules, and weight decay settings as reported in their respective publications (reported in detail in the supplementary material). 

Our method was trained for $800$~epochs with a batch size of~$32$ at~$128\times256$ and of $16$ at~$256\times512$, using the Adam optimizer~\cite{kingma2014adam} with a learning rate of $1e^{-4}$ linearly decayed to $1e^{-6}$. We applied Polyak averaging with~$\alpha=0.999$.

Performance is measured with the mean intersection-over-union~(mIoU). Unlike GED and HM-IoU, the metric mIoU is incompatible with multiple label maps per image. During inference, CCDM generates multiple label maps per image that are subsequently fused into a single label map for performance assessment. We found that fusing by averaging the predicted probabilities resulted in superior performances compared to fusing by majority vote.

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/quali/lidc_quali.pdf}
    \caption{Qualitative results on four LIDC images with our method. (a) shows the image, (b)-(e) its four labels, (f) the mean prediction of our CCDM over six predictions, and (g)-(l) six individual predictions.}
    \label{fig:quali_lidc}
\end{figure*}

\section{Results}
\label{sec:results}

\subsection{LIDC}
We report performances on LIDCv1 and LIDCv2 in Tab.~\ref{tab:lidc} and qualitative results in Fig.~\ref{fig:quali_lidc}. Due to the lack of consistent evaluation protocols, we use a total of 10 metrics, thereby covering all the baselines and allowing for direct comparisons.

Our CCDM reaches the best performance for eight out of the ten metrics, despite its relatively small size, with 9~million parameters compared to, \eg,~the 42~million parameters of MoSE. On HM-IoU\textsubscript{16}, the CCDM has a lower mean performance than MoSE by 0.001, but with only half the standard deviation. The JProb.~Unet reaches a higher HM-IoU\textsubscript{32} than other all methods, despite being considerably worse for GED\textsubscript{32} than our~CCDM. Furthermore, on LIDCv2, the JProb.~Unet achieves only the third-best score on GED\textsubscript{16}, and fourth-best on HM-IoU\textsubscript{16}. This result indicates how comparing results obtained on different LIDC versions with each other can be misleading.

Fig.~\ref{fig:quali_lidc} presents qualitative results from our method. In columns (g)-(l), we see that our CCDM generates a distribution of samples that captures the annotation variability created by the four expert raters. Further, as seen in the bottom example, the CCDM also generates empty samples according to the annotations (b)-(e).

\noindent \textbf{Reduced number of time steps for sampling:} During inference, traversing the $T$~steps of the reverse process makes sampling from DDPMs slow. A straightforward solution~\cite{nichol2021improved} involves traversing only a subset of nodes of the reverse process, $\{\x_{k\tau}:\tau\in\{0, \ldots, T/k\}\}$, reducing the number of steps by a factor~$k$. This technique accelerates inference at the expense of reduced performance. To illustrate the trade-offs between performance and speed, Fig.~\ref{fig:sampling_steps} presents the evolution of GED$_{16}$ and HM-IoU$_{16}$ as the number of inference steps is reduced. As expected, CCDMs perform best when the number of training and inference steps are equal, but a reasonable increase in speed without a large sacrifice in performance is possible.

\subsection{Cityscapes}

\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{figures/fig_cts_qualitative.png}
\caption[]{Qualitative comparisons on Cityscapes. All methods are trained and tested at a resolution of $256\times512$. Our method produces structures with greater visual realism than other baselines. This is especially noticeable inside the marked regions.
}
\label{fig:quali_cts}
\end{figure*}

\begin{table}[ht]
% \centering
\resizebox{80mm}{!}{
\begin{tabular}{*5c}
    \toprule
    \multicolumn{3}{c}{\textbf{Method}}             & \multicolumn{2}{c}{mIoU final (best)} \\
    \cmidrule(r){1-3}                                 \cmidrule(l){4-5}         
    Architecture       & \mc{Backbone}   & \#params     & \mc{$128\times256$}    &  \mc{$256\times512$}\\
    \midrule
    \mc{\dv} \cite{DeepLabv3}      & \mc{ResNet50}\ (\tick)                       & 39m                  & 43.4 (44.1)     &   58.6 (59.2)       \\ 
    \mc{\dv} \cite{DeepLabv3}     & \mc{ResNet101} (\tick)                       & 58m                  & 43.8 (45.5)     &   59.2 (59.8)    \\ 
    \mc{\upr} \cite{UPerNet}     & \mc{ResNet101} (\tick)                     & 83m                 & 45.5 (47.1)      &  60.7 (61.2)     \\ 
    \mc{\hrn} \cite{HRNet}     & \mc{w48v2}  (\tick)                    & 70m                 & 48.2 (49.5)      &  63.3 (64.2)         \\ 
    \mc{\upr} \cite{Swin}      & \mc{Swin-Tiny} (\tick)                    & 58m                 & 54.2 (55.9)      &   \underline{65.5} (66.0)    \\ 

    \midrule
    \mc{CCDM} (ours) & \mc{-}               &  \quad          &  \quad        & \quad   \\  
    
    samples=1 & \mc{}               & 30m           &  53.2         & 60.3     \\  
    samples=5 & \mc{}                & 30m            &  55.4           & 62.0   \\  
    samples=10 & \mc{}               & 30m            & 56.2          &  62.4    \\  
  
    \midrule

    \mc{CCDM (ours)} & \mc{Dino ViT-S} ($\dagger$)             & \quad           &  \quad        & \quad   \\  
    samples=1 & \mc{}               & 30m + \textcolor{gray}{20M}           &  55.5         &  64.0     \\  
    samples=5 & \mc{}               & 30m + \textcolor{gray}{20M}           & \underline{56.9}           & 65.4   \\  
    samples=10 & \mc{}              & 30m + \textcolor{gray}{20M}           & \textbf{57.3}          &  \textbf{65.8}    \\  
    \bottomrule
    
\end{tabular}}

\caption{Results on Cityscapes-val for resolutions $128\times256$ and $256\times512$. \textbf{Bold} and \underline{underlined} indicate best and second best per column, respectively. (\tick) and ($\dagger$) indicate supervised and self-supervised pretraining of the backbone, respectively. \textcolor{gray}{Gray} indicates pretrained, non-finetuned parameters. We report final performance for our method and baselines. For the latter we also provide best achieved performance during training (in parenthesis). For CCDM methods, the field \emph{samples} indicates the number of generated samples for label map fusion, as explained in Sect~\ref{sec:single_annotation}.} 
\label{tab:cts}
\end{table}

\begin{table}[ht]
\centering

\resizebox{60mm}{!}{
  \begin{tabular}{*5c}
    \toprule
    \multicolumn{2}{c}{CCDM Capacity}             &       \multicolumn{3}{c}{mIoU}($128\times256$)   \\
    \cmidrule(r){1-2}                       \cmidrule(l){3-5}   
    \#params  & UNet Levels         & samples=1 & samples=5 & samples=10\\
    \midrule
    5.4M & 4          &  37.8 & 39.7 & 40.6  \\    
    7.5M & 5       &  44.7 & 48.3 & 48.5          \\
    22M  & 4       & 51.6 & 54.0 & 53.6 \\
    30M  & 5        &  \textbf{53.2} & \textbf{55.4} & \textbf{56.2}   \\    
    \bottomrule
\end{tabular}
}
% }
\caption{Effect of increasing CCDM capacity (without feature conditioning). (\tick) and ($\dagger$) indicate supervised and self-supervised pretraining of the backbone, respectively.}
\label{tab:cts_ablations}


\end{table}

Experimental comparisons on Cityscapes are presented in Tab.~\ref{tab:cts}, and qualitative examples are provided in Fig.~\ref{fig:quali_cts}. Experiments at $128\times256$ demonstrate that CCDM-Dino outperforms all other methods, even when only a single sample is used. CCDM-raw also remains competitive, being outperformed only by one baseline (UPerNet+Swin-Tiny), despite using only between $36\%$ and $51\%$~of the parameters of other models. Similarly, at $256\times512$, CCDM-Dino outperforms four of the baselines with a single sample, lags behind UPerNet+Swin-Tiny only by $0.1$~percent points with $5$~samples, and outperforms all baselines with $10$~samples. As expected, averaging across more samples improves performance for both CCDM-raw and CCDM-Dino, albeit with diminishing gains. Furthermore, the addition of Dino features boosts single-sample performance by $2.3$~percent points at $128\times256$, and $3.7$~percent points at $256\times512$, hinting the greater value of adding feature conditioning for generating segmentation at a higher resolution. 

\noindent \textbf{CCDM Capacity}: Tab.~\ref{tab:cts_ablations}(b) demonstrates the effect of increasing the capacity of CCDM. Using more U-Net feature levels, and increasing the number of parameters by doubling the number of channels per level, increases the performance regardless of the number of samples used for inference.


\begin{figure}
\centering
\includegraphics[width=0.3\textwidth]{figures/figure_sampling_steps_v7.pdf}
\caption[]{LIDC GED and HM-IoU versus the number of sampling steps on LIDC. Evaluated on 500 random test images using 16 samples each, over 3 seeds.}
\label{fig:sampling_steps}
\end{figure}