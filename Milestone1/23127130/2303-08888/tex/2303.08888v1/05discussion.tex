\section{Conclusion} 

We introduced conditional categorical diffusion models (CCDMs) that are capable of effectively modeling pixel-level semantic distributions. Notably, and contrary to standard deterministic segmentation approaches, our model can produce diverse samples given an input image, thereby capturing the aleatoric uncertainty. Our method learns a multi-modal label distribution of segmentations, induced by annotations from multiple expert raters, for which it achieves state-of-the-art results on a challenging medical imaging dataset, LIDC. Additionally, we demonstrate that it can achieve competitive performance on a standard multi-class semantic segmentation benchmark, Cityscapes, by outperforming several established, heavily engineered baselines despite using significantly fewer parameters. 

One limitation of our method is the requirement of several iterations for producing a sample, which is a common shortcoming of diffusion models. Accelerating sampling constitutes a crucial research direction, orthogonal to the present work. 
Finally, resolution scaling remains notoriously difficult for diffusion models, with successful examples relying on massive computational resources to train cascades of models that gradually increase resolution~\cite{cascaded_DDPM,Imagen} or operate on the latent space of existing embedding methods for continuous data (\eg~images) \cite{LatentDiffusionModels} that are not available for categorical data.
