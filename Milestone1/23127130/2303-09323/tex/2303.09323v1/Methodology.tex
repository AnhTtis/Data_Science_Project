We propose a 2D convolutional encoder for extracting hierarchical latent representations from raw daily stock prices. Our approach is designed to capture both short-term patterns and long-term changes in the data, which are essential for accurate prediction. To accomplish this, we make use of multiple time frames inputs and parallel encoders. The parallel encoders allow us to properly encode each time frame based on its specific number. Having different encoder, or parallel encoders is the popular choice for video next-frame prediction models. Our encoder employs Atrous convolutions blocks in multiple stages to downsample the input frames, and the decoder gradually upsamples the concatenated same-stage decoded data for each frame. The decoder is essential for preserving the diligently encoded data and for predicting a fine segmentation map. It is designed to be highly effective, ensuring that the encoded information is not lost and that the final segmentation map is accurate. 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{new_net.png}
     \caption{Propose framework. Green stacks represent the ASPP blocks. The N streams of multi-scale features are concatenated to form three scales of feature stacks }
     \label{net}
\end{figure}


\subsection{Atrous Spatial Pyramid Pooling}

Atrous convolution is a generalization of standard convolution which allows for explicit control of feature resolution and filter field-of-view in deep convolutional neural networks, enabling the capture of multi-scale information. For two-dimensional feature map $x$ and a filter $f$, each location $i$ on the output $y$ can be defined as:
\begin{equation}
y[i] = \sum_{j} x[i + r \times j] f[j] 
\end{equation}

where $r$ is the dilation or atrous rate. In another word, the dilation rate determines the sampling rate of the input signal, which is similar to convolving the input x with filters that have been upsampled by adding $r - 1$ zeros between each filter value in each dimension. Regular convolution is equivalent to atrous convolution when the dilation rate is set to 1, and adjusting the filter's field of view is achieved by increasing the rate value. Fig. \ref{atrous} illustrates atrous convolutions with different dilation rates.


We employ Atrous Spatial Pyramid Pooling as encoder building blocks. The ASPP, which was introduced in \cite{deeplab}, employs the use of four parallel atrous convolutions with different rates on the feature map. This method builds upon the success of spatial pyramid pooling \cite{spp} by resampling features at various scales to classify regions of arbitrary scale accurately and efficiently. ASPP with its various atrous rates effectively captures multi-scale information, however, as the sampling rate increases the number of valid filter weights decreases \cite{deeplabv3}. Hence, we set the maximum rate to 3, creating a pyramid of 3 feature stacks with dilation rates of 1,2, and 3.


\begin{figure}
%\centering
\includegraphics[width=.45\textwidth]{Atrous.png}\hfill
\caption{Atrous convolutions with 3x3 kernel and 9x9 feature map}
\label{atrous}
\end{figure}



\subsection{Parallel Multi-Scale Dense Feature Extraction}
Let $F = \{f_1, f_2, â€¦, f_N\}$ denote a sequence of $N$ consecutive frames, $f_i \in \mathbb{R}^{T \times 4}$, containing daily prices for T days each. We learn hierarchical latent representation for $f_i$s separately and in parallel. Due to the complicated nature of stock market data, boosting feature extraction power is imperative. Thus, in each stage, we employ ASPP blocks which we will investigate shortly. After each ASPP block the height and maps of the feature stacks are reduced to half their size, resulting in three scales of the feature map. Fig. \ref{net} illustrates the network architecture.\


Conceptually, filters from the lower layers capture primitive local context information while higher-level blocks capture more complicated relationships and coarse features. Such a bottom-up architecture models multi-variate time series data by hierarchical embedding in coarse 3D feature stacks. After encoding these $f_i$s in parallel, we apply the convolution operation to the concatenated feature maps formed by concatenating same-scale features stacks from all $f_i$s, resulting in feature maps $h_1, h_2, h_3$.




\subsection{Transposed Convolutions and Skip Architecture}

Inspired from \cite{unet}, we concatenate the same-scale encoded features with upsampled feature stacks in each decoder stage. This approach helps to prevent loss of information by combining the deep coarse features with the same-scale features from the encoder. In more detail, the deep coarse feature stacks are upsampled using transposed convolutions with strides of 2. The resulting feature stack is then concatenated with the same-scale feature stack from the encoder, followed by another transposed convolution with a stride of 2. This process is repeated again to produce the full-resolution feature maps, and finally, a 1 x 1 standard convolution with sigmoid activation is applied to acquire the final segmentation mask. This approach has been proven to be highly effective in preserving important features and improving the overall performance of the model.



