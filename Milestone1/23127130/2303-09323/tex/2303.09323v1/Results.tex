In this section, we comprehensively evaluate the performance of multiple semantic segmentation models and our proposed framework for predicting stock trends. Furthermore, we analyze the influence of the size of the input frames and the number of frames used on the accuracy of the predictions for various output time horizons. We utilized daily price time-series for five companies from 1970-01-02 to 2017-11-10, resulting in 600 and 300 frames for T of 20 and 40, respectively.


Each stock was trained and tested separately, with a dataset division of 65\%, 10\%, and 25\% for training, validation, and testing. All models were implemented in Keras and trained on T4 GPU with 16GB of memory, utilizing binary cross-entropy as the loss function and Adam optimizer. 


\subsection{Semantic Segmentation Models}
In this study, we evaluated the performance of our semantic segmentation model by changing the input-to-output ratio. Initially, we started with an equal input and output frame size, which is an intuitive choice for semantic segmentation. We observed the changing pattern in performance over the whole output horizon and 4 prices. To further investigate this, we visualized the average accuracy of the segmentation maps per pixel. Fig. \ref{maps} shows the accuracy for each price and day, where network (a) takes a 20 x 4 frame as input, and network (b) takes a 40 x 4 frame as input but outputs a 20 x 4 prediction for the next 20 days.

In (a), Although the results were promising for the first few days, with an accuracy of 84.03\% for the first day, it gradually declined to about 52 \% for the final days. However, when we increased the input-to-output ratio to 2, the accuracy improved significantly. On the first day, the accuracy was 54\%, but it quickly increased to 69\% on the second day, reaching its peak at 88\% on the 10th day, before declining to 72\% on the last day. This increase in accuracy can be attributed to the network's ability to capture the "zoomed-out" perspective of the 20-day change patterns with the increased length of input frames. Both models showed impressive total accuracy, but the model with an input-to-output ratio of 2 exhibited the most consistent accuracy across all days and higher maximum accuracy.

Additionally, we compared our results to other semantic segmentation networks in the field. To the best of our knowledge, the application of these models for financial time-series classification has not been explored previously. In an attempt to enhance the network's ability to capture patterns in each frame, we experimented with inputting trends instead of prices. The input trends frames were created similar to the labels and were used instead of the prices frames. We compared the performance of 6 different models, including FCN, SegNet, U-Net, DeepLab V3+, and two proposed models with 20 and 40 days input frames respectively. The results of this comparison are shown in Table \ref{sem}. The table summarizes the AUC and Accuracy of each model when using both price and trend frames as input. 

When analyzing the results for price frames, the proposed-40 model stands out with the highest AUC of 0.88 and accuracy of 78.18\%. The U-Net and FCN models follow closely with AUCs of 0.80 and 0.79 and accuracy scores of 70.77\% and 70.25\%, respectively. The proposed-20 model and DeepLab V3+ also performed well, achieving AUCs of 0.78 and 0.77 and accuracy scores of 68.21\% and 70.32\%, respectively. On the other hand, SegNet had the weakest performance with an AUC of 0.58 and an accuracy of 51.56\%.
For trend input, the FCN model demonstrated the best results with an AUC of 0.77 and an accuracy of 70.77\%. The U-Net model was close behind with an AUC of 0.76 and an accuracy of 69.29\%. The proposed-20 and proposed-40 models still performed relatively well, with AUCs of 0.72 and 0.71 and accuracy scores of 62.82\% and 65.28\%, respectively. The decrease in performance when switching to trend input was minor for FCN and U-Net, likely due to their ability to incorporate similar features from the early encoding stages into the final layers, making learning more effective when input and output are similar.


\begin{figure}
    \centering
    \subfloat[\centering 20 days frame input and 20 days frame output]{{\includegraphics[scale=0.45]{20to20_preds.png} }}%
    \qquad
    \subfloat[\centering 40 days frame input and 20 days frame output]{{\includegraphics[scale=0.45]{40to20_preds.png} }}%
    \caption{Accuracy maps for networks with different input frame sizes.}%
    \label{maps}%
\end{figure}


\begin{table}
    \centering
    \caption{Comparison with other semantic segmentation encoder-decoder architectures}
    \begin{tabular}{l*{4}{c}}
        \toprule
        \multirow{2}{*}{} & \multicolumn{2}{c}  {Prices} &  \multicolumn{2}{c}  {Trends}\\

          Model         &  AUC &  Accuracy    & AUC &  Accuracy\\
        \midrule
         FCN & 0.79 & 70.25\% & 0.77 & 70.77\%  \\ 
         SegNet &      0.58 & 51.56\% & 0.52 & 52.23\% \\ 
         U-Net &       0.80 & 70.77\% & 0.76 & 69.29\% \\
         DeepLab V3+ & 0.77 & 70.32\% & 0.67 & 57.16\% \\ 
         Proposed-20 & 0.78 & 68.21\% & 0.72 & 62.82\% \\ 
         Proposed-40 & 0.88 & 78.18\% & 0.71 & 65.28\% \\ 
         
        \bottomrule  
    \end{tabular}
\label{sem}
\end{table}


\subsection{Input Time Horizon}
We experimented with changing the number of input frames to see the changes in evaluation metrics when the input time horizon increases. We systematically varied the number of input frames from 1 to 11, and used 40-days input frames. Our findings suggest that with only 1 input frame, the network's behavior follows the typical pattern observed in CNN models, exhibiting better results for short-term predictions. As the number of input frames increases, the longer-term results improve, a phenomenon reminiscent of the behavior of recursive networks.

The results reveal that the best overall model is the one with 9 input frames (corresponding to 360 days), achieving an AUC of 0.88 and an accuracy of 88\% in total. For first-day predictions, models with 1-2 input frames perform the best, with a rapid decline in accuracy and AUC as the number of input frames increases. For day 5, models with 2 frames input perform the best, with the 9-frame model also exhibiting an impressive AUC of 90\% and the fastest growth among long-term input models. For day 10, the 9-frame model reaches the highest AUC and accuracy among all models and days in the table, with an AUC of 0.95, an accuracy of 88\%, and an F1 score of 0.89. For day 20, the best overall metrics are achieved by the 7-frame input model, with an accuracy of 77\% and an F1 score of 0.78. The 9-frame input model, however, results in a higher AUC of 0.85.

In conclusion, while among the models with a higher number of input frames, the 9-frame model performs the best overall, among the models with a lower number of input frames, the 2-frame model performs the best, although the single-frame model has close results. Eventually, this shows that itâ€™s possible to predict the next few days' trends with very high accuracy using semantic segmentation with only the last 1-2 trading months input.  It also confirms that short-term relationships are decent predictors for the next 1-2 days' trends. With an increasing number of input frames, the maximum accuracy increases in value and moves to further days. Except for 1 and 2 frames models, for day 1, the other models do not perform well. This probably is due to incorporating long-term information for the 1-2 days, which is not much of use for those cases. However, as can be seen in Figure \ref{maps} (b) it improves really fast on days 2-3 for the 9-frame version.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{table*}[] \centering
%\ra{1.3}
\begin{small}
\begin{tabular}{@{}lrrrrrrrrrrrr@{}}\toprule
No. of Frames & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9}& \textbf{10}  & \textbf{11}\\ \midrule
 %\textbf{Total} &  &  & &  &  &  &  &  & \\ \midrule

\textbf{Day 1} &  &  & &  &  &  &  &  & \\ \midrule
\textbf{AUC}	 & 	\textbf{0.90}	 & 	0.88	 & 	0.66	 & 	0.51	 & 	0.59	 & 	0.46	 & 	0.57	 & 	0.39	 & 	0.54	 & 	0.49	 & 	0.57\\    \hdashline
\textbf{Acc. (\%)}	 & 	74.17	 & 	\textbf{75.02}	 & 	56.25	 & 	52.50	 & 	53.75	 & 	49.15	 & 	48.73	 & 	45.76	 & 	53.81	 & 	48.73	 & 	60.34\\    \hdashline
\textbf{Precision}	 & 	0.68	 & 	\textbf{0.69}	 & 	0.53	 & 	0.51	 & 	0.52	 & 	0.50	 & 	0.49	 & 	0.47	 & 	0.53	 & 	0.49	 & 	0.57\\    \hdashline
\textbf{Recall}	 & 	0.92	 & 	0.91	 & 	\textbf{0.98}	 & 	0.86	 & 	0.83	 & 	0.50	 & 	0.70	 & 	0.61	 & 	0.86	 & 	0.73	 & 	0.83\\    \hdashline
\textbf{f1}    &		\textbf{0.78}    &	\textbf{0.78}    &	0.69    &	0.64    &	0.64    &	0.50    &	0.58    &	0.53    &	0.65    &	0.59    &	0.68\\

\midrule


\textbf{Day 5} &  &  & &  &  &  &  &  & \\ \midrule
\textbf{AUC}	 & 	0.90	 & 	\textbf{0.92}	 & 	0.73	 & 	0.75	 & 	0.73	 & 	0.80	 & 	0.88	 & 	0.85	 & 	\textbf{0.92}	 & 	0.83	 & 	0.89\\    \hdashline
\textbf{Acc. (\%)}	 & 	82.08	 & 	\textbf{87.50}	 & 	62.50	 & 	65.00	 & 	72.50	 & 	76.69	 & 	84.75	 & 	79.66	 & 	83.47	 & 	72.46	 & 	81.03\\    \hdashline
\textbf{Precision}	 & 	0.83	 & 	\textbf{0.89}	 & 	0.60	 & 	0.63	 & 	0.71	 & 	0.83	 & 	0.82	 & 	0.85	 & 	0.86	 & 	0.72	 & 	0.78\\    \hdashline
\textbf{Recall}	 & 	0.83	 & 	0.87	 & 	0.89	 & 	0.81	 & 	0.83	 & 	0.69	 & 	\textbf{0.90}	 & 	0.74	 & 	0.81	 & 	0.77	 & 	\textbf{0.90}\\    \hdashline

\textbf{f1}    &	0.83    &	\textbf{0.88}    &	0.72    &	0.71    &	0.76    &	0.76    &	0.86    &	0.79    &	0.84    &	0.75    &	0.84 \\
         
\midrule



\textbf{Day 10} &  &  & &  &  &  &  &  & \\ \midrule
\textbf{AUC}	 & 	0.89	 & 	0.84	 & 	0.87	 & 	0.91	 & 	0.90	 & 	0.94	 & 	0.93	 & 	0.94	 & 	\textbf{0.95}	 & 	0.94	 & 	0.92\\    \hdashline
\textbf{Acc. (\%)}	 & 	82.50	 & 	73.33	 & 	77.08	 & 	85.42	 & 	87.08	 & 	80.08	 & 	85.17	 & 	86.86	 & 	\textbf{88.14}	 & 	87.71	 & 	83.19\\    \hdashline
\textbf{Precision}	 & 	0.81	 & 	0.79	 & 	0.73	 & 	0.86	 & 	\textbf{0.89}	 & 	0.79	 & 	0.86	 & 	\textbf{0.89}	 & 	\textbf{0.89}	 & 	0.88	 & 	0.82\\    \hdashline
\textbf{Recall}	 & 	0.87	 & 	0.69	 & 	\textbf{0.92}	 & 	0.87	 & 	0.87	 & 	0.87	 & 	0.87	 & 	0.87	 & 	0.89	 & 	0.90	 & 	0.89\\   \hdashline
\textbf{f1}	    &	0.84	    &	0.74    &		0.81    &		0.87    &		0.88    &		0.82    &		0.86    &		0.88    &		\textbf{0.89}    &		\textbf{0.89}    &		0.85 \\
\midrule


\textbf{Day 20} &  &  & &  &  &  &  &  & \\ \midrule
\textbf{AUC}	 & 	0.71	 & 	0.77	 & 	0.81	 & 	0.80	 & 	0.83	 & 	0.84	 & 	0.84	 & 	0.84	 & 	\textbf{0.85}	 & 	0.83	 & 	0.80\\    \hdashline
\textbf{Acc. (\%)}	 & 	57.08	 &	65.41	 &	64.17	 &	65.83	 &	75.00	 &	75.42	 &	\textbf{77.54}	 &	74.58	 &	72.03	 &	73.73	 &	69.83\\    \hdashline
\textbf{Precision}	 & 	0.54	 & 	0.61	 & 	0.59	 & 	0.63	 & 	0.75	 & 	0.71	 & 	\textbf{0.76}	 & 	0.73	 & 	0.68	 & 	0.69	 & 	0.66\\    \hdashline
\textbf{Recall}	 & 	\textbf{0.95}	 & 	0.89	 & 	0.93	 & 	0.82	 & 	0.76	 & 	0.85	 & 	0.81	 & 	0.77	 & 	0.85	 & 	0.86	 & 	0.83\\ \hdashline
\textbf{f1}	    & 	0.69	& 0.72	& 0.73   & 	0.71   &	0.76	 &  \textbf{0.78}	&  \textbf{0.78}  &	0.75 & 	0.75 &	0.77  &	0.74\\

\toprule


\textbf{Total} &  &  & &  &  &  &  &  & \\ \midrule
\textbf{AUC}	 & 	0.85	 & 	0.84	 & 	0.80	 & 	0.81	 & 	0.81	 & 	0.82	 & 	0.85	 & 	0.85	 & 	\textbf{0.88}	 & 	0.86	 & 	0.84\\    \hdashline
\textbf{Acc. (\%)}	 & 	73.87	 & 	74.90	 & 	67.98	 & 	72.65	 & 	74.63	 & 	74.85	 & 	77.61	 & 	77.06	 & 	\textbf{78.18}	 & 	76.02	 & 	75.13\\    \hdashline
\textbf{Precision}	 & 	0.69	 & 	0.74	 & 	0.64	 & 	0.70	 & 	0.74	 & 	0.75	 & 	0.76	 & 	\textbf{0.78}	 & 	0.76	 & 	0.74	 & 	0.72\\    \hdashline
\textbf{Recall}	 & 	0.88	 & 	0.81	 & 	0.90	 & 	0.82	 & 	0.80	 & 	0.77	 & 	0.82	 & 	0.76	 & 	\textbf{0.85}	 & 	0.82	 & 	0.84\\    \hdashline

\textbf{f1}	    &	0.77    &	0.77    &	0.75    &	0.76    &	0.77    &	0.76    &	0.79    &	0.77    &	\textbf{0.80}    &	0.78    &	0.78 \\


\bottomrule
\end{tabular}
\end{small}
\caption{ Short-term to long-term. The effect of increasing the number of input frames on different days in the output frame. Days 1, 5, 10, and 20 are picked for comparison. }
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Deep Learning Stock Trend Prediction Models}
We compared our model with several models designed for stock trend prediction. We chose two models that use CNN as an encoder and then use fully connected and LSTM as decoder units to compare our model with, in conjunction with a Multi-Layer Perceptron model. We used 40-day input (as in Fig. \ref{net}) with 1-frame and 9-frame inputs and changed the prediction horizons of the baseline models. For next-day prediction, among the baselines, Conv-LSTM performs the best, with 63.41\% accuracy. Our proposed 1-frame network predictably performs the best with 75\% accuracy. Our 9-frame network performs similarly to other baselines, with only about 1\% increase in accuracy. 

For the next 4 days' prediction, both versions of the proposed networks have similar results with about 85\% and 0.91 in accuracy and AUC.  While the performance of the Conv-LSTM model decreased to 61.73\%, the other two, interestingly the ones with fully connected layers increased their accuracy to 68.75\% and 55.46\%, for CNN-based one and DNN one respectively. For the next 20 days prediction, the 9-frame proposed network works the best, with 69.69\% and 0.83 in accuracy and AUC, respectively, and CNN+FC is next with 68.75\% and 0.61. Finally, DNN and proposed-1 come next with 60.92\% and 60.33\% in accuracy. As opposed to proposed-1, As the prediction horizon increases, the performance of the DNN model improves.



\begin{table}
\begin{tabular}{cccc}
\toprule
Time Interval  & Model & Acc.(\%) & AUC   \\ 
\midrule
\multicolumn{1}{c}{}           & \multicolumn{1}{c}{CNN+FC \cite{cnn}}    &     51.26    &    0.54       \\
\hdashline

\multicolumn{1}{c}{\relax}             & \multicolumn{1}{c}{CNN+LSTM \cite{convlstm}}     
&    63.41      &     0.49     \\
\hdashline
\multicolumn{1}{c}{1 Day}                               & \multicolumn{1}{c}{DNN \cite{dnn}}   &     52.94     &     0.53     \\ 
\hdashline
\multicolumn{1}{c}{\relax}                               & \multicolumn{1}{c}{Proposed-1}   
&     75.00     &     0.90      \\ 
\hdashline
\multicolumn{1}{c}{\relax}                               & \multicolumn{1}{c}{Proposed-9}   
&     54.24     &     0.54      \\ 
\midrule
\multicolumn{1}{c}{}   & \multicolumn{1}{c}{CNN+FC}     
&     68.75    &   0.64     \\
\hdashline
\multicolumn{1}{c}{}             & \multicolumn{1}{c}{CNN+LSTM}     
&    61.73      &     0.53     \\
\hdashline
\multicolumn{1}{c}{4 Days}                               & \multicolumn{1}{c}{DNN}
&     55.46     &     0.52     \\ 
\hdashline
\multicolumn{1}{c}{\relax}                               & \multicolumn{1}{c}{Proposed-1}   
&     85.01     &     0.92      \\
\hdashline
\multicolumn{1}{c}{\relax}                               & \multicolumn{1}{c}{Proposed-9}   
&     84.75     &     0.90      \\ 
\midrule
\multicolumn{1}{c}{}                & \multicolumn{1}{c}{CNN+FC}     
&     68.75    &    0.61      \\
\hdashline
\multicolumn{1}{c}{\relax}             & \multicolumn{1}{c}{CNN+LSTM}     
&    55.56      &     0.54      \\
\hdashline
\multicolumn{1}{c}{20 Days}                               & \multicolumn{1}{c}{DNN}   &     60.92     &     0.60      \\ 
\hdashline
\multicolumn{1}{c}{\relax}                               & \multicolumn{1}{c}{Proposed-1}   
&     60.33     &     0.74      \\ 
\hdashline
\multicolumn{1}{c}{\relax}                               & \multicolumn{1}{c}{Proposed-9}   
&     69.49     &     0.83     \\ 
\bottomrule
\end{tabular}
\caption{Comparison of the stock trend prediction of different models for different output time horizons}
\label{Fin}
\end{table}
