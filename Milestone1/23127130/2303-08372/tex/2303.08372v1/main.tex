% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------

\documentclass{article}
\usepackage{ieeetrantools}
\usepackage{cite} 
\usepackage{spconf,amsmath,graphicx}
\usepackage{tabularx}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage[labelfont=bf]{caption} % for the figure and Tables Caption (bold)
\usepackage{hyperref,url}
\usepackage{multirow}
\usepackage{booktabs}
%\usepackage{cite} 
\usepackage{amssymb} 


% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
%\title{Target Sound Extraction with Multi-modal Clues}
\title{Target Sound Extraction with Variable Cross-modality Clues}
%
% Single address.
% ---------------
%\name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
%\address{Author Affiliation(s)}
\name{
\begin{tabular}{c}
Chenda Li$^{1,2,\dag}$, Yao Qian$^2$, Zhuo Chen$^2$, Dongmei Wang$^2$, \\
Takuya Yoshioka$^2$, Shujie Liu$^2$, Yanmin Qian$^1$, Michael Zeng$^2$
\end{tabular}  \thanks{$^{\dag}$The first author conducted this work during internship at Microsoft.}}

\address{
  $^1$MoE Key Lab of Artificial Intelligence, AI Institute\\
$^1$X-LANCE Lab, Department of Computer Science and Engineering, Shanghai Jiao Tong University\\
  $^2$Microsoft, Redmond, WA, USA\\
  % {\small \{lichenda1996, yanminqian\}@sjtu.edu.cn,~\{yaoqian, zhuc, dongmei.wang, tayoshio, shujliu, nzeng\}@microsoft.com}
  }




\begin{document}
\bstctlcite{IEEEexample:BSTcontrol} % place this line right below \begin{document}
\ninept
%
\maketitle
%
\begin{abstract}

Automatic target sound extraction (TSE) is a machine learning approach to mimic the human auditory perception capability of attending to a sound source of interest from a mixture of sources. 
It often uses a model conditioned on a fixed form of target sound clues, such as a sound class label, which limits the ways in which users can interact with the model to specify the target sounds.
To leverage variable number of clues cross modalities available in the inference phase, including a video, a sound event class, and a text caption, we propose a unified transformer-based TSE model architecture, where a multi-clue attention module integrates all the clues across the modalities. Since there is no off-the-shelf benchmark to evaluate our proposed approach, we build a dataset \footnote{Python scripts to generate this dataset can be found at \url{https://github.com/LiChenda/Multi-clue-TSE-data}.} based on public corpora, Audioset and AudioCaps. Experimental results for seen and unseen target-sound evaluation sets show that our proposed TSE model can effectively deal with a varying number of clues which improves the TSE performance and robustness against partially compromised clues. 
\end{abstract}
%
\begin{keywords}
Target sound extraction, cross-modality attention, multi-clue processing
\end{keywords}
%
\input{tex/intro}
\input{tex/method}
\input{tex/exp}

\vspace{-0.8em}
\section{Conclusion}
\label{sec:conclusion}

In this work, we propose a multi-clue model for target sound extraction. The proposed model can freely combine different modal clues to extract target sounds. 
The experiments show that the proposed multi-clue TSE can leverage each single clue effectively to extract target sounds, achieving comparable performance with single-clue based systems. 
When combining clues from multiple modalities, the proposed model shows further improvements in both performance and robustness.
These advantages make the proposed TSE system strong, robust, and flexible in real applications. 
With these observations, in future work, we would like to investigate further integration with more clues and their interaction with each other during TSE process.


\bibliographystyle{IEEEtran}
\bibliography{refs}

\end{document}
