\section{Target Sound Extraction}
\label{sec:baseline}



The goal of target sound extraction (TSE) is to extract the sound of interest from an audio mixture given a set of one or more target clues.
Let $\mathbf{y}$ denote the input audio mixture consisting of $J$ sound sources $\mathbf{s}_{1}, \cdots, \mathbf{s}_{J}$, and suppose the $j$-th source to be the target sound. 
Then, the TSE mapping function can be formulated as
\begin{align}
		f_{TSE}(\mathbf{y}, \mathbf{c}_{j}) = \hat{\mathbf{s}}_{j} \rightarrow \mathbf{s}_{j}, 
\end{align}
where $\hat{\mathbf{s}}_{j}$ is the estimated target sound and $\mathbf{c}_j$ is a target sound representation from the provided clue set. 
One of the simplest forms of the clue set contains only one sound event tag represented as a one-hot vector \cite{kongSourceSeparationWeakly2020,liCategoryAdaptedSoundEvent2022}, and it is used as our baseline system. In our proposed multi-clue system, $\mathbf{c}_{j}$ is produced by a multi-clue processing net as described in Section \ref{sec:multi-clue-net}.

The backbone of our TSE system is based on a deep complex convolution recurrent network (DCCRN) \cite{huDCCRNDeepComplex2020}. It was originally proposed for speech enhancement using complex time-frequency spectra and also applied to target speech extraction~\cite{9746962}. As Fig. \ref{fig:fig1} (A) shows, the DCCRN consists of an encoder, an enhancement LSTM, and a decoder. The encoder and decoder consist of complex convolution layers and are connected with U-Net-like  skip-connections~\cite{ronnebergerUNetConvolutionalNetworks2015,stollerWaveUNetMultiScaleNeural2018}. The enhancement LSTM between the encoder and the decoder processes the sum of the complex deep encoded features and the clue embedding features as follows: 
\begin{align}
 \label{eq:dccrn_1}
 \mathbf{F}_{rr} &= \text{LSTM}_{r}(\mathbf{Y}_r + \mathbf{c}_{j}), \quad \mathbf{F}_{ir} = \text{LSTM}_{r}(\mathbf{Y}_i + \mathbf{c}_{j}),  \\
\label{eq:dccrn_2}
 \mathbf{F}_{ri} &= \text{LSTM}_{i}(\mathbf{Y}_r + \mathbf{c}_{j}), 
 \quad \mathbf{F}_{ii} = \text{LSTM}_{i}(\mathbf{Y}_i + \mathbf{c}_{j}), \\
  \mathbf{F}_{out} & = ( \mathbf{F}_{rr} -  \mathbf{F}_{ii}) + ( \mathbf{F}_{ri} +  \mathbf{F}_{ir})i, 
\end{align}
where $\mathbf{Y}_{r}, \mathbf{Y}_{i} \in \mathbb{R} ^{T \times D} $ are the real and imaginary parts of the complex deep features generated by the DCCRN encoder with $T$ and $D$ being the feature sequence length and dimension, respectively. $\mathbf{c}_j \in \mathbb{R} ^{T \times D} $ is the encoded clue of the target sound, which is mapped from the one-hot tag vector with a linear layer and tiled to length $T$. $\text{LSTM}_{r},\text{LSTM}_{i}$ are LSTMs for the real and imaginary features.  $\mathbf{F}_{out}$ is the complex features that are fed to the DCCRN decoder.
The DCCRN decoder maps $\mathbf{F}_{out}$ into the estimate target sound $\hat{\mathbf{s}}_{j}$. 

The model is trained to minimize the following loss function: 
\begin{align}
\label{eq:loss}
	\mathcal{L} &= \mathcal{L}_{snr} + \lambda \mathcal{L}_{L1}, \\
	\mathcal{L}_{snr} &= -10 \log_{10}{\left ( \frac{\|\mathbf{s}_{j}\|_2^{2}}{\|\mathbf{s}_j - \hat{\mathbf{s}}_j \|_2^{2} } \right )},  \\
	\mathcal{L}_{L1} &= \|\mathbf{S}_j - \hat{\mathbf{S}}_j \|_1,
\end{align}
where $\mathbf{S}_j$ and $\hat{\mathbf{S}}_j$ are the complex spectra of the target and estimated sources, respectively, and $\lambda$ was set at 5 in our experiments.



\section{Multi-clue Processing Net}
\label{sec:multi-clue-net}

Fig. \ref{fig:fig1} shows a diagram of the proposed multi-clue TSE model.
It consists of two modules: a DCCRN TSE backbone (Fig. \ref{fig:fig1} (A)) and a multi-clue processing net (Fig. \ref{fig:fig1} (B)), where the backbone model is the same as the one described in the previous section. 
The clue processing net takes a variable number of clues as input and generates a time-aligned fused clue, which is fed to DCCRN's LSTM block as $\mathbf{c}_j$ in Eqs. \eqref{eq:dccrn_1} and \eqref{eq:dccrn_2}.
To deal with clues from different modalities, the clue processing net uses different modal encoders, including a sound encoder, a text clue encoder, a video clue encoder, and a sound tag encoder, which transform the corresponding clue inputs to a unified $D$-dimensional space. 
On top of the modal encoders, we use an attention module \cite{vaswaniAttentionAllYou2017} for clue fusion.

\noindent \textbf{Sound encoder}: The sound encoder extracts $D$-dimensional sound embeddings from the input mixture signal.
% We intuitively believe it's better if the encoder can analyze what sound events are contained in the input mixture audio.
We adopt a pre-trained sound event detection (SED) model \cite{kongPANNsLargeScalePretrained2020} as the sound encoder to capture the sound event discriminated representation.
The original SED model is pre-trained for an audio classification task and outputs an SED probability mass over the audio classes for an entire input audio clip.
To keep dynamic time-dependent information, we use the frame-wise hidden embeddings obtained before the temporal aggregation layer of the SED model.
Then, we use a trainable modality projection net, to obtain a $D$-dimensional sound embedding sequence $\mathbf{Q} \in \mathbb{R}^{T_{a} \times D}$, where $T_{a}$ is the sound embedding sequence length.
The modality projection net comprises a layer-norm module and a fully connected layer with ReLU activation. Modality projection nets of this structure are also used for the text and video clues to map the text and video embeddings to the $D$-dimensional unified space. 

\noindent \textbf{Text clue encoder}: The text clue is a natural language description about the target sound. The function of the text encoder is to transform the target sound sentence into deep embeddings that can be used as the TSE clue.
% Natural language processing is a challenging task, and training a good text encoder from scratch with an end-to-end TSE loss is difficult. 
We adopt a pre-trained DistilBERT \cite{sanhDistilBERTDistilledVersion2019} as our text encoder, where DistilBERT is a self-supervised learning (SSL) model trained on large-scale text data.
%DistilBERT is a self-supervised learning (SSL) model on large-scale text.
Token-level hidden embeddings are extracted with the pre-trained DistilBERT and then mapped by a trainable modality projection net to obtain $D$-dimensional text clue embedding $\mathbf{O} \in \mathbb{R}^{T_{t} \times D}$ in the unified space, where $T_t$ is the number of word tokens in the sentence.

\noindent \textbf{Video clue encoder}: The video clue is based on a video clip related to the target sound. As with the text clue encoder, we use an SSL model based on Swin Transformer \cite{liuSwinTransformerHierarchical2021} that is pre-trained on large-scale image data as our video clue encoder. Each image frame in the video clip is processed by the pre-trained Swin Transformer, and its output is mapped into the unified embedding space with a learnable modality projection net. The video clue embedding sequence is denoted as $\mathbf{V} \in \mathbb{R}^{T_{v} \times D}$, where $T_{v}$ is the number of the image frames in the video clip.

\noindent \textbf{Sound tag encoder}: The sound tag encoder is a simple linear layer, it takes a one-hot sound event tag as input and outputs an embedding vector $\mathbf{E} \in \mathbb{R}^{1 \times D}$.

\noindent \textbf{Multi-Clue Attention}: The sound tag encoder fuses the embeddings from the text clue, video clue, and sound tag encoders to generate a clue sequence alinged with the embeddings from the sound encoder. 
We achieve this by using source-target attention \cite{vaswaniAttentionAllYou2017}, where the queries are extracted from the sound encoder output $\mathbf{Q}$ while the key-value pairs are obtained from the concatenated embeddings. 
%The source-target attention method \cite{vaswaniAttentionAllYou2017} can naturally deal with the alignment between the query and key-value vectors in two sequences. Each query vector can attend to all vectors in the key-value sequence. And the output is computed as a weighted sum of the values.
Specifically, we concatenate the embedded clues \footnote{When one of the clues, for example, the video clue is missing, the video encoder obtains no input and outputs nothing. Then, Eq. \ref{eq:cat} will be $	\mathbf{U} = \text{Concatenate}(\mathbf{O}; \mathbf{E}) \in \mathbb{R}^{(T_{t} + 1) \times D}$.} of different modalities along the sequence dimension to obtain the concatenated multi-modal clue $\mathbf{U}$ in the unified embedding space, i.e., 
\begin{align}
\label{eq:cat}
	\mathbf{U} = \text{Concatenate}(\mathbf{O};\mathbf{V}; \mathbf{E}) \in \mathbb{R}^{(T_{t} + T_{v} + 1) \times D}.
\end{align} 

By using the sound embedding sequence, $\mathbf{Q}$, and $\mathbf{U}$ as the query and key-value pairs in the attention module respectively, we can get fused clue $\mathbf{C}_{u}$ as follows:
\begin{align}
	\mathbf{C}_{u} &= \text{MultiHeadAttention}(\mathbf{Q}, \mathbf{U},\mathbf{U}) \in \mathbb{R}^{T_{a} \times D}, 
 % \mathbf{C}_{u} &= [\text{head}_{1}, \cdots, \text{head}_{h}] \mathbf{W}_{0}  \in \mathbb{R}^{T_{a} \times D} , \\
 % \text{head}_{i} &= \text{softmax}\left( \frac{\mathbf{Q}\mathbf{W}^{Q}_{i}(\mathbf{U}\mathbf{W}^{K}_{i})^{T}}{\sqrt{D}} \mathbf{U}\mathbf{W}^{V}_{i} \right)
\end{align}
where $\text{MultiHeadAttention}(Q, K, V )$ represents a multi-head attention \cite{vaswaniAttentionAllYou2017} with query $Q$, key $K$, and value $V$.
% where $\mathbf{W}$ are all learnable parameter matrices in the multi-head attention \cite{vaswaniAttentionAllYou2017} . 
%Although feature lengths varies among clues, 
We can see that the fused clue $\mathbf{C}_{u}$ have the same length, $T_{a}$, as the sound embedding $\mathbf{Q}$. 
%At the same time, the alignment between the sound embedding and different clues can be implicitly learned.
To match the sequence length ($T_a$) to that of the DCCRN encoder $T$, $\mathbf{C}_{u}$ is up-sampled before being added to 
$\mathbf{Y}$ in Eqs. \eqref{eq:dccrn_1} and \eqref{eq:dccrn_2}.


