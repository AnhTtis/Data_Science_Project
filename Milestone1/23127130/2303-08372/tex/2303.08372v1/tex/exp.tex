\begin{table}[]
\caption{SNRi (dB) for  seen and unseen test sets. $\checkmark$: clue is available in inference stage.}
\label{tab:tab1}
\vspace{-.8em}
\centering
%    \setlength{\tabcolsep}{3mm}{
\begin{tabular}{l|ccc|cc}
\hline
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{Model}} & \multicolumn{3}{c|}{Clues used} & \multicolumn{1}{c}{\multirow{2}{*}{Seen}} & \multicolumn{1}{c}{\multirow{2}{*}{Unseen}} \\ \cline{2-4}
\multicolumn{1}{c|}{}                       &       tag      &     text      &      video     & \multicolumn{1}{c}{}                      \\ \hline
DCCRN-tag-clue                           &       $\checkmark$     &           &           &     6.4           & 6.0                           \\ 
%exp_tse_audio/enh_train_tses_dccrn_tag_raw_tag_onehot/enhanced_audiocaps_v2_test
\hline
DCCRN-text-clue                                &            &      $\checkmark$      &           &     6.3   & 5.9                                   \\ \hline
DCCRN-video-clue                              &            &           &        $\checkmark$    &     5.9        & 5.6                              \\ \hline
\multirow{7}{*}{DCCRN-multi-clue}         &     $\checkmark$        &     $\checkmark$       &     $\checkmark$       &  \textbf{6.9} & \textbf{6.5}  \\ \cline{2-6} 
                                            &      $\checkmark$       &   $\checkmark$   &           &  6.8  & 6.4  \\ \cline{2-6}
                                            &          &     $\checkmark$        &       $\checkmark$      &  6.5  & 6.4 \\ \cline{2-6}
                                            &      $\checkmark$       &           &      $\checkmark$        &  6.6  & 6.4  \\ \cline{2-6}

                                            &      $\checkmark$       &           &           &  6.4  & 6.2 \\ \cline{2-6}
                                            &            &       $\checkmark$     &           &   6.3  & 6.0 \\\cline{2-6}
                                            &            &           &     $\checkmark$       &   5.8 & 5.9  \\
                                            \hline
\hline                                      
\end{tabular}
%}
% \vspace{1em}
\end{table}
%
\begin{table}[]
\caption{Impacts of compromised clues on SNRi (dB). $*$: compromised clue.}
\vspace{-.8em}
\label{tab:tab2}
\centering
%    \setlength{\tabcolsep}{3mm}{
% \begin{tabular}{c|ccc|c}
% \hline
% \hline
% \multicolumn{1}{c|}{\multirow{2}{*}{Model}} & \multicolumn{3}{c|}{Correct/compromised clues} & \multicolumn{1}{c}{\multirow{2}{*}{SNRi}}  \\ \cline{2-4}
% \multicolumn{1}{c|}{}    & ~~~tag~~~ & ~text~ & video & \multicolumn{1}{c}{}                      \\ \hline
% \multirow{7}{*}{DCCRN (multi-clue)}    &     $\odot $      &  $\odot$    &   $\odot$    &  6.87  \\ \cline{2-5} 
% &     $\times$       &  $\odot$    &   $\odot $   &   5.58 \\ \cline{2-5} 
% &     $\odot$       &  $\times$    &   $\odot $   &  6.23   \\ \cline{2-5} 
% &     $\odot$       &  $\odot$    &   $\times $   &  6.64  \\ \cline{2-5} 
% &     $\times$       &  $\times$    &   $\odot $   &  5.34   \\ \cline{2-5} 
% &     $\times$       &  $\odot$    &   $\times $   &  5.68  \\ \cline{2-5} 
% &     $\odot$       &  $\times$    &   $\times $   &  6.50  \\ \cline{2-5} 
% \hline
% \hline                                      
% \end{tabular}

\begin{tabular}{ccc|cc|c}
\hline
\hline
 \multicolumn{5}{c|}{Correct/compromised clues} & \multicolumn{1}{c}{\multirow{2}{*}{SNRi}}  \\ \cline{1-5}
& text~~~  & ~~text$*$~~ &  ~video~ & ~video$*$~ & \multicolumn{1}{c}{}                      \\ \hline
  % $\checkmark$   & $\checkmark$  &   &  $\checkmark$  &      &  6.87 \\  \hline
  % $\checkmark$   &   & $\checkmark$  &    &      &  6.51 \\  \hline
  % $\checkmark$   &   & $\checkmark$  &  $\checkmark$  &      &  6.23  \\  \hline
  % $\checkmark$   &   &   &    &  $\checkmark$   &  6.18 \\  \hline
  % $\checkmark$   & $\checkmark$  &   &    &  $\checkmark$  & 6.64 \\  \hline
  % $\checkmark$   &   &  $\checkmark$ &    &  $\checkmark$  & 6.50 \\  \hline
  &   & $\checkmark$  &    &      &  5.8  \\  \hline
  &   & $\checkmark$  &  $\checkmark$  &      &  5.8  \\  \hline
    &   &   &    &  $\checkmark$   &  5.4 \\  \hline
  & $\checkmark$  &   &    &  $\checkmark$  & 6.3 \\  \hline
  &   &  $\checkmark$ &    &  $\checkmark$  & 6.0 \\  \hline

\hline
\hline                                      
\end{tabular}
%}
% \vspace{-1em}
\end{table}


\begin{figure*}[t]

\centering
\includegraphics[width=0.75\linewidth]{figs/attention.pdf}
\caption{Analysis of attention scores with different clues.}
\label{fig:fig2}
\end{figure*}
\vspace{-.9em}


\section{Experiments}
\label{sec:exp}


\subsection{Dataset}

\noindent \textbf{Audio simulation}.
We first simulate a dataset for the TSE task based on AudioSet \cite{gemmekeAudioSetOntology2017}.
AudioSet is a large-scale audio dataset drawn from YouTube videos, and it has 527 sound classes labeled by humans.
Most of the data in AudioSet are 10-second video clips with sound track.
AudioSet is a weakly labeled dataset. 
There is usually more than one sound event in a video clip with labeled tags, and the occurrence time of the sound events is not provided.
To ensure each clip has only one audio source and correct label,  the pre-processing method from  \cite{kongSourceSeparationWeakly2020} was employed in our simulation.
% In the TSE model training, we want the target audio label to be a clean and single-source audio clip for better training.
% We follow the pre-processing method that is used in \cite{kongSourceSeparationWeakly2020} for data simulation.
% A pre-trained SED model \cite{kongPANNsLargeScalePretrained2020} is used to find the sound event anchor \cite{kongSourceSeparationWeakly2020} based on the SED probability in a 10s audio clip, and we cut out the 2s audio around the sound event anchor.
For each audio clip, a pre-trained SED model \cite{kongPANNsLargeScalePretrained2020} is applied to locate its sound event anchor by comparing the SED probability in 10s audio clip. The audio of 2s around the event anchor is then selected for each clip for later simulation.
% Then it is reasonable to assume that there is only one sound source within a 2s audio clip.
For the training set, we clipped $64$k (about $35$h) sound sources of $463$ classes for simulation and simulated $124$k (about $70$h) audio mixture of two sound sources. 
The signal-to-noise ratio (SNR) of the target sound is randomly sampled between $-2$ to $2$ dB during the simulation.
For validation and testing, we simulated $0.5$h and $1$h data, respectively.
And all the target sound classes in the validation and test set are seen in the training.
Besides, we also simulated $0.7$h data of unseen target sound classes for testing, in which most of the sound classes are musical instruments.

\noindent \textbf{Text clue}. The AudioCaps \cite{kimAudioCapsGeneratingCaptions2019} provides human-written natural language description for a subset of the AudioSet. However, after the single-source clipping for the target sound, the description of the original 10s audio is no longer precise for the clipped-out 2s audio.
So, we adopt an audio caption generation model \cite{wuAudioCaptionListen2019} to create pseudo natural descriptions as the text clues.
The 2s target sound is sent to the audio caption generation model, and the model outputs a sentence that describes the target sound.


\noindent \textbf{Visual clue}. The data in the AudioSet has parallel video and audio. For the visual clue, we simply use the frames (with FPS 15) from the $2s$ video that aligned with the target sound. 
%The FPS of the videos.

\noindent \textbf{One-hot tag clue}. As mentioned above, the AudioSet only provided weak sound class labels without the occurrence time, and we clipped out 2s audio from the original data with a SED model.
During the clipping, we transform the SED probability of the 2s audio into one-hot vectors as the clue of the target sound.

\subsection{Model configuration and training}
The FFT size, the window length, and the hop length for STFT were set to $512$, $400$, and $100$, respectively, for 16 kHz input. 
The channel numbers of the DCCRN encoder were $\{ 32, 64, 128, 256,256,256\}$.
The complex LSTM consisted of two (real and imaginary) two-layer bidirectional LSTMs with $512$ hidden units.
% Except for the sound tag encoder, the parameters of all the modality encoders in the clue processing net are frozen during the training. In order to train faster, the deep embeddings from the modality encoders are extracted and stored in disk in advance.
% A learnable linear projection layer is used to map each modality clue into the unified embedding space, respectively.
% The multi-clue attention is built with a multi-head attention module. 
% The number of attention heads is $4$ and the total dimension is $512$.
We used ESPNet-SE toolkit \cite{liESPnetSEEndToEndSpeech2021} for implementation. 
Open text\footnote{{https://huggingface.co/distilroberta-base}} and vision\footnote{{https://huggingface.co/microsoft/swin-large-patch4-window7-224}} encoders were used for the text  and video clues. 

The proposed multi-clue TSE system was trained  with two stages. 
In the first stage, we first trained the DCCRN backbone by using only the sound tag clue. 
In the second stage, starting from this pre-trained model, we trained all the model parameters except for the pre-trained text and video encoders to minimize the extraction loss of Eq. \eqref{eq:loss}. 
We adopted the two-stage approach because our preliminary experiment found that training the entire model from scratch in an end-to-end fashion based on Eq. \eqref{eq:loss} was hard to achieve optimal checkpoint.  
This could be because the clue processing net cannot produce stable clue embeddings in the early training steps. The two-stage training approach addresses this by starting the training with the simple sound tag-based clue. 
%When training the multi-clue TSE system, we found that directly using the extraction loss (eq. \ref{eq:loss}) to train both the DCCRN backbone and the clue processing net in an end-to-end style is very difficult. One possible reason is that in the early training stage, the clue processing net can not produce stable clue embedding and there will be a permutation issue \cite{hersheyDeepClusteringDiscriminative2016,kolbaekMultitalkerSpeechSeparation2017} for the DCCRN model to estimate the correct target source.
%Thereby, we first train a DCCRN backbone with only the sound tag clue, and use the pre-trained parameter to initialize the DCCRN backbone in the multi-clue TSE system.
To enable fair comparison, the same initialization scheme was used for training the single-clue baselines.
The training was performed by using an Adam optimizer with an initial learning rate of $ 0.5 \times 10^{-4}$ and recuding it by a factor of $0.97$ for every epoch until convergence.







\subsection{Results}

\noindent\textbf{Comparison with baselines}.
Table \ref{tab:tab1} compares the proposed multi-clue TSE system with three single-clue TSE baselines for different clue-usage conditions in terms of SNR improvement (SNRi).
The proposed TSE system achieved the best SNRi score for both \textit{seen} and \textit{unseen} test sets when it used all the three clues. 
Even with two clues, our multi-clue TSE outperformed all the single-clue baselines.
When only one clue was provided,  the proposed multi-clue TSE performed comparably with the single-cue baselines for the seen test set and marginally performed better for the unseen test set. These results show that the superiority of the proposed system in terms of both effectiveness and flexibility. 

\noindent\textbf{Robustness to compromised clues}
In real applications, some of the the input clues may sometimes become inaccurate. 
To test the robustness of the proposed model, we carried out experiments with text and video clues where one or two of them were artificially compromised. 
This was done as follows. 
% For the tag clue, we randomly replaced the correct tag with another one. 
% We assume the tag clue is always correct, and compared ywith 
For the text clue, one third of the words in the clue sentence were replaced with random words. 
For the video clue, a Gaussian noise of $-2.5$dB is added to the original video clips.
Since there is no ambiguity in tag clues, we do not include the tag clues here.
Table \ref{tab:tab2} shows the experimental results. While the performance degradation was observed when one or two clues were compromised, good SNR improvements were still observed for all conditions. 
By comparing the 1st row with the 2nd row, and the 3rd row with the 4th row, we found when the text clue was compromised, adding correct video clue helps, and vice versa.
When both clues were compromised, it still showed better performance than only using one compromised clue.
% Also, it was observed that the sound tag clue had the biggest impact on the performance in this experiment setting. 
%When only one of the three clues was compromised, the performance drop was relatively small (about $1$ dB). 
%The sound tag clue plays an important role when another two clues are both inaccurate.

\subsection{Analysis of multi-clue attention}
To analyze the attention mechanism by which the multi-clue net processes clues, we plot the attention weight matrix in figure. \ref{fig:fig2}. The input audio is mixed by two sound events, a man's speech and the sound of a car horn.
The available clues are the sound tag ``vehicle'', a natural description of the car horn, and a video in which a car is driving.
Figure \ref{fig:fig2}.b plots the attention scores in the multi-clue attention module. We can see that the highest scores are mainly in the tag and text clues. In this case, the video clues contributed little. That may be because the car in the video is too far from the camera.
When we only use the text as the extraction clue, we can see both the sound of the car horn and the sound of speech can be extracted with different descriptions as figure \ref{fig:fig2}.c and figure \ref{fig:fig2}.d) show. And the words in the sentence related to the target sound get higher scores in the attention matrix.



















% \begin{figure*}[t]

% \centering
% \includegraphics[width=1.0\linewidth]{figs/snri_break}
% \caption{SNRi of target sound classes.}
% \label{fig:fig1}

% \end{figure*}


% \begin{table}[]
% \caption{The overall SNR improvement (SNRi) on the unseen testing set.}
% \label{tab:tab1}
% \centering
% %    \setlength{\tabcolsep}{3mm}{
% \begin{tabular}{c|ccc|c}
% \hline
% \hline
% \multicolumn{1}{c|}{\multirow{2}{*}{Model}} & \multicolumn{3}{c|}{Testing clues} & \multicolumn{1}{c}{\multirow{2}{*}{SNRi}} \\ \cline{2-4}
% \multicolumn{1}{c|}{}                       &       tag      &     text      &      video     & \multicolumn{1}{c}{}                      \\ \hline
% %DCCRN (sound tag)                           &       $\checkmark$     &           &           &     6.38                                      \\ \hline
% %DCCRN (text)                                &            &      $\checkmark$      &           &     6.35                                      \\ \hline
% %DCCRN (video)                               &            &           &        $\checkmark$    &     4.68                                      \\ \hline
% \multirow{7}{*}{DCCRN (multi-clue)}         &     $\checkmark$        &     $\checkmark$       &     $\checkmark$       & 6.51   \\ \cline{2-5} 
%                                             &      $\checkmark$       &   $\checkmark$   &           &  6.37    \\ \cline{2-5}
%                                             &          &     $\checkmark$        &       $\checkmark$      &  6.40  \\ \cline{2-5}
%                                             &      $\checkmark$       &           &      $\checkmark$        &  6.42   \\ \cline{2-5}

%                                             &      $\checkmark$       &           &           &  6.20  \\ \cline{2-5}
%                                             &            &       $\checkmark$     &           &   6.03  \\\cline{2-5}
%                                             &            &           &     $\checkmark$       &   5.94  \\
%                                             \hline
% \hline                                      
% \end{tabular}
% %}
% \end{table}