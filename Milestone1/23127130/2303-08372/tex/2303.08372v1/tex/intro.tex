\section{Introduction}

People can focus their auditory attention on the sound of their interest in a complex acoustic environment \cite{cherryExperimentsRecognitionSpeech1953}.
Researchers have attempted to endow machines with a similar capability by audio source separation, a process of separating all audio sources out of their mixture.
Audio source separation includes speech separation \cite{hersheyDeepClusteringDiscriminative2016,kolbaekMultitalkerSpeechSeparation2017,luoConvTasNetSurpassingIdeal2019}, music separation \cite{liuVoiceAccompanimentSeparation2020,liuChannelWiseSubbandInput2020,defossezMusicSourceSeparation2021}, and universal sound separation \cite{kavalerovUniversalSoundSeparation2019,tzinisImprovingUniversalSound2020,kongSourceSeparationWeakly2020}.

In some cases, instead of separating all sound sources, we may only be interested in a specific source in the mixed signal. 
With target sound extraction (TSE), only the sound of interest is extracted from the audio mixture given a target clue. The clues for the TSE systems can be provided in various forms.
Sound-related clues include a sound tag \cite{kongSourceSeparationWeakly2020,liCategoryAdaptedSoundEvent2022}, a reference speech signal and a target speaker embedding \cite{delcroixSingleChannelTarget2018,wangVoiceFilterTargetedVoice2019,wangSpeechSeparationUsing2019,xuSpExMultiScaleTime2020,delcroixImprovingSpeakerDiscrimination2020}. 
Visual information can also be used as the extraction clue in multi-modal target sound extraction \cite{afourasConversationDeepAudioVisual2018,gaoVisualVoiceAudioVisualSpeech2021,tzinisAudioScopeV2AudioVisualAttention2022a}. Some recent works use natural language descriptions as the clues~\cite{kilgourTextDrivenSeparationArbitrary2022,liuSeparateWhatYou2022b}. 
In \cite{ohishiConceptBeamConceptDriven2022a}, the authors use `concept' as the clue for target speech extraction, where the concept can be extracted from an image or a speech signal that is related to the target speech.




While there are various TSE systems each of which handles a specific clue form, there is still room for improvement in practical applications.
First, a single clue may be insufficient to describe a specific target sound. 
Secondly, the single-clue TSE system is not robust against device failures. For example, the single-clue vision-based TSE system may become incapacitated when there is a camera failure.
Lastly, the pieces of information provided by multiple types of clues may be complementary to each other to create a more comprehensive clue about the target sound, which could  result in extraction performance improvement.
Some recent works took advantage of multiple clues for target speech extraction \cite{liListenWatchUnderstand2020,tanAudioVisualSpeechSeparation2020,liVCSETimeDomainVisualContextual2022,rahimiReadingListenCocktail2022} and showed the performance superiority to single-clue systems.
However, most of these systems, except for \cite{rahimiReadingListenCocktail2022}, require all the clues to be available during the inference.
Also, they were built for speech signals and did not cope with general non-speech sounds. 


In this paper, we propose a unified TSE system that can extract the target sound by flexibly combining multiple clues from different modalities that are available at test time, including a sound event tag, a text description, and a video clip related to the target sound. 
Designing such a system gives rise to several challenges.
First, clues of different modalities take different input forms, requiring a unified approach to processing them in the embedding space.
Secondly, some of the clues (e.g. the sound event tag) provide static information about the sound to be extracted, while %some of the clues 
others (e.g. the video clip) provide dynamic information which changes over time.
It is important to appropriately deal with the alignment between the clues and the input audio features.
Thirdly, the multi-clue TSE system should be able to deal with various number of input clues.  
To solve these challenges, we design a clue processing network based on multi-clue attention.
The basic idea is inspired by \cite{rahimiReadingListenCocktail2022}, which propose a unified Transformer-based model for speech extraction with text and video clues. 
In our TSE system, the observed sound mixture and all the input clues are firstly encoded into a unified embedding space with the corresponding modality encoders. Then, the encoded sound mixture can attend to all the different clues at the same time to synthesize a cross-modality clue.
This process does not require all the clues to be present, and the alignment problem is handled with an attention mechanism.
The contributions of this paper are as follows. (1) We propose a TSE model based on a multi-clue attention module to leverage a variable number of clues with different modalities. 
(2) The system robustness and the details of the attention mechanism are experimentally analyzed. (3) We build a multi-modal TSE dataset based on  public corpora, Audioset \cite{gemmekeAudioSetOntology2017} and AudioCaps \cite{kimAudioCapsGeneratingCaptions2019}.




\begin{figure*}[t]

\centering
\includegraphics[width=0.8\linewidth]{figs/fig1.pdf}
\caption{Proposed multi-clue target sound extraction network.}
\label{fig:fig1}

\end{figure*}