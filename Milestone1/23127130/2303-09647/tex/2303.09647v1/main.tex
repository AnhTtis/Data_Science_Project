\documentclass[10pt]{IEEEtran}

\ifCLASSINFOpdf
\else
   \usepackage[dvips]{graphicx}
\fi
\usepackage{url}

\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{graphicx}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsfonts,amsthm,amsmath,xcolor,amssymb}
\usepackage{diagbox}
%\usepackage{1-in-2}
\usepackage{ulem}
\usepackage{algorithmic}


\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{corollary}{Corollary}
\newtheorem{rem}{Remark}
\newtheorem{prob}{Problem}

% this makes the columns on the last page equal
\usepackage{flushend}

\newcommand{\utheta}{\Theta}
\newcommand{\ltheta}{\theta}
\newcommand{\vtheta}{\vartheta}
\newcommand{\tutheta}{\tilde{\utheta}}
\newcommand{\tltheta}{\tilde{\ltheta}}
\newcommand{\cx}{\mathcal{X}}
\newcommand{\mybfC}{\mathbf{C}}
\newcommand{\updatedbn}{\texttt{Update}}
\newcommand{\prior}{\hat{\pi}}
\newcommand{\du}{\delta_U}
\newcommand{\dl}{\delta_L}
\newcommand{\gu}{\gamma_U}
\newcommand{\gl}{\gamma_L}
\newcommand{\bl}{\bar{\lambda}}

%% covariance estimation white paper
%% covariance estimation white paper
\newcommand{\bfr}{\mathbf{R}}
\newcommand{\bfrh}{\hat{\mathbf{R}}}
\newcommand{\bfrc}{\check{\bfr}}
\newcommand{\bfx}{\mathbf{X}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cy}{\mathcal{Y}}
\newcommand{\myM}{M}
\newcommand{\mym}{m}
\newcommand{\cm}{\mathcal{M}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\bfa}{\mathbf{A}}
\newcommand{\bfb}{\mathbf{B}}
\newcommand{\bfc}{\mathbf{C}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\rmid}{\mathrm{id}}
\newcommand{\bfv}{\mathbf{V}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\wthree}{\kappa}
\newcommand{\trR}{\text{admissible}}
\newcommand{\snr}{\eta}
\newcommand{\snrhat}{\hat{\eta}}
\newcommand{\snrtilde}{\tilde{\eta}}
\newcommand{\um}{\breve{m}}
\newcommand{\toas}{\overset{\text{a.s.}}{\to}}
\newcommand{\rhi}{\hat{R}^{-1}}
\newcommand{\rhrrh}{\hat{R}^{-1}R\hat{R}^{-1}}
\newcommand{\buni}{\bu_{n,i}}
\newcommand{\lamni}{\lambda_{n,i}}
\newcommand{\bfrs}{\mathbf{R}^*}
\newcommand{\bfrsi}{\mathbf{R}^{*-1}}
\newcommand{\toprob}{ \overset{\mathrm{p}}{\to} }
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bfT}{\mathbf{T}}
\newcommand{\bssig}{\boldsymbol{\Sigma}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\fH}{\mathcal{H}}
\newcommand{\fK}{\mathcal{K}}
\newcommand{\fB}{\mathcal{B}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\mydelta}{\delta}
\newcommand{\myphi}{\varphi}
\newcommand{\ala}{\emph{\`{a} la}}
\newcommand{\effone}{f_1}
\newcommand{\effzero}{f_0}
\newcommand{\tautilde}{\tilde{\tau}}
\newcommand{\myem}{T}
\newcommand{\myenzero}{\overline{n}}
\newcommand{\mylam}{\Lambda}
\newcommand{\laipoor}{\texttt{Round-Robin}}
\newcommand{\proposed}{\texttt{Proposed}}
\newcommand{\mystop}{\hat{t}}
\newcommand{\bdr}[1]{{\color{red} [BDR: #1]}}
\newcommand{\ubl}[1]{{\color{green} [UBL: #1]}}
\newcommand{\mh}[1]{{\color{blue} [MH: #1]}}

\begin{document}

\title{Anomaly Search Over Many Sequences \\ With Switching Costs}

\author{Matthew Ubl, \IEEEmembership{Student Member, IEEE}, Benjamin D. Robinson, \IEEEmembership{Member, IEEE}, \\
and Matthew Hale, \IEEEmembership{Member, IEEE}
\thanks{Date submitted: 2 March 2023. AFRL Public Affair number AFRL-2023-0996. This work was supported by 
the Office of Naval Research under grants
N00014-22-1-2435 and N00014-21-1-2495, by
the Air Force Office of Scientific Research under grants FA9550-19-1-0169 and RYCOR036, and by the Air Force Research Lab Sensors Directorate.}
\thanks{Benjamin Robinson is with the Air Force Research Lab Sensors Directorate, WPAFB, OH 45433 USA.  (email: benjamin.robinson.8@us.af.mil)}
\thanks{Matthew Ubl and Matthew Hale are with the University of Florida Aerospace Engineering Department, Gainesville, FL, 32611 USA.  (Respective emails: m.ubl@ufl.edu and matthewhale@ufl.edu)}}

% \markboth{Journal of \LaTeX\ Class Files, Vol. 14, No. 8, August 2015}
% {Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
\maketitle

\begin{abstract}
This paper considers the quickest search problem to identify anomalies among large numbers of data streams. These streams can model, for example, disjoint regions monitored by a mobile robot. A particular challenge is a version of the problem in which the experimenter must suffer a cost each time the data stream being sampled changes, such as the time the robot must spend moving between regions. In this paper, we propose an algorithm which accounts for switching costs by varying a confidence threshold that governs when the algorithm switches to a new data stream. 
Our main contributions are easily computable approximations for both the optimal value of
this threshold and the optimal value of the parameter that determines when a stream
must be re-sampled. Further, we  empirically 
show (i) a uniform improvement for switching costs of interest 
and (ii) roughly equivalent performance for small switching  costs when
comparing to the closest available algorithm.
% We show that this algorithm approaches optimality for problems of interest, and demonstrate its
% superiority 
% practical value in numerical experiments \ubl{by showing BLANK improvement relative to a comparable algorithm}.
\end{abstract}

\begin{IEEEkeywords}
Quickest search, sequential analysis, controlled sensing, scanning rule, switching costs
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle



\section{Introduction}

\IEEEPARstart{A}{}fundamental problem in sensing and signal processing is online anomaly search.  With origins in Chernoff's sequential design of experiments \cite{chernoff1959sequential}, the aim of online anomaly search is to develop an efficient policy for sampling a subset of several data streams over time that quickly and accurately identifies any anomalous ones.  Some variations on Chernoff's method include \cite{dragalin1996simple,nitinawarat2013controlled,naghshvar2013active,nitinawarat2015controlled,cohen2015active,huang2018active,tsopelakos2019sequential}. An important application is clinical-trial design, where the drug tested in each trial depends on the drugs tested and outcomes obtained in all previous trials of the study.  
Another application is online search for an open channel in cognitive radio, where the sampling policy may depend on past observations.  
%\mh{This last sentence feels disconnected from the preceding one.}



%as do methods for handling an essentially infinite number of experiments \cite{lai2011quickest,malloy2012quickest,tajer2013quick,malloy2014sequential}.

Recently several authors have considered the problem of online anomaly search in which a cost is incurred any time the current channel sampled differs from the last one \cite{vaidhiyan2017neural, vaidhiyan2015active, lambez2021anomaly}.  This new formulation models online anomaly search more realistically than the traditional formulation because it accounts for possible switching costs in hardware or software, e.g., if an autonomous agent must move to observe a new location or if equipment must be repositioned to do so. This formulation is also more challenging because conventional analyses of exploration versus exploitation 
do not apply.
% when a gimbeled radar searches two locations alternately with a switching time that is nonzero.  Another relevant scenario would be searching multiple locations for a target event using an autonomous platform, perhaps for disaster search-and-rescue.  
Table~\ref{tab:context} shows how the problem setup in this paper relates to that of existing work.
%\mh{We should do the compare and describe part.}
We emphasize that we differ from these existing works by considering switching costs and an infinite number of data streams simultaneously.  To the best of our knowledge, this is the first work to do so.

Our approach is to adapt an existing method to the case of switching costs.  We do this by (i) introducing a new  parameter that controls switching between data streams and (ii) numerically optimizing both this switching parameter and an existing stopping parameter.
Our contributions are:
\begin{itemize}
\item We develop an algorithm that generalizes the algorithm in \cite{lai2011quickest}, which is known to be Bayes-optimal for the setting without switching costs.
\item We show that in a certain asymptotic regime, the optimal parameters for this algorithm can be found by exactly solving an algebraic equation and numerically solving a strongly convex optimization problem over a scalar decision variable.
This approach compares favorably with the Monte Carlo method of \cite{lai2011quickest} in terms of computational efficiency.
%\item \bdr{\sout{We derive consistent estimates of the algorithm's error probability and stopping time and illustrate the accuracy of these bounds relative to our numerical results.}}
\item We illustrate an almost uniform improvement in performance over the closest existing method in experiments. We illustrate empirically a uniform improvement for switching costs of interest and roughly equivalent performance for small switching costs, compared to \cite{lai2011quickest}.
\end{itemize}

\begin{figure}
    \centering
\begin{tabular}{|c||c|c|} \hline
     &  Switching costs  & No switching costs \\ \hline\hline
  Finite \# seqs   &  \cite{lambez2021anomaly}   & \cite{chernoff1959sequential} \cite{dragalin1996simple} \\ \hline
  Infinite \# seqs     &  This work    & \cite{lai2011quickest} \\ \hline
\end{tabular}
    \caption{Relationship of this work (lower left quadrant) to existing works; the other
    three quadrants contain references to representative existing works
    on those classes of problems.}
    \label{tab:context}
\end{figure}

The rest of this paper is organized as follows. In Section~\ref{sec:prelim} we provide our problem setup and discuss the existing optimal solution for the setting without switching costs. In Section~\ref{sec:probstate} we introduce switching costs and derive an approximation of the combined observation-switching cost, and derive the parameter choices that minimize this approximation. In Section~\ref{sec:discussion} we justify this approximation and explore the conditions for which it is accurate. In Section~\ref{sec:simulations} we provide numerical results, and we conclude in Section~\ref{sec:con}.

\section{Preliminaries} \label{sec:prelim}
\subsection{Problem Setup}
We consider data streams indexed over~$k \in \{1, 2,  \ldots\}$. 
Data stream $k$ generates the i.i.d. sequence of random variables $X^k_1, X^k_2,\dots$ which have sample space $\mathcal{X}$. Each data stream obeys one of two hypotheses, $H_0$ or $H_1$. Consider two distinct distributions on $\mathcal{X}$: $F_0$ and $F_1$. We say hypothesis $H_0$ is true for data stream $k$ if $X^k_t \sim F_0$ for all $t \in \{1,2,\dots\}$, and that $H_1$ is true for data stream $k$ if $X^k_t \sim F_1$ for all $t \in \{1,2,\dots\}$. We use $f_0$ and $f_1$ to denote the PDFs of $F_0$ and $F_1$ respectively. In this paper if $H_0$ is true for a particular data stream, we say that stream is \textit{nominal}, and if $H_1$ is true, that the stream is a \textit{target} stream. We assume that for any given data stream, $H_1$ is true with prior probability $\prior$ and $H_0$ is true with prior probability $1-\prior$, where $\prior \in (0,1)$. We use $\mathbb{E}_{i}$ to denote the expectation under hypothesis $i$, and define $\mathbb{E}_{\prior}[\,\cdot\,] = \prior \mathbb{E}_1[\,\cdot\,] + (1-\prior) \mathbb{E}_0[\,\cdot\,]$.

We assume we have a single observer that can sample one and only one data stream at a time. That is, if the observer samples data stream $k$ at time $t$ it receives $X^k_t$ but no information from the other data streams. Our goal is to design an algorithm for this observer to identify a target data stream as quickly as possible. In much of the existing literature, ``as quickly as possible'' means minimizing the expected number of observations required while satisfying some constraint on the error probability, i.e., 
satisfying an upper bound on the probability
that the stream we identify as a target is actually nominal. We use $\tau$ to denote the number of observations taken before the algorithm terminates and declares a particular data stream, which we denote as $k_{\tau}$, as a target. We use $H^{k_{\tau}}$ to denote the hypothesis obeyed by data stream $k_{\tau}$, and $P(H^{k_{\tau}} = H_0)$ as the error rate, i.e., the probability that the stream $k_\tau$ is actually nominal. Therefore, our goal is to find the algorithm that minimizes $\mathbb{E}_{\prior}[\tau]$ while ensuring $P(H^{k_{\tau}} = H_0) \leq \epsilon$, where $\epsilon > 0$ is an allowable error rate. 

%\mh{I think somewhere around here we should make it clear that we're still talking about the case without switching costs.
%Right now, a lazy reader could get confused by the next sentence: if CUSUM is optimal, let's just use it. And same for the last sentence
%of this section. A lazy reader might think we're just going to run numerical experiments
%for~$\gamma_U$. So it is worth framing this discussion as background for existing work without switching costs to make absolutely sure
%the reader follows what we're doing. Maybe we can put the first 2 paragraphs of this section in a subsection called ``Problem Setup''
%and then put the rest of the section into a subsection called ``Solution Without Switching Costs''. What do you think?
%}

\subsection{Solution Without Switching Costs}
This subsection briefly reviews related work on problems without switching costs; switching costs will be introduced in the next section. It was shown in~\cite{lai2011quickest} that a cumulative-sum-based (CUSUM-based) test is the optimal algorithm for the setting without switching costs. This algorithm is defined by two threshold parameters: $\gl \leq 0 \leq \gu$. In this algorithm the observer maintains a statistic $\mylam^k_t$ for stream $k$ which is updated after every observation and is initialized as $\mylam^1_0 = 0$. If at time $t$ the observer samples data stream $k$ (i.e., the observer receives $X^k_t$), then it performs the update $\mylam^k_t = \mylam^k_{t-1} + \log \left(\frac{f_1(X^k_t)}{f_0(X^k_t)}\right)$. If $\gl \leq \mylam^k_t < \gu$, then the observer will sample data stream $k$ again at time $t+1$. If $\mylam^k_t < \gl$, then the observer has declared stream $k$ as nominal. The observer will begin using the new statistic $\mylam^{k+1}_t=0$ and will switch to sampling data stream $k+1$ (we assume the streams are either pre-ordered or the next stream is selected at random) beginning at time $t+1$. This procedure describes the $k^{th}$ \textit{stage} of the algorithm, during which data stream $k$ is observed. 

The algorithm carries out the same procedure on data streams $k+1,k+2,\dots$ until a target stream is indicated. Specifically, if $\mylam^k_t \geq \gu$, then the algorithm terminates and the observer declares data stream $k$ as a target stream. The optimal choice for $\gl$ is $0$ regardless of $F_0$, $F_1$, $\prior$, or $\epsilon$~\cite[Section IV]{lai2011quickest}. Because $\mathbb{E}_{\prior}[\tau]$ monotonically increases as $\gu \rightarrow \infty$ and $P(H^{k_{\tau}} = H_0)$ monotonically decreases as $\gu \rightarrow \infty$, the optimal choice for $\gu$ is the smallest value for which $P(H^{k_{\tau}} = H_0) \le \epsilon$. However, a closed form for this $\gu$ is not known; it must be estimated using numerical experiments.


\section{Problem Statement with Switching Costs} \label{sec:probstate}
We now introduce switching costs to the model described in the previous section, and this will give the problem formulation that we consider
in this paper. 
When the observer switches from data stream $k$ to data stream $k+1$, it now incurs a cost $\lambda_k$ drawn from some non-negative distribution $L$ for all $k$. That is, $\lambda_k \geq 0$ and $\mathbb{E}[\lambda_k] = \bl$ is finite. 
 We also assume that the costs $\lambda_k$ and the observations $X^{k'}_t$ are mutually independent for all $k$, $k'$, and $t$. This cost models applications in which observing a new data stream requires ``deadtime'' when no observations can be taken, such as when equipment needs to be re-positioned or re-calibrated. It also models problems in which observations and switches are ``costly'' in some resource other than time, such as energy or money. Under this switching cost assumption, the new optimization problem becomes:
\begin{prob} \label{prob1}
Let an error tolerance $\epsilon \in (0,1)$ and prior $\prior \in (0,1)$ be given. Then
\begin{align}
    &\text{minimize}_{\gl \leq 0 \leq \gu} \mathbb{E}_{\prior} [\tau] + \mathbb{E}_{\prior}[s] \label{eqn:optprob}\\
    &\text{s.t. } P(H^{k_{\tau}} = H_0) \leq \epsilon, \label{eqn:constraint}
\end{align}
\noindent where $s = \sum_{i=1}^{k_{\tau}-1} \lambda_k$ is the total switching cost incurred before terminating the algorithm.
\end{prob}

To derive threshold choices for this problem we will next rewrite the problem in terms of the \textit{stage-wise} false-positive and false-negative rates $\alpha$ and $\beta$ of the algorithm, and then establish the relationship between these rates and the threshold choices $\gl$ and $\gu$.  
By the stage-wise false-positive rate, we mean the probability that a stage's 
%\mh{Have we defined a ``stage''? I think we are using it to mean ``interval in which we sample a single arm over and over'', but we should say
%that explicitly.  BDR: I just suggested a place to define ``stage'' in an earlier comment.}
terminal value of $\mylam$ exceeds (or equals) $\gu$ given that the stage's data follow $H_0$, i.e., the probability that a stream is declared a target even though the stage's data are nominal.
The stage-wise false-negative rate is similarly defined as the probability that $\gl$ exceeds the stage's terminal value of $\mylam$ given that the stage's data follow $H_1$, i.e., the stage's data are not nominal, but the stream is labelled nominal. 
% Let $\mystop$ be the stopping time of the first stage.  Then that stage's \textit{false positive} rate $\alpha = \mathbb{P}[\Lambda_{\mystop} \geq \gu \mid H_0]$ \bdr{I added a subscript of $T$ to $\Lambda$, which would not have been defined otherwise} is the probability that the observer erroneously flags a particular \textit{nominal} data stream as a \textit{target} stream, while that stage's \textit{false negative} rate $\beta = \mathbb{P}[\Lambda_{\mystop} < \gl \mid H_1]$ is the probability that the observer erroneously flags a particular \textit{target} data stream as a \textit{nominal} stream

Let $t_k$ be the time when the algorithm takes its last observation of stream $k$ (i.e., $\mylam^k_{t_k} \notin [\gamma_L, \gamma_U))$. 
Then $\hat{t} = t_k-t_{k-1}$ is the \textit{stage-wise stopping time} of stage $k$, or the number of observations taken of stream $k$ before making a decision. Then we may more compactly write that $\alpha = \mathbb{P}_0[\Lambda^k_{t_k} \geq \gu]$ and $\beta = \mathbb{P}_1[\Lambda^k_{t_k} < \gl]$, where $\mathbb{P}_i$ is the probability under hypothesis $i$.  
Consider the inequalities 
\begin{align}
   & \gamma_L \ge \log (\beta/(1-\alpha)) \label{eq:gl-approx} \\
   & \gamma_U \le \log((1-\beta)/\alpha)) \label{eq:gu-approx} \\
   & \mathbb{E}_{\prior}[\Lambda^k_{t_k} \mid \Lambda^k_{t_k} \ge \gu] \ge \gu \label{eq:gu-ineq} \\
& \mathbb{E}_{\prior}[\Lambda^k_{t_k} \mid \Lambda^k_{t_k} < \gl] \le \gl, \label{eq:gl-ineq}
\end{align}
which are given in \cite[Equations~(2.9) and (2.10)]{siegmund1985sequential}. In accordance with \cite{siegmund1985sequential}, we assume these inequalites are approximate equalities for the remainder of this section.  These approximations are known as ``Brownian motion approximations''.
%\bdr{``In addition, threshold crossings are assumed to have no overshoots?'' or something like that?  Nevermind: added two equations to the above}
%\mh{What are we saying here? That we will use these as approximately equal down below? BDR: Did I fix this?}
\begin{rem}[Brownian Motion Approximations]
The assumption that \eqref{eq:gl-approx}-\eqref{eq:gl-ineq} are approximate equalities is accurate under the conditions that (a) $f_0$ and $f_1$ are sufficiently close, (b) $\bl$ is sufficiently large, and (c) $\epsilon$ is small.  These conditions imply that the step size in the sequential probability ratio test of one stage is small compared to the decision thresholds, and thus overshoots of decision thresholds are also comparatively small.  We will elaborate on these conditions in Section~\ref{sec:discussion}. 
%\mh{Re: my comments the other day: I like this the way it is!}
\end{rem}
% \bdr{We note that from Equations 2.9 and 2.10 in [ref here**], $\gamma_L \ge \log (\beta/(1-\alpha))$ and $\gamma_U \le \log((1-\beta)/\alpha)$.  In what follows, we will make the \textit{tightness} assumption that these inequalities are approximate equalities, which is justified under the conditions that (a) $f_0$ and $f_1$ are sufficiently close, (b) $\lambda$ is sufficiently large, or (c) $\epsilon$ is small.
% These conditions imply that the step size in the SPRT of one stage is small compared to the decision thresholds, and thus overshoots of decision thresholds are smaller.}
% Further, these inequalities are approximately equalities provided that $\Lambda_t$ is approximately $\alpha$ or $\beta$.  We will discuss when this assumption can be made after we give our main result, bu

The quantities $\alpha$ and $\beta$ appear in Problem~1 in the following manner: using Wald's Identity 
%(cite?) \bdr{People will know what you're talking about since this is very much standard ... I don't think we need to go back and cite the original 1944 paper} 
we see that 
% $\mathbb{E}_{\prior} [\tau] = \mathbb{E}_{\prior}[S+1] \mathbb{E}_{\prior}[\mystop]$, \sout{where $\mathbb{E}_{\prior}[\mystop]$ is  
% \sout{the expected number of observations before $\Lambda_{\mystop} \notin [\gl, \gu)$ for a given data stream }
% \bdr{the expectation of $\mystop$
% given the prior $\prior$}.
$\mathbb{E}_{\prior} [\tau] = \mathbb{E}_{\prior}[k_{\tau}] \mathbb{E}_{\prior}[\mystop]$. 
Intuitively, this result states that the expected number of total observations before termination is equal to the expected number of data streams visited ($k_{\tau}$) multiplied by the expected number of observations per data stream (which is $\mystop$). Using Wald's Identity again gives $\mathbb{E}_{\prior} [s] = \mathbb{E}_{\prior}[k_{\tau}-1] \bl$, which states that the expected total switching cost incurred over time is equal to the expected number of switches (one less than the number of streams visited) multiplied by the expected switching cost per switch.

We first address the term $\mathbb{E}_{\prior}[\mystop]$. Assume that a particular data stream $k$ being sampled by the observer is a target, and assume that the observer takes its last sample of $k$ at time $t_k$. Then one of the termination criteria has been met and~$\Lambda^k_{t_k} \not\in [\gl, \gu)$. 
%\mh{I changed this sentence but kept the interval as~$[\gl,\gu)$. Should it be~$[\gl, \gu]$ instead?}
Let $f_0,f_1$ be the PDFs of $F_0,F_1$ respectively. 
%\mh{I thought~$F_0$ and~$F_1$ were the PDFs. Can you specify what they are up above when they're introduced?}
Using Wald's Identity again, we have $\mathbb{E}_{1} [\Lambda^k_{t_k}] = D(f_1||f_0) \mathbb{E}_1[t_k]$, where $D(f_1||f_0) = \mathbb{E}_1\left[\log{\frac{f_1(x)}{f_0(x)}}\right]$ is the Kullback-Leibler (KL) divergence of $f_0$ from $f_1$. Using the same procedure we see $\mathbb{E}_{0} [\Lambda^k_{t_k}] = -D(f_0||f_1) \mathbb{E}_0[\mystop]$. From the definition of $\mathbb{E}_{\prior}[\cdot]$, we can write
\begin{equation}
\mathbb{E}_{\prior}[\mystop] = (1-\prior)\frac{\mathbb{E}_{0} [\Lambda^k_{t_k}]}{-D(f_0||f_1)} + \prior \frac{\mathbb{E}_{1} [\Lambda^k_{t_k}]}{D(f_1||f_0)}.
\end{equation}
Furthermore, we see that $\mathbb{E}_{0} [\Lambda^k_{t_k}] = \alpha \mathbb{E}_{0} [\Lambda^k_{t_k} | \Lambda^k_{t_k} \geq \gu] + (1-\alpha) \mathbb{E}_{0} [\Lambda^k_{t_k} | \Lambda^k_{t_k} < \gl]$. By the Brownian motion approximations we have $\mathbb{E}_{0} [\Lambda^k_{t_k} | \Lambda^k_{t_k} \geq \gu]  \approx \gu$ and $\mathbb{E}_{0} [\Lambda^k_{t_k} | \Lambda^k_{t_k} < \gl]  \approx \gl$.
%\bdr{IGNORE: Seems like it would be good to mention that this is because we assume threshold crossings have no overshoots, but not sure how to say this concisely and accurately.}
%\bdr{\sout{We will elaborate on the quality of this approximation in Section~\ref{sec:discussion}.}} 
Following equivalent steps for $\mathbb{E}_{1} [\Lambda^k_{t_k}]$ gives
\begin{equation} \label{eqn:alphagamma}
\mathbb{E}_{\prior}[\mystop] \approx (1-\prior)\frac{\alpha \gu + (1-\alpha)\gl}{-D(f_0||f_1)} + \prior \frac{(1-\beta)\gu + \beta \gl}{D(f_1||f_0)}.
\end{equation}
Furthermore, from~\cite[Equation (30)]{lai2011quickest} we also have 
\begin{equation} \label{eqn:switchesalphabeta}
\mathbb{E}_{\prior} [k_{\tau}] = \frac{1}{(1-\prior)\alpha + \prior(1-\beta)}.
\end{equation}

For ease of notation, we now define $\du = \exp (\gu)$ and $\dl = \exp (\gl)$. While $\gl \leq 0 \leq \gu$ are the actual thresholds used by the algorithm, rewriting the problem in terms of $0 < \dl \leq 1 \leq \du$ makes the following notation simpler. 
%\sout{From equations 2.9 and 2.10 in~\cite{siegmund1985sequential} \bdr{broken ref}, we see that $\dl \geq \frac{\beta}{1-\alpha}$ and $\du \leq \frac{1-\beta}{\alpha}$. 
% Again, this inequality only fails to be an equality because $\Lambda_{k_i}$ overshoots the thresholds by a small amount.  
% %\bdr{maybe this statement can be used to make unnecessary the earier problem of describing overshoots informally?} 
% However it is common practice to treat these relationships as equalities in order to derive useful approximations.
Inverting the approximate equalities in \eqref{eq:gl-approx} and \eqref{eq:gu-approx}, we obtain the following:
%\mh{We should say where earlier those tightness assumptions were made}
$\alpha \approx \frac{1-\dl}{\du-\dl}$, $\beta \approx \dl\frac{\du-1}{\du-\dl}$, $1-\alpha \approx \frac{\du-1}{\du-\dl}$, and $1-\beta \approx \du\frac{1-\dl}{\du-\dl}$. 
Substituting these into~\eqref{eqn:alphagamma} and~\eqref{eqn:switchesalphabeta} and simplifying 
%\mh{Saying ``algebraic manipulation'' makes it sound like we're leaving a lot out. Is it fair to just saying ``simplifying''}
yields that the function
\begin{multline} 
  C(\dl,\du) := \frac{1-\prior}{-D(f_0||f_1)} \frac{\log(\du)+\frac{\du-1}{1-\dl}\log(\dl)}{1+\prior(\du-1)} \\ \qquad\qquad\qquad\quad + \frac{\prior}{D(f_1||f_0)} \frac{\du\log(\du)+\dl\frac{\du-1}{1-\dl}\log(\dl)}{1+\prior(\du-1)} \\ 
  + \frac{\bl\frac{\du-\dl}{1-\dl}}{1+\prior(\du-1)} \label{eqn:cost}
\end{multline}
approximates the cost given in \eqref{eqn:optprob}.
%\mh{What do you think of making this statement a lemma? The last 2-3 paragraphs are its proof, so we'd just change
%some formatting.}

%\mathfootnote{
%\begin{equation} 
%  C(\dl,\du) := \frac{1-\prior}{-D(f_0||f_1)} \frac{\log(\du)+\frac{\du-1}{1-\dl}\log(\dl)}{1+\prior(\du-1)} + \frac{\prior}{D(f_1||f_0)} \frac{\du\log(\du)+\dl\frac{\du-1}{1-\dl}\log(\dl)}{1+\prior(\du-1)} + \frac{\bl\frac{\du-\dl}{1-\dl}}{1+\prior(\du-1)} \label{eqn:cost}
%\end{equation}
%}

Furthermore, from~\cite[Equation~(29)]{lai2011quickest}  we have that that $P(H^{k_{\tau}} = H_0) = \frac{(1-\prior)\alpha}{(1-\prior)\alpha + \prior(1-\beta)}$, which is bounded above by $ \frac{1-\prior}{1+\prior(\du-1)}$  since $(1-\beta)/\alpha \ge \delta_U$. As a result, this inequality is an approximate equality under the Brownian motion approximations. Therefore, following some algebraic manipulation we see that $P(H^{k_{\tau}} = H_0) \leq \epsilon$ if $\du \geq \frac{1-\prior}{\prior}\frac{1-\epsilon}{\epsilon}$. Note that $\frac{1-\prior}{\prior}\frac{1-\epsilon}{\epsilon} > 1$ so long as $\epsilon < 1-\prior$, which should always be the case; if the tolerable error rate is greater than the prevalence of nominal data streams then the optimal algorithm is to take no observations and flag a data stream at random.

Therefore, we can find an approximate solution to Problem~\ref{prob1} by solving the following problem.
%\mh{What do you think of putting this under a formal problem heading?}
%\mh{I think we should say ``Let~$\epsilon > 0$ be given'' inside the problem.}
\begin{prob}
Let an error tolerance $\epsilon \in (0,1)$ and a prior $\prior \in (0,1)$ be given. Then find
\begin{align}
 \text{Find } (\dl^*,\du^*) = \underset{\dl,\du}{\arg\min }  \text{ } & C(\dl,\du) \label{approxprob}\\
 \text{s.t. } &\du \geq \frac{1-\prior}{\prior}\frac{1-\epsilon}{\epsilon} \\
& \dl \in [0,1].
\end{align}
\end{prob}

From the structure of $C(\dl,\du)$, we can derive the following propositions:
\begin{prop} \label{prop1}
For any fixed $\hat{\delta}_L \in (0,1)$, $C(\hat{\delta}_L,\cdot)$ is monotonically increasing on the domain $[1,\infty)$. From this fact, we get $\du^* = \frac{1-\prior}{\prior}\frac{1-\epsilon}{\epsilon}$.
\end{prop}
\textit{Proof:} The monotonic behavior of $C(\hat{\delta}_L,\cdot)$ on this domain is apparent by inspection. Because we wish to minimize $C(\hat{\delta}_L,\cdot)$, we want to set $\du$ to its minimum allowable value. Since that value is $\frac{1-\prior}{\prior}\frac{1-\epsilon}{\epsilon}$, we have $\du^* = \frac{1-\prior}{\prior}\frac{1-\epsilon}{\epsilon}$ regardless of the value of $\hat{\delta}_L$. $\hfill\blacksquare$
\begin{prop} \label{prop2}
    For any fixed $\hat{\delta}_U > 1$, $C(\cdot,\hat{\delta}_U)$ is strongly convex on the domain $(0,1]$. From this fact $\dl^*$ exists, is unique, and can be found by solving the scalar optimization problem $\dl^* = \underset{\dl \in [0,1]}{\arg \min} \text{ } C(\dl,\du^*)$. Furthermore, $\dl^*$ lies in the interior of this interval for $\bl > 0$ (i.e., $\dl^* \in (0,1))$. 
\end{prop}
%\begin{proof}
%(This is a more terse proof that we might be more appropriate for a journal.)

%Observe that $C(\dl, \hat{\delta}_U)$ is of the form $A - B \log(\dl)/(1-\dl) + C \dl \log(\dl)/(1-\dl)+  D/(1-\dl)$, where  $B,C,D>0$ when $\hat{\delta}_U > 1$.  
%By inspection, the derivative of  $-\log(\dl)/(1-\dl)$, which is the coefficient of $B$, is negative and strictly increasing on $(0,1)$ and $-\infty$ at $0+$.  
%Similarly, the derivative of $\dl \log(\dl)/(1-\dl)$, which is the coefficient of $C$, is negative and strictly increasing on $(0,1)$.  
%In addition, the derivative of $1/(1-\dl)$, which is the coefficient of $D$, is positive and strictly increasing on $(0,1)$ and infinite at $1-$. 
%Thus, $C(\,\cdot\,, \delta^*_U)$ is strictly convex in $(0,1)$, and has a unique zero $\delta^*_L$, which can be found by a variety of numerical methods. 
%\end{proof}

\textit{Proof:} Differentiation shows $\lim_{\dl \rightarrow 0^+} \frac{\partial C(\dl,\hat{\delta}_U)}{\partial \dl} = -\infty$ and $\lim_{\dl \rightarrow 1^-} \frac{\partial C(\dl,\hat{\delta}_U)}{\partial \dl} = \infty$ so long as $\bl > 0$. Therefore, from the Intermediate Value Theorem there must exist some value $\hat{\delta}_L \in (0,1)$ for which $\frac{\partial C(\dl,\hat{\delta}_U)}{\partial \dl} = 0$ at $\dl = \hat{\delta}_L$. Strong convexity is established by characterizing the limiting behavior of $\frac{\partial^2 C(\dl,\hat{\delta}_U)}{\partial \dl^2}$. Observe that $C(\dl,\du)$ is a sum of three terms: the first two which contain the KL divergences $D(f_0||f_1)$ and $D(f_1||f_0)$, and the third which contains $\bl$. Name these terms $C_1$ $C_2$, and $C_3$ respectively. 
First, we see that $\lim_{\dl \rightarrow 0^+} \frac{\partial^2 (C_1(\dl,\hat{\delta}_U) + C_2(\dl,\hat{\delta}_U))}{\partial \dl^2} = \infty$ and $\lim_{\dl \rightarrow 1^-} \frac{\partial^2 (C_1(\dl,\hat{\delta}_U) + C_2(\dl,\hat{\delta}_U))}{\partial \dl^2} = \frac{\hat{\delta}_U-1}{1+\prior(\hat{\delta}_U-1)}(\frac{2}{3}\frac{1-\prior}{D(f_0||f_1)}+\frac{1}{3}\frac{\prior}{D(f_1||f_0)})$, and that $\frac{\partial^2 (C_1(\dl,\hat{\delta}_U) + C_2(\dl,\hat{\delta}_U))}{\partial \dl^2}$ is monotonically decreasing with $\dl$ on~$[0, 1]$.
Additionally, we see $\lim_{\dl \rightarrow 0^+} \frac{\partial^2 C_3(\dl,\hat{\delta}_U)}{\partial \dl^2} = \frac{\bl(\hat{\delta}_U-1)}{1+\prior(\hat{\delta}_U-1)}$ and $\lim_{\dl \rightarrow 1^-} \frac{\partial^2 C_3(\dl,\hat{\delta}_U)}{\partial \dl^2} = \infty$, and that $\frac{\partial^2 C_3(\hat{\delta}_L,\hat{\delta}_U)}{\partial \dl^2}$ is monotonically increasing with $\dl$ on~$[0,1]$.
Therefore, $C(\,\cdot\,, \hat{\delta}_U)$ is $\frac{\hat{\delta}_U-1}{1+\prior(\hat{\delta}_U-1)}(\frac{2}{3}\frac{1-\prior}{D(f_0||f_1)}+\frac{1}{3}\frac{\prior}{D(f_1||f_0)}+\bl)$-strongly convex on this interval. 
%\mh{I changed ``at least [big number]-strongly convex'' to just ``[big number]-strongly convex''. 
%A~$2$-strongly convex function is also~$1$-strongly convex (even though~$1$ isn't the biggest number we could use), and this simplifies the wording.
%}
Therefore $\hat{\delta}_L$ is a unique minimizer of $C(\,\cdot\,,\hat{\delta}_U)$, and $\dl^*$ can be found by minimizing $C(\,\cdot\,,\du^*)$. $\hfill\blacksquare$

%\bdr{these propositions look great} \mh{I agree!}

Therefore, Propositions~\ref{prop1} and~\ref{prop2} tell us we can calculate $\du^*$ explicitly as a function of $\prior$ and $\epsilon$, and $\dl^*$ numerically as the solution to a scalar, set-constrained, strongly convex optimization problem. These steps give rise to the Quickest Search Algorithm with switching Costs, which is Algorithm~1.

\begin{algorithm}[tb]
   \caption{Quickest Search Algorithm with Switching Costs}
   \label{alg1}
\begin{algorithmic}
   \STATE{\textbf{Input}: $\epsilon \in (0,1)$, $\prior \in (0,1)$, $D(f_1||f_0) > 0$, $D(f_0||f_1) > 0$}
   \STATE {$\gamma_U \gets \log \left(\frac{1-\epsilon}{\epsilon}\frac{1-\prior}{\prior}\right)$}
   \STATE{$\gamma_L \gets \log\left(\arg\min_{[0,1]}C\left(\cdot,\frac{1-\epsilon}{\epsilon}\frac{1-\prior}{\prior}\right)\right)$}
   \STATE{$t \gets 0$, $\Lambda^1_0 \gets 0$, $k \gets 1$}
   \WHILE{$\Lambda^k_t < \gamma_U$}
   \IF{$\Lambda^k_t \geq \gamma_L$}
   \STATE{\textbf{Observe}: $X^k_{t+1}$}
   \STATE{$\Lambda^k_{t+1} \gets \Lambda^k_{t} + \log\left(\frac{f_1(X^k_{t+1})}{f_0(X^k_{t+1})}\right)$}
   \STATE{$t \gets t+1$}
   \ELSE
   \STATE{$k \gets k+1$}
   \STATE{$\Lambda^k_t \gets 0$}
   \ENDIF
   \ENDWHILE
   \STATE{Label arm $k$ as a target}
\end{algorithmic}
\end{algorithm}

\section{Discussion of Brownian-Motion Approximations} \label{sec:discussion}

Now that we have presented Problem~2 as a solvable approximation of Problem~1, we will justify this substitution by showing that in the limiting cases described in Remark~1 in Section~\ref{sec:probstate}, the approximate inequalities used to formulate Problem~2 approach equalities. We are interested in the case where $f_0$ and $f_1$ are ``close'' since if they are easily distinguished, the problem is easy and optimality is not crucial.  We are interested in the case where $\epsilon$ is small because we want few errors.  Further, we are interested in the case where $\bl$ is relatively large because otherwise, the problem is solvable by the existing method of \cite{lai2011quickest}.
%\ubl{i.e. if your problem \textit{doesn't} satisfy the conditions in Remark~1, it's not an interesting problem and isn't hard to solve anyway.} %\bdr{Maybe we can say that the most interesting case is when SNR  (i.e., KL divergences) is low compared to the switching cost, or something like that.  The fact that you want error probability to be low compared to SNR is sort of a given, I think.}

%\bdr{``Recall that the inequalities \eqref{eq:gl-approx} through \eqref{eq:gl-ineq} have been treated as approximate equalities.  We will now show using standard techniques that these approximations, well-known as ``Brownian motion approximations,'' are justified under the assumptions of Remark~1.''}

Recall from \cite{siegmund1985sequential} that that the inequalities \eqref{eq:gl-approx}-\eqref{eq:gl-ineq} being treated as equalities only fail to be equalities if the statistic $\Lambda^k_{t_k}$ overshoots the relevant threshold $\gamma_U$ or $\gamma_L$, rather than hitting it exactly. %\bdr{IGNORE: This was alluded to in Remark 1, but did we really establish it with any weight?  Might be good to make the ``no overshoots'' the axiom and use a citation to justify the tight inequalities \eqref{eq:gl-approx} and \eqref{eq:gu-approx}}
That is, while we will always have $\Lambda^k_{t_k} \geq \gamma_U$ (or $\Lambda^k_{t_k} < \gamma_L$) at the end of any stage, Problem~2 is derived by assuming $\Lambda^k_{t_k} = \gamma_U$ (or $\Lambda^k_{t_k} = \gamma_L$). This approximation is reasonable when the expected overshoot of a particular threshold is small with respect to the threshold itself, i.e., if $\mathbb{E}\left[\frac{\Lambda^k_{t_k}-\gamma_U}{\gamma_U} \mid \Lambda^k_{t_k} \geq \gamma_U \right]$ and $\mathbb{E}\left[\frac{\Lambda^k_{t_k}-\gamma_L}{\gamma_L} \mid \Lambda^k_{t_k} < \gamma_L \right]$ are small. The remainder of this section shows that 
these terms are indeed small when a problem satisfies the conditions in Remark~1.

%\mh{In a conditional expectation, use the command \texttt{\textbackslash mid} rather than just the \texttt{|} character. It makes the spacing better.}
\subsection{The case of ``close'' $f_0$ and $f_1$}
%\subsection{\bdr{The case of ``close'' $f_0$ and $f_1$ \sout{As $f_0$ and $f_1$ become sufficiently close, $\mathbb{E}[\Lambda_{k_i}-\gamma_U | \Lambda_{k_i} \geq \gamma_U]$ and $\mathbb{E}[\Lambda_{k_i}-\gamma_L | \Lambda_{k_i} < \gamma_L]$ approach zero.}}}

Here the phrase ``sufficiently close'' means the KL divergences $D(f_1||f_0)$ and $D(f_0||f_1)$ are small. Because $\mathbb{E}[\Lambda^k_{t_k}-\gamma_U | \Lambda^k_{t_k} \geq \gamma_U]  \leq \mathbb{E}[\Lambda^k_{t_k}-\Lambda^k_{t_k-1} | \Lambda^k_{t_k} \geq \gamma_U]$, we can see from the update law for $\Lambda^k$ in Algorithm~1 and the definition of the KL divergences that this expected overshoot approaches zero as $D(f_1 || f_0)$ and $D(f_0 || f_1)$ approach zero, as desired.
%This makes sense, as $\Lambda^k_t - \Lambda^k_{t-1}$ corresponds to the amount of information gained from the observation made at time $t$. If $f_0$ and $f_1$ are close, then we gain only a small amount of information per observation. If each observation only increments $\Lambda^k$ by a small amount, the overshoot of the relevant threshold will be small. We expect to have this condition in any problem of interest: if $f_0$ and $f_1$ are easy to distinguish from one another with only a few observations, then the analysis provided in this paper or in any other quickest search algorithm is unnecessary.
        
\subsection{The case of small $\epsilon$}
%\subsection{\bdr{The case of small $\epsilon$ \sout{As $\epsilon$ becomes sufficiently small, $\gamma^*_U$ approaches $\infty$.}}}

As $\epsilon$ shrinks, we must enforce a smaller error probability $P(H^{k_{\tau}} = H_0)$. From its definition, a smaller error probability directly implies a larger value of $\frac{1-\beta}{\alpha}$, which implies a larger value of $\gamma_U^*$. This relationship is intuitive: while both $\gamma_U$ and $\gamma_L$ affect $P(H^{k_{\tau}} = H_0)$, the effect of $\gamma_U$ is significantly greater since the algorithm only terminates at data stream $k$ if $\Lambda^k_{t_k} \geq \gamma_U$. 
Having a large $\gamma^*_U$ means the expected overshoots are small, as desired. 
%\bdr{I don't think we need the rest of this paragraph} Requiring a small $\epsilon$ is another condition we expect to have in any interesting problem: if $\epsilon$ is large, this implies there is little penalty to incorrectly identifying a target stream, and any algorithm will tend towards simply identifying one of the first data streams it observes as a target. \bdr{Is the rest of this paragraph necessary?} In fact, if $\epsilon \geq \prior$, then the optimal algorithm is actually to take no observations whatsoever and flag a data stream at random, since in this scenario $P(H^{k_{\tau}} = H_0) = \prior \leq \epsilon$. Note that our algorithm captures this behavior: if $\epsilon \geq \prior$, then $\gamma^*_U < 0$, and because $\Lambda^1_0 = 0$ our algorithm immediately terminates.

\subsection{The case of large $\bl$}
%\subsection{\bdr{The case of large $\bl$ \sout{As $\bl$ becomes sufficiently small, $\gamma_L^*$ approaches $-\infty$.}}}

The relationship between $\bl$ and $\gamma^*_L$ is perhaps the most interesting one. Consider the high-level goal of our analysis: to minimize the number of switches our algorithm makes before finding and identifying (hopefully correctly) a target data stream. The requirement that we find a target data stream (with probability $1-\epsilon$) means that we specifically want to avoid switching away from a target data stream. In other words, the goal is to reduce $\beta$. As with $P(H^{k_{\tau}} = H_0)$, $\beta$ depends on both $\gamma_U$ and $\gamma_L$, but the effect of $\gamma_L$ is significantly greater as the algorithm only switches away from stream $k$ if $\Lambda^k_{t_k} < \gamma_L$. Having a very negative $\gamma_L^*$ means the expected  overshoots are close to zero, as desired. %\bdr{I don't think we need the rest of this paragraph.} 

The purpose of this analysis is to address non-trivial switching costs.
%, so 
% As the purpose of this analysis is to address switching costs 
% we expect our settings to have switching costs large enough that they cannot be neglected. 
However, we do note that our rule for selecting $\gamma^*_L$ is optimal as $\bl$ approaches zero as well. Recall from Section~\ref{sec:prelim} and~\cite{lai2011quickest} that the true optimal value of $\gamma_L$ for $\bl = 0$ (i.e., the value that minimizes~\eqref{eqn:optprob}) is $\gamma_L = 0$. As $\bl \rightarrow 0$, our value of $\gamma_L^*$ found by solving Problem~2 also approaches zero, implying that the algorithm described in~\cite{lai2011quickest} is a special case of the one we develop here.

\section{Numerical Results} \label{sec:simulations}
We now compare the performance of our algorithm with the one described in~\cite{lai2011quickest}, which is the closest comparable algorithm, in a setting where switching costs are present. While the algorithm in~\cite{lai2011quickest} is optimal for the case where $\bl = 0$, it does not take into account switching costs. Furthermore, the optimal threshold $\gamma_U$ cannot be directly calculated for that algorithm, and must be estimated via Monte Carlo simulations. In contrast, our algorithm directly accounts for switching costs and uses thresholds that can be directly calculated.

In this setting, target data streams occur with prior probability $\prior = 0.1$, and obey the distribution $F_1 = \mathcal{N}(0,1)$. Nominal data streams obey $F_0 = \mathcal{N}(0,1.5)$, and we choose $\epsilon = 0.01$ to be our maximum tolerable error rate. Switching costs are drawn from a gamma distribution $\lambda_k \sim \Gamma(a,b)$, which has $\bl = \frac{a}{b}$. We will compare the performances of both algorithms across a range of values of $\bl$, by keeping $b = 1$ constant and exploring $a \in [0,5]$. The algorithm from~\cite{lai2011quickest} uses thresholds $\gamma_L = 0$ and $\gamma_U = 6.130$ for this problem regardless of the switching costs. For the algorithm described in this paper, $\gamma_U = 6.794$ regardless of switching costs, and $\gamma_L$ is chosen by solving Problem 2 with $\bl$. The values of $\gamma_L$ used and their corresponding values of $\bl$ are plotted in Figure~2.

\begin{figure}[htp]
    \centering
    \includegraphics[draft = false,width = 8cm]{Real_dL.PNG}
    \caption{The larger $\overline{\lambda}$ is, the lower $\gamma_L^*$ is in Algorithm~1.}
\end{figure}

The algorithm from~\cite{lai2011quickest} achieves $\mathbb{E}_{\prior}[\tau] = 109.42$ and $\mathbb{E}_{\prior}[k_{\tau}-1] = 42.15$, and our algorithm achieves the similar numbers $\mathbb{E}_{\prior}[\tau] = 113.21$ and $\mathbb{E}_{\prior}[k_{\tau}-1] = 42.04$ for $\bl = 0$. We also note that our algorithm achieves an error rate of $0.005$. As $\bl$ grows, our goal is to reduce the combined observation/switching cost formulated in~\eqref{eqn:optprob} by reducing the number of switches. In Figure~3, we see that this is achieved. As $\gamma_L$ is varied to account for higher values of $\bl$, we see that the number of expected switches drops significantly. 
%Note that, given the prior $\prior = 0.1$, a ``perfect'' algorithm (one that perfectly identifies all nominal and target streams and achieves $\alpha, \beta = 0$) would have an expected number of switches of $10$, since on average the algorithm would have to scan through $10$ streams before encountering its first target stream.

\begin{figure}[htp]
    \centering
    \includegraphics[draft = false,width = 8cm]{Switch_Clean.PNG}
    \caption{The expected number of switches under Algorithm~1 for different values of $\bar{\lambda}$.}
\end{figure}

The combined observation/switching costs for both algorithms are plotted in Figure~4. We can see that in the large $\bl$ region, our algorithm significantly outperforms the algorithm from~\cite{lai2011quickest}. The algorithms perform comparably up until around $\bl = 1$, after which the cost for our algorithm is always lower than the algorithm in~\cite{lai2011quickest}. Specifically, the observation/switching cost for the algorithm from~\cite{lai2011quickest} grows linearly with $\bl$. It increases by $42.15$ for every unit increase of $\bl$, since $42.15$ is the expected number of switches under that algorithm. In contrast, in the regime explored in this simulation, the use of Algorithm~1 increases the observation/switching cost at a rate of around $16.3$ per unit increase of $\bl$, meaning the cost of Algorithm~1 grows at a rate $61.3\%$ slower than the~\cite{lai2011quickest} algorithm.

\begin{figure}[htp]
    \centering
    \includegraphics[draft = false,width = 8cm]{Cost_Clean.PNG}
    \caption{The combined observation/switching costs for our Algorithm~1 (blue solid line) and the algorithm from~\cite{lai2011quickest} (orange dashed line).}
\end{figure}

\section{Conclusion} \label{sec:con}
In this paper we introduced an algorithm which performs online  anomaly search over many sequences with switching costs, with parameters that can be directly calculated, and almost uniform improvement over the best comparable method that does not account for switching costs \cite{lai2011quickest}. We showed that the approximations used to derive this algorithm are accurate for problems of interest, and demonstrated the success of this algorithm with numerical simulations. Future work will embed the problem into a physical setting and perform optimal routing and control for a physical vehicle that incorporates the cost of switching that is due, e.g., to the downtime incurred by moving.

% use section* for acknowledgment
\section*{Acknowledgements}
The views and opinions expressed in this article are those of the authors and do not necessarily reflect the official policy or position of any agency of the U.S. government. Examples of analysis performed within this article are only examples. Assumptions made within the analysis are also not reflective of the position of any U.S. Government entity. The Public Affairs approval number of this document is AFRL-2023-0996.  % Thanks to Doug Cochran for many discussions leading to some helpful insights. %AFRL-2021-4155.




% \IEEEPARstart{T}{his} document is a template for \LaTeX. If you are reading a paper or PDF version of this document, please download the electronic file, trans\_jour.tex, from the IEEE Web site at \url{http://www.ieee.org/authortools/trans_jour.tex} so you can use it to prepare your manuscript. If you would prefer to use LaTeX, download IEEE's LaTeX style and sample files from the same Web page. You can also explore using the Overleaf editor at {https://www.overleaf.com/blog/278-how-to-use-overleaf-with-ieee-collabratec-your-quick-guide-to-getting-started\#.xsVp6tpPkrKM9}

% If your paper is intended for a conference, please contact your conference editor concerning acceptable word processor formats for your particular conference.  


% \section{Guidelines For Manuscript Preparation}


% The IEEEtran\_HOWTO.pdf is the complete guide of \LaTeX\ for manuscript preparation included with this stuff. 


% \subsection{Information for Authors}

% {\em IEEE Signal Processing Letters} allows only four-page articles. A fifth page is allowed for ``References'' only, though ``References'' may begin before the fifth page. Author biographies or photographs are not allowed in Signal Processing Letters. Please review the Information for Authors at for {\em IEEE Signal Processing Letters:} https://signalprocessingsociety.org/publications-resources/ieee-signal-processing-letters/information-authors-spl



% \section{Guidelines for Graphics Preparation and Submission}
% \label{sec:guidelines}

% \subsection{Types of Graphics}
% The following list outlines the different types of graphics published in 
% {\it IEEE Signal Processing Letters}. They are categorized based on their construction, and use of 
% color/shades of gray:

% \subsubsection{Color/Grayscale figures}
% {Figures that are meant to appear in color, or shades of black/gray. Such 
% figures may include photographs, illustrations, multicolor graphs, and 
% flowcharts.}

% \subsubsection{Line Art figures}
% {Figures that are composed of only black lines and shapes. These figures 
% should have no shades or half-tones of gray, only black and white.}

% \subsubsection{Tables}
% {Data charts which are typically black and white, but sometimes include 
% color.}



% \subsection{Multipart figures}
% Figures compiled of more than one sub-figure presented side-by-side, or 
% stacked. If a multipart figure is made up of multiple figure
% types (one part is lineart, and another is grayscale or color) the figure 
% should meet the stricter guidelines.

% \subsection{File Formats For Graphics}\label{formats}
% Format and save your graphics using a suitable graphics processing program 
% that will allow you to create the images as PostScript (PS), Encapsulated 
% PostScript (.EPS), Tagged Image File Format (.TIFF), Portable Document 
% Format (.PDF), Portable Network Graphics (.PNG), or Metapost (.MPS), sizes them, and adjusts 
% the resolution settings. When 
% submitting your final paper, your graphics should all be submitted 
% individually in one of these formats along with the manuscript.

% \subsection{Sizing of Graphics}
% Most charts, graphs, and tables are one column wide (3.5 inches/88 
% millimeters/21 picas) or page wide (7.16 inches/181 millimeters/43 
% picas). The maximum depth a graphic can be is 8.5 inches (216 millimeters/54
% picas). When choosing the depth of a graphic, please allow space for a 
% caption. Figures can be sized between column and page widths if the author 
% chooses, however it is recommended that figures are not sized less than 
% column width unless when necessary. 

% \begin{figure}
% \centerline{\includegraphics[width=\columnwidth]{fig1.png}}
% \caption{Magnetization as a function of applied field. Note that ``Fig.'' is abbreviated. There is a period after the figure number, followed by two spaces. It is good practice to explain the significance of the figure in the caption.}
% \end{figure}

% \begin{table}
% \caption{Units for Magnetic Properties}
% \label{table}
% \small
% \setlength{\tabcolsep}{3pt}
% \begin{tabular}{|p{25pt}|p{75pt}|p{110pt}|}
% \hline
% Symbol& 
% Quantity& 
% Conversion from Gaussian and \par CGS EMU to SI$^{\mathrm{a}}$ \\
% \hline
% $\Phi $& 
% Magnetic flux& 
% 1 Mx $\to  10^{-8}$ Wb $= 10^{-8}$ V $\cdot$ s \\
% $B$& 
% Magnetic flux density, \par magnetic induction& 
% 1 G $\to  10^{-4}$ T $= 10^{-4}$ Wb/m$^{2}$ \\
% $H$& 
% Magnetic field strength& 
% 1 Oe $\to  10^{-3}/(4\pi )$ A/m \\
% $m$& 
% Magnetic moment& 
% 1 erg/G $=$ 1 emu \par $\to 10^{-3}$ A $\cdot$ m$^{2} = 10^{-3}$ J/T \\
% $M$& 
% Magnetization& 
% 1 erg/(G $\cdot$ cm$^{3}) =$ 1 emu/cm$^{3}$ \par $\to 10^{-3}$ A/m \\
% 4$\pi M$& 
% Magnetization& 
% 1 G $\to  10^{-3}/(4\pi )$ A/m \\
% $\sigma $& 
% Specific magnetization& 
% 1 erg/(G $\cdot$ g) $=$ 1 emu/g $\to $ 1 A $\cdot$ m$^{2}$/kg \\
% $j$& 
% Magnetic dipole \par moment& 
% 1 erg/G $=$ 1 emu \par $\to 4\pi \times  10^{-10}$ Wb $\cdot$ m \\
% $J$& 
% Magnetic polarization& 
% 1 erg/(G $\cdot$ cm$^{3}) =$ 1 emu/cm$^{3}$ \par $\to 4\pi \times  10^{-4}$ T \\
% $\chi , \kappa $& 
% Susceptibility& 
% 1 $\to  4\pi $ \\
% $\chi_{\rho }$& 
% Mass susceptibility& 
% 1 cm$^{3}$/g $\to  4\pi \times  10^{-3}$ m$^{3}$/kg \\
% $\mu $& 
% Permeability& 
% 1 $\to  4\pi \times  10^{-7}$ H/m \par $= 4\pi \times  10^{-7}$ Wb/(A $\cdot$ m) \\
% $\mu_{r}$& 
% Relative permeability& 
% $\mu \to \mu_{r}$ \\
% $w, W$& 
% Energy density& 
% 1 erg/cm$^{3} \to  10^{-1}$ J/m$^{3}$ \\
% $N, D$& 
% Demagnetizing factor& 
% 1 $\to  1/(4\pi )$ \\
% \hline
% \multicolumn{3}{p{251pt}}{Vertical lines are optional in tables. Statements that serve as captions for 
% the entire table do not need footnote letters. }\\
% \multicolumn{3}{p{251pt}}{$^{\mathrm{a}}$Gaussian units are the same as cg emu for magnetostatics; Mx 
% $=$ maxwell, G $=$ gauss, Oe $=$ oersted; Wb $=$ weber, V $=$ volt, s $=$ 
% second, T $=$ tesla, m $=$ meter, A $=$ ampere, J $=$ joule, kg $=$ 
% kilogram, H $=$ henry.}
% \end{tabular}
% \label{tab1}
% \end{table}


% \subsection{Resolution }
% The proper resolution of your figures will depend on the type of figure it 
% is as defined in the ``Types of Figures'' section. Author photographs, 
% color, and grayscale figures should be at least 300dpi. Line art, including 
% tables should be a minimum of 600dpi.

% \subsection{Vector Art}
% In order to preserve the figures' integrity across multiple computer 
% platforms, we accept files in the following formats: .EPS/.PDF/.PS. All 
% fonts must be embedded or text converted to outlines in order to achieve the 
% best-quality results.


% \subsection{Accepted Fonts Within Figures}
% When preparing your graphics IEEE suggests that you use of one of the 
% following Open Type fonts: Times New Roman, Helvetica, Arial, Cambria, and 
% Symbol. If you are supplying EPS, PS, or PDF files all fonts must be 
% embedded. Some fonts may only be native to your operating system; without 
% the fonts embedded, parts of the graphic may be distorted or missing.

% A safe option when finalizing your figures is to strip out the fonts before 
% you save the files, creating ``outline'' type. This converts fonts to 
% artwork what will appear uniformly on any screen.

% \subsection{Using Labels Within Figures}

% \subsubsection{Figure Axis labels }
% Figure axis labels are often a source of confusion. Use words rather than 
% symbols. As an example, write the quantity ``Magnetization,'' or 
% ``Magnetization M,'' not just ``M.'' Put units in parentheses. Do not label 
% axes only with units. As in Fig. 1, for example, write ``Magnetization 
% (A/m)'' or ``Magnetization (A$\cdot$m$^{-1}$),'' not just ``A/m.'' Do not label axes with a ratio of quantities and 
% units. For example, write ``Temperature (K),'' not ``Temperature/K.'' 

% Multipliers can be especially confusing. Write ``Magnetization (kA/m)'' or 
% ``Magnetization (10$^{3}$ A/m).'' Do not write ``Magnetization 
% (A/m)$\,\times\,$1000'' because the reader would not know whether the top 
% axis label in Fig. 1 meant 16000 A/m or 0.016 A/m. Figure labels should be 
% legible, approximately 8 to 10 point type.

% \subsubsection{Subfigure Labels in Multipart Figures and Tables}
% Multipart figures should be combined and labeled before final submission. 
% Labels should appear centered below each subfigure in 8 point Times New 
% Roman font in the format of (a) (b) (c). 

% \subsection{File Naming}
% Figures (line artwork or photographs) should be named starting with the 
% first 5 letters of the author's last name. The next characters in the 
% filename should be the number that represents the sequential 
% location of this image in your article. For example, in author 
% ``Anderson's'' paper, the first three figures would be named ander1.tif, 
% ander2.tif, and ander3.ps.

% Tables should contain only the body of the table (not the caption) and 
% should be named similarly to figures, except that `.t' is inserted 
% in-between the author's name and the table number. For example, author 
% Anderson's first three tables would be named ander.t1.tif, ander.t2.ps, 
% ander.t3.eps.

% \subsection{Referencing a Figure or Table Within Your Paper}
% When referencing your figures and tables within your paper, use the 
% abbreviation ``Fig.'' even at the beginning of a sentence. Do not abbreviate 
% ``Table.'' Tables should be numbered with Roman Numerals.

% \subsection{Checking Your Figures: The IEEE Graphics Analyzer}
% The IEEE Graphics Analyzer enables authors to pre-screen their graphics for 
% compliance with IEEE Transactions and Journals standards before submission. 
% The online tool, located at
% \underline{http://graphicsqc.ieee.org/}, allows authors to 
% upload their graphics in order to check that each file is the correct file 
% format, resolution, size and colorspace; that no fonts are missing or 
% corrupt; that figures are not compiled in layers or have transparency, and 
% that they are named according to the IEEE Transactions and Journals naming 
% convention. At the end of this automated process, authors are provided with 
% a detailed report on each graphic within the web applet, as well as by 
% email.

% For more information on using the Graphics Analyzer or any other graphics 
% related topic, contact the IEEE Graphics Help Desk by e-mail at 
% graphics@ieee.org.

% \subsection{Submitting Your Graphics}
% Because IEEE will do the final formatting of your paper,
% you do not need to position figures and tables at the top and bottom of each 
% column. In fact, all figures, figure captions, and tables can be placed at 
% the end of your paper. In addition to, or even in lieu of submitting figures 
% within your final manuscript, figures should be submitted individually, 
% separate from the manuscript in one of the file formats listed above in 
% Section \ref{formats}. Place figure captions below the figures; place table titles 
% above the tables. Please do not include captions as part of the figures, or 
% put them in ``text boxes'' linked to the figures. Also, do not place borders 
% around the outside of your figures.

% \subsection{Color Processing/Printing in IEEE Journals}
% All IEEE Transactions, Journals, and Letters allow an author to publish 
% color figures on IEEE Xplore\textregistered\ at no charge, and automatically 
% convert them to grayscale for print versions. In most journals, figures and 
% tables may alternatively be printed in color if an author chooses to do so. 
% Please note that this service comes at an extra expense to the author. If 
% you intend to have print color graphics, include a note with your final 
% paper indicating which figures or tables you would like to be handled that 
% way, and stating that you are willing to pay the additional fee.


% \section{Conclusion}

% A conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A conclusion might elaborate on the importance of the work or suggest applications and extensions. 

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in American English is without an ``e'' after the ``g.'' Use the singular heading even if you have many acknowledgments. Avoid expressions such as ``One of us (S.B.A.) would like to thank . . . .'' Instead, write F. A. Author thanks ... . In most cases, sponsor and financial support acknowledgments are placed in the unnumbered footnote on the first page, not here.

% \section*{References and Footnotes}

% \subsection{References}

% References need not be cited in text. When they are, they appear on the line, in square brackets, inside the punctuation.  Multiple references are each numbered with separate brackets. When citing a section in a book, please give the relevant page numbers. In text, refer simply to the reference number. Do not use ``Ref.'' or ``reference'' except at the beginning of a sentence: ``Reference [3] shows . . . .'' Please do not use automatic endnotes in {\em Word}, rather, type the reference list at the end of the paper using the ``References'' style.

% Reference numbers are set flush left and form a column of their own, hanging out beyond the body of the reference. The reference numbers are on the line, enclosed in square brackets. In all references, the given name of the author or editor is abbreviated to the initial only and precedes the last name. Use them all; use {\em et al.} only if names are not given. Use commas around Jr., Sr., and III in names. Abbreviate conference titles.  When citing IEEE transactions, provide the issue number, page range, volume number, year, and/or month if available. When referencing a patent, provide the day and the month of issue, or application. References may not include all information; please obtain and include relevant information. Do not combine references. There must be only one reference with each number. If there is a URL included with the print reference, it can be included at the end of the reference.

% Other than books, capitalize only the first word in a paper title, except for proper nouns and element symbols. For papers published in translation journals, please give the English citation first, followed by the original foreign-language citation. See the end of this document for formats and examples of common references. For a complete discussion of references and their formats, see the IEEE style manual at www.ieee.org/authortools.

% \subsection{Footnotes}

% Number footnotes separately in superscripts (Insert $\mid$ Footnote).\footnote{It is recommended that footnotes be avoided (except for the unnumbered footnote with the receipt date on the first page). Instead, try to integrate the footnote information into the text.}  Place the actual footnote at the bottom of the column in which it is cited; do not put footnotes in the reference list (endnotes). Use letters for table footnotes (see Table I). 


% \section*{References}

\bibliographystyle{IEEEtran}
\bibliography{Biblio}

% \subsection*{Basic format for books:}

% J. K. Author, ``Title of chapter in the book,'' in {\em Title of His Published Book}, xth ed. City of Publisher, (only U.S. State), Country: Abbrev. of Publisher, year, ch. x, sec. x, pp. xxx--xxx.

% \subsection*{Examples:}
% \def\refname{}
% \begin{thebibliography}{34}

% \bibitem{}G. O. Young, ``Synthetic structure of industrial plastics,'' in {\em Plastics}, 2nd ed., vol. 3, J. Peters, Ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15--64.

% \bibitem{}W.-K. Chen, {\it Linear Networks and Systems}. Belmont, CA, USA: Wadsworth, 1993, pp. 123--135.

% \end{thebibliography}

% \subsection*{Basic format for periodicals:}

% J. K. Author, ``Name of paper,'' Abbrev. Title of Periodical, vol. x,   no. x, pp. xxx--xxx, Abbrev. Month, year, DOI. 10.1109.XXX.123--456.

% \subsection*{Examples:}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{2}

% \bibitem{}J. U. Duncombe, ``Infrared navigation Part I: An assessment of feasibility,'' {\em IEEE Trans. Electron Devices}, vol. ED-11, no. 1, pp. 34--39, Jan. 1959,10.1109/TED.2016.2628402.

% \bibitem{}E. P. Wigner, ``Theory of traveling-wave optical laser,''
% {\em Phys. Rev.},  vol. 134, pp. A635--A646, Dec. 1965.

% \bibitem{}E. H. Miller, ``A note on reflector arrays,'' {\em IEEE Trans. Antennas Propagat.}, to be published.
% \end{thebibliography}


% \subsection*{Basic format for reports:}

% J. K. Author, ``Title of report,'' Abbrev. Name of Co., City of Co., Abbrev. State, Country, Rep. xxx, year.

% \subsection*{Examples:}
% \begin{thebibliography}{34}
% \setcounter{enumiv}{5}

% \bibitem{} E. E. Reber, R. L. Michell, and C. J. Carter, ``Oxygen absorption in the earths atmosphere,'' Aerospace Corp., Los Angeles, CA, USA, Tech. Rep. TR-0200 (4230-46)-3, Nov. 1988.

% \bibitem{} J. H. Davis and J. R. Cogdell, ``Calibration program for the 16-foot antenna,'' Elect. Eng. Res. Lab., Univ. Texas, Austin, TX, USA, Tech. Memo. NGL-006-69-3, Nov. 15, 1987.
% \end{thebibliography}

% \subsection*{Basic format for handbooks:}

% {\em Name of Manual/Handbook}, x ed., Abbrev. Name of Co., City of Co., Abbrev. State, Country, year, pp. xxx--xxx.

% \subsection*{Examples:}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{7}

% \bibitem{} {\em Transmission Systems for Communications}, 3rd ed., Western Electric Co., Winston-Salem, NC, USA, 1985, pp. 44--60.

% \bibitem{} {\em Motorola Semiconductor Data Manual}, Motorola Semiconductor Products Inc., Phoenix, AZ, USA, 1989.
% \end{thebibliography}

% \subsection*{Basic format for books (when available online):}

% J. K. Author, ``Title of chapter in the book,'' in {\em Title of Published Book}, xth ed. City of Publisher, State, Country: Abbrev. of Publisher, year, ch. x, sec. x, pp. xxx xxx. [Online]. Available: http://www.web.com 

% \subsection*{Examples:}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{9}

% \bibitem{}G. O. Young, ``Synthetic structure of industrial plastics,'' in Plastics, vol. 3, Polymers of Hexadromicon, J. Peters, Ed., 2nd ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15--64. [Online]. Available: http://www.bookref.com. 

% \bibitem{} {\em The Founders Constitution}, Philip B. Kurland and Ralph Lerner, eds., Chicago, IL, USA: Univ. Chicago Press, 1987. [Online]. Available: http://press-pubs.uchicago.edu/founders/

% \bibitem{} The Terahertz Wave eBook. ZOmega Terahertz Corp., 2014. [Online]. Available: http://dl.z-thz.com/eBook/zomega\_ebook\_pdf\_1206\_sr.pdf. Accessed on: May 19, 2014. 

% \bibitem{} Philip B. Kurland and Ralph Lerner, eds., {\em The Founders Constitution}. Chicago, IL, USA: Univ. of Chicago Press, 1987, Accessed on: Feb. 28, 2010, [Online] Available: http://press-pubs.uchicago.edu/founders/ 
% \end{thebibliography}

% \subsection*{Basic format for journals (when available online):}

% J. K. Author, ``Name of paper,'' {\em Abbrev. Title of Periodical}, vol. x, no. x, pp. xxx--xxx, Abbrev. Month, year. Accessed on: Month, Day, year, doi: 10.1109.XXX.123456, [Online].

% \subsection*{Examples:}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{13}

% \bibitem{}J. S. Turner, ``New directions in communications,'' {\em IEEE J. Sel. Areas Commun.}, vol. 13, no. 1, pp. 11--23, Jan. 1995. 

% \bibitem{} W. P. Risk, G. S. Kino, and H. J. Shaw, ``Fiber-optic frequency shifter using a surface acoustic wave incident at an oblique angle,'' {\em Opt. Lett.}, vol. 11, no. 2, pp. 115--117, Feb. 1986.

% \bibitem{} P. Kopyt {\em et al.}, ``Electric properties of graphene-based conductive layers from DC up to terahertz range,'' {\em IEEE THz Sci. Technol.}, to be published. doi: 10.1109/TTHZ.2016.2544142.
% \end{thebibliography}

% \subsection*{Basic format for papers presented at conferences (when available online):}

% J.K. Author. (year, month). Title. presented at abbrev. conference title. [Type of Medium]. Available: site/path/file

% \subsection*{Example:}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{16}

% \bibitem{}PROCESS Corporation, Boston, MA, USA. Intranets: Internet technologies deployed behind the firewall for corporate productivity. Presented at INET96 Annual Meeting. [Online]. Available: http://home.process.com/Intranets/wp2.htp
% \end{thebibliography}

% \subsection*{Basic format for reports  and  handbooks (when available online):}
  
% J. K. Author. ``Title of report,'' Company. City, State, Country. Rep. no., (optional: vol./issue), Date. [Online] Available: site/path/file 

% \subsection*{Examples:}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{17}

% \bibitem{}R. J. Hijmans and J. van Etten, ``Raster: Geographic analysis and modeling with raster data,'' R Package Version 2.0-12, Jan. 12, 2012. [Online]. Available: http://CRAN.R-project.org/package=raster 

% \bibitem{}Teralyzer. Lytera UG, Kirchhain, Germany [Online]. Available: http://www.lytera.de/Terahertz\_THz\_Spectroscopy.php?id=home, Accessed on: Jun. 5, 2014.
% \end{thebibliography}

% \subsection*{Basic format for computer programs and electronic documents (when available online):}

% Legislative body. Number of Congress, Session. (year, month day). {\em Number of bill or resolution, Title}. [Type of medium]. Available: site/path/file
% {\em NOTE:} ISO recommends that capitalization follow the accepted practice for the language or script in which the information is given.

% \subsection*{Example:}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{19}

% \bibitem{}U. S. House. 102nd Congress, 1st Session. (1991, Jan. 11). {\em H. Con. Res. 1, Sense of the Congress on Approval of Military Action}. [Online]. Available: LEXIS Library: GENFED File: BILLS 
% \end{thebibliography}

% \subsection*{Basic format for patents (when available online):}

% Name of the invention, by inventors name. (year, month day). Patent Number [Type of medium]. Available:site/path/file

% \subsection*{Example:}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{20}

% \bibitem{}Musical tooth brush with mirror, by L. M. R. Brooks. (1992, May 19). Patent D 326 189
% [Online]. Available: NEXIS Library: LEXPAT File:   DES 

% \end{thebibliography}

% \subsection*{Basic format for conference proceedings (published):}

% J. K. Author, ``Title of paper,'' in {\em Abbreviated Name of Conf.}, City of Conf., Abbrev. State (if given), Country, year, pp. xxx--xxx.

% \subsection*{Example:}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{21}

% \bibitem{}D. B. Payne and J. R. Stern, ``Wavelength-switched passively coupled single-mode optical network,'' in {\em Proc. IOOC-ECOC}, Boston, MA, USA, 1985,
% pp. 585--590.

% \end{thebibliography}

% \subsection*{Example for papers presented at conferences (unpublished):}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{22}

% \bibitem{}D. E behard and E. Voges, ``Digital single sideband detection for inter ferometric sensors,'' presented at the {\em 2nd Int. Conf. Optical Fiber Sensors}, Stuttgart, Germany, Jan. 2--5, 1984.
% \end{thebibliography}

% \subsection*{Basic formatfor patents:}

% J. K. Author, ``Title of patent,'' U. S. Patent x xxx xxx, Abbrev. Month, day, year.

% \subsection*{Example:}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{23}

% \bibitem{}G. Brandli and M. Dick, ``Alternating current fed power supply,'' U. S. Patent 4 084 217, Nov. 4, 1978.
% \end{thebibliography}

% \subsection*{Basic format for theses (M.S.) and dissertations (Ph.D.):}

% a) J. K. Author, ``Title of thesis,'' M. S. thesis, Abbrev. Dept., Abbrev. Univ., City of Univ., Abbrev. State, year.

% b) J. K. Author, ``Title of dissertation,'' Ph.D. dissertation, Abbrev. Dept., Abbrev. Univ., City of Univ., Abbrev. State, year.

% \subsection*{Examples:}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{24}

% \bibitem{}J. O. Williams, ``Narrow-band analyzer,'' Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, USA, 1993.

% \bibitem{}N. Kawasaki, ``Parametric study of thermal and chemical nonequilibrium nozzle flow,'' M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
% \end{thebibliography}

% \subsection*{Basic format for the most common types of unpublished references:}

% a) J. K. Author, private communication, Abbrev. Month, year.

% b) J. K. Author, ``Title of paper,'' unpublished.

% c) J. K. Author, ``Title of paper,'' to be published.

% \subsection*{Examples:}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{26}

% \bibitem{}A. Harrison, private communication, May 1995.

% \bibitem{}B. Smith, ``An approach to graphs of linear forms,'' unpublished.

% \bibitem{}A. Brahms, ``Representation error for real numbers in binary computer arithmetic,'' IEEE Computer Group Repository, Paper R-67-85.
% \end{thebibliography}

% \subsection*{Basic formats for standards:}

% a) {\em Title of Standard}, Standard number, date.

% b) {\em Title of Standard}, Standard number, Corporate author, location, date.

% \subsection*{Examples:}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{29}


% \bibitem{}IEEE Criteria for Class IE Electric Systems, IEEE Standard 308, 1969.

% \bibitem{} Letter Symbols for Quantities, ANSI Standard Y10.5-1968.
% \end{thebibliography}

% \subsection*{Article number in reference examples:}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{31}

% \bibitem{}R. Fardel, M. Nagel, F. Nuesch, T. Lippert, and A. Wokaun, ``Fabrication of organic light emitting diode pixels by laser-assisted forward transfer,'' {\em Appl. Phys. Lett.}, vol. 91, no. 6, Aug. 2007, Art. no. 061103. 

% \bibitem{} J. Zhang and N. Tansu, ``Optical gain and laser characteristics of InGaN quantum wells on ternary InGaN substrates,'' {\em IEEE Photon.} J., vol. 5, no. 2, Apr. 2013, Art. no. 2600111
% \end{thebibliography}

% \subsection*{Example when using et al.:}

% \begin{thebibliography}{34}
% \setcounter{enumiv}{33}

% \bibitem{}S. Azodolmolky {\em et al.}, Experimental demonstration of an impairment aware network planning and operation tool for transparent/translucent optical networks,'' {\em J. Lightw. Technol.}, vol. 29, no. 4, pp. 439--448, Sep. 2011.
% \end{thebibliography}

\end{document}
