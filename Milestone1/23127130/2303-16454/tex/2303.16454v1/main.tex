\documentclass[10pt]{article}
\usepackage[nohead,margin=1.0in]{geometry}
\usepackage{amssymb, amsmath, amsthm,amsfonts}
\usepackage{graphicx,epsfig}
\usepackage[tight]{subfigure}
\usepackage{cite}
\usepackage{times}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{appendix}
\usepackage{color}
\usepackage{epstopdf}
\usepackage{mathrsfs}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage[colorlinks,linktocpage,linkcolor=blue]{hyperref}

\graphicspath{{./pics/}}
\usepackage{booktabs}
\usepackage{threeparttable}

\numberwithin{equation}{section}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}{Example}[section]
\allowdisplaybreaks
\def\n{\mathrm{\bf n}}
\def\P01{P_{\mathcal{A}}}

\def\dv{{\rm div}}
\def\bsgamma{\boldsymbol{\gamma}}
% title+author+address+email+acknowledgement+keywords


\title{Conductivity Imaging from Internal Measurements with Mixed Least-Squares Deep Neural Networks\thanks{The work of B. Jin is supported by UK EPSRC grant EP/T000864/1 and EP/V026259/1, and a start-up fund from The Chinese University of Hong Kong. The work of  Z. Zhou is supported by Hong Kong Research Grants Council (15303021) and an internal grant of Hong Kong Polytechnic University (Project ID: P0038888, Work Programme: ZVX3).}}

\author{Bangti Jin\thanks{Department of Mathematics, The Chinese University of Hong Kong, Shatin, New Territories, Hong Kong, P.R. China (\texttt{bangti.jin@gmail.com, btjin@math.cuhk.edu.hk}).} \and Xiyao Li\thanks{Department of Computer Science, University College London, Gower Street, London WC1E 6BT, UK (\texttt{xiyao.li.20@ucl.ac.uk})} \and Qimeng Quan\thanks{School of Mathematics and Statistics, Wuhan University, Wuhan 430072, P. R. China (\texttt{quanqm@whu.edu.cn})} \and Zhi Zhou\thanks{Department of Applied Mathematics,
The Hong Kong Polytechnic University, Kowloon, Hong Kong, P.R. China (\texttt{zhizhou@polyu.edu.hk})}}

\begin{document}

\maketitle

\begin{abstract}
In this work we develop a novel approach using deep neural networks to reconstruct the conductivity distribution in elliptic problems from one internal measurement. The approach is based on a mixed reformulation of the governing equation and utilizes the standard least-squares objective to approximate the conductivity and flux simultaneously, with deep neural networks as ansatz functions. We provide a thorough analysis of the neural network approximations for both continuous and empirical losses, including rigorous error estimates that are explicit in terms of the noise level, various penalty parameters and neural network architectural parameters (depth, width and parameter bound). We also provide extensive numerical experiments in two- and multi-dimensions to illustrate distinct features of the approach, e.g., excellent stability with respect to data noise and capability of solving high-dimensional problems.\vskip5pt
\noindent\textbf{Key words}: conductivity imaging, least-squares approach, neural network, generalization error, error estimate
\end{abstract}
%\tableofcontents

\section{Introduction}

The conductivity value varies widely with the composition and type of materials and its accurate imaging can provide valuable structural information about the object. This observation underpins several important imaging modalities, e.g., electrical impedance tomography, current density impedance imaging, and acousto-electrical tomography; see the works \cite{Bal:2013,WidlakScherzer:2012} for overviews on mathematical models and theory. In this work, we aim at identifying the conductivity distribution in elliptic problems from internal data using deep neural networks. Let $\Omega\subset \mathbb{R}^d$ be a simply connected open bounded domain with a smooth boundary $\partial\Omega$. Consider the following Neumann boundary value problem for the function $u$ 
\begin{equation}\label{equ:Neu problem}
	\left\{
	\begin{aligned}
		-\nabla\cdot(q\nabla u) &= f, \ &\mbox{in}&\ \Omega, \\
		q\partial_\n u&=g, \ &\mbox{on}&\ \partial\Omega,
	\end{aligned}
	\right.
\end{equation}
where $\n$ denotes the unit outward normal direction to the boundary $\partial\Omega$ and $\partial_\n$ denotes taking the normal derivative. The functions $f$ and $g$ in \eqref{equ:Neu problem} are the given source and flux, respectively, and satisfy the standard compatibility condition $\int_\Omega f\ {\rm d} x + \int_{\partial\Omega}g\ {\rm d} S = 0$ in order to ensure solvability, and the solution $u$ is unique under suitable normalization condition, e.g., $\int_\Omega u{\rm d} x=0$. The conductivity $q$ is assumed to belong to the following admissible set
\begin{equation*}
	\mathcal{A} = \{q\in H^1(\Omega): c_0\leq q\leq c_1 \mbox{ a.e. in } \Omega\},
\end{equation*}
with the constants $ 0<c_0<c_1<\infty $ being the lower and upper bounds on $q$, respectively. We use the notation $u(q)$
to explicitly indicate the dependence of the solution $u$ to problem \eqref{equ:Neu problem} on the coefficient $q$.

The concerned inverse problem is to recover the conductivity $q$ from an
internal observation of the solution $u$. It has been extensively studied in both engineering and mathematics communities. For
example, the model \eqref{equ:Neu problem} is often used to describe the behavior
of a confined inhomogeneous aquifer, where the variable $u$ represents the piezometric head, $f$
is the recharge, and $q$ is hydraulic conductivity (or transmissivity
in the two-dimensional case); see the works \cite{FrindPinder:1973,Yeh:1986} for extensive discussions
on parameter identifications in hydrology. Theoretically, H\"{o}lder type stability
estimates of the inverse problem were established under different settings \cite{Richter:1981,
Alessandrini:1986,bonito2017diffusion}. Numerically, the reconstruction can be
obtained using the regularized output least-squares approach \cite{Acar:1993,
ChenZou:1999,DeckelnickHinze:2012}, equation error approach \cite{Falk:1983,
Karkkainen:1997,AlJamalGockenbach:2012} and mixed type formulation \cite{KohnLowe:1988} etc.
Error bounds on the numerical approximation obtained by the Galerkin finite element method (FEM)
of the regularized formulation were established in \cite{WangZou:2010,jin2021error}.

In this work, we develop a new approach for conductivity reconstruction using deep
neural networks (DNNs). It is based on a least-squares mixed-type reformulation of the
governing equation, with an $H^1(\Omega)$
penalty on the unknown conductivity $q$. The mixed least-squares formulation was first proposed by Kohn and Lowe \cite{KohnLowe:1988} (and hence also known as the Kohn-Lowe approach), where both $q$ and $u$ were approximated using the Galerkin FEM. In our approach, we approximate both current density $\sigma$ and conductivity $q$ 
separately using two DNNs, adopt a least-squares objective for all the equality constraints, and minimize the overall loss with respect to the DNN parameters. The use of DNNs in place of FEM allows exploiting inductive bias and expressivity of DNNs for function approximations, which can be highly beneficial for numerical recovery. By leveraging the approximation theory of DNNs \cite{guhring2021approximation}, nonstandard energy argument \cite{KohnLowe:1988,jin2021error} and statistical
learning theory \cite{AnthonyBartlett:1999,BartlettMendelson:2002}, we derive
novel error bounds on the approximations in terms of the accuracy of
the observational data, DNN architecture (depth, width and parameter bound), and
the numbers of sampling points in the domain and on the boundary etc. This is carried out for both population and empirical losses (resulting from Monte Carlo quadrature of the integrals). These error bounds provide theoretical underpinnings for the approach. In practice,
the proposed approach is easy to implement, robust with respect to noise and can handle high-dimensional inverse
problems (e.g., $d=5$). For example, the approach can still yield reasonable approximations in the presence of
10\% noise in the data, as is confirmed by the extensive numerical experiments in
both low and high-dimensional settings. These distinct
features make the method highly attractive, and the numerical results clearly
show its significant potential for solving the inverse problem.
The development of the DNN formulations, error analysis and extensive numerical validation represent the main contributions of the present work.

In recent years, the use of DNNs for solving direct and inverse problems for PDEs has received a lot of attention; see \cite{EHanJentzen:2022,TanyuMaass:2022} for overviews. Existing neural inverse schemes using DNNs can roughly be divided into two groups: supervised approaches (see, e.g., \cite{SeoKimHarrach:2019,KhooYing:2019,GuoJiang:2021}) and unsupervised approaches (see, e.g., \cite{BaoYeZang:2020,BarSochen:2021,PakravanMistani:2021,XuDarve:2022,jin2022imaging}). Supervised methods exploit the availability of (abundant) paired training data to extract problem-specific features, and are concerned with learning forward operators or their (regularized) inverses.  %Within this paradigm, Khoo and Ying \cite{KhooYing:2019} proposed a novel neural network architecture, SwitchNet, for solving the wave equation based inverse scattering problems via constructing maps between the scatterers and the scattered field using training data. This is achieved via a careful analysis of the forward map from scatters to the far field pattern. 
%Seo et al. \cite{SeoKimHarrach:2019} developed a supervised approach for nonlinear inverse problems based on variational autoencoder using a low-dimensional manifold assumption, which allows converting the problem into a (conditionally) well-posed one, and demonstrated the idea on time difference electrical impedance tomography (EIT). Guo and Jiang \cite{GuoJiang:2021} developed a neural network enhancement strategy for the direct sampling method for EIT. 
In contrast, unsupervised approaches exploit expressivity of DNNs as universal function approximators for the unknown coefficient and the state, i.e., using DNNs as ansatz functions in the approximation scheme, which enjoy excellent approximation properties for high-dimensional functions (in favorable situations), and the associated inductive biases \cite{Rahaman:2019}. The works \cite{BaoYeZang:2020}
and \cite{BarSochen:2021} investigated image reconstruction in the classical EIT problem, using the weak and strong formulations (also with the $L^\infty$ norm consistency for the latter), respectively. The work \cite{jin2022imaging} applied the deep Ritz method to a least-gradient reformulation for the current density impedance imaging, and derived a generalization error for the loss function. The approach performs reasonably well for both full and partial interior current density data, and shows remarkable robustness against data noise. Pakravan et al \cite{PakravanMistani:2021} developed a hybrid approach, blending high expressivity of DNNs with the accuracy and reliability of traditional numerical methods for PDEs, and showed the approach for recovering the diffusion coefficient in one- and two-dimensional elliptic PDEs; see also \cite{CenJinQuanZhou:2023} for a hybrid DNN-FEM approach for recovering the conductivity coefficient in elliptic and parabolic problems, where the conductivity and state are approximated using DNNs and Galerkin FEM, respectively. The work \cite{CenJinQuanZhou:2023} aims at combining the strengths of neural network and classical Galerkin FEM, i.e., expressivity of DNNs and solid theoretical foundations of the FEM, and provides a thorough theoretical analysis. However, the approach is limited to low-dimensional problems. All these works have presented very encouraging empirical results for a range of PDE inverse problems, and clearly showed enormous potentials of DNNs in solving PDE inverse problems. Our approach follows the unsupervised paradigm, but unlike existing approaches, it employs a mixed formulation of the governing equation and thus differs greatly from the aforementioned ones. In addition, we have established rigorous error bounds on the approximation. Note that  the theoretical analysis of neural inverse schemes is largely elusive, due to the outstanding challenges, mostly associated with nonconvexity of the objective function and a lack of linear structure of the approximation space.

The rest of the paper is organized as follows. In Section \ref{sec:prelim} we recall preliminaries on neural networks, especially
approximation theory. In Section \ref{sec:Neumann}, we develop the reconstruction approach for a Neumann boundary value problem \eqref{equ:Neu problem}
based on a mixed formulation of the governing equation. Further we present an error analysis of the approach for both population and empirical losses, using tools from partial differential equations and statistical learning theory.
In Section \ref{sec:Diri}, we describe the extension of the approach to the case of a Dirichlet boundary value problem.
In Section \ref{sec:numer}, we present extensive numerical experiments to validate the effectiveness of the proposed approach, including highly challenging cases with large noise and high-dimensionality. Throughout, we denote by $W^{k,p}(\Omega)$ and $W^{k,p}_0(\Omega)$ the standard Sobolev spaces of order $k$ for any integer $k\geq0$ and real $p\geq1$, equipped with the norm $\|\cdot\|_{W^{k,p}(\Omega)}$. Further, we denote by $W^{-k,p'}(\Omega)$ the dual space of $W^{k,p}_0(\Omega)$, with the pair $(p,p')$ being the H{\"o}lder conjugate exponents. We also write $H^k(\Omega)$ and $H^k_{0}(\Omega)$ with the norm $\|\cdot\|_{H^k(\Omega)}$ if $p=2$ and write $L^p(\Omega)$ with the norm $\|\cdot\|_{L^p(\Omega)}$ if $k=0$. The spaces
on the boundary $\partial\Omega$ are defined similarly. The notation $(\cdot,\cdot)$ denotes the standard $L^2(\Omega)$ inner product. For a Banach space $B$, and the notation $B^d$ represent the $d$-fold product space. We denote by $c$ a generic constant not necessarily the same at each occurrence but it is always independent of the approximation accuracy $\epsilon$ of the DNN, the noise level $\delta$ and the penalty parameters ($\gamma_\sigma$, $\gamma_b$ and $\gamma_q$).

\section{Preliminaries on DNNs}\label{sec:prelim}
In this section, we describe useful notation and properties on fully connected feedforward
neural networks. Let $\{d_\ell\}_{\ell=0}^L \subset\mathbb{N}$ be fixed natural numbers with
the input dimensionality $d_0=d$ and output dimensionality $d_L$, and a parameterization $\Theta=
\{(A^{(\ell)},b^{(\ell)})_{\ell=1}^L\}$ consisting of weight matrices and bias vectors, with
$A^{(\ell)}=[W_{ij}^{(\ell)}]\in \mathbb{R}^{d_\ell\times d_{\ell-1}}$ and $b^{(\ell)}=
[b^{(\ell)}_i]\in\mathbb{R}^{d_{\ell}}$ the weight matrix and bias vector at the $\ell$-th layer.
Then a DNN function $v_\theta:= v^{(L)}:\Omega\subset\mathbb{R}^d\to\mathbb{R}^{d_L}$
realized by the parameter $\theta\in\Theta$ is defined recursively by
\begin{equation}\label{eqn:NN-realization}
\mbox{DNN realization:}\quad
\left\{\begin{aligned}
v^{(0)}&=x,\quad x\in\Omega\subset\mathbb{R}^d,\\		
v^{(\ell)}&=\rho(A^{(\ell)}v^{(\ell-1)}+b^{(\ell)}),\quad \ell=1,2,\cdots,L-1,\\
		v^{(L)}&=A^{(L)}v^{(L-1)}+b^{(L)},
	\end{aligned}\right.
\end{equation}
where the nonlinear activation function $\rho:\mathbb{R}\to\mathbb{R}$ is applied
componentwise to a vector. The DNN has a depth $L$ and width $W:=
\max_{\ell=0,\dots,L}(d_{\ell})$ and the total number of parameters $
\sum_{\ell=1}^L d_\ell d_{\ell-1}+d_{\ell}$.
Given the parametrization $\Theta$ (i.e., architecture) and the DNN realization, we denote the associated DNN function class $\mathcal{N}_\Theta$ by $\mathcal{N}_\Theta:=
\{v_\theta,\ \theta\in \Theta\}$. Throughout, we fix $\rho\equiv\tanh$: $x\to
\frac{e^x-e^{-x}}{e^x+e^{-x}}$ and denote the corresponding DNN as $\tanh$-DNN. The
following approximation result holds \cite[Proposition 4.8]{guhring2021approximation}.
\begin{lemma}\label{lem:tanh-approx}
Let $s\in\mathbb{N}_0$ and $p\in[1,\infty]$ be fixed, and $v\in W^{k,p}
(\Omega)$ with $\mathbb{N}\ni k\geq s+1$. Then for any $\epsilon>0$, there exists at least one
 $\theta\in\Theta$ with depth $O(\log(d+k))$ and number of nonzero weights $O
\big(\epsilon^{-\frac{d}{k-s-\mu (s=2)}}\big)$ and weight parameters bounded by $O(\epsilon^{-2-\frac{2(d/p+d+k+\mu(s=2))+d/p+d}{k-s-\mu(s=2)}})$ in the maximum norm, where $\mu>0$ is arbitrarily small,
such that the DNN realization $v_\theta\in \mathcal{N}_\Theta$ satisfies
\begin{equation*}
	\|v-v_\theta\|_{W^{s,p}(\Omega)} \leq \epsilon.
\end{equation*}
\end{lemma}

\begin{remark}
On the domain $\Omega=(0,1)^d$, Guhring and Raslan \cite[pp. 127-128]{guhring2021approximation} proved Lemma \ref{lem:tanh-approx} using two steps. They first approximate a function $v\in W^{k,p}(\Omega)$ by a localized Taylor polynomial $v_{\rm poly}$:
\begin{equation*}
    \|v-v_{\rm poly}\|_{W^{s,p}(\Omega)}\leq c_{\rm poly}N^{-(k-s-\mu(s=2))},
\end{equation*}
where the construction $($see \cite[Definition 4.4]{guhring2021approximation} for details$)$ of $v_{\rm poly}$ relies on an approximate partition of unity $($depending on $N\in\mathbb{N}$$)$ and the constant $c_{\rm poly}=c_{\rm poly}(d, p, k,s)>0$. Next they show that there exists a neural network parameter $\theta$, satisfying the conditions in Lemma \ref{lem:tanh-approx}, such that \cite[Lemma D.5]{guhring2021approximation}:
     \begin{equation*}
        \|v_{\rm poly}-v_\theta\|_{W^{s,p}(\Omega)} \leq c_{\rm NN}\|v\|_{W^{s,p}(\Omega)}\tilde{\epsilon},
     \end{equation*}
    where the constant $c_{\rm NN}=c_{\rm NN}(d,p,k,s)>0$ and  $\tilde{\epsilon}\in(0,\frac12)$.
    Now for small $\epsilon>0$, the desired estimate follows directly from the choice below
    \begin{equation*}
        N=\Big(\frac{\epsilon}{2c_{\rm poly}}\Big)^{-\frac{1}{k-s-\mu(s=2)}}\quad \mbox{and}\quad \tilde{\epsilon}=\frac{\epsilon}{2c_{\rm NN}\|v\|_{W^{s,p}(\Omega)}}.
    \end{equation*}
\end{remark}

We denote the set of DNNs of depth $L$, $N_\theta$ nonzero weights, and maximum bound $R$ on the parameter vector $\theta$ by
\begin{equation*}
    \mathcal{N}(L,N_\theta,R) =: \{v_\theta \mbox{ is an DNN of depth }L: \|\theta\|_{\ell^0}\leq N_\theta, \|\theta\|_{\ell^\infty}\leq R\},
\end{equation*}
where $\|\cdot\|_{\ell^0}$ and $\|\cdot\|_{\ell^\infty}$, denote respectively, the number of nonzero entries in and the maximum norm of a vector. Furthermore, for any $\epsilon>0$ and $p\geq 1$, we denote the DNN parameter set by $\mathfrak{P}_{p,\epsilon}$ for the set
\begin{equation*}
\mathcal{N}\Big(c\log(d+2), c\epsilon^{-\frac{d}{1-\mu}}, c \epsilon^{-2-\frac{2p+3d+3pd+2p\mu}{p(1-\mu)}}\Big).
\end{equation*}

Below, for a vector-valued function, we use the DNN function
class to approximate its components. This can be easily achieved by DNN
parallelization, which combines multiple $\tanh$-DNNs into one larger DNN.
Then an induction argument allow assembling multiple DNNs into one big DNN. Moreover,
the new DNN does not change the depth, and its width equals to the sum of that of subnetworks.
\begin{lemma}\label{lem:NN-paral}
Let $\bar \theta=\{(\bar A^{(\ell)},\bar b^{(\ell)})\}_{\ell=1}^L,\tilde\theta = \{(\tilde A^{(\ell)},\tilde b^{(\ell)})\}_{\ell=1}^L\in\Theta$,  let $\bar v$ and $\tilde v$ be their DNN realizations, and define $\theta=\{( A^{(\ell)}, b^{(\ell)})\}_{\ell=1}^L$ by
	\begin{align*}
		 A^{(1)}&=
		\begin{bmatrix}
		\bar A^{(1)}  \\
		\tilde A^{(1)}
		\end{bmatrix}\in\mathbb{R}^{2d_1\times d_0}, \
	     A^{(\ell)}=
	    \begin{bmatrix}
	   \bar A^{(\ell)} & 0 \\
	    	0            & \tilde A^{(\ell)}
	    \end{bmatrix}\in\mathbb{R}^{2d_\ell\times 2d_{\ell-1}},\quad  \ell=2,\dots,L; \\
		 b^{(\ell)} &=
		\begin{bmatrix}
	\bar b^{(\ell)} \\
		\tilde b^{(\ell)}
		\end{bmatrix}\in \mathbb{R}^{2d_\ell}, \quad \ell=1,\dots,L.
	\end{align*}
Then $ v_{\bar \theta}=(\bar v_\theta,\tilde v_{\tilde \theta})^\top $ is the DNN realization of $\theta$,
of depth $L$ and width $2{W}$.
\end{lemma}


\section{Inverse conductivity problem in the Neumann case}
\label{sec:Neumann}

In this section we discuss the inverse conductivity problem for the
Neumann boundary value problem \eqref{equ:Neu problem}.

\subsection{Mixed formulation and its DNN approximation}
To develop the reconstruction method, we let $\sigma=q\nabla u$ and rewrite problem \eqref{equ:Neu problem} into a first-order system
\begin{equation}\label{eqn:mixed-Neum}
	\left\{\begin{aligned}
		\sigma & = q\nabla u,&& \mbox{in}\ \Omega, \\
		-\nabla\cdot\sigma &= f, &&\mbox{in}\ \Omega, \\
		\sigma \cdot \n&=g, &&\mbox{on}\ \partial\Omega.
	\end{aligned}\right.
\end{equation}
To recover the conductivity $q$ from the observation $z^\delta$, we employ a residual
minimization scheme based on the first-order system \eqref{eqn:mixed-Neum}. We approximate both $\sigma$ and $q$ using DNNs, and substitute the noisy data $z^\delta$
for the scalar field $u$ in the first equation. For the scalar field $q$,
we use a $\tanh$-DNN function class (of depth $L_q$ and width $W_q$) with the parametrization $\mathfrak{P}_{p,\epsilon_q}$ (with $p\geq 2$ and tolerance $\epsilon_q$). Similarly, for the vector field
$\sigma:\Omega\to\mathbb{R}^d$, we 
employ $d$ identical $\tanh$-DNN function
classes (of depth $L_{\sigma}$ and width $W_{\sigma}$) with the parametrizations $\mathfrak{P}_{2,\epsilon_\sigma}$ (with tolerance $\epsilon_\sigma$), and stack them into one multi-output DNN (via the
parallelization technique in Lemma \ref{lem:NN-paral}).

To enforce the box constraint of the coefficient $q$, 
we employ a cutoff operation $\P01: H^1(\Omega) \rightarrow \mathcal{A}$ defined by
$\P01(v) = \min(\max(c_0,v),c_1). 
$ The operator $P_\mathcal{A}$ is stable in the following sense \cite[Corollary 2.1.8]{Ziemer:1989}
\begin{equation}\label{eqn:P01-stab}
\| \nabla \P01(v) \|_{L^p(\Omega)} \le \|\nabla v \|_{L^p(\Omega)},\quad \forall v \in W^{1,p}(\Omega),p\in[1,\infty], 
\end{equation}
and moreover, for all $v\in \mathcal{A}$, there holds
\begin{equation}\label{eqn:P01-approx}
\|  \P01(w) - v \|_{L^p(\Omega)} \le \| w - v \|_{L^p(\Omega)},\quad \forall w \in L^{p}(\Omega),~p\in[1,\infty].
\end{equation}
Using the standard least-squares formulation on all equality constraints, we obtain the following objective
\begin{equation*}
    J_{\bsgamma}(\theta,\kappa)=\|\sigma_\kappa- \P01(q_\theta)\nabla z^\delta\|_{L^2(\Omega)}^2+\gamma_\sigma\|\nabla\cdot\sigma_\kappa+f\|_{L^2(\Omega)}^2+\gamma_b\|\sigma_\kappa\cdot \n-g\|^2_{L^2(\partial\Omega)}+\gamma_q
    \| \nabla q_\theta\|_{L^2(\Omega)}^2.
\end{equation*}
Then the DNN reconstruction problem reads
\begin{equation}\label{eqn:obj-Neum}
	\min_{(\theta,\kappa)\in (\mathfrak{P}_{p,\epsilon_q},\mathfrak{P}_{2,\epsilon_\sigma}^{\otimes d})} J_{\bsgamma}(\theta,\kappa),
\end{equation}
where the superscript $\otimes d$ denotes the $d$-fold direct product, $\gamma_\sigma>0$, $\gamma_b>0$ and $\gamma_q>0$ are penalty parameters that balance the different terms and
have to be tuned suitably, and we write $\bsgamma=(\gamma_\sigma,\gamma_b,\gamma_q)\in\mathbb{R}_+^3$ below.
The $H^1(\Omega)$ seminorm penalty on the approximation $q_\theta$ is to stabilize the minimization process. It is essential for overcoming the ill-posedness of the inverse problem \cite{EnglHankeNeubauer:1996,ItoJin:2015}. $z^\delta$ is the noisy measurement of the exact data
$u(q^\dag)$ in the domain $\Omega$ and the noise level $\delta$ is defined by 
\begin{equation}\label{eqn:delta}
\delta:= \|\nabla(u(q^\dagger)-z^\delta)\|_{L^2(\Omega)}.
\end{equation} 
Note that in \eqref{eqn:delta} we impose a mild regularity condition on the noisy
data $z^\delta$ (smoother than the popular $L^2(\Omega)$), which may be obtained by presmoothing the
raw noisy data beforehand. This assumption is commonly used in energy type formulations,
e.g., Kohn--Lowe \cite{KohnLowe:1988} and equation error  \cite{AlJamalGockenbach:2012} formulations. The well-posedness of problem
\eqref{eqn:obj-Neum} follows by a standard argument in calculus of variation. Indeed, the
compactness of the parametrizations $\mathfrak{P}_{p,\epsilon_q}$ and $\mathfrak{P}_{2,\epsilon_\sigma}$
holds due to the uniform boundedness on the parameter vectors and finite-dimensionality of the space. Meanwhile, the smoothness
of the $\tanh$ activation function implies the continuity of the loss $J_{\bsgamma}$
in the DNN parameters $(\theta,\kappa)$. These two properties imply the existence of a minimizer $(\theta^*,\kappa^*)$.

Note that the objective $J_{\bsgamma}$ involves high-dimensional integrals, and
hence quadrature is needed in practice. This may be achieved using
any standard quadrature rules, and predominantly Monte Carlo methods in high-dimension. In this work, we employ the Monte Carlo method. Let $\mathcal{U}(\Omega)$ and $\mathcal{U}(\partial\Omega)$ be the uniform distributions
over the domain $\Omega$ and the boundary $\partial\Omega$, respectively, and
$(q_\theta,\sigma_\kappa)$ be the DNN realization of $(\theta,\kappa)$.
Using the expectation $\mathbb{E}_{\mathcal{U}(\cdot)}[\cdot]$ with respect to $\mathcal{U}(\cdot)$, we
can rewrite the loss $J_{\bsgamma}$ as
\begin{align*}
J_{\bsgamma}(\theta,\kappa)&=
|\Omega|\mathbb{E}_{X\sim\mathcal{U}(\Omega)}\Big[\|\sigma_{\kappa}(X)- \P01(q_\theta(X))\nabla z^\delta(X)\|_{\ell^2}^2\Big] +\gamma_\sigma|\Omega|\mathbb{E}_{X\sim\mathcal{U}(\Omega)}\Big[\big(\nabla\cdot\sigma_\kappa(X)+f(X)\big)^2\Big]\\
&\quad+\gamma_b|\partial\Omega|\mathbb{E}_{Y\sim\mathcal{U}(\partial\Omega)}\Big[\big(\sigma_\kappa(Y)\cdot \n-g(Y)\big)^2\Big]+\gamma_q|\Omega|\mathbb{E}_{X\sim\mathcal{U}(\Omega)}\Big[\|\nabla q_\theta(X)\|_{\ell^2}^2 \Big]   \\
	&=: \mathcal{E}_d(\sigma_{\kappa},q_\theta) + \gamma_\sigma \mathcal{E}_\sigma(\sigma_\kappa)+\gamma_b \mathcal{E}_b(\sigma_\kappa)+\gamma_q\mathcal{E}_q(q_\theta),
\end{align*}
where $|\Omega|$ and $|\partial\Omega|$ denote the Lebesgue measure of $\Omega$ and $\partial\Omega$, respectively, and $\|\cdot\|_{\ell^2}$ denotes the Euclidean norm on $\mathbb{R}^d$. Next let
$X=\{X_j\}_{j=1}^{n_r}$ and $Y=\{Y_{j}\}_{j=1}^{n_b}$ be independent and identically
distributed (i.i.d.) samples drawn from the uniform distributions $\mathcal{U}
(\Omega)$ and $\mathcal{U}(\partial\Omega)$, respectively, where $n_r$ and $n_b$ are the numbers of sampling points in the domain $\Omega$ and on the boundary $\partial\Omega$, respectively.
Then the empirical loss $\widehat{J}_{\bsgamma}(\theta,\kappa)$ is given by
\begin{equation}\label{eqn:obj-Neum-dis}
	\widehat{J}_{\bsgamma}(\theta,\kappa)=:\mathcal{\widehat{E}}_d(\sigma_{\kappa},q_\theta) + \gamma_\sigma \mathcal{\widehat{E}}_\sigma(\sigma_\kappa)+\gamma_b\mathcal{\widehat{E}}_b(\sigma_\kappa)+\gamma_q\mathcal{\widehat{E}}_q(q_\theta),
\end{equation}
where $\mathcal{\widehat{E}}_d(\sigma_{\kappa},q_\theta)$, $\mathcal{\widehat{E}}_\sigma(\sigma_\kappa)$,
$\mathcal{\widehat{E}}_b(\sigma_{\kappa})$ and $\mathcal{\widehat{E}}_q(q_\theta)$ are Monte Carlo
approximations of $\mathcal{E}_d(\sigma_{\kappa},q_\theta)$, $\mathcal{E}_\sigma(\sigma_\kappa)$,
$\mathcal{E}_b(\sigma_{\kappa})$ and $\mathcal{E}_q(q_\theta)$, obtained by replacing the expectation with a
sample mean, and are defined by
\begin{align}
\mathcal{\widehat{E}}_d(\sigma_{\kappa},q_\theta)&:=n_r^{-1}|\Omega|\sum_{j=1}^{n_r} \|\sigma_{\kappa}(X_{j})
-\P01(q_\theta(X_{j}))\nabla z^\delta(X_{j})\|_{\ell^2}^2, \label{eqn:loss0}\\
\mathcal{\widehat{E}}_\sigma(\sigma_\kappa)&:=n_r^{-1}|\Omega|\sum_{j=1}^{n_r}\big(\nabla\cdot\sigma_\kappa(X_j)+f(X_j)\big)^2,\label{eqn:loss1} \\
\mathcal{\widehat{E}}_b(\sigma_\kappa)&:=n_b^{-1}|\partial\Omega|\sum_{j=1}^{n_b}\big(\sigma_\kappa(Y_j)\cdot \n-g(Y_j)\big)^2,\label{eqn:loss2} \\
\mathcal{\widehat{E}}_q(q_\theta)&:=n_r^{-1}|\Omega|\sum_{j=1}^{n_r}\|\nabla q_\theta(X_j)\|_{\ell^2}^2.\label{eqn:loss3}
\end{align}
Additionally, we define variants of $\mathcal{E}_b$ and $\widehat{\mathcal{E}}_b$ by
\begin{equation}
    \mathcal{E}_{b'} =|\partial\Omega|\mathbb{E}_{\mathcal{U}(\partial\Omega)}[\|\sigma(Y)-q^\dag(Y)\nabla z^\delta(Y)\|_{\ell^2}^2]
    \quad \mbox{and}\quad
    \widehat{\mathcal{E}}_{b'} =n_b^{-1}|\partial\Omega|\sum_{j=1}^{n_b}\|\sigma(Y_j)-q^\dag(Y_j)\nabla z^\delta(Y_j)\|_{\ell^2}^2.\label{eqn:loss5}
\end{equation}
These quantities will be needed in the investigation of the inverse problem in the Dirichlet case in Section \ref{sec:Diri}.
The estimation of Monte Carlo errors will be discussed in Section \ref{sec:Neum-MC} below.

\subsection{Error analysis of the population loss}\label{sec:Neumann-pop}
Now we derive (weighted) $L^2(\Omega)$ error bounds on $q^\dagger-q_{\theta}^*$ for the DNN realization
$q_\theta^*$ of a minimizer $(\theta^*,\kappa^*)$ to the population loss $J_{\bsgamma}(\theta,\kappa)$
under Assumption \ref{ass:Neum} below.

\begin{assumption}\label{ass:Neum}
$q^\dag\in W^{2,p}(\Omega)\cap \mathcal{A}$, $f\in H^1(\Omega)\cap L^{\infty}(\Omega)$ and $g\in H^{\frac32}(\partial\Omega)\cap L^{\infty}(\partial\Omega)$, with $p=\max(2,d+\nu)$ for small $\nu>0$.
\end{assumption}

The following \textit{a priori} regularity holds under Assumption \ref{ass:Neum} for $u^\dag:=u(q^\dag)$ and $\sigma^\dag:=q^\dag\nabla u(q^\dag)$.
\begin{lemma}\label{lem:reg}
Under Assumption \ref{ass:Neum}, the
solution $u^\dag$ to problem \eqref{equ:Neu problem} {\rm(}with $q=q^\dag${\rm)} satisfies $u^\dag\in H^3(\Omega)\cap H_0^1
(\Omega)$ and $\sigma^\dag\in (H^2(\Omega))^d$.    
\end{lemma}
\begin{proof}
By Sobolev embedding theorem \cite[Theorem 4.12, p. 85]{AdamsFournier:2003}, $q^\dag\in W^{2,p}(\Omega)\hookrightarrow W^{1,\infty}(\Omega)$ for $p>\max(2,p+\nu)$. Since $f\in L^\infty(\Omega)$ and $g\in L^\infty (\partial\Omega)$, standard elliptic regularity theory implies $u^\dag\in H^2(\Omega)\cap W^{1,\infty}(\Omega)$. Next, upon expansion, we have
\begin{equation*}
  \left\{\begin{aligned} -\Delta u^\dag &= \tfrac{f}{q^{\dag}} +\tfrac{\nabla q^\dag\cdot\nabla u^\dag}{q^\dag}, &&\quad \mbox{in }\Omega,\\
  \partial_n u^\dag & = \tfrac{g}{q^\dag}, &&\quad \mbox{on }\partial\Omega,
  \end{aligned}\right.
\end{equation*}
where $\cdot$ denotes Euclidean inner product on $\mathbb{R}^d$.
Since $q^\dag\in W^{1,\infty}(\Omega)\cap \mathcal{A}$ and $f\in L^\infty(\Omega)\cap H^1(\Omega)$, we have for $i=1,\ldots,d$,
$\partial_{x_i}\Big(\frac{f}{q^\dag}\Big) = -\frac{f\partial_{x_i} q^\dag}{(q^\dag)^2} + \frac{\partial_{x_i} f}{q^\dag} \in L^2(\Omega)$,   
i.e., $\frac{f}{q^\dag}\in H^1(\Omega)$. Likewise,
since $q^\dag\in W^{1,\infty}(\Omega)\cap W^{2,p}(\Omega)$ and $u^\dag\in W^{1,\infty}(\Omega)\cap H^2(\Omega)$, we have for any $i=1,\ldots,d$,
\begin{equation*}
   \partial_{x_i} \Big(\frac{\nabla q^\dag\cdot\nabla u^\dag}{q^{\dag}}\Big) = 
   -\frac{\partial_{x_i}q^\dag (\nabla q^\dag\cdot \nabla u^\dag)}{(q^\dag)^2} + \frac{\nabla (\partial_{x_i} q^\dag) \cdot \nabla u^\dag}{q^\dag} + \frac{\nabla (\partial_{x_i}u^\dag)\cdot \nabla q^\dag}{q^\dag} \in L^2(\Omega), 
\end{equation*}
i.e., $\frac{\nabla q^\dag\cdot\nabla u^\dag}{q^\dag}\in H^1(\Omega)$ also. Since $g\in L^{\infty}(\partial\Omega)\cap H^\frac{3}{2}(\partial\Omega)$, by the trace theorem \cite[Theorem 5.36, p. 164]{AdamsFournier:2003}, there exists $\tilde g\in L^{\infty}(\Omega)\cap H^2(\Omega)$. Then repeating the preceding argument gives for any $i,j=1,\ldots,d$
\begin{align*}
    \partial_{x_ix_j}\Big(\frac{\tilde g}{q^\dag}\Big) %&= \partial_{x_j}\Big(\frac{\partial_{x_i}\tilde g}{q^\dag}-\frac{\tilde g \partial_{x_i}q^\dag}{(q^\dag)^2}\Big)\\ 
    &=\frac{q^\dag\partial_{x_{i}x_j}\tilde g-\partial_{x_i}\tilde g \partial_{x_j}q^\dag}{(q^\dag)^2}-\frac{(\partial_{x_j}\tilde g)(\partial_{x_i}q^\dag) + \tilde g\partial_{x_ix_j}q^\dag}{(q^\dag)^2}+2\frac{\tilde g(\partial_{x_i}q^\dag)(\partial_{x_j}q^\dag)}{(q^\dag)^3}\in L^2(\Omega),
\end{align*}
i.e., $\frac{\tilde g}{q^\dag}\in H^2(\Omega)$. This and trace theorem imply $\frac{g}{q^\dag}\in H^\frac{3}{2}(\partial\Omega)$. Applying the standard elliptic regularity theory again yields $u^\dag\in H^3(\Omega)$. Finally, it follows from  $q^\dag \in W^{2,p}(\Omega)$ and $u^\dag\in H^3(\Omega)\cap W^{1,\infty}(\Omega)$ that $\sigma^\dag\equiv q^\dag\nabla u^\dag\in(H^2(\Omega))^d$.
\end{proof}

The next lemma gives an \textit{a priori} estimate on the DNN realization
$(q_\theta^*,\sigma_\kappa^*)$ of the minimizer $(\theta^*,\kappa^*)$. 
\begin{lemma}\label{lem:Neu pri sigma q}
Let Assumption \ref{ass:Neum} hold. Fix small $\epsilon_\sigma$, $\epsilon_q>0$, and let $(\theta^*,\kappa^*)\in( \mathfrak{P}_{p,\epsilon_q}, \mathfrak{P}_{2,\epsilon_\sigma}^{\otimes d})$ be a minimizer of problem
\eqref{eqn:obj-Neum}. Then the following estimate holds
\begin{equation*}
	J_{\bsgamma}(\theta^*,\kappa^*)\leq c\big(\epsilon_q^2+(\gamma_\sigma+\gamma_b+1)\epsilon_\sigma^2+\delta^2+\gamma_q\big).
\end{equation*}
\end{lemma}
\begin{proof}
First, Assumption \ref{ass:Neum} and Lemma \ref{lem:reg} imply $\sigma^\dag\in (H^2(\Omega))^d$. By Lemma \ref{lem:tanh-approx}, there
exists one $(\theta_\epsilon,\kappa_\epsilon) \in (\mathfrak{P}_{p,\epsilon_q},
\mathfrak{P}_{2,\epsilon_\sigma}^{\otimes d})$ such that its DNN realization $(q_{\theta_\epsilon},\sigma_{\kappa_\epsilon})$ satisfies
\begin{equation}\label{eqn:bdd-NN}
   \|q^\dag-q_{\theta_\epsilon}\|_{W^{1,p}(\Omega)}\leq\epsilon_q\quad \mbox{and}\quad  \|\sigma^\dag-\sigma_{\kappa_\epsilon}\|_{H^1(\Omega)}\leq \epsilon_\sigma.
\end{equation}
Then the triangle inequality leads to $\| \nabla q_{\theta_\epsilon} \|_{L^2(\Omega)} \le c $.
The embedding $W^{1,p}(\Omega)\hookrightarrow L^\infty(\Omega)$ \cite[Theorem 4.12, p. 85]{AdamsFournier:2003} implies
\begin{equation}\label{eqn:bdd-1}
   \|q^\dag-q_{\theta_\epsilon}\|_{L^\infty(\Omega)}\leq c\epsilon_q.
   \end{equation} 
By the minimizing property of $(\theta^*,\kappa^*)$ and the triangle inequality, we obtain
\begin{align*}
&\quad J_{\bsgamma}(\theta^*,\kappa^*)\leq J_{\bsgamma}(\theta_\epsilon,\kappa_\epsilon) \\
& = \|\sigma_{\kappa_\epsilon}- \P01(q_{\theta_\epsilon})\nabla z^\delta\|_{L^2(\Omega)}^2+\gamma_\sigma\|\nabla\cdot\sigma_{\kappa_\epsilon}+f\|_{L^2(\Omega)}^2+\gamma_b\|\sigma_{\kappa_\epsilon}\cdot \n-g\|^2_{L^2(\partial\Omega)}+\gamma_q\|\nabla q_{\theta_\epsilon}\|_{L^2(\Omega)}^2 \\
&\leq c\big[\|\sigma_{\kappa_\epsilon}-\sigma^\dag\|^2_{L^2(\Omega)}+ \|(q^\dag-\P01(q_{\theta_\epsilon}))\nabla u^\dag\|_{L^2(\Omega)}^2+ \|\P01(q_{\theta_\epsilon})\nabla (u^\dag-z^\delta)\|_{L^2(\Omega)}^2\\
&\quad+ \gamma_\sigma\|\nabla\cdot\sigma_{\theta_\epsilon}-\nabla\cdot \sigma^\dag\|_{L^2(\Omega)}^2+\gamma_b\|(\sigma_{\kappa_\epsilon}-\sigma^\dag)\cdot \n\|^2_{L^2(\partial\Omega)}+\gamma_q\|\nabla q_{\theta_\epsilon}\|_{L^2(\Omega)}^2\big].
\end{align*}
Then the continuous embedding $W^{1,p}(\Omega)\hookrightarrow L^\infty(\Omega)$ 
and the stability estimates \eqref{eqn:P01-stab}--\eqref{eqn:P01-approx} of $\P01$ lead to
\begin{equation*}\label{eqn:bdd-2}
\begin{split}
  \|(q^\dag-\P01(q_{\theta_\epsilon}))\nabla u(q^\dag)\|_{L^2(\Omega)} & \leq \|q^\dag-\P01(q_{\theta_\epsilon})\|_{L^\infty(\Omega)}\|\nabla u^\dag\|_{L^2(\Omega)} \leq \|q^\dag-q_{\theta_\epsilon}\|_{L^\infty(\Omega)}\|\nabla u^\dag\|_{L^2(\Omega)},\\
  \|\P01(q_{\theta_\epsilon})\nabla (u^\dag-z^\delta)\|_{L^2(\Omega)} & \leq \|\P01(q_{\theta_\epsilon})\|_{L^{\infty}(\Omega)}\|\nabla(u^\dag-z^\delta)\|_{L^2(\Omega)} \le c \| \nabla(u^\dag-z^\delta)\|_{L^2(\Omega)} .%\nonumber\\
  %&\leq c\|q_{\theta_\epsilon}\|_{H^1(\Omega)}\|q^\dag-z^\delta\|_{W^{1,s}(\Omega)}.\label{eqn:bdd-embed}
\end{split}
\end{equation*}
Now the \textit{a priori} estimates in \eqref{eqn:bdd-1} and the trace theorem \cite[Theorem 5.36, p. 164]{AdamsFournier:2003} imply
\begin{align*}
J_{\bsgamma}(\theta^*,\kappa^*)&\leq c\big[\epsilon_\sigma^2 + \|q^\dag-q_{\theta_\epsilon}\|^2_{L^\infty(\Omega)} +  \|\nabla (u^\dag-z^\delta)\|^{2}_{L^2(\Omega)}\\
 &\quad + (\gamma_\sigma+\gamma_b)\|\sigma^\dag-\sigma_{\kappa_\epsilon}\|^2_{H^1(\Omega)}+\gamma_q  \| \nabla q_{\theta_\epsilon} \|^2_{L^2(\Omega)} \big]\\
&\leq c\big(\epsilon_q^2+(\gamma_\sigma+\gamma_b+1)\epsilon_\sigma^2+\delta^2+\gamma_q\big).
\end{align*}
This completes the proof of the lemma.
\end{proof}

Next we give a crucial condition for the following error analysis. It holds under suitable
conditions on $u^\dag$: $u^\dag\in C^2(\overline{\Omega})$ and $|\nabla u^\dag|
\neq0$ on a smooth domain $\Omega\subset\mathbb{R}^2$; see \cite[Lemma 5]{KohnLowe:1988} for detailed
discussions.
\begin{condition}\label{cond: Neu cond}
For any $\psi\in H^1(\Omega)$, there exists a solution $v_\psi$ of the equation
$\nabla u^\dag\cdot\nabla v_\psi = \psi$ almost everywhere  in $\Omega$ and it satisfies
\begin{equation*}
	\|v_\psi\|_{H^1(\Omega)}\leq c\|\psi\|_{H^1(\Omega)}.
\end{equation*}
\end{condition}

Now we can state a first error estimate on the approximation $q^*_\theta$.
\begin{theorem}\label{thm:pop-loss-Neum}
Let Assumption \ref{ass:Neum} and Condition \ref{cond: Neu cond} hold. Fix small
$\epsilon_q,\epsilon_\sigma>0$, and let $(q_\theta^*,\sigma_\kappa^*)$ be the
DNN realization of a minimizer $(\theta^*,\kappa^*)\in (\mathfrak{P}_{p,\epsilon_q},
\mathfrak{P}_{2,\epsilon_\sigma}^{\otimes d})$ of problem \eqref{eqn:obj-Neum}.
Then with 
$\eta:=\epsilon_q+(\gamma_\sigma^{\frac12}+\gamma_b^{\frac12}+1)\epsilon_\sigma
+\delta+\gamma_q^{\frac12},$
the following error estimate holds
\begin{equation*}
	\|q^\dag-\P01(q_\theta^*)\|_{L^2(\Omega)}\leq c(1+\gamma_\sigma^{-\frac14}+\gamma_b^{-\frac14} )(1+\gamma_q^{-\frac14} \eta^\frac12) \eta^{\frac12}.
\end{equation*}
\end{theorem}
\begin{proof}
Setting $\psi=q^\dag- \P01(q_\theta^*)\in H^1(\Omega)$ in Condition \ref{cond: Neu cond} and applying
integration by parts yield
\begin{align*}
	 &\quad \|q^\dag- \P01(q_\theta^*)\|^2_{L^2(\Omega)}
	=\big((q^\dag-\P01(q_\theta^*))\nabla u^\dag,\nabla v_\psi\big) = (\sigma^\dag,\nabla v_\psi) - (\P01(q^*_\theta)\nabla u^\dag,\nabla v_\psi)\\
	&=-\big(\nabla\cdot\sigma^\dagger, v_\psi\big) 
	- \big(\P01(q_\theta^*)\nabla u^\dag,\nabla v_\psi\big) + (g,v_\psi)_{L^2(\partial\Omega)}  \\
	& = \big(f+\nabla\cdot\sigma_\kappa^*, v_\psi\big) + \big(\sigma_\kappa^*-\P01(q_\theta^*)\nabla z^\delta, \nabla v_\psi \big) + \big( \P01(q_\theta^*)\nabla(z^\delta- u^\dag),\nabla v_\psi\big)  + (g-\sigma_{\kappa}^*\cdot \n,v_\psi)_{L^2(\partial\Omega)}.
\end{align*}
Then Cauchy-Schwarz inequality, the trace theorem \cite[Theorem 5.36, p. 164]{AdamsFournier:2003}, 
Lemma \ref{lem:Neu pri sigma q} and Condition \ref{cond: Neu cond} imply
\begin{align*}
 \|q^\dag-\P01(q_\theta^*)\|^2_{L^2(\Omega)}
&\leq c\big[\|f+\nabla\cdot\sigma_\kappa^*\|_{L^2(\Omega)} + \|\sigma_\kappa^*- \P01(q_\theta^*)\nabla z^\delta\|_{L^2(\Omega)}+ \|\P01(q_\theta^*)\|_{L^{\infty}(\Omega)}\|\nabla (z^\delta-u^\dag)\|_{L^2(\Omega)}\\
&\quad+\|g-\sigma_{\kappa}^*\cdot \n\|_{L^2(\partial\Omega)}\big]\|v_\psi\|_{H^1(\Omega)} 
	\leq c \big(1+\gamma_\sigma^{-\frac12}+\gamma_b^{-\frac12} \big) \eta\|q^\dag-\P01(q_\theta^*)\|_{H^1(\Omega)}.
\end{align*}
By the definition of the $(H^1(\Omega))'$-norm, we have
\begin{equation*}
   \|q^\dag- \P01(q_\theta^*)\|_{(H^1(\Omega))'}\leq c \big(1+\gamma_\sigma^{-\frac12}+\gamma_b^{-\frac12} \big) \eta.
\end{equation*}
This, duality pairing and the bound on $\|\nabla q_{\theta}^*\|_{L^2(\Omega)}$ in Lemma \ref{lem:Neu pri sigma q} 
and the stability of $\P01$ in \eqref{eqn:P01-stab} imply
\begin{align*}
   &\|q^\dag-\P01(q_\theta^*)\|_{L^2(\Omega)}
   \leq \|q^\dag-\P01(q_\theta^*)\|^{\frac12}_{(H^{1}(\Omega))'}\|q^\dag-\P01(q_\theta^*)\|^{\frac12}_{H^1(\Omega)} \\
           \leq &c \|q^\dag-q_\theta^*\|^{\frac12}_{(H^{1}(\Omega))'}\big(1+\|\nabla \P01(q_{\theta}^*)\|_{L^2(\Omega)}^{\frac12}\big)
     \leq c \|q^\dag-q_\theta^*\|^{\frac12}_{(H^{1}(\Omega))'}\big(1+\|\nabla q_{\theta}^*\|_{L^2(\Omega)}^{\frac12}\big)\\
    	  \leq & c(1+\gamma_\sigma^{-\frac14}+\gamma_b^{-\frac14} )(1+\gamma_q^{-\frac14} \eta^\frac12) \eta^{\frac12}.
\end{align*}
Thus we complete the proof of the theorem.
\end{proof}

\begin{remark}\label{rmk:para-Neum}
Theorem \ref{thm:pop-loss-Neum} provides useful guidelines for choosing
the parameters: 
$\gamma_\sigma=O(1)$, $\gamma_b=O(1)$, $\gamma_q=O(\delta^2)$, $\epsilon_q=O(\delta)$ and $ \epsilon_\sigma=O(\delta)$.
Then we obtain the following error estimate
$\|q^\dag- \P01(q_\theta^*)\|_{L^2(\Omega)}\leq c\delta^\frac12.$
\end{remark}

\subsection{Error analysis of the empirical loss}\label{sec:Neum-MC}

Now we analyze the error of the approximation $\widehat{q}_\theta^*$, i.e., the DNN realization
of a minimizer $(\widehat\theta^*,\widehat\kappa^*)$ to the empirical loss $\widehat{J}_{\bsgamma}
(\theta,\kappa)$. The loss $\widehat{J}_{\bsgamma}(\theta,\kappa)$ involves also the
quadrature error arising from approximating the integrals via Monte Carlo methods. The analysis requires
the following assumption.
The $L^\infty$ bound is needed in order to apply the standard Rademacher complexity argument (cf. Dudley's formula in Lemma \ref{lem:Dudley}).
\begin{assumption}\label{ass:Neum-gen}
 $f\in L^\infty(\Omega)$, and $z^\delta\in W^{1,\infty}(\Omega)$.
\end{assumption}

The key of the analysis is to bound the error $\sup_{q_\theta\in\mathcal{N}_q,\sigma_\kappa
\in\mathcal{N}_\sigma} \big|J_{\bsgamma}(q_\theta,\sigma_\kappa)-\widehat{J}_{\bsgamma} (q_\theta,
\sigma_\kappa)\big|$ for suitable DNN function classes $\mathcal{N}_q$ and $\mathcal{N}_\sigma$ (corresponding to the sets $\mathfrak{P}_{p,\epsilon_q}$ and $
\mathfrak{P}_{2,\epsilon_\sigma}^{\otimes d}$, respectively), which
are also known as statistical errors in statistical learning theory
\cite{AnthonyBartlett:1999,ShalevBen:20214}. The starting point of the analysis is the following splitting:
\begin{equation*}
\sup_{q_\theta\in\mathcal{N}_q,\sigma_\kappa\in\mathcal{N}_\sigma} \big|J_{\bsgamma}(q_\theta,\sigma_\kappa)-\widehat{J}_{\bsgamma}
(q_\theta,\sigma_\kappa)\big|\leq \Delta\mathcal{E}_d + \gamma_\sigma \Delta\mathcal{E}_\sigma + \gamma_b\Delta\mathcal{E}_b + \gamma_q \Delta\mathcal{E}_q,
\end{equation*}
with the error components given respectively by
\begin{align*}
&\Delta\mathcal{E}_d: = \sup_{\sigma_{\kappa}\in\mathcal{N}_{\sigma},q_\theta\in\mathcal{N}_q}\big| \mathcal{E}_d(\sigma_{\kappa},q_\theta)-\mathcal{\widehat{E}}_d(\sigma_{\kappa},q_\theta)\big|,
&&\Delta\mathcal{E}_\sigma:= \sup_{\sigma_{\kappa}\in\mathcal{N}_{\sigma}}\big|\mathcal{E}_\sigma(\sigma_\kappa)-\mathcal{\widehat{E}}_\sigma(\sigma_\kappa)\big|, \\ &\Delta\mathcal{E}_b:=\sup_{\sigma_{\kappa}\in\mathcal{N}_{\sigma}}\big|\mathcal{E}_b (\sigma_\kappa)-\mathcal{\widehat{E}}_b(\sigma_\kappa)\big|, &&\Delta\mathcal{E}_q:=\sup_{q_\theta\in\mathcal{N}_q}\big|\mathcal{E}_q(q_\theta)-\mathcal{\widehat{E}}_q(q_\theta)\big|.
\end{align*}
Further we define $\Delta\mathcal{E}_{b'}:=\sup_{\sigma_{\kappa}\in\mathcal{N}_{\sigma}}\big|\mathcal{E}_{b'} (\sigma_\kappa)-\mathcal{\widehat{E}}_{b'}(\sigma_\kappa)\big|.$


Now we state the quadrature error for each term (in high probability). The proof is based on PAC-type generalization bounds in Lemma \ref{lem:PAC}, which in turn employs the Rademacher complexity of the associated function classes via Dudley's formula in Lemma \ref{lem:Dudley}. The overall proof is lengthy and hence it is deferred to the appendix.
\begin{theorem}\label{thm:stat-err}
Let Assumptions \ref{ass:Neum} and \ref{ass:Neum-gen} hold, and $q_\theta\in \mathcal{N}(L_q,N_{\theta},R_q)$ and $\sigma_\kappa \in \mathcal{N}(L_\sigma,N_{\kappa},R_\sigma)$. Fix $\tau\in(0,\frac18)$, and define the following bounds
\begin{align*}
e_d&:=c\frac{R_\sigma^{2}N_\kappa^{2}(N_\kappa+N_\theta)^{\frac12}(\log^\frac12 R_\sigma +\log^\frac12 N_{\kappa} + \log^\frac12 R_q+\log^\frac12 N_\theta+\log^\frac12 n_r)}{\sqrt{n_r}} + \tilde c R^2_\sigma N_{\kappa}^2\sqrt{\frac{\log\frac{1}{\tau}}{2n_r}},\\
e_\sigma&:=c\frac{R_\sigma^{2L_\sigma}N_{\kappa}^{2L_\sigma-\frac32}
        \big(\log^\frac12R_\sigma+\log^\frac12N_{\kappa}+\log^\frac12n_r\big)}{\sqrt{n_r}}+ \tilde cR_\sigma^{2L_\sigma}N_\kappa^{2L_\sigma-2}\sqrt{\frac{\log\frac{1}{\tau}}{2n_r}},\\
e_b&:=c\frac{R_\sigma^2N_{\kappa}^{\frac52}\big(\log^\frac12 R_\sigma+\log^\frac12 N_{\kappa}+\log^\frac12 n_b\big)}{\sqrt{n_b}}+\tilde cR_\sigma^{2}N_\kappa^{2}\sqrt{\frac{\log\frac{1}{\tau}}{2n_b}}, \\
e_{b'}&:=c\frac{R_\sigma^2N_{\kappa}^{\frac52}\big(\log^\frac12 R_\sigma+\log^\frac12 N_{\kappa}+\log^\frac12 n_b\big)}{\sqrt{n_b}}+\tilde cR_\sigma^{2}N_\kappa^{2}\sqrt{\frac{\log\frac{1}{\tau}}{2n_b}},\\
e_q&:=c\frac{R_q^{2L_q}N_\theta^{2L_q-\frac32}
   \big(\log^\frac12R_q+\log^\frac12N_{\theta}+\log^\frac12n_r\big)}{\sqrt{n_r}}+\tilde cR_q^{2L_q}N_\theta^{2L_q-2}\sqrt{\frac{\log\frac{1}{\tau}}{2n_r}},
\end{align*}
where the constants $c$ and $\tilde c$ may depend on  $|\Omega|$, $|\partial\Omega|$, $d$, $\|z^\delta\|_{W^{1,\infty}(\Omega)}$, $\|f\|_{L^\infty(\Omega)}$, and $\|g\|_{L^\infty(\partial\Omega)}$ at most polynomially. Then, with a probability
at least $1-\tau$, each of the following statements holds
\begin{equation*}
    \Delta \mathcal{E}_i \leq e_{i}, \quad i\in\{d,\sigma,b,b',q\}.
\end{equation*}
\end{theorem}

Now we can state an error estimate on the numerical approximation $\widehat{q}_\theta^*$.
\begin{theorem}\label{thm:err-Neum-emp}
Let Assumptions \ref{ass:Neum} and \ref{ass:Neum-gen} and Condition \ref{cond: Neu cond} hold.
Fix small $\epsilon_q$, $\epsilon_\sigma>0$, and let $(\widehat{\theta}^*,\widehat{\kappa}^*)\in
(\mathfrak{P}_{p,\epsilon_q}, \mathfrak{P}_{2,\epsilon_\sigma}^{\otimes d})$ be a
minimizer of the empirical loss \eqref{eqn:obj-Neum-dis}, and $\widehat{q}_\theta^*$ and
$\widehat{\sigma}_\kappa^*$ their NN realizations. Fix $\tau\in (0,\frac18)$, let the bounds $e_d$,
$e_\sigma$, $e_b$ and $e_q$ be defined in Theorem \ref{thm:stat-err}, and further define $\eta$ by $\eta:=e_d+\gamma_\sigma e_\sigma+\gamma_be_b+\gamma_qe_q+\epsilon_q^2+(\gamma_\sigma+\gamma_b+1)\epsilon_\sigma^2
+\delta^2+\gamma_q.$
Then the following error bound holds with probability at least $1-4\tau$
\begin{equation*}
\|q^\dag-\P01(\widehat{q}_\theta^*)\|_{L^2(\Omega)}\leq c \big((e_d+\eta)^\frac12+(e_\sigma+\gamma_\sigma^{-1}\eta)^\frac12
+(e_b+\gamma_b^{-1}\eta)^\frac12+(e_q+\gamma_q^{-1}\eta)^\frac12\delta\big)^\frac12
\big(1+(e_q+\gamma_q^{-1}\eta)^\frac14\big).
\end{equation*}
\end{theorem}
\begin{proof}
Let $(\theta^*,\kappa^*)\in (\mathfrak{P}_{p,\epsilon_q}, \mathfrak{P}_{2,\epsilon_\sigma}^{\otimes d})$ be a minimizer of problem \eqref{eqn:obj-Neum}. Then the
minimizing property of $(\widehat{\theta}^*,\widehat{\kappa}^*)$ to the empirical
loss $\widehat J_{\bsgamma}(\theta,\kappa)$ implies the following decomposition
\begin{align*}
  \widehat J_{\bsgamma}(\widehat\theta^*,\widehat\kappa^*)
   & \leq [\widehat J_{\bsgamma}(\theta^*,\kappa^*) - J_{\bsgamma}(\theta^*,\kappa^*)] + J_{\bsgamma}(\theta^*,\kappa^*)
    \leq |J_{\bsgamma}(\theta^*,\kappa^*) - \widehat J_{\bsgamma}(\theta^*,\kappa^*)| +  J_{\bsgamma}(\theta^*,\kappa^*).
\end{align*}
Consequently, we deduce
\begin{equation*}
   \widehat{J}_{\bsgamma}(\widehat\theta^*,\widehat\kappa^*)  \leq J_{\bsgamma}(\theta^*,\kappa^*) + \sup_{(\theta,\kappa)\in (\mathfrak{P}_{p,\epsilon_q}, \mathfrak{P}_{2,\epsilon_\sigma}^{\otimes d})} |J_{\bsgamma}(\theta,\kappa) - \widehat J_{\bsgamma}(\theta,\kappa)|.
\end{equation*}
The two terms represent the approximation error and statistical error, respectively,
and the former was already bounded in Section \ref{sec:Neumann-pop}. It follows directly from Lemma
\ref{lem:Neu pri sigma q} and Theorem \ref{thm:stat-err}  that
with a probability at least $1-4\tau$,
\begin{equation*}
\widehat{J}_{\bsgamma}(\widehat{\theta}^*,\widehat{\kappa}^*)\leq c\big(e_d+\gamma_\sigma e_\sigma+\gamma_be_b+\gamma_qe_q+\epsilon_q^2+(\gamma_\sigma+\gamma_b+1)\epsilon_\sigma^2+\delta^2+\gamma_q\big),
\end{equation*}
i.e., $\widehat{J}_{\bsgamma}(\widehat{\theta}^*,\widehat{\kappa}^*)\leq c\eta$.
This estimate and the triangle inequality imply that with probability at least $1-4\tau$,
\begin{equation*}
\|\widehat{\sigma}_\kappa^*-\P01(\widehat{q}_\theta^*)\nabla z^\delta\|_{L^2(\Omega)}^2\leq \big[\mathcal{E}_d(\widehat{\sigma}^*_{\kappa},\widehat{q}^*_\theta) -\mathcal{\widehat{E}}_d(\widehat{\sigma}^*_{\kappa},\widehat{q}^*_\theta)\big]+  \mathcal{\widehat{E}}_d(\widehat{\sigma}^*_{\kappa},\widehat{q}^*_\theta)\leq c(e_d+\eta).
\end{equation*}
Similarly, the following estimates hold simultaneously with a probability at least $1-4\tau$,
\begin{align*}
\|f+\nabla\cdot\widehat{\sigma}_\kappa^*\|_{L^2(\Omega)}^2\leq c (e_\sigma+\gamma_\sigma^{-1}\eta),\quad
\|g-\widehat{\sigma}_{\kappa}^*\cdot \n\|_{L^2(\partial\Omega)}^2\leq c(e_b+\gamma_b^{-1}\eta),\quad
\|\nabla \widehat{q}_\theta^*\|^2_{L^2(\Omega)}\leq c(e_q+\gamma_q^{-1}\eta).
\end{align*}
Replacing $q_\theta^*$ by $\widehat{q}_\theta^*$ and repeating the argument of Theorem \ref{thm:pop-loss-Neum} gives
\begin{align*}
\|q^\dag-\P01(\widehat{q}_\theta^*)\|^2_{L^2(\Omega)}\le& c\big(\|f+\nabla\cdot\widehat{\sigma}_\kappa^*\|_{L^2(\Omega)} + \|\widehat{\sigma}_\kappa^*-\P01(\widehat{q}_\theta^*)\nabla z^\delta\|_{L^2(\Omega)}\\
&+ \|\P01(\widehat{q}_\theta^*)\|_{L^{\infty}(\Omega)}\|\nabla (z^\delta-u^\dagger)\|_{L^{2}(\Omega)}+\|g-\widehat{\sigma}_{\kappa}^*\cdot \n\|_{L^2(\partial\Omega)}\big)\|v_\psi\|_{H^1(\Omega)}.
\end{align*}
This and the preceding estimates and Condition \ref{cond: Neu cond} imply (with $v_\psi$ from Condition \ref{cond: Neu cond} for $\psi=q^\dag-\P01(\widehat{q}_\theta^*)$)
\begin{equation*}
\|q^\dag-\P01(\widehat{q}_\theta^*)\|_{H^{-1}(\Omega)}\leq c\big((e_d+\eta)^\frac12+(e_\sigma+\gamma_\sigma^{-1}\eta)^\frac12+(e_b+\gamma_b^{-1}\eta)^\frac12 +(e_q+\gamma_q^{-1}\eta)^\frac12\delta\big).
\end{equation*}
The desired estimate now follows from the argument of Theorem \ref{thm:pop-loss-Neum} and the bound on $\|\nabla\widehat{q}_\theta^*\|^2_{L^2(\Omega)}$.
\end{proof}

\begin{remark}
Under the assumptions of Theorem \ref{thm:stat-err} and the parameter selections in Remark \ref{rmk:para-Neum}, we may choose the numbers $n_r$ and $n_b$ of sample points in the domain $\Omega$ and on the boundary $\partial\Omega$ by
\begin{equation*}
    n_r = O\Big( \max\Big(\frac{R_\sigma^4N_\kappa^4(N_\theta+N_\kappa)}{\delta^{4+s}}, \frac{R_\sigma^{4L_\sigma}N_\kappa^{4L_\sigma-3}}{\delta^{4+s}}, \frac{R_q^{4L_q}N_\kappa^{4L_q-3}}{\delta^{s}}\Big)\Big)\quad \mbox{and}\quad n_b=O\Big(\frac{R_\sigma^4N_\kappa^5}{\delta^{4+s}}\Big),
\end{equation*}
for any $s>0$ $($i.e., using the power $s$ to absorb the $\log$ factor$)$. Note that by Lemma \ref{lem:tanh-approx}, $R_q=O(\delta^{-2-\frac{2p+3d+3pd+2p\mu}{p(1-\mu)}})$, $N_\theta=O(\delta^{-\frac{d}{1-\mu}})$, $R_\sigma=O(\delta^{-2-\frac{4(1+\mu)+9d}{2(1-\mu)}})$ and $N_\kappa=O(\delta^{-\frac{d}{1-\mu}})$. Then with probability at least $1-4\tau$, we have
    \begin{equation*}
    \|q^\dag- \P01(\widehat{q}_\theta^*)\|_{L^2(\Omega)}\leq c\delta^\frac12.
\end{equation*}
\end{remark}

\section{Inverse conductivity problem in the Dirichlet case}
\label{sec:Diri}

In this section, we extend the approach to the Dirichlet boundary value problem:
\begin{equation}\label{eqn:Diri}
	\left\{\begin{aligned}
		-\nabla\cdot(q\nabla u) &= f, \ &\mbox{in}&\ \Omega, \\
		u&=0, \ &\mbox{on}&\ \partial\Omega.
	\end{aligned}\right.
\end{equation}
We also provide relevant analysis of the reconstruction scheme.

\subsection{Mixed formulation and its DNN approximation}

Like in Section \ref{sec:Neumann}, to develop a reconstruction algorithm, we rewrite \eqref{eqn:Diri} into a
first-order system:
\begin{equation}\label{eqn:mixed-Diri}
	\left\{\begin{aligned}
		\sigma & = q\nabla u, &&\mbox{in }\ \Omega, \\
		-\nabla\cdot\sigma &= f, &&\mbox{in}\ \Omega, \\
		u&=0,  &&\mbox{on}\ \partial\Omega.
	\end{aligned}\right.
\end{equation}
To recover the conductivity coefficient $q$, we discretize the scalar field $q$ and
vector field $\sigma$ by two DNN function classes $\mathfrak{P}_{p,\epsilon_q}$ and $ \mathfrak{P}_{2,\epsilon_\sigma}^{\otimes d}$, respectively. Then with the notation in Section \ref{sec:Neumann}, the DNN
approximation scheme reads
\begin{equation}\label{eqn:obj-Diri}
	\min_{(\theta,\kappa)\in(\mathfrak{P}_{p,\epsilon_q}, \mathfrak{P}_{2,\epsilon_\sigma}^{\otimes d})} J_{\bsgamma}(\theta,\kappa)=\|\sigma_\kappa-\P01(q_\theta)\nabla z^\delta\|_{L^2(\Omega)}^2+\gamma_\sigma\|\nabla\cdot\sigma_\kappa+f\|_{L^2(\Omega)}^2+\gamma_q\|\nabla q_\theta\|_{L^2(\Omega)}^2,
\end{equation}
where $z^\delta$ is a noisy measurement of the exact data $u(q^\dagger)$ with
the noise level $\delta$ satisfying
$ \delta:=\|u(q^\dagger)-z^\delta\|_{W^{\frac32,2}(\Omega)}$.
In practice, however, the formulation
\eqref{eqn:obj-Diri} does not lend itself to high-quality reconstructions. This is
attributed to the absence of the current density $\sigma$ on the boundary $\partial\Omega$ so that the first term does not allow learning the current density $\sigma$ accurately. Hence, we supplement the loss \eqref{eqn:obj-Diri} with an additional boundary term
\begin{equation}\label{eqn:obj-Diri1}
\begin{split}
	\min_{(\theta,\kappa)\in (\mathfrak{P}_{p,\epsilon_q}, \mathfrak{P}_{2,\epsilon_\sigma}^{\otimes d})} 
	J_{\bsgamma}(\theta,\kappa)
&=\|\sigma_\kappa-\P01(q_\theta)\nabla z^\delta\|_{L^2(\Omega)}^2+\gamma_\sigma\|\nabla\cdot\sigma_\kappa+f\|_{L^2(\Omega)}^2
	\\
&\quad	+\gamma_q\|\nabla q_\theta\|_{L^2(\Omega)}^2 
 +\gamma_b\|\sigma_\kappa-q^\dag\nabla z^\delta\|^2_{L^2(\partial\Omega)}.
\end{split}
\end{equation}
This modified formulation
requires a knowledge of the exact conductivity $q^\dag$ on the boundary $\partial\Omega$. Note that this
assumption is frequently made in existing uniqueness analysis \cite{Alessandrini:1986,Richter:1981} and numerical studies \cite{BaoYeZang:2020,BarSochen:2021}. Similarly, one can ensure the existence of a minimizer
of \eqref{eqn:obj-Diri1}. Let $(\theta^*,\kappa^*)$ be a minimizer of problem
\eqref{eqn:obj-Diri1} and  $(q_\theta^*,\sigma_{\kappa}^*)$ be its DNN realization. In practice, we approximate the integrals using Monte Carlo methods. Using the uniform
distributions $\mathcal{U}(\Omega)$ and $\mathcal{U}(\partial\Omega)$, we rewrite the population loss \eqref{eqn:obj-Diri1} as
\begin{align*}
J_{\bsgamma}(\theta,\kappa)&=|\Omega|\mathbb{E}_{X\sim\mathcal{U}(\Omega)} \Big[\|\sigma_{\kappa}(X)-\P01(q_\theta(X))\nabla z^\delta(X)\|_{\ell^2}^2 \Big] +\gamma_\sigma|\Omega|\mathbb{E}_{X\sim\mathcal{U}(\Omega)}\Big[\big(\nabla\cdot\sigma_\kappa(X)+f(X)\big)^2\Big]\\
&\quad+\gamma_b|\partial\Omega|\mathbb{E}_{Y\sim\mathcal{U}(\partial\Omega)}\Big[\|\sigma_{\kappa}-q^\dagger\nabla z^\delta(Y)\|_{\ell^2}^2\Big]+\gamma_q|\Omega|\mathbb{E}_{X\sim\mathcal{U}(\Omega)}\Big[ \|\nabla q_\theta(X)\|_{\ell^2}^2 \Big] \\
	&=: \mathcal{E}_d(\sigma_{\kappa},q_\theta) + \gamma_\sigma \mathcal{E}_{\sigma}(\sigma_\kappa)+\gamma_b\mathcal{E}_{b'}(\sigma_{\kappa})+\gamma_q\mathcal{E}_q(q_\theta).
\end{align*}
Now we draw i.i.d. samples $X=\{X_{j}\}_{j=1}^{n_r}$ and $Y=\{Y_{j}
\}_{j=1}^{n_b}$ from the uniform distributions $\mathcal{U}(\Omega)$ and $\mathcal{U}(\partial\Omega)$.
The empirical loss $\widehat{J}_{\bsgamma}(\theta,\kappa)$ is given by
\begin{equation}\label{equ: Diri empir loss}
	\widehat{J}_{\bsgamma}(\theta,\kappa):= \mathcal{\widehat{E}}_d(\sigma_{\kappa},q_\theta) + \gamma_\sigma \mathcal{\widehat{E}}_\sigma(\sigma_\kappa)+\gamma_q\mathcal{\widehat{E}}_q(q_\theta)+\gamma_b\mathcal{\widehat{E}}_{b'}(\sigma_{\kappa}),
\end{equation}
where $\mathcal{\widehat{E}}_d(\sigma_{\kappa},q_\theta)$, $\mathcal{\widehat{E}}_\sigma(\sigma_\kappa)$
and $\mathcal{\widehat{E}}_q(q_\theta)$ are given by \eqref{eqn:loss0}--\eqref{eqn:loss2}, and
$\mathcal{\widehat{E}}_{b'}(\sigma_{\kappa,i})$ is given in \eqref{eqn:loss5}.

\begin{remark}\label{rmk:loss}
Instead of \eqref{eqn:obj-Diri1}, there are alternative formulations. For example, one may use the loss \begin{equation}\label{eqn:diriloss1}
J_{\bsgamma}(\theta,\kappa)=\|\sigma_\kappa-\P01(q_\theta)\nabla z^\delta\|_{L^2(\Omega)}^2+\gamma_\sigma\|\nabla\cdot\sigma_\kappa+f\|_{L^2(\Omega)}^2+\gamma_q\|\nabla q_\theta\|_{L^2(\Omega)}^2 + \gamma_b\|q_\theta-q^\dag\|_{L^2(\partial\Omega)}^2,
\end{equation}
which enforces the boundary condition $q_\theta=q^\dag$ on $\partial\Omega$ directly. It can be analyzed analogously.
However, numerically, it is less robust than \eqref{eqn:obj-Diri1} for noisy data.
See Section \ref{sec:numer} for numerical illustrations.
\end{remark}


\subsection{Error analysis of the population loss}

First we analyze the error of the DNN realization $q_\theta^*\in\mathcal{N}_q$ of the minimizer $\theta^*$ to the population loss \eqref{eqn:obj-Diri1}
under the following assumption.
\begin{assumption}\label{ass:Diri}
$q^\dag\in W^{2,p}(\Omega)\cap \mathcal{A}$, and $f\in H^1(\Omega)\cap L^{\infty}(\Omega)$, with $p=\max(2,d+\nu)$ for some small $\nu>0$.
\end{assumption}

The following regularity result holds for $u^\dag:=u(q^\dag)$ and $\sigma^\dag:=q^\dag \nabla u^\dag$. The proof follows exactly as Lemma \ref{lem:reg}, and hence it is omitted.
\begin{lemma}\label{lem:reg-Neum}
Let Assumption \ref{ass:Diri} hold. Then the solution $u^\dag$ $($with $q=q^\dag$$)$ to problem \eqref{eqn:Diri} satisfies $u^\dag\in H^3(\Omega)\cap W^{2,p}(\Omega)\cap H_0^1(\Omega)$ and $\sigma^\dag\in (H^2(\Omega))^d$.   
\end{lemma}

\begin{lemma}\label{lem: Diri pri sigma q}
Let Assumption \ref{ass:Diri} hold. Fix small $\epsilon_q$, $\epsilon_\sigma>0$, and let
$(\theta^*,\kappa^*)\in (\mathfrak{P}_{p,\epsilon_q}, \mathfrak{P}_{2,\epsilon_\sigma
}^{\otimes d})$ be a minimizer of problem \eqref{eqn:obj-Diri1}. Then the following estimate holds
\begin{equation*}
	J_{\bsgamma}(\theta^*,\kappa^*)\leq c\big(\epsilon_q^2+(1+\gamma_\sigma+\gamma_b)\epsilon_\sigma^2+(1+\gamma_\sigma)\delta^2+\gamma_q\big).
\end{equation*}
\end{lemma}
\begin{proof}
Assumption \ref{ass:Neum} and Lemma \ref{lem:reg-Neum} imply $\sigma^\dag\in (H^2(\Omega))^d$. Then by Lemma \ref{lem:tanh-approx}, there exists at least one $(\theta_\epsilon,\kappa_\epsilon)\in
(\mathfrak{P}_{p,\epsilon_q}, \mathfrak{P}_{2,\epsilon_\sigma
}^{\otimes d})$ such that its DNN realization $(q_{\theta_\epsilon},\sigma_{\kappa_\epsilon})$ satisfies
\begin{equation*}
\|q^\dag-q_{\theta_\epsilon}\|_{W^{1,p}(\Omega)}\leq\epsilon_q\quad \mbox{and}\quad  \|\sigma^\dag-\sigma_{\kappa_\epsilon}\|_{H^1(\Omega)}\leq \epsilon_\sigma.
\end{equation*}
Then by the minimizing property of $(\theta^*,\kappa^*)$ and the triangle inequality, we have
\begin{align*}
	&\quad J_{\bsgamma}(\theta^*,\kappa^*)\leq J_{\bsgamma}(\theta_\epsilon,\kappa_\epsilon) \\
	& = \|\sigma_{\kappa_\epsilon}-\P01(q_{\theta_\epsilon})\nabla z^\delta\|_{L^2(\Omega)}^2+\gamma_\sigma\|\nabla\cdot\sigma_{\kappa_\epsilon}+f\|_{L^2(\Omega)}^2+\gamma_b\|\sigma_{\kappa_\epsilon}-q^\dag\nabla z^\delta\|^2_{L^2(\partial\Omega)} +\gamma_q\|\nabla q_{\theta_\epsilon}\|_{L^2(\Omega)}^2 \\
&\leq c\big(\|\sigma_{\kappa_\epsilon}-\sigma^\dag\|^2_{L^2(\Omega)}+ \|(q^\dag-\P01(q_{\theta_\epsilon}))\nabla u^\dag\|_{L^2(\Omega)}^2+ \|\P01(q_{\theta_\epsilon})\nabla (u^\dag-z^\delta)\|_{L^2(\Omega)}^2 \\ &\qquad+\gamma_\sigma\|\nabla\cdot(\sigma_{\kappa_\epsilon}- \sigma^\dag)\|_{L^2(\Omega)}^2+\gamma_q\|\nabla q_{\theta_\epsilon}\|_{L^2(\Omega)}^2+\gamma_b\|\sigma_{\kappa_\epsilon}-\sigma^\dag\|^2_{H^1(\Omega)} +\gamma_b\|q^\dag\nabla(u^\dag-z^\delta)\|^2_{H^\frac12(\partial\Omega)}\big).
\end{align*}
Now the estimate \eqref{eqn:bdd-2} and Assumption \ref{ass:Diri} imply
\begin{align*}
	J_{\bsgamma}(\theta^*,\kappa^*)	&\leq c\big(\epsilon_\sigma^2 +
\|q^\dag-\P01(q_{\theta_\epsilon})\|^2_{L^\infty(\Omega)}\|\nabla u^\dag\|^2_{L^2(\Omega)} \\
 &\quad + \|\P01(q_{\theta_\epsilon})\|^{2}_{L^\infty(\Omega)}\|\nabla(u^\dag-z^\delta)\|^{2}_{L^2(\Omega)}+ \gamma_\sigma\epsilon_\sigma^2+\gamma_q+\gamma_b\epsilon_\sigma^2+\gamma_b\delta^2\big)\\
		&\leq c\big(\epsilon_q^2+(1+\gamma_\sigma+\gamma_b)\epsilon_\sigma^2+(1+\gamma_b)\delta^2+\gamma_q\big).
\end{align*}
This completes the proof of the lemma.
\end{proof}

\begin{condition}\label{Cond:P}
There exist some $\beta>0$ and $c$ such that 
$q^\dag |\nabla u^\dag|^2+fu^\dag\geq c\ {\rm dist}(x,\partial\Omega)^\beta$ almost every in $\Omega$.
\end{condition}
This condition holds under mild assumptions \cite[Lemmas 3.3 and 3.7]{bonito2017diffusion}: it holds with $\beta=2$ if $q^\dag\in\mathcal{A}$, and $f \in  L^2(\Omega)$ with $f\geq c_f>0$  (with $c_f\in\mathbb{R}$) over a Lipschitz domain $\Omega$, and with $\beta=0$ when $q^\dagger\in C^{1,\alpha}(\overline{\Omega})\cap\mathcal{A}$, and $f\in C^{0,\alpha}(\overline{\Omega})$  and $f\geq c_f>0$on a $C^{2,\alpha}$ domain $\Omega$ for some $\alpha>0$.
\begin{theorem}\label{thm: Diri error estimate on conduc}
Let Assumption \ref{ass:Diri} hold. Fix small $\epsilon_q$, $\epsilon_\sigma>0$, and let
$(\theta^*,\kappa^*) \in (\mathfrak{P}_{p,\epsilon_q}, \mathfrak{P}_{2,\epsilon_\sigma
}^{\otimes d})$ be a minimizer of problem \eqref{eqn:obj-Diri1} and $q_\theta^*$  the DNN realization
of $\theta^*$. Then with
$\eta:=\big(\epsilon_q^2+(1+\gamma_\sigma+\gamma_b)\epsilon_\sigma^2+(1+\gamma_b)\delta^2+\gamma_q\big)^\frac12,$ there holds
\begin{equation*}
\int_{\Omega}\Big(\frac{q^\dag-q_\theta^*}{q^\dag}\Big)^2\big(q^\dag |\nabla u^\dag|^2+fu^\dag\big)\ \mathrm{d}x\leq  c\big(\gamma_\sigma^{-\frac12}+\gamma_q^{-\frac12}\eta+1\big) \eta.
\end{equation*}
Moreover, if Condition \ref{Cond:P} holds, then
\begin{equation*}
\|q^\dag-q_\theta^*\|_{L^2(\Omega)}\leq c[\big(\gamma_\sigma^{-\frac12}+\gamma_q^{-\frac12}\eta+1\big) \eta]^{\frac{1}{2(\beta+1)}}.
\end{equation*}
\end{theorem}
\begin{proof}
For any test function $\varphi\in H_0^1(\Omega)$, we have
	\begin{align*}
		\big((q^\dag-\P01(q_\theta^*))\nabla u^\dag,\nabla\varphi\big) &= \big (\sigma^\dag-\sigma_\kappa^*,\nabla\varphi\big)+\big(\P01(q_\theta^*)\nabla(z^\delta-u^\dag),\nabla\varphi\big)+\big(\sigma_\kappa^*-\P01(q_\theta^*)\nabla z^\delta,\nabla\varphi\big)  \\
		& = -\big (\nabla\cdot(\sigma^\dag-\sigma_\kappa^*),\varphi\big)+\big(\P01(q_\theta^*)\nabla(z^\delta-u^\dag), \nabla\varphi\big)+\big(\sigma_\kappa^*-\P01(q_\theta^*)\nabla z^\delta,\nabla\varphi\big).
\end{align*}
Let $\varphi\equiv\frac{q^\dag-\P01(q_\theta^*)}{q^\dag}u^\dag$. Then by direct computation, we have
\begin{equation*}
	\nabla\varphi = \frac{\nabla(q^\dag-\P01(q_\theta^*))}{q^\dag}u^\dag+\frac{(q^\dag-\P01(q_\theta^*))}{q^\dag}\nabla u^\dag- \frac{(q^\dag-\P01(q_\theta^*))}{(q^\dag)^2}(\nabla q^\dag) u^\dag.
\end{equation*}
Using Assumption \ref{ass:Diri}, the box constraint on $q^\dagger$ and $\P01(q_\theta^*)$, and the stability estimate \eqref{eqn:P01-stab} of $P_\mathcal{A}$, we arrive at
\begin{align*}
&\Big\|\frac{\nabla(q^\dag-\P01(q_\theta^*))}{q^\dag}u^\dag\Big\|_{L^2(\Omega)}
\leq c\big(1+\|\nabla \P01(q_\theta^*)\|_{L^2(\Omega)}\big)
\leq c\big(1+\|\nabla q_\theta^*\|_{L^2(\Omega)}\big),\\
&\Big\|\frac{(q^\dag-\P01(q_\theta^*))}{q^\dag}\nabla u^\dag\Big\|_{L^2(\Omega)}+\Big\|\frac{(q^\dag-\P01(q_\theta^*))}{(q^\dag)^2}(\nabla q^\dag) u^\dag\Big\|_{L^2(\Omega)}\leq c .
\end{align*}
This implies $\varphi\in H_0^1(\Omega)$ with the following \textit{a priori} bound
\begin{equation*}
	\|\varphi\|_{L^2(\Omega)} \leq c \quad \text{and}\quad  \|\nabla\varphi\|_{L^2(\Omega)}\leq c(1+\|\nabla q_\theta^*\|_{L^2(\Omega)}).
\end{equation*}
By Lemma \ref{lem: Diri pri sigma q} and the Cauchy-Schwarz inequality, we have
\begin{equation*}
|(\nabla\cdot(\sigma^\dagger-\sigma_\kappa^*),\varphi\big)|\leq  \|\nabla\cdot(\sigma^\dagger-\sigma_\kappa^*)\|_{L^2(\Omega)}\|\varphi\|_{L^2(\Omega)}\leq c\gamma_\sigma^{-\frac12}  \eta.
\end{equation*}
Similarly, we deduce
\begin{equation*}
		|\big(\sigma_\kappa^*-\P01(q_\theta^*)\nabla z^\delta,\nabla\varphi\big)|\leq \|\sigma_{\kappa}^*-q_\theta^*\nabla z^\delta\|_{L^2(\Omega)}\|\nabla\varphi\|_{L^2(\Omega)}\leq c (1+\gamma_q^{-\frac12}\eta)\eta.
\end{equation*}
Meanwhile, it follows from the Cauchy--Schwartz inequality that
\begin{equation*}
|\big(\P01(q_\theta^*)\nabla(z^\delta-u^\dagger), \nabla\varphi\big)|\leq c\|\P01(q_\theta^*)\|_{L^{\infty}(\Omega)}
\|\nabla(z^\delta-u^\dagger)\|_{L^2(\Omega)}\|\nabla\varphi\|_{L^2(\Omega)} \leq c(1+\gamma_q^{-\frac12}\eta)\delta.
\end{equation*}
Upon repeating the argument in \cite{bonito2017diffusion,jin2021error}, we obtain
\begin{equation*}
	\big((q^\dag-\P01(q_\theta^*))\nabla u^\dag,\nabla\varphi\big) = \frac12\int_{\Omega}\Big(\frac{q^\dag-\P01(q_\theta^*)}{q^\dag}\Big)^2\big(q^\dag|\nabla u^\dagger|^2+fu^\dagger\big)\ \mathrm{d}x.
\end{equation*}
Combining the preceding estimates yields the first assertion.
Next we bound $\|q^\dagger- \P01(q_\theta^*)\|_{L^2(\Omega)}$. Upon fixing $\rho>0$, we split the domain $\Omega$ into two disjoint sets $\Omega=\Omega_\rho\cup \Omega_\rho^c$, with $\Omega_\rho=\{x\in\Omega:{\rm dist}(x,\partial\Omega)\geq\rho\}$ and $\Omega_\rho^c=\Omega\setminus\Omega_\rho$. Then by the box constraint $q^\dag\in\mathcal{A}$, we have
\begin{align*}
	\|q^\dagger-\P01(q_\theta^*)\|_{L^2(\Omega_\rho)}^2 &= \rho^{-\beta}\int_{\Omega_\rho }(q^\dagger-\P01(q_\theta^*))^2\rho^{\beta}{\rm d}x
    \leq \rho^{-\beta}\int_{\Omega_\rho }(q^\dag-\P01(q_\theta^*))^2\mathrm{dist}(x,\partial\Omega)^{\beta}{\rm d}x\\
	& \leq c\rho^{-\beta}\int_{\Omega_\rho }\Big(\frac{q^\dag-\P01(q_\theta^*)}{q^\dag}\Big)^2\big(q^\dag |\nabla u^\dag|^2+fu^\dag\big){\rm d}x
    \leq c\rho^{-\beta}\big(\gamma_\sigma^{-\frac12}+\gamma_q^{-\frac12}\eta+1\big)\eta.
\end{align*}
Meanwhile, using the box constraint  $q^\dag,\P01(q_\theta^*)\in\mathcal{A}$, we obtain
\begin{equation*}
\|q^\dag-\P01(q_\theta^*)\|_{L^2(\Omega^c_\rho)}^2
\leq  c \int_{\Omega_\rho^c}1\ {\rm d}x \|q^\dag-\P01(q_\theta^*)\|^{2}_{L^\infty(\Omega_\rho^c)}
\leq c\rho.
\end{equation*}
By combining the last two estimates and then optimizing in $\rho$, we get
the desired bound on $ \|q^\dag-q_\theta^*\|_{L^2(\Omega)}$.
\end{proof}

\subsection{Error analysis of the empirical loss}

Now we analyze the impact of quadrature error on the reconstruction, under the following
condition. 
\begin{assumption}\label{ass:Diri-gen}
 $f\in L^\infty(\Omega)$ and $\nabla z^\delta\in L^{\infty}(\Omega)\cap L^{\infty}(\partial\Omega)$.
\end{assumption}

We have the following error bound on the DNN realization $\widehat{q}_\theta^*$ of a minimizer
$\widehat\theta^*$ of the loss \eqref{equ: Diri empir loss}.
\begin{theorem}\label{thm:err-Diri-emp}
Let Assumptions \ref{ass:Diri} and \ref{ass:Diri-gen} hold. Fix small $\epsilon_q$, $\epsilon_\sigma>0$, and let $(\widehat{\theta}^*,\widehat{\kappa}^*)\in (\mathfrak{P}_{p,\epsilon_q}, \mathfrak{P}_{\infty,\epsilon_\sigma}^{\otimes d})$ be a minimizer of \eqref{equ: Diri empir loss} and $\widehat{q}_\theta^*$ the
DNN realization of $\widehat{\theta}^*$. Fix $\tau\in(0,\frac18)$, let the quantities $e_d$, $e_\sigma$, $e_{b'}$ and
$e_q$ be defined in Theorem \ref{thm:stat-err}, and define $\eta$ by 
$ \eta:=e_d+\gamma_\sigma e_\sigma+\gamma_be_{b'}+\gamma_qe_q
+\epsilon_q^2+ (1+\gamma_\sigma+\gamma_b)\epsilon_\sigma^2+(1+\gamma_b) \delta^2+\gamma_q.$ 
Then with probability at least $1-4\tau$, 
there holds
\begin{equation*}
\int_{\Omega}\Big(\frac{q^\dag-\P01(\widehat{q}_\theta^*)}{q^\dag}\Big)^2\big(q^\dag |\nabla u^\dag|^2+fu^\dag\big)\ \mathrm{d}x\leq  c\big((e_d+\eta)^\frac12+(e_\sigma+\gamma_\sigma^{-1}\eta)^\frac12+\delta(1+e_q+\gamma_q^{-1}\eta)^\frac12\big)\big(1+ e_q+\gamma_q^{-1}\eta\big)^\frac12,
\end{equation*}
where the constant $c>0$ depends on  $|\Omega|$, $|\partial\Omega|$, $d$,  $\|z^\delta\|_{W^{1,\infty} (\partial\Omega)}$, $\|f\|_{L^\infty(\Omega)}$ and $\|q^\dagger\|_{L^\infty(\partial\Omega)}$ at most polynomially.
Moreover, if Condition \ref{Cond:P} holds, then with probability at least $1-4\tau$,
\begin{equation*}
\|q^\dagger-\P01(\widehat{q}_\theta^*)\|_{L^2(\Omega)}\leq c\big(\big((e_d+\eta)^\frac12+(e_\sigma+\gamma_\sigma^{-1}\eta)^\frac12+\delta(1+e_q+\gamma_q^{-1}\eta)^\frac12\big)\big(1+ e_q+\gamma_q^{-1}\eta\big)^\frac12\big)^{\frac{1}{2(\beta+1)}}.
\end{equation*}
\end{theorem}
\begin{proof}
The proof is similar to Theorem \ref{thm:err-Neum-emp}, using
instead the estimate on $e_{b'}$. Indeed, the following estimate holds
\begin{equation*}
\sup_{(\theta,\kappa)\in(\mathfrak{P}_{p,\epsilon_q},\mathfrak{P}_{\infty,\epsilon_\sigma}^{\otimes d})} \big|J_{\bsgamma}(q_\theta,\sigma_\kappa)-\widehat{J}_{\bsgamma}(q_\theta,\sigma_\kappa)\big|\leq
\Delta\mathcal{E}_d+\gamma_\sigma\Delta \mathcal{E}_\sigma+\gamma_{b}\Delta \mathcal{E}_{b'}+\gamma_q\Delta \mathcal{E}_q.
\end{equation*}
Then for any minimizer $(\widehat{\theta}^*,\widehat{\kappa}^*)$ of 
the empirical loss \eqref{equ: Diri empir loss}, with probability at least $1-4\tau$, 
\begin{equation*}
\widehat{J}_{\bsgamma}(\widehat{\theta}^*,\widehat{\kappa}^*)\leq c\big(e_d+\gamma_\sigma e_\sigma+\gamma_be_{b'}+\gamma_qe_q+\epsilon_q^2+(1+\gamma_\sigma+\gamma_b)\epsilon_\sigma^2+(1+\gamma_b)\delta^2+\gamma_q\big).
\end{equation*}
Then by repeating the argument from Theorem \ref{thm:err-Neum-emp} and 
replacing $q_\theta^*$ by $\widehat{q}_\theta^*$, we deduce that 
with probability at least $1-4\tau$, the following three estimates hold simultaneously
\begin{align*}
  \|\widehat{\sigma}_\kappa^*-\P01(\widehat{q}_\theta^*)\nabla z^\delta\|_{L^2(\Omega)}^2\leq c(e_d+\eta), \quad \|f+\nabla\cdot\hat{\sigma}_\kappa^*\|_{L^2(\Omega)}^2\leq c (e_\sigma+\gamma_\sigma^{-1}\eta),\quad
   \|\nabla\widehat{q}_\theta^*\|^2_{L^2(\Omega)}&\leq c(e_q+\gamma_q^{-1}\eta).
\end{align*}
Last, the desired estimates follow from the argument of Theorem
\ref{thm: Diri error estimate on conduc}.
\end{proof}
\begin{remark}
Under the assumptions in Theorem \ref{thm:err-Diri-emp} and the choice of the numbers of sampling points in Remark \ref{rmk:para-Neum}, there holds with probability at least $1-4\tau$, $\|q^\dag- \P01(\widehat{q}_\theta^*)\|_{L^2(\Omega)}\leq c\delta^\frac{1}{2(1+\beta)}$.
\end{remark}

\section{Numerical experiments and discussions}\label{sec:numer}
Now we showcase the performance of the proposed approach. All computations are performed on TensorFlow
1.15.0 using Intel Core i7-11700K Processor with 16 CPUs. We measure the accuracy of a reconstruction $\hat q$ (with respect to the exact one $q^\dag$) by the relative $L^2(\Omega)$ error $e(\hat q)$ defined by
$$e(\hat q)=\|q^\dag-\hat q\|_{L^2(\Omega)}/\|q^\dag\|_{L^2(\Omega)}.$$
Throughout, for an elliptic problem in $\mathbb{R}^d$, we use DNNs with an output dimension $1$ and $d$ to approximate the conductivity 
$q$ and the flux $\sigma$, respectively. Unless otherwise stated, both DNNs have 4 hidden layers (i.e., depth 5) with 26, 26, 26, and 10
neurons on each layer. The activation function $\rho$ is taken to be ${\rm tanh}$. The penalty parameters $\gamma_\sigma$, $\gamma_b$ and $\gamma_q$ are for the divergence term, boundary term and $H^1(\Omega)$-penalty
term, respectively, in the losses \eqref{eqn:obj-Neum} and \eqref{eqn:obj-Diri1}, and are determined in a trial-and-error way. The numbers of training points in the domain $\Omega$ and
on the boundary $\partial\Omega$ are denoted by $n_r$ and $n_b$, respectively. The empirical loss $\widehat{L}_{\bsgamma}$
is  minimized by ADAM \cite{KingmaBa:2015}. We adopt an exponentially decaying learning rate schedule, which is determined by the starting learning rate (lr), decay rate (dr) and the epoch number at which the decay takes place (step). The epoch refers to the total number of epochs used for the reconstruction. Table \ref{tab:algpara}
summarizes the algorithmic parameters for the experiments where the numbers in the brackets indicate the parameters used for noisy data ($\delta=10\%$). 

%We first specify the explicit forms of the voltage potential function $u^\dag$ and the conductivity function $q^\dag$, so that their gradients, $\nabla u^\dag$ and $\nabla q^\dag$, can be derived analytically. In case of the Neumann formulation, data $f$ and $g$ are computed as $f=-\nabla\cdot(q^\dag\nabla u^\dag)$ and $g=q^\dag|_{\partial\Omega}(\nabla u^\dag\cdot \mathbf{n})$. Similarly for the Dirichlet formulation, data $f$ and $\mathbf{g}^\dag$ are computed as $f=-\nabla\cdot(q^\dag\nabla u^\dag)$ and $\mathbf{g}^\dag=q^\dag|_{\partial\Omega}\nabla u^\dag|_{\partial\Omega}$. In case of the Neuamnn problem, noisy observations $\nabla u^\delta$ are generated by adding Gaussian
%random noise pointwise as
%\begin{equation} \label{noisegradu}\nabla u^\delta(x)=\nabla u^\dag(x)+\delta\cdot \nabla u^\dag(x)\xi(x),\end{equation}
%where $\delta\geq0$ denotes the (relative) noise level, and the random variable $\xi(x)$ follows the standard
%Gaussian distribution. In case of the Dirichlet problem, since observations of $\nabla u^\dag$ over the whole domain are perturbed by noise, in addition to $\nabla u^\delta$, we will also have noisy measurements $$\mathbf{g}^\delta = q^\dag|_{\partial\Omega}\cdot\nabla u^\delta|_{\partial\Omega},$$
%where $\nabla u^\delta$ is calculated in the same way as \eqref{noisegradu}.


\begin{table}
\begin{center}
\caption{The algorithmic parameters used for the examples.}\label{tab:algpara}
{
\setlength{\tabcolsep}{.1cm}
\begin{tabular}{c|cccc|cccc}
\toprule
  para. $\backslash$ Ex. No.& \ref{exam:neu1} & \ref{exam:discon}& \ref{exam:neu2}& \ref{exam:neudim5} & \ref{exam:diri1}&\ref{exam:diridisctn}& \ref{exam:diri2}& \ref{exam:diridim5}\\
\midrule
     $\gamma_\sigma$&10(100)&10(100)&10&1(10) & 10&5(1)&10&10\\
     $\gamma_b$&10(100)&10(100)&10(100)&10(100) &10&10&10&10(50)\\
     $\gamma_q$&1e-5&1e-5&1e-5&1e-5&1e-5&1e-5&1e-5&1e-5\\
     $n_r$&4e4&4e4&4e4&6e4&4e4&4e4&4e4&6e4\\
     $n_b$&4e3&4e3&6e3&3e4&4e3&4e3&6e3&3e4\\
     lr&2e-3&1e-3&3e-3&3e-3&2e-3&3e-3&3e-3&3e-3\\
     dr & 0.7 & 0.75 &0.7 & 0.75 & 0.7 &0.8& 0.8 & 0.75\\
     step & 2000 & 1500 & 2500 & 3000 & 2000 &2500& 3000 & 3000\\
     epoch   & 6e4(3e4) &5e4(2e4) &6e4(3e4) & 6e4(2e4)&6e4(2e4)&8e4(2e4)&6e4(3e4)& 6e4(2e4)\\
\bottomrule
\end{tabular}}
\end{center}
\end{table}

\subsection{The Neumann problem}\label{sec:neu}
The first example is about recovering a smooth conductivity $q^\dag$ with three modes in 2D.
\begin{example}
$\Omega = (-1,1)^2$, $q^\dag= 1 +s_1(x_1, x_2) + s_2(x_1, x_2) + s_3(x_1, x_2),$
with $s_1=0.3e^{-20(x_1-0.3)^2-15(x_2-0.3)^2}$,
$s_2= -0.3e^{-10x_1^2-10(x_2+0.5)^2}$ and $s_3 = 0.2e^{-15(x_1+0.4)^2-15(x_2-0.35)^2}$, and $u^\dag=x_1+x_2+\frac{1}{3}(x_1^3+x_2^3)$.
\label{exam:neu1}
\end{example}

\begin{figure}[htbp]
\centering
\setlength{\tabcolsep}{0em}
\begin{tabular}{ccc}
\includegraphics[width=0.32\textwidth]{neu1exact.png} &
\includegraphics[width=0.32\textwidth]{neu1prjr.png} &
\includegraphics[width=0.32\textwidth]{neu1prjer.png}\\
\includegraphics[width=0.32\textwidth]{neu1exact.png} &
\includegraphics[width=0.32\textwidth]{neu1prjn10r.png} &
\includegraphics[width=0.32\textwidth]{neu1prjn10er.png}\\
(a) $q^\dag$  & (b) $\hat q$ & (c) $|\hat q-q^\dag|$
\end{tabular}
\caption{The reconstructions for Example \ref{exam:neu1} with exact data $($top$)$ and noisy data $(\delta=10\%$, bottom$)$.}
\label{fig:neu1}
\end{figure}

This choice of $u^\dag$ ensures that both $\nabla u^\dag$ and $\Delta u^\dag$ do not vanish over the domain $\Omega$, which are beneficial for the numerical construction and is also important for theoretical analysis \cite{Alessandrini:1986}. %The training process took 50,000
%epochs (20,000 epochs for noisy data). 
Fig. \ref{fig:neu1} shows the
reconstructions for exact data (top) and noisy data (bottom), where
the pointwise errors $|\hat q-q^\dag|$ are also shown. The shape and
location of the modes and the overall structure of $q^\dag$ are well resolved, except that the
middle part between the top two bumps were slightly bridged. The method is observed to be very robust with respect to data noise, except very slight under-estimation of the peak values of the top two bumps, even in the presence of 10\% noise. Surprisingly, the approach seems fairly stable with respect to the iteration index, cf. Fig. \ref{fig:neu1losse}. Indeed, the presence of the noise does not affect much the convergence behavior of the loss and the error, and the final values of the error are close for all the noise levels.
This observation agrees well with that for the pointwise error in Fig. \ref{fig:neu1}.

\begin{figure}[htbp]
\centering
\setlength{\tabcolsep}{0em}
\begin{tabular}{ccc}
\includegraphics[width=0.32\textwidth]{neu1prjl.png} &
\includegraphics[width=0.32\textwidth]{neu1prjn1l.png} &
\includegraphics[width=0.32\textwidth]{neu1prjn10l.png}\\
\includegraphics[width=0.32\textwidth]{neu1prje.png} &
\includegraphics[width=0.32\textwidth]{neu1prjn1e.png} &
\includegraphics[width=0.32\textwidth]{neu1prjn10e.png}\\
(a) $\delta=0\%$  & (b) $\delta=1\%$ & (c) $\delta=10\%$
\end{tabular}
\caption{The variation of the loss $($top$)$ and the relative $L^2(\Omega)$ error $e$ $($bottom$)$ during the training process for Example \ref{exam:neu1} at different noise levels.}
\label{fig:neu1losse}
\end{figure}

There are several algorithmic parameters influencing the overall accuracy of the DNN approximation $\hat q$, e.g., the number of training points ($n_r$ and $n_b$), DNN architectural parameters {(width, depth, and activation function)} and noise level $\delta$. A practical yet provable guidance
for choosing these parameters is unfortunately still missing. Instead, we explore the issue empirically.
Table \ref{table1} shows the relative $L^2(\Omega)$-error of the
reconstruction $\hat q$ at different noise levels
and different $\gamma_q$. The method is observed to be very robust with respect to the presence
of data noise - the results remain fairly accurate even for up to 10\% data noise, and do
not vary much with the penalty parameter $\gamma_q$ (for the $H^1(\Omega)$-penalty), provided that it is properly chosen. However,
there is also an inherent accuracy limitation of the approach, i.e., the reconstruction cannot be made
arbitrarily accurate for exact data $\nabla u^\dag$. This may be attributed to the optimization
error: due to the nonconvexity of the loss landscape, the optimizer may fail to find a global minimizer but instead only an approximate local minimizer. The saturation phenomenon has been consistently observed across a broad range of solvers based on DNNs
\cite{RaissiPerdikarisKarniadakis:2019,EYu:2018,jin2022imaging}. Tables \ref{table3}-\ref{table4}
show that the relative $L^2(\Omega)$ error $e(\hat q)$ of the reconstruction $\hat q$ does not vary much
with different DNN architectures and numbers of training points. This observation
agrees with the convergence behavior of the optimization algorithm in Fig.
\ref{fig:neu1losse}, where the value of the loss $\hat J_{\bsgamma}$ and the error $e(\hat q)$ eventually stagnates
at a certain level.

\begin{table}[htp!]
  \centering
  \caption{The variation of the relative $L^2(\Omega)$ error $e(\hat q)$ with respect to various algorithmic parameters. }
\begin{threeparttable}


\subfigure[$e$ v.s. $\gamma_q$ and $\delta$\label{table1}]{\begin{tabular}{c|ccc}
\toprule
  $\gamma_q\backslash\delta$&   0\% &  1\%& 10\%\\
\midrule
     1e-2&  2.03e-2 &1.95e-2&4.36e-2 \\
     1e-3&  1.10e-2 &8.86e-3&4.30e-2\\
     1e-4&  1.01e-2 &8.52e-3&4.30e-2\\
     1e-5&  1.35e-2 &1.27e-2&4.30e-2\\
\bottomrule
\end{tabular}}
\subfigure[$e$ v.s. $W$ and  $L$\label{table3}]{
\begin{tabular}{c|ccc}
\toprule
${W}\backslash L$&   5 &  10& 20\\
\midrule
     4&8.58e-2&1.11e-1&2.77e-1 \\
     12&2.27e-2&3.32e-2&9.91e-3\\
     26&1.35e-2&3.88e-3&1.38e-2\\
     40&1.15e-2&1.26e-2&5.17e-3\\
\bottomrule
\end{tabular}}
\subfigure[$e$ v.s. $n_r$ and $n_b$\label{table4}]{
\begin{tabular}{c|cccc}
\toprule
$n_b\backslash n_r$&   5000 &  10000& 20000&40000\\
\midrule
     500&  1.25e-2&1.52e-2&1.23e-2&9.14e-3 \\
     1000& 9.87e-3&2.79e-2&2.91e-2&1.34e-2\\
     4000& 9.71e-3&1.55e-2&1.39e-2&1.35e-2\\

\bottomrule
\end{tabular}}
\end{threeparttable}
\end{table}



\begin{figure}[htbp]
\centering
\setlength{\tabcolsep}{0em}
\begin{tabular}{ccc}
\includegraphics[width=0.32\textwidth]{neudisctnexact.png} &
\includegraphics[width=0.32\textwidth]{neudisctnr.png} &
\includegraphics[width=0.32\textwidth]{neudisctner.png}\\
\includegraphics[width=0.32\textwidth]{neudisctnexact.png} &
\includegraphics[width=0.32\textwidth]{neudisctnn10r.png} &
\includegraphics[width=0.32\textwidth]{neudisctnn10er.png}\\
(a) $q^\dag$  & (b) $\hat q$ & (c) $|\hat q-q^\dag|$
\end{tabular}
\caption{The reconstructions for Example \ref{exam:discon} with exact data $($top$)$ and noisy data $($$\delta=10\%$, bottom$)$.}
\label{fig:neudisctn}
\end{figure}

\begin{figure}[htbp]
\centering
\setlength{\tabcolsep}{0em}
\begin{tabular}{ccc}
\includegraphics[width=0.32\textwidth]{neudisctnexact.png} &
\includegraphics[width=0.32\textwidth]{neudisctntvr.png} &
\includegraphics[width=0.32\textwidth]{neudisctntver.png}\\
\includegraphics[width=0.32\textwidth]{neudisctnexact.png} &
\includegraphics[width=0.32\textwidth]{neudisctntvrn10.png} &
\includegraphics[width=0.32\textwidth]{neudisctntvern10.png}\\
(a) $q^\dag$  & (b) $\hat q$ & (c) $|\hat q-q^\dag|$
\end{tabular}
\caption{The reconstructions for Example \ref{exam:discon} using a loss function including the total variation term, with exact data $($top$)$ and noisy data $($$\delta=10\%$, bottom$)$.}
\label{fig:neudisctntv}
\end{figure}


The second example is about recovering a discontinuous conductivity $q^\dag$.
The notation $\chi_S$ denotes the characteristic function of the set $S$.
\begin{example} \label{exam:discon}
$\Omega = (-1,1)^2$,
$q^\dag = 1+0.25\cdot\chi_{\{(x_1+0.15)^2+(x_2+0.3)^2\leq0.25^2\}}$, $f\equiv0$ and $g=x_1$.
\end{example}

Given the exact conductivity $q^\dag$, the field $u^\dag$ is computed by solving the Neumann problem \eqref{equ:Neu problem} using the public software package FreeFEM++ \cite{Hecht:2012}, and the (exact) observation $\nabla u^\dag$
at the training points is evaluated by numerical interpolation. Since $q^\dag$
is piece-wise constant, we may also include the popular total variation penalty \cite{RudinOsherFatemi:1992},
i.e., $\gamma_{tv}| q|_{\rm TV}$, to the loss $J_{\bsgamma}(\theta,\kappa)$ to
promote piecewise constancy of the reconstruction. Fig. \ref{fig:neudisctn}
shows the error plots without the
total variation term, and Fig.  \ref{fig:neudisctntv} including the total variation
term ($\gamma_{tv}=0.01$ for both exact and noisy data). The reconstruction quality is improved by including the total variation term. However, the reconstruction $\hat q_\theta$ still exhibits slight blurring effect around the discontinuous interface. Moreover, it is worth
noting that the reconstruction $\hat q$ remains accurate for up to $10\%$ data noise, indicating high robustness of the approach with respect to noise.

\begin{figure}[htbp]
\centering
\setlength{\tabcolsep}{0em}
\begin{tabular}{ccc}
\includegraphics[width=0.33\textwidth]{neu2ex.png} &
\includegraphics[width=0.33\textwidth]{neu2r.png} &
\includegraphics[width=0.33\textwidth]{neu2er.png}\\
\includegraphics[width=0.33\textwidth]{neu2ex.png} &
\includegraphics[width=0.33\textwidth]{neu2n10r.png} &
\includegraphics[width=0.33\textwidth]{neu2n10er.png}\\
(a) $q^\dag$  & (b) $\hat q$ & (c) $|\hat q-q^\dag|$
\end{tabular}
\caption{The reconstructions for Example \ref{exam:neu2} with exact data $($top$)$ and  noisy data $(\delta=10\%$, bottom$)$.}
\label{fig:neu2}
\end{figure}

The third example is about recovering a conductivity coefficient in 3D.
\begin{example}\label{exam:neu2}
$\Omega=(0,1)^3$, $q^\dag=1 + 0.3e^{-20(x_1-0.5)^2-20(x_2-0.5)^2-20(x_3-0.5)^2}$, and $u^\dag=x_1+x_2+x_3+\frac{1}{3}(x_1^3+x_2^3+x_3^3)$.
\end{example}

%The training process took 60,000 epochs (30,000 for the reconstruction with noisy data). 
Fig. \ref{fig:neu2} shows the reconstruction at a 2D cross section, i.e., the surface $x_3=0.5$, using the exact data $\nabla u^\dag$ and noisy data ($\delta=10\%$). The shape and the overall structure of $q^\dag$ are well recovered in both cases. The relative $L^2(\Omega)$-error $e(\hat q)$ is 1.49e-2 and 3.42e-2,
for exact and noisy data, respectively. The error in the noisy case is higher than that for exact data, but still quite acceptable. This again shows the high robustness of the approach with respect to noise.

\begin{figure}[htbp]
\centering
\setlength{\tabcolsep}{0em}
\begin{tabular}{ccc}
\includegraphics[width=0.32\textwidth]{neudim5ex.png} &
\includegraphics[width=0.32\textwidth]{neudim5r.png} &
\includegraphics[width=0.32\textwidth]{neudim5er.png}\\
\includegraphics[width=0.32\textwidth]{neudim5ex.png} &
\includegraphics[width=0.32\textwidth]{neudim5n10r.png} &
\includegraphics[width=0.32\textwidth]{neudim5n10er.png}\\
(a) $q^\dag$  & (b) $\hat q$ & (c) $|\hat q-q^\dag|$
\end{tabular}
\caption{The reconstructions for Example \ref{exam:neudim5} with exact data $($top$)$ and noisy data $(\delta=10\%$, bottom$)$.}
\label{fig:neudim5}
\end{figure}

The last example is about recovering a conductivity function in 5D.
\begin{example}\label{exam:neudim5}
$\Omega=(0,1)^5$, $q^\dag=1-(x_1-0.5)^2-(x_2-0.5)^2+\cos(\pi(x_3+1.5))+\cos(\pi(x_4+1.5))+\cos(\pi(x_5+1.5))$, and $u^\dag=x_1+x_2+x_3+x_4+x_5+\frac{1}{3}(x_1^3+x_2^3+x_3^3+x_4^3+x_5^3)$.
\end{example}

%The training process took 60,000 epochs (20,000 for the noisy data). 
Fig. \ref{fig:neudim5} shows the reconstruction on a 2D cross section of the domain $\Omega$, i.e., the
surface ($x_3=x_4=x_5=0.5$). The relative $L^2(\Omega)$-error $e(\hat q)$ for the exact and noisy data is 3.44e-3
and 2.27e-2, respectively. The DNN approximations do capture the overall structure of the exact conductivity $q^\dag$
for both exact and noisy data, showing again the remarkable robustness of the method
with respect to the presence of noise. Moreover, it also shows the advantage of using DNNs: it can solve inverse problems for high-dimensional PDEs, which is not easily
tractable for more traditional approaches, e.g.,  FEM. Surprisingly, the approach can produce high
quality results without using too many training points in the domain $\Omega$, despite the high-dimensionality of the problem.

\subsection{The Dirichlet problem}\label{sec:diri}

\begin{figure}[htbp]
\centering
\setlength{\tabcolsep}{0em}
\begin{tabular}{ccc}
\includegraphics[width=0.32\textwidth]{diri1exact.png} &
\includegraphics[width=0.32\textwidth]{diri1r.png} &
\includegraphics[width=0.32\textwidth]{diri1er.png}\\
\includegraphics[width=0.32\textwidth]{diri1exact.png} &
\includegraphics[width=0.32\textwidth]{diri1n10r.png} &
\includegraphics[width=0.32\textwidth]{diri1n10er.png}\\
(a) $q^\dag$  & (b) $\hat q$ & (c) $|\hat q-q^\dag|$
\end{tabular}
\caption{The reconstructions for Example \ref{exam:diri1} with exact data $($top$)$ and noisy data $(\delta=10\%$, bottom$)$.}
\label{fig:diri1}
\end{figure}

The first example is about recovering a smooth conductivity in 2D.
\begin{example}\label{exam:diri1}
$\Omega=(-1,1)^2$, $q^\dag = 1 +s_1(x_1, x_2) + s_2(x_1, x_2)$
with $s_1=0.4e^{-15(x_1-0.5)^2-15x_2^2}$ and
$s_2 = -0.4e^{-15(x_1+0.5)^2-15x_2^2}$, and $u^\dag=x_1+x_2+\frac{1}{3}(x_1^3+x_2^3)$.
\end{example}

%The training process took 60,000 epochs (20,000 for noisy data). 
Fig. \ref{fig:diri1} shows the reconstruction $\hat q$ using the loss
\eqref{eqn:obj-Diri1} with exact data
(top) and noisy data (bottom, $\delta=10\%$). The relative $L^2(\Omega)$-error of the reconstructions
is 9.72e-3 and 3.78e-2 for exact and noisy data, respectively. Again we observe the robustness
of the proposed approach in the presence of noise - there is almost no degradation in the reconstruction
quality, except slight underestimation of the peak values of the two bumps and very mild
oscillations near the boundary. The locations and shape of the bumps were well captured.
The convergence behavior of the optimizer in Fig. \ref{fig:diri1losse} shows that
the loss $J_{\bsgamma}$ and  error $e(\hat q)$ stagnate at a comparable level regardless of
the noise level. In light of Remark \ref{rmk:loss}, Fig. \ref{fig:diri1sb} shows the results by the loss \eqref{eqn:diriloss1} with exact (top), $1\%$ noise (middle) and $\delta=10\%$ (bottom). The accuracy of reconstructions with exact data and data with $1\%$ noise is satisfactory, but the neural network fails to learn accurately using the loss \eqref{eqn:diriloss1} when the data is very noisy.

\begin{figure}[htbp]
\centering
\setlength{\tabcolsep}{0em}
\begin{tabular}{ccc}
\includegraphics[width=0.32\textwidth]{diri1prjl.png} &
\includegraphics[width=0.32\textwidth]{diri1n1prjl.png} &
\includegraphics[width=0.32\textwidth]{diri1n10prjl.png}\\
\includegraphics[width=0.32\textwidth]{diri1prje.png} &
\includegraphics[width=0.32\textwidth]{diri1n1prje.png} &
\includegraphics[width=0.32\textwidth]{diri1n10prje.png}\\
(a) $\delta=0\%$  & (b) $\delta=1\%$ & (c) $\delta=10\%$
\end{tabular}
\caption{The variation of the loss $($top$)$ and relative $L^2(\Omega)$ error $e(\hat q)$ $($bottom$)$ during the training process for Example \ref{exam:diri1} at three different noise levels.}
\label{fig:diri1losse}
\end{figure}

    
\begin{figure}[htbp!]
\centering
\setlength{\tabcolsep}{0em}
\begin{tabular}{ccc}
\includegraphics[width=0.32\textwidth]{diri1exact.png} &
\includegraphics[width=0.32\textwidth]{diri1sbr.png} &
\includegraphics[width=0.32\textwidth]{diri1sber.png}\\
\includegraphics[width=0.32\textwidth]{diri1exact.png} &
\includegraphics[width=0.32\textwidth]{diri1sbn1r.png} &
\includegraphics[width=0.32\textwidth]{diri1sbn1er.png}\\
\includegraphics[width=0.32\textwidth]{diri1exact.png} &
\includegraphics[width=0.32\textwidth]{diri1sbn10r.png} &
\includegraphics[width=0.32\textwidth]{diri1sbn10er.png}\\
(a) $q^\dag$  & (b) $\hat q$ & (c) $|\hat q-q^\dag|$
\end{tabular}
\caption{The reconstructions for Example \ref{exam:diri1} with exact data $($top$)$ and noisy data $($$\delta=1\%$, middle and $\delta=10\%$, bottom$)$.}
\label{fig:diri1sb}
\end{figure}



The second example is concerned with recovering a nearly piecewise constant conductivity function in 2D.
\begin{example}\label{exam:diridisctn}
$\Omega=(0,1)^2$, $q^\dag=1+0.3/(1+\tau(x,y))$, with $\tau(x,y)=e^{400((x-0.65)^2+2(y-0.7)^2-0.15^2)}$, and $u^\dag=\sin(\pi x)\sin(\pi y)$.
\end{example}

Note that in this example, %$u|_{\partial \Omega}=0$ and 
$\nabla u$ vanishes at the four corners of the domain and the point $(\frac{1}{2},\frac{1}{2})$. Similar to Example \ref{exam:discon}, an additional total variation penalty,
i.e., $\gamma_{tv}| q|_{\rm TV}$ ($\gamma_{tv}=0.01$) is added to the loss $J_{\bsgamma}(\theta,\kappa)$ in order to promote piecewise constancy of the reconstruction. Fig. \ref{fig:diridisctn} shows the reconstruction using the loss
\eqref{eqn:obj-Diri1} with
exact (top) and noisy (bottom, $\delta=10\%$) data. It is observed that the reconstruction is accurate for exact data, and in the presence of $10\%$ data noise, the reconstruction quality deteriorates only near the points where $\nabla u$ vanishes. This is expected since substituting $\nabla u =0$ back into the formulation \eqref{eqn:obj-Diri1} leads to a loss essentially independent of $q_{\theta}$, thus the inaccuracy in the reconstruction in a small neighbourhood around these points.

\begin{figure}[htbp]
\centering
\setlength{\tabcolsep}{0em}
\begin{tabular}{ccc}
\includegraphics[width=0.33\textwidth]{diridisctnex.png} &
\includegraphics[width=0.33\textwidth]{diridisctnr.png} &
\includegraphics[width=0.33\textwidth]{diridisctner.png}\\
\includegraphics[width=0.33\textwidth]{diridisctnex.png} &
\includegraphics[width=0.33\textwidth]{diridisctnn10r.png} &
\includegraphics[width=0.33\textwidth]{diridisctnn10er.png}\\
(a) $q^\dag$  & (b) $\hat q$ & (c) $|\hat q-q^\dag|$
\end{tabular}
\caption{The reconstructions for Example \ref{exam:diridisctn} with exact data $($top$)$ and noisy data $(\delta=10\%$, bottom$)$.}
\label{fig:diridisctn}
\end{figure}

\begin{figure}[htbp]
\centering
\setlength{\tabcolsep}{0em}
\begin{tabular}{ccc}
\includegraphics[width=0.33\textwidth]{diri2ex.png} &
\includegraphics[width=0.33\textwidth]{diri2r.png} &
\includegraphics[width=0.33\textwidth]{diri2er.png}\\
\includegraphics[width=0.33\textwidth]{diri2ex.png} &
\includegraphics[width=0.33\textwidth]{diri2n10r.png} &
\includegraphics[width=0.33\textwidth]{diri2n10er.png}\\
(a) $q^\dag$  & (b) $\hat q$ & (c) $|\hat q-q^\dag|$
\end{tabular}
\caption{The reconstructions for Example \ref{exam:diri2} with exact data $($top$)$ and noisy data $(\delta=10\%$, bottom$)$.}
\label{fig:diri2}
\end{figure}


The third example is about recovering a conductivity in 3D.
\begin{example}\label{exam:diri2}
$\Omega=(0,1)^3$, $q^\dag=1+e^{-(12(x_1-0.5)(x_2-0.5))^2}+x_3$, and $u^\dag=x_1+x_2+x_3+\frac{1}{3}(x_1^3+x_2^3+x_3^3)$.
\end{example}

%The training process takes 60,000 epochs (20,000 for noisy data). 
Fig. \ref{fig:diri2} show the reconstruction using the loss
\eqref{eqn:obj-Diri1} on a 2D cross section at $x_3=0.5$, with
exact (top) and noisy (bottom, $\delta=10\%$) data, with the relative $L^2(\Omega)$-error being 3.28e-3 and 3.48e-2, respectively, indicating the excellent robustness of the approach with respect to data noise: there
is only very mild deterioration in the reconstruction $\hat q$.

\begin{figure}[htbp]
\centering
\setlength{\tabcolsep}{0em}
\begin{tabular}{ccc}
\includegraphics[width=0.32\textwidth]{diridim5ex.png} &
\includegraphics[width=0.32\textwidth]{diridim5r.png} &
\includegraphics[width=0.32\textwidth]{diridim5er.png}\\
\includegraphics[width=0.32\textwidth]{diridim5ex.png} &
\includegraphics[width=0.32\textwidth]{diridim5n10r.png} &
\includegraphics[width=0.32\textwidth]{diridim5n10er.png}\\
(a) $q^\dag$  & (b) $\hat q$ & (c) $|\hat q-q^\dag|$
\end{tabular}
\caption{The reconstructions for Example \ref{exam:diridim5} with exact data $($top$)$ and noisy data $(\delta=10\%$, bottom$)$.}
\label{fig:diridim5}
\end{figure}


The last example is concerned with recovering a conductivity function in high-dimension.
\begin{example}\label{exam:diridim5}
$\Omega=(0,1)^5$, $q^\dag=1+0.5(x_1x_5+x_2x_4+x_3^2)-0.3e^{-25(x_1-0.5)^2-25(x_2-0.5)^2}$, and $u^\dag=x_1+x_2+x_3+x_4+x_5+\frac{1}{3}(x_1^3+x_2^3+x_3^3+x_4^3+x_5^3)$.
\end{example}

%The training process took 60,000 epochs (20,000 for noisy data). 
Fig. \ref{fig:diridim5} shows the reconstructions using the loss \eqref{eqn:obj-Diri1} on a 2D cross section with $x_3=x_4=x_5=0.5$, with exact (top) and noisy (bottom, $\delta=10\%$) data. The relative $L^2(\Omega)$-error is 5.78e-3 and 2.86e-2 for exact and noisy data, respectively. The features of the true conductivity have been successfully recovered and visually there is almost no difference between the reconstructions  from exact and noisy data. This again showcases the significant potential of NN approximations compared to more traditional approaches for solving high-dimensional inverse problems.

\appendix

\section{The proof of Theorem \ref{thm:stat-err}} 
In this appendix, we prove Theorem \ref{thm:stat-err}, which gives high probability bounds on the statistical errors 
$\Delta \mathcal{E}_{i}$, $i\in \{d,b,b',q,\sigma\}$. These bounds play a crucial role in deriving error estimates in Theorems \ref{thm:err-Neum-emp} and \ref{thm:err-Diri-emp}. By slightly abusing the notation, let $\mathcal{N}_\sigma\equiv \mathcal{N}(L_\sigma,W_\sigma,R_\sigma)$ 
and $\mathcal{N}_q=\mathcal{N}(L_q,W_q,R_q)$ be two DNN function
classes of given depth, width and parameter bound for approximating the current density $\sigma$ and the conductivity $q$, respectively.
Then we define the following function classes
\begin{align*}
  \mathcal{H}_d  &= \{h: \Omega\to \mathbb{R}|\,\, h(x) = \|\sigma_\kappa(x)-P_\mathcal{A}(q_\theta(x))\nabla z^\delta(x)\|_{\ell^2}^2, \sigma_\kappa\in \mathcal{N}_\sigma, q_\theta\in \mathcal{N}_q\},\\
  \mathcal{H}_\sigma & = \{h: \Omega\to \mathbb{R}|\,\, h(x) = |\nabla\cdot \sigma_\kappa(x)+f(x)|^2, \sigma_\kappa\in \mathcal{N}_\sigma\},\\
  \mathcal{H}_b & = \{h:\partial\Omega\to \mathbb{R}|\,\, h(x) = |\n \cdot \sigma_\kappa(x)-g(x)|^2, \sigma_\kappa\in \mathcal{N}_\sigma\},\\
  \mathcal{H}_{b'} & = \{h:\partial\Omega\to \mathbb{R}|\,\, h(x) = \|\sigma_\kappa(x)-q^\dag \nabla z^\delta\|_{\ell^2}^2, \sigma_\kappa\in \mathcal{N}_\sigma\},\\
  \mathcal{H}_q & = \{h:\Omega\to \mathbb{R}|\,\, h(x) =  \|\nabla q_\theta(x)\|_{\ell^2}^2, q_\theta\in \mathcal{N}_q\}.
\end{align*}

To bound these errors, we employ Rademacher complexity \cite{BartlettMendelson:2002} and boundedness and
Lipschitz continuity of DNN functions and their derivatives with respect to the DNN parameters.
Rademacher complexity measures the complexity of a collection of functions
by the correlation between function values with Rademacher random variables.
\begin{definition}\label{def: Rademacher}
Let $\mathcal{F}$ be a real-valued function class defined on the domain $\Omega$ {\rm(}or the boundary $\partial\Omega${\rm)} and $\xi=\{\xi_j\}_{j=1}^n$ be i.i.d. samples from the distribution $\mathcal{U}(\Omega)$ {\rm(}or the distribution $\mathcal{U}(\partial\Omega)${\rm)}. Then
	the Rademacher complexity $\mathfrak{R}_n(\mathcal{F})$ of the class $\mathcal{F}$ is defined by
	\begin{equation*}
		\mathfrak{R}_n(\mathcal{F})=\mathbb{E}_{\xi,\omega}\bigg{[}\sup_{v\in\mathcal{F}}\ n^{-1}\bigg{\lvert}\ \sum_{j=1}^{n}\omega_j v(\xi_j)\ \bigg{\rvert} \bigg{]},
	\end{equation*}
	where $\omega=\{\omega_j\}_{j=1}^n$ are i.i.d  Rademacher random
	variables with probability
	$P(\omega_j=1)=P(\omega_j=-1)=\frac12$.
\end{definition}

We use the following PAC-type generalization bound
\cite[Theorem 3.1]{mohri2018foundations} via Rademacher complexity. Note that the statement in \cite[Theorem 3.1]{mohri2018foundations} only discusses the case of the function class ranging in $[0 ,1]$. For a general bounded function, applying McDiarmids inequality and the original argument yields the following result.
\begin{lemma}\label{lem:PAC}
Let $X_1,\ldots,X_n$ be a set of i.i.d. random variables. Let $\mathcal{F}$
be a function class defined on $D$ such that $\sup_{v\in\mathcal{F}}\|v\|_{L^\infty(D)}\leq M_\mathcal{F}<\infty$. Then for any $\tau\in(0,1)$, with probability at least 
$1-\tau$:
\begin{equation*}
  \sup_{v\in \mathcal{F}}\bigg|n^{-1}\sum_{j=1}^n v(X_j)-\mathbb{E}[v(X)]\bigg| \leq 2\mathfrak{R}_n(\mathcal{F}) + 2M_\mathcal{F}\sqrt{\frac{\log\frac{1}{\tau}}{2n}}.
\end{equation*}
\end{lemma}


To apply Lemma \ref{lem:PAC}, we have to bound the Rademacher complexity of the function classes $\mathcal{H}_i$, $i\in\{d,\sigma,b,b',q\}$. This is achieved by combining Lipschitz continuity of DNN functions (or its derivatives) with the target function class in the DNN parameter, and Dudley's formula in Lemma \ref{lem:Dudley}. The next lemma gives useful boundedness and Lipschitz continuity of a $\tanh$-DNN function
class with respect to the DNN parameters. Note that Lemma \ref{lem:NN-Lip} also holds with
the $L^\infty(\partial\Omega)$ norm in place of the $L^\infty(\Omega)$ norm, since the
overall argument depends only on the boundedness of the activation function $\rho=\tanh$ and
its derivative on $\mathbb{R}$. 
\begin{lemma}\label{lem:NN-Lip}
Let $\Theta$ be a parametrization with depth $L$ and width $W$, and $\theta=\{(A^{(\ell)},b^{(\ell)})_{\ell=1}^L\}, \tilde{\theta}=\{(\tilde{A}^{(\ell)},\tilde{b}^{(\ell)})_{\ell=1}^L\}\in\Theta$. Then for the DNN realizations
$v,\tilde{v}:\Omega\to\mathbb{R}$ of $\theta,\tilde{\theta}$ with $\|\theta\|_{\ell^\infty},
\|\tilde{\theta}\|_{\ell^\infty}\leq R$, the following estimates hold
\begin{enumerate}
\item[{\rm(i)}] $\|v\|_{L^\infty(\Omega)}\leq R(W+1), \quad \|\nabla v\|_{L^\infty(\Omega; \mathbb{R}^d)}\leq \sqrt{d}R^LW^{L-1}$;
\item[{\rm(ii)}] $\|v-\tilde{v}\|_{L^\infty(\Omega)}\leq 2LR^{L-1}W^{L}\|\theta-\tilde\theta\|_{\ell^\infty} ,\quad\|\nabla (v-\tilde{v})\|_{L^\infty(\Omega; \mathbb{R}^d)}\leq  \sqrt{d}L^2R^{2L-2}W^{2L-2}\|\theta-\tilde\theta\|_{\ell^\infty}$.
\end{enumerate}
\end{lemma}
\begin{proof}
All the estimates are already contained in \cite{jin2022imaging}; see \cite[p. 19 and p. 22 of Lemma 3.4]{jin2022imaging} and \cite[Remark 3.3]{jin2022imaging}. 
%We only sketch the proof of the bounds on $\|v\|_{L^\infty(\Omega)}$ and $\|v-\tilde v\|_{L^\infty(\Omega)}$, since the other two bounds are given explicitly, cf. \cite[Remark 3.3]{jin2022imaging}. By the NN realization \eqref{eqn:NN-realization}, we have \begin{align*} v&=A^{(L)}\rho(A^{(L-1)}\rho(A^{(L-2)}\cdots+b^{(L-2)})+b^{(L-1)})+b^{(L)},\ \mbox{with}\ A^{(L)}\in\mathbb{R}^{1\times d_{L-1}},\ b^{(L)}\in\mathbb{R}, \\\tilde{v}&=\tilde{A}^{(L)}\rho(\tilde{A}^{(L-1)}\rho(\tilde{A}^{(L-2)}\cdots+\tilde{b}^{(L-2)})+\tilde{b}^{(L-1)})+\tilde{b}^{(L)},\ \mbox{with}\ \tilde{A}^{(L)}\in\mathbb{R}^{1\times d_{L-1}},\ \tilde{b}^{(L)}\in\mathbb{R}. \end{align*}
%First, we bound $v$  in the $L^\infty(\Omega)$ norm. Let $A^{(L)}=(a^{(L)}_1,a^{(L)}_2,\dots,a^{(L)}_{d_{L-1}})\in \mathbb{R}^{1\times d_{L-1}}$. Since $\|\rho\|_{L^\infty(\mathbb{R})}=1$, by the box constraint $\|\theta\|_{\ell^\infty} \leq R$, we arrive at
%\begin{equation*}
%\|v\|_{L^\infty(\Omega)}=\|A^{(L)}v^{(L-1)}+b^{(L)}\|_{L^\infty(\Omega)}\leq\sum_{j=1}^{d_{L-1}}|a_j^{(L)}|+|b^{(L)}|\leq R(W+1).
%\end{equation*}
%Next we bound $\|v-\tilde{v}\|_{L^\infty(\Omega)}$. Note that the following estimate holds \cite[Lemma 3.4, eq. (3.7)]{jin2022imaging}
%\begin{equation*}
%\|v_j^{(L-1)}-\tilde{v}_j^{(L-1)}\|_{L^\infty(\Omega)}\leq2(L-1)R^{L-2} W^{L-1}\|\theta-\tilde\theta\|_{\ell^\infty}, \quad j=1,\ldots,d_{L-1}.
%\end{equation*}
%Thus, by the triangle inequality,
%\begin{align*}		\|v-\tilde{v}\|_{L^\infty(\Omega)}&=\|A^{(L)}v^{(L-1)}+b^{(L)}-\big(\tilde{A}^{(L)}\tilde{v}^{(L-1)}+\tilde{b}^{(L)}\big)\|_{L^\infty(\Omega)}\\
%&\leq\|\big(A^{(L)}-\tilde{A}^{(L)}\big)v^{(L-1)}\|_{L^\infty(\Omega)}+\|\tilde{A}^{(L)}(v^{(L-1)}-\tilde{v}^{(L-1)})\|_{L^\infty(\Omega)} +\|b^{(L)}-\tilde{b}^{(L)}\|_{L^\infty(\Omega)} \\  &\leq (W+2(L-1)R^{L-1}W^{L}+1)\|\theta-\tilde\theta\|_{\ell^\infty}\leq 2LR^{L-1}W^L\|\theta-\tilde \theta\|_{\ell^\infty}. \end{align*}
%This and \cite[Lemma 3.4]{jin2022imaging} complete the proof of the lemma.
%{\color{blue}[qqm: it seems that we do not need to sketch the proof since the results also could be found in \cite[Lemma 3.4 and Lemma 3.6]{jin2022imaging}? For example, see \cite[the bottom of pp. 19]{jin2022imaging} for the term $\|v-\tilde v\|_{L^\infty(\Omega)}$ and \cite[the bottom of pp. 22]{jin2022imaging} for the term $\|v\|_{L^\infty(\Omega)}$.]}
\end{proof}

\begin{comment}
First, we bound the errors $\Delta \mathcal{E}_i$, $i\in \{d,b,b',q,\sigma\}$,
by the Rademacher complexities.
\begin{lemma}\label{lem:NN-stat}
The errors $\Delta\mathcal{E}_i$, $i\in \{d,b,b',q,\sigma\}$, are bounded by 
\begin{equation*}
  \Delta\mathcal{E}_i \leq 2\mathcal{R}_n(\mathcal{H}_i),\quad i\in \{d,b,b',q,\sigma\}.
\end{equation*}
\end{lemma}
\begin{proof}
The proof follows by a standard symmetrization argument. First we prove the estimate for $\Delta\mathcal{E}_d$. 
Let $\tilde X = \{\tilde{X}_{j}\}_{j=1}^n$ be independent
samples drawn from the uniform distribution $\mathcal{U}(\Omega)$, independent 
of the samples $\{X_j\}_{j=1}^n$. Then by the definition of $\mathcal{\widehat{E}}_d
(\sigma_{\kappa}, q_\theta)$, Jensen's inequality with respect to 
$\mathbb{E}_{\tilde{X}}$ (since $\sup$ is a convex function), Fubini's Theorem 
and the triangle inequality, we have
\begin{align*}
\mathbb{E}_{X}\big[\Delta\mathcal{E}_d\big] &= \mathbb{E}_{X} \bigg[\sup_{\sigma_{\kappa}\in\mathcal{N}_{\sigma},q_\theta\in\mathcal{N}_q}\bigg| n^{-1}|\Omega|\sum_{j=1}^n\mathbb{E}_{\tilde{X}}[h(\tilde{X}_j)]-\mathcal{\widehat{E}}_d(\sigma_{\kappa},q_\theta)\bigg|\bigg]\\
		&\leq \mathbb{E}_{X,\tilde{X}} \bigg[\sup_{\sigma_{\kappa}\in\mathcal{N}_{\sigma},q_\theta\in\mathcal{N}_q}n^{-1}|\Omega|\bigg|
\sum_{j=1}^n\big(h(\tilde{X}_{j})-h(X_{j})\big)\bigg|\bigg].
\end{align*}
Since the random variables $X$ and $\tilde X$ are independent, 
the inequality holds for any Rademacher random variables 
$\{\omega_j\}_{j=1}^n$:
\begin{align*}
&\mathbb{E}_{X}\big[\Delta\mathcal{E}_d\big] 
		\leq \mathbb{E}_{X,\tilde{X},\omega} \bigg[\sup_{\sigma_{\kappa}\in\mathcal{N}_{\sigma},q_\theta\in\mathcal{N}_q}n^{-1}|\Omega|\bigg|
\sum_{j=1}^n\omega_j\big(h(\tilde{X}_{j})-h(X_{j})\big)\bigg|\bigg]
\end{align*}
Then applying the triangle inequality leads to 
\begin{align*}
\mathbb{E}_{X}\big[\Delta\mathcal{E}_d\big]
        \leq &\mathbb{E}_{\tilde{X},\omega} \bigg[\sup_{\sigma_{\kappa}\in\mathcal{N}_{\sigma},q_\theta\in\mathcal{N}_q}n^{-1}|\Omega|\bigg|
\sum_{j=1}^n\omega_jh(\tilde{X}_{j})\bigg|\bigg] \\
    &+ \mathbb{E}_{X,\omega} \bigg[\sup_{\sigma_{\kappa}\in\mathcal{N}_{\sigma},q_\theta\in\mathcal{N}_q}n^{-1}|\Omega|\bigg|
\sum_{j=1}^n\omega_jh(X_{j})\bigg|\bigg]
 = 2\mathcal{R}_n(\mathcal{H}_d).
\end{align*}
The proofs for the remaining cases are identical. This completes the proof of the lemma.
\end{proof}
\end{comment}

Next we appeal to reduction to 
parameterisation. This is achieved using the Lipschitz continuity
of the functions in the function classes $\mathcal{H}_i$ with respect to the DNN parameters, following from Lemma \ref{lem:NN-Lip}.
\begin{lemma}\label{lem:fcn-Lip}
Let $c_z=\|\nabla z^\delta\|_{L^\infty(\Omega)}$ and $c_z'=\|\nabla z^\delta\|_{L^\infty(\partial\Omega)}$. For the function 
classes $\mathcal{H}_i$, $i\in \{d,\sigma,b,b',q\}$, the functions are 
uniformly bounded:
\begin{align*}
  \|h\|_{L^\infty(\Omega)} &\leq \left\{\begin{aligned}
    M_d&=2(dR_\sigma^{2}(W_\sigma+1)^{2}+c_1^2c_z^2), &&\quad h\in \mathcal{H}_d,\\
    M_\sigma &= 2(d^2R_\sigma^{2L_\sigma}W_\sigma^{2L_\sigma-2}+\|f\|^2_{L^\infty(\Omega)}), &&\quad h\in \mathcal{H}_\sigma,\\
    M_q &= dR_q^{2L_q}W_q^{2L_q-2}, && \quad h\in \mathcal{H}_q,
  \end{aligned}\right. \\
  \|h\|_{L^\infty(\partial\Omega)}& \leq \left\{\begin{aligned}
    M_b &= 2(dR_\sigma^{2}(W_\sigma+1)^2+\|g\|^2_{L^\infty(\partial\Omega)}), && \quad h\in \mathcal{H}_b,\\
    M_{b'} &= 2(dR_\sigma^{2}(W_\sigma+1)^{2}+c_1^2(c_z')^2),  && \quad h\in \mathcal{H}_{b'}.
  \end{aligned}\right. 
\end{align*}

Moreover, the following Lipschitz continuity estimates in the DNN parameters hold
\begin{align*}
  \|h-\tilde h\|_{L^\infty(\Omega)} & \le  \Lambda_d (\|\theta -\tilde\theta\|_{\ell^\infty} + \|\kappa-\tilde\kappa\|_{\ell^\infty}),\quad \forall h,\tilde h\in \mathcal{H}_d,\\
  \|h-\tilde h\|_{L^\infty(\Omega)} & \leq \Lambda_\sigma \|\kappa-\tilde \kappa\|_{\ell^\infty},\quad \forall h,\tilde h\in \mathcal{H}_\sigma,\\
  \|h-\tilde h\|_{L^\infty(\partial\Omega)} & \leq \Lambda_b \|\kappa-\tilde \kappa\|_{\ell^\infty},\quad \forall h,\tilde h\in \mathcal{H}_b,\\
  \|h-\tilde h\|_{L^\infty(\partial\Omega)} & \leq \Lambda_{b'} \|\kappa-\tilde \kappa\|_{\ell^\infty},\quad \forall h,\tilde h\in \mathcal{H}_{b'},\\
  \|h_{\theta}-h_{\tilde \theta}\|_{L^\infty(\Omega)} & \leq \Lambda_q \|\theta-\tilde \theta\|_{\ell^\infty},\quad \forall h,\tilde h\in \mathcal{H}_q,
\end{align*}   
with the Lipschitz constants $\Lambda_i$, $i\in \{d,\sigma,b,b',q\}$, given by 
\begin{align*}
  \Lambda_d & =2\big(\sqrt{d}R_\sigma(W_\sigma+1)+c_1c_z\big)\max(2\sqrt{d}L_\sigma R_\sigma^{L_\sigma-1}W_\sigma^{L_\sigma},c_zL_qR_q^{L_q-1}W_q^{L_q}),\\
  \Lambda_\sigma & =  2(dR_\sigma^{L_\sigma}W^{L_\sigma-1}_{\sigma} +\|f\|_{L^\infty(\Omega)})dL_\sigma^2R_\sigma^{2L_\sigma-2}W_\sigma^{2L_\sigma-2},\\
  \Lambda_b & = 4(\sqrt{d}R_\sigma(W_{\sigma}+1)+\|g\|_{L^\infty(\partial\Omega)})\sqrt{d}L_\sigma R_\sigma^{L_\sigma-1}W_\sigma^{L_\sigma},\\
  \Lambda_{b'} & = 4\big(\sqrt{d}R_\sigma(W_\sigma+1)+c_1c_z'\big)\sqrt{d}L_\sigma R_\sigma^{L_\sigma-1}W_\sigma^{L_\sigma},\\
  \Lambda_q & = 2dL_q^2R_q^{3L_q-2}W_q^{3L_q-3}.
\end{align*}
\end{lemma}
\begin{proof}
The assertions follow directly from Lemma \ref{lem:NN-Lip}. Indeed, for $h_{\theta,\kappa}\in \mathcal{H}_d$, we have
\begin{align*}
  |h_{\theta,\kappa}(x)| &\leq 2(\|\sigma_\kappa(x)\|_{\ell^2}^2 +\|\P01(q_\theta(x))\nabla z^\delta(x)\|_{\ell^2}^2)
 \leq 2(dR_\sigma^{2}(W_\sigma+1)^{2}+c_1^2c_z^2).
\end{align*}
For any $h_{\theta,\kappa}, h_{\tilde\theta,\tilde\kappa}\in \mathcal{H}_d$, by completing the squares and Cauchy-Schwarz inequality, meanwhile noting the stability of $\P01$ in \eqref{eqn:P01-stab} we have
\begin{align*}
    & h_{\theta,\kappa}(x) - h_{\tilde\theta,\tilde\kappa}(x) = \|\sigma_\kappa(x)-\P01(q_\theta(x))\nabla z^\delta(x)\|_{\ell^2}^2 - 
  \|\sigma_{\tilde\kappa}(x)-\P01(q_{\tilde\theta}(x))\nabla z^\delta(x)\|_{\ell^2}^2\\
    =& \big(\sigma_\kappa(x)-\P01(q_\theta(x))\nabla z^\delta(x)+ \sigma_{\tilde\kappa}(x)-\P01(q_{\tilde \theta}(x))\nabla z^\delta(x),\\
    &\quad \sigma_\kappa(x)- \sigma_{\tilde\kappa}(x)+(-\P01(q_\theta(x))+\P01(q_{\tilde\theta}(x)))\nabla z^\delta(x)\big)\\
  \leq & 2\big(\sup_{\sigma_\kappa\in \mathcal{N}_\kappa}\|\sigma_\kappa(x)\|_{L^\infty(\Omega;\mathbb{R}^d)} + c_1c_z\big) \big(\|\sigma_\kappa(x)-\sigma_{\tilde\kappa}(x)\|_{L^\infty(\Omega;\mathbb{R}^d)}+c_z\|q_\theta(x)-q_{\tilde\theta}(x)\|_{L^\infty(\Omega)}\big).
\end{align*}
Then by Lemma \ref{lem:NN-Lip}, we deduce
\begin{align*}
      &|h_{\theta,\kappa}(x) - h_{\tilde\theta,\tilde\kappa}(x)| \\
      \leq &
      2\big(\sqrt{d}R_\sigma(W_\sigma+1)+c_1c_z\big)\times (2\sqrt{d}L_\sigma R_\sigma^{L_\sigma-1}W_\sigma^{L_\sigma}\|\kappa-\tilde\kappa\|_{\ell^\infty}+c_zL_qR_q^{L_q-1}W_q^{L_q}\|\theta-\tilde\theta\|_{\ell^\infty})\\
       \le& 2\big(\sqrt{d}R_\sigma(W_\sigma+1)+c_1c_z\big)\max
      (2\sqrt{d}L_\sigma R_\sigma^{L_\sigma-1}W_\sigma^{L_\sigma},c_zL_qR_q^{L_q-1}W_q^{L_q})
      (\|\kappa-\tilde\kappa\|_{\ell^\infty}+\|\theta-\tilde\theta\|_{\ell^\infty}).
\end{align*}
The remaining estimates follow similarly. This completes the proof of the lemma.
\end{proof}

Next we bound the Rademacher complexities $\mathcal{R}_n(\mathcal{H}_i)$, $i\in\{d,
\sigma,b,b',q\}$, using the concept of the covering number. Let $\mathcal{G}$ be a 
real-valued function class equipped with the metric $\rho$. An
$\epsilon$-cover of the class $\mathcal{G}$ with respect to the metric $\rho$ is a collection
of points $\{x_i\}_{i=1}^n \subset \mathcal{G}$ such that for every $x\in \mathcal{G}$, there
exists at least one $i \in \{1,\dots,n\}$ such that $\rho(x, x_i) \leq \epsilon$. The
$\epsilon$-covering number $\mathcal{C}(\mathcal{G}, \rho, \epsilon)$ is the minimum
cardinality among all $\epsilon$-cover of the class $\mathcal{G}$ with respect to the metric
$\rho$. Then we can state the well-known Dudley's theorem \cite[Theorem 9]{lu2021priori} and \cite[Theorem 1.19]{wolf2018mathematical}.
\begin{lemma}\label{lem:Dudley}
Let $M_\mathcal{F}:=\sup_{f\in\mathcal{F}} \|f\|_{L^{\infty}(\Omega)}$, and $\mathcal{C}(\mathcal{F},\|\cdot\|_{L^{\infty}(\Omega)},\epsilon)$ be the covering number of the set $\mathcal{F}$. Then the Rademacher complexity $\mathfrak{R}_n(\mathcal{F})$ is bounded by
\begin{equation*}
\mathfrak{R}_n(\mathcal{F})\leq\inf_{0<s< M_\mathcal{F}}\bigg(4s\ +\ 12n^{-\frac12}\int^{M_\mathcal{F}}_{s}\big(\log\mathcal{C}(\mathcal{F},\|\cdot\|_{L^{\infty}(\Omega)},\epsilon)\big)^{\frac12}\ {\rm d}\epsilon\bigg).
\end{equation*}
\end{lemma}

Now we can state the proof of Theorem \ref{thm:stat-err}.
\begin{proof}
By the Lipschitz continuity of DNN functions with respect to the DNN parameters, the covering number of the corresponding function class can be bounded by that of the parametrization. For any $n \in \mathbb{N }$, $R \in [1, \infty)$, $\epsilon \in  (0,1)$,
and $ B_R := \{x\in\mathbb{R}^n:\ \|x\|_{\ell^\infty}\leq R\}$, then \cite[Proposition 5]{CuckerSmale:2002}
\begin{equation*}
	\log \mathcal{C}(B_R,\|\cdot\|_{\ell^\infty},\epsilon)\leq n\log (2R\epsilon^{-1}).
\end{equation*}
It follows directly from the Lipschitz continuity in Lemma \ref{lem:NN-Lip} that
\begin{align*}
&\mathcal{C}(\mathcal{H}_d,\|\cdot\|_{L^{\infty}(\Omega)},\epsilon)\leq \mathcal{C}(\Theta_\sigma\otimes \Theta_q,\|\cdot\|_{\ell^\infty},\Lambda_d^{-1}\epsilon)\leq (N_{\kappa}+N_\theta)\log(2\max(R_q,R_\sigma)\Lambda_d\epsilon^{-1}),
\end{align*}
with $\Theta_\sigma$ and $\Theta_q$ denoting the neural network parameter sets for $\sigma$ and $q$, respectively, $\Lambda_d:=2\big(\sqrt{d}R_\sigma(W_\sigma+1)+c_1c_z\big)\max(2\sqrt{d}L_\sigma R_\sigma^{L_\sigma-1}W_\sigma^{L_\sigma},c_zL_qR_q^{L_q-1}W_q^{L_q})$, 
cf. Lemma \ref{lem:fcn-Lip}. By Lemma \ref{lem:fcn-Lip}, we also have $M_d
=2(dR_\sigma^{2}(W_\sigma+1)^{2}+c_1^2c_z^2)$. Then letting $s=n^{-\frac12}$ 
in Lemma \ref{lem:Dudley} gives
\begin{align*}
    &\quad \mathfrak{R}_n(\mathcal{H}_d)\leq4n^{-\frac12}+12n^{-\frac12}\int^{M_d}_{n^{-\frac12}}{\big((N_{\kappa}+N_\theta) \mbox{log}(2\max(R_q,R_\sigma)\Lambda_d\epsilon^{-1})\big)}^{\frac12}\ {\rm d}\epsilon\\
			&\leq4n^{-\frac12}+12n^{-\frac12}M_d{\big((N_{\kappa}+N_\theta) \mbox{log}(2\max(R_q,R_\sigma)\Lambda_dn^{\frac12})\big)}^\frac12 \\&
			\leq4n^{-\frac12}+24n^{-\frac12}(d R_\sigma^2(W_\sigma+1)^2+c_1^2c_z^2)(N_\kappa+N_\theta)^{\frac12}\big(\log \max(R_q,R_\sigma)+\log \Lambda_d+\log n+ \log 2\big)^\frac12.
\end{align*}
Since $1\leq R_q,R_\sigma$, $1\leq W_q\leq N_\theta$, $1\leq W_\sigma\leq N_\kappa$ and $2\leq L_q, L_{\sigma}\leq c\log(d+2)$ (due to Lemma \ref{lem:tanh-approx}), we can bound the term $\log\Lambda_d$ by
\begin{equation*}
	\log\Lambda_d\leq c(\log R_\sigma +\log N_{\kappa} + \log R_q+\log N_\theta+\tilde c),
\end{equation*}
with the constants $c$ and $\tilde c$ depending on $c_1$, $c_z$, $d$, $L_q$ and $L_\sigma$ at most polynomially.
Hence, we have
\begin{equation*}
   \mathfrak{R}_n(\mathcal{H}_d)\leq c_{d} n^{-\frac12}R_\sigma^{2}N_\kappa^{2}(N_\kappa+N_\theta)^{\frac12}(\log^\frac12 R_\sigma +\log^\frac12 N_{\kappa} + \log^\frac12 R_q+\log^\frac12 N_\theta+\log^\frac12 n),
\end{equation*}
where $c_{d}>0$ depends on $d$, $c_1$ and $c_z$ at most polynomially.
Similarly, repeating the preceding argument leads to
\begin{align*}
\mathfrak{R}_n(\mathcal{H}_\sigma)&\leq c_\sigma n^{-\frac12}R_\sigma^{2L_\sigma}N_{\kappa}^{2L_\sigma-\frac32}\big(\log^\frac12R_\sigma+\log^\frac12N_{\kappa}+\log^\frac12n\big),\\
\mathfrak{R}_n(\mathcal{H}_b)&\leq c_bn^{-\frac12}R_\sigma^2 N_{\kappa}^{\frac52}\big(\log^\frac12 R_\sigma+\log^\frac12 N_{\kappa}+\log^\frac12 n\big),\\
\mathfrak{R}_n(\mathcal{H}_{b'})&\leq c_{b'}n^{-\frac12}R_\sigma^2 N_{\kappa}^{\frac52}\big(\log^\frac12 R_\sigma+\log^\frac12 N_{\kappa}+\log^\frac12 n\big),\\
\mathfrak{R}_n(\mathcal{H}_q)&\leq c_qn^{-\frac12}R_q^{2L_q}N_\theta^{2L_q-\frac32}\big(\log^\frac12R_q+\log^\frac12N_{\theta}+\log^\frac12n\big),
\end{align*}
where the involved constants depend on $d$ at most polynomially. 
Last, the desired estimates follow from the PAC-type generalization  bound in Lemma \ref{lem:PAC}.
\end{proof}

\bibliographystyle{siam}
\bibliography{reference}
\end{document}