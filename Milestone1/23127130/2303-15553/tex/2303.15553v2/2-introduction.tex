\section{Introduction}

With the advent of Vision Transformer (ViT), transformers have gained increasing popularity in the field of medical image analysis~\cite{vit}, due to the capability of capturing long-range dependencies.
% 
However, ViT and its variants require considerably larger dataset sizes to achieve competitive results with convolutional neural networks (CNNs), due to larger model sizes and the absence of convolutional inductive bias~\cite{vit,liu2021efficient}.
% 
Indeed, ViT performs worse than ResNet~\cite{resnet}, a model of similar capacity, on the ImageNet benchmark\cite{imagenet}, if ViT does not enjoy pre-training on JFT-300M~\cite{jft300m}, a large-scale dataset with 303 million weakly annotated natural images.
% 
The drawback of requiring exceptionally large datasets prevents transformer-based architectures to fully evolve its potential in the medical image analysis context, where data collection and annotation continue to pose considerable challenges.
% 
To capitalize on the benefits of transformer-based architectures for medical image analysis, we seek to develop an effective ViT framework capable of performing competitively even when only comparably small data is available.


In the literature, the problematic requirement for large data is partly alleviated by extra supervisory signals. Data-efficient Image Transformer (DeiT), for example, distills hard labels from a strong teacher transformer~\cite{touvron2021training}. 
%
Unfortunately, this approach only applies to problems where data-costly, high-capacity teacher transformer can be developed. 
% 
Moreover, DeiT enables the training on student transformers exclusively for mid-size datasets, between 10k to 100k samples, and the performance dramatically declines when the data scale is small~\cite{touvron2021training}.
%
Concurrently, another line of work attempts to introduce the shift, scale, and distortion invariance properties from CNNs to transformers, resulting in a series of hybrid architecture designs~\cite{li2021localvit,wu2021cvt,xu2021co,yuan2021incorporating}.
%
To give a few examples, Van \etal fed the extracted features from CNNs into a transformer for multi-view fusion in COVID diagnosis~\cite{tulder2021multi}.
% 
Barhoumi \etal extended a single CNN to multiple CNNs for feature extraction before the fusion by a tansformer~\cite{barhoumi2021scopeformer}.
% 
Importantly, they note that pre-training on ImageNet is still required to fuse convolutional operations with self-attention mechanisms, particularly in the medical context~\cite{barhoumi2021scopeformer}.
% 
Yet, pre-training on large-scale medical dataset is practically unaffordable, due to the absence of centralized dataset as well as the privacy regularizations. 
% 



Our developments to combat the need for large data to train transformer-models is loosely inspired by the process that clinicians use when learning how to diagnose medical images from a relative very limited number of cases compared to regular data size.
% 
To mimic this human decision-making process, where new information or ``evidence'' is often conceptually correlated with previously memorized facts or ``experience'', we present the \textit{Memorizing Vision Transformer} (MoViT) for efficient medical image analysis.
% 
MoViT introduces external memory, allowing the transformer to access previously memorized experience, \ie keys and values, in the self-attention heads generated during the training.
% 
In the inference stage, the external memory then enhances the instance-level attention by looking up the correlated memorized facts.
% 
Introducing external memory enables long-range context to be captured through attention similar to language modeling, which provides supplementary attention with the current ViT and variants~\cite{mem_vit,khandelwal2019generalization,guo2022learning}.


The contributions of this paper are three-fold, summarized as follows. 
% 
(1) A novel \textit{Memorizing Vision Transformer} (MoViT), which introduces storage for past attention cues by caching them into external memory, without introducing additional trainable parameters.
% 
(2) A new approach to updating the memory using a \textit{Attention Temporal Moving Average} scheme, that accumulates attention snapshots and optimizes data in the external memory dynamically. In contrast, previous work, such as \cite{mem_vit}, is restricted to a random dropping-out scheme to keep a fixed amount of external memorized events.
% 
(3) A new \textit{post hoc} scheme, \textit{Prototypical Attention Learning}, to distill the large-scale cached data into a representative prototypical subset, which accelerates computation during inference.
% 
Experiments are carried out across different modalities, \ie Magnetic Resonance (MRI) Images and histopathological images, demonstrating superior performance to vanilla transformer models across all data regimes, especially when only small amounts of training samples are available. 



