\section{Conclusion}
In conclusion, we show that the use of memory in transformer architectures is beneficial for reducing the amount of training data needed to train generalizable transformer models. The reduction in data needs is particularly appealing in the context of medical image analysis, where large-scale data continues to pose challenges. 
% 
Our model, the Memorizing Vision Transformer (MoViT) for medical image analysis, caches and updates relevant key and value pairs during training, and then uses them to enrich the attention context for the inference stage. 
% 
MoViT's implementation is straightforward and can easily be plugged into various transformer models to achieve performance competitive to vanilla ViT with much less training data.
% 
Consequently, our method has the potential for benefiting a broad range of applications in the medical image analysis context. 
% 
Future work includes a hybrid of MoViT with convolutional neural networks for more comprehensive feature extraction. 

