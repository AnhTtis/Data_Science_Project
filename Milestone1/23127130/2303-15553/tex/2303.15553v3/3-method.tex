\section{Methods}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/framework.pdf}
    \caption{An overview of the proposed Memorizing Vision Transformer (MoViT). 
    }
    \label{fig:overview}
\end{figure}

\subsubsection{Memorizing Vision Transformer.} 
% 
Memorizing Vision Transformer (MoViT) accumulates snapshots of attention cues \ie key $k$ and value $v$ generated by the attention heads as the memorized ``experience'', and caches them to an external memory bank in the form of an indexed triplet ($ID$, $k$, $v$) during the training process.
% 
The enumerated index $ID$ of the data sample and the generated attention fact ($k,v$) are prepared for an efficient lookup and update in the subsequent Attention Temporal Moving Average (ATMA) scheme.
% 
Importantly, the gradients are not back-propagated into the memory bank, and thus, the caching operation only costs slightly extra training.
%
This approach is practical in that the proposed MoViT can be easily plugged into any Vision Transformer (ViT) or its variants, by replacing one or multiple vanilla transformer blocks with MoViT blocks.
% 
An overview of the MoViT framework is presented in \figref{fig:overview}.
% 


\subsubsection{Attention Temporal Moving Average.} 
% 
To remove stale memorized ``experience''~\cite{wang2020cross}, and also to prevent overfitting to the training set, we introduce a novel Attention Temporal Moving Average (ATMA) strategy to update external memory.
% 
Current approaches routinely employ a fixed capacity to store triplets, where outdated cached memory triplets are dropped and new facts are taken in randomly.
% 
Different from this approach, we improve the mechanism by accumulating all the past snapshots \textit{w.r.t} index $ID$, with the introduction of Exponential Moving Average update~\cite{meanteacher}.
% 
The outdated cached ``experience'' $(k_\text{old},v_\text{old})$ \textit{w.r.t} index $ID$ denote the fact generated in the previous epoch, and is updated by the subsequently generated facts $(k_\text{generated},v_\text{generated})$ in the proposed ATMA according to:
\begin{equation}
\begin{cases}
\begin{aligned}
    k_{\text{new}} & = \alpha_k \cdot k_{\text{generated}} + (1-\alpha_k) \cdot k_{\text{old}}, \\
    v_{\text{new}} & = \alpha_v \cdot v_{\text{generated}} + (1-\alpha_v) \cdot v_{\text{old}},
\end{aligned}
\end{cases}
\end{equation}
where the subscripts ``new'' denotes the updated facts to the external memory, and $\alpha_k$, $\alpha_v$ are the friction terms.
% 
In the smoothing process, both coefficients uniformly follow the ramp-down scheme~\cite{ramp} for a steady update. 
Specifically, the coefficients are subject to the current training epoch number $t$, \ie
\begin{equation}
  \alpha = 
  \begin{cases}
 1 - \alpha_0 \cdot \exp (-t_0(1 - \frac{t}{t_0})^2, &t \le t_0 \\
 1 - \alpha_0, &t > t_0 \\
  \end{cases}
\end{equation}
with $a_0 = 0.01$ and $t_0$ set to 10\% of the total training epochs as in previous work~\cite{meanteacher}.
% 
The number of stored facts $M$ is exclusively correlated with the dataset scale, and network architecture \ie $M=\#(\text{training samples})\times\#(\text{attention heads})$, where the number of attention heads is often empirically set between the range of 3-12~\cite{vit}, leading to a bounded $M$.



\subsubsection{Prototypical Attention Learning.}  
% 
We write all the cached experience from the training stage of MoViT as $\mathcal{F}=\{(k_i,v_i)\}_{i=1}^M$. 
% 
Then, prototypical attention facts refer to a small number of representative facts to describe $\mathcal{F}$ \ie $\mathcal{P}=\{(k_i^p,v_i^p)\}_{i=1}^P$, where $P$ represents the total number of prototypes. 
% 
To distill the external memorized facts into representative prototypes for efficient inference, we introduce Prototypical Attention Learning (PAL), which is applied \textit{post hoc} to the external memory after model training. 
% 
To identify the prototype keys from the cached keys $\{k_i\}_{i=1}^M$, we leverage the Maximum Mean Discrepancy (MMD) metric~\cite{mmd} to measure the discrepancy between two distributions.
% 
Subsequently, the objective in PAL is steered toward minimizing the MMD metric, \ie
\begin{equation}
% \argmin_{k_i^p} \Big(
MMD^2 = \frac{1}{P^2}\sum_{i,j=1}^P D(k_i^p,k_j^p) - 
\frac{1}{PM}\sum_{i,j=1}^{P,M}D(k_i^p,k_j) + 
\frac{1}{M^2}\sum_{i,j=1}^MD(k_i,k_j)
% \Big)
,
\label{eq:mmd}
\end{equation}
where $D(\cdot,\cdot)$ denotes the cosine similarity. 
% 
We employ a greedy search to find $\{k_i^p\}_{i=1}^P$ from Eq. \eqref{eq:mmd}. 
% 
To integrate all information after deriving prototype keys, we leverage the weighted average to derive the associated $\{v_i^p\}_{i=1}^P$ \ie
\begin{equation}
    v_i^p = \sum_{j=1}^M w_{j,i} v_j~~\text{with}~ w_{j,i}=\frac{\exp(D(v_j,v_i^p)/\tau)}{\sum \exp_{k=1}^M (D(v_k,v_i^p)/\tau)},\label{eq:avg}
\end{equation}
where the weights $w_{j,i}$ are normalized by the \texttt{softmax} operation, and temperature $\tau$ is a hyper-parameter to determine the confidence of normalization.


\subsubsection{Inference Stage.} 
To apply the attention facts $\mathcal{F}$ or prototypes $\mathcal{P}$ stored in the external memory during inference, approximate $k$-nearest-neighbor (kNN) search is employed to look up the top $k$ pairs of (key, value) \textit{w.r.t} the given the local queries. 
% 
In this way, the same batch of queries generated from the test sample is used for both the multi-head self attentions and external memory retrievals.
% 
With the retrieved keys, the attention matrix is derived by computing the \texttt{softmax} operated dot product with each query.
% 
Afterwards, we use the attention matrix to compute a weighted sum over the retrieved values.
% 
The results attended to local context and external memories are combined using a learned gate scheme~\cite{mem_vit}.





