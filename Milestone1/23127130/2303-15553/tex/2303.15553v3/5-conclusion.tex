\section{Conclusion}
In conclusion, we show that using memory in transformer architectures is beneficial for reducing the amount of training data needed to train generalizable transformer models. The reduction in data needs is particularly appealing in medical image analysis, where large-scale data continues to pose challenges. 
% 
Our model, the Memorizing Vision Transformer (MoViT) for medical image analysis, caches and updates relevant key and value pairs during training. It then uses them to enrich the attention context for the inference stage. 
% 
MoViT's implementation is straightforward and can easily be plugged into various transformer models to achieve performance competitive to vanilla ViT with much less training data.
% 
Consequently, our method has the potential to benefit a broad range of applications in the medical image analysis context. 
% 
Future work includes a hybrid of MoViT with convolutional neural networks for more comprehensive feature extraction. 

\subsubsection{Acknowledgments:}
This work was supported in part by grants from the National Institutes of Health (R37CA248077, R01CA228188). 
The MRI equipment in this study was funded by the NIH grant: 1S10ODO21648.
