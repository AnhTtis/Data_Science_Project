\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath}
\usepackage[pdftex]{graphicx}
\usepackage{algorithm} 
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{array}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{url}
\usepackage{amsthm}
\newcounter{theorem}
\usepackage[noabbrev]{cleveref}
\usepackage{amsfonts}
\newtheorem{lemma}[theorem]{Lemma}
\usepackage{subfig}
%\usepackage{subcaption}
\usepackage[autostyle]{csquotes}  
\usepackage{algpseudocode} 
\usepackage{mwe}
\usepackage{physics}
\newlength{\tempdim}
\renewcommand{\algorithmicrequire}{\textbf{Input:}} % Use Input in the format of Algorithm 
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm
\newtheorem{definition}{Definition}

\usepackage{enumitem}
\setlist[enumerate]{leftmargin=*}
\setlist[itemize]{leftmargin=*}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


\usepackage{xcolor}

%\newcommand{\shuiqiao}[1]{\textcolor{blue}{#1}}

\newcommand{\di}[1]{{\color{red}Di: #1}}



% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding}

\author{Borui~Cai,
        Yong~Xiang,~\IEEEmembership{Senior Member,~IEEE},
        Longxiang~Gao,~\IEEEmembership{Senior Member,~IEEE},
        Di~Wu,
        He~Zhang,
        Jiong~Jin,~\IEEEmembership{Member,~IEEE}
        and~Tom~Luan,~\IEEEmembership{Senior Member,~IEEE}% <-this % stops a space

\thanks{This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.} 
     
\thanks{B. Cai and Y. Xiang are with the School of Information Technology, Deakin University, VIC 3125, Australia.
E-mail: \{b.cai,yong.xiang\}@deakin.edu.au.}

\thanks{L. Gao is with Qilu University of Technology (Shandong Academy of Sciences), China. E-mail: gaolx@sdas.org.}

\thanks{D. Wu is with School of Mathematics, Physics and Computing, University of Southern Queensland, QLD 4350, Australia. E-mail: di.wu@usq.edu.au.}

\thanks{H. Zhang is with CNPIEC KEXIN LTD, China. E-mail:  zhanghe@kxsz.net.}

\thanks{J. Jin is with the School of Science, Computing and Engineering Technologies, Swinburne University of Technology, VIC 3122, Australia. E-mail: jiongjin@swin.edu.au.}

\thanks{T. Luan is with the School of Cyber Engineering,
Xidian University, Xiâ€™an, 710071, China. E-mail: tom.luan@xidian.edu.cn.}

}


% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}


\maketitle

\begin{abstract}
Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity representations, i.e., a narrow embedding layer and a multi-layer dimension lifting network (LiftNet). Experiments on three public datasets show that the proposed method (implemented based on TransE and DistMult) with 4-dimensional entity representations achieves more accurate link prediction results than counterpart parameter-efficient KGE methods and strong KGE baselines, including TransE and DistMult with 512-dimensional entity representations.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Knowledge graph embedding, deep neural network, parameter-efficiency, representation learning.
\end{IEEEkeywords}


% make the title area




\section{Introduction}
\label{sect:1}
\IEEEPARstart{K}{nowledge} graphs containing a large number of facts benefit various downstream applications, ranging from open-domain question answering \cite{answer}, content-based recommender systems \cite{recommendation}, to text-centric information retrieval \cite{ir}. A fact in a knowledge graph is represented as a triple, which includes a head/subject entity, a tail/object entity, and the relation identifying the relationship between them \cite{transe}; for example, $\{Da\ Vinci, painted, Mona\ Lisa\}$. Due to the complex multi-relational structure, directly applying knowledge graphs to downstream tasks is difficult. Knowledge Graph Embedding (KGE) methods \cite{survey} instead map entities and relations of a knowledge graph into vector representations, which preserve structural information of the knowledge graph and are convenient to use \cite{recommendation}.

%
\begin{figure}[t!]
\centering
\subfloat[]{\includegraphics[width=1.62in]{fig/a.png}} \hspace{0.0in}
\subfloat[]{\includegraphics[width=1.82in]{fig/b.png}} \\
\caption{In (a), conventional KGE models that use high-dimensional entity representations equal to enlarging the width of the embedding layer. But we tend to achieve parameter efficiency by increasing the depth of the embedding network, i.e., a narrower embedding layer (low-dimensional entity representations) plus the LiftNet as shown in (b).}
\label{fig:intro}
\end{figure}

Conventional KGE methods (e.g., TransE \cite{transe}) require relatively high-dimensional entity/relation representations to ensure that the complex structural information of knowledge graphs can be accurately preserved \cite{hyper}. However, since the scale of model parameters grows linearly with the representation dimension, these KGE models consume huge memory resources, especially for large-scale real-life knowledge graphs. For example, if the embedding dimension is 512, a KGE model for Google knowledge graph \cite{google} (covering 5 billion entities by 2020) requires more than $2,560$ billion model parameters; that is even 14 times larger than GPT-3 (one of the largest language models) \cite{gpt3}. This problem greatly limits the adaptation of KGE models on resource-constrained platforms. 

Entity representations take up most of KGE model parameters because the number of entities usually is much larger than that of relations in real-life knowledge graphs \cite{code}. Therefore, existing methods improve parameter efficiency by reducing the dimension of entity representations and developing techniques to compensate for the dimension reduction. For example, adopting knowledge distillation to train low-dimensional entity representations from pre-trained high-dimensional KGE models \cite{dist2}. Other works redesign the expression of low-dimensional entity representations, e.g., hyperbolic expressions \cite{hyper}, or compositional anchor nodes \cite{anchor}. However, a major limitation is that they sacrifice the model accuracy and fail to achieve a desirable reduction of model parameters.

To achieve parameter-efficient KGE without sacrificing accuracy, we turn to a different perspective by viewing the concatenation of all entity representations as an embedding layer. Then, conventional KGE models that use high-dimensional entity representations equal to enlarging the width of the embedding layer to gain model expressiveness (Fig. \ref{fig:intro} (a)). Unlike that, we aim to improve expressiveness by increasing the depth of the embedding network as shown in Fig. \ref{fig:intro} (b). The reason is that compared to wider networks, deeper networks reduce model parameters exponentially \cite{wide1,deep1} to exhibit similar expressiveness for compositional structures (widely exist among knowledge graph entities \cite{code,anchor}).

In this paper, we propose a multi-layer dimension lifting network (LiftNet) method for parameter-efficient KGE. Instead of enlarging the width of the embedding layer, we develop a parameter-efficient deeper embedding network for entity representations, i.e., a narrow embedding layer and the LiftNet. Specifically, LiftNet uses multiple transposed convolution (TC) layers to progressively lift the low-dimensional ($\hat{n}$) entity representations to the high-dimensional ($n$). In this way, without affecting the final dimension of entity representations, the number of model parameters for entity representations is reduced from $|\mathcal{E}|n$ to $|\mathcal{E}|\hat{n}+P_{net}$. $|\mathcal{E}|$ is the number of entities and $P_{net}$ is the number of model parameters in LiftNet, which is negligible compared to $|\mathcal{E}|\hat{n}$. For the evaluation, we integrate LiftNet with two conventional KGE models (TransE \cite{transe} and DistMult \cite{distmult}) to present LiftNet-TransE and LiftNet-DistMult. Experiments on three public knowledge graph datasets show that LiftNet-TransE and LiftNet-DistMult, with only 4-dimensional entity representations, achieve more accurate link prediction results than strong baseline KGE methods and counterpart parameter-efficient KGE methods.


\section{Related Work}

\subsection{Knowledge graph embedding}
KGE methods learn vector representations for knowledge graphs, and we roughly categorize them into three types. First, distance-based methods describe a fact with mathematical operations, e.g., TransE defines a relation as the translation. To better model N-N relations, TransH \cite{transh} and STransE \cite{stranse} project entities to relation-aware subspace with hyperplanes and matrices, respectively. Operations in the complex space \cite{rotate} or the polar coordinate system \cite{hake} are also introduced to improve flexibility. 
Second, tensor factorization methods, e.g., RESCAL \cite{rescal}, model the knowledge graph as a three-way tensor, and apply tensor decomposition to obtain entity/relation representations. Later methods further improve the effectiveness of decomposition. For example, DistMult \cite{distmult} applies an efficient diagonal relation matrix, ComplEx \cite{complex} introduces complex embedding to capture relation asymmetry, and SimplE \cite{simple} addresses the independence of entity embedding. 
Third, deep learning methods adopt deep neural networks to capture the complex relationships of entities and relations. ConvE \cite{conve}, and CapsE \cite{capsule} learn the complex interactions between entities and relations through convolutional layers and capsule layers, respectively. While CompGCN \cite{compgcn} leverages GCN layers with entity-relation composition operations to capture interactions among entities and relations.


\subsection{Parameter-efficient knowledge graph embedding}
Since the scale of model parameters is linear with the embedding dimensions, many works focus on employing different techniques to improve the effectiveness of low-dimensional entity representations. 
Knowledge distillation is adopted to train low-dimensional representations with multiple pre-trained KGE models \cite{dist2}, which shows better performance than training from scratch. The number of pre-trained models is further reduced to one \cite{dist3} by introducing the dual influence between the pre-trained and the target models. Meanwhile, to accelerate the distillation, the low-dimensional model training is replaced by more efficient feature pruning of pre-trained models \cite{green}.
However, the obtained low-dimensional entity representations still show degraded performance.
To improve the performance, ROTH \cite{hyper} adopts more flexible hyperbolic space (compared to Euclidean space) for low-dimensional entity representations, but that also brings more complex hyperbolic geometry \cite{nonhyer}. To avoid that, contrastive learning is employed to flexibly control the strength of penalties of easy and difficult instances \cite{dist1}. Other works reinvent the entity representations as more expressive forms. For example, through learning a vocabulary of abstractive codewords that capture shared features of entities, entity representations become the composition of relevant codewords \cite{code,anchor}.


\section{The Proposed Method}

\subsection{Preliminaries}
A knowledge graph is a multi-relational graph denoted as $\mathcal{G} = (\mathcal{E}, \mathcal{R}, \mathcal{D})$. $\mathcal{E}$ and $\mathcal{R}$ are the collections of entities and relations, respectively, and $\mathcal{D}$ is the collection of facts contained in the knowledge graph. A fact $d\in \mathcal{D}$ is a triple denoted as $\{h,r,t\}$. In $d$, $h\in \mathcal{E}$ is the head entity, $t\in \mathcal{E}$ is the tail entity, and $r\in \mathcal{R}$ is the relation between the head entity and tail entity.
Knowledge graph embedding aims at learning vector representations for entities and relations while preserving the structural information of the knowledge graph. We denote the vector representations corresponding to $\{h,r,t\}$ as $\{e^{h},e^{r},e^{t}\}$. Conventional KGE methods map $h,r$, and $t$ into the same latent space, i.e., $e^{h},e^{r}$,$e^{t}\in \mathbb{R}^{n}$, and learn entity/relation representations by minimizing score functions defined as vector operations; for example, $\l(h,r,t)=\|e^{h}+e^{r}-e^{t}\|$ (TransE) or $\l(h,r,t)=(e^{h})^{T}M_{r}e^{t}$ (DistMult \cite{distmult}, $M_{r}\in \mathbb{R}^{n\times n}$ is a diagonal matrix).

Most KGE methods require relatively high embedding dimensions to sufficiently express the structural information of knowledge graph \cite{transe}; however, that leads to oversized model parameters. 
Adopting low-dimensional ($\hat{e}\in \mathbb{R}^{\hat{n}}, \hat{n}\ll n$) entity representations can significantly reduce model parameters but results in low performance due to decreased expressiveness.
To achieve parameter-efficient KGE, we aim at learning low-dimensional representations ($\hat{e}\in \mathbb{R}^{\hat{n}}, \hat{n}\ll n$), without degrading the performance. Following previous methods \cite{lre,code}, we focus on reducing the dimensions of entity representations since the number of entities is much larger than that of relations in many real-life knowledge graphs.




\subsection{Dimension Lifting Network}
As discussed above, we see the concatenation of entity representations as an embedding layer. To achieve parameter efficiency, we aim at replacing the wide embedding layer of high-dimensional entity representations ($\{e_{1}||e_{2}||...||e_{|\mathcal{E}|}\}$) with a deeper embedding network; that is, a narrow embedding layer of low-dimensional entity representations ($\{\hat{e}_{1}||\hat{e}_{2}||...||\hat{e}_{|\mathcal{E}|}\}$) and a multi-layer dimension lifting network (LiftNet).
$\{\hat{e}_{1}||\hat{e}_{2}||...||\hat{e}_{|\mathcal{E}|}\}$ and LiftNet are fully-connected; that means LiftNet can be interpreted as a dimension lifting function $f(*)$ that lifts $\hat{e}_{i}\in \mathbb{R}^{\hat{n}}$ to $e=f(\hat{e}_{i})\in \mathbb{R}^{n}$. $f(\hat{e}_{i})$ is used together with the original relation representations for score measurements and inference.

%
\begin{figure}[htbp]
\centering
\includegraphics[width=3.3in]{fig/frame.png} \\
\caption{The structure of LiftNet. $\hat{e}$ is the low-dimensional input entity representation, and LiftNet uses two TC layers to progressively lift it to high-dimensional output $e$.}
\label{fig:frame}
\end{figure}

The main task is to design an effective $f(*)$ for KGE. 
An intuitive choice of $f(*)$ is multiple fully-connected (FC) layers; however, FC layers require large numbers of parameters and are prone to overfitting for KGE \cite{conve}. Inspired by image processing \cite{gan}, we refer to feature upsampling techniques for dimension lifting. 
Specifically, we adopt transposed convolution (TC) layer in LiftNet. Different from traditional upsampling methods (e.g., nearest-neighbor, bilinear, and bicubic interpolation \cite{upsampling}), TC is a parameter-efficient convolution layer and can capture the interactions among the parameters of entity representations. The structure of LiftNet is shown in Fig. \ref{fig:frame}. Note that previous work (LRE) \cite{lre} equals to defining $f(*)$ as a transfer matrix $M\in \mathbb{R}^{\hat{n}\times n}$ (i.e., $f(\hat{e})=\hat{e}M$). Unfortunately, the linear transformation is difficult to capture the complex structural information of entities, and thus causes a limited rate of model parameter reduction (50\%) with slightly degraded link prediction accuracy. 

LiftNet adopts multiple TC layers to progressively lift the low-dimensional $\hat{e}$ to the high-dimensional $e$ (we find in experiments that LiftNet with only two FC layers already shows promising performance on three public knowledge graph datasets). 
In the forward pass, LiftNet reshapes $\hat{e}$ as square feature maps and then feeds them into the TC layers. After that, the representation is lifted into $\mathbb{R}^{c\times m_{1}\times m_{2}}$, where $c$ is the output channel number and $m_{1}\times m_{2}$ is the size of output feature map. The output of the last TC layer is then reshaped back to the expected high-dimensional $e$.
We adopt ReLU \cite{relu} as the non-linear activation function for fast training, and Batch Normalization \cite{batch} to regularize the intermediate layer outputs. We use TanH on the output of the last FC layer to provide non-linearity while constraining the output values. We choose Adam as the optimizer \cite{adam} for the model training. 

LiftNet is model-agnostic and can be conveniently integrated into most conventional KGE methods for parameter-efficient representation learning, e.g., TransE, DistMult, or RotatE, by simply replacing the entity representation $e$ with $f(\hat{e})$. Here we implement LiftNet with two representative KGE models for the demonstration, the translational TransE and the bi-linear DistMult. TransE and DistMult are shallow KGE models and the representation of entities and relations take all the model parameters. The score functions of LiftNet-TransE and LiftNet-DistMult are $\l(h,r,t)=\|f(\hat{e}^{h})+e^{r}-f(\hat{e}^{t})\|$ and $\l(h,r,t)=f(\hat{e}^{h})^{T}M_{r}f(\hat{e}^{t})$, respectively.

The parameter size of LiftNet-TransE/LiftNet-DistMult is $|\mathcal{E}|\hat{n}+|\mathcal{R}|n+P_{net}$, where $P_{net}$ is the size of LiftNet parameters (mainly the convolution filters in FC layers and the Batch Normalization layer) and $P_{net}\ll |\mathcal{E}|\hat{n}$. Therefore, for many real-world KG datasets that have much more entities than relations ($|\mathcal{E}|\gg |\mathcal{R}|$), LiftNet-TransE/LiftNet-DistMult only require approximately $\frac{\hat{n}}{n}$ parameters of TransE/DistMult that need high-dimensional entity representations.

%
\begin{table}[h]
\centering
\caption{Statistics of the datasets.}
\begin{tabular}{lccccc}
\toprule 
Dataset & \#E & \#R &Train/Valid/Test\\
\midrule
WN18RR    &40,943	&11  &86,835/3,034/3,134  \\
FB15K237  &14,541	&237  &272,115/17,535/20,466   \\ 
YAGO3-10  &123,182	&37  &1,079,040/5,000/5,000  \\ 
\bottomrule
\end{tabular}
\label{tab:dt}
\end{table}

%
\begin{table*}[h]
\centering
\caption{Link prediction accuracy the datasets (with the best in bold and the second-best underlined). Other than TransE, DistMult, LiftNet-TransE and LiftNet-DistMult, we present the published results of the rest methods. }
\begin{tabular}{lccccccccc}
\toprule
\multirow{2}{*}{\textbf{Methods} (dim)} & \multicolumn{3}{c}{\textbf{WN18RR}}& \multicolumn{3}{c}{\textbf{FB15K237}} & \multicolumn{3}{c}{\textbf{YAGO3-10}} \\
\cmidrule(lr){2-4}  \cmidrule(lr){5-7} \cmidrule(lr){8-10}
&MRR &H@1 &H@10 &MRR &H@1 &H@10 &MRR &H@1 &H@10 \\
\midrule
TransE (512)           &.134	&.014	&.390   &.194	&.092	&.364   &.139	&.050	&.302 \\
DistMult  (512)        &.387	&.322	&.471   &.200	&.125	&.351   &.195	&.115	&.359 \\
RotatE (500)           &.476 &.428   &.571   &.338   &.241   &.533   &.495   &.402   &.670 \\
HAKE (1000)             &.497 &.452   &.582   &.346   &.250   &.542   &.545   &.462   &.694 \\
ComplEx-ER (200)       &.494 &.453   &.575   &.374   &.282   &.563   &.588   &.515   &\underline{.718} \\
CompGCN (200)           &.479 &.443   &.546   &.355   &.264   &.535   &-      &-      &-    \\
\midrule
LightKG (10)           &.427 &.406   &.463   &.288   &.196   &.475   &-      &-      &-    \\
ROTH (32)             &.496 &.449   &.586   &.344   &.246   &.535   &.570   &.495   &.706 \\
GreenKGC (32)         &.326 &.248   &.479   &.411   &.367   &.491   &.453   &.361   &.629 \\
MulDE (32)            &.481 &.433   &.574   &.328   &.237   &.515   &-      &-      &-    \\
LiftNet-TransE (4)   &\underline{.689}	&\underline{.650}	&\underline{.733}   &\underline{.831}	&\underline{.790}	&\textbf{.908}   &\underline{.628}	&\underline{.598}	&.651 \\
LiftNet-DistMult (4)  &\textbf{.816}	&\textbf{.815}	&\textbf{.816}   &\textbf{.864}	&\textbf{.863}	&\underline{.864}   &\textbf{.890}	&\textbf{.889}	&\textbf{.891} \\
\bottomrule
\end{tabular}
\label{tab:acc}
\end{table*}


\section{Experiments}



\subsection{Experiment Setup}
\paragraph{Datasets}
We adopt three public knowledge graph datasets widely used for KGE and link prediction, and they are WN18RR \cite{transe}, FB15K237 \cite{fb15k} and YAGO3-10 \cite{conve}. Detailed statistics of these datasets are summarized in Table \ref{tab:dt}. WN18RR is a subset of WN18 that removes inverse relations to avoid test leakage. FB15K237 is a subset of FB15K \cite{transe}, and it adopts a similar process to remove nearly identical and inverse relations. YAGO3-10 is sampled from the YAGO3 knowledge graph. 






\paragraph{Evaluation metrics}
We evaluate the performance of KGE with link prediction \cite{transe}, which predicts missing entity given a query ($\{h,r,?\}$ or $\{?,r,t\}$). 
Following \cite{transe,distmult}, we use mean reciprocal ranking (MRR) and H@k to measure the prediction accuracy. MRR is the average inverse rank for all test triples, and H@k is the percentage of ranks lower than or equal to k. The maximum values of MRR and H@k are both 1, and the higher MRR or H@k, the better the performance. We adopt the filtered setting \cite{hyper} to exclude candidate triples that have been seen in training, validation, and testing sets.


\paragraph{Baseline models}
We choose a set of strong baseline KGE and parameter-efficient KGE models to compare with the proposed LiftNet-based methods. For conventional KGE models, we report the performance of TransE, DistMult, RotatE \cite{rotate}, HAKE \cite{hake}, ComplEx-ER \cite{er} and CompGCN \cite{compgcn} on the three datasets. 
For parameter-efficient KGE models, we report LightKG \cite{lightkg}, ROTH \cite{hyper}, GreenKGC \cite{green}, and MulDE \cite{dist3}. We implement LRE \cite{lre} (adopts transfer matrix to lift entity dimensions) as LRE-TransE and LRE-DistMult to compare with the LiftNet-based methods.


\paragraph{Implementation details}
We implement TransE, DistMult, LiftNet-TransE, and LiftNet-DistMult \footnote{https://github.com/brcai/LiftNet} using OpenKE \cite{openke}. Non-sampling loss \cite{nonsampling}, which removes the time-consuming negative sampling, is adopted for efficient model training. We set the embedding dimensions of TransE and DistMult as 512 considering both the link prediction accuracy and the parameter size.
We implement a two-layer LiftNet for LiftNet-TransE and LiftNet-DistMult, which lifts the representation dimensions from 4 to 512. In LiftNet, the parameters of the two TC layers as set as \{InChan:1, OutChan:4, Filer:4\} and \{InChan:4, OutChan:8, Filer:4\}, respectively, with stride=1 and padding=0. We fix the random seed for all experiments and use the MRR of the validation set to find the optimal learning rate from \{0.0001, 0.001, 0.01, 0.05\}. The maximum epoch number is 500.



\subsection{Main results}
\paragraph{Link prediction accuracy}
The accuracy of link prediction of the proposed LiftNet-TransE/LiftNet-DistMult and compared baseline methods are shown in Table \ref{tab:acc}. 
We observe that LiftNet-TransE and LiftNet-DistMult, with only 4-dimensional entity representations, achieve more accurate results than strong KGE models. The accuracy achieved by LiftNet-TransE and LiftNet-DistMult is higher than 0.6 and 0.8 on all datasets, respectively.
Especially, LiftNet-TransE and LiftNet-DistMult significantly outperform the original TransE and DistMult that require 512 dimensions. The end-to-end LiftNet-TransE and LiftNet-DistMult also beat HAKE, ComplEx-ER, and CompGCN, which adopt specific structures to capture entity and relation hierarchies/semantics. 

In addition, LiftNet-TransE and LiftNet-DistMult also outperform counterpart parameter-efficient KGE methods to a large extent, with smaller representation dimensions. The average improvement (H@1) of LiftNet-DistMult over counterpart methods on all datasets is larger than 0.4. For LiftNet-TransE, although its H@10 is slightly lower than that of the hyperbolic ROTH (0.651 to 0.706) on YAGO3-10, the rest results are all better than compared parameter-efficient KGE methods (especially on FB15K237). We also observe that counterpart parameter-efficient KGE methods slightly sacrifice prediction accuracy for the reduction of model parameters (they all achieve lower accuracy than the best-performed KGE methods); while LiftNet-TransE and LiftNet-DistMult achieve more accurate link prediction results with fewer model parameters.

%
\begin{table}[h]
\centering
\caption{Parameter efficiency (M is short for million).}
\begin{tabular}{lccc}
\toprule 
& WN18RR & FB15K237 & YAGO3-10\\
\midrule
TransE             &20.97M	&7.57M  &63.08M  \\
LiftNet-TransE	   &0.17M	&0.18M  &0.51M   \\ 
\midrule
\textit{Percentage}         &0.8\%	&2.4\%  &0.8\%  \\ 
\bottomrule
\end{tabular}
\label{tab:param}
\end{table}

\paragraph{Parameter efficiency}
In Table \ref{tab:param}, we show the effectiveness of LiftNet-TransE on reducing model parameters (LiftNet-DistMult has a similar number of model parameters as LiftNet-TransE). On WN18RR and YAGO3-10, LiftNet-TransE only needs 0.8\% parameters of the original TransE, i.e., the number of model parameters is reduced from 20.97M to 0.17M and from 63.08M to 0.51M, respectively. This is because in WN18RR and YAGO3-10 $|\mathcal{E}|$ is three magnitudes larger than $|\mathcal{R}|$ and the number of parameters in LiftNet is small; thus, the percentage of model parameters approximate to $\frac{\hat{n}}{n}=\frac{4}{512}\approx 0.8\%$. In FB15K237, $|\mathcal{E}|$ is two magnitudes larger than $|\mathcal{R}|$, and thus the number of model parameters in LiftNet-TransE is 2.4\% of TransE.









\subsection{Sensitivity Analysis}
\paragraph{Sensitivity to input dimensions}
The input dimensions highly affect the parameter efficiency of LiftNet-based methods; thus we analyze its influence on the model performance. We evaluate LiftNet-TransE and LiftNet-DistMult with four input dimensions $\{4,8,16,32\}$. To do that, we adjust the set-up of TC layers (channel number, filter size, stride, and padding) accordingly to lift the dimensions of entity representations to 512. We further implement the LRE-based methods \cite{lre} (LRE-TransE and LRE-DistMult), which adopts a linear transfer matrix to lift the entity dimension, to compare with the proposed LiftNet-based methods.

The results of LiftNet-based methods and LRE-based methods for knowledge graph link prediction (accuracy measured by H@10) are shown in Fig. \ref{fig:in}. Generally, on all three datasets, LiftNet-TransE and LiftNet-DistMult significantly outperform LRE-TransE and LRE-DistMult with varying input dimensions; that again shows the effectiveness of the non-linear and deeper LiftNet to improve entity representation expressiveness. Meanwhile, different from LRE-based methods that have higher accuracy with larger input dimensions, the performance of LiftNet-TransE and LiftNet-DistMult slightly decreases with larger input dimensions partly due to over-parameterization. That shows the effectiveness of LiftNet with low-dimensional entity representations.


%
\begin{figure}[t!]
\centering
\subfloat[]{\includegraphics[width=1.7in]{fig/indim_transe.png}}
\subfloat[]{\includegraphics[width=1.7in]{fig/indim_distmult.png}} \\
\caption{Link prediction accuracy w.r.t. input dimensions.}
\label{fig:in}
\end{figure}

\paragraph{Sensitivity to output dimension}
We show the sensitivity of the performance of LiftNet-TransE and LiftNet-DistMult regarding the output dimensions. To do that, we vary the output dimensions from 128 to 1024 and adjust the set-ups of the two TC layers in LiftNet accordingly. The results in Fig. \ref{fig:out} show that the performance of LiftNet-TransE and LiftNet-DistMult roughly increases with the higher output dimensions because of the increasing model capacity. Note that compared with input dimensions, the increase of output dimensions (e.g., by using larger TC filters) results in much fewer new parameters and thus is less sensitive to over-parameterization.
These results also explain why LiftNet needs to "lift" the output dimensions, rather than maintain or even reduce them.

%
\begin{figure}[t!]
\centering
\subfloat[]{\includegraphics[width=1.7in]{fig/outdim_transe.png}}
\subfloat[]{\includegraphics[width=1.7in]{fig/outdim_distmult.png}} \\
\caption{Link prediction accuracy w.r.t. output dimensions.}
\label{fig:out}
\end{figure}


\paragraph{Number of layers}
We further analyze the sensitivity of LiftNet regarding the number of TC layers. For demonstration, we implement LiftNet-TransE with one TC layer, two TC layers, and three TC layers, respectively. We adjust the set-ups of TC layers by increasing/decreasing the filter size accordingly to ensure the output dimensions is 512. The results with respect to link prediction accuracy (MRR) are shown in Table \ref{tab:layer}. LiftNet-TransE with two TC layers performs the best among the three methods. LiftNet-TransE with one TC layer produces the lowest accuracy on all three datasets, which is even lower than the original high-dimensional TransE, and that indicates limited improvement on low-dimensional input embedding. LiftNet-TransE of three TC layers also obtains slightly lower performance than two TC layers on all three datasets, especially on FB15K237 (MRR reduces from 0.831 to 0.712), partly due to training difficulties and overfitting.
%
\begin{table}[h]
\centering
\caption{The link prediction accuracy (measured by MRR) of LiftNet-TransE w.r.t. the number of TC layers.}
\begin{tabular}{cccc}
\toprule 
Number Layers& WN18RR & FB15K237 & YAGO3-10\\
\midrule
1      &.021	&.044  &.004  \\
2	   &\textbf{.689}	&\textbf{.831}  &\textbf{.628}   \\ 
3      &.663	&.712  &.565  \\ 
\bottomrule
\end{tabular}
\label{tab:layer}
\end{table}


%
\begin{figure}[t!]
\centering
\includegraphics[width=3.0in]{fig/layer.png} \\
\caption{Link prediction accuracy (MRR) of LiftNet and its two variants.}
\label{fig:variant}
\end{figure}



\subsection{LiftNet variants}
In LiftNet, we adopt TC layers to progressively lift the dimensions. To demonstrate the effectiveness of such a design, we implement two variants of LiftNet for comparison regarding link prediction accuracy (MRR). FC LiftNet replaces the two TC layers with fully-connect (FC) layers, and Hybrid LiftNet only replaces the second TC layer with the FC layer. All three types of LiftNet are run with LiftNet-DistMult, and the results are shown in Fig \ref{fig:variant}. Generally, trends are similar across all three datasets, TC LiftNet achieves the best results, Hybrid LiftNet is the second best, and FC LiftNet performs the worst. We do not claim that TC layers are better than FC layers, but in KGE that the model is sensitive to insufficient expressiveness or over-parameterization, LiftNet with TC layers is relatively easier to achieve more accurate link prediction results.






\section{Conclusion}
In this paper, we propose a parameter-efficient LiftNet method for KGE. LiftNet increases the depth of the embedding layer of entities and lifts low-dimensional input entity representations to high-dimensional outputs. Experiments conducted on three public knowledge graph datasets show that the proposed LiftNet-TransE and LiftNet-DistMult, with the smallest dimension (4) of entity representations, achieve more accurate link prediction results than strong KGE methods and counterpart parameter-efficient KGE methods. Moreover, LiftNet-TransE and LiftNet-DistMult only require 1.33\% parameters of the original 512-dimensional TransE and DistMult. The sensitivity analysis and the comparison with two variants of LiftNet further verify the effectiveness of the proposed LiftNet method.


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{ref.bib}


% that's all folks
\end{document}
