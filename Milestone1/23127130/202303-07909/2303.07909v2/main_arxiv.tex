


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% % correct bad hyphenation here
% \hyphenation{op-tical net-works semi-conduc-tor}


\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
% \usepackage{xcolor}
\usepackage{bbm}
\usepackage{adjustbox}
\input{math_commands.tex}
\usepackage[usenames,dvipsnames]{xcolor}
% \usepackage[numbers]{natbib}
\usepackage{xcolor}
%\usepackage{\makecell}
\usepackage{pifont}
\usepackage{algorithm}
\usepackage{listings}
\usepackage{comment}
\usepackage{makecell}
\usepackage{lipsum}  


\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\renewcommand{\paragraph}[1]{\vspace{1.25mm}\noindent\textbf{#1}}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}




\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.


\title{Text-to-image Diffusion Models in  Generative AI: A Survey}

\author{Chenshuang~Zhang, Chaoning~Zhang, Mengchun~Zhang, In So~Kweon%
\IEEEcompsocitemizethanks{

\IEEEcompsocthanksitem Chenshuang Zhang, Mengchun Zhang, In So Kweon 
are with KAIST, South Korea (Email:zcs15@kaist.ac.kr, zhangmengchun527@gmail.com, iskweon77@kaist.ac.kr). \protect\\
\IEEEcompsocthanksitem Chaoning Zhang (correspondence author) is with Kyung Hee University, South Korea (Email: chaoningzhang1990@gmail.com) 
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
% E-mail: zcs15@kaist.ac.kr, 
%\IEEEcompsocthanksitem Chaoning Zhang is with KAIST.
}% <-this % stops an unwanted space
% \thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Zhang \MakeLowercase{\textit{et al.}}: Text-to-image Diffusion Models in  Generative AI: A Survey}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}

This survey  reviews text-to-image diffusion models in the context that diffusion models have emerged to be popular for a wide range of generative tasks. As a self-contained work, this survey starts with a brief introduction of how a basic diffusion model works for image synthesis, followed by how condition or guidance improves learning. Based on that, we present a review of state-of-the-art methods on text-conditioned image synthesis,~\textit{i.e.} text-to-image. We further summarize applications beyond text-to-image generation: text-guided creative generation and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.

\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Survey, Generative AI, AIGC, Diffusion model, Text-to-image,  Image generation, Image editing
\end{IEEEkeywords}}



% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.




% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.

\IEEEPARstart{A}{picture} is worth a thousand words. As this old saying goes, images tell a better story than pure text.  When humans read a story in text, they can  draw relevant images in their heads by imagination, which helps them understand and enjoy more. Therefore, designing an automatic system that generates visually realistic images from textural descriptions, i.e., the text-to-image task, is a non-trivial task and therefore can be seen as a major milestone toward human-like or general artificial intelligence~\cite{goertzel2007artificial,muller2016future,clune2019ai,fjelland2020general}. With the development of deep learning~\cite{lecun2015deep}, text-to-image task has become one of the most impressive applications in computer vision
~\cite{mansimov2015generating,reed2016generative,zhang2017stackgan,xu2018attngan,li2019controllable,ramesh2021zero,ding2021cogview,wu2022nuwa,yu2022scaling,nichol2021glide,saharia2022photorealistic,rombach2022high,ramesh2022hierarchical}.

\begin{figure*}[!htbp]\centering
\includegraphics[width=0.90\linewidth]{figs/timeline.png}\\
\caption{Representive works on text-to-image task over time. The GAN-based methods, autoregressive methods, and diffusion-based methods are masked in yellow, blue and red, respectively.
}
\label{fig:timeline}
\end{figure*}


We summarize the timeline of representative works for text-to-image generation in Figure~\ref{fig:timeline}. As summarized in Figure~\ref{fig:timeline}, AlignDRAW~\cite{mansimov2015generating} is a pioneering work that generates images from natural languages, but suffers from unrealistic results. After that, Text-conditional GAN~\cite{reed2016generative} is the first end-to-end differential architecture from the character level to pixel level. Different from GAN-based methods~\cite{reed2016generative,zhang2017stackgan,xu2018attngan,li2019controllable} that are mainly in the small-scale data regime, autoregressive methods~\cite{ramesh2021zero,ding2021cogview,wu2022nuwa, yu2022scaling} exploit large-scale data for text-to-image generation,  with representative methods including DALL-E~\cite{ramesh2021zero} from OpenAI and Parti~\cite{yu2022scaling} from Google.  However, autoregressive nature makes these methods~\cite{ramesh2021zero,ding2021cogview,wu2022nuwa, yu2022scaling} suffer from high computation costs and sequential error accumulation.


More recently, there has been an emerging trend of diffusion model (DM) to become the new state-of-the-art model in text-to-image generation~\cite{nichol2021glide,saharia2022photorealistic,rombach2022high,ramesh2022hierarchical}. Diffusion-based text-to-image synthesis has also attracted massive attention in social media. Numerous works on the text-to-image diffusion model have already come out in the past year, and yet more works are expected to appear in the near future. The volume of the relevant works makes it increasingly challenging for readers to keep abreast of the recent development of text-to-image diffusion model without a comprehensive survey.  However, as far as we know, there is no survey work focusing on recent progress of diffusion-based text-to-image generation yet. A branch of related surveys ~\cite{yang2022diffusion,croitoru2022diffusion,ulhaq2022efficient,cao2022survey} reviews the progress of diffusion model in all fields, making them limited to providing limited coverage on the task of test-to-image synthesis. The other  stream of surveys~\cite{frolov2021adversarial,ulhaq2022efficient,zhou2021survey} focuses on text-to-image task but is limited to the GAN-based approaches, making them somewhat outdated considering the recent trend of the diffusion model replacing GAN. This work fills the gap between the above two streams by providing a comprehensive introduction on the recent progress 
of text-to-image task based on diffusion model, as well as providing an outlook on its future directions. 


\textbf{Related survey works and paper structure.} With multiple works~\cite{yang2022diffusion,cao2022survey} reviewing the progress of diffusion models in all fields but lacking a detailed introduction to a certain specific field, some works dive deep into specific fields, including audio diffusion models~\cite{zhang2023audio}, graph diffusion models~\cite{zhang2023graph_survey}. Complementary to~\cite{zhang2023audio,zhang2023graph_survey}, this work conducts a survey on audio diffusion models. Through the lens of AI-generated content (AIGC), this survey is also related to works that survey generative AI (see~\cite{zhang2023complete}) and ChatGPT (see~\cite{zhang2023ChatGPT} for a survey). Overall, this survey is the first to review the recent progress of text-to-image task based on diffusion models. We organize the rest of this paper as follows. Section~\ref{sec:background} introduces the background of diffusion model, including guidance methods that are important for text-to-image synthesis. Section~\ref{sec:pioneering_work} discusses the pioneering works on text-to-image task based on diffusion model, including GLIDE~\cite{nichol2021glide}, Imagen~\cite{saharia2022photorealistic}, Stable diffusion~\cite{rombach2022high} and DALL-E2 ~\cite{ramesh2022hierarchical}. Section~\ref{sec:improve_works} further discusses the follow-up  studies that improve the pioneering works in Section~\ref{sec:pioneering_work} from various aspects. By summarizing recent benchmarks and analysis, we further evaluate these text-to-image methods from  technical and ethical perspectives in Section~\ref{sec:eval_ethical}. Apart from text-to-image generation, we also introduce related tasks in Section~\ref{sec:applications}, including text-guided creative generation (e.g., text-to-video) and text-guided image editing. 
Finally, we revisit  various applications beyond text-to-image generation, and discuss the challenges as well as  future opportunities. 







\section{Background on diffusion model}
\label{sec:background}

Diffusion models (DMs), also widely known as diffusion probabilistic models~\cite{sohl2015deep}, are a family of generated models that are Markov chains trained with variational inference~\cite{ho2020denoising}. The learning goal of DM is to reserve a  process of perturbing the data with noise, \textit{i.e.} diffusion, for sample generation~\cite{sohl2015deep,ho2020denoising}. As a milestone work, denoising diffusion probabilistic model (DDPM)~\cite{ho2020denoising} was published in 2020 and sparked an exponentially increasing interest in the community of generative models afterwards.  Here, we provide a self-contained introduction to DDPM by covering the most related progress before DDPM and how unconditional DDPM works with image synthesis as a concrete example. Moreover, we summarize how guidance helps in conditional DM, which is an important foundation for understanding text-conditional DM for text-to-image. 

\subsection{Development before DDPM}
The advent of DDPM~\cite{ho2020denoising} can be mainly attributed to two early attempts: score-based generative models (SGM)~\cite{song2019generative} being investigated in 2019 and diffusion probabilistic models (DPM)~\cite{sohl2015deep} emerging as early as in 2015. Therefore, it is important to revisit the working mechanism of DPM~\cite{sohl2015deep} and SDE~\cite{song2019generative} before we introduce DDPM. 

\textbf{Diffusion Probabilistic Models (DPM).} DPM ~\cite{sohl2015deep} is the first work to model probability distribution by estimating the reversal of Markov diffusion chain which maps data to a simple distribution. Specifically, DPM ~\cite{sohl2015deep} defines a forward (inference) process  which converts a complex data distribution to a much simpler one, and then learns the mapping by reversing  this diffusion process. Experimental results on multiple datasets show the effectiveness of DPM when estimating complex data distribution. 
DPM ~\cite{sohl2015deep} can be viewed as the foundation of DDPM ~\cite{ho2020denoising}, while DDPM ~\cite{ho2020denoising} optimizes DPM ~\cite{sohl2015deep} with improved implementations.

\textbf{Score-based Generative model(SGM).} Techniques for improving score-based generative models have also been investigated in~\cite{song2019generative,song2020improved}. SGM~\cite{song2019generative} proposes to perturb the data with random Gaussian noise of various magnitudes. With the gradient of log probability density as  score function\cite{song2019generative}, SGM generates the samples towards decreasing  noise levels and trains the model by estimating the score functions for noisy data distribution. Despite different motivations, SGM shares a similar optimization objective with DDPM during training, which is also discussed in~\cite{ho2020denoising} that the DDPM under a certain parameterization is equivalent to SGM during training. A improved variant of SGM is investigated in~\cite{song2020improved} for generalizing to high-resolution images.



\subsection{How does DDPM work for image synthesis?}

Denoising diffusion probabilistic models (DDPMs) are defined as a parameterized Markov chain, which generates images from noise within finite transitions during inference. During training, the transition kernels are learned in a reversed direction of perturbing natural images with noise, where the noise is added to the data in each step and estimated as the optimization target.

\textbf{Forward pass.} In the forward pass, DDPM is a Markov chain where Gaussian noise is added to data in each step until the images are destroyed. Given a data distribution $\mathbf{x}_0 \sim q(\mathbf{x}_0)$, DDPM generates $\mathbf{x}_T$ successively with  $q(\mathbf{x}_t\mid\mathbf{x}_{t-1})$~\cite{ho2020denoising,yang2022diffusionsurvey}:
\begin{align}
q(x_{1:T} | x_0) := \prod_{t=1}^T q( x_t | x_{t-1} ), \label{eq:forwardprocess_1}
\end{align}

\begin{align}
q(x_t|x_{t-1}) := \mathcal{N}(x_t;\sqrt{1-\beta_t} x_{t-1},\beta_t I) \label{eq:forwardprocess_2}
\end{align}

where $T$ and  $\beta_t$ are the diffusion steps and hyper-parameters, respectively. We only discuss the case of Gaussian noise as transition kernels for simplicity, indicated as $\mathcal{N}$ in Eq.~\ref{eq:forwardprocess_2}. With  $\alpha_t := 1 - \beta_t$ and $\bar{\alpha}_t := \prod_{s=0}^{t} \alpha_s$, we can obtain noised image at arbitrary step $t$ as follows~\cite{yang2022diffusionsurvey}:


\begin{align}
q(x_t|x_{0}) := \mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_{0},(1 - \bar{\alpha}_t) I) \label{eq:forwardprocess_3}
\end{align}


\begin{figure*}[!htbp]
    \centering
    \setlength{\tabcolsep}{2.0pt}
    \begin{tabular}{ccccc}
        \includegraphics[width=0.22\textwidth]{figs/hedgehog.png} &
        \includegraphics[width=0.22\textwidth]{figs/corgi.png} &
        \includegraphics[width=0.22\textwidth]{figs/robots.jpg} &
        \includegraphics[width=0.22\textwidth]{figs/landscape.jpg} \\

        \scriptsize \makecell{``a hedgehog using a \\ calculator''} &
        \scriptsize \makecell{``a corgi wearing a red bowtie \\ and a purple party hat''} &
        \scriptsize \makecell{``robots meditating in a \\ vipassana retreat''} &
        \scriptsize \makecell{``a fall landscape with a small \\ cottage next to a lake''} \\
        \rule{0pt}{0.2pt} \\

    \end{tabular}
    \caption{Images generated by GLIDE~\cite{nichol2021glide}.}
    \label{fig:glide}
    \vskip -0.1in
\end{figure*}

\textbf{Reverse pass.} With the forward pass defined above, we can train the transition kernels with a reverse process. Starting from $p_\theta(T)$, we hope the generated  $p_\theta(x_0)$ can follow the true data distribution $q(x_0)$. Therefore, the optimization objective of model is as follows(quoted from ~\cite{yang2022diffusionsurvey}):


\begin{align}
     E_{t \sim \mathcal{U}( 1,T ), \mathbf x_0 \sim q(\mathbf x_0), \epsilon \sim \mathcal{N}(\mathbf{0},\mathbf{I})}{ \lambda(t)  \left\| \epsilon - \epsilon_\theta(\mathbf{x}_t, t) \right\|^2} \label{eq:loss}
\end{align}


Considering the optimization objective similarities between DDPM and SGM, thy are unified in~\cite{song2020score} from the perspective of stochastic differential equations perspective, allowing more flexible sampling methods.

\subsection{Guidance in diffusion-based image synthesis}
\label{sec:guidance}



\textbf{Labels improve image synthesis.} Early works on generative adversarial models (GAN) have shown that class labels improve the image synthesis quality \cite{mirza2014conditional,odena2017conditional,nguyen2017plug,miyato2018cgans,brock2018large}. As a pioneering work, Conditional GAN~\cite{mirza2014conditional} feeds the class label as an additional input layer to the model. Moreover, \cite{dumoulin2016learned} applies class-conditional normalization statistics in image generation. In addition, AC-GAN~\cite{miyato2018cgans} explicitly adds an auxiliary classifier loss. In other words, labels can help improve the GAN image synthesis quality by providing a conditional input or guiding the image synthesis via an auxiliary classifier. Following these success practices,~\cite{dhariwal2021diffusion} introduces class-conditional normalization and an auxiliary classifier into diffusion models. In order to distinguish whether the label information is added as a conditional input or an auxiliary loss with gradients, we follow~\cite{dhariwal2021diffusion} to define the \textit{conditional diffusion model} and \textit{guided diffusion model} as follows. 

\textit{Conditional diffusion model:} A conditional  diffusion model learns  from  additional information  (e.g., class and text) by taking them as model input.


\textit{Guided diffusion model:} During the training of a guided diffusion model, the class-induced gradients (e.g. through an auxiliary classfier) are involved in the sampling process. 


 
\textbf{Classifier-free guidance.} Different from classifier-guided diffusion model \cite{dhariwal2021diffusion} that exploits an additional classfier, it is found  in~\cite{ho2022classifier} that the guidance can be obtained by the generative model itself without a classifier, termed as \textit{classifier-free guidance}.
Specifically, classifier-free guidance jointly trains a single model with  the unconditional  score estimator $\epsilon_\theta(x)$ and  the conditional  $\epsilon_\theta(x, c)$, where $c$ denotes the class label. A null token $\varnothing$  is placed as the class label in the unconditional part, i.e., $\epsilon_\theta(x)$ = $\epsilon_\theta(x, \varnothing)$. Experimental results in~\cite{ho2022classifier} show that classifier-free guidance achieves a trade-off between quality and diversity similar to that achieved by classifier guidance. Without resorting to a classifier, classifier-free diffusion facilitates more modalities, e.g., text in text-to-image, as guidance.

\section{Pioneering Text-to-image diffusion models}
\label{sec:pioneering_work}





In this section, we  introduce  the pioneering text-to-image frameworks based on diffusion model, which can be roughly categorized  considering where the diffusion prior is conducted, i.e., the pixel space or latent space. The first class of methods generate images directly from the high-dimensional pixel level, including  GLIDE~\cite{nichol2021glide} and Imagen~\cite{saharia2022photorealistic}. Another stream of works propose to first compress the image to a low-dimensional space, and then train the diffusion model on this latent space. Representative methods falling into the class of latent space include Stable Diffusion~\cite{rombach2022high}, VQ-diffusion~\cite{gu2022vector} and DALL-E 2~\cite{ramesh2022hierarchical}. 

\subsection{Frameworks in pixel space}

\textbf{GLIDE: the first T2I work on DM.}
In essence, text-to-image is text-conditioned image synthesis. Therefore, it is intuitive to replace the label in class-conditioned DM with \textit{text} for making the sampling generation conditioned on text. As discussed in Sec.~\ref{sec:guidance}, guided diffusion improves the photorealism of samples~\cite{dhariwal2021diffusion} in conditional DM and its classifier-free variant~\cite{ho2022classifier} facilitates handling free-form prompts. Motivated by this, GLIDE~\cite{nichol2021glide} adopts classifier-free guidance in T2I by replacing original class label with text.  GLIDE~\cite{nichol2021glide} also investigated CLIP guidance but is less preferred by human evaluators than classifier-free guidance for the sample photorealism and caption similarity. As an important component in their framework, the text encoder is set to a transformer~\cite{vaswani2017attention} with 24 residual blocks with the width of 2048 (roughly 1.2B parameters). Experimental results show that GLIDE~\cite{nichol2021glide} outperforms DALL-E~\cite{ramesh2021zero}  in both FID  and human evaluation. Example images generated by GLIDE are shown in Figure~\ref{fig:glide}.


\begin{figure}[!htbp]\centering
\includegraphics[width=0.90\linewidth]{figs/imagen.PNG}\\
\caption{Model diagram from Imagen~\cite{saharia2022photorealistic}.
}
\label{fig:imagen}
\end{figure}


\textbf{Imagen: encoding text with pretrained  language model.} Following GLIDE~\cite{nichol2021glide}, Imagen~\cite{saharia2022photorealistic} adopts classifier-free guidance for image generation.  A core difference between GLIDE and Imagen lies in their choice of text encoder, as shown in Figure~\ref{fig:imagen}. Specifically, GLIDE trains the text encoder together with the diffusion prior with paired image-text data, while Imagen~\cite{saharia2022photorealistic} adopts a pretrained and frozen large language model as the  text encoder. Freezing the weights of the pretrained encoder facilitates offline text embedding, which reduces negligible computation burden to the online training of the text-to-image diffusion prior. Moreover, the text encoder can be pretrained on either image-text data (e.g., CLIP~\cite{radford2021learning}) or text-only corpus (e.g., BERT~\cite{devlin2019bert}, GPT~\cite{radford2018improving,radford2019language,brown2020language} and T5~\cite{raffel2020exploring}). The text-only corpus is significantly larger than paired image-text data, making those large language models exposed to text with a rich and wide distribution. For example, the text-only corpus used in BERT~\cite{devlin2019bert} is approximately 20GB and that used in T5~\cite{raffel2020exploring} is approximately 800GB. With different T5~\cite{raffel2020exploring} variants  as the text encoder, ~\cite{saharia2022photorealistic} reveals that increasing the size of language model improves   the image fidelity and image-text alignment more than enlarging the diffusion model size in Imagen.





\subsection{Frameworks in latent space}

\textbf{Stable diffusion: a milestone work on latent space.} A representative framework that  trains the diffusion models on latent space is Stable Diffusion, which is a scaled-up version of Latent Diffusion Model (LDM)~\cite{rombach2022high}. 
 Following Dall-E~\cite{ramesh2021zero} that adopts a  VQ-VAE to learn a visual codebook, Stable diffusion applies VQ-GAN~\cite{esser2021taming} for the latent representation in the first stage. Notebly, VQ-GAN improves VQ-VAE by adding an adversarial objective to increase the naturalness of synthesized images. With the pretrained VAE, stable diffusion reverses a forward diffusion process that perturbs latent space with noise. Stable diffusion also introduces cross-attention as general-purpose conditioning for various condition signals like text. Experimental results in ~\cite{rombach2022high} highlight that performing diffusion modeling on the latent space significantly outperforms that on the pixel space in terms of complexity reduction and detail preservation. A similar approach has also been investigated in VQ-diffusion~\cite{gu2022vector} with a mask-then-replace diffusion strategy. Resembling the finding in pixel-space method, classifier-free guidance also significantly improve the text-to-image diffusion models in latent space~\cite{rombach2022high,tang2022improved}.



\begin{figure}[!htbp]\centering
\includegraphics[width=0.90\linewidth]{figs/stable_diffusion.PNG}\\
\caption{Overview of Stable Diffusion~\cite{rombach2022high}.}
\label{fig:stable_diffusion}
\end{figure}

\textbf{DALL-E2: with multimodal latent space.} Another stream of text-to-image diffusion models in latent space relies on multimodal contrasitve models~\cite{radford2021learning,jia2021scaling,yuan2021florence}, where image embedding and text encoding are matched in the same representation space. For example, CLIP~\cite{radford2021learning} is a pioneering work learning the multimodal representations and has been widely used in numerous text-to-image models~\cite{ramesh2022hierarchical,pinkney2022clip2latent}. A representative work applying CLIP is DALL-E 2,
also known as unCLIP~\cite{ramesh2022hierarchical}, which adopts the CLIP text encoder but inverts the CLIP image encoder with a diffusion model that generates images from CLIP latent space. 
 Such a combination of  encoder and decoder resembles the structure of VAE~\cite{esser2021taming} adopted in LDM, even though the inverting decoder is non-deterministic~\cite{ramesh2022hierarchical}. 
Therefore, the remaining task is to train a prior to bridge the gap between CLIP text and image latent space, and we term it as \textit{text-image latent prior} for brevity. As shown in Figure~\ref{fig:dalle2},DALL-E2 ~\cite{ramesh2022hierarchical} finds that this prior can be learned by either autoregressive method or diffusion model, but diffusion prior achieves superior performance. Moreover, experimental results show that removing this \textit{text-image latent prior} leads to a performance drop by a large margin~\cite{ramesh2022hierarchical}, which highlights the importance of learning the \textit{text-image latent prior}. Inspired by the \textit{text-image latent prior} in DALL-E 2, clip2latent~\cite{pinkney2022clip2latent} proposes to train a diffusion model that bridges the gap between CLIP embedding and a pretrained generative model (e.g.,StyleGAN~\cite{karras2019style,karras2020analyzing,karras2021alias}). Specifically, the diffusion model is 
trained to generate the latent space of StyleGAN from the CLIP image embeddings. During inference, the latent of StyleGAN is generated from text embeddings directly as if they were image embeddings, which enables language-free training as in ~\cite{wang2022clip,zhou2022towards} for text-to-image diffusion.


 \begin{figure*}[!htbp]\centering
\includegraphics[width=0.80\linewidth]{figs/dalle2.png}\\
\caption{Overview of DALL-E2 ~\cite{ramesh2022hierarchical}.
}
\label{fig:dalle2}
\end{figure*}




\section{Improving text-to-image diffusion models}
\label{sec:improve_works}

\subsection{Improving model architectures}

\textbf{On the choice of guidance.} Beyond the classifier-free guidance, some works~\cite{nichol2021glide,liu2021more,crowson2021clip2} have also explored cross-modal guidance with CLIP~\cite{radford2021learning}. Specifically, GLIDE~\cite{nichol2021glide} finds that CLIP-guidance underperforms the classifier-free variant of guidance. By  contrast, another work UPainting~\cite{li2022upainting}
points out that lacking of a large-scale transformer language model makes these models with CLIP guidance difficult to encode text prompts and generate complex scenes with details. By combing large language model  and cross-modal matching models, UPainting~\cite{li2022upainting} significantly  improves the sample fidelity and image-text alignment of generated images. The general image synthesis  capability enables  UPainting~\cite{li2022upainting} to generate images in both simple and complex scenes. 


\textbf{On the choice of denoiser.} By default, DM during inference repeats the denoising process on the same denoiser model, which makes sense for an unconditional image synthesis since the goal is only to get a high-fidelity image. In the task of text-to-image synthesis, the generated image is also required to align with the text, which implies that the denoiser model has to make a trade-off between these two goals. Specifically, two recent works~\cite{feng2022ernie,balaji2022ediffi} point out a phenomenon: the early sampling stage strongly relies on the text prompt for the goal of aligning with the caption, but the later stage focuses on improving image quality while almost ignoring the text guidance. Therefore, they abort the practice of sharing model parameters during the denoising process and propose to adopt multiple denoiser models which are specialized for different generation stages. Specifically, ERNIE-ViLG 2.0~\cite{feng2022ernie} also mitigates the problem of object-attribute by the guidance of a text parser and object detector, improving the fine-grained semantic control.



\subsection{Sketch for spatial control}
Despite their unprecedented high image fidelity and caption similarity, most text-to-image DMs like Imagen~\cite{saharia2022photorealistic} and DALL-E2~\cite{ramesh2022hierarchical} do not provide fine-grained control of spatial layout. To this end, SpaText~\cite{avrahami2022spatext} introduces spatio-textual (ST) representation which can be included to finetune a SOTA DM by adapting its decoder. Specifically, the new encoder conditions both local ST and existing global text. Therefore, the core of SpaText~\cite{avrahami2022spatext} lies in ST where the diffusion prior in trained separately to convert the image embeddings in CLIP to its text embeddings. During training, the ST is generated directly by using the CLIP image encoder taking the segmented image object as input. A concurrent work~\cite{voynov2022sketch} proposes to realize fine-grained local control through a simple sketch image. Core to their approach is a Latent Guidance Predictor (LGP)that is a pixel-wise MLP mapping the latent feature of a noisy image to that of its corresponding sketch input. After being trained (see~\cite{voynov2022sketch} for more training details), the LGP can be deployed to the pretrained text-to-image DM without the need for fine-tuning


\subsection{Textual inversion for concept control}

Pioneering works on text-to-image generation ~\cite{nichol2021glide,saharia2022photorealistic,rombach2022high,ramesh2022hierarchical} rely on natural language  to describe the content and styles of generated images. However, there are cases when the text cannot exactly describe the desired semantics by users, e.g., generating a new subject.  In order to synthesize novel scenes with certain concepts or subjects, ~\cite{gal2022image,ruiz2022dreambooth}  introduces several reference images with the desired concepts, then inverts the reference images to the textual descriptions. Specifically, ~\cite{gal2022image} inverts the shared concept in a couple of reference images into the text (embedding) space, i.e. "pseudo-words". The generated "pseudo-words" can be used for personalized generation. DreamBooth~\cite{ruiz2022dreambooth} adopts a similar technique and mainly differs by fine-tuning (instead of freezing) the pretrained DM model for preserving key visual features from the subject identity. 



\subsection{Retrieval for out-of-distribution}

The impressive performance of SOTA text-to-image models is based on the assumption that the model is well exposed to the text that describes the common entities with the training style. However, this assumption does not hold when the entity is rare, or when the desired style is greatly different from the training style. To mitigate the significant out-of-distribution performance drop, multiple works~\cite{blattmann2022retrieval,sheynin2022knn,rombach2022text,chen2022re} have utilized the technique of retrieval with external database as a memory. Such a technique first gained attention in NLP~\cite{khandelwal2019generalization,khandelwal2020nearest,guu2020retrieval,meng2021gnn,borgeaud2021improving} and, more recently, in GAN-based image synthesis~\cite{li2022memory}, by turning fully parametric models into semi-parametric ones. Motivated by this, ~\cite{blattmann2022retrieval} has augmented diffusion models with retrieval. A retrieval-augmented diffusion model (RDM)~\cite{blattmann2022retrieval} consists of a conditional DM and an image database which is interpreted as an explicit part of the model. With the distance measured in CLIP, $k$-nearest neighbors are queried for each query, i.e. training sample, in the external database The diffusion prior is guided by the more informative embeddings of KNN neighbors with fixed CLIP image encoder instead of the text embeddings.
KNN-diffusion~\cite{sheynin2022knn} adopts a fundamentally similar approach and mainly differs by making the diffusion prior addtionally conditioned on the text embeddings to improve the generated sample quality. This practice has also been adopted in follow-up Re-Imagen~\cite{chen2022re}. In contrast to RDM~\cite{blattmann2022retrieval} and KNN-diffusion~\cite{sheynin2022knn} with a two-stage framework, Re-Imagen~\cite{chen2022re} adopts a single-stage framework and selects the K-NN neighbors not with the distance in latent space. Moreover, Re-Imagen also allows the retrieved neighbors to be both images and text. As claimed in~\cite{chen2022re}, Re-Imagen outperforms KNN-diffusion by a large margin on the benchmark COCO dataset. 


\section{Evaluation from technical and ethical perspectives}
\label{sec:eval_ethical}

\subsection{Technical evaluation of text-to-image methods}
\label{sec:tech_eval}

 Image quality and text-image alignment are two main  criteria to evaluate the text-to-image models,  which indicates  the photorealism or fidelity of generated images and whether the generated images align well with the text semantics, respectively~\cite{cho2022dall}. A common metric to evaluate image  quality quantitatively is  Frchet Inception Distance (FID)~\cite{heusel2017gans}, which measures the Frchet distance~\cite{frechet1957distance}  (also known as Wasserstein-2 distance~\cite{vaserstein1969markov}) between synthetic and real-world images.  We summarize  the evaluation results of representative methods  on  MS-COCO dataset in Table ~\ref{tab:results_fid} for reference. The smaller the FID is, the higher image fidelity. 
 To measure the text-image alignment, CLIP score~\cite{hessel2021clipscore} are widely applied, which trades off against FID. There are also other metrics for text-to-image evaluation, including Inception score (IS)~\cite{salimans2016improved} for image quality and R-precision for text-to-image generation~\cite{xu2018attngan}.
 


\begin{table}[!htbp] \centering 
\caption{FID of representative methods on MS-COCO dataset}
\label{tab:results_fid}
\resizebox{0.5\linewidth}{!}{
\begin{tabular}{lllllll}
\hline
model     &FID  \\
\hline 

CogView~\cite{ding2021cogview}& 27.10  \\
LAFITE ~\cite{zhou2022towards}  & 26.94 \\
DALLE~\cite{ramesh2021zero} & 17.89 \\
\hline 
 GLIDE~\cite{nichol2021glide}  & 12.24   \\
Imagen~\cite{saharia2022photorealistic}   & 7.27  \\
 Stable Diffusion~\cite{rombach2022high}  & 12.63  \\
 VQ-Diffussion~\cite{gu2022vector}   & 13.86 \\
 DALL-E 2~\cite{ramesh2022hierarchical}  &  10.39  \\
Upainting~\cite{li2022upainting} & 8.34  \\
ERNIE-ViLG 2.0~\cite{feng2022ernie} & 6.75 \\
eDiff-I ~\cite{balaji2022ediffi} & 6.95 \\
\hline 
\end{tabular}}
\end{table}


\begin{table*}[!htbp] \centering 
\caption{Benchmarks for text-to-image generation}
\label{tab:benchmark}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lllllllllllllll}
\hline
Benchmark   & Measurement & Metric  & Auto-eval &  Human-eval     &  Language\\
\hline 
DrawBench\cite{saharia2022photorealistic}  &  Fidelity, alignment& User preference rates &  N  &  Y   & English \\
UniBench~\cite{li2022upainting}  & Fidelity, alignment  & User preference  rates & N &  Y    &  English, Chinese\\
PartiPrompts~\cite{yu2022scaling} & Fidelity, alignment & Qualitative  & N &  Y   & English\\
PaintSKills~\cite{cho2022dall} & Visual reasoning skills, social biases & Statistics  & Y  &  Y   &  English\\
EntityDrawBench~\cite{chen2022re} & Entity-centric  faithfulness & Human rating  & N &  Y   &  English\\
Multi-Task Benchmark~\cite{petsiuk2022human} & Various capabilities & Human rating  & N &  Y   &    English\\
\hline 
\end{tabular}}
\end{table*}


\textbf{Recent evaluation benchmarks.} Apart from the automatic metrics discussed above, multiple works involve human evaluation and propose their new evaluation benchmarks ~\cite{cho2022dall,saharia2022photorealistic,yu2022scaling,li2022upainting,petsiuk2022human,chen2022re,liao2022artbench}. We summarize representative benchmarks in Table~\ref{tab:benchmark}. For a better evaluation of fidelity and text-image alignment, DrawBench\cite{saharia2022photorealistic}, PartiPropts~\cite{yu2022scaling}  and UniBench~\cite{li2022upainting} ask the human raters to compare generated images from different models. Specifically,  UniBench~\cite{li2022upainting} proposes to evaluate the model on both simple and complex scenes, and includes both Chinese and English prompts.  PartiPropts~\cite{yu2022scaling} introduces a diverse set of over 1600 (English) prompts, and also proposes a challenge dimension that highlighting why this prompt is difficult. To evaluate the model from more various aspects,   PaintSKills~\cite{cho2022dall}  evaluates the \textit{visual reasoning skills} and \textit{social biases} apart from image quality and text-image alignment. However, PaintSKills~\cite{cho2022dall} only focuses on unseen object-color and object-shape scenario~\cite{li2022upainting}.  EntityDrawBench~\cite{chen2022re} further evaluates the model with various infrequent entities in different scenes. Compared to PartiPropts~\cite{yu2022scaling} with  prompts at different difficulty levels, Multi-Task Benchmark~\cite{petsiuk2022human} proposes thirty-two tasks that evaluates different capabilities, and divides each task into three difficulty levels.





\subsection{Ethical issues and risks}

\textbf{Ethical risks from the datasets.} Text-to-image generation is a highly data-driven task, and thus models trained on large-scale unfiltered data may suffer from even reinforce the biases from the dataset, leading to ethical risks.
~\cite{schramowski2022safe} finds  a large amount of inappropriate content in the generated images by Stable diffusion~\cite{rombach2022high} (e.g., offensive, insulting, or threatening information), and  first establishes a new test bed to evaluate them. Moreover, it proposes Safe Latent Diffusion, which successfully removes and suppresses the inappropriate content with additional guidance.
Another ethical issue,  the  fairness of social group, is studied in ~\cite{struppek2022biased, bansal2022well}. Specifically, ~\cite{struppek2022biased} finds that simple homoglyph replacements in the text descriptions can induce culture bias in  models, i.e., generating images from different culture. ~\cite{bansal2022well} introduce an Ethical NaTural Language Interventions
in Text-to-Image GENeration (ENTIGEN)
benchmark dataset, which can evaluate the change of generated images with ethical interventions by three axes: gender, skin color, and culture. With intervented  text prompts, ~\cite{bansal2022well} improves diffusion models (e.g., Stable diffusion~\cite{rombach2022high}) from the social diversity perspective.



\textbf{Misuse for malicious purposes.} Text-to-image diffusion models have shown their power in generating high-quality images. However, this also raises great concern that the generated images may be used for malicious purposes, e.g., falsifying electronic evidence~\cite{sha2022fake}. DE-FAKE~\cite{sha2022fake} is the first to conduct a systematical study on visual forgeries of the text-to-image diffusion models, which aims to distinguish generated images from the real ones, and also further track the source model of each fake image.
% To achieve these two goals, DE-FAKE~\cite{sha2022fake} assesses the authenticity of fake image generated by text-to-image diffusion models from the visual modality (universal detection and source attribution) and linguistic modality (prompt analysis). 
To achieve these two goals, DE-FAKE~\cite{sha2022fake} analyzes from  visual modality perspective,  and finds  that images generated by  different diffusion models share common features and also present unique model-wise fingerprints. 
Two concurrent works \cite{ricker2022towards,corvi2022detection}  approaches the detection of faked images both by evaluating the existing detection methods on images generated by diffusion model, and also analyze the frequency discrepancy of images by  GAN and diffusion models.
\cite{ricker2022towards,corvi2022detection} find that the  performance of existing detection methods drops significantly on generated images by diffusion models compared to GAN. Moreover, \cite{ricker2022towards} attributes the failure of existing methods to the mismatch of high-frequencies between images generated by diffusion models and GAN.  Another work ~\cite{ghosh2022can} discusses the concern of the artistic image generation from the perspective of artists. Although agreeing that the artistic image generation may be a promising modality  for the development of art, ~\cite{ghosh2022can} points out that the artistic image generation may cause plagiarism and profit shifting (profits in the art market shift from artists to model owners) problems if  not properly used.
 


\textbf{Security and privacy risks.} While text-to-image diffusion models have attracted great attention, the security and privacy risks have  been neglected so far. Two pioneering works~\cite{struppek2022rickrolling,wu2022membership} discuss the backdoor attack and privacy issues, respectively. Inspired by the finding in 
~\cite{struppek2022biased} that a simple word replacement can invert culture bias to models, Rickrolling the Artist~\cite{struppek2022rickrolling} proposes to inject the backdoors into the pre-trained text encoders, which will force the generated image to follow a specific  description or include certain attributes if the trigger exists in the text prompt. ~\cite{wu2022membership} is the first to analyze the membership leakage problem in text-to-image generation models, where whether a certain image is used to train the target text-to-image model is inferred. Specifically, ~\cite{wu2022membership} proposes three intuitions on the membership information and four attack methods accordingly. Experiments show that all the proposed attack methods achieve impressive results, highlighting the threat of membership leakage.





\section{Applications beyond text-to-image generation}
\label{sec:applications}


The recent advancement of diffusion models has inspired multiple interesting applications beyond text-to-image generation, including artistic painting\cite{huang2022draw,huang2022diffstyler, wu2022creative,gallego2022personalizing,rombach2022text,jain2022vectorfusion,rombach2022text,pan2022arbitrary} and text-guided image editing~\cite{kim2021diffusionclip,chandramouli2022ldedit,kwon2022diffusion}


\subsection{Text-guided creative generation}



\subsubsection{Visual art generation}



Artistic painting is an interesting and imaginative area that benefits from the success of generative models. Despite the progress of GAN-based painting\cite{jabbar2021survey}, they suffer from the unstable training and model collapse problem brought by GAN. Recently, multiple works present impressive painting images based on diffusion models, investigating improved prompts and different scenes. 
Multimodal guided artwork diffusion (MGAD) ~\cite{huang2022draw} refines the generative process of diffusion model with multimodal guidance (text and image) and achieves excellent results regarding both the diversity and quality of generated digital artworks.  In order to maintain the global content of the input image, DiffStyler~\cite{huang2022diffstyler} propose a controllable dual diffusion model with learnable noise in the diffusion process of content image. During inference, explicit content and abstract aesthetics can both be learned with two diffusion models. Experimental results show that DiffStyler~\cite{huang2022diffstyler} achieve excellent results on both quantitative metrics and manual evaluation. To improve the creativity of Stable Diffusion model,  ~\cite{wu2022creative} proposes two directions of textual condition extension and model retraining with the Wikiart dataset, enables the users to ask the famous artists to draw novel images. ~\cite{gallego2022personalizing} personalizes text-to-image generation by customizing the aesthetic styles with a set of images, while ~\cite{jain2022vectorfusion} extends generated images to Scalable Vector Graphics (SVGs) for digital icons or arts.  In order to improve computation efficiency, ~\cite{rombach2022text} propose to generate artictis images based on retrieval-augmented diffusion models. By retrieving neighbors from specialized datasets (e.g., Wikiart), ~\cite{rombach2022text} obtains fine-grained control on the image style. In order to specify more fine-grained style features (e.g., color distribution and brush strokes),  ~\cite{pan2022arbitrary} proposes  supervised style guidance and self style guidance method, which can generate images of more diverse styles. 




\subsubsection{Video generation and story visualization}
\textbf{Text-to-video.} Since video is just a sequence of images, a natural application of text-to-image is to make a video conditioned on the text input. Conceptually, text-to-video DM lies in the intersection between text-to-image DM and video DM. Regarding text-to-video DM, there are two pioneering works: Make-A-Video~\cite{singer2022make} adapting a pretrained text-to-image DM to text-to-video and Video Imagen~\cite{ho2022imagen} extending an existing video DM method to text-to-video. Make-A-Video~\cite{singer2022make} generates high-quality videos by including temporal information in  the pretrained text-to-image models, and  trains spatial super-resolution models as well as frame interpolation models to enhance the visual quality. With pretrained text-to-image models and unsupervised learning on video data, Make-A-Video~\cite{singer2022make} successfully accelerates the training of a text-to-video model without the need of paired text-video data. By contrast, Imagen Video~\cite{ho2022imagen} is a text-to-video system composed of cascaded video diffusion models~\cite{ho2022video}. As for the model design,  Imagen Video~\cite{ho2022imagen}  points out that some recent findings (e.g., frozen encoder text conditioning) in text-to-image can transfer to video generation, and findings for video diffusion models(e.g., v-prediction parameterization) also provides insights for general diffusion models. 


\textbf{Text-to-story generation (story synthesis).} The success of text-to-video naturally inspires a future direction of novel-to-movie.  Make-A-Story~\cite{rahman2022make} and AR-LDM~\cite{pan2022synthesizing}.  shows the potential of DM for story visualization, \textit{i.e.} generating a video that matches the text-based story. Different from general text-to-video tasks, story visualization requires the model to \textit{reason} at each frame about whether to maintain the consistency of actors and backgrounds between frames or scenes, based on the story progress ~\cite{rahman2022make}. To solve this problem, Make-A-Story~\cite{rahman2022make} proposes an autoregressive diffusion-based framework, with a visual memory module implicitly capturing the actor and background context across the frames. For consistency across scenes, Make-A-Story~\cite{rahman2022make} proposes a sentence-conditioned soft attention over the memories for   visio-lingual co-reference resolution. Another concurrent work AR-LDM~\cite{pan2022synthesizing}  also focuses on the text-to-story generation task based on stable diffusion ~\cite{rombach2022high}. AR-LDM~\cite{pan2022synthesizing} is guided not only by the present caption, but also by previously generated images image-caption history for each frame. This allows AR-LDM to generate relevant and coherent images across frames. Moreover, AR-LDM~\cite{pan2022synthesizing} shows the consistency for unseen characters, and also the ability for real-world story synthesis on a new introduced dataset VIST~\cite{huang2016visual}. 



\subsubsection{3D generation}


 
\textbf{3D object generation.} The generation of 3D objects is evidently much more sophisticated than their 2D counterpart, i.e., 2D image synthesis task. DeepFusion~\cite{poole2022dreamfusion} is the first work that successfully applies  diffusion models to 3D object synthesis. Inspired by Dream Fields~\cite{jain2022zero} which applies 2D image-text models (i.e., CLIP) for 3D synthesis, DeepFusion~\cite{poole2022dreamfusion} trains a randomly initialized NeRF~\cite{mildenhall2021nerf} with the distillation  of  a pretrained 2D diffusion model (i.e., Imagen). However, according to  Magic3D ~\cite{lin2022magic3d},  the low-resolution image supervision and extremely slow optimization of NeRF  result in low-quality generation and  long processing time of DeepFusion~\cite{poole2022dreamfusion}.  For higher-resolution results, Magic3D ~\cite{lin2022magic3d} proposes a coarse-to-fine optimization approach with coarse representation as initialization as the first step, and optimizing  mesh representations with high-resolution diffusion priors.  Magic3D ~\cite{lin2022magic3d} also accelerates the generation process with a sparse 3D hash grid structure. 3DDesigner~\cite{li20223ddesigner}  focuses on another topic of 3D object generation, \textit{consistency}, which indicates the cross-view correspondence. With low-resolution results from NeRF-based condition module as the prior, a two-stream asynchronous diffusion module further enhances the consistency, and achieves 360-degree consistent results.






\subsection{Text-guided image editing}


Prior to DM gaining popularity, zero-shot image editing had been dominated by GAN inversion methods~\cite{radford2021learning,abdal2020image2stylegan++,alaluf2021restyle,bau2020semantic,brock2016neural,richardson2021encoding,tov2021designing} combined with CLIP. However, GAN is often constrained to have limited inversion capability, causing unintended changes to the image content.





\subsubsection{General image editing}
DiffusionCLIP~\cite{kim2021diffusionclip} is a pioneering work to introduce DM to alleviate this problem. Specifically, it first adopts a pretrained DM to convert the input image to the latent space and then finetunes the DM at the reverse path with a loss consisting of two terms: local directional CLIP loss~\cite{gal2022stylegan} and an identity loss. The former is employed to guide the target image to align with the text and the latter mitigates unwanted changes. To enable full inversion, it adopts a deterministic DDIM~\cite{song2020denoising} instead of DDPM reverse process~\cite{ho2020denoising}. Benefiting from DM's excellent inversion property, DiffusionCLIP~\cite{kim2021diffusionclip} has shown superior performance for both in-domain and out-of-domain manipulation. A drawback of DiffusionClip is that it requires the mode to be finetuned to transfer to a new domain. To avoid fine-tuning, LDEdit~\cite{chandramouli2022ldedit} proposes a combine of DDIM and LDM. Specifically, LDEdit~\cite{chandramouli2022ldedit} employs a deterministic forward diffusion in the latent space, and then makes the reverse process conditioned on the target text. Despite its simplicity, it  performs well in a wide range of image editing tasks, constituting a generalized framework.

To solve the problem that a simple modification of text prompt may leads to a different output,
Prompt-to-Prompt~\cite{hertz2022prompt}  proposes to use a cross-attention map during the diffusion progress, which  represents the relation between each image pixel and word in the text prompt. In image-to-image translation task, ~\cite{tumanyan2022plug} also works on the semantic features in the diffusion latent space, and finds that manipulating spatial features and their self-attention inside the model can control the image translation process. 
An unsupervised image translation
method is also proposed in DiffusionIT~\cite{kwon2022diffusion}, which disentangles style and content representation. As a definitional reformulation,
CycleDiffusion ~\cite{wu2022unifying} unifies generative models by  reformulating the latent space of diffusion models, and shows that diffusion models can be guided similarly with GANs.


Direct Inversion ~\cite{elarabawy2022direct} applies a similar two-step procedure, i.e., encoding the image to its corresponding noise and then generating the edited image with inverted noises. However, it requires no optimization or model finetuning in Direct Inversion ~\cite{elarabawy2022direct}.
In the generative process, a diffusion model starts from a noise vector, and can generate  images by iteratively denoising.  For the image editing task, an exact  mapping  process from   image to noise and back are necessary. Instead of DDPM~\cite{ho2020denoising}, DDIM~\cite{song2020denoising} has been widely applied for its nearly perfect inversion~\cite{kim2021diffusionclip}. However, due to the local linearization assumptions, DDIM~\cite{song2020denoising} may lead to incorrect image reconstruction with the error propagation ~\cite{wallace2022edict}. To mitigate this problem, Exact Diffusion Inversion via Coupled Transformations (EDICT)~\cite{wallace2022edict} proposes to maintain two coupled noise vectors in the diffusion process and achieves higher reconstruction quality than DDIM~\cite{song2020denoising}. However, the computation time of EDICT~\cite{wallace2022edict} is almost twice of  DDIM~\cite{song2020denoising}. Another work Null-text Inversion~\cite{mokady2022null} improves image editing with Diffusion Pivotal Inversion and null-text  optimization, 
Inspired by the finding that the accumulated error in DDIM ~\cite{song2020denoising} can be neglected in the unconditional diffusion models, but is amplified when applying classifier-guidance with a large guidance scale $w$ in image editing,  ~\cite{mokady2022null} proposes to take the initial DDIM inversion with guidance scale $w=1$ as the pivotal trajectory, and then optimize with standard guidance $w > 1$. ~\cite{mokady2022null} also proposes  to  replace the embedding of null-text with the optimized embedding (Null-text optimization), achieving high-fidelity   editing results of real images. 










\subsubsection{Image editing with masks}
Manipulating the image mainly on a local (masked) region~\cite{avrahami2022blended1} constitutes a major challenge to the task of image editing. The difficulty lies in guaranteeing a seamless coherence between the masked region and the background. Similar to~\cite{kim2021diffusionclip}, Blended diffusion~\cite{avrahami2022blended1} is based on pretrained CLIP and adopts two loss terms: one for encouraging the alignment between the masked image and text caption and the other for keeping the unmasked region not deviate from its original content. Notably, to guarantee the seamless coherence between the edited region and the remaining part, it spatially blends noisy image with the local text-guided diffusion latent in a progressive manner. This approached is further combined with LDM~\cite{rombach2022high} to yield a blended latent diffusion to accelerate the local text-driven image editing~\cite{avrahami2022blended2}. A multi-stage variant of blended diffusion is also investigated for a super high-resolution setting~\cite{ackermann2022high}. The above works ~\cite{avrahami2022blended1,avrahami2022blended2,ackermann2022high} require a manually designed mask so that the model can tell which part to edit. By contrast, DiffEdit~\cite{couairon2022diffedit} proposes to automatically generate the mask to indicate which part to be edited. Specifically, the mask is inferred by the difference in noise estimates between query text and reference text conditions. With inferred masks, DiffEdit~\cite{couairon2022diffedit} replaces the region of interest with pixels corresponding to the query text.



\subsubsection{Model training with a single image}
The success of diffusion models in the text-to-image synthesis area relies on a large amount of training samples. An interesting topic is  how to train a generative model with  a single image, e.g., SinGAN~\cite{shaham2019singan}. SinGAN\cite{shaham2019singan} can generate similar images and perform well on multiple tasks (e.g., image editing) after training on a single image. There are also research on diffusion models with a single image~\cite{kulikov2022sinddm,wang2022sindiffusion,valevski2022unitune}. Single Denoising Diffusion Models (SinDDM)~\cite{kulikov2022sinddm} proposes a hierarchical diffusion model inspired by the multi-scale SinGAN~\cite{shaham2019singan}. The convolutional denoiser is trained on various scales of images which are corrupted by multiple levels of noise. Compared to SDEdit\cite{meng2021sdedit}, SinDDM~\cite{kulikov2022sinddm} can generating images with different dimensions. By contrast, Single-image Diffusion Model (SinDiffusion)~\cite{wang2022sindiffusion} is trained on a single image at a single scale, avoiding the accumulation of errors. Moreover, SinDiffusion~\cite{wang2022sindiffusion} proposes patch-level receptive field, which encourages the model  to learn patch statistics instead of memorizing the whole image in prior diffusion models~\cite{dhariwal2021diffusion}. Different from ~\cite{kulikov2022sinddm,wang2022sindiffusion} training the model from scratch, UniTune~\cite{valevski2022unitune} finetunes a pretrained large text-to-image diffusion model (e.g.,Imagen) on a single image.

\subsubsection{3D object editing}
Apart from 3D generation, 3DDesigner~\cite{li20223ddesigner} is also the first to perform 360-degree manipulation by editing from a single view. With the given text, 3DDesigner~\cite{li20223ddesigner} generates corresponding text embedding by first obtaining blended noises with 2D local editing, and then mapping the blended noise to a view-independent text embedding space. The 360-degree results can be generated once the corresponding text embedding is obtained. 
DATID-3D~\cite{kim2022datid} works on another topic, text-guided domain adaption of 3D objects, which suffers from three limitations: catastrophic diversity loss, inferior text-image correspondence and poor image quality~\cite{kim2022datid}. To solve these problems, DATID-3D~\cite{kim2022datid} first obtains diverse pose-aware target images with diffusion models, and then rectifies the obtained target image by improved CLIP and filtering process. Evaluated with the state-of-the-art 3D generator, EG3D~\cite{chan2022efficient}, DATID-3D~\cite{kim2022datid} achieves high-resolution target results with multi-view consistency.

\subsubsection{Other interesting tasks}  For the complex edits further, Imagic~\cite{kawar2022imagic} is   the first to perform  text-based semantic edits to a single image. Specifically, Imagic~\cite{kawar2022imagic} first obtains an optimized embedding for the target text, which generates similar images to the input. Then, Imagic~\cite{kawar2022imagic} fine-tunes the pretrained diffusion models with the optimized embedding with the reconstruction loss, and linearly interpolates between the target text embedding and the optimized one. This generated representation is then sent to the fine-tuned model and generates the edited images. 
Apart from editing the attribute or styles of an image, there are also other interesting tasks regarding image editing. Paint by example~\cite{yang2022paint} proposes a  semantic image composition problem reference image is semantically transformed and harmonized before blending into another image~\cite{yang2022paint}. Further, 
MagicMix~\cite{liew2022magicmix}  proposes a new task called semantic mixing, which blends two different semantics (e.g.,corgi and coffee machine) to create a new concept (corgi-like coffee machine).  Inspired by the property of diffusion models that layout(e.g., shape and color) and semantic contents appears at earlier and later stage of denoising, respectively,  MagicMix~\cite{liew2022magicmix} proposes to mix two concepts with the content at different timesteps. 
InstructPix2Pix~\cite{brooks2022instructpix2pix} works on the task of  editing the image with human-written human instructions. Based on a large model (GPT-3) and a text-to-image model (Stable diffusion), ~\cite{brooks2022instructpix2pix} first generates a dataset for this new task, and trains a conditional diffusion model  InstructPix2Pix which generalize to real images well. However, it is admitted in ~\cite{brooks2022instructpix2pix} there still remains some limitations, e.g., the model is limited by the visual quality of generated datasets. Another work ~\cite{wang2022pretraining} proposes to regard the image-to-image translation problem as a downstream task and successfully synthesizes images of unprecedented realism and faithfulness based on a pretrained diffusion model, termed as pretraining-based image-to-image translation (PITI)~\cite{wang2022pretraining}.

\section{Challenges and outlook}





\subsection{Challenges}



\textbf{Challenges on dataset bias.}
Since these large models are trained on the collected text-image pair data, which inevitably introduces data bias, such as race and gender. Moreover, the current models predominantly or exclusively adopt English as the default language for input text. This might further put those people who do not understand English in an unfavored situation. A more diverse and balanced dataset and new methods are  preferred to eliminate the influence of dataset bias on the model.

\textbf{Challenges on data and computation.} As widely recognized, the success of deep learning heavily depends on the labeled data. In the context of text-to-image DM, this is especially true. For example, the major frameworks such as DALL-E 2~\cite{ramesh2022hierarchical}, GLIDE~\cite{nichol2021glide}, Imagen~\cite{saharia2022photorealistic}, all are trained with hundreds of millions of image-text pairs~\cite{radford2021learning,ramesh2021zero}. Moreover, the computation overhead is so large that it renders the opportunities to train such a model from scratch to large companies, such as OpenAI~\cite{ramesh2022hierarchical}, Google~\cite{saharia2022photorealistic}, Baidu~\cite{li2022upainting}. Notably, the model size is also very large, preventing their deployment in efficiency-oriented environments, such as edge devices. 



\textbf{Challenges on evaluation.}  Despite the attempts on  evaluation criteria, diverse and efficient evaluation is still challenging.  First, existing automatic evaluation metrics have their limitations, e.g., FID is not always consistent with perceptual quality~\cite{parmar2022aliased,saharia2022photorealistic}, and  CLIP score is found ineffective at counting 
~\cite{radford2021learning,saharia2022photorealistic}.  More reliable and diverse automatic evaluation criteria are needed.
Second, human evaluation depends on aesthetic differences among raters and limits the number of prompts due to the efficiency problem. Third, most benchmarks introduce various text prompts to evaluate the model from different aspects. However,  biases may exist in the human-designed prompts, and the quality of prompts may be limited especially for the evaluation of complex scenes.









\subsection{Outlook}



\textbf{Diverse and effective evaluation.} So far, the evaluation of text-to-image methods relies on quantative metrics and human evaluation. Moreover, different papers may has their own benchmarks. A unified evaluation framework is needed, which  should have clear and diverse evaluation criteria (e.g., more metrics) and can be reproduced by different researchers  for fair comparison.



\textbf{Unified multi-modality framework.} The core of text-to-image generation is generate images from text, thus it can be seen  as one part of the multi-modality learning. Most works focus on the single task of  text-to-image generation, but unifying multiple task into a single model can be a promising trend. For example, 
UniD3~\cite{hu2022unified} and Versatile Diffusion  ~\cite{xu2022versatile}  unify text-to-image generation and image captioning with a single diffusion model. The unified multi-modality model can boost each task by learning  representations from each modality better.


\textbf{Collaboration with other fields.} In the past few years, deep learning has made great progress in multiple areas, including masked autoencoder in  self-supervised learning and recent ChatGPT in the natual language processing field. How to collaborate text-to-image diffusion model and these recent findings in active research fields, is an exciting topic to be explored.






\bibliographystyle{IEEEtran}
\bibliography{bib_mixed,bib_local}


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\end{document}


