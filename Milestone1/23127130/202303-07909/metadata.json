{
    "arxiv_id": "2303.07909",
    "paper_title": "Text-to-image Diffusion Models in Generative AI: A Survey",
    "authors": [
        "Chenshuang Zhang",
        "Chaoning Zhang",
        "Mengchun Zhang",
        "In So Kweon"
    ],
    "submission_date": "2023-03-14",
    "revised_dates": [
        "2023-04-02"
    ],
    "latest_version": 2,
    "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
    ],
    "abstract": "This survey reviews text-to-image diffusion models in the context that\ndiffusion models have emerged to be popular for a wide range of generative\ntasks. As a self-contained work, this survey starts with a brief introduction\nof how a basic diffusion model works for image synthesis, followed by how\ncondition or guidance improves learning. Based on that, we present a review of\nstate-of-the-art methods on text-conditioned image synthesis, i.e.,\ntext-to-image. We further summarize applications beyond text-to-image\ngeneration: text-guided creative generation and text-guided image editing.\nBeyond the progress made so far, we discuss existing challenges and promising\nfuture directions.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.07909v1",
        "http://arxiv.org/pdf/2303.07909v2"
    ],
    "publication_venue": "First survey on the recent progress of text-to-image generation based\n  on the diffusion model (under progress)"
}