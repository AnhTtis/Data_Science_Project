\section{Introduction}

Policy optimization problems in reinforcement learning (RL) are usually formulated as the maximization of a non-concave objective function over a convex constraint set. 
Such non-convex programs are generally difficult to solve globally, as gradient-based optimization algorithms can be trapped in sub-optimal first-order stationary points.
% 
Interestingly, recent advances in RL theory \citep{fazel2018global,agarwal2021theory,mei2020global} have discovered a ``gradient domination'' structure in the optimization landscape, which qualitatively means that every stationary point of the objective function is globally optimal. 
An important consequence of this condition is that any first-order algorithm that converges to a stationary point is guaranteed to find the global optimality. 




In this work, our aim is to enhance the understanding of the optimization landscape in RL beyond the gradient domination condition.
Inspired by \citet{mohammadi2021convergence,fatkhullin2021optimizing} that discuss properties of the sublevel set for the linear-quadratic regulator (LQR), we study the superlevel set of the policy optimization objective under a Markov decision process (MDP) framework and prove that it is always connected.
% To the best of our knowledge, our paper is the first to rigorously investigate this subject.

As an immediate consequence, we show that any minimax optimization program which is convex on one side and is an RL objective on the other side observes the minimax equality. We apply this result to derive an interesting and previously unknown minimax theorem for robust RL. We also note that it is unclear at the moment, but certainly possible, that the result on connected superlevel sets may be exploited to design more efficient and reliable policy optimization algorithms in the future.


\subsection{Main Contribution}
Our first contribution in this work is to show that the superlevel set of the policy optimization problem in RL is always connected under a tabular policy representation. We then extend this result to the deep reinforcement learning setting, where the policy is represented by a class of over-parameterized neural networks. We show that the superlevel set of the underlying objective function with respect to the policy parameters (i.e. weights of the neural networks) is connected at all levels. We further prove that the policy optimization objective as a function of the policy parameter and reward is ``equiconnected'', which is a stronger result that we will define and introduce later in the paper.
To the best of our knowledge, our paper is the first to rigorously investigate the connectedness of the superlevel sets for the MDP policy optimization program, both in the tabular case and with a neural network policy class.


As a downstream application, we discuss how our main results can be used to derive a minimax theorem for robust RL. We consider the scenario where an adversary strategically modifies the reward function to trick the learning agent into adopting a target policy. Aware of the attack, the learning agent defends against the poisoned reward by solving a minimax optimization program. The formulation for this problem is proposed and considered in \citet{banihashem2021defense,rakhsha2020policy}. However, as a fundamental question, the validity of the minimax theorem (or equivalently, the existence of a Nash equilibrium) is still unknown. We fill in this gap by establishing the minimax theorem as a simple consequence of the equiconnectedness of the policy optimization objective.




\subsection{Related Works}
Our paper is closely connected to the existing works that study the structure of policy optimization problems in RL, especially those on the gradient domination condition. Our result also relates to the literature on minimax optimization for various function classes and robust RL. We discuss the recent advances in these domains to give context to our contributions.


\noindent\textbf{Gradient Domination Condition.}
The policy optimization problem in RL is non-convex but obeys the special ``gradient domination'' structure that allows first-order algorithms to provably converge to the globally optimal policy. In the settings of LQR \citep{fazel2018global,yang2019provably} and entropy-regularized MDP \citep{mei2020global,cen2022fast}, the gradient domination structure can be mathematically described by the Polyak-\L ojasiewicz (P\L) condition, which bears a resemblance to strong convexity but does not even imply convexity. It is known that functions observing this condition can be optimized globally and efficiently by (stochastic) optimization algorithms \citep{karimi2016linear,gower2021sgd,zeng2021two}. When the policy optimization problem under a standard, non-regularized MDP is considered, the gradient domination structure is weaker than the P\L~condition but still takes the form of upper bounding a global optimality gap by a measure of the magnitude of the gradient \citep{bhandari2019global,agarwal2020optimality,agarwal2021theory}. In all scenarios, the gradient domination structure prevents any stationary point from being sub-optimal.



It may be tempting to think that the gradient domination condition and the connectedness of the superlevel sets are strongly connected notions or may even imply one another.
For 1-dimensional function ($f:\mathbb{R}^n\rightarrow \mathbb{R}$ with $n=1$), it is easy to verify that the gradient domination condition necessarily implies the connectedness of the superlevel sets. However, when $n\geq2$ this is no longer true. In general, the gradient domination condition neither implies nor is implied by the connectedness of superlevel sets, which we illustrate with examples in Section~\ref{sec:Connected_GradDom}. These two structural properties are distinct concepts that characterize the optimization landscape from different angles. This observation precludes the possibility of deriving the connectedness of the superlevel sets in RL simply from the existing results on the gradient domination condition, and suggests that a tailored analysis is required.



\noindent\textbf{Minimax Optimization \& Minimax Theorems.} Consider a function $f:\Xcal\times\Ycal\rightarrow\mathbb{R}$ on convex sets $\Xcal,\Ycal$. In general, the minimax inequality always holds
\begin{align*}
    \sup_{x\in\Xcal}\inf_{y\in\Ycal}f(x,y)\leq\inf_{y\in\Ycal} \sup_{x\in\Xcal} f(x,y).
\end{align*}
% For special classes of functions, this inequality can hold as an equality. 
The seminal work \citet{neumann1928theorie} shows that this inequality holds as an equality for matrix games where $\Xcal\subseteq\mathbb{R}^m,\Ycal\subseteq\mathbb{R}^n$ are probability simplexes and we have $f(x,y)=x^{\top}Ay$ given a payoff matrix $A\in\mathbb{R}^{m\times n}$. The result later gets generalized to the setting where $\Xcal,\Ycal$ are compact sets, $f(x,\cdot)$ is quasi-convex for all $x\in\Xcal$, and $f(\cdot,y)$ is quasi-concave for all $y\in\Ycal$ \citep{fan1953minimax,sion1958general}. Much more recently, \citet{yang2020global} establishes the minimax equality when $f$ satisfies the two-sided P\L~condition.
For arbitrary functions $f$, the minimax equality need not be valid. 

The validity of the minimax equality is essentially equivalent to the existence of a global Nash equilibrium $(x^{\star},y^{\star})$ such that
\begin{align*}
    f(x,y^{\star})\leq f(x^{\star},y^{\star})\leq f(x^{\star},y),\quad\forall x\in\Xcal,y\in\Ycal.
\end{align*}
The Nash equilibrium $(x^{\star},y^{\star})$ is a point where neither player can improve their objective function value by changing its strategy.
In general nonconvex-nonconcave settings where the global Nash equilibrium may not exist, alternative approximate local/global optimality notions are proposed \citep{daskalakis2018limit,nouiehed2019solving,adolphs2019local,jin2020local}.

\noindent\textbf{Robust Reinforcement Learning.}
Robust RL studies finding the optimal policy in the worst-case scenario under environment uncertainty and/or possible adversarial attacks. Various robust RL models have been considered in the existing literature, such as: 1) the learning agent operates under uncertainty in the transition probability kernel \citep{goyal2022robust,li2022first,panaganti2022sample,wang2023robust}, 2) an adversary exists and plays a two-player zero-sum Markov game against the learning agent \citep{pinto2017robust,tessler2019action}, 3) the adversary does not affect the state transition but may manipulate the state observation \citep{havens2018online,zhang2020robust}, 4) there is uncertainty or attack only on the reward \citep{wang2020reinforcement,banihashem2021defense,sarkar2022reward}, 5) the learning agent defends attacks from a population of adversaries rather than a single one \citep{vinitsky2020robust}. The particular attack and defense model considered later in our paper is adapted from \citet{banihashem2021defense}.


\noindent\textbf{Other Works on Connected Level Sets in Machine Learning.}
Last but not least, we note that our paper is related to the works that study the connectedness of the sublevel sets for the LQR optimization problem \citep{fatkhullin2021optimizing} and for deep supervised learning under a regression loss \citep{nguyen2019connected}. 
% An interesting observation made by \citep{fatkhullin2021optimizing} is that the original, full-information LQR problem has connected sublevel sets, but the partially observable LQR does not.  
The neural network architecture considered in our paper is inspired by and similar to the one in \citet{nguyen2019connected}. However, our result and analysis on deep RL are novel and significantly more challenging to establish, since 1) the underlying loss function in \citet{nguyen2019connected} is convex, while ours is a non-convex policy optimization objective, 2) the analysis of \citet{nguyen2019connected} relies critically on the assumption that the activation functions are uniquely invertible, while we use a non-uniquely invertible softmax activation function to generate policies within the probability simplex.

\input{Connected_GradDom}


\noindent\textbf{Outline of the paper.} The rest of the paper is organized as follows. In Section~\ref{sec:tabular}, we discuss the policy optimization problem in the tabular setting and establish the connectedness of the superlevel sets. Section~\ref{sec:NN} generalizes the result to a class of policies represented by over-parameterized neural networks. We introduce the structure of the neural network and the definition of super level sets in this context, and present our theoretical result. In Section~\ref{sec:application}, we use our main results on superlevel sets to derive two minimax theorems for robust RL. Finally, we conclude in Section~\ref{sec:conclusion} with remarks on future directions.\looseness=-1
