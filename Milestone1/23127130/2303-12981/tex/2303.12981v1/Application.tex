\section{A Minimax Theorem for Robust Reinforcement Learning}\label{sec:application}



In this section, we discuss the reward poisoning attack considered in \citet{banihashem2021defense}, which can be formulated as a convex-nonconcave minimax optimization program. We show that the minimax equality holds in this optimization problem in the tabular policy setting and under policies represented by a class of neural networks, as a consequence of our results in Sections~\ref{sec:tabular} and \ref{sec:NN}. To our best knowledge, the existence of the Nash equilibrium for this robust RL problem has not been established before even in the tabular case.


We again consider the infinite horizon, average reward MDP $\Mcal=(\Scal,\Acal,\Pcal,r)$ introduced in Section~\ref{sec:tabular}, where $r$ is the true, unpoisoned reward function. 
% We use $\pi^{\star}$ to denote an optimal policy, which is a (not necessarily unique) solution to \eqref{eq:obj}. 
% To reflect the dependency of the value function on the reward, we denote
% \begin{align*}
%     J_r(\pi)&=\lim_{K \rightarrow \infty} \frac{\sum_{k=0}^{K} r(s_k, a_k)}{K}=\mathbb{E}_{s\sim\mu_{\pi}, a\sim \pi}[r(s_k,a_k)]=r^{\top}\widehat{\mu}_{\pi}.
%     % \pi_r^{\star}&=\argmax_{\pi\in\Delta(\Acal)^{\Scal}}J_r(\pi).
% \end{align*}
Let $\Pi^{\text{det}}$ denote the set of deterministic policies from $\Scal$ to $\Acal$. With the perfect knowledge of this MDP, an attacker has a target policy $\pi_{\dagger}\in\Pi^{\text{det}}$ and tries to make the learning agent adopt the policy by manipulating the reward function. 
Mathematically, the goal of the attacker can be described by the function $\operatorname{Attack}(r,\pi_{\dagger},\epsilon_{\dagger})$ which returns a poisoned reward under the true reward $r$, the target policy $\pi_{\dagger}$, and a pre-selected margin parameter $\epsilon_{\dagger}\geq0$. $\operatorname{Attack}(r,\pi_{\dagger},\epsilon_{\dagger})$ is the solution to the following optimization problem
\begin{align}
    \begin{aligned}
    \operatorname{Attack}(r,\pi_{\dagger},\epsilon_{\dagger})\quad=\quad\argmin_{r'}\quad&\sum_{s\in\Scal,a\in\Acal}\left(r'(s,a)-r(s,a)\right)^2\\
    \operatorname{s.t.}\quad& J_{r'}(\pi_{\dagger})\geq J_{r'}(\pi)+\epsilon_{\dagger},\quad\forall \pi\in\Pi^{\text{det}}\backslash\pi_{\dagger}.
    \end{aligned}\label{eq:attack_obj}
\end{align}
In other words, the attacker needs to minimally modify the reward function to make $\pi_{\dagger}$ the optimal policy under the poisoned reward. This optimization program minimizes a quadratic loss under a finite number of linear constraints and is obviously convex.

The learning agent observes the poisoned reward $r_{\dagger}=\operatorname{Attack}(r,\pi_{\dagger},\epsilon_{\dagger})$ rather than the original reward $r$. As noted in \citet{banihashem2021defense}, without any defense, the learning agent solves the policy optimization problem under $r_{\dagger}$ to find $\pi_{\dagger}$, which may perform arbitrarily badly under the original reward. One way to defend against the attack is to maximize the performance of the agent in the worst possible case of the original reward, which leads to solving a minimax optimization program of the form
\begin{align}
    \max_{\pi\in\Delta_{\Acal}^{\Scal}}\min_{r'} J_{r'}(\pi)\quad\operatorname{s.t.}\,\, \operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}.\label{eq:robustrl_obj}
\end{align}
% When we fix the reward $r'$, \eqref{eq:robustrl_obj} reduces to a standard policy optimization problem, which we have shown is non-convex but has connected superlevel sets. On the other hand, when the policy $\pi$ is fixed, \eqref{eq:robustrl_obj} reduces to 
% \begin{align}
%     \min_{r'} J_{r'}(\pi)\quad\operatorname{s.t.}\,\, \operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger},\label{eq:robustrl_convexside}
% \end{align}
% which has a linear objective function and a convex (and compact) constraint set and is therefore a convex program\footnote{We justify this claim in Section~\ref{sec:robustrl_convexside_proof} of the appendix.}.
When the policy $\pi$ is fixed, \eqref{eq:robustrl_obj} reduces to 
\begin{align}
    \min_{r'} J_{r'}(\pi)\quad\operatorname{s.t.}\,\, \operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}.\label{eq:robustrl_convexside}
\end{align}
With the justification deferred to Appendix~\ref{sec:robustrl_convexside_proof}, we point out that \eqref{eq:robustrl_convexside} consists of a linear objective function and a convex (and compact) constraint set, and is therefore a convex program. On the other hand, when we fix the reward $r'$, \eqref{eq:robustrl_obj} reduces to a standard policy optimization problem.

We are interested in investigating whether the following minimax equality holds.
\begin{align}
    \max_{\pi\in\Delta_{\Acal}^{\Scal}}\min_{r':\operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}} J_{r'}(\pi) = \min_{r':\operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}} \max_{\pi\in\Delta_{\Acal}^{\Scal}}J_{r'}(\pi).
    \label{eq:robustrl_minimax}
\end{align}
This is a fundamental question to ask in minimax optimization, as it is is an important characterization of the optimization landscape. The minimax equality implies the existence of a Nash equilibrium. At the Nash equilibrium solution pair, neither player can achieve a better function value by changing its strategy, which provides a strong notion of equilibrium and global optimality.
The knowledge of the existence of the Nash equilibrium may be useful for designing and analyzing algorithms for solving the problem \citep{kim2008minimax,ricceri2008recent}. 

% It is known that the minimax inequality always holds
% \begin{align*}
%     \max_{\pi}\min_{r':\operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}} J_{r'}(\pi) \leq \min_{r':\operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}} \max_{\pi}J_{r'}(\pi).
% \end{align*}
% However, as the program is not convex-concave, it is unclear whether this condition holds as an equality.

% In the rest of the section, we show that the connectedness of the superlevel sets in reinforcement learning implies the minimax equality
% \begin{align}
%     \max_{\pi}\min_{r':\operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}} J_{r'}(\pi) = \min_{r':\operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}} \max_{\pi}J_{r'}(\pi).
%     \label{eq:robustrl_minimax}
% \end{align}
% Equivalently, this equality means that there exists a Nash equilibrium solution pair $(\pi^{\star},r^{\star})$.


It is well-known that the minimax equality 
\begin{align}
    \min_{y \in \Ycal} \max_{x \in \Xcal} f(x, y)=\max_{x \in \Xcal} \min_{y \in \Ycal} f(x, y)
\end{align}
holds for function $f:\Xcal\times \Ycal\rightarrow\mathbb{R}$ if 1) $\Xcal,\Ycal$ are finite-dimensional simplexes \citep{neumann1928theorie}, or 2) $f$ is continuous quasiconvex-quasiconcave and $\Xcal,\Ycal$ are convex compact sets \citep{sion1958general,kindler2005simple}. However, the function $J_r(\pi)$ does not fall under either category. This makes the validity of equation \eqref{eq:robustrl_minimax} unclear from the existing literature.




% In the rest of the section, we prove that the connectedness of the superlevel sets in reinforcement learning implies a minimax theorem for \eqref{eq:robustrl_obj}
% \begin{align}
%     \sup_{\theta\in\Omega}\min_{r\in \Rcal}\mathbb{E}[J_r(\pi_{\theta})] = \min_{r\in \Rcal}\sup_{\theta\in\Omega}\mathbb{E}[J_r(\pi_{\theta})].
%     \label{eq:robustrl_minimax}
% \end{align}
% The reason of using supremum rather than the maximum is that the optimal policy may be deterministic. Under a softmax policy parameterization, a deterministic policy can only be achieved in the limit by sending certain parameters to infinity.

In the rest of this section, we establish the equality \eqref{eq:robustrl_minimax} and show that it is a simple consequence of the connectedness of the superlevel sets in reinforcement learning and a minimax theorem adapted from \citet{simons1995minimax} on a special class of convex-nonconcave functions. We now state this minimax theorem and note that this is essentially a simplified version of \citet{simons1995minimax}[Theorem 4].
% specialized to the Euclidean space.


\begin{thm}\label{thm:minimax_simplified}
Consider a separately continuous function $f:\Xcal\times \Ycal\rightarrow\mathbb{R}$, with $\Ycal$ being a convex, compact set. 
Suppose that $f(x,\cdot)$ is convex for all $x\in\Xcal$. Also suppose that the collection of functions $\{f(\cdot,y)\}_{y\in\Ycal}$ is equiconnected. Then, we have
\begin{align}
    \sup_{x \in \Xcal} \min_{y\in \Ycal} f(x,y)=\min_{y\in \Ycal} \sup_{x \in \Xcal} f(x,y).
\end{align}
\end{thm}
Theorem~\ref{thm:minimax_simplified} states that the minimax equality holds under two main conditions (other than the continuity condition, which can easily be verified to hold for $J_{r}(\pi)$). First, the function $f(x,y)$ needs to be convex with respect to the variable $y$ within a convex, compact constraint set. Second, $f(x,y)$ needs to have a connected superlevel set with respect to $x$, and the path function constructed to prove the connectedness of the superlevel set is independent of $y$. As we have shown in this section and earlier in the paper, if we model $J_r(\pi)$ by $f(x,y)$ with $\pi$ and $r$ corresponding to $x$ and $y$, both conditions are observed in the optimization problem \eqref{eq:robustrl_obj}, which allows us to state the following corollary.
\begin{cor}\label{cor:minimax_robustrl_tabular}
Suppose that the Markov chain $\Mcal$ satisfies Assumption~\ref{assump:ergodicity} on ergodicity. Then, the minimax equality \eqref{eq:robustrl_minimax} holds.
\end{cor}

When the neural network presented in Section~\ref{sec:NN} is used to represent the policy, the collection of functions $\{J_{r,\Omega}\}_{r}$ is also equiconnected. This allows us to extend the minimax equality above to the neural network policy class. Specifically, consider the poisoned reward defense problem \eqref{eq:robustrl_obj} where the policy $\pi_{\theta}$ is represented by the parameter $\theta\in\Omega$ as described in Section~\ref{sec:NN}. 
Using $f(x,y)$ to model $J_r(\pi_{\theta})$ with $x$ and $y$ mirroring $\theta$ and $r$, we can easily establish the minimax theorem in this case as a consequence of Theorem~\ref{thm:connected_firstlayer2S} and \ref{thm:minimax_simplified}.

\begin{cor}\label{cor:minimax_robustrl_NN}
Suppose that the Markov chain $\Mcal$ satisfies Assumption \ref{assump:ergodicity} on ergodicity and that the neural policy class satisfies Assumptions~\ref{assump:sigma}-\ref{assump:network_dimension}. Then, we have
\begin{align}
    \sup_{\theta\in\Omega}\min_{r':\operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}} J_{r'}(\pi_{\theta}) = \min_{r':\operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}} \sup_{\theta\in\Omega}J_{r'}(\pi_{\theta}).
\end{align}
\end{cor}

Corollary~\ref{cor:minimax_robustrl_tabular} and \ref{cor:minimax_robustrl_NN} establish the minimax equality (or equivalently, the existence of the Nash equilibrium) for the robust reinforcement learning problem under adversarial reward attack for the tabular and neural network policy class, respectively. To our best knowledge, these results are both novel and previously unknown in the existing literature. The Nash equilibrium is an important global optimality notion in minimax optimization, and the knowledge on its existence can provide strong guidance on designing and analyzing algorithms for solving the problem.



