\documentclass[letterpage, 11pt, notitlepage]{article}

\usepackage[margin=1.0in]{geometry}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bbold}
\usepackage{capt-of}

\usepackage[center]{caption}


\usepackage{natbib,graphicx,epstopdf,color,soul}
% \usepackage{algorithm,algorithmic,tabularx}
% \usepackage{tabularx}
% \usepackage[ruled,vlined]{algorithm2e}
\usepackage{color,amssymb}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{multicol}
\usepackage{empheq}
\usepackage{caption} 
% \captionsetup[table]{skip=10pt}



\input{header}
\input{defs-mlmath.tex}
\newcommand{\qed}{\hfill $\blacksquare$}


\title{Connected Superlevel Set in (Deep) Reinforcement Learning and its Application to Minimax Theorems}
\author{Sihan Zeng\thanks{School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA
.}
\and Thinh T. Doan\thanks{Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA.}
\and Justin Romberg\footnotemark[1]}
\let\underbrace\LaTeXunderbrace
\let\overbrace\LaTeXoverbrace


\begin{document}

\maketitle
\begin{abstract}
% The policy optimization problem in reinforcement learning can be formulated as a non-convex optimization program with a recently discovered ``gradient domination’’ condition, which guarantees that every stationary point of the objective function is globally optimal. Apart from this condition, our knowledge on the optimization landscape is still limited. The aim of this paper is to provide more insight on the structure of the policy optimization problem. Specifically, we show that the superlevel set of the objective function with respect to the policy parameter is always a connected set both in the tabular setting and under policies represented by a class of neural networks. To our best knowledge, this is a novel discovery in the literature.

The aim of this paper is to improve the understanding of the optimization landscape for policy optimization problems in reinforcement learning. Specifically, we show that the superlevel set of the objective function with respect to the policy parameter is always a connected set both in the tabular setting and under policies represented by a class of neural networks.
In addition, we show that the optimization objective as a function of the policy parameter and reward satisfies a stronger ``equiconnectedness'' property.
To our best knowledge, these are novel and previously unknown discoveries.


We present an application of the connectedness of these superlevel sets to the derivation of minimax theorems for robust reinforcement learning. We show that any minimax optimization program which is convex on one side and is equiconnected on the other side observes the minimax equality (i.e. has a Nash equilibrium). We find that this exact structure is exhibited by an interesting robust reinforcement learning problem under an adversarial reward attack, and the validity of its minimax equality immediately follows. This is the first time such a result is established in the literature.
\end{abstract}

\input{Introduction}
\input{Tabular}
\input{NeuralNetwork}
\input{Application}
\input{Conclusion}

\medskip

\bibliographystyle{plainnat}
\bibliography{references}
\clearpage
\appendix

\input{Proof_Theorems}
\input{Proof_Proposition}
\input{Proof_Lemmas}
\input{RobustRL_OnesideConvex}




\end{document}
