\section{Connected Superlevel Set Under Neural Network Parameterized Policy}\label{sec:NN}

In real-world reinforcement learning applications, it is common to use a deep neural network to parameterize the policy \citep{silver2016mastering,arulkumaran2017deep}. 
% In general, neural networks break the gradient domination structure of the underlying optimization problem, though under specific assumptions (such as over-parameterization, sufficient width of the layers, and/or small number of the layers) the gradient domination condition may be recovered \citep{liu2019neural,wangneural,cayci2022finite}. 
In this section, we consider the policy optimization problem under a special class of policies represented by an over-parameterized neural network and show that this problem still enjoys the important structure --- the connectedness of the superlevel sets --- despite the presence of the highly complex function approximation. Illustrated in Fig.~\ref{fig:NN}, the neural network parameterizes the policy in a very natural manner which matches how neural networks are actually used in practice.


\begin{figure}[h]
  \centering
  \includegraphics[width=.95\linewidth]{Figures/NN.png}
  \caption{Neural Network Policy Representation}
  \label{fig:NN}
\end{figure}

Mathematically, the parameterization can be described as follows. Each state $s\in\Scal$ is associated with a feature vector $\phi(s)\in\mathbb{R}^d$, which in practice is usually carefully selected to summarize the key information of the state. 
For state identifiability, we assume that the feature vector of each state is unique, i.e.
\begin{align*}
    \phi(s)\neq\phi(s'),\quad \forall s,s'\in\Scal\text{ and }s\neq s'.
\end{align*}

To map a feature vector $\phi(s)$ to a policy distribution over state $s$, we employ a $L$-layer neural network, which in the $k_{\text{th}}$ layer has weight matrix $W_k\in\mathbb{R}^{n_{k-1}\times n_k}$ and bias vector $b_k\in\mathbb{R}^{n_k}$ with $n_0=d$ and $n_L=|\Acal|$. For the simplicity of notation, we use $\Omega_k$ to denote the space of weight and bias parameters $(W_k,b_k)$ of layer $k$, and we write $\Omega=\Omega_1\times\cdots\times\Omega_L$.
We use $\theta$ to denote the collection of the weights and biases
\[\theta=((W_1,b_1),\cdots,(W_L,b_L))\in\Omega\]
We use the same activation function for layers $1$ through $L-1$, denoted by $\sigma:\mathbb{R}\rightarrow\mathbb{R}$, applied in an element-wise fashion to vectors. 
To ensure that the output of the neural network is a valid probability distribution, the activation function for the last layer is a softmax function, denoted by $\psi:\mathbb{R}^{|\Acal|}\rightarrow\Delta_{\Acal}$, i.e. for any vector $v\in\mathbb{R}^{|\Acal|}$
\begin{align*}
    \psi(v)_{i}=\frac{\exp(v_i)}{\sum_{i'=1}^{|\Acal|}\exp(v_{i'})}, \quad \forall i=1,...,|\Acal|.
\end{align*}
With $v\in\mathbb{R}^{d}$ as the input to a neural network with parameters $\theta$, we use $f_k^{\theta}(v)\in\mathbb{R}^{n_k}$ to denote the output of the network at layer $k$. For $k=1,\cdots,L$, $f_k^{\theta}(v)$ is computed as
\begin{align}
    f_k^{\theta}(v) = 
    \left\{\begin{array}{ll}
        \sigma\left(W_1^{\top}v+b_{1}\right) & k=1 \\
        \sigma\left(W_k^{\top}f_{k-1}(v)+ b_{k}\right) & k = 2, 3,..., L-1 \\
        \psi\left(W_{L}^{\top}f_{L-1}(v)+b_{L}\right) & k=L.
    \end{array}\right.\label{eq:def_f_k}
\end{align}
The policy $\pi_{\theta}\in\mathbb{R}^{|\Scal|\times|\Acal|}$ parametrized by $\theta$ is the output of the final layer:
%We define $\pi_{\theta}\in\mathbb{R}^{|\Scal|\times|\Acal|}$ such that
\[\pi_{\theta}(\cdot\mid s)=f_L^{\theta}(\phi(s))\in\Delta_{\Acal},\quad\forall s\in\Scal.\]


Our analysis relies two assumptions about the structure of the neural network.  The first concerns the invertibility of $\sigma(\cdot)$ as well as the continuity and uniqueness of its inverse, which can be guaranteed by the following:
\begin{assump}\label{assump:sigma}
$\sigma$ is strictly monotonic and $\sigma(\mathbb{R})=\mathbb{R}$. In addition, there do not exist non-zero scalars $\{p_i,q_i\}_{i=1}^{m}$ with $q_i\neq q_j,\,\forall i\neq j$ such that for some $m>0$, $\sigma(x)=\sum_{i=1}^{m}p_i\sigma(x-q_i),\,\forall x\in\mathbb{R}$.
\end{assump}
We note that this assumption holds for common activation functions including leaky-ReLU and parametric ReLU \citep{xu2015empirical}.


Our second assumption is that the neural network is sufficiently over-parameterized and that the number of parameters decreases with each layer.  
\begin{assump}\label{assump:network_dimension}
The output of the first layer is wider than $2|\Scal|$, and the width of the network decreases over the layers, i.e.
\begin{align*}
    n_1\geq 2|\Scal|, \text{ and } n_1>n_2>...>n_L=|\mathcal{A}|.
\end{align*}
\end{assump}
%We next consider an assumption on the widths of the layers which ensures the neural network to be sufficiently over-parameterized. 
Neural networks meeting this criteria have a number of weight parameters that is larger than the cardinality of the state space, making them impractical for large $|\mathcal{S}|$.
%which seems to defeat the purpose of using a function approximation in the first place. 
%
While ongoing work seeks to relax or remove this assumption, we point out that similar over-parameterization assumptions are critical and very common in most existing works on the theory of neural networks \citep{zou2019improved,nguyen2019connected,liu2022loss,martinetz2022highly,pandey2023exploring}.


% This assumption is also made in \citet{nguyen2019connected} which studies the connectedness of sublevel sets for deep (convex) supervised learning.





The $\lambda$-superlevel set of the value function with respect to $\theta$ under reward function $r$ is
\begin{align*}
    \Ucal_{\lambda,r}^{\Omega}\triangleq\{\theta\in\Omega\mid J_r(\pi_{\theta})\geq \lambda\}.
\end{align*}

Our next main theoretical result guarantees the connectedness of $\Ucal_{\lambda,r}^{\Omega}$.
\begin{thm}\label{thm:connected_firstlayer2S}
Under Assumptions \ref{assump:ergodicity}-\ref{assump:network_dimension}, the superlevel set $\Ucal_{\lambda,r}^{\Omega}$ is connected for any $\lambda\in\mathbb{R}$. 
In addition, with $J_{r,\Omega}(\theta)\triangleq J_r(\pi_{\theta})$, the collection of functions $\{J_{r,\Omega}(\cdot):\Omega\rightarrow\mathbb{R}\}_{r\in\mathbb{R}^{|\Scal|\times|\Acal|}}$ is equiconnected.
\end{thm}

The proof of this theorem is deferred to the appendix. Similar to Theorem~\ref{thm:connected_tabular}, the claim in Theorem~\ref{thm:connected_firstlayer2S} on the equiconnectedness of $\{J_{r,\Omega}\}_{r\in\mathbb{R}^{|\Scal|\times|\Acal|}}$ is again stronger than the connectedness of $\Ucal_{\lambda,r}^{\Omega}$ and needs to be derived for the application to minimax theorems, which we discuss in the next section.

