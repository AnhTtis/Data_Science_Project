\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% % ready for submission
% \usepackage{neurips_2023}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}
% \let\proof\relax
% \let\endproof\relax

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts,amssymb}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bbold}
\usepackage{capt-of}

\usepackage[center]{caption}


\usepackage{cite,epstopdf,color,soul}
% \usepackage{algorithm,algorithmic,tabularx}
% \usepackage{tabularx}
% \usepackage[ruled,vlined]{algorithm2e}
% \allowdisplaybreaks
\usepackage{color}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{multicol}
\usepackage{empheq}
\usepackage{caption} 
% \captionsetup[table]{skip=10pt}



\input{header}
\input{defs-mlmath.tex}
\newcommand{\qed}{\hfill $\blacksquare$}

\title{Connected Superlevel Set in (Deep) Reinforcement Learning and its Application to Minimax Theorems}
\usepackage{times}

\author{
  Sihan Zeng \\
  Dept. of Electrical and Computer Engineering\\
  Georgia Institute of Technology\\
  Atlanta, GA 30318 \\
  \texttt{szeng30@gatech.edu} \\
  \And
  Thinh T. Doan \\
  Dept. of Electrical and Computer Engineering \\
  Virginia Tech \\
  Blacksburg, VA 24061 \\
  \texttt{thinhdoan@vt.edu} \\
  \And
  Justin Romberg \\
  Dept. of Electrical and Computer Engineering \\
  Georgia Institute of Technology \\
  Atlanta, GA 30318 \\
  \texttt{jrom@ece.gatech.edu} \\
}


% This work was supported in part by the NSF AI Institute AI4OPT, NSF 2112533.



\begin{document}

\maketitle
\begin{abstract}
% The policy optimization problem in reinforcement learning can be formulated as a non-convex optimization program with a recently discovered ``gradient domination’’ condition, which guarantees that every stationary point of the objective function is globally optimal. Apart from this condition, our knowledge on the optimization landscape is still limited. The aim of this paper is to provide more insight on the structure of the policy optimization problem. Specifically, we show that the superlevel set of the objective function with respect to the policy parameter is always a connected set both in the tabular setting and under policies represented by a class of neural networks. To our best knowledge, this is a novel discovery in the literature.

The aim of this paper is to improve the understanding of the optimization landscape for policy optimization problems in reinforcement learning. Specifically, we show that the superlevel set of the objective function with respect to the policy parameter is always a connected set both in the tabular setting and under policies represented by a class of neural networks.
In addition, we show that the optimization objective as a function of the policy parameter and reward satisfies a stronger ``equiconnectedness'' property.
To our best knowledge, these are novel and previously unknown discoveries.


We present an application of the connectedness of these superlevel sets to the derivation of minimax theorems for robust reinforcement learning. We show that any minimax optimization program which is convex on one side and is equiconnected on the other side observes the minimax equality (i.e. has a Nash equilibrium). We find that this exact structure is exhibited by an interesting class of robust reinforcement learning problems under an adversarial reward attack, and the validity of its minimax equality immediately follows. This is the first time such a result is established in the literature.
\end{abstract}




\input{Introduction}
\input{Tabular}
\input{NeuralNetwork}
\input{Application}
\input{Conclusion}

% \section*{Acknowledgement}
% This work was supported in part by the NSF AI Institute AI4OPT, NSF 2112533.

\medskip
\bibliographystyle{plainnat}
\bibliography{references}
\clearpage
\appendix

\input{Proof_Theorems}
\input{Proof_Proposition}
\input{Proof_Lemmas}
\input{RobustRL_OnesideConvex}




\end{document}
