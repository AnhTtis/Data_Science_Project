\section{Proof of Theorems}

\subsection{Proof of Theorem~\ref{thm:connected_tabular}:}\label{sec:thm:connected_tabular}
We note that there exists a bijective map between $\pi$ and $\widehat{\mu}_{\pi}$ where $\widehat{\mu}_{\pi}$ is induced by $\pi$ according to \eqref{eq:mu_hat} and conversely
\begin{align}
    \pi(a\mid s)=\frac{\widehat{\mu}_{\pi}(s,a)}{\mu_{\pi}(s)}=\frac{\widehat{\mu}_{\pi}(s,a)}{\sum_{a\in\Acal}\widehat{\mu}_{\pi}(s,a)},\label{thm:connected_tabular:proof_eq1}
\end{align}
provided that $\mu_{\pi}(s)\neq 0$, which is guaranteed by Assumption \ref{assump:ergodicity}. Eq.~\eqref{thm:connected_tabular:proof_eq1} inspires the construction of the path map.


To prove that the superlevel set is connected, we show that for any $\lambda\in\mathbb{R}$ and $\pi_1,\pi_2\in \Ucal_{\lambda,r}$, there exists a continuous path map $p:[0,1]\rightarrow \Ucal_{\lambda,r}$ such that $p(0)=\pi_1$ and $p(1)=\pi_2$. 
We now construct the path function $p$ by defining
\begin{align*}
    p(\alpha)(a\mid s) = \frac{\alpha \mu_{\pi_1}(s)\pi_1(a\mid s)+(1-\alpha)\mu_{\pi_2}(s)\pi_2(a\mid s)}{\alpha \mu_{\pi_1}(s)+(1-\alpha)\mu_{\pi_2}(s)},
\end{align*}
which is well-defined for all $\alpha\in[0,1]$ as $\mu_{\pi_1}(s),\mu_{\pi_2}(s)$ are positive for all $s\in\Scal$.
Note that the construction of $p$ does not depend on the reward function $r$.
It is easy to see that $p(\alpha)\in\Delta_{\Acal}^{\Scal}$ is a continuous in $\alpha$. 
To stress that $p(\alpha)$ is in the policy space, we denote $\pi_{\alpha}=p(\alpha)$.

Recall the definition of the transition probability matrix in \eqref{eq:transition_matrix}. We define $B\in\mathbb{R}^{|\Scal|}$ as
\begin{align*}
    B=P^{\pi_{\alpha}}\cdot\left(\alpha\mu_{\pi_1}+(1-\alpha)\mu_{\pi_2}\right).
\end{align*}

Each entry of $B$ can be expressed as
\begin{align*}
    B(s') &= \sum_{s,a}\Pcal(s'\mid s,a)\pi_{\alpha}(a\mid s)\left(\alpha\mu_{\pi_1}(s)+(1-\alpha)\mu_{\pi_2}(s)\right)\notag\\
    &=\sum_{s,a}\Pcal(s'\mid s,a)\frac{\alpha \mu_{\pi_1}(s)\pi_1(a\mid s)+(1-\alpha)\mu_{\pi_2}(s)\pi_2(a\mid s)}{\alpha \mu_{\pi_1}(s)+(1-\alpha)\mu_{\pi_2}(s)}\left(\alpha\mu_{\pi_1}(s)+(1-\alpha)\mu_{\pi_2}(s)\right)\notag\\
    &=\sum_{s,a}\Pcal(s'\mid s,a)\alpha \mu_{\pi_1}(s)\pi_1(a\mid s)+\sum_{s,a}\Pcal(s'\mid s,a)(1-\alpha)\mu_{\pi_2}(s)\pi_2(a\mid s)\notag\\
    &=\alpha\sum_{s,a}P^{\pi_1}_{s',s} \mu_{\pi_1}(s)+(1-\alpha)\sum_{s,a}P^{\pi_2}_{s',s}\mu_{\pi_2}(s)\notag\\
    &=\alpha\mu_{\pi_1}(s')+(1-\alpha)\mu_{\pi_2}(s'),
\end{align*}
which implies
\begin{align}
    P^{\pi_{\alpha}}\cdot\left(\alpha\mu_{\pi_1}+(1-\alpha)\mu_{\pi_2}\right)=\alpha\mu_{\pi_1}+(1-\alpha)\mu_{\pi_2}.\label{thm:connected_tabular:proof_eq1.5}
\end{align}
A consequence of Assumption \ref{assump:ergodicity} is that for any policy $\pi$ there is a unique eigenvector of $P^{\pi}$ associated with the eigenvalue $1$, and this eigenvector (properly normalized) is the stationary distribution. Therefore, \eqref{thm:connected_tabular:proof_eq1.5} means that $\alpha\mu_{\pi_1}+(1-\alpha)\mu_{\pi_2}$ has to be the stationary distribution under policy $\pi_{\alpha}$, i.e.\looseness=-1
\begin{align*}
    \mu_{\pi_{\alpha}} = \alpha\mu_{\pi_1}+(1-\alpha)\mu_{\pi_2}.
\end{align*}
As a result, for all $s\in\Scal,a\in\Acal$
\begin{align*}
    \widehat{\mu}_{\pi_{\alpha}}(s,a) &= \mu_{\pi_{\alpha}}(s)\pi_{\alpha}(a\mid s)\\
    &=\left(\alpha\mu_{\pi_1}(s)+(1-\alpha)\mu_{\pi_2}(s)\right)\frac{\alpha \mu_{\pi_1}(s)\pi_1(a\mid s)+(1-\alpha)\mu_{\pi_2}(s)\pi_2(a\mid s)}{\alpha \mu_{\pi_1}(s)+(1-\alpha)\mu_{\pi_2}(s)}\\
    &=\alpha \mu_{\pi_1}(s)\pi_1(a\mid s)+(1-\alpha)\mu_{\pi_2}(s)\pi_2(a\mid s)\\
    &=\alpha\widehat{\mu}_{\pi_1}(s,a)+(1-\alpha)\widehat{\mu}_{\pi_2}(s,a).
\end{align*}

Note that $J_r(\pi) = \sum_{s\in\Scal,a\in\Acal}r(s,a)\widehat{\mu}_{\pi}(s,a)$.
Since $\pi_{\pi_1},\pi_{\pi_2}\in \Ucal_{\lambda,r}$, we know
\begin{align*}
    \sum_{s\in\Scal,a\in\Acal}r(s,a)\widehat{\mu}_{\pi_1}(s,a)\geq \lambda,\quad\sum_{s\in\Scal,a\in\Acal}r(s,a)\widehat{\mu}_{\pi_2}(s,a)\geq \lambda.
\end{align*}
Therefore, we have for any $\alpha\in[0,1]$
\[
    J_r(\pi_{\alpha})=\sum_{s\in\Scal,a\in\Acal}r(s,a)\widehat{\mu}_{\pi_{\alpha}}(s,a)=\sum_{s\in\Scal,a\in\Acal}r(s,a)\left(\alpha\widehat{\mu}_{\pi_1}(s,a)+(1-\alpha)\widehat{\mu}_{\pi_2}(s,a)\right)\geq \lambda,
\]
which implies $\pi_{\alpha}\in \Ucal_{\lambda,r}$. So far we have verifed that the constructed path map $p$ is indeed continuous and maps $\alpha\in[0,1]$ to $\Ucal_{\lambda,r}$ with $p(0)=\pi_1$ and $p(1)=\pi_2$. This concludes the proof on the connectedness of the superlevel set $\Ucal_{\lambda,r}$. The claim on the equiconnectedness simply follows from the fact that the construction of the path map $p$ does not depend on the reward function.

\qed


\subsection{Proof of Theorem~\ref{thm:connected_firstlayer2S}}
We use $X$ to denote the concatenation of the feature vectors across all states
\begin{align*}
    X \triangleq \left[\begin{array}{c}\phi(s_1)^{\top} \\ \phi(s_2)^{\top} \\ \vdots \\ \phi(s_{|\Scal|})^{\top}\end{array}\right]\in\mathbb{R}^{|\Scal|\times d}
\end{align*}

In the analysis we may apply the softmax function $\psi$ to a matrix in a row-wise fashion. Specifically, for any $n\geq 1$ and matrix $M\in\mathbb{R}^{n\times|\Acal|}$, we have
\begin{align*}
    \psi(M)_{i,j}=\frac{\exp(M_{i,j})}{\sum_{j'=1}^{|\Acal|}\exp(M_{i,j'})} \quad \forall i=1,...,n.
\end{align*}

The softmax operator $\psi$ can be inverted up to an additive constant factor. We define $\psi_{inv}$ for any matrix $M\in\mathbb{R}^{n\times|\Acal|}$ as
\[
    \psi_{inv}(M)_{i,j}=\log(M_{i,j})+c_i \quad \forall i,j,
\]
with $c_i$ determined such that $\sum_{j=1}^{|\Acal|}\psi_{inv}(M)_{i,j}=0$. Note that $\psi_{inv}$ is a right inverse of $\psi$, i.e. $\psi(\psi_{inv}(M))=M$ for all matrix $A$.

When the input to a neural network with parameter $\theta$ is the feature table $X$, we denote the output of layer $k$ by $F_k^{\theta}\in\mathbb{R}^{|\Scal|\times n_k}$. According to \eqref{eq:def_f_k}, $F_k^{\theta}$ can be expressed as
\begin{align*}
    F_k^{\theta} = 
    \left\{\begin{array}{ll}
        \sigma\left(X W_1+\1_{|\Scal|} b_{1}^{\top}\right) & k=1 \\
        \sigma\left(F_{k-1}^{\theta} W_k+\1_{|\Scal|} b_{k}^{\top}\right) & k = 2, 3,..., L-1 \\
        \psi\left(F_{L-1}^{\theta} W_{L}+\1_{|\Scal|} b_{L}^{\top}\right) & k=L
    \end{array}\right.
\end{align*}
where $\1_{|\Scal|}$ is the all-one vector of dimension $|\Scal|\times 1$. Note that $F_L^{\theta}\in\mathbb{R}^{|\Scal|\times|\Acal|}$ is the policy table produced by the neural network, i.e. $\pi_{\theta}=F_L^{\theta}$.

The proof of Theorem~\ref{thm:connected_firstlayer2S} relies on the following intermediate results, which we now present. The proof of Proposition~\ref{prop:connected_linearindepedentX} can be found in Appendix~\ref{sec:proof_prop}.




\begin{prop}\label{prop:connected_linearindepedentX}
If $\rank(X)=|\Scal|$, then under Assumption \ref{assump:ergodicity} and \ref{assump:sigma}, the superlevel set $\Ucal_{\lambda,r}^{\Omega}$ is connected for all $\lambda\in\mathbb{R}$.
\end{prop}


\begin{lem}\label{lem:connected_firstlayer2S:lem1}
Let $(X,W,b,V)\in\mathbb{R}^{|\Scal|\times n_0}\times \mathbb{R}^{n_0\times n_1}\times\mathbb{R}^{n_1}\times \mathbb{R}^{n_1\times n_2}$. Let $Z=\sigma(XW+1_{|\Scal|b^{\top}})V$. Suppose $X$ has distinct rows. Then, under Assumption \ref{assump:sigma} and \ref{assump:network_dimension}, there exists a continuous path map $c:[0,1]\rightarrow \mathbb{R}^{n_0\times n_1}\times\mathbb{R}^{n_1}\times \mathbb{R}^{n_1\times n_2}$ with $c(\lambda)=(W(\lambda), b(\lambda), V(\lambda))$ such that

1) c(0) = (W,b,V),

2) $\sigma\left(X W(\lambda)+\mathbf{1}_{|\Scal|} b(\lambda)^{T}\right) V(\lambda)=Z, \forall \lambda \in[0,1]$,

3) $\rank\left(\sigma\left(X W(1)+\mathbf{1}_{|\Scal|} b(1)^{T}\right)\right)=N$.
\end{lem}


\begin{lem}\label{lem:connected_firstlayer2S:lem2}
Let $(X,W,V,W')\in\mathbb{R}^{|\Scal|\times n_0}\times \mathbb{R}^{n_0\times n_1}\times \mathbb{R}^{n_1\times n_2}\times \mathbb{R}^{n_0\times n_1}$. Suppose $\rank(\sigma(XW))=|\Scal|$ and $\rank(\sigma(XW'))=|\Scal|$. Then, under Assumption \ref{assump:sigma} and \ref{assump:network_dimension}, there exists a continuous path map $c:[0,1]\rightarrow \mathbb{R}^{n_0\times n_1}\times \mathbb{R}^{n_1\times n_2}$ with $c(\lambda)=(W(\lambda), V(\lambda))$ such that

1) c(0) = (W,V),

2) $\sigma\left(X W(\lambda)\right) V(\lambda)=\sigma\left(X W\right)V, \forall \lambda \in[0,1]$,

3) $W(1)=W'$.
\end{lem}

To prove Theorem \ref{thm:connected_firstlayer2S}, it suffices to show that for any $\theta_1=(W_{1,l},b_{1,l})_{l=1}^{L}\in \Ucal_{\lambda,r}^{\Omega}$ and $\theta_2=(W_{2,l},b_{2,l})_{l=1}^{L}\in \Ucal_{\lambda,r}^{\Omega}$ there exists a connected path that is completely within $\Ucal_{\lambda,r}^{\Omega}$.

Applying Lemma \ref{lem:connected_firstlayer2S:lem1} with $(X,W_{1,1},b_{1,1},W_{1,2})$ and $(X,W_{2,1},b_{2,1},W_{2,2})$, the problem simplifies to showing the existence of a continuous path within $\Ucal_{\lambda,r}^{\Omega}$ that connects 
\[\theta_1'=((W_{1,1}',b_{1,1}'),(W_{1,2}',b_{1,2}),(W_{1,l},b_{1,l})_{l=3}^{L})\] 
and 
\[\theta_2'=((W_{2,1}',b_{2,1}'),(W_{2,2}',b_{1,2}),(W_{2,l},b_{2,l})_{l=3}^{L})\] 
such that
\[\rank(F_1^{\theta_1'})=\rank(F_1^{\theta_2'})=|\Scal|.\]

Then, we can apply Lemma \ref{lem:connected_firstlayer2S:lem2} with $([X,1_{|\Scal|}],[W_{1,1}'^{\top},b_{1,1}']^{\top},W_{1,2}',[W_{2,1}'^{\top},b_{2,1}']^{\top})$ to show that there is a continuous path between $\theta_1'$ and $\theta_1''$ with $\theta_1''=((W_{2,1}',b_{2,1}'),(W_{1,2}'',b_{1,2}),(W_{1,l},b_{1,l})_{l=3}^{L})$ such that
\[\rank(F_1^{\theta_1''})=\rank(F_1^{\theta_1'})=|\Scal|.\]

As a consequence, now we simply have to show that $\theta_1''$ and $\theta_2'$ is connected by a continuous path within $\Ucal_{\lambda,r}^{\Omega}$.

Note that $\theta_1''$ and $\theta_2'$ have identical first layer parameters and thus the same first layer output, which is full rank. This allows us to treat the layers from $2$ to $L$ as a new network and apply Proposition \ref{prop:connected_linearindepedentX} (which requires the input to be full rank) to the new network to guarantee that there exists a continuous path map $c:[0,1]\rightarrow \Omega_2\times...\times\Omega_k$ such that
$c(0)=((W_{1,2}'',b_{1,2}),(W_{1,l},b_{1,l})_{l=3}^{L})$, $c(1)=((W_{2,2}',b_{1,2}),(W_{2,l},b_{2,l})_{l=3}^{L})$, and
\[
    \min\{J_r(\pi_{\theta_1}),J_r(\pi_{\theta_2})\}\leq J_r(\pi_{((W_{2,1}',b_{2,1}'),c(\alpha))})\leq \max\{J_r(\pi_{\theta_1}),J_r(\pi_{\theta_2})\}
\]
for all $\alpha\in[0,1]$. This implies that there is indeed a continuous path between $\theta_1''$ and $\theta_2'$ within $\Ucal_{\lambda,r}^{\Omega}$.

Similar to the proof of Theorem~\ref{thm:connected_tabular}, the claim on the connectedness simply follows from the fact that the construction of the path map $p$ does not depend on the reward function.
\qed




