\section{Application to Robust Reinforcement Learning}\label{sec:application}

In this section, we consider the robust RL problem under adversarial reward attack, which can be formulated as a convex-nonconcave minimax optimization program. In Section~\ref{sec:minimax_theorem}, we show that the minimax equality holds for this optimization program in the tabular policy setting and under policies represented by a class of neural networks, as a consequence of our results in Sections~\ref{sec:tabular} and \ref{sec:NN}. To our best knowledge, the existence of the Nash equilibrium for this robust RL problem has not been established before even in the tabular case. A specific example of this type of robust RL problems is given in Section~\ref{sec:reward_poison}.

\subsection{Minimax Theorem}\label{sec:minimax_theorem}
Robust RL in general studies identifying a policy with reliable performance under uncertainty or attacks. A wide range of formulations have been proposed for robust RL (which we reviewed in details in Section~\ref{sec:related_works}), and an important class of formulations takes the form of defending against an adversary that can modify the reward function in a convex manner. Specifically, the objective of the learning agent can be described as solving the following minimax optimization problem
\begin{align}
\max_{\pi\in\Delta_{\Acal}^{\Scal}}\min_{r\in\Ccal}J_r(\pi),\label{eq:minimax_generalform}
\end{align}
where $\Ccal$ is some convex set. It is unclear from the existing literature whether minimax inequality holds for this problem, i.e.
\begin{align}
\max_{\pi\in\Delta_{\Acal}^{\Scal}}\min_{r\in\Ccal}J_r(\pi)=\min_{r\in\Ccal}\max_{\pi\in\Delta_{\Acal}^{\Scal}}J_r(\pi),\label{eq:minimax_equality}
\end{align}
and we provide a definitive answer to this question. We note that there exists a classic minimax theorem on a special class of convex-nonconcave functions \citep{simons1995minimax}, which we adapt and simplify as follows. 
\begin{thm}\label{thm:minimax_simplified}
Consider a separately continuous function $f:\Xcal\times \Ycal\rightarrow\mathbb{R}$, with $\Ycal$ being a convex, compact set. 
Suppose that $f(x,\cdot)$ is convex for all $x\in\Xcal$. Also suppose that the collection of functions $\{f(\cdot,y)\}_{y\in\Ycal}$ is equiconnected. Then, we have
\begin{align}
    \sup_{x \in \Xcal} \min_{y\in \Ycal} f(x,y)=\min_{y\in \Ycal} \sup_{x \in \Xcal} f(x,y).
\end{align}
\end{thm}
Theorem~\ref{thm:minimax_simplified} states that the minimax equality holds under two main conditions (besides the continuity condition, which can easily be verified to hold for $J_{r}(\pi)$). First, the function $f(x,y)$ needs to be convex with respect to the variable $y$ within a convex, compact constraint set. Second, $f(x,y)$ needs to have a connected superlevel set with respect to $x$, and the path function constructed to prove the connectedness of the superlevel set is independent of $y$. 
% 
As we have shown in this section and earlier in the paper, if we model $J_r(\pi)$ by $f(x,y)$ with $\pi$ and $r$ corresponding to $x$ and $y$, both conditions are observed by the optimization problem \eqref{eq:minimax_generalform}, which allows us to state the following corollary.
\begin{cor}\label{cor:minimax_robustrl_tabular}
Suppose that the Markov chain $\Mcal$ satisfies Assumption~\ref{assump:ergodicity} on ergodicity. Then, the minimax equality \eqref{eq:minimax_equality} holds.
\end{cor}

When the neural network presented in Section~\ref{sec:NN} is used to represent the policy, the collection of functions $\{J_{r,\Omega}\}_{r}$ is also equiconnected. This allows us to extend the minimax equality above to the neural network policy class. Specifically, consider problem \eqref{eq:minimax_generalform} where the policy $\pi_{\theta}$ is represented by the parameter $\theta\in\Omega$ as described in Section~\ref{sec:NN}. 
Using $f(x,y)$ to model $J_r(\pi_{\theta})$ with $x$ and $y$ mirroring $\theta$ and $r$, we can easily establish the minimax theorem in this case as a consequence of Theorem~\ref{thm:connected_firstlayer2S} and \ref{thm:minimax_simplified}.\looseness=-1

\begin{cor}\label{cor:minimax_robustrl_NN}
Suppose that the Markov chain $\Mcal$ satisfies Assumption \ref{assump:ergodicity} on ergodicity and that the neural policy class satisfies Assumptions~\ref{assump:sigma}-\ref{assump:network_dimension}. Then, we have
\begin{align}
\sup_{\theta\in\Omega}\min_{r\in\Ccal} J_{r}(\pi_{\theta}) = \min_{r\in\Ccal} \sup_{\theta\in\Omega}J_{r}(\pi_{\theta}).
\end{align}
\end{cor}

Corollary~\ref{cor:minimax_robustrl_tabular} and \ref{cor:minimax_robustrl_NN} establish the minimax equality (or equivalently, the existence of the Nash equilibrium) for the robust reinforcement learning problem under adversarial reward attack for the tabular and neural network policy class, respectively. To our best knowledge, these results are both novel and previously unknown in the existing literature. The Nash equilibrium is an important global optimality notion in minimax optimization, and the knowledge on its existence can provide strong guidance on designing and analyzing algorithms for solving the problem.




\subsection{Example - Defense Against Reward Poisoning}\label{sec:reward_poison}

We now discuss a particular example of \eqref{eq:minimax_generalform}.
We consider the infinite horizon, average reward MDP $\Mcal=(\Scal,\Acal,\Pcal,r)$ introduced in Section~\ref{sec:tabular}, where $r$ is the true, unpoisoned reward function. 
Let $\Pi^{\text{det}}$ denote the set of deterministic policies from $\Scal$ to $\Acal$. With the perfect knowledge of this MDP, an attacker has a target policy $\pi_{\dagger}\in\Pi^{\text{det}}$ and tries to make the learning agent adopt the policy by manipulating the reward function. 
Mathematically, the goal of the attacker can be described by the function $\operatorname{Attack}(r,\pi_{\dagger},\epsilon_{\dagger})$ which returns a poisoned reward under the true reward $r$, the target policy $\pi_{\dagger}$, and a pre-selected margin parameter $\epsilon_{\dagger}\geq0$. $\operatorname{Attack}(r,\pi_{\dagger},\epsilon_{\dagger})$ is the solution to the following optimization problem
\begin{align}
    \begin{aligned}
    \operatorname{Attack}(r,\pi_{\dagger},\epsilon_{\dagger})\quad=\quad\argmin_{r'}\quad&\sum_{s\in\Scal,a\in\Acal}\left(r'(s,a)-r(s,a)\right)^2\\
    \operatorname{s.t.}\quad& J_{r'}(\pi_{\dagger})\geq J_{r'}(\pi)+\epsilon_{\dagger},\quad\forall \pi\in\Pi^{\text{det}}\backslash\pi_{\dagger}.
    \end{aligned}\label{eq:attack_obj}
\end{align}
In other words, the attacker needs to minimally modify the reward function to make $\pi_{\dagger}$ the optimal policy under the poisoned reward. This optimization program minimizes a quadratic loss under a finite number of linear constraints and is obviously convex.

The learning agent observes the poisoned reward $r_{\dagger}=\operatorname{Attack}(r,\pi_{\dagger},\epsilon_{\dagger})$ rather than the original reward $r$. As noted in \citet{banihashem2021defense}, without any defense, the learning agent solves the policy optimization problem under $r_{\dagger}$ to find $\pi_{\dagger}$, which may perform arbitrarily badly under the original reward. One way to defend against the attack is to maximize the performance of the agent in the worst possible case of the original reward, which leads to solving a minimax optimization program of the form
\begin{align}
    \max_{\pi\in\Delta_{\Acal}^{\Scal}}\min_{r'} J_{r'}(\pi)\quad\operatorname{s.t.}\,\, \operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}.\label{eq:robustrl_obj}
\end{align}
When the policy $\pi$ is fixed, \eqref{eq:robustrl_obj} reduces to 
\begin{align}
    \min_{r'} J_{r'}(\pi)\quad\operatorname{s.t.}\,\, \operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}.\label{eq:robustrl_convexside}
\end{align}
With the justification deferred to Appendix~\ref{sec:robustrl_convexside_proof}, we point out that \eqref{eq:robustrl_convexside} consists of a linear objective function and a convex (and compact) constraint set, and is therefore a convex program. On the other side, when we fix the reward $r'$, \eqref{eq:robustrl_obj} reduces to a standard policy optimization problem.

We are interested in investigating whether the following minimax equality holds.
\begin{align}
    \max_{\pi\in\Delta_{\Acal}^{\Scal}}\min_{r':\operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}} J_{r'}(\pi) = \min_{r':\operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}} \max_{\pi\in\Delta_{\Acal}^{\Scal}}J_{r'}(\pi).
    \label{eq:robustrl_minimax}
\end{align}
This is a special case of \eqref{eq:minimax_generalform} with $\Ccal=\{r'\mid \operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}\}$, which can be verified to be a convex set. Therefore, the validity of \eqref{eq:robustrl_minimax} directly follows from Corollary~\ref{cor:minimax_robustrl_tabular}. Similarly, in the setting of neural network parameterized policy we can establish
\begin{align*}
\max_{\theta\in\Omega}\min_{r':\operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}} J_{r'}(\pi_{\theta}) = \min_{r':\operatorname{Attack}(r',\pi_{\dagger},\epsilon_{\dagger})=r_{\dagger}} \max_{\theta\in\Omega}J_{r'}(\pi_{\theta})
\end{align*}
as a result of Corollary~\ref{cor:minimax_robustrl_NN}.