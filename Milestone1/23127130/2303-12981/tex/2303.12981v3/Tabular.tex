\section{Connected Superlevel Set Under Tabular Policy}\label{sec:tabular}

We consider the infinite-horizon average-reward MDP characterized by $\Mcal=(\Scal,\Acal,\Pcal,r)$. We use $\Scal$ and $\Acal$ to denote the state and action spaces, which we assume are finite. The transition probability kernel is denoted by $\Pcal:\Scal\times\Acal\rightarrow \Delta_{\Scal}$, where $\Delta_{\Scal}$ denotes the probability simplex over $\Scal$. The reward function $r:\Scal\times\Acal\rightarrow[0,U_r]$ is bounded for some positive constant $U_r$ and can also be regarded as a vector in $\mathbb{R}^{|\Scal|\times|\Acal|}$. We use $P^{\pi}\in\mathbb{R}^{\Scal\times\Scal}$ to represent the state transition probability matrix under policy $\pi\in\Delta_{\Acal}^{\Scal}$, where $\Delta_{\Acal}^{\Scal}$ is the collection of probability simplexes over $\Acal$ across the state space
\begin{align}
    P^{\pi}_{s',s}=\sum_{a\in\Acal}\Pcal(s'\mid s,a)\pi(a\mid s),\quad\forall s',s\in\Scal.\label{eq:transition_matrix}
\end{align}
% $\mu_{\pi}\in\Delta_{\Scal}$ denotes the stationary distribution of the states induced by policy $\pi$. It is well-known that $\mu_{\pi}$ is an eigenvector of $P^{\pi}$ with the associated eigenvalue equal to $1$, i.e. $\mu_{\pi}=P^{\pi}\mu_{\pi}$. 

We consider the following ergodicity assumption in the rest of the paper, which is commonly made in the RL literature \citep{wang2017primal,wei2020model,wu2020finite}.
\begin{assump}\label{assump:ergodicity}
Given any policy $\pi$, the Markov chain formed under the transition probability matrix $P^{\pi}$ is ergodic, i.e. irreducible and aperiodic.
\end{assump}
Let $\mu_{\pi}\in\Delta_{\Scal}$ denote the stationary distribution of the states induced by policy $\pi$. As a consequence of Assumption~\ref{assump:ergodicity}, the stationary distribution $\mu_{\pi}$ is unique and uniformly bounded away from $0$ under any $\pi$. In addition, $\mu_{\pi}$ is the unique eigenvector of $P^{\pi}$ with the associated eigenvalue equal to $1$, i.e. $\mu_{\pi}=P^{\pi}\mu_{\pi}$. 
Let $\widehat{\mu}_{\pi}\in\Delta_{\Scal\times\Acal}$ denote the state-action stationary distribution induced by $\pi$, which can be expressed as
\begin{align}
    \widehat{\mu}_{\pi}(s,a)=\mu_{\pi}(s)\pi(a\mid s).\label{eq:mu_hat}
\end{align}

We measure the performance of a policy $\pi$ under reward function $r$ by the average cumulative reward $J_r(\pi)$
\begin{align*}
    J_r(\pi)\triangleq\lim_{K \rightarrow \infty} \frac{\sum_{k=0}^{K} r(s_k, a_k)}{K}=\mathbb{E}_{s\sim\mu_{\pi}, a\sim \pi}[r(s_k,a_k)]=\sum_{s,a}r(s,a)\widehat{\mu}_{\pi}(s,a).
\end{align*}

The objective of the policy optimization problem is to find the policy $\pi$ that maximizes the average cumulative reward
\begin{align}
    \max_{\pi\in\Delta_{\Acal}^{\Scal}}J_r(\pi).\label{eq:obj}
\end{align}

The superlevel set of $J_r$ is the set of policies that achieve a value function greater than or equal to a specified level. Formally, given $\lambda\in\mathbb{R}$, the $\lambda$-superlevel set (or superlevel set) under reward $r$ is defined as
\[
    \Ucal_{\lambda,r}\triangleq\{\pi\in\Delta_{\Acal}^{\Scal}\mid J_r(\pi)\geq \lambda\}.
\]

The main focus of this section is to study the connectedness of this set $\Ucal_{\lambda,r}$, which requires us to formally define a connected set.
\begin{definition}\label{def:connectedset}
A set $\Ucal$ is connected if for any $x,y\in\Ucal$ there exists a continuous map $p:[0,1]\rightarrow\Ucal$ such that $p(0)=x$ and $p(1)=y$.
\end{definition}
We say that a function is connected if its superlevel sets are connected at all levels. We also introduce the definition of equiconnected functions. 
\begin{definition}\label{def:equiconnectedfunc}
Given two spaces $\Xcal$ and $\Ycal$, the collection of functions $\{f_y:\Xcal\rightarrow\mathbb{R}\}_{y\in\Ycal}$ is said to be equiconnected if for every $x_1,x_2\in\Xcal$, there exists a continuous path map $p:[0,1]\rightarrow\Xcal$ such that
\begin{align*}
    p(0)=x_1,\quad p(1)=x_2,\quad f_y(p(\alpha))\geq\min\{f_y(x_1),f_y(x_2)\},
\end{align*}
for all $\alpha\in[0,1]$ and $y\in\Ycal$.
\end{definition}
Conceptually, the collection of functions $\{f_y:\Xcal\rightarrow\mathbb{R}\}_{y\in\Ycal}$ being equiconnected requires 1) that $f_y(\cdot)$ is a connected function for all $y\in\Ycal$ (or equivalently, the set $\{x\in\Xcal:f_y(x)\geq\lambda\}$ is connected for all $\lambda\in\mathbb{R}$ and $y\in\Ycal$) and 2) that the path map constructed to prove the connectedness of $\{x\in\Xcal:f_y(x)\geq\lambda\}$ is independent of $y$.


We now present our first main result of the paper, which states that the superlevel set $\Ucal_{\lambda,r}$ is always connected.
\begin{thm}\label{thm:connected_tabular}
    Under Assumption \ref{assump:ergodicity}, the superlevel set $\Ucal_{\lambda,r}$ is connected for any $\lambda\in\mathbb{R}$ and $r\in\mathbb{R}^{|\Scal||\Acal|}$.
    In addition, the collection of functions $\{J_r(\cdot):\Delta_{\Acal}^{\Scal}\rightarrow\mathbb{R}\}_{r\in\mathbb{R}^{|\Scal|\times|\Acal|}}$ is equiconnected.
\end{thm}


Our result here extends easily to the infinite-horizon discounted-reward setting since a discounted-reward MDP can be regarded as an average-reward one with a slightly modified transition kernel \citep{kondathesis}.


The claim in Theorem~\ref{thm:connected_tabular} on the equiconnectedness of $\{J_r\}_{r\in\mathbb{R}^{|\Scal|\times|\Acal|}}$ is a slightly stronger result than the connectedness of $\Ucal_{\lambda,r}$, and plays an important role in the application to minimax theorems discussed later in Section~\ref{sec:application}.


We note that the proof, presented in Section~\ref{sec:thm:connected_tabular} of the appendix, mainly leverages the fact that the value function $J_r(\pi)$ is linear in the state-action stationary distribution $\widehat{\mu}_{\pi}$ and that there is a special connection (though nonlinear and nonconvex) between $\widehat{\mu}_{\pi}$ and the policy $\pi$, which we take advantage of to construct the continuous path map for the analysis. Specifically, given two policies $\pi_1,\pi_2$ with $J_r(\pi_1),J_r(\pi_2)\geq \lambda$, we show that the policy $\pi_{\alpha}$ defined as
\begin{align*}
    \pi_{\alpha}(a\mid s) = \frac{\alpha \mu_{\pi_1}(s)\pi_1(a\mid s)+(1-\alpha)\mu_{\pi_2}(s)\pi_2(a\mid s)}{\alpha \mu_{\pi_1}(s)+(1-\alpha)\mu_{\pi_2}(s)},\quad\forall \alpha\in[0,1]
\end{align*}
is guaranteed to achieve $J_r(\pi_{\alpha})\geq\lambda$ for all $\alpha\in[0,1]$. 

Besides playing a key role in the proof of Theorem~\ref{thm:connected_tabular}, our construction of this path map may inform the design of algorithms in the future.
Given any two policies with a certain guaranteed performance, we can generate a continuum of policies at least as good. As a consequence, if we find two optimal policies (possibly by gradient descent from different initializations) we can generate a range of interpolating optimal policies. If the agent has a preference over these policy (for example, to minimize certain energy like in $H_1$ control, or if some policies are easier to implement physically), then the selection can be made on the continuum of optimal policies, which eventually leads to a more preferred policy.
