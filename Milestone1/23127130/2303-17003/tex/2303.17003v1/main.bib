
@misc{VaswaniAttentionAllYou2017a,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-12-07},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{https://doi.org/10.48550/arxiv.1907.05338,
  doi = {10.48550/ARXIV.1907.05338},
  url = {https://arxiv.org/abs/1907.05338},
  author = {Wang, Ran and Su, Haibo and Wang, Chunye and Ji, Kailin and Ding, Jupeng},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {To Tune or Not To Tune? How About the Best of Both Worlds?},
  publisher = {arXiv},
  year = {2019},
  copyright = {Creative Commons Zero v1.0 Universal}
}


@article{kuvcak2018machine,
  title={MACHINE LEARNING IN EDUCATION-A SURVEY OF CURRENT RESEARCH TRENDS.},
  author={Ku{\v{c}}ak, Danijel and Juri{\v{c}}i{\'c}, Vedran and {\DJ}ambi{\'c}, Goran},
  journal={Annals of DAAAM \& Proceedings},
  volume={29},
  year={2018}
}

@article{luan2021review,
  title={A review of using machine learning approaches for precision education},
  author={Luan, Hui and Tsai, Chin-Chung},
  journal={Educational Technology \& Society},
  volume={24},
  number={1},
  pages={250--266},
  year={2021},
  publisher={JSTOR}
}

@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@misc{rae2022scaling,
      title={Scaling Language Models: Methods, Analysis \& Insights from Training Gopher}, 
      author={Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John Mellor and Irina Higgins and Antonia Creswell and Nat McAleese and Amy Wu and Erich Elsen and Siddhant Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and Laurent Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and Nikolai Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Toby Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew Johnson and Blake Hechtman and Laura Weidinger and Iason Gabriel and William Isaac and Ed Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem Ayoub and Jeff Stanway and Lorrayne Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},
      year={2022},
      eprint={2112.11446},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@misc{hoffmann2022training,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@INPROCEEDINGS {silveira2018enem,
author = {I. Cataneo Silveira and D. Deratani Maua},
booktitle = {2018 7th Brazilian Conference on Intelligent Systems (BRACIS)},
title = {Advances in Automatically Solving the ENEM},
year = {2018},
volume = {},
issn = {},
pages = {43-48},
abstract = {Answering questions formulated in natural language is a long standing quest in Artificial Intelligence. However, even formulating the problem in precise terms has proven to be too challenging, which lead many researchers to focus on Multiple-Choice Question Answering problems. One particularly interesting type of the latter problem is solving standardized tests such as university entrance exams. The Exame Nacional do Ensino Médio (ENEM) is a High School level exam widely used by Brazilian universities as entrance exam, and the world's second biggest university entrance examination in number of registered candidates. In this work we tackle the problem of answering purely textual multiple-choice questions from the ENEM. We build on a previous solution that formulated the problem as a text information retrieval problem. In particular, we investigate how to enhance these methods by text augmentation using Word Embedding and WordNet, a structured lexical database where words are connected according to some relations like synonymy and hypernymy. We also investigate how to boost performance by building ensembles of weakly correlated solvers. Our approaches obtain accuracies ranging from 26% to 29.3%, outperforming the previous approach.},
keywords = {knowledge based systems;information retrieval;encyclopedias;electronic publishing;internet;mathematics},
doi = {10.1109/BRACIS.2018.00016},
url = {https://doi.ieeecomputersociety.org/10.1109/BRACIS.2018.00016},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {oct}
}

@article{DBLP:journals/corr/abs-2201-11903,
  author    = {Jason Wei and
               Xuezhi Wang and
               Dale Schuurmans and
               Maarten Bosma and
               Ed H. Chi and
               Quoc Le and
               Denny Zhou},
  title     = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  journal   = {CoRR},
  volume    = {abs/2201.11903},
  year      = {2022},
  url       = {https://arxiv.org/abs/2201.11903},
  eprinttype = {arXiv},
  eprint    = {2201.11903},
  timestamp = {Fri, 22 Apr 2022 16:06:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2201-11903.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{MikolovEfficientEstimationWord2013a,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013-09-06},
  number = {arXiv:1301.3781},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1301.3781},
  url = {http://arxiv.org/abs/1301.3781},
  urldate = {2022-09-21},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}


@article{bommarito2022gpt,
  title={GPT Takes the Bar Exam},
  author={Bommarito, Michael James and Katz, Daniel Martin},
  journal={Available at SSRN 4314839},
  year={2022}
}

@article{Bommarito2023Gpt,
	author = {Bommarito, Jillian and Bommarito, Michael James and Katz, Jessica and Katz, Daniel Martin},
	journal = {SSRN Electronic Journal},
	year = {2023},
	publisher = {Elsevier BV},
	title = {Gpt as {Knowledge} {Worker}: A {Zero}-{Shot} {Evaluation} of ({AI}){CPA} {Capabilities}},
	url = {http://dx.doi.org/10.2139/ssrn.4322372},
	doi = {10.2139/ssrn.4322372},
	abstract = {},
}

@inproceedings{Si2022BenchmarkingGF,
  title={Benchmarking GPT-3 For Closed-Book QA: Strengths and Weaknesses},
  author={Chenglei Si and Naman Molri and Gurmehar Cheema and Elliot Huang and Arjun Akkiraju},
  year={2022}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{singhal2022large,
  title={Large Language Models Encode Clinical Knowledge},
  author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others},
  journal={arXiv preprint arXiv:2212.13138},
  year={2022}
}

@article{miller1995wordnet,
author = {Miller, George A.},
title = {WordNet: A Lexical Database for English},
year = {1995},
issue_date = {Nov. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/219717.219748},
doi = {10.1145/219717.219748},
abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
journal = {Commun. ACM},
month = {nov},
pages = {39–41},
numpages = {3}
}

@misc{ahuja2023mega,
      title={MEGA: Multilingual Evaluation of Generative AI}, 
      author={Kabir Ahuja and Rishav Hada and Millicent Ochieng and Prachi Jain and Harshita Diddee and Samuel Maina and Tanuja Ganu and Sameer Segal and Maxamed Axmed and Kalika Bali and Sunayana Sitaram},
      year={2023},
      eprint={2303.12528},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{nori2023capabilities,
      title={Capabilities of GPT-4 on Medical Challenge Problems}, 
      author={Harsha Nori and Nicholas King and Scott Mayer McKinney and Dean Carignan and Eric Horvitz},
      year={2023},
      eprint={2303.13375},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{BERTIN-GPT,
  author        = {Javier De la Rosa and Andres Fernández},
  editor        = {Manuel Montes-y-Gómez and Julio Gonzalo and Francisco Rangel and Marco Casavantes and Miguel Ángel Álvarez-Carmona and Gemma Bel-Enguix and Hugo Jair Escalante and Larissa Freitas and Antonio Miranda-Escalada and Francisco Rodríguez-Sánchez and Aiala Rosá and Marco Antonio Sobrevilla-Cabezudo and Mariona Taulé and Rafael Valencia-García},
  title         = {Zero-shot Reading Comprehension and Reasoning for Spanish with {BERTIN} {GPT-J-6B}},
  date          = {2022-09},
  booktitle     = {Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2022)},
  booktitleaddon = {Co-located with the Conference of the Spanish Society for Natural Language Processing (SEPLN 2022)},
  eventdate     = {2022-09-20/2022-09-25},
  venue         = {A Coru\~{n}a, Spain},
  year = {2022},
  publisher     = {CEUR Workshop Proceedings},
}

@misc{zeng2022glm130b,
      title={GLM-130B: An Open Bilingual Pre-trained Model}, 
      author={Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and Peng Zhang and Yuxiao Dong and Jie Tang},
      year={2022},
      eprint={2210.02414},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{xue2021mt5,
  title={mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer},
  author={Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={483--498},
  year={2021}
}

@article{xue2022byt5,
  title={ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models},
  author={Xue, Linting and Barua, Aditya and Constant, Noah and Al-Rfou, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={291--306},
  year={2022}
}

@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@misc{muennighoff2022crosslingual,
      title={Crosslingual Generalization through Multitask Finetuning}, 
      author={Niklas Muennighoff and Thomas Wang and Lintang Sutawika and Adam Roberts and Stella Biderman and Teven Le Scao and M Saiful Bari and Sheng Shen and Zheng-Xin Yong and Hailey Schoelkopf and Xiangru Tang and Dragomir Radev and Alham Fikri Aji and Khalid Almubarak and Samuel Albanie and Zaid Alyafeai and Albert Webson and Edward Raff and Colin Raffel},
      year={2022},
      eprint={2211.01786},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}