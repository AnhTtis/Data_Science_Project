\section*{Appendix}
\addcontentsline{toc}{section}{Appendix}
% printing the subsection counter in capital alphabetic form
\renewcommand{\thesubsection}{\Alph{subsection}}

\newtheorem{appdxlemma}{Lemma}
\numberwithin{appdxlemma}{subsection} % important bit
\numberwithin{equation}{subsection} % numbering of each equation to be of the form ([Appendix section index].[index of equation])

\subsection{Proof of Lemma~\ref{le:bias}}\label{pf:bias}

By the tower property $\Tilde{\mathcal{F}}_k \coloneqq \sigma\{\mathcal{F}_k \cup \sigma\{u_{k,0}\}\} \supseteq \mathcal{F}_k$ and the linearity of conditional expectation, we have
\begin{align*}
& \expt{}{G^i_k \mid \mathcal{F}_k} = \Bexpt{}{\expt{}{G^i_k \mid \Tilde{\mathcal{F}}_k}\mid \mathcal{F}_k} \\
& = \frac{1}{T_k} \sum_{t=1}^{T_k}\Bexpt{}{\frac{n^i}{\delta_k}\expt{}{\Big(J^i(\hat{X}_{k+1/2,t}) - J^i(\hat{X}_{k+1/2,0})\Big)u^i_{k,t} \mid \Tilde{\mathcal{F}}_k} \mid \mathcal{F}_k}. 
\end{align*}
For every $t \in \{1, \ldots, T_k\}$, it follows from Lemma~1 of \cite{huang2023zeroth} that $\nabla_{x^i} \Tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2})$ is a version of $\frac{n^i}{\delta_k}\expt{}{(J^i(\hat{X}_{k+1/2,t}) - J^i(\hat{X}_{k+1/2,0}))u^i_{k,t} \mid \Tilde{\mathcal{F}}_k}$. 
Based on the fact that $\bar{X}_{k+1/2} \in \mathcal{F}_k$, we have the following relation holds a.s.:
\begin{align*}
& \expt{}{G^i_k \mid \mathcal{F}_k} = \frac{1}{T_k} \sum_{t=1}^{T_k}\expt{}{\nabla_{x^i} \Tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2}) \mid \mathcal{F}_k} = \nabla_{x^i} \Tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2}). 
\end{align*}
With the above results in hand, the norm of systematic error $\norm{B^i_k}_{\mathcal{H}^i}$ can be reformulated as $\norm{B^i_k}_{\mathcal{H}^i} = \norm{\nabla_{x^i}\tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2}) - \nabla_{x^i}J^i(X_{k+1/2})}_{\mathcal{H}^i}$, and the proof for Lemma~2 of \cite{huang2023zeroth} directly carries over. 


\subsection{Proof of Lemma~\ref{le:variance}}\label{pf:variance}
Using the definition of \eqref{eq:mpg} and the linearity of conditional expectation, we have:
\begin{align*}
& \expt{}{\norm{G^i_k}^2_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_k} = \Big(\frac{n^i}{\delta_k T_k}\Big)^2 \Bexpt{}{\Bnorm{\sum_{t=1}^{T_k} (\hat{J}^i_{k,t} - \hat{J}^i_{k,0})u^i_{k,t}}^2_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_k} \\
& = \Big(\frac{n^i}{\delta_k T_k}\Big)^2 \Big(\sum_{t=1}^{T_k}\expt{}{\norm{ (\hat{J}^i_{k,t} - \hat{J}^i_{k,0})u^i_{k,t}}^2_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_k} + \sum_{1 \leq s, t \leq T_k, s\neq t} \\
& \expt{}{(\hat{J}^i_{k,s} - \hat{J}^i_{k,0})(\hat{J}^i_{k,t} - \hat{J}^i_{k,0}) \cdot \langle u^i_{k,s}, u^i_{k,t}\rangle_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_k}\Big). 
\end{align*}
For each pair $(s, t)$ with $s \neq t$, denote $\Tilde{\mathcal{F}}_{k,s} \coloneqq \sigma\{\Tilde{\mathcal{F}}_k\cup \sigma\{u_{k,s}\}\}$ and the conditional expectation of the inner product can be reformulated as follows:
(TODO: maybe more arguments here are needed to assert that we can take the measurable part outside the general inner product)
\begin{align*}
& \Big(\frac{n^i}{\delta_k}\Big)^2\expt{}{\langle (\hat{J}^i_{k,s} - \hat{J}^i_{k,0})u^i_{k,s}, (\hat{J}^i_{k,t} - \hat{J}^i_{k,0})u^i_{k,t}\rangle_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_k} \\
& = \Bexpt{}{\expt{}{\langle \frac{n^i}{\delta_k}(\hat{J}^i_{k,s} - \hat{J}^i_{k,0})u^i_{k,s}, \frac{n^i}{\delta_k}(\hat{J}^i_{k,t} - \hat{J}^i_{k,0})u^i_{k,t}\rangle_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_{k,s}} \mid \Tilde{\mathcal{F}}_k} \\
& = \Bexpt{}{\langle \frac{n^i}{\delta_k}(\hat{J}^i_{k,s} - \hat{J}^i_{k,0})u^i_{k,s}, \expt{}{\frac{n^i}{\delta_k}(\hat{J}^i_{k,t} - \hat{J}^i_{k,0})u^i_{k,t}\mid \Tilde{\mathcal{F}}_{k,s}}\rangle_{\mathcal{H}^i}\mid \Tilde{\mathcal{F}}_k} \\
& = \Bexpt{}{\langle \frac{n^i}{\delta_k}(\hat{J}^i_{k,s} - \hat{J}^i_{k,0})u^i_{k,s}, \nabla_{x^i} \Tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2})\rangle_{\mathcal{H}^i}\mid \Tilde{\mathcal{F}}_k} \\
& = \langle \expt{}{\frac{n^i}{\delta_k}(\hat{J}^i_{k,s} - \hat{J}^i_{k,0})u^i_{k,s}\mid \Tilde{\mathcal{F}}_k}, \nabla_{x^i} \Tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2})\rangle_{\mathcal{H}^i} \\
& = \langle \nabla_{x^i} \Tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2}), \nabla_{x^i} \Tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2})\rangle_{\mathcal{H}^i} = \norm{\nabla_{x^i} \Tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2})}^2_{\mathcal{H}^i} \;\text{a.s.}
\end{align*}
Combining the observations above yields: 
\begin{align*}
& \expt{}{\norm{G^i_k}^2_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_k} = \Big(\frac{n^i}{\delta_k T_k}\Big)^2 \sum_{t=1}^{T_k}\expt{}{\norm{ (\hat{J}^i_{k,t} - \hat{J}^i_{k,0})u^i_{k,t}}^2_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_k} \\
& \qquad + (1 - \frac{1}{T_k})\norm{\nabla_{x^i} \Tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2})}^2_{\mathcal{H}^i}, \; \text{a.s.}
\end{align*}
For the stochastic error $V^i_k \coloneqq G^i_k - \expt{}{G^i_k \mid \mathcal{F}_k} = G^i_k - \nabla_{x^i} \Tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2})$, applying the results for $\expt{}{\norm{G^i_k}^2_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_k}$ gives:
\begin{align*}
& \expt{}{\norm{V^i_k}^2_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_k} = \expt{}{\norm{G^i_k - \nabla_{x^i} \Tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2})}^2_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_k} \\
& = \expt{}{\norm{G^i_k}^2_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_k} - 2\expt{}{\langle G^i_k, \nabla_{x^i} \Tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2})\rangle_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_k} \\
& \qquad + \norm{\nabla_{x^i} \Tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2})}^2_{\mathcal{H}^i} \\
& = \expt{}{\norm{G^i_k}^2_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_k} - \norm{\nabla_{x^i} \Tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2})}^2_{\mathcal{H}^i} \\
& = \Big(\frac{n^i}{\delta_k T_k}\Big)^2 \sum_{t=1}^{T_k}\expt{}{\norm{ (\hat{J}^i_{k,t} - \hat{J}^i_{k,0})u^i_{k,t}}^2_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_k} \\
& \qquad - \frac{1}{T_k}\norm{\nabla_{x^i} \Tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2})}^2_{\mathcal{H}^i} \\
& \leq \Big(\frac{n^i}{\delta_k}\Big)^2\frac{1}{T_k}\expt{}{(\hat{J}^i_{k,t} - \hat{J}^i_{k,0})^2\norm{ u^i_{k,t}}^2_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_k} \;\text{a.s.}
\end{align*}
The difference $(\hat{J}^i_{k,t} - \hat{J}^i_{k,0})^2$ can be further upper bounded as: 
\begin{align}\label{eq:var-eq1}
\begin{split}
& (\hat{J}^i_{k,t} - \hat{J}^i_{k,0})^2 \overset{(a)}{=} (\langle \nabla_x J^i(Z), \hat{X}_{k+1/2,t} - \hat{X}_{k+1/2,0}\rangle)^2 \\
& \leq \norm{\nabla_x J^i(Z)}^2_2 \cdot \norm{\hat{X}_{k+1/2,t} - \hat{X}_{k+1/2,0}}^2_2 \\
& \overset{(b)}{\leq} \bar{\nabla}^2_i \cdot \delta_k^2 \norm{u_{k,t} - u_{k,0}}^2_2 = 4N\bar{\nabla}^2_i\delta_k^2, 
\end{split}
\end{align}
where, in $(a)$, we apply the mean value theorem for differentiable function and let $Z$ denote some convex combination of $\hat{X}_{k+1/2,t}$ and $\hat{X}_{k+1/2,0}$; 
for the relation $(b)$ we let $\bar{\nabla}_i \coloneqq \max_{x \in \mathcal{X}} \norm{\nabla_{x}J^i(x)}_2$ and apply the definition in \eqref{eq:perb}. 
Let $\bar{u}^i \coloneqq \norm{u^i_{k,t}}_{\mathcal{H}^i}$. 
Consequently, it can be directly inferred that: 
\begin{align*}
& \expt{}{\norm{V^i_k}^2_{\mathcal{H}^i} \mid \Tilde{\mathcal{F}}_k} \leq 4N(\bar{\nabla}_in^i)^2\bar{u}^i/T_k, \\
& \expt{}{\norm{V_k}^2_{\mathcal{H}} \mid \Tilde{\mathcal{F}}_k} \leq 4N\sum_{i\in \mathcal{N}\bar{u}^i}(\bar{\nabla}_in^i)^2/T_k. 
\end{align*}

% \subsection{Proof of Lemma~\ref{le:pgrad-bdd}}\label{pf:pgrad-bdd}
% From the definition \eqref{eq:mpg}, observe that
% \begin{align*}
% & \norm{G^i_k}_{\mathcal{H}^i} \leq \frac{n^i}{\delta_kT_k}\sum_{t=1}^{T_k}\abs{J^i(\hat{X}_{k+1/2,t}) - J^i(\hat{X}_{k+1/2,0})} \norm{u^i_{k,t}}_{\mathcal{H}^i} \\
% & \overset{(a)}{\leq} \frac{n^i}{\delta_kT_k} \cdot T_k 2\sqrt{N}\bar{\nabla}_*\delta_k \bar{u}^* = 2\sqrt{N}n^i\bar{\nabla}_*\bar{u}^*, 
% \end{align*}
% where $(a)$ is a consequence of \eqref{eq:var-eq1} by additionally setting $\bar{\nabla}_* \coloneqq \max_{i \in \mathcal{N}} \bar{\nabla}_i$ and $\bar{u}^* \coloneqq \max_{i \in \mathcal{N}} \big(\max_{u \in \mathcal{H}^i, \norm{u}_2 = 1} \norm{u}_{\mathcal{H}^i}\big)$.
% Thus, $\norm{G_k}_{\mathcal{H}} = \sum_{i \in \mathcal{N}}\norm{G^i_k}_{\mathcal{H}^i} \leq 2N^{3/2}\bar{\nabla}_*\bar{u}^* = \bar{g}$, and the proof is complete. 

\subsection{Proof of Theorem~\ref{thm:ext-rs}}\label{pf:ext-rs}
Before proceeding, we attribute the proving technique leveraged below to that of \cite[Thm.~2.3.5]{gadat2017stochastic}, while we provide complete proof for a simplified version and fill out some omitted steps of the reference for the completeness of this work. 
By letting $\hat{\zeta}_k \coloneqq \sum_{t=2}^k \zeta_t$ for $k \geq 2$ and $\hat{\zeta}_1 = 0$, the recurrent inequality can be expressed as
\begin{align}\label{eq:recur-1}
\expt{}{Z_{k+1} + \hat{\zeta}_{k+1} \mid \mathcal{F}_k} \leq  Z_{k} + \hat{\zeta}_{k} + \xi_k, \forall k \in \nset{}{+}. 
\end{align}
Likewise, let $\hat{\xi}_k \coloneqq \sum_{t=2}^k \xi_t$ for $k \geq 2$ and $\hat{\xi}_1 = 0$, and we have $0 \leq \hat{\xi}_k \nearrow \hat{\xi}_\infty$. 
It follows from the monotone convergence theorem that $\expt{}{\hat{\xi}_k} \nearrow \expt{}{\hat{\xi}_{\infty}}$ and $\sum_{k \in \nset{}{+}} \expt{}{\xi_k} < \infty$ implies $\expt{}{\hat{\xi}_{\infty}} < \infty$. 
Through the integration of this definition into \eqref{eq:recur-1}, we can construct a new recurrent inequality as follows: 
\begin{align}
\begin{split}
& \expt{}{Z_{k+1} + \hat{\zeta}_{k+1} + \expt{}{\hat{\xi}_{\infty} \mid \mathcal{F}_{k+1}} - \hat{\xi}_{k+1}\mid \mathcal{F}_k} \\
& \qquad \leq Z_{k} + \hat{\zeta}_{k} + \expt{}{\hat{\xi}_{\infty}\mid \mathcal{F}_k} - \hat{\xi}_{k}.
\end{split}
\end{align}
Based on the observation that $\hat{\xi}_{\infty} - \hat{\xi}_k \geq 0$, we can let $\Tilde{Z}_k \coloneqq Z_{k} + \hat{\zeta}_{k} + \expt{}{\hat{\xi}_{\infty}\mid \mathcal{F}_k} - \hat{\xi}_{k}$, which forms a sequence of non-negative random variables, and deduce that:
\begin{align}
\expt{}{\Tilde{Z}_{k+1} \mid \mathcal{F}_k} \leq \Tilde{Z}_k. 
\end{align}
Furthermore, for each $k \in \nset{}{+}$, $\expt{}{\Tilde{Z}_k} \leq \expt{}{\Tilde{Z}_1} = \expt{}{Z_1} + \expt{}{\hat{\xi}_{\infty}} < \infty$, which together with the preceding observations indicates that $(\Tilde{Z}_k)_{k \in \nset{}{+}}$ is a non-negative super-martingale. 
Straightforward application of the martingale convergence theorem yields: $\lim_{k \to \infty} \Tilde{Z}_k = \Tilde{Z}_\infty$ a.s. where $\Tilde{Z}_\infty$ is a $L^1$ random variable, i.e., $\expt{}{\abs{\Tilde{Z}_{\infty}}} < \infty$. 
Denote $\hat{\xi}^c_k \coloneqq \expt{}{\hat{\xi}_{\infty} \mid \mathcal{F}_k} - \hat{\xi}_k \in \mathcal{F}_k$. 
Note that $\lim_{k \to \infty} \expt{}{\hat{\xi}^c_k} = \lim_{k \to \infty} \expt{}{\expt{}{\hat{\xi}_{\infty} \mid \mathcal{F}_k} - \hat{\xi}_k} = \lim_{k \to \infty}(\expt{}{\hat{\xi}_{\infty}} - \expt{}{\hat{\xi}_k}) = \expt{}{\hat{\xi}_{\infty}} - \lim_{k \to \infty}\expt{}{\hat{\xi}_k} = 0$ as demonstrated earlier, and thus $\hat{\xi}^c_k \overset{k \to \infty}{\to} 0$ a.s.. 
As a result, $\lim_{k\to\infty}(Z_{k} + \hat{\zeta}_k) = \Tilde{Z}_{\infty}$ a.s. 
Since the sequence $(\hat{\zeta}_k)_{k \in \nset{}{+}}$ is non-negative, monotonically increasing and bounded from above, its limit exists a.s., i.e., $\lim_{k\to\infty} \hat{\zeta}_k = \hat{\zeta}_{\infty}$ a.s. 
Moreover, due to the surrogate relation that $\hat{\zeta}_{\infty} \leq \Tilde{Z}_{\infty}$ and $\expt{}{\Tilde{Z}_{\infty}} < \infty$, we then obtain $\expt{}{\hat{\zeta}_{\infty}} < \infty$. 
Therefore, we arrive at the conclusion that $\sum_{k \in \nset{}{+}} \zeta_k = \lim_{k \to \infty}\hat{\zeta}_k < \infty$ a.s. and $\lim_{k \in\nset{}{+}} Z_k = \Tilde{Z}_{\infty} - \hat{\zeta}_{\infty}$ a.s. and the limit is $L^1$, i.e., $\expt{}{\Tilde{Z}_{\infty} - \hat{\zeta}_{\infty}} < \infty$.


\subsection{Proof of Theorem~\ref{thm:convg}}\label{pf:convg}
By applying the standing recurrent inequality of OMD \cite[Lem~A.2]{huang2023zeroth}\cite[Prop.~B.3]{mertikopoulos2018optimistic} and letting $x_*$ denote one CP of $\mathcal{G}$, we can obtain the following relation for the $k$-th iteration:
\begin{align*}
& D(x_*, X_{k+1}) \leq D(x_*, X_k) - \tau \langle G_k, X_{k+1/2} - x_*\rangle \\
& \qquad + \frac{\tau^2}{2\Tilde{\mu}}\norm{G_k - G_{k-1}}^2_{\mathcal{H}} - \frac{\Tilde{\mu}}{2}\norm{X_{k+1/2} - X_{k}}^2_{\mathcal{H}} \\
& \leq D(x_*, X_k) - \tau \langle F(X_{k+1/2}), X_{k+1/2} - x_*\rangle - \frac{\Tilde{\mu}}{2}\norm{X_{k+1/2} - X_{k}}^2_{\mathcal{H}} \\
& \qquad - \tau \langle B_k, X_{k+1/2} - x_*\rangle - \tau \langle V_k, X_{k+1/2} - x_*\rangle \\
& \qquad + \frac{\tau^2}{2\Tilde{\mu}}\norm{F(X_{k+1/2}) - F(X_{k-1/2}) + B_k - B_{k-1} + V_k - V_{k-1}}^2_{\mathcal{H}}. 
\end{align*}
Since $x_*$ is a CP of $\mathcal{G}$, we have $\langle F(X_{k+1/2}), X_{k+1/2} - x_*\rangle \geq 0$, as posited in Assumption~\ref{asp:vs}. 
Now take the conditional expectation $\expt{}{\cdot \mid \mathcal{F}_k}$ of both sides of the above inequality. 
For the inner product of systematic error $B_k$, in light of Lemma~\ref{le:bias}, $-\langle B_k, X_{k+1/2} - x_*\rangle \leq \alpha_BD_{\mathcal{X}}\delta_k$, where $D_{\mathcal{X}}$ denotes the diameter of the feasible set, i.e., $D_{\mathcal{X}} \coloneqq \max_{x, y \in \mathcal{X}}\norm{x - y}_{\mathcal{H}}$. 
Since $\expt{}{V_k \mid \mathcal{F}_k} = 0$ and $X_{k+1/2} \in \mathcal{F}_k$, $\expt{}{\langle V_k, X_{k+1/2} - x_*\rangle \mid \mathcal{F}_k} = \langle \expt{}{V_k\mid \mathcal{F}_k}, X_{k+1/2} - x_*\rangle = 0$. 
By appealing to the Cauchy-Schwarz inequality and the $L$-Lipschitz continuity of $F$, we can derive that 
\begin{align}\label{eq:convg-1}
\begin{split}
& \expt{}{D(x_*, X_{k+1}) \mid \mathcal{F}_k} \leq D(x_*, X_k) - \frac{\Tilde{\mu}}{2}\norm{X_{k+1/2} - X_{k}}^2_{\mathcal{H}} \\
&  \qquad + \frac{(\tau L)^2}{\Tilde{\mu}}\norm{X_{k+1/2} - X_{k-1/2}}^2_{\mathcal{H}} + \hat{\Delta}_{k,1}, 
\end{split}
\end{align}
where $\hat{\Delta}_{k,1} \coloneqq C_{\Delta, 1}\big(\delta_k + \delta_k^2 + \delta_{k-1}^2 + \expt{}{\norm{V_k}^2_{\mathcal{H}} \mid \mathcal{F}_k} + \norm{V_{k-1}}^2_{\mathcal{H}}\big)$ with some fixed constant $C_{\Delta, 1}$. 

In order to facilitate the convergence analysis in the merely monotone scenario, we are led to upper bound $-\norm{X_{k+1/2} - X_k}^2_{\mathcal{H}}$ as follows: 
\begin{align*}
& -\norm{X_{k+1/2} - X_k}^2_{\mathcal{H}} \leq -\frac{1}{2}\norm{X_k - P_{X_k, \mathcal{X}}(-\tau F(X_k))}^2_{\mathcal{H}} \\
& \qquad + \norm{P_{X_k, \mathcal{X}}(-\tau G_{k-1}) - P_{X_k, \mathcal{X}}(-\tau F(X_k))}^2_{\mathcal{H}} \\
& \leq -\frac{1}{2}\norm{X_k - P_{X_k, \mathcal{X}}(-\tau F(X_k))}^2_{\mathcal{H}} + \frac{\tau^2}{\Tilde{\mu}^2}\norm{G_{k-1} - F(X_k)}^2_{\mathcal{H}} \\
& \leq -\frac{1}{2}\norm{X_k - P_{X_k, \mathcal{X}}(-\tau F(X_k))}^2_{\mathcal{H}} + 2\big(\frac{\tau L}{\Tilde{\mu}}\big)^2\norm{X_{k-1/2} - X_k}^2_{\mathcal{H}} \\
& \qquad + 2\big(\frac{\tau}{\Tilde{\mu}}\big)^2\norm{B_{k-1} + V_{k-1}}^2_{\mathcal{H}} \\
& \leq -\frac{1}{2}\norm{X_k - P_{X_k, \mathcal{X}}(-\tau F(X_k))}^2_{\mathcal{H}} + 4\big(\frac{\tau L}{\Tilde{\mu}}\big)^2\norm{X_{k-1/2} - X_{k+1/2}}^2_{\mathcal{H}} \\
& \qquad + 4\big(\frac{\tau L}{\Tilde{\mu}}\big)^2\norm{X_{k+1/2} - X_{k}}^2_{\mathcal{H}} + 2\big(\frac{\tau}{\Tilde{\mu}}\big)^2\norm{B_{k-1} + V_{k-1}}^2_{\mathcal{H}}, 
\end{align*}
where $\varepsilon(x) \coloneqq \norm{x - P_{x, \mathcal{X}}(-\tau F(x))}^2_{\mathcal{H}}$ serves as a residual function. 
By the observation that $\varepsilon(x_\star) = 0$ is equivalent to the zero inclusion that $0 \in N_{\mathcal{X}}(x_\star) + \tau F(x_\star)$, we can assert that $x_{\star}$ is a CP of $\mathcal{G} \iff \varepsilon(x_\star) = 0$. 
In light of the upper bound derived above and the choice of step size $(\tau L/\Tilde{\mu})^2 \leq 1/12$, \eqref{eq:convg-1} can be reformulated as: 
\begin{align}\label{eq:convg-2}
\begin{split}
& \expt{}{D(x_*, X_{k+1}) \mid \mathcal{F}_k} \leq D(x_*, X_k) -\frac{\Tilde{\mu}}{2}(1 - \frac{1}{10})\norm{X_{k+1/2} - X_{k}}^2_{\mathcal{H}} \\
&  + \frac{1}{10}\big(-\frac{\Tilde{\mu}}{4}\varepsilon(X_k) + \frac{\Tilde{\mu}}{6}\norm{X_{k-1/2} - X_{k+1/2}}^2_{\mathcal{H}} + \frac{\Tilde{\mu}}{6}\norm{X_{k+1/2} - X_{k}}^2_{\mathcal{H}}\big) \\
&   + \frac{\Tilde{\mu}}{12}\norm{X_{k+1/2} - X_{k-1/2}}^2_{\mathcal{H}} + \hat{\Delta}_{k,2}, 
\end{split}
\end{align}
where $\hat{\Delta}_{k,2} \coloneqq C_{\Delta, 2}\big(\delta_k + \delta_k^2 + \delta_{k-1}^2 + \expt{}{\norm{V_k}^2_{\mathcal{H}} \mid \mathcal{F}_k} + \norm{V_{k-1}}^2_{\mathcal{H}}\big)$ with some larger fixed constant $C_{\Delta, 2}$.

We reapply the Cauchy-Schwarz inequality to $\norm{X_{k+1/2} - X_{k-1/2}}^2_{\mathcal{H}}$, yielding 
\begin{align*}
\norm{X_{k+1/2} - X_{k-1/2}}^2_{\mathcal{H}} \leq 2\norm{X_{k+1/2} - X_{k}}^2_{\mathcal{H}} + 2\norm{X_{k} - X_{k-1/2}}^2_{\mathcal{H}},
\end{align*}
while it can be recursively obtained that for all $k \geq 3$, 
\begin{align*}
& \norm{X_{k} - X_{k-1/2}}^2_{\mathcal{H}} = \norm{\nabla \psi^*(\nabla \psi(X_{k-1}) - \tau G_{k-1}) \\
& - \nabla \psi^*(\nabla \psi(X_{k-1}) - \tau G_{k-2})}^2_{\mathcal{H}} \leq (\tau/\Tilde{\mu})^2\norm{G_{k-1} - G_{k-2}}^2_{\mathcal{H}} \\
& \leq 2(\tau L/\Tilde{\mu})^2\norm{X_{k-1/2} - X_{k-3/2}}^2_{\mathcal{H}} + 2(\tau/\Tilde{\mu})^2 \Delta_{k,3}, 
\end{align*}
with $ \Delta_{k,3} \coloneqq \norm{B_{k-1} - B_{k-2} + V_{k-1} - V_{k-2}}^2_{\mathcal{H}}$. 

Adding $(\Tilde{\mu}/10)\norm{X_{k+1/2} - X_{k-1/2}}^2_{\mathcal{H}}$ to both sides of \eqref{eq:convg-2} and substituting $\norm{X_{k+1/2} - X_{k-1/2}}^2_{\mathcal{H}}$ of R.H.S. with the proceeding inequality produces:
\begin{align*}
\begin{split}
& \expt{}{D(x_*, X_{k+1}) + \frac{\Tilde{\mu}}{10}\norm{X_{k+1/2} - X_{k-1/2}}^2_{\mathcal{H}} \mid \mathcal{F}_k} \leq D(x_*, X_k) \\
&  -\frac{13\Tilde{\mu}}{30}\norm{X_{k+1/2} - X_{k}}^2_{\mathcal{H}} - \frac{\Tilde{\mu}}{40}\varepsilon(X_k) + \frac{\Tilde{\mu}}{5}\norm{X_{k+1/2} - X_{k-1/2}}^2_{\mathcal{H}} + \hat{\Delta}_{k,2} \\
& \leq D(x_*, X_k) + \frac{\Tilde{\mu}}{15}\norm{X_{k-1/2} - X_{k-3/2}}^2_{\mathcal{H}} \\
&  -\frac{\Tilde{\mu}}{30}\norm{X_{k+1/2} - X_{k}}^2_{\mathcal{H}} - \frac{\Tilde{\mu}}{40}\varepsilon(X_k) + \hat{\Delta}_{k,3}, \forall k \geq 3, \\
\end{split}
\end{align*}
where $\hat{\Delta}_{k,3} \coloneqq C_{\Delta,3}\big(\delta_k + \sum_{t=k-2}^k (\delta_t^2 + \expt{}{\norm{V_t}^2_{\mathcal{H}} \mid \mathcal{F}_k})\big)$ for some further larger constant $C_{\Delta,3}$. 
Further manipulating the coefficients of $\norm{X_{k+1/2} - X_{k-1/2}}^2_{\mathcal{H}}$ gives $\forall k \geq 3$: 
\begin{align*}
\begin{split}
& \expt{}{D(x_*, X_{k+1}) + \frac{\Tilde{\mu}}{15}\norm{X_{k+1/2} - X_{k-1/2}}^2_{\mathcal{H}} \mid \mathcal{F}_k} \\
& \leq D(x_*, X_k) + \frac{\Tilde{\mu}}{15}\norm{X_{k-1/2} - X_{k-3/2}}^2_{\mathcal{H}} - \frac{\Tilde{\mu}}{30}\norm{X_{k+1/2} - X_{k-1/2}}^2_{\mathcal{H}}  \\
& - \frac{\Tilde{\mu}}{30}\norm{X_{k+1/2} - X_{k}}^2_{\mathcal{H}} - \frac{\Tilde{\mu}}{40}\varepsilon(X_k) + \hat{\Delta}_{k,3}. 
\end{split}
\end{align*}

Using Lemma~\ref{le:variance}, we have $\expt{}{\hat{\Delta}_{k,3}} \leq C_{\Delta,3}\big(\delta_k + \sum_{t=k-2}^k (\delta_t^2 + \alpha_V/T_t)\big)$, and the summability conditions $\sum_{k \in \nset{}{+}} \delta_k < \infty$ and $\sum_{k \in \nset{}{+}} 1/T_k < \infty$ entail that $\sum_{k \geq 3} \expt{}{\hat{\Delta}_{k,3}} < \infty$. 
Then the application of Theorem~\ref{thm:ext-rs} allows us to assert the following: 
\begin{outline}[enumerate]
\1 $\sum_{k \geq 3} \Tilde{\mu}/40 \cdot \varepsilon(X_k) < \infty$ a.s.;
\1 $\sum_{k \geq 3} \Tilde{\mu}/30 \cdot \norm{X_{k+1/2} - X_{k-1/2}}^2_{\mathcal{H}} < \infty$ a.s.; 
\1 $\sum_{k \geq 3} \Tilde{\mu}/30 \cdot \norm{X_{k+1/2} - X_{k}}^2_{\mathcal{H}} < \infty$ a.s.; 
\1 $D(x_*, X_{k+1}) + \Tilde{\mu}/15\cdot \norm{X_{k+1/2} - X_{k-1/2}}^2_{\mathcal{H}}$ converges a.s. to some $L^1$ random variable. 
\end{outline}
These results entail that there exists a sample set $\hat{\Omega} \subseteq \Omega$ and $\mathcal{P}(\hat{\Omega}) = 1$ such that for any $\omega \in \hat{\Omega}$, the above statements $(\romannum{1})-(\romannum{4})$ hold true for the deterministic sequences $(X_k(\omega))_{k \in \nset{}{+}}$ and $(X_{k+1/2}(\omega))_{k \in \nset{}{+}}$. 
Moreover, since $\big(X_k(\omega)\big)_{k \in \nset{}{}} \in \mathcal{X}$ and the map $P_{x, \mathcal{X}}(-\tau F(x))$ is continuous in $x$, there exists a subsequence $(k_m)_{m \in \nset{}{+}}$ such that $X_{k_m}(\omega) \overset{m\to\infty}{\to} x_{\star}$ and $\lim_{m \to \infty} \varepsilon\big(X_{k_m}(\omega)\big) = \varepsilon\big(x_{\star}\big) = 0$, i.e., $x_\star$ is a CP of $\mathcal{G}$. 
We can then substitute $x_\star$ for $x_*$ in $(\romannum{4})$.  Since $(\romannum{2})$ suggests that $\norm{X_{k+1/2} - X_{k-1/2}}^2_{\mathcal{H}}(\omega) \overset{k \to \infty}{\to} 0$, we can assert that $D(x_\star, X_{k}(\omega))$ admits a finite limit. 
In conjunction with Assumption~\ref{asp:recip}, it follows that $D(x_\star, X_{k_m}(\omega)) \overset{m \to \infty}{\to} 0$ and hence $D(x_\star, X_{k}(\omega)) \overset{k \to \infty}{\to} 0$, i.e., the base states $\big(X_{k}(\omega)\big)_{k \in \nset{}{+}}$ converge to $x_\star$. 
Combining this result with $(\romannum{2})$ yields that the leading states $\big(X_{k+1/2}(\omega)\big)_{k \in \nset{}{+}}$ converge to $x_\star$, and the a.s. convergence of the actual sequence of play $\big(\hat{X}_{k+1/2,t}(\omega)\big)_{k \in \nset{}{+}}$ to $x_\star$ directly derives from \eqref{eq:perb} and $\delta_k \overset{k \to \infty}{\to} 0$. 
