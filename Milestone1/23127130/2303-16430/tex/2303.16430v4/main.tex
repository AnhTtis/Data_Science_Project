%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Bandit Online Learning in Merely Coherent Games with Multi-Point Pseudo-Gradient Estimate
}

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb}
% \usepackage[english]{babel}

\usepackage[dvipsnames]{xcolor}
\usepackage{mathrsfs}
\usepackage{bbm, dsfont}
\let\labelindent\relax % Error: Command \labelindent already defined
\usepackage{amsmath, amsfonts, outlines, mathtools, outlines, cancel, enumitem, float, stackengine, graphicx, romannum}
% accents: \bar below symbol
\usepackage{subcaption}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{newtxtext,newtxmath}
\usepackage{algorithm, algpseudocode}
% \usepackage[ruled,vlined]{algorithm2e}
% \usepackage{algorithmic}

\setenumerate[1]{label=(\roman*)}

% % calligraphic latters
% % Ralph Smith’s Formal Script Font (rsfs)
% \usepackage{mathrsfs} % $\mathscr{ABCDEFGHIJKLMNOPQRSTUVWXYZ}$
% % Euler Script font: Use the “euscript” package
% \usepackage[mathscr]{euscript} % $\mathscr{ABCDEFGHIJKLMNOPQRSTUVWXYZ}$

\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{propst}{Proposition}
\newtheorem{envdef}{Definition}
\newtheorem{coro}{Corollary}
\raggedbottom

\DeclareMathSizes{10}{9}{6}{5}
\allowdisplaybreaks

\input{macros.tex}
\pdfminorversion=4

\author{Yuanhanqing Huang$^{1}$ and Jianghai Hu$^{1}$% <-this % stops a space
\thanks{This work was supported by the National Science Foundation under Grant No. 2014816 and No.2038410. }% <-this % stops a space
\thanks{$^{1}$The authors are with the Elmore Family School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, 47907, USA 
        {\tt\small \{huan1282, jianghai\}@purdue.edu}}%
}

%% Switch between the conference proceeding version and the arXiv version
\newif\ifproceeding
\newif\ifarxiv

% Uncomment to generate the version for the conference proceeding
% \proceedingtrue
% \arxivfalse

% Uncomment to generate the version for arXiv 
\proceedingfalse
\arxivtrue

\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Non-cooperative games serve as a powerful framework for capturing the interactions among self-interested players and have broad applicability in modeling a wide range of practical scenarios, ranging from power management to drug delivery. 
Although most existing solution algorithms assume the availability of first-order information or full knowledge of the objectives and others' action profiles, there are situations where the only accessible information at players' disposal is the realized objective function values. 
In this paper, we devise a bandit online learning algorithm for merely coherent games that integrates the optimistic mirror descent scheme and multi-point pseudo-gradient estimates. 
We further demonstrate that the generated actual sequence of play can converge a.s. to a critical point if the sequences of query radius and sample size are chosen properly, without resorting to extra Tikhonov regularization terms or additional norm conditions. 
Finally, we illustrate the validity of the proposed algorithm via a Rock-Paper-Scissors game and a least square estimation game. 
\end{abstract} 


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Recent years have witnessed considerably increasing interest in the analysis of multi-agent systems and large-scale networks, which find a wide range of applications such as thermal load management of autonomous buildings \cite{jiang2021game}, power management in sensor network \cite{campos2008game}, optimal drug delivery in the treatment of disease \cite{wu2012evolutionary}, control of environmental pollution \cite{du2015game}, etc. 
One primary objective in multi-agent systems is to devise local protocols for each agent, by following which, the resulting group behavior is optimal as measured by a certain system-level metric \cite{li2013designing}. 
With its origins in \cite{nash1950equilibrium}, game theory offers the theoretical tools to model and examine the strategic choices and associated outcomes of rational players who make decisions in a non-cooperative manner.  
In particular, in the Nash equilibrium problem (NEP), this group of players seeks to reach a stationary point known as Nash equilibrium (NE), where no rational player has any incentive to unilaterally deviate from it. 

In order to devise an algorithm for the NEP or its variants, it is crucial to have access to the first-order information, i.e., the partial gradient of the local objective function of each player, the evaluation of which nevertheless usually requires the action profile from all players. 
In view of this, in some studies \cite{mertikopoulos2019learning,yi2019operator,tatarenko2020geometric}, the availability of first-order oracles is taken as a given, whereas some other studies \cite{pavel2019distributed, bianchi2022fast, huang2022distributed} investigate network games where a communication network exists and players are willing to communicate with their trusted neighbors and keep local estimates of others' action profiles. 
Despite the notable progress discussed above, there are many real-world scenarios where players only have access to the observed objective values of selected actions, which makes the bandit/zeroth-order learning strategy a compelling choice. 
Our primary objective in this work is to develop an online learning algorithm for multi-player continuous games that possess mere coherence with 
bandit information. 

\textit{Related Work: }
\Tblue{There have been several recent notable contributions to the field of bandit learning in games. 
In their work \cite{bravo2018bandit}, Bravo et al. proposed a bandit version of mirror descent (MD), which guarantees a.s. convergence to an NE when the game is strictly monotone and achieves a convergence rate of $O(1/t^{1/3})$ for strongly monotone cases. 
% By employing a barrier-based method, Lin et al. \cite{lin2021optimal} improved the convergence rate for strongly monotone games from $O(1/t^{1/3})$ to $O(1/t^{1/2})$. 
Concerning the study of convergence rates in the realm of strongly monotone games or strongly variationally stable Nash equilibrium seeking, \cite{lin2021optimal, tatarenko2022rate, tatarenko2023convergence, drusvyatskiy2022improved} have succeeded in elevating the convergence rates from $O(1/t^{1/3})$ to $O(1/t^{1/2})$. 
Huang et al. \cite{huang2023zeroth} developed two bandit learning algorithms by integrating residual pseudo-gradient estimates into single-call extra-gradient schemes that ensure a.s. convergence to critical points of pseudo-monotone plus games. 
Moreover, in strongly pseudo-monotone plus games, by employing the proposed algorithms, the convergence rate is further elevated to  $O(1/t^{1-\epsilon})$. }

To extend the analysis beyond the realm of strictly monotone and pseudo-monotone plus games, Tatarenko et al. \cite{tatarenko2020bandit} utilized the single time-scale Tikhonov regularization and a doubly regularized approximate gradient descent strategy to develop an algorithm that converges to NEs in probability when the game is monotone and four decaying sequences are tuned properly. 
In a recent study \cite{gao2022bandit}, Gao et al. introduced an algorithm that integrates second-order learning dynamics and Tikhonov regularization and established the a.s. convergence of the sequence of play under the assumption that there exists at least one interior variationally stable state (VSS). 
Yet, the convergence is contingent on the norm condition that the $\ell_2$-norm of the state sequence should be greater than that of the VSS, which can be challenging to verify during the iterative process. 

In the literature of variational inequalities (VIs) and their stochastic versions (SVIs), Mertikopoulos et al. \cite{mertikopoulos2018optimistic} showed that the vanilla MD converges when the problem is strictly coherent, a relaxed variant of strict monotonicity, but fails to converge in merely coherent VIs. 
In contrast, the extra-gradient (EG) method is capable of achieving convergence to a solution in all coherent VIs, but it requires the exact operator values. 
In the presence of random noise in operator values, strict coherence is necessary to establish the convergence of the EG iteration. 
Similar convergence analysis is also reported in \cite{kannan2019optimal} for pseudo-monotone plus SVIs. 
To address the challenges posed by random noise, Iusem et al. \cite{iusem2017extragradient} developed an extra-gradient method for pseudo-monotone SVIs that incorporates an iterative variance reduction procedure and established both asymptotic convergence and non-asymptotic convergence rates for the proposed algorithm. 

\textit{Contributions: }
In this work, we develop a bandit online learning algorithm and establish the a.s. convergence of the generated sequence of play under the regularity condition that the game is merely coherent, which is broader and more general than the games investigated in \cite{tatarenko2020geometric, bravo2018bandit, lin2021optimal, huang2023zeroth}.  
The proposed algorithm leverages the optimistic mirror descent (OMD) \cite{azizian2021last, hsieh2019convergence}, a single-call extra-gradient scheme, as the backbone, which enables us to contend with the absence of strict coherence and reduces the query cost induced by the extra step. 
Alongside the OMD updates, the multi-point pseudo-gradient estimation is employed and the decaying rate of the variance of zeroth-order estimations can be controlled by properly tuning the query count per iteration. 
Furthermore, the validity of the proposed algorithm is verified through a Rock-Paper-Scissors game and a least square estimation game. 
\ifproceeding
All the proofs are included in \cite{huang2023bandit} due to the page limit. 
\fi

\textit{Basic Notations: } 
For a set of vectors $\{v_i\}_{i \in S}$, $[v_i]_{i \in S}$ or $[v_1; \cdots; v_{|S|}]$ denotes their vertical stack. 
For a vector $v$ and a positive integer $i$, $[v]_i$ denotes the $i$-th entry of $v$. 
We let $\norm{\cdot}$ denote the $\ell_2$-norm and $\langle, \rangle$ represent the canonical dot product. 
Let $\cl{\mathcal{S}}$ denote the closure of set $\mathcal{S}$, $\text{int}(\mathcal{S})$ the interior, and $\partial \mathcal{S}$ the boundary. 

\section{SETUP AND PRELIMINARIES}

\subsection{Game Formulation}
In a multi-player non-cooperative game $\mathcal{G}$ with the presence of $N$ players, indexed by $\mathcal{N} \coloneqq \{1, \ldots, N\}$, each player $i \in \mathcal{N}$ aims to optimize its own local objective $J^i$ by adjusting its action $x^i \in \mathcal{X}^i \subseteq \rset{n^i}{}$, which can be described as follows:
\begin{align}
\minimize_{x^i \in \mathcal{X}^i} J^i(x^i; x^{-i}),  
\end{align}
where $x^{-i} \coloneqq [x^j]_{j \in \mathcal{N}_{-i}}$ denotes the stack action of other players that parameterizes the objective $J^i$ with $\mathcal{N}_{-i} \coloneqq \mathcal{N}\backslash\{i\}$ and $x \coloneqq [x^j]_{j \in \mathcal{N}}$; 
$\mathcal{X}^i$ denotes the feasible set of player $i$, and for brevity, we let $\mathcal{X} \coloneqq \prod_{j \in \mathcal{N}} \mathcal{X}^j \subseteq \rset{n}{}$ represent the global strategy space and $\mathcal{X}^{-i} \coloneqq \prod_{j \in \mathcal{N}} \mathcal{X}^j \subseteq \rset{n^{-i}}{}$ with $n \coloneqq \sum_{j \in \mathcal{N}} n^j$ and $n^{-i} \coloneqq \sum_{j \in \mathcal{N}^{-i}} n^j$. 
Our blanket assumptions for the objective functions $J^i$'s and the local feasible sets $\mathcal{X}^i$'s will be as follows:

\begin{assumption}\label{asp:objt-set}
For each player $i$, the local objective function $J^i$ is continuously differentiable in $x$ over the global strategy space $\mathcal{X}$.
Moreover, its individual strategy space $\mathcal{X}^i$ is compact and convex, and has a non-empty interior. 
\end{assumption}

Given the smoothness posited in Assumption~\ref{asp:objt-set}, a single-valued operator that we will leverage extensively throughout is the pseudo-gradient operator $F: \rset{n}{} \to \rset{n}{}$. 
It is defined as the concatenation of all the partial gradient operators, i.e., 
\begin{align}
F: x \mapsto [\nabla_{x^i} J^i(x^i; x^{-i})]_{i \in \mathcal{N}}. 
\end{align}

% An operator that we will leverage extensively throughout is  the pseudo-gradient operator $F$, which is defined as the direct product of sub-gradients, i.e., 
% \begin{align}
% F: x \mapsto \prod_{i \in \mathcal{N}} [\partial_{x^i} J^i(x^i; x^{-i})]. 
% \end{align}
% Given the smoothness posited in Assumption~\ref{asp:objt-set}, $F$ is single-valued, i.e., $F: \rset{n}{} \to \rset{n}{}$ and can be reduced as below: 
% \begin{align*}
% F: x \mapsto \prod_{i \in \mathcal{N}} [\nabla_{x^i} J^i(x^i; x^{-i})] = [\nabla_{x^i} J^i(x^i; x^{-i})]_{i \in \mathcal{N}}. 
% \end{align*}

% In the sequel, our analysis will take place in the Hilbert space $\mathcal{H} \coloneqq \prod_{i \in \mathcal{N}} \mathcal{H}^i$, where each $\mathcal{H}^i$ is a lower-dimensional Hilbert space constructed by equipping the vector space $\mathbb{R}^{n^i}$ with inner product $\langle \cdot, \cdot, \rangle_{\mathcal{H}^i}$. 
% For $x \coloneqq [x^i]_{i \in \mathcal{N}} \in \rset{n}{}$ and $y \coloneqq [y^i]_{i \in \mathcal{N}} \in \rset{n}{}$, the inner product of $\mathcal{H}$ is then defined as $\langle x, y \rangle_{\mathcal{H}} \coloneqq \sum_{i \in \mathcal{N}} \langle x^i, y^i \rangle_{\mathcal{H}^i}$. 
% As such, the norm induced by the inner product satisfies $\norm{x}^2_{\mathcal{H}} \coloneqq \sum_{i \in \mathcal{N}} \norm{x^i}^2_{\mathcal{H}^i}$. 
% Our rationale for working in $\mathcal{H}$ rather than the Euclidean space is that the induced flexibility may enable us to accommodate scenarios such as heterogeneous updating step sizes among players. 

Before proceeding, we remark that Assumption~\ref{asp:objt-set} implicitly implies that $F$ is Lipschitz continuous on $\mathcal{X}$ with some constant $L$, i.e., for any $x$ and $x^\prime \in \mathcal{X}$, we have 
\begin{align}
\norm{F(x) - F(x^\prime)} \leq L\norm{x - x^\prime}. 
\end{align}

As for the solution concept, we focus on critical points (CPs) \cite[Sec.~2.2]{mertikopoulos2022learning}, a more relaxed solution concept than Nash equilibria (NEs), whose definition is given as follows. 
\begin{envdef}\label{def:cps} (Critical Points)
A decision profile $x_* \in \mathcal{X}$ is a critical point of the game $\mathcal{G}$ if it is a solution to the associated (Stampacchia) variational inequality (VI), i.e., 
\begin{align}\label{eq:cps}
\langle F(x_*), x - x_*\rangle \geq 0, \; \forall x \in \mathcal{X}. 
\end{align}
\end{envdef}
We postulate that the games discussed in this work admit at least one critical point inside $\mathcal{X}$. 
A well-known result is that CPs coincide with NEs when $J^i$ is convex and continuously differentiable in $x^i$ for all $i$ \cite[Sec.~1.4.2]{facchinei2003finite}. 

In this work, our aim is to propose a new algorithm that is applicable to a broader class of games as compared to strictly monotone games and pseudo-monotone plus games. 
Moreover, we intend to further relax pseudo-monotonicity assumptions that are usually imposed upon the structure of the game to the ones merely upon equilibria. 
\begin{assumption}\label{asp:vs} (Mere Coherence)
The game $\mathcal{G}$ is merely coherent if every critical point (CP) $x_*$ of $\mathcal{G}$ is merely variationally stable, i.e., $\langle F(x), x - x_*\rangle \geq 0$ for all $x \in \mathcal{X}$. 
\end{assumption}

Before we proceed, it is pertinent to make a few comments. 
% We investigate a Hilbert space that is more general than Euclidean space to accommodate greater flexibility, such as heterogeneous choices of step sizes for future exploration. 
Our analysis primarily lies within Euclidean space; however, we recognize the potential for extending its applicability to finite-dimensional Hilbert spaces.
In addition, we employ mere coherence rather than pseudo-monotonicity as the standing assumption, as the former one is less restrictive. 
Recall that an operator $F$ is pseudo-monotone if for all $x, y \in \mathcal{X}$, $\langle F(y), x - y\rangle \geq 0 \implies \langle F(x), x - y\rangle \geq 0$. 
Nonetheless, the latter is generally the more readily verifiable assumption in practical applications, since it does not needs the CPs $x_*$'s to be known a priori.

% Actually, assumption 1 implicitly introduces Lipschitz continuity. 
% Introduce the constant associated with it. 

\subsection{Optimistic Mirror Descent}\label{subsec:omd}

% The definition of mirror map and proxy map
In this subsection, we shall provide a brief overview of the optimistic mirror descent algorithm, as well as related concepts and results. 
As an extension of the Euclidean projection, the mirror map $\nabla \psi^*: \rset{}{} \to \rset{}{}$ is defined as: 
\begin{align}
\nabla \psi^*(z) = \argmax_{x \in \mathcal{X}} \{\langle z, x\rangle - \psi(x)\}, 
\end{align}
where $\psi:\dom\psi \to \rset{}{}$ is a so-called distance-generating function (DGF) with $\dom\psi$ denoting a convex and open set where $\psi$ is well-defined. 
The DGF fulfills the following conditions \cite[Sect.~4.1]{bubeck2014theory}:
$(\romannum{1})$ $\psi$ is differentiable and $\Tilde{\mu}$-strongly convex for some $\Tilde{\mu} > 0$; 
$(\romannum{2})$ $\nabla \psi(\dom \psi) = \rset{n}{}$; 
$(\romannum{3})$ $\cl{\dom \psi} \supseteq \mathcal{X}$ and $\lim_{x \to \partial (\dom \psi)}$ $\norm{\nabla \psi(x)}_* = +\infty $. 
The definition of DGF $\psi$ allows us to introduce a pseudo-distance called the Bregman divergence, which is defined as:
\begin{align}
D(p,x) = \psi(p) - \psi(x) - \langle \nabla \psi(x), p - x\rangle, \forall p, x \in \dom \psi.
\end{align}
To let $D(p, \cdot)$ represent a certain distance measure to $p$ and use this measure to define a neighborhood of $p$, we make the following assumption. 
\begin{assumption}\label{asp:recip}
(Bregman Reciprocity) The chosen DGF $\psi$ satisfies that if the sequence $(x_k)_{k \in \nset{}{+}}$ converges to some point $p$, i.e., $\norm{x_k - p} \to 0$, then $D(p, x_k) \to 0$.
\end{assumption}
Then, the Bregman divergence generates the prox-mapping $P_{x, \mathcal{X}}: \mathcal{H} \to \dom{\psi} \cap \mathcal{X}$ for some fixed $x \in \mathcal{X} \cap \dom{\psi}$ that plays a critical role in mirror descent and its variants:
\begin{align}
P_{x, \mathcal{X}}(y) = \argmin_{x^\prime \in \mathcal{X}}\{\langle y, x - x^\prime\rangle + D(x^\prime, x)\}.
\end{align}

With all these in hand, the optimistic mirror descent (OMD) \cite{azizian2021last, hsieh2019convergence} can be expressed as below: 
\begin{align}\label{eq:omd}
\begin{split}
X_{k+1/2} &= P_{X_k, \mathcal{X}}(-\tau_k F(X_{k-1/2})) \\
X_{k+1} &= P_{X_k, \mathcal{X}}(-\tau_k F(X_{k+1/2})),
\end{split}
\end{align}
\Tblue{where $(\tau_k)_{k \in \nset{}{+}}$ denotes a proper sequence of step sizes. }
The update consists of the following two steps. 
Given the base state $X_k$ at step $k$, in the look-forward step, the leading state $X_{k+1/2}$ is procured by updating $X_k$ with the proxy $F(X_{k-1/2})$ queried at $X_{k-1/2}$ rather than the exact pseudo-gradient $F(X_{k})$ queried at $X_k$ to reduce the oracle call per iteration. 
This step is essential in anticipating the landscape of $F$ and facilitating the convergence when $F$ is merely monotone, i.e., $\langle F(x) - F(y), x - y \rangle \geq 0$, for all $x$ and $y$ feasible. 
In the state-updating step, the base state $X_k$ is revised to $X_{k+1}$ following the pseudo-gradient information $F(X_{k+1/2})$. 
The OMD falls into the single-call category, distinguishing itself from the conventional extra gradient algorithm \cite{iusem2017extragradient} by exclusively utilizing the first-order information at $X_{k+1/2}$, without requiring information from both $X_{k}$ and $X_{k+1/2}$.

% A paragraph about the convergence of OMD:
%   - deterministic case, it can converge to a CP if the game is pseudo-monotone
%   - stochastic case, it requires some strict monotonicity

\section{MULTI-POINT PSEUDO-GRADIENT ESTIMATION}

In this paper, we examine the scenario where the first-order information at the leading state, i.e., $F(X_{k+1/2})$ is not readily available, and players need to estimate them based on the realized objective function values. 
A prevalent technique in the literature of first-order information estimation methods is the simultaneous perturbation stochastic approximation (SPSA) approach \cite{bravo2018bandit}. 
For each $i \in \mathcal{N}$, let $\mathbb{B}_i, \mathbb{S}_i \subseteq \rset{n^i}{}$ denote the unit ball and the unit sphere centered at the origin. 
At each iteration $k$, before implementing the SPSA estimate, we initially undertake the following perturbation step: 
\begin{align}\label{eq:perb}
\begin{split}
& \hat{X}^i_{k+1/2} = (1 - \frac{\delta_k}{r^i})X^i_{k+1/2}+\frac{\delta_k}{r^i}(p^i + r^iu^i_k) = \bar{X}^i_{k+1/2} + \delta_k u^i_k,
\end{split}
\end{align}
where 
$u^i_k$ is randomly sampled from $\mathbb{S}_{i} \subseteq \rset{n^i}{}$ and we define $u_k \coloneqq [u^i_k]_{i \in \mathcal{N}}$; 
$\delta_k$ represents the random query radius at iteration $k$;
$\mathbb{B}(p^i, r^i) \subseteq \mathcal{X}^i$ is an arbitrary fixed Euclidean ball within the feasible set $\mathcal{X}^i$ that centers at $p^i$ with radius $r^i$; 
$\bar{X}^i_{k+1/2} \coloneqq (1 - \delta_k/r^i)X^i_{k+1/2} + (\delta_k/r^i) p^i$. 
Denote $\bar{X}_{k+1/2} \coloneqq [\bar{X}^i_{k+1/2}]_{i \in \mathcal{N}}$. 
In the merit of the feasibility adjustment in \eqref{eq:perb}, the action to be taken will sit within the feasible set, i.e., $\hat{X}^i_{k+1/2} \in \mathcal{X}^i$ and $\hat{X}_{k+1/2} \coloneqq [\hat{X}^i_{k+1/2}]_{i \in \mathcal{N}} \in \mathcal{X}$. 
With this in hand, the SPSA estimation can be expressed as $\frac{n^i}{\delta_k} J^i(\hat{X}_{k+1/2})u^i_k$. 
Nevertheless, as previously noted in \cite{bravo2018bandit}, the SPSA approach incurs a larger estimation variance with a decrease in query radius aimed at improving estimation accuracy, which results in conservative choices of updating step sizes $\tau_k$ and significant degradation of the convergence rate. 
To resolve this conundrum, there has been increased consideration given to schemes such as two-point estimation and residual estimation to keep the variance bounded. 
On account of this, we consider the multi-point pseudo-gradient estimation (MPG) scheme, the counterparts of which in the field of optimization can be found in \cite{duchi2015optimal}.
At every iteration $k$, each player $i$ executes the perturbation step in \eqref{eq:perb} $(T_k + 1)$ times in an independent manner, takes the action $\hat{X}^i_{k+1/2,t}$, and observes the associated realized objective function values $J^i(\hat{X}_{k+1/2,t})$, where the variable $t \in \nset{}{}$ is an index of the multiple samples taken per iteration.
The multi-point pseudo-gradient estimate can be formulated as below:
\begin{align}\label{eq:mpg}
\tag{MPG}
G^i_k \coloneqq \frac{n^i}{\delta_kT_k}\sum_{t=1}^{T_k}\big(J^i(\hat{X}_{k+1/2,t}) - J^i(\hat{X}_{k+1/2,0})\big)u^i_{k,t}, 
\end{align}
where $(u^i_{k,t})_{t=0, \ldots, T_k}$ are i.i.d. random variables uniformly distributed over $\mathbb{S}_i$; 
the action taken by player $i$ is given by 
$\hat{X}^i_{k+1/2,t} \coloneqq (1 - \frac{\delta_k}{r^i})X^i_{k+1/2}+\frac{\delta_k}{r^i}(p^i + r^iu^i_{k,t}) = \bar{X}^i_{k+1/2} + \delta_k u^i_{k,t}$; $\hat{X}_{k+1/2,t} \coloneqq [\hat{X}^i_{k+1/2,t}]_{i \in \mathcal{N}}$. 
To simplify the presentation, we will henceforth use $\hat{J}^i_{k,t}$ to represent the realized objective value $J^i(\hat{X}_{k+1/2,t})$ for the $t$-th sample at iteration $k$. 
Prior to delving into the properties of \ref{eq:mpg}, we first outline the probability setup to streamline our later discussion. 
Let $(\Omega, \mathcal{F}, \mathcal{P})$ denote the underlying probability space. 
The filtration $(\mathcal{F}_k)_{k \in\nset{}{+}}$ is constructed as $\mathcal{F}_k \coloneqq \sigma\big\{X_0, \{u_{1,t}\}_{t=0}^{T_1}, \ldots, \{u_{k-1,t}\}_{t=0}^{T_{k-1}}\big\}$, which captures the update that results in $X_k$, i.e., the entire information up to and including iteration $k-1$. . 
Then to characterize \ref{eq:mpg}, we start by considering the following decomposition of it:
\begin{align*}
G^i_k =& \nabla_{x^i} J^i(X_{k+1/2}) + \big(G^i_k - \expt{}{G^i_k \mid \mathcal{F}_k}\big) \\
& + \big(\expt{}{G^i_k \mid \mathcal{F}_k} - \nabla_{x^i} J^i(X_{k+1/2})\big). 
\end{align*}
For brevity, we let $B^i_k \coloneqq \expt{}{G^i_k \mid \mathcal{F}_k} - \nabla_{x^i} J^i(X_{k+1/2})$ represent the systematic error and $V^i_k \coloneqq G^i_k - \expt{}{G^i_k \mid \mathcal{F}_k}$ the stochastic error. 
To facilitate later analysis, for each $J^i$, we introduce the $\delta$-smoothed objective function $\Tilde{J}^i_{\delta}$: 
\begin{align}
\Tilde{J}^i_\delta(x^i; x^{-i}) \coloneqq \frac{1}{\mathbb{V}^i_{\delta}}\int_{\delta \mathbb{S}_{-i}}\int_{\delta \mathbb{B}_i} J^i(x^i + \Tilde{\tau}^i; x^{-i} + \tau^{-i})d\Tilde{\tau}^id\tau^{-i}, 
\end{align}
where $\mathbb{S}_{-i} \coloneqq \prod_{j \in \mathcal{N}^{-i}} \mathbb{S}_j \subseteq \rset{n^{-i}}{}$; 
$\mathbb{V}^i_{\delta} \coloneqq \vol{\delta \mathbb{B}_i} \cdot \vol{\delta \mathbb{S}_{-i}}$.  
The lemmas presented below provide an examination of the properties of $B^i_k$ and $V^i_k$, which will be later employed in the proof of the main theorem. 

\begin{lemma}\label{le:bias}
Suppose that Assumption~\ref{asp:objt-set} holds. 
Then at each iteration $k$, the conditional expectation satisfies $\expt{}{G^i_k \mid \mathcal{F}_k} = \nabla_{x^i} \Tilde{J}^i_{\delta_k}(\bar{X}_{k+1/2})$ a.s. for every $i \in \mathcal{N}$. 
Moreover the systematic error $B_k \coloneqq [B^i_k]_{i \in \mathcal{N}}$ possesses a decaying upper bound $\norm{B_k} \leq \alpha_B \delta_k$ for some positive constant $\alpha_B$.
\end{lemma}
\begin{proof}
\ifarxiv
See Appendix~\ref{pf:bias}. 
\fi
\ifproceeding
See \cite[Appendix~A]{huang2023bandit}.
\fi
\end{proof}

In contrast to the single-point or two-point estimates, the advantage of utilizing \ref{eq:mpg} is primarily demonstrated in the following lemma, which measures the decaying rate of the stochastic error w.r.t. the number of samples. 
\begin{lemma}\label{le:variance}
Suppose that Assumption~\ref{asp:objt-set} holds. 
Then at each iteration $k$, the squared norm of $V_k \coloneqq [V^i_k]_{i \in \mathcal{N}}$ satisfies $\expt{}{\norm{V_k}^2 \mid \mathcal{F}_k} \leq \alpha_V / T_k$ for some positive constant $\alpha_V$. 
\end{lemma}
\begin{proof}
\ifarxiv
See Appendix~\ref{pf:variance}. 
\fi
\ifproceeding
See \cite[Appendix~B]{huang2023bandit}.
\fi
\end{proof}

% In Lemma~\ref{le:variance}, the decaying bound is derived with respect to the conditional expectation rather than the range of the stochastic error, while subsequently, we will show that the range of the random variable $\norm{G_k}_{\mathcal{H}}$ is still bounded by some constant. 

% \begin{lemma}\label{le:pgrad-bdd}
% Suppose that Assumption~\ref{asp:objt-set} holds. Then, for all $k \in \nset{}{+}$, there exists some constant $\bar{g} > 0$ such that $\norm{G_k}_{\mathcal{H}} < \bar{g}$. 
% \end{lemma}
% \begin{proof}
% See Appendix~\ref{pf:pgrad-bdd}. 
% \end{proof}

\section{A VARIANCE-REDUCTION LEARNING ALGORITHM AND CONVERGENCE ANALYSIS}

% Present the algorithm
In view of the convergence properties of OMD introduced in Sec.~\ref{subsec:omd}, we design a zeroth-order algorithm for merely monotone games by incorporating \ref{eq:mpg} into OMD, the precision of which can be controlled by adjusting the sample size per iteration. 
Each player of the group possesses their own local $\Tilde{\mu}^i$-strongly convex DGF, denoted by $\psi^i$. 
Additionally, the function $\psi(x) \coloneqq \sum_{i \in \mathcal{N}} \psi^i(x^i)$ with $x \coloneqq [x^i]_{i \in \mathcal{N}}$ represents the group DGF, which is $\Tilde{\mu}$-strongly convex. 
The proposed approach is outlined in Algorithm~\ref{alg:vr-bdt-omd}. 

\begin{algorithm}
\caption{Zeroth-Order Variance-Reduced Learning of CPs Based on Optimistic Mirror Descent (Player $i$)}\label{alg:vr-bdt-omd}
\begin{algorithmic}[1]
\State \textbf{Initialize:} $X^i_0 = X^i_{1/2} = X^i_1 \in \mathcal{X}^i \cap \dom{\psi^i}$ arbitrarily; 
$G^i_{0} = \bzero_{n^i}$; 
$p^i, r^i$ to be the center and radius of an arbitrary ball within the set $\mathcal{X}^i$
\Procedure{At the $k$-th iteration ($k \in \nset{}{+}$)}{}
\State $X^i_{k+1/2} \leftarrow P_{X^i_k, \mathcal{X}^i}(-\tau G^i_{k-1})$ 
\For{$t = 0, \ldots, T_k$}
    \State Randomly sample the direction $u^i_{k,t}$ from $\mathbb{S}_i$
    \State $\hat{X}^{i}_{k+1/2,t} \leftarrow (1 - \frac{\delta_k}{r^i})X^{i}_{k+1/2,t} + \frac{\delta_k}{r^i}(p^i + r^i u^{i}_{k,t})$ 
    \State Take action $\hat{X}^{i}_{k+1/2,t}$
    \State Observe the realized objective function value $\hat{J}^{i}_{k,t} \coloneqq J^i(\hat{X}^{i}_{k+1/2,t}; \hat{X}^{-i}_{k+1/2,t})$
\EndFor
\State $G^i_k \leftarrow \frac{n^i}{\delta_kT_k}\sum_{t=1}^{T_k}(\hat{J}^{i}_{k,t} - \hat{J}^{i}_{k,0})u^{i}_{k,t} = \frac{1}{T_k}\sum_{t=1}^{T_k}G^{i}_{k,t}$
\State $X^i_{k+1} \leftarrow P_{X^i_k, \mathcal{X}^i}(-\tau G^i_{k})$
\EndProcedure
\State \textbf{Return:} $\{\hat{X}^i_{k+1/2}\}_{i \in \playerN}$
\end{algorithmic}
\end{algorithm}

% Extended Robbins-Siegmund Theorem
The Robbins-Siegmund (R-S) theorem serves as a heavy-lifting tool in the field of stochastic optimization to examine the convergence of sequences.
Its formal statement is presented as follows. 

\begin{lemma}(\cite[Thm.~1]{robbins1971convergence})
Let $(\Omega, \mathcal{F}, \mathcal{P})$ be a probability space and $(\mathcal{F}_k)_{k}$ a filtration of $\mathcal{F}$. 
For each $k = 1, 2, \ldots$, $Z_k$, $\beta_k$, $\xi_k$, and $\zeta_k$ are non-negative $\mathcal{F}_k$-measurable random variables that satisfy $\expt{}{Z_{k+1} \mid \mathcal{F}_k} \leq (1 + \beta_k)Z_{k} + \xi_k - \zeta_k$. 
If $\sum_{k \in \nset{}{+}} \beta_k < \infty$ a.s. and $\sum_{k \in \nset{}{+}} \xi_k < \infty$ a.s., then $\lim_{k \to \infty} Z_k$ exists and is finite a.s. and $\sum_{k \in \nset{}{+}} \zeta_k < \infty$ a.s.
\end{lemma}

To employ the theorem, it is necessary to guarantee that $\sum_{k \in \nset{}{+}}\xi_k$ is finite a.s. 
Recall from Lemma~\ref{le:variance}, in the variance reduction scenario, the decaying upper bound is constructed for $\expt{}{\norm{V_k}^2 \mid \mathcal{F}_k}$ rather than the random variable $\norm{V_k}^2$. 
In the meantime, unlike the typical extra-gradient method, OMD leverages the pseudo-gradient $F(X_{k-1/2})$ from the last iteration when updating to the leading state $X_{k+1/2}$. 
This approximation brings the stochastic error $\norm{V_{k-1}}^2$ into the recurrent inequality which, due to the absence of the averaging effect, does not possess a decaying upper bound and prevents us from applying the R-S theorem. 
Motivated by the consideration above, our next step will be establishing a variant of the R-S theorem by relaxing the condition imposed upon the sequence $(\xi_k)_{k \in \nset{}{+}}$.  

\begin{theorem}\label{thm:ext-rs}
Let $(\Omega, \mathcal{F}, \mathcal{P})$ be a probability space and $(\mathcal{F}_k)_{k}$ a filtration of $\mathcal{F}$. 
For each $k = 1, 2, \ldots$, $Z_k$, $\xi_k$, and $\zeta_k$ are non-negative $\mathcal{F}_k$-measurable random variables that satisfy $\expt{}{Z_{k+1} \mid \mathcal{F}_k} \leq Z_{k} + \xi_k - \zeta_k$ with $\expt{}{Z_1} < \infty$. 
If $\sum_{k \in \nset{}{+}} \expt{}{\xi_k} < \infty$, then $Z_k$ converges a.s. to some random variable $Z_{\infty}$ with $\expt{}{Z_{\infty}} < \infty$ and $\sum_{k \in \nset{}{+}} \zeta_k < \infty$ a.s.
\end{theorem}
\begin{proof}
\ifarxiv
See Appendix~\ref{pf:ext-rs}. 
\fi
\ifproceeding
See \cite[Appendix~C]{huang2023bandit}.
\fi
\end{proof}

With this conclusion available, we can establish the following results about the convergence of Algorithm~\ref{alg:vr-bdt-omd} and the sufficient conditions to guarantee it. 

\begin{theorem}\label{thm:convg}
Consider a multi-player game $\mathcal{G}$. 
Suppose that Assumptions~\ref{asp:objt-set} to \ref{asp:recip} hold.
In addition, the sequence of query radius $(\delta_k)_{k \in \nset{}{+}}$ and the sequence of the reciprocal of sample size $(1/T_k)_{k \in \nset{}{+}}$ are monotonically decreasing and satisfy
\begin{align}
\sum_{k \in \nset{}{+}} \delta_k < \infty, \;\sum_{k \in \nset{}{+}}1/T_k < \infty.
\end{align}
The step size $\tau$ satisfies $(\tau L /\Tilde{\mu})^2 \leq 1/12$. 
Then the base state $(X_{k})_{k \in \nset{}{+}}$ as well as the leading state $(X_{k+1/2})_{k \in \nset{}{+}}$ converge a.s. to a CP $x_*$ of $\mathcal{G}$. 
Moreover, the actual sequence of play also satisfy $\lim_{k \to \infty} \hat{X}_{k+1/2,t} = x_*$ a.s., for arbitrary sample $t$. 
\end{theorem}
\begin{proof}
\ifarxiv
See Appendix~\ref{pf:convg}. 
\fi
\ifproceeding
See \cite[Appendix~D]{huang2023bandit}.
\fi
\end{proof}

% Main convergence results

\section{NUMERICAL EXPERIMENTS}
\subsection{The Rock-Paper-Scissors (RPS) Game}
Consider the zero-sum rock-paper-scissors game between two players. 
The payoff matrices $A^a$ and $A^b$ of player $a$ and $b$ are set respectively as
\begin{align*}
A^a \coloneqq \begin{bmatrix} 0 & -1 & 1 \\ 1 & 0 & -1 \\ -1 & 1 & 0 \end{bmatrix}
\;\text{and}\;
A^b \coloneqq \begin{bmatrix} 0 & 1 & -1 \\ -1 & 0 & 1 \\ 1 & -1 & 0 \end{bmatrix} = -A^a, 
\end{align*}
which further give rise to the objective functions: $J^a(x^a; x^b) = -(x^a)^TA^ax^b$ and $J^b(x^b; x^a) = -(x^a)^TA^bx^b$. 
The associated strategy spaces are the probability simplices, i.e., $\mathcal{X}^a = \mathcal{X}^b \coloneqq \{x \in \rset{3}{} \mid 0 \leq x \leq 1, \bone^T x = 1\}$. 
The RPS game is merely monotone and admits a unique CP/NE at $[1/3; 1/3; 1/3]$ for both players. 
To fulfill the requirement about the non-empty interior in Assumption~\ref{asp:objt-set}, taking player $a$ as an example, we can employ a simple coordinate transformation $\varphi: \rset{2}{} \to \rset{3}{}$ with $\varphi: [y_1; y_2] \mapsto [y_1; y_2; 1 - y_1 - y_2] = [x_1; x_2; x_3]$ and $\varphi^{-1}(\mathcal{X}^a) = \Tilde{\mathcal{X}}^a = \{y \in \rset{2}{} \mid 0 \leq y \leq 1, \bone^T y \leq 1\}$. 
Then \ref{eq:mpg} is applied to obtain a pseudo-gradient estimate $\Tilde{G}_k \in \rset{2}{}$, and we use another map $\phi: \rset{2}{} \to \rset{3}{}$ to pull the pseudo-gradient from $y$-coordinate system back to $x$-coordinate system. 
The map $\phi$ is defined as $\phi: [\Tilde{g}_1; \Tilde{g}_2] \mapsto [2/3 \Tilde{g}_1 - 1/3\Tilde{g}_2; -1/3\Tilde{g}_1 + 2/3\Tilde{g}_2; -1/3\Tilde{g}_1-1/3\Tilde{g}_2]$, which is derived from the observation that  
\begin{align*}
    \Tilde{g}_i = \sum_{j=1,2,3}\frac{\partial J^a}{\partial x_j}\cdot \frac{\partial x_j}{\partial y_i} = \sum_{j=1,2,3} g_j\cdot \frac{\partial x_j}{\partial y_i} \;\text{and}\;\sum_{j=1,2,3}g_j = 0. 
\end{align*}
A similar procedure can be applied to player $b$ to guarantee the fulfillment of the assumption. 

In the numerical simulation, we choose $\tau = 0.1$, the decaying query radius $\delta_k = 0.1 \times (k + 20)^{-1.1}$, and the increasing number of queries per iteration $T_k = \ceil{10^{-3} \times k^{1.1} + 20}$. 
Since the negative entropy $h(x) = \sum_{i=1,2,3} [x]_i\log[x]_i$ is $1$-strongly convex in $\norm{\cdot}$ and satisfies all the requirements discussed in Sec.~\ref{subsec:omd}, it can be chosen as a DGF for player $a$ and $b$. 
The simulation results are illustrated in Fig.~\ref{fig:rps}, with Fig.~\ref{fig:rps} (a) and (b) visualizing the actual sequences of play of player $a$ and $b$.
To compare with \cite{gao2022bandit} (MD2-rb), Fig.~\ref{fig:rps} (c) and (d) illustrate the relative distance $\norm{\hat{X}_{k+1/2} - x_*}/\norm{x_*}$ to the CP/NE $x_*$, where the $x$-axis denotes the sample count and iteration index, respectively. 
The selection of parameters for \cite{gao2022bandit} (MD2-rb) adheres to the specifications provided in its corresponding section. 
As depicted in the figure, \cite{gao2022bandit} (MD2-rb) displays a faster decline in the early iterations, whereas Algorithm~\ref{alg:vr-bdt-omd} achieves a superior convergence rate as the progress advances. 
\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figs/rps_vared_1673374001.png}
    \caption{Performance of Algorithm~\ref{alg:vr-bdt-omd} in the RPS Game}
    \label{fig:rps}
\end{figure}

\subsection{Least Square Estimation in Linear Models}
In this numerical experiment, we convert the linear regression to a zero-sum bilinear game between two players \cite[Sec.~VI]{gao2020continuous}. 
Given a set of data samples $\{(z_j, y_j)\}_{j=1}^{M}$ where $z_j \in \rset{N}{}$ and $y_j \in \rset{}{}$ represent the input feature vector and the output scalar, respectively. 
In addition, $y_j = w_0 + w^Tz_j + \xi_j$, with $\Tilde{w} \coloneqq [w_0; w] \in [-\bar{w}, \bar{w}]^{N+1} \subseteq \rset{N+1}{}$ denoting the parameters to be determined and $\xi_j$ some random noise. 
Here, the region $[-\bar{w}, \bar{w}]^{N+1}$ with $\bar{w} \in \rset{+}{}$ is enforced to ensure the strategy space is bounded. 
For brevity, denote $\Tilde{z}_j \coloneqq [1; z_j]$, $\Tilde{Z} \coloneqq [\Tilde{z}_1, \ldots, \Tilde{z}_M]$ and $y = [y_1; \cdots; y_M]$. 
We can then formulate this least square estimation problem as: 
\begin{align}\label{eq:lse-obj}
\underset{\Tilde{w} \in [-\Tilde{w}, \Tilde{w}]^{N+1}}{\minimize} \; \frac{1}{2}\norm{\Tilde{Z}^T\Tilde{w} - y}^2_2. 
\end{align}
To convert it into a two-player game, we leverage an auxiliary variable $\lambda \in \rset{M}{}$ and the fact that $\frac{1}{2}\norm{\Tilde{Z}^T\Tilde{w} - y}^2_2 = \max_{\lambda \in \rset{M}{}}\lambda^T(\Tilde{Z}^T\Tilde{w} - y) - \frac{1}{2}\norm{\lambda}^2_2 = \max_{\lambda \in \rset{M}{}} J(\Tilde{w}, \lambda)$. 
Taking the boundedness of $\Tilde{w}$ into account, it can be asserted that there exists a bounded set $[-\bar{\lambda}, \bar{\lambda}]^M$ such that the solution $\lambda$ to the maximization problem above satisfies $\lambda \in [-\bar{\lambda}, \bar{\lambda}]^M$. 
As such, let $J^1(x^1; x^2) = J(x^1, x^2)$ and $J^2(x^2; x^1) = -J(x^1, x^2)$, and this game can be formulated as follows: 
\begin{align*}
\text{Player 1: }\underset{-\Bar{w} \leq x^1 \leq \Bar{w}}{\minimize}\; J^1(x^1; x^2), \; \text{Player 2: }\underset{-\Bar{\lambda} \leq x^2 \leq \Bar{\lambda}}{\minimize}\; J^2(x^2; x^1). 
\end{align*}
For the verification of the remaining assumptions, showing the uniqueness of the CP, and other detailed discussions, we refer the interested reader to \cite[Sec.~V-B]{huang2023zeroth}\cite[Sec.~VI]{gao2020continuous}. 

When implementing the experiments, we choose $N = 5$, $M = 20$, and $\bar{w} = \bar{\lambda} = 5$. 
Then random noise $\xi_i$ is uniformly distributed over the interval $[-0.6, 0.6]$. 
We compare different sets of the sequences of query radius $\delta_k$ and query samples per iteration $T_k$. 
In Fig.~\ref{fig:lse} (a), the original curve to fit, the noisy data samples used, the optimal solution that can be procured from the existing data, and one OMD solution generated by Algorithm~\ref{alg:vr-bdt-omd} are illustrated. 
Comparing the results with different choices of $\delta_k$, we note that for this problem when $\delta_k$ decays comparable to or faster than $O(k^{-1.1})$, further increasing the decaying rate contributes little to speed up the convergence rate of the sequence. 
As for the influence of different $T_k$, when $T_k$ is a small constant, the generated sequences will diverge; 
when $T_k = 10$ increases to some sufficiently large constant $T_k=15$, the associated sequences demonstrate the trend of convergence to some $\epsilon$-neighborhood of the CP; 
when $T_k$ decays no slower than $O(k^{-1.1})$, as reflected in Fig.~\ref{fig:lse} (c) and (e), the fluctuations of the relative step sizes are mitigated; yet little difference can be observed regarding the relative distance to the CP, as shown in Fig.~\ref{fig:lse} (b) and (d). 

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figs/linear_est_res_1677080300.png}
    \caption{Performance of Algorithm~\ref{alg:vr-bdt-omd} in the Least Square Estimation: \Tblue{in Panel (a), the optimal solution is obtained by solving \eqref{eq:lse-obj} analytically; the OMD solution corresponds to the case when $\delta_k = O(k^{-1.1})$ and $T_k = \ceil{0.1 \times (k+50)^{-1.1}}$}; Panel (b) and (d) visualize the relative distance to the unique CP, i.e., the metric is given by $\norm{\hat{X}_{k+1/2} - x_*}_2/\norm{x_*}_2$; Panel (c) and (e) report the relative updating step sizes per iteration, i.e., $\norm{\hat{X}_{k+1/2} - \hat{X}_{k-1/2}}_2/\norm{\hat{X}_{k-1/2}}_2$. The rolling averages with a window size of 100 and the original fluctuations are illustrated in solid curves and semi-transparent curves, respectively. }
    \label{fig:lse}
\end{figure}

\section{CONCLUSION}
In this work, we investigate bandit learning in multi-player continuous games with an emphasis on handling merely coherent cases.  
A new learning algorithm is proposed, by integrating the idea of optimistic mirror descent and multi-point pseudo-gradient estimation. 
Under the assumptions posited and the conditions that the sequences of query radius $\delta_k$ and the reciprocal of sample size $T_k$ are absolutely summable, the actual sequence of play generated by the proposed algorithm is shown to converge a.s. to a CP of the game. 
There are several potential directions for future exploration. 
The first one is relaxing the requirements for the number of samples per iteration $T_k$, since the superlinear growth of $T_k$ may prevent the application of the proposed algorithm when the bandit feedback is inadequate. 
Furthermore, when it comes to a large-scale player network, the asynchronicity of the updates is a prevalent issue and the cost of synchronization is prohibitive, which is further exacerbated by the multi-point scheme considered. 
We intend to address these questions in future work.

\ifarxiv
\appendices 
\input{appendix}
\fi

%%%%%%%%%%%%%%%%%%
% References
%%%%%%%%%%%%%%%%%%
% \nocite{*}
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}


\end{document}