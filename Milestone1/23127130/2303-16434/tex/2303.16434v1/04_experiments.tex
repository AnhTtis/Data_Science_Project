\section{Case Study}
\label{sec:case_study}
To demonstrate how TaskMatrix.AI operates, we will take PowerPoint automation as a case study. We aim to use general methods for each module so that the whole pipeline can be easily adapted to other situations. We will explain how we implement MCFM, the API platform, the action executor, and the feedback to API developers in the next subsections. We skip the API selector, because we only need a few PowerPoint APIs for this case. We also postpone RLHF for future work, since we do not have enough user feedback at this point.

\subsection{Multimodal Conversational Foundation Model}
We utilize ChatGPT\footnote{https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt} as the MCFM in this scenario. The inputs to the MCFM include the API platform, conversational context, and general prompts. The general prompts aim to align the MCFM with the needs of TaskMatrix.AI, by instructing the model to follow user instructions, generate action codes using defined APIs, and specify the output format.

The PowerPoint content includes the text, position of each text box, images, and other shapes on each slide. With textual content, the MCFM can process textual queries such as "insert the corresponding company logo onto each slide" without requiring specific company names in user instructions. With visual content, the MCFM is capable of resizing and moving the image based on its current size and position.


Since ChatGPT cannot process images, we utilize PowerPoint APIs to parse the visual content into structured textual data. To extend this capability to other software, we can access the operating system APIs or employ tools like ScreenParser ~\citep{wu2021screenparser} to parse elements on the screen. In the future, we can leverage GPT-4 to directly comprehend image inputs. The PowerPoint API we use is the python-pptx package\footnote{https://python-pptx.readthedocs.io/}, which provides the textual content and relative position of each text box and image. We employ a light rectangle detection model to identify the area that displays the current slide. Then, we calculate the position of each shape on the screen and leverage mouse APIs to move and resize objects, similar to how humans interact with the interface. An example of parsed PPT content is shown in Figure \ref{fig:ppt_content_reader}.

\input{case_study_codes/ppt_content_reader}
\definecolor{responsecolor}{gray}{0.85}


\subsection{API Platform}
\label{sec:case_study_api_platform}
This subsection demonstrates the construction of the API platform for PowerPoint and how to teach TaskMatrix.AI to utilize these APIs. Previous research ~\citep{vemprala2023chatgpt_robotics, wu2023visualchatgpt} has demonstrated the importance of API names, descriptions, and parameter lists in enabling correct API usage. In this study, we emphasize the importance of composition instructions for composing multiple APIs to complete complex user instructions. This is demonstrated through subsequent ablation studies.


In Figure \ref{fig:ppt_case1}, we demonstrate how TaskMatrix.AI can be instructed to generate multiple slides, each corresponding to a different company. The API platform for PowerPoint consists of a list of APIs, each accompanied by its name, parameter list, description, and composition instructions, as detailed in Section \ref{sec:overview_api_platform}. These properties are summarized in a single paragraph for ease of understanding. We highlight the composition rules with light green.



\input{case_study_codes/ppt_case1}

In this case, the API documentation provides the necessary APIs to accomplish this task. The user doesn't provide the company list but it can be obtained from the content of PowerPoint. Since this is a complex instruction, TaskMatrix.AI must break it down into roughly 25 API calls to complete the task. We omit the step of generating a solution outline, as ChatGPT can accurately generate it directly.

The process of decomposing user instructions may vary depending on the API design being used. Thus, it is essential for API developers to provide composition instructions to guide API usage. We have created composition instructions for three APIs: \pyth{insert_text}, \pyth{select_title}, and \pyth{select_content}. The composition instructions for \pyth{insert_text} specify that the content often contains multiple sentences. For \pyth{select_title} and \pyth{select_content}, the composition instructions specify the order in which these APIs should be used with other APIs, such as inserting and deleting text in a text box. These composition instructions can cover multiple combination examples in a single sentence, making them more efficient than in-content learning, which would require multiple examples to cover all these combinations.





\input{case_study_codes/ppt_case3}

Figure \ref{fig:ppt_case3} illustrates the results obtained by removing all composition instructions. The model produces a lengthy paragraph without any line breaks. Conversely, the results in Figure \ref{fig:ppt_case1} contain content with multiple lines separated by \pyth{"\n"}, and the model calls \pyth{insert_text} before selecting the target text box. In our experiments, we added irrelevant APIs, changing the prompt to MCFM, and found that there is no consistent order for inserting text and selecting the text box. However, including the composition instructions for \pyth{select_title} and \pyth{select_content} ensures that MCFM always selects the target text box before inserting the text.

Throughout our experiments, we also observed that the model was sometimes able to execute user instructions correctly, but the results were not consistent. This inconsistency may be due to the fact that the model encountered similar knowledge during pre-training, especially for long-standing scenarios that existed prior to the foundation model pre-training. However, we strongly encourage API developers to include composition instructions in order to improve stability, particularly when designing new APIs after foundation model pre-training.

\subsection{Action Executor}

We utilized the mouse and keyboard API to execute the action codes, as it's a universal method for manipulating the operating system. Specifically, in our experiments, we chose to use the APIs provided in the PyAutoGUI package\footnote{https://pyautogui.readthedocs.io/} for Python. We used the keyboard API to control functions in PowerPoint that have keyboard shortcuts available. When the user clicks the Alt key, PowerPoint displays a shortcut instruction for each button, enabling the user to select the corresponding button using the keyboard. An example is shown in Figure \ref{fig:keyboard_for_ppt}. We also used the mouse API to resize and move images and shapes. The pixel position to operate on was calculated based on the detected position of the area that shows the current slide and the related position of each element provided by the PowerPoint API.


\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/keyboard_for_ppt.png}
    \caption{A screenshot of PowerPoint software. When using PowerPoint software, you can press the "alt" key to display one or several characters on each button. This allows you to select the corresponding button using the keyboard.}
    \label{fig:keyboard_for_ppt}
\end{figure}


\subsection{Feedback to API Developers}
\label{sec:feedback_to_api_doc}
API developers can modify the API documentation based on user feedback, including cases where the API fails to execute user instructions. Such cases can provide valuable information for improving API documentation. In this work, we demonstrated that a powerful foundation model, such as ChatGPT, can induce general knowledge in natural language based on specific cases and automatically improve API documentation. An example is shown in Figure \ref{fig:ppt_case4}.
\input{case_study_codes/ppt_case4}

Specifically, we utilized ChatGPT with inputs consisting of <user instruction, wrong API calls, correct API calls>, where the correct API calls were obtained from user demonstrations. We provided the API documentation without any composition instructions and instructed the foundation models to summarize the feedback on using APIs. Based on the results, we found that the first two items of feedback were highly related to our composition instructions, while the other feedback items were specific to this particular case. API developers can select the first two feedback items and add them to the API documentation. This process can also be automated by incorporating a separate module to test each feedback item individually and adding them when the test results demonstrate improvement.