\section{TaskMatrix.AI Architecture}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Figures/overview.pdf}
    \caption{Overview of TaskMatrix.AI. Given user instruction and the conversational context, the multimodal conversational foundation model (MCFM) first generates a solution outline (step \textcircled{1}), which is a textual description of the steps needed to solve the task. Then, the API selector chooses the most relevant APIs from the API platform according to the solution outline (step \textcircled{2}). Next, MCFM generates action codes using the recommended APIs, which will be further executed by calling APIs. Last, the user feedback on task completion is returned to MCFM and API developers.}
    \label{fig:overview}
    %\vspace{-6mm}
\end{figure*}

\subsection{Overview}

The overall architecture (Figure 1) of TaskMatrix.AI consists of the following four key components: (1) \textbf{Multimodal Conversational Foundation Model (MCFM)}, which is responsible for communicating with users, understanding  their goals and (multimodal) contexts, and generating executable codes based on APIs to accomplish specific tasks. (2) \textbf{API Platform}, which provides a unified API documentation schema to store millions of APIs with different kinds of functions and allows API developers or owners to register, update and delete their APIs. (3) \textbf{API Selector}, which can recommend related APIs based on MCFM’s comprehension of the user command. (4) \textbf{API Executor}, which can execute the generated action codes by calling the relevant APIs and return the intermediate and final execution results. 

The key procedure in this architecture is the MCFM's ability to generate action codes based on user instructions. We formulate this procedure as:
\begin{equation}
    \mathcal{A}=\mathrm{MCFM}(\theta, \mathcal{P}, \mathcal{I}, \mathcal{C})
\end{equation}
The MCFM takes four inputs: the parameter of the foundation model, denoted as $\theta$; the API platform, denoted as $\mathcal{P}$; the user instruction, denoted as $\mathcal{I}$; and the conversational context, denoted as $\mathcal{C}$. Using these inputs, the model generates action codes, denoted as $\mathcal{A}$, to accomplish the user's instruction. 

TaskMatrix.AI also provides two learnable mechanisms to align MCFM with APIs in a better way. Both these two mechanisms need user feedback, denoted as $\mathcal{F}$. We calculate the loss function, denoted as $\mathcal{L}(\mathcal{A}, \mathcal{F})=\mathcal{L}(\mathrm{MCFM}(\theta, \mathcal{P}, \mathcal{I}, \mathcal{C}), \mathcal{F})$. This loss function can then be optimized by tuning the parameter of the foundation model, $\theta$, and updating the document in the API platform, $\mathcal{P}$. First, the output signals can be used by the \textbf{Reinforce Learning from Human Feedback (RLHF)} mechanism to enhance MCFM’s skill in API comprehension and action code generation from user commands, as well as the retrieval performance of API selector. We represent it as $min_{\theta} \mathcal{L}(\mathrm{MCFM}(\theta, \mathcal{P}, \mathcal{I}, \mathcal{C}), \mathcal{F})$. Second, the output signals can be also used as the \textbf{Feedback to API Developers}, for them to improve the documentation of APIs and make them easier to understand and called by the MCFM. We represent it as $min_{\mathcal{P}} \mathcal{L}(\mathrm{MCFM}(\theta, \mathcal{P}, \mathcal{I}, \mathcal{C}), \mathcal{F})$.


\subsection{Multimodal Conversational Foundation Model (MCFM)}

An ideal MCFM should have four main capabilities: (1) It should be able to take multimodal inputs and contexts (such as text, image, video, audio, and code) and generate executable codes based on APIs that can complete specific tasks. Most existing multimodal models (e.g., CLIP and Flamingo) are not suitable for this task as they can only encode different modalities but lack the conversational ability and code-generation skills. ChatGPT is a model that can understand language and conversation well and generate code accordingly, but it only works with text and code modalities. GPT-4 is the most suitable model until now, as it can deal with multimodal inputs and generate both text and code as outputs. (2) It should be able to extract specific tasks from user instructions and propose reasonable solution outlines (as shown in Figure 1) that can help select the most relevant APIs for code generation. Both ChatGPT and GPT-4 have this capability as they were pre-trained on both text and code corpora, which gives these two models strong knowledge to reason and plan. (3) It should be able to quickly learn how to use APIs from their documentation and match them to specific tasks based on common sense and API usage history. (4) It should incorporate an explicit code verification mechanism to confirm the reliability and trustworthiness of the generated codes.

With these capabilities, MCFM is involved in two primary steps (illustrated as step \textcircled{1} and step \textcircled{2} in Figure 1). First, step \textcircled{1} takes each user instruction and the corresponding conversational context as input and generates a solution outline. Users often use brief expressions to convey their high-level task intentions, so MCFM generates a more comprehensive textual description of the steps required to complete the task by leveraging its deep understanding of world knowledge. Users can then actively edit the outline during the conversation. MCFM can also edit the outline in cases where there is no suitable API to fulfill a particular step, or where the result of a certain step fails to satisfy the user's instructions. However, if the user's instructions already provide sufficient details for task completion, this step can be skipped. Second, after the API selector takes the solution outline as input and retrieves related APIs, step \textcircled{2} generates the code of action using the selected APIs. Here, MCFM must support acting with a dynamic action space, as developers continuously upload and modify APIs, and the retrieval results vary for different user instructions. While generating a sequence of actions is often sufficient, incorporating action codes as generation results can enhance the expression capacity.

\subsection{API Platform}
\label{sec:overview_api_platform}

The API platform has two main functions: first, it provides storage for different types of APIs that MCFM can access; second, it allows API developers or owners to manage their APIs by registering, updating, or deleting them.
To help MCFM understand and utilize APIs better, the API platform specifies a unified API documentation schema, which consists of five aspects for each API document:


\begin{itemize}
    \item \textbf{API Name}: The API name provides an abstract of the API. It helps MCFM to link user instructions to this API and serves as an entry for the action executor. The name should be clear and precise in natural language and avoid ambiguity with other API names. 
    \item \textbf{Parameter List}: The parameter list for an API includes the input parameters and return value, and each parameter has a parameter name, parameter description, data type, and default value. This information can assist MCFM in correctly filling the parameters in the corresponding positions with the appropriate format.
    \item \textbf{API Description}: Compared to the API name, the API description contains more information about what the API does, how it works, what are its inputs and outputs, and any potential errors or exceptions that may be raised.
    \item \textbf{Usage Example (Optional)}: Providing a usage example for complex APIs can help demonstrate how the API can be used, while it may not be necessary for simple APIs.
    \item \textbf{Composition Instructions (Optional)}: Developers who offer a package of APIs could provide composition instructions. This can serve as guidance to the model on how to combine multiple APIs to accomplish complex user instructions. For instance, in a file editing scenario, the model may need to open the target file before making edits and then save the file after completing the edits.
\end{itemize}
We provide an example of an API document for opening a file, which is a simplified version of $open$ in python.\\

\input{case_study_codes/API_document_case}


\subsection{API Selector}

The goal of API selector is to identify and select the most suitable APIs from API platform that fit the task requirement and solution outline as understood by MCFM. Since the API platform may have millions of APIs, the API selector needs the search capability to retrieve semantically relevant APIs. The API selector can also leverage a module strategy to quickly locate relevant APIs. Each package corresponds to a specific domain, such as a package of visual models, math, specific software, or specific physical devices.


\subsection{Action Executor}

The action executor is designed to execute the action codes. TaskMatrix.AI uses an action executor to run various APIs, ranging from simple HTTP requests to complex algorithms or AI models that need multiple input parameters. After the execution, the action executor will return the results to users. To enhance accuracy and reliability, the action executor also requires a verification mechanism to confirm whether the generated code or outcomes satisfy the tasks specified in human instructions.

\subsection{Reinforcement Learning with Human Feedback (RLHF)}

RLHF is a general technique that uses reinforcement learning methods to optimize machine learning models based on human feedback. It has been successfully used to align large models trained on the general corpus of text data with user instructions, such as InstructGPT \citep{ouyang2022training}.

In TaskMatrix.AI, we use RLHF to benefit from the knowledge and insight of human feedback to enhance MCFM and API selector. This can result in faster convergence and better performance of TaskMatrix.AI on complex tasks.
Specifically, we leverage human feedback to train a reward model that can classify whether the task has been completed. During RLHF training, MCFM and API selector can explore various strategies to plan solution outlines, select and compose APIs, and the reward model can provide feedback. Using RLHF, MCFM and API selector can optimize their policy and discover better ways to accomplish tasks. 

\subsection{Feedback to API Developers}
After TaskMatrix.AI has performed a specific task, the user feedback will be delivered to the API developers in an appropriate manner to indicate whether their APIs have been successfully used to complete the task or not. Such <user instruction, API calls, user feedback> triples can serve as either demonstration examples of how to use specific API, or as guidance for API developers to improve the API documentations to make them more understandable for MCFM and API selector.

Specifically, we treat the API documentation as learnable parameters, similar to the parameters of MCFM. User feedback can help the developer to understand how well the API works during different inputs and when combined with different APIs. This step can also be aided by a model, such as ChatGPT, that takes human feedback as input and generates natural language suggestions to improve the API documentation. We provide an example in Section \ref{sec:feedback_to_api_doc}.
