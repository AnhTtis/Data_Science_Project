\section{Application Scenarios}

In this section, we present some examples of how TaskMatrix.AI can be applied in different application scenarios. We show how TaskMatrix.AI can assist in creating AI-powered content in Section \ref{sec:visualchatgpt} and \ref{sec:complex_aigc}. We demonstrate how TaskMatrix.AI can facilitate office automation and cloud service usage in Section \ref{sec:ppt} and \ref{sec:cloud}. We illustrate how TaskMatrix.AI can perform tasks in the physical world by interacting with robots and IoT devices in Section \ref{sec:iot}. \textit{All these cases have been implemented in practice and will be supported by the online system of TaskMatrix.AI, which will be released soon.} We also explore more potential applications in Section \ref{sec:more}.

\subsection{Visual Task Completion}
\label{sec:visualchatgpt}

\begin{figure*}
    \centering
\includegraphics[width=1.0\textwidth]{Figures/visual_chat_gpt.pdf} 
\caption{Multiple rounds of dialogue between human and Visual ChatGPT~\citet{wu2023visualchatgpt}. In the dialogues, Visual ChatGPT, an initial version TaskMatrix.AI, can understand human intentions, support the language and image inputs, and provide complex visual tasks such as generation, question, and editing.} 
\label{fig:FullCase}
\end{figure*}

TaskMatrix.AI enables the user to interact with AI by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. More details are described at ~\citet{wu2023visualchatgpt}.
We demonstrate this with an example in Figrue~\ref{fig:FullCase}. The APIs related to this include:


\begin{figure*}
    \centering
\includegraphics[width=0.95\textwidth]{Figures/OutCase.pdf} 
\caption{An image outpainting example. In this example, we define a solution outline using three APIs: Image Question Answering, Image Captioning, Replace Objects from Image. By iteratively get captions of an image and ask LLM to imagine and replace the surroundings of it, we finally get a high-resolution image of $2048\times 4096$.} 
\vspace{-5mm}
\label{fig:OutCase}
\end{figure*}



\begin{itemize}
    \item \textbf{Image Editing} Image Editing includes removing or replacing objects of an image, or changing the style of an image. Removing objects from an image involves using image editing tools or algorithms to get rid of unwanted elements. On the other hand, replacing objects with new ones involves swapping out an element in an image with another one that is more suitable. Finally, changing an image using text involves using machine learning algorithms to generate an image based on a textual description.
    \item \textbf{Image Question Answering}  This refers to the process of using machine learning algorithms to answer questions about an image, often by analyzing the contents of the image and providing relevant information. This can be useful in situations where the image contains important information that needs to be extracted.
    \item \textbf{Image Captioning} This refers to the process of using machine learning algorithms to generate textual descriptions of an image, often by analyzing the contents of the image and providing relevant information. 
    \item \textbf{Text-to-Image} This refers to the process of generating an image from a textual description, often using machine learning algorithms that can generate realistic images based on textual input. 
    \item \textbf{Image-to-Sketch/Depth/Hed/Line}  This refers to the process of converting an image to a sketch, depth, Hed (Holistically-nested edge detection), or line, often using image processing techniques or computer algorithms.
    \item \textbf{Sketch/Depth/Hed/Line-to-Image} This refers to the process of generating an image from a sketch, depth, Hed (Holistically-nested edge detection), or line.

\end{itemize}
In the Fig.~\ref{fig:FullCase} example, TaskMatrix.AI is capable of understanding images as inputs and generating images, while the MCFM in this implementation is ChatGPT, which can only process text. It demonstrates the ability to extend to more modalities without retraining the MCFM. Additionally, TaskMatrix.AI can compose multiple APIs to accomplish complex user intentions, including tasks such as generation, questioning, and editing. This approach provides more interpretability and controllability than an end-to-end model. 

Fig.~\ref{fig:OutCase} illustrates an example of high-resolution image generation, where multiple APIs collaborate to produce the final result. In this example, we define a solution outline consisting of three APIs: Image Question Answering, Image Captioning, and Replace Objects from Image. The left dashed box in Fig.~\ref{fig:OutCase} demonstrates how the solution outline assists in extending an image to a $2048\times 4096$ resolution. First, the Image Question Answering API is employed to identify two crucial features of an image: the background color and the style. These answers are essential for extension, as the expanded image should maintain the background color and style of the original image. Second, the image captioning model is utilized to obtain a description of the image, which provides fundamental information about the original image. Third, a multi-modal conversational foundation model is used to merge all the obtained information and envision the surrounding descriptions of the image. Fourth, an Image Editing API (Replace Objects from Image) is employed to substitute the surrounding unknown regions with the envisioned descriptions. By iteratively executing the four pre-defined steps in the solution outline, TaskMatrix.AI can generate a high-resolution image of any desired size (in this case, $2048\times 4096$).

\subsection{Multimodal Long Content Generation}
\label{sec:complex_aigc}
TaskMatrix.AI can help users to create multimodal long content including text and image elements. Motivated by the planning-based method in long text generation task\citep{wang2022language} aimed to improve coherence, an explicit planning process is involved to improve both textual and visual consistency in this multimodal content scenario. We show an example in Figure \ref{fig:complex_aigc_1} and Figure \ref{fig:complex_aigc_2}. TaskMatrix.AI can take high-level instructions as input, and generate a solution outline to accomplish this task. The planning enhanced MCFM automatically decomposes the task into small sub-tasks. And the solution outline in this scenario is a step-by-step proposal that could be interactively modified by the user in later conversation rounds. Based on the finalized proposal, then it can leverage API exposure and generate action codes that are capable of integrating different APIs in every single step. Finally, we can get the generated multimodal content with the action executor to run the action code. The APIs related to this example include:
\begin{itemize}
    \item \textbf{Search API} This API allows for retrieving information through a search engine. We name the API as \pyth{search_query}. Most search engines provide API for developers. 
    \item \textbf{Text-to-Image API} This API allows for generating an image from a textual description. We name the API as \pyth{text_to_image}. In our experiment, we leverage Midjourney V5\footnote{https://docs.midjourney.com/docs/models} to generate the image.
    \item \textbf{Large Language Model API} This API allows for text generation based on given prompts. We name the API as \pyth{llm_prompting}. In our experiment, we leverage the ChatGPT API from azure OpenAI service\footnote{https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt} to generate text throughout the whole process.
\end{itemize}

\begin{figure*}
    \centering
\includegraphics[width=1.0\textwidth]{Figures/complex_aigc1.pdf} 
\caption{Multiple rounds of dialogue between user and TaskMatrix.AI. TaskMatrix.AI can comprehend high-level user instructions and assist in generating structured, multimodal content including text and image elements. TaskMatrix.AI first generates a solution outline needed to accomplish the task, makes decisions on when and how to use the APIs, then generates code based on it. The solution outline and generated codes can be applied to similar tasks of generating multimodal long content.} 
\label{fig:complex_aigc_1}
\end{figure*}

\begin{figure*}
    \centering
\includegraphics[width=1.0\textwidth]{Figures/complex_aigc2.pdf} 
\caption{Multiple rounds of dialogue between user and TaskMatrix.AI. TaskMatrix.AI can comprehend high-level user instructions and assist in generating structured, multimodal content including text and image elements. TaskMatrix.AI first generates a solution outline needed to accomplish the task, makes decisions on when and how to use the APIs, then generates code based on it. The solution outline and generated codes can be applied to similar tasks of generating multimodal long content.} 
\label{fig:complex_aigc_2}
\end{figure*}
In this example, TaskMatrix.AI can accomplish complex user instructions by planning solution outlines first, making decisions on when and how to use the APIs, then generating the action code, and finally finishing an essay including text and image elements. To be noticed, the \textbf{Large Language Model API} is fully leveraged to generate not only text elements but also search keywords for \textbf{Search API} and text prompts for \textbf{Text-to-Image API}. During the experiment, we found that this could improve the controllability and quality of generated content. Furthermore, the solution outlines can be applied to similar user instructions. Users can also reuse the solution outlines as templates. Since solution outlines are entirely described in natural language which is now becoming a general interface for large language models, they could potentially become shared knowledge among humans and models.

\subsection{Office Automation}
\label{sec:ppt}
TaskMatrix.AI has the capability to comprehend user instructions received through voice and automate the operation of the software on computers and applications on phones, as well as the operating system. People often rely on using the mouse, keyboard, and fingers to perform various tasks and have a very heavy workload in accomplishing their complex goals. TaskMatrix.AI introduces a natural language interface and automates user instructions, thereby reducing the workload. With TaskMatrix.AI, users can easily use complex software without requiring extensive training, find the right features without searching, and adapt to software updates or new software with minimal effort. This can help to relieve humans from mundane work and allow them to focus on the creative aspects of their work and make high-level decisions.

We demonstrate this with an example of PowerPoint automation, shown in Figure ~\ref{fig:office_demo1} and Figure ~\ref{fig:office_demo2}. TaskMatrix.AI can help to create slides related to a specific topic, change the content, insert and adjust the images, and change themes. The details to implement this scenario is in Section \ref{sec:case_study}. The APIs related to this example include:
\begin{itemize}
    \item \textbf{Mouse and Keyboard API} To control PowerPoint, we utilize the mouse and keyboard API as it is a universal method to manipulate the operating system. This API is provided in the PyAutoGUI package\footnote{https://pyautogui.readthedocs.io/} of Python. % We use keyboard API to control most functions that have shortcuts available in PowerPoint. We use mouse API to resize and move images and shapes.
    \item \textbf{PPT File Reader API} The content provides essential information to understand user instructions. We utilize the python-pptx package\footnote{https://python-pptx.readthedocs.io/} to extract content from saved PPT files. The content includes the text on each page and the position of each text box, image, and other shapes. For other software, we can replace this package with operating system APIs or visual understanding models for more flexibility.
    \item \textbf{PowerPoint APIs} We leverage the APIs provided by PowerPoint software to control it, which include the APIs to create a new slide \pyth{create_slide}, select title and content before editing it \pyth{select_title, select_content}, insert text to a specific text box \pyth{insert_text}, move to a specific page \pyth{move_to_slide}, resize and move images \pyth{resize_picture, move_picture}. We also include several infrequently used functions like converting to smart art \pyth{convert_to_smart_art}, inserting images from the internet \pyth{insert_internet_picture}, and changing themes \pyth{change_theme}.

    
\end{itemize}
In this example, TaskMatrix.AI is capable of decomposing high-level instructions into multiple PowerPoint APIs. For instance, the third query requires 25 APIs to complete. Additionally, TaskMatrix.AI can understand user instructions based on PowerPoint content. For instance, it can generate five pages based on the company list on page 2 and insert a logo based on the title of each page, and it can determine the page index based on the user's coarse-grained command, such as "about five companies".


\begin{figure*}
    \centering
\includegraphics[width=1.0\textwidth]{Figures/ppt_generation_example1.pdf} 
\caption{Multiple rounds of dialogue between user and TaskMatrix.AI. TaskMatrix.AI can understand user instructions and operate PowerPoint on behalf of users. TaskMatrix.AI is capable of breaking down the user's complex instructions into multiple PowerPoint operations, assisting users in finding and using infrequent features, and generalizing the same patterns across multiple pages. While we display the API calls in a gray text box, this information is not necessary for the user.} 
\label{fig:office_demo1}
\end{figure*}

\begin{figure*}
    \centering
\includegraphics[width=1.0\textwidth]{Figures/ppt_generation_example2.pdf} 
\caption{More rounds of dialogue between user and TaskMatrix.AI. TaskMatrix.AI can accomplish the insert logo instruction by the insert internet feature of PowerPoint with API \pyth{insert_internet_image("Microsoft logo")}. This feature will provide multiple images for users. TaskMatrix.AI can take the user's instructions to select one of them. In the example, we omitted the selection steps for brevity.} 
\label{fig:office_demo2}
\end{figure*}
\subsection{Cloud Services Utilization}
\label{sec:cloud}
TaskMatrix.AI can help users to access the services on Cloud, which provide computing, storage, networking, analytics security, and more. Cloud services offer a multitude of APIs, and new APIs are constantly being developed. TaskMatrix.AI can understand these APIs and learn new APIs, then recommend appropriate APIs based on user instructions.

In Figure \ref{fig:azure_assistant}, we provide an example of how Azure Cloud APIs can assist users in building a personal conversation model. With this example, users can easily manage data, computing resources, model training, and deployment, even with minimal expertise. This scenario was initially conceived by New Bing, and most of the step-by-step knowledge about fine-tuning the model comes from the Azure OpenAI fine-tuning document \footnote{https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/fine-tuning\label{azure_open_ai}}. We define the APIs based on the related documents the test whether TaskMatrix.AI can link user instructions to the correct API and fill the parameters accurately. The APIs related to this example include:
\begin{itemize}
    \item \textbf{OpenAI's Data Preparation API \textsuperscript{\ref{azure_open_ai}}}: OpenAI's CLI (command-line interface) data preparation tools can validate, give suggestions, and reformat user's data into a JSONL file for fine-tuning. This tool accepts input in various formats such as Microsoft Excel workbooks, comma-separated values, JSON Lines, and others. We name this API as \pyth{data_preparation}.
    \item \textbf{Data Uploading API\textsuperscript{\ref{azure_open_ai}}}: Azure provides APIs that enable users to upload training data to the service from either a local file or Azure Blob Storage. Alternatively, users may choose to use previously uploaded data and skip this step. We name this API as \pyth{data_uploading}.
    \item \textbf{Model Listing API\footnote{https://learn.microsoft.com/en-us/rest/api/cognitiveservices/azureopenaistable/models}} The model list API will provide a list of all models that are accessible through the Azure OpenAI resource. Each model in the list includes its name and capabilities, such as whether it supports completion, inference, and fine-tuning. We name this API as \pyth{model_listing}.
    \item \textbf{Fine-tuning API\footnote{https://learn.microsoft.com/en-us/rest/api/cognitiveservices/azureopenaistable/fine-tunes/create}} The fine-tuning API can create a new fine-tuning job given the training dataset and base model. Users can specify the hyper-parameters of training, such as training epochs, batch size, and learning rate. We name this API as \pyth{fine_tune_model}.
    \item \textbf{Job Analyzing API\textsuperscript{\ref{azure_open_ai}}} The job analyzing API can provide the status of the fine-tuning process. The returned values include various information, such as the training step, training loss, and training sequence accuracy. We name this API as \pyth{job_analyzing}.
    \item \textbf{Model Deployment API \textsuperscript{\ref{azure_open_ai}}} The deployment API can create a model deployment for the fine-tuned model, which can then be used by the user like any other models provided by OpenAI. We name this API as \pyth{model_deployment}.
    \item \textbf{Speech-to-text and Text-to-speech API \footnote{https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/}} Speech-to-text API can convert audio to text from a range of sources, including microphones, audio files, and blob storage. Text-to-speech API can convert input text into humanlike synthesized speech. We name these APIs as \pyth{speech_to_text, text_to_speech}.
\end{itemize}

In this example, the user provides only high-level instruction. However, with detailed documentation for fine-tuning the model step-by-step, TaskMatrix.AI is able to manage the entire conversation and even can answer user questions like "what should I do first" and "what's next". This illustrates the powerful potential of composing instructions, which teaches TaskMatrix.AI to achieve high-level intents by composing multiple APIs.

\begin{figure*}
    \centering
\includegraphics[width=1.0\textwidth]{Figures/azure_assistant.pdf} 
\caption{TaskMatrix.AI assists users in building a personal conversation model using Azure Cloud APIs. TaskMatrix.AI can compose a solution with multiple Azure APIs to accomplish user instruction. It can help user to manage data, train models, check training status, and deploy models.} 
\label{fig:azure_assistant}
\end{figure*}

\subsection{Robotics and IoT Devices Controlling}
\label{sec:iot}
TaskMatrix.AI can help users to interact with the real world by instructing robots and IoT devices. MCFM can understand the environment with camera API, and transform user instructions to action APIs provided by robots and IoT devices. TaskMatrix.AI can facilitate the handling of physical work with the assistance of robots and the construction of smart homes by connecting IoT devices.

We show an example in Figure \ref{fig:smart_home}. TaskMatrix.AI can utilize the robotics described in PaLM-E~\citep{driess2023palme} and Microsoft Robotics ~\citep{vemprala2023chatgpt_robotics} to perform tasks such as picking and placing objects, controlling IoT devices in the home. In addition, several popular internet services, such as calendar API, weather API, and news API are included in this scenario. The robotics and IoT devices APIs related to this example include:
\begin{itemize}
    \item \textbf{Robotics Pick, Move and Put APIs} The robots described in PaLM-E and Microsoft Robotics are capable of finding objects, picking up, moving around, and placing objects using their robotic arms, we name them as \pyth{robot_find, robot_pick_up, robot_put_down, robot_go_to}. Although this example utilizes only four APIs, it's worth noting that robots have the potential to support many more APIs in the future.
    \item \textbf{Car Air Conditioner API} The car APIs allow for the control of various car devices, including the air conditioner, location services, and others. The air conditioner API enables users to remotely turn the air conditioner on or off and adjust the temperature settings. We name the APIs as \pyth{ac_open, ac_close, ac_set_temperature}.
    \item \textbf{TV API} The TV API enables users to remotely control their televisions by opening or closing them and playing specific videos. We name the APIs as \pyth{tv_open, tv_close, tv_play_video}. Most smart TVs support this function.
    \item \textbf{Music playing API} The music playing API is capable of playing music on speakers and can receive parameters to specify the name or style of the desired song. We name the API as \pyth{play_music}.
\end{itemize}

In this example, TaskMatrix.AI is able to connect user instructions to APIs of robots and IoT devices. We have pre-defined rules, such as the rule for activating the car's air conditioner which requires two conditions to be met: the temperature must be below 5 degrees Celsius and the user must be leaving. TaskMatrix.AI can accurately execute the API call when both conditions are satisfied. Additionally, the model is capable of correctly filling the parameter with the time "7:50", which is ten minutes before the user's departure.

\begin{figure*}
    \centering
\includegraphics[width=1.0\textwidth]{Figures/smart_home.pdf} 
\caption{TaskMatrix.AI in smart home scenarios. The user told TaskMatrix.AI several rules before the conversation began, and the conversation is triggered by "it's 7 am". TaskMatrix.AI is capable of integrating with various devices in the user's home, robots, cars, and accessing internet services. By combining these APIs, TaskMatrix.AI can assist users in controlling their devices using voice commands, scheduling items on their calendars, accessing the internet, and taking actions based on specific conditions.} 
\label{fig:smart_home}
\end{figure*}


\subsection{More Scenarios}
\label{sec:more}
TaskMatrix.AI can be applied to much more scenarios by connecting with their APIs. Here, we list three scenarios as examples:

\textbf{Accessing Internet} TaskMatrix.AI can assist users in accessing knowledge and services on the internet. For example, New Bing has successfully leveraged ChatGPT to generate search keywords and summarize search results. TaskMatrix.AI also can interact with internet services such as planning traveling, booking flights, finding products, and replying to emails. This has the potential to facilitate the development of the next-generation web browser and voice assistant.

\textbf{Accessing Metaverse} The Metaverse includes a blend of digital and physical worlds, and TaskMatrix.AI can help users access it in the same way they access operating systems and the internet in digital worlds, as well as robots and IoT devices in physical worlds. Additionally, it can create new simulation experiences by conversing with and instructing AI agents. For instance, Deepmind has developed various AIs for games and virtual environments, such as a team of AIs to play football fully automatically ~\citep{liu2022deepmind_football}. By providing several high-level APIs and integrating with TaskMatrix.AI to build a natural language interface, human players can ask their AI teammates to execute specific tactics and collaborate with them, making the player feel like a team leader. This would enhance the fun factor, as players would no longer be limited to controlling only themselves or letting AI control all units in Deepmind scenarios.


\textbf{Achieving Neuro-Symbolic AI} TaskMatrix.AI can achieve neuro-symbolic integration by formulating user instructions and accessing symbolic modules as API. The symbolic modules can include formal reasoning engines, algorithms implemented by programming languages, and software like Matlab and Mathematica. The expert systems built by humans have been well-verified and exhibit good consistency in solving their targeted problems. By delegating appropriate tasks to expert systems rather than having large models handle everything, TaskMatrix.AI can significantly improve the quality of its output.