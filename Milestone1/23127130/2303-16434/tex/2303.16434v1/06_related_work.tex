\section{Related Work}

Improving the performance of specific tasks with existing APIs has been studied in various scenarios. For example, WebGPT~\citep{nakano2021webgpt}, ReAct~\citep{yao2022react}, and ~\citet{ lazaridou2022internet} leveraged search APIs to provide comprehensive information for more trustworthy text generation. ChatGPT Robotics~\citep{vemprala2023chatgpt_robotics}, PaLM-SAYCAN~\citep{ahn2022saycan}, PaLM-E~\citep{driess2023palme} and \citet{liang2022code_as_policy} instructed robotics to finish physical world tasks by leveraging high-level robotics APIs. To solve mathematical problems, ~\citet{cobbe2021gsm8k} used a calculator to fix the calculation error, ~\citet{gao2022pal} used the code interpreter to execute the code generated from the input text, ~\citet{jiang2022draft_sketch_prove} leverage mathematical prover to prove the complex mathematical theory. ToolFormer~\citep{schick2023toolformer} leveraged search API, question answering API, machine translation API and calculator to solve various NLP tasks. ART~\citep{paranjape2023art} leveraged five kinds of tools (arithmetic, code, search, free-form reasoning and string operations) to improve the performance on BigBench \citep{ghazal2013bigbench}. ~\citet{mialon2023augmentedlm} provided a detailed survey for these works. Visual ChatGPT ~\citep{wu2023visualchatgpt} and MM-REACT~\citep{yang2023mmreact} incorporated multiple visual models for better image generation and understanding. However, all these works only focus on a few fixed APIs in a specific domain. We propose to build an API platform that may contain millions of APIs to help solve problems in various domains.

There are four different methods for teaching models using APIs. First, Galactica~\citep{taylor2022galactica} pre-trained the model and ToolFormer~\citep{schick2023toolformer} fine-tuned the model with a corpus of examples that utilize APIs. Second, create a few examples that use APIs and leverage in-context learning to teach the model ~\citep{saycan2022arxiv, gao2022pal, lazaridou2022internet}. Third, leverage reinforcement learning with human feedback to improve the performance of use APIs ~\citep{nakano2021webgpt}. Fourth,  create natural language documents ~\citep{vemprala2023chatgpt_robotics} or structure programs ~\citep{paranjape2023art} to instruct the model on how to use APIs. The pre-training and fine-tuning approach requires a fixed API space and is hard to adapt to API updates. And in-context learning can't handle a large number of APIs. We leverage API documents to connect user instructions to APIs, and we leverage RLHF to improve the connection ability from user feedback. We also implement a feedback loop to assist API developers in improving their documentation, which can achieve lifelong learning with natural language.

Several preliminary products share a similar idea of TaskMatrix.AI, such as the ACT-1 of ADEPT\footnote{https://www.adept.ai/blog/act-1}, which target building models that can take actions in the digital world. LangChain\footnote{https://python.langchain.com/en/latest/index.html} targets to combine multiple language models and other sources of computation or knowledge that take in a string and return a string. We also proposed and open-sourced Visual ChatGPT\footnote{https://github.com/microsoft/visual-chatgpt} as the first scenarios of TaskMatrix.AI, which can handle complex visual tasks, such as image-based question answering, generation, and editing. The ChatGPT Plugins \footnote{https://openai.com/blog/chatgpt-plugins} of OpenAI can help ChatGPT access up-to-date information, run computations, or use third-party services. Together with these works, we aim to facilitate research in this area and share ideas about how to implement similar products.