\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
%\usepackage[ruled,vlined,linesnumbered,noend]{algorithm2e}
%\usepackage{amsmath}

\title{J-Parallelio - automatic parallelization framework for java virtual machine code}


\author{
  Krzysztof Stuglik \\
  Institute of Computer Science\\
  University of Science and Technology\\
  Cracow, Poland \\
  \texttt{stuglik@student.agh.edu.pl} \\
  %% examples of more authors
   \And
 Piotr Listkiewicz \\
  Institute of Computer Science\\
  University of Science and Technology\\
  Cracow, Poland \\
  \texttt{listkiewicz@student.agh.edu.pl} \\
  %% examples of more authors
   \And
 Mateusz Kulczyk \\
  Institute of Computer Science\\
  University of Science and Technology\\
  Cracow, Poland \\
  \texttt{kulczyk@student.agh.edu.pl} \\
  %% examples of more authors
   \And
 Marcin Pietron \\
  Institute of Electronics\\
  University of Science and Technology\\
  Cracow, Poland \\
  deep-cogni.com \\
  \texttt{pietron@agh.edu.pl} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle

\begin{abstract}
Manual translation of the algorithms from sequential version to its parallel counterpart is time consuming and can be done only with the specific knowledge of hardware accelerator architecture, parallel programming or programming environment. The automation of this process makes porting the code much easier and faster. The key aspect in this case is how efficient the generated parallel code will be. The paper describes J-Parallelio, the framework for automatic analysis of the bytecode source codes and its parallelisation on multicore processors. The process consists of a few steps. First step is a process of decompilation of JVM and its translation to internal abstract syntax tree, the dependency extraction and memory analysis is performed. Finally, the mapping process is performed which consists of a set of rules responsible for translating the input virtual machine source code to its parallel version. 
%Machine learning techniques were added to the system to choose if given algorithm should be run on CPU or GPU and to estimate parameters for the parallel code.
The main novelty is that it can deal with pure java virtual machine and can generate parallel code for multicore processors. This makes the system portable and it can work with different languages based on JVM after some small modifications. The efficiency of automatically translated source codes were compared with their manually written counterparts on chosen benchmarks.
\end{abstract}


% keywords can be removed
\keywords{automatic parallelization \and java virtual machine \and multicore processors \and HPC}

\section{Introduction}
% no \IEEEPARstart
Over the last few years we have observed a lot of trials of building tools that can help with automatic code parallelisation on different hardware platforms. We have also observed intensive research in parallelisation of source codes written in C/C++ language and speed up using OpenMP and OpenCL environments \cite{IEEEhowto:bondhugula}, \cite{IEEEhowto:amini}, \cite{IEEEhowto:liao}. Recently, we have been able notice that tools for transforming C to CUDA or OpenCL for GPU acceleration are a common topic of interest in automatic code parallelisation \cite{IEEEhowto:baskaran}, \cite{IEEEhowto:elmqvist}, \cite{IEEEhowto:setoain}, \cite{IEEEhowto:amini} and \cite{IEEEhowto:hou}. There were also trials to automatically speed up different languages (e.g. Java \cite{IEEEhowto:rafael}) on multicore processors.
In the J-Paralellio system, a fairly new approach is presented. The framework is fully based on java virtual machine code and language independent, fig. \ref{fig:parallelio}. The main input is java virtual machine code. JVM is parsed and partially decompiled and transformed to intermediate structures, then analysed, instrumented and transformed back to virtual machine in a  parallelised version. %The system can generate both multicore CPUs and GPUs accelerated codes. 
The crucial element of its functionality is an engine incorporated with a decompiler and abstract syntax tree builder, pool of algorithms responsible for dependency and memory access analysis and set of rules for java virtual machine code transformation. %The system consists of three main parts: front-end, engine and back-end. 
In the first stage, the system parses JVM source code and extracts loops from the JVM code. It then generates an Abstract Syntax Tree with Data Flow Graph which helps to identify the potential parallelism by extracting internal dependencies. The system mainly concentrates on loops analysis. %The loops are parts of code in which programs spend most of their execution time and they are main source of hidden parallelism. 
The framework performs the dependency analysis using the same algorithms as other automatic paralellisation systems \cite{IEEEhowto:bondhugula}, \cite{IEEEhowto:baskaran}, \cite{IEEEhowto:ppcg}, \cite{IEEEhowto:amini}.
%System mainly concentrates on loops analysis. %The Parallelio represents information about loops using polyhedra model. %There are also few algorithms to do calculation on polytopes which are very useful in doing dependency analysis. In presented system Barvinok Counting and Fourier Motzkin with special modifications for symbolic computations are used for dependency analysis. For less sophisticated examples other algorithms with lower complexity are used. 
%Additionally, system computes dependence vectors if they exist and has some algorithms which deal with some non-linear subscripts. 
Finally, the system maps the loops iterations and data structures to the hardware accelerator by using built-in translation rules. After the system engine analysis, it generates a parallel version of java virtual machine code. It instruments java virtual machine code with instructions responsible for the creation of threads. It automatically maps the loops to multi-thread execution. 
%The framework is incorporated with a mode to exchange the loops with its GPU-based counterparts. Additionally, the machine learning module is responsible for fine-tuning the parameters of parallel execution like the number of threads, the number of blocks in the GPU, the number of threads per block etc. 
Finally, the work presents the results of mapping a few benchmarks from sequential to parallel versions.
The main advantage of the presented approach is its portability. There are several languages which are based on Java virtual machine. JRuby and Jython are perhaps the most well-known ports of existing languages (Ruby and Python respectively). The new languages that have been created from scratch to compile to Java bytecode, Clojure, Groovy and Scala may be the most popular examples.
%Additionally, parallel loops are run on the GPU by using JCuda wrapper and compared with CPU version. 
%data transfer to GPU and kernel invocation. The module is also responsible for generating CUDA code from specified part of abstract syntax tree. 
%In order to describe loop analysis and loop transformation in strict mathematical manner, this paper introduces new formalism and notation to reason about loops. The engine of the system has pool of algorithms responsible for memory analysis, code and loop transformations and rules for directive generation. 
%what in case of GPU

%Presented system analyses the input source code written in C language and using algorithms responsible for dependency and memory usage analysis. Second module in the system is responsible for analyzing java virtual machine. It parses java virtual machine using BCEL library and transform it AST. After system engine analysis it generates parallel version of java virtual machine.
%It also decides which part should be computed on CPU host or accelerator. %machine learning module. 
%Additionally it generates code responsible for communication between host and accelerator. 
%Simulations presented in the paper were run on GPU Tesla K80 and Intel Xeon processors. 
%Our baselines were results achieved in manual implementation published in \cite{}. 
%The main novelty of the proposed system is module with java virtual machine parallelization. 
The paper is organised as follows: the second section describes the related works in the field of automatic parallelisation. The next sections concentrates on methodology and algorithms which analyse and translate the parallel version of the input sequential JVM source code. The fifth section presents the results, the sixth conclusions and future work. 

\section{Related works}
The articles \cite{IEEEhowto:rafael}, \cite{IEEEhowto:sun} and \cite{IEEEhowto:bradel} present approaches for automatic java source code parallelisation. In \cite{IEEEhowto:rafael} dependency extraction methods are used for java source code analysis. The translator is built for sequential JAVA code which generates a highly parallel version of the same program. The translation process interprets the AST nodes for signatures such as read-write access, execution-flow modifications, among others and generates a set of dependencies between executable tasks. The presented approach has been applied for recursive Fibonacci and FFT algorithms. The methods obtained a 10.97x and 9.0x increase in speed on a twelve-core machine.
The latter two methods \cite{IEEEhowto:sun} and \cite{IEEEhowto:bradel} concentrate on parallelisation using trace information. The approach presented in \cite{IEEEhowto:sun} collects on-line trace information during program execution, and dynamically recompiles methods that can be executed in parallel. In \cite{IEEEhowto:bradel}, the authors implement a system that demonstrates the benefits and addresses the challenges of using traces for data-parallel applications. They propose an execution model for automatic parallelisation based on traces. In \cite{IEEEhowto:chan}, a novel approach is described and evaluated for the automatic parallelisation of programs that use pointer-based dynamic data structures written in Java. The approach exploits parallelism among methods by creating an asynchronous thread of execution for each method invocation in a program. The only work in which java code parallelisation is done directly on a java virtual machine is shown in \cite{IEEEhowto:felber}. It is semi-automatic, there is no detailed JVM code analysis, it has not decompilation, automatic JVM code transformation and generation. It presents the process of the automatic instrumentation of virtual machine code by preparing and invoking special adapters which can run the original methods in a multi-threaded java environment. The results are presented using the Mandelbrot benchmark. The drawback of the article is the lack of results of running the algorithm on more benchmarks.  

Several other systems were designed for automatic code parallelisation which are mainly based on C/C++ language. YUCCA \cite{IEEEhowto:smitha} designed by KPIT Technologies is an automatic parallelisation tool for projects written in C language. It provides source to source conversion - on input, it takes the source code of the application written in C and produces a parallelised version of the source code as an output. YUCCA output is a multithreaded version of the input with Pthreads or OpenMP pragmas inserted at appropriate places. YUCCA uses PThreads to perform task parallelisation and OpenMP to make loops run in parallel. YUCCA consists of two main parts: the front-end, which is responsible for parsing source code, the back-end which performs static dependency analysis to identify parts of code that is worth being parallelised. 
PLUTO \cite{IEEEhowto:bondhugula}, \cite{IEEEhowto:bondhugula2008} is an automatic parallelisation tool based on a polyhedral model. PLUTO performs source to source transformation - it conducts coarse-grained parallelism and at the same time ensures data locality. The core transformation framework mainly works by finding affine transformations for efficient tiling. PLUTO performs parallelisation with OpenMP and the code is also transformed for locality. The tool provides a number of options to tune aspects such as tile sizes, unroll factors and outer loop fusion structure.
C-to-CUDA \cite{IEEEhowto:baskaran} and PPCG \cite{IEEEhowto:ppcg} propose similar steps to solve the automatic GPGPU code-generation problem. They concentrate on finding parallel loops, the creation of a polyhedral model from the loops; they tile and map the loops to GPU blocks and threads and determine where to place the data.

Par4All \cite{IEEEhowto:amini} is an automatic parallelising and optimising compiler for C and Fortran sequential programs. The purpose of this source-to-source compiler is to adapt existing applications to various hardware targets such as multicore systems, high performance computers and GPUs. It creates a new source code and thus allows the original source code of the application to remain unchanged.
The auto-parallelisation feature of the Intel C++ Compiler \cite{IEEEhowto:intel_c} automatically translates serial portions of the input program into semantically equivalent multi-threaded code. Automatic parallelisation determines the loops that are good candidates, performs the data-flow analysis to verify correct parallel execution, and partitions the data for threaded code generation as is needed in programming with OpenMP directives. The OpenMP and auto-parallelisation applications provide the performance gains from shared memory on multiprocessor systems. 
%The Intel Advisor 2017 \cite{} is a vectorization optimization and thread prototyping tool. It integrates several steps into its workflow to search for parallel sites, enable users to mark loops for vectorization and threading, check loop-carried dependencies and memory access patterns for marked loops, and insert pragmas for vectorization and threading. 
AutoPar \cite{IEEEhowto:liao} is a tool which can automatically insert OpenMP pragmas into input serial C/C++ codes. For input programs with existing OpenMP directives, the tool double checks the correctness when the right option is turned on. Compared to conventional tools, AutoPar can incorporate user knowledge (semantics) to discover more parallelisation opportunities.

The iPat/OMP \cite{IEEEhowto:ishikara} tool provides users with the assistance needed for the OpenMP parallelisation of a sequential program. This tool is implemented as a set of functions on the Emacs editor. All the activities related to program parallelisation, such as selecting a target portion of the program, invoking an assistance command, and modifying the program based on the assistance information shown by the tool, can be handled in the source program editor environment. OMP2MPI \cite{IEEEhowto:garriga_} automatically generates MPI source code from OpenMP, allowing the program to exploit non shared-memory architectures such as cluster, or Network-on-Chip-based (NoC-based) Multiprocessors-System-on-Chip (MPSoC). OMP2MPI provides a solution that allows further optimisation by an expert who wants to achieve better results. 
%OMP2HMPP \cite{IEEEhowto:garriga__} a tool that, automatically translates a high-level C source code(OpenMP) code into HMPP. The generated version rarely will differs from a hand-coded HMPP version, and will provide an important speedup, near 113%, that could be later improved by hand-coded CUDA.


%add java i gpu Coimbra


%The Polyhedral Library(PolyLib) is a library for doing computations on object that consists of unions of polyhedra. This library contains functions for operation on those structures. It is mainly used in parallelization domain where polyhedra is used as convenient representation of nested loop iteration structure. The Polylib website is located in \cite{Remondino04polylib}, and good description of the Polylib is \cite{polylibmanual}.
\section{Methodology}
The framework consists of a few submodules. The first is a decompilation comprised of AST building components. It is responsible for transforming the Java bytecode to Java instructions and building an Abstract Syntax Tree from them. In parallel with decompilation, the loops extraction module works. It enables extraction of the loops in a java bytecode. The loops are the analysed by a specialised algorithm to extract potential parallelism (see Section 3.1).  %Before the code transformation the process of analysis of the source code is performed. T
After analysis, mapping the bytecode to a multithread version is performed. The bytecode is instrumented with special instructions which are responsible for the thread, the task and their memory management. The multithreaded bytecode can then be run or decompiled to any language based on a java virtual machine.  
The main phases of the framework method are: 
%to decide if it can be delegated to GPU or if it can be parallelized on CPU more efficiently. 

\begin{figure*}
\centering
\includegraphics[scale = 0.8]{parallelio_.png}
\caption{\footnotesize Automatic java virtual machine parallelization system.}
\label{fig:parallelio}
\end{figure*}

\begin{itemize}
\item Decompilation
\item Abstract syntax tree building
\item Loops extraction
\item Data flow and dependency analysis  
\item Multithread mapping and JVM code instrumentation
\end{itemize}

\subsection{Java virtual machine and BCEL}
A Java virtual machine (JVM) is an abstract computing machine that enables a computer to run a Java program or program in other language which is based on JVM (e.g. Clojure, Groovy or Scala). One of the organisational units of JVM bytecode is a class. The JVM has instructions for the following groups of tasks: arithmetic operations, load and store arithmetic, type conversion, object creation and manipulation, stack management (stack operations push / pop), control transfer (branching), field access, method invocation, throwing exceptions and monitor-based concurrency. The JVM operation set can be represented as: $OP\ ARG$, where $OP$ belongs to set of JVM available elementary operations, $ARG$ is an argument. Argument can be constant or variable. In each single instruction can be one or two arguments. The bytecode instruction set currently consists of 212 instructions and 44 opcodes. 

The presented framework uses ByteCode Engineering Library (BCEL) which enables reading and manipulating in Java bytecode. The BCEL is intended to give users a convenient way to analyse, create, and manipulate Java class files. Classes are represented by objects which contain all the symbolic information of the given class: methods, fields and bytecode instructions, in particular.
Such objects can be read from an existing file, transformed by a program (e.g. a class loader at run-time) and written to a file again. %An even more interesting application is the creation of classes from scratch at run-time. %The Byte Code Engineering Library (BCEL) may be also useful if you want to learn about the Java Virtual Machine (JVM) and the format of Java .class files.  

%The $FOR$ loop can be expressed as 5-tuple with defined
%order in the form:
%\begin{align}
%	\begin{split}
%		L =  \{iter, init\_value, end\_value, iter\_update(), E\}
%	\end{split}
%\end{align}

%where:
%\begin{enumerate}
%\item $iter$ - iteration variable
%\item $init\_value$ - initial value of iteration variable
%\item $end\_value$ - end value of iteration variable
%\item $iter\_update(iter)$ - is a function of iter which defines updates of iteration variable in each iteration - $\Delta_{iter}$
%\item $E = \{e_1, e_2,... \}$ is a set of statement expressions inside given loops
%\end{enumerate}

%\begin{algorithm}
%\begin{algorithmic}[1]
%\STATE{decompilation}
%\STATE{abstract syntax tree building}
%\STATE{loops extraction}
%\STATE{data flow and dependency analysis}
%\STATE{multithread mapping process}
%\STATE{jvm code instrumentation}
%\caption{The main flow of the framework}
%\label{alg:main}
%\end{algorithmic}
%\end{algorithm}

\subsection{Decompilation and AST building}

The first stage of the framework is responsible for translation of the raw Java bytecode to higher level instructions. Next, the translated instructions should be analysed to extract dependency between program components. Therefore the algorithm was built to fulfill this goal. 
%The module responsible for byte code analysis partially decompiles the code. %It achieves this goal by monitoring the state of the JVM stack.
Each single bytecode operation can pop or push elements on the JVM stack. Each single instruction in high-level language ends when the stack is empty. Therefore to extract whole Java instruction the module monitors the state of the stack.
The approach is presented in Algorithm \ref{alg:decomp}. It loads a class implementation in Java bytecode and gets a list of its methods (line 1). It also initializes the stack of the virtual machine ($S$) and decompiled instruction collections ($J_{i}$ and $J$).
 
\begin{algorithm}[H]
\begin{algorithmic}[1]
\STATE {$S$ $\gets$ $\emptyset$, $J_{i}$ $\gets$ $\emptyset$, $J$ $\gets$ $\emptyset$, $C$ $\gets$ load class, $M_{C}$ $\gets$ get methods in $C$} 
\FOR {$m$ \textbf{in} $M_{C}$}
\STATE {$I_{set}$ $\gets$ get instructions from $m$}
\WHILE {$I_{set}$ $\notin$ $\emptyset$}
\STATE {$i$ $\gets$ remove first instruction from $I_{set}$}
\STATE {$push$ $i$ to $S$}
\WHILE {$S$ $\notin$ $\emptyset$}
\STATE {$i$ $\gets$ remove first instruction from $I_{set}$} 
\IF {$i$ $\in$ $OP$}
%\STATE {$v$ $\gets$ pop from $S$}
\STATE {$ARG$ $\gets$ $pop$ $ARG$ from $S$}
\STATE {$R$ $\gets$ $OP$ $ARG$}
\STATE {$push$ $R$ to $S$}
%\STATE {$S$ $\gets$ push $e$} 
\ENDIF
\IF {$i$ $\in$ $PUSH$}
\STATE {$push$ $ARG$ to $S$} 
\ENDIF 
\IF {$i$ $\in$ $POP$}
%\IF {$i$ $\in$ $CMP$}
\STATE {$J_{i}$ $\gets$ $pop$ from $S$}
%\STATE {$S$ $\gets$ $\emptyset$}
%\ELSE
%\STATE {$I_{S}$ $\gets$ $pop$ from $S$}
%\STATE {$J_{i}$ $\gets$ var $\cup$ $J_{i}$}
%\STATE {$J$ $\gets$ $J_{i}$}
%\ENDIF
\ENDIF
\ENDWHILE
\STATE {return $J_{i}$}
\STATE {$J$ $\gets$ $J_{i}$ $\cup$ $J$}
\ENDWHILE
\ENDFOR
\end{algorithmic}
\caption{Decompilation the JVM}
\label{alg:decomp}
\end{algorithm}

Then the algorithm iterates over the class methods. It gets an instruction list from each method $m$ (line 3). It translates each instruction in a sequence and monitors the state of the stack. At the beginning it pushes the first instruction $i$ of $J_{i}$ to $S$ (line 6). In a while loop it processes next bytecode instructions until stack $S$ is empty. If it recognizes the operand instruction then it takes arguments of operation from the stack (line 10).
It forms the triple address instruction (line 11) and pushes it to the stack. If $PUSH$ operation is recognized (line 14) it pushes the argument of bytecode instruction to the stack. Finally if a single $POP$ instruction is met the algorithm takes the final decompiled instruction ($J_{i}$) from the stack (line 18). All decompiled instructions are stored in $J$ list (line 22). 

The Algorithm \ref{alg:ast} presents building Abstract Syntax Tree from the list of the decompiled instructions. It initializes data structures (line 1) in which it stores dependencies between instructions ($T$) and list of assigned variables ($L$). Then, it goes through the list of instructions and takes left and right hand variables from the analysed instruction (line 3-5). It checks in a loop if every right hand variable of $J_{i}$ (from $J_{iR}$) already exists in $L$. If it is true, the algorithm takes the last instruction in which the specified right hand variable has appeared ($var$, line 8).

\begin{algorithm}
\begin{algorithmic}[1]
\STATE{$T$ $\gets$ $\emptyset$, $L$ $\gets$ $\emptyset$}
\WHILE{$J$ $\notin$ $\emptyset$}
\STATE{$J_{i}$ $\gets$ remove first instruction from $J$}
\STATE{$J_{iR}$ $\gets$ get right hand variables of $J_{i}$}
\STATE{$J_{iL}$ $\gets$ get left hand variable of $J_{i}$}
\FOR{$var$ \textbf{in} $J_{iR}$}
\IF{$var$ $\in$ $L$}
\STATE{$I_{R}$ $\gets$ take last instruction from $L$: $var$ $\in$ $I_{R}$}
%\FOR{$i_{R}$ $\in$ $I_{R}$}
\STATE{$d$ $\gets$ ($I_{R}$ $\rightarrow$ $J_{i}$)}
\STATE{$T$ $\gets$ $T$ $\cup$ $d$}
%\ENDFOR
\ENDIF
%\STATE{take iteration variable}
%\STATE{$\gets$ extract body of the loop}
\ENDFOR
\STATE{$L$ $\gets$ $J_{iR}$ $\cup$ $L$}
\ENDWHILE
\STATE{return T}
\caption{AST building}
\label{alg:ast}
\end{algorithmic}
\end{algorithm}

The dependency $d$ is extracted between these two instructions ($I_{R}$ and $J_{i}$, line 9) and added to the set $T$. After the instruction is processed the right hand variable is added to $L$ set (line 13). At the end of the algorithm set of dependencies $T$ is returned (line 15) which can be directly used to create the AST.  

\subsection{Loops extraction and data flow analysis}

The loop extraction and their analysis are the next step of Java bytecode analysis. It is a crucial stage in the automatic parallelisation because loops are the main source of hidden parallelism. The Algorithm \ref{alg:loop} presents how loops are extracted from the bytecode. The process starts from finding jump $GOTO$ instruction. Then the argument of jump instruction is read (jump address, line 2). The address is the start of the program loop. The algorithm goes to this location and parses the iteration variable with its initialization value and loop boundary condition. It takes the list of the instruction from the location just after conditional bytecode instruction to the $GOTO$ instruction (line 6). The extracted body of the loop can be decompiled using Algorithm \ref{alg:decomp}. 
Very often loops can be nested. Therefore to extract hierarchy of the loops presented method should run recursively. 

\begin{algorithm}
\begin{algorithmic}[1]
\STATE{$I_{goto}$ $\gets$ find $GOTO$ instruction}
\STATE{$L_{s}$ $\gets$ take address from $I_{goto}$ instruction}
\STATE{jump to $L_{s}$}
\STATE{$i$ $\gets$ take iteration variable}
\STATE{$B$ $\gets$ take condition boundary of the loop}
\STATE{$L$ $\gets$ take block from condition to $I_{goto}$}
\STATE{decompile loop $L$  //Algorithm \ref{alg:decomp}}
\caption{The JVM loop extraction}
\label{alg:loop}
\end{algorithmic}
\end{algorithm}


%A formal analysis of the algorithm is based on generating its data flow graph and extracting potential parallelism. 
%The main sources of parallelism are loops. 
After loop extraction the loop analysis is performed. Formal analysis is based on a polyhedral model; algorithms for dependency detection are run by using symbolic Fourier-Motzkin elimination. %Additionally, loops with SIMD operation can be easy and efficiently mapped to SIMT architecture. The SIMD operation can be extracted using memory-access pattern analysis. 

%Partially parallel loops should be further analyzed.

%\begin{align}
%	\begin{split}
%		E = \{LHS\_e, RHS\_e\} \\
%	\end{split}
%\end{align}

%The function $rhs(E)$  returns $RHS\_e$, $lhs(E)$ returns $LHS\_e$.
%$LHS_t(E) $ is a function which return left hand variable if it is an array or null if it isn't. $RHS_t(E) $ is a function which return right  hand variables  which are arrays. The function $subscript(var)$  is a method for extracting subscript expression from array variable. \\

%Following definitions describe write after write, read after write, write after read dependencies \footnote{ab}: 

%\begin{align}
%	\begin{split}
%		RAW(E1, E2)  \equiv  \{ E1 \in predecessors(E2) \land  rhs(E2) \in lhs(E1) \} \\
%		WAR(E1, E2)  \equiv  \{ E1 \in predecessors(E2) \land  rhs(E2) \in rhs(E1) \} \\
%		WAW(E1, E2)  \equiv  \{ E1 \in predecessors(E2) \land  lhs(E2) \in lhs(E1) \} \\
%	\end{split}
%\end{align}


%In this case it depends how many iterations can be run in parallel and how many operations can be done by single thread between barriers. 
%Finally mapping process of iterations to single thread with memory mapping is performed \ref{eq:map}.


%In case of SDLS and tabu search loop responsible for improving each solution independently (fully parallelized) is chosen as a part of code to be fully parallelized in GPU.

%\subsection{Java virtual machine code analysis}


%Memory analysis process is crucial to decide which part of code delegate to hardware accelerator. Hardware accelerator like GPU has its own memory hierarchy structure. Some memories are shared by group of threads. These memories are quite fast but have reduced capacity. Global memories are much larger but significantly slower. Additionally threads have access to cached local memories and its registers. Therefore analysis of source code toward estimation of memory requirements is crucial step to decide if part of code can be efficiently executed in GPU.

%\begin{equation}
%   map(iter_{i})=threadIdx 
%\label{eq:map}
%\end{equation}

%where: 
%$iter_{i} \in$ Polyhedra Domain, $threadIdx \in$ GPU Grid

%\begin{algorithm}[H]
%\begin{algorithmic}[1]
%\STATE {$create\_AST()$}
%\STATE {$loops = extract\_loops\_from\_AST()$}
%\FOR{$p:=0$ to $ len(loops)$}
%\STATE {$symbolic\_fourier\_motzkin(p)$}
%\ENDFOR
%\end{algorithmic}
%\caption{Algorithm for data flow analysis}
%\label{alg:dep}
%\end{algorithm}

\section{Java virtual machine automatic parallelisation module}

%The aim is binary compatibility. Each particular host operating system needs its own implementation of the JVM and runtime. These JVMs interpret the bytecode semantically the same way, but the actual implementation may be different. More complex than just emulating bytecode is compatibly and efficiently implementing the Java core API that must be mapped to each host operating system.

The Algorithm \ref{alg:jvm_p} describes the process of instrumenting the JVM bytecode. The first two steps are responsible for initialization the thread executors and tasks list (line 1 and 2). The next part depends on type of parallelism. If data-driven
dependency is recognized (lines 3-7, e.g. histogram) the input data is divided to independent data chunks (line 4). Output data is copied and create separate instance for each parallel thread (line 5). Then subtasks methods are created to work on these data chunks (line 6). At the end the single thread code responsible for merging result data is added (line 7). 

\begin{algorithm}
\begin{algorithmic}[1]
\STATE{$Ex$ $\gets$ create executors}
\STATE{[$T_{1}$, $T_{2}$,..., $T_{N}$] $\gets$ create task list}
\IF{$P_t$ \textbf{is} $DP$}
\STATE{[$D_{i1}$, $D_{i2}$, ..., $D_{iN}$] $\gets$ divide the input data $D_{i}$}
\STATE{[$D_{o1}$, $D_{o2}$, ..., $D_{oN}$] $\gets$ make copy of output data $D_{o}$}
\STATE{$P$ $\gets$ add pool of subtasks ($D_{in}$, $D_{on}$)}
\STATE{$D_{o}$ $\gets$ add merging output data $D$}
\ENDIF
\IF{$P_t$ \textbf{is} $IP$}
\STATE{[$(start_{1}$, $step_{1}$), ($start_{2}$, $step_{2}$), ..., ($start_{N}$, $step_{N}$)] $\gets$ divide iterations to chunks}
\STATE{$P$ $\gets$ add pool of subtasks in parallelised regions ($start_{n}$, $step_{n}$)}
\ENDIF

\caption{The JVM parallelisation}
\label{alg:jvm_p}
\end{algorithmic}
\end{algorithm}

The JVM instrumentation is finally used in the main parallelisation algorithm which is shown in Algorithm \ref{alg:par}. The algorithm tries to find best parallel configuration of the input sequential program. The parallelisation concentrates mainly on program loops extracted by Algorithm \ref{alg:loop} (line 2). Then the dependency analysis is performed (line 3). The loops can be fully or partially parallelised or unable to run in parallel (line 4). In the case of fully parallel loops the main decision is which loop to choose in a nested loop structure. 
%The choice is based on the amount of data needed for loop computation, which data can be shared between iterations and whatever memory access patterns are used. 
Additionally, loops can be transformed by interchanging, tiling, skewing etc. By making appropriate selections from this choice of transformations it is possible to achieve better mapping and more efficient implementation. All these configurations are generated in line 5. Then they are tested in a loop (lines 6-10). Each candidate transformation is parallelised in JVM. Next, they are run with reduced number of iteration $r$ (line 8). Finally, the most efficient configuration is chosen (lines 12 and 13). 
%The tiling transformation helps further analysis and the division of this group of threads between CUDA blocks.
%The code analyser is also incorporated with an algorithm which recognises single instruction multiple data instructions (e.g. a histogram).


\begin{algorithm}
\begin{algorithmic}[1]
\STATE{$E$ $\gets$ $\emptyset$, $V$ $\gets$ $\emptyset$}
\STATE{$L_{d}$ $\gets$ Algorithm \ref{alg:loop}}
\STATE{$L_{p}$ $\gets$ Fourier-Motzkin($L_{d}$)}
\STATE{$P_{t}$ $\gets$ check parallelism type($L_{d}$)}
\STATE{$L_{t}$ $\gets$ get loop transformations}
\FOR{$l_{t}$ $\textbf{in}$ $L_{t}$}
%\STATE{$L$ = loop\_transform($L_{d}$, $l_{t}$)}
\STATE{$L$ = parallelise\_jvm($l_{t}$)} //Algorithm \ref{alg:jvm_p}
\STATE{$e$ = run($L$, $r$)}
\STATE{$E$ $\gets$ $E$ $\cup$ $e$}
\STATE{$V$ $\gets$ $V$ $\cup$ $L$}
\ENDFOR
\STATE{$id_{min}$ $\gets$ $arg$ $min$ $E$}
\RETURN{$V[id_{min}]$}
\caption{The parallelisation algorithm}
\label{alg:par}
\end{algorithmic}
\end{algorithm}

%The byte code instruction set currently consists of 212 instructions and 44 opcodes are marked as reserved and may be used for future extensions or intermediate optimisations within the Java virtual machine. 


%Memory analysis process is crucial to decide which part of code delegate to hardware accelerator. Hardware accelerator like GPU has its own memory hierarchy structure. Some memories are shared by group of threads. These memories are quite fast but have reduced capacity. Global memories are much larger but significantly slower. Additionally threads have access to cached local memories and its registers. Therefore analysis of source code toward estimation of memory requirements is crucial step to decide if the part of the code can be efficiently executed in GPU. The amount of memory per thread is analyzed, data that can be shared between threads and data reusability.
%The SDLS and tabu search algorithms need local memories for storing temporal solutions, and additional structures C(S) and T(S) for theirs fitness computation.  

%The alg. \ref{alg:rules} describes more accurate analysis and insertion of openmp directives rules based on analysis described in the work.

%\subsubsection{Machine learning for setting up parallelization parameters}

%The machine learning module is an additional module which supports the choice of hardware accelerator (GPU or CPU multicore) and setting up near optimal program-execution parameters (the number of threads and the number of GPU blocks). The module has a decision tree model which is trained with the following parameters:

%\begin{itemize}
%    \item $T_{p}$ - type of parallelization (), 
%    \item $N_{c}$ - number of cores in CPU or threads in GPU block, 
%    \item $N_{b}$ - number of blocks
%    \item $N_{i}$ - number of arithmetic operations in a single iteration
%\end{itemize}

%\begin{algorithm}[H]
%\begin{algorithmic}[1]
%\STATE   Parsing the code by ANTLR
%\STATE   Data Flow Graph building (with $WAW, RAW, WAR$ dependence) and Abstract Syntax Tree
%\STATE   Dependence analysis in loops (pool of algorithms: banarjee, range test, FME, barvinok counting etc.)
%\STATE   Vector dependence computing
%\STATE   Loops invariant analysis (alg. \ref{alg:codem}) and reduction extraction (alg. \ref{alg:reduction})
%\STATE   Loop transformations (interchanging, unrolling, fusion, distribution), $if$ instructions inside loops analysis (alg. \ref{alg:if}), critical section analysis (alg. \ref{alg:critical})
%\STATE  loop tiling (improving data locality for cache optimization)
%\STATE   Data analysis for each transformation from previous step (alg. \ref{alg:memory})
%\STATE  Directives adding rules
%\end{algorithmic}
%\caption{Main algorithm of the system}
%\label{alg:main}
%\end{algorithm}


%\begin{algorithm}[H]
%\begin{algorithmic}[1]
%\STATE loops transformation - best configuration (permutation of loop transformations introduced) of loops is chosen based on data analysis (e.g. matrix multiplication loops interchange to improve data locality and blocking by tiling transformation)
%\STATE parallel nested loops - omp parallel directive as high as possible (minimize openmp library overhead)
%\STATE $if$ expression - loop distribution
%\STATE reduction recognize - openmp reduction directive adding (see reduction)
%\STATE critical section extracted - locks creation (see histogram)
%\STATE inside loops of different size of iterations - schedule directive (see NBody)
%\STATE data locality in parallel loops - omp simd and alligned directives (see matrix multiplication)  
%\STATE private data and shared data allocation- shared and private directives
%\end{algorithmic}
%\caption{Directives adding rules}
%\label{alg:rules}
%\end{algorithm}

%\begin{algorithm}[H]
%\begin{algorithmic}[1]
%\STATE {$checking\_type\_of\_parallelism()$}
%\IF {$instruction\_parallelism$}
%\STATE {$transformations = loop\_transformations()$}
%\ENDIF
%\IF {$data\_parallelism$}
%\STATE {$compute\_amount\_of\_read\_data()$}
%\STATE {$compute\_amount\_of\_write\_data()$}
%\IF {$WARP\_MAPPING$}
%\STATE{$compute\_block\_memory()$}
%\ENDIF
%\IF {$THREAD\_MAPPING$}
%\STATE{$compute\_block\_memory()$}
%\STATE{$compute\_block\_data()$}

%\ENDIF
%\ENDIF
%\STATE {$compute\_amount\_of\_data\_for\_GPU()$}
%\STATE {$generate\_GPU\_code\_according\_the\_rules()$}
%\end{algorithmic}
%\caption{Java byte code parallelization}
%\label{alg:mapping}
%\end{algorithm}


%\subsection{OpenMP code generation}

%The openMP code generation is designed according to previously created rules. It receives results from dependency and data flow analysis module. It also has output from memory analyzer. Based on these data it chooses best loops transformation and apply openmp directives \cite{barney_laboratory}. In some cases adding directives even in fully parallelized loops can slows down the code. This happens when threads heavily access memory or number of non-memory operations is too low. Therefore in our system additional algorithm is added that decides based on decision trees and gathered statistics if directive in such cases can be added or not.    

%\section{CUDA code generation}

%The CUDA code generator is responsible for extracting the part of the input code and transforming it to CUDA. The wide spectrum of use cases were tested: TABU and SLDS in LABS problem, RMHC with crossover in Golomb Ruler, matrix multiplication, histogram computing, FFT and N-body problem. Therefore many case studies with different parallelization schemes were tested. Firstly, the type of parallelism is recognized. The algorithm can be data or instruction parallel. In these two cases different approaches are performed. 


%Crossover operator is not so efficient because of the number of operations per thread, not regular accesses to memory and domination of memory operations. Therefore ratio of arithmetic operations to memory access is lower so the speed up is significantly limited. 
%The mapping algorithm is presented in alg.\ref{alg:mapping}
%The main rules are:
%\begin{itemize}
%\item checking type of parallelism - data or instruction level of parallelism
%\item data that is re-used and shared between iterations should be transfered to shared memory
%\item data that is re-used between operations but used only by single thread should be transfered to register and local memory respectively
%\item read only constants between threads should be transfered to constant memory
%\item use number of kernel blocks that guarantee usage of all available multiprocessors
%\item minimize bank conflicts in shared memory (by reorganizing data access or data allocation in shared memory)
%\item if possible use coalesced access (write and read) to device memory
%\item choosing loop for parallelization that has high ratio of executed operations to memory accesses per thread 
%and its data can be mapped to shared and local memory and registers
%\end{itemize}

%The methods are:

%\begin{itemize}
%\item $compute\_block\_memory()$ = $number\_of\_threads\_per\_block$ $\times$ $size\_of\_data_to\_write$
%\item $compute\_block\_data()$ = $number\_of\_threads\_per\_block$ $\times$ $size\_of\_data\_chunk\_to\_read$ 
%\item $generate\_block\_index\_to\_data()$ = $block\_data$ $\times$ $blockId$ 
%\item compute\_threadPos\_with\_minimizing\_block\_conflicts()

%\end{itemize}

%\begin{algorithm}[H]
%\begin{algorithmic}[1]
%\STATE {$checking\_type\_of\_parallelism()$}
%\IF {$instruction\_parallelism$}
%\STATE {$transformations = loop\_transformations()$}
%\FOR{$p:=0$ to $ len(transformations)$}
%\STATE {$memory\_analysis(p)$}
%\ENDFOR
%\STATE {$choose\_best\_transformation()$}
%\ENDIF
%\IF {$data\_parallelism$}
%\STATE {$compute\_amount\_of\_read\_data()$}
%\STATE {$compute\_amount\_of\_write\_data()$}
%\IF {$WARP\_MAPPING$}
%\STATE{$compute\_block\_memory()$}

%\ENDIF
%\IF {$THREAD\_MAPPING$}
%\STATE{$compute\_block\_memory()$}
%\STATE{$compute\_block\_data()$}
%\STATE{$generate\_block\_index\_to\_read\_data()$}
%\STATE{$compute\_threadPos\_with\_minimizing\_block\_conflicts()$}
%\STATE{$allocate\_and\_clear\_shared\_memory(block\_memory)$}
%\STATE{synchronize\_threads()}
%\STATE{read\_and\_process\_data()}
%\STATE{merge\_data()}

%\ENDIF
%\ENDIF
%\STATE {$compute\_amount\_of\_data\_for\_GPU()$}
%\STATE {$generate\_GPU\_code\_according\_the\_rules()$}
%\end{algorithmic}
%\caption{Mapping methodology}
%\label{alg:mapping}
%\end{algorithm}

%\begin{equation}
%   map(iter_{i})=threadIdx 
%\label{eq:map}
%\end{equation}



%The algorithm for minimizing bank conflicts and iteration to thread mapping that maximizes data re-usability.

%The rules responsible for generating the CUDA source code were created. TODO:example in fig.

%\begin{itemize}
%\item interface generation
%\item kernel signature
%\item threadIdx generation
%\item data from device to shared code generation
%\item iteration internal body mapping for thread CUDA source code
%\end{itemize}

 

%\section{Parsing java virtual machine}
%to be done: finish java parser
%\subsection{Algorithm of parsing}
%stack monitoring, partial decompilation,

%each operations high level ends when stack is empty

%\subsection{Online jvm analysis}
%extractiong operations 


%\subsection{Java virtual machine parallelization}

\section{Results}

The presented framework was run on the following benchmarks: matrix multiplication, histogram computing, vanilla NBody problem and Fast Fourier Transform. The two parameters are efficiency and speedup are main indicators of parallelisation algorithm quality. The efficiency (E) is defined as: 
\begin{equation}
 E(N,P) = \frac{S(N,P)}{P} = \frac{T(N,1))}{P*T(N,P)}   
\end{equation}

and speedup (S):
\begin{equation}
S(N,P) = \frac{T(N,1)}{T(N,P)}    
\end{equation}

where: 
\begin{itemize}
    \item N - size of the problem, 
    \item P - number of cores, 
    \item T(N,P) - time execution for problem with size N and with P cores
\end{itemize}
In Table \ref{table:63} and Figures \ref{fig:matrix_parallel_1} and \ref{fig:matrix_parallel_2} the results for matrix multiplication are described. 

 \begin{table}[tpbh!]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|} 
 \hline
 &
\multicolumn{3}{c|}{1024x1024} &
\multicolumn{3}{c|}{4096x4096} &
\multicolumn{3}{c|}{8192x8192} \\ [0.5ex] 
\hline\hline 
P & T[s] & E & S & T[s] & E & S & T[s] & E & S \\
\hline
1 & 1.15 & 1 & 1 & 463.41 & 1 & 1 & 4 248.56 & 1 & 1 \\
\hline
2 & 0.78 & 0.74 & 1.48 & 227.99 & 1.02 & 2.03 & 2 180.12 & 0.97 & 1.95 \\
\hline
4 & 0.42 & 0.68 & 2.72 & 114.42 & 1.01 & 4.05 & 1 087.61 & 0.98 & 3.91 \\
\hline
8 & 0.30 & 0.49 & 3.90 & 60.57 & 0.96 & 7.65 & 568.00 & 0.93 & 7.48 \\
\hline
10 & 0.37 & 0.31 & 3.10 & 59.24 & 0.78 & 7.82 &  &  &  \\
\hline
12 & 0.39 & 0.24 & 2.94 & 57.50 & 0.67 & 8.06 &  &  &  \\
\hline
14 & 0.42 & 0.20 & 2.76 & 55.49 & 0.60 & 8.35 &  &  &  \\
\hline
16 & 0.46 & 0.16 & 2.49 & 54.98 & 0.53 & 8.43 & 538.77 & 0.49 & 7.89 \\
\hline
\end{tabular}
\caption{Results of matrix multiplication with efficiency parameter E and acceleration S}
\label{table:63}
\end{table}

In Table \ref{table:63}, the results for different sizes of matrix are shown (1024x1024, 4096x4096 and 8192x8192). The efficiency is presented for serial and multicore version. The results are described for different numbers of cores. It can be observed that for smaller matrices (1024x1024) the peak performance is in the case of using eight cores. 

\begin{figure}[!htbp]
  \centering
  \includegraphics[scale=1.25]{matrix_chart.png}
    \caption{Logarithmic chart from serial and parallel time execution of matrix multiplication.}
  \label{fig:matrix_parallel_1}
\end{figure}
 
 \begin{figure}[!htbp]
  \centering
  \includegraphics[scale=0.45]{imgs/chart2.png}
    \caption{Logarithmic chart from matrix multiplication on different number of processor cores.}
  \label{fig:matrix_parallel_2}
\end{figure}
If the size is bigger (4096x4096 and 8192x8192) the best speedup is achieved while using all available cores - sixteen.
%\begin{table}[tpbh!]
%\centering
%\begin{tabular}{|l|l|l|l|} 
% \hline
% &
%\multicolumn{1}{c|}{SERIAL} &
%\multicolumn{1}{c|}{PARALLEL} &
%\multicolumn{1}{c|}{JCUDA} \\ [0.5ex]
%\hline
%DIM[NxN] & T[s] & T[s] & T[s] \\ [0.5ex] 
%\hline\hline 
%8*8 & 1E-6 & 0.003 & 0.002\\
%\hline
%16*16 & 1E-5 & 0.005 & 0.001\\
%\hline
%32*32 & 1E-5 & 0.008 & 0.002\\
%\hline
%64*64 & 1E-4 & 0.014 & 0.001\\
%\hline
%128*128 & 0.001 & 0.022 & 0.002\\
%\hline
%256*256 & 0.014 & 0.041 & 0.002\\
%\hline
%512*512 & 0.182 & 0.137 & 0.002\\
%\hline
%1024*1024 & 1.427 & 0.627 & 0.005\\
%\hline
%2048*2048 & 50,8 & 6.341 & 0.014\\
%\hline
%4096*4096 & 463.413 & 53.9 & 0.057\\
%\hline
%8192*8192 & 4248.5644 & 538.767 & 0.878\\
%\hline
%\end{tabular}
%\caption{Times of sequential and parallel matrix multiplication execution}
%\label{table:61}
%\end{table}

%\begin{figure}[!htbp]
%  \centering
%  \includegraphics[scale=0.45]{imgs/chart.png}
%    \caption{\centering Wykres logarytmiczny czasu trwania obliczeń w zależności od rozmiaru mnożonych macierzy}
%  \label{fig:char1}
%\end{figure}

%\begin{table}[tpbh!]
%\centering
%\begin{tabular}{|l|l|l|l|} 
% \hline
%  DIM[NxN] 
% &
%\multicolumn{1}{c|}{1024x1024} &
%\multicolumn{1}{c|}{4096x4096} &
%\multicolumn{1}{c|}{8192x8192} \\ [0.5ex]
%\hline
% P & T[s] & T[s] & T[s]  \\ [0.5ex] 
% \hline\hline 
% 1 & 1.1492 & 463.413 & 4248.564  \\
% \hline
% 2 & 0.777 & 227.994 & 2180.122  \\
% \hline
% 4 & 0.422 & 114.424 & 1087.609\\
% \hline
% 8 & 0.295 & 60.569 & 568.002\\
% \hline
% 10 & 0.371 & 59.241 & \\
% \hline
% 12 & 0.391 & 57.495 & \\
% \hline
% 14 & 0.417 & 55.486 & \\
% \hline
% 16 & 0.461 & 54.981 & 538.767 \\
% \hline
% \end{tabular}
% \caption{Times of sequential and parallel execution of matrix multiplication with different number of threads}
% \label{table:62}
% \end{table}
 
%In Table 2, the dependency between speedup and the number of cores is described. 
Figures \ref{fig:matrix_parallel_1} and \ref{fig:matrix_parallel_2} present these results using a logarithmic scale. Figure \ref{fig:matrix_parallel_1} shows comparison of execution times between serial and automatically generated parallel versions. It can be observed that around 256x256 size the generated code outperforms the serial one. 

\begin{figure}[!htbp]
  \centering
  \includegraphics[scale=0.8]{hist_w1.jpg}
    \caption{Scalability of histogram.}
  \label{fig:hist_1}
\end{figure}

%\begin{figure}[!htbp]
%  \centering
%  \includegraphics[scale=1.55]{hist_w2.png}
%    \caption{\centering Results of histogram.}
%  \label{fig:chart8192ES}
%\end{figure}

\begin{figure}[!htbp]
  \centering
  \includegraphics[scale=1.55]{hist_es1.png}
    \caption{Histogram efficiency.}
  \label{fig:hist_2}
\end{figure}
Figures \ref{fig:hist_1} and \ref{fig:hist_2} show histogram efficiency related to the size of input data and the number of cores. The histogram algorithm has data-driven parallelism. When the amount of data is about $10^7$ or higher then the acceleration can be noticed (Figure \ref{fig:hist_1}). Figure \ref{fig:nbody} presents Nbody efficiency. 

\begin{figure}[!htbp]
  \centering
  \includegraphics[scale=1.3]{nbody_es1.png}
    \caption{Nbody efficiency.}
  \label{fig:nbody}
\end{figure}
It shows that maximum speedup achieved by automatically generated code was around 1.5 (for eight cores). Figure \ref{fig:fft} describes FFT scalability. 
\begin{figure}[!htbp]
  \centering
  \includegraphics[scale=1.3]{fft_chart.png}
    \caption{Logarithmic chart from FFT with different number of data points.}
  \label{fig:fft}
\end{figure}
The serial version is slightly more efficient than parallel. In all experiments the automatic parallel version is at the peak 15\% worse than its manually created counterpart.
 
%\begin{figure}[!htbp]
%  \centering
%  \includegraphics[scale=0.45]{imgs/chart1024ES.png}
%    \caption{\centering Efficiency and speedup with different number of threads for matrix: 1024x1024}
%  \label{fig:chart1024ES}
%\end{figure}

%\begin{figure}[!htbp]
%  \centering
%  \includegraphics[scale=0.3]{imgs/chart4096ES.png}
%    \caption{\centering Efficiency of matrix multiplication of size 4096x4096}
%  \label{fig:chart4096ES}
%\end{figure}


%\begin{figure}[!htbp]
%  \centering
%  \includegraphics[scale=0.45]{imgs/chart8192ES.png}
%    \caption{\centering Efficiency and speedup with different number of threads for matrix: 8192x8192 }
%  \label{fig:chart8192ES}
%\end{figure}



%\begin{figure}[!htbp]
%  \centering
%  \includegraphics[scale=1.5]{nbody_w1.png}
%    \caption{\centering Results of Nbody.}
%  \label{fig:chart8192ES}
%\end{figure}



%\begin{figure}[!htbp]
%  \centering
%  \includegraphics[scale=1.55]{nbody_w2.png}
%    \caption{\centering Results of nbody.}
%  \label{fig:chart8192ES}
%\end{figure}

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals/conferences use top floats
% exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the \fnbelowfloat
% command of the stfloats package.

%\begin {table}[H]
%\caption{Results of automatic reduction generation [ms].}
%\begin{center}
%    \begin{tabular}{ | c | c | c | c |}
%    \hline
%    no of elements & GPU & man. GPU & CPU \\ \hline
%   4096  & 0.11 & 0 & 8\\ \hline
%   16384  & 0.13 & 1 & 3.58\\ \hline
%   65536 & 0.47  & 5 & 4.56\\ \hline
%   262144 & 1.14 & 9.745 & 8.98\\ \hline
%   1048576 & 2.57  & 13.98 & 7.87\\ \hline
%   4194304 & 5.59 & 29 & 21.72\\ \hline
%   16777216 & 17.44 & 98 & 70 \\ \hline
%    \end{tabular}  
%\end{center}
%\end{table}

%\begin {table}[H]
%\caption{Results of automatic histogram 64 generation [ms].}
%\begin{center}
%    \begin{tabular}{ | c | c | c | c |}
%    \hline
%    no of elements & GPU & man. GPU & CPU\\ \hline
%   960  & 0.2 & 0.2 & 0.001\\ \hline
%   9600  & 0.24 & 0.24 & 0.007\\ \hline
%   96k & 0.47  & 0.46 & 0.055\\ \hline
%   960k & 0.5 & 0.5 & 0.7\\ \hline
%   9600k & 0.17 & 0.13 & 7\\ \hline
%   960M & 0.97 & 0.72 & 55\\ \hline
   
%    \end{tabular}  
%\end{center}
%\end{table}


%\begin {table}[H]
%\caption{Results of automatic histogram 256 generation [ms].}
%\begin{center}
%    \begin{tabular}{ | c | c | c | c |}
%    \hline
%    no of elements & GPU & man. GPU & CPU \\ \hline
%   10k  & 0.038 & 0.01 & 0.03\\ \hline
%   100k  & 0.037 & 0.037 & 0.04\\ \hline
%   1M & 0.07  & 0. & 0.07\\ \hline
%   10M & 0.38 & 0.36 & 5\\ \hline
%   100M & 3.4  & 3.23 & 55\\ \hline
   
%    \end{tabular}  
%\end{center}
%\end{table}

%\begin {table}[H]
%\caption{Results of automatic nbody generation [ms].}
%\begin{center}
%    \begin{tabular}{ | c | c | c | c |}
%    \hline
%    no of bodies & GPU & man. GPU & CPU \\ \hline
%   128  & 3.71 & 3.5 & 12\\ \hline
%   256  & 7.02 & 6.6 & 48\\ \hline
%   512 & 13.65  & 12.8 & 201\\ \hline
%   1024 & 26.9 & 25.3 & 765\\ \hline
%   2048 & 53.6  & 84.1 & 2901\\ \hline
%   4096 & 200.65 & 177.9 & 11450\\ \hline
   
%    \end{tabular}  
%\end{center}
%\end{table}

%\begin {table}[H]
%\caption{Results of automatic matrix multiplication generation [ms].}
%\begin{center}
%    \begin{tabular}{ | c | c | c | c |}
%    \hline
%    no of elements & GPU & cublas GPU \\ \hline
%   256  & 0.099 & 0.063 \\ \hline
%   512  & 0.46 & 0.11 \\ \hline
%   1024 & 4.16  & 0.48 \\ \hline
%   2048 & 27.59 & 3.42 \\ \hline
%    \end{tabular}  
%\end{center}
%\end{table}

%\begin{table}
%\label{tab:translate}
%\caption{Results of automatic parallelization of java byte code}
%\begin{center}
%\begin{tabular}{c|c|c|c|c|}
%\cline{1-5}
%\hline
%                        & histogram & matrix mul & nbody & fft \\ \hline
%\multicolumn{1}{|c|}{1 core / 1 thread}  & 0.324  & 16.297 & 18.9  &  9.144            \\ \hline
%\multicolumn{1}{|c|}{4 cores / 8 threads } & 1.014  & 80.575 & 70.542 & 10.750             \\ \hline
%\end{tabular}
%\end{center}
%\end{table}

%\begin {table}[H]
%\caption{Results of automatic matrix multiplication generation [ms].}
%\begin{center}
%    \begin{tabular}{ | c | c | c | c |}
%    \hline
%    no of elements & sequential CPU & parallel CPU \\ \hline
%   2560  & 148.631 & 44.149 \\ \hline
%   2048  & 80.575 & 16.297 \\ \hline
%   1536 & 21.942 & 8.476 \\ \hline
%   1024 & 1.709 & 2.650 \\ \hline
%    \end{tabular}  
%\end{center}
%\end{table}

%\begin {table}[H]
%\caption{Results of automatic histogram generation [ms].}
%\begin{center}
%    \begin{tabular}{ | c | c | c | c |}
%    \hline
%    no of elements & sequential CPU & parallel CPU \\ \hline
%   2 GB  & 1.014 & 0.324 \\ \hline
%   1 GB  & 0.663 & 0.198 \\ \hline
%   500 MB & 0.332 & 0.096 \\ \hline
%   250 MB & 0.167 & 0.049 \\ \hline
%    \end{tabular}  
%\end{center}
%\end{table}

%\begin {table}[H]
%\caption{Results of automatic nbody generation [ms].}
%\begin{center}
%    \begin{tabular}{ | c | c | c | c |}
%    \hline
%    no of elements & sequential CPU & parallel CPU \\ \hline
%   1500  & 184.022 & 50.294 \\ \hline
%   1000  & 70.542 & 18.900 \\ \hline
%   500 & 21.802 & 8.476 \\ \hline
%   250 & 2.650 & 1.709 \\ \hline
%    \end{tabular}  
%\end{center}
%\end{table}

%\begin {table}[H]
%\caption{Results of automatic fft generation [ms].}
%\begin{center}
%    \begin{tabular}{ | c | c | c | c |}
%    \hline
%    no of elements & sequential CPU & parallel CPU \\ \hline
%   67 MB  & 23.003 & 20.866 \\ \hline
%   33 MB  & 10.750 & 9.144 \\ \hline
%   1 MB & 0.168 & 0.168 \\ \hline
%   32kB & 0.069 & 0.064\\ \hline
%    \end{tabular}  
%\end{center}
%\end{table}

%\section{Machine learning module}

%Machine learning module in our system was created to decide if the algorithm should be run on CPU or GPU and for setting appropriate parameters for parallel code generation like number of threads per block, unrolling factor etc. As it was shown in case of vectors it depends on algorithm parameters if code can be faster on GPU or CPU in given configuration. Loops with small number of operations and low reusage of data should be run sequentially and are significantly faster on CPU than GPU. The cost of running them in parallel is too high because of latency of initializing threads and on GPU the parallelism it too low so not all available resources are used. Additionally, too low ratio between shared/local memory and register to device accesses gives smaller chances to speedup algorithm on GPU. The next factor which has influence on final efficiency is the amount of operations assigned to threads. The other parameters are usage of registers, local memories, unrolling factor etc. The characteristic of the source code can be described with many parameters with different ranges. Therefore in the system decision tree approach was tested to decide on which hardware accelerator the algorithm should be run and in which configuration. The algorithm was trained with vectors in which dimensions are represented by following parameters: 

%\begin{itemize}
%\item number of barriers
%\item number of conditional branches
%\item ratio number of operations to shared memory accesses
%\item ratio number of operations to local memory accesses
%\item ratio number of operations to register accesses
%\item number of operations per thread
%\item ratio number of operations to device memory accesses
%\item number of threads per block
%\item number of blocks
%\item level of parallelization (which loop parallelized in loop nesting)
%\end{itemize}

%The test dataset is group of presented benchmarks: vector addition, matrix multiplication, histograms, chosen genetic algorithms, nbody problem, reduction. Then these examples were modified in various ways to achieve some abstract source codes and make training data more diverse (data augmentation).  The chosen examples are benchmarked with different parameters (e.g. number of threads per block - 32, 64, 128, 256, 512) to filled the training data for predicting different parameters. The presented examples are quite 

%The few trees are created for predicting each parameter independently: unrolling factor, number of threads per block, hardware accelerator. The system allows to adding own source codes to enlarge training datasets.

%The trees at initial stage were trained with about 200-300 test vectors. After that trained trees were tested on modified fast fourier trees
%, random hill climbing for golomb ruler and gaussian elimination for solving linear equation. The decision tree was able to predict correct number of threads (128 or 256) and the same with unrolling factor  (in all cases factor 2 was also the best in manual implementations) and the treshold for data needed for speed up the algorithm on GPU was predicted with the error up to 500 of elements in FFT, in golomb ruler it was up to 50, and in gaussian elimination the dimension of the input matrix up to 16 (error means what is the biggest difference in the size of input data for uncorrect prediction).
%It is worth to say that for given hardware the user should run the training process on predefined or its own dataset. 


%\begin{algorithm}[H]
%\begin{algorithmic}[1]
%\STATE {$checking\_type\_of\_parallelism()$}
%\IF {$instruction\_parallelism$}
%\STATE {$transformations = loop\_transformations()$}
%\ENDIF
%\IF {$data\_parallelism$}
%\STATE {$compute\_amount\_of\_read\_data()$}
%\STATE {$compute\_amount\_of\_write\_data()$}
%\IF {$WARP\_MAPPING$}
%\STATE{$compute\_block\_memory()$}
%\ENDIF
%\IF {$THREAD\_MAPPING$}
%\STATE{$compute\_block\_memory()$}
%\STATE{$compute\_block\_data()$}

%\ENDIF
%\ENDIF
%\STATE {$compute\_amount\_of\_data\_for\_GPU()$}
%\STATE {$generate\_GPU\_code\_according\_the\_rules()$}
%\end{algorithmic}
%\caption{Mapping methodology}
%\label{alg:mapping}
%\end{algorithm}

%\begin{figure}[!htbp]
%  \centering
%  \includegraphics[scale=1.3]{fft_w2.png}
%    \caption{\centering Logarithmic chart from FFT with different number of data points and number of processor cores.}
%  \label{fig:matrix}
%\end{figure}



All experiments were run on the processor Intel Core i9-9900K, 3.6GHz, RAM 16MB. Each experiment was repeated five times and average values were computed. The version of Java used in simulations was JDK 12.0. 

%\begin{figure}[!htbp]
%  \centering
%  \includegraphics[scale=1.4]{fft_es1.png}
%    \caption{\centering Efficiency of FFT for 2 10.}
%  \label{fig:fft1}
%\end{figure}

%\begin{figure}[!htbp]
%  \centering
%  \includegraphics[scale=1.4]{fft_es2.png}
%    \caption{\centering Efficiency of FFT for 2 15.}
%  \label{fig:fft2}
%\end{figure}

\section{Conclusions and future work}

Presented results show that described automatic translation algorithms can speedup various algorithms in java virtual machine. Moreover in many cases the generated parallel code can be as efficient as manually written code. Additionally, the depicted system can choose a proper accelerator and use appropriate strategy by using machine learning approaches. Future work will concentrate on further improvements in automatic parallelisation and testing JVM parallelisation modules on more languages like Scala, JRuby etc. 
New improvements will also concern machine learning techniques for execution parameter prediction and partial parallel code generation. Further work will also concentrate on testing more complex testbench algorithms for parallelisation.

% conference papers do not normally have an appendix


% use section* for acknowledgement

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{IEEEhowto:jaorafael}
Joao Rafael, Ivo Correia, Alcides Fonseca, and Bruno Cabral, \emph{Dependency-Based Automatic Parallelization of Java Applications}, August 2014, DOI: 10.1007/978-3-319-14313-2\_16

\bibitem{IEEEhowto:smitha}
Smitha K.P, Aditi Sahasrabudhe, Vinay Vaidya, 
\emph{Method of Extracting Parallelization in Very Large Applications through Automated Tool and Iterative Manual Intervention}, Center for Research in Engineering Sciences and Technology (CREST), KPIT Technologies, Pune, India, 2012

\bibitem{IEEEhowto:elmqvist}
Hilding Elmqvist, Hans Olsson, Axel Goteman, Vilhelm Roxling,  Dirk Zimmer, Alexander Pollok, \emph{Automatic GPU Code Generation of Modelica Functions}, September 2015,  DOI: 10.3384/ecp15118235, 11th International Modelica Conference, At Paris, France

\bibitem{IEEEhowto:baskaran}
Muthu Manikandan Baskaran, J. Ramanujam and P. Sadayappan, \emph{Automatic C-to-CUDA Code Generation for Affine Programs}.
International Conference on Compiler Construction (CC) 2010

\bibitem{IEEEhowto:ppcg}
PPCG Project, http://freecode.com/projects/ppcg 

\bibitem{IEEEhowto:setoain}
Javier Setoain, Christian Tenllado, Jose Ignacio Gomez, Manuel Arenaz, Manuel Prieto, and Juan Tourino, \emph{Towards Automatic Code Generation for GPUs}

\bibitem{IEEEhowto:hou}
Kaixi Hou, Hao Wang, Wu-chun Feng, \emph{GPU-UniCache: Automatic Code Generation of Spatial Blocking for Stencils on GPUs}

\bibitem{IEEEhowto:werkhoven}
Benvan Werkhoven, \emph{Kernel Tuner: A search-optimizing GPU code auto-tuner}, Future Generation Computer Systems
Volume 90, January 2019, Pages 347-358

\bibitem{IEEEhowto:fonseca}
Alcides Fonseca, Bruno Cabral, João Rafael, Ivo Correia, \emph{Automatic Parallelization: Executing Sequential Programs on a Task-Based Parallel Runtime}

\bibitem{IEEEhowto:chan}
Bryan Chan, \emph{Run-Time Support for the Automatic Parallelization of Java Programs}

\bibitem{IEEEhowto:bondhugula}
Uday Bondhugula, M. Baskaran, S. Krishnamoorthy, J. Ramanujam, A. Rountev, P. Sadayappan. \emph{Automatic Transformations for Communication-Minimized Parallelization and Locality Optimization in the Polyhedral Model}. International Conference on Compiler Construction (ETAPS CC), Apr 2008, Budapest, Hungary.

\bibitem{IEEEhowto:bondhugula2008}
Uday Bondhugula, A. Hartono, J. Ramanujan, P. Sadayappan. \emph{A Practical Automatic Polyhedral Parallelizer and Locality Optimizer}. ACM SIGPLAN Programming Languages Design and Implementation (PLDI), Jun 2008, Tucson, Arizona.

\bibitem{IEEEhowto:amini}
Mehdi Amini, Beatrice Creusillet, Stephanie Even, Ronan Keryell, Onig Goubier, Serge Guelton, Janice Onanian McMahon, François-Xavier Pasquier, Grégoire Péan, Pierre Villalon
\emph{Par4All: From Convex Array Regions to Heterogeneous Computing}. IMPACT 2012, Second International Workshop on Polyhedral Compilation TechniquesJan 23, 2012, Paris, France.

\bibitem{IEEEhowto:liao}
Chunhua Liao, Daniel J. Quinlan, Jeremiah J. Willcock, Thomas Panas. \emph{Semantic-Aware Automatic Parallelization of Modern Applications Using High-Level Abstractions}. Journal of Parallel Programming, 2010.

\bibitem{IEEEhowto:intel_c}
Intel Corporation (2021). \emph{Intel® oneAPI DPC++/C++ Compiler}. software.intel.com.

\bibitem{IEEEhowto:ishikara}
Makoto Ishihara, Hiroki Honda, Mitsuhisa Sato. \emph{Development and Implementation of an Interactive Parallelization Assistance Tool for OpenMP: iPat/OMP}.

\bibitem{IEEEhowto:garriga_}
Albert Saa-Garriga, David Castells-Rufas, Jordi Carrabina. \emph{OMP2MPI: Automatic MPI code generation from OpenMP programs}. In High Performance Energy Efficient Embedded Systems, 2015.

\bibitem{IEEEhowto:garriga__}
Albert Saa-Garriga, David Castells-Rufas, Jordi Carrabina. \emph{OMP2HMPP: HMPP Source Code Generation from Programs with Pragma Extensions}. In High Performance Energy Efficient Embedded Systems, 2014.

\bibitem{IEEEhowto:felber}
Pascal A. Felber.  \emph{OMP2HMPP: Semi-automatic Parallelization of Java Applications}. On The Move to Meaningful Internet Systems 2003, pp. 1369–1383, LNCS, volume 2888.

\bibitem{IEEEhowto:rafael}
João Rafael, Ivo Correia, Alcides Fonseca, Bruno Cabral. \emph{Dependency-Based Automatic Parallelization of Java Applications}. Euro-Par 2014: Parallel Processing Workshops, pp. 182–193, LNCS, volume 8806.

\bibitem{IEEEhowto:chan}
Bryan Chan, Tarek S. Abdelrahman. \emph{Run-Time Support for the Automatic Parallelization
of Java Programs}. The Journal of Supercomputing, pp. 91–117, 2004.

\bibitem{IEEEhowto:han}
Guodong Han, Chenggang Zhang, King Tin Lam, Cho-Li Wang. \emph{Java with Auto-Parallelization on Graphics Coprocessing Architecture}. Conference: 42nd International Conference on Parallel Processing (ICPP '13), Lyon, France, October 2013.

\bibitem{IEEEhowto:sun}
Yu Sun; Wei Zhang. \emph{On-Line Trace Based Automatic Parallelization of Java Programs on Multicore Platforms}. 15th Workshop on Interaction between Compilers and Computer Architectures, 2011.

\bibitem{IEEEhowto:bradel}
Borys J. Bradel; Tarek S. Abdelrahman. \emph{Automatic Trace-Based Parallelization of Java Programs}. International Conference on Parallel Processing, ICPP 2007.


\end{thebibliography}

% that's all folks


\end{document}
