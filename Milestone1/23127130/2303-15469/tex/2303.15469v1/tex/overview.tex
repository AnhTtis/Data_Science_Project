In this paper, we focus on synthesizing functional manipulation for a specific task goal defined on a known category of articulated or rigid objects (\eg opening a laptop). The input of our system can be split into three parts:

\begin{itemize}
    \item A \textbf{novel object instance} from a known object category $\mathcal{C}$. The object instance is described by triangular object part meshes $\{\mathbf{M}_k\}_{k=1}^N$, where $N>1$ indicates an articulated object and $N=1$ indicates a rigid object. We assume that the number of rigid parts $N$ for each object category is known and constant.
    \item A \textbf{goal sequence} $\mathbf{G}=\{(\mathbf{S}^j\to \mathbf{S}^{j+1}, \mathbf{t}^j)\}_{j=0}^M$ that defines the goal of a manipulation task by the movement of object's parts, in which $t^j$ denotes the lasting time of the $j$-th stage, and $\mathbf{S}^j=\{\mathbf{S}^j_k\in SE(3)\}_{k=1}^N$ denotes the 6D poses of each rigid part at the $j$-th stage transition point. The whole manipulation procedure is subdivided into multiple stages according to the object's state of movement (\eg A goal sequence of the \textit{opening} task for laptops consists of two stages: \textit{approaching} and \textit{opening}, as shown in Figure~\ref{fig:Head}). %\qymodify{$\mathbf{G}=\{(\mathbf{S}^j\to \mathbf{S}^{j+1}, \mathbf{t}^j)\}_{j=0}^M$ still looks confusing here}
    \item An \textbf{initial hand pose} $\mathbf{H}_0\in\mathbb R^{51}$ represented by the $48$-dimensional MANO\cite{mano} pose parameters and a 3D wrist position.
\end{itemize}

The goal is to generate a sequence of human-like hand motions that is represented by a hand pose sequence $\{\theta_t\in\mathbb{R}^{51}\}_{t=1}^T$. Such a sequence of motion should be consistent with the given goal sequence of the manipulated object. To make this generation task reasonable, the given goal sequence should conform to the object's articulation constraint and follow its functionality.% \modify{how to claim out goal? functionality? @zjt}*
% In order to generate realistic and human-like motions, our framework is supervised by datasets of collected human motion priors. For each object category and a corresponding manipulation task, our framework is trained with a motion dataset that demonstrates how a human would accomplish the task on a diverse set of object instances in the category. In testing stage, our framework needs to generate motions of the same task for unseen testing objects. The testing objects can be either unused objects in training dataset, or out-of-domain objects from other datasets.



% \subsection{System Overview}

% The overall framework of our system is illustrated in (\figref{fig:framework}). The core of our system is the novel object-centric \textbf{CAnonicalized Manipulation Spaces (CAMS)} representation of finger-object interaction. The CAMS representation consists of two components: the \textbf{local reference frame} that centers at the contact point on object surface, and 