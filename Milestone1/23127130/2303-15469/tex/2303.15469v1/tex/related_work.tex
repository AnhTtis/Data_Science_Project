\label{sec:relatedwork}

\subsection{Human Motion Synthesis}

Human motion synthesis, including motion prediction, interpolation, and completion, has attracted many interests these years \cite{humanmotion4, humanmotion1, zhang2021mojo, cai2021unifiedhumancvae, humanmotion2, humanmotion3, transcvae, actionhumanmotion, henter2020moglow, longtermsceneawarecvae, awarehuman, couch, li2021ai, avatarclip, huang2023diffusion, lee2023locomotion, saga, zhang2022wanderings}. Conditional variational autoencoder\cite{cvae} (CVAE) was widely used\cite{zhang2021mojo, cai2021unifiedhumancvae, actionhumanmotion, transcvae} for its generalizability across various scenes and human motions. % Different from CVAEs, MoGlow\cite{henter2020moglow} proposed a Glow-based\cite{glow} method to generate deterministic data-driven human motion. 
These works have achieved great success in human motion synthesis, while their potential in HOM synthesis was overlooked. In particular, \cite{longtermsceneawarecvae, awarehuman, couch} focused on modeling human-scene interaction when generating human motion. Such scene-aware or context-aware methods might also suit HOM synthesis.

\subsection{Physics-Based Object Manipulation Synthesis}

Generating high-quality human grasps remains challenging due to the complex geometry and complicated skeletal constraints. Physics-based methods\cite{samplebased1, samplebased2, physic1, ibs, Dgrasp, chopstick} were favored for in-hand manipulation synthesis since the generated motion was physically plausible. IBS\cite{ibs} presented a novel representation of hand-object interaction and leveraged reinforcement learning (RL) methods with execution success, and geometric measure \cite{219918} rewards to generate successful grasping motion. D-Grasp\cite{Dgrasp} developed its grasping policy based on the physical attributes of hand and object, including angles and velocities. Yang \etal \cite{chopstick} concentrated on using chopsticks in diverse gripping styles, and it solved this rather difficult task by first optimizing physically valid gripping poses with predefined gripping styles and then utilizing the carefully designed hand-controlled policies to synthesize manipulation.

\subsection{Data-Driven Object Manipulation Synthesis}

Besides physics-based methods, a line of data-driven approaches\cite{graspingfield, grasptta, CPFChen_2022, static1, static2, static3, static4, static5, zhang2021manipnet, GRAB:2020} could generate manipulation in a more natural and human-like manner and generalize to novel object instances. Most of the previous data-driven works focused on reconstruction and synthesis for the static grasp\cite{graspingfield, grasptta, CPFChen_2022, static1, static2, static3, static4, static5}. Grasping Field\cite{graspingfield} and CPF\cite{CPFChen_2022} reconstructed a static grasping field in 3D space for hand-object interaction based on an RGB image. GraspTTA\cite{grasptta} predicted a static joint-angle configuration of the grasping hand from a given object point cloud and contributed a Test Time Adaptation (TTA) strategy for helping the method generalize to novel objects.
% GraspTTA had the potential to synthesize dynamic grasps for its Test Time Adaption(TTA) strategy, 
% \modify{what is the main reason?}
% \qymodify{whereas it is not that easy to transfer static grasping synthesis to dynamic grasping synthesis, considering that the dynamic grasping contains continuous changes of hand pose, which are subtle and hard to learn}.
Going beyond static grasping synthesis,  ManipNet\cite{zhang2021manipnet} proposed several geometric sensors and managed to generate long-term complex manipulation sequences. TOCH\cite{TOCH} achieves a data-driven approach of dynamic motion refinement in hand object motion synthesis.
% However, ManipNet required a given hand wrist trajectory, and different from our work, ManipNet only generated random manipulation based on a given trajectory, while our work could generate the animation with functional-intent\modify{functional-intent about what?}.
% However, the grasps generated by ManipNet were randomly distributed around the object but lacked the understanding of object functionality, which were different from the humans behaviors.
% To summarize, synthesis of dynamic human-like in-hand manipulation using data-driven methods remained unexplored. Our work proposes a new representation for modeling dynamic hand-object interaction from real data, and could generate long-term functional-intent human-like manipulation sequences with diverse grasping type taking only start and end object state and start hand state as input. We will illustrate that this pipeline could not only handle manipulation of rigid objects, but also complex articulated objects, and could be generalized in category level later.


% A proper representation is essential in hand-object interaction modeling, considering that there are specific affordance\modify{what is "specific affordance"?} and contact reasoning in such interaction.
% A proper representation is essential in Hand-Object Interaction (HOI) modeling.
% \modify{what is the relationship between "proper representation" and the subsequent sentences?}
% Many works\modify{cite} about HOI reconstruction and synthesis utilized joint-angle configuration of the grasping hand directly and added physical constraints on it.
% D-Grasp\cite{Dgrasp} constructed a grasping policy conditioning on velocities and angles of hand and object, which could enhance the physical plausibility during the synthesis procedure. To generate complicated in-hand manipulation with high quality, previous progresses\cite{affordancenet, grasptta, chopstick} utilized contact heuristics and affordance. 3D AffordanceNet\cite{affordancenet} indicated the importance of visual affordance for learning functionality of hand-object interaction. GraspTTA\cite{grasptta} leveraged contact map as supplementary affordance to learn grasping. Yang \etal \cite{chopstick} used predefined gripping poses to guide the interaction between hand and chopsticks.

% Given a parametric human hand model (\eg MANO\cite{mano}), instead of directly regressing the hand poses, methods\cite{CPFChen_2022, graspingfield, grasptta, ibs, TOCH, zhang2021manipnet} for grasp synthesis predicted a carefully-designed HOM representation as a guidance of generating realistic human grasps. 
% Such HOM representation could encode the contact pattern between the hand and the interacted object.
% Various HOI representations have been proposed for synthesizing static\cite{CPFChen_2022, graspingfield, grasptta} or dynamic\cite{ibs, TOCH, zhang2021manipnet} grasps.
% CPF\cite{CPFChen_2022} presented an explicit representation that modeled contact as a spring-mass system correlating the hand and the object. Grasping Field\cite{graspingfield} constructed hand grasps on a signed distance field by mapping an arbitrary 3D point to both the hand surface and the object surface. GraspTTA\cite{grasptta} leveraged contact map as supplementary affordance to learn grasping.
% To handle the grasping dynamics, IBS\cite{ibs} designed Interaction Bisector Surfaces to encode interaction of hand and object in motion planning, while TOCH\cite{TOCH} presented TOCH field that encoded the projection from hand to object, including the corresponding object normal vectors implicitly, to refine grasping motion.
% To synthesize complex long-term in-hand manipulation sequence, ManipNet\cite{zhang2021manipnet} utilized several distance-based representations generated from virtual sensors, which provided detailed grasping information and ample local object geometry to guide the grasp generation.
% However, such method was too expensive. Our work differs in that we propose an object-centric and contact centric representation - canonicalized manipulation space (CAMS), which concentrates on information about contact and is suitable for task-specific long-term manipulation synthesis. Another advantage of representing contact in CAMS is that, such unified space helps to generalize contact from seen object to new shape in category level.