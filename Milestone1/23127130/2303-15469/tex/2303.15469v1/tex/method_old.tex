\label{sec:method}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=2\columnwidth]{images/overview.png}
\end{center}
   % \caption{\textbf{System Overview.} Our framework mainly consists of two parts: (a) Planner and (b) Synthesizer. (a) The Planner is used to generate CAMS embedding. At training time, the Planner takes the object condition and the whole manipulation as input and encode them into the latent space. During inference, the Planner could sample a latent code from the latent space and decode it CAMS embedding. (b) The Synthesizer focuses on generating real manipulation configuration based on CAMS embedding. It first fits the initial hand configuration with finger embedding predicted by the Planner, and then optimizes the whole manipulation based on the CAMS embedding.}
   \caption{\textbf{System Overview.} Our framework mainly consists of a CVAE-based planner module and an optimization-based synthesizer module. Given the generation condition as input, the planner first generates a per-stage CAMS representation containing contact reference frames and sequences of finger embedding. Then the synthesizer optimizes the whole manipulation animation based on the CAMS embedding.}
   \vspace{-0.4cm}
\label{fig:framework}
\end{figure*}

Figure~\ref{fig:framework} shows the framework of our system. We first present \textbf{CAnonicalized Manipulation Spaces} (\textbf{CAMS}, see Section~\ref{sec:cams}), a two-level space hierarchy that allows representing dynamic hand motions in an object-centric and contact-centric way.
By embedding moving hands in CAMS, we obtain a more compact description for dynamic manipulations which is more friendly to learning.
%Leveraging, as the intermediate representation in our synthesizing pipeline. Based on the CAMS representation, 
To learn to synthesize human-like manipulations, we propose a two-stage framework consisting of a CVAE-based \textbf{planner} module (see Section~\ref{sec:planner}) and an optimization-based \textbf{synthesizer} module (see Section~\ref{sec:synthesizer}). Taking the object shape $\mathbf{O}$, the manipulation goal $\mathbf{G}$ and the initial hand configuration $\mathbf{H}_0$ as input, we first separate the whole manipulation process into several \textbf{stages} by the sequential goals. Then the planner module generates
% discrete contact patterns for each stage, together with 
sequences of finger embeddings in CAMS as a guidance for hand manipulation synthesis. Finally, the synthesizer module generates a human-like and physically realistic HOM animation by fitting the generated CAMS embeddings.
% Taking a CAD object model from a known category, and a manipulation goal, our method could generate a sequence of task-specific, human-like, realistic hand-object interaction animation.
% Note that the animation we wanted here is task-specific, meaning that a default task is implicitly bounded with the known object category.

% Specifically, a manipulation goal is defined as $\mathbf{G}=\{\mathbf{H}_s, \mathbf{O}_s, \mathbf{O}_e\}$, where the parameters $\mathbf{H}_s$, $\mathbf{O}_s$, $\mathbf{O}_e$ denote initial hand configuration, initial object 6D pose, and end object 6D pose, respectively. \modify{update the setting} \modify{high-level functions of CAMS} The Planner will plan a hand motion in CAMS using CAMSNet from the CAD object model and the manipulation goal. Then the Synthesizer would refine the generated motion with ???@lixing.

% Given above input and goal, there are several challenges we  face. First, our framework should handle problem of object shape diversity, so that we could generate animation in category level. Second, our framework needs the ability to handle the problem of manipulation diversity to show different manipulation mode for a specific task in synthesised animation. And finally, although we don't use strict physical constraints, our results should be at least physically realistic, and ensure that there is no obvious penetration or fake manipulation (manipulate object with no contact between hand and object). To overcome these problems, we propose canonicalized manipulation spaces in (Section~\ref{sec:cams}). We will argue that such canonicalized hand motion representation concentrates more on local object geometry, where hand-object interaction tends to occur, and encode more details about contact between hand and object, to increases the generalizability of our framework. 

\subsection{CAnonicalized Manipulation Spaces (CAMS)}
\label{sec:cams}

It is challenging to directly regress a human-like hand animation from the given object shape, due to the shape variety within a category and the huge diversity of human manipulation patterns. For two object instances that have a non-negligible geometry difference (e.g. two mugs with different sizes of handle), even if we apply the same manipulation pattern on them (e.g. grasping the handle in a same way), the resulting hand shape can also be significantly different. However, if we express each finger's motion in the reference frame centered at its contact point on the object's surface, the resulting representation will rely much less on the object instance's shape. Starting from this idea, we introduce CAnonicalized Manipulation Spaces (CAMS) with two-level canonicalization for manipulation representation. At the root level, all corresponding parts from the category of interest are scale-normalized and consistently oriented, to canonicalize the potential contact positions. At the leaf level, each potential contact point would induce a local contact reference frame along the object surface to further canonicalize the corresponding finger pose, resulting a finger-level CAMS embedding for the whole hand.
We will first present \textbf{canonicalized contact reference frames} (see Section~\ref{subsec:CR}) which describe the discrete contact information and are keys to constructing CAMS.
% in canonical frames of object parts.
Then, we present a \textbf{canonicalized finger embedding} strategy (see Section~\ref{subsec:FE}) to transform the absolute hand pose into finger-level CAMS embeddings.
% object-centric representation.
% \eric{CAMS should be a space rather than the embedding. fix}.
Finally, we present how we generate a sequence of finger-level CAMS embeddings to support
% Together, we propose CAMS as the combination of both the discrete contact reference frames and the corresponding sequences of finger embedding. We finally describe the approach of applying CAMS to support 
HOM synthesis in Section~\ref{subsec:CAMS_in_Sequence}.

% (zjt) We first sample one frame where contact happens from the sequence to illustrate how to model hand-object interaction using CAMS, and then introduce how to expand CAMS from one frame to the whole sequence. Without loss of generality, we use one rigid object as an example, and for articulated object, we can treat them as several detached rigid objects and process them identically in this section for simplicity. The CAMS is a space built based on two-level hierarchical canonicalization, defined as a 3D space within a unit cube $i.e., \{x, y, z\} \in [0,1]$ similar to NOCS \cite{nocs}. We normalize the given object CAD model and the contact using the object-oriented canonicalization, and center the object within the cube. In CAMS, we represent the contact between hand and object as contact references for five fingers respectively and finger embedding built upon contact references by contact-oriented canonicalization.

\begin{figure*}[!h]
    \centering
    \vspace{-5pt}
    \includegraphics[width=\linewidth]{images/CAMS.png}
    \caption{\textbf{Overview of CAnonicalized Manipulation Spaces (CAMS)}. (a) We first predict object-centric canonicalized contact reference frames $F_c=R\dot U$ as local contact priors for grasping. (b) We then separately embed the contact features to each finger, where the feature for a finger is exemplified in (c).}
    \label{fig:CAMS}
    \vspace{-5pt}
\end{figure*}

\subsubsection{Canonicalized Contact Reference Frames}
\label{subsec:CR}
The canonicalized contact reference frames  $\mathbf{F_c}$ connects the two levels of spaces in CAMS. 

defines a set of spaces

is a space built based on two-level hierarchical canonicalizations $\mathbf{F_c} = \mathbf{R} \cdot \mathbf{U}$, as shown in Figure~\ref{fig:CAMS} (a).% where $\mathbf{U}$ maps the object points to a category-level normalized object coordinate space\cite{nocs}, and $\mathbf{R}$ .

\textbf{Object-centric canonicalization $\mathbf{U}$.} Inspired by NOCS\cite{nocs}\eric{should this include ANSCH as well?}, for every object category, we normalize all the rigid parts into canonicalized 3D spaces within the unit cube ($i.e., \{x, y, z\} \in [0,1]$). The normalized object part has the same center as the unit cube, as well as a category-specific constant orientation.

\textbf{Contact-centric canonicalization $\mathbf{R}$.} After normalizing an object part to the category-level canonical space, we represent the contact between the hand and the normalized object part as contact references for five fingers, respectively. For the $i$-th finger, we formulate contact references as $\mathbf{R}_i=\{(\mathbf{C}_i, \mathbf{V}_i, \mathbf{N}_i)\}_{i=1}^5$, where $\mathbf{C}_i \in [0,1]$ is a binary flag showing whether the $i$-th finger is in contact with the object part,  $\mathbf{V}_i \in \mathbb{R}^3$ is a point on the object part surface indicating the center of contact between vertices of the $i$-th finger and the object part, and $\mathbf{N}_i \in \mathbb{R}^3$ is the corresponding normal vector of $\mathbf{V}_i$ on the part surface.\eric{what about the tangent directions? this does not form a frame no?}

% The insights of our design are twofold. First, the contact references encode local object geometry explicitly by using $\mathbf{V}_i$ and $\mathbf{N}_i$, which could exploit the similarity of local patterns and help CAMS generate to novel object instances. Second, as shown in Figure \modify{need a figure}, such designed contact reference field could be strong related to the HOM motion type.
% For example, \emph{???same manipulation type, contact reference remains the same, while hand pose changes drastically, scale?}. On the other hand, contact reference is sensitive to different manipulation types for a specific task, for example, \emph{???different manipulation types, contact reference changes drastically, while hand pose remains similar, laptop?}```

% (zjt) We formulate contact reference as $\mathbf{R}=\{(\mathbf{C}_i, \mathbf{V}_i, \mathbf{N}_i)\}_{i=1}^5$, where $\mathbf{C}_i \in [0,1]$ is a binary flag showing whether the $i$-th finger is in contact with the object,  $\mathbf{V}_i \in \mathbb{R}^3$ is a point on the object surface indicating the center of contact between vertices of the $i$-th finger and the object, and $\mathbf{N}_i \in \mathbb{R}^3$ is the corresponding normal vector of $\mathbf{V}_i$ on the object surface. The contact reference encodes local object geometry explicitly by using $\mathbf{V}_i$ to show relative  contact position of the object in CAMS and $\mathbf{N}_i$ to indicate local surface information. It is essential to sense local object geometry to handle shape diversity, considering that however different shapes in a category might be, the local object geometry tends to be similar within the category, with similar 'function of contact'. Because of the ability to sense local object geometry, our CAMS representation could be generalized to various shapes including unseen object in category level. What's more, different from representations like hand pose, contact reference is an unified representation for a specific manipulation type, irrespective of scale of object. For example, \emph{???same manipulation type, contact reference remains the same, while hand pose changes drastically, scale?}. On the other hand, contact reference is sensitive to different manipulation types for a specific task, for example, \emph{???different manipulation types, contact reference changes drastically, while hand pose remains similar, laptop?}

\subsubsection{Canonicalized Finger Embedding}
\label{subsec:FE}

Figure~\ref{fig:CAMS} (b) and (c) illustrate our canonicalized finger embedding. Regarding $\{\mathbf{V}_i\}_{i=1}^5$ as the reference positions for the hand tips, we define the canonicalized finger embedding as $\mathbf{F}=\{\mathbf{F}_i=(\mathbf{J}^{tip}_i, \vec{\mathbf{J}}^1_i, \vec{\mathbf{J}}^2_i, \vec{\mathbf{J}}^3_i, \vec{\mathbf{J}}^{root}_i)\}_{i=1}^5$, where $\mathbf{J}^{tip}_i$ denotes the position of the $i$-th finger tip represented by the coordinate system of contact reference frame, and $\vec{\mathbf{J}}^1_i$, $\vec{\mathbf{J}}^2_i$, $\vec{\mathbf{J}}^3_i$, $\vec{\mathbf{J}}^{root}_i$ are four direction vectors from $\mathbf{J}^{tip}_i$ to the second, the third, the forth MANO joints of the $i$-th finger and the root joint of the hand, respectively. In such form of embedding, the relative motion between finger and the reference point is encoded in $J^{tip}_i$, while the finger's direction is encoded in the rest terms\eric{what does this mean?}.

% (zjt) Taking $\mathbf{V}_i$ in the corresponding contact reference as original point, we can extract finger embedding $\mathbf{F}=\{(\mathbf{J}^{tip}_i, \vec{\mathbf{J}}^1_i, \vec{\mathbf{J}}^2_i, \vec{\mathbf{J}}^3_i, \vec{\mathbf{J}}^{root}_i)\}_{i=1}^5$, where $\mathbf{J}^{tip}_i$ is the position of the finger tip of the $i$-th finger in contact reference coordinate, and $\vec{\mathbf{J}}^1_i$, $\vec{\mathbf{J}}^2_i$, $\vec{\mathbf{J}}^3_i$, $\vec{\mathbf{J}}^{root}_i$ are four normalized vector pointing to the second, the third, the forth joints of the $i$-th finger and the root joint of the hand respectively, starting from $\mathbf{J}^{tip}_i$. We use relative normalized vectors rather than absolute positions to avoid conflict and stress the importance of finger tip, considering that there are frequent contacts between finger tip and object, and there is a specific relative normalized vectors pattern for the following finger joints after restricting the finger tip. The finger embedding indicates the state of a finger when contact happens, and given that it is canonicalized by contact reference, it helps the model to concentrate more on relations between finger and local object geometry in a local space.

\subsubsection{CAMS Embedding Sequence}
\label{subsec:CAMS_in_Sequence}

Our CAMS embedding sequence is defined as $\{\mathbf{R}^{(i)}, \mathbf{F}^{(i)}\}_{i=1}^N$ for $N$ object parts \eric{R is not embedding not?}, where $\mathbf{R}^{(i)}$ and $\mathbf{F}^{(i)}$ are introduced subsequently. For the $i$-th object part, we regard the manipulation phase of this part as a sequence of $S^{(i)}\geq1$ static contact stages in which the contact points remains approximately stationary related to the object part. We define the $S^{(i)}+1$ frames that separate the static contact stages as reference frames $\{\mathbf{I}_s^{(i)}\}_{s=1}^{S^{(i)}+1}$. We generate canonicalized contact reference fields $\mathbf{R}^{(i)}=\{\mathbf{R}_s^{(i)}\}_{s=1}^{S^{(i)}+1}$ for all reference frames, where $\mathbf{R}_s^{(i)}=\{\mathbf{C}_s^{(i),f}, \mathbf{V}_s^{(i),f}, \mathbf{N}_s^{(i),f}\}_{1\leq f \leq 5}$ for five fingers respectively. For each static stage $s$, given both $\mathbf{R}_s^{(i)}$ and $\mathbf{R}_{s+1}^{(i)}$, we subsequently construct a temporally contiguous bidirectional finger embedding denoted as $\mathbf{F}_s^{(i)}=\{\mathbf{F}_s^{(i),start}, \mathbf{F}_s^{(i),end}\}$, where $\mathbf{F}_s^{(i),start}$ and $\mathbf{F}_s^{(i),end}$ are respectively represented in the object part coordinate system of reference frame $\mathbf{I}_s^{(i)}$ and $\mathbf{I}_{s+1}^{(i)}$. Each $\mathbf{F}_s^{(i),*}$ consists of $\{\mathbf{J}_t^{(i),tip}, \mathbf{J}_t^{(i),1}, \mathbf{J}_t^{(i),2}, \mathbf{J}_t^{(i),3}, \mathbf{J}_t^{(i),root}\}_{1 \leq t \leq T^{\prime}}$, where $T^{\prime}$ is the number of frames in stage $s$. The $\mathbf{F}^{(i)}$ denotes $\{\mathbf{F}_s^{(i)}\}_{s=1}^{S^{(i)}}$.
% To summarize, we expand CAMS in the manipulation period with separative contact reference fields and continuous finger embedding, which jointly formulates the CAMS embedding sequence.
% Benefiting from CAMS, we can sense local object geometry of contact and represent contact explicitly, rather than representing contact implicitly in a mass of point clouds of hand and object. \modify{why it can solve shape diversity?}

% (zjt) Now we can represent the contact of a frame in CAMS, however, it is not that easy to find a unified representation for the whole sequence, which means we should even build CAMS on frames where no contact happens. Based on our observation, a manipulation could be empirically split into several stages: approaching stage, manipulation stage, and leaving stage. The approaching stage and the leaving stage are easy to understand, denoting hand moving from one position to object, and leaving object, respectively, while the manipulation stage might be confusing. We define a manipulation stage as a short period of time when the hand reaches a robust, static contact with object, here a static contact means contact pairs of hand and object don't change obviously, while the hand motion might keep changing. Some complex manipulation might contain several static contacts and thus there are several manipulation stages (like example of Scissor in (Section~\ref{})), but we just explain condition of one manipulation stage here for simplicity. After splitting the manipulation into different stages, we can generate contact references. Considering that the static contact along a stage won't change, we just need contact references for the start frame and the end frame of each stage to represent static contact, and for those frames without a static contact, like the start frame in approaching stage, we just ignore them. With the generated start and end contact references in a stage, we can build a sequence of bidirectional finger embedding for each frame in the stage. For convenience, we unify symbols of CAMS in manipulation here: Given a manipulation over the course of $T$ frames, we can split the sequence into $S$ stages based on $S+1$ static contacts in $S+1$ reference frames respectively(including the start and the end frame of the sequence). From static contacts, we can generate discrete contact reference in each reference frame, denoted as $\mathbf{R}=\{\mathbf{R}_s\}_{1 \leq s \leq S}$ for $S$ reference frames, where $\mathbf{R}_s=\{\mathbf{C}_s^f, \mathbf{V}_s^f, \mathbf{N}_s^f\}_{1\leq f \leq 5}$ for five fingers respectively. The bidirectional finger embedding is built based on contact reference, specifically, in a random stage $s \in [1, S]$, the bidirectional finger embedding is represented as $\mathbf{F}=\{\mathbf{F}_s^{start}, \mathbf{F}_s^{end}\}_{1\leq s \leq S}$, where $\mathbf{F}_s^{start}$ is centriced in the coordinate of the contact reference of the start frame of the stage $s$, and $\mathbf{F}_s^{end}$ is in the coordinate of end frame contact reference. And $\mathbf{F}_s$ consists of $\{\mathbf{J}_t^{tip}, \mathbf{J}_t^1, \mathbf{J}_t^2, \mathbf{J}_t^3, \mathbf{J}_t^{root}\}_{1 \leq t \leq T^{\prime}}$, where $T^{\prime}$ is the length of the sequence in stage $s$. To summarize, we expand CAMS in manipulation sequence with discrete contact references and continuous finger embedding. With CAMS, we can sense local object geometry of contact and represent contact explicitly, rather than representing contact implicitly in a mass of point clouds of hand and object. \emph{why it can solve shape diversity?} 

\subsection{Planner}
\label{sec:planner}

Given an object point cloud and a manipulation goal, we present CAMSNet, a novel planner for generating a CAMS embedding sequence introduced in Section~\ref{subsec:CAMS_in_Sequence}.

\textbf{Network Architecture}
An overview of CAMSNet architecture is shown in Figure~\ref{fig:network}. Inspired by \cite{actionhumanmotion, transcvae}, we utilize a CVAE-based structure to enhance the generalizability among various manipulation modes. Our CAMSNet extracts the object feature from the given object point cloud $\object \in \mathbb{R}^{k\times 3}$ 
% (where $k$ indicates the number of points) 
using a PointNet encoder \cite{pointnet}, and subsequently combines the object feature with $\mathbf{G}=\{\mathbf{S},\mathbf{H}_s\}$ to generate the condition $\mathcal{F}_{cond}$ for CVAE. 
% The condition is denoted as $\mathcal{F}_{cond} \in \mathbb{R}^{32}$. 
During the training procedure, we encode the manipulation by concatenating the condition $\mathcal{F}_{cond}$, contact reference $\mathbf{R}$ 
% \in \mathbb{R}^{S \times N \times 35}$
, and the bidirectional finger embedding $\mathbf{F}_{bi}$.
%\in \mathbb{R}^{T \times S \times N \times 160} $ in CAMS, where $T$ is the length of the sequence, $S$ is the number of predefined stages, and $N$ is the number of part of object in articulated category. 
% For a single object part in each frame, $\mathbf{F}_{bi}$ is a 160D vector which consists of a bidirectional finger embedding $\{\mathbf{F}^{start}, \mathbf{F}^{end}\} \in \mathbb{R}^{2 \times 5 \times 5 \times 3}$, as well as a contact flag $Flag_{c} \in \mathbb{R}^5$ and a near-object flag $Flag_{n} \in \mathbb{R}^5$ for convenience of loss computing. 
The VAE encoder predicts mean $\mu \in \mathbb{R}^{64}$ and variance $\theta^2 \in \mathbb{R}^{64}$ of the posterior Gaussian distribution $\mathbf{Q}(z|\mu, \theta^2)$ \cite{vae}.
% Our CVAE-based CAMSNet is able to encode different manipulations into latent space, and has the ability to sample a task-specific manipulation in random manipulation type conditioned on $\mathcal{F}_{cond}$ because of the generalizability of CAMS.
After completing the portrayal of the latent space distribution, we sample a latent code $z$ from latent space and pass it to our decoder together with a time embedding $\mathcal{T}$ and the condition $\mathcal{F}_{cond}$.
% The time embedding $\mathcal{T}$ comes from normalized time sequence $\mathbf{T} \in [0, 1]$ denoting the position of a frame in manipulation relatively.
The decoder generates predicted motion represented by CAMS, denoted as contact reference $\hat{\mathbf{R}}=\{\hat{\mathbf{R}}_s\}_{1\leq s \leq S}$ for $S$ stages, as well as the bidirectional finger embedding sequence $\hat{\mathbf{F}}=\{\hat{\mathbf{F}}_s^{start}, \hat{\mathbf{F}}_s^{end}\}_{1\leq s \leq 
S}$.
%, where $\hat{\mathbf{F}}_s=\{\hat{\mathbf{J}}_t^{tip}, \hat{\mathbf{J}}_t^1, \hat{\mathbf{J}}_t^2, \hat{\mathbf{J}}_t^3, \hat{\mathbf{J}}_t^{root}\}_{1\leq t \leq T^{\prime}}$.

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=2\columnwidth]{images/network.png}
\end{center}
   \caption{\textbf{Network Architecture.} Our network mainly consists of (a) an encoder, and (b) a decoder. (a) The encoder first encodes the condition and time sclae to get $\mathcal{F}_{cond}$ and time embedding $\mathcal{T}$. At training time, the network takes $\mathcal{F}_{cond}$ and the whole manipulation as input and encode them into the latent space. During inference, the network could sample a latent code from the latent space directly. (b) The latent code is concatenated with the condition $\mathcal{F}_{cond}$ first, and they are sent to a stage decoder to generate contact reference frames. Meanwhile, they are expanded $\mathbf{T}$ times and then concatenated with the time embedding $\mathcal{T}$ to get the sequence of finger embedding in different stages.}
 
\label{fig:network}
\end{figure*}

\textbf{Loss Function}
For simplicity, we use hatted symbols consistently in the following equations to denote network prediction, and vise versa for the ground truth values. Please refer to the supplementary for details of the following loss.
We leverage several losses to optimize the CAMSNet.
The first objective is a Binary Cross Entropy (BCE) loss $\mathcal{L}_{flag}$ between the predicted flags and ground-truth flags denoting whether there is contact between the finger and object at different manipulation stages, and this flag corresponds to the contact reference frames.  
We then adopt the $L_2$ distance to compute the reconstruction error of $\mathbf{V}_j$ and $\mathbf{N}_j$ in contact reference
\begin{equation}
\begin{split}
& \mathcal{L}_{pos}=\sum_{j=1}^J \sum_{f=1}^5 \mathbf{C}_j^f( \|\hat{\mathbf{V}}_j^f-\mathbf{V}_j^f\|_2^2), \\
& \mathcal{L}_{dir}=\sum_{j=1}^J \sum_{f=1}^5 \mathbf{C}_j^f( \|\hat{\mathbf{N}}_j^f-\mathbf{N}_j^f\|_2^2),
\end{split}
\end{equation}
where we only compute $\mathcal{L}_{pos}$ and $\mathcal{L}_{dir}$ when the ground truth contact flag $\mathbf{C}_j^f=1$. Similarly, we also utilize the $L_2$ distance to optimize finger embedding when $\mathbf{C}_j^f=1$, denoted as $\mathcal{L}_{tip}$ for $\hat{\mathbf{J}^{tip}}$ and $\mathcal{L}_{vec}$ for $\hat{\mathbf{J}^{joints}}$, where $joints \in \{1, 2, 3, root\}$ indicates different joints on a finger.

%We also utilize the $L_2$ distance to optimize finger embedding. %Considering that we only care about the hand configuration when it is close to the object, we use $Flag_{n}$ to filter out frames when the hand is far from the object. 
%The loss functions for finger embedding are
% Note that there is finger embedding only if there is corresponding contact reference, since that it is built on contact reference.

In addition, we use the KL-Divergence to constrain the distribution of latent code $\mathbf{Q}(z|\mu, \theta^2)$  following \cite{vae} and force it to be close to the Gaussian distribution, formulated as $\mathcal{L}_{KLD}$

The final loss function is a weighted summation of 6 different losses:
\begin{equation}
\begin{split}
    \mathcal{L} & = \lambda_{flag} \cdot \mathcal{L}_{flag} + \lambda_{pos} \cdot \mathcal{L}_{pos} + \lambda_{dir} \cdot \mathcal{L}_{dir} \\ & + \lambda_{tip} \cdot \mathcal{L}_{tip} + \lambda_{vec} \cdot \mathcal{L}_{vec} + \lambda_{kld} \cdot \mathcal{L}_{KLD}.
\end{split}
\end{equation}


\subsection{Synthesizer}
\label{sec:synthesizer}

Once a CAMS embedding sequence has been generated from the planner, our synthesizer subsequently predicts a complete HOM sequence. We simply use Bessel curve based interpolation to generate the object trajectory, and thus focus on synthesizing a hand trajectory that satisfies our goal (Section~\ref{sec:overview}).
% The CAMS embedding sequence contains only finger joints embedding, which cannot be directly used for contact optimization.
Given the object shape and trajectory, as well as a CAMS embedding sequence, our synthesizer adopts a two-stage optimization method that first optimize the MANO pose parameters to best fit the CAMS finger embedding (see Section~\ref{subsec:fitting_finger_embedding}), and then optimize the contact effect to improve physical plausibility (see Section~\ref{subsec:optimizing_contact_and_penetration}).

% \subsubsection{Planning object trajectory}
% \label{subsec:planning_object_trajectory}
% To plan the trajectory of object, we apply linear interpolation to get the 6D pose $\bar{\mathbf{S}}^{j}_{i,k}=\mathbf{S}^{j}_i+(\mathbf{S}^{j}_{i+1}-\mathbf{S}^{j}_i)\cdot k/t_i$ of object $\mathbf{M}_j$ for each frame $k$ in stage $i$.

\subsubsection{Fitting Finger Embedding}
\label{subsec:fitting_finger_embedding}

Given the contact reference $\hat{\mathbf{R}}$ and the bidirectional finger embedding $\hat{\mathbf{F}}$ from a CAMS embedding sequence, we optimize the MANO parameters of hand pose and transition $\theta=\{\theta_i\}_{i=1}^{T}$ by minimizing:

\begin{equation} \label{eq:fit_loss}
\begin{split}
    \mathcal{L}(\theta) = & \lambda_{\mathrm{tip}}\sum_{i=1}^{T}\mathcal{L}_{\mathrm{tip}}(\theta_i)+\lambda_{\mathrm{joint}}\sum_{i=1}^{T}\mathcal{L}_{\mathrm{joint}}(\theta_i) \\
    &+ \lambda_{\mathrm{smooth}}\mathcal{L}_{\mathrm{smooth}}(\theta).
\end{split}
\end{equation}

The first term of Eq.\eqref{eq:fit_loss} is the tip transition loss, which constrains the tip position $\mathbf{J}^{tip}$ of the finger in the canocicalized finger embedding. The second term of Eq.\eqref{eq:fit_loss} is the joint orientation loss, which is used to optimize the direction vectors of four subsequent joints $\vec{\mathbf{J}}^1_i$, $\vec{\mathbf{J}}^2_i$, $\vec{\mathbf{J}}^3_i$, $\vec{\mathbf{J}}^{root}_i$ of the finger in the canocicalized finger embedding. And the last term of \eqref{eq:fit_loss} is a smoothness loss for improving temporal continuity

\subsubsection{Optimizing Contact and Penetration}
\label{subsec:optimizing_contact_and_penetration}

After fitting MANO parameters $\theta$ by finger embedding, we leverage another optimization-based method to handle the penetration and inaccurate contact issues of the hand pose. The optimization course refines $\theta$ to physically realistic MANO parameters $\theta^\prime$ as the final result of synthesis.
% We first construct the hand mesh sequence by MANO model from the current MANO parameters $\theta$.

To achieve better contact quality, we define a contact loss $\mathcal{L}_{\mathrm{contact}}$ to attract the nearby finger vertices to the local surface section. We first find all frame-part-finger tuples $(i,j,k)$ where $\hat{\mathbf{C}}_s^f=1$ between the $j$-th object part and the $k$-th finger in frame $i$, indicating where contact should take place. For each of such tuple $(i,j,k)$, we search for a local section $\mathbf{M}^\prime_{i,j,k}$ of object part geometry $\mathbf{M}_j$ which is very close to the predicted reference frame $\hat{\mathbf{V}}_{i,j,k}$ and all the vertex normals in such local section are within a fixed included angle range $\alpha$ compared to the predicted contact normal $\hat{\mathbf{N}}_{i,j,k}$. We calculate the signed distances $\{\mathbf{d}_{i,j,k,l}\}_{l=1}^{n(k)}$ from each of the finger vertices to $\mathbf{M}^\prime_{i,j,k}$, as well as the corresponding nearest points $\{\mathbf{p}_{i,j,k,l}\}_{l=1}^{n(k)}$ on $\mathbf{M}^\prime_{i,j,k}$, where $n(k)$ is the number of vertices on the $k$-th finger.

To avoid penetration, for each frame $i$, we also use a penetration loss $\mathcal{L}_{\mathrm{penetr}}$ by calculating hand mesh vertices $\mathbf{v}_i$ that are inside the object mesh. 

Besides, three additional losses $\mathcal{L}_{\mathrm{trans}}(\theta^\prime_{\cdots;0,1,2})=\sum_{i}\lVert\theta^\prime_{i;0,1,2}-\theta^\prime_{i+1;0,1,2}\rVert^2$, $\mathcal{L}_{v}(\theta^\prime)=\lVert \dot{\mathbf{p}}\rVert^2$ and $\mathcal{L}_{a}(\theta^\prime)=\lVert \ddot{\mathbf{p}}\rVert^2$ are utilized for smoothening the pose parameter, velocity and acceleration of hand, respectively.

We minimize the overall loss value defined as

\begin{equation} \label{eq:syn_all_loss}
\begin{split}
    \mathcal{L}(\theta^\prime) &= \lambda_{\mathrm{contact}}(\mathcal{L}_{\mathrm{contact}}(\theta^\prime)+\mathcal{L}_{\mathrm{penetr}}(\theta^\prime)) \\
    & + \lambda_{\mathrm{trans}}\mathcal{L}_{\mathrm{trans}}(\theta^\prime_{\cdots;0,1,2}) \\
    & + \lambda_{\mathrm{smooth}}(\lambda_{v}\mathcal{L}_{v}(\theta^\prime)+\lambda_{a}\mathcal{L}_{a}(\theta^\prime)).
\end{split}
\end{equation}

To produce more accurate contacts, we iteratively conduct such optimization process for several epochs, in each epoch we feed the current $\theta^\prime$ to the optimization course for a further improvement.
