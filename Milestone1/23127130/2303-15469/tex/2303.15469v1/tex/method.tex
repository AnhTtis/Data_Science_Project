

\label{sec:method}

Figure~\ref{fig:framework} shows the framework of our system. We first introduce \textbf{CAnonicalized Manipulation Spaces} (\textbf{CAMS}), a two-level space hierarchy that allows representing dynamic hand motions in an object-centric and contact-centric view (see Section~\ref{sec:cams}).
By embedding hand motions in CAMS, we obtain a more compact description for dynamic manipulations, which is more friendly to learning.
%Leveraging, as the intermediate representation in our synthesizing pipeline. Based on the CAMS representation, 

To learn to synthesize human-like manipulations, we propose a two-stage framework consisting of a CVAE-based \textbf{planner} module (see Section~\ref{sec:planner}) and an optimization-based \textbf{synthesizer} module (see Section~\ref{sec:synthesizer}). Taking the object shape $\mathbf{O}$, the manipulation goal $\mathbf{G}$ and the initial hand configuration $\mathbf{H}_0$ as input, we first divide the whole manipulation process into several motion stages by the sequential goals. Then, the planner module generates stage-wise \textbf{contact targets} and time-continuous \textbf{finger embeddings} as guidance for HOM synthesis. Finally, the synthesizer module takes the generated contact targets and hand embeddings as inputs and leverages an optimization strategy to produce a human-like and physically realistic HOM animation.
% Taking a CAD object model from a known category and a manipulation goal, our method could generate a sequence of task-specific, human-like, realistic hand-object interaction animation.
% Note that the animation we wanted here is task-specific, meaning that a default task is implicitly bounded with the known object category.

% Specifically, a manipulation goal is defined as $\mathbf{G}=\{\mathbf{H}_s, \mathbf{O}_s, \mathbf{O}_e\}$, where the parameters $\mathbf{H}_s$, $\mathbf{O}_s$, $\mathbf{O}_e$ denote initial hand configuration, initial object 6D pose, and end object 6D pose, respectively. \modify{update the setting} \modify{high-level functions of CAMS} The Planner will plan a hand motion in CAMS using CAMS-CVAE from the CAD object model and the manipulation goal. Then the Synthesizer would refine the generated motion with ???@lixing.

% Given above input and goal, there are several challenges we  face. First, our framework should handle problem of object shape diversity, so that we could generate animation in category level. Second, our framework needs the ability to handle the problem of manipulation diversity to show different manipulation mode for a specific task in synthesized animation. And finally, although we don't use strict physical constraints, our results should be at least physically realistic, and ensure that there is no obvious penetration or fake manipulation (manipulate object with no contact between hand and object). To overcome these problems, we propose canonicalized manipulation spaces in (Section~\ref{sec:cams}). We will argue that such canonicalized hand motion representation concentrates more on local object geometry, where hand-object interaction tends to occur, and encode more details about contact between hand and object, to increases the generalizability of our framework. 

\begin{figure}
    \centering
    \vspace{-5pt}
    \includegraphics[width=\linewidth]{images/demo.png}
    \vspace{-10pt}
    \caption{\textbf{CAMS Representation of Hand Motion.} We present Canonicalized Manipulation Spaces, a novel representation for hand-object interaction, to express the motion under an object-centric and contact-centric view. The top right figure demonstrates discrete contact targets defined at each stage transition point. The bottom right figure shows the time-continuous finger embedding in one frame. See Section~\ref{sec:cams} for details.}
    \label{fig:CAMS}
    \vspace{-20pt}
\end{figure}

\subsection{CAnonicalized Manipulation Spaces (CAMS)}
\label{sec:cams}

It is challenging to model the space of hand manipulation from the given object shape due to the shape variety within a category and the huge diversity of human manipulation styles. For two object instances that have a non-negligible geometry difference, even if we apply the same manipulation style on them (\eg same finger placement on two pairs of scissors with different sizes), the resulting hand pose can also be significantly different. Hence it is difficult for previous approaches that directly learn the MANO parameters to generalize to novel object instances, especially when object shapes are similar during training. 
% To address this difficulty, we observed that if each finger's motion is expressed in a canonical reference frame centered at its contact point between the object, the resulting representation will rely much less on the object instance's overall shape.
To address this challenge, we propose to express the motion of each finger in a canonical reference frame centered at a contact point on the object, thus associating such motion to the local contact region rather than the whole object shape.

Based on this motion expression, we introduce CAnonicalized Manipulation Spaces with two-level canonicalization for manipulation representation. At the root level, the \textit{canonicalized contact targets} (see Section~\ref{subsec:CR}) describe the discrete contact information. At the leaf level, the \textit{canonicalized finger embedding} (see Section~\ref{subsec:FE}) transforms finger motion from global space into local reference frames defined on the contact targets.


% (zjt) We first sample one frame where contact happens from the sequence to illustrate how to model hand-object interaction using CAMS, and then introduce how to expand CAMS from one frame to the whole sequence. Without loss of generality, we use one rigid object as an example, and for articulated objects, we can treat them as several detached rigid objects and process them identically in this section for simplicity. The CAMS is a space built based on two-level hierarchical canonicalization, defined as a 3D space within a unit cube $i.e., \{x, y, z\} \in [0,1]$ similar to NOCS \cite{nocs}. We normalize the given object CAD model and the contact using the object-oriented canonicalization, and center the object within the cube. In CAMS, we represent the contact between hand and object as contact references for five fingers respectively and finger embedding built upon contact references by contact-oriented canonicalization.

% \begin{figure*}[!h]
%     \centering
%     \vspace{-5pt}
%     \includegraphics[width=\linewidth]{images/CAMS.png}
%     \caption{\textbf{Overview of CAnonicalized Manipulation Spaces (CAMS)}. (a) We first predict object-centric canonicalized contact reference frames $\mathbf{F_c} = \mathbf{R} \cdot \mathbf{U}$ as local contact priors for grasping. (b) We then separately tranfer the contact priors to each finger as the contact-centric canonicalized finger embedding, where the embedding for a finger is exemplified in (c).}
%     \label{fig:CAMS}
%     \vspace{-5pt}
% \end{figure*}

\begin{figure*}[!h]
\begin{center}
   \includegraphics[width=2\columnwidth]{images/arch.png}
\end{center}
   \vspace{-0.5cm}
   \caption{\textbf{Network Architecture of CAMS-CVAE.} On the left side, we show the design of the condition encoder, in which the system input is encoded into a condition embedding for CVAE. The components in the dotted box are repeated for each rigid part of the object. On the right side, we show the architecture of the whole CVAE. The components in the dotted boxes are repeated for each tuple $(i,j,k)$.}
   \vspace{-0.3cm}
\label{fig:network}
\end{figure*}

\subsubsection{Canonicalized Contact Targets}
\label{subsec:CR}

At the root level of CAMS, the canonicalized contact targets describe the contact information between each finger and the object. The contact targets are first used to define the contact-centric reference frames for finger embeddings (Section~\ref{subsec:FE}), and then allow the contact optimization of the synthesizer to achieve grasps with accurate contacts (Section~\ref{sec:synthesizer}).

Formally, we define the contact targets as a sequence

\vspace{-10pt}
$$\mathbf{C}=\{(\mathbf{c}_{i,j,k},\mathbf{\Tilde{V}}_{i,j,k}, \mathbf{\Tilde N}_{i,j,k})\}_{i,j,k},$$

in which $1\le i\le 5$ indicates the index of finger, $0\le j\le M$ indicates the index of stage transition point and $1\le k\le N$ indicates the index of object's part. $\mathbf c_{i,j,k}\in[0,1]$ are binary flags of whether the finger $i$ is in contact with the object part $k$ at stage transition point $j$, and $\mathbf{\Tilde V}_{i,j,k}\in\mathbb R^3,\mathbf{\Tilde N}_{i,j,k}\in\mathbb R^3$ are canonicalized positions and normal directions of the corresponding contact point. We follow NPCS~\cite{nocs, li2020category} to normalize all the rigid parts into a normalized coordinate space within the unit cube (i.e., $x, y, z \in [0,1]$), aligned with a category-level canonical orientation. Contact positions $\mathbf{\Tilde V}_{i,j,k}$ as well as the surface normals $\mathbf{\Tilde N}_{i,j,k}$ are defined in the normalized space, in order to make the representation more compactly distributed and easier to learn.

Our generative model learns the distribution of discrete contact targets conditioned on the object shape and task configuration. The benifit of learning surface normal $\mathbf{\Tilde{N}}_{i,j,k}$ together with position $\mathbf{\Tilde{V}}_{i,j,k}$ is two fold. First, for an unseen object with complex shapes (\eg scissors), due to the limitation of network capacity and training data diversity, we cannot guarantee that the predicted contact positions are always exactly on the object surface. When projecting the contact point onto the surface, the predicted normal directions help filter out the incorrect surface parts near the predicted point. Second, the difference in normal directions among different finger placement styles is more significant. It makes it easier for the VAE-based model to distinguish the discrete manipulation styles if normal directions are regarded as reconstruction targets. %\lxmodify{Examples needed here?}

% As we introduced, CAMS are defined as a two-level space hierarchy to represent manipulations. What connecting the two levels of spaces is a set of canonicalized contact reference frames $\mathbf{R}=\{\mathbf{R}_i=(\mathbf{C}_i, \mathbf{I}_i, \mathbf{V}_i, \mathbf{N}_i)\}_{i=1}^5$, where $\mathbf{C}_i \in [0,1]$ is a binary flag showing whether the $i$-th finger is in contact with the object part, $\mathbf{I}_i$ is the canonical reference frame of the part closest to the $i$-th finger tip, $\mathbf{V}_i \in \mathbb{R}^3$ is a point on the object part surface indicating the center of contact between vertices of the $i$-th finger and the object part, and $\mathbf{N}_i \in \mathbb{R}^3$ is the corresponding normal vector of $\mathbf{V}_i$ on the part surface. Computing $\mathbf{R}_i$ requires first canonicalizing each object part and obtaining its own frame, and then moving the corresponding part frame to the potential contact point. Such reference frames define the leaf level spaces of CAMS, and require the root-level canonicalization for its estimation. To estimate $\mathbf{I}_i$, we need to canonicalize each object part on the category-level.

At the training stage, the ground truth contact targets are obtained by applying a contact analysis to training data. At the inference stage, given $\mathbf {\Tilde V}_{i,j,k}$ and $\mathbf{\Tilde N}_{i,j,k}$ with $\mathbf c_{i,j,k}=1$, the actual contact point $\mathbf P_{i,j,k}$ on the object surface is determined by a matching process on the object surface. For more details, please refer to our supplementary material.

% The canonicalized contact reference frames  $\mathbf{F_c}$ connects the two levels of spaces in CAMS. 
% defines a set of spaces
% is a space built based on two-level hierarchical canonicalizations $\mathbf{F_c} = \mathbf{R} \cdot \mathbf{U}$, as shown in Figure~\ref{fig:CAMS} (a).% where $\mathbf{U}$ maps the object points to a category-level normalized object coordinate space\cite{nocs}, and $\mathbf{R}$ .

% \textbf{Normalization $\mathbf{U}$.} Inspired by NOCS\cite{nocs}\eric{should this include ANSCH as well?}, for every object category, we normalize all the rigid parts into canonicalized 3D spaces within the unit cube ($i.e., \{x, y, z\} \in [0,1]$). The normalized object part has the same center as the unit cube, as well as a category-specific constant orientation.

% \textbf{Object-centric canonicalization $\mathbf{R}$.} After normalizing an object part to the category-level canonical space, we represent the contact between the hand and the normalized object part as contact references for five fingers, respectively. For the $i$-th finger, we formulate contact references as $\mathbf{R}_i=\{(\mathbf{C}_i, \mathbf{V}_i, \mathbf{N}_i)\}_{i=1}^5$, where $\mathbf{C}_i \in [0,1]$ is a binary flag showing whether the $i$-th finger is in contact with the object part,  $\mathbf{V}_i \in \mathbb{R}^3$ is a point on the object part surface indicating the center of contact between vertices of the $i$-th finger and the object part, and $\mathbf{N}_i \in \mathbb{R}^3$ is the corresponding normal vector of $\mathbf{V}_i$ on the part surface.\eric{what about the tangent directions? this does not form a frame no?}

% The insights of our design are twofold. First, the contact references encode local object geometry explicitly by using $\mathbf{V}_i$ and $\mathbf{N}_i$, which could exploit the similarity of local patterns and help CAMS generate to novel object instances. Second, as shown in Figure \modify{need a figure}, such designed contact reference field could be strong related to the HOM motion type.
% For example, \emph{???same manipulation type, contact reference remains the same, while hand pose changes drastically, scale?}. On the other hand, contact reference is sensitive to different manipulation types for a specific task, for example, \emph{???different manipulation types, contact reference changes drastically, while hand pose remains similar, laptop?}```

% (zjt) We formulate contact reference as $\mathbf{R}=\{(\mathbf{C}_i, \mathbf{V}_i, \mathbf{N}_i)\}_{i=1}^5$, where $\mathbf{C}_i \in [0,1]$ is a binary flag showing whether the $i$-th finger is in contact with the object,  $\mathbf{V}_i \in \mathbb{R}^3$ is a point on the object surface indicating the center of contact between vertices of the $i$-th finger and the object, and $\mathbf{N}_i \in \mathbb{R}^3$ is the corresponding normal vector of $\mathbf{V}_i$ on the object surface. The contact reference encodes local object geometry explicitly by using $\mathbf{V}_i$ to show relative  contact position of the object in CAMS and $\mathbf{N}_i$ to indicate local surface information. It is essential to sense local object geometry to handle shape diversity, considering that however different shapes in a category might be, the local object geometry tends to be similar within the category, with similar 'function of contact'. Because of the ability to sense local object geometry, our CAMS representation could be generalized to various shapes including unseen object in category level. What's more, different from representations like hand pose, contact reference is an unified representation for a specific manipulation type, irrespective of scale of object. For example, \emph{???same manipulation type, contact reference remains the same, while hand pose changes drastically, scale?}. On the other hand, contact reference is sensitive to different manipulation types for a specific task, for example, \emph{???different manipulation types, contact reference changes drastically, while hand pose remains similar, laptop?}

\subsubsection{Canonicalized Finger Embeddings}
\label{subsec:FE}

Given contact points $\mathbf P_{i,j,k}$ from contact targets, we can now build contact-centric spaces for finger embedding. We denote $\mathbf R_{i,j,k}$ as the contact reference frame centered at $\mathbf P_{i,j,k}$, with orientation aligned to the corresponding rigid part of the object (as shown in \ref{fig:CAMS}). For a static hand pose represented by MANO parameters $\theta$ and $\beta$, we first calculate the corresponding MANO joint coordinates $\mathbf J^{tip},\mathbf J^{dip},\mathbf J^{pip},\mathbf J^{mcp}$ of finger $i$ and $\mathbf J^{root}$ for the hand wrist under reference frame $\mathbf{R}_{i,j,k}$. Based on joint positions, we further compute the normalized directions
\vspace{-5pt}
$$
\mathbf{D}^{joint}=\frac{\mathbf{J}^{joint}-\mathbf J^{tip}}{\|\mathbf{J}^{joint}-\mathbf J^{tip}\|},joint\in\{dip,pip,mcp,root\}
$$
from the fingertip to every other joint. The resulting finger embedding for the static hand pose is $\mathbf F=(\mathbf J^{tip},\mathbf D^{dip},\mathbf D^{pip},\mathbf D^{mcp},\mathbf D^{root})\in\mathbb R^{15}$. Similar to contact targets, we aim to enlarge the discrepancy between different modes by using directions as learning targets. Also, directions between joints are invariant under global translation relative to the contact center, and therefore data noise caused by the contact center's fluctuation can be reduced. The tip joint is selected as the reference joint since it's typically close to the contact position in most of the manipulation styles.

To handle dynamic hand motion synthesis, we further extend the static finger embedding to \textit{continuous-time sequences}. More specifically, for a finger interacting with the object at a stage in time period $[t_0,t_1]$, the continuous-time finger embedding sequence is a continuous function $\mathbf f:[0,1]\to\mathbb R^{15}$ that maps the normalized time $\Tilde t=(t-t_0)/(t_1-t_0)$ to a finger embedding $\mathbf F\in\mathbb F^{15}$. Inspired by recent works\cite{nerf, nerfies, d-nerf} of implicit neural representation, the continuous mapping is learned by a neural network which maps the \textit{temporal encoding} $\mathcal T\in\mathbb R^{12}$ defined as
$$\mathcal T(\Tilde t)=\{(\sin2^k\pi \Tilde t,\cos2^k\pi \Tilde t)\}_{k=0}^5$$
together with a latent code to $\mathbf{F}\in\mathbb R^{15}$ (see Section~\ref{sec:planner}).

% Treating $\mathbf{R}_i$ as the reference frame for the $i$-th finger, we define the canonicalized finger embedding as $\mathbf{F}_i=(\mathbf{J}^{tip}_i, \vec{\mathbf{J}}^1_i, \vec{\mathbf{J}}^2_i, \vec{\mathbf{J}}^3_i, \vec{\mathbf{J}}^{root}_i)$, where $\mathbf{J}^{tip}_i$ denotes the position of the $i$-th finger tip in the $\mathbf{R}_i$ frame, and $\vec{\mathbf{J}}^1_i$, $\vec{\mathbf{J}}^2_i$, $\vec{\mathbf{J}}^3_i$, $\vec{\mathbf{J}}^{root}_i$ are four direction vectors from $\mathbf{J}^{tip}_i$ to the second, the third, the forth MANO joints of the $i$-th finger and the root joint of the hand, respectively. In such form of embedding, the finger position regarding the reference point is encoded in $J^{tip}_i$, while the finger's articulated pose is encoded in the rest terms.

% (zjt) Taking $\mathbf{V}_i$ in the corresponding contact reference as original point, we can extract finger embedding $\mathbf{F}=\{(\mathbf{J}^{tip}_i, \vec{\mathbf{J}}^1_i, \vec{\mathbf{J}}^2_i, \vec{\mathbf{J}}^3_i, \vec{\mathbf{J}}^{root}_i)\}_{i=1}^5$, where $\mathbf{J}^{tip}_i$ is the position of the finger tip of the $i$-th finger in contact reference coordinate, and $\vec{\mathbf{J}}^1_i$, $\vec{\mathbf{J}}^2_i$, $\vec{\mathbf{J}}^3_i$, $\vec{\mathbf{J}}^{root}_i$ are four normalized vector pointing to the second, the third, the forth joints of the $i$-th finger and the root joint of the hand respectively, starting from $\mathbf{J}^{tip}_i$. We use relative normalized vectors rather than absolute positions to avoid conflict and stress the importance of finger tip, considering that there are frequent contacts between finger tip and object, and there is a specific relative normalized vectors pattern for the following finger joints after restricting the finger tip. The finger embedding indicates the state of a finger when contact happens, and given that it is canonicalized by contact reference, it helps the model to concentrate more on relations between finger and local object geometry in a local space.

% \vspace{-13pt}
% \subsubsection{CAMS Embedding Sequence}
% \label{subsec:CAMS_in_Sequence}
% To represent CAMS embedding for a sequence of hand poses, one natural way is to predict the canonicalized contact reference frames for each hand pose. In reality, we find this not quite necessary since in a lot of situations, the contact remains quasi-static within a certain time range. In this case, only predicting $\mathbf{R}_i$ at the starting and ending time is sufficient for CAMS embedding, which also smooths the embedding to facilitate learning. Specifically, we assume the contact is quasi-static between each pair of functional goals $(\mathbf{S}^j, \mathbf{S}^{j+1})$. We use $\mathbf{R}^{j}$ and $\mathbf{R}^{j+1}$ to represent the canonicalized contact reference frames at the beginning and the end of the interval. Then the CAMS embedding for the hand becomes $\mathbf{F}=\{\mathbf{F}^{j},\mathbf{F}^{j+1}\}$ within the interval. Notice we augment $\mathbf{F}$ with two embeddings to provide a synthesizer more flexibility in converting such a representation into a real hand animation.


% Given a hand manipulating an object 

% Our CAMS embedding sequence is defined as $\{\mathbf{R}^{(i)}, \mathbf{F}^{(i)}\}_{i=1}^N$ for $N$ object parts \eric{R is not embedding not?}, where $\mathbf{R}^{(i)}$ and $\mathbf{F}^{(i)}$ are introduced subsequently. For the $i$-th object part, we regard the manipulation phase of this part as a sequence of $S^{(i)}\geq1$ static contact stages in which the contact points remains approximately stationary related to the object part. We define the $S^{(i)}+1$ frames that separate the static contact stages as reference frames $\{\mathbf{I}_s^{(i)}\}_{s=1}^{S^{(i)}+1}$. We generate canonicalized contact reference frames $\mathbf{R}^{(i)}=\{\mathbf{R}_s^{(i)}\}_{s=1}^{S^{(i)}+1}$ for all reference frames, where $\mathbf{R}_s^{(i)}=\{\mathbf{C}_s^{(i),f}, \mathbf{V}_s^{(i),f}, \mathbf{N}_s^{(i),f}\}_{1\leq f \leq 5}$ for five fingers respectively. For each static stage $s$, given both $\mathbf{R}_s^{(i)}$ and $\mathbf{R}_{s+1}^{(i)}$, we subsequently construct a temporally contiguous bidirectional finger embedding denoted as $\mathbf{F}_s^{(i)}=\{\mathbf{F}_s^{(i),start}, \mathbf{F}_s^{(i),end}\}$, where $\mathbf{F}_s^{(i),start}$ and $\mathbf{F}_s^{(i),end}$ are respectively represented in the object part coordinate system of reference frame $\mathbf{I}_s^{(i)}$ and $\mathbf{I}_{s+1}^{(i)}$. Each $\mathbf{F}_s^{(i),*}$ consists of $\{\mathbf{J}_t^{(i),tip}, \mathbf{J}_t^{(i),1}, \mathbf{J}_t^{(i),2}, \mathbf{J}_t^{(i),3}, \mathbf{J}_t^{(i),root}\}_{1 \leq t \leq T^{\prime}}$, where $T^{\prime}$ is the number of frames in stage $s$. The $\mathbf{F}^{(i)}$ denotes $\{\mathbf{F}_s^{(i)}\}_{s=1}^{S^{(i)}}$.
% To summarize, we expand CAMS in the manipulation period with separative contact reference fields and continuous finger embedding, which jointly formulates the CAMS embedding sequence.
% Benefiting from CAMS, we can sense local object geometry of contact and represent contact explicitly, rather than representing contact implicitly in a mass of point clouds of hand and object. \modify{why it can solve shape diversity?}

% (zjt) Now we can represent the contact of a frame in CAMS, however, it is not that easy to find a unified representation for the whole sequence, which means we should even build CAMS on frames where no contact happens. Based on our observation, a manipulation could be empirically split into several stages: approaching stage, manipulation stage, and leaving stage. The approaching stage and the leaving stage are easy to understand, denoting hand moving from one position to object, and leaving object, respectively, while the manipulation stage might be confusing. We define a manipulation stage as a short period of time when the hand reaches a robust, static contact with object, here a static contact means contact pairs of hand and object don't change obviously, while the hand motion might keep changing. Some complex manipulation might contain several static contacts and thus there are several manipulation stages (like example of Scissor in (Section~\ref{})), but we just explain condition of one manipulation stage here for simplicity. After splitting the manipulation into different stages, we can generate contact references. Considering that the static contact along a stage won't change, we just need contact references for the start frame and the end frame of each stage to represent static contact, and for those frames without a static contact, like the start frame in approaching stage, we just ignore them. With the generated start and end contact references in a stage, we can build a sequence of bidirectional finger embedding for each frame in the stage. For convenience, we unify symbols of CAMS in manipulation here: Given a manipulation over the course of $T$ frames, we can split the sequence into $S$ stages based on $S+1$ static contacts in $S+1$ reference frames respectively(including the start and the end frame of the sequence). From static contacts, we can generate discrete contact reference in each reference frame, denoted as $\mathbf{R}=\{\mathbf{R}_s\}_{1 \leq s \leq S}$ for $S$ reference frames, where $\mathbf{R}_s=\{\mathbf{C}_s^f, \mathbf{V}_s^f, \mathbf{N}_s^f\}_{1\leq f \leq 5}$ for five fingers respectively. The bidirectional finger embedding is built based on contact reference, specifically, in a random stage $s \in [1, S]$, the bidirectional finger embedding is represented as $\mathbf{F}=\{\mathbf{F}_s^{start}, \mathbf{F}_s^{end}\}_{1\leq s \leq S}$, where $\mathbf{F}_s^{start}$ is centriced in the coordinate of the contact reference of the start frame of the stage $s$, and $\mathbf{F}_s^{end}$ is in the coordinate of end frame contact reference. And $\mathbf{F}_s$ consists of $\{\mathbf{J}_t^{tip}, \mathbf{J}_t^1, \mathbf{J}_t^2, \mathbf{J}_t^3, \mathbf{J}_t^{root}\}_{1 \leq t \leq T^{\prime}}$, where $T^{\prime}$ is the length of the sequence in stage $s$. To summarize, we expand CAMS in manipulation sequence with discrete contact references and continuous finger embedding. With CAMS, we can sense local object geometry of contact and represent contact explicitly, rather than representing contact implicitly in a mass of point clouds of hand and object. \emph{why it can solve shape diversity?} 


\subsection{CAMS-CVAE: the Motion Planner}
\label{sec:planner}

Given an object instance with task configuration, we present CAMS-CVAE, a CVAE-based generative motion planner for generating CAMS sequences, including contact targets and continuous-time finger embeddings. An overview of the model architecture is shown in Figure~\ref{fig:network}.

\textbf{Condition Encoding} As mentioned earlier, our model takes several inputs as generating conditions, including the object part meshes $\{\mathbf M_k\}_{k=1}^N$, the goal sequence $\mathbf G$ and the initial hand configuration $\mathbf H_0$. We encode this condition information into a vector using a multi-head condition encoder module, with each head corresponding to an object's rigid part. For each part, we first sample $2000$ points on the mesh surface with normal directions, and a PointNet structure is used to embed the point cloud information $\mathbf O\in\mathbb R^{2000\times6}$ into a shape feature $\bm z_{O}\in\mathbb R^{32}$. The shape feature is then concatenated with the part's 6D-pose sequence $\mathbf S\in\mathbb R^{6\times(M+1)}$, and all part's information is concatenated together with the initial hand pose $\mathbf H_0\in\mathbb R^{51}$. Finally, an MLP structure encodes the concatenated vector into the condition embedding $\bm z_C\in\mathbb R^{32}$.

\textbf{Motion Encoding} In the encoder part of the CVAE structure, we encode the hand motion represented by CAMS sequences into a diagonal-covariance Gaussian distribution in a $64$-dimensional latent space. We first concatenate the whole sequence of contact targets into a vector $\mathbf C\in\mathbb R^{7\times5\times N\times(M+1)}$ (as introduced in Section~\ref{subsec:CR}). In each stage, we evenly spaced sample $10$ timestamps $\{0,1/9,2/9,\cdots,1\}$ in the normalized time range and calculated the corresponding finger embeddings (as introduced in Section~\ref{subsec:FE}) under contact targets at the stage's two end-points. Besides finger embeddings, we also extract two binary flags $\textbf f_c,\textbf f_n$ (see Section~\ref{sec:synthesizer}) used in the synthesizer module at each sampled timestamp for each tuple $(i,j,k)$. For non-existing contact targets with $\mathbf c_{i,j,k}=0$, the corresponding values are filled with zero. All these values are concatenated are encoded by an MLP to predicted Gaussian parameters $\bm\mu\in\mathbb R^{64},\bm\sigma\in\mathbb R^{64}$.

\textbf{Motion Decoding} After sampling latent code $\bm z\in\mathbb R^{64}$ from either the predicted distribution $\mathcal N(\bm\mu,\bm\sigma^2)$ or standard Gaussian $\mathcal N(\mathbf 0,\mathbf I)$, we concatenate it with the generating condition $\bm z_C$, and a two-branch decoder is used to convert them into CAMS representation of desired hand motion. The \textit{discrete branch} is a multi-head MLP that outputs discrete contact targets $(\mathbf{c}_{i,j,k},\mathbf{\Tilde{V}}_{i,j,k}, \mathbf{\Tilde N}_{i,j,k})\in\mathbb R^7$ introduced in Section~\ref{subsec:CR}, with each head corresponding to a tuple $(i,j,k)$ indicating the index of the finger, stage transition point, and object's part. The \textit{continuous-time branch} is another multi-head MLP that takes the temporal encoding $\mathcal{T}(\Tilde{t})\in\mathbb R^{12}$ as extra input and outputs the corresponding finger embeddings $\mathbf F_1,\mathbf F_2\in\mathbb R^{15}$ relative to contact targets at both end-points of the stage. The continuous-time branch also predicts the two binary flags $\textbf f_c,\textbf f_n$ for the synthesizer.

% After completing the portrayal of the latent space distribution, we sample a latent code $z$ from latent space and pass it to our decoder together with a time embedding $\mathcal{T}$ and the condition $\mathcal{F}_{cond}$.
% The time embedding $\mathcal{T}$ comes from normalized time sequence $\mathbf{T} \in [0, 1]$ denoting the position of a frame in manipulation relatively.
% The decoder generates predicted motion represented by CAMS, denoted as contact reference $\hat{\mathbf{R}}=\{\hat{\mathbf{R}}^j\}_{1\leq j \leq J}$ for $J$ stages, as well as the bidirectional finger embedding sequence $\hat{\mathbf{F}}=\{\hat{\mathbf{F}}^j_{start}, \hat{\mathbf{F}}^j_{end}\}_{1\leq j \leq 
% J}$.
%, where $\hat{\mathbf{F}}_s=\{\hat{\mathbf{J}}_t^{tip}, \hat{\mathbf{J}}_t^1, \hat{\mathbf{J}}_t^2, \hat{\mathbf{J}}_t^3, \hat{\mathbf{J}}_t^{root}\}_{1\leq t \leq T^{\prime}}$.



\textbf{Training Loss}
We use several losses to train CAMS-CVAE. The first loss is a Binary Cross Entropy (BCE) loss $\mathcal{L}_{flag}$ on the contact target flags $\mathbf c_{i,j,k}$ and per-frame flags $\mathbf f_c,\mathbf f_n$ for synthesizer, between predicted values and the ground truth.
We then calculate the $L_2$ distance losses $\mathcal L_{pos}$ of $\mathbf{\Tilde{V}}_{i,j,k}$ and $\mathcal L_{dir}$ for $\mathbf{\Tilde N}_{i,j,k}$, between predicted values and the ground truth. These two losses are computed only for $(i,j,k)$ with the ground truth contact flag $\mathbf c_{i,j,k}=1$.
Similarly, we also compute the $L_2$ losses $\mathcal{L}_{tip}$ for predicted $\mathbf J^{tip}$ and $\mathcal{L}_{vec}$ for predicted $\mathbf D^{dip},\mathbf D^{pip},\mathbf D^{mcp},\mathbf D^{root}$.
Finally, we use the KL-Divergence $\mathcal{L}_{KLD}$ to constrain the latent distribution $\mathcal N(\bm\mu,\bm\sigma^2)$ to be close to the standard Gaussian distribution following \cite{vae}.
The total loss is a weighted summation of all six loss terms:
\begin{equation}
\begin{split}
    \mathcal{L} & = \lambda_{flag} \cdot \mathcal{L}_{flag} + \lambda_{pos} \cdot \mathcal{L}_{pos} + \lambda_{dir} \cdot \mathcal{L}_{dir} \\ & + \lambda_{tip} \cdot \mathcal{L}_{tip} + \lambda_{vec} \cdot \mathcal{L}_{vec} + \lambda_{kld} \cdot \mathcal{L}_{KLD}.
\end{split}
\end{equation}

For a detailed calculation of the loss terms, please refer to our released code.

\subsection{Optimization-Based Motion Synthesizer}
\label{sec:synthesizer}

Once a CAMS embedding sequence has been generated from the planner, our optimization-based synthesizer subsequently produces a complete HOM sequence. We simply use BÃ©zier curve-based interpolation to generate the object trajectory and thus focus on synthesizing a hand trajectory that satisfies our goal (Section~\ref{sec:overview}).
% The CAMS embedding sequence contains only finger joint embedding, which cannot be directly used for contact optimization.
Given the object shape and trajectory, as well as a CAMS embedding sequence, our synthesizer adopts a two-stage optimization method that first optimizes the MANO pose parameters to best fit the CAMS finger embedding (see Section~\ref{subsec:fitting_finger_embedding}) and then optimizes the contact effect to improve physical plausibility (see Section~\ref{subsec:optimizing_contact_and_penetration}).
In each frame, the synthesizer reads a binary flag $\mathbf f_n$ indicating whether the finger is near enough (10cm) that the finger embedding will be used to guide the generated motion, and a binary flag $\mathbf f_c$ indicating whether there is a contact between the finger and object.

% \subsubsection{Planning object trajectory}
% \label{subsec:planning_object_trajectory}
% To plan the trajectory of an object, we apply linear interpolation to get the 6D pose $\bar{\mathbf{S}}^{j}_{i,k}=\mathbf{S}^{j}_i+(\mathbf{S}^{j}_{i+1}-\mathbf{S}^{j}_i)\cdot k/t_i$ of object $\mathbf{M}_j$ for each frame $k$ in stage $i$.

\subsubsection{Fitting Finger Embedding}
\label{subsec:fitting_finger_embedding}

Given the predicted contact targets $\mathbf{C}$, the bidirectional finger embedding $\mathbf{F}_1,\mathbf{F}_2$ and the binary flags $\mathbf{f}_c,\mathbf{f}_n$, we optimize the MANO parameters of hand pose and transition $\theta=\{\theta_t\}_{t=1}^{T}$ by minimizing:
\vspace{-20pt}

\begin{equation} \label{eq:fit_loss}
\begin{split}
    \mathcal{L}(\theta) = & \lambda_{\mathrm{tip}}\sum_{t=1}^{T}\mathcal{L}_{\mathrm{tip}}(\theta_t)+\lambda_{\mathrm{joint}}\sum_{t=1}^{T}\mathcal{L}_{\mathrm{joint}}(\theta_t) \\
    &+ \lambda_{\mathrm{smooth}}\mathcal{L}_{\mathrm{smooth}}(\theta).
\end{split}
\end{equation}

The first term of Eq.\eqref{eq:fit_loss} is the tip transition loss, which constrains the tip position $\mathbf{J}^{tip}$ of the finger in the canonicalized finger embedding. The second term of Eq.\eqref{eq:fit_loss} is the joint orientation loss, which is used to optimize the direction vectors of four subsequent joints $\mathbf{D}^{dip}$, $\mathbf{D}^{pip}$, $\mathbf{D}^{mcp}$, $\mathbf{D}^{root}$ of the finger in the canonicalized finger embedding. And the last term of \eqref{eq:fit_loss} is a smoothness loss for improving temporal continuity.

\subsubsection{Optimizing Contact and Penetration}
\label{subsec:optimizing_contact_and_penetration}

After fitting MANO parameters $\theta$ by finger embedding, we leverage another optimization-based method to handle the penetration and inaccurate contact issues of the hand pose. The optimization course refines $\theta$ to physically realistic MANO parameters $\theta^\prime$ as the final result of synthesis.
% We first construct the hand mesh sequence by MANO model from the current MANO parameters $\theta$.

To achieve better contact quality, we define a contact loss $\mathcal{L}_{\mathrm{contact}}$ to attract the nearby finger vertices to the local surface section. And to avoid penetration, we also use a penetration loss $\mathcal{L}_{\mathrm{penetr}}$ by repulsing hand mesh vertices that are inside the object mesh. 

Besides, three additional losses $\mathcal{L}_{\mathrm{trans}}(\theta^\prime_{\cdots;0,1,2})=\sum_{t}\lVert\theta^\prime_{t;0,1,2}-\theta^\prime_{t+1;0,1,2}\rVert^2$, $\mathcal{L}_{v}(\theta^\prime)=\lVert \dot{\mathbf{p}}\rVert^2$ and $\mathcal{L}_{a}(\theta^\prime)=\lVert \ddot{\mathbf{p}}\rVert^2$ are utilized for smoothening the wrist transition, velocity and acceleration of hand joints, respectively.

We minimize the overall loss value defined as

\vspace{-10pt}

\begin{equation} \label{eq:syn_all_loss}
\begin{split}
    \mathcal{L}(\theta^\prime) &= \lambda_{\mathrm{contact}}(\mathcal{L}_{\mathrm{contact}}(\theta^\prime)+\mathcal{L}_{\mathrm{penetr}}(\theta^\prime)) \\
    & + \lambda_{\mathrm{trans}}\mathcal{L}_{\mathrm{trans}}(\theta^\prime_{\cdots;0,1,2}) \\
    & + \lambda_{\mathrm{smooth}}(\lambda_{v}\mathcal{L}_{v}(\theta^\prime)+\lambda_{a}\mathcal{L}_{a}(\theta^\prime)).
\end{split}
\end{equation}

To produce more accurate contacts, we iteratively conduct such an optimization process for several epochs. In each epoch we feed the current $\theta^\prime$ to the optimization course for further improvement.
