\input{tex/tables/dataset.tex}

\begin{figure}
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.9\linewidth]{images/modediversity.png}
    \caption{\textbf{Visualization of different grasp modes on the same laptop.} We show 3 different grasp modes for opening a laptop. Both the starting and ending hand poses of the grasps are shown. }
    \label{fig:result-modediversity}
\end{figure}

\begin{figure}
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.9\linewidth]{images/comparison.png}
    \caption{\textbf{Qualitative results compared with GraspTTA \cite{grasptta} and ManipNet \cite{zhang2021manipnet}.} The results generated by our method are more realistic.}
    \label{fig:result-comparison}
    \vspace{-10pt}
\end{figure}


\label{sec:exp}
In this section, we
% explain details of our framework and
apply our method to synthesize HOM, and evaluate the effect of our method with various metrics. We first introduce the experimental settings including dataset (Section~\ref{sec:data}), baselines (Section~\ref{sec:baselines}), and evaluation metrics (Section~\ref{sec:metrics}), while the experimental details are presented in our supplementary material.
% , and experimental details including baselines from other methods in Section~\ref{sec:detail}
We then show that our method could handle shape diversity and manipulation diversity and achieve a surpassing synthesis performance compared with other methods in Section~\ref{sec:compare}.
% Finally, we propose ablation studies in Section~\ref{sec:ablation}.

\subsection{Dataset}
\label{sec:data}

We utilize HOI4D Dataset \cite{hoi4d} in our experiment. HOI4D is a real-world dataset that contains dynamic HOM data spanning various rigid and articulated object categories. We select five object categories with different functionalities in our experiment, as shown in (\tabref{tab:dataset}). % Especially, we specify several different manipulation types in \emph{Laptop} category with \emph{Open the Laptop} task, and use \emph{Scissors} to show that we can generate complex manipulation. 
The selected HOM data contains both various manipulation types (\eg opening a laptop with different human preferences) and complex manipulation processes (\eg opening a small scissor by putting fingers through the hole) to improve the diversity and complexity of our synthesis results. To improve the usability of HOI4D under our setting, we applied some data cleaning and augmentation methods to the raw data (see our supplementary material for more details).
% Note that all of our articulated objects consist of two parts, whereas it doesn't mean that our method could not be generalized to a more complicated articulated object category.

% \textbf{Data Processing}
% \modify{move to the supplementary material?}

% \subsection{Experimental Details}
% \label{sec:detail}

% \modify{can we move this whole subsection to the supplementary-material? now there's little space to write (8-page limit)}

% Given triangular meshes $\{\mathbf{M}_i\}_{i=1}^N$ of the manipulated object, we sample 1000 points for each mesh, and each part of the object is normalised to our CAMS. We use a batch size of 64 for training, and the training contains 1000 epochs. We use $\lambda_{flag}=0.1$, $\lambda_{pos}=500$, $\lambda_{dir}=100$, $\lambda_{tip}=100$, $\lambda_{vec}=1$, and $\lambda_{kld}=5$ during training, we will further explain how to balance loss in (Section~\ref{sec:ablation}).

% \lxmodify{As for the synthesizer, we empirically set $\lambda_{\mathrm{tip}}=50$, $\lambda_{\mathrm{joint}}=1$, and $\lambda_{\mathrm{smooth}}=0.05$ and $1000$ respectively for the MANO pose parameters and hand transition part in $\theta$ for fitting finger embedding. Our training contains 2000 epochs in this step. When optimizing contact and penetration, we empirically set $\lambda_{\mathrm{contact}}=80$, $\lambda_{\mathrm{trans}}=1$, $\lambda_v=5$ and $\lambda_a=20$. This step runs for $6$ iterations and $\lambda_{\mathrm{smooth}}$ in this step is set to $1$ in the first $2$ iterations, $10$ in the following $2$ iterations and $500$ in the last $2$ iterations. Each iteration runs for $500$ epochs.}

\input{tex/tables/evaluation.tex}

\input{tex/tables/userstudy.tex}

\subsection{Evaluation Metrics}
\label{sec:metrics}

We report several evaluation metrics on our experiments. We use these metrics to quantize the human-likeness and physical plausibility of motion synthesis results.

% \textbf{Contact Space Human-Likeness} We evaluate the smallest distance between the synthesized manipulation and all collected ground truth data on the same object instance. We calculate the distance under a contact space, i.e., we extract the contact positions with normal directions, and then calculate a $L_2$ loss on them.
% 
% \textbf{Pose Space Human-Likeness} This metric is similar to the contact space human-likeness, but the $L_2$ distance is calculated directly on the MANO joint positions.

\textbf{Contact-Movement Consistency} We evaluate whether the object's movement can align with the contact forces produced by hand-object contacts, using the same physics model as in ManipNet\cite{zhang2021manipnet}. For articulated objects, we assume there are free opposite forces at the spin axis of the object. We calculate the proportion of frames that the contacts align with the object motion.

\textbf{Articulation Consistency} For articulated objects, we evaluate whether the hand pose can manipulate the object in a human-like manner. In particular, for each part of the object in each frame, we compute the torque of all contact points w.r.t. the object's spin axis if a unit force is applied along the normal direction of the contact point. If the maximal attitude of the torque with the same direction of object rotation exceeds a threshold, we regard such frame as qualified. We calculate the proportion of qualified frames.


\textbf{Penetration Rate} We compute the mean penetration proportion of hand vertices for each sequence. We regard penetrations within a small threshold $\lambda$=5mm as not penetrated since small penetrations can be seen as contacts made by a soft hand in real.

\textbf{Perceptual Score} We collect human perceptual scores to judge the naturalness of the motion sequences. We ask people not familiar with motion synthesis to give discrete perceptual scores for the results and calculate a mean score for each category using each baseline method. The detailed approach is left to the supplementary material.


\subsection{Baselines}
\label{sec:baselines}
As introduced in Section~\ref{sec:relatedwork}, there are only a few works about dynamic HOM generation using a learning-based method, while there are various techniques that could have the potential to be used in this task. Consequently, we design the following baselines.

\textbf{GraspTTA\cite{grasptta}:} GraspTTA proposed a strong baseline in static grasp generation. We use it to generate static grasps of several key snapshots in manipulation and then refine the result at test time using the TTA loss (We do not refine the network at test time). The dynamic HOM animation is thus generated from interpolation based on these snapshots.

\textbf{ManipNet\cite{zhang2021manipnet}:} Benefiting from carefully designed geometric sensors, ManipNet has shown a strong ability to generalization on generic object manipulation synthesis tasks. Different from our setting, ManipNet assumes additional input of wrist trajectory. To compare it with other methods, we provide it with the wrist trajectory generated by CAMS as input (and thus it only differs at fingers).

Though both GraspTTA and ManipNet have their own mechanisms to improve synthesis quality (\eg reduce penetration), they can also benefit from our optimization-based motion synthesizer (optimizing $\mathcal L_{penetr}$ to reduce penetration). We combine all baselines with an optimization stage (denoted as ``w/ opt'') and compare them with the CAMS-CVAE motion planner.

\subsection{Ablation Studies}
\label{sec:ablation}

\textbf{Remove Contact Optimization:} To demonstrate the advantages of contact optimizations in motion synthesis, we train a baseline model with the contact optimizations removed (CAMS-).

Besides removing the contact optimization in the synthesizer, we also did several ablation studies using different representation spaces of fingers. These experiments are left to our supplementary material.

\subsection{Comparison}
\label{sec:compare}

\textbf{Quantitative Results} Table~\ref{tab:quantitative_result} and Table~\ref{tab:user_study} show the quantitative results of our method and all baselines. Our method outperforms previous work on all tasks, even if contact optimization is applied to them.

An observation is that after applying the offline optimization stage, the performance gain of our method is significantly higher than the baselines. This can be explained by that the $\mathcal L_{contact}$ term in contact optimization takes the contact targets from our planner as input, and it is the key to producing gradients guiding the finger placement. Without intermediate contact target information, the optimization can only leverage the $\mathcal L_{penetr}$ loss term, and it may push the finger out of the object in unpredictable directions.

\textbf{Qualitative Results} Besides \figref{fig:Head}, \figref{fig:result-modediversity} shows that our method can generate diverse grasp modes on a single object instance. \figref{fig:result-comparison} shows that our method can generate reasonable poses given complex object shapes. We also show full result demonstrations in our video, including the generated whole HOM processes for different tasks, robustness to different object shapes and sizes, comparison between baseline methods, and diversity of generated manipulation styles.

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{images/motionseq.png}
%     \caption{\textbf{Visualization of the hand motion sequence manipulating scissors.} The sequence is split into 3 stages, where 4 frames are shown in each stage. }
%     \label{fig:result-motionsequence}
%     \vspace{-5pt}
% \end{figure}




% \label{sec:ablation}

% \textbf{Importance of Normal Vector in CAMS}
% We argue that the normal vector $\mathbf{V}_i$ in $\mathbf{R}=\{(\mathbf{C}_i, \mathbf{V}_i, \mathbf{N}_i)\}_{i=1}^5$ is % essential not only for finding correct contact using Synthesizer, but also for encoding the manipulation. We conduct % several experiments to study the effect of normal vector in Planner, by using different $\lambda_{vec}$. We find that % normal vector plays a deterministic role in manipulation diversity encoding. As shown in \emph{fig!!}, the % $\mathcal{L}_{KLD}$ decreases significantly when we lower the $\lambda_{vec}$ from 200 to 0. We know that a lower % $\mathcal{L}_{KLD}$ doesn't always creates a better result, on the one hand, a lower $\mathcal{L}_{KLD}$ means that the % sampled latent code from the standard Gaussian distribution is more similar to the latent code sampled from the real % latent space, on the other hand, a lower $\mathcal{L}_{KLD}$ might come from KL-vanishing, resulting in the degradation % of the network. In our experiment, when the $\mathcal{L}_{KLD}$ decreases under 0.18, the network starts to overfit and % there is less and less variation for generated manipulation. There are original three manipulation types for % \emph{Laptop} and we could generate them randomly by sampling a latent code from the standard Gaussian distribution with % $\lambda_{vec}=100$, as shown in \emph{fig!!}, however, when we lower the $\lambda_{vec}$ to 1, or just abandon it, there % is only one manipulation type and it is just an average of three manipulation types, causing lots of problems, as shown % in \emph{fig!!}. It is reasonable considering that different manipulation types look similar from the perspective of % contact position: there is no difference when we grasp different surfaces of the \emph{Laptop} and grasp different % positions of the same surface if they have similar $L_2$ distance. However, the normal vector of different surfaces % changes drastically, and it's an important clue when we decide the way to grasp something.

% \textbf{Finger Embedding in CAMS}
% joint conflicts?

% \textbf{contact reference along the whole sequence}
% more like a projection of contact