\section{Experiments}\label{sec:expeirment}
\subsection{Environments}
\smalltitle{Dataset}
Elbow AP and lateral pairs of radiographs were collected from three anonymous tertiary hospitals \footnote{Data sources are currently undergoing disclosure procedures, then will be revealed.}.
The dataset consists of 4,615 radiograph pairs of AP and lateral view positions collected from the two institutions are used for training and validation.
In addition, 164 pairs of AP and lateral radiograph images collected from another different institution were used for the test dataset.
Three and five researchers with more than 7 years of experience labeled the data providing the maturity score of each landmark for each dataset respectively (e.g., lateral condyle, trochlea, olecranon, proximal).
Lastly, landmarks have been extracted from each AP and lateral view elbow radiographs with a key point detection network as used in previous work.\cite{ahn2021assessment}.
Each extracted landmark is resized by $384 \times 384$.
More detailed statistics for the dataset are represented in the table in the supplementary material.

\smalltitle{Baseline algorithms}
Due to lack of Sauvegrain related works, we compare the previous work ~\cite{ahn2021assessment} and its variants to evaluate the BAA performance of SAT: an ensemble of $R$ single-view single-task CNNs (SV-ST CNNs), multi-view single-task CNNs (MV-ST CNNs), and multi-view multi-task CNN (MV-MT CNN), an ensemble of $R$ multi-view single-task ViTs (MV-ST ViTs) and multi-view multi-task ViT (MV-MT ViT).

\smalltitle{Training details}
The CNN used in ViT to embed the token is ResNet18~\cite{resnet}.
We set the depth to 12 with 6 heads and the embedding dimension to 384 for training the SAT and its variants. 
Models were trained using the SAM optimizer \cite{sam} with cosine annealing on a batch size of $16$ for 30 epochs.
The initial learning rate is 0.01.
For the data augmentation, we have rotated the image randomly between ±15°, shifted randomly to ±32 pixels, and flipped horizontally \footnote{Github repository URL will be updated after the review.}.

\begin{table*}[t]
\centering
\caption{Comparison results on the MAE for each landmark score, summation of the score, and skeletal age on
\textbf{test dataset}. TR refers to token replay. 
Wilcoxon $t$-test is performed between SAT and others on BAA. ($^{*}$: $P$-value<0.05; $^{**}$: $P$-value<0.01; $^{***}$: $P$-value<0.001.).
}
\label{tab:baa-result}
\begin{tabular}{c|c||c|c|c|c|c|c|c}
\toprule
Method &$\#$  & Lat. & Troch. & Prox. & Olec. & Prox. & Sum 
& BAA \\
& params & cond. &  & (AP) &  & (Lat) & 
& 
\\
\midrule
SAT (ours) & 33M &
\textbf{0.354} &
0.190 & 0.276 & \textbf{0.169} & 0.308
& \textbf{0.709} & \textbf{0.261} \\
\hline
\hline
SV-ST CNNs~\cite{ahn2021assessment} & 90M
& 0.455 & 0.301 & 0.398
& 0.252 & 0.372 & 1.054 & 0.372$^{***}$ \\
MV-ST CNNs & 90M
& 0.411 & 0.331 & 0.336
& 0.239 & 0.330 & 0.915 & 0.334$^{***}$ \\
MV-MT CNN & 18M
& 0.422 & 0.282 & 0.457
& 0.265 & 0.451 & 1.032 & 0.381$^{***}$ \\
MV-ST ViTs & 168M
& 0.357 & 0.210 & \textbf{0.256}
& 0.192 &\textbf{0.257} & 0.728 & 0.263
\\
MV-MT ViT & 33M
& 0.451 & 0.376 & 0.288
& 0.330 & 0.288 & 0.922 & 0.324$^{***}$
\\
\hline
\hline
SAT w/o TR & 33M & 0.439 & 0.341 & 0.279
& 0.321 & 0.289 & 0.888 & 0.318$^{***}$
\\
SAT w/o RAB & 33M & 0.356 & \textbf{0.189} & 0.281
& 0.176 & 0.299 & 0.720 & 0.261
\\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Experimental Results}
\smalltitle{Performance Comparison}
Table \ref{tab:baa-result} presents the number of parameters and mean absolute error (MAE) of each landmark, their sum, and their conversion to BAA.
The performance of MAE has been evaluated on the test set using five-fold cross-validation on the training set (80$\%$ training and 20$\%$ validation). 
The SAT model outperforms other compared methods, achieving 0.261 MAE on BAA.
Our results showed that the multi-view strategy is effective in both CNN-based and ViT-based architectures for solving the Sauvegrain method.
However, the multi-task problem remains challenging, as demonstrated by the inferior performance of MV-MT CNN and MV-MT ViT compared to MV-ST CNNs and MV-ST ViTs, respectively. 
It could be interpreted as multi-task methods were detrimentally affected by over-reliance on inter-landmark inputs.
Interestingly, our SAT model even outperformed MV-ST ViTs with significantly fewer parameters, suggesting that SAT explicitly emphasizes the attention of landmarks isotropically and benefits from both multi-view information and the interplay between landmarks, while addressing the multi-task problem.
Comparison using cumulative score~\cite{cumulative_score2,cumulative_score} is reported in the supplementary section.

\smalltitle{Ablation Study}
The last two rows in Table \ref{tab:baa-result} show the influence of each SAT component.
We have removed each component of our SAT and reported the BAA results.
Compared with MV-MT ViT, both \textit{token replay} and \textit{RAB} are crucial for the performance, however, the former contribute the most when combined together as shown in Table \ref{tab:baa-result}.
Nonetheless, utilizing RAB consistently outperforms ViT and induces higher relevance maps in its attention which is desirable in understanding what the classifier attributes its predictions to.

\begin{table*}
  \centering
     \caption{Comparison results on BAA differing encoder of SAT. Wilcoxon $t$-test is performed between ResNet and others, but there is no statistical significance.}
    \label{tab:encode_ablation}
    \begin{tabular}{c||c|c|c||c|c}
    \toprule
        Encoder & $\#$ & Inference time &  Training  & Sum & BAA \\
       & Params & (CPU/GPU) & time & \\
        \midrule
            ResNet18\cite{resnet} & 33M & 
            0.21s / 0.03s & 2.7h & 0.709 & 0.261 \\
            VGG16 \cite{vgg} &  157M &
                1.13s / 0.06s  & 10h & 0.674 & 0.248 \\
            Densenet121 \cite{densenet} & 29M &
                0.98s / 0.05s  & 6h & 0.708 &  0.260 \\
            HR-Net-w18-small\cite{hrnet} & 33M & 
                0.26s / 0.04s  & 3.7h & 0.710 & 0.261 \\
            ResNext50-32x4d \cite{resnext} & 44M &
                0.60s / 0.06s  & 8.5h & 0.699 & 0.252 \\
        \bottomrule
      \end{tabular}
\end{table*} 

 \begin{figure*}[t]
	\centering 
{\includegraphics[width=0.9\columnwidth]{./new_figures/MICCAI_new_figure7.pdf}\hspace{0mm}}
	\caption{Three cases of prediction results on the single-view single-task (SV-ST) CNNs and SAT with ground truth score. The red color highlights the prediction result where the difference from the ground truth is more than 3.}
	\label{fig:showcases}
\end{figure*}

Comparisons between other CNN encoders are reported in Table \ref{tab:encode_ablation}. 
We have observed that ResNet-18 shows the most efficient computational cost.
Though other models, such as VGG and ResNext, shows better BAA results than ResNet-18, their computational cost is not efficient as ResNet.
There is no statistically significant difference between the ResNet-18 and other models.
Thus, we have used the baseline encoder of SAT for ResNet-18 which has the least number of parameters.

\smalltitle{Case Analysis}
To verify that SAT indeed has lower variance in predicting the maturity score in real data, we have analyzed the 3 cases where SV-ST CNNs \cite{ahn2021assessment} has shown the largest MAE in summation of scores in Fig. \ref{fig:showcases}. Prediction score for SV-ST CNNs and SAT is reported with ground truth score.
As shown in Fig \ref{fig:showcases}, not only the prediction error of SV-ST CNNs but also the variance in the scores is much bigger than SAT does.
It demonstrates that SV-ST CNNs have limitations when applied to Sauvegrain methods, where the maturity score of each RoI is highly correlated.
On the other hand, SAT shows better prediction on each RoI and lower variance than SV-ST CNNs, demonstrating that SAT could be a practical solution for the hard cases.
