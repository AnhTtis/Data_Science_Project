% Template for ICME 2022 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP/ICME LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,epsfig}
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{multirow}
\usepackage[dvipsnames, svgnames, x11names]{xcolor} 

% \usepackage{caption}
% \captionsetup[figure]{font=small}
% \captionsetup[table]{font=small}

\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}
  \setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt plus 0.3ex}
}

\pagestyle{empty}


\begin{document}\sloppy

\setlength{\abovedisplayskip}{0pt}
% \setlength{\belowdisplayskip}{3pt}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}


% Title.
% ------
\title{A real-time blind quality-of-experience assessment metric\\ for HTTP adaptive streaming}
% \title{A real-time blind QoE assessment metric for HTTP adaptive streaming}
%
% Single address.
% ---------------
\name{Chunyi Li \textsuperscript{1}, May Lim\textsuperscript{2} , Abdelhak Bentaleb \textsuperscript{3}, and Roger Zimmermann\textsuperscript{2} \vspace*{-4mm}}
%Address and e-mail should NOT be added in the submission paper. They should be present only in the camera ready paper. 
\address{Shanghai Jiao Tong University\textsuperscript{1}, National University of Singapore\textsuperscript{2}, Concordia University\textsuperscript{3} \vspace*{-4mm}}   
  


\maketitle


%
\begin{abstract}
In today's Internet, HTTP Adaptive Streaming (HAS) is the mainstream standard
for video streaming, which switches the bitrate of the video content based on an Adaptive BitRate (ABR) algorithm. An effective Quality of Experience (QoE) assessment metric can provide crucial feedback to an ABR algorithm. However, predicting such real-time QoE on the client side is challenging. The QoE prediction requires high consistency with the Human Visual System (HVS), low latency, and blind assessment, which are difficult to realize together. To address this challenge, we analyzed various characteristics of HAS systems and propose a non-uniform sampling metric to reduce time complexity.
% Therefore, through analyzing the characteristics of HAS segments, 
Furthermore, we design an effective QoE metric that integrates resolution and rebuffering time as the Quality of Service (QoS), as well as spatiotemporal output from a deep neural network and specific switching events as content information. These reward and penalty features are regressed into quality scores with a Support Vector Regression (SVR) model. Experimental results show that the accuracy of our metric outperforms the mainstream blind QoE metrics by 0.3, and its computing time is only 60\% of the video playback, indicating that the proposed metric is capable of providing real-time guidance to ABR algorithms and improving the overall performance of HAS.
\end{abstract}
%
\begin{keywords}
Quality of Experience, HTTP Adaptive Streaming, Blind Quality Assessment;
\end{keywords}
%
\section{Introduction}
\label{sec:intro}

Nowadays, video has become the dominant application on the Internet. According to Cisco's survey \cite{cisco2020cisco}, video services already consume more than 80\% of current Internet capacity and demand is still growing. To meet the challenges posed by the transmission of large volumes of video data, content providers often use HTTP Adaptive Streaming (HAS),
%to deliver video streaming services to users 
which can adapt to dynamic network conditions and various device resolutions.
% , including mobile phones, tablets, laptops and HDTVs. 
HAS delivers media content in small segments over the HTTP/TCP protocol stack, and because of its adoption by leading content providers, HAS has become the dominant delivery method for Video on Demand (VoD) services.

% For video transmission, in the presence of bandwidth fluctuations due to factors like packet loss, signal strength, network congestion events, etc. \cite{surveyHAS}, the HAS designed a transmission protocol for the server and client. 

% HAS is designed as a client-server solution. that can deliver video in the presence of network fluctuations due to factors like packet loss, varying signal strength and network congestion events \cite{surveyHAS}. For the server, different bitrate levels for each video segment are provided for adaptation. For the client, 
In a HAS client (\emph{i.e.,} the media player), an Adaptive BitRate (ABR) algorithm selects a suitable bitrate level for the download of each video segment. The goal of ABR algorithms~\cite{bentaleb2018survey} is to maximize the user's Quality of Experience (QoE). Therefore, an effective objective QoE metric is crucial in HAS systems. It is either used after a video session ends to evaluate the performance of an ABR algorithm or during playback to guide the ABR algorithm in selecting the most appropriate bitrate demand for the next video segment.
% and to guide the network resource allocation.
% as Fig. \ref{fig:HAS} shows.

% \begin{figure}[tbp]
% 	\centering
% 	\includegraphics[width=0.48\textwidth]{ABR.pdf}
% 	\caption{A schematic diagram of HTTP adaptive streaming and QoE prediction models. The QoE model above is for providing guidance on resource allocation and the model below is for evaluating the performance of the ABR scheme.}
% 	\label{fig:HAS}
% \end{figure}

% Providing guidance for resource allocation 
% It is generally more challenging to design effective QoE metrics for bitrate guidance than for ABR performance evaluation due to its three special requirements: 
Designing an effective objective QoE metric for bitrate guidance is challenging due to the following three requirements.
% , besides the fundamental need for high consistency with the human visual system (HVS), it also needs to achieve low latency and  access to the reference data in its computation.
($i$) \textbf{High Consistency with HVS}: The QoE metric needs to be consistent with HVS so that it can accurately predict the user's perceived quality of video playback. ($ii$) \textbf{Low Latency}: To provide guidance during transmission, the feedback of the QoE metric should be computed along with the video playback, and not after transmission like in ABR performance evaluation. In this case, the computation time of the QoE metric cannot be longer than the segment's playback time, and hence its complexity should be reduced to ensure real-time feedback. Finally, ($iii$) \textbf{Blind Assessment}: The real-time prediction mentioned above is performed on the client. Hence, this should be a No-Reference (NR) task that uses only the compressed/distorted video available at the client, instead of Full-Reference (FR) or Reduced-Reference (RR) scenarios which use the original uncompressed source video or some of its reference features that have to be separately acquired from the server.
%...
%
% The rest of the paper is organized as follows. Section~\ref{sec:RW} shows the existing solutions for QoE optimization. Section~\ref{sec:OV} describes the \Name solution, followed by the performance evaluation in Section~\ref{sec:PE}. Section~\ref{sec:DI}  
% concludes the paper.
% \begin{itemize}
%     \item \textbf{High Consistency}: The QoE model needs to be consistent with the Human Visual Systems (HVS) mechanism so that it can accurately predict the perceptual quality of a streaming video; 
%     \item \textbf{Low Latency}: To provide guidance during transmission, the feedback of the QoE model should be computed along with the video playback, and not after transmission like in ABR performance evaluation. In this case, the computation time of the QoE model cannot be longer than the video itself, and hence its complexity should be reduced to ensure real-time feedback; 
%     \item \textbf{Blind Assessment}: The real-time prediction mentioned above is performed on the client, instead of the server, which means that this is a No-Reference (NR) task that uses only the compressed/distorted video available at the client, instead of Full-Reference (FR) or Reduced-Reference (RR) scenarios where some reference features from the high bitrate video on the server have to be acquired.
% \end{itemize}

\section{Related Work and Contributions}
\label{sec:relatedworks}
% Generally, previous QoE models provide essential guidance for the HAS transmission \cite{QoEforHAS}, which include 
Generally speaking, existing QoE metrics that provide guidance for HAS delivery can be classified into three types \cite{C:KSQI}: QoS-based, signal fidelity/content-based, and hybrid metrics.
% As discussed below, each of these category of metrics has its own merits, but their drawbacks make it difficult for them to meet the requirements of the quality assessment task for bitrate guidance.
% (highlighted in Section \ref{sec:intro}). 

\begin{figure*}[tbp]
	\centering
	\includegraphics[width=0.98\textwidth]{FrameworkNarrow.pdf}
 \vspace{-3mm}
	\caption{The framework of the proposed method.}
	\label{fig:Framework}
 \vspace*{-5mm}
\end{figure*}

\textbf{QoS-based} metrics \cite{A:FTW,A:Yin2015} have the lowest complexity as their computation typically involves a simple combination of network and/or client-side data, such as video bitrate, startup latency, and rebuffering duration.
% by computing only network or client-side factors, such as the video bitrate and height/width, to guide resource allocation.
%For instance, a metric could model the positive impact of video bitrate and the negative impact of bitrate switches or rebuffering in predicting the QoE. 
% It analyses the positive impact of video bitrate/quantization parameter (QP) / resolution and the negative impact of re-buffering / bitrate switching to predict the QoE of a client.
However, such metrics tend to show the least consistency with HVS as the video content is not considered in its computation, which has a strong influence on perceptual quality.
% , whose prediction \textbf{consistency} for HVS's perceptual quality is not enough.

\textbf{Content-based} metrics \cite{B:Brisque,B:Viideo} analyze the video content and its signal fidelity using image or video quality assessment (I/VQA) metrics to predict a video's distortion level \cite{Zhai2020PerceptualIQ}. IQA metrics take selected video frames as input and build a time series model \cite{NR:Series} based on the quality of these frames to output a quality score for the video. Since an IQA metric needs to be computed repeatedly for each frame, it may not scale well. Existing VQA metrics also tend to have long run times.
% IQA metrics should be conducted repetitively and the current VQA metric has a high time complexity. 
%Moreover, NR metrics compute signal distribution within a video, rather than simply comparing with a reference like FR metrics \cite{FR:Videval}, which is typically more time-consuming. 
Only a few simplified NR-VQA metrics \cite{B:Rapique, B:FastVQA} are able to provide real-time feedback for ABR, and such simplification leads to some degradation in their performance.
% Although some advanced NR metrics such as RAPIQUE \cite{B:Rapique} and Fast-VQA \cite{B:FastVQA} can complete VQA tasks quickly on the server, for the client the \textbf{latency} is still longer than the video length itself to reach real-time feedback. 

\textbf{Hybrid} metrics \cite{C:Bentaleb2016,C:SQI} combine data from both the QoS and video content. Their QoE score is generally composed of a reward function represented by IQA/VQA metrics and a penalty function based on QoS factors. This allows the metric to reach a balance between time complexity and consistency with HVS, which is promising for its use in bitrate guidance.
% correlation with human perception. 
Unfortunately, some of the existing hybrid metrics have IQA/VQA kernels that are designed for FR tasks only \cite{C:Bentaleb2016}, while the universal models \cite{C:TV-QoE} tend to show high consistency with HVS only on the FR or RR kernels but low consistency when it comes to NR. Therefore, the performance of such hybrid NR metrics can be further improved, as we show in this work.
% \cite{FR:MS-SSIM}
% \cite{RR:ST-RRED}

As none of the existing metrics above can satisfy the three requirements of Section \ref{sec:intro} altogether, we propose a new QoE assessment metric for HAS with the following contributions. ($i$) We perform a non-uniform sampling scheme since HVS has an increasing focus tendency throughout each segment. Without analyzing too many frames at the start of a segment, gradually increasing the sampling rate can reduce time complexity. ($ii$) We introduce QoS into reward features. As NR functions are not as effective as FR/RR, some QoS information can help our model perform better for blind assessment. ($iii$) We introduce content into penalty features. By analyzing specific frame changes instead of QoS fluctuations, the user's QoE can be better characterized.

% with a Spearman rank correlation coefficient (SRoCC) between QoE model and HVS more than 0.8, but when it comes to NR, such factors reduced to 0.4. Therefore, although hybrid models realized a trade-off between high consistency and low latency, their performance on \textbf{blind quality assessment} tasks still needs to be improved.

% \begin{figure}[tbp]
% 	\centering
% 	\includegraphics[width=0.48\textwidth]{CompareModel.pdf}
% 	\caption{The comparison between QoS-based, content-based, hybrid, and our model.}
% 	\label{fig:CompareModel}
% \end{figure}

% These three types of models mentioned above are shown in Fig. \ref{fig:CompareModel}. From Fig. \ref{fig:CompareModel} and the analysis above

% // REMOVED & SUMMARIZED IN SECTION 3 //
% From the discussion above, it can be seen that for the real-time blind quality assessment task of HAS, these existing QoE models are inadequate for high consistency / low latency / no-reference requirements. Therefore, based on the hybrid model, we propose a QoE model which extracts features referring to three characteristics of HAS to predict its perceptual quality:

% \begin{itemize}
%     \item HAS had 13 encoding levels with bitrates ranging from 200 to 16,000 kB/s. With frequent switching of network conditions, such bitrate switching of each video segment harm clients' QoE. While traditional hybrid models use QoS as a penalty for switching events, we incorporate content into this penalty to analyze the specific frame changes, rather than bitrate changes. Such a penalty optimized the prediction of switching events and improve the \textbf{consistency} of QoE prediction scores with HVS scores.
    
%     \item HAS had good timing correlation and therefore does not need to sample all frames of the video. While traditional hybrid models sample every certain frame, we sample non-uniformly in chronological order at the currently analyzed segment. Since HVS has an increasing tendency to focus on each segment, instead of sampling densely all the time, we gradually increase the sampling rate. Thus, the computing \textbf{latency} is reduced by analyzing fewer frames and the performance of the model won't degrade significantly.
    
%     \item HAS can use QoS factors to reflect the perceptual quality of HVS to some extent. While traditional hybrid models use NR IQA / VQA metrics as a reward for the perceptual quality of video segments, we introduced QoS factors into this reward as previous QoS models do. As NR functions are not effective for the hybrid model compared with FR/RR, some QoS information can help our model performs better in \textbf{blind} quality assessment tasks.
% \end{itemize}

% From such characteristics of HVS, we extract reward and penalty features from both video content and QoS. After extracting these 4 types of features, the quality score is obtained by regressing the features mentioned above through support vector regression (SVR). The proposed method is mainly tested on the waterloo sQoE \uppercase\expandafter{\romannumeral3} dataset \cite{w3}, which is specially established for assessing the QoE of adaptive bitrate video streaming. The experimental results show that the proposed method outperforms other mainstream blind QoE prediction models for HAS content.

\section{Proposed Method}
\label{sec:proposed}

% To design a blind QoE metric that can effectively balance the trade-offs between high consistency with HVS and low latency, 
To design a QoE metric that can effectively meet the requirements discussed in Section~\ref{sec:intro}, we identified novel features and adaptations that contribute materially to these requirements. The framework of our blind QoE metric is shown in Fig.~\ref{fig:Framework} and includes three parts: \emph{sampling}, \emph{feature extraction}, and \emph{quality regression}. Taking inspiration from hybrid metrics in Section~\ref{sec:relatedworks}, we extract four types of features: reward/penalty QoS features and reward/penalty content features. For content features, each video segment in the client's buffer is first non-uniformly sampled to select a suitable subset of image frames for feature extraction. The QoS/content features are extracted by ResNet-50, texture analysis, and inter-frame difference, and then regressed through a support vector regression (SVR) model to give a quality score representing the user's current QoE. We discuss the details of each step below.
% Firstly, the data in the client's buffer are divided into two parts, namely QoS factors and video content. For content, each video segment is non-uniformly sampled as several images. Then, the reward and penalty features are extracted from the QoS factors and images mentioned above. Finally, for five segments, a total of 36 features are regressed into quality values through an SVR model to represent the client's current QoE.

\vspace*{-1mm}
\subsection{Sampling}
\label{sec:samp}
\vspace*{-1mm}

For real-time QoE assessment, content-based/hybrid metrics analyze only a subset of frames to reduce complexity. 
% However, the following characteristics of HAS result at the end of the segment having a greater impact on QoE than at the start:
%\begin{itemize}
%    \item During the execution of the ABR algorithm, rebuffering may occur between different slices of the video, and the Waterloo sQoE \uppercase\expandafter{\romannumeral3} dataset \cite{w3} shows that 29.28\% of the segments have rebuffering between them, and 11.94\% are longer than 1 second. In the above rebuffering events, the client perceives the last frame of the slice continuously, resulting in a high correlation between QoE and the end of the segment's quality.
%    \item The fluctuating network condition forced the ABR algorithm to switch bitrate, and the Waterloo sQoE \uppercase\expandafter{\romannumeral3} dataset \cite{w3} shows that 30.94\% of the segments experienced bitrate change. In the above switch events, fluctuation in code rate could change the visual saliency \cite{VS} of the HVS, causing the HVS to re-perceive the new slice and not consider it to be continuous with the previous slice. Such a mechanism of HVS cause HVS not to pay enough attention to the start of a video segment.
%\end{itemize}
% Considering two HAS characteristics mentioned above, 
While traditional metrics sample each segment uniformly, it is generally believed that frames within a segment may not contribute equally to QoE. To study their respective contributions, we run Brisque \cite{B:Brisque}, a widely used NR-VQA metric, on the
% video segments in the 
Waterloo sQoE \uppercase\expandafter{\romannumeral3} video dataset~\cite{w3}. As each segment is two seconds long 
% \cite{SegmentHAS} 
and an intra-coded frame usually appears every one second~\cite{HAScoding}, we divide a segment into two halves (start/end) {for non-uniform sampling} and represent its $QoE$ as: 
$QoE {\,=\,} {w_s}Qo{E_s} + {w_e}Qo{E_e}$,
% \begin{equation}
%     QoE = {w_s}Qo{E_s} + {w_e}Qo{E_e}
%     \label{equ:QoEw}
% \end{equation}
where $w_{s}$, $w_{e}$ are weight parameters for the QoE of the start/end of a segment $Qo{E_s}$ and $Qo{E_e}$, respectively. The Spearman Rank-order Correlation Coefficient (SRoCC) is used as the correlation function $\mathcal{S}$ between $QoE$ and the single-stimulus mean opinion scores (MOS) $M$ obtained from the subjective assessment on the dataset: $\mathcal{S}(QoE,M) {\,=\,} {w_s}\mathcal{S}(Qo{E_s}^{f{r_s}},M) + {w_e}\mathcal{S}(Qo{E_e}^{f{r_e}},M)$,
% \begin{equation}
%     \begin{array}{c}
%     \mathcal{S}(QoE,M) = {w_s}\mathcal{S}(Qo{E_s},M) + {w_e}\mathcal{S}(Qo{E_e},M)\\
%      = {w_s}\mathcal{S}(f{r_s}) + {w_e}\mathcal{S}(f{r_e})
%     \end{array}
%     \label{equ:SRoCC}
% \end{equation}
where $fr_s$ and $fr_e$ are the sampled frames from the start/end of a segment. 
% Sampling more frames generally improves the metric's prediction performance in terms of its correlation with subjective scores.
% The more frames sampled, the better the QoE model's predictions correlated with a subjective score. 

% To explore the mapping relation between them, w
To better understand the relationship between sampling rate and correlation performance, we first used eight different sampling rates and three common IQA metrics (Niqe~\cite{B:Niqe}, Piqe~\cite{B:Piqe}, Brisque~\cite{B:Brisque}) to predict QoE. The normalized SRoCC between predicted QoEs and subjective scores show that the correlation factor is logarithmically related to the sampling rate as seen in Fig.~\ref{fig:Log}.
% The normalized SRoCC between predicted QoE and subjective score results from Fig. \ref{fig:Log} show that the correlation factor is logarithmically related to the sampling rate. 
Hence, the sampling scheme can be transformed into the following optimization problem:
\vspace*{-6pt}
% So with a certain number of sampling frames, the specific sampling scheme can be transformed into an optimization problem:


\begin{figure}[tbp]
    \centering	\includegraphics[width=0.48\textwidth]{MeanBar.pdf}
    \vspace*{-7mm}
	\caption{{Approximate convex logarithmic relationship between sampling rate and the normalized SRoCC.}}
    \label{fig:Log}
    \vspace*{-3mm}
\end{figure}

{\small
\begin{equation}
    \left\{ \begin{array}{l}
fr = f{r_s} + f{r_e}\\
{\mathop{\rm maximize}\nolimits} ({w_s}\log (f{r_s}) + {w_e}\log (f{r_e}))
\end{array} \right.
    \label{equ:convex}
\end{equation}
}
\hspace*{-2mm}
where $fr$ is the total number of sampled frames desired. Via 
Lagrange multiplier, the derivative of the log function in (\ref{equ:convex}) gives $fr$ as proportional to $w$ for the best sampling scheme:
\vspace*{-6pt}

{\small
\begin{equation}
    \frac{{{fr_s}}}{{{fr_e}}} =\frac{{{w_s}}}{{{w_e}}} = \frac{{\mathcal{S}({\mathop{\rm brisque}\nolimits}(seg_s),M)}}{{\mathcal{S}({\mathop{\rm brisque}\nolimits}(seg_e),M)}}
    \label{equ:sampling}
\end{equation}
}
\hspace*{-2mm}
where $seg_s$ and $seg_e$ are the start/end of a segment and QoE is predicted by ${\rm brisque}(\cdot)$. %Results \cite{w3} show that a sampling scheme of $\frac{fr_s}{fr_e}=\frac{0.367}{0.633}$ gives the best QoE prediction performance. 
The detailed proportion and derivation are attached in the supplementary.
% In Waterloo sQoE \uppercase\expandafter{\romannumeral3} dataset \cite{w3}, result from brisque shows that a sampling scheme with $\frac{fr_s}{fr_e}=\frac{0.367}{0.633}$ can reach the best QoE prediction performance.

\subsection{Feature Extraction}

We discuss how the four types of features are extracted below.

\textbf{Reward QoS Feature}. QoS refers to the basic network or client-side metrics during a video streaming session.
% QoS factors represent the basic information of a segment in HAS. 
Among them, video bitrate, quantization parameter (QP), frame rate, and resolution (height/width) have a positive impact on the user's QoE. After studying the correlation performance of these factors, %(similar to Section~\ref{sec:samp})
% A comparison is made among those factors in Tab.\ref{tab:onlyQoS}, where video height shows the best correlation for subjective score. 
we found video height performs well as a reward feature $r_1$ to characterize the perceptual quality. Hence, $r_1={\rm height}(seg)$,
% can be used to characterize the perceptual quality as reward $r_1$:
%
% \begin{table}[btp]
% \caption{The SRoCC between QoS facthe tors and subjective score on Waterloo sQoE \uppercase\expandafter{\romannumeral3} dataset.}
% \begin{tabular}{lllllll}
% \hline
% \multicolumn{1}{|c|}{QoS} & \multicolumn{1}{c|}{height} &  \multicolumn{1}{c|}{width} & \multicolumn{1}{c|}{bitrate} & \multicolumn{1}{c|}{QP} & \multicolumn{1}{c|}{framerate} \\ \hline
% \multicolumn{1}{|c|}{SRoCC} & \multicolumn{1}{c|}{\textbf{0.5584}} &  \multicolumn{1}{c|}{0.5411} & \multicolumn{1}{c|}{0.5060} & \multicolumn{1}{c|}{0.2416} & \multicolumn{1}{c|}{0.0053} \\ \hline
% \end{tabular}
% \label{tab:onlyQoS}
% \end{table}
%
% \begin{equation}
%     r_1={\rm height}(seg)
%     \label{equ:r1}
% \end{equation}
where $seg$ is a video segment in HAS. 
% It will be used as the major feature to represent the positive experience for the client on the playing video segments. 

%\subsection{Reward Content Feature Extraction}

\textbf{Reward Content Features}. 
After sampling the frames in Section~\ref{sec:samp}, we analyze this series of images which has three types of attainable features: structural, temporal, and chrominance. Structural features in images can reflect perceptual quality very well~\cite{Structural1} 
% as images are the building blocks of video, 
and are widely used for QoE prediction. Temporal features can be computed independently or from the structural features via a spatial-temporal fusion.
% After structural features are computed, temporal features can be predicted by an independent module or by spatial-temporal fusion from structural features. 
Chrominance features, like structural features, are computed independently from images. However, as the HVS processes visual signals in three channels, computing chrominance features may increase complexity considerably.
% as the visual cells of the human eye are divided into 3 channels\cite{HSV}, chrominance features may triple the model's complexity. 
% Considering the negative impact of temporal and chrominance features on our real-time requirement, 
To meet the real-time requirement, we include structural features and integrate temporal features with spatial-temporal fusion, while abandoning chrominance features.
% whose extracting mechanism is shown in Fig. \ref{fig:ResNet} and Fig .\ref{fig:texture} respectively.

The structural features can be divided into spatial and texture features. Spatial features refer to the relative spatial positioning or orientation of different elements in an image. 
% (adjacent, overlapping, etc.). 
An overview of our spatial feature extraction method is shown in Fig.~\ref{fig:ResNet}. ResNet-50 is the backbone of the module, which can represent the spatial correlation between pixels and has proven to be quality-aware \cite{Qaware}. For the $i$-th sampled frame in a segment, we transform it into a gray map $g(i)$ as the input to ResNet-50, 
% with a 2048 meta-array as the penultimate layer. 
which extracts four features $L_i={l_{1\sim4}}(i)$ as:
\vspace*{-2pt}
% for each image in the last layer as:
{\small
\begin{equation}
    \left\{ {\begin{array}{l}
    {P(i) = {\rm{ResNet50}}(g(i))}\\
    {[{l_1}(i),{l_2}(i)] = [\max (P(i)),\min (P(i))]}\\
    {[{l_3}(i),{l_4}(i)] = [\rm{avg} {(P(i))},\rm{std} {(P(i))}]}
\end{array}} \right.
    \label{equ:ResNet}
\end{equation}
}
\hspace*{-2mm}
where $P(n)$ is the 
% 2048-item output from penultimate layer 
output from ResNet-50 and $i$ is the frame index.
% between 1 and total sampled frames $fr$ in current segment. 
%
%The global maxima and minima are used in computing the global average pooling $\rm{avg}$ and global standard deviation pooling $\rm{std}$ layer.
% Global maximum and minimum are conducted to find a range of output, which will help a global average pooling $\rm{avg}$ and a global standard deviation pooling $\rm{std}$ layer to downgrade features for images. 
%
On a segment level, instead of computing the four spatial features as a global average, we introduce a gated recurrent unit (GRU) \cite{GRUfang} to capture the temporal relation between the spatial features. The spatiotemporal rewards $r_{2\sim5}$ are:
\vspace*{-6pt}

\begin{figure}[btp]
	\centering
	\includegraphics[width=0.48\textwidth]{ResNet50.pdf}
        \vspace*{-7mm}
	\caption{The spatial feature extraction method.}
	\label{fig:ResNet}
        \vspace*{-3mm}
\end{figure}

{\small
\begin{equation}
    \left\{ {\begin{array}{*{20}{l}}
{{h_i} = GRU\left( {W\cdot{L_i} + b,{\rm{ }}{h_{i - 1}}} \right)}\\
r_{2\sim5}=h_{fr}
\end{array}} \right.
    \label{equ:r2-5}
\end{equation}
}
\hspace*{-2mm}
where $W$ and $b$ are the weights and bias parameters in the
% FC layer of 
GRU \cite{GRUfang}, while ${h_i}$ is a four-cell meta-array like ${L_i}$ that restores the memory-forgetting mechanism of HVS in the time domain. Thus, these four features combine both spatial and temporal information.
% Thus, we extract 4 spatial features, while taking temporal information into account without introducing further time complexity from any extra feature extraction module.

Texture features refer to the surface characteristics of objects within images.
% , when combined with spatial features (relation between objects), the main properties of an image/video can be characterized. 
Although operators such as Sobel, Laplace \cite{operator} are commonly used for texture extraction, given the complexity that ResNet-50 has already introduced for spatial features above, a simpler solution is needed here.
%
% \begin{figure}[tbp]
% 	\centering
% 	\includegraphics[width=0.48\textwidth]{textureNew.pdf}
% 	\caption{The texture feature extracting method.}
% 	\label{fig:texture}
% \end{figure}
%
Similar to how MacroBlocks (MBs) are used as { the basic processing unit in video compression~\cite{Patrick4}}, we first compute the average row and column intensity values $Ra_y$, $Ca_x$ from its gray map.
% In today's video compression algorithms, intra-frame prediction is used to remove spatial redundancy from the video. As the video content in HAS follows protocols such as H264, H265\cite{HEVC}, etc., the macroblock (MB) can be used as the basic unit for complexity analysis. We first compute the average row/col value $Ra_y$ and $Ca_x$ from its gray map.
%\begin{equation}
%    \left\{ {\begin{array}{*{20}{c}}
%{Ra_y = \frac{1}{{row}}\sum\limits_{x = 1}^{row} {{g_{x,y}}(i)} %}\\
%{Ca_x = \frac{1}{{col}}\sum\limits_{y = 1}^{row} {{g_{x,y}}(i)} }
%\end{array}} \right.
%    \label{equ:RaCa}
%\end{equation}
%where $(row,col)$ is the size of image with $(x,y)$ as index.
Then we divide the gray map $g(i)$ into multiple 16$\times$16 MBs. For each MB, we calculate the difference between the gray map value and the above-average values in the respective directions (horizontally, vertically, and diagonally). We select the minimum difference above as an MB's texture, and combine them into the texture feature $r_6$ as:
{\small
\begin{equation}
\left\{ {\begin{array}{*{20}{l}}
{Ho{r_j} = \sum {|R{a_y}(i) - {g_{x,y}}(i)|} }\\
{Ve{r_j} = \sum {|C{a_x}(i) - {g_{x,y}}(i)|} }\\
{Di{a_j} = \sum {|0.5(C{a_x}(i) + R{a_y}(i)) - {g_{x,y}}(i)|} }\\
{{r_6} = \sum {\min (Ho{r_j},Ve{r_j},Di{a_j})} }
\end{array}} \right.
\label{equ:r6}
\end{equation}
}
\hspace*{-2mm}
where $Hor$, $Ver$, and $Dia$ are the texture information computed in three directions using the row average $R{a_y}$ and column average $C{a_x}$, while $j$ is the MB index. 
% After that, all structural features including spatial and textural features are computed, which can improve QoE from different aspects.

%\subsection{Penalty QoS Feature Extraction}
%\label{sec:rebuffering}

\textbf{Penalty QoS Features}. Among QoS metrics, rebuffering has one of the largest negative impacts on QoE \cite{rebuffering1}. The rebuffering duration and number of rebuffering events are common penalty features used in QoE models~\cite{A:Mok2011, C:Bentaleb2016}.
% , regardless of QoS \cite{A:Mok2011}-based or hybrid \cite{C:Bentaleb2016} model. 
Besides, players tend to bear an initial buffering (of fewer than two seconds) in exchange for less rebuffering during playback \cite{rebuffering2}. We also note that the negative impact on QoE scales linearly with both rebuffering duration and number of rebuffering events and so we only consider one of them.
Hence, we include the initial buffering duration and average rebuffering duration as penalty features $p_1$ and $p_2$:
% Besides, players tend to bear an initial rebuffering event of fewer than 2 seconds in exchange for a rebuffering-free future playback \cite{rebuffering2}. However, the following event's QoE decreased evenly with the extension of rebuffering time, and each rebuffering event contributed equally \cite{A:Liu2012} for such QoE decrease. Therefore, we use the initial rebuffering time as a single penalty feature $p_1$ and the average following rebuffering time as another penalty feature $p_2$:
{\small
\begin{equation}
    \left\{ {\begin{array}{*{20}{l}}
{{p_1} = {D_1}}\\
{{p_2} = \frac{1}{{T - 1}}\sum\limits_{t = 2}^T {{D_t}} }
\end{array}} \right.
    \label{equ:p1p2}
\end{equation}
}
\hspace*{-3mm}
where $T$ is the total number of segments, $t$ is the segment index, and $D_t$ is the buffering duration of the segment.
% where the $T$-th video segment is playing with an index $t$ and $R_t$ refers to its rebuffering time. From this, we have constructed two penalty features based on QoS that can fit the HVS's perception mechanism for rebuffering events.

%\subsection{Penalty Content Feature Extraction}


\textbf{Penalty Content Features}. 
Research into video QoS analysis \cite{w3} has shown that bitrate switching and rebuffering events can create a poor experience for users, the impact of which depends on several factors, including: ($i$) Switching pattern: It is generally believed \cite{C:Bentaleb2016} that dropping from high to low bitrate creates a more undesirable experience than going from low to high bitrate, and a long rebuffering duration \cite{Stalling} can further amplify the negative effects of such switching event.
% ($ii$) Rebuffering duration: As switching events occur between segments where rebuffering could also occur, a long/short rebuffering duration may influence the negative impact of the switching event differently; 
($ii$) Video content: For a video where objects are moving slowly with minimal scene changes, the impact of a bitrate switching or rebuffering event is limited, while in an action movie, if the switching/rebuffering occurs during intense motion or when the scene has just changed, it generally leads to a significant drop in user's QoE.
% the extent of bitrate shift and the video content on which the switch happens.
% in three ways: 
% ($i$) user may notice bitrate changes, ($ii$) user may experience rebuffering in upshifts, ($iii$) user ..?
% switching level, rebuffering time and video content. 
The negative impact of each switching (or rebuffering) event is traditionally calculated using the change in inter-segment bitrate (or rebuffering duration), without considering the video content.
% the inter-segment bitrate differential via a ReLU function without considering other factors such as rebuffering duration and video content.
% but the video content tends to be ignored.
%
%\begin{itemize}
%    \item Rebuffering time: As bitrate switching events occur between video segments, they belong to the following rebuffering type in Sec \ref{sec:rebuffering}. Thus, we assume the adverse impact of rebuffering on QoE growing linearly over time.
%    \item Switching level: Generally HAS has 13 bitrate level\cite{surveyHAS} that affect clients in two ways\cite{C:KSQI}. Firstly in a given total bitrate, the QoE of a medium bitrate video tends to be greater than that of a high and low bitrate combined video. Secondly for a low and a high bitrate segment, dropping from high to low bitrate creates a more undesirable experience for the client than going from low to high bitrate, even though the two segments only switched their order. Traditional methods quantify this negative impact by bitrate differential between segments \cite{C:Bentaleb2016}, or global bit-rate variance \cite{A:Xue2014}. Therefore, we calculate the negative effect of each switch using the inter-segment bitrate differential and map it to the penalty via the ReLU function.
%    \item Video content: The first two items are characterized by QoS and ignore the content of the video itself, which has a significant impact on the switching and rebuffering events. 
    % As shown in Fig. \ref{fig:switch}, 
% For example, for a video where objects are moving slowly with minimal scene switches, the impact of bitrate switching (and rebuffering) events is limited; however, in an action movie, if bitrate switching occurs during intense motion or when the scene has just switched, it generally leads to a significant drop in user QoE. 
Conversely, we use the efficient FastSSIM \cite{fastSSIM} metric to represent the inter-frame difference in video content between segments.
% So we use the inter-frame difference to consider what exactly is being switched, with an efficient FastSSIM\cite{fastSSIM} metric to calculate this difference.
%\end{itemize}
%
% \begin{figure}[tbp]
% 	\centering
% 	\includegraphics[width=0.48\textwidth]{switch.pdf}
% 	\caption{A rebuffering and switching event from a landscape (above) and an action (below) video segment.}
% 	\label{fig:switch}
% \end{figure}
%
The penalty content features $p_3$ combine {all three factors above, namely the rebuffering time, switching level, and content mentioned in supplementary} as shown in (\ref{equ:p3}).% The negative impact of a switching event is represented by $swh$. Then the content-aware penalty features $p_3$ combined with the three items mentioned above:
{\small
\begin{equation}
\left\{ {\begin{array}{*{20}{c}}
{swh = {\mathop{\rm ReLU}\nolimits} (bitrate(se{g_t}) - bitrate(se{g_{t + 1}}))}\\
{{p_3} = (1 + \frac{{D_t}}{C_1})(1 + \frac{{swh}}{C_2})/{\mathop{\rm ssim}\nolimits} (se{g_t},se{g_{t + 1}})}
\end{array}} \right.
    \label{equ:p3}
\end{equation}
}
\hspace*{-2.5mm}
where $swh$ is the inter-segment bitrate differential mapped to a ReLU function as it can characterize bitrate decline in the switching pattern mentioned above, while $C_1$ and $C_2$ are constants for normalization. The FastSSIM function ${\rm ssim}(\cdot)$ is performed on the last sampled frame in segment $seg_t$ and the first sampled frame in $seg_{t+1}$. 
% Thus, we took the video content itself into account, which further shows each rebuffering and switching event.

\vspace*{-2mm}
\subsection{Overall QoE Regression}

After the above operations, we finally obtain two reward feature groups $r_1$ and $r_{2 \sim 6}$, and two penalty feature groups $p_{1,2}$ and $p_{3}$ to represent the positive and negative impact on user's QoE, respectively. The feature groups are mapped to features $f_{1 \sim 36}$ as shown in Table~\ref{tab:feature}. Using these features extracted from QoS and video content information, a quality prediction model is constructed via support vector regression (SVR) to generate the overall QoE score as shown in Fig.~\ref{fig:Framework}.
%
\begin{table}[]
\caption{The list of feature groups and their definitions.}
\small
\hspace*{-3mm}
\begin{tabular}{|c|c|c|c|}
\hline
Feature Group                   & Component & Origin & Description                            \\ \hline
Reward QoS                      & $f_{1\sim5}$      & $r_1$     & Resolution             \\ \hline
\multirow{2}{*}{Reward Content} & $f_{6\sim25}$     & $r_{2\sim5}$   & Spatial-Temporal \\ \cline{2-4} 
                                & $f_{26\sim30}$    & $r_6$     & Texture       \\ \hline
Penalty QoS                     & $f_{31\sim32}$    & $p_{1\sim2}$   & Rebuffering                       \\ \hline
Penalty Content                 & $f_{33\sim36}$    & $p_3$     & 
\begin{tabular}{@{}c@{}}Switching, Rebuffering \\ (content-aware)\end{tabular}
\\ \hline
\end{tabular}
\label{tab:feature}
\vspace{-4mm}
\end{table}
%
% Based on the perceptual and forgetting mechanism
As suggested in prior studies \cite{w3}, we use the most recent five segments in the playback for prediction in the SVR model.
%\begin{equation}
%\left\{ \begin{array}{l}
%{f_{6(x - 1) + y}} = {r_y}(se{g_x})\\
%{f_{31}} = {p_1}(se{g_5})\\
%{f_{32}} = {p_2}(se{g_1},se{g_2},se{g_3},se{g_4})\\
%{f_{32 + z}} = {p_3}(se{g_z},se{g_{z + 1}})
%\end{array} \right.
%    \label{equ:feature}
%\end{equation}
The SVR model is implemented using LIBSVM \cite{libsvm} with {a radial basis function (RBF) kernel for feature fusion\cite{Patrick2}}.

\begin{table*}[t]
	\centering
        \small
        \vspace*{-2mm}
	\caption{Performance results on the Waterloo sQoE \uppercase\expandafter{\romannumeral3} and LIVE Netflix \uppercase\expandafter{\romannumeral2} datasets.}
	\label{performance}
\begin{tabular}{|c|c|c|cccc|cccc|}
\hline
\multirow{2}{*}{Type}    & \multirow{2}{*}{Subtype} & \multirow{2}{*}{Method} & \multicolumn{4}{c|}{Waterloo sQoE \uppercase\expandafter{\romannumeral3}}                                                                                                             & \multicolumn{4}{c|}{LIVE Netflix \uppercase\expandafter{\romannumeral2}}                                                                                                              \\ \cline{4-11} 
                         &                          &                         & SRoCC & KRoCC & \multicolumn{1}{c|}{PLCC ↑} & Time ↓ & SRoCC & KRoCC & \multicolumn{1}{c|}{PLCC ↑} & Time ↓\\ \hline
\multirow{9}{*}{Content} & \multirow{5}{*}{IQA}     & Brisque5\cite{B:Brisque}               & 0.4959                        & 0.3468                        & \multicolumn{1}{c|}{0.4690}                       & 1.944                        & 0.3146                        & 0.2182                        & \multicolumn{1}{c|}{0.2009}                       & 1.614                        \\
                         &                          & Brisque10\cite{B:Brisque}               & 0.4842                        & 0.3394                        & \multicolumn{1}{c|}{0.4600}                       & 0.965                        & 0.2704                        & 0.1880                         & \multicolumn{1}{c|}{0.1344}                       & 1.276                        \\
                         &                          & Brisque20\cite{B:Brisque}               & 0.4547                        & 0.3146                        & \multicolumn{1}{c|}{0.4478}                       & 0.498                        & 0.2543                        & 0.1794                        & \multicolumn{1}{c|}{0.1517}                       & 0.455                        \\
                         &                          & Niqe10\cite{B:Niqe}                  & 0.4021                        & 0.2732                        & \multicolumn{1}{c|}{0.4488}                       & 1.027                        & 0.6538                        & 0.4761                        & \multicolumn{1}{c|}{0.6684}                       & 0.529                        \\
                         &                          & Piqe10\cite{B:Piqe}                  & 0.4232                        & 0.2807                        & \multicolumn{1}{c|}{0.4349}                       & 1.128                        & 0.6746                        & 0.4898                        & \multicolumn{1}{c|}{0.6871}                       & 0.541                        \\ \cline{2-11} 
                         & \multirow{4}{*}{VQA}     & VIIDEO\cite{B:Viideo}                  & 0.3946                        & 0.2651                        & \multicolumn{1}{c|}{0.4903}                       & 8.047                        & 0.2843                        & 0.1885                        & \multicolumn{1}{c|}{0.3228}                       & 5.624                        \\
                         &                          & V-BLIINDS\cite{B:Vblind}               & 0.7389                        & 0.5456                        & \multicolumn{1}{c|}{0.7244}                       & 45.625                       & 0.7510                         & 0.5755                        & \multicolumn{1}{c|}{0.7653}                       & 42.755                       \\
                         &                          & Resnet50\cite{B:Resnet}                & 0.5707                        & 0.4113                        & \multicolumn{1}{c|}{0.5635}                       & 0.381                        & 0.4278                        & 0.2995                        & \multicolumn{1}{c|}{0.4317}                       & 0.307                        \\
                         &                          & FAST-VQA\cite{B:FastVQA}                & 0.7391                        & 0.5500                        & \multicolumn{1}{c|}{0.7710}                       & 0.246                        & 0.5137                        & 0.3644                        & \multicolumn{1}{c|}{0.5748}                       & 0.232                        \\ \hline
\multirow{5}{*}{QoS}     & \multirow{2}{*}{Client}    & FTW\cite{A:FTW}                     & 0.1835                        & 0.1337                        & \multicolumn{1}{c|}{0.3229}                       & 0.001                        & 0.0804                        & 0.0858                        & \multicolumn{1}{c|}{0.0648}                       & 0.001                        \\
                         &                          & MoK2011\cite{A:Mok2011}                 & 0.1687                        & 0.1294                        & \multicolumn{1}{c|}{0.2156}                       & 0.001                        & 0.0795                        & 0.0650                        & \multicolumn{1}{c|}{0.0874}                       & 0.001                        \\ \cline{2-11} 
                         & \multirow{3}{*}{Network} & Liu2012\cite{A:Liu2012}                 & 0.2529                        & 0.1717                        & \multicolumn{1}{c|}{0.2424}                       & 0.001                        & 0.6633                        & 0.4684                        & \multicolumn{1}{c|}{0.6366}                       & 0.001                        \\
                         &                          & Xue2014\cite{A:Xue2014}                 & 0.3412                        & 0.2245                        & \multicolumn{1}{c|}{0.3081}                       & 0.003                        & 0.5830                        & 0.4123                        & \multicolumn{1}{c|}{0.4961}                       & 0.003                        \\
                         &                          & Yin2015\cite{A:Yin2015}                 & 0.1458                        & 0.0932                        & \multicolumn{1}{c|}{0.3232}                       & 0.007                        & 0.0804                        & 0.0616                        & \multicolumn{1}{c|}{0.0648}                       & 0.007                        \\ \hline
\multirow{5}{*}{Hybrid} & \multirow{5}{*}{Mix}     & SQI\cite{C:SQI}                     & 0.1515                        & 0.1100                        & \multicolumn{1}{c|}{0.2225}                       & 0.501                        & 0.7347                        & 0.5298                        & \multicolumn{1}{c|}{0.6329}                       & 0.458                        \\
                         &                          & TV-QoE\cite{C:TV-QoE}                  & 0.5068                        & 0.3565                        & \multicolumn{1}{c|}{0.4667}                       & 0.524                        & 0.6686                        & 0.4136                        & \multicolumn{1}{c|}{0.5109}                       & 0.482                        \\
                         &                          & Bentaleb2016\cite{C:Bentaleb2016}            & 0.1979                        & 0.1387                        & \multicolumn{1}{c|}{0.3405}                       & 0.498                        & 0.4454                        & 0.2982                        & \multicolumn{1}{c|}{0.4530}                       & 0.456                        \\
                         &                          & KSQI\cite{C:KSQI}                    & 0.5285                        & 0.3875                        & \multicolumn{1}{c|}{0.5268}                       & 0.505                        & 0.7394                        & 0.5492                        & \multicolumn{1}{c|}{0.7315}                       & 0.462                        \\ \cline{3-11} 
                         &                          & Proposed                & \textbf{0.8627}               & \textbf{0.6871}               & \multicolumn{1}{c|}{\textbf{0.8824}}              & 0.606                        & \textbf{0.7739}               & \textbf{0.5914}               & \multicolumn{1}{c|}{\textbf{0.7898}}              & 0.691                        \\ \hline
\end{tabular}
\vspace*{-5mm}
\end{table*}

\section{Performance Evaluation}
% \subsection{Validated Database}
% \label{database}

\subsection{Experiment Setup}
\label{experimentsetup}

The proposed metric is validated on the Waterloo sQoE \uppercase\expandafter{\romannumeral3} \cite{w3} and the LIVE Netflix \uppercase\expandafter{\romannumeral2} \cite{LIVE2} datasets, which contain various subjectively-rated videos of diverse content types and video codecs, and streamed over various network conditions and ABR algorithms. The subjective study was done using dual-task single-stimulus (SS) experiments with user ratings provided as ground truth. The dataset is split randomly in an 80/20 ratio for training/testing while ensuring the same video content falls into the same set \cite{B:Rapique}.
% We randomly divide 80\% of the dataset for training and the remaining 20\% for testing, with same video content divided into the same set \cite{B:Rapique}. 
The partitioning and evaluation process is repeated 1,000 times for a fair comparison, and the average result is reported as the final performance.

%which records not only quality deterioration events continuously while participants are viewing test stimulus, but also retrospective scores at the end of the presentation. The two tasks can assist each other, allowing users to give more accurate QoE scores at the end of the video. 
%
% \subsection{Experiment Setting}
%
We evaluate our metric in the following ways: First, to evaluate its consistency with HVS, we use three common correlation functions, namely SRoCC, Kendall rank-order correlation coefficient (KRoCC), and Pearson linear correlation coefficient (PLCC), to measure how well our metric correlates with the subjective scores. Second, to evaluate its latency, we measure the computation time of our metric on a laptop with an i7-8750H CPU, which is a common client specification for HAS services \cite{cisco2020cisco}. %If the ratio of QoE calculation time to segment playback time is less than 1, this model can realize real-time feedback while the video is being played, which can guide the ABR module to encode on the server. 
Third, since this is a blind assessment scenario, we compare our metric to 18 mainstream blind QoE assessment metrics as baselines. 
% proposed between 2010--2022, including models that use NR kernel. 
For model training, we left the parameters of ResNet-50 unchanged (which has been pre-trained for image classification on ImageNet \cite{imagenet}), and update the other parameters using the Adam optimizer \cite{adam}. The other learning-based methods are trained with similar mechanisms for a fair comparison. In the IQA models, we evaluated three different sampling rates by uniformly sampling every 5, 10, and 20 frames; in the hybrid models, as their original SSIM feature is not attainable for the NR task, we uniformly sample every 20 frames and apply the widely used Brisque~\cite{B:Brisque} metric in their content-based features.
% in the hybrid model, we uniformly use the widely-used brisque \cite{B:Brisque} metric every 30 frames to represent perceptual quality.

\subsection{Experiment Results and Discussion}

Table \ref{performance} and Fig.~\ref{fig:vis} show the performance results of the baseline and proposed methods, %where a larger SRoCC/KRoCC/PLCC indicates a higher correlation between the metric and subjective scores (and higher consistency with HVS). 
where a larger correlation factor indicates a higher consistency with the HVS.
Computation time is calculated as a ratio of the video duration, where a smaller ratio indicates lower complexity and a value below 1 is needed to meet the real-time requirement.
% A smaller ratio of QoE computing time to video playback time indicates a lower complexity, which needs to be less than 1 to meet the real-time requirements. 
Among the content-based metrics, V-BLIINDS~\cite{B:Vblind} has the best prediction performance but requires a computation time that is more than 40$\times$ longer than the video itself, resulting in its inability to provide real-time feedback for bitrate guidance.
% Those content-based metrics have better prediction performance, but to ensure this performance, it requires IQA to sample the video intensively, and make the prediction time even \textbf{40+} times longer than the segment, resulting in its inability to provide real-time coding guidance. 
The QoS-based metrics have much faster computation times of less than 1\% of the video duration, but the predicted QoE has a relatively poor correlation with subjective scores.
% QoS-based metrics can give immediate feedback for ABR consuming less than \textbf{1\%} time of the video duration, but the predicted QoE quality hardly correlates with the subjective quality. 
%
Most of the hybrid models achieve a better balance between consistency and latency. However, they are generally designed for FR features but their prediction performance becomes less ideal when switched to NR.
% Most of the hybrid models achieve a better balance between consistency and latency, but the rewards are usually represented by FR features, and in the case of TV-QoE \cite{C:TV-QoE}, switching to NR features reduces the SRoCC to about 0.5.
%
% We, therefore, fused content and QoS factors to characterize both favorable/unfavorable experiences in HAS. 
%
Results show that our metric outperforms all QoS-based and hybrid metrics in all three correlation measures for both datasets (with a gain of up to 0.35 against other hybrid metrics), and outperforms all content-based metrics in SRoCC and PLCC for both datasets while keeping the computation time ratio below 1.
% The results show that our metric outperforms all metrics in SRoCC and PLCC for both datasets, and outperforms other hybrid metrics in all three correlation measures for both datasets (with a gain of up to 0.35), while keeping the computation time ratio below 1.
%
% The results also show that our metric outperforms other hybrid metrics by more than 0.35 in correlation and that good real-time performance is still guaranteed. 
{In terms of global QoE metric performance factors\cite{Patrick1}, our Area Under the ROC Curve (AUC) value is about 0.09 ahead of SOTA HAS QoE metric and outperforms all current metrics for correct classification.}
Hence, our metric can overcome the absence of reference information in blind assessment scenarios and still provide effective QoE predictions that have high consistency with HVS (with an average SRoCC of 0.81) and sufficiently low latency (of about 65\% of the video playback duration).
% In summary, our model can overcome the absence of reference video for the blind quality assessment task and give fast and accurate QoE predictions.

\begin{figure}[tbp]
	\centering
	\includegraphics[width=0.45\textwidth]{mapNarrow.pdf}
        \vspace*{-4mm}
	\caption{SRoCC and time ratio performance of the baseline and proposed methods. Metrics with time ratio $<$ 1 (white panel) can realize real-time QoE prediction for bitrate guidance.}
	\label{fig:vis}
        \vspace*{-5mm}
\end{figure}

\vspace*{-1mm}
\subsection{Ablation Study}
\vspace*{-1mm}

To validate the contributions of our sampling method and the different feature types, we also conduct an ablation study and its results are shown in Table \ref{abandon}. The factors are specified as: (1) Non-uniform sampling, (2) Reward QoS features, (3) Reward content features (both spatial and textural), (4) Penalty QoS features, and (5) Penalty content features.
% Our unique sampling mechanism is shown as (1) Non-uniform Sampling. Specifically, the extracted features are categorized into four groups: (2) QoS Reward, (3) Content Reward, (4) QoS Penalty, and (5) Content penalty.
The results show that removing any single factor leads to performance degradation, which confirms that they all contribute to the performance results in Table \ref{performance}. 
{The time cost for each group can ensure a real-time assessment.}
Ablation results also show that QoS feature groups (2)(4) are more effective than the content feature groups (3)(5), and a combination of them achieves desirable performance,
while the sampling method (1) further enhances the model's performance with little extra time cost.

\begin{table}[t]
\vspace*{-3mm}
\centering
\small
\caption{Performance results of abandoning different features on the Waterloo sQoE \uppercase\expandafter{\romannumeral3} dataset.}
\label{abandon}
\begin{tabular}{|c|ccc|c|}
\hline
Abandoned & SRoCC & KRoCC & PLCC & Time \\ \hline
None & \textbf{0.8627} & \textbf{0.6871} & \textbf{0.8824} & 0.606 \\
(1) & 0.8514 & 0.6716 & 0.8694 & 0.605 \\
(2) & 0.4521 & 0.3270 & 0.4761 & 0.606 \\
{(3) $r_2 \sim r_5$} & {0.7662} & {0.5822} & {0.8194} & {0.381} \\
{(3) $r_6$} & {0.8468} & {0.6680} & {0.8637} & {0.441} \\
(3) All & 0.7313 & 0.5478 & 0.7490 & 0.216 \\
(4) & 0.8319 & 0.6544 & 0.8691 & 0.606 \\
(5) & 0.8434 & 0.6767 & 0.8639 & 0.390 \\
(2)(4) & 0.3204 & 0.2214 & 0.4006 & 0.605 \\
(3)(5) & 0.7010 & 0.5215 & 0.7073 & 0.001 \\ \hline
\end{tabular}
\vspace*{-4mm}
\end{table}

% The QoS feature groups (2)(4) serve as major QoE factors while QoE factors (3)(5) can further increase the model's performance. Non-uniform sampling can increase the model's performance with no extra time cost. In all, the feature extraction mechanism enables our method to have a more comprehensive assessment of QoE for clients.

\vspace*{-1mm}
\section{Conclusions}
\vspace*{-2mm}

In this study, we target the challenge of measuring perceptual quality in HAS clients for bitrate guidance.
% we analyzed the perception mechanism for HAS on the client side. 
A blind QoE assessment metric is proposed to provide QoE feedback of high consistency with HVS, at low latency, and without the use of reference information. Specifically, we map QoS and video content information to both reward and penalty features and include a non-uniform sampling mechanism to identify relevant frames for analysis.
% helps us to analyze certain important images instead of the whole video. 
Experiments show that the proposed metric achieves the best results in {two databases across three correlation measures}, which suggests strong consistency with HVS, while meeting latency and blind assessment requirements.
% is more in line with HVS perceptual mechanism for the video segments in HAS. 
This metric is suited to perform real-time QoE assessment on the client side, which can help improve the overall resource utilization of HAS services. 

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
% May: To abbreviate first names: IEEEbib -> IEEEbib_custom
% \bibliographystyle{IEEEbib}
% \newpage
{\small
\vspace*{-2mm}
\begin{thebibliography}{10}
	
	\bibitem{cisco2020cisco}
	Cisco,
	\newblock ``{Cisco Visual Networking Index: Forecast and Trends, 2018--2023},''
	\newblock {\em White Paper}, 2020.
	
	\bibitem{bentaleb2018survey}
	A. Bentaleb, B. Taani, A.~C. Begen, C. Timmerer, and R. Zimmermann,
	\newblock ``{A Survey on Bitrate Adaptation Schemes for Streaming Media Over
		HTTP},''
	\newblock {\em IEEE Communications Surveys \& Tutorials}.
	
	\bibitem{C:KSQI}
	Z. Duanmu, W. Liu, D. Chen, Z. Li, Z. Wang, et~al.,
	\newblock ``A knowledge-driven quality-of-experience model for adaptive
	streaming videos,''
	\newblock {\em arXiv preprint arXiv:1911.07944}, 2019.
	
	\bibitem{A:FTW}
	T. Ho{\ss}feld, R. Schatz, E. Biersack, and L. Plissonneau,
	\newblock ``Internet video delivery in {YouTube}: From traffic measurements to
	quality of experience,''
	\newblock in {\em Data Traffic Monitoring and Analysis}. Springer, 2013.
	
	\bibitem{A:Yin2015}
	X. Yin, A. Jindal, V. Sekar, and B. Sinopoli,
	\newblock ``A control-theoretic approach for dynamic adaptive video streaming
	over {HTTP},''
	\newblock in {\em ACM SIGCOMM}, 2015.
	
	\bibitem{B:Brisque}
	A. Mittal, A.~K. Moorthy, and A.~C. Bovik,
	\newblock ``No-reference image quality assessment in the spatial domain,''
	\newblock {\em IEEE TIP}, 2012.
	
	\bibitem{B:Viideo}
	A. Mittal, M.~A. Saad, and A.~C. Bovik,
	\newblock ``A completely blind video integrity oracle,''
	\newblock {\em IEEE TIP}, 2015.
	
	\bibitem{Zhai2020PerceptualIQ}
	G. Zhai and X. Min,
	\newblock ``Perceptual image quality assessment: a survey,''
	\newblock {\em Science China Information Sciences}, 2020.
	
	\bibitem{NR:Series}
	X. Sui, K. Ma, Y. Yao, and Y. Fang,
	\newblock ``Perceptual quality assessment of omnidirectional images as moving
	camera videos,''
	\newblock {\em IEEE TVCG}, 2021.
	
	\bibitem{B:Rapique}
	Z. Tu, X. Yu, Y. Wang, N. Birkbeck, B. Adsumilli, and A.~C. Bovik,
	\newblock ``{RAPIQUE}: Rapid and accurate video quality prediction of user
	generated content,''
	\newblock {\em IEEE OJSP}, 2021.
	
	\bibitem{B:FastVQA}
	H. Wu, C. Chen, J. Hou, L. Liao, A. Wang, W. Sun, Q. Yan, and W. Lin,
	\newblock ``{FAST-VQA}: Efficient end-to-end video quality assessment with
	fragment sampling,''
	\newblock {\em ECCV}, 2022.
	
	\bibitem{C:Bentaleb2016}
	A. Bentaleb, A.~C. Begen, and R. Zimmermann,
	\newblock ``{SDNDASH: Improving QoE of HTTP adaptive streaming using software
		defined networking},''
	\newblock in {\em ACM Multimedia}, 2016.
	
	\bibitem{C:SQI}
	Z. Duanmu, K. Zeng, K. Ma, A. Rehman, and Z. Wang,
	\newblock ``A quality-of-experience index for streaming video,''
	\newblock {\em IEEE Journal of Selected Topics in Signal Processing}, 2016.
	
	\bibitem{C:TV-QoE}
	D. Ghadiyaram, J. Pan, and A.~C. Bovik,
	\newblock ``Learning a continuous-time streaming video {QoE} model,''
	\newblock {\em IEEE TIP}, 2018.
	
	\bibitem{w3}
	Z. Duanmu, A. Rehman, and Z. Wang,
	\newblock ``A quality-of-experience database for adaptive video streaming,''
	\newblock {\em IEEE Transactions on Broadcasting}, 2018.
	
	\bibitem{HAScoding}
	H. Amirpour, E. Çetinkaya, C. Timmerer, and M. Ghanbari,
	\newblock ``Fast multi-rate encoding for adaptive http streaming,''
	\newblock in {\em Data Compression Conference (DCC)}, 2020.
	
	\bibitem{B:Niqe}
	A. Mittal, R. Soundararajan, and A.~C. Bovik,
	\newblock ``Making a “completely blind” image quality analyzer,''
	\newblock {\em IEEE Signal Processing Letters}, 2012.
	
	\bibitem{B:Piqe}
	N. Venkatanath, D. Praneeth, M.~C. Bh, S.~S. Channappayya, and S.~S. Medasani,
	\newblock ``Blind image quality evaluation using perception based features,''
	\newblock in {\em IEEE NCC}, 2015.
	
	\bibitem{Structural1}
	Q. Li, W. Lin, J. Xu, and Y. Fang,
	\newblock ``Blind image quality assessment using statistical structural and
	luminance features,''
	\newblock {\em IEEE TMM}, 2016.
	
	\bibitem{Qaware}
	J. Kim, H. Zeng, D. Ghadiyaram, S. Lee, L. Zhang, and A.~C. Bovik,
	\newblock ``Deep convolutional neural models for picture-quality prediction:
	Challenges and solutions to data-driven image quality assessment,''
	\newblock {\em IEEE Signal Process Mag}, 2017.
	
	\bibitem{GRUfang}
	J. Yan, J. Li, Y. Fang, Z. Che, X. Xia, and Y. Liu,
	\newblock ``Subjective and objective quality of experience of free viewpoint
	videos,''
	\newblock {\em IEEE TIP}, 2022.
	
	\bibitem{operator}
	M. Sonka, V. Hlavac, and R. Boyle,
	\newblock {\em Image processing, analysis, and machine vision},
	\newblock Cengage Learning, 2014.
	
	\bibitem{Patrick4}
	X. Min, J. Zhou, G. Zhai, P. Le~Callet, X. Yang, and X. Guan,
	\newblock ``A metric for light field reconstruction, compression, and display
	quality evaluation,''
	\newblock {\em IEEE TIP}, 2020.
	
	\bibitem{rebuffering1}
	C.~G. Bampis and A.~C. Bovik,
	\newblock ``Learning to predict streaming video qoe: Distortions, rebuffering
	and memory,''
	\newblock {\em arXiv preprint arXiv:1703.00633}, 2017.
	
	\bibitem{A:Mok2011}
	R.~K. Mok, X. Luo, E.~W. Chan, and R.~K. Chang,
	\newblock ``{QDASH: a QoE-aware DASH system},''
	\newblock in {\em ACM MMSys}, 2012.
	
	\bibitem{rebuffering2}
	Tisa-Selma, A. Bentaleb, and S. Harous,
	\newblock ``Video qoe inference with machine learning,''
	\newblock in {\em IEEE IWCMC}, 2021.
	
	\bibitem{Stalling}
	B. Taraghi, M. Nguyen, H. Amirpour, and C. Timmerer,
	\newblock ``Intense: In-depth studies on stall events and quality switches and
	their impact on the quality of experience in http adaptive streaming,''
	\newblock {\em IEEE Access}, 2021.
	
	\bibitem{fastSSIM}
	M.-J. Chen and A.~C. Bovik,
	\newblock ``Fast structural similarity index algorithm,''
	\newblock {\em Journal of Real-Time Image Processing}, 2011.
	
	\bibitem{libsvm}
	C.-C. Chang and C.-J. Lin,
	\newblock ``Libsvm: a library for support vector machines,''
	\newblock {\em ACM TIST}, 2011.
	
	\bibitem{Patrick2}
	L. Krasula, K. Fliegel, and P. Le~Callet,
	\newblock ``Fftmi: Features fusion for natural tone-mapped images quality
	evaluation,''
	\newblock {\em IEEE TMM}, 2020.
	
	\bibitem{B:Vblind}
	M.~A. Saad, A.~C. Bovik, and C. Charrier,
	\newblock ``Blind prediction of natural video quality,''
	\newblock {\em IEEE TIP}, 2014.
	
	\bibitem{B:Resnet}
	K. He, X. Zhang, S. Ren, and J. Sun,
	\newblock ``Deep residual learning for image recognition,''
	\newblock in {\em IEEE CVPR}, 2016.
	
	\bibitem{A:Liu2012}
	X. Liu, F. Dobrian, H. Milner, J. Jiang, V. Sekar, I. Stoica, and H. Zhang,
	\newblock ``A case for a coordinated internet video control plane,''
	\newblock in {\em ACM SIGCOMM}, 2012.
	
	\bibitem{A:Xue2014}
	J. Xue, D.-Q. Zhang, H. Yu, and C.~W. Chen,
	\newblock ``Assessing quality of experience for adaptive {HTTP} video
	streaming,''
	\newblock in {\em IEEE ICME Workshops}, 2014.
	
	\bibitem{LIVE2}
	C.~G. Bampis, Z. Li, I. Katsavounidis, T.-Y. Huang, C. Ekanadham, and A.~C.
	Bovik,
	\newblock ``Towards perceptually optimized end-to-end adaptive video
	streaming,''
	\newblock {\em arXiv preprint arXiv:1808.03898}, 2018.
	
	\bibitem{imagenet}
	O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A.
	Karpathy, A. Khosla, M. Bernstein, et~al.,
	\newblock ``Imagenet large scale visual recognition challenge,''
	\newblock {\em IJCV}, 2015.
	
	\bibitem{adam}
	D.~P. Kingma and J. Ba,
	\newblock ``Adam: A method for stochastic optimization,''
	\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.
	
	\bibitem{Patrick1}
	L. Krasula, K. Fliegel, P. Le~Callet, and M. Klíma,
	\newblock ``On the accuracy of objective image and video quality models: New
	methodology for performance evaluation,''
	\newblock in {\em QoMEX}, 2016.
	
\end{thebibliography}
}

\end{document}
