\documentclass[11pt]{article}
% smile package defines theorems, so to use cleveref we need to first load amsthm (which is needed before cleveref is loaded, and because cleveref must be loaded before the theorem definitions happen)
\usepackage{amsthm,amsmath} % amsmath must also be loaded before cleveref
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=blue]{hyperref} % hyperref also need to be loaded before cleveref
\usepackage[capitalize]{cleveref} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{math_commands}

\usepackage{extarrows}
\usepackage[OT1]{fontenc}
\usepackage{fullpage}
\usepackage[protrusion=false,expansion=true]{microtype}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{stmaryrd}
%\usepackage[dvipsnames]{xcolor}
%\RequirePackage{times}
\RequirePackage{natbib}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\tv}{\text{vec}}
\usepackage{authblk}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\purple{\color{purple}}
\def\botao{\color{red}}

\begin{document}
\title{ \bf Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale}
\author[1]{Botao Hao}
\author[2]{Rahul Jain}
\author[2]{Dengwang Tang}
\author[1]{Zheng Wen \thanks{Alphabetical order. Corresponding to Rahul Jain: ahul.jain@usc.edu.}}
\affil[1]{Deepmind}
\affil[2]{University of Southern California}
\maketitle
\begin{abstract}
In this paper, we address the following problem: Given an offline demonstration dataset from an imperfect expert, what is the best way to leverage it to bootstrap online learning performance in MDPs. We first propose an Informed Posterior Sampling-based RL (iPSRL) algorithm that uses the offline dataset, and information about the expert's behavioral policy used to generate the offline dataset. Its cumulative Bayesian regret goes down to zero exponentially fast in $N$, the offline dataset size if the expert is competent enough. Since this algorithm is computationally impractical, we then propose the iRLSVI~algorithm that can be seen as a combination of the RLSVI~algorithm for online RL, and imitation learning. Our empirical results show that the proposed iRLSVI~algorithm is able to achieve significant reduction in regret as compared to two baselines: no offline data, and offline dataset but used without information about the generative policy.
Our algorithm bridges online RL and imitation learning for the first time. 
\end{abstract}



\section{Introduction}
\label{sec:intro}

An early vision of the Reinforcement Learning (RL) field is to design a learning agent that when let loose in an unknown environment, learns by interacting with it. Such an agent starts with a blank slate (with possibly, arbitrary initialization), takes actions, receives state and reward observations, and thus learns by ``reinforcement''. This remains a goal but at the same time, it is recognized that in this paradigm  learning is too slow, inefficient and often impractical. Such a learning agent takes too long to learn near-optimal policies way beyond practical time horizons of interest. Furthermore, deploying an agent that learns by exploration over long time periods may simply be impractical.

%In contrast, the field of Imitation Learning has the goal of learning the ``optimal" policy from demonstration data from it. Unfortunately, imitation learning remains impractical on large state space problems, and often suffers from the \textit{sim2real} problem in practice, i.e., the learnt policy upon deployment performs poorly on out-of-distribution state-action space. Thus, often there is a need for adaptation and fine-tuning upon deployment. 

In fact, reinforcement learning is often deployed to solve complicated engineering problems by first collecting  offline data using a behavioral policy, and then using off-policy reinforcement learning, or imitation learning methods (if the goal is to imitate the policy that generated the offline dataset)   on such datasets to learn a policy. This often suffers from the \textit{sim2real} problem, i.e., the learnt policy upon deployment often performs poorly on out-of-distribution state-action space. Thus, there is a need for adaptation and fine-tuning upon deployment.

In this paper, we propose a systematic way to use offline datasets to bootstrap online RL algorithms. Performance of online learning agents is often measured in terms of cumulative (expected) regret. We show that, as expected, there is a gain in performance (reflected in reduction in cumulative regret) of the learning agent as compared to when it did not use such an offline dataset. We call such an online learning agent as being \textit{partially informed}. However, somewhat surprisingly, if the  agent is further  informed about the behavioral policy that generated the offline dataset, such an \textit{informed (online learning) agent}  can do substantially better, reducing cumulative regret significantly. In fact, we also show that if the behavioral policy is suitably parameterized by a \textit{competence parameter}, wherein the behavioral policy is asymptotically the optimal policy, then the higher the ``competence'' level, the better the performance in terms of regret reduction over the baseline case of no offline dataset.

We first propose an ideal (informed) $\ipsrl$~(posterior sampling-based RL) algorithm and show via theoretical analysis that under some mild assumptions, its expected cumulative regret is bounded as $\tilde O(\sqrt{T})$ where $T$ is the number of episodes. In fact, we show that if the competence of the expert is high enough (quantified in terms of a parameter we introduce), the regret goes to zero exponentially fast as $N$, the offline dataset size grows. This is accomplished through a novel prior-dependent regret analysis of the \psrl~algorithm, the first such result to the best of our knowledge. Unfortunately, posterior updates in this algorithm can be computationally impractical. Thus, we introduce a Bayesian-bootstrapped algorithm for approximate posterior sampling, called the (informed) $\irlsvi$~algorithm (due to its commonality with the \rlsvi~algorithm introduced in \cite{osband2019deep}). The $\irlsvi$~algorithm involves optimizing a loss function that is an \textit{optimistic} upper bound on the loss function for MAP estimates for the unknown parameters. Thus, while inspired by the posterior sampling principle, it also has an optimism flavor to it. Through, numerical experiments, we show that the $\irlsvi$~algorithm performs substantially better than both the partially informed-\rlsvi~(which uses the offline dataset naively) as well as the uninformed-\rlsvi~algorithm (which doesn't use it at all). 

We also show that the $\irlsvi$~algorithm can be seen as bridging online reinforcement learning with imitation learning since its loss function can be seen as a combination of an online learning term as well as an imitation learning term. And if there is no offline dataset, it essentially behaves like an online RL algorithm. Of course, in various regimes in the middle it is able to interpolate seamlessly. We note that this is the first algorithm of its kind. 

%In the rest of the paper, we start with preliminaries, develop the algorithmic framework, establish theoretical regret bounds on an ideal version of the algorithm, then propose systematic approximations for scalability, and then end with empirical evidence via experimental results on Deep Sea.
%Montezuma's revenge and Ms. Pacman environments.
%\zheng{we have only done experiment in deep sea}

\textbf{Related Work.} Because of the surging use of offline datasets for pre-training (e.g., in Large Language models (LLMs), e.g., see \cite{brown2020language,thoppilan2022lamda,hoffmann2022training}), there has been a lot of interest in Offline RL, i.e., RL using offline datasets \citep{levine2020offline}. A fundamental issue this literature addresses is RL algorithm design \citep{nair2020awac,kostrikov2021offline,kumar2020conservative,nguyenprovably,fujimoto2019off,fujimoto2021minimalist,ghosh2022offline} and analysis to best address the ``out-of-distribution'' (OOD) problem, i.e., policies learnt from offline datasets may not perform so well upon deployment. The dominant design approach is based on `pessimism' \citep{jin2021pessimism,xie2021bellman,rashidinejad2021bridging} which often results in conservative performance in practice. Some of the theoretical literature  \citep{xie2021bellman,rashidinejad2021bridging,uehara2021pessimistic,agarwal2022model} has focused on investigation of sufficient conditions such as ``concentrability measures'' under which such offline RL algorithms can have  guaranteed performance. Unfortunately, such measures of offline dataset quality are hard to compute, and of limited practical relevance \citep{argenson2020model,nair2020awac,kumar2020conservative,levine2020offline,kostrikov2021offline,wagenmaker2022leveraging}.

There is of course, a large body of literature on online RL \citep{dann2021provably,tiapkin2022optimistic,ecoffet2021first,guo2022byol,ecoffet2019go,osband2019deep} with two dominant design philosophies:  Optimism-based algorithms such as UCRL2 in \cite{auer2008near}, and Posterior Sampling (PS)-type algorithms such as PSRL~\citep{osband2013more,ouyang2017learning}, etc.  \citep{osband2019deep,osband2016deep,russo2018learning,zanette2017information,hao2022regret}. However, none of these algorithms consider starting the learning agent with an offline dataset. Of course, imitation learning \citep{hester2018deep,beliaev2022imitation,schaal1996learning} is exactly concerned with learning the expert's behavioral policy (which may not be optimal) from the offline datasets but with no online finetuning of the policy learnt.  Several  papers have actually studied bridging offline RL and imitation learning \citep{ernst2005tree,kumar2022should,rashidinejad2021bridging,hansen2022modem,vecerik2017leveraging,lee2022offline}. Some have also studied offline RL followed by a small amount of policy fine-tuning \citep{song2022hybrid,fang2022planning,xie2021policy,wan2022safe,schrittwieser2021online,ball2023efficient,uehara2021pessimistic,xie2021policy,agarwal2022model} with the goal of finding policies that optimize simple regret. 

But none have studied the problem we introduce and study in this paper: Namely, given an offline demonstration dataset from an imperfect expert, what is the best way to leverage it to bootstrap online learning performance (in terms of cumulative regret) in MDPs. What is the best regret reduction that is achievable by use of offline datasets? How it depends on the quality and quantity of demonstrations, and what algorithms can one devise to achieve them? And does any information about the offline-dataset generation process help in regret reduction? We answer some of these questions in this paper.



\section{Preliminaries}
\label{sec:prelims}

\paragraph{Episodic Reinforcement Learning.}  Consider a scenario where an agent repeatedly interacts with an environment modelled as a finite-horizon MDP, and refer to each interaction as an episode. The finite-horizon MDP is represented by a tuple ${\mathcal M} = (\cS, \cA, P, r, H, \nu)$, where $\cS$ is a finite state space (of size $S$), $\cA$ is a finite action space (of size $A$), $P$ encodes the transition probabilities, $r$ is the reward function, $H$ is the time horizon length, and $\nu$ is the initial state distribution. The interaction protocol is as follows: at the beginning of each episode $t$, the initial state $s^t_0$ is independently drawn from $\nu$. Then, at each period $h=0,1, \ldots, H-1$ in episode $t$, if the agent takes action $a^t_h \in \cA$ at the current state $s^t_h \in \cS$, then it will receive a reward $r_h(s^t_h, a^t_h)$ and transit to the next state $s^t_{h+1} \in  P_h(\cdot | s^t_h, a^t_h)$. An episode terminates once the agent arrives at state $s^t_H$ in period $H$ and receives a reward $r_H(s^t_H)$. We abuse notation for the sake of simplicity, and just use $r_H(s^t_H, a^t_H)$ instead of $r_H(s^t_H)$, though no action is taken at period $H$. The objective is to maximize its expected total reward over $T$ episodes.
%
% which denotes its state space, its action space, the transition probabilities, the reward function and the time horizon length respectively, and $\nu$ is the initial state distribution which we designate by $s_0$. We will consider the transition probabilities $P$ and the initial state $s_0$ to be random. We take a Bayesian perspective, and assume all unknown quantities are actually random variables defined on a common probability space $(\Omega, \mathcal{F}, \mathbb{P})$. 

%Bellman optimality eq. for discounted MDPs: \begin{equation} Q^*(s,a; \theta) = r(s,a) + \gamma \sum_{s'}\theta (s'|s,a)V^*(s';\theta), \label{eq:bellman-exact} \end{equation}
% with $\pi^*(a|s; \theta) = \arg \max_{\pi} \sum_a Q^*(s,a; \theta)\pi(a|s)$ and $V^*(s';\theta) := \max_b Q^ *(s',b; \theta).$ 

Let $Q^*_h$ and $V^*_h$ respectively denote the optimal state-action value and state value functions at period $h$. Then, the Bellman equation for MDP ${\mathcal M}$ is
%
% Let $Q^*_h(s,a)$ and $V^*_h(s)$ denote the optimal state-action and state value functions at time $h$ respectively. Then, the Bellman optimality equation for episodic MDPs is given by
\begin{equation}
Q_{h}^*(s,a) = r_h(s,a) +  \sum_{s'}P_h (s'|s,a)V_{h+1}^*(s'),
\label{eq:bellman-exact}
\end{equation}
where $V_{h+1}^*(s') := \max_b Q_{h+1}^*(s',b)$, if $h< H-1$ and $V^*_{h+1}(s')=0$, if $h=H-1$. We define a policy $\pi$ as a mapping from a state-period pair to a probability distribution over the action space $A$.  
A policy $\pi^*$ is optimal if $\pi_h^*(\cdot|s) \in \arg \max_{\pi_h} \sum_a Q_h^*(s,a)\pi_h(a|s)$ for all $s \in \cS$ and all $h$.

% Note that in general a policy at time $h$ is the following map $\pi_h: S \to \Delta_A$, from the state space to the probability simplex over the action space. Above, we abuse notation a bit for the sake of simple notation, and just use $r_H(s_H,A_H)$ instead of $r_H(s_H)$, the terminal reward where no further action $a_H$ is taken.

\paragraph{Agent's Prior Knowledge about ${\mathcal M}$.} 
We assume that the agent does not fully know the environment ${\mathcal M}$; otherwise, there is no need for learning and this problem reduces to an optimization problem. However, the agent usually has some prior knowledge about the unknown part of ${\mathcal M}$. For instance, the agent might know that ${\mathcal M}$ lies in a low-dimensional subspace, and/or have a prior distribution over ${\mathcal M}$. We use the notation ${\mathcal M}(\theta)$ where $\theta$ parameterizes the unknown part of the MDP. When we want to emphasize it as a random quantity, we will denote it by $\theta^*$. 
Of course, different assumptions about the agent's prior knowledge lead to different problem formulations and algorithm designs. As a first step, we consider two canonical settings:
\begin{itemize}
    \item \textbf{Tabular RL:} The agent knows $\cS, \cA, r, H$ and $\nu$, but does not know $P$. That is, $\theta^*=P$ in this setting. We also assume that the agent has a prior over $P$, and this prior is independent across state-period-action triples. 
    \item \textbf{Linear value function generalization:} The agent knows $\cS, \cA, H$ and $\nu$, but does not know $P$ and $r$. Moreover, the agent knows that for all $h$, $Q^*_h$ lies in a low-dimensional subspace $\mathrm{span}(\Phi_h)$, where $\Phi_h \in \Re^{|S||A| \times d}$ is a known matrix. In other words, $Q_h^* = \Phi_h \theta^*_h$ for some $\theta^*_h \in \Re^d$. Thus, in this setting $\theta^*=\left[\theta^{* \top}_0, \ldots, \theta^{* \top}_{H-1} \right]^\top$. We also assume that the agent has a Gaussian prior over $\theta^*$.
\end{itemize}
As we will discuss later, the insights developed in this paper could potentially be extended to more general cases.
%\zheng{I use $\theta^*$ to denote the environment parameters, and reserve $\theta$ to denote the algorithm parameters. Also, it is not clear to me if we will have any results for the value function generalization case.} 

% While our results (e.g., theoretical analysis) are presented for the tabular case, we will consider a parameterization of the $Q$-value function to allow for model-free learning in both tabular and non-tabular settings. Thus, we will use the following notation to indicate this parameterization: $Q_h(s,a; \theta)$ and $V_h(s ; \theta)$, with $\theta \in \Theta$. \botao{We need to specify the parameterization form?}\rahul{not necessary I think - the algorithm later holds for any.}  Note that the parameter $\theta$ is unknown, and taking a Bayesian perspective, we assume a prior distribution on it, for example, $f(\theta) = \cN(\bar{\theta},\Sigma_0)$ when appropriate though this is not crucial. 
% \botao{What's the point to assume this?}
% We assume that the parameter set $\Theta$ is sufficient for realizability \botao{What does this mean?}, though the algorithm and most of the arguments in Section \ref{sec:approx-iPSRL} will still go through even when this does not hold. 

\paragraph{Offline Datasets.} We denote an \textit{offline dataset} with $L$ episodes as $\mathcal{D}_0 = \{(\bar{s}_0^l,\bar{a}_0^l,\cdots,\bar{s}_H^l)_{l=0}^{L-1}\}$, where  $N=HL$ denotes the dataset size in terms of number of observed transitions. For the sake of simplicity, we assume we have complete trajectories in the dataset but it can easily be generalized if not. We denote an  \textit{online dataset} with $t$ episodes as
$\mathcal{H}_t = \{(s_{0}^l,a_{0}^l,\cdots,s_{H}^l)_{l=0}^{t}\}$ and 
$\mathcal{D}_t = \mathcal{D}_0 \oplus \mathcal{H}_t$. 
%\zheng{I think we should use ``concatenation" rather than ``union", ``union" is not quite correct when there is duplicated data.}

\paragraph{The Notion of Regret.}

A online learning algorithm $\phi$ is a map for each episode $t$, and time $h$, $\phi_{t,h}: \mathcal{D}_t \to \Delta_A$, the probability simplex over actions.  We define the Bayesian regret of an online learning algorithm $\phi$ over $T$ episodes as 
\begin{equation*}
\BR_T(\phi) :=  \mathbb E\left[\sum_{t=1}^T  \left(V^*_0(s_0^t; \theta^*) - \sum_{h=0}^{H} r_h(s_h^t,a_h^t)\right)\right]\,,
\end{equation*}
where the $(s_h^t,a_h^t)$'s are the state-action tuples from using the learning algorithm $\phi$, and the expectation is over the sequence induced by the interaction of the learning algorithm and the environment, the prior distributions over the unknown parameters $\theta^*$ and the offline dataset $\cD_0$.  

\paragraph{Expert's behavioral policy and competence.} We assume that the expert that generated the offline demonstrations may not be perfect, i.e., the actions it takes are only approximately optimal with respect to the optimal $Q$-value function. To that end, we model the expert's policy by use of the following generative model,
\begin{equation}
\pi_h^{\beta}(a|s) = \frac{\exp(\beta(s) Q_h^*(s,a))}{\sum_a \exp(\beta(s) Q_h^*(s,a))},
\label{eq:expert}    
\end{equation}
where $\beta(s) \geq 0$ is called the \textit{state-dependent deliberateness} parameter, e.g., when $\beta(s) = 0$, the expert behaves naively in state $s$, and takes actions uniformly randomly. When $\beta(s) \to \infty$, the expert uses the optimal  policy when in state $s$. When $\beta(\cdot)$ is unknown, we will assume an independent exponential prior for the sake of analytical simplicity, $f_2(\beta(s)) = \lambda_2 \exp (-\lambda_2\beta(s))$ over $\beta(s)$ where $\lambda_2 > 0$ is the same for all $s$. In our experiments, we will regard $\beta(s)$ as being the same for all states, and hence a single parameter. 
% \zheng{We do not use the prior over $\beta(s)$, why should we mention it?}\rahul{actually we do: in obtaining the loss function $\lambda_2\beta$ term is because of this prior.}\zheng{I see, thank you. Will delete this question.}

The above assumes the expert is knowledgeable about $Q^*$. However, it may know it only approximately. To model that, we introduce a \textit{knowledgeability} parameter $\lambda \geq 0$. The expert then knows
$\tilde{Q}$ which is distributed as $\cN(Q^*, \mathbb I / \lambda^2)$ conditioned on $\theta$,  and selects actions according to the softmax policy \eqref{eq:expert}, with the $Q^*$ replaced by $\tilde{Q}$. The two parameters $(\beta,\lambda)$ together will be referred to as the \textit{competence} of the expert.
In this case, we denote the expert's policy as $\pi_h^{\beta, \lambda}$.

\begin{remark}
While the form of the generative policy in eq. \eqref{eq:expert} seems specific, $\pi_h^{\beta}(\cdot|s)$ is a random vector with support over the entire probability simplex. In particular, if one regards $\beta(s)$ and $\tilde{Q}_h(s,\cdot)$ as parameters that parameterize the policy, the softmax policy structure as in  \eqref{eq:expert} is enough to realize any stationary policy. 
\end{remark}

Furthermore, we note that our main objective here is to yield clear and useful insights when information is available to be able to model the expert's behavioral policy with varying competence levels. Other forms of generative policies can also be used including $\epsilon$-optimal policies introduced in \citep{beliaev2022imitation}, and the framework  extended. 

\section{The Informed \psrl~Algorithm}
\label{sec:iPSRL}

%\subsubsection*{Agent 0 (ideal, can compute exact posterior):}
We now introduce a simple \textit{Informed Posterior Sampling-based Reinforcement Learning} ($\ipsrl$) algorithm that naturally uses the offline dataset $\mathcal{D}_0$ and action generation information to construct an informed prior distribution over $\theta^*$. The  realization of  $\theta^*$ is assumed known to the expert (but not the learning agent) with $\tilde{Q}(\cdot,\cdot; \theta^*) = Q(\cdot,\cdot; \theta^*)$,  and $\beta (s) := \beta \geq 0$ (i.e., it is state-invariant) is also known to the expert. Thus, the learning agent's posterior distribution over $\theta^*$ given the offline dataset is,
\begin{equation}
\begin{split}
     &\mathbb P(\theta^* \in \cdot|\mathcal{D}_0) \propto  \mathbb P(\mathcal{D}_0|\theta^* \in \cdot)\mathbb P(\theta^* \in \cdot) \\
    = ~ & \mathbb P(\theta^* \in \cdot) \times \int_{\theta \in \cdot}\prod_{l=0}^{L-1} \prod_{h=0}^{H-1} \theta(\bar{s}_l^{h+1} | \bar{s}_l^h, \bar{a}_l^h)\pi_h^{\beta}(\bar a_{l}^h|\bar s_l^h, \theta)\nu(\bar{s}^0_l) \, d\theta.
    \end{split}
\label{eq:infor_prior}
\end{equation}

A \psrl~agent \citep{osband2013more,ouyang2017learning} takes this as the prior, and then updates the posterior distribution over $\theta^*$ as online observation tuples, $(s_t,a_t,s_t',r_t)$ become available.  Such an agent is really an ideal agent with assumed posterior distribution updates being exact. In practice, this is computationally intractable and we will need to get samples from an approximate posterior distribution, an issue which we will address in the next section.

\subsection{Prior-dependent  Regret Bound}

It is natural to expect some regret reduction if an offline demonstration dataset is available to warm-start the online learning. However, the degree of improvement must depend on the “quality” of demonstrations, for example through the competence parameter $\beta$. Further note that the role of the offline dataset is via the prior distribution the \psrl~algorithm uses. Thus, theoretical analysis involves obtaining a prior-dependent regret bound, which we obtain next.

We use $\mathbb{H}$ to denote Shannon entropy (with the natural logarithm). We start by establishing a Bayesian regret bound of \psrl~algorithm for MDPs with any prior distribution. 
\begin{lemma}\label{lem:regboundlinbandit}
    Let $\nu$ be the prior distribution of $\pi^*$, then the \psrl~algorithm satisfies 
    \begin{equation*}
        \begin{split}
         \BR_T(\phi^{\text{PSRL}})\leq \min&\Big\{H\sqrt{(SA)^H \mathbb{H}(\nu) T/2},\sqrt{S^2A^2H^4T\log(STH)}\Big\}.
        \end{split}
    \end{equation*}
\end{lemma}
The proof can be found in the Appendix. Note that the first part of the above bound reflects the prior effect though $\mathbb{H}(\nu)$ while the second part is prior-independent. This gives us the following corollary for the $\ipsrl$~ algorithm:
\begin{corollary}\label{lem:russovanroy}
    For the $\ipsrl$~ algorithm,
    \begin{equation}
    \begin{split}
           \BR_T(\phi^{\text{iPSRL}}) \leq \min\Big\{ &H\sqrt{(SA)^H \mathbb{H}(\pi^*|\mathcal{D}_0) T /2}, \sqrt{S^2A^2H^4T\log(STH)}\Big\}.
    \end{split}
    \end{equation}
\end{corollary}

\begin{proof}
    Conditioning on $\mathcal{D}_0 = \bar{\cD}_0$, by applying Lemma \ref{lem:regboundlinbandit}, we can obtain
    \begin{equation*}
        \begin{split}
            & \mathbb E\left[\sum_{t=1}^T  \left(V^*_0(s_0^t; \theta^*) - \sum_{h=0}^{H} r_h(s_h^t,a_h^t)\right)|\mathcal{D}_0 = \bar{\cD}_0\right]\leq  H\sqrt{(SA)^H \mathbb{H}(\pi^*|\mathcal{D}_0 = \bar{\cD}_0) T /2}.
        \end{split}
    \end{equation*}
    The corollary then follows by taking expectations on both sides over $\cD_0$ along with the concavity of the square root function. 
\end{proof}

The conditional information $\mathbb{H}(\pi^*|\mathcal{D}_0)$ measures the amount of randomness of $\pi^*$ given $\mathcal{D}_0$. Therefore, Corollary \ref{lem:russovanroy} means that the more certain about $\pi^*$ we are, the less regret the $\ipsrl$~ algorithm will incur. In the rest of the proof, we provide an upper bound for $\mathbb{H}(\pi^*|\mathcal{D}_0)$ by use of Fano's inequality. For positive integer $K$, let $[K] := \{1,2, \ldots, K\}$.

\begin{lemma}[Fano's Inequality]\label{lem:fano}
    Let $Y, \hat{Y}$ be random variables on $[K]$ such that $\Pr(Y \neq \hat{Y}) \leq \varepsilon \leq 1/2$. Then, 
    \begin{equation}
        \mathbb{H}(Y|\hat{Y}) \leq \varepsilon\log K + \mathbf{h}(\varepsilon), 
    \end{equation}
    where $\mathbf{h}(\epsilon) = -\epsilon\log \epsilon - (1-\epsilon)\log(1-\epsilon)$ is the binary entropy function.
\end{lemma}


We assume the following about the prior distribution of $\theta^*$.

\begin{assumption}\label{assump:qgap}
    There exists a $\Delta > 0$ such that for all ${\theta} \in \Theta$, $h\in [H]$, and $s\in \mathcal{S}$, there exists an $a^*\in \mathcal{A}$ such that $Q_h(s, a^*; {\theta}) \geq Q_h(s, a'; {\theta}) + \Delta, ~\forall a'\in \mathcal{A}\backslash\{a^*\}$.   
\end{assumption}

Define $p_h(s; {\theta}):=\Pr_{{\theta}, \pi^*({\theta})}(s_h = s)$. 

\begin{assumption}\label{assump:pgap}
    The infimum probability of any reachable state, defined as
\begin{equation*}
    \underline{p} := \inf\{p_h(s;{\theta}) : h\in [H], s\in\mathcal{S}, {\theta}\in\Theta, p_h(s; {\theta})> 0 \}
\end{equation*}
satisfies $\underline{p} > 0$.
\end{assumption}

We now describe a procedure to construct an estimator $\hat{\pi}^*$ from $\mathcal{D}_0$ so that $\Pr(\pi^* \neq \hat{\pi}^*)$ is small. Fix an integer $N$, and choose a $\delta \in (0,1)$. For each $\theta\in \Theta$, define a deterministic Markov policy $\pi^{*}(\theta) = (\pi_h^{*}(\cdot ; \theta) )_{h=1}^H$ sequentially through 
\begin{equation}
 \pi_h^{*}(s; \theta) =    \begin{cases}
        \arg\max_a Q_h(s, a; \theta), & \text{if }\Pr_{\theta, \pi_{1:h-1}^{*}(\theta) }(s_h = s) > 0\\
        \bar{a}_0, &\text{if }\Pr_{\bar{\theta}, \pi_{1:h-1}^{*}(\bar{\theta}) }(s_h = s) > 0,
    \end{cases}
\end{equation}
where the tiebreaker for the argmax operation is based on a fixed order on actions, and $\bar{a}_0\in\mathcal{A}$ is a fixed action in $\mathcal{A}$. It is clear that $\pi^{*}(\theta)$ is an optimal policy for the MDP $\theta$. Furthermore, for those states that are impossible to be visited, we choose to take a fixed action $\bar{a}_0$. Although the choice of action at those states doesn't matter, our construction will be helpful for the proofs.


\textbf{Construction of $\hat{\pi}^*$:} Let $N_h(s)$ (resp. $N_h(s, a)$) be the number of times state $s$ (resp. state-action pair $(s, a)$) appears at time $h$ in dataset $\mathcal{D}_0$. Define $\hat{\pi}^*$ to be such that: 
\begin{itemize}
    \item $\hat{\pi}_h^*(s) = \arg\max_{a\in\mathcal{A}} N_h(s, a)$ (ties are broken through some fixed ordering of actions) whenever $N_h(s) \geq \delta N$;
    \item $\hat{\pi}_h^*(s) = \bar{a}_0$ whenever $N_h(s) < \delta N$. $\bar{a}_0$ is a fixed action in $\mathcal{A}$ that was used in the definition of $\pi^*(\theta)$. %\rahul{where is $\pi^*$ defined?}
\end{itemize}

The idea of the proof is that for sufficiently large $\beta$ and $N$, we can choose a $\delta \in (0,1)$ such that 
\begin{itemize}
    \item \textit{Claim 1:} If $s\in\mathcal{S}$ is probable at time $h$ under $\pi^*(\theta)$, then $N_h(s)\geq \delta N$ with large probability. Furthermore, $\pi_h^*(s) = \arg\max_{a\in\mathcal{A}} N_h(s, a)$ with large probability as well.
    \item \textit{Claim 2:} If $s\in\mathcal{S}$ is improbable at time $h$ under $\pi^*(\theta)$, then $N_h(s)< \delta N$ with large probability;
\end{itemize}

Given the two claims, we can then conclude that $\pi^*=\hat{\pi}^*$ with high  probability via a standard union bound argument.

\begin{lemma}\label{lem:binomial}
    Let $X$ be the sum of $N$ i.i.d. Bernoulli random variables with mean $p\in (0, 1)$. Let $q\in (0, 1)$, then 
    \begin{align*}
        \Pr(X \leq qN) &\leq \exp\left(-2N(q-p)^2\right),\qquad\text{if }q < p,\\
        \Pr(X \geq qN) &\leq \exp\left(-2N(q-p)^2\right),\qquad\text{if }q > p.
    \end{align*}
\end{lemma}

\begin{proof}
    Both inequalities can be obtained by applying Hoeffding's Inequality.
\end{proof}

\newcommand{\probbound}{SH\left[\exp\left(-\dfrac{N\underline{p}^2}{18}\right) +\exp\left(-\dfrac{N\underline{p}}{36}\right) \right]}

\begin{lemma}\label{lem:piestimator}
    Let $\Delta$ and  $\underline{p}$ be as in Assumptions  \ref{assump:qgap} and \ref{assump:pgap} respectively and let $$\underline{\beta} := [\log 3 - \log \underline{p} + \log (H-1) + \log(A-1)]/\Delta.$$
    For any $\beta \geq \underline{\beta}$ and $N\in\mathbb{N}$, there exists an estimator $\hat{\pi}^*$ constructed from $\mathcal{D}_0$ that satisfies
    \begin{align*}
        \Pr(\pi^*\neq \hat{\pi}^*) &\leq \probbound .
    \end{align*}
\end{lemma}
The proof is available in the appendix.

\begin{theorem}
    Let $\beta\geq \underline{\beta}$, then for all $N$ such that $\varepsilon_N\leq 1/2$, we have
    \begin{equation}\label{eq:finalregbound}
    \begin{split}
        \BR_T(\phi^{\text{iPSRL}})  \leq \min\Big\{\sqrt{S^2A^2H^4T\log(STH)},H\sqrt{(SA)^H (\varepsilon_N SH\log A + \mathbf{h}(\varepsilon_N) ) T/2}\Big\}.        
        \end{split}
    \end{equation}
    where
    \begin{equation*}
        \varepsilon_N = \probbound .
    \end{equation*}
\end{theorem}

\begin{proof}
    Since $\hat{\pi}^*$ is a function of $\mathcal{D}_0$, we have $ \mathbb{H}(\pi^*|\mathcal{D}_0) \leq \mathbb{H}(\pi^*|\hat{\pi}^*)$. The result then follows from Lemma \ref{lem:russovanroy}, Lemma \ref{lem:fano}, and Lemma \ref{lem:piestimator}.
\end{proof}

Note that using the inequality $\mathbf{h}(\varepsilon) \leq 2\sqrt{\varepsilon}-\varepsilon$ for all $\varepsilon\in (0, 1)$, we see that the right-hand side of \eqref{eq:finalregbound} converges to zero exponentially fast as $N\rightarrow\infty$.

\begin{remark}
(a) For fixed $N$, and large $S$ and $A$, the regret bound is $\tilde{O}(SAH^2\sqrt{T})$, which possibly could be improved in $H$.
(b) For a suitably large $\beta$, the regret bound obtained goes to zero exponentially fast as $N$, the offline dataset size, goes to infinity thus indicating the online learning algorithm's ability to learn via imitation of the expert. 
(c) Corollary \ref{lem:russovanroy} can be improved to remove the exponential dependency on $H$ by using the Cauchy–Schwarz inequality in the space of state-action occupancy measures. Such technique has been successfully used in \citep{hao2022regret} in a purely online setting. We leave this refinement as a part of future work. 
\end{remark}
%Our regret bound does not go to zero as $\beta$ goes to infinity for any fixed $N$. This is not an artifact of our proof: different from the bandit setting, the randomness in the MDP itself cannot be eliminated as $\beta$ goes to infinity. As a result, there's non-vanishing probability (with respect to $\beta$) that the dataset $\mathcal{D}_0$ fail to cover certain states in the system, and we learn nearly nothing about what is optimal in those states from the data alone. This in turn means that we will incur non-vanishing Bayesian regret.



%\input{temp}

\section{Approximating iPSRL}\label{sec:approx-iPSRL}

\subsection{The Informed \rlsvi~Algorithm}

The $\ipsrl$~algorithm  introduced in the previous section assumes that posterior updates can be done exactly. In practice, the posterior update in Eq. \eqref{eq:infor_prior} is challenging due to the loss of conjugacy while using the Bayes rule. Thus, we must find a computationally efficient way to do approximate posterior updates (and obtain samples from it) to enable practical implementation. Hence, we propose a novel approach based on Bayesian bootstrapping to obtain
approximate posterior samples. The key idea is to perturb the loss function for the maximum a posterior (MAP) estimate and use the point estimate as a surrogate for the exact posterior sample.

Note that in the ensuing, we regard $\beta$ as also unknown to the learning agent (and $\lambda=\infty$ for simplicity). Thus, the learning agent must form a belief over both $\theta$ and $\beta$ via a joint posterior distribution conditioned on the offline dataset $\mathcal{D}_0$ and the online data at time $t$, $\mathcal{H}_t$. We denote the prior pdf over $\theta$ by $f(\cdot)$ and prior pdf over $\beta$ by $f_2(\cdot)$.

For the sake of compact notation, we denote $Q_h^{*}(s,a; \theta)$ as $\Qth_h(s,a)$ in this section. Now, consider the offline dataset, 
$$\cD_0 = \{((s_h^l,a_h^l,\check{s}_h^l,r_h^l)_{h=0}^{H-1})_{l=1}^{L}\}$$ 
and denote $\theta = (\theta_h)_{h=0}^{H-1}$. We introduce the  \textit{temporal difference error} ${\cE}_h^l$ (parameterized by a given $\Qth$),
$$\cE_h^l(\Qth) : = \left(r_h^l + \max_b \Qth_{h+1}(\check{s}_h^l,b) - \Qth_h(s_h^l,a_h^l)\right).$$ 
We will regard $\Qth_h$ to only be parameterized by $\theta_h$, i.e., $Q^{\theta_h}_{h}$ but abuse notation for the sake of simplicity.
We use this to construct a \textit{parameterized offline dataset}, 
$$\cD_0(\Qth) = \{((s_h^l,a_h^l,\check{s}_h^l,\cE_h^l(\Qth))_{h=0:H-1})_{l=1:L}\}.$$ 
A parametrized online dataset $\cH_t(\Qth)$ after episode $t$ can be similarly defined. To ease notation, we will regard the $j$th episode during the online phase as the $(L+j)$th observed episode. Thus,
\[
\cH_t(\Qth) = \{((s_h^k,a_h^k,\check{s}_h^k,\cE_h^k(\Qth))_{h=0:H-1})_{k=L+1:L+t}\},\]
the dataset observed during the online phase by episode $t$.

Note that $\Qth$ is to be regarded as a parameter. Now,  at time $t$, we would like to obtain a \textbf{MAP estimate} for $(\theta,\beta)$ by solving the following:
\begin{equation}
\begin{split}
\textbf{MAP:} & ~~~\arg \max_{\theta,\beta} \log P(\cH_t(\Qth)|\cD_0(\Qth), \theta, \beta)+  \log P(\cD_0(\Qth)|\theta, \beta) + \log f(\theta) + \log f_2(\beta).
\label{eq:map-mdps}
\end{split}
\end{equation}
Denote a perturbed version of the $\Qth$-parameterized offline dataset by $$\tilde{\cD}_0 (\Qth) = \{((s_h^l,\tilde{a}_h^l,\check{s}_h^l,\tilde{\cE}_h^l)_{h=0:H-1})_{l=1:L}\}$$ where random perturbations are added: (i) actions have perturbation $w_l^h \sim \exp(1)$, (ii) rewards have perturbations $z_h^l \sim \cN(0,\sigma^2)$,  and (iii) the prior $\tilde{\theta} \sim \cN(0,\Sigma_0)$. 

Note that the first and second terms involving $\cH_t$ and $\cD_0$ in \eqref{eq:map-mdps} are independent of $\beta$ when conditioned on the actions. Thus, we have a sum of \textit{log-likelihood of TD error, transition and action} as follows: 
\begin{equation*}
\begin{split}
\log P(\tilde{\cD}_0(\Qth)|\Qth_{0:H}) 
&=  \sum_{l=1}^L \sum_h \Big( \log P(\tilde{\cE}_h^l|\check{s}_h^l,a_h^l,s_h^l,\Qth_{0:H})  \textcolor{black}{+ \log P (\check{s}_h^l|a_h^l,s_h^l,\Qth_{0:H})} 
 + \log P(a_h^l|s_h^l,\Qth_{0:H})\Big)\\
%& \quad + \nu (\theta,Q_{0:H}),\\
%& = \sum_l \sum_h \left( \log P(\cE_h^l|\check{s}_h^l,a_h^l,s_h^l,\Qth_{h:h+1}) + \log \theta (\check{s}_h^l|a_h^l,s_h^l) + \log \pi^{\beta}_h(a_h^l|s_h^l,\Qth_h)\right)\\
%& \quad + \nu (\theta,Q_{0:H}),\\
&\leq  \sum_{l=1}^L \sum_h \Big( \log P(\tilde{\cE}_h^l|\check{s}_h^l,a_h^l,s_h^l,\Qth_{h:h+1})+  \log \pi^{\beta}_h(a_h^l|s_h^l,\Qth_h)\Big).
%& \quad + \nu (\theta,Q_{0:H}),\\
%& \quad + \nu (\theta,Q_{0:H}),\\
\end{split}
\end{equation*}
By ignoring the log-likelihood of the \textcolor{black}{transition term} (akin to optimizing an upper bound on the negative loss function), we are actually being  \textit{optimistic}.


For the terms in the upper bound above, under the random perturbations assumed above, we have
\begin{equation*}
\begin{split}
& \log P(\tilde{\cE}_h^l|\check{s}_h^l,a_h^l,s_h^l,\Qth_{h:h+1})
=  - \frac{1}{2}\left(r_h^l + z_h^l + \max_b \Qth_{h+1}(\check{s}_h^l,b) - \Qth_h(s_h^l,a_h^l)\right)^2  + ~\text{constant}
\end{split}
\end{equation*}
and
\begin{equation*}
\begin{split}
& \log \pi^{\beta}_h(a_h^l|s_h^l,\Qth_h) 
=  w_h^l\left(\beta \Qth_{h}(s_h^l, a_h^l)-\log\sum_b\exp\left(\beta \Qth_{h}(s_h^l, b)\right)\right).
\end{split}
\end{equation*}

Now, denote a perturbed version of the $\Qth$-parametrized online dataset,  $$\tilde{\cH}_t(\Qth) = \{((s_h^k,a_h^k,\check{s}_h^k,\tilde{\cE}_h^k)_{h=0:H-1})_{k=L+1:L+t}\},$$ and thus similar to before, we have
\begin{equation*}
\begin{split}
\log P(\tilde{\cH}_t(\Qth)|\tilde{\cD}_0(\Qth),\Qth_{0:H})
&=  \sum_{k=L+1}^{L+t} \sum_h \Big( \log P(\tilde{\cE}_h^k(\Qth)|\check{s}_h^k,a_h^k,s_h^k,\Qth_{0:H}) 
\textcolor{black}{+ \log P (\check{s}_h^k|a_h^k,s_h^k,\Qth)} \Big),\\ 
%+ \log P(a_h^l|s_h^l,\theta,Q)\\
%& \quad + \nu (\theta,Q_{0:H}),\\
&\leq  \sum_{k=L+1}^{L+t} \sum_h \left( \log P(\tilde{\cE}_h^k|\check{s}_h^k,a_h^k,s_h^k,\Qth_{h:h+1})\right),\\
%& \quad + \nu (\theta,Q_{0:H}),\\
\end{split}
\end{equation*}
where we again ignored the transition term to obtain an \textit{optimistic} upper bound.

Given the random perturbations above, we have
\begin{equation*}
\begin{split}
\log P(\tilde{\cE}_h^k(\Qth)|\check{s}_h^k,a_h^k,s_h^k,\Qth_{h:h+1}) 
=  - \frac{1}{2}\left(r_h^k + z_h^k + \max_b \Qth_{h+1}(\check{s}_h^k,b) - \Qth_h(s_h^k,a_h^k)\right)^2 
 + ~\text{constant}.
\end{split}
\end{equation*}

The prior over $\beta$, $f_2(\beta)$ is assumed to be an exponential pdf $\lambda_2\exp (-\lambda_2\beta), \beta \geq 0$, while that over $\theta$ is assumed Gaussian. Thus, 
putting it all together, we get the following \textbf{\textit{optimistic loss function}} (to minimize over $\theta$ and $\beta$),
\begin{equation}
\begin{split}
& \tilde{\mathcal{L}}(\theta,\beta) = 
\frac{1}{2\sigma^2}\sum_{k=1}^{L+t} \sum_{h=0}^{H-1}\left(r_h^k + z_h^k + \max_b \Qth_{h+1}(\check{s}_h^k,b) - \Qth_h(s_h^k,a_h^k)\right)^2\\
& - \sum_{l=1}^{L} \sum_{h=0}^{H-1}w_h^l\left(\beta \Qth_{h}(s_h^l, a_h^l)-\log\sum_b\exp\left(\beta \Qth_{h}(s_h^l, b)\right)\right)+ \frac{1}{2} (\theta - \tilde{\theta})^\top\Sigma_0(\theta - \tilde{\theta}) +\lambda_2\beta .
\end{split}
\label{eq:lossfn-iPSRL}
\end{equation}

The above loss function is difficult to optimize in general due to the $\max$ operation, and the $Q$-value function in general having a nonlinear form. 

\begin{remark}
Note that the loss function in \eqref{eq:lossfn-iPSRL} can be hard to jointly optimize over $\theta$ and $\beta$. In particular, estimates of $\beta$ can be quite noisy when $\beta$ is large, and the near-optimal expert policy only covers the state-action space partially. Thus, we consider other methods of estimating $\beta$ that are more robust, which can then be plugged into the loss function in \eqref{eq:lossfn-iPSRL}.
Specifically, we could simply look at the entropy of the empirical distribution of the action in the offline dataset. Suppose the empirical distribution of $\{\bar{a}_0^l,\ldots \bar{a}_H^l\}_{l=1}^L$ is $\mu_A$. Then we use $c_0/\cH(\mu_A)$ as an estimation for $\beta$, where $c_0>0$ is a hyperparameter. The intuition is that for smaller $\beta$, the offline actions tend to be more uniform and thus the entropy will be large. This is an unsupervised approach and agnostic to specific offline data generation process. 

\end{remark}

\begin{remark}
In the loss function in \eqref{eq:lossfn-iPSRL}, the parameter $\theta$ appears inside the $\max$ operation. Thus, it can be quite difficult to optimize over $\beta$. Since the loss function is typically optimized via an iterative algorithm such as a gradient descent method, a simple and scalable solution that works well in practice is to use the parameter estimate $\theta$ from the previous iteration inside the $\max$ operation, and thus optimize over $\theta$ only in the other terms. 
\end{remark}


\subsection{iRLSVI ~bridges Online RL and Imitation Learning}

In the previous subsection, we derived $\irlsvi$, a Bayesian-bootstrapped algorithm. We now present interpretation of the algorithm as bridging online RL (via commonality with the \rlsvi~algorithm \citep{osband2016deep} and imitation learning, and hence a way for its generalization.

Consider the \rlsvi~algorithm for online reinforcement learning as introduced in \citep{osband2019deep}. It draws its inspiration from the posterior sampling principle for online learning, and has excellent cumulative regret performance. \rlsvi, that uses all of the data available at the end of episode $t$, including any offline dataset involves minimizing the corresponding loss function at each time step:
\begin{equation*}
\begin{split}
& \tilde{\mathcal{L}}_{\text{RLSVI}}(\theta) =  \frac{1}{2\sigma^2}\sum_{k=1}^{L+t} \sum_{h=0}^{H-1}\left(r_h^k + \max_b Q^{\theta}_{h+1}(\check{s}_h^k,b) - \Qth_h(s_h^k,a_h^k)\right)^2 + \frac{1}{2} ({\theta_{0:H}} - {\tilde{\theta}_{0:H}})^\top\Sigma_0({\theta_{0:H}} - {\tilde{\theta}_{0:H}}). \end{split}
\label{eq:loss-rlsvi}
\end{equation*}

Now, let us consider an imitation learning setting. Let $\tau_l = (s_h^l,a_h^l,\check{s}_h^l)_{h=0}^{H-1}$ be the trajectory of the $l$th episode. Let $\hat{\pi}_h (a|s)$ denote the empirical estimate of probability of taking action $a$ in state $s$ at time $h$, i.e., an empirical estimate of the expert's randomized policy. Let $p(\tau)$ denote the probability of observing the trajectory under the policy $\hat{\pi}$.

Let $\pi_h^{\beta, \theta}(\cdot|s)$ denote the parametric representation of the policy used by the expert.  And let $p^{\beta, \theta}(\tau)$ denote the probability of observing the trajectory $\tau$ under the policy ${\pi}^{\beta, \theta}$. Then, the loss function corresponding to the KL divergence between $\Pi_{l=1}^Lp(\tau_l)$ and $\Pi_{l=1}^Lp^{\beta, \theta}(\tau_l)$ is given by
\begin{equation*}
\begin{split} 
\tilde{\mathcal{L}}_{\text{IL}}(\beta,\theta) &= 
D_{KL}\left(\Pi_{l=1}^Lp(\tau_l)||\Pi_{l=1}^Lp^{\beta, \theta}(\tau_l)\right)  = \int \Pi_{l=1}^Lp(\tau_l) \log \frac{\Pi_{l=1}^Lp(\tau_l)}{\Pi_{l=1}^Lp^{\beta}(\tau_l)} = \sum_{l=1}^L \int p(\tau_l) \log \frac{p(\tau_l)}{p^{\beta, \theta}(\tau_l)},\\
& = \sum_{l=1}^L \sum_{h=0}^{H-1} \log \frac{\hat{\pi}_h(a_h^l|s_h^l)}{\pi^{\beta, \theta}_h(a_h^l|s_h^l)}\\
& = \sum_{l=1}^L \sum_{h=0}^{H-1} [\log \hat{\pi}_h (a_h^l|s_h^l) - \log \pi_h^{\beta, \theta}(a_h^l|s_h^l)]\\
&= - \sum_{l=1}^{L} \sum_{h=0}^{H-1}\left(\beta \Qth_{h}(s_h^l, a_h^l)-\log\sum_b\exp\left(\beta \Qth_{h}(s_h^l, b)\right)\right) \quad + \text{constant}.
\end{split}
\label{eq:loss-bc}
\end{equation*}

\begin{remark}
(i) The loss function $\tilde{\mathcal{L}}_{\text{IL}}(\beta,\theta)$ is the same as the second (action-likelihood) term in \eqref{eq:lossfn-iPSRL} while the loss function $\tilde{\mathcal{L}}_{\text{RLSVI}}(\theta)$ is the same as the first and third terms there (except for perturbation) and minus the $\lambda_2\beta$ term that corresponds to the prior over $\beta$.
(ii) Note that while we used the more common KL divergence for the imitation learning loss function, use of log loss would yield the same outcome.
\end{remark}

Thus, the $\irlsvi$~ loss function can be viewed as
\begin{equation}
    \tilde{\mathcal{L}}(\beta,\theta) =  \tilde{\mathcal{L}}_{\text{RLSVI}}(\theta) + \tilde{\mathcal{L}}_{\text{IL}}(\beta,\theta) + \lambda_2\beta,
\end{equation}
thus establishing that the proposed algorithm may be viewed as bridging Online RL with Imitation Learning. Note that the last term corresponds to the prior over $\beta$. If $\beta$ is known (or uniform), it will not show up in the loss function above.

The above also suggests a possible way to generalize and obtain other online learning algorithms that can bootstrap by use of offline datasets. Namely, at each step, they can optimize a general loss function of the following kind: 
\begin{equation}
    \tilde{\mathcal{L}}_{\alpha}(\beta,\theta) =  \alpha\tilde{\mathcal{L}}_{\text{ORL}}(\theta) + (1-\alpha) \tilde{\mathcal{L}}_{\text{IL}}(\beta,\theta) + \lambda_2\beta,
\end{equation}
where $\tilde{\mathcal{L}}_{\text{ORL}}$ is a loss function for an Online RL algorithm, $\tilde{\mathcal{L}}_{\text{IL}}$ is a loss function for some Imitation Learning algorithm, and factor $\alpha \in [0,1]$ provides a way to tune between emphasizing the offline imitation learning and the online reinforcement learning.

\section{Empirical Results}
\label{sec:empirical}

\newcommand{\la}{{\tt left}}
\newcommand{\ra}{{\tt right}}

\textbf{Performance on the Deep Sea Environment.} We now present some empirical results on ``deep sea", a prototypical environment for online reinforcement learning \citep{osband2019deep}. We compare three variants of the $\irlsvi$~agents, which are respectively referred to as \emph{informed} \rlsvi~($\irlsvi$), \emph{partially informed} \rlsvi~($\pirlsvi$), and \emph{uninformed} \rlsvi~($\urlsvi$). All three agents are tabular \rlsvi~agents with similar posterior sampling-type exploration schemes. However, they differ in whether or not and how to exploit the offline dataset. In particular, $\urlsvi$ ignores the offline dataset; $\pirlsvi$ exploits the offline dataset but does not utilize the information about the generative policy; while $\irlsvi$ fully exploits the information in the offline dataset, about both the generative policy and the reward feedback. We note no other algorithms are known for the problem as posed. 

Deep sea is an episodic reinforcement learning problem with state space $\cS = \left \{ 0, 1,  \ldots, M \right \}^2$ and , where $M$ is its size. The state at period $h$ in episode $t$ is $s_{h}^t = \left(x_{h}^t, d_{h}^t \right) \in \cS$, where $x_h^t=0,1,\ldots,M$ is the horizontal position while $d_h^t=0,1,\ldots, M$ is the depth (vertical position). Its action space is $\cA= \left \{ \la, \ra \right \}$ and time horizon length is $H=M$.
Its reward function is as follows: If the agent chooses an action $\ra$ in period $h<H$, then it will receive a reward $-0.1/M$, which corresponds to a ``small cost"; If the agent successfully arrives at state $(M, M)$ in period $H=M$, then it will receive a reward $1$, which corresponds to a ``big bonus"; otherwise, the agent will receive reward $0$. The system dynamics are as follows: for period $h<H$, the agent's depth in the next period is always increased by $1$, i.e.,  $d^t_{h+1} = d^t_h + 1$. For the agent's horizontal position, if $a^t_h = \la$, then $x^t_{h+1} = \max \{ x^t_h - 1, 0 \}$, i.e., the agent will move left if possible. On the other hand, if $a^t_h = \ra$, then we have $x^t_{h+1} = \min \{ x^t_h + 1, M\}$ with prob. $1-1/M$ and $x^t_{h+1} = x^t_h$ with prob. $1/M$. The initial state of this environment is fixed at state $(0, 0)$.

The offline dataset is generated based on the expert's policy specified in Eq.~\eqref{eq:expert}, and we assume $\beta(s)=\beta$ (a constant) across all states. We set the size of the offline dataset $\mathcal{D}_0$ as $ |\mathcal{D}_0| = \kappa |\cA| |\cS| $, where $\kappa \geq 0$ is referred to as \emph{data ratio}.
%
We fix the size of deep sea as $M=10$.  We run the experiment for $T=300$ episodes, and the empirical cumulative regrets are averaged over $50$ simulations. The experimental results are illustrated in Figure~\ref{fig:regret_vs_beta},
as well as Figure~\ref{fig:cum_regret} in Appendix~\ref{app:more_empirical}.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\textwidth]{regret_vs_beta.pdf}
  \caption{Cumulative regret vs. $\beta$ in deep sea.}
  \label{fig:regret_vs_beta}
\end{figure}

Specifically, Figure~\ref{fig:regret_vs_beta} plots the cumulative regret in the first $T=300$ episodes as a function of the expert's deliberateness $\beta$, for two different data ratio $\kappa=1, 5$. There are several interesting observations based on Figure~\ref{fig:regret_vs_beta}: (i) Figure~\ref{fig:regret_vs_beta} shows that $\irlsvi$ and $\pirlsvi$ tend to perform much better than $\urlsvi$, which demonstrates the advantages of exploiting the offline dataset, and this improvement tends to be more dramatic with a larger offline dataset. (ii) When we compare $\irlsvi$ and $\pirlsvi$, we note that their performance is similar when $\beta$ is small, but $\irlsvi$ performs much better than $\pirlsvi$ when $\beta$ is large. This is because when $\beta$ is small, the expert's generative policy does not contain much information; and as $\beta$ gets larger, it contains more information and eventually it behaves like imitation learning and learns the optimal policy as $\beta \rightarrow \infty$. Note that the error bars denote the standard errors of the empirical cumulative regrets, hence the improvements are statistically significant. 
%Figure~\ref{fig:cum_regret} plots the cumulative regret vs. $t$, the number of episodes.
% \begin{itemize}
%     \item 
%     \item 
% \end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\textwidth]{misspecified_beta.pdf}
  \caption{Robustness of $\irlsvi$~to misspecification.}
  \label{fig:regret_misspecification}
\end{figure}



\textbf{Robustness to misspecification of $\beta$.} We also investigate the robustness of various RLSVI agents with respect to the possible misspecification of $\beta$. In particular, we demonstrate empirically that in the deep sea environment with $M=10$, with offline dataset is generated by an expert with deliberateness $\beta=5$, the $\irlsvi$ agent is quite robust to moderate misspecification. Here, the misspecified deliberateness parameter is denoted $\tilde{\beta}$. The empirical results are illustrated in Figure~\ref{fig:regret_misspecification}, where the experiment is run for $T=300$ episodes and the empirical cumulative regrets are averaged over $50$ simulations.

Since $\urlsvi$ and $\pirlsvi$ do not use parameter $\tilde{\beta}$, thus, as expected, their performance is constant over $\tilde{\beta}$. On the other hand, $\irlsvi$ explicitly uses parameter $\tilde{\beta}$. As Figure~\ref{fig:regret_misspecification} shows, the performance of $\irlsvi$ does not vary much as long as $\tilde{\beta}$ has the same order of magnitude as $\beta$. However, there will be significant performance loss when $\tilde{\beta}$ is too small, especially when the data ratio is also small. This makes sense since when $\tilde{\beta}$ is too small, $\irlsvi$ will choose to ignore all the information about the generative policy and eventually reduces to $\pirlsvi$. 

\section{Conclusions}\label{sec:conclusions}

In this paper, we have introduced and studied a new problem: Given an offline demonstration dataset from an imperfect expert, what is the best way to leverage it to bootstrap online learning performance in MDPs. We have followed a principled approach and introduced two algorithms: the ideal $\ipsrl$~ algorithm, and the $\irlsvi$~ algorithm that is computationally practical and seamlessly bridges online RL and imitation learning in a very natural way. We have shown significant reduction in regret both empirically, and theoretically as compared to two natural baselines. The dependence of the regret bound on some of the parameters (e.g., $H$) could be improved upon, and is a good direction for future work. In future work, we will also combine the $\irlsvi$~algorithm with deep learning to leverage offline datasets effectively for continuous state and action spaces as well.

\bibliographystyle{plainnat}%Used BibTeX style is unsrt
\bibliography{main}

\appendix
\section{Proof of Lemma \ref{lem:regboundlinbandit}}
\begin{proof}
    To simplify notations, for a deterministic Markov policy $\pi=(\pi_h)_{h=1}^H$, $\pi_h: \mathcal{S}\mapsto \mathcal{A}$, set $\pi_h(a|s) = \mathbf{1}_{\{\pi_{h}(s) = a \}}$.

    For each deterministic Markov policy $\pi$, define a $(SA)^H$-dimensional indicator vector $x_{\pi}\in \{0, 1\}^{(\mathcal{S}\times\mathcal{A})^{H}}$ by
    \begin{align}
        x_{\pi}\left(s_{1:H}, a_{1:H}\right) &= \prod_{h=1}^H\pi_h(a_h|s_h).
    \end{align}

    We claim that $V_0^\pi(\theta)$, the value of policy $\pi$ in an MDP $\theta$, is a linear function of $x_{\pi}$. This can be seen since the probability of each trajectory $(s_{1:H}, a_{1:H})$ is given by
    \begin{align}
        &\quad~\Pr_{\theta, \pi}(s_{1:H}, a_{1:H}) \\
        &= P_0^\theta(s_1)\pi_1(a_1|s_1)  P_1^{\theta}(s_2|s_1, a_1) \pi_2(a_2|s_2) P_2^\theta(s_3|s_2, a_2)\times\cdots \\&\qquad\times P_{H-1}^{\theta}(s_H|s_{H-1}, a_{H-1})\pi_H(a_H|s_H)\\
        &:= P^\theta(s_{1:H}, a_{1:H}) x_{\pi}(s_{1:H}, a_{1:H}),
    \end{align}
    and $V_0^{\pi}(\theta)$ is a linear function of the probability measure on the trajectory. %For each $\theta$, let $v^\theta\in\mathbb{R}^{(\mathcal{S}\times\mathcal{A})^H}$ be such that $V_0^{\pi}(\theta)/H = (v^\theta)^T x_{\pi}$

    Let $\{\pi^1, \pi^2,\cdots, \pi^M \}$ where $M = A^{SH}$ be the set of all deterministic Markov policies. Let $\{x^1, x^2, \cdots, x^M\}$ be their corresponding $(SA)^H$-dimensional indicator vectors. If we treat $x^i$'s as ``actions'', and the state-action trajectories as ``observations'', then the MDP learning process can be treated as a linear bandit process with bandit feedback. Applying Proposition 1 and Proposition 5 in \citep{russo2016information} we obtain the result.

    For notational simplicity define $\nu_i = \Pr(\pi^* = \pi^i)$. Let $R_i$ denote the random total reward divided by $H$ in MDP $\theta$ if one applies policy $\pi^i$. We have $R_i\in [0, 1]$ holds with probability 1. We have 
    \begin{align}
        \E[R_i] &= \E[(v^\theta)^T x^i ] = \left(\E[v^\theta]\right)^T x^i\\
        \E[R_i|\pi^* = \pi^j] &=  \E[(v^\theta)^T x^i |\pi^* = \pi^j ] = \left(\E[v^\theta|\pi^* = \pi^j ]\right)^T x^i.
    \end{align}

    Set $\mu = \E[v^\theta]$ and $\mu^j = \E[v^\theta|\pi^* = \pi^j ]$.
    Define a matrix $\Phi\in\mathbb{R}^{M\times M}$ by
    \begin{align}
        \Phi_{ij} &= \sqrt{\alpha_i\alpha_j}(\E[R_i|\pi^* = \pi^j] - \E[R_i])\\
        &= \sqrt{\alpha_i\alpha_j}(\mu^j - \mu)^T x^i.
    \end{align}

    Then following the proof of \citep{russo2016information} we have
    \begin{equation}
        \E[\mathrm{Reg}(T)/H]  \leq \sqrt{\mathrm{rank}(\Phi) \mathbb{H}(\nu) T/2}.
    \end{equation}

    It remains to bound $\mathrm{rank}(\Phi)$. Notice that
    \begin{align}
        \Phi = \begin{bmatrix}
            \sqrt{\alpha_1}(\mu^1 - \mu)^T\\
            \sqrt{\alpha_2}(\mu^2 - \mu)^T\\
            \vdots\\
            \sqrt{\alpha_M}(\mu^M - \mu)^T
        \end{bmatrix}
        \begin{bmatrix}
            \sqrt{\alpha_1}x^1& \sqrt{\alpha_2}x^1 & \cdots & \sqrt{\alpha_M}x^M.
        \end{bmatrix}
    \end{align}
    which means that $\Phi$ is the product of a $M\times (SA)^H$ matrix and a $(SA)^H\times M$ matrix. This means that $\mathrm{rank}(M) \leq (SA)^H$. 
    
On the other hand, we could follow the proof of Theorem 4.11 in \citep{hao2022regret} to obtain the second part of the regret upper bound.   
    
    
\end{proof}
\section{Proof of Lemma \ref{lem:piestimator}}
\begin{proof}
    For convenience, write $\Pr_\theta(\cdot) = \Pr(\cdot|\theta)$.
    Define the event $\mathcal{E}_{n, h} = \{\Bar{a}_{n, h}\neq a_{h}^*(\Bar{s}_{n, h};\theta) \}$, i.e. in the $n$-th round of demonstration, the expert did not take the optimal action at time $h$. Given the expert's randomized policy $\phi$, we have
    \begin{align}
        &\quad~\Pr_\theta(\mathcal{E}_{n, h}|\Bar{s}_{n,1:h},\Bar{a}_{n,1:h-1}) \\&= 1-\dfrac{1}{1 + \sum_{a\neq a_{h}^*(\Bar{s}_{n, h};\theta)}\exp(-\beta\Delta_h(\Bar{s}_{n, h}, a;\theta))}\\
        &\leq \sum_{a\neq a_{h}^*(\Bar{s}_{n, h};\theta)}\exp(-\beta\Delta_h(\Bar{s}_{n, h}, a;\theta))\\
        &\leq (A-1)\exp(-\beta\Delta)=:\Tilde{\kappa}_\beta.
    \end{align}

    Define $\kappa_\beta = (H-1)\Tilde{\kappa}_\beta$. Then $\beta\geq\underline{\beta}$ means that $\kappa_\beta\leq \underline{p}/3$.
    
    Consider each $(h, s)\in[H]\times \mathcal{S}$, conditioning on $\theta$ there are two cases:
    \begin{itemize}
        \item If $p_h(s;\theta) > 0$ then
        \begin{align}
            \Pr_\theta (\Bar{s}_{n, h} = s) &\geq \left(1-\Tilde{\kappa}_\beta\right)^{h-1} p_h(s;\theta) \geq (1-\kappa_\beta) \underline{p} \geq \frac{2}{3}\underline{p}.\label{eq:probablestate}
        \end{align}
        
        % Forget the coupling argument. Hard to explain and formalize in P(s'|s,a) kind of formulation. It would be easier in the s' = func(s, a, randomness) formulation
        The first inequality in \eqref{eq:probablestate} can be established via induction on $h$: First observe that $\Pr_\theta (\Bar{s}_{n, 1} = s) = p_1(s;\theta)$ for all $s\in\mathcal{S}$ by definition. Suppose that we have proved the statement for time $h$, i.e. $\Pr_\theta (\Bar{s}_{n, h} = s) \geq (1-\Tilde{\kappa}_\beta)^{h-1} p_h(s;\theta)$ for all $s\in\mathcal{S}$. Then we have
        \begin{align}
            p_{h+1}(s';\theta) &= \sum_{s\in\mathcal{S}} \Pr_\theta(s' |s, a_h^*(s;\theta) ) p_h(s;\theta)\label{eq:markovtransition}\\
            \Pr_\theta(\Bar{s}_{n, h+1} = s') &\geq \sum_{s\in\mathcal{S}} \phi_h^\beta(a_h^*(s;\theta)|s;\theta) \Pr_\theta(s' |s, a_h^*(s;\theta) ) \Pr_\theta(\Bar{s}_{n, h} = s)\\
            &\geq \sum_{s\in\mathcal{S}}\left(1-\Tilde{\kappa}_\beta\right) \Pr_\theta(s' |s, a_h^*(s;\theta) ) \Pr_\theta(\Bar{s}_{n, h} = s)\\
            &\geq \sum_{s\in\mathcal{S}}\left(1-\Tilde{\kappa}_\beta\right)^h \Pr_\theta(s' |s, a_h^*(s;\theta) ) p_h(s;\theta).\label{ineq:markovtransition}
        \end{align}
        The statement for $h+1$ then follows by comparing \eqref{eq:markovtransition} and \eqref{ineq:markovtransition}, establishing the induction step.
        
        \item If $p_h(s;\theta) = 0$, then if $\Bar{s}_{n, h} = s$, the expert must have chosen some action that was not optimal before time $h$ in the $n$-th round of demonstration. We conclude that
        \begin{align}
            \Pr_\theta(\Bar{s}_{n, h} = s) \leq \Pr_\theta\left(\bigcup_{\Tilde{h}=1}^{h-1} \mathcal{E}_{n, \Tilde{h}}\right) \leq \sum_{\Tilde{h}=1}^{h-1} \Pr_\theta(\mathcal{E}_{n, \Tilde{h}})\leq \kappa_\beta\leq \frac{1}{3}\underline{p}.\label{eq:inprobablestate}
        \end{align}
    \end{itemize}

    The above argument shows that there's a separation of probability between two types of state and time index pairs under the expert's policy $\phi^\beta(\theta)$: the ones that are probable under the optimal policy $\pi^*(\theta)$ and the ones that are not. Using this separation, we will proceed to show that when $N$ is large, we can distinguish the two types of state and time index pairs through their appearance counts in $\mathcal{D}_0$. This will allow us to construct a good estimator of $\pi^*$.

    Define $\hat{\pi}^*$ to be the estimator of $\pi^*$ constructed with $\delta = \underline{p}/2$. If $\hat{\pi}^*\neq \pi$, then either one of the following cases happens
    \begin{itemize}
        \item There exists an $(s, h)\in\mathcal{S}\times [H]$ pair such that $p_h(s;\theta) > 0$ but $N_h(s) < \delta N$;
        \item There exists an $(s, h)\in\mathcal{S}\times [H]$ pair such that $p_h(s;\theta) = 0$ but $N_h(s) \geq \delta N$;
        \item There exists an $(s, h)\in\mathcal{S}\times [H]$ pair such that $p_h(s;\theta) > 0$ and $N_h(s) \geq \delta N$, but $\pi_h^*(s;\theta) = a_h^*(s;\theta)\neq \arg\max_{a}N_h(s, a)= \hat{\pi}_h^*(s)$;
    \end{itemize}

    Using union bound, we have
    \begin{align}
        &\quad~\Pr_\theta(\pi^*\neq \hat{\pi}^*)\\
        &\leq \sum_{(s, h): p_h(s;\theta) > 0} \Pr_\theta(N_h(s) < \delta N) + \sum_{(s, h): p_h(s;\theta) = 0}\Pr_\theta(N_h(s) \geq \delta N) \\
        &\quad~+ \sum_{(s, h): p_h(s;\theta) > 0} \Pr_\theta(N_h(s) \geq \delta N, a_h^*(s;\theta)\neq \arg\max_{a}N_h(s, a) ). \label{eq:errordecomp}
    \end{align}

    Let $\mathrm{Bin}(M, q)$ denote a binomial random variable with parameters $M\in\mathbb{N}$ and $q\in[0, 1]$. Notice that conditioning on $\theta$, each $N_h(s)$ is a binomial random variable with parameters $N$ and $\Tilde{p}_{\theta, h}(s):=\Pr_\theta(\Bar{s}_{1, h} = s)$. 
    
    Using \eqref{eq:probablestate} and Lemma \ref{lem:binomial}, we conclude that each term in the first summation of \eqref{eq:errordecomp} satisfies
    \begin{align}
        \Pr_\theta(N_h(s)< \delta N) &\leq \Pr(\mathrm{Bin}(N, 2\underline{p}/3) < (\underline{p}/2) N)\\
        &\leq \exp(-2N(\underline{p}/6)^2) = \exp\left(-\dfrac{N\underline{p}^2}{18}\right).
    \end{align}

    Using \eqref{eq:inprobablestate} and Lemma \ref{lem:binomial}, we conclude that each term in the second summation of \eqref{eq:errordecomp} satisfies
    \begin{align}
        \Pr_\theta(N_h(s)\geq \delta N) &\leq \Pr(\mathrm{Bin}(N, \kappa_\beta) \leq (\underline{p}/2) N)\\
        &\leq \exp\left(-2N\left(\frac{\underline{p}}{2} - \kappa_\beta\right)^2 \right)%\wedge \dfrac{\kappa_\beta(1-\kappa_\beta) }{N(\underline{p}/6)^2 } \\
        \leq \exp\left(-\dfrac{N\underline{p}^2}{18}\right). %\wedge \dfrac{36\kappa_\beta}{N\underline{p}^2}
    \end{align}

    Again, using Lemma \ref{lem:binomial}, each term in the third summation of \eqref{eq:errordecomp} satisfies
    \begin{align}
        &\quad~\Pr_\theta(N_h(s) \geq \delta N, a_h^*(s;\theta)\neq \arg\max_{a}N_h(s, a) )\\
        &\leq \Pr_\theta(a_h^*(s;\theta)\neq \arg\max_{a}N_h(s, a) ~|~ N_h(s) \geq \delta N)\\
        &\leq \Pr_\theta(N_h(s) - N_h(s, a_h^*(s;\theta)) \geq N_h(s)/2~|~ N_h(s) \geq \delta N)\\
        &\leq \Pr_\theta(\mathrm{Bin}(N_h(s) , \Tilde{\kappa}_\beta) \geq N_h(s)/2 ~|~ N_h(s) \geq \delta N )\\
        &\leq \Pr_\theta(\mathrm{Bin}(N_h(s) , 1/3) \geq N_h(s)/2 ~|~ N_h(s) \geq \delta N )\\
        &\leq \E_\theta\left[\exp\left(-\dfrac{N_h(s)}{18}\right)~\Big|~N_h(s) \geq \delta N \right]\\
        &\leq \exp\left(-\dfrac{\delta N}{18}\right) = \exp\left(-\dfrac{N\underline{p}}{36}\right).
    \end{align}

    Combining the above we obtain
    \begin{align}
        \Pr_\theta(\pi^*\not\in \hat{\pi}^*)\leq \probbound .
    \end{align}
\end{proof}

\section{More empirical results on deep sea}
\label{app:more_empirical}

In this appendix, we provide more empirical results for the deep sea experiment described in Section~\ref{sec:empirical}. Specifically, for deep sea with size $M=10$, data ratio $\kappa=1, 5$, and expert's deliberateness $\beta=1, 10$, we plot the cumulative regret of $\irlsvi$, $\pirlsvi$, and $\urlsvi$ as a function of the number of episodes $t$ for the first $T=300$ episodes. The experiment results are averaged over $50$ simulations and are illustrated in Figure~\ref{fig:cum_regret}.

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.85\textwidth]{cum_regret.pdf}
  \caption{Cumulative regret vs. number of episodes in deep sea.}
  \label{fig:cum_regret}
\end{figure*}

As we have discussed in the main body of the paper, when both data ratio $\kappa$ and the expert's deliberateness $\beta$ are small, then there are not many offline data and the expert's generative policy is also not very informative. In this case, $\irlsvi$, $\pirlsvi$, and $\urlsvi$ perform similarly. On the other hand, when the data ratio $\kappa$ is large, $\irlsvi$ and $\pirlsvi$ tend to perform much better than $\urlsvi$, which does not use the offline dataset. Similarly, when the expert's deliberateness $\beta$ is large, then the expert's generative policy is informative. In this case, $\irlsvi$ performs much better than $\pirlsvi$ and $\urlsvi$.





\end{document}