{
    "arxiv_id": "2303.11369",
    "paper_title": "Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale",
    "authors": [
        "Botao Hao",
        "Rahul Jain",
        "Dengwang Tang",
        "Zheng Wen"
    ],
    "submission_date": "2023-03-20",
    "revised_dates": [
        "2023-03-22"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "abstract": "In this paper, we address the following problem: Given an offline demonstration dataset from an imperfect expert, what is the best way to leverage it to bootstrap online learning performance in MDPs. We first propose an Informed Posterior Sampling-based RL (iPSRL) algorithm that uses the offline dataset, and information about the expert's behavioral policy used to generate the offline dataset. Its cumulative Bayesian regret goes down to zero exponentially fast in N, the offline dataset size if the expert is competent enough. Since this algorithm is computationally impractical, we then propose the iRLSVI algorithm that can be seen as a combination of the RLSVI algorithm for online RL, and imitation learning. Our empirical results show that the proposed iRLSVI algorithm is able to achieve significant reduction in regret as compared to two baselines: no offline data, and offline dataset but used without information about the generative policy. Our algorithm bridges online RL and imitation learning for the first time.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11369v1"
    ],
    "publication_venue": "Alphabetical order. Corresponding to Rahul Jain"
}