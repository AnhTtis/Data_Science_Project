\section{Related Works}

\textbf{Deep RL and Sim2Real for Robot Learning:}
In recent years, deep RL has led to many successes in the field of robotics \cite{asyncnaf2017, qtopt2018, rubik2019}. However, training neural network policies through RL requires millions of environment interactions for even the simplest of tasks \cite{dmcontrol2018} \footnote{Indeed, as we will describe below, training policies for our ``U-shape Block Assembly Task" requires $\sim$1 billion environment steps, which amounts to about 3000 days of robot operation time.}. This, in addition to concerns of hardware cost and safety, makes RL policies infeasible to train directly in real-world setups. Thus, in this work, we focus on training bi-manual manipulation policies in a simulated setup.
    
Unfortunately, policies trained in simulation rarely transfer well to real-world systems due to model discrepancies and noise in real-world state-estimation. This problem -- often referred to as the ``Sim2Real'' problem -- has led to a significant body of work in recent literature \cite{sim2real2017,sim2real2018,rubik2019,isim2real2022}. Our policy learns without human demonstrations and transfers successfully to the real robots without on-robot fine-tuning.

\textbf{Bi-Manual Robot Learning:} In contrast to the large body of work and benchmarks for single-arm manipulation~\cite{her2017,asyncnaf2017,qtopt2018,metaworld2020,rlbench2020} and dexterous in-hand manipulation~\cite{kumar2016,rubiks2020,baoding2020,robel2020}, those for bi-manual are comparatively limited due to various additional difficulties, including higher action dimensions and larger workspace. Policy learning on real robots is studied in \cite{luck2017} for bi-manual pick up, \cite{amadio2019} for towel folding, and \cite{peg2022} for bi-manual peg insertion (using human demonstrations). \cite{robosuite2020} developed a rich simulation benchmark including bi-manual pickup and peg insertion tasks. \cite{bidex2022} designed a suite of tasks with two floating five-finger-hand manipulators, but despite high dexterity of the tasks, these are only within simulation and the learned behaviors look unnatural for real robot deployment. Our results present a substantial advancement compared to these prior bi-manual tasks, as we require long-horizon planning with multi-step grasping and complex collision avoidance to account for close and intricate bi-arm interaction. 
    
\textbf{Assembly and Construction:}
Most prior learning-based assembly results were only in simulators, often with unrealistic physics simplifications to improve learning through RL. For example, \cite{bapst2019structured} studied 2D simulated task-oriented construction environments, but many realistic and difficult details of assembly are abstracted away, as the agent has the ability to directly actuate a block of choice anywhere in the scene and weld blocks via an explicit action.
\cite{blockassemble2022} studied 3D simulated block assembly environments with similar direct block actuation and achieved combinatorial generalization, assembling target blueprints consisting of up to 16 blocks using a graph neural network policy. \cite{lee2019ikea,lee2021adversarial} also introduce a 3D simulated assembly environment for furniture design from a blueprint, but with limited success in policy generalization. In contrast, our environment contains all of the necessary features for real-world execution (e.g. physical blocks, full robot arm with gripper model, noises and actuation constraints). Also, we successfully executed real-time joint-space control by our agent in the real-world to construct a U-shaped structure where three magnetic blocks are connected.
\cite{suarez2018can} studied assembly of a single chair with real-world bi-manual robots using offline planning methods. In contrast, we use a neural network policy which generates online actions. 
As we will discuss in Section~\ref{sec:emergent_behaviors}, the neural network policy is even able to produce robust emergent behavior, e.g. retries of picking up after accidentally dropping the blocks.
% As mentioned in Section \ref{sec:emergent_behaviors}, the agent can produce robust emergent behavior which retries picking after the robot accidentally drop the blocks.

% \textbf{Generalization in Robot Manipulation:}
% \ps{(I feel we should focus on RL for long horizon robotic tasks and zero-shot real world transfer instead of Generalization, since we don't really have results specifically about generalization.)}
% \sg{I agree}
% Much recent work in robot manipulation focused on the tasks of object grasping \cite{levine2018learning,kalashnikov2018qt}, in-hand object manipulation \cite{andrychowicz2020learning,chen2021system,huang2021generalization}, or execution of a motor skill \cite{yu2020meta}, where variation comes from diversity of object shapes and arrangements involved. By contrast, we use two robotic arms and generates joint trajectories directly from the agent which can handle noisy perception and actuation through the neural network in real world.

% A key point of differentiation from the above works is that we use bi-manual object manipulation as a domain for generalization for sim to real transfer, and train a neural network policy for semi real-time joint control that can solve the tasks with noisy perception and actuation in the real world without needing additional layers for collision checking, perception filtering, inverse kinematics and trajectory generation apart from behaviors produced by the simulator.