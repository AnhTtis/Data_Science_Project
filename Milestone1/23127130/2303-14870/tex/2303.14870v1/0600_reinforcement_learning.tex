\section{Learning Setup}
    \label{sec:sim}
    To solve the U-shape block assembly task, we use RL to learn robust control policies in simulation,
    % and do Sim2Real transfer for real-world execution.
    and transfer learned policies to the real-world without any additional fine-tuning.
    First, we describe the learning setup in simulation.
    %In this section, we detail critical components of our system that enable both successful learning and transfer of bi-manual dexterous assembly policies.
    
  

    \subsection{Policy Network Architecture}
        % \paragraph{Networks}
        Observations are fed to a neural network policy which consists of a 4-layer MLP, with 1024 dimensional hidden layers and swish non-linearities. The output of the policy network consists of two vectors representing the mean and standard deviations for a diagonal multivariate Gaussian distribution. A sample is then squashed by an elementwise Tanh function, and used as the control action.

    \subsection{Algorithm}
       \paragraph{Distributed PPO} We train our RL policies using Proximal Policy Optimization (PPO) \cite{schulman2017proximal} with Generalized Advantage Estimation (GAE) ~\cite{schulman2015high}, and follow the practical PPO training advice of \cite{andrychowicz2020matters}. As will be shown below, a key ingredient in enabling the training of our bi-manual object manipulation agents is the scale of training.
       Our distributed PPO agents are implemented using Jax~\cite{jax2018github}, Haiku~\cite{haiku2020github}, and Acme~\cite{hoffman2020acme}. Agents are trained for 2-3 billion environment steps, using 1 Nvidia V100 GPU for training, and 4000 preemptible CPUs for generating rollouts in the environment, capable of generating 1 billion environment steps in 8 hours (35,000 steps per sec). %34722 steps/s
    %   We use Mujoco~\cite{mujoco2012} as the simulator for the training environment.
    %   In each training episode, the simulated environment starts with the blocks randomly dispersed on the ground, and the robots are reset to predefined initial poses as shown in Figure \ref{fig:three_block_task}, top.
       % \ps{(Someone please Confirm this is right figure)}\sk{yeah this should be ok.  To save the space, we use Figure 1 from many places :)}
       
       \paragraph{Distributed SAC (ablation)} We also train policies with a distributed Soft Actor-Critic (SAC)~\cite{sac2018} implementation provided by Acme~\cite{hoffman2020acme}.
    %   Being optimized for sample efficiency, it runs 120$\times$ slower than PPO in terms of wall-clock time, with 1 GPU and 4000 CPUs, and success rate is 0\% after training 72hours.
        Using identical compute infrastructure, and despite training for over 72 hours, SAC was unable to solve the U-shape assembly task, and had a success rate of 0\%.
        In contrast to the successful application of SAC to standard deep RL control benchmarks~\cite{dmcontrol2018} and single-arm manipulation tasks~\cite{metaworld2020}, our results emphasize the difficulty of our U-shape assembly task, which requires both high precision contact and long-horizon coordination, and is not solvable by the aggressive exploration behavior of SAC.
    %   This training efficiency difference emphasizes the difficulty of our task, which requires both high precision contact and long-horizon coordination, in contrast to standard deep RL control benchmarks~\cite{dmcontrol2018} and single-arm manipulation tasks~\cite{metaworld2020}.
       % it runs significantly slower in terms of wall-clock time, capable of 1 million environment steps in 1 hour (300 steps per sec), i.e. more than 100x slower than distributed PPO in terms of samples on the same hardware resource of 1 GPU and 4000 CPUs. % 278 steps/s
       % Despite being another state-of-the-art algorithm, SAC struggles in our task, requiring 150x wall-clock time to achieve a reward threshold than PPO, which also means that it takes as many samples as PPO for learning our task (Section~\ref{sec:sim_results}).
       % Such result emphasizes the difficulty of our task, which requires both high precision contact and long-horizon coordination, in contrast to standard deep RL control benchmarks~\cite{dmcontrol2018} and single-arm manipulation tasks~\cite{metaworld2020}.
        % \sk{Shane, can you help on explaining why PPO, and also mention we've tried SAC? SAC can achive rew=40 in 3days where PPO can achieve rew=40 in 30minutes.  Maybe mention about our task requires high precision like less than 4.5mm for magnet connection, which SAC is not good?  I think it'd be nice if we can mention about that briefly because we don't have statistical analysis of accuracy performance of SAC. https://drive.google.com/file/d/1LqymDjBqs-Cvcs68Gk8TlRYFo21YjfJS/view?usp=sharing&resourcekey=0-kwbZujvh9-LUILMk52Vymg}

% \section{Sim-To-Real Gap Solution}
\section{Overcoming the Sim-To-Real Gap}
\label{sec:sim2real}

% \sg{todo: maybe add a discussion on another advantage of sim2real: use of privileged information for policy optimization (not policy network input). Contact force constraint is not possible in real world, but is essential for RL of the policy. We can use any sim info we want in "optimization", even if they arent available in real, as long as "policy function" does not take in those info.}

Many of the choices in Task Setup (Section~\ref{sec:task_setup}) and Learning Setup (Section~\ref{sec:sim}) have been specifically made for successful simulation-to-real transfer, without any additional finetuning of RL policies in the real world.
In this section we present additional design details for overcoming the simulation-to-real gap, which have proven to be critical in our experimental ablations (Section~\ref{sec:real_results}).
%   \kg{idk if this sentence makes sense} 
%     \sg{it's a bit rushed, but i feel it's a good point to make either here or in experiment. blockassemble can already assemble 16 blocks, but for ease of RL in simulation, sacrificed a lot of realism for sim2real. so we spent so much effort here to modify envs, each making RL learning harder, but resulting policy can work on real world}
%     Critically, sim-based RL and Sim2Real transfer have a trade-off relationship, where making one easier makes the other one harder. For example, ~\cite{blockassemble2022} shows RL-based learning of assembly up to 16 blocks, but the resulting policy is completely nontransferable to the real world, since it uses direct actuation of blocks which is unrealistic to play in the real world. Each design choice to allow more realism, from removing direct actuation, to using joint-space control, to adding strict safety limits, makes RL learning in simulation more difficult. Therefore, we list them all in this section and show how each is essential for the success of the whole RL-Sim2Real system. \sg{i'll probably rewrite this based on sections below.} \kg{Maybe replacing this paragraph with something like "here are the key reasons we found for policies not transferring to real, and here is how we resolved them"}\sk{I split this section to Method and Sim To Real Gap Solution.  Can you revise this statement accordingly?}
    \subsection{Behavior Constraints}
    \label{sec:behavior_constraints}
        \paragraph{Joint Velocity and Acceleration Limits}
        To enable joint-space policies to produce feasible joint position targets in the real world, it is important to reproduce strict constraints for joint velocity and acceleration limits in simulation. To this end, the policy outputs -- which are target joint velocities for the robotic arms -- are clipped in order to satisfy the joint acceleration limits.

        \paragraph{Applied Force Limit}
        \label{sec:applied_force_limit}
        Excessive forces applied to the robots can cause robots to fault in the real world.
        Additionally, high applied forces may easily lead to object breakages in real world, even if the applied forces are within the robots' configuration limits. To encourage policies to learn less aggressive behaviors and avoid excessive forces, in each simulation step we calculate applied forces for all contacts and apply negative rewards for violations of force limits. We have two types of the applied force limitations. The first one is a general applied force limit which is used for all contacts. This limitation helps the policy avoid causing unrealistic forces between all objects. The second limit is an applied force limit between the blocks and the floor. This limitation helps the robots avoid pushing gripped objects to the ground, which can easily lead to robots' faulting due to joint load limits.
        In the Mujoco simulator, information regarding applied forces can be obtained through a ``constraint force" function. Although force limit is an important factor for successful Sim2Real transfer, limiting applied force significantly increases task difficulty for RL training. We show the training performance across different force limits in simulation in Section~\ref{sec:applied_force_limt_result}.
    
    \subsection{Perception and Actuation Noise}
    \label{sec:perception_and_actuation_noise}
        In the real world, perception data from sensors are inevitably noisy. To address this problem, instead of using statistical filters (e.g. Kalman filter) at runtime, we opted for incorporating noise into our simulation. Specifically, we applied a zero-mean diagonal Gaussian noise (noise scale estimated from real-world data) to both the positions and the orientations of the blocks. Incorporating noise directly into our simulation environment prevents the policy from learning highly agile behaviors that take advantage of idiosyncracies of simulator physics, and results in more conservative policy behaviors that are robust to observation noise.
        Additionally, this allows us to remove an extra abstraction layer for filtering sensor data in the real world,
        which not only contributes to the simplicity of the system, but also reduces the gap between the simulation and real world software stacks.  We show results of real world demonstration with and without noise in real world result Section~\ref{sec:real_results}.
        
    \subsection{Action Delay and Interpolation}
        In zero-shot Sim2Real transfer, the difference of joint behavior between sim and real is a critical gap which prevents the policy from reproducing the expected behaviors in the real world. To address this issue, we characterize the joint behavior by defining 1) the delay between application of joint action and joint action fulfillment, and 2) the start time and duration of action interpolation (policies run at 4Hz and actions are interpolated for 100Hz control). We apply these two parameters in both sim and real environments in order to reproduce the same joint behavior in sim and real. Figure~\ref{fig:action_delay_and_interpolation_joint_graph} shows how the produced action is interpolated and delayed before sending commands to the robot in both sim and real. These graphs demonstrate how the joint position actions are converted to joint position command and fulfilled on the robots. Note that the joint position action is the sum of the last target joint position and the policy's velocity action output.
        % \ps{(I feel Figure 5 is not self-explanatory. So we might want to explain a bit more on what is happening.)}
        \begin{figure}[t]
            \centering
            \includegraphics[scale=.165]{ICRA_images/simulation_joint_trajectory.png}
            \includegraphics[scale=.165]{ICRA_images/real_joint_trajectory.png}
            \caption{
                \small{
                    Joint Trajectory with Delayed and Interpolated Actions
                }
            }
            \label{fig:action_delay_and_interpolation_joint_graph}
            \vspace{-5mm}
        \end{figure}
        We show the effect of modeling these parameters in the real world results in Section~\ref{sec:real_results}.
        % Since these parameters are specific to Sim2Real gap compensation, we show the difference of this parameter in the real world result section. 
        % \sk{show results in real world results.}
        % \sk{show graph for action interpolation between sim/real}  

    % \subsection{Training Performance Evaluation}
    %    \kg{this is not important for the main text, we already discussed success criteria etc., should be moved to appendix or removed, wastes space}
    %    During the training, we evaluate trained policies continuously, approximately every 10 minutes, by freezing the policy and computing 
    %    average success rate over 40 episodes. This continuous evaluation is executed on a dedicated CPU in parallel to the training processes. Additionally, in each evaluation cycle we generate a video to visualize the agents' current learned behavior. This generates a better understanding of training performance and has been an invaluable tool in debugging and research iteration.
