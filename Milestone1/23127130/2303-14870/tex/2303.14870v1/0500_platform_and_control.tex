% \section{System Overview}
    \subsection{Hardware Platform}
        We now describe the real-world setup for the block assembly task.
        We construct a physical work cell (Figure~\ref{fig:icra_cameras}) which consists of two robotics arms (UFACTORY xArm6 \cite{xarm6}), three cameras (Intel RealSense D455 \cite{d455}), and three cuboid blocks with magnetic connection points (Fig~\ref{fig:icra_blocks}). To estimate the position and orientation of the blocks, we use the AprilTag \cite{apriltag} tracking library. All blocks have AprilTag markers attached to their surfaces. We also put AprilTag markers on the workcell floor to track relative positions between the base and blocks. Blocks are printed with a 3D printer using PLA, and consist of two TypeA and one TypeB blocks (Figure~\ref{fig:icra_blocks}):
        \begin{itemize}[leftmargin=*]
            \item \textbf{TypeA ($\bm{\times2}$)}: Size is 30mm $\times$ 30mm $\times$ 150mm. Two magnets (18mm $\times$ 18mm) of opposite polarizations are embedded onto each of the two square sides.
            \item \textbf{TypeB ($\bm{\times1}$)} Size is also 30mm $\times$ 30mm $\times$ 150mm. Two same magnets of opposite polarization are embedded onto one long side of the block.
        \end{itemize}
            
        \begin{figure}[t]
            \begin{minipage}{0.22\textwidth}
                \centering
                \includegraphics[scale=.16]{ICRA_images/robot_camera_and_lighting.pdf}
                \caption{Robot Setup.}
                \label{fig:icra_cameras}
            \end{minipage}        
            \begin{minipage}{0.23\textwidth}
                \centering
                \includegraphics[scale=.16]{ICRA_images/magnetic_blocks.pdf}
                \caption{
                        % Magnetic blocks and their components.
                        Magnetic blocks.
                }
                \label{fig:icra_blocks}
            \end{minipage}\hfill
            \vspace{-6mm}
        \end{figure}
        
   For tracking, we use three Intel RealSense D455 \cite{d455} cameras. While the cameras can provide depth images as well as RGB, we only utilize the RGB images at a resolution of 1280 $\times$ 800 at 30fps. The produced images are used for the AprilTag tracking system \cite{apriltag} and visualization of experiments. Figure \ref{fig:icra_cameras} shows the camera placements.

    % \vspace{-3mm}
    \subsection{Direct Joint-Space Control}
        In this work we have decided to train policies for real-time (4Hz) joint velocity control of the robot arms. As opposed to Cartesian control of end-effector positions, joint-space control not only alleviates the need for inverse kinematics (IK),
        but enables policies to fully utilize the joint-space to learn optimal bi-manual behaviors that avoid collisions between the robot arms while executing the desired task.
        The policy produces joint \textit{velocity} controls at 4Hz, and we convert the joint velocity controls to 100Hz joint \textit{position} commands which are directly sent to the xArm6 controller.

    \subsection{Simulated Platform}
        We construct the simulated counterpart to our real-world setup using the Mujoco simulator~\cite{mujoco2012}, which we refer to as the \textit{magnetic assembly environment}. This environment contains the same set of 3 cuboid blocks of the two different types (TypeA and TypeB, Fig~\ref{fig:three_block_task}), where positive and negative magnets are rendered as red and blue respectively.
        % , are positioned on the blocks' surfaces near the ends.
        Positive and negative magnets ``snap" together when they are within 1cm and disconnect when adequate pulling force is applied.
        % \yc{I don't think we ever modeled disconnection, did we @satok?}\sk{good catch!  Currently, the disconnection is supported but it depends mujoco's default behavior.  We've not engineered a lot but still this statement is true.}
        % Magnets enable creation of arbitrarily complex composed structures from the given building blocks.
        % For the U-shape block assembly task, the problem statement in our magnetic assembly environment is simple to describe: in each episode, the agent must assemble the blocks to create a U-shape using two TypeA blocks and one TypeB block.
        % When training RL policies, each episode begins with all blocks randomly scattered on the ground.
        In each training episode, the simulated environment starts with the blocks randomly dispersed on the ground, and the robots are reset to predefined initial poses as shown in Figure \ref{fig:three_block_task}, top.
        Episodes are 200 environment steps long, translating to a length of 50 seconds in the real world.
        
        At each stage of the assembly process, we assign each gripper $g_i$ to a particular block $b_i$ that it must manipulate, and the two arms must connect the two corresponding blocks using the correct magnet pairs. The assignments are scripted and dependent on which magnets are currently connected.
        We made design choices in the RL reward function (a function of the current state, $s_t$) to elicit the connecting behavior, and the following three additive terms (each weighted by $w$'
        s) were effective for this goal.
        % We design the RL reward function (a function of the current state, $s_t$)  based on the sum of following major three terms, weighted by $w$'s.
        % At each stage of the assembly process, we assign each gripper to the particular block that it must manipulate.
        The first reward term is the sum of distances between each $i^{\text{th}}$ gripper's position
          %\yc{(the $i^{\text{th}}$ gripper's position)}
          $p_{g_i}\in\mathbb{R}^3$ and its target block's center-of-mass position $p_{b_i}\in\mathbb{R}^3$:
        % \sg{maybe use norm where it applies? what are i, j indexing? put $\sum_{i=...}$ to tell which summmation is for which index, and what index represents}
        \vspace{-2mm}
        \begin{equation}
            %r_{b}(s_{t}) = -a \times [\sum_{0}^3 abs(p_{i}-q_{i})^2 ]^{1/2}
            r_{grasp}(s_t) = -w_g \sum_{i=0}^1\norm{ p_{g_i} - p_{b_i}}_2.
            \vspace{-2mm}
        \end{equation}
        The second and third reward terms are for aligning two correct magnet pairs. For a desired connection $j$, let the positive and negative magnet positions be denoted by $p_j, q_j \in \mathbb{R}^3$, and their rotation matrices denoted by $u_j, v_j \in \mathbb{R}^{3\times3}$ respectively: %\ps{(what are positive and negative site positions?)}
        \vspace{-2mm}
        \begin{equation}
            r_{mpos}(s_{t}) = -w_{mp} \sum_{j=0}^1 \norm{ p_{j}-q_{j} }_2 
            \vspace{-7mm}
        \end{equation}
         \begin{equation}   
            r_{mrot}(s_{t}) = -w_{mr} \sum_{j=0}^1(1.0 - \arccos(u_j[2] \cdot v_j[2])).
            \vspace{-2mm}
        \end{equation}
        % The target block to grasp for each gripper ($p_{b,i}$) is scripted and changes based on which magnet is already connected. %\ps{(It will good to clarify if that is learnt or scripted?)}

        % \sg{Satok-san, can you add a math equation for the reward function?}\sk{gotcha}
        
    \subsection{Environment Observation and Action}
        When designing observations to provide to agents, we have taken the effort to ensure the same observations are available in both simulation and the real world.
        \paragraph{Block Observations}
        We use very minimal block observations, namely the global position and orientation (represented as a 3x3 rotation matrix) for all blocks. In the real world environment, we simply transform tracking results from our AprilTag tracking system for the blocks to match our simulator's coordinate system, without using any additional statistical estimation filtering, and directly feed them as input to the policy.
        \paragraph{Robot Observations}
        For each robotic arm, we include as observations: current joint positions, current end-effector positions, target joint positions in the previous timestep, and the extent of gripper opening width
        % (\yc{gripper opening width?})
        in the previous timestep. In the real world environment, these states are obtained from the robot controllers.
        % The end-effector positions are transformed to match the simulator's coordinate system before being fed into the policy.
        
        % \paragraph{Frame Stacking}
        To improve tolerance to noises incorporated into the simulator (Section~\ref{sec:perception_and_actuation_noise}), we form the complete environment observations by concatenating the observations from current and previous timesteps.
        % , append the observation from 1 previous environment step.
    
        \paragraph{Robot Action}
        Our agent's action consists of joint velocities and gripper opening width for two arms.
        
        % \yc{Should we specify the dimension of each of the above observations/actions in this section (E)?}\sk{yeah let's addres if we have space (moved to TODO)}
        
        % \subsection{Task Reward Function}
        % We used the following reward function as task reward for simulation environment. The reward is shaped such that the resulting behavior accomplishes the task, while looking natural and realistic.
        
        
        