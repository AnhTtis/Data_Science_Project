%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%%% Small single column format, used for CIE, CSUR, DTRAP, JACM, JDIQ, JEA, JERIC, JETC, PACMCGIT, TAAS, TACCESS, TACO, TALG, TALLIP (formerly TALIP), TCPS, TDSCI, TEAC, TECS, TELO, THRI, TIIS, TIOT, TISSEC, TIST, TKDD, TMIS, TOCE, TOCHI, TOCL, TOCS, TOCT, TODAES, TODS, TOIS, TOIT, TOMACS, TOMM (formerly TOMCCAP), TOMPECS, TOMS, TOPC, TOPLAS, TOPS, TOS, TOSEM, TOSN, TQC, TRETS, TSAS, TSC, TSLP, TWEB.
% \documentclass[acmsmall]{acmart}

%%%% Large single column format, used for IMWUT, JOCCH, PACMPL, POMACS, TAP, PACMHCI
% \documentclass[acmlarge,screen]{acmart}

%%%% Large double column format, used for TOG
% \documentclass[acmtog, authorversion]{acmart}

%%%% Generic manuscript mode, required for submission
%%%% and peer review
%\documentclass[manuscript,screen,review]{acmart}
%\documentclass[manuscript]{acmart}
%\documentclass[manuscript,authorversion,noacm]{acmart}
%\documentclass[sigconf,authorversion,noacm]{acmart}
\documentclass[sigconf,authorversion]{acmart}
%\documentclass[sigconf]{acmart}

%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.

%\setcopyright{acmcopyright}
%\copyrightyear{2023}
%\acmYear{2023}
%\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[IUI '23]{}{March 27--31, 2023}{Sydney, Australia}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{28th International Conference on Intelligent User Interfaces (IUI ’23), March 27--31, 2023, Sydney, Australia} 
%\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}


\copyrightyear{2023} 
\acmYear{2023} 
\setcopyright{acmlicensed}\acmConference[IUI '23]{28th International Conference on Intelligent User Interfaces}{March 27--31, 2023}{Sydney, NSW, Australia}
\acmBooktitle{28th International Conference on Intelligent User Interfaces (IUI '23), March 27--31, 2023, Sydney, NSW, Australia}
\acmPrice{15.00}
\acmDOI{10.1145/3581641.3584052}
\acmISBN{979-8-4007-0106-1/23/03}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{cleveref}
%\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem, hyperref}
\usepackage{siunitx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.

\title[The Effect of AI Delegation on Human Task Performance and Task Satisfaction]{Human-AI Collaboration: The Effect of AI Delegation on Human Task Performance and Task Satisfaction}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Patrick Hemmer}
\affiliation{%
  \institution{Karlsruhe Institute of Technology}
  \city{Karlsruhe}
  \country{Germany}
  }
\email{patrick.hemmer@kit.edu}

\author{Monika Westphal}

\affiliation{%
 \institution{Ben-Gurion University of the Negev}
 \city{Be'er Sheva}
 \country{Israel}
 }
\email{monika.westphal@post.bgu.ac.il}


\author{Max Schemmer}
\affiliation{%
  \institution{Karlsruhe Institute of Technology}
  \city{Karlsruhe}
  \country{Germany}
  }
\email{max.schemmer@kit.edu}


 
 \author{Sebastian Vetter}
\affiliation{%
  \institution{Karlsruhe Institute of Technology}
  \city{Karlsruhe}
  \country{Germany}
}
\email{sebastian.vetter@alumni.kit.edu}


\author{Michael Vössing}
\affiliation{%
  \institution{Karlsruhe Institute of Technology}
  \city{Karlsruhe}
  \country{Germany}
  }
\email{michael.voessing@kit.edu}


\author{Gerhard Satzger}
\affiliation{%
  \institution{Karlsruhe Institute of Technology}
  \city{Karlsruhe}
  \country{Germany}
  }
\email{gerhard.satzger@kit.edu}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.

\renewcommand{\shortauthors}{Hemmer et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Recent work has proposed artificial intelligence (AI) models that can learn to decide whether to make a prediction for an instance of a task or to delegate it to a human by considering both parties' capabilities. In simulations with synthetically generated or context-independent human predictions, delegation can help improve the performance of human-AI teams---compared to humans or the AI model completing the task alone. However, so far, it remains unclear how humans perform and how they perceive the task when they are aware that an AI model delegated task instances to them. In an experimental study with 196 participants, we show that task performance and task satisfaction improve through AI delegation, regardless of whether humans are aware of the delegation. Additionally, we identify humans' increased levels of self-efficacy as the underlying mechanism for these improvements in performance and satisfaction. Our findings provide initial evidence that allowing AI models to take over more management responsibilities can be an effective form of human-AI collaboration in workplaces.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003121.10011748</concept_id>
       <concept_desc>Human-centered computing~Empirical studies in HCI</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178</concept_id>
       <concept_desc>Computing methodologies~Artificial intelligence</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Empirical studies in HCI}
\ccsdesc[500]{Computing methodologies~Artificial intelligence}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Human-AI Collaboration, AI Delegation, Task Performance, Task Satisfaction, Self-efficacy}



%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}


Over the last few years, the capabilities of artificial intelligence (AI) have undergone considerable technical advances. Nowadays, the performance of AI models is similar to, and in certain application areas even exceeds the performance of human experts \cite{brown2019superhuman,he2015delving,silver2018general}. For example, in the medical domain, AI models can detect certain diseases as accurately as radiologists \cite{esteva2017dermatologist,gulshan2019performance,irvin2019chexpert}. Yet, despite these impressive advances, human predictions often remain more accurate for certain cases \cite{geirhos2021partial,wilder2020learning}. On the one hand, this may be due to limited model capacity, limited training data, or outliers unknown to the AI model. On the other hand, humans might have access to side information that is not readily available to the AI model, enabling them to make more accurate decisions for particular cases \cite{hemmer2022effect}. 
\begin{figure*}%
    \centering
    \subfloat[\centering Training of the AI model.]{{\includegraphics[width=0.48\textwidth]{Figure1.pdf} }}%
    %\qquad
    \subfloat[\centering Deployment of the AI model.]{{\includegraphics[width=0.48\textwidth]{Figure2.pdf} }}%
    \caption{A schematic overview of the AI model. During training (a), the AI model learns to make a prediction for a task from the available ground truth labels. Additionally, human predictions allow the AI model to learn the capabilities of humans simultaneously. After deployment (b), the AI model decides to make a prediction or to delegate an instance to the human, depending on whether the AI model or the human is expected to make a correct prediction with higher probability.}%
    \label{fig:example_ai_delegation}%
    \Description{A schematic overview of the AI model. During training (a), the AI model learns to make a prediction for a task from the available ground truth labels. Additionally, human predictions allow the AI model to learn the capabilities of humans simultaneously. After deployment, (b) the AI model decides to make a prediction or to delegate an instance to the human, depending on whether the AI model or the human is expected to make a correct prediction with higher probability.}
\end{figure*}
These potentially complementary capabilities motivated researchers to investigate how the abilities of humans and AI models can be combined to further improve overall decision-making performance \cite{Bansal2021,hemmer2schemmer021}. 

One noteworthy form of collaboration is the delegation of instances to a human by the AI model (i.e., AI delegation) \cite{leitao2022human}. Figure \ref{fig:example_ai_delegation} provides a schematic overview of AI delegation. This approach is particularly beneficial in application areas where tasks can be completed independently by both humans and AI models (e.g., crowdworking tasks like image recognition or content moderation) \cite{Desmond_2022,Lai2022}. AI delegation could also be used in high-stakes decision-making domains (e.g., medicine) to reduce the workload of medical experts. For instance, in the context of cancer screening, the AI model can be used to identify simple cases so that the medical experts can focus on the complex cases delegated to them \cite{bilal2022ai}. Several recent works propose approaches that enable the AI model to delegate a subset of instances to a human while taking both its own and the human's capabilities into consideration \cite{ijcai_multiple_experts,keswani2021towards,mozannar2020consistent,okati2021differentiable,Raghu2019,wilder2020learning}. One way to achieve this is to estimate both the AI model and the human prediction confidence on an instance basis and to delegate each instance to the team member with the higher estimated prediction confidence \cite{Raghu2019}. Generally, these works assume that the behavior and perceptions of humans, and thus their decision-making performance, remain unchanged whether or not an AI model delegates instances of a task. Previous research has demonstrated the potential of these approaches in experiments with either synthetically generated human predictions or with predictions that were collected in annotation settings without any AI involvement. However, human behavior might deviate when teaming up with an AI model. For example, humans' attitudes towards AI, their experience with algorithms, or exposure to an AI model that determines their task are factors that can influence their decision-making performance \cite{Bondi2022,dietvorst2015algorithm,Rastogi2022}. Thus, it remains an open question of how and why humans' performance is affected when the task is determined by an AI model. Following this, the first goal of this study is to investigate the effect of AI delegation on human task performance and to explore what drives this effect. 

Besides performance, the collaboration with an AI model that determines the task for humans might also have an effect on their perception of the work and the nature of the task. Human task satisfaction plays an increasingly important role in today's workplaces. In particular, humans' satisfaction with their work determines key organizational outcomes, e.g., commitment to or productivity of an organization, and is thus decisive for its long-term success \cite{Gerdenitsch2017,sadeghian2022artificial}. By delegating instances to the human, the AI model determines the nature of the task the human has to conduct, potentially affecting their satisfaction. Therefore, we investigate the effect of delegation by an AI model on human task satisfaction besides task performance, as well as what drives this effect (i.e., the underlying mechanism). We hypothesize improvements in both task performance and task satisfaction following AI delegation. Further, we expect increases in self-efficacy, i.e., a person's belief in one's own ability to complete a task successfully \cite{Bandura1977} to explain these positive effects. To summarize, in this work, we pose the following three research questions:
\begin{itemize}
    \item[\textbf{RQ1:}] \textit{How does AI delegation affect task performance compared to a human and an AI working alone?}
    \item[\textbf{RQ2:}] \textit{How does AI delegation affect task satisfaction compared to a human working alone?}
    \item[\textbf{RQ3:}] \textit{What explains the effect of AI delegation on task performance and task satisfaction?}
\end{itemize}

To answer these research questions, we conduct a randomized experiment with 196 participants recruited online via Prolific. Participants are asked to complete an image classification task based on a modified subset of the ImageNet data set \cite{ILSVRC15,Steyvers2022}. We select this task because it does not require any task-specific training to achieve similar performance compared to modern AI models \cite{Steyvers2022}. We employ an AI model that learns to classify images and simultaneously estimates the instance-specific human classification confidence that is compared with the confidence of the classifier. Instances are delegated if the estimated human confidence is higher than the confidence of the classifier \cite{Raghu2019}. The experiment includes two ``delegation'' treatment groups that receive a randomly drawn subset of the images in the test set that the trained AI model had selected for delegation to humans. In one of the groups, humans are aware of the AI delegation taking place, while in the other, humans classify the same images without knowing about the AI delegation. We investigate the effect of AI delegation on task performance and task satisfaction in these two delegation groups compared to a control group (i.e., ``human-alone''), where humans classify a subset of images randomly selected from the test set. In addition, we compare the performance of these groups with the performance of the AI model if it had conducted the task alone. We find that humans' performance increases significantly for the instances delegated by the AI model, which results in an overall team performance exceeding the performance of both humans and the AI conducting the task independently. Additionally, we find that humans' task satisfaction increases significantly when they work on the delegated set of instances. Both effects can be explained by an increase in humans' self-efficacy. Interestingly, we find no differences in human task performance and task satisfaction, regardless of whether humans are informed about the AI delegation. Thus, we can conclude that the modified nature of the task drives the observed positive effects of delegation through the AI model. All these findings show the potential of AI delegation as an appropriate form of collaboration between humans and AI.

To summarize, our contributions are as follows: (1) We propose a behavioral model to analyze the effect of AI delegation on human task performance and task satisfaction in human-AI collaboration. (2) We validate our model in a randomized experiment with human participants and show that their performance and task satisfaction are improved through the delegation of instances by the AI model. Moreover, we show that the overall team performance surpasses the individual performance of both team members working alone. (3) We identify self-efficacy as an underlying mechanism to the effect of AI delegation on task performance and on task satisfaction.


\section{Related work}

The collaboration between humans and AI models can be instantiated in different ways. One of the most common collaboration forms between humans and AI is AI-assisted decision-making---a setting in which an AI model provides recommendations to support the human. The human is in the role of making the final decision and can, therefore, either accept or override the recommendation \cite{Schemmer2022_meta,wang2021explanations}. Establishing an appropriate level of reliance on the AI model becomes one of the central challenges \cite{schemmer2022should}. Thus, the AI model often provides the confidence level of the decision \cite{nguyen2021effectiveness,zhang_efect_2020} or an additional explanation for its decision \cite{adadi_peeking_2018,lai_human_2019}. Several works have evaluated whether different types of explanations can support humans' understanding of the AI model so that they identify the right cases to rely on the recommendations \cite{alufaisan_does_2020,bucinca_proxy_2020,carton_feature-based_2020,van_der_waa_evaluating_2021}. Explanations can lead people to rely too much on the decision of the AI model, particularly when its suggestion is incorrect \cite{Bansal2021}. This over-reliance also depends on the humans' level of task-specific expertise. For example, people with higher task expertise become more confident in overruling the recommendation of the AI model \cite{feng2019can}. Wrong predictions, recognized as such by humans, can lead to a loss of trust in the system \cite{dietvorst2015algorithm,nourani2020role}. Recent research investigates other factors that might play an essential role in AI-assisted decision-making, e.g., whether humans' performance benefits from receiving tutorials about the functionality of the AI model \cite{lai_why_2020} or whether specific design elements can foster people's engagement with AI explanations \cite{bucinca_trust_2021}.

Besides AI-assisted decision-making, a different type of human-AI collaboration has attracted increasing interest in research over the past few years---delegation initiated by the AI model (i.e., AI delegation) \cite{leitao2022human}. The AI model learns to decide whether to make a prediction itself for a given task instance or to delegate it to a human. In application domains with a high number of individual decisions delegating instances of a task can reduce human effort and improve overall performance. Instances are distributed to the team member who is most likely to make the correct decision. Typically, these approaches take not only their own but also the capabilities of the humans into consideration \cite{ijcai_multiple_experts,keswani2021towards,mozannar2020consistent,okati2021differentiable,Raghu2019,wilder2020learning}. The AI model learns the strengths and weaknesses of the human team member from human predictions used during model training in addition to the ground truth labels. Such individual human predictions are noisy compared to the ground truth labels. The latter are typically determined by experts or multiple individual human predictions to ensure high label quality \cite{Kerrigan2021}. Different algorithms have been proposed that can either complement the capabilities of a single \cite{mozannar2020consistent,okati2021differentiable,Raghu2019,wilder2020learning} or multiple \cite{ijcai_multiple_experts,keswani2021towards} humans who are part of the human-AI team. So far, these approaches have solely been evaluated with synthetically generated or context-independent human predictions that were collected in annotation settings without any AI involvement. However, human predictions might deviate when they are aware of the AI delegation taking place, e.g., due to their attitude or prior experience with algorithms or due to being attributed with particular competence by the AI model that takes on the role of a manager \cite{Bondi2022,dietvorst2015algorithm}. Therefore, it remains an open question whether humans' individual performance and the overall team performance in real-world settings would benefit from delegation algorithms that consider both parties' capabilities. Furthermore, research has so far neglected the possible effect on humans’ perceptions of being managed by the AI model, e.g., expressed through task satisfaction. However, task satisfaction plays a central role in workplaces where people are increasingly exposed to working with AI models, especially when they decide on the task a human has to complete. Research lacks an understanding of the underlying mechanisms of the effect of AI delegation on task performance and task satisfaction in human-AI collaboration. Only \citet{Bondi2022} and \citet{fuegener2022cognitive} investigated AI delegation in behavioral experiments. However, these studies differ in two ways from the current study: First, the algorithms used for delegation do not learn the capabilities of the humans. Second, they do not investigate humans' perceptions when the AI model delegates task instances, nor do they aim to understand the underlying mechanisms driving the effects on task performance and task satisfaction.  




\section{Theory Development and Hypotheses}
\label{sec:Theoretical development}

So far, AI models that learn to decide whether to make a prediction themselves or to delegate an instance to a human have been evaluated with synthetically generated or context-independent human predictions \cite{ijcai_multiple_experts,keswani2021towards,mozannar2020consistent,okati2021differentiable,Raghu2019,wilder2020learning}. However, humans' behavior may deviate when they are aware that they are part of a human-AI team \cite{Bondi2022,dietvorst2015algorithm}. It remains unclear how such a team setting would affect human performance and other individual task outcomes (e.g., task satisfaction) in real-world settings. Furthermore, it is not yet known, why AI delegation may affect individual task outcomes. In this study, we examine how AI delegation affects task performance and task satisfaction, considering self-efficacy as a possible underlying mechanism. We draw upon experimental studies in organizational behavior literature on the effect of supervisor-to-employee delegation, and its relation to task performance, satisfaction, and self-efficacy \cite{Locke1984,schriesheim1998delegation,shore1989job}. Based on this literature, we develop four hypotheses that are subsequently tested in an experimental study. 

Research in organizational behavior suggests that delegation from a supervisor to an employee can serve multiple purposes. For example, supervisors delegate due to a lack of time, missing competencies, or to empower employees for their personal development \cite{bass1990bass,schriesheim1998delegation}. Several works identified a positive relationship between supervisor delegation and employee performance \cite{al2015impact,Leana1986,leana1987power,xiong2007delegation}. When aligned with the employees' competencies, delegation results in more empowered, motivated, and higher-performing employees \cite{ugoani2020}. We transfer these insights to the modern context of human-AI collaboration. We propose a positive effect of AI delegation in human-AI collaboration, given the AI model learns the strengths and weaknesses of the human collaborator:
 

\begin{itemize}
    \item[\textbf{H1:}] \textit{AI delegation improves human task performance compared to an AI and a human working alone.}
\end{itemize}


Organizational behavior research has investigated employee job satisfaction as another important factor besides performance; precisely because it determines key organizational outcomes such as employee organizational commitment and turnover \cite{shore1989job}. Several works identified delegation as positively related to employees' job satisfaction \cite{farrow1980comparison,ugoani2020,xiong2007delegation}. For example, \citet{schriesheim1998delegation} found that perceived delegation by employees improved their intrinsic and extrinsic job satisfaction. We take these insights to the modern context of AI delegation in human-AI collaboration and propose the following effect, given the AI model delegates instances to the humans that align with their competencies: 

\begin{itemize}
    \item[\textbf{H2:}] \textit{AI delegation improves human task satisfaction compared to a human working alone.}
\end{itemize}


Besides examining the direct effect of AI delegation on task performance and task satisfaction, we aim to understand why these proposed effects occur. We investigate self-efficacy as a potential underlying mechanism. Self-efficacy refers to the confidence in one's own ability to complete a task successfully \cite{Bandura1977}. Again drawing upon experimental studies in organizational research, we find that delegation from a supervisor to an employee enhances psychological empowerment \cite{zhang2017leaders}. In other words, delegation makes employees feel that their job is meaningful and that they are responsible for work outcomes. We are not aware of any study showing that delegation increases self-efficacy. Still, many studies are pointing to the role of (increased) self-efficacy in improving organizational performance-related outcomes. For example, self-efficacy influences learning and the level of effort put into work \cite{lunenburg2011self}. Further, self-efficacy predicts several work-related performance outcomes \cite{stajkovic1998self, Locke1984}. In a learning environment, self-efficacy correlates with increased task performance \cite{Afzal2019}. Besides performance, self-efficacy improves job satisfaction through higher meaningfulness \cite{baumeister2002,gecas1982}. Organizations should select potential employees based on their self-efficacy levels; employees with high self-efficacy levels are more motivated and more likely to yield desired outcomes for the organization \cite{lunenburg2011self}. Self-efficacy has important implications for organizational behavior and human resource management \cite{gist1987self}. 

In the current study, we are interested in understanding self-efficacy in a modern work context---human-AI collaboration, where an AI model delegates task instances. The hope is to yield higher levels of task performance and task satisfaction. Based on the findings of experimental studies in organizational behavior research mentioned above, we propose an additional set of hypotheses:

\begin{itemize}
    \item[\textbf{H3:}] \textit{Self-efficacy mediates the effect of AI delegation on human task performance. In particular, AI delegation increases self-efficacy, and this increased self-efficacy improves task performance compared to a human working alone.}
\end{itemize}

\begin{itemize}
    \item[\textbf{H4:}] \textit{Self-efficacy mediates the effect of AI delegation on human task satisfaction. In particular, AI delegation increases self-efficacy, and this increased self-efficacy improves task satisfaction compared to a human working alone.}
\end{itemize}


Figure \ref{Fig:research_model} provides an overview of our research model and proposed effects:


\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{hypotheses.pdf}
  \caption{Research model: Proposed effects of AI delegation on human task performance and satisfaction.}
  \label{Fig:research_model}
  \Description{Research model: Proposed effects of AI delegation on human task performance and satisfaction.}
\end{figure}

As stated in the hypotheses, we compare the task outcomes for humans working on instances delegated by an AI (i.e., AI delegation) to two control groups where no delegation takes place (i.e., humans working alone; AI working alone). 

In addition to the proposed effect of AI delegation on task performance and task satisfaction, we examine another exploratory research question: Does AI delegation affect task performance and task satisfaction differently when the delegation is \textit{not} communicated to the human? In other words, the human works (only) on the instances delegated by the AI model but is not informed about it. To test this exploratory research question, we include a second delegation group in the design, the ``hidden delegation'' group. Though the task experience might be similar to when working alone, we think the human might experience the task more positively because the delegation algorithm works well by delegating the right instances. The question is whether humans will experience the task even more positively than those in the delegation group who are informed about the delegation. In any case, we expect to see a positive effect of AI delegation on task performance and task satisfaction. Next, we outline how we tested our propositions in an experimental study.


\section{Methodology}

In this section, we first provide information about the data we used. Then, we describe the development of the AI model. Finally, we present the experiment that we conducted to test the hypotheses. 

\subsection{Data}\label{sec:data}

We used the image data set provided by \citet{Steyvers2022} for our study. The data set is a subset of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 database \cite{ILSVRC15}. It consists of 1,200 images equally balanced over 16 classes, e.g., airplane, bear, or boat. Additionally, phase noise distortion was applied to the images at each spatial frequency, uniformly distributed in the interval \([\omega,\omega]\) with \(\omega=110\) to increase the difficulty of the classification task both for humans and the AI model. Despite the increased difficulty level, both humans and the AI model can achieve a similar performance level on the task. We refer to \citet{Steyvers2022} for additional details. We chose this data set as a test bed for our proposed behavioral model for multiple reasons: First, it includes a generic, non-specialized task that can be conducted by non-specialized participants. Hereby, we aim to ensure a certain degree of generalizability of the results. Second, in addition to the ground truth labels, the data set provides multiple human predictions for each image collected from 145 Amazon Mechanical Turk workers. Thus, it fulfills our requirement that the AI model can learn the humans' strengths and weaknesses. 

We prepared the data set by randomly selecting a human prediction for each image that is subsequently used together with the ground truth labels for the training of the AI model. We divided the data set into a training, validation, and test set, with 60\%, 20\%, and 20\% of the data, respectively. 

\begin{figure*}[h]
    \centering
    \subfloat[\centering Exemplary image of the classification task.]{{\includegraphics[width=0.46\textwidth]{Task_Screenshot1.pdf}
    }\label{Fig:task_a}}%
    %\qquad
    \subfloat[\centering Exemplary additional image classified by the AI model.]{{\includegraphics[width=0.46\textwidth]{Task_Screenshot2.pdf}
    }\label{Fig:task_b}}%
    \caption{Interface of the image classification task, exemplified by the delegation condition. Participants were informed that the AI model delegated the respective image to them. Additionally, between the individual instances, participants saw images that the AI model had already classified and, thus, did not delegate to them.}%
    \label{Fig:task}%
    \Description{Interface of the image classification task, exemplified by the delegation condition. Participants were informed that the AI model delegated the respective image to them. Additionally, between the individual instances, participants saw images that the AI model had already classified and, thus, did not delegate to them.}
\end{figure*}


\subsection{Development of the AI Model}
\label{sec:development}

For the AI model, we implemented the approach proposed by \citet{Raghu2019}. It consists of two components: First, a classification model that learns the image classification task. Second, a human error model that learns to predict whether humans would classify an instance correctly based on the provided human predictions. The delegation decision is made based on the instance-level confidence of both components. If the human error model has higher confidence than the classification model, the instance is subsequently delegated to the human. Both the classification and human error models consist of a DenseNet-161 \cite{huang2017} pre-trained on ImageNet. We fine-tuned both models on the distorted images over 100 epochs using SGD as an optimizer with a learning rate of \(1 \cdot 10^{-4}\), weight decay of \(5 \cdot 10^{-4}\), a cosine annealing learning rate scheduler and a batch size of 16. Additionally, we applied early stopping on the validation loss.



\subsection{Experimental Design}
\label{sec:experimental design}

To test the effect of AI delegation on human performance and satisfaction, as well as self-efficacy (see Hypotheses 1--4), we conducted a web-based experiment. Next, we outline the experimental design: participants, study procedure, and evaluation measures.

\subsubsection{Participants}

We calculated the required sample size using G-Power \cite{faul2007g} and assumed a small effect (0.10). Accordingly, 176 participants were necessary to detect effects between three groups in a multiple linear regression (including three predictors, fixed model, R$^2$ deviation from zero), with a power of 0.95. As it is common for some participants to fail attention or manipulation checks or drop out of the study, we recruited a larger number of participants (roughly 15\% on top of the calculated number). Following this, we recruited 210 participants online via Prolific Academic. Participants received \$1.5 for their participation in the task that took approximately 10 minutes. We excluded 13 participants because they failed the attention or manipulation check, and an additional participant due to missing data. Hence, our final sample was 196 participants ($Mean=39.43$ years, $SD=13.13$; 58.67\% female).


\subsubsection{Study Procedure}

At the beginning of the study, participants were asked to perform an ``unrelated task'' that estimated their cognitive ability to handle visual cognitive tasks. We included this variable as a control in our analyses. Next, participants had to pass an attention check. Following that, they started the practice round of the main task: We asked them to classify three images, one after another, to familiarize themselves with the task. Participants had to choose from a four-by-four matrix including 16 icons of the objects, each representing a different class, with the name of the class displayed underneath the icon (e.g., dog, airplane, truck). They saw the three images in random order and the 16 icons of the objects in alphabetic order. We chose the images randomly based on the test set as outlined in Section \ref{sec:data}. After the practice round, participants proceeded to the main task. They were asked to classify another 20 images. We randomly assigned them to three experimental conditions. 

In the \textit{delegation} condition, we informed them that ``this time, the AI will decide for each image whether to label it alone or to pass it on to you for labeling''. The 20 images the participants had to label were randomly drawn from the subset of images in the test set that the trained AI model had selected for delegation to humans. Moreover, we included five additional images and communicated that the AI had already labeled this image and that they could proceed to the next image. We neither mentioned the accuracy of the AI nor revealed the ground truth itself. Figure \ref{Fig:task} displays the interface presented to the participants. In detail, Figure \ref{Fig:task_a} shows an exemplary instance that was delegated to the participants by the AI model. Figure \ref{Fig:task_b} depicts one of the five additional images notifying the participants that the AI model has already labeled it. 

We included a second delegation condition in the design---the \textit{hidden delegation} condition. Just as in the delegation condition, participants were asked to label 20 images randomly drawn from the subset of images in the test set that the trained AI model had selected for delegation to humans. But this time, we did not communicate the delegation to the participants; we just told them: ``Just as previously, you will decide on the label for each image''. 

The third condition---the \textit{human-alone} condition---represented the control condition. Participants received the same information as in the \textit{hidden delegation} condition about the task. However, the 20 images were randomly drawn from the entire test set. 

Once participants had classified the 20 images, they responded to several follow-up questions that measured self-efficacy and task satisfaction, as well as recorded demographics and included a manipulation check. 



\subsubsection{Evaluation Measures}\label{sec:evaluation_measures}

We measured the following variables to evaluate the effect of AI delegation: 

\textbf{\textit{Task performance.}} As instances are equally distributed over the 16 classes, task performance was measured by the percentage of correctly classified images, i.e., classification accuracy (\(acc\)). To assess the human performance, we calculated this measure for all three experimental conditions. Regarding the AI model, we calculated its performance on the test set and on the set of instances delegated to the humans.

Besides individual task performance, we were interested in the combined human-AI team performance. Hence, we also calculated the performance of the AI model on the subset of the test set not delegated to the humans (\(acc_{AI,\,\neg delegated}\)). We determined the team performance for each of the \(N\) participants in the delegation group by weighting their individual performance (\(acc_{human, \, delegated}\)) by the ratio of delegated images in the test set \(X\). Then, we combined it with the performance of the AI model weighted by the ratio of not delegated images in the test set \(1-X\). Lastly, we calculated the average team performance of all participants in the group. We refer to Equation \ref{eq:ctp} for this procedure:
\begin{equation}
    acc_{team} = \frac{1}{N} \sum_{i=1}^{N} \bigl( acc_{human,\,delegated}^{(i)} \cdot X + acc_{AI,\,\neg delegated} \cdot (1-X) \label{eq:ctp} \bigl)
\end{equation}

\textit{\textbf{Task satisfaction.}} We measured task satisfaction using three items based on \citet{Hofmann1995} on a validated, 5-point Likert scale (1 - `not at all' to 5 - `totally'). The three items were: 'Overall, how satisfied are you with your performance on this task?', `Overall, how satisfied are you with how much you learned?', and `Overall, how much did you enjoy performing this task?'. Cronbach's Alpha was 0.73 (sufficient).

\textit{\textbf{Self-efficacy.}} We measured self-efficacy using three items adapted from \citet{spreitzer1995} on a validated, 7-point Likert scale (1 - `I totally disagree' to 7 - `I totally agree'). The three items were: `I am confident about my ability to do the task.', `I have mastered the skills necessary for the task.', and `I am self-assured about my capabilities to perform the task.'. Cronbach's Alpha was 0.89 (good).

\textbf{\textit{Control variables.}}
We assessed the \textit{cognitive ability} to handle complex visual tasks, based on four items by \citet{Jacobs2014}. Participants were asked how easy or difficult they perceive certain tasks, compared to others of the same age, and evaluated the following four statements on a validated, 7-point Likert scale (1 - `extremely difficult' to 7 - `extremely easy'): `interpret visually displayed information', `understand information presented in a visual format', `imagine what an object would look like from a different angle', and `mentally rotate three-dimensional images in my mind'. Cronbach's Alpha was 0.86 (good). Besides \textit{cognitive ability}, we recorded participants' \textit{task experience}, \textit{algorithm attitude}, \textit{algorithm use}, \textit{education}, \textit{age}, and \textit{gender}.



\textbf{\textit{Manipulation checks.}}
We included the following statement in the description of the main task to make sure that participants in the delegation condition were aware of the delegation taking place: `Please show us that you have read the task description above by choosing the right response': (a) `Next, I will label all images alone (just as in the practice round)', (b) `either the AI or I will label the image', or (c) `the AI will label all images'. Participants were included in the analysis if they chose (b). Additionally, to make sure participants paid attention to the condition they were assigned to, we asked them at the end of the study whether (a) they `labeled all images alone, just as in the practice round', or (b) `an AI passed some of the images on to them for labeling', or (c) they `don't remember'. Only participants who chose (a) and were indeed either in the hidden delegation or the human-alone condition and participants who chose (b) and were indeed in the delegation condition were included in the final sample.

\begin{figure*}[t]
    \centering
    \resizebox{\textwidth}{!}{%
    \subfloat[\centering Task Performance.]{{\includegraphics[width=0.42\textwidth]{results_performance_experimental_conditions_significance.pdf}
    }\label{Fig:performance}}%
    %\qquad
    \subfloat[\centering Task Satisfaction.]{{\includegraphics[width=0.42\textwidth]{perception_self_task_satisfaction_significance.pdf}
    }\label{Fig:satisfaction}}%
    \subfloat[\centering Self-efficacy.]{{\includegraphics[width=0.42\textwidth]{perception_self_efficacy_significance.pdf}
    }\label{Fig:self_efficacy}}%
    }
    \caption{Task performance, task satisfaction, and self-efficacy of the participants, split by conditions. All bars include 95\% confidence intervals. Note: $^{***}$ $p<0.001$; $^{**}$ $p<0.01$; $^{*}$ $p<0.05$.}%
    \label{Fig:results_performance}%
    \Description{Task performance measured in accuracy, task satisfaction, and self-efficacy of the participants split by conditions. All bars include 95\% confidence intervals. Note: $^{***}$ $p<0.001$; $^{**}$ $p0<.01$; $^{*}$ $p<0.05$.}
\end{figure*}

\section{Results}
\label{sec:results}
\subsection{Statistical Specification} 
Our experiment examined four hypotheses that we developed in Section \ref{sec:Theoretical development}. The first set of hypotheses regarded the effect of AI delegation on task performance (H1) and task satisfaction (H2). The second set of hypotheses regarded the mediating role of self-efficacy in the effect of AI delegation on task performance (H3) and task satisfaction (H4). To test Hypotheses 1 and 2 (direct effect of AI delegation), we ran a univariate regression analysis that predicted task performance, and another one that predicted task satisfaction. To test Hypotheses 3 and 4 (indirect effect of AI delegation), we ran a mediation analysis for each of the two outcomes, using PROCESS \cite[model no.\ 4,][]{Hayes2017IntroductionApproach}, and including the mediation indices. 

In all analyses, we included all control variables, i.e., participants' task experience, algorithm attitude, algorithm use, cognitive ability, education, age, and gender. 





\subsection{Effect of AI Delegation on Task Performance}

The overall regression model---testing for the direct effect of AI delegation on task performance---is significant, $F(9,186)=11.817, R^2=0.364, p<0.001$. As hypothesized, participants in both the delegation group and hidden delegation group yield higher levels of task performance ($Mean=84.51\%, SD=11.24, p<0.001$ and $Mean=83.73\%, SD=12.29, p<0.001$, respectively), compared to humans working alone ($Mean=67.13\%, SD=13.11$). A Tukey post hoc test reveals no significant difference in task performance between the two delegation groups ($p=0.932$). Figure \ref{Fig:performance} displays the performance results of all three groups. We observe that participants with a more positive attitude towards algorithms, and those who are younger, perform better ($p=0.009$ and $p=0.008$, respectively). For an overview of the regression results, see Figure \ref{Fig:results_mediation} and Table \ref{tab:results_regression} (Columns: `Model I---Direct effect of AI delegation', `Task performance'). 



Additionally, we compare the task performance of the delegation group with the performance of the AI model if it had classified the images presented to the delegation group alone. The accuracy of the delegation group is significantly higher ($p<0.001$, one-sample, one-tailed Wilcoxon signed rank test) than the accuracy of the AI model on the delegated set ($60\%$), indicating that these instances better align with the capabilities of the participants.

As a next step, we investigate the effect of AI delegation on the overall team performance. To evaluate whether AI delegation achieves complementary team performance, we determine the human-AI team performance as described in Equation \ref{eq:ctp} (see Section \ref{sec:evaluation_measures}). The combined human-AI team performance is $80.01\%$, which is significantly higher ($p<0.001$, one-sample, one-tailed Wilcoxon signed rank test) than the performance of the AI model on the test set ($75.83\%$) and significantly higher ($p<0.001$, one-tailed Mann-Whitney U test) than the performance of the humans ($67.13\%$) working alone. 

To summarize, H1 is supported. When the AI model delegates instances to the participants, their task performance on these images improves, compared to other participants and the AI model conducting the task alone. Task performance improves for both the delegation and the hidden delegation group. The combined human-AI team performance even surpasses the team members' individual performance given they conducted the task independently. 







\begin{table*}[t] %[htb]
\caption{Regression results: Direct and indirect effect of AI delegation on task performance and task satisfaction.}
\label{tab:results_regression}
%\resizebox{15cm}{3.75cm}
%\resizebox{\linewidth}{!}
\begin{tabular}{llclcllclclc}
\toprule
Regression Model & \multicolumn{4}{c}{Model I---Direct effect of AI delegation} && \multicolumn{6}{c}{Model II---Indirect effect of AI delegation} \\
\midrule
Variable & \multicolumn{2}{c}{Task performance} & \multicolumn{2}{c}{Task satisfaction} && \multicolumn{2}{c}{Self-efficacy} & \multicolumn{2}{c}{Task performance} & \multicolumn{2}{c}{Task satisfaction} \\
\multicolumn{1}{l}{} &   \multicolumn{1}{c}{\textit{coeff}} & \multicolumn{1}{c}{\textit{se}} &  \multicolumn{1}{c}{\textit{coeff}} & \multicolumn{1}{c}{\textit{se}} && \multicolumn{1}{c}{\textit{coeff}} & \multicolumn{1}{c}{\textit{se}} & \multicolumn{1}{c}{\textit{coeff}} & \multicolumn{1}{c}{\textit{se}} & \multicolumn{1}{c}{\textit{coeff}} & \multicolumn{1}{c}{\textit{se}} \\
\midrule
Intercept & ~12.60$^{***}$ & 1.39 & ~1.86$^{***}$ & 0.39 && ~1.44$^{*}$ & 0.63 & 11.92$^{***}$ & 1.46 & ~1.21$^{***}$ & 0.34 \\
AI Delegation & & & & && & & & & & \\
- \textit{Delegation} & ~3.44$^{***}$ & 0.43 & ~0.34$^{**}$ & 0.12 && ~0.75$^{***}$ & 0.18 & ~2.97$^{***}$ & 0.43 & ~0.05 & 0.10 \\
- \textit{Hidden delegation} & ~3.40$^{***}$ & 0.41 & ~0.30$^{*}$ & 0.12 && ~0.80$^{***}$ & 0.17 & ~2.90$^{***}$ & 0.42 & -0.01 & 0.10 \\
- \textit{Human-alone} (baseline) & & & & && & & & & & \\
Self-efficacy & & && & & \multicolumn{2}{c}{} & ~0.62$^{***}$ & 0.17 & ~0.39$^{***}$ & 0.04 \\
Task experience & -0.03 & 0.29 & ~0.13 & 0.08 && ~0.08 & 0.12 & -0.08 & 0.28 & ~0.10 & 0.06  \\
Algorithm attitude & ~0.59$^{**}$ & 0.22 & ~0.10$^{\dagger}$ & 0.06 && ~0.05 & 0.09 & ~0.55$^{*}$ & 0.21 & ~0.08$^{\dagger}$ & 0.05 \\
Algorithm use & -0.31 & 0.20 & -0.09$^{\dagger}$ & 0.06 && -0.06 & 0.08 & -0.28 & 0.19 & -0.07 & 0.04  \\
Cognitive ability & ~0.09 & 0.18 & ~0.21$^{***}$ & 0.05 && ~0.46$^{***}$ & 0.07 & -0.19 & 0.19 & ~0.03 & 0.04 \\
Education & ~0.15 & 0.21 & ~0.00 & 0.06 && ~0.09 & 0.09 & ~0.09 & 0.20 & -0.03 & 0.05  \\
Age & -0.04$^{**}$ & 0.01 & ~0.00 & 0.00 && ~0.01 & 0.01 & -0.04$^{**}$ & 0.01 & ~0.00 & 0.00  \\
Gender & ~0.22 & 0.37 & -0.08 & 0.10 && ~0.17 & 0.16 & -0.32 & 0.36 & ~0.02 & 0.08  \\
\midrule
R$^{2}$& \multicolumn{2}{c}{0.364} & \multicolumn{2}{c}{0.160} && \multicolumn{2}{c}{0.258} & \multicolumn{2}{c}{0.408} & \multicolumn{2}{c}{0.455} \\
Adj. R$^{2}$ & \multicolumn{2}{c}{0.333} & \multicolumn{2}{c}{0.119} && \multicolumn{2}{c}{0.222} & \multicolumn{2}{c}{0.376} & \multicolumn{2}{c}{0.426} \\
MSE & \multicolumn{2}{c}{5.729} & \multicolumn{2}{c}{0.445} && \multicolumn{2}{c}{1.020} & \multicolumn{2}{c}{5.361} & \multicolumn{2}{c}{0.290} \\
F(df) & \multicolumn{2}{c}{11.817$^{***}$ (9,186)} & \multicolumn{2}{c}{3.925$^{***}$ (9,186)} && \multicolumn{2}{c}{7.166$^{***}$ (9,186)} & \multicolumn{2}{c}{12.747$^{***}$(10,185)} & \multicolumn{2}{c}{15.447$^{***}$(10,185)} \\
\bottomrule
\multicolumn{12}{l}{Note: $^{***}$ $p<0.001$; $^{**}$ $p<0.01$; $^{*}$ $p<0.05$; $^{\dagger}$ $p<0.10$}	\\
\end{tabular}
\Description{Regression results: Direct and indirect effect of AI delegation on task performance and task satisfaction.}
\end{table*}


\subsection{Effect of AI Delegation on Task Satisfaction}



The overall regression model---testing for the direct effect of AI delegation on task satisfaction---is significant, $F(9,186)=3.925, R^2=0.160, p<0.001$. 
Participants in both the delegation and hidden delegation group yield higher levels of task satisfaction ($Mean=3.65, SD=0.66, p<0.004$ and $Mean=3.62, SD=0.73, p<0.010$, respectively), compared to humans working alone ($Mean=3.35, SD=0.70$). A Tukey post hoc test reveals no significant difference in task satisfaction between the two delegation groups ($p=0.961$). Figure \ref{Fig:satisfaction} displays the results of all three groups. We observe that higher cognitive abilities strongly improve ($p<0.001$), and a more positive attitude towards algorithms slightly improves task satisfaction ($p<0.094$). Interestingly, participants who use algorithms more often experience slightly lower task satisfaction ($p<0.097$). For an overview of the regression results, see Figure \ref{Fig:results_mediation} and Table \ref{tab:results_regression} (Columns: `Model I---Direct effect of AI delegation', `Task satisfaction'). 

We conclude that H2 is supported. When the AI delegates task instances to the participants, task satisfaction improves compared to participants working alone. Interestingly, participants' task satisfaction improves significantly, regardless of whether the delegation is communicated to them or not. 

\begin{figure}[b]
  \centering
  \includegraphics[width=\linewidth]{mediation_results.pdf}
  \caption{Overview of the direct and indirect effect of AI delegation on task performance and task satisfaction.}
  \label{Fig:results_mediation}
  \Description{Overview of the direct and indirect effect of AI delegation on task performance and task satisfaction.}
\end{figure}


\subsection{Mediation of Self-efficacy in Effect of AI Delegation on Task Performance}

The mediation model---testing for the indirect effect of AI delegation on task performance through increased self-efficacy---is significant, $F(10,185)=12.747, R^2=0.408, p<0.001$. Participants in both the delegation and hidden delegation group have higher self-efficacy ($Mean=5.29, SD=0.96,p<0.001$ and $Mean=5.37, SD=1.05, p<0.001$, respectively), compared to participants in the human-alone group ($Mean=4.63, SD=1.25$), see Figure \ref{Fig:self_efficacy}. A Tukey post hoc test reveals no significant difference in self-efficacy between the two delegation groups ($p=0.904$). Besides AI delegation, high levels of cognitive ability increase self-efficacy ($p<0.001$). This increased self-efficacy improves task performance ($p<0.001$). A more positive attitude towards algorithms and a younger age also improve task performance ($p<0.011$ and $p<0.002$, respectively). Results of the mediation analysis are displayed in Figure \ref{Fig:results_mediation} and Table \ref{tab:results_regression} (Columns: `Model II---Indirect effect of AI delegation', `Task performance'). Self-efficacy mediates the effect of AI delegation---for both the delegation and hidden delegation group---on task performance, as the mediation indices show ($\beta=0.47$, $SE=0.18$, $95\%$ CI $[0.15, 0.84]$ and $\beta=0.50$, $SE=0.20$, $95\%$ CI $[0.15, 0.94]$, respectively). 

To summarize, H3 is supported. For participants in the delegation groups---whether communicated or not---self-efficacy increases and improves task performance compared to participants working alone (see Table \ref{tab:results_regression}). This means that their self-efficacy increases, regardless of whether they are informed about the delegation or not. 


\subsection{Mediation of Self-efficacy in Effect of AI Delegation on Task Satisfaction} 

The mediation model---testing for the indirect effect of AI delegation on task satisfaction through increased self-efficacy---is significant, $F(10,185)=15.447, R^2=0.455, p<0.001$. We already know that AI delegation increases participants' self-efficacy compared to when humans work alone. Besides task performance, this increased self-efficacy also improves task satisfaction ($p<0.001$). A more positive attitude towards algorithms marginally improves task satisfaction ($p<0.097$). Results of the mediation analysis are displayed in Figure \ref{Fig:results_mediation} and Table \ref{tab:results_regression} (Columns: `Model II---Indirect effect of AI delegation', `Task satisfaction'). Self-efficacy mediates the effect of AI delegation---for both the delegation and hidden delegation group---on task satisfaction, as the mediation indices show ($\beta=0.29$, $SE=0.08$, $95\%$ CI $[0.15, 0.46]$ and $\beta=0.31$, $SE=0.08$, $95\%$ CI $[0.17, 0.47]$, respectively). 


We conclude that H4 is supported. Participants in the delegation group show increased self-efficacy and thereby improved task satisfaction compared to participants working alone (see Table \ref{tab:results_regression}). Participants' self-efficacy increases, regardless of whether the delegation is communicated to them or not.

\section{Discussion}
\label{sec:discussion}
The main goal of this study was to investigate how and why AI delegation affects human task performance and task satisfaction. We developed a research model inspired by organizational behavior research and tested it using an image classification task. AI delegation refers to both the actual act of allocating task instances and the communication of the delegation to the human team members.

Our results demonstrate that AI delegation improves human task performance, regardless of whether humans know about the delegation taking place. Awareness about delegation neither boosts nor reduces human task performance in the current study. Humans receive exactly those images that match their skills. When working together, this effect results in complementary team performance---i.e., the combined human-AI team performance surpasses both human and AI model performance compared to either conducting the task alone. 

In addition to task performance, we were also interested in the impact of AI delegation on human task satisfaction. Task or, more generally, job satisfaction is critical because it predicts employee well-being \cite{Gerdenitsch2017}, productivity \cite{sadeghian2022artificial}, and commitment to the organization \cite{shore1989job}. Our study shows that AI delegation increases task satisfaction, regardless of whether humans know that an AI model delegates instances of the task. As previously stated, knowing about the AI model that takes on the role of a manager leaves task satisfaction unaffected. 

To understand why the observed effects of AI delegation on task performance and task satisfaction occur, the proposed behavioral model allows us to analyze a possible underlying mechanism, i.e., self-efficacy. We find that the effects of AI delegation on task performance and task satisfaction are driven by an increase in humans' self-efficacy. In other words, humans are more confident in their own ability to complete the task when it is composed by the AI model. As a result, they perform better and are more satisfied with the task. While self-efficacy partially mediates the effect of AI delegation on human task performance, it fully mediates the effect of AI delegation on task satisfaction.

\textit{\textbf{Choice of the Task.}}
The following factors determined how we selected our experimental setting and task: First, AI delegation is usually useful in domains where many individual decisions need to be made. Moreover, the AI model has to be able to conduct the task independently. Otherwise, the allocation of instances between the AI model and humans is not possible. We chose our task with these prerequisites in mind and selected image classification as a test bed to evaluate how and why AI delegation influences humans. We believe that image classification is a suitable delegation task since there are many real-world situations where humans need to classify many individual images. Tasks can range from low-stakes tasks, such as animal classification \cite{Bondi2022,8354227}, to high-stakes tasks, such as cancer detection \cite{hekler2019superior}. Additionally, image classification is a task where prior research has shown that humans and AI have complementary strengths and, thus, the potential to reach complementary team performance exists \cite{geirhos2021partial}. 



\textit{\textbf{Implications for Human-AI Collaboration.}}
AI delegation, as a special case of human-AI collaboration, has the potential to reduce human effort in tedious tasks and improve human and overall team performance. Prior work has focused on delegation algorithms in user studies that do not learn both the capabilities of humans and the AI model \cite{Bondi2022,fuegener2022cognitive}. Moreover, these studies do not consider humans' perceptions when an AI model manages the delegation of instances. The current study does not only confirm the benefits of AI delegation in general, it also demonstrates the advantages when the capabilities of both team members are taken into account. Furthermore, it enables us to provide insights into humans' perception of the AI model as the ``manager'', distributing task instances between team members. Our study identifies self-efficacy as an underlying mechanism for the effect of AI delegation on task performance and task satisfaction. Hence, managers could consider applying AI delegation to yield higher levels of performance and employee satisfaction. Interestingly, communicating the AI delegation did not further affect self-efficacy, task performance, and task satisfaction. We can conclude that the modified nature of the task through AI delegation was responsible for the increases in task performance and task-related perceptions. Whether modified tasks increase self-efficacy in general and are perceived as satisfying may depend on humans' preferences, personality, and task context. Some people like tasks that are challenging for them, while others prefer more trivial tasks. 

\textit{\textbf{Implications for Algorithmic Management.}}
In the following, we discuss possible implications for the design of ``AI managers''---a special case of algorithmic management \cite{lee2015working}. Algorithmic management can be understood as transferring managerial functions to algorithms, which is, for example, a central element of the gig economy \cite{benlian2022algorithmic,lee2015working,noponen2019impact}. The gig economy focuses on tasks with many repetitions, such as language translation or image classification. Gig economy platform providers such as Uber organize the matching and delegation of instances based on algorithms. Usually, these algorithms distribute instances to different employees \cite{benlian2022algorithmic}. The AI delegation presented here differentiates from this setting by fulfilling both the role of a manager and an employee. This could open up new scaling potential in the gig economy. For example, digital services could be processed either by humans or algorithms, depending on different criteria, e.g., the urgency of task completion, special task requirements, or the availability of human service providers.

Algorithmic management is usually seen as a double-sided sword. On the one hand, it may lead to efficiency and even performance gains, which is important for the scalability of platform business models \cite{benlian2022algorithmic}. The current research also shows improved perceptions, i.e., self-efficacy and task satisfaction. On the other hand, algorithmic management may induce uncertainty and discomfort among employees \cite{benlian2022algorithmic}. For example, a study on Uber drivers shows that some drivers associate negative feelings with working ``for'' an algorithm \cite{mohlmann2019people}. Future research should examine further when and why people perceive algorithmic management as positive or negative.

To summarize, we wanted to illustrate the existing potential for implementing ``AI managers'' in human-AI collaborations. AI delegation can yield higher task performance and task satisfaction through increased feelings of competence in completing the task. 


\textit{\textbf{Limitations.}}
We do not observe any effect of communicating that delegation takes place through an AI model. Future research should investigate other forms of communication and task settings to verify the robustness of this finding. For example, we suggest including explanations for the delegation rationale, e.g., why and in which cases the AI delegates task instances. Furthermore, our current experimental design did not examine human delegation. Previous research has shown that humans generally have difficulty correctly assessing their own abilities compared to an AI \cite{fuegener2022cognitive}. Hence, it is likely that human delegation would result in lower performance levels. Moreover, we conducted our study with non-experts drawing upon a non-specialized task. In environments that require expert knowledge, AI delegation may have different effects on human behavior, such as a greater desire for agency or transparency of AI decisions \cite{vossing2022designing}. 

\textit{\textbf{Future Work.}}
The potential of AI delegation as a lever to improve task-related outcomes opens up several opportunities for future research. People who identify with their work may perceive AI delegation differently. For example, if an employee sees strong meaning in performing a task, AI delegation could be seen as something negative that takes away the desired work. On the other hand, if the work is perceived as tedious, AI delegation could be seen as positive. Whether AI delegation is perceived as positive or negative could also vary greatly from person to person. Future work could examine personality traits of people who are more willing to participate in AI delegation to identify differences in people's reactions to AI delegation. In addition, algorithmic opacity, which refers to the transparency of the delegation algorithm, is a major issue in the algorithmic management literature. We chose to communicate the delegation decision to the human without explaining the rationale for that decision. Research in other areas shows that additional information, e.g., the confidence of the algorithm or explanations for a particular decision, can help improve decision-making performance \cite{Bansal2021,alufaisan_does_2020}. We propose investigating whether additional information, e.g., information indicating why or in which cases task instances are delegated, may affect various task outcomes. Lastly, it may be interesting to test whether task performance and task satisfaction can be further improved by personalized delegation or design features lowering the psychological distance. 


\section{Conclusion}
\label{sec:conclusion} 
 
This work studies AI delegation as a special form of human-AI collaboration from a human-centered perspective. We propose a behavioral model that allows us to investigate not only the effect of AI delegation on human task performance and task satisfaction but also to understand why the proposed effects occur. Our findings show that AI delegation improves human task performance and task satisfaction while increases in humans' self-efficacy to complete the task explain these positive effects. The question arises whether ``humans managed by AI models'' can be a suitable form of collaboration for particular workplace settings. 


%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
