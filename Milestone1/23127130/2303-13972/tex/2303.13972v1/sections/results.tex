\section{Results}
\label{sec:results}
In this section, we report the results of the experiments formulated in Sections~\ref{subsec:ex1} and~\ref{subsec:ex2}.

\begin{figure*}[!ht]
    \centering
    \subcaptionbox{DensePolyNN \label{fig:e1_fm_5_poly}}[.3\linewidth]{
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
                \begin{axis}[xlabel={Optimisation rounds},ylabel={Highest accuracy},legend style={at={(0.98,0.02)},anchor=south east}]
                    \addplot[color=color1] table [x=x1, y=y1, col sep=tab]{data/e1/fm_5_poly.dat};
                    \addplot[color=color2] table [x=x1, y=y2, col sep=tab]{data/e1/fm_5_poly.dat};
                    \addplot[color=color3] table [x=x1, y=y3, col sep=tab]{data/e1/fm_5_poly.dat};
                    \legend{Random,Grid,Bayesian}
                \end{axis}
            \end{tikzpicture}
        }
    }
    \subcaptionbox{DenseLinearNN  \label{fig:e1_fm_5_linear}}[.3\linewidth]{
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
                \begin{axis}[xlabel={Optimisation rounds},ylabel={Highest accuracy},legend style={at={(0.98,0.02)},anchor=south east}]
                    \addplot[color=color1] table [x=x1, y=y1, col sep=tab]{data/e1/fm_5_linear.dat};
                    \addplot[color=color2] table [x=x1, y=y2, col sep=tab]{data/e1/fm_5_linear.dat};
                    \addplot[color=color3] table [x=x1, y=y3, col sep=tab]{data/e1/fm_5_linear.dat};
                    \legend{Random,Grid,Bayesian}
                \end{axis}
        \end{tikzpicture}
        }
    }
    \subcaptionbox{SimpleCNN \label{fig:e1_fm_5_cnn}}[.3\linewidth]{
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
                \begin{axis}[xlabel={Optimisation rounds},ylabel={Highest accuracy},legend style={at={(0.98,0.02)},anchor=south east}]
                    \addplot[color=color1] table [x=x1, y=y1, col sep=tab]{data/e1/fm_5_cnn.dat};
                    \addplot[color=color2] table [x=x1, y=y2, col sep=tab]{data/e1/fm_5_cnn.dat};
                    \addplot[color=color3] table [x=x1, y=y3, col sep=tab]{data/e1/fm_5_cnn.dat};
                    \legend{Random,Grid,Bayesian}
                \end{axis}
        \end{tikzpicture}
        }
    }
    \caption{Convergence graphs for the hyperparameter optimisation experiment with 5 parameters on FashionMNIST}
    \label{fig:e1_fm_5}
    
    \par\bigskip
    
    \subcaptionbox{DensePolyNN \label{fig:e1_c10_5_poly}}[.3\linewidth]{
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
                \begin{axis}[xlabel={Optimisation rounds},ylabel={Highest accuracy},legend style={at={(0.98,0.02)},anchor=south east}]
                    \addplot[color=color1] table [x=x1, y=y1, col sep=tab]{data/e1/c10_5_poly.dat};
                    \addplot[color=color2] table [x=x1, y=y2, col sep=tab]{data/e1/c10_5_poly.dat};
                    \addplot[color=color3] table [x=x1, y=y3, col sep=tab]{data/e1/c10_5_poly.dat};
                    \legend{Random,Grid,Bayesian}
                \end{axis}
            \end{tikzpicture}
        }
    }
    \subcaptionbox{DenseLinearNN  \label{fig:e1_c10_5_linear}}[.3\linewidth]{
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
                \begin{axis}[xlabel={Optimisation rounds},ylabel={Highest accuracy},legend style={at={(0.98,0.02)},anchor=south east}]
                    \addplot[color=color1] table [x=x1, y=y1, col sep=tab]{data/e1/c10_5_linear.dat};
                    \addplot[color=color2] table [x=x1, y=y2, col sep=tab]{data/e1/c10_5_linear.dat};
                    \addplot[color=color3] table [x=x1, y=y3, col sep=tab]{data/e1/c10_5_linear.dat};
                    \legend{Random,Grid,Bayesian}
                \end{axis}
        \end{tikzpicture}
        }
    }
    \subcaptionbox{SimpleCNN \label{fig:e1_c10_5_cnn}}[.3\linewidth]{
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
                \begin{axis}[xlabel={Optimisation rounds},ylabel={Highest accuracy},legend style={at={(0.98,0.02)},anchor=south east}]
                    \addplot[color=color1] table [x=x1, y=y1, col sep=tab]{data/e1/c10_5_cnn.dat};
                    \addplot[color=color2] table [x=x1, y=y2, col sep=tab]{data/e1/c10_5_cnn.dat};
                    \addplot[color=color3] table [x=x1, y=y3, col sep=tab]{data/e1/c10_5_cnn.dat};
                    \legend{Random,Grid,Bayesian}
                \end{axis}
        \end{tikzpicture}
        }
    }
    \caption{Convergence graphs for the hyperparameter optimisation experiment with 5 parameters on CIFAR-10}
    \label{fig:e1_c10_5}

    \par\bigskip

    \subcaptionbox{DensePolyNN \label{fig:e1_fm_3_poly}}[.3\linewidth]{
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
                \begin{axis}[xlabel={Optimisation rounds},ylabel={Highest accuracy},legend style={at={(0.98,0.02)},anchor=south east}]
                    \addplot[color=color1] table [x=x1, y=y1, col sep=tab]{data/e1/fm_3_poly.dat};
                    \addplot[color=color2] table [x=x1, y=y2, col sep=tab]{data/e1/fm_3_poly.dat};
                    \addplot[color=color3] table [x=x1, y=y3, col sep=tab]{data/e1/fm_3_poly.dat};
                    \legend{Random,Grid,Bayesian}
                \end{axis}
            \end{tikzpicture}
        }
    }
    \subcaptionbox{DenseLinearNN  \label{fig:e1_fm_3_linear}}[.3\linewidth]{
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
                \begin{axis}[xlabel={Optimisation rounds},ylabel={Highest accuracy},legend style={at={(0.98,0.02)},anchor=south east}]
                    \addplot[color=color1] table [x=x1, y=y1, col sep=tab]{data/e1/fm_3_linear.dat};
                    \addplot[color=color2] table [x=x1, y=y2, col sep=tab]{data/e1/fm_3_linear.dat};
                    \addplot[color=color3] table [x=x1, y=y3, col sep=tab]{data/e1/fm_3_linear.dat};
                    \legend{Random,Grid,Bayesian}
                \end{axis}
        \end{tikzpicture}
        }
    }
    \subcaptionbox{SimpleCNN \label{fig:e1_fm_3_cnn}}[.3\linewidth]{
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
                \begin{axis}[xlabel={Optimisation rounds},ylabel={Highest accuracy},legend style={at={(0.98,0.02)},anchor=south east}]
                    \addplot[color=color1] table [x=x1, y=y1, col sep=tab]{data/e1/fm_3_cnn.dat};
                    \addplot[color=color2] table [x=x1, y=y2, col sep=tab]{data/e1/fm_3_cnn.dat};
                    \addplot[color=color3] table [x=x1, y=y3, col sep=tab]{data/e1/fm_3_cnn.dat};
                    \legend{Random,Grid,Bayesian}
                \end{axis}
        \end{tikzpicture}
        }
    }
    \caption{Convergence graphs for the hyperparameter optimisation experiment with 3 parameters on FashionMNIST}
    \label{fig:e1_fm_3}

    \par\bigskip

    \subcaptionbox{DensePolyNN \label{fig:e1_c10_3_poly}}[.3\linewidth]{
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
                \begin{axis}[xlabel={Optimisation rounds},ylabel={Highest accuracy},legend style={at={(0.98,0.02)},anchor=south east}]
                    \addplot[color=color1] table [x=x1, y=y1, col sep=tab]{data/e1/c10_3_poly.dat};
                    \addplot[color=color2] table [x=x1, y=y2, col sep=tab]{data/e1/c10_3_poly.dat};
                    \addplot[color=color3] table [x=x1, y=y3, col sep=tab]{data/e1/c10_3_poly.dat};
                    \legend{Random,Grid,Bayesian}
                \end{axis}
            \end{tikzpicture}
        }
    }
    \subcaptionbox{DenseLinearNN  \label{fig:e1_c10_3_linear}}[.3\linewidth]{
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
                \begin{axis}[xlabel={Optimisation rounds},ylabel={Highest accuracy},legend style={at={(0.98,0.02)},anchor=south east}]
                    \addplot[color=color1] table [x=x1, y=y1, col sep=tab]{data/e1/c10_3_linear.dat};
                    \addplot[color=color2] table [x=x1, y=y2, col sep=tab]{data/e1/c10_3_linear.dat};
                    \addplot[color=color3] table [x=x1, y=y3, col sep=tab]{data/e1/c10_3_linear.dat};
                    \legend{Random,Grid,Bayesian}
                \end{axis}
        \end{tikzpicture}
        }
    }
    \subcaptionbox{SimpleCNN \label{fig:e1_c10_3_cnn}}[.3\linewidth]{
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
                \begin{axis}[xlabel={Optimisation rounds},ylabel={Highest accuracy},legend style={at={(0.98,0.02)},anchor=south east}]
                    \addplot[color=color1] table [x=x1, y=y1, col sep=tab]{data/e1/c10_3_cnn.dat};
                    \addplot[color=color2] table [x=x1, y=y2, col sep=tab]{data/e1/c10_3_cnn.dat};
                    \addplot[color=color3] table [x=x1, y=y3, col sep=tab]{data/e1/c10_3_cnn.dat};
                    \legend{Random,Grid,Bayesian}
                \end{axis}
        \end{tikzpicture}
        }
    }
    \caption{Convergence graphs for the hyperparameter optimisation experiment with 3 parameters on CIFAR-10}
    \label{fig:e1_c10_3}
\end{figure*}


\begin{table*}[bp]
    \centering
    \caption{Summary of the results for the optimisation experiment. Values are reported as x $|$ y, where x represents the results with 5 hyperparameters (i.e. $\alpha, \beta_1, \beta_2, \epsilon, w$), and y those with 3 (i.e. $\alpha, \beta_1, \beta_2$). In bold are the values of the highest accuracy or lowest number of optimisation rounds among the three strategies for each network and dataset.}
    \resizebox{.9\linewidth}{!}{
    \begin{tabular}{lccccc}
    \toprule
                                       &          & \multicolumn{2}{c}{CIFAR-10}      & \multicolumn{2}{c}{FashionMNIST}     \\ 
                                       &          & Accuracy             & Optimisation rounds             & Accuracy                & Optimisation rounds              \\ 
        \midrule
        DensePolyNN   & Random   & 0.33 \textbar{} 0.32 & 56 \textbar{} 27 & 0.826 \textbar{} 0.83   & 56 \textbar{} 27  \\
                                       & Grid     & 0.37 \textbar{} 0.40 & 42 \textbar{} 55 & 0.81 \textbar{} 0.81    & 40 \textbar{} \textbf{19}  \\
                                       & Bayesian & \textbf{0.40} \textbar{} \textbf{0.41} & \textbf{27} \textbar{} \textbf{11} & \textbf{0.833} \textbar{} \textbf{0.85}   & \textbf{29} \textbar{} 46*  \\ 
        \midrule
        DenseLinearNN & Random   & 0.35 \textbar{} 0.38 & 50 \textbar{} 63 & 0.81 \textbar{} 0.84    & 52 \textbar{} \textbf{23}  \\
                                       & Grid     & \textbf{0.40} \textbar{} \textbf{0.40} & 53 \textbar{} \textbf{35} & 0.81 \textbar{} 0.81    & 30 \textbar{} 54  \\
                                       & Bayesian & 0.37 \textbar{} 0.39 & \textbf{4} \textbar{} 38  & \textbf{0.84} \textbar{} \textbf{0.85}    & \textbf{18} \textbar{} 47*  \\ 
        \midrule
        SimpleCNN     & Random   & 0.60 \textbar{} 0.62 & 40 \textbar{} \textbf{4}  & \textbf{0.8879} \textbar{} 0.879 & 29 \textbar{} 39  \\
                                       & Grid     & 0.58 \textbar{} 0.59 & 17 \textbar{} 55 & 0.86 \textbar{} 0.875   & \textbf{14} \textbar{} \textbf{20}  \\
                                       & Bayesian & \textbf{0.68} \textbar{} \textbf{0.66} & \textbf{16} \textbar{} 26 & 0.8876 \textbar{} \textbf{0.884} & 20 \textbar{} 36 \\
        \bottomrule                               
    \end{tabular}}
    \label{tab:e1_summary}
\end{table*}

\subsection{Hyperparameter Optimisation}
\label{subsec:res1}
The line graphs in Figures~\ref{fig:e1_fm_5}~and~\ref{fig:e1_c10_5} show the highest achieved accuracy by the number of optimisation rounds for all the settings with 5 hyperparameters (i.e., $\alpha, \beta_1, \beta_2, \epsilon$ and $w$) on both the FashionMNIST and CIFAR-10 datasets. Figures~\ref{fig:e1_fm_3}~and~\ref{fig:e1_c10_3}, display the results for all settings with 3 parameters (i.e., $\alpha, \beta_1$ and $\beta_2$). The figures are separated into subfigures to distinguish between the results for the DensePolyNN~(\subref{fig:e1_fm_5_poly}), DenseLinearNN~(\subref{fig:e1_fm_5_linear}) and SimpleCNN~(\subref{fig:e1_fm_5_cnn}) that were introduced in Section~\ref{sec:methods}. The total runtime of the hyperparameter optimisation experiment~(Section~\ref{subsec:ex1}) amounts to $\pm85$~hours. 

With an initial visual assessment, a few observations can be made. First, Bayesian optimisation proves to be the most effective strategy when compared with random and grid search. Regardless of the network or workload, it consistently outperforms the other strategies, only being overtaken slightly by grid search twice (\ref{fig:e1_c10_5_linear}~and~\ref{fig:e1_c10_3_linear}) and narrowly matched by random search three times (\ref{fig:e1_fm_5_cnn},~\ref{fig:e1_fm_3_linear}~and~\ref{fig:e1_fm_3_cnn}). Second, between grid search and random search, there is no definitive winner. Random search performed better than grid search 5 out of 6 times on the FashionMNIST dataset and 2 out of 6 times on CIFAR-10. We have also summarised this data in Table~\ref{tab:e1_summary}. This table presents the optimum accuracy (cf. Section~\ref{sec:methods:analysis}) for every experimental configuration (i.e. network $\times$ optimisation strategy $\times$ dataset $\times$ \#hyperparameters) together with the number of optimisation rounds it took to achieve that accuracy.

Finally, notice that the Bayesian optimisation strategy converges to an accuracy optimum within 27 optimisation rounds on average. The two outliers with regard to this rule are marked by an asterisk~(*) in Table~\ref{tab:e1_summary}. 
Nonetheless, a quick inspection of the corresponding graphs (\ref{fig:e1_fm_3_poly}~\&~\ref{fig:e1_fm_3_linear}) shows that there is only a very slight increase compared to the accuracy that was achieved after 27 optimisation rounds. The same cannot exactly be said for random optimisation. Most of the graphs follow a much more gradual incline with bigger jumps in accuracy. Overall, this strategy takes longer to converge. Grid search, on the other hand, does seem to converge rapidly. The numbers in Table~\ref{tab:e1_summary} might suggest otherwise, but similar to what we observe with Bayesian optimisation, the increases in accuracy past 27 optimisation rounds are minimal.

\subsection{Network Architecture}
\label{subsec:res2}
The total runtime for all the different configurations of the network architecture experiment~(Section~\ref{subsec:ex2}) approximately amounts to 46~hours. The purpose of this experiment is to quantify the relationship between the network architecture and the amount of energy that is being consumed during training. 

To reinforce the validity of our results, we first show that the values of the hyperparameters, as chosen by the random optimisation function, do not significantly impact the energy consumption. The coefficient of variance (CV) is a metric that explains the relative size of the standard deviation to the mean. Because we assume that the hyperparameter setting has little to no influence on the energy consumption, we expect a very small CV ($<1$\%) for all the optimisation rounds of a network. The histogram in Figure~\ref{fig:e2_cv_c10} depicts the CVs for every row in Table~\ref{tab:exp2} on both the FashionMNIST and CIFAR-10 datasets. Every data point is a calculation of 8 optimisation rounds, including 24 repetitions. We find an average CV of 0.009 and a maximum value of 0.018.

\begin{figure}[h]
\centering
    \resizebox{.9\linewidth}{!}{
        \begin{tikzpicture}
        \begin{axis}[yscale=.7,ybar,ymin=0,xlabel={Coefficient of variance},ylabel={Optimisation rounds},legend style={at={(0.98,0.02)},anchor=south east}]
            \addplot +[hist={bins={10},data min={0},data max={0.05}}, opacity=0.7, color=color1] table [y index=0] {data/e2/cv_c10.dat};
            \addplot +[hist={bins={10},data min={0},data max={0.05}},opacity=0.7,color=color2] table [y index=0] {data/e2/cv_fm.dat};
            \legend{CIFAR-10,FashionMNIST}
        \end{axis}
    \end{tikzpicture}
}
    \caption{Histogram of the coefficient of variance for each run on the CIFAR-10 and FasionMNIST datasets}
    \label{fig:e2_cv_c10}
\end{figure}

Now that we have shown that the energy consumption of a training iteration is independent of the hyperparameter settings in this experiment, we can analyse the network architecture in isolation. Because the data is not normally distributed, a conclusion made following the procedure described in Section~\ref{sec:methods:analysis}, we perform the non-parametric Kruskal-Wallis test to identify if the energy consumed to train the network architectures can be distinguished statistically. Table~\ref{tab:kw-test} presents the corresponding p-values and effect sizes ($\eta^2$). Notice that out of the three layer types, convolutional layers and linear layers have a large degree of influence on the energy consumption, while the influence of ReLU layers is small. An additional post hoc comparison shows that all combinations of independent variables are significant as well. To put these statistics into perspective, we compare the increase in average energy consumption by fixing each layer type. We use the notation $x|y$ to distinguish results on the FashionMNIST dataset~($x$) from those on the CIFAR-10 dataset~($y$). The presence of ReLU layers contributes an average increase of $2.7\%|2.9\%$. For the linear layers, the jump from 3 to 7 layers accounts for an increase of $4.9\%|6.6\%$. The convolutional layers are the largest sources of energy usage. Introducing 3 additional layers on top of the first one increases the overall consumption by $95.3\%|66.4\%$. 


\begin{table}[h]
	\centering
	\caption{Kruskal-Wallis test results. From top to bottom, the tables refer to the experiments on the FashionMNIST and CIFAR-10 datasets respectively.}
	\label{tab:kw-test}
	\resizebox{\linewidth}{!}{
	\begin{tabular}{lrrrc}
		\toprule
		Factor (layer type) & Statistic & p & $\eta^2$ & magnitude \\
		\midrule
		Linear & $481.799$ & $<$ .001 & 0.155 & large\\
		Convolutional & $2303.250$ & $<$ .001 & 0.749 & large\\
		ReLU & $176.545$ & $<$ .001 & 0.055 & small\\
		\midrule
		Factor (layer type) & Statistic & p & $\eta^2$ & magnitude \\
		\midrule
		Linear & $496.807$ & $<$ .001 & 0.160 & large\\
		Convolutional & $2303.250$ & $<$ .001 & 0.749 & large\\
		ReLU & $106.655$ & $<$ .001 & 0.033 & small\\
		\bottomrule
	\end{tabular}}
\end{table}


\begin{figure*}[bp]
    \centering
    \subcaptionbox{FashionMNIST \label{fig:e2_evsa_fm}}[.4\linewidth]{
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
                \begin{axis}[
                    xlabel=Energy (J),
                    ylabel=Accuracy, 
                    axis lines=left, 
                    clip=false,
                    x dir=reverse
                ]
                        \addplot +[only marks, mark options = {fill=color1, draw=color1dark}] table [x=energy, y=accuracy, col sep=tab]{data/e2/e_vs_a_fm.dat};
                \end{axis}
                \node[text=color3] at (2,5) {$E^+$};
                \draw[color3,thick] (0.8,3.2) ellipse (1cm and 3cm);
                \node[text=color2] at (5.4,5) {$E^-$};
                \draw[color2,thick] (6.4,3.2) ellipse (0.5cm and 3cm);
            \end{tikzpicture}
        }
    }
    \subcaptionbox{CIFAR-10 \label{fig:e2_evsa_c10}}[.4\linewidth]{
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
                \begin{axis}[
                    xlabel=Energy (J),
                    ylabel=Accuracy, 
                    axis lines=left, 
                    clip=false,
                    x dir=reverse
                ]
                        \addplot +[only marks, mark options = {fill=color1, draw=color1dark}] table [x=energy, y=accuracy, col sep=tab]{data/e2/e_vs_a_c10.dat};
                \end{axis}
                \node[text=color3] at (2.2,5) {$E^+$};
                \draw[color3,thick] (1,3.2) ellipse (1cm and 3cm);
                \node[text=color2] at (5.2,5) {$E^-$};
                \draw[color2,thick] (6.2,3.2) ellipse (0.8cm and 3cm);
            \end{tikzpicture}
        }
    }
    \caption{Scatter plots of the energy consumption vs the achieved accuracy}
    \label{fig:e2_evsa}
\end{figure*}

Moreover, we carry out a trade-off analysis with respect to the energy consumption of a neural network and its achieved accuracy on the problem set. This comparison is visualised in Figure~\ref{fig:e2_evsa}. The scatter plots in this figure highlight the relationship of the energy consumption in Joules and the achieved accuracy on the test sets of FashionMNIST~(\subref{fig:e2_evsa_fm}) and CIFAR-10~(\subref{fig:e2_evsa_c10}). In both scatter plots, we can discern two clusters; one spread around a higher energy consumption which we will refer to as $E^+$; the other spread around a lower energy consumption, we call this cluster $E^-$ (notice that the energy axis is reversed). All data points in $E^-$ correspond to network architectures with a single convolutional layer, while the $E^+$ cluster contains all the networks with four convolutional layers.

Table~\ref{tab:accuracy_tradeoff} summarises the data from the scatter plots into numerical values. The first two columns show the average energy consumption, average accuracy, maximum accuracy and the standard deviation of the accuracy for the $E^+$ and $E^-$ clusters on the FashionMINST dataset. The latter two columns show the same information on the CIFAR-10 set. Notice that the average and maximum accuracy for both clusters on the FashionMNIST dataset are particularly close together, only varying by less than 1\%. For CIFAR-10, which is a more computationally complex set, this difference is more significant. A little over 6\% for the average and almost 12\% for the maximum accuracy.

\begin{table}[h]
    \centering
    \caption{Low energy performance compared against high energy performance.}
    \label{tab:accuracy_tradeoff}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lllll}
        \toprule
                           & \multicolumn{2}{c}{FashionMNIST} & \multicolumn{2}{c}{CIFAR-10}  \\ 
                           & \multicolumn{1}{c}{$E^+$}    & \multicolumn{1}{c}{$E^-$}                     & \multicolumn{1}{c}{$E^+$}    & \multicolumn{1}{c}{$E^-$}                   \\ 
        \midrule
        Average energy     & 1674 J& 857 J                   & 4588 J& 2758 J                \\
        Average accuracy   & 0.872 & 0.864                   & 0.639 & 0.572                \\
        Max accuracy       & 0.889 & 0.887                   & 0.725 & 0.609                \\
        Std accuracy       & 0.011 & 0.010                   & 0.056 & 0.021                \\
        \bottomrule
    \end{tabular}}
\end{table}

 




