\section{Introduction}
%When we think about saving energy, trivial matters come to mind first: turn off the lights when the sun is out, wash your clothes with cold water and take shorter showers. We like to think that every small bit helps, but in reality, the consumption of the individual is often overshadowed by that of large corporations. In 2020, Microsoft announced a new supercomputer in collaboration with OpenAI. 
%The build was a single system containing 10,000 GPUs~\footnote{\hyperlink{https://blogs.microsoft.com/ai/openai-azure-supercomputer/}{https://blogs.microsoft.com/ai/openai-azure-supercomputer/}}. 
%An average European household consumes 1.3 toe (tonne of oil equivalent) of energy in a year~\footnote{\href{https://www.enerdata.net/publications/executive-briefing/households-energy-efficiency.html}{https://www.enerdata.net/publications/executive-briefing/households-energy-efficiency.html}}, which is around 15,000 kWh. 
%Solely considering the GPUs used in the supercomputer and assuming that they are V100s with a lower bound power usage of 250 watts, the estimated energy cost of this system adds up to 21,900,000 kWh or just over 1,450 households.

AI practices are expensive and can have a significant environmental impact.
That is not surprising, since an important challenge within the AI community is improving the accuracy of previously reported systems~\cite{schwartz2020green}. Now, a new field is emerging to address this problem: \greenai{}, with its roots planted deep into the discipline of Sustainable Software Engineering. The software engineering community has increasingly studied the energy efficiency of software systems by developing energy estimation models~\cite{chowdhury2019greenscaler, linares2014mining}; developing code analysis and optimisation tools to improve energy efficiency~\cite{cruz2017performance, cruz2017leafactor, banerjee2016automated, linares2017gemma}; studying practices that lead to green software~\cite{cruz2019catalog, chowdhury2018exploratory, feitosa2017investigating} and so on. Recently, a new trend is calling for software engineering approaches that consider `data as the new code', challenging practitioners with new software systems that ship AI-based features. This intersection between Green Software Engineering and AI Engineering is where we find the origin of \greenai{}. The initial contributions in this field consist of positional papers that are calling for a new research agenda~\cite{bender2021dangers, strubell2019energy, schwartz2020green}. Since then, the community has developed into studying the energy footprint of AI at different levels~\cite{SLRGreenAI}.
This involves the measurement and reporting of energy consumption~\cite{garcia2019estimation} next to accuracy, but also the appreciation of research efforts that do not necessarily rely on enterprise-sized data~\cite{datacentricgreenai} or training budgets.  

This study focuses on deep learning, a subset of machine learning and the driver behind many AI applications and services. All experiments are performed with rudimentary neural networks that comprise the building blocks of more complex models. We train these networks on two popular image vision problem sets: FashionMNIST~\cite{xiao2017} and CIFAR-10~\cite{krizhevsky2009learning}. 
We adopt the idea of designing neural networks with energy consumption as one of the main considerations. Specifically, we direct our attention to the early phases of the deep learning pipeline and formulate the following research questions:

\begin{enumerate}
    \item[$RQ_1$:] Between Bayesian optimisation, random optimisation and grid search; which strategy is the most energy-efficient for training a neural network?
    \item[$RQ_2$:] Can the complexity of a neural network be reduced such that it consumes less energy while maintaining an acceptable level of accuracy? 
\end{enumerate}

First, we analyse Bayesian optimisation, random optimisation and grid search, three popular optimisation strategies, to identify best practices in terms of energy efficiency considerations. Classically, grid search has served as the most popular baseline optimisation strategy in the context of hyperparameter tuning~\cite{bergstra2012random}. Nonetheless, there have been studies that present random search as an alternative baseline that competes with or even exceeds grid search in multi-dimensional optimisation problems~\cite{bergstra2011algorithms,bergstra2012random,liashchynskyi2019grid}. Bayesian optimisation is a more powerful strategy that is also more difficult to implement and parallelise. Apart from comparing these three strategies, we demonstrate that further optimisation attempts past a specific point are met with diminishing returns in performance that might not be worth the additional cost of training. Training times can vary greatly depending on the workload and network architecture and there are no rules that state how many optimisation rounds one should perform. This is where the potential opportunity for energy savings lies.

Secondly, we quantify the effect of a network's architecture in terms of layers, on the actual energy consumption of the GPU. In a similar fashion, we show how more complex models see diminishing returns in their performance, while the energy consumption keeps increasing at a steady rate. By analysing the accuracy of a neural network together with its energy consumption, the perspective of what is currently considered `the best' model could see a dramatic shift. We believe it is the job of the \greenai{} community to make such data and observations available to the public so that software engineers can make more informed trade-offs, and more sustainable decisions.

% We go through complementary studies that focus on Sustainable Software Engineering \greenai{} practices in Section~\ref{sec:related}. The concepts mentioned throughout this paper are discussed in Section~\ref{sec:background}. The methodology and resources necessary for reproduction purposes are presented in Section~\ref{sec:methods}. Section~\ref{sec:experiments} details the design of two experiments that were constructed around the research questions. Section~\ref{sec:results} collects and parses all the results which we then discuss in Section~\ref{sec:discussion}. Finally, we elaborate on the threats to the validity of our study in Section~\ref{sec:t2v} and finish with the conclusion in Section~\ref{sec:conclusion}.

