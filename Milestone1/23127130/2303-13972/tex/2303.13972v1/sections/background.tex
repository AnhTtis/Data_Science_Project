%\vspace{-0.5em}
\section{Background}
\label{sec:background}
This section introduces the FashionMNIST and CIFAR-10 datasets; elaborates on grid search, random search and Bayesian optimisation and establishes a basic understanding of linear, convolutional and ReLU layers inside a neural network.

\subsection{Datasets}
The original MNIST dataset consists of many grey-scale images of handwritten digits. MNIST has been used excessively as a benchmark to validate many different models. However, with modern technology, the MNIST problem set has become too trivial. Because most networks can achieve near-perfect accuracy on the set, researchers from Zalando have proposed the use of \textbf{FashionMNIST} as a direct drop-in-replacement~\cite{xiao2017}. As such, the dataset is comprised of 28$\times$28 grayscale images of 70,000 different fashion products. Just like in the original MNIST set, the products are separated into 10 categories. Because both datasets are shaped identically, FashionMNIST is immediately compatible with any machine learning package that works with MNIST.

The \textbf{CIFAR-10} dataset is a subset of the tiny images dataset~\cite{torralba200880}. It is composed of 60,000 32$\times$32 RGB images divided into 10 classes~\cite{krizhevsky2009learning}. CIFAR-10 presents a challenge that is very similar to FashionMNIST, however, the larger image size and two additional layers increase the complexity of the task significantly.  

\subsection{Optimisation Strategies}
\textbf{Grid search} is a traditional optimisation strategy that applies an exhaustive search over the hyperparameter space. For discrete variables, this means that the algorithm considers the Cartesian product of all the values. For continuous variables, it is necessary to select a distribution first. One could for example choose a uniform or log-uniform distribution to map the continuous space to a discrete one. The computational complexity of grid search is exponential in the number of parameters, therefore it quickly becomes impractical to calculate it all the way through. Nevertheless, because the search space is determined at the beginning, the workload can very easily be parallelised, somewhat offsetting this drawback. 

In a \textbf{random search}, the well-defined structure of the grid is replaced by random selection. Because every drawn sample is completely independent, parallelisation of this algorithm is as trivial as with grid search.

In the context of hyperparameter tuning, the \textbf{Bayesian optimisation} algorithm creates and refines a probabilistic regression model of a function $f(x)$ that can be exploited to return the predicted accuracy and corresponding standard deviation. An acquisition function is then used to determine the next most promising set of input variables. For the probabilistic model, Gaussian Processes (GP) are the most popular choice amongst many studies~\cite{klein2017fast,snoek2012practical,wu2019hyperparameter}. Bayesian optimisation is especially effective in scenarios where the true value of $f(x)$ is hard to compute, which is the case with neural network training. This is because the probabilistic model needs to evaluate a sufficiently large quantity of samples.

The purpose of the acquisition function is to determine the most promising sample from a set of randomly selected input variables. There are many possible choices that can be considered:
\begin{itemize}
    \item Probability of Improvement (PI)~\cite{kushner1964new}
    \item Expected Improvement (EI)~\cite{mockus1978application}
    \item Upper Confidence Bound (UCB)~\cite{srinivas2010gaussian}
    \item Entropy Search (ES)~\cite{hennig2012entropy}
    \item Predictive Entropy Search (PES)~\cite{hernandez2014predictive}
    \item Knowledge Gradient (KG)~\cite{scott2011correlated}
\end{itemize}

We now elaborate on the PI acquisition function, which is also the function that we use in all of the experiments.
\begin{equation}
PI(x) = P(f(x) \geq f(x_{best})) = \Phi(\frac{\mu(x) - f(x_{best})}{\sigma(x) + \epsilon})
\label{eq:pi1}
\end{equation}
\begin{equation}
    PI(x) = P(f(x) \leq f(x_{best})) = \Phi(\frac{f(x_{best}) - \mu(x)}{\sigma(x) + \epsilon})
\label{eq:pi2}
\end{equation}

Equations~\ref{eq:pi1} and~\ref{eq:pi2} are used to calculate the probability of improvement for maximisation and minimisation problems respectively. Since we are interested in maximising the accuracy of a neural network, we will use Equation~\ref{eq:pi1}. Here, $\Phi$ refers to the cumulative density function of a normal distribution; $\mu$ and $\sigma$ are the predicted value and standard deviation retrieved from the Gaussian regressor, and $f(x_{best})$ is the highest actual accuracy found so far. Algorithm~\ref{alg:baypi} displays an example implementation of a single PI iteration. 

\begin{algorithm}
\caption{Bayesian Optimisation - Probability of Improvement}
    \begin{algorithmic}
        \State $y = max(Y)$
        \State $Candidates \gets N$ random input samples
        \State $x', pi'$
        \For{$x \in Candidates$}
            \State $\mu, \sigma = predict(x)$
            \State $pi = \Phi(\frac{\mu - y}{\sigma + \epsilon})$
            \If{$pi > pi'$}
                \State $x' = x, pi' = pi$
            \EndIf
        \EndFor
        \State $X \gets x', Y \gets f(x')$
        \State $fit(X, Y)$
    \end{algorithmic}
    \label{alg:baypi}
    \vspace{-0.2em}
\end{algorithm}

%\vspace{-0.5em}
\subsection{Neural Network Layers}
Every layer inside a neural network performs some transformation on an input vector~$x$. The obtained output is then passed on to the next layer. \textbf{Linear}, or \textbf{fully connected layers}, calculate an output by applying a linear transformation through a matrix of weights~$W$~\cite{ma2017equivalence}. The values of $W$ are optimised and updated during training. The term fully connected comes from the fact that every element of $x$ is mapped to every other element in the output by the matrix multiplication $W^Tx$.

Inside a \textbf{convolutional layer}, a kernel is used to calculate a weighted summation of the elements of the input layer. The kernel slides across the input layer, considering all elements and their neighbours. A convolutional operation is defined by stride, kernel size and zero padding\cite{albawi2017understanding}. The stride determines how many places the kernel slides after each calculation; the kernel size represents the dimensions of the filter and zero padding adds zeros to the outer edges of the input layer. Generally speaking, the output layer is always smaller than the input layer, limiting the maximum number of convolutional layers that can be implemented. However, by applying zero padding, one can prevent this shrinking behaviour if desired. The convolution operation is shown graphically in Figure~\ref{fig:convolution}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5\linewidth]{img/convolution_image.png}
    \caption{Convolution operation on an input layer using a 3$\times$3 kernel.}
    \label{fig:convolution}
    %\vspace{-1em}
\end{figure}


The \textbf{Rectified Linear Unit (ReLU) layer} introduces an activation function that applies non-linearity to the input. ReLU is the most common form of non-linearity in CNNs~\cite{albawi2017understanding}. The function is very simple: An element is deactivated (set to 0) if it is negative; otherwise, the value remains the same.
