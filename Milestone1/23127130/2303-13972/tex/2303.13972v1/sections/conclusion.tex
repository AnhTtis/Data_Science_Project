\section{Conclusion}
\label{sec:conclusion}
In this study, we have expanded the horizon of green software to the realm of AI applications. Our empirical study shows that Bayesian optimisation can find the most optimal set of hyperparameters within the least number of iterations, where 27 should be sufficient in most instances ($RQ_1$). Grid search and random search have their purposes as baseline algorithms. If the parameter bounds are chosen with care, neither of those two strategies significantly dominates the other. Nevertheless, we advocate the use of random optimisation since the exhaustive nature of grid search often implies that one cannot consider the complete search space anyway. Additionally, because the function of hyperparameters has a low effective dimensionality, it is more reasonable to introduce randomness to the search space.

Furthermore, we have investigated the impact of a neural network's architecture on its energy consumption, followed by a trade-off analysis regarding the accuracy ($RQ_2$). We found that for a substantial increase in energy consumption, the increases in accuracy see diminishing returns. We advise reducing the number of convolutional layers to a point where the accuracy is still within a reasonable margin. This entirely depends on the project in question and should be evaluated case by case. 

We hope that our study sheds light on the lopsided relationship between accuracy and energy; that it sparks interest in efficient design practices and helps to shift the evaluation criteria for neural networks to more conservative models. 
% Ultimately, we show the importance of monitoring energy efficiency within AI development environments.