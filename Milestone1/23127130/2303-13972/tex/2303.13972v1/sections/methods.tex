\section{Research Methods}
\label{sec:methods}
The goal of this study is to identify trade-off points with respect to the energy consumption during the training phase of the deep learning pipeline.

%We study the training phase by measuring the accuracy and energy consumption of generated models during each optimisation round, corresponding to a specific configuration (network architecture, hyperparameters, 
%while varying the configurations () for each optimisation round
%The Figure~\ref{fig:methods} presents the steps involved in the methodology.



%\vspace{-1em}
\subsection{Case Selection}
Achieving state-of-the-art accuracy results on challenging data sets is not the main focus. For this reason, we will be working with rudimentary networks architectures that can be trained using consumer-grade hardware. The simplicity of the models facilitates the design of more intricate experimentation and encourages inclusivity. We choose to direct our efforts to image recognition problems. Image recognition is a canonical problem that can be solved with neural networks, and there are a plethora of easily accessible data sets available.

The experiments are performed using three neural networks written with the PyTorch framework\footnote{\href{https://pytorch.org}{https://pytorch.org}}. These networks are trained on a single GeForce GTX-1080\footnote{\href{https://www.nvidia.com/nl-nl/geforce/10-series/}{https://www.nvidia.com/nl-nl/geforce/10-series/}} GPU with images from the FashionMNIST and CIFAR-10 datasets. During every \textbf{optimisation round} mentioned in this study, an optimisation algorithm chooses a set of hyperparameter values. An optimisation round lasts for 8 repetitions, during which we use the same set of hyperparameters. A single repetition consists of 25 training epochs\footnote{During one optimisation round with eight repetitions, a network undergoes $25 \times 8 $ training epochs with the same set of hyperparameters}. After the 8 repetitions, the highest accuracy and average energy consumption are logged and a new optimisation round starts. We present this experimental design schematically in Figure~\ref{fig:methods}. The structure of the different networks is as follows:
\begin{itemize}
    \item \textbf{DenseLinearNN}: N linear layers, where the number of neurons in each layer scales down linearly towards the number of problem classes.
    \item \textbf{DensePolyNN}: N linear layers, every layer has half the number of neurons as the layer before it. 
    \item \textbf{SimpleCNN}: M convolutional layers, each followed by a BatchNorm2d, ReLU and MaxPool2d layer, and N linear layers where every layer has half the number of neurons as the layer before it.
\end{itemize}

\begin{figure}[!hb]
    \centering
    \includegraphics[width=0.9\linewidth]{img/Exp_design.pdf}
    \caption{Methodology process}
    \label{fig:methods}
\end{figure}


\subsection{Experimental Tooling}
To facilitate and standardise the data collection, we develop a test suite that automates the execution of the experiments.
This test suite is available online\footnote{\href{\materialurl}{\materialurl}} and contains the implementations of the aforementioned neural networks that can be trained on all the visual problem datasets provided by Pytorch\footnote{\href{https://pytorch.org/vision/0.8/datasets.html}{https://pytorch.org/vision/0.8/datasets.html}}. The test bed is designed to be modular, and we encourage other researchers to add additional neural network designs or different hyperparameter optimisation functions. All the results in this study have been accumulated with this test bed. 

\subsection{Data Collection}
To answer $RQ_1$, we compare the convergence rate of three different hyperparameter tuning strategies:  Grid search, random optimisation and Bayesian optimisation with the PI acquisition function. Because grid search is an exhaustive method, it quickly becomes infeasible to train a network on every hyperparameter set. To fairly compare grid search to the other strategies, we first generate the complete search space and then proceed to pick random samples from that space until we reach the desired amount of optimisation rounds. Variables with continuous ranges are divided into five uniformly-distributed values. Although this is a partial grid search, the most important difference with random search remains intact: because we select samples from the grid, a limited number of values are considered for every hyperparameter.

For $RQ_2$, we examine the effect of the neural network's architecture on the absolute energy consumption. To obtain the power usage of the GPU, we query the NVIDIA System Management Interface\footnote{\href{https://developer.nvidia.com/nvidia-system-management-interface}{https://developer.nvidia.com/nvidia-system-management-interface}} every 100 milliseconds. We use this to compute the total energy consumption of a training iteration and then factor out the idle energy consumption of the GPU. 

\vspace{-0.5em}
\subsection{Data Analysis}
\label{sec:methods:analysis}
To assess the effectiveness of the hyperparameter tuning strategies, we study the convergence rate of the model accuracy according to the number of optimisation rounds. We determine the optimum accuracy as the highest accuracy after which no substantial increase for each additional optimisation round is observed. Similarly, we define the optimum round as the number of the optimisation round when the optimum accuracy is reached. By identifying those values, we can ascertain the most efficient strategy, and establish the optimal number of optimisation rounds sufficient to provide the model with optimum accuracy.

To accurately analyse the effect of the neural network architecture in relation to the energy consumption, first, we would like to show that the hyperparameter set does not contaminate the results. We do so by calculating the coefficient of variance (CV) of the energy consumption and showing that it is very low ($<0.01$). The CV is calculated as the standard deviation of a sequence divided by its average.

Prior to any further in-depth analysis, we need to assess whether the energy results obtained in the experiments follow a normal distribution. After a visual inspection of the quantile-quantile (Q-Q) plot, followed by the Shapiro-Wilk test\footnote{\href{https://www.statskingdom.com/shapiro-wilk-test-calculator.html}{https://www.statskingdom.com/shapiro-wilk-test-calculator.html}}, we conclude that our data is not normally distributed. Hence, we opt for a non-parametric analysis and apply the Kruskal-Wallis test to indicate the significance of our independent variables, i.e. whether we may conclude that the layer types have a statistically meaningful impact on the energy consumption. Additionally, we calculate the $\eta^2$ as the effect size. We evaluate these effect sizes based on the rules of thumb for Cohen's $f$~\cite{cohen2013statistical}, which is calculated as $f=\sqrt{\frac{\eta^2}{1-\eta^2}}$. Cohen suggests that the values 0.10, 0.25 and 0.40 convey a small, medium and large effect size respectively. We invert the function to obtain the effect thresholds for $\eta^2$: 0.01, 0.06 and 0.14.
