\section{Experiments}
\label{sec:experiments}
In this section, we present the design of two different experiments, each related to one of the research questions. Section~\ref{subsec:ex1} describes the experiment to compare the hyperparameter optimisation strategies. The second experiment, to investigate the relationship between neural network architectures and energy consumption, is described in Section~\ref{subsec:ex2}. 

%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.9\linewidth]{img/Core-Method.pdf}
%    \caption{Methodology process}
%    \label{fig:methods}
%\end{figure}

\subsection{Hyperparameter Optimisation}
\label{subsec:ex1}
Given that the response function of a hyperparameter optimisation problem $f(x_1,...,x_n)$ has a \textit{low effective dimensionality}~\cite{bergstra2012random}, meaning that the function can be approximated by another function $g(x_1,...,x_{n-i})$ with less variables, the hypothesis for $RQ_1$ is that random search will converge faster than grid search, because it does not consider two identical values more than once. Given enough time, Bayesian optimisation should outperform the other two strategies. However, with a limited run budget, we might observe that the Bayesian strategy performs worse because it chooses to exploit suboptimal solutions rather than explore better ones.

The setup of the experiment, as is depicted in Table~\ref{tab:exp1}, involves 18 different configurations. Each optimisation strategy is applied twice to the DensePolyNN, DenseLinearNN and SimpleCNN mentioned in section~\ref{sec:methods}. The \textit{hyperparameters} column in Table~\ref{tab:exp1} shows how many parameters are optimised during a run. The five hyperparameters refer to the learning rate ($\alpha$), betas ($\beta_1, \beta_2$), epsilon ($\epsilon$) and weight decay ($w$) of the ADAM optimiser provided by PyTorch\footnote{\href{https://pytorch.org/docs/stable/generated/torch.optim.Adam.html}{https://pytorch.org/docs/stable/generated/torch.optim.Adam.html}}. The entire experiment is repeated for both the FashionMNIST and CIFAR-10 datasets.

\begin{table}[h]
    \centering
    \caption{Comparison of optimisation strategies.}
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{llll}
        \toprule
        Strategy                        & Network & hyperparameters                         \\ 
        \midrule
        Bayesian  & DensePolyNN   & $\alpha, \beta_1, \beta_2$               \\
        & DenseLinearNN     & $\alpha, \beta_1, \beta_2$               \\
        & SimpleCNN & $\alpha, \beta_1, \beta_2$               \\
        & DensePolyNN   & $\alpha, \beta_1, \beta_2, \epsilon, w$  \\
        & DenseLinearNN     & $\alpha, \beta_1, \beta_2, \epsilon, w$  \\
        & SimpleCNN  & $\alpha, \beta_1, \beta_2, \epsilon, w$  \\
        \midrule
        Random & DensePolyNN   & $\alpha, \beta_1, \beta_2$               \\
        & DenseLinearNN     & $\alpha, \beta_1, \beta_2$               \\
        & SimpleCNN & $\alpha, \beta_1, \beta_2$               \\
        & DensePolyNN   & $\alpha, \beta_1, \beta_2, \epsilon, w$  \\
        & DenseLinearNN     & $\alpha, \beta_1, \beta_2, \epsilon, w$  \\
        & SimpleCNN  & $\alpha, \beta_1, \beta_2, \epsilon, w$  \\
        \midrule
        Grid     & DensePolyNN   & $\alpha, \beta_1, \beta_2$               \\
        & DenseLinearNN     & $\alpha, \beta_1, \beta_2$               \\
        & SimpleCNN & $\alpha, \beta_1, \beta_2$               \\
        & DensePolyNN   & $\alpha, \beta_1, \beta_2, \epsilon, w$  \\
        & DenseLinearNN     & $\alpha, \beta_1, \beta_2, \epsilon, w$  \\
        & SimpleCNN  & $\alpha, \beta_1, \beta_2, \epsilon, w$\\
        \bottomrule
    \end{tabular}}
    \label{tab:exp1}
\end{table}

For every row in Table~\ref{tab:exp1}, a network is trained on 64 different hyperparameter settings with 8 repetitions for each setting, amounting to 512 training iterations. After each set of repetitions, the optimisation function provides a new set of values for the hyperparameters. A trained model is evaluated and the results are logged. In total, we run 18~configurations $\times$ 64~optimisation rounds $\times$ 8~repetitions $\times$ 2~data sets $=$ 18,432~training iterations.

\subsection{Network Architecture}
\label{subsec:ex2}
The second experiment aims to answer $RQ_2$ by collecting empirical data that shows the relationship between the structure of a neural network and its energy consumption. We present a full factorial design in Table~\ref{tab:exp2}. The results of this experiment highlight the energy efficiency or lack thereof for the linear, convolutional and ReLU layers. The interesting point for discussion will be whether reducing the network complexity has a significant, positive influence on the energy efficiency, without too heavily compromising on the accuracy.  

\begin{table}[ht]
    \centering
    \caption{Configurations of the model architecture.}
    %\vspace{-1em}
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{llll}
        \toprule
        Linear layers & Convolutional layers & ReLU layers  \\ 
        \midrule
        3           & 1            & 0            \\
        3           & 1            & 1           \\
        3           & 4            & 0            \\
        3           & 4            & 4           \\
        7           & 1            & 0            \\
        7           & 1            & 1           \\
        7           & 4            & 0            \\
        7           & 4            & 4           \\
        \bottomrule
    \end{tabular}}
    \label{tab:exp2}
\end{table}

For every row in Table~\ref{tab:exp2}, the SimpleCNN model from Section~\ref{sec:methods} is trained on 8 different hyperparameter settings with 24 repetitions for each setting, using the random optimisation strategy. Again, the experiment is repeated for both the FashionMNIST and CIFAR-10 datasets. Because accuracy is not the main metric for this experiment, we are less interested in finding different hyperparameter settings as opposed to the first experiment. For this reason, we reduce the number of optimisation rounds and increase the number of repetitions. In total, we run 8~configurations $\times$ 8~optimisation rounds $\times$ 24~repetitions $\times$ 2~data sets $=$ 3072~training iterations.
