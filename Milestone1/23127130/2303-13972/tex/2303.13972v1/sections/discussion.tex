\section{Discussion}
\label{sec:discussion}
This empirical study aims to provide insights into possible improvements for deep learning pipelines out of environmental considerations. In this section, we answer both research questions by analysing the results of the experiments.

\subsection{Hyperparameter Optimisation}
The conclusion to $RQ_1$ is that \textit{Bayesian optimisation} is the most energy-efficient strategy during the training of a machine learning model. Out of all three strategies, Bayesian optimisation consistently finds hyperparameter sets that result in the highest accuracy and it does so within the least amount of optimisation rounds ($\pm$27). Because this strategy requires the storage and constant fitting of a probabilistic model, one downside is the difficulty of parallelisation, but even that is not impossible~\cite{snoek2012practical}. Based on our results, there seems to be no good argument to choose one of the other methods. 

Nevertheless, we cannot deny the presence of grid search and random search within the deep learning field. Both algorithms are easy to understand and implement, and could serve a purpose during early exploration or calibration. In this context, does one of the algorithms dominate the other? Solely based on our results, we cannot make any decisive claims. We can, however, assess the practicality of both solutions. Grid search is an exhaustive method with a search space that increases exponentially by the number of hyperparameters. Considering every set in that search space is not feasible and goes against our philosophy of energy-efficient training. As a consequence, we can only consider a portion of the complete search space, which defeats the purpose of the grid search. By randomly selecting samples from the search space, grid search devolves into a random search with a finite number of options. Furthermore, as Bergstra and Bengio~\cite{bergstra2012random} explain: hyperparameter optimisation problems in high-dimensional spaces have a low effective dimensionality. What this entails in our context is that some parameters will have a much larger influence on the accuracy than others. Figure~\ref{fig:grid_vs_random} illustrates how random search exploits this property more effectively than grid search. The cubes in the image represent a three-dimensional problem where only one parameter has a significant influence on the function value. With the grid search (left), although we consider 27 distinct samples, only 3 values of the important parameter are tested. On the contrary, the random search (right) tests a new value for every sample. 

Furthermore, random search facilitates the job of AI engineers as it does not require any human guidance apart from selecting the bounds. For these reasons, we recommend the use of random search over grid search for the early stages of the training. Our results show that random search is not worse than grid search for problems with $\leq5$ hyperparameters. Additionally, random search should remain a valid baseline strategy with an increasing number of parameters, while grid search will fall short due to the expanding search space. 

\begin{figure}
    \centering
    \includegraphics[width=.85\linewidth]{img/random_vs_grid_final2.png}
    \caption{Grid search (left) vs Random search (right) on a problem with a low effective dimensionality.}
    \label{fig:grid_vs_random}
\end{figure}

\subsection{Network Architecture}
To answer $RQ_2$: reducing the layer complexity of a neural network is a valid option to lower the energy consumption of a deep learning pipeline. Besides computer vision, we believe that our results can be generalised to other fields such as speech recognition or natural language processing as well. The computational complexity of different layer types is a constant that will present similar effects on the energy consumption in a different context. The observation of diminishing performance is also not a very bold claim. However, whether the loss in accuracy resulting from architecture simplifications is acceptable, depends largely on the context. As the visuals in Figure~\ref{fig:e2_evsa_fm} and the corresponding summarised data in Table~\ref{tab:accuracy_tradeoff} make apparent, the accuracy gain for introducing complexity on a relatively simple problem (FashionMINST) is very small. In this case, we would argue that the diminishing return in accuracy is not worth doubling the number of expended Joules. For the CIFAR10 problem set, although there are still diminishing returns, the difference in accuracy we observe is quite significant. Ultimately, what it comes down to is how much error is acceptable for the application in question. To aid this decision, it is important that researchers monitor the performance slope of their model and that they report some metric that relates to the energy efficiency throughout the pipeline, such as Joules or FLOPs. By combining the accuracy and energy trends, we can make more considerate design choices, reduce the layer complexity, and improve the efficiency of the pipeline. As we have shown, these changes could lower the overall energy consumption of a training pipeline by half (i.e., increasing the number of layers from 1 to 4 leads to a 95\% energy consumption increase). Many state-of-the-art models could also benefit from this philosophy. Following the current trend, new models are becoming exponentially larger and more costly, while the performance only sees marginal increases. If all these models would also report their energy consumption, it would vastly change the perspective of which one is `the best' and give rise to new research efforts that focus on energy-efficient design.

\subsection{\textit{Extra:} optimising GPU load}

While collecting energy measurements and analysing results, we were faced with a natural follow-up question: do we really need to measure energy consumption or could we simply rely on time efficiency? While looking at our data, we noticed that different experiments yield a different GPU load. Hence, a model that trains faster might be using the GPU more efficiently, but one cannot immediately draw conclusions w.r.t. the pipeline's total energy consumption.

Nevertheless, we know from previous research that AI systems that optimise GPU usage can reduce their energy consumption by a factor of 10~\cite{wu2021sustainable}. When we look at our experiments, model training resulted in a GPU load that ranged between 40\% and 60\%. Similar values have been reported by a study conducted at Facebook AI~\cite{wu2021sustainable}: a large portion of machine learning model experimentation only utilise their GPUs at 30--50\%. This shows that an important step for Green AI engineering is to monitor and optimise GPU acceleration in pipelines. One should also consider the embodied carbon from the GPU hardware. This is a serious problem because underutilising models require more GPUs than what should be theoretically sufficient~\cite{yeung2020towards}.

Hence, we argue that AI frameworks ought to feature GPU-enabled operations out of the box. Tools should be improved to support both AI practitioners and software engineers to monitor and optimise the GPU usage of their AI systems and pipelines. Developing AI systems is already a transdisciplinary field that requires expertise across different domains. Therefore, making energy consumption a first-class citizen for designing these systems will facilitate communication across disciplines and boost \greenai{} efforts substantially.
