\vspace{-0.5em}
\section{Threats to Validity}
\label{sec:t2v}
%\vspace{-0.5em}
In this section, we go through potential threats to the internal, external and construct validity, as well as the reliability.

\mypar{Internal validity}
It could be argued that our method of measuring energy for $RQ_2$ does not provide an unbiased value. Different tasks running in the background could introduce noise to our measurements. To reduce the influence of this threat, the experiment was performed on a clean installation of Ubuntu 20.04. The only redundant program that might have had a slight impact on the measurements was a running instance of TeamViewer\footnote{\href{https://www.teamviewer.com/nl/}{https://www.teamviewer.com/nl/}} that was used for periodic monitoring. Every optimisation round included 24 repetitions to drown out this effect.
Moreover, when calculating the effect sizes of the layer types, we omit the hyperparameter sets. It is possible that different hyperparameter settings change the overall energy consumption of a neural network, however, in Section~\ref{subsec:res2} we calculate the coefficient of variance to show that this effect is negligible.

\mypar{External validity}
During the experiments, we did not consider the optimal utilisation of the GPU. This might have a negative impact on the generalizability because the relation between utilisation and power is not necessarily linear. Kistowski et al.~\cite{v2015analysis} find that for CPUs, there is a steep increase in power output starting at around 80\% utilisation. Furthermore, we performed the study by using the PyTorch framework only, which is deemed less energy-efficient compared to TensorFlow~\cite{Georgiou2022May}. Yet, we kept the same framework for all the experiments to limit the impacting factors on the measured variables, and to mitigate the associated threats.

\mypar{Construct validity}
Because we solely consider the power usage of the GPU and ignore the contributions of other components, such as memory access, we do not capture the actual energy consumption for training a model. Nevertheless, we specifically selected GPU-heavy workloads and made sure to factor out the idle energy consumption. The results are therefore still valuable to compare relative to each other.

\mypar{Reliability}
To increase the reliability, we made an effort to assemble a complete replication package. The source code for training the neural networks, along with the results of the experiments and the statistical analysis, are all available online\footnote{\href{\materialurl}{\materialurl}}. These components were created by a single developer, but all the involved authors reviewed and approved the entire process. The statistical analysis was replicated by one of the authors to confirm the findings.