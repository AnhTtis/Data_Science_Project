\vspace{-0.5em}
\section{Related Work}
\label{sec:related}
\begin{comment}
There is a general consensus that developing energy-efficient software systems is far from trivial~\cite{pang2015programmers,sahin2014code}. Hence, the discipline of green software engineering has long been studying how to realise this. Previous work proposes a catalog of design patterns aiming to reduce the energy consumption of mobile applications~\cite{cruz2019catalog}. The authors mine 1,700+ mobile applications to collect code changes introduced by developers to improve energy efficiency. The code changes are then studied and grouped in different patterns, resulting in a catalog of 22 energy patterns. However, since the study is targeted to mobile applications, it is not clear which of these patterns would fit an AI system.

%Other works aim at comparing the energy consumption of different development environments. Pereira et al. showcase the energy consumption of 27 different programming languages~\cite{pereira2021ranking}. They use different benchmarks for validation and compare results with memory efficiency and time efficiency. Overall, programming languages such as C, Rust, Ada and C++ consistently yield high energy efficiency. In contrast, Python -- one of the most popular programming languages in AI projects -- scored very low when compared to other modern languages. However, the benchmark used in this study is not representative of common AI systems which typically rely on API calls to fully optimised frameworks. Hence, most energy consumption in AI is likely to stem from external libraries and frameworks that are not necessarily written in the same language of the system -- e.g,. NVIDIA Cuda python.

Within the green software community, AI has been used to provide more accurate estimations models of software energy consumption~\cite{chowdhury2019greenscaler}. In their work, Chowdbury et al. found that CPU-usage is a good proxy for energy consumption in software applications. While AI is a powerful tool to address energy efficiency in traditional software systems, we are interested in the opposite: we apply an empirical software engineering approach to address energy efficiency in AI systems. Moreover, it is not clear to what extent previous results still apply to AI software.

Contributions to green software have also scoped a more holistic approach within software organisations. Hankel et al. have studied the factors influencing the adoption of green processes within ICT organisations~\cite{hankel2019green}. Verdecchia et al. propose short-term solutions that lead to impact on energy efficiency~\cite{verdecchia2021green} -- examples include opting for green energy sources and utilising AI-specific hardware. Our work addresses green technologies at a higher-level of abstraction -- for instance, by addressing energy efficiency at the level of designing the AI pipeline.
\end{comment}

Given the particularities of different types of software systems, green software contributions span across multiple sub-fields of computer science: mobile computing~\cite{cruz2019catalog,chowdhury2019greenscaler}, Web~\cite{de2021runtime}, robotics~\cite{malavolta2021mining}, and so on. 
In our work, we challenge the green software engineering field to expand to AI systems. To the best of our knowledge, related research in \greenai{} is still preliminary and does not yet follow the scientific method that drives the research in green software. We pinpoint below the most relevant related contributions in \greenai{}.

Schwartz et al.~\cite{schwartz2020green} present an elegant introductory article into this field of research. The authors introduce two novel terms to guide future conversations: \greenai{}, which refers to AI research that considers computational cost as a primary metric next to accuracy; and \redai{}, the most common form of AI research that seeks to improve accuracy without any regards for the computational resources required. Ultimately, Schwartz et al. call for a research agenda that aims to reduce carbon emissions and make the deep learning field more accessible to everyone. Our work takes a preliminary step towards this goal, by presenting empirical results focused on different parts of the training pipeline that can lead to energy-efficiency gains on a larger scale. 

Strubell et al.~\cite{strubell2019energy} look into the quantity of energy consumption in the domain of Natural Language Processing (NLP). The authors present preliminary results showing that the accuracy of trained NLP models has improved substantially at the expense of a serious amount of energy. Our study aims to provide scientifically proven advice to help design energy-efficient AI systems, including NLP.

Li et al.~\cite{li2016evaluating} address a similar problem specifically targeted towards applications of convolutional neural networks (CNNs). In their work, the authors compare a set of well-known CNN models in terms of energy efficiency. They also assess to what degree the different types of layers contribute to the overall energy consumption. As such, they provide percentages for the convolutional layers, fully connected (or linear) layers, pooling layers and ReLU layers. Apart from finding the energy efficiency of these layer types, our study also includes a trade-off analysis where we compare energy consumption to accuracy.

% Liashchynskyi and Liashchynskyi~\cite{liashchynskyi2019grid} conduct a small-scale experiment comparable to the one presented in this paper. Although the focus is not so much on energy efficiency, the proposed methods can easily be used for a different purpose. The authors use classical hyperparameter optimisation algorithms to construct the most optimal neural network in terms of architecture. In their study, they compare grid search to random search and evolutionary search.  

% García-Martín et al.~\cite{garcia2019estimation} shed light on the fact that current machine learning practices neglect energy efficiency as an important metric. The authors believe that researchers tend to disregard energy consumption because of a lack of familiarity with modern energy estimation approaches. To address this concern, their paper features a literary review of different energy estimation and measurement approaches. We adopt one of these methods to estimate the energy consumption of GPU intensive workloads, this is elaborated on further in Section~\ref{sec:methods}.
% IF POSSIBLE MOVE IT TO DISCUSSION -- the importance of measuring energy.

Yang et al.~\cite{yang2017designing} propose a new pruning method, named energy-aware pruning, that removes layers' weights to reduce the energy consumption. They report a reduction in the overall consumption by a factor between 1.6x and 3.7x, with an insignificant loss in accuracy. Our approach to optimise the network architecture is much more coarse-grained compared to the techniques described in this paper. Rather than focusing on fine-tuning the weights inside the layers, we investigate the factor of redundancy of those layers and provide advice that is more relevant from a design-level perspective. Both philosophies could be used together.    


%Georgiou et al.~\cite{Georgiou2022May} compare deep learning frameworks PyTorch and TensorFlow during training and inference, to determine the most efficient in terms of energy and run-time performance.
%The authors also assess the accuracy of models using the different frameworks. Their results indicate that there are differences in the performance between the frameworks.
%, and for the pipeline stages.
%TensorFlow is more energy and run-time efficient for training, while Pytorch is the best for the inference stage. They note that overall the training phase is more expensive than inference.
%Moreover, they show that prioritizing the energy efficiency does not necessarily reflect negatively on the accuracy of the model when choosing the framework. 
%In our study, we share the same view that opting for considering energy efficiency while designing the deep learning models does not impair their accuracy in a substantial way. Instead of examining that trade-off regarding the framework, we inspect the training practices in more depth, focusing on hyperparameter tuning and the network architecture.

Two other related studies examine the trade-off between energy-efficiency and accuracy, either regarding the learning frameworks PyTorch and TensorFlow during training and inference~\cite{Georgiou2022May}, or the solvers used for the training of logistic regression models~\cite{Gutierrez2022}.
For the framework, TensorFlow is more energy and run-time efficient for training, while Pytorch is the best for the inference stage.
As for the solver, LBFGS is shown to be more energy-efficient than Newton-CG and SAG.
%
Moreover, these works demonstrate that prioritizing the energy efficiency does not necessarily reflect negatively on the accuracy of the model.
%when choosing the framework or the solvers for logistic regression models. 
In our study, we share the same view that opting for considering energy efficiency while designing the deep learning models does not impair their accuracy in a substantial way. Instead of examining that trade-off regarding the framework or logistic regression models, we inspect the deep learning training practices in more details, focusing on hyperparameter tuning and the network architecture.