\begin{abstract} \textbf{
%Â The sustainability of software systems is rising to the top of the agenda for tech organisations. It is therefore important to explore new practices for the development of energy-efficient software. This concern takes another level with AI-enabled applications.
Modern AI practices all strive towards the same goal: better results. In the context of deep learning, the term ``results'' often refers to the achieved accuracy on a competitive problem set. 
%
In this paper, we adopt an idea from the emerging field of \greenai{} to consider energy consumption as a metric of equal importance to accuracy and to reduce any irrelevant tasks or energy usage.  
%
We examine the training stage of the deep learning pipeline from a sustainability perspective, through the study of hyperparameter tuning strategies and the model complexity, two factors vastly impacting the overall pipeline's energy consumption.
%
First, we investigate the effectiveness of grid search, random search and Bayesian optimisation during hyperparameter tuning, and we find that Bayesian optimisation significantly dominates the other strategies.
%We find that Bayesian optimisation significantly dominates the other strategies, as it can find the most optimal set of hyperparameters within the least number of optimisation rounds. 
%
Furthermore, we analyse the architecture of convolutional neural networks with the energy consumption of three prominent layer types: convolutional, linear and ReLU layers. The results show that convolutional layers are the most computationally expensive by a strong margin. Additionally, we observe diminishing returns in accuracy for more energy-hungry models. The overall energy consumption of training can be halved by reducing the network complexity.
%In some cases, a doubling of the energy consumption is reflected by a single per cent increase in accuracy.
%
%In conclusion, we highlight promising energy-efficient practices for training deep learning models, and we advocate for more research and practices in \greenai{}, leading to more sustainable design decisions.
In conclusion, we highlight innovative and promising energy-efficient practices for training deep learning models. To expand the application of \greenai{}, we advocate for a shift in the design of deep learning models, by considering the trade-off between energy efficiency and accuracy.
}
\end{abstract}

\begin{comment}
%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011074.10011075.10011078</concept_id>
<concept_desc>Software and its engineering~Software design tradeoffs</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10010940.10011003</concept_id>
<concept_desc>Software and its engineering~Extra-functional properties</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software design tradeoffs}
\ccsdesc[500]{Software and its engineering~Extra-functional properties}
\end{comment}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
%\keywords{green software, green ai, deep learning, hyperparameter tuning}
