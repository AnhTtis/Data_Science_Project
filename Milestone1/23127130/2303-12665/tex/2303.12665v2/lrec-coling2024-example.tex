% LREC-COLING 2024 Example; 
% LREC Is now using templates similar to the ACL ones. 
\documentclass[10pt, a4paper]{article}

\usepackage{lrec-coling2024} % this is the new style



\title{Can We Identify Stance Without Target Arguments? A Study for Rumour Stance Classification}
%Maybe it would be good to change the title options:
%\title{Can we identify stance without target arguments? A study for Rumour Stance Classification}


\name{Yue Li, Carolina Scarton} 

\address{University of Sheffield\\
         211 Portobello, Sheffield, UK\\
         {yli381, c.scarton}@sheffield.ac.uk\\}



% \abstract{
% Each paper must include an abstract of 150 to 200 words in Arial 9 pt with interlinear spacing of 10 pt. The heading Abstract should be centered, font Arial 10 pt bold. This short abstract will also be used for producing the Booklet of Abstracts (PDF) containing the abstracts of all papers presented at the Conference.

\abstract{
Considering a conversation thread, rumour stance classification aims to identify the opinion (e.g. agree or disagree) of replies towards a \textit{target} (rumour story). Although the target is expected to be an essential component in traditional stance classification, we show that rumour stance classification datasets contain a considerable amount of real-world data whose stance could be naturally inferred directly from the replies, %without reading the target, 
contributing to the strong performance of the supervised models without awareness of the target. We find that current target-aware models %exhibit disappointing 
underperform in cases where the context of the target is crucial. Finally, we propose a simple yet effective framework to enhance reasoning with the targets, achieving %the best 
state-of-the-art performance on two benchmark datasets.
 \\ \newline \Keywords{rumour stance classification, rumour analysis on social media, stance classification} }

\begin{document}

\maketitleabstract

\section{Introduction}

Automatic stance classification that aims to identify the type of an expressed opinion towards a single or multiple \textit{targets}, plays a key role in many Natural Language Processing (NLP) applications, such as %argument mining \citep{lawrence2020argument}, 
rumour analysis \citep{zubiaga2016analysing}. %, and information retrieval \citep{sen2018stance}.  
A target could be a person, an organisation, or rumour story, depending on the use case \citep{hossain-etal-2020-covidlies,zubiaga2016analysing,ferreira-vlachos-2016-emergent,allaway-mckeown-2020-zero}. The target plays a fundamental role in stance classification, being expected to appear either explicitly or implicitly, making it a key difference from sentiment analysis that can be framed as target-independent \citep{kuccuk2020stance,liu-etal-2022-target}.

Previous work shows that a BERT-based model, without awareness of the target, achieves comparable or even better performance than target-aware models on many stance classification datasets, due to spurious sentiment- and lexicon-stance correlations in the training sets \citep{kaushal-etal-2021-twt}. Similar results are observed in other context-dependent tasks, such as Natural Language Inference and Argument Reasoning
Comprehension, where models without background knowledge achieve an impressive performance due to spurious or superficial cues in the datasets \citep{poliak-etal-2018-hypothesis,niven-kao-2019-probing}. 

In this paper, we further analyse the above phenomenon for \textit{rumour stance classification} on Twitter. Given a conversation initialised by a rumourous \textit{source tweet}, this task aims to classify the stance of each reply towards the rumour into \textit{support}, \textit{deny}, \textit{query} and \textit{comment}. The vagueness and lack of specificity in the reply tweets result in the disparity between rumour stance classification and traditional stance classification datasets. %Consider an example of rumour stance classification in Figure \ref{fig:example}. 
For instance, in Figure \ref{fig:example}, one can reasonably deduce that the reply from \textit{u2} disagrees with the target before reading the content of the target. This is in contrast to traditional stance classification where the stance may vary for different targets, making it always essential to consider them (e.g., \citealp{sobhani-etal-2017-dataset,conforti-etal-2020-will}). %, making it always essential to consider the target. 
%In this paper, 

%%%%%%%%%%%%
\begin{figure}[t!]
 \centering

 \includegraphics[width=0.35\textwidth]{image/E9.png}
 
 \caption{Example of Target-Independent (T-I) and Target-Dependent (T-D) direct replies that \textit{deny} a target from \citet{gorrell-etal-2019-semeval}.}
 \label{fig:example}

\end{figure}
%%%%%%%%%%

We empirically show that the strong behaviour of models without awareness of the target (dubbed \textit{target-oblivious}) could be explained by the existence of the reply posts whose stance can be naturally inferred without knowing the target. More importantly, we demonstrate that current state-of-the-art target-aware models lack reasoning with the target, performing unexpectedly poorly on the cases when the target is necessary. Based on our observations, we propose a simple yet effective framework which would benefit from the target-oblivious model and would also enhance the reasoning with the targets. 
 


\section{The Role of Target Arguments} %in Dataset and Models}

% We hypothesise that there is real-world data where target is not necessary to infer the stance of the reply (Figure \ref{fig:example}). 
We conduct an annotation study by categorising the replies into \textit{target-dependent} (i.e. target is essential for stance inference) and \textit{target-independent} (i.e. target is unnecessary for stance inference). We then evaluate various models trained with or without awareness of the target (i.e. \textit{target-aware} and \textit{target-oblivious} models). 

\subsection{Data Annotation} 

\paragraph{Dataset} Three established English datasets are available for rumour stance classification on social media: \textit{PHEME} \citep{zubiaga2016analysing}, \textit{RumourEval 2017} \citep{derczynski-etal-2017-semeval} and \textit{RumourEval 2019} \citep{gorrell-etal-2019-semeval}. RumourEval 2017 consists of the English PHEME dataset, and RumourEval 2019 is an extension of the 2017 dataset. Therefore, we consider the largest RumourEval 2019 dataset.\footnote{The dataset contains Twitter and Redddit. To alleviate the impact of text length, we focus on the Twitter data only}
RumourEval 2019 training and validation sets consist of conversations regarding rumour stories which emerged during breaking news (e.g., Germanwings plane crash, and shooting in Ottawa), and the test data contains unseen rumours about natural disasters. %such as hurricanes and floods. 
The target of the stance, rumour story, is implied by the source tweet that initialises the conversation. Hence we consider the source tweet as the target.
Among the four stances, support and deny classes are the most informative for rumour verification, while the comment class is the least useful \citep{scarton-etal-2020-measuring}. Therefore, we annotate all the replies in support, deny and query classes in the validation and test sets, with 50 randomly sampled comments from each set.

\paragraph{Annotation Process} Two annotators manually categorised each reply into either target-independent or -dependent, by answering one question: “\textit{do you think you need the source tweet to infer the stance of this reply?}” Aiming to validate the annotations, annotators were also asked to classify the stance of the tweets. We then compared their assigned class with the gold standard label and, if they differed, we altered their annotation from target-independent to -dependent. Annotators did not have access to the source tweet and the tweets from validation and test sets were shuffled before annotation. The inter-annotator agreement is of 72.5\% and Cohen’s Kappa is 0.565.

\paragraph{Result} We observe a significant amount of data whose stance can be deduced without knowing the specific rumour story (Table \ref{tab:results-annotate-target}), especially in the deny and query classes. More than 50\% denies are target-independent in the validation and test sets.
Target-independent denies are tweets that directly cast doubt with negation words (e.g. “Fake news”, "This is false"). The queries tend to be target-independent, since most of them are interrogative sentences asking for more evidence. 
% However, the disagreement between two annotators largely falls into this category due to the ambiguous or uninformative texts (e.g., “blood clot?”, “WHAT?”). 
However, the annotators did not identify many of them due to the ambiguity or non-informativeness of the texts (e.g., “blood clot?”, “WHAT?”). Most of the target-independent supports are retweets and quote tweets, whose context is self-contained. Tweets in the comment class are less relevant to the veracity of the rumour story, however, determining their relevance normally necessitates reasoning with the rumour story itself. %Therefore, no target-independent tweets in the comment class were identified. 
% We present more examples of target-independent and -dependent tweets in the Appendix.

\begin{table}[t!]
\centering
\scalebox{0.65}{
\begin{tabular}{lcccc}
\hline
Dataset & Support & Deny & Query & Comment\\
\hline
Validation & 20 (29\%) & 35 (51\%) & 42 (40\%) & 0 (0\%)\\
\hline
Test & 12 (13\%) & 66 (72\%) & 17 (30\%) & 0 (0\%)\\
\hline
\end{tabular}
}
\caption{Number of target-independent tweets in each stance in the validation and test sets (proportion in brackets).} 
\label{tab:results-annotate-target}
\end{table}


% Table \ref{tab:examples} presents  


% \begin{table*}[ht!]
% \centering
% \scalebox{0.8}{
% \begin{tabular}{lp{9cm}p{8cm}}
% \hline
% \textbf{Stance} & \textbf{Source Tweet} & \textbf{Reply Tweet}\\
% \hline
% \multicolumn{3}{l}{\textbf{Target-dependent replies}} \\
% \hline
% Deny & 267 days since Sick Hillary had a press conference. & @USER wha do you mean she had one with Anderson cooper over the telephone \\
% Deny & BREAKING: At least 10 killed in shooting at French satirical newspaper Charlie Hebdo, Paris prosecutor's office says. & @USER 11 killed \\
% Support & Germanwings co-pilot had serious depressive episode: Bild newspaper & @USER The pilot was NOT FIT TO FLY ! \\
% Support & Report: Red Cross Was Stealing from Church Doorsteps to Redistribute or Sell Items for Profit? & @USER @USER Stealing is stealing, regardless of how you want to dress it up. \\
% \hline
% \multicolumn{3}{l}{\textbf{Target-independent replies}} \\
% \hline
% Deny & BREAKING: Illegal Muslim From Iran Arrested For Starting California Wildfire HTTPURL & @USER No source cited in this article, no date... I would not rely on this and neither should you. \\
% Deny & Prince William and Harry donates \$ 100 million to Hurricane Harvey Victims – News 360 & @USER Fake news!! \\
% Query & Black Lives Matter THUGS Blocking Emergency Crews From Reaching Hurricane Victims via @USER & @USER @USER @USER Where and when ? Other links to ? \\
% Support & Ongoing hostage situation in Sydney café. Major landmarks like the Sydney Opera House evacuated & Special Prayers for tonight "@USER: Ongoing hostage situation in Sydney café.” \\
% Support & Mike Pence Disappointed God Has Never Asked Him To Kill One Of Own Children & @USER There's lot of truth in this \\
% \hline
% \end{tabular}
% }
% \caption{Examples of target-independent and -dependent tweets}
% \label{tab:examples}
% \end{table*}

\subsection{Model Evaluation} 

Given a source tweet ($s_i$), reply tweet to classify ($r_i$), other replies in the conversation ($o_i$) and stance label ($l_i$), we consider two types of supervised models: target-oblivious ($f(r_i)\rightarrow l_i$) and target-aware ($f(s_i,r_i)$ or $f(s_i,r_i,o_i)\rightarrow l_i$) models. We also evaluate a recent large language model (LLM) in zero-shot setting.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[ht!]
\centering
\scalebox{0.65}{
\begin{tabular}{ll|l|lll|lll}
\hline
Type & Model & Full set & \multicolumn{3}{l}{Target-dependent subset} & \multicolumn{3}{|l}{Target-independent subset} \\ 
& & $wF_2$ & $wF_2$ & $F_2(S)$ & $F_2(D)$ & $wF2$ & $F_2(S)$ & $F_2(D)$ \\ \hline \hline
Target-oblivious & BERTweet & \textbf{0.477} & \textbf{0.346} & 0.294 & 0.206 & \textbf{0.749} & 0.615 & \textbf{0.894}\\ \hline \hline
Target-aware & BERTweet & 0.435 & 0.329 & 0.313 & 0.167 & 0.635 & 0.464 & 0.778 \\ 
& BLCU-NLP & 0.371 & 0.223 & 0.080 & 0.217 & 0.399 & 0.000 & 0.737 \\
& BUT-FIT & 0.309 & 0.176 & 0.020 & 0.047 & 0.371 & 0.102 & 0.495 \\
& Branch-LSTM & 0.150 & 0.139 & 0.020 & 0.048 & 0.142 & 0.102 & 0.056 \\
& Hierarchical-BERT & 0.235 & 0.137 & 0.065 & 0.017 & 0.234 & 0.017 & 0.293\\ \hline \hline
LLMs & LLaMA (reply) & 0.256 & 0.227 & \textbf{0.390} & 0.000 & 0.319 & 0.417 & 0.093 \\
& LLaMA (source \& reply) & 0.419 & 0.318 & 0.326 & \textbf{0.234} & 0.685 & \textbf{0.678} & 0.714 \\ \hline \hline

\end{tabular}
}
\caption[]{Model performance over the full test set, target-dependent and -independent direct replies (averaged over experiments.). $F_2(S)$ and $F_2(D)$ denote the $F_2$ scores over support and deny classes, respectively. Highest performance is in bold, with statistical significance (t test, p value<0.05).}
\label{tab:model performance}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{table*}[t!]
\centering
\scalebox{0.65}{
\begin{tabular}{l|cccc|cccc}
\hline
& \multicolumn{4}{c}{Target-dependent subset} & \multicolumn{4}{|c}{Target-independent subset} \\
& Support & Deny & Query & Comment & Support & Deny & Query & Comment\\
\hline
Mask Source Tweet & 40.3 & 69.9 & 98.7 & 85.7 & 43.0 & 90.8 & 98.7 & 89.0\\
\hline
Shuffle Source Tweet & 54.1 & 82.9 & 93.6 & 90.9 & 57.7 & 94.9 & 97.3 & 83.1 \\
\hline
\end{tabular}
}
\caption{The proportion (\%) of target-aware BERTweet predictions of direct replies in each class that are not influenced by the masking or shuffling of the source tweets.}
\label{tab:results-perturbation}
\end{table*}

\subsubsection{Experimental Setups}

\paragraph{Target-oblivious Models} We fine-tune different transformer-based models, whose input is the reply tweet ($f(r_i)$). %In this paper, 
We present the results using BERTweet \citep{nguyen-etal-2020-bertweet} (experiments with BERT \citep{devlin-etal-2019-bert} and Roberta \citep{liu2019roberta} achieved similar performance).
% Performance of other models can be found in the Appendix.

\paragraph{Target-aware Models} We fine-tune BERTweet, which takes input as both source and reply tweets ($f(s_i,r_i)$). We also evaluate four competitive systems that model the whole conversation thread ($f(s_i,r_i,o_i)$):\footnote{Performances of these models are lower than the figures reported in their original paper. The reason is that we do not consider the stance of the source tweet towards rumour, mainly belonging to the support class.} (1) The winner of the RumourEval 2019 shared task, i.e. \textit{BLCU-NLP} \citep{yang-etal-2019-blcu}; (2) \textit{BUT-FIT} \citep{fajcik-etal-2019-fit}, the second place in the 2019 shared task; (3) \textit{Hierarchical-BERT} \citep{yu-etal-2020-coupled}, achieving state-of-the-art performance \citep{hardalov-etal-2022-survey} on the RumourEval 2017 dataset \citep{derczynski-etal-2017-semeval}; (4) \textit{Branch-LSTM} \citep{kochkina-etal-2017-turing}, the winner of the RumourEval 2017 shared task and the baseline model for the 2019 task. 

\paragraph{LLMs} %Due to ethical considerations regarding the exposure of personal data (e.g., to ChatGPT), 
We experiment with the OpenAssistant LLaMA-Based Model \citep{kopf2023openassistant}.\footnote{https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor} We compare the performance between when the source tweet is provided and when it is not (\textit{LLaMA (source + reply)} or \textit{LLaMA (reply)}).\footnote{Due to ethical considerations regarding the exposure of personal data (e.g., to ChatGPT), we opt to use an open-source LLM which was downloaded and hosted on our own server.} 

\paragraph{Evaluation} %Unlike traditional stance classification, the recall of the support and deny classes are more important for rumour stance classification. Thus 
We adopt the weighted $F_2$ score proposed by \citet{scarton-etal-2020-measuring}, which gives higher weights to the support and deny classes, being more adequate to rumour stance classification. %tasks. 


\subsubsection{Results}

As shown in Table \ref{tab:model performance}, not surprisingly, all the models achieve better results on the target-independent samples, since they normally contain explicit stance-associated words or signals, especially for the deny and query classes. The target-oblivious model exhibits strong performance over target-independent tweets, indicating that its performance can be attributed to the existence of these samples in the dataset. 

We expected that target-aware models, especially the ones that consider the whole conversation information, would perform significantly better than target-oblivious models on the target-dependent tweets for which the context of the source tweet is essential. However, %we observe that 
BUT-FIT, Branch-LSTM and Hierarchical-BERT could not correctly predict any target-dependent supports or denies that the simple target-oblivious BERTweet fails to identify, casting doubt on the usefulness of these approaches. BLCU-NLP is the only conversation-based system that outperforms the target-oblivious model over the target-dependent denies, likely due to their data augmentation for this class. But its performance over the target-dependent supports is rather disappointing.

Target-aware BERTweet shows strength on detecting target-dependent supports, when compared with its target-oblivious counterpart; however, it falls behind on the deny class. The existence of negation words (e.g., “not”) in the target-dependent denies may contribute to the good generalisation of target-oblivious BERTweet. %on these tweets. 

LLaMA exhibits competitive results, achieving best performance on the target-dependent samples in the support and deny classes. However, gaps still exist between the fine-tuned BERTweet models on the full test set. Without the source tweet, the performance drops significantly, except for the target-dependent supports. 






\subsubsection{Target perturbations} Aiming to further investigate the role of the target in target-aware models, we experiment with two perturbations during inference: (1) Masking: the entire source tweet is replaced by a white space; (2) Shuffling: the original source tweet is replaced by a source tweet related to another rumour story so that the reply and “new” source tweets are mismatched. Both approaches should significantly change the model performance over the target-dependent tweets, provided the source tweet is properly reasoned with. We expect the comment class to be less impacted because the irrelevance between source and reply tweets should be considered as comment. We discuss the results of the target-aware BERTweet, since it is the best performing model in this category (other models showed similar results).

%As shown in , 
Masking or shuffling the source tweets has minimum impact over the predictions for the \textit{deny}, \textit{query} and \textit{comment} classes (Table \ref{tab:results-perturbation}). More than 69\% of predictions in each class stay the same, no matter whether the target is essential or not. For the \textit{support} class in which target-aware BERTweet achieves better results over target-dependent samples, 40\% to 60\% of predictions do not change. The results suggest that target-aware models may be overfitting towards the replies, behaving like a target-oblivious model. 



\section{Ensemble-based Framework}

Equipped with the observation of target-independent cases and the lack of reasoning with the target in target-aware models, we propose a simple yet effective ensemble-based framework to leverage the advantage of the target-oblivious model meanwhile improving the performance over the target-dependent samples. 

We assume a pre-trained target-oblivious model ($f(r_i;\theta)=p_i$). The aim is to adopt an ensemble with a target-aware model ($f'(s_i,r_i;\theta')=q_i$) where $p_i$ and $q_i$ are posterior probability distribution over the four stance classes for a sample $i$ with a pair of source ($s_i$) and reply ($r_i$) tweets. To encourage the target-aware model to learn from target-dependent samples during training, we propose a cross-attention based architecture with a sample re-weight mechanism. 

\paragraph{Siamese Network with Cross-attention} We utilise a siamese pre-trained transformer-based network \citep{reimers-gurevych-2019-sentence} to encode the source ($s_i$) and reply ($r_i$) tweets. Then, to explicitly indicate the importance of the tokens in the reply representation ($h_{r_i}$) with respect to the source representation ($h_{s_i}$), we calculate the cross-attention \citep{vaswani2017attention} between them, with $h_{s_i}$ as the key and value, and $h_{r_i}$ as the query.  

\paragraph{Sample Re-weight} We train the model on weighted data, where the weight of instance $i$ is $1-p_{y_i}$ ($p_{y_i}$ is the posterior probability assigned to the true label $y_i$) \citep{clark-etal-2019-dont}. The intuition is to encourage the target-aware model to focus on potential target-dependent examples that the target-oblivious model gets wrong. 
% The implementation details can be found in Appendix.

\paragraph{Implementation} Target-oblivious and -aware models are based on BERTweet but our method can be easily generalised to other pre-trained language models. The optimal target-oblivious model is chosen based on the validation set. 

\subsection{Experimental Setup}

\paragraph{Datasets} We validate our proposed framework on two benchmark datasets: RumourEval 2017 and 2019 datasets.

\paragraph{Comparing Baselines} We compare with the Pretext Task-based Hierarchical Contrastive Learning model (\textit{PT-HCL}) \citep{liang2022zero}. To the best of our knowledge, PT-HCL is the only study that exploits “target-invariant/-specific features” \citep{liang2022zero} in traditional stance classification. We also present ablations for our proposed method, by removing the sample re-weighting mechanism (\textit{w/o weight}), replacing the cross-attention by self-attention on the concatenation of the source and reply tweet representations (\textit{w/o cross-att}), or both simultaneously (\textit{w/o weight,cross-att}). Performance over RumourEval 2019 dataset is comparable with models in Table \ref{tab:results-proposed}. As for RumourEval 2017, we also compare with its state-of-the-art model (Hierarchical-BERT), target-oblivious and -aware BERTweet and OpenAssistant LLaMa.

\subsection{Results}

As shown in Table \ref{tab:results-proposed}, our proposed approach outperforms PT-HCL on both datasets, also surpassing other models. Removing sample weights or cross-attention would reduce the model performance, indicating their contribution. %to the model performance. 

\begin{table}[ht!]
\centering
\scalebox{0.65}{
\begin{tabular}{l|cc}
\hline
Method & RumourEval2019 & RumourEval2017 \\
\hline
PT-HCL & 0.452 & 0.431 \\
\hline
Hierarchical-BERT & 0.235 & 0.275\\
LLaMA & 0.419 & 0.314\\
Target-oblivious BERTweet & 0.477 & 0.425 \\
Target-aware BERTweet & 0.435 & 0.426 \\
\hline
\textbf{Proposed Method} & \textbf{0.510} & \textbf{0.452} \\
w/o weight & 0.458 & 0.421 \\
w/o cross-att & 0.438 & 0.417\\
w/o weight,cross-att & 0.451 & 0.419\\

\hline
\end{tabular} 
}
\caption{Averaged $wF_2$ over experiments for two datasets. Highest performance is in bold, with statistical significance between the proposed method (t test, p value <0.05).}
\label{tab:results-proposed}
\end{table}

We also evaluate our proposed method on target-dependent and -independent subsets. 
% (Table \ref{tab:results-proposed-TI-TD}). 
Comparing with Table \ref{tab:model performance}, it achieves the best results on both target-dependent ($wF_2=0.396$, with $F_2(S)=0.399$, $F_2(D)=0.211$) and -independent examples ($wF_2=0.802$, with $F_2(S)=0.732$, $F_2(D)=0.901$) , confirming that our proposed framework could not only benefit from the target-oblivious model but also enhance the inference between source and reply tweets. 

% \begin{table}[ht!]
% \centering
% \scalebox{0.65}{
% \begin{tabular}{llll}
% \hline
% Subset & $wF_2$ & $F_2(S)$ & $F_2(D)$ \\ \hline
% Target-dependent & 0.396 & 0.399 & 0.211 \\
% Target-independent & 0.802 & 0.732 & 0.901 \\ 
% \hline
% \end{tabular}
% }
% \caption{Performance of our proposed method over target-dependent and -independent subsets of RumourEval testset.}
% \label{tab:results-proposed-TI-TD}
% \end{table}


\section{Conclusion}

In this paper, we explore the role of the target in rumour stance classification. Our study suggests the strong performance of the target-oblivious model could be explained by the existence of target-independent texts in real-world data. We point out the unexpected weakness of the target-aware models and consequently propose a cross-attention based architecture with sample re-weight mechanism, achieving the best result on two benchmark datasets. We also release our annotation to facilitate future research and model evaluations.\footnote{The link will be available upon acceptance.}

\section{Acknowledgements}

% Place all acknowledgments (including those concerning research grants and funding) in a separate section at the end of the paper.

This work is funded by the European Union under action number 2020-EU-IA-0282 and agreement number INEA/CEF/ICT/A2020/2381686 (EDMO Ireland).\footnote{\url{https://edmohub.ie}} and by EMIF managed by the Calouste Gulbenkian Foundation\footnote{The sole responsibility for any content supported by the European Media and Information Fund lies with the author(s) and it may not necessarily reflect the positions of the EMIF and the Fund Partners, the Calouste Gulbenkian Foundation and the European University Institute.} under the "Supporting Research into Media, Disinformation and Information Literacy Across Europe" call (ExU -- project number: 291191).\footnote{\url{exuproject.sites.sheffield.ac.uk}} Yue Li is supported by a Sheffield–China Scholarships Council PhD Scholarship.

%\nocite{*}
\section{Bibliographical References}\label{sec:reference}

\bibliographystyle{lrec-coling2024-natbib}
\bibliography{lrec-coling2024-example}

%\section{Language Resource References}
%\label{lr:ref}
%\bibliographystylelanguageresource{lrec-coling2024-natbib}
%\bibliographylanguageresource{languageresource}



\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
