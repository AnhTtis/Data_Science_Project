
@article{kuhl2021conduct,
  title={How to conduct rigorous supervised machine learning in information systems research: the supervised machine learning report card},
  author={K{\"u}hl, Niklas and Hirt, Robin and Baier, Lucas and Schmitz, Bj{\"o}rn and Satzger, Gerhard},
  journal={Communications of the Association for Information Systems},
  volume={48},
  number={1},
  pages={46},
  year={2021}
}

@article{kuhl2022artificial,
  title={Artificial intelligence and machine learning},
  author={K{\"u}hl, Niklas and Schemmer, Max and Goutier, Marc and Satzger, Gerhard},
  journal={Electronic Markets},
  pages={1--10},
  year={2022},
  publisher={Springer}
}

@article{koza1996automated,
  title={Automated design of both the topology and sizing of analog electrical circuits using genetic programming},
  author={Koza, John R and Bennett, Forrest H and Andre, David and Keane, Martin A},
  journal={Artificial intelligence in design’96},
  pages={151--170},
  year={1996},
  publisher={Springer}
}

@article{xu2021federated,
  title={Federated learning for healthcare informatics},
  author={Xu, Jie and Glicksberg, Benjamin S and Su, Chang and Walker, Peter and Bian, Jiang and Wang, Fei},
  journal={Journal of Healthcare Informatics Research},
  volume={5},
  number={1},
  pages={1--19},
  year={2021},
  publisher={Springer}
}

@Article{s131115582,
AUTHOR = {Abu-Elkheir, Mervat and Hayajneh, Mohammad and Ali, Najah Abu},
TITLE = {Data Management for the Internet of Things: Design Primitives and Solution},
JOURNAL = {Sensors},
VOLUME = {13},
YEAR = {2013},
NUMBER = {11},
PAGES = {15582--15612},
URL = {https://www.mdpi.com/1424-8220/13/11/15582},
PubMedID = {24240599},
ISSN = {1424-8220},
DOI = {10.3390/s131115582}
}




@article{jiang2020federated,
  title={Federated learning in smart city sensing: Challenges and opportunities},
  author={Jiang, Ji Chu and Kantarci, Burak and Oktug, Sema and Soyata, Tolga},
  journal={Sensors},
  volume={20},
  number={21},
  pages={6230},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{rieke2020future,
  title={The future of digital health with federated learning},
  author={Rieke, Nicola and Hancox, Jonny and Li, Wenqi and Milletari, Fausto and Roth, Holger R and Albarqouni, Shadi and Bakas, Spyridon and Galtier, Mathieu N and Landman, Bennett A and Maier-Hein, Klaus and others},
  journal={NPJ digital medicine},
  volume={3},
  number={1},
  pages={1--7},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{saputra2020federated,
  title={Federated Learning Meets Contract Theory: Economic-Efficiency Framework for Electric Vehicle Networks},
  author={Saputra, Yuris Mulya and Nguyen, Diep and Dinh, Hoang Thai and Vu, Thang X and Dutkiewicz, Eryk and Chatzinotas, Symeon},
  journal={IEEE Transactions on Mobile Computing},
  year={2020},
  publisher={IEEE}
}

@article{tuladhar2020building,
  title={Building machine learning models without sharing patient data: A simulation-based analysis of distributed learning by ensembling},
  author={Tuladhar, Anup and Gill, Sascha and Ismail, Zahinoor and Forkert, Nils D and Alzheimer's Disease Neuroimaging Initiative and others},
  journal={Journal of biomedical informatics},
  volume={106},
  pages={103424},
  year={2020},
  publisher={Elsevier}
}

@article{betti27share,
  title={Share to Gain: Unlocking Data Value in Manufacturing},
  author={Betti, F and Bezamat, F and Fendri, M and Fernandez, B},
  journal = {{World Economic Forum}},
  year={2020}
}

@inproceedings{dwork2018privacy,
  title={Privacy-preserving prediction},
  author={Dwork, Cynthia and Feldman, Vitaly},
  booktitle={Conference On Learning Theory},
  pages={1693--1702},
  year={2018},
  organization={PMLR}
}
@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}
@article{anagnostopoulos2018scalable,
  title={Scalable aggregation predictive analytics},
  author={Anagnostopoulos, Christos and Savva, Fotis and Triantafillou, Peter},
  journal={Applied Intelligence},
  volume={48},
  number={9},
  pages={2546--2567},
  year={2018},
  publisher={Springer}
}
@article{armstrongmasking,
author = {Armstrong, Marc P. and Rushton, Gerard and Zimmerman, Dale L.},
title = {Geographically masking health data to preserve confidentiality},
journal = {Statistics in Medicine},
volume = {18},
number = {5},
pages = {497-525},
doi = {https://doi.org/10.1002/(SICI)1097-0258(19990315)18:5<497::AID-SIM45>3.0.CO;2-\#},
year = {1999}
}
@incollection{kocabacs2016medical,
  title={Medical data analytics in the cloud using homomorphic encryption},
  author={Kocaba{\c{s}}, {\"O}v{\"u}n{\c{c}} and Soyata, Tolga},
  booktitle={E-Health and Telemedicine: Concepts, Methodologies, Tools, and Applications},
  pages={751--768},
  year={2016},
  publisher={IGI Global}
}
@inproceedings{graepel2012ml,
  title={ML confidential: Machine learning on encrypted data},
  author={Graepel, Thore and Lauter, Kristin and Naehrig, Michael},
  booktitle={International Conference on Information Security and Cryptology},
  pages={1--21},
  year={2012},
  organization={Springer}
}
@phdthesis{asenjo2017data,
  title={Data masking, encryption, and their effect on classification performance: trade-offs between data security and utility},
  author={Asenjo, Juan C},
  year={2017},
  school={Nova Southeastern University}
}
@inproceedings{bunde2021ai,
  title={AI-Assisted and Explainable Hate Speech Detection for Social Media Moderators--A Design Science Approach},
  author={Bunde, Enrico},
  booktitle={Proceedings of the 54th Hawaii International Conference on System Sciences},
  pages={1264},
  year={2021}
}

@inproceedings{hew2021using,
  title={Using Chatbots in Flipped Learning Online Sessions: Perceived Usefulness and Ease of Use},
  author={Hew, Khe Foon and Huang, Weijiao and Du, Jiahui and Jia, Chengyuan},
  booktitle={International Conference on Blended Learning},
  pages={164--175},
  year={2021},
  organization={Springer}
}


@article{delibasic2013white,
  title={White-box decision tree algorithms: A pilot study on perceived usefulness, perceived ease of use, and perceived understanding},
  author={Delibasic, Boris and Vukicevic, Milan and Jovanovic, MILO},
  journal={International Journal of Engineering Education},
  volume={29},
  number={3},
  pages={674--687},
  year={2013}
}

@article{fink2021artificial,
  title={Artificial intelligence across company borders},
  author={Fink, Olga and Netland, Torbj{\o}rn and Feuerriegel, Stefan},
  journal={Communications of the ACM},
  year={2021}
}

@article{Abbasi2012,
abstract = {Financial fraud can have serious ramifications for the long-term sustainability of an organization, as well as adverse effects on its employees and investors, and on the economy as a whole. Several of the largest bankruptcies in U.S. history involved firms that engaged in major fraud. Accordingly, there has been considerable emphasis on the development of automated approaches for detecting financial fraud. However, most methods have yielded performance results that are less than ideal. In consequence, financial fraud detection continues as an important challenge for business intelligence technologies. In light of the need for more robust identification methods, we use a design science approach to develop MetaFraud, a novel meta-learning framework for enhanced financial fraud detection. To evaluate the proposed framework, a series of experiments are conducted on a test bed encompassing thousands of legitimate and fraudulent firms. The results reveal that each component of the framework significantly contributes to its overall effectiveness. Additional experiments demonstrate the effectiveness of the meta-learning framework over state-of-the-art financial fraud detection methods. Moreover, the MetaFraud framework generates confidence scores associated with each prediction that can facilitate unprecedented financial fraud detection performance and serve as a useful decision-making aid The results have important implications for several stakeholder groups, including compliance officers, investors, audit firms, and regulators.},
author = {Abbasi, Ahmed and Albrecht, Conan and Vance, Anthony and Hansen, James},
isbn = {0276-7783},
issn = {02767783},
journal = {MIS Quarterly},
keywords = {business,design science,feature construction,financial statement fraud,fraud detection,intelligence,meta-learning},
number = {4},
pages = {1293--1327},
pmid = {83465956},
title = {{Metafraud: A meta-learning framework for detecting financial fraud}},
volume = {36},
year = {2012}
}
@article{Adi2006,
abstract = {Complex event processing (CEP) is an emerging technology for extracting information from distributed message-based systems. CEP is software used to create and deploy applications that process large volumes of incoming messages or events, analyze those messages or events in various ways, and respond to conditions of interest in real-time. This technology allows users of a system to specify the information that is of interest to them. This paper presents an over-view of complex event processing applied to the domain of financial services demonstrated using a CEP product called AMiT (active middleware technology). Two case studies (in the banking and the insurance industries) are introduced in order to demonstrate the special use of the CEP concept in the financial services domain and the advantages both on built-time and run-time. The various usages of CEP for BAM, messaging, decision-making in financial services applications express the increasing role of CEP in intelligent event-driven financial services solutions},
author = {Adi, Asaf and Botzer, David and Nechushtai, Gil and Sharon, Guy},
isbn = {VO -},
journal = {Services Computing Workshops, 2006. SCW '06. IEEE},
keywords = {Automation,Condition monitoring,Databases,Engines,Event detection,Gas insulated transmission lines,Runtime environment,active middleware technology,bank data processing,banking,complex event processing,distributed message-based system,financial service,information extraction,insurance,insurance data processing,message passing,middleware},
title = {{Complex Event Processing for Financial Services}},
year = {2006}
}
@article{Agrawal2000,
abstract = {A fruitful direction for future data mining research will be the development of techniques that incorporate privacy concerns. Specifically, we address the following question. Since the primary task in data mining is the development of models about aggregated data, can we develop accurate models without access to precise information in individual data records? We consider the concrete case of building a decision-tree classifier from training data in which the values of individual records have been perturbed. The resulting data records look very different from the original records and the distribution of data values is also very different from the original distribution. While it is not possible to accurately estimate original values in individual data records, we propose a novel reconstruction procedure to accurately estimate the distribution of original data values. By using these reconstructed distributions, we are able to build classifiers whose accuracy is comparable to the accuracy of classifiers built with the original data.},
author = {Agrawal, Rakesh and Srikant, Ramakrishnan},
isbn = {1581132174},
issn = {0163-5808},
journal = {Proceedings of the 2000 ACM SIGMOD international conference on Management of data - SIGMOD '00},
number = {2},
pages = {439--450},
title = {{Privacy-preserving data mining}},
volume = {29},
year = {2000}
}
@article{Alsabawy2016,
author = {Alsabawy, Ahmed Younis and Cater-Steel, Aileen and Soar, Jeffrey},
journal = {Computers in Human Behavior},
pages = {843--858},
publisher = {Elsevier},
title = {{Determinants of perceived usefulness of e-learning systems}},
volume = {64},
year = {2016}
}
@article{Anderson1994,
abstract = {In business-to-business settings, dyadic relationships between firms are of paramount interest. Recent developments in business practice strongly suggest that to understand these business relationships, greater attention must be directed to the embedded context within which dyadic business relationships take place. Drawing on business network research and social exchanage theory, a fundamental conceptualization is provided to capture network properties and relatioship connectedness within dyadic business relationship models.},
author = {Anderson, James C and Hakansson, Hakan and Johanson, Jan},
isbn = {00222429},
issn = {00222429},
journal = {Journal of Marketing},
number = {4},
pages = {1},
pmid = {9410316029},
title = {{Dyadic Business Relationships within a Business Network Context}},
volume = {58},
year = {1994}
}
@article{Bach2020,
author = {Bach, Volker and Gao, Junbin and Chen, Xi},
journal = {Electronic Markets},
title = {{Special Issue: Machine Learning in Business Networks}},
year = {2020}
}
@book{bach2013business,
author = {Bach, Volker and Vogler, Petra and {\"{O}}sterle, Hubert},
publisher = {Springer-Verlag},
title = {{Business Knowledge Management: Praxiserfahrungen mit Intranetbasierten L{\"{o}}sungen}},
year = {2013}
}
@article{Bagozzi2007,
author = {Bagozzi, Richard P},
journal = {Journal of the association for information systems},
number = {4},
pages = {3},
title = {{The legacy of the technology acceptance model and a proposal for a paradigm shift.}},
volume = {8},
year = {2007}
}
@article{belanger2015role,
author = {Belanger, France and Xu, Heng},
journal = {Information Systems Journal},
number = {6},
pages = {573--578},
publisher = {Wiley Online Library},
title = {{The role of information systems research in shaping the future of information privacy}},
volume = {25},
year = {2015}
}
@article{Bengtsson2000,
abstract = {Existing theory and research on relationships among competitors focuses either on competitive or on cooperative relationships between them, and the one relationship is argued to harm or threaten the other. Little research has considered that two firms can be involved in and benefit from both cooperation and competition simultaneously, and hence that both types of relationships need to be emphasized at the same time. In this article, it is argued that the most complex, but also the most advantageous relationship between competitors, is "coopetition" where two competitors both compete and cooperate with each other. Complexity is due to the fundamentally different and contradictory logics of interaction that competition and cooperation are built on. It is of crucial importance to separate the two different parts of the relationship to manage the complexity and thereby make it possible to benefit from such a relationship. This article uses an explorative case study of two Swedish and one Finnish industries where coopetition is to be found, to develop propositions about how the competitive and cooperative part of the relationship can be divided and managed. It is shown that the two parts can be separated depending on the activities degree of proximity to the customer and on the competitors' access to specific resources. It is also shown that individuals within the firm only can act in accordance with one of the two logics of interaction at a time and hence that either the two parts have to be divided between individuals within the company, or that one part needs to be controlled and regulated by an intermediate actor such as a collective association. {\textcopyright}2000 Elsevier Science Inc. All rights reserved.},
author = {Bengtsson, Maria and Kock, S{\"{o}}ren},
isbn = {0019-8501},
issn = {00198501},
journal = {Industrial Marketing Management},
pmid = {12143263},
title = {{"Coopetition" in business networks - To cooperate and compete simultaneously}},
year = {2000}
}
@article{bhattacharya2015privacy,
author = {Bhattacharya, Prasanta and Phan, Tuan and Liu, Linlin},
title = {{Privacy-preserving Distributed Analytics: Addressing the Privacy-Utility Tradeoff Using Homomorphic Encryption for Peer-to-Peer Analytics}},
year = {2015}
}
@article{Bonomi2012,
abstract = {Fog Computing extends the Cloud Computing paradigm to the edge of the network, thus enabling a new breed of applications and services. Defining characteristics of the Fog are: a) Low latency and location awareness; b) Wide-spread geographical distribution},
author = {Bonomi, Flavio and Milito, Rodolfo and Zhu, Jiang and Addepalli, Sateesh},
isbn = {9781450315197},
issn = {978-1-4503-1519-7},
journal = {Proceedings of the first edition of the MCC workshop on Mobile cloud computing},
keywords = {analytics,cloud computing,defined networks,fog computing,iot,real time systems,software,wsan},
pages = {13--16},
title = {{Fog Computing and Its Role in the Internet of Things}},
year = {2012}
}
@article{Boughorbel2017,
abstract = {Data imbalance is frequently encountered in biomedical applications. Resampling techniques can be used in binary classification to tackle this issue. However such solutions are not desired when the number of samples in the small class is limited. Moreover the use of inadequate performance metrics, such as accuracy, lead to poor generalization results because the classifiers tend to predict the largest size class. One of the good approaches to deal with this issue is to optimize performance metrics that are designed to handle data imbalance. Matthews Correlation Coefficient (MCC) is widely used in Bioinformatics as a performance metric. We are interested in developing a new classifier based on the MCC metric to handle imbalanced data. We derive an optimal Bayes classifier for the MCC metric using an approach based on Frechet derivative. We show that the proposed algorithm has the nice theoretical property of consistency. Using simulated data, we verify the correctness of our optimality result by searching in the space of all possible binary classifiers. The proposed classifier is evaluated on 64 datasets from a wide range data imbalance. We compare both classification performance and CPU efficiency for three classifiers: 1) the proposed algorithm (MCC-classifier), the Bayes classifier with a default threshold (MCC-base) and imbalanced SVM (SVM-imba). The experimental evaluation shows that MCC-classifier has a close performance to SVM-imba while being simpler and more efficient.},
author = {Boughorbel, Sabri and Jarray, Fethi and El-Anbari, Mohammed},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
pmid = {28574989},
title = {{Optimal classifier for imbalanced data using Matthews Correlation Coefficient metric}},
year = {2017}
}
@book{Brazdil2008,
abstract = {The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.},
author = {Brazdil, Pavel and Carrier, Christophe Giraud and Soares, Carlos and Vilalta, Ricardo},
booktitle = {Metalearning: Applications to Data Mining},
isbn = {978-3-540-73262-4},
issn = {1611-2482},
keywords = {bct},
pages = {33--61},
title = {{Metalearning: Applications to Data Mining}},
year = {2008}
}
@article{Breiman1996,
abstract = {Bagging predictors is a method for generating multiple versions of a pre-dictor and using these to get an aggregated predictor. The aggregation av-erages over the versions when predicting a numerical outcome and does a plurality v ote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classiication and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability o f the prediction method. If perturbing the learning set can cause signiicant changes in the predictor constructed, then bagging can improve accuracy.},
author = {Breiman, Leo},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {aggregation,averaging,bootstrap,combining},
number = {421},
pages = {123--140},
pmid = {17634459},
title = {{Bagging Predictors}},
volume = {24},
year = {1996}
}
@article{Cawley2010a,
author = {Cawley, Gavin C and Talbot, Nicola L C},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {bias-variance trade-off,model selection,over-,performance evaluation,selection bias},
title = {{On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation}},
year = {2010}
}
@article{Chen2009,
abstract = {Privacy is an important issue when one wants to make use of data that involves individuals' sensitive information. Research on protecting the privacy of individuals and the confidentiality of data has received contributions from many fields, including computer science, statistics, economics, and social science. In this paper, we survey research work in privacy-preserving data publishing. This is an area that attempts to answer the problem of how an organization, such as a hospital, government agency, or insurance company, can release data to the public without violating the confidentiality of personal information. We focus on privacy criteria that provide formal safety guarantees, present algorithms that sanitize data to make it safe for release while preserving useful information, and discuss ways of analyzing the sanitized data. Many challenges still remain. This survey provides a summary of the current state-of-the-art, based on which we expect to see advances in years to come. � 2009 B.-C. Chen, D. Kifer, K. LeFevre and A. Machanavajjhala.},
author = {Chen, Bee-Chung and Kifer, Daniel and LeFevre, Kristen and Machanavajjhala, Ashwin},
isbn = {ISSN{\~{}}{\~{}}26176633},
issn = {03600300},
journal = {Foundations and Trends in Databases},
pages = {1--167},
pmid = {16914745},
title = {{Privacy-preserving data publishing}},
volume = {2},
year = {2009}
}
@article{Cherdantseva2013,
abstract = {Information Assurance {\&} Security (IAS) is a dynamic domain which changes continuously in response to the evolution of society, business needs and technology. This paper proposes a Reference Model of Information Assurance {\&} Security (RMIAS), which endeavours to address the recent trends in the IAS evolution, namely diversification and deperimetrisation. The model incorporates four dimensions: Information System Security Life Cycle, Information Taxonomy, Security Goals and Security Countermeasures. In addition to the descriptive knowledge, the RMIAS embeds the methodological knowledge. A case study demonstrates how the RMIAS assists with the development and revision of an Information Security Policy Document.},
author = {Cherdantseva, Yulia and Hilton, Jeremy},
isbn = {978-0-7695-5008-4},
journal = {International Conference on Availability, Reliability and Security},
keywords = {conceptual model,information assurance,information security,reference model},
pages = {546--555},
title = {{A Reference Model of Information Assurance {\&} Security}},
year = {2013}
}
@article{davenport2006competing,
author = {Davenport, Thomas H},
journal = {harvard business review},
number = {1},
pages = {98},
title = {{Competing on analytics}},
volume = {84},
year = {2006}
}
@article{Davis1989,
author = {Davis, Fred D},
journal = {MIS quarterly},
pages = {319--340},
publisher = {JSTOR},
title = {{Perceived usefulness, perceived ease of use, and user acceptance of information technology}},
year = {1989}
}
@article{Demirkan2013,
abstract = {Using service-oriented decision support systems (DSS in cloud) is one of the major trends for many organizations in hopes of becoming more agile. In this paper, after defining a list of requirements for service-oriented DSS, we propose a conceptual framework for DSS in cloud, and discus about research directions. A unique contribution of this paper is its perspective on how to servitize the product oriented DSS environment, and demonstrate the opportunities and challenges of engineering service oriented DSS in cloud. When we define data, information and analytics as services, we see that traditional measurement mechanisms, which are mainly time and cost driven, do not work well. Organizations need to consider value of service level and quality in addition to the cost and duration of delivered services. DSS in CLOUD enables scale, scope and speed economies. This article contributes new knowledge in service science by tying the information technology strategy perspectives to the database and design science perspectives for a broader audience.},
author = {Demirkan, Haluk and Delen, Dursun},
isbn = {0167-9236},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {Analytics-as-a-service,Big data,Cloud computing,Data-as-a-service,Information-as-a-service,Service orientation,Service science},
number = {1},
pages = {412--421},
pmid = {1351355586},
publisher = {Elsevier B.V.},
title = {{Leveraging the capabilities of service-oriented decision support systems: Putting analytics and big data in cloud}},
volume = {55},
year = {2013}
}
@article{Duncan2011,
abstract = {The article reviews the book "Statistical Confidentiality: Principles and Practice," by George T. Duncan, Mark Elliot and Juan-Jos{\'{e}} Salazar-Gonz{\'{a}}lez.},
author = {Duncan, George T and Elliot, Mark and Salazar-Gonz{\'{a}}lez, Juan-Jos{\'{e}}},
issn = {03067734},
journal = {International Statistical Review},
number = {3},
pages = {479--480},
title = {{Statistical Confidentiality: Principles and Practice}},
volume = {80},
year = {2011}
}
@misc{Duncan2009,
abstract = {Protecting confidentiality is essential to the functioning of systems for collecting and disseminating data on individuals and enterprises that are necessary for evidence‐based public policy formulation. Deidentification of records, defined as removing obvious identifiers such as name and address, is not sufficient to protect confidentiality. Microdata have characteristics that lead to increased disclosure risk, such as existence of identification files, geographical detail, outliers, many/detailed attribute variables, or longitudinal or panel structure in the data. Data stewardship organizations can lower disclosure risk through disclosure limitation methods and through the construction of synthetic data. Both record and attribute suppression can be represented by matrix masks, as can perturbation through noise addition, and data swapping. Also sampling and aggregation have matrix mask representations. Distinct from masking methods, synthetic data construction considers the microdata to be a realization of some statistical model. It then replaces the true microdata with samples generated according to the model. The released data consist of records of individual synthetic units rather than records for the actual units. The organization must recognize uncertainty in both model form and values of model parameters. This argues for the relevance of hierarchical and mixture models to generate the synthetic data. Synthetic data has an advantage as a disclosure limitation method over masked data because it is easier for the user to analyze. Copyright {\textcopyright}2009 Wiley Periodicals, Inc., A Wiley Company},
author = {Duncan, George and Stokes, Lynne},
booktitle = {Wiley Interdisciplinary Reviews: Computational Statistics},
doi = {10.1002/wics.3},
issn = {19395108},
number = {1},
pages = {83--92},
title = {{Data masking for disclosure limitation}},
volume = {1},
year = {2009}
}
@article{Dunkel2009,
abstract = {Sensor networks have to cope with a high volume of events continuously. Conventional software architectures do not explicitly target the efficient processing of continuous event streams. Recently, event-driven architectures (EDA) have been proposed as a new paradigm for event-based applications. In this paper we propose a reference architecture for sensor-based networks, which enables the analysis and processing of complex event streams in real-time. Our approach is based on semantically rich event models using ontologies that allow representation of structural properties of event types and constraints between them. Then, we argue in favor of a declarative approach of complex event processing that draws upon rule languages such as Esper and JESS. Also, we illustrate our approach in the domain of road traffic management for the high-capacity road network in Bilbao, Spain.},
author = {Dunkel, Jurgen},
journal = {Proceedings - 2009 International Symposium on Autonomous Decentralized Systems, ISADS 2009},
keywords = {Highway traffic control;Motor transportation;Netwo},
pages = {249--254},
title = {{On complex event processing for sensor networks}},
year = {2009}
}
@article{Dzeroski2004,
abstract = {This paper presents a subgroup discovery algorithm APRIORI-SD, developed by adapting association rule learning to subgroup discovery. The paper contributes to subgroup discovery, to a better understanding of the weighted covering algorithm, and the properties of the weighted relative accuracy heuristic by analyzing their performance in the ROC space. An experimental comparison with rule learners CN2, RIPPER, and APRIORI-C on UCI data sets demonstrates that APRIORI-SD produces substantially smaller rulesets, where individual rules have higher coverage and significance. APRIORI-SD is also compared to subgroup discovery algorithms CN2-SD and SubgroupMiner. The comparisons performed on U.K. traffic accident data show that APRIORI-SD is a competitive subgroup discovery algorithm.},
author = {D{\v{z}}eroski, Saso and {\v{Z}}enko, Bernard},
isbn = {1-55860-873-7},
issn = {08856125},
journal = {Machine Learning},
keywords = {Combining classifiers,Ensembles of classifiers,Meta-learning,Multi-response model trees,Stacking},
number = {3},
pages = {255--273},
title = {{Is combining classifiers with stacking better than selecting the best one?}},
volume = {54},
year = {2004}
}
@article{Emerson1976,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Emerson, Richard M},
eprint = {arXiv:1011.1669v3},
isbn = {9781931695923},
issn = {03600572},
journal = {Annual Review of Sociology},
pmid = {10456520},
title = {{Social Exchange Theory}},
year = {1976}
}
@article{Freund1996a,
abstract = {In an earlier paper [9], we introduced a new "boosting" algorithm called {\{}AdaBoost{\}} which, theoretically, can be used to significantly reduce the error of any learning algorithm that consistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a "pseudo-loss" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe...},
archivePrefix = {arXiv},
arxivId = {10.1007/978-0-387-09823-4{\_}45},
author = {Freund, Yoav and Schapire, Robert E},
eprint = {978-0-387-09823-4{\_}45},
isbn = {1558604197},
issn = {0706-652X, 1205-7533},
journal = {International Conference on Machine Learning},
keywords = {boosting},
pages = {148--156},
pmid = {15003161},
primaryClass = {10.1007},
title = {{Experiments with a New Boosting Algorithm}},
year = {1996}
}
@article{gao2018security,
author = {Gao, Junbin},
journal = {Future Network Systems and Security},
pages = {197},
publisher = {Springer},
title = {{Security and Privacy Protection for eHealth Data}},
year = {2018}
}
@article{Goldfarb2011,
abstract = {Advertisers use online customer data to target their marketing appeals. This has heightened consumers' privacy concerns, leading governments to pass laws designed to protect consumer privacy by restricting the use of data and by restricting online tracking techniques used by websites. We use the responses of 3.3 million survey takers who had been randomly exposed to 9,596 online display (banner) advertising campaigns to explore how privacy regulation in the European Union (EU) has influenced advertising effectiveness. This privacy regulation restricted advertisers' ability to collect data on Web users in order to target ad campaigns. We find that, on average, display advertising became far less effective at changing stated purchase intent after the EU laws were enacted, relative to display advertising in other countries. The loss in effectiveness was more pronounced for websites that had general content (such as news sites), where non-data-driven targeting is particularly hard to do. The loss of effectiveness was also more pronounced for ads with a smaller presence on the webpage and for ads that did not have additional interactive, video, or audio features.},
author = {Goldfarb, Avi and Tucker, Catherine E},
isbn = {0025-1909},
issn = {0025-1909},
journal = {Management Science},
pmid = {57434320},
title = {{Privacy Regulation and Online Advertising}},
year = {2011}
}
@article{Gregor2006,
abstract = {The aim of this research essay is to examine the structural nature of theory in Information Systems. Despite the importance of theory, questions relating to its form and structure are neglected in comparison with questions relating to epistemology. The essay addresses issues of causality, explanation, prediction, and generalization that underlie an understanding of theory. A taxonomy is proposed that classifies information systems theories with respect to the manner in which four central goals are addressed: analysis, explanation, prediction, and prescription. Five interrelated types of theory are distinguished: (1) theory for analyzing, (2) theory for explaining, (3) theory for predicting, (4) theory for explaining and predicting, and (5) theory for design and action. Examples illustrate the nature of each theory type. The applicability of the taxonomy is demonstrated by classifying a sample of journal articles. The paper contributes by showing that multiple views of theory exist and by exposing the assumptions underlying different viewpoints. In addition, it is suggested that the type of theory under development can influence the choice of an epistemological approach. Support is given for the legitimacy and value of each theory type. The building of integrated bodies of theory that encompass all theory types is advocated.},
author = {Gregor, Shirley},
journal = {MIS Quarterly},
keywords = {Theory,causality,design,design theory,explanation,generalization,information,interpretivist theory,of social sciences,philosophy,philosophy of science,prediction,science,systems discipline,theory structure,theory taxonomy},
number = {3},
pages = {611--642},
title = {{Research Essay: The Nature of Theory in Information Systems}},
volume = {30},
year = {2006}
}
@article{Gregor2007,
abstract = {Design work and design knowledge in Information Systems (IS) is important for both research and practice. Yet there has been comparatively little critical attention paid to the problem of specifying design theory so that it can be communicated, justified, and developed cumulatively. In this essay we focus on the structural components or anatomy of design theories in IS as a special class of theory. In doing so, we aim to extend the work of Walls, Widemeyer and El Sawy (1992) on the specification of information systems design theories (ISDT), drawing on other streams of thought on design research and theory to provide a basis for a more systematic and useable formulation of these theories. We identify eight separate components of design theories: (1) purpose and scope, (2) constructs, (3) principles of form and function, (4) artifact mutability, (5) testable propositions, (6) justificatory knowledge (kernel theories), (7) principles of implementation, and (8) an expository instantiation. This specification includes components missing in the Walls et al. adaptation of Dubin (1978) and Simon (1969) and also addresses explicitly problems associated with the role of instantiations and the specification of design theories for methodologies and interventions as well as for products and applications. The essay is significant as the unambiguous establishment of design knowledge as theory gives a sounder base for arguments for the rigor and legitimacy of IS as an applied discipline and for its continuing progress. A craft can proceed with the copying of one example of a design artifact by one artisan after another. A discipline cannot. [ABSTRACT FROM AUTHOR] Copyright of Journal of the Association for Information Systems is the property of Association for Information Systems and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gregor, Shirley and Jones, David},
eprint = {arXiv:1011.1669v3},
isbn = {15369323},
issn = {15369323},
journal = {Journal of the Association for Information Systems},
pmid = {25513243},
title = {{The Anatomy of a Design Theory}},
year = {2007}
}
@article{Hakanen2012,
abstract = {Purpose – Increased competition and more extensive customer needs have motivated companies to develop integrated solutions. In practice, companies struggle to co-create effective solutions that meet customer needs. The purpose of this paper is to identify critical factors affecting the effective co-creation of customer-focused solutions within business networks. Design/methodology/approach – The study investigates the co-creation of two different types of solution. Data were collected from two business networks comprising 13 companies, including suppliers and their customers. The empirical data comprise 51 interviews and observations made at 21 company workshops. Findings – Effective co-creation of solutions requires a fit between the perceptions of multiple suppliers and their customers with regard to core content, operations and processes, customer experience and value of the solution. Co-creation is affected by, e.g. customer's preferences for participation and value, and the degree of competition, clarity of role division and rapport among the suppliers. Research limitations/implications – Further empirical research is needed to examine how companies could overcome the problems identified, and reap the opportunities arising from the factors affecting the co-creation of solutions. Practical implications – The paper presents a framework that outlines practical activities that help firms to reconcile the perspectives of different actors, and to facilitate the integration of resources when co-creating solutions within business networks. Originality/value – The paper contributes to the solutions literature by studying solutions as a network-level process of resource integration between multiple suppliers and their mutual customers, and by applying a service concept framework to the study of integrated solutions. Introduction In many industries, increased competition, declining margins and more extensive customer needs have made it essential for companies to seek differentiation and customer loyalty by offering integrated solutions rather than individual products or},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1108/BIJ-10-2012-0068},
author = {Hakanen, Taru and Jaakkola, Elina},
eprint = {/dx.doi.org/10.1108/BIJ-10-2012-0068},
isbn = {10.1108/09564231211260431},
issn = {17575818},
journal = {Journal of Service Management},
keywords = {Business development,Business network,Co-creation,Customers,Integrated solution,Service concept},
pmid = {42012058},
primaryClass = {http:},
title = {{Co-creating customer-focused solutions within business networks: A service perspective}},
year = {2012}
}
@article{Hann2007,
abstract = {The advent of the Internet has made the transmission of personally identifiable information more common and often unintended by the user. As personal information becomes more accessible, individuals worry that businesses misuse the information that is collected while they are online. Organizations have tried to mitigate this concern in two ways: (1) by offering privacy policies regarding the handling and use of personal information and (2) by offering benefits such as financial gains or convenience. In this paper, we interpret these actions in the context of the information-processing theory of motivation. Information-processing theories, also known as expectancy theories in the context of motivated behavior, are built on the premise that people process information about behavior-outcome relationships. By doing so, they are forming expectations and making decisions about what behavior to choose. Using an experimental setting, we empirically validate predictions that the means to mitigate privacy concerns are associated with positive valences resulting in an increase in motivational score. In a conjoint analysis exercise, 268 participants from the United States and Singapore face trade-off situations, where an organization may only offer incomplete privacy protection or some benefits. While privacy protections (against secondary use, improper access, and error) are associated with positive valences, we also find that financial gains and convenience can significantly increase individuals' motivational score of registering with a Web site. We find that benefits-monetary reward and future convenience-significantly affect individuals' preferences over Web sites with differing privacy policies. We also quantify the value of Web site privacy protection. Among U.S. subjects, protection against errors, improper access, and secondary use of personal information is worth {\$}30.49-{\$}44.62. Finally, our approach also allows us to identify three distinct segments of Internet users-privacy guardians, information sellers, and convenience seekers.},
author = {Hann, Il-Horn and Hui, Kai-Lung and Lee, Sang-Yong T and Png, Ivan P L},
isbn = {0742-1222},
issn = {0742-1222},
journal = {Journal of Management Information Systems},
keywords = {attitudes,computers,conjoint analysis,conjoint-measurement,decision,employee perceptions,exchange,expectancy theory,expectancy-theory,financial reward,information privacy,management,model,online privacy,organizations,segmentation},
pmid = {27328839},
title = {{Overcoming Online information privacy concerns: An information-processing theory approach}},
year = {2007}
}
@incollection{Hevner2010,
author = {Hevner, Alan and Chatterjee, Samir},
booktitle = {Design research in information systems},
pages = {9--22},
publisher = {Springer},
title = {{Design science research in information systems}},
year = {2010}
}
@inproceedings{hirt2018service,
author = {Hirt, Robin and K{\"{u}}hl, Niklas and Schmitz, Bj{\"{o}}rn and Satzger, Gerhard},
booktitle = {Proceedings of the 51st Hawaii International Conference on System Sciences},
title = {{Towards Service-Oriented Cognitive Analytics for Smart Service Systems}},
year = {2018}
}
@article{Huhns2005,
abstract = {Although traditional approaches--the ones embodied in CASE tools and modeling frameworks--are appropriate for building individual software components, but they aren't designed to face the challenges of open environments. Service-oriented computing provides a way to create a new architecture that reflects components' tendencies toward autonomy and heterogeneity.},
author = {Huhns, M N and Singh, M P},
isbn = {1089-7801 VO - 9},
issn = {1089-7801},
journal = {IEEE Internet Computing},
number = {1},
pages = {75--81},
title = {{Service-oriented computing: key concepts and principles}},
volume = {9},
year = {2005}
}
@article{Kambil1994,
abstract = {Describes how organizations can and have employed information technology (It) variables to create four prototypical forms. Virtual organization: Negotiated organizations; Traditional organizations with IT-enabled components; Vertically integrated conglomerates},
author = {Kambil, Ajit and Short, James E},
issn = {0742-1222},
journal = {Journal of Management Information Systems},
keywords = {Business network redesign,Business reengineering,Electronic integration,Information Technology,Interorganizational systems},
number = {March 1993},
pages = {15},
title = {{Electronic Integration and Business Network Redesign-A Roles-Linkage Perspective}},
volume = {10},
year = {1994}
}
@article{Kieseberg2014,
abstract = {The collection, processing, and selling of personal data is an integral part of today's electronic markets, either as means for operating business, or as an asset itself. However, the exchange of sensitive information between companies is limited by two major issues: Firstly, regulatory compliance with laws such as SOX requires anonymization of personal data prior to transmission to other parties. Secondly, transmission always implicates some loss of control over the data since further dissemination is possible without knowledge of the owner. In this paper, we extend an approach based on the utilization of k-anonymity that aims at solving both concerns in one single step - anonymization and fingerprinting of microdata such as database records. Furthermore, we develop criteria to achieve detectability of colluding attackers, as well as an anonymization strategy that resists combined efforts of colluding attackers on reducing the anonymization-level. Based on these results we propose an algorithm for the generation of collusion-resistant fingerprints for microdata.},
author = {Kieseberg, Peter and Schrittwieser, Sebastian and Mulazzani, Martin and Echizen, Isao and Weippl, Edgar},
issn = {14228890},
journal = {Electronic Markets},
number = {2},
pages = {113--124},
title = {{An algorithm for collusion-resistant anonymization and fingerprinting of sensitive microdata}},
volume = {24},
year = {2014}
}
@book{kitchin2014data,
author = {Kitchin, Rob},
publisher = {Sage},
title = {{The data revolution: Big data, open data, data infrastructures and their consequences}},
year = {2014}
}
@article{Lemke2015,
abstract = {Metalearning attracted considerable interest in the machine learning community in the last years. Yet, some disagreement remains on what does or what does not constitute a metalearning problem and in which contexts the term is used in. This survey aims at giving an all-encompassing overview of the research directions pursued under the umbrella of metalearning, reconciling different definitions given in scientific literature, listing the choices involved when designing a metalearning system and identifying some of the future research challenges in this domain.},
author = {Lemke, Christiane and Budka, Marcin and Gabrys, Bogdan},
isbn = {9783540732631},
issn = {15737462},
journal = {Artificial Intelligence Review},
keywords = {Life-long learning,Metaknowledge extraction,Metalearning},
number = {1},
pages = {117--130},
pmid = {26069389},
publisher = {Springer Netherlands},
title = {{Metalearning: a survey of trends and technologies}},
volume = {44},
year = {2015}
}
@article{Li2013b,
abstract = {In contrast to popular literature on technology acceptance, this research-in-progress paper does not intend to build an explanatory model of technology acceptance but a predictive model so as to predict whether a specific person is likely to accept some technology. We show that the constructs that were identified in the classic UTAUT (such as performance expectancy, effort expectancy and social influence) can be used in a predictive model but that better predictions of system use can be made using knowledge about social networks that exist between people. Both social influence and social selection data are valuable to make predictions. Our approach is tested in the context of a video system which is part of an online learning platform, using a sample of 133 interconnected students.},
author = {Li, Libo and Goethals, Frank and Giangreco, Antonio and Baesens, Bart},
isbn = {9781629934266},
journal = {Icis-Rp},
keywords = {adoption,predictive modeling,utaut},
pages = {1--10},
title = {{Using Social Network Data To Predict Technology Acceptance}},
year = {2013}
}
@inproceedings{Liu2015,
abstract = {Electric power networks are among the world's most complex human-made systems. The developing smart grid is an inherently complex system which is rapidly evolving in both definition and implementation. Deployment of advanced technologies within the electric utility sector and usage of state-of-the-art computing systems provides companies with innovative capabilities to forecast electricity demand, influence customer usage patterns, create demand response program, optimize unit commitment, and prevent power outages. At the same time, these advances also lead to the generation of unprecedented data volumes, high data communications requirements, and increased system complexity. Utility companies must be capable of high-volume, high-speed data management and advanced analytics which are designed to transform data into actionable insights, if they strive to successfully implement a modern smart grid. As smart grid operations will leverage Advanced Metering Infrastructure (AMI) to drive more real time decision making and operational activities, complex event processing and stream computing are needed for the modern smart grid. This paper explores the challenges and benefits of transitioning to a smart grid, and explores new architectural approaches built on Lambda Architecture and other emerging software standards which may more effectively leverage established forms of complex event processing.},
author = {Liu, Guangyi and Zhu, Wendong and Saunders, Chris and Gao, Feng and Yu, Yang},
booktitle = {Procedia Computer Science},
isbn = {1877-0509},
issn = {18770509},
keywords = {Big Data,Complex Event Processing,Lambda Architecture,Real-Time Analytics,Smart Grid},
title = {{Real-time Complex Event Processing and Analytics for Smart Grid}},
year = {2015}
}
@inproceedings{Luckham2008,
abstract = {Complex event processing (CEP) is a set of techniques and tools to help us understand and control event-driven information systems. And today, any kind of information system, from the Internet to a cell phone, is driven by events. What is a complex event? It is an event that could only happen if lots of other events happened. For example, suppose you see a car you like at your favorite car dealership. That car is on the showroom floor only because a number of other events took place—events in the inventory control systems of the dealership and the manufacturer, shipping events, customs events at the port of entry, and so on. Of course, when you see exactly what you want in the showroom, you don't ask how or why. But if you don't see the model, make, or color you want and ask why not, you'll get an explanation about allocation quotas, backlogs at the factory, or some other factors that affect events in the causal history leading up to the event you wanted. This illustrates one of the ideas behind CEP. Events are related in various ways, by cause, by timing, and by membership. CEP applies to electronic information systems. It makes use of relationships between events to answer questions like, "Is our system providing the correct level of service to our customers," "Will our shipment arrive on time," and "Is someone trying to steal our information?" CEP adds a new dimension of event processing to what our event-driven information systems already do. Why is there a need for CEP? Let's look at the situation briefly. Today's information society is founded upon gathering and sharing information. All our organizations—commercial, government,and military—are dependent upon electronic information processing. Their foundational backbone is the kind of distributed computing system based on computer networks that is nowadays called the "information technology layer" (or IT layer) of the organization. The use of these systems has expanded rapidly over the past ten years to meet the increasing demands of automation, electronic commerce, and the Internet explosion. In vestment in technology has focused on making IT systems faster, capable of handling larger and larger amounts of information, and able to collaborate with one another. We now live in the world of the open enterprise, where commerce and information move across the boundaries of organizations and nations. Our society has become dependent upon IT systems. Less investment has been devoted to develop technology to solve the increasing problem of understanding what is happening in our IT systems. Whenever there is a crisis—a denial-of-service attack or a system failure—at first we don't understand what is going on or how to fix it, and then in the aftermath, we scramble for weeks to find out what caused it. We need to understand and control our critical information infrastructures better than that! A lot of the information in IT systems is never recognized. Messages—or events—pass silently back and forth across our information systems as unrelated pieces of communication. They are a source of great power, for when they are aggregated together, and correlated, and their relationships understood, they yield a wealth of information. A new technology is needed to harness the power of events in global information systems. This book is about such a technology.},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Luckham, David},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
eprint = {9780201398298},
isbn = {3540888071},
issn = {03029743},
pmid = {1107026},
title = {{The power of events: An introduction to complex event processing in distributed enterprise systems}},
year = {2008}
}
@article{Markus2000,
abstract = {Implementing ERP systems can be quite straightforward when organizations are simply structured and operate in one or a few locations. But when organizations are structurally complex and geographically dispersed, implementing ERP systems involves difficult, possibly unique, technical and managerial choices and challenges. There are at least five different ways in which organizations can arrange the relationships among business units: 1. total local autonomy, 2. headquarters control only at the financial level, 3 headquarters coordination of operations, 4. network coordination of operations, and 5. total centralization. ERP vendors have designed their packages to support a variety of logical organizational structures.},
archivePrefix = {arXiv},
arxivId = {0002-0782/00/0400},
author = {Markus, M Lynne and Tanis, Cornelis and van Fenema, Paul C},
eprint = {00/0400},
isbn = {00010782},
issn = {00010782},
journal = {Communications of the ACM},
pmid = {3275},
primaryClass = {0002-0782},
title = {{Enterprise resource planning: multisite ERP implementations}},
year = {2000}
}
@inproceedings{Martin2019,
author = {Martin, Dominik and K{\"{u}}hl, Niklas},
booktitle = {Proceedings of the 52nd Hawaii International Conference on System Sciences},
title = {{Holistic System-Analytics as an Alternative to Isolated Sensor Technology: A Condition Monitoring Use Case}},
year = {2019}
}
@book{mccormack2016supply,
author = {McCormack, Kevin P and Johnson, William C},
publisher = {CRC Press},
title = {{Supply chain networks and business process orientation: advanced strategies and best practices}},
year = {2016}
}
@article{mitchell1997,
author = {Mitchell, Tom M},
journal = {AI magazine},
number = {3},
pages = {11},
title = {{Does machine learning really work?}},
volume = {18},
year = {1997}
}
@inproceedings{Narayanan2008,
abstract = {We present a new class of statistical de- anonymization attacks against high-dimensional micro-data, such as individual preferences, recommen- dations, transaction records and so on. Our techniques are robust to perturbation in the data and tolerate some mistakes in the adversary's background knowledge. We apply our de-anonymization methodology to the Netflix Prize dataset, which contains anonymous movie ratings of 500,000 subscribers of Netflix, the world's largest online movie rental service. We demonstrate that an adversary who knows only a little bit about an individual subscriber can easily identify this sub- scriber's record in the dataset. Using the Internet Movie Database as the source of background knowl- edge, we successfully identified the Netflix records of known users, uncovering their apparent political pref- erences and other potentially sensitive information. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:cs/0610105v2},
author = {Narayanan, Arvind and Shmatikov, Vitaly},
booktitle = {Proceedings - IEEE Symposium on Security and Privacy},
eprint = {0610105v2},
isbn = {9780769531687},
issn = {10816011},
pmid = {20113465},
primaryClass = {arXiv:cs},
title = {{Robust de-anonymization of large sparse datasets}},
year = {2008}
}
@inproceedings{Pavlyshenko2016,
author = {Pavlyshenko, Bohdan},
booktitle = {2016 IEEE International Conference on Big Data (Big Data)},
pages = {2046--2050},
title = {{Machine learning, linear and Bayesian models for logistic regression in failure detection problems}},
year = {2016}
}
@inproceedings{Phang2005,
author = {Phang, Chee Wei and Li, Yan and Sutanto, Juliana and Kankanhalli, Atreyi},
booktitle = {Proceedings of the 38th annual Hawaii international conference on system sciences},
pages = {130a----130a},
title = {{Senior citizens' adoption of E-government: In quest of the antecedents of perceived usefulness}},
year = {2005}
}
@article{Pournaras2017,
abstract = {—The Internet of Things empowers citizens to inter-connect their devices, such as smart phones, into large-scale participatory decentralized networks, which they can use to make real-time collective measurements as public good, for instance, crowd-sourcing the monitoring of traffic in a city. This approach is an alternative to big data analytics systems that are often ex-pensive to access, privacy-intrusive and allow discriminatory and profiling actions over citizens' data. On the contrary, large-scale decentralized networks are complex to manage and collective measurements, i.e. computations of aggregation functions, need to encounter several dynamics such as continuously changing input data streams and highly varying temporal demand for access to the collective measurements. This paper proposes a highly reactive self-adaptation model to tackle the challenge of dynamic computational demand in large-scale decentralized in-network aggregation. The self-adaptation process makes nodes self-aware about other nodes that join and leave the network and therefore it makes them capable of self-orchestrating the communication to improve accuracy and minimize communication cost. The model is simple, yet agile. This is shown when applied in DIAS, the Dynamic Intelligent Aggregation Service without introducing architectural changes. Evaluation using data from a real-world smart grid pilot project as well as extreme demand profiles that scale up and down the demand 50{\%} on average confirm the cost-effectiveness of in-network aggregation empowered by self-adaptation. The findings are confirmed both in simulation and a large-scale live deployment in a cluster infrastructure with 3000 independent Java virtual machines each running a DIAS node. Overall, the results encourage new promising pathways towards the broader adoption of self-adaptive participatory data analytics in large-scale decentralized networks.},
author = {Pournaras, Evangelos and Nikolic, Jovan},
isbn = {9781538614655},
journal = {2017 IEEE 16th International Symposium on Network Computing and Applications, NCA 2017},
keywords = {accuracy,agent,aggregation,data analytics,decentralized network,gossip communication,participation,self-adaptation},
pages = {1--10},
title = {{On-demand self-adaptive data analytics in large-scale decentralized networks}},
volume = {2017-Janua},
year = {2017}
}
@article{Riquelme2014,
abstract = {This article analyzes the relationships among online trust and two of its most important antecedents, namely privacy and security, and explains how consumers' characteristics (gender, age, education and extraversion), moderate the influence of both privacy and security in online trust. This study expands previous literature by identifying the conditions under which perceived privacy and security are likely to have the greatest positive effects on consumer trust in the online retailer. Based on data from 398 online consumers, the results revealed that the influence of both privacy and security on online trust was stronger for male, younger, more educated, and less extraverted consumers. Implications for theory and management are discussed.},
author = {Riquelme, Isabel P and Rom{\'{a}}n, Sergio},
isbn = {1019-6781},
issn = {14228890},
journal = {Electronic Markets},
number = {2},
pages = {135--149},
title = {{Is the influence of privacy and security on online trust the same for all type of consumers}},
volume = {24},
year = {2014}
}
@article{Robins2010,
abstract = {This paper examines the foundations and state of the research in Complex Event Processing (CEP), a layer built on top of Event Driven Architecture (EDA).},
author = {Robins, David B},
isbn = {978-1-4244-6388-6},
issn = {1611-2776},
journal = {2010 Second International Workshop on Education Technology and Computer Science},
keywords = {cep,complex event,eda,event driven architecture,events,pattern matching,processing},
pages = {10},
title = {{Complex Event Processing}},
year = {2010}
}
@article{Roman2018,
abstract = {For various reasons, the cloud computing paradigm is unable to meet certain requirements (e.g. low latency and jitter, context awareness, mobility support) that are crucial for several applications (e.g. vehicular networks, augmented reality). To fulfill these requirements, various paradigms, such as fog computing, mobile edge computing, and mobile cloud computing, have emerged in recent years. While these edge paradigms share several features, most of the existing research is compartmentalized; no synergies have been explored. This is especially true in the field of security, where most analyses focus only on one edge paradigm, while ignoring the others. The main goal of this study is to holistically analyze the security threats, challenges, and mechanisms inherent in all edge paradigms, while highlighting potential synergies and venues of collaboration. In our results, we will show that all edge paradigms should consider the advances in other paradigms.},
archivePrefix = {arXiv},
arxivId = {1602.00484},
author = {Roman, Rodrigo and Lopez, Javier and Mambo, Masahiro},
eprint = {1602.00484},
isbn = {9781450335249},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Cloud computing,Fog computing,Mobile cloud computing,Mobile edge computing,Privacy,Security},
pmid = {18255791},
title = {{Mobile edge computing, Fog et al.: A survey and analysis of security threats and challenges}},
year = {2018}
}
@article{Sadeghi2015,
abstract = {The future Internet of Things as an intelligent collaboration of minia- turized sensors poses new challenges to security and end-user privacy. The ITU has identified that the protection of data and privacy of users is one of the key chal- lenges in the Internet of Things [Int05]: lack of confidence about privacy will result in decreased adoption among users and therefore is one of the driving factors in the success of the Internet of Things. This paper gives an overview, categorization, and analysis of security and privacy challenges in the Internet of Things.},
author = {Sadeghi, Ahmad-Reza and Wachsmann, Christian and Waidner, Michael},
isbn = {9781450335201},
issn = {18632122},
journal = {Proceedings of the 52nd Annual Design Automation Conference on - DAC '15},
keywords = {future internet,global sensor networks,privacy,security},
pages = {1--6},
title = {{Security and privacy challenges in industrial internet of things}},
volume = {17},
year = {2015}
}
@article{Saeed2008,
author = {Saeed, Khawaja A and Abdinnour-Helm, Sue},
journal = {Information {\&} Management},
number = {6},
pages = {376--386},
publisher = {Elsevier},
title = {{Examining the effects of information system characteristics and perceived usefulness on post adoption usage of information systems}},
volume = {45},
year = {2008}
}
@article{Sarlis2015,
abstract = {The ever-increasing Internet traffic poses challenges to network operators and administrators that have to analyze large network datasets in a timely manner to make decisions regarding network routing, dimensioning, accountability and security. Network datasets collected at large networks such as Internet Service Providers (ISPs) or Internet Exchange Points (IXPs) can be in the order of Terabytes per hour. Unfortunately, most of the current network analysis approaches are ad-hoc and centralized, and thus not scalable. In this paper, we present Datix, a fully decentralized, open-source analytics system for network traffic data that relies on smart partitioning storage schemes to support fast join algorithms and efficient execution of filtering queries. We outline the architecture and design of Datix and we present the evaluation of Datix using real traces from an operational IXP. Datix is a system that deals with an important problem in the intersection of data management and network monitoring while utilizing state-of-the-art distributed processing engines. In brief, Datix manages to efficiently answer queries within minutes compared to more than 24 hours processing when executing existing Python based code in single node setups. Datix also achieves nearly 70{\%} speedup compared to baseline query implementations of popular big data analytics engines such as Hive and Shark.},
author = {Sarlis, Dimitrios and Papailiou, Nikolaos and Konstantinou, Ioannis and Smaragdakis, Georgios and Koziris, Nectarios},
isbn = {978-1-4503-2758-9},
issn = {0146-4833},
journal = {SIGCOMM Comput. Commun. Rev.},
number = {5},
pages = {21--28},
title = {{SIGMOD '15- Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data}},
volume = {45},
year = {2015}
}
@article{Satyanarayanan2017,
abstract = {ndustry investment and research interest in edge computing, in which computing and storage nodes are placed at the Internet's edge in close proximity to mobile devices or sensors, have grown dramatically in recent years. This emerging technology promises to deliver highly responsive cloud services for mobile computing, scalability and privacy-policy enforcement for the Internet of Things, and the ability to mask transient cloud outages. The web extra at www.youtube.com/playlist?list=PLmrZVvFtthdP3fwHPy{\_}4d61oDvQY{\_}RBgS includes a five-video playlist demonstrating proof-of-concept implementations for three tasks: assembling 2D Lego models, freehand sketching, and playing Ping-Pong.},
author = {Satyanarayanan, Mahadev},
isbn = {0018-9162 VO - 50},
issn = {00189162},
journal = {Computer},
number = {1},
pages = {30--39},
title = {{The emergence of edge computing}},
volume = {50},
year = {2017}
}
@inproceedings{Schultz-Moller2009,
abstract = {The nature of data in enterprises and on the Internet is changing. Data used to be stored in a database first and queried later. Today timely processing of new data, represented as events, is increasingly valuable. In many domains, complex event processing (CEP) systems detect patterns of events for decision making. Examples include processing of environmental sensor data, trades in financial markets and RSS web feeds. Unlike conventional database systems, most current CEP systems pay little attention to query optimisation. They do not rewrite queries to more efficient representations or make decisions about operator distribution, limiting their overall scalability. This paper describes the Next CEP system that was especially designed for query rewriting and distribution. Event patterns are specified in a high-level query language and, before being translated into event automata, are rewritten in a more efficient form. Automata are then distributed across a cluster of machines for detection scalability. We present algorithms for query rewriting and distributed placement. Our experiments on the Emulab test-bed show a significant improvement in system scalability due to rewriting and distribution.},
author = {Schultz-M{\o}ller, Nicholas Poul and Migliavacca, Matteo and Pietzuch, Peter},
booktitle = {Proceedings of the Third ACM International Conference on Distributed Event-Based Systems - DEBS '09},
isbn = {9781605586656},
title = {{Distributed complex event processing with query rewriting}},
year = {2009}
}
@inproceedings{Schuritz2016,
abstract = {Business models have been a concept widely discussed over the last 20 years. The increasing availability of data and the growing capability to exploit them with analytics has sparked a new set of discussions, though: it is claimed that data and analytics bring to bear entirely new “data-based” or “datadriven” business models. However, there is neither a common understanding of these business models nor of the ways existing business models are transformed into those. This paper aims to create a coherent framework and common understanding of the infusion of business models by data and analytics.},
author = {Sch{\"{u}}ritz, Ronny and Satzger, Gerhard},
booktitle = {Proceedings - CBI 2016: 18th IEEE Conference on Business Informatics},
isbn = {9781509032310},
keywords = {analytics,big data,business models,data,service innovation},
title = {{Patterns of Data-Infused Business Model Innovation}},
year = {2016}
}
@article{Shi2016,
abstract = {The success of the Internet of Things and rich cloud services have helped create the need for edge computing, in which data processing occurs in part at the network edge, rather than completely in the cloud. Edge computing could address concerns such as latency, mobile devices' limited battery life, bandwidth costs, security, and privacy.},
author = {Shi, Weisong and Dustdar, Schahram},
isbn = {0018-9162},
issn = {00189162},
journal = {Computer},
keywords = {Cloud Cover,Internet,Internet of Things,big data,cloud,data processing,edge computing,mobile computing},
title = {{The Promise of Edge Computing}},
year = {2016}
}
@article{Shmueli2011,
abstract = {This research essay highlights the need to integrate predictive analytics into information systems research and shows several concrete ways in which this goal can be accomplished. Predictive analytics include empirical methods (statistical and other) that generate data predictions as well as methods for assessing predictive power. Predictive analytics not only assist in creating practically useful models, they also play an important role alongside explanatory modeling in theory building and theory testing. We describe six roles for predictive analvtics: new theory generation, measurement development, comparison of competing theories, improvement of existing models, relevance assessment, and assessment of the predictability of empirical phenomena. Despite the importance of predictive analytics, we find that they are rare in the empirical IS literature. Extant IS literature relies nearly exclusively on explanatory statistical modeling, where statistical inference is used to test and evaluate the explanatory power of underlying causal models, and predictive power is assumed to follow automatically from the explanatory model. However, explanatory power does not imply predictive power and thus predictive analytics are necessary for assessing predictive power and for building empirical models that predict well. To show that predictive analytics and explanatory statistical modeling are fundamentally disparate, we show that they are different in each step of the modeling process. These differences translate into different final models, so that a pure explanatory statistical model is best tuned for testing causal hypotheses and a pure predictive model is best in terms of predictive power. We convert a well-known explanatory paper on TAM to a predictive context to illustrate these differences and show how predictive analytics can add theoretical and practical value to IS research.},
author = {Shmueli, Galit and Koppius, Otto R},
doi = {10.2139/ssrn.1606674},
isbn = {0276-7783},
issn = {02767783},
journal = {Mis Quarterly},
pmid = {63604908},
title = {{Predictive Analytics in Information Systems Research}},
year = {2011}
}
@article{Sturm2019,
author = {Sturm, Benjamin and Sunyaev, Ali},
journal = {Business {\&} Information Systems Engineering},
number = {1},
pages = {91--111},
publisher = {Springer},
title = {{Design Principles for Systematic Search Systems: A Holistic Synthesis of a Rigorous Multi-cycle Design Science Research Journey}},
volume = {61},
year = {2019}
}
@article{Talia2013,
abstract = {Extracting useful knowledge from huge digital datasets requires smart and scalable analytics services, programming tools, and applications.},
author = {Talia, Domenico},
doi = {10.1109/MC.2013.162},
isbn = {0018-9162},
issn = {00189162},
journal = {Computer},
keywords = {Data Mining Cloud Framework,big data,cloud computing,data analytics},
title = {{Clouds for scalable big data analytics}},
year = {2013}
}
@article{Todorovski2003,
abstract = {The paper introduces meta decision trees (MDTs), a novel method for combining multiple classifiers. Instead of giving a prediction, MDT leaves specify which classifier should be used to obtain a prediction. We present an algorithm for learning MDTs based on the C4.5 algorithm for learning ordinary decision trees (ODTs). An extensive experimental evaluation of the new algorithm is performed on twenty-one data sets, combining classifiers generated by five learning algorithms: two algorithms for learning decision trees, a rule learning algorithm, a nearest neighbor algorithm and a naive Bayes algorithm. In terms of performance, stacking with MDTs combines classifiers better than voting and stacking with ODTs. In addition, the MDTs are much more concise than the ODTs and are thus a step towards comprehensible combination of multiple classifiers. MDTs also perform better than several other approaches to stacking.},
author = {Todorovski, Ljupco and D{\v{z}}eroski, Sa{\v{s}}o},
issn = {08856125},
journal = {Machine Learning},
keywords = {Combining classifiers,Decision trees,Ensembles of classifiers,Meta-level learning,Stacking},
number = {3},
pages = {223--249},
title = {{Combining classifiers with meta decision trees}},
volume = {50},
year = {2003}
}
@article{Uhlmann2017,
abstract = {Due to the increased digital networking of machines and systems in the production area, large datasets are generated. In addition, more external sensors are installed at production systems to acquire data for production and maintenance optimization purposes. Therefore, data analytics and interpretation is one of the challenges in Industrie 4.0 applications. Reliable analysis of data (e.g. internal and external sensors), such as system-internal alarms and messages produced during the operation, can be used to optimize production and maintenance processes. Furthermore, information and knowledge can be extracted from raw data and used to develop data-driven business models and services, e.g. offer new availability contracts for production systems. This paper illustrates an approach for decentralized data analytics based on smart sensor networks.},
author = {Uhlmann, Eckart and Laghmouchi, Abdelhakim and Geisert, Claudio and Hohwieler, Eckhard},
issn = {23519789},
journal = {Procedia Manufacturing},
keywords = {Industrie 4.0,algorithms,classifier,data analytics,production systems},
number = {June},
pages = {1120--1126},
publisher = {The Author(s)},
title = {{Decentralized Data Analytics for Maintenance in Industrie 4.0}},
volume = {11},
year = {2017}
}
@article{VanHeck2007,
abstract = {Realizing scenarios in which business is conducted through a rapidly formed network with anyone, anywhere, anytime regardless of different computer systems and business processes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {van Heck, Eric and Vervest, Peter},
eprint = {arXiv:1011.1669v3},
isbn = {9781604138795},
issn = {00010782},
journal = {Communications of the Acm},
keywords = {emergency,luation of non-urgent visits,to a busy urban},
pages = {3--8},
pmid = {26047380},
title = {{Smart Business networks: how the network wins}},
volume = {33},
year = {2007}
}
@article{Venable2016,
abstract = {Evaluation of design artefacts and design theories is a key activity in Design Science Research (DSR), as it provides feedback for further development and (if done correctly) assures the rigour of the research. However, the extant DSR literature provides insufficient guidance on evaluation to enable Design Science Researchers to effectively design and incorporate evaluation activities into a DSR project that can achieve DSR goals and objectives. To address this research gap, this research paper develops, explicates, and provides evidence for the utility of a Framework for Evaluation in Design Science (FEDS) together with a process to guide design science researchers in developing a strategy for evaluating the artefacts they develop within a DSR project. A FEDS strategy considers why, when, how, and what to evaluate. FEDS includes a two-dimensional characterisation of DSR evaluation episodes (particular evaluations), with one dimension being the functional purpose of the evaluation (formative or summative) and the other dimension being the paradigm of the evaluation (artificial or naturalistic). The FEDS evaluation design process is comprised of four steps: (1) explicate the goals of the evaluation, (2) choose the evaluation strategy or strategies, (3) determine the properties to evaluate, and (4) design the individual evaluation episode(s). The paper illustrates the framework with two examples and provides evidence of its utility via a naturalistic, summative evaluation through its use on an actual DSR project.},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Venable, John and Pries-Heje, Jan and Baskerville, Richard},
doi = {10.1057/ejis.2014.36},
eprint = {9780201398298},
isbn = {978-3-642-29862-2},
issn = {14769344},
journal = {European Journal of Information Systems},
keywords = {Design Science Research,artefact evaluation,information systems evaluation,research design,research methodology,utility evaluation},
number = {1},
pages = {77--89},
pmid = {4520227},
title = {{FEDS: A Framework for Evaluation in Design Science Research}},
volume = {25},
year = {2016}
}
@article{Vilalta2002,
abstract = {Different researchers hold different views of what the term meta-learning exactlymeans. The first part of this paper provides our own perspective view in which the goal isto build self-adaptive learners (i.e. learning algorithms that improve their bias dynamicallythrough experience by accumulating meta-knowledge). The second part provides a survey ofmeta-learning as reported by the machine-learning literature. We find that, despite differentviews and research lines, a question remains constant: how can we exploit knowledge aboutlearning (i.e. meta-knowledge) to improve the performance of learning algorithms? Clearlythe answer to this question is key to the advancement of the field and continues being thesubject of intensive research.},
archivePrefix = {arXiv},
arxivId = {arXiv:astro-ph/0005074v1},
author = {Vilalta, Ricardo and Drissi, Youssef},
eprint = {0005074v1},
isbn = {1009982220290},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Classification,Inductive learning,Meta-knowledge},
number = {2},
pages = {77--95},
pmid = {1284},
primaryClass = {arXiv:astro-ph},
title = {{A perspective view and survey of meta-learning}},
volume = {18},
year = {2002}
}
@article{Walls1992,
abstract = {This paper defiens an information system desgin theory (ISDT) to be a prescriptive theory which integrates normative and descriptive theories into design paths intended to produce more effective information systems. The nature of ISDTs is articulated using Dubin's concept of theory building and Simon's idea of a science of the artificial. An example of an ISDT is presented in the context of Executive Information Systems (EIS). Despite the increasing awareness of the potential of EIS for enhancing executive strategic decision making effectiveness, there exists little theoretical work which directly guides EIS design. (continued)},
author = {Walls, Joseph G and Widmeyer, George R and {El Sawy}, Omar A},
isbn = {1047-7047},
issn = {1047-7047},
journal = {Information Systems Research},
number = {1},
pages = {36--59},
title = {{Building an information system design theory for vigilant EIS}},
volume = {3},
year = {1992}
}
@inproceedings{Wang2008,
abstract = {In an integration system of RFID and wireless sensor network (WSN), RFID is used to identify objects while WSN can provide context environment information of these objects. Thus, it increases system intelligent in pervasive computing. We propose the EPC sensor network (ESN) architecture as an integration system of RFID and WSN. This ESN architecture is based on EPCglobal architecture, the de facto international standard for RFID. The core of ESN is the middleware part which is also implemented in our work. In this paper, complex event processing (CEP) technology is used in our ESN middleware which can handle large volume of events from distributed RFID and sensor readers in real time. Through filtering, grouping, aggregating and constructing complex event, ESN middleware provides a more meaningful report for the clients and increases system automation.},
author = {Wang, Weixin and Sung, Jongwoo and Kim, Daeyoung},
booktitle = {Proceedings - 11th IEEE Symposium on Object/Component/Service-Oriented Real-Time Distributed Computing, ISORC 2008},
isbn = {9780769531328},
title = {{Complex event processing in EPC sensor network middleware for both RFID and WSN}},
year = {2008}
}
@article{WilsonA14,
archivePrefix = {arXiv},
arxivId = {1404.2697},
author = {Wilson, Duane and Ateniese, Giuseppe},
eprint = {1404.2697},
journal = {CoRR},
title = {{To Share or Not to Share in Client-Side Encrypted Clouds}},
volume = {abs/1404.2},
year = {2014}
}
@article{Wixom2017,
abstract = {Companies create customer value with analytics by wrapping them around core offerings to reinforce, streamline, or enrich the usage or experience of the offering; companies can apply data wrapping to any customer touchpoint. Regardless of where data wrapping falls along the customer journey, two key capabilities are required to generate customer value: analytics and customer intimacy. These capabilities allow companies to create wrapping that is tailored to the customer and automated.},
author = {Wixom, Barbara H and Sch{\"{u}}ritz, Ronny},
journal = {CISR Research Briefing},
title = {{Creating Customer Value Using Analytics}},
year = {2017}
}
@misc{Wohlgemuth2014,
abstract = {Business networking relies on application-specific quantity and quality of information in order to support social infrastructures in, e.g., energy allocation coordinated by smart grids, healthcare services with electronic health records, traffic management with personal sensors, RFID in retail and logistics, or integration of individuals' social network information into good, services, and rescue operations. Due to the increasing reliance of networking applications on sharing ICT services, dependencies threaten privacy, security, and reliability of information and, thus, innovative business applications in smart societies. Resilience is becoming a new security approach, since it takes dependencies into account and aims at achieving equilibriums in case of opposite requirements. This special issue on 'Security and Privacy in Business Networking' contributes to the journal 'Electronic Markets' by introducing a different view on achieving acceptable secure business networking applications in spite of threats due to covert channels. This view is on adapting resilience to enforcement of IT security in business networking applications. Our analysis shows that privacy is an evidence to measure and improve trustworthy relationships and reliable interactions between participants of business processes and their IT systems. The articles of this special issue, which have been accepted after a double-blind peer review, contribute to this view on interdisciplinary security engineering in regard to the stages of security and privacy requirements analysis, enforcement of resulting security requirements for an information exchange, testing with a privacy-preserving detection of policy violations, and knowledge management for the purpose of keeping business processes resilient.},
author = {Wohlgemuth, Sven and Sackmann, Stefan and Sonehara, Noboru and Tjoa, A Min},
booktitle = {Electronic Markets},
doi = {10.1007/s12525-014-0158-6},
isbn = {1019-6781},
issn = {14228890},
number = {2},
pages = {81--88},
title = {{Security and privacy in business networking}},
volume = {24},
year = {2014}
}
@article{Wolpert1992,
abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory. ?? 1992 Pergamon Press Ltd.},
author = {Wolpert, David H},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Combining generalizers,Error estimation and correction,Generalization and induction,Learning set preprocessing,cross-validation},
number = {2},
pages = {241--259},
pmid = {17947137},
title = {{Stacked generalization}},
volume = {5},
year = {1992}
}
@article{Yi2015,
abstract = {Despite the increasing usage of cloud computing, there are still issues unsolved due to inherent problems of cloud computing such as unreliable latency, lack of mobility support and location-awareness. Fog computing can address those problems by providing elastic resources and services to end users at the edge of network, while cloud computing are more about providing resources distributed in the core network. This survey discusses the definition of fog computing and similar concepts, introduces representative application scenarios, and identifies various aspects of issues we may encounter when designing and implementing fog computing systems. It also highlights some opportunities and challenges, as direction of potential future work, in related techniques that need to be considered in the context of fog computing.},
author = {Yi, Shanhe and Li, Cheng and Li, Qun},
isbn = {9781450335249},
journal = {Proceedings of the 2015 Workshop on Mobile Big Data - Mobidata '15},
keywords = {cloud computing,edge computing,fog computing,mobile cloud computing,mobile edge computing,review},
title = {{A Survey of Fog Computing: Concepts, Applications and Issues}},
year = {2015}
}
@inproceedings{Zhang2016,
abstract = {The implementation of advanced technologies in manufacturing has created large amounts of data. The data can be utilized to create predictive models for quality control, which allows manufacturers to produce higher quality products at a lower cost. Bosch has provided a large-scale data set of a production line and hosted a challenge on Kaggle aiming to predict the manufacturing failures using the anonymized features. We proposed a two-stage method first to cluster the data into groups based on the manufacturing process and then use supervised learning to predict the failed product in each cluster. This approach reduces the sparsity of the data set. Various algorithms were compared. The random forest algorithm achieved the highest performance score and was chosen as the final model.},
author = {Zhang, Darui and Xu, Bin and Wood, Jasmine},
booktitle = {Proceedings - 2016 IEEE International Conference on Big Data, Big Data 2016},
isbn = {9781467390040},
keywords = {clustering,manufacturing,quality control,supervised learning},
title = {{Predict failures in production lines: A two-stage approach with clustering and supervised learning}},
year = {2016}
}
@article{Zhang2011,
author = {Zhang, Jian},
keywords = {Communication in science--Data processing,Information science,Information visualization},
number = {May},
title = {{Data Use and Access Behavior in eScience—Exploring data practices in the new data-intensive science paradigm}},
year = {2011}
}

@article{hirt2019cognitive,
  title={Cognitive computing for customer profiling: meta classification for gender prediction},
  author={Hirt, Robin and K{\"u}hl, Niklas and Satzger, Gerhard},
  journal={Electronic Markets},
  volume={29},
  number={1},
  pages={93--106},
  year={2019},
  publisher={Springer}
}
