% Please refer README file for more details about this document

\documentclass[twoside,twocolumn]{article}

\usepackage{tabulary,graphicx,times,caption,fancyhdr,amsfonts,amssymb,amsbsy,latexsym,amsmath}
\usepackage[utf8]{inputenc}
\usepackage{url,multirow,morefloats,floatflt,cancel,tfrupee,textcomp,colortbl,xcolor,pifont}
\usepackage[nointegrals]{wasysym}
\urlstyle{rm}

\makeatletter

%Etal definition in references
\usepackage{ifxetex}
\ifxetex\else
  \usepackage{dblfloatfix}
\fi

\@ifundefined{subparagraph}{
\def\subparagraph{\@startsection{paragraph}{5}{2\parindent}{0ex plus 0.1ex minus 0.1ex}%
{0ex}{\normalfont\small\itshape}}%
}{}

\def\URL#1#2{\@ifundefined{href}{#2}{\href{#1}{#2}}}

%%For url break
\def\UrlOrds{\do\*\do\-\do\~\do\'\do\"\do\-}%
\g@addto@macro{\UrlBreaks}{\UrlOrds}

\makeatother
\def\floatpagefraction{0.8} 
\def\dblfloatpagefraction{0.8}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[paperheight=11in,paperwidth=8.3in,margin=2.5cm,headsep=.7cm,top=2.5cm]{geometry}
\usepackage[T1]{fontenc}
\def\floatpagefraction{0.8}
\widowpenalty 10000
\clubpenalty 10000

\renewenvironment{abstract}
	{\trivlist\item[]\leftskip0pt\par\vskip4pt\noindent
  	\textbf{\abstractname}\mbox{\null}\\}
	{\par\noindent\endtrivlist}

\def\keywords#1{\par\medskip\par\noindent\textbf{Keywords}: #1\par}

\linespread{1.13} \date{} \emergencystretch 8pt

\captionsetup[figure]{labelfont=normal,skip=1.4pt,aboveskip=1pc}
\captionsetup[table]{labelfont=normal,skip=1.4pt}

\makeatletter
\def\author#1{\gdef\@author{\hskip-\tabcolsep%
	\parbox{\textwidth}{\raggedright\bfseries#1\\[1pc]}}}
\def\address[#1]#2{\g@addto@macro\@author{\\\hskip-\tabcolsep\parbox{\textwidth}{\raggedright%
	\normalsize\normalfont\textsuperscript{#1}#2}}}
\let\addresslink\textsuperscript
\def\correspondence#1{\g@addto@macro\@author{\\\hskip-\tabcolsep\parbox{\textwidth}{\raggedright%
	\vspace*{10pt}\normalsize\normalfont~\\#1~\\[12pt]}}}
\def\email#1{\g@addto@macro\@author{\\\hskip-\tabcolsep\parbox{\textwidth}{\raggedright%
	\normalsize\normalfont Emails: #1}}}

\def\title#1{\gdef\@title{\vspace*{-30pt}%
	\raggedright\textbf{\@journaltitle}~\\%
  \raggedright\bfseries\ifx\@articleType\@empty\vspace*{20pt}\else%
  \vspace*{20pt}\@articleType\vspace*{20pt}\\\fi#1}}
\let\@journaltitle\@empty \def\journaltitle#1{\gdef\@journaltitle{{\normalfont\itshape#1}}}
\let\@articleType\@empty \def\articletype#1{\gdef\@articleType{{\normalfont\itshape#1}}}

\let\@runningHead\@empty \def\RunningHead#1{\gdef\@runningHead{{\normalfont #1}}}

\usepackage{fancyhdr}
\fancypagestyle{headings}{\renewcommand{\headrulewidth}{0pt}\fancyhf{}
  \fancyhead[R]{\itshape\@runningHead}
  \fancyfoot[C]{\thepage}}
\pagestyle{headings}

\fancypagestyle{plain}{\renewcommand{\headrulewidth}{0pt}%
	%\fancyhf{}\fancyhead[R]{International Journal of Intelligent Systems}
  \fancyfoot[C]{\thepage}}
\makeatother

\usepackage[%
	numbers,sort&compress%
	%authoryear
  ]{natbib}

\setcounter{secnumdepth}{0}
\usepackage{float,xcolor}

\usepackage{hyperref}
\usepackage{multirow}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{cuted}
\usepackage{amsmath, amsfonts}

%\journaltitle{International Journal of Intelligent Systems}
\articletype{Research Article} % Research Article/Review Article/Clinical Study

\begin{document}

% Title of the document
\title{Rethinking the U-Net, ResUnet, and U-Net3+ architectures with dual skip connections for building footprint extraction}

% Author names
\author{%
		B. Neupane\addresslink{1,2},
  	J. Aryal\addresslink{1,2} and
  	A. Rajabifard\addresslink{2}
    }
		
% Affiliation
\address[1]{Earth Observation and AI Research Group, Faculty of Engineering and IT, The University of Melbourne, VIC 3010, Australia}
\address[2]{Department of Infrastructure Engineering, Faculty of Engineering and IT, The University of Melbourne, VIC 3010, Australia.}

% Corresponding author details
\correspondence{Correspondence should be addressed to 
    	B. Neupane: bneupane@student.unimelb.edu.au}

% Emails of authors
\email{bneupane@student.unimelb.edu.au (B. Neupane), jagannath.aryal@unimelb.edu.au (J. Aryal), abbas.r@unimelb.edu.au (A. Rajabifard)}%

% Running Head
%\RunningHead{International Journal of Intelligent Systems}

\maketitle 

% Abstract
\begin{abstract}
The importance of building footprints and their inventory has been recognised as foundational spatial information for multiple societal problems. Extracting complex urban buildings involves the segmentation of very high-resolution (VHR) earth observation (EO) images. U-Net is a common deep learning network and foundation for its new incarnations like ResUnet, U-Net++ and U-Net3+ for such segmentation. The re-incarnations look for efficiency gain by re-designing the skip connection component and exploiting the multi-scale features in U-Net. However, skip connections do not always improve these networks and removing some of them provides efficiency gains and reduced network parameters. In this paper, we propose three dual skip connection mechanisms for U-Net, ResUnet, and U-Net3+. These mechanisms deepen the feature maps forwarded by the skip connections and allow us to study which skip connections need to be denser to yield the highest efficiency gain. The mechanisms are evaluated on feature maps of different scales in the three networks, producing nine new network configurations. The networks are evaluated against their original vanilla versions using four building footprint datasets (three existing and one new) of different spatial resolutions: VHR (0.3m), high-resolution (1m and 1.2m), and multi-resolution (0.3+0.6+1.2m). The proposed mechanisms report efficiency gain on four evaluation measures for U-Net and ResUnet, and up to 17.7\% and 18.4\% gain in F1 score and Intersection over Union (IoU) for U-Net3+. The codes will be available in a GitHub link after peer review.

% Keywords - if any
\keywords{building footprint extraction; u-net; skip connection; multi-scale feature aggregation; spatial information; semantic segmentation}
\end{abstract}
    
% First level heading
\section{Introduction}
Semantic segmentation of earth observation (EO) images is the current state-of-the-art (SOTA) method for urban feature extraction and land use and land cover classification (LULC) \cite{neupane2021deep}. The evolving nature of semantic segmentation has seen traditional classifiers including edge-based, shadow-based, region-based, and machine learning approaches \cite{hossain2019segmentation}. These methods lack precision due to their dependency on mid-level semantic characteristics (hand-crafted features). Neural networks overcome this limitation with an increased number of ``layers'' and provide contrasting results when compared to machine learning predecessors like random forest \cite{du2015semantic} and support vector machine \cite{huang2012svm}. An encoder-decoder network (EDN) with a convolutional neural network (CNN) as its fundamental backbone (encoder) has become the common structural configuration for semantic segmentation \cite{minaee2021image}. U-Net \cite{ronneberger2015u} is the most common EDN for semantic segmentation with its legacy in all application domains ranging from the segmentation of medical imagery to satellite imagery. Therefore, U-Net is one of the most revisited and modified networks for efficiency gain. In this paper, we rethink the architecture of three widely appreciated EDNs $-$ U-Net, ResUnet, and U-Net3+ $-$, particularly the ``skip connection'' components of the networks that propagate the context information learned by CNN encoder to the higher-resolution layers of the decoder. The motivation for revisiting these skip connections comes from recent studies that found the skip connections do not always provide efficiency gain \cite{wang2022uctransnet}, and our own experiments conclude the same. 

To explain the limitation of skip connections, let us start by explaining how EDN such as U-Net works. An encoder is essentially a CNN that generates multi-scale feature maps using convolutions and down-sampling operations. These maps comprise low-level and fine-grained information. The decoder part of U-Net is symmetrical to its encoder with up-sampling operations replacing the down-sampling operations. The decoder comprises high-level and coarse-grained semantic information. A bridge called ``skip connection'' pass the low-level information from the encoder layers to the corresponding decoder layers to make the utmost use of the encoder-decoder structure. U-Net uses these connections between all encoder and decoder layers if the feature maps share the same dimension. For example, if the U-Net has an encoder of four scale layers except for the last bottleneck block, four skip connections are used between the encoder and decoder. Our experiments show that using only one skip connection between the second or third scale layers yields higher accuracy scores. Thus, removing the first and fourth skip connections not only improves the accuracy but also reduces the network parameters. On the contrary, the newer version of U-Net such as U-Net++ and U-Net3+ feature more denser skip connections for efficiency gain. With U-Nets getting denser skip connections, and our finding of not all skip connections are needed in U-Net, we revisit them to find which connections need to be denser for efficiency gain. In particular, we revisit the plain skip connections of U-Net, the full-skip connection mechanism of U-Net3+ \cite{huang2020unet} and the residual path skip connection of ResUnet \cite{zhang2018road} as these three U-Net variants have the most impact in the domain of semantic segmentation.

The popularity of U-Net and its newer versions comes from a gradually improved trade-off between the use of context information (generated by the encoders) and precise localisation (in the layers of the decoder) for precise segmentation. U-Net++ \cite{zhou2019unet++} replaces the skip connections of U-Net with nested and dense skip connections. These dense connections with rich information and reduced inter-encoder-decoder semantic gaps increase the precision of segmentation. The limitation of the dense connections is increased network parameters. U-Net3+ comes with a more efficient way of exploiting the multi-scale features with full-scale skip connections. The inter-encoder-decoder skip connections of U-Net are re-designed, and new intra-connections between the decoder layers are added to capture fine-grained details and coarse-grained semantics in full scales. The full-scale skip connection assumes equal weights among the feature maps generated at different scales. However, different scales of feature maps possess different levels of discrimination and context information, which is not realised in the U-Net3+ configuration. To address these weaknesses of U-Net and U-Net3+, we propose a dual skip connection mechanism (DSCM) for U-Net and a dual full-scale skip connection mechanism (DFSCM) for U-Net3+ with a new multi-scale feature aggregation technique that aggregates the feature maps of different scales with increased weights for the smaller-scale feature maps. Further, we test the concept of dula skip connections in ResUnet with a new dual respath skip connection mechanism (DRSCM).

The proposed connection mechanisms are different from the existing implementation of dual skip connections like DR-Unet \cite{le2021dr} and DPN-Unet-TypeII \cite{xu2021dual}. Our approach deepens the networks with denser convolution operations and enriches the information in the skip connections. DSCM in U-Net deepens the convolutional blocks of different scales and DRSCM on ResUnet deepens its residual block and doubles the residual skip connections. In U-Net3+, a new multi-scale feature aggregation technique is introduced to integrate the DFSCM. Unlike, the aggregation that assumes equal weights for all feature maps in U-Net3+, the proposed DFSCM gives more weight to the small-scale feature maps, where the context information is still intact. A lower weight is given to the large-scale features as they lose context with downsampling operations. Such denser skip connections have not been studied in U-Net3+ to the best of our knowledge. All three proposed mechanisms can be plugged to deepen only the desired scale layers of networks, as not all skip connections provide the efficiency gain. To study the effects of dual skip connections in different scale layers, we propose and experiment with three scale variants of each network. With the three variants of the three connection mechanisms on the three networks, a total of nine new network configurations are proposed in this paper.

The experimental design of the proposed networks is categorised in terms of the spatial resolution of datasets. Four datasets of VHR and high-resolution types are used for the evaluation, with one newly developed multi-resolution dataset of complex urban building samples. The performance of the nine new networks is compared to the vanilla versions of U-Net, U-Net3+, ResUnet and other SOTA EDNs like U-Net++ \cite{zhou2019unet++}, Deeplabv3+ \cite{chen2017deeplab}, and SegNet \cite{badrinarayanan2017segnet}. All of the proposed and vanilla networks are evaluated with four evaluation measures. The contributions of this paper are:

\begin{enumerate}
    \setcounter{enumi}{0}
    \item We propose DSCM and DRSCM for U-Net and ResUnet to deepen the encoder and enrich the skip connection for an effective trade-off between the use of context and precise localisation of building footprints from VHR and high-resolution images.
    \item We propose DFSCM for U-Net3+ with a new multi-scale feature aggregation technique. Unlike in U-Net3+, the feature maps of different scales are aggregated with increased weights for the smaller-scale feature maps, where the context information is intact.
    \item We present comprehensive experiments to study the proposed mechanisms on different scale layers of U-Net, ResUnet, and U-Net3+ with a total of nine new network configurations. The nine networks are tested on four datasets of different spatial resolutions.
    \item A new multi-resolution dataset is developed for a comprehensive robustness check with urban building samples of different spatial resolutions.
    \item An ablation study is provided to find the scale layers in U-Net that provide the highest efficiency gain with the proposed DSCM.
\end{enumerate}


\section{Related Work} \label{sec:relw}

\subsection{CNNs, FCNs and EDNs} \label{sec:relw:CNNs}
The earliest forms of CNN are AlexNet \cite{krizhevsky2012imagenet}, VGGNet \cite{simonyan2014very}, GoogleNet \cite{szegedy2015going}, ResNet \cite{he2016deep}, and Xception \cite{chollet2017xception}. They are now used to extract multi-scale feature information from images \cite{chen2018aerial,ayala2021deep} for segmentation purposes in FCNs and EDNs like SegNet \cite{badrinarayanan2017segnet} and U-Net \cite{ronneberger2015u}. Some of the earliest studies of CNN-based building footprint extraction \cite{mnih2013machine,saito2016multiple} perform patch-based segmentation supported by post-processing methods to improve accuracy and precision. CNN-based pixel-level building segmentation is achieved by Mask R-CNN that uses CNNs like ResNet as feature extractor (aka. backbone) \cite{zhao2018building,griffiths2019improving}. Similarly, building footprint extraction has also seen the use of FCN \cite{maggiori2016convolutional} and different versions of it: FCN-2s, FCN-4s, and FCN-8s \cite{zhong2016fully,yang2018building}.

The early form of EDNs (SegNet and U-Net) are first introduced to segment indoor/outdoor scenes and medical images. They followed their way to EO images including multi-spectral images from WorldView-3 \cite{li2019semantic}, synthetic aperture radar (SAR) and multi-spectral images from Sentinel-1 and 2 \cite{ayala2021deep} for building segmentation. Some have also achieved building classification \cite{pan2020deep}. SegNet is also experimented with further modifications \cite{bischke2019multi,sariturk2020feature} and in combination with U-Net to form Seg-Unet \cite{abdollahi2022ensemble}. Similarly, ResUnet \cite{zhang2018road} is introduced to extract road networks with residual units added to U-Net to ease the training and further enrich the skip connections. ResUnet is further experimented for building segmentation with multi-modal hand-crafted features \cite{xu2018building} and ultra-high resolution (UHR) images \cite{li2019semantic}. Due to its legacy in the domain of computer vision, widely used versions of the U-Net family are the focus of this paper.

\subsection{The legacy and evolution of U-Net} \label{sec:relw:unet}
The popularity of U-Net \cite{ronneberger2015u} comes from addressing the loss of context information in the CNNs due to (i) the heavy use of pooling and max-pooling operations, and (ii) the imbalanced trade-off between localisation and the use of context. U-Net has seen an explosion in usage in medical imaging since its introduction and several variants have been proposed in this domain \cite{siddique2021u}. However, the loss of context information cannot be avoided while using CNNs as a feature extractor. U-Net is followed by a number of improvements that are focused on utilising the multi-scale feature maps of CNNs in an efficient manner. Among many, some of the successful variants are 3D U-Net \cite{cciccek20163d}, V-Net \cite{milletari2016v} attention U-Net \cite{oktay2018attention}, U-Net++ \cite{zhou2019unet++}, R2U-Net \cite{alom2018recurrent}, Inception-U-Net \cite{zhang2020dense}, ResUnet \cite{zhang2018road}, Dense U-Net \cite{wang2019dense}, adversarial U-Net\cite{schonfeld2020u}, U$^2$Net \cite{qin2020u2}, and U-Net3+ \cite{huang2020unet}. These U-Nets are widely used to segment both medical and EO images. The recent advancement in U-Net seems also to be affected by the rise of Vision Transformer (ViT) \cite{dosovitskiy2020image}, which brings the science of Transformer \cite{vaswani2017attention} from natural language processing to a computer vision problem. The concept of ViT tries to replace CNNs in semantic segmentation but has still not been able to fully replace it. The adaptation of ViT in the recent U-Net versions includes TransUnet \cite{chen2021transunet}, TransFuse \cite{zhang2021transfuse}, and Swin-Unet \cite{cao2021swin}. TransUnet builds upon the problem of CNNs failing to fully learn the global and remote semantic information interaction because of the involvement of the convolutional process. The prior concepts of feature pyramid \cite{lin2017feature}, DeepLab \cite{chen2017deeplab}, atrous convolution layers \cite{chen2018encoder}, context encoder network (CE-Net) \cite{gu2019net}, self-attention \cite{wang2018non}, and attention gate \cite{schlemper2019attention} also have tried to address this problem. With U-Net3+ as the most recent version based on the naming, there are some recent studies that have tried to improve the network with an addition/integration of attention module \cite{li2020macu}, residual unit \cite{qin2022improved}, and transformer \cite{chen2023improved}. O-Net \cite{wang2022net} is a recent development that realises the addition of a self-attention mechanism on the transformer and combining it with CNN can marginally improve the network when compared to computational-heavy networks based on ViT. The improvement in performance is vitally important to computer vision but raises some serious questions regarding the computation expense. Unlike computationally super-expensive transformer-based semantic segmentation to address the problem of inadequate learning of global and remote semantic information, some other advancements in U-Net variants seek to lower the number of network parameters in U-Net while improving the performance of U-Net. The aim of effective exploitation of multi-scale features has brought the researcher to re-think the U-Net networks with re-designed skip connections, which we review next.

\subsection{Re-designing skip connections} \label{sec:relw:skip}
U-Net passes the features from the encoder layers to the decoder layers through skip connections and concatenates them to the upsampled outputs. The redesigning of these skip connections is ongoing research. The recent variants of U-Net like NAS-UNet \cite{weng2019unet}, U-Net3+ \cite{huang2020unet}, DC-UNet \cite{lou2021dc}, and Half-UNet \cite{lu2022half} re-design skip connections in U-Net while reducing the number of network parameters. U-Net++ and U-Net3+ change the terminology of ``skip connections'' to ``plain skip connections'' with the realisation of this skip connection. U-Net++ \cite{zhou2019unet++} propose dense nested skip connections for performance gain, but with added complexity and network parameters. U-Net3+ on the other hand re-designs the skip connections with fewer network parameters with a newly proposed full-scale skip connection. This connection utilises inter-encoder-decoder and intra-decoder skip connections to capture both fine- and coarse-grained semantics from full scales. To concatenate the maps of five scales, a multi-scale feature aggregation mechanism is developed for U-Net3+. Moving away from dense and full-skip connections, MultiResUnet \cite{ibtehaz2020multiresunet} and DR-Unet \cite{le2021dr} replace plain skip connections with \textit{Res} paths for improved performance in U-Net and ResUnet. DPN-Unets \cite{xu2021dual} utilise a dual path network (DPN) \cite{chen2017dual} to combine DenseNet and ResNet in parallel. The recent-most network that re-design the skip connections in U-Net is UC-TransNet \cite{wang2022uctransnet}. The authors of UC-TransNet have highlighted that not all skip connections provide efficiency gain, and some even influence the original U-Net negatively. Their skip connection utilises a channel-wise cross-fusion transformer and channel-wise cross-attention for efficiency gain. It can be seen that the baseline for all the re-designing of skip connections are U-Net and ResUnet. Leaving the added complexity of transformers are other components behind, we point out that the advancement in skip connections to utilise multi-scale features is an evolving work. Besides the aggregation of features from the encoder side, some aggregate features of the decoder side \cite{aryal2023multi}, which is out of the scope of our study. Our focus is on the skip connections and the multi-scale aggregation of features from the encoder.

Exploiting multi-scale features with new aggregation and fusion techniques is popular in urban feature extraction from EO images \cite{wu2018automatic,wei2019toward,ji2019scale}. U-Net and U-Net3+ rely on concatenation to aggregate the multi-scale feature maps. Some networks like Half-UNet \cite{lu2022half} aggregate them using an addition operation inspired by ResNet to increase the amount of information without increasing the dimension. The final segmented output is generated without the large-scale decoder layers resulting in 100x lower network parameters with similar performance when compared to U-Net3+. The experiments also show the lower performance of U-Net3+ against U-Net in some of the datasets. In a similar attempt to subtract the components of U-Net, Fu et al. \cite{fu2021keep} design ``additive'' and ``subtractive'' variants of U-Net. The additive variants add a dense block, residual block, side-output block, and dilated convolution block to the U-Net and the subtractive variants: U-Net without Rectified Linear Units (ReLU) activation, U-Net without skip connections, and U-Net with one convolutional layer per level. The subtractive variants show a lower dice score compared to the additive variants. In their experiments, U-Net is less competitive without skip connections. Other studies have highlighted that not all skip connections between different scales of encoder-decoder improve these U-Net configurations \cite{wang2022uctransnet}. However, there is a lack of conclusions on which scales are to be focused. In this study, we take the vanilla version of U-Net, ResUnet, and U-Net3+ and enrich their skip connections. Furthermore, we confirm which scales of encoder-decoder configurations are to be focused on for efficiency gain.

\section{Method} \label{sec:method}
\subsection{DSCM for U-Net} \label{sec:meth:arch}
Let us start with simplifying a U-Net of 5-scale layers. The encoder $En$ consecutively passes the learned feature maps along the encoder layers $X_{En}^{n}$ (where $n=1,\dots,5$). An $n^{th}$ layer of encoder $X_{En}^{n}$ consists of two recurring unpadded 3x3 convolutions $\mathcal{C}(\cdot)$ followed by a batch normalization (BN). Each $\mathcal{C}(\cdot)$ is activated with a ReLU activation function $\sigma(\cdot)$. The features are down-scaled with operation $\mathcal{D}(\cdot)$ of max-pooling with stride 2 along the consecutive encoder layers. After the final encoder layer $X_{En}^{5}$, the decoder starts by upsampling the low-resolution feature maps of $X_{En}^{5}$, and keeps doing so along the decoder layers $X_{De}^{n}$ (where $n=4,\dots,1$). An $n^{th}$ layer of decoder $X_{De}^{n}$ starts with an up-sampling operation $\mathcal{U}(\cdot)$ of a 2x2 ``up-convolution'' that halves the number of feature channels from the previous layer. The output of $\mathcal{U}(\cdot)$ is concatenated to the feature map of the corresponding encoder layer $X_{En}^{n}$, followed by a $\mathcal{C}(\cdot)$. The feature map of $X_{En}^{n}$ is brought to $X_{Dn}^{n}$ by a plain skip connection for matching index $n$. A 1x1 convolution in the final layer maps the 64-component feature vectors to the number of classes. This general 5-scale U-Net is illustrated in Figure \ref{fig:dualskipconnection}(a). With this process, U-Net utilises both low- and high-resolution features, conserving the spatial integrity of objects that is crucial in the semantic segmentation of features in EO data. However, the problem with this architecture of U-Net lies in the down-scaling as $\mathcal{D}(\cdot)$ increases the scale while halving the dimension of the feature maps, thus losing the context information at each $\mathcal{D}(\cdot)$ operation. 

  \begin{figure*}[!ht]
  \centering
  \includegraphics[width=16cm]{images/neupa1.png}
  \caption{Illustration of the plain skip connections in U-Net and the proposed DSCM.}
      \label{fig:dualskipconnection}
      \end{figure*}

To explain the proposed DSCM, let us denote one sequence of $\mathcal{C}(\cdot)$ and BN as $\mathcal{G}(\cdot)$. Then a layer of $X_{En}^{n}$ in U-Net sequentially consists of $\mathcal{C}(\cdot)$, $\mathcal{G}(\cdot)$, and $\mathcal{D}(\cdot)$ operations. To compensate for the lost context information in U-Net, we propose DSCM as illustrated in Figure \ref{fig:dualskipconnection}. With DSCM, instead of one, an encoder layer now consists of two sequential $\mathcal{G}(\cdot)$: $\mathcal{G}_{1}(\cdot)$ and $\mathcal{G}_{2}(\cdot)$. Then, an $X_{En}^{n}$ consists of $\mathcal{C}(\cdot)$, $\mathcal{G}_{1}(\cdot)$, $\mathcal{G}_{2}(\cdot)$, and $\mathcal{D}(\cdot)$ sequential operations. Similar to the U-Net, $\mathcal{D}(\cdot)$ down-scales the feature maps of the first sequence $\mathcal{G}_{1}(\cdot)$, along $X_{En}^{n}$ (where $n=1,\dots,5$). Instead of one plain skip connection, DSCM consists of two skip connections: $\mathcal{S}_{1}(\cdot)$ and $\mathcal{S}_{2}(\cdot)$, which respectively pass the features of $\mathcal{G}_{1}(\cdot)$ and $\mathcal{G}_{2}(\cdot)$ of $X_{En}^{n}$ to $X_{De}^{n}$ for matching $n$. In $X_{De}^{n}$, the feature maps brought by $\mathcal{S}_{1}(\cdot)$ and $\mathcal{S}_{2}(\cdot)$ are concatenated to the output of $\mathcal{U}(\cdot)$ that upsamples the features of the previous layer of decoder $X_{De}^{n+1}$. In case of $X_{De}^{4}$, it receives an upsampled output of $X_{En}^{5}$. The output feature map of a $X_{De}^{n}$ for $n=4,\dots,1$ can be denoted as

\begin{equation}
X_{De}^{n}=\mathcal{C}(\mathcal{C}(\mathcal{U}(X_{De}^{n+1})))
\oplus \mathcal{S}_{1}(X_{En}^{n})
\oplus \mathcal{S}_{2}(X_{En}^{n})
\label{eqn:DSCM}
\end{equation}

where, the $\mathcal{C}(\cdot)$ is supported by a ReLU $\sigma(\cdot)$, and the feature maps carried by skip connections $\mathcal{S}_{1}$ and $\mathcal{S}_{2}$ is denoted as

\begin{equation}
\mathcal{S}_{1}(X_{En}^{n})=\mathcal{G}_1(\mathcal{C}(X_{En}^{n-1}))
\label{eqn:DSCM-skip1}
\end{equation}

\begin{equation}
\mathcal{S}_{2}(X_{En}^{n})=\mathcal{G}_2(\mathcal{S}_{1}(X_{En}^{n}))
\label{eqn:DSCM-skip2}
\end{equation}

%A 1x1 convolution in the final layer of the decoder ($X_{De}^{1}$) maps the 64-component feature vectors to the number of classes.

\subsection{DSCM in different scale features of U-Net} \label{sec:meth:dualskip}
The DSCM can be plugged into any scale layers of U-Net. To study the effects of DSCM on different scales of fine-grained detailed information and coarse-grained semantic information, we propose three novel U-Net network architectures: dual skip on large-scale features (DS-UNet-L), dual skip on small-scale features (DS-UNet-S), and dual skip on all scale features (DS-UNet-A). 

\begin{enumerate}
    \setcounter{enumi}{0}
    \item \textit{DS-UNet-L} applies DSCM between the large-scale layers of encoder $X_{En}^{n}$ and decoder $X_{De}^{n}$ for $n\in [3,4]$.
    \item \textit{DS-UNet-S} applies DSCM between the small-scale layers of encoder $X_{En}^{n}$ and decoder $X_{De}^{n}$ for $n\in [1,2]$.
    \item \textit{DS-UNet-A} applies DSCM between all four scale layers of encoder $X_{En}^{n}$ and decoder $X_{De}^{n}$ for $n\in [1,2,3,4]$.
\end{enumerate}

      
\subsection{DRSCM for ResUnet} \label{sec:meth:dsresunet}
DSCM cannot be directly implemented in ResUnet because of its residual units. For this, we propose a DRSCM for ResUnet. We name the ResUnet with DRSCM as a DS-ResUnet. In addition to the U-Net configuration, a ResUnet includes a skip connection (we name it respath) within its residual unit \cite{zhang2018road}. Therefore, to deepen the residual unit with one more 3x3 convolution, one more respath needs to be added as shown in the illustration of the proposed DRSCM in Figure \ref{fig:dsresunet}(c). Unlike the existing dual skips in \cite{le2021dr} and \cite{xu2021dual}, the residual unit is deeper and denser in our proposed DRSCM with an added 3x3 convolution layer $\mathcal{C}(\cdot)$, BN, ReLU $\sigma(\cdot)$, and a respath $\mathcal{R}(\cdot)$. 

  \begin{figure*}[!ht]
  \centering
  \includegraphics[width=16cm]{images/neupa3.png}
  \caption{Illustration of DS-ResUNet-A along with the differences between the plain skip connections in ResUnet and the proposed DRSCM}
      \label{fig:dsresunet}
      \end{figure*}

Let us start by explaining a ResUNet of 5-scale layers. Encoder $En$ consecutively passes the learned feature maps along $X_{En}^{n}$ (where $n=1,\dots,5$). Lets denote one sequence of BN and unpadded 3x3 convolution $\mathcal{C}(\cdot)$ as $\mathcal{G}(\cdot)$. Each $\mathcal{C}(\cdot)$ is activated with a ReLU $\sigma(\cdot)$. Then, $X_{En}^{n}$ consists of two sequences of $\mathcal{G}(\cdot)$ i.e., $\mathcal{G}_{1}(\cdot)$ followed by $\mathcal{G}_{2}(\cdot)$. The output of $\mathcal{G}_{2}(\cdot)$ is added to the output of a respath skip connection $\mathcal{R}(\cdot)$. A respath skip connection $\mathcal{R}(\cdot)$ consists of a 1x1 $\mathcal{C}(\cdot)$ followed by a BN on the input feature map of $X_{En}^{n-1}$. 

With DRSCM integrated into a residual block of a ResUNet, we add a third $\mathcal{G}(\cdot)$ and $\mathcal{R}(\cdot)$ operations to deepen the residual block. Now, a $X_{En}^{n}$ consists of $\mathcal{G}_{1}(\cdot)$, $\mathcal{G}_{2}(\cdot)$, $\mathcal{R}_{1}(\cdot)$, $\mathcal{G}_{3}(\cdot)$, and $\mathcal{R}_{2}(\cdot)$ sequential operations. The output of $\mathcal{G}_{3}(\cdot)$ is added to the output of second respath skip connection $\mathcal{R}_{2}(\cdot)$. The output features of the $\mathcal{R}_{1}(\cdot)$ are down-scaled with operation $\mathcal{D}(\cdot)$ of max-pooling with stride 2 along $n$ residual units. After the final encoder layer $X_{En}^{5}$, the decoder starts by up-sampling the low-resolution feature maps of $X_{En}^{5}$, and keeps on doing so along the decoder layers $X_{De}^{n}$ (where $n=4,\dots,1$). An $n_{th}$ decoder layer $X_{De}^{n}$ starts with an up-sampling operation $\mathcal{U}(\cdot)$ of 2x2 ``up-convolution''. In case of $X_{De}^{4}$, it receives an upsampled output of $X_{En}^{5}$. The output of $\mathcal{U}(\cdot)$ is concatenated to the two sets of feature maps of the corresponding encoder layer $X_{En}^{n}$, followed by two consecutive $\mathcal{G}(\cdot)$. The two sets of feature maps are brought from $X_{En}^{n}$ to $X_{De}^{n}$ for matching index $n$ by two skip connections of DRSCM, each consisting of the output of $\mathcal{R}_{1}(\cdot)$+$\mathcal{G}_{2}(\cdot)$ and $\mathcal{R}_{2}(\cdot)$+$\mathcal{G}_{3}(\cdot)$. The output feature map of a $X_{De}^{n}$ for $n=4,\dots,1$ can be denoted as

\begin{equation}
\begin{split}
X_{De}^{n}=[\mathcal{G}(\mathcal{G}(\mathcal{U}(X_{De}^{n+1})))
\oplus \mathcal{S}_{1}(X_{En}^{n})
\oplus \mathcal{S}_{2}(X_{En}^{n})] \\ + \mathcal{R}(X_{De}^{n+1})
\label{eqn:DRSCM}
\end{split}
\end{equation}

where, the feature maps carried by skip connections $\mathcal{S}_{1}$ and $\mathcal{S}_{2}$ can be denoted as

\begin{equation}
\mathcal{S}_{1}(X_{En}^{n})=\mathcal{G}_{2}(\mathcal{G}_{1}(X_{En}^{n-1}))+\mathcal{R}_{1}(X_{En}^{n-1})
\label{eqn:DRSCM-skip1}
\end{equation}

\begin{equation}
\mathcal{S}_{2}(X_{En}^{n})=\mathcal{G}_{3}(\mathcal{S}_{1}(X_{En}^{n}))+\mathcal{R}_{2}(X_{En}^{n-1})
\label{eqn:DRSCM-skip2}
\end{equation}

Similar to the three variations of DS-UNet, we experiment with the integration of DRSCM in the large-scale (DS-ResUNet-L), small-scale (DS-ResUNet-S), and all-scale (DS-ResUNet-A) features.
      
\subsection{DFSCM for U-Net3+} \label{sec:meth:dsunet3+}
The integration of dual skip connections in U-Net3+ is more sophisticated because of the existing full-skip connections in U-Net3+ \cite{huang2020unet}. Unlike the plain skip connections of U-Net and ResUnet that pass the features from $X_{En}^{n}$ to $X_{De}^{n}$ only for matching index $n$, the full-skip connections allow aggregating the feature maps of all $n$ scales to capture both fine- and coarse-grained semantics in $X_{De}^{n}$. Let us take an example of the third decoder layer $X_{De}^{3}$ of U-Net3+. $X_{De}^{3}$ receives the feature maps of (i) same-scale encoder layer $X_{En}^{3}$ similar to U-Net, (ii) smaller-scale encoder layer $X_{En}^{2}$ and $X_{En}^{1}$ carrying coarse-grained detailed information through inter encoder-decoder skip connections supported by non-overlapping max pooling operations, and (iii) larger-scale decoder layer $X_{De}^{4}$ and $X_{De}^{5}$ carrying fine-grained semantic information through intra-decoder connections supported by bilinear interpolation. The number of channels in all incoming five same-resolution feature maps is unified with 64 filters of 3x3 size. A feature aggregation mechanism is applied on the concatenated maps of five scales that consist of 320 (64 times 5) filters of 3x3 size, a BN, and a ReLU activation function. The feature aggregation mechanism in U-Net3+ assumes equal weights for all feature maps at different scales. However, the different scales of feature maps possess different levels of discrimination. The context information is more intact in the small-scale feature maps as the large-scale features get smaller in size and lose them because of the downsampling operations.

We integrate the concepts of DSCM and full-scale skip connections from U-Net3+ to propose DFSCM for U-Net3+. We name the formulated networks DS-UNet3+. The difference in U-Net3+ and DS-UNet3+ is illustrated in Figure \ref{fig:dsunet3plus}. The skip connections are similar to those of U-Net3+, except the single inter-encoder-decoder plain skip connections are all replaced by two plain skip connections of DSCM. To support the two connections in DS-UNet3+, we develop a dual skip feature aggregation mechanism (abbr. DSFAM). The DSFAM consists of a different number of filters when compared to U-Net3+. For example, Figure \ref{fig:dsunet3plus-De3} illustrates the construction of feature maps in the third decoder layer of DS-UNet3+. 

  \begin{figure*}[!hbt]
  \centering
  \includegraphics[width=16cm]{images/neupa4.png}
  \caption{Illustration of the proposed DS-UNet3+ networks DS-UNet3+(L), DS-UNet3+(S), and DS-UNet3+(A).}
      \label{fig:dsunet3plus}
      \end{figure*}
      
  \begin{figure}[!hbt]
  \centering
  \includegraphics[width=7.5cm]{images/neupa5.png}
  \caption{Illustration of dual skip feature aggregation mechanism (abbr. DSFAM) at the third decoder layer $X_{De}^{3}$ of DS-UNet3+ (figure adapted and modified from \cite{huang2020unet}).}
      \label{fig:dsunet3plus-De3}
      \end{figure}
      
The $X_{De}^{3}$ receives the feature maps of same-scale encoder layer $X_{En}^{3}$ with two plain skip connections. The feature maps of smaller-scale encoder layer $X_{En}^{2}$ and $X_{En}^{1}$ carrying coarse-grained detailed information are transported through two inter-encoder-decoder skip connections supported by down-scaling operation $\mathcal{D}(\cdot)$ of non-overlapping max pooling operations. Finally, the feature maps from larger-scale decoder layer $X_{De}^{4}$ and $X_{De}^{5}$ carrying fine-grained semantic information are transported through intra-decoder connections supported by upsampling operation $\mathcal{U}(\cdot)$ of bilinear interpolation. In case of $X_{De}^{4}$, it receives an upsampled output of $X_{En}^{5}$. Similar to U-Net3+, the number of channels in all incoming same-resolution feature maps is unified with 64 filters of 3x3 size. The output feature map $f$ of a $X_{De}^{n}$ for $n=4,\dots,1$ of a $N$-scaled DS-UNet3+ can be denoted as


\begin{equation}
\begin{split}
X_{De}^{n}=\mathcal{C} \big( \mathcal{C} \big( \mathcal{U}(X_{De}^{n+1}) \big) \big)  
\oplus 
\underbrace{ \mathcal{DS}_{F} \big( X_{En}^{n}, \dots, \mathcal{D}_{2i}(X_{En}^{1})\big) }_{\text{Scales}: \ n^{th} \sim 1^{th}} 
\\
\oplus  
\underbrace{ \mathcal{S}_{De} \big( \mathcal{U}_{2j}(X_{De}^{n}), \dots, \mathcal{U}_{2j}(X_{De}^{N}) \big) }_{\text{Scales}: \ (n+1)^{th} \sim (N-1)^{th}},
\\
i = k \ (\text{for} \ k=1 \ \text{to} \ n),
\\
j = m \ (\text{for} \ m=1 \ \text{to} \ n-k+1)
\label{eqn:DSFAM}
\end{split}
\end{equation}

where, $\mathcal{C}(\cdot)$ denotes unpadded 3x3 convolution activated with ReLU $\sigma(\cdot)$. $\mathcal{DS}_{F}(\cdot)$ denotes DFSCM that brings the features from $\mathcal{S}_{1}$ (refer to Eqn. \ref{eqn:DSCM-skip1}) and $\mathcal{S}_{2}$ (refer to Eqn. \ref{eqn:DSCM-skip2}) from each encoder layers $X_{En}^{n}, \dots, X_{En}^{1}$. Similar to the DS-UNets, the second and third convolutional layer in each encoder block is followed by a BN. $\mathcal{S}_{De}(\cdot)$ refers to the intra-decoder connections that bring the features from previous decoder layers $X_{De}^{n}, \dots, X_{De}^{N}$. The value of $i$ and $j$ denote stride size for down-sampling operation $\mathcal{D}_{2i}(\cdot)$ and bilinear up-sampling operation $\mathcal{U}_{2j}(\cdot)$ respectively. Here, $i$ ranges from $1$ to $n$ and $j$ ranges from $1$ to $n-i+1$ depending upon the value of $n$.


Similar to DS-UNet and DS-ResUNet, three variants of DS-UNet3+ are studied: DS-UNet3+(L), DS-UNet3+(S), and DS-UNet3+(A). In DS-UNet3+(A), the DSFAM concatenates 9, 8, 7, and 6 sets of feature maps with a total of 576, 512, 448, and 384 filters of 3x3 size on decoder layers $X_{De}^{4}$, $X_{De}^{3}$, $X_{De}^{2}$, and $X_{De}^{1}$ respectively. In DS-UNet3+(L), the DFSCM is integrated between third ($X_{En}^{3}$ to $X_{De}^{3}$) and fourth ($X_{En}^{4}$ to $X_{De}^{4}$) scale layers of U-Net3+. Similarly, in the small-scale variation DS-UNet3+(S), the DFSCM is integrated between first ($X_{En}^{1}$ to $X_{De}^{1}$) and second ($X_{En}^{2}$ to $X_{De}^{2}$) scale layers. The proposed DS-UNet3+ networks consist of fewer network parameters than U-Net, ResUnet, DS-UNets, and DS-ResUNets.


\section{Dataset and Experimental Setup} \label{sec:experiments}

\subsection{Datasets} \label{sec:exp:data}
We experiment on three existing datasets of VHR and the high-resolution type and a newly developed multi-resolution building footprint dataset.

The first dataset is the VHR WHU Building dataset \cite{ji2018fully} (abbr. WHU). It includes satellite images of Christchurch, New Zealand with a spatial resolution of 0.3m. To maintain uniform image size between all the datasets in our experiments, the originally 512x512 sized-image patches of the WHU are tiled into 256x256, increasing the number of image-label pairs by four times. Thus, 23088 training tiles and 9664 validation tiles are prepared.

The second dataset is the high-resolution (1m) Massachusetts Building dataset \cite{mnih2013machine}. The original 1500x1500 tiles are cropped to 256x256 by generating a grid of coordinates. The validation images provided are used for validation. The partial tiles on the edges of the tiles are ignored, by iterating only through the Cartesian product between the two intervals $range(0, h-h \bmod d, d)$ and $range(0,w-w \bmod d, d)$ for width $w$, height $h$, and output tile size of 256 as $d$. Let $H=h-h \bmod d$ and $W=w-w \bmod d$. Then the Cartesian product of the two intervals is the set of all ordered pairs $(i,j)$, and can be defined as

\begin{equation}
\begin{split}
\mathcal{P} = \{(i,j)\ |\ 0\le i<H, 0\le j<W, i\in\mathbb{N}, j\in\mathbb{N}, \\  i\equiv 0\ (\textrm{mod}\ d), j\equiv 0\ (\textrm{mod}\ d)\}
\label{eqn:cartesian}
\end{split}
\end{equation}

where, $i$, and $j$ are non-negative integers among the set of natural numbers $\mathbb{N}$.

The third dataset that we have developed (label, image) is named as Melbourne building footprint dataset (abbr. MELB). It is the first multi-resolution building dataset that covers samples from a complex urban environment i.e., the City of Melbourne, Australia. The labels are developed by masking and tiling the building roof samples provided by the City of Melbourne. The corresponding image tiles of 0.3m, 0.6m, and 1.2m spatial resolution of the labels are collected from Nearmap's API service. The number of training and validation samples are divided as 70\% and 30\% respectively. Therefore, this dataset is prepared in an end-to-end manner, without manual annotations. It is made sure that the image and labels are of the same projection system and of the same year.

The fourth dataset is a 1.2m subset of the MELB dataset that we developed in our previous work \cite{neupane2022building}. We use this dataset to experiment with the proposed networks on high-resolution images.


\subsection{Comparison to state-of-the-art} \label{sec:exp:sota}
The proposed variations of DS-UNet, DS-ResUNet, and DS-UNet3+ are compared to the vanilla U-Net, ResUnet, and U-Net3+. Furthermore, the networks are also compared to U-Net++ \cite{zhou2019unet++}, Deeplabv3+ \cite{chen2018encoder}, and SegNet \cite{badrinarayanan2017segnet} with VGG-16 encoder. The encoder of the SegNet is chosen to be VGG-16 to keep the network parameters similar to the U-Net that we compare to. No components such as attention gates, deep supervision, and classification-guided module (CGM) are added to the vanilla version of the SOTA networks. For uniformity in experiments, the same loss function is used to evaluate all networks. A dice loss \cite{sudre2017generalised} is used as the loss function to monitor the proposed and the compared networks in our experiments. This loss calculates the measure of overlap to assess the performance of segmentation when a ground truth (GT) is available. Furthermore, it maximises the dice coefficient (aka. F1 score) and minimises the possible class imbalance between the number of ``background'' and ``building pixels'' in binary classification. Dice loss is denoted by (\ref{eqn:diceloss}) as

    \begin{equation}\label{eqn:diceloss}
        \mathcal{L}(y,\hat{p}) = 1 - \frac{2y\hat{p}+1} {y+\hat{p}+1} 
    \end{equation}
    
where $y$ and $\hat{p}$ represent the GT and prediction respectively. The smooth value of 1 is added in the numerator and denominator considering the edge case scenario of $y=\hat{p}=0$. The product $y\hat{p}$ represents the intersection between the GT and prediction.

\subsection{Implementation details} \label{sec:exp:train}
All DL networks are wrapped in the Keras framework with a mini-batch size of 2. The step/epoch is set as the ratio of the number of training images to the batch size. The epoch is set such that the total number of steps is kept the same or similar (approx. 60000). A learning rate of 1e-4 is used to train all the networks. \textit{Adam}, \textit{He Normal}, and ReLU) are the optimizer, initializer, and activation functions respectively. A \textit{sigmoid} function is used to obtain the final output maps as the dataset is binary, and a \textit{dropout} of 50\% is used to avoid over-fitting. The hyper-parameters are kept the same to train all the proposed, and SOTA networks.

\subsection{Evaluation Metrics} \label{sec:exp:eval}
The evaluation of the networks is performed using (i) pixel accuracy (PA), (ii) adjusted accuracy (AA), (iii) F1 score (F1), and (iv) intersection over union (IoU). PA (\ref{eqn:pixelacc}) measures the frequency of match between the predictions and the binary labels. AA (\ref{eqn:avgacc}) is an average of Sensitivity (\ref{eqn:sensitivity}) and specificity (\ref{eqn:specificity}), which quantify the proportion of correctly identified actual positives and actual negatives respectively. F1 (\ref{eqn:f1score}) and IoU (\ref{eqn:iou}) are calculated from the `area of overlap' between prediction and binary labels and `area of union' (all of the predictions + binary labels - the overlap). Symbolic representations of the metrics are:

    \begin{equation}\label{eqn:pixelacc}
        Pixel\ accuracy\ (PA) = \frac{TP + TN} {TP + TN + FP + FN} 
    \end{equation} 
    
    \begin{equation}\label{eqn:avgacc}
        Adjusted\ accuracy\ (AA) = \frac{Sensitivity + Specificity} {2} 
    \end{equation} 
    
    \begin{equation}\label{eqn:sensitivity}
        Sensitivity = \frac{TP} {TP + FN}
    \end{equation}
    
    \begin{equation}\label{eqn:specificity}
        Specificity = 1 - \frac{TN} {TN + FP}
    \end{equation}

    \begin{equation}\label{eqn:f1score}
        F1 score = \frac{2 \times TP} {2 \times TP + FN + FP} 
    \end{equation}
    
    \begin{equation}\label{eqn:iou}
        IoU = \frac{TP} {TP + FN + FP}
    \end{equation}
    
where, TP is true positive (i.e., prediction = 1, label = 1); FP is false positive (prediction = 1, label = 0); FN is false negative (prediction = 0, label = 1); and TN is true negative (prediction = 0, label = 0). Other than the four accuracy metrics, the number of network parameters (abbr. Par.) are also compared.


\section{Results and Discussion} \label{sec:res}
The results and comparisons from the experiments are presented for three experimental settings categorised based on spatial resolutions: VHR, high-resolution, and multi-resolution. The proposed networks are compared to the vanilla versions of U-Net, ResUnet, U-Net3+, U-Net++, Deeplabv3+, and SegNet. Further, the DSCM is studied at different scales of U-Net and differently scaled U-Nets as an ablation study. In overall, 70 experiments are performed in this section to evaluate the proposed dual skip connection mechanisms in U-Net, ResUnet, and U-Net3+. The limitations of the experiments finalise the discussion at the end of this section.

\subsection{Results on VHR building dataset (WHU)} \label{sec:res:VHR}
Table \ref{tab:res:VHR} presents the performance of three versions of DS-UNet, DS-ResUNet, and DS-UNet3+ on the VHR benchmark WHU building dataset. The proposed networks are compared to their original architectures of vanilla U-Net, ResUnet, and U-Net3+, and also to U-Net++, Deeplabv3+, and SegNet. Figure \ref{fig:results} shows the sample results from all the proposed and original vanilla networks on the WHU dataset. 

\begin{table}[!hbt]
\centering
\caption{DS-UNet, DS-ResUNet, and DS-UNet3+ on 0.3m VHR WHU building dataset. The highest scores are highlighted in bold in each group of experiments of DS-UNets, DS-ResUNets, and DS-UNet3+.}
\label{tab:res:VHR}
\addtolength{\tabcolsep}{-0.2em}
\small
\centering
\begin{tabular}{lccccc}
\hline
Networks & Par. (M) & PA & AA & F1 & IoU \\ \hline
U-Net & 31.041 & 0.973 & 0.826 & 0.844 & 0.770 \\
ResUNet & 75.346 & 0.969 & 0.833 & 0.822 & 0.741 \\
U-Net++ & 34.538 & 0.974 & 0.828 & 0.823 & 0.749 \\
U-Net3+ & 22.891 & \textbf{0.981} & \textbf{0.855} & 0.688 & 0.615 \\
Deeplabv3+ & 11.852 & 0.975 & 0.854 & \textbf{0.852} & \textbf{0.784} \\
SegNet & 29.458 & 0.969 & 0.831 & 0.666 & 0.584 \\ \hline
DS-UNet-L & 36.943 & \textbf{0.976} & \textbf{0.851} & \textbf{0.865} & \textbf{0.799} \\
DS-UNet-S & 31.410 & 0.973 & 0.849 & 0.853 & 0.781 \\
DS-UNet-A & 37.312 & 0.975 & 0.837 & 0.848 & 0.777 \\ \hline
DS-ResUNet-L & 85.024 & 0.972 & 0.845 & 0.833 & 0.760 \\
DS-ResUNet-S & 75.951 & 0.969 & \textbf{0.853} & 0.800 & 0.720 \\
DS-ResUNet-A & 85.629 & \textbf{0.974} & 0.842 & \textbf{0.844} & \textbf{0.770} \\ \hline
DS-UNet3+(L) & 27.101 & 0.967 & 0.838 & 0.822 & 0.741 \\
DS-UNet3+(S) & 23.335 & 0.974 & \textbf{0.851} & 0.835 & 0.761 \\
DS-UNet3+(A) & 27.359 & \textbf{0.976} & 0.850 & \textbf{0.863} & \textbf{0.795} \\ \hline
\end{tabular}
\end{table}

\subsubsection{DSCM for U-Net (DS-UNets)} \label{sec:res:VHR:dsunets}
All three versions of the proposed DS-UNet outperform U-Net on the WHU dataset in terms of all accuracy measures as shown in Table \ref{tab:res:VHR}. DS-UNet-L shows the highest performance among the three versions and also outperforms all compared SOTA networks. DS-UNet-S outperforms DS-UNet-A in terms of AA, F1, and IoU. The results on the VHR dataset show that concatenating the doubled large-scale features in U-Net with DSCM results in the highest performance with about a 14\% increase in network parameters. The small-scale features can be kept the same without being doubled as seen from DS-UNet-A. This supports our initial argument that the trade-off between the use of context and precise location can be improved in U-Net. This can be done by increasing the large-scale feature maps, where the contexts are lost due to down-sampling operations such as max-pooling.

\subsubsection{DRSCM for ResUnet (DS-ResUNet)}\label{sec:res:VHR:dsresunets}
All three DS-ResUNets outperform the vanilla ResUnet network. Among the three, DS-ResUNet-A achieves the highest in three out of four evaluation measures. The smallest variant DS-ResUNet-S outperforms ResUnet in two evaluation measures with approx. 0.8\% increase in network parameter. Similarly, the largest variant DS-ResUNet-A outperformed ResUnet in all measures with 13.6\% rise in network parameters. The results are shown in Table \ref{tab:res:VHR}.

\subsubsection{DFSCM for U-Net3+ (DS-UNet3+)} \label{sec:res:VHR:dsunet3plus}
The results demonstrate the success of DSCM and DRSCM in plain skip connections in U-Net and ResUnet on the VHR dataset. However, the results from DFSCM on DS-UNet3+ show mixed performance when compared to U-Net3+ in WHU dataset. All three DS-UNet3+ networks significantly outperform U-Net3+ in terms of F1 (0.863 vs. 0.688) and IoU (0.795 vs. 0.615), while producing 1-2\% lower PA (0.976 vs. 0.981) and AA (0.850 vs. 0.855). The significant increase in F1 and IoU is worth noting with approx. 16\% increase and 12\% decrease in network parameters of the largest variant DS-UNet3+(A) compared to U-Net3+ and U-Net respectively. The results are shown in Table \ref{tab:res:VHR}. DS-UNet3+ variants perform better than U-Net3+ in all four datasets, which are demonstrated in the next sections.

\subsubsection{Ablation Study of DS-UNets} \label{sec:res:VHR:ablation}
Studies have shown that not all skip connections improve the performance of U-Net \cite{wang2022uctransnet}. Table \ref{tab:res:VHR} supports this argument. Therefore, an ablation study is conducted on the VHR WHU Building dataset to verify the effectiveness of the proposed DSCM on different scale layers of U-Net and different scaled U-Nets. The design of this ablation study is categorised into two groups. The first group of ablation experiments take a U-Net of five-scale layers as shown in Figure \ref{fig:dualskipconnection}(a) before, and applies DSCM between each scale layer of its encoder and decoder i.e., $X_{En}^{n}$ and $X_{De}^{n}$. Figure \ref{fig:ablation}(a)-(d) illustrates the four DS-UNet formed in this group of experiments: DS-UNet-1, DS-UNet-2, DS-UNet-3, and DS-UNet-4. The second group of ablation experiments investigates the smaller DS-UNets with a lower number of scales as illustrated in Figures \ref{fig:ablation}(e)-(g). Unlike the DS-UNets of five scale layers, these experiments compare DS-UNets of 4, 3, and 2 scale layers and are named DS-UNet-4sc, DS-UNet-3sc, and DS-UNet-2sc respectively. All networks in the two ablation groups are experimented on 0.3m VHR WHU building dataset.

  \begin{figure*}[!hbt]
  \centering
  \includegraphics[width=14cm]{images/neupa6.png}
  \caption{Illustration of the seven DS-UNet networks for ablation study. Sub-figure (a)-(d) and sub-figure (e)-(g) show the DS-UNets of the first and second group of ablation experiments respectively.}
      \label{fig:ablation}
      \end{figure*}

The performance evaluation of the first ablation group is shown in Table \ref{tab:res:indiscale}. All variants except DS-UNet-3 outperform the vanilla U-Net in terms of PA and AA, with the highest scores from DS-UNet-2. Only the DS-UNet-4 outperform U-Net in term of F1 and IoU, the value of which is seen to have a gradual increase in numbers from DS-UNet-1 to DS-UNet-4. DSCM on the fourth scale layer of U-Net, therefore, aids in DS-UNet-L of the previous experiments to achieve the highest evaluation scores.

\begin{table}[!hbt]
\centering
\caption{First group of ablation: proposed DSCM between each scale of encoder-decoder layers, and their comparison to U-Net.}
\label{tab:res:indiscale}
\centering
\addtolength{\tabcolsep}{-0.2em}
\small
\begin{tabular}{lccccc}
\hline
Networks & Par. (M) & PA & AA & F1 & IoU \\ \hline
U-Net & 31.041 & 0.973 & 0.826 & 0.844 & 0.770 \\
DS-UNet-1 & 31.114 & 0.977 & 0.842 & 0.674 & 0.597 \\
DS-UNet-2 & 31.336 & \textbf{0.982} & \textbf{0.853} & 0.690 & 0.616 \\
DS-UNet-3 & 32.221 & 0.972 & 0.827 & 0.833 & 0.758 \\
DS-UNet-4 & 35.761 & 0.976 & 0.844 & \textbf{0.855} & \textbf{0.786} \\ \hline
\end{tabular}
\end{table}

Table \ref{tab:res:smallscales} presents the comparison of the three DS-UNets and their corresponding U-Nets from the second group of the ablation experiment. These three DS-UNets integrate DSCM on only the largest scale layer available in the network structure as the first group of ablation experiments shows that the DSCM between the largest scale layers yields the highest performance gain in terms of F1 and IoU. As reported, DSCM does not improve the evaluation measures of 2-scale and 3-scale U-Nets but improves those of 4-scale U-Net. Also, when compared to the 5-scale DS-UNet (DS-UNet-4 from Table \ref{tab:res:indiscale}), it can be seen that the performance rises along with the number of scales, with the DS-UNet-5sc as the best performer. When compared to the 5-scale U-Net, DS-UNet-4sc achieves a slightly higher AA score with 22M fewer parameters.

\begin{table}[!hbt]
\centering
\caption{Second group of ablation: proposed DSCM on 4, 3, and 2 scale U-Nets.}
\label{tab:res:smallscales}
\centering
\addtolength{\tabcolsep}{-0.2em}
\small
\begin{tabular}{lccccc}
\hline
Networks & Par. (M) & PA & AA & F1 & IoU \\ \hline
UNet-2sc & 0.405 & 0.936 & 0.803 & 0.635 & 0.532 \\
DS-UNet-2sc & 0.479 & 0.944 & 0.788 & 0.625 & 0.519 \\ \hline
UNet-3sc & 1.865 & 0.957 & 0.846 & 0.756 & 0.672 \\
DS-UNet-3sc & 2.161 & 0.962 & 0.839 & 0.750 & 0.663 \\ \hline
UNet-4sc & 7.702 & \textbf{0.970} & 0.833 & 0.796 & 0.717 \\
DS-UNet-4sc & 8.883 & 0.969 & \textbf{0.839} & \textbf{0.817} & \textbf{0.738} \\ \hline
UNet-5sc & 31.041 & 0.973 & 0.826 & 0.844 & 0.770 \\
DS-UNet-5sc & 36.943 & \textbf{0.976} & \textbf{0.844} & \textbf{0.855} & \textbf{0.786} \\ \hline
\end{tabular}
\end{table}


\subsection{Results on high-resolution dataset (Massachusetts)} \label{sec:res:massachusetts}
Table \ref{tab:res:mass} presents the results of all proposed and SOTA networks on the high-resolution Massachusetts building dataset. All three DS-UNets and DS-ResUNets outperform U-Net and ResUnet, with the highest in all four metrics from DS-UNet-L and DS-ResUNet-A respectively. Once again, the large-scale variant of DS-UNet and an all-scale variant of DS-ResUNet perform better in the high-resolution dataset. All three variants of DS-UNet3+ outperform U-Net3+ in terms of F1 and IoU. All the proposed networks outperform the compared SOTA networks. Figure \ref{fig:results} shows the segmentation results of all proposed and original vanilla networks. 

\begin{table}[!hbt]
\centering
\caption{DS-UNet, DS-ResUNet, and DS-UNet3+ on 1m high-resolution Massachusetts building dataset. The highest scores are highlighted in bold in each group of experiments.}
\label{tab:res:mass}
\centering
\small
\begin{tabular}{lcccc}
\hline
Networks & PA & AA & F1 & IoU \\ \hline
U-Net & 0.953 & 0.879 & 0.749 & 0.609 \\
ResUNet & 0.949 & 0.880 & \textbf{0.770} & \textbf{0.633} \\
U-Net++ & 0.945 & 0.858 & 0.741 & 0.596 \\
U-Net3+ & \textbf{0.953} & \textbf{0.889} & 0.757 & 0.617 \\
Deeplabv3+ & 0.942 & 0.861 & 0.733 & 0.586 \\
SegNet & 0.944 & 0.864 & 0.730 & 0.583 \\ \hline
DS-UNet-L & 0.952 & \textbf{0.892} & \textbf{0.756} & \textbf{0.618} \\
DS-UNet-S & 0.951 & 0.878 & 0.747 & 0.607 \\
DS-UNet-A & 0.951 & 0.889 & 0.752 & 0.612 \\ \hline
DS-ResUNet-L & 0.934 & 0.897 & 0.792 & 0.663 \\
DS-ResUNet-S & 0.932 & 0.884 & 0.785 & 0.649 \\
DS-ResUNet-A & 0.937 & \textbf{0.892} & \textbf{0.800} & \textbf{0.671} \\ \hline
DS-UNet3+(L) & 0.931 & 0.881 & \textbf{0.782} & \textbf{0.649} \\
DS-UNet3+(S) & 0.929 & 0.881 & 0.773 & 0.635 \\
DS-UNet3+(A) & \textbf{0.952} & \textbf{0.887} & 0.750 & 0.611 \\ \hline
\end{tabular}
\end{table}

\subsection{Results on multi-resolution dataset (MELB)} \label{sec:res:multi}
Table \ref{tab:res:multi} presents the results of all proposed and SOTA networks on the multi-resolution MELB dataset. Among the three DS-UNets, the DS-UNet-L and DS-UNet-A outperform U-Net on the MELB dataset, while DS-UNet-S fall behind U-Net. DS-UNet-L shows the highest PA and AA. DS-UNet-A shows the highest F1 and IoU. All DS-ResUNets outperform ResUnet with DS-ResUNet-S being the highest in all scores. The DS-UNet3+(L) and DS-UNet3+(S) outperform U-Net3+ in terms of F1 and IoU.  All proposed networks outperform the compared SOTA networks. Figure \ref{fig:results} shows the segmentation results of all the networks on two samples: a high-rise building and a complex building structure.

\begin{table}[!hbt]
\centering
\caption{DS-UNet, DS-ResUNet, and DS-UNet3+ on multi-resolution (0.3m + 0.6m + 1.2m) MELB dataset.  The highest scores are highlighted in bold in each group of experiments.}
\label{tab:res:multi}
\centering
\small
\begin{tabular}{lcccc}
\hline
Networks & PA & AA & F1 & IoU \\ \hline
U-Net & 0.892 & 0.893 & 0.826 & 0.718 \\
ResUNet & 0.894 & 0.888 & 0.823 & 0.714 \\
U-Net++ & 0.890 & 0.888 & 0.820 & 0.709 \\
U-Net3+ & \textbf{0.904} & \textbf{0.899} & \textbf{0.832} & \textbf{0.724} \\
Deeplabv3+ & 0.891 & 0.882 & 0.819 & 0.708 \\
SegNet & 0.897 & 0.885 & 0.819 & 0.705 \\ \hline
DS-UNet-L & \textbf{0.902} & \textbf{0.896} & 0.829 & 0.720 \\
DS-UNet-S & 0.894 & 0.891 & 0.824 & 0.716 \\
DS-UNet-A & 0.898 & 0.894 & \textbf{0.834} & \textbf{0.726} \\ \hline
DS-ResUNet-L & 0.894 & 0.893 & 0.825 & 0.716 \\
DS-ResUNet-S & \textbf{0.895} & \textbf{0.894} & \textbf{0.826} & \textbf{0.718} \\
DS-ResUNet-A & \textbf{0.895} & 0.892 & \textbf{0.826} & 0.717 \\ \hline
DS-UNet3+(L) & \textbf{0.903} & \textbf{0.894} & 0.834 & 0.728 \\
DS-UNet3+(S) & 0.901 & \textbf{0.894} & \textbf{0.836} & \textbf{0.729} \\
DS-UNet3+(A) & 0.897 & 0.895 & 0.831 & 0.724 \\ \hline
\end{tabular}
\end{table}

  \begin{figure}[!hbt]
  \centering
  \includegraphics[width=7.5cm]{images/neupa7.png}
  \caption{Segmentation output from the proposed networks and their original vanilla baseline networks on WHU, MASS, and MELB building datasets. (a) Sample image tile, (b) U-Net, (c) DS-UNet-L, (d) DS-UNet-S, (e) DS-UNet-A, (f) ResUnet (g) DS-ResUNet-L, (h) DS-ResUNet-S, (i) DS-ResUNet-A, (j) U-Net3+, (k) DS-UNet3+(L), (l) DS-UNet3+(S), (m) DS-UNet3+(A). There are two samples for each of the three datasets. For the MELB dataset, the first sample shows the segmentation results on a high-rise building of 63.88m in height. The second sample shows the complexity of the building samples in the dataset.}
      \label{fig:results}
      \end{figure}

\subsection{Results on 1.2m subset of MELB dataset} \label{sec:res:MELB12}
The proposed networks outperform their original counterparts on the benchmark VHR and high-resolution dataset in all accuracy measures and in the majority of measures on the multi-resolution MELB dataset. Table \ref{tab:res:melb12} presents the results from the networks on the 1.2m high-resolution building footprint dataset, which is a subset of MELB. The comparison of the DS-UNets shows reduced PA and AA by up to 2-5\% when compared to U-Net. However, a significant increase of up to approx. 20\% is observed in terms of F1 and IoU from all versions of DS-UNets. Similarly, DS-ResUNet-S outperforms ResUnet in terms of three measures. Similar to the MELB dataset, DS-UNet3+(L) and DS-UNet3+(S) outperform U-Net3+ in terms of F1 and IoU. The majority of the proposed networks outperform the compared SOTA networks in most of the accuracy measures. A mixed performance is seen in this subset of the MELB dataset because of the inability of the networks to generalise complex urban buildings in 1.2m spatial resolution. VHR and multi-resolution datasets like MELB are more reliable to extract urban buildings.

\begin{table}[!hbt]
\centering
\caption{DS-UNet, DS-ResUNet, and DS-UNet3+ on 1.2m subset of MELB dataset. The highest scores are highlighted in bold in each group of experiments.}
\label{tab:res:melb12}
\centering
\small
\begin{tabular}{lcccc}
\hline
Networks & PA & AA & F1 & IoU \\ \hline
U-Net & 0.928 & 0.764 & 0.437 & 0.342 \\
ResUNet & 0.872 & 0.745 & 0.545 & 0.451 \\
U-Net++ & 0.930 & 0.728 & \textbf{0.602} & \textbf{0.510} \\
U-Net3+ & \textbf{0.953} & \textbf{0.778} & 0.471 & 0.380 \\
Deeplabv3+ & 0.940 & 0.684 & 0.570 & 0.473 \\
SegNet & 0.854 & 0.715 & 0.356 & 0.262 \\ \hline
DS-UNet-L & 0.942 & \textbf{0.722} & 0.617 & 0.523 \\
DS-UNet-S & \textbf{0.947} & 0.703 & \textbf{0.636} & \textbf{0.547} \\
DS-UNet-A & 0.942 & 0.721 & 0.635 & 0.543 \\ \hline
DS-ResUNet-L & 0.924 & \textbf{0.743} & 0.527 & 0.429 \\
DS-ResUNet-S & \textbf{0.933} & 0.731 & \textbf{0.561} & \textbf{0.467} \\
DS-ResUNet-A & 0.919 & 0.688 & 0.520 & 0.416 \\ \hline
DS-UNet3+(L) & 0.940 & 0.727 & \textbf{0.603} & \textbf{0.502} \\
DS-UNet3+(S) & 0.938 & 0.701 & 0.602 & 0.506 \\
DS-UNet3+(A) & \textbf{0.945} & \textbf{0.772} & 0.439 & 0.343 \\ \hline
\end{tabular}
\end{table}


\subsection{Limitations} \label{sec:res:limit}
A common experimental setup is used to compare the proposed networks, the original vanilla networks, and other SOTA networks. The experiments are designed to focus on the efficiency gain in the segmentation of VHR and high-resolution EO images. No pre-processing and post-processing methods are used for the regularisation of building boundaries. The MELB dataset is prepared end-to-end without manual annotations, with images from an API service of Nearmap and labels from a secondary source. However, due to the UAV-based sources, the dataset is affected by the off-nadir angle of source images. This problem results in a misalignment between images and labels, which rises along with the height of the buildings. The misalignment further rises in the higher spatial resolution subset if measured in pixels. This means that in a 256x256 tile, there is more misalignment between an object (building) and its label in 0.3m spatial resolution when compared to a 1.2m resolution image. Therefore, training only on a lower-resolution dataset might not result in a higher performance score, but it definitely assists in the generalisation of objects if bundled together with a higher-resolution dataset. The majority of the publicly available building footprint dataset covers residential buildings with no complex high-rises and skyscrapers. The end-to-end prepared multi-resolution MELB dataset could therefore be useful for the precise and automated extraction of urban buildings.

\section{Conclusion} \label{sec:conclusion}
This paper dissects and re-thinks the network configuration of three widely used SOTA EDNs that are based on CNNs deep learning architectures for building footprint extraction. The particular focus is on re-designing the skip connections of U-Net, ResUnet, and U-Net3+ that transport the learned vectors from their CNN encoder to their decoder. Three dual skip connection mechanisms $-$ DSCM, DRSCM, and DFSCM $-$ are designed for U-Net, ResUnet, and U-Net3+ respectively to compensate for their limitations and to achieve efficiency gain. Moreover, they provide an extensive experimental study to find the skip connections that can be made denser to achieve higher accuracy measures. This further provides a more accurate trade-off between the use of context and precise localisation for semantic segmentation of VHR and high-resolution EO images. Further, a dual full-scale skip connection mechanism (DFSCM) is proposed for the U-Net3+. DFSCM incorporates the increased low-level context information with high-level semantics from the feature maps at different scales using a multi-scale feature aggregation technique. Unlike U-Net3+, the aggregation proposed aggregates the multi-scale feature maps with increased weights for the smaller-scale feature maps, where the context information is still intact.

The three proposed mechanisms double the feature maps in the encoder before passing them to the decoder and they can be plugged into specific scale layers of the networks. With this advantage, the dual skip connection mechanism is tested in different scale layers of three networks, resulting in a total of nine networks with different configurations of skip connection. Furthermore, an ablation study is provided to confirm the optimal scale layer for efficiency gain. The following conclusions can be made from our experiments on VHR and high-resolution building footprint datasets:

\begin{enumerate}
    \setcounter{enumi}{0}
    \item Enriching the plain-skip connections of U-Net with increased large-scale features from DSCM provides efficiency gain in extracting buildings from VHR and high-resolution EO images. 
    \item Increasing the largest-scale features (before the bottleneck) in U-Net produces the highest F1 and IoU. This works in U-Net with more than three scale layers (refer to Table \ref{tab:res:smallscales}).
    \item The residual blocks of ResUnet can be deepened with DRSCM for efficiency gain.
    \item U-Net3+ can yield a higher F1 and IoU with increased feature maps using DFSCM and DSFAM aggregation. In the VHR dataset, the proposed DS-UNet3+ networks achieved up to 17.7\% and 18.4\% gain in F1 and IoU respectively. Therefore, we conclude that assuming equal weights for all feature maps is a limitation of U-Net3+, which can be addressed by using DSFAM to give different weights to the different scales of feature maps that possess different levels of discrimination.
    \item The 1.2m subset of the MELB dataset among the three spatial resolutions of 0.3m, 0.6m, and 1.2m, is insufficient to generalise these complex buildings. However, it improves the performance scores when bundled together with higher-resolution samples, especially if there exists a misalignment between images and labels in complex urban settings. A multi-resolution dataset like ours can improve the networks affected by off-nadir images.
\end{enumerate}

The studies from this paper highlight several gaps in the literature that could be addressed in future works. The gaps are: (i) lack of studies to point out the optimal scale layers that need skip connections, (ii) the majority of EO-based high-resolution building datasets cover residential areas resulting in a lack of studies in complex urban settings that include high-rise and skyscrapers, (iii) lack of conclusions that point out the optimal resolution of EO images for urban feature extraction, and (iv) the misalignment between roof labels and UAV-based aerial images due to inaccurate ortho-rectification methods, resulting in inaccurate urban building footprint extraction. The contributions in this paper touch on some of these gaps, however, our future works will be dedicated to addressing and minimising the other gaps.


\section*{Acknowledgement}
The first author (B.N.) is supported by the University of Melbourne for his PhD research and is awarded by Melbourne Research Scholarship.


% Bibliography style - if using a .bib file
\bibliographystyle{hindawi_bib_style}
%\bibliography{Reference} % without .bib extension
{\small
\bibliography{reference}}
  
  %Or
  
  %\begin{thebibliography}{00}
  %\bibitem{b1}
		%J. Bruix and M. Sherman, ``Management of hepatocellular carcinoma: 
		%an update'', \textit{Hepatology}, vol. 53, no. 3, pp. 1020--1022, 2011.
 
 %  \bibitem{b2}
		% ...

 %  \bibitem{b3}
		% ...
 
 %\end{thebibliography}  

\end{document}
