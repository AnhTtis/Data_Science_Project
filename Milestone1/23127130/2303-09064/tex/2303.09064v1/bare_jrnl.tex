%% bare_jrnl.tex
%% V1.4a
%% 2014/09/17
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8a or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_conf_compsoc.tex,
%%                    bare_jrnl_compsoc.tex, bare_jrnl_transmag.tex
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices and paper sizes can       ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{hyperref}
\usepackage{multirow}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{cuted}
\usepackage{fancyhdr}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{A novel dual skip connection mechanism in U-Nets for building footprint extraction}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Bipul~Neupane,
        Jagannath~Aryal,~\IEEEmembership{Member,~IEEE,}
        and~Abbas~Rajabifard% <-this % stops a space
%\thanks{Manuscript received January XX, 2023; revised XXX, XXX.}
%\thanks{This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.}
\thanks{All authors are from the Department of Infrastructure Engineering, Faculty of Engineering and IT, The University of Melbourne, Melbourne, VIC 3010, Australia. Bipul Neupane is supported by the University of Melbourne for his PhD research and is awarded by Melbourne Research Scholarship. Corresponding author: Bipul Neupane (e-mail: bneupane@student.unimelb.edu.au).}% <-this % stops a space
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.

%%%%%%
%%%%%%
%%   IEEE arxiv policy
%%    https://journals.ieeeauthorcenter.ieee.org/become-an-ieee-journal-author/publishing-ethics/guidelines-and-policies/post-publication-policies/#preprint

% The paper headers
%\markboth{IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,~Vol.~XX, No.~XX, December~2022}%
%{Neupane \MakeLowercase{\textit{et al.}}: A novel dual skip connection mechanism in U-Nets for building footprint extraction}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2014 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}


\fancyhead{}
\fancyfoot{}
% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
The importance of building footprints and their inventory has been recognised as an enabler for multiple societal problems. Extracting urban building footprint is complex and requires semantic segmentation of very high-resolution (VHR) earth observation (EO) images. U-Net is a common deep learning architecture for such segmentation. It has seen several re-incarnation including U-Net++ and U-Net3+ with a focus on multi-scale feature aggregation with re-designed skip connections. However, the exploitation of multi-scale information is still evolving. In this paper, we propose a dual skip connection mechanism (DSCM) for U-Net and a dual full-scale skip connection mechanism (DFSCM) for U-Net3+. The DSCM in U-Net doubles the features in the encoder and passes them to the decoder for precise localisation. Similarly, the DFSCM incorporates increased low-level context information with high-level semantics from feature maps in different scales. The DSCM is further tested in ResUnet and different scales of U-Net. The proposed mechanisms, therefore, produce several novel networks that are evaluated in a benchmark WHU building dataset and a multi-resolution dataset that we develop for the City of Melbourne. The results on the benchmark dataset demonstrate 17.7\% and 18.4\% gain in F1 score and Intersection over Union (IoU) compared to the state-of-the-art vanilla U-Net3+. In the same experimental setup, DSCM on U-Net and ResUnet provides a gain in five accuracy measures against the original networks. The codes will be available in a GitHub link after peer review.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
building footprint extraction, DS-UNET, DS-ResUnet, DS-Unet3+, dual skip connection, multi-scale feature aggregation.
\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction} \label{sec:intro}
\IEEEPARstart{S}{emantic} segmentation of earth observation (EO) images is the current state-of-the-art (SOTA) method for urban feature extraction and land use and land cover classification (LULC) \cite{neupane2021deep}. The evolving nature of semantic segmentation has seen traditional classifiers (e.g. threshold-based method \cite{otsu1979threshold}, region growing \cite{adams1994seeded}, edge detection \cite{canny1986computational}), and machine learning methods such as random forest \cite{du2015semantic} and support vector machine \cite{huang2012svm} in remote sensing. These methods lack precision due to their dependency on mid-level semantic characteristics (hand-crafted features). 

An accurate and precise extraction of complex urban features such as building footprints is only possible due to the availability of very high-resolution (VHR) EO images. Semantic segmentation of VHR images for building footprint extraction, in recent years, is performed using convolutional neural networks (CNNs) \cite{lecun1989backpropagation} and fully convolutional neural networks (FCNs) \cite{long2015fully}. These neural networks allow fast and automatic feature extraction from an adequately large dataset with an increased number of ``layers'' to reduce classification errors in regression. An encoder-decoder architecture with a CNN as a fundamental backbone has become the common structural configuration for semantic segmentation \cite{minaee2021image}. U-Net \cite{ronneberger2015u} is the most common example. An encoder is essentially a CNN that generates feature maps of different scales and sizes using convolutions and down-sampling operations. These maps comprise low-level and fine-grained information. A decoder is generally symmetrical to an encoder with up-sampling operations. The decoder comprises high-level and coarse-grained semantic information. A bridge called ``skip connections'' pass the low-level information from the encoder layers to the decoder layers to make the utmost use of the encoder-decoder structure.

Context information and precise localisation are fundamental for urban feature extraction from EO images. The popularity of U-Net comes from a trade-off between the use of context information (generated by the encoders) and precise localisation (in the layers of the decoder) for precise segmentation. However, the problem with U-Net is that the context information is lost because of the down-sampling operations that reduce the size of the feature maps. To address the issue, U-Net++ \cite{zhou2019unet++} replaces the plain skip connections with nested and dense skip connections. These dense connections with rich information and reduced inter-encoder-decoder semantic gaps increased the precision of segmentation while increasing the network parameters. Both U-Net and U-Net++ fail to make full use of multi-scale feature information. The more recent iteration of U-Net called U-Net3+ \cite{huang2020unet} realises the problem and introduces full-scale skip connections. These connections re-design the inter-encoder-decoder plain skip connections and also the intra-connections between the decoder layers to capture fine-grained details and coarse-grained semantics from full scales. The full-scale skip connections assume equal weights among the feature maps generated at different scales. However, different scales of feature maps possess different levels of discrimination and context information, which is not realised in U-Net3+. To find a more effective trade-off between context and localisation, we propose a dual skip connection mechanism (DSCM) for U-Net. Similarly, we propose a dual full-scale skip connection mechanism (DFSCM) in U-Net3+ with a multi-scale feature aggregation technique that aggregates the feature maps of different scales with increased weights for the smaller-scale feature maps.

The proposed DSCM in U-Net compensates the lost context features with increased features in the encoder of U-Net and enriches the plain skip connections with doubled feature maps. Our previous work \cite{neupane2022building} shows the increment of large-scale feature maps increases the performance of U-Net. We add DSCM to the feature-increased U-Net to pass the doubled feature maps from the encoder to the decoder. The doubled feature maps allow the decoder to retain the boundary from increased feature vectors. We study the effects of DSCM on large, small, and all scales of feature maps generated in the encoder of U-Net. The study, therefore, proposes three novel architectures based on DSCM. The concept of dual skip connection is further integrated into U-Net3+ with a new multi-scale feature aggregation technique. The aggregation allows increasing the number of features at small-scale layers of U-net3+. The context information is higher in small-scale layers compared to large-scale layers because the downsampling operations reduce the size of feature maps as the scale increases. Different multi-scale feature aggregation techniques are performed to increase the context information in EO-based urban feature extraction \cite{wu2018automatic,wei2019toward,ji2019scale} from the literature. Further, the DSCM is also tested for another widely used architecture of ResUnet \cite{zhang2018road}. The performance of all the proposed new networks is compared to the vanilla versions of U-Net, U-Net3+, ResUnet and SOTA networks like U-Net++ \cite{zhou2019unet++}, Deeplabv3+ \cite{chen2017deeplab}, and SegNet \cite{badrinarayanan2017segnet}. The proposed networks seek efficiency gains in terms of five different accuracy measures without significantly increasing the network parameters. Two benchmark datasets of VHR and high-resolution types are used for the experiment. Further, a multi-resolution building dataset is developed to test the robustness of the networks in different spatial resolutions. The major contributions of this paper are: 

\begin{enumerate}
    \setcounter{enumi}{0}
    \item We design a dual skip connection mechanism (DSCM) for U-Net to find an effective trade-off between the use of context and precise localisation of building footprints from VHR and high-resolution images. We further test the mechanism in ResUnet.
    \item We design a dual full-scale skip connection mechanism (DFSCM) for U-Net3+ with a new multi-scale feature aggregation technique. Unlike in U-Net3+, the feature maps of different scales are aggregated with increased weights for the smaller-scale feature maps, where the context information is intact.
    \item A multi-resolution building dataset is developed for a comprehensive robustness check on different spatial resolutions of EO images.
\end{enumerate}


\section{Related Work} \label{sec:relw}

\subsection{CNNs, FCNs and encoder-decoder networks} \label{sec:relw:CNNs}
The earliest forms of CNN are AlexNet \cite{krizhevsky2012imagenet}, VGGNet \cite{simonyan2014very}, GoogleNet \cite{szegedy2015going}, ResNet \cite{he2016deep}, Xception \cite{chollet2017xception}, and RefineNet \cite{lin2017refinenet}. They are now used to extract multi-scale feature information from images \cite{chen2018aerial,ayala2021deep} for segmentation purposes in FCNs and encoder-decoder FCNs like SegNet \cite{badrinarayanan2017segnet} and U-Net \cite{ronneberger2015u}. Some of the earliest studies of CNN-based building footprint extraction \cite{mnih2013machine,saito2015building,saito2016multiple,vakalopoulou2015building,marcu2017object} perform patch-based segmentation supported by post-processing methods to improve accuracy and precision. CNN-based pixel-level building segmentation is achieved by Mask R-CNN that uses CNNs like ResNet as feature extractor (aka. backbone) \cite{zhao2018building,griffiths2019improving}. Similarly, building footprint extraction has also seen different versions of FCN (FCN-2s, FCN-4s, and FCN-8s) \cite{zhong2016fully,maggiori2016convolutional,yang2018building,qin2019semantic}. 

The early form of encoder-decoder networks (SegNet and U-Net) are first introduced to segment indoor/outdoor scenes and medical images. They followed their way to EO images including multi-spectral images from WorldView-3 \cite{li2019semantic}, synthetic aperture radar (SAR) and multi-spectral images from Sentinel-1 and 2 \cite{ayala2021deep} for building segmentation. Some have also achieved building classification \cite{pan2020deep}. SegNet is also experimented with further modifications \cite{bischke2019multi,sariturk5feature} and in combination with U-Net to form Seg-Unet \cite{abdollahi2020ensemble}. Similarly, ResUnet \cite{zhang2018road} is introduced to extract road networks with residual units added to U-Net to ease the training and further enrich the skip connections. ResUnet is further experimented for building segmentation with multi-modal hand-crafted features \cite{xu2018building} and ultra-high resolution (UHR) images \cite{li2019semantic}. Due to its legacy in the domain of computer vision, widely used versions of the U-Net family are the focus of this paper.

\subsection{The legacy and evolution of U-Net} \label{sec:relw:unet}
The network architecture of U-Net \cite{ronneberger2015u} addresses the loss of information in the CNNs due to (i) heavy use of pooling and max-pooling operations, and (ii) the imbalanced trade-off between localisation and context information. U-Net builds upon FCN and designs an encoder-decoder structure. The encoder half of the network (contracting path) is a usual CNN network that uses pooling operations to build a stack of high-resolution feature maps with context information at each layer. The encoder is supplemented with a decoder half (expanding path) with successive layers. The pooling operations are replaced by upsampling operations to increase the resolution of the output. The precision of localisation is improved by concatenating the high-resolution features from the encoder to the upsampled output in the corresponding layer of the decoder. The means to transport the features from the encoder layers to the decoder layers are called ``skip connections''. The network produces the final segmentation map with pixels from the context available in the input image, replacing the fully connected layers used in the FCNs. The missing contexts are extrapolated by mirroring the input image to predict the pixels in the border region and the training samples are minimised by heavy use of augmentation.

U-Net has seen an explosion in usage in medical imaging since its introduction and several variants have been proposed in this domain \cite{siddique2021u}. Among many, some of the successful variants are 3D U-Net \cite{cciccek20163d}, V-Net \cite{milletari2016v} attention U-Net \cite{oktay2018attention}, U-Net++ \cite{zhou2019unet++}, R2U-Net \cite{alom2018recurrent}, Inception-U-Net \cite{zhang2020dense}, ResUnet \cite{zhang2018road}, Dense U-Net \cite{wang2019dense}, adversarial U-Net\cite{schonfeld2020u}, U$^2$Net \cite{qin2020u2}, and U-Net 3+ \cite{huang2020unet}. These U-Nets are widely used to segment both medical and EO images. The recent advancement in U-Net seems also to be affected by the rise of Vision Transformer (ViT) \cite{dosovitskiy2020image}, which brings the science of Transformer \cite{vaswani2017attention} from natural language processing to a computer vision problem. The concept of ViT tries to replace CNNs in semantic segmentation but has still not been able to fully replace it. The adaptation of ViT in the recent U-Net versions include TransUnet \cite{chen2021transunet}, TransFuse \cite{zhang2021transfuse}, and Swin-Unet \cite{cao2021swin}. TransUnet builds upon the problem of CNNs failing to fully learn the global and remote semantic information interaction because of the involvement of the convolutional process. The prior concepts of feature pyramid \cite{lin2017feature}, DeepLab \cite{chen2017deeplab}, atrous convolution layers \cite{chen2018encoder}, context encoder network (CE-Net) \cite{gu2019net}, self-attention \cite{wang2018non}, and attention gate \cite{schlemper2019attention} also have tried to address this problem. The most recent development of O-Net \cite{wang2022net} realises that adding a self-attention mechanism on the transformer and combining it with CNN can improve upon the dice-coefficient metric by marginal numbers when compared to already computational-heavy networks based on ViT. The improvement in performance is vitally important to computer vision but raises some serious questions regarding the computation expense. Unlike computationally super-expensive Transformer-based semantic segmentation to address the problem of inadequate learning of global and remote semantic information, some other advancements in U-Net variants seek to lower the number of network parameters in U-Net while improving the performance of U-Net. Our focus is on those computationally effective variants of U-Nets.

\subsection{Re-designing skip connections} \label{sec:relw:skip}
The original U-Net passes the features from the encoder layers to the decoder layers through skip connections and concatenates them to the upsampled outputs. The recent variants of U-Net like NAS-UNet \cite{weng2019unet}, U-Net3+ \cite{huang2020unet}, DC-UNet \cite{lou2021dc}, and Half-UNet \cite{lu2022half} re-design skip connections in U-Net while reducing the number of network parameters. 

U-Net3+ aggregates feature maps of all scales with the concept of full-skip connections with similar or increased performance over U-Net. This re-iteration of skip connection is an advancement over the previously introduced concept of nested and dense skip connections in U-Net++ to capture both fine- and coarse-grained semantics from full scales. As an example, the third scale layer in the decoder of U-Net3+ receives the feature maps of (i) the same-scale encoder layer similar to U-Net, (ii) smaller-scale encoder layers carrying lower-level (coarse-grained) detailed information through inter encoder-decoder skip connections, and (iii) larger-scale decoder layers carrying high-level (fine-grained) semantic information through intra-decoder connections. The number of channels in all incoming five same-resolution feature maps is unified. A feature aggregation mechanism is applied on the concatenated maps of five scales. The full-scale skip connections and feature aggregation provide improvement in dice-coefficient while also lowering the network parameters when compared to U-Net and U-Net++ with VGG-16 and ResNet-101 as encoders.

U-Net and U-Net3+ rely on concatenation to aggregate the multi-scale feature maps. Half-UNet \cite{lu2022half} aggregates them using an addition operation inspired by ResNet to increase the amount of information without increasing the dimension. This allows it to directly obtain final image segmentation without large-scale decoder layers of U-Net. This heavily reduces the network parameters compared to U-Net3+ (0.21M Vs. 26.97M) with similar performance. The experiments also show the lower performance of U-Net3+ against U-Net in some of the datasets. In a similar attempt to subtract the components of U-Net, Fu et al. \cite{fu2021keep} design ``additive'' and ``subtractive'' variants of U-Net. The additive variants add a dense block, residual block, side-output block, and dilated convolution block to the U-Net and the subtractive variants: U-Net without Rectified Linear Units (ReLU) layers, U-Net without skip connections, and U-Net with one convolutional layer per level. The experiments show a dice score ranging from 79\% to 81\% among all variants in four datasets. The subtractive variants show lower dice-score of up to 74\% while it ranged from 79\% to 81\% for the additive variants. Without skip connections, U-Net is less competitive in their experiment.

In summary, U-Net3+ have changed the terminology of ``skip connections'' to ``plain skip connections'' with the realisation of full-scale skip connections. The redesigning of skip connections is ongoing research in many studies to reduce the network parameters while increasing the accuracy and precision at the same time. In this paper, we propose a dual skip connection mechanism to enrich the plain skip connections of U-Net and ResUnet and the full-scale skip connections of U-Net3+.

\section{Method} \label{sec:method}
\subsection{Dual skip connection mechanism (DSCM) for U-Net} \label{sec:meth:arch}
To explain the dual skip connection mechanism (DSCM), we take in a U-Net of five scale layers. The encoder $En$ consecutively passes the learned feature maps from layers $X_{En}^{1}$ to $X_{En}^{5}$. A layer of encoder $X_{En}^{n}$ consist of two recurring unpadded 3x3 convolutions $\mathcal{C}(\cdot)$, a batch normalization (BN), and a ReLU $\sigma(\cdot)$. The features are down-scaled with operation $\mathcal{D}(\cdot)$ of max-pooling with stride 2 from $X_{En}^{n}$ to $X_{En}^{n+1}$. The decoder upsamples the low-resolution feature maps of the encoder from layers $X_{De}^{4}$ to $X_{De}^{1}$. A layer of decoder $X_{De}^{n}$ starts with an up-sampling operation $\mathcal{U}(\cdot)$ of a 2x2 ``up-convolution'' that halves the number of feature channels from the previous layer. The output of $\mathcal{U}(\cdot)$ is concatenated to the feature map of the corresponding encoder layer $X_{En}^{n}$, followed by a 3x3 convolution $\mathcal{C}(\cdot)$. A 1x1 convolution in the final layer maps the 64-component feature vectors to the number of classes. This general 5-scale U-Net is illustrated in Figure \ref{fig:dualskipconnection}(a). With this process, U-Net utilises both low- and high-resolution features, conserving the spatial integrity of objects that is crucial in the semantic segmentation of features in EO data. 

However, the problem with this architecture of U-Net lies in the down-scaling as $\mathcal{D}(\cdot)$ increases the scale while halving the dimension of the feature maps, thus losing the context information at each $\mathcal{D}(\cdot)$ operation. We increase the context information in $X_{En}^{n}$ by adding one more layer of 3x3 convolution layer with BN and $\sigma(\cdot)$ and pass the feature maps from the added and the previous convolution layers using the proposed dual skip connection mechanism (DSCM). A plain skip connection $\mathcal{S}(\cdot)$ in U-Net passes the output of BN and $\sigma(\cdot)$ on the second $\mathcal{C}(\cdot)$ layer of $X_{En}^{n}$ to be concatenated with the output of $\mathcal{U}(\cdot)$ in $X_{De}^{n}$. In DSCM, in addition to the $\mathcal{S}(\cdot)$, a second skip connection passes the output of the third $\mathcal{C}(\cdot)$ layer of $X_{En}^{n}$ to be concatenated with the output of $\mathcal{U}(\cdot)$ in $X_{De}^{n}$. The features from both skip connections are concatenated to the $\mathcal{U}(\cdot)$ layer. Similar to the U-Net, the final output of the second $\mathcal{C}(\cdot)$ layer is down-scaled from $X_{En}^{n}$. Figure \ref{fig:dualskipconnection} illustrates the proposed DSCM.

  \begin{figure*}[!h]
  \centering
  \includegraphics[width=18cm]{images/neupa1.png}
  \caption{Illustration of the plain skip connections in U-Net and the proposed dual skip connection mechanism (DSCM).}
      \label{fig:dualskipconnection}
      \end{figure*}

\subsection{DSCM in different scale features of U-Net} \label{sec:meth:dualskip}
To study the effects of DSCM on different scales of fine-grained detailed information and coarse-grained semantic information, we propose three novel U-Net network architectures: dual skip on large-scale features (DS-UNET-l), dual skip on small-scale features (DS-UNET-s), and dual skip on all scale features (DS-UNET-a). Figure \ref{fig:ds-unets} illustrates the three networks with DSCM.

\begin{enumerate}
    \setcounter{enumi}{0}
    \item \textit{DS-UNET-l} applies DSCM between the large-scale layers of encoder $X_{En}^{n}$ and decoder $X_{De}^{n}$ for $n\in [3,4]$.
    \item \textit{DS-UNET-s} applies DSCM between the small-scale layers of encoder $X_{En}^{n}$ and decoder $X_{De}^{n}$ for $n\in [1,2]$.
    \item \textit{DS-UNET-a} applies DSCM between the four scales layers of encoder $X_{En}^{n}$ and decoder $X_{De}^{n}$ for $n\in [1,2,3,4]$.
\end{enumerate}

  \begin{figure*}[!h]
  \centering
  \includegraphics[width=18cm]{images/neupa2.png}
  \caption{Illustration of the proposed dual skip connection mechanism (DSCM) on different scale layers of U-Net to form DS-UNET-l, DS-UNET-s, and DS-UNET-a.}
      \label{fig:ds-unets}
      \end{figure*}

After a comprehensive study of DSCM on different scales of U-Net, we also test the efficiency gain from DSCM on another popular variant of U-Net called ResUnet as explained next.
      
\subsection{Dual skip connections in ResUnet} \label{sec:meth:dsresunet}
The experiments from DS-UNETs show the highest efficiency gain from DS-UNET-l. Following the conclusion of the experiment, DSCM is applied to the large-scale features of ResUnet ($X_{En}^{3}$ and $X_{En}^{4}$). The DSCM is similar to that in DS-UNET-l, but this time the number of features is kept the same in the encoder, however, more features are passed to the decoder with DSCM. This means that no convolution is added to increase the number of feature maps in the encoder layers. The DSCM passes the output features of the two existing convolutions in the encoder layers of ResUnet. This is done to avoid a significant increase in the number of network parameters, while also avoiding increased complexity in the network. Figure \ref{fig:dsresunet} illustrates the proposed DS-ResUnet-l and shows the difference between the plain skip connections on ResUnet and DSCM on DS-ResUnet-l.

  \begin{figure*}[!h]
  \centering
  \includegraphics[width=18cm]{images/neupa3.png}
  \caption{Illustration of the proposed DS-ResUnet-l and the differences between the plain skip connections in ResUnet and DSCM in ResUnet.}
      \label{fig:dsresunet}
      \end{figure*}
      
\subsection{Dual full-scale skip connection mechanism (DFSCM) for U-Net3+} \label{sec:meth:dsunet3+}
In addition to different DS-UNETs and DS-ResUnet-l, we also propose a dual skip U-Net3+ with full-scale skip connections. The full-skip connections allow U-Net3+ \cite{huang2020unet} to aggregate the feature maps of all scales to capture both fine- and coarse-grained semantics. As an example from U-Net3+, $X_{De}^{3}$ receives the feature maps of (i) same-scale encoder layer $X_{En}^{3}$ similar to U-Net, (ii) smaller-scale encoder layer $X_{En}^{2}$ and $X_{En}^{1}$ carrying coarse-grained detailed information through inter encoder-decoder skip connections supported by non-overlapping max pooling operations, and (iii) larger-scale decoder layer $X_{De}^{4}$ and $X_{De}^{5}$ carrying fine-grained semantic information through intra-decoder connections supported by bilinear interpolation. The number of channels in all incoming five same-resolution feature maps is unified with 64 filters of 3x3 size. A feature aggregation mechanism is applied on the concatenated maps of five scales that consist of 320 filters of 3x3 size, a batch normalization, and a ReLU activation function. The feature aggregation mechanism in U-Net3+ assumes equal weights for all feature maps at different scales. However, the different scales of feature maps possess different levels of discrimination. The context information is more intact in the small-scale feature maps as the large-scale features get smaller in size and lose them because of the downsampling operations.

We integrate the concepts of DSCM and full-scale skip connections from U-Net3+ to propose a dual full-scale skip connection mechanism (DFSCM) for U-Net3+ (abbr. DS-Unet3+). The difference in U-Net3+ and DS-Unet3+ is illustrated in Figure \ref{fig:dsunet3plus}. The skip connections are similar to those of U-Net3+, except there are two inter-encoder-decoder skip connections. To support the two connections, we develop a dual skip feature aggregation mechanism (abbr. DSFAM). The DSFAM consists of a different number of filters when compared to U-Net3+. As an example, Figure \ref{fig:dsunet3plus-De3} illustrates the construction of feature maps in the third scale layer of the decoder of DS-Unet3+. The $X_{De}^{3}$ receives the feature maps of same-scale encoder layer $X_{En}^{3}$ with two plain skip connections. The feature maps of smaller-scale encoder layer $X_{En}^{2}$ and $X_{En}^{1}$ carrying coarse-grained detailed information are transported through two inter encoder-decoder skip connections supported by down-scaling operation $\mathcal{D}(\cdot)$ of non-overlapping max pooling operations. Finally, the feature maps from larger-scale decoder layer $X_{De}^{4}$ and $X_{De}^{5}$ carrying fine-grained semantic information are transported through intra-decoder connections supported by upsampling operation $\mathcal{U}(\cdot)$ of bilinear interpolation. Similar to U-Net3+, the number of channels in all incoming same-resolution feature maps is unified with 64 filters of 3x3 size. Unlike U-Net3+, the aggregation mechanism concatenates the maps of 9, 8, 7, and 6 scales with a total of 576, 512, 448, and 384 filters of 3x3 size on decoder layers $X_{De}^{4}$, $X_{De}^{3}$, $X_{De}^{2}$, and $X_{De}^{1}$ respectively. In addition to the batch normalization and ReLU activation function, a dropout of 50\% is added to prevent over-fitting. The overall procedure for $i^{th}$ down-sampling layer of the encoder can be formally denoted as (\ref{eqn:DSUnet3plus}).

\begin{strip}
\begin{equation}
X_{De}^{i}=\mathrm{\Bigg\{}_{i=1,\dots,N-1}^{X_{En}^{i},i=N}\mathcal{DS} { \Bigg(\bigg[ 
\underbrace{{\mathcal{C}\big(\mathcal{D}(X_{En}^{k})\big)}_{k=1}^{i-1}, \mathcal{C}(X_{En}^{i})}_{Scales: \ 1^{th}-i^{th}},  
\underbrace{{\mathcal{C}\big(\mathcal{U}(X_{De}^{k})\big)}_{k=i+1}^{N}}_{Scales: \ (i+1)^{th}-N^{th}}
\bigg]\Bigg) }
\label{eqn:DSUnet3plus}
\end{equation}
\end{strip}

where $\mathcal{C}(\cdot)$, $\mathcal{D}(\cdot)$, and $\mathcal{U}(\cdot)$ denotes convolution, down-sampling, and up-sampling operations. $\mathcal{DS}(\cdot)$ denotes the dual skip feature aggregation mechanism with convolution followed by batch normalization and a ReLU activation. The proposed DS-Unet3+ consists of fewer network parameters than U-Net and the three versions of DS-UNETs.

  \begin{figure*}[!hbt]
  \centering
  \includegraphics[width=14cm]{images/neupa4.png}
  \caption{Illustration of the U-Net3+ and DS-Unet3+. (a) Full-scale skip connections of U-Net3+ \cite{huang2020unet}. (b) DFSCM in DS-Unet3+}
      \label{fig:dsunet3plus}
      \end{figure*}
      
  \begin{figure}[!hbt]
  \centering
  \includegraphics[width=8cm]{images/neupa5.png}
  \caption{Illustration of dual skip feature aggregation mechanism (abbr. DSFAM) at the third decoder layer $X_{De}^{3}$ of DS-Unet3+ (figure adapted and modified from \cite{huang2020unet}).}
      \label{fig:dsunet3plus-De3}
      \end{figure}


\section{Dataset and Experimental Setup} \label{sec:experiments}

\subsection{Datasets} \label{sec:exp:data}
We experiment on two benchmark datasets and two newly developed building footprint datasets. The benchmark datasets are of very-high-resolution (VHR) and high-resolution types. The newly developed datasets are of multi-resolution and high-resolution types.

The first dataset is the VHR WHU Building dataset (abbr. WHU). It includes satellite images of Christchurch, New Zealand with a spatial resolution of 0.3m as generated by Ji et al. in \cite{ji2018fully}. To maintain uniform image size between the datasets that we experiment on, the 512x512 sized-image samples of the WHU dataset are tiled into 256x256. The training and validation samples of 23088 and 9664 tiles are prepared.

The second dataset is the high-resolution (1m) Massachusetts Building dataset \cite{mnih2013machine}. The original 1500x1500 tiles are cropped to 256x256 by generating a grid of coordinates. The partial tiles on the edges are ignored, only iterating through the Cartesian product between the two intervals, i.e. $range(0, h-h\%d, d) \times range(0,w-w\%d, d)$ where, $w$, $h$, and $d$ are the width, height, and output tile size of 256 respectively. The validation images provided are used for validation.

The third dataset that we develop (label, image) is named as Melbourne building footprint dataset (abbr. MELB), which is an upgrade to the datasets that we proposed in our previous work \cite{neupane2022building}. It comprises building footprint samples of Melbourne, Australia. The labels are developed by masking and tiling the building roof samples provided by the City of Melbourne. The corresponding image tiles of 0.3m, 0.6m, and 1.2m spatial resolution of the labels are collected from Nearmap's API service. The number of training and validation samples are divided as 70\% and 30\% respectively. 

The fourth dataset is a 1.2m subset of the MELB dataset, prepared to experiment with the proposed networks on high-resolution images.


\subsection{Comparison to state-of-the-art} \label{sec:exp:sota}
The proposed DS-UNETs, DS-ResUnet, and DS-Unet3+ are compared to the vanilla U-Net, ResUnet, and U-Net3+. Furthermore, the networks are also compared to U-Net++ \cite{zhou2019unet++}, Deeplabv3+ \cite{chen2018encoder}, and SegNet \cite{badrinarayanan2017segnet} with VGG-16 encoder. The encoder of the SegNet is chosen to be VGG-16 to keep the network parameters similar to the U-Net that we compare to. No components such as attention gates, deep supervision, and classification-guided module (CGM) are added to the vanilla version of the SOTA networks. For uniformity in experiments, the same loss function is used to evaluate all networks. To maximise the dice coefficient (aka. F1 score) and minimise the possible imbalance between the number of ``background'' and ``building pixels'', we use the dice loss \cite{sudre2017generalised} as the loss function for binary classification. This loss calculates the measure of overlap to assess the performance of segmentation when a ground truth (GT) is available. Dice loss is denoted by (\ref{eqn:diceloss}).

    \begin{equation}\label{eqn:diceloss}
        DL(y,\hat{p}) = 1 - \frac{2y\hat{p}+1} {y+\hat{p}+1} 
    \end{equation}
    
where $y$ and $\hat{p}$ represent the GT and prediction respectively. The smooth value of 1 is added in the numerator and denominator to make sure that the function is defined in the case of $y=\hat{p}=0$, which is called an edge case scenario. In the dice loss, the product of $y$ and $\hat{p}$ represents the intersection between the GT and prediction. In other words, dice loss is the negative of the dice coefficient that we use as one of our accuracy measures.

\subsection{Implementation details} \label{sec:exp:train}
All DL networks are wrapped in the Keras framework with a mini-batch size of 2. The step/epoch is set as the ratio of the number of training images to the batch size. The epoch is set such that the total number of steps is kept the same or similar (approx. 60000). A learning rate of 1e-4 is used to train all the networks. \textit{Adam}, \textit{He Normal}, and \textit{Rectified Linear Units (ReLU)} are the optimizer, initializer, and activation functions respectively. A \textit{sigmoid} function is used to obtain the final output maps as the dataset is binary, and a \textit{dropout} of 50\% is used to avoid over-fitting. A dice-coefficient loss is used to monitor the models. All the hyper-parameters are kept the same among all models used for comparison, except in the SegNet, an \textit{argmax} function is used for pooling unlike the max-pooling in other models.

\subsection{Evaluation Metrics} \label{sec:exp:eval}
The evaluation of the networks is performed using (i) pixel accuracy, (ii) adjusted accuracy, (iii) F1 score, (iv) intersection over union (IoU), and (v) Matthews correlation coefficient (MCC). Pixel accuracy (\ref{eqn:pixelacc}) measures how often the predictions and the binary labels match. Adjusted accuracy (\ref{eqn:avgacc}) takes the average of Sensitivity (\ref{eqn:sensitivity}) and specificity (\ref{eqn:specificity}), which measures the proportion of correctly identified actual positives and actual negatives respectively. F1 score (\ref{eqn:f1score}) and IoU (\ref{eqn:iou}) are measured from the `area of overlap' between prediction and binary labels and `area of union' (all of the predictions + binary labels - the overlap). Lastly, a measure of the difference between the binary labels and the prediction with consideration of the ratio between positive and negative elements is calculated by MCC (\ref{eqn:mcc}) \cite{chicco2020advantages}. The symbolic representation of the metrics are:

    \begin{equation}\label{eqn:pixelacc}
        Pixel\ accuracy\ (PA) = \frac{TP + TN} {TP + TN + FP + FN} 
    \end{equation} 
    
    \begin{equation}\label{eqn:avgacc}
        Adjusted\ accuracy\ (AA) = \frac{Sensitivity + Specificity} {2} 
    \end{equation} 
    
    \begin{equation}\label{eqn:sensitivity}
        Sensitivity = \frac{TP} {TP + FN}
    \end{equation}
    
    \begin{equation}\label{eqn:specificity}
        Specificity = 1 - \frac{TN} {TN + FP}
    \end{equation}

    \begin{equation}\label{eqn:f1score}
        F1 score = \frac{2 \times TP} {2 \times TP + FN + FP} 
    \end{equation}
    
    \begin{equation}\label{eqn:iou}
        IoU = \frac{TP} {TP + FN + FP}
    \end{equation}
    
    \begin{equation}\label{eqn:mcc}
        MCC = \frac{(TP \times TN) - (FP \times FN)} {\sqrt{(TP + FP) (TP + FN) (TN + FP ) (TN + FN)}}
    \end{equation}
    
where, TP is true positive (i.e. prediction = 1, label = 1); FP is false positive (prediction = 1, label = 0); FN is false negative (prediction = 0, label = 1); and TN is true negative (prediction = 0, label = 0). Other than the five accuracy metrics, the number of network parameters (abbr. Par.) are also compared.


\section{Results and Discussion} \label{sec:res}
The results and comparisons from the experiments are presented for three experimental settings categorised based on spatial resolutions: VHR, high-resolution, and multi-resolution. The proposed networks are compared to the vanilla versions of U-Net, ResUnet, U-Net3+, U-Net++, Deeplabv3+, and SegNet. Further, the DSCM is studied at different scales of DS-UNETs. The limitations of the experiments and future direction finalise the discussion.

\subsection{Results on VHR building dataset (WHU)} \label{sec:res:VHR}
Table \ref{tab:res:VHR} presents the performance of three versions of DS-UNETs, DS-ResUnet-l, and DS-Unet3+ on the VHR benchmark WHU building dataset. The proposed networks are compared to their original architectures of vanilla U-Net, ResUnet, and U-Net3+, and also to U-Net++, Deeplabv3+, and SegNet. Figure \ref{fig:WHU-results} shows the sample results from all the proposed and vanilla networks on the WHU dataset. 

\begin{table}[]
\centering
\caption{DS-UNETs, DS-ResUnet-l, and DS-Unet3+ on VHR WHU building dataset.}
\label{tab:res:VHR}
\centering
\begin{tabular}{lcccccc}
\hline
Networks & Par. (M) & PA & AA & F1 & IoU & MCC \\ \hline
U-Net & 31.041 & 0.973 & 0.826 & 0.844 & 0.770 & 0.681 \\
DS-UNET-l & 36.943 & 0.976 & 0.851 & 0.865 & 0.799 & 0.701 \\
DS-UNET-s & 31.410 & 0.973 & 0.849 & 0.853 & 0.781 & 0.683 \\
DS-UNET-a & 37.312 & 0.975 & 0.837 & 0.848 & 0.777 & 0.691 \\ \hline
U-Net3+ & 22.891 & 0.981 & 0.855 & 0.688 & 0.615 & 0.723 \\
DS-Unet3+ & 27.359 & 0.976 & 0.850 & 0.863 & 0.795 & 0.697 \\ \hline
ResUnet & 75.346 & 0.969 & 0.833 & 0.822 & 0.741 & 0.665 \\
DS-ResUnet-l & 81.906 & 0.975 & 0.843 & 0.847 & 0.775 & 0.690 \\ \hline
U-Net++ & 34.538 & 0.974 & 0.828 & 0.823 & 0.749 & 0.685 \\
Deeplabv3+ & 11.852 & 0.975 & 0.854 & 0.852 & 0.784 & 0.696 \\
SegNet & 29.458 & 0.969 & 0.831 & 0.666 & 0.584 & 0.673 \\ \hline
\end{tabular}
\end{table}

  \begin{figure*}[!hbt]
  \centering
  \includegraphics[width=18cm]{images/neupa6.png}
  \caption{Segmentation output from the proposed DS-UNET-l, DS-UNET-s, DS-UNET-a, DS-ResUnet-l, and DS-Unet3+ and their comparison to the original vanilla networks of U-Net, ResUnet, and U-Net3+ on VHR WHU Building dataset.}
      \label{fig:WHU-results}
      \end{figure*}

\subsubsection{DSCM on large- and small-scale features of U-Net (DS-UNETs)} \label{sec:res:VHR:dsunets}
All three versions of the proposed DS-UNETs outperform U-Net on the WHU dataset in terms of all accuracy measures as shown in Table \ref{tab:res:VHR}. DS-UNET-l shows the highest performance among the three versions and also outperforms all compared SOTA networks. DS-UNET-s outperforms DS-UNET-a in terms of adjacent accuracy, F1 score, and IoU. The results on the VHR dataset show that concatenating the doubled large-scale features in U-Net with DSCM results in the highest performance with about a 14\% increase in network parameters. The small-scale features can be kept the same without being doubled as seen from DS-UNET-a. This supports our initial argument that the trade-off between the use of context and precise location can be improved in U-Net. This can be done by increasing the large-scale feature maps, where the contexts are lost due to down-sampling operations such as max-pooling.

\subsubsection{DSCM on each scale of encoder in U-Net} \label{sec:res:indiscale}
The experiment of the large- and small-scale features is further broken down into individual scales. For this experiment, we derive four DS-UNETs with DSCM between individual scales of features: DS-UNET-1, DS-UNET-2, DS-UNET-3, and DS-UNET-4. DS-UNET-1 represents DSCM between $X_{En}^{1}$ and $X_{De}^{1}$. Similarly, the DS-UNET-2, DS-UNET-3, and DS-UNET-4 represent the integration of DSCM between the encoder and decoder layers of the second, third, and fourth scale layers. The performance of the four DS-UNETs is shown in Table \ref{tab:res:indiscale}. The results show that the performance increase as the DSCM is applied from the smallest scale layers in DS-UNET-1 to the largest scale layers in DS-UNET-4. The performance gain over vanilla U-Net is obtained only at the fourth scale layer, which explains why DS-UNET-l outperforms U-Net with DSCM on the third and fourth scale layers.

\begin{table}[]
\centering
\caption{Proposed DSCM between each scale of encoder-decoder layers.}
\label{tab:res:indiscale}
\centering
\begin{tabular}{lcccccc}
\hline
Networks & Par. (M) & PA & AA & F1 & IoU & MCC \\ \hline
DS-UNET-1 & 31.114 & 0.977 & 0.842 & 0.674 & 0.597 & 0.707 \\
DS-UNET-2 & 31.336 & 0.982 & 0.853 & 0.690 & 0.616 & 0.724 \\
DS-UNET-3 & 32.221 & 0.972 & 0.827 & 0.833 & 0.758 & 0.680 \\
DS-UNET-4 & 35.761 & 0.976 & 0.844 & 0.855 & 0.786 & 0.693 \\ \hline
\end{tabular}
\end{table}

\subsubsection{DSCM on 4, 3, and 2 scale U-Nets} \label{sec:res:smallunets}
The previous experiments of DS-UNETs study DSCM on a vanilla U-Net of five scales (64, 128, 256, 512 and a bottleneck of 1024 scale). The experiments are further carried out on the smaller U-Nets with a lower number of scales: DS-UNET-4sc, DS-UNET-3sc, and DS-UNET-2sc. DS-UNET-4sc represents U-Net with an encoder of four scales (64, 128, 256 and a bottleneck of 512). Similarly, DS-UNET-3sc represents U-Net with an encoder of three scales (64, 128 and a bottleneck of 256). Finally, DS-UNET-2sc represents U-Net with an encoder of two scales (64 and a bottleneck of 128). The three DS-UNETs integrate DSCM on only the largest scale layer available in the network structure as our previous experiments show that the DSCM between the largest scale layers yields the highest performance gain. Table \ref{tab:res:smallscales} shows the comparison of the three DS-UNETs and their corresponding U-Nets. DSCM does not improve the accuracy measures of 2-scale and 3-scale U-Nets but improves those of 4-scale U-Net.

\begin{table}[]
\centering
\caption{Proposed DSCM on 4, 3, and 2 scale U-Nets.}
\label{tab:res:smallscales}
\centering
\begin{tabular}{lcccccc}
\hline
Networks & Par. (M) & PA & AA & F1 & IoU & MCC \\ \hline
UNet-2sc & 0.405 & 0.936 & 0.803 & 0.635 & 0.532 & 0.585 \\
DS-UNET-2sc & 0.479 & 0.944 & 0.788 & 0.625 & 0.519 & 0.579 \\ \hline
UNet-3sc & 1.865 & 0.957 & 0.846 & 0.756 & 0.672 & 0.665 \\
DS-UNET-3sc & 2.161 & 0.962 & 0.839 & 0.750 & 0.663 & 0.649 \\ \hline
UNet-4sc & 7.702 & 0.970 & 0.833 & 0.796 & 0.717 & 0.673 \\
DS-UNET-4sc & 8.883 & 0.969 & 0.839 & 0.817 & 0.738 & 0.675 \\ \hline
\end{tabular}
\end{table}

\subsubsection{DSCM on large-scale features of ResUnet (DS-ResUnet-l)} \label{sec:res:VHR:dsresunets}
With the improved results from DS-UNET-l, DSCM is applied to the large-scale layers of ResUnet. The proposed DS-ResUnet-l concatenates the feature maps from the two existing convolution operations in the large-scale encoder layers. The results show that DS-ResUnet-l outperforms ResUnet in all accuracy measures except the pixel accuracy with a slight increase in network parameters of approx. 8\%. The results are shown in Table \ref{tab:res:VHR}.

\subsubsection{DFSCM on U-Net3+ (DS-Unet3+)} \label{sec:res:VHR:dsunet3plus}
The results demonstrate the success of DSCM in plain skip connections in U-Net and ResUnet on the VHR dataset. However, the results from DFSCM on DS-Unet3+ show mixed performance when compared to U-Net3+. DS-Unet3+ significantly outperforms U-Net3+ in terms of F1 score (0.863 vs. 0.688) and IoU (0.795 vs. 0.615), while producing 1-2\% lower pixel accuracy (0.976 vs. 0.981), adjacent accuracy (0.850 vs. 0.855), and MCC (0.697 vs. 0.723). The significant increase in F1 score and IoU is worth noting with approx. 16\% increase and 12\% decrease in network parameters compared to U-Net3+ and U-Net respectively. The results are shown in Table \ref{tab:res:VHR}.


\subsection{Results on high-resolution dataset (Massachusetts)} \label{sec:res:massachusetts}
Table \ref{tab:res:mass} presents the results of all proposed, and SOTA networks on the high-resolution Massachusetts building dataset. All three DS-UNETs outperform U-Net, with the highest in all five metrics from DS-UNET-l. The accuracy measures of DS-ResUnet-l are higher than the vanilla ResUnet and DS-Unet3+ outperforms U-Net3+ in terms of F1 score and IoU. Figure \ref{fig:MASS-results} shows the segmentation results of all proposed and vanilla networks. All the proposed networks outperform the compared SOTA networks.

\begin{table}[]
\centering
\caption{DS-UNETs, DS-Unet3+, and DS-ResUnet-l on Massachusetts Dataset of 1m.}
\label{tab:res:mass}
\centering
\begin{tabular}{lccccc}
\hline
Networks & PA & AA & F1 & IoU & MCC \\ \hline
U-Net & 0.953 & 0.879 & 0.749 & 0.609 & 0.751 \\
DS-UNET-l & 0.952 & 0.892 & 0.756 & 0.618 & 0.760 \\
DS-UNET-s & 0.951 & 0.878 & 0.747 & 0.607 & 0.752 \\
DS-UNET-a & 0.951 & 0.889 & 0.752 & 0.612 & 0.754 \\ \hline
U-Net3+ & 0.953 & 0.889 & 0.757 & 0.617 & 0.760 \\
DS-Unet3+ & 0.952 & 0.887 & 0.750 & 0.611 & 0.754 \\ \hline
ResUnet & 0.949 & 0.880 & 0.770 & 0.633 & 0.744 \\
DS-ResUnet-l & 0.951 & 0.881 & 0.774 & 0.640 & 0.750 \\ \hline
U-Net++ & 0.945 & 0.858 & 0.741 & 0.596 & 0.712 \\
Deeplabv3+ & 0.942 & 0.861 & 0.733 & 0.586 & 0.705 \\
SegNet & 0.944 & 0.864 & 0.730 & 0.583 & 0.713 \\ \hline
\end{tabular}
\end{table}

  \begin{figure*}[!hbt]
  \centering
  \includegraphics[width=18cm]{images/neupa7.png}
  \caption{Segmentation output from the proposed DS-UNET-l, DS-UNET-s, DS-UNET-a, DS-ResUnet-l, and DS-Unet3+ and their comparison to the original vanilla networks of U-Net, ResUnet, and U-Net3+ on Massachusetts Building dataset.}
      \label{fig:MASS-results}
      \end{figure*}

\subsection{Results on multi-resolution dataset (MELB)} \label{sec:res:multi}
Table \ref{tab:res:multi} presents the results of all proposed, and SOTA networks on the proposed multi-resolution MELB dataset. Among the three DS-UNETs, the DS-UNET-l and DS-UNET-a outperform U-Net on the MELB dataset, while DS-UNET-s fall behind U-Net. DS-UNET-l shows the highest pixel accuracy, adjacent accuracy, and MCC. DS-UNET-a shows the highest F1 score and IoU. The accuracy measures of DS-ResUnet-l are slightly higher than the vanilla ResUnet and DS-Unet3+ outperforms U-Net3+ only in terms of IoU in the MELB dataset. Figure \ref{fig:MELB-results} shows the segmentation results of all the networks on a high-rise building and a complex building structure. All proposed networks outperform the compared SOTA networks. 

\begin{table}[]
\centering
\caption{DS-UNETs, DS-Unet3+, and DS-ResUnet-l on MELB dataset.}
\label{tab:res:multi}
\centering
\begin{tabular}{lccccc}
\hline
Networks & PA & AA & F1 & IoU & MCC \\ \hline
U-Net & 0.892 & 0.893 & 0.826 & 0.718 & 0.752 \\
DS-UNET-l & 0.902 & 0.896 & 0.829 & 0.720 & 0.769 \\
DS-UNET-s & 0.894 & 0.891 & 0.824 & 0.716 & 0.751 \\
DS-UNET-a & 0.898 & 0.894 & 0.834 & 0.726 & 0.761 \\ \hline
U-Net3+ & 0.904 & 0.899 & 0.832 & 0.724 & 0.773 \\
DS-Unet3+ & 0.897 & 0.895 & 0.831 & 0.724 & 0.760 \\ \hline
ResUnet & 0.894 & 0.888 & 0.823 & 0.714 & 0.749 \\
DS-ResUnet-l & 0.894 & 0.891 & 0.823 & 0.714 & 0.749 \\ \hline
U-Net++ & 0.890 & 0.888 & 0.820 & 0.709 & 0.744 \\
Deeplabv3+ & 0.891 & 0.882 & 0.819 & 0.708 & 0.742 \\
SegNet & 0.897 & 0.885 & 0.819 & 0.705 & 0.751 \\ \hline
\end{tabular}
\end{table}

  \begin{figure*}[!hbt]
  \centering
  \includegraphics[width=18cm]{images/neupa8.png}
  \caption{Segmentation output from the proposed DS-UNET-l, DS-UNET-s, DS-UNET-a, DS-ResUnet-l, and DS-Unet3+ and their comparison to the original vanilla networks of U-Net, ResUnet, and U-Net3+ on MELB dataset. The first sample (top) shows the segmentation result on a high-rise building of 63.88m in height. The second sample (bottom) shows the complexity of the building samples in the MELB dataset.}
      \label{fig:MELB-results}
      \end{figure*}

\subsection{Results on 1.2m subset of MELB dataset} \label{sec:res:MELB12}
The proposed networks outperform their original counterparts on the benchmark VHR and high-resolution dataset in all accuracy measures and in some measures on the multi-resolution MELB dataset. Table \ref{tab:res:melb12} presents the results from the networks on the 1.2m high-resolution building footprint dataset, which is a subset of MELB. The comparison of the DS-UNETs shows reduced pixel accuracy, adjacent accuracy, and MCC by up to 2-5\% when compared to U-Net. However, a significant increase of up to approx. 20\% is observed in terms of F1 score and IoU from all versions of DS-UNETs. Similarly, DS-ResUnet-l outperforms ResUnet in terms of all measures except the adjacent accuracy. This improved result from DS-ResUnet-l on the 1.2m subset of MELB is opposed to the results on the MELB dataset itself. DS-Unet3+ suffer a similar fate on the 1.2m data as compared to the multi-resolution MELB dataset. The proposed DS-UNETs outperform the compared SOTA networks in the majority of the accuracy measures. However, the proposed DS-ResUnet-l and DS-Unet3+, and their original vanilla version both suffer from low F1 scores and IoU.

\begin{table}[]
\centering
\caption{DS-UNETs, DS-Unet3+, and DS-ResUnet-l on 1.2m subset of MELB dataset.}
\label{tab:res:melb12}
\centering
\begin{tabular}{lccccc}
\hline
Networks & PA & AA & F1 & IoU & MCC \\ \hline
U-Net & 0.928 & 0.764 & 0.437 & 0.342 & 0.479 \\
DS-UNET-l & 0.942 & 0.722 & 0.617 & 0.523 & 0.425 \\
DS-UNET-s & 0.947 & 0.703 & 0.636 & 0.547 & 0.411 \\
DS-UNET-a & 0.942 & 0.721 & 0.635 & 0.543 & 0.415 \\ \hline
U-Net3+ & 0.953 & 0.778 & 0.471 & 0.380 & 0.496 \\
DS-Unet3+ & 0.945 & 0.772 & 0.439 & 0.343 & 0.471 \\ \hline
ResUnet & 0.872 & 0.745 & 0.545 & 0.451 & 0.351 \\
DS-ResUnet-l & 0.907 & 0.729 & 0.558 & 0.459 & 0.360 \\ \hline
U-Net++ & 0.930 & 0.728 & 0.602 & 0.510 & 0.389 \\
Deeplabv3+ & 0.940 & 0.684 & 0.570 & 0.473 & 0.378 \\
SegNet & 0.854 & 0.715 & 0.356 & 0.262 & 0.346 \\ \hline
\end{tabular}
\end{table}


\subsection{Limitations and future directions} \label{sec:res:limit}
A common experimental setup is used to compare the proposed networks, the original vanilla networks, and other SOTA networks. The experiments are designed with a focus on the efficiency gain in the segmentation of VHR and high-resolution EO images. No pre-processing and post-processing methods are used for the regularisation of building boundaries. The multi-resolution dataset that we develop for the City of Melbourne is prepared without manual annotations. The multi-resolution VHR images are collected from an API service from Nearmap. The labels are developed by masking and tiling the building roof samples provided by the City of Melbourne. It is made sure that the image and labels are of the same projection system and of the same year. However, it was noticed that the dataset is affected by the off-nadir angle of source images. This problem when occurs, the labels do not align properly with the buildings in the images. The problem increases along with the height of buildings in a complex urban setting from the City of Melbourne. The continuation of this work, therefore, will be focused on addressing the problem of off-nadir source imagery.

\section{Conclusion} \label{sec:conclusion}
In this article, we re-design the skip connections in the state-of-the-art encoder-decoder CNN-based deep learning architectures of U-Net and U-Net3+. A dual skip connection mechanism (DSCM) is proposed for the U-Net that doubles the large-scale features in the encoder before passing them to the decoder. Further, a dual full-scale skip connection mechanism (DFSCM) is proposed for the U-Net3+. DFSCM incorporates the increased low-level context information with high-level semantics from the feature maps at different scales using a multi-scale feature aggregation technique. Unlike U-Net3+, the aggregation proposed aggregates the multi-scale feature maps with increased weights for the smaller-scale feature maps, where the context information is still intact. The DSCM and DFSCM provide an effective trade-off between the use of context and precise localisation for semantic segmentation of VHR EO images. The DSCM is tested for different scales of U-Net with a low to a high number of network parameters, further proposing several novel architectures. DSCM is also tested on another popular architecture of ResUnet. The networks used for comparison are the vanilla versions of U-Net, ResUnet, and U-Net3+, and furthermore on U-Net++, Deeplab3+, and SegNet. The experimental dataset includes a VHR (0.3m spatial resolution) benchmark WHU Building dataset, a high-resolution (1m) benchmark dataset of Massachusetts Building dataset, a multi-resolution building dataset (0.3, 0.6, and 1.2m) that we develop for the City of Melbourne, and a high-resolution (1.2m) subset of the multi-resolution dataset.

An efficiency gain over five accuracy metrics is achieved in U-Net and ResUnet with DSCM applied on the large-scale layers. Similarly, DFSCM on vanilla U-Net3+ demonstrates 17.7\% and 18.4\% gain in F1 score and Intersection over Union (IoU). All proposed networks achieve the efficiency gain with up to 14\%, 8\%, and 16\% increase in network parameters compared to U-Net, ResUnet, and U-Net3+ respectively. Therefore, it can be concluded that a better trade-off between the use of context and precise location can be achieved in state-of-the-art U-Net, ResUnet, and U-Net3+. This can be done by enriching and re-designing skip connections with increased large-scale feature maps, where the contexts are lost due to down-sampling operations such as max-pooling. The proposed networks can be further studied in combination with evolving advancements in computer vision tasks such as vision transformers.

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgment
\section*{Acknowledgment}
The authors would like to thank Nearmap for providing very high-resolution aerial images through their API services. 


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{IEEEtran}
\bibliography{Reference}

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

%\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/neupa9.jpg}}]{Bipul Neupane} received his B.E. in Geomatics Engineering from Kathmandu University, Nepal in 2015 and an M.S. in Engineering and Technology from Sirindhorn International Institute of Technology (SIIT), Thammasat University, Thailand in 2019.

He is currently a PhD Candidate at the Department of Infrastructure Engineering, University of Melbourne, Australia. Before, he was a Researcher at SIIT, Thammasat University, where he carried out several research works related to location intelligence using earth observation in urban and agricultural applications. His research interests include digital image processing and computer vision using sensing technologies. His past work includes being GIS/M\&E Officer for location intelligence in the health sector and land-use planning services.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/neupa10.png}}]{Jagannath Aryal} (M) received the PhD degree in optimization and systems modelling from the Centre for Advanced Computational Solutions (C-fACS), New Zealand. 

He is currently an Associate Professor in Digital Infrastructure Engineering with the Faculty of Engineering and Information Technology (FEIT), Department of Infrastructure Engineering, University of Melbourne, Melbourne, Australia. He is also the National Chair of the Remote sensing, and photogrammetry commission, SSSI Australia. His research interests and contributions include advancing the knowledge in Earth Observation Science, Digital Information Transformation, and Disaster Management using intelligent modelling approaches such as deep learning, geographic object-based artificial intelligence (GEOAI), resilience engineering, and spatial statistics. The application areas include urban systems, and urban features and disaster risk reductions.

Dr Aryal is on the Scientific Committee of the European GIS Conference (AGILE), GEOBIA, and other spatial analytics and modelling conferences. He serves on the editorial board of the Journal of Spatial Science (Taylor \& Francis), and Remote Sensing (MDPI), and reviews the scientific journals for IEEE, Springer, and Elsevier.
\end{IEEEbiography}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/neupa11.png}}]{Abbas Rajabifard} is the Leader of Geomatics Discipline at the Faculty of Engineering and IT (FEIT), University of Melbourne, Melbourne, Australia.

Professor Rajabifard is an internationally recognised scholar and geospatial engineer. He is the director of the Centre for SDIs and land administration (CSDILA) which is a world-leading research and development centre in the domain of land administration system modernisation and spatial data infrastructure. He is also leading research in sustainability and resilience, and Digital Twin for urban systems. Prof Rajabifards passion is in the field of research and innovation to serve the global community. He is also an Advisory Board Member of the UN-GGIM Academic Network. He has led multiple projects nationally and internationally in improving urban geospatial and land systems, 3D cadastre, and resilience impact particularly in the Asia-Pacific, Europe, North America and Latin America, where he has developed strategies, designed and implementation of policies, technologies and applications in a wide variety of application areas.
\end{IEEEbiography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}