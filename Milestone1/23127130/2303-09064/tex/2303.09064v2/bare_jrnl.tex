%% bare_jrnl.tex
%% V1.4a
%% 2014/09/17
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8a or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_conf_compsoc.tex,
%%                    bare_jrnl_compsoc.tex, bare_jrnl_transmag.tex
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices and paper sizes can       ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{hyperref}
\usepackage{multirow}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{cuted}
\usepackage[numbers,sort&compress]{natbib}
\setcitestyle{open=[,close=]}
\usepackage{amsmath, amsfonts}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Rethinking the U-Net, ResUnet, and U-Net3+ architectures with dual skip connections for building footprint extraction}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Bipul~Neupane,
        Jagannath~Aryal,~\IEEEmembership{Member,~IEEE,}
        and~Abbas~Rajabifard% <-this % stops a space
%\thanks{Manuscript received April XX, 2023; revised XXX, XXX.}
\thanks{All authors are from the Department of Infrastructure Engineering, Faculty of Engineering and IT, The University of Melbourne, Melbourne, VIC 3010, Australia. Bipul Neupane is supported by the University of Melbourne for his PhD research and is awarded by Melbourne Research Scholarship. Corresponding author: Bipul Neupane (e-mail: bneupane@student.unimelb.edu.au).}% <-this % stops a space
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,~Vol.~XX, No.~XX, April~2023}%
%{Neupane \MakeLowercase{\textit{et al.}}: Rethinking the U-Net, ResUnet, and U-Net3+ architectures with dual skip connections for building footprint extraction}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2014 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
The importance of building footprints and their inventory has been recognised as foundational spatial information for multiple societal problems. Extracting complex urban buildings involves the segmentation of very high-resolution (VHR) earth observation (EO) images. U-Net is a common deep learning network and foundation for its new incarnations like ResUnet, U-Net++ and U-Net3+ for such segmentation. The re-incarnations look for efficiency gain by re-designing the skip connection component and exploiting the multi-scale features in U-Net. However, skip connections do not always improve these networks and context information is lost in the multi-scale features. In this paper, we propose three novel dual skip connection mechanisms for U-Net, ResUnet, and U-Net3+. This deepens the feature maps forwarded by the skip connections to find a more accurate trade-off between context and localisation within these networks. The mechanisms are evaluated on feature maps of different scales in the three networks, producing nine new network configurations. The networks are evaluated against their original vanilla versions using four building footprint datasets (three existing and one new) of different spatial resolutions: VHR (0.3m), high-resolution (1m and 1.2m), and multi-resolution (0.3+0.6+1.2m). The proposed mechanisms report efficiency gain on five evaluation measures for U-Net and ResUnet, and up to 17.7\% and 18.4\% gain in F1 score and Intersection over Union (IoU) for U-Net3+. The codes will be available in a GitHub link after peer review.
\end{abstract}

%% The importance of building footprints and their inventory has been recognised as an enabler for multiple societal problems. Extracting urban building footprint is complex and requires semantic segmentation of very high-resolution (VHR) earth observation (EO) images. U-Net is a common Deep Learning network for such segmentation. It has seen several major re-incarnations including ResUnet, U-Net++ and U-Net3+ with interventions to exploit multi-scale features and re-design skip connections. However, the interventions are still being explored for a more accurate trade-off between the use of context and precise localisation in the ``layers'' of U-Net architectures. In this paper, we propose a dual skip connection mechanism (DSCM) for U-Net, a dual respath skip connection mechanism (DRSCM) for ResUnet and a dual full-scale skip connection mechanism (DFSCM) for U-Net3+. The three novel mechanisms deepen the networks and double the features that are passed from the encoder to the decoder of the networks for precise localisation. Further, the mechanisms are evaluated on feature maps of different scales in the three networks. The proposed mechanisms, therefore, produce several novel networks that are evaluated on four building footprint datasets (three existing and one new) of different spatial resolutions: VHR (0.3m), high-resolution (1m and 1.2m), and multi-resolution (0.3+0.6+1.2m). DSCM and DRSCM on U-Net and ResUnet result in efficiency gains in terms of five evaluation measures. Similarly, DFSCM on U-Net3+ yields up to 17.7\% and 18.4\% gain in F1 score and Intersection over Union (IoU). The codes will be available in a GitHub link after peer review.

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
building footprint extraction, u-net, skip connection, multi-scale feature aggregation, spatial information, semantic segmentation.
\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction} \label{sec:intro}
\IEEEPARstart{S}{emantic} segmentation of earth observation (EO) images is the current state-of-the-art (SOTA) method for urban feature extraction and land use and land cover classification (LULC) \cite{neupane2021deep}. The evolving nature of semantic segmentation has seen traditional classifiers including edge-based, shadow-based, region-based, and machine learning approaches \cite{hossain2019segmentation}. However, these methods lack precision due to their dependency on mid-level semantic characteristics (hand-crafted features). 
%Machine learning such as random forest \cite{du2015semantic} and support vector machine \cite{huang2012svm} are predecessors to the current SOTA of deep learning.

%The evolving nature of semantic segmentation has seen traditional classifiers (e.g. threshold-based method \cite{otsu1979threshold}, region growing \cite{adams1994seeded}, edge detection \cite{canny1986computational}), and machine learning methods such as random forest \cite{du2015semantic} and support vector machine \cite{huang2012svm} in remote sensing. These methods lack precision due to their dependency on mid-level semantic characteristics (hand-crafted features). 

An accurate and precise extraction of complex urban features such as building footprints is only possible due to the availability of very high-resolution (VHR) EO images. Semantic segmentation of VHR images for building footprint extraction, in recent years, is performed using convolutional neural networks (CNNs) \cite{wei2019toward} and fully convolutional neural networks (FCNs) \cite{long2015fully}. These neural networks allow fast and automatic feature extraction from an adequately large dataset with an increased number of ``layers'' to reduce classification errors in regression. This provides contrasting results when compared to machine learning predecessors like random forest \cite{du2015semantic} and support vector machine \cite{huang2012svm}. An encoder-decoder architecture with a CNN as a fundamental backbone has become the common structural configuration for semantic segmentation \cite{minaee2021image}. U-Net \cite{ronneberger2015u} is the most common example. An encoder is essentially a CNN that generates feature maps of different scales and sizes using convolutions and down-sampling operations. These maps comprise low-level and fine-grained information. A decoder is generally symmetrical to an encoder with up-sampling operations. The decoder comprises high-level and coarse-grained semantic information. A bridge called ``skip connection'' pass the low-level information from the encoder layers to the decoder layers to make the utmost use of the encoder-decoder structure. The popularity of U-Net comes from a trade-off between the use of context information (generated by the encoders) and precise localisation (in the layers of the decoder) for precise segmentation. However, the problem with U-Net is that the context information is lost when generating multi-scale feature maps because of the down-sampling operations that reduce the size of those feature maps. Urban feature extraction from EO images requires both context information and precise localisation. Therefore, despite the loss of context information during multi-scale feature generation, a more accurate trade-off between context and localisation is needed in U-Net-like networks.

The improved versions of the U-Net architecture focus on its skip connection for a more efficient multi-scale feature aggregation. U-Net++ \cite{zhou2019unet++} replaces the connections with nested and dense skip connections. These dense connections with rich information and reduced inter-encoder-decoder semantic gaps increase the precision of segmentation while also increasing the network parameters. U-Net3+ \cite{huang2020unet} comes with a more efficient way of exploiting the multi-scale features with full-scale skip connections. The inter-encoder-decoder skip connections of U-Net are re-designed, and new intra-connections between the decoder layers are added to capture fine-grained details and coarse-grained semantics in full scales. The full-scale skip connection assumes equal weights among the feature maps generated at different scales. However, different scales of feature maps possess different levels of discrimination and context information, which is not realised in the U-Net3+ configuration. To address these weaknesses of U-Net and U-Net3+, we propose a dual skip connection mechanism (DSCM) for U-Net and a dual full-scale skip connection mechanism (DFSCM) for U-Net3+ with a new multi-scale feature aggregation technique that aggregates the feature maps of different scales with increased weights for the smaller-scale feature maps. Further, we test the concept of DSCM in another popular variant of U-Net, i.e., ResUnet \cite{zhang2018road} with a dual respath skip connection mechanism (DRSCM). 

The proposed connection mechanisms deepen the encoder of the three networks with denser convolution operations. Therefore, these new connections are different from the existing implementation of dual skip connections like DR-Unet \cite{le2021dr} and DPN-Unet-TypeII \cite{xu2021dual}. The dual skip connection on U-Net3+ is not studied to the best of our knowledge. In particular, DSCM in U-Net deepens the convolutional blocks of different scales. Similarly, DRSCM on ResUnet deepens the residual block of the network and doubles the residual skip connections in those blocks. In the case of U-Net3+, a new multi-scale feature aggregation technique is introduced to integrate the DFSCM. Unlike, the aggregation that assumes equal weights for all feature maps in U-Net3+, the proposed DFSCM gives more weight to the small-scale feature maps, where the context information is still intact. A lower weight is given to the large-scale features as they lose context with downsampling operations. All three proposed mechanisms can be plugged to deepen only the desired scale layers of networks, as not all skip connections provide the efficiency gain \cite{wang2022uctransnet}. To study the effects of dual skip connections in different scale layers, we propose and experiment with three scale variants of each network. With the three variants of the three dual skip connection mechanisms on the three networks, a total of nine new network configurations are proposed in this paper.

The experimental design of the proposed networks is categorised in terms of the spatial resolution of datasets. Four datasets of VHR and high-resolution types are used for the evaluation, with one newly developed multi-resolution dataset of complex urban building samples. The performance of the nine new networks is compared to the vanilla versions of U-Net, U-Net3+, ResUnet and other SOTA encoder-decoder networks like U-Net++ \cite{zhou2019unet++}, Deeplabv3+ \cite{chen2017deeplab}, and SegNet \cite{badrinarayanan2017segnet}. All of the proposed and vanilla networks are evaluated with five evaluation measures. The contributions of this paper are:

\begin{enumerate}
    \setcounter{enumi}{0}
    \item We propose DSCM and DRSCM for U-Net and ResUnet to deepen the encoder and enrich the skip connection for an effective trade-off between the use of context and precise localisation of building footprints from VHR and high-resolution images.
    \item We propose DFSCM for U-Net3+ with a new multi-scale feature aggregation technique. Unlike in U-Net3+, the feature maps of different scales are aggregated with increased weights for the smaller-scale feature maps, where the context information is intact.
    \item We present comprehensive experiments to study the proposed mechanisms on different scale layers of U-Net, ResUnet, and U-Net3+ with a total of nine new network configurations. The nine networks are tested on four datasets of different spatial resolutions.
    \item A new multi-resolution dataset is developed for a comprehensive robustness check with urban building samples of different spatial resolutions.
    \item An ablation study is provided to find the scale layers in U-Net that provide the highest efficiency gain with the proposed DSCM.
\end{enumerate}


\section{Related Work} \label{sec:relw}

\subsection{CNNs, FCNs and encoder-decoder networks} \label{sec:relw:CNNs}
The earliest forms of CNN are AlexNet \cite{krizhevsky2012imagenet}, VGGNet \cite{simonyan2014very}, GoogleNet \cite{szegedy2015going}, ResNet \cite{he2016deep}, Xception \cite{chollet2017xception}, and RefineNet \cite{lin2017refinenet}. They are now used to extract multi-scale feature information from images \cite{chen2018aerial,ayala2021deep} for segmentation purposes in FCNs and encoder-decoder FCNs like SegNet \cite{badrinarayanan2017segnet} and U-Net \cite{ronneberger2015u}. Some of the earliest studies of CNN-based building footprint extraction \cite{mnih2013machine,saito2015building,saito2016multiple,vakalopoulou2015building,marcu2017object} perform patch-based segmentation supported by post-processing methods to improve accuracy and precision. CNN-based pixel-level building segmentation is achieved by Mask R-CNN that uses CNNs like ResNet as feature extractor (aka. backbone) \cite{zhao2018building,griffiths2019improving}. Similarly, building footprint extraction has also seen different versions of FCN (FCN-2s, FCN-4s, and FCN-8s) \cite{zhong2016fully,maggiori2016convolutional,yang2018building,qin2019semantic}. 

The early form of encoder-decoder networks (SegNet and U-Net) are first introduced to segment indoor/outdoor scenes and medical images. They followed their way to EO images including multi-spectral images from WorldView-3 \cite{li2019semantic}, synthetic aperture radar (SAR) and multi-spectral images from Sentinel-1 and 2 \cite{ayala2021deep} for building segmentation. Some have also achieved building classification \cite{pan2020deep}. SegNet is also experimented with further modifications \cite{bischke2019multi,sariturk5feature} and in combination with U-Net to form Seg-Unet \cite{abdollahi2020ensemble}. Similarly, ResUnet \cite{zhang2018road} is introduced to extract road networks with residual units added to U-Net to ease the training and further enrich the skip connections. ResUnet is further experimented for building segmentation with multi-modal hand-crafted features \cite{xu2018building} and ultra-high resolution (UHR) images \cite{li2019semantic}. Due to its legacy in the domain of computer vision, widely used versions of the U-Net family are the focus of this paper.

\subsection{The legacy and evolution of U-Net} \label{sec:relw:unet}
The popularity of U-Net \cite{ronneberger2015u} comes from addressing the loss of context information in the CNNs due to (i) the heavy use of pooling and max-pooling operations, and (ii) the imbalanced trade-off between localisation and the use of context. U-Net has seen an explosion in usage in medical imaging since its introduction and several variants have been proposed in this domain \cite{siddique2021u}. However, the loss of context information cannot be avoided while using CNNs as a feature extractor. U-Net is followed by a number of improvements that are focused on utilising the multi-scale feature maps of CNNs in an efficient manner. Among many, some of the successful variants are 3D U-Net \cite{cciccek20163d}, V-Net \cite{milletari2016v} attention U-Net \cite{oktay2018attention}, U-Net++ \cite{zhou2019unet++}, R2U-Net \cite{alom2018recurrent}, Inception-U-Net \cite{zhang2020dense}, ResUnet \cite{zhang2018road}, Dense U-Net \cite{wang2019dense}, adversarial U-Net\cite{schonfeld2020u}, U$^2$Net \cite{qin2020u2}, and U-Net3+ \cite{huang2020unet}. These U-Nets are widely used to segment both medical and EO images. The recent advancement in U-Net seems also to be affected by the rise of Vision Transformer (ViT) \cite{dosovitskiy2020image}, which brings the science of Transformer \cite{vaswani2017attention} from natural language processing to a computer vision problem. The concept of ViT tries to replace CNNs in semantic segmentation but has still not been able to fully replace it. The adaptation of ViT in the recent U-Net versions includes TransUnet \cite{chen2021transunet}, TransFuse \cite{zhang2021transfuse}, and Swin-Unet \cite{cao2021swin}. TransUnet builds upon the problem of CNNs failing to fully learn the global and remote semantic information interaction because of the involvement of the convolutional process. The prior concepts of feature pyramid \cite{lin2017feature}, DeepLab \cite{chen2017deeplab}, atrous convolution layers \cite{chen2018encoder}, context encoder network (CE-Net) \cite{gu2019net}, self-attention \cite{wang2018non}, and attention gate \cite{schlemper2019attention} also have tried to address this problem. With U-Net3+ as the most recent version based on the naming, there are some recent studies that have tried to improve the network with an addition/integration of attention module \cite{li2020macu}, residual unit \cite{qin2022improved}, and transformer \cite{chen2023improved}. O-Net \cite{wang2022net} is a recent development that realises the addition of a self-attention mechanism on the transformer and combining it with CNN can marginally improve the network when compared to computational-heavy networks based on ViT. The improvement in performance is vitally important to computer vision but raises some serious questions regarding the computation expense. Unlike computationally super-expensive transformer-based semantic segmentation to address the problem of inadequate learning of global and remote semantic information, some other advancements in U-Net variants seek to lower the number of network parameters in U-Net while improving the performance of U-Net. The aim of effective exploitation of multi-scale features has brought the researcher to re-think the U-Net networks with re-designed skip connections, which we review next.

\subsection{Re-designing skip connections} \label{sec:relw:skip}
U-Net passes the features from the encoder layers to the decoder layers through skip connections and concatenates them to the upsampled outputs. The redesigning of these skip connections is ongoing research. The recent variants of U-Net like NAS-UNet \cite{weng2019unet}, U-Net3+ \cite{huang2020unet}, DC-UNet \cite{lou2021dc}, and Half-UNet \cite{lu2022half} re-design skip connections in U-Net while reducing the number of network parameters. U-Net++ and U-Net3+ change the terminology of ``skip connections'' to ``plain skip connections'' with the realisation of this skip connection. U-Net++ \cite{zhou2019unet++} propose dense nested skip connections for performance gain, but with added complexity and network parameters. U-Net3+ on the other hand re-designs the skip connections with fewer network parameters with a newly proposed full-scale skip connection. This connection utilises inter-encoder-decoder and intra-decoder skip connections to capture both fine- and coarse-grained semantics from full scales. To concatenate the maps of five scales, a multi-scale feature aggregation mechanism is developed for U-Net3+. Moving away from dense and full-skip connections, MultiResUnet \cite{ibtehaz2020multiresunet} and DR-Unet \cite{le2021dr} replace plain skip connections with \textit{Res} paths for improved performance in U-Net and ResUnet. DPN-Unets \cite{xu2021dual} utilise a dual path network (DPN) \cite{chen2017dual} to combine DenseNet and ResNet in parallel. The recent-most network that re-design the skip connections in U-Net is UC-TransNet \cite{wang2022uctransnet}. The authors of UC-TransNet have highlighted that not all skip connections provide efficiency gain, and some even influence the original U-Net negatively. Their skip connection utilises a channel-wise cross-fusion transformer and channel-wise cross-attention for efficiency gain. It can be seen that the baseline for all the re-designing of skip connections are U-Net and ResUnet. Leaving the added complexity of transformers are other components behind, we point out that the advancement in skip connections to utilise multi-scale features is an evolving work. Besides the aggregation of features from the encoder side, some aggregate features of the decoder side \cite{aryal2023multi}, which is out of the scope of our study. Our focus is on the skip connections and the multi-scale aggregation of features from the encoder.

Exploiting multi-scale features with new aggregation and fusion techniques is popular in urban feature extraction from EO images \cite{wu2018automatic,wei2019toward,ji2019scale}. U-Net and U-Net3+ rely on concatenation to aggregate the multi-scale feature maps. Some networks like Half-UNet \cite{lu2022half} aggregate them using an addition operation inspired by ResNet to increase the amount of information without increasing the dimension. The final segmented output is generated without the large-scale decoder layers resulting in 100x lower network parameters with similar performance when compared to U-Net3+. The experiments also show the lower performance of U-Net3+ against U-Net in some of the datasets. In a similar attempt to subtract the components of U-Net, Fu et al. \cite{fu2021keep} design ``additive'' and ``subtractive'' variants of U-Net. The additive variants add a dense block, residual block, side-output block, and dilated convolution block to the U-Net and the subtractive variants: U-Net without Rectified Linear Units (ReLU) activation, U-Net without skip connections, and U-Net with one convolutional layer per level. The subtractive variants show a lower dice score compared to the additive variants. In their experiments, U-Net is less competitive without skip connections. Other studies have highlighted that not all skip connections between different scales of encoder-decoder improve these U-Net configurations \cite{wang2022uctransnet}. However, there is a lack of conclusions on which scales are to be focused. In this study, we take the vanilla version of U-Net, ResUnet, and U-Net3+ and enrich their skip connections. Furthermore, we confirm which scales of encoder-decoder configurations are to be focused on for efficiency gain.

\section{Method} \label{sec:method}
\subsection{DSCM for U-Net} \label{sec:meth:arch}
Let us start with simplifying a U-Net of 5-scale layers. The encoder $En$ consecutively passes the learned feature maps along the encoder layers $X_{En}^{n}$ (where $n=1,\dots,5$). An $n^{th}$ layer of encoder $X_{En}^{n}$ consists of two recurring unpadded 3x3 convolutions $\mathcal{C}(\cdot)$ followed by a batch normalization (BN). Each $\mathcal{C}(\cdot)$ is activated with a ReLU activation function $\sigma(\cdot)$. The features are down-scaled with operation $\mathcal{D}(\cdot)$ of max-pooling with stride 2 along the consecutive encoder layers. After the final encoder layer $X_{En}^{5}$, the decoder starts by upsampling the low-resolution feature maps of $X_{En}^{5}$, and keeps doing so along the decoder layers $X_{De}^{n}$ (where $n=4,\dots,1$). An $n^{th}$ layer of decoder $X_{De}^{n}$ starts with an up-sampling operation $\mathcal{U}(\cdot)$ of a 2x2 ``up-convolution'' that halves the number of feature channels from the previous layer. The output of $\mathcal{U}(\cdot)$ is concatenated to the feature map of the corresponding encoder layer $X_{En}^{n}$, followed by a $\mathcal{C}(\cdot)$. The feature map of $X_{En}^{n}$ is brought to $X_{Dn}^{n}$ by a plain skip connection for matching index $n$. A 1x1 convolution in the final layer maps the 64-component feature vectors to the number of classes. This general 5-scale U-Net is illustrated in Figure \ref{fig:dualskipconnection}(a). With this process, U-Net utilises both low- and high-resolution features, conserving the spatial integrity of objects that is crucial in the semantic segmentation of features in EO data. However, the problem with this architecture of U-Net lies in the down-scaling as $\mathcal{D}(\cdot)$ increases the scale while halving the dimension of the feature maps, thus losing the context information at each $\mathcal{D}(\cdot)$ operation. 

  \begin{figure*}[!h]
  \centering
  \includegraphics[width=18cm]{images/neupa1.png}
  \caption{Illustration of the plain skip connections in U-Net and the proposed DSCM.}
      \label{fig:dualskipconnection}
      \end{figure*}

To explain the proposed DSCM, let us denote one sequence of $\mathcal{C}(\cdot)$ and BN as $\mathcal{G}(\cdot)$. Then a layer of $X_{En}^{n}$ in U-Net sequentially consists of $\mathcal{C}(\cdot)$, $\mathcal{G}(\cdot)$, and $\mathcal{D}(\cdot)$ operations. To compensate for the lost context information in U-Net, we propose DSCM as illustrated in Figure \ref{fig:dualskipconnection}. With DSCM, instead of one, an encoder layer now consists of two sequential $\mathcal{G}(\cdot)$: $\mathcal{G}_{1}(\cdot)$ and $\mathcal{G}_{2}(\cdot)$. Then, an $X_{En}^{n}$ consists of $\mathcal{C}(\cdot)$, $\mathcal{G}_{1}(\cdot)$, $\mathcal{G}_{2}(\cdot)$, and $\mathcal{D}(\cdot)$ sequential operations. Similar to the U-Net, $\mathcal{D}(\cdot)$ down-scales the feature maps of the first sequence $\mathcal{G}_{1}(\cdot)$, along $X_{En}^{n}$ (where $n=1,\dots,5$). Instead of one plain skip connection, DSCM consists of two skip connections: $\mathcal{S}_{1}(\cdot)$ and $\mathcal{S}_{2}(\cdot)$, which respectively pass the features of $\mathcal{G}_{1}(\cdot)$ and $\mathcal{G}_{2}(\cdot)$ of $X_{En}^{n}$ to $X_{De}^{n}$ for matching $n$. In $X_{De}^{n}$, the feature maps brought by $\mathcal{S}_{1}(\cdot)$ and $\mathcal{S}_{2}(\cdot)$ are concatenated to the output of $\mathcal{U}(\cdot)$ that upsamples the features of the previous layer of decoder $X_{De}^{n+1}$. In case of $X_{De}^{4}$, it receives an upsampled output of $X_{En}^{5}$. The output feature map of a $X_{De}^{n}$ for $n=4,\dots,1$ can be denoted as

\begin{equation}
X_{De}^{n}=\mathcal{C}(\mathcal{C}(\mathcal{U}(X_{De}^{n+1})))
\oplus \mathcal{S}_{1}(X_{En}^{n})
\oplus \mathcal{S}_{2}(X_{En}^{n})
\label{eqn:DSCM}
\end{equation}

where, the $\mathcal{C}(\cdot)$ is supported by a ReLU $\sigma(\cdot)$, and the feature maps carried by skip connections $\mathcal{S}_{1}$ and $\mathcal{S}_{2}$ is denoted as

\begin{equation}
\mathcal{S}_{1}(X_{En}^{n})=\mathcal{G}_1(\mathcal{C}(X_{En}^{n-1}))
\label{eqn:DSCM-skip1}
\end{equation}

\begin{equation}
\mathcal{S}_{2}(X_{En}^{n})=\mathcal{G}_2(\mathcal{S}_{1}(X_{En}^{n}))
\label{eqn:DSCM-skip2}
\end{equation}

%A 1x1 convolution in the final layer of the decoder ($X_{De}^{1}$) maps the 64-component feature vectors to the number of classes.

\subsection{DSCM in different scale features of U-Net} \label{sec:meth:dualskip}
The DSCM can be plugged into any scale layers of U-Net. To study the effects of DSCM on different scales of fine-grained detailed information and coarse-grained semantic information, we propose three novel U-Net network architectures: dual skip on large-scale features (DS-UNet-L), dual skip on small-scale features (DS-UNet-S), and dual skip on all scale features (DS-UNet-A). Figure \ref{fig:ds-unets} illustrates the three networks with DSCM.

\begin{enumerate}
    \setcounter{enumi}{0}
    \item \textit{DS-UNet-L} applies DSCM between the large-scale layers of encoder $X_{En}^{n}$ and decoder $X_{De}^{n}$ for $n\in [3,4]$.
    \item \textit{DS-UNet-S} applies DSCM between the small-scale layers of encoder $X_{En}^{n}$ and decoder $X_{De}^{n}$ for $n\in [1,2]$.
    \item \textit{DS-UNet-A} applies DSCM between all four scale layers of encoder $X_{En}^{n}$ and decoder $X_{De}^{n}$ for $n\in [1,2,3,4]$.
\end{enumerate}

  \begin{figure*}[!h]
  \centering
  \includegraphics[width=18cm]{images/neupa2.png}
  \caption{Illustration of the proposed DSCM on different scale layers of U-Net to form DS-UNet-L, DS-UNet-S, and DS-UNet-A.}
      \label{fig:ds-unets}
      \end{figure*}

      
\subsection{DRSCM for ResUnet} \label{sec:meth:dsresunet}
DSCM cannot be directly implemented in ResUnet because of its residual units. For this, we propose a DRSCM for ResUnet. We name the ResUnet with DRSCM as a DS-ResUnet. In addition to the U-Net configuration, a ResUnet includes a skip connection (we name it respath) within its residual unit \cite{zhang2018road}. Therefore, to deepen the residual unit with one more 3x3 convolution, one more respath needs to be added as shown in the illustration of the proposed DRSCM in Figure \ref{fig:dsresunet}(c). Unlike the existing dual skips in \cite{le2021dr} and \cite{xu2021dual}, the residual unit is deeper and denser in our proposed DRSCM with an added 3x3 convolution layer $\mathcal{C}(\cdot)$, BN, ReLU $\sigma(\cdot)$, and a respath $\mathcal{R}(\cdot)$. 

  \begin{figure*}[!h]
  \centering
  \includegraphics[width=18cm]{images/neupa3.png}
  \caption{Illustration of DS-ResUNet-A along with the differences between the plain skip connections in ResUnet and the proposed DRSCM}
      \label{fig:dsresunet}
      \end{figure*}

Let us start by explaining a ResUNet of 5-scale layers. Encoder $En$ consecutively passes the learned feature maps along $X_{En}^{n}$ (where $n=1,\dots,5$). Lets denote one sequence of BN and unpadded 3x3 convolution $\mathcal{C}(\cdot)$ as $\mathcal{G}(\cdot)$. Each $\mathcal{C}(\cdot)$ is activated with a ReLU $\sigma(\cdot)$. Then, $X_{En}^{n}$ consists of two sequences of $\mathcal{G}(\cdot)$ i.e., $\mathcal{G}_{1}(\cdot)$ followed by $\mathcal{G}_{2}(\cdot)$. The output of $\mathcal{G}_{2}(\cdot)$ is added to the output of a respath skip connection $\mathcal{R}(\cdot)$. A respath skip connection $\mathcal{R}(\cdot)$ consists of a 1x1 $\mathcal{C}(\cdot)$ followed by a BN on the input feature map of $X_{En}^{n-1}$. 

With DRSCM integrated into a residual block of a ResUNet, we add a third $\mathcal{G}(\cdot)$ and $\mathcal{R}(\cdot)$ operations to deepen the residual block. Now, a $X_{En}^{n}$ consists of $\mathcal{G}_{1}(\cdot)$, $\mathcal{G}_{2}(\cdot)$, $\mathcal{R}_{1}(\cdot)$, $\mathcal{G}_{3}(\cdot)$, and $\mathcal{R}_{2}(\cdot)$ sequential operations. The output of $\mathcal{G}_{3}(\cdot)$ is added to the output of second respath skip connection $\mathcal{R}_{2}(\cdot)$. The output features of the $\mathcal{R}_{1}(\cdot)$ are down-scaled with operation $\mathcal{D}(\cdot)$ of max-pooling with stride 2 along $n$ residual units. After the final encoder layer $X_{En}^{5}$, the decoder starts by up-sampling the low-resolution feature maps of $X_{En}^{5}$, and keeps on doing so along the decoder layers $X_{De}^{n}$ (where $n=4,\dots,1$). An $n_{th}$ decoder layer $X_{De}^{n}$ starts with an up-sampling operation $\mathcal{U}(\cdot)$ of 2x2 ``up-convolution''. In case of $X_{De}^{4}$, it receives an upsampled output of $X_{En}^{5}$. The output of $\mathcal{U}(\cdot)$ is concatenated to the two sets of feature maps of the corresponding encoder layer $X_{En}^{n}$, followed by two consecutive $\mathcal{G}(\cdot)$. The two sets of feature maps are brought from $X_{En}^{n}$ to $X_{De}^{n}$ for matching index $n$ by two skip connections of DRSCM, each consisting of the output of $\mathcal{R}_{1}(\cdot)$+$\mathcal{G}_{2}(\cdot)$ and $\mathcal{R}_{2}(\cdot)$+$\mathcal{G}_{3}(\cdot)$. The output feature map of a $X_{De}^{n}$ for $n=4,\dots,1$ can be denoted as

\begin{equation}
\begin{split}
X_{De}^{n}=[\mathcal{G}(\mathcal{G}(\mathcal{U}(X_{De}^{n+1})))
\oplus \mathcal{S}_{1}(X_{En}^{n})
\oplus \mathcal{S}_{2}(X_{En}^{n})] \\ + \mathcal{R}(X_{De}^{n+1})
\label{eqn:DRSCM}
\end{split}
\end{equation}

where, the feature maps carried by skip connections $\mathcal{S}_{1}$ and $\mathcal{S}_{2}$ can be denoted as

\begin{equation}
\mathcal{S}_{1}(X_{En}^{n})=\mathcal{G}_{2}(\mathcal{G}_{1}(X_{En}^{n-1}))+\mathcal{R}_{1}(X_{En}^{n-1})
\label{eqn:DRSCM-skip1}
\end{equation}

\begin{equation}
\mathcal{S}_{2}(X_{En}^{n})=\mathcal{G}_{3}(\mathcal{S}_{1}(X_{En}^{n}))+\mathcal{R}_{2}(X_{En}^{n-1})
\label{eqn:DRSCM-skip2}
\end{equation}

Similar to the three variations of DS-UNet, we experiment with the integration of DRSCM in the large-scale (DS-ResUNet-L), small-scale (DS-ResUNet-S), and all-scale (DS-ResUNet-A) features.
      
\subsection{DFSCM for U-Net3+} \label{sec:meth:dsunet3+}
The integration of dual skip connections in U-Net3+ is more sophisticated because of the existing full-skip connections in U-Net3+ \cite{huang2020unet}. Unlike the plain skip connections of U-Net and ResUnet that pass the features from $X_{En}^{n}$ to $X_{De}^{n}$ only for matching index $n$, the full-skip connections allow aggregating the feature maps of all $n$ scales to capture both fine- and coarse-grained semantics in $X_{De}^{n}$. Let us take an example of the third decoder layer $X_{De}^{3}$ of U-Net3+. $X_{De}^{3}$ receives the feature maps of (i) same-scale encoder layer $X_{En}^{3}$ similar to U-Net, (ii) smaller-scale encoder layer $X_{En}^{2}$ and $X_{En}^{1}$ carrying coarse-grained detailed information through inter encoder-decoder skip connections supported by non-overlapping max pooling operations, and (iii) larger-scale decoder layer $X_{De}^{4}$ and $X_{De}^{5}$ carrying fine-grained semantic information through intra-decoder connections supported by bilinear interpolation. The number of channels in all incoming five same-resolution feature maps is unified with 64 filters of 3x3 size. A feature aggregation mechanism is applied on the concatenated maps of five scales that consist of 320 (64 times 5) filters of 3x3 size, a BN, and a ReLU activation function. The feature aggregation mechanism in U-Net3+ assumes equal weights for all feature maps at different scales. However, the different scales of feature maps possess different levels of discrimination. The context information is more intact in the small-scale feature maps as the large-scale features get smaller in size and lose them because of the downsampling operations.

We integrate the concepts of DSCM and full-scale skip connections from U-Net3+ to propose DFSCM for U-Net3+. We name the formulated networks DS-UNet3+. The difference in U-Net3+ and DS-UNet3+ is illustrated in Figure \ref{fig:dsunet3plus}. The skip connections are similar to those of U-Net3+, except the single inter-encoder-decoder plain skip connections are all replaced by two plain skip connections of DSCM. To support the two connections in DS-UNet3+, we develop a dual skip feature aggregation mechanism (abbr. DSFAM). The DSFAM consists of a different number of filters when compared to U-Net3+. For example, Figure \ref{fig:dsunet3plus-De3} illustrates the construction of feature maps in the third decoder layer of DS-UNet3+. 

  \begin{figure*}[!hbt]
  \centering
  \includegraphics[width=18cm]{images/neupa4.png}
  \caption{Illustration of the proposed DS-UNet3+ networks DS-UNet3+(L), DS-UNet3+(S), and DS-UNet3+(A).}
      \label{fig:dsunet3plus}
      \end{figure*}
      
  \begin{figure}[!hbt]
  \centering
  \includegraphics[width=8cm]{images/neupa5.png}
  \caption{Illustration of dual skip feature aggregation mechanism (abbr. DSFAM) at the third decoder layer $X_{De}^{3}$ of DS-UNet3+ (figure adapted and modified from \cite{huang2020unet}).}
      \label{fig:dsunet3plus-De3}
      \end{figure}
      
The $X_{De}^{3}$ receives the feature maps of same-scale encoder layer $X_{En}^{3}$ with two plain skip connections. The feature maps of smaller-scale encoder layer $X_{En}^{2}$ and $X_{En}^{1}$ carrying coarse-grained detailed information are transported through two inter encoder-decoder skip connections supported by down-scaling operation $\mathcal{D}(\cdot)$ of non-overlapping max pooling operations. Finally, the feature maps from larger-scale decoder layer $X_{De}^{4}$ and $X_{De}^{5}$ carrying fine-grained semantic information are transported through intra-decoder connections supported by upsampling operation $\mathcal{U}(\cdot)$ of bilinear interpolation. In case of $X_{De}^{4}$, it receives an upsampled output of $X_{En}^{5}$. Similar to U-Net3+, the number of channels in all incoming same-resolution feature maps is unified with 64 filters of 3x3 size. The output feature map $f$ of a $X_{De}^{n}$ for $n=4,\dots,1$ of a $N$-scaled DS-UNet3+ can be denoted as


\begin{equation}
\begin{split}
X_{De}^{n}=\mathcal{C} \big( \mathcal{C} \big( \mathcal{U}(X_{De}^{n+1}) \big) \big)  
\oplus 
\underbrace{ \mathcal{DS}_{F} \big( X_{En}^{n}, \dots, \mathcal{D}_{2i}(X_{En}^{1})\big) }_{\text{Scales}: \ n^{th} \sim 1^{th}} 
\\
\oplus  
\underbrace{ \mathcal{S}_{De} \big( \mathcal{U}_{2j}(X_{De}^{n}), \dots, \mathcal{U}_{2j}(X_{De}^{N}) \big) }_{\text{Scales}: \ (n+1)^{th} \sim (N-1)^{th}},
\\
i = k \ (\text{for} \ k=1 \ \text{to} \ n),
\\
j = m \ (\text{for} \ m=1 \ \text{to} \ n-k+1)
\label{eqn:DSFAM}
\end{split}
\end{equation}

where, $\mathcal{C}(\cdot)$ denotes unpadded 3x3 convolution activated with ReLU $\sigma(\cdot)$. $\mathcal{DS}_{F}(\cdot)$ denotes DFSCM that brings the features from $\mathcal{S}_{1}$ (refer to Eqn. \ref{eqn:DSCM-skip1}) and $\mathcal{S}_{2}$ (refer to Eqn. \ref{eqn:DSCM-skip2}) from each encoder layers $X_{En}^{n}, \dots, X_{En}^{1}$. Similar to the DS-UNets, the second and third convolutional layer in each encoder block is followed by a BN. $\mathcal{S}_{De}(\cdot)$ refers to the intra-decoder connections that bring the features from previous decoder layers $X_{De}^{n}, \dots, X_{De}^{N}$. The value of $i$ and $j$ denote stride size for down-sampling operation $\mathcal{D}_{2i}(\cdot)$ and bilinear up-sampling operation $\mathcal{U}_{2j}(\cdot)$ respectively. Here, $i$ ranges from $1$ to $n$ and $j$ ranges from $1$ to $n-i+1$ depending upon the value of $n$.


Similar to DS-UNet and DS-ResUNet, three variants of DS-UNet3+ are studied: DS-UNet3+(L), DS-UNet3+(S), and DS-UNet3+(A). In DS-UNet3+(A), the DSFAM concatenates 9, 8, 7, and 6 sets of feature maps with a total of 576, 512, 448, and 384 filters of 3x3 size on decoder layers $X_{De}^{4}$, $X_{De}^{3}$, $X_{De}^{2}$, and $X_{De}^{1}$ respectively. In DS-UNet3+(L), the DFSCM is integrated between third ($X_{En}^{3}$ to $X_{De}^{3}$) and fourth ($X_{En}^{4}$ to $X_{De}^{4}$) scale layers of U-Net3+. Similarly, in the small-scale variation DS-UNet3+(S), the DFSCM is integrated between first ($X_{En}^{1}$ to $X_{De}^{1}$) and second ($X_{En}^{2}$ to $X_{De}^{2}$) scale layers. The proposed DS-UNet3+ networks consist of fewer network parameters than U-Net, ResUnet, DS-UNets, and DS-ResUNets.


\section{Dataset and Experimental Setup} \label{sec:experiments}

\subsection{Datasets} \label{sec:exp:data}
We experiment on three existing datasets of VHR and high-resolution type and a newly developed multi-resolution building footprint dataset.

The first dataset is the VHR WHU Building dataset \cite{ji2018fully} (abbr. WHU). It includes satellite images of Christchurch, New Zealand with a spatial resolution of 0.3m. To maintain uniform image size between all the datasets in our experiments, the originally 512x512 sized-image patches of the WHU are tiled into 256x256, increasing the number of image-label pairs by four times. Thus, 23088 training tiles and 9664 validation tiles are prepared.

The second dataset is the high-resolution (1m) Massachusetts Building dataset \cite{mnih2013machine}. The original 1500x1500 tiles are cropped to 256x256 by generating a grid of coordinates. The validation images provided are used for validation. The partial tiles on the edges of the tiles are ignored, by iterating only through the Cartesian product between the two intervals $range(0, h-h \bmod d, d)$ and $range(0,w-w \bmod d, d)$ for width $w$, height $h$, and output tile size of 256 as $d$. Let $H=h-h \bmod d$ and $W=w-w \bmod d$. Then the Cartesian product of the two intervals is the set of all ordered pairs $(i,j)$, and can be defined as

\begin{equation}
\begin{split}
\mathcal{P} = \{(i,j)\ |\ 0\le i<H, 0\le j<W, i\in\mathbb{N}, j\in\mathbb{N}, \\  i\equiv 0\ (\textrm{mod}\ d), j\equiv 0\ (\textrm{mod}\ d)\}
\label{eqn:cartesian}
\end{split}
\end{equation}

where, $i$, and $j$ are non-negative integers among the set of natural numbers $\mathbb{N}$.

The third dataset that we have developed (label, image) is named as Melbourne building footprint dataset (abbr. MELB). It is the first multi-resolution building dataset that covers samples from a complex urban environment i.e., the City of Melbourne, Australia. The labels are developed by masking and tiling the building roof samples provided by the City of Melbourne. The corresponding image tiles of 0.3m, 0.6m, and 1.2m spatial resolution of the labels are collected from Nearmap's API service. The number of training and validation samples are divided as 70\% and 30\% respectively. Therefore, this dataset is prepared in an end-to-end manner, without manual annotations. It is made sure that the image and labels are of the same projection system and of the same year.

The fourth dataset is a 1.2m subset of the MELB dataset that we developed in our previous work \cite{neupane2022building}. We use this dataset to experiment with the proposed networks on high-resolution images.


\subsection{Comparison to state-of-the-art} \label{sec:exp:sota}
The proposed variations of DS-UNet, DS-ResUNet, and DS-UNet3+ are compared to the vanilla U-Net, ResUnet, and U-Net3+. Furthermore, the networks are also compared to U-Net++ \cite{zhou2019unet++}, Deeplabv3+ \cite{chen2018encoder}, and SegNet \cite{badrinarayanan2017segnet} with VGG-16 encoder. The encoder of the SegNet is chosen to be VGG-16 to keep the network parameters similar to the U-Net that we compare to. No components such as attention gates, deep supervision, and classification-guided module (CGM) are added to the vanilla version of the SOTA networks. For uniformity in experiments, the same loss function is used to evaluate all networks. A dice loss \cite{sudre2017generalised} is used as the loss function to monitor the proposed and the compared networks in our experiments. This loss calculates the measure of overlap to assess the performance of segmentation when a ground truth (GT) is available. Furthermore, it maximises the dice coefficient (aka. F1 score) and minimises the possible class imbalance between the number of ``background'' and ``building pixels'' in binary classification. Dice loss is denoted by (\ref{eqn:diceloss}) as

    \begin{equation}\label{eqn:diceloss}
        \mathcal{L}(y,\hat{p}) = 1 - \frac{2y\hat{p}+1} {y+\hat{p}+1} 
    \end{equation}
    
where $y$ and $\hat{p}$ represent the GT and prediction respectively. The smooth value of 1 is added in the numerator and denominator considering the edge case scenario of $y=\hat{p}=0$. The product $y\hat{p}$ represents the intersection between the GT and prediction.

\subsection{Implementation details} \label{sec:exp:train}
All DL networks are wrapped in the Keras framework with a mini-batch size of 2. The step/epoch is set as the ratio of the number of training images to the batch size. The epoch is set such that the total number of steps is kept the same or similar (approx. 60000). A learning rate of 1e-4 is used to train all the networks. \textit{Adam}, \textit{He Normal}, and ReLU) are the optimizer, initializer, and activation functions respectively. A \textit{sigmoid} function is used to obtain the final output maps as the dataset is binary, and a \textit{dropout} of 50\% is used to avoid over-fitting. The hyper-parameters are kept the same to train all the proposed, and SOTA networks.

\subsection{Evaluation Metrics} \label{sec:exp:eval}
The evaluation of the networks is performed using (i) pixel accuracy (PA), (ii) adjusted accuracy (AA), (iii) F1 score (F1), (iv) intersection over union (IoU), and (v) Matthews correlation coefficient (MCC). PA (\ref{eqn:pixelacc}) measures the frequency of match between the predictions and the binary labels. AA (\ref{eqn:avgacc}) is an average of Sensitivity (\ref{eqn:sensitivity}) and specificity (\ref{eqn:specificity}), which quantify the proportion of correctly identified actual positives and actual negatives respectively. F1 (\ref{eqn:f1score}) and IoU (\ref{eqn:iou}) are calculated from the `area of overlap' between prediction and binary labels and `area of union' (all of the predictions + binary labels - the overlap). Lastly, MCC measures the disagreement between the prediction from the binary labels with consideration of the ratio between positive and negative elements (\ref{eqn:mcc}) \cite{chicco2020advantages}. Symbolic representation of the metrics are:

    \begin{equation}\label{eqn:pixelacc}
        Pixel\ accuracy\ (PA) = \frac{TP + TN} {TP + TN + FP + FN} 
    \end{equation} 
    
    \begin{equation}\label{eqn:avgacc}
        Adjusted\ accuracy\ (AA) = \frac{Sensitivity + Specificity} {2} 
    \end{equation} 
    
    \begin{equation}\label{eqn:sensitivity}
        Sensitivity = \frac{TP} {TP + FN}
    \end{equation}
    
    \begin{equation}\label{eqn:specificity}
        Specificity = 1 - \frac{TN} {TN + FP}
    \end{equation}

    \begin{equation}\label{eqn:f1score}
        F1 score = \frac{2 \times TP} {2 \times TP + FN + FP} 
    \end{equation}
    
    \begin{equation}\label{eqn:iou}
        IoU = \frac{TP} {TP + FN + FP}
    \end{equation}
    
    \begin{equation}\label{eqn:mcc}
        MCC = \frac{(TP \times TN) - (FP \times FN)} {\sqrt{(TP + FP) (TP + FN) (TN + FP ) (TN + FN)}}
    \end{equation}
    
where, TP is true positive (i.e., prediction = 1, label = 1); FP is false positive (prediction = 1, label = 0); FN is false negative (prediction = 0, label = 1); and TN is true negative (prediction = 0, label = 0). Other than the five accuracy metrics, the number of network parameters (abbr. Par.) are also compared.


\section{Results and Discussion} \label{sec:res}
The results and comparisons from the experiments are presented for three experimental settings categorised based on spatial resolutions: VHR, high-resolution, and multi-resolution. The proposed networks are compared to the vanilla versions of U-Net, ResUnet, U-Net3+, U-Net++, Deeplabv3+, and SegNet. Further, the DSCM is studied at different scales of U-Net and differently scaled U-Nets as an ablation study. In overall, 70 experiments are performed in this section to evaluate the proposed dual skip connection mechanisms in U-Net, ResUnet, and U-Net3+. The limitations of the experiments finalise the discussion at the end of this section.

\subsection{Results on VHR building dataset (WHU)} \label{sec:res:VHR}
Table \ref{tab:res:VHR} presents the performance of three versions of DS-UNet, DS-ResUNet, and DS-UNet3+ on the VHR benchmark WHU building dataset. The proposed networks are compared to their original architectures of vanilla U-Net, ResUnet, and U-Net3+, and also to U-Net++, Deeplabv3+, and SegNet. Figure \ref{fig:results} shows the sample results from all the proposed and original vanilla networks on the WHU dataset. 

\begin{table}[]
\centering
\caption{DS-UNet, DS-ResUNet, and DS-UNet3+ on 0.3m VHR WHU building dataset. The highest scores are highlighted in bold in each group of experiments of DS-UNets, DS-ResUNets, and DS-UNet3+.}
\label{tab:res:VHR}
\centering
\begin{tabular}{lcccccc}
\hline
Networks & Par. (M) & PA & AA & F1 & IoU & MCC \\ \hline
U-Net & 31.041 & 0.973 & 0.826 & 0.844 & 0.770 & 0.681 \\
DS-UNet-L & 36.943 & \textbf{0.976} & \textbf{0.851} & \textbf{0.865} & \textbf{0.799} & \textbf{0.701} \\
DS-UNet-S & 31.410 & 0.973 & 0.849 & 0.853 & 0.781 & 0.683 \\
DS-UNet-A & 37.312 & 0.975 & 0.837 & 0.848 & 0.777 & 0.691 \\ \hline
ResUNet & 75.346 & 0.969 & 0.833 & 0.822 & 0.741 & 0.665 \\
DS-ResUNet-L & 85.024 & 0.972 & 0.845 & 0.833 & 0.760 & 0.688 \\
DS-ResUNet-S & 75.951 & 0.969 & \textbf{0.853} & 0.800 & 0.720 & 0.670 \\
DS-ResUNet-A & 85.629 & \textbf{0.974} & 0.842 & \textbf{0.844} & \textbf{0.770} & \textbf{0.690} \\ \hline
U-Net3+ & 22.891 & \textbf{0.981} & \textbf{0.855} & 0.688 & 0.615 & \textbf{0.723} \\
DS-UNet3+(L) & 27.101 & 0.967 & 0.838 & 0.822 & 0.741 & 0.665 \\
DS-UNet3+(S) & 23.335 & 0.974 & 0.851 & 0.835 & 0.761 & 0.692 \\
DS-UNet3+(A) & 27.359 & 0.976 & 0.850 & \textbf{0.863} & \textbf{0.795} & 0.697 \\ \hline
U-Net++ & 34.538 & 0.974 & 0.828 & 0.823 & 0.749 & 0.685 \\
Deeplabv3+ & 11.852 & 0.975 & 0.854 & 0.852 & 0.784 & 0.696 \\
SegNet & 29.458 & 0.969 & 0.831 & 0.666 & 0.584 & 0.673 \\ \hline
\end{tabular}
\end{table}

\subsubsection{DSCM for U-Net (DS-UNets)} \label{sec:res:VHR:dsunets}
All three versions of the proposed DS-UNet outperform U-Net on the WHU dataset in terms of all accuracy measures as shown in Table \ref{tab:res:VHR}. DS-UNet-L shows the highest performance among the three versions and also outperforms all compared SOTA networks. DS-UNet-S outperforms DS-UNet-A in terms of AA, F1, and IoU. The results on the VHR dataset show that concatenating the doubled large-scale features in U-Net with DSCM results in the highest performance with about a 14\% increase in network parameters. The small-scale features can be kept the same without being doubled as seen from DS-UNet-A. This supports our initial argument that the trade-off between the use of context and precise location can be improved in U-Net. This can be done by increasing the large-scale feature maps, where the contexts are lost due to down-sampling operations such as max-pooling.

\subsubsection{DRSCM for ResUnet (DS-ResUNet)}\label{sec:res:VHR:dsresunets}
All three DS-ResUNets outperform the vanilla ResUnet network. Among the three, DS-ResUNet-A achieves the highest in four out of five evaluation measures. The smallest variant DS-ResUNet-S outperformed ResUnet in three evaluation measures with approx. 0.8\% increase in network parameter. Similarly, the largest variant DS-ResUNet-A outperformed ResUnet in all measures with 13.6\% rise in network parameters. The results are shown in Table \ref{tab:res:VHR}.

\subsubsection{DFSCM for U-Net3+ (DS-UNet3+)} \label{sec:res:VHR:dsunet3plus}
The results demonstrate the success of DSCM and DRSCM in plain skip connections in U-Net and ResUnet on the VHR dataset. However, the results from DFSCM on DS-UNet3+ show mixed performance when compared to U-Net3+. All three DS-UNet3+ networks significantly outperform U-Net3+ in terms of F1 (0.863 vs. 0.688) and IoU (0.795 vs. 0.615), while producing 1-2\% lower PA (0.976 vs. 0.981), AA (0.850 vs. 0.855), and MCC (0.697 vs. 0.723). The significant increase in F1 and IoU is worth noting with approx. 16\% increase and 12\% decrease in network parameters of the largest variant DS-UNet3+(A) compared to U-Net3+ and U-Net respectively. The results are shown in Table \ref{tab:res:VHR}.

The proposed DS-UNet3+ networks report higher F1 and IoU, and lower MCC. There could be different reasons for such inconsistencies. One reason is class imbalance. MCC can be less sensitive to class imbalance compared to F1 and IoU, depending on the dataset and the degree of imbalance. Looking at the performance of DS-UNet3+, we can say that WHU datasets have imbalanced classes of ``buildings'' and ``non-buildings''. But the other models of DS-UNet and DS-ResUNet variants show consistency in F1, IoU and MCC. Therefore, DS-UNet3+ could be performing differently than the other models in certain areas of the images within the dataset. Simply because MCC involves TNs, which could be affected differently than TPs by the difference in model performance across the image. More investigation is required to make conclusions on this inconsistency.

\subsubsection{Ablation Study of DS-UNets} \label{sec:res:VHR:ablation}
Studies have shown that not all skip connections improve the performance of U-Net \cite{wang2022uctransnet}. Table \ref{tab:res:VHR} supports this argument. Therefore, an ablation study is conducted on the VHR WHU Building dataset to verify the effectiveness of the proposed DSCM on different scale layers of U-Net and different scaled U-Nets. The design of this ablation study is categorised into two groups. The first group of ablation experiments take a U-Net of five-scale layers as shown in Figure \ref{fig:dualskipconnection}(a) before, and applies DSCM between each scale layer of its encoder and decoder i.e., $X_{En}^{n}$ and $X_{De}^{n}$. Figure \ref{fig:ablation}(a)-(d) illustrates the four DS-UNet formed in this group of experiments: DS-UNet-1, DS-UNet-2, DS-UNet-3, and DS-UNet-4. The second group of ablation experiments investigates the smaller DS-UNets with a lower number of scales as illustrated in Figures \ref{fig:ablation}(e)-(g). Unlike the DS-UNets of five scale layers, these experiments compare DS-UNets of 4, 3, and 2 scale layers and are named DS-UNet-4sc, DS-UNet-3sc, and DS-UNet-2sc respectively. All networks in the two ablation groups are experimented on 0.3m VHR WHU building dataset.

  \begin{figure*}[!hbt]
  \centering
  \includegraphics[width=14cm]{images/neupa6-2.png}
  \caption{Illustration of the seven DS-UNet networks for ablation study. Sub-figure (a)-(d) and sub-figure (e)-(g) show the DS-UNets of the first and second group of ablation experiments respectively.}
      \label{fig:ablation}
      \end{figure*}

The performance evaluation of the first ablation group is shown in Table \ref{tab:res:indiscale}. All variants except DS-UNet-3 outperform the vanilla U-Net in terms of PA, AA, and MCC. Only the DS-UNet-4 outperform U-Net in term of F1 and IoU, which are seen to have a gradual increase in numbers from DS-UNet-1 to DS-UNet-4. DSCM on the fourth scale layer of U-Net, therefore, aids in DS-UNet-L in the previous experiments to achieve the highest evaluation scores.

\begin{table}[]
\centering
\caption{First group of ablation: proposed DSCM between each scale of encoder-decoder layers, and their comparison to U-Net.}
\label{tab:res:indiscale}
\centering
\begin{tabular}{lcccccc}
\hline
Networks & Par. (M) & PA & AA & F1 & IoU & MCC \\ \hline
U-Net & 31.041 & 0.973 & 0.826 & 0.844 & 0.770 & 0.681 \\
DS-UNet-1 & 31.114 & 0.977 & 0.842 & 0.674 & 0.597 & 0.707 \\
DS-UNet-2 & 31.336 & \textbf{0.982} & \textbf{0.853} & 0.690 & 0.616 & \textbf{0.724} \\
DS-UNet-3 & 32.221 & 0.972 & 0.827 & 0.833 & 0.758 & 0.680 \\
DS-UNet-4 & 35.761 & 0.976 & 0.844 & \textbf{0.855} & \textbf{0.786} & 0.693 \\ \hline
\end{tabular}
\end{table}

Table \ref{tab:res:smallscales} presents the comparison of the three DS-UNets and their corresponding U-Nets from the second group of the ablation experiment. These three DS-UNets integrate DSCM on only the largest scale layer available in the network structure as the first group of ablation experiments shows that the DSCM between the largest scale layers yields the highest performance gain in terms of F1 and IoU. As reported, DSCM does not improve the evaluation measures of 2-scale and 3-scale U-Nets but improves those of 4-scale U-Net. Also, when compared to the 5-scale DS-UNet (DS-UNet-4 from Table \ref{tab:res:indiscale}), it can be seen that the performance rises along with the number of scales, with the 5-scale DS-UNet as the best performer. When compared to the 5-scale U-Net (U-Net from Table \ref{tab:res:indiscale}), the 4-scale DS-UNet achieves a slightly higher AA score with 22M fewer parameters.

\begin{table}[]
\centering
\caption{Second group of ablation: proposed DSCM on 4, 3, and 2 scale U-Nets.}
\label{tab:res:smallscales}
\centering
\begin{tabular}{lcccccc}
\hline
Networks & Par. (M) & PA & AA & F1 & IoU & MCC \\ \hline
UNet-2sc & 0.405 & 0.936 & 0.803 & 0.635 & 0.532 & 0.585 \\
DS-UNet-2sc & 0.479 & 0.944 & 0.788 & 0.625 & 0.519 & 0.579 \\ \hline
UNet-3sc & 1.865 & 0.957 & 0.846 & 0.756 & 0.672 & 0.665 \\
DS-UNet-3sc & 2.161 & 0.962 & 0.839 & 0.750 & 0.663 & 0.649 \\ \hline
UNet-4sc & 7.702 & 0.970 & 0.833 & 0.796 & 0.717 & 0.673 \\
DS-UNet-4sc & 8.883 & 0.969 & 0.839 & 0.817 & 0.738 & 0.675 \\ \hline
\end{tabular}
\end{table}


\subsection{Results on high-resolution dataset (Massachusetts)} \label{sec:res:massachusetts}
Table \ref{tab:res:mass} presents the results of all proposed and SOTA networks on the high-resolution Massachusetts building dataset. All three DS-UNets and DS-ResUNets outperform U-Net and ResUnet, with the highest in all five metrics from DS-UNet-L and DS-ResUNet-A respectively. Once again, the large-scale variant of DS-UNet and all-scale variant of DS-ResUNet perform better in high-resolution dataset. All three variants of DS-UNet3+ outperform U-Net3+ in terms of F1 and IoU. The other measures are lower than the U-Net3+. All the proposed networks outperform the compared SOTA networks. Figure \ref{fig:results} shows the segmentation results of all proposed and the original vanilla networks. 

\begin{table}[]
\centering
\caption{DS-UNet, DS-ResUNet, and DS-UNet3+ on 1m high-resolution Massachusetts building dataset. The highest scores are highlighted in bold in each group of experiments.}
\label{tab:res:mass}
\centering
\begin{tabular}{lccccc}
\hline
Networks & PA & AA & F1 & IoU & MCC \\ \hline
U-Net & \textbf{0.953} & 0.879 & 0.749 & 0.609 & 0.751 \\
DS-UNet-L & 0.952 & \textbf{0.892} & \textbf{0.756} & \textbf{0.618} & \textbf{0.760} \\
DS-UNet-S & 0.951 & 0.878 & 0.747 & 0.607 & 0.752 \\
DS-UNet-A & 0.951 & 0.889 & 0.752 & 0.612 & 0.754 \\ \hline
ResUNet & \textbf{0.949} & 0.880 & 0.770 & 0.633 & 0.744 \\
DS-ResUNet-L & 0.934 & 0.897 & 0.792 & 0.663 & 0.755 \\
DS-ResUNet-S & 0.932 & 0.884 & 0.785 & 0.649 & 0.745 \\
DS-ResUNet-A & 0.937 & \textbf{0.892} & \textbf{0.800} & \textbf{0.671} & \textbf{0.763} \\ \hline
U-Net3+ & \textbf{0.953} & \textbf{0.889} & 0.757 & 0.617 & \textbf{0.760} \\
DS-UNet3+(L) & 0.931 & 0.881 & \textbf{0.782} & \textbf{0.649} & 0.743 \\
DS-UNet3+(S) & 0.929 & 0.881 & 0.773 & 0.635 & 0.735 \\
DS-UNet3+(A) & 0.952 & 0.887 & 0.750 & 0.611 & 0.754 \\ \hline
U-Net++ & 0.945 & 0.858 & 0.741 & 0.596 & 0.712 \\
Deeplabv3+ & 0.942 & 0.861 & 0.733 & 0.586 & 0.705 \\
SegNet & 0.944 & 0.864 & 0.730 & 0.583 & 0.713 \\ \hline
\end{tabular}
\end{table}

\subsection{Results on multi-resolution dataset (MELB)} \label{sec:res:multi}
Table \ref{tab:res:multi} presents the results of all proposed and SOTA networks on the multi-resolution MELB dataset. Among the three DS-UNets, the DS-UNet-L and DS-UNet-A outperform U-Net on the MELB dataset, while DS-UNet-S fall behind U-Net. DS-UNet-L shows the highest PA, AA, and MCC. DS-UNet-A shows the highest F1 and IoU. All DS-ResUNets outperform ResUnet with DS-ResUNet-S being the highest in all measures. The DS-UNet3+(L) and DS-UNet3+(S) outperform U-Net3+ in terms of F1 and IoU.  All proposed networks outperform the compared SOTA networks. Figure \ref{fig:results} shows the segmentation results of all the networks on two samples: a high-rise building and a complex building structure.

\begin{table}[]
\centering
\caption{DS-UNet, DS-ResUNet, and DS-UNet3+ on multi-resolution (0.3m + 0.6m + 1.2m) MELB dataset.  The highest scores are highlighted in bold in each group of experiments.}
\label{tab:res:multi}
\centering
\begin{tabular}{lccccc}
\hline
Networks & PA & AA & F1 & IoU & MCC \\ \hline
U-Net & 0.892 & 0.893 & 0.826 & 0.718 & 0.752 \\
DS-UNet-L & \textbf{0.902} & \textbf{0.896} & 0.829 & 0.720 & \textbf{0.769} \\
DS-UNet-S & 0.894 & 0.891 & 0.824 & 0.716 & 0.751 \\
DS-UNet-A & 0.898 & 0.894 & \textbf{0.834} & \textbf{0.726} & 0.761 \\ \hline
ResUNet & 0.894 & 0.888 & 0.823 & 0.714 & 0.749 \\
DS-ResUNet-L & 0.894 & 0.893 & 0.825 & 0.716 & 0.752 \\
DS-ResUNet-S & \textbf{0.895} & \textbf{0.894} & \textbf{0.826} & \textbf{0.718} & \textbf{0.754} \\
DS-ResUNet-A & \textbf{0.895} & 0.892 & \textbf{0.826} & 0.717 & 0.753 \\ \hline
U-Net3+ & \textbf{0.904} & \textbf{0.899} & 0.832 & 0.724 & \textbf{0.773} \\
DS-UNet3+(L) & 0.903 & 0.894 & 0.834 & 0.728 & 0.766 \\
DS-UNet3+(S) & 0.901 & 0.894 & \textbf{0.836} & \textbf{0.729} & 0.766 \\
DS-UNet3+(A) & 0.897 & 0.895 & 0.831 & 0.724 & 0.760 \\ \hline
U-Net++ & 0.890 & 0.888 & 0.820 & 0.709 & 0.744 \\
Deeplabv3+ & 0.891 & 0.882 & 0.819 & 0.708 & 0.742 \\
SegNet & 0.897 & 0.885 & 0.819 & 0.705 & 0.751 \\ \hline
\end{tabular}
\end{table}

  \begin{figure}[!hbt]
  \centering
  \includegraphics[height=17.5cm]{images/neupa8-2.png}
  \caption{Segmentation output from the proposed networks and their original vanilla baseline networks on WHU, MASS, and MELB building datasets. (a) Sample image tile, (b) U-Net, (c) DS-UNet-L, (d) DS-UNet-S, (e) DS-UNet-A, (f) ResUnet (g) DS-ResUNet-L, (h) DS-ResUNet-S, (i) DS-ResUNet-A, (j) U-Net3+, (k) DS-UNet3+(L), (l) DS-UNet3+(S), (m) DS-UNet3+(A). There are two samples for each of the three datasets. For the MELB dataset, the first sample shows the segmentation results on a high-rise building of 63.88m in height. The second sample shows the complexity of the building samples in the dataset.}
      \label{fig:results}
      \end{figure}

\subsection{Results on 1.2m subset of MELB dataset} \label{sec:res:MELB12}
The proposed networks outperform their original counterparts on the benchmark VHR and high-resolution dataset in all accuracy measures and in the majority of measures on the multi-resolution MELB dataset. Table \ref{tab:res:melb12} presents the results from the networks on the 1.2m high-resolution building footprint dataset, which is a subset of MELB. The comparison of the DS-UNets shows reduced PA, AA, and MCC by up to 2-5\% when compared to U-Net. However, a significant increase of up to approx. 20\% is observed in terms of F1 and IoU from all versions of DS-UNets. Similarly, DS-ResUNet-S and DS-ResUNet-L outperform ResUnet in terms of three and two measures respectively. Similar to the MELB dataset, DS-UNet3+(L) and DS-UNet3+(S) outperform U-Net3+ in terms of F1 and IoU. The majority of the proposed networks outperform the compared SOTA networks in most of the accuracy measures. A mixed performance is seen in this subset of the dataset because of the inability of the networks to generalise complex urban buildings in 1.2m spatial resolution. VHR and multi-resolution datasets like MELB are more reliable to extract urban buildings.

\begin{table}[]
\centering
\caption{DS-UNet, DS-ResUNet, and DS-UNet3+ on 1.2m subset of MELB dataset. The highest scores are highlighted in bold in each group of experiments.}
\label{tab:res:melb12}
\centering
\begin{tabular}{lccccc}
\hline
Networks & PA & AA & F1 & IoU & MCC \\ \hline
U-Net & 0.928 & \textbf{0.764} & 0.437 & 0.342 & \textbf{0.479} \\
DS-UNet-L & 0.942 & 0.722 & 0.617 & 0.523 & 0.425 \\
DS-UNet-S & \textbf{0.947} & 0.703 & \textbf{0.636} & \textbf{0.547} & 0.411 \\
DS-UNet-A & 0.942 & 0.721 & 0.635 & 0.543 & 0.415 \\ \hline
ResUNet & 0.872 & 0.745 & 0.545 & 0.451 & 0.351 \\
DS-ResUNet-L & 0.924 & \textbf{0.743} & 0.527 & 0.429 & \textbf{0.404} \\
DS-ResUNet-S & \textbf{0.933} & 0.731 & \textbf{0.561} & \textbf{0.467} & 0.403 \\
DS-ResUNet-A & 0.919 & 0.688 & 0.520 & 0.416 & 0.344 \\ \hline
U-Net3+ & \textbf{0.953} & \textbf{0.778} & 0.471 & 0.380 & \textbf{0.496} \\
DS-UNet3+(L) & 0.940 & 0.727 & \textbf{0.603} & \textbf{0.502} & 0.415 \\
DS-UNet3+(S) & 0.938 & 0.701 & 0.602 & 0.506 & 0.393 \\
DS-UNet3+(A) & 0.945 & 0.772 & 0.439 & 0.343 & 0.471 \\ \hline
U-Net++ & 0.930 & 0.728 & 0.602 & 0.510 & 0.389 \\
Deeplabv3+ & 0.940 & 0.684 & 0.570 & 0.473 & 0.378 \\
SegNet & 0.854 & 0.715 & 0.356 & 0.262 & 0.346 \\ \hline
\end{tabular}
\end{table}


\subsection{Limitations} \label{sec:res:limit}
A common experimental setup is used to compare the proposed networks, the original vanilla networks, and other SOTA networks. The experiments are designed to focus on the efficiency gain in the segmentation of VHR and high-resolution EO images. No pre-processing and post-processing methods are used for the regularisation of building boundaries. The MELB dataset is prepared end-to-end without manual annotations, with images from an API service of Nearmap and labels from a secondary source. However, due to the UAV-based sources, the dataset is affected by the off-nadir angle of source images. This problem results in a misalignment between images and labels, which rises along with the height of the buildings. The misalignment further rises in the higher spatial resolution subset if measured in pixels. This means that in a 256x256 tile, there is more misalignment between an object (building) and its label in 0.3m spatial resolution when compared to a 1.2m resolution image. Therefore, training only on a lower-resolution dataset might not result in a higher performance score, but it definitely assists in the generalisation of objects if bundled together with a higher-resolution dataset. The majority of the publicly available building footprint dataset covers residential buildings with no complex high-rises and skyscrapers. The end-to-end prepared multi-resolution MELB dataset could therefore be useful for the precise and automated extraction of urban buildings.

\section{Conclusion} \label{sec:conclusion}
This paper dissects and re-thinks the network configuration of three widely used state-of-the-art encoder-decoder CNN-based deep learning architectures for building footprint extraction. The particular focus is on re-designing the skip connections of U-Net, ResUnet, and U-Net3+ that transport the learned vectors from their CNN encoder to their decoder. Three dual skip connection mechanisms $-$ DSCM, DRSCM, and DFSCM $-$ are designed for U-Net, ResUnet, and U-Net3+ respectively to compensate for their limitations and to achieve efficiency gain. Moreover, they are aimed to provide a more accurate trade-off between the use of context and precise localisation for semantic segmentation of VHR and high-resolution EO images. Further, a dual full-scale skip connection mechanism (DFSCM) is proposed for the U-Net3+. DFSCM incorporates the increased low-level context information with high-level semantics from the feature maps at different scales using a multi-scale feature aggregation technique. Unlike U-Net3+, the aggregation proposed aggregates the multi-scale feature maps with increased weights for the smaller-scale feature maps, where the context information is still intact.

The three proposed mechanisms double the feature maps in the encoder before passing them to the decoder and they can be plugged into specific scale layers of the networks. With this advantage, the dual skip connection mechanism is tested in different scale layers of the three networks, resulting in a total of nine networks with different configurations of skip connection. Furthermore, an ablation study is provided to confirm the optimal scale layer for efficiency gain. The following conclusions can be made from our experiments on VHR and high-resolution building footprint datasets:

\begin{enumerate}
    \setcounter{enumi}{0}
    \item Enriching the plain-skip connections of U-Net with increased large-scale features from DSCM provides efficiency gain in extracting buildings from VHR and high-resolution EO images. 
    \item Increasing the largest-scale features (before the bottleneck) in U-Net produces the highest F1 and IoU. This works in U-Net with more than three scale layers (refer to Table \ref{tab:res:smallscales}).
    \item The residual blocks of ResUnet can be deepened with DRSCM for efficiency gain.
    \item U-Net3+ can yield a higher F1 and IoU with increased feature maps using DFSCM and DSFAM aggregation. In the VHR dataset, the proposed DS-UNet3+ networks achieved up to 17.7\% and 18.4\% gain in F1 and IoU respectively. Therefore, we conclude that assuming equal weights for all feature maps is a limitation of U-Net3+, which can be addressed by using DSFAM to give different weights to the different scales of feature maps that possess different levels of discrimination.
    \item The 1.2m subset of the MELB dataset among the three spatial resolutions of 0.3m, 0.6m, and 1.2m, is insufficient to generalise these complex buildings. However, it improves the performance scores when bundled together with higher-resolution samples, especially if there exists a misalignment between images and labels in complex urban settings. A multi-resolution dataset like ours can improve the networks affected by off-nadir images.
\end{enumerate}

The studies from this paper highlight several gaps in the literature that could be addressed in future works. The gaps are: (i) lack of studies to point out the optimal scale layers that need skip connections, (ii) the majority of EO-based high-resolution building datasets cover residential areas resulting in a lack of studies in complex urban settings that include high-rise and skyscrapers, (iii) lack of conclusions that point out the optimal resolution of EO images for urban feature extraction, and (iv) the misalignment between roof labels and UAV-based aerial images due to inaccurate ortho-rectification methods, resulting in inaccurate urban building footprint extraction. The contributions in this paper touch on some of these gaps, however, our future works will be dedicated to addressing and minimising the other gaps.

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgment
\section*{Acknowledgment}
The authors would like to thank Nearmap for providing very high-resolution aerial images through their API services. 


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{IEEEtran}
\bibliography{Reference}

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

%\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/neupa9.jpg}}]{Bipul Neupane} is a Geomatics Engineer who graduated from Kathmandu University, Nepal. He has a Master's degree in Engineering and Technology from Sirindhorn International Institute of Technology (SIIT), Thammasat University, Thailand.

He is currently a PhD Candidate at the Department of Infrastructure Engineering, University of Melbourne, Australia. Before, he was a Researcher at SIIT, Thammasat University, where he carried out several research works related to location intelligence using earth observation in urban and agricultural applications. His research interests include digital image processing and computer vision using sensing technologies. His past work includes being GIS/M\&E Officer for location intelligence in the health sector and land-use planning services.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/neupa10.png}}]{Jagannath Aryal} (M) received the PhD degree in optimization and systems modelling from the Centre for Advanced Computational Solutions (C-fACS), New Zealand, in 2010. 

He is currently an Associate Professor in Digital Infrastructure Engineering with the Faculty of Engineering and Information Technology (FEIT), Department of Infrastructure Engineering, University of Melbourne, Melbourne, Australia. His research interests include optimal utilization of Earth observation, geo-information, and geo-statistics to develop new methods in object recognition and geospatial situational awareness in disaster and emergency situations.

Dr Aryal serves as an Associate Editor for IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING. He leads the Remote Sensing and Photogrammetry Commission of SSSI, Australia as a National Chair. He is a Reviewer for remote sensing-related journals, including the IEEE and Elsevier Science
Journals. He also serves on the editorial board of the \textit{Journal of Spatial Science} of the Taylor and Francis Group.
\end{IEEEbiography}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/neupa11.png}}]{Abbas Rajabifard} received the PhD degree in spatial data infrastructure from the University of Melbourne, Australia. 

He is an internationally recognized scholar and Geospatial Engineer. He is the Discipline Leader in Digital Infrastructure Engineering, and Director of the Centre for SDIs and Land Administration at the University of Melbourne. He leads research in land administration, sustainability, and resilience, and Digital Twin for urban systems. He is an Advisory Board Member and former Chair of the United Nations Global Geospatial Information Management (UN-GGIM) Academic Network. He has led multiple projects nationally and internationally in improving urban geospatial and land systems, 3-D cadastre, and resilience impact, particularly in the Asia-Pacific, Europe, North America and Latin America, where he has developed strategies, and designed and implemented policies, technologies and applications in a wide variety of application areas.
\end{IEEEbiography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}