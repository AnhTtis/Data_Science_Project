
% \begin{figure}[bp]
% \hspace*{-0.1\textwidth}

\begin{algorithm}[H]
\caption{\textsc{BAM-QMDP}(episodes)}
\label{alg:bamqmdp_run}
\begin{algorithmic}
\For{$s,s' \in S $ and $a \in A$}
    \State Set $\alpha_{s,a,s'} = 1/|S|, N_Q(s,a) = 0 $
    \State Set $Q(s,a) = 0$, $Q_{opt}(s,a) = 1, R(s,a) = 0$
\EndFor
\State Set $r_{total} = 0$
\For{$i < $episodes}
    \State $r_{eps}, Q, Q_{opt}, \vec{\alpha}, R $ 
    \State \hspace{15pt} $\leftarrow$ \textsc{RunEpisode}($Q, Q_{opt}, \vec{\alpha}, R, b_0$)
    \State $r_{total} \leftarrow r_{total} + r_{eps}$
\EndFor
\State $\mathbf{return } r_{total}$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\caption{\textsc{RunEpisode} ($Q, Q_{opt},\vec{\alpha},R, b_0$)}
\label{alg:bamqmdp}
\begin{algorithmic}
\State $b \leftarrow b_0$
\State $r_{episode} = 0$
\While{episode not done}
    \State $a \xleftarrow{} $\textsc{FindGreedyAction}($Q_{\text{opt}},b$)
    \State $b_{\text{next}} \xleftarrow{} $\textsc{SampleNextBelief}($P,b,a$)
    \State $m \xleftarrow{} ( \exists s, b(s) = 1 \land \alpha_{s,a} < N_m ) \lor \mathbf{R}(b_{\text{next}}) <c$
    \State Take action $(a,m) \rightarrow (o,r)$
    \If{$m = 1$}
        \State $P,R, \vec{\alpha} \leftarrow $\textsc{UpdateModel}($R, \vec{\alpha}, b,a,o,r$)
        \State $b \leftarrow o$
    \Else
        \State $b \leftarrow b_{\text{next}}$
    \EndIf
    \State $Q, Q_{\text{opt}} \leftarrow $ \textsc{UpdateQ}($P,Q_{\text{}},b,a,r$)
    \State $Q, Q_{\text{opt}} \leftarrow$ \textsc{ModelBasedTraining}($P,Q,R$)
    
    \State $r_{episode} \leftarrow r_{episode}+r$
\EndWhile
\State $\mathbf{return } r_{episode}, Q, Q_{opt}, \vec{\alpha},R$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{\textsc{SampleNextBelief}($P,b,a$)}
\label{alg:sample_b}
\begin{algorithmic}
\For{$s \in S$}
    \For{$s' \in S$}
        \State $b_{\text{next,full}}(s') \leftarrow b_{\text{next,full}}(s') + P(b(s),a,s')$
    \EndFor
\EndFor
    \For{$i<N_b$}
        \State $s_{\text{next,i}} \sim b_{\text{next,full}}$
        \State $b_{\text{next}}(s_{\text{next,i}}) \leftarrow b_{\text{next}}(s_{\text{next,i}}) + 1/N_b$
    \EndFor
\State $\mathbf{return } b_{\text{next}}$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\caption{\textsc{FindGreedyAction}($Q,b$)}
\label{alg:find_action}
\begin{algorithmic}
\State initialise $Q_b(a) = 0, \forall a \in A$
\For{$ (s,a) \in S\times A$}
        \State $Q_b(a) \leftarrow Q_b(a) + b(s) Q(s,a)$
\EndFor
\State $\mathbf{return } \arg \max_{a\in A}Q_b(a)$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{\textsc{UpdateModel}($\vec{\alpha},b,a,o,r$)}
\label{alg:update_P}
\begin{algorithmic}
\If{ $[\exists s \in S | b(s) = 1]$}
    \State $\alpha_{s,a,o} \leftarrow  \alpha_{s,a,o} + 1$
    \State $\alpha_{s,a} \leftarrow \alpha_{s,a} + 1$
    \For{$s' \in S$}
        \State $P(s'\mid s,a) = \alpha_{s,a,s'} / \alpha_{s,a} $
    \EndFor
    \State $R(s,a) \leftarrow \frac{r + (\alpha_{o,a}-1)R(s,a)}{\alpha_{o,a}}$
\EndIf
\State $\mathbf{return } P, \vec{\alpha}, R$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{\textsc{UpdateQ}($P,Q,R,b,a,r$)}
\label{alg:update_Q}
\begin{algorithmic}
\For{$s \in S$}
    \State $\eta_{s} \leftarrow b(s)\eta$
    \State $\Psi \leftarrow \sum_{s' \in S}P(s'\mid s,a) \max_{a'}Q(s', a')$
    \State $Q(s,a) \leftarrow [1-\eta_{s}]Q(s,a) + \eta_{s} [r+\gamma \Psi]$
    \State $Q_{\text{opt}}(s,a) \leftarrow Q(s,a) + \text{R-Max*}(s,a) $
\EndFor
\State $\mathbf{return } Q, Q_{\text{opt}}$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\caption{\textsc{ModelBasedTraining}($Q, P, R$)}
\label{alg:update_Q_global}
\begin{algorithmic}
\For{$ i < N_{\text{train}}$}
    \State Pick a random $s \in S$
    \For{$s' \in S$}
        \State $b(s') \leftarrow \delta_{s,s'}$
    \EndFor
    \State $a \leftarrow \arg \max_{a} Q(s,a)$ with $p=\frac{1}{2}$, otherwise random
    \State $r \leftarrow R(s,a)$
    \State \textsc{UpdateQ}($P,Q,R,b,a,r$)
    
\EndFor
\State $\mathbf{return } Q, Q_{\text{opt}}$
\end{algorithmic}
\end{algorithm}

