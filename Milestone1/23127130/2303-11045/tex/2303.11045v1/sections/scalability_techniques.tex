\section{SoK: Scalability-enhancing Techniques}
\label{sect:scalability}

Improving the scalability of Byzantine consensus is an ongoing research field that
requires  the exploration, advancement, and combination of several methods and approaches.
In order to divide the different directions of approaches, we identified the following categories for scalability-enhancing techniques 
which aim to increase the scalability (and efficiency) of Byzantine consensus:

% \lj{TODO: Adjust the order below to fit both Fig4 and the sections below.}
\begin{itemize}[leftmargin=1em, itemsep=.1em, parsep=.1em, topsep=.1em,partopsep=.1em]
\item \textit{Communication topologies and strategies}. How can the communication flow be optimized? Increasing the communication efficiency might require a suitable \textit{communication topology}, \eg tree-based or overlay / gossip-based. 
\item \textit{Pipelining}. What benefit can be achieved by employing pipelining techniques for parallel executions of agreement instances?
\item \textit{Cryptographic primitives}. How can \textit{suitable cryptographic primitives} serve as building blocks for new scalability-enhancing techniques?
\item \textit{Independend groups}. How can transactions be ordered and committed \textit{in independent groups}, \eg through sharding?
\item \textit{Consensus committee selection}. How are roles in achieving agreement distributed among network nodes? Does one (or multiple) flexibly selected \textit{representative committee(s)} decide?
\item \textit{Hardware support}. How can we improve consensus efficiency using \textit{\acp{TEE}}?
\end{itemize}

%Our categorization of the ongoing efforts is shown in~\cref{fig:scalability}. 

In~\cref{fig:papersPerCategory}, we show an overview of how many papers we found to fit into each category.


\begin{figure}
    \centering
    \input{diagram/papersPerCategory}
    \caption{For each category: How many papers used at least one scalability-enhancing technique fitting to this category.}
    \label{fig:papersPerCategory}
\end{figure}


\subsection{Communication Topologies and Strategies} % Abstrakte Challenges herausarbeiten - z.B. mitigating tree latency in Kauri
\label{sec:communication-topologies}

Improving the scalability of coordinating agreement between a large number of $n$ nodes requires avoiding bottleneck situations, such as burdening too much communication effort on a single leader.
An asymmetric utilization of network or computing resources hinders scalability, as observed in many traditional BFT protocols resembling PBFT. 
Instead, one of the main goals of the \enquote{blockchain generation} of BFT protocols is to distribute the communication load among replicas as evenly as possible. 
For instance, some approaches try to balance network capabilities under a single leader by using message forwarding and aggregation along a communication tree~\cite{kogias2016enhancing, neiheiser2021kauri} or gossip~\cite{buchman2018latest}, while others rely on utilizing multiple leaders~\cite{stathakopoulou2019mir, alqahtani2021bigbft} or even work fully decentralized without a leader~\cite{crain2018dbft, voron2019dispel, crain2021red, antoniadis2021leaderless}. 

Another reason why the well-known PBFT protocol does not scale well is its all-to-all broadcast phases which make the network topology a \textit{clique}, which is usually always a bad choice for scalability as it means an incurred $O(n^2)$ number of messages (and necessary message authenticators). Unsurprisingly, this design is not an ideal fit for a large network size $n$ and was later replaced by protocols that only require linear message complexity by collecting votes~\cite{gueta2019sbft, hotstuff19}. 

In the following, we review and explain different approaches for improving communication flow in BFT protocols categorized by the network topology they employ.




\subsubsection{Star-based} 

Recently, many BFT protocols have proposed to employ linear communication complexity by employing a star-based communication topology~\cite{hotstuff19, gueta2019sbft, jalalzai2018window, jalalzai2019proteus, gupta2021poe, gelashvili2021jolteon, dang2019towards}. We briefly explain this idea and its limitations on the example of HotStuff~\cite{hotstuff19} (which is illustrated in~\cref{fig:scalability:communication:star}).


The HotStuff leader acts as a \textit{collector}, which gathers votes from other replicas, and, as soon as a quorum of votes is received, creates a \textit{quorum certificate} that can convince any node to progress to the next protocol stage. The rather costly \textit{view-change} subprotocol of PBFT can be avoided by adding a further protocol stage.
Thus, agreement in HotStuff requires four protocol stages ({\smaller\textsc{prepare}, \textsc{pre-commit}, \textsc{commit},} and {\smaller\textsc{decide}}) shown in \cref{fig:scalability:hotstuff-protocol}.
% which allows a new HotStuff leader to simply extend from the highest quorum certificate it knows of.

Important for scalability is that the cost of transmitting message authenticators can be reduced by a simple aggregation technique: The leader compresses $n-f$ signatures into a single threshold signature of fixed size. Because the threshold signature scheme uses the quorum size as a threshold, a valid threshold signature implies a quorum of replicas is among the signers. The threshold signature has the size of $O(1)$ -- a significant improvement over letting the leader transmit $O(n)$ individual signatures.
HotStuff also employs pipelining and leader rotation which are addressed in \cref{sect:pipelining}.

As we can see in~\cref{fig:scalability:hotstuff-protocol}, the communication flow still is imbalanced:
%
Every follower replica communicates only with the leader, but the leader still has to communicate with all other replicas. 
This bottleneck can be alleviated by tree-based or randomized topologies, as we explain next.

% HotStuff strives to mitigate this imbalance with two techniques: \textit{pipelining} and \textit{leader rotation}. Pipelining allows reusing a single quorum certificate for multiple instances of the agreement pattern, albeit pipelined-HotStuff cannot employ a variable pipeline stretch but is limited to the 4 protocol stages ({\smaller\textsc{prepare}, \textsc{pre-commit}, \textsc{commit},} and {\smaller\textsc{decide}}) shown in \cref{fig:scalability:hotstuff-protocol}. If agreement instances run in parallel, leader rotation can, to some extent, distribute the burden of being leader among these instances (yet, this has its limitations: the pipeline stretch is fixed, and the sequential executions of agreement can still lead to a bottleneck). Optimizing the vote aggregation and pipelining of HotStuff is explored in a more recent BFT protocol called Kauri with promising scalability improvements~\cite{neiheiser2021kauri}.
% Pipelining and leader rotation


\begin{figure}[t]
  \centering
\begin{subfigure}[h]{\columnwidth}
  \centering
    \includegraphics[width=0.35\textwidth]{img/star_topology.pdf}
    \caption{Star topology: A collector (C) collects votes and distributes quorum certificates (in Figure~\ref{fig:scalability:hotstuff-protocol}, replica $r_0$ has this role).}
    \label{fig:scalability:star-topology}
\end{subfigure}
\hspace{0.4cm}
\begin{subfigure}[h]{\columnwidth}
  \centering
    \includegraphics[width=\textwidth]{img/hotstuff.pdf}
    \caption{HotStuff has linear communication complexity: The leader collects the votes from the other replicas and distributed the quorum certificates. Aggregation can be achieved through threshold signatures.}
    \label{fig:scalability:hotstuff-protocol}
\end{subfigure}
\caption[]{Linear communication over a star-based network topology on the example of HotStuff.}
\label{fig:scalability:communication:star} 
\end{figure}
\begin{figure}[t]
  \centering
\begin{subfigure}[h]{\columnwidth}
  \centering
    \includegraphics[width=0.35\textwidth]{img/tree_topology.pdf}
    \caption{Tree topology: The proposer (P) disseminates messages  and collects votes along a tree. %, aggregation happens at each inner node of the tree.
    }
    \label{fig:scalability:tree-topology}
\end{subfigure}
\begin{subfigure}[h]{\columnwidth}
  \centering
    \includegraphics[width=\textwidth]{img/kauri.pdf}
    \caption{In Kauri, the leader disseminates a proposal along a communication tree. The inner nodes forward messages to their children and collect and aggregate their children's votes, to be returned to the parent.}
    \label{fig:scalability:kauri-protocol}
\end{subfigure}
\caption[]{Communication over a tree-based network topology on the example of Kauri.}
\label{fig:scalability:communication:tree} 
\end{figure}


\subsubsection{Tree-based} A few novel BFT protocols employ a tree-based communication topology~\cite{kogias2016enhancing, li2021scalable, liu2018scalable, neiheiser2021kauri}. This has the advantage that the leader is relieved of the burden of being the sole aggregation and dissemination point for votes and the generated quorum certificates.
The tree-based communication pattern can be introduced into either the PBFT algorithm, as done in ByzCoin~\cite{kogias2016enhancing}, or HotStuff, as done in Kauri~\cite{neiheiser2021kauri}, without requiring significant changes to the protocol.
ByzCoin was the first to show the potential of this topology, while Kauri later showed how to overcome the shortcomings of this approach.

The tree-based communication topology raises several problems, such as added latency, compared to the star topology and the necessity to react to failures of internal and leaf nodes.
In the following, we briefly explain these challenges and the solutions proposed.
\cref{fig:scalability:communication:tree} shows the communication pattern in Kauri.
Compared to the pattern in~\cref*{fig:scalability:communication:star}, it is clear that each level in the tree increases latency. 
To address this issue, Kauri only considers trees of height 3, as shown in the Figure.
Additionally, in the star topology, only leader failure is critical and typically requires a view change. 
In the tree topology, the failure of an internal node prevents the aggregation of votes from its children and thus requires reconfiguration.
Further, the failure of a leaf node may cause its parent to wait indefinitely for its contribution.
Kauri introduces a timeout for aggregation to handle leaf failure. 
To address the failure of internal nodes, Kauri proposes a reconfiguration scheme, which guarantees to find a correct set of internal nodes, 
given that failures lie below a threshold $k$.
Otherwise, Kauri falls back on a star-based topology.
The threshold $k$ here depends on the tree layout but lies at $\sqrt{n}$ for a balanced tree.

To achieve competitive throughput and utilize the available bandwidth despite the latency added through the tree overlay and through timeouts on leaf nodes, Kauri relies on pipelining. 
We will see more details on how pipelining can serve as a means for increasing scalability in~\cref{sect:pipelining}.


% In the timeline of BFT protocol design, this idea was first explored by Byzcoin~\cite{kogias2016enhancing}, and then later refined by Kauri~\cite{neiheiser2021kauri}, which in particular, improve the fallback strategy in case of Byzantine nodes in the inner nodes of the tree, and, improving the pipelining technique of its underlying core protocol, HotStuff. We briefly explain this idea on the example of Kauri (which is illustrated in~\cref{fig:scalability:communication:tree}).


% At its core, Kauri uses the same protocol stages as HotStuff, but replaces the star-based communication pattern with dissemination/aggregation trees. For this purpose, Kauri uses an abstraction called \textit{impatient channel}, that are parameterized through a timer and will eventually either receive some value $v$ or return $\perp$. Using impatient channels, Kauri realises the abstractions (1)~\textit{broadcastMsg(data)} and (2)~\textit{waitFor (N - f) votes} which replace HotStuff's original communication flow by (1) letting nodes forward to-be-broadcasted data along the tree, where each node sends the data received by its parent to its children and (2) letting each node
% collect and aggregate votes from its children and forward the aggregate to its parent. \lj{Rather than talking about the specific naming/implementation in Kauri, I think it would be good to talk about the generic challenges that trees bring, \ie the need to wait for children before forwarding own contribution, added latency, and the need to react to failures of internal nodes.} Kauri also proposes a fallback strategy that allows to quickly find a \textit{robust} tree, where all internal nodes are correct, but quick recovery only works for a small number of faulty replicas (smaller than the fanout of the tree). To mitigate the increased latency which is caused by additional communication steps along the tree topology, Kauri uses a pipelining technique with variable pipeline depth that allows to disseminate many batches in parallel (the number is not limited by the agreement stages as in HotStuff) and thus can better utilize available bandwidth. We will see more details on how pipelining can serve as a mean for increasing scalability in~\cref{sect:pipelining}.



\begin{figure}[t]
  \centering
\begin{subfigure}[h]{\columnwidth}
  \centering
  \centering
    \includegraphics[width=0.35\textwidth]{img/gossip.pdf}
    \caption{Randomized topology: Gossip between randomly chosen, connected nodes that are called \textit{neighbors}. %, aggregation happens at each inner node of the tree.
    }
    \label{fig:scalability:gossip}
\end{subfigure}
\hspace{0.4cm}
\begin{subfigure}[h]{\columnwidth}
  \centering
    \includegraphics[width=\textwidth]{img/src/gosig.pdf}
    \caption{In Gosig, replicas communicate with a \textit{fanout} (\ie number of randomly selected neighbors) to disseminate a block or votes for a block within each protocol stage. Each instance is started by a randomly selected leader.}
    \label{fig:scalability:gosig-protocol}
\end{subfigure}
\caption[]{A randomized communication strategy through gossip on the example of Gosig~\cite{li2020gosig}.}
\label{fig:scalability:communication:gossip} 
\end{figure}


\subsubsection{Gossip and Randomized Topology} Furthermore, many recent BFT protocols rely on gossip, or randomized communication topologies, for instance 
Internet Computer Consensus~\cite{camenisch2022internet}, Tendermint~\cite{cason2021design}, Algorand~\cite{gilad2017algorand}, Gosig~\cite{li2020gosig}, scalable and leaderless Byz consensus~\cite{lim2014scalable}, Avalanche~\cite{rocket2020avalanche} and  RapidChain~\cite{zamani2018rapidchain}.

As shown earlier, deterministic leader-based BFT consensus protocols are much affected by the high communication costs of broadcasts, \ie if the leader disseminates a block of ordered transactions to all other nodes. An idea that enhances scalability is to relieve the leader and shift this burden equally to all nodes by disseminating messages through gossiping over a randomized overlay network – which was recently proposed by protocols like Algorand~\cite{gilad2017algorand}, Tendermint~\cite{cason2021design}, or Gosig~\cite{li2020gosig}. 

When using gossip, the leader proposes a message $m$ to only a constant number of $k$ other, randomly chosen nodes. Nodes that receive $m$ forward the message to their own randomly chosen set of connected neighbors, just like spreading a rumor with high reliability in the network. Note that this technique is probabilistic, and the probability of success (as well as the propagation speed) depends on the fanout parameter $k$, the number of hops, and the number of Byzantine nodes present in the system.
Gossip allows for the  communication burden to be lifted from the leader, leading to a fairer distribution of bandwidth utilization, where each node communicates only with its $O(k)$ neighbors, thus improving scalability. 

\cref{fig:scalability:communication:gossip} shows the communication pattern of Gosig~\cite{li2020gosig}.
Similar to the tree-based overlay, the communication pattern leads to increased latency.
As Kauri, Gosig uses pipelining to mitigate this latency.
Different from Kauri, randomized topologies do not require reconfiguration in case of failures.

%While gossip is a technique that makes use of randomness, protocols can still provide deterministic safety and liveness guarantees, as is the case for Tendermint~\cite{cason2021design} and Internet Computer Consensus~\cite{camenisch2022internet}.
%However, it is more often combined with probabilistic guarantees~\cite{gilad2017algorand,lim2014scalable,rocket2020avalanche,zamani2018rapidchain}.
%An overview of the guarantees of all protocols is provided in \cref{tab:protocols}.

Random overlay network topologies can also be utilized in leaderless BFT protocols. Avalanche~\cite{rocket2020avalanche} is a novel, leaderless BFT consensus protocol that achieves consensus through a metastability mechanism that is inspired by gossip algorithms. In Avalanche, nodes build up confidence in a consensus value by iterative, randomized mutual sampling. Each node queries $k$ randomly chosen other nodes in a loop while adopting the value replied by an adjusted majority of nodes so that eventually, correct nodes are being steered towards the same consensus value. In this probabilistic solution, nodes can quickly converge to an irreversible state, even for large networks.


% Gossip (or also refereed to as epidemic broadcast) is a scalability enhancing technique used by many popular BFT protocols such as  Algorand~\cite{gilad2017algorand},  Gosig~\cite{li2020gosig} or Tendermint~\cite{cason2021design}. Also, Avalanche's~\cite{rocket2020avalanche} idea of building up confidence in a consensus value by iterative, randomized mutual sampling is borrowed from how gossip protocols work. 
% Data (\ie a block proposal, or votes for it) is being disseminated by allowing nodes forward it to a constant, and randomly chosen  number of other nodes they know.  





\subsection{Pipelining}
\label{sect:pipelining}
%// cb

%\todo{rewrite so that it's not as structured according to protocols. add new pipelining protocols from table: HermesBFT~\cite{jalalzai2021hermes}, RCC~\cite{rcc2021gupta}, Gosig~\cite{li2020gosig}, Narwhal-HotStuff/Tusk~\cite{danezis2022narwhal}, Joleon/Ditto~\cite{gelashvili2021jolteon}}

%\cb{todo: I need to somhehow fit Gosig in this subsection?}

Pipelining is a technique that allows replicas to run multiple instances of consensus concurrently, and it is employed with a special focus on improving the scalability of the BFT protocol, \eg in HotStuff~\cite{hotstuff19}, Kauri~\cite{neiheiser2021kauri}, BigBFT~\cite{alqahtani2021bigbft}, SBFT~\cite{gueta2019sbft}, \ac{PoE}~\cite{gupta2021poe}, ResilientDB~\cite{gupta2020resilientdb}, 
%\todo{resilientDB is not tagged for pipelining in table 2}
Dispel~\cite{voron2019dispel}, RapidChain~\cite{zamani2018rapidchain}, Hermes~\cite{jalalzai2021hermes}, RCC~\cite{rcc2021gupta}, Gosig~\cite{li2020gosig}, Narwhal-HotStuff/Tusk~\cite{danezis2022narwhal}, and Jolteon/Ditto~\cite{gelashvili2021jolteon}.


Further, pipelining  boosts performance and scalability because replicas  maximize their available resource utilization. For instance, the available bandwidth can be better utilized if data belonging to future consensus instances can be disseminated while  replicas still wait to collect votes from previous, ongoing consensus instances.
This means pipelining can systematically decrease the time replicas spend in an idle state, \eg waiting to collect messages from others. 
%\sr{Vorschlag: 
Gosig~\cite{li2020gosig}, for example, pipelines both the BFT protocol, as committed nodes can start a round while still forwarding gossip messages on the previous one, and the gossip layer, by decoupling block and signature propagation.
With pipelining, a replica can participate in multiple consensus stages simultaneously, %depending on the number of pipelining stages, 
and thus help to overcome \textit{limitations} for system performance, such as \textit{reusing quorum certificates} for multiple consensus stages in HotStuff or \textit{mitigating tree latency} in Kauri.

\subsubsection{Out-of-Order Processing}
A basic variant of pipelining was also introduced with PBFT: The leader can build a pipeline of concurrent consensus  instances by starting multiple instances for a specific allowed range defined through a low and high watermark. 
Within this bound, pipelining is \textit{dynamic}, allowing the protocol to start fewer or more concurrent instances, based on the load.
PBFT employs \textit{out-of-order} processing (see~\cref{fig:pipelinePBFT}), which means that replicas in the same view vote on and commit in all allowed consensus instances without waiting for preceding instances to complete first. Yet, in this case, executions must be delayed until all transactions within lower-numbered consensus instances have been executed.

Similar to PBFT,  the SBFT protocol~\cite{gueta2019sbft} allows the parallelization of blocks, in which multiple instances of the protocol can run concurrently,
but it additionally introduces an adaptive learning heuristic to \textit{dynamically optimize its block size}. %\lj{Is pipelining of PBFT explained earlier? cb: yes}
This heuristic employs a configurable maximal recommended parallelism parameter, the number of currently ongoing, pending blocks, and the number of outstanding client requests. SBFT's heuristic strives to distribute and balance pending client transactions evenly among the recommended number of parallel executing consensus instances, and new requests are not required to wait when currently no decision block is being processed. %\lj{Is wait-free here referring to the technical term or does it just mean, no waiting required?}

Moreover, \ac{PoE}~\cite{gupta2021poe} is a new BFT design employed within ResilientDB~\cite{gupta2020resilientdb} for achieving high throughput using a multi-threaded pipelined architecture. 
Its core innovation is to combine out-of-order processing and dynamic pipelining, as introduced in PBFT, with \textit{speculative executions}, where transactions are executed before agreement.
%This allows making full use of the leader bandwidth, an important factor for achieving high throughput in geo-replicated systems. 
Since \ac{PoE} allows replicas to execute requests speculatively, it introduces a novel view-change protocol that can rollback requests. 


\subsubsection{Chain-based Pipelining \& QC Reusage}

In chain-based protocols like HotStuff, a block is committed after having passed through several protocol stages, as shown in \cref{fig:scalability:hotstuff-protocol}. In each stage, the leader collects votes from the other replicas, aggregates these in a quorum certificate (QC), and disseminates this QC to all as proof of completing the respective protocol stage. 
HotStuff incorporates the idea of pipelining by making each protocol stage into a pipelining stage (see \cref{fig:pipelineHS}), and thus allowing a QC to certify advanced stages of previous, concurrently executing consensus instances, \eg a {\small\textsc{decide}} QC of instance $i$ can act as a {\small\textsc{commit}} QC of instance $i+1$ and {\small\textsc{pre-commit}} QC of instance $i+2$. 
Pipelining in HotStuff is \textit{logical}, meaning that concurrent instances are using the same messages.
The number of pipelining stages in HotStuff equals its number of protocol stages, namely four. 
Narwhal-HotStuff and Tusk~\cite{danezis2022narwhal} as well as Jolteon and Ditto~\cite{gelashvili2021jolteon} built on HotStuff and use the same pipelining mechanism.



\subsubsection{Multiplexing Consensus Instances per Protocol Stage}
Chain-based pipelining can be further optimized by multiplexing consensus instances per protocol stage, as done by Kauri. For this purpose, 
Kauri refines the communication flow of HotStuff by proposing a tree topology with a fixed-sized fanout (number of communication partners). This load-balancing technique relieves the necessary uplink bandwidth (and thus also the time needed to send data) for message distribution at the root (leader). %\lj{What are message dissemination times?}
But at the same time, the communication tree increases the number of communication steps necessary for reaching agreement and thus impacts the overall latency of the SMR protocol. In Kauri, a more sophisticated pipelining method than in HotStuff acts as a solution to sustain performance despite this higher number of communication steps: The number of concurrently run consensus instances is decoupled from the fixed number of protocol stages by introducing a \textit{stretching factor}, which is a parameterizable multiplicative to the fixed pipelining depth of HotStuff. For instance, if a pipeline stretch of $3$ is used, the Kauri leader can simultaneously start 3 consensus instances every time a HotStuff leader would start a single instance, leading to a total of $12$ concurrent consensus instances instead of 4 (like in HotStuff) once the pipeline is filled. In contrast to PBFT, the pipeline stretch in Kauri is static and cannot dynamically adapt to the load, and instances cannot complete out-of-context (see \cref{fig:pipelineKauri}).


\begin{figure}[t]
  \centering
  \begin{subfigure}[h]{\columnwidth}
  \centering
    \includegraphics[width=\textwidth]{img/pipelinePBFTinst.pdf}
    \caption{\textit{Out-of-Order Processing} (here on the example of PBFT): The leader can start multiple instances for a given window of allowed consensus instances concurrently. QCs are not being reused. 
    }
    \label{fig:pipelinePBFT}
\end{subfigure}
\begin{subfigure}[h]{\columnwidth}
  \centering
    \includegraphics[width=\textwidth]{img/pipelineHotStuffInst.pdf}
    \caption{\textit{Chain-based Pipelining} uses one pipeline stage per protocol stage (this example shows HotStuff). QCs can be reused to verify incremental protocol stages of concurrently running instances. 
    }
    \label{fig:pipelineHS}
\end{subfigure}
%\hspace{0.4cm}
\begin{subfigure}[h]{\columnwidth}
  \centering
    \includegraphics[width=\textwidth]{img/pipelineKauriInst.pdf}
    \caption{\textit{Multiplexing Consensus Instances per Protocol Stage}: A \textit{pipelining stretch} number of consensus  instances can be concurrently started per protocol stage (as done in Kauri). QCs can be reused.}
    \label{fig:pipelineKauri}
\end{subfigure}
\caption[]{Pipelining variations of single leader BFT SMR.}
\label{fig:scalability:communication:pipelining} 
\end{figure}




\subsubsection{Pipelining in Multi-leader and Leaderless Protocols} Many BFT protocols rely on the idea of having not only one but several leaders that can concurrently start agreement instances (\eg~\cite{alqahtani2021bigbft, cong2018blockchain, stathakopoulou2019mir}). This design benefits scalability because it brings a fairer distribution of the task of broadcasting block proposals and can prove a suitable solution for the leader bottleneck problem. Yet, it also introduces novel challenges, in particular when it comes to the \textit{coordination of leaders}, to prevent conflicts (\ie identical transaction being proposed by multiple leaders in different blocks) and also to maintain \textit{liveness} for all transactions in the presence of Byzantine leaders.  
To serve as an example, Mir-BFT~\cite{stathakopoulou2019mir} is a multi-leader protocol in which several leaders propose blocks independently and in parallel, by employing a mechanism that rotates the assignment of a partitioned transaction hash space to the leaders. Another example is 
RapidChain~\cite{zamani2018rapidchain}, which is a BFT protocol that uses sharding and allows a shard leader to propose a new block while re-proposing the headers of the yet uncommitted blocks, thus creating a pipeline to improve performance. %\lj{What are pending blocks, and what is the benefit of this technique?}

BigBFT~\cite{alqahtani2021bigbft} is a pipelined multi-leader BFT design that aims to overcome the
leader bottleneck of traditional BFT protocols by partitioning the space of sequence numbers (consensus instances) among selected leaders (coordination stage) and thus
allowing multi-leader executions. Further, it logically separates the coordination stage from the dissemination and voting of new blocks. Pipelining is employed in two ways.
Different leaders perform their instances concurrently.
Additionally, the next coordination stage is run concurrently with the last agreement stages.
Similarly, RCC~\cite{rcc2021gupta} concurrently executes multiple instances with different leaders.
Instead of a coordination stage, RCC uses a separate, independent instance of a BFT protocol for coordination.


A key innovation of Dispel~\cite{voron2019dispel} is its \textit{distributed pipeline} which builds on previous work on leaderless BFT consensus~\cite{crain2018dbft}. 
In contrast to centralized pipelining (where the decision, when to create a new agreement instance, is only up to the leader(s)), %\todo{soll der ganze Satz kursiv sein?}%\lj{What is centralized pipelining?} 
all Dispel replicas can locally decide whether to start new consensus instances based on their available system resources like network bandwidth or memory. Dispel builds a consensus pipeline by creating pipelining stages for \textit{individual tasks} within running consensus that consumes different system resources. In particular, it employs four stages: network reception, network transmission, CPU-intensive hash, and latency-bound consensus~\cite{voron2019dispel}. The idea of Dispel is that a replica can maximize its own resource utilization by executing four consensus instances, one for each stage, concurrently.
 We cover
 \textit{leaderless  protocols} in more detail later in \cref{randomized-sampling}.

\subsection{Cryptographic Primitives}
\label{sec:crypto}

Efficient cryptography schemes can increase a protocol's scalability.
We identified several approaches, most aimed at reducing the communication complexity via message aggregation: multi-signatures (\cref{sec:crypto:aggregation}), %7 papers
threshold signatures (\cref{sec:crypto:threshold}), %9 papers
secret sharing (\cref{sec:crypto:secretsharing}), %2 papers
erasure coding (\cref{sec:crypto:erasurecoding}), %3 papers
as well as efficiently selecting a committee of nodes using \acp{VRF} (\cref{sec:crypto:vrf}). %5 papers

%\textbf{papers}
%
%\begin{tabular}{c|c}
%    crypto & 30 \\
%    crypto-aggregation & 6 \\
%    crypto-threshold & 9 \\
%    crypto-secretsharing & 1 \\
%    crypto-erasurecoding & 2 \\
%    crypto-bls & 5 \\
%    crypto-vrf & 5 \\
%\end{tabular}

\subsubsection{Multi-Signatures}
\label{sec:crypto:aggregation}
%\lj{Should this section be called vote aggregation not message aggregation?}
Seven papers that match our criteria aggregate messages using multi-signatures: Kauri~\cite{neiheiser2021kauri}, Gosig~\cite{li2020gosig}, ByzCoin~\cite{kogias2016enhancing}, Musch~\cite{jalalzai2018window}, AHL~\cite{dang2019towards}, BigBFT~\cite{alqahtani2021bigbft}, and RepChain~\cite{huang2021repchain}.
%
A multi-signature allows $n$ participants, who want to jointly sign a common message $m$, to create a signature $\sigma$ of $m$ so that verification can confirm that all $n$ participants have indeed signed $m$.
This can be achieved by aggregating multiple signatures, \eg via multiplication, resulting in a multi-signature whose combined size and verification cost is comparable to that of an individual signature~\cite{boneh2018multisigbls}.
% The corresponding public keys to be used for verification can also be aggregated to one short public key, which allows for fast verification using the multi-signature, the aggregated public key, and the message $m$. \todo{check if this also applies to CoSi}
% \lj{Can you add explicitly that the combined signature has the size of an individual signature?}

%\lj{Maybe we should refer to BLS only once as example and otherwise call them Multi-Signatures?}\sr{I tried to split into BLS and CoSi as the two schemes I've seen in the papers, and explain both shortly}
In BFT systems, multi-signatures are often used to combine the votes of multiple participants to reduce the message complexity and the memory overhead of the protocol from quadratic to linear~\cite{li2020gosig, neiheiser2021kauri, alqahtani2021bigbft, jalalzai2018window}.
A replica casts its vote by adding its partial signature share to the multi-signature, and the voting concludes once a quorum of signature shares has been received~\cite{neiheiser2021kauri}.
% \lj{Can you explain or omit the aggregation from quorum?} \sr{explained better? which part should be clarified further?}
Most protocols make use of asynchronous non-interactive multi-signatures, \ie BLS multi-signatures~\cite{boneh01bls, boneh2018multisigbls}.
Replicas aggregate their signature share in one small multi-signature.
This reduces the signature size compared to individual replica signatures; however, the computation takes longer compared to, \eg ECDSA signatures.
Multi-signatures thus offer accountability, as it can be checked who signed a message.
Further, they have the advantage that the order of signatures can be arbitrary, making them resilient against adaptive chosen-player attacks~\cite{li2020gosig}.
%\lj{Does gosig aggregate some signatures multiple times?}\sr{that can happen but the number of times a node has signed is counted. why?}
A node can sign the message multiple times, \eg when it receives the same message from different nodes during gossip communication.
To ensure correct verification, the number of times each node signs the message is tracked~\cite{li2020gosig}.

Signatures are collected during the voting phase, \eg during gossip as in Gosig or during vote aggregation phases as in Kauri, where each replica receives votes from its children in the communication tree, and once a quorum of signatures is collected, the replica enters the next phase. 
% \lj{I think voting phase is not introduced. Maybe check writing in 3.1.} 
BigBFT~\cite{alqahtani2021bigbft} is a multi-leader protocol where each leader proposes client requests in a block in instance $r$. 
All blocks are signed by all other nodes during the vote phase upon reception.
The set of $n-f$ votes for a block is then aggregated in a multi-signature, which is piggybacked onto the proposed block in the next instance $r+1$ to commit the block of instance $r$.
% \lj{Does this mean, here signatures for different message are aggregated?}
%A replica receives $N-F$ blocks, it signs these and sends the aggregated signatures to all other leaders. 
%Once they have received $N-F$ vote set messages, they combine these into an aggregated quorum certificate of all proposed blocks, and piggyback this aggregated signature onto the next proposed block, leading to the block of the previous round being committed.
BigBFT avoids the communication bottleneck of single-leader protocols and reaches consensus in two communication rounds by pipelining blocks using message aggregation and multi-signatures to reduce message complexity. %\lj{Ich versteh die Beschreibung oben nicht ganz. Was ist das tolle and BigBFT?}
In Musch~\cite{jalalzai2018window}, %nodes communicate not with all other nodes in the network in case of faults but instead with an exponentially increasing window of nodes.
%For example, if a node does not receive any new requests from the primary, then it contacts first one other node and, if this node cannot give an answer, up to $2f$ nodes are queried to ensure querying at least one correct node.
%When the primary proposes a block, nodes respond with the signed hash of this block.
nodes exchange and aggregate signed hashes of blocks to create one collective signature instead of repeating multiple replica signatures, thus keeping the signature size constant.
If not enough signature shares can be collected, a view change is performed, which also employs aggregated multi-signatures.
% \lj{Unsicher ob die beschreibung von Musch hier relevant ist. Der erste teil hört sich eher relevant für communication an. Wie unterscheidet sich Musch im gebrauch von multisignaturen von GoSig, Kauri, ...?}

Another scheme that is used to aggregate signatures is the collective signing protocol CoSi~\cite{syta16cosi}, which can effectively aggregate a large number of signatures.
It is based on Schnorr multi–signatures and, contrary to the non-interactive BLS multi-signatures, it requires a four-phase protocol run over two round-trips to generate a CoSi multi-signature.
The participants can be organized in communication trees for efficiency and scalability, as discussed in \cref{sec:communication-topologies}.
A node can request a statement, \ie a request, to be signed by a group of witnesses, \ie the replicas, and the collective signature attests that the node, as well as the witnesses, have observed this request.
It is used in ByzCoin~\cite{kogias2016enhancing} to reduce the cost of the underlying PBFT's prepare and commit phases, as well as in RepChain~\cite{huang2021repchain}, to enable efficient cross-shard transactions without requiring multiple individual signatures.
However, the security of two-round multi-signatures has been shown to be compromised~\cite{drijvers2019multisigsecurity}.
% \todo{highlight difference to bls}
%This optimization of combining signatures, where the primary collects and aggregates other nodes' signatures into a single authenticated message and where nodes forward their signed messages to the primary and verify its created multi-signature, can also be performed inside a trusted subsystem, as done by AHL~\cite{dang2019towards}.
% \lj{I think the last part is not relevant. We should highlight the difference from BLS.} \sr{moved this to the tee section}
%\lj{We should sign this which shows that CoSi is insecure:
%M. Drijvers et al., "On the Security of Two-Round Multi-Signatures," 2019 IEEE Symposium on Security and Privacy (SP), 2019, pp. 1084-1101, doi: 10.1109/SP.2019.00050.}


\subsubsection{Threshold Signatures}
\label{sec:crypto:threshold}

A total of nine papers utilize threshold signatures in their protocol design: Jolteon/Ditto~\cite{gelashvili2021jolteon}, Saguaro~\cite{amiri2021saguaro}, ICC~\cite{camenisch2022internet}, Narwhal/Tusk~\cite{danezis2022narwhal}, \ac{PoE}~\cite{gupta2021poe}, HotStuff~\cite{hotstuff19}, SBFT~\cite{gueta2019sbft}, Cumulus~\cite{gai2021cumulus}, and Dumbo~\cite{guo2020dumbo}.
In threshold cryptosystems, participants each have a public key and a share of the corresponding private key: in $(t, n)$-threshold systems, at least $t$ partial shares from participants are needed to decrypt or sign a message.
Participants can sign a message with their secret key share, generating a signature share.
This signature share can be verified or combined with others into an aggregated signature, which can again be verified.
This makes threshold signatures a special case of multi-signatures, where instead of all participants ($n$-out-of-$n$) only a subset ($t$-out-of-$n$, $t < n$) have to participate.
BLS multi-signatures can be transformed into threshold signatures; however, the implementation of threshold signature schemes is more complex, and multi-signatures require less computation~\cite{gueta2019sbft}.

Threshold signatures are, therefore, often used in BFT systems for aggregation of messages, similar to multi-signatures: a block or vote message is signed by a quorum of nodes (commonly with a threshold $t = 2f+1$), the quorum's shares are combined in one authenticator, and the signature share is used as a replica's vote to confirm consensus~\cite{amiri2021saguaro, camenisch2022internet, gelashvili2021jolteon, gupta2021poe, hotstuff19}, or for example to create a quorum certificate for side-chain checkpoints as in Cumulus~\cite{gai2021cumulus}.
BLS signatures can be used for this as well, as done by ICC~\cite{camenisch2022internet} and \ac{PoE}~\cite{gupta2021poe}: either by using the standard BLS scheme with a secret key shared amongst all participants, which creates unique and compact signatures, or by using BLS multi-signatures, where a signature share is a BLS signature which then gets combined into a new signature on an aggregate of the individual public key.
\ac{PoE} can use threshold signatures or message authentication codes depending on the number of participants in the network.

Threshold signatures can be used just as multi-signatures to split one phase of high-complexity broadcast communication into two phases of linear communication complexity.
Dumbo~\cite{guo2020dumbo} introduces two new asynchronous atomic broadcasting protocols.
The first protocol, Dumbo1, reaches asymptotical efficiency, improving upon the design of threshold encryption and asynchronous common subsets of HoneyBadgerBFT~\cite{miller2016honey}.
% The first protocol, Dumbo1, reduces the number of asynchronous binary agreement (ABA) required for asynchronous common subsets (ACSs) to include only $\kappa$ instances, with $\kappa \ll n$.
% The atomic broadcast is realized similarly to HoneyBadgerBFT with a combination of ACS and threshold encryption.
The second protocol, Dumbo2, further reduces the overhead to constant by efficiently using multi-valued Byzantine agreement (MVBA) running over a reliable broadcast which outputs a threshold signature proof that of all receivers of an input, at least one receiver is an honest peer.
% This is achieved via threshold signing of the reliable broadcast index.
SBFT~\cite{gueta2019sbft} uses threshold signatures to reduce the communication complexity to linear by extending PBFT by $c+1$ collectors.
Nodes send their messages to the collectors, who then broadcast the combined threshold signature once $3f+c+1$ shares have been received. %Upon a C-collector accepting 3f+c+1 distinct sign-share messages it forms a combined signature σ(h)
Using threshold signatures reduces the message size of the collector from linear to constant, and the client overhead is reduced as only one signature has to be verified.
The execution is similarly aggregated by $c+1$ execution collectors who collect $f+1$ signature shares.
SBFT uses BLS signatures, though BLS multi-signatures instead of threshold signatures are used on the fast path as these require less computation.

Another usage of threshold cryptography is generating randomness used for leader election in asynchronous networks.
Narwhal~\cite{danezis2022narwhal} uses an adaptively secure threshold signature scheme to generate a distributed perfect coin, while Jolteon~\cite{gelashvili2021jolteon} generates randomness for each view by hashing the threshold signature of the view number in order to create an asynchronous fallback protocol to circumvent the FLP impossibility.
For ICC~\cite{camenisch2022internet}, a sequence of random beacon values is created to determine the permutation of the participants and thus the leader, similar to verifiable random functions (\cf \cref{sec:crypto:vrf}).
% \lj{Is this different from VRFs?}
Starting from a known initial value, a participant generates the next sequence of the random beacon by broadcasting its signature share of the current random beacon value.
At least one honest participant's signature share is required to form a comprehensible random value.


\subsubsection{Secret Sharing}
\label{sec:crypto:secretsharing}
% \lj{I miss a sentence that says, what secret sharing is.}
In secret sharing approaches, a secret is split and distributed amongst a group of $n$ nodes so that it can be reconstructed for cryptographic operations when a sufficient number (\ie a threshold $t$) of shares are combined but no single node can reconstruct the full secret by itself.
The splitting and distribution are typically performed by a dealer.
This secret sharing amongst replicas can, similar to threshold signatures, reduce the overhead of multi-signatures.
Contrary to the threshold signatures of \cref{sec:crypto:threshold}, however, the secret sharing in FastBFT~\cite{liu2018scalable} requires hardware-based trusted compartments on all replicas.
The trusted compartment on the leader can securely create secrets, split them, and deliver them to all other replicas.
This is performed in a separate pre-processing phase before the agreement phase.
Once the order has been established, the replicas release in the final step their shares of the secret to allow verification of the agreement.
All secrets are one-time secrets, and a monotonic counter value is bound to the secret shares in order to prevent equivocation of the leader.
For more details on trusted counters and trusted execution environments, we refer to \cref{sec:tee}.
% 
Secret sharing can also be used to prevent leakage of sensitive data: clients in Qanaat~\cite{amiri2021qanaat} use $(f+1, n)$-threshold secret-sharing to keep data confidential.


\subsubsection{Erasure Coding}
\label{sec:crypto:erasurecoding}

% \lj{Es wäre gut explicit zu erklären wie EC scalability unterstützt. I.e., instead of broadcasting a given payload, a primary can encode this payload and send the individual fragments to different replicas. With a correct encoding, the payload can be reconstructed, even if some replicas remain unresponsive. While reducing network load on the primary, this technique adds computational overhead for encoding and decoding.}
Three protocols use erasure codes: Dumbo~\cite{guo2020dumbo}, ICC2~\cite{camenisch2022internet}, and DispersedLedger~\cite{yang2022dispersed}.
The first two aim to reduce the communication overhead and lessen the bottleneck on the leader, whereas DispersedLedger focuses on data storage and availability.
Erasure codes are forward error correction codes that can efficiently handle bit erasures during transmissions. 
In $(n, n-2t)$-erasure codes, a message $m$ is split into $n$ fragments of larger size so that any subset of $n-2t$ fragments can be used to reconstruct the original message $m$ even if fragments get lost or corrupted.
Instead of broadcasting a given payload, a leader can encode this payload and send the individual fragments to different replicas. 
With a correct encoding, the payload can be reconstructed, even if some replicas remain unresponsive. 
While reducing network load on the leader, this technique adds computational overhead for encoding and decoding.
In ICC, large blocks have to be disseminated, which can become a bottleneck, so as an improvement of their peer-to-peer gossip sub-layer (ICC0/1), they propose a subprotocol of reliable broadcast with $(n, n-2t)$-erasure codes ($t<n/3$) in ICC2 in combination with threshold signatures.
Dumbo's reliable broadcast protocol can be optimized as well using a $(n-2t, n)$-erasure code scheme in combination with a Merkle tree, which tolerates the maximal adversary boundary and thus helps honest nodes recover efficiently.
In DispersedLedger, data is stored across $n$ nodes via a verifiable information dispersal protocol making use of erasure codes.
This guarantees data availability and allows separating the tasks of agreeing on a short, ordered log and of downloading large blocks with full transactions for execution.
This decoupling of the protocol stages leads to faster progression of the protocol as the high-bandwidth task of downloading transactions is no longer on the critical path.


\subsubsection{Verifiable Random Functions}
\label{sec:crypto:vrf}

% \lj{Vielleicht reicht hier eigentlich weniger discussion und statt dessen eine Referenz zu 3.5.}
% \sr{shorten if too similar to \cref{sec:consensus-selection}}
Five papers incorporate \acfp{VRF}~\cite{micali99vrf} into their design: Algorand~\cite{gilad2017algorand}, Beh-Raft-Chain~\cite{wang2021behraftchain}, Cumulus~\cite{gai2021cumulus}, Proof-of-QoS~\cite{yu2019proofofqos}, and DLattice~\cite{zhou2019dlattice}.
A VRF is a cryptographic function $\textit{VRF}_{sk}(x)$ that for an input string $x$ returns both a hash value and a proof $\pi$.
The hash value is here uniquely determined by both the user's secret key $sk$ as well as the input $x$, but appears undistinguishable from a random value for anyone not in possession of $sk$.
The proof $\pi$ allows anyone who knows the public key $pk$ to verify whether the hash value corresponds to $x$, without revealing $sk$~\cite{gilad2017algorand}.
VRFs in BFT protocols facilitate the selection of committees, which in turn can efficiently perform the consensus.
Their non-interactive nature makes them desirable as it prevents any targeted attack on the leader(s) or committee members of the next instance, as their membership status is not known in advance.
For more details on the usage of committees for scalability in BFT protocols, we refer to \cref{sec:consensus-selection}.

Algorand uses a VRF based on the nodes' key pairs as well as publicly available blockchain information for cryptographic sortition, meaning to select the committee members in a private and non-interactive way so that nodes can independently determine whether they are in the committee for this instance.
VRFs create a pseudo-random hash value that is uniformly distributed between 0 and $2^{hashlen}-1$, meaning that nodes get selected at random to be in the committee.
As VRFs do not require interaction between the nodes and are calculated using private information, a node's membership status cannot be determined in advance to launch a targeted DoS attack or allow malicious nodes to collude; instead, a node's membership status is only known retrospectively.
Algorand also includes a seed in each instance, which is calculated using the VRF result in combination with the previous instance's seed; if this seed is not included in a proposed block, it is discarded as invalid.
If multiple blocks get proposed by committee members, then the included seed value is used to determine a priority amongst these blocks.
Furthermore, the VRF value can be used as a random value for coins, for example, in order to resume normal operation after a network partition.
Algorand implements its VRFs over Curve25519~\cite{goldberg16vrfcurve, goldbe-vrf-00}, and shows that VRF and signature verification are CPU bottlenecks in the protocol.
% 
VRFs are used similarly in Beh-Raft-Chain~\cite{wang2021behraftchain} and DLattice~\cite{zhou2019dlattice} for cryptographic sortition in order to determine a node's role and membership status, \eg (local) committee leader.
% Here as well the membership cannot be determined in advance, but others can retrospectively verify it using the proof $\pi$.
% This prevents DoS attacks as well as collusion between malicious nodes.
Proof-of-QoS~\cite{yu2019proofofqos} splits nodes into regions, and one node per region is selected for the BFT committee based on its quality of service~(QoS). 
Out of the $\kappa$ nodes with the highest QoS in a region, the node with the smallest hash of id and seed value is selected. 
This node's membership status is confirmed with its VRF hash, and others verify the corresponding proof. % $\pi$.

In Cumulus~\cite{gai2021cumulus}, VRFs are used in a novel cryptographic sortition protocol called Proof-of-Wait, which is used by this side-chain protocol to select the epoch's representative to interact with the mainchain.
At most, one representative can be chosen per epoch, and it is based on nodes calculating a random waiting time based on their VRF output.
The VRF is implemented based on the elliptic curve Secp256k1.



\subsection{Independent Groups}
%\begin{itemize}
%    \item Process Grouping
%    \item Sharding, flat vs hierarchical (nochmal rüber gucken)
%    \item parallelization
%\end{itemize}
In this section, we look at how subsets of all nodes, or groups, can process transactions independently from the rest of the nodes.
Of the 22 papers in this section, we classified 16 as using sharding techniques.
The remaining six papers are classified as hierarchical consensus.

Sharding, traditionally used in database systems, splits a dataset into smaller subsets.
In database systems, this is used when the data cannot fit onto a single machine anymore.
This technique has been used by \ac{BFT} protocols to improve scalability.
In a sharding algorithm, the application's state is distributed over multiple, possibly overlapping, groups called shards~\cite{hellings2020cerberus, wang2021behraftchain, hong2021pyramid, amiri2021sharper}.
This allows efficient processing of transactions within shards, as only a subset of nodes has to participate in the consensus. Further, only the responsible group must execute the transaction.
% , with the added benefit that the group can process transactions, only affecting the shard, independently of the other shards.

Hierarchical consensus can but does not have to split state across different groups.
% This way, the line between sharding and hierarchical consensus can blur.
Here, the groups use a higher-level consensus mechanism to coordinate.
While shards can also communicate with each other, \eg for cross-shard transactions, the higher-level consensus is a significant aspect of hierarchical consensus, which we use to distinguish these two categories.
% We distinguished between sharding and hierarchical consensus by the fact that, in hierarchical consensus, the groups can use a higher-level consensus mechanism to coordinate.


%The line between sharding and hierarchical consensus can be blurry.
%Generally, sharding algorithms divide the system and assign each subdivision to a subset of nodes to handle.
%The advantage here is that for the most part (we will address cross-shard transactions below in \cref{sec:sharding}) the different subdivisions, or shards, can process transactions independently from each other.
%Some hierarchical algorithms have similar characteristics, where independent groups can process requests independently of other hierarchical groups~\cite{amiri2021saguaro, zheng2020dphybrid}.
%The main difference is that for hierarchical algorithms the different groups are part of higher-level groups where global consensus between groups is achieved.
%A global consensus is thus only reached if the parent group of all the groups affected by a transaction is reached.
%For sharding, the cross-shard consensus generally only requires coordination between the affected shards without any hierarchical ordering of shards.




\subsubsection{Sharding}\label{sec:sharding}

%\begin{figure}
%    \centering
%    \includegraphics[width=0.67\columnwidth]{img/sharding.pdf}
%    \caption{Replicas using sharding. Communication mainly occurs within shards, commonly only requiring coordination between shards for cross-shard transactions.}
%    \label{fig:sharding}
%\end{figure}

\newcommand{\threewaysubfigurewidth}{0.67\columnwidth}
\newcommand{\threewaygraphicswidth}{0.77\columnwidth}
\begin{figure*}[t]
  \centering
\begin{subfigure}[h]{\threewaysubfigurewidth}
    \centering
    \includegraphics[width=\threewaygraphicswidth]{img/sharding.pdf}
    \caption{Replicas using sharding. Coordination between shards is commonly only required for cross-shard transactions.}
    \label{fig:sharding}
\end{subfigure}
\begin{subfigure}[h]{\threewaysubfigurewidth}
    \centering
    \includegraphics[width=\threewaygraphicswidth]{img/hierarchical_green.pdf}
    \caption{Hierachical consensus with two layers with ``representatives'' of the lower layer running the consensus in an upper layer.}
    \label{fig:hierarchical}
\end{subfigure}
\begin{subfigure}[h]{\threewaysubfigurewidth}
    \centering
    \includegraphics[width=0.74\columnwidth]{img/committee.pdf}
    \caption{Schematic overview of a committee algorithm. Only a subset of all nodes performs consensus on transactions.}
    \label{fig:committee}
\end{subfigure}
\caption[]{Three ways of reducing consensus participants: sharding, hierarchical consensus, and consensus by committee.}
\label{fig:limited-consensus-participants} 
\end{figure*}


Sharding is a technique to split the system into multiple \textit{shards}, each containing a subset of the replicated state and being managed by a subset of the nodes, as seen in~\cref{fig:sharding}.
Transactions that affect only a single shard can then be processed independently. 
This improves scalability by reducing the number of messages that have to be exchanged to reach consensus within shards. 
Within a shard, nodes typically run a classic consensus protocol like PBFT~\cite{hellings2020cerberus} or Raft~\cite{wang2021behraftchain}.


Sharding also brings new challenges. %\lj{Can you write a paragraph here first summarizing the challenges?} 
First, as shards contain only a subset of nodes, special care must be taken so that the shards are not vulnerable.
Next, once the shards are created, clients need to know where to send transactions.
Lastly, some transactions affect multiple shards, which requires coordination between shards to process these multi-shard transactions.


The first challenge requires the protocol to assign nodes to shards while keeping the system safe and live. %\lj{Can you mention what are the challenges here, e.g. one shard being overwhelmed. I think it would also be relevant to mention if the papers assume an adaptive or slowly adaptive adversary.}\av{added adaptive adversary below, very few papers mention hotspots in load}
For this, different sharding algorithms assume different threat models.
Assuming a global threshold of $\frac{1}{3}$ faulty nodes, there is the risk that more than $\frac{1}{3}$ of nodes within a shard are faulty, jeopardizing safety.
This is especially a threat if an adaptive adversary is considered.
Most algorithms stipulate that the adversary can only corrupt nodes between epochs, but not within an epoch~\cite{wang2021behraftchain, hellings2020cerberus, hong2021pyramid, zamani2018rapidchain}.
This moves the challenge of safety to the shard formation.
To ensure the distribution of malicious nodes within any shard is equal to the global share of malicious nodes, techniques like \acp{VRF}~\cite{micali99vrf} or \acp{TEE}~\cite{dang2019towards} can be used.
A more challenging threat model is adaptive corruption without stipulations on when corruption can occur.
In this setting, algorithms ensure safety by resampling the shards faster than an adaptive adversary can corrupt nodes~\cite{david2021gearbox, dang2019towards}.
Some approaches form shards based on criteria such as reputation or past behavior to guarantee that well-behaved shards are formed~\cite{huang2021repchain, wang2021behraftchain}.
Chainspace~\cite{al-bassam2018chainspace} also allows for whole shards to be controlled by an adversary while still enforcing some restricted safety criteria.
Explicitly, Chainspace can still guarantee encapsulation between state objects (smart contracts) and non-repudiation in case a complete shard is controlled by the adversary. 
The encapsulation of state objects guarantees that malicious smart contracts cannot interfere with non-malicious ones, \ie the control mechanism of Chainspace.
Combined with non-repudiation, where message authorship cannot get disputed, sources of inconsistencies can get identified by the control mechanism and thus punished.

%Other algorithms assume only a fixed global set of nodes can get corrupted~\cite{huang2021repchain}, while others assume that for each specific shard the $3f+1$ threshold can not be broken~\cite{rahnama2021ringbft}.


%Corruption of whole shards~\cite{al-bassam2018chainspace}
%Static faults~\cite{david2021gearbox}
%Static during epoch ~\cite{hong2021pyramid, zamani2018rapidchain}


With a sharding architecture, some care has to be taken so that transactions by clients end up in the correct shard.
This is especially true for cross-shard transactions.
The simplest solution is to have clients send transactions to all shards~\cite{hellings2021byshard, hellings2020cerberus, al-bassam2018chainspace, huang2021repchain}.
This way the shards can decide if a transaction is relevant to the shard or discard the transaction otherwise.
This is more work for clients but, as ignoring transactions not relevant for a shard is easy, the overhead for the shards is small.
Transaction routing has also been adapted in different ways.
One is that clients can send transactions to an arbitrary shard, which then forwards the transaction to the correct shard~\cite{rahnama2021ringbft, amiri2021sharper}.
In the Red Belly Blockchain~\cite{crain2021red}, clients send balance and transaction requests to known proposers published in configuration blocks.
The proposers can respond directly for balance (\ie read) requests and forward transactions (\ie write requests) to consensus instances.
%Similarly, in Qanaat~\cite{amiri2021qanaat} clients send transactions to a coordinator cluster.
%In this coordinator cluster, an internal order is established before forwarding the transaction to the involved shards.\lj{Does this not make Qanaat a hierarchical approach?}


Lastly, transactions can affect multiple shards~\cite{hellings2021byshard, hellings2020cerberus, hong2021pyramid, amiri2021sharper, zamani2018rapidchain, amiri2021qanaat}. 
This occurs when a transaction requires data from multiple shards.
As such, these shards must coordinate access to ensure consistency.
The system then has to coordinate cross-shard transactions to make sure the transaction is accepted by each shard. 
Systems like RapidChain handle cross-shard transactions by splitting them into multiple sub-transactions that get handled by the respective shards~\cite{zamani2018rapidchain}.
This is possible as RapidChain uses the \ac{UTXO} state model, which allows splitting transactions in the spending of existing outputs and the creation of new outputs.
RapidChain calculates that more than 96\% of transactions in their system are cross-shard transactions.
As an optimization, Pyramid, which uses an account model, proposes to overlap shards, allowing cross-shard transactions to be processed by the nodes overlapping both shards~\cite{hong2021pyramid}. 
This way, overlapping nodes efficiently process cross-shard transactions, requiring no additional work. 
The overlapping nodes generate blocks from the transactions and send them to the rest of the shard for commitment.
In Cerberus~\cite{hellings2020cerberus}, each shard locally reaches consensus on \ac{UTXO} transactions.
If input from other shards is necessary, each shard will send its own local input to the other affected shards, thereby promising to process the transaction once all the required input is available by the other shards.
As all shards do the same procedure, they will either receive all required inputs and process the transaction, or all abort the transaction.
In SharPer~\cite{amiri2021sharper}, which also uses an account model, cross-shard transactions are handled by clients sending the transactions to some leader of a shard from which data is needed.
This leader is then responsible for sending the transaction to all nodes of all other involved shards.

% TODO if enough space add some text on this
%Both Beh-Raft-Chain and RepChain use a monitoring mechanism to manage the reputation of nodes, and to confirm transactions into blocks for RepChain.
%This allows them to use Raft, famously only a crash-fault tolerant algorithm, to efficiently process transactions.

%Another challenge is the prevention of some shards becoming overwhelmed.
%This can occur when some shard has to process more transactions than the other shards.
%In this case the overwhelmed shard becomes the bottleneck of the system.


%In systems like Beh-Raft-Chain~\cite{wang2021behraftchain} nodes can join the network in reconfiguration events between epochs.\lj{Does any paper say how long an epoch should be or does analysis on this? What are the heuristics for deciding epoch length?}\av{}
%This is a common pattern used in multiple papers~\cite{dang2019towards}.
%Beh-Raft-Chain also uses this mechanism to dynamically adjust the number of shards.
%As each shard is only supposed to contain a fixed number of nodes, the reconfiguration is used to adjust the number of shards to adapt to nodes joining and leaving.
%Assignment to shards is done differently in different papers.
%\lj{What do the other papers do? Do all papers try to assign randomly?}
%Dang et al. use TEE to generate assignments using random numbers.
%They generate random numbers inside of TEEs to prevent attackers from influencing the random numbers and to prevent brute force attacks.
%This way they guarantee that each node has one chance to generate two random numbers per epoch.
%If the first random number is lower than some threshold the other random number is used as seed to determine the sharding assignment.
%\lj{How do clients know which shard to send their transaction to?}



%Early works in sharding operating in permissionless settings with a focus on UTXO data models and introduce mechanisms to support cross shard transactions\cite{al-bassam2018chainspace, hellings2020cerberus}.
%Later (towards scaling) also presents an approach to shard permissioned blockchain with general purpose smart contracts~\cite{al-bassam2018chainspace}.

%At first, sharding system assumed that faulty node are equally distributed across all shards. 
%Later approaches form shards based on criteria such as reputation or past behavior to guarantee that well behaving shards are formed~\cite{huang2021repchain, wang2021behraftchain}.
%Beh-Raft-Chain~\cite{wang2021behraftchain} for example ranks nodes by behavior by detecting if nodes act maliciously.
%Special monitor nodes detect if nodes stop working, accept messages that should be denied, or vice versa if a node accepts messages that should be accepted.
%Normal behaving nodes get a higher behavior score while malicious nodes get a lower score.
%These behavior scores get managed by a behavior committee, whos members are selected from the nodes with the highest behavior scores.
%In this way, Beh-Raft-Chain manages to use Raft, famously a crash fault tolerant algorithm, in a BFT algorithm.
%Using the weighting the impact of malicious nodes on the algorithm is minimized, with additional security guaranteed by the monitoring.
%RepChain~\cite{huang2021repchain} on the other hand uses a double-chain architecture.
%Here one chain is used for transactions while the other is used for reputation management.
%With this approach Raft can be used for the transaction chain to achieve fast progress, while a BFT consensus algorithm is used to confirm the transaction blocks and the reputation chain.
%These internal chains are synchronized across shards after each epoch, using optimizations like collective signatures and combined UTXOs so that validators don't have to download the whole chains from the other shards but can use the synchronization to get the relevant data.

%GearBox optimizes shard sizes to achieve higher throughput.
%While always guaranteeing safety it lowers the size of shards to optimize for throughput.
%This can risk liveness.
%For this purpose GearBox keeps a second blockchain in which it stores heartbeats from the shards.
%If a deadlock of a shard is detected it is restarted with more nodes, thus with a lower throughput but higher probability of liveness.
%This can also be reversed, shards are restarted with lower number of nodes to achieve higher throughput, if a lower corruption threshold is detected for some period of time.



\subsubsection{Hierarchical Consensus}
\label{subsec:independent_hierachical}

%\begin{figure}
%    \centering
%    \includegraphics[width=0.67\columnwidth]{img/hierarchical_all_same.pdf}
%    \caption{Hierachical consensus with two layers with the leaders of the lower layers acting as ``representatives'' in the upper layer.}
%    \label{fig:hierarchical}
%\end{figure}
In a hierarchical consensus scheme, nodes on lower hierarchy levels can operate in independent groups and use higher levels to coordinate.
The papers in this section use different approaches to improve scalability.
The biggest benefit, as with sharding, is that the smaller groups allow for more efficient communication, as the number of communicating nodes is reduced~\cite{xu2021concurrent, amiri2021saguaro, zheng2020dphybrid}.

This architecture poses multiple questions.
Firstly, how is the hierarchy structured, and how is it determined?
How many layers are there, and what information has to get shared between layers?
Finally, and most importantly for scalability, what degree of independence can lower levels have, and when are higher levels of consensus necessary?

Regarding the hierarchy structure, of the papers considered, four picked a two-layer hierarchy~\cite{zheng2020dphybrid, li2021optimized, wang2021behraftchain, xu2021concurrent}, while two chose a variable number of layers~\cite{li2021scalable, amiri2021saguaro}.
%Two DP-Hybrid~\cite{zheng2020dphybrid}, SHBFT~\cite{li2021optimized}, Beh-Raft-Chain~\cite{wang2021behraftchain}, C-PBFT~\cite{xu2021concurrent}
%More X-Layer~\cite{li2020scalable}, Saguaro~\cite{amiri2021saguaro}
For the two-layer structure, a structure using ``representatives'' is most common~\cite{li2021optimized, wang2021behraftchain, xu2021concurrent}, which is also used by X-Layer~\cite{li2021scalable} for multiple layers.
In this case, as seen in \cref{fig:hierarchical}, the lower levels have their ``representative'', commonly leaders, representing the group in the upper layer.
Similarly, GeoBFT~\cite{gupta2020resilientdb} shares transactions globally by sending them to the leaders of other clusters before executing them.
The difference is that global sharing does not require coordination between the leaders, but the leaders share the transaction within their own cluster.
In another way, DP-Hybrid~\cite{zheng2020dphybrid} chooses to use \ac{PoW} as the mechanism for the upper layer where all nodes can participate.
Focusing on processing transactions in wide area networks, Saguaro~\cite{amiri2021saguaro} uses multiple layers, with different layers composed of edge devices, edge servers, fog servers, and cloud servers.
The structure of hierarchies can be dynamic~\cite{wang2021behraftchain, li2021optimized}, where new shards are created if enough new nodes join the network, or static as in the cases of DP-Hybrid~\cite{zheng2020dphybrid} and Saguaro~\cite{amiri2021saguaro}, where the algorithm is focused on companies using the blockchain.


%Next is the question what information the higher-level works on.
%In C-PBFT~\cite{xu2021concurrent}, the lower layer constructs blocks which then are packaged into ``primary'' blocks by the upper layer.
%Here the primary block contains the headers of the blocks of the lower layer.
%Similarly, in Saguaro~\cite{amiri2021saguaro}, the blocks of lower layers are combined in the upper layers in a directed acyclic graph into higher-level blocks.
%In DP-Hybrid~\cite{zheng2020dphybrid}, on the other hand, the global consensus creates blocks consisting of already accepted transactions of the lower layer.
%In GeoBFT~\cite{gupta2020resilientdb}, after local consensus and as an extra step before execution, transactions are globally shared.
%\lj{I wonder if below, we could put more emphasis on whether the actual transactions are included in higher level blocks, or only their hashes or block headers.}\av{changed text}
Next is the question on what information the higher level works.
There are two broad approaches to this: either with the actual transactions~\cite{zheng2020dphybrid, gupta2020resilientdb, li2021optimized, amiri2019caper}, or with ``prepackaged'' blocks of transactions~\cite{xu2021concurrent, amiri2021saguaro} arranged in the upper layer.
For example, in GeoBFT~\cite{gupta2020resilientdb} or DP-Hybrid~\cite{zheng2020dphybrid}, every transaction is shared through the upper layer with the other groups.
In C-PBFT~\cite{xu2021concurrent}, on the other hand, the lower layer already constructs blocks of transactions, which are then confirmed by adding the block header to the upper-layer blockchain.

As with sharding, it can happen that some transaction requires data from multiple groups.
For this, affected groups can coordinate with their least common ancestor group~\cite{amiri2021saguaro}.
As all transactions are globally shared in GeoBFT~\cite{gupta2020resilientdb}, each cluster has access to all transactions, making cross-cluster transactions cheap.
As global sharing is performed before execution, this leads to higher latencies.

Different layers can have different failure modes~\cite{amiri2021saguaro, amiri2019caper}, \eg when different applications use a shared blockchain.
This can be used to optimize the system, \eg if the applications require different security or fault tolerance levels.
Then it might be possible to use different consensus protocols in separate lower layers, such as BFT or crash fault-tolerant protocols.

Further, there are protocols that measure the behavior of nodes and reward good behavior, \eg to allow well-acting nodes to become leaders more often~\cite{wang2021behraftchain, xu2021concurrent}.
The behavior-measuring metrics consist of the detection of misbehaving nodes~\cite{wang2021behraftchain} and qualitative measures like payment times and amounts~\cite{xu2021concurrent}.
\enquote{Good} participants are thus rewarded for their good metrics, creating incentives for good behavior.

%Another benefit is that groups in lower hierarchies can respond to clients about their local decision without having to reach a global consensus.
%This is for example used by DP-Hybrid~\cite{zheng2020dphybrid}.

%A good illustration of this approach is DP-Hybrid~\cite{zheng2020dphybrid} with two layers. 
%In the bottom layer, nodes that perform \ac{PBFT} to reach local consensus are grouped. 
%This local consensus is then broadcast to all other participants, local and external, and transmitted to the client.
%These consensus decisions are then collected into blocks.
%In the top layer, nodes maintain a blockchain using Constrained \ac{PoW} that finally commits the collected decision blocks.
%The top layer consists of all the nodes, where nodes from the bottom layer represent their group in the top layer.
%The bottom layer sub-groups work independently, thus allowing for requests to be processed independently. 
%The authors of DP-Hybrid give an example of this being useful for different independent companies using a blockchain for data sharing.
%The advantage is that local consensus does not require the participation of other companies, allowing for independent and efficient processing.
%A similar approach is also used by Saguaro~\cite{amiri2021saguaro}.
%Focusing on processing transactions in wide area networks, Saguaro uses multiple layers, with different layers composed of edge devices, Edge Servers, Fog Servers, and Cloud Servers.
%Within each layer, there are multiple, mostly independent domains.
%Requests are ordered internally within each domain if possible, only requiring cross-domain ordering if information from multiple domains is required, \eg for payment transactions.
%Requests for multiple domains get distributed to the higher hierarchies by sending blocks of transactions up the hierarchy at the end of rounds.
%As the top layer might receive blocks from multiple children containing potentially cross-domain transactions, a consensus is then reached inside the top layer on the blocks of the children.
%In both Saguaro and DP-Hybrid, a client gets a reply for a request before the request is processed by the upper layers.
%%\lj{So the main difference between DP-Hybrid and Saguaro is that in DP-Hybrid the top layer fixes all requests, while in Saguaro, top layer sees only requests that affect multiple shards?}
%%\av{In Saguaro the top layers also see (all) transactions}
%
%Differently, in SHBFT~\cite{li2021optimized} each request has to go through all levels.
%Different hierarchical layers exist, but these are responsible for collecting and ordering requests.
%In each different group, there is a primary node that collects messages from the other nodes inside of the group.
%After collecting messages, the primary node sorts the messages and builds a set.
%This set of messages is then verified by the other nodes in the group.
%After this verification, the primaries of the subgroups submit the message set to the nodes in an upper layer.
%In this layer, the same procedure occurs, where the primary of the upper layer merges all message sets and proposes it as a new block to the other nodes in the upper layer.
%Once confirmed the block is disseminated to the lower layers by the primaries, where it again gets verified by all the nodes.
%If verification fails at any point or the primaries at any level are found to not be working, they can be impeached and will be replaced.
%\lj{Based on this description, I am not sure if there even is independent processing in SHBFT? If there is no independent processing, it should not be in this section.}
%
%C-PBFT~\cite{xu2021concurrent} deals with Supply Chains.
%Nodes are sorted into clusters based on their transaction history. %\lj{I do not understand the grouping, can you explain?}\av{added more}
%The cluster a node is assigned to is the cluster for which the largest portion of product transactions has been made by the node.
%This way, nodes inside a cluster mainly process transactions about the same product.
%This way, nodes with similar transaction behavior are clustered together, allowing for more efficient communication.
%E.g., suppliers, manufacturers, dealers, and retailers mainly trading cars are clustered together. %\av{more text on grouping (clustering)}
%C-PBFT uses reputation assessment based on payment times, amounts, and partners to rank nodes, picking the primary peer with the highest reputation.
%To work independently, each group generates its own blocks.
%For consensus on these blocks, the primary sends the block to add to the main blockchain to all other clusters.
%The clusters can then perform consensus in parallel on the block.
%After the internal consensus, a consensus is then formed between groups.
%If all succeeds, the block is added to the blockchain.
%
%GeoBFT~\cite{gupta2020resilientdb} clusters nodes according to their topology.
%The motivation behind this is that nodes that are closer together have lower latency between each other, thus improving the consensus performance.
%GeoBFT performs \ac{PBFT} inside of the local clusters and then uses a global sharing algorithm to disseminate the proof of local replication to all clusters, similar to DP-Hybrid~\cite{zheng2020dphybrid}.
%For global sharing, the proof is sent by the leader to f+1 nodes in the other cluster. 
%The nodes receiving the decision then broadcast the decision internally.
%Clusters can detect when the primaries of other clusters are not proposing any messages.
%If this is the case, the cluster will send a view change request to the other nodes of the failed primary cluster.
%Adding the extra step of global sharing to \ac{PBFT} does not increase the latency as most of the work, \ie \ac{PBFT}, is performed inside clusters with low internal latency.
%Global sharing only sends a limited number of messages, thus not impeding the overall latency of the system.

%These approaches show how hierarchical groups can work in parallel.
%The work can be divided between groups such that requests can be handled internally, or lower-level groups can perform work upfront so that work at higher levels is reduced.




\subsection{Selection of Consensus Committees}\label{sec:consensus-selection}
%\begin{figure}
%    \centering
%    \includegraphics[width=0.67\columnwidth]{img/committee.pdf}
%    \caption{Schematic overview of a consensus algorithm. Only a subset of all nodes performs consensus on transactions.}
%    \label{fig:committee}
%\end{figure}
As described in~\cref{sec:communication-topologies}, one approach to improve scalability is to avoid bottleneck situations.
In this direction, several approaches regarding nodes' roles for agreement exist.
Of the 20 papers in this section, 10 focus on consensus by committee, eight on hierarchical consensus (\cf \cref{subsec:independent_hierachical}), and two on randomized sampling.
One approach is that only a substantially smaller subset of participants, \ie committees, participate in finding agreement~\cite{gilad2017algorand, zamani2018rapidchain, jalalzai2019proteus, jalalzai2021hermes}.
As these committees are significantly smaller than the whole set of participants, consensus protocols---which, in theory, would not scale for the whole set of nodes---become sufficiently efficient again.
The remaining nodes often take a passive role and only observe and verify the committee's work.
A related technique is a hierarchical consensus where multiple layers are used for consensus~\cite{li2021scalable, li2021optimized, amiri2019caper, xu2021concurrent, amiri2021saguaro}, which faces similar problems as consensus by committee.
In this section, we investigate protocols using these techniques to see how committees are formed, how they work, and what aspects should be considered for scalability.

\subsubsection{Committee}
In a committee, only a subset of nodes participates in the consensus algorithm, as depicted in~\cref{fig:committee}, while non-committee members observe the results of the consensus~\cite{gilad2017algorand, zhan2021drbft, jalalzai2021hermes}.
As only a substantially smaller subset of the total nodes participate in the consensus algorithm, the communication efforts needed inside of the committee are reduced.
The crucial question is: how is it decided which participants will be included in the committee without being vulnerable to attacks?
The main committee formation scheme is based on randomness.
Randomness makes it unlikely that a critical amount of malicious nodes will enter the committee.
Different algorithms use different randomness to select committee nodes.
For example, Algorand utilizes \acp{VRF}~\cite{gilad2017algorand} (\cf \cref{sec:crypto:vrf}), DRBFT picks committee nodes based on previous blocks~\cite{zhan2021drbft}, and RapidChain uses verifiable secret sharing to generate randomness within committees~\cite{zamani2018rapidchain}.
Some algorithms modify the committee formation based on metrics such as stake~\cite{gilad2017algorand, zhou2019dlattice} or quality characteristics~\cite{yu2019proofofqos} such as bandwidth or latency to prioritize nodes by some weight.
In permissionless protocols like Algorand, the stake is necessary to prevent Sybil attacks based on pseudonyms.
Otherwise, attackers could create an arbitrary number of nodes to gain control of the committee to influence the consensus maliciously.
The weighting of quality metrics incentivizes nodes to provide more efficient services to become committee nodes.
Thus, preferred nodes are presumably better committee members than randomly selected ones.

Malicious committee members pose a risk to the safety and liveness of protocols.
Inside the committee, \ac{BFT} protocols are used to tolerate malicious behavior.
Additionally, some algorithms use different mechanisms to reduce the risk of a malicious committee.
Commonly the results of the committee are verified.
This verification is both to check that progress is made, \eg that blocks are proposed, but also of the blocks themselves~\cite{zhan2021drbft, jalalzai2019proteus}.
For example, in Proteus, a committee is replaced with new members if the committee does not generate valid blocks~\cite{jalalzai2019proteus}.
To encourage participation, committee members who act maliciously risk forfeiture of their stake in protocols requiring a deposit to join the committee~\cite{zhou2019dlattice}.

%PoW to stay in the network~\cite{zamani2018rapidchain}, PoS to join network~\cite{jalalzai2021hermes}
%Replacement after every message~\cite{gilad2017algorand}
%Weight (money)~\cite{gilad2017algorand, david2021gearbox} (performance storage, network, bandwidth, latency etc) ~\cite{zhan2021drbft, yu2019proofofqos}
%Voting for candidates -> PBFT members from candidates through random selection~\cite{zhan2021drbft}
%Size optimization~\cite{david2021gearbox}

By definition, committees consist of only a subset of the total nodes.
This opens up the risk that an adversary could gain control of a committee by chance or by adaptive corruptions while controlling less than $\frac{1}{3}$ of the total nodes.
One way to overcome this risk of adaptive corruption is to change out the committee members regularly~\cite{matt2022formalizing}.
For example, in Algorand~\cite{gilad2017algorand} any committee member is replaced after any message sent by the member.
Thus, committee members are immediately replaced once identified as potential victims, making it impossible for attackers to target committee members.
In contrast, for Dumbo~\cite{guo2020dumbo} or in PoQ~\cite{yu2019proofofqos}, committees are persistent for one block generation.
This makes committee members vulnerable but limits the attack impact to one block.
Similarly, in AHL each committee is replaced after every epoch~\cite{dang2019towards}. % it is not further specified how long a epoch lasts
This is assumed to be safe even in the presence of an adaptive attacker model where nodes are not instantly corrupted but rather after some time.
RapidChain~\cite{zamani2018rapidchain} also replaces committee members after every epoch, though as an optimization, it replaces only a subset of committee members.
AHL and RapidChain make limitations on the threat model.
AHL relies on TEEs, which are assumed to only fail by crashing. %\sr{do they really claim to be "impossible to corrupt", or would it better be "correct and fail only by crashing"?}
RapidChain assumes that the adaptive adversary can only corrupt nodes at the start of the protocol and in between epochs, but not within an epoch.
In Hermes~\cite{jalalzai2021hermes}, which does not assume an adaptive adversary, committees can get replaced by view changes.
Non-committee members initiate a view change after not receiving a block over some period of time, after receiving an invalid block proposal, or after multiple proposals with the same sequence number.
% \lj{Kannst du für Hermes noch assumptions dazuschreiben, e.g. Thus Hermes assumes that the increasing corruption of a committee can be detected before the threshold is reached?}
% \av{"...All of these protocols operate in a synchronous environment under an adaptive adversarial model. On the contrary, Hermes can operate in asynchronous environments and does not use the adaptive adversarial model."}

%\subsubsection{Hierarchical}
%\lj{Es ist unklar was hierarchical in dieser section zu tun hat. Geht es darum, dass protokolle die hierarchical sharding benutzen andere metoden haben um in committee für den globale consensus auszuwählen?}\av{vielleicht einfach kürzen aus platz gründen}
%The challenges for hierarchical consensus are similar to the ones for committees and sharding, \eg how layers are formed and how they are monitored.
%Different than consensus by committee, hierarchical consensus uses, potentially multiple, layers~\cite{li2021scalable, li2021optimized, amiri2019caper, xu2021concurrent, amiri2021saguaro} and can like sharding, allow for independent processing (see~\cref{subsec:independent_hierachical}).
%
%Like consensus by committee there is the question how the upper layers should be formed and monitored.
%Some hierarchical algorithms use the leaders of the lower levels as ``representatives'' in the upper levels~\cite{li2021optimized, xu2021concurrent}, as depicted in~\cref{fig:hierarchical}.
%In this way, the monitoring of the committee members and their formation follows the order of the lower-levels.
%For example, in X-Layer~\cite{li2021scalable} all layers monitor the upper layer and issue ``cross-layer'' view changes if inactivity is detected.
%GeoBFT~\cite{gupta2020resilientdb} shares transactions between groups by passing transactions to the leaders of other groups such that all nodes receive the transactions.
%This requires the need for a ``remote'' view-change where one group and initiate a view change in a different group.
%Bypassing the need for a committee like structure, DP-Hybrid~\cite{zheng2020dphybrid} uses a \ac{PoW} approach for odering as an upper layer.


%%A common pattern for hierarhical consensus is that the leaders of the sub-layers act as ``representatives'' of the group in the upper layer as depicted in~\cref{fig:hierarchical}~\cite{li2021scalable, li2021optimized, amiri2019caper, xu2021concurrent}.
%%Here, the global consensus is achieved by the leaders of the sub-layers while application-specific consensus is achieved in sub-layers~\cite{xu2021concurrent, amiri2021saguaro}. %\lj{Can you elaborate, how this increases scalability?}\av{added text}
%%The decision of sub-layers can be collected into blocks to be processed in the upper layers~\cite{amiri2021saguaro, xu2021concurrent, li2021optimized}.
%%As the upper layers only have to reach a consensus on the collected blocks instead of every single request, the work of the upper layers is reduced.
%%Other reasons for a hierarchical consensus might be on location-based factors~\cite{hellings2021byshard} where nodes closer to each other are put into groups, or simply for scalability optimizations~\cite{li2021scalable, li2021optimized} where smaller consensus groups afford higher scalability.
%
%%With algorithms that use ``representatives'' for sub-layers in upper layers, there exist ways for the sub-layer group to impeach the representative. 
%%This is needed to remove crashed or misbehaving representatives.
%%Detection of this can be done through heartbeats to the representative to verify it's still working, verification of the work done by the representative by checking messages sent by the representative against previously received messages~\cite{li2021optimized}.





\subsubsection{Randomized Sampling}
\label{randomized-sampling}
%\sr{oben steht, dass es drei random sampling paper gibt, aber hier sind nur zwei referenziert?}
We have seen how leaders can become the bottleneck regarding scalability.
Randomized sampling can be used to create leaderless consensus protocols, thus avoiding bottlenecks at a single leader~\cite{rocket2020avalanche, lim2014scalable}.
With randomized sampling, nodes only communicate with a subset of the total nodes.
In the process, the nodes exchange information locally about their values.
These values can be the value a node has locally decided as in~\cite{rocket2020avalanche}, or DecicionVectors as in~\cite{lim2014scalable} where the decisions of all nodes are exchanged.
A global consensus is achieved by running these local updates multiple times so that, over time the nodes converge to a single decision.
With these local updates, there is no need for a leader to drive the consensus forward.
This removes the common bottleneck in other consensus protocols, as no single node is responsible for disseminating or collecting any information to and from all other nodes.
The required communication stays constant for each node as it only needs to exchange information with a configurable but constant subset of local nodes~\cite{rocket2020avalanche, lim2014scalable}.
Nodes learn of proposed values from others.
In Avalanche, this happens after the node queries neighboring nodes, while in ~\cite{lim2014scalable}, nodes can also actively push their value to other nodes.
However, they only provide probabilistic safety guarantees. %~\cite{rocket2020avalanche, lim2014scalable}.
%\lj{I do not really understand how this replaced the leader. Does it only replace the leader in collecting and disseminating votes, or also in proposing?}\av{added more text about this}

%Avalanche~\cite{rocket2020avalanche} probabilistic safety 
%Lim~\cite{lim2014scalable} safety hinges on gossip message delivery probability



%\subsubsection{Speculative Execution}
%One paper~\cite{gupta2021poe}.
%\begin{itemize}
%    \item Sees itself between execute-order and order-execute~\cite{androulaki2018hyperledger, kapritsos2012eve}
%    \item Speculative execution of transactions on backups -> assuming primary is correct
%    \item Rollbacks on malicious behavior detection
%    \item Eliminates the last phase of pbft
%    \begin{itemize}
%        \item Clients need the PoE
%        \item Rollback capabilities
%    \end{itemize}
%\end{itemize}

\subsection{Hardware Support}
\label{sec:tee}

%\begin{itemize}
    %\item two papers about TEE, none about TPM?
    % \item
    % \item overview TEE concept: isolation, remote attestation, confidentiality and integrity of computation
    % \item can be used to reduce required resources for protocols, eg less complex crypto, fewer replicas, fewer communication phases, ... as it can only crash, not be Byzantine, prevent equivocation (hybrid fault model)
%\end{itemize}


%\cite{liu2018scalable} FastBFT: novel message aggregation technique combining hardware-based trusted execution environments with lightweight secret sharing; optimizations: optimistic execution, tree topology, failure detection
%cryptographic keys for encryption and signature generation, primary TEE has monotonic counter
%increases scalability by combining TEEs with secret sharing for efficient message aggregation

%\cite{dang2019towards}: secure and efficient shard formation via trusted randomness beacon; use TEE to eliminate equivocation to reduce required resources; append-only memory (to prevent equivocation); message aggregation (issuing proofs of quorums)

%TODO nur zwei paper, die TEE verwenden und unseren Kriterien entsprechen
%bessere Abgrenzung zwischen alten Prinzipien und neuen Papern
%TODO übersicht wie viele paper pro Mechanismus

The scalability of \ac{BFT} protocols can also be improved via hardware support as offered by \acfp{TEE}, as has been done in two papers matching our criteria: FastBFT~\cite{liu2018scalable} and AHL~\cite{dang2019towards}.
The most commonly used \ac{TEE} is \ac{SGX} due to its high availability; however, many approaches are independent of the underlying \ac{TEE}, allowing the use of \eg \acp{TPM}, ARM TrustZone, or AMD SEV-SNP.
%explain TEEs with the example of SGX?
\ac{SGX} is an x86 instruction set extension that allows the creation of so-called \emph{enclaves}, in which confidentiality of execution and integrity of data is ensured from privileged software via hardware-based memory encryption and checksums.
Enclaves can be remotely attested: users can verify an enclave's code and data, and before execution of the enclave, the provided code and data is hashed to create a measurement.
This measurement can then be compared against the value of the verified enclave to ensure that the user communicates with a genuine, correct, and unmodified version of the expected enclave. 

\acp{TEE} can therefore be used as a trusted subsystem in \ac{BFT} protocols with a \emph{hybrid} fault model:
while the remainder of the \ac{BFT} system can still behave arbitrarily faulty, the trusted subsystem is assumed to behave correctly and can only fail by crashing.
% trusted counter
This can be used to prevent equivocation, \ie sending conflicting messages to different communication partners in the protocol.
Primitives that make use of such trusted subsystems are \eg trusted counters~\cite{levin09trinc} or attested append-only memory~\cite{chun07a2m}.
FastBFT~\cite{liu2018scalable} employs trusted monotonic counters that are provided by the \ac{TEE} running on the leader replica.
A counter value extends every message sent by the leader, and as every value can only be used once and the counter is monotonically increasing, it can thus be detected if the leader equivocates.
Relying on trusted hardware and therefore preventing equivocation allows reducing the complexity of \ac{BFT} protocols, \eg by decreasing the number of replicas from $3f+1$ to $2f+1$ or the required communication rounds for agreement, or by using less expensive cryptographic primitives (see also \cref{sec:crypto}). 

%message aggregation
\acp{TEE} are also used to efficiently aggregate messages \cite{dang2019towards, liu2018scalable}, \eg by combining a quorum of $2f+1$ messages into a proof issued by the \ac{TEE}~\cite{dang2019towards}.
Here, the leader collects and aggregates other nodes' signatures into a single authenticated message, while nodes forward their signed messages to the leader and verify the created multi-signature.
% For this, we also refer to the description in~\cref{sec:crypto:aggregation}.
%
%shard formation
%TODO is this covered in the sharding subsection?
Furthermore, \acp{TEE} can also be used as a source for a trusted randomness beacon, which can be used to efficiently partition the system for sharding~\cite{dang2019towards} (see also \cref{sec:sharding}).





\newcommand{\cell}[2]{\multicolumn{1}{c|}{\cellcolor[HTML]{#1}{#2}}}
\newcommand{\doublecell}[2]{\begin{tabular}[c]{@{}c@{}}{#1}\\ {#2}\end{tabular}}

\newcommand{\conf}{conf.}

\newcommand{\networkunk}{network?}
\newcommand{\networksync}{sync}
\newcommand{\networkasync}{async}
\newcommand{\networkpartialsync}{part. sync}

\newcommand{\maxfhalf}{$f<\frac{1}{2}n$}
\newcommand{\maxfthird}{$f<\frac{1}{3}n$}
\newcommand{\maxfonethreen}{1/3n}
\newcommand{\maxfunk}{maxf?}
\newcommand{\maxfpowbound}{PoW bound}
\newcommand{\maxfparameterizable}{paramet.}
\newcommand{\maxfresdb}{\doublecell{$f<\frac{1}{3}n$}{per cluster}}
\newcommand{\maxfSBFT}{\doublecell{$n=$}{$3f+2c+1$}}

\newcommand{\membershipdynamic}{dynamic}
\newcommand{\membershipstatic}{static}

\newcommand{\safetydeterministic}{det.}
\newcommand{\safetyprobabilistic}{prob.}

\newcommand{\leadernone}{none}
\newcommand{\leadermulti}{multi}
\newcommand{\leaderpowelected}{PoW-elected}
\newcommand{\leaderrandomselection}{random select.}
\newcommand{\leaderrotating}{rotating}
\newcommand{\leadersingle}{single}


\newcommand{\cryptovrf}{VRF}
\newcommand{\cryptoaggregate}{aggregate}
\newcommand{\cryptoaggregation}{\cryptoaggregate}
\newcommand{\cryptobls}{BLS}
\newcommand{\cryptoerasurecoding}{erasure coding}
\newcommand{\cryptothreshold}{threshold sig.}
\newcommand{\cryptomultisignature}{multi sig.}
\newcommand{\cryptosecretsharing}{secret sharing}
\newcommand{\cryptothresholdencryption}{\cryptothreshold}

\newcommand{\exchangegossip}{gossip}
\newcommand{\exchangestar}{star}
\newcommand{\exchangeclique}{clique}
\newcommand{\exchangering}{ring}
\newcommand{\exchangetree}{tree}

\newcommand{\pipelininghotstuff}{c.f. HotStuff}

\newcommand{\parallelizationsharding}{sharding}
\newcommand{\parallelizationclustering}{clustering}
\newcommand{\parallelizationhierachicalconsensus}{hierarchical}

\newcommand{\consensushierarchical}{hierarchical}
\newcommand{\consensuscommittee}{committee}
\newcommand{\consensusrandomizedsampling}{rand. sampling}
\newcommand{\consensusspeculativeexecution}{\doublecell{speculative}{execution}}


\newcommand{\colorassumption}{C9FEFC}
\newcommand{\colorguarantees}{E0FFE0}
\newcommand{\colorscaling}{FFFFC7}


\input{sections/table}


\subsection{Summary of BFT Protocols}
All discussed protocols are shown in \cref{tab:protocols}.
We list their assumptions regarding the synchrony model, the number of faults that can be tolerated, membership of nodes (\ie whether nodes are static or can dynamically join or leave the network), and the guarantees for safety and liveness.
Further, we give an overview of which scaling techniques have been employed and combined in the protocols.
PBFT, the starting point for many protocols, assumes a partially synchronous network of static nodes, whereas many of the blockchain BFT protocols target a dynamic network of nodes or the asynchronous model.
Many protocols replace PBFT's single leader with multiple leaders to share the load and reduce its bottleneck, or with a leaderless approach.
Not all blockchain BFT protocols keep PBFT's clique communication; we take clique as the default and list in \cref{tab:protocols} which protocols deviate from this pattern.
While many protocols target improving scalability within the consensus group while increasing the number of nodes, protocols tagged as \enquote{committee}, \enquote{sharding}, and \enquote{hierarchical} generally target scalability and performance improvements by using subsets of nodes participating in consensus.
Some listed approaches are generic frameworks that are not fixed to one specific protocol, \eg RCC~\cite{rcc2021gupta}, Ostraka~\cite{manuskin2020ostraka}, Mitosis~\cite{marson2021mitosis}, RingBFT~\cite{rahnama2021ringbft}, and ByShard~\cite{hellings2021byshard}.
Here, we list assumptions and guarantees if explicitly stated in the corresponding papers or list them as configurable if applicable.
