% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{times}
\usepackage{epsfig}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{color}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
\usepackage{color}
\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\begin{document}
%
%\title{Contribution Title\thanks{Supported by organization x.}}
\title{SwinVFTR: A Novel Volumetric Feature-learning Transformer for 3D OCT Fluid Segmentation}
%
\titlerunning{SwinVFTR}
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%\author{Anonymous}
\author{
Sharif Amit Kamran\inst{1} \and
Khondker Fariha Hossain\inst{1} \and
Alireza Tavakkoli\inst{1} \and 
Salah A. Baker\inst{2} \and
Stewart Lee Zuckerbrod\inst{3}
%Kenton M. Sanders\inst{3} \and 
}

%\author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
%Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
%Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
%
\authorrunning{Kamran et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Dept. of Computer Science \& Engineering, University of Nevada, Reno, NV, USA\and School of Medicine, University of Nevada, Reno, NV \and Houston Eye Associates, Houston, TX}
%\institute{Anonymous organization\\
%\email{***@*****.***}\\}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Accurately segmenting fluid in 3D volumetric optical coherence tomography (OCT) images is a crucial yet challenging task for detecting eye diseases. Traditional autoencoding-based segmentation approaches have limitations in extracting fluid regions due to successive resolution loss in the encoding phase and the inability to recover lost information in the decoding phase. Although current transformer-based models for medical image segmentation addresses this limitation, they are not designed to be applied out-of-the-box for 3D OCT volumes, which have a wide-ranging channel-axis size based on different vendor device and extraction technique. To address these issues, we propose SwinVFTR, a new transformer-based architecture designed for precise fluid segmentation in 3D volumetric OCT images. We first utilize a channel-wise volumetric sampling for training on OCT volumes with varying depths (B-scans). Next, the model uses a novel shifted window transformer block in the encoder to achieve better localization and segmentation of fluid regions. Additionally, we propose a new volumetric attention block for spatial and depth-wise attention, which improves upon traditional residual skip connections. Consequently, utilizing multi-class dice loss, the proposed architecture outperforms other existing architectures on the three publicly available vendor-specific OCT datasets, namely Spectralis, Cirrus, and Topcon, with mean dice scores of 0.72, 0.59, and 0.68, respectively. Additionally, SwinVFTR outperforms other architectures in two additional relevant metrics, mean intersection-over-union (Mean-IOU) and structural similarity measure (SSIM).

%The abstract should briefly summarize the contents of the paper in
%150--250 words.

\keywords{Fluid Segmentation  \and Optical Coherence Tomography \and Swin Transformer \and OCT Segmentation.}
\end{abstract}
%
%
%
\section{Introduction}
The macula is part of the retinal subspace primarily responsible for central vision. Fluid buildup, or macular edema in retinal layers, is a common reason for blindness and retinal degeneration \cite{goatman2006reference}. Possible factors include Drusen, Choroidal neovascularization (CNV), Age-related macular degeneration (AMD), and Diabetic retinopathy (DR) \cite{goatman2006reference,tranos2004macular}. Age-related macular degeneration causes irreversible blindness in approximately 8.7\% of people worldwide and is a leading cause of vision loss. Furthermore, it is projected to increase threefold each decade for people over 40 in developed countries \cite{wong2014global}. Similarly, Diabetic retinopathy affects one-third of every diabetic patient \cite{ting2016diabetic}, which is 2.8\% of the world's population and the second leading cause of blindness \cite{bourne2013causes}. As a result, early diagnosis, localization, and segmentation of retinal layer fluid accumulation can help with effective treatments and personalized therapy. Optical coherence tomography (OCT) is a non-invasive retinal imaging method \cite{srinivasan2014fully} that yields 3D volumetric cross-sectional images for viewing the morphology of retinal layers and underlying pathologies. Although the image is extracted through this approach, the differential diagnosis and fluid localization are supervised by an expert ophthalmologist. 

Manual annotation and segmentation of sub-retinal fluid can be time-consuming, subject to error, and tedious task. Hence, experts in the past proposed and incorporated many image-processing \cite{garvin2008intraretinal,garvin2009automated} and machine learning \cite{chiu2015kernel,quellec2010three} techniques to alleviate this problem. However, those traditional approaches required handcrafted feature engineering and pruning, trouble learning spatial and depth features, and less generalization ability. With the advent of deep learning, automated segmentation tasks for medical imaging has surged in popularity, given their effectiveness to pixel-wise segment with high accuracy and precise surface estimation for volumetric and spatial imaging data. For retinal fluid segmentation, 2D U-Net-like auto-encoder models have been predominantly incorporated in early \cite{roy2017relaynet,schlegl2018fully} and recent works \cite{yang2020rmppnet,ma2021lf,xing2022multi,he2022intra}.  These models achieved quite good results for multi-layer fluid segmentation in overall metrics. However, these architectures fail when segmenting fine fluid-layer boundaries and detecting small deposits of fluids. In contrast,  recent vision transformer-based auto-encoders for fluid segmentation try to address this problem by utilizing multi-headed window attention \cite{wang2022tiny}, or shifted-window attention \cite{philippi2023vision} for capturing local context, improving tiny fluid segmentation. The biggest drawback is these approaches are trained and tested on 2D slices, which only contain spatial features and have no context regarding inter-slice depth information. Though an early work \cite{lin2017focal} has utilized a 3D U-Net model for retinal fluid segmentation, any new development since then has been stagnant. 


\textbf{Our Contributions:} By taking all this into account, we propose a novel architecture termed Swin Volumetric Feature-learning Transformer (SwinVFTR) that utilizes a swin-transformer as an encoder and joins it to a 3D convolution-based decoder at distinct resolutions via novel volumetric spatial and depth attention block. Moreover, we modify the swin-transformer block with a Multi-receptive field residual block instead of MLP. Our model employs a channel-wise overlapped sampling technique to crop OCT volumes only in the depth axis, while retaining the spatial information. \iffalse and a new volumetric feature similarity loss function that minimizes the distance between encoder and decoder features. \fi To validate our work, we compare four different 3D convolution and transformer-based architectures for medical image segmentation on three vendor-specific OCT datasets: Spectralis, Cirrus, and Topcon \cite{bogunovic2019retouch}. From Fig.~\ref{fig2}, it is apparent that our architecture segments retinal fluid with high dice-score and mean-IOU.

\begin{figure}[tp!]
    \centering
    \includegraphics[width=1\linewidth]{Fig1.png}
    \caption{Proposed SwinVFTR architecture which takes 3D OCT volume input with channel-wise sampling technique and outputs a 3D segmentation map of the fluid accumulation. The SwinVFTR encoder incorporates a new swin-transformer block consisting of Shifted window attention, Multi-headed attention and Multi-Receptive Field (MRF) sub-block with both convolution and dilated convolution layers. The encoder features are sequentially added with the decoder using a skip-connection consisting of a volumetric attention (VA) block.}
    \label{fig1}
\end{figure}


\section{SwinVFTR}
\subsection{Channel-wise Volumetric Sampling}
Sampling OCT B-scans at different depths can affect the outcome of recognizing retinal disease pathology for accurate diagnosis \cite{nittala2011effect}. Although U-Net-like architectures are flexible in handling OCT volumes of different depths, current transformer-based architecture cannot take OCTs with smaller depths. For example, UNETR \cite{hatamizadeh2022unetr} and Swin-UNETR \cite{hatamizadeh2022swin}, two state-of-the-art models for medical image segmentation, utilize patch-merging layer to downsample $\times 32$. As a result, any OCT with less than 64 B-scans cannot be used out-of-the-box for these models. Since we are working on diversified OCT volumes with B-scans of 49 to 128, utilizing volumetric cropping would be ideal. However, we want to retain the spatial information while sampling a section of the original B-scans. So we introduce a channel-wise sampling technique that samples a $ H \times W \times D$ dimensional cropped image from an image with $H \times W \times C$ dimensions, where $D < C$ and $D=32$. We also utilize one less swin-tranformer and patch-merging block to make our downsampling $\times 16$. While producing the output, we do channel-wise overlapped volume stitching (25\% overlap) which is given in Fig.~\ref{fig1}. 


\subsection{Proposed Swin-Transformer Block}
Regular window-based multi-head self-attention (W-MSA), which was incorporated in Vision Transformer (ViT) \cite{dosovitskiyimage}, employs a single low-resolution window for constructing a global feature map and has quadratic computation complexity. Contrastly, the Swin Transformer architecture proposed in \cite{liu2021swin} integrates shifted windows multi-head self-attention (SW-MSA), which builds hierarchical local feature maps and has linear computation complexity. Recently, Swin-UNETR \cite{hatamizadeh2022swin} adopted this swin-transformer block without making any fundamental changes and achieved state-of-the-art dice scores in different 3D medical image segmentation tasks. One of the most significant drawback of these blocks is using a Multi-layer perceptron block (MLP) after the post-normalization layer. MLP utilizes two linear (dense) layers, which are computationally more expensive than a 1D convolution with a small kernel. For example, a linear embedding output from a swin-transformer layer having dimension $X$ (where $X$ = $H\times W\times D$), and input channel, $C_{in}$ and output channel, $C_{out}$ will have a total number of parameters, $X\times C_{in} \times C_{out}$. In contrast, a 1D Conv with kernel size, $k$ with the same input and output will have less number of parameters, $k\times C_{in} \times C_{out}$. Here, we did not consider any parameters for bias and the value of $k=\{1,3\}$. On the other hand, using 1D convolution will drastically affect the performance, given that small receptive fields only accounts for local features and not global ones. Hence, we employ a multi-branch residual block with vanilla and dilated convolution termed, Multi-receptive field Block (MRF) to address this. So for subsequent layers, $l$ and $l+1$, the proposed swin-transformer block can be defined as Eq.~\ref{eq1}.

\begin{equation}
    \begin{split}
    &d^{l} = W{\text -}MSA(\psi(d^{l-1}))\\
    &d^{l} = MRF(\psi(d^{l})) + d^{l}) \\
    &d^{l+1} = SW{\text -}MSA(\sigma(d^{l}))\\
    &d^{l+1} = MRF(\psi(d^{l+1})) + d^{l+1}) 
    \end{split}
    \label{eq1}
\end{equation}

In Eq.~\ref{eq1}, we visualize the first sub-block of swin-transformer consisting of LayerNorm ($\psi$) layer, multi-head self attention module (W-MSA), residual connection (+) and Multi-receptive field block (MRF). In a similar manner, the second sub-block of swin transformer consisting of LayerNorm ($\psi]$) layer, shifted window multi-head self attention module (SW-MSA), residual skip-connection (+) and Multi-receptive field block (MRF). Moreover, $l$ signifies the layer number and $d$ is the feature-map. The MRF block can be further elaborated as given in Eq.~\ref{eq2}.

\begin{equation}
    \begin{split}
    &x^1 = \delta(Conv(x_{in}))\\
    &x^2 = \delta(Depthwise\_Conv(x^1)) \\
    &x^3 = \delta(Dilated\_Conv(x_{in})) \\
    &x_{out} =  \delta(Conv(x^1 + x^2 + x^3))
    \end{split}
    \label{eq2}
\end{equation}

In Eq.~\ref{eq2}, we first use convolution with kernel size, $k=1$, and stride, $s=1$ to extract local features with a small receptive field. Then, the output of these layers is inserted into a depth-wise convolution layer ($k=1$, $s=1$). In a parallel branch, a dilated convolution ($k=3$, $s=1$) with dilation rate, $d=2$ is utilized to extract features with a larger receptive field. Finally, we add all these outputs from these three convolution layers and then apply a convolution ($k=1$, $s=1$) to get the final result. Here, $\delta$ signifies the GELU activation, which is applied to all convolution layers.

\subsection{Encoder}
Before the encoder can take the input with dimensions $H \times W \times D$, we transform the OCT volumetric images. We employ a patch partition step to create a sequence of 3D tokens with a dimension of $\frac{H}{P} \times \frac{W}{P} \times \frac{D}{P}$ and these features are then projected to an embedding space with dimension $C$. Specifically, our encoder has a non-overlapping patch with a size of 2 × 2 × 2 and a feature dimension of 2 × 2 × 2 × 1 = 16 by considering one channel of the OCT. We assign the embedding space size, C=24, in our encoder. So the feature output of the patch-parition layer is $\frac{H}{2} \times \frac{W}{2} \times \frac{D}{2} \times 24$ .  Likewise, each encoder stage downsamples the features by utilizing two swin-transformer blocks followed by a patch-merging block. So, the  features size changes from $\frac{H}{2} \times \frac{W}{2} \times \frac{D}{2} \times C$ to $\frac{H}{4} \times \frac{W}{4} \times \frac{D}{4} \times 2C$, from $\frac{H}{4} \times \frac{W}{4} \times \frac{D}{4} \times 2C$ to $\frac{H}{8} \times \frac{W}{8} \times \frac{D}{8} \times 4C$, and from $\frac{H}{8} \times \frac{W}{8} \times \frac{D}{8} \times 4C$ to $\frac{H}{16} \times \frac{W}{16} \times \frac{D}{16} \times 8C$, successively. We incorporate two swin-transformer blocks after the last patch-merging layer to finalize the encoder.

\subsection{Volumetric Attention Block}
In 3D UNet-like architectures \cite{hatamizadeh2022unetr,lin2017focal,li2019segmentation}, skip connections concatenate the encoder and decoder features to retain loss of information. However, to make these features more robust, Swin-UNETR incorporated residual attention block with two convolution layers similar to \cite{oktayattention,kerfoot2019left}. The problem with this approach is that it utilizes regular convolution, which only applies attention spatially and ignores any channel-wise attention. To alleviate this, we propose a volumetric attention (VA) block consisting of separate branches. In the first branch, we have a $3 \times 3 \times 3$ followed by a $1 \times 1 \times 1$ convolution for spatial attention. In the following branch, we have a $1 \times 1 \times 1 $ depth-wise convolution followed by a $1 \times 1 \times 1$ point-wise convolution for channel-wise attention. In the final branch, we have an identity function that copies the input features. Consequently, we add all of these features to generate our last output feature.

\subsection{Decoder}
Similar to our encoder, we design a symmetric decoder composed of multiple transposed convolution blocks and a volumetric concatenation layer between each stage of the encoder and decoder features.  At each stage n ($n \in {1,2,3}$) in the encoder and bottleneck ($n=4$), the volumetric feature representations are reshaped to $\frac{H}{2^n} \times \frac{H}{2^n} \times \frac{H}{2^n}$ and inserted into a residual convolution block with with two $3 \times 3 \times 3$ convolution followed by instance normalization layer. Each decoder's feature maps are doubled in size using a transposed convolution layer. Moreover, each encoder's skip feature maps through the VA blocks are concatenated with the outputs of the previous decoder. Finally, a residual convolution block is applied to the feature with two $3 \times 3 \times 3$ convolutions followed by an instance normalization laye. The final segmentation output is generated using a $1 \times 1 \times 1$  convolutional layer and a softmax activation function.


\iffalse
\subsection{Proposed Volumetric Feature Similarity Loss}

We lose significant volumetric feature information by successive downsampling and upsampling, so we need to utilize a similarity-based learnable parameter that can help our overall performance. To alleviate this, we propose volumetric feature similarity loss ($L_{vfs}$ as given in Eq.~\ref{eq3} that minimizes the distance encoder and decoder's volumetric features.

\begin{equation}
 \mathcal{L}_{vfs} = \mathbb{E}_{x} \frac{1}{N} \sum_{i=1}^{k}\parallel Enc_{f}^{i}(x)-Dec_{f}^{i}(x)\parallel
    \label{eq3}
\end{equation}

Eq.~\ref{eq3} is calculated by taking the features from three stages of the patch-merging blocks of the encoder, $Enc^{i}_{f}$ and subtracting it from the subsequent decoders, $Dec^{i}_{f}$.  Here, $x \epsilon \mathbb{R}^{\frac{H}{2^n} \times \frac{W}{2^n} \times \frac{D}{2^n}}$ is the input feature and $N$ stands for the number of features. 
\fi

\subsection{Objective Function}
For our segmentation output we utilize the Dice coefficient loss given in Eq.~\ref{eq4}. For dice-coefficient we use $\epsilon=1.0$ in numerator and denominator for addressing the division by zero. Here, $\mathbb{E}$ signifies,
expected values given, $y'$ (prediction) and $y$ (ground-truth).
\begin{equation}
     \mathcal{L}_{dice} =\mathbb{E}_{y',y} \big[ 1-\frac{2\sum_{i=1}^{N}y'_{i}y_{i}+\varepsilon}{\sum_{i=1}^{N}y'_{i}+\sum_{i=1}^{N}y_{i}+\varepsilon}\big]
    \label{eq4}
\end{equation}

\iffalse
By combining Eq.~\ref{eq3} and Eq.~\ref{eq4} we create our final cost function as provided in Eq.~\ref{eq5}. Here, $\lambda$ is the weights for each of the losses.

\begin{equation}
     \mathcal{L} = \lambda_{dice}\mathcal{L}_{dice} + \lambda_{vfs}\mathcal{L}_{vfs} 
    \label{eq5}
\end{equation}
\fi

\begin{figure}[tp!]
    \centering
    \includegraphics[height=7cm,width=0.8\linewidth]{Fig2.png}
    \caption{SwinVFTR segments fluid with better precision than other 3D CNN and Transformer architectures. The row contains Cirrus, Spectralis and Topcon data-sets. Whereas the column contains ground-truths and segmentation maps for SwinVFTR, SwinUNETR, UNETR, ResUNet-3D and Attention-UNet-3D. Here, IRF, SRF, and PED fluids are colored as {\color{red} Red}, {\color{yellow} Yellow} and {\color{blue} Blue}.}
    \label{fig2}
\end{figure}




\section{Experiments}
\subsection{Dataset and Preprocessing}
For benchmarking, we use the RETOUCH public dataset \cite{bogunovic2019retouch}, which contains three image sets from three unique vendor devices and, in total, has 70 volumes. Out of this, 24 volumes were obtained with Cirrus (Zeiss), 24 volumes with Spectralis (Heidelberg), and 22 volumes with T-1000 and T-2000 (Topcon) devices. The numbers of B-scans (volume depths) were 128, 49, and 128, with resolutions of 512×1024, 512×496, and 512×885, respectively, for each of these vendor devices. We only resize Cirrus and Topcon volumes to $512\times 512$ resolution. The volume contained three different fluids such as intra-retinal fluid (IRF), sub-retinal fluid (SRF), and pigment epithelial detachment (PED), which were manually annotated by an expert as separate ground truth volumes. As test datasets are no more publicly available, we separate the original image sets into training and test set. So for Cirrus and Spectralis, we had 19 training and 5 test volumes, whereas, for Topcon, we had 18 training and 4 test volumes. We further utilize 5-fold cross-validation to find the model with the highest dice score. For image transformations, we apply Random Intensitity Shift ( +/- 10 with 50\% probability) and Random Channel-wise volumetric cropping for training our model.\\
\textbf{Data use declaration and acknowledgment:} The dataset was released as part of \href{https://retouch.grand-challenge.org}{Retouch Challenge}. For usage, please refer to the \href{https://rumc-gcorg-p-public.s3.amazonaws.com/f/challenge/111/7915e6c2-66b9-4888-971c-1fcabae862da/RETOUCH-Agreement_of_Data_Confidentiality.pdf}{data confidentiality agreement}.

\subsection{Hyper-parameter Initialization}
We used Adam optimizer \cite{kingma2014adam}, with learning rate $\alpha=0.0001$, $\beta_1=0.9$ and $\beta_2=0.999$. We train with mini-batches with batch size, $b=1$ for 600 epochs using PyTorch and MONAI library \href{https://monai.io/}{monai.io}. It took between 8-12 hours to train our model on NVIDIA A30 GPU, depending on the data-set. Because Spectralis has a lower number of B-scans compared to Cirrus and Topcon, it takes less amount to train. The inference time is $0.5$ second per volume. We provide ablation for hyper-parameter selections in subsection~\ref{subsec:ablation} and in the supplementary materials. The code repository is provided in this \href{https://github.com/SharifAmit/Swin-VFTR}{link}. % Code will be publicly released. 
\begin{table}[tp!]
\centering
\caption{Quantitative comparison on Spectralis,  Cirrus, \&  Topcon \cite{bogunovic2019retouch}.}
\begin{adjustbox}{width=1\linewidth}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multirow{2}{*}{Year}  & \multirow{2}{*}{SSIM}  & \multirow{2}{*}{Mean-IOU}  & \multicolumn{3}{c|}{Dice Score}  &  \multicolumn{2}{c}{Mean Dice} \\ \cline{6-10} 
& & & & & IRF & SRF & PED & w/o BG & w/ BG \\ \hline
\multirow{5}{*}{Spectralis}  & Attention-UNet-3D \cite{oktayattention} & 2018 &  0.914 &   0.439 &  0.608 & 0.394 & 0.096 & 0.366 & 0.517 \\ 
& ResUNet-3D\cite{kerfoot2019left} & 2019 & 0.985 & 0.595 & 0.602 & 0.574 & 0.602 & 0.592  &  0.694 \\ 
& UNETR \cite{hatamizadeh2022unetr} & 2022 & 0.984 & 0.546 & 0.567 & 0.493 & 0.544 & 0.534 & 0.651 \\ 
& SwinUNETR \cite{hatamizadeh2022swin,tang2022self} & 2022 & 0.985 & 0.613 & 0.601 & 0.544  & 0.662 & 0.602 &  0.701     \\ 
& \textbf{SwinVFTR} & 2023 & \textbf{0.987} & \textbf{0.625} & \textbf{0.624} & \textbf{0.578} & \textbf{0.670} & \textbf{0.624} & \textbf{0.718}  \\\hline	
\multirow{5}{*}{Cirrus}& Attention-UNet-3D   \cite{oktayattention} & 2018 & 0.928 & 0.446 & 0.664 & 0.472 & 0.011 & 0.382 & 0.527\\
& ResUNet-3D \cite{kerfoot2019left}  & 2019 &  0.983 & 0.490 & 0.648 & \textbf{0.622} & 0.012 & 0.427 & 0.570 \\ 
& UNETR \cite{hatamizadeh2022unetr} & 2022 & 0.987 & 0.487 & 0.635 & 0.594 & 0.081 &  0.436 & 0.577     \\
& SwinUNETR \cite{hatamizadeh2022swin,tang2022self} & 2022 & 0.986 & 0.452 & 0.682 & 0.338 & 0.131 & 0.384 & 0.537\\
& \textbf{SwinVFTR} & 2021 & \textbf{0.988} & \textbf{0.492} & \textbf{0.691} & 0.507 & \textbf{0.146} & \textbf{0.448} & \textbf{0.587}  \\\hline
\multirow{5}{*}{Topconn}& Attention-UNet-3D \cite{oktayattention} & 2018 & 0.894 & 0.412 & 0.526 & 0.542 & 0.083 & 0.383 & 0.519\\
& ResUNet-3D \cite{kerfoot2019left}  & 2019  & 0.974 & 0.526 & 0.534 & 0.\textbf{648} & 0.419 & 0.534 & 0.649 \\ 
& UNETR \cite{hatamizadeh2022unetr} & 2022 & 0.979 & 0.495 & 0.592 & 0.416 & 0.427 & 0.478 &  0.607      \\
& SwinUNETR \cite{hatamizadeh2022swin,tang2022self} & 2022 & 0.980 & 0.483 & 0.583 & 0.451 & 0.331 & 0.455 & 0.590 \\
& \textbf{SwinVFTR} & 2023 & \textbf{0.981} & \textbf{0.553} & \textbf{0.638} & 0.523 & \textbf{0.548} & \textbf{0.571} & \textbf{0.678}  \\\hline	
\end{tabular}
\label{table1}
\end{adjustbox}
\end{table}

\subsection{Quantitative Evaluation}
We compared our architecture with some best-performing 3D CNN and Transformer architectures, including ResUNet-3D \cite{kerfoot2019left}, AttentionUNet-3D \cite{oktayattention}, UNETR \cite{hatamizadeh2022unetr} and SwinUNETR \cite{hatamizadeh2022swin} as illustrated in Fig.~\ref{fig2}. We trained and evaluated all four architectures using their publicly available source code on the three datasets. SwinUNETR utilizes a swin-transformer encoder as a backbone and a step-wise decoder with transposed convolution and residual blocks to upsample the features. In contrast, the UNETR employs a vision transformer with self-attention layers as encoders and deconvolution layers for upsampling. ResUnet-3D and Attention-UNet-3D are simple modifications of UNet 3D architectures, with the first using residual layers in the encoder and decoders and the second incorporating attention in the skip connections between them. In Fig.~\ref{fig2}, we visualize segmentation results for intra-retinal fluid (IRF), sub-retinal fluid (SRF), and pigment epithelium detachments (PED). It is apparent from the figure that our model's prediction is more accurate than other transformer and CNN-based architectures, and the segmentation boundary is finer and less coarse than SwinUNETR and UNETR.

Next, we quantitatively evaluate all five models using mean-intersection-over-union (mIOU), dice scores, and structural similarity index (SSIM) as shown in Table.~\ref{table1}. We also provide fluid-wise dice scores for IRF, SRF, and PED. Table.~\ref{table1} shows that our model's overall dice score, SSIM, and mIOU far exceed other architectures. Although for Cirrus and Topcon, our model's segmentation performance is a little worse for SRF fluid against ResUNet-3D, the dice score PED is almost $10\times$ better for Cirrus and $1.2\times$ better for Topcon. Another essential evaluation we did was calculating dice score with (w/ BG) and without background (w/o BG), as background contains the majority of the pixels, and it can skew the results with high false-positive rates. As the table shows, our model outperforms other architectures with a higher dice score for both with and without background.


\subsection{Ablation Study}
\label{subsec:ablation}
\textbf{Hyper-parameter search:} In supplementary Table.~1, we have provided the effects of different hyper-parameters such as epoch $e$, learning rate, $\alpha$ and batch-size, $b$. Our search space for epoch, $e=\{100,300,600\}$, learning-rate, $\alpha=\{10e-4, 5e-5, 10e-5\}$, and batch-size, $b=\{1,2,3\}$. We found $e=600$, $\alpha=10e-4$, and $b=1$ to be the best-performing hyper-parameters for SwinVFTR. 
\\\textbf{Effects of Architectural Change:} In the supplementary Table.~2, we provide the comparative comparison of SwinVFSTR's performance with and without VA (Volumetric Attention) and MRF (Multi-receptive field) Blocks. As the results show, our model with VA and MRF achieves the highest dice score and mean-IOU, and our model with VA only performs the second best. The model without VA and MRF blocks incorporates residual convolution for skip connected instead of VA and regular swin-transformer layer with MLP instead of MRF.




\section{Conclusion}
In this paper,  we proposed a new 3D transformer-based fluid segmentation architecture called SwinVFTR. Combining our novel channel-wise sampling technique, incorporating volumetric attention, and utilizing a multi-receptive field swin-transformer block, the architecture segments fluid volumes with high precision for three relevant metrics. We provide comparative comparisons with state-of-the-art 3D CNN and Transformer models, where our model supersedes them. Clinical experts can efficiently employ our architecture in various segmentation applications and ophthalmic modalities. The model is best suited for locating inter- and sub-retinal layer fluid deposits for early disease diagnosis and monitoring future prognosis. We hope to extend this work to other ophthalmic modalities.


\section*{Acknowledgement}
This material is based upon work supported by the **** under Grant No. **** issued through ****. 


\iffalse
\section{First Section}
\subsection{A Subsection Sample}
Please note that the first paragraph of a section or subsection is
not indented. The first paragraph that follows a table, figure,
equation etc. does not need an indent, either.

Subsequent paragraphs, however, are indented.

\subsubsection{Sample Heading (Third Level)} Only two levels of
headings should be numbered. Lower level headings remain unnumbered;
they are formatted as run-in headings.
\paragraph{Sample Heading (Fourth Level)}
The contribution should contain no more than four levels of
headings. Table~\ref{tab1} gives a summary of all heading levels.

\begin{table}
\caption{Table captions should be placed above the
tables.}\label{tab1}
\begin{tabular}{|l|l|l|}
\hline
Heading level &  Example & Font size and style\\
\hline
Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
\hline
\end{tabular}
\end{table}


\noindent Displayed equations are centered and set on a separate
line.
\begin{equation}
x + y = z
\end{equation}
Please try to avoid rasterized images for line-art diagrams and
schemas. Whenever possible, use vector graphics instead (see
Fig.~\ref{fig1}).

\begin{figure}
\includegraphics[width=\textwidth]{fig1.eps}
\caption{A figure caption is always placed below the illustration.
Please note that short captions are centered, while long ones are
justified by the macro package automatically.} \label{fig1}
\end{figure}

\begin{theorem}
This is a sample theorem. The run-in heading is set in bold, while
the following text appears in italics. Definitions, lemmas,
propositions, and corollaries are styled the same way.
\end{theorem}
%
% the environments 'definition', 'lemma', 'proposition', 'corollary',
% 'remark', and 'example' are defined in the LLNCS documentclass as well.
%
\begin{proof}
Proofs, examples, and remarks have the initial word in italics,
while the following text appears in normal font.
\end{proof}
For citations of references, we prefer the use of square brackets
and consecutive numbers. Citations using labels or the author/year
convention are also acceptable. The following bibliography provides
a sample reference list with entries for journal
articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
and a homepage~\cite{ref_url1}. Multiple citations are grouped
\cite{ref_article1,ref_lncs1,ref_book1},
\cite{ref_article1,ref_book1,ref_proc1,ref_url1}.

\subsubsection{Acknowledgements} Please place your acknowledgments at
the end of the paper, preceded by an unnumbered run-in heading (i.e.
3rd-level heading).


%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
\begin{thebibliography}{8}
\iffalse
\bibitem{ref_article1}
Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

\bibitem{ref_lncs1}
Author, F., Author, S.: Title of a proceedings paper. In: Editor,
F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
Springer, Heidelberg (2016). \doi{10.10007/1234567890}

\bibitem{ref_book1}
Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
Location (1999)

\bibitem{ref_proc1}
Author, A.-B.: Contribution title. In: 9th International Proceedings
on Proceedings, pp. 1--2. Publisher, Location (2010)

\bibitem{ref_url1}
LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
Oct 2017
\fi
\end{thebibliography}
\fi


\bibliographystyle{splncs04}
\bibliography{reference}
\end{document}
