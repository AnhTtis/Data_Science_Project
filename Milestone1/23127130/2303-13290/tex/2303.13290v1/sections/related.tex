%+++++++++++++++++++++++++++++++++++++
\begin{figure*}[!hbt]
\centering
\includegraphics[width=1.0\textwidth]{figures/framework.pdf}
\caption{
\ourmethod uses shared weighted network extracts point-level features $\bm{\mathcal{F}}^s$ and $\bm{\mathcal{F}}^t$, overlap scores $\bm{O}^s$ and $\bm{O}^t$ from point clouds $\bm{\mathcal{P}}^s$ and $\bm{\mathcal{P}}^t$, respectively.
Cluster head consumes $\bm{\mathcal{F}}^s$ and $\bm{\mathcal{F}}^t$ to calculate probability matrices $\bm{S}^s$ and $\bm{S}^t$, which are applied to estimate the parameters $(\pi^x_{j},\bm{\mu}^x_{j},\bm{\Sigma}^x_{j}), x{\in}\{s,t,f_s,f_t\}$, of GMMs. Next, cluster-level and point-level matching modules estimate the correspondences $\mathcal{M}$, which are used to estimate the transformation $T$. The network is trained using local contrastive, self-consistency, and cross-consistency losses. $\bm{S}$ is the concatenation of $\bm{S}^s$ and $\bm{S}^t$.
$\bm{\mathcal{P}}$ and $\bm{\mathcal{F}}$ are the concatenation of $\bm{\mathcal{P}}^s$ and $\bm{\mathcal{P}}^t$, and $\bm{\mathcal{F}}^s$ and $\bm{\mathcal{F}}^t$, respectively.}
\vspace{-0.4cm}
\label{fig:gmm}
\end{figure*}
%--------------------------------------------------------------------

\section{Related Work}
% We review correspondence-based registration, including point-level and distribution-level methods, since our work follows the line of correspondence-based methods. As unsupervised learning is a major component in our proposed learning framework, we also review work on this topic.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Point-Level Methods.}
Point-level registration approaches first extract point-wise features, then establish point-to-point correspondences through feature matching, followed by outlier rejection and robust estimation of the rigid transformation.
Numerous works, such as FCGF~\cite{choy2019fully} and RGM~\cite{fu2021robust}, focus on extracting discriminative features for geometric correspondences. 
For the correspondence prediction,
DCP~\cite{wang2019deep}, RPMNet \cite{yew2020rpm}, and REGTR~\cite{yew2022regtr}  perform feature matching by integrating the Sinkhorn algorithm or Transformer~\cite{vaswani2017attention} into a network to generate soft correspondences from local features. IDAM \cite{li2019iterative} incorporates both geometric and distance features into the iterative matching process. 
To reject outliers, DGR~\cite{choy2020deep} and 3DRegNet~\cite{pais20203dregnet} use networks to estimate the inliers.  Predator~\cite{huang2021predator} and PRNet~\cite{wang2019prnet} focus on detecting points in the overlap region and utilizing their features to generate matches.  Keypoint-free methods~\cite{mei2021point,zhang2022patchformer,yu2021cofinet} first downsample the point clouds into super-points and then match them by examining whether their neighborhoods (patch) overlap. Though achieving remarkable performance, most of these methods rely on large amounts of ground-truth transformations, as inaccessible or expensive as such annotation may get. 
This said, the ground-truth geometric labels could potentially be obtained from full 3D reconstruction pipelines~\cite{choi2015robust},
but these require delicate parameter tuning, partial human supervision, and extra sensory information such as GPS.
As a result, the success of learning-based techniques has been limited to a handful of datasets with ground-truth annotations. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Distribution-Level Methods.}
Distribution-level methods model the point clouds as probability distributions, often via the use of GMMs, and perform alignment either by employing a correlation-based or an EM-based optimization framework.
The correlation-based methods~\cite{jian2010robust,yuan2020deepgmr} first build GMM probability distributions for both the source and target point clouds. Then, the transformation is estimated by minimizing a metric or divergence between the distributions.  However, these methods lead to nonlinear optimization problems with nonconvex constraints~\cite{lawin2018density}. Unlike correlation-based methods, the
EM-based approaches, such as JRMPC~\cite{evangelidis2017joint}, CPD~\cite{myronenko2010point}, and FilterReg \cite{gao2019filterreg}, represent the geometry of one point cloud using a GMM distribution over 3D Euclidean space. 
The transformation is then calculated by fitting another point cloud to the GMM distribution under the maximum likelihood estimation (MLE) framework.
These methods are robust to noise and density variation~\cite{yuan2020deepgmr}. Most of them utilize robust discrepancies to reduce the influence of outliers by greedily aligning the largest possible fraction of points while being tolerant to a small number of outliers. However, if outliers dominate, the greedy behavior of these methods easily emphasizes outliers, leading to degraded registration results~\cite{evangelidis2017joint}.
Considering these factors, we formulate registration in a novel partial distribution matching framework, where we only seek to partially match the distributions.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Unsupervised Point Cloud Registration.}
To handle ground-truth labeling issues, great efforts~\cite{deng2018ppf,huang2020feature,yang2021self,jiang2021sampling,shen2022reliable,wang2019prnet} have been devoted to unsupervised deep point cloud registration. The existing methods mainly lie in auto-encoders~\cite{deng2018ppf,huang2020feature,shen2022reliable} with a reconstruction loss or contrastive learning \cite{xie2020pointcontrast,el2021unsupervisedr,choy2019fully} with data augmentation.
Although encouraging results have been achieved, some limitations remain to be addressed. 
Firstly, they depend on the point-level loss, such as Chamfer distance in auto-encoder~\cite{deng2018ppf}, finding it difficult to handle large-scale scenarios due to computational complexity.
Secondly, many pipelines~\cite{wang2019prnet} apply fixed/handcrafted data augmentation to generate transformations or correspondences, leading to sub-optimal learning. 
This is because they cannot fully use the cross information of partially overlapping point clouds without geometric labels, and the shape complexity of the samples is ignored in the fixed augmentation~\cite{li2022hybridcr}.
To overcome these limitations, we provide a distribution consistency-based unsupervised method, which utilizes the distribution-level loss to reduce the computational complexity. Even without using any data augmentation, the proposed method is still suitable and available. 
