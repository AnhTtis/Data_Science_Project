\section{Experiments}\label{sec:exp}
We conduct extensive experiments to evaluate the performance of our method on the real datasets 3DMatch \cite{zeng20173dmatch} and 3DLoMatch~\cite{huang2021predator}, as well as on the synthetic datasets ModelNet~\cite{wu20153d} and ModelLoNet~\cite{huang2021predator}.



\subsection{Implementation Details} 
Our method is implemented in PyTorch and was trained on one Quadro GV100 GPU (32G) and two Intel(R) Xeon(R) Gold 6226 CPUs.  
We used the AdamW optimizer with an initial learning rate of $1e{-}4$ and a weight decay of $1e{-}6$.  
We adopted the similar encoder and decoder architectures used in~\cite{qin2022geometric}.
For the 3DMatch dataset, we trained for 200 epochs with a batch size of 1, halving the learning rate every 70 epochs.
We trained on the ModelNet for 400 epochs with a batch size of 1, halving the learning rate every 100 epochs. 
On 3DMatch and 3DLoMatch, we set $L{=}128$ with truncated patch size $K{=}64$. 
On ModelNet and ModelLoNet, we set $L{=}64$ with truncated patch size $K{=}32$. 
The cluster head MLP consists of 3 fully connected layers. Each layer is composed of a linear layer followed by batch normalization. The hidden layer and the final linear layer output dimension are 512 and clusters, respectively. Except for the final layer, each layer has a LeakyReLU activation.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
	\centering 
\begin{overpic}[width=0.95\columnwidth]{figures/indoor}
    \put(-3.7,60.0){\color{black}\footnotesize\rotatebox{90}{\textbf{Input}}}
    \put(-3.7,41.5){\color{black}\footnotesize\rotatebox{90}{\textbf{SGP}}}
    \put(-3.7,25.4){\color{black}\footnotesize\rotatebox{90}{\textbf{Ours}}}
    \put(-3.7,7.0){\color{black}\footnotesize\rotatebox{90}{\textbf{GT}}}
\end{overpic}
\caption{Example qualitative registration results for 3DMatch. The unsuccessful cases are enclosed in red boxes.}
	\label{fig:3dvs}
 \vspace{-0.4cm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Evaluation on 3DMatch and 3DLoMatch}
\noindent\textbf{Datasets and Metrics.} 3DMatch \cite{zeng20173dmatch} and 3DLoMatch \cite{huang2021predator} are two widely used indoor datasets with more than $30\%$ and $10\% {\sim} 30\%$ partially overlapping scene pairs, respectively. 3DMatch contains 62 scenes, from which we use 46 for training, 8 for validation, and 8 for testing. The test set contains 1,623 partially overlapping point cloud fragments and their corresponding transformation matrices. We used training data preprocessed by \cite{huang2021predator} and evaluated with both the 3DMatch and 3DLoMatch protocols.  Each input point cloud contains an average of about 20,000 points. We performed training data augmentation by applying small rigid perturbations, jittering the point locations, and shuffling points.
Following REGTR~\cite{yew2022regtr} and SGP~\cite{yang2021self}, we evaluated the Relative Rotation Errors (RRE) and Relative Translation Errors (RTE) that measure the accuracy of successful registrations. We also assessed Registration Recall (RR), the fraction of point cloud pairs whose transformation error is smaller than a threshold (i.e., 0.2m).



\begin{table}[!tbp]
	\centering
	\caption{Results on both 3DMatch and 3DLoMatch datasets. The best results for each criterion are labeled in bold, and the best results of unsupervised methods are underlined.}
	\resizebox{1\linewidth}{!}{%
	\begin{tabular}{r | c c c | c c c}
	\toprule
	~ &  \multicolumn{3}{c|}{3DMatch} &  \multicolumn{3}{c}{3DLoMatch} \\
	Method  & RR$\uparrow$ & RRE $\downarrow$ & RTE $\downarrow$
    & RR $\uparrow$ & RRE $\downarrow$ & RTE $\downarrow$ \\
    \midrule
    & \multicolumn{6}{c}{Supervised Methods} \\
    \midrule
	FCGF\cite{choy2019fully}  
    & 85.1\% & 1.949 & 0.066 
    & 40.1\% & 3.147 & 0.100 \\
	D3Feat\cite{bai2020d3feat} 	
    & 81.6\% & 2.161 & 0.067 
    & 37.2\% & 3.361 & 0.103 \\
    OMNet~\cite{xu2021omnet} 
    & 35.9\% & 4.166 & 0.105 
    & 8.4\%  & 7.299 & 0.151 \\
	DGR~\cite{choy2020deep} 
    & 85.3\% & 2.103 & 0.067 
    & 48.7\% & 3.954 & 0.113 \\
	Predator1K~\cite{huang2021predator} 
    & 90.5\% & 2.062 & 0.068 
    & 62.5\% & 3.159 & 0.096 \\
	CoFiNet\cite{yu2021cofinet}	  
    & 89.7\% & 2.147 & 0.067
    & 67.2\% & 3.271 & 0.090 \\
	GeoTrans~\cite{qin2022geometric} 
    & \bf92.0\% & 1.808 & 0.063 
    & \bf74.0\% & 2.934 & 0.089 \\
    REGTR~\cite{yew2022regtr} 
    & \bf92.0\% & \bf1.567 & \bf0.049
    & 64.8\% & \bf2.827 & \bf0.077 \\
    \midrule
    & \multicolumn{6}{c}{Unsupervised Methods} \\
    \midrule
    PPFFoldNet~\cite{deng2018ppf} 
    & 69.3\% & 3.021 & 0.089
    & 24.8\% & 7.527 & 1.884\\
    SGP + R10K~\cite{yang2021self}	  
    & 85.5\% & 1.986 & 0.079 
    & 39.4\% & 3.529 & 0.099\\
	\ourmethod (Ours)	
    & \underline{91.4\%} & \underline{1.642} & \underline{0.064}
    & \underline{64.3\%} & \underline{2.951} & \underline{0.086} \\			
	\bottomrule
	\end{tabular}
	}
\label{table:3dm}
	\vspace{-0.4cm}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.1cm}
\noindent\textbf{Baselines.} We chose supervised state-of-the-art (SOTA) methods: OMNet~\cite{xu2021omnet}, FCGF \cite{choy2019fully}, D3Feat \cite{bai2020d3feat}, SpinNet \cite{ao2021spinnet}, Predator \cite{huang2021predator}, REGTR~\cite{yew2022regtr}, CoFiNet \cite{yu2021cofinet}, and GeoTransformer\cite{qin2022geometric}, as well as unsupervised PPFFoldNet~\cite{deng2018ppf} and SGP~\cite{yang2021self} as our baselines.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.1cm}
\noindent\textbf{Registration Results.} 
The results of various methods are shown in Table~\ref{table:3dm}, where the best performance is highlighted in bold while the best-unsupervised results are marked with an underline. For both 3DMatch and 3DLoMatch, our method outperforms all unsupervised methods and achieves the lowest average rotation (RRE) and translation (RTE) errors across scenes. Our method also achieves the highest average registration recall, which reflects the final performance on point cloud registration (91.4\% on 3DMatch and 64.3\% on 3DLoMatch). Specifically, \ourmethod largely exceeds the previous winner and our closest competitor, SGP,  (85.5\% RR on 3DMatch) by about 5.9\% and (39.4\% RR on 3DLoMatch) by 24.9\%. Interestingly, our method also exceeds some supervised methods, {\em e.g.} OGMM, FCGF, D3Feat, DGR, and Predator1K, showing its efficacy in both high- and low-overlap scenarios. Even compared with recent supervised SOTA methods, our method achieves competitive results. Figs.~\ref{fig:3dvs} and~\ref{fig:3dvslo} show examples of qualitative results on both 3DMatch and 3DLoMatch. GT indicates ground truth. SGP failed in one case of Fig.~\ref{fig:3dvs} on 3DMatch and failed in two cases of Fig.~\ref{fig:3dvslo} on 3DLoMatch, but our method succeeded in all cases. This is because our unsupervised method can learn more discriminative features and our matching strategy can deal with partial overlap registration, which further shows the effectiveness of \ourmethod.

\begin{figure}[t]
	\centering 
\begin{overpic}[width=0.95\columnwidth]{figures/loindoor}
    \put(-3.8,55.0){\color{black}\footnotesize\rotatebox{90}{\textbf{Input}}}
    \put(-3.8,40.0){\color{black}\footnotesize\rotatebox{90}{\textbf{SGP}}}
    \put(-3.8,24.0){\color{black}\footnotesize\rotatebox{90}{\textbf{Ours}}}
    \put(-3.8,7.0){\color{black}\footnotesize\rotatebox{90}{\textbf{GT}}}
\end{overpic}
\caption{Example qualitative registration results for 3DLoMatch. The unsuccessful cases are enclosed in red boxes.} \label{fig:3dvslo}
\vspace{-0.4cm}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Evaluation on ModelNet40}
\noindent\textbf{Datasets and Metrics.} ModelNet40 \cite{wu20153d} contains 12,311 meshed CAD models from 40 categories. Following the data setup in~\cite{huang2021predator,yew2022regtr}, each point cloud is sampled from ModelNet40 with 1,024 points followed by cropping and sub-sampling into two partial overlap settings: ModelNet has 73.5\% pairwise overlap on average, and ModelLoNet contains a lower 53.6\% average overlap. We train only on ModelNet and generalize to ModelLoNet. We follow~\cite{yew2022regtr} and measure the performance using Relative Rotation Error (RRE) and Relative Translation Error (RTE) on all point clouds and as Chamfer distance (CD) between scans.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!tbp]
	\centering
	\caption{Results on both ModelNet and ModelLoNet datasets. The best results for each criterion are labeled in bold, and the best results of unsupervised methods are underlined.}
	\resizebox{1\linewidth}{!}{%
	\begin{tabular}{r | c c c | c c c}
	\toprule
	~ &  \multicolumn{3}{c|}{ModelNet} &  \multicolumn{3}{c}{ModelLoNet} \\
	Method  & RRE $\downarrow$ & RTE $\downarrow$ & CD $\downarrow$
    & RRE $\downarrow$ & RTE $\downarrow$  & CD $\downarrow$ \\
    \midrule
    & \multicolumn{6}{c}{Traditional Methods} \\
    \midrule
    ICP~\cite{besl1992method} 
    & 13.74 & 0.132 & 0.1225
    & 24.13 & 0.224 & 0.1289\\
    FGR~\cite{zhou2016fast}
    & 28.68 & 0.160 & 0.1290
    & 34.39 & 0.244 & 0.1339\\
    CPD~\cite{myronenko2010point} 
    & 14.17 & 0.139 & 0.1277
    & 28.78 & 0.253 & 0.1320\\
    GMMReg~\cite{jian2010robust} 
    & 16.41 & 0.163 & 0.1304
    & 24.03 & 0.243 & 0.1298\\
    SVR~\cite{campbell2015adaptive}  
    & 14.40 & 0.140 & 0.1279
    & 23.45 & 0.222 & 0.1322\\
    FilterReg~\cite{gao2019filterreg} 
    & 24.07 & 0.193 & 0.1336
    & 37.28 & 0.298 & 0.1367\\
	\midrule
    & \multicolumn{6}{c}{Supervised Methods} \\
    \midrule
    DCP-v2~\cite{wang2019deep} 
    & 11.98 & 0.171 & 0.0117 
    & 16.50 & 0.300 & 0.0268 \\
    DeepGMR \cite{yuan2020deepgmr}
    & 7.871 & 0.108 & 0.0056
    & 9.867 & 0.117 & 0.0064\\
	OMNet~\cite{xu2021omnet} 
    & 2.947 & 0.032 & 0.0015 
    & 6.517 & 0.129 & 0.0074 \\
	RPM-Net~\cite{yew2020rpm} 
    & 1.712 & 0.018 & 0.0009
    & 7.342 & 0.124 & 0.0050 \\
	Predator~\cite{huang2021predator} 
    & 1.739 & 0.019 & 0.0009
    & 5.235 & 0.132 & 0.0083 \\
	GeoTrans~\cite{qin2022geometric} 
    & 2.145 & 0.020 & \bf0.0003
    & 4.741 & 0.103 & 0.0143 \\
    REGTR~\cite{yew2022regtr} 
    & 1.473 & 0.014 & 0.0008 
    & 3.930 & 0.087 & \bf0.0037 \\
    \midrule
    & \multicolumn{6}{c}{Unsupervised Methods} \\
    \midrule
    CEMNet~\cite{jiang2021sampling}
    & 2.575 & 0.019 & 0.0368
    & 9.417 & 0.151 & 0.0861\\
    RIENet \cite{shen2022reliable}
    & 2.447 & 0.018 & 0.0365
    & 14.49 & 0.105 & 0.0828\\
    UGMM~\cite{huang2022unsupervised}
    & 13.65 & 0.124 & 0.0753
    & 17.39 & 0.161 & 0.0745 \\
	\ourmethod (Ours)	
    & \bf\underline{1.331} & \bf\underline{0.011} & \underline{0.0306}
    & \bf\underline{3.578} & \bf\underline{0.069} & \underline{0.0416} \\			
	\bottomrule
	\end{tabular}
	}
\label{table:mnet}
\vspace{-0.4cm}
\end{table}


\begin{table}[!t]
	\centering
	\caption{The results of different combinations of loss functions in both ModelNet and ModelLoNet datasets. The best results for each criterion are labeled in bold.}
	\resizebox{0.9\linewidth}{!}{%
	\begin{tabular}{r | c c c | c c c}
	\toprule
	~ &  \multicolumn{3}{c|}{ModelNet} &  \multicolumn{3}{c}{ModelLoNet} \\
	Method  & RRE $\downarrow$ & RTE $\downarrow$ & CD $\downarrow$
    & RRE $\downarrow$ & RTE $\downarrow$  & CD $\downarrow$ \\
	\midrule   
    CC
    & 6.985 & 0.087 & 0.0357 
    & 8.176 & 0.084 & 0.0483\\
    SC
    & 5.898 & 0.045 & 0.0314
    & 8.104 & 0.081 & 0.0470\\
    LC
    & 7.871 & 0.046 & 0.0393
    & 8.790 & 0.091 & 0.0482\\
    SC + LC
    & 3.742 & 0.062 & 0.0324
    & 5.835 & 0.084 & 0.0334\\    
    CC + LC
    & 3.867 & 0.059 & 0.0314
    & 5.256 & \bf0.061 & 0.0422\\ 
    CC + SC  
    & 3.421 & 0.048 & 0.0360
    & 5.229 & 0.064 & 0.0423 \\
    CC + SC + LC 
    & \bf 1.331 & \bf 0.011 & \bf 0.0306
    & \bf 3.578 & 0.069 & \bf 0.0416 \\	 		
	\bottomrule
	\end{tabular}}
\label{tb:loss}
\vspace{-0.4cm}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Baselines.} We chose recent supervised SOTA methods: DCP-v2~\cite{wang2019deep}, OMNet~\cite{xu2021omnet}, RPM-Net~\cite{yew2020rpm}, Predator \cite{huang2021predator}, REGTR~\cite{yew2022regtr}, CoFiNet \cite{yu2021cofinet}, and GeoTransformer\cite{qin2022geometric}, as well as unsupervised method RIENet~\cite{shen2022reliable} and UGMM~\cite{huang2022unsupervised} as our baselines.
For traditional methods, we choose point-level methods ICP~\cite{besl1992method} and FGR~\cite{zhou2016fast}, as well as probabilistic methods CPD~\cite{myronenko2010point}, GMMReg~\cite{jian2010robust}, SVR~\cite{campbell2015adaptive}, and FilterReg~\cite{gao2019filterreg} as baselines.
For Predator, RPM-Net, OMNet, and REGTR, we use the results provided in REGTR. In REGTR, Predator samples 450 points in the experiment, and OMNet obtained a slightly improved result in all categories.
We utilize the codes provided by the authors for probabilistic methods. To improve the results for partial registration, we replace PointNet with DGCNN in DeepGMR. Additionally, we use Open3D for ICP and FGR.

%++++++++++++++++++++++++++++++++++++++
\vspace{0.1cm}
\noindent\textbf{Registration Results.} 
Table~\ref{table:mnet} reports registration results on ModelNet40, in which the best results for each criterion are labeled in bold, and the best results by unsupervised methods are underlined.
We compare against the recent unsupervised~\cite{shen2022reliable} and supervised~\cite{wang2019deep,xu2021omnet,huang2021predator,yew2022regtr,yu2021cofinet,qin2022geometric} methods. When compared with unsupervised methods, our \ourmethod outperforms the correspondence-based CEMNet, RIENet and GMM-based UGMM~\cite{huang2022unsupervised} in all metrics under both normal overlap (ModelNet) and low overlap (ModelLoNet) regimes. 
Compared with supervised methods, our approach also achieves competitive results. 
Specifically, our \ourmethod outperforms all previous methods regarding rotation and translation criteria. It is worth noting that RPM-Net~\cite{yew2020rpm} additionally uses surface normals and is trained with transformation information. Despite this, the \ourmethod still performs better. In addition to the quantitative results, Fig.~\ref{fig:mndet} shows results on ModelNet with more than 70.0\% partial overlap. We also offer registration results for ModelLoNet with more than 50.0\% partial overlap in Fig.~\ref{fig:mnlonet}. 
Compared with the recent SOTA unsupervised method RIENet, our \ourmethod recovers the transformation more accurately on the challenging dataset ModelLoNet.


%+++++++++++++++++++++++++++++++++++
\begin{figure}[!t]
	\centering
    \begin{overpic}[width=0.95\columnwidth]{figures/mn70.pdf}
    % \put(30, 32){\color{black}\footnotesize\textbf{successful}}
    \put(-3.5,85.0){\color{black}\footnotesize\rotatebox{90}{\textbf{Input}}}
    \put(-3.5,69.0){\color{black}\footnotesize\rotatebox{90}{\textbf{ICP}}}
    \put(-3.5,48.0){\color{black}\footnotesize\rotatebox{90}{\textbf{RIENet}}}
    \put(-3.5,25.4){\color{black}\footnotesize\rotatebox{90}{\textbf{REGTR}}}
    \put(-3.5,10.0){\color{black}\footnotesize\rotatebox{90}{\textbf{Ours}}}
    % \vspace{-0.2cm}
    \end{overpic}
	\caption{Registration results of different methods on ModelNet with more than 70\% partial overlaps.}
	\label{fig:mndet}
 \vspace{-0.3cm}
\end{figure}
%+++++++++++++++++++++++++++++++++++


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%+++++++++++++++++++++++++++++++++++
\begin{figure}[!t]
	\centering
    \begin{overpic}[width=0.95\columnwidth]{figures/mn50.pdf}
    \put(-3.5,69.0){\color{black}\footnotesize\rotatebox{90}{\textbf{Input}}}
    \put(-3.5,48.0){\color{black}\footnotesize\rotatebox{90}{\textbf{ICP}}}
    \put(-3.5,25.4){\color{black}\footnotesize\rotatebox{90}{\textbf{RIENet}}}
    % \put(-3.5,25.4){\color{black}\footnotesize\rotatebox{90}{\textbf{REGTR}}}
    \put(-3.5,8.0){\color{black}\footnotesize\rotatebox{90}{\textbf{Ours}}}
    \end{overpic}
	\caption{Registration results of different methods on ModelLoNet with more than 50\% partial overlaps.}
	\label{fig:mnlonet}
 \vspace{-0.3cm}
\end{figure}
%+++++++++++++++++++++++++++++++++++

\vspace{0.1cm}
\noindent\textbf{Loss Functions.} We trained our model with different combinations of the local contrastive loss (LC), cross consistency loss (CC), and self-consistency loss (SC), where the experiments were conducted on both ModelNet and ModelLoNet. Table~\ref{tb:loss} shows that the cross-consistency, self-consistency, and local contrastive losses can boost registration precision. Specifically, for a single loss, self-consistency loss archives the best results, and local contrastive loss performs worse on all metrics on both datasets.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.1cm}
\noindent\textbf{Influence of the Number of Clusters.}
We assess the effect of the number of clusters $L$ for ModelNet and ModelLoNet.
We trained \ourmethod with different values of $L$, from 4 to 160, and report the results in Table~\ref{tab:ablation_j}.
\ourmethod achieves the best results with $L{=}64$ on both benchmarks. 
The results are stable for $16 {\leq} L {\leq} 96$. This suggests that the number of clusters has little influence as long as there are ``enough". 

%++++++++++++++++++++++++++++++++++++++
\begin{table}[t]
	\centering
	\caption{Ablation study results of \ourmethod on ModelNet40 with different number of clusters $L$. The best results for each criterion are labeled in bold.}
	\label{tab:ablation_j}
 	\resizebox{0.9\linewidth}{!}{%
	\begin{tabular}{r | c c c | c c c}
	\toprule
	~ &  \multicolumn{3}{c|}{ModelNet} &  \multicolumn{3}{c}{ModelLoNet} \\
	Clusters  & RRE $\downarrow$ & RTE $\downarrow$ & CD $\downarrow$
    & RRE $\downarrow$ & RTE $\downarrow$  & CD $\downarrow$ \\
	\midrule
	4 
    & 1.504 & 0.009 & 0.0366 
    & 4.348 & 0.068 & 0.0452\\ 
    16  
    & 1.439 & 0.007 & 0.0334 
    & 3.713 & 0.057 & 0.0424 \\
    32
    & \bf1.305 & 0.014 & 0.0339 
    & 3.659 & 0.058 & 0.0419\\
    64
    & 1.331 & \bf0.011 & \bf0.0306
    & \bf3.578 & \bf0.069 & \bf0.0416 \\
    96
    & 1.454 & 0.021 & 0.0367
    & 3.598 & 0.070 & 0.0428 \\
    128
    & 1.468 & 0.009 & 0.0310
    & 4.440 & 0.057 & 0.0399\\  
    160
    & 1.530 & 0.009 & 0.0338
    & 4.564 & 0.059 & 0.0422\\
		\bottomrule
	\end{tabular}}
\label{tb:clus}
% \vspace{-0.3cm}
\end{table}
%++++++++++++++++++++++++++++++++++++++

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.1cm}
\noindent\textbf{Importance of Individual Modules.} 
In the registration process, \ourmethod extracts hierarchical correspondences from clusters to points. Therefore, we further explore the efficiency of the hierarchical registration strategy. Table~\ref{tb:cfreg} reports the results on ModelNet and ModelLoNet, where \textit{Cluster}, \textit{Point}, and \textit{Cluster-point} indicate distribution-level, point-level, and distribution-based point-level correspondences, respectively. In the first experiment, we only used distribution-level correspondences for point cloud registration. Unsurprisingly, it performs worse on all metrics, indicating \ourmethod benefits from point-level matching. In the second experiment, we directly predict the point-level correspondences to estimate transformation by performing feature matching. Its performance is still worse than that of the hierarchical registration strategy, further showing the effectiveness of our correspondence prediction strategy.

%++++++++++++++++++++++++++++++++++++++
\begin{table}[t]
	\centering
	\caption{Ablation study of individual modules on ModelNet and ModelLoNet. The best performance is highlighted in bold.}
	\label{tab:ab_clu}
 	\resizebox{0.95\linewidth}{!}{%
	\begin{tabular}{r | c c c | c c c}
	\toprule
	~ &  \multicolumn{3}{c|}{ModelNet} &  \multicolumn{3}{c}{ModelLoNet} \\
	Method  & RRE $\downarrow$ & RTE $\downarrow$ & CD $\downarrow$
    & RRE $\downarrow$ & RTE $\downarrow$  & CD $\downarrow$ \\
	\midrule
	Cluster 
    & 3.932 & 0.033 & 0.0330
    & 6.018 & 0.182 & 0.0463 \\	
    Point  
    & 2.505 & 0.014 & 0.0311
    & 4.264 & 0.096 & 0.0431\\
    Cluster-Point
    & \bf 1.331 & \bf 0.011 & \bf 0.0306
    & \bf 3.578 & \bf 0.069 & \bf 0.0416 \\	    
	\bottomrule
	\end{tabular}}
\label{tb:cfreg}
\vspace{-0.3cm}
\end{table}
%++++++++++++++++++++++++++++++++++++++

