\section{Introduction}\label{sec:intro}
Rigid point cloud registration refers to the problem of finding the optimal transformation parameters to align two partially overlapping point clouds into one coherent coordinate system~\cite{huang2020feature,mei2022overlap,mei2021point,mei2022partial}. It is a fundamental computer vision task that has applications in many areas, such as robotics~\cite{Zhou2022}, augmenting reality~\cite{borrmann2018large}, autonomous driving~\cite{chen20203d,nagy2018real,wang2019robust}, and radiotherapy~\cite{li2019noninvasive,ma2018point}, to name a few. 
% However, registering the 3D point clouds of real-world scenes is still a challenging task due to sensor noise, varying point densities, outliers, occlusions, and partial views~\cite{li2022wsdesc}. Besides, the recent state-of-the-art learning-based methods require ground-truth transformation or correspondences, which are usually obtained by manual annotation or in real scenes.
Recently, point cloud registration has gained wide interest recently owing to the development of 3D point representation learning and differentiable optimization~\cite{qin2022geometric}. 

Existing registration methods can be broadly categorized into \textit{correspondence-free} and \textit{correspondence-based} \cite{mei2021point,mei2022overlap}.
The former approaches~\cite{aoki2019pointnetlk,huang2020feature,mei2021point,mei2022partial} are formulated by minimizing the difference between global features extracted from two input point clouds.
These global features are typically computed based on all the points of a point cloud, making correspondence-free approaches inadequate to handle the real scenes with partial overlaps~\cite{zhang2020deep,choy2020deep}. 
The correspondence-based methods~\cite{choy2020deep,bai2021pointdsc,huang2021predator,yew2022regtr} firstly extract local features, followed by establishing the point-level~\cite{choy2020deep,huang2020feature,huang2021predator,fu2021robust} or distribution-level~\cite{magnusson2007scan,magnusson2009evaluation,stoyanov2012point,evangelidis2017joint,yuan2020deepgmr} correspondences using the local features, and finally, estimate transformation from the correspondences.
However, the point-level registration method does not work well in practical applications when conditions involve varying point densities and repetitive patterns. This issue is especially prominent in indoor environments, where low-texture regions or repetitive patterns sometimes occupy most areas in the field of view.
Distribution-level methods align two point clouds without establishing explicit point correspondence. Unfortunately, they are inflexible and cannot handle point clouds with partial overlapping in real scenes~\cite{mei2022overlap,li2022gaussian}. 
Moreover, most of the success of the correspondence-based methods mainly depends on large amounts of ground truth transformations or correspondences as the supervision signal for model training. 
However, collecting required ground truth transformations is either inaccessible sometimes or expensive in human annotation, which may significantly increase the training cost and hinder their applications in the real world~\cite{shen2022reliable}. 
Although the ground-truth geometric labels can potentially be obtained from full 3D reconstruction pipelines~\cite{choi2015robust} that require delicate parameter tuning, partial human supervision, and extra sensory information such as GPS. Data augmentation is also applied to generate labels, but it still underutilizes the information of partially overlapped point clouds without providing any ground-truth geometric labels.
As a result, the success of learning-based techniques is limited to a handful of datasets with ground-truth annotations.
To handle the issues that arise in ground-truth labeling, much more effort~\cite{deng2018ppf,huang2020feature,yang2021self,jiang2021sampling,shen2022reliable,wang2019prnet} has been devoted to the deep point cloud registration field without supervision, typically by taking auto-encoders~\cite{deng2018ppf,huang2020feature} with a reconstruction loss.
Although the existing methods have achieved encouraging results, some limitations remain to be addressed. 
Firstly, they depend on the point-level loss, such as Chamfer distance, raising difficulty in handling large-scale scenarios due to the computing complexity.
Secondly, they do not adequately consider the effects of sensor noise, varying point densities, outliers, occlusions, and partial views.
Besides, many pipelines~\cite{wang2019prnet} apply fixed/handcrafted data augmentation to generate transformation or correspondences leading to sub-optimal learning. 
This is because they cannot fully use the cross information of partially overlapping point clouds without geometric labels, and the shape complexity of the samples is ignored in the fixed augmentation~\cite{li2022hybridcr}.

To alleviate the aforementioned limitations, this paper proposes an unsupervised deep probabilistic registration framework without any ground-truth geometric labels. 
Specifically, we extend the distribution-to-distribution (D2D) method to solve partial point cloud registration by adopting the Sinkhorn algorithm to predict distribution-level correspondences. The correspondences are estimated under the guidance of mixing coefficients, which measure the ratio of each cluster containing the number of points of each point cloud. This means that the clusters in the source point cloud with low ratios tend to align the clusters in the target point with low probability. Therefore, it can deal with point cloud registration with partial overlapping.
In order to make the network extract geometric consistent and semantic consistent features, we design distribution-consistent losses, i.e., self-consistent and cross-consistent losses, to train the networks without using any ground-truth pose and correspondences. The self-consistent loss encourages the extracted features to be geometric consistent by forcing the features and coordinates to share the posterior probability. The cross-consistent loss prompts the extracted features to be geometric consistent by forcing the partially overlapped point clouds to share the same clusters.  Besides, we introduce a local contrastive loss to learn more discriminative cluster centroids. by pulling features of points belonging to the same clusters together while pushing dissimilar features of points coming from different clusters. 
Our \ourmethod is inspired by DeepGMR~\cite{yuan2020deepgmr}, but it differs from DeepGMR in two ways. 
First, DeepGMR assumes that there exists some probability distribution for which the point cloud is treated as a set of independent and identically (iid) distributed samples.
This assumption directly leads to various failure cases when handling registration with partial overlap.
Our probabilistic paradigm can handle partial-to-partial point cloud registration problems through the cluster ratio constraint.
Second, our network learns a consistent GMM representation, in an unsupervised manner, across feature and geometric spaces rather than fitting a GMM with supervision in a single feature space.
We evaluate our approach on 3DMatch~\cite{zeng20173dmatch} and ModelNet40 \cite{wu20153d}, comparing our approach against traditional and deep learning-based point cloud registration approaches.
We use the typical evaluation protocol for point cloud registration used in this literature~\cite{yew2022regtr}.
\ourmethod achieves state-of-the-art results and significantly outperforms DeepGMR on all the benchmarks.

% \vspace{.1cm}
% \noindent 
In summary, the main contributions of this work are:
\begin{itemize}[leftmargin=*]
    \item We propose an unsupervised learning-based probabilistic framework to deal with point cloud registration with partial overlapping.
    \item We extend the learning-based GMM registration method to handle point cloud registration with partial overlapping.
    \item We formulate self-consistent, cross-consistent, and local-contrastive losses, to make the posterior probability in both coordinate and feature spaces consistent so that the feature extractor can be trained in an unsupervised way.
    \item We achieve state-of-the-art accuracy and efficiency on a comprehensive set of experiments, including synthetic and real-world datasets.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.3cm}
\subsubsection{Encoder}
\vspace{-0.2cm}
Following~\cite{huang2021predator}, a shared KPConv-FPN~\cite{thomas2019kpconv}, which consists of a series of ResNet-like blocks and stridden convolutions, simultaneously down-samples the raw point clouds $\bm{\mathcal{P}}^s$ and $\bm{\mathcal{P}}^t$ into superpoints $\bar{\bm{\mathcal{P}}}^s$ and $\bar{\bm{\mathcal{P}}}^t$ and extracts associated features $\bar{\bm{\mathcal{F}}}^s{=}\{\bar{\bm{f}}^s_i{\in}\mathbb{R}^{b}|i{=}1, 2, ..., \bar{N}_s\}$ and $\bar{\bm{\mathcal{F}}}^t{=}\{\bar{\bm{f}}^t_j{\in}\mathbb{R}^{b}|j{=}1, 2, ..., \bar{N}_t\}$, respectively. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.3cm}
\subsubsection{Transformer}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.2cm}
The transformer consists of positional encoding, self-attention, and cross-attention. The positional encoding assigns intrinsic geometric properties to a per-point feature, thus enhancing distinctions among point features in indistinctive regions. Self-attention is thus introduced to model the long-range dependencies. And cross attention module exploits the intra-relationship within the source and target point clouds, which encodes contextual information between two partially overlapped point clouds. Here, we detail the individual parts hereafter.

\noindent\textbf{Positional Encoding.} 
The positional encoding scheme, which assigns intrinsic geometric properties to the per-point feature by adding unique positional information, enhances distinctions among point features in indistinctive regions. Given two superpoints $\bar{\bm{p}}^s_i$ and $\bar{\bm{p}}^s_j$ of $\bar{\bm{\mathcal{P}}}^s$, we first select the $k=5$ nearest neighbors $\mathcal{K}_i$ of $\bar{\bm{p}}_i$ and compute the centroid $\bar{\bm{p}}^s_c=\sum_{i=1}^{\bar{N}_s}\bar{\bm{p}}^s_i$ of $\bar{\bm{\mathcal{P}}}^s$. For each $\bar{\bm{p}}^s_x\in \mathcal{K}_i$, we denote the angle between two vectors $\bar{\bm{p}}^s_i-\bar{\bm{p}}^s_c $ and $ \bar{\bm{p}}^s_x-\bar{\bm{p}}^s_c$ is $\alpha_{ix}$.
The position encoding $\bar{\bm{g}}^s_i$ of $\bar{\bm{p}}_i$ is defined as follows: 
\begin{equation}\label{eq:pos}
	\bar{\bm{g}}^s_i= \varphi\left(\|\bar{\bm{p}}^s_i-\bar{\bm{p}}^s_c\|_2\right) +\max_{x\in \mathcal{K}_i}\{\phi\left(\alpha_{ix}\right)\},
\end{equation}
where $\varphi$ and $\phi$ are two MLPs, and each MLP consists of a linear layer and one ReLU nonlinearity function.

\noindent\textbf{Self-Attention and Cross-Attention.} Let ${^{(l)}\bar{\bm{\mathcal{F}}}^s}$ be the intermediate representation for $\bar{\bm{\mathcal{P}}}$ at layer $l$ and let ${^{(0)}\bar{\bm{\mathcal{F}}}^x}{=}\{\bar{\bm{g}}^s_i{+}\bar{\bm{f}}^s_i\}_{i=1}^{\bar{N}_s}$. We use a multi-attention layer consisting of four attention heads to update the ${^{(l)}\bar{\bm{\mathcal{F}}}^s}$ via
\begin{equation}\label{eq:sim}
	\begin{aligned}
			&\bm{S}^s{=} {^{\left(l\right)}\bm{W}}_1{^{(l)}\bar{\bm{\mathcal{F}}}^s} {+} {^{(l)}\bm{b}}_1, \bm{K}^{x} {=} {^{\left(l\right)}\bm{W}}_2 {^{(l)}\bar{\bm{\mathcal{F}}}^x} {+} {^{(l)}\bm{b}}_2, \\
			&\bm{V}^{x}{=} {^{\left(l\right)}\bm{W}}_3{^{(l)}\bar{\bm{\mathcal{F}}}^x} {+} {^{(l)}\bm{b}}_3, \bm{A} {=} \mbox{softmax}\left(\frac{{\bm{S}^s}^\top \bm{K}^{x}}{\sqrt{b}}\right), \\
			&{^{(l+1)}\bar{\bm{\mathcal{F}}}^s} {=}{^{(l)}\bar{\bm{\mathcal{F}}}^s} {+} {^{(l)}h}\left(\bm{A}\bm{V}^x\right).
		\end{aligned}
\end{equation}
Here, if $x{=}s$ represents self-attention, and if $x{=}t$ indicates cross-attention. $^{(l)}h\left(\cdot\right)$ is a three-layer fully connected network consisting of a linear layer, instance normalization, and a LeakyReLU activation. The same attention module is also simultaneously performed for all points in point cloud $\bar{\bm{\mathcal{P}}}^t$. A fixed number of layers $L{=}2$ with different parameters are chained and, alternatively, aggregate along the self- and cross-attention. As such, start from $l {=} 0, x{=}s$ if $l$ is even and $x=t$ if $l$ is odd. The final outputs of attention module are $\bar{\bm{\mathcal{F}}}^s{=}{^{(3)}\bar{\bm{\mathcal{F}}}^s}$ for $\bar{\bm{\mathcal{P}}}^s$ and $\bar{\bm{\mathcal{F}}}^t{=}{^{(3)}\bar{\bm{\mathcal{F}}}^t}$ for $\bar{\bm{\mathcal{P}}}^t$. The latent features $\bar{\bm{\mathcal{F}}}^s$ have the knowledge of $\bar{\bm{\mathcal{F}}}^t$ and vice versa. 

\vspace{-0.3cm}
\subsubsection{Decoder} 
\vspace{-0.2cm}
Our decoder starts with conditioned features $\bm{\mathcal{F}}_{\bar{p}}$, and outputs the per-point feature descriptor $\bm{\mathcal{F}}^{s}\in\mathbb{R}^{N\times d}$. The decoder combines NN-upsampling with linear layers and includes skip connections from the corresponding encoder layers. The same operator is applied to get $\bm{\mathcal{F}}^t\in\mathbb{R}^{M\times d}$.