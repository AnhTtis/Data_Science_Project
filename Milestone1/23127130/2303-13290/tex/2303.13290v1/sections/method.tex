\section{Method}
% This section details our distribution consistency-based unsupervised probabilistic registration framework. 
% \vspace{-0.2cm}
\subsection{Problem Formulation}
Point cloud registration aims to seek a transformation $T{\in} SE(3)$ that optimally aligns the source point cloud $\bm{\mathcal{P}}^s{=}\{\bm{p}^s_i {\in}\mathbb{R}^{3}\big|i {=} 1, 2, ..., N_s\}$ to the target point cloud $\bm{\mathcal{P}}^t{=}\{\bm{p}^t_j {\in}\mathbb{R}^{3}\big|j {=} 1, 2, ..., N_t\}$.  $\bm{\mathcal{P}}^s$ and $\bm{\mathcal{P}}^t$ contain $N_s$ and $N_t$ points, respectively. $T$ consists of rotation $R{\in} SO(3)$ and translation $\bm{t}{\in} \mathbb{R}^3$.  
Instead of directly employing the point-level solution, we apply the distribution-to-distribution (D2D) approach to fit these two point clouds and obtain individual potential GMMs, where each component represents the density of the spatial coordinates and features in a local region. The transformation is then recovered from the learned GMMs. Our goal is to learn GMMs of point clouds for registration without any ground-truth geometric labels. 
Our \ourmethod framework is conceptually simple and is illustrated in Fig.~\ref{fig:gmm}.
The shared weighted feature extractor consisting of an encoder, Transformer (self- and cross-attention), and decoder first extracts point-wise features $\bm{\mathcal{F}}^s$ and $\bm{\mathcal{F}}^t$, overlap scores $\bm{O}^s$ and $\bm{O}^t$ from point clouds $\bm{\mathcal{P}}^s$ and $\bm{\mathcal{P}}^t$, respectively. 
$\bm{\mathcal{F}}^s$ and $\bm{\mathcal{F}}^t$ are then fed to cluster head to estimate the distributions (GMMs) of $\bm{\mathcal{P}}^s$ and $\bm{\mathcal{P}}^t$ in both coordinate and feature spaces. 
After that, the correspondences $\mathcal{M}$ are estimated by performing cluster-level and point-level matching based on the Sinkhorn algorithm \cite{cuturi2013sinkhorn}.
Finally, a variant of RANSAC \cite{fischler1981random} specialized to 3D registration is adopted to calculate $T$ based on the estimated correspondences.
The network is trained using the proposed self-consistency, cross-consistency, and local contrastive losses in an unsupervised manner.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The Proposed GMM-Based Registration}\label{subs:point}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Feature Extraction.}
Following~\cite{huang2021predator,qin2022geometric,mei2022overlap}, a shared encoder KPConv-FPN~\cite{thomas2019kpconv}, which is composed of a series of ResNet-like blocks and stridden convolutions, simultaneously downsamples the raw point clouds $\bm{\mathcal{P}}^s$ and $\bm{\mathcal{P}}^t$ into superpoints $\bar{\bm{\mathcal{P}}}^s$ and $\bar{\bm{\mathcal{P}}}^t$ and extracts associated features $\bar{\bm{\mathcal{F}}}^s{=}\{\bar{\bm{f}}^s_i{\in}\mathbb{R}^{b}|i{=}1, 2, ..., \bar{N}_s\}$ and $\bar{\bm{\mathcal{F}}}^t{=}\{\bar{\bm{f}}^t_j{\in}\mathbb{R}^{b}|j{=}1, 2, ..., \bar{N}_t\}$, respectively. $b$ is dimension. 
Then, self- and cross-attention are applied to encode contextual information of two point clouds with partial overlaps, which outputs conditioned features $\bar{\bm{\mathcal{F}}}^s$ and $\bar{\bm{\mathcal{F}}}^t$.
Finally, the shared decoder starts with conditioned features $\bar{\bm{\mathcal{F}}}^s$ and $\bar{\bm{\mathcal{F}}}^t$, and outputs the point-wise feature descriptor $\bm{\mathcal{F}}^{s}{\in}\mathbb{R}^{N_s\times d}$ and $\bm{\mathcal{F}}^t{\in}\mathbb{R}^{N_t\times d}$ and overlap scores $\bm{O}^s{=}\{o_i^s\}{\in}\mathbb{R}_+^{N_s}$ and $\bm{O}^t{=}\{o_j^t\}{\in}\mathbb{R}_+^{N_t}$. 
$d$ is the dimension of features. The decoder combines NN-upsampling with linear layers and includes skip connections from the corresponding encoder layers.
For more details on feature extraction,  please refer to the supplementary material.
%+++++++++++++++++++++++++++++++++++++++++++++

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.1cm}
\noindent\textbf{Learning Posterior.}
Different from the previous works~\cite{yuan2020deepgmr,eckart2018hgmr} only considering the spatial coordinates of the points in the probabilistic registration model, we propose a method to learn the joint distribution over the spatial coordinate and feature spaces.
Specifically, we apply a multi-layer perceptron (MLP), i.e., cluster head $\psi$,  that takes as input $\bm{\mathcal{F}}^s$ and $\bm{\mathcal{F}}^t$ and outputs joint log probabilities and a Softmax operator that acts on log probabilities to generate probability matrices $\bm{S}^s{=}\{s^s_{ij}\}_{i,j=1}^{N^s,L{-}1}$ and $\bm{S}^t{=}\{s^t_{ij}\}_{i,j=1}^{N^t,L{-}1}$, respectively. 
To deal with outliers, it is straightforward to add a Gaussian kernel density. We define $\hat{\bm{S}}^x{=}\{s^x_{ij}\}_{i,j=1}^{N^x,L}~(x{\in}\{s,t\})$ with elements satisfying $\hat{s}^x_{iL}{=}1.0{-}o^x_{i}$ and $\hat{s}^x_{iL}{=}o^x_{i}s^x_{ij}, 1\leq j< L$.
\ourmethod assumes that coordinate and feature spaces share the same probability matrix (posterior distribution).
The GMM parameters $\bm{\Theta}^x$ for point cloud $\bm{\mathcal{P}}^x$, in 3D coordinate space, consists of $L$ triples $(\pi^x_{j},\bm{\mu}^x_{j},\bm{\Sigma}^x_{j})$, where $\pi^x_{j}$ is the mixing weight of component $j$ satisfying $\sum^L_{j=1}\pi^x_{j}=1$, $\bm{\mu}^x_{j}$ is a $3{\times} 1$ mean vector and $\bm{\Sigma}^x_{j}$ is a $3{\times} 3$ covariance matrix of the $j$-th component. 
Given the outputs $\bm{S}^x$ of $\psi$ together with the point coordinates $\bm{\mathcal{P}}^x$, the GMMs are calculated as:
%+++++++++++++++++++++++++++++++++++++++++++++
\vspace{-0.2cm}
\begin{equation}\label{eq:gmm_3d}
\small
    \begin{aligned}
    &\pi^x_{j} {=} \frac{1}{N_x}\sum_{i=1}\hat{s}^x_{ij}, \bm{\mu}^x_{j} {=} \frac{1}{N_x\pi^x_{j}}\sum_{i=1}\hat{s}^x_{ij}\bm{p}^x_i, \\
    &\bm{\Sigma}^x_{j} {=} \sum_{i=1}\hat{s}^x_{ij}\left(\bm{p}^x_i {-} \bm{\mu}^x_{j}\right)\left(\bm{p}^x_i{-}\bm{\mu}^x_{j}\right)^\top, \\
    &G^{x}\left(\bm{x}\right) {=} \sum _{j=1}\pi^x_{j}\mathcal{N} \left(\bm{x}|\bm{\mu}^x_{j}, \bm{\Sigma}^x_{j}\right),x\in\{s, t\}.
    \end{aligned}
\vspace{-0.2cm}
\end{equation}
%+++++++++++++++++++++++++++++++++++++++++++++
Similar in the coordinate space, based on probability matrices $\bm{S}^s$ and $\bm{S}^t$, the GMM parameters of point clouds $\bm{\mathcal{P}}^s$ and $\bm{\mathcal{P}}^t$ in feature space are also computed as:
%+++++++++++++++++++++++++++++++++++++++++++++
\vspace{-0.2cm}
\begin{equation*}
\small
\bm{\mu}^{f_x}_j{=}\sum_{i=1}^{N_x}\frac{\hat{s}^x_{ij}\bm{f}^x_i}{N_x\pi^x_{j}}, \bm{\Sigma}^{f_x}_j {=} \sum_{i=1}^{N_x}\hat{s}^x_{ij}\left(\bm{f}^x_{i}{-}\bm{\mu}^{f_x}_j\right)\left(\bm{f}^x_{i}{-}\bm{\mu}^{f_x}_j\right)^\top,
\vspace{-0.2cm}
\end{equation*}
%+++++++++++++++++++++++++++++++++++++++++++++
where subscript $x{\in}\{s, t\}$. Note that the GMMs in coordinate and feature spaces share mixing coefficients. For simplify, we denote $\Phi_k^{f_x}(\bm{x}){=}\mathcal{N} \left(\bm{x}|\bm{\mu}^{f_x}_{k}, \bm{\Sigma}^{f_x}_{k}\right)$ with $k{\in}\{1,\cdots, L\}$. The GMMs of point clouds $\bm{\mathcal{P}}^s$ and $\bm{\mathcal{P}}^t$ in feature space are then given as:
%+++++++++++++++++++++++++++++++++++++++++++++
\vspace{-0.2cm}
\begin{equation} \label{eq:postf}
\small
    G^{f_s}\left(\bm{x}\right) {=} \sum_{j{=}1}^{L}\pi^s_{j}\Phi^{f_s}_j(\bm{x}), \quad
    G^{f_t}\left(\bm{x}\right) {=} \sum_{j{=}1}^{L}\pi^t_{j}\Phi^{f_t}_j(\bm{x}).
\vspace{-0.2cm}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Cluster-Level Matching.}
Instead of indirectly performing the maximum likelihood estimation between $G^s$ and $G^t$, weighted distribution-level correspondences are represented as soft assignments to the components based on the mixing weights of GMMs and the $L_2$ distance~\cite{jian2010robust} of distribution in the feature space. This is because $(\pi^s_{j},\bm{\mu}^s_{j},\bm{\Sigma}^s_{j})$ and $(\pi^t_{j},\bm{\mu}^t_{j},\bm{\Sigma}^t_{j})$ are not wholly matched when two point clouds are partially overlapped. Moreover, the aligned components should have similar mixing weights and small distances. To estimate the correspondences, we first calculate the distance between two GMMs as follows:
%+++++++++++++++++++++++++++++++++++++++++++++
\begin{equation}
\small
\mathcal{D}(\Phi^{f_s}_i,\Phi^{f_t}_j)=\int_{\mathbb{R}}\left(\Phi^{f_s}_i(\bm{x})-\Phi^{f_t}_j(\bm{x})\right)^2d\bm{x}.
\end{equation}
%+++++++++++++++++++++++++++++++++++++++++++++
We denote $r^x{=}\sum^{L-1}_{i=1}\max(\frac{\pi^x_i}{1-\pi^{x}_L}-\frac{\pi^y_i}{1-\pi^{y}_L},0)$ and cost matrix $\bm{D}$ with elements satisfying $\bm{D}_{ij}{=}\mathcal{D}(\Phi^{f_s}_i,\Phi^{f_t}_j)$. $x=s,y=t$ or $x=t,y=s$. 
In partially overlapping registration, some components are occluded in the other frame. Similar to~\cite{yu2021cofinet}, we propose here to solve it directly by changing the cost matrix as $\hat{\bm{D}}$ with elements satisfying,  if $i, j{<} L$, $\hat{\bm{D}}_{ij}{=}\bm{D}_{ij}$ otherwise $\bm{D}_{ij}{=}z$. $z$ is a learnable parameter.
The extended assignment matrix $\Gamma{\in}\mathbf{R}^{L\times L}$ can be estimated by solving the following optimization problem:
%---------------------------------
\vspace{-0.2cm}
\begin{equation}\label{eq:feature}
\begin{aligned}
& \min_{\Gamma}\sum_{ij}\Gamma_{ij}\hat{\bm{D}}_{ij},\\
& \mbox{s.t.,}~ \Gamma\bm{1}_{L}  {=} \hat{\bm{\pi}}^{s}, \Gamma^\top\bm{1}_{L} {=} \hat{\bm{\pi}}^{t}, \Gamma_{ij}\in[0, 1],
\end{aligned}
\vspace{-0.2cm}
\end{equation}
%-----------------------
where $\hat{\bm{\pi}}^{x}{=}\frac{1}{1+r^x-\pi^{x}_L}(\pi^{x}_1, \pi^{x}_2, \cdots, \pi^{x}_{L{-}1}, r^x), x{\in}\{s, t\}$. We run the Sinkhorn
Algorithm~\cite{cuturi2013sinkhorn} to seek an optimal solution.
After that, each entry $(i,j)$ of $\Gamma$ implies the matching confidence between components. Following~\cite{yu2021cofinet}, we pick correspondences whose confidence scores are above a threshold $\tau=0.1$.
We define the picked distribution-level correspondence set as $\bar{C}{=}\{(\bar{\bm{\mu}}^s_i,\bar{\bm{\mu}}^t_i)\}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.1cm}
\noindent\textbf{Point-Level Registration.}
We first partition the points into clusters by assigning each point to its closest centroid in the geometric space. Once grouped, we obtain 3D patches comprised of points along with their corresponding clustering scores and descriptors. These patches enable us to extract point correspondences.
For a centroid $\bm{\mu}^s_i$, its associated point set $\mathrm{C}^s_{i}$ and feature set $\mathrm{F}^s_{i}$ are denoted as:
%--------------------------------
\vspace{-0.2cm}
\begin{equation*}
\small
	\begin{cases}
		\mathrm{C}^s_{i}=\{\bm{p}^s\in \bm{\mathcal{P}}^s\big|\|\bm{p}^s-\bm{\mu}^s_i\|_2\leq\|\bm{p}^s-\bar{\bm{\mu}}^s_j\|_2, i\neq j\}, \\
		\mathrm{F}^s_{i}=\{\bm{f}^s_{j}\in \bm{\mathcal{F}}^s\big|\bm{p}^s_j \in \mathrm{C}^s_{i}\},\\
		\mathrm{S}^s_{i}=\{\bm{s}^s_{ji}\in \bm{s}^s_i\big|\bm{p}^s_j \in \mathrm{C}^s_{i}\}.
	\end{cases}
 \vspace{-0.2cm}
\end{equation*}
%------------------------------
The same operator is also performed for $\bm{\mu}^t_j$ and we get $\mathrm{C}^t_{i}$, $\mathrm{F}^t_{i}$, and $\mathrm{S}^t_{i}$. The cluster-level correspondence set $\mathcal{M}^\prime$ are expanded to its corresponding 3D patch, both in geometry space $\mathcal{M}_C{=}\{(\mathrm{C}^s_{i}, \mathrm{C}^t_{i})\}$, feature space $\mathcal{M}_F{=}\{(\mathrm{F}^s_{i}, \mathrm{F}^t_{i})\}$, and normalized clustering scores $\mathcal{M}_S{=}\{(\mathrm{S}^s_{i}, \mathrm{S}^t_{i})\}$. 
For computational efficiency, every patch samples the $K$ number of points based on the probability. Similar to cluster-level prediction, given a pair of overlapped patches $(\mathrm{C}^s_{i}, \mathrm{F}^s_{i}, \mathrm{S}^s_{i})$ and $(\mathrm{C}^t_{i}, \mathrm{F}^t_{i}, \mathrm{S}^t_{i})$, extracting point correspondences is  to match two smaller corresponded scale point clouds $(\mathrm{C}^s_{i}, \mathrm{C}^t_{i})$ by solving an optimization problem:
%--------------------------------
\vspace{-0.2cm}
\begin{equation}\label{eq:point}
 \min_{\bm{\Gamma}^i}\left<\bm{D}^i, \bm{\Gamma}^i\right>,
 \mbox{s.t.,~}{\bm{\Gamma}^i}^\top\bm{1}_K=\mathrm{S}^t_{i}, \bm{\Gamma}^i\bm{1}_K=\mathrm{S}^s_{i},
 \vspace{-0.2cm}
\end{equation}
where each $\bm{\Gamma}^i{=}[\bm{\Gamma}^i]^{K\times K}_{kl}$ represents an assignment matrix and $\bm{D}^i{=}[\bm{D}^i]_{kl}$ with $\bm{D}^i_{kl}{=}\|\frac{\mathrm{F}^s_{i}\left(k\right)}{\|\mathrm{F}^s_{i}(k)\|_2}{-}\frac{\mathrm{F}^t_{i}(l)}{\|\mathrm{F}^t_{i}(l)\|_2}\|_2$. 
After reaching $\bm{\Gamma}^i$, we select correspondences from $(\mathrm{C}^s_{i}, \mathrm{C}^t_{i})$ with maximum confidence score for each row of $\bm{\Gamma}^i$. 
We denote each correspondence set extracted from a pair of patches as $\mathcal{M}^i{=}\{(\bm{p}^s_{\hat{i}} {\in} \mathrm{C}^s_{i},\bm{p}^t_{\hat{j}} {\in} \mathrm{C}^t_{i}),\hat{i}{=}1,2,\cdots,K\big|\hat{j} {=} \arg \max_{k} \bm{\Gamma}^{i}_{\hat{i},k}\}$.
%--------------------------------
% \vspace{-0.2cm}
% \begin{equation*}
% \mathcal{M}^i{=}\{(\bm{p}^s_{\hat{i}} {\in} \mathrm{C}^s_{i},\bm{p}^t_{\hat{j}} {\in} \mathrm{C}^t_{i})\big|\hat{j} {=} \arg \max_{k} \bm{\Gamma}^{i}_{\hat{i},k},\hat{i}{=}1,2,\cdots,K\}.
% \vspace{-0.2cm}
% \end{equation*}
%--------------------------------
The final point correspondence set $\mathcal{M}$ consists of the union of all the obtained patch-level correspondence sets $\mathcal{M}^i$. Following \cite{bai2021pointdsc,yu2021cofinet}, a variant of RANSAC \cite{fischler1981random} that is specialized to 3D registration takes $\mathcal{M}$ as an input to estimate the transformation.

\subsection{Consistency-Based Unsupervised Learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Self-Consistency Loss.}
Our self-consistency loss encourages point clouds to share an identical posterior distribution in coordinate and feature spaces. It can be directly used without using any data augmentation.
Because training the network parameters is equivalent to optimizing the $\bm{\Theta}^s$ and $\bm{\Theta}^t$, the GMMs parameters can be fitted to the observed data points via maximizing the log-likelihood of samples to $\bm{\Theta}^s$ and $\bm{\Theta}^t$.
% %-------------------------------------
% \vspace{-0.2cm}
% \begin{equation*}
%     \max_{\bm{\Theta}^s,\bm{\Theta}^t} \left(\frac{1}{N}\sum_{i=1}^{N}\log\left(G^{s}\left(\bm{p}^s_i\right)\right)+ \frac{1}{M}\sum_{i=1}^{M}\log\left(G^t\left(\bm{p}^t_j\right)\right)\right).
%     \vspace{-0.2cm}
% \end{equation*}
% %-------------------------------------
However, the log-likelihood function is unstable in the training processing since its value goes to infinity for a specific combination of means and some degenerating covariance matrices.
To avoid covariance degeneration, we approximate the probabilities of points belonging to each cluster based on their distance to the centroids estimated by Eq.~\eqref{eq:gmm_3d} under the constraints of the mixture weights.  
We denote the empirical distribution matrices of $\bm{\mathcal{P}}^s$ and $\bm{\mathcal{P}}^t$ as $\bm{\gamma}^s=\{\bm{\gamma}^s_{ij}\}$ and $\bm{\gamma}^t=\{\bm{\gamma}^t_{ij}\}$.
This results in the following optimization objective:
%-------------------------------------
\vspace{-.2cm}
\begin{equation}\label{eq:sgamma}
\begin{aligned}
& \min_{\bm{\gamma}^x}\sum_{i,j}\bm{\gamma}^x_{ij}\|\bm{p}^x_i-\bm{\mu}^x_j\|^2_2,\\
& \mbox{s.t.,}~ \sum_i\bm{\gamma}^x_{ij} {=} N_x\bm{\pi}^{x}_j, \sum_j\bm{\gamma}^x_{ij} {=} 1, \bm{\gamma}_{ij}\in[0, 1],
\end{aligned}
\vspace{-.2cm}
\end{equation}
%-------------------------------------
where $x \in \{s, t\}$. 
$\sum_j\bm{\gamma}^x_{ij}{=}1$ is based on the property of the probability that the sum of all the probabilities for all possible events is equal to one. 
$\sum_i\bm{\gamma}^x_{ij} {=} N_x\bm{\pi}^{x}_j$ is the constraints of the mixture weights.
We address the minimization of Eq.~\eqref{eq:sgamma} by adopting an efficient version of the Sinkhorn algorithm~\cite{cuturi2013sinkhorn}. Coordinate and feature spaces share an identical posterior distribution means that $\bm{S}^x$ and $\bm{\gamma}^x$ should be equal, which leads to a cross-entropy loss. Our self-consistency loss is thus formulated as follows:
%-------------------------------------
\vspace{-0.2cm}
\begin{equation}\label{eq:sc}
    \mathcal{L}_{sc} = -\sum_{ij}\bm{\gamma}^s_{ij}\log s^t_{ij} - \sum_{ij}\bm{\gamma}^t_{ij}\log s^t_{ij}.
\vspace{-0.2cm}
\end{equation}
%-------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Cross-Consistency Loss.} 
The described self-consistency loss only encourages the learned representation to be spatially sensitive, but it cannot ensure that the learned features be transformation invariant.
Therefore, we introduce a cross-consistency loss to encourage the network to learn transformation-invariant feature representations.
Our cross-consistency loss is based on the fact that
the cluster labeling should not change if the points are rigidly transformed.
This fact means that if points $\bm{p}^s{\in}\bm{\mathcal{P}}^s$ and $\bm{p}^t{\in}\bm{\mathcal{P}}^t$ belong to the same cluster, they should share the same cluster centroid.
Therefore, the cross-consistency loss can make full use of the information from both aligned point clouds.
Concretely, for two input features sets $\left(\bm{\mathcal{F}}^s, \bm{\mathcal{F}}^t\right)$, and two probability matrices $\left(\bm{S}^s, \bm{S}^t\right)$, we obtain a new feature set $\bm{\mathcal{F}} {=} cat\left(\bm{\mathcal{F}}^s, \bm{\mathcal{F}}^t\right)$ and a probability matrix $\bm{\mathcal{S}} {=} cat\left(\bm{S}^s, \bm{S}^t\right)$. 
$cat(\cdot,\cdot)$ means concatenation.
We assume the current estimated rotation and translation are $\bm{R}$ and $\bm{t}$. We define $\bm{\mathcal{\bar{P}}}^s{=}\bm{R}\bm{\mathcal{P}}^s{+}\bm{t}$  and $\bm{\mathcal{P}}{=}cat(\bm{\mathcal{\bar{P}}}^s,\bm{\mathcal{P}}^t)$.
Then, we calculate the parameters of global GMMs in both feature and euclidean spaces as:
%+++++++++++++++++++++++++++++++++++++++++++++
\vspace{-0.2cm}
\begin{equation*}
\small
   \pi_j = \frac{\sum_{i}\bm{s}_{ij}}{N},
    ~\bm{\mu}^{f}_j = \frac{\sum_{i}\bm{s}_{ij}\bm{f}_{i}}{\pi_jN}, ~\bm{\mu}^{e}_j = \frac{\sum_{i}\bm{s}_{ij}\bm{p}_{i}}{\pi_jN},
\vspace{-0.2cm}
\end{equation*}
%+++++++++++++++++++++++++++++++++++++++++++++
where $N{=}N_s{+}N_t$.
To avoid two aligned point clouds being grouped into separate clusters, we assume that clustering satisfies two constraints:
\begin{itemize} [leftmargin=*]
\item GMMs are coupled with approximate uniform mixing weights in coordinate and feature spaces.
\item If a point $\bm{p}_i$ belongs to partition $j$, point $\bm{p}_i$ and its coupled centroid should have the shortest distance.
\end{itemize}
Let $\bm{\gamma}{=}\{\gamma_{ij}\}$ to be the empirical probability matrix. 
The two constraints can then be ensured by minimizing the following objective:
%-------------------------------------
\vspace{-.2cm}
\begin{equation}\label{eq:mgamma}
\small
    \begin{aligned}
        & \min_{\bm{\gamma}}\sum_{ij}\left(\lambda_1\|\bm{p}_i-\bm{\mu}^e_j\|^2_2+\lambda_2\|\bm{f}_i-\bm{\mu}^f_j\|^2_2\right)\gamma_{ij}, \\
        & \mbox{s.t.,} \sum_i\gamma_{ij} = 1, \sum_j\gamma_{ij} = \frac{N}{L}, \gamma_{ij}\in[0, 1],
    \end{aligned}
\vspace{-.2cm}
\end{equation}
%-------------------------------------
where $\lambda_i {\in} [0,1]$ are learned parameters. 
After solving Eq.~\eqref{eq:mgamma}, we then infer our cross-consistency loss as:
%-------------------------------------
\vspace{-.2cm}
\begin{equation}\label{eq:scsup}
    \mathcal{L}_{cc}(\bm{\gamma}, \bm{S}) = -\sum_{ij}\bm{\gamma}_{ij}\log s_{ij},
\vspace{-.2cm}
\end{equation}
%-------------------------------------
which corresponds to the minimization of the standard cross-entropy loss between $\bm{\gamma}$ and predictions $\bm{S}$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.1cm}
\noindent\textbf{Local Contrastive Loss.}
%-------------------------------------
The local neighbors provide essential information for feature learning on the objects of the point clouds~\cite{li2022hybridcr}. 
For instance, occlusions and holes always occur in objects in indoor and outdoor scenes~\cite{li2022hybridcr}. 
If the network captures the local structure information from other complete objects, it can boost the model robustness on incomplete objects during training. 
While the local descriptors of the point clouds mainly derive from the points and their neighbors~\cite{li2022hybridcr}, which motivates us to model the local information of the point cloud by introducing local contrastive loss. 
Specifically, given a centroid $\bm{\mu}^x_i$ of point cloud $\bm{\mathcal{P}}^x$ with $x{\in}\{s,t\}$, we search its nearest point $\bm{p}^x_i$ and associated feature vector $\bm{f}^x_i$ by the point-wise Euclidean distance. Based on this, we construct the local contrastive loss $\mathcal{L}_{lc}$ following InfoNCE~\cite{xie2020pointcontrast} by pulling $\bm{f}^x_i$ close to $\bm{\mu}^x_i$, while pushing it away from the neighbor vector of other points. We also encourage $\bm{\mu}^{f_s}_i$ and $\bm{\mu}^{f_t}_i$ to be similar:
%-------------------------------------
\vspace{-0.2cm}
\begin{equation*}
\small
\begin{aligned}
&\mathcal{L}_{lc} {=}{-}\frac{1}{L}\sum^L_{i=1}
\log \frac{\exp\left(\bm{\mu}^{f_s}_i{\bm{\mu}^{f_t}_i}^\top\right)}{\sum^L_{j=1}\exp\left(\bm{\mu}^{f_s}_i{\bm{\mu}^{f_t}_j}^\top\right)}{-}\\
&\frac{1}{L}\sum^L_{i=1}\log \frac{\exp\left(\bm{\mu}^{f_s}_i{\bm{f}^s_i}^\top\right)\exp\left(\bm{\mu}^{f_t}_i{\bm{f}^t_i}^\top\right)}{\sum^L_{j=1}\exp\left(\bm{\mu}^{f_s}_i{\bm{f}^s_j}^\top\right)\sum^L_{j=1}\exp\left(\bm{\mu}^{f_t}_i{\bm{f}^t_j}^\top\right)}.
\end{aligned}
\vspace{-0.2cm}
\end{equation*}
%-------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Thus, the final loss is the combination of self-consistency loss, cross-consistency loss, and local contrastive loss as:
%-------------------------------------
\vspace{-0.2cm}
\begin{equation}
\small
    \mathcal{L} = \mathcal{L}_{sc} + \mathcal{L}_{cc} + \mathcal{L}_{lc}.
    \vspace{-0.2cm}
\end{equation}
%-------------------------------------
In particular, different from most existing methods, the correspondence or pose between two partially overlapping point clouds is unknown in our training processing.
% {\color{red} cross contra for overlap scores}

