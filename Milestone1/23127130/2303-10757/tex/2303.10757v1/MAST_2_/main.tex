% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}

\usepackage{booktabs} % To thicken table lines

\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{amsmath}
\def\ie{\emph{i.e.}}
\def\eg{\emph{e.g.}}
\def\etc{\emph{etc}}
\def\etal{\emph{et al.}}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{MULTISCALE AUDIO SPECTROGRAM TRANSFORMER \\ FOR EFFICIENT AUDIO CLASSIFICATION}
%
% Single address.
% ---------------


\name{Wentao Zhu$^{\star}$ \qquad Mohamed Omar$^{\star}$}

 

\address{$^{\star}$ Amazon}
% \name{Wentao Zhu, Mohamed Omar}%Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
% \address{Amazon}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
%\ninept
%
% Papers may be no longer than 5 pages, including all text, figures, and references, and the 5th page may contain only references.
\maketitle
%
\begin{abstract}
% Audio can be efficiently perceived in a hierarchical structure,~\eg, from each individual audio signal per sampling time point to audio activities and the category for audio classification.
Audio event has a hierarchical architecture in both time and frequency and can be grouped together to construct more abstract semantic audio classes. In this work, we develop a multiscale audio spectrogram Transformer (MAST) that employs hierarchical representation learning for efficient audio classification. Specifically, MAST employs one-dimensional (and two-dimensional) pooling operators along the time (and frequency domains) in different stages, and progressively reduces the number of tokens and increases the feature dimensions. MAST significantly outperforms AST~\cite{gong2021ast} by 22.2\%, 4.4\% and 4.7\% on Kinetics-Sounds, Epic-Kitchens-100 and VGGSound in terms of the top-1 accuracy without external training data. On the downloaded AudioSet dataset, which has over 20\% missing audios, MAST also achieves slightly better accuracy than AST. In addition, MAST is 5$\times$ more efficient in terms of multiply-accumulates (MACs) with 42\% reduction in the number of parameters compared to AST. Through clustering metrics and visualizations, we demonstrate that the proposed MAST can learn semantically more separable feature representations from audio signals.  % For single model comparison
\end{abstract}
%
\begin{keywords}
Audio event classification, audio Transformer, multiscale audio Transformer
\end{keywords}
%
\section{Introduction}
\label{sec:intro}
Audio classification has many applications, such as speaker recognition~\cite{zhu2021speechnas}, event, emotion and intent classification~\cite{li2018attention,gong2021ast}. The audio classification has been improved from manually designed feature based approaches~\cite{eyben2013recent,schuller2013interspeech} and hidden Markov models (HMM)~\cite{woodard1992modeling} to deep learning based end-to-end solutions~\cite{jaitly2011learning,dieleman2014end,trigeorgis2016adieu}. Among these deep learning models, convolutional neural networks have become a \textit{de facto} standard component to model various fixed lengths of dependencies along the time dimension for audio classification~\cite{lecun1995convolutional,hershey2017cnn}. Recently, pure self-attention based deep learning architectures, without convolutional network's inductive bias,~\ie, spatial locality and translation equivariance, have outperformed conventional convolutional networks on audio classification~\cite{gong2021ast,gong2022ssast}.  %, image classification~\cite{dosovitskiy2020image},~\etc.  % , and multimodal action and event classification~\cite{nagrani2021attention}  and event classification~\cite{gong2021ast}

On the other hand, audio can be efficiently perceived in a hierarchical structure~\cite{dieleman2013multiscale,snyder2018x},~\eg, from each individual audio sample to audio activities and semantic audio classes. In the convolutional networks, the hierarchical feature learning can be achieved through various dilation rates and pooling strategies along the time dimension~\cite{snyder2018x}. To the best of our knowledge, there is no multiscale pure-Transformer architecture for audio classification, which can be utilized to learn hierarchical audio representations. %For pure-Transformer architecture, multiscale structure can be utilized to learn hierarchical representations from dense and simple to coarse and complex for video classification~\cite{li2021improved}.

\begin{figure}
	\centering  % trim={<left> <lower> <right> <upper>}
	\includegraphics[width=\linewidth,trim=0.1 1 0.5 0.5,clip]{mast_v7}
	\caption{One block of multiscale audio spectrogram Transformer (MAST). The pooling operator in the block permits to construct representations from dense to coarse resolution and is able to effectively learn hierarchical audio representations.}  % We leverage the multiscale attention architecture~\citep{li2021improved}
	\label{fig:audio_framework}
\end{figure}

In this work, we design a multiscale audio spectrogram transformer (MAST) which processes the audio spectrogram for audio classification. We compare our MAST's architecture with widely used AST~\cite{gong2021ast} explicitly in table~\ref{tab:arch} of section~\ref{sec:method}. MAST utilizes a multiscale architecture, which is efficient and yields a discriminative representation for audio classification. MAST outperforms AST by a large margin on Kinetics-Sounds~\cite{arandjelovic2017look,kay2017kinetics}, Epic-Kitchens-100~\cite{Damen2021RESCALING,Damen2018EPICKITCHENS,Damen2021PAMI} and VGGSound~\cite{chen2020vggsound}. MAST also achieves slightly better accuracy than AST on the downloaded AudioSet~\cite{gemmeke2017audio}. Moreover, MAST is \textbf{5}$\times$ more efficient based on the number of MACs with only 58\% parameter numbers than AST. Through UMAP~\cite{mcinnes2018umap} visualization and clustering metric discussion based on representations, we demonstrate that MAST can learn semantically more separable representations from audio signals. The efficient and light weighted MAST can be an essential component for multimodal architecture design and a strong baseline for audio representation learning.

\begin{table*}[]
    \centering
    \caption{Architecture comparison between AST and MAST on AudioSet~\cite{gemmeke2017audio}. MAST employs multiscale representation learning and uses 58\% of the number of AST parameters and 24\% MACs of AST.}
    \label{tab:arch}
    \begin{tabular}{c|c|c|c|c}
    \toprule
     &\multicolumn{2}{c|}{AST}&\multicolumn{2}{|c}{MAST}\\ 
        Block & Feature & Arch./Param.& Feature & Arch./Param.\\ \midrule
        Input & 1$\times$128$\times$1024 & 0 & 1$\times$128$\times$1024 & 0 \\ \midrule
        \begin{tabular}{@{}c@{}}Patch Embed.\end{tabular}&768$\times$(1212=12$\times$101) & 768$\times$16$\times$16$\times$1 & 96$\times$(8192=32$\times$256) & 96$\times$7$\times$7$\times$1 \\ \midrule
        Block \{0, 1\}& 768$\times$1212 & Attn-MLP& 96$\times$8192 & Attn-MLP \\ \midrule
        Block 2 & 768$\times$1212 & Attn-MLP& 192$\times$(2048=16$\times$128) & MMSA-MLP \\ \midrule
        Block \{3, 4\}& 768$\times$1212 & Attn-MLP& 192$\times$2048 & Attn-MLP \\ \midrule
        Block 5 & 768$\times$1212 & Attn-MLP& 384$\times$(512=8$\times$64) & MMSA-MLP \\ \midrule
        Block \{6,$\cdots$,11\}& 768$\times$1212 & Attn-MLP& 384$\times$512 & Attn-MLP \\ \midrule
        Block 12& 527 & 527$\times$768& 384$\times$512 & Attn-MLP \\ \midrule
        Block \{13,$\cdots$20\}& - & -& 384$\times$512 & Attn-MLP \\ \midrule
        Block 21& - & -& 768$\times$(256=8$\times$32) & MMSA-MLP \\ \midrule
        Block \{22,23\}& - & -& 768$\times$256 & Attn-MLP \\ \midrule
        Block 24& - & -& 527 & 527$\times$768 \\ \bottomrule
    \end{tabular}
\end{table*}
\section{Related Work}
\label{sec:related}
With large scale and realistic datasets,~\eg, AudioSet~\cite{gemmeke2017audio}, advanced network architectures have been adopted for audio classification including convolutional neural networks~\cite{kong2020panns,wang2019comparison}, convolutional-attention networks~\cite{gong2021psla,kong2020sound}, and recent pure-attention based networks~\cite{gong2021ast,chen2022hts}. Particularly, AST~\cite{gong2021ast} outperforms previous state-of-the-art audio classification approaches, and obtains widely adoption in many tasks,~\eg, multimodal event classification~\cite{zhumultiscale,zhuavt} and video retrieval~\cite{lin2022eclipse}. CMKD~\cite{gong2022cmkd} further designs cross-modal knowledge distillation between convolutional networks and AST for audio classification. SSAST~\cite{gong2022ssast} conducts masked spectrogram patch modeling in self-supervised learning to reduce the need for large amount of labeled data.  %, and it achieves comparable accuracy as AST. % HTS-AT~\cite{chen2022hts} is a hierarchical, scalable and lightweight token-semantic transformer for audio classification. It achieves better accuracy on AudioSet and ESC-50. Furthermore, HTS-AT~\cite{chen2022hts} employs the token-semantic module to locate the event’s start and end time.

There are several hierarchical Transformers for efficient language processing and computer vision. In language processing, Funnel-Transformer~\cite{dai2020funnel} gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. Swin Transformer~\cite{liu2021swin} designs a shifted window strategy in an image Transformer. PVT~\cite{wang2021pyramid} uses a progressive shrinking pyramid to reduce the computations of large feature maps for dense prediction tasks,~\eg, object detection and semantic segmentation. Multiscale Transformers~\cite{fan2021multiscale,li2021improved} adopts several channel-resolution scale stages and hierarchically expands the channel capacity while reducing the spatial resolution. We design a multiscale audio Transformer with one-dimensional and two-dimensional pooling operators along the time dimension and frequency dimension in audio spectrogram for audio classification, which achieves better accuracy than AST with much more efficient number of parameters and MACs.
\section{Multiscale Audio Transformer}
\label{sec:method}

We can perceive an audio sequence in a hierarchical structure, from one signal value at each sampling time point to audio activities and an audio classification category for the whole sequence. Therefore, hierarchical representational learning from an audio spectrogram, which progressively reduces the temporal length and increases the channel dimensions, improves audio-based action recognition. We construct a multiscale audio spectrogram Transformer (MAST) with an audio spectrogram $X \in \mathbb{R}^{h\times T}$ as input, where $h$ is the number of triangular mel-frequency bins, and $T$ is the temporal length. The multiscale audio spectrogram Transformer (MAST) is illustrated in Fig.~\ref{fig:audio_framework}. After the patch embedding, which can be a convolutional block in table~\ref{tab:arch}, conducted in the audio spectrogram, we obtain the embedding token matrix $A \in \mathbb{R}^{d\times N}$, where $d$ is the embedding dimension and $N$ is the number of tokens. One block of MAST can be a stack of multihead multiscale self-attention (MMSA), layer normalization (LN) and multilayer perceptron (MLP)
\begin{equation}\label{eq:mat}
\begin{aligned}
    &A^{\prime} = \text{MMSA}( \text{LN} (A) ) + \mathcal{P}(A), \\ &\text{Block}(A) = \text{MLP} ( \text{LN} (A^{\prime}) ) + A^{\prime},
\end{aligned}
\end{equation}
where $\mathcal{P}$ is a pooling operator, which can be a one-dimensional pooling along the time dimension or a two-dimensional pooling along both the time and frequency dimensions. One head in multihead multiscale self-attention~\cite{li2021improved} (MSAttn) can be % formulated as
\begin{equation}
\begin{aligned}
    &Q = \mathcal{P}_Q (A W_Q), \, K = \mathcal{P}_K (A W_K), \, V = \mathcal{P}_V (A W_V), \\ &\text{MSAttn}(A) = Q + \text{Softmax}((QK^T + E^{(rel)}) / \sqrt{d} )V,
\end{aligned}
\end{equation}
where $E^{(rel)}_{ij} = Q_i \cdot R_{p(i),p(j)} = Q_i \cdot (R^t_{t(i),t(j)} + R^f_{f(i),f(j)})$, $R^t$ and $R^f$ are positional embeddings along the temporal and feature axes in the spectrogram.  % , and $d$ is the embedding dimension. %$A$ can be temporally residual pooling~\citep{li2021improved} based intermediate audio features from the last block or input. 

The multihead multiscale self-attention (MMSA) can be stacked to construct the multiscale audio spectrogram Transformer (MAST) for audio classification. To explicitly demonstrate the details of network architecture, we list and compare the networks of AST and MAST in table~\ref{tab:arch}. In block 21, the pooling is one-dimensional and conducted on the time dimension to retain eight dimensions of spectrogram features, compared with 12 dimensions in AST. MAST employs fewer number of feature dimensions than AST in the first 21 blocks, and it utilizes fewer number of tokens than AST after the 5th block. The multiscale design leads to fewer number of parameters and MACs of MAST than AST. We also experiment other multiscale pooling schedules and strategies, and we find the design in table~\ref{tab:arch} yields the best accuracy in section~\ref{sec:result}.
% Network Architecture:

% AST:

% 128 x 1024

% (12x101 = 1212) x 768 - 12 blocks

% 768

% 527

% MAST:

% 128 x 1024

% (32 x 256 = 8192) x 96 - 2 blocks

% (16 x 128 = 2048) x 192 - 3 blocks

% (8 x 64 = 512) x 384 - 19 blocks

% (4 x 32 = 128) x 768 - 3 blocks

% 768

% 527

% MAST-T

% 128 x 1024

% (32 x 256 = 8192) x 96 - 2 blocks

% (16 x 128 = 2048) x 192 - 3 blocks

% (16 x 64 = 1024) x 384 - 19 blocks

% (16 x 32 = 1024) x 768 - 3 blocks

% 768

% 527

Compared with the previous audio spectrogram Transformer~\cite{gong2021ast}, MAST can efficiently extract representation that effectively models hierarchical characteristics of audio signals. In section~\ref{sec:result}, we demonstrate that MAST significantly reduces the number of parameters and MACs. The efficient MAST is light-weighted and can be used as a component in multimodal networks.

\section{Experimental Results}
\label{sec:result}
% \subsection{Datasets}
We experiment with four audio event classification datasets – Kinetics-Sounds~\cite{arandjelovic2017look,kay2017kinetics}, Epic-Kitchens-100~\cite{Damen2021RESCALING,Damen2018EPICKITCHENS,Damen2021PAMI}, VGGSound~\cite{chen2020vggsound} and AudioSet~\cite{gemmeke2017audio}. %kay2017kinetics Results on two additional in-house datasets are in the appendix.


Kinetics-Sounds is a commonly used subset of Kinetics~\cite{kay2017kinetics}, which consists of 10-second audios from YouTube. As Kinetics-400 is a dynamic dataset and audios may be removed from YouTube, we follow the dataset collection protocol in Xiao~\etal~\cite{xiao2020audiovisual}, and we collect 22,914 valid training audios and 1,585 valid test audios. 

Epic-Kitchens-100 consists of 90,000 variable length egocentric clips spanning 100 hours capturing daily kitchen activities. The dataset formulates each action into a verb and a noun. We employ two classification heads, one for verb classification and the other one for noun classification.


VGGSound is a large scale action recognition dataset, which consists of about 200K 10-second clips and 309 categories ranging from human actions and sound-emitting objects to human-object interactions. Like other YouTube datasets,~\eg, AudioSet~\cite{gemmeke2017audio}, some audios are no longer available. After removing invalid audios, we collect 159,223 valid training audios and 12,790 valid test audios.

AudioSet~\cite{gemmeke2017audio} is another YouTube dataset, which consists of almost 2 million 10-second video clips annotated with 527 classes. After removing invalid audios, this gives us 15,818 audios for the test set, which misses 23\% audios compared with 20,372 audios in the original test set, 17,823 audios for the balanced training set, which misses 20\% audios compared with 22,162 audios in the original balanced training set, and 1,592,753 audios for the full training set, which misses about 25\% audios compared with the original 2M unbalanced training set. Over 20\% audios are missing on both training and test sets, and rerun the experiments on the downloaded AudioSet based on the official code of AST (https://github.com/YuanGongND/ast) is necessary for a fair comparison. We train MAST with a binary cross-entropy (BCE) loss and report mean average precision (mAP) over all classes for multi-label classification. %, following standard practice.
\begin{table}%[h]
	\caption{Comparison to state of the art on Kinetics-Sounds. We report top-1 and top-5 classification accuracy.}  % ~\citep{arandjelovic2017look}
	\label{tab:ks}
	\begin{center}
		\begin{tabular}{lll}
			\toprule
			Models  &Top-1 & Top-5
			\\ \midrule 
			AST~\cite{nagrani2021attention} & 52.6&71.5 \\ \midrule
			MAST (Ours)  & \textbf{74.8} & \textbf{93.1} \\ \bottomrule
			\\
		\end{tabular}
	\end{center}
\end{table}
\begin{table}[h]
	\caption{Comparison to state of the art on VGGSound~\cite{chen2020vggsound}.}  %  We report top-1 and top-5 classification accuracy.
	\label{tab:vgg}
	\begin{center}
		\begin{tabular}{lll}
			\toprule
			Models   & Top-1 & Top-5
			\\ \midrule 
			Chen~\etal~\cite{chen2020vggsound}        & 48.8 & 76.5 \\
			AudioSlowFast~\cite{kazakos2021slow}  & 50.1 & 77.9 \\
			AST~\cite{nagrani2021attention}  &52.3 &78.1 \\ \midrule 
			 MAST (Ours)  & \textbf{57.0} &\textbf{81.3} \\ \bottomrule
			\\
		\end{tabular}
	\end{center}
\end{table}
\begin{table}[h]
	\caption{Comparison to state of the art on Epic-Kitchens-100.}  % ~\citep{Damen2021RESCALING}  Modalities are A: Audio, V: Visual,
	\label{tab:epic}
	\begin{center}
		\begin{tabular}{llll}
			\toprule
			Models & Verb & Noun & Action
			\\  \midrule  
			Damen~\etal~\cite{Damen2021RESCALING} & 42.1 & 21.5 & 14.8 \\
			AudioSlowFast~\cite{kazakos2021slow}            & 46.5 & 22.8 & 15.4 \\ 
			AST~\cite{nagrani2021attention} &44.3 &22.4 & 13.0 \\
			\midrule 
			MAST (Ours) & \textbf{50.1}&\textbf{24.2} &\textbf{17.4} \\ \bottomrule
			\\
		\end{tabular}
	\end{center}
\end{table}
\begin{table}[h]  %[!htbp]
	\caption{Comparison to state of the art on AudioSet (single model) based on mAP. Our AS denotes evaluation on the downloaded AudioSet. MACs (G), \#Params (million).}  % ~\citep{Damen2021RESCALING}  Modalities are A: Audio, V: Visual,
	\label{tab:as}
	\begin{center}
		\begin{tabular}{llllll}
			\toprule
			Models & Balanced & Full & MACs &\#Params
			\\  \midrule  
			{\color{gray}Baseline~\cite{gemmeke2017audio}} & {\color{gray}-} & {\color{gray}31.4}&{\color{gray}-}&{\color{gray}-} \\
			{\color{gray}PANNs~\cite{kong2020panns}} & {\color{gray}27.8} & {\color{gray}43.9}&{\color{gray}-}&{\color{gray}-} \\
			{\color{gray}PSLA~\cite{gong2021psla}} & {\color{gray}31.9} & {\color{gray}44.4}&{\color{gray}-}&{\color{gray}-} \\
			{\color{gray}AST~\cite{gong2021ast}}            & {\color{gray}34.7} & {\color{gray}45.9}& {\color{gray}103.4}&{\color{gray}88.1} \\ % 88.132 million
			{\color{gray}MBT (AST)~\cite{nagrani2021attention}} &{\color{gray}31.3} &{\color{gray}44.3}& {\color{gray}103.4}&{\color{gray}88.1}\\ \midrule 
			AST~\cite{gong2021ast} (Our AS) &31.3 & 38.9& 103.4&88.1 \\
			\midrule  
% 			MAST & 29.7 &37.3  \\ % ./AudioSet_MAST_IN21K/ep155 ./AudioSet_full_MAST_IN21K/ep64
			MAST (Ours) & \textbf{31.4} &\textbf{39.0} & \textbf{25.6}& \textbf{51.3} \\ \bottomrule
			\\
		\end{tabular}
	\end{center}
\end{table}

% \begin{table}
% 	\caption{Ablation study on AudioSet.}  % ~\citep{Damen2021RESCALING}  Modalities are A: Audio, V: Visual,
% 	\label{tab:ablation}
% 	\begin{center}
% 		\begin{tabular}{llll}
% 			\toprule
% 			Models & Verb & Noun & Action
% 			\\  \midrule  
% 			Damen~\etal~\cite{Damen2021RESCALING} & 42.1 & 21.5 & 14.8 \\
% 			AudioSlowFast~\cite{kazakos2021slow}            & 46.5 & 22.8 & 15.4 \\ 
% 			AST~\cite{nagrani2021attention} &44.3 &22.4 & 13.0 \\
% 			\midrule 
% 			MAST (Ours) & \textbf{50.1}&\textbf{24.2} &\textbf{17.4} \\ \bottomrule
% 			\\
% 		\end{tabular}
% 	\end{center}
% \end{table}

% 0.2943 run_tune_time_stride_noprint.log 51.380 million
% 
% Average Silhouette
% Adjusted Rand Index
% Homogeneity Score
\begin{table}[h]%[!htbp]
\centering
\caption{Statistical metrics for representations of all categories on VGGSound test set.}\label{tab:vgg_feat_ablation}
    \begin{tabular}{l|ll}
	\toprule
	 Metrics & AST~\cite{gong2021ast} & MAST (Ours) \\ \midrule
% 	AS & 0.248 & 0.527 & 0.311  \\  % Average Silhouette
	Average silhouette~\cite{rousseeuw1987silhouettes} & 0.343 &  \textbf{0.527}  \\  % Adjusted Rand Index
	Adjusted rand index~\cite{hubert1985comparing} & 0.292 &  \textbf{0.375}  \\
	Homogeneity score~\cite{rosenberg2007v} &0.695 &\textbf{0.720} \\
% 	NMI & 0.292& 0.275\\
			\bottomrule
		\end{tabular}
\end{table}
\begin{figure}[h]
	\centering  % trim={<left> <lower> <right> <upper>}
	\includegraphics[width=\linewidth,trim=0 4 0 3,clip]{umap_vgg_30_ast_mast_v2.png}
	\caption{UMAP~\cite{mcinnes2018umap} visualizations for test sample representations of the first 30 classes on VGGSound from AST (Left) and MAST (Right). MAST extracts semantically more separable representations than AST from the circled classes.}  % We leverage the multiscale attention architecture~\citep{li2021improved}
	\label{fig:umap}
\end{figure}

For hyperparameters in MAST, we follow MViTv2-B~\cite{li2021improved} and use ImageNet-1K publicly available pretrained weights. AdamW~\cite{loshchilov2018decoupled} is used in the backpropagation and the learning rate is set as 0.00001 with cosine annealing schedule~\cite{loshchilov2016sgdr}. The numbers of epochs are set as 300, 100, 50, 50 and 10 for Kinetics-Sounds, Epic-Kitchens-100, VGGSound, AudioSet blanced and full sets, respectively. We employ the code of AST to calculate the number of parameters, and ptflops library (https://pypi.org/project/ptflops/) to calculate the number of multiply-accumulates (MACs). Note that one MAC is roughly equal to two floating point operations (FLOPs).

We compare MAST with the previous state-of-the-art single models on the four datasets in table~\ref{tab:ks},~\ref{tab:vgg},~\ref{tab:epic},~\ref{tab:as}. Best scores are in \textbf{bold} face. MAST outperforms AST on all the four datasets, with only 24\% MACs and 58\% parameter numbers as those of AST. Specifically, MAST outperforms AST by 22.2\%, 4.7\% and 4.4\% based on the top-1 accuracy on Kinetics-Sounds, VGGSound and Epic-Kitchens-100 datasets, respectively. Because the downloaded AudioSet has much fewer training samples as the used dataset of AST~\cite{gong2021ast}, we rerun the AST using the official code and use (Our AS) to denote the difference. On the downloaded AudioSet, MAST with much fewer MACs and number of parameters achieves slightly better mAP than AST. %Note that MAST can employ cross knowledge distillation~\cite{gong2022cmkd} and self-supervised learning~\cite{gong2022ssast} to further improve accuracy and efficiency.

We conduct ablation study $\it w.r.t.$ the number of pooling operators and pooling strategies on the balanced AudioSet. To compare the architecture without 1D pooling, we employ the 2D pooling in the block 21 and obtain 29.7\%, which is probably because we reduce the dimension along the triangular mel-frequency bins dimension too much. We also try to use 2 pooling operators and remove the pooling in the block 21, which achieve 28.8\% mAP. We only retain the first pooling and remove the rest two pooling operators, and obtain 28.8\% mAP. Without multiscale pooling, the architecture achieves 28.0\% mAP on balanced AudioSet. The accuracy gap between fewer numbers of pooling operators and MAST is probably because the multiscale pooling benefits the learning of MAST for audio classification.

To further understand the representations learned from MAST and AST, we employ UMAP~\cite{mcinnes2018umap} to visualize the classification tokens in the second last layer. To clearly visualize the representations, we only use the test set of the first 30 classes in VGGSound. For UMAP, we use the default hyperparameters,~\ie, the number of neighbors of 15 and the minimal distance of 0.1. From Fig.~\ref{fig:umap}, MAST can learn semantically more separable representations than AST from the circled categories. We further calculate the statistic metric based on the clustering for the representations on the VGGSound full test set in table~\ref{tab:vgg_feat_ablation}. We utilize average silhouette~\cite{rousseeuw1987silhouettes}, adjusted rand index~\cite{hubert1985comparing} and homogeneity score~\cite{rosenberg2007v} in Scikit-learn package, and MAST achieves the best scores based on all the three metrics.
Our MAST learns a compact and discriminative representation.
\section{Conclusion}
\label{sec:conclusion}
In this work, we have presented a multiscale Transformer for audio classification, named multiscale audio spectrum Transformer (MAST). MAST learns hierarchical representations from dense and simple to coarse and complex. It outperforms AST by a large margin on Kinetics-Sounds, Epic-Kitechens-100 and VGGSound. On the downloaded AudioSet, MAST achieves a slightly better mAP than AST, with only 24\% MACs and 58\% parameter numbers. MAST is efficient, light-weighted and high accurate, and it can be utilized as an essential building component for other applications,~\eg, multimodal classification.

% These guidelines include complete descriptions of the fonts, spacing, and
% related information for producing your proceedings manuscripts. Please follow
% them and if you have any questions, direct them to Conference Management
% Services, Inc.: Phone +1-979-846-6800 or email
% to \\\texttt{papers@2021.ieeeicassp.org}.

% \section{Formatting your paper}
% \label{sec:format}

% All printed material, including text, illustrations, and charts, must be kept
% within a print area of 7 inches (178 mm) wide by 9 inches (229 mm) high. Do
% not write or print anything outside the print area. The top margin must be 1
% inch (25 mm), except for the title page, and the left margin must be 0.75 inch
% (19 mm).  All {\it text} must be in a two-column format. Columns are to be 3.39
% inches (86 mm) wide, with a 0.24 inch (6 mm) space between them. Text must be
% fully justified.

% \section{PAGE TITLE SECTION}
% \label{sec:pagestyle}

% The paper title (on the first page) should begin 1.38 inches (35 mm) from the
% top edge of the page, centered, completely capitalized, and in Times 14-point,
% boldface type.  The authors' name(s) and affiliation(s) appear below the title
% in capital and lower case letters.  Papers with multiple authors and
% affiliations may require two or more lines for this information. Please note
% that papers should not be submitted blind; include the authors' names on the
% PDF.

% \section{TYPE-STYLE AND FONTS}
% \label{sec:typestyle}

% To achieve the best rendering both in printed proceedings and electronic proceedings, we
% strongly encourage you to use Times-Roman font.  In addition, this will give
% the proceedings a more uniform look.  Use a font that is no smaller than nine
% point type throughout the paper, including figure captions.

% In nine point type font, capital letters are 2 mm high.  {\bf If you use the
% smallest point size, there should be no more than 3.2 lines/cm (8 lines/inch)
% vertically.}  This is a minimum spacing; 2.75 lines/cm (7 lines/inch) will make
% the paper much more readable.  Larger type sizes require correspondingly larger
% vertical spacing.  Please do not double-space your paper.  TrueType or
% Postscript Type 1 fonts are preferred.

% The first paragraph in each section should not be indented, but all the
% following paragraphs within the section should be indented as these paragraphs
% demonstrate.

% \section{MAJOR HEADINGS}
% \label{sec:majhead}

% Major headings, for example, "1. Introduction", should appear in all capital
% letters, bold face if possible, centered in the column, with one blank line
% before, and one blank line after. Use a period (".") after the heading number,
% not a colon.

% \subsection{Subheadings}
% \label{ssec:subhead}

% Subheadings should appear in lower case (initial word capitalized) in
% boldface.  They should start at the left margin on a separate line.
 
% \subsubsection{Sub-subheadings}
% \label{sssec:subsubhead}

% Sub-subheadings, as in this paragraph, are discouraged. However, if you
% must use them, they should appear in lower case (initial word
% capitalized) and start at the left margin on a separate line, with paragraph
% text beginning on the following line.  They should be in italics.

% \section{PRINTING YOUR PAPER}
% \label{sec:print}

% Print your properly formatted text on high-quality, 8.5 x 11-inch white printer
% paper. A4 paper is also acceptable, but please leave the extra 0.5 inch (12 mm)
% empty at the BOTTOM of the page and follow the top and left margins as
% specified.  If the last page of your paper is only partially filled, arrange
% the columns so that they are evenly balanced if possible, rather than having
% one long column.

% In LaTeX, to start a new column (but not a new page) and help balance the
% last-page column lengths, you can use the command ``$\backslash$pagebreak'' as
% demonstrated on this page (see the LaTeX source below).

% \section{PAGE NUMBERING}
% \label{sec:page}

% Please do {\bf not} paginate your paper.  Page numbers, session numbers, and
% conference identification will be inserted when the paper is included in the
% proceedings.

% \section{ILLUSTRATIONS, GRAPHS, AND PHOTOGRAPHS}
% \label{sec:illust}

% Illustrations must appear within the designated margins.  They may span the two
% columns.  If possible, position illustrations at the top of columns, rather
% than in the middle or at the bottom.  Caption and number every illustration.
% All halftone illustrations must be clear black and white prints.  Colors may be
% used, but they should be selected so as to be readable when printed on a
% black-only printer.

% Since there are many ways, often incompatible, of including images (e.g., with
% experimental results) in a LaTeX document, below is an example of how to do
% this \cite{Lamp86}.

% \section{FOOTNOTES}
% \label{sec:foot}

% Use footnotes sparingly (or not at all!) and place them at the bottom of the
% column on the page on which they are referenced. Use Times 9-point type,
% single-spaced. To help your readers, avoid using footnotes altogether and
% include necessary peripheral observations in the text (within parentheses, if
% you prefer, as in this sentence).

% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------
% \begin{figure}[htb]

% \begin{minipage}[b]{1.0\linewidth}
%   \centering
%   \centerline{\includegraphics[width=8.5cm]{image1}}
% %  \vspace{2.0cm}
%   \centerline{(a) Result 1}\medskip
% \end{minipage}
% %
% \begin{minipage}[b]{.48\linewidth}
%   \centering
%   \centerline{\includegraphics[width=4.0cm]{image3}}
% %  \vspace{1.5cm}
%   \centerline{(b) Results 3}\medskip
% \end{minipage}
% \hfill
% \begin{minipage}[b]{0.48\linewidth}
%   \centering
%   \centerline{\includegraphics[width=4.0cm]{image4}}
% %  \vspace{1.5cm}
%   \centerline{(c) Result 4}\medskip
% \end{minipage}
% %
% \caption{Example of placing a figure with experimental results.}
% \label{fig:res}
% %
% \end{figure}


% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak

% \section{COPYRIGHT FORMS}
% \label{sec:copyright}

% You must submit your fully completed, signed IEEE electronic copyright release
% form when you submit your paper. We {\bf must} have this form before your paper
% can be published in the proceedings.

% \section{RELATION TO PRIOR WORK}
% \label{sec:prior}

% The text of the paper should contain discussions on how the paper's
% contributions are related to prior work in the field. It is important
% to put new work in  context, to give credit to foundational work, and
% to provide details associated with the previous work that have appeared
% in the literature. This discussion may be a separate, numbered section
% or it may appear elsewhere in the body of the manuscript, but it must
% be present.

% You should differentiate what is new and how your work expands on
% or takes a different path from the prior studies. An example might
% read something to the effect: "The work presented here has focused
% on the formulation of the ABC algorithm, which takes advantage of
% non-uniform time-frequency domain analysis of data. The work by
% Smith and Cohen \cite{Lamp86} considers only fixed time-domain analysis and
% the work by Jones et al \cite{C2} takes a different approach based on
% fixed frequency partitioning. While the present study is related
% to recent approaches in time-frequency analysis [3-5], it capitalizes
% on a new feature space, which was not considered in these earlier
% studies."

% \vfill\pagebreak

% \section{REFERENCES}
% \label{sec:refs}

% List and number all bibliographical references at the end of the
% paper. The references can be numbered in alphabetic order or in
% order of appearance in the document. When referring to them in
% the text, type the corresponding reference number in square
% brackets as shown at the end of this sentence \cite{C2}. An
% additional final page (the fifth page, in most cases) is
% allowed, but must contain only references to the prior
% literature.

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
{\small
\bibliographystyle{IEEEbib}
\bibliography{strings,refs}}

\end{document}
