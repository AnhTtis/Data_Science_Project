\newif\ifArXiV

\ArXiVtrue	%ArXiV style
%\ArXiVfalse %Elsevier style

\ifArXiV
\documentclass[a4paper]{article}
\usepackage[affil-it]{authblk}
\usepackage[authoryear]{natbib}
\newenvironment{frontmatter}{}{}
\newenvironment{keywords}{\small \textbf{Keywords: }}{}
\let\address\affil
\newenvironment{keyword}{\small \textbf{Keywords: }}{}
\else
\documentclass[authoryear,preprint]{elsarticle}     % onecolumn (second format)
\fi
\usepackage{fullpage}

\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{pdflscape}
\usepackage{secdot}
\usepackage{subcaption}
\usepackage{amsmath, amssymb,amsthm}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{enumitem,blindtext}
\usepackage[export]{adjustbox}
\usepackage{mathrsfs}
\usepackage[usenames,dvipsnames]{color}
\usepackage[mathcal]{eucal}
\usepackage{caption}
\usepackage{multirow}
\captionsetup[algorithm]{font=footnotesize}
\usepackage{amsmath,amsfonts}
\usepackage[linesnumbered, noend, ruled]{algorithm2e}
\usepackage{setspace}
\newcommand{\comments}[1]{}
\DeclareGraphicsExtensions{.png,.pdf,.jpg}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{observation}{Observation}
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\usepackage{tikz}
\usepackage{tikzsymbols}
\usetikzlibrary{arrows,automata}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary{positioning}
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}

%\renewcommand{\paragraph}[1]{{\vskip2mm\noindent \bf #1}\hspace{0.4cm}}
% \newcommand{\KT}[1]{{\color{red}#1}} 
%  \newcommand{\MS}[1]{{\color{olive}#1}} 
%  \newcommand{\NA }[1]{{\color{blue}#1}} 
% \definecolor{purple}{RGB}{153,50,204}
%  \newcommand{\EG}[1]{{\color{purple}#1}} 

 \newcommand{\KT}[1]{{\color{black}#1}} 
 \newcommand{\MS}[1]{{\color{black}#1}} 
 \newcommand{\NA }[1]{{\color{black}#1}} 
\newcommand{\EG}[1]{{\color{black}#1}} 

\newcommand{\DSPa}[0]{$\text{DSP1}(\bar{x},\omega)$\xspace}
\newcommand{\SPa}[0]{$\text{SP1}(\bar{x},\omega)$\xspace}
\newcommand{\DSPp}[0]{$\text{DSP2}(\bar{x},\omega)$\xspace}
\newcommand{\DSPL}[0]{$\text{DSP3}(\bar{x},\omega)$\xspace}
\newcommand{\SPp}[0]{$\text{SP2}(\bar{x},\omega)$\xspace}
\newcommand{\Problem}[0]{MBSMP\xspace}
\newcommand{\BC}[0]{BC\xspace}
\newcommand{\ILP}[0]{ILP\xspace}
\newcommand{\BD}[0]{BD\xspace}
\newcommand{\BDs}[0]{BDs\xspace}
\newcommand{\IC}[0]{IC\xspace}


%\renewcommand{\paragraph}[1]{{\vskip0.5mm\noindent \bf #1}\hspace{0.4cm}}
\usepackage[normalem]{ulem}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{xstring}

\usepackage{hyperref}

\newcommand{\mytag}[1]{(\hypertarget{#1}{\mathrm{#1}})}
\newcommand{\myref}[1]{\textnormal{(\hyperlink{#1}{#1})}}

\begin{document}

\begin{frontmatter}

\title{Benders decomposition algorithms for minimizing the spread of harmful contagions in networks}

\ifArXiV
\author[1]{K\"ubra Tan{\i}nm{\i}\c{s}\thanks{ktaninmis@ku.edu.tr}}
\author[2]{Necati Aras\thanks{arasn@boun.edu.tr}}
\author[3]{Evren G\"uney\thanks{guneye@mef.edu.tr}}
\author[4]{Markus Sinnl\thanks{markus.sinnl@jku.at}}
\date{}
\else
\author[1]{K\"ubra Tan{\i}nm{\i}\c{s}}
\ead{ktaninmis@ku.edu.tr}
\author[2]{Necati Aras}
\ead{arasn@boun.edu.tr}
\author[3]{Evren G\"uney}
\ead{guneye@mef.edu.tr}
\author[4]{Markus Sinnl}
\ead{markus.sinnl@jku.at}
\fi


%\author{Other Author 3}
\address[1]{Department of Industrial Engineering, Ko\c{c} University, \.{I}stanbul, Turkey }
\address[2]{Department of Industrial Engineering, Bo\u{g}azi\c{c}i University, \.{I}stanbul, Turkey }
\address[3]{Department of Industrial Engineering, MEF University, Turkey }
\address[4]{Institute of Production and Logistics Management/JKU Business School, Johannes Kepler University Linz, Austria}
\ifArXiV
\maketitle	
\fi
\begin{abstract}

The COVID-19 pandemic has been a recent example for the spread of a harmful contagion in large populations. Moreover, the spread of harmful contagions is not only restricted to an infectious disease, but \NA {is also relevant to} computer viruses and malware in computer networks. Furthermore, the spread of fake news and propaganda in online social networks is also of \MS{major} concern. In this study, we introduce the measure-based spread minimization problem (\Problem), which can help policy makers in minimizing the spread of harmful contagions in large networks. We develop exact solution methods based on branch-and-Benders-cut algorithms that make use of the application of Benders decomposition method to two different mixed-integer programming formulations of the \Problem: an arc-based formulation and a path-based formulation. We show that for both formulations the Benders optimality cuts can be generated using a combinatorial procedure rather than solving the dual subproblems using linear programming. Additional improvements such as using scenario-dependent extended seed sets, initial cuts, and a starting heuristic are also incorporated into our branch-and-Benders-cut algorithms. We investigate the contribution of various components of the solution algorithms to the performance on the basis of computational results obtained on a set of instances derived from existing ones in the literature.
\end{abstract}

\ifArXiV

\begin{keyword}
Combinatorial optimization, Benders decomposition, stochastic optimization, spread minimization
\end{keyword}
\else

\begin{keyword}
Combinatorial optimization \sep Benders decomposition \sep stochastic optimization \sep spread minimization
\end{keyword}
\fi

\end{frontmatter}


\section{Introduction and motivation}

The COVID-19 pandemic is a somber reminder of the danger of the spread of harmful contagions in networks. It has infected over 759 million people and caused the death of almost 6.8 million people\footnote{according to \url{https://covid19.who.int/}, accessed on March 18, 2023}. The pandemic also caused serious economic damage, see, e.g., \cite{magistretti2021after}. Moreover, the spread of harmful contagions is not only restricted to an infectious disease, but can also occur as computer viruses and malware in computer networks \citep{zimba2019economic}. Furthermore, the spread of fake news and propaganda in online social networks is also a very pressing issue \citep{lazer2018science}. 

In this work, we introduce the \emph{measure-based spread minimization problem} (\Problem), which can be used to model the way how to optimally minimize the spread of harmful contagions in networks. Suppose that we are given a directed graph $G=(V,A)$ representing a network, a stochastic diffusion model for the spread in the network, and a set of initially infected nodes $I \subset V$ from which the spread of the contagion starts in the network according to the diffusion model. We also have a set of labels $K$ for the arcs, each of which represents a certain relationship (contact) type. While there can be parallel arcs between a node pair, each arc is categorized with exactly one label. Blocking a label means taking a measure that prevents the corresponding contact between every pair of nodes connected via an arc having that label. In other words, there is a measure associated with each label, and taking a measure implies removing the arcs with the corresponding label from the network. In the context of disease spread, some possible measures could be closing of schools, closing of department stores, closing of restaurants, and the lock-down of a certain area. The goal is to take measures (i.e., remove sets of arcs) in such a way that the spread of the harmful contagion is minimized while respecting a given budget for taking the measures. In this work, we focus on the \emph{independent cascade model (\IC)} as the stochastic diffusion model. This is a popular diffusion model which has been used for many different optimization problems related to spread or influence maximization/minimization in the context of (social) networks. In the \IC model, each infected node has a ``one-shot'' chance to spread the contagion. We discuss the \IC model in more detail in Section \ref{sec:deftheory}, where we also give a formal definition of the \Problem.

The \Problem is related to the \emph{spread blocking problems} considered in \cite{kuhlman2013blocking} \NA{among others}. In \cite{kuhlman2013blocking}, the authors consider a deterministic diffusion model and define various problems based on different objective functions. Next to minimizing the spread, they also consider an objective of saving all non-seed nodes from infection while minimizing the cost for the needed blocking-actions. Moreover, some of the problems consider blocking of edges, while others consider \MS{blocking of} labels. The authors prove the NP-hardness for some of the resulting problems, while for other variants they show that they can be solved in polynomial time. For the NP-hard problems, \cite{kuhlman2013blocking} present simple heuristics. Similar heuristics for other variants of spread-blocking problems are also developed for other problem variants based on edge/node deletion in deterministic networks \citep{eubank2006structure,kuhlman2010finding, enns2015link, kimura2009blocking}. In \citet{gillen2018critical} a deterministic edge-based spread-blocking problem is studied and integer linear programming (\ILP) approaches to solve the problem to proven optimality are presented. The \Problem extends the existing state-of-the-art by considering i) a stochastic diffusion model, and ii) label-based blocking of arcs, a combination which, to the best of our knowledge, has not been considered before in literature. Moreover, we present exact solution algorithms, while most of the work in the area focuses on heuristic approaches. A more detailed review on previous related work can be found in Section \ref{sec:prev}.

\paragraph{Contribution and outline}

In this work, we present Benders decomposition-based algorithms for solving the \Problem. We first show that contrary to many other spread-related problems such as the influence maximization problem, the objective function in our problem is neither \emph{supermodular} nor \emph{submodular}. We first present a compact, arc-based \ILP formulation for the problem and describe how to apply Benders decomposition (\BD) to this formulation. Then, we present an alternative, path-based \ILP formulation with an exponential number of constraints, and show how \BD can also be applied to this formulation. We discuss how the latter formulation can be strengthened by down-lifting inequalities present in the \ILP and how this strengthening can be incorporated in the \BD. We show that the Benders optimality cuts in both \BDs can be separated using a combinatorial procedure, i.e., there is no need to solve linear programming subproblems to obtain the optimality cuts.

Based on the presented formulations, we develop Branch-and-Benders-cut (\BC) solution algorithms, where the Benders optimality cuts are generated as needed. We also develop additional improvements to the solution algorithms, which include using i) scenario-dependent extended seed sets, ii) initial cuts, and iii) a starting heuristic. We present a computational study on synthetic instances and network instances from the literature. In the computational study, we analyze the influence of both the various ingredients of our solution algorithm and instance characteristics on the solution performance.

The remainder of the paper is organized as follows: In Section \ref{sec:prev} we provide a discussion of previous and related work. In Section \ref{sec:deftheory} we give a formal problem definition, including a discussion of the \IC used to model the diffusion. We also show that the objective function in our problem is neither \emph{supermodular} nor \emph{submodular}. Section \ref{sec:arc-based} contains the compact arc-based formulation and the associated \BD method and Section \ref{sec:path-based} contains the path-based formulation and the associated \BD method. In this section we also describe how to down-lift the constraints of the path-based formulation, discuss the relationship to the compact formulation from the previous section, and show that the Benders optimality cuts can be separated in a combinatorial fashion. The details of our \BC algorithms are explained in Section \ref{sec:algorithmic_details}, including a description of our proposed enhancements to the algorithms and implementation details of the cut separation. Computational results are provided in Section \ref{sec:results}. We conclude the paper in Section \ref{sec:conclusion} with some remarks and suggestions for future research.

\section{Previous and related work} \label{sec:prev}

%\KT{TODO: @all: check the recent papers}
In the literature there exist several studies which model the spread of a disease in a population using a network approach. Under this approach, spread reduction measures such as the isolation of people/regions and preventing the contacts among them are modeled by the removal of certain edges and/or nodes from the network. A class of studies such as \cite{eubank2006structure}, \citet{kuhlman2010finding}, \citet{kuhlman2013blocking}, \cite{wang2013negative}, and \cite{ju2021node} focus on the minimization of disease spread via edge/node deletion and propose greedy algorithms or heuristic methods while considering a deterministic model for the spread of the contagion. Some of these methods are simply based on the centrality measures of the nodes in the network. In \cite{enns2015link}, several heuristics for minimizing the spread of diseases in networks are surveyed and compared, while a heuristic for vaccination in networks is proposed in \cite{holme2004efficient}. In \cite{kimura2009blocking} another heuristic method is considered for blocking links in networks using a ``contamination degree" as the criterion. Interestingly, these heuristic studies often give different recommendations regarding which strategies perform the best, which is caused by the fact that heuristics as opposed to optimal solution algorithms are employed, and different underlying assumptions about the network are made. 

In \cite{gillen2018critical}, the authors study which arcs to remove to minimize the spread in a deterministic linear threshold diffusion model and present \ILP approaches for their problem. Another deterministic node-deletion problem motivated by the spread of influenza-virus is investigated in \cite{charkhgard2018integer}, but the developed \ILP model which is obtained by linearizing a nonlinear formulation of the problem can only handle small-scale problem instances with up to 200 nodes. In \cite{gillen2021fortification}, a robust version of a node-deletion problem is considered and \ILP models are presented for its solution. In another class of such problems, the connectivity of the network is reduced instead of directly minimizing the disease/influence spread. This is the case in \cite{shen2012exact} where node deletions are used to achieve this goal. A similar problem is also tackled with \ILP approaches and heuristics in \cite{arulselvan2009detecting}. Although reducing connectivity could eventually help to reduce the spread, it is not a true measure of the spread. \cite{nandi2016methods} define two ``spread related" deterministic metrics and develop \ILP formulations to optimize these metrics through link removal. A recent survey about node-deletion problems is given by \cite{lalou2018critical}. In \cite{taninmics2022improved}, a bilevel stochastic spread-blocking problem based on node-deletion is considered and solved using \ILP.

In the seminal work of \cite{kempe2003maximizing}, the \emph{influence maximization problem (IMP)} is introduced to study the effect of viral marketing. In the IMP, we are given a network $G=(V,A)$ with a stochastic diffusion model and an integer number $k$. It involves finding a seed set $S$ of size $k$ in $V$ to start the stochastic diffusion process so as to maximize the expected number of influenced nodes when the process ends. It is an NP-hard problem under several widely used stochastic diffusion models. The work of \citet{kempe2003maximizing} spurred a flurry of activity in the research community, which resulted in the study of many different variants of the problem. In recent years, exact and heuristic solution approaches have emerged to solve variants of the IMP that occur in the analysis of social networks.

The surveys \citet{sun2011survey}, \cite{li2018influence}, and \cite{banerjee2020survey} provide an overview on the developments of heuristic solution approaches in the IMP. There is a less amount of work on solving this problem exactly. In \cite{wu2018two} an \ILP-based approach which exploits the submodularity of the objective function is proposed. \cite{guney2019optimal} considers a Lagrangian relaxation approach based on an \ILP-formulation for the IMP, while \cite{guney2021large} develop a \BD algorithm to solve the same problem. A robust version of the IMP is introduced and tackled by \ILP approaches in \cite{nannicini2020exact}. In the \emph{least cost-influence maximization problem}, the goal is to find the minimum number of seeds to influence the complete network. Exact \ILP approaches for this problem were designed in \citet{fischetti2018least} and \citet{gunnecc2020branch}. The \emph{weighted target set selection problem} studied in \citet{raghavan2019branch} can be seen as a deterministic variant of the IMP in which the spread is a function of the number of influenced individuals at the previous time step rather than being dependent on the spread probability. In the context of social networks, interdiction-type problems have been considered recently, focusing on interdicting \emph{cliques} or clique-like structures. For example, \citet{furini2019maximum} are concerned with removing nodes as to minimize the maximal clique in the remaining graph, while \citet{furini2021branch} study the same problem with removing edges. The weighted version of such problems are studied in \citet{nasirian2019exact} and \cite{pajouh2019minimum}. However, while these problems make sense in the area of social network analysis, their applicability in preventing the spread of harmful contagions in networks is rather limited, as the resulting networks after interdiction can still be highly connected. 

\section{Problem definition and theoretical results \label{sec:deftheory}}

In this section, we first give an overview on stochastic diffusion models and show how we use the \IC model in the context of our problem. Next, we present a formal problem definition of the \Problem. We then show that contrary to many other influence-spread problems in networks, the objective function of the \Problem is neither submodular nor supermodular.

\subsection{Stochastic diffusion models \label{sec:diff}}
A stochastic diffusion model allows to model the spread of harmful contagions in networks with probabilistic infection between nodes. The diffusion process proceeds in time steps, and at each time step additional nodes may get infected. There exist two stochastic diffusion model types frequently considered in the literature: \NA{\emph{cascade models} and \emph{threshold models}} (see, e.g., \citet{kempe2003maximizing}, \citet{kempe2015maximizing}, \citet{wu2018two}, \citet{han2018influence}, \citet{taninmics2019influence}, \citet{guney2021large}, \citet{kahr2021benders}, and \citet{taninmics2022improved}). In cascade models, each arc $(i,j) \in A$ is assigned a probability $p_{ij}$ which represents the chance that node $i$ is successful in infecting node $j$. Each node $i$ has a single attempt at activating (infecting) node $j$ during the diffusion process, which occurs immediately after node $i$ gets activated itself. In threshold models each node is assigned a (probabilistic) threshold value and each arc $(i,j)$ is assigned a certain influence value. A node $j$ gets infected during the diffusion process if the sum of the influence values on the arcs incoming from the already infected neighboring nodes exceeds the threshold value of node $j$.

In this work, we use a special case of cascade models that has received considerable attention in the literature, namely the \IC model in which each activation probability is constant. Following other works in the literature (see, e.g., \citet{kempe2003maximizing}, \citet{kempe2015maximizing}, \citet{wu2018two}, \citet{guney2021large}, and \citet{kahr2021benders}) we use a discrete set of scenarios $\Omega$ to model the stochastic diffusion process caused by the \IC model. Each scenario $\omega \in \Omega$ can then be represented using a so-called \emph{live-arc} graph $G^\omega=(V,A^\omega)$, where the presence of $(i,j) \in A^\omega$ indicates that node $i$ is successful in activating node $j$ in scenario $\omega$. This result follows from the equivalence between the IC and \emph{triggering set} models as shown by \citet{kempe2003maximizing}. The set $\Omega$ is obtained by Monte-Carlo sampling based on the given probabilities $p_{ij}$ for the arcs $(i,j) \in A$, i.e., for each scenario $\omega \in \Omega$, $(i,j) \in A^\omega$ is determined by a (biased) coin flip according to probability $p_{ij}$.

Let $A_k$ be the set of arcs with label $k \in K$. Given a seed set $I \subset V$ and a set $K'\subset K$ of blocked labels, the quantity  $\Phi^\omega(I,K')$ represents the number of nodes affected by the harmful contagion in scenario $\omega$. Formally, it is defined as
\begin{equation}
  \Phi^\omega(I,K')=\big | \{v \in V: \exists \text{ a path from at least one node }i \in I \text{ to } v \text{ when considering } A^\omega\setminus \cup_{k \in K'} A_k \} \big |, \tag{IC-S-$\omega$} \label{eq:spread}
\end{equation} i.e., the cardinality of the set of nodes which can be reached from the seed set $I$ using the non-blocked live-arcs for the considered scenario. We note that \citet{kempe2003maximizing} showed that the \emph{linear threshold} model which is a special case of threshold models can also be represented using live-arc graphs.


\subsection{Problem definition\label{sec:def}}
A formal definition of the \Problem using the \IC model with a discrete set of scenarios is given in the following. In the remainder of this paper, we consider this variant of the \Problem, and for the ease of readability, we keep the abbreviation \Problem.

\begin{definition}[Measure-based spread minimization problem (\Problem)]
Let $G=(V,A)$ be a directed graph and $K$ be a finite set of labels. For each $k \in K$, we are given a \emph{measure cost} $c_k \geq 0$, and an arc set $A_k$, i.e., arcs having the label $k$, that are disjoint and satisfy $\cup_k A_k =A$. We are also given a set of initially infected nodes $I\subset V$, and probabilities $p_{ij}$ for each $(i,j) \in A$ indicating the probability that node $i$ is successful in activating (influencing) node $j$. Let $\Omega$ be a set of scenarios obtained by using Monte-Carlo sampling based on probabilities $p_{ij}$. Let $\Phi^\omega(I,K')$ denote the number of nodes to which the harmful contagion spreads in scenario $\omega$ for the seed set $I\subset V$ and a set of blocked labels $K'\subset K$ as defined in equation \eqref{eq:spread}. The \emph{measure-based spread minimization problem} consists of finding a set of measures to take (labels to block) within a budget $B$ so that the expected number of nodes where the harmful contagion has spread from the given seed set $I$ is minimized.
%\footnote{\KT{to me the last part sounds like the number of seed nodes}}. 
Mathematically, it is defined as
		\begin{equation}
		\min_{K' \subset K: c(K^\prime)\leq B} f(K')=\sum_{\omega \in \Omega} \frac{1}{|\Omega|} \Phi^\omega(I,K') \tag{P} \label{eq:P}
		\end{equation}
		where $c(K')=\sum_{k \in K'} c_k\leq B$ is the budget constraint.
\end{definition}

\begin{remark}
The graph $G=(V,A)$ can have parallel arcs with the same head and tail nodes with different labels. Suppose that there are $p$ arcs from $i\in V$ to $j \in V$ with labels $k_1,\ldots, k_p$. Thus, node $i$ can spread the contagion to node $j$ via one of the $p$ arcs, i.e., contact types, independently. In order to prevent $i$ from spreading the contagion to $j$ in a scenario, each of the labels $k_1,\ldots, k_p$ of the live-arcs in that scenario needs to be blocked.
\label{remark:multiple}
\end{remark}

The \Problem is a generalization of the simple contagion variant of the \emph{small weighted critical edge set problem (SWCES)}, which is an NP-hard problem studied in \cite{eubank2006structure} and \cite{kuhlman2013blocking}. In the SWCES a deterministic diffusion model is considered.  Moreover, there are no edge labels and each edge can only be directly blocked, i.e., each edge has a unique label.

In Figure \ref{fig:problem}, a small instance of the \Problem on a graph with six nodes is given, together with a solution. In this instance $I=\{1,4\}$, $|K|=4$, $c_k=1$, for all $k \in K$, $B=1$ and $|\Omega|=3$. The arc-labels are given as numbers next to the arcs in the figure. In the considered solution, label $2$ is blocked, i.e., $K'=\{2\}$, which is indicated with a dashed line for the arcs with this label in the live-arc graphs for the scenarios. The objective values for the scenarios are $\Phi^1(I,K')=5$ (nodes $1,3,4,5,6$ can be reached), $\Phi^2(I,K')=4$ (nodes $1,3,4,6$ can be reached), $\Phi^3(I,K')=5$ (nodes $1,2,3,4,5$ can be reached), resulting in $f(K')=\frac{1}{3} \cdot 5+\frac{1}{3} \cdot 4+\frac{1}{3} \cdot 5=\frac{14}{3}=4.66$. The optimal solution for this instance is to block label $0$. For this optimal solution, we have $\Phi^1(I,K')=4$ (nodes $1,4,5,6$ can be reached), $\Phi^2(I,K')=3$ (nodes $1,4,6$ can be reached), $\Phi^3(I,K')=4$ (nodes $1,2,4,5$ can be reached) resulting in $f(K')=3.66$

\begin{figure}[h!tb]
\begin{subfigure}{0.5\textwidth}
\centering
 \fbox{
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,
                    thick, main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
\tikzstyle{every state}=[fill=white,draw=black,text=black,scale=0.9]

  \node[state,fill=black!20!white] 		   (n1)             		  {1};
  \node[state]         (n2) [above right of=n1] 	  {2};
  \node[state]         (n3) [below right of=n1]       {3};
  \node[state]         (n5) [right of=n2] {5};
  \node[state]         (n0) [right of=n3] {6};
  \node[state,fill=black!20!white]         (n4) [below right of=n5] {4};
  \path (n1) edge[bend left]			  node {1} (n2)
            edge            node[xshift=-3pt] {0} (n3)    
        (n2) edge [bend left]			  node[above, yshift=2pt]  {3} (n1)
        	edge			  node {0} (n5)
        (n3)	edge	   node[left] {2} (n2)
         edge              node {0} (n4)
         edge              node[below] {2} (n0)
        (n4) edge 			  node[xshift=3pt] {1} (n5)
    	    edge			  node[below] {3} (n0)
        ;
\end{tikzpicture}
}
\caption{Instance graph}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\centering
 \fbox{
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,
                    thick, main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
\tikzstyle{every state}=[fill=white,draw=black,text=black,scale=0.9]

  \node[state,fill=black!20!white] 		   (n1)             		  {1};
  \node[state]         (n2) [above right of=n1] 	  {2};
  \node[state,fill=black!50!white]         (n3) [below right of=n1]       {3};
  \node[state,fill=black!50!white]         (n5) [right of=n2] {5};
  \node[state,fill=black!50!white]         (n0) [right of=n3] {6};
  \node[state,fill=black!20!white]         (n4) [below right of=n5] {4};
  \path (n1)  %edge[bend left]			  node {1} (n2)
        edge            node[xshift=-3pt] {0} (n3)    
        (n2) %edge [bend left]			  node[above, yshift=2pt]  {3} (n1)
        	edge  			  node {0} (n5)
        (n3)	edge [dashed]	   node[left] {2} (n2)
         %edge              node {0} (n4)
         edge     [dashed]         node[below] {2} (n0)
        (n4) edge 			  node[xshift=3pt] {1} (n5)
    	    edge			  node[below] {3} (n0)
        ;
\end{tikzpicture}
}
\caption{Scenario 1 \label{fig:example_s1}}
\end{subfigure}
\vspace*{10pt}
\begin{subfigure}{0.5\textwidth}
\centering
 \fbox{
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,
                    thick, main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
\tikzstyle{every state}=[fill=white,draw=black,text=black,scale=0.9]

  \node[state,fill=black!20!white] 		   (n1)             		  {1};
  \node[state]         (n2) [above right of=n1] 	  {2};
  \node[state,fill=black!50!white]         (n3) [below right of=n1]       {3};
  \node[state]         (n5) [right of=n2] {5};
  \node[state,fill=black!50!white]         (n0) [right of=n3] {6};
  \node[state,fill=black!20!white]         (n4) [below right of=n5] {4};
  \path (n1) %edge[bend left]			  node {1} (n2)
            edge            node[xshift=-3pt] {0} (n3)    
        (n2) edge [bend left]			  node[above, yshift=2pt]  {3} (n1)
        	edge			  node {0} (n5)
        (n3)	edge[dashed]	   node[left] {2} (n2)
         edge               node {0} (n4)
        % edge              node[below] {2} (n0)
        (n4) %edge 			  node[xshift=3pt] {1} (n5)
    	    edge			  node[below] {3} (n0)
        ;
\end{tikzpicture}
}
\caption{Scenario 2 \label{fig:example_s2}}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\centering
 \fbox{
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,
                    thick, main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
\tikzstyle{every state}=[fill=white,draw=black,text=black,scale=0.9]

  \node[state,fill=black!20!white] 		   (n1)             		  {1};
  \node[state,fill=black!50!white]         (n2) [above right of=n1] 	  {2};
  \node[state,fill=black!50!white]         (n3) [below right of=n1]       {3};
  \node[state,fill=black!50!white]         (n5) [right of=n2] {5};
  \node[state]         (n0) [right of=n3] {6};
  \node[state,fill=black!20!white]         (n4) [below right of=n5] {4};
  \path (n1) edge[bend left]			  node {1} (n2)
            edge            node[xshift=-3pt] {0} (n3)    
        (n2)% edge [bend left]			  node[above, yshift=2pt]  {3} (n1)
        	edge 			  node {0} (n5)
        (n3)	edge [dashed]	   node[left] {2} (n2)
         %edge              node {0} (n4)
         edge      [dashed]        node[below] {2} (n0)
        (n4) edge 			  node[xshift=3pt] {1} (n5)
    	    %edge			  node[below] {3} (n0)
        ;
\end{tikzpicture}
}
\caption{Scenario 3 \label{fig:example_s3}}
\end{subfigure}
\caption{An instance of \Problem with six nodes and four arc labels, \MS{seed set} $I=\{1,4\}$ (shown in light grey) and $|\Omega|=3$. For the scenarios, the live-arcs are shown in \ref{fig:example_s1}, \ref{fig:example_s2} and \ref{fig:example_s3}. The budget allows to block at most one label. The solution $K'=\{2\}$ is illustrated, with the blocked arcs shown as dashed arrows. The nodes reached by the spread in each scenario (excluding the seed set) are displayed in dark grey.}\label{fig:problem}
 
\end{figure}	

An example of how blocking labels affects the final spread is displayed in Figure \ref{fig:HepPh} on a collaboration network instance whose details are explained in Section \ref{sec:results}.
\begin{figure}[h!tb]
    \centering 
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\linewidth]{Figures/unblocked.pdf}
        \caption{No blocking}
        \label{fig:HepPh_initial}
    \end{subfigure}\hspace{5pt}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\linewidth]{Figures/blocked.pdf}
        \caption{Four labels blocked}
        \label{fig:HepPh_final}
    \end{subfigure}
    \caption{The final spread on HepPh collaboration network with 12,008 nodes and 118,521 edges, (a) when no labels are blocked and (b) when four labels are blocked. 
    There are 50 seed nodes, 50 diffusion scenarios and 21 labels. The filling of the nodes depends on the number of scenarios each node is activated in. The darkest blue filling corresponds to the seed nodes, i.e., nodes which are activated in each of 50 scenarios. Arcs with blocked labels are removed from the graph in (b).} \label{fig:HepPh}
\end{figure}



\begin{theorem}
The objective function $f(K')$ of \eqref{eq:P} is neither submodular nor supermodular.
\end{theorem}

\begin{proof}
We show this by means of a counterexample given in Figure \ref{fig:example} where arc labels are shown next to the arcs. We assume that all the influence probabilities being equal to one, i.e., there is only one possible live-arc scenario, and all the arcs are live in that scenario. In this special case, it is enough to evaluate the number of nodes reachable from the seed set on the original graph to compute the expected spread.

\begin{figure}[h!tb]
\centering
  \fbox{
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,
                    thick, main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
\tikzstyle{every state}=[fill=white,draw=black,text=black,scale=0.9]

  \node[state,fill=black!20!white] 		   (n1)             		  {1};
  \node[state]         (n2) [above right of=n1] 	  {2};
  \node[state]         (n3) [below right of=n1]       {3};
  \node[state]         (n5) [right of=n2] {5};
  \node[state]         (n0) [right of=n3] {6};
  \node[state,fill=black!20!white]         (n4) [below right of=n5] {4};
  \path (n1) edge[bend left]			  node {1} (n2)
            edge            node[xshift=-3pt] {0} (n3)    
        (n2) edge [bend left]			  node[above, yshift=2pt]  {3} (n1)
        	edge			  node {0} (n5)
        (n3)	edge	   node[left] {2} (n2)
         edge              node {0} (n4)
         edge              node[below] {2} (n0)
        (n4) edge 			  node[xshift=3pt] {1} (n5)
    	    edge			  node[below] {3} (n0)
        ;
\end{tikzpicture}
}
\caption{A small directed network with four labels.}\label{fig:example}
\end{figure}

%
Let $I=\{1,4\}$ be the seed set. For $f$ to be submodular, it needs to satisfy $f(X\cup \{i\})- f(X)\geq f(Y\cup \{i\})- f(Y)$ for all $X\subseteq Y \subseteq K$ and $i\in K\setminus Y$. For supermodularity $-f$ should be submodular. 
As a counterexample, consider the sets $X=\{3\}$ and $Y=\{2,3\}$. It is easy to compute $f(X)=6$ and $f(Y)=5$. If we choose $i=1$, then we get $f(X\cup \{i\})=f(\{3,1\})=6$ and $f(Y\cup \{i\})=f(\{2,3,1\})=3$, which violates the inequality we need for supermodularity. Now, suppose that we choose $i=6$ which yields $f(X\cup \{i\})=f(\{3,6\})=4$ and $f(Y\cup \{i\})=f(\{2,3,6\})=4$. These numbers violate the submodular inequality. We conclude that $f$ is neither submodular nor supermodular, as the requirements are not satisfied in the \MS{given counterexample}.
\end{proof}


\section{An arc-based compact formulation} \label{sec:arc-based}

In this section, we first present an arc-based compact formulation, and then show how \BD can be applied to this formulation. 

\subsection{The compact formulation}
%Recall that each arc in the directed graph has exactly one label, whereas it is possible to have \MS{parallel} arcs with different labels between a pair of nodes, and that blocking a label implies the removal of the arcs with the corresponding label from the network. The problem involves finding a set of labels to block within a given amount of budget so that the expected spread is minimized. 

For a scenario $\omega$, let $A^\omega_k$ denote the set of arcs with label $k$ in the live-arc graph. Let binary variable $x_k$, $k\in K$ be one iff label $k$ is blocked. Moreover, let binary variable $y^\omega_i$, $i \in V$, $\omega \in \Omega$ be one iff the spread of the harmful contagion reaches node $i$ in scenario $\omega$. Using this notation, a formulation of the \Problem can be given as follows, which we denote as \myref{ABF}.



\begin{align}
\mytag{ABF} \quad & \min \frac{1}{|\Omega|}\sum_{\omega\in \Omega} \sum_{i\in V} y_i^\omega \tag{ABF.OBJ}\label{mip00}\\
& \text{s.t.} \notag \\
& \hspace{15pt} \sum_{k \in K} c_k x_k \leq B \tag{ABF.BUD}\label{mip01}\\
& \hspace{15pt} y_i^\omega \geq 1 &i&\in I, \omega \in \Omega \tag{ABF.SEED} \label{mip03}\\
& \hspace{15pt}y_j^\omega\geq y_i^\omega -x_{k} &(&i,j) \in A_k^\omega , k\in K, \omega\in \Omega \tag{ABF.SPR} \label{mip02}\\
& \hspace{15pt}x_k\in \{0,1\} &k&\in K \notag\\
& \hspace{15pt}y_i^\omega\in \{0,1\} &i&\in V, \omega \in \Omega \notag
\end{align}

The objective function \eqref{mip00} minimizes the expected spread of the harmful contagion. Constraint \eqref{mip01} ensures that the budget for taking the measures is not exceeded. Constraints \eqref{mip03} set the correct value of the $y$-variables for all the seed nodes (we model this with an inequalitiy instead of an equality to simplify the dualizing-step we need for the BD \NA{formulation} below). Constraints \eqref{mip02} model the spread of the harmful contagion: if in a scenario $\omega$ the contagion has spread to node $i$ and there is an arc between $i$ and $j$ with label $k$, then the contagion will also spread to node $j$ unless the label $k$ is blocked. We note that the number of variables in the formulation \myref{ABF} is in $O(|V||\Omega|)$ and the number of constraints is in $O(|A||\Omega|)$. Thus, while the formulation is compact, for large-scale graphs or a large number of scenarios, the size of the formulation can become prohibitive \NA{to be solved directly}. To deal with this issue, we next present a \BD approach.

\subsection{Benders decomposition formulation}

It is easy to see that in formulation \myref{ABF} the $y$-variables can be relaxed to be continuous (i.e., $y^\omega_i\geq 0$), as long as the $x$-variables are binary. Due to the constraints \eqref{mip02} and the minimization-objective, the $y$-variables will also take binary values in a solution, \KT{for a fixed binary $x$}. This allows us to apply a scenario-based \BD to formulation \myref{ABF}. Let $\theta^\omega$, $\omega \in \Omega$ be a continuous variable to measure the spread of the harmful contagion in scenario $\omega$. The \BD reformulation of \myref{ABF} is given as follows.

\begin{align}
& \mytag{BEN} \quad \min\frac{1}{|\Omega|} \sum_{\omega\in \Omega} \theta^\omega \tag{BEN.OBJ} \label{eq:BMP_obj}\\
& \text{s.t.} \notag \\
& \hspace{15pt} \sum_{k \in K} c_k x_k \leq B \tag{BEN.BUD} \label{eq:BMP_c1}\\
& \hspace{15pt} \theta^\omega \geq \Theta^\omega(x) &\omega &\in \Omega \tag{BEN.OBJ2} \label{eq:BMP_c2}\\
& \hspace{15pt}x_k\in \{0,1\} &k&\in K \notag
\end{align}

Inequalities \eqref{eq:BMP_c2} provide a lower bound on the number of nodes which are reached by the spread of the harmful contagion for a given scenario $\omega \in \Omega$ and any $x \in \{0,1\}^K$. Benders optimality cuts \NA{can be derived} to model the function $\Theta^\omega(x)$ for each $\omega \in \Omega$ and a fixed $\bar x$, using linear programming (LP) duality. Note that for any fixed solution $\bar x$ of \myref{BEN}, the remaining subproblem in $y$ is always feasible, thus no Benders feasibility cuts are needed.  
Let $\alpha_i^\omega$ and $\beta_a^\omega$ be the dual variables associated with constraints \eqref{mip03} and \eqref{mip02}. Let $\delta^+_\omega(i)$ and $\delta^-_\omega(i)$, respectively, be the set of outgoing and incoming arcs of node $i$ in the live-arc graph of scenario $\omega$. We obtain the following dual for a particular scenario $\omega \in \Omega$.
%
\begin{align}
\mytag{DSABF} \quad & \max \sum_{i\in I} \alpha_i^\omega - \sum_{k\in K} \sum_{a\in A_k^\omega} \beta_a^\omega \bar{x}_{k} \label{eq:DSPr_obj} \tag{DSABF.OBJ}\\
& \text{s.t.} \notag \\
& \hspace{15pt} \alpha_i^\omega - \sum_{a\in \delta^{+}_\omega(i)} \beta_a^\omega   + \sum_{a\in \delta^{-}_\omega(i)} \beta_a^\omega \leq 1 &i& \in I \label{eq:DSPr_c1} \tag{DSABF.SEED} \\
& \hspace{15pt} - \sum_{a\in \delta^{+}_\omega(i)} \beta_a^\omega   + \sum_{a\in \delta^{-}_\omega(i)} \beta_a^\omega \leq 1 &i&\in V \setminus I \label{eq:DSPr_c2} \tag{DSABF.NONSEED}\\
& \hspace{15pt}\alpha_i^\omega\geq 0 &i&\in I \notag \\
& \hspace{15pt} \beta_a^\omega \geq 0 &a& \in A_k^\omega, k\in K \notag
\end{align}
%where $A_{i\omega}^{+}$ and $A_{i\omega}^{-}$ are, \NA{respectively,} the sets of outgoing and incoming arcs of node $i$ that are live in scenario $\omega$, respectively. 
For a solution $(\hat \alpha^\omega,\hat \beta^\omega)$ of \myref{DSABF}, the resulting Benders optimality cut is
%A single Benders optimality cut is
% \begin{equation}
% \theta \geq \sum_{r\in \Omega}\Big( \sum_{i\in I}  \hat{\alpha}_{ir} - \sum_{a\in A_r} \hat{\beta}_a^\omega x_{k(a)} 
% \Big)
% \end{equation} 
%or
\begin{equation}
\theta^\omega \geq \sum_{i\in I}  \hat{\alpha}_{i}^{\omega} - \sum_{k\in K}\sum_{a\in A_k^\omega} \hat{\beta}_a^\omega x_{k} \label{eq:BendersCut}\tag{BOC.ABF}
\end{equation} 

As \myref{DSABF} is a compact LP formulation, for the given scenario $\omega$ and the value of $\bar{x}$, \eqref{eq:BendersCut} can be separated by solving \myref{DSABF} as an LP. However, as an alternative method, the cuts can also be generated in a combinatorial and exact way, without solving the LP formulation of \myref{DSABF}. Let $G^\omega(\bar{x})$ denote the live-arc graph $G^\omega$ where the length of each arc $(i,j)\in A_k^\omega$ is equal to $\bar{x}_k$ for all $k\in K$. We also define $d_i^\omega(\bar{x})$ as the length of the shortest path from the seed set $I$ to $i$ on $G^\omega(\bar{x})$. In other words, $d_i^\omega(\bar{x})=\min_{j\in I} d_{ji}^\omega(\bar{x})$ where $d_{ji}^\omega(\bar{x})$ is the shortest path from $j \in I$ to $i$ on $G^\omega(\bar{x})$.

\begin{definition}
    Given a solution $\bar{x}$ to \myref{ABF} and a scenario $\omega$, a node $i \in V$ is called \emph{reachable} if $d_i^\omega(\bar{x})<1$ and not reachable otherwise. 
\end{definition}

\begin{definition}
    The path that is associated with the shortest path distance $d_i^\omega(\bar{x})$ from the seed set to the reachable node $i$ is the \emph{activation path} of $i$ and each reachable node has exactly one activation path (ties are broken arbitrarily). 
    \end{definition}


\begin{theorem}
Consider the solution $(\hat{\alpha},\hat{\beta})$ to \myref{DSABF} where $\hat{\alpha}_{i}$ is the number of nodes that are reachable via a path emerging from node $i\in I$, and $\hat{\beta}_a$ is the number of nodes whose activation path contains the arc $a \in A^\omega$. This solution $(\hat{\alpha},\hat{\beta})$ is an optimal solution to \myref{DSABF}. 
\label{prop:DSP_solution}
\end{theorem}

\begin{proof}
See \ref{appendix_proof}.
\end{proof}

We note that the optimal solution $(\hat{\alpha},\hat{\beta})$ satisfying the condition in Theorem \ref{prop:DSP_solution} can be computed using a shortest path algorithm. In the cut obtained, the constant part $\sum_{i\in I} \hat{\alpha}_{i}^{\omega}$ corresponds to the total number of nodes that are fully ($y_i^\omega=1$) or partially ($0<y_i^\omega<1$) activated by $I$ in scenario $\omega$, \NA{when} $x=\bar{x}$. The coefficients $\sum_{a\in A_k^\omega} \hat{\beta}_a^\omega$ of each $x_k$ correspond to the maximum possible reduction in the number of 
%newly activated
active nodes if label $k$ is blocked. Being able to solve the subproblems to obtain Benders optimality cuts in a combinatorial fashion by shortest path computations instead of solving LPs can be very useful. However, as we show in the next section, it is possible to obtain a different formulation, which also allows for such combinatorial computation of Benders optimality cuts, and additionally allows for lifting of these cuts. The drawback of the formulation of the next section is that it has exponentially many constraints. Thus, compared to the compact formulation provided in this section, it is not possible to give it directly to an \ILP solver\MS{\ if one wants to solve the problem without implementing a BD,} \KT{or to solve the Benders subproblems as LPs. We describe how to handle this drawback in the following section.}

\section{A path-based formulation} \label{sec:path-based}

In this section, we first describe the path-based formulation, and then show that the LP-relaxation of this formulation and that of the arc-based formulation of the previous section have the same optimal objective value. Next, we present a pair of valid inequalities which are obtained by down-lifting the constraints of the path-based formulation and show that these inequalities \MS{can} improve the LP-relaxation. We then discuss how to apply \BD to the path-based formulation and show that despite the exponential number of inequalities in the formulation, the Benders optimality cuts (based on the non-lifted inequalities) can be obtained in a combinatorial fashion in polynomial time. Finally, we discuss why the separation of Benders optimality cuts is not straightforward in the case of the path-based formulation with the down-lifted version of the constraints. 

\subsection{Formulation}

The path-based formulation is based on directly modeling the fact that the contagion reaches a node $i \in V$ in a scenario $\omega$ iff there exist an unblocked path from the seed set $I$ %$S$
to this node $i$ in the live-arc graph of the scenario (see equation \eqref{eq:spread} in Section \ref{sec:diff} where this is stated in a cumulative fashion). Let $P^\omega(i)$ denote the set of all paths from the seed set to a given node $i$ in scenario $\omega$. For the ease of notation, we assume that for a seed node $i \in I$ this set contains an empty path (which by definition is unblockable since there is no arc on it).
%We can define an alternative model by summing up constraints \eqref{mip02} for all possible paths from the seed set $I$ to any node $i \in V^\prime$. Let $P^\omega(i)$ denote the set of such paths for a given node $i$ in scenario $\omega$. 
%
Moreover, for a given path $p \in P^\omega(i)$, let $K_p$ be the set of labels that is encountered on $p$ and $h_p(k)$ be the number of times label $k$ occurs on this path, i.e., the number of arcs $(i,j) \in p$ with label $k$. 
%\footnote{\KT{Do we need a different notation than $d$ as we use it for shortest path distances in Section 3.2} \NA{I think so, we can use $h_p(k)$}}
%
We obtain the following formulation:
%For ease of notation let $P^\omega(i)=\emptyset$ for $i \in I$.

\begin{align}
\mytag{PBF} \quad& \min \frac{1}{|\Omega|}\sum_{\omega\in \Omega} \sum_{i\in V} y_i^\omega \tag{PBF.OBJ} \label{nmip00}\\
& \text{s.t.} \notag \\
& \hspace{15pt} \sum_{k \in K} c_k x_k \leq B \label{nmip01} \tag{PBF.BUD}\\
& \hspace{15pt}y_i^\omega\geq 1 -\sum_{k \in K_p}h_p(k) x_{k} &i&  \in V,  \omega\in \Omega , p \in P^\omega(i) \label{nmip02} \tag{PBF.PATHS} \\
& \hspace{15pt}x_k\in \{0,1\} &k&\in K \notag \\
& \hspace{15pt}y_i^\omega \in \{0,1\} &i&\in V, \omega \in \Omega \notag
\end{align}

The objective function \eqref{nmip00} and the budget constraint \eqref{nmip01} are the same as in the formulation \myref{ABF}. Constraints \eqref{nmip02} ensure that \NA{for a given $\bar x$} the variable $y_i^\omega$ needs to have the value one if there exists at least one unblocked path from the seed set. We note that there can be an exponential number of these inequalities since there can be an exponential number of paths.

Next, we compare the LP-relaxations of \myref{ABF} and \myref{PBF}. In these relaxations, the binary variables $x_k$, $k\in K$ and $y_i^\omega$, $i\in V, \omega \in \Omega$ are relaxed to $0\leq x_k\leq 1$ and $0\leq y_i^\omega\leq 1$.

\begin{theorem}
The optimal objective function values of the LP-relaxations of \myref{ABF} and \myref{PBF} are the same.
\end{theorem}

\begin{proof}
We note that constraints \eqref{nmip02} of the formulation \myref{PBF} can be obtained by summing up constraints \eqref{mip03} and \eqref{mip02} for all possible paths. Aside from these constraints, both formulations are the same. Thus, the feasible region of the LP-relaxation of the formulation \myref{ABF} is at most as large as the feasible region of the LP-relaxation of formulation \myref{PBF}. Consequently, the optimal objective value of the LP-relaxation of the formulation \myref{ABF} is always greater than or equal to the optimal objective value of the LP-relaxation of the formulation \myref{PBF}.

To show equality, we next show that the optimal solution $(\hat x, \hat y)$ of the LP-relaxation of the formulation \myref{PBF} is also feasible for the LP-relaxation of the formulation \myref{ABF}. Suppose this is not the case. This means $(\hat x, \hat y)$ either violates one of the constraints \eqref{mip03} or one of the constraints \eqref{mip02}. We first note that $(\hat x, \hat y)$ cannot violate any of the constraints \eqref{mip03} as they actually are a special case of constraints \eqref{nmip02} for $i \in I$. Thus, suppose $(\hat x, \hat y)$ violates a constraint \eqref{mip02} which would mean $\hat y^\omega_j<\hat y^\omega_i-\hat x_k$ for some $\omega \in \Omega$, $k \in K$, $(i,j) \in A^\omega_k$. Due to $\hat y^\omega_j \geq 0$ this is only possible if $y^\omega_i>0$.  Due to the minimization objective and constraints \eqref{nmip02}, there must be a path $p^* \in P^\omega(i)$ with $\hat y^\omega_i =1 -\sum_{k' \in K_{p^*}}h_{p^*}(k') \hat x_{k'}$. Moreover, the path $q^* \in P^\omega(j)=p^* \cup (i,j)$ must also exist as otherwise there would be no constraint \eqref{mip02} for the considered $\omega \in \Omega$, $k \in K,(i,j) \in A^\omega_k$. For this path it must hold that $\hat y^\omega_j \geq 1 -\sum_{k' \in K_{q^*}}h_{q^*}(k') \hat x_{k'}$ due to constraints \eqref{nmip02}. From this we get $\hat y^\omega_j\geq \hat y^\omega_i-\hat x_k$, which is a contradiction to our assumption $\hat y^\omega_j<\hat y^\omega_i-\hat x_k$. As a consequence the solution $(\hat x, \hat y)$ is feasible, \NA{and thus optimal,} for the LP-relaxation of the formulation \myref{ABF}.

%\footnote{\KT{if the arc is $(i,j)$, $q^*$ should be the longer path}\NA{you are right, what about the switch of $i$ and $j$?}}

\end{proof}

Constraints \eqref{nmip02} can be down-lifted as described in the following theorem.

\begin{theorem}
Given a scenario $\omega \in \Omega$, node $i \in V$ and path $p \in P^\omega(i)$, inequalities
\begin{equation}
    y_i^\omega\geq 1 -\sum_{k \in K_p} x_{k} \tag{PBF.PATHSL}
    \label{nmip02lifted}
\end{equation}
are valid.
\end{theorem}

\begin{proof}
   For each $\omega \in \Omega$, node $i \in V$ and path $p \in P^\omega(i)$ a feasible solution $(\hat x, \hat y)$ must \NA{satisfy the associated} constraint~\eqref{nmip02}. The right-hand-side of this constraint is one iff all variables $\hat x_k$ for $k \in K_p$ are zero, otherwise the right-hand-side of this constraint is zero or smaller than zero. As each $y^\omega_i$ can only take values zero or one, this means the constraint~\eqref{nmip02} becomes redundant as soon as one of the variables in $\hat x_k$ for $k \in K_p$ has the value one. It is easy to see that constraint~\eqref{nmip02lifted} behaves exactly the same.
\end{proof}

Let $\mytag{PBF.L}$ denote the lifted path-based formulation where inequalities \eqref{nmip02lifted} replace inequalities \eqref{nmip02}.
\begin{theorem}
There exist instances, where the optimal objective value of the LP-relaxation of \myref{PBF.L} is strictly greater than the optimal objective value of the LP-relaxation of \myref{PBF}.
\end{theorem}

\begin{proof}
Consider an instance with $V=\{a,b,c\}$, $I=\{a\}$, $|\Omega|=1$ and the following arcs exist in the live-arc graph for the single scenario: $(a,b), (b,c)$. Moreover, we have a single label $\ell$ (both arcs have the same label), and blocking this label has a cost of two units. The budget $B$ is one. Thus, we cannot block the label $\ell$, and in the optimal solution, the contagion spreads to all three nodes. Hence the optimal objective value is equal to three. In the LP-relaxation of \myref{PBF}, we get an optimal value of $1.5$, since $\hat x_\ell=0.5$ is feasible, and we get $\hat y^\omega_a=1$, $\hat y^\omega_b=0.5$ and $\hat y^\omega_c=0$. In the LP-relaxation of \myref{PBF.L}, we get an optimal value of two since for $\hat x_\ell=0.5$ we get $\hat y^\omega_a=1$, $\hat y^\omega_b=0.5$ and $\hat y^\omega_c=0.5$. The difference in the value of $\hat y^\omega_c$ in both formulations is caused by the fact that in \myref{PBF}, we have $-2x_k$ on the right-hand-side of the inequality \eqref{nmip02} for the path to $c$, while in \myref{PBF.L}, we have $-x_k$ on the right-hand-side of the inequality \eqref{nmip02lifted}.
\end{proof}

\subsection{Benders decomposition formulations}
\label{sec:nonlifted_benders}

We first focus on \BD for the formulation \myref{PBF} and then extend it to \BD for the formulation \myref{PBF.L}. As a starting point, we observe that similar to the formulation \myref{ABF}, we can relax the $y$-variables to $y^\omega_i\geq0$ in \myref{PBF} (and also \myref{PBF.L}), and still get binary $y$-values in an optimal solution for a fixed $\bar x$. The Benders master problem is the same as \myref{BEN} and only the subproblem for a fixed $\bar x$ changes with the path-based formulations. For a given $\bar{x}$, the number of nodes \NA{to which} the contagion has spread in scenario $\omega$ can be obtained via solving the following (primal) subproblem when considering \myref{PBF}.
%
\begin{align}
\mytag{PSPBF}\quad& \min \sum_{i\in V} y_i^\omega \label{eq:nSPr_obj} \tag{PSPBF.OBJ}\\
& \text{s.t.} \notag \\
& \hspace{15pt}y_i^\omega\geq 1 -\sum_{k \in K_p}h_p(k) \bar{x}_{k} & & i \in V, p \in P^\omega(i) \label{eq:nSPr_c2} \tag{PSPBF.PATHS}\\
%& \hspace{15pt}y_i^\omega\geq 1 &i&\in I\\
& \hspace{15pt}y_i^\omega\geq 0 &i&\in V \notag
\end{align}

Let $\gamma^\omega_{ip}$ be dual variables associated with constraints \eqref{eq:nSPr_c2}. The dual of the \myref{PSPBF} is given as follows.


\begin{align}
\mytag{DSPBF}\quad& \max \sum_{i\in V} \sum_{p\in P^\omega(i)} \big ( 1-\sum_{k \in K_p}h_p(k) \bar x_{k} \big ) \gamma^\omega_{ip} \label{eq:nDSPr_obj} \tag{DSPBF.OBJ}\\
& \text{s.t.} \notag \\
& \sum_{p\in P^\omega(i)} \gamma^\omega_{ip} \leq 1 & & i \in V \tag{DSPBF.PATHS} \label{eq:nDSPr_c2}\\
& \hspace{15pt}\gamma_{ip} \geq 0 &i&\in V, p\in P^\omega(i) \notag
\end{align}

For a solution $\hat \gamma^\omega$, the resulting Benders optimality cut is

\begin{equation}
   \theta^\omega \geq \sum_{i\in V} \sum_{p\in P^\omega(i)} \hat{\gamma}^{\omega}_{ip}   -\sum_{i\in V} \sum_{p\in P^\omega(i)} \sum_{k \in K_p} h_p(k) \hat{\gamma}^{\omega}_{ip} x_{k} \label{eq:BendersCut2}. \tag{BOC.PBF}
\end{equation}

The formulation \myref{DSPBF} includes an exponential number of variables and solving it as an LP requires the enumeration of all $p\in P^\omega(i)$ for all $i\in V \setminus I$ (for $i \in I$, the empty path is trivially the best and thus the only one we need to include). Therefore, \myref{PBF} is not a suitable formulation for a \BD method where the subproblems are solved as LPs. However, the structure of \myref{DSPBF} allows obtaining the cut \eqref{eq:BendersCut2} in polynomial time in a combinatorial way, using the characterization of an optimal solution to \myref{DSPBF} which is described in the following theorem. 

\begin{theorem}
Let $\hat{\gamma}^\omega$ be a solution to \myref{DSPBF} such that $\hat \gamma^\omega_{ip}$ is equal to one if $i$ is reachable and path $p$ is the activation path of $i$, and equal to zero otherwise.  Then $\hat{\gamma}^\omega$ is an optimal solution to \myref{DSPBF}. 
\label{prop:BendersCut_Path}
\end{theorem}


\begin{proof}
It is easy to see that \myref{DSPBF} decomposes into $|V|$ subproblems for the nodes $i \in V$, where each has the objective $\max \sum_{p\in P^\omega(i)} \big ( 1-\sum_{k \in K_p}h_p(k) \bar x_{k} \big ) \gamma^\omega_{ip}$. The value of $\sum_{k \in K_p}h_p(k) \bar x_{k}$ is equal to the length of the path $p$ on $G^\omega(\bar{x})$. For given $i$, let $p^\prime \in P^\omega(i)$ be the path with the shortest length on $G^\omega(\bar{x})$ (ties broken arbitrarily). If $p^\prime$ has a length less than one, i.e., $i$ is reachable and $p^\prime$ is its activation path, setting $\gamma^\omega_{ip^\prime}=1$ and $\gamma^\omega_{ip}=0$ for all $p\neq p^\prime$ gives an optimal decision for the problem associated with $i$ as this results in the largest possible contribution to the objective function.  If $p^\prime$ has a length of at least one, then the objective function coefficients of all $\gamma^\omega_{ip}$ are non-positive and thus we need to have $\gamma^\omega_{ip}=0$ for all $p\in P^\omega(i)$ in an optimal solution. 
\end{proof}

For an optimal value of $\hat{\gamma}^\omega$ as described in Theorem \ref{prop:BendersCut_Path}, $\sum_{i\in V} \sum_{p\in P^\omega(i)} \hat{\gamma}^\omega_{ip}$ gives the number of nodes that are reachable in scenario $\omega$. The coefficient of $x_k$ in \eqref{eq:BendersCut2} represents the number of times an arc with label $k$ appears on the activation paths of all nodes, as in \eqref{eq:BendersCut}.

\begin{remark}
It is easy to observe that $d_i^\omega(\bar{x})\in \mathbb{N}$ for $\bar{x}\in \{0,1\}^{|K|}$. In this case, if the shortest path to node $i$ includes at least one blocked arc, i.e., some arc $(i,j)\in A_k^\omega$ such that $\bar{x}_k=1$, then $d_i^\omega(\bar{x})\geq 1$ and node $i$ is not reachable. Therefore, instead of the shortest path lengths, it is enough to have the information 
if node $i$ is accessible from the seed set $I$ via the arcs in $\cup_{k|\bar{x}_k=0}A_k^\omega$. By Theorem \ref{prop:BendersCut_Path}, $\hat{\gamma}^\omega_{ip}=0$ 
%for any $a\in \cup_{k|\bar{x}_k=1}A_k^\omega$. 
for all paths $p$ such that $\sum_{k\in K_p} \bar{x}_k\geq 1$.
As a result, we get 
that the optimal solution value of \myref{DSPBF} in this case is
$\sum_{i\in V} \sum_{p\in P^\omega(i)} \hat{\gamma}^\omega_{ip}$. This is equal to the number of nodes \NA{to which} the contagion can spread in scenario $\omega$ for given $\bar{x}$, since a reachable node for a given $\bar{x}$ corresponds to an activated node ($y_i=1$) in the diffusion process.
\label{remark:integerSep}
\end{remark}

%\KT{TODO: rewrite the following separation paragraphs}

Based on Theorem \ref{prop:BendersCut_Path} and Remark \ref{remark:integerSep}, we can make use of a simple graph search algorithm to separate the cut \eqref{eq:BendersCut2} via computing an optimal dual subproblem solution $\hat{\gamma}^\omega$, when $\bar{x}\in\{0,1\}^{|K|}$. 
In this method, for a given scenario $\omega\in \Omega$, first the nodes that are accessible from $I$ via paths consisting only of the arcs in $\cup_{k:\bar{x}_k=0} A_k^\omega$ are determined. 
There is a violated cut if the number of accessible nodes is strictly greater than $\bar{\theta}^\omega$. In this case, we compute the cut coefficients $\sum_{i\in V} \sum_{p\in P^\omega(i)} h_p(k) \hat \gamma^\omega_{ip}$ for each $k \in K$ by tracking along the activation paths. However, using a single source shortest path algorithm, an exact polynomial time separation of the cuts is also possible for any fractional solution $\bar{x}$. We discuss this separation algorithm in detail in Section \ref{sec:separation}.

For the \BD of \myref{PBF.L}, we observe that the resulting dual subproblem, denoted as $\mytag{DSPBF.L}$, is almost the same as \myref{DSPBF} in which only the objective function changes to

\begin{equation}
    \max \sum_{i\in V} \sum_{p\in P^\omega(i)} (1-\sum_{k \in K_p} \bar x_{k}) \gamma^\omega_{ip} \tag{DSPBF.L.OBJ} \label{eq:nDSPr_obj_lifted}
\end{equation}

and consequently, the Benders optimality cut changes to
\begin{equation}
   \theta^\omega \geq \sum_{i\in V} \sum_{p\in P^\omega(i)} \hat{\gamma}^\omega_{ip}   -\sum_{i\in V} \sum_{p\in P^\omega(i)} \sum_{k \in K_p} \hat{\gamma}^\omega_{ip} x_{k}. \label{eq:BendersCut3} \tag{BOC.PBF.L}
\end{equation}

Note that, differently from \eqref{eq:BendersCut} and \eqref{eq:BendersCut2}, the coefficient of $x_k$ in \eqref{eq:BendersCut3} is equal to the number of activation paths that include at least one arc with label $k$ (and not the cumulative number of occurences an arc with label $k$ appears on the activation paths).

Similar to \myref{DSPBF}, \myref{DSPBF.L} has an objective function that is decomposable in $i$. However, an optimal solution to \myref{DSPBF.L} cannot be found via computing shortest paths on $G^\omega(\bar{x})$.
%as was the case with \DSPp. 
Instead, it is required to find the path $p\in P^\omega(i)$ with the smallest value of $\sum_{k\in K_p}\bar{x}_k$, for each $i\in V$. 
%
Note that for binary valued $\bar{x}$, $\sum_{k\in K_p}\bar{x}_k=0$ (for an optimal path) if $d_i^\omega(\bar{x})=0$, and $\sum_{k\in K_p}\bar{x}_k\geq 1$ otherwise. Therefore, for binary valued $\bar{x}$, the the graph search approach still works for finding a path minimizing $\sum_{k\in K_p}\bar{x}_k$. This means for binary valued $\bar{x}$, we can solve the separation problem for \eqref{eq:BendersCut3} exactly in polynomial time.
However, for fractional $ \bar{x}$, a shortest path approach is likely to fail because once we determine the shortest path to a node $i$, it does not necessarily lead to the best path(s) via $i$ for its successors. For example, suppose node $i$ can be reached by two different labels, say $0$ and $1$ from the seed set $I$, with $\bar x_0=0.4$ and $\bar x_1=0.5$. Thus, the shortest path to $i$ is achieved by taking the arc with label $0$. However, suppose now there is also a node $i'$, which is reachable from $i$ with an arc, which has label $1$ again. The path to $i'$ continuing from the shortest path to $i$ has the value $\bar x_0+\bar x_1=0.9$, while the path to $i'$ taking only label $1$ has the value $0.5$ and thus is better. Therefore, for solving the separation problem for \eqref{eq:BendersCut3} in case $\bar{x}$ is fractional we propose a heuristic method which is explained in Section \ref{sec:separation}. 

\section{Algorithmic details} \label{sec:algorithmic_details}

We propose solution frameworks for solving the Benders master problem \myref{BEN} within a \BC algorithm and add the Benders optimality cuts on-the-fly as they are needed. We develop such a \BD algorithm based on the arc-based formulation \myref{ABF}, and another one based on the path-based formulation \myref{PBF} and its lifted version \myref{PBF.L}. In the \BD algorithm based on the arc-based formulation, which serves as a baseline in our computational experiments, the subproblem for obtaining the Benders optimality cuts is compact and can be solved via linear programming. %Thus, the resulting \BD is a standard ``textbook'' implementation of the \BD algorithm.
The one based on the path-based formulation is the main algorithm considered in this work where we generate the Benders optimality cuts \eqref{eq:BendersCut2} (resp., \eqref{eq:BendersCut3}) with combinatorial algorithms, which we describe in Section \ref{sec:separation}. Moreover, this \BD algorithm also contains additional speed-ups such as scenario-dependent initial seed sets, cut sampling, a starting heuristic and initial cuts. All these enhancements are also discussed in this section. In our computational study, we do not only compare the performance of this enhanced \BD algorithm against the standard implementation of the \BD algorithm based on \myref{ABF}, but also analyze the effect of the individual enhancements.

\subsection{Separation algorithms for Benders optimality cuts}
\label{sec:separation}

Let $(\overline{\mathrm{BEN}})$
%\footnote{\KT{the overline looks too long to me, but I couldn't shorten it while still using a ref \NA{why don't we write simply $(\overline{\mathrm{BEN}})$ without using the reference?}}}
denote the problem to be solved at a node of the branch-and-cut tree. The problem $(\overline{\mathrm{BEN}})$ is a restricted version of the LP-relaxation of \myref{BEN}, excluding the constraints \eqref{eq:BMP_c2}, but (potentially) containing previously obtained Benders optimality cuts and branching decisions. Let $(\bar{\theta},\bar{x})$ be a feasible solution to $\overline{\myref{BEN}}$. If the optimal objective value of the considered dual subproblem (i.e., \myref{DSABF}, \myref{DSPBF} or \myref{DSPBF.L} depending on the chosen algorithmic setting) is greater than $\bar{\theta}^\omega$ for any $\omega\in \Omega$, then a violated Benders optimality cut exists and $(\bar{\theta},\bar{x})$ needs to be cut off. For the correctness of the algorithm, it is only required to check the existence of a violated cut for the solutions that satisfy the binary constraint $x\in\{0,1\}^{|K|}$, as fractional solutions are handled via branching in any case. However, separating fractional solutions can tighten the dual bounds and therefore improve the efficiency of the \BC algorithm. Thus, we also consider separating fractional solutions in some of our algorithmic settings which are considered in our computational study in Section \ref{sec:results}.

%If a solution $(\bar{\theta},\bar{x})$ satisfies the binary constraints but is not feasible for constraints \eqref{eq:BMP_c2}, at least one violated Benders optimality cut must be added to \myref{BEN}.

\paragraph{A basic separation algorithm}

As discussed above, in the basic implementation of the \BD algorithm, we solve \myref{DSABF}, which is a compact formulation, for each scenario $\omega \in \Omega$ as an LP and use its optimal solution to obtain the Benders optimality cut \eqref{eq:BendersCut}.

\paragraph{A combinatorial separation algorithm}

In order to solve the subproblem \myref{DSPBF} in a combinatorial way for given $\omega$ and $\bar{x}$ to obtain Benders optimality cuts \eqref{eq:BendersCut2}, we use the following algorithm. It works for both binary and fractional $\bar{x}$. As explained in the proof of Theorem \ref{prop:BendersCut_Path}, we need to compute the shortest path distances $d_i^\omega(\bar{x})$ for each $i\in V$. Note that for all seed nodes $i \in I$, this distance is trivially zero. Thus, we only need to focus on the nodes $V'=V \setminus I$. To do so, we use the priority queue version of the Dijkstra's shortest path algorithm, where we initialize the node distances as one for $i\in V'$ and as zero for $i\in I$, since $d_i^\omega(\bar{x})$ has to be strictly less than one for node $i$ to be \emph{reachable}. Furthermore, we stop the algorithm when the minimum distance in the course of the algorithm reaches one, as it is not possible to find any reachable nodes (i.e., nodes with $d_i^\omega(\bar{x})<1$) after that point. To compute the value of the cut coefficients (i.e., the value of the dual variables $\gamma^\omega_{ip}$), we backtrack on the paths after the Disjktra's algorithm terminates. 
%Recall that we need the  $\hat{\beta}$ values to decide if there is a violated cut \eqref{eq:BendersCut2} at a fractional solution $\bar{x}$. 
The overall separation procedure is described in more detail in Algorithm \ref{alg:Separation_scenario} which also has an option to heuristically solve the  subproblem \myref{DSPBF.L}, i.e., to heuristically obtain the lifted Benders optimality cuts \eqref{eq:BendersCut3}. This heuristic modification is discussed later below. 

In Algorithm \ref{alg:Separation_scenario} the option $\texttt{lift}=\texttt{N}$ means that the heuristic-part is not invoked. For a given scenario $\omega$, Algorithm \ref{alg:Separation_scenario} starts with initializing a priority queue \texttt{PQ} with the seed nodes. For each node $i\in V^\prime$, we denote by $d_i$ the distance from the seed set,  and by $K_{p_i}$ the set of labels used on the path $p_i$ to node $i$. 
We also initialize the variable $d_{p_i}(k)$, the number of times label $k$ is encountered on $p_i$, with zero.
If the new distance is smaller than the current one, it is updated as well as the path $K_{p_i}$ and $d_{p_i}(k)$. Once the priority queue is empty or the minimum distance is already one, we proceed to the last part where we compute the cut coefficients \KT{$c^\omega_k$, $\forall k,$ and the constant part of the cut $C^\omega$}. In line 12, we add \MS{a small} disturbance factor $\epsilon>0$ to each newly calculated distance so that we avoid using unnecessarily long paths \MS{(i.e., containing many arcs with labels $k$ where $\bar x_k=0$)} which could lead to weaker cuts due to double counting.

\paragraph{A modified combinatorial separation to \MS{heuristically} obtain lifted optimality cuts}


In order to obtain a lifted cut \eqref{eq:BendersCut3}\sloppy, we propose two heuristic \MS{enhancements which are} obtained by modifying the original separation algorithm in the distance calculation and path determination steps. In the first \MS{enhancement} which we call the \emph{posterior lifting}, we calculate the node distances using $\bar{x}$ as before, and while determining the number of times that a label is used on a path $p_i$ we count each label at most once. In other words, the maximum value that $d_{p_i}(k)$ can take is one (in line 18 of Algorithm \ref{alg:Separation_scenario}). This lifting option will be indicated by $\texttt{lift=P}$. 
In the second version which we refer to as \emph{heuristic lifting}, we change the way we compute the path lengths in a way that can help finding a path $p\in P^\omega(i)$ with a smaller value of $\sum_{k\in K_p}\bar{x}_k$, for each $i\in V^\prime$. To this end, when updating the distance of a node $v$ through another node $u$, we treat as if the length of arc $(u,v)$ is zero if the label of $(u,v)$ is already visited on the path to $u$, independent of the value of $\bar{x}_k$ where $(u,v)\in A^\omega_k$. Thus, we provide advantage to the paths that use a smaller number of distinct labels. As in the case of posterior lifting, we count each label at most once on a path, so that we obtain the lifted cut \eqref{eq:BendersCut3}, not \eqref{eq:BendersCut2}. We denote this lifting approach by setting the parameter $\texttt{lift=H}$. Note that, as can be seen in line 14, under this setting we do not punish the usage of long paths with the $\epsilon$ value since the length of the path does not have a direct effect on the strength of \eqref{eq:BendersCut3}.

\begin{algorithm}[htbp]
\caption{\texttt{Separation($\omega, \bar{\theta}, \bar{x}$,\texttt{lift})}}\label{alg:Separation_scenario}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{An infeasible master solution ($\bar{\theta},\bar{x}$)}
\Output{A (possibly violated) cut $\theta^\omega\geq C^\omega - \sum_{k\in K}c^\omega_k x_k$} 
Initialize a priority queue $\texttt{PQ}\leftarrow \emptyset$\; 
Initialize $C^\omega\leftarrow 0$ and $c_k^\omega\leftarrow 0 $, $\forall k \in K$\;
%Initialize $\texttt{reached}\leftarrow \texttt{false}$\;
Initialize the distance $d_i\leftarrow 1$, set of path labels $K_{p_i} \leftarrow \emptyset$, and label counts $d_{p_i}(k)\leftarrow 0$, $\forall i\in V, k\in K$\;
\For{each $i\in I$}
{
Set $d_i=0$, insert $(d_i,i)$ into \texttt{PQ}\;
}
\While{\texttt{PQ} $\neq \emptyset$}
{
    Choose a minimum distance unreached node $u \in \texttt{PQ}$ , label $u$ as \texttt{reached}\;
    \If{$d_u\geq 1$}{\textbf{break}\;}
    
\For{each outgoing arc of $(u,v)\in A^\omega$}
{
\If{$v$ is not \texttt{reached}}
{
Determine the label $k$ of arc $(u,v)\in A_k^\omega$, set $\hat{d}_v \leftarrow d_u + \max\{\bar{x}_k, \epsilon \}$ \;
\If{\texttt{lift=H} and $k\notin K_{p_v}$}
{
Set $\hat{d}_v \leftarrow d_u$ \;
}
\If{$\hat{d}_v < d_v$}
{
Set $d_v \leftarrow \hat{d}_v$, $K_{p_v} \leftarrow K_{p_u} \cup \{k\}$ \;
\If{\texttt{lift}=\texttt{N} or $d_{p_v}(k)=0$}
{Set $d_{p_v}(k) \leftarrow d_{p_v}(k) +1 $ \;}
Insert the pair $(d_v,v)$ into the \texttt{PQ}\;
}
}
}
}
\For{each \texttt{reached} $i\in V^\prime$}
{
Set $C^\omega\leftarrow C^\omega +1$\;
\For{each label $k \in K_{p_i}$}
{ 
Set $c_k^\omega\leftarrow c_k^\omega + d_{p_i}(k)$ 
%for $k:(u,v)\in A_k^\omega$
\;}
}
\end{algorithm}

Another characteristic of the implementation with option $\texttt{lift=H}$ is that we carry \MS{out} a pre-processing step, where we enumerate all the seed-to-node pairs having a directed path, which we refer to as \emph{pure label path}, consisting of arcs with the same label. If there is a pure label path from some $i\in I$ to $j\in V^\prime$ with label $k$, then we add an extra arc $(i,j)$ to $A_k$ if it is not already present. This approach supports the lifting procedure and facilitates finding better paths with respect to the objective function 
\eqref{eq:nDSPr_obj_lifted}
although it does not ensure finding the optimal one.




\subsection{Scenario-dependent extended seed sets}

%As described earlier, some of the labels cannot be blocked and they are aggregated under label 0. 
%\footnote{\KT{I think we removed the unblockable label from the problem definition, so we need to introduce it here?}}
While implementing the \BC algorithm, we explicitly consider the \MS{potential} presence of some labels with an extremely high blocking cost. These labels are not feasible to be blocked and therefore are treated as \emph{unblockable} labels.
Under any feasible blocking decision $\bar{x}$, the seed nodes can infect new nodes via arcs with an unblockable label. Therefore, the nodes that are reachable from $I$ on $G^\omega$ via paths consisting of arcs with such labels behave exactly in the same way as the seed nodes in scenario $\omega$. Let $I^\omega$ denote the set of seed nodes extended with these \emph{seed-like} nodes of scenario $\omega$.  
\begin{theorem}
The objective function of \myref{BEN} can be reformulated as
\begin{equation*}
    \frac{1}{|\Omega|} \sum_{\omega \in \Omega} |I^\omega|+\theta^\omega,
\end{equation*}
%if $I$ is replaced with $I^\omega$ in each \SPa{} and \DSPa{}.
if the Benders optimality cuts are modified so that $\theta^\omega$ estimates the number of activated nodes that are not in $I^\omega$.
\label{prop:extendedSeed}
\end{theorem}
To change the quantity that each $\theta^\omega$ estimates in the desired direction, we need to rewrite the objective functions in our primal Benders subproblems as $\sum_{i\in V\setminus I^\omega} y_i^\omega$ and modify the dual subproblems and the optimality cuts accordingly.
This modification results in a possibly a smaller cut constant,
%$\sum_{i\in I^\omega}  \hat{\alpha}_{i}^{\omega}$,
which in turn could lead to tighter cuts due to down-lifting the cut coefficients 
based on the cut constant. Note that $I^\omega$ is independent of $\bar{x}$, and therefore it needs to be computed only once in a pre-processing step.


\subsection{Cut sampling}

In our basic algorithmic setting, we generate one Benders optimality cut for each scenario $\omega \in \Omega$. However this is not mandatory for correctness of the algorithm as one violated cut is enough to cut off an infeasible solution \MS{$(\bar \theta,\bar x)$}. To take advantage of this flexibility, we include the option to sample a subset of $\Omega$ to generate cuts, i.e., sample cuts from the set of all possible cuts.
%Since fractional cuts are optional in our algorithm, whenever we choose to include them in the algorithm we only generate a subset of the cuts. 
We denote by $\tau$ the ratio of the \MS{number of} scenarios \MS{for which} we aim to generate a violated cut for a given solution $(\bar{\theta},\bar{x})$. 
%We first shuffle the scenarios and solve the subproblem for each scenario until we have $\tau_f \times|\Omega|$ violated cuts or all scenarios are considered.
We sort the scenarios in non-decreasing order of the $\bar{\theta}^\omega$ values (ties are broken arbitrarily) and following this order we generate cuts until we have $\tau \times|\Omega|$ violated cuts or all scenarios \NA{have already been} considered. \MS{The cut sampling procedure is detailed in Algorithm \ref{alg:Separation}.}

\begin{algorithm}[htbp]
\caption{\texttt{CutSampling($\bar{\theta}, \bar{x}$,\texttt{lift},$\tau$)}}\label{alg:Separation}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
%\Input{A solution ($\bar{\theta},\bar{x}$) to the current \BMP}
%\Output{A (violated cut $\theta^\omega\geq C^\omega - \sum_{k\in K}c^\omega_k x_k$}
Set the desired number of violated cuts $count_{max}=\tau |\Omega|$ and $count\leftarrow 0$ \;
Define $\Omega^\prime \leftarrow \Omega$ \;
\While{$count < count_{max}$ and $\Omega^\prime \neq \emptyset$ }
{
Choose $\omega^\prime = \argmin_{\omega \in \Omega^\prime} \bar{\theta}^\omega$, set $\Omega^\prime \leftarrow \Omega^\prime \setminus \{\omega^\prime\}$\; 
Obtain a cut $BC \leftarrow \texttt{Separation($\omega^\prime, \bar{\theta}, \bar{x}$,\texttt{lift})}$\;
\If{$BC$ is violated at $(\bar{\theta},\bar{x})$}
{
Add $BC$ to BMP, set $count\leftarrow count +1$
}
}
\end{algorithm}


\subsection{Initial solution and initial cuts}
\label{sec:initialization}

%\KT{TODO: rewrite and explain initial cuts (obtained from the greedy solution, one for each scenario)}

%$|K|\times |\Omega|$ initial cuts. For each $k\in \{1,\ldots, |K|\}$, we generate the blocking strategy $\bar{x}$ with $\bar{x}_k=1$ and $\bar{x}_\ell=0$ for $\ell \neq k$. Then we execute for each $\omega\in \Omega$ the separation procedure that we use for integer separation, to generate $|\Omega|$ cuts for $\bar{x}$

%An initial heuristic solution could be fed into the \BC algorithm so that an initial non-trivial primal (upper) bound is available. 
\MS{We implement a greedy heuristic to generate an initial primal solution for our \BC algorithms.}
%A greedy algorithm can be used to obtain such a primal solution although it would not provide an approximation guarantee due to the fact that the objective function is not supermodular, as shown in Section \ref{sec:deftheory}.
In the greedy algorithm, starting with no blockings, i.e. $\mathbf{x}=0$, at each iteration we determine the label $k^\prime$ whose blocking causes the largest marginal reduction in the final spread and set $x_{k^\prime}=1$. We continue until no more blocking is possible within the budget $B$.

In order to obtain an initial non-trivial dual (lower) bound, it is possible to initialize the Benders master problem with a set of optimality cuts. To this end, we propose to use the greedy solution and generate one cut of type \eqref{eq:BendersCut}, \eqref{eq:BendersCut2}, or \eqref{eq:BendersCut3} (depending on the current algorithm setting) for each $\omega \in \Omega$.  

\section{Computational results \label{sec:results}}

The solution algorithm was implemented in C++ and IBM ILOG CPLEX 12.10 was used to solve the MIP models. The computations were made on a single core of an Intel Xeon E5-2670v2 machine with 2.5 GHz and 3GB of RAM, and all CPLEX settings were left on their default values. 

%We set a timelimit 1200 seconds for our all runs. 

\subsection{Instances}
\label{sec:instances}

Our instances \MS{are based on} two types of \NA{networks}: real social networks and randomly generated \NA{ones}. We obtain the real social network data at \url{https://snap.stanford.edu/data/\#socnets}. Two of them, Twitter and Epinions are online social networks, Enron is a communication network, and HepPh is a collaboration network. For random \NA{networks}, we consider the Barab\'asi-Albert (BA)\citep{barabasi1999emergence} and Erd\H{o}s-R\'enyi (ER)\citep{Erdos2022OnRG} models and use the \texttt{R} package \texttt{igraph} to generate them \citep{csardi2006igraph}.
The type of the underlying graphs (directed/undirected), the number of nodes $n$ and the number of arcs/edges $m$ are provided in Table \ref{tab:instances}, together with the number of parallel arcs, if any exist. \MS{As the \Problem is defined on directed graphs, we consider \NA{below} directed versions of the undirected graphs. More detail on the \NA{conversion} procedure is given below, where label creation is also discussed. We observe that most of the instances in Table \ref{tab:instances} do not have any parallel arcs and the instances with parallel arcs just have very few. We thus generate another set of instances based on the instances in Table \ref{tab:instances} by randomly adding (additional) parallel arcs. However, the computational results for these instances are quite similar to the results obtained when considering the instances in Table \ref{tab:instances}. Thus, the results for the instances with (additional) parallel arcs can be found in \ref{appendix_multiple_arcs} for ease of readability.}

The node degree distributions of our instances are shown in Figure \ref{fig:degreeDist}. The plots show that, except ER graphs, all the graphs follow a power law distribution.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering \small
  \caption{Description of instances}
    \begin{tabular}{lrrcc}
    \toprule
          & \multicolumn{1}{c}{$n$} & \multicolumn{1}{c}{$m$} & Type & Parallel Arcs\\
    \midrule
    Twitter & 81,306 & 1,768,135 & Directed & $-$\\
    Epinions & 75,879 & 508,837 & Directed & $-$\\
    Enron & 36,692 & 183,831 & Undirected & $-$\\
    HepPh & 12,008 &  118,521 & Undirected & $-$ \\
    BA.1   & 50,000 & 249,995 & Undirected & 310\\
    ER.1   & 50,000 & 250,000 & Undirected & $-$\\
    BA.2    & 50,000 & 499,990 & Undirected & 1807\\
    ER.2    & 50,000 & 500,000 & Undirected & $-$\\
    \bottomrule
    \end{tabular}%
  \label{tab:instances}%
\end{table}%



\iffalse
\begin{table}[htbp]
  \centering \small
  \caption{Description of instances (original and with additional arcs)}
    \begin{tabular}{lcccc}
    \toprule
          & Type & $n$ & $m$ & $m^\prime$ \\
    \midrule
    Twitter & Directed & 81,306 & 1,768,135 & 2,122,399 \\
    Epinions & Directed & 75,879 & 508,837 & 610,720 \\
    Enron & Undirected & 36,692 & 183,831 & 220,719  \\
    HepPh & Undirected & 12,008 &  118,521 &  142,337  \\
    BA.1  & Undirected  & 50,000 & 249,995 & 300,375  \\
    ER.1 & Undirected   & 50,000 & 250,000 & 299,909  \\
    BA.2 & Undirected   & 50,000 & 499,990 & 599,876  \\
    ER.2 & Undirected   & 50,000 & 500,000 & 600,114  \\
    \bottomrule
    \end{tabular}%
  \label{tab:instances}%
\end{table}%
\fi

\begin{figure}[h!tb]
    \centering
    \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Figures/Twitter_degreedist.png}\caption{Twitter}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Figures/Epinions_degreedist.png}\caption{Epinions}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Figures/Enron_degreedist.png}\caption{Enron}%
    \end{subfigure}
    
    \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Figures/HepPh_degreedist.png}\caption{HepPh}%
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Figures/BA_n50000_d5_undirec_degree.png}\caption{BA.1}%
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Figures/BA_undirec_degreedist.png} \caption{BA.2}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Figures/ER_n50000_d5_undirec_degree.png} \caption{ER.1}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Figures/ER_undirec_degreedist.png} \caption{ER.2}
    \end{subfigure}
    \caption{Degree distributions of the instances}
    \label{fig:degreeDist}
\end{figure}

\paragraph{Assigning arc labels}
We consider $n_L \in \{20,30\}$ labels in addition to one unblockable label $k=0$. We create two classes of instances using the networks described in Table \ref{tab:instances}. For a given network and the value of $n_L$, a label for each arc is generated following a negative binomial distribution with size parameter (the number of targeted successes) equals to one.   
\begin{itemize}
    \item Class 1: The mean of the distribution is chosen as five if $n_L=20$ and eight if $n_L=30$,
    \item Class 2: The mean of the distribution is chosen as eight if $n_L=20$ and twelve if $n_L=30$.
\end{itemize}
Once we sample $m$ numbers following the distribution we choose, we shift the resulting values by +1 and aggregate the ones that are larger than $n_L$ under label 0 so that we have $K = \{0, 1, \ldots, n_L\}$. We assign each number in the resulting vector to the arcs, respectively. 
\KT{Note that as a consequence of this procedure, Class 2 instances have smaller difference between label frequencies.}
If the underlying graph of the original network is undirected, then we assign labels to the edges first, and then replace each edge with two arcs having the same label as the original edge. 

\paragraph{Seed set generation}
We consider $|I|\in \{50,100\}$ as the size of the seed set $I$. The way we choose $I$ determines the magnitude of the expected spread without any blocking, and the difficulty of limiting the spread. Therefore, we would like to have a seed set for which blocking the spread is not trivial. Since the classical greedy approach \citep{kempe2003maximizing} is inefficient for moderate seed sizes like the ones we consider, we apply a simplified version of the IMM (influence maximization with martingales) algorithm proposed by \citet{tang2015influence}. Differently from the original version of the algorithm, where there is a dedicated sampling phase, we sample a fixed number of 1000 reverse reachable sets (this number is based on preliminary experiments), and then proceed to the node selection phase. Although the algorithm lacks an approximation guarantee with this modification, it still produces good solutions. 


\iffalse
\subsection{Determining the sample size}
\EG{There is a vast literature on the quality of the approximations via Sample Average Approximation (SAA) and the strategy applied to choose the best sample size. We will briefly mention some of the most significant properties relevant for our case. Let $z^*$ and $z^*_{\Omega}$ be the optimal objective values of the original problem and the approximation problem (MBSMP), respectively.
Also, let $x^*$ and $x^*_{\Omega}$ be the optimal solutions (set of labels) provided by the two formulations. Finally, let $(x^*)_\epsilon$ and $(x^*_{\Omega})_\epsilon$ represent the set of all $\epsilon$-optimal solutions to the original and approximation problems, respectively. Since we have a discrete solution set consisting of a subset of all labels and a linear objective function, the following conditions hold for the (MBSMP).
\begin{enumerate}
	\item The expected value of the optimal objective value of the (MBSMP) is a lower bound on the true optimal objective value, i.e., $\mathbb{E}[z^*_{\Omega}]\leq z^*$
	\item $z^*_{\Omega} \rightarrow z^*$ and $x^*_{\Omega} \rightarrow x^*$ with probability one as $|{\Omega}| \rightarrow \infty$.
	\item The probability of having the optimal SAA solution $x^*_{\Omega}$ as the true optimum solution $x^*$ converges to one exponentially fast as $|{\Omega}| \rightarrow \infty$.
	\item Given the desired significance levels $\rho \in [0,\epsilon)$ and $\alpha \in (0, 1)$, the probability $P \big \{ (x^*_{\Omega})_\epsilon \subset (x^*)_\epsilon \big \} \geq 1-\alpha$. This means that any $\rho$-optimal solution of the (MBSMP) is an $\epsilon$-optimal solution to the original problem with probability at least equal to $1-\alpha$ when the sample size is given as
\begin{align}
|{\Omega}| \geq \frac{3\sigma^2_{\max}}{(\epsilon-\rho)^2}log\left(\frac{\vert \mathcal{X} \vert}{\alpha} \right).
\label{samplesize}
\end{align}
\end{enumerate}
In \eqref{samplesize}, $\sigma^2_{\max}$ is a measure of the maximal variance between the set of optimal solutions and the remaining set of solutions. The parameter $\vert \mathcal{X} \vert$ shows the size of the set of all feasible solutions. Thus, the theoretically required sample size $|\Omega|$ may be too large for practical purposes. Therefore, various strategies are provided in the literature for implementing the SAA efficiently.
\citet{wu2018two}, \citet{han2018influence}, \citet{taninmics2019influence}, \citet{guney2021large},\citet{ kahr2021benders}, and \citet{taninmics2022improved} use a single batch of scenarios with a sample size as large as possible. In this work, we follow a similar approach, which is also called the Single Replication Procedure (SRP) category of the SAA method \cite{Homemdemello2014}. 

%(*** Note: I used $|\Omega|$ to show sample size inline with the model objective functions in section 3.1/3.2/4.1 and proposition 6. However in section 6.1 we use $|\omega|\in \{50,100\}$. \NA{I think we have to change the notation in 6.1)}
}
\fi



\paragraph{Sampling the live-arc scenarios}
For our computational study, we assume that the diffusion follows the IC model. 
Live-arc equivalents of the IC scenarios are obtained by treating each arc $(i,j)$ independently and labelling it as live with a probability equal to the arc probability $p_{ij}$ \citep{kempe2003maximizing}. We end up with a set of live-arcs for each scenario $\omega$ yielding the graph $G^\omega=(V,A^\omega)$. We assume $p_{ij}=0.1$ as it is a commonly used value in related works such as \cite{kempe2003maximizing}, \cite{wu2018two}, \cite{guney2021large}, and \cite{kahr2021benders}. 

In order to determine the number of scenarios to sample, we consider the works on the quality of the approximations via sample average approximation (SAA) and the strategy applied to choose the best sample size. We will briefly mention some of the most significant properties relevant for our case \MS{(see, e.g. \citet{Homemdemello2014} for more details on SAA)}. Let $z^*$ and $z^*_{\Omega}$ be the optimal objective values of the original problem and the approximation problem (\Problem), respectively.
Also, let $x^*$ and $x^*_{\Omega}$ be the optimal solutions (set of labels) provided by the two formulations. Finally, let $(x^*)_\epsilon$ and $(x^*_{\Omega})_\epsilon$ represent the set of all $\epsilon$-optimal solutions to the original and approximation problems, respectively. Since we have a discrete solution set consisting of a subset of all labels and a linear objective function, the following conditions hold for the \Problem:
\begin{enumerate}
	\item The expected value of the optimal objective value of the \Problem is a lower bound on the true optimal objective value, i.e., $\mathbb{E}[z^*_{\Omega}]\leq z^*$
	\item $z^*_{\Omega} \rightarrow z^*$ and $x^*_{\Omega} \rightarrow x^*$ with probability one as $|{\Omega}| \rightarrow \infty$.
	\item The probability of having the optimal SAA solution $x^*_{\Omega}$ as the true optimum solution $x^*$ converges to one exponentially fast as $|{\Omega}| \rightarrow \infty$.
	\item Given the desired significance levels $\rho \in [0,\epsilon)$ and $\alpha \in (0, 1)$, the probability $P \big \{ (x^*_{\Omega})_\epsilon \subset (x^*)_\epsilon \big \} \geq 1-\alpha$. This means that any $\rho$-optimal solution of the \Problem is an $\epsilon$-optimal solution to the original problem with probability at least equal to $1-\alpha$ when the sample size is given as
\begin{align}
|{\Omega}| \geq \frac{3\sigma^2_{\max}}{(\epsilon-\rho)^2}log\left(\frac{\vert \mathcal{X} \vert}{\alpha} \right). \tag{SAA-SS}
\label{samplesize}
\end{align}
\end{enumerate}
In \eqref{samplesize}, $\sigma^2_{\max}$ is a measure of the maximal variance between the set of optimal solutions and the remaining set of solutions. The parameter $\vert \mathcal{X} \vert$ shows the size of the set of all feasible solutions. Thus, the theoretically required sample size $|\Omega|$ may be too large for practical purposes. Therefore, various strategies are provided in the literature for implementing the SAA efficiently. The works
\citet{wu2018two}, \citet{han2018influence},  \citet{taninmics2019influence}, \citet{guney2021large}, \citet{ kahr2021benders}, and \citet{taninmics2022improved} use a single batch of scenarios with a sample size as large as possible. In this work, we follow a similar approach, which is also called the single replication procedure 
%(SRP) 
category of the SAA method \citep{Homemdemello2014}, and consider two values of the number of scenarios, i.e., $|\Omega|\in \{50,100\}$.
\bigskip

In addition to the parameter values explained before, we consider two budget levels $B\in \{4,6\}$ with unit blocking costs, i.e., a cardinality constraint on the blocking strategy. Overall, we obtain 256 instances with 128 instances in each class.


\subsection{Results \label{sec:results2}}
%We \NA{implement} our solution algorithm for the \Problem on our test instances using several settings \NA{which are} summarized as follows.
\MS{We consider the following settings in our computational study.}

\begin{itemize}
    \item LP: The basic implementation of the BD algorithm \MS{based on the arc-based formulation} is carried out, where an LP is solved for each scenario for separation.
    \item I: The combinatorial separation algorithm is used with the option $\texttt{lift}=\texttt{N}$, and only integer solutions are separated.
    \item I+: Within setting I, extended seed sets $I^\omega$ are computed, an initial primal feasible heuristic solution is obtained and initial cuts are added to the Benders master problem, as described in Section \ref{sec:algorithmic_details} 
    \item I+S: In addition to setting I+, we sample scenarios for cut generation using $\tau=0.2$.
    \item I+SF: In addition to setting I+S, we separate fractional solutions using $\tau=0.2$.
    \item I+SFP: Differently from setting I+SF we execute the combinatorial separation algorithm with $\texttt{lift}=\texttt{P}$
    \item I+SFH: Differently from setting I+SF we execute the combinatorial separation algorithm with $\texttt{lift}=\texttt{H}$
\end{itemize}

We also consider the greedy algorithm described in Section \ref{sec:initialization} \MS{as a standalone option} and denote it by G. We set the time limit to one hour and run the algorithms until the time limit is reached or a proven optimal solution is obtained. The aggregated results over all Class 1 (Class 2) instances are shown in Table \ref{tab:Class1} (Table \ref{tab:Class2}) for each of our settings. 
%The results for the modified instances with additional arcs are shown in Table \ref{} and Table \ref{}. 
These tables display the average solution time in seconds ($t(s)$\MS{, where TL indicates that all runs reached the time limit}), the best known upper bound (UB), the best known lower bound (LB), the optimality gap percentage (Gap), lower bound at the B\&B root node ($\text{LB}_0$), number of B\&B nodes explored (nBB), number of optimality cuts added to separate integer solutions (nIntCut), number of optimality cuts added to separate fractional solutions (nFrCut), and number of optimally solved instances out of 128 (nOpt), respectively.

It is understood from the tables that solving the Benders subproblems in a combinatorial way decreases the solution times dramatically. Moreover, the lower bounds which cannot be improved from a trial value within the time limit under setting LP, becomes much better under setting I. This reflects on the optimality gaps and the number of optimally solved instances. We are able to improve the average gaps further with the other enhancements such as cut sampling and initial cuts. Another big improvement is due to separating fractional solutions which increases nOpt significantly. While both lifting options work very well on the overall, the heuristic lifting is not always more efficient than the posterior cut lifting, as the optimality gaps and solution times suggest. The only clear improvement in our measures that the heuristic lifting brings seems to be root lower bounds. The improvement in this measure does not impact the final results except nBB, in the same amount. We can see that setting I+SFH manages to solve all instances of Class 1 to optimality, \NA{while} for Class 2, 9 out of 128 instances remain unsolved with this setting. \KT{The numbers in the tables show that Class 2 instances are more difficult to solve, which can be interpreted \MS{that} it becomes more difficult to determine the best labels to block when the label frequencies are close.}



\iffalse

% Table generated by Excel2LaTeX from sheet 'paperTables'
\begin{table}[htbp]
  \centering \footnotesize
  \caption{Average results for the instances with negative binomial label distribution.}
    \begin{tabular}{clrrrrrrrrrr}
    \toprule
    \multicolumn{1}{l}{Instance} & Method & \multicolumn{1}{l}{Spread0} & \multicolumn{1}{l}{UB} & \multicolumn{1}{l}{LB} & \multicolumn{1}{l}{$t(s)$} & \multicolumn{1}{l}{Gap} & \multicolumn{1}{l}{nOpt} & \multicolumn{1}{l}{nBest} & \multicolumn{1}{l}{nIter} & \multicolumn{1}{l}{nIntCut} & \multicolumn{1}{l}{nFracCuts} \\
    \midrule
    \multirow{5}[2]{*}{Twitter} & LP   & 36478.3 & 20361.4 & 75.0  & 3600.3 & 99.6  & 0     & 6     & 36.4  & 1537.7 & - \\
          & BI    & 36478.3 & 19543.6 & 14953.8 & 2500.3 & 21.7  & 10    & 15    & 3556.8 & 43671.9 & - \\
          & $\text{BF}^+$   & 36478.3 & 19526.5 & 17738.4 & 1770.3 & 8.8   & 13    & 16    & 1645.1 & 7405.0 & 16615.0 \\
          & LR    & 36478.3 & 19526.5 & 128.2 & 981.6 & 99.3  & 0     & 16    & 333.6 & -     & - \\
          & G     & 36478.3 & 19526.5 & -     & 126.4 & -     & -     & 16    & -     & -     & - \\
    \midrule
    \multirow{5}[2]{*}{Epinions} & LP   & 10375.6 & 5596.3 & 77.7  & 3600.2 & 98.5  & 0     & 8     & 254.3 & 5430.5 & - \\
          & BI    & 10375.6 & 5479.0 & 5329.4 & 1173.5 & 2.6   & 14    & 16    & 4206.8 & 49976.6 & - \\
          & $\text{BF}^+$   & 10375.6 & 5479.0 & 5479.0 & 388.8 & 0.0   & 16    & 16    & 964.2 & 5942.9 & 9997.5 \\
          & LR    & 10375.6 & 5479.0 & 100.4 & 290.5 & 98.1  & 0     & 16    & 333.6 & -     & - \\
          & G     & 10375.6 & 5479.0 & -     & 26.2  & -     & -     & 16    & -     & -     & - \\
    \midrule
    \multirow{5}[2]{*}{Enron} & LP   & 9625.3 & 5376.1 & 75.0  & 3600.1 & 98.6  & 0     & 6     & 238.7 & 4876.6 & - \\
          & BI    & 9625.3 & 5309.0 & 5224.3 & 949.9 & 1.6   & 14    & 16    & 3778.3 & 42750.0 & - \\
          & $\text{BF}^+$   & 9625.3 & 5309.0 & 5309.0 & 211.9 & 0.0   & 16    & 16    & 572.5 & 4257.8 & 6160.0 \\
          & LR    & 9625.3 & 5377.1 & 96.9  & 340.7 & 98.2  & 0     & 14    & 333.4 & -     & - \\
          & G     & 9625.3 & 5309.0 & -     & 29.3  & -     & -     & 16    & -     & -     & - \\
    \midrule
    \multirow{5}[2]{*}{BA.1} & LP   & 16055.8 & 2467.8 & 75.0  & 3600.2 & 93.9  & 0     & 10    & 242.8 & 5227.6 & - \\
          & BI    & 16055.8 & 2379.2 & 2379.2 & 280.2 & 0.0   & 16    & 16    & 1776.4 & 16428.0 & - \\
          & $\text{BF}^+$   & 16055.8 & 2379.2 & 2379.2 & 212.1 & 0.0   & 16    & 16    & 479.3 & 6224.8 & 5346.9 \\
          & LR    & 16055.8 & 2454.5 & 98.1  & 271.2 & 93.2  & 0     & 15    & 333.1 & -     & - \\
          & G     & 16055.8 & 2379.2 & -     & 34.0  & 100.0 & -     & 16    & -     & -     & - \\
    \midrule
    \multirow{5}[2]{*}{BA.2} & LP   & 36117.7 & 15948.0 & 75.0  & 3600.1 & 99.4  & 0     & 7     & 91.1  & 2629.1 & - \\
          & BI    & 36117.7 & 15264.1 & 14501.0 & 1652.7 & 4.8   & 14    & 16    & 3354.7 & 32605.1 & - \\
          & $\text{BF}^+$   & 36117.7 & 15264.1 & 15264.1 & 1069.5 & 0.0   & 16    & 16    & 951.9 & 7581.3 & 9637.5 \\
          & LR    & 36117.7 & 15264.1 & 171.9 & 673.5 & 98.8  & 0     & 16    & 333.5 & -     & - \\
          & G     & 36117.7 & 15264.1 & -     & 177.6 & -     & -     & 16    & -     & -     & - \\
    \midrule
    \multirow{5}[2]{*}{ER.1} & LP   & 3171.8 & 164.4 & 75.0  & 3600.1 & 52.1  & 0     & 12    & 245.5 & 8030.5 & - \\
          & BI    & 3171.8 & 163.9 & 163.9 & 26.1  & 0.0   & 16    & 16    & 206.6 & 2729.6 & - \\
          & $\text{BF}^+$   & 3171.8 & 163.9 & 163.9 & 27.2  & 0.0   & 16    & 16    & 47.0  & 2715.1 & 1182.5 \\
          & LR    & 3171.8 & 171.0 & 75.5  & 253.1 & 53.4  & 0     & 10    & 332.8 & -     & - \\
          & G     & 3171.8 & 163.9 & -     & 2.1   & -     & -     & 16    & -     & -     & - \\
    \midrule
    \multirow{5}[2]{*}{ER.2} & LP   & 39843.4 & 6069.9 & 75.0  & 3600.3 & 89.5  & 0     & 9     & 76.7  & 2597.6 & - \\
          & BI    & 39843.4 & 5713.5 & 3201.6 & 2014.3 & 32.5  & 10    & 14    & 5796.6 & 36786.1 & - \\
          & $\text{BF}^+$   & 39843.4 & 5549.0 & 3760.8 & 1788.6 & 30.9  & 10    & 16    & 1384.1 & 9578.0 & 15285.2 \\
          & LR    & 39843.4 & 7080.9 & 101.4 & 590.4 & 88.9  & 0     & 13    & 332.6 & -     & - \\
          & G     & 39843.4 & 5549.0 & -     & 143.3 & -     & -     & 16    & -     & -     & - \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%

\fi

% Table generated by Excel2LaTeX from sheet 'Summary (7)'
\begin{table}[htbp]
  \centering \footnotesize
  \caption{Aggregated results for Class 1 instances}
    \begin{tabular}{lrrrrrrrrr}
    \hline
    Method & \multicolumn{1}{l}{$t(s)$} & \multicolumn{1}{l}{UB} & \multicolumn{1}{l}{LB} & \multicolumn{1}{l}{Gap} & \multicolumn{1}{l}{$\text{LB}_0$} & \multicolumn{1}{l}{nBB} & \multicolumn{1}{l}{nIntCut} & \multicolumn{1}{l}{nFrCut} & \multicolumn{1}{l}{nOpt} \\
    \hline
    LP   & TL & 7258.3 & 75.3  & 91.0  & 75.0  & 259.7 & 5419.9 & 0.0   & 0 \\
    I     & 1087.4 & 7072.3 & 6030.7 & 7.9   & 77.8  & 2657.1 & 31047.7 & 0.0   & 108 \\
    I+    & 1108.3 & 6963.9 & 6011.1 & 6.6   & 121.7 & 3044.0 & 29729.1 & 0.0   & 109 \\
    I+S   & 692.9 & 6963.9 & 6707.4 & 1.5   & 120.8 & 4793.5 & 15091.1 & 0.0   & 121 \\
    I+SF  & 542.8 & 6963.9 & 6865.0 & 1.8   & 1219.5 & 593.3 & 2149.3 & 10229.2 & 124 \\
    I+SFP & 364.5 & 6963.9 & 6963.9 & 0.0   & 1719.9 & 376.5 & 1579.1 & 5977.9 & 128 \\
    I+SFH & 394.3 & 6963.9 & 6963.9 & 0.0   & 2618.8 & 263.3 & 1136.6 & 5579.0 & 128 \\
    G     & 75.1  & 6963.9 &       &       &       &       &       &       & \\
    \hline
    \end{tabular}%
  \label{tab:Class1}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'Summary (7)'
\begin{table}[htbp]
  \centering \footnotesize
  \caption{Aggregated results for Class 2 instances}
    \begin{tabular}{lrrrrrrrrr}
    \hline
    Method & \multicolumn{1}{l}{$t(s)$} & \multicolumn{1}{l}{UB} & \multicolumn{1}{l}{LB} & \multicolumn{1}{l}{Gap} & \multicolumn{1}{l}{$\text{LB}_0$} & \multicolumn{1}{l}{nBB} & \multicolumn{1}{l}{nIntCut} & \multicolumn{1}{l}{nFrCut} & \multicolumn{1}{l}{nOpt} \\
    \hline
    LP   & TL & 10917.3 & 75.2  & 94.5  & 75.0  & 209.8 & 5011.0 & 0.0   & 0 \\
    I     & 1996.5 & 10848.6 & 7300.5 & 18.4  & 332.3 & 4005.6 & 53473.0 & 0.0   & 75 \\
    I+    & 2019.1 & 10637.9 & 7382.6 & 17.3  & 993.0 & 4443.2 & 51634.0 & 0.0   & 76 \\
    I+S   & 1565.6 & 10637.9 & 8500.1 & 10.2  & 986.8 & 7860.4 & 32052.2 & 0.0   & 92 \\
    I+SF  & 1106.6 & 10637.9 & 9251.4 & 7.6   & 3556.7 & 1276.7 & 3229.1 & 17619.6 & 109 \\
    I+SFP & 919.2 & 10637.9 & 9675.0 & 4.6   & 4095.8 & 1047.0 & 2536.7 & 13309.5 & 116 \\
    I+SFH & 858.9 & 10637.9 & 9893.8 & 3.7   & 4755.8 & 869.0 & 2198.4 & 12080.8 & 119 \\
    G     & 81.3  & 10637.9 &       &       &       &       &       &       &  \\
    \hline
    \end{tabular}%
  \label{tab:Class2}%
\end{table}%


Next, we plot the cumulative distribution of the running time, final optimality gaps and root gaps in Figure \ref{fig:cum_plots}, for all settings except G and LP. The reason is that G is a heuristic and thus does not provide a final gap and takes very short to terminate compared to the exact methods, as expected, and LP takes very long and yields a very large gap. 
%While Figure \ref{} shows the plots for original networks, Figure \ref{} shows the results for the modified networks. 
We see that, the components with largest marginal contribution in terms of runtime are cut sampling (setting I+S) and fractional separation (I+SF), followed by posterior lifting (I+SFP). A similar situation is observed when we focus on the final optimality gaps. In terms of root gaps, initialization (I+), fractional separation, and heuristic lifting (I+SFH) cause the biggest marginal effect.  



\begin{figure}[h!tb]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{New Figures/Class1_1_final.png}
         \caption{Class 1 running times}
         \label{fig:Class1runtime}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{New Figures/Class2_1_final.png}
         \caption{Class 2 running times}
         \label{fig:Class2runtime}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{New Figures/Class1_2_final.png}
         \caption{Class 1 final gaps}
         \label{fig:Class1gap}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{New Figures/Class2_2_final.png}
         \caption{Class 2 final gaps}
         \label{fig:Class2gap}
    \end{subfigure}

\end{figure}

\begin{figure}[h!tb]\ContinuedFloat
    \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{New Figures/Class1_3_final.png}
         \caption{Class 1 root gaps}
         \label{fig:Class1rootgap}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{New Figures/Class2_3_final.png}
         \caption{Class 2 root gaps}
         \label{fig:Class2rootgap}
    \end{subfigure}
    \caption{Cumulative distribution of running times, final gaps, and root gaps, for Class 1 and Class 2 instances}
    \label{fig:cum_plots}
\end{figure}







\section{Conclusions and outlook} \label{sec:conclusion}

In this paper, we introduce the \emph{measure-based spread minimization problem} (\Problem). The problem is defined on a network with arc labels. It involves selecting a set of arcs labels subject to a budget constraint. For a given selection of labels, all the arcs with these labels are removed from the network. The goal is to select the labels in such a way as to minimize the spread of a contagion, which starts from a given set of seed nodes. We adopt the independent cascade model which is a stochastic diffusion model for the spread dynamics. The \Problem extends the existing studies in the literature that focus on the challenging problem of minimizing the spread of infections by link removal and node deletions by considering a stochastic diffusion model instead of a deterministic one, and by considering removal of labels and \NA{rather than} single node/arc deletions. Moreover, as opposed to many studies, exact solution algorithms are developed to provide a performance guarantee for quite large networks.

We present two integer linear programming formulations for the problem and based on them, we develop Branch-and-Benders-cut solution algorithms. We show that the Benders optimality cuts can be separated in a combinatorial fashion, i.e., without the need of solving linear programs. We also present a set of valid inequalities for one of the formulations and show how these inequalities can be incorporated in the Benders-based algorithms. Moreover, we develop additional enhancements for our algorithms, namely using scenario-dependent extended seed sets, generating initial cuts, and implementing a starting heuristic.
 
There can be several future research directions. First, the model could be extended to implement measures on nodes types in addition to contact types represented by arc labels. For example, all the nodes belonging to a population group could be blocked, which will remove all the associated nodes from network. An example would be to impose a lockdown for people within a certain age interval. Another research direction could be investigating the strategy of partial blocking of arc labels rather than the case of complete blocking in this study. Partial blocking corresponds to allowing people continuing with the contact type but with additional restrictions. For example, an authority may limit the time spent in a restaurant rather than closing it. On the algorithmic side, it could be interesting to try to determine additional valid inequalities. Moreover, a decomposition approach based on Lagrangian decomposition instead of Benders decomposition could also be a fruitful avenue for further work.  

\section*{Acknowledgments}
This research was funded in whole, or in part, by the Austrian Science Fund (FWF)[P 35160-N]. For the purpose of open access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.

%\section*{References}

\bibliographystyle{elsarticle-harv}
\bibliography{action.bib}


\newpage
\appendix

\section{Proof of Theorem \ref{prop:DSP_solution}}
\label{appendix_proof}

\begin{proof}
We start with showing that $(\hat{\alpha},\hat{\beta})$ is a feasible solution to \myref{DSABF}. By their definitions, $\hat{\alpha}\geq 0$ and $\hat{\beta} \geq 0$. We need to show that $(\hat{\alpha},\hat{\beta})$ satisfies \eqref{eq:DSPr_c1} and \eqref{eq:DSPr_c2}. Consider any $i\in I$. The constraint \eqref{eq:DSPr_c1} can be rewritten as
\begin{equation}
1+\sum_{a\in \delta^{+}_\omega(i)} \beta_a^\omega - \sum_{\delta^{-}_\omega(i)} \beta_a^\omega \geq \alpha_i^\omega .
\label{eq:DSPr_c1repeat}
\end{equation}
At $(\hat{\alpha},\hat{\beta})$, the right-hand-side of \eqref{eq:DSPr_c1repeat} is equal to the number of nodes that are reachable on $G^\omega(\bar{x})$ with an activation path starting at $i$, including itself. By definition of $\hat{\beta}$, $\sum_{a\in \delta^{-}_\omega(i)} \hat{\beta}^\omega_a=0$ since $i$ is a seed node. The value of the first term $\sum_{a\in \delta^{+}_\omega(i)} \beta_a^\omega$ on the left-hand-side is equal to the number of nodes whose activation path contains one of the outgoing arcs of $i$, which is equal to the value of $\alpha_i^\omega$ minus one (note that each seed also activates itself and for this, no arc is needed). Thus, the constraint is satisfied.

Next, consider $i\in V \setminus I$. We show that \eqref{eq:DSPr_c2} is satisfied at $(\hat{\alpha},\hat{\beta})$ by a case distinction. 

\begin{itemize}
\item Case 1: $\sum_{a\in \delta^{-}_\omega(i)} \hat{\beta}^\omega_a = 0$. This means that the total number of nodes that are reachable on $G^\omega(\bar{x})$ via the incoming arcs of $i$, is zero. This implies that $i$ is not reachable as well. Therefore, the number of nodes that are reachable via the outgoing arcs of $i$ is also zero, i.e., $\sum_{a\in A_{i\omega}^{+}} \hat{\beta}^\omega_a=0$. The value of the left-hand-side of \eqref{eq:DSPr_c2} becomes zero and the constraint is satisfied.

\item Case 2: $\sum_{a\in \delta^{-}_\omega(i)} \hat{\beta}^\omega_a > 0$. In this case, at least one of the incoming arcs of $i$ is on the activation path of some node(s) and $i$ is located on some activation path(s). The number of nodes that are reachable via its outgoing arcs, i.e., $\sum_{a\in \delta^{+}_\omega(i)} \hat{\beta}^\omega_a$, is exactly $\sum_{a\in \delta^{-}_\omega(i)} \hat{\beta}^\omega_a-1$ because one of the nodes that are reachable via its incoming arcs is $i$ itself. Thus, the left-hand-side of \eqref{eq:DSPr_c2} is equal to one and the constraint is satisfied. 

\end{itemize}

In order to prove the optimality of $(\hat{\alpha},\hat{\beta})$, we show that the objective value of \myref{DSABF} at $(\hat{\alpha},\hat{\beta})$ is equal to the optimal objective function value of the primal subproblem for the given scenario $\omega$ and solution $\bar x$ of \myref{BEN}. The primal subproblem reads as follows.

\begin{align}
\mytag{PSABF} \quad &\min \sum_{i\in V} y_i^\omega \label{eq:SPr_obj}\\
& \text{s.t.} \notag \\
& \hspace{15pt} y_i^\omega \geq 1 &i&\in I \label{eq:SPr_c1}\\
& \hspace{15pt}y_j^\omega\geq y_i^\omega -\bar{x}_{k} &(&i,j) \in A_k^\omega, k\in K \label{eq:SPr_c2}\\
& \hspace{15pt}y_i^\omega\geq 0 &i&\in V \setminus I
\end{align}

It can be observed that the value of each $y_i^\omega$ for $i \in I$ in an optimal solution to \myref{PABF} is equal to
one due to constraint \eqref{eq:SPr_c1} and it is $\max\{0,1-d_i^\omega(\bar{x})\}$ for $i \in V \setminus I$ due to the cumulative impact of $\bar{x}_k$ in \eqref{eq:SPr_c2}. This results in a total objective value that is equal to the number of $i\in V$ such that $d_i^\omega(\bar{x})<1$, i.e., $i$ is reachable on $G^\omega(\bar{x})$, minus their total shortest path distances (recall that seed nodes $i \in I$ are trivially reachable). 
Now let us consider the objective value of \myref{DSABF} for $(\hat{\alpha},\hat{\beta})$: While $\sum_{i \in I}\hat{\alpha}_i$ accounts for the number of reachable nodes, $\sum_{k\in K} \sum_{a\in A_k^\omega} \hat{\beta}_a^\omega \bar{x}_{k}$ accumulates $\bar{x}_{k}$ for each node whose activation path contains $a$ and as a result keeps track of total shortest path distance. Therefore, we get an objective value that is equal to the optimal objective function value of \myref{PABF}. Together with the feasibility result, this completes the proof.
\end{proof}





\section{Results for instances with (additional) parallel arcs}
\label{appendix_multiple_arcs}

As explained in Remark \ref{remark:multiple}, the underlying graph of the problem instance is allowed to contain parallel arcs between any pair of nodes, where these arcs have distinct labels. In order to observe the effect of these arcs and also the impact of having denser graphs in general, we generate a second set of instances where we created new parallel arcs in addition to the (potentially existing) original ones.
%We also generate instances where there can exist multiple arcs/edges with different labels between the same pair of nodes. 
Towards this end, we take the original instances (i.e., the ones discussed in Section \ref{sec:instances}) and for each existing arc we create one or two copies of the arc with probabilities 0.10 and 0.05, respectively. Each newly created arc is assigned a label randomly such that the original and additional arcs have different labels. The resulting number of arcs are shown under column $m^\prime$ in Table \ref{tab:instances}, next to the original number of arcs $m$.

\begin{table}[htbp]
  \centering \small
  \caption{Description of instances with (additional) parallel arcs}
    \begin{tabular}{lcccc}
    \toprule
          & $m$ & $m^\prime$ \\
    \midrule
    Twitter   & 1,768,135 & 2,122,399 \\
    Epinions   & 508,837 & 610,720 \\
    Enron  & 183,831 & 220,719  \\
    HepPh   &  118,521 &  142,337  \\
    BA.1     & 249,995 & 300,375  \\
    ER.1     & 250,000 & 299,909  \\
    BA.2     & 499,990 & 599,876  \\
    ER.2     & 500,000 & 600,114  \\
    \bottomrule
    \end{tabular}%
  \label{tab:new_instances}%
\end{table}%

Table \ref{tab:Class1_multiple} (Class 1) and Table \ref{tab:Class2_multiple} (Class 2) present the average results of the algorithms over these new instances. The columns are as explained in Section \ref{sec:results2}. We see that the overall solution times are increased with the newly added arcs/edges. Similarly, the number of optimally solved instances is decreased for both classes. However, the performance of the algorithms with respect to each other remains similar and I+SFP and I+SFH are the best exact algorithms among those we propose, in terms of the running time, final gaps, and nOpt. The cumulative distribution of the running times, root gaps and final gaps are shown in Figure \ref{fig:MultArcResults}, which are in line with the results of the original instances. Thus, we see that having a significant number of parallel arcs does not seem to have a clear impact on the algorithms.

% Table generated by Excel2LaTeX from sheet 'Summary (paper)'
\begin{table}[htbp]
  \centering \footnotesize
  \caption{Aggregated results for Class 1 instances with (additional) parallel arcs}
    \begin{tabular}{lrrrrrrrrr}
    \hline
    Method & \multicolumn{1}{l}{$t(s)$} & \multicolumn{1}{l}{UB} & \multicolumn{1}{l}{LB} & \multicolumn{1}{l}{Gap} & \multicolumn{1}{l}{$\text{LB}_0$} & \multicolumn{1}{l}{nBB} & \multicolumn{1}{l}{nIntCut} & \multicolumn{1}{l}{nFrCut} & \multicolumn{1}{l}{nOpt} \\
    \hline
    LP   & TL & 7258.3 & 75.3  & 91.0  & 75.0  & 259.7 & 5419.9 & 0.0   & 0 \\
    I     & 1246.5 & 9422.2 & 7561.5 & 9.4   & 76.8  & 2788.1 & 31168.0 & 0.0   & 106 \\
    I+    & 1269.5 & 9321.6 & 7617.7 & 8.9   & 113.8 & 2840.8 & 29549.7 & 0.0   & 106 \\
    I+S   & 851.2 & 9321.6 & 8887.2 & 2.5   & 111.7 & 4693.0 & 16067.3 & 0.0   & 119 \\
    I+SF  & 651.1 & 9321.6 & 9036.1 & 2.0   & 1567.5 & 567.1 & 2306.6 & 10189.5 & 123 \\
    I+SFP & 490.2 & 9321.6 & 9321.6 & 0.0   & 2073.2 & 406.0 & 1758.3 & 6243.5 & 128 \\
    I+SFH & 502.6 & 9321.6 & 9290.9 & 0.3   & 3359.4 & 264.0 & 1207.7 & 5367.0 & 127 \\
    G     & 100.5 & 9322.4 &       &       &       &       &       &       &  \\
    \hline
    \end{tabular}%
  \label{tab:Class1_multiple}%
\end{table}%



% Table generated by Excel2LaTeX from sheet 'Summary (paper)'
\begin{table}[htbp]
  \centering \footnotesize
  \caption{Aggregated results for Class 2 instances with (additional) parallel arcs}
    \begin{tabular}{lrrrrrrrrr}
    \hline
    Method & \multicolumn{1}{l}{$t(s)$} & \multicolumn{1}{l}{UB} & \multicolumn{1}{l}{LB} & \multicolumn{1}{l}{Gap} & \multicolumn{1}{l}{$\text{LB}_0$} & \multicolumn{1}{l}{nBB} & \multicolumn{1}{l}{nIntCut} & \multicolumn{1}{l}{nFrCut} & \multicolumn{1}{l}{nOpt} \\
    \hline
    LP   & TL & 10917.3 & 75.2  & 94.5  & 75.0  & 209.8 & 5011.0 & 0.0   & 0 \\
    I     & 2164.8 & 13803.3 & 8574.8 & 21.4  & 828.6 & 3614.9 & 50541.4 & 0.0   & 68 \\
    I+    & 2180.8 & 13701.1 & 8651.5 & 21.0  & 1671.9 & 3764.2 & 49033.9 & 0.0   & 67 \\
    I+S   & 1816.7 & 13701.1 & 10336.5 & 13.4  & 1658.6 & 7997.1 & 32152.0 & 0.0   & 84 \\
    I+SF  & 1274.7 & 13701.1 & 11401.7 & 8.5   & 4812.7 & 1206.1 & 3439.8 & 16702.6 & 106 \\
    I+SFP & 1164.3 & 13701.1 & 12231.6 & 5.5   & 5514.6 & 1081.1 & 2761.1 & 13357.8 & 110 \\
    I+SFH & 1067.4 & 13701.1 & 12650.9 & 3.6   & 6339.0 & 888.7 & 2384.6 & 12122.1 & 115 \\
    G     & 110.2 & 13701.1 &       &       &       &       &       &       &  \\
    \hline
    \end{tabular}%
  \label{tab:Class2_multiple}%
\end{table}%



\begin{figure}[h!tb]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{New Figures/Class1_1_final.png}
         \caption{Class 1 running times}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{New Figures/Class2_1_final.png}
         \caption{Class 2 running times}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{New Figures/Class1_2_final.png}
         \caption{Class 1 final gaps}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{New Figures/Class2_2_final.png}
         \caption{Class 2 final gaps}
    \end{subfigure}

\end{figure}

\begin{figure}[h!tb]\ContinuedFloat
    \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{New Figures/Class1_3_final.png}
         \caption{Class 1 root gaps}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{New Figures/Class2_3_final.png}
         \caption{Class 2 root gaps}
    \end{subfigure}
    \caption{Cumulative distribution of running times, final gaps, and root gaps, for Class 1 and Class 2 instances with (additional) parallel arcs}
    \label{fig:MultArcResults}
\end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{New Figures/Mult_Class1_1_final.png}%
%     \includegraphics[width=0.5\linewidth]{New Figures/Mult_Class2_1_final.png}
%     \includegraphics[width=0.5\linewidth]{New Figures/Mult_Class1_2_final.png}%
%     \includegraphics[width=0.5\linewidth]{New Figures/Mult_Class2_2_final.png}
%      \includegraphics[width=0.5\linewidth]{New Figures/Mult_Class1_3_final.png}%
%     \includegraphics[width=0.5\linewidth]{New Figures/Mult_Class2_3_final.png}
%     \caption{Instances with (additional) multiple arcs: Class 1 time, Class 2 time, Class 1 gap, Class 2 gap, Class 1 root gap, Class 2 root gap}
%     \label{fig:MultArcResults}
% \end{figure}



\end{document}


\iffalse
\subsubsection{Valid inequalities \KT{(TODO: remove)}}

\begin{theorem}
Given a scenario $\omega\in \Omega$, a label $k\in K$, and an arc $(i,j) \in A_k^\omega$, suppose that node $i$ is not reachable on $G^\omega$ from any seed node via a path not containing an arc having label $k$. The following inequality is valid for \SPa{} for any value of $\bar{x}$ with $\bar{x}_k\in \{0,1\}$.
\begin{equation}
    y_j^\omega \geq y_i^\omega
    \label{eq:validIneq1}
\end{equation}
\label{theo:validIneq}
\end{theorem}

\begin{proof}
Suppose that node $i$ is not reachable from $I$ on $G^\omega$. Then in an optimal solution to \SPa{} we have $y_i^\omega=0$, and \eqref{eq:validIneq1} trivially holds.

Now suppose that node $i$ is reachable from $I$ on $G^\omega$. Having $y_i^\omega>0$ in an optimal solution implies that $\bar{x}_k$=0, due to the assumptions that node $i$ is not reachable without passing through arcs with label $k$ and $\bar{x}_k\in \{0,1\}$. As a result $j$ is activated as much as $i$, i.e., $y_j^\omega\geq y_i^\omega$, whenever $y_i^\omega > 0$.
%, since $(i,j) \in A_k^\omega$.
\end{proof}

When a valid inequality of type \eqref{eq:validIneq1} is found for $a^\prime=(i,j)\in A^\omega_k$, it can replace the associated constraint \eqref{eq:SPr_c2}. This would result in the following change in the \DSPa{} objective function. The term $\beta_{a^\prime}^\omega \bar{x}_k$ will not appear in \eqref{eq:DSPr_obj}, i.e., the Benders optimality cuts will be lifted. 
\fi


\subsection{The Lagrangean Relaxation and Outline of the Heuristic}
Lagrangean Relaxation (LR) is a popular and efficient method for solving complex combinatorial optimization problems \cite{Fisher1981}. Basically, in LR a set of complicating constraints are relaxed and carried to the objective function with a penalty cost to punish the violation of the relaxed constraint. The original MIP formulation has two sets of constraints other than the variable bounds and it is possible to propose an LR scheme by relaxing either (\ref{mip01}) or (\ref{mip02}). When the knapsack constraint (\ref{mip01}) is relaxed, the problem can be decomposed over scenarios. In the stochastic optimization literature, there is a plethora of studies regarding scenario decomposition approaches to stochastic optimization problems. \cite{Kumar2012} first creates copies of the seed set variables ($y_i$) and then relaxes the so called \textit{nonanticipativity} constraints to decompose the problem for each scenario. Then they apply sub-gradient optimization to solve the relaxed problem and use a Lagrangean heuristic to build a feasible solution.

We diverge from this strategy and propose an arc-based decomposition strategy. Namely, we relax the linking constraints (\ref{mip02}) by defining the non-negative Lagrangean multipliers $\lambda_a^\omega$ for each arc-scenario couple. 
After re-arranging the terms we obtain the following relaxed formulation  (LRP): 

% \begin{align}
%  \min \emph{\ \ }z_{LR}& =\frac{1}{|\Omega|}\sum\limits_{\omega \in \Omega}\sum\limits_{i\in \mathcal{N}}
%  \left(1-\sum\limits_{(i,j)\in \mathcal{A}_{\omega}}\lambda_{(i,j)\omega}+\sum\limits_{(j,i)\in \mathcal{A}_{\omega}}\lambda_{(i,j)\omega}\right) y_i^\omega
%  -\sum\limits_{\omega \in \Omega}\left(\sum\limits_{(i,j)\in \mathcal{A}_{\omega}}\sum\limits_{k \in K}\lambda_{(i,j)\omega}\right) x_k
% \label{imbip1a} \\
%  \text{s.t.}& (\ref{mip01}),(\ref{mip03})-(\ref{mip06})
%  \end{align}

\begin{align}
\min \emph{\ \ }z_{LR}& =\frac{1}{|\Omega|}\left(\sum\limits_{\omega \in \Omega}\sum\limits_{i\in  A^\omega}
\left(1-\sum\limits_{a\in A_{i\omega}^{+}}\lambda_a^\omega+\sum\limits_{a\in A_{i\omega}^{-}}\lambda_a^\omega\right) y_i^\omega
-\sum\limits_{k \in K}\left(\sum\limits_{a\in A_k^\omega}\sum\limits_{\omega \in \Omega}\lambda_a^\omega\right) x_k\right)
\label{imbip1a} \\
\text{s.t.}& (\ref{mip01}),(\ref{mip03})-(\ref{mip06})
\end{align}

Observe that LRP is decomposable. The values of each $y_i^\omega$ can be determined by inspection. Simply, we set $y_i^\omega=1$, if $d_i^\omega=\left(1-\sum\limits_{a\in A_{i\omega}^{+}}\lambda_a^\omega+\sum\limits_{a\in A_{i\omega}^{-}}\lambda_a^\omega\right)\leq 0$. 
For the $x_{k}$ variables, together with constraint (\ref{mip01}), a $(0-1)$ knapsack problem arises, which can be easily solved by dynamic programming, where the Lagrangean cost coefficients $e_k=\sum\limits_{a\in A_k^\omega}\sum\limits_{\omega \in \Omega}\lambda_a^\omega$ are introduced.
When the unit cost version of MIP is under consideration, where all $c_k=1$, then the knapsack problem is trivially solved by simply ranking the $x_k$ variables with respect to increasing $e_k$ values and top $C$ of $x_k=1$.
For any set of non-negative multipliers $\lambda_a^\omega$, 
the objective function of the relaxed problem $z_{LR}$ yields a lower bound to the objective function of (MIP),
therefore one tries to obtain the optimal multiplier set that maximizes $z_{LR}$.
Now we are ready to provide the details of our Lagrangean Heuristic.

\subsubsection{Lagrangean Heuristic}
%Even though the formulation of (LRP) requires the forward reachable set $F_{i\omega}$, actually one need to only count the size of these sets and it is not mandatory to store them.
%This is achieved by setting all $\lambda_a^\omega=1$ initially \KT{(they are initialized as zero in the implementation)}.
%This yields an initial solution where all the $y_i^\omega$ to be zero and setting $x_k=1$ corresponding to the top $k$ $C_i$ values \KT{(do we mean the labels with largest $C_k$ values which can be selected within the blocking budget $C$, because we also allow different blocking costs, not only a cardinality constraint)}.
%Let this initial seed set \KT{of labels} be $S_0$. 
%Then the initial objective function value of (LRP) becomes $z^0_{LR}=\sum\limits_{k \in S_0}C_k$.
%And since all $\lambda_a^\omega=1$, it simply counts all the nodes that are activated by node $i$ thru active links summed over all scenarios.
%$S_0$ is also a feasible solution to (SAAP), so one can run the diffusion process starting from $S_0$ and count all the distinct accessible nodes for all scenarios to determine the initial lower bound. Let this objective function value be $z^0_{LB}$.

Lagrangean Heuristic together with subgradient optimization is an iterative procedure. For a given iteration, we define $Z_{LR}$ and $Z^*_{LR}$ to be the current and best (maximum) objective function value of (LRP), respectively.
Note that, $Z^*_{LR}$ is a lowerbound to (MIP). 
Also  let $Z_{LB}$ and $Z^*_{LB}$ be the current and best feasible solutions, respectively.
Let $T$ be the number of subgradient iterations since the best upper bound $Z^*_{LR}$ has changed and $\tau$ is the step length parameter used in the subgradient optimization.

Given these definitions, step-by-step explanation of the Lagrangean Heuristic is as follows:

\textbf{Step 0.} Initialize Lagrangean parameters $\lambda_a^\omega=0, \forall a\in A^\omega, \omega \in \Omega, T=0, \tau=2. $


\textbf{Step 1.} Solve (LRP) by inspection with the current set of $\lambda$ to obtain the current solution $\hat{x}, \hat{y}$ and objective function value $Z_{LR}$.
If $Z_{LR} \geq  Z^*_{LR}$, set $N=0, Z^*_{LR}=Z_{LR}$, otherwise set $N=N+1$. 

\textbf{Step 2.} Generate a feasible solution by running a breadth-first-search starting from the initial infected set over the live arcs whose labels are not blocked to compute $Z_{UB}$.
If $Z_{UB}\leq Z^*_{UB}$, set $Z^*_{UB}=Z_{UB}$ and $x^*=\hat{x}$.

\textbf{Step 3.} Stop if $Z^*_{LR}=Z_{UB}$, $Z_{UB}$ is the optimal objective function function value to (MIP) and $\hat{x}$ is the optimal label set.

\textbf{Step 4.} If $N=30$, then the subgradient procedure has performed 30 iterations without increasing the upper bound $Z_{LR}$, 
so we halve the step length parameter by setting $\tau=\tau/2$ and reset $N=0$.

\textbf{Step 5.} Calculate the subgradients:
$G_{a}^\omega= y_i^\omega-y_j^\omega-x_k$ for every $a\in\mathcal{A}_k^\omega, \omega \in \Omega$.

\textbf{Step 6.} If $\sum\limits_{\omega\in \Omega}\sum\limits_{i\in \mathcal{V}}G_{i\omega}=0$ or $\tau < \epsilon$ then go to Step 8.

\textbf{Step 7a.} Compute the step size $\psi=\frac{\tau (1.05Z^*_{LR}-Z^*_{LB})}{\sum\limits_{\omega\in \Omega}\sum\limits_{i\in \mathcal{V}}(G_{i\omega})^2} $.

\textbf{Step 7b.} Update the Lagrangean multipliers by $\lambda_a^\omega=max\{0,\lambda_a^\omega+\psi G_{a\omega}\}$.
If $\lambda_a^\omega$ has changed in this iteration, update all the $C_j$ values for all predecessors of $i$, i.e. for all $j \in \mathcal{P}_{i\omega}$.
Go to Step 1.

\textbf{Step 8.} Report $Z^*_{LR}$ as an lower bound to $(MIP), Z^*_{UB}$ as the best objective function value of (MIP) and $x^*$ as the best label set.
\iffalse
\vspace{0.5cm}
\begin{tabular}{ll}
Sets:\\
$V$: & Set of nodes \\
$A$: & Set of arcs \\
$K$: & Set of arc labels (relationship types)\\
$\Omega$: & Live-arc scenarios of the diffusion process (Independent Cascade or Linear Threshold)\\
$A_k^\omega$: & Set of live arcs with label $k$ in scenario $\omega$ \\
$I$: & Set of initially infected nodes \\
$V^\prime$: & \KT{The set of nodes that are not infected initially, $V\setminus I$}\\
Parameters:\\
$B$: & Amount of budget for blocking labels \\
$c_k$: & Cost of blocking label $k$\\
Decision variables:\\
$x_k$: &1 if label $k$ is blocked, 0 otherwise \\
$y_i^\omega$: &1 if node $i$ is infected in scenario $\omega$, 0 otherwise  %\\
%$p_r$: & probability of scenario $\omega$
\end{tabular}
\fi
%	\begin{definition}[Measure-based spread minimization problem]
%Let $G=(V,A)$ be a graph representing a network and $K$ be a finite set of labels. For each $k \in K$, we are given a \emph{measure cost} $c_k \geq 0$, and an arc set $A_k$, i.e., arcs having the label $k$, that are disjoint and satisfy $\cup_k A_k =A$. We are also given a set of initially infected nodes $I\subset V$, and a stochastic diffusion model $\mathcal{M}$. Let $\sigma_\mathcal{M}(G,I)$ denote the expected number of infected nodes due to a spread triggered by the seed set $I\subset V$ on network $G$, under a stochastic diffusion model $\mathcal{M}$. The \emph{measure-based spread minimization problem} consists of finding a set of measures to take (labels to block) within a budget \NA{$B$} so that the expected number of infected nodes is minimized. Formally it is defined as
%\begin{equation}
%\min_{K^\prime \subset K: c(K^\prime)\leq B} f(K^\prime)= \sigma_\mathcal{M}(G_{K^\prime},I) \tag{P} \label{eq:P}
%\end{equation}
%where $G_{K^\prime}=(V,A\setminus \cup_{k\in K^\prime} A_k )$ and $c(K')\leq B$ is the budget constraint.
%\end{definition}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.