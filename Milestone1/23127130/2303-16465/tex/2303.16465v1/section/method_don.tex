\input{figures/cube_definition}
\section{Method}
\label{sec:method}


\noindent \textbf{Overview.} Different from traditional keypoint fitting workflows~\cite{yu2018ec,wang2020pie,bazazian2021edc,matveev2022def}, we present a new paradigm to generate accurate parametric curves using a neural volumetric edge representation, named \emph{NerVE}. First, we introduce the definition of % neural edge in the volumetric grid.
NerVE and then propose a dedicated network to learn NerVE that supports direct estimation of structured 3D edges.
%Specifically, we integrate both advanced point convolutions and volumetric convolutions to extract features from input point clouds to generate our neural volumetric edges. 
These edges compose PWL curves that can not only provide precise edge positions but also contain valuable topology information, which eases the extraction of parametric curves. For the inferred PWL curves, we further utilize a simple post-processing procedure to correct their topology errors. Finally, parametric curves can be obtained using a straightforward graph search and spline fitting algorithm.
% \begin{comment}
% Finally, considering that the prediction of NerVE may produce some artifacts, we adopt a refinement network to reduce the edge inference error, and utilize a post-processing method to refine the extracted parametric curves, further improving the accuracy of parametric curve extraction from point clouds.
% \end{comment}

\subsection{NerVE Representation}

Learning 3D parametric curves from a point cloud is a challenging task due to the complexity of both curve types and connections. %containing diverse topology relationships. 
Existing works mainly resort to a keypoint fitting strategy and tend to generate noisy or error-prone results. In this work, we propose a unified edge representation, \modelName{}, that is compatible with different types of curves and is also feasible to be inferred with the latest volumetric learning techniques.
% is also suitable for learning using  deep neural networks.

\subsubsection{Volumetric Edge Definition}
\label{sec:ve_def}

We follow the voxel representation of a 3D shape~\cite{choy20163d} to discretize continuous 3D curves into a volumetric grid. As shown in Fig.~\ref{fig:cube_definition}, each cube in NerVE contains three attributes: 1) edge occupancy \symbcubeocc{}. $o\in\{0, 1\}$, which is a binary value to define whether the cube contains edges; 2) edge orientations \symbcubeori{}. $e_i\in\{0, 1\}, i=1,2,3$, which are 3 binary values that represent whether the cube should connect with its adjacent cubes to construct edge pieces. Note that a cube has 6 faces and shares with its neighbors, thus we define only three statuses on the left, bottom, and back faces in a cube; 3) edge point position \symbcubepos{}, which defines an edge point in the cube. With the three defined attributes, we can discretize continuous curves in a unified and regular volumetric representation, which can be learned directly via neural networks.

For NerVE, the higher resolution could keep more curve information but also increase the calculation consumption for network training and curve extraction. In our method, we use the resolution of $32^3$, which we found it works well in our experiments. We provide the ablation of different NerVE resolutions in Sec.~\ref{exp:ablation}. Regarding the determination of ground-truth  \symbcubepos{}, we generally choose the midpoint of the truncated curve inside a cube, which is natural and works well. For a special case where multiple curves appears in a cube, we calculate the average midpoint position or pick the endpoint (if any) as \symbcubepos{}. We also provide an ablation study about the selection of \symbcubepos{} in Sec.~\ref{exp:ablation}.

% \begin{comment}
% Note that we only define a single point in a cube, it may cause confusion when two curves are too close and appear in the same cubes. A natural solution is to increase the resolution of tessellations, similar to traditional process in 3D shape discretization and extraction~\cite{lorensen1987marching, ju2002dual}. In our experiments, we found that the resolution of $32^3$ works well in most cases. Regarding the selection of $p$, we choose the midpoint of the truncated curve inside the cube, which is natural and works well. We also provide an ablation study about the selection of $p$ in Sec.~\ref{exp:ablation}.
% \end{comment}


\subsubsection{PWL Curve Extraction}

Given our volumetric edge representation (i.e., NerVE), the underlying curves can be easily extracted in the form of PWL curves. Specifically, we pick the cube points $\{p\}$ when their occupancy statuses are true, and connect them with their neighbor points if the edge orientation statuses are true. Hence, a PWL curve is obtained, as illustrated in Fig.~\ref{fig:pipeline}. %Our NerVE directly represents structured edges, making the curve extraction much simpler than existing methods. Furthermore, it is a general solution for different curve types.

\subsection{NerVE Learning}

Given a point cloud $\mathcal{X}$ in the 3D space, we adopt a dense and simplified PointNet++~\cite{qi2017pointnet++} module and a simplified 3D CNN module~\cite{choy20163d} to extract efficient local features for the unorganized point cloud, then use an MLP-based module to generate our NerVE, denoted as $\{y_{(i,j,k)}\}$, where $y=(o,e,p)$ is the volumetric cube attributes at the position of $(i,j,k)$. We can formulate it as learning a function $\mathcal{F_\theta}$,
\begin{equation}
    \mathcal{F}_\theta: \mathcal{X} \longrightarrow \{y_{(i,j,k)}\}, \forall (i,j,k),
\end{equation}
where $\theta$ denotes the parameters in a neural network.


\subsubsection{Network Architecture}

Our network adopts an encoder-decoder paradigm to individually learn each attribute ($o, e, p$) in NerVE cubes (see Fig.~\ref{fig:pipeline}). Note that the encoder is comprised of point convolutions (1-dimensional convolution) and volume convolutions (3D CNN). We use the average pooling to connect these two kinds of convolutions, %using an intermediate point-aware cube feature fusion 
providing a feature grid for the following learning. The decoder is based on MLP layers.

\vspace{1mm}
\noindent \textbf{Encoder.} Our encoder is a dense and simplified PointNet++~\cite{qi2017pointnet++} combined with a 3D CNN module. Specifically, for each input point $x\in\mathcal{X}$, we first calculate its $k$-nearest neighbors ($k=8$ in our experiments, following~\cite{chen2022neural}), and then apply 4 MLP layers, each layer followed by a leaky ReLU activation, to extract 128-dimension point features. Then we utilize average pooling to fuse the features of points if they fall into the same cubes. Hence, we encode the point features into cubes. For these cubes without points inside, we initialize their features with zeros for the following 3D CNN learning. The 3D CNN has 3 3D-convolution layers, each having a kernel size of 3, stride length of 1, and padding size of 1. A leaky ReLU activation is appended after each convolution layer. %In our experiments, we found the zero initialization performs better than random initialization and learning-based initialization for our edge representation learning. \xiangyu{no such experiments, better not mention}

\vspace{1mm}
\noindent \textbf{Decoder.} We apply three decoders to learn the three cube attributes, respectively. Each decoder is comprised of 5 MLP layers with a leaky ReLU activation except the final one. Using the 128-dimension volumetric features from our encoder as input, we decode each cube feature to predict an occupancy \symbcubeocc{} and a point \symbcubepos{}. For the learning of edge orientation, we concatenate a cube feature with its three adjacent ones before decoding, as shown in Fig.~\ref{fig:pipeline}. Furthermore, we use a sigmoid function for the outputs of \symbcubeocc{} and \symbcubeori{}. %and a tanh function\xiangyu{no tanh or any activ, simply clip(not mention)} for the output of $p$.


\subsubsection{Training}
\label{sec:training}
% \begin{comment}
% In our experiments, we learn the three cube attributes $(c, f, p)$ separately, which can achieve better results than jointly learning the 7-dimension outputs. To be specific, we use the same network expect the output dimension (1 for $c$, 3 for $f$ and $p$) of the last layer and the output activation (sigmoid for $c$ and $f$, tanh for $p$) mentioned above. On the one hand, since $c, f, p$ can be regarded as independent attributes, separate learning could make the network focus on specific attribute learning and reduce the complexity of training networks. On the other hand, the significance of three attributes are different. More accurate inference of $c$ and $f$ can produce better edge topology, while more accurate $p$ can generate more precise edge position. They together construct the final edge results which benefit from separate learning. 
% \end{comment}

Compared to the 3D shape volume, the edge volume representation is much more sparse. To address the data imbalance problem in training and reduce the calculation consumption, we learn each cube attribute with specific masks, denoted \maskcubeocc{}, \maskcubeori{}, \maskcubepos{}. Specifically, given a point cloud, %the shape edges will only appear on the surface. 
we pick all the volumetric surface cubes (\maskcubeocc{}) to train their edge occupancy $\{o\}$. The edge orientations $\{e\}$ and edge points $\{p\}$ are learned from those occupied edge cubes. Thus, we use the ground-truth edge cubes as masks \maskcubeori{}, \maskcubepos{} to constrain that the edge orientations and edge point positions are learned in edge cubes during training.
During inference, we use the inferred edge occupancies $\{o\}$ as the mask to predict their edge orientations $\{e\}$ and edge points $\{p\}$. 

% \begin{comment}
% In our experiments, we also tried a modified $\mathcal{M}_c$ by learning the distance from sampled points to ground-truth edges (inspired by DEF~\cite{matveev2022def}), and used a small threshold to obtain a narrow shell as the mask to train cube $c$. This narrow shell will only warp the ground-truth edge region, making the masked area smaller. However, the improvement was insignificant but introduced a lot of consumption of calculation.
% \end{comment}

To train our network, we choose the binary cross entropy (BCE) loss for the learning of edge occupancy \symbcubeocc{} and edge orientation \symbcubeori{}, and adopt the $L_1$ loss for the regression of edge point \symbcubepos{}, i.e., 

\begin{align}
    L_o &= 1/|\mathcal{M}_o| \sum_{o \in \mathcal{M}_o} \text{BCE}(o, o_{gt}),
\end{align}
\begin{align}
    L_e &= 1/|\mathcal{M}_e| \sum_{e \in \mathcal{M}_e} \text{BCE}(e, e_{gt}), \\
% \end{align}
% \begin{align}
    L_p &= 1/|\mathcal{M}_p| \sum_{p \in \mathcal{M}_p} \left \| p - p_{gt} \right \|_1,
\end{align}
% \begin{align}
%     L_o &= 1/|\mathcal{M}_o| \sum_{o \in \mathcal{M}_o} \text{BCE}(o, o_{gt}),
% \end{align}
% \begin{align}
%     L_e &= 1/|\mathcal{M}_e| \sum_{e \in \mathcal{M}_e} \text{BCE}(e, e_{gt}), 
% \end{align}
% \begin{align}
%     L_p &= 1/|\mathcal{M}_p| \sum_{p \in \mathcal{M}_p} \left \| p - p_{gt} \right \|_1,
% \end{align}
where $o_{gt}, e_{gt}, p_{gt}$ are the ground-truth cube attributes in the corresponding masked cubes.

% \begin{comment}
% We also train a refinement network to improve the results of $c, f, p$. Given the inferred cube attributes and the input point cloud, we utilize a similar network to extract point features around the inferred cubes with the predicted cube status $c>0.5$ and decode them into the final outputs. However, the improvement is negligible, which also indicates that our learning with masks is efficient.
% \end{comment}

\input{figures/PWL2CAD}

\subsection{Parametric Curve Extraction}

Most existing methods for parametric curve estimation struggle with the endpoint estimation \& connection, and the curve type recognition (e.g., line, circle, B-spline) from a point cloud in the learning stage. In contrast, our representation of \modelName{} is unified for learning various types of curves. \modelName{} can directly predict structured edges in the form of PWL curves, establishing a topology graph of edge points, as shown in Fig.~\ref{fig:pwl2cad}. The parametric curve extraction is then reduced to a simple graph-search problem. In this paper, we focus on curve extraction of CAD models.
%The endpoints can be easily figured out by checking the degrees of cube points. Here, the degree of a cube point is the number of edges connecting with the point in the PWL curves. Then e find the connection between the endpoints under the guidance of the topology of PWL curves. Our representation is also general and suitable for all kinds of curves.\xiangyu{use a better expression}

For CAD models which are known as boundary representation (B-Rep), there are two constraints for parametric curves: 1) one open curve has only two endpoints, and two open curves can only be intersected at one endpoint; 2) the closed curves have zero endpoints~\cite{hofmann1989geometric,stroud2006boundary}. Given the PWL curves estimated from the inferred NerVE, we first choose the edge points with degree$>$2~\cite{weiler1986topological,stroud2006boundary} as endpoints. Here, the point degree is the number of edges that connect with the point in the PWL curves. Then we start from an endpoint and search along the PWL curve to obtain another endpoint. We thus fit with these two endpoints and all edge points on the path using an off-the-shelf spline fitting library. For the closed curve extraction, we just pick an edge point with a degree of 2 and search along the PLW curve graph until it ends up, and then fit with all points on the path. Our PWL curve graph greatly simplifies parametric curve extraction compared to previous methods.
% the method in~\cite{matveev2022def}

However, it is inevitable to produce artifacts for a learning-based method, reducing the accuracy of curve extraction. To this end, we propose a straightforward post-processing method to refine the topology of PWL curves extracted from NerVE. Specifically, we connect two points with degree 1 if the distance between these two points is smaller than a given threshold and their tangent vectors are consistent. We also remove superfluous curve segments if their lengths are very short. More details are provided in the supplemental.