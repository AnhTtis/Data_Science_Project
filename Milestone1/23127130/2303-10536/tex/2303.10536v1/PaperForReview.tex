% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{TempT: Temporal consistency for Test-time adaptation}
%\title{Improving Facial Expression Recognition Models Through Domain Adaptation}

\author{Onur Cezmi Mutlu, Mohammadmahdi Honarmand, Saimourya Surabhi, Dennis P. Wall\\
Stanford University\\
{\tt\small \{cezmi, mhonar, mourya, dpwall\}@stanford.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}
\maketitle

% In this technical report, we introduce our method for facial expression recognition (FER) on videos and explore the performance on the AffWild2 dataset as a part of Expression Classification Challenge at 5th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW). Our algorithm focuses solely on the unimodal visual aspect of the data and uses a popular 2D CNN backbone as opposed to sequential or attention-based models. We then enforce temporal consistency among consecutive model predictions on a per-frame basis to adapt the model to a given video. Our model shows competitive performance in comparison to previous years' reported performances and provides a proof of concept for in-the-wild personalized computer vision models.

%%%%%%%%% ABSTRACT
\begin{abstract}
     In this technical report, we introduce TempT, a novel method for test-time adaptation on videos by ensuring temporal coherence of predictions across sequential frames. TempT is a powerful tool with broad applications in computer vision tasks, including facial expression recognition (FER) in videos. We evaluate TempT's performance on the AffWild2 dataset as part of the Expression Classification Challenge at the 5th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW). Our approach focuses solely on the unimodal visual aspect of the data and utilizes a popular 2D CNN backbone, in contrast to larger sequential or attention-based models. Our experimental results demonstrate that TempT has competitive performance in comparison to previous years' reported performances, and its efficacy provides a compelling proof-of-concept for its use in various real-world applications. 
    %We invite readers to explore the potential of TempT in advancing computer vision
   % The ABSTRACT is to be in fully justified italicized text, at the top of the left-hand column, below the author and affiliation information.
   % Use the word ``Abstract'' as the title, in 12-point Times, boldface type, centered relative to the column, initially capitalized.
   % The abstract is to be in 10-point, single-spaced type.
   % Leave two blank lines after the Abstract, then begin the main text.
   % Look at previous CVPR abstracts to get a feel for style and length.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Affective computing aims to develop technologies with the capabilities like recognizing, interpreting, and simulating human affects. Expressions being one of the primary means of conveying emotion, facial expression recognition (FER) often constitutes an important part of human affective behavior analysis. There is an increasing number of use cases from driver safety applications to diagnosis and therapy of developmental problems of children \cite{kalantarian2019labeling}. With the continuous improvement in the computer vision field through extensive adoption of deep learning approaches, real world use of such algorithms is becoming easier and universal. However, robustness and reliability of aforementioned algorithms tend to suffer from the domain shift phenomena which is still a prominent problem for computer vision models with limited generalization capability.

In this work we address this phenomenon by treating each video as a different domain, i.e. coming from different distributions. We do not assume access to target domains during training time (validation and test data in this scenario) and aim to adapt to a given video at test time. Moreover, we do not assume access to labels of target domain samples, which puts our approach under the unsupervised/self-supervised adaptation class. These type of approaches are often referred to as Unsupervised Source-Free Domain Adaptation or Test-time Adaptation in the literature and we use insights from previous works when building our algorithm.


% \section{Related Work}

% FER papers
% Test time adaptation 
% source free domain adaptation



\section{Our Approach}
ABAW competition and Affwild2 dataset \cite{kollias2023abaw,kollias2022abaw,kollias2021distribution,kollias2021analysing,kollias2021affect,kollias2020analysing,kollias2019expression,kollias2019face,kollias2019deep,zafeiriou2017aff} are useful resources to benchmark the in-the-wild performance of FER algorithms. The task of video assessment at the frame level is a natural environment for machine learning models with spatiotemporal inductive biases, since the ability to model inter-frame relations could potentially be useful. Examples of such models are 3D convolutional neural networks (CNN) \cite{conv3d}, attention-based models \cite{transformer}, or hybrid approaches combining 2D CNNs with recurrent neural networks (RNN) \cite{cnnlstm}. First two of these approaches usually suffer from greater computational requirements than 2D CNN's, whereas the last method has unstable training time behavior under inputs with longer duration. There are numerous solutions to these problems but in our work we focus on exploring an adaptive approach where a simple 2D CNN model, which lacks useful biases for the setting, uses temporal predictive consistency as a self-supervision signal to adapt at test-time.

\begin{table*}[ht]
  \centering
  \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \toprule
    &Neutral&Anger&Disgust&Fear&Happiness&Sadness&Surprise&Other&\textbf{Total} \\
    \midrule
    Affwild2&44676&32962&7851&9730&2622&3296&5540&31412&\textbf{138089}\\
    Affectnet&55670&118605&19650&11647&5670&3626&19325&0&\textbf{234193}\\
    RAF-DB&3096&5771&2390&1571&347&865&846&0&\textbf{14886}\\
    \midrule   \textbf{Total}&\textbf{103442}&\textbf{157338}&\textbf{29891}&\textbf{22948}&\textbf{8639}&\textbf{7787}&\textbf{25711}&\textbf{31412}&\textbf{387168}
  \end{tabular}
  \caption{Label distribution of pretraining datasets}
  \label{tab:label_stats}
\end{table*}

\subsection{Datasets and Preprocessing}

Focusing on training a vision model that operates on images, we have numerous data sources that are popular in the FER literature. We combine Affwild2 with Affectnet \cite{mollahosseini2017affectnet}, Real-world Affective Faces Database (RAF-DB) \cite{rafdb1, rafdb2} to create a larger and more diverse training dataset. Affwild2 is significantly larger in comparison to the others and has a label imbalance as can be seen in  \cref{fig:affwild_train}, \cref{fig:affwild_val}. In order to overcome this, we perform a random sampling on it by limiting the number of frames to 300  per video per expression class basis. Detailed statistics of resultant dataset are given in \cref{tab:label_stats}.
\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{aff_wild_train.png}
    \caption{Label distribution of Affwild2 train set}
    \label{fig:affwild_train}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{aff_wild_validation.png}
    \caption{Label distribution of Affwild2 validation set }
    \label{fig:affwild_val}
  \end{minipage}
\end{figure}
We use provided cropped and aligned images in Affwild2, and others are only available in cropped versions, so we do not require any additional spatial preprocessing for any of the datasets. We then resize images to $112\times112px$ with antialiasing. For training purposes, we use common imagesaugmentation methods such as color jitter, brightness and contrast shift, histogram equalization, channel dropout, blur, and random horizontal flip. 


\subsection{Modeling approach}
Our approach is based on individual predictions on video frames, which allows us to use popular image processing architectures in the literature. Due to their proven performance and stability of training, we use models from Resnet \cite{resnet} family, in particular with aggregated residual transformations \cite{resnext} and squeeze-and-excitation blocks \cite{squeezeexcite}. Generated embeddings are processed by two fully-connected layers with second, i.e. output, layer is subject to weight and input normalization \cite{salimans2016weight} to prevent overconfidence, improve smoothness, and generalization. 

Supervised training of this model is performed with back-propagation algorithm using LDAM loss \cite{ldam} to account for significant data imbalance. Adam \cite{adam} optimizer with weight decay \cite{adamw} is used for optimization where learning rates were subject to a step-decay schedule. Modeling and training were performed using PyTorch \cite{paszke2019pytorch} framework on NVIDIA V100 GPUs.

\begin{figure*}
  \centering
    \includegraphics[width=\linewidth]{before_after.png}
    \caption{Model predictions before (dashed) and after (solid) adaptation}
    \label{fig:before_after}
\end{figure*}

\subsection{Adaptation to video}

Being trained on static images as opposed to videos, 2D CNN models do not have an implicit bias for the smoothness and or consistency in their predictions across frames. We propose using this fact to generate a supervision signal to adapt the network and improve classification performance. In particular, we temporally smooth the model predictions using a low-pass filter and set it as the desired signal. We calculate the mean-squared error between the original and target signals and use back-propagation to update a subset of model parameters. 

More formally, let $x_t^i\in \mathbb{R}^{112\times112\times3}$ be the $t^{th}$ frame of $i^th$ video and $f(.):\mathbb{R}^{112\times112\times3}\rightarrow\mathbb{R}^{8}$ be the trained neural network of interest. We first pass all frames from the pipeline to obtain an initial set of unnormalized scores $y_t^i\in\mathbb{R}^{8}$. Empirically we found these time series to contain strong high-frequency components and when they are subject to a low pass filter, the results look more desirable. Originating from this idea, we propose using the error signal in \cref{eq:loss_signal} as a self-supervision loss function to fine-tune the model (index for video is omitted for simplicity). $LPF(.)$ can be any low pass filter; in our experiments, we use a median filter, due to its robustness to outliers.

\begin{equation}
\label{eq:loss_signal}
    \mathcal{L}(x) = \sum_{t} \| x_{t} - LPF(x)_t \|
\end{equation}

Being differentiable, this loss allows the use of backpropagation to update model parameters. The choice of parameters has an important effect on the performance of the adapted model since the selection defines the expressivity of the model. Following the analysis in \cite{bn_freeze_adapt}, we select this subset to be the weight and bias terms in batch normalization layers, while freezing the running statistics. This has been shown to yield enough expressivity while preventing overfitting.

We randomly sample frame sequences of equal length from the given video and use this loss function to train target parameters. We again use Adam optimizer for the adaptation process and take 10 training steps, a number that has proven empirically optimal in our hyperparameter searches. 

A sample set of time series showing the effect of adaptation can be seen in \cref{fig:before_after}. The adaptation clearly reduces the number of changes in the prediction, and provides a more coherent predictions over time.


\section{Experiments}

We tested our algorithm on AffWild2 dataset. We performed an extensive hyperparameter search on the adaptation parameters, such as number of steps, learning rate, optimizer etc., and report the performance of best configuration in \cref{tab:results}. Static models' performances are deterministic whereas for adaptation cases we report average F1 score over 20 experiments to account for stochasticity arising from random sampling of frames.

\begin{table}
  \centering
  \begin{tabular}{@{}lc@{}}
    \toprule
    Method & F1 Score \\
    \midrule
    Resnet-18 & 0.307\\
    SE-ResNext-101 & 0.325 \\
    Resnet-18 + Adaptation & 0.323\\
    SE-ResNext-101 + Adaptation& 0.345 \\
    \bottomrule
  \end{tabular}
  \caption{Average F1 Score performances on validation set}
  \label{tab:results}
\end{table}


% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    %\includegraphics[width=0.8\linewidth]{egfigure.eps}

%    \caption{Example of caption.
%    It is set in Roman so that mathematics (always set in Roman: $B \sin A = A \sin B$) may be included without an ugly clash.}
%    \label{fig:onecol}
% \end{figure}

% \begin{figure*}
%   \centering
%   \begin{subfigure}{0.68\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{An example of a subfigure.}
%     \label{fig:short-a}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.28\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{Another example of a subfigure.}
%     \label{fig:short-b}
%   \end{subfigure}
%   \caption{Example of a short caption, which should be centered.}
%   \label{fig:short}
% \end{figure*}

\section{Conclusion and Future Work}
In our work, we explored a novel algorithm which is model agnotsic that can have real-life applications for similar tasks, and have shown that this adaptive method can enhance model performance without any additional means of supervision. On the other hand, performance variance due to stochasticity in the sampling from the target video is a problem that needs to be solved to obtain a more deterministic understanding of the limits and behavior of the algorithm.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
