\documentclass[twoside,11pt]{article}

\usepackage[preprint]{jmlr2e}
\usepackage{tcolorbox}
\usepackage{color-edits}
\usepackage[scaled=0.8]{DejaVuSansMono}
\usepackage{enumitem}

\ShortHeadings{Fairlearn}{Weerts, Dud{\'i}k, Edgar, Jalali, Lutz, and Madaio}

\firstpageno{1}
\begin{document}
\title{Fairlearn: Assessing and Improving Fairness of AI Systems}

\author{\name Hilde Weerts\textsuperscript{1} \email h.j.p.weerts@tue.nl \\
        \name Miroslav Dud{\'i}k\textsuperscript{2} \email mdudik@microsoft.com \\
        \name Richard Edgar\textsuperscript{2} \email riedgar@microsoft.com \\
        \name Adrin Jalali
        \email adrin.jalali@gmail.com \\
        \name Roman Lutz\textsuperscript{2} \email romanlutz@microsoft.com \\
        \name Michael Madaio\textsuperscript{2}\thanks{Michael is currently employed by Google, but contributed to this work while at Microsoft.} \email madaiom@google.com \\
        \addr
            The authors are the current maintainers of Fairlearn, and additionally have the following affiliations:
        \\
        \addr
            \textsuperscript{1}Eindhoven University of Technology,
            \textsuperscript{2}Microsoft
       }

\editor{Editor Name}

\maketitle

\begin{abstract}
Fairlearn is an open source project to help practitioners assess and improve fairness of artificial intelligence (AI) systems. The associated Python library, also named \emph{fairlearn}, supports
evaluation of a model's output across affected populations and includes several algorithms for mitigating fairness issues. Grounded in the understanding that fairness is a sociotechnical challenge, the project integrates learning resources that aid practitioners in considering a system's broader societal context.
\end{abstract}

\begin{keywords}
  algorithmic fairness, artificial intelligence, machine learning, Python
\end{keywords}

\defcitealias{ec2021proposal}{EC, 2021}
\defcitealias{ostp2022blueprint}{OSTP, 2022}


\section{Introduction}
As artificial intelligence (AI) impacts more of our everyday lives,
there is a growing need to ensure that algorithmic systems do not disproportionately harm minorities, historically disadvantaged populations, and other groups considered sensitive from an ethical or legal perspective
\citep{crawford2013hidden, o2016weapons, broussard2018artificial, noble2018algorithms, benjamin2019race}.
Fairness of AI systems is a topic of multiple academic venues,\footnote{%
  \url{https://facctconference.org/} (FAccT)\\
  \url{https://www.aies-conference.com/} (AIES)}
a priority for regulators~\citepalias{ec2021proposal,ostp2022blueprint},
and a focus of several open source projects~\citep{lee2021landscape}.

In this paper, we describe Fairlearn, an open source project that
seeks to help data science practitioners with assessing and improving fairness of AI systems.
The project consists of a Python library, called \emph{fairlearn}, accompanied with various learning resources. Both the library and the learning resources are licensed under MIT license
and available online.\footnote{%
  \url{https://github.com/fairlearn/fairlearn}\\
  \url{https://fairlearn.org}}
%
The library aims to provide an easy-to-use API that blends well with popular libraries of the Python ecosystem, including \emph{scikit-learn} \citep{scikit-learn}, \emph{pandas} \citep{pandas}, \emph{matplotlib} \citep{matplotlib}, \emph{TensorFlow}  \citep{tensorflow}, and \emph{PyTorch} \citep{pytorch}. Through our learning resources, we hope to provide practitioners with the knowledge and skills to effectively assess and mitigate unfairness.

Fairlearn is a community-driven project with independent governance,\footnote{%
The project started
in May 2018 as a Microsoft open source project and its initial scope was outlined in a Microsoft technical report~\citep{bird2020fairlearn}, but it transitioned to independent governance in~2021:\\\url{https://github.com/fairlearn/governance/blob/main/ORG-GOVERNANCE.md}}
following a code of conduct adapted from the Contributor Covenant.\footnote{\url{https://www.contributor-covenant.org}}
The project is under active development and welcomes community contributions to the source code and the learning resources.

\paragraph{Fairlearn Perspective on AI Fairness.}

In Fairlearn, we consider AI fairness through the lens of fairness-related harms~\citep{crawford2017trouble}, by which we mean negative impacts for groups of people, such as those defined in terms of race, gender, age or disability status.

Development of Fairlearn is firmly grounded in the understanding that fairness of AI systems is a sociotechnical challenge \cite[cf.][]{green2021contestation}. Because there are many complex sources of unfairness---some societal and some technical---it is not possible to fully ``de-bias'' a system or to guarantee fairness \cite[e.g.,][]{blodgett2020language}. Instead, our goal is to help practitioners assess fairness-related harms, review the impacts of different mitigation strategies, and make trade-offs appropriate to their scenario. This may sometimes mean advocating for not deploying the system at all \citep{baumer2011implication}. AI fairness is related to, but distinct from anti-discrimination laws~\citep{xiang2019legal}, so our documentation avoids (mis)use of legal terminology \citep{watkins2022four} and encourages users to understand what fairness means for their sociotechnical context before applying or adapting Fairlearn.\looseness=-1

Fairlearn largely focuses on two types of fairness-related harms: \emph{allocation harms} and \emph{quality-of-service harms}.
Allocation harms occur when AI systems are used to allocate opportunities or resources in ways that can have significant negative impacts on people's lives, for example,
when an AI system for recommending patients into high-risk care management programs is less likely to select Black patients than white patients of similar health~\citep{obermeyer2019dissecting}. Quality-of-service harms occur when a system does not work as well for members of one group as it does for members of another group, for example, when a computer vision system has higher error rates for images of women with darker skin than for images of men with lighter skin~\citep{buolamwini18gender}.

\section{Fairness Assessment}

One of the key goals of the \emph{fairlearn} library is to support fairness assessment.
The goal of fairness assessment is to answer the question: \emph{Which groups of people may be disproportionately negatively impacted by an AI system and in what ways?}
In the context of allocation and quality-of-service harms, this means to evaluate how well the system performs for different population groups by calculating some performance metric, like an error rate, on different slices of data. This is called \emph{disaggregated evaluation}~\citep{barocas2021designing}.

\paragraph{\texttt{MetricFrame} class.}
The primary tool for disaggregated evaluation in the \emph{fairlearn} library is the \path{MetricFrame} class in the \path{fairlearn.metrics} module. Its API combines \emph{scikit-learn} and \emph{pandas} conventions. In its simplest form, \path{MetricFrame} is initialized by providing one or more metric functions together with input arrays \path{y_true}, \path{y_pred}, and \path{sensitive_features}. The first two arrays serve as inputs to metric functions, whereas the \path{sensitive_features} array is used to split the data into slices for disaggregated evaluation.
Once a \path{MetricFrame} is constructed, the disaggregated metrics can be accessed as a \emph{pandas} \path{Series} (for a single metric) or a \emph{pandas} \path{DataFrame} (if multiple metrics are provided).
\path{MetricFrame} also enables a comparison of metric values across groups, for example, in terms of differences or ratios. Plotting of results is supported via existing integration of \emph{pandas} with \emph{matplotlib}.

\paragraph{Fairness Metrics.}

The module \path{fairlearn.metrics} also provides metric functions that return scalars much like typical \emph{scikit-learn} metrics.
For example, functions \path{demographic_parity_difference} and \path{equalized_odds_difference} quantify how much
the predictions of a given classifier depart from the fairness criteria known as \emph{demographic parity} and \emph{equalized odds} \citep[see, e.g.,][]{hardt2016equality}. These two metrics are derived from a \path{MetricFrame} with a specific choice of input arguments.
New fairness metrics can be obtained by using the \path{make_derived_metric} function, which wraps some of the \path{MetricFrame} functionality.

\paragraph{Comparison of Multiple Models.}

In addition to assessing fairness of a single model, \path{fairlearn.metrics} also enables a comparison of multiple models. For example, the function \path{plot_model_comparison} can be used to create a scatter plot, where each model is represented as a point with one coordinate equal to a metric quantifying overall performance and the other to a metric quantifying fairness, like the metrics from the \path{fairlearn.metrics} module.

\section{Algorithmic Mitigation of Fairness-related Harms}

The \emph{fairlearn} library includes several methods for mitigating fairness-related
harms. Many of the included methods are \emph{meta-algorithms} in the sense that they act as wrappers around any standard (i.e., fairness-unaware) machine learning algorithms. This makes them quite versatile in practice. All of the implementations follow
the API conventions of \emph{scikit-learn}.

Following \citet{barocas-hardt-narayanan}, \emph{fairlearn} mitigation algorithms can be divided into three groups according to when they are applied relative to model training:

\paragraph{Pre-processing.}
Algorithms in this group mitigate unfairness by transforming input data before it is passed to a standard training algorithm.
For example, \path{CorrelationRemover} in the module \path{fairlearn.preprocessing} applies
a linear transformation to input features in order to remove any correlation with sensitive features. It follows the API
of a \emph{scikit-learn} transformer and therefore can be incorporated in a \emph{scikit-learn} pipeline.

\paragraph{In-training.\protect\footnote{Also called \emph{in-processing} by some authors~\citep{kamiran2013techniques}.}}
%
Algorithms in this group directly train a model to satisfy fairness constraints.
For example, the meta-algorithm \path{ExponentiatedGradient} in the module \path{fairlearn.reductions} implements
the reduction approach of \citet{agarwal2018reductions,agarwal2019fair}. This meta-algorithm supports a wide range of fairness constraints and wraps any standard classification or regression algorithm, such as \path{LogisticRegression} from \path{sklearn.linear_model} or \path{XGBRegressor} from \path{xgboost}. An input to a reduction algorithm is an object that supports training on any provided (weighted) data set as well as a data set that includes sensitive features. The goal is to optimize a performance metric (such as classification accuracy) subject to fairness constraints (such as an upper bound on a difference between false negative rates).\looseness=-1

As another example, \path{AdversarialClassifier} and \path{AdversarialRegressor} in the module
\path{fairlearn.adversarial} implement the adversarial mitigation approach of \citet{zhang2018mitigating}. These algorithms simultaneously train two neural network models, a predictor model and an adversarial model. The predictor model seeks to minimize the prediction loss function while also ensuring that the adversary model cannot infer sensitive features from the predictor outputs.
The predictor and adversary neural nets can be defined either as \emph{PyTorch} modules or \emph{TensorFlow} models.

\paragraph{Post-processing.}
Algorithms in this group transform the output of a trained model.
For example,
\path{ThresholdOptimizer} in the module \path{fairlearn.postprocessing} implements the approach of \citet{hardt2016equality}, which takes in an existing (possibly pre-fit) machine learning model, uses its predictions as a scoring function, and identifies a separate threshold for each group defined by a sensitive feature in order to optimize some specified objective (such as balanced accuracy) subject to specified fairness constraints (such as false negative rate parity). The resulting classifier is a thresholded version of the provided machine learning model.\looseness=-1

\section{Learning Resources}

Tackling fairness-related harms requires more than technical tools alone \citep{holstein2019improving}. In a community-based effort, we have developed a comprehensive set of \textit{learning objectives}
that highlight what practitioners should know or be able to do when assessing and improving fairness of AI systems. These objectives are the basis for our learning resources.\looseness=-1

To avoid divorcing technical and social aspects of AI fairness, our learning resources are integrated
in the API reference and user guide of the \emph{fairlearn} library. Besides coding examples and explanations, our user guide covers important concepts central to understanding machine learning models as part of a sociotechnical system, such as construct validity~\citep{jacobs2021measurement} and the risks of abstracting away social context~\citep{selbst2019fairness}.\looseness=-1

Examples are crucial when learning to view fairness from a sociotechnical perspective. We provide tutorials~\citep[e.g.,][]{scipytutorial2021} and example notebooks downloadable in the Jupyter format~\citep{kluyver2016jupyter}. We try to ensure that each notebook describes a real-world or realistic deployment context, focuses on real harms to real people, and avoids \textit{abstraction traps}~\citep{selbst2019fairness}.

The data sets provided in the module \path{fairlearn.datasets} also serve an educational role, as we use them to highlight sociotechnical aspects of fairness, with sections of the user guide highlighting fairness-related issues with several popular benchmark data sets.


\section{Conclusions}

Fairlearn is built and maintained by contributors with a variety of backgrounds and expertise. We believe that meaningful progress toward fairer AI systems
requires input from a breadth of perspectives. We therefore encourage researchers,
practitioners, and other stakeholders to contribute to Fairlearn as we experiment, learn, and evolve the project together.\looseness=-1

\newpage
\acks{We would like to thank
Sarah Bird, Brandon Horn, Vanessa Milan, Mehrnoosh Sameki, Hanna Wallach, and Kathleen Walker
for their critical contributions to the initial Fairlearn project~\citep{bird2020fairlearn}.
We would also like to thank all the members of the Fairlearn community who have contributed to the project in various ways, including documentation, code, bug reports, feature requests, and participation in our community calls. In particular, we would like to thank Michael Amoako, Alexandra Chouldechova, Parul Gupta, Laura Gutierrez Funderburk, Abdul Hannan Kanji, Kenneth Holstein, Lisa Iba\~nez, Sean McCarren, Manojit Nandi, Ayodele Odubela, Rens Oostenbach, Alex Quach, Kevin Robinson, Allie Saizan, Bram Schut, and Vincent Warmerdam for their valuable contributions.\looseness=-1}

\bibliography{sample}

\end{document}
