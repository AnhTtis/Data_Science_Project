\section{Security and Privacy}\label{sec_privacy}
%% table for Security and Privacy
\begin{table*}
\renewcommand\arraystretch{1.35}
	\caption{Summary of the security and privacy methods}\label{privacy}
	\centering
\linespread{1}\selectfont
		\begin{tabular}{|p{2.5cm}<{\centering}|p{2cm}|p{2.5cm}|p{3cm}<{\centering}|p{3cm}<{\centering}|p{2.5cm}<{\centering}|}
			\hline \bf{Category} & \bf{Literature} & \bf{Method} & \bf{End Device Layer} & \bf{Edge/Fog Layer} & \bf{Cloud Layer} \\ \hline
			\hline \multirow{6}{*}{\shortstack{Privacy-preserving\\ Video Collection}} & Kim \emph{et al.} \cite{Kim2014ICUAS} & Filtering, Encryption & Filtering frames according to preset privacy map & Applying encrypt operation& Applying decrypt operation \\
			\cline{2-6} &  \cite{Wang2017MMSys, Wang2018TOMCCAP} & Denaturing, Advanced encryption system (AES) & $ \times  $ & Using denaturing to blur faces and encrypt raw video & Data decryption \\
			\cline{2-6} & Wang \emph{et al.} \cite{Wang2019Access} &  Advanced encryption system (AES)  & $\times $ & Using AES to generate secret face feature & Split the secret feature into all edge servers for storing  \\
			\cline{2-6} & Li \emph{et al.} \cite{Li2018IOTJ} & Encryption & Encrypting raw video & Verifying and aggregating the encrypted data & Data decryption\\
			\cline{2-6} & Zarepour \emph{et al.} \cite{Zarepour16PerCom} & Filtering & Applying filtering operation & $\times$ & $\times$ \\
			\cline{2-6} & \cite{wang2019arXiv, xu2018Communications_Magazine,mao2018USENIX} & Differential Privacy & Adding differential noise & Analyze obfuscated data & Analyzing obfuscated data \\
			
			\hline \multirow{5}{*}{\shortstack{Privacy-preserving \\Video Analytics}} & \cite{Jiang17DSC,Khazbak2019IoTDI,Li2019FCS} & Homomorphic Encryption  & $\times$ & Homomorphic Encryption & Video analytics on encrypted data directly \\
			\cline{2-6} & Akkaya \emph{et al.} \cite{akkaya2019CNS} & Background Subtraction, Homomorphic Encryption & Background subtraction on UAV & Homomorphic encryption & Further analytics on encrypted data \\
			\cline{2-6} & \cite{sada2019DASC, chen2019GLOBECOM, liu2020arXiv} & Federated Learning & $\times$ & The federated learning layer deployed on edge & $\times$ \\
			\cline{2-6} & \cite{xu2019arXiv_3, osia2020IOT} & NN-based Obfuscation & $\times$ & Using NN to extract features from video frames & Further analytics on frame features \\
			\cline{2-6} & Hu \emph{et al.} \cite{Hu2021Springer} & DP-empower Federated Learning & $\times$ & Training local model at the edge and conduct perturbation on model parameters & maintaining public model  \\

			\hline \multirow{2}{*}{\shortstack{Privacy-preserving \\Video Storage}} & \cite{Nigel2016HotMobile,Wang2017MMSys} &  Encryption  & $\times$ & Trusted edge servers provide data storage & $\times$ \\
			\cline{2-6} & \cite{Wang2020JPDC, Xiao2020Sensors} & Frame data dividing& $\times$ & Dividing video data into different parts and storing on edge layer &  $\times$ \\
			\hline
	\end{tabular}
\end{table*}

People are extremely concerned about privacy leakage, especially for video applications with rich personal information. Privacy protection enables individuals to have a certain degree of control over their sensitive data, preventing it from being abused by third parties. In cloud-based video analytics, the privacy-preserving operations need to be locally executed, which demands a high level of computation resources and a low complexity on the encryption algorithms. With the help of edge servers, the privacy-preserving operations can be executed at edge servers, avoiding straight exposure of privacy information at the cloud. For a video analytics-based application, massively captured video data commonly go through the following three phases: \emph{Data Collection}, \emph{Data Analytics} and \emph{Data Storage}. Different phases suffer from different privacy risks and corresponding privacy preserving mechanisms are required for privacy protection. In this section, we summarize the related works on these three categories.


\subsection{Privacy-preserving Video Collection}

In the stage of data collection, privacy concerns can come from unreliable application owners as well as unreliable network conditions. For video analytics applications, some sensitive information (e.g., facial information and the ID number) should not be collected by an untrusted party in avoid of being illegally used. Besides, in over-the-air transmission of raw video contents, there is a giant risk of information leakage against eavesdroppers. Considering these issues, it is essential to design privacy-preserving video collection solutions. As an efficient and proven solution, it is the features not the raw contents that should be transmitted to the edge or cloud servers. For example, it is sufficient to transmit the outline of a pedestrian without the need for facial information in a pedestrian counting application. Thus, the facial information as the private source should not be exposed to the central cloud server, and thus, the privacy can be reserved. Based on the integrity of the collected video contents, we summarize recent works as \emph{complete collection} and \emph{partial collection} as follows.


\subsubsection{Complete Collection}

Encryption methods are widely used for a full collection of video contents.
Kim \emph{et al.} \cite{Kim2014ICUAS} encrypted video frames on edge servers in the surveillance station and then delivered encrypted frames to a trusted third-party cloud server.  The specific regions of a video frame are first filtered out according to a preset privacy map and then decrypted into original video data with a shared key at the cloud server. However, privacy map cannot provide a more fine-grained and automated privacy protection. Besides, it does not always  provide a trusted third-party server as a medium between video data and video subscribers. 
Li \emph{et al.} \cite{Li2018IOTJ} proposed a privacy-preserving data aggregation scheme, where the edge server aggregates the encrypted data from terminal devices and sends data to the cloud server, which can then decrypt the aggregated data through its private key. In this kind of privacy-preserving systems, edge servers always serve as an intermediate layer to provide privacy protection for the entire system. 

Traditional encryption methods are generally computation-intensive, which might not work well on resource-limited edge servers. 
To address such deficiency, Wang \emph{et al.} \cite{wang2019arXiv} proposed a VideoDP platform that provides a novel differential privacy (DP) function. In VideoDP, adding or removing any sensitive visual element into/from the input video does not significantly affect the analytical result.
Xu \emph{et al.} \cite{xu2018Communications_Magazine} proposed a local DP obfuscation framework for data analytics, where data is distilled in edge servers with limited ability to make inferences about users' sensitive data. 
Mao \emph{et al.} \cite{mao2018USENIX} proposed to partition a DNN model after the first convolutional layer between the end device side and the edge server side. Then, DP is applied to protect  convolutional layers to guarantee the privacy of users' sensitive data.


\subsubsection{Partial Collection}

Recall the frame cropping technology introduced in Sec. \ref{sec_frame_crop}, many video analytics-related applications focused on RoIs in each video frame, such as the face regions in face detection and the vehicle regions in vehicle monitoring. Thus,  end devices (e.g., cameras) should also enhance the privacy preservation in partial video collection.

Neural networks can offer more granular privacy protection measures. OpenFace \cite{Wang2017MMSys, Wang2018TOMCCAP} implemented a privacy-preserving data collection mechanism by denaturing video data on the edge server instead of directly sending the raw video to the cloud. This technology selectively blurs faces that appear in video frames to alleviate privacy concerns related to face data. Similarly, Wang \emph{et al.} \cite{Wang2019Access} proposed a privacy-preserving face verification system, which applies the edge server to extract face feature by CNNs, as well as encryption of the feature data before sending to the cloud server by using advanced encryption system (AES). Zarepour \emph{et al.} \cite{Zarepour16PerCom} introduced a novel context-aware privacy-preserving framework, which uses the contextual information to estimate the set of potential sensitive subjects in each image. In this framework, user activity extraction and sensitive information filtering are completed on the end device locally before publishing the raw image.



\subsection{Privacy-preserving Video Analytics}

During video analytics, edge servers with limited computing power may need to process sensitive information on untrusted platforms. Therefore, before the original data is submitted to a untrusted platform for further analysis, corresponding preprocessing should be performed at the edge, such as encryption or abstract knowledge extraction. A crucial question is how to apply the computing power of the cloud server without exposing the privacy of raw video. There are three technologies that can be possible solutions, which are \emph{encryption-based technology}, \emph{obfuscation-based technology} and \emph{privacy-preserving machine learning}.



\subsubsection{Privacy-preserving Training}

In an edge-based video analytics system, data can be easily collected by terminal devices. However, plain-text data cannot be directly sent to the cloud server when users are concerned about privacy. Instead, an edge server can use its own data to conduct model training locally without sharing data with the cloud server, or use a neural network to perform preprocessing.

The use of edge computing has enabled video analytics applications to benefit from low-latency and distributed data processing services by leveraging the storage and computing capabilities of nearby end devices. However, in a distributed environment, the training process remains challenging due to the fragmented knowledge base across edge and cloud servers.
As a promising solution, \emph{federated learning} enables a distributed server to train its model locally and does not need to share local data with others. Thus, federated learning greatly alleviates the risk of privacy leakage caused in data sharing. 
Sada \emph{et al.} \cite{sada2019DASC} proposed an edge-based video analytics architecture, using federated learning to update object detection models and avoid sending the local data to the cloud. The federated learning layer is deployed in an edge server, which is situated between end devices and the cloud server. 
Similarly, Chen \emph{et al.} \cite{chen2019GLOBECOM} proposed a distributed learning framework that can be trained at each base station and cooperatively builds a learning model which can predict the mobility and orientations of users. 
% Liu \emph{et al.} \cite{liu2020arXiv} developed an end-to-end platform called FedVision that enables the easy development of FL-powered computer vision applications. The platform addresses the challenges of developing effective visual object detection models using image data owned by multiple organizations through FL. This work represents the first industry application of FL in computer vision-based tasks, and has the potential to help organizations comply with stricter data privacy protection laws, such as GDPR.
Liu \emph{et al.} \cite{liu2020arXiv} developed a platform called FedVision, which allows for the development of federated learning powered computer vision applications. It aims to develop effective visual object detection models by utilizing image data owned by multiple organizations through federated learning. FedVision is the first industry application of FL in computer vision-based tasks, and it has the potential to assist organizations in complying with stricter data privacy protection laws, such as GDPR.

 
Although federated learning does not require the transmission of raw data during training, the attacker may still obtain user privacy from the exposure of gradients. For example, \cite{melis2019sp} shows that an attacker can infer whether a participant's data has been included in the dataset by collecting and analyzing shared models with an accuracy of 90\%. 
In federated learning paradigm, there are several methods that can be used to improve the video analytics privacy-preserving level. The main methods include the \emph{homomorphic encryption} method and the \emph{differential privacy} method.


\emph{(i) Homomorphic encryption-empowered solution}

Homomorphic encryption \cite{FHE_Gentry2009} enables data to be analyzed and manipulated while still encrypted, without the need for decryption.
Jiang \emph{et al.} \cite{Jiang17DSC} explored the use of homomorphic encryption to perform scale-invariant feature transform on encrypted images, enabling data analytics to be performed directly on encrypted data. To enable homomorphic encryption operations on resource-constrained edge servers, TargetFinder \cite{Khazbak2019IoTDI} applied optimization techniques to reduce computation overhead of cryptography primitives. These technologies enable secure and privacy-preserving image processing and analysis on edge devices with limited resources.
Li \emph{et al.} \cite{Li2019FCS} proposed a novel framework for privacy-preserving computing that utilizes lightweight permutation-substitution encryption and homomorphic encryption on end devices. This edge-assisted framework offloads the burden of computation, communication and storage while ensuring data security.
Akkaya \emph{et al.} \cite{akkaya2019CNS} proposed to perform background subtraction to get the foreground to transmit, and thus reduce the size of the data to be transmitted and the computational cost for applying homomorphic encryption. In this way, the receiver can aggregate the background and foreground to do further analytics solely based on the encrypted data.
Ma \emph{et al.} \cite{ma2018INFOCOM} presented a privacy-preserving motion detection algorithm for HEVC compressed videos which operates in the compressed domain. It can detect the coarse-grained shapes of moving objects and estimate the motion trajectory without decoding the video. By searching in the compressed-domain, the algorithm can preserving the compression efficiency of the video codec without incurring extra transmission bandwidth or storage overhead.

\emph{(ii) Differential privacy-empowered solution}

Without the high computation burden of the homomorphic operation, differential privacy provides a lightweight solution for the federated learning paradigm. Differential privacy-empowered solution adds zero-mean ``noises" to the trained parameters by using some randomized mechanism which is called differential privacy-preserving, such as Gaussian mechanism. 
Hu \emph{et al.} \cite{Hu2021Springer} proposed \emph{FedEVA}, a distributed training framework for edge video analytics that protects user privacy with fast convergence rates. The framework implements local differential privacy (LDP) on user updates before sending  gradients to the parameter server, which then updates the neural network model based on the perturbed gradients. Experimental results shows that the proposed framework can ensure privacy preservation while maintaining the same convergence rate.


\subsubsection{Privacy-preserving inference}

We have seen that model partition technologies can extract abstract features of raw video data on the edge layer, while further analytics tasks are completed on the cloud. However, input recovery attacks can occur during inference, aiming to recover raw image data from image features. \emph{Privacy-preserving inference} pays more attention to resisting input recovery attacks during the model inference phase. 
% Chi \emph{et al.} \cite{chi2018arXiv} proposed a new framework for protecting user data during the model inference phase where users use their data to get classification results. The output of a deep learning neural network construction is comprised of multiple intermediate layers that encode information regarding the previous layers, providing a channel for unauthorized access to privacy-sensitive data. 
Chi \emph{et al.} \cite{chi2018arXiv} proposed a framework to ensures user data privacy during the model inference stage, when users utilize their data to obtain classification results. This framework addresses the potential for unauthorized access to privacy-sensitive information like encoding information about previous layers, which may occur due to the presence of multiple intermediate layers in the output of a deep learning neural network. 
Osia \emph{et al.} \cite{osia2017privacy} introduced a method to manipulate the extracted features by altering the training phase when applying the Siamese network \cite{chopra2005learning} and a noise addition mechanism for improving privacy protection. Moreover, they applied transfer learning and deep visualization techniques to quantify the privacy guarantees of their approach. 
Similarly, Osia \emph{et al.} \cite{osia2020IOT} proposed a novel approach that leverages an edge device to run the initial layers of a neural network for protecting user privacy. The output is then sent to the cloud to process the remaining layers and produce the final results. To further improve privacy protection, they employed Siamese \cite{chopra2005learning} fine-tuning to ensure that only necessary information is contained on the user's device for the main task, thus preventing any secondary inference on the data.
Xu \emph{et al.} \cite{xu2019arXiv_3} presented a lightweight and unobtrusive approach to obfuscating the inference data at user devices. The edge servers only need to execute a lightweight neural network to obfuscate the inference data implying that thus the neural network can be easily deployed on a resource constrained edge server or device introducing light compute overhead. 



\subsection{Privacy-preserving Video Storage}

Reliable data storage is important for video analytics tasks, especially for video retrieval applications. Privacy concerns come from sending sensitive data directly to the cloud with a lack of user control. As we know, more and more end devices have the ability to collect high-definition video content with a large data size, which inevitably causes the storage problem. Unlike mobile devices, edge and cloud servers have more powerful  storage and computing capabilities. However, the pure video storage on the remote servers will cause the leakage of  data privacy.
Davies \emph{et al.} \cite{Nigel2016HotMobile} proposed a software solution called Privacy Mediator, which is in the same administrative domain with  end devices. Therefore, Privacy Mediator can provide a reliable data storage service. OpenFace\cite{Wang2017MMSys} ensures a stronger privacy protection with a similar way by applying a trusted edge server to perform video denaturing and provide edge-based data storage, which keeps  privacy data away from unreliable network transmission. 

To protect privacy, Neff \emph{et al.} \cite{neff2019IOT} proposed REVAMP2T, which does not store or transfer any image data across the network. The edege server will destroy the image as soon as the image is processed. Instead, it works on an encoded feature representation of an individual, which has no meaning outside of the REVAMP2T system and cannot be interpreted by humans. However, it is important to utilize the computing power on the cloud side when the task suffers from a high computational complexity. 
Wang \emph{et al.} \cite{Wang2020JPDC} proposed a three-layer storage architecture, in which edge servers can offer a computing and storage service while the rest of data is transmitted to the cloud. With the proposed storage architecture, privacy data cannot be retrieved even when we use a cloud storage service. 
Similarly, Xiao \emph{et al.} \cite{Xiao2020Sensors} fully utilized the storage space of edge and cloud servers by proposing a hierarchical edge computing architecture and they divide video frames into three parts. To be specific, the most significant bits of key frames are stored in local with full control and the least significant bits are encrypted before sending to the edge servers. Finally, non-key frames are compressed and encrypted before they are transmitted to the cloud. For one thing, the above architecture makes full use of different levels of storage space; for another, it also provides a more fine-grained level of privacy protection.   

