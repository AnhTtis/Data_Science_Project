\section{Techniques and Methods}\label{sec_tech}
%\begin{figure}
%  \includegraphics[width=0.5\textwidth]{./images/tech.eps}
%  \caption{Classification of techniques and methods for EVA}
%\end{figure}

% Edge-based video analytics aim at coordinating a multitude of collaborative local devices, edge nodes and cloud servers to process the video analytics tasks. With different techniques and methods, edge-based video analytics enable the computing tasks accomplished directly at the edge of the network, perform task preprocessing at the edge node, or conduct computation offloading across different layers.

In this section, we overview various technologies and methods for edge-based video analytics, describe their roles and functions, and list some typical examples for each type.


%% ============
\subsection{Video Preprocessing}\label{subsec_preprocess}
Generally, the practice of offloading all raw videos to the edge or cloud will cause a huge burden on the network and may lead to intolerable latency. Facing this challenge, a direct approach is to preprocess the raw videos before offloading them to the edge or cloud. Many researches have applied various techniques for video preprocessing, including \emph{frame sampling}, \emph{frame cropping}, \emph{compression}, and \emph{feature extraction}. We briefly introduce these techniques as follows.

%% table for Video Preprocessing
\begin{table*}
\renewcommand\arraystretch{1.35}
	%\large
	\caption{Summary of the Video Preprocessing methods}\label{video preprocessing}
	\centering
\linespread{1}\selectfont
		\begin{tabular}{|p{2.5cm}<{\centering}|p{2cm}<{\centering}|p{2cm}<{\centering}|p{3cm}<{\centering}|p{3cm}<{\centering}|p{3cm}<{\centering}|}
			\hline \bf{Category} & \bf{Literature} & \bf{Method} & \bf{End Device Layer} & \bf{Edge/Fog Layer} & \bf{Cloud Layer} \\ \hline
			\hline \multirow{5}{*}{\shortstack{Frame \\ Sampling}} & Xu \emph{et al.} \cite{Xu2019ISBN} & Frame filtering & Split-phase planning for making heterogeneous sampling decisions & $\times$ & $\times$ \\
			\cline{2-6} &  Zhang \emph{et al.} \cite{Zhang2018ICPP} & Frame filtering & Apply multi-stage filters and a global feedback-queue mechanism & $\times$ & $\times$  \\
			\cline{2-6} & Zhang \emph{et al.} \cite{Zhang2016SEC} & Frame filtering & $\times$ & Use OpenCV & $\times$ \\
			\cline{2-6} & Chowdhery \emph{et al.} \cite{Chowdhery2018SECON} & Frame filtering & Use predicted drone trajectory & $\times$ & $\times$ \\
			\cline{2-6} & Wang \emph{et al.} \cite{Wang2018SEC} &  Frame filtering &  Transfer learning on the drone & $\times$ & $\times$  \\
			\cline{2-6} & \cite{Zhang2015MobiCom,Dao2017MASS} &  Cross cameras &  Jointly consider views from multiple cameras & $\times$ & $\times$  \\

			\hline \multirow{2}{*}{\shortstack{Frame \\ Cropping}} & Chen \emph{et al.} \cite{Chen2016SEC, Chen2016BigMM} & RoI-based  & $\times$ & Extract the RoI of frames & $\times$ \\
			\cline{2-6} & Guo \emph{et al.} \cite{Guo2019TMM} &  RoI-based & Apply a RoI based image compression algorithm & $\times$ & $\times$ \\

			\hline \multirow{3}{*}{\shortstack{Compression}} & Wang \emph{et al.}\cite{Wang2016TMM} &  Coding optimization  & $\times$ & Use Gaussian low-pass filtering & $\times$ \\
			\cline{2-6} & CloudSeg \cite{Wang2019HotCloud} &  Super resolution & $\times$ & Lower the quality of videos &  Run the SR procedure to reconstruct the high-quality videos \\
			\cline{2-6} & SR360 \cite{Chen2020NOSSDAV} &  Super resolution & Run the SR procedure to boost the video tiles & $\times$ &  Stream requested video contents to the client \\
			
			\hline \multirow{3}{*}{\shortstack{Feature \\ Extraction}} & Canel \emph{et al.} \cite{Canel2019SysML} &   DNN-based  & $\times$ &  Feature maps are produced from the intermediate results of a single reference DNN & $\times$ \\
			\cline{2-6} & George \emph{et al.} \cite{George2019HotMobile} &  Computer vision-based & Register the drone view to a reference image & Use SIFT matching to find correspondences between the live camera view and the reference images &  $\times$ \\
			\cline{2-6} & Jiang \emph{et al.} \cite{Jiang2018ATC} &  DNN-based & $\times$ & Exploit partial-DNN compute sharing among applications trained through transfer learning & $\times$ \\
			\hline
	\end{tabular}
\end{table*}


\subsubsection{Frame Sampling} \label{subsubsec_fs}

Frame sampling skips frames that may be ``useless'' in video streams for analytics tasks \cite{Lu18INFOCOM, Lu18TMC_CrowdVision}.
For example, Xu \emph{et al.} \cite{Xu2019ISBN} proposed a split-phase planning mechanism for making frame sampling decisions and resolving the tension of frame capturing/processing.
The FFS-VA system proposed in \cite{Zhang2018ICPP} utilizes two prepositive stream-specialized filters and a tiny-YOLO model \cite{tyolo} to filter out irrelevant frames.
Zhang \emph{et al.} \cite{Zhang2016SEC} extracted valuable information by using OpenCV\cite{pulli2012real} on edge nodes instead of sending the raw videos to cloud.
Chowdhery \emph{et al.} \cite{Chowdhery2018SECON} proposed a method that uses the predicted trajectory of a drone to choose and send the most relevant frames to a ground station, with the goal of maximizing the utility of application while minimizing the consumption of bandwidth.
Wang \emph{et al.} \cite{Wang2018SEC} leveraged state-of-the-art deep neural networks (e.g., MobileNet \cite{howard2017mobilenets}) with transfer learning technology to transmit the interesting data selectively from the video stream.
These solutions significantly reduce the number of video frames sent to the cloud for video analytics tasks.


Recently, some researches used the correlations among cross-cameras views to further improve the efficiency of frame sampling. 
For instance, Vigil \cite{Zhang2015MobiCom} only uploads the most relevant frames to the cloud accoding to the user's query when there are different views of an object or person captured by multiple cameras.
Similarly, Dao \emph{et al.} \cite{Dao2017MASS} utilized the joint camera views of the object to determine the views which achieve the best accuracy regard to the object of interest and improve the quality of detection. 
Collaboration between cameras reduces the amount of offloaded data, thereby reducing the bandwidth consumption. 


\subsubsection{Frame Cropping}\label{sec_frame_crop}
Different from frame sampling, frame cropping focuses on regions of interest (RoIs) in video frames, such as the face regions for face detection tasks or the vehicle regions for vehicle monitoring tasks. 
Frame cropping removes the unimportant regions of the raw images, thus, reduces the data transferred in the network.
Chen \emph{et al.} \cite{Chen2016SEC, Chen2016BigMM} extracted the RoI containing the suspicious vehicle instead of whole video frame to improve the video analytics efficiency. Guo \emph{et al.} \cite{Guo2019TMM} captured image data on end devices in a real-time manner and compressed them by a RoI based image compression algorithm. The compressed images were then transmitted to the cloud or the edge server, reducing the bandwidth usage and improving the transmission efficiency.


\subsubsection{Compression}
% The RoI algorithm mentioned above can be seen as a spatial compression operation on the image, and the general spatio-temporal compression operation is to compress the entire video stream. Thus, video compression can signiﬁcantly reduce transmission traffic, save storage space, increase throughput and even help better understand and parse videos using deep neural networks.
The compression technology has been widely used in video analytics applications to ensure a high processing efficiency.
Wang \emph{et al.}\cite{Wang2016TMM} proposed an adaptive approach for compressing screen content videos based on their utility, which involves identifying and processing low-utility content with a Gaussian low-pass filtering. 
Rippel \emph{et al.}\cite{Rippel2019ICCV} developed an architecture for video compression that extends motion estimation to perform learned compensation beyond simple translations.
Fouladi \emph{et al.}\cite{Fouladi2017nsdi} presented a video encoder that reach a fine-grained parallelism while keeping compression efficiency.

Recently, the super resolution (SR) technology has been regarded as an essential solution for video compression and transmission.
With SR techniques, the cameras or edge servers can send the video stream in low resolution, while the high-resolution frames can be recovered from the low-resolution stream. 
Through SR technology, it can reduce the transmission delay and reduce the pressure on network bandwidth while meeting the high requirements of video quality and analytics accuracy \cite{Wang2019HotCloud, Chen2020NOSSDAV}.

For instance, Wang \emph{et al.} \cite{Wang2019HotCloud} presented CloudSeg, which reduces the quality of the video during transmission to the cloud, but then performs the super-resolution process on the cloud server to reconstruct high-quality videos before executing video analytics.
Yang \emph{et al.} \cite{Yang2018Access} proposed an SR method to minimize SR errors by dividing the training samples into multiple clusters and learning dictionaries to achieve more faithful reconstructions in edge video analytics.
Chen \emph{et al.} \cite{Chen2020NOSSDAV} presented SR360 framework, in which the low resolution video tile in 360-degree videos can be upsampled at the client side to a high resolution tile with SR techniques.
Guo \emph{et al.} \cite{Guo2019sensors} proposed a semantic-aware SR transmission system for wireless multimedia sensor networks. The system encodes different bit-rate video with semantic information on the multimedia sensor, and uploads them to users. On the user side, the video quality is enhanced using SR techniques.


\subsubsection{Feature Extraction}
Feature extraction is a process that extracts image information and identifies whether each image point belongs to an image feature or not. Based on a preliminarily trained and lightweight neural network, the extracted feature map can be directly sent to the video analytics functions at the edge or cloud server.
The extracted feature map can be regarded as inputs to the object classifier, or stored and converted into useful information for other functions \cite{Uddin2019Symmetry}. 

Canel \emph{et al.} \cite{Canel2019SysML} presented FilterForward, where the feature maps are extracted from video frames on edge servers by a single reference DNN. Micro-classifiers are trained to take these feature maps as input and return the relevance of the specific applications and the frames. Besides, FilterForward allows micro-classifiers to get the feature maps of one layer of the model, making it versatile enough to support various tasks.

George \emph{et al.} \cite{George2019HotMobile} proposed an edge-based prototype that employed computer vision algorithms for live building inspection with drone-sourced video. Their system used visual features and Scale Invariant Feature Transform (SIFT) features \cite{lowe2004distinctive} matching to identify relevance between the reference images and view of live camera.

Grassi \emph{et al.} \cite{Grassi2017SEC} presented ParkMaster, which captures the parked vehicles in the mobile camera, extracts the feature, and then uploads the data to the cloud to run a clustering algorithm to count the number of parked cars on the road.

Mainstream \cite{Jiang2018ATC} is an edge system for video processing that employs transfer learning from a common base DNN model to train multiple applications.  By sharing partial-DNN compute among these applications, it reduce the computing time of per-frame aggregation.
Kang \emph{et al.} \cite{Kang2017PVLDB_NoScope} performed model specialization by using the full neural network to generate labeled  training  data  and  subsequently  training  smaller neural networks that are tailored to a given video stream and to a smaller class of objects. 


\subsection{Video Analytics}\label{subsec_DNN}

%% table for Video Analytics
\begin{table*}
\renewcommand\arraystretch{1.35}
	\caption{Summary of the Video Analytics methods}\label{video analytics}
	\centering
\linespread{1}\selectfont
		\begin{tabular}{|p{2cm}<{\centering}|p{2cm}<{\centering}|p{2.5cm}<{\centering}|p{2.5cm}<{\centering}|p{2cm}<{\centering}|p{2cm}<{\centering}|p{2cm}<{\centering}|}
			\hline \bf{Category} & \bf{Literature} & \bf{Contribution} & \bf{End Device Layer} & \bf{Edge/Fog Layer} & \bf{Cloud Layer} & \bf{Evaluation}\\ \hline
			\hline \multirow{4}{*}{\shortstack{Deep Neural \\ Network-based \\ Processing}} & EdgeEye \cite{Liu2018EdgeSys_EdgeEye} & Provides a high-level abstraction of video analytics functions based on DNNs & $\times$ & Offload the live video analytics tasks to the server using its API & $\times$ & Inference Speed: 55 Mean FPS \\
			\cline{2-7} &  Augur\cite{Lu17MM} & A CNN performance analyzer to determine the efficiency of a CNN on a given mobile platform & Take a CNN configuration as the input and predicts the computational time and resource utilization of the CNN & $\times$ & $\times$ & Deploy 4 kinds of CNNs on mobile platforms to measure and analyze the performance and resource usage \\
			\cline{2-7} & NetVision \cite{Lu2020ToN_NetVision} & Leverages DNNs across a network of mobiles and edge devices to minimize the querying response time & Determine a sequence with minimizing the query response time for the video offloading and transmission. & $\times$ & $\times$ & The greedy algorithm is close to the optimum, and the adaptive algorithm performs better \\
			\cline{2-7} & Huang \emph{et al.} \cite{Huang18MM_QARC} & Leverages a deep reinforcement learning algorithm to achieve higher perceptual quality rate & $\times$ & DRL model selects sending bitrate for future video frames & $\times$ & Average video quality: 18\% - 25\% , decreasing in average latency with 23\% -45\% \\
			
			\hline \multirow{2}{*}{\shortstack{Profiling}} & VideoStorm \cite{Zhang2017nsdi} &  A video analytics system that processes thousands of video analytics queries on live video streams over large clusters  & $\times$ & $\times$ & An offline profiler generating query resource-quality profile and an online scheduler for resource allocation & Quality of real-world queries: 80\% improvement, lag: 7$\times$ better \\
			\cline{2-7} & VideoEdge \cite{Hung2018SEC} &  Achieve a trade-off between resources and accuracy for live video analytics & Merge common components across queries & Narrows the search space by identifying a ``Pareto band" of promising configurations & $\times$ & Improves accuracy: 5.4$\times$ - 25.4$\times$ \\
			\hline
	\end{tabular}
\end{table*}

\subsubsection{DNN-Based Processing}
Undoubtedly, DNNs contribute significantly to video analytics as they are believed to be the backbone of recognition and detection in images and videos. These DNNs can be a part of tool or library or they can be embedded into the hardware for on-device video analytics. 

Augur \cite{Lu17MM} is a tool providing insights about the efficiency of a CNN on specify mobile platform. Augur profiles and models the resource requirements of CNNs by using a configuration of the CNN to predicts the computational overhead and resource utilization of the model. The CNNs are selected from wide range of libraries such as ResNet \cite{he2016identity}, VGG \cite{simonyan2014very}, NASNet \cite{zoph2018learning}, AlexNet \cite{alexnet}, and GoogleNet \cite{gglnet} on NVIDIA TK1 and TX1 hardware \cite{jetsondevkit}. 
% Deployment of these CNN models onto Caffe deep learning framework \cite{caffefw} provided insights about resource requirements on two mobile platforms without implementation and deployment.

Augur \cite{Lu17MM} evaluates each model on CPU and GPU considering memory where holds the parameters of the CNN, stores intermediate data, and the workspace for computation. Unlike workstations, in mobile platforms GPU shares the system memory with CPU that is not addressed by Caffe, hence, it causes generating redundant copy of memory.
Augur finds that the matrix multiplications of CNN computation is the core for performance evaluation. They are measured by the BLAS and cuBLAS libraries for matrix multiplications on CPUs and GPUs, respectively. 
The matrix size of a CONV or FC layer is related to the input dimension (e.g., images size) and network configuration (e.g., kernel size). Besides, time measurements have shown that \emph{matmul} contributes more than 60\% of the computation time of a CNN on mobile platforms. To model the time for prediction purposes, several matrix sizes are benchmarked with respect to the number of kernels, the size of a kernel, and the spatial size of output feature maps. 
Therefore, Augur first parses the CNN descriptor and determines the minimal memory needed based on the type and setting of each layer. Then, Augur extracts matrix multiplications (matmuls) from the computation of the CNN and calculates the compute time of individual matmuls. Finally, Augur sums up the compute time of all matmuls to provide an estimate of the total computation time of the model on the mobile platform.

Deep learning-based video analytics systems may consist a few hyper-parameters, including \emph{learning rate}, \emph{activation function} and \emph{weight parameter initialization}.
Yaseen and colleagues \cite{Yaseen2019TSMC} addressed the challenge of optimizing hyper-parameters in deep learning-based video analytics systems. They proposed a mathematical model to evaluate the impact of different hyper-parameter values on system performance. Their work also included proposing an automatic object classification pipeline for efficient large-scale object classification in video data.
Nvidia Deep Learning GPU Training System (DIGITS) proposed a general DNN architecture.
Liu \emph{et al.} \cite{Liu2018EdgeSys_EdgeEye} used Nvidia Deep Learning GPU Training System (DIGITS), a general DNN architecture, to design their DetectNet, and proposed a framework called EdgeEye for video analytics in real-time at the edge. The EdgeEye server allows applications to offload live video analytics tasks through its API, eliminating the need for deep learning framework specific APIs.

Other usage of DNNs are found in \cite{Lu2020ToN_NetVision, Huang18MM_QARC, Han2016MobiSys} studies for reducing the response time and improving the transmission rates. Lu \emph{et al.} \cite{Lu2020ToN_NetVision} present NetVision  as a system for on-demand video processing that uses deep learning to minimize query response time across a network of mobile and edge devices.


The transmission rate is also improved in \cite{Huang18MM_QARC} which relies on a deep reinforcement learning algorithm focusing on higher perceptual quality rate with lower bitrate. The algorithm works based on a trained neural network to predict future bitrates based on observation of network status. The proposed model in their study relies on two separate neural networks; (1) it  precisely predicts future video qualities based on the previous video frames, and (2) a reinforcement learning algorithm to determine the proper bit rates with respect to the output of the first neural network. The output of the first neural network model relies on a combination of CNNs to extract image features and a recurrent neural network for capturing temporary features for providing the better video qualities. 

Han \emph{et al.} \cite{Han2016MobiSys} studied the DNNs usage to execute multiple applications on cloud-connected mobiles to process a stream of data. It relies on a trade-off between resource usage and accuracy to be coped with workloads considering less accurate variants of optimized models. Hence, an adaptive framework was presented to select model variants at different accuracy levels while staying with the request resource constraints and energy constraints. The framework keeps track of accuracy, energy usage, and resource usage to form a catalogue based on a series of model optimization techniques. It uses different settings and heuristically allocates resources in proportion according to the frequency of use and selection of the most accurate corresponding model variant. This selection is interpreted as a model execution either on the device or on the cloud such that which application model should be chosen at a specific time step and which models should be evicted from the mobile cache. 



\subsubsection{Profiling}
Profiling reduces computation overload of a system for obtaining handful configurations for video analytics. VideoStorm \cite{Zhang2017nsdi} and VideoEdge \cite{Hung2018SEC} are systems that take advantage of profiling for improving performance. In Zhang \emph{et al.} \cite{Zhang2017nsdi}, the system processes many video analytics queries on live video stream over large clusters. The system leverages an offline profiler generating a profile of query resources and uses an online scheduler for resource allocation to maximize the performance in terms of both quality and response time. The quality and lag are encoded as utility functions in which becomes penalized for violations. The profiler uses greedy search and domain sampling specific for obtaining a set of handful configurations on the Pareto boundary of profile to be taken into account by the scheduler. 

Hung \emph{et al.} \cite{Hung2018SEC} showed that VideoEdge is capable of achieving a trade-off between resource consumption and model accuracy by narrowing down the configuration space in the hierarchical structure (i.e., cameras, private clusters, and public clouds) for live video analytics. In VideoEdge, the configuration space is downsized through maximum computation of maximum demand to capacity ratio for each configuration to find the dominant demand. This enables system to compare configurations across demand and accuracy. The system also leverages a profiler for efficient merging components (e.g., decoder) for improving the accuracy and reducing the computation workload.


\subsection{Computation offloading}\label{subsec_offload}

%% table for Computation offloading and Collaborative Intelligence
\begin{table*}
\renewcommand\arraystretch{1.35}
	\caption{Summary of the Computation offloading and Collaborative Intelligence methods}\label{CoCI}
	\centering
\linespread{1}\selectfont
		\begin{tabular}{|p{2cm}<{\centering}|p{2cm}<{\centering}|p{2cm}<{\centering}|p{2.5cm}<{\centering}|p{2cm}<{\centering}|p{2cm}<{\centering}|p{2cm}<{\centering}|}
			\hline \bf{Category} & \bf{Literature} & \bf{Method} & \bf{End Device Layer} & \bf{Edge/Fog Layer} & \bf{Cloud Layer} & \bf{Performance}\\ \hline
			\hline \multirow{4}{*}{\shortstack{Computation\\offloading}} & DeepDecision \cite{Ran2018INFOCOM_DeepDecision} & Compute locally or offload & Determine an optimal offload strategy by using estimates of network conditions, the application’s requirements and specific tradeoffs of models & Receive the offloaded tasks and process with big CNN& $\times$ & Improve accuracy: about 30\%\\
			\cline{2-7} &  PicSys\cite{Felemban2020TMC} & Compute locally or offload & Multi-stage decision technique to decide process the image locally with the accelerated CNN or offload & $\times$ & Receive the offloaded tasks and process with a more complex and accurate CNN & Time savings: 35\% \\
			\cline{2-7} & Neurosurgeon \cite{Kang2017ASPLOS} & Partition the entire task to each computing nodes & Intelligently partition DNN computation between the mobile and cloud, the mobile process parts of the DNN & $\times$ & Receive the intermediate data and process the rest parts & End-to-end latency: 3.1$\times$ improvement, mobile energy consumption: 59.5\% reduction \\
			\cline{2-7} & Emmons \emph{et al.}\cite{Emmons2019HotEdgeVideo} & Partition the entire task to each computing nodes & Process partly subject to a limit on computation & $\times$ & Receive the intermediate values limited by network capacity, and process further DNN inference. & $\times$ \\
			
			\hline \multirow{4}{*}{\shortstack{Collaborative\\Intelligence}} & Chameleon \cite{Jiang2018SIGCOMM_Chameleon} &  Mobile Collaboration  & Use temporal and spatial correlation of cross-camera to picks the best configurations for existing NN-based video analytics pipelines & $\times$ & $\times$ & 20-50\% higher accuracy with the same amount of resources, or the same accuracy with 30-50\% of the resources \\
			\cline{2-7} & Dao \emph{et al.}\cite{Dao2017ICDCS} &  Mobile Collaboration & Coordinate among cameras to deliver highly accurate detection of objects efficiently & $\times$ & $\times$ & Reduces energy consumption: 40\% \\
			\cline{2-7} & Long \emph{et al.}\cite{Long2017TMM} &  Edge Collaboration & $\times$ & Optimally form devices into video processing groups and dispatch video chunks to proper groups & $\times$ & Improve the human detection accuracy: 19\% \\
			\cline{2-7} & Wang \emph{et al.}\cite{Wang2017SmartIOT} &  Edge Collaboration & $\times$ & A group of VMs/VNFs launched to work together for a surveillance task & $\times$ & Latency: about 6.0$\times$ improvement \\
			\hline
	\end{tabular}
\end{table*}

Deep learning algorithms are often computationally intensive, while the front-end equipment usually lacks the computing power for executing large-scale deep learning tasks. Transferring the data to a powerful cloud and executing deep learning algorithms in the cloud may cause unacceptable latency for users. Cloud-based solutions for deep learning are dependent on reliable network access, and it is challenging to offload the full or partial compute tasks to the proximate edge servers.

\subsubsection{Full Offloading}
Some researches focused on the offloading strategy to decide whether to perform lightweight analytics on the edge or send the videos or images to the cloud for more computationally intensive analytics.
By considering current network conditions and application’s requirements as the trade-offs, Ran \emph{et al.} \cite{Ran2018INFOCOM_DeepDecision} presented DeepDecision to determine an optimal offload strategy in real-time AR applications, and decide either analyze the input video locally with small CNNs or send to the server to analyze with a big CNN.
Felemban \emph{et al.} \cite{Felemban2020TMC} proposed PicSys, an intelligent system that decides whether to process images locally with an accelerated CNN or offload them to the cloud for processing with a more complex and accurate CNN. This decision is made based on several factors such as network conditions, energy state of the mobile device, cloud backlog, and the hit-rate estimate. 
Ananthanarayanan \emph{et al.} \cite{Ananthanarayanan2019MobiSys} optimized their work based on the previous work \cite{Ananthanarayanan2017computer}. They executed a cheap CNN at the edge and a heavy CNN at the cloud. Only if the lightweight CNN model does not have sufficient confidence do they invoke the heavy CNN model.
Yi \emph{et al.}\cite{Yi2017SEC_LAVEA} presented LAVEA, a edge computing platform that utilizes serverless architecture to enable computation offloading between clients and edge nodes. They formulated the offloading task selection as an optimization problem to prioritize offloading requests in order to minimize response time.

\subsubsection{Partial Offloading}
Some researches partition the entire analytics task to each computing nodes, that is, the edge node shares part of the computing pressure for the cloud node to reduce end-to-end latency and resource consumption.
Kang \emph{et al.} \cite{Kang2017ASPLOS} proposed a system called Neurosurgeon to optimize the partitioning of deep neural network (DNN) computation between mobile and cloud. The system employs a series of models to predict the response time and power consumption of the model according to its configuration and type. This allows Neurosurgeon to lower the system latency, reduce mobile energy consumption, and enhance datacenter throughput.
% Emmons \emph{et al.}\cite{Emmons2019HotEdgeVideo} proposed ``split-brain'' inference, where video is processed partly on the camera, subject to a limit on computation. Then, intermediate values are transmitted, limited by network capacity, to a cloud datacenter for further DNN inference.
Emmons \emph{et al.} \cite{Emmons2019HotEdgeVideo} proposed a concept of ``split-brain'' inference to perform video analytics. The approach involves processing the video partially on the camera, with the computation limited to a certain extent. Constrained by network capacity, the intermediate results are then transmitted to a cloud datacenter for further DNN inference.


%% ==========
\subsection{Collaborative Intelligence}\label{subsec_collab}

Computation offloading introduced above can be viewed as collaboration between nodes at different levels. We will then introduce collaborative intelligence, which refers to the collaboration between nodes at the same level.

\subsubsection{Mobile Collaboration}
Many districts are deploying cameras on a large scale, and when considering this scenario, we have to face the problem of increased massive deployment cost and effort. For example, when the overlapping angles of deployment of multiple cameras are large, or the same video analytics tasks are performed, a large amount of redundant data is generated and unnecessary computing resources are consumed.
In general, it is assumed that the computing power of the camera nodes is very limited, and it can only collect video streams, perform certain preprocessing and then send it to the edge or cloud. We have briefly introduced cross-camera collaboration in Sec \ref{subsubsec_fs} to optimize frame sampling and suppress redundancy \cite{Zhang2015MobiCom}. Next, we will introduce other research work on mobile collaboration.

Khochare \emph{et al.} \cite{Khochare2019CCGRID} developed Anveshak, a framework for creating video analytics applications that track objects across a camera network. In case an object is not detected by one camera, the system expands its search to include more cameras.
% O'Gorman \emph{et al.} \cite{O'Gorman2018ICPR} quantified the temporal and spatial video characteristics and understand how real-world sparse signals from public cameras can substantially reduce loads, which do help to understand placement decision of methods between edge and cloud. 
O'Gorman \emph{et al.} \cite{O'Gorman2018ICPR} quantified the temporal and spatial characteristics of video data and demonstrated how the sparse nature of real-world signals from public cameras can significantly reduce the computational load. This understanding can aid in making decisions to place the tasks on the edge or in the cloud.
Dao \emph{et al.} \cite{Dao2017ICDCS} proposed a framework that allows cameras coordination for high accuracy and reducing energy consumption. By coordination among cameras, each camera can use the non-optimal algorithm for the detection task and avoid unnecessary energy consumption.

Video analytics pipelines involve several adjustable parameters or ``knobs'', such as frame resolution, sampling rate, and detector models. The selection of these configurations has an impact on both the accuracy and the resources consumed of the video analytics. However, the number of potential configurations can increase exponentially with the number of knobs and their respective values. The Microsoft Research team is focusing on using cross-camera correlation to optimize the search space.
Jiang \emph{et al.} \cite{Jiang2018SIGCOMM_Chameleon} presented Chameleon, a controller that selects the optimal configuration for neural-based video analytics pipelines dynamically. The insight behind Chameleon is that the underlying characteristic that impacts the best configuration has enough spatio-temporal correlation, amortizing the search cost over time and across several video feeds.
Jain \emph{et al.} \cite{Jain2019HotMobile} proposed ReXCam, which leverages spatio-temporal correlations in video feeds from wide-area camera deployments to reduce search space of inference. By exploiting these correlations, ReXCam reduces both the workload and false positive rates in multi-camera video analytics. Specifically, ReXCam guides its search for a query identity by taking advantage of spatial and temporal locality dynamically of the camera networks.


\subsubsection{Edge Collaboration}
Compared with camera nodes, edge nodes have stronger computing power and can complete part of or even entire video analytics tasks. It is urgent to design task scheduling strategies or to share computation results among multiple edge nodes. Assigning a task to only one edge node may result in underutilization of the redundant computational resources of the other edge nodes. Hence, there is a need to explore optimal ways to group edge nodes and enable collaborative processing of video analytics tasks.

Long \emph{et al.} \cite{Long2017TMM} presented a framework for edge computing that enables cooperative processing of latency-sensitive multimedia tasks on resource-rich mobile devices. The framework relies on optimally dividing the mobile devices into different groups and assigning video chunks to the proper group to enable cooperative processing of tasks.
Wang \emph{et al.}\cite{Wang2017SmartIOT} designed a three-tiers architecture for real-time surveillance applications of edge computing system. The architecture is designed to elastically adjust computing capacity and dynamically route data to the most appropriate edge server. Surveillance tasks are executed by a group of Virtual Machines (VMs) or Virtualized Network Functions (VNFs) that work together on specific tasks. The system is effectively configured, monitored, and managed by the SDN controller.
