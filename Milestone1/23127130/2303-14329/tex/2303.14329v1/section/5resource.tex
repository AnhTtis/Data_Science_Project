\section{Resource management}\label{sec_resource}
Determination and detection of temporal and spatial events can be automatically done by video content analytics. This capability is supported by resources allocated to algorithms on cameras or devices to compute and output the object of interest \cite{Wang2018IC-NIDC, Lu18INFOCOM, Lu18TMC_CrowdVision}. Accordingly, three main dimensions are identified to characterize resource management in the context of video analytics, each consisting of various sub-dimensions. Hence, in a general overview, resource management is illustrated based on the following classification, and the summary is given in Table \ref{tb:rm_summary}.


\begin{table*}
\renewcommand\arraystretch{1.35}
	\caption{Summary of the literature for resource management}\label{CoCI}
	\centering
\linespread{1}\selectfont
		\begin{tabular}{|p{2cm}<{\centering}|p{2cm}<{\centering}|p{2cm}<{\centering}|p{2.5cm}<{\centering}|p{2cm}<{\centering}|p{2cm}<{\centering}|p{2cm}<{\centering}|}
			\hline \bf{Category} & \bf{Literature} & \bf{Method} & \bf{End Device Layer} & \bf{Edge/Fog Layer} & \bf{Cloud Layer} & \bf{Performance}\\
			\hline
			\hline
			\multirow{4}{*}{\shortstack{QoE Metrics}} & Hung \emph{et al.} \cite{ Hung2018SEC} & Narrowing down the configuration space & Choosing from configuration space for live video feed analytics & Determination of demand to the capacity ratio for configurations & \checkmark & Accuracy and response time \\
			\cline{2-7} &  Zhang \emph{et al.} \cite{Zhang2017nsdi} & Combination of offline profiler and online scheduler for resource-quality profile and allocation & Resource-quality profile of queries & $\times$ & \checkmark & Performance and lag \\
			\cline{2-7} & Lu \emph{et al.} \cite{Lu18INFOCOM,Lu18TMC_CrowdVision} & Batch processing and filtering out videos & Optimal video offloading and transmission sequence in terms of minimizing the query response time. & Deep learning techniques for object detection & $\times$ & Distributed processing, energy efficiency, and performance improvement \\
			\cline{2-7} & Lu \emph{et al.} \cite{Lu17MM} & Profiling and modeling resource requirements on the mobile platform for CNNs & Estimation of resource usage and compute time for CNN configurations  & $\times$ & $\times$ & Compute time \\
			\cline{2-7} & Lu \emph{et al.} \cite{Lu2020ToN_NetVision} & Usage of greedy algorithm and adaptive algorithm & Control transmission rate & Optimal video offloading and processing & $\times$ &Response time and transmission rate \\
			
			\cline{2-7} &Han \emph{et al.} \cite{Han2016MobiSys} & Heuristic scheduling algorithm & Frequency of use and the choosing the accurate model & $\times$ & DNN optimization at the cloud layer & Accuracy \\
			
			\cline{2-7} &Wang \emph{et al.} \cite{Wang2018IC-NIDC} & Light-weight edge cloud platform & $\times$ & Network switch and intelligent edge servers & Usage of container and SDN & Response time and QoE \\
			\cline{2-7} &Xu \emph{et al.} \cite{xu2018ATC} & Tuning canary inputs and usage of prediction  & Configuration selection considering video encoding parameters & $\times$ & $\times$ & Accuracy, low-latency and low-energy processing \\
			
			\cline{2-7} &Fu \emph{et al.} \cite{Fu2019ATC_EdgeWise} & Balancing operation queues by thread workers & Optimizing data flow by a congestion-aware scheduler & $\times$ & $\times$ & Latency \\
			\hline \multirow{4}{*}{\shortstack{Resource \\ Provisioning\\ Method}} & \cite{Yi2017SEC_LAVEA,Grassi2017SEC,Shen2017CVPR} &  Heuristic-based  & Localization algorithm or choosing among CNNs' models  & Usage of nearby devices for speeding up processing & $\times$ & Performance \\
% 			\hline
			\cline{2-7} &\cite{Drolia2017ICDCS_Cachier,Long2017TMM,O'Gorman2018ICPR,Valls2018MILCOM} & Optimization-based & Load balancing  & Algorithm placement  & \checkmark  & Latency and performance\\
% 			\hline
			\cline{2-7} &\cite{Huang18MM_QARC,Rao2017ICCV,Supancic2017ICCV} & DRL-based & Combination of deep reinforcement learning and ANN & $\times$ & $\times$ & Bitrate and network usage \\
% 			\hline
			\cline{2-7} &\cite{Zhan2016MSN,Wu2017TMM,Hu2020TPDS} & Incentive-aware & Game theory strategies & $\times$ & $\times$ & Latency and cost \\
			\cline{2-7} & \cite{Stone2019SECON,Liu2018EdgeSys_EdgeEye,Poostchi2017CoRR} & CPU/GPU Optimization & Optimization techniques and libraries & $\times$ & $\times$ & Performance and cost \\
			\cline{2-7} &\cite{Xu2019arXiv_2,Huang2017sosp_SVE} & Data/Streaming Storage & Local storage & Maintaining cache servers & \checkmark & Latency and reducing network load \\
			\hline
\end{tabular}
\label{tb:rm_summary}
\end{table*}


\begin{itemize}
    \item \textit{Quality of Experience (QoE) Metrics} as a holistic concept is to measure a user satisfaction degree for a service or an application. This satisfaction can be fallen into three major groups: accuracy, latency, and performance (e.g., energy), taking into account edge or cloud resources. 
    \item \textit{Resources}  to \emph{be provisioned} for providing the guaranteed performance for application through selection, deployment, and runtime management of software and hardware resources contributing to video analytics.
    \item \textit{Resource provisioning methods} to \emph{deal} with providing the guaranteed performance for applications. This can be done by a heuristic-based resource provisioning scheme enforced by deep learning strategies, or it may consider content-aware approaches for resource allocation prioritization.  
\end{itemize}

%\begin{figure}
%\centering
%  \includegraphics[scale=0.5, keepaspectratio]{images/rm_clsn.pdf}
%  \caption{Classification of techniques for resource management.}
%  \label{fig:rs_cls}
%\end{figure}


\subsection{QoE Metrics}\label{sec:qoe}

\subsubsection{Accuracy}
Canary inputs (i.e., small input numbers, e.g., subsampling) have contributed to video accuracy optimization. Xu \emph{et al.} \cite{xu2018ATC} proposed VideoChef that uses canary inputs to tune the computation accuracy and transferring the approximate configurations to full inputs, which also uses a prediction model considering user constraints (e.g., Peak Signal-to-Noise Ratio) with respect to canary input's error. The canary inputs are chosen based on the dissimilarity ratio between the sample and the full-size video to determine how close a canary input is. This roots in the fact that most stages in the video pipeline are approximate, leading to low-latency and low-energy video processing in specific/generic domains. These approximations may depend on both algorithms and the video content, which necessitate exploring a large number of configurations for finding the optimal one. This selection takes place offline and through an efficient search strategy considering video encoding parameters. Since searching the best approximate level is \emph{computation-intensive}, VideoChef aims at an encoder-based approach emphasizing significant video changes through pixel and histogram-based for detecting the scenes.

Hung \emph{et al.} \cite{ Hung2018SEC} present VideoEdge as a system that is capable of achieving a trade-off between resource and accuracy through narrowing down the configuration space in a hierarchical structure (i.e., cameras, private clusters, and public clouds) for live video feed analytics. The configuration space is downsized by computation of maximum demand to the capacity ratio for each configuration to find the dominant demand, allowing the system to compare configurations across demand and accuracy. In addition, streaming and multi-programming can also lead to improving the accuracy as Han \emph{et al.} \cite{Han2016MobiSys} proposed a heuristic scheduling algorithm for allocating resources proportionally based on the frequency of use and selection of the most accurate model variant. 

Felemban \emph{et al.} \cite{Felemban2020TMC} proposed PicSys, a system that optimizes the deployment of a CNN pipeline by assigning each computation stage to resources that maximize their utilization. PicSys splits computation into several filtering stages, and assigns each stage to a resource using a heuristic algorithm that solves an optimization problem. To balance the trade-off between accuracy and speed, PicSys uses a lighter version of the CNN that requires less computation.

\subsubsection{Latency}
Wang \emph{et al.}, \cite{Wang2018IC-NIDC} proposed a lightweight edge computing platform that is powered by small-size and cost-efficient edge intelligent servers integrating computation and network capabilities to constitute a large scale edge cloud. The framework targets reducing response time while improving the quality of experiences. These goals are achieved by integrating software-defined network and container features. The latter provides network switches and the ease of services integration within containers for resource integration and controlling management.

The streaming processing engine presented in \cite{Fu2019ATC_EdgeWise} addressed improvement for latency through focusing on the operation queues to be balanced by thread workers. The engine that is called EdgeWise is powered by a congestion-aware scheduler that monitors the queues for selection of the highest operation priority to optimize the data flow. This optimization is mainly done by considering fixed worker pools for decoupling workers from operations (i.e., thread contention) stemming from ready threads to be in charge of operations with the most pending data. Moreover, EdgeWise considers data consumption policy for the queues to improve the overall scheduling decisions allocation of heavy-loaded operations with more frequent workers. The NetVision system \cite{Lu2020ToN_NetVision} used a greedy algorithm along with an adaptive algorithm to manage variate transmission rates to improve latency. It provided a solution for optimizing query response time in on-demand video processing scenario by formulating the processing scheduling problem.


\subsubsection{Performance}
Cloud resources may also contribute to performance improvement by pushing video processing to the cloud (i.e., offloading), which can be seen in \cite{Lu18INFOCOM,Lu18TMC_CrowdVision}. In these works, a deep learning-based CrowdVision platform is proposed that is distributed and energy-efficient and considers batch processing as the characteristics of CNNs. The features contribute to the distributed processing, balancing the waiting time of frame rates and the processing time of each batch for performance improvement while considering the network conditions for offloading benefits. To further improve the video processing, CrowdVision filters out videos by location and timestamp and applies deep learning techniques for the object of interest recognition in a batch-like manner. 
Zhang \emph{et al.} \cite{Zhang2017nsdi} proposed a video analytics system, VideoStorm, which leverages an offline profiler generating query resource-quality profile and employs an online scheduler to allocate resources for maximizing the performance on quality and lag rather than relying on fair sharing in clusters. The profiler uses greedy search and domain sampling specific for obtaining a set of handful configurations on the Pareto boundary of the profile, which the scheduler takes into account. The quality and lag are encoded as utility functions, which are penalized for violations and prioritization.

Artificial neural networks can also assist performance improvement. Lu \emph{et al.} \cite{Lu17MM} present Augur as a CNN performance analyzer to determine the efficiency of a CNN on a given mobile platform. This tool profiles and models the resource requirements of CNNs (i.e., the forward pass) by taking the configuration of CNN to estimate the computational overhead of the model.


% \centering
% \caption{Summary of literature for resource management.}
% \begin{adjustbox}{width=1\linewidth}
% \begin{tabular}{ll}
% \hline
% Literature & Aim \\ \hline
% Wang \emph{et al.} \cite{Wang2018IC-NIDC} & Response time and QoE \\
% Lu \emph{et al.} \cite{Lu18INFOCOM,Lu18TMC_CrowdVision} & Distributed processing, energy efficiency, performance improvement \\ 
% Fu \emph{et al.} \cite{Fu2019ATC_EdgeWise} & Latency\\
%  Xu \emph{et al.} \cite{xu2018ATC} & Accuracy, low-latency and low-energy processing\\
%  Lu \emph{et al.} \cite{Lu17MM} & Compute time\\ 
%  Lu \emph{et al.} \cite{Lu2020ToN_NetVision} & Response time and transmission rate\\
%  Huang \emph{et al.} \cite{Huang18MM_QARC} & Transmission rate\\
%  Han \emph{et al.} \cite{Han2016MobiSys} & Accuracy\\
%  Hung \emph{et al.} \cite{ Hung2018SEC} & Accuracy and response time\\
%  Zhang \emph{et al.} \cite{Zhang2017nsdi} & Performance and lag\\
%  \hline
% \end{tabular}
% \end{adjustbox}
% \label{tb:rm_summary}
% \end{table}



\subsection{Resources}

\subsubsection{Hardware}
Power-efficient AI computing devices (i.e., AI-kit) have enabled video analytics at the edge and provide users with applications including image classification, object detection, segmentation, and speech processing running in parallel. These devices are also suitable for use in environments with intermittent connectivity, such as remote locations.

NVIDIA Jetson TX1 and TX2 \cite{jetsonnvidiatx2}, and Jetson Nano \cite{jetsonnvidianano} are power-efficient embedded AI computing devices that bring trustworthy AI computing at the edge. They have emerged from NVIDIA Pascal-family GPU capable of being integrated into any products due to having hardware standard features. 

Amazon Snowball \cite{awssnowball} is a device optimized for edge computing that provides virtual CPUs, block and object storage, and even an optional GPU. These devices can be clustered together in a rack-mounted form to create larger temporary installations. Snowball has been designed to support advanced machine learning and video analytics applications, especially in disconnected environments such as manufacturing, industrial, and transportation settings or in highly remote locations, including military or maritime operations.

Azure IoT Starter kit \cite{azureiot} is a vision AI developer kit to run AI models at intelligent edge. The kit runs models built by Microsoft Azure Machine Learning (AML) and other Azure services for edge analytics and AI processing.

Intel NUC mini PC \cite{intelnuc} is an AI Development Kitequipped with a powerful Intel Core processor, integrated graphics, and the advanced Intel Movidius Myriad X Vision Processing Unit (VPU). This combination allows for seamless execution of a wide range of AI workloads with high performance and low power consumption. The kit offers a comprehensive AI capability and is suitable for running diverse AI workloads.


Raspberry Pi \cite{raspi} is known for a series of small single-board computers that provides computing in a low cost, credit-card sized computer. There are also a majority of alternatives for this device which fall into single-board computers (SBCs) with powerful system-on-chip (SoC) such as Onion Omega2+ \cite{onionomega}, Orange Pi \cite{orangepi}, Banana Pi \cite{bananapi}, Rock64 \cite{rock}, Arduino \cite{arduino}, Asus Tinker Board \cite{tinkerboard}, Odroid \cite{odroid}, Pin64 \cite{pin64}, Cubieboard \cite{cubieboard}, BeagleBoard \cite{beagleboard}, LattePanda Alpha \cite{lattepanda}, UDOO BOLT \cite{udoobolt}, Libre Computer Le Potato \cite{librecomputer}, and NanoPi \cite{nanopi}.


\subsubsection{Software}

From a software perspective, there have been libraries for processing, such as OpenCV \cite{opencv} a cross-platform library that contains various image process and computer vision algorithm. This library also supports deep learning models (e.g., TensorFlow) and over 2500 optimized classic or state-of-the-art computer vision algorithms using MMX and SSE instructions when available. There are other frameworks alike OpenCV including SimpleCV \cite{simplecv} a Python-based open-source framework that is recommended for prototyping, and the other Python-based library is Scikit-image\cite{scikitimage} acting as a toolbox for SciPy and provides different algorithms for image processing. Accord.NET framework \cite{accordnetframework} that is a C\#-based framework providing several functionalities such as providing machine learning, computer vision, and image processing methods. BoofCV \cite{boofcv} as an open-source java library for real-time computer vision and robotics applications consisting of application-based packages for image processing, standard functions, and feature extraction algorithms. FastCV computer vision \cite{fastcv} that is implemented for ARM architecture and is optimized for Qualcommâ€™s Snapdragon processors for providing the most frequently used vision processing functions. MATLAB \cite{matlab} is a paid programming platform that comes with the computer vision processing toolbox. Deepface \cite{deepface} is a Python-based and lightweight face recognition and facial attribute analytics framework (e.g., age, gender, or emotion and race) that employs state-of-the-art models such as Google FaceNet \cite{schroff2015facenet} or VGG-Face \cite{vggface}. Point Cloud Library (PCL) is a C++-based library for three-dimensional image processing \cite{pcl}.  In addition, NVIDIA CUDA-X \cite{nvidialib1} provides libraries, tools, and technologies for delivering high-performance artificial intelligence application domains. There is also the NVIDIA Performance Primitives (NPP) \cite{nvidialib2} library that facilitates GPU-accelerated vision processing. Detectron2 \cite{detectron2} is Facebook artificial intelligence library for providing the latest technology and development in detection and segmentation algorithms, and supports computer vision research projects and production applications in Facebook.

Moreover, neural network frameworks (and related tools) have been introduced to the image processing which have been used widely such as YOLOv3 \cite{yolov3}, MobileNet \cite{howard2017mobilenets}, ResNet \cite{he2016identity}, VGG \cite{simonyan2014very}, NASNet \cite{zoph2018learning}, PNASNet \cite{liu2018progressive}, Keras \cite{kerasapi}, Caffe \cite{cafeapi}, PyTorch \cite{pytorch}, Albumentations \cite{albumentations}, OpenVINO \cite{openvino}, and TensorFlow \cite{tensorflow}. These are based on the convolutional deep neural network for the purpose of object of interest detection.




\subsection{Resource Provisioning Methods}

\subsubsection{Heuristic-based}
Previous studies mentioned in Section \ref{sec:qoe} utilize heuristic methods to improve the QoE, e.g., \cite{Lu2020ToN_NetVision, Felemban2020TMC, Hung2018SEC, Zhang2017nsdi, Hung2018SEC, Han2016MobiSys}. There still exist other research studies whose aims lead to better resource allocation. 

Grassi \emph{et al.} \cite{Grassi2017SEC} proposed a method to estimate the location of parked cars in a single frame using a localization algorithm. The approach involves camera calibration against well-known objects in the surrounding environment.  In contrast, Yi \emph{et al.} \cite{Yi2017SEC_LAVEA} proposed a light-weight virtualization on top of the operating system. It is a two-phase optimization process in which, in the first phase, bandwidth is allocated, and the next phase aims to leverage nearby edge resources to expedite the task completion time. Shen \emph{et al.} \cite{Shen2017CVPR} leveraged short-term class skew (i.e., objects of interests over some time) to accelerate video classification using CNNs. The research proposes a heuristic algorithm based on exploration and exploitation strategies to address the sequential model selection problem, which is formulated as the Oracle Bandit Problem (OBP). The proposed approach seeks to estimate skew at test-time, producing a specialized model if possible and using the model as long as the skew lasts and then reverts to one of the classifier models referred to as oracle (e.g., GoogleNet). To create a specialized model, the research study selects a specified number of dominant classes and a randomly selected subset from other classes with a different label from the original data, creating new training data sets. The dominant classes make up a percentage of the new dataset used to train the compact model. The oracle model is swapped with a less expensive but compact model for exploiting skew to return early with the classification result if inputs belong to the frequent classes in the incoming distribution. Otherwise, it uses the oracle model.

The research proposes a heuristic algorithm based on exploration and exploitation strategies to address the sequential model selection problem, which is formulated as the Oracle Bandit Problem (OBP). 

\subsubsection{Optimization theory-based}

Similar to \cite{Lu18INFOCOM, Lu18TMC_CrowdVision,Fu2019ATC_EdgeWise,Yi2017SEC_LAVEA}, 
Drolia \emph{et al.} \cite{Drolia2017ICDCS_Cachier} applied tunable cache size as the knob to minimize the latency. The authors presented Cachier, a system that optimizes the cache size to minimize latency in computation-intensive recognition applications. The authors model edge servers as caches and use novel optimizations to adaptively balance load between the edge and the cloud.
Long \emph{et al.} \cite{Long2017TMM} proposed a solution for the group formation problem by transforming it into a winner determination problem. This new formulation can be solved using a 2-approximation algorithm that significantly reduces the complexity of the problem.


O'Gorman \emph{et al.} \cite{O'Gorman2018ICPR} studied the upper-bound requirements for processing and bandwidth for a video analytics application analytically and experimentally on real videos to provide guidance for algorithm placement between the edge and the cloud. However, Valls \emph{et al.} \cite{Valls2018MILCOM} modeled the resource allocation as a network processing model in which targets the order of processing, where two resource allocation algorithms are presented due to the dynamicity of the network. The proposed approach utilizes two algorithms: a backpressure-based algorithm and a backpressure plus interior-point method algorithm. The former allocates data analytics resources based solely on the congestion level in the system, while the latter considers the fact that some resources have a constant demand.


\subsubsection{DRL-based}
Huang \emph{et al.} \cite{Huang18MM_QARC} proposed a rate deep reinforcement learning algorithm focusing on a higher perceptual quality rate with lower bitrate. Based on a trained neural network, the algorithm predicts future bitrates considering the observation of current network status. The model is divided into two separate neural networks; an ANN to predict future video quality based on the previous video frame and the other ANN as a reinforcement learning one that determines the bitrate with the first model's result. The first model has a combination of CNNs to extract image features and a recurrent neural network for capturing temporary features. 
Rao \emph{et al.} \cite{Rao2017ICCV} introduced a deep reinforcement learning method for video face recognition that leverages attention mechanisms. They approached the problem of identifying relevant regions of a video as a Markov decision process and trained an attention model through a DRL framework, which does not require additional labeling.
Supancic \emph{et al.} \cite{Supancic2017ICCV} modeled online video object tracking and transformed the tracking formula into a partially observable decision-making process with DRL to understand the best decision-making strategy.


\subsubsection{Incentive-aware}
Zhan \emph{et al.} \cite{Zhan2016MSN} presented a scheme for optimizing the interaction between video providers and mobile users, by formulating it as a two-person cooperative game. The proposed scheme used Nash bargain game theory to obtain the optimal cooperation decision, resulting in a high data delivery ratio, low communication and computation overhead, and excellent economic properties.
Wu \emph{et al.} \cite{Wu2017TMM} proposed a pricing mechanism based on Stackelberg game theory to inspire device-to-device video distribution from core users. This approach can reduce the load on base stations and improve the effectiveness and reliability of video transmission.
Hu \emph{et al.} \cite{Hu2020TPDS} present a solution to the challenge of offloading heterogeneous video analytics tasks that leverages game theory. They cast the problem as a minority game, in which each participant must make decisions independently in each round, and the players who make the minority choice win. The game is played by multiple players who lack complete information sucn as the number of video analytics tasks or resources, which creates incentives for players to cooperate with each other.


\subsection{Other Optimizations}
In the following, some other optimization studies in video analytics not mentioned above will be introduced, including GPU/CPU acceleration and data/streaming storage.


\subsubsection{GPU/CPU acceleration}
To reduce the costs of supporting a large-scale deployment with hundreds of video cameras, Stone \emph{et al.} \cite{Stone2019SECON} introduced Tetris, a system that incorporates various optimization techniques from computer vision and deep learning fields in a synergistic manner.
Liu \emph{et al.} \cite{Liu2018EdgeSys_EdgeEye} simplify the memory transfer between CPU and GPU with Nvidia CUDA mapped memory feature. The DetectNet component leverages TensorRT to manage the GPU inference engine, including tasks such as initializing the inference engine, creating the inference runtime, loading the serialized model, creating the inference execution context, and executing the inference engine.


Integral histograms are widely used for extracting multi-scale histogram-based regional descriptors, which are essential components of many video content analytics frameworks. Poostchi \emph{et al.} \cite{Poostchi2017CoRR} evaluated different approaches to mapping the computation of integral histograms onto GPUs, using various kernel optimization strategies.

\subsubsection{Data/Streaming Storage}

Maintaining a cache server in the edge node can directly provide video service-related content to users and at the same time, reduce the number of requests forwarded to back-end servers. It can
minimize the expected latency for users and reduce network load and back-end load. The storage system can store various data ingested and generated during the video encoding process as this data can play a role in the subsequent analytics, and the video service can be pushed to the user faster.
Huang \emph{et al.} \cite{Huang2017sosp_SVE} presented a streaming video engine that uses a preprocessor to store original upload videos in parallel onto disk for fault tolerance. To process the video data, the engine employs a scheduler that schedules tasks on workers. These workers pull data from the preprocessor and store the processed data onto disk.


The term ``zero-streaming" camera refers to cameras that capture videos to local storage without streaming anything. They are reactive and highly efficient, consuming only network and cloud resources to analyze queried video footage.
Xu \emph{et al.} \cite{Xu2019arXiv_2} exploited the zero-streaming paradigm and minimized the ingestion cost, shifting as much as possible to the query execution. Zero-streaming cameras capture videos to the local flash storage (cheap and large) without uploading any; only in response to user queries, they communicate and cooperate with the cloud to analyze the stored videos.

