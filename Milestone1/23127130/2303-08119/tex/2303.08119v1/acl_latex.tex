% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{acl}
\usepackage{acl}


% Standard package includes
\usepackage{times}
\newcommand{\Rom}[1]{\uppercase\expandafter{\romannumeral #1\relax}}
\newcommand{\rom}[1]{\lowercase\expandafter{\romannumeral #1\relax}}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{lipsum}
\usepackage{graphicx}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\newcommand{\jiuhai}[1]{{ \color{blue}{[JC: #1]} }}
\newcommand{\ty}[1]{\textcolor{orange}{Tianyi: #1}}
\newcommand{\clc}[1]{\textcolor{red}{LC: #1}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{It Takes One to Tango but More Make Trouble?\\In-Context Training with Different Number of Demonstrations}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Jiuhai Chen [1] \And Lichang Chen \And Chen Zhu \And Tianyi Zhou \AND
%         [1] University of Maryland \And [2] Google \AND    \\}
% if the names do not fit well n one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
\author{Jiuhai Chen \\ University of Maryland \\  jchen169@umd.edu
        \And  Lichang Chen \\ University of Maryland \\  bobchen@umd.edu %\And Chen Zhu \\ Google \\  chenzhu@umd.edu 
        \And
        Tianyi Zhou \\ University of Maryland \\tianyi@umd.edu }
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


% \author{First Author \\
%   Affiliation / Address line 1 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   \texttt{email@domain} \\}

\begin{document}
\maketitle
\begin{abstract}
Large language models (LLMs) are capable to perform complex reasoning by in-context learning (ICL) when provided with a few input-output demonstrations (demos) and more powerful when intermediate reasoning steps (``chain of thoughts (CoT)'') of the demos are given. Is it necessary to use multi-demo in ICL? In this paper, we study ICL using fewer demos for each test query on the tasks in~\cite{wei2022chain}. Surprisingly, we do not observe significant degradation when using only one randomly chosen demo. To study this phenomenon, for each test query, we categorize demos into ``correct demos'' leading to the correct answer, and ``wrong demos'' resulting in wrong answers. Our analysis reveals an inherent bias in those widely studied datasets: most demos are correct for a majority of test queries, which explains the good performance of using one random demo. Moreover, ICL (with and w/o CoT) using only one correct demo significantly outperforms all-demo ICL adopted by most previous works, indicating the weakness of LLMs in finding correct demo(s) for input queries, which is difficult to evaluate on the biased datasets. Furthermore, we observe a counterintuitive behavior of ICL using multi-demo, i.e., its accuracy degrades(improves) when given more correct(wrong) demos. This implies that ICL can be easily misguided by interference among demos and their spurious correlations. Our analyses highlight several fundamental challenges that need to be addressed in LLMs training, ICL, and benchmark design. 
% on all seven datasets, we find that a single demo selected performs significantly better than all demos or a subset of them. 
% Moreover, randomly selecting one demo leads to a similar or even better performance than using all demos. 
% For every test query, CoT prompting. We further observe that CoT with multiple correct demos can result in wrong answers for some test queries while multiple wrong demos sometimes lead to correct answers. These observations expose the weakness of LLMs in distinguishing correct demos from wrong ones and the harmful interference among (even all correct) demos in CoT prompting. In addition, our study reveals an inherent bias widely existing in CoT benchmark datasets, i.e., a majority of test queries have much more correct demos than the others so they are easy samples that can hardly reflect the true capability of LLMs on selecting correct demos and aligning the test query to correct reasoning steps. 
\end{abstract}

\section{Introduction}

% \ty{Introduce few-shot prompting first and then the improvement by CoT. Introduce how the demos are generated for CoT. Then list some questions relevant to our study, e.g., what is the role each demo plays in CoT? Are they equally important? How does LLM exploit multiple demos to produce the answer for a given test query? etc.} 
% The recent progress in Large Language models (LLMs) \cite{brown2020language, chowdhery2022palm, thoppilan2022lamda, rae2021scaling} has demonstrated the impressive capabilities of LLMs in reasoning tasks. One of the most remarkable behaviors observed in LLMs is in-context learning \cite{brown2020language}, which provides LLMs with instruction and exemplars. However, conventional few-shot prompting performs poorly on complex reasoning tasks \cite{wei2022chain}. Recently, a typical way of applying LLMs in complex reasoning tasks including arithmetic reasoning, commonsense reasoning, and symbolic reasoning is to elaborate the intermediate reasoning step in the exemplar \cite{wei2022chain}, namely Chain-of-thoughts (CoT) prompting. 

The recent race of large Language models (LLMs) \cite{brown2020language, chowdhery2022palm, thoppilan2022lamda, rae2021scaling} shows that the capability of reasoning can be significantly improved with the scaling of model size. One of the most remarkable behaviors observed in LLMs is in-context learning (ICL) \cite{brown2020language}, which provides LLMs with human-written instruction and a few exemplars or demonstrations (demos), along with the input queries. However, conventional few-shot prompting performs poorly on complex reasoning tasks~\cite{wei2022chain}. Recently, an effective ICL strategy for complex reasoning tasks, including arithmetic reasoning, commonsense reasoning, and symbolic reasoning, is to elaborate the intermediate reasoning step in each demo \cite{wei2022chain}, namely Chain-of-thoughts (CoT) prompting. 

% \ty{This can be moved to related work or experiments} Furthermore, incorporating with output space emsembling, self-consistency \cite{wang2022self} generates several chain of thoughts and arrives at the final answers via majority vote. 

ICL relies on human engineering and expertise in designing demo questions, intermediate reasoning steps, and final answers so the LLM can generalize them to a variety of unseen queries. However, it is inefficient and impractical to design demos for different queries but a fixed set of demos might not cover all the possible queries. In addition, adding demos (especially in CoT prompting) significantly increases input tokens, which are costly and may exceed the maximum input length of LLMs. 
% choose the questions and annotate them with intermediate reasoning steps and final answers. 
% To mitigate the manual effort, there has been 
To provide better demos for efficient ICL and save human efforts, a very recent line of work studies automatic prompting~\cite{zhang2022automatic, wang2023large, arora2022ask}, which leverage LLMs to select demo questions and construct their answers and CoTs for ICL. 
They save human labor on creating demos but do not address several fundamental questions in ICL, e.g., \textit{How many demos are necessary for ICL? Can LLMs in ICL figure out which demo(s) is more useful to each test query? Does ICL leverage all the demos or mainly rely on a few of them to resolve each test query? Can LLMs in ICL combine the strengths of multiple demos to improve the answers?}

% To eliminate manual designs, they leverage the LLMs to automatically construct the demos with reasoning chains. Unlike previous works focusing on finding better Demos, our study investigates the capability of LLMs on (1) distinguishing the relevant demos from misleading demos for each test query and (2) building the correct (instead of spurious) connections between the test query and given demos.

% \ty{Try to either use the observations from these works as motivations for our study or move them to related work} 
% Another research line attempts to explore the reasons behind the success of chain-of-thoughs \cite{madaan2022text, saparov2022language}. These works undertake initial steps towards the understanding behind the success of CoT. However, the systematical mechanism behind CoT still remains ambiguous. \clc{You could consider moving some experiment results here as the motivation since our main point is not on investigating the mechanism behind the CoT.}

% Our questions are as follows:

% \begin{itemize}
%     \item Are multiple demos (e.g., 8 demos for arithmetic reasoning) necessarily the most effective and efficient for each test query?
%     \item Can we decrease the number of demos without hampering the model performance?
%     \item  What is the number of the minimal requisite demos to facilitate the LLMs to generate the expected answers?
% \end{itemize}

In this paper, we take the first step toward better understanding the effect of multiple demos 
in ICL through a series of empirical studies on the demos and benchmarks widely used in CoT prompting~\cite{wei2022chain}, which covers a diverse set of reasoning tasks. 
In particular, we investigate how the ICL (with and w/o CoT) performance changes when varying the number of demos and the impact of each demo on different test queries. We start with an extreme case of ICL using only one demo randomly chosen for each query. Surprisingly, compared to the default 8(or 7)-demo ICL in previous work, we do not observe a significant drop in the test accuracy. But does this imply that multiple demos are unnecessary to ICL? To study this phenomenon, we take a closer look at the proportion of correct demos (i.e., the demos leading to correct answers in one-demo ICL) for each test query. Statistics on all the datasets reveal a widely existing bias of easy queries, i.e., most demos are correct for a majority of queries, for which one (random) demo is all they need. 

That being said, how does ICL perform on the rest queries with fewer correct demos? Unfortunately, though provided with some correct demos, ICL fails to produce correct answers for many of them. We verify this by evaluating ICL with one correct demo, which significantly outperforms the widely used multi-demo ICL. This exposes a weakness of LLMs, i.e., they are not good at identifying the correct demo(s) and ignoring the wrong ones for each query in multi-demo ICL, even when more details such as CoT are given. Our further analysis reveals another deeper reason for this. Specifically, we start from ICL with one correct(wrong) demo but adding more correct(wrong) demos results in a counterintuitive degradation(improvement) of ICL accuracy, indicating a negative impact of the interference or spurious correlation among demos on the LLMs. Therefore, multiple demos might provide more information than a single demo but the current LLMs and ICL methods cannot fully exploit them and filter out misleading interference. 


% \ty{Try to ask a question (as the motivation) before introducing each observation/insight. I think we have three observations related to (1) Single-demo CoT prompting; (2) multiple all-correct or all-wrong demos based CoT prompting; (3) lack of hard examples in the current benchmark datasets for CoT.} In this work, we follow the second pathway to develop deeper understanding of CoT mechanism from the perspective of number of demos. For instance, \citep{wei2022chain} hand-crafts 8 demos for all arithmetic reasoning tasks, which brings into the question that \emph{do all 8 demos plays equal role or several of them play a primary role in enabling the success of CoT in divers reasoning task?}


% Our observations are as follows:

% \begin{itemize}
%     \item For most datasets, randomly selecting one exemplar along with self-consistency can achieve comparable performance with employing full list of demos. 
%     \item 
%     \item
% \end{itemize}

\section{Related Work}

% \ty{Try to write related work as a rebuttal to potential questions from reviewers such as ``what is the main novelty/difference of your work compared to xxx?''. This section currently is less organized. Try to use each related work as a support or motivation for our study, for example, they did not consider xxx which is a key to demystifying CoT prompting, or they observed that the prompting results are sensitive to the demos and focus on finding better demos but did not analyze whether LLM uses the demos in the correct way, etc.}
\paragraph{ICL.}
In-context learning has been key to superior performance in LLM \cite{brown2020language} over a variety of downstream tasks. Prior works primarily focus on how to formulate the prompts, e.g., retrieving 
semantically similar to a test sample to formulate its corresponding prompt \cite{liu2021makes}, estimating the model's bias, and then fitting calibration parameters \cite{zhao2021calibrate}, training an efficient dense retriever from the data to retrieve training examples as prompts at test time \cite{rubin2021learning}.

\paragraph{CoT and its variants.} CoT has been recently introduced to elicit the reasoning abilities of LLMs \cite{wei2022chain} by augmenting each demo with a chain of rational steps. Many follow-ups works further improve the performance of CoT, e.g., self-consistency~\cite{wang2022self} draws an ensemble of outputs for majority voting to replace the greedy decoding. However, CoT still heavily relies on human expertise to annotate the reasoning chains. A handful of recent works have explored the idea of automatic prompting~\cite{zhang2022automatic, huang2022large,wang2023large}. For instance, Auto-CoT \cite{zhang2022automatic} proposes to select queries of the demos via clustering all test queries and sampling demo queries with diversity. \cite{huang2022large} fine-tunes an LLM with high-confidence rationale-augmented answers for unlabeled questions. \citet{wang2023large} views the LLM as a topic model and proposes an algorithm selecting the optimal demo from a set of annotated data. While beneficial, most automatic prompting methods focus on bypassing human engineering and building better demos from a set of questions. But they do not investigate whether demos are used in the correct way by LLMs in ICL. In contrast, we find that the original demos provided by \cite{wei2022chain} include adequate information (e.g., one correct demo per query) for the LLMs to produce correct answers. 
 


\paragraph{The role of demos in ICL.} Several works have                       explored the mechanism behind the success of CoT prompting. \citet{min2022rethinking} observes that label correctness is not the critical reason for the success of few-shot ICL/prompting. \citet{madaan2022text} also finds that the label correctness is immaterial to the task on GSM8K. Instead, \citet{madaan2022text} constructs three key components in rational and identifies which component plays a vital role in CoT. \citet{saparov2022language} concludes that LLMs are capable of making correct individual deduction steps but have difficulty systematically exploring the different options. \citet{wang2022towards} shows that CoT  reasoning is possible even with invalid demos. These works try to understand what makes CoT prompting effective. However, few works focus on varying the number of demos and inherent dataset bias in few-shot ICL or CoT prompting. 


\section{Background and Experimental Setup}

\subsection{Tasks and Datasets}
We conduct a series of experiments on a variety of reasoning benchmarks: 
\textbf{arithmetic reasoning:} GSM8K \cite{cobbe2021training}, MultiArith \cite{roy2016solving}, AddSub \cite{hosseini2014learning}, SVAMP \cite{patel2021nlp}, AQuA \cite{ling2017program} and SingleOp \cite{wei2022chain}. 
\textbf{commonsense reasoning:} CSQA \cite{talmor2018commonsenseqa}.
\textbf{symbolic reasoning:} Coin-flip \cite{wei2022chain}. 

The overall statistics are listed in table \ref{tab:data}.

\begin{table}[ht]
\centering
\scalebox{0.85}{
\begin{tabular}{llll}
% - & \multicolumn{5}{c}{\textbf{arithmetic reasoning}} & \textbf{commonsense reasoning} & \textbf{symbolic reasoning} \\\
\hline
\hline
 & TASK & \# Demo & \# Query  \\
 \hline
GSM8K & Arithmetic & 8 & 1319   \\
MultiArith   & Arithmetic & 8 & 600   \\
AddSub & Arithmetic & 8 &  395  \\
SVAMP  & Arithmetic & 8 & 1000  \\
AQuA & Arithmetic & 4 & 254 \\
SingleOp  & Arithmetic & 8 & 508  \\
CSQA  & Commonsense & 7 & 1221  \\
% Last Letter & Symbolic & 4 & 500  \\
Coin-flip  & Symbolic & 8 & 500  \\
\hline
\hline
\end{tabular}}
\caption{\label{tab:data}
Statistics of datasets. \# Demo is the number of CoT exemplars provided by \citet{wei2022chain}. }
\end{table}

\subsection{Language model and prompt}
To efficiently conduct an extensive number of experiments, we focus on code-davinci-002 \cite{chen2021evaluating, chowdhery2022palm} from the GPT-3 model family to evaluate performance. We choose Codex because it is a programming generation model with superior performance, especially for reasoning tasks \cite{wang2022self, zhang2022automatic}. We explore two prompting settings as follow: 

\paragraph{Few-shot prompting.} Standard few-shot prompting~\cite{brown2020language} in which demos are formatted as \emph{Question + Answer} pairs appended to each test query. 
\paragraph{CoT prompting.} We also conduct experiments on CoT prompting where each demo is augmented by a chain of thoughts \cite{wei2022chain} in the form of \emph{Question + rationale + Answer}. 


% \begin{table*}[ht]
% \centering
% \begin{tabular}{lllllll}
% % - & \multicolumn{5}{c}{\textbf{arithmetic reasoning}} & \textbf{commonsense reasoning} & \textbf{symbolic reasoning} \\\
% \hline
%  & GSM8K & MultiArith & AddSub & SVAMP & AQuA & SingleEq  \\
%  \hline
% Standard (full demo) & 19.7 & 44.0 & 90.9 & 69.9 & 29.5 & 86.8  \\
% Standard (one demo)  & 17.9 & 41.5 & 83.8 & 68.5 & 28.3 & 83.1  \\
% \hline
% CoT (full demo) & 60.1 & 96.2 & 89.4 & 75.8 & 39.8 & 93.1  \\
% CoT (one demo)  & 53.2 & 93.7 & 83.3 & 68.2 & 37.2 & 88.8  \\
% \hline
% CoT + self-consistency (full demo) & 78.0 & 100.0 & 91.6 & 86.8 & 52.0 & 92.8  \\
% CoT + self-consistency (one demo)  & 73.1 & 100.0 & 91.1 & 84.6 & 46.4 & 94.9  \\
% \hline
% \end{tabular}
% \caption{\label{tab:arithmetic}
% Accuracy comparison on arithmetic reasoning task with full prompts and randomly selecting one prompt. }
% \end{table*}


% \begin{table*}[t]
% \centering
% \begin{tabular}{lllll}
% % - & \multicolumn{5}{c}{\textbf{arithmetic reasoning}} & \textbf{commonsense reasoning} & \textbf{symbolic reasoning} \\\
% \hline
%  & StrategyQA & CSQA & Letter & Coinflip \\
%  \hline
%  Standard (full demo) & 67.1  & 82.3 & -  & -\\
% Standard (one demo)  & - & 78.1 & - & 55.0 \\
% \hline
% CoT (full demo) & 73.4  & 79.0 & 70.4  & 99.0\\
% CoT (one demo)  & - & 70.3 & 71.4 & 91.6 \\
% \hline
% Self-consistency (full demo) & 79.8 & 81.5 & 73.4 & 99.5\\
% Self-consistency (one demo)  & - & 81.2 & 75.4 & 97.8 \\
% \hline
% \end{tabular}
% \caption{\label{tab:csqa}
% Accuracy comparison on commonsense reasoning and symbolic reasoning task with full prompts and randomly selecting one prompt.
% }
% \end{table*}

\section{One-Demo Prompting}

Recall that Manual-CoT handcrafts a few demos for different tasks shown in Table \ref{tab:data} (e.g., 8 prompted exemplars in arithmetic). However, 8 human-annotated demos are not necessarily the most effective for each test sample. In this section, we explore the minimal requisite demos to facilitate the LLMs to generate the expected answers. In particular, we observe that the number of demos plays a key role in model performance, and including more demos may  hamper the performance. 

\subsection{Prompting with One Random Demo}

We compare the following two methods for both Few-shot prompting and CoT prompting:

\textbf{One Random Demo} is randomly selected from a few demos crafted by \citep{wei2022chain}. We prepend this single demo to the test sample and query the language model once. 
% For all arithmetic reasoning tasks, we randomly choose one demo from 8 manually written exemplars in \citep{wei2022chain}; for CSQA, one exemplar is randomly selected from 7 hand-crafted demos provided by \citep{wei2022chain}; for Last Letter Concatenation, we randomly select one prompt from 4 manually written exemplars in \citep{wei2022chain}. \ty{Consider moving this paragraph to background} 
%  We also consider self-consistency for further boosting the performance under randomly selecting one demo.

\textbf{All Demos} is the baseline reported by \cite{wei2022chain}, prepending all demos (e.g., 8 demos for arithmetic reasoning tasks) to the test sample and query the model once.

% \textbf{Self-Consistency} is also considered to further boost the performance of CoT prompting \cite{wang2022self}. Under self-consistency setting, we prompt the language model with randomly selecting one demo and query the model once with temperature $T=0.7$. We repeat the procedure 40 times following \citep{wang2022self} and take the majority vote for the final answer. 

Results under Few-shot prompting and CoT prompting setting on various datasets are shown in Figure \ref{fig:bar_wo_cot} and Figure \ref{fig:bar_cot} respectively. For both Few-shot prompting and CoT prompting, the performance of One random demo (blue bar) slightly decreases in the range of 0-7\% accuracy compared with all demos (green bar). \textbf{This suggests that reducing the number of demos to ONE only marginally hurts performance, all demos are not necessary to achieve significant performance gains.} 
% Meanwhile, Self-Consistency effectively narrows the gap between randomly choosing one demo and employing full demos. In particular, for MultiArith, Addsub, SVAMP, SingleOp and Coinflip, reducing the  number of demos has little to no impact on the reasoning performance. Notably, for SingleOp and Letter, one randomly selected demo outperforms full demo under self-consistency setting. 

\begin{figure}[ht]
\includegraphics[width=8cm]{fig0b.pdf}
\centering
\caption{\label{fig:bar_wo_cot}
 ICL without CoT: Prompting with one random demo has a slightly lower accuracy than few-shot prompting (8 or 7 demos). Prompting with one correct demo significantly outperforms few-shot prompting. \looseness-1}
\end{figure}


\begin{figure*}[ht]
\includegraphics[width=12cm]{fig0a.pdf}
\centering
\caption{\label{fig:bar_cot}
ICL with CoT: Prompting with one random demo has a slightly lower accuracy than CoT prompting (8 or 7 demos). Prompting with one correct demo significantly outperforms CoT prompting. 
 % The performance of One Random Demo is slightly worse than full Demos under CoT prompting setting. One Correct demo significantly outperforms all demo under.
 }
\end{figure*}

Based on the observation, we conclude that reducing the redundant exemplars leads to a comparable performance with all demos used. 
The minimal requisite demos number for most datasets is one, even randomly selecting one demo, which is adequate to empower the LLMs to generate the expected answer. This conclusion is of great practical value since it is able to significantly save cost when calling the API model, e.g., GPT3 is billed based on the number of tokens sent in the prompt (\$ 0.02 per 1000 tokens). Less number of input tokens straightly translates to reduced billing cost, run-time, and carbon footprint. We only keep one demo for each test sample, drastically shorting the length of input prompts token (e.g., for arithmetic reasoning task, eight times less than the standard CoT \cite{wei2022chain}) and leading to comparable performance. Furthermore, the observation brings to the questions: 
\begin{itemize}
    \item \emph{What is the potential performance if we always select the most effective demo rather than random selection? }
     \item \emph{For each test-time sample, what is the percentage of the effective demo which individually facilitates the LLM to generate factual conclusions?}
\end{itemize}






% \section{CoT with Known Label}
\subsection{Correct/Wrong Demos and Hard/Easy Samples in Datasets}

To proceed with the questions, we first define the Correct/Wrong demo. For each test-time sample, we partition all demos into "\textbf{Correct Demos}" enabling the model to generate an expected answer and \textbf{"Wrong Demos"} leading to wrong answer under single-demo prompting setting. One example of "Wrong/Correct Demo" under CoT prompting is shown in Figure \ref{fig:correct/wrong demo}. 
 
 \begin{figure*}[ht!]
 \centering
\includegraphics[width=0.9\textwidth]{fig5.pdf}
\caption{\label{fig:correct/wrong demo}
Wrong/Correct Demo. In one-demo ICL for a test query, a wrong demo leads to an incorrect answer while a correct demo results in the correct answer.
% The illustration of Wrong/Correct Demo. For the same test query, Wrong Demo leads to incorrect answer while Correct Demo results in right answer.
}
\end{figure*}

For each test-time sample, we partition all demos into "\textbf{Correct Demos}" and \textbf{"Wrong Demos"} in such single-demo prompting. Figure \ref{fig:easy/hard demo} gives one example of a test query with 8 Correct Demos and the other example with 8 Wrong Demos under CoT prompting. For each test-time sample, we also define the \textbf{Easy Sample} with 6 or over 6 Correct Demos and \textbf{Hard Sample} with 0 Correct Demo. The test queries shown in Figure \ref{fig:easy/hard demo} are also Hard/Easy samples. 

In this section, we attempt to solve the question: \emph{For each test-time sample, what is the percentage of Correct/Wrong Demo? For each dataset, what is the percentage of Easy/Hard samples? }

We choose two representative datasets in arithmetic reasoning and commonsense reasoning tasks: CSQA and GSM8K. Figure \ref{fig:stat_gsm8k} (a) and Figure \ref{fig:stat_csqa} (a) give statistics of all test queries in terms of the number of correct demos in GSM8K and CSQA. For both CSQA and GSM8K, we observe that Easy samples account for the majority part of the dataset while Hard samples take up a very small fraction. Notably, from Figure \ref{fig:stat_csqa} (a), around 58 \% of test examples in the CSQA dataset have 6 or over 6 correct demos (Easy sample). In other words, 58 \% of test queries in CSQA are more likely to generate the correct answer with selecting any single demo. In contrast, only 8 \% of test samples in the CSQA dataset own zero correct demos (Hard sample), indicating that any single demo fails to result in the expected answer.  Furthermore, it reveals that the percentage of easy and hard sample plays vital roles in the performance of original GSM8K and CSQA datasets. Note that for both GSM8K and CSQA, there is a moderate amount of test data (around 50\%) where any single demo is adequate to drive the LLMs to superior performance. It can act as an explanation to Figure \ref{fig:bar_wo_cot} and Figure \ref{fig:bar_cot} that the performance of randomly selecting a single demo closely matches for using all demos. Another interesting perspective, deduced from Figure \ref{fig:stat_gsm8k} (a) and Figure \ref{fig:stat_csqa} (a), we can calculate the expected accuracy by randomly selecting one demo. For instance, let $N$ be the number of demos in full demo list provided by \cite{wei2022chain} ($N=8$ for GSM8K and $N=7$ for CSQA), $p_n$ is the percentage value with $n$ correct demos shown in  Figure \ref{fig:stat_gsm8k} (a)  and Figure \ref{fig:stat_csqa} (a), then we have the expected accuracy under randomly choosing one demo: $\frac{1}{N}\sum_{n=1}^{N} p_n n$. The average accuracy for GSM8K is 52 \% and for CSQA is 71 \%, closely matching the results in Figure \ref{fig:bar_cot}.





 %% In line with one research line investigates the LLMs are sensitive to the format of prompt \cite{mishra2021reframing, rubin2021learning}, the order of example in the prompt \cite{lu2021fantastically} and instructions in the prompt \cite{ouyang2022training}. We also find the performance of LLMs is  sensitive to the demo selection. 
%  To this end, we oracle the upper bounds on the model performance in the case of randomly selecting one demo. In other words, for each test query, we select the Correct Demo and query LLMs once, namely Oracle Selected Demo. In the foregoing section, we have investigated the performance of randomly selecting one demo, and Oracle Selected Demo will hint at hypothesis that one demo potentially yields the superior performance under the appropriate  demo selection. 
% \subsection{Oracle selected Demo}
% To this end, for each test query, we select the Correct Demo and query LLMs once, namely Oracle Selected Demo.

\subsection{Prompting with One Correct Demo}
 From the foregoing section, we see Wrong/Correct demo partition varies with each test-time sample, and the selection of the demo has drastic repercussions on the model performance.  In this section,  we attempt to investigate the performance if we always select one Correct Demo, namely One Correct demo.

Figure \ref{fig:bar_wo_cot} and \ref{fig:bar_cot} illustrate the considerable improvement in the model accuracy if the One Correct Demo is selected under Few-shot prompting and CoT prompting respectively. For instance, the performance of GSM8K under One Correct demo (orange bar) in Figure \ref{fig:bar_cot} is around 83\%, a substantial improvement from fully using all demos. In other words, there are 83\% samples in GSM8K possessing at least one Correct Demo. The trend is consistent over all datasets: model witnesses a large performance boost from All demos case to One Correct demo. Based on the observation, we conclude that one Correct Demo is adequate for LLMs to generate the expected answer. Meanwhile, notice that Correct Demo is always included in the all-demo list, but One Correct Demo consistently outperforms all demos. This observation reveals that including redundant demos may potentially hamper the model's performance. Furthermore, it also exposes the weakness of LLMs in distinguishing Correct Demos from wrong ones, LLMs fail to capture the most effective demo and are easily distracted by Wrong demos.

% \subsection{TBD}
% Despite impressive results of Oracle Selected Demo, the rationale behind the success of Oracle Selected Demo remains unclear. A promising candidate explanation is to verify similarity between demo and test query since similarity-based retrieval methods widely utilized by LLMs prompting \cite{rubin2021learning, su2022selective}. 
% \begin{itemize}
%     \item \emph{Can we select the Correct Demo with similarities-based retrieving method? }
% \end{itemize}

% Unfortunately, retrieving method fails to identify the correct demo. We use all-MiniLM-L6-v2 \footnote{https://huggingface.co/tasks/sentence-similarity} to encode the sentence into numerical embedding and retrieve the most similar demo with test query based on cosine similarity. Table \ref{tab:retrieve} presents the results of similarity-based retrieval methods, showing that Retrieval-CoT closely matches the performance of Random-CoT.

% \begin{table*}[h]
% \centering
% \begin{tabular}{lllllll}
% % - & \multicolumn{5}{c}{\textbf{arithmetic reasoning}} & \textbf{commonsense reasoning} & \textbf{symbolic reasoning} \\\
% \hline
%  & GSM8K & MultiArith & AddSub & SVAMP & AQuA & SingleOp  \\
% \hline
% Random-CoT   & 53.2 & 93.7 & 83.3 & 68.2 & 37.2 & 88.8  \\
% Retrieval-CoT  & - & - & - & - & - & -  \\
% \hline
% Random-CoT + self-consistency  & 53.2 & 93.7 & 83.3 & 68.2 & 37.2 & 88.8  \\
% Retrieval-CoT + self-consistency & - & - & - & - & - & -  \\
% \hline
% \end{tabular}
% \caption{\label{tab:retrieve}
% Accuracy comparison on arithmetic reasoning task with retrieval-based and randomly selecting one prompt. }
% \end{table*}





% \begin{table*}[t]
% \centering
% \begin{tabular}{llllllll}
% % - & \multicolumn{5}{c}{\textbf{arithmetic reasoning}} & \textbf{commonsense reasoning} & \textbf{symbolic reasoning} \\\
% \hline
%  & GSM8K & MultiArith & AddSub & SVAMP & AQUA-RAT & CSQA & Letter \\
% \hline
% CoT (full demo) & 60.1 & 96.2 & 89.4 & 75.8 & 39.8 & 79.0 & 70.4 \\
% Self-consistency (full demo) & 78.0 & 100.0 & 91.6 & 86.8 & 52.0 & 81.5 & 73.4 \\
% \hline
% Oracle (one demo)  & 83.0  & 100.0 & 94.7 & 90.5 & 68.5 & 91.6 & 78.2 \\
% % Oracle (one demo) + s.c  & 86.7  & 100.0 & 95.2 & 94.0 & 66.5 & 92.4 & 84.6 \\
% \hline
% \end{tabular}
% \caption{\label{oracle}
% Accuracy comparison on different datasets of full prompts and randomly selecting one prompt.
% }
% \end{table*}


\section{Prompting with More Correct/Wrong Demos: Do more demos improve ICL?}
% In the foregoing section, we conclude that prepending more demos may significantly degrades the model performance. For each test query, we firstly partition all demos into "Correct Demos" enabling the model to generate expected answer and "Wrong Demos" leading to wrong answer in such single-demo prompting.



% \begin{table*}[th]
% \centering
% \begin{tabular}{p{0.7\linewidth} | l | l}
% % \hline
% % \hline
% % \multicolumn{3}{c}{\textbf{Question: Q: James is counting his Pokemon cards. He has 30 fire type, 20 grass type, and 40 water type. If he loses 8 of the water 
% %  }} \\
% % \multicolumn{3}{c}{\textbf{type and buys 14 grass type, what's the percentage chance (rounded to the nearest integer) that a randomly picked card will be a water type?}} \\
% \hline
% \hline
%  Demo & Output & Demo Type  \\
% \hline
% There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? & 33 \% & Correct Demo \\
% \hline
% If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?  & 33 \%  & Correct Demo \\
% \hline
% Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? & 33 \%  & Correct Demo \\
% \hline
% Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?  & \textcolor{red}{48 \%}  & \textcolor{red}{Wrong Demo} \\
% \hline
% Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?  & 33 \%  & Correct Demo \\
% \hline
% There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?  & 33 \%  & Correct Demo \\
% \hline
% Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?  & 33 \% & Correct Demo \\
% \hline
% Olivia has 23. She bought five bagels for 3 each. How much money does she have left?  & 33 \% & Correct Demo \\
% \hline
% \hline
% \end{tabular}
% \caption{\label{tab:example} An example of test query with 7 Correct Demo and 1 Wrong Demo. The test query is \emph{James is counting his Pokemon cards. He has 30 fire type, 20 grass type, and 40 water type. If he loses 8 of the water type and buys 14 grass type, what's the percentage chance (rounded to the nearest integer) that a randomly picked card will be a water type?} We present the partition of full demo into Correct Demo and Wrong Demo. Here we omit the details of rationale for each demo.
% }
% \end{table*}

 \begin{figure*}[ht!]
 \centering
\includegraphics[width=1.0\textwidth]{fig4.pdf}
\caption{\label{fig:easy/hard demo}
Easy/Hard Samples from GSM8K: for the hard query (Mark plants a beanstalk ...), all the 8 demos are wrong and result in wrong answers in one-demo ICL; for the easy query (Alisa biked 12 miles ...), all the 8 demos are correct and lead to the correct answer. The 8 demos for arithmetic problems are from~\cite{wei2022chain}. 
% right-hand-side outputs are the outcome of two test queries under each single demo. 
}
\end{figure*}


\begin{figure}[th]
\includegraphics[width=8cm]{fig1a.pdf}
\centering
\caption{Pie chart on the number of correct demos per sample/query ($0\sim 6$ outside the pie chart) for queries in (a) the whole GSM8K dataset \label{fig:stat_gsm8k}; (b) GSM-Hard; (c): GSM-Easy.
}
\end{figure}

\begin{figure}[th]
\includegraphics[width=8cm]{fig1b.pdf}
\centering
\caption{
Pie chart on the number of correct demos per sample/query ($0\sim 6$ outside the pie chart) for queries in (a) the whole CSQA dataset \label{fig:stat_csqa}; (b) CSQA-Hard; (c): CSQA-Easy.
% Number $0\sim 6$ around pie chart denote the number of correct demos. (a):  The overall statistics of the correct demos for each test query in CSQA\label{fig:stat_csqa}. (b) and (c): The illustration of CSQA-Hard and CSQA-Easy.
}
\end{figure}

We raise the following two questions based on the foregoing section:
\begin{itemize}

\item \emph{Do existing CoT benchmark datasets really reflect the true capability of LLMs? Since datasets are composed of unbalanced easy and hard samples. }

\item \emph{Compared with one-demo ICL, do multiple demos really offer benefits to the LLMs to arrive at the correct answer?} 
\end{itemize}


To proceed with our investigation, we curate two benchmark variations: Commonsense Reasoning with easy samples (CSQA-Easy), which only comprises  test examples with 6 or over 6 correct demos (Easy sample) illustrated in Figure \ref{fig:stat_csqa} (c). Correspondingly, Commonsense Reasoning with Hard examples (CSQA-Hard) is composed of 8 \% test data with zero correct demo and 5 \% test data with one correct demo shown in \ref{fig:stat_csqa} (b). Similarly, for GSM8K dataset, we construct the GSM-Easy and GSM-Hard derived from GSM8K seen in Figure \ref{fig:stat_gsm8k} (b) and Figure \ref{fig:stat_gsm8k} (c). 

The benefit of such a tailored design is to tease apart the hard component of datasets that truly reflect the capability of LLMs. We also disentangle the easy component of the dataset which any randomly selected demo leads to the correct answer. We aim to investigate whether multiple demos are the most effective prompts for both easy and hard samples. 

\begin{figure}[!htb]
    \begin{minipage}{.5\textwidth}\centering
        \includegraphics[width=7cm, height=0.2\textheight]{fig2a.pdf}
        \caption{\label{fig:gsm8k_cot} Increasing demos in \textbf{CoT Prompting on GSM8K}: for each query in GSM-Easy(GSM-Hard), we start from a correct(wrong) demo, add more correct(wrong) demos to the prompt, but observe an accuracy degradation(improvement). 
        % Starting from randomly selecting one demo to randomly selecting 6 demo, more correct demos degrade the accuracy for GSM-Easy while more wrong demos improves the accuracy for GSM-Hard.
        }
    \end{minipage}%
    \hfill
    \begin{minipage}{0.5\textwidth}\centering
        \includegraphics[width=7cm, height=0.2\textheight]{fig2b.pdf}
        \caption{\label{fig:csqa_cot} 
        Increasing demos in \textbf{CoT Prompting on CSQA}: for each query in CSQA-Easy(CSQA-Hard), we start from a correct(wrong) demo, add more correct(wrong) demos to the prompt, but observe an accuracy degradation(improvement).
        % Increasing demos in CoT Prompting on CSQA: Starting from randomly selecting one demo to randomly selecting 6 demo, more correct demos degrade the accuracy for CSQA-Easy while more wrong demos improves the accuracy for CSQA-Hard.
        }
    \end{minipage}
\end{figure}

% \section{Spurious Correlations among Demos: Do multiple demos improve CoT?}
For GSM-Easy and GSM-Hard, we analyze the accuracy for problems with respect to the number of demos randomly selected from the full list of demos given by \cite{wei2022chain}. From Figure \ref{fig:gsm8k_cot}, we see a performance drop for GSM-Easy and a performance improvement for GSM-Hard with adding more demos. In particular, prepending any single demo achieves nearly 99\% accuracy for GSM-Easy, even 2-demo degrades the performance significantly. Surprisingly, composing multiple correct demos that individually generate the correct answer may hamper the model's performance. LLMs are easily distracted by more information and fail to capture the most useful instructions. In contrast to GSM-Easy, stacking more Wrong demos which individually generate the wrong answer may improve the model performance. Our finding reveals that for hard sample, multiple demos enhance the reasoning capabilities of LLMs, it facilitates the LLMs to gather useful information from diverse demos. The trend is consistent over CSQA-Easy and CSQA-Hard from Figure \ref{fig:csqa_cot}: multiple demos degrade the performance of CSQA-Easy while improving the accuracy of CSQA-Hard.


% In contrast to CSQA-Easy and GSM-Easy, the accuracy improvement from adding more demos occurs for both CSQA-Hard and GSM-Hard shown in Figure (\ref{fig:hard_cot}). Consistently around 10 \% improvement is observed when adding more exemplars into instruction.  


We conduct similar experiments on Few-shot prompting setting, Figure (\ref{fig:gsm8k_standard}) and Figure (\ref{fig:csqa_standard}) reflect the same conclusion: composing more demos that individually generate the correct answer may hamper the model performance for GSM-Easy and CSQA-Easy. Stacking more wrong demos enhances the reasoning capabilities of LLMs only for GSM-Hard and CSQA-Hard.

We conclude that existing CoT benchmarks fail to fully reflect the true capability of LLMs since Easy samples take up the majority part of datasets. Easy samples are insensitive to the demo selection. We advocate that in this scenario, demo selection including auto-prompting techniques is not necessarily the most effective and efficient way to arrive at the correct answer. Since any single demo is adequate to facilitate the LLMs to generate the expected answer. Furthermore, while sensitive to the demo selection, Hard samples account for a minority part of the dataset. Based on the foregoing observation, the success of Hard samples requires the meticulous design of prompts. Thus Hard samples potentially play a critical role in the design of prompts and exploration of the capability of LLMs. Adjusting the proportion of hard/easy examples is an interesting future work. 








\begin{figure}[!htb]
    \begin{minipage}{.5\textwidth}\centering
        \includegraphics[width=7cm, height=0.2\textheight]{fig3a.pdf}
        \caption{\label{fig:gsm8k_standard}
        Increasing demos in \textbf{Few-shot Prompting on GSM8K}: for each query in GSM-Easy(GSM-Hard), we start from a correct(wrong) demo, add more correct(wrong) demos to the prompt, but observe an accuracy degradation(improvement).
        % (Few-shot prompting for GSM8K) Starting from randomly selecting one demo to randomly selecting 6 demo, more correct demos degrade the accuracy for GSM-Easy while more wrong demos improves the accuracy for GSM-Hard.
        }
    \end{minipage}%
    \hfill
    \begin{minipage}{0.5\textwidth}\centering
        \includegraphics[width=7cm, height=0.2\textheight]{fig3b.pdf}
        \caption{\label{fig:csqa_standard}
        Increasing demos in \textbf{Few-shot Prompting on CSQA}: for each query in CSQA-Easy(CSQA-Hard), we start from a correct(wrong) demo, add more correct(wrong) demos to the prompt, but observe an accuracy degradation(improvement).
        % (Few-shot prompting for CSQA) Starting from randomly selecting one demo to randomly selecting 6 demo, more correct demos degrade the accuracy for CSQA-Easy while more wrong demos improves the accuracy for CSQA-Hard
        }
    \end{minipage}
\end{figure}


\section{Discussion}
In this paper, we study the effect of changing the number of demos on in-context learning (with and w/o chain-of-thoughts). We find that a single demo barely hurts the performance: ICL performance drops only marginally when the random demo is selected, single correct demo significantly improves the performance over the default multiple-demo ICL in previous work. We then investigate a series of aspects in the existing CoT benchmarks and examine which test query actually benefits the multiple demos. Results reveal that for hard sample, multiple demos truly enhances the reasoning capabilities of LLM, while for easy sample, multiple demos drop ICL performance. Furthermore, we examine the ratio of Hard/Easy sample for existing CoT benchmarks and find that Easy sample takes up the majority part of the datasets and the Hard sample only accounts for a small fraction of datasets under CoT prompting setting. Under this scenario, one demo is adequate to result in the expected answer for most test queries. Together, our findings expose the weakness of the LLM: they are not good at distinguishing the useful demo from multiple demos and easily mislead by the wrong demo. 

Our analyses also highlight several fundamental challenges that need to be addressed in the future, e.g., designing the benchmarks which can truly reflect the capacities of LLMs, and further understanding the role of demos in the ICL.













% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,acl_latex}
\bibliographystyle{acl_natbib}

\appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
