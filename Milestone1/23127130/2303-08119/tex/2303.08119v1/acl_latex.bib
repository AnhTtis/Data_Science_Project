% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{roy2016solving,
  title={Solving general arithmetic word problems},
  author={Roy, Subhro and Roth, Dan},
  journal={arXiv preprint arXiv:1608.01413},
  year={2016}
}


@inproceedings{hosseini2014learning,
  title={Learning to solve arithmetic word problems with verb categorization.},
  author={Hosseini, Mohammad Javad and Hajishirzi, Hannaneh and Etzioni, Oren and Kushman, Nate},
  booktitle={EMNLP},
  pages={523--533},
  year={2014}
}

@article{patel2021nlp,
  title={Are NLP models really able to solve simple math word problems?},
  author={Patel, Arkil and Bhattamishra, Satwik and Goyal, Navin},
  journal={arXiv preprint arXiv:2103.07191},
  year={2021}
}

@article{ling2017program,
  title={Program induction by rationale generation: Learning to solve and explain algebraic word problems},
  author={Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
  journal={arXiv preprint arXiv:1705.04146},
  year={2017}
}

@article{talmor2018commonsenseqa,
  title={Commonsenseqa: A question answering challenge targeting commonsense knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  journal={arXiv preprint arXiv:1811.00937},
  year={2018}
}

@article{mishra2021reframing,
  title={Reframing Instructional Prompts to GPTk's Language},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Choi, Yejin and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2109.07830},
  year={2021}
}


@article{rubin2021learning,
  title={Learning to retrieve prompts for in-context learning},
  author={Rubin, Ohad and Herzig, Jonathan and Berant, Jonathan},
  journal={arXiv preprint arXiv:2112.08633},
  year={2021}
}


@article{lu2021fantastically,
  title={Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  journal={arXiv preprint arXiv:2104.08786},
  year={2021}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{zhang2022automatic,
  title={Automatic chain of thought prompting in large language models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  journal={arXiv preprint arXiv:2210.03493},
  year={2022}
}

@article{wang2023large,
  title={Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning},
  author={Wang, Xinyi and Zhu, Wanrong and Wang, William Yang},
  journal={arXiv preprint arXiv:2301.11916},
  year={2023}
}

@article{arora2022ask,
  title={Ask Me Anything: A simple strategy for prompting language models},
  author={Arora, Simran and Narayan, Avanika and Chen, Mayee F and Orr, Laurel J and Guha, Neel and Bhatia, Kush and Chami, Ines and Sala, Frederic and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2210.02441},
  year={2022}
}


@article{madaan2022text,
  title={Text and patterns: For effective chain of thought, it takes two to tango},
  author={Madaan, Aman and Yazdanbakhsh, Amir},
  journal={arXiv preprint arXiv:2209.07686},
  year={2022}
}

@article{saparov2022language,
  title={Language models are greedy reasoners: A systematic formal analysis of chain-of-thought},
  author={Saparov, Abulhair and He, He},
  journal={arXiv preprint arXiv:2210.01240},
  year={2022}
}

@article{huang2022large,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}

@article{min2022rethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022}
}


@article{su2022selective,
  title={Selective annotation makes language models better few-shot learners},
  author={Su, Hongjin and Kasai, Jungo and Wu, Chen Henry and Shi, Weijia and Wang, Tianlu and Xin, Jiayi and Zhang, Rui and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A and others},
  journal={arXiv preprint arXiv:2209.01975},
  year={2022}
}


@article{wang2022towards,
  title={Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters},
  author={Wang, Boshi and Min, Sewon and Deng, Xiang and Shen, Jiaming and Wu, You and Zettlemoyer, Luke and Sun, Huan},
  journal={arXiv preprint arXiv:2212.10001},
  year={2022}
}

@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}

@inproceedings{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={International Conference on Machine Learning},
  pages={12697--12706},
  year={2021},
  organization={PMLR}
}

