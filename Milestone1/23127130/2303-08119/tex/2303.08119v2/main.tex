% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{acl}
\usepackage{acl}


% Standard package includes
\usepackage{times}
\newcommand{\Rom}[1]{\uppercase\expandafter{\romannumeral #1\relax}}
\newcommand{\rom}[1]{\lowercase\expandafter{\romannumeral #1\relax}}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{booktabs} % To thicken table lines

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\newcommand{\jiuhai}[1]{{ \color{blue}{[JC: #1]} }}
\newcommand{\ty}[1]{\textcolor{orange}{Tianyi: #1}}
\newcommand{\clc}[1]{\textcolor{red}{LC: #1}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{It Takes One to Tango but More Make Trouble?\\
The Number of Demonstrations Needed for In-Context Learning 
% In-Context Learning with Different Numbers of Demonstrations}
}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Jiuhai Chen [1] \And Lichang Chen \And Chen Zhu \And Tianyi Zhou \AND
%         [1] University of Maryland \And [2] Google \AND    \\}
% if the names do not fit well n one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
\author{Jiuhai Chen \\ University of Maryland \\  jchen169@umd.edu
        \And  Lichang Chen \\ University of Maryland \\  bobchen@umd.edu \And Chen Zhu \\  University of Maryland \\  chenzhu@umd.edu 
        \And
        Tianyi Zhou \\ University of Maryland \\tianyi@umd.edu }
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


% \author{First Author \\
%   Affiliation / Address line 1 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   \texttt{email@domain} \\}

\begin{document}
\maketitle
\begin{abstract}
Large language models (LLMs) are capable to perform complex reasoning by in-context learning (ICL) when provided with a few input-output demonstrations (demos) and more powerful when intermediate reasoning steps (``chain of thoughts (CoT)'') of the demos are given. Is it necessary to use multi-demo in ICL? In this paper, we study ICL using fewer demos for each test query on the tasks in~\cite{wei2022chain}. Surprisingly, we do not observe significant degradation when using only one randomly chosen demo. To study this phenomenon, for each test query, we categorize demos into ``correct demos'' leading to the correct answer, and ``wrong demos'' resulting in wrong answers. Our analysis reveals an inherent bias in those widely studied datasets and the redundancy of demos: most demos are correct for a majority of test queries, which explains the good performance of ICL with one random demo. Moreover, ICL (with and w/o CoT) using only one correct demo significantly outperforms multi-demo ICL adopted by most previous works, indicating the weakness of LLMs in finding correct demo(s) for input queries, which is difficult to evaluate on the biased datasets. Furthermore, we observe a counterintuitive behavior of ICL using multi-demo, i.e., its accuracy degrades(improves) when given more correct(wrong) demos. This implies that ICL can be easily misguided by interference among demos and their spurious correlations. Our analyses highlight several fundamental challenges that need to be addressed in LLMs training, ICL, and benchmark design. 
% on all seven datasets, we find that a single demo selected performs significantly better than all demos or a subset of them. 
% Moreover, randomly selecting one demo leads to a similar or even better performance than using all demos. 
% For every test query, CoT prompting. We further observe that CoT with multiple correct demos can result in wrong answers for some test queries while multiple wrong demos sometimes lead to correct answers. These observations expose the weakness of LLMs in distinguishing correct demos from wrong ones and the harmful interference among (even all correct) demos in CoT prompting. In addition, our study reveals an inherent bias widely existing in CoT benchmark datasets, i.e., a majority of test queries have much more correct demos than the others so they are easy samples that can hardly reflect the true capability of LLMs on selecting correct demos and aligning the test query to correct reasoning steps. 
\end{abstract}

\section{Introduction}

% \ty{Introduce few-shot prompting first and then the improvement by CoT. Introduce how the demos are generated for CoT. Then list some questions relevant to our study, e.g., what is the role each demo plays in CoT? Are they equally important? How does LLM exploit multiple demos to produce the answer for a given test query? etc.} 
% The recent progress in Large Language models (LLMs) \cite{brown2020language, chowdhery2022palm, thoppilan2022lamda, rae2021scaling} has demonstrated the impressive capabilities of LLMs in reasoning tasks. One of the most remarkable behaviors observed in LLMs is in-context learning \cite{brown2020language}, which provides LLMs with instruction and exemplars. However, conventional few-shot prompting performs poorly on complex reasoning tasks \cite{wei2022chain}. Recently, a typical way of applying LLMs in complex reasoning tasks including arithmetic reasoning, commonsense reasoning, and symbolic reasoning is to elaborate the intermediate reasoning step in the exemplar \cite{wei2022chain}, namely Chain-of-thoughts (CoT) prompting. 

The recent race of large Language models (LLMs) \cite{brown2020language, chowdhery2022palm, thoppilan2022lamda, rae2021scaling} shows that the capability of reasoning can be significantly improved with the scaling of model size. One of the most remarkable behaviors observed in LLMs is in-context learning (ICL) \cite{brown2020language}, which provides LLMs with human-written instruction and a few exemplars or demonstrations (demos), along with the input queries. However, conventional few-shot prompting performs poorly on complex reasoning tasks~\cite{wei2022chain}. Recently, an effective ICL strategy for complex reasoning tasks, including arithmetic reasoning, commonsense reasoning, and symbolic reasoning, is to elaborate the intermediate reasoning step in each demo \cite{wei2022chain}, namely Chain-of-thoughts (CoT) prompting. 

% \ty{This can be moved to related work or experiments} Furthermore, incorporating with output space ensembling, self-consistency \cite{wang2022self} generates several chain of thoughts and arrives at the final answers via majority vote. 

ICL relies on human engineering and expertise in designing demo questions, intermediate reasoning steps, and final answers so the LLM can generalize them to a variety of unseen queries. However, it is inefficient and impractical to design demos for different queries but a fixed set of demos might not cover all the possible queries. In addition, adding demos (especially in CoT prompting) significantly increases input tokens, which are costly and may exceed the maximum input length of LLMs. 
% choose the questions and annotate them with intermediate reasoning steps and final answers. 
% To mitigate the manual effort, there has been 
To provide better demos for efficient ICL and save human efforts, a very recent line of work studies automatic prompting~\cite{zhang2022automatic, wang2023large, arora2022ask}, which leverage LLMs to select demo questions and construct their answers and CoTs for ICL. 
They save human labor on creating demos but do not address several fundamental questions in ICL, e.g., \textit{How many demos are necessary for ICL? Can LLMs in ICL figure out which demo(s) is more useful to each test query? Does ICL leverage all the demos or mainly rely on a few of them to resolve each test query? Can LLMs in ICL combine the strengths of multiple demos to improve the answers?}

% To eliminate manual designs, they leverage the LLMs to automatically construct the demos with reasoning chains. Unlike previous works focusing on finding better Demos, our study investigates the capability of LLMs on (1) distinguishing the relevant demos from misleading demos for each test query and (2) building the correct (instead of spurious) connections between the test query and given demos.

% \ty{Try to either use the observations from these works as motivations for our study or move them to related work} 
% Another research line attempts to explore the reasons behind the success of chain-of-thoughs \cite{madaan2022text, saparov2022language}. These works undertake initial steps towards the understanding behind the success of CoT. However, the systematical mechanism behind CoT still remains ambiguous. \clc{You could consider moving some experiment results here as the motivation since our main point is not on investigating the mechanism behind the CoT.}

% Our questions are as follows:

% \begin{itemize}
%     \item Are multiple demos (e.g., 8 demos for arithmetic reasoning) necessarily the most effective and efficient for each test query?
%     \item Can we decrease the number of demos without hampering the model performance?
%     \item  What is the number of the minimal requisite demos to facilitate the LLMs to generate the expected answers?
% \end{itemize}

In this paper, we take the first step toward better understanding the effect of multiple demos 
in ICL through a series of empirical studies on the demos and benchmarks widely used in CoT prompting~\cite{wei2022chain}, which covers a diverse set of reasoning tasks. 
In particular, we investigate how the ICL (with and w/o CoT) performance changes when varying the number of demos and the impact of each demo on different test queries. We start with an extreme case of ICL using only one demo randomly chosen for each query. Surprisingly, compared to the default 8 (or 7)-demo ICL in previous work, we do not observe a significant drop in the test accuracy. But does this imply that multiple demos are unnecessary to ICL? To study this phenomenon, we take a closer look at the proportion of correct demos (i.e., the demos leading to correct answers in one demo ICL) for each test query. Statistics on all the datasets reveal a widely existing bias of easy queries, i.e., most demos are correct for a majority of queries, for which one (random) demo is all they need. 

That being said, how does ICL perform on the test queries with fewer correct demos? Unfortunately, though provided with some correct demos, ICL fails to produce correct answers for many of them. We verify this by evaluating ICL with one correct demo, which significantly outperforms the widely used multi-demo ICL. This exposes a weakness of LLMs, i.e., they are not good at identifying the correct demo(s) and ignoring the wrong ones for each query in multi-demo ICL, even when more details such as CoT are given. Our further analysis reveals another deeper reason for this. Specifically, we start from ICL with one correct (wrong) demo but adding more correct (wrong) demos results in a counterintuitive degradation (improvement) of ICL accuracy, indicating a negative impact of the interference or spurious correlation among demos on the LLMs. Therefore, multiple demos might provide more information than a single demo but the current LLMs and ICL methods cannot fully exploit them and filter out misleading interference. 


% \ty{Try to ask a question (as the motivation) before introducing each observation/insight. I think we have three observations related to (1) Single-demo CoT prompting; (2) multiple all-correct or all-wrong demos based CoT prompting; (3) lack of hard examples in the current benchmark datasets for CoT.} In this work, we follow the second pathway to develop deeper understanding of CoT mechanism from the perspective of number of demos. For instance, \citep{wei2022chain} hand-crafts 8 demos for all arithmetic reasoning tasks, which brings into the question that \emph{do all 8 demos plays equal role or several of them play a primary role in enabling the success of CoT in divers reasoning task?}


% Our observations are as follows:

% \begin{itemize}
%     \item For most datasets, randomly selecting one exemplar along with self-consistency can achieve comparable performance with employing full list of demos. 
%     \item 
%     \item
% \end{itemize}

\section{Related Work}

% \ty{Try to write related work as a rebuttal to potential questions from reviewers such as ``what is the main novelty/difference of your work compared to xxx?''. This section currently is less organized. Try to use each related work as a support or motivation for our study, for example, they did not consider xxx which is a key to demystifying CoT prompting, or they observed that the prompting results are sensitive to the demos and focus on finding better demos but did not analyze whether LLM uses the demos in the correct way, etc.}
\paragraph{In-Context Learning (ICL).}
In-context learning (ICL) provides an efficient strategy to perform downstream task adaptations on pretrained LLMs~\cite{brown2020language}. By prepending task-specific instructions and some demos to each test query, the LLM is able to accomplish highly specified tasks. Recent work in ICL focuses on automatically determining the prompts, e.g., training a dense retriever to allocate semantically similar training examples~\cite{liu2021makes} for each test query~\cite{rubin2021learning}, estimating the LLM's bias for better learning calibration parameters \cite{zhao2021calibrate}, etc.

% has been key to superior performance in LLMs~\cite{brown2020language} over a variety of downstream tasks. Prior works primarily focus on how to formulate the prompts, e.g., retrieving 
% semantically similar to a test sample to formulate its corresponding prompt \cite{liu2021makes}, estimating the LLM's bias and then fitting calibration parameters \cite{zhao2021calibrate}, training an efficient dense retriever of prompts from training examples~\cite{rubin2021learning}.

\paragraph{Chain-of-Thoughts (CoT) and its variants.} CoT has been recently introduced to elicit the reasoning abilities of LLMs \cite{wei2022chain} by augmenting each demo with a chain of rational steps. Many follow-ups works further improve the performance of CoT, e.g., self-consistency~\cite{wang2022self} draws an ensemble of outputs for majority voting to replace the greedy decoding. However, CoT still heavily relies on human expertise to annotate the reasoning chains. A handful of recent works have explored the idea of automatic prompting~\cite{zhang2022automatic, huang2022large,wang2023large}. For instance, Auto-CoT \cite{zhang2022automatic} proposes to select queries of the demos via clustering all test queries and sampling demo queries with diversity. \cite{huang2022large} fine-tunes an LLM with high-confidence rationale-augmented answers for unlabeled questions. \citet{wang2023large} views the LLM as a topic model and proposes an algorithm selecting the optimal demo from a set of annotated data. While beneficial, most automatic prompting methods focus on bypassing human engineering and building better demos from a set of questions. But they do not investigate whether demos are used in the correct way by LLMs in ICL. In contrast, we find that the original demos provided by \cite{wei2022chain} include adequate information (e.g., one correct demo per query) for the LLMs to produce correct answers. 
 


\paragraph{The role of demos in ICL.} Several works have                       explored the mechanism behind the success of CoT prompting. \citet{min2022rethinking} observes that label correctness is not the critical reason for the success of few-shot ICL/prompting. \citet{madaan2022text} also finds that the label correctness is immaterial to the task on GSM8K. Instead, \citet{madaan2022text} constructs three key components in rational and identifies which component plays a vital role in CoT. \citet{saparov2022language} concludes that LLMs are capable of making correct individual deduction steps but have difficulty systematically exploring the different options. \citet{wang2022towards} shows that CoT  reasoning is possible even with invalid demos. These works try to understand what makes CoT prompting effective. However, few works focus on varying the number of demos and inherent dataset bias in few-shot ICL or CoT prompting. 


\section{Background and Experimental Setup}

\subsection{Tasks and Datasets}
We conduct a series of experiments on a variety of reasoning benchmarks: 
\textbf{arithmetic reasoning:} GSM8K \cite{cobbe2021training}, MultiArith \cite{roy2016solving}, AddSub \cite{hosseini2014learning}, SVAMP \cite{patel2021nlp}, AQuA \cite{ling2017program} and SingleOp \cite{wei2022chain}. 
\textbf{commonsense reasoning:} CSQA \cite{talmor2018commonsenseqa}.
\textbf{symbolic reasoning:} Coin-flip \cite{wei2022chain}. 

The overall statistics are listed in table \ref{tab:data}.

\begin{table}[ht]
\centering
\scalebox{0.85}{
\begin{tabular}{llll}
% - & \multicolumn{5}{c}{\textbf{arithmetic reasoning}} & \textbf{commonsense reasoning} & \textbf{symbolic reasoning} \\\
\hline
\hline
 & TASK & \# Demo & \# Query  \\
 \hline
GSM8K & Arithmetic & 8 & 1319   \\
MultiArith   & Arithmetic & 8 & 600   \\
AddSub & Arithmetic & 8 &  395  \\
SVAMP  & Arithmetic & 8 & 1000  \\
AQuA & Arithmetic & 4 & 254 \\
SingleOp  & Arithmetic & 8 & 508  \\
CSQA  & Commonsense & 7 & 1221  \\
% Last Letter & Symbolic & 4 & 500  \\
Coin-flip  & Symbolic & 8 & 500  \\
\hline
\hline
\end{tabular}}
\caption{\label{tab:data}
Statistics of datasets. \# Demo is the number of CoT exemplars provided by \citet{wei2022chain}. }
\end{table}

\subsection{Language Model and In-Context Learning}
To efficiently conduct an extensive number of experiments, we focus on code-davinci-002 \cite{chen2021evaluating, chowdhery2022palm} from the GPT-3 model family to evaluate performance. We choose code-davinci-002 because it is a programming generation engine with superior performance, especially for reasoning tasks \cite{wang2022self, zhang2022automatic}. We explore two prompting settings for in-context learning: 

\paragraph{Few-shot prompting.} Standard few-shot prompting~\cite{brown2020language} in which demos are formatted as \emph{Question + Answer} pairs appended to each test query. 
\paragraph{CoT prompting.} We also conduct experiments on CoT prompting where each demo is augmented by a chain of thoughts \cite{wei2022chain} in the form of \emph{Question + rationale + Answer}. 


\begin{table*}[ht]
\centering
\begin{tabular}{llllllll}
% - & \multicolumn{5}{c}{\textbf{arithmetic reasoning}} & \textbf{commonsense reasoning} & \textbf{symbolic reasoning} \\\
\toprule
 & GSM8K & MultiArith & AddSub & SVAMP & AQuA & SingleEq & CSQA  \\
 \midrule
Easy Sample (\%) & 40.0 & 94.3 & 82.5 & 62.6 & 28.3 & 89.0 & 58.0 \\
Hard Sample (\%) & 28.3 & 0.0 & 6.1 & 14.3 & 55.1 & 5.3 & 13.0 \\
\bottomrule
\end{tabular}
\caption{\label{tab:easy/hard ration}
The percentage of Easy/Hard samples (ICL with CoT) in each benchmark dataset. Easy samples dominate in most datasets while hard samples only take up a small fraction.}
\end{table*}



% \begin{table*}[ht]
% \centering
% \begin{tabular}{lllllll}
% % - & \multicolumn{5}{c}{\textbf{arithmetic reasoning}} & \textbf{commonsense reasoning} & \textbf{symbolic reasoning} \\\
% \hline
%  & GSM8K & MultiArith & AddSub & SVAMP & AQuA & SingleEq  \\
%  \hline
% Standard (full demo) & 19.7 & 44.0 & 90.9 & 69.9 & 29.5 & 86.8  \\
% Standard (one demo)  & 17.9 & 41.5 & 83.8 & 68.5 & 28.3 & 83.1  \\
% \hline
% CoT (full demo) & 60.1 & 96.2 & 89.4 & 75.8 & 39.8 & 93.1  \\
% CoT (one demo)  & 53.2 & 93.7 & 83.3 & 68.2 & 37.2 & 88.8  \\
% \hline
% CoT + self-consistency (full demo) & 78.0 & 100.0 & 91.6 & 86.8 & 52.0 & 92.8  \\
% CoT + self-consistency (one demo)  & 73.1 & 100.0 & 91.1 & 84.6 & 46.4 & 94.9  \\
% \hline
% \end{tabular}
% \caption{\label{tab:arithmetic}
% Accuracy comparison on arithmetic reasoning task with full prompts and randomly selecting one prompt. }
% \end{table*}


% \begin{table*}[t]
% \centering
% \begin{tabular}{lllll}
% % - & \multicolumn{5}{c}{\textbf{arithmetic reasoning}} & \textbf{commonsense reasoning} & \textbf{symbolic reasoning} \\\
% \hline
%  & StrategyQA & CSQA & Letter & Coinflip \\
%  \hline
%  Standard (full demo) & 67.1  & 82.3 & -  & -\\
% Standard (one demo)  & - & 78.1 & - & 55.0 \\
% \hline
% CoT (full demo) & 73.4  & 79.0 & 70.4  & 99.0\\
% CoT (one demo)  & - & 70.3 & 71.4 & 91.6 \\
% \hline
% Self-consistency (full demo) & 79.8 & 81.5 & 73.4 & 99.5\\
% Self-consistency (one demo)  & - & 81.2 & 75.4 & 97.8 \\
% \hline
% \end{tabular}
% \caption{\label{tab:csqa}
% Accuracy comparison on commonsense reasoning and symbolic reasoning task with full prompts and randomly selecting one prompt.
% }
% \end{table*}

\section{One-Demo Prompting}

It is common to use multiple demos in ICL, e.g., manual-CoT~\citep{wei2022chain} relies on humans to create a few demos for different tasks as shown in Table~\ref{tab:data}. 
% (e.g., 8 prompted exemplars for arithmetic tasks). 
But do multiple demos really improve ICL performance? How many demos are needed for complex reasoning tasks? To answer these questions, we start by investigating the simplest case, i.e., one-demo ICL. Surprisingly, reducing the number of demos to one does not bring critical degradation even when the demo is randomly selected. When we can filter out wrong demos (defined in Section~\ref{sec:easyhardquery}) for each test query, one demo ICL significantly outperforms the widely used multi-demo ICL. We provide an in-depth analysis of the reasons behind these phenomena and they reveal some fundamental issues of LLMs and benchmarks.    


% Manual-CoT relies on humans to create a few demos for different tasks shown in Table \ref{tab:data} (e.g., 8 prompted exemplars for arithmetic tasks). However, 8 human-annotated demos are not necessarily the most effective for each test sample. In this section, we explore the minimal requisite demos to facilitate the LLMs to generate the expected answers. In particular, we observe that the number of demos plays a key role in model performance, and including more demos may  hamper the performance. 

\subsection{Prompting with One Random Demo}

We compare ICL with one random demo with ICL with all demos when each demo is associated with/without CoT.

\paragraph{One Random Demo} is randomly selected from a few demos crafted by \citep{wei2022chain}. We prepend this single demo to the test sample and query the language model once. 
% For all arithmetic reasoning tasks, we randomly choose one demo from 8 manually written exemplars in \citep{wei2022chain}; for CSQA, one exemplar is randomly selected from 7 hand-crafted demos provided by \citep{wei2022chain}; for Last Letter Concatenation, we randomly select one prompt from 4 manually written exemplars in \citep{wei2022chain}. \ty{Consider moving this paragraph to background} 
%  We also consider self-consistency for further boosting the performance under randomly selecting one demo.

\paragraph{All Demos} is the baseline reported by \cite{wei2022chain}, prepending all demos (e.g., 8 demos for arithmetic reasoning tasks) to the test sample and query the model once.

% \textbf{Self-Consistency} is also considered to further boost the performance of CoT prompting \cite{wang2022self}. Under self-consistency setting, we prompt the language model with randomly selecting one demo and query the model once with temperature $T=0.7$. We repeat the procedure 40 times following \citep{wang2022self} and take the majority vote for the final answer. 

Results for few-shot (without CoT) and CoT prompting on a variety of datasets are reported in Figure~\ref{fig:bar_wo_cot} and Figure~\ref{fig:bar_cot}, respectively. For both ICL methods, reducing seven or eight demos (green bar) to one random demo (blue bar) causes only slight degradation (0-7\%) on the test accuracy, while significantly reducing the input length and computational cost. These savings are attractive since most API LLMs are billed based on the number of input tokens (e.g., \$ 0.02 per 1000 tokens for GPT-3). On most evaluated tasks, \textbf{one random demo suffices to achieve the most phenomenal improvement by ICL but using more than one demo only brings marginal improvement.} 
% From the perspective of ICL, the demos are training examples but increasing them test performance. 
\textbf{It indicates an inefficient usage of demos in ICL}, despite their presumed high quality and diversity (as they are carefully created by humans).  

% This conclusion is of great practical value since it is able to significantly save cost when calling the API model, e.g., GPT3 is billed based on the number of tokens sent in the prompt (\$ 0.02 per 1000 tokens). Less number of input tokens straightly translates to reduced billing cost, run-time, and carbon footprint. We only keep one demo for each test sample, drastically shorting the length of input prompts token (e.g., for arithmetic reasoning task, eight times less than the standard CoT \cite{wei2022chain}) and leading to comparable performance. 
% Meanwhile, Self-Consistency effectively narrows the gap between randomly choosing one demo and employing full demos. In particular, for MultiArith, Addsub, SVAMP, SingleOp and Coinflip, reducing the  number of demos has little to no impact on the reasoning performance. Notably, for SingleOp and Letter, one randomly selected demo outperforms full demo under self-consistency setting. 

\begin{figure}[ht]
\includegraphics[width=8cm]{fig0b.pdf}
\centering
\caption{\label{fig:bar_wo_cot}
 ICL without CoT: Prompting with one random demo has a slightly lower accuracy than few-shot prompting (8 or 7 demos). Prompting with one correct demo significantly outperforms few-shot prompting. \looseness-1}
\end{figure}


\begin{figure}[ht]
\includegraphics[width=8cm]{fig0a.pdf}
\centering
\caption{\label{fig:bar_cot}
ICL with CoT: Prompting with one random demo has a slightly lower accuracy than CoT prompting (8 or 7 demos). Prompting with one correct demo significantly outperforms CoT prompting. 
 % The performance of One Random Demo is slightly worse than full Demos under CoT prompting setting. One Correct demo significantly outperforms all demo under.
 }
\end{figure}

% Based on the observation, we conclude that reducing the redundant exemplars leads to a comparable performance with all demos used. 
But \textit{what are the reasons behind this inefficient usage of multiple demos? Is it due to a weakness of current LLMs or ICL on exploiting demos or an inherent redundancy of the handcrafted demos for these benchmark tasks?} Given the above observations, it is plausible that different demos might provide redundant information to each test query so any randomly chosen one should do the same job. \textit{But does this hold for all test queries? Does there exist the best demo for each query? When the LLMs are API models, their weights cannot be further finetuned, is it still possible to improve their ICL performance through the demos?}

% The minimal requisite demos number for most datasets is one, even randomly selecting one demo, which is adequate to empower the LLMs to generate the expected answer. Furthermore, the observation brings to the questions: 
% \begin{itemize}
%     \item \emph{What is the potential performance if we always select the most effective demo rather than random selection? }
%      \item \emph{For each test-time sample, what is the percentage of the effective demo which individually facilitates the LLM to generate factual conclusions?}
% \end{itemize}






% \section{CoT with Known Label}
\subsection{Correct/Wrong Demos and Hard/Easy Samples in Datasets}\label{sec:easyhardquery}

For an in-depth study of these questions, we categorize all the demos into correct/wrong demos for each input query in the one-demo prompting setting, i.e., ``\textbf{Correct Demos}'' enabling the LLMs to produce a correct answer while ``\textbf{Wrong Demos}'' results in wrong answers. One example of "Wrong/Correct Demo" under CoT prompting is shown in Figure \ref{fig:correct/wrong demo}. We then study the proportions of correct demos for test queries in each benchmark dataset, which reflect the probability of randomly sampling a correct demo that can be used to explain previous observations.  
 
 \begin{figure*}[ht!]
 \centering
\includegraphics[width=0.9\textwidth]{fig5.pdf}
\caption{\label{fig:correct/wrong demo}
Wrong/Correct Demo. In one demo ICL for a test query, a wrong demo leads to an incorrect answer while a correct demo results in the correct answer.
% The illustration of Wrong/Correct Demo. For the same test query, Wrong Demo leads to incorrect answer while Correct Demo results in right answer.
}
\end{figure*}

% For each test-time sample, we partition all demos into "\textbf{Correct Demos}" and \textbf{"Wrong Demos"} in such single-demo prompting. 
A demo can be correct for a query but wrong for another query. For example, Figure \ref{fig:easy/hard demo} shows that the eight demos designed for GSM8K are all correct for an easy query but all wrong and lead to incorrect answers for another hard query. Hence, it is interesting to study the proportion of easy and hard queries in the widely used benchmark datasets. Given that we have eight demos in total, it is reasonable to define the \textbf{Easy Sample} to be the queries with $\geq 6$ demos to be correct and \textbf{Hard Sample} to be the queries with merely $\leq 1$ correct demo. Therefore, the probability of choosing a correct demo for easy samples in the one random demo prompting is $\geq 75\%$ (at least 6 correct demos from 8 demos) while the probability for hard samples is $\leq 12.5\%$ (at most 1 correct demos from 8 demos). To explain the high accuracy of prompting with one random demo, a natural problem to study is: \textit{what is the percentage of easy/hard samples in each dataset?}
% The test queries shown in Figure \ref{fig:easy/hard demo} are also Hard/Easy samples. 

% In this section, we attempt to solve the question: \emph{For each test-time sample, what is the percentage of Correct/Wrong Demo? For each dataset, what is the percentage of Easy/Hard samples? }

We report the statistics of easy and hard samples according to the number of correct demos for each sample in two commonly used ICL datasets, CSQA and GSM8K, where the former is for arithmetic reasoning and the latter is for commonsense reasoning. 
% We choose two representative datasets in arithmetic reasoning and commonsense reasoning tasks: CSQA and GSM8K. 
In particular, Figure \ref{fig:stat_gsm8k} (a) and Figure \ref{fig:stat_csqa} (a) give statistics of all test queries in terms of the number of correct demos in GSM8K and CSQA. On both datasets, we observe that easy samples are the majority while hard samples take up a very small fraction. Notably, as shown in Figure \ref{fig:stat_csqa} (a), $\sim58\%$ of test queries in CSQA dataset are easy (having $\geq 6$ correct demos out of $8$ so the success probability of one random demo prompting for these samples is $\geq 75\%$.  
% In other words, 58 \% of test queries in CSQA are more likely to generate the correct answer with selecting any single demo. 
In contrast, only 8 \% of CSQA dataset are hard samples, for which randomly selecting a demo out of the eight results in $\leq 12.5\%$ ICL accuracy. 
% indicating that any single demo fails to result in the expected answer. 
Hence, easy samples dominate CSQA, for which the $8$ or $7$ demos are highly redundant. Moreover, we observe similar statistics on other datasets such as GSM8K and almost all the datasets used in CoT prompting papers (see Table \ref{tab:easy/hard ration}). 
This explains the marginal improvement of multi-demo ICL over ICL with only one random demo (shown in Figure \ref{fig:bar_wo_cot} and Figure \ref{fig:bar_cot}): most queries in these datasets are easy samples that only require one random demo to produce the correct answers so increasing the demos does not bring significant improvement. 
% Furthermore, it reveals that the percentage of easy and hard sample plays vital roles in the performance of original GSM8K and CSQA datasets. Note that for both GSM8K and CSQA, there is a moderate amount of test data (around 50\%) where any single demo is adequate to drive the LLMs to superior performance. It can act as an explanation to Figure \ref{fig:bar_wo_cot} and Figure \ref{fig:bar_cot} that the performance of randomly selecting a single demo closely matches for using all demos.

Given the statistics of correct demos per sample in a dataset, we can estimate the accuracy of prompting with one random demo by the expected probability of a randomly chosen demo being correct for queries from each dataset. 
% Another interesting perspective, deduced from Figure \ref{fig:stat_gsm8k} (a) and Figure \ref{fig:stat_csqa} (a), we can calculate the expected accuracy by randomly selecting one demo. 
Specifically, let $N$ be the number of available demos ($N=8$ for GSM8K and $N=7$ for CSQA) and $p_n$ be the percentage of samples with $n$ correct demos, then the estimated accuracy of one random demo ICL is $\sum_{n=1}^{N} p_n \frac{n}{N}$. 
For instance, given the statistics in Figure \ref{fig:stat_gsm8k} (a) and Figure \ref{fig:stat_csqa} (a), the estimated accuracy is $52 \%$  for GSM8K and $71 \%$ for CSQA, which matches the empirical accuracy for one random demo ICL reported in Figure \ref{fig:bar_cot}. 

This reveals a widely existing dataset bias, i.e., \textbf{easy samples dominate these benchmark datasets} and the difficulty follows a long tail distribution. Though this could be claimed as an advantage of the human-designed demos, it implies redundancy and inefficient usage of these demos: one correct demo suffices to produce the correct answers for most queries (as they are easy), while multi-demo ICL multiplies the cost but only brings marginal improvement to a few queries. 
Since the maximum input length for LLMs is strictly limited, this also indicates a bottleneck of multi-demo ICL when the targeted tasks become practically more complicated and requires more diverse demos for different types of queries. 

The inefficient usage of multiple demos also exposes a weakness of LLMs when applied for ICL, i.e., they can only produce correct answers (with high probability) for easy samples when most demos are correct but can be easily confused/misguided by a few wrong demos, even the majority are still correct demos. This also explains the marginal difference between all-demo ICL and one random demo ICL on different data groups as shown in Figure \ref{fig:acc_gsm8k}-\ref{fig:acc_csqa}, in which ICL with one correct demo instead can achieve $100\%$ accuracy for data groups with $\geq 1$ correct demos. In other words, \textbf{LLMs cannot precisely distinguish correct and wrong demos for a query.} Unfortunately, this weakness of LLMs cannot be reflected by evaluations on most existing ICL benchmarks because of the aforementioned dataset bias. 



% $p_n$ is the percentage value with $n$ correct demos shown in  Figure \ref{fig:stat_gsm8k} (a)  and Figure \ref{fig:stat_csqa} (a), then we have the expected accuracy under randomly choosing one demo: $\frac{1}{N}\sum_{n=1}^{N} p_n n$. The average accuracy for GSM8K is 52 \% and for CSQA is 71 \%, closely matching the results in Figure \ref{fig:bar_cot}.





 %% In line with one research line investigates the LLMs are sensitive to the format of prompt \cite{mishra2021reframing, rubin2021learning}, the order of example in the prompt \cite{lu2021fantastically} and instructions in the prompt \cite{ouyang2022training}. We also find the performance of LLMs is  sensitive to the demo selection. 
%  To this end, we oracle the upper bounds on the model performance in the case of randomly selecting one demo. In other words, for each test query, we select the Correct Demo and query LLMs once, namely Oracle Selected Demo. In the foregoing section, we have investigated the performance of randomly selecting one demo, and Oracle Selected Demo will hint at hypothesis that one demo potentially yields the superior performance under the appropriate  demo selection. 
% \subsection{Oracle selected Demo}
% To this end, for each test query, we select the Correct Demo and query LLMs once, namely Oracle Selected Demo.

\subsection{Prompting with One Correct Demo}

Multi-demo ICL is inefficient in the usage of input tokens and can easily be misguided by a few wrong demos. On the other hand, by definition, a correct demo results in a correct answer, and most samples in those datasets have at least one correct demo, according to the statistics such as Figure \ref{fig:stat_gsm8k} (a) and Figure \ref{fig:stat_csqa} (a). 

Hence, it is intuitive to compare the widely used multi-demo ICL with ICL including only one single correct demo in the prompt\footnote{We randomly choose one demo for samples without any correct demo.}. Surprisingly, as shown in Figure \ref{fig:bar_wo_cot} and \ref{fig:bar_cot}, \textbf{one correct demo ICL significantly outperforms the multi-demo ICL}, even the latter spends $8\times$ (or $7\times$) cost of the former and includes the demo used in the former. For example, there are $83\%$ samples in GSM8K with $\geq 1$ correct demos so the one correct demo ICL enjoys an accuracy of $\sim 83\%$, which is much better than the $\sim 60\%$ accuracy of ICL using all the eight demos. 
We consistently observe similar performance gaps on all evaluated datasets, in both ICL without CoT (few-shot prompting) and ICL with CoT prompting. Therefore, these comparisons suggest that \textbf{selecting one correct demo can be both more efficient and more effective than using multiple demos in ICL.} 
% The statistics of easy/hard samples for all the benchmark datasets shows that hard samples with $\leq 1$ correct demo only constitute a very small portion of the whole dataset. Hence, it is 

% From the foregoing section, we see Wrong/Correct demo partition varies with each test-time sample, and the selection of the demo has drastic repercussions on the model performance.  In this section,  we attempt to investigate the performance if we always select one Correct Demo, namely One Correct demo.

% Figure \ref{fig:bar_wo_cot} and \ref{fig:bar_cot} illustrate the considerable improvement in the model accuracy if the One Correct Demo is selected under Few-shot prompting and CoT prompting respectively. For instance, the performance of GSM8K under One Correct demo (orange bar) in Figure \ref{fig:bar_cot} is around 83\%, a substantial improvement from fully using all demos. In other words, there are 83\% samples in GSM8K possessing at least one Correct Demo. The trend is consistent over all datasets: model witnesses a large performance boost from All demos case to One Correct demo. Based on the observation, we conclude that one Correct Demo is adequate for LLMs to generate the expected answer. 

Moreover, considering that the correct demo for each query is already included in the multiple demos, the poorer performance of multi-demo indicates misguidance from wrong demos or harmful interference between multiple demos. As shown in Figure \ref{fig:acc_gsm8k}-\ref{fig:acc_csqa}, ICL with one random demo, despite ignoring useful information in the rest demos, also removes the misguidance and interference and thus achieves similar or even better accuracy than all-demo ICL. In contrast, all-demo ICL, even with most ($>6$) demos are correct, is prone to cross-demo interference and misguidance, leading to poorer accuracy than one random demo ICL. \textit{But which is the essential reason for this gap? Can we improve the ICL performance by introducing more correct-only demos? Is it possible for LLMs to stitch the relevant/correct pieces of wrong demos to build a correct answer?}  

% notice that Correct Demo is always included in the all-demo list, but One Correct Demo consistently outperforms all demos. This observation reveals that including redundant demos may potentially hamper the model's performance. 
% Furthermore, it also exposes the weakness of LLMs in distinguishing Correct Demos from wrong ones, LLMs fail to capture the most effective demo and are easily distracted by Wrong demos.

% This conclusion is of great practical value since it is able to significantly save cost when calling the API model, e.g., GPT3 is billed based on the number of tokens sent in the prompt (\$ 0.02 per 1000 tokens). Less number of input tokens straightly translates to reduced billing cost, run-time, and carbon footprint. We only keep one demo for each test sample, drastically shorting the length of input prompts token (e.g., for arithmetic reasoning task, eight times less than the standard CoT \cite{wei2022chain}) and leading to comparable performance. 

% \subsection{TBD}
% Despite impressive results of Oracle Selected Demo, the rationale behind the success of Oracle Selected Demo remains unclear. A promising candidate explanation is to verify similarity between demo and test query since similarity-based retrieval methods widely utilized by LLMs prompting \cite{rubin2021learning, su2022selective}. 
% \begin{itemize}
%     \item \emph{Can we select the Correct Demo with similarities-based retrieving method? }
% \end{itemize}

% Unfortunately, retrieving method fails to identify the correct demo. We use all-MiniLM-L6-v2 \footnote{https://huggingface.co/tasks/sentence-similarity} to encode the sentence into numerical embedding and retrieve the most similar demo with test query based on cosine similarity. Table \ref{tab:retrieve} presents the results of similarity-based retrieval methods, showing that Retrieval-CoT closely matches the performance of Random-CoT.

% \begin{table*}[h]
% \centering
% \begin{tabular}{lllllll}
% % - & \multicolumn{5}{c}{\textbf{arithmetic reasoning}} & \textbf{commonsense reasoning} & \textbf{symbolic reasoning} \\\
% \hline
%  & GSM8K & MultiArith & AddSub & SVAMP & AQuA & SingleOp  \\
% \hline
% Random-CoT   & 53.2 & 93.7 & 83.3 & 68.2 & 37.2 & 88.8  \\
% Retrieval-CoT  & - & - & - & - & - & -  \\
% \hline
% Random-CoT + self-consistency  & 53.2 & 93.7 & 83.3 & 68.2 & 37.2 & 88.8  \\
% Retrieval-CoT + self-consistency & - & - & - & - & - & -  \\
% \hline
% \end{tabular}
% \caption{\label{tab:retrieve}
% Accuracy comparison on arithmetic reasoning task with retrieval-based and randomly selecting one prompt. }
% \end{table*}





% \begin{table*}[t]
% \centering
% \begin{tabular}{llllllll}
% % - & \multicolumn{5}{c}{\textbf{arithmetic reasoning}} & \textbf{commonsense reasoning} & \textbf{symbolic reasoning} \\\
% \hline
%  & GSM8K & MultiArith & AddSub & SVAMP & AQUA-RAT & CSQA & Letter \\
% \hline
% CoT (full demo) & 60.1 & 96.2 & 89.4 & 75.8 & 39.8 & 79.0 & 70.4 \\
% Self-consistency (full demo) & 78.0 & 100.0 & 91.6 & 86.8 & 52.0 & 81.5 & 73.4 \\
% \hline
% Oracle (one demo)  & 83.0  & 100.0 & 94.7 & 90.5 & 68.5 & 91.6 & 78.2 \\
% % Oracle (one demo) + s.c  & 86.7  & 100.0 & 95.2 & 94.0 & 66.5 & 92.4 & 84.6 \\
% \hline
% \end{tabular}
% \caption{\label{oracle}
% Accuracy comparison on different datasets of full prompts and randomly selecting one prompt.
% }
% \end{table*}


\section{Adding More Demos to Prompt: Does it improve or confuse ICL?\looseness-1}

In the previous section, we mainly focus on the performance of one demo ICL and the reasons behind it. A counterintuitive observation is that ICL with multiple demos performs even worse than ICL with only one correct demo. \textit{Why does multi-demo ICL perform worse and when can it bring additional improvement?}
To address these questions, we study the following two problems.
\begin{itemize}
    \item \textbf{Problem I}: Starting from a prompt of one correct demo, will the accuracy be further boosted if adding more correct demos?
    \item \textbf{Problem II}: Starting from a prompt of one wrong demo, what will happen if adding more wrong demos?
\end{itemize}
These two scenarios mainly focus on all-correct or all-wrong demo cases. This is because we already observed in the previous section that a mix of wrong and correct demos can misguide ICL since the evaluated LLMs are not good at distinguishing wrong and correct demos. On the other hand, it is still unclear whether the LLMs exploit the correlations between multiple demos and how they affect the ICL process. For example, \textit{when all the demos are correct (wrong), will the LLMs treat all demos to be independent and thus keep the answer correct (wrong), or will the answer be changed due to the cross-demo correlations?}

% In particular, we observe that the number of demos plays a key role in model performance, and including more demos may  hamper the performance. 

% In the foregoing section, we conclude that prepending more demos may significantly degrades the model performance. For each test query, we firstly partition all demos into "Correct Demos" enabling the model to generate expected answer and "Wrong Demos" leading to wrong answer in such single-demo prompting.



% \begin{table*}[th]
% \centering
% \begin{tabular}{p{0.7\linewidth} | l | l}
% % \hline
% % \hline
% % \multicolumn{3}{c}{\textbf{Question: Q: James is counting his Pokemon cards. He has 30 fire type, 20 grass type, and 40 water type. If he loses 8 of the water 
% %  }} \\
% % \multicolumn{3}{c}{\textbf{type and buys 14 grass type, what's the percentage chance (rounded to the nearest integer) that a randomly picked card will be a water type?}} \\
% \hline
% \hline
%  Demo & Output & Demo Type  \\
% \hline
% There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? & 33 \% & Correct Demo \\
% \hline
% If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?  & 33 \%  & Correct Demo \\
% \hline
% Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? & 33 \%  & Correct Demo \\
% \hline
% Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?  & \textcolor{red}{48 \%}  & \textcolor{red}{Wrong Demo} \\
% \hline
% Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?  & 33 \%  & Correct Demo \\
% \hline
% There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?  & 33 \%  & Correct Demo \\
% \hline
% Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?  & 33 \% & Correct Demo \\
% \hline
% Olivia has 23. She bought five bagels for 3 each. How much money does she have left?  & 33 \% & Correct Demo \\
% \hline
% \hline
% \end{tabular}
% \caption{\label{tab:example} An example of test query with 7 Correct Demo and 1 Wrong Demo. The test query is \emph{James is counting his Pokemon cards. He has 30 fire type, 20 grass type, and 40 water type. If he loses 8 of the water type and buys 14 grass type, what's the percentage chance (rounded to the nearest integer) that a randomly picked card will be a water type?} We present the partition of full demo into Correct Demo and Wrong Demo. Here we omit the details of rationale for each demo.
% }
% \end{table*}

 \begin{figure*}[ht!]
 \centering
\includegraphics[width=1.0\textwidth]{fig4.pdf}
\caption{\label{fig:easy/hard demo}
Easy/Hard Samples from GSM8K: for the hard query (Mark plants a beanstalk ...), all the 8 demos are wrong and result in wrong answers in one-demo ICL; for the easy query (Alisa biked 12 miles ...), all the 8 demos are correct and lead to the correct answer. The 8 demos for arithmetic problems are from~\cite{wei2022chain}.}
\end{figure*}


\begin{figure}[th]
\includegraphics[width=8cm]{fig1a.pdf}
\centering
\caption{Pie chart on the number of correct demos (ICL with CoT) per sample/query ($0\sim 6$ inside the pie chart) for queries in (a) the whole GSM8K dataset \label{fig:stat_gsm8k}; (b) GSM-Hard; (c): GSM-Easy.
}
\end{figure}

\begin{figure}[th]
\includegraphics[width=8cm]{fig1b.pdf}
\centering
\caption{
Pie chart on the number of correct demos (ICL with CoT) per sample/query ($0\sim 6$ inside the pie chart) for queries in (a) the whole CSQA dataset \label{fig:stat_csqa}; (b) CSQA-Hard; (c): CSQA-Easy.
% Number $0\sim 6$ around pie chart denote the number of correct demos. (a):  The overall statistics of the correct demos for each test query in CSQA\label{fig:stat_csqa}. (b) and (c): The illustration of CSQA-Hard and CSQA-Easy.
}
\end{figure}

\begin{figure}[th]
\includegraphics[width=8cm]{fig7a.pdf}
\centering
\vspace{-1.em}
\caption{
Accuracy of ICL using all eight demos and one random demo (with CoT) on fine-grained data groups different in the number of correct demos (Figure \ref{fig:stat_gsm8k} (a)) for GSM8K dataset. The size of each dot is proportional to the data percentage. All-demo ICL brings improvement only to hard samples with fewer correct demos, while one random demo performs similarly or even better than all eight demos, indicating inefficient usage of multiple demos when easy samples dominate the dataset. 
% We partition the GSM8K dataset with respect to the number of correct demos (area size denotes the proportion of each division), we show the performance of all demos ICL and one random demo ICL. We observe that all demos is required for hard samples to result in correct answer, meanwhile one random demo is adequate for easy samples to generate the correct answer. 
\label{fig:acc_gsm8k}
}
\vspace{-1.8em}
\end{figure}

\begin{figure}[th]
\includegraphics[width=8cm]{fig7b.pdf}
\centering
\vspace{-.5em}
\caption{
Accuracy of ICL using all seven demos and one random demo (with CoT) on fine-grained data groups different in the number of correct demos (Figure \ref{fig:stat_csqa} (a)) for CSQA dataset. The size of each dot is proportional to the data percentage. The observations and conclusions on CSQA are similar to those for Figure \ref{fig:acc_gsm8k}.
% We partition the CSQA dataset with respect to the number of correct demos (area size denotes the proportion of each division), we show the performance of all demos ICL and one random demo ICL. We observe that all demos is required for hard samples to result in correct answer, meanwhile one random demo is adequate for easy samples to generate the correct answer. 
\label{fig:acc_csqa}
}
\vspace{-1.em}
\end{figure}

% We raise the following two questions based on the foregoing section:
% \begin{itemize}

% \item \emph{Do existing CoT benchmark datasets really reflect the true capability of LLMs? Since datasets are composed of unbalanced easy and hard samples. }

% \item \emph{Compared with one-demo ICL, do multiple demos really offer benefits to the LLMs to arrive at the correct answer?} 
% \end{itemize}

To ensure enough number of correct (wrong) demos are added for each sample, we use CSQA-Easy and GSM8K-Easy as evaluation sets for Problem I and CSQA-Hard and GSM8K-Hard as evaluation sets for Problem II. As illustrated in Figure \ref{fig:stat_gsm8k} and Figure \ref{fig:stat_csqa}, the two easy sets of samples have $\geq 6$ correct demos while the two hard sets of samples have $\leq 1$ correct demo. Hence, we are able to increase the number of correct (wrong) demos from $1$ to $6$ in the two studied problems, producing a full spectrum of varying accuracy over different numbers of demos. 

% To proceed with our investigation, we curate two benchmark variations: Commonsense Reasoning with easy samples (CSQA-Easy), which only comprises  test examples with 6 or over 6 correct demos (Easy sample) illustrated in Figure \ref{fig:stat_csqa} (c). Correspondingly, Commonsense Reasoning with Hard examples (CSQA-Hard) is composed of 8 \% test data with zero correct demo and 5 \% test data with one correct demo shown in \ref{fig:stat_csqa} (b). Similarly, for GSM8K dataset, we construct the GSM-Easy and GSM-Hard derived from GSM8K seen in Figure \ref{fig:stat_gsm8k} (b) and Figure \ref{fig:stat_gsm8k} (c). 

% The benefit of such a tailored design is to tease apart the hard component of datasets that truly reflect the capability of LLMs. We also disentangle the easy component of the dataset which any randomly selected demo leads to the correct answer. We aim to investigate whether multiple demos are the most effective prompts for both easy and hard samples. 

\begin{figure}[!htb]
    \begin{minipage}{.5\textwidth}\centering
        \includegraphics[width=7cm, height=0.2\textheight]{fig2a.pdf}
        \caption{\label{fig:gsm8k_cot} Increasing demos in \textbf{CoT Prompting on GSM8K}: for each query in GSM-Easy(GSM-Hard), we start from a correct(wrong) demo, add more correct(wrong) demos to the prompt, but observe an accuracy degradation(improvement). 
        % Starting from randomly selecting one demo to randomly selecting 6 demo, more correct demos degrade the accuracy for GSM-Easy while more wrong demos improves the accuracy for GSM-Hard.
        }
    \end{minipage}%
    \hfill
    \begin{minipage}{0.5\textwidth}\centering
        \includegraphics[width=7cm, height=0.2\textheight]{fig2b.pdf}
        \caption{\label{fig:csqa_cot} 
        Increasing demos in \textbf{CoT Prompting on CSQA}: for each query in CSQA-Easy(CSQA-Hard), we start from a correct(wrong) demo, add more correct(wrong) demos to the prompt, but observe an accuracy degradation(improvement).
        % Increasing demos in CoT Prompting on CSQA: Starting from randomly selecting one demo to randomly selecting 6 demo, more correct demos degrade the accuracy for CSQA-Easy while more wrong demos improves the accuracy for CSQA-Hard.
        }
    \end{minipage}
\end{figure}

% \section{Spurious Correlations among Demos: Do multiple demos improve CoT?}
In Figure~\ref{fig:gsm8k_cot}-\ref{fig:csqa_standard}, we report the results for Problem I \& II when applying CoT prompting and few-shot prompting to CSQA-Easy/Hard and GSM8K-Easy/Hard. Surprisingly, on all datasets and ICL strategies, we consistently observe that \textbf{increasing correct demos in the prompt results in lower accuracy on the easy samples, while increasing the wrong demos improves the accuracy on the hard samples.} This indicates that LLMs in ICL, when given multiple demos, do take the correlations among demos into account rather than simply treating them independently. However, the correlations do not always bring improvement to ICL for multiple correct demos, for example, ICL with one correct demo achieves nearly 99\% accuracy on GSM8K-Easy but adding an additional correct demo leads to significant degradation. In this case, the interference and spurious correlations among multiple demos concatenated in the prompt are harmful to ICL and tend to misguide the LLMs toward finding the correct answer. On the other hand, ICL with multiple wrong demos is able to extract the relevant information for the test query from multiple demos and combine them by LLMs to achieve an improved answer. Though the improvement is not highly phenomenal, we consistently observe it in all the plots, indicating a non-trivial composition of clues from multiple demos commonly happening during ICL and resulting in better answers even for hard samples.  

% For GSM-Easy and GSM-Hard, we analyze the accuracy for problems with respect to the number of demos randomly selected from the full list of demos given by \cite{wei2022chain}. From Figure \ref{fig:gsm8k_cot}, we see a performance drop for GSM-Easy and a performance improvement for GSM-Hard with adding more demos. In particular, prepending any single demo achieves nearly 99\% accuracy for GSM-Easy, even 2-demo degrades the performance significantly. Surprisingly, composing multiple correct demos that individually generate the correct answer may hamper the model's performance. 
% LLMs are easily distracted by more information and fail to capture the most useful instructions. In contrast to GSM-Easy, stacking more Wrong demos which individually generate the wrong answer may improve the model performance. Our finding reveals that for hard samples, multiple demos enhance the reasoning capabilities of LLMs, it facilitates the LLMs to gather useful information from diverse demos. The trend is consistent over CSQA-Easy and CSQA-Hard from Figure \ref{fig:csqa_cot}: multiple demos degrade the performance of CSQA-Easy while improving the accuracy of CSQA-Hard.


% In contrast to CSQA-Easy and GSM-Easy, the accuracy improvement from adding more demos occurs for both CSQA-Hard and GSM-Hard shown in Figure (\ref{fig:hard_cot}). Consistently around 10 \% improvement is observed when adding more exemplars into instruction.  


% We conduct similar experiments on Few-shot prompting setting, Figure (\ref{fig:gsm8k_standard}) and Figure (\ref{fig:csqa_standard}) reflect the same conclusion: composing more demos that individually generate the correct answer may hamper the model performance for GSM-Easy and CSQA-Easy. Stacking more wrong demos enhances the reasoning capabilities of LLMs only for GSM-Hard and CSQA-Hard.

Hence, increasing the number of correct (wrong) demos does not intuitively improve (weaken) the ICL performance and the main reason lies in the extraction and exploitation of cross-demo correlations in ICL. Since multiple demos in ICL are concatenated together and then appended to the query as the whole input, a pretrained LLM might lack the capability to completely separate all demos and choose the correct one to follow during the ICL inference process, especially when the LLM's pretraining does not cover such inputs and tasks. Therefore, our study exposes a weakness of the current LLMs in modeling cross-demo correlations, which can be one of the main reasons for the marginal improvement brought by multi-demo ICL. To mitigate this problem, one may modify the pretraining recipe with additional training tasks/objectives to encourage beneficial cross-demo attention and restrain harmful interference.  
% We conclude that existing CoT benchmarks fail to fully reflect the true capability of LLMs since Easy samples take up the majority part of datasets. Easy samples are insensitive to the demo selection. We advocate that in this scenario, demo selection including auto-prompting techniques is not necessarily the most effective and efficient way to arrive at the correct answer. Since any single demo is adequate to facilitate the LLMs to generate the expected answer. Furthermore, while sensitive to the demo selection, Hard samples account for a minority part of the dataset. Based on the foregoing observation, the success of Hard samples requires the meticulous design of prompts. Thus Hard samples potentially play a critical role in the design of prompts and exploration of the capability of LLMs. Adjusting the proportion of hard/easy examples is an interesting future work. 








\begin{figure}[!htb]
    \begin{minipage}{.5\textwidth}\centering
        \includegraphics[width=7cm, height=0.2\textheight]{fig3a.pdf}
        \caption{\label{fig:gsm8k_standard}
        Increasing demos in \textbf{Few-shot Prompting on GSM8K}: for each query in GSM-Easy(GSM-Hard), we start from a correct(wrong) demo, add more correct(wrong) demos to the prompt, but observe an accuracy degradation(improvement).
        % (Few-shot prompting for GSM8K) Starting from randomly selecting one demo to randomly selecting 6 demo, more correct demos degrade the accuracy for GSM-Easy while more wrong demos improves the accuracy for GSM-Hard.
        }
    \end{minipage}%
    \hfill
    \begin{minipage}{0.5\textwidth}\centering
        \includegraphics[width=7cm, height=0.2\textheight]{fig3b.pdf}
        \caption{\label{fig:csqa_standard}
        Increasing demos in \textbf{Few-shot Prompting on CSQA}: for each query in CSQA-Easy(CSQA-Hard), we start from a correct(wrong) demo, add more correct(wrong) demos to the prompt, but observe an accuracy degradation(improvement).
        % (Few-shot prompting for CSQA) Starting from randomly selecting one demo to randomly selecting 6 demo, more correct demos degrade the accuracy for CSQA-Easy while more wrong demos improves the accuracy for CSQA-Hard
        }
    \end{minipage}
\end{figure}


\section{Discussion}
In-context learning (ICL) plays an important role in the ecosystem of LLMs. It only needs to append a few exemplars (demos) to an input query and recent LLMs are capable of directly generating customized outputs by following the demos. However, it is not clear how many demos suffice to produce high-quality answers. In this paper, for the first time, we study the performance of ICL (with or without chain-of-thoughts prompting) under different numbers of demos and provide an in-depth investigation of the observations across several widely used benchmark datasets. 

In particular, we found that randomly selecting one single demo barely hurts the performance while increasing the demos merely brings marginal improvement. We then study how many demos can lead to correct answers in the one-demo ICL for each sample and analyze its statistics over all samples in widely used benchmark datasets. The statistics reveal a widely existing dataset bias that easy samples with many correct demos dominate the datasets, which explains the high accuracy of ICL with one random demo. It also exposes a weakness of LLMs in distinguishing wrong/correct demos in ICL. Moreover, we found that only one correct demo is sufficient to significantly outperform multi-demo ICL, while saving a great amount of unnecessary cost. This indicates an inefficient exploitation of multiple demos in ICL. Furthermore, we study the contribution and interference of cross-demo correlations to ICL by investigating how the accuracy changes as we add more correct (wrong) demos to the prompt. Surprisingly, adding correct demos reduces the accuracy while adding wrong demos brings improvement, indicating a problematic interpretation and exploitation of the cross-demo correlations by LLMs in ICL. 

% In this paper, we study the effect of changing the number of demos on in-context learning (with and w/o chain-of-thoughts). We find that a single demo barely hurts the performance: ICL performance drops only marginally when the random demo is selected, single correct demo significantly improves the performance over the default multiple-demo ICL in previous work. We then investigate a series of aspects in the existing CoT benchmarks and examine which test query actually benefits the multiple demos. Results reveal that for hard sample, multiple demos truly enhances the reasoning capabilities of LLM, while for easy sample, multiple demos drop ICL performance. Furthermore, we examine the ratio of Hard/Easy sample for existing CoT benchmarks and find that Easy sample takes up the majority part of the datasets and the Hard sample only accounts for a small fraction of datasets under CoT prompting setting. Under this scenario, one demo is adequate to result in the expected answer for most test queries. Together, our findings expose the weakness of the LLM: they are not good at distinguishing the useful demo from multiple demos and easily mislead by the wrong demo. 

Our analyses highlight several fundamental challenges that need to be addressed in the future, e.g., how to design less biased benchmarks and more diverse demos that can be used to better evaluate LLMs' capability of distinguishing correct demos from the wrong ones; how to improve the efficiency and effectiveness of multi-demo usage in ICL; how to avoid the harmful interference caused by cross-demo correlations and meanwhile leverage them to improve the ICL performance on hard samples with fewer correct demos; how to select correct demos for a given query, etc.













% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology, main}
\bibliographystyle{acl_natbib}

\appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
