
\documentclass[english]{article}
%\documentclass[english, draft]{article}
\usepackage{custom_tex}


\usepackage{fullpage}
\usepackage[normalem]{ulem}

\newenvironment{col-par}{%
   \setlength{\parindent}{0pt}
   \itshape
   \color{gray}
}{}

%\renewcommand*{\thetheorem}{\Alph{theorem}}
\newtheorem{assumption}{Assumption}
\title{Supplementary Material for Proximal Causal Inference without Uniqueness Assumptions}


\date{\today}
\author{Jeffrey Zhang, Wei Li, Wang Miao, Eric Tchetgen Tchetgen}


\begin{document}
\maketitle
\appendix
In this supplementary material document, we first restate all assumptions from the main manuscript. Section \ref{appendix} collects additional lemmas and proofs of results in the main manuscript. Section \ref{appendix_class} outlines how the approach in the main manuscript could be adjusted so that it can be applied to a larger class of functionals.


\begin{assumption}
\label{cons}
(Consistency) $Y=AY(1)+(1-A)Y(0)$ almost surely.
\end{assumption}
\begin{comment}
\begin{assumption}
\label{pos1}
(Positivity)  $0 < \mathbb{P}(A = a|L) < 1$  $a = 0, 1$ almost surely.
\end{assumption}
\begin{assumption}
\label{exch}
(Exchangeability) $Y (a) \perp A|L$ for $a = 0, 1$.
\end{assumption}
\end{comment}
\begin{assumption}
\label{lat_unc}
(Latent Unconfoundedness)  $$(Z,A) \perp (Y(a),W) | U,X \text{ for } a = 0, 1.$$ 
\end{assumption}
\begin{assumption}
\label{pos2}
(Positivity) $0 < \mathbb{P}(A = a|U, X) < 1$ almost surely, $a = 0, 1$.
\end{assumption}
\begin{assumption}
\label{comp1}
(Completeness 1)
\begin{enumerate}
    \item For any $a, x$, if $\E[g(U)|Z, A = a, X = x] = 0$ almost surely, then $g(U) = 0$ almost
surely.
    \item For any $a, x$, if $\E[g(Z)|W, A = a, X = x] = 0$ almost surely, then $g(Z) = 0$ almost surely.
\end{enumerate}
\end{assumption}
\begin{assumption}
\label{comp2}
(Completeness 2) 
\begin{enumerate}
    \item 
For any a, x, if $\E[g(U)|W, A = a, X = x] = 0$ almost surely, then $g(U) = 0 $ almost
surely.
\item For any a, x, if $\E[g(W)|Z, A = a, X = x] = 0$ almost surely, then $g(W) = 0$ almost
surely.
\end{enumerate}
\end{assumption}
\begin{assumption}
\label{compact} 
%(Analogous to Assumption 3 from \cite{nonig})
The vector of covariates $X \in \RR^d$ has support $[0,1]^d$, and outcome $Y \in \mathbb{R}$ and proxies $Z,W \in \RR$ have compact support.
\end{assumption}
\begin{assumption}
\label{H_approx}
%(Analogous to Assumption 4 from \cite{nonig})
The following conditions hold:
\begin{enumerate}[(i)]
    \item $\sup_{h \in \hh} ||h||_{\infty,\alpha} < \infty$ for some $\alpha > (d+1)/2$; $\hh_0 \neq \emptyset$, $\hh_n$ and $\hh$ are closed.
    \item for every $h \in \hh$, there is a $\Pi_n h \in \hh_n$ such that $\sup_{h \in \hh}||h-\Pi_n h||_{\infty} = O(\eta_n)$ for some $\eta_n=o(1)$.
\end{enumerate}
\end{assumption}
\begin{assumption}
\label{Phi_approx}
%(Analogous to Assumption 5 from \cite{nonig})
The following conditions hold:
\begin{enumerate}[(i)]
\item The smallest and largest eigenvalues of $\E[\phi(Z,A,X)\phi(Z,A,X)^T]$ are bounded above and away from zero for all $k_n$.
\item for every $h \in \hh$, there is a $\mathbf{\pi}_n(h) \in \RR^{k_n}$ such that $$\sup_{h\in\hh} ||\E[h(W,A,X)|a,z,x]-\phi^T(z,a,x)\mathbf{\pi}_n(h)||_{\infty}=O\left(k_n^{-\frac{\alpha}{d+1}}\right).$$
\item $\xi_n^2k_n=o(n)$, where $\xi_n=\sup_{z,a,x}||\phi(z,a,x)||_2.$
\end{enumerate}
\end{assumption}
\begin{assumption}
\label{M}
%(Analogous to Assumption 6 from \cite{nonig})
The function set $\hh$ is convex; the functional $M:\hh \to \R$ is strictly convex and attains a unique minimum at $h_0$ on $\hh_0$; its sample analogue $M_n: \hh \to \R$ is continuous and $\sup_{h\in \hh}|M_n(h)-M(h)|=o_p(1)$. 
\end{assumption}
\begin{assumption}
\label{rep}
%(Analogous to Assumption 7 from \cite{nonig})
The following conditions hold:
\begin{enumerate}[(i)]
    \item there exists a function $g_0 \in \hh$ such that
    $$\la g_0, h \ra_w=\E\{h(W,a,X)\}$$ for all $h \in \overline{\hh}$.
    \item $\eta_n=o(n^{-1/3}), k_n^{-3\alpha/(d+1)}=o(n^{-1}), k_n^3=o(n), \xi_n^2k_n^2=o(n)$, and $\xi_n^2k_n^{-2\alpha/(d+1)}=o(1)$.
\end{enumerate}
\end{assumption}

\section{Lemmas and Proofs}
\label{appendix}
\begin{lemma} (Theorem B.1 in \cite{SANTOS2011129})
\label{lem:a1}
Assume
\begin{enumerate}[(i)]
    \item $C(h) \geq 0$ for any $h \in \hh$ with $\hh$ compact in some norm $\Norm{\cdot}$.
    \item $\hh_n \subset \hh$ are closed and $\sup_{\hh}\inf_{\hh_n}\Norm{h-h_n}=O(c_{1n})$. 
    \item Uniformly on $\hh_n$, $C_n(h)\leq C_1C(h)+O_p(c_{2n})$ and $C(h)\leq C_1C_n(h)+O_p(c_{2n})$ with probability approaching one.
    \item $C(h)\leq C_2 \inf_{h_0\in \hh_0}\Norm{h-h_0}^{\kappa_1}$ for some $\kappa_1>0$. 
    \\ \\ Then for $a_n=(\max\{c_{1n}^{\kappa_1},c_{2n}\}^{-1})$ and $b_n \to \infty$ with $b_n=o(a_n)$, the set $\wh{\hh}_0$ with $c_n=b_n/a_n$ satisfies $d_H(\wh{\hh}_0,\hh_0,\Norm{\cdot})=o_p(1)$. If in addition,
    \item $C(h)\geq \inf_{h_0 \in \hh_0}C_2\Norm{h-h_0}^{\kappa_2}$ for some $\kappa_2 > 0$.\\ \\ then $d_H(\wh{\hh}_0,\hh_0,\Norm{\cdot})=O_p(\max\{(b_n/a_n)^{1/\max(\kappa_2,1)},c_{2n}\})$.
\end{enumerate}
\end{lemma}
\begin{lemma}
\label{lem:a2}
(Lemma A.2 in \cite{nonig})
For $\omega:[0,1]^d\times \yy \times \cw \to \RR$, let $\overline{\E}[\omega(X,Y,W)|A=a,z,x]=\phi^T(z,a,x)(\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\sum_{i=1}^n\E\{\omega(X,A,Y,W)|A_i,Z_i,X_i \}$. If assumptions \ref{compact}-\ref{Phi_approx} hold, then
\begin{enumerate}[(i)]
    \item $\sup_{h \in \hh_n}\frac{1}{n}\sum_{i=1}^n\left[\wh{\E}\{h(W,A,X)|A_i,Z_i,X_i\}-\E\{h(W,A,X)|A_i,Z_i,X_i\}\right]^2=O_p\p{\frac{k_n}{n}+k_n^{-\frac{2\alpha}{d+1}}}.$
    \item $\sup_{h \in \hh_n} \E \sqb{\E\{Y-h(W,A,X)|A,Z,X\}-\overline{\E}\{Y-h(W,A,X)|A,Z,X\}}^2=O_p\p{k_n^{-\frac{2\alpha}{d+1}}}.$
\end{enumerate}
\end{lemma}
\begin{comment}
\begin{proof}
First, following \cite{NEWEY1997147}, under assumption \ref{Phi_approx}(i), we may assume without loss of generality that $\E\{I(A=a)\phi(Z,X)\phi(Z,X)^T\}=I$. Let $\gg_n$ be the linear span of $I(\tilde{a}=a)\phi(z,x)$. Then it follows that for any $g(\tilde{a},z,x)=I(\tilde{a}=a)\sum_{j=1}^{k_n}\beta_j\phi_j(z,x) \in \mathcal{G}_n$, we have
\begin{equation*}
    \frac{\Norm{g}_\infty}{\Norm{g}_{L^2}}=\sup_{\tilde{a},z,x}\frac{\left|I(\tilde{a}=a)\sum_{j=1}^{k_n}b_j\phi_j(z,x)\right|}{(\sum_{j=1}^{k_n}n_j^2)^{1/2}}\leq \sup_{z,x}\Norm{\phi(z,x)}
\end{equation*}
Let $A_n=\sup_{\gcal}\Norm{g}_\infty/\Norm{g}_{L^2}$. Then assumption \ref{Phi_approx}(ii) implies that $A_n^2k_n/n \to 0$. Following Huang, we have that with probability tending to one,
\begin{equation}
    \frac{1}{2}\E[g^2(A,Z,X)] \leq \frac{1}{n}\sum_{i=1}^n g^2(A_i,Z_i,X_i) \leq \E[g^2(A,Z,X)]
\end{equation}. 
\end{proof}
\end{comment}
\begin{corollary}
\label{cor:a3}
(Corollary A.1 in \cite{nonig})
If assumptions \ref{compact}-\ref{Phi_approx} hold, the uniformly in $\hh_n$, with probability tending to one, we have 
\begin{enumerate}[(i)]
    \item $C_n(h) \leq 8C(h)+O_p(\tau_n),$
    \item $C(h) \leq 8C_n(h)+O_p(\tau_n),$
\end{enumerate}
for $\tau_n=\frac{k_n}{n}+k_n^{-\frac{2\alpha}{d+1}}$.
\end{corollary}

\begin{proof}[Proof of Proposition 2.1]
The proof follows from verifying conditions in Lemma \ref{lem:a1}. Conditions (i) and (ii) hold with $\Norm{\cdot}=\Norm{\cdot}_\infty$ and $c_{1n}=\eta_n$ by assumption \ref{H_approx}. Condition (iii) holds with $c_{2n}=-\frac{k_n}{n}+k_n^{\frac{-2\alpha}{d+1}}$ and $C_1=8$ by \ref{cor:a3}. Also, note that
\begin{equation*}
\begin{aligned}
    C(h)&=\E[\E[Y-h(W,A,X)|Z,A,X]^2] \\ &= \E[\E[h_0(W,A,X)-h(W,A,X)|Z,A,X]^2] \\ &\leq \inf_{h \in \hh_0}\Norm{h_0-h}_\infty^2
\end{aligned}
\end{equation*}
So condition (iv) holds with $C_2=1$ and $\kappa_1=2$. Then by \ref{lem:a1}, the first part of the proposition holds. For the second, we can verify the conditions (i)-(iii) for the norm $\Norm{\cdot}_w$. Also, $C(h)=\Norm{h-h_0}_w^2$ so conditions (iv) and (v) from \ref{lem:a1} hold with  $C1 = C2 = 1$ and $\kappa_1 = \kappa_2 = 2$. Then by Lemmma \ref{lem:a1}, 
\begin{equation*}
    d_H(\wh{\hh}_0,\hh_0)=O_p(\max\{c_n^{1/2},\eta_n\}).
\end{equation*}
Since $c_n^{1/2}/\eta_n \to \infty$, we get 
\begin{equation*}
    d_H(\wh{\hh}_0,\hh_0)=O_p(c_n^{1/2}).
\end{equation*}
\end{proof}
\begin{lemma}
\label{lem:a4}
(Theorem 3.2 in \cite{SANTOS2011129})
Suppose that the following conditions hold: 
\begin{enumerate}[(i)]
    \item  $\hh_0 \subset \hh$ is closed with
$\hh$ compact and $M : \hh \to \RR$ has a unique minimum on $\hh_0$ at $h_0$.
\item $\wh{\hh}_0 \subset \hh$ satisfies $d_H(\wh{\hh}_0, \hh_0, \Norm{\cdot}) = o_p(1)$.
\item $M_n : \hh \to \RR$ and $M : \hh  \to \RR$ are continuous.
\item 
$\sup_{h \in \hh} |M_n(h) - M(h)| = o_p(1)$.
\end{enumerate}
If $\wh{h}_0 \in \argmin_{h\in\wh{\hh}_0}  M_n(h)$, then $\Norm{\wh{h}_0-h_0}= o_p(1).$
\end{lemma}
\begin{proof}[Proof of Proposition 2.2]
We show the first claim of the proposition by verifying conditions of Lemma \ref{lem:a4}. First, by Assumption \ref{H_approx}, both $\hh_0$ and $\hh$ are compact under $\Norm{\cdot}_\infty$, and hence condition (i) of Lemma \ref{lem:a4} holds. Second, the convexity of $\hh_0$ and strict convexity of
$M$ implies that $M$ has a unique minimum on $\hh_0$. With additional conditions in the proposition,
Lemma \ref{lem:a4} implies that
$\Norm{\wh{h}_0-h_0}_\infty = o_p(1)$.
For the second claim of the proposition, we note that
\begin{equation*}
    \Norm{\wh{h}_0-h_0}_w \leq d_H(\wh{\hh}_0,\hh_0),\Norm{\cdot}_w).
\end{equation*}
This, combined with Proposition 2.1, shows the second claim.
\end{proof}
\begin{lemma}
\label{lem:a5} (Lemma A.4 in \cite{nonig})
Suppose assumptions \ref{compact} and \ref{H_approx}(i) hold. Let $\A_n$ be the $\sigma$-filed generated by $\{A_i,X_i,Z_i\}$ and let $\{W_{in}\}_{i=1}^n$ be a triangle array of random variables that is measurable with respect to $\A_n$. If $n^{-1}\sum_{i=1}^n W_{in}^2=O_p(1)$ and $\Norm{\wh{h}_0-h_0}_\infty=o_p(1)$, then it follows that
\begin{equation*}
    \frac{1}{\sqrt{n}}\sum_{i=1}^n W_{in}\sqb{\wh{h}_0(W_i,A_i,X_i)-h_0(W_i,A_i,X_i)-\E \{\wh{h}_0(W,A,X)-h_0(W,A,X)|A_i,Z_i,X_i\}}=o_p(1).
\end{equation*}
\end{lemma}
\begin{lemma}
\label{lem:a6} (Lemma A.5 in \cite{nonig})
Suppose assumptions \ref{compact} and \ref{H_approx}(i) hold. Let $\A_n$ be the $\sigma$-filed generated by $\{A_i,X_i,Z_i\}$ and let $\{W_{in}\}_{i=1}^n$ and $\{\widetilde{W}_{in}\}_{i=1}^n$ be triangle arrays of random variables that are measurable with respect to $\A_n$ such that $n^{-1/2}\sum_i (W_{in}-\widetilde{W}_{in})^2=O_p(1)$. If $\wh{h}_0\in \hh_n$ satisfies $\Norm{\wh{h}_0-h_0}_\infty=o_p(1)$ and $\Norm{\wh{h}_0-h_0}_w=o_p(n^{-1/4})$, then it follows that 
\begin{equation*}
    \frac{1}{\sqrt{n}}\sum_{i=1}^nW_{in}\{Y_i-\wh{h}_0(W_iA_i,X_i)\}= \frac{1}{\sqrt{n}}\sum_{i=1}^n\widetilde{W}_{in}\{Y_i-\wh{h}_0(W_i,A_i,X_i)\}+o_p(1).
\end{equation*}
\end{lemma}
\begin{lemma}
\label{lem:a7} (Lemma A.6 in \cite{nonig})
Suppose assumptions \ref{compact}, \ref{H_approx}(i), \ref{Phi_approx}, \ref{rep}(ii) hold. Let the function class $\gcal$ satisfy: for all $g \in \gcal$, $\Norm{g}_\infty \leq K$ for some $K>0$, $\E[g(W,A,X)|Z,A,X]=0$ and $\int_0^\infty \sqrt{\log N(\epsilon, \gcal,\Norm{\cdot}_\infty} d\epsilon < \infty$. If $\overline{\E}[g(W,A,X)|a,z,x]=\phi^T(z,a,x)(\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\sum_{i=1}^n\E\{g(W,A,X)|A_i,Z_i,X_i \}$ then it holds that 
\begin{equation*}
    \sup_{\gcal, \hh_{n}}\left|\frac{1}{\sqrt{n}} \sum_{i=1}^{n} \left[\E\left\{h(W,A,X) \mid A_i, Z_{i}, X_{i}\right\}-\bar{\E}\left\{h(W,A,X) \mid A_i, X_{i}, Z_{i}\right\}\right] g\left(W_{i},A_i, X_{i}\right)\right|=o_{p}(1).
\end{equation*}
\end{lemma}

\begin{lemma}
\label{lem:a8} (Lemma A.7 in \cite{nonig})
Suppose assumptions \ref{compact}-\ref{Phi_approx} and \ref{rep} hold. Let $u =\pm g_0$ and $u_n = \Pi_n u $. If $\wh{h}_0 \in \hh_n$ satisfies $\Norm{\wh{h}_0-h_0}_w=o_p(n^{-1/4})$ and $\Norm{\wh{h}_0-h_0}_\infty=o_p(1)$ then
\begin{enumerate}[(i)]
\item $$\frac{1}{\sqrt{n}}\sum_{i=1}^n I(A_i=a)\E\{u(W,A,X)|Z_i,A_i,X_i\} \sqb{\wh{e}(X_i,A_i,Z_i,\wh{h}_0)-(Y_i-\wh{h}_0(W_i,A_i,X_i))}=o_p(1);$$
    \item $$\sqrt{n}\la u, h_0-\wh{h}_0 \ra_w=\frac{1}{\sqrt{n}}\sum_{i=1}^n I(A_i=a) \E\{u(W,A,X)|Z_i,A_i,X_i\} \sqb{\wh{e}(X_i,A_i,Z_i,\wh{h}_0)-\wh{e}(X_i,A_i,Z_i,h_0)}+o_p(1).$$
\end{enumerate}
\end{lemma}
\begin{theorem}
\label{thm:2.3}
%(Analogous to Theorem 3.4 from \cite{nonig})
Suppose that assumptions \ref{compact}-\ref{rep} hold. Then we have that 
\begin{equation*}
\begin{aligned}
    \sqrt{n}(\wh{\mu}_a-\mu_a)&=\frac{1}{\sqrt{n}}\sum_{i=1}^n\sqb{h_0(W_i,a,X_i)-\mu_a+I(A_i=a)\E\{g_0(W,A,X)|A_i,Z_i,X_i\}\times(Y_i-h_0(W_i,A_i,X_i))}\\&-\sqrt{n}r_n(\wh{h}_0)+o_p(1),
    \end{aligned}
\end{equation*}
where 
\begin{equation}
r_n(\wh{h}_0)=\frac{1}{n}\sum_{i=1}^n  I(A_i=a)\wh{\E}\{\Pi_n g_0(W,A,X)|A_i,Z_i,X_i\}\wh{e}(Z_i,A_i,X_i,\wh{h}_0).
\end{equation}
\end{theorem}  
\begin{proof}[Proof of Theorem A.9]
By assumption \ref{H_approx}(i) and Theorem 2.7.1 from \cite{van1996weak}, we have that $\hh$ is a Donsker class. In addition, since $\Norm{\wh{h}_0-h_0}_\infty=o_p(1)$, we have that $\sup_{w,a,x}\mid \wh{h}_0(w,a,x)-h_0(w,a,x)\mid = o_p(1)$. As a result, we have 
\begin{equation}
    \frac{1}{\sqrt{n}}\sum_{i=1}^n \sqb{\{\wh{h}_0(W_i,a,X_i)-h_0(W_i,a,X_i)\}-\E\{\wh{h}_0(W,a,X)-h_0(W,a,X)\}}=o_p(1).
\end{equation}
Thus, 
\begin{equation}
    \sqrt{n}(\wh{\mu}_a-\mu_a)=\frac{1}{\sqrt{n}}\sum_{i=1}^n \{h_0(W_i,a,X_i)-\mu_a\}+\sqrt{n} \E[\wh{h}_0(W,a,X)-h_0(W,a,X)]+o_p(1).
\end{equation}
By definition of $g_0$, 
\begin{equation*}
    \sqrt{n} \E[\wh{h}_0(W,a,X)-h_0(W,a,X)]=\sqrt{n}\la g_0, \wh{h}_0-h_0\ra_w.
\end{equation*}
Applying Lemma \ref{lem:a8}, we derive that 
\begin{equation}
\begin{aligned}
    \sqrt{n} &\E[\wh{h}_0(W,a,X)-h_0(W,a,X)] \\ &= \frac{1}{\sqrt{n}}\sum_{i=1}^n I(A_i=a)\E\{g_0(W,A,X)|Z_i,A_i,X_i\} \sqb{\wh{e}(X_i,A_i,Z_i,h_0)-\wh{e}(X_i,A_i,Z_i,\wh{h}_0)}+o_p(1) \\ &= \frac{1}{\sqrt{n}}\sum_{i=1}^n I(A_i=a)\E\{g_0(W,A,X)|Z_i,A_i,X_i\} \sqb{Y_i-h_0(W_i,A_i,X_i)}\\ &-\frac{1}{\sqrt{n}}\sum_{i=1}^n I(A_i=a)\E\{g_0(W,A,X)|Z_i,A_i,X_i\} \sqb{\wh{e}(X_i,A_i,Z_i,\wh{h}_0)}+o_p(1)
    \end{aligned}
\end{equation}
By Cauchy-Schwarz,
\begin{equation}
\begin{aligned}
    &\frac{1}{\sqrt{n}}\sum_{i=1}^n \E\{g_0(W,A,X)-\Pi_ng_0(W,A,X)|Z_i,A_i,X_i\} \wh{e}(X_i,A_i,Z_i,\wh{h}_0) \\ &\leq \sqb{\frac{1}{\sqrt{n}}\sum_{i=1}^n \E\{g_0(W,A,X)-\Pi_ng_0(W,A,X)|Z_i,A_i,X_i\}^2}^{1/2} \\ &\times \cb{\frac{1}{\sqrt{n}}\sum_{i=1}^n \wh{e}(X_i,A_i,Z_i,\wh{h}_0)^2}^{1/2} \\ &\lesssim n^{1/4}\Norm{g_0-\Pi_n g_0}_\infty \times n^{1/4} \cb{C_n(\wh{h}_0)}^{1/2}
    \end{aligned}
\end{equation}
Then by Corollary A.3, Assumptions \ref{H_approx}(ii) and \ref{rep}(ii), 
\begin{equation}
    \frac{1}{\sqrt{n}}\sum_{i=1}^n  I(A_i=a)\E\{g_0(W,A,X)-\Pi_ng_0(W,A,X)|Z_i,A_i,X_i\} \wh{e}(X_i,A_i,Z_i,\wh{h}_0)=o_p(1).
\end{equation}
In a similar vein, using Cauchy Schwarz, Lemma \ref{lem:a2}, and Corollary \ref{cor:a3}, and assumption \ref{rep}(ii) we get
\begin{equation}
     \frac{1}{\sqrt{n}}\sum_{i=1}^n  I(A_i=a)\sqb{\E\{\Pi_ng_0(W,A,X)|Z_i,A_i,X_i\}-\wh{\E}\{\Pi_ng_0(W,A,X)|Z_i,A_i,X_i\}} \wh{e}(X_i,A_i,Z_i,\wh{h}_0)=o_p(1).
\end{equation}
Then by combining the above equations, we get the result
\begin{equation*}
\begin{aligned}
    \sqrt{n}(\wh{\mu}_a-\mu_a)&=\frac{1}{\sqrt{n}}\sum_{i=1}^n\sqb{h_0(W_i,a,X_i)-\mu_a+I(A_i=a)\E\{g_0(W,A,X)|A_i,Z_i,X_i\}\times(Y_i-h_0(W_i,A_i,X_i))}\\&-\sqrt{n}r_n(\wh{h}_0)+o_p(1),
    \end{aligned}
\end{equation*}
where 
\begin{equation*}
r_n(\wh{h}_0)=\frac{1}{n}\sum_{i=1}^n  I(A_i=a)\wh{\E}\{\Pi_n g_0(W,A,X)|A_i,Z_i,X_i\}\wh{e}(Z_i,A_i,X_i,\wh{h}_0).
\end{equation*}
\end{proof}


\begin{proof}[Proof of Lemma 3.3]
First, by assumption \ref{H_approx}(ii), we have that 
\begin{equation*}
    \Norm{\wh{g}-\Pi_ng_0}_w^2 \leq 2 \Norm{\wh{g}-g_0}_w^2+O(\eta_n^2).
\end{equation*}
Observe that $R(\wh{g})-R(\Pi_n g_0)=\Norm{\wh{g}-g_0}_w^2-\Norm{\Pi_n g_0-g_0}_w^2$. Thus we have
\begin{equation*}
    \begin{aligned}
\left\|\widehat{g}-\Pi_{n} g_{0}\right\|_{w}^{2} & \leq 2\left\{R(\widehat{g})-R\left(\Pi_{n} g_{0}\right)\right\}+O\left(\eta_{n}^{2}\right) \\
& \leq 2\left\{R(\widehat{g})-R_{n}(\widehat{g})+R_{n}(\widehat{g})-R_{n}\left(\Pi_{n} g_{0}\right)+R_{n}\left(\Pi_{n} g_{0}\right)-R\left(\Pi_{n} g_{0}\right)\right\}+O\left(\eta_{n}^{2}\right) \\
& \leq 4 \sup _{h \in \hh_{n}}\left|R(h)-R_{n}(h)\right|+O\left(\eta_{n}^{2}\right)
\end{aligned}
\end{equation*}
Next, we have 
\begin{equation}
    \begin{aligned}
    &\sup_{h \in \hh_n} \mid R(h)-R_n(h) \mid
    \\ &\lesssim \sup_{h \in \hh_n} \left| \E\sqb{\E\{h(W,A,X)|Z,A,X\}^2}-\frac{1}{n}\sum_{i=1}^n\wh{\E}\{h(W,A,X)|Z_i,A_i,X_i\}^2 \right| \\ &+ \sup_{h \in \hh_n}\left| \E[h(W,a,X)]-\frac{1}{n}h(W_i,a,X_i)\right| \\ &= \sup_{h \in \hh_n} \left| \E\sqb{\E\{h(W,A,X)|Z,A,X\}^2}-\frac{1}{n}\sum_{i=1}^n\wh{\E}\{h(W,A,X)|Z_i,A_i,X_i\}^2 \right| \\ &+ O_p(n^{-1/2})
    \end{aligned}
\end{equation}
since $\hh$ is a Donsker class as argued in the proof of Theorem 2.3. Next, define
\begin{equation*}
    \mathcal{F}=\{f(a,z,x)=I(\E[h(W,A,X)|a,z,x]^2:h \in \hh\}.
\end{equation*}
Note that we have for any $h_1,h_2 \in \hh$
\begin{equation*}
    \begin{aligned}
&\left|\left[E\left\{h_{1}(W, A,X) \mid a, x, z\right\}\right]^{2}-(\left[E\left\{h_{2}(W, A, X) \mid a, x, z\right\}\right]^{2}\right| \\
\leq &\left|E\left\{h_{1}(W,A, X)-h_{2}(W, A,X) \mid a, x, z\right\}\right| \times\left|E\left\{h_{1}(W, A,X)+h_{2}(W,A, X) \mid a, x, z\right\}\right| \\
\lesssim &\left\|h_{1}-h_{2}\right\|_{\infty}
\end{aligned}
\end{equation*}
Then by Theorems 2.7.11, 2.7.1, and 2.5.6 from \cite{van1996weak}, $\mathcal{F}$ is a Donsker class. So
\begin{equation}
    \sup_{h \in \hh_n} \left| \frac{1}{n}\sum_{i=1}^n\E\sqb{h(W,A,X)|Z_i,A_i,X_i}^2-\E\sqb{\E\sqb{h(W,A,X)|Z,A,X}^2}\right|=O_p(n^{-1/2}).
\end{equation}
We next bound the quantity:
\begin{equation*}
    \begin{aligned}
    &\sup_{h \in \hh_n} \left| \frac{1}{n}\sum_{i=1}^n \sqb{\E[h(W,A,X)|Z_i,A_i,X_i]}^2-\frac{1}{n}\sum_{i=1}^n \sqb{\wh{\E}[h(W,A,X)|Z_i,A_i,X_i]}^2 \right| \\ &\equiv \sup_{h \in \hh_n} \left| \frac{1}{n}(B_{i1}^2-B_{i2}^2)\right|
    \end{aligned}
\end{equation*}
We can bound 
\begin{equation*}
\begin{aligned}
\sup_{h \in \hh_n} \left| \frac{1}{n}\sum_{i=1}^n(B_{i1}^2-B_{i2}^2)\right| &\lesssim \sup_{h \in \hh_n}  \frac{1}{n}\sum_{i=1}^n(B_{i1}-B_{i2})^2+\sup_{h \in \hh_n} \left| \frac{1}{n}\sum_{i=1}^n B_{i1}(B_{i2}-B_{i1})\right| \\ &\lesssim \sup_{h \in \hh_n}  \frac{1}{n}\sum_{i=1}^n(B_{i1}-B_{i2})^2+\sup_{h \in \hh_n} \p{\frac{1}{n}\sum_{i=1}^nB_{i1}^2}^{1/2} \p{\frac{1}{n}\sum_{i=1}^n (B_{i2}-B_{i1})^2}^{1/2}
\end{aligned}
\end{equation*}
Then by assumption \ref{H_approx}(i) and Lemma \ref{lem:a2}, we have
\begin{equation*}
\begin{aligned}
    &\sup_{h \in \hh_n}\left|\frac{1}{n}\sum_{i=1}^n \sqb{\E[h(W,A,X)|A_i,Z_i,X_i]}^2-\frac{1}{n}\sum_{i=1}^n \sqb{\wh{\E}[h(W,A,X)|A_i,Z_i,X_i]}^2\right| \\ &=O_p\cb{(k_n/n)^{1/2}+k_n^{-\alpha/(d+1)}}
\end{aligned}
\end{equation*}
This equation with equations 29 and 30 imply that 
\begin{equation*}
    \sup_{h \in \hh_n} \left|R(h)-R_n(h) \right|=O_p\cb{(k_n/n)^{1/2}+k_n^{-\alpha/(d+1)}},
\end{equation*}
and so 
\begin{equation*}
    \sup_{h \in \hh_n} \Norm{\wh{g}-g_0}_w^2=O_p\cb{(k_n/n)^{1/2}+k_n^{-\alpha/(d+1)}}.
\end{equation*}
Finally, by Cauchy-Schwartz, we get 
\begin{equation*}
\begin{aligned}
    &\sup_{h \in \hh_n}  \sqrt{n} \mid \wh{r}_n(\wh{h}_0)-r_n(\wh{h}_0)\mid \\ & \leq \sqb{\frac{1}{n}\sum_{i=1}^n \{\wh{\E}(\wh{g}(W,A,X)-\Pi_n g_0(W,A,X)|A_i,Z_i,X_i)\}^2}^{1/2}\times\sup_{h \in \hh_n}\{C_n(\wh{h}_0\}^{1/2} \\ &\lesssim \sqb{\cb{\frac{1}{n}\sum_{i=1}^n\{\E(\wh{g}(W,A,X)-\Pi_n g_0(W,A,X)|A_i,Z_i,X_i)\}^2}^{1/2}+O_p\cb{(k_n/n)^{1/2}+k_n^{-\alpha/(d+1)}}}\\ & \times c_n^{1/2} \\ &\lesssim \sqb{\sup_{h \in \hh_n}\Norm{\wh{g}-\Pi_n g_0}_w+O_p\cb{(k_n/n)^{1/2}+k_n^{-\alpha/(d+1)}}}\times c_n^{1/2} \\ &= O_{p}\left[c_{n}^{1 / 2}\left\{\left(\frac{k_{n}}{n}\right)^{1 / 4}+k_{n}^{-\frac{\alpha}{2(d+1)}}\right\}\right]
\end{aligned}
\end{equation*}
where the second line follows from Lemma \ref{lem:a2}.
\end{proof}

\begin{proof}[Proof of Theorem 3.4]
Under assumption \ref{rep}, we have $\left(\frac{k_n}{n} \right)^{1/4}=o(n^{-1/6})$ and $k_n^{-\frac{\alpha}{2(d+1)}}=o(n^{-1/6})$. In addition, since $n^{2/3}b_n=o(a_n)$, by Lemma \ref{lem:a6}
\begin{equation*}
    \sup_{\wh{h}_0\in \wh{\hh}_0} \sqrt{n} \mid \wh{r}_n(\wh{h}_0)-r_n(\wh{h}_0)\mid =o_p(1).
\end{equation*}
This combined with Theorem A.9 implies the result.
\end{proof}



\begin{assumption}
\label{regularityh}
\
\begin{enumerate}[(i)]
    \item Denote $\Psi_o(z,a,x)=\E[(Y-h^*(W,A,X))^2|Z=z,A=a,X=x]$. Suppose $0 < \inf_{z,a,x} \Psi_o(z,a,x) < \sup_{z,a,x} \Psi_o(z,a,x) < \infty$.
    \item $T_o'$ is compact.
\end{enumerate}
where $h^*(W,A,X)=P_{\mathcal{N}(T_o)^{\perp}}h(W,A,X)$.
\end{assumption}
\begin{assumption}
\label{regularityq}
\
\begin{enumerate}[(i)]
    \item Denote $\Psi_{tr}(w,a,x)=\E[(q^*(Z,A,X))^2|W=w,A=a,X=x]$. Suppose $0 < \inf_{w,a,x} \Psi_{tr}(w,a,x) < \sup_{w,a,x} \Psi_{tr}(w,a,x) < \infty$.
    \item $T_{tr}'$ is compact.
\end{enumerate}
where  $q^*(Z,A,X)=P_{\mathcal{N}(T_{tr})^{\perp}}q(Z,A,X)$.
\end{assumption}
\begin{proof}[Proof of Lemma 2.4]
Let $t$ index a regular parametric submodel for the data which includes the true data-generating mechanism at $t=0$. We have that 
\begin{equation*}
    \E_t[h_t^*(W,A,X)|Z,A,X]=\E_t[Y|Z,A,X],
\end{equation*} for all t. \cite{semi_proxy} demonstrated that the tangent space assuming Equation 1 is as follows:
\begin{align*}
    \Lambda_1+\Lambda_2 &\equiv \{S(Z,A,X) \in L_2(Z,A,X): \E[S(Z,A,X)]=0\} \\ &+ \{S(Y,W|Z,A,X) \in L_2(Z,A,X)^{\perp}: \E[\epsilon S(Y,W|Z,A,X)|Z,A,X]\in \text{cl}(R(T_o)\},
\end{align*}
where $\epsilon=Y-h^*(W,A,X)$. They also showed that 
\begin{equation*}
    \E[\epsilon S(Y,W|Z,A,X)|Z,A,X]=\E[\partial h_t(W,A,X)/\partial t|_{t=0}|Z,A,X],
\end{equation*}
or in the notation of \cite{SEVERINI2012491}, 
\begin{equation*}
    \E[\epsilon S(Y,W|Z,A,X)|Z,A,X]=T_o\partial h_t(W,A,X)/\partial t|_{t=0},
\end{equation*}
which implies that 
\begin{equation*}
    T_o^+\E[\epsilon S(Y,W|Z,A,X)|Z,A,X]=\partial h^*_t(W,A,X)/\partial t|_{t=0},
\end{equation*}
where $ T_o^+$ is the Moore-Penrose pseudo-inverse of $T_o$. Consider the class of functionals $\beta_o=\E[\phi_o(W,A,X)h(W,A,X)]$. Following \cite{SEVERINI2012491}, it is possible to show that 
\begin{equation*}
    \partial \beta_o / \partial t |_{t=0}= \E[\phi_o \partial h_t(W,A,X)/\partial t|_{t=0} ]+\E[\phi_o h^*(W,A,X)S(Y,Z,A,W,X)].
\end{equation*}
Next, we follow the proof of Lemma 4.1 in \cite{SEVERINI2012491}.  Let $(\lambda_j,a_j,b_j)$ denote the singular system for $T_o$ and $T_o'$ where $(a_j)$ and $(b_j)$ are orthonormal bases for $\mathcal{N}(T_o)^{\perp}$ and $\text{cl}(\mathcal{R}(T_o))$, and $\lambda_j$ are singular values. We then have $\phi_o=\sum_{j=1}^\infty F_{a_j}(\phi_o)a_j$ where $F_{a_j}(\phi_o):=\la \phi_o,a_j\ra$. Then
\begin{equation*}
    \E[\phi_o T_o^+\E[\epsilon S(Y,W|Z,A,X)|Z,A,X] = \sum_{j=1}^\infty \lambda_j^{-1} F_{a_j}(\phi_o)F_{b_j}(\E[\epsilon S(Y,W|Z,A,X)|Z,A,X]) =: K_{\phi_o}(S).
\end{equation*}
Following arguments in \cite{SEVERINI2012491}, $K_{\phi_o}(S)$ is well-defined on $\Lambda_2$. Note that any $\phi_o$ that makes $K_{\phi_o}(S)$ unbounded will make $\partial \beta_o / \partial t |_{t=0}$ unbounded, and so the corresponding functional $\E[\phi_o h^*(W,A,X)]$ will not be $\sqrt{n}$-estimable. Let $S \in \Lambda_2$. Then by Cauchy-Schwarz and Bessel's inequality, 
\begin{equation*}
\begin{aligned}
|K_{\phi_o}(S)|^2 &\leq \sum_{j=1}^\infty \lambda_j^{-2}F_{a_j}^2(\phi_o)\Norm{\E[\epsilon S(Y,W|Z,A,X)|Z,A,X]}_{L_2(Y,W,Z,A,X)}^2 \\ &\lesssim \sum_{j=1}^\infty \lambda_j^{-2}F_{a_j}^2(\phi_o)\Norm{ S(Y,W|Z,A,X)}_{L_2(Y,W,Z,A,X)}^2
\end{aligned}
\end{equation*}
Since $\mathcal{R}(T_o')= \{S \in L_2(W,A,X):\sum_{j=1}^\infty \lambda_j^{-2}\la S, a_j \ra^2 < \infty\}$ by SVD of $T_o'$, $K_{\phi_o}(S)$ is bounded whenever $\phi_o \in \mathcal{R}(T_o')$. Next let, $\phi_o^0 \in \text{cl}(\mathcal{R}(T_o')) - \mathcal{R}(T_o')$. Then $\sum_{j=1}^\infty \lambda_j^{-2}F_{a_j}^2(\phi_o^0) = \infty$. By assumption \ref{regularityh}, for any $r$,
\begin{equation*}
    d_r:=(Y-h^*(W,A,X)) \Psi_o \sum_{i=1}^r \lambda_i^{-1}F_{a_i}(\psi_0)b_i \in L_2(Y,W,Z,A,X).
\end{equation*}
Moreover, $\E[Y-h^*(W,A,X)) \Psi_o \sum_{i=1}^r \lambda_i^{-1}F_{a_i}(\psi_0)b_i|Z,A,X]=0 $ and $\E[(Y-h^*(W,A,X))^2 \Psi_o \sum_{i=1}^r \lambda_i^{-1}F_{a_i}(\psi_0)b_i|Z,A,X] \in \text{cl}(\mathcal{R}(T_o)) $. Thus, $d_r \in \Lambda_2$. Also, we see that $K_{\phi_o}(d_r)=\Norm{d_r}_{L_2(Y,W,Z,A,X)}^2$. Then letting $f_r=d_r/\Norm{d_r}$, we see that $f_r$ is a sequence of unit vectors in $\Lambda_2$ such that \begin{equation*}
    \lim_{r\to \infty} K_{\phi_o^0}(f_r)= \left(\sum_{j=1}^\infty \lambda_j^{-2}F_{a_j}^2(\phi_o^0)\right)^{1/2} = \infty,
\end{equation*}
and so $K_{\phi_o^0}(S)$ is unbounded.
\end{proof}

\begin{proof}[Proof of Lemma 2.6]
Let $t$ index a regular parametric submodel for the data which includes the true data-generating mechanism at $t=0$. We have that 
\begin{equation*}
    \E_t[q_t^*(W,A,X)|Z,A,X]=1/P_t(A=a|W,X).
\end{equation*} for all t. First, note that following \cite{kallus}, the tangent space assuming Equation 3 is as follows: 
\begin{align*}
    \Lambda_1+\Lambda_2 &\equiv \{S(W,X) \in L_2(W,X): \E[S(W,X)]=0\} \\ &+ \{S(Z,A|W,X) \in L_2(W,X)^{\perp}: \E[q^*(Z,A,X) S(Z,A|W,X)|W,X]\in \text{cl}(R(T_{tr})\}.
\end{align*} Further, they showed that 
\begin{equation*}
    \E[q^*(Z,A,X)S(Z,A|W,X)|W,A,X]=-\E[\partial q_t(Z,A,X)/\partial t|_{t=0}|W,A,X],
\end{equation*}
or in the notation of \cite{SEVERINI2012491}, 
\begin{equation*}
    \E[q^*(Z,A,X)S(Z,A|W,X)|W,A,X]=-T_{tr}\partial q_t(Z,A,X)/\partial t|_{t=0},
\end{equation*}
which implies that 
\begin{equation*}
    T_{tr}^+\E[q^*(Z,A,X)S(Z,A|W,X)|W,A,X]=-\partial q_t(Z,A,X)/\partial t|_{t=0},
\end{equation*}
where $T_{tr}^+$ is the Moore-Penrose pseudo-inverse of $T_{tr}$. Consider the class of functionals $\beta_{tr}=\E[\phi_{tr}(Z,A,X)q(Z,A,X)]$. Following \cite{SEVERINI2012491}, it is possible to show that 
\begin{equation*}
    \partial \beta_{tr} / \partial t |_{t=0}= \E[\phi_{tr} \partial q_t(W,A,X)/\partial t|_{t=0} ]+\E[\phi_{tr} q^*(Z,A,X)S(Z,A,W,X)].
\end{equation*}
Next, we follow the proof of Lemma 4.1 in \cite{SEVERINI2012491}. Let $(\lambda_j,a_j,b_j)$ denote the singular system for $T_{tr}$ and $T_{tr}'$ where $(a_j)$ and $(b_j)$ are orthonormal bases for $\mathcal{N}(T_{tr})^{\perp}$ and $\text{cl}(\mathcal{R}(T_{tr}))$, and $\lambda_j$ are singular values. We then have $\phi_{tr}=\sum_{j=1}^\infty F_{a_j}(\phi_{tr})a_j$ where $F_{a_j}(\phi_{tr}):=\la \phi_{tr},a_j\ra$. Then
\begin{equation*}
    \E[\phi_{tr} T_{tr}^+\E[q^*(Z,A,X) S(Z,A|W,X)|W,A,X] = \sum_{j=1}^\infty \lambda_j^{-1} F_{a_j}(\phi_o)F_{b_j}(\E[q^*(Z,A,X) S(Z,A|W,X)|W,A,X]) =: K_{\phi_{tr}}(S).
\end{equation*}
Following arguments in \cite{SEVERINI2012491}, $K_{\phi_{tr}}(S)$ is well-defined on $\Lambda_2$.
Note that any $\phi_{tr}$ that makes $K_{\phi_{tr}}(S)$ unbounded will make $\partial \beta_{tr} / \partial t |_{t=0}$ unbounded, and so the corresponding functional $\E[\phi_{tr} q^*(Z,A,X)]$ will not be $\sqrt{n}$-estimable. Let $S \in \Lambda_2$. Then by Cauchy-Schwarz and Bessel's inequality, 
\begin{equation*}
\begin{aligned}
|K_{\phi_{tr}}(S)|^2 &\leq \sum_{j=1}^\infty \lambda_j^{-2}F_{a_j}^2(\phi_{tr})\Norm{\E[q^*(Z,A,X)S(Z,A|W,X)]}_{L_2(Z,A,W,X)}^2 \\ &\lesssim \sum_{j=1}^\infty \lambda_j^{-2}F_{a_j}^2(\phi_{tr})\Norm{ S(Z,A|W,X)}_{L_2(Z,A,W,X)}^2
\end{aligned}
\end{equation*}
Since $\mathcal{R}(T_{tr}')= \{S \in L_2(Z,A,X):\sum_{j=1}^\infty \lambda_j^{-2}\la S, a_j \ra^2 < \infty\}$ by SVD of $T_{tr}'$, $K_{\phi_{tr}}(S)$ is bounded whenever $\phi_{tr} \in \mathcal{R}(T_{tr}')$. Next let, $\phi_{tr}^0 \in \text{cl}(\mathcal{R}(T_{tr}')) - \mathcal{R}(T_{tr}')$. Then $\sum_{j=1}^\infty \lambda_j^{-2}F_{a_j}^2(\phi_{tr}^0) = \infty$. By assumption \ref{regularityq}, for any $r$,
\begin{equation*}
    d_r:=(q^*(Z,A,X)) \Psi_{tr} \sum_{i=1}^r \lambda_i^{-1}F_{a_i}(\psi_{tr})b_i \in L_2(W,Z,A,X).
\end{equation*}
Moreover, $\E[q^*(Z,A,X)) \Psi_{tr} \sum_{i=1}^r \lambda_i^{-1}F_{a_i}(\psi_0)b_i|W,A,X]=0 $ and $\E[q^*(W,a,X)^2 \Psi_{tr} \sum_{i=1}^r \lambda_i^{-1}F_{a_i}(\psi_0)b_i|W,A,X] \in \text{cl}(\mathcal{R}(T_{tr})) $. Thus, $d_r \in \Lambda_2$. Also, we see that $K_{\phi_{tr}}(d_r)=\Norm{d_r}^2$. Then letting $f_r=d_r/\Norm{d_r}$, we see that $f_r$ is a sequence of unit vectors in $\Lambda_2$ such that \begin{equation*}
    \lim_{r\to \infty} K_{\phi_o^t}(f_r)= \left(\sum_{j=1}^\infty \lambda_j^{-2}F_{a_j}^2(\phi_{tr}^0)\right)^{1/2} = \infty,
\end{equation*}
and so $K_{\phi_{tr}^0}(S)$ is unbounded.
\end{proof}
\section{Discussion on Efficiency}
Let $\mathcal{M}$ be the model which places no restriction on the observed data distribution other than existence (but not necessarily uniqueness)
of a bridge function h that solves Equation 1 at all regular laws. We do not compute the semiparametric efficiency bound under this model. Within this model, \cite{semi_proxy} consider the submodel in which the following also hold:
\begin{enumerate}[(a)]
    \item The maps $T_o$ and $T_o'$ are surjective.
    \item Equation 3 holds at the true data generating law.
    \item The $h$ and $q$ that solve 1 and 3 are unique.
\end{enumerate}
They derive that the efficient influence function for the counterfactual mean under the full model $\mathcal{M}$ evaluated at the submodel where a - c hold is $h(W,a,X)-\mu_a+I(A=a)q(Z,a,X)\times(Y-h(W,a,X))$. Thus,
we get the following corollary of Theorem 3.4:
\begin{corollary}
\label{cor:2.6}
Suppose that assumptions 1-5 and equation 15 in the main manuscript hold. The influence function 19 of the de-biased estimator equals the efficient influence function of $\mu_a$ under model $\mathcal{M}$ if a - c also hold at the true data generating law. Thus, the debiased estimator is locally efficient.
\end{corollary}
Importantly, we do not assume a - c for our estimator to be regular and asymptotically linear, nor do we assume a - c hold for all laws in $\mathcal{M}$ for local efficiency, only that they hold at the true data generating law. For more details about local efficiency, we refer the reader to \cite{Bickel1993EfficientAA}. A useful illustration of local efficiency is that of the ordinary least squares estimator. Let $(Y_i,X_i) \sim_{iid} F$ for $F$ unknown. Consider the linear model that only a priori assumes that $E(Y|X)=\beta'X$, but allows the distribution of $Y|X$ to otherwise be unspecified. Then the OLS estimator is regular and asymptotically linear at all laws of the linear model, but also attains the semiparametric efficiency bound for the linear model at the submodel where Var$(Y|X)$ does not depend on $X$, i.e. when the errors are homescedastic. Thus, the least squares estimator is locally efficient. Analogously, our estimator is regular and asymptotically linear at all laws in $\mathcal{M}$, but also attains the semiparametric efficiency bound at the submodel introduced in \cite{semi_proxy}. 
\section{A Larger Class of Functionals}
\label{appendix_class}
Consider the setting from \cite{ghassami}. Let $V$ denote the variables from which independent and identically distributed data is collected. $V_q$ and $V_h$ are (not necessarily disjoint) subsets
of $V$, and $g_1$, $g_2$, $g_3$, and $g_4$ are known measurable functions. Consider estimating a functional that takes the following form:
\begin{equation*}
    \psi := \mathbb{E}[q\left(V_{q}\right) h\left(V_{h}\right) g_{1}(V)+q\left(V_{q}\right) g_{2}(V)+h\left(V_{h}\right) g_{3}(V)+g_{4}(V)],
\end{equation*}
where $h$ and $q$ are nuisance functions and are known to to satisfy 
\begin{equation}
\label{eq:nuisance}
\begin{aligned}
&\mathbb{E}\left[{h}\left(V_{h}\right) g_{1}(V)+g_{2}(V) \mid V_{q}\right]=0, \\
&\mathbb{E}\left[{q}\left(V_{q}\right) g_{1}(V)+g_{3}(V) \mid V_{h}\right]=0,
\end{aligned}
\end{equation}
although the solution to the above equations may not be unique. This includes the functional studied in \cite{nonig}, taking their $\delta$ as $h$ and their $\gamma$ as $q$. Moreover, take $V_h=(X,Z)$, $V_q=(X,Y)$, $g_1(V)=-R$, $g_2(V)=R\tau(X,Y)$, $g_3(V)=(1-R)$, $g_4(V)=R\tau(X,Y)$. In the proximal setting, we consider the counterfactual mean $\mathbb{E}[Y(a)]$. Explicitly, let $V_h = (W,X)$, $V_q = (Z,X)$, and take $h(W,X)$ to be the function that solves $\mathbb{E}[I(A=a)h(W,X)\mid Z,X]=\mathbb{E}[I(A=a)Y\mid Z,X]$ and $q(Z,X)$ to be the function that solves $\mathbb{E}[I(A=a)q(Z,X)-1\mid W,X]=0$. Take $g_1(V)=-I(A=a)$, $g_2(V)=I(A=a)Y$, $g_3(V)=1$, $g_4(V)=0$. One can readily verify that the choices of the $g_i$ and $h$ and $q$ conform to the general class as well as satisfy \ref{eq:nuisance}. The formulation for the proximal setting here does not exactly match the formulation in the main manuscript stylistically. However, the estimation strategy outlined in the main manuscript indeed matches the strategy for the general class outlined below up to some notational differences. Back to the general setting, note that under \ref{eq:nuisance}, 
\begin{equation*}
    \psi = \mathbb{E}[h(V_h)g_3(V)+g_4(V)] =\mathbb{E}[q(V_q)g_2(V)+g_4(V)].
\end{equation*}
Then estimation and inference of $\psi$ can proceed in an analogous manner as in Section 3 of the main manuscript. First, choose to either estimate the solution set for $\mathbb{E}\left[{h}\left(V_{h}\right) g_{1}(V)+g_{2}(V) \mid V_{q}\right]=0$ or \\$\mathbb{E}\left[{q}\left(V_{q}\right) g_{1}(V)+g_{3}(V) \mid V_{h}\right]=0$. For this illustration, suppose we estimate the solution set $\mathcal{H}_0 := \{h \in \mathcal{H}: \mathbb{E}\left[{h}\left(V_{h}\right) g_{1}(V)+g_{2}(V) \mid V_{q}\right]=0\}$. Explicitly, let $\mathcal{H}_n$ be a sieve for $\mathcal{H}$. Let $\widehat{\E}[\cdot \mid V_q]$ be a nonparametric condtional expectation operator and define $e(V_i,h)=\E[h(V_h)g_1(V)-g_2(V)|{V_q}_i]$ and $\wh{e}(V_i,h)=\wh{\mathbb{E}}[h(V_h)g_1(V)+g_2(V)|{V_q}_i]$. Define $C_n(h)=\frac{1}{n}\sum_{i=1}^n \wh{e}^2(V_i,h)$. Let $\widehat{\mathcal{H}}_0=\{ h \in \mathcal{H}_n:C_n(h) \leq c_n\}$ be an estimate for $\mathcal{H}_0$. We can similarly construct an estimate $\widehat{h}_0 \in \widehat{\mathcal{H}}_0$ that converges to a specific element $h_0 \in \mathcal{H}_0$, specifically, $h_0 =\argmin_{h \in \mathcal{H}_0} M(h)$ where $M$ is a convex function, for example $M(h)=\E[h(V_h)^2]$. The sample analog is $M_n(h)=\frac{1}{n}\sum_{i=1}^n h^2({V_h}_{i})$. Then select 
\begin{equation*}
\wh{h}_0\in \argmin_{h \in \wh{\hh}_0}M_n(h).
\end{equation*}
Next, we can construct a plug-in estimator for $\psi$ by taking $\widehat{\psi}=\frac{1}{n}\sum_{i=1}^n g_3(V_i)\widehat{h}_0({V_h}_i)+g_4(V_i)$. Such an estimator will presumably not be root-$n$ consistent. Instead, define the inner product 
\begin{equation*}
    \langle h_1, h_2 \rangle_w=\E[\E[g_1(V)h_1(V_h)|V_q]\E[g_1(V)h_2(V_h)|V_q]].
\end{equation*}
Suppose that the following holds:
\begin{assumption}
There exists a function $h_r \in \mathcal{H}$ such that $\langle h_r, h \rangle_w=\E[h(V_h)g_3(V)]$ for all $h \in \overline{\mathcal{H}}$.
\end{assumption}
This assumption holds if there exists an $h_r$ that satisfies $\E[g_1(V)h_r(V_h)|V_q]=-q(V_q)$ for any $q$ that satisfies the moment equation $\mathbb{E}\left[{q}\left(V_{q}\right) g_{1}(V)+g_{3}(V) \mid V_{h}\right]=0$. To see this, note that
\begin{equation*}
    \begin{aligned}
     \langle h_r, h \rangle_w &= \E[\E[g_1(V)h_r(V_h)|V_q]\E[g_1(V)h(V_h)|V_q]] \\ &= \E[-q(V_q)\E[g_1(V)h(V_h)|V_q]] \\ &= \E[-g_1(V)q(V_q)h(V_h)] \\ &= \E[\E[-g_1(V)q(V_q)h(V_h)|V_h]] \\ &= \E[\E[g_3(V)h(V_h)|V_h]] \\ &= \E[g_3(V)h(V_h)]
    \end{aligned}
\end{equation*}
Under similar regularity conditions as in the main manuscript, it is possible to show that 
\begin{equation*}
    \sqrt{n}(\wh{\psi}-\psi)=\frac{1}{\sqrt{n}}\sum_{i=1}^n g_3(V_i)h_0({V_h}_i)-\E[g_1(V)h_r(V_h) \mid {V_q}_i]\times (h({V_h}_i)g_1(V_i)+g_2(V_i))+g_4(V_i) - \psi -\sqrt{n}r_n(\wh{h}_0)+o_p(1),
\end{equation*}
where $\sqrt{n}r_n(\wh{h}_0)=\frac{1}{n}\sum_{i=1}^n \wh{E}\{g_1(V)h_r(V_h) \mid {V_q}_i\}\wh{e}({V_i},\wh{h}_0)$.
Consider the criterion function
\begin{equation*}
    R(h)=\E[\E[g_1(V)h(V_h) \mid V_q]^2]-2\E[g_3(V)h(V_h)].
\end{equation*}
In a similar vein as in the main manuscript, we can construct a debiased estimator by estimating $r_n(\wh{h}_0)$. First, let \begin{equation*}
    R_n(h)=\frac{1}{n}\sum_{i=1}^n\wh{\E}\{g_1(V)h(V_h)|V_{q_i}\}^2-\frac{2}{n}\sum_{i=1}^n g_3(V_i)h(V_{h_i}) \ , \ h \in \hh
\end{equation*}
be the sample version of the criterion function $R(h)$.
Observe that since $\la h_r, h \ra_w=\E\{g_3(V)h(V_h)\}$, we have that $R(h)=\Norm{h-h_r}^2_w-\Norm{h_r}_w^2$. It follows that $h_r$ is the unique minimizer of the mapping $h \to R(h)$. Then  we can estimate the term $h_r$ by 
\begin{equation}
    \wh{h}_r \in \argmin_{h \in \hh_n } R_n(h).
\end{equation}
With this estimate, we can construct the following estimator for $r_n(\wh{h}_0)$
as 
\begin{equation}
    \wh{r}_n(\wh{h}_0)=\frac{1}{n}\sum_{i=1}^n  \wh{\E}\{g_1(V)\wh{h}_r(V_h)|V_{q_i}\}\wh{e}(V_i,\wh{h}_0).
\end{equation}
The debiased estimator
$\wh{\psi}_{db}=\wh{\psi}+ \wh{r}_n(\wh{h}_0)$  can be shown to have influence function $g_3(V)h_0({V_h})-\E[g_1(V)h_r(V_h) \mid {V_q}]\times (h({V_h})g_1(V)+g_2(V))+g_4(V) - \psi$ under similar regularity conditions as in the main manuscript. If $\E[g_1(V)h_r(V_h) \mid {V_q}] = -q(V_q)$, this equals $$g_3(V)h_0({V_h})+q(V_q) h_0({V_h})g_1(V)+g_2(V)q(V_q)+g_4(V) - \psi,$$
which corresponds to the influence function for functionals studied in \cite{ghassami} if $h$ and $q$ are unique.

\bibliographystyle{abbrvnat}
%\bibliography{template_short}
\bibliography{zupplement.bib}
\end{document}