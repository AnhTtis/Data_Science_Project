\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

\input{math_commands.tex}

\usepackage{varioref}
\usepackage{hyperref}
\usepackage{url} 

\usepackage{titletoc}
\usepackage[page, header]{appendix}



\usepackage{booktabs}       % 

\usepackage{cleveref}

\usepackage{mathtools}
\usepackage{xcolor}


\usepackage{enumitem}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{xcolor,colortbl}%%table
\definecolor{DarkBlue}{RGB}{22,54,93}%%table
\def \CC {\textcolor{red}}


\hypersetup{linkcolor =red, citecolor = blue}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{adjustbox}
\usepackage{comment}
\usepackage{bbm}


% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2022}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{threeparttable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% theoremS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\theoremstyle{remark}
\newtheorem{remark}{Remark}


\newcommand{\Pb}{\mathbb{P}}
\newcommand{\Rb}{\mathbb{R}}
\newcommand{\hP}{\hat{P}}
\newcommand{\hPb}{\hat{\mathbb{P}}}
\newcommand{\hphi}{\hat{\phi}}
\newcommand{\hmu}{\hat{\mu}}
\newcommand{\tphi}{\tilde{\phi}}
\newcommand{\sphi}{\phi^\star}
\newcommand{\tmu}{\tilde{\mu}}
\newcommand{\eo}{\epsilon_0}
\newcommand{\bpi}{\bar{\pi}}
\newcommand{\Eb}{\mathbb{E}}
\newcommand{\hEb}{\hat{\mathbb{E}}}
\newcommand{\sEb}{\overset{*}{\mathbb{E}}}
\newcommand{\Ps}{{P^\star}}
\newcommand{\Pshi}{\phi^\star}
\newcommand{\smu}{\mu^\star}
\newcommand{\Mc}{\mathcal{M}}
\newcommand{\Uc}{\mathcal{U}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Ac}{\mathcal{A}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\hb}{\hat{b}}
\newcommand{\ph}{{h^\prime}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Lc}{\mathcal{L}}
\newcommand{\Bc}{\mathcal{B}}

\newcommand{\hV}{{\hat{V}}}

\newcommand{\Mt}{\mathtt{M}}
\newcommand{\Ec}{\mathcal{E}}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\RM}[1]{\left(\romannumeral#1\right)}
\newcommand{\hw}{\widehat{w}} 
\newcommand{\tw}{\widetilde{w}} 



\newenvironment{Proof}[1]{\medskip\par\noindent
	{\bf Proof:\,}\,#1}{{\mbox{\,$\blacksquare$}\par}}

\newcommand{\hrq}[1]{{\color{blue}(Hrq: #1)}}
\newcommand{\cy}[1]{{\color{brown}( #1)}}
\newcommand{\yl}[1]{{\color{red}(YL: #1)}}
\newcommand{\ylfixed}[1]{{\color{green}(YL: #1)}}

\newcommand{\tcr}[1]{{\color{magenta}(JY: #1)}}
\newcommand{\jing}[1]{{\color{magenta} #1}}
\newcommand{\jingc}[1]{{\color{magenta} (JY: #1)}}


\title{Improved Sample Complexity for Reward-free Reinforcement Learning under Low-rank MDPs}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Yuan Cheng\thanks{Equal contribution}\\ 
%School of Statistics and Finance\\
University of Science and Technology of China\\
%Hefei, China\\
{\tt   cy16@mail.ustc.edu.cn}\\
	\And
	Ruiquan Huang$^{*}$\\
 %School of Electrical Engineering and Computer Science\\
 The Pennsylvania State University\\
 %University Park, USA\\
 {\tt   rzh5514@psu.edu}\\
 \AND Jing Yang\\
 %School of Electrical Engineering and Computer Science\\
 The Pennsylvania State University\\
 %University Park, USA\\
{\tt  yangjing@psu.edu }
	\And
	Yingbin Liang\\
 %Department of Electrical and Computer Engineering\\
 The Ohio State University\\
 %Columbus, USA\\
 {\tt   liang.889@osu.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\iclrfinalcopy
\begin{document}
	\maketitle
\begin{abstract}

In reward-free reinforcement learning (RL), an agent explores the environment first without any reward information, in order to achieve certain learning goals afterwards for any given reward. In this paper we focus on reward-free RL under low-rank MDP models, in which both the representation and linear weight vectors are unknown. Although various algorithms have been proposed for reward-free low-rank MDPs, the corresponding sample complexity is still far from being satisfactory. In this work, we first provide the first known sample complexity lower bound that holds for any algorithm under low-rank MDPs. This lower bound implies it is strictly harder to find a near-optimal policy under low-rank MDPs than under linear MDPs. We then propose a novel model-based algorithm, coined RAFFLE, and show it can both find an $\epsilon$-optimal policy and achieve an $\epsilon$-accurate system identification via reward-free exploration, with a sample complexity significantly improving the previous results. Such a sample complexity matches our lower bound in the dependence on $\epsilon$, as well as on $K$ {in the large $d$ regime}, where $d$ and $K$ respectively denote the representation dimension and action space cardinality. Finally, we provide a planning algorithm (without further interaction with true environment) for RAFFLE to learn a near-accurate representation, which is the first known representation learning guarantee under the same setting.  
\end{abstract}

\section{Introduction}\label{intro}

Reward-free reinforcement learning, recently formalized by \citet{jin2020provably}, arises as a powerful framework to accommodate diverse demands in sequential learning applications. Under the reward-free RL framework, an agent first explores the environment without reward information during the exploration phase, with the objective to achieve certain learning goals later on for any given reward function during the planning phase. Such a learning goal can be to find an $\epsilon$-optimal policy, to achieve an $\epsilon$-accurate system identification, etc. 
	%\citet{pmlr-v119-jin20d} %first formalized the reward-free paradigm and showed that collecting $\tilde{O}(poly(SAH)/\epsilon^2)$ samples in the exploration phase are necessary and sufficient in order to output an $\epsilon$-optimal policies for any given reward in the planning phase. Here, $S, A$ are the number of states and number of actions, H is the episodes horizon. Follow-up theoretic works has applied to various reward-free RL settings. 
	The reward-free RL paradigm may find broad application in many real-world engineering problems. For instance, reward-free exploration can be efficient when various reward functions are taken into consideration over a single environment, such as safe RL \citep{ miryoosefi2021simple,huang2022safe}, multi-objective RL~\citep{wu2021accommodating}, multi-task RL~\citep{agarwal2022provable,DBLP:journals/corr/abs-2206-05900}, etc. Studies of reward-free RL on the theoretical side have been largely focused on characterizing the sample complexity to achieve a learning goal under various MDP models. 
	%Most existing works on reward-free RL focus on tabular RL. The optimal sample complexity on $|\Sc|$ and $|\Ac|$ has bee achieved \cite{menard2021fast}, where $\Sc$ is the state sapce and $\Ac$ is the action space. When the state space is undesirably large, function approximation is needed. Some researchers study the reward-free RL with the linear function approximation\cite{wang2020reward} or linear mixture model \cite{zhang2021reward} and achieve acceptable sample complexity, i.e. polynomial dependency on the feature dimension $d$. However, linear or linear mixture MDP assume that the features are known in prior, which is usually unrealistic. Therefore, we aim to study the reward-free RL with a more general function approximation.
	Specifically, reward-free tabular RL has been studied in \citet{pmlr-v119-jin20d,menard2021fast,kaufmann2021adaptive,zhang2020nearly}.  
	%Specifically, for tabular RL, \citet{pmlr-v119-jin20d} considered the tabular MDP and established the necessary and sufficient sample complexities during the exploration phase in order to output an $\epsilon$-optimal policy during the planning phase. \citet{menard2021fast,kaufmann2021adaptive,zhang2020nearly} then proposed improved algorithms to achieve better or nearly minimax optimal sample complexity.  
	For reward-free RL with function approximation, \citet{wang2020reward} studied \textit{linear MDPs} introduced by \citet{jin2020provably}, 
	where both the transition and the reward are linear functions of a given feature extractor, \citet{zhang2021reward} studied \textit{linear mixture MDPs} introduced by \citet{ayoub2020model}, and
	% , where the transition is a linear mixture of known transition kernels. 
	\citet{zanette2020provably} considered a classes of MDPs with \textit{low inherent Bellman error} introduced by \citet{zanette2020learning}. 
	% \begin{table*}[t]
		% \caption{Comparison of reward-free RL algorithms under low-rank MDPs. }
		% \label{Table: Comparison}
		% \vskip 0.15in
		% \begin{center}
			% \begin{small}
				% \begin{threeparttable}
					% \begin{sc}
						% \begin{tabular}{lll}
							% \toprule
							% Methods & Setting & Complexity 
							% %& Computation 
							% \\
							% \midrule
							% FLAMBE~\citep{NEURIPS2020_e894d787}    & Low-rank MDP & $\tilde{O}(\frac{H^{22}K^9d^7}{\epsilon^{10}})$
							% %& Oracle Efficient 
							% \\
							% MOFFLE~\citep{modi2021model}    & Low-nonnegative-rank MDP & $\tilde{O}(\frac{H^5d^3_{LV}K^5}{\epsilon^2\eta})$
							% %& Oracle Efficient
							% \\
							% %$\tilde{O}(\frac{d^8H^5K^{13}}{\min(\eta^5,\epsilon^2\eta)})$
							% %HOMER\citep{misra2020kinematic} & Block MDP & $\tilde{O}(\frac{d^8H^4K^4}{\min(\eta^3,\epsilon^2)})$ & Free\\
							% RFOLIVE~\tnote{1}~\citep{DBLP:journals/corr/abs-2206-10770}    & Low Bellman Eluder Dimension& $\tilde{O}(\frac{H^8d^3Kd_{\mathcal{R}}}{\epsilon^2})$ 
							% %& Inefficient 
							% \\
							% RAFFLE (Ours)    & Low-rank MDP& $\tilde{O}(\frac{H^3d^2K(d^2+K)}{\epsilon^2})$ %& Oracle Efficient 
							% \\
							% \bottomrule
							% \end{tabular}
						% \end{sc}
					% \begin{tablenotes}
						% 	% \item[1] This column identifies whether an algorithm is reward-free (marked as FREE) or reward-known (marked as KNOWN).
						% 	% \item[2] For reward-know RL, we do not include algorithms designed for low Bellman rank \citep{jiang2017contextual}, low witness rank \citep{sun2019model}, bilinear classes \citep{du2021bilinear} and low Bellman eluder dimension\citep{jin2021bellman} of MDPs, which can specialize to low-rank MDPs. Although their sample complexity may have sharper dependence on $d,K$ or $H$, these algorithms are not computationally efficient as noted in Section 2 in \citet{DBLP:conf/iclr/UeharaZS22}.
						%     \item[1]  The result of RFOLIVE~\citep{DBLP:journals/corr/abs-2206-10770} 
						%     %consider general nonlinear function approximation with low Bellman Eluder Dimension~\citep{jin2021bellman}, which can specialize to low-rank MDPs. Although they achieves a lower dependence on $K$, it 
						%     is not directly comparable with our result: $\RM{1}$ Similar to its reward-known version OLIVE~\citep{jiang2017contextual}, RFOLIVE is not computationally efficient as noted in \citet{NEURIPS2020_e894d787,DBLP:conf/iclr/UeharaZS22}, $\RM{2}$ RFOLIVE assumes parametric growth of the covering number w.r.t $d_{\mathcal{R}}$ for the reward class, whereas we do not make any assumption on the reward class. 	
						% \end{tablenotes}
					% \end{threeparttable}
				% \end{small}
			% \end{center}
		% \vskip -0.1in
		% \end{table*}
		
	In this paper, we focus on reward-free RL under low-rank MDPs, where the transition kernel admits a decomposition into two embedding functions that map to low dimensional spaces. Compared with linear MDPs, the feature functions (i.e., the representation) under low-rank MDPs are unknown, hence the design further requires representation learning and becomes more challenging.  Reward-free RL under low-rank MDPs was first studied by \citet{NEURIPS2020_e894d787}, and the authors introduced a provably efficient algorithm FLAMBE, 
	which achieves the learning goal of system identification 
	%(that implies the best-policy goal) 
	with a sample complexity of $\tilde{O}(\frac{H^{22}K^9d^7}{\epsilon^{10}})$. Here $d$, $H$ and $K$ respectively denote the representation dimension, episode horizon, and action space cardinality. Later on, \citet{modi2021model} proposed a model-free algorithm MOFFLE for reward-free RL under low-nonnegative-rank MDPs (where feature functions are non-negative), for which the sample complexity for finding an $\epsilon$-optimal policy scales as $\tilde{O}(\frac{H^5K^5d^3_{LV}}{\epsilon^2\eta})$ (which is rescaled under the condition of $\sum_{h=1}^{H}r_h \leq 1$ for fair comparison). Here, $d_{LV}$ denotes the non-negative rank of the transition kernel, which may be exponentially larger than $d$ as shown in \citet{NEURIPS2020_e894d787}, and $\eta$ denotes the positive reachability probability to all states, where $1/\eta$ can be as large as $\sqrt{d_{LV}}$ as shown in \citet{DBLP:conf/iclr/UeharaZS22}. Recently, a reward-free algorithm called RFOLIVE has been proposed under non-linear MDPs with low Bellman Eluder dimension \citep{DBLP:journals/corr/abs-2206-10770}, which can be specialized to low-rank MDPs. However, RFOLIVE is computationally more costly and considers a special reward function class, making their complexity result not directly comparable to other studies on reward-free low-rank MDPs.
	
This paper investigates reward-free RL under low-rank MDPs to address the following important open questions:
\begin{list}{$\bullet$}{\topsep=0.1ex \leftmargin=0.15in \rightmargin=0.1in \itemsep =0.01in}

\item For low-rank MDPs, none of previous studies establishes a lower bound on the sample complexity showing a necessary sample complexity requirement for near-optimal policy finding. 

%What is the fundamental limit on the required sample complexity to learn a low-rank MDP?
		%From the previous studies, it is unclear how difficult low-rank MDPs can be efficiently learned. 
%		While this question can be answered by a lower bound on the sample complexity for low-rank MDPs, none of the previous studies on low-rank MDPs establish such a lower bound.
 
%\item Can we design a reward-free algorithm for low-rank MDPs that improves the sample complexity of FLAMBE in \citet{NEURIPS2020_e894d787} and MOFFLE in \citet{modi2021model}, and achieves the goal of system identification as in \citet{NEURIPS2020_e894d787} and finds near-optimal policy as in \citet{modi2021model}?

\item The sample complexity of previous algorithms in \citet{NEURIPS2020_e894d787,modi2021model} on reward-free low-rank MDP is polynomial in the involved parameters, but still much higher than desirable. It is vital to improve the algorithm to further reduce the sample complexity. 
		
		%Can we improve the sample complexity in \citet{modi2021model} for the same setting without the non-negative feature assumption and without introducing the positive reachability probability $\eta$? 
		
\item Previous studies on low-rank MDPs did not provide estimation accuracy guarantee on the learned representation (only on the transition kernels). However, such a representation learning guarantee can be very beneficial to reuse the learned representation in other RL environment.
		
%Intuitively, the features can be utilized if learned accurately. However, 
		
%none of previous studies on low-rank MDPs provide a performance guarantee on their learned representation (they provided only the guarantee on estimation accuracy of the transition probability, not on the representation.   
\end{list}
	
	%The result was then improved by \citet{modi2021model} and \citet{DBLP:conf/iclr/UeharaZS22} with additional assumptions. hence, the following fundamental question remains open: Can we further improve the sample complexity under the low-rank MDP without any reward information?  
	
	
	%$P(s_{h+1}|s_h,a_h) = \langle\phi_h(s_h,a_h), \mu_h(s_{h+1})\rangle$, which is the probability mass or probability density on the next state $s_{h+1}$ given the previous state $s_h$ and action $a_h$. Here, $\phi_h$ and $\mu_h$ are embedding functions mapping to low dimensional spaces. The major challenge is that the embedding functions or features are unknown in the low-rank MDP. Unknown features are common in modern machine learning, since real world data does not reveal the features directly. However, good features or representations improve the learning performance dramatically, which brings us two important questions: how can we learn a good representation and what is the learning rate of a valid representation learning algorithm. FLAMBE \cite{NEURIPS2020_e894d787} first introduce a provably algorithm which requires polynomial sample complexity. The result was then improved by \citet{modi2021model} and \citet{DBLP:conf/iclr/UeharaZS22} with additional assumptions. hence, the following fundamental question remains open: Can we further improve the sample complexity under the low-rank MDP without any reward information?  
	\vspace{-0.022in}
	\subsection{Main Contributions} \label{subsec: contribution}
	\vspace{-0.022in}
	We summarize our main contributions in this work below.
\begin{list}{$\bullet$}{\topsep=0.1ex \leftmargin=0.15in \rightmargin=0.1in \itemsep =0.01in}

\item {\bf Lower bound:} We provide the first-known lower bound $\tilde{\Omega}(\frac{HdK}{\epsilon^2})$ on the sample complexity that holds for any algorithm under the same low-rank MDP setting. Our proof lies in a novel construction of hard MDP instances that capture the necessity of the cardinality of the action space on the sample complexity. Interestingly, comparing this lower bound for low-rank MDPs with the upper bound for linear MDPs in \citet{wang2020reward} further implies that it is strictly more challenging to find near-optimal policy under low-rank MDPs than linear MDPs. 
  
\item {\bf Algorithm:} We propose a new model-based reward-free RL algorithm under low-rank MDPs. The central idea of RAFFLE lies in the construction of a novel exploration-driven reward, whose corresponding value function serves as an upper bound on the model estimation error. Hence, such a pseudo-reward encourages the exploration to collect samples over those state-action space where the model estimation error is large so that later stage of the algorithm can further reduce such an error based on those samples. Such reward construction is new for low-rank MDPs, and serve as the key reason for our improved sample complexity. 


		
%our algorithm efficiently and extensively collect data under rich observation assumptions.
		
%Our key difference from FLAMBE~\cite{NEURIPS2020_e894d787} lies in more sample-efficient MLE estimation of the transition kernel by the exploration policy. Rather than collecting multiple samples for every single exploration policy at each time step as FLAMBE, we collect only one episode for each policy at each time step, and further combine all the policy before the $n$-th episode into a mixture policy and collect all episodes into the dataset for MLE estimation in order to guarantee the estimation accuracy. \yl{Yuan and Ruiquan: check if my statement is precise} 
%Compared to FLAMBE~\cite{NEURIPS2020_e894d787}, our algorithm only require executing each exploration policy for $H$ times. In addition, we are able to locate a single model which achieves the same guarantee as in FLAMBE during the exploration process.
		
%\textcolor{red}{[Summarize the main novelty of the algorithm; its difference from previous design; from the design point of view why such an algorithm can be more sample efficient]}
		
\item {\bf Sample complexity:} We show that our algorithm can both find an $\epsilon$-optimal policy and achieve an $\epsilon$-accurate system identification via reward-free exploration, with a sample complexity of $\tilde{O}(\frac{H^3d^2K(d^2+K)}{\epsilon^2})$, which matches our lower bound in terms of the dependence on $\epsilon$ as well as on $K$ {in the large $d$ regime}. Our result significantly improves that of $\tilde{O}(\frac{H^{22}K^9d^7}{\epsilon^{10}})$ in \citet{NEURIPS2020_e894d787} to achieve the same goal. Our result also improves the sample complexity of $\tilde{O}(\frac{H^5K^5d^3_{LV}}{\epsilon^2\eta})$ in \citet{modi2021model} in three aspects: order on $K$ is reduced; $d$ can be exponentially smaller than $d_{LV}$ as shown in \citet{NEURIPS2020_e894d787}; and no introduction of $\eta$, where $1/\eta$ can be as large as $\sqrt{d_{LV}}$. Further, our result on reward-free RL naturally achieves the goal of reward-known RL, 
%with the same sample complexity, 
which improves that of $\tilde{O}\left(\frac{H^5d^4K^2}{\epsilon^2}\right)$ in~\citet{DBLP:conf/iclr/UeharaZS22} by $ \Theta(H^2)$. 
%Further, our result of reward-free RL is naturally applicable to reward-known RL, which improves the sample complexity of $\tilde{O}\left(\frac{H^5d^4K^2}{\epsilon^2}\right)$ there in~\citep{DBLP:conf/iclr/UeharaZS22} by a factor of $\mathcal O(H^2)$.
%can naturally extend to reward-known RL with same sample complexity, and compared with the SOTA~\citep{DBLP:conf/iclr/UeharaZS22} with sample complexity $\tilde{O}\left(\frac{H^5d^4K^2}{\epsilon^2}\right)$, our result improves the dependence on $H$.  

\item {\bf Near-accurate representation learning:}
%In addition to the goal of system identification, which guarantees a near-accurate transition kernel estimation, 
We design a planning algorithm that exploits the exploration phase of RAFFLE to further learn a provably near-accurate representation of the transition kernel without requiring further interaction with the environment. To the best of our knowledge, this is the first theoretical guarantee on representation learning for low-rank MDPs.
		
		
		%estimate the representation individually with only access to the learned transition kernels from RAFFLE and without interacting with true environment, and then theoretically characterize the closeness between the learned and the ground truth representations.
		
\end{list}

% \textcolor{blue}{We specially note that this work was initially submitted for potential publication in January 2022, after which many studies have been developed on various extended low-rank models (see more details in \Cref{sec:relatedwork}). Our results on the lower bound and representation learning are still completely new even given those later developments, and our algorithm design and result on sample complexity are so far still the best-known result for vanilla reward-free low-rank MDPs. More details of related work can be seen in \Cref{sec:relatedwork}.}


\section{Preliminaries and Problem Formulation}

\textbf{Notation.} For any $H\in\mathbb{N}$, we denote $[H]:=\{1,\dots,H\}$. For any vector $x$ and symmetric matrix $A$, we denote $\norm{x}_2$ as the $\ell_2$ norm of $x$ and $\|x\|_A : = \sqrt{x^\top Ax}$. For any matrix $A$, we denote $\norm{A}_F$ as its Frobenius norm and let $\sigma_i(A)$ be its $i$-th largest singular value. For two probability measures $P, Q\in \Omega$, we use $\|P-Q\|_{TV}$ to denote their total variation distance.  
\vspace{-0.1in}
\subsection{Episodic MDPs}
%	\vspace{-0.1in}
We consider an episodic Markov decision process (MDP) $\Mc = (\Sc,\Ac, H, P, r)$, where $\Sc$ can be an arbitrarily large state space; $\Ac$ is a finite action space with cardinality $K$; $H$ is the number of steps in each episode; $P: \Sc \times \Ac \times \Sc\rightarrow [0,1]$ is the time-dependent transition kernel, where $P_h(s_{h+1}|s_h,a_h)$ denotes the transition probability from the state-action pair $(s_h,a_h)$ at step $h$ to state $s_{h+1}$ in the next step; $r_h: \Sc \times \Ac \rightarrow [0,1]$ denotes the deterministic
	%\footnote{We study deterministic reward function for simplicity, but our results can easily generalize to randomized reward function.} 
	reward function  at step $h$; We further normalize the summation of reward function as $\sum_{h}^H r_h\leq 1$.
	%$d_0: \Sc \rightarrow [0,1]$ denotes the initial distribution. To simplify the presentation, we assume $d_0$ is known\footnote{to be decided}.
	A policy $\pi$ is a set of mappings $\{\pi_h : \Sc \rightarrow \Delta(\Ac)\}_{h\in[H]}$, where $\Delta(\Ac)$ is the set of all probability distributions over the action space $\Ac$. 
	% In particular, $\pi_h(a|s)$ denotes the probability of selecting action $a$ in state $s$ at time step $h$. 
	Further, $a \sim \Uc(\Ac)$ indicates the uniform selection of an action $a$ from $\Ac$.
	
	In each episode of the MDP, we assume that a fixed initial state $s_1$ is drawn. Then, at each step $h \in [H]$. the agent observes state $s_h 
	\in \Sc$, takes an action $a_h \in \Ac$ under a policy $\pi_h$, and receives a reward $r_h(s_h,a_h)$ (in the reward-known setting), and then the system transits to the next state $s_{h+1}$ with probability $P_h(s_{h+1}|s_h,a_h)$. The episode ends after $H$ steps.
	%, and no reward is received at step $H+1$.
	
	As standard in the literature, we use $s_h \sim (P, \pi)$ to denote a state sampled by executing the policy $\pi$ under the transition kernel $P$ for $h-1$ steps. If the previous state-action pair $(s_{h-1},a_{h-1})$ is given, we use $s_h\sim P$ to denote that $s_h$ follows the distribution $P_h(\cdot|s_{h-1},a_{h-1})$. We use the notation $\Eb_{(s_h, a_h) \sim (P, \pi)}\left[\cdot\right]$ to denote the expectation over states $s_h \sim (P,\pi)$ and actions $a_h \sim \pi$. 
	
	For a given policy $\pi$ and an MDP $\mathcal{M} = (\Sc,\Ac, H, P, r)$, we  denote the value function starting from state $s_h$ at step $h$ as $V^\pi_{h,P,r}(s_h): = \Eb_{(s_{h^\prime},a_{h^\prime}) \sim (P,\pi)}\left[\sum_{h^\prime=h}^{H}r_{h^\prime}(s_{h^\prime},a_{h^\prime})|s_h\right].$
	We use $V^\pi_{P,r}$ to denote $V^\pi_{1,P,r}(s_1)$ for simplicity.
	Similarly, we denote the action-value function starting from state action pair $(s_h,a_h)$ at step $h$ as  $Q^\pi_{h,P,r}(s_h,a_h):= r_h(s_h,a_h)+ \Eb_{s_{h+1} \sim P}\left[V^{\pi}_{h+1,P,r}(s_{h+1})|s_h,a_h\right].$
	
	We use $P^\star$ to denote the transition kernel of the true environment and for simplicity, denote $\Eb_{(s_h,a_h) \sim (P^\star,\pi)}[\cdot]$ as $\Eb_{\pi}^\star[\cdot]$. Given a reward function $r$, there always exists an optimal policy $\pi^\star$ that yields the optimal value $V^\star_{P^\star,r}=\sup_{\pi} V_{P^\star,r}^{\pi}$. 
	
	%Finally, for simplicity, we use $[H]$ to denote$\{1,\dots,H-1\}$. $a \sim \Uc(\Ac)$ denotes to uniformly select action  $a$ from $\Ac$. 
	
\vspace{-0.05in}	
	\subsection{Low-rank MDPs}
 \vspace{-0.05in}
	This paper focuses on the low-rank MDPs \citep{NEURIPS2020_e894d787} defined as follows.
	\begin{definition}\label{definition: Low_rank}
		{\rm (Low-rank MDPs).} A transition probability $P^\star_h: \Sc \times \Ac \rightarrow \Delta(\Ac)$ admits a low-rank decomposition with dimension $d \in \mathbb{N}$ if there exists two embedding functions $\phi^\star_h: \Sc \times \Ac \rightarrow \Rb^d$ and $\mu^\star_h: \Sc \rightarrow \Rb^d$ such that $P^\star_h(s^\prime|s,a)=\left<\phi^\star_h(s,a),\mu^\star_h(s^\prime)\right>, \forall s, s^\prime \in \Sc, a \in \Ac.$
		
		
		For normalization, we assume $\|\phi^\star_h(s,a)\|_2\leq 1$ for all $(s,a)$, and for any function $g: \Sc \rightarrow [0,1], \|\int \mu^\star_h(s)g(s)ds \|_2\leq\sqrt{d}$. An MDP $\Mc$ is a low-rank MDP with dimension $d$ if for each $h \in [H]$, $P_h$ admits a low-rank decomposition with dimension $d$. We use $\phi^\star=\{\phi^\star_h\}_{h \in [H]}$ and $\mu^\star=\{\mu^\star_h\}_{h \in [H]}$ to denote the embeddings for $P^\star$. 
	\end{definition}
	
	
	We remark that when $\phi_h$ is revealed to the agent, low-rank MDPs specialize to linear MDPs \citep{wang2020reward, jin2020provably}. Essentially, low-rank MDPs do not assume that the features $\{\phi_h\}_h$ are known a priori. The lack of knowledge on features in fact invokes a nonlinear structure, which makes the model strictly harder than linear MDPs or tabular models. Since it is impossible to learn a model in polynomial time if there is no assumption on features $\phi_h$ and $\mu_h$, we adopt the following conventional assumption from the recent studies on low-rank MDPs.
	
	
	%\textbf{Function approximation for representation learning.}
	%It is impossible to learn a model in polynomial time in the absence of assumption on features $\phi_h$ and $\mu_h$. We adopt a common assumption of studying low-rank MDPs in recent literature.
	
	\begin{assumption}\label{assumption: realizability}(Realizability).
		A learning agent can access to a model class $ \{(\Phi,\Psi)\}$ that contains the true model, i.e., the embeddings  $\phi^\star \in \Phi, \mu^\star \in \Psi$. 
	\end{assumption}
	
	%Since in our setting, state spaces $\Sc$ can be arbitrarily large, function approximation is needed to generalize. Here we adopt the standard function approximation in FLAMBE\cite{NEURIPS2020_e894d787}, in which the agent has access to two function classes $\Phi \subset \Sc \times \Ac \rightarrow \Rb^d$ and $\Psi \subset \Sc \rightarrow \Rb^d$.
	
	While we assume the cardinality of the function classes to be finite for simplicity, extensions to infinite classes with bounded statistical complexity (such as bounded covering number) are not difficult \citep{ sun2019model,NEURIPS2020_e894d787}. 
	
	%We present the following standard oracle as a computational abstraction, which is commonly adopted in the literature \citep{NEURIPS2020_e894d787, DBLP:conf/iclr/UeharaZS22}. 
\vspace{-0.05in}	
	\subsection{Reward-free RL and Learning Objectives}
 \vspace{-0.05in}
	%In reinforcement learning literature, there are several learning goals related to reward free reinforcement learning. We state them as follows formally. 
	
	%We introduce the reward-free RL setting as follows.
	
	Reward-free RL typically has two phases: exploration and planning. In the {\em exploration} phase, an agent explores the state space via interaction with the true environment and can collect samples over multiple episodes, but without access to the reward information. In the {\em planning} phase, the agent is no longer allowed to interact with the environment, and for any given reward function, is required to achieve certain learning goals (elaborated below) based on the outcome of the exploration phase.
	
	%\textbf{Reward-free Exploration}. In the reward free setting, the exploration and planning process can be totally separated into two parts. hence the algorithm can be divided into two phases: exploration phase and planning phase. In the exploration phase, we are supposed to design algorithms that The exploration phase requires an agent to efficiently explore the state space without the reward information. During exploration phase, the exploration algorithm interacts with the true environment, collects $N$ episodes and may use these episodes to learn model such as transition kernel $\hP$ and exploration bonus $\hb$, which is the outcome of exploration phase.
	
	%In the planning phase, the algorithm is no longer allowed to interact with environment. The planning algorithm is given an arbitrary reward function and find a near optimal policy based on the outcome of exploration phase.
	
	The planning phase may require the agent to achieve different learning goals. In this paper, we focus on three of such goals. The most popular goal in reward-free RL is to find a near-optimal policy that achieves the best value function under the true environment with $\epsilon$-accuracy, as defined below.
	%which we call it \textit{best policy} for simplicity and formalize it below}. 
\begin{definition}\label{definition: best policy}($\epsilon$-optimal policy). Fix $\epsilon > 0$. For any given reward function $r$, a learned policy $\pi$ is $\epsilon$-optimal if it satisfies
		$V^{\pi^\star}_{\Ps,r}-V^{\pi}_{\Ps,r} \leq \epsilon$.
\end{definition}

%\textbf{System Identification.} There is a stronger learning goal purposed in FLAMBE\cite{NEURIPS2020_e894d787} called \textit{system identification}. Given a model class $\Mc = {(\Phi,\Psi})$, the algorithm should learn a model $\hat{M} := (\hat{\phi},\hmu)$ that uniformly approximate the true environment $\Mc^\star$, which can beternmalized with the following performance criteria:
For model-based learning, \citet{NEURIPS2020_e894d787} proposed  \textit{system identification} as another useful learning goal, defined as follows. 
\begin{definition}[$\epsilon$-accurate system identification]\label{definition: system identification} Fix $\epsilon > 0$. Given a model class $ {(\Phi,\Psi})$, a learned model $ (\hat{\phi},\hmu)$ is said to achieve $\epsilon$-accurate system identification if it uniformly approximates the true model $P^\star$, i.e., $\forall\pi, h \in \left[H\right]$, $\Eb^\star_{\pi}\left[\left\|\left<\hphi_h(s_h,a_h),\hmu_h(\cdot)\right>-P_h^\star(\cdot|s_h,a_h)\right\|_{TV}\right] \leq \epsilon$.
	
\end{definition}
\vspace{-0.1in}


{Besides those two common learning goals, we also propose an additional goal on near-accurate representation learning. Towards that,}
we introduce the following divergence-based metric to quantify distance between two representations, which has been used in supervised learning~\citep{DBLP:conf/iclr/DuHKLL21}.
\begin{definition}(Divergence between two representations). \label{def:divergence}
Given a distribution $q$ over $\Sc \times \Ac$
	and two representations $\phi,\phi^\prime \in \Phi$, define the covariance between $\phi$ and $\phi^\prime$ w.r.t $q$ as $\Sigma_{(s,a) \sim q}(\phi,\phi^\prime)=\Eb\left[\phi(s,a)\phi^\prime(s,a)^\top\right]$. Then, the divergence between $\phi$ and $\phi^\prime$ with respect to $q$ is defined as
	\begin{align*}
		& D_q(\phi,\phi^\prime)= \Sigma_{(s,a) \sim q}(\phi^\prime,\phi^\prime) -
		\Sigma_{(s,a) \sim q}(\phi^\prime,\phi)
		\left(\Sigma_{(s,a) \sim q}(\phi,\phi)\right)^\dagger
		\Sigma_{(s,a) \sim q}(\phi,\phi^\prime).
	\end{align*}
\end{definition}
It can be verified that $D_q(\phi,\phi^\prime) \succeq 0$ (i.e., positive semidefinite) and $D_q(\phi,\phi) = 0$ for any $\phi,\phi^\prime \in \Phi$.

% \jingc{Shall we formally define $\epsilon$-accurate representation here?}
%\yl{shall we also define $\epsilon$-accurate representation here based on divergence?}

\section{Lower Bound on Sample Complexity}

In this section, we provide a lower bound on the sample complexity that all reward-free RL algorithms must satisfy under low-rank MDPs. The detailed proof can be found in \Cref{sec:lower_bound}. 
\begin{theorem}[Lower bound]\label{th:lowerbound}
	For any algorithm that can output an $\epsilon$-optimal policy (as \Cref{definition: best policy}), if $H>\max(24\epsilon,4),S\geq6,K\geq 3$ and $\delta<1/16$, then there exists a low-rank MDP model ${\Mc}$ such that the number of trajectories sampled by the algorithm is at least $\Omega\left(\frac{HdK}{\epsilon^2}\right)$.
\end{theorem}
To the best of our knowledge, \Cref{th:lowerbound} establishes the first lower bound for learning low-rank MDPs {in the reward-free setting}. More importantly, \Cref{th:lowerbound} shows that it is strictly more costly in terms of sample complexity to find near-optimal policies under {\em low-rank} MDPs (which have unknown representations) than {\em linear} MDPs (which have known representations) by at least a factor of $\Omega(K)$. This can be seen as the lower bound in \Cref{th:lowerbound} for low-rank MDPs has an additional term $K$ compared to the upper bound $\tilde{O}\left(\frac{d^3H^4}{\epsilon^2}\right)$ provided in \citet{wang2020reward} for linear MDPs. This can be explained intuitively as follows. In linear models, all the representations $\phi: \Sc\times \Ac \rightarrow \mathbb{R}^d$ are known. Then, it requires at most $O(d)$ actions with linearly independent features to realize all transitions $\langle\phi,\mu\rangle$. However, learning low-rank MDPs requires the agent to further select $O(K)$ actions to access the unknown features, leading to a dependence on $K$. 

Our proof of the new lower bound mainly features the following two novel ingredients in the construction of hard MDP instances. a) We divide the actions into two types. The first type of actions is mainly used to form a large state space through a tree structure. The second type of actions is mainly used to distinguish different MDPs. Such a construction allows us to separately treat the state space and the action space, so that both state space and action space can be arbitrarily large. b) We explicitly define the feature vectors for all state-action pairs, and more importantly, the dimension is less than or equal to the number of states. {These two ingredients together guarantee that the number of actions $K$ can be arbitrarily large and independent with other parameters $d$ and $S$, which indicates that the dependence on the number of actions $K$ is unavoidable.  }%Thus, there is no constraint on the relationship between the dimension $d$ and the number of actions $K$,  the dependence of the number of actions is unavoidable. 


%\Cref{th:lowerbound} also shows that reward-free RL under {\em low-rank} MDPs is strictly harder than {\em linear} MDPs by at least a factor of $\Omega(K)$, because the lower bound in \Cref{th:lowerbound} for low-rank MDPs has an additional term $K$ compared to the upper bound $\tilde{O}\left(\frac{d^3H^4}{\epsilon^2}\right)$ provided in \citet{wang2020reward} for linear MDPs. This can be explained intuitively as follows. In linear models, all the representations $\phi: \Sc\times \Ac \rightarrow \mathbb{R}^d$ are known. Then it requires at most $O(d)$ actions with linearly independent features to realize all transitions $\langle\phi,\mu\rangle$. However, learning low-rank MDPs requires the agent to further select $O(K)$ actions to access to unknown features, leading to a dependence on $K$. 

\section{The RAFFLE Algorithm}\label{sec:algorithm}
%\vspace{-0.12in}

%\vspace{-0.022in}

In this section, we propose RAFFLE (see \Cref{Algorithm: RAFFLE}) for reward-free RL under low-rank MDPs. 

{\bf Summary of design novelty:} The central idea of RAFFLE lies in the construction of a novel exploration-driven reward, which is desirable because its corresponding value function serves as an upper bound on the model estimation error during the exploration phase. Hence, such a pseudo-reward encourages the exploration to collect samples over those state-action space where the model estimation error is large so that later stage of the algorithm can further reduce such an error based on those samples. Such reward construction are new for low-rank MDPs, and serve as the key enabler for our improved sample complexity. They also necessitate various new ingredients in other steps of algorithms, as elaborated below.  



%In the exploration phase, the agent samples $H$ trajectories within each episode. All trajectories are under the guidance of a single exploration policy, except that we uniform randomly select two consecutive actions $a_{h-1}$ and $a_h$ for the $h$-th trajectory.  We then build estimator of transition $\hP^{(n)}=(\hphi^{(n)},\hmu^{(n)})$ iteratively. Also it computes the covariance matrix $\hat{U}_h^{(n)}$ and bonus term $\hat{b}^{(n)}$, which controls the difference between learned transition and the true transition. In the planning phase, given the reward function $r$, the algorithm finds the optimal policy under the learned transition $\hP$ with corresponding bonus-enhanced reward function $r+\hb$. 

%For the exploration phase, in general, without the reward functions, the algorithm designs an exploration-driven reward function $\hb^{(n)}$ that guides the agent to explore the unknown state space under the learned transition $\hP^{(n)}$. In details, in each $n$-th iteration, the algorithm first samples new data by using exploration policy computed in last iteration and add these data into dataset $\Dc^n$. Next RAFFLE calls MLE oracle to estimate transition $\hP^{(n)}$ and updates empirical covariance matrix $\hat{U}^{(n)}$ and exploration-driven reward function as $\hat{b}_h^{(n)}(s,a) = \alpha_n_n\|\hphi_h^{(n)}(s,a)\|_{(\hat{U}_h^{(n)})^{-1}}$. Finally RAFFLE computer $\pi_n$: the optimal policy of $V^\pi_{\hP^{(n)},\hb^{(n)}}$ as the exploration policy of next iteration. Intuitively, $\hat{b}_h^{(n)}(s,a)$ can upper bound the difference between learned transition and true transition. Formally, for any $\pi, h$ we have $\Eb^\star_{\pi}\left[\|\left<\hphi^{(n)}_h(s_h,a_h),\hmu^{(n)}_h(\cdot)\right>-P_h^\star(\cdot|s_h,a_h)\|_{TV}\right]\leq c V^\pi_{\hP^{(n)},\hb^{(n)}}$. hence, to achieve system identification goal, it suffices to find the optimal policy of $V^\pi_{\hP^{(n)},\hb^{(n)}}$. We remark here $\hat{b}_h^{(n)}(s,a)$ can not represent the point-wise uncertainty level of state-action pair $(s,a)$ as many other UCB-form algorithm. Finally, the exploration return an learned transition $\hP$ and bonus term $\hb$ that enables $\arg \max_{\pi} V_{\hP,\hb}^{\pi}$ to be small enough. 

%At each iteration $n$ in the exploration phase, RAFFLE designs an exploration-driven reward function $\hb^{(n)}$ which aids  exploration of the state space under the learned transition $\hP^{(n)}$. 


\textbf{Exploration and MLE model estimation.} In each iteration $n$ during the exploration phase, for each $h\in[H]$, the agent executes the exploration policy $\pi_{n-1}$ (defined in the previous iteration) up to step $h-1$, after which it takes two uniformly selected actions, and stops after step $h+1$. 
%Such a termination step enables our algorithm to identify an accurate model and later find a near optimal policy with fewer sample collections in the exploration phase than FLAMBE in \citet{NEURIPS2020_e894d787}. 
Different from FLAMBE \citep{NEURIPS2020_e894d787} that collects a large number of samples for each episode, our algorithm uses each exploration policy to collect only one sample trajectory at each episode, indexed by $(n,h)$. %Such a more sample-efficient design is reflected by the fact that
Hence, the sample complexity of RAFFLE is much smaller than that of FLAMBE. In fact, such an efficient sampling together with our new termination idea introduced later benefit sample complexity.
%that enables our algorithm to identify an accurate model and later find a near optimal policy with fewer sample collections. 
%in the exploration phase than FLAMBE in \citet{NEURIPS2020_e894d787}. 



%Note that $\pi_{n-1}$ is designed at the previous iteration.  
Then, the agent estimates the low-rank components $\hphi_h^{(n)}$ and $\hmu_h^{(n)}$ via the MLE oracle  
with given model class $(\Phi, \Psi)$ and a dataset $\Dc_h^n$ as follows 
\begin{align*}
	&\textstyle\mathrm{MLE} (\Dc_h^n) : = \arg\max_{\phi \in \Phi, \mu \in \Psi}\sum_{(s, a, s^\prime)\in\Dc_h^n} {\rm log}\left<\phi_h(s,a),\mu_h(s^\prime)\right>.
\end{align*}
% The agent then updates the estimated transition kernel with $\hphi_h^{(n)}$ and $\hmu_h^{(n)}$.
\textbf{Design of exploration reward.} 
%With estimated $\hphi_h^{(n)}$, 
The agent updates the empirical covariance matrix $\hat{U}_h^{(n)}$ as
\begin{align}
\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}
	&\textstyle\hat{U}_h^{(n)} = \lambda_n I +  \sum_{\tau=1}^{n} {\hphi}_h^{(n)}(s_h^{(\tau,h+1)},a_h^{(\tau,h+1)})  (\hphi_h^{(n)}(s_h^{(\tau,h+1)},a_h^{(\tau,h+1)}))^{\top} , \label{eq:u}
\end{align}
where 
$\{s_h^{(\tau,h+1)},a_h^{(\tau,h+1)}, s_{h+1}^{(\tau,h+1)}\}$ is collected at iteration $\tau$, episode $(h+1)$, and step $h$.

Next, the agent uses both $\hphi_h^{(n)}$ and $\hat{U}_h^{(n)}$ to construct an {\em exploration-driven reward} function as 
\begin{equation}
\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}
	\textstyle\hat{b}_h^{(n)}(s,a) = \min\{\hat{\alpha}_n\|\hphi_h^{(n)}(s,a)\|_{(\hat{U}_h^{(n)})^{-1}},1\},\label{eq:bn}
\end{equation}
where $\hat{\alpha}_n$ is a pre-determined parameter. We note that although individual $\hat{b}_h^{(n)}(s,a)$ for each step may not represent point-wise uncertainty as indicated in \citet{DBLP:conf/iclr/UeharaZS22}, we find its total cumulative version $\hV_{\hP^{(n)},\hat{b}^{(n)}}^{\pi}$ can serve as a \textit{trajectory-wise} uncertainty measure to select exploration policy. 
%Using $\hat{b}_h^{(n)}(s,a)$ as an exploration reward in the exploration phase follows from a new insight that the value function $\hV_{\hP^{(n)},\hat{b}^{(n)}}^{\pi_n}$ can serve as a desirable metric of uncertainty quantification for the learned model $\hP^{(n)}$. 
To see this, it can be shown that for any $\pi$ and $h$, $\Eb^\star_{\pi}\left[\left\|\left<\hphi^{(n)}_h(s_h,a_h),\hmu^{(n)}_h(\cdot)\right>-P_h^\star(\cdot|s_h,a_h)\right\|_{TV}\right]\leq c^\prime \hV^{\pi_n}_{\hP^{(n)},\hb^{(n)}}+\sqrt{\frac{c_n}{n}}$, where $c^\prime$ is a constant and $c_n = O(\log n)$. As iteration number $n$ grows, the second term diminishes to zero, which indicates that $\hV_{\hP^{(n)},\hat{b}^{(n)}}^{\pi_n}$ (under the reward of $\hat{b}^{(n)}$) serves as a good upper bound on the estimation error for the true transition kernel. Hence, exploration guided by maximizing $\hV_{\hP^{(n)},\hat{b}^{(n)}}^{\pi}$ will collect more trajectories over which the learned transition kernels are not estimated well. This will help to reduce the model estimation error in the future. 


\begin{algorithm}[H]
	\caption{{\textbf {RAFFLE} (\textbf{R}ew\textbf{A}rd-\textbf{F}ree \textbf{F}eature \textbf{LE}arning)} \label{Algorithm: RAFFLE}}
	
	\begin{algorithmic}[1]
		
		
		\STATE {\bfseries Input:} {$\hat{\alpha}_n$, $\zeta_n$, $\epsilon > 0$, $\delta \in (0,1)$,  regularizer $\lambda_n$, model classes $ \{(\mu,\phi): \mu \in \Psi, \phi \in \Phi \}$}.
		
		\STATE Initialize $\pi_0(\cdot|s)$ to be uniform; set $\mathcal{D}_h^0=\emptyset$.\;
		
		\STATE \textbf{Phase \uppercase\expandafter{\romannumeral1}: Exploration Phase}
		
		\FOR{$n=1,\ldots$}
		
		\FOR{$h=1,\ldots, H$}
		
		\STATE Use $\pi_{n-1}$: roll into $s_{h-1}$, uniformly choose $a_{h-1},a_{h}$, enter into $s_{h},s_{h+1}$.
		
		\STATE Collect data {\small $s_1^{(n,h)}, a_1^{(n,h)},\ldots, s_h^{(n,h)}, a_h^{(n,h)}, s_{h+1}^{(n,h)}$.}
		%\[s_1^{(n,h)}, a_1^{(n,h)},\ldots, s_h^{(n,h)}, a_h^{(n,h)}, s_{h+1}^{(n,h)}.\]
		
		\STATE Add the triple $(s_h^{(n,h)}, a_h^{(n,h)}, s_{h+1}^{(n,h)})$ to the dataset $\Dc_h^n = \Dc_h^{n-1} \cup \{(s_h^{(n,h)}, a_h^{(n,h)}, s_{h+1}^{(n,h)})\}$.
		
		\STATE Learn $(\hphi_h^{(n)},\hmu_h^{(n)}) = \mbox{MLE}(\Dc_h^n)$.
		
		\STATE Update transition dynamics $\hP^{(n)}$ as $\hP_h^{(n)}(s'|s,a) = \langle \hphi_h^{(n)}(s,a), \hmu_h^{(n)}(s')\rangle$.
		%\[\hP_h^{(n)}(s'|s,a) = \langle \hphi_h^{(n)}(s,a), \hmu_h^{(n)}(s')\rangle \]
		
		\ENDFOR
		
		\STATE Update empirical covariance matrix $\hat{U}_h^{(n)}$ as in \Cref{eq:u}.
		
		%$\hat{U}_h^{(n)} = \sum_{\tau=1}^{n} {\hphi}_h^{(n)}(\hphi_h^{(n)})^{\top} + \lambda_n_n I$ using data $\{s_h^{(\tau,h+1)},a_h^{(\tau,h+1)}\}_{\tau\in[n]}$:
		%\begin{align*}
		%    \hat{U}_h^{(n)} =& \textstyle \sum_{\tau=1}^{n} {\hphi}_h^{(n)}(s_h^{(\tau,h+1)},a_h^{(\tau,h+1)})\\
		%   &\textstyle (\hphi_h^{(n)}(s_h^{(\tau,h+1)},a_h^{(\tau,h+1)}))^{\top} + \lambda_n_n I.
		%\end{align*}
		
		
		\STATE Define exploration-driven reward function $\hat{b}_h^{(n)}$ as in \Cref{eq:bn}.
		%$\hat{b}_h^{(n)}(\cdot,\cdot) = \min\{\tilde{\alpha_n}_n\|\hphi_h^{(n)}(\cdot,\cdot)\|_{(\hat{U}_h^{(n)})^{-1}},1\}$. \yl{should there be a clipping here?}
		\STATE Define an estimated value function $\hV_{{\hP}^{(n)}, \hat{b}^{(n)}}^{\pi}$ based on ${\hP}^{(n)}$ and $\hat{b}^{(n)}$ as in \Cref{ineq: def of hV}.  
		\STATE Find exploration policy $\pi_n = \arg\max_{\pi} \hV_{{\hP}^{(n)}, \hat{b}^{(n)}}^{\pi}$.
		
		\IF{$2\hat{V}_{{\hP}^{(n)}, \hat{b}^{(n)}}^{\pi_n} + 2\sqrt{K\zeta_n}\leq \epsilon$}
		%$n \geq \frac{9K\log(\frac{9|\Phi||\Psi|HK}{\epsilon^2\delta})}{\epsilon^2}$
		
		\STATE Terminate \textbf{Phase \uppercase\expandafter{\romannumeral1}: Exploration Phase} and set $\hP^{\epsilon} = {\hP}^{(n)}, \hb^{\epsilon} =  \hat{b}^{(n)}, \pi_{\epsilon}= \pi_n, n_\epsilon = n$.
		
		\ENDIF
		
		\ENDFOR
		
		\STATE \textbf{Phase \uppercase\expandafter{\romannumeral2}: Planning  Phase}
		
		\STATE \textbf{Option 1} (learn near-optimal policy): Receive reward function $r=\{r_h\}_{h=1}^{H}$, and compute policy $\bar{\pi}=\arg\max_{\pi} V_{{\hP^{\epsilon}}, r}^{\pi}$.

           \STATE \textbf{Option 2} (system identification): let $\hat{P}=\hP^{\epsilon}$.

           \STATE \textbf{Option 3} (learn near-accurate representation): call \Cref{alg: feature} of RepLearn and obtain $\tphi$.
		
		%\STATE Compute policy $\bar{\pi}=\arg\max_{\pi} V_{{\hP^{\epsilon}}, r}^{\pi}$.
		
	\STATE	\textbf{Output:} policy $\bar{\pi}$, learned transition dynamics $\hP^{\epsilon}$, learned representation $\tphi$.
		
	\end{algorithmic}
\end{algorithm}

\textbf{Design of exploration policy.} The agent defines a {\em truncated value function} iteratively using the estimated transition kernel and the exploration-driven reward as follows:
\begin{align}
	\textstyle&\hat{Q}_{h,\hP^{(n)},\hat{b}^{(n)}}^{\pi}(s_h,a_h) = \min\left\{1, \hat{b}^{(n)}_h(s_h,a_h) + \hP_h^{(n)}\hV_{h+1,\hP^{(n)},\hat{b}^{(n)}}^{\pi}(s_h,a_h)\right\},\nonumber\\
	&\hat{V}_{h,\hP^{(n)},\hat{b}^{(n)}}^{\pi}(s_h) =  \mathop{\Eb}_{\pi}\left[\hat{Q}^{\pi}_{h,\hP^{(n)},\hat{b}^{(n)}}(s_h,a_h)\right]\label{ineq: def of hV}.
\end{align}
The truncation technique here is important for the improvement of the sample complexity on the dependence of $H$.
%
%When $h=1$, the index of $h$ is omitted. 
The agent finally finds an optimal policy maximizing $\hV^\pi_{\hP^{(n)},\hb^{(n)}}$, and uses this policy as the exploration policy for the next iteration.

%\textcolor{green}{explain termination condition}
\textbf{Novel termination criterion.} RAFFLE does not require a pre-determined maximum number of iterations as its input. Instead, it will \textbf{terminate} and output the current estimated model if the optimal value function $\hV^\pi_{\hP^{(n)},\hb^{(n)}}$ plus a minor term is below a threshold. Such a termination criterion essentially guarantees that the value functions under the estimated and true models are close to each other under any reward and policy, hence the exploration can be terminated in finite steps. Such a termination criterion enables our algorithm to identify an accurate model and later find a near-optimal policy with fewer sample collections than FLAMBE in \citet{NEURIPS2020_e894d787} as we discuss in the exploration phase. Additionally, our termination criterion provides strong performance guarantees on the output policy and estimator from the last iteration. On the contrary, \citet{DBLP:conf/iclr/UeharaZS22} can only provide guarantees on a random mixture of the policies obtained over all iterations.  
%To be more specific, our algorithm uses each exploration policy to collect only one sample trajectory at each episode, whereas FLAMBE collects a large number of samples for each of the episodes. %Such a more sample-efficient design is reflected by the fact that Consequently, the sample complexity of RAFFLE is much smaller than that of FLAMBE. \yl{should the last sentence be moved to Exploration step?}

%Loosely speaking, the optimal value function controls the difference in the value functions under the true model and the estimated model for any policy and reward. Therefore, the termination condition suffices to guarantee the proximity of the returned model $\hP^{\epsilon}$ and the true model.

\textbf{Planning phase.} Given any reward function $r$, the agent finds a near-optimal policy by planning with the learned transition dynamics $\hP^\epsilon$ and the given reward $r$. Note that such planning with a known low-rank MDP is computationally efficient by assumption. 

\begin{comment}
	We next explain the \textbf{core ideas} underpinning our design as follows.
	
	$\RM{1}$ Our idea of using $\hat{b}_h^{(n)}(s,a)$ as an exploration reward in the exploration phase follows from the insight that the value function $\hV_{\hP^{(n)},\hat{b}^{(n)}}^{\pi_n}$ can serve as a desired metric of uncertainty quantification for the learned model $\hP^{(n)}$. To see this, it can be shown that for any $\pi$ and $h$, $\Eb^\star_{\pi}\left[\left\|\left<\hphi^{(n)}_h(s_h,a_h),\hmu^{(n)}_h(\cdot)\right>-P_h^\star(\cdot|s_h,a_h)\right\|_{TV}\right]\leq c^\prime \hV^{\pi_n}_{\hP^{(n)},\hb^{(n)}}+\sqrt{\frac{c_n}{n}}$, where $c^\prime$ is a constant and $c_n = O(\log n)$. As iteration number $n$ grows, the second term diminishes to zero, which indicates that $\hV_{\hP^{(n)},\hat{b}^{(n)}}^{\pi_n}$ (under the reward of $\hat{b}^{(n)}$) serves as a good upper bound on the estimation error for the true transition kernel. Hence, exploration guided by maximizing $\hV_{\hP^{(n)},\hat{b}^{(n)}}^{\pi}$ will collect more trajectories over which the learned transition kernels are not estimated well. This will help to reduce the error in the future. We stress here although \cite{DBLP:conf/iclr/UeharaZS22} claims that individual $\hat{b}_h^{(n)}(s,a)$ for each step cannot represent point-wise uncertainty, we find its total cumulative version $\hV_{\hP^{(n)},\hat{b}^{(n)}}^{\pi}$ can serve a trajectory-wise uncertainty.  
	
	$\RM{2}$ We use a truncation technique on constructing model-based estimated value function here, which brings a $H^2$ improvement on sample complexity.
	
	$\RM{3}$ Our algorithm also features a termination step towards the end of the exploration phase, which has not been used in other reward-free algorithms such as FLAMBE in \citet{NEURIPS2020_e894d787}. Such a termination condition essentially guarantees that the value functions under the estimated and true models are close to each other under any reward and policy, and hence the exploration can be terminated. Such a termination step enables our algorithm to identify an accurate model and later find a near optimal policy with fewer sample collections in the exploration phase than FLAMBE. To be more specific, our algorithm uses each exploration policy to collect only one sample trajectory at each episode, whereas FLAMBE collects a large number of samples for each of the episodes. %Such a more sample-efficient design is reflected by the fact that
	Consequently, the sample complexity of RAFFLE is much smaller than that of FLAMBE.
\end{comment}
%\yl{comment on design differences from Flambe}

%\yl{comment on the difference from reward-free linear MDP}

\vspace{-0.1in}
\section{Upper Bounds on Sample Complexity}\label{sec:upperbound}
\vspace{-0.03in}
In this section, we first show that the policy returned by RAFFLE  is an $\epsilon$-optimal policy with respect to any given reward $r$ in the planning phase. The detailed proof can be found in \Cref{sec: A}.
% The following theorem establishes the sample complexity upper bound.

\begin{theorem}[$\epsilon$-optimal policy] \label{thm1: reward-free sample complexity}
	Assume $\Mc$ is a low-rank MDP with dimension $d$, and Assumption \ref{assumption: realizability} holds. Given any $\epsilon,\delta \in (0,1)$, and any reward function $r$, let $\bar{\pi}$ and $\hP^{\epsilon}$ be the output of RAFFLE %\Cref{Algorithm: RAFFLE} 
	and $\pi^\star:= \arg\max_{\pi}V^\pi_{P^\star,r}$ be the optimal policy under the true model $P^\star$. Set  $\hat{\alpha}_n=\tilde{O}(\sqrt{K+d^2})$ and $\lambda_n=\tilde{O}(d)$. Then, with probability at least $1-\delta$, we have $V^{\pi^\star}_{P^\star,r}-V^{\bar{\pi}}_{P^\star,r}\leq \epsilon$, and the total number of trajectories collected by RAFFLE is upper bounded by
		$\tilde{O}(\frac{H^3d^2K(d^2+K)}{\epsilon^2})$.

\end{theorem}

We note that  the upper bound in \Cref{thm1: reward-free sample complexity}  matches the lower bound in \Cref{th:lowerbound} in terms of the dependence on $\epsilon$ as well as on $K$ {in the large $d$ regime}. 

Compared with MOFFLE \citep{modi2021model}, which also finds an $\epsilon$-optimal policy in reward-free RL, our result improves their sample complexity of $\tilde{O}\left(\frac{H^5K^5d^3_{LV}}{\epsilon^2\eta}\right)$ in three aspects. First, the order on $K$ is reduced. Second, the dimension $d_{LV}$ of the
underlying function class in MOFFLE can be exponentially larger than $d$ as shown in \citet{NEURIPS2020_e894d787}. Finally, MOFFLE requires reachability assumption, leading to a factor $1/\eta$ in the sample complexity, which can be as large as $\sqrt{d_{LV}}$. 
%Further, our result of reward-free RL is naturally applicable to reward-known RL, which improves the sample complexity of $\tilde{O}\left(\frac{H^5d^4K^2}{\epsilon^2}\right)$ there in~\citep{DBLP:conf/iclr/UeharaZS22} by a factor of $\mathcal O(H^2)$.
Further, \Cref{thm1: reward-free sample complexity} naturally achieves the goal of reward-known RL with the same sample complexity, which improves that of $\tilde{O}\left(\frac{H^5d^4K^2}{\epsilon^2}\right)$ in~\citet{DBLP:conf/iclr/UeharaZS22} by a factor of $O(H^2)$. 
%with sample complexity $\tilde{O}\left(\frac{H^5d^4K^2}{\epsilon^2}\right)$, our result improves the dependence on $H$. 

Proceeding to the learning objective of system identification, the learned transition kernel output by \Cref{Algorithm: RAFFLE} achieves the goal of $\epsilon$-accurate system identification with the same sample complexity as follows. The detailed proof can be found in \Cref{subsec:System_proof}.



\begin{theorem}[$\epsilon$-accurate system identification] \label{Theorem: system identification sample complexity}
	Under the same condition of \Cref{thm1: reward-free sample complexity} and let $\hP^{\epsilon} = \{\hphi_h^{\epsilon},\hmu_h^{\epsilon}\}$ be the output of RAFFLE.
	%the \Cref{Algorithm: RAFFLE}. 
	Then, with probability at least $1-\delta$, $\hP^{\epsilon}$ achieves $\epsilon$-accurate system identification, i.e. for any $\pi$ and $h$:
	$\Eb^\star_{\pi}\left[\left\|\left<\hphi^{\epsilon}_h(s_h,a_h),\hmu^{\epsilon}_h(\cdot)\right>-P_h^\star(\cdot|s_h,a_h)\right\|_{TV}\right] \leq \epsilon$,
	and the number of trajectories collected by RAFFLE is upper bounded by
		$\tilde{O}(\frac{H^3d^2K(d^2+K)}{\epsilon^2})$.
\end{theorem}
\Cref{Theorem: system identification sample complexity} 
%achieves an $\epsilon$-accurate system identification with the sample complexity of $\tilde{O}(\frac{H^3d^2K(d^2+K)}{\epsilon^2})$, which 
significantly improves the sample complexity of $\tilde{O}(\frac{H^{22}K^9d^7}{\epsilon^{10}})$ in \citet{NEURIPS2020_e894d787} on the dependence of all involved parameters for achieving $\epsilon$-accurate system identification.

%Compared with the existing work \citep{NEURIPS2020_e894d787}, our algorithm achieves an $\epsilon$-accurate system identification via reward-free exploration, with improved sample complexity of $\tilde{O}\left(\frac{H^3d^2K(d^2+K)}{\epsilon^2}\right)$.




%From a high level of idea, we treat the objective function in system identification, i.e. the LHS of \Cref{eqn:Theorem_system_Id_guarantee}, as a value function  under the true environment $P^\star$ and any policy $\pi$ with reward defined only on the step $h$, i.e. $r_{h'}(s,a) = \mathbf{1}_{\{h'=h\}}f_h^{(n)}(s,a)$. In addition, with the same policy and reward,  the difference of value functions under two models $P^\star$ and $\hP^{(n)}$ is controlled by  $f_{h'}^{(n)}(s,a)$, i.e. the total variation distance between two models. Formally, we have,




\iffalse
%given any policy $\pi$ and reward, the return under the estimated model $\hP^{(n)}$ is upper bounded by... To simplify the notation, let $f_h^{(n)}(s,a)=\|\hP_h^{(n)}(\cdot|s,a) - P^\star_h(\cdot|s,a)\|_{TV}$, which is the estimation error in the $n$-th iteration at the step $h$, given the state $s$ and the action $a$, in terms of the total variation distance. 

%Therefore, RAFFLE successfully improve the sample complexity of identifying an $\epsilon$-accurate system model from $\frac{H^{22}K^9d^7}{\epsilon^{10}}$ to $\frac{H^5K^2d^4}{\epsilon^2}$. While it is straightforward to realize the improvement, we highlight a key advancement over prior works. 

%When one estimates a transition kernel, or identifies an $\epsilon$-optimal policy in a model-based algorithm, it is plausible to make the algorithm "converge" in a sense that the estimated model in future episodes is more accurate than that in previous episodes. However, by leveraging the sub-linear summation, we are able to show that there exists a good estimation hidden in the history episodes. Thanks to the following lemmas, RAFFLE can locate the $\epsilon$-accurate system model.

\begin{proposition}\label{lemma: bound model error with V_hat}
	For any $\delta \in (0,1)$, with probability $1-\delta/2$, for all $h \in [H], n \in \mathbb{N}^+$, policy $\pi$,
	\begin{align}
		\Eb^\star_{\pi}[f_h^{(n)}(s_h,a_h)] \leq \sqrt{K\zeta_n}+2 V_{\hP^{(n)},b^{(n)}}^{\pi_n},\label{eqn:bound model error with V_hat}
	\end{align}
	where $\zeta_n : = \frac{\log(2|\Phi||\Psi|nH/\delta)}{n}$.
\end{proposition}
Here, we introduce the notation that $f_h^{(n)}(s,a)=\|\hP_h^{(n)}(\cdot|s,a) - P^\star_h(\cdot|s,a)\|_{TV}$ is the estimation error in the $n$-th iteration at the step $h$, given the state $s$ and the action $a$, in terms of the total variation distance.


\begin{lemma}\label{lemma: bound on summation of V_hat}(Informal)
	For any $\delta \in (0,1)$, with probability at least $1-\delta/2$, the summation of value functions $V^{\pi_n}_{\hP^{(n)},\hb^{(n)}}$ under exploration policies $\{\pi_n\}_{n\in[N]}$ with exploration-driven reward functions $\hb^{(n)}$ is sublinear, i.e.
	\begin{align}
		\sum_{n\in[N]}& V_{\hP^{(n)},\hb^{(n)}}^{\pi_n}  \leq
		\tilde{O}\left(H^2d\sqrt{K(K+d^2)N}\right).
	\end{align}
\end{lemma}
\fi 



%Now, it is straightforward to confirm the existence of $n$ such that $V_{\hP^{(n)},\hb^{(n)}}^{\pi_n} + \sqrt{c_n/n}\leq \epsilon/2 $ if $N$ is sufficiently large. Otherwise, the LHS of \Cref{eqn:bound_sum_V_hat} is at least $\epsilon N/2$, while the RHS of \Cref{eqn:bound_sum_V_hat} is at most $\tilde{O}(\sqrt{N})$, which is a contradiction. More importantly, as mentioned earlier, this value function $V_{\hP^{(n)},\hb^{(n)}}^{\pi_n}$ and constant $c_n$ are computable so that RAFFLE can utilize the stopping criterion $2V_{\hP^{(n)},\hb^{(n)}}^{\pi_n} + 2\sqrt{c_n/n}\leq \epsilon$ to return an $\epsilon$-accurate system model.







%has a similar guarantee of common regret upper bound, it ensures that this upper bound also holds for value functions under different models $\hP^{(n)}$ estimated by RAFFLE. This is not trivial since the algorithm should control the estimation performance on $\hP^{(n)}$ and the regret incurred by $\pi_n$ under the true environment simultaneously. 
%Combining \Cref{lemma: bound model error with V_hat} and \Cref{lemma: bound on summation of V_hat}, we are able to show that there exists an $n\in[N]$ such that $\Eb^\star_{\pi}[f_h^{(n)}(s_h,a_h)]\leq \epsilon$ when $N$ is sufficiently large, since the average of $\Eb^\star_{\pi}[f_h^{(n)}(s_h,a_h)]$ over $N$ episodes scales in the order of $\Tilde{O}(\sqrt{1/N})$. Moreover, all quantities in the RHS of \Cref{eqn:bound model error with V_hat} are computable. Therefore, as long as RAFFLE find that $V_{\hP^{(n)},\hb^{(n)}}^{\pi_n}$ is below $\epsilon/3$ when $n$ is reasonably large, the estimated model $\hP^{(n)}$ achieves system identification with $\epsilon$-accuracy. Finally, we remark that 
%the stopping condition $V_{\hP^{(n)},\hb^{(n)}}^{\pi_n}\leq \epsilon/3$ makes RAFFLE explores in a more \textcolor{red}{smart}(efficient) way, because it does not require the input of total episodes $N$, and can identify the desired model automatically.

%By ensuring an $\epsilon$-accurate system model, various downstream tasks in reinforcement learning can be tackled. For instance, we are able to estimate the true value function $V_{P^\star,r}^{\pi}$ under any reward functions and policies by examining the value function $V_{\hP,r}^{\pi}$ on the estimated model under the same reward and policy. It can be shown that the estimation error is at most $H\epsilon$. We remark that this property also appears in other reward-free literature~\cite{pmlr-v119-jin20d,NEURIPS2020_e894d787}. Similar to system identification, we term this as \textit{uniform convergence in value function}.


%With a little algebra, it can be shown that if a model $\hP$ can achieve $\epsilon$-uniform convergence in value function, then given any reward function $r$, the optimal policy with respect to $\hP^\epsilon$ and $r$ is $3\epsilon$-optimal with respect to the true environment. While it is an open question that whether $\epsilon$-accurate system can achieve $\epsilon-$uniform convergence in value function, simply applying \Cref{Theorem: system identification sample complexity} does not provide a good sample complexity for learning an $\epsilon$-optimal policy. It will introduce an additional $H^2$ factor in the sample complexity of identifying a near-optimal policy. However, RAFFLE can achieve the same sample complexity for learning an $\epsilon$-uniform convergence in value function, and therefore, for learning an $\epsilon-$optimal policy. The formal statement is in the following theorem.



\section{Near-Accurate Representation Learning}\label{sec: feature learning}

In low-rank MDPs, it is of great interest to learn the representation $\phi$ accurately, because other similar RL environments can very likely share the same representation~\citep{DBLP:journals/corr/RusuRDSKKPH16,DBLP:journals/corr/abs-2009-07888,DBLP:journals/neco/Dayan93a} and hence such learned representation can be directly reused in those environments. Thus, the third objective of RAFFLE in the planning phase is to provide an accurate estimation of $\phi$. We note that although RAFFLE provides an estimation of $\phi$ during its execution, such an estimation does not come with an accuracy guarantee. Besides, \Cref{Theorem: system identification sample complexity} on system identification does not provide the guarantee on the representation $\hat\phi$, but only on the entire transition kernel $\hat P$. Further, none of previous studies of reward-free RL under low-rank MDPs~\citep{NEURIPS2020_e894d787,modi2021model,DBLP:conf/iclr/UeharaZS22} established the guarantee on $\hat\phi$. 

%=================
\begin{comment}
We first introduce the following divergence-based metric to quantify the distance between two representations, which has been used in supervised learning~\citep{DBLP:conf/iclr/DuHKLL21}.
\begin{definition}(Divergence between two representations). Given a distribution $q$ over $\Sc \times \Ac$
	and two representations $\phi,\phi^\prime \in \Phi$, define the covariance between $\phi$ and $\phi^\prime$ w.r.t $q$ as $\Sigma_{(s,a) \sim q}(\phi,\phi^\prime)=\Eb\left[\phi(s,a)\phi^\prime(s,a)^\top\right]$. Then, the divergence between $\phi$ and $\phi^\prime$ with respect to $q$ is defined as
	\begin{align*}
		& D_q(\phi,\phi^\prime)= \Sigma_{(s,a) \sim q}(\phi^\prime,\phi^\prime)\\
		&\quad -
		\Sigma_{(s,a) \sim q}(\phi^\prime,\phi)
		\left(\Sigma_{(s,a) \sim q}(\phi,\phi)\right)^\dagger
		\Sigma_{(s,a) \sim q}(\phi,\phi^\prime).
	\end{align*}
\end{definition}
It can be verified that $D_q(\phi,\phi^\prime) \succeq 0$ (i.e., positive semidefinite) and $D_q(\phi,\phi) = 0$ for any $\phi,\phi^\prime \in \Phi$.
\end{comment}
%============

%In this section, we design an algorithm, which exploits the learned transition kernel from RAFFLE and learns an accurate representation without additional interaction with the environment.

\subsection{The RepLearn Algorithm}
{In this section, we present the following algorithm of RepLearn, which 
exploits the learned transition kernel from RAFFLE and learns a near-accurate representation without additional interaction with the environment. The formal version of RepLearn, \Cref{alg: feature}, is delayed in \Cref{sec: app feature learning}. We explain the main idea of \Cref{alg: feature} as follows.} First, for each $h \in [H], t\in [T]$, where $T$ is the number of rewards, $N_f$ pairs of state-action $(s_h,a_h)$ are generated based on distribution $q_h$.
% and the transition kernel $\hP$ given by RAFFLE. 
Note that the agent does not interact with the true environment during such data generation. Then, for any $h$, if we set the reward $r$ at step $h$ to be zero, $Q^{\pi}_{P^{\star},h, r}(s_h,a_h)$ can have a linear structure in terms of the true representation $\sphi_h(s_h,a_h)$. Namely, there exists a $w_h$ decided by $r$, $\Ps$ and $\pi$ such that $Q^{\pi}_{P^{\star},h, r}(s_h,a_h)=\langle\sphi_h(s_h,a_h),w_h\rangle$. Then, with the estimated transition kernel $\hP$ that RAFFLE provides and the well-designed rewards $r^{h,t}$, $Q^{\pi^t}_{\hP,h, r^{h,t}}(s_h,a_h)$ can be computed efficiently and serve as a target to learn the representation via the following regression problem:
\begin{align}     \textstyle \mathop{\arg\min}_{\phi_h \in \Phi,w_h^t\in \Rb^d}\sum_{t\in[T]}\sum_{(s_h,a_h) \in \Dc_h^t} (Q^{\pi^t}_{\hP,h, r^{h,t}}(s_h,a_h)-\langle\phi_h(s_h,a_h),w_h^t\rangle)^2 \label{ineq: alg-linear regression}.
\end{align}

The main difference between our algorithm and that in \citet{lu2021power} for representation learning is that, our data generation is based on $\hP$ from RAFFLE, which carries a natural estimation error but requires no interaction with the environment, whereas their algorithm assumes a generative model to collect data from the ground-truth transition kernel.

%\yl{The algorithm description may need further elaboration on novel parts}

\subsection{Guarantee on Accuracy}

In order to guarantee the learned representation by \Cref{alg: feature} is sufficiently close to the ground truth, we need to employ the following two somewhat necessary assumptions.

%On one hand, if the input distribution $\{q_h\}_{h \in [H]}$ satisfy Assumption \ref{assump2}, then following form \Cref{Theorem: system identification sample complexity}, it shows that for any reward $r$, $Q^{\pi}_{\hP,h, r}(s_h,a_h)$ can approximate when $(s_h,a_h)$ subjects to certain distribution $\{q_h\}_{h=1}^{H}$. 

%\textbf{Assumptions:} 
%We then introduce several assumptions on the input of our algorithm that is necessary for learning a good representation. 

First, for the distributions of the state-action pairs $\{q_h\}_{h=1}^{H}$ in \Cref{alg: feature}, it is desirable to have $\hP(\cdot|s,a)$ approximates true $\Ps(\cdot|s,a)$ well over those distributions, so that $Q^{\pi}_{\hP,h, r}(s_h,a_h)$ can approximate the ground truth well. Intuitively, if some state-action pairs can hardly be visited under any policy, the output $\hP(\cdot|s,a)$ of \Cref{Algorithm: RAFFLE} cannot approximate true $\Ps(\cdot|s,a)$ well over these state-action pairs. Hence, we assume reachability type assumptions for MDPs as following so that all state-action pairs is likely to be visited by certain policy. Such reachability type assumption is common in relevant literature ~\citep{modi2021model,NEURIPS2020_e894d787}.  

As discussed in \Cref{sec: feature learning}, intuitively, if some state action pairs can be hardly visited by any policy, the output of \Cref{Algorithm: RAFFLE} $\hP(\cdot|s,a)$ can not approximate true $\Ps(\cdot|s,a)$ well over these state action pairs, so a standard reachability type assumption is necessary so that all states can be visited. 
\begin{assumption}[Reachability]\label{assumption: reachability}
	For the true transition kernel $P^{\star}$, there exists a policy $\pi^0$ such that $\min_{s\in\Sc} \Pb^{\pi^0}_h(s)\geq \eta_{\mathrm{min}}$, where $\Pb^{\pi^0}_h(\cdot): \Sc \rightarrow \Rb$ is the density function over $\Sc$ using policy $\pi^0$ to roll into state $s$ at timestep $h$.  
\end{assumption}

 We further assume the input distributions $\{q_h\}_{h=1}^{H}$ are bounded with constant $C_B$. Then, together with Assumption \ref{assumption: reachability}, for any $(s,a) \in \Sc\times\Ac$, we have  $q_h(s,a) \leq C_\mathrm{min}\Pb^{\pi^0}_h(s,a)$, where  $C_{\mathrm{min}}=\frac{C_B}{\eta_{\mathrm{min}}}$. 
%\end{remark}

% \begin{remark}
	%      Intuitively, type of Assumption \ref{assump2} is necessary. With such type of assumptions, if some state action pairs can be hardly visited by any policy, the output of \Cref{Algorithm: RAFFLE} $\hP(\cdot|s,a)$ can not approximate true $\Ps(\cdot|s,a)$ well over these state action pairs and on the input distribution $\{q_h\}_{h=1}^{H}$. Furthermore, Assumption \ref{assump2} is not very strict. For any bounded distribution, if a reachability assumption is satisfied, then Assumption \ref{assump2} holds. We defer the detialed discussion to \Cref{sec: app feature learning}.
	% \end{remark}
% \begin{assumption}\label{assumption: finite measurement}
	% 	Assume the state space $\Sc$ is compact and has finite measure  $1/\upsilon$. Hence, the uniform distribution on $\Sc$ has the density function $f(s)=\upsilon$.
	% \end{assumption}

Next, we assume that the rewards chosen for generating target $Q$-functions are sufficiently diverse
%Furthermore, if $T$ different rewards are chosen and are diverse enough (by Assumption \ref{assump3: Diverse Rewards}), then 
so that the target $Q^{\pi}_{P^{\star},h, r}(s_h,a_h)$ spans over the entire representation space to guarantee accurate representation learning in \Cref{ineq: alg-linear regression}.
%. Hence, $Q^{\pi}_{\hP,h, r}(s_h,a_h)$ can be viewed as a good label (target) for regression to learn an accurate representation as in \cref{ineq: alg-linear regression}. 
%In this way, the learned representation approximates the true representation along all directions of the representation function.
%The next assumption requires the rewards are diverse enough, which
Such an assumption is commonly adopted in multi-task representation learning literature~\citep{DBLP:conf/iclr/DuHKLL21,DBLP:conf/iclr/YangHLD21,lu2021power}. To formally state the assumption, for any $h \in [H]$, let $\{r^{h,t}\}_{t\in[T]}$ be a set of $T$ rewards (where $T \geq d$) satisfying $r^{h,t}_h=0$. As a result, for each $t$, given $\pi^t$, there exists ${w_h^t}^\star \in \Rb^{d}$ such that $Q^{\pi}_{P^{\star},h, r^{h,t}}(s_h,a_h)=\langle\sphi_h(s_h,a_h),{w_h^t}^\star\rangle$. Let $W_h^\star=[{w_h^1}^\star,\ldots,{w_h^T}^\star] \in \Rb^{d \times T}$.
\begin{assumption}[Diverse rewards]\label{assump3: Diverse Rewards} The smallest singular value $\sigma_d(W_h^\star)$ of $W_h^\star$ defined above satisfies $\sigma^2_d(W_h^\star) \geq \Omega(\frac{T}{d})$, i.e., there exists a constant $C_D > 0$ such that $\sigma^2_d(W_h^\star) \geq \frac{C_DT}{d}$.  
\end{assumption}
%We elaborate the ideas of representation learning and defer our formal algorithm \Cref{alg: feature} to \Cref{sec: app feature learning}. For each $h \in [H], t\in [T]$, $N_f$ pairs of state action $(s_h,a_h)$ are drawn from known distribution $\{q_h\}$ that satisfies Assumption \ref{assump2}. Then with known estimated transition kernel $\hP$ from \Cref{Algorithm: RAFFLE} and artificial designed rewards $r^{h,t}$ that satisfies Assumption \ref{assump3: Diverse Rewards}, $Q^{\pi}_{\hP,h, r^{h,t}}(s_h,a_h)$ can be computed efficiently and serve as a target for the agent to learn representation through following regression.
%   \begin{align}     \textstyle \mathop{\arg\min}_{\phi_h \in \Phi,w_h^t\in \Rb^d}\sum_{t\in[T]}\sum_{(s_h,a_h) \in \Dc_h^t} (Q^{\pi}_{\hP,h, r^{h,t}}(s_h,a_h)-\langle\phi_h(s_h,a_h),w_h^t\rangle)^2 \label{ineq: alg-linear regression}.
	%  \end{align}

%On one hand, if the input distribution $\{q_h\}_{h \in [H]}$ satisfy Assumption \ref{assump2}, then following form \Cref{Theorem: system identification sample complexity}, it shows that for any reward $r$, $Q^{\pi}_{\hP,h, r}(s_h,a_h)$ can approximate when $(s_h,a_h)$ subjects to certain distribution $\{q_h\}_{h=1}^{H}$. On the other hand, for any $h$, if we set the reward at step $h$ to be zero, $Q^{\pi}_{P^{\star},h, r}(s_h,a_h)$ can have a linear structure w.r.t true representation $\sphi_h(s_h,a_h)$. In other words, there exists a $w_h$ decided by $r$, $\Ps$ and $\pi$ such that $Q^{\pi}_{P^{\star},h, r}(s_h,a_h)=\langle\sphi_h(s_h,a_h),w_h\rangle$.

%  Furthermore, if $T$ different rewards are chosen and are diverse enough (by Assumption \ref{assump3: Diverse Rewards}), then the corresponding $Q^{\pi}_{P^{\star},h, r}(s_h,a_h)$ spans the whole representation space. Hence, $Q^{\pi}_{\hP,h, r}(s_h,a_h)$ can be viewed as a good label (target) for regression to learn an accurate representation as in \cref{ineq: alg-linear regression}. In this way, the learned representation approximates the true representation along all directions of the representation function.

We next characterize the accuracy of the output representation of \Cref{alg: feature} in terms of the divergence \Cref{def:divergence} between the learned and the ground truth representations in the following theorem and defer the proof to \Cref{sec: app feature learning}.
\begin{theorem}[Guarantee for representation learning] \label{Thm: individual representation}
	Under Assumption \ref{assumption: realizability} and Assumption \ref{assump3: Diverse Rewards}, for any $\epsilon,\delta \in (0,1)$, any $h \in [H]$, and sufficiently large $N_f$, let $\hP$ be the output transition kernel of RAFFLE that
	%\Cref{Algorithm: RAFFLE} 
	satisfies \Cref{Theorem: system identification sample complexity}. With probability at least $1-\delta$, the output $\tphi$ of RAFFLE satisfies
	%can be close to the true representation. Specially,
	\begin{align}
		\textstyle
		\left\| D_{ q_h}(\Pshi_{h},\tphi_{h})^{1/2}\right\|_F^2 = O(\frac{\epsilon d C_\mathrm{min}}{C_D}+\frac{d}{C_D}\sqrt{\frac{\log{\frac{2}{\delta}}}{TN_f}}). \label{Ineq: closeness of individual feature} 
	\end{align} 
\end{theorem}
We explain the bound in \Cref{Ineq: closeness of individual feature} as follows. The first term arises from the system identification error $\epsilon$ by using the output of RAFFLE, and can be made as small as possible by choosing appropriate $\epsilon$. The second term is related to the randomness caused by sampling state-action pairs from input distribution $\{q_h\}_{h \in [H]}$, and vanishes as $N_f$ becomes large. Note that $N_f$ is the number of simulated samples in \Cref{alg: feature}, which does not require interaction with the true environment. Hence, it can be made sufficiently large to guarantee a small error.	

%For the sample complexity bound in \Cref{Ineq: closeness of individual feature}, the first term scales with $\epsilon$, which arises from the errors using output of \Cref{Algorithm: RAFFLE} as target. If the model learned by \Cref{Algorithm: RAFFLE} is accurate enough, this term can be small enough. The second term is related to estimation errors in the linear regression in \Cref{ineq: alg-linear regression} and is inversely proportional to $N_f$. Notice that because sampling in \Cref{alg: feature} doesn't require interaction with true environment, $N_f$ can be arbitrarily large and the second term can be arbitrarily small.

\Cref{Thm: individual representation} shows RAFFLE can learn a near-accurate representation, based on which learned representations can be further used in other RL environments sharing common representations, similarly in spirit to how representation learning has been exploited in supervised learning~\citep{DBLP:conf/iclr/DuHKLL21}. 

%Specifically, when the agent is assigned with a new task sharing common representation, the agent can select a reasonable input distribution $\{q_h\}_{h \in [H]}$ according to the task and use \Cref{alg: feature} to pre-train a representation, which can be applied for the new task.




%We summarize the novelty here. 1) Unlike previous RL works only providing system identification guarantee, \Cref{alg: feature} learns a near accurate representation with closeness guarantee w.r.t true representation. 2) \Cref{alg: feature} doesn't require collecting any samples from true environment and is highly \textbf{statistically efficient}. 3) With the guarantee of representation learning in \Cref{Thm: individual representation}, the learned representation can be further used for few shot learning as in supervised learning~\citep{DBLP:conf/iclr/DuHKLL21}. Specifically, when the agent is assigned with a new task sharing common representation, the agent can select a reasonable input distribution $\{q_h\}_{h \in [H]}$ according to the task and use \Cref{alg: feature} to pre-train a representation, which can be applied for the new task.
\vspace{-0.05in}
\section{Related Work}\label{sec:relatedwork}
\vspace{-0.05in}
 \textbf{Reward-free RL.} While various studies \citep{oudeyer2007intrinsic,bellemare2016unifying,burda2018exploration,DBLP:journals/corr/abs-1810-06284,nair2018visual,eysenbach2018diversity,co2018self,hazan2019provably,du2019provably,pong2019skew,misra2020kinematic} proposed exploration algorithms for good coverage on the state space without using explicit reward signals, theoretically speaking, the paradigm of reward-free RL was first formalized by \citet{pmlr-v119-jin20d}, where they provided both upper and lower bounds on the sample complexity. 
	For tabular case, several follow-up studies~\citep{kaufmann2021adaptive,menard2021fast} further improved the sample complexity, and \citet{zhang2020nearly} established the minimax optimality guarantee. Reward-free RL was also studied with function approximation. \citet{wang2020reward} studied \textit{linear MDPs}, and \citet{zhang2021reward} studied \textit{linear mixture MDPs}. Further,
	% , where the transition is a linear mixture of known transition kernels. 
	\citet{zanette2020provably} considered a class of MDPs with \textit{low inherent Bellman error} introduced by \citet{zanette2020learning}. \citet{DBLP:journals/corr/abs-2206-10770} proposed a reward-free algorithm called RFOLIVE under non-linear MDPs with low Bellman Eluder dimension. In addition, \citet{miryoosefi2021simple} proposed a reward-free approach to solving constrained RL problems with any given reward-free RL oracle under both the tabular and linear MDPs. 
	
	%Reward-free RL was also studied under function approximation settings. For linear MDPs, \citet{wang2020reward} propose an algorithm that achieves the sample complexity of $\tilde{O}(\frac{H^6d^3}{\epsilon^2})$, where $d$ is the dimension of the feature space. \citet{zhang2021reward} studied linear mixture MDPs and proposed the UCRL-RFE algorithm, which achieves the sample complexity of $\tilde{O}(\frac{H^4d(H+d)}{\epsilon^2})$, with $d$ being the dimension of the kernel space. They also provided a lower bound of $\tilde{O}(\frac{H^2d}{\epsilon^2})$ on the sample complexity, which matches their upper bound partially. %matches lower bound with respect to $d$ in large H regime. \citet{zanette2020provably} considered a class of MDPs with low inherent Bellman error, and proposed an online PAC algorithm that learns a near-optimal policy for reward-free RL with sample complexity of $\tilde{O}(\frac{H^5d^3}{\epsilon^2})$. Recently, a reward-free algorithm called RFOLIVE has been proposed under non-linear MDPs with low Bellman Eluder dimension by \cite{DBLP:journals/corr/abs-2206-10770}, which be specialized to low-rank MDPs. However, RFOLIVE is not computationally efficient and makes assumptions on the reward functions, although their sample complexity may have sharper dependence on $K$. In addition, \citet{miryoosefi2021simple} proposed a reward-free approach to solving constrained RL problems with any given reward-free RL oracle under both the tabular and linear MDPs. 
	
	
	%\yl{reward free RL was not started by Jin's paper. See his paper for some earlier references}\cy{\citet{pmlr-v119-jin20d} claims reward-free RL is a novel paradigm, \citet{wang2020reward} also writes that although practitioners have proposed algorithms without using explicit reward, \citet{pmlr-v119-jin20d} is the first work formalized reward-free RL, can I introduce previous work as the first sentence?}
	
	\textbf{Reward-free RL under low-rank MDPs.} 
	As discussed in \Cref{intro}, reward-free RL under low-rank MDPs have been studied recently \citep{NEURIPS2020_e894d787,modi2021model}, and our result significant improves the sample complexity therein.
	%Reward-free RL under low-rank MDPs was first studied by \citet{NEURIPS2020_e894d787}, where they proposed an algorithm coined FLAMBE and showed that it achieves the learning goal of system identification 
	%(that implies the best-policy goal) 
	%with the sample complexity of $\tilde{O}(\frac{H^{22}K^9d^7}{\epsilon^{10}})$. \citet{DBLP:conf/iclr/UeharaZS22} proposed a model-free algorithm named MOFFLE for reward-free RL under low-nonnegative-rank MDPs, which achieves the sample complexity of $\tilde{O}(\frac{H^5K^5d^3_{LV}}{\epsilon^2\eta})$ for finding an $\epsilon$-optimal policy. 
	%Such a result is better than that in \citet{NEURIPS2020_e894d787} in terms of $\epsilon$, $H$, and $K$, but worse in terms of $d_{LV}$ and $\eta$, because $d_{LV}$ may be exponentially larger than $d$ as shown in \citet{NEURIPS2020_e894d787} and $1/\eta$ can be as larger as $\sqrt{d_{LV}}$. 
	%Our proposed algorithm in this work improves the sample complexity in both of the above studies. 
	When finite latent state space is assumed, low-rank MDPs specialize to block MDPs, under which algorithms termed as PCID~\citep{du2019provably} and HOMER~\citep{misra2020kinematic} achieved sample complexities of $\tilde{O}(\frac{d^4H^2K^4}{\min(\eta^4\gamma_n^2,\epsilon^2)})$ and $\tilde{O}(\frac{d^8H^4K^4}{\min(\eta^3,\epsilon^2)})$, respectively. Our result on the general low-rank MDPs can be utilized to further improve those results for block MDPs.
	
	%FLAMBE\cite{NEURIPS2020_e894d787} is a state-of-art model-based oracle-efficient algorithm for low-rank MDPs. Achieving the same learning goal of system identification, the sample complexity of our algorithm RAFFLE is much better than FLAMBE with respect to all parameters. Our algorithm and FLAMBE both assume realizability and MLE oracle. But to get a MLE guarantee on the estimation error of the true transition kernel under the exploration policy, FLAMBE collects multiple samples for every single exploration policy at each timestep which results in its worse sample complexity. In our algorithm, RAFFLE collects only one episode for each policy at each timestep and combine all the policy before the $n$-th episode to be a mixture policy and all the episodes together to be a whole dataset. In this way, RAFFLE achieves MLE guarantee with much less samples.  
	
	%Moffle\cite{DBLP:conf/iclr/UeharaZS22} is a model-free algorithm for low-nonnegative-rank MDPs, with two more assumptions: (1) the transition has low-nonnegative-rank and (2) reachability in late variable state. Under these two assumptions, the sample complexity scales with $\tilde{O}(\frac{H^5d^3_{LV}K^5}{\epsilon^2\eta})$\cy{We rescale it with the condition: $\sum_{h=1}^{H}r_h \leq 1$}. Where we denote $d_{LV}$ the nonnegative rank of the transition and $\eta$ the reachability probability. Note that under the first assumption, low-nonnegative rank represents different structure from low rank in our work, and Proposition 2 in \citet{NEURIPS2020_e894d787} shows that $d_{LV}$ may be exponentially larger than $d$. Under the second assumption, $1/\eta$ could be as larger as $d_{LV}$. As a result, achieving the same learning goal of reward free learning, our sample complexity is better than Moffle with respect to $K$ and $d$.
	
	\textbf{Reward-known RL under low-rank MDPs.}
	For reward-known RL, \citet{DBLP:conf/iclr/UeharaZS22} proposed a computationally efficient algorithm REP-UCB under low-rank MDPs. Our design of the reward-free algorithm for low-rank MDPs is inspired by their algorithm with several new ingredients as  discussed in \Cref{sec:algorithm}, and improves their sample complexity on the dependence of $H$.
	%, which achieves sample complexity of $\tilde{O}(\frac{H^{5}K^2d^4}{\epsilon^{2}})$. 
	Meanwhile, algorithms have been proposed for MDP models with low Bellman rank \citep{jiang2017contextual}, low witness rank \citep{sun2019model},  bilinear classes \citep{du2021bilinear} and low Bellman eluder dimension~\citep{jin2021bellman}, which can specialize to low-rank MDPs. 
	However, those algorithms are computationally more costly as remarked in \citet{DBLP:conf/iclr/UeharaZS22},
	although their sample complexity may have sharper dependence on $d,K$ or $H$. Specializing to block MDPs, \citet{zhang2022efficient} proposed an algorithm called BRIEE, which empirically achieves the state-of-art sample complexity for block MDP models. Besides, \citet{DBLP:journals/corr/abs-2106-11935} proposed an algorithm coined ReLEX for a slightly different low-rank model, and obtained a problem dependent regret upper bound.

%We specially note that this work was initially submitted for potential publication in January 2022, after which many studies have been developed on various extended low-rank models (see more details in \Cref{sec:relatedwork}). Our results on lower bound and representation learning are still completely new even given those developments, and our algorithm design and result on sample complexity are so far still the best-known result for vanilla reward-free low-rank MDPs.

 

%This work focuses on the most fundamental setting in the study of low-rank MDPs, and our novel algorithm design and theoretical analysis can be adopted to further improve the sample complexity in many of these studies. 
	
	%This work was finished and submitted for potential publication in early 2022. After that there have been a line of work studying different scenarios and frameworks under low-rank MDP.     \cite{DBLP:journals/corr/abs-2205-13476,uehara2022provably} studied Partially Observable Markov Decision Process (POMDP) with latent low-rank structure. \cite{zhan2022pac} studied Predictive State Representations model, which captures POMDP and then applied their results to POMDP with latent low-rank structure. \cite{DBLP:journals/corr/abs-2206-05900,agarwal2022provable} studied benefits of multitask representation learning under low-rank MDPs. \cite{huang2022safe} proposed a meta safe RL algorithm and applied it to low-rank MDPs. \cite{ren2022spectral} studied reward known RL under low-rank MDPs and proposed a spectral method to aside computation oracle. As our work studied fundamental reward-free exploration under low-rank MDPs, these works all focus on different interests with us.
	
\section{Conclusions}

In this paper, we investigate the reward-free reinforcement learning, in which the underlying model admits a low-rank structure. Without further assumptions, we propose an algorithm called RAFFLE which significantly improves the state-of-the-art sample complexity for accurate model estimation and near-optimal policy identification. We further design an algorithm to exploit the model learned by RAFFLE for accurate representation learning without further interaction with the environment. Although $\epsilon$-accurate system identification can easily induce $H\epsilon$-optimal policy, the relationship between these two learning goals under general reward-free exploration setting remain under-explored, and is an interesting topic for future investigation.  
%It appears to be yet to understand that whether we can develop an algorithm that finds a near-optimal policy but has poor performance on system identification.

%Inspired by the optimistic principle, we carefully design the exploration-driven reward functions and the corresponding exploration policies so that the corresponding value functions control the difference in the value functions under the true model and estimated model for any policy and reward. Our algorithm can identify an $\epsilon$-optimal policy and an $\epsilon$-accurate system simultaneously. 

%We next point out some interesting future directions. For learning goals of reward-free exploration, although $\epsilon$-accurate system identification can easily induce $H\epsilon$-accurate best policy, the relationship between these two learning goals under general reward-free exploration setting remain under-explored. It appears to be an open question that whether we can develop an algorithm that finds a near-optimal policy but has poor performance on system identification. Another question is that whether reward-known algorithms enjoy strictly lower sample complexity compared with reward-free algorithms.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite



% \section{Final instructions}
% Do not change any aspects of the formatting parameters in the style files.
% In particular, do not modify the width or length of the rectangle the text
% should fit into, and do not change font sizes (except perhaps in the
% \textsc{References} section; see below). Please note that pages should be
% numbered.

% \section{Preparing PostScript or PDF files}

% Please prepare PostScript or PDF files with paper size ``US Letter'', and
% not, for example, ``A4''. The -t
% letter option on dvips will produce US Letter files.

% Consider directly generating PDF files using \verb+pdflatex+
% (especially if you are a MiKTeX user).
% PDF figures must be substituted for EPS figures, however.

% Otherwise, please generate your PostScript and PDF files with the following commands:
% \begin{verbatim}
	% dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
	% ps2pdf mypaper.ps mypaper.pdf
	% \end{verbatim}

% \subsection{Margins in LaTeX}

% Most of the margin problems come from figures positioned by hand using
% \verb+\Psecial+ or other commands. We suggest using the command
% \verb+\includegraphics+
% from the graphicx package. Always specify the figure width as a multiple of
% the line width as in the example below using .eps graphics
% \begin{verbatim}
	%   \usepackage[dvips]{graphicx} ...
	%   \includegraphics[width=0.8\linewidth]{myfile.eps}
	% \end{verbatim}
% or % Apr 2009 addition
% \begin{verbatim}
	%   \usepackage[pdftex]{graphicx} ...
	%   \includegraphics[width=0.8\linewidth]{myfile.pdf}
	% \end{verbatim}
% for .pdf graphics.
% See section~4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})

% A number of width problems arise when LaTeX cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command.

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{RFRL}
\bibliographystyle{iclr2023_conference}

% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \startcontents[section]
% {
	% \hypersetup{linkcolor=black}
	% \printcontents[section]{l}{1}{\setcounter{tocdepth}{2}}
	% }


\section{Proof of \Cref{thm1: reward-free sample complexity}}\label{sec: A}
We first provide a proof outline to highlight our key ideas in the analysis of \Cref{thm1: reward-free sample complexity}, and then provide the detailed proof. To simplify the notation, we denote the total variation distance $\left\|\hP_h^{(n)}(s_h,a_h) - P_h^\star(s_h,a_h)\right\|_{TV}$ by $f_h^{(n)}(s_h,a_h)$.
\begin{proof}[Proof Outline for \Cref{thm1: reward-free sample complexity}]
	\textbf{Step 1} provides an upper bound on the difference of value functions under estimated model $\hP^{(n)}$ and the true model $P^\star$ for any given policy $\pi$ and reward $r$, as given in the following proposition (see  \cref{prop:step1_appendix} in \cref{subsec:reward_free_proof}).
	\begin{proposition}[Informal]\label{prop:step1}
		There exist constants $c_n=O(\log n)$ and $c'=O(1)$ such that for any policy $\pi$ and reward $r$, with high probability, we have
		\begin{equation*}
			\left|V_{P^\star,r}^{\pi} - V_{\hP^{(n)}, r}^{\pi}\right|\leq \hat{V}_{\hP^{(n)},\hb^{(n)}}^{\pi_n} + \sqrt{\frac{c_n}{n}}.
		\end{equation*}
	\end{proposition}
	This proposition is inspired by \citet{DBLP:conf/iclr/UeharaZS22}, while it generalizes to non-stationary setting and arbitrary reward scenario from infinite stationary MDP and fixed reward. %generalizes a result of the similar type in  \tcr{from the infinite stationary MDP setting} to the episodic non-stationary setting. 
	The main proof idea is to first notice that for any reward, we have
	\begin{equation}
		\left|V_{P^\star,r}^{\pi} - V_{\hP^{(n)}, r}^{\pi}\right|\leq \hat{V}_{\hP^{(n)}, f^{(n)}}^{\pi}.\label{eqn:simulation_equation}
	\end{equation}
	Then, we show that $\hV_{\hP^{(n)},f^{(n)}}^{\pi}\leq \hV_{\hP^{(n)},\hb^{(n)}}^{\pi}+\sqrt{\frac{c_n}{n}}\leq \hV_{\hP^{(n)},\hb^{(n)}}^{\pi_n}+\sqrt{\frac{c_n}{n}}$ due to the optimality of the exploration policy $\pi_n$.
	%\textcolor{green}{$f^{(n)}$ is not introduced yet.}
	
	% \textbf{Step 2} explicitly bounds the difference of value functions under $\hP^{(n)}$ and $P^\star$ for our exploration-driven reward $b^{(n)}$ and exploration policy $\pi_n$, as given in the following proposition (see  \cref{prop:step2_appendix} in \cref{sec: A}.)
	% \begin{proposition}[Informal]\label{prop:step2}
		% Under the same setting of \Cref{prop:step1}, for any $N$ and with probability at least $1-\delta$, we have 
		% \begin{equation*}
			%     \sum_{n=1}^N\left|V_{\hP^{(n)},\hb^{(n)}}^{\pi_n}-V_{P^\star,\hb^{(n)}}^{\pi_n}\right|\leq \tilde{O}\left( H^2\sqrt{dK(K+d^2)N}\right),
			% \end{equation*}
		% where $\phi^\star_h$ is a shorthand of $\phi^\star(s_h,a_h)$ and $U_h^{(n)}$ is the expectation of $\hat{U}_h^{(n)}$ over the distributions induced by previous exploration policies.
		% \end{proposition}
	% %\tcr{should we include a short summary of the proof of the propositions?}
	% We point out that the major difference in the analysis of Step 2 and Step 1 is that, instead of applying  \cref{eqn:simulation_equation}, switching the underlying model $\hP^{(n)}$ at the RHS of \cref{eqn:simulation_equation} to $P^\star$ is required.
	
	
	\textbf{Step 2} shows sublinearity of the summation of $\hV_{\hP^{(n)},\hb^{(n)}}^{\pi_n}$, which is an upper bound of value function difference under the true  model with the estimated model (see  \cref{prop:step2_appendix} in \cref{sec: A.3}).
	
	\begin{proposition}[Informal]\label{prop:step2}
		%\label{lemma: bound on summation of V_hat}
		Under the same setting of \Cref{prop:step1}, with high probability, the summation of value functions $V^{\pi_n}_{\hP^{(n)},\hb^{(n)}}$ under exploration policies $\{\pi_n\}_{n\in[N]}$ with exploration-driven reward functions $\hb^{(n)}$ is sublinear, as given by
		\begin{align*}
			\sum_{n\in[N]}& \hat{V}_{\hP^{(n)},\hb^{(n)}}^{\pi_n}  \leq
			\tilde{O}\left(Hd\sqrt{K(K+d^2)N}\right).
		\end{align*}
	\end{proposition}
	
	
	\textbf{Step 3} combines Step 2 and Step 1. We are able to conclude that RAFFLE will terminate with polynomial sample complexity such that, the value function difference of $P^\star$ and the returned environment $\hP^{\epsilon}$ is at most $\epsilon$. 
	
	
	\begin{proposition}\label{prop:step3}
		With high probability, RAFFLE terminates after at most $\Tilde{O}\left(H^2d^2K(d^2+K)/\epsilon^2\right)$ iterations, and the output model $\hP^{\epsilon}$ satisfies
		\begin{equation*}
			\left|V_{P^\star,r}^{\pi} - V_{\hP^{(\epsilon)}, r}^{\pi}\right|\leq \hat{V}_{\hP^{(\epsilon)},\hb^{(n_\epsilon)}}^{\pi_{n_\epsilon}} + \sqrt{\frac{c_{n_\epsilon}}{n_\epsilon}}\leq \epsilon/2,
		\end{equation*}
		where $n_\epsilon$ is the iteration number where RAFFLE terminates, i.e. $\hP^{\epsilon} = \hP^{(n_\epsilon)}$.
	\end{proposition} 
	
	Finally, with some algebraic operations, \Cref{prop:step3} concludes the proof of \Cref{thm1: reward-free sample complexity}.
\end{proof}

\subsection{Supporting Lemmas}\label{app:supportlemmalowrank}
We first present the high probability event.

\begin{lemma}\label{lemma:high_prob_event}
	We define $\Pi_n$ to be a uniform mixture of previous $n-1$ exploration policies:
	\begin{align*}
		\Pi_n = \Uc(\pi_0, \pi_1,...,\pi_{n-1}).
	\end{align*}
	Denote the total variation of $\hP^{(n)}$ and $P^\star$, and the expected matrix of $\hat{U}_h^{(n)} $ as follows.
	
	\begin{align}
		&f_h^{(n)}(s_h,a_h) = \left\|P_h^\star(\cdot|s_h,a_h)-\hP_h^{(n)}(\cdot|s_h,a_h)\right\|_{TV},\\
		&U_{h,\phi}^{(n)} = n\mathop{\Eb}_{s_h\sim(P^\star,\Pi_n )\atop a_h\sim \Uc(\Ac)}\left[\phi(s_h,a_h)(\phi(s_h,a_h))^\top\right] + \lambda_n I,\label{eqn:expected_U_matrix}\\
		&W_{h,\phi}^{(n)} = n\mathop{\Eb}_{(s_h,a_h)\sim(P^\star,\Pi_n)}\left[\phi(s_h,a_h)(\phi(s_h,a_h))^\top\right] + \lambda_n I.
	\end{align}
	where $\lambda_n = \beta_3 d\log(2nH|\Phi|/\delta))$ and $\beta_3 = O(1)$ is a constant coefficient. 
	Suppose \Cref{Algorithm: RAFFLE} runs $N$ iterations and let events $\mathcal{E}_0$ and $\mathcal{E}_1$ be defined as follows.   
	
	
	\begin{align*}
		&\mathcal{E}_0 = \bigg\{\forall n\in[N],h\in[H],s_h\in\Sc,a_h\in\Ac,  \mathop{\Eb}_{s_{h}\sim (P^\star,\Pi_n)\atop a_h\sim \mathcal{U}}\left[f_h^{(n)}(s_h,a_h)^2\right]\leq \zeta_n\bigg\}, \\
		&\mathcal{E}_1 = \bigg\{\forall n\in[N],h\in[H], s_h\in\Sc, a_h\in\Ac, \\
		&\quad\quad \frac{1}{5} \left\|\hphi_{h-1}^{(n)}(s,a)\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}} \leq \left\|\hphi_{h-1}^{(n)}(s,a)\right\|_{(\hat{U}_{h-1}^{(n)})^{-1}} \leq 3 \left\|\hphi_{h-1}^{(n)}(s,a)\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}}\bigg\}.
	\end{align*}
	where $\zeta_n = \log\left(2|\Phi||\Psi|nH/\delta\right)/n$. We further denote $\zeta=N\zeta_N=\log\left(2|\Phi||\Psi|NH/\delta\right)$ for simplicity. Denote $\mathcal{E} = \mathcal{E}_0\cap\mathcal{E}_1$ as the intersection of the two events. Then, $\Pb[\mathcal{E}]\geq 1-\delta$.
	
	
\end{lemma}


\begin{proof}
	
	By \Cref{coro:MLE} in \Cref{appx:auxiliary}, we have $\Pb[\mathcal{E}_0]\geq 1-\delta/2$.  Further, by Lemma 39 in \citet{zanette2021cautiously} for the version of fixed $\phi$ and Lemma 11 in \citet{DBLP:conf/iclr/UeharaZS22}, we have $\Pb[\mathcal{E}_1]\geq 1-\delta/2$. Therefore, $\Pb[\mathcal{E}]\geq 1-\delta$. 
\end{proof}

Based on \Cref{lemma:high_prob_event}, we can bound the exlporation-driven reward in RAFFLE as follows.
\begin{corollary}\label{coro:concentration on b}
	Given that the event $\Ec$ occurs, the following inequality holds for any $n\in[N],h\in[H], s_h\in\Sc,a_h\in\Ac$:
	\begin{align*}
		\min\left\{\frac{\hat{\alpha}_n} {5}\left\|\hphi_{h}^{(n)}(s_{h},a_{h})\right\|_{(U_{h,\hphi}^{(n)})^{-1}},1\right\} \leq \hb_h^{(n)}(s_h,a_h) \leq  3\hat{\alpha}_n \left\|\hphi_{h}^{(n)}(s_{h},a_{h})\right\|_{(U_{h,\hphi}^{(n)})^{-1}},
	\end{align*}
	where $\hat{\alpha}_n = 5\sqrt{2\beta_3n\zeta_n (K+d^2)}$.
\end{corollary}
\begin{proof}
	Recall $\hb_h^{(n)}(s_h,a_h)=\min\left\{\hat{\alpha}_n\left\|\hphi_{h}^{(n)}(s,a)\right\|_{(\hat{U}_{h}^{(n)})^{-1}},1\right\}$. Applying \Cref{lemma:high_prob_event}, we can immediately obtain the result.
\end{proof}







The following lemma extends the Lemmas 12 and 13 under infinite discount MDPs in \cite{DBLP:conf/iclr/UeharaZS22} to episodic MDPs.  We provide the proof for completeness.



\begin{lemma}\label{lemma:Step_Back} 
	Let $P_{h-1} = \langle\phi_{h-1},\mu_{h-1}\rangle$ be a generic MDP model, and $\Pi$ be an arbitrary and possibly mixture policy. Define an expected Gram matrix as follows
	
	\[M_{h-1,\phi} = 
	\lambda_n I + n\mathop{\Eb}_{s_{h-1}\sim (P^\star,\Pi) \atop a_{h-1}\sim \Pi}\left[\phi_{h-1}(s_{h-1},a_{h-1})\left(\phi_{h-1}(s_{h-1},a_{h-1})\right)^\top\right].
	\] 
	
	%be an expected gram matrix, where $\Pi = G_{h-1}^{\eo}\Pi_n$ when $M_{h,\phi} = U^{(n)}_{h,\phi}$, and $\Pi = \Pi_n$ when $M_{h,\phi} = W_{h,\phi}^{(n)}$. Note that $\phi_h\in\{\hphi_h^{(n)},\phi_h^\star\}$. 
	Further, let $f_{h-1}(s_{h-1},a_{h-1})$ be the total variation between $P_{h-1}^\star$ and $P_{h-1}$ at time step $h-1$. Suppose $g \in \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is bounded by $B\in(0,\infty)$, i.e., $\|g\|_\infty \leq B$. Then, $\forall h \geq 2, \forall\, {\rm policy }\,\pi_h$,
	\begin{align*}
		\mathop{\Eb}_{s_h \sim P_{h-1} \atop a_h \sim \pi_h}&[g(s_h,a_h)|s_{h-1},a_{h-1}]  \\
		&\leq \left\|\phi_{h-1}(s_{h-1},a_{h-1})\right\|_{(M_{h-1,\phi})^{-1}} \times\nonumber\\
		&\quad
		\sqrt{n{K}\mathop{\Eb}_{s_{h}\sim(P^\star, \Pi)\atop a_h \sim \Uc }[g^2(s_h,a_h)]+\lambda_n dB^2 + nB^2\mathop{\Eb}_{s_{h-1}\sim(P^\star,\Pi)\atop a_{h-1}\sim\Pi }\left[f_{h-1}(s_{h-1},a_{h-1})^2\right]}.
	\end{align*}
\end{lemma}


\begin{proof}
	We first derive the following bound:
	\begin{align*}
		& \mathop{\Eb}_{s_h \sim P_{h-1} \atop a_h \sim \pi_h}\left[ 
		g(s_h,a_h)|s_{h-1},a_{h-1}\right]\nonumber\\
		&=\int_{s_h}\sum_{a_h}g(s_h,a_h)\pi(a_h|s_h)\langle\phi_{h-1}(s_{h-1},a_{h-1}),\mu_{h-1}(s_h)\rangle d{s_h}\\
		& \leq \left\|\phi_{h-1}(s_{h-1},a_{h-1})\right\|_{(M_{h-1,\phi})^{-1}}\left\|\int\sum_{a_h}g(s_h,a_h)\pi(a_h|s_h)\mu_{h-1}(s_h)d{s_h}\right\|_{M_{h-1,\phi}},
	\end{align*}
	where the inequality follows from Cauchy-Schwarz inequality. We further expand the second term in the RHS of the above inequality as follows.
	\begin{align*}
		&\hspace{-5mm} \left\|\int\sum_{a_h}g(s_h,a_h)\pi(a_h|s_h)\mu_{h-1}(s_h)d{s_h}\right\|_{M_{h-1,\phi}}^2\\
		& \overset{(\romannumeral1)}{\leq} n \mathop{\Eb}_{s_{h-1}\sim (P^\star,\Pi) \atop a_{h-1}\sim \Pi}
		\left[\left(\int_{s_h}\sum_{a_h}g(s_h,a_h)\pi_h(a_h|s_h)\mu(s_h)^\top\phi(s_{h-1},a_{h-1})d{s_h}\right)^2\right] + \lambda_n d B^2\\
		&= n \mathop{\Eb}_{s_{h-1}\sim(P^\star,\Pi) \atop a_{h-1} \sim \Pi } \left[\left(\mathop{\Eb}_{s_h \sim P_{h-1} \atop a_h \sim \pi_h} \left[g(s_h,a_h)\bigg|s_{h-1},a_{h-1}\right]\right)^2\right] + \lambda_n d B^2\\
		&\overset{(\romannumeral2)}{\leq} 2n \mathop{\Eb}_{s_{h-1}\sim(P^\star,\Pi) \atop a_{h-1}\sim\Pi }\left[\mathop{\Eb}_{s_h\sim P_{h-1}^\star \atop a_h\sim \pi_h}\left[g(s_h,a_h)\bigg|s_{h-1},a_{h-1}\right]^2\right]+ \lambda_n d B^2 \\
            &\quad + 2nB^2\mathop{\Eb}_{s_{h-1}\sim(P^\star,\Pi) \atop a_{h-1}\sim\Pi }\left[f_{h-1}(s_{h-1},a_{h-1})\right]^2\\
  		& \overset{(\romannumeral3)}\leq 2n \mathop{\Eb}_{s_{h-1}\sim(P^\star,\Pi) \atop a_{h-1} \sim \Pi } \left[\mathop{\Eb}_{s_h \sim P_{h-1}^\star \atop a_h \sim \pi_h} \left[g(s_h,a_h)^2\bigg|s_{h-1},a_{h-1}\right]\right] + \lambda_n d B^2\\
		&\quad + 2nB^2\mathop{\Eb}_{s_{h-1}\sim(P^\star,\Pi) \atop a_{h-1}\sim\Pi }\left[f_{h-1}(s_{h-1},a_{h-1})^2\right]\\
		&\overset{(\romannumeral4)}{\leq} 2n{K} \mathop{\Eb}_{s_{h}\sim(P^\star,\Pi)\atop a_h\sim \Uc }\left[g(s_h,a_h)^2\right] + \lambda_n d B^2 + 2nB^2\mathop{\Eb}_{s_{h-1}\sim(P^\star,\Pi) \atop a_{h-1}\sim \Pi }\left[f
		_{h-1}(s_{h-1},a_{h-1})^2\right],
	\end{align*}
	where $(\romannumeral1)$ follows from the assumption that $\|g\|_{\infty}\leq B$, $(\romannumeral2)$ is due to that $f_{h-1}(s_{h-1},a_{h-1})$ is the total variation between $P_{h-1}^\star$ and $P_{h-1}$ at time step $h-1$ and the fact that $(a+b)^2\leq 2a^2+2b^2$, $(\romannumeral3)$ follows from Jensen's inequality, and $(\romannumeral4)$ is due to importance sampling.
	This finishes the proof.
\end{proof}



Based on \Cref{lemma:Step_Back}, we summarize three useful inequalities which bridges the total variation $f_h^{(n)}$ and the exploration-driven reward $\hb_h^{(n)}$. 
\begin{lemma}\label{lemma:Bound_TV}
	Define 
	\begin{align}
		W_{h,\phi}^{(n)} = n\mathop{\Eb}_{s_h\sim(P^\star,\Pi_n) \atop a_h\sim \Pi_n}\left[\phi(s_h,a_h)(\phi(s_h,a_h))^\top\right] + \lambda_n I,
	\end{align}
	where $\lambda_n = \beta_3 d\log(2nH|\Phi|/\delta)$.
	Given that the event $\mathcal{E}$ occurs, the following inequalities hold. For any $n$, when $h \geq 2$, 
	\begin{align} 
		& \mathop{\Eb}_{s_{h}\sim\hP_{h-1}^{(n)}\atop a_{h}\sim \pi }\left[f_{h}^{(n)}(s_{h},a_{h})\bigg|s_{h-1},a_{h-1}\right]\leq \alpha_n \left\|\hphi_{h-1}^{(n)}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}},\label{ineq:hP_Bound_TV}\\
		& \mathop{\Eb}_{s_{h}\sim P_{h-1}^{*}\atop a_{h\sim \pi}} \left[f_{h}^{(n)}(s_{h},a_{h})\bigg|s_{h-1},a_{h-1}\right]\leq \alpha_n\left\|\phi_{h-1}^{*}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\phi^\star}^{(n)})^{-1}},\label{ineq:P*_Bound_TV}\\
		&\mathop{\Eb}_{s_{h}\sim P_{h-1}^{*} \atop a_{h}\sim \pi}\left[\hb_{h}^{(n)}(s_{h},a_{h})\bigg|s_{h-1},a_{h-1}\right]\leq \gamma_n \left\|\phi_{h-1}^{*}(s_{h-1},a_{h-1})\right\|_{(W_{h-1,\phi^\star}^{(n)})^{-1}},\label{ineq:P*_Bound_bn}
	\end{align}
	where \begin{align*}
		\alpha_n  = \sqrt{2\beta_3n\zeta_n({K} +  d^2)}, \quad \gamma_n = \sqrt{45\beta_3n\zeta_n{K} d({K}+d^2)}.
	\end{align*}
	Specially, when $h = 1$, 
	\begin{align}
		&\mathop{\Eb}_{a_1\sim \pi}\left[f_1^{(n)}(s_1,a_1)\right]
		\leq \sqrt{{K}\zeta_n}, 
		&\mathop{\Eb}_{a_1\sim \pi}\left[\hb(s_1,a_1)\right]
		\leq 15\alpha_n\sqrt{\frac{d{K}}{n}}.\label{ineq:Step_1_Bound}
	\end{align}
\end{lemma}

\begin{proof}
	We start by developing \Cref{ineq:hP_Bound_TV} as follows. Given that the event $\mathcal{E}$ occurs, for $h \geq 2$ we have
	\begin{align*}
		\mathop{\Eb}_{s_{h}\sim\hP_{h-1}^{(n)}\atop a_{h}\sim \pi }
		&\left[f_{h}^{(n)}(s_{h},a_{h})\bigg|s_{h-1},a_{h-1}\right] \\
		&\overset{(\romannumeral1)}{\leq} \left\|\hphi_{h-1}^{(n)}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}} \times \\
		&\sqrt{n{K}\mathop{\Eb}_{s_{h-1}\sim(P^\star, \Pi_n)\atop {a_{h-1}, a_h \sim \mathcal{U}\atop s_h \sim \Ps_h(\cdot|s_{h-1},a_{h-1})}}[f^{(n)}_h(s_h,a_h)^2]+\lambda_n d + n\mathop{\Eb}_{s_{h-1}\sim(P^\star,\Pi_n)\atop a_{h-1}\sim \Uc }\left[f^{(n)}_{h-1}(s_{h-1},a_{h-1})^2\right]}\\
		&\overset{(\romannumeral2)}{\leq} \left\|\hphi_{h-1}^{(n)}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}} \times \\
		&\sqrt{n{K}\mathop{\Eb}_{s_{h-1}\sim(P^\star, \Pi_n)\atop {a_{h-1}, a_h \sim \mathcal{U}\atop s_h \sim \Ps_h(\cdot|s_{h-1},a_{h-1})}}[f^{(n)}_h(s_h,a_h)^2]+\lambda_n d + n{K}\mathop{\Eb}_{{s_{h-2}\sim(P^\star, \Pi_n)\atop {a_{h-2}, a_{h-1} \sim \mathcal{U}\atop s_{h -1}\sim \Ps_{h-1}(\cdot|s_{h-2},a_{h-2})}} }\left[f^{(n)}_{h-1}(s_{h-1},a_{h-1})^2\right]}\\
		& \overset{(\romannumeral3)}{\leq}\left\|\hphi_{h-1}^{(n)}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}}\sqrt{2n\zeta_n{K} + \beta_3n\zeta_n d^2 }\\
		&\leq \alpha_n\left\|\hphi_{h-1}^{(n)}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}},
	\end{align*}
	where $(\romannumeral1)$ follows from \Cref{lemma:Step_Back} and the fact that $f_{h}^{(n)}(s_{h},a_{h}) \leq 1$, $(\romannumeral2)$ follows from importance sampling at time step $h-2$, and $(\romannumeral3)$ follows from  \Cref{lemma:high_prob_event}.
	
	\Cref{ineq:P*_Bound_TV} follows from the arguments similar to the above. 
	
	To obtain \Cref{ineq:P*_Bound_bn}, we first apply \Cref{lemma:Step_Back} and obtain
	\begin{align*}  
		& \mathop{\Eb}_{s_{h}\sim P^\star_{h-1} \atop a_{h} \sim {\pi_n} }\left[\hat{b}^{(n)}_{h}(s_{h},a_{h})\bigg|s_{h-1},a_{h-1}\right]\\
		& \leq \left\|\phi_{h-1}^\star(s_{h-1},a_{h-1})\right\|_{(W_{h-1,\Pshi}^{(n)})^{-1}}
		\sqrt{n{K}\mathop{\Eb}_{s_{h}\sim (P^\star, \Pi_n)\atop a_h\sim \Uc}[\{\hat{b}_h^{(n)}(s_h,a_h)\}^2]+\lambda_n d },
	\end{align*}
	where we use the fact that $\hat{b}_h^{(n)}(s_h,a_h)\leq 1$. We further bound the term $n\mathop{\Eb}_{s_{h}\sim (P^\star, {\Pi}_n)\atop a_h\sim \mathcal{U}}[(\hat{b}_h^{(n)}(s_h,a_h))^2]$ as follows:
	\begin{align*}
		& \quad n\mathop{\Eb}_{s_{h}\sim (P^\star, \Pi_n)\atop a_h\sim \mathcal{U}}\left[\left(\hat{b}_h^{(n)}(s_h,a_h)\right)^2\right]\\
		& \leq n\mathop{\Eb}_{s_{h}\sim (P^\star, \Pi_n)\atop a_h\sim \mathcal{U}}\left[\hat{\alpha}_n^2 \left\|\hphi_h^{(n)}(s_h,a_h)\right\|^2_{(\hat{U}_{h,\hphi}^{(n)})^{-1}}\right]\\
		& \overset{(\romannumeral1)}\leq n\mathop{\Eb}_{s_{h}\sim (P^\star, \Pi_n)\atop a_h\sim \mathcal{U}}\left[9 \hat{\alpha}_n^2 \left\|\hphi_h^{(n)}(s_h,a_h)\right\|^2_{({U}_{h,\hphi}^{(n)})^{-1}}\right]\\
		& = 9\hat{\alpha}_n^2 {\rm tr}\left\{n\mathop{\Eb}_{s_{h}\sim (P^\star,\Pi_n) \atop a_h\sim \mathcal{U}}\left[\hphi_h^{(n)}(s_h,a_h)\hphi_h^{(n)}(s_h,a_h)^\top\left(n\mathop{\Eb}_{s_h\sim(P^\star,\Pi_n) \atop a_h\sim \mathcal{U}}\left[\hphi_h(s_h,a_h)\hphi_h^{(n)}(s_h,a_h)^\top\right] + \lambda_n I\right)^{-1}\right]\right\}\\
		& \leq 9\hat{\alpha}_n^2 {\rm tr}(I) = 9 \hat{\alpha}_n^2  d,
	\end{align*}
	where $(\romannumeral1)$ follows from \Cref{lemma:high_prob_event}, and we use ${\rm tr}(A)$ to denote the trace of any matrix $A$.
	
	Hence,
	\begin{align*}
		\mathop{\Eb}_{s_{h}\sim P_{h-1}^{*} \atop a_{h}\sim \pi}\left[\hb_{h}^{(n)}(s_{h},a_{h})\bigg|s_{h-1},a_{h-1}\right]&\leq  \left\|\phi_{h-1}^{*}(s_{h-1},a_{h-1})\right\|_{(W_{h-1,\phi^\star}^{(n)})^{-1}}\sqrt{ 9 {K}\hat{\alpha}_n^2 d+ \lambda_n d}\\
		&\leq  \gamma_n \left\|\phi_{h-1}^{*}(s_{h-1},a_{h-1})\right\|_{W_{h-1,\phi^\star}^{(n)})^{-1}},
	\end{align*}
	where the last inequality follows from that $\hat{\alpha}_n=5\alpha_n$ and the definition of $\gamma_n$ 
	In addition, for $h=1$, we have %\tcr{relation to the next equations?}
	
	\begin{equation*}
		\mathop{\Eb}_{a_1\sim {\pi_n}}\left[f_1^{(n)}(s_1,a_1)\right]
		\overset{(\romannumeral1)}{\leq} \sqrt{{K} \mathop{\Eb}_{a_1\sim \Uc}\left[f_1^{(n)}(s_1,a_1)^2\right]} \leq \sqrt{{K} \zeta_n},
	\end{equation*}
	and
	\begin{align*}
		\mathop{\Eb}_{a_1\sim {\pi_n}}\left[\hb(s_1,a_1)\right]
		&\overset{(\romannumeral2)}{\leq}
		\hat{\alpha}_n\sqrt{{K} \mathop{\Eb}_{a_1\sim \Uc}\left[\|\hphi_1(s_1,a_1)\|^2_{(\hat{U}_{1,\hphi}^{(n)})^{-1}}\right]}\\
		&\leq 3\hat{\alpha}_n\sqrt{{K} \mathop{\Eb}_{a_1\sim \Uc}\left[\|\hphi_1(s_1,a_1)\|^2_{(U_{1,\hphi}^{(n)})^{-1}}\right]}\\
		&\leq 3\sqrt{\frac{25{K} \alpha_n^2  d}{n}} = 15\alpha_n\sqrt{\frac{dK}{n}},
	\end{align*}
	
	
	
\end{proof}

\subsection{Proof of \Cref{prop:step1}}\label{subsec:reward_free_proof}

Equipped with \Cref{lemma:Bound_TV}, the following proposition provides an upper bound on the difference of value functions under the estimated model $\hP^{(n)}$ and the true model $P^\star$ for any given policy $\pi$ and reward $r$.
\begin{proposition}[Restatement of \Cref{prop:step1}]
	\label{prop:step1_appendix}
	%\label{lemma:lowrank_errorbound}
	For all $n\in[N]$, policy $\pi$ and reward $r$, given that the event $\mathcal{E}$ occurs, we have
	\begin{equation*}
		\left| V_{P^\star,r}^{\pi} - V_{\hP^{(n)},r}^{\pi}\right| \leq   \hat{V}_{\hP^{(n)},\hb^{(n)}}^{\pi}+\sqrt{K \zeta_n}.
	\end{equation*}
\end{proposition}


\begin{proof}
	
	
	\textbf{Step 1. } We first show that $\left| V_{P^\star,r}^{\pi} - V_{\hP^{(n)},r}^{\pi}\right| \leq   \hat{V}_{\hP^{(n)},f^{(n)}}^{\pi}$.
	
	
	Recall the definition of estimated value functions $\hat{V}_{h,\hP^{(n)} , r} (s_h)$ and $\hat{Q}_{h,\hP^{(n)}, r}(s_h,a_h)$:
	\begin{align*}
		&\hat{Q}_{h,\hP^{(n)},r}^{\pi}(s_h,a_h) = \min\left\{1, r_h(s_h,a_h) + \hP_h^{(n)}\hV_{h+1,\hP^{(n)},r}^{\pi}(s_h,a_h)\right\},\\
		&\hat{V}_{h,\hP^{(n)},r}^{\pi}(s_h) =  \mathop{\Eb}_{\pi}\left[\hat{Q}^{\pi}_{h,\hP^{(n)},r}(s_h,a_h)\right].
	\end{align*}
	We develop the proof by induction. For the base case $h=H+1$, we have $\left|V_{H+1,\hP^{(n)},r}^{\pi}(s_{H+1}) - V_{H+1,P^\star,r}^{\pi}(s_{H+1}) \right| = 0  = \hat{V}_{H+1,\hP^{(n)},f^{(n)}}^{\pi}(s_{H+1})$.
	
	Assume that $\left|V_{h+1,\hP^{(n)},r}^{\pi}(s_{h+1}) - V^{\pi}_{h+1,P^\star,r}(s_{h+1}) \right| \leq \hV_{h+1,\hP^{(n)},f^{(n)}}^{\pi}(s_{h+1})$ holds for any $s_{h+1}$.
	
	Then, from Bellman equation, we have,
	\begin{align}
		\bigg|Q^{\pi}_{h,\hP^{(n)},r}&(s_h,a_h) - Q^{\pi}_{h,P^\star,r}(s_h,a_h) \bigg| \nonumber\\
		& = \bigg|\hP_h^{(n)}V^{\pi}_{h,\hP^{(n)},r}(s_h,a_h) - P_h^\star V^{\pi}_{h+1,P^\star,r}(s_h,a_h)\bigg| \nonumber\\
		& = \bigg| \hP_h^{(n)}\left(V_{h+1,\hP^{(n)},r}^{\pi} - V^{\pi}_{h+1,P^\star,r}\right)(s_h,a_h) + \left(\hP_h^{(n)} - P_h^\star\right)V^{\pi}_{h,P^\star,r}(s_h,a_h)\bigg| \nonumber\\
		&\overset{(\romannumeral1)}\leq \min\bigg\{ 1, f_h^{(n)}(s_h,a_h) + \hP_h^{(n)} \bigg|V_{h+1,\hP^{(n)},r}^{\pi} - V^{\pi}_{h+1,P^\star,r}\bigg|(s_h,a_h) \bigg\}\nonumber\\
		&\overset{(\romannumeral2)}\leq \min\bigg\{ 1, f_h^{(n)}(s_h,a_h) + \hP_h^{(n)} \hV_{h+1,\hP^{(n)},f^{(n)}}^{\pi}(s_h,a_h) \bigg\}\nonumber\\
		& = \hat{Q}_{h,\hP^{(n)},f^{(n)}}^{\pi}(s_h,a_h), \label{eqn:lowrank:hatQ-Q<hatQ}
	\end{align}
	where $(\romannumeral1)$ follows from  the (action) value function is at most 1, and $(\romannumeral2)$ follows from the induction hypothesis.
	
	Then, by the definition of $\hV^{\pi}_{h,\hP^{(n)},r}(s_h)$, we have
	\begin{align*}
		\bigg| V_{h, \hP^{(n)},r}^{\pi}&(s_h) - V_{h, P^\star,r}^{\pi}(s_h)\bigg|\\
		& = \bigg| \mathop{\Eb}_{\pi}\left[Q_{h,\hP^{(n)}, r}^{\pi} (s_h,a_h)\right] - \mathop{\Eb}_{\pi}\left[Q_{h,P^\star,r}^{\pi}(s_h,a_h)\right]\bigg| \\
		& \leq   \mathop{\Eb}_{\pi}\left[\bigg|Q_{h,\hP^{(n)}, r}^{\pi} (s_h,a_h) - Q_{h,P^\star,r}^{\pi}(s_h,a_h)\bigg|\right] \\
		&\overset{(\romannumeral1)}\leq  \mathop{\Eb}_{\pi}\left[\hat{Q}_{h,\hP^{(n)}, f^{(n)}}^{\pi} (s_h,a_h) \right] \\
		&= \hV_{h,\hP^{(n)},f^{(n)}}^{\pi}(s_h),
	\end{align*}
	where $(\romannumeral1)$ follows from \Cref{eqn:lowrank:hatQ-Q<hatQ}.
	
	Therefore, by induction, we have 
	\begin{align*}
		\left| V_{P^\star,r}^{\pi}-V_{\hP^{(n)},r}^{\pi}\right|\leq
		\hV_{\hP^{(n)},f^{(n)}}^{\pi}.
	\end{align*}
	
	
	
	\textbf{Step 2.} Then, we show that $\hV_{\hP^{(n)},f^{(n)}}^{\pi}\leq \hV_{\hP^{(n)},\hb^{(n)}}^{\pi}+\sqrt{K \zeta_n}$.
	
	By \Cref{ineq:hP_Bound_TV} and the fact that the total variation distance is upper bounded by 1,  with probability at least $1-\delta/2$, we have 
	\begin{align}
		\mathop{\Eb}_{\hP^{(n)},\pi}\left[f_h^{(n)}(s_h,a_h)\bigg| s_{h-1}\right] \leq \mathop{\Eb}_{a_{h-1} \sim \pi }\left[\min\left(\alpha_n\left\|\hphi_{h-1}^{(n)}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}}, 1\right) \right], \forall h\geq2.\label{ineq: f_1 bound}
	\end{align}
	Similarly, when $h=1$, 
	\begin{align}
		\mathop{\Eb}_{a_1\sim\pi}\left[f_1^{(n)}(s_1,a_1)\right]\leq \sqrt{K\mathop{\Eb}_{a\sim \Uc}\left[\left(f_1^{(n)}(s_1,a_1)\right)^2\right]}\leq\sqrt{K\zeta_n}.\label{ineq:f1<K_zeta}
	\end{align}
	Based on \Cref{coro:concentration on b}, \Cref{ineq: f_1 bound} and $\alpha_n = 5 \hat{\alpha}_n$, we have
	\begin{align}
		\mathop{\Eb}_{\pi} \left[\hb^{(n)}_h(s_h,a_h)\bigg|s_h\right] \geq \mathop{\Eb}_{\pi}\left[\min\left(\alpha_n\left\|\hphi_{h}^{(n)}(s_h,a_h)\right\|_{(U_{h,\hphi}^{(n)})^{-1}}, 1\right) \right] \geq \mathop{\Eb}_{\hP^{(n)},\pi }\left[f_{h+1}^{(n)}(s_{h+1},a_{h+1})\bigg|s_h\right] .\label{ineq:f<b}
	\end{align}
	
	
	For the base case $h= H$, we have 
	
	\begin{align*}
		\mathop{\Eb}_{\hP^{(n)},\pi}\left[\hV_{H,\hP^{(n)},f^{(n)}}^{\pi}(s_H)\bigg|s_{H-1}\right] 
		&= \mathop{\Eb}_{\hP^{(n)}, \pi}\left[f_H^{(n)}(s_H,a_H)\bigg| s_{H-1}\right]\\
		&\leq \mathop{\Eb}_{\pi}\left[b_{H-1}^{(n)}(s_{H-1},a_{H-1})|s_{H-1}\right]\\
		&\leq \min\left\{1, \mathop{\Eb}_{\pi}\left[\hat{Q}_{H-1,\hP^{(n)},\hb^{(n)}}^{\pi}(s_{H-1},a_{H-1})\bigg|s_{H-1}\right]\right\}\\
		& = \hV^{\pi}_{H-1,\hP^{(n)},\hb^{(n)}}(s_{H-1}).
	\end{align*}
	
	Assume that $\mathop{\Eb}_{\hP^{(n)}, \pi}\left[\hV_{h+1,\hP^{(n)},f^{(n)}}^{\pi}(s_{h+1})\bigg|s_{h}\right]\leq \hV_{h,\hP^{(n)}, \hb^{(n)}}^{\pi}(s_h)$ holds for step $h+1$. Then, by Jensen's inequality, we obtain
	\begin{align*}
		\mathop{\Eb}_{\hP^{(n)}, \pi}&\bigg[\hV_{h,\hP^{(n)},f^{(n)}}^{\pi}(s_h)\bigg|s_{h-1}\bigg] \\
		& \leq \min\left\{1, \mathop{\Eb}_{\hP^{(n)}, \pi}\left[ f_h^{(n)}(s_h,a_h) + \hP_h^{(n)}\hV_{h+1,\hP^{(n)},f^{(n)}}^{\pi}(s_h,a_h)\bigg|s_{h-1}\right]\right\}\\
		& \overset{(\romannumeral1)}\leq \min\left\{1, \mathop{\Eb}_{\pi}\left[ \hb_{h-1}^{(n)}(s_{h-1},a_{h-1})\right] + \mathop{\Eb}_{\hP^{(n)},\pi}\left[\mathop{\Eb}_{\hP^{(n)},\pi} \left[ \hV_{h+1,\hP^{(n)},f^{(n)}}^{\pi}(s_{h+1})\bigg|s_{h}\right]\bigg|s_{h-1}\right]\right\}\\
		& \overset{(\romannumeral2)}\leq \min\left\{1, \mathop{\Eb}_{\pi}\left[ b_{h-1}^{(n)}(s_{h-1},a_{h-1})\right] + \mathop{\Eb}_{\hP^{(n)},\pi} \left[ \hV_{h,\hP^{(n)},\hb^{(n)}}^{\pi}(s_{h})\bigg|s_{h-1}\right]\right\}\\
		& = \min\left\{1, \mathop{\Eb}_{\pi}\left[\hat{Q}_{h-1,\hP^{(n)},\hb^{(n)}}^{\pi}(s_{h-1},a_{h-1})\right] \right\}\\
		& = \hV^{\pi}_{h-1,\hP^{(n)},\hb^{(n)}}(s_{h-1}),
	\end{align*}
	where $(\romannumeral1)$ follows from \Cref{ineq:f<b}, and $(\romannumeral2)$ is due to the induction hypothesis.
	
	
	By induction, we conclude that
	\begin{align*}
		\hV_{\hP^{(n)},f^{(n)}}^{\pi} & = \mathop{\Eb}_{\pi}\left[f_1^{(s)}(s_1,a_1)\right] + \mathop{\Eb}_{\hP^{(n)},\pi}\left[\hV_{2,\hP^{(n)},f^{(n)}}^{\pi}(s_2)\bigg|s_1\right]\\
		&\leq \sqrt{K\zeta_n} + \hV_{\hP^{(n)},\hb^{(n)}}^{\pi}.
	\end{align*}
	
	Combining Step 1 and Step 2, we conclude that
	\[\left| V_{P^\star,r}^{\pi} - V_{\hP^{(n)},r}^{\pi}\right|\leq \sqrt{K\zeta_n} + \hV_{\hP^{(n)},\hb^{(n)}}^{\pi}.\]
	
	
	
	
	
	
\end{proof}



\subsection{Proof of \Cref{prop:step2}}\label{sec: A.3}
The following lemma is key to ensure that RAFFLE terminates in finite episodes.
\begin{proposition}[Restatement of \Cref{prop:step2}]
	\label{prop:step2_appendix}
	Given that the event $\Ec$ occurs, $\zeta=\log\left(2|\Phi||\Psi|NH/\delta\right)$ the summation of the truncated value functions $\hV^{{\pi_n}}_{\hP^{(n)},\hb^{(n)}}$ under exploration policies $\{{\pi_n}\}_{n\in[N]}$ is sublinear, i.e., the following bound holds:
	\begin{align*}
		\sum_{n\in[N]} \hV_{\hP^{(n)},\hb^{(n)}}^{{\pi_n}} + \sqrt{{K}\zeta_n}  \leq 32\zeta Hd\sqrt{\beta_3{K}(d^2+{K})N}.
	\end{align*}
\end{proposition}

\begin{proof}
	Note that $\hV_{h,\hP^{(n)} , \hb^{(n)}}^{\pi}\leq 1$ holds for any policy $\pi$ and $h\in[H]$. We first have 
	\begin{align*}
		\hV_{\hP^{(n)} , \hb^{(n)}}^{{\pi_n}} - V^{{\pi_n}}_{P^\star,\hb^{(n)}}& \leq
		\mathop{\Eb}_{{\pi_n}}\left[\hP^{(n)}_{1}\hV^{{\pi_n}}_{2,\hP^{(n)},\hb^{(n)}}(s_1,a_1) - P^\star_1 V^{{\pi_n}}_{2,P^\star,\hb^{(n)}}(s_1,a_1)\right]\\
		& = \mathop{\Eb}_{{\pi_n}}\left[\left(\hP^{(n)}_{1} - P^\star_1\right)\hV^{{\pi_n}}_{2,\hP^{(n)},\hb^{(n)}}(s_1,a_1) + P^\star_1\left(\hV^{{\pi_n}}_{2,\hP^{(n)},\hb^{(n)}} - V^{{\pi_n}}_{2,P^\star,\hb^{(n)}}\right)(s_1,a_1)\right]\\
		&\leq \mathop{\Eb}_{{\pi_n}}\left[f_1^{(n)}(s_1,a_1) + P^\star_1\left(\hV^{{\pi_n}}_{2,\hP^{(n)},\hb^{(n)}} - V^{{\pi_n}}_{2,P^\star,\hb^{(n)}}\right)\right]\\
		&\leq \ldots\\
		&\leq {\Eb}_{(s_h,a_h) \sim (P^\star, {\pi_n})}\left[\sum_{h=1}^H f^{(n)}(s_h,a_h)\right] = V^{{\pi_n}}_{P^\star,f^{(n)}},
	\end{align*}
	
	which implies
	$
	\hV_{\hP^{(n)} , \hb^{(n)}}^{{\pi_n}} \leq  V^{{\pi_n}}_{P^\star,\hb^{(n)}} + V^{{\pi_n}}_{P^\star,f^{(n)}}.
	$
	
	
	
	
	Applying the \Cref{ineq:P*_Bound_bn} and \Cref{ineq:Step_1_Bound}, we obtain the following bound on the value function $V_{P^\star, {\hb}^{(n)}}^{{\pi}_n}$:
	\begin{align*}
		V_{P^\star, {\hb}^{(n)}}^{{\pi}_n}
		& = \sum_{h=1}^{H} \mathop{\Eb}_{s_h\sim (P^\star, {\pi_n}) \atop a_h\sim{\pi_n}}\left[\hat{b}_n(s_h,a_h)\right]\\
		& \leq \sum_{h=2}^{H} \mathop{\Eb}_{s_{h-1}\sim (P^\star, {{\pi_n}}) \atop a_{h-1}\sim{\pi_n}} 
		\left[\gamma_n\left\|\phi_{h-1}^\star(s_{h-1},a_{h-1})\right\|_{(W_{h-1,\Pshi}^{(n)})^{-1}} \right]
		+  15 \alpha_n\sqrt{\frac{d{K}}{n}}\\
		&\leq\sum_{h=1}^{H} \mathop{\Eb}_{s_{h}\sim (P^\star, {\pi_n}) \atop a_{h} \sim {\pi_n}} \left[\gamma_n\left\|\phi_{h}^\star(s_{h},a_{h})\right\|_{(W_{h,\Pshi}^{(n)})^{-1}}
		\right]
		+ 15 \alpha_n\sqrt{\frac{d{K}}{n}}.
	\end{align*}
	
	
	Similarly, we obtain
	\begin{align*}
		V_{P^\star, f^{(n)}}^{{\pi}_n}
		& = \sum_{h=1}^{H} \mathop{\Eb}_{s_h\sim (P^\star, {\pi_n}) \atop a_h\sim{\pi_n}}\left[f_h^{(n)}(s_h,a_h)\right]\\
		& \leq \sum_{h=2}^{H} \mathop{\Eb}_{s_{h-1}\sim (P^\star, {{\pi_n}}) \atop a_{h-1}\sim{\pi_n}} 
		\left[\alpha_n\left\|\phi_{h-1}^\star(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\Pshi}^{(n)})^{-1}} \right]
		+  \sqrt{{K}\zeta_n}\\
		&\leq\sum_{h=1}^{H} \mathop{\Eb}_{s_{h}\sim (P^\star, {\pi_n}) \atop a_{h} \sim {\pi_n}} \left[\alpha_n\left\|\phi_{h}^\star(s_{h},a_{h})\right\|_{(U_{h,\Pshi}^{(n)})^{-1}}
		\right]
		+ \sqrt{{K}\zeta_n}.
	\end{align*}
	
	Then, taking the summation of $V_{P^\star, {\hb}^{(n)}+f^{(n)}}^{{\pi}_n}$ over $n\in[N]$, we have
	\begin{align*}
		\sum_{n\in[N]}&V_{P^\star, f^{(n)} + \hb^{(n)}}^{{\pi}_n} + \sqrt{{K}\zeta_n}\\
		&\leq \sum_{n\in[N]}15\alpha_n\sqrt{\frac{d{K}}{n}} + 2\sum_{n\in[N]}\sqrt{K\zeta_n}  + \sum_{n\in[N]}\sum_{h=1}^{H} \mathop{\Eb}_{s_{h}\sim (P^\star, {\pi_n}) \atop a_{h} \sim {\pi_n}} \left[\gamma_n\left\|\phi_{h}^\star(s_{h},a_{h})\right\|_{(W_{h,\Pshi}^{(n)})^{-1}}
		\right] \\
		& \quad + \sum_{n\in[N]} \sum_{h=1}^{H} \mathop{\Eb}_{s_{h}\sim (P^\star, {\pi_n}) \atop a_{h} \sim {\pi_n}} \left[\alpha_n\left\|\phi_{h}^\star(s_{h},a_{h})\right\|_{(U_{h,\Pshi}^{(n)})^{-1}}
		\right]\\
		&\overset{(\romannumeral1)}{\leq} 17\alpha_N\sqrt{d{K} N} + \gamma_N\sum_{h=1}^{H}\sqrt{N \sum_{n\in [N]}\mathop{\Eb}_{s_{h}\sim(P^\star,{\pi_n}) \atop a_h\sim{\pi_n}}\left[\left\|\phi_{h}^\star(s_{h},a_{h})\right\|^2_{(W_{h,\Pshi}^{(n)})^{-1}}\right]}\\
		&\quad + \alpha_N\sum_{h=1}^{H}\sqrt{{K} N \sum_{n\in [N]}\mathop{\Eb}_{s_{h}\sim(P^\star,{\pi_n}) \atop a_h\sim \Uc}\left[\left\|\phi_{h}^\star(s_{h},a_{h})\right\|^2_{(U_{h,\Pshi}^{(n)})^{-1}}\right]}\\
		&\overset{(\romannumeral2)}{\leq} 17\sqrt{\zeta}\sqrt{2\beta_3 d{K}({K}+d^2)N } + H\sqrt{45\beta_3\zeta d{K}({K}+d^2)}\sqrt{d N\zeta}\\
		& \quad + H\sqrt{\beta_3\zeta({K}+d^2)}\sqrt{d{K} N \zeta}\\
		& \leq 32\zeta Hd\sqrt{\beta_3{K}(d^2+{K})N},
	\end{align*}
	where $(\romannumeral1)$ follows from  Cauchy-Schwarz inequality and importance sampling, and $(\romannumeral2)$ follows from \Cref{lemma: Elliptical_potential}.
	Hence, the statement of \Cref{prop:step2_appendix} is verified.
\end{proof}






\subsection{Proof of \Cref{prop:step3}}

Based on \Cref{prop:step2_appendix}, we argue that with enough number of iterations, RAFFLE can find $\hP^{\epsilon}$ satisfying the condition in line 15 of \Cref{Algorithm: RAFFLE}.

\begin{proposition}
	[Restatement of \Cref{prop:step3}] \label{prop:step3_appendix}
	Fix any $\delta \in (0,1), \epsilon >0$. Suppose the algorithm runs for $N=\frac{2^{14}\beta_3H^2d^2K(d^2+K)\log^2 (2|\Phi||\Psi|H^3d^2K(d^2+K)/(\delta\epsilon^2))}{\epsilon^2}$ iterations, with probability at least $1-\delta$, RAFFLE can find an $n_\epsilon \leq N$ in the exploration phase such that $2\hat{V}^{\pi_{n_\epsilon}}_{\hP^{(n_\epsilon)},\hb^{(n_\epsilon)}}+2\sqrt{K\zeta_{n_\epsilon}}\leq \epsilon$. In other words, \Cref{Algorithm: RAFFLE} can output $\hP^{\epsilon}=\hP^{(n_\epsilon)}$ satisfying the condition in line 15. In addition 
	\begin{align*}
		\left|V_{P^\star,r}^{\pi} - V_{\hP^{(n_\epsilon)}, r}^{\pi}\right|\leq \epsilon/2.
	\end{align*}
\end{proposition}
\begin{proof}
	
	
	We show that the algorithm terminates by contradiction. If it does not stop, applying \Cref{prop:step2_appendix}, we have
	\begin{align*}
		\epsilon N/2&< \sum_{n\in[N]}\left(\hV_{\hP^{(n)},\hb^{(n)}}^{\pi_n} + \sqrt{K\zeta_n}\right)\\
		&\leq 32\zeta Hd\sqrt{\beta_3{K}(d^2+{K})N}.
	\end{align*}
	
	Therefore,
	\[N < \frac{2^{12}\beta_3H^2d^2K(d^2+K)\zeta^2}{\epsilon^2}\]
	Recall $\zeta=N\zeta_N = \log\left(2|\Phi||\Psi|NH/\delta\right)$. Using the fact that $ n \leq c \log^2 (\alpha_n n) \Rightarrow n \leq 4c\log^2 (\alpha_n c), \forall c \geq e^2, n \geq 1, \alpha_n  \in \Rb^+$, it can be concluded that
	\begin{align*}
		N < \frac{2^{14}\beta_3H^2d^2K(d^2+K)\log^2 (2|\Phi||\Psi|H^3d^2K(d^2+K)/(\delta\epsilon^2))}{\epsilon^2},
	\end{align*}
	which is a contradiction. 
	
	Therefore, there exists an $n_{\epsilon} = O(\frac{H^2d^2K(d^2+K)\log^2 (|\Phi||\Psi|H^3d^2K(d^2+K)/(\delta\epsilon^2))}{\epsilon^2})$ such that $\hP^{\epsilon}=\hP^{(n_\epsilon)}$ satisfies
	\begin{align*}
		2\hat{V}^{\pi_{n_\epsilon}}_{\hP^{(n_\epsilon)},\hb^{(n_\epsilon)}}+2\sqrt{K\zeta_{n_\epsilon}}\leq \epsilon.
	\end{align*} 
	Combining \Cref{prop:step1_appendix}, we finish the proof.
	
	
\end{proof}






\begin{proof}[Proof of \Cref{thm1: reward-free sample complexity}]
	Recall that $\hP^\epsilon$ is the output of RAFFLE in the $n_\epsilon$-iteration. Then, by \Cref{prop:step3_appendix}
	\begin{align*}
		V^\star_{P^\star, r} &- V^{\bpi}_{P^\star, r}\\
		&\leq V_{\hat{P}^{\epsilon}, r}^{\pi^\star}-V_{P^\star, r}^{\Bar{\pi}}+\epsilon/2\nonumber\\
		&\overset{(\romannumeral1)}{\leq} V_{\hat{P}^\epsilon, r}^{\bar{\pi}}-V_{P^\star, r}^{\bar{\pi}}+\epsilon/2\nonumber\\
		&\leq 
		\epsilon/2+\epsilon/2\nonumber\\
		&= \epsilon,
	\end{align*}
	where $(\romannumeral1)$ follows from the definition of $\bpi$.
	The number of trajectories $n_{\epsilon} H $ is at at most
	\[ O\left(\frac{H^3d^2K(d^2+K)\log^2 (|\Phi||\Psi|H^3d^2K(d^2+K)/(\delta\epsilon^2))}{\epsilon^2}\right)\]
\end{proof}









%%%%%%%%%%%%%%%%%%%
\iffalse

\section{Proof of \Cref{thm1: reward-free sample complexity}}\label{sec: A}
We summarize the notations we frequently use throughout the appendices here.

We define $\Pi_n$ to be a uniform mixture of previous $n-1$ exploration policies:
\begin{align*}
	\Pi_n = \Uc(\pi_1,...,\pi_{n-1}).
\end{align*}
We also define two matrices as follows:
\begin{align*}
	&U_{h,\phi}^{(n)} = n\Eb_{s_h\sim(P^\star,\Pi_n),a_h\sim \Uc(\Ac)}\left[\phi(s_h,a_h)(\phi(s_h,a_h))^\top\right] + \lambda_n_n I,\\
	&W_{h,\phi}^{(n)} = n\Eb_{(s_h,a_h)\sim(P^\star,\Pi_n)}\left[\phi(s_h,a_h)(\phi(s_h,a_h))^\top\right] + \lambda_n_n I.
\end{align*}
\subsection{Supporting Lemmas}
Recall $U_{h,\phi}^{(n)} = n\Eb_{s_h\sim(P^\star,\Pi_n),a_h\sim \Uc(\Ac)}\left[\phi(s_h,a_h)(\phi(s_h,a_h))^\top\right] + \lambda_n_n I$. Then $U_{h,\hphi}^{(n)}$ is obviously the counterpart of $\hat{U}_h^{(n)}$ in expectation. The next lemma provides concentration of the bonus term. See Lemma 39 in \citet{zanette2020learning} for the version of fixed $\phi$ and Lemma 11 in \citet{DBLP:conf/iclr/UeharaZS22}.


\begin{lemma}\label{lemma: concentration of the bonus term}
	({\rm Concentration of the bonus term}). Set $\lambda_n_n = \Theta(d\log(2nH|\Phi|/\delta))$ for any $n$.
	With probability $1-\delta/2$, we have that 
	$\forall n \in \mathbb{N}^+, h \in [H], \phi \in \Phi, $                                            
	\begin{equation*}
		\beta_1 \left\|\hphi_{h-1}^{(n)}(s,a)\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}} \leq \left\|\hphi_{h-1}^{(n)}(s,a)\right\|_{(\hat{U}_{h-1}^{(n)})^{-1}} \leq \beta_2 \left\|\hphi_{h-1}^{(n)}(s,a)\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}}.
	\end{equation*}
\end{lemma}


Recall $\hb_h^{(n)}(s_h,a_h)=\min\left\{\tilde{\alpha_n}_n\left\|\hphi_{h}^{(n)}(s,a)\right\|_{(\hat{U}_{h}^{(n)})^{-1}},1\right\}$. Setting $\hat{\alpha}_n=\frac{\alpha_n_n} {\beta_1}$ and applying \Cref{lemma: concentration of the bonus term}, we can immediately obtain the following corollary.

\begin{corollary} \label{coro: concentration of the bonus term}
	Fix $\delta  \in (0,1)$, under the same setting of \Cref{lemma: concentration of the bonus term}, with probability at least $1-\delta/2$, we have that $\forall n \in \mathbb{N}^+, h \in [H], \phi \in \Phi,$
	\begin{equation*}
		\min\left\{\alpha_n_n \left\|\hphi_{h}^{(n)}(s_{h},a_{h})\right\|_{(U_{h,\hphi}^{(n)})^{-1}},1\right\} \leq \hb_h^{(n)}(s_h,a_h) \leq \frac{\beta_2}{\beta_1} \alpha_n_n \left\|\hphi_{h}^{(n)}(s_{h},a_{h})\right\|_{(U_{h,\hphi}^{(n)})^{-1}}.
	\end{equation*}
\end{corollary}



Similarly to the idea in \citet{DBLP:conf/iclr/UeharaZS22,NEURIPS2020_e894d787}, we extend the one-step back inequality to non-stationary transition kernels with finite horizon. For any function $g \in \Sc \times \Ac \rightarrow \Rb$, policy $\pi$ and transition kernel $P$, with the help of the following lemma, we can upper bound the expectation $\scriptstyle \mathop{\Eb}_{s_h \sim (P, {\pi}) \atop a_h \sim \pi}[g(s_h,a_h)]$ by the product of two terms. The first term represents the convergence guarantee of $g(s_h,a_h)$ following other policies, which is $\scriptstyle \mathop{\Eb}_{s_{h}\sim(P^\star,\Pi)\atop a_h \sim \Uc(\Ac) }[g^2(s_h,a_h)]$. The second term can be described as the distribution shift coefficient $\scriptstyle \mathop{\Eb}_{s_{h-1}\sim (P, \pi) \atop a_{h-1} \sim \pi}\left[\left\|\phi_{h-1}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\phi})^{-1}}\right]$, which measures the difference caused by distribution shift from $\pi$ to other policies.


\begin{lemma}\label{lemma:Step_Back}
	{\rm (One-step back inequality)}. Let $P\in\{\hP^{(n)},P^\star\}$ be a generic MDP model, and $U_{h,\phi} = \lambda_n I + n\Eb_{s_h,a_h\sim (P^\star,\Pi)}[\phi\phi^\top]\in\{U_{h,\phi}^{(n)}, W_{h,\phi}^{(n)}\}$ be the covariance matrix following a generic policy $\Pi$ under the true environment $P^\star$. Note that $\phi\in\{\hphi^{(n)},\phi^\star\}$. Further, let $f(s_h,a_h)$ be the total variation between $P^\star$ and $P$ at time step $h$. Take any $g \in \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ such that $\|g\|_\infty \leq B$. Then, $\forall h \geq 2, \forall\, {\rm policy }\,\pi$,
	\begin{align*}
		\mathop{\Eb}_{s_h \sim (P, {\pi}) \atop a_h \sim \pi}[g(s_h,a_h)]  &\leq \mathop{\Eb}_{s_{h-1}\sim (P, \pi) \atop a_{h-1} \sim \pi}\left[\left\|\phi_{h-1}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\phi})^{-1}} \times\right.\nonumber\\
		&\quad\left.
		\sqrt{nK\mathop{\Eb}_{s_{h}\sim(P^\star,\Pi)\atop a_h \sim \Uc(\Ac) }[g^2(s_h,a_h)]+\lambda_n dB^2 + nB^2\mathop{\Eb}_{s_{h-1}\sim(P^\star,\Pi)\atop a_{h-1}\sim\Pi }\left[f(s_{h-1},a_{h-1})^2\right]}\right].
	\end{align*}
\end{lemma}


\begin{proof}
	First, we have
	\begin{align*}
		&\quad \ \mathop{\Eb}_{s_h\sim(P,\pi) \atop a_h \sim \pi}\left[ 
		g(s_h,a_h)\right]\nonumber\\
		&=\mathop{\Eb}_{s_{h-1} \sim (P,\pi) \atop a_{h-1} \sim \pi} \left[\int_{s_h}\sum_{a_h}g(s_h,a_h)\pi(a_h|s_h)\langle\phi_{h-1}(s_{h-1},a_{h-1}),\mu_{h-1}(s_h)\rangle d{s_h}\right]\\
		& \leq\mathop{\Eb}_{s_{h-1} \sim (P^\star, \pi) \atop a_{h-1}\sim\pi} \left[\left\|\phi_{h-1}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\phi})^{-1}}\left\|\int\sum_{a_h}g(s_h,a_h)\pi(a_h|s_h)\mu_{h-1}(s_h)d{s_h}\right\|_{U_{h-1,\phi}}\right],
	\end{align*}
	where the inequality follows from Cauchy's inequality. We further develop the following bound:
	\begin{align*}
		&\hspace{-5mm} \left\|\int\sum_{a_h}g(s_h,a_h)\pi(a_h|s_h)\mu_{h-1}(s_h)d{s_h}\right\|_{U_{h-1,\phi}}^2\\
		& \overset{(\romannumeral1)}{\leq} n \mathop{\Eb}_{s_{h-1}\sim (P^\star,\Pi) \atop a_{h-1}\sim \Pi}
		\left[\left(\int_{s_h}\sum_{a_h}g(s_h,a_h)\pi(a_h|s_h)\mu(s_h)^\top\phi(s_{h-1},a_{h-1})d{s_h}\right)^2\right] + \lambda_n d B^2\\
		& \leq n \mathop{\Eb}_{s_{h-1}\sim(P^\star,\Pi) \atop a_{h-1} \sim \Pi } \left[\mathop{\Eb}_{s_h \sim P(\cdot|s_{h-1},a_{h-1}) \atop a_h \sim \pi} \left[g(s_h,a_h)^2\right]\right] + \lambda_n d B^2\\
		&\overset{(\romannumeral2)}{\leq} n \mathop{\Eb}_{s_{h-1}\sim(P^\star,\Pi) \atop a_{h-1}\sim\Pi }\left[\mathop{\Eb}_{s_h\sim P^\star \atop a_h\sim \pi}\left[g(s_h,a_h)^2\right]\right] + \lambda_n d B^2 + nB^2\mathop{\Eb}_{s_{h-1}\sim(P^\star,\Pi) \atop a_{h-1}\sim\Pi }\left[f(s_{h-1},a_{h-1})^2\right]\\
		&\overset{(\romannumeral3)}{\leq} n K\mathop{\Eb}_{s_{h}\sim(P^\star,\Pi)\atop a_h\sim \Uc(\Ac) }\left[g(s_h,a_h)^2\right] + \lambda_n d B^2 + nB^2\mathop{\Eb}_{s_{h-1}\sim(P^\star,\Pi) \atop a_{h-1}\sim \Pi }\left[f(s_{h-1},a_{h-1})^2\right],
	\end{align*}
	where $(\romannumeral1)$ follows from the assumption $\|g\|_{\infty}\leq B$, $(\romannumeral2)$ follows because $f(s_h,a_h)$ is the total variation between $P^\star$ and $P$ at time step $h$, and $(\romannumeral3)$ follows from importance sampling.
	This finishes the proof.
\end{proof}
Equipped with \Cref{lemma:Step_Back}, we can bound the conditional expectation of the total variation distance and the bonus term with true features or learned features, depending on whether the expectation is taken over true environment or learn environment. This helps us to bound the unknown total variation distance with a known term. 
\begin{lemma}\label{lemma:Bound_TV}
	%let \textcolor{red}{$\gamma_n_n = \sqrt{K\alpha_n_n^2\beta_2^2 d+\lambda_n_nd}=\tilde{O}(\sqrt{(K+d^2)Kd\beta_2^2\log(|\Phi||\Psi|nH/\delta)+d^2\log (|\Phi||\Psi|nH/\delta)})$, (note $\gamma_n_1 \leq \dots \leq \gamma_n_N = \sqrt{dK(d^2+K)\beta_2^2\log(|\Phi||\Psi|NH/\delta)}$)}.
	%and 
	%\textcolor{red}{${\alpha_n}_n$ = $\sqrt{n\zeta_n(K+1) + \lambda_n_n d} = \tilde{O}(\sqrt{(K+d^2)\log(|\Phi||\Psi|nH/\delta)})$\cy{$\alpha_n_1 \leq \dots \leq \alpha_n_N = \sqrt{(K+d^2)\log |\Phi||\Psi|NH/\delta)}$}.}
	Fix $\delta \in (0,1)$, with probability at least $1-\delta/2$, the following inequalities hold. 
	
	For any $n$, when $h \geq 2$, 
	\begin{align} 
		& \mathop{\Eb}_{s_{h}\sim\hP_{h-1}^{(n)}\atop a_{h}\sim \pi }\left[f_{h}^{(n)}(s_{h},a_{h})\bigg|s_{h-1},a_{h-1}\right]\leq \alpha_n_n \left\|\hphi_{h-1}^{(n)}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}},\label{ineq:hP_Bound_TV}\\
		& \mathop{\Eb}_{s_{h}\sim P_{h-1}^{*}\atop a_{h\sim \pi}} \left[f_{h}^{(n)}(s_{h},a_{h})\bigg|s_{h-1},a_{h-1}\right]\leq \alpha_n_n\left\|\phi_{h-1}^{*}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\phi^\star}^{(n)})^{-1}},\label{ineq:P*_Bound_TV}\\
		&\mathop{\Eb}_{s_{h}\sim P_{h-1}^{*} \atop a_{h}\sim \pi}\left[\hb_{h}^{(n)}(s_{h},a_{h})\bigg|s_{h-1},a_{h-1}\right]\leq \gamma_n_n \left\|\phi_{h-1}^{*}(s_{h-1},a_{h-1})\right\|_{(W_{h-1,\phi^\star}^{(n)})^{-1}}.\label{ineq:P*_Bound_bn}
	\end{align}
	Specially, when $h = 1$, 
	\begin{align}
		&\mathop{\Eb}_{a_1\sim \pi}\left[f_1^{(n)}(s_1,a_1)\right]
		\leq \sqrt{K\zeta_n}, 
		&\mathop{\Eb}_{a_1\sim \pi}\left[\hb(s_1,a_1)\right]
		\leq \alpha_n_n\beta_2\sqrt{\frac{dK}{n}},\label{ineq:Step_1_Bound}
	\end{align}
	where \begin{align*}
		{\alpha_n}_n& = \sqrt{n\zeta_n(K+1) + \lambda_n_n d} = {O}(\sqrt{(K+d^2)\log(2|\Phi||\Psi|nH/\delta)}), \\ \gamma_n_n &= \sqrt{K\alpha_n_n^2\beta_2^2 d+\lambda_n_nd}={O}(\sqrt{(K+d^2)Kd\beta_2^2\log(2|\Phi||\Psi|nH/\delta)+d^2\log (2|\Phi||\Psi|NH/\delta)}).
	\end{align*}
\end{lemma}

\begin{proof}
	
	We start by developing \Cref{ineq:hP_Bound_TV} as follows:
	\begin{align*}
		\mathop{\Eb}_{s_{h}\sim\hP_{h-1}^{(n)}\atop a_{h}\sim \pi }
		&\left[f_{h}^{(n)}(s_{h},a_{h})\bigg|s_{h-1},a_{h-1}\right] \\
		&\overset{(\romannumeral1)}{\leq} \left\|\hphi_{h-1}^{(n)}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}} \times \\
		&\sqrt{nK\mathop{\Eb}_{s_{h-1}\sim(P^\star,\Pi_n)\atop (a_{h-1},a_h) \sim \Uc(\Ac) }[f^{(n)}_h(s_h,a_h)^2]+\lambda_n dB^2 + nB^2\mathop{\Eb}_{s_{h-1}\sim(P^\star,\Pi_n)\atop (a_{h-2},a_{h-1})\sim\Uc(\Ac) }\left[f^{(n)}_{h-1}(s_{h-1},a_{h-1})^2\right]}\\
		& \overset{(\romannumeral2)}{\leq}\left\|\hphi_{h-1}^{(n)}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}}\sqrt{nK\zeta_n + \lambda_n_n d + n\zeta_n}\\
		&=\alpha_n_n\left\|\hphi_{h-1}^{(n)}(s_{h-1},a_{h-1})\right\|,
	\end{align*}
	where $(\romannumeral1)$ follows from \Cref{lemma:Step_Back} and $(\romannumeral2)$ follows from the MLE guarantee in \Cref{lemma:MLE} and the fact that $f_{h}^{(n)}(s_{h},a_{h}) \leq 1$.
	%
	\Cref{ineq:P*_Bound_TV} follows from similar arguments above.
	To obtain \Cref{ineq:P*_Bound_bn}, we first apply \Cref{lemma:Step_Back}.
	\begin{align*}  
		& \mathop{\Eb}_{s_{h}\sim P^\star_{h-1} \atop a_{h} \sim \pi_n }\left[\hat{b}^{(n)}_{h}(s_{h},a_{h})\bigg|s_{h-1},a_{h-1}\right]\\
		& \leq \left\|\phi_{h-1}^\star(s_{h-1},a_{h-1})\right\|_{(W_{h-1,\Pshi}^{(n)})^{-1}}
		\sqrt{nK\mathop{\Eb}_{s_{h}\sim (P^\star, {\Pi}_n)\atop a_h\sim \Uc(\Ac)}[\{\hat{b}_h^{(n)}(s_h,a_h)\}^2]+\lambda_n_nd },
	\end{align*}
	where we use the fact that $\hat{b}_h^{(n)}(s_h,a_h)\leq 1$. For the term $n\mathop{\Eb}_{s_{h}\sim (P^\star, {\Pi}_n)\atop a_h\sim \Uc(\Ac)}[\{\hat{b}_h^{(n)}(s_h,a_h)\}^2]$, we have
	\begin{align*}
		& \quad n\mathop{\Eb}_{s_{h}\sim (P^\star, {\Pi}_n)\atop a_h\sim \Uc(\Ac)}\left[\left\{\hat{b}_h^{(n)}(s_h,a_h)\right\}^2\right]\\
		& = n\mathop{\Eb}_{s_{h}\sim (P^\star, {\Pi}_n)\atop a_h\sim \Uc(\Ac)}\left[\alpha_n_n^2 \left\|\hphi_h^{(n)}(s_h,a_h)\right\|^2_{(\hat{U}_{h,\hphi}^{(n)})^{-1}}\right]\\
		& \overset{(\romannumeral1)}\leq n\mathop{\Eb}_{s_{h}\sim (P^\star, {\Pi}_n)\atop a_h\sim \Uc(\Ac)}\left[\alpha_n_n^2\beta_2^2 \left\|\hphi_h^{(n)}(s_h,a_h)\right\|^2_{({U}_{h,\hphi}^{(n)})^{-1}}\right]\\
		& = \alpha_n_n^2\beta_2^2  {\rm tr}\left\{n\mathop{\Eb}_{s_{h}\sim (P^\star,{\Pi}_n) \atop a_h\sim \Uc(\Ac)}\left[\hphi_h^{(n)}(s_h,a_h)\hphi_h^{(n)}(s_h,a_h)^\top\left(n\mathop{\Eb}_{s_h\sim(P^\star,\Pi_n) \atop a_h\sim \Uc(\Ac)}\left[\hphi_h(s_h,a_h)\hphi_h^{(n)}(s_h,a_h)^\top\right] + \lambda_n_n I_d\right)^{-1}\right]\right\}\\
		& \leq \alpha_n_n^2\beta_2^2 {\rm tr}(I_d) =\alpha_n_n^2\beta_2^2  d,
	\end{align*}
	where $(\romannumeral1)$ follows from \Cref{lemma: concentration of the bonus term} and for any matrix $A$, we use ${\rm tr}(A)$ to denote the trace of matrix $A$.
	
	Hence,
	\begin{align*}
		\mathop{\Eb}_{s_{h}\sim P_{h-1}^{*} \atop a_{h}\sim \pi}\left[\hb_{h}^{(n)}(s_{h},a_{h})\bigg|s_{h-1},a_{h-1}\right]&\leq  \left\|\phi_{h-1}^{*}(s_{h-1},a_{h-1})\right\|_{(W_{h-1,\phi^\star}^{(n)})^{-1}}\sqrt{K\alpha_n_n^2\beta_2^2 d+ \lambda_n_n d}\\
		&= \gamma_n_n \left\|\phi_{h-1}^{*}(s_{h-1},a_{h-1})\right\|_{(W_{h-1,\phi^\star}^{(n)})^{-1}}.
	\end{align*}
	In addition, for $h=1$, we have %\tcr{relation to the next equations?}
	
	\begin{equation*}
		\mathop{\Eb}_{a_1\sim \pi_n}\left[f_1^{(n)}(s_1,a_1)\right]
		\overset{(\romannumeral1)}{\leq} \sqrt{K\mathop{\Eb}_{a_h\sim \Uc(\Ac)}\left[f_1^{(n)}(s_1,a_1)^2\right]} \leq \sqrt{K\zeta_n},
	\end{equation*}
	and
	\begin{align*}
		\mathop{\Eb}_{a_1\sim \pi_n}\left[\hb(s_1,a_1)\right]
		&\overset{(\romannumeral2)}{\leq}
		\alpha_n_n\sqrt{K\mathop{\Eb}_{a_h\sim \Uc(\Ac)}\left[\|\hphi_1(s_1,a_1)\|^2_{(\hat{U}_{1,\hphi}^{(n)})^{-1}}\right]}\\
		&\leq \alpha_n_n\beta_2\sqrt{K\mathop{\Eb}_{a_h\sim \Uc(\Ac)}\left[\|\hphi_1(s_1,a_1)\|^2_{(U_{1,\hphi}^{(n)})^{-1}}\right]}
		\leq \sqrt{\frac{K\alpha_n_n^2\beta_2^2  d}{n}},
	\end{align*}
	where $(\romannumeral1)$ and $(\romannumeral2)$ follow from Jensen's inequality and important sampling.
\end{proof}




\iffalse
\begin{lemma}\label{prop:step3_appendix}
	Fix $\delta  \in (0,1)$, with probability at least $1-\delta/2$, the summation of value functions $V^{\pi_n}_{P^\star,\hb^{(n)}}$ under exploration policies $\{\pi_n\}_{n\in[N]}$ with exploration-driven reward functions $\hb^{(n)}$ is sublinear with respect to $|\Nc|$ for any $\Nc = {n_1, \dots, n_{|\Nc|}}\subset[N]$, i.e.
	\begin{align}
		\sum_{n\in\Nc} V_{P^{(n)},\hb^{(n)}}^{\pi_n}  \leq  4H(\sqrt{d}+H)\sqrt{dK(d^2+K)|\Nc|\log (2|\Phi||\Psi||\Nc|H/\delta)\log\left(1+\frac{|\Nc|}{d\lambda_n_1}\right)}.
	\end{align}
\end{lemma}


\begin{proof}
	
	
	We first bound the value function $V_{P^\star, {\hb}^{(n)}}^{{\pi}_n}$ by the \Cref{ineq:P*_Bound_bn} \Cref{ineq:Step_1_Bound}.
	\begin{align}
		V_{P^\star, {\hb}^{(n)}}^{{\pi}_n}
		& = \sum_{h=1}^{H} \mathop{\Eb}_{s_h\sim (P^\star, \pi_n) \atop a_h\sim\pi_n}\left[\hat{b}_n(s_h,a_h)\right]\\
		& \leq \sum_{h=2}^{H} \mathop{\Eb}_{s_{h-1}\sim (P^\star, {\pi_n}) \atop a_{h-1}\sim\pi_n} 
		\left[\gamma_n_n\left\|\phi_{h-1}^\star(s_{h-1},a_{h-1})\right\|_{(W_{h-1,\Pshi}^{(n)})^{-1}} \right]
		+  \alpha_n_n\beta_2\sqrt{\frac{dK}{n}}\\
		&\leq\sum_{h=1}^{H-1} \mathop{\Eb}_{s_{h}\sim (P^\star, \pi_n) \atop a_{h} \sim \pi_n} \left[\gamma_n_n\left\|\phi_{h}^\star(s_{h},a_{h})\right\|_{(W_{h,\Pshi}^{(n)})^{-1}}
		\right]
		+ \alpha_n_n\beta_2\sqrt{\frac{dK}{n}}.
	\end{align}
	
	Then, consider the difference between the value functions under learned transition dynamics and true transition dynamics, i.e. $\left|V_{\hat{P}_n, \hat{b}_n}^{{\pi}_n} - V_{P^\star, \hat{b}_n}^{{\pi}_n}\right|$. Note that $V_{h+1,\hat{P}_n, \hat{b}_n}^{{\pi}_n}(s_{h+1})\leq H, \forall h,s_{h+1}$. We have:
	
	\begin{align}
		\left|V_{\hat{P}_n, \hat{b}_n}^{{\pi}_n} - V_{P^\star, \hat{b}_n}^{{\pi}_n}\right| 
		&= \left|\mathop{\Eb}_{s_{h}\sim(P^\star,{\pi}_n) \atop a_h \sim \pi_n}\left[\sum_{h=1}^{H}	    \int(P^\star(s_{h+1}|s_h,a_h)-\hat{P}_n(s_{h+1}|s_h,a_h))V_{h+1,\hat{P}_n, \hat{b}_n}^{{\pi}_n}(s_{h+1})\right]\right| \\
		& \leq H\sum_{h=1}^{H}\mathop{\Eb}_{s_h\sim (P^\star, {\pi}_n), \atop a_h \sim \pi_n}\left[f_h^{(n)}(s_h,a_h)\right] \\
		&\leq H\sqrt{K\zeta_n} + H\sum_{h=2}^H\Eb_{s_{h-1}\sim(P^\star,\pi_n) \atop a_{h-1}\sim\pi_n} \left[\alpha_n_n\left\|\Pshi_{h-1}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\Pshi}^{(n)})^{-1}}\right]\\
		&\leq H\sqrt{K\zeta_n} + H\sum_{h=1}^{H-1}\Eb_{s_{h}\sim(P^\star,\pi_n) \atop a_{h}\sim\pi_n} \left[\alpha_n_n\left\|\Pshi_{h}(s_{h},a_{h})\right\|_{(U_{h,\Pshi}^{(n)})^{-1}}\right],
	\end{align}
	where the second inequality again follows \Cref{ineq:P*_Bound_TV}.
	
	Now plugging in all the inequality above, we are ready to upper bound the summation of $V_{\hP^{(n)},\hb^{(n)}}^{\pi}$ over any subset $\Nc$.
	\begin{align}
		\sum_{n\in\Nc} V_{P^{(n)},\hb^{(n)}}^{\pi_n} & \leq \sum_{n \in \Nc} \left|V_{\hat{P}_n, \hat{b}_n}^{{\pi}_n} - V_{P^\star, \hat{b}_n}^{{\pi}_n}\right| + \sum_{n\in\Nc}V_{P^\star, \hat{b}_n}^{{\pi}_n}\\
		&\leq \sum_{n\in\Nc}\left(\alpha_n_n\beta_2\sqrt{\frac{dK}{n}}+H\sqrt{K\zeta_n}\right) + \sum_{n\in\Nc}\sum_{h=1}^{H-1} \mathop{\Eb}_{s_{h}\sim (P^\star, \pi_n) \atop a_{h} \sim \pi_n} \left[\gamma_n_n\left\|\phi_{h}^\star(s_{h},a_{h})\right\|_{(W_{h,\Pshi}^{(n)})^{-1}}
		\right]\\`
		& \quad + H\sum_{n\in\Nc}\sum_{h=1}^{H-1}\Eb_{s_{h}\sim(P^\star,\pi_n) \atop a_{h}\sim\pi_n} \left[\alpha_n_n\left\|\Pshi_{h}(s_{h},a_{h})\right\|_{(U_{h,\Pshi}^{(n)})^{-1}}\right]\\
		&\overset{(\romannumeral1)}{\leq} \left(\alpha_n_{|\Nc|}\beta_2\sqrt{dK} + H\sqrt{{K\log (2|\Phi||\Psi||\Nc| H/\delta)} }\right)\sqrt{|\Nc|} + \gamma_n_{|\Nc|}\sum_{h=1}^{H}\sqrt{\sum_{n\in\Nc}\mathop{\Eb}_{s_{h}\sim(P^\star,\pi_n) \atop a_h\sim\pi_n}\left[\left\|\phi_{h}^\star(s_{h},a_{h})\right\|^2_{(W_{h,\Pshi}^{(n)})^{-1}}\right]}\\
		& \quad + \alpha_n_{|\Nc|}H\sum_{h=1}^H\sqrt{K\sum_{n\in\Nc}
			\mathop{\Eb}_{s_{h}\sim(P^\star,\pi_n) \atop a_h\sim \Uc(\Ac)} \left[\left\|\phi_{h}^\star(s_{h},a_{h})\right\|^2_{(U_{h,\Pshi}^{(n)})^{-1}}\right]},
	\end{align}
	where the second term and third term in $(\romannumeral1)$  follows Cauchy-Schwarz inequality and importance sampling. $\alpha_n_{|\Nc|}$ in $(\romannumeral1)$ follows that $\alpha_n_1 \leq \dots \leq \alpha_n_{|\Nc|}$. 
	\begin{align}
		&\sum_{n\in\Nc} V_{P^{(n)},\hb^{(n)}}^{\pi_n}\\
		&\overset{(\romannumeral1)}{\leq} \left(\beta_2\sqrt{dK(K+d^2)\log (2|\Phi||\Psi||\Nc|H/\delta)} + H\sqrt{K\log (2|\Phi||\Psi||\Nc| H/\delta)}\right)\sqrt{|\Nc|}\\
		&\quad + H\sqrt{dK(K+d^2)\log (2|\Phi||\Psi||\Nc|H/\delta)}\sqrt{d|\Nc|\log\left(1+\frac{|\Nc|}{d\lambda_n_1}\right)}\\
		&\quad + H^2\sqrt{(K+d^2)\log (2|\Phi||\Psi||\Nc|H/\delta)}\sqrt{Kd|\Nc|\log\left(1+\frac{|\Nc|}{d\lambda_n_1}\right)}\\
		& \overset{(\romannumeral2)}{\leq} 4H(\sqrt{d}+H)\sqrt{dK(d^2+K)|\Nc|\log (2|\Phi||\Psi||\Nc|H/\delta)\log\left(1+\frac{|\Nc|}{d\lambda_n_1}\right)},
	\end{align}
	where $(\romannumeral1)$ follows \Cref{corollary: revised elliptical potential lemma} and the fact that $\lambda_n_1\leq \dots \leq \lambda_n_{|\Nc|}$ and $(\romannumeral2)$ follows simple algebra to take the dominate term.
	
	
	
	\iffalse
	\begin{align}
		\sum_{n\in\Nc}V_{P^\star, \hat{b}_n}^{{\pi}_n} &\leq \sum_{n\in\Nc}\sum_{h=2}^{H} 
		\mathop{\Eb}_{s_{h-1}\sim (P^\star, {\pi_n}) \atop a_{h-1}\sim \pi_n} \gamma_n_n\left[\|\phi^\star(s_{h-1},a_{h-1}\|_{(W_{h-1,\Pshi}^{(n)})^{-1}}
		\right]+\sum_{n\in\Nc}\sqrt{\frac{K\alpha_n_n^2\beta_2^2  d}{n}}\\
		& \leq \sum_{h=1}^{H-1}\sum_{n\in\Nc} \mathop{E}_{s_{h}\sim (P^\star, {\pi_n}) \atop a_h\sim\pi_n} \gamma_n_n\left[\|\phi^\star(s_{h},a_{h}\|_{(W_{h,\Pshi}^{(n)})^{-1}}
		\right]+\sqrt{K\alpha_n_n^2\beta_2^2 d |\Nc|}\\
		& \leq \sum_{h=1}^{H-1} \gamma_n_N  \sum_{j=1}^{|\Nc|} \mathop{E}_{s_{h}\sim (P^\star, {\pi_n}) \atop a_h\sim\pi_n} \left[\|\phi^\star(s_{h},a_{h}\|_{(W_{h,\Pshi}^{(n_j)})^{-1}}
		\right]+\sqrt{K\alpha_n_n^2\beta_2^2 d |\Nc|}\\
		& \leq \gamma_n_N H\sqrt{d|\Nc|\log(1+\frac{|\Nc|}{
				\lambda_n_1 d})}+\sqrt{Kd(K+d^2)|\Nc|}\\
		&\leq 2Hd\sqrt{K(d+K^2)|\Nc|\log(1+|\Nc|)},
	\end{align}
	where the third inequality holds from $W_{h,\Pshi}^{(n_j)} = \sum_{i=1}^{j-1} \Eb_{(s_h,a_h)\sim(P^\star,\pi_{n_i})}\left[\Pshi_h(s_h,a_h)(\Pshi_h(s_h,a_h))^\top\right] + \lambda_n_n I \leq W_{h,\Pshi}^{(n_j)}$ and the forth inequality comes from lemma \Cref{lemma: Elliptical_potential}.
	
	\begin{align}
		\sum_{n \in \Nc} \left|V_{\hat{P}_n, \hat{b}_n}^{{\pi}_n} - V_{P^\star, \hat{b}_n}^{{\pi}_n}\right| &\leq \sum_{n \in \Nc} H\sqrt{\frac{K\log(|\Phi||\Psi|nH/\delta)}{n}}+\sum_{n \in \Nc}H\sum_{h=2}^H\alpha_n_n\Eb^{\pi_n}\left[\|\Pshi_{h-1}\|_{(U_{h-1,\Pshi}^{(n)})^{-1}}\right]\\
		&\leq H\sqrt{|\Nc|K\log(|\Phi||\Psi|nH/\delta)}+H\sum_{h=1}^{H-1}\sum_{n \in \Nc}\alpha_n_n\Eb^{\pi_n}\left[\|\phi_{h}^\star\|_{(U_{h,\Pshi}^{(n)})^{-1}}\right]\\
		& \leq H\sqrt{|\Nc|K\log(|\Phi||\Psi|nH/\delta)}+ H^2\alpha_n_{|\Nc|}\sqrt{dK|\Nc|\log(1+\frac{|\Nc|}{
				\lambda_n_1 d})}\\
		& \leq 2H^2\sqrt{dK(K+d^2)|\Nc|\log(1+{|\Nc|})}.
	\end{align}
	As a result,
	\begin{align}
		\sum_{n\in\Nc} V_{P^{(n)},\hb^{(n)}}^{\pi_n} & \leq \sum_{n \in \Nc} \left|V_{\hat{P}_n, \hat{b}_n}^{{\pi}_n} - V_{P^\star, \hat{b}_n}^{{\pi}_n}\right| + \sum_{n\in\Nc}V_{P^\star, \hat{b}_n}^{{\pi}_n}\\
		& \leq 2H^2\sqrt{dK(K+d^2)|\Nc|\log(1+{|\Nc|})}+2Hd\sqrt{K(d+K^2)|\Nc|\log(1+|\Nc|)}\\
		& = 2H(H+\sqrt{d})\sqrt{dK(d+K^2)|\Nc|\log(1+|\Nc|)}.
	\end{align}
	\fi
	
\end{proof}
\fi







\iffalse
For part (b), let $f_h^{(n)}\leq B$. Then, fix $h\geq 2$ we have 
\begin{align}
	&\quad\ \Eb_{(s_h,a_h)\sim(\hP^{(n)},\pi)}\left[f_h^{(n)}(s_h,a_h)\right]\nonumber\\
	&=\Eb_{(s_{h-1},a_{h-1})\sim(\hP^{(n)},\pi)}\left[\int_{s_h}\sum_{a_h}f_h^{(n)}(s_h,a_h)\pi(a_h|s_h)\langle\hphi_{h-1}^{(n)}(s_{h-1},a_{h-1}),\hmu_{h-1}^{(n)}(s_h)\rangle\right]\\
	&\leq\Eb_{(s_{h-1},a_{h-1})\sim(\hP^{(n)},\pi)}\left[\left\|\hphi_{h-1}^{(n)}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}}\left\|\int_{s_h}\sum_{a_h}f_h^{(n)}(s_h,a_h)\pi(a_h|s_h)\hmu_{h-1}^{(n)}(s_h)\right\|_{U_{h-1,\hphi}^{(n)}}\right]\\
	&=\hEb_n^{\pi}\left[\left\|\hphi_{h-1}^{(n)}\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}}\sqrt{n\Eb_{s_{h-1}\sim\Pi_n,a_{h-1}\sim \Uc(\Ac)}\left[\left(\Eb_{(s_h,a_h)\sim(\hPb(\cdot|s_{h-1},a_{h-1}),\pi)}\left[f_h^{(n)}(s_h,a_h)\right]\right)^2\right]+\lambda_n_0 B^2d}\right]\\
	&\leq\hEb_n^{\pi}\left[\left\|\hphi_{h-1}^{(n)}\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}}\sqrt{n\Eb_{s_{h-1}\sim\Pi_n,a_{h-1}\sim \Uc(\Ac)}\left[\Eb_{(s_h,a_h)\sim(\hPb(\cdot|s_{h-1},a_{h-1}),\pi)}\left[(f_h^{(n)})^2\right]\right]+\lambda_n_0 B^2d}\right]\\
	&\leq\hEb_n^{\pi}\left[\left\|\hphi_{h-1}^{(n)}\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}}\sqrt{n\Eb_{s_{h-1}\sim\Pi_n,a_{h-1}\sim \Uc(\Ac)}\left[\Eb_{(s_h,a_h)\sim(\Pb^\star(\cdot|s_{h-1},a_{h-1}),\pi)}\left[(f_h^{(n)})^2\right]\right]+\lambda_n_0 B^2d+n\zeta_nB^2} \right]\\
	&\leq\hEb_n^{\pi}\left[\left\|\hphi_{h-1}^{(n)}\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}}\sqrt{nK\Eb_{s_{h-1}\sim\Pi_n,(a_{h-1},a_{h})\sim \Uc(\Ac),s_{h}}\left[\left[(f_h^{(n)})^2\right]\right]+\lambda_n_0 B^2d+n\zeta_nB^2} \right]\\
	&\leq\hEb_n^{\pi}\left[\left\|\hphi_{h-1}^{(n)}\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}}\sqrt{nK\zeta_n+\lambda_n_0 B^2d+n\zeta_nB^2} \right]
\end{align}

For $h=1$, we have:
\begin{align}
	\Eb_{(s_1,a_1)\sim(\hP,\pi)}\left[f_1^{(n)}(s_1,a_1)\right]&=\int_{s_1}\sum_{a_1}f_1^{(n)}(s_1,a_1)\pi(a_1|s_1) P_0(s_1)\\
	&\leq\sqrt{K\Eb_{s_1\sim P_0,a\sim \Uc(\Ac)}\left[(f_1^{(n)})^2\right]}\\
	&\leq\sqrt{K\zeta_n}
\end{align}

Note that
\[\alpha_n_n\beta_2^2\|\hphi_h^{(n)}(s_h,a_h)\|_{({U}_{h,\hphi}^{(n)})^{-1}}\geq b_h^{(n)}(s_h,a_h) = \alpha_n_n\|\hphi_h^{(n)}(s_h,a_h)\|_{(\hat{U}_{h,\hphi}^{(n)})^{-1}}\geq\alpha_n_n\beta_1\|\hphi_h^{(n)}(s_h,a_h)\|_{({U}_{h,\hphi}^{(n)})^{-1}}\]

Therefore, by choosing $\alpha_n_n\beta_1 = \sqrt{2nK\zeta_n+\lambda_n_0 B^2d+2n\zeta_nB^2}$, we have:
\begin{align}
	&V_{P^\star,r}^{\pi} - V_{\hP^{(n)},r+b^{(n)}}^{\pi}\nonumber\\
	&\leq\sqrt{K\zeta_n}+\sum_{h=1}^H \hEb_n^{\pi}\left[-\alpha_n_n\beta_1\|\hphi_{h}^{(n)}\|_{(U_{h,\hphi}^{(n)})^{-1}}\right] + \sum_{h=2}^H\hEb_n^{\pi}\left[\alpha_n_n\beta_1\|\hphi_{h-1}^{(n)}\|_{(U_{h-1,\hphi}^{(n)})^{-1}}\right]\\
	&\leq \sqrt{K\zeta_n}
\end{align}

\textcolor{red}{Note $\alpha_n_n = \tilde{O}(\sqrt{K + d^2})$}
\fi











\iffalse
\begin{lemma}
	\begin{align}\label{lemma:Regret}
		V_{P^\star, r}^{\pi^\star}-V_{P^\star, r}^{\Bar{\pi}_n}\leq something
	\end{align}
\end{lemma}
\begin{Proof}
	We mainly focus on $ V_{\hat{P}^{(n)}, r+\hat{b}^{(n)}}^{\bpi_n}-V_{P^\star, r}^{\Bar{\pi}_n}$ because $V_{P^\star, r}^{\pi^\star}\leq V_{\hat{P}^{(n)}, r+\hat{b}^{(n)}}^{\pi^\star} + \sqrt{K\zeta_n} \leq V_{\hat{P}^{(n)}, r+b^{(n)}}^{\bpi_n} + \sqrt{K\zeta_n} $.
	\begin{align}
		&\quad V_{\hat{P}^{(n)}, r+\hat{b}^{(n)}}^{\bar{\pi}_n}-V_{P^\star, r}^{\Bar{\pi}_n}\nonumber\\
		&=\mathop{\Eb}_{s_h\sim(\hP^{(n)},\bar{\pi}_n) \atop a_h \sim \bar{\pi}_n} \left [\sum_{h=1}^{H}\left(\hat{b}_h^n(s_h,a_h)+
		\int(P^\star(s_{h+1}|s_h,a_h)-
		\hat{P}^{(n)}(s_{h+1}|s_h,a_h))V_{P^\star, r}^{\Bar{\pi}_n}(s_{h+1})\right)\right]\\
		&\leq V_{\hat{P}^{(n)}, \hat{b}^{(n)}}^{\Bar{\pi}_n}+{\sum_{h=1}^{H}\mathop{\Eb}_{s_h\sim(\hP^{(n)},\bar{\pi}_n) \atop a_h \sim \bar{\pi}_n}\left[f_h^{(n)}(s_h,a_h)\right]}\\
		&= V_{\hat{P}^{(n)}, \hat{b}^{(n)}}^{{\pi}_n} +
		V_{\hat{P}^{(n)}, f^{(n)}}^{{\pi}_n}\\
		&\leq 2V_{\hat{P}^{(n)}, \hat{b}^{(n)}}^{{\pi}_n} +\sqrt{K\zeta_n}.
	\end{align}
	For the term $V_{\hat{P}^{(n)}, \hat{b}^{(n)}}^{{\pi}_n}$, consider the case where $n\in\{n: V_{\hP^{(n)}, \hat{b}_n}^{{\pi}_n}\leq 1\}$
	\begin{align}
		&\quad \left|V_{\hat{P}^{(n)}, \hat{b}^{(n)}}^{{\pi}_n} - V_{P^\star, \hat{b}^{(n)}}^{{\pi}_n}\right| \nonumber \\
		&= \left|\mathop{E}_{s_h\sim(P^\star,{\pi}_n)\atop a_h \sim \pi_n } \left[\sum_{h=1}^{H}
		\int(P_h^\star(s_{h+1}|s_h,a_h)-\hat{P}_h^{(n)}(s_{h+1}|s_h,a_h))V_{h+1,\hat{P}^{(n)}, \hat{b}^{(n)}}^{{\pi}_n}(s_{h+1})\right]\right| \\
		& \leq \sum_{h=1}^{H} \mathop{E}_{s_h\sim(P^\star,{\pi}_n)\atop a_h \sim \pi_n }[H f_h^{(n)}(s_h,a_h)] \\
		&\leq H\sqrt{K\zeta_n} + H\sum_{h=2}^H\mathop{\Eb}_{s_{h-1}\sim (P^\star,\pi_n) \atop a_{h-1}\sim \pi_n } \left[ b_{h-1,\phi^\star}^{(n)}(s_{h-1},a_{h-1})\right]:=H\sqrt{K\zeta_n} + HF_n,
	\end{align}
	where \[F_n = \sum_{h=1}^{H-1}\mathop{\Eb}_{s_{h}\sim (P^\star,\pi_n) \atop a_{h}\sim \pi_n }\left[b_{h,\phi^\star}^{(n)}(s_{h},a_{h})\right]\]. 
	
	We further denote 
	\[G_n = \mathop{\Eb}_{s_{h}\sim (P^\star,\pi_n) \atop a_{h}\sim \pi_n }\left[\tilde{b}_{h,\phi^\star}^{(n)}(s_{h},a_{h})\right].\]
	
	Then, by the inequality~(\Cref{ineq:Bound_V_b}), combine all things above,
	\begin{align}
		&\quad V_{P^\star, r}^{\pi^\star}-V_{P^\star, r}^{\Bar{\pi}_n}\\
		&\leq \sqrt{K\zeta_n} + 2\left(HF_n+ H\sqrt{K\zeta_n} + G_n +  \sqrt{\frac{K\alpha_n_n^2\beta_2d}{n}} \right) \\
		& = (2H+1)\sqrt{K\zeta_n} + 2\left(HF_n + G_n +  \sqrt{\frac{K\alpha_n_n^2\beta_2d}{n}} \right)
	\end{align}
\end{Proof}
\fi

\subsection{Proof of \Cref{prop:step1}}\label{subsec:reward_free_proof}

Equipped with \Cref{lemma:Bound_TV}, the following proposition provides an upper bound on the difference of value functions under the estimated model $\hP^{(n)}$ and the true model $P^\star$ for any given policy $\pi$ and reward $r$.
\begin{proposition}[Restatement of \Cref{prop:step1}]
	\label{prop:step1_appendix}
	Fix $n\in[N]$, $\zeta_n : = \frac{\log(2|\Phi||\Psi|nH/\delta)}{n}$. Then for any policy $\pi$ and reward $r$, with probability at least $1-\delta/2$, we have
	\begin{equation*}
		\left| V_{P^\star,r}^{\pi}-V_{\hP^{(n)},r}^{\pi}\right| \leq   V_{\hP^{(n)},\hb^{(n)}}^{\pi_n}+\sqrt{K\zeta_n}.
	\end{equation*}
\end{proposition}


\begin{proof}
	First, by \Cref{lemma: Simulation}, we have
	\begin{align}
		\left|V_{P^\star,r}^{\pi} - V_{\hP^{(n)},r}^{\pi}\right|
		&= \left| \sum_{h=1}^H \mathop{\Eb}_{s_h\sim(\hP^{(n)},\pi) \atop a_h \sim \pi} \left[ (P^\star_{h} - \hP_{h})V^{\pi}_{h+1,\Ps,r}(s_h,a_h)\right]\right|\nonumber\\
		&\leq \sum_{h=1}^H \mathop{\Eb}_{s_h\sim(\hP^{(n)},\pi) \atop a_h \sim \pi} \left[ {f_h^{(n)}(s_h,a_h)}\right],\label{eqn:Near_Optimal}
	\end{align}
	where the inequality is due to the normalized value function i.e. $V^{\pi}_{P^\star,r}\leq 1$  and $0 \leq f_h^{(n)}$.
	
	By \Cref{ineq:hP_Bound_TV} and the fact that the total variation distance is upper bounded by 1,  with probability at least $1-\delta/2$, we have 
	\begin{align}
		\mathop{\Eb}_{s_h\sim(\hP^{(n)},\pi) \atop a_{h} \sim \pi }\left[f_h^{(n)}(s_h,a_h)\right] \leq \mathop{\Eb}_{s_{h-1}\sim(\hP^{(n)},\pi) \atop a_{h-1} \sim \pi}\left[\min\left(\alpha_n_n\left\|\hphi_{h-1}^{(n)}\right\|_{(U_{h-1,\hphi}^{(n)})^{-1}}, 1\right) \right], \forall h\geq2.\label{ineq: f_1 bound}
	\end{align}
	Similarly, when $h=1$, 
	\begin{align}
		\mathop{\Eb}_{a_1\sim\pi}\left[f_1^{(n)}(s_1,a_1)\right]\leq \sqrt{K\mathop{\Eb}_{a\sim\Uc(\Ac)}\left[\left(f_1^{(n)}(s_1,a_1)\right)^2\right]}\leq\sqrt{K\zeta_n}.\label{ineq:f1<K_zeta}
	\end{align}
	Based on \Cref{coro: concentration of the bonus term} and \Cref{ineq: f_1 bound}, we have
	\begin{align}
		\mathop{\Eb}_{s_h\sim(\hP^{(n)},\pi) \atop a_h \sim \pi} \left[\hb^{(n)}_h(s_h,a_h)\right] \geq \mathop{\Eb}_{s_h\sim(\hP^{(n)},\pi) \atop a_h \sim \pi}\left[\min\left(\alpha_n_n\left\|\hphi_{h}^{(n)}\right\|_{(U_{h,\hphi}^{(n)})^{-1}}, 1\right) \right] \geq \mathop{\Eb}_{s_{h+1}\sim(\hP^{(n)},\pi) \atop a_{h+1} \sim \pi }\left[f_{h+1}^{(n)}(s_{h+1},a_{h+1})\right] .\label{ineq:f<b}
	\end{align}
	Substituting \Cref{ineq:f1<K_zeta} and \Cref{ineq:f<b} to \Cref{eqn:Near_Optimal}, we have
	\begin{align*}
		&\left|V_{P^\star,r}^{\pi} - V_{\hP^{(n)},r}^{\pi}\right|\\
		&\leq\sqrt{K\zeta_n} + \sum_{h=1}^{H-1}\mathop{\Eb}_{s_h\sim(\hP^{(n)},\pi) \atop a_h \sim \pi}\left[\hb^{(n)}_h(s_h,a_h)\right]\\
		&\leq \sqrt{K\zeta_n}+V_{\hP^{(n)},\hb^{(n)}}^{\pi}.
	\end{align*}
	We complete the proof by noting that $\pi_n$ is the optimal policy under the estimated model $\hP^{(n)}$ and reward $\hb^{(n)}$.
\end{proof}
From \Cref{ineq:f1<K_zeta} and \Cref{ineq:f<b} in the proof of \Cref{prop:step1_appendix}, we can get an intermediate result.
\begin{corollary}\label{corollary:V_f_n<V_b_n}
	Fix $n\in[N]$, for any policy $\pi$ and reward $r$, with probability at least $1-\delta$, we have:
	\begin{equation*}
		V_{\hP^{(n)},f^{(n)}}^{\pi}\leq \sqrt{K\zeta_n}
		+ V_{\hP^{(n)},\hb^{(n)}}^{\pi}
	\end{equation*}
\end{corollary}

\subsection{Proof of \Cref{prop:step2}}
\begin{proposition}[Restatement of \Cref{prop:step2}].
	\label{prop:step2_appendix}
	Fix $\delta  \in (0,1)$, with probability at least $1-\delta/2$, the summation of the difference of value functions $\left |V_{\hat{P}_n, \hat{b}_n}^{{\pi}_n}-V^{\pi_n}_{P^\star,\hb^{(n)}}\right|$ under exploration policies $\{\pi_n\}_{n\in\Nc}$ with exploration-driven reward functions $\hb^{(n)}$ is sublinear with respect to $|\Nc|$ for any $\Nc = \{n_1, \dots, n_{|\Nc|}\}\subset[N]$, i.e.
	\begin{align}
		\sum_{n\in\Nc} \left|V_{\hat{P}_n, \hat{b}_n}^{{\pi}_n} - V_{P^\star, \hat{b}_n}^{{\pi}_n}\right|  \leq  4H^2\sqrt{dK(d^2+K)|\Nc|\log (2|\Phi||\Psi|NH/\delta)\log\left(1+\frac{N}{d\lambda_n_1}\right)}.
	\end{align}
\end{proposition}


\begin{proof}
	Note that $\forall h,s_{h+1}, V_{h+1,\hat{P}_n, \hat{b}_n}^{{\pi}_n}(s_{h+1})\leq H$. We have
	\begin{align*}
		\left|V_{\hat{P}_n, \hat{b}_n}^{{\pi}_n} - V_{P^\star, \hat{b}_n}^{{\pi}_n}\right| 
		&= \left|\mathop{\Eb}_{s_{h}\sim(P^\star,{\pi}_n) \atop a_h \sim \pi_n}\left[\sum_{h=1}^{H}	    \int(P^\star(s_{h+1}|s_h,a_h)-\hat{P}_n(s_{h+1}|s_h,a_h))V_{h+1,\hat{P}_n, \hat{b}_n}^{{\pi}_n}(s_{h+1})\right]\right| \\
		& \leq H\sum_{h=1}^{H}\mathop{\Eb}_{s_h\sim (P^\star, {\pi}_n), \atop a_h \sim \pi_n}\left[f_h^{(n)}(s_h,a_h)\right] \\
		&\leq H\sqrt{K\zeta_n} + H\sum_{h=2}^H\Eb_{s_{h-1}\sim(P^\star,\pi_n) \atop a_{h-1}\sim\pi_n} \left[\alpha_n_n\left\|\Pshi_{h-1}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\Pshi}^{(n)})^{-1}}\right]\\
		&\leq H\sqrt{K\zeta_n} + H\sum_{h=1}^{H-1}\Eb_{s_{h}\sim(P^\star,\pi_n) \atop a_{h}\sim\pi_n} \left[\alpha_n_n\left\|\Pshi_{h}(s_{h},a_{h})\right\|_{(U_{h,\Pshi}^{(n)})^{-1}}\right],
	\end{align*}
	where the second inequality again follows from \Cref{ineq:P*_Bound_TV}.
	
	Then, we sum $\left|V_{\hat{P}_n, \hat{b}_n}^{{\pi}_n} - V_{P^\star, \hat{b}_n}^{{\pi}_n}\right| $ over the subset $\Nc$. 
	\begin{align*}
		\sum_{n\in\Nc} \left|V_{\hat{P}_n, \hat{b}_n}^{{\pi}_n} - V_{P^\star, \hat{b}_n}^{{\pi}_n}\right|
		&\leq \sum_{n\in\Nc}H\sqrt{K\zeta_n} 
		+ H\sum_{n\in\Nc}\sum_{h=1}^{H-1}\Eb_{s_{h}\sim(P^\star,\pi_n) \atop a_{h}\sim\pi_n} \left[\alpha_n_n\left\|\Pshi_{h}(s_{h},a_{h})\right\|_{(U_{h,\Pshi}^{(n)})^{-1}}\right]\\
		&\overset{(\romannumeral1)}{\leq} H\sqrt{{K|\Nc|\log (2|\Phi||\Psi||\Nc| H/\delta)} } +  \alpha_n_{N}H\sum_{h=1}^H\sqrt{K|\Nc|\sum_{n\in\Nc}
			\mathop{\Eb}_{s_{h}\sim(P^\star,\pi_n) \atop a_h\sim \Uc(\Ac)} \left[\left\|\phi_{h}^\star(s_{h},a_{h})\right\|^2_{(U_{h,\Pshi}^{(n)})^{-1}}\right]}\\
		&\leq H\sqrt{K|\Nc|\log (2|\Phi||\Psi|N H/\delta)} \\
		&\quad  + \alpha_n_{N}H\sum_{h=1}^H\sqrt{K|\Nc|\sum_{n\in\Nc}
			\mathop{\Eb}_{s_{h}\sim(P^\star,\pi_n) \atop a_h\sim \Uc(\Ac)} \left[{\rm tr}\left (\phi_{h}^\star(s_{h},a_{h})\phi_{h}^\star(s_{h},a_{h})^\top(U_{h,\Pshi}^{(n)})^{-1}\right)\right]}\\
		&\overset{(\romannumeral2)}{\leq} H\sqrt{K|\Nc|\log (2|\Phi||\Psi|N H/\delta)} + H^2\sqrt{dK(K+d^2)|\Nc|\log (2|\Phi||\Psi|NH/\delta)\log\left(1+\frac{N}{d\lambda_n_1}\right)}\\
		& \leq 4H^2\sqrt{dK(d^2+K)|\Nc|\log (2|\Phi||\Psi|NH/\delta)\log\left(1+\frac{N}{d\lambda_n_1}\right)},
	\end{align*}
	where $(\romannumeral1)$ follows Cauchy-Schwarz inequality , importance sampling and the fact $\alpha_n_1\leq \dots \leq \alpha_n_N$ and $(\romannumeral2)$ follows \Cref{corollary: revised elliptical potential lemma}.
	
	
	
	
	
\end{proof}











\subsection{Proof of \Cref{lemma: bound on summation of V_hat}}
We present a more general version of \Cref{lemma: bound on summation of V_hat} formally. 
\begin{proposition}[Restatement of \Cref{lemma: bound on summation of V_hat}].
	\label{prop:step3_appendix}
	Fix $\delta  \in (0,1)$, with probability at least $1-\delta/2$, the summation of value functions $V^{\pi_n}_{\hP^{(n)},\hb^{(n)}}$ under exploration policies $\{\pi_n\}_{n\in\Nc}$ with exploration-driven reward functions $\hb^{(n)}$ is sublinear with respect to $|\Nc|$ for any $\Nc = {n_1, \dots, n_{|\Nc|}}\subset[N]$, i.e.
	\begin{align}
		\sum_{n\in\Nc} V_{\hP^{(n)},\hb^{(n)}}^{\pi_n}  \leq  4H(\sqrt{d}+H)\sqrt{dK(d^2+K)|\Nc|\log (2|\Phi||\Psi|NH/\delta)\log\left(1+\frac{N}{d\lambda_n_1}\right)}.
	\end{align}
\end{proposition}
\begin{proof}
	We aim to bound the value function $V_{P^\star, {\hb}^{(n)}}^{{\pi}_n}$ by the \Cref{ineq:P*_Bound_bn} and \Cref{ineq:Step_1_Bound}.
	\begin{align*}
		V_{P^\star, {\hb}^{(n)}}^{{\pi}_n}
		& = \sum_{h=1}^{H} \mathop{\Eb}_{s_h\sim (P^\star, \pi_n) \atop a_h\sim\pi_n}\left[\hat{b}_n(s_h,a_h)\right]\\
		& \leq \sum_{h=2}^{H} \mathop{\Eb}_{s_{h-1}\sim (P^\star, {\pi_n}) \atop a_{h-1}\sim\pi_n} 
		\left[\gamma_n_n\left\|\phi_{h-1}^\star(s_{h-1},a_{h-1})\right\|_{(W_{h-1,\Pshi}^{(n)})^{-1}} \right]
		+  \alpha_n_n\beta_2\sqrt{\frac{dK}{n}}\\
		&\leq\sum_{h=1}^{H-1} \mathop{\Eb}_{s_{h}\sim (P^\star, \pi_n) \atop a_{h} \sim \pi_n} \left[\gamma_n_n\left\|\phi_{h}^\star(s_{h},a_{h})\right\|_{(W_{h,\Pshi}^{(n)})^{-1}}
		\right]
		+ \alpha_n_n\beta_2\sqrt{\frac{dK}{n}}.
	\end{align*}
	Then, summing $V_{P^\star, {\hb}^{(n)}}^{{\pi}_n}$, we have
	
	\begin{align*}
		\sum_{n\in\Nc}V_{P^\star, \hat{b}_n}^{{\pi}_n}
		&\leq \sum_{n\in\Nc}\alpha_n_n\beta_2\sqrt{\frac{dK}{n}} + \sum_{n\in\Nc}\sum_{h=1}^{H-1} \mathop{\Eb}_{s_{h}\sim (P^\star, \pi_n) \atop a_{h} \sim \pi_n} \left[\gamma_n_n\left\|\phi_{h}^\star(s_{h},a_{h})\right\|_{(W_{h,\Pshi}^{(n)})^{-1}}
		\right]\\`
		&\overset{(\romannumeral1)}{\leq} \alpha_n_{N}\beta_2\sqrt{dK|\Nc|} + \gamma_n_{N}\sum_{h=1}^{H}\sqrt{|\Nc|\sum_{n\in\Nc}\mathop{\Eb}_{s_{h}\sim(P^\star,\pi_n) \atop a_h\sim\pi_n}\left[\left\|\phi_{h}^\star(s_{h},a_{h})\right\|^2_{(W_{h,\Pshi}^{(n)})^{-1}}\right]}\\
		&\overset{(\romannumeral2)}{\leq} \beta_2\sqrt{dK(K+d^2)|\Nc|\log (2|\Phi||\Psi|NH/\delta)}\\
		&\quad + H\sqrt{dK(K+d^2)\log (2|\Phi||\Psi|NH/\delta)}\sqrt{d|\Nc|\log\left(1+\frac{N}{d\lambda_n_1}\right)}\\
		& {\leq} 4Hd\sqrt{K(d^2+K)|\Nc|\log (2|\Phi||\Psi|NH/\delta)\log\left(1+\frac{N}{d\lambda_n_1}\right)},
	\end{align*}
	where $(\romannumeral1)$ follows from  Cauchy-Schwarz inequality and the fact that $\gamma_n_1\leq \dots \leq \gamma_n_N$ and $(\romannumeral2)$ follows from \Cref{corollary: revised elliptical potential lemma}.
	Combining the above inequality with \Cref{prop:step2_appendix}, we complete the proof.
\end{proof}



\iffalse
The following lemma shows that there cannot be many iterations during which $V_{\hP^{(n)},\hb^{(n)}}^{\pi_n}$ is large. 
\begin{lemma} \label{lemma: finite set}
	Given any $\eta>0$, define $\Nc_{\eta}=\{n: V_{\hP^{(n)} ,\hb^{(n)}} + \sqrt{K\zeta_n}>\eta\}$ to be the set of iterations in which the optimal value function with respect to the learned model, after adding $\sqrt{K\zeta_n}$, is larger that $\eta$. Then, under the setting of \Cref{prop:step3_appendix}, $|\Nc_{\eta}|\leq \frac{64H^4d^4K^2}{\eta^2}\log^2\left(\frac{16H^4d^4K^2|\Phi||\Psi|H}{\eta^2\delta}\right).$
\end{lemma}


\begin{proof}
	
	Applying \Cref{prop:step3_appendix} on the set $\Nc_{\eta}\subset[N]$, we have
	\begin{align*}
		\eta|\Nc_{\eta}|&<\sum_{n\in\Nc_{\eta}}V_{\hP^{(n)},\hb^{(n)}} + \sqrt{K\zeta_n}\\
		&\leq 5H(\sqrt{d}+H)\sqrt{dK(d^2+K)|\Nc_\eta|\log (2|\Phi||\Psi|NH/\delta)\log\left(1+\frac{N}{d\lambda_n_1}\right)}\\
		&\leq 5H(\sqrt{d}+H)\sqrt{dK(d^2+K)|\Nc_\eta|\log^2 (2|\Phi||\Psi|NH/\delta)}.
	\end{align*}
	
	Denote $c_1=\frac{4H\left(\sqrt{d}+H\right)\sqrt{dK(d^2+K)}}{\eta},c_2=2|\Phi||\Psi|H/\delta$. Then, we have $|\Nc_\eta|\leq c_1^2 \log^2\left(c_2|\Nc_\eta|\right)$.
	With some algebraic manipulation and \Cref{lemma: log transform}, it can be concluded that
	\begin{align*}
		|\Nc_{\eta}|< 4c_1^2\log^2 (c_1^2c_2) \leq \frac{64H^4d^4K^2}{\eta^2}\log^2\left(\frac{16H^4d^4K^2|\Phi||\Psi|H}{\eta^2\delta}\right).
	\end{align*}
	
	
	
	
	
	
	
\end{proof}



\fi





\subsection{Proof of \Cref{prop:step3}}

Based on \Cref{prop:step3_appendix}, we argue that with enough number of iterations, RAFFLE can find $\hP^{\epsilon}$ satisfying the condition in line 15 of \Cref{Algorithm: RAFFLE}.

\begin{lemma}[Restatement of \Cref{prop:step3}] \label{prop:step3_appendix}
	Fix any $\delta \in (0,1), \epsilon >0$. Suppose the algorithm runs for $N=\frac{400H^4d^2{K(d^2+K)\log^2 (2|\Phi||\Psi|H/(\delta\epsilon^2))}}{\epsilon^2}$ iterations, with probability at least $1-\delta$, RAFFLE can find an $n_\epsilon \leq N$ in the exploration phase such that $2V^{\pi_{n_\epsilon}}_{\hP^{(n_\epsilon)},\hb^{(n_\epsilon)}}+2\sqrt{Kn\zeta_n_{n_\epsilon}}\leq \epsilon$. In other words, \Cref{Algorithm: RAFFLE} can output $\hP^{\epsilon}=\hP^{(n_\epsilon)}$ satisfying the condition in line 15. In addition 
	\begin{align*}
		\left|V_{P^\star,r}^{\pi} - V_{\hP^{(n_\epsilon)}, r}^{\pi}\right|\leq \epsilon/2.
	\end{align*}
\end{lemma}
\begin{proof}
	
	
	We show that the algorithm terminates by contradiction. If it does not stop, applying \Cref{prop:step3_appendix} on the set $[N]$, we have
	\begin{align*}
		\epsilon N/2&< \sum_{n\in[N]}\left(V_{\hP^{(n)},\hb^{(n)}} + \sqrt{K\zeta_n}\right)\\
		&\leq 5H(\sqrt{d}+H)\sqrt{dK(d^2+K)N\log (2|\Phi||\Psi|NH/\delta)\log\left(1+\frac{N}{d\lambda_n_1}\right)}\\
		&\leq 5H(\sqrt{d}+H)\sqrt{dK(d^2+K)N\log^2 (2|\Phi||\Psi|NH/\delta)}.
	\end{align*}
	
	Therefore,
	\[N < \frac{100H^4d^2K(d^2+K)\log^2 (2|\Phi||\Psi|NH/\delta)}{\epsilon^2}\]
	With some algebraic manipulation and \Cref{lemma: log transform}, it can be concluded that
	\begin{align*}
		N < \frac{400H^4d^2K(d^2+K)\log^2 (2|\Phi||\Psi|H^5d^2K(d^2+K)/(\delta\epsilon^2))}{\epsilon^2},
	\end{align*}
	which is a contradiction. 
	Therefore, there exists an $n_{\epsilon} = O(\frac{400H^4d^2K(d^2+K)\log^2 (2|\Phi||\Psi|H^5d^2K(d^2+K)/(\delta\epsilon^2))}{\epsilon^2})$ such that $\hP^{\epsilon}=\hP^{(n_\epsilon)}$ satisfies
	\begin{align*}
		2V^{\pi_{n_\epsilon}}_{\hP^{(n_\epsilon)},\hb^{(n_\epsilon)}}+2\sqrt{Kn\zeta_n_{n_\epsilon}}\leq \epsilon
	\end{align*} 
	Combining \Cref{prop:step1_appendix}, we finish the proof.
	
	
	
	
\end{proof}






\begin{proof}[Proof of \Cref{thm1: reward-free sample complexity}]
	Recall that $\hP^\epsilon$ is the output of RAFFLE in the $n_\epsilon$-iteration. Then, by \Cref{prop:step5_appendix}
	\begin{align*}
		V^\star_{P^\star, r} &- V^{\bpi}_{P^\star, r}\\
		&{\leq} V_{\hat{P}^{\epsilon}, r}^{\pi^\star}-V_{P^\star, r}^{\Bar{\pi}}+\epsilon/2\nonumber\\
		&\overset{(\romannumeral1)}{\leq} V_{\hat{P}^\epsilon, r}^{\bar{\pi}}-V_{P^\star, r}^{\bar{\pi}}+\epsilon/2\nonumber\\
		&{\leq} 
		\epsilon/2+\epsilon/2\nonumber\\
		&= \epsilon,
	\end{align*}
	where $(\romannumeral1)$ follows from the definition of $\bpi$.
	The number of trajectories $n_{\epsilon} H $ is at at most
	\[ O\left(\frac{400H^4d^2K(d^2+K)\log^2 (2|\Phi||\Psi|H^5d^2K(d^2+K)/(\delta\epsilon^2))}{\epsilon^2}\right)\]
\end{proof}
\fi



\section{Proof of \Cref{Theorem: system identification sample complexity}}\label{subsec:System_proof}
In this section, we adopt the same notations as in \Cref{sec: A}. The following lemma provides an upper bound for the estimation error of any learned model from the true model.
\begin{lemma}\label{lemma: V_b bound TV}
	Fix $\delta \in (0,1)$, for any $h \in [H], n \in \mathbb{N}^+$, any policy $\pi$, with probability at least $1-\delta/2$, 
	\begin{align*}
		\mathop{\Eb}_{s_h \sim (\Ps,\pi)\atop s_h \sim \pi}\left[f_h^{(n)}(s_h,a_h)\right] \leq 2\sqrt{K\zeta_n}+2 \hV_{\hP^{(n)},\hb^{(n)}}^{\pi_n}.
	\end{align*}
\end{lemma}
\begin{proof}
	Recall that $f_h^{(n)}(s,a)=\left\|\hP_h^{(n)}(\cdot|s,a) - P^\star_h(\cdot|s,a)\right\|_{TV}$.
	Fix any policy $\pi$, for any $h \geq 2$, we have 
	\begin{align}
		\mathop{\Eb}_{s_{h} \sim (\hP^{(n)},\pi)\atop a_{h} \sim \pi}\left[\hat{Q}_{h,\hP^{(n)},\hb^{(n)}}^{\pi}(s_{h},a_{h})\right]
		&=\mathop{\Eb}_{s_{h-1} \sim (\hP^{(n)},\pi)\atop a_{h-1} \sim \pi}\left[\hP^{(n)}_{h}\hat{V}_{h,\hP^{(n)},\hb^{(n)}}^{\pi}(s_{h-1},a_{h-1})\right]\nonumber\\
		&\leq \mathop{\Eb}_{s_{h-1} \sim (\hP^{(n)},\pi)\atop a_{h-1} \sim \pi}\left[\min\left\{1,\hb^{(n)}_{h-1}(s_{h-1},a_{h-1})+\hP^{(n)}_{h-1}\hat{V}_{h,\hP^{(n)},\hb^{(n)}}^{\pi}(s_{h-1},a_{h-1})\right\}\right]\nonumber\\
		&=\mathop{\Eb}_{s_{h-1} \sim (\hP^{(n)},\pi)\atop a_{h-1} \sim \pi}\left[\hat{Q}_{h-1,\hP^{(n)},\hb^{(n)}}^{\pi}(s_{h-1},a_{h-1})\right]\nonumber\\
		& \leq \ldots \nonumber\\
		& \leq \mathop{\Eb}_{ a_{1} \sim \pi}\left[\hat{Q}_{1,\hP^{(n)},\hb^{(n)}}^{\pi}(s_{1},a_{1})\right]\nonumber\\
		& = \hat{V}_{\hP^{(n)},\hb^{(n)}}^{\pi}.\label{ineq: lemmaB1-1}
	\end{align}
	Hence, for $h \geq 2$, we have
	\begin{align}
		\mathop{\Eb}_{s_h \sim (\hP^{(n)},\pi)\atop a_h \sim \pi}\left[f_h^{(n)}(s_h,a_h)\right] &\overset{(\romannumeral1)}{\leq}\mathop{\Eb}_{s_{h-1} \sim (\hP^{(n)},\pi)\atop a_{h-1} \sim \pi}\left[\hb_{h-1}^{(n)}(s_{h-1},a_{h-1})\right]\nonumber\\
		&\overset{(\romannumeral2)}{\leq}\mathop{\Eb}_{s_{h-1} \sim (\hP^{(n)},\pi)\atop a_{h-1} \sim \pi}\left[\hat{Q}_{h-1,\hP^{(n)},\hb^{(n)}}^{\pi}(s_{h-1},a_{h-1})\right]\nonumber\\
		&\overset{(\romannumeral3)}{\leq}\hat{V}_{\hP^{(n)},\hb^{(n)}}^{\pi},\label{ineq: lemmaB1-2}
	\end{align}
	where $(\romannumeral1)$ follows from \Cref{ineq:f<b}, $(\romannumeral2)$ follows from the definition of $\hat{Q}_{h-1,\hP^{(n)},\hb^{(n)}}^{\pi}(s_{h-1},a_{h-1})$ and $(\romannumeral3)$ follows from \Cref{ineq: lemmaB1-1}.
	\begin{align*}
		\scriptstyle\mathop{\Eb}_{ \sim (\Ps,\pi)\atop s_h \sim \pi}\left[f_h^{(n)}(s_h,a_h)\right]
		&\leq  \mathop{\Eb}_{s_h \sim (\hP^{(n)},\pi)\atop a_h \sim \pi}\left[f_h^{(n)}(s_h,a_h)\right]+\left| \mathop{\Eb}_{s_h \sim (\Ps,\pi)\atop a_h \sim \pi}\left[f_h^{(n)}(s_h,a_h)\right]-\mathop{\Eb}_{s_h \sim (\hP^{(n)},\pi)\atop a_h \sim \pi}\left[f_h^{(n)}(s_h,a_h)\right]\right|\\
		& \overset{(\romannumeral1)}{\leq} (\hV_{\hP^{(n)},\hb^{(n)}}^{\pi}+\sqrt{K\zeta_n}) +\left(\sqrt{K\zeta_n}+ \hV_{\hP^{(n)},\hb^{(n)}}^{\pi}\right)\\
		& \overset{(\romannumeral2)}{\leq} 2\sqrt{K\zeta_n}+ 2\hV_{\hP^{(n)},\hb^{(n)}}^{\pi_n},
	\end{align*}
	where the first term in $(\romannumeral1)$ is due to \Cref{ineq: lemmaB1-2} and \Cref{ineq:Step_1_Bound}, the second term in $(\romannumeral1)$ is due to \Cref{prop:step1_appendix} and $(\romannumeral2)$ follows from the definition of $\pi_n$.
	% Specially, for $h=1$, by \Cref{ineq:Step_1_Bound}, we have $\mathop{\Eb}_{a_1 \sim \pi}\left[f_1^{(n)}(s_1,a_1)\right] \leq \sqrt{Kn\zeta_n/n}$.
\end{proof}




\begin{proof}[Proof of \Cref{Theorem: system identification sample complexity}]
	By \Cref{prop:step3_appendix}, let $n_{\epsilon}=O\left(\frac{H^2d^2K(d^2+K)\log^2 (|\Phi||\Psi|NH^3d^2K(K+d^2)/(\delta\epsilon^2))}{\epsilon^2}\right)$, with no more than $n_{\epsilon}H$ trajectories, RAFFLE can learn a model $\hP^\epsilon$, bonus $\hb^{\epsilon}$ and policy $\pi_\epsilon$ at the $n_\epsilon$-th iteration satisfying
	$2V^{\pi_\epsilon}_{\hP^\epsilon,\hb^\epsilon}+2\sqrt{K\zeta_{n_\epsilon}} \leq \epsilon$. Then, following \Cref{lemma: V_b bound TV}, we have
	\begin{align*}
		\mathop{\Eb}_{s_h \sim (\Ps,\pi)\atop s_h \sim \pi}\left[\|\hP_h^{\epsilon}(\cdot|s_h,a_h)-P_h^\star(\cdot|s_h,a_h)\|_{TV}\right] \leq 2\sqrt{K\zeta_{n_\epsilon}}+2 V_{\hP^{\epsilon},\hb^{\epsilon}}^{\pi_\epsilon}\leq \epsilon.
	\end{align*} 
\end{proof}




\iffalse
Then, we use \Cref{prop:step3_appendix} to bound the regret.
\begin{align}
	&\quad V_{P^\star, r}^{\pi^\star}-V_{P^\star, r}^{\Bar{\pi}}\nonumber\\
	&=\frac{1}{N}\sum_{n=1}^{N} V_{P^\star, r}^{\pi^\star}-V_{P^\star, r}^{\Bar{\pi}_n}\\
	&\leq\frac{1}{N} \sum_{n=1}^{N}\left(2V_{\hat{P}^{(n)}, \hat{b}^{(n)}}^{{\pi}_n} +\sqrt{K\zeta_n}\right)\\
	&\leq \frac{1}{N}\left(8H(\sqrt{d}+H)\sqrt{dK(d^2+K)N\log (2|\Phi||\Psi|NH/\delta)\log(1+\frac{N}{d\lambda_n_1})} + \sqrt{KN\log (2|\Phi||\Psi|NH/\delta)}\right)\\
	&\leq 9\sqrt{\frac{H^2(\sqrt{d}+H)^2dK(d^2+K)\log (2|\Phi||\Psi|NH/\delta)}{N}}
\end{align}    
Therefore, to get an $\epsilon$-optimal policy, the total number of iterations $N$ should satisfy that 
\begin{align}
	\frac{81H^2(\sqrt{d}+H)^2dK(d^2+K)\log (2|\Phi||\Psi|NH/\delta)}{\epsilon^2} \leq N
\end{align}
Apply \Cref{lemma: log transform} and drop the log term with respect to $H,d,K$, iteration number $N$ only needs to be in  
\[O\left(\frac{H^{2} d^2 K \max(K,d^2)\log(|\Phi||\Psi|H/\delta)}{\epsilon^2}\right)\leq \tilde{O}\left(\frac{d^4K^2H^4}{\epsilon^2}\right)\]

\[\frac{H^4d^4K^2\log^2(\frac{2|\Phi||\Psi|H^5d^4K^2}{\delta\epsilon^2})}{\epsilon^2} \]

The number of trajectories is hence in the order of
\[\tilde{O}\left(\frac{d^4K^2H^5}{\epsilon^2}\right)\]
\fi   








\iffalse

\begin{theorem} \label{theorem: reward free sample complexity guarantee}(thm1: reward-free sample complexity)
	\[\text{Sample Complexity =} O(\frac{H^{5} K^2 d^4\log(|\Phi||\Psi|/\delta)}{\epsilon^2})\]
\end{theorem}

\begin{Proof}
	For simplicity, let $M_1 = \max(\alpha_n_n) = \tilde{O}(\sqrt{K+d^2})$ and $M_2 = \max \sqrt{\alpha_n_n^2\beta_2} = \tilde{O}(\sqrt{K+d^2})$. $\gamma_n = \max(\gamma_n_n) = \tilde{O}(\sqrt{dK(K+d^2)})$.
	
	By \Cref{prop:step1_appendix}   and \Cref{corollary:V_f_n<V_b_n}
	\begin{align}
		&\quad V^\star_{P^\star, r} - V^{\bpi_n}_{P^\star, r}\\
		&\leq V_{\hat{P}^{(n)}, r+\hat{b}^{(n)}}^{\pi^\star}-V_{P^\star, r}^{\Bar{\pi}_n}\nonumber\\
		&\leq V_{\hat{P}^{(n)}, r+\hat{b}^{(n)}}^{\bar{\pi}_n}-V_{P^\star, r}^{\Bar{\pi}_n}\nonumber\\
		&=\mathop{\Eb}_{s_h\sim(\hP^{(n)},\bar{\pi}_n) \atop a_h \sim \bar{\pi}_n} \left [\sum_{h=1}^{H}\left(\hat{b}_h^n(s_h,a_h)+
		\int(P^\star(s_{h+1}|s_h,a_h)-
		\hat{P}^{(n)}(s_{h+1}|s_h,a_h))V_{P^\star, r}^{\Bar{\pi}_n}(s_{h+1})\right)\right]\\
		&\leq V_{\hat{P}^{(n)}, \hat{b}^{(n)}}^{\Bar{\pi}_n}+{\sum_{h=1}^{H}\mathop{\Eb}_{s_h\sim(\hP^{(n)},\bar{\pi}_n) \atop a_h \sim \bar{\pi}_n}\left[f_h^{(n)}(s_h,a_h)\right]}\\
		&= V_{\hat{P}^{(n)}, \hat{b}^{(n)}}^{{\pi}_n} +
		V_{\hat{P}^{(n)}, f^{(n)}}^{{\pi}_n}\\
		&\leq 2V_{\hat{P}^{(n)}, \hat{b}^{(n)}}^{{\pi}_n} +\sqrt{K\zeta_n}.
	\end{align}
	
	Then, we use \Cref{prop:step3_appendix} to bound the regret.
	
	\begin{align}
		&\quad V_{P^\star, r}^{\pi^\star}-V_{P^\star, r}^{\Bar{\pi}}\nonumber\\
		&=\frac{1}{N}\sum_{n=1}^{N} V_{P^\star, r}^{\pi^\star}-V_{P^\star, r}^{\Bar{\pi}_n}\\
		&\leq\frac{1}{N} \sum_{n=1}^{N}\left(2V_{\hat{P}^{(n)}, \hat{b}^{(n)}}^{{\pi}_n} +\sqrt{K\zeta_n}\right)\\
		&\leq \frac{1}{N}\left(8H(\sqrt{d}+H)\sqrt{dK(d^2+K)N\log (2|\Phi||\Psi|NH/\delta)\log(1+\frac{N}{d\lambda_n_1})} + \sqrt{KN\log (2|\Phi||\Psi|NH/\delta)}\right)\\
		&\leq 9\sqrt{\frac{H^2(\sqrt{d}+H)^2dK(d^2+K)\log (2|\Phi||\Psi|NH/\delta)}{N}}
	\end{align}    
	Therefore, to get an $\epsilon$-optimal policy, the total number of iterations $N$ should satisfy that 
	\begin{align}
		\frac{81H^2(\sqrt{d}+H)^2dK(d^2+K)\log (2|\Phi||\Psi|NH/\delta)}{\epsilon^2} \leq N
	\end{align}
	Apply \Cref{lemma: log transform} and drop the log term with respect to $H,d,K$, iteration number $N$ only needs to be 
	\[O\left(\frac{H^{2} d^2 K \max(K,d^2)\log(|\Phi||\Psi|H/\delta)}{\epsilon^2}\right)\leq \tilde{O}\left(\frac{d^4K^2H^4}{\epsilon^2}\right)\]
	
	\[\frac{H^4d^4K^2\log^2(\frac{2|\Phi||\Psi|H^5d^4K^2}{\delta\epsilon^2})}{\epsilon^2} \]
	
	The number of trajectories is :
	\[\tilde{O}\left(\frac{d^4K^2H^5}{\epsilon^2}\right)\]
	
\end{Proof}
Note that by elliptical potential inequality:
\begin{align}
	\sum_{n=1}^{N} \frac{F_n}{\alpha_n_n}  \leq H\sqrt{dKN\log(1+\frac{N}{
			\lambda_n_1 d})}
\end{align}

and

\begin{align}
	\sum_{n=1}^{N}\frac{G_n}{\gamma_n_n}\leq H\sqrt{dN\log(1+\frac{N}{
			\lambda_n_1 d})}
\end{align}

Then, we have 
\begin{align}
	&\quad V_{P^\star, r}^{\pi^\star}-V_{P^\star, r}^{\Bar{\pi}}\nonumber\\
	&\leq 3H\sqrt{\frac{K\log(|\Phi||\Psi|N/\delta)}{N}} + 2M_2\sqrt{\frac{dK}{N}} +  2M_1H^2\sqrt{\frac{dK\log(1+N/\lambda_n_1/d)}{N}} + \gamma_n H\sqrt{\frac{d\log(1+N/\lambda_n_1/d)}{N}}
\end{align}

Using big-O notation, we have 

\begin{align}
	&\quad V_{P^\star, r}^{\pi^\star}-V_{P^\star, r}^{\Bar{\pi}}\nonumber\\
	&\leq \tilde{O}\left(\sqrt{H\frac{K}{N}} +  \frac{\sqrt{dK(K+d^2)}}{\sqrt{N}}  +H^2\sqrt{\frac{dK(K+d^2)}{N}}+ \frac{H\sqrt{dK(K+d^2)}\sqrt{d}}{\sqrt{N}}\right)\\
	&\leq \tilde{O}\left(\frac{H(H+\sqrt{d})\sqrt{dK(K+d^2)}}{\sqrt{N}}\right)
\end{align}
\fi




% \section{Proof of \Cref{th:lowerbound} (Lower Bound on Sample Complexity)}\label{sec:lower_bound}
% We first summarize the outline of our proof here.
% The idea of the proof is as follows. We first construct a low-rank representation for the tabular case with low dimension. The state-of-the-art lower bound for tabular MDPs is given by $\Omega\big(\frac{H|\Sc| |\Ac|}{\epsilon^2}\big)$, where $\Sc$ and $|\Ac|$ are the state and action spaces, respectively. By embedding $\Sc\times\Ac$ into a $d=\Omega\left(|\Sc|\right)$-dimensional space, we are able to show that the lower bound for low-rank MDPs is given by $\Omega\big(\frac{HdK}{\epsilon^2}\big)$. 

% \begin{figure}[th]
% 	\centering
% 	\includegraphics[scale=0.4]{tree.png}
% 	\caption{Tree-structure of the MDP.}
% 	\label{fig:tree}
% \end{figure}

% \begin{proof}[Proof of \Cref{th:lowerbound}]
% 	Section 3.1 in \citet{domingues2020episodic} provides a lower bound $\Omega\left(\frac{H|\Sc||\Ac|\log(1/\delta)}{\epsilon^2}\right)$ for tabular MDP by constructing a set of MDP instances. Specifically, There are $H|\Sc||\Ac|$ MDPs which are hard to distinguish in KL divergence, while the corresponding optimal policies are completely different. By leveraging tools from information theory, it shows that at least $\Omega\left(H|\Sc||\Ac|/\epsilon^2\right)$ sample trajectories are required to identify an $\epsilon$-optimal policy. %\textcolor{green}{should briefly explain how such construction relates to the proof of lower bound.}
	
% 	Below, we restate the MDP structure as in \Cref{fig:tree} and then embed the states and actions into $\Rb^{\Omega(|\Sc|)}$, where $\Sc$ is the state space. We normalize the reward functions to satisfy $\sum_{h}r_h\leq 1.$.
% 	Note that for any transition probability $P(s_{h+1}|s_h,a_h): \Sc\times \Ac \times \Sc \rightarrow [0,1]$, we can always find embeddings $\phi:\Sc\times\Ac\rightarrow\Rb^d$ and $\mu\rightarrow\Rb^d$, where $d = |\Sc||\Ac|$. If we index coordinates by $(s,a)$, then $\phi(s,a)$ is a standard unit vector $e_{(s,a)}$ and the $(s,a)$-th coordinate of $\mu(s_{h+1})$ is the transition probability $P(s_{h+1}|s,a)$. We call such construction the standard embedding with dimension $d=|\Sc||\Ac|$. Another fact is that, we can always embedding a low dimensional space into a higher dimensional space.
	
% 	Now, we are ready to construct the representations for instances in \citet{domingues2020episodic}. Since the MDP has a $K$-ary tree, as illustrated in \Cref{fig:tree}, at the level of leaves, we have the most states denoted by $s^1,\ldots, s^L$. Let $h$ be the step where the leaves lie. Note that at $h+1$, we only have two states $s_g$ and $s_b$ representing a good state and a bad state. Suppose choosing $a^k$ at state $s^\ell$ will increase the probability visiting $s_g$ by $\epsilon_0$. Then, the transition probabilities at step $h$ are defined as follows.
% 	\begin{align}
% 		&P_h(s_g | s_h, a_h) = \frac{1}{2} + \epsilon_0\mathbf{1}_{\{s_h=s^\ell,a_h = a^k \}},\\
% 		&P_h(s_b | s_h, a_h) = \frac{1}{2} - \epsilon_0\mathbf{1}_{\{s_h=s^\ell,a_h = a^k \}}.
% 	\end{align}
	
% 	We can define 2-dimensional features over the transition probabilities as follow. 
% 	\begin{align}
% 		&\phi_h(s_h,a_h) = \left(\frac{1}{2}, \frac{1}{2}\right)^\top +  \mathbf{1}_{\{s_h=s^\ell,a_h = a^k \}}(\epsilon_0,-\epsilon_0)^\top,\\
% 		&\mu_h(s_g) = (1,0), \\
% 		&\mu_{h}(s_b) = (0,1).
% 	\end{align}
% 	We finally construct a standard embedding with dimension $L$ on the $(h-1)$-th step, since the number of state is $L/K$ and the number of actions is $K$. If an algorithm $\mathtt{Alg}$ can identify an $\epsilon$-optimal policy on those MDPs, according to Section 3.1 in \citet{domingues2020episodic}, $\mathtt{Alg}$ should collect at least $\Omega\left(HL|\Ac|\log(1/\delta)/\epsilon^2\right)$ sample trajectories. With the fact that $d=L$, a lower bound $\Omega\left(\frac{HdK}{\epsilon^2}\right)$ for low-rank MDP is hence established. 
% \end{proof}




\section{{Proof of \Cref{th:lowerbound} (Lower Bound on Sample Complexity)} }\label{sec:lower_bound}
\subsection{{Step 1: Construction of Hard MDP instances}}
Our hard MDP instances is inspired by \citet{domingues2020episodic} for tabular MDPs.  However, the lower bound in \cite{domingues2020episodic} requires $S \geq K$, where $S,K$ denote the cardinality of state and action space respectively. Our hard MDP instances remove the assumption that $S\geq K$ by constructing the action set with two types of actions. The first type of actions is mainly used to form a large state space through a tree structure. The second type of actions is mainly used to distinguish different MDPs. Such a construction allows us to separately treat the state space and the action space, so that both state and action spaces can be arbitrarily large. We then explicitly define the feature vectors for all state-action pairs and show our hard MDP instances have a low-rank structure with dimension $d=S$. %We consider hard MDP instances with finite state space $\mathcal{S}$ and finite action space $\mathcal{A}$, where $|\Sc|=S, |\Ac|=K$. 
In a nutshell, we construct a family of $HdK$ MDPs that are hard to distinguish in KL divergence, while the corresponding optimal policies are very different as shown in \Cref{fig:tree}. 
\begin{figure}[th]
	\centering
	\includegraphics[scale=0.3]{Hard_instances.jpg}
	\caption{Hard MDP instances.}
	\label{fig:tree}
\end{figure}


\if{0}
Below, we first state the MDP structure as in \Cref{fig:tree} and then show that the MDP has a low-rank structure.

There are 4 special states: a waiting state $s_w$, a good state $s_g$, a bad state $s_b$ and a break state $s_{break}$. The action state is divided into two disjoint action sets: $\mathcal{A}_0$ and $\mathcal{A}_1$, where $|\Ac_0|=K_0, |\Ac_1|=K_1$.

The agent starts in the \textit{waiting state} $s_w$ where it can take an action $a_w$ to stay in $s_w$ up to a stage $\bar{H} \leq H$, after which the agent has to leave $s_w$. 

From $s_w$, the agent can only transition to the \text{root state} $s_{root}$, from which she can reach a $(K_0+1)$-ary tree formed by the rest $S-4$ states. We assume deep of the tree is $D$ and the number of leaves states of the tree is $L$, and $S-4=\sum_{i=1}^{D-1}(K_0)^{i}, L=K_0^{D-1}$. In the first $D-1$ layer, for each parent state, if the agent takes any action $a\in\left\{a^1,\ldots,a^{K_0}\right\}$ from the action set $\Ac_0$, then she will take a deterministic transition and reach a corresponding child state, which then becomes the parent state of next stage. If the agent takes any action $a \in \left\{a^{K_0+1}, \ldots, a^{K_0+K_1}\right\}$, she will take a deterministic transition to a special break state $s_{break}$, which is an absorbing state no matter what action $a\in\Ac$ the agent takes. In addition, $s_{break}$ will not receive any rewards at any stage. 

In the last $D$ layer, there are $L$ leaves states $\Lc=\{s^1, \ldots, s^L\}$, from which the agent can reach the good states $s_g$ and the bad state $s_b$. $s_g$ is absorbing and is the only state where the agent obtains rewards. The reward of $s_g$ starts to be 1 at stage $\bar{H}+D+1$. and a bad state $s_b$ that is absorbing and gives no reward. In addition, the good state and the bad state can only reached from leaves states. There is a single action $a^\star \in \Ac$ in a single state $s^{\ell^*} \in \Lc$ of leaves states required to be taken only at certain stage $h^* \in [\bar{H}+D]$ that can increase the probability of arriving to the good state by $\epsilon$. We remark here, to reach the good state $s_g$ and the bad state $s_b$ from the leaves state, the agent can take any action $a \in \Ac$, but to reach the leaves states and avoid entering the break state $s_{break}$, the agent should take action belongs to $\Ac_0$. 

\fi






First, we define a reference MDP $\Mc_0$ as follows. We start with the construction of the state space $\Sc$ and the action space $\Ac$.
 
\begin{itemize}
    \item Let $\Sc=\{s_w,s_{o},s_g,s_b\}\bigcup_{i\in[D],j\in[2^{i-1}]}\{s^{i,j}\}$, where $s_w,s_{o},s_g$ and $s_b$ denote `waiting state', `outlier state', `good state', and `bad state', respectively. The states in $\bigcup_{i\in[D],j\in[2^{i-1}]}\{s^{i,j}\}$ form a binary tree, where $s^{i,j}$ denotes the $j$-th branch node of the layer $i$.
 
     \item Let $\Ac=\{a_w,a^1,a^2\}\bigcup\Ac_0$, where $a_w$ denotes `waiting action', $a^1,a^2$ are two unique actions that form the binary tree, and $|\Ac_0| = K-3.$ 
\end{itemize}
 
 
Then, the transition probabilities of $\Mc_0$ are specified through the following rules.
 \begin{itemize}
     \item The initial state is the waiting state $s_w$.
     
     \item If the agent takes the waiting action $a_w$ before time step $\bar{H}$, waiting state $s_w$ stays on itself. Otherwise, $s_w$ transits to the root state $s^{1,1}$ of the binary tree. 
     
     Mathematically, $\Pb_h[s_w|s_w,a_w]=\mathbbm{1}_{h\leq \bar{H}}$, and  $\Pb_h[s^{1,1}|s_w,a]=\mathbbm{1}_{a\neq a_w \text{ or } h> \bar{H}}.$ 
     \item When $i< D$, for states $s^{i,j}$ in the binary tree, we have the following transition rules:
     
     \begin{itemize}
     \item If the agent takes actions $a_1$ or $a_2$, $s^{i,j}$ deterministically transits to its children $s^{i+1,2j-1}$ or $s^{i+1,2j}$, respectively.
     
     Mathematically, $\Pb_h[s^{i+1,2j-1}|s^{i,j},a^1] =1$, and $\Pb[s^{i+1,2j}|s^{i,j},a^2] = 1.$
     
     \item If the agent takes any action other than $a^1$ or $a^2$, the agent will reach the outlier state $s_{o}$.
     
     Mathematically, $\Pb_h[s_o|s^{i,j},a]=1, \forall a\neq a^1,a^2.$
     \end{itemize}
     
     \item Leaf state $s^{D,j}$ uniformly transits to good state $s_g$ and bad state $s_b$ no matter what action the agent takes.
     
     Mathematically, $\Pb_h[s_g|s^{D,j},a] = \Pb_h[s_b|s^{D,j},a] = \frac{1}{2}, \forall a\in\mathcal{A}.$
     
     \item Good state $s_g$, bad state $s_b$, and outlier state $s_o$ are absorbing states.
 \end{itemize}
 

 
Now, we define the features as follows, which are $S$-dimensional vectors.
 
 \begin{align*}
 \begin{array}{lll}
    s_w & \phi_h(s_w,a_w) = (1,0,\mathbf{0}_{S-5},0,0,0)    & \mu_h(s_w) = (\mathbbm{1}_{h\leq \bar{H}},0,\mathbf{0}_{S-5},0,0,0) \\ 
    \vspace{0.2cm}
     &\phi_h(s_w, a) = (0,1,\mathbf{0}_{S-5},0,0), a\neq a_w & \mu_h(s^{1,1}) = (\mathbbm{1}_{h>\bar{H}},1,\mathbf{0}_{S-5},0,0,0)\\
     s^{i,j},i<D &\phi_h(s^{i,j},a^{\omega}) = (0,0,\mathbf{e}_{i+1,2j+\omega-2},0,0,0), \omega=1,2 & \mu_h(s^{k,\ell}) = (0,0,\mathbf{e}_{k,\ell},0,0,0), 1<k\leq D\\
     \vspace{0.2cm}
      & \phi_h(s^{i,j},a)=(0,0,\mathbf{0}_{S-5},1,0,0),a\neq a^1,a^2 & \mu_h(s_o) = (0,0,\mathbf{0}_{S-5},1,0,0) \\
     \vspace{0.2cm}
     s^{D,j} & \phi_h(s^{D,j}, a) = (0,0,\mathbf{0}_{S-5},0,\frac{1}{2},\frac{1}{2}) &\mu_h(s_g) = (0,0,\mathbf{0}_{S-5},0,1,0)\\
     s_g,s_b &\phi_h(s_g,a) = \mu_h(s_g),~~\phi_h(s_b,a) = \mu_h(s_b) & \mu_h(s_b) = (0,0,\mathbf{0}_{S-5},0,0,1)\\
     s_o& \phi_h(s_o,a)=\mu_h(s_o),
\end{array}
\end{align*}
    where $\mathbf{0}_{S-5} \in \Rb^{S-5}$ denotes the $S-5$ dimension vector with all zeros and $\mathbf{e}_{i,j} \in \Rb^{S-5}$ denotes the one-hot vector that is zero everywhere except the $(2^{i-1}+j-2)$-th coordinate. Here $2\leq i\leq D, 1\leq j\leq 2^{i-1}.$

For each $(h^*, \ell^*, a^*) \in \{1+D,\ldots,\bar{H}+D\} \times [2^{D-1}] \times \Ac$, we define an MDP $\mathcal{M}_{(h^*,\ell^*,a^*)}$ through $\Mc_0$. Specifically, the only difference of $\mathcal{M}_{(h^*,\ell^*,a^*)}$ from $\Mc_0$ is that the transition probability from the leaf state $s^{D,\ell^*}$ and action $a^*$ to the good state $s_g$ increases $\epsilon_0$, i.e. $\Pb_{h^*}[s_g|s^{D,\ell^*}, a^*] = \frac{1}{2}+\epsilon_0,$ and $\Pb_{h^*}[s_b|s^{D,\ell^*},a^*] = \frac{1}{2}-\epsilon_0$, where $\epsilon_0$ will be specified later. We note that the features of $\Mc_{(h^*,\ell^*,a^*)}$ are the same as those of $\Mc_0$ except that $\phi_{h^*}(s^{D,\ell^*},a^*) = (0,0,\mathbf{0}_{S-5},0,\frac{1}{2}+\epsilon_0,\frac{1}{2}-\epsilon_0)$.

 

% \textbf{Transition kernels from $s_w$}:
% \begin{align}
% 	&P_h(s_w|s_w,a_h)=\mathbbm{1}_{\{a_h=a_w,h \leq\bar{H}\}},\\
% 	&P_h(s_{root}|s_w,a_h)=1-P_h(s_w|s_w,a_h),\\
% 	&P_h(s_h|s_w,a_h)=0, \forall a_h \in \mathcal{A}, s\neq s_w, s_{root}.
% \end{align}
% For $h \leq \bar{H}$, we can define the 2-dimensional features as following:
% \begin{align*}
% 	&\phi_h(s_w,a_w) = (1,0),\\
% 	&\phi_h(s_w,a_h)=(0,1), a_h\neq a_w,\\
% 	&\mu_h(s_w) = (1,0), \mu_{h}(s_{root}) = (0,1),\\
% 	&\mu_{h}(s)=(0,0), s\neq s_w ,s_{root}.
% \end{align*}
% For $h \geq \bar{H}$, we can define the 2-dimensional features as following:
% \begin{align*}
% 	&\phi_h(s_w,a_h) = (1,0), \forall a_h.\\
% 	&\mu_h(s_{root}) = (1,0), \mu_{h}(s_w) = (0,1),\\
% 	&\mu_{h}(s)=(0,0), s\neq s_w ,s_{root}.
% \end{align*}
% \textbf{Transition kernels of the tree structure}: in any layer $\bar{D}\in[D-1]$, there are $K_0^{\bar{D}-1}$ parent states (branch states) denoted as $\Bc^{\bar{D}}=\{s^{(\bar{D},1)}, \ldots, s^{(\bar{D},K_0^{\bar{D}-1})}\}$. For layer $\bar{D}\in[D-1]$, any $i \in [K_0^{\bar{D}-1}], k\in [K_0+K_1], j\in [K_0^{\bar{D}}]$,  consider the state $s^{(\bar{D},i)}$, action $a^k$, the transition kernel is as following:
% \begin{align*}
% 	&P_h(s^{(\bar{D}+1,j)}|s^{(\bar{D},i)},a^k)=\mathbbm{1}_{\{j=K_0(i-1)+k,k\in[K_0]\}},\\
% 	&P_h(s_{break}|s^{(\bar{D},i)},a^k)=\mathbbm{1}_{\{k\in\{K_0+1,\ldots,K_0+K_1\}\}},
% \end{align*} 
% which admits the following decomposition: 
% \begin{align*}
% 	&\phi_h(s^{(\bar{D},i)},a^k) = \mathbbm{1}_{\{k\in[K_0]\}}{e_{K_0(i-1)+k}^{\bar{D}}}^\top + \mathbbm{1}_{\{k\in\left\{K_0+1,\ldots,K_0+K_1\right\}\}}{e_{K_0^{\bar{D}}+1}^{\bar{D}}}^\top,\\
% 	&\mu_h(s^{(\bar{D}+1,j)}) = e_j^{\bar{D}} , \mu_{h}(s_{break}) = e_{K_0^{\bar{D}}+1}^{\bar{D}}.
% \end{align*}
% where $e_i^{\bar{D}} \in \Rb^{K_0^{\bar{D}}+1}$ denote a one-hot vector that is zero everywhere except that the entry corresponding to $i$ is one.

% \textbf{Transition kernels from leaves states $\Lc$ to the good state or the bad state}: for any stage $h \in \{1+D,\ldots,\bar{H}+D\}$, state $s^\ell \in \Lc$ and action $a_h \in \Ac$,
% \begin{align*}
% 	&P_h(s_g | s^\ell, a_h) = \frac{1}{2} + \epsilon_0\mathbbm{1}_{\{h=h^*,s^\ell=s^{\ell^*},a_h = a^* \}},\\
% 	&P_h(s_b | s^\ell, a_h) = \frac{1}{2} - \epsilon_0\mathbbm{1}_{\{h=h^*,s^\ell=s^{\ell^*},a_h = a^* \}},\\
% 	&P_h(s_{h+1} | s^\ell, a_h) = 0, s_{h+1} \neq s_g, s_b,
% \end{align*}
% which means that the leaves states can only reach the good state and the bad state and admits a 2-dimension decomposition: 
% \begin{align*}
% 	&\phi_h(s^\ell,a) = \left(\frac{1}{2}, \frac{1}{2}\right)^\top +  \mathbbm{1}_{\{h=h^*,s^\ell=s^{\ell^*},a_h = a^* \}}(\epsilon_0,-\epsilon_0)^\top,\\
% 	&\mu_h(s_g) = (1,0), \mu_{h}(s_b) = (0,1), \mu_{h}(s_h) = (0,0), s_h \neq s_g, s_b.
% \end{align*}

% \textbf{Transition kernels from the good state or the bad state}: for any $a_h \in \Ac$,
% \begin{align*}
% 	P_h(s_g | s_g, a_h) = 1,P_h(s_b | s_b, a_h) = 1,
% \end{align*}
% which admits a 2-dimension decomposition:
% \begin{align*}
% 	&\phi_h(s_h,a_h) = \left(\mathbbm{1}_{\{s_h=s_g\}}, \mathbbm{1}_{\{s_h=s_b\}}\right)^\top,\\
% 	&\mu_h(s_{h+1}) = \left(\mathbbm{1}_{\{s_{h+1}=s_g\}}, \mathbbm{1}_{\{s_{h+1}=s_b\}}\right).
% \end{align*}

We remark here that the cardinality $K$ of $\Ac$ can be arbitrarily large, so that the resulting lower bound will hold for both $d \leq K$ and $d \geq K$ regimes. In addition, although in our hard instances, $d=S$, it is straightforward to generalize it to the regime with $S>d$ if we set the outlier state $S_o$ to be a set of outlier states $\mathcal{S}_o$.

\textbf{Definition of reward}: the reward can only be attained in two special states: the good state $s_g$ and the outlier state $s_o$ at the last stage $H$. 
\begin{align*}
	\forall \Sc \in \Ac, a \in \Ac, r_h(s,a)= \mathbbm{1}_{\{s=s_g,h=H\}}+\frac{1}{2}\mathbbm{1}_{\{s=s_o,h=H\}},
\end{align*}
and $r_h(s,a)$ still belongs to $[0,1]$.
\subsection{{Step 2: Analysis of Hard MDP Instances}}

%Our analysis adapts the proof steps in \cite{domingues2020episodic} to the very different hard MDP instances that we construct in Step 1.

%The analysis of lower bound is inspired by \cite{domingues2020episodic}. And our analysFor completeness, we provide the detailed proof here. 
%



\iffalse
\subsubsection{Supporting Lemma}


\begin{lemma}[Lemma 5 of \cite{domingues2020episodic}]\label{lemma: lb-1}
	Let $\Mc$ and $\Mc^\prime$ be two MDPs that are identical except for different transition kernels. We denote the corresponding transition kernels as $P_h$ and $P_h^\prime$. Then for any stopping time $\tau$ w.r.t filtration $(\Fc_H^t)_{t \geq 1}$ that satisfies $\Pb_{\Mc}[\tau \leq \infty]=1$, we have 
	\begin{align*}
		\mathrm{KL}\left(\Pb_{\Mc}^{I_H^\tau},\Pb_{\Mc^\prime}^{I_H^\tau}\right)=\sum_{s \in \Sc}\sum_{a \in \Ac} \sum_{h \in [H-1]}\Eb_{\Mc}[N_{h,s,a}^\tau]\mathrm{KL}\left(P_h(\cdot|s,a),P_h^\prime(\cdot|s,a)\right),
	\end{align*}
	where $N_{h,s,a}^\tau=\sum_{n=1}^\tau\mathbbm{1}_{\left\{(s_h^n,a_h^n)=(s,a)\right\}}$ and $I_H^\tau$ is the random vector representing the history up to the episode $\tau$.
\end{lemma}

\begin{lemma}[Lemma 6 of \cite{domingues2020episodic}]\label{lemma: lb-2}
Consider a measurable space $(\Omega,\Fc)$ equipped with two distribution $\Pb_1$ and $\Pb_{2}$. For any $\Fc-$ measurable function $Z: \Omega \rightarrow [0,1]$, we have \begin{align*}
    \text{KL}\left(\Pb_0,\Pb_1\right) \geq \text{kl}\left(\Eb_1[Z],\Eb_2[Z]\right).
\end{align*}
	
\end{lemma}
\fi


\begin{proof}[Proof of \Cref{th:lowerbound}]
Let $\epsilon_0=2\epsilon$.

For any MDP $\Mc_{h^*,a^*}$, the optimal policy is to take action $a_w$ to stay at state $s_w$ until stage $h^*-D$, and then take the corresponding action to the only state $s^{D,\ell^*}$ at stage $h^*$. At state $s^{D,\ell^*}$, the agent takes the only optimal action $a^*$. The optimal value function $V_{\mathcal{M}_{(h^*,\ell^*,a^*)}}^*=\frac{1}{2}+\epsilon_0$, and the value function of the output policy of $\mathtt{Alg}$ is given by
	\begin{align}
		V^{\hat{\pi}_\tau}_{\mathcal{M}_{(h^*,\ell^*,a^*)}}=\frac{1}{2}+\epsilon_0\Pb^{\hat{\pi}_\tau}_{\mathcal{M}_{(h^*,\ell^*,a^*)}}[s_{h^*}=s^{\ell^*},a_{h^*}=a^*], \label{Eq: lb-4}
	\end{align} 
where $\Pb^{\hat{\pi}_\tau}_{\mathcal{M}_{(h^*,\ell^*,a^*)}}$ is the probability distribution over the states and actions $(s_h,a_h)$ following the Markov policy $\hat{\pi}_\tau$ in the MDP ${\mathcal{M}_{(h^*,\ell^*,a^*)}}$. We remark that the reward of outlier state are specially designed to be $1/2$ at stage $H$ to make \Cref{Eq: lb-4} hold for policy $\hat{\pi}_\tau$ falling into the outlier state. 

Hence,
\begin{align*}
	V_{\mathcal{M}_{(h^*,\ell^*,a^*)}}^*-V^{\hat{\pi}_\tau}_{\mathcal{M}_{(h^*,\ell^*,a^*)}} 
	&= \epsilon_0(1-\Pb^{\hat{\pi}_\tau}_{\mathcal{M}_{(h^*,\ell^*,a^*)}}[s_{h^*}=s^{\ell^*},a_{h^*}=a^*])\\
	& = 2\epsilon(1-\Pb^{\hat{\pi}_\tau}_{\mathcal{M}_{(h^*,\ell^*,a^*)}}[s_{h^*}=s^{\ell^*},a_{h^*}=a^*]).
\end{align*} 
and 
\begin{align}
	V_{\mathcal{M}_{(h^*,\ell^*,a^*)}}^*-V^{\hat{\pi}_\tau}_{\mathcal{M}_{(h^*,\ell^*,a^*)}} \leq \epsilon \Leftrightarrow \Pb^{\hat{\pi}_\tau}_{\mathcal{M}_{(h^*,\ell^*,a^*)}}[s_{h^*}=s^{\ell^*},a_{h^*}=a^*] \geq \frac{1}{2} \label{Eq: lb-3}.
\end{align} 


The transitions of all MDPs are the same when the leaves states are reached. We define the event 
\begin{align*}
    \varepsilon^\tau_{(h^*,\ell^*,a^*)}=\left\{\Pb^{\hat{\pi}_\tau}_{\mathcal{M}_{(h^*,\ell^*,a^*)}}[s_{h^*}=s^{\ell^*},a_{h^*}=a^*] \geq \frac{1}{2}\right\}.
\end{align*}
From \Cref{Eq: lb-3}, the event is equal to the event $\{V_{\mathcal{M}_{(h^*,\ell^*,a^*)}}^*-V^{\hat{\pi}_\tau}_{\mathcal{M}_{(h^*,\ell^*,a^*)}} \leq \epsilon\}$. As a result,
\begin{align*}
    \Pb_{(h^*,\ell^*,a^*)}\left[\varepsilon^\tau_{(h^*,\ell^*,a^*)}\right]= \Pb_{(h^*,\ell^*,a^*)}\left[V_{\mathcal{M}_{(h^*,\ell^*,a^*)}}^*-V^{\hat{\pi}_\tau}_{\mathcal{M}_{(h^*,\ell^*,a^*)}} \leq \epsilon\right] \geq 1-\delta.
\end{align*}


Recall that 
$N^\tau_{(h^*,\ell^*,a^*)}= \sum_{n=1}^{\tau}\mathbbm{1}_{\left\{(s_{h^*}^n,s_{h^*}^n)=(s^{\ell^*},a^*)\right\}}$
such that $\sum_{(h^*,\ell^*,a^*)}N^\tau_{(h^*,\ell^*,a^*)}\leq\tau$. This inequality holds because the agent is likely to fall into the outlier state $s_{o}$. We denote $\Pb_0$ and $\Eb_0$ to be with respect to $\Mc_0$. 

Now, we invoke an intermediate result in the proof of Theorem 7 in \citet{domingues2020episodic} to conclude that
\iffalse
For any $\Fc^\tau_H$-measurable random variable $Z$ taking values in $[0,1]$, we have
\begin{align*}
    16\epsilon^2\Eb_{0}\left[N^\tau_{(h^*,\ell^*,a^*)}\right] &= \Eb_{0}\left[N^\tau_{(h^*,\ell^*,a^*)}\right]{4\epsilon_0^2}\\
    &{\geq} \Eb_{0}\left[N^\tau_{(h^*,\ell^*,a^*)}\right]\text{kl}(\frac{1}{2},\frac{1}{2}+\epsilon_0)\\
    &\overset{\RM{1}}{=}\text{kl}(\Pb_0^{I_H^\tau},\Pb_{(h^*,\ell^*,a^*)}^{I_H^\tau})\\
    &\overset{\RM{2}}{\geq}\text{kl}(\Eb_{0}\left[Z\right],\Eb_{(h^*,\ell^*,a^*)}\left[Z\right])
\end{align*}
where $\RM{1}$ follows from \Cref{lemma: lb-1} and $\RM{2}$ follows from \Cref{lemma: lb-2}. \yl{Should your ``kl" be ``KL"?}




For any $(h^*,\ell^*,a^*)$, if $\epsilon_0 \leq \frac{1}{4}$ and let $Z=\mathbbm{1}_{\{\varepsilon^\tau_{(h^*,\ell^*,a^*)}\}}$, we have 
\begin{align*}
    &\text{kl}(\Eb_{0}\left[Z\right],\Eb_{(h^*,\ell^*,a^*)}\left[Z\right]) = \text{kl}\left(\Pb_{0}\left[\{\varepsilon^\tau_{(h^*,\ell^*,a^*)}\}\right],\Pb_{(h^*,\ell^*,a^*)}\left[\{\varepsilon^\tau_{(h^*,\ell^*,a^*)}\}\right]\right)\\
    & \quad \geq \left(1-\Pb_{0}\left[\{\varepsilon^\tau_{(h^*,\ell^*,a^*)}\}\right]\right)\log\left(\frac{1}{1-\Pb_{(h^*,\ell^*,a^*)}\left[\{\varepsilon^\tau_{(h^*,\ell^*,a^*)}\}\right]}\right)-\log(2)\\
    & \quad \geq \left(1-\Pb_{0}\left[\{\varepsilon^\tau_{(h^*,\ell^*,a^*)}\}\right]\right)\log\left(\frac{1}{\delta}\right)-\log(2).
\end{align*}

\fi
\begin{align*}
     \Eb_{0}\left[N^\tau_{(h^*,\ell^*,a^*)}\right]
     \geq \frac{1}{16\epsilon^2}\left[\left(1-\Pb_{0}\left[\{\varepsilon^\tau_{(h^*,\ell^*,a^*)}\}\right]\right)\log\left(\frac{1}{\delta}\right)-\log(2)\right].
\end{align*}
Summing over all $(h^*,\ell^*,a^*)$, we have 
\begin{align}
    \Eb_{0}[\tau] &\geq \sum_{(h^*,\ell^*,a^*)}\Eb_{0}\left[N^\tau_{(h^*,\ell^*,a^*)}\right]\nonumber\\
    & \geq \frac{1}{16\epsilon^2}\left[\left(\bar{H}LK-\sum_{(h^*,\ell^*,a^*)}\Pb_{0}\left[\varepsilon^\tau_{(h^*,\ell^*,a^*)}\right]\right)\log(\frac{1}{\delta})-\bar{H}LK\log2\right]. \label{Eq: LB-1}
\end{align}
Notice that 
\begin{align}
    \sum_{(h^*,\ell^*,a^*)}\Pb_{0}\left[\varepsilon^\tau_{(h^*,\ell^*,a^*)}\right]=\Eb_{0}\left[\sum_{(h^*,\ell^*,a^*)}\mathbbm{1}_{\{\Pb^{\hat{\pi}_\tau}_{\mathcal{M}_{(h^*,\ell^*,a^*)}}[s_{h^*}=s^{\ell^*},a_{h^*}=a^*] \geq \frac{1}{2}\}}\right] \leq 1. \label{Eq: LB-2}
\end{align}
Substituting \Cref{Eq: LB-2} into \Cref{Eq: LB-1} yields
\begin{align*}
    \Eb_{0}[\tau] & \geq \frac{1}{16\epsilon^2}\left[\left(\bar{H}LK-1\right)\log(\frac{1}{\delta})-\bar{H}LK\log2\right] \\
   & \geq \frac{1}{32\epsilon^2}\bar{H}LK\log(\frac{1}{\delta}),
\end{align*}
where we use the fact that $\delta<1/16$. With the assumption of $K\geq 3, S\geq 6$, we have $d=S$. Taking $\bar{H}=\frac{H}{3}$ and with the assumption of $D \leq H/3$, we have 
\begin{align*}
    \Eb_{0}[\tau]=\Omega\left(\frac{HdK}{\epsilon^2}\log(\frac{1}{\delta})\right).
\end{align*}
Then following the analysis similar to that for Corollary 8 in \cite{domingues2020episodic}, with probability at least $1-\delta$, the number of iterarion is at least
\begin{align*}
    \Omega\left(\frac{HdK}{\epsilon^2}\log(\frac{1}{\delta})\right).
\end{align*}
\end{proof}

\section{\Cref{alg: feature}: RepLearn and Proof of \Cref{Thm: individual representation}}\label{sec: app feature learning}
We first present the full algorithm in \Cref{sec: feature learning} below as \Cref{alg: feature}.
%%%%Algorithm
% \begin{algorithm}
% 	\begin{algorithmic}[1]
% 		\caption{Representation Learning via RAFFLE}\label{alg: feature}
% 		\STATE {\bfseries Input:} {Sample size $N_f$, state action pair distribution $\{q_h\}_{h=1}^{H}$, special designed reward function $\{r^{h,t}\}_{h \in [H], t \in [T]}$ and policy $\{\pi^t\}_{t \in [T]}$ and estimated transition kernel $\hP$} from the output of \Cref{Algorithm: RAFFLE}, model class: $\Phi$.
% 		\STATE Initialize $\mathcal{D}_h^{0,0}=\emptyset$.
% 		\FOR{$n=1,\ldots,N_f$}
% 		\FOR{$h=1,\ldots,H$}
% 		\FOR{$t=1,\ldots,T$}
% 		\STATE Choose $(s_h^{n,t},a_h^{n,t})\sim q_h$ and add $(s_h^{n,t},a_h^{n,t})$ to the dataset $\Dc_h^{n,t}=\Dc_{h}^{n-1,t} \cup (s_h^{n,t},a_h^{n,t})$.
% 		\ENDFOR
% 		\ENDFOR
% 		\ENDFOR
% 		\FOR{$h=1,\ldots,H$}
% 		\STATE Learn $(\tphi_h,\tilde{w}_h^1,\ldots,\tilde{w}_h^T)$ by  
% 		\begin{align*}
% 			\mathop{\arg\min}_{\phi_h \in \Phi,w_h^t\in \Rb^d}\sum_{t\in[T]}\sum_{(s_h,a_h) \in \Dc^{N_f,t}_h} (Q^{\pi^t}_{\hP,h, r^{h,t}}(s_h,a_h)-\langle\phi_h(s_h,a_h),w_h^t\rangle)^2.
% 		\end{align*}
% 		\ENDFOR
% 		\STATE \textbf{Output:} $\tphi=\{\tphi_h\}_{h\in[H]}$.
% 	\end{algorithmic}
% \end{algorithm}
%%%%%%%%
\begin{algorithm}
	\begin{algorithmic}[1]
		\caption{{\bf RepLearn}: Representation Learning in Planning Phase of RAFFLE}\label{alg: feature}
		\STATE {\bfseries Input:} {Sample size $N_f$, state-action pair distributions $\{q_h\}_{h=1}^{H}$, special designed reward function $\{r^{h,t}\}_{h \in [H], t \in [T]}$ and policy $\{\pi^t\}_{t \in [T]}$, and estimated transition kernel $\hP$} from the output of \Cref{Algorithm: RAFFLE}, model class: $\Phi$.
		\STATE Initialize $\mathcal{D}_h^{0,0}=\emptyset$.
		\FOR{$n=1,\ldots,N_f$}
		\FOR{$h=1,\ldots,H$}
		\FOR{$t=1,\ldots,T$}
		\STATE Choose $(s_h^{n,t},a_h^{n,t})\sim q_h$ and add $(s_h^{n,t},a_h^{n,t})$ to dataset $\Dc_h^{n,t}=\Dc_{h}^{n-1,t} \cup (s_h^{n,t},a_h^{n,t})$.
		\ENDFOR
		\ENDFOR
		\ENDFOR
		\FOR{$h=1,\ldots,H$}
		\STATE Learn $(\tphi_h,\tilde{w}_h^1,\ldots,\tilde{w}_h^T)$ as in \Cref{ineq: alg-linear regression}.
		\ENDFOR
		\STATE \textbf{Output:} $\tphi=\{\tphi_h\}_{h\in[H]}$.
	\end{algorithmic}
\end{algorithm}



%%%%%%%%%%%%
% \subsection{Discussion on Reachability Assumption: Assumption \ref{assump2}}\label{sec: D.1}
% As discussed in \Cref{sec: feature learning}, intuitively, if some state action pairs can be hardly visited by any policy, the output of \Cref{Algorithm: RAFFLE} $\hP(\cdot|s,a)$ can not approximate true $\Ps(\cdot|s,a)$ well over these state action pairs, so the input distribution naturally requires restrictions. On the other hand, it is easy for input distributions $\{q_h\}_{h=1}^{H}$ to satisfy Assumption \ref{assump2} if each state action pair under true MDPs can be explored well by some policies. In fact if we assume following reachability assumption, Assumption \ref{assump2} holds with for any $\{q_h\}_{h=1}^{H}$ with finite bound $C_B$. Such reachability type assumption is common in relevant literature ~\citep{modi2021model,NEURIPS2020_e894d787}.
% \begin{assumption}[Reachability]\label{assumption: reachability}
% 	For the true transition kernel $P^{\star}$, there exists a policy $\pi^0$ such that $\min_{s\in\Sc} \Pb^{\pi^0}_h(s)\geq \eta_{\mathrm{min}}$, where $\Pb^{\pi^0}_h(\cdot): \Sc \rightarrow \Rb$ is the density function over $\Sc$ using policy $\pi^0$ to roll into state $s$ at timestep $h$.  
% \end{assumption}
% % \begin{assumption}\label{assumption: finite measurement}
% 	% 	Assume the state space $\Sc$ is compact and has finite measure  $1/\upsilon$. Hence, the uniform distribution on $\Sc$ has the density function $f(s)=\upsilon$.
% 	% \end{assumption}
% Under Assumption \ref{assumption: reachability}, for any bounded input distributions $\{q_h\}_{h=1}^{H}$ with bound $C_B$, Assumption \ref{assumption: reachability} holds with $C_{\mathrm{min}}=\frac{C_B}{\eta_{\mathrm{min}}}$. 



\subsection{Supporting Lemmas}
We first show that $Q^{\pi}_{\hP,h, r}$ can approximate $Q^{\pi}_{\Ps,h, r}$ well over distribution $\{q_h\}_{h \in [H]}$ by following two lemmas.
\begin{lemma}\label{lemma: Upstream Q-function bound}
	Given any $\delta\in(0,1)$. Let $\hP$ be the output of \Cref{Algorithm: RAFFLE}, for any policy $\pi$ and rewards $r$, with probability at least $1-\delta$, we have
	\begin{align*}
		\Eb_{(s_h,a_h) \sim (\hP, {\pi})}&\left[\left|Q^{\pi}_{P^{\star},h, r}(s_h,a_h) - Q^{\pi}_{\hP,h, r}(s_h,a_h)\right|\right]
		{\leq} \epsilon\\
		\Eb_{(s_h,a_h) \sim (P^{\star}, {\pi})}&\left[\left|Q^{\pi}_{P^{\star},h, r}(s_h,a_h) - Q^{\pi}_{\hP,h, r}(s_h,a_h)\right|\right]
		{\leq} \epsilon.
	\end{align*}
\end{lemma}
\begin{proof}
	Define $\hat{f}_h(s_h,a_h)=\left\|\hP_h(\cdot|s_h,a_h)-\Ps(\cdot|s_h,a_h)\right\|_{TV}$ and $\hat{f}$ is a collection of all $\hat{f}_h$, i.e. $\left\{\hat{f}_h\right\}_{h\in [H]}$. 		\begin{align}
		&\Eb_{(s_h,a_h) \sim (\hP, {\pi})}\left[\left|Q^{\pi}_{P^{\star},h, r}(s_h,a_h) - Q^{\pi}_{\hP,h, r}(s_h,a_h)\right|\right]\nonumber\\
		&\quad \overset{(\romannumeral1)}{\leq} \Eb_{(s_h,a_h) \sim (\hP, {\pi})}\left[ \hat{Q}_{h,\hP,\hat{f}}^{\pi}(s_h,a_h)\right]\nonumber\\
		& \quad \overset{(\romannumeral2)}{\leq} \hat{V}^{\pi}_{\hP,\hat{f}}\nonumber\\
		& \quad \overset{\RM{3}}{\leq} \epsilon/2 \label{ineq: FL1-inq1},
	\end{align}
	where $\RM{1}$ follows from \Cref{eqn:lowrank:hatQ-Q<hatQ}, $\RM{2}$ follows from the definition of $\hat{V}$ and $\hat{Q}$, and $\RM{3}$ follows from the proof of \Cref{thm1: reward-free sample complexity}.
	
	Then for the second inequality, 
	\begin{align*}
		&\mathop{\Eb}_{(s_h, a_h) \sim (\Ps,\pi)}\left[\left|Q^{\pi}_{P^{\star},h, r}(s_h,a_h) - Q^{\pi}_{\hP,h, r}(s_h,a_h)\right|\right]\\
		& \quad \leq  \mathop{\Eb}_{(s_h, a_h) \sim (\hP,\pi)}\left[\left|Q^{\pi}_{P^{\star},h, r}(s_h,a_h) - Q^{\pi}_{\hP,h, r}(s_h,a_h)\right|\right]\\
		& \quad \scriptstyle +\left| \mathop{\Eb}_{(s_h,a_h) \sim (\Ps,\pi)}\left[\left|Q^{\pi}_{P^{\star},h, r}(s_h,a_h) - Q^{\pi}_{\hP,h, r}(s_h,a_h)\right|\right]-\mathop{\Eb}_{(s_h,a_h) \sim (\hP,\pi)}\left[\left|Q^{\pi}_{P^{\star},h, r}(s_h,a_h) - Q^{\pi}_{\hP,h, r}(s_h,a_h)\right|\right]\right|\\
		& \quad \overset{(\romannumeral1)}{\leq} \epsilon/2 + V_{\hP,\hat{f}}^{\pi}\\
		& \quad \overset{(\romannumeral2)}{\leq} \epsilon,
	\end{align*}
	where the first term in $(\romannumeral1)$ is due to \Cref{ineq: FL1-inq1} and \Cref{ineq:Step_1_Bound}, the second term in $(\romannumeral1)$ is due to Step 1 in \Cref{prop:step1_appendix} and $(\romannumeral2)$ follows from the proof of \Cref{thm1: reward-free sample complexity}.
\end{proof} 


\begin{lemma}\label{lemma: Target Q function guarantee}
	Given any $\delta,\epsilon\in(0,1)$ and the output of \Cref{Algorithm: RAFFLE}, under Assumption \ref{assumption: reachability}, for any policy $\pi$ and rewards $r$, let the input distributions $\{q_h\}_{h=1}^{H}$ are bounded with constant $C_B$. Denote $C_{\mathrm{min}}=\frac{C_B}{\eta_{\mathrm{min}}}$. Then with probability at least $1-\delta$, for each $h \in [H]$,  $\Eb_{(s_h,a_h)\sim q_h}\left[\left|Q^\pi_{P^{\star},h, r}(s_h,a_h) - Q^{\pi}_{\hP,h, r}(s_h,a_h)\right|\right]\leq \epsilon C_\mathrm{min}$.
\end{lemma}
\begin{proof}
First, together with Assumption \ref{assumption: reachability}, for any $(s,a) \in \Sc\times\Ac$, we have $q_h(s,a) \leq C_\mathrm{min}\Pb^{\pi^0}_h(s,a)$, then
	\begin{align*}
		& \Eb_{(s_h,a_h)\sim q_h}\left[\left|Q^\pi_{P^{\star},h, r}(s_h,a_h) - Q^{\pi}_{\hP,h, r}(s_h,a_h)\right|\right]\\
		&\quad=\int q_h(s_h,a_h)\left|Q^\pi_{P^{\star},h, r}(s_h,a_h) - Q^{\pi}_{\hP,h, r}(s_h,a_h)\right|ds_hda_h\\
		&\quad\overset{(\romannumeral1)}{=}\int C_\mathrm{min}{\Pb^{\pi^0}_h(s_h)}\left|Q^\pi_{P^{\star},h, r}(s_h,a_h) - Q^{\pi}_{\hP,h, r}(s_h,a_h)\right|ds_hda_h\\
		&\quad =C_\mathrm{min}\mathop{\Eb}_{(s_h,a_h) \sim (P^{\star}, {\pi^0})\atop  }\left[\left|Q^\pi_{P^{\star},h, r}(s_h,a_h) - Q^{\pi}_{\hP^,h, r}(s_h,a_h)\right|\right]\\
		&\quad \overset{(\romannumeral2)}{\leq} \epsilon C_\mathrm{min},
	\end{align*}
	where $(\romannumeral1)$ follows Assumption \ref{assumption: reachability} and $(\romannumeral2)$ follows \Cref{lemma: Upstream Q-function bound}.
\end{proof}
The lemma above shows that for any reward $r$, $Q^{\pi}_{\hP,h, r}(s_h,a_h)$ can be the target of $Q^{\pi}_{P^{\star},h, r}(s_h,a_h)$ when $(s_h,a_h)$ are chosen from given distribution $q_h$.

% In \Cref{alg: feature}, special rewards and policy are designed delicately to achieve several goals. First, $Q^{\pi}_{P^{\star},h, r}(s_h,a_h)$ can be linear combination of $\Pshi(s_h,a_h)$ for any $(s_h,a_h)$, i.e. there exists a linear coefficient $w_h^{\star} \in \Rb^d$ such that $Q^{\pi}_{P^{\star},h, r}(s_h,a_h)=\left\langle\Pshi(s_h,a_h),w_h^{\star}\right\rangle$. \cy{Second, the linear coefficient $w_h^{\star}$ is smooth among its $d$ coordinates, i.e. each factor such that each direction of feature $\Pshi(s_h,a_h)$ can be explored well. We next explained how the rewards are designed. still thinking}

% For each $\ph$ fixed, We set reward function $r^\ph(s_h,a_h)$ accordingly.
% \begin{align*}
	% r^{\ph}_{h}(s_h,a_h)&=0,h \neq \ph+1,\\
	% r^{\ph}_h(s_h,a_h)&=\frac{1}{2}+\min\left\{\frac{1}{2},\frac{c_{\smu}}{4\sqrt{d}},\norm{\hmu_h(s_h)}_2\right\}:= r^{\ph}_h(s_h),h = \ph+1.
	% \end{align*}
% Then prove $w_\ph^{\star}$

We then show that for any $h$, when reward $r$ is set to be zero at step $h$, $Q^{\pi}_{\Ps,h, r}$ has a linear structure w.r.t $\sphi_h$.
\begin{lemma}
	For any $h \in [H]$, policy $\pi$ and given $(s_h,a_h) \in \Sc \times \Ac$, given any $r$ such that $r$ is set to be zero at step $h$, i.e. $r_h=0$, then $Q^{\pi}_{P^{\star},h,r}(s_h,a_h)$ is linear with respect to $\sphi(s_h,a_h)$, i.e. there exist a $w_h^\star$ such that $Q^{\pi}_{P^{\star},h,r}(s_h,a_h)=\left\langle \Pshi_h(s_h,a_h),w_h^{\star}\right\rangle$. 
\end{lemma}
\begin{proof}
	\begin{align*}
		Q^{\pi}_{P^{\star},h,r}(s_h,a_h)&=r_h(s_h,a_h)+\Eb_{s_{h+1}\sim \Ps(\cdot|s_h,a_h)}\left[V^{\pi}_{\Ps,h+1,r}(s_{h+1})\big|s_h,a_h\right]\\
		&=\int P^{\star}(s_{h+1}|s_h,a_h)V^{\pi}_{\Ps,h+1,r}(s_{h+1})ds_{h+1}\\
		&= \left\langle \Pshi_h(s_h,a_h),\int\mu^{\star}_{h}(s_{h+1})V^{\pi}_{\Ps,h+1,r}(s_{h+1})ds_{h+1}\right\rangle\\
		&=\left\langle \Pshi_h(s_h,a_h),w_h^{\star}\right\rangle,
	\end{align*}
	where $w_h^{\star}=\int\mu^{\star}_{h}(s_{h+1})V^{\pi}_{\Ps,h+1,r}(s_{h+1})ds_{h+1}$.
	
	% \textbf{Assumptions to make the minimum of elements of $w_\ph^{\star}$ big enough:}\\
	% A oracle of reward: input: $\theta_d \in \Rb^{d}$ and output $\left\langle w_h^{\star},\theta_d\right\rangle$.\\
	% Assume $\left|\int\mu^{\star}_{h}(s_{h})[i]ds_{h}\right| \geq c_{\mu^{\star}}$ for each $h \in [H]$ and each coordinate $i \in [d]$: $|w_h^{\star}[i]| \geq \frac{c_{\smu}}{2}-\frac{c_{\smu}}{4\sqrt{d}}\times\sqrt{d}=\frac{c_{\smu}}{4}$.
	
	% Because $r^{\ph}_{\ph+1}(s_{\ph+1},a_{\ph+1}) \geq \frac{1}{2}$, we have 
	% \begin{align*}
		%     \frac{1}{2} &\leq \Eb_{\Ps}\left[r^{\ph}_{\ph+1}(s_{\ph+1},a_{\ph+1})\big|s_\ph,a_\ph\right]\\
		%     & = \left\langle \Pshi_\ph(s_\ph,a_\ph),w_\ph^{\star}\right\rangle\\
		%     & \overset{\RM{1}}{\leq} \norm{\Pshi_\ph(s_\ph,a_\ph)}_2 \norm{w_\ph^{\star}}_2\\
		%     & \overset{\RM{2}}{\leq} \norm{w_\ph^{\star}}_2,
		% \end{align*}
	% where $\RM{1}$ follows from Cauchy-Swarchtz inequality and $\RM{2}$ follows from \Cref{definition: Low_rank} that $\|\phi^\star_h(s,a)\|_2\leq 1$ for all $(s,a)$.
	
	% Furthermore, $\sigma_d(\left|{w_h^t}^{\star}\right|) \geq \frac{c_{\smu}}{2}$.
\end{proof}
\subsection{Proof of \Cref{Thm: individual representation}}	
\begin{proof}[Proof of \Cref{Thm: individual representation}]
	We allow $\phi$ and $Q^{\pi^t}_{\hP,h, r^{h,t}}$ to apply to all the samples in a dataset matrix simultaneously, i.e. $\phi_h(\Dc^{N_f,t}_h) = (\phi_h(s_h^{1,t}),\dots,\phi_h(s_h^{N_f,t}))^\top \in \Rb^{N_f \times d}$ and $Q^{\pi^t}_{\hP,h, r^{h,t}}(\Dc^{N_f,t}_h) = (Q^{\pi^t}_{\hP,h, r^{h,t}}(s_h^{1,t}),\dots,Q^{\pi^t}_{\hP,h, r^{h,t}}(s_h^{N_f,t}))^\top \in \Rb^{N_f}$.
	After we estimating $\tphi_h$ and then taking it as known, from the property of linear regression in \Cref{ineq: alg-linear regression}, for any reward function $r^{h,t}$, any policy $\pi^t$ we got 
	\begin{align*}
		&\tw_h^t=(\tphi_h(\Dc^{N_f,t}_h)^\top\tphi_h(\Dc^{N_f,t}_h))^\dagger\tphi_h(\Dc^{N_f,t}_h)^\top Q^{\pi}_{\hP,h, r^{h,t}}(\Dc^{N_f,t}_h)\\
		&\tphi_h(\Dc^{N_f,t}_h)\tw_h^t=P_{\tphi_h(\Dc^{N_f,t}_h)}Q^{\pi^t}_{\hP,h, r^{h,t}}(\Dc^{N_f,t}_h),
	\end{align*}
	where $P_{\tphi_h(\Dc^{N_f,t}_h)}=\tphi_h(\Dc^{N_f,t}_h)(\tphi_h(\Dc^{N_f,t}_h)^\top\tphi_h(\Dc^{N_f,t}_h))^\dagger\tphi_h(\Dc^{N_f,t}_h)^\top$, represents the projection operator to the column spaces of $\tphi_h(\Dc^{N_f,t}_h)$. Then 
	\begin{align}
		&\sum_{t \in [T]}\norm{P_{\tphi_h(\Dc^{N_f,t}_h)}^\perp Q^{\pi^t}_{\hP,h, r^{h,t}}(\Dc^{N_f,t}_h)}^2\nonumber\\
		& = \sum_{t \in [T]} \norm{\tphi_h(\Dc^{N_f,t}_h)\tw_h^t-Q^{\pi^t}_{\hP,h, r^{h,t}}(\Dc^{N_f,t}_h)}^2\nonumber\\
		&\overset{\RM{1}}{\leq} \sum_{t \in [T]} \norm{\Pshi_h(\Dc^{N_f,t}_h){w_h^t}^{\star}-Q^{\pi^t}_{\hP,h, r^{h,t}}(\Dc^{N_f,t}_h)}^2\nonumber\\
		&=  \sum_{t \in [T]}\sum_{n=1}^{N_f}\left(Q^{\pi^t}_{P^{\star},h, r^{h,t}}(s_h^{n,t},a_h^{n,t})-Q^{\pi^t}_{\hP,h, r^{h,t}}(s_h^{n,t},a_h^{n,t})\right)^2\nonumber\\
		&\overset{(\romannumeral2)}{\leq} N_f \sum_{t \in [T]}\Eb_{(s_h,a_h)\sim q_h}\left[\left(Q^{\pi^t}_{P^{\star},h, r^{h,t}}(s_h^{n,t},a_h^{n,t})-Q^{\pi^t}_{\hP,h, r^{h,t}}(s_h^{n,t},a_h^{n,t})\right)^2\right]+ \sqrt{\frac{TN_f\log{\frac{2}{\delta}}}{2}}\nonumber\\
		&\overset{(\romannumeral3)}{\leq} N_f \sum_{t \in [T]} \Eb_{(s_h,a_h)\sim q_h}\left[\left|Q^{\pi^t}_{P^{\star},h, r^{h,t}}(s_h^{n,t},a_h^{n,t})-Q^{\pi^t}_{\hP,h, r^{h,t}}(s_h^{n,t},a_h^{n,t})\right|\right]+\sqrt{\frac{TN_f\log{\frac{2}{\delta}}}{2}}\nonumber\\
		&\overset{(\romannumeral4)}{\leq} {\epsilon C_{\mathrm{min}}N_f}T+\sqrt{\frac{TN_f\log{\frac{2}{\delta}}}{2}} \label{ineq: FL5-ineq1},
	\end{align}
	where $\RM{1}$ follows from minimality of $\{\tphi_h^t\}_{t \in [T]}$ and $\{\tw_h^t\}_{t \in[T]}$, $(\romannumeral2)$ follows Hoeffding's inequality, $(\romannumeral3)$ follows that $\left|Q^{\pi^t}_{P^{\star},h, r^{h,t}}(s_h^{n,t},a_h^{n,t})-Q^{\pi^t}_{\hP,h, r^{h,t}}(s_h^{n,t},a_h^{n,t})\right|\leq 1$ and $(\romannumeral4)$ follows \Cref{lemma: Target Q function guarantee}.
	
	As a result:
	\begin{align*} &\sum_{t\in[T]}\norm{P_{\tphi_h(\Dc^{N_f,t}_h)}^\perp \Pshi_h(\Dc^{N_f,t}_h){w_h^t}^{\star}}^2\\
		&\quad=\sum_{t\in[T]}\norm{P_{\tphi_h(\Dc^{N_f,t}_h)}^\perp Q^{\pi^t}_{P^{\star},h, r^{h,t}}(\Dc^{N_f,t}_h)}^2\\
		&\quad \leq \sum_{t\in[T]}\left\{ \norm{P_{\tphi_h(\Dc^{N_f,t}_h)}^\perp Q^{\pi^t}_{\hP,h, r^{h,t}}(\Dc^{N_f,t}_h)}^2+\norm{P_{\tphi_h(\Dc^{N_f,t}_h)}^\perp \left(Q^{\pi^t}_{P^{\star},h, r^{h,t}}(\Dc^{N_f,t}_h)-Q^{\pi}_{\hP,h, r^{h,t}}(\Dc^{N_f,t}_h)\right)}^2\right\}\\
		&\quad \overset{(\romannumeral1)}{\leq} \sum_{t\in[T]}{\epsilon C_{\mathrm{min}}N_f}T+\sqrt{\frac{TN_f\log{\frac{2}{\delta}}}{2}}+\sum_{t \in [T]}\sigma_1^2(P_{\tphi_h(\Dc^{N_f,t}_h)}^\perp)\|Q^{\pi^t}_{P^{\star},h, r^{h,t}}(\Dc^{N_f,t}_h))-Q^{\pi^t}_{\hP,h, r^{h,t}}(\Dc^{N_f,t}_h)\|^2\\
		&\quad \overset{(\romannumeral2)}{\leq}  {2\epsilon C_{\mathrm{min}}N_f}T+\sqrt{2TN_f\log{\frac{2}{\delta}}},
	\end{align*}
	where $\RM{1}$ follows from $\norm{Av}_2 \leq \sigma_1(A)\norm{v}_2$ and $\RM{2}$ follows from that $\sigma_1(P_{\tphi_h(\Dc^{N_f,t}_h)}^\perp)\leq 1$ and the process to derive \Cref{ineq: FL5-ineq1}.
	
	We finally use the technique in \cite{DBLP:conf/iclr/DuHKLL21} to derive the super population guarantee. 
	% Define $W^{(\star,T)}=(w^{(\star,1)}_h,\ldots,w^{(\star,T)}_h) \in \Rb^{d \times T}$ and $\sigma_{\min\left\{d,T\right\}}(W^{(\star,T)}_h)^2$ to be the smallest singular value of matrix $\sigma^2_{\min\left\{d,T\right\}}(W^{(\star,T)}_h)$.
	\begin{align*}
		&{2\epsilon C_{\mathrm{min}}N_f}T+\sqrt{2TN_f\log{\frac{2}{\delta}}}\\
		&\quad\geq \sum_{t \in [T]}\norm{P_{\tphi_h(\Dc^{N_f,t}_h)}^\perp \Pshi_h(\Dc^{N_f,t}_h){w^t_h}^{\star}}^2_F\\
		&\quad=\left\|\left(I-\tphi_h(\Dc^{N_f,t}_h)\left(\tphi_h(\Dc^{N_f,t}_h)^\top\tphi_h(\Dc^{N_f,t}_h)\right)^\dagger\tphi_h(\Dc^{N_f,t}_h)^\top\right)\Pshi_h(\Dc^{N_f,t}_h){w^t_h}^{\star}\right\|_F^2\\
		%  	&=\left\|\left(I-\hphi_h(\Dc^{(N_f,t)}_h)(\hphi_h(\Dc^{(N_f,t)}_h)^\top\hphi_h(\Dc^{(N_f,t)}_h))^\dagger\hphi_h(\Dc^{(N_f,t)}_h)^\top\right)\Pshi_h(\Dc^{(N_f,t)}_h)W^{(\star,T)}_h\right\|_F^2\\
		&\quad=\sum_{t\in[T]}({w_h^t}^{\star})^\top\Pshi_h(\Dc^{N_f,t}_h)^\top	\left(I-\tphi_h(\Dc^{N_f,t}_h)(\tphi_h(\Dc^{N_f,t}_h)^\top\tphi_h(\Dc^{N_f,t}_h))^\dagger\tphi_h(\Dc^{N_f,t}_h)^\top\right)\Pshi_h(\Dc^{N_f,t}_h){w_h^t}^{\star}\\
		&\quad=\sum_{t\in[T]}N_f({w_h^t}^{\star})^\top D_{\Dc^{N_f,t}_h}(\Pshi_{h},\tphi_{h}){w_h^t}^{\star}\\
		&\quad\overset{(\romannumeral1)}{\geq} 0.9 \sum_{t\in[T]}N_f({w_h^t}^{\star})^\top D_{q_h}(\Pshi_{h},\tphi_{h}){w_h^t}^{\star}\\
		&\quad= 0.9N_f\sum_{t\in[T]}\left\| D_{q_h}(\Pshi_{h},\tphi_{h})^{1/2}{w_h^t}^{\star}\right\|^2\\
		&\quad\geq 0.9N_f\left\| D_{q_h}(\Pshi_{h},\tphi_{h})^{1/2}W^\star_h\right\|_F^2\\
		&\quad\overset{\RM{2}}{\geq} 0.9N_f\left\| D_{q_h}(\Pshi_{h},\tphi_{h})^{1/2}\right\|_F^2\sigma_d^2( W^\star_h)\\
		& \quad\geq 0.9N_f\left\| D_{q_h}(\Pshi_{h},\tphi_{h})^{1/2}\right\|_F^2\frac{C_D T}{d},
	\end{align*}
	where $\RM{1}$ follows from lemma B.1 in \cite{DBLP:conf/iclr/DuHKLL21} and $N_f$ can be large enough, and 
	$\RM{2}$ follows from that $\norm{AB}_{F}^2 \geq \norm{A}_{F}^2\sigma_{\mathrm{min}}^2(B)$, where $\sigma_{\mathrm{min}}(B)$ denote the smallest singular value of matrix $B$.  
	Then 
	\begin{align*}
		\left\| D_{q_h}(\Pshi_{h},\tphi_{h})^{1/2}\right\|_F^2 \leq
		\frac{2\epsilon d C_\mathrm{min}}{0.9C_D}+\frac{d}{0.9C_D}\sqrt{\frac{2\log{\frac{2}{\delta}}}{TN_f}}.
	\end{align*} 
\end{proof}




\section{More Discussion on Related Work}
In this section, we first summarize the directly comparable work in \Cref{Table: Comparison}. 
\begin{table*}
{\caption{Comparison among provably efficient RL algorithms under low-rank MDPs.}} 
\label{Table: Comparison}
\tabcolsep=2pt
\vskip 0.15in
\begin{center}
	\begin{small}
		\begin{threeparttable}
			\begin{sc}
				\begin{tabular}{lllr}
				\toprule
				Methods & Setting & Sample Complexity   \\
							\midrule
				FLAMBE~\citep{NEURIPS2020_e894d787}    & Low-rank MDP & $\tilde{O}(\frac{H^{22}K^9d^7}{\epsilon^{10}})$\\
				MOFFLE~\citep{modi2021model}    & Low-nonnegative-rank & $\tilde{O}(\frac{H^5d^3_{LV}K^5}{\epsilon^2\eta})$\\
				HOMER~\citep{misra2020kinematic} & Block MDP & $\tilde{O}(d^8H^4K^4(\frac{1}{\epsilon^2}+\frac{1}{\eta^3}))$ \\
				RAFFLE (Ours)    & Low-rank MDP& $\tilde{O}(\frac{H^3d^2K(d^2+K)}{\epsilon^2})$  \\
				\bottomrule
				\end{tabular}
				\end{sc}
				\begin{tablenotes}
				\item[1] We do not include reward-known RL under low-rank MDPs in this table and only focus reward-free RL. The detailed discussion of reward-known RL under low-rank MDPs can be found in \Cref{sec:relatedwork}.
				% \item[2] For reward-known RL, we do not include algorithms designed for low Bellman rank \citep{jiang2017contextual}, low witness rank \citep{sun2019model} and bilinear classes \citep{du2021bilinear} of MDPs. For reward-free RL, we do not include algorithms designed for low Bellman Eluder dimension \citep{DBLP:journals/corr/abs-2206-10770}. Although their MDPs can specialize to low-rank MDPs with sample complexities that may have sharper dependence on $d,K$ or $H$, these algorithms are not computationally efficient as noted in Section 2 in \citet{uehara2021representation}. \yl{Should we remove this paragraph?}
				\end{tablenotes}
				\end{threeparttable}
			\end{small}
		\end{center}
		\vskip -0.1in
	\end{table*}
  \subsection{Low-rank MDPs in extended RL settings}
 Many studies have been developed on various extended low-rank models. \citet{DBLP:journals/corr/abs-2205-13476,uehara2022provably} studied partially observable Markov decision process (POMDP) with latent low-rank structure. \citet{zhan2022pac} studied predictive state
representations model, and 
%captures POMDP and then 
applied their results to POMDP with latent low-rank structure. \citet{DBLP:journals/corr/abs-2206-05900,agarwal2022provable} studied benefits of multitask representation learning under low-rank MDPs. \citet{huang2022safe} proposed a general safe RL framework and instantiated it to low-rank MDPs. \citet{ren2022spectral} studied reward-known RL under low-rank MDPs and proposed a spectral method to replace the computation oracle. We note that even given those further developments, our results on lower bound and representation learning are still completely new, and our algorithm design and result on sample complexity are so far still the best-known result for standard reward-free low-rank MDPs.

\subsection{{Discussion on optimistic MLE-based approach for different settings}}
There is some concurrent work also using an optimistic MLE-based approach for different settings (POMDP)~\citep{liu2022optimistic,chen2022partially}. We elaborate the key differences between our paper and \cite{liu2022optimistic,chen2022partially} as follows. 
\begin{itemize}
    \item The two POMDP papers mentioned above study reward-known setting, whereas our focus here is on the reward-free setting. 
    \item Although optimistic MLE-based approach is used in both settings, the design of exploration policy in the two settings are different. Our algorithm identifies which estimated model is used for exploration policy design and then calculate the exploration policy based on bonus terms designed for the value function. However, the POMDP papers construct a confidence set about the true model and solve an optimization problem within this generic model set. Such an oracle may not be easy to realize computationally.
    \item Due to the hardness of POMDP, MLE approach only guarantees that the estimated distribution of the trajectories is close to the true one. In contrast, in low-rank MDP we study here, we show that the estimation error for the transition probabilities is controlled at each time step.
\end{itemize}



\section{Auxiliary Lemmas}\label{appx:auxiliary}
Recall $f_h^{(n)}(s,a)=\|\hP_h^{(n)}(\cdot|s,a) - P^\star_h(\cdot|s,a)\|_{TV}$ represents the estimation error in the $n$-th iteration at step $h$, given state $s$ and action $a$, in terms of the total variation distance. By using Theorem 21 in \citet{NEURIPS2020_e894d787}, we are able to guarantee that under all exploration policies, the estimation error can be bounded with high probability. 
\begin{lemma}\label{lemma:MLE}
	{\rm(MLE guarantee).} Given $\delta\in(0,1)$, we have the following inequality holds for any $n,h\geq 2$ with probability at least $1-\delta/2$:
	\begin{align*}
		\sum_{\tau=0}^{n-1} \mathop{\Eb}_{s_{h-1}\sim (P^\star,\pi_\tau)\atop {(a_{h-1},a_h)\sim \Uc(\Ac)\atop 
				s_h\sim P^\star(\cdot|s_{h-1},a_{h-1})
		}}\left[f_h^{n}(s_h,a_h)^2\right]\leq n\zeta_n, \quad\mbox{ where } \zeta_n : = \frac{\log\left(2|\Phi||\Psi|nH/\delta\right)}{n} .
	\end{align*}
	In addition, for $h=1$,
	\begin{align*}
		\sum_{\tau=0}^{n-1} \mathop{\Eb}_{a_1 \sim \Uc(\Ac)}\left[f_1^{n}(s_1,a_1)^2\right]\leq n\zeta_n. %, \zeta_n : = \frac{\log\left(2|\Phi||\Psi|nH/\delta\right)}{n} .
	\end{align*}
	
\end{lemma}

Divide both sides of the result of lemma~\Cref{lemma:MLE} by $n$, and define $\Pi_n = \Uc(\pi_1,\ldots,\pi_{n-1})$, we have the following corollary which will be intensively used in the analysis.


\begin{corollary}\label{coro:MLE}
	Given $\delta\in(0,1)$, the following inequality holds for any $n,h\geq 2$ with probability at least $1-\delta/2$:
	\[ \mathop{\Eb}_{s_{h-1}\sim (P^\star,\Pi_n)\atop {a_{h-1},a_h\sim \Uc(\Ac)
			\atop s_h\sim P^\star(\cdot|s_{h-1},a_{h-1})}}\left[f_h^{n}(s_h,a_h)^2\right]\leq \zeta_n.\]
	In addition, for $h=1$,
	\begin{align*}
		\mathop{\Eb}_{a_1 \sim \Uc(\Ac)}\left[f_1^{n}(s_1,a_1)^2\right]\leq \zeta_n.
	\end{align*}
\end{corollary}






The following lemma \citep{dann2017unifying} will be useful to measure the difference between two value functions under two MDPs and reward functions. We define $P_h V_{h+1}(s_h,a_h) = \Eb_{s\sim P_h(\cdot|s_h,a_h)}\left[V(s)\right]$ for shorthand.
\begin{lemma} \label{lemma: Simulation}{\rm(Simulation lemma).}
	Suppose $P_1$ and $P_2$ are two MDPs and $r_1$, $r_2$ are the corresponding reward functions. Given a policy $\pi$, we have,  
	\begin{align*}
		V_{h,P_1,r_1}^{\pi}(s_h) &- V_{h,P_2,r_2}^{\pi}(s_h)\\
		&= \sum_{\ph=h}^H  \mathop{\Eb}_{s_\ph \sim (P_2,\pi) \atop a_\ph \sim \pi}\left[r_1(s_\ph,a_\ph) - r_2(s_\ph,a_\ph) + (P_{1,\ph} - P_{2,\ph})V^{\pi}_{\ph+1,P_1,r}(s_\ph,a_\ph)|s_h\right]\\
		& = \sum_{\ph=h}^H  \mathop{\Eb}_{s_\ph \sim (P_1,\pi) \atop a_\ph \sim \pi}\left[r_1(s_\ph,a_\ph) - r_2(s_\ph,a_\ph) + (P_{1,\ph} - P_{2,\ph})V^{\pi}_{\ph+1,P_2,r}(s_\ph,a_\ph)|s_h\right].
	\end{align*}
\end{lemma}
The following lemma is a standard inequality in regret analysis for linear models in reinforcement learning (see Lemma G.2 in \citet{NEURIPS2020_e894d787} and Lemma 10 in \citet{uehara2021representation}).
\begin{lemma} \label{lemma: Elliptical_potential} {\rm (Elliptical potential lemma).}
	Consider a sequence of $d \times d$ positive semidefinite matrices $X_1, \dots, X_N$ with ${\rm tr}(X_n) \leq 1$ for all $n \in [N]$. Define $M_0=\lambda_0 I$ and $M_n=M_{n-1}+X_n$. Then
	\begin{equation*}
		\sum_{n=1}^N {\rm tr}\left(X_nM_{n-1}^{-1}\right) \leq 2\log \det(M_N)- 2\log \det(M_0) \leq 2d\log\left(1+\frac{N}{d\lambda_0}\right).
	\end{equation*}
\end{lemma}
If we choose any subset of the set $\{X_nM_{n-1}^{-1}\}_{n=1}^{N}$, we can still get a sublinear summation as follows.
% \begin{lemma}
	% \label{corollary: revised elliptical potential lemma} {\rm (Revised Elliptical potential lemma).}  Consider the setup of lemma \Cref{lemma: Elliptical_potential}, then 
	% $\forall \Nc  = \{n_1, \dots, n_{|\Nc|}\} \subset [N]$,
	% \begin{equation*}
		%     \sum_{n \in \Nc} {\rm tr}(X_nM_{n-1}^{-1})  \leq 2d\log\left(1+\frac{|\Nc|}{d\lambda_0}\right).
		% \end{equation*}
	% \end{lemma} 
% \begin{proof}
	% Define $\tilde{M_0}=\lambda_0 I,\tilde{M_i}=\tilde{M}_{i-1}+X_{n_i}$ for $i \geq 1$.
	% \begin{align*}
		%     \sum_{n \in \Nc} {\rm tr}(X_nM_{n-1}^{-1}) 
		%     &= \sum_{i=1}^{|\Nc|}{\rm tr}(X_{n_i}M_{n_{i-1}}^{-1}) \\
		%     &\leq 
		%     \sum_{i=1}^{|\Nc|}{\rm tr}(X_{n_i}\tilde{M}_{i-1}^{-1})\leq 2\log \det\left(\sum_{n \in \Nc}X_n+M_0\right)- 2\log \det(M_0) \leq 2d\log\left(1+\frac{|\Nc|}{d\lambda_0}\right).
		% \end{align*}
	% \end{proof}

\end{document}
