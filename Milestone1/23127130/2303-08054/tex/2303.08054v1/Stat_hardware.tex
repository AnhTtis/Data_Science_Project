\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}

%user defined packages
\usepackage{xcolor}
\usepackage{soul}
\usepackage{algorithmic}
\usepackage[ruled]{algorithm2e} 

\usepackage{booktabs}
\usepackage{multirow}

\begin{document}
\title{Statistical Hardware Design With Multi-model Active Learning}
\author{Alireza Ghaffari, Masoud Asgharian and Yvon Savaria.
% \thanks{Manuscript created October 2020; This work was developed by the IEEE Publication Technology Department. This work is distributed under the \LaTeX \ Project Public License (LPPL) ( http://www.latex-project.org/ ) version 1.3. A copy of the LPPL, version 1.3, is included in the base \LaTeX \ documentation of all distributions of \LaTeX \ released 2003/12/01 or later. The opinions expressed here are entirely that of the author. No warranty is expressed or implied. User assumes all risk.}}
\thanks{Alireza Ghaffari and Yvon Savaria are with the department of Electrical Engineering Polytechnique Montreal, and Masoud Asgharian is with the department of Mathematics and Statistics, McGill Univesrity. \textit{Corresponding Author: seyed-alireza.ghaffari@polymtl.ca}.}}

% \markboth{Journal of \LaTeX\ Class Files,~Vol.~18, No.~9, September~2020}%
\markboth{}
{How to Use the IEEEtran \LaTeX \ Templates}

\maketitle

\begin{abstract}
With the rising complexity of numerous novel applications that serve our modern society comes the strong need to design efficient computing platforms. Designing efficient hardware is, however, a complex multi-objective problem that deals with multiple parameters and their interactions. Given that there are a large number of parameters and objectives involved in hardware design, synthesizing all possible combinations is not a feasible method to find the optimal solution. One promising approach to tackle this problem is statistical modeling of a desired hardware performance. Here, we propose a model-based active learning approach to solve this problem. Our proposed method uses Bayesian models to characterize various aspects of hardware performance. We also use transfer learning and Gaussian regression bootstrapping techniques in conjunction with active learning to create more accurate models. Our proposed statistical modeling method provides hardware models that are sufficiently accurate to perform design space exploration as well as performance prediction simultaneously. We use our proposed method to perform design space exploration and performance prediction for various hardware setups, such as micro-architecture design and OpenCL kernels for FPGA targets. Our experiments show that the number of samples required to create performance models significantly reduces while maintaining the predictive power of our proposed statistical models. For instance, in our performance prediction setting, the proposed method needs 65\% fewer samples to create the model, and in the design space exploration setting, our proposed method can find the best parameter  settings by exploring less than 50 samples.
\end{abstract}

\begin{IEEEkeywords}
Statistical modeling, Design space exploration, Hardware performance prediction, Transfer learning, Active learning, Bayesian models, Gaussian regression bootstrap.
\end{IEEEkeywords}


\section{Introduction}
\IEEEPARstart{E}{fficient} hardware design is essential for various industrial and scientific applications. With the emergence of new applications such as machine learning, the Internet of Things (IoT), and 5G wireless communication, there is a strong need to design computing platforms that provide the required computational power. Designing efficient hardware is challenging and requires considering various hardware and software parameters and their interactions. In most cases, designing efficient digital hardware is a multi-objective optimization problem where the designers try to optimize multiple competing objectives, such as energy consumption and throughput. Moreover, performing a brute-force search to find the optimum design is not feasible, because synthesizing complex hardware designs is very time-consuming. In addition, designers must test a large number of parameter combinations to find the optimum solution.

Another prominent challenge of efficient hardware design is that the objective function is not smooth, and most parameters are discrete. For example, the memory delay, the number of floating-point arithmetic units, and clock frequencies are parameters that need to be tuned for throughput. Since these parameters are discrete, the objective function is not differentiable. Thus using gradient-based methods such as Stochastic Gradient Descent (SGD) to find the optimum parameter set is not possible.

For years, meta-heuristic methods, such as Genetic Algorithm, were used to perform design space exploration \cite{deb2002fast,palesi2002multi}. Although these meta-heuristic algorithms can find the optimum setup in various settings, they cannot provide a model that predicts the performance. Moreover, the heuristic methods cannot provide an abstract model to study the interactions and inter-dependencies between the parameters and their effect on the desired performance metrics. Researchers proposed Bayesian models for design space exploration tasks to solve this issue. Bayesian optimization is not only capable of finding the optimum parameter setup but is also able to reveal the interactions and inter-dependencies between the parameters of the design.  

Although research on Bayesian models has produced remarkable results for design space exploration as well as abstract modeling of parameter inter-dependencies, they incorporate two fundamental shortcomings. First, there is a need for a feedback mechanism from a real-world application to update the Bayesian model to improve accuracy. Second, Bayesian models are application specific, meaning that a model trained for a particular task cannot be used for another. Thus, there is a need to propose a \textit{transfer learning} strategy to use currently trained models to predict the performance of applications that were not previously exposed to the model. Using transfer learning is crucial in hardware design since it provides the ability to perform design space exploration for emerging applications using prior knowledge acquired from previous experiences and applications.

Here we propose a novel methodology that solves the two shortcomings mentioned above. Besides the Bayesian modeling approach, we suggest using Bayesian active learning (\cite{kandasamy2015bayesian,brochu2010tutorial}) to increase our proposed statistical model accuracy. Bayesian active learning is an iterative machine learning algorithm that actively selects some informative samples to construct the Bayesian model of the objective function. The iterative nature of the Bayesian active learning method helps the Bayesian model to gradually learn the underlying structure of the data. The ultimate goal of Bayesian active learning is to provide a surrogate function that closely resembles the behavior of the system with the minimum number of samples.
In our proposed methodology, once a Bayesian model is created, it can be updated using a feedback mechanism that is provided by a real-world application. For instance, a Bayesian surrogate model can be updated using a processor or computing cluster performance metrics reflecting its performance for different computation loads. Furthermore, we propose using the Bayesian transfer learning \cite{skolidis2012transfer}, where the information of one domain can be transferred to another domain to improve the accuracy of the statistical model of the target task. Transfer learning is particularly useful in hardware design in scenarios where the target application is novel and there are limited information and data on the performance of the hardware in that application. In such cases, the information of previously trained hardware models can be leveraged to infer the performance of the new application.
%that could be obtained from previously observed tasks or algorithms can be used, to create a statistical model for new tasks.

To further reduce the number of samples needed to make hardware performance prediction, we suggest using \textit{Gaussian regression bootstrapping}. Once we have a Bayesian model of the hardware, it is possible to re-sample the model in order to acquire new composite samples to perform other types of statistical analysis. In this paper, we used Gaussian regression bootstrapping to generate new data to perform various regression analyses of the hardware model and showed that the results that are based on bootstrapped samples could closely resemble the analyses that are done on the real data.

To summarize, this paper makes the following contributions:
\begin{itemize}
    \item We propose a \textit{model-based active learning} framework that generates statistical models of different aspects of hardware performance.
    \item We propose \textit{Bayesian transfer learning} to effectively perform hardware design space exploration using prior hardware performance information.
    \item We use a \textit{Gaussian regression bootstrapping} method to effectively estimate the behavior of the hardware from the limited samples acquired from real-world applications.
    %\item To the best of our knowledge, this is the first time that model-based active learning is used in conjunction with Bayesian transfer learning to provide accurate statistical hardware models for design-space exploration. Furthermore, this is the first time non-parametric Bayesian bootstrapping is used to better estimate the behavior of specific hardware.
\end{itemize}
To the best of our knowledge, this is the first time that model-based active learning has been used in conjunction with Bayesian transfer learning to provide accurate statistical hardware models for design-space exploration. Furthermore, this is the first time that Bayesian regression bootstrapping is used to better estimate the behavior of specific hardware.

The rest of the paper is structured as follows. Section \ref{sec:related-work} reviews related works in the hardware design space exploration and performance prediction. We introduce our methodology in Section \ref{sec:method}. In Section \ref{sec:dse-vs-reg}, we distinguish two important settings, notably design space exploration and performance modeling. In Section \ref{sec:results}, we show that our proposed methodology is effective for both design space exploration and performance modeling.



\section{Related works}\label{sec:related-work}

Exploring various design parameters in hardware design is a ubiquitous problem. The most obvious but inefficient solution is brute-force analysis or exhaustive search. The complexity of brute-force analysis increases exponentially with the number of parameters, making this method infeasible in practical problems. For years, researchers have used meta-heuristic methods such as Genetic Algorithms (GAs) \cite{palesi2002multi} to perform design space exploration and obtain the Pareto optimal solutions. Although meta-heuristic methods are effective in finding optimum solutions, they do not provide a mathematical model reflecting inter-dependencies between parameters and their effects on the objectives.

Another valuable design space exploration approach is Bayesian Optimization (BO) \cite{brochu2010tutorial}. Bayesian Optimization is especially interesting for design space exploration applications since (i) it provides a Bayesian surrogate model that helps to understand the inter-dependencies of the parameters and (ii) it is an iterative method that incorporates information from previously observed samples in the surrogate model. BO has been widely used in hardware design space exploration. For instance, in \cite{mehrabi2020bayesian}, BO was found to have superior performance for design space exploration of \texttt{C/C++} High-Level Synthesis (HLS) compared to traditional search methods. Likewise, BO was used in \cite{reagen2017case} to optimize hardware accelerators for deep neural networks. The authors reported that the BO method outperforms traditional design space methods in all metrics of interest, such as accuracy and energy consumption. Furthermore, in \cite{lo2016model}, BO is deployed to tune HLS directives to achieve minimum latency.

Inspired by traditional meta-heuristic approaches, researchers have combined BO and Grey Wolf Optimization (GWO) \cite{mirjalili2014grey} to perform design space exploration for HLS of OpenCL kernels \cite{ghaffari2021efficient}. The hybrid GWO-BO method was shown to outperform both BO and traditional search methods.

Other iterative methods such as Reinforcement Learning (RL) \cite{kaelbling1996reinforcement} are also effective for design space exploration applications. An RL agent tunes hardware parameters through some iterations to maximize the reward function related to metrics of interest.  For example, in \cite{de2020automated}, the RL method is used to explore parameters that optimize the latency of deep neural networks on the ARM-Cortex-A CPUs. Likewise, in \cite{ghaffari2020cnn2gate}, a time-limited reinforcement learning \cite{mnih2016asynchronous} is used to perform design space exploration for deeply pipelined OpenCL kernels of the convolutional neural networks. In \cite{hosny2020drills}, an Advantage Actor Critic (A2C) \cite{konda2003a2c} design space exploration is proposed to minimize area subject to a timing constraint in logic synthesis. Additionally, in  \cite{kao2020confuciux}, the authors proposed an RL-based design space exploration method and used the performance predicted by their cost model to compute the reward values. Moreover, they augmented their RL algorithm with a genetic algorithm to improve the design space exploration results.


Furthermore, using other statistical methods for hardware analysis and design is commonplace in the hardware community. For instance, in \cite{lee2006accurate}, authors studied accurate regression modeling of micro-architecture energy consumption and throughput. Moreover, in \cite{liu2013learning}, Random Forest was used for HLS design space exploration. Similarly in \cite{singh2019napel}, Random Forest regression is used for predicting instruction per cycles for near memory computing applications. In \cite{beltrame2010decision} Markov decision process was found effective for the design space exploration of multi-processor platforms. In \cite{nia2022rethinking}, parametric bootstrapping was used to perform stochastic ordering deep learning models that run efficiently on different hardware, while the inference efficiency and training efficiency are chosen as the metrics of interest.

Our proposed method is different from the previous Bayesian methods (\cite{mehrabi2020bayesian, reagen2017case, lo2016model}) in the way that it uses Active Learning in conjunction with Bayesian Optimization to create more accurate Bayesian models. We present results showing that our method is more effective than the hybrid GWO-BO method proposed in \cite{ghaffari2021efficient}. Furthermore, we use Bayesian Transfer Learning (TL) \cite{skolidis2012transfer} and show that the performance of the parameter search is significantly improved by incorporating information from other correlated hardware design tasks. Our proposed method differs from \cite{nia2022rethinking} since it can produce Bayesian models capable of being used for \textit{Gaussian regression bootstrapping} \cite{GPRbootstrap}. Also, after performing Gaussian regression bootstrapping, it is possible to produce composite data from our Bayesian surrogate model and perform statistical regression analysis similar to (\cite{lee2006accurate, beltrame2010decision}) without requiring real-world data samples. 


\section{Methodology}\label{sec:method}
Statistical modeling of hardware and related applications such as design space exploration and performance estimation can be classified as Black Box Optimization (BBO) \cite{audet2017derivative} problems. This is because the objective function and its behavior, such as smoothness, continuity, and convexity, are unknown. Thus, gradient-based methods such as Gradient Descent (GD) and Stochastic Gradient Descent (SGD) are not the best choice for these applications. On the other hand, derivative-free methods such as meta-heuristic methods and Bayesian Optimization are among the best choices for these applications.

Here we propose a \textit{Model-Based Active Learning} framework for statistical hardware  performance modeling. We further analyze this method in the context of design space exploration and performance prediction (regression analysis). In our proposed method, we use Bayesian Optimization with Gaussian Processes. We also use Bayesian Transfer Learning and Bayesian regression bootstrapping methods to further improve such models in practical applications. Fig.~\ref{fig:main-arch}  shows an overview of our proposed framework. In this figure, Bayesian models are de facto Gaussian processes that are updated iteratively using active learning. These Bayesian models can be used in design space exploration or for performance prediction. The details of our mathematical modeling are elaborated on in the following sections.


\begin{figure*}[!t]
\centering
\includegraphics[width=0.7\linewidth]{figs/al.drawio.pdf}
\caption{Statistical modeling of hardware using \textit{Multi-model Bayesian Active Learning}. The framework also incorporates an optional\textit{ Bayesian Transfer Learning} setup to improve the accuracy of the Bayesian models.}
\label{fig:main-arch} 
\end{figure*} 

\subsection{Multi-Model Active Learning}\label{sec:methodAL}


We use active learning to update the Gaussian process models. Our proposed framework uses multiple Bayesian models for various objectives. For instance, power consumption and throughput are each associated with their corresponding models. The algorithm starts from a random set of parameters for all models. It then queries the synthesis tool and creates initial Gaussian processes. Next, the algorithm evaluates the Gaussian processes and gathers each model parameters candidate to be sent to the synthesis tool in the next query. This process continues iteratively until a particular stopping criterion is met. Furthermore, when used with active learning, the algorithm chooses the next set of parameters using the Gaussian models that are also affected by other pre-trained Gaussian processes, as explained in Section~\ref{sec:methodTL}. Also, note that our proposed algorithm considers the direction of optimization of each objective function. For instance, designers usually aim to minimize power consumption while maximizing the throughput. Therefore, our proposed algorithm chooses the sets of parameters that respect the optimization constraints of each objective function. Having multiple models associated with multiple objectives enables us to study the Pareto efficiency of the solutions. Section~\ref{sec:multi-objective-pareto} reports experimental results that validate the proposed method. Algorithm~\ref{alg:al_alg} provides a pseudo-code for our proposed algorithm. 

{
\centering
\begin{algorithm}[!b]{
\textbf{Step 1.} Randomly choose the first set of parameters $\mathbf{x}_{1:N} = \{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N\}$ for query. \\

\While{\textbf{stop criterion} is not satisfied} 
{
    \textbf{Step 2.} Get the query Objectives $O(\mathbf{x}_{1:N})$ . \\
    \textbf{Step 3.} For all $\{\mathcal{GP}_1, \mathcal{GP}_2, \dots \mathcal{GP}_M\}$, calculate new covariance matrix using eq~\eqref{eq:cov_matrix} \\
    \textbf{Step 4.} Update all $\{\mathcal{GP}_1, \mathcal{GP}_2, \dots \mathcal{GP}_M\}$ using eq~\eqref{eq:gp}\\
    \textbf{Step 5.} If transfer learning is needed: Update $\{\mathcal{GP}_1, \mathcal{GP}_2, \dots \mathcal{GP}_M\}$ mean vector and covariance matrix using eq~\eqref{eq:tfl}\\
    \textbf{Step 6.} Evaluate all  $\{\mathcal{GP}_1, \mathcal{GP}_2, \dots \mathcal{GP}_M\}$  and update the set $\mathbf{x}_{1:N}$
}

\textbf{return}  $\{\mathcal{GP}_1, \mathcal{GP}_2, \dots \mathcal{GP}_M\}$ 
\caption{Multi-Model Bayesian Active Learning.}
\label{alg:al_alg}
}\end{algorithm}
}

\subsection{Bayesian Optimization with Gaussian Processes}\label{sec:methodBO}

Bayesian Optimization \cite{brochu2010tutorial} is a powerful tool to find the extrema of an objective function that is hard to evaluate or does not have a close mathematical form.  Statistical modeling of hardware implementations deals with both problems simultaneously. It is well-known that synthesizing hardware models takes a very long time, and generally, there is no closed-form expression of the power consumption and latency of the resulting hardware models.

In Bayesian Optimization, the goal is to find a surrogate function $f_s$ that closely resembles the behavior of the desired objective function $O$. Note that in Bayesian Optimization, $f_s$  is a stochastic process.

Let us assume $\mathbf{x}_i$ is the \textit{i}$^{th}$ sample, and $O(\mathbf{x}_i)$ is the evaluation of the objective function for that sample. Using Bayes theorem, for a collection of $N$ samples such as $C_{1:N} = \{\mathbf{x}_{1:N}, O(\mathbf{x}_{1:N})\}$, we have

\begin{equation}
\label{eq:bayes}
P(O|C_{1:N}) \propto P(C_{1:N} | O) P(O),
\end{equation}
where $P(O|C_{1:N})$ is the posterior distribution. The posterior distribution $P$ incorporates probabilistic information about the objective function $O$. Also, the commonplace assumption in Bayesian optimization is that the objective function is continuous. This assumption does not hold in our experimental setup.  However, our goal is to perform Bayesian optimization on a discrete objective function by fitting a continuous posterior distribution $P$ that passes through the discrete points of the objective function $O$, see \cite{luong2019bayesian}.

We note that the surrogate function $f_s$ is a stochastic process. Let us further assume that $f_s$ is a Gaussian Process that provides a mean and a standard deviation at each point. i.e. 
\begin{equation}
\label{eq:gp}
f_s(\mathbf{x}) = \mathcal{GP}(\mathit{\mu}(\mathbf{x}), \mathbf{K} =\mathit{k}(\mathbf{x},\mathbf{x'}))).
\end{equation}
This assumption implies that the sampling process of $O(\mathbf{x}_i)$ is noisy with Gaussian distribution $\mathcal{N}(0,\sigma^2)$. 
%Furthermore, we can define the close form of the surrogate function $f_s$ as

In eq~\eqref{eq:gp},  $\mathcal{GP}$ denotes a Gaussian process with a mean function $\mathit{\mu}$ and a covariance operator $\mathbf{K}$.
%that contains joint Gaussian distribution between the sample populations $\mathbf{x}$ and $\mathbf{x}'$. 
Here,  we use the squared exponential covariance kernel $\mathit{k}$ defined below to construct the covariance matrix $\mathbf{K} $
 \begin{equation}
\label{eq:cov_matrix}
\mathbf{K}_{ij}=\mathit{k}({x}_i, {x}_j) = \mathrm{exp}{(-\frac{|{x}_i - {x}_j|^2}{2})}
\end{equation}

There are other types of covariance kernels such as \textit{Matern kernels} \cite{genton2001classes} that are suitable for non-smooth data. Although studying the effect of various covariance kernels is not the main focus of this work, we show in Appendix B that our proposed method for hardware performance modelling is robust in terms of changing the covariance kernel to Matern.


\subsection{Transfer Learning}\label{sec:methodTL}
We propose using \textit{Transfer Learning} to transfer prior information from a \textit{source} surrogate function to a \textit{target} function. Let us denote source domain $\mathcal{D}_\mathrm{source}$ that contains design parameters of our hardware design problem $\mathbf{x}=\{x_1,x_2 \dots x_p\}$. The surrogate function of source task $\mathcal{T}_\mathrm{source}$ is also denoted as $f_\mathrm{source}$. The goal is to improve the learning process of a target task $\mathcal{T}_\mathrm{source}$ with the surrogate function $f_\mathrm{target}$ and domain $\mathcal{D}_\mathrm{target}$. Note that our application considers \textit{Inductive Transfer Learning}, where $\mathcal{T}_\mathrm{source} \neq \mathcal{T}_\mathrm{target}$ and $\mathcal{D}_\mathrm{source} = \mathcal{D}_\mathrm{target}$. This means that in our experimental setup, although the tasks and their surrogate functions are different, the design parameters $\mathbf{x}=\{x_1,x_2 \dots x_p\}$ of our hardware design problems are the same.

In our proposed method, we consider \textit{Inductive Gaussian Process Transfer Learning} where we allow the mean 
function and covariance operator of the source surrogate function to affect the target surrogate function. To this end, we define
 \begin{equation}
\label{eq:tfl}
    \begin{cases}
        \mathit{\mu}^\mathrm{TL}(\mathbf{x}) = \mathit{\mu}_\mathrm{target}(\mathbf{x}) + \lambda_1 \mathit{\mu}_\mathrm{source}(\mathbf{x}) \\
        \mathbf{K}^\mathrm{TL} = \mathbf{K}^\mathrm{target} + \lambda_2\mathbf{K}^\mathrm{source}
    \end{cases}
\end{equation}
where %in eq \eqref{eq:tfl}, 
$\lambda_1$,$\lambda_2$ are hyper-parameters that regularize the magnitude of the source information transferred to the target.


We emphasize that we only use transfer learning for design-space exploration experiments where finding the position of the objective function optimum point is more important than its value, see Section \ref{sec:dse-vs-reg}. This method is beneficial when the $\mathcal{T}_\mathrm{source}$ and $\mathcal{T}_\mathrm{target}$ are correlated. We will show in Section \ref{sec:dse-tl} that this assumption is true for our intended application.


\subsection{Gaussian Regression Bootstrapping}\label{sec:methodBOOTSTRAP}
In statistical hardware modeling problems, the aim is to predict the behavior of an objective function $O$. Examples of such objective functions are power consumption and latency. In practice, we have access to limited samples of this objective function because synthesizing a hardware design is a very time-consuming process. Thus, we can use the Bayesian surrogate function $f_s$ to evaluate the behavior of the objective function $O$ for new sets of parameters. This method is commonly known as \textit{Gaussian Regression Bootstrapping} \cite{GPRbootstrap}. This is specifically beneficial  because, unlike $O$, sampling $f_s$ is inexpensive. Note that $f_s$ is a Gaussian process incorporating mean and variance information. Here, we can use the mean function $\mathit{\mu}$ of $f_s$ for resampling the objective function $O$ 

\begin{equation}
\label{eq:gp-bootstrap}
O(\mathbf{x}) = \mathit{\mu}(\mathbf{x})+\varepsilon (\mathbf{x})
\end{equation}
which provides an inexpensive evaluation of  $O$ for hardware parameter setups that are not actually synthesized. Note that $\epsilon$ denotes the error of a  $\mathcal{GP}$'s with a mean function equal to zero and a covariance operator equal to $\mathbf{K}$. 
%mean vector when compared to the actual value of $O$.

\begin{figure}[b]
\centering
\includegraphics[width=0.95\linewidth]{figs/cmp-dse-reg.drawio.pdf}\\
~~~(a)\hspace{4cm}(b)
\caption{(a) Performance modeling (regression) setting where the accuracy of the model is evaluated with the Mean Square Error (MSE) and (b) Design space exploration setting where the algorithm tries to find the parameters that correspond to the design with maximum performance. }
\label{fig:compare-dse-reg} 
\end{figure} 

\section{Design Space Exploration vs Performance Modeling Settings}\label{sec:dse-vs-reg}
Hardware design space exploration and hardware performance modeling are two related but distinct tasks in hardware design.
Our proposed statistical modeling methodology provides a unified framework for both applications that have been crucial for hardware designers historically. 
In the \textit{Performance Modeling} application, the goal is to accurately \textit{predict} the hardware performance (or value of the objective function $O$) for a specific hardware design parameter setting.  Thus, the performance modeling application is essentially a regression problem and the accuracy of the model can be measured with \textit{Mean Squared Error (MSE)} between the surrogate function $f_s$ and the actual objective function $O$ over $N$ parameter sets $\mathbf{x}_{1:N} = \{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N\}$

\begin{equation}
\label{eq:mse}
\text{MSE} 
 = \frac{1}{N}\sum_{i=1}^N\left( O(\mathbf{x}_i) - f_s(\mathbf{x}_i) \right)^2 
 = \frac{1}{N}\sum_{i=1}^N\varepsilon_i^2.
\end{equation}
Fig.~\ref{fig:compare-dse-reg} (a) demonstrates the performance modeling setup. The black curve is the actual objective function, while the dotted red curve is the regression model. Note that in our experiments, we report $\sqrt{\text{MSE}}/\mu_{f_s(\mathbf{x}_i)}$ to normalize MSE with respect to the mean of the surrogate function $\mu_{f_s(\mathbf{x}_i)}$.

On the other hand, in the \textit{Design Space Exploration (DSE)} application, the goal is to find the hardware design parameter setting that maximizes the objective function. Moreover, in the case of multi-objective optimization, DSE aims to find the dominant or Pareto optimal solutions. Fig.~\ref{fig:compare-dse-reg} (b) shows the DSE setup. The model is accurate if 

\begin{equation}
\label{eq:dse-setup}
{{\arg\max(f_s) = \arg\max}}(O), 
\end{equation}

meaning that the surrogate function $f_s$ can accurately predict the parameters setting that maximizes the objective function. Also, note that here \textit{argmax} is used for the objectives that need to be maximized, such as throughput. Likewise, the goal for the objective functions that are to be minimized, e.g. power consumption, is the same, except that $\arg\max$ should be replaced by $\arg\min$. %However, without loss of generality, \textit{argmin} can be used for objectives that are needed to be minimized e.g. power consumption.



\begin{table*}[!b]
\centering
\renewcommand\thetable{1}
\caption{Design space exploration using Bayesian Active Learning and Comparison with previous works.}
\label{table:compare}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{m{95pt}>{\centering}m{60pt}>{\centering}m{60pt}>{\centering\arraybackslash}m{60pt}>{\centering\arraybackslash}m{60pt}>{\centering\arraybackslash}m{60pt}>{\centering\arraybackslash}m{60pt}}
\hline
\textbf{Benchmark Function} & \textbf{\small{Random Search \cite{bergstra2012random} Average  Latency(ms)}} &{\textbf{\small{LASSO model (ms) \cite{gautier2016spector}}}}& {\textbf{\small{hybrid GWO-BO \cite{ghaffari2021efficient} Average Latency (ms)}}}& {\textbf{\small{This work Latency (ms)}}} \\
\hline
\hline
\textbf{Breath-First Search Dense} & 4.372  &  4.3825 & {4.329} & \textbf{4.2804} \\
% \hline
\textbf{Breath-First Search Sparse} & 13.371 & 13.2857 & {12.5151} & \textbf{12.5151} \\
% \hline
\textbf{Discrete Cosine Transform } & 2.904  & 2.9522 & {2.5891} & \textbf{2.1916} \\
% \hline
\textbf{FIR} & 1.8$\times 10^{-3}$  &  2.4$\times 10^{-3}$  & {1.6${\times 10^{-3}}$} & \textbf{7.11$\mathbf{\times 10^{-4}}$}\\
% \hline
\textbf{Histogram} & 1.6632 & 1.5715 & {1.5652} &  \textbf{1.5481}\\
% \hline
\textbf{Mergesort} & 1.9955 & 2.0441 & {1.9386} &  \textbf{1.9269} \\
% \hline
\textbf{Matrix  Multiplication} & 40.9993 & 40.6412 & {36.8913} &  \textbf{34.198} \\
% \hline
\textbf{Normal Estimation} & 6.5449 & 6.4763 & {6.4315} &  \textbf{6.2779}\\
% \hline
\textbf{Sobel Filter} & 1.8448 & 1.9331 & {1.7434} & \textbf{1.719} \\
% \hline
\textbf{Sparse Matrix Vector Multiplication} & 0.1729 & 0.1798 & {0.1671} &  \textbf{0.1661} \\
\hline

\end{tabular}

\end{table*} 


\section{Results and discussions}\label{sec:results}
Here, we discuss the performance of our proposed method and compare it with previous works. We show that our proposed method not only outperforms other algorithms  for design space exploration applications, but it also provides a viable performance prediction in the regression setting.
\subsection{Design Space Exploration (DSE) Setting}
We study the performance of our algorithm for DSE setting for two applications. In Section~\ref{sec:multi-objective-pareto}, we study the multi-objective optimization of OpenCL kernels on FPGA targets, and in  Section~\ref{sec:dse-tl} we show the effect of transfer learning on design space exploration of the CPU micro-architecture design.


\subsubsection{Case Study I: Multi-Objective Optimization of OpenCL kernels on FPGA targets}\label{sec:multi-objective-pareto}

As studied previously in (\cite{gautier2016spector,ghaffari2021efficient}), design space exploration of the OpenCL kernels on FPGA targets is a challenging task that involves optimizing multiple competing performance objectives such as throughput and area utilization  using various design parameters. 
These design parameters include, but are not limited to, the number of work-groups, the number of compute units, SIMD size, unrolling, etc. Furthermore, such kernels must be optimized for both area and throughput. Having two objectives, i.e., logic utilization and throughput, makes the problem a multi-objective problem where the \textit{Pareto Frontier} must be found by the proposed algorithm. To do so, we create a  Bayesian active learning multi-model for throughput and area according to Algorithm~\ref{alg:al_alg}. 

The idea behind using our proposed method is to iteratively select new design parameters and configure the application based on them in order to find the optimal set of parameters that achieve the required performance. As shown in Algorithm~\ref{alg:al_alg}, the Bayesian models of area utilization and latency are trained on a set of initial design parameters and their corresponding performance metrics using Gaussian processes. Then, these models are used to model the performance of other design configurations and also as a guide for the selection of new parameters to be evaluated by the Bayesian model. By using active learning, the optimization process proceeds to update the Bayesian model based on the new samples evaluated by the model and it continues until satisfactory results are achieved for the performance metrics. For this experiment, we retain the five candidate solutions defined by a set of design parameters that give the best score according to the surrogate function. More generically, there could be more than one objective function, in which case each objective function would point to five best designs. Finally, the associated \textit{parameter sets} producing the best performance in the surrogate models are used to update the Bayesian models in each active learning round. 

%For convenience, we call the parameter sets producing the best scores according to each objective function of interest \textit{top-5} solutions.

\begin{figure}[h]
\vspace{-2.5cm}
\centering
\includegraphics[width=1\linewidth]{figs/mm-pareto.pdf}
\vspace{-2.7cm}
\caption{Objective space of the Matrix Multiplication OpenCL kernel is demonstrated by the blue dots. The Pareto frontier estimated by our proposed multi-model active learning method is depicted with red triangles.}
\label{fig:mm-pareto} 
\end{figure} 


Fig. \ref{fig:mm-pareto} shows the objective space of the matrix multiplication OpenCL kernel\footnote{For a complete list of matrix multiplication HLS design parameters, refer to \textbf{Appendix A}.}. The red triangles show the set of dominant solutions, also known as Pareto frontier \cite{ngatchou2005pareto} that is estimated by our proposed method. The estimated Pareto frontier resembles the actual Pareto frontier in \cite{gautier2016spector}. 


Table \ref{table:compare} shows the performance of the proposed method when optimizing the OpenCL kernels for latency. Our proposed method outperforms all the previously reported methods to find the optimum parameters for achieving the best latency results in various OpenCL benchmarks.


\subsubsection{Case Study II: Design Space Exploration of Micro-Architectures using Transfer Learning}\label{sec:dse-tl}

Design space exploration of the micro-architectures are one of the most prominent problems of digital hardware design. Similar to other design space exploration problems, the number of possible designs grows exponentially with the number of parameters (See Appendix A for list of design parameters). As such, finding the optimal design parameters that meet the performance requirement is a challenging and time-consuming task that requires significant computational and synthesis resources. Thus, there is a need to find performance models with a minimum number of queries to the synthesis tool. In order to show that our proposed algorithm performs efficiently in this setup, similar to \cite{lee2006accurate}, we consider the SPEC2k \cite{Spec2k} benchmarks to evaluate our proposed methodology. More specifically, we use the ammp, applu, equake, gcc, gzip, mesa, twolf and jbb benchmarks.  The SPEC2k dataset exposes twelve parameters that can be tuned to optimize performance. These parameters are included but not limited to the cache size, the memory latency, the fixed-point and floating-point latency, etc. \cite{lee2006accurate}. Moreover, considering the number of parameters, the design  space for each benchmark is about \textit{one Billion} design points, which makes it impossible to do design space exploration using exhaustive search methods such as brute force.

Similar to Section \ref{sec:multi-objective-pareto}, we can consider the design space exploration of the micro-architectures as a multi-objective problem and illustrate the dominant solutions using a Pareto frontier. Fig.~\ref{fig:mesa-pareto} shows a selected group of samples for objective space for SPEC2k 3-D graphics benchmark (mesa). In this figure, the Pareto frontier shows the trade-off between the Billion Instructions per Second (BIPS) rating and the normalized power, confirming that a faster microprocessor needs to consume more power. Also note that the red dots in Fig.~\ref{fig:mesa-pareto} are determined by our proposed Bayesian multi-model active learning of power consumption and instructions per second (BIPS) rating created using 600 samples. As discussed in Section \ref{sec:regression}, our proposed method considerably reduces the number of samples needed to perform regression analysis compared to the method proposed by \cite{lee2006accurate}. 
%comparing to what proposed in \cite{lee2006accurate}.

\begin{figure}[t]
\vspace{-2.5cm}
\centering
\includegraphics[width=0.95\linewidth]{figs/mesa.pdf}
\vspace{-2.7cm}
\caption{Selected samples of the objective space for the SPEC2k 3-D graphics benchmark (mesa) demonstrated by the blue dots. The estimated Pareto frontier obtained with our proposed multi-model active learning method is depicted with red triangles.}
\label{fig:mesa-pareto} 
\end{figure} 



Now we show that Bayesian Transfer Learning, as introduced in Section \ref{sec:methodTL}, can further reduce the number of samples required for design space exploration. We use transfer learning as a technique to leverage knowledge gained from training Bayesian models on a source task to improve the modeling process of a target task. Let us consider the SPEC2k Java Business Benchmark (jbb) as the source task $\mathcal{T}_{source}$ and show that the information of its Gaussian surrogate function can be used to reduce the number of samples needed to perform design space exploration for other benchmark tasks. Our approach for transfer learning in this context is to use a Bayesian model that is trained using SPEC2k Java Business Benchmark (jbb) and use its Gaussian process information to affect the training of other target tasks as explained in Section~\ref{sec:methodTL}. By doing so, the number of design evaluations is reduced, which leads to faster and more efficient design space exploration.

Note that the performance results of a micro-architecture on various benchmark tasks are strongly correlated. This means that we expect a powerful microprocessor to perform better in all tasks. This correlation is a crucial characteristic of design space exploration that can be exploited by inductive transfer learning. Table \ref{table:corr} shows this phenomenon for our selected SPEC2k benchmark tasks with the SPEC2k jbb benchmark. Note that small \textit{pvalues} in Table \ref{table:corr} shows that the correlation results are statistically significant. In other words, it confirms the hypothesis that there is a strong correlation between the metric performance of the considered tasks.


\begin{table}[!t]
\centering
\renewcommand\thetable{2}
\caption{Correlation of the SPEC2k benchmarks with SPEC2k Java Business Benchmark (jbb).}
\label{table:corr}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lcc}
\hline
\textbf{Benchmark Task (BIPS)} & \textbf{Correlation ($\rho$) }& \textbf{pvalue} \\
\hline
\hline
\textbf{ammp} & 0.6395  &  3.25$\times 10^{-230}$  \\
\textbf{applu} & 0.7523  &  0  \\
\textbf{equake} & 0.9100  &  0  \\
\textbf{gcc} & 0.9397  &  0  \\
\textbf{gzip} &  0.8469  &  0  \\
\textbf{mesa} & 0.8720  &  0  \\
\textbf{twolf} & 0.8920  &  0  \\
\hline
\end{tabular}
\end{table} 

Table \ref{table:dse-spec2k} shows the experimental results for design space exploration using transfer learning for different target tasks where the source task is the SPEC2k jbb benchmark. The numbers reported in the table indicate a considerably faster convergence to the desired BIPS. 
%Table \ref{table:dse-spec2k} shows the experimental results where the design space exploration with transfer learning converges faster to the desired BIPS. Moreover, in this table, we consider transfer learning setup where the source task is SPEC2k jbb benchmark and the destination tasks are the benchmarks reported on the table. 
For this experiment, $\lambda_2=0$ and $\lambda_1$, in eq \eqref{eq:tfl}, are both set to $0.5$ at the beginning of the iterations and linearly decreased to zero through the 
course of iterations. Also note that each iteration is a query to the synthesis environment, each of which consists of five samples, to create the Gaussian processes. 
%Furthermore, each iteration consists of query five samples to the synthesis environment to create the Gaussian processes.


\begin{table}[!t]
\centering
\renewcommand\thetable{3}
\caption{Design space exploration of the SPEC2k benchmarks for the Billions of Instructions Per Seconds (BIPS) performance.}
\label{table:dse-spec2k}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lccc}
\hline
                         &  & Number of  Samples & Number of Samples \\
                         &  & without & with  \\
 \textbf{Benchmark Task} & BIPS & Transfer Learning & Transfer Learning  \\
\hline
\hline
\textbf{ammp} & 1.302 & 35  & 25   \\
\textbf{applu} & 1.060 & 115  & 100 \\
\textbf{equake} & 1.027& 15 & 15 \\
\textbf{gcc} & 1.009 & 20  & 15\\
\textbf{gzip} & 1.005 & 25  & 15  \\
\textbf{mesa} & 2.012 & 25 & 20 \\
\textbf{twolf} & 1.138 & 20 & 15 \\
% 
% \textbf{ammp} & 35  &  130264223 & 25 & 130453711  \\
% \textbf{applu} & 115  &  106009866 & 100 &  106022010 \\
% \textbf{equake} & 15  & 102708887  & 15 & 102708887 \\
% \textbf{gcc} & 15  &  100975250 &15& 100991754\\
% \textbf{gzip} &  25  &  100510901 & 15 & 100510901 \\
% \textbf{mesa} & 25  &  201250787 & 20 & 201250787\\
% \textbf{twolf} & 20  &  113826857  & 15 & 114428663\\
% 
% \textbf{ammp} & 25  &  129747648 & 25 & 130453711  \\
% \textbf{applu} & 100  &  105435414 & 100 &  106022010 \\
% \textbf{equake} & 15  & 102708887  & 15 & 102708887 \\
% \textbf{gcc} & 15  &  100975250 &15& 100991754\\
% \textbf{gzip} &  15  &  100499808 & 15 & 100510901 \\
% \textbf{mesa} & 20  &  199688359 & 20 & 201250787\\
% \textbf{twolf} & 15  &  113826857  & 15 & 114428663\\
% 
\hline
\end{tabular}
\end{table} 

\begin{table*}[!t]
\centering
\renewcommand\thetable{4}
\caption{Comparison of the Predictive power of regression models with or without using Gaussian regression bootstrapping on the SPECT2k AMMP benchmark.}
\label{table:regression-mse}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lcc}
\hline

                         &                             & Regression using \\
                         & Original                    &  Gaussian Bootstrapping\\
                         &          regression setting &  simulated data\\
                         &          $\sqrt{\text{MSE}}/\mu$ &  $\sqrt{\text{MSE}}/\mu$\\
\hline
\hline
\textbf{Linear Regression Instructions Per Second} & 0.056   & 0.061   \\
\textbf{Linear Regression Power Consumption (mW)} & 0.45 &  0.46   \\
\textbf{Random Forest Instructions Per Second} & 0.017  &  0.055  \\
\textbf{Random Forest Power Consumption (mW)} & 0.050  & 0.14   \\
% \textbf{Linear Regression Instructions Per Second} & $3.72\times 10^{13}$   &  $4.48\times 10^{13}$   \\
% \textbf{Linear Regression Power Consumption (mW)} & $2.61\times 10^{8}$  &  $2.76\times 10^{8}$   \\
% \textbf{Random Forest Instructions Per Second} & $3.65\times 10^{12}$  &  $3.69\times 10^{13}$  \\
% \textbf{Random Forest Power Consumption (mW)} & $3.36\times 10^{6}$   &  $2.88\times 10^{7}$   \\
\hline
\end{tabular}
\end{table*} 

\begin{figure*}[!t]
\vspace{1cm}
\centering
\includegraphics[width=0.4\linewidth]{figs/lasso.pdf}
\hspace{1.5cm}
\includegraphics[width=0.4\linewidth]{figs/lasso-BT.pdf}\\
\hspace{2cm}(a)\hspace{8cm} (b)\hspace{1cm}
% \vspace{-1.7cm}
\caption{ (a) Original LASSO model with a shrinkage factor $\lambda$ for the coefficients of the power consumption model produced using 4000 samples (b) LASSO shrinkage factor $\lambda$ effect on the coefficients of the power consumption model that is created using 700 samples and Gaussian regression bootstrapping. The comparison shows the extent to which Gaussian regression bootstrapping is efficient in producing simulated data samples to create statistical hardware performance prediction models. }
\label{fig:lasso} 
\end{figure*} 


\subsection{Gaussian Regression Bootstrapping and Performance Prediction Models}\label{sec:regression}

Bootstrap is a resampling technique in statistics that allows one to create simulated data from a given set of samples. In our proposed method, when the Bayesian models are created using Gaussian processes, it is possible to use such probabilistic methods to create simulated data that closely resemble the behavior of the hardware. Gaussian regression bootstrapping can be used in various contexts such as bioinformatics \cite{GPRbootstrap}. 

The Gaussian regression bootstrapping involves fitting Gaussian process regression to a set of performance data and using this model to resample simulated data to create new statistical models. This allows quantifying the uncertainty in the data and improves the accuracy of the statistical modeling for performance prediction models. The simulated data that is generated by Gaussian regression bootstrapping can be used in a variety of statistical analyses to predict the behavior of the hardware. For instance, we show that Gaussian regression bootstrapping can generate robust simulated data that can be efficiently used in various regression analyses such as linear regression, LASSO, and Random Forest regression.

Here we explore the effectiveness of our proposed active learning multi-model to predict the performance of hardware benchmarks. As mentioned previously in Section \ref{sec:dse-vs-reg}, the regression setting is substantially different from design space exploration and requires more samples from the actual hardware to provide acceptable models. For instance, in the case of SPEC2k benchmarks, the authors of \cite{lee2006accurate} show that 2000 samples are enough to create performance regression models. However, using our proposed methodology, it is possible to reduce this number to 700 samples using Gaussian regression bootstrapping which is 65\% sample reduction.


\subsubsection{Linear Regression}
Linear regression is the most basic tool for the statistical modeling of hardware. The underlying assumption, of course, is that the performance of the hardware under study is a linear function of the design parameters. Linear regression can be used to develop performance models of hardware based on tuning various design parameters such as cache size, floating point delay and etc.  Table \ref{table:regression-mse} shows the comparison of $\sqrt{\text{MSE}}/\mu$ for linear regression using 2000 samples suggested in \cite{lee2006accurate} and 700 samples using our proposed Gaussian regression bootstrapping method.

\subsubsection{Random Forest}

Random forest \cite{liaw2002classificationRandomF} is a tree-based regression method that is used previously for design space exploration of hardware \cite{liu2013learning}. Given the non-linear nature of hardware design, non-linear regression methods, such as Random Forest, might behave better in such scenarios. In random forest regression, multiple decision trees are created and each tree chooses a random subset of design parameters from the training data to fit a statistical model. The main advantage of Random Forest regression over linear regression is that it can model the complex and nonlinear relation between the parameters and the desired performance objective. Table \ref{table:regression-mse} reports that the performance of the random forest regression based on simulated data using  the Gaussian regression bootstrapping is close to the original Random Forest regression. This means that the random forest regression on the simulated data and actual data are producing similar results.

Note that the $\sqrt{\text{MSE}}/\mu$  values in Table  \ref{table:regression-mse} are reported for the case when the covariance kernel function is squared exponential. We show in \textbf{Appendix B} that using other types of covariance kernel (such as Matern covariance kernels) for this specific application has a negligible effect on the reported results.


\subsubsection{LASSO}

The Least Absolute Shrinkage and Selection Operator (LASSO) \cite{tibshirani1996regressionLASSO} is a  regularization and variable selection method that is used in regression analysis. 
% LASSO is commonly used for variable selection and dimensionality reduction in statistical analysis.  
LASSO is also used in \cite{gautier2016spector} for performance modeling of hardware. Furthermore, LASSO can be used in hardware performance prediction models to identify the most important design parameters by leveraging the variable selection property of the LASSO. The variable selection of the LASSO is particularly important for hardware performance prediction models when working with large datasets with many design parameters since the inclusion of irrelevant parameters in the performance model can lead to over-fitting the hardware performance model. Fig. \ref{fig:lasso}.a  shows that as the LASSO shrinkage factor $\lambda$ increases, the design parameter coefficients shrink to zero. The first coefficient that collapses to zero identifies the least significant parameter in the design space and also the last parameter that collapses to zero is the most important parameter in the design space. The top axis in this figure shows the number of variables that are not zero at the given value of $\lambda$. Fig. \ref{fig:lasso}.b depicts 
%under different shrinkage factor $\lambda$ of LASSO, 
the behavior of the design parameter coefficients for the regression model created from simulated data using Gaussian regression bootstrapping. Comparing Fig. \ref{fig:lasso}.a and Fig. \ref{fig:lasso}.b reveals that the behavior of the Bayesian model created using simulated data  and the original LASSO model are similar for different values of the LASSO shrinkage factor, $\lambda$. For instance, in both LASSO models, \textit{`Fixed point latency'} and \textit{`Number of reservation stations'} are the last design parameter coefficients that are shrunk to zero. Also, note a complete list of design parameters is reported in \textbf{Appendix A}. This shows that Gaussian regression bootstrapping is efficient in producing simulated data samples to create a reliable statistical model for hardware performance prediction.



% With the rising complexity of numerous novel applications that serve our modern society, there is a need to design efficient computing platforms that provide the computational power required for various applications. On the other hand, designing efficient hardware is a complex multi-objective problem that deals with multiple parameters and their interactions. Furthermore, synthesizing all possible combinations of parameters is not a feasible solution given a large number of parameters. One promising way to tackle this problem is statistical modeling of a desired hardware performance. Here, we propose a model-based active learning approach to solve this problem. Our proposed method uses Bayesian models to characterize various aspects of the hardware performance. We also use transfer learning and Gaussian regression bootstrapping techniques in conjunction with active learning to create more accurate models. Our proposed statistical modeling method can provide hardware models that are accurate to perform design space exploration as well as performance prediction simultaneously. We used our approach to perform design space exploration and performance prediction for various hardware setups such as micro-architecture design and FPGA OpenCL kernels. Our experiments show that the number of samples required to create performance models significantly reduces while maintaining the models' predictive power. For instance, in performance prediction mode, our proposed method needs 65\% fewer samples to create the model, and in the design space exploration setting, our proposed method can find the best parameters' settings by exploring less than 50 samples.

\section{Conclusion}
We proposed a multi-model Bayesian active learning framework to statistically model digital hardware performance. Our proposed model uses Gaussian processes to characterize the performance of hardware designs. We also use active learning to iteratively guide the sampling process in order to reduce the number of samples needed to create the statistical model. Furthermore, we used Bayesian transfer learning in order to incorporate  the prior design knowledge acquired in a source application to better model a target application with fewer data samples in the design space exploration task. We further propose using the Gaussian regression bootstrapping method to reduce the number of samples required for the performance prediction task. To show the effectiveness of our proposed method, we performed design space exploration and performance prediction for various hardware setups, such as micro-architecture design and OpenCL kernels. For OpenCL kernels on FPGA targets, our approach was able to identify the minimum latency in all benchmarks as well as predicting the correct Pareto frontier. Our experiments show that our approach can detect the optimal design parameters for processors micro-architecture with less than 50 samples in most cases. Our experiments also show that the number of samples required to create performance models significantly reduces, by 65\%, while maintaining the predictive power of the model. We have also performed commonplace regression analysis, such as linear regression, LASSO, and random forest regressions,  using simulated data generated by Gaussian regression bootstrapping and demonstrated that our proposed statistical hardware models closely follow the actual hardware behavior in various scenarios. 


\section{Acknowledgments}
\noindent The authors would like to thank Dr. Vahid Partovi Nia for his valuable comments to improve this paper.

% \section{The Design, Intent and \\ Limitations of the Templates}
% \noindent The templates are intended to {\bf{approximate the final look and page length of the articles/papers}}. Therefore, {\bf{they are NOT intended to be the final produced work that is displayed in print or on IEEEXplore\textsuperscript{\textregistered}}}. They will help to give the authors an approximation of the number of pages that will be in the final version. The structure of the \LaTeX files, as designed, enable easy conversion to XML for the composition systems used by the IEEE's outsource vendors. The XML files are used to produce the final print/IEEEXplore\textsuperscript{\textregistered} pdf and then converted to HTML for IEEEXplore\textsuperscript{\textregistered}. Have you looked at your article/paper in the HTML version?

% \section{\LaTeX \ Distributions: Where to Get Them}
% \noindent IEEE recommends using the distribution from the \TeX User Group at \url{http://www.tug.org}. You can join TUG and obtain a DVD distribution or download for free  from the links provided on their website: \url{http://www.tug.org/texlive/}. The DVD includes distributions for Windows, Mac OS X and Linux operating systems.
 
% \section{Where to get the IEEEtran Templates}
% \noindent The {\bf{IEEE Template Selector}} will always have the most up-to-date versions of the \LaTeX\ and MSWord templates. Please see: \url{https://template-selector.ieee.org/} and follow the steps to find the correct template for your intended publication. Many publications use the IEEETran LaTeX templates, however, some publications have their own special templates. Many of these are  based on IEEEtran, but may have special instructions that vary slightly from those in this document.

% \section{Where to get \LaTeX \ help - user groups}
% \noindent The following on-line groups are very helpful to beginning and experienced \LaTeX\ users. A search through their archives can provide many answers to common questions.
% \begin{list}{}{}
% \item{\url{http://www.latex-community.org/}} 
% \item{\url{https://tex.stackexchange.com/} }
% \end{list}

% \section{Document Class Options in IEEEtran}
% \noindent At the beginning of your \LaTeX\ file you will need to establish what type of publication style you intend to use. The following list shows appropriate documentclass options for each of the types covered by IEEEtran.

% \begin{list}{}{}
% \item{Regular Journal Article}
% \item{{\tt{$\backslash$documentclass[journal]{IEEEtran}}}}\\
% \item{{Conference Paper}}
% \item{{\tt{$\backslash$documentclass[conference]{IEEEtran}}}}\\
% \item{Computer Society Journal Article}
% \item{{\tt{$\backslash$documentclass[10pt,journal,compsoc]{IEEEtran}}}}\\
% \item{Computer Society Conference Paper}
% \item{{\tt{$\backslash$documentclass[conference,compsoc]{IEEEtran}}}}\\
% \item{{Communications Society Journal Article}}
% \item{{\tt{$\backslash$documentclass[journal,comsoc]{IEEEtran}}}}\\
% \item{{Brief, Correspondence or Technote}}
% \item{{\tt{$\backslash$documentclass[9pt,technote]{IEEEtran}}}}
% \end{list}

% There are other options available for each of these when submitting for peer review or other special requirements. IEEE recommends to compose your article in the base 2-column format to make sure all your equations, tables and graphics will fit the final 2-column format. Please refer to the document ``IEEEtran\_HOWTO.pdf'' for more information on settings for peer review submission if required by your EIC.

% \section{How to Create Common Front Matter}
% \noindent The following sections describe general coding for these common elements. Computer Society publications and Conferences may have their own special variations and will be noted below.
% \subsection{Paper Title}
% \noindent The title of your paper is coded as:

% \begin{verbatim}
% \title{The Title of Your Paper}
% \end{verbatim}

% \noindent Please try to avoid the use of math or chemical formulas in your title if possible.

% \subsection{Author Names and Affiliations}
% \noindent The author section should be coded as follows:
% \begin{verbatim}
% \author{Masahito Hayashi 
% \IEEEmembership{Fellow, IEEE}, Masaki Owari
% \thanks{M. Hayashi is with Graduate School 
% of Mathematics, Nagoya University, Nagoya, 
% Japan}
% \thanks{M. Owari is with the Faculty of 
% Informatics, Shizuoka University, 
% Hamamatsu, Shizuoka, Japan.}
% }
% \end{verbatim}
% Be sure to use the $\backslash$IEEEmembership command to identify IEEE membership status.
% Please see the ``IEEEtran\_HOWTO.pdf'' for specific information on coding authors for Conferences and Computer Society publications. Note that the closing curly brace for the author group comes at the end of the thanks group. This will prevent you from creating a blank first page.

% \subsection{Running Heads}
% \noindent The running heads are declared by using the $\backslash${\tt{markboth}} command. There are two arguments to this command: the first contains the journal name information and the second contains the author names and paper title.
% \begin{verbatim}
% \markboth{Journal of Quantum Electronics, 
% Vol. 1, No. 1, January 2021}
% {Author1, Author2, 
% \MakeLowercase{\textit{(et al.)}: 
% Paper Title}
% \end{verbatim}

% \subsection{Copyright Line}
% \noindent For Transactions and Journals papers, this is not necessary to use at the submission stage of your paper. The IEEE production process will add the appropriate copyright line. If you are writing a conference paper, please see the ``IEEEtran\_HOWTO.pdf'' for specific information on how to code "Publication ID Marks".

% \subsection{Abstracts}
% \noindent The abstract is the first element of a paper after the $\backslash${\tt{maketitle}} macro is invoked.  The coding is simply:
% \begin{verbatim}
% \begin{abstract}
% Text of your abstract.
% \end{abstract}
% \end{verbatim}
% Please try to avoid mathematical and chemical formulas in the abstract.

% \subsection{Index Terms}
% \noindent The index terms are used to help other researchers discover your paper. Each society may have it's own keyword set. Contact the EIC of your intended publication for this list.
% \begin{verbatim}
% \begin{IEEEkeywords}
% Broad band networks, quality of service
% \end{IEEEkeywords}
% \end{verbatim}
% \section{How to Create Common Body Elements}
% \noindent The following sections describe common body text elements and how to code them.

% \subsection{Initial Drop Cap Letter}
% \noindent The first text paragraph uses a ``drop cap'' followed by the first word in ALL CAPS. This is accomplished by using the $\backslash${\tt{IEEEPARstart}} command as follows:
% \begin{verbatim}
% \IEEEPARstart{T}{his} is the first paragraph 
% of your paper. . .
% \end{verbatim}

% \subsection{Sections and Subsections}
% \noindent Section headings use standard \LaTeX\ commands: $\backslash${\tt{section}}, $\backslash${\tt{subsection}} and $\backslash${\tt{subsubsection}}. Numbering is handled automatically for you and varies according to type of publication. It is common to not indent the first paragraph following a section head by using $\backslash${\tt{noindent}} as follows:
% \begin{verbatim}
% \section{Section Head}
% \noindent The text of your paragraph . . .
% \end{verbatim}

% \subsection{Citations to the Bibliography}
% \noindent The coding for the citations are made with the \LaTeX\ $\backslash${\tt{cite}} command. This will produce individual bracketed reference numbers in the IEEE style. At the top of your \LaTeX\ file you should include:
% \begin{verbatim}
% \usepackage{cite}
% \end{verbatim}
% For a single citation code as follows:
% \begin{verbatim}
% see \cite{ams}
% \end{verbatim}
% This will display as: see \cite{ams}\\

% For multiple citations code as follows:
% \begin{verbatim}
% \cite{ams,oxford,lacomp}
% \end{verbatim}

% This will display as \cite{ams,oxford,lacomp}

% \subsection{Figures}
% \noindent Figures are coded with the standard \LaTeX\ commands as follows:
% \begin{verbatim}
% \begin{figure}[!t]
% \centering
% \includegraphics[width=2.5in]{fig1}
% \caption{This is the caption for one fig.}
% \label{fig1}
% \end{figure}
% \end{verbatim}
% The [!t] argument enables floats to the top of the page to follow IEEE style. Make sure you include:
% \begin{verbatim}
% \usepackage{graphicx}
% \end{verbatim}
 
% \noindent at the top of your \LaTeX file with the other package declarations. 

% To cross-reference your figures in the text use the following code example:
% \begin{verbatim}
% See figure \ref{fig1} ...
% \end{verbatim}
% This will produce:\\
% See figure \ref{fig1} . . .

% \begin{figure}[!t]
% \centering
% \includegraphics[width=2.5in]{fig1}
% \caption{This is the caption for one fig.}
% \label{fig1}
% \end{figure}

% \subsection{Tables}
% \noindent Tables should be coded with the standard \LaTeX\ coding. The following example shows a simple table.


% \begin{verbatim}
% \begin{table}
% \begin{center}
% \caption{Filter design equations  ...}
% \label{tab1}
% \begin{tabular}{| c | c | c |}
% \hline
% Order & Arbitrary coefficients & 
% coefficients\\
% of filter & $e_m$ &   $b_{ij}$ \\
% \hline
% 1& $b_{ij}=\hat{e}.\hat{\beta_{ij}}$, 
% & $b_{00}=0$\\
% \hline
% 2&$\beta_{22}=(~1,-1,-1,~~1,~~1,~~1)$ &\\ 
% \hline
% 3& $b_{ij}=\hat{e}.\hat{\beta_{ij}}$, 
% & $b_{00}=0$,\\
% \hline 
% \end{tabular}
% \end{center}
% \end{table}
% \end{verbatim}
% To reference the table in the text, code as follows:
% \begin{verbatim}Table~\ref{tab1} lists the closed-form...\end{verbatim}
% to produce:

% Table~\ref{tab1} lists the closed-form . . .


% %moved here for pagination purposes
% \begin{table}
% \begin{center}
% \caption{A Simple Table Example.}
% \label{tab1}
% \begin{tabular}{| c | c | c |}
% \hline
% Order & Arbitrary coefficients & coefficients\\
% of filter & $e_m$ &   $b_{ij}$ \\
% \hline
% 1& $b_{ij}=\hat{e}.\hat{\beta_{ij}}$, & $b_{00}=0$\\
% \hline
% 2&$\beta_{22}=(~1,-1,-1,~~1,~~1,~~1)$ &\\ 
% \hline
% 3& $b_{ij}=\hat{e}.\hat{\beta_{ij}}$, & $b_{00}=0$,\\
% \hline 
% \end{tabular}
% \end{center}
% \end{table}


% \subsection{Lists}
% \noindent In this section, we will consider three types of lists: simple unnumbered, numbered and bulleted. There have been numerous options added to IEEEtran to enhance the creation of lists. If your lists are more complex than those shown below, please refer to the  ``IEEEtran\_HOWTO.pdf'' for additional options.\\

% \noindent{\bf A plain  unnumbered list}

% \begin{list}{}{}
% \item{bare\_jrnl.tex}
% \item{bare\_conf.tex}
% \item{bare\_jrnl\_compsoc.tex}
% \item{bare\_conf\_compsoc.tex}
% \item{bare\_jrnl\_comsoc.tex}
% \end{list}

% \noindent coded as:
% \begin{verbatim}
% \begin{list}{}{}
% \item{bare\_jrnl.tex}
% \item{bare\_conf.tex}
% \item{bare\_jrnl\_compsoc.tex}
% \item{bare\_conf\_compsoc.tex}
% \item{bare\_jrnl\_comsoc.tex}
% \end{list}
% \end{verbatim}
% \noindent{\bf A simple numbered list}

% \begin{enumerate}
% \item{bare\_jrnl.tex}
% \item{bare\_conf.tex}
% \item{bare\_jrnl\_compsoc.tex}
% \item{bare\_conf\_compsoc.tex}
% \item{bare\_jrnl\_comsoc.tex}
% \end{enumerate}
% \noindent coded as: 
% \begin{verbatim}
% \begin{enumerate}
% \item{bare\_jrnl.tex}
% \item{bare\_conf.tex}
% \item{bare\_jrnl\_compsoc.tex}
% \item{bare\_conf\_compsoc.tex}
% \item{bare\_jrnl\_comsoc.tex}
% \end{enumerate}
% \end{verbatim}

% \noindent{\bf A simple bulleted list}

% \begin{itemize}
% \item{bare\_jrnl.tex}
% \item{bare\_conf.tex}
% \item{bare\_jrnl\_compsoc.tex}
% \item{bare\_conf\_compsoc.tex}
% \item{bare\_jrnl\_comsoc.tex}
% \end{itemize}

% \noindent coded as:

% \begin{verbatim}
% \begin{itemize}
% \item{bare\_jrnl.tex}
% \item{bare\_conf.tex}
% \item{bare\_jrnl\_compsoc.tex}
% \item{bare\_conf\_compsoc.tex}
% \item{bare\_jrnl\_comsoc.tex}
% \end{itemize}
% \end{verbatim}


% \subsection{Other Elements}
% \noindent For other less common elements such as Algorithms, Theorems and Proofs, and Floating Structures such as page-wide tables, figures or equations, please refer to the ``IEEEtran\_HOWTO.pdf'' section on ``Double Column Floats.''


% \section{How to Create Common Back Matter Elements}
% \noindent The following sections demonstrate common back matter elements such as Acknowledgments, Bibliographies, Appendicies and Author Biographies.

% \subsection{Acknowledgments}
% \noindent This should be a simple paragraph before the bibliography to thank those individuals and institutions who have supported your work on this article.

% \begin{verbatim}
% \section{Acknowledgments}
% \noindent Text describing those who 
% supported your paper.
% \end{verbatim}

% \subsection{Bibliographies}
% \noindent {\bf{References Simplified:}} A simple way of composing references is to use the $\backslash${\tt{bibitem}} macro to define the beginning of a reference as in the following examples:\\


% \noindent [6] H. Sira-Ramirez. ``On the sliding mode control of nonlinear systems,'' \textit{Systems \& Control Letters}, vol. 19, pp. 303--312, 1992.

% \noindent coded as:
% \begin{verbatim}
% \bibitem{Sira3}
% H. Sira-Ramirez. ``On the sliding mode 
% control of nonlinear systems,'' 
% \textit{Systems \& Control Letters}, 
% vol. 19, pp. 303--312, 1992.
% \end{verbatim}

% \noindent [7] A. Levant.``Exact differentiation of signals with unbounded higher derivatives,''  in \textit{Proceedings of the 45th IEEE Conference on Decision and Control}, San Diego, California, USA, pp. 5585--5590, 2006.

% \noindent coded as:
% \begin{verbatim}\bibitem{Levant}
% A. Levant. ``Exact differentiation of 
% signals with unbounded higher 
% derivatives,''  in \textit{Proceedings 
% of the 45th IEEE Conference on 
% Decision and Control}, San Diego, 
% California, USA, pp. 5585--5590, 2006.
% \end{verbatim}


% \noindent [8] M. Fliess, C. Join, and H. Sira-Ramirez. ``Non-linear estimation is easy,'' \textit{International Journal of Modelling, Identification and Control}, vol. 4, no. 1, pp. 12--27, 2008.

% \noindent coded as:
% \begin{verbatim}
% \bibitem{Cedric}
% M. Fliess, C. Join, and H. Sira-Ramirez. 
% ``Non-linear estimation is easy,'' 
% \textit{International Journal of Modelling, 
% Identification and Control}, vol. 4, 
% no. 1, pp. 12--27, 2008.
% \end{verbatim}

% \noindent [9] R. Ortega, A. Astolfi, G. Bastin, and H. Rodriguez. ``Stabilization of food-chain systems using a port-controlled Hamiltonian description,'' in \textit{Proceedings of the American Control Conference}, Chicago, Illinois, USA, pp. 2245--2249, 2000.

% \noindent coded as:
% \begin{verbatim}
% \bibitem{Ortega}
% R. Ortega, A. Astolfi, G. Bastin, and H. 
% Rodriguez. ``Stabilization of food-chain 
% systems using a port-controlled Hamiltonian 
% description,'' in \textit{Proceedings of the 
% American Control Conference}, Chicago, 
% Illinois, USA, pp. 2245--2249, 2000.
% \end{verbatim}

% \subsection{Accented Characters in References}
% \noindent When using accented characters in references, please use the standard LaTeX coding for accents. {\bf{Do not use math coding for character accents}}. For example:
% \begin{verbatim}
% \'e, \"o, \`a, \~e 
% \end{verbatim}
% will produce: \'e, \"o, \`a, \~e 


% \subsection{Use of BibTeX}
% \noindent If you wish to use BibTeX, please see the documentation that accompanies the IEEEtran Bibliography package.

% \subsection{Biographies and Author Photos}
% \noindent Authors may have options to include their photo or not. Photos should be a bit-map graphic (.tif or .jpg) and sized to fit in the space allowed. Please see the coding samples below:
% \begin{verbatim}
% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here without a photo.
% \end{IEEEbiographynophoto}
% \end{verbatim}
% or a biography with a photo

% \begin{verbatim}
% \begin{IEEEbiography}[{\includegraphics
% [width=1in,height=1.25in,clip,
% keepaspectratio]{fig1.png}}]
% {IEEE Publications Technology Team} 
% In this paragraph you can place 
% your educational, professional background 
% and research and other interests.
% \end{IEEEbiography}
% \end{verbatim}

% Please see the end of this document to see the output of these coding examples.



% \section{Mathematical Typography \\ and Why It Matters}

% \noindent Typographical conventions for mathematical formulas have been developed to {\bf provide uniformity and clarity of presentation across mathematical texts}. This enables the readers of those texts to both understand the author's ideas and to grasp new concepts quickly. While software such as \LaTeX \ and MathType\textsuperscript{\textregistered} can produce aesthetically pleasing math when used properly, it is also very easy to misuse the software, potentially resulting in incorrect math display.

% IEEE aims to provide authors with the proper guidance on mathematical typesetting style and assist them in writing the best possible article.

% As such, IEEE has assembled a set of examples of good and bad mathematical typesetting. You will see how various issues are dealt with. The following publications have been referenced in preparing this material:

% \begin{list}{}{}
% \item{\emph{Mathematics into Type}, published by the American Mathematical Society}
% \item{\emph{The Printing of Mathematics}, published by Oxford University Press}
% \item{\emph{The \LaTeX Companion}, by F. Mittelbach and M. Goossens}
% \item{\emph{More Math into LaTeX}, by G. Gr\"atzer}
% \item{AMS-StyleGuide-online.pdf, published by the American Mathematical Society}
% \end{list}

% Further examples can be seen at \url{http://journals.ieeeauthorcenter.ieee.org/wp-content/uploads/sites/7/IEEE-Math-Typesetting-Guide.pdf}

% \subsection{Display Equations}
% \noindent A simple display equation example shown below uses the ``equation'' environment. To number the equations, use the $\backslash${\tt{label}} macro to create an identifier for the equation. LaTeX will automatically number the equation for you.
% \begin{equation}
% \label{deqn_ex1}
% x = \sum_{i=0}^{n} 2{i} Q.
% \end{equation}

% \noindent is coded as follows:
% \begin{verbatim}
% \begin{equation}
% \label{deqn_ex1}
% x = \sum_{i=0}^{n} 2{i} Q.
% \end{equation}
% \end{verbatim}

% To reference this equation in the text use the $\backslash${\tt{ref}} macro. 
% Please see (\ref{deqn_ex1})\\
% \noindent is coded as follows:
% \begin{verbatim}
% Please see (\ref{deqn_ex1})\end{verbatim}

% \subsection{Equation Numbering}
% \noindent {\bf{Consecutive Numbering:}} Equations within an article are numbered consecutively from the beginning of the
% article to the end, i.e., (1), (2), (3), (4), (5), etc. Do not use roman numerals or section numbers for equation numbering.\\

% \noindent {\bf{Appendix Equations:}} The continuation of consecutively numbered equations is best in the Appendix, but numbering
%  as (A1), (A2), etc., is permissible.\\

% \noindent {\bf{Hyphens and Periods}}: Hyphens and periods should not be used in equation numbers, i.e., use (1a) rather than
% (1-a) and (2a) rather than (2.a) for sub-equations. This should be consistent throughout the article.

% \subsection{Multi-line equations and alignment}
% \noindent Here we show several examples of multi-line equations and proper alignments.

% \noindent {\bf{A single equation that must break over multiple lines due to length with no specific alignment.}}
% \begin{multline}
% \text{The first line of this example}\\
% \text{The second line of this example}\\
% \text{The third line of this example}
% \end{multline}

% \noindent is coded as:
% \begin{verbatim}
% \begin{multline}
% \text{The first line of this example}\\
% \text{The second line of this example}\\
% \text{The third line of this example}
% \end{multline}
% \end{verbatim}

% \noindent {\bf{A single equation with multiple lines aligned at the = signs}}
% \begin{align}
% a &= c+d \\
% b &= e+f
% \end{align}
% \noindent is coded as:
% \begin{verbatim}
% \begin{align}
% a &= c+d \\
% b &= e+f
% \end{align}
% \end{verbatim}

% The {\tt{align}} environment can align on multiple  points as shown in the following example:
% \begin{align}
% x &= y & X & =Y & a &=bc\\
% x' &= y' & X' &=Y' &a' &=bz
% \end{align}
% \noindent is coded as:
% \begin{verbatim}
% \begin{align}
% x &= y & X & =Y & a &=bc\\
% x' &= y' & X' &=Y' &a' &=bz
% \end{align}
% \end{verbatim}





% \subsection{Subnumbering}
% \noindent The amsmath package provides a {\tt{subequations}} environment to facilitate subnumbering. An example:

% \begin{subequations}\label{eq:2}
% \begin{align}
% f&=g \label{eq:2A}\\
% f' &=g' \label{eq:2B}\\
% \mathcal{L}f &= \mathcal{L}g \label{eq:2c}
% \end{align}
% \end{subequations}

% \noindent is coded as:
% \begin{verbatim}
% \begin{subequations}\label{eq:2}
% \begin{align}
% f&=g \label{eq:2A}\\
% f' &=g' \label{eq:2B}\\
% \mathcal{L}f &= \mathcal{L}g \label{eq:2c}
% \end{align}
% \end{subequations}

% \end{verbatim}

% \subsection{Matrices}
% \noindent There are several useful matrix environments that can save you some keystrokes. See the example coding below and the output.

% \noindent {\bf{A simple matrix:}}
% \begin{equation}
% \begin{matrix}  0 &  1 \\ 
% 1 &  0 \end{matrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{matrix}  0 &  1 \\ 
% 1 &  0 \end{matrix}
% \end{equation}
% \end{verbatim}

% \noindent {\bf{A matrix with parenthesis}}
% \begin{equation}
% \begin{pmatrix} 0 & -i \\
%  i &  0 \end{pmatrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{pmatrix} 0 & -i \\
%  i &  0 \end{pmatrix}
% \end{equation}
% \end{verbatim}

% \noindent {\bf{A matrix with square brackets}}
% \begin{equation}
% \begin{bmatrix} 0 & -1 \\ 
% 1 &  0 \end{bmatrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{bmatrix} 0 & -1 \\ 
% 1 &  0 \end{bmatrix}
% \end{equation}
% \end{verbatim}

% \noindent {\bf{A matrix with curly braces}}
% \begin{equation}
% \begin{Bmatrix} 1 &  0 \\ 
% 0 & -1 \end{Bmatrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{Bmatrix} 1 &  0 \\ 
% 0 & -1 \end{Bmatrix}
% \end{equation}\end{verbatim}

% \noindent {\bf{A matrix with single verticals}}
% \begin{equation}
% \begin{vmatrix} a &  b \\ 
% c &  d \end{vmatrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{vmatrix} a &  b \\ 
% c &  d \end{vmatrix}
% \end{equation}\end{verbatim}

% \noindent {\bf{A matrix with double verticals}}
% \begin{equation}
% \begin{Vmatrix} i &  0 \\ 
% 0 & -i \end{Vmatrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{Vmatrix} i &  0 \\ 
% 0 & -i \end{Vmatrix}
% \end{equation}\end{verbatim}

% \subsection{Arrays}
% \noindent The {\tt{array}} environment allows you some options for matrix-like equations. You will have to manually key the fences, but you'll have options for alignment of the columns and for setting horizontal and vertical rules. The argument to {\tt{array}} controls alignment and placement of vertical rules.

% A simple array
% \begin{equation}
% \left(
% \begin{array}{cccc}
% a+b+c & uv & x-y & 27\\
% a+b & u+v & z & 134
% \end{array}\right)
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \left(
% \begin{array}{cccc}
% a+b+c & uv & x-y & 27\\
% a+b & u+v & z & 134
% \end{array} \right)
% \end{equation}
% \end{verbatim}

% A slight variation on this to better align the numbers in the last column
% \begin{equation}
% \left(
% \begin{array}{cccr}
% a+b+c & uv & x-y & 27\\
% a+b & u+v & z & 134
% \end{array}\right)
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \left(
% \begin{array}{cccr}
% a+b+c & uv & x-y & 27\\
% a+b & u+v & z & 134
% \end{array} \right)
% \end{equation}
% \end{verbatim}

% An array with vertical and horizontal rules
% \begin{equation}
% \left( \begin{array}{c|c|c|r}
% a+b+c & uv & x-y & 27\\ \hline
% a+b & u+v & z & 134
% \end{array}\right)
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \left(
% \begin{array}{c|c|c|r}
% a+b+c & uv & x-y & 27\\
% a+b & u+v & z & 134
% \end{array} \right)
% \end{equation}
% \end{verbatim}
% Note the argument now has the pipe "$\vert$" included to indicate the placement of the vertical rules.


% \subsection{Cases Structures}
% \noindent Many times we find cases coded using the wrong environment, i.e., {\tt{array}}. Using the {\tt{cases}} environment will save keystrokes (from not having to type the $\backslash${\tt{left}}$\backslash${\tt{lbrace}}) and automatically provide the correct column alignment.
% \begin{equation*}
% {z_m(t)} = \begin{cases}
% 1,&{\text{if}}\ {\beta }_m(t) \\ 
% {0,}&{\text{otherwise.}} 
% \end{cases}
% \end{equation*}
% \noindent is coded as follows:
% \begin{verbatim}
% \begin{equation*}
% {z_m(t)} = 
% \begin{cases}
% 1,&{\text{if}}\ {\beta }_m(t),\\ 
% {0,}&{\text{otherwise.}} 
% \end{cases}
% \end{equation*}
% \end{verbatim}
% \noindent Note that the ``\&'' is used to mark the tabular alignment. This is important to get  proper column alignment. Do not use $\backslash${\tt{quad}} or other fixed spaces to try and align the columns. Also, note the use of the $\backslash${\tt{text}} macro for text elements such as ``if'' and ``otherwise''.

% \subsection{Function Formatting in Equations}
% In many cases there is an easy way to properly format most common functions. Use of the $\backslash$ in front of the function name will in most cases, provide the correct formatting. When this does not work, the following example provides a solution using the $\backslash${\tt{text}} macro.

% \begin{equation*} 
%   d_{R}^{KM} = \underset {d_{l}^{KM}} {\text{arg min}} \{ d_{1}^{KM},\ldots,d_{6}^{KM}\}.
% \end{equation*}

% \noindent is coded as follows:
% \begin{verbatim}
% \begin{equation*} 
%  d_{R}^{KM} = \underset {d_{l}^{KM}} 
%  {\text{arg min}} \{ d_{1}^{KM},
%  \ldots,d_{6}^{KM}\}.
% \end{equation*}
% \end{verbatim}

% \subsection{ Text Acronyms inside equations}
% \noindent This example shows where the acronym ``MSE" is coded using $\backslash${\tt{text\{\}}} to match how it appears in the text.

% \begin{equation*}
%  \text{MSE} = \frac {1}{n}\sum _{i=1}^{n}(Y_{i} - \hat {Y_{i}})^{2}
% \end{equation*}

% \begin{verbatim}
% \begin{equation*}
%  \text{MSE} = \frac {1}{n}\sum _{i=1}^{n}
% (Y_{i} - \hat {Y_{i}})^{2}
% \end{equation*}
% \end{verbatim}

% \subsection{Obsolete Coding}
% \noindent Avoid the use of outdated environments, such as {\tt{eqnarray}} and \$\$ math delimiters, for display equations. The \$\$ display math delimiters are left over from PlainTeX and should not be used in \LaTeX, ever. Poor vertical spacing will result.
% \subsection{Use Appropriate Delimiters for Display Equations}
% \noindent Some improper mathematical coding advice has been given in various YouTube\textsuperscript{TM} videos on how to write scholarly articles, so please follow these good examples:\\

% For {\bf{single-line unnumbered display equations}}, please use the following delimiters: 
% \begin{verbatim}\[ . . . \] or \end{verbatim} 
% \begin{verbatim}\begin{equation*} . . . \end{equation*}\end{verbatim}
% Note that the * in the environment name turns off equation numbering.\\

% For {\bf{multiline unnumbered display equations}} that have alignment requirements, please use the following delimiters: 
% \begin{verbatim}
% \begin{align*} . . . \end{align*}
% \end{verbatim}

% For {\bf{single-line numbered display equations}}, please use the following delimiters: 
% \begin{verbatim}
% \begin{equation} . . . \end{equation}
% \end{verbatim}

% For {\bf{multiline numbered display equations}}, please use the following delimiters: 
% \begin{verbatim}
% \begin{align} . . . \end{align}
% \end{verbatim}

% \section{LaTeX Package Suggestions}
% \noindent Immediately after your documenttype declaration at the top of your \LaTeX\ file is the place where you should declare any packages that are being used. The following packages were used in the production of this document.
% \begin{verbatim}
% \usepackage{amsmath,amsfonts}
% \usepackage{algorithmic}
% \usepackage{array}
% \usepackage[caption=false,font=normalsize,
%   labelfont=sf,textfont=sf]{subfig}
% \u00sepackage{textcomp}
% \usepackage{stfloats}
% \usepackage{url}
% \usepackage{verbatim}
% \usepackage{graphicx}
% \usepackage{balance}
% \end{verbatim}

% \section{Additional Advice}

% Please use ``soft'' (e.g., \verb|\eqref{Eq}|) or \verb|(\ref{Eq})|
% cross references instead of ``hard'' references (e.g., \verb|(1)|).
% That will make it possible to combine sections, add equations, or
% change the order of figures or citations without having to go through
% the file line by line.

% Please note that the \verb|{subequations}| environment in {\LaTeX}
% will increment the main equation counter even when there are no
% equation numbers displayed. If you forget that, you might write an
% article in which the equation numbers skip from (17) to (20), causing
% the copy editors to wonder if you've discovered a new method of
% counting.

% {\BibTeX} does not work by magic. It doesn't get the bibliographic
% data from thin air but from .bib files. If you use {\BibTeX} to produce a
% bibliography you must send the .bib files. 

% {\LaTeX} can't read your mind. If you assign the same label to a
% subsubsection and a table, you might find that Table I has been cross
% referenced as Table IV-B3. 

% {\LaTeX} does not have precognitive abilities. If you put a
% \verb|\label| command before the command that updates the counter it's
% supposed to be using, the label will pick up the last counter to be
% cross referenced instead. In particular, a \verb|\label| command
% should not go before the caption of a figure or a table.

% Please do not use \verb|\nonumber| or \verb|\notag| inside the
% \verb|{array}| environment. It will not stop equation numbers inside
% \verb|{array}| (there won't be any anyway) and it might stop a wanted
% equation number in the surrounding equation.

% \balance

% \section{A Final Checklist}
% \begin{enumerate}{}{}
% \item{Make sure that your equations are numbered sequentially and there are no equation numbers missing or duplicated. Avoid hyphens and periods in your equation numbering. Stay with IEEE style, i.e., (1), (2), (3) or for sub-equations (1a), (1b). For equations in the appendix (A1), (A2), etc.}. 
% \item{Are your equations properly formatted? Text, functions, alignment points in cases and arrays, etc. }
% \item{Make sure all graphics are included.}
% \item{Make sure your references are included either in your main LaTeX file or a separate .bib file if calling the external file.}
% \end{enumerate}

\bibliographystyle{IEEEtran}
\bibliography{biblio.bib}

% \begin{thebibliography}{1}

% \bibitem{ams}
% {\it{Mathematics into Type}}, American Mathematical Society. Online available: 

% \bibitem{oxford}
% T.W. Chaundy, P.R. Barrett and C. Batey, {\it{The Printing of Mathematics}}, Oxford University Press. London, 1954.

% \bibitem{lacomp}{\it{The \LaTeX Companion}}, by F. Mittelbach and M. Goossens

% \bibitem{mmt}{\it{More Math into LaTeX}}, by G. Gr\"atzer

% \bibitem{amstyle}{\it{AMS-StyleGuide-online.pdf,}} published by the American Mathematical Society

% \bibitem{Sira3}
% H. Sira-Ramirez. ``On the sliding mode control of nonlinear systems,'' \textit{Systems \& Control Letters}, vol. 19, pp. 303--312, 1992.

% \bibitem{Levant}
% A. Levant. ``Exact differentiation of signals with unbounded higher derivatives,''  in \textit{Proceedings of the 45th IEEE Conference on Decision and Control}, San Diego, California, USA, pp. 5585--5590, 2006.

% \bibitem{Cedric}
% M. Fliess, C. Join, and H. Sira-Ramirez. ``Non-linear estimation is easy,'' \textit{International Journal of Modelling, Identification and Control}, vol. 4, no. 1, pp. 12--27, 2008.

% \bibitem{Ortega}
% R. Ortega, A. Astolfi, G. Bastin, and H. Rodriguez. ``Stabilization of food-chain systems using a port-controlled Hamiltonian description,'' in \textit{Proceedings of the American Control Conference}, Chicago, Illinois, USA, pp. 2245--2249, 2000.

% \end{thebibliography}

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here without a photo.
% \end{IEEEbiographynophoto}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1.png}}]{IEEE Publications Technology Team}
% In this paragraph you can place your educational, professional background and research and other interests.\end{IEEEbiography}
\section{\textbf{Appendix A:} Hardware design parameters}

The HLS design parameters of OpenCL matrix multiplication kernel are listed in Table \ref{table:opencl_params}. Additionally, the design parameters of SPEC2k benchmark are reported in Table \ref{table:spec_params}.



\begin{table}[h]
\centering
\renewcommand\thetable{5}
\caption{OpenCL Matrix multiplication design parameters.}
\label{table:opencl_params}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lc}
\hline
\#&\textbf{Parameter}\\
\hline
\hline
1&number of blocks\\
2&Sub-dimension x\\
3&Sub-dimension y\\
4&Manual SIMD x\\
5&Manual SIMD y\\
6&SIMD\\
7&Number of compute units\\
8&Enable Unroll\\
9&Unroll factor\\
\hline
\end{tabular}
\end{table} 

\begin{table}[h]
\centering
\renewcommand\thetable{6}
\caption{Spec2k design parameters.}
\label{table:spec_params}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lcc}
\hline
\#&\textbf{Parameter}\\
\hline
\hline
1&  FO4 depth\\
2&  Load/store queue width\\
3&  Number of physical registers\\
4&  Number of reservation stations\\
5&  i-L1 cache\\
6&  d-L1 cache\\
7&  L2 cache\\
8&  Control latency\\
9&  Floating point latency\\
10&  Fixed point latency\\
11&  Load/store latency\\
12&  Memory latency\\

\hline
\end{tabular}
\end{table} 


\section{\textbf{Appendix B:} Bayesian Optimization using Matern covariance kernel}
 \begin{table*}[b]
\centering
\renewcommand\thetable{7}
\caption{Comparison of Matern kernels vs squared exponential kernel on Gaussian regression bootstrapping on the SPECT2k AMMP benchmark.}
\label{table:regression-mse-matern}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lcccc}
\hline
\textbf{ Covariance Kernel}&                             & Squared Exponential & Matern ($\nu=3/2$)~~~~ & ~~~~(Matern $\nu=5/2$)\\
\hline
                         % &                             & Regression using& Regression using& Regression using \\
                         & Original                    &  Gaussian Bootstrapping&  Gaussian Bootstrapping&  Gaussian Bootstrapping\\
                         % &          regression setting &  simulated data&  simulated data&  simulated data\\
                         &          $\sqrt{\text{MSE}}/\mu$ &  $\sqrt{\text{MSE}}/\mu$&  $\sqrt{\text{MSE}}/\mu$&  $\sqrt{\text{MSE}}/\mu$\\
\hline
\hline
\textbf{Linear Regression Instructions Per Second} & 0.056   & 0.061  & 0.061 & 0.062 \\
\textbf{Linear Regression Power Consumption (mW)} & 0.45 &  0.46  & 0.46 & 0.46 \\
\textbf{Random Forest Instructions Per Second} & 0.017  &  0.055 & 0.052 & 0.056 \\
\textbf{Random Forest Power Consumption (mW)} & 0.050  & 0.14  & 0.13& 0.13\\
\hline
\end{tabular}
\end{table*} 

In Bayesian optimization, choosing a proper covariance kernel is essential for accuracy of the algorithm. Matern kernels are a class of stationary covariance function that depends on two hyper-parameters known as the length scale $l$ and the smoothness $\nu$. By choosing a suitable value for $\nu$ the Bayesian model can capture the smoothness and roughness of the objective function. The Matern class of covariance kernels is defined as
 \begin{align}
\label{eq:cov_matrix}
&\mathbf{K}_{ij}=\mathit{k_{\text{Matern}}}({x}_i, {x}_j) \\ \nonumber
& =\frac{1}{2^{\nu-1}\Gamma(\nu)} \left ( \frac{\sqrt{2\nu}|{x}_i- {x}_j|}{l} \right)^\nu {H}_v\left ( \frac{\sqrt{2\nu}|{x}_i- {x}_j|}{l} \right),
\end{align}
where $\Gamma$ is the Gamma function and $H_v$ is modified Bessel function of the second kind of order $\nu$. A common choice for $\nu$ are $\frac{3}{2}, \frac{5}{2}$ and also when $\nu \to \infty$ the Matern kernels becomes the squared exponential kernel \cite{genton2001classes}.

We repeated the experiments in Table \ref{table:regression-mse} using Matern covariance kernels with $\nu \in \left\{\frac{3}{2}, \frac{5}{2}\right\}$ and reported the results in Table \ref{table:regression-mse-matern}. The reported $\sqrt{\text{MSE}}/\mu$ in Table \ref{table:regression-mse-matern} shows that our method exhibits a robust behavior in terms of choosing various covariance kernels for modeling SPECT2k ammp benchmark. Note that the choice of covariance kernel depends on the nature of the data and can vary in different applications.



\end{document}


