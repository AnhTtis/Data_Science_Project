{
    "arxiv_id": "2303.08054",
    "paper_title": "Statistical Hardware Design With Multi-model Active Learning",
    "authors": [
        "Alireza Ghaffari",
        "Masoud Asgharian",
        "Yvon Savaria"
    ],
    "submission_date": "2023-03-14",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 4,
    "categories": [
        "cs.AR",
        "cs.LG"
    ],
    "abstract": "With the rising complexity of numerous novel applications that serve our modern society comes the strong need to design efficient computing platforms. Designing efficient hardware is, however, a complex multi-objective problem that deals with multiple parameters and their interactions. Given that there are a large number of parameters and objectives involved in hardware design, synthesizing all possible combinations is not a feasible method to find the optimal solution. One promising approach to tackle this problem is statistical modeling of a desired hardware performance. Here, we propose a model-based active learning approach to solve this problem. Our proposed method uses Bayesian models to characterize various aspects of hardware performance. We also use transfer learning and Gaussian regression bootstrapping techniques in conjunction with active learning to create more accurate models. Our proposed statistical modeling method provides hardware models that are sufficiently accurate to perform design space exploration as well as performance prediction simultaneously. We use our proposed method to perform design space exploration and performance prediction for various hardware setups, such as micro-architecture design and OpenCL kernels for FPGA targets. Our experiments show that the number of samples required to create performance models significantly reduces while maintaining the predictive power of our proposed statistical models. For instance, in our performance prediction setting, the proposed method needs 65% fewer samples to create the model, and in the design space exploration setting, our proposed method can find the best parameter settings by exploring less than 50 samples.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08054v1",
        "http://arxiv.org/pdf/2303.08054v2",
        "http://arxiv.org/pdf/2303.08054v3",
        "http://arxiv.org/pdf/2303.08054v4"
    ],
    "publication_venue": "added a reference for GRP subsampling and corrected typos"
}