% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
\usepackage{color}
\usepackage{hyperref}
\renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{tablefootnote}
\usepackage{color}


\newcommand{\ceil}[1]{\lceil {#1} \rceil}
\usepackage{tikz}
\usepackage{pifont}
            
\newcommand{\KM}[1]{{\color{red} \textbf{KM}: #1}}
\newcommand{\JZ}[1]{{\color{blue} \textbf{JZ}: #1}}
\newcommand{\PP}[1]{{\color{cyan} \textbf{PP}: #1}}
\newcommand{\MV}[1]{{\color{magenta} \textbf{MV}: #1}}

% \newcommand{\resultsection}[1]{\noindent\textbf{{#1}}}
\newcommand{\resultsection}[1]{\subsubsection{#1}}
\newcommand{\datasection}[1]{\textbf{#1}}


%
\begin{document}
%
\title{Prompt-MIL: Boosting Multi-Instance Learning Schemes via Task-specific Prompt Tuning}

\titlerunning{Prompt-MIL: Boosting MIL Schemes via Prompt Tuning}

\author{Jingwei Zhang\inst{1} \and
Saarthak Kapse\inst{1} \and Ke Ma \inst{2} \and Prateek Prasanna\inst{1} \and
Joel Saltz\inst{1} \and Maria Vakalopoulou\inst{3} 
\and Dimitris Samaras\inst{1}}
%index{Zhang, Jingwei} 
%index{Kapse, Saarthak} 
%index{Ma, Ke}
%index{Prasanna, Prateek}
%index{Saltz, Joel}
%index{Vakalopoulou, Maria}
%index{Samaras, Dimitris}
\authorrunning{J. Zhang et al.}

\institute{
    Stony Brook University, USA \and Snap Inc., USA
        \and
        CentraleSup√©lec, University of Paris-Saclay, France\\
    \email{\email{\{jingwezhang, kemma, samaras\}@cs.stonybrook.edu}} \\
    \email{\{saarthak.kapse, prateek.prasanna\}@stonybrook.edu} \\
    \email{Joel.Saltz@stonybrookmedicine.edu   maria.vakalopoulou@centralesupelec.fr}
}

% \author{Anonymous}
% \authorrunning{Anonymous et al.}
% \institute{Anonymous Organization\\
% { \email{**@******.***} } }

%%%% llncs was modified by adding paragraphdata

%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Whole slide image (WSI) classification is a critical task in computational pathology, requiring the processing of gigapixel-sized images, which is challenging for current deep-learning methods. 
Current state of the art methods are based on multi-instance learning schemes (MIL), which usually rely on pretrained features to represent the instances. 
Due to the lack of task-specific annotated data, these features are either obtained from well-established backbones on natural images, or, more recently from self-supervised models pretrained on histopathology. 
However, both approaches yield task-agnostic features, resulting in performance loss compared to the appropriate task-related supervision, if available.
In this paper, we show that when task-specific annotations are limited, we can inject such supervision into downstream task training, to reduce the gap between fully task-tuned and task agnostic features. 
We propose Prompt-MIL, an MIL framework that integrates prompts into WSI classification. 
Prompt-MIL adopts a prompt tuning mechanism, where only a small fraction of parameters calibrates the pretrained features to encode task-specific information, rather than the conventional full fine-tuning approaches.
Extensive experiments on three WSI datasets, TCGA-BRCA, TCGA-CRC, and BRIGHT, demonstrate the superiority of Prompt-MIL over conventional MIL methods, achieving a relative improvement of 1.49\%-4.03\% in accuracy and 0.25\%-8.97\% in AUROC while using fewer than 0.3\% additional parameters. 
Compared to conventional full fine-tuning approaches, we fine-tune less than 1.3\% of the parameters, yet achieve a relative improvement of 1.29\%-13.61\% in accuracy and 3.22\%-27.18\% in AUROC and reduce GPU memory consumption by 38\%-45\% while training 21\%-27\% faster.


\keywords{Whole slide image classification  \and Multiple instance
learning \and Prompt tuning.}
\end{abstract}
%
%
%

\input{1_intro}
\input{2_method}
\input{3_exp}
\input{4_sum}

\subsubsection{Acknowledgements} This work was partially supported by the ANR Hagnodice ANR-21-CE45-0007, the NSF IIS-2212046, the NSF IIS-2123920, the NIH 1R21CA258493-01A1, the NCI UH3CA225021 and Stony Brook University Provost Funds. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.


%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{bibliography}
%
\end{document}
