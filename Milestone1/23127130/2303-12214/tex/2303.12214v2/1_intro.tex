\section{Introduction}

Whole slide image (WSI) classification is a critical task in computational pathology enabling disease diagnosis and subtyping using automatic tools. 
Owing to the paucity of patch-level annotations, multiple instance learning (MIL) ~\cite{hou2016patch,lu2021data_clam,shao2021transmil} techniques have become a staple in WSI classification. 
Under an MIL scheme, WSIs are divided into tissue patches or instances, and a feature extractor is used to generate features for each instance. 
These features are then aggregated using different pooling or attention-based operators to provide a WSI-level prediction.
ImageNet pretrained networks have been widely used as MIL feature extractors. 
More recently, self-supervised learning (SSL), using a large amount of unlabeled histopathology data, has become quite popular for WSI classification~\cite{li2021dual_dsmil,chen2022scaling_hipt} as it outperforms ImageNet feature encoders.


Most existing MIL methods do not fine-tune their feature extractor together with their classification task; this stems from the requirement for far larger GPU memory than is available currently due to the gigapixel nature of WSIs, e.g. training a WSI at 10x magnification may require more than 300 Gb of GPU memory.
Recently, researchers have started to explore optimization methods to enable end-to-end training of the entire network and entire WSI within GPU memory~\cite{takahama2019multi_retaining,pinckaers2020streaming,zhang2022gigapixel_local_learning}.
These methods show better performance compared to conventional MIL; they suffer, however, from two limitations. 
First, they are ImageNet-pretrained and do not leverage the powerful learning capabilities of histology-trained SSL models. 
Second, these are mostly limited to convolutional architectures rather than more effective attention-based architectures such as vision transformers~\cite{vit}.

\textbf{Motivation:} To improve WSI-level analysis, we explore end-to-end training of the entire network using SSL pretrained ViTs. To achieve this, we use the patch batching and gradient retaining techniques in~\cite{takahama2019multi_retaining}. 
However, we find that conventional fine-tuning approaches, where the entire network is fine-tuned, achieve low performance.
For example, on the BRIGHT dataset\cite{bright}, the accuracy drops more than 5\% compared to the conventional MIL approaches.
The poor performance is probably caused by the large network over-fitted to the limited downstream training data, leading to suboptimal feature representation. 
Indeed, especially for weakly supervised WSI classification, where annotated data for downstream tasks is significantly less compared to natural image datasets, conventional fine-tuning schemes can prove to be quite challenging.

To address the subpar performance of SSL-pretrained vision transformers, 
% we hypothesize that prompt tuning can be a promising solution. 
we utilize the prompt tuning techniques.
Initially proposed in natural language processing, a prompt is a trainable or a pre-defined natural language statement that is provided as additional input to a transformer to guide the neural network towards learning a specific task or objective~\cite{brown2020language_prompt_gpt,lester2021power_prompt_org}. 
Using prompt tuning we \textit{fine-tune only the prompt and downstream network without re-training the large backbone} (e.g. GPT-3 with 17B parameters). 
This approach is parameter efficient~\cite{lester2021power_prompt_org,liu2022p_p_tuning} and has been shown to better inject task-specific information and reduce the overfitting in downstream tasks, particularly in limited data scenarios~\cite{schucher2022power_prompt_lwo_res,gu2022ppt_prompt_few_shot}. 
% Recently, prompts have also been adopted in computer vision to facilitate efficient and effective transfer learning on various tasks, demonstrating superior performance compared to conventional fine-tuning methods~\cite{jia2022visual_prompt_vpt}.
Recently, prompts have also been adopted in computer vision and demonstrated superior performance compared to conventional fine-tuning methods~\cite{jia2022visual_prompt_vpt}.
Prompt tuning performs well even when only limited labeled data is available for training, making it particularly attractive in computational pathology. 
The process of prompt tuning thus involves providing a form of limited guidance during the training of downstream tasks, with the goal of minimizing the discrepancy between feature representations that are fully tuned to the task and those that are not task-specific. 
% By injecting this limited supervision, prompt tuning aims to reduce the gap between these two types of feature representations.   %Hence, we regard it as a promising solution to the subpar performanced networks. 

% Prompt-tuning is particularly attractive to computational pathology because 1) ....2) ....Hence, we regard it as a promising solution to the subpar performanced networks. 

In this paper, we propose a novel framework, Prompt-MIL, which uses prompts for WSI-level classification tasks within an MIL paradigm. Our contributions are: %We summarize the novelty and contributions as follows:
\begin{itemize}
    \item \textbf{Fine-tuning:} Unlike existing works in histopathology image analysis, Prompt-MIL is fine-tuned using prompts rather than conventional full fine-tuning methods.
    \item \textbf{Task-specific representation learning:} Our framework employs an SSL pretrained ViT feature extractor with a trainable prompt that calibrates the representations making them task-specific. By doing so, only the prompt parameters together with the classifier, are optimized. This avoids potential overfitting while still injecting task-specific knowledge into the learned representations.
\end{itemize}
Extensive experiments on three public WSI datasets, TCGA-BRCA, TCGA-CRC, and BRIGHT demonstrate the superiority of Prompt-MIL over conventional MIL methods, achieving a relative improvement of 1.49\%-4.03\% in accuracy and 0.25\%-8.97\% in AUROC by using only less than 0.3\% additional parameters. 
Compared to the conventional full fine-tuning approach, we fine-tune less than 1.3\% of the parameters, yet achieve a relative improvement of 1.29\%-13.61\% in accuracy and 3.22\%-27.18\% in AUROC. 
Moreover, compared to the full fine-tuning approach, our method reduces GPU memory consumption by 38\%-45\% and trains 21\%-27\% faster. 
To the best of our knowledge, this is the first work where prompts are explored for WSI classification. 
While our method is quite simple, it is versatile as it is agnostic to the MIL scheme and can be easily applied to different MIL methods.
Our code is available at \href{https://github.com/cvlab-stonybrook/PromptMIL}{https://github.com/cvlab-stonybrook/PromptMIL}.