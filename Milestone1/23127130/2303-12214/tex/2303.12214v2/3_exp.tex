\section{Experiments and Discussion}
\subsection{Datasets}
We assessed Prompt-MIL using three histopathological WSI datasets: TCGA-BRCA \cite{tcga_brca}, TCGA-CRC~\cite{cancer2012comprehensive}, and BRIGHT~\cite{bright}.
These datasets were utilized for both the self-supervised feature extractor pretraining and the end-to-end fine-tuning (with or without prompts), including the MIL component. 
Note that the testing data were not used in the SSL pretraining. \datasection{TCGA-BRCA} contains 1034 diagnostic digital slides of two breast cancer subtypes: invasive ductal carcinoma (IDC) and invasive lobular carcinoma (ILC). 
We used the same training, validation, and test split as that in the first fold cross validation in~\cite{chen2022scaling_hipt}. 
The cropped patches (790K training, 90K test) were extracted at 5$\times$  magnification. 
\datasection{TCGA-CRC} contains 430 diagnostic digital slides of colorectal cancer for a binary classification task: chromosomal instability (CIN) or genome stable (GS). 
Following the common 4-fold data split~\cite{bilal2021development,liu2018comparative}, we used the first three folds for training (236 GS, 89 CIN), and the fourth for testing (77 GS, 28 CIN). 
We further split 20\% (65 slides) training data as a validation set. The cropped patches (1.07M training, 370K test) were extracted at 10$\times$  magnification. 
\datasection{BRIGHT} contains 503 diagnostic slides of breast tissues. 
We used the official training (423 WSIs) and test (80 WSIs) splits. 
The task involves classifying non-cancerous (196 training, 25 test) vs. pre-cancerous (66 training, 23 test) vs. cancerous (161 training, 32 test). 
We further used 20\% (85 slides) training slides for validation. 
The cropped patches (1.24M training, 195K test) were extracted at 10$\times$ magnification. 

\begin{table}[t]
\caption{Comparison of accuracy and AUROC on three datasets. Reported metrics (in $\%$age) are the average across 3 runs. "Num. of Parameters" represents the number of optimized parameters}
\label{table:result:accuracy}
\begin{center}
\setlength{\tabcolsep}{0.9mm}{

\begin{tabular}{l |c c c c c c |c}
\toprule
\multicolumn{1}{c|}{Dataset} 
    & \multicolumn{2}{c}{TCGA-BRCA} 
        & \multicolumn{2}{c}{TCGA-CRC}
            & \multicolumn{2}{c}{BRIGHT}
    & \multicolumn{1}{|c}{Num. of}
\\
% \midrule
\multicolumn{1}{c|}{Metric} 
    & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{AUROC} 
        & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{AUROC}
            & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{AUROC}
    & \multicolumn{1}{|c}{Parameters} 
\\
\midrule
Conventional MIL 
    & $92.10$          & $96.65$       
        & $73.02$         & $69.24$
            & $62.08$         & $80.96$
    & 70k
\\
Full fine-tuning 
    & $88.14$ & $93.78$
        & $74.53$ & $56.63$
            & $56.13$ & $75.87$
    & 5.6M
\\ 
Prompt-MIL (ours) 
    & $\bm{93.47}$ & $\bm{96.89}$
        & $\bm{75.47}$  & $\bm{75.45}$ 
            & $\bm{64.58}$  & $\bm{81.31}$ 
            
    & 70k+192
\\
\bottomrule
\end{tabular}
}
\end{center}
\end{table}


\subsection{Implementation Details}
We cropped non-overlapping 224 $\times$ 224 sized patches in all our experiments and used ViT-Tiny (ViT-T/16)~\cite{vit} for feature extraction.
For SSL pretraining, we leveraged the DINO framework~\cite{dino} with the default hyperparameters, but adjusted the batch size to 256 and employed the global average pooling for token aggregation. 
We pretrained separate ViT models on the TCGA-CRC datasets for 50 epochs, on the BRIGHT dataset for 50 epochs, and on the BRCA dataset for 30 epochs. 
For TCGA-BRCA, we used the AdamW~\cite{loshchilov2017adamW} optimizer with a learning rate of $1e-4$, $1e-2$ weight decay, and trained for 40 epochs.
For TCGA-CRC, we also used the AdamW optimizer with a learning rate of $5e-4$ and trained for 40 epochs.
For Bright, we used the Adam~\cite{adam} optimizer with a learning rate of $1e-4$, $5e-2$ weight decay and trained for 40 epochs.
We applied a cosine annealing learning rate decay policy in all our experiments.
For the MIL baselines, we employed the same hyperparameters as above.
For all full fine-tuning experiments, we used the learning rate in the corresponding prompt experiment as the base learning rate. For parameters in the feature model $F(\cdot)$, which are SSL pretrained, we use 1/10 of the base learning rate. For parameters in the Classifier $G(\cdot)$, which are randomly initialized, we use the base learning rate. We train the full tuning model for 10 more epochs than our prompt training to allow full convergence. This training strategy is optimized using the validation datasets.
All model implementations were in PyTorch~\cite{paszke2019pytorch} on a NVIDIA Tesla V100 or a Nvidia Quadro RTX 8000.

\subsection{Results}
% We tested the effectiveness of our proposed method on the three downstream datasets comprising prostate, colon, and breast cancer. 
% \KM{may add some details about the methods you compared to here if space allows.}
We chose overall accuracy and Area Under Receiver Operating Characteristic curve (AUROC) as the evaluation metrics. 


% \noindent\textbf{Evaluation of prompt tuning performance:} \label{sec:result_prompt}
\resultsection{Evaluation of prompt tuning performance:} \label{sec:result_prompt}
We compared the proposed Prompt-MIL with two baselines: 1) a conventional MIL model with a frozen feature extractor~\cite{li2021dual_dsmil}, 2) fine-tuning all parameters in the feature model (full fine-tuning).
Table~\ref{table:result:accuracy} highlights that our Prompt-MIL consistently outperformed both.
%the conventional MIL method and the full fine-tuning method. 
%added another 192 para, which is less than 0.3\% of the total parameters of the conventional MIL competitor.
% With such a negligible parameter overhead, our Prompt-MIL 
Compared to the conventional MIL method, Prompt-MIL added negligible parameters (192, less than 0.3\% of the total parameters), 
achieving a relative improvement of 1.49\% in accuracy and 0.25\% in AUROC on TCGA-BRCA, 3.36\% in accuracy and 8.97\% in AUROC on TCGA-CRC, and 4.03\% in accuracy and 0.43\% in AUROC on BRIGHT.
The observed improvement can be attributed to a more optimal alignment between the feature representation learned during the SSL pretraining and the downstream task, i.e., the prompt explicitly calibrated the features toward the downstream task.  


The computationally intensive full fine-tuning method under-performed conventional MIL and Prompt-MIL. 
Compared to the full fine-tuning method, our method achieved a relative improvement of 1.29\% to 13.61\% in accuracy and 3.22\% to 27.18\% in AUROC on the three datasets.
Due to the relatively small amount of slide-level labels (few hundred to a few thousands) fully fine tuning 5M parameters in the feature model might suffer from overfitting. 
In contrast, our method contained less than 1.3\% of parameters compared to full fine-tuning, leading to robust training.


\begin{table}[t]
\caption{Comparison of GPU memory consumption and training speed per slide benchmarked on the BRIGHT dataset between the full fine-tuning and our prompt tuning on four slides with different sizes. Our prompt method requires far less memory and is significantly faster.}  
\begin{center}
\setlength{\tabcolsep}{1.6mm}{
\begin{tabular}{clcccc}
\toprule
\multicolumn{2}{c}{WSI size}  
    & $44k\times21k$ & $26k\times21k$ 
        & $22k\times17k$ & $11k\times16k$ \\
\multicolumn{2}{c}{\#Tissue patches} 
    & 9212               & 4765               
        & 2307               & 1108               \\ 
\midrule
GPU        & Full fine-tuning 
    & 21.81G             & 18.22G             
        & 16.37G             & 12.71G             \\
Mem.     & Prompt (ours)    
    & $\bm{12.04}$G             & $\bm{10.66}$G             
        & $\bm{10.00}$G              & $\bm{7.90}$G              \\ 
\cmidrule{2-6}
        & Reduction percentage 
    & 44.79\%  & 41.50\% & 38.92\% & 37.84\% \\
\midrule
Time      & Full fine-tuning 
    & 17.73s             & 8.92s              
        & 4.37s              & 2.15s              \\
per slide  & Prompt (ours)    
    & $\bm{13.92}$s             & $\bm{7.09}$s              
        & $\bm{3.35}$s              & $\bm{1.56}$s              \\
\cmidrule{2-6}
& Reduction percentage 
    & 21.49\%  & 20.51\% & 23.32\% & 27.27\% \\
\bottomrule
\end{tabular}
}
\end{center}
\label{table:result:gpu}
\end{table}

\begin{table}[b]
\caption{Comparison of accuracy and AUROC on three datasets for a pathological foundation model.
}
\label{table:result:universal_models}
\begin{center}
% \setlength{\tabcolsep}{1.6mm}{

\begin{tabular}{l c c c c }
\toprule
\multicolumn{1}{c}{Dataset} & \multicolumn{2}{c}{TCGA-BRCA} & \multicolumn{2}{c}{BRIGHT} \\
% \midrule
\multicolumn{1}{c}{Metric} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{AUROC} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{AUROC}  \\
\midrule
ViT-small~\cite{wang2022transformer}
    & $91.75$          & $97.03$       
        & $54.17$  & $76.76$ \\
ViT-small w/ Prompt-MIL
    & $\bm{92.78} $ & $\bm{97.53}$
        & $\bm{57.50}$  & $\bm{78.29}$ \\
\bottomrule
\end{tabular}
% }
\end{center}
\end{table}


\resultsection{Evaluation of time and GPU memory efficiency:} Prompt-MIL is an efficient method requiring less GPU memory to train and running much faster than full fine-tuning methods. 
We evaluated the training speed and memory consumption of our method and compared to the full fine-tuning baseline on four different sized WSIs in the BRIGHT dataset.
% As sizes and areas of tissue regions varies among slides in a dataset, instead of evaluating the memory consumption and training speed on three datasets, 
% We evaluated it on four different sized WSIs in the BRIGHT dataset.
As shown in Table~\ref{table:result:gpu}, our method consumed around 38\% to 45\% less GPU memory compared to full fine-tuning and was 21\% to 27\% faster. 
As we scaled up the WSI size (i.e. WSIs with more number of patches), the memory cost difference between Prompt-MIL and full fine-tuning further widened. 
%Therefore our method is particularly even more crucial for processing large WSIs or for processing at higher magnification (eg. 20X, 40X) which contains much more number of patches per WSI. 


\resultsection{Evaluation on the pathological foundation models:} 
% The effectiveness of our prompt-MIL in augmenting the performance of foundational pathology models has been also evaluated. 
We demonstrated our Prompt-MIL also had a better performance when used with the pathological foundation model.
Foundational models refer to those trained on large-scale pathology datasets (e.g. the entire TCGA Pan-cancer dataset~\cite{weinstein2013cancer}). 
We utilized the publicly available~\cite{wang2022transformer,transpath} ViT-Small network pretrained using MoCo v3~\cite{mocov3} on all the slides from TCGA~\cite{weinstein2013cancer} and PAIP~\cite{paip}. 
In Table~\ref{table:result:universal_models}, we showed that our method robustly boosted the performance on both TCGA (the same domain as the foundation model trained on) and BRIGHT (a different domain). 
The improvement is more prominent in BRIGHT, which further confirmed that Prompt-MIL aligns the feature extractor to be more task-specific.

\begin{table}[h]
\caption{Performance with a different number of prompt tokens. For two different WSI classification tasks, one token was enough to boost the performance of the conventional MIL schemes.}
\label{table:result:abalation}
\begin{center}
\setlength{\tabcolsep}{1.6mm}{

\begin{tabular}{ccccc}
\toprule
\multicolumn{1}{c}{Dataset} 
    & \multicolumn{2}{c}{TCGA-BRCA}        
        & \multicolumn{2}{c}{BRIGHT}           \\
\#prompt tokens $k$             
    & \multicolumn{1}{l}{Accuracy} & AUROC 
        & \multicolumn{1}{l}{Accuracy} & AUROC \\ 
\midrule
$k=1$                           
    & $\bm{93.47}$  & $\bm{96.89}$ 
        & $\bm{64.58}$  & $\bm{81.31}$ \\
$k=2$                       
    & 93.13   & $\bm{96.93}$ 
        & 60.41 & 79.74 \\
$k=3$                           
    &  $\bm{93.47}$  &  $96.86$     
        &  59.17   &  76.75     \\ 
\bottomrule
\end{tabular}
}
\end{center}
\end{table}

\resultsection{Ablation study:}
An ablation was performed to study the effect of the number of trainable prompt tokens on downstream tasks. 
%We used the same pretrained feature models as that in section~\ref{sec:result_prompt}.
Table~\ref{table:result:abalation} shows the accuracy and AUROC of our Prompt-MIL model with 1, 2 and 3 trainable prompt tokens ($k=1, 2, 3$) on the TCGA-BRCA and the BRIGHT datasets.
On the TCGA-BRCA dataset, our Prompt-MIL model with 1 to 3 prompt tokens reported similar performance.
On the BRIGHT dataset, the performance of our model dropped with the increased number of prompt tokens. 
% This drop is consistant with
% Such a performance drop is consistent with the large performance drop of our method and full fine-tuning method in Table~\ref{table:result:accuracy}.
Empirically, this ablation study shows that for classification tasks, one prompt token is sufficient to boost the performance of conventional MIL methods.
% It is probably because the number of training samples in the BRIGHT dataset is only around half of that in the TCGA-BRCA dataset, and thus the increased trainable tokens may cause over-fitting.
% Overall, this ablation study showed that for classification tasks, one prompt token is sufficient to boost the performance of conventional MIL methods.



