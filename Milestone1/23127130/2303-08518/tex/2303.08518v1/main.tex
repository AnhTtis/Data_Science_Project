% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage[table,xcdraw]{xcolor}

% Remove the "review" option to generate the final version.
\usepackage[]{EMNLP2022}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
% \usepackage{inconsolata}
% newly added packages%%%%%%%%
\usepackage{multirow}
\usepackage{graphics}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath,mathtools}
\usepackage{bm}
\usepackage{amsfonts} 
\usepackage{enumitem}
\setlist[itemize]{noitemsep, topsep=0pt}
\usepackage{comment}
\def\todo#1{\textcolor{blue}{[todo: #1]}}
\DeclareMathOperator*{\argmax}{argmax}
\usepackage[ruled]{algorithm2e}
\SetKwRepeat{Do}{do}{while}%
 \SetKwInOut{Input}{Input}
 \SetKwInOut{Output}{Output}
\DeclareOldFontCommand{\rm}{\normalfont\rmfamily}{\mathrm}


%underline
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}

\usepackage{array}

%model name
\newcommand\ttsmall[1]{\texttt{\textmd {#1}}}

%ours
\newcommand\ours{\textsc{Uprise}}
%%%%%%%
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{10cm}
%
% and set <dim> to something 5cm or larger.

\title{\textsc{Uprise}: Universal Prompt Retrieval for Improving Zero-Shot Evaluation}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu\\ {\bf Yujing Wang, Hao Sun, Furu Wei, Denvy Deng, Qi Zhang}  \\
         Microsoft Corporation \\
         \small{daixuancheng6@gmail.com} ~~~~~~~~~\small{bijunyu21@mails.ucas.ac.cn}\\
         \small{{\{shaohanh, yuefzh, jianfengliu, yujing.wang, hasun, fuwei, dedeng, qizhang\}@microsoft.com}}
         }
\begin{document}
\maketitle
\begin{abstract}
Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose \ours~(\textbf{U}niversal \textbf{P}rompt \textbf{R}etrieval for \textbf{I}mproving zero-\textbf{S}hot \textbf{E}valuation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on a diverse set of tasks, but tested on unseen task types; we use a small frozen LLM, \ttsmall{GPT-Neo-2.7B}, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as \ttsmall{BLOOM-7.1B}, \ttsmall{OPT-66B} and \ttsmall{GPT3-175B}. Additionally, we show that \ours~mitigates the hallucination problem in our experiments with \ttsmall{ChatGPT}, suggesting its potential to improve even the strongest LLMs.

\end{abstract}

\section{Introduction}
Large Language Models (LLMs) such as \ttsmall{GPT-3}~\cite{ICL}, \ttsmall{OPT}~\cite{OPT}, and \ttsmall{BLOOM}~\cite{BLOOM} have shown impressive capabilities across a wide range of tasks. Recent research proposes two main approaches to further improve their performance: fine-tuning LLMs to follow prompts~\cite{LoRA, Adapter, BitFit, FLAN, T0} and developing prompt engineering techniques to guide the LLMs~\cite{ICL, CoT, p-tuning, google_prompt_tuning}.

Fine-tuning LLMs adjusts their weights to fit specific prompts and improve task performance. However, this may be limited by computational resources and unavailable model weights~\cite{LoRA}. Multi-task tuning provides an alternative approach to improve zero-shot task generalization~\cite{FLAN,T0}, which partially justifies the tuning cost. Yet, the constant evolution of LLMs creates a need for tuning new models, which makes the the cumulative fine-tuning cost a big concern.

\begin{figure}[!t]
\centerline{\includegraphics[width=\columnwidth]{intro_fig.pdf}}
\caption{\ours~tunes a prompt retriever on multiple tasks with a small frozen LLM, but conducts inference on unseen task types with a different larger LLM.}
\label{fig: intro}
\end{figure}

Prompt engineering constructs prompts to guide frozen LLMs. Prompt design adds an engineered natural language prompt to the task input, to teach the LLM to learn in context~\cite{ICL} or induce the LLM to reason~\cite{CoT}. Prompt tuning adds a soft prompt represented by continuous parameters, and optimizes it through gradient propagation~\cite{p-tuning,Prefix-Tuning,google_prompt_tuning}. While these methods can achieve excellent performance for specific tasks, it is uncertain whether the prompts designed for one task can generalize to unseen task types, as prompt designers are blind in strict zero-shot settings~\cite{dont_prompt}.

In this paper, we propose \ours~(\textbf{U}niversal \textbf{P}rompt \textbf{R}etrieval for \textbf{I}mproving Zero-\textbf{S}hot \textbf{E}valuation), which tunes a lightweight and versatile retriever that automatically retrieves prompts from a pre-constructed pool of data, given a zero-shot task input. As illustrated in Figure~\ref{fig: intro}, the retriever is trained to retrieve prompts for multiple tasks, enabling it to generalize to unseen task types during inference. In addition, we demonstrate that the cross-task capabilities can generalize well from a small LLM to different LLMs of much larger scales: we use \ttsmall{GPT-Neo-2.7B}~\cite{gpt-neo} to guide the tuning of the retriever and evaluate the retriever's performance on \ttsmall{BLOOM-7.1B}~\cite{BLOOM}, \ttsmall{OPT-66B}~\cite{OPT}, and \ttsmall{GPT3-175B}~\cite{ICL}. The cross-model and cross-task generalization of \ours~makes it a promising and practical solution for real-world applications.

Furthermore, our approach demonstrates the potential for enhancing even the most powerful LLMs, as shown in our experiments with \ttsmall{ChatGPT}. Despite its impressive abilities, \ttsmall{ChatGPT} has been found to struggle with serious hallucination problems, leading to responses that are factually inaccurate~\cite{ChatGPT_eval}. However, \ours~is able to address this issue on fact-checking tasks by prompting the model to draw correct inferences from its built-in knowledge.

In summary, our contributions include:
\begin{itemize}[leftmargin=*]
\itemsep0em 
\item We introduce \ours, a light-weight and versatile approach to improve zero-shot performance of LLMs in the cross-task and cross-model scenarios.
\item \ours~is tuned with \ttsmall{GPT-Neo-2.7B}, but can also benefit different LLMs of much larger scales, such as \ttsmall{BLOOM-7.1B}, \ttsmall{OPT-66B} and \ttsmall{GPT3-175B}.
\item Our exploration on \ttsmall{ChatGPT} demonstrates the potential of \ours~in improving performance of even the strongest LLMs.
\end{itemize}


\section{Problem Definition}\label{sec: Problem Definition}
We aim to improve zero-shot performance of LLMs by training a prompt retriever to retrieve prompts for any given task input. Specifically, \ours~decomposes the prompting process into two steps: \emph{retrieve} then \emph{predict}. Given an input $x$, we first retrieve a set of positive prompts $\mathcal{P}^{+}$ from a pre-constructed pool $\mathcal{P}$:
\begin{equation}
\mathcal{P}^{+}=\mathcal{R}(x,\mathcal{P}).
\end{equation}
Then we concatenate $\mathcal{P}^{+}$ with $x$ to form an input sequence for a frozen LLM, which generates a predicted output:
\begin{equation}
y^{\mathcal{P}^{+}}=\mathrm{LM}\left(y^{\mathcal{P}^{+}}|\mathcal{P}^{+} \oplus x\right).
\end{equation}
Our objective is to optimize performance of $y^{\mathcal{P}^{+}}$ to match the target $y$ by updating the retriever $\mathcal{R}$.

Figure~\ref{fig: prompt engineering methods} compares prompt retrieval with typical prompt engineering methods: prompt design adds an engineered natural language prompt~\cite{ICL,CoT} and prompt tuning tunes a soft prompt~\cite{p-tuning, google_prompt_tuning}.
In contrast, prompt retrieval tunes a retriever to retrieve natural language prompts, which is both interpretable and flexible. It uses the language model itself to label each prompt in the prompt pool as positive/negative, and then tuning a retriever from this signal~\cite{EPR}. Such fine-tuned prompt retrieval has demonstrated effectiveness in tuning task-specific retrievers: EPR~\cite{EPR} and CEIL~\cite{CEIL} train a single prompt retriever for a specific task using the input and output pairs in the training set as the prompt pool. The retriever is then evaluated on the inputs of the corresponding test set.

Our work is to achieve universality of the prompt retriever, which means the fine-tuned retriever can be directly used to retrieve prompts for unseen tasks and various inference LLMs, without the need for further tuning. We define the universality from two perspectives: cross-task retrieval and cross-model retrieval. 

\begin{figure}[!t]
\centerline{\includegraphics[width=\columnwidth]{comparison.pdf}}
\caption{Typical prompt engineering methods and prompt retrieval. Prompt retrieval prepends a natural language prompt to the task input and uses a frozen LLM to evaluate the prompt's performance, which is then used to tune the retriever reversely.}
\label{fig: prompt engineering methods}
\end{figure}

\textbf{Cross-task retrieval.} In order to address the diversity of tasks in real-world applications, we propose cross-task retrieval to retrieve for task types on which the prompt retriever has not been trained. We simulate this setting by evaluating the prompt retriever on unseen task types~\cite{FLAN}: various tasks are grouped into different clusters based on their task types, and we hold out each task cluster for evaluation while training the retriever on all remaining clusters. 

\textbf{Cross-model retrieval.} Due to the high cost of tuning a prompt retriever with a large-scale LLM (e.g., larger than 5B), we propose to evaluate the generalization of small-to-large LLMs. Specifically, we use a relatively small LLM for tuning the retriever, while using a much larger LLM for inference. Furthermore, we suggest exploring cross-model source generalization, as there are LLMs developed by different companies or institutions.

\begin{figure*}[!htb]
\centerline{\includegraphics[width=\textwidth]{pipeline.pdf}}
\caption{Training and inference pipeline. In the training stage, a frozen LLM is used to supervise the tuning of a prompt retriever, where both the LLM and the retriever take the prompt-input pairs as input, and we use the task scores given by the LLM to supervise the contrastive learning of the retriever. In the inference stage, for each task input, the tuned prompt retriever retrieve positive prompt(s) to guide the inference model to predict a task output. Overall, we follow a cross-task and cross-model paradigm where the task types and LLMs for training could be different from those for inference.} 
\label{fig: pipeline}

\end{figure*}
\section{Method}
As shown in Figure~\ref{fig: pipeline}, \ours~uses a frozen LLM to supervise the fine-tuning of a prompt retriever on a diverse set of tasks, and then using this trained retriever to retrieve prompts for different task types during inference with different LLMs. In this section, we elaborate on our data construction, and training and inference pipeline.

\subsection{Data Construction}
\textbf{Task Data.} We use instruction templates from FLAN~\cite{FLAN} to convert task datasets into natural language instructions\footnote{We exclude templates that ``turn the task around'', such as asking a sentiment classification task to generate a movie review.}. Each task dataset corresponds to approximately seven templates. For each data example $\left(x_{i}, y_{i}\right)$, we randomly select one of the seven templates to convert $x_{i}$ into an input instruction and $y_{i}$ into a label completion. The option suffices and new line characters ``$\backslash n$'' are then automatically removed from the instructions, to make the instruction format more similar to that of the pre-training corpus, thus improving prompting performance~\cite{dont_prompt}. 

\textbf{Prompt pool.} For each testing cluster, the prompt pool to be retrieved is made of training demonstrations of the remaining task clusters (i.e., the clusters for training the retriever). This is inspired by in-context learning \cite{ICL}, which presents a few task demonstrations before the task input to improve model performance. Each demonstration is a concatenation of the input instruction and the label completion, separated by a text blank space. Our motivation is that even if the testing task type has no overlap with the training task types, the testing input may still benefit from similar question types, topics, or reasoning chains in the retrieved demonstrations.

\subsection{Prompt Scoring}\label{sec: task score}
For each training example $\left (x_{i}, y_{i} \right)$ in the training clusters, we collect a set of positive and negative prompts from the prompt pool $\mathcal{P}=\left \{p_{j} \right \}_{j=1}^{N_{\mathcal{P}}}$, where the positive prompt indicates that the frozen LLM achieves good task scores conditioned on the prompt-input pair. We use these positive and negative labels to supervise the contrastive learning of the retriever.

We categorize all tasks into two question types: text completion and multiple-choice, and use different methods to score the prompts for each training example.

\textbf{Text completion} is the question to do free-form completion. We calculate score of the prompt using the following equation:
\begin{equation}
\label{eq: completion task score}
\mathrm{score}\left(p_{j}, x_{i}\right) = \mathrm{metric}\left(y_{i}, y_{i}^{p_{j}}\right),
\end{equation}
where $y_{i}^{p_{j}} = \mathrm{LM}\left(y_{i}^{p_{j}}|p_{j} \oplus x_{i}\right)$ is the model prediction based on the input concatenation $p_{j} \oplus x_{i}$, and $\oplus$ is a text delimiter ``$\backslash n$''. $\mathrm{metric}\left(\cdot\right)$ is the function used to calculate the task metric score (e.g., F1 or Exact Match).

\textbf{Multiple choice} is the question to choose one correct completion from several options. Suppose there are $M$ options in a multiple choice task $\left (x_{i},y_{i},\left \{o_{m} \right \}_{m=1}^{M} \right )$, where $\left \{o_{m} \right \}_{m=1}^{M} $ is the option set and $o_{y_{i}}$ denotes the gold option. We feed the concatenation $p_{j} \oplus x_{i}$ to the LLM and calculate per-token likelihood of each option: $\mathrm{LH}\left(o_{m}\right)$. The option with the highest likelihood is considered as the model prediction $y_{i}^{p_{j}}$.

Accuracy of the prediction $\mathrm{acc}\left(y_{i}, y_{i}^{p_{j}}\right)$ could evaluate the prompt, but it only produces binary results which makes it hard to compare prompt effectiveness. To address this, we calculate the final score using the equation:
\begin{equation}
\label{eq: multiple choice task score}
\mathrm{score}\left(p_{j}, x_{i} \right )=\mathrm{acc}\left(y_{i}, y_{i}^{p_{j}}\right)\cdot\frac{\mathrm{LH}\left(o_{y_{i}} \right )}{\sum_{m=1}^{M}\mathrm{LH}\left (o_{m}\right)},
\end{equation}
where we multiply the accuracy score with the normalized likelihood of the gold option, to achieve a fine-grained comparison.

\textbf{Prompt filtering.} Intuitively, to collect the positive and negative prompts for each training example, we need to score every prompt in the prompt pool and identify the prompt that yields the best score as the positive prompt. Conversely, prompts that lead to the worst scores are labeled as negative prompts. However, scoring all the prompts can be computationally expensive, even with a relatively small LLM. To address this, we design a prompt filtering mechanism to reduce the number of prompts that need to be scored.

Inspired by one-shot in-context learning~\cite{ICL}, we observe that a single training demonstration, even if randomly sampled, can improve the evaluation performance of the testing sample, since the training demonstration and the testing sample belong to the same task, where they could share common patterns or structures. Therefore, we randomly sample a small subset $L$ of training demonstrations from the prompt pool for each training example $\left(x_{i}, y_{i} \right)$. Specifically, we select only those demonstrations that belong to the same task as $\left(x_{i}, y_{i} \right)$. This significantly reduces the number of prompts to be scored while still increasing the likelihood of identifying positive prompts in the sampled subset.

Furthermore, in the case of difficult questions, all $L$ prompt-input pairs may result in a score of $0$. To address this, we repeat the sampling process to score another subset of $L$ prompts with the same task as $\left(x_{i}, y_{i} \right)$ until we find at least one prompt with a score greater than $0$.

For all the scored prompts for a training example, we label the prompt with the highest score as positive. For negative samples, we randomly sample $B$ training demonstrations from the prompt pool, with different tasks than $\left(x_{i}, y_{i} \right)$. In addition, we label $B$ demonstrations corresponding to the lowest $B$ scores in the sampled prompts ($B<L$) as hard negatives, which are of the same task with $\left(x_{i}, y_{i} \right)$ but are less effective.

\subsection{Retriever Tuning}
After labeling the positive and (hard) negative prompts for each training example $\left(x_{i}, y_{i} \right)$, we split the collected data into two sets: 90\% for training and 10\% for validation. The prompt retriever is a bi-encoder model where the input encoder $E_X(\cdot)$ takes the task input $x_{i}$ as input, and the prompt encoder $E_P(\cdot)$ takes prompt $p_{j}$ as input. 

To train the prompt retriever, InfoNCE loss~\cite{InfoNCE} is used to maximize the similarity score between the encoded prompt and input for positive prompt-input pairs, and minimize it for (hard) negative prompt-input pairs. For a single training example $\left(x_{i}, y_{i} \right)$, the loss function for its corresponding positive and negative prompts is defined as follows~\cite{EPR}:
\begin{eqnarray}
&& L(x_i, p^+_{i}, p^-_{i,1},\dots p^-_{i,2B}) \label{eq:training} \\
&=& -\log \frac{ e^{\mathrm{sim}(x_i, p_i^+)} } {e^{\mathrm{sim}(x_{i}, p_{i}^+)}+\sum_{j=1}^{2B}{e^{\mathrm{sim}(x_{i}, p_{i,j}^-)} }  }, \nonumber
\end{eqnarray}
where $p^+_{i}$ is the positive prompt, and $p^-_{i,j}$ is one of the (hard) negative prompts, and $\mathrm{sim}(x_{i}, p)  =  E_X(x_{i})^{\top}E_P(p)$ calculates the similarity score between input $x_{i}$ and prompt $p$ using inner products.

\subsection{Inference}\label{Inference}
After fine-tuning the prompt encoder, we use it to encode the entire prompt pool with $E_P(\cdot)$. At inference time, for a testing input instruction $x_{\text{test}}$, we compute its encoding $E_X(x_{\text{test}})$ and then use maximum inner-product search over the prompt pool to retrieve $K$ most similar prompts, sorted by their inner product in descending order, denoted as $\mathcal{P}^{+}=(p_1, ..., p_K)$. We then concatenate the prompts with the task input, resulting in the concatenation $p_K\oplus ... \oplus p_1 \oplus x_{\text{test}}$. 

To evaluate the inference results, we use the same method described in Section~\ref{sec: task score} to generate model predictions, and then use each task's corresponding evaluation metric to compute the scores.

\section{Experiment Settings}\label{sec: Experiment Settings}

\textbf{Task clustering.} We group the tasks used in our method into clusters, including Reading Comprehension, Closed-book QA, Paraphrase Detection, Natural Language Inference, Sentiment Analysis, Commonsense Reasoning, Coreference Resolution, Structure to Text, and Summarization. Additional information about the datasets in each cluster can be found in Appendix~\ref{appendix: task clustering}.

\textbf{Data sampling.} To prevent the retriever tuning from being dominated by large datasets, we randomly sample up to $10k$ data examples from each task's training set, while also maintaining class balance in classification tasks\footnote{For instance, in a four-classification task, we sample a maximum of $2.5k$ data examples from each class.}. The prompt pool consists only of the sampled training data. On average, for each testing task cluster, there are approximately $180k$ training examples sampled from the remaining clusters.

\textbf{LLMs.} We use \ttsmall{GPT-Neo-2.7B}~\cite{gpt-neo} from EleutherAI to tune the retriever, and evaluate the performance on larger LLMs from various sources in the inference stage, including \ttsmall{BLOOM-7.1B}~\cite{BLOOM} from BigScience, \ttsmall{OPT-66B}~\cite{OPT} from Meta, and \ttsmall{Davinci} and \ttsmall{text-davinci-001} from OpenAI, both belonging to the \ttsmall{GPT3-175B}~\cite{ICL} series. We use greedy search to obtain predictions from all the LLMs.

\textbf{Prompt scoring.} We set the size of the randomly sampled subset to $L=50$ and the number of (hard) negatives to $B=20$. For difficult questions, we repeat the re-sampling process up to seven rounds, as we found that this is sufficient to identify a positive prompt for 90\% of the training examples. If no sampled prompt yields a score greater than $0$, we filter out the corresponding training example.

\textbf{Tuning.} We initialize both encoders of the retriever with ${\rm BERT}_{\rm BASE}$~\cite{bert}. Each retriever is fine-tuned for three epochs, and the best checkpoint is chosen based on retrieval accuracy using the validation set. For detailed tuning hyperparameters, please refer to Appendix~\ref{appendix: Tuning Details}. 

\textbf{Inference.} During inference, we set the number $K$ of concatenated prompts to a relatively small value of $3$, to balance between prompting performance and inference efficiency. For each dataset in the testing cluster, We report metric scores on the test set when available, falling back to the validation set otherwise.


\begin{table*}[!htb]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{llcccccccccc}
\toprule
 \textbf{Task}              &  \small \textbf{Metric}   & \multicolumn{2}{c}{\small \textbf{GPT-Neo-2.7B}} &  \multicolumn{2}{c}{\small \textbf{BLOOM-7.1B}} &  \multicolumn{2}{c}{ \small \textbf{OPT-66B}}   &  \multicolumn{2}{c}{\small \textbf{Davinci}} &  \multicolumn{2}{c}{\small \textbf{Davinci-001}} \\
                  \cmidrule(r){3-4} \cmidrule(r){5-6} \cmidrule(r){7-8} \cmidrule(r){9-10} \cmidrule(r){11-12} 
                  &          &  \small \textsc{0-shot}          & \small \ours           & \small \textsc{0-shot}     &  \small \ours              &  \small \textsc{0-shot}        &  \small \ours           &  \small \textsc{0-shot}         &  \small \ours           & \small \textsc{0-shot}           & 
\small \ours     \\ \midrule
\multicolumn{12}{l}{\hspace{-0.22cm} {\ul \textit{Reading Comprehension}}}                                                                                                                      \\\vspace{0.09cm}
\multirow{2}{*}{SQuADv1} & F1       & \large 4.4             & 26.4           & 4.5        & 5.5               & 6.1           & 7.5           & 6.5            & 6.0           & 41.6             & 57.7            \\
                  & EM       & 0.4             & 14.3           & 0.0        & 0.0               & 0.0           & 0.6           & 0.0            & 0.0           & 16.4             & 36.8            \\\vspace{0.09cm}
BoolQ             & Acc & 54.5            & 59.4           & 54.0       & 60.2              & 60.7          & 63.5          & 62.0           & 65.7          & 64.2             & 65.7           \\\vspace{0.09cm}
MultiRC           & F1       &57.1            & 58.1           & 58.8       & 59.8              & 59.6          & 60.4          & 59.8           & 60.0          & 54.3             & 58.9      \\\vspace{0.09cm}
OBQA              & Acc & 41.8            & 42.2           & 44.0       & 41.8              & 46.4          & 48.8          & 49.2           & 52.4          & 52.8             & 48.8            \\\vspace{0.09cm}
\textbf{Average} &        & 31.6            & \textbf{40.1}  & 32.3       & \textbf{33.5}     & 34.6          & \textbf{36.2} & 35.5           & \textbf{36.8} & 45.9             & \textbf{53.6}    \\ \midrule 
\multicolumn{12}{l}{ \hspace{-0.22cm}{\ul \textit{Closed-book QA}} }                                                                                                                            \\\vspace{0.09cm}
ARC-e                 & Acc & 45.7            & 55.6           & 53.7       & 60.9              & 56.2          & 66.0          & 64.1           & 71.8          & 67.0             & 74.4            \\\vspace{0.09cm}
ARC-c            & Acc & 29.3            & 30.0           & 33.2       & 34.2              & 36.7          & 40.2          & 40.8           & 45.2          & 46.2             & 50.4           \\\vspace{0.09cm}
\multirow{2}{*}{NQ} & F1       & 1.3             & 5.6            & 0.9        & 1.4               & 2.5           & 2.1           & 0.0            & 2.2           & 18.3             & 18.2            \\
                  & EM       & 0.5             & 2.2            & 0.0        & 0.1               & 0.3           & 0.4           & 0.0            & 0.0           & 4.8              & 8.7              \\\vspace{0.09cm}
\textbf{Average} &        & 19.2            & \textbf{23.3}  & 22.0       & \textbf{24.2}     & 23.9          & \textbf{27.2} & 26.2           & \textbf{29.8} & 34.1             & \textbf{37.9}  \\  \midrule
\multicolumn{12}{l}{\hspace{-0.22cm}{\ul \textit{Paraphrase Detection}}}                                                                                                                      \\\vspace{0.09cm}
\multirow{2}{*}{MRPC}              & Acc & 46.6            & 67.9           & 51.0       & 70.6              & 51.0          & 68.9          & 54.4           & 62.3          & 40.0             & 61.3            \\\vspace{0.09cm}
                  & F1       & 46.0            & 80.4           & 58.0       & 82.1              & 57.8          & 81.5          & 68.9           & 81.4          & 39.2             & 72.9            \\\vspace{0.09cm}
\multirow{2}{*}{QQP}               & Acc & 48.4            & 54.3           & 49.5       & 53.1              & 50.5          & 49.7          & 55.2           & 52.4          & 60.9             & 62.6             \\\vspace{0.09cm}
                  & F1       &  42.2            & 59.8           & 46.7       & 59.6              & 43.7          & 58.5          & 33.7           & 57.9          & 43.0             & 45.9            \\\vspace{0.09cm}
PAWS         & Acc & 51.7            & 45.7           & 50.8       & 45.9              & 50.5          & 44.4          & 52.4           & 44.5          & 53.2             & 52.3           \\\vspace{0.09cm}
\textbf{Average} &        & 47.0            & \textbf{61.6}  & 51.2       & \textbf{62.3}     & 50.7          & \textbf{60.6} & 52.9           & \textbf{59.7} & 47.3             & \textbf{59.0}   \\\midrule
\multicolumn{12}{l}{\hspace{-0.22cm} {\ul \textit{Natural Language Inference}}}                                                                                                                \\ \vspace{0.09cm}
MNLI-m            & Acc & 35.3            & 41.3           & 35.4       & 36.0              & 37.0          & 40.4          & 34.2           & 38.2          & 44.7             & 41.1            \\\vspace{0.09cm}
MNLI-mm           & Acc & 36.4            & 43.1           & 34.9       & 35.8              & 37.1          & 41.2          & 34.2           & 38.6          & 46.5             & 42.1            \\\vspace{0.09cm}
QNLI              & Acc & 50.9            & 53.8           & 49.9       & 51.3              & 54.2          & 53.7          & 51.7           & 51.1          & 60.0             & 58.4             \\\vspace{0.09cm}
SNLI              & Acc & 35.2            & 42.3           & 35.2       & 34.4              & 34.5          & 40.2          & 33.5           & 37.9          & 47.5             & 42.0            \\\vspace{0.09cm}
RTE               & Acc & 33.6            & 34.7           & 50.5       & 49.8              & 52.3          & 46.9          & 51.3           & 45.5          & 52.3             & 50.9            \\ \vspace{0.09cm}
\textbf{Average} &        & 38.3            & \textbf{43.0}  & 41.2       & \textbf{41.5}     & 43.0          & \textbf{44.5} & 41.0           & \textbf{42.3} & \textbf{50.2}    & 46.9           \\ \midrule
\multicolumn{12}{l}{\hspace{-0.22cm}{\ul \textit{Sentiment Analysis}}}                                                                                                                         \\\vspace{0.09cm}
SST-2                    & Acc & 52.4            & 56.2           & 63.2       & 69.1              & 57.9          & 65.3          & 52.3           & 64.3          & 90.5             & 90.5            \\\vspace{0.09cm}
Yelp                     & Acc & 71.7            & 67.8           & 56.1       & 58.0              & 67.6          & 63.5          & 59.8           & 65.3          & 80.3             & 80.2            \\\vspace{0.09cm}
Sent140             & Acc & 64.1            & 61.3           & 74.5       & 72.1              & 59.1          & 61.6          & 64.3           & 72.1          & 87.2             & 89.1            \\\vspace{0.09cm}
\textbf{Average} &        & \textbf{62.7}   & 61.8           & 64.6       & \textbf{66.4}     & 61.5          & \textbf{63.5} & 58.8           & \textbf{67.3} & 86.0    & \textbf{86.6}  \\\midrule
\multicolumn{12}{l}{ \hspace{-0.22cm}{\ul \textit{Commonsense Reasoning}} }                                                                                                                            \\\vspace{0.09cm}
PiQA                       & Acc & 70.2          & 70.4          & 71.5          & 72.1          & 76.5          & 80.4          & 79.1          & 81.3          & 79.1          & 79.1 \\\vspace{0.09cm}
COPA                       & Acc & 67.0          & 64.0          & 67.0          & 67.0          & 74.0          & 76.0          & 80.0          & 83.0          & 83.0          & 80.0 \\\vspace{0.09cm}
HellaSwag                  & Acc & 54.4          & 52.1          & 59.6          & 58.8          & 72.9          & 71.4          & 76.9          & 76.7          & 77.6          & 78.2 \\\vspace{0.09cm}
\textbf{Average}           &          & \textbf{63.9} & 62.2          & \textbf{66.0} & \textbf{66.0} & 74.5          & \textbf{75.9} & 78.7          & \textbf{80.3} & \textbf{79.9} & 79.1 \\\midrule
\multicolumn{12}{l}{ \hspace{-0.22cm} {\ul \textit{Coreference Resolution}} }                                                                                                                            \\\vspace{0.09cm}
WSC273                     & Acc & 73.6          & 76.6          & 78.0          & 81.0          & 83.9          & 86.1          & 60.6          & 50.0          & 78.8          & 75.5 \\\vspace{0.09cm}
DPR                        & Acc & 59.6          & 51.0          & 64.4          & 55.8          & 66.3          & 50.0          & 82.1          & 83.9          & 64.4          & 58.7 \\\vspace{0.09cm}
Winogrande                 & Acc & 58.9          & 58.6          & 65.9          & 64.3          & 69.2          & 67.8          & 68.6          & 70.2          & 66.3          & 64.7 \\\vspace{0.09cm}
\textbf{Average}           &          & \textbf{64.0} & 62.1          & \textbf{69.4} & 67.0          & \textbf{73.1} & 68.0          & \textbf{70.4} & 68.0          & \textbf{69.8} & 66.3 \\\bottomrule
\end{tabular}%
}
\caption{Zero-shot performance across tasks and LLMs. The model Davinci-001 is the fine-tuned version \ttsmall{text-davinci-001} of \ttsmall{Davinci}. The method \textsc{0-shot} is the vanilla zero-shot method with only the input instruction fed into the LLM.}
\label{tab: main results}
\end{table*}

\section{Main Results}
We evaluate our prompt retriever on natural language understanding tasks where generative LLMs are known to need improvement~\cite{p-tuning}. Table~\ref{tab: main results} compares the performance of \ours~to vanilla zero-shot prompting.

\subsection{Cross-Task Prompt Retrieval}
Based on the results in the \ttsmall{GPT-Neo-2.7B} column, we can assess our ability of generalizing across different task types. \ours~has positive impacts on most of the testing clusters. Specifically, we achieve performance gains of 8.5\% and 14.6\% absolute points in reading comprehension and paraphrase detection tasks, respectively, when compared to the zero-shot baseline. We also find that \ours~shows consistent performance improvements across all tasks in closed-book QA and natural language inference. 

However, \ours~has negative impacts on tasks in commonsense reasoning and coreference resolution clusters. This aligns with the findings reported by~\citet{FLAN}: instruction templates may not be effective for tasks that are directly formulated as language modeling problems. Alternative techniques such as chain-of-thought prompting~\cite{CoT} may be more effective.

In Table~\ref{tab: case-study reading comprehension}-\ref{tab: case-study Sentiment Analysis} in Appendix, we show two cases of each testing cluster, and analyze the relevance between the retrieved prompts and task input in the caption. We observe the cross-task improvement benefits from similar question types, topics, text formats, or logical relationships.

\begin{figure}[!tb]
\centerline{\includegraphics[width=\columnwidth]{cross-model_res.pdf}}
\caption{cross-model results of the cross-task retriever.} 
\label{cross-model res}
\end{figure}
\subsection{Cross-Model Prompt Retrieval}
In addition to evaluating cross-task generalization, we can explore the cross-model ability by examining the results of \ttsmall{BLOOM}, \ttsmall{OPT},  \ttsmall{Davinci} and \ttsmall{text-davinci-001}. \ours~continues to improve performance on reading comprehension, closed-book QA, and paraphrase detection tasks across all LLMs. While the performance on sentiment analysis is negative with the small 2.7B \ttsmall{GPT-Neo}, we observe positive impacts when using larger models. We achieve consistent gains on natural language inference tasks with the models that have not been fine-tuned (\ttsmall{BLOOM}, \ttsmall{OPT}, and \ttsmall{Davinci}), 
but experience a drop in performance on \ttsmall{text-davinci-001}, which could potentially be due to the model being specifically fine-tuned on such tasks to improve performance

Generally, we presenting the average performance of reading comprehension, closed-book QA, paraphrase detection, natural language inference, and sentiment analysis in Figure~\ref{cross-model res}. The results indicate consistent performance gains across all LLMs.

\section{Hallucination Mitigation of ChatGPT}
Despite the impressive abilities of \ttsmall{ChatGPT} in many areas, recent reports have shown that it suffers from a significant issue known as hallucination: providing factually incorrect responses to users~\cite{ChatGPT_eval}.

To assess the versatility of our prompt retriever, we also investigate whether \ours~can mitigate the hallucination problem. We evaluate on three tasks: TruthfulQA~\cite{TruthfulQA} for detecting human falsehood, FEVER2.0~\cite{FEVER} and the scientific spilt of Covid-19~\cite{Covid19} for fact-checking.
\begin{table}[!tb]
\centering
\resizebox{0.95\columnwidth}{!}{%
\begin{tabular}{lccc}
\toprule
\textbf{Method~~~~}         & TruthfulQA & FEVER2.0 & Covid-19 \\ \midrule
\textsc{0-shot}             & \textbf{86.3}       & 51.0        & 47.0  \\
\textsc{\ours}             & \textbf{86.3}     & \textbf{56.0}       & \textbf{83.0}        \\ \bottomrule 
\end{tabular}%
}
\caption{Comparative results of \ttsmall{ChatGPT} on hallucination detection tasks.}
\label{tab: ChatGPT}
\end{table}

\begin{figure}[!tb]
\centerline{\includegraphics[width=\columnwidth]{chatgpt_case_fever.pdf}}
\caption{Case of the chats of vanilla zero-shot prompting and \ours~on the FEVER2.0 dataset, the label completion is ``True''.} 
\label{chatgpt_case_fever}
\end{figure}

\begin{figure*}[!htb]
\centerline{\includegraphics[width=0.95\textwidth]{cross-task_results.pdf}}
\caption{Comparison of different universal retrievers, we report the average performance on each testing cluster.} 
\label{cross-task_results}
\end{figure*}
Table~\ref{tab: ChatGPT} shows that \ours~outperforms vanilla zero-shot prompting in the two fact-checking tasks. Figure~\ref{chatgpt_case_fever} presents an interesting case where zero-shot prompting results in a correct generation of information (``Sonny \& Cher... consisting of Sonny Bono and his wife Cher.''), but an incorrect answer. In contrast, \ours~successfully induces a precise answer. We attribute this improvement to the retrieved demonstration, which is of the natural language inference task type that may motivate the model to correctly infer from its parametric memory. This finding suggests that the limited memory\footnote{``limited memory'' means that \ttsmall{ChatGPT} does not have access to external knowledge bases.} of \ttsmall{ChatGPT} may not be the only factor leading to the hallucination challenge. Rather, it highlights the importance of having effective inference mechanisms. Prompt engineering techniques such as \ours~can help address this issue. For further evaluation details and analysis, please refer to Appendix~\ref{appendix: ChatGPT}.

\section{Ablation Study}
\subsection{Universal Prompt Retriever}
We replace the universal retriever with three alternatives: 1) \textsc{Random} randomly samples prompts from the prompt pool, and reports the score averaged on five random seeds, 2) \textsc{TopK-BM25} uses the sparse retriever BM25~\cite{BM25} to retrieve prompts similar to the testing input, and 3) \textsc{TopK-BERT} follows KATE~\cite{KATE} to use SBERT~\cite{SBERT} to retrieve similar prompts.

Figure~\ref{cross-task_results} displays the comparative performance using \ttsmall{GPT-Neo-2.7B}, where \ours~achieves the best results among all the universal retrievers. This suggests that word-level (\textsc{TopK-BM25}) or sentence-level (\textsc{TopK-BERT}) similarity to the testing input is not the only decisive factor for a good prompt. This finding underscores the effectiveness of finetuning a retriever with the language model itself as a data labeler.

We also believe that the diversity of the instruction templates inherited from FLAN~\cite{FLAN} contributes to our good cross-task transferability. By leveraging this diverse prompts, we are able to tune a light-weight retriever instead of a large language model, which allows us to follow zero-shot instructions more efficiently.

\subsection{Universal Prompt Pool}
For each testing task cluster, we use training demonstrations of the remaining clusters to construct the prompt pool. To evaluate its effectiveness, we replace it with the raw texts of wikitext-103~\cite{wikitext}, which belongs to the pre-training corpus of many LLMs. The results in Table~\ref{tab: prompt pool} show the effectiveness of our prompt pool, which outperforms the raw texts on all the testing clusters. One possible reason is that even though we follow the cross-task type paradigm, there still exist some training demonstrations that are beneficial for the testing input.

\begin{table}[!tb]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
\textbf{Prompt Pool}                & Read. & Closed. & Para. & NLI  & Senti. \\ \midrule
\textsc{Raw Text} & 32.0     & 19.3       & 44.7       & 37.5 & 60.3      \\
\ours            & \textbf{40.1}     & \textbf{23.4}       & \textbf{61.6}       & \textbf{43.0} & \textbf{61.8}    \\ \bottomrule 
\end{tabular}%
}
\caption{Comparison of average performance on \ttsmall{GPT-Neo-2.7B} with different prompt pool: \textsc{Raw Text} uses raw data of the pre-training corpus, \ours~uses training demonstrations of the trained tasks.}
\label{tab: prompt pool}
\end{table}

\begin{table}[!tb]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
\textbf{Method~~~~~~}        & Read. & Closed. & Para. & NLI  & Senti. \\ \midrule
\textsc{0-shot} & 31.6     & 19.2       & 47.0       & 38.3 &  \textbf{62.7}      \\
\textsc{Random}             & \textbf{33.7}     & \textbf{21.0}       & \textbf{51.9}       & \textbf{38.9} & 61.2    \\ \bottomrule 
\end{tabular}%
}
\caption{Comparative results on \ttsmall{GPT-Neo-2.7B} of vanilla zero-shot prompting (the first row) and augmenting zero-shot prompting with randomly sampled prompts of our prompt pool (the second row).}
\label{tab: random prompt pool}
\end{table}
As shown in Table~\ref{tab: random prompt pool}, we compare the results of vanilla zero-shot prompting and prompting augmented with randomly sampled prompts from our prompt pool, where we see that even the randomly sampled prompts could improve the performance on four of the five clusters. This suggests that presenting some input-output demonstrations, even if the tasks of the prompts are different from those for testing, might be able to induce the large model to think and reason~\cite{CoT}, leading to inferring the correct answers.

%\section{Case study}\label{Case Study}



\section{Related Work}
Our work is related to prompt engineering works including prompt design, prompt tuning, and prompt search.

\textbf{Prompt Design} In-context Learning~\cite{ICL} is a method that helps LLMs transfer to new tasks via inference alone by conditioning a concatenation of demonstrations and test input, without any gradient updates. The demonstrations consist of $k$ text samples. A in-context learning method is classified as zero-shot when $k=0$ and few-shot when $k>0$.

With standard in-context learning, LLMs struggle to tackle complex arithmetic, commonsense, and symbolic reasoning tasks. Chain-of-Thoughts (CoT)~\cite{CoT} proposes providing LLMs with a series of intermediate reasoning steps as demonstrations to induce LLMs to produce another series of intermediate reasoning steps that lead to the final answer. 

\textbf{Prompt Tuning.} Traditional natural language prompts require significant human engineering and can lead to suboptimal performance. Prompt tuning proposes to learn a prompt represented by continuous parameters rather than discrete natural language tokens~\cite{p-tuning}. Prompt tuning takes the source text embedded by the LM input embeddings and prepends learnable embeddings to obtain a new embedded sequence. A variant of prompt tuning is prefix tuning~\cite{Prefix-Tuning,google_prompt_tuning}, where the learnable vectors are added not only to the input but to all transformer layers. The input-level method is referred to as shallow prompt tuning, and the layer-specific method is referred to as deep prompt tuning~\cite{prompt_tuning_survey}.

\textbf{Prompt Search.} Prompt search is another approach to avoid manual prompt design. It involves searching for prompts from pre-training corpora or downstream task datasets to construct a prompt text~\cite{LM-BFF,KATE,dont_prompt,CEIL}. To retrieve similar prompts for the test examples, retrievers such as the sparse retriever BM25~\cite{BM25} or the dense retriever based on SBERT~\cite{SBERT} are employed. Furthermore, methods like EPR~\cite{EPR} and CEIL~\cite{CEIL} use the LLM itself to score the generated or searched prompts, thereby eliminating the need for manual prompt engineering and ensuring prompting performance. 

\section{Conclusion}
In this paper, we propose \ours~, a lightweight and versatile approach to improve the zero-shot performance of different LLMs on various tasks. Specifically, we explore a cross-task and cross-model scenario, to evaluate the universality of the retriever to generalize from the trained task types to unseen task types, and from a small LLM to LLMs of much larger scales and different sources. Besides, the exploration on \ttsmall{ChatGPT} demonstrates our potential for improving even the strongest LLMs. Overall, \ours~provides a promising direction of develop a lightweight and versatile module to enhance the performance of different LLMs.

\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\clearpage

\section*{Appendices}
\section{Task Clustering}\label{appendix: task clustering}
We use the following datasets for each task cluster.
\begin{itemize}[leftmargin=*]
    \itemsep0em 
    \item \textbf{Reading Comprehension}: SQuADv1~\cite{SQuADv1}, BoolQ~\cite{BoolQ}, MultiRC~\cite{MultiRC}, and OBQA~\cite{OBQA}.
    \item \textbf{Closed-book QA}: ARC-c/e~\cite{ARC} and NQ~\cite{NQ}.
    \item \textbf{Paraphrase Detection}: MRPC~\cite{MRPC}, QQP~\cite{QQP}, and Paws Wiki~\cite{Paws}.
    \item \textbf{Natural Language Inference}: MNLI-m/mm~\cite{MNLI}, QNLI~\cite{QNLI}, SNLI~\cite{SNLI}, and RTE~\cite{RTE}.
    \item \textbf{Sentiment Analysis}: SST-2~\cite{SST-2}, Yelp~\cite{Yelp}, and Sentiment140~\cite{Sent140}.
    \item \textbf{Commonsense Reasoning}: COPA~\cite{COPA}, HellaSwag~\cite{HellaSwag}, and PIQA~\cite{PIQA}.
    \item \textbf{Coreferenece Resolution}: Winogrande~\cite{Winogrande}, DPR~\cite{DPR}, and WSC273~\cite{WSC273}.
    \item \textbf{Structure to Text}: CommonGen~\cite{CommonGen}, E2ENLG~\cite{E2ENLG}, and DART~\cite{DART}.
    \item \textbf{Summarization}: AESLC~\cite{AESLC}, AGNews~\cite{Yelp}, and Gigaword~\cite{Gigaword}.
\end{itemize}

\section{Tuning Details}\label{appendix: Tuning Details}
\begin{table}[!htb]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ll}
\bottomrule  
\textbf{Hyperparameter} & \textbf{Assignment}        \\ \bottomrule
Computing Infrastructure & 8 V100-32GB GPUs         \\
Number of epochs        & 3                          \\ 
Batch size per GPU      & 16                        \\ 
Maximum sequence length & 256                       \\ 
Maximum learning rate   & 1e-5                      \\ 
Optimizer               & Adam                       \\ 
Adam epsilon            & 1e-8                       \\ 
Adam beta weights       & 0.9, 0.999                  \\ 
Learning rate scheduler & warmup linear              \\ 
Weight decay            & 0.0                       \\ 
Warmup steps            & 1000                        \\ 
Learning rate decay     & linear                     \\ \bottomrule
\end{tabular}%
}
\caption{Hyperparameter settings of tuning a prompt retriever}
\label{tab: training parameters}
\end{table}

\section{Hallucination Mitigation of ChatGPT}\label{appendix: ChatGPT}
We evaluate \ttsmall{ChatGPT}'s performance using its released API, with the gpt-3.5-turbo-0301 model and a temperature of $0$ for greedy search. Human evaluation is conducted to check the accuracy on sampled test examples from each dataset, including $66$ from TruthfulQA to follow~\citet{ChatGPT_eval}, $100$ from FEVER2.0, and $100$ from the scientific subset of Covid-19. As types of these tasks have no overlap with the types we listed in Appendix~\ref{appendix: task clustering}, we use the retriever trained with all the listed task types for cross-task and cross-model evaluation.

On the Covid-19 dataset, which requires a true/false answer to input claims, we observe vanilla zero-shot prompting often leads to ambiguous answers, which we consider the answer incorrect. However, by concatenating retrieved prompts, primarily focused on natural language inference, \ttsmall{ChatGPT} generates the correct answer. An example is shown in Figure~\ref{fig: chatgpt_case_covid19}. This suggests the model has access to necessary information but struggles with making inferences, possibly due to RLHF~\cite{RLHF} training or inherent difficulty. 

\begin{figure}[!htb]
\centerline{\includegraphics[width=\columnwidth]{chatgpt_case_covid19.pdf}}
\caption{Case of the chats of vanilla zero-shot prompting and \ours~on Covid-19 dataset, the label completion is ``True''.} 
\label{fig: chatgpt_case_covid19}
\end{figure}

%reading comprehension
\begin{table*}[!htb]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l}
\toprule
{\ul \textsc{\textbf{Testing Cluster: Task}}}                             \\ 
Reading Comprehension: SQuADv1~\cite{SQuADv1}  \\                                                          
{\ul \textsc{\textbf{Input Instruction}}   }                                                                                                                                                                                  \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}Here is a question about this article: As of August 2010, Victoria had 1,548 public schools, 489 Catholic schools and 214 independent schools. Just under 540,800 students were enrolled in public schools, and just over 311,800 in private schools. Over 61 per cent of private students attend Catholic schools. More than 462,000 students were enrolled in primary schools and more than 390,000 in secondary schools. Retention rates for the final two years of secondary school were 77 per cent for public school students and 90 per cent for private school students. Victoria has about 63,519 full-time teachers. What is the answer to this question: What percentage of private school students go to Catholic schools? \end{tabular} \\ 
{\ul \textsc{\textbf{Label Completion}}}                                      \\
61                                                                                                    \\ \\
{\ul \textsc{\textbf{Prompt Cluster: Task}}}                             \\ 
Closed-book QA: Natural Questions~\cite{NQ}                         \\ 
{\ul \textsc{\textbf{Demonstration Input}}}                           \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}What is the answer to this question? what is the official poverty rate in the us?\end{tabular}                                                 \\ 
{\ul \textsc{\textbf{Demonstration Answer}}}                                     \\ 
In 2015, 13.5\%                                                   \\\bottomrule \\ 
\end{tabular}%

} 

\resizebox{\textwidth}{!}{%
\begin{tabular}{l}
\toprule
{\ul \textsc{\textbf{Testing Cluster: Task}}}                                                                                                                                                                                                                                                                 \\ 
Reading Comprehension: MultiRC~\cite{MultiRC}   \\     
{\ul \textsc{\textbf{Input Instruction}}   }                    \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}What causes a change in motion? The application of a force. Any time an object changes motion, a force has been applied. In what ways can this happen? Force can cause an object at rest to start moving. Forces can cause objects to speed up or slow down. Forces can cause a moving object to stop. Forces can also cause a change in direction. In short, forces cause changes in motion. The moving object may change its speed, its direction, or both. We know that changes in motion require a force. We know that the size of the force determines the change in motion. How much an objects motion changes when a force is applied depends on two things. It depends on the strength of the force. It also depends on the objects mass. Think about some simple tasks you may regularly do. You may pick up a baseball. This requires only a very small force. After reading the above, is ``No'' the correct answer to the question ``Would the mass of a baseball affect how much force you have to use to pick it up?''?,\end{tabular} \\ 
{\ul \textsc{\textbf{Label Completion}}}                                      \\
No                                                                                                   \\ \\
{\ul \textsc{\textbf{Prompt Cluster: Task}}}             \\ 
Natural Language Inference: QNLI~\cite{QNLI}               \\ 
{\ul \textsc{\textbf{Demonstration Input}}}                        \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}Q: What temperature are cask ales stored at before being tapped? A: Typically, when a cask arrives in a pub, it is placed horizontally on a frame called a ``stillage'' which is designed to hold it steady and at the right angle, and then allowed to cool to cellar temperature, before being tapped and vented tap is driven through a (usually rubber) bung at the bottom of one end, and a hard spile or other implement is used to open a hole in the side of the cask, which is now uppermost. Does the answer correctly answer the question?\end{tabular}                                                  \\ 
{\ul \textsc{\textbf{Demonstration Answer}}}                                    \\ 
Yes                                                                                                             \\ \bottomrule
\end{tabular}%

}
\caption{Examples of testing input and target of \textbf{Reading Comprehension} cluster, and the retrieved top-1 demonstration from the remaining clusters. The first example involves statistical questions in both the testing input and prompt, while the second example requires a binary "Yes" or "No" answer in both the input and prompt.}
\label{tab: case-study reading comprehension}
\end{table*}

%close qa
\begin{table*}[!htb]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l}
\toprule
{\ul \textsc{\textbf{Testing Cluster: Task}}}                             \\ 
Closed-book QA: ARC~\cite{ARC}  \\                                                          
{\ul \textsc{\textbf{Input Instruction}}   }                                                                                                                                                                                  \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}Which statement best explains why photosynthesis is the foundation of most food webs? Pick the answer from these options.\end{tabular} \\ 
{\ul \textsc{\textbf{Label Completion}}}                                      \\
Sunlight is the source of energy for nearly all ecosystems.                                                                                                    \\ \\
{\ul \textsc{\textbf{Prompt Cluster: Task}}}                             \\ 
Reading Comprehension: OBQA~\cite{OBQA}                         \\ 
{\ul \textsc{\textbf{Demonstration Input}}}                           \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}roots are a vehicle for absorbing water and nutrients from soil into the plant Which of the following is likely to reject nutrients from food?\end{tabular}                                                 \\ 
{\ul \textsc{\textbf{Demonstration Answer}}}                                     \\ 
Bamboo                                                  \\\bottomrule \\ 
\end{tabular}%

} 

\resizebox{\textwidth}{!}{%
\begin{tabular}{l}
\toprule
{\ul \textsc{\textbf{Testing Cluster: Task}}}              \\ 
Closed-book QA: Natural Questions~\cite{NQ}   \\     
{\ul \textsc{\textbf{Input Instruction}}   }                    \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}Q: When did Taylor Swift's first album release? A:\end{tabular} \\ 
{\ul \textsc{\textbf{Label Completion}}}                                      \\
October 24, 2006                                                                                                   \\ \\
{\ul \textsc{\textbf{Prompt Cluster: Task}}}             \\ 
Reading Comprehension: SQuADv1~\cite{SQuADv1}               \\ 
{\ul \textsc{\textbf{Demonstration Input}}}                        \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}Beyonc\'e In October 2014, Beyonc\'e signed a deal to launch an activewear line of clothing with British fashion retailer Topshop. The 50-50 venture is called Parkwood Topshop Athletic Ltd and is scheduled to launch its first dance, fitness and sports ranges in autumn 2015. The line will launch in April 2016. Q: When will the full line appear?\end{tabular}                                                  \\ 
{\ul \textsc{\textbf{Demonstration Answer}}}                                    \\ 
April 2016                                                                                                             \\ \bottomrule
\end{tabular}%

}
\caption{Examples of testing input and target of \textbf{Closed-book QA} cluster, and the retrieved top-1 demonstration from the remaining clusters. In the first case, both the testing input and the prompt relate to the topic of botany. In the second case, both the input and prompt involve questions about time and share the topic of American singers (Taylor Swift and Beyonc\'e).}
\label{tab: case-study Closed-book QA}
\end{table*}

%paraphrase
\begin{table*}[!htb]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l}
\toprule
{\ul \textsc{\textbf{Testing Cluster: Task}}}                             \\ 
Paraphrase Detection: Paws Wiki~\cite{Paws}  \\                                                          
{\ul \textsc{\textbf{Input Instruction}}   }                                                                \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}1.John Barrow Island is a member of the Queen Elizabeth Islands and the Canadian Arctic Archipelago in the territory of Nunavut. 2.John Barrow Island is a member of the Canadian Arctic Archipelago and the Queen Elizabeth Islands in the Nunavut area. Are these two sentences paraphrases of each other?\end{tabular} \\ 
{\ul \textsc{\textbf{Label Completion}}}                                      \\
No                                                                                                   \\ \\
{\ul \textsc{\textbf{Prompt Cluster: Task}}}                             \\ 
Coreference Resolution: DPR~\cite{DPR}                         \\ 
{\ul \textsc{\textbf{Demonstration Input}}}                           \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}Consider this sentence: When Mr.Bond, the veterinarian, came to look at the black horse that lay groaning on the grass, he felt him all over, and shook his head; one of his legs was broken. Are ``his'' and ``the black horse'' the same?\end{tabular}                                                 \\ 
{\ul \textsc{\textbf{Demonstration Answer}}}                                     \\ 
Yes                                                  \\\bottomrule \\ 
\end{tabular}%

} 

\resizebox{\textwidth}{!}{%
\begin{tabular}{l}
\toprule
{\ul \textsc{\textbf{Testing Cluster: Task}}}              \\ 
Paraphrase Detection: MRPC~\cite{MRPC}   \\     
{\ul \textsc{\textbf{Input Instruction}}   }                    \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}This integrates with Rational PurifyPlus and allows developers to work in supported versions of Java, Visual C\# and Visual Basic.NET. IBM said the Rational products were also integrated with Rational PurifyPlus , which allows developers to work in Java, Visual C\# and VisualBasic.Net. If the first sentence is true, is the second one also true?\end{tabular} \\ 
{\ul \textsc{\textbf{Label Completion}}}                                      \\
Yes                                                                                                   \\ \\
{\ul \textsc{\textbf{Prompt Cluster: Task}}}             \\ 
Natural Language Inference: MNLI~\cite{MNLI}               \\ 
{\ul \textsc{\textbf{Demonstration Input}}}                        \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}Sentence 1: ``up on the tidal bulge into a storm'sbarometric low,'' Sentence 2: ``A storm's barometric low was on the tidal bulge.'' If the first sentence is true, then is the second sentence true? Yes, No, or Maybe?\end{tabular}                                                  \\ 
{\ul \textsc{\textbf{Demonstration Answer}}}                                    \\ 
Yes                                                                                                             \\ \bottomrule
\end{tabular}%

}
\caption{Examples of testing input and target of \textbf{Paraphrase Detection} cluster, and the retrieved top-1 demonstration from the remaining clusters.  In both cases, the retrieved prompts have similar sentence formats to the testing input.}
\label{tab: case-study Paraphrase Detection}
\end{table*}

%nli
\begin{table*}[!htb]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l}
\toprule
{\ul \textsc{\textbf{Testing Cluster: Task}}}                             \\ 
Natural Language Inference: MNLI~\cite{MNLI}  \\                                                          
{\ul \textsc{\textbf{Input Instruction}}   }                                                                \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}Here is a premise: ``This site includes a list of all award winners and a searchable database of Government Executive articles.'' Here is a hypothesis: ``The Government Executive articles housed on the website are not able to be searched.'' Is it possible to conclude that if the premise is true, then so is the hypothesis? Yes, No, or Maybe?\end{tabular} \\ 
{\ul \textsc{\textbf{Label Completion}}}                                      \\
No                                                                                                   \\ \\
{\ul \textsc{\textbf{Prompt Cluster: Task}}}                             \\ 
Paraphrase Detection: MRPC~\cite{MRPC}                         \\ 
{\ul \textsc{\textbf{Demonstration Input}}}                           \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}``And they will learn the meaning of American justice,'' he said to strong and extended applause. `` The U.S. will find the killers and they will learn the meaning of American justice,'' Bush told the crowd, which burst into applause. If the first sentence is true, is the second one also true?\end{tabular}                                                 \\ 
{\ul \textsc{\textbf{Demonstration Answer}}}                                     \\ 
No                                                  \\\bottomrule \\ 
\end{tabular}%

} 

\resizebox{\textwidth}{!}{%
\begin{tabular}{l}
\toprule
{\ul \textsc{\textbf{Testing Cluster: Task}}}              \\ 
Natural Language Inference: QNLI~\cite{QNLI}   \\     
{\ul \textsc{\textbf{Input Instruction}}   }                    \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}Does the sentence ``The symptoms of inflammation are redness, swelling, heat, and pain, which are caused by increased blood flow into tissue.'' provide a valid answer to the question ``What causes the symptoms of inflammation?''?\end{tabular} \\ 
{\ul \textsc{\textbf{Label Completion}}}                                      \\
Yes                                                                                                   \\ \\
{\ul \textsc{\textbf{Prompt Cluster: Task}}}             \\ 
Commonsense Reasoning: COPA~\cite{COPA}               \\ 
{\ul \textsc{\textbf{Demonstration Input}}}                        \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}Answer the following question about this sentence: ``The spy discovered the enemy's location.'' What is the cause?\end{tabular}                                                  \\ 
{\ul \textsc{\textbf{Demonstration Answer}}}                                    \\ 
The spy bugged the enemy's phone.                                                                                                             \\ \bottomrule
\end{tabular}%

}
\caption{Examples of testing input and target of \textbf{Natural Language Inference} cluster, and the retrieved top-1 demonstration from the remaining clusters. In the first case, both the testing input and the prompts share a similar question format, asking whether something remains true under certain conditions. In the second case, both the input and prompt ask a question about the logical relationship between cause and effect.}
\label{tab: case-study Natural Language Inference}
\end{table*}

%sentiment
\begin{table*}[!htb]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l}
\toprule
{\ul \textsc{\textbf{Testing Cluster: Task}}}                             \\ 
Sentiment Analysis: SST-2~\cite{SST-2}  \\                                                          
{\ul \textsc{\textbf{Input Instruction}}   }                                                                \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}``it's slow---very, very slow.'' How would the sentiment of this sentence be perceived?\end{tabular} \\ 
{\ul \textsc{\textbf{Label Completion}}}                                      \\
Negative                                                                                                   \\ \\
{\ul \textsc{\textbf{Prompt Cluster: Task}}}                             \\ 
Commonsense Reasoning: COPA~\cite{COPA}                           \\ 
{\ul \textsc{\textbf{Demonstration Input}}}                           \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}``The man went into denial about the tragedy.'' What is the effect of the preceding sentence?\end{tabular}                                                 \\ 
{\ul \textsc{\textbf{Demonstration Answer}}}                                     \\ 
He refused to talk about it.                                                 \\\bottomrule \\ 
\end{tabular}%

} 

\resizebox{\textwidth}{!}{%
\begin{tabular}{l}
\toprule
{\ul \textsc{\textbf{Testing Cluster: Task}}}              \\ 
Sentiment Analysis: Sentiment140~\cite{Sent140}   \\     
{\ul \textsc{\textbf{Input Instruction}}   }                    \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}Reading my kindle2... Love it... Lee childs is good read. How would the sentiment of this tweet be described?\end{tabular} \\ 
{\ul \textsc{\textbf{Label Completion}}}                                      \\
Positive                                                                                                   \\ \\
{\ul \textsc{\textbf{Prompt Cluster: Task}}}             \\ 
Summarization: AESLC~\cite{AESLC}               \\ 
{\ul \textsc{\textbf{Demonstration Input}}}                        \\ 
\begin{tabular}[c]{@{}m{\textwidth}@{}}Wit makes its own welcome, and levels all distinctions. No dignity, no learning, no force of character, can make any stand against good wit. -Ralph. Generate a subject line for this email.\end{tabular}                                                  \\ 
{\ul \textsc{\textbf{Demonstration Answer}}}                                    \\ 
What a wonder is a wonderful wit...                                                                                                             \\ \bottomrule
\end{tabular}%

}
\caption{Examples of testing input and target of \textbf{Sentiment Analysis} cluster, and the retrieved top-1 demonstration from the remaining clusters.  In both cases, the retrieved prompt shares the same sentiment as the testing input, facilitating the language model to accurately predict the sentiment.}
\label{tab: case-study Sentiment Analysis}
\end{table*}

\end{document}
