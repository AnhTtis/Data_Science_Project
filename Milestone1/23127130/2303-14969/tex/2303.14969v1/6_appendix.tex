\newpage
\appendix

\section*{\LARGE Appendix}


\section{Dataset Details}
\label{sec:dataset_details}

This section describes the details about the dataset we used in experiments (Section~\ref{sec:experiment}).

We use "tiny" version of Taskonomy dataset provided by \citep{taskonomy2018}, which consists of images and labels collected from 35 different buildings.
We use the train and val split for training and early-stopping, respectively, and use the "muleshoe" building included in the test split for evaluation.

To demonstrate our universal few-shot learner, we use ten dense prediction tasks in Taskonomy dataset~\citep{taskonomy2018}, which are semantic segmentation (SS), surface normal (SN), Euclidean distance (ED), Z-buffer depth (ZD), texture edge (TE), occlusion edge (OE), 2D keypoints (K2), 3D keypoints (K3), reshading (RS), and principal curvature (PC).
All labels are normalized into $[0, 1]$ with task-specific pre-processing.
For details on the pre-processing, we refer readers to \cite{taskonomy2018}.
Based on the annotations provided by Taskonomy, we preprocess some tasks to increase the diversity of tasks.
Specifically, we modify three single-channel tasks that can be easily augmented: Euclidean distance, texture edge, and occlusion edge.
\begin{enumerate}[leftmargin=0.5cm]
    \item 
    \textbf{Texture edge} (TE) labels are generated by applying Sobel edge detector~\citep{kanopoulos1988design} to RGB images, which consists of a Gaussian filter and image gradient computation.
    The Gaussian filter has two hyper-parameters, namely kernel size and the standard deviation, where adjusting those hyper-parameters yield different \emph{thickness} of detected edges.
    We use three different sets of hyper-parameters -- $(3, 1), (11, 2), (19, 3)$ -- to produce $3$-channel labels.
    We give an example of each channel of TE task in Figure~\ref{fig:texture_edge_augmentation}.
    
    \item
    \textbf{Euclidean distance} (ED) labels consists of pixel-wise depth map, where the depth is computed by the Euclidean distance from each image pixel to the camera's optical center.
    As this task is very similar to the Z-buffer depth prediction (ZD) whose label pixels are the distance from each image pixel to the camera plane, we augment the ED task by segmenting the depth range and re-normalizing within each segment.
    Specifically, we compute the $5$-quantiles of the pixel-wise depth labels in the whole dataset, then use each quantile as different channels after re-noramlization into $[0, 1]$.
    Thus the objective of each channel of the augmented ED task is to predict Euclidean distance within a specific range, where the ranges are disjoint for different channels.
    We give an example of each channel of ED task in Figure~\ref{fig:euclidean_distance_augmentation}.
    To visualize 5-channel labels, we average the first and the second channels as "R"-channel, the third and the fourth channels as "G"-channel, and use the fifth channel as "B"-channel.
    
    \item
    \textbf{Occlusion edge} (OE) labels are similar to texture edge, but they are constructed to depend on only the 3D geometry rather than color or lighting~\citep{taskonomy2018}.
    We observe that the channel augmentation by quantiles (that we apply to Euclidean distance task) can fairly diversify the labels.
    Therefore, we augment the OE labels into 5-channel labels, where we visualize them similar to the ED labels.
    We give an example of each channel of OE task in Figure~\ref{fig:occlusion_edge_augmentation}.
\end{enumerate}

Also, for semantic segmentation, we exclude three classes ("bottle", "toilet", "book"), as little images of the classes are included in the Taskonomy dataset.
The 12 classes we used in experiments are: "chair", "couch", "plant", "bed", "dining table", "tv", "mircrowave", "oven", "sink", "fridge", "clock", and "base".

\input{figures/texture_edge_augmentation}
\input{figures/euclidean_distance_augmentation}
\input{figures/occlusion_edge_augmentation}


\clearpage
\section{Implementation Details}
\label{sec:implementation_details}

This section describes the implementation details in our experiments (Section~\ref{sec:experiment}).

\subsection{Architecture Details of VTM}
\label{sec:arch-vtm}
\paragraph{Encoders and Decoders}
We employ BEiT-B architecture~\citep{bao2021beit} pretrained on Imagenet-22k dataset~\citep{deng2009imagenet} with $224 \times 224$ resolution as our image encoder.
For our label encoder and decoder, we follow the DPT-B architecture~\citep{ranftl2021vision}.
Specifically, we use a randomly initialized ViT-B~\citep{dosovitskiy2020image} as label encoder $g$ and extract features from $3, 6, 9, 12$-th layers of the encoder to form multi-level label features (label tokens).
Similarly, we extract multi-level image features (image tokens) from $3, 6, 9, 12$-th layers of the image encoder (BEiT).
As the DPT-B architecture decodes four-level features using RefineNet-based decoder~\citep{lin2017refinenet}, we pass the predicted query label features from matching module at each layer to the decoder.
As the label values of tasks in Taskonomy are normalized to $[0, 1]$, we use a sigmoid activation function at the head of the decoder to produce values in $[0, 1]$.
To predict semantic segmentation task whose label values are discrete (either $0$ or $1$), we discretize the predicted label with threshold $0.1$.

\paragraph{Matching Modules}
In the implementation of the matching module with multihead attention, we adopt three conventions in vision transformer~\citep{dosovitskiy2020image} which slightly modifies the equations described in Section~\ref{sec:architecture}.
Recall that the matching module is computed on three input matrices $\mathbf{q}\in\mathbb{R}^{M\times d}$ and $\mathbf{k},\mathbf{v}\in\mathbb{R}^{NM\times d}$ as follows:
\begin{align}
    \text{MHA}(\mathbf{q},\mathbf{k},\mathbf{v}) &= \text{Concat}(\mathbf{o}_1, ..., \mathbf{o}_H)w^O, \\
    \text{where }\mathbf{o}_h &= \text{Softmax}\left(\frac{\mathbf{q}w_h^Q(\mathbf{k}w_h^K)^\top}{\sqrt{d_H}}\right)\mathbf{v}w_h^V,
\end{align}
where $H$ is number of heads, $d_H$ is head size, and $w_h^Q,w_h^K,w_h^V\in\mathbb{R}^{d\times d_H}$, $w^O\in\mathbb{R}^{Hd_H\times d}$.
First, we perform layer normalization~\citep{ba2016layer} before each input projection matrices $w_h^Q, w_h^K, w_h^V$ and after the output projection matrix $w^O$, where we share the layer normalization parameters for $w_h^Q$ and $w_h^K$.
Second, we add a residual connection with GELU non-linearity~\citep{hendrycks2016gaussian} after gathering the outputs from multiple heads as follows:
\begin{align}
    \text{MHA}(\mathbf{q}, \mathbf{k}, \mathbf{v}) &= \mathbf{o} + \text{GELU}(\mathbf{o}w^O), \\
    \text{where}~\mathbf{o} &= \text{Concat}(\mathbf{o}_1, \mathbf{o}_2, \cdots, \mathbf{o}_H).
\end{align}
Finally, we apply Dropout~\citep{srivastava2014dropout} with rate 0.1 in the attention scores.

\subsection{Architecture Details of Baselines}
\label{sec:arch-baseline}
\paragraph{Encoders and Decoders}
For the supervised learning baselines based on transformer encoder (DPT and InvPT), we use the same encoder backbone with ours (BEiT pretrained on ImageNet-22k).
We use the decoder of DPT-B configuration in \cite{ranftl2021vision} for DPT as ours, and use the original multi-task decoder implementation provided by \cite{ye2022inverted} for InvPT.
For few-shot learning baselines (HSNet, VAT, DGPNet), we use ResNet-101~\citep{he2016deep} pretrained on ImageNet-1k~\citep{deng2009imagenet} as their encoder backbones, which is their best configuration.
For the other architectural details, we follow the original implementation of each method provided by \cite{min2021hypercorrelation} (HSNet), \cite{hong2022cost} (VAT), and \cite{johnander2021dense} (DGPNet).

\paragraph{Modification on Few-shot Baselines}
As HSNet and VAT are designed for semantic segmentation, we slightly modify their architectures to train them on general dense prediction tasks.
Specifically, both models involve a binary masking operation to filter out support image features using their labels (which are assumed to be binary), before computing 4D correlation tensor between support and query feature pixels.
For continuous labels of general dense prediction tasks, the binary masking becomes pixel-wise multiplication with labels.
However, as the correlation is computed by cosine similarity between feature pixels that is norm-invariant, all non-zero feature pixels with the same direction are treated in the same manner.
This make them unable to discriminate different non-zero label values, \emph{e.g.}, correlation between query and support feature pixels would be the same regardless of the assigned support label values. 
Therefore, we move the masking operation to after computing the cosine-similarity, so that the models can recognize different non-zero label values through different norms of the masked features by (non-binary) labels.

We use the DGPNet without modification as it is based on a regression method (Gaussian Processes) which is inherently applicable to general dense prediction tasks with continuous labels.


\subsection{Training Details}

\paragraph{Training}
We train all models with 300,000 iterations using the Adam optimizer~\citep{kingma2015adam}, and use \emph{poly} learning rate schedule~\citep{liu2015parsenet} with base learning rates $10^{-5}$ for pre-trained parameters and $10^{-4}$ for parameters trained from scratch.
The models are early-stopped based on the validation metric.
At each episodic training of iteration, we sample a batch of episodes with size 8.
In each episode, we construct a 5-channel task from the training tasks $\mathcal{T}_\text{train}$ by first splitting all channels of training tasks and randomly sample 5 channels among them.
Then support and query sets are sampled for the selected channels, where we use support and query size of 4 for Ours and DGP, while using 1 for HSNet and VAT as they only supports 1-shot training.
To train DPT, we construct a batch of each target task $\mathcal{T}_\text{test}$, whose channels are given at once, with batch size $64$.
To train InvPT, we construct a batch of all ten tasks, whose channels are all given at once, while using batch size $16$ due to its large memory consumption.

\paragraph{Data Augmentation}
We apply random crop (from $256 \times 256$ resolution to $224 \times 224$) and random horizontal flip to images, where the random horizontal flip is applied except for surface normal labels as their values are sensitive to the horizontal direction (flipping images and labels together changes the semantics of the task).
As we apply random crop during training, the resolution of test images ($256 \times 256$) differs from the training images.
To evaluate the models with consistent resolution, we perform five-crop (cropping the four corners and center of an image) to test query images so that the model also predicts five-cropped labels, then aggregate them by averaging the overlapping regions to produce final prediction for evaluation of resolution $(256 \times 256)$.
For few-shot models, we apply center crop to support images at test-time.

\paragraph{Task Augmentation}
For episodic training of few-shot models, we further apply two kinds of task augmentation.
First, for each channel of $C$-channel labels sampled at each episode ($C=5$ in our experiments), we apply random jittering and gaussian blur on each channel independently.
Then we apply MixUp~\citep{zhang2018mixup} on the augmented channels and auxiliary channels which are additionally sampled from the training tasks $\mathcal{T}_\text{train}$, to create a linearly interpolated label of two channels.
We apply the task augmentation consistently in each episode to preserve the task identity.


\clearpage
\section{Additional Results}
\label{sec:additional_results}

This section provides additional results on our experiments (Section~\ref{sec:experiment}).


\subsection{Additional Results on Ablation Study}
\label{sec:additional_results_on_ablation_study}

\subsubsection{Sensitivity to the Choice of Support Set}
\label{sec:support_set_sensitivity}
As discussed in Section~\ref{sec:experiment}, we evaluate the $10$-shot performance of our VTM with four different support sets that are disjointly sampled from the training data $\mathcal{D}_\text{train}$.
We report the results in Table~\ref{tab:support_set_choice}, which shows that our model is robust to the choice of support set.
We use the first support set ($\#1$) in Table~\ref{tab:support_set_choice} for comparison with other baselines or ablated variants in Section~\ref{sec:experiment}, due to the huge computational cost for evaluating few-shot baselines HSNet and VAT.

\input{tables/support_set_choice}


% \paragraph{Ablation Study on Training Procedure}
\subsubsection{Ablation Study on Training Procedure}
\label{sec:training_procedure}
To understand the source of the generalization performance of our method more clearly, we conduct an ablation study on training procedure.
We compare four models based on DPT architecture with different training procedures as follows.
\begin{itemize}[leftmargin=0.5cm]
    \item \textbf{M1}: Randomly initialized DPT, 10-shot trained.
    \item \textbf{M2}: DPT with BEiT pre-trained encoder, 10-shot fine-tuned.
    \item \textbf{M3} (Ours w/o Matching): DPT with BEiT pre-trained encoder, multi-task trained with task-specific bias tuning, and then 10-shot fine-tuned.
    \item \textbf{M4} (Ours): DPT with BEiT pre-trained encoder, meta-trained with task-specific bias tuning, and then 10-shot fine-tuned.

\end{itemize}

\input{tables/training_procedure_ablation}
\input{figures/training_procedure_qualitative}

We summarize the quantitative result in Table~\ref{tab:training_procedure_ablation} and qualitative comparison in Figure~\ref{fig:training_procedure_qualitative}.
First, as expected, we observe that DPT with naive 10-shot training (M1) fails to generalize to the test examples in most of the tasks, except for two 2D texture-related tasks (TE, K2). We conjecture that TE and K2 are “easy” cases in terms of few-shot learning, as they are defined as low-level computational algorithms on RGB images, while other high-level tasks require knowledge about semantics (SS) or 3D space (SN, ED, ZD, OE, K3, RS, PC).
Second, we note that BEiT pretraining (M2) largely improves the few-shot generalization performance, allowing the model to produce coarse predictions of the dense labels. However, it still cannot capture object-level fine-grained details in many tasks.
Third, we observe that multi-task training and few-shot adaptation, combined with an efficient parameter-sharing strategy of bias tuning (M3, M4), further improves the performance with a clear gap with M2 where the predictions are also qualitatively finer than M2’s.
Finally, as discussed in Section~\ref{sec:ablation_study}, M4 still further improves over M3 with a clear gap. This shows that in a few-shot learning setting, our matching framework and episodic training are more effective than simple multi-task pretraining employed in M3.
In summary, we may conclude that the fast generalization of Ours is benefitted from episodic training of various tasks followed by parameter-efficient few-shot adaptation as well as powerful pre-training of the encoder (BEiT).



\subsubsection{Fine-tuning with Full Supervision}
To further explore how our method scales well when a large labeled dataset is given, we also fine-tuned our VTM with full supervision of test tasks.
For the fine-tuning, we used the same training dataset as the fully-supervised DPT and employed the episodic fine-tuning objective (Section 3.3). For evaluation, since providing the entire training data as the support set for the matching module is infeasible, we provide a random subset of the training data as the support set to the model.
We summarize the result in Figure~\ref{fig:performance_on_shots_with_full}, which extends Figure~\ref{fig:performance_on_shots} in Section~\ref{sec:experiment}.
In most tasks, our model consistently improves when more supervision is given.
With full supervision at test tasks, our model performs slightly worse than the DPT baseline in seven tasks and performs better or similarly in the other three tasks.
We conjecture that the performance degradation comes from two aspects: (1) the absence of direct input-output connection, \emph{i.e.}, the matching module serves as a bottleneck, and (2) negative transfer from meta-training tasks to test tasks.

\input{figures/performance_on_shots_with_full}

\subsubsection{Effect of Number of Training Tasks}
\label{sec:number_of_training_tasks}
The amount of meta-training tasks is an important factor that can affect the performance of the universal few-shot learner.
To verify this, we fixed two test tasks (SS, SN) and trained our VTM on five different subsets of the original eight training tasks (three different subsets with two tasks and two different subsets with five tasks).
We summarize the results in the Table~\ref{tab:training_tasks_ablation}.
As expected, the performance consistently improves as we increase the number of training tasks.
We also note that the few-shot performance becomes sensitive to the choice of training tasks when their number is small (two), presumably as the model becomes reliant on training tasks more correlated to test tasks, while the variance decreases substantially when more training tasks are added.
In addition, the experiment with incomplete training data (Appendix~\ref{sec:incomplete_experiment}) shows the potential ability of our methods in more realistic settings where the training dataset is formed by a combination of different task-specific datasets.
From these results, we expect that our model can further enhance its universality on few-shot learning by utilizing a combined training dataset of much more diverse tasks, which we leave as future work.

\input{tables/training_tasks_ablation}


\subsubsection{Episodic Training with Incomplete Dataset}
\label{sec:incomplete_experiment}
It would make our method more practical if the model could learn from an incomplete dataset where images are not associated with whole training task labels.
To see how our framework extends to such incomplete settings, we conducted an additional experiment.
We simulate the extreme case of incomplete data by partitioning the training images, such that each image is associated with only a single task out of 8 training tasks.
Specifically, we partitioned the buildings in Taskonomy into eight groups – each corresponds to a different training task.
As this reduces the effective size of training data by the number of training tasks (1/8 in our case), we also train a baseline where we use complete data but use only 1/8 of the training images (for each building, we discard 7/8 of the images).
The results are summarized in Table~\ref{tab:incomplete_dataset}.
We can see that the performance degradation is marginal when we give incomplete data, which implies that our method can be promising in handling realistic scenarios where the training data is a collection of heterogeneous datasets with different label annotations.

\input{tables/incomplete_dataset}


\subsection{Further Analysis}

\subsubsection{Parameter-Efficiency Analysis}
We report the number of task-specific and shared parameters of our VTM and two supervised baselines, DPT and InvPT, to compare how our task adaptation is parameter-efficient.
As DPT is a single-task learning model, no parameters are shared across tasks and the whole network should be trained independently for every new task.
InvPT, which is a multi-task learning model, shares a large portion of its parameters across tasks (\emph{e.g.}, encoder backbone), still consumes many parameters for each task in the decoder.

Due to the extensive amount of parameter-sharing, our method is also promising in continual learning setting.
As all task-specific knowledge is included in the bias parameters of the image encoder, the knowledge acquired from past tasks can be recalled without forgetting by keeping the corresponding bias parameters and switching to them whenever a past model is needed.
We especially note that the size of bias parameters is fairly small (288 KB, which amounts to keeping about 3 labeled images of 256x256 resolution for each task).
This allows our model to retain past knowledge very efficiently by keeping the tuned bias parameters plus a few-shot support set, whose external memory requirement is far less compared to memory-based approaches in continual learning that keep hundreds of images~\citep{bang2021rainbow,wang2022continual}.
While the continual learning setting is not our main focus, applying our method to a continual learning setting would be an interesting future direction.

\input{tables/parameters_table}

\subsubsection{Computation Cost Analysis}
To analyze how our method is computationally efficient compared to supervised DPT, we measured the MACs (multiply–accumulate operations) of our model and DPT using an open-source python library thop~\footnote{https://github.com/Lyken17/pytorch-OpCounter}.
We report the results in Table~\ref{tab:computation_cost}.
Having encoded the support set (e.g., 10-shot), we can see that the computational cost of our model’s inference on a single query image is about 30\% larger than the cost of DPT’s, due to the Matching part.

\input{tables/computation_table}

\subsubsection{Role of Attention Heads}
To analyze the role of attention heads, in Figure~\ref{fig:multihead_attention}, we visualized the attention maps for each head over support images for a given query patch, feature level (3rd level in this example), and task (RS in this example).
The figure shows that each head attends to different regions of the support images.
Moreover, we can find some patterns in heads; for example, the first head tends to attend to flat areas of the scene, such as the floor or ceiling (low-frequency features), while the third head tends to attend to objects, such as couch or plant (high-frequency features).
To further verify the benefit of multi-head attention in the matching module, we also trained our VTM with single head in the matching modules.
The result is summarized in the table below and Table~\ref{tab:attention_heads}.
We can see the performance drop in both SS and SN tasks, which supports that exploiting multiple heads benefits our matching framework.

\input{figures/multihead_attention}
\input{tables/attention_heads}


\clearpage
\subsection{Additional Qualitative Comparison with Baselines}
\label{sec:additional_qualitative_comparison_with_baselines}

We provide additional results on the qualitative evaluation of our model and the baselines.
Figure~\ref{fig:appendix_comparison_1}-\ref{fig:appendix_comparison_4} show visualizations on different query image and support set, where we vary the class of semantic segmentation task included in each support.
The result shows consistent trends of that we discussed in Section~\ref{sec:experiment}.
Ours is competitive to the fully supervised baselines (DPT and InvPT), while the other few-shot baselines (HSNet, VAT, DGPNet) fail to learn different dense prediction tasks.


In Figure~\ref{fig:appendix_comparison_2}, even the GT label for semantic segmentation ("couch" class) is noisy as it is a pseudo-label generated by a pre-trained segmentation model~\citep{taskonomy2018}, our model successfully segments two couches present in the figure.
This can be attributed to the task-agnostic architecture of VTM based on non-parametric matching.

\input{figures/appendix_comparison}


\clearpage
\subsection{Additional Qualitative Comparison with Our Variants}
\label{sec:additional_qualitative_comparison_with_our_variants}

We also provide additional results on the qualitative evaluation of our model and our ablated variants, Ours w/o Matching and Ours w/o Adaptation.
Figure~\ref{fig:appendix_ablation_1}-\ref{fig:appendix_ablation_4} show visualizations on different query image and support set. 
The results show a consistent trend with the quantitative results in Table~\ref{tab:main_table}.
Interestingly, our method without adaptation already exhibits some degree of adaptation to the unseen tasks even without fine-tuning and task-specific components, showing that the non-parametric architecture of our model and the parameter sharing derived from is appropriate to learn generalizable knowledge to understand the novel tasks.
On the other hand, adding a task-specific component and adaptation mechanism to the model allows more dramatic improvement in understanding novel tasks from few-shot examples, showing the importance of the adaptation mechanism in our task.
Finally, we observe that equipping the matching mechanism with the adaptation module provides much sharper and fast adaptation to the unseen tasks, which verifies our claims.


\input{figures/appendix_ablation}