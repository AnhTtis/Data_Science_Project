\cutsectionup
\section{Introduction}
\cutparagraphup

Dense prediction tasks constitute a fundamental class of computer vision problems, where the goal is to learn a mapping from an input image to a pixel-wise annotated label.
Examples include semantic segmentation, depth estimation, edge detection, and keypoint detection, to name a few~\citep{taskonomy2018, cai2019flattenet}.
While supervised methods achieved remarkable progress, they require a substantial amount of manually annotated pixel-wise labels, leading to a massive and often prohibitive per-task labeling cost~\citep{kang2019few, liu2020part, ouali2020semi}.
Prior work involving transfer and multi-task learning have made efforts to generally relieve the burden, but they often assume that relations between tasks are known in advance, and still require a fairly large amount of labeled images of the task of interest (\emph{e.g.}, thousands)~\citep{taskonomy2018, standley2020tasks, o2020unsupervised, wang2021dense}.
This motivates us to seek a \emph{few-shot learning} solution that can \textbf{\emph{universally} learn arbitrary dense prediction tasks from \emph{a few} (\emph{e.g.}, ten) labeled images}.

\vspace{-0.05cm}
However, existing few-shot learning methods for computer vision are specifically targeted to solve a restricted set of tasks, such as classification, object detection, and semantic segmentation~\citep{vinyals2016matching, kang2019few, min2021hypercorrelation}.
As a result, they often exploit prior knowledge and assumptions specific to these tasks in designing model architecture and training procedure, therefore not suited for generalizing to arbitrary dense prediction tasks~\citep{snell2017prototypical, fan2022self, iqbal2022msanet, hong2022cost}.
To our knowledge, no prior work in few-shot learning provided approaches to solve arbitrary dense prediction tasks in a universal manner.

\vspace{-0.05cm}
We argue that designing a universal few-shot learner for arbitrary dense prediction tasks must meet the following desiderata.
First, the learner must have a unified architecture that can handle arbitrary tasks by design, and share most of the parameters across tasks so that it can acquire generalizable knowledge for few-shot learning of arbitrary unseen tasks.
Second, the learner should flexibly adapt its prediction mechanism to solve diverse tasks of unseen semantics, while being efficient enough to prevent over-fitting.
Designing such a learner is highly challenging, as it should be general and unified while being able to flexibly adapt to any unseen task without over-fitting few examples.

\vspace{-0.05cm}
In this work, we propose \textbf{Visual Token Matching (VTM)}, a universal few-shot learner for arbitrary dense prediction tasks.
We draw inspiration from the cognitive process of analogy making~\citep{mitchell2021abstraction}; given a few examples of a new task, humans can quickly understand how to relate input and output based on a \emph{similarity} between examples (\emph{i.e.}, assign similar outputs to similar inputs), while flexibly changing the notion of similarity to the given context.
In VTM, we implement analogy-making for dense prediction as patch-level non-parametric matching, where the model learns the similarity in image patches that captures the similarity in label patches.
Given a few labeled examples of a novel task, it first adapts its similarity that describes the given examples well, then predicts the labels of an unseen image by combining the label patches of the examples based on image patch similarity.
Despite the simplicity, the model has a unified architecture for arbitrary dense prediction tasks since the matching algorithm encapsulates all tasks and label structures (\emph{e.g.}, continuous or discrete) by nature.
Also, we introduce only a small amount of task-specific parameters, which makes our model robust to over-fitting as well as flexible.

\vspace{-0.1cm}
Our contributions are as follows.
\textbf{(1)} For the first time to our knowledge, we propose and tackle the problem of universal few-shot learning of arbitrary dense prediction tasks.
We formulate the problem as episodic meta-learning and identify two key desiderata of the learner -- unified architecture and adaptation mechanism.
\textbf{(2)} We propose Visual Token Matching (VTM), a novel universal few-shot learner for dense prediction tasks.
It employs non-parametric matching on tokenized image and label embeddings, which flexibly adapts to unseen tasks using a tiny amount of task-specific parameters.
\textbf{(3)} We implement VTM as a powerful hierarchical encoder-decoder architecture, where token matching is performed at multiple feature hierarchies using attention mechanism.
We employ ViT image and label encoders~\citep{dosovitskiy2020image} and a convolutional decoder~\citep{ranftl2021vision}, which seamlessly works with our algorithm.
\textbf{(4)} We demonstrate VTM on a challenging variant of Taskonomy dataset~\citep{taskonomy2018} and observe that it robustly few-shot learns various unseen dense prediction tasks.
Surprisingly, it is competitive to or outperforms fully supervised baselines given extremely few examples~($0.1\%$), \textbf{sometimes using only 10 labeled images~($<0.004\%$)}.