\cutparagraphup
\section{Conclusion}
\cutparagraphup
\label{sec:conclusion}
In this paper, we proposed and addressed the problem of building a universal few-shot learner for arbitrary dense prediction tasks.
Our proposed Visual Token Matching (VTM) addresses the challenge of the problem by extending a task-agnostic architecture of Matching Networks to patch-level embeddings of images and labels, and introducing a flexible adaptation mechanism in the image encoder.
We evaluated our VTM in a challenging variant of the Taskonomy dataset consisting of ten dense prediction tasks, and showed that VTM can achieve competitive performance to fully supervised baselines on novel dense prediction tasks with an extremely small amount of labeled examples ($<$0.004\% of full supervision), and is able to even closer the gap or outperform the supervised methods with fairly small amount of additional data ($<$0.1\% of full supervision).