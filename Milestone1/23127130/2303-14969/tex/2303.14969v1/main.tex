
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{multibib}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{enumitem}	
\usepackage{color, colortbl}

\newcommand{\todo}[1]{\textcolor{red}{{[todo: #1]}}}
\newcommand{\jw}[1]{\textcolor{teal}{{#1}}}
\newcommand{\red}[1]{\textcolor{red}{{#1}}}
\newcommand{\blue}[1]{\textcolor{blue}{{#1}}}
\newcommand{\dg}[1]{\textcolor{red}{{#1}}}

\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

% magical magic commands for compact draft
\newcommand{\cutabstractup}{\vspace*{-0.2in}}
\newcommand{\cutabstractdown}{\vspace*{-0.2in}}
\newcommand{\cutsectionup}{\vspace*{-0.15in}}
\newcommand{\cutsectiondown}{\vspace*{-0.12in}}
\newcommand{\cutsubsectionup}{\vspace*{-0.1in}}
\newcommand{\cutsubsectiondown}{\vspace*{-0.07in}}
\newcommand{\cutparagraphup}{\vspace*{-0.1in}}

\def\paratitle{} %% display titles for all paragraphs

\title{Universal Few-shot Learning of Dense Prediction Tasks with Visual Token Matching}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\author{%
  Donggyun Kim$^1$, Jinwoo Kim$^1$, Seongwoong Cho$^1$, Chong Luo$^2$, Seunghoon Hong$^1$ \\
  $^1$ School of Computing, KAIST \\
  $^2$ Microsoft Research Asia \\
  \footnotesize{\texttt{\{kdgyun425, jinwoo-kim, seongwoongjo, seunghoon.hong\}@kaist.ac.kr}}, \\
  \footnotesize{\texttt{chong.luo@microsoft.com}} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\sh}[1]{\textcolor{blue}{[SH: {#1}]}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\vspace{-0.5cm}
\begin{abstract}
\vspace{-0.3cm}
Dense prediction tasks are a fundamental class of problems in computer vision.
As supervised methods suffer from high pixel-wise labeling cost, a few-shot learning solution that can learn any dense task from a few labeled images is desired.
Yet, current few-shot learning methods target a restricted set of tasks such as semantic segmentation, presumably due to challenges in designing a general and unified model that is able to flexibly and efficiently adapt to arbitrary tasks of unseen semantics.
We propose Visual Token Matching (VTM), a universal few-shot learner for arbitrary dense prediction tasks.
It employs non-parametric matching on patch-level embedded tokens of images and labels that encapsulates all tasks.
Also, VTM flexibly adapts to any task with a tiny amount of task-specific parameters that modulate the matching algorithm.
We implement VTM as a powerful hierarchical encoder-decoder architecture involving ViT backbones where token matching is performed at multiple feature hierarchies.
We experiment VTM on a challenging variant of Taskonomy dataset and observe that it robustly few-shot learns various unseen dense prediction tasks.
Surprisingly, it is competitive with fully supervised baselines using only 10 labeled examples of novel tasks (0.004\% of full supervision) and sometimes outperforms using 0.1\% of full supervision.
Codes are available at {\footnotesize\url{https://github.com/GitGyun/visual_token_matching}}.
\end{abstract}
\vspace{-0.2cm}

\input{1_introduction}
\input{2_method}
\input{3_related_work}
\input{4_experiments}
\input{5_conclusion}
\newpage
\paragraph{Acknowledgements}
This work was supported in part by the Institute of Information \& communications Technology Planning \& Evaluation (IITP) (No. 2022-0-00926, 2022-0-00959, and 2019-0-00075) and the National Research Foundation of Korea (NRF) (No. 2021R1C1C1012540 and 2021R1A4A3032834) funded by the Korea government (MSIT), and Microsoft Research Asia Collaborative Research Project.

\paragraph{Reproducibility Statement}
To help readers reproduce our experiments, we provided detailed descriptions of our architectures in Section~\ref{sec:arch-vtm}, and implementation details of the baseline methods in Table~\ref{tab:main_table} in Section~\ref{sec:arch-baseline}.  
Since our work proposes the new experiment settings for the few-shot learning of arbitrary dense prediction tasks, we also provide details of the dataset construction process in the main text (Section~\ref{sec:experiment}) and the appendix (Section~\ref{sec:dataset_details}), which includes details of the data splits, specification of tasks, data preprocessing, and evaluation protocols.
We plan to release the source codes and the dataset to ensure the reproducibility of this paper. 


\paragraph{Ethics Statement}
We have read the ICLR Code of Ethics and ensures that this work follows it.
All data and pre-trained models used in our experiments are publically available and has no ethical concerns.


\bibliography{main}
\bibliographystyle{iclr2023_conference}


\appendix
\input{6_appendix}

\end{document}
