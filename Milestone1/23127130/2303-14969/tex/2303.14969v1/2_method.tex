\cutparagraphup
\section{Problem Setup}\label{sec:proposed_problem_setup}
\cutparagraphup
We propose and tackle the problem of universal few-shot learning of arbitrary dense prediction tasks.
In our setup, we consider any arbitrary task $\mathcal{T}$ that can be expressed as follows:
\begin{equation}\label{eqn:dense_task}
    \mathcal{T}: \mathbb{R}^{H \times W \times 3} \to \mathbb{R}^{H \times W \times C_\mathcal{T}}, \quad C_\mathcal{T} \in \mathbb{N}.
\end{equation}
This subsumes a wide range of vision tasks including semantic segmentation, depth estimation, surface normal prediction, edge prediction, to name a few, varying in structure of output space, \emph{e.g.}, dimensionality ($C_\mathcal{T}$) and topology (discrete or continuous), as well as the required knowledge.

\vspace{-0.05cm}
Our goal is to build a universal few-shot learner $\mathcal{F}$ that, for \emph{any} such task $\mathcal{T}$, can produce predictions $\hat{Y}^{q}$ for an unseen image (\emph{query}) $X^{q}$ given a few labeled examples (\emph{support set}) $\mathcal{S}_\mathcal{T}$:
\begin{equation}\label{eqn:universal_few_shot_learner}
    \hat{Y}^{q} = \mathcal{F}(X^{q}; \mathcal{S}_\mathcal{T}), \quad \mathcal{S}_\mathcal{T} = \{(X^i, Y^i)\}_{i\leq N}.
\end{equation}
To build such a universal few-shot learner $\mathcal{F}$, we adopt the conventional episodic training protocol where the training is composed of multiple \emph{episodes}, each simulating a few-shot learning problem.
To this end, we utilize a meta-training dataset $\mathcal{D}_\text{train}$ that contains labeled examples of diverse dense prediction tasks.
Each training episode simulates a few-shot learning scenario of a specific task $\mathcal{T}_\text{train}$ in the dataset -- the objective is to produce correct labels for query images given a support set.
By experiencing multiple episodes of few-shot learning, the model is expected to learn general knowledge for fast and flexible adaptation to novel tasks.
At test time, the model is asked to perform few-shot learning on arbitrary \emph{unseen} tasks $\mathcal{T}_\text{test}$ not included in the training dataset ($\mathcal{D}_\text{train}$).


\vspace{-0.05cm}
An immediate challenge in handling arbitrary tasks in Eq.~\ref{eqn:dense_task} is that each task in both meta-training and testing has different output structures (\emph{i.e.}, output dimension $C_{\mathcal{T}}$ varies per task), making it difficult to design a single, unified parameterization of a model for all tasks. 
As a simple yet general solution, we cast a task $\mathcal{T}: \mathbb{R}^{H \times W \times 3} \to \mathbb{R}^{H \times W \times C_\mathcal{T}}$ into $C_\mathcal{T}$ \emph{single-channel} sub-tasks $\mathcal{T}_1, \cdots, \mathcal{T}_{C_\mathcal{T}}$ of learning each channel, and model each sub-task $\mathcal{T}_c: \mathbb{R}^{H \times W \times 3} \to \mathbb{R}^{H \times W \times 1}$ independently using the shared model $\mathcal{F}$ in Eq.~\ref{eqn:universal_few_shot_learner}.
Although multi-channel information is beneficial in general, we observe that its impact is negligible in practice, while the channel-wise decomposition introduces other useful benefits such as augmenting the number of tasks in meta-training, flexibility to generalize to arbitrary dimension of unseen tasks, and more efficient parameter-sharing within and across tasks.
Without loss of generality, the rest of the paper considers that every task is of single-channel label.


\cutparagraphup
\subsection{Challenges and Desiderata}
\cutparagraphup
The above problem setup is universal, potentially benefiting various downstream vision applications.
Yet, to our knowledge, no prior work in few-shot learning attempted to solve it.
We attribute this to two unique desiderata for a universal few-shot learner that pose a challenge to current methods.

\vspace{-0.05cm}
\cutparagraphup
\paragraph{Task-Agnostic Architecture}
As any arbitrary unseen task $\mathcal{T}_\text{test}$ can be encountered in test-time, the few-shot learner must have a unified architecture that can handle all dense prediction tasks by design.
This means we cannot exploit any kind of prior knowledge or inductive bias specific to certain tasks.
For example, we cannot adopt common strategies in few-shot segmentation such as class prototype or binary masking, as they rely on the discrete nature of categorical labels while the label space of $\mathcal{T}_\text{test}$ can be arbitrary (continuous or discrete).
Ideally, the unified architecture would allow the learner to acquire generalizable knowledge for few-shot learning any unseen tasks, as it enables sharing most of the model parameters across all tasks in meta-training and testing.

\vspace{-0.05cm}
\cutparagraphup
\paragraph{Adaptation Mechanism}
On top of the unified architecture, the learner should have a flexible and efficient adaptation mechanism to address highly diverse semantics of the unseen tasks $\mathcal{T}_\text{test}$.
This is because tasks of different semantics can require distinct sets of \emph{features} -- depth estimation requires 3D scene understanding, while edge estimation prefers low-level image gradient.
As a result, even with a unified architecture, a learner that depends on a fixed algorithm and features would either underfit to various training tasks in $\mathcal{D}_\text{train}$, or fail for an unseen task $\mathcal{T}_\text{test}$ that has a completely novel semantics.
Thus, our few-shot learner should be flexibly adapt its features to a given task $\mathcal{T}$ based on the support set $\mathcal{S}_\mathcal{T}$, \emph{e.g.}, through task-specific parameters.
At the same time, the adaptation mechanism should be parameter-efficient (\emph{e.g.}, using a tiny amount of task-specific parameters) to prevent over-fitting to training tasks $\mathcal{T}_\text{train}$ or test-time support set $\mathcal{S}_{\mathcal{T}_\text{test}}$.

\vspace{-0.1cm}
\section{Visual Token Matching}\label{sec:visual_token_matching}
\cutparagraphup
We introduce \textbf{Visual Token Matching (VTM)}, a universal few-shot learner for arbitrary dense prediction tasks that is designed to flexibly adapt to novel tasks of diverse semantics.
We first discuss the underlying motivation of VTM in Section~\ref{sec:motivation} and discuss its architecture in Section~\ref{sec:architecture}.

\cutparagraphup
\subsection{Motivation}
\cutparagraphup
\label{sec:motivation}
We seek for a general function form of Eq.~\ref{eqn:universal_few_shot_learner} that produces structured labels of arbitrary tasks with a unified framework.
To this end, we opt into a non-parametric approach that operates on patches, where the query label is obtained by weighted combination of support labels.
Let $X = \{\mathbf{x}_j\}_{j\leq M}$ denote an image (or label) on patch grid of size $M = h\times w$, where $\mathbf{x}_j$ is $j$-th patch.
Given a query image $X^q=\{\mathbf{x}^q_j\}_{j\leq M}$ and a support set $\{(X^i, Y^i)\}_{i\leq N} = \{(\mathbf{x}_k^i,\mathbf{y}_k^i)\}_{k\leq M, i\leq N}$ for a task $\mathcal{T}$, we project all patches to embedding spaces and predict the query label $Y^q = \{\mathbf{y}_j^q\}_{j\leq M}$ patch-wise by,
\begin{align}
    g(\mathbf{y}^q_j)=\sum_{i\leq N}\sum_{k\leq M} \sigma\left( f_\mathcal{T}(\mathbf{x}^q_j), f_\mathcal{T}(\mathbf{x}^i_k) \right) g(\mathbf{y}^i_k),
    \label{eqn:matching}
\end{align}
where $f_\mathcal{T}(\mathbf{x})=f(\mathbf{x};\theta, \theta_\mathcal{T})\in\mathbb{R}^{d}$ and $g(\mathbf{y})=g(\mathbf{y};\phi)\in\mathbb{R}^{d}$ correspond to the image and label encoder, respectively, and $\sigma: \mathbb{R}^d \times \mathbb{R}^d \to [0, 1]$ denotes a similarity function defined on the image patch embeddings.
Then, each predicted label embedding can be mapped to a label patch $\hat{\mathbf{y}}^q_j=h(g(\mathbf{y}^q_j))$ by introducing a label decoder $h\approx g^{-1}$.

\vspace{-0.05cm}
Note that Eq.~\ref{eqn:matching} generalizes the Matching Network (MN)~\citep{vinyals2016matching}, and serves as a general framework for arbitrary dense prediction tasks\footnote{While MN performs image-level classification, we consider its extension to patch-level embeddings.}.
First, while MN interpolates raw categorical labels $\mathbf{y}$ for classification (\emph{i.e.}, $g$ is an identity function), we perform matching on the general embedding space of the label encoder $g(\mathbf{y})$; it encapsulates arbitrary tasks (\emph{e.g.}, discrete or continuous) into the common embedding space, thus enabling the matching to work in a consistent way agnostic to tasks.
Second, while MN exploits a fixed similarity of images $\sigma(f(\mathbf{x}^q), f(\mathbf{x}^i))$, we modulate the similarity $\sigma(f_\mathcal{T}(\mathbf{x}^q), f_\mathcal{T}(\mathbf{x}^i))$ adaptively to any given task by switching the task-specific parameters $\theta_\mathcal{T}$.
Having adaptable components in the image encoder $f_\mathcal{T}$ is essential in our problem since it can adapt the representations to reflect features unique in each task.
Finally, our method employs an additional decoder $h$ to project the predicted label embeddings to the output space. 

\vspace{-0.05cm}
Once trained, the prediction mechanism in Eq.~\ref{eqn:matching} can easily adapt to \emph{unseen} tasks at test-time.
Since the label encoder $g$ is shared across tasks, we can use it to embed the label patches of unseen tasks with frozen parameters $\phi$.
Adaptation to a novel task is performed by the image encoder $f_\mathcal{T}$, by optimizing the task-specific parameters $\theta_\mathcal{T}$ which take a small portion of the model.
This allows our model to robustly adapt to unseen tasks of various semantics with a small support set.
In experiments, our model becomes competitive to supervised method using less than 0.1\% of labels.

\begin{figure}[t!]
    \centering
    \vspace{-0.3cm}
    \includegraphics[width=\textwidth]{figure_files/Architecture.pdf}
    \vspace{-0.1cm}
    \caption{
    Overall architecture of VTM.
    Our model is a hierarchical encoder-decoder with four main components: the image encoder $f_\mathcal{T}$, label encoder $g$, label decoder $h$, and the matching module.
    See the text for more detailed descriptions.
    }
    \label{fig:model_architecture}
    \vspace{-0.5cm}
\end{figure}

\vspace{-0.1cm}
\subsection{Architecture}
\cutparagraphup
\label{sec:architecture}
Figure~\ref{fig:model_architecture} illustrates our model.
Our model has a hierarchical encoder-decoder architecture that implements patch-level non-parametric matching of Eq.~\ref{eqn:matching} in multiple hierarchies with four components: image encoder $f_\mathcal{T}$, label encoder $g$, label decoder $h$, and the matching module.
Given the query image and the support set, the image encoder first extracts patch-level embeddings (\emph{tokens}) of each query and support image independently.
The label encoder similarly extracts tokens of each support label.
Given the tokens at each hierarchy, the matching module performs non-parametric matching of Eq.~\ref{eqn:matching} to infer the tokens of query label, from which the label decoder forms the raw query label.

\cutparagraphup
\paragraph{Image Encoder}
We employ a Vision Transformer (ViT)~\citep{dosovitskiy2020image} for our image encoder.
The ViT is applied to query and each support image independently while sharing weights, which produces tokenized representation of image patches at multiple hierarchies.
Similar to \citet{ranftl2021vision}, we extract tokens at four intermediate ViT blocks to form hierarchical features.
To aid learning general representation for a wide range of tasks, we initialize the parameters from pre-trained BEiT~\citep{bao2021beit}, which is self-supervised thus less biased towards specific tasks.

\vspace{-0.05cm}
As discussed in Section~\ref{sec:motivation}, we design the image encoder to have two sets of parameters $\theta$~and~$\theta_\mathcal{T}$, where $\theta$ is shared across all tasks and $\theta_\mathcal{T}$ is specific to each task $\mathcal{T}$.
Among many candidates to design an adaptation mechanism through $\theta_\mathcal{T}$, we find that \emph{bias tuning}~\citep{cai2020tinytl,zaken2022bitfit} provides the best efficiency and performance empirically.
To this end, we employ separate sets of biases for each task in both meta-training and meta-testing, while sharing all the other parameters.

\cutparagraphup
\paragraph{Label Encoder}
The label encoder employs the same ViT architecture as the image encoder and extracts token embeddings of the support labels.
Similar to the image encoder, the label encoder is applied to each support label independently, with a major difference that it sees one channel at a time since we treat each channel as an independent task as discussed in Section~\ref{sec:proposed_problem_setup}.
Then the label tokens are extracted from multiple hierarchies that matches the image encoder.
Contrary to the image encoder, all parameters of the label encoder are trained from scratch and shared across tasks.


\cutparagraphup
\paragraph{Matching Module}
We implement the matching module at each hierarchy as a multihead attention layer~\citep{vaswani2017attention}.
At each hierarchy of the image and label encoder, we first obtain the tokens of the query image $X^q$ as $\{\mathbf{q}_j\}_{j\leq M}$ and support set $\{(X^i, Y^i)\}_{i\leq N}$ as $\{(\mathbf{k}_k^i, \mathbf{v}_k^i)\}_{k\leq M,i\leq N}$ from the intermediate layers of image and label encoders, respectively.
We then stack the tokens to row-matrices, $\mathbf{q}\in\mathbb{R}^{M\times d}$ and $\mathbf{k},\mathbf{v}\in\mathbb{R}^{NM\times d}$.
Then, the query label tokens at the hierarchy are inferred as output of a multihead attention layer, as follows:
\begin{align}
    \text{MHA}(\mathbf{q},\mathbf{k},\mathbf{v}) &= \text{Concat}(\mathbf{o}_1, ..., \mathbf{o}_H)w^O,\label{eqn:multihead_attention} \\
    \text{where }\mathbf{o}_h &= \text{Softmax}\left(\frac{\mathbf{q}w_h^Q(\mathbf{k}w_h^K)^\top}{\sqrt{d_H}}\right)\mathbf{v}w_h^V,\label{eqn:single_head_attention}
\end{align}
where $H$ is number of heads, $d_H$ is head size, and $w_h^Q,w_h^K,w_h^V\in\mathbb{R}^{d\times d_H}$, $w^O\in\mathbb{R}^{Hd_H\times d}$.

\vspace{-0.05cm}
Remarkably, each attention head in Eq.~\ref{eqn:single_head_attention} implements the intuition of the non-parametric matching in Eq.~\ref{eqn:matching}.
This is because each query label token is inferred as a weighted combination of support label tokens $\mathbf{v}$, based on the similarity between the query and support image tokens $\mathbf{q}$, $\mathbf{k}$.
Here, the similarity function $\sigma$ of Eq.~\ref{eqn:matching} is implemented as scaled dot-product attention.
Since each head involves different trainable projection matrices $w_h^Q,w_h^K,w_h^V$, the multihead attention layer in Eq.~\ref{eqn:multihead_attention} is able to learn multiple branches (heads) of matching algorithm with distinct similarity functions.

\cutparagraphup
\paragraph{Label Decoder}
The label decoder $h$ receives the query label tokens inferred at multiple hierarchies, and combines them to predict the query label of original resolution.
We adopt the multi-scale decoder architecture of Dense Prediction Transformer~\citep{ranftl2021vision} as it seamlessly works with ViT encoders and multi-level tokens.
At each hierarchy of the decoder, the inferred query label tokens are first spatially concatenated to a feature map of constant size ($M\to h\times w$).
Then, (transposed) convolution layers of different strides are applied to each feature map, producing a feature pyramid of increasing resolution.
The multi-scale features are progressively upsampled and fused by convolutional blocks, followed by a convolutional head for final prediction.

\vspace{-0.05cm}
Similar to the label encoder, all parameters of the label decoder are trained from scratch and shared across tasks.
This lets the decoder to meta-learn a generalizable strategy of decoding a structured label from the predicted query label tokens.
Following the channel split in Section~\ref{sec:proposed_problem_setup}, the output of the decoder is single-channel, which allows it to be applied to tasks of arbitrary number of channels.


\cutsubsectionup
\subsection{Training and Inference}\label{sec:learning_and_inference}
\cutparagraphup
We train our model on a labeled dataset $\mathcal{D}_\text{train}$ of training tasks $\mathcal{T}_\text{train}$ following the standard episodic meta-learning protocol.
At each episode of task $\mathcal{T}$, we sample two labeled sets $\mathcal{S}_\mathcal{T}, \mathcal{Q}_\mathcal{T}$ from $\mathcal{D}_\text{train}$.
Then we train the model to predict labels in $\mathcal{Q}_\mathcal{T}$ using $\mathcal{S}_\mathcal{T}$ as support set.
We repeat the episodes with various dense prediction tasks in $\mathcal{D}_\text{train}$ so that the model can learn a general knowledge about few-shot learning.
Let $\mathcal{F}(X^{q}; \mathcal{S}_\mathcal{T})$ denote the prediction of the model on $X^{q}$ using the support set $\mathcal{S}_\mathcal{T}$.
Then the model ($f_\mathcal{T}, g, h, \sigma$) is trained by the following learning objective in end-to-end:
\begin{equation}
    \underset{f_\mathcal{T}, g, h, \sigma}{\text{min}}~ \mathbb{E}_{\mathcal{S}_\mathcal{T}, \mathcal{Q}_\mathcal{T} \sim \mathcal{D}_\text{train}} \left[
    \frac{1}{|\mathcal{Q}_\mathcal{T}|} \sum_{(X^{q}, Y^{q}) \in \mathcal{Q}_\mathcal{T}} \mathcal{L}\left(
    Y^{q}, \mathcal{F}(X^{q}; \mathcal{S}_\mathcal{T}))
    \right)
    \right],
    \label{eqn:training_objective}
\end{equation}
where $\mathcal{L}$ is the loss function. 
We use cross-entropy loss for semantic segmentation task and $L1$~loss for the others in our experiments.
Note that the objective~(Eq.~\ref{eqn:training_objective}) does not explicitly enforce the matching equation~(Eq.~\ref{eqn:matching}) in token space, allowing some knowledge for prediction to be handled by the label decoder $h$, since we found that introducing explicit reconstruction loss on tokens deteriorates the performance in our initial experiments.
During training, as we have a fixed number of training tasks $\mathcal{T}_\text{train}$, we keep and train separate sets of bias parameters of the image encoder $f_\mathcal{T}$ for each training task (which are assumed to be channel-splitted).

After training on $\mathcal{D}_\text{train}$, the model is few-shot evaluated on novel tasks $\mathcal{T}_\text{test}$ given a support set $\mathcal{S}_{\mathcal{T}_\text{test}}$.
We first perform adaptation of the model by fine-tuning bias parameters of the image encoder $f_\mathcal{T}$ using the support set $\mathcal{S}_{\mathcal{T}_\text{test}}$.
For this, we simulate episodic meta-learning by randomly partitioning the support set into a sub-support set $\tilde{\mathcal{S}}$ and a sub-query set $\tilde{\mathcal{Q}}$, such that $\mathcal{S}_{\mathcal{T}_\text{test}}=\tilde{\mathcal{S}}\mathop{\dot{\cup}}\tilde{\mathcal{Q}}$.
\begin{equation}
    \underset{\theta_\mathcal{T}}{\text{min}}~ \mathbb{E}_{\tilde{\mathcal{S}}, \tilde{\mathcal{Q}} \sim \mathcal{S}_{\mathcal{T}_\text{test}}} \left[
    \frac{1}{|\tilde{\mathcal{Q}}|} \sum_{(X^{q}, Y^{q}) \in \tilde{\mathcal{Q}}} \mathcal{L}\left(
    Y^{q}, \mathcal{F}(X^{q}; \tilde{\mathcal{S}})
    \right)
    \right],
\end{equation}
where $\theta_\mathcal{T}$ denotes bias parameters of the image encoder $f_\mathcal{T}$.
% In practice, we also fine-tune input projection of the label encoder $g$ and output head of the label decoder $h$, which empirically improves performance.
The portion of parameters to be fine-tuned is negligible so the model can avoid over-fitting on the small support set $\mathcal{S}_{\mathcal{T}_\text{test}}$.
After fine-tuned, the model is evaluated by predicting the label of unseen query image using the support set $ \mathcal{S}_{\mathcal{T}_\text{test}}$.
