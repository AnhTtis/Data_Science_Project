\section{Experiments}\label{sec:experiment}
\subsection{Experimental Setup}
\paragraph{Dataset}
We construct a variant of the Taskonomy dataset~\citep{taskonomy2018} to simulate few-shot learning of unseen dense prediction tasks.
Taskonomy contains indoor images with various annotations, where we choose ten dense prediction tasks of diverse semantics and output dimensions: semantic segmentation (SS), surface normal (SN), Euclidean distance (ED), Z-buffer depth (ZD), texture edge (TE), occlusion edge (OE), 2D keypoints (K2), 3D keypoints (K3), reshading (RS), and principal curvature (PC),\footnote{We choose all dense prediction tasks defined on RGB images with pixel-wise loss functions.}.
We partition the ten tasks to construct a $5$-fold split, in each of which two tasks are used for few-shot evaluation ($\mathcal{T}_\text{test}$) and the remaining eight are used for training ($\mathcal{T}_\text{train}$).
To perform evaluation on tasks of novel semantics, we carefully construct the partition such that tasks for training and test are sufficiently different from each other \emph{e.g.}, by grouping edge tasks (TE, OE) together as test tasks.
The split is shown in Table~\ref{tab:main_table}.
% \dg{We provide analysis on the number of training tasks in Appendix~\ref{sec:number_of_training_tasks}}.
% \dg{We also investigate an incomplete setting where images are not associated with whole training task labels in Appendix~\ref{sec:incomplete_experiment}.}
We process some single-channel tasks (ED, TE, OE) to multiple channels to increase task diversity, and standardize all labels to $[0, 1]$.
Additional details are in Appendix~\ref{sec:dataset_details}.

\paragraph{Baselines}
We compare our method (VTM) with two classes of learning approaches.
\vspace{-0.2cm}
\begin{itemize}[leftmargin=0.5cm]
    \item \textbf{Fully supervised baselines} have an access to the full supervision of test tasks $\mathcal{T}_\text{test}$ during training, and thus serve as upper bounds of few-shot performance.
    We consider two state-of-the-art baselines in supervised learning and multi-task learning of general dense prediction tasks -- DPT~\citep{ranftl2021vision} and InvPT~\citep{ye2022inverted}, respectively, where DPT is trained on each single task independently and InvPT is trained jointly on all tasks.
    
    \item \textbf{Few-shot learning baselines} do not have an access to the test tasks $\mathcal{T}_\text{test}$ during training, and are given only a few labeled images at the test-time.
    As there are no prior few-shot method developed for universal dense prediction tasks, we adapt state-of-the-art few-shot segmentation methods to our setup.
    We choose three methods, DGPNet~\citep{johnander2021dense}, HSNet~\citep{min2021hypercorrelation}, and VAT~\citep{hong2022cost}, whose architectures are either inherently task-agnostic (DGPNet) or can be simply extended (HSNet, VAT) to handle general label spaces for dense prediction tasks.
    We describe the modification on HSNet and VAT in Appendix~\ref{sec:implementation_details}.
\end{itemize}

\cutparagraphup
\paragraph{Implementation}
For models based on ViT architecture (Ours, DPT, InvPT), we use BEiT-B~\citep{bao2021beit} backbone as image encoder, which is pre-trained on ImageNet-22k~\citep{deng2009imagenet} with self-supervision.
For the other baselines (DPGNet, HSNet, VAT), as they rely on convolutional encoder and it is nontrivial to transfer them to ViT, we use ResNet-101~\citep{he2016deep} backbone pre-trained on ImageNet-1k with image classification, which is their best-performing configuration.
During episodic training, we perform task augmentation based on color jittering and MixUp~\citep{zhang2018mixup} to increase the effective number of training tasks.
For all few-shot learning models, we further apply label channel split as described in Section~\ref{sec:proposed_problem_setup}.
Further details are in Appendix~\ref{sec:implementation_details}.

\vspace{-0.05cm}
\cutparagraphup
\paragraph{Evaluation Protocol}
We use the train/val/test split of the Taskonomy-tiny partition provided by \cite{taskonomy2018}.
We train all models on the train split, where DPT and InvPT are trained with full supervision of test tasks $\mathcal{T}_\text{test}$ and the few-shot models are trained by episodic learning of training tasks $\mathcal{T}_\text{train}$ only.
The final evaluation on test tasks $\mathcal{T}_\text{test}$ is done on the test split.
During the evaluation, all few-shot models are given a support set randomly sampled from the train split, which is also used for task-specific adaptation of VTM as described in Section~\ref{sec:learning_and_inference}.
For evaluation on semantic segmentation (SS), we follow the standard binary segmentation protocol in few-shot semantic segmentation~\citep{shaban2017one} and report the mean intersection over union (mIoU) for all classes.
For tasks with continuous labels, we use the mean angle error (mErr) for surface normal prediction (SN)~\citep{eigen2015predicting} and root mean square error (RMSE) for the others.

\cutparagraphup
\subsection{Results}
\cutparagraphup

\input{tables/main_table}


In Table~\ref{tab:main_table}, we report the 10-shot performance of our model and the baselines on ten dense prediction tasks.
Our model outperforms all few-shot baselines by a large margin, and is competitive with supervised baselines on many tasks.
In Figure~\ref{fig:qualitative_comparison}, we show a qualitative comparison where the few-shot baselines catastrophically underfit to novel tasks while our model successfully learns all tasks.
We provide further qualitative comparisons of ours and the baselines in Appendix~\ref{sec:additional_qualitative_comparison_with_baselines}.

The large performance gap between the few-shot learning baselines and our model can be explained by two factors.
\textbf{(1)} The core architectural component of HSNet and VAT (feature masking) implicitly relies on the discrete nature of labels, and thus fails to learn tasks with continuous labels whose values are densely distributed.
\textbf{(2)} Since the baselines are designed to solve tasks without any adaptation of their parameters, the core prediction mechanism (\emph{e.g.}, hypercorrelation of HSNet and VAT, kernel of DGPNet) is fixed and cannot adapt to different semantics of tasks.
Unlike the baselines, our model is general for all dense prediction tasks, and has a flexible task adaptation mechanism of the similarity in matching.
Our model can also be robustly fine-tuned on few-shot support, thanks to the parameter efficiency of adaptation ($0.28\%$ of all parameters; see Table~\ref{tab:number_of_parameters} for comparison with supervised baselines).
To demonstrate how VTM performs task adaptation, we visualize the attention of the matching module (Eq.~\ref{eqn:multihead_attention}).
Figure~\ref{fig:attention_visualization_new} shows that, when adapted for different tasks, the model flexibly changes the similarity to attend to support patches appropriate for the given task.

Surprisingly, with $<0.004\%$ of the full supervision (10 labeled images), our model even performs better than fully-supervised InvPT on some tasks (\emph{e.g.}, SS and SN).
This can be attributed to the robust matching architecture equipped with a flexible adaptation mechanism that enables efficient knowledge transfer across tasks.
In ablation study, we show that our model can be further improved by increasing the support size, reaching or sometimes surpassing supervised baselines in many tasks.


\input{figures/qualitative_comparison}

\input{figures/performance_on_shots}

\vspace{-0.3cm}
\subsubsection{Ablation Study}
\label{sec:ablation_study}
\vspace{-0.1cm}

\paragraph{Component-Wise Analysis}
To analyze the effectiveness of our design, we conduct an ablation study with two variants.
(1)~\textbf{Ours w/o Adaptation} does not adapt the similarity for each task in the non-parametric matching.
This variant shares all parameters across all training tasks $\mathcal{T}_\text{train}$ as well as test tasks $\mathcal{T}_\text{test}$, and is evaluated without fine-tuning.
(2)~\textbf{Ours w/o Matching} predicts the query label tokens directly from the query image tokens, replacing the matching module with a parametric linear mapping at each hierarchy.
This variant retains the task-specific parameters of ours, thus identical in terms of task adaptation; it utilizes the separate sets of task-specific biases in the image encoder to learn the training tasks $\mathcal{T}_\text{train}$, and fine-tune it on the support set of $\mathcal{T}_\text{test}$ in the test time.

\vspace{-0.1cm}
The results are in Table~\ref{tab:abaltion_table}.
Both variants show lower performance than ours in general, which demonstrates that incorporating non-parametric matching and task-specific adaptation are both beneficial for universal few-shot dense prediction problems.
It seems that the task-specific adaptation is a crucial component for our problem, as Ours w/o Adaptation suffers from the high discrepancy between training and test tasks.
For some low-level tasks whose labels are synthetically generated by applying a computational algorithm on an RGB-image (\emph{e.g.}, TE, K2), Ours w/o Matching achieves slightly better performance than Ours.
Yet, for tasks requiring a high-level knowledge of object semantics (\emph{e.g.}, SS) or 3D space (\emph{e.g.}, ED, OE, K3, RS), the introduction of non-parametric matching fairly improves the few-shot learning performance.
% We provide qualitative comparison of VTM and its variants in Appendix~\ref{sec:additional_qualitative_comparison_with_our_variants}.
Qualitative comparison is in Appendix~\ref{sec:additional_qualitative_comparison_with_our_variants}.
% \dg{Further analysis on architecture and pretraining is in Appendix~\ref{sec:training_procedure}.}

\cutparagraphup
\paragraph{Impact of Support Size}
As our method already performs well with ten labeled examples, a natural question arises: can it reach the performance of fully-supervised approaches if more examples are given?
In Figure~\ref{fig:performance_on_shots}, we plot the performance of VTM by increasing the size of support set from 10 ($<0.004\%$) to 275 ($0.1\%$).
Our model reaches or surpasses the performance of the supervised methods on many tasks with additional data (yet much smaller than full), which implies potential benefits in specialized domains (\emph{e.g.}, medical) where the number of available labels ranges from dozens to hundreds.

\cutparagraphup
\paragraph{Sensitivity to the Choice of Support Set}
As our model few-shot learns a new task from a very small subset of the full dataset, we analyze the sensitivity of the performance to the choice of support set.
Table~\ref{tab:support_set_choice} in Appendix reports the $10$-shot evaluation results with 4 different support sets disjointly sampled from the whole train split.
We see that the standard deviation is marginal.
This shows the robustness of our model to the support set, which would be important in practical scenarios.

\input{tables/ablation_table}
\input{figures/attention_visualization}

\cutparagraphup
\paragraph{Impact of Training Data}
The amount and quality of training data is an important factor that can affect the performance of our universal few-shot learner.
In Appendix~\ref{sec:number_of_training_tasks}, we analyze the effect of the number of training tasks and show that few-shot performance consistently improves when more tasks are added to meta-training data.
Also, in a more practical scenario, we may have an \emph{incomplete} dataset where images are not associated with whole training task labels.
We investigate this setting in Appendix~\ref{sec:incomplete_experiment} and show that our model trained with an incomplete dataset still performs well.
