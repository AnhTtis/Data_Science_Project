\section{Related fields}
\label{sec:related}
Some areas related to causal inference may be unfamiliar to recommender system researchers; thus, we introduce them and carefully clarify the connections and differences between them and causal inference.

\textbf{Causal Discovery~\cite{ peters2017elements}:} Causal discovery is a crucial technique in causality. Causality is the science of cause and effect~\cite{ pearl2018book}. In Pearl’s theory, causality contains two fundamental problems: the first is to prove that one variable is the cause of another or find the cause of a variable, and the second is to draw a conclusion of what might be the effects if changing the value of a variable. The former corresponds to causal discovery, also called causal structure learning,  seeking to discover causal relations, which are stable physical mechanisms in nature and manifest themselves in determined functional relationships between variables, from the data based on some causal assumptions that are hardly testable in observational studies~\cite{pearl2010causal}. The latter corresponds to causal inference, which estimates the outcome after an intervention with a given causal relationship (usually a causal structure obtained by causal discovery or empirically based hypotheses) ~\cite{pearl2018book, yao2021survey, pearl2009causality}. In other words, the former is the basis of the latter, because it is impossible to tell what variables would be affected by an intervention without a causal structure, and thus no interventions and counterfactuals can be implemented. For example, we cannot determine the effect of opening umbrellas on rain if we do not know the causal relevance between them since they always coincide. Note that most methods for causal discovery rely on the SCM framework~\cite{naser2022causality}.

\textbf{Bayesian Inference:} Bayesian inference is a popular approach to data analysis based on Bayes’ theorem, where all observed and unobserved parameters are given a joint probability distribution, i.e., the prior and data distributions, to inference prior distribution~\cite{ van2021bayesian}. Bayesian inference is regarded as one of key techniques and integral components in both PO and SCM: In PO, with assignment mechanisms and the definition of potential outcomes, a Bayesian model can be used to connect the treatment and potential outcomes in real world or counterfactual world; in SCM, Bayesian networks are widely used as causal graphs to present causal associations between variables. Nevertheless, there is a primary distinction between Bayesian inference and causal inference. Bayesian inference is causality-free statistics that focus on associations, such as dependence, likelihood, etc., which can be formulated in terms of distribution functions. However, what is unique to causal inference is that causal concepts cannot be defined from statistics associations alone. As the example mentioned above, it is impossible to tell from the statistics whether raining causes the behavior of opening umbrellas or vice versa. This core distinction leads to two differences between Bayesian inference and causal inference in their specific manifestations, including assumptions and notations. 1) Bayesian inference is based on associational assumptions, which, even untested, are testable in principle~\cite{pearl2010causal}. However, as for causal inference, causal assumptions, in contrast, cannot be verified even in principle unless we proactively influence the observed data, i.e., resort to experimental control. In general, the sensitivity to priors in Bayesian statistics, such as the IID assumption, will decrease with increasing sample size, while sensitivity to prior causal assumptions, say that whether to open umbrellas does not affect the weather, remains substantial regardless of sample size. 2) New notations are introduced to causal inference as causal expressions compared with Bayesian statistics, which is presented in detail in Section ~\ref{sec:foundation}.