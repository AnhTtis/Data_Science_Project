\section{SCM-based Methods}
\label{sec:scm_based}



Unlike the PO framework, Structural Causal Model explicitly expresses the causal relationship between variables on a causal graph, based on the experiences, before analyzing the causal effect. Its intuitive features make it win undivided admiration among researchers in computer field. In this section, the corresponding strategies is classified according to their causal structures, i.e., collider, mediator, and confounder. We focus on how researchers abstract recommendation issues into causal problems with causal graphs and exploit tools in causal inference to cope with it.

\subsection{Causal Recommendation with Collider Structure}



As represented in Fig. ~\ref{fig:junction}(c), a collider node occurs when it receives effects from two or more other factors. Collider exists in recommender systems. For instance, item positions in the ranking list are influenced by user preference and item popularity. 

Analyzing the dependency between variables in collider structures will contribute to its utilization in recommender systems. Although $A$ and $C$ are independent, i.e., for all $a$ and $c$, $\textrm{Pr}(A=a|B=b)=\textrm{Pr}(A=a)$, conditioning on the collision node $C$ produces a dependence between the node’s parents, i.e., for some $a,b,c$, $\textrm{Pr}(A=a|B=b, C=c)=\textrm{Pr}(A=a|C=c)$.  %One of the simplest samples will suffice to understand the point. 
To understand the point, let us consider the most basic example where $C=A+B$, and 
$A$ and $B$ are independent variables~\cite{glymour2016causal}. In this case, given $C=10$, knowing $A=3$ means we can immediately calculate that $B=7$. Thus, $A$ and $B$ are dependent, given that $C=10$. This characteristic inspires us that in RS issues with collider structure, knowing the common effect and one of the causes would provide information for another effect ~\cite{zhang2022causal}.

Though collider structures permeate RSs, they are usually compounded by other causal relationships and are treated as other causal structures, which results in minor literature discussing purely colliders. A representative work is DICE~\cite{zheng2021disentangling}, which is proposed by Zheng et al. and tracks the popularity issue from the user’s perspective instead of eliminating popularity bias from the item’s perspective. Zheng et al. argue that users’ interactions are driven by individual interest as well as users’ conformity, which is independent of user interest and describes how users tend to follow other people, and provides a causal graph as shown in Fig. \ref{fig:collider} (a). From this point of view,  DICE splits user and item embeddings into interest and conformity embeddings, respectively, and learns disentangled representations with conformity-specific and interest-specific data, driven by the colliding effect: if a user interacts with a less popular item, not conforming to the mainstream, it usually indicates that the user is highly interested in the item itself, and vice versa. Further, ~\cite{ding2022causal} proposes CIGC (Causal Incremental Graph Convolution), which includes a new operator named CED (Colliding Effect Distillation), to efficiently retrain graph convolution network (GCN) based recommender models. CED frames the whole incremental training phase as a causal graph (see Fig. \ref{fig:collider} (b)) and create a collider $S_t$ between inactive nodes $R_{In,t}$ and new data $R_{Ac,t}$, which is represented as the pair-wise distance. Therefore, the incremental integration data $I_t $ can update both $R_{Ac,t}$ and $R_{In,t} $, since conditioning on the collider $S_t$ opens the path $I_t \rightarrow R_{Ac,t} \leftrightarrow  R_{In,t} $.

\begin{figure}[t!]
	\centering
    \vspace{-4mm}
	\subfloat[DICE]{
		%\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=0.23\textwidth, trim=0 -20 0 -10, clip]{pic/conformity.pdf}
		%\end{minipage}
	}
	\centering
	\subfloat[CIGC]{
		%\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=0.28\textwidth, trim=8 0 10 5, clip]{pic/CollidingEffectDistillation.pdf}
	}
	\centering
    \vspace{-3mm}
	\caption{Causal graphs of collider in recommender systems.}
    \vspace{-6mm}
	\label{fig:collider}
\end{figure}


\subsection{Causal Recommendation with Mediator Structure}




When one variable causes another, it may not do it directly but through a set of mediating variables instead. For example, an item purchased by your friends increases your purchase probability not only directly through the recommendation that integrates social network, but also indirectly through increased trust in the item.

The distinction between direct and indirect effects of the change of treatment on outcome is key to the utilization of the mediator structure, which can be done by conditioning on the mediating variable traditionally~\cite{glymour2016causal}. Specifically, as illustrated in Fig. \ref{fig:mediator} (a), the \textit{total effect} (ToE) of $I=i$ on $Y$ is defined as:
\begin{equation}
ToE=Y(I=i, K(I=i))-Y(I=i^*, K(I=i^*)),
\end{equation}
$I = i^{*}$ refers to the situation where the value of $I$ is different from the reality, i.e., counterfactual.
%typically set as null.
Total effect can be further decomposed into \textit{natural direct effect} (NDE) and \textit{total indirect effect} (TIE). NDE reflects the effect of $I$ on $Y$ through the direct path, i.e., $I \rightarrow Y$, while $K$ is set to the value when $I=i^*$:
\begin{equation}
    NDE = Y(I=i, K(I=i^*))-Y(I=i^*, K(I=i^*)).
\end{equation}
TIE is defined as the difference between TE and NIE, denoted as:
\begin{equation}
\label{eq:tie}
    TIE = ToE - NDE  = Y(I=i, K(I=i))-Y(I=i, K(I=i^*)),
\end{equation}
which represents the effect of $I$ on $Y$ through the indirect path $I \rightarrow K \rightarrow Y$. TE can also be decomposed into \textit{natural indirect effect} (NIE) and \textit{total direct effect} (TDE). NIE represents the effect of $I$ on $Y$ through the mediator, i.e., $I \rightarrow K \rightarrow  Y$, while the direct effect on $I \rightarrow Y$ is blocked by setting $I$ as $I*$, denoted as:
\begin{equation}
    NIE = Y(I=i*, K(I=i))-Y(I=i^*, K(I=i^*)).
\end{equation}
In linear systems, NIE and TIE have the same value, and NDE and TDE have the same value~\cite{glymour2016causal, pearl2022direct}.



However, if there are confounders of the mediator and the outcome, as the case of ~\cite{wei2021model} shown in Fig. \ref{fig:mediator} (b), conditioning on the mediator means conditioning on a collider, and thus indirect dependence will pass through the confounder to the outcome and misguide the calculation of indirect effect. To tackle the problem, we should intervene on the mediator, which involves counterfactuals. The controlled direct effect (CDE) on $Y$ of $I$ is defined as:
\begin{equation}
    CDE = Y(do(I=i), do(K=k))-Y(do(I=i^*), do(K=k)).
\end{equation}
The difference between NDE and CDE is explained in ~\cite{glymour2016causal}.





% Table generated from sheet 'collider_mediator'
\begin{table*}[pt]
  \small
  \centering
  % \renewcommand\arraystretch{1.5}
  \vspace{-3mm}
  \caption{\normalsize Summary of recommendation models with collider structure and mediator structure.}
  \vspace{-3mm}
  \setlength{\tabcolsep}{2.2mm}{
    \begin{tabular}{c|c|c|c|c|c}
    \toprule
    \textbf{Category} & \textbf{Model} & \textbf{Causal method} & \textbf{Backbone model} & \textbf{Issue of concern} & \textbf{Year} \\
    \midrule
    \multirow{2}[2]{*}{\makecell{Causal\\ recommendation\\ with collider structure}} & DICE~\cite{zheng2021disentangling} & \makecell{(causal view)} & MF(multi-task) & Popularity bias & 2021 \\
          & CIGC~\cite{ding2022causal} & \makecell{Intervention on \\the cause factor} & LightGCN~\cite{he2020lightgcn} & GCN model retraining & 2022 \\
    \midrule
    \multirow{6}[2]{*}{\makecell{Causal\\ recommendation \\with mediator \\structure}} & ~\cite{choi2011influence} & Mediation analysis & -     & Effect of social presence & 2011 \\
          & ~\cite{luo2013impact} & Mediation analysis & -     & Effect of informational factors & 2013 \\
          & CMA~\cite{yin2019identification} & NDE, TIE & -     & Effect of induced change & 2019 \\
          & MACR~\cite{wei2021model} & TIE   & (model-agnostic, multi-task) & Popularity bias & 2021 \\
          & CIRS~\cite{gao2022cirs} & \makecell{Intervention on \\the mediator} & PPO~\cite{schulman2017proximal} & Filter bubble~\cite{pariser2011filter} & 2022 \\
          & CCF~\cite{xu2021causal} & \makecell{Intervention on\\ the mediator, \\counterfactuals} & \makecell{NCF~\cite{he2017neural}, \\GRU4Rec~\cite{hidasi2015session}, etc.} & Historical bias & 2023 \\
    \bottomrule
    \end{tabular}}%
  \vspace{-3mm}
  \label{tab:collider_mediator}%
\end{table*}%

\begin{figure}[t!]
	\centering
    \vspace{-3mm}
	\subfloat[Simple mediator]{
		%\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=0.23\textwidth, trim=-20 -20 -20 0, clip]{pic/simple_mediator.pdf}
		%\end{minipage}
	}
	\centering
	\subfloat[Mediator with confounder]{
		%\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=0.23\textwidth, trim=-20 0 -20 0, clip]{pic/mediator_confounder.pdf}
	}
	\centering
    \vspace{-1mm}
	\caption{Causal graphs of mediators in recommender systems, where $I$, $Y$, $K$ and $U$ denote cause, effect, mediator and confounder variable of the mediator and the outcome, respectively.}
    \vspace{-2mm}
	\label{fig:mediator}
\end{figure}

Some works are generally interested in how much of the treatment’s causal effect on variable $Y$ is direct and how much is indirect, which is usually explored with the technique of \textit{mediation analysis}~\cite{kenny1979correlation, baron1986moderator}, similar to SCM but without exogenous variables and the introduction of counterfactuals. For example, in early studies, ~\cite{choi2011influence} conducts an experiment varying the level of social presence over hundreds of testers and examines the effect of social presence on users’ reuse intention and trust through mediation analysis. A similar structure is used to evaluate how electronic word-of-mouth affects user interactions in ~\cite{luo2013impact}. Further, Yin et al.~\cite{yin2019identification} aim to separate the direct effects of the change in user behaviors in the tested product from the effect of changes in user behaviors in other products, aka induced changes, for example, the effects of significant lifts in CTR on the recommendation list and of significant decreases in CTR on organic search results on the final insignificant lifts in the sitewide CVR during the A/B test of a new version of recommendation module. Therefore, they use causal mediation analysis (CMA) of potential outcome framework to estimate causal effects of the induced changes and also discuss the estimation under the situation that multiple unmeasured causally-dependent mediators exist with the help of a directed acyclic graph.


Some other works utilize Pearl’s counterfactual tool to cope with mediator structure in order to improve accuracy~\cite{wei2021model, xu2021causal, gao2022cirs}. Wei et al.~\cite{wei2021model} explore the popularity issue with the SCM framework and formulate the causal graph as Fig. \ref{fig:mediator} (b) shown, in which the probability of interaction $Y$ is influenced by three main factors: user-item matching ($K(U, I) \rightarrow Y$), item popularity ($I \rightarrow Y$) and user conformity ($U \rightarrow Y$), the last two of which are usually ignored by existing models and thus result in the terrible Matthew effect. Following this causal graph, Wei et al. propose MACR (Model-Agnostic Counterfactual Reasoning), a multi-task framework that consists of three modules to jointly learn the effects of $U \rightarrow Y$, $U\&I \rightarrow K \rightarrow Y$, and $I \rightarrow Y$,  respectively during recommender training and estimates TIE of $I$ on $Y$ in counterfactual inference:
\begin{equation}
\begin{aligned}
\label{eq:tie_macr}
 TIE= &ToE - NDE  \\
= &Y(U=u, I=i, K=K(U=u, I=i))-Y(U=u, I=i, do(K=K(U=u^*,I=i^*))) 	\\
= &Y_k(K(U=u, I=i))*Y_u(U=u)*Y_i(I=i)-Y_k(K(U=u^*, I=i^*)) *Y_u(U=u)*Y_i(I=i)\\
=&\hat{y}_{k} * \sigma\left(\hat{y}_{i}\right) * \sigma\left(\hat{y}_{u}\right)-c * \sigma\left(\hat{y}_{i}\right) * \sigma\left(\hat{y}_{u}\right),
\end{aligned}
\end{equation}
where $\sigma(\cdot)$ denotes the sigmoid function, and $c$ is a hyper-parameter that represents $Y_k(K(U=u*, I=I*))$, the reference situation of $Y_k(K(U=u, I=i))$. With counterfactual inference, MACR could rank items without popularity bias by reducing the direct effect from item properties to the ranking score.



\begin{figure}[t!]
	\centering
    \vspace{-3mm}
	\subfloat[]{
		%\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=0.23\textwidth, trim=-20 0 -20 0, clip]{pic/CCF0.pdf}
		%\end{minipage}
	}
	\centering
	\subfloat[]{
		%\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=0.23\textwidth, trim=-20 0 -20 0, clip]{pic/CCF1.pdf}
	}
	\centering
    \vspace{-2mm}
	\caption{(a-b) Causal graphs of the CCF model before and after intervention. $U$ and $I$ are user and item representation, respectively, $Y$ is preference score, and $H$ denotes user interaction history.}
    \vspace{-2mm}
	\label{fig:mediator_ccf}
\end{figure}




The work by Xu et al.~\cite{xu2021causal} regards the user interaction history $H$ as a mediator (Fig. \ref{fig:mediator_ccf} (a)) and proposes CCF (Causal Collaborative Filtering) to estimate $\textrm{Pr}(Y=y|U=u, do(I=i))$, where $u, i$ is a user-item pair and $y$ is the preference score for the pair. More specifically, $H=f_h(U=u)$ is a database retrieval operation that returns a user’s interaction history from the observational data, $I=f_0(U=u, H=h)$ means the recommended item $I$ returned from the already deployed recommendation system based on the user and the user’s interaction history, and $Y=f(U=u, I=i)$ represents the estimation of unbiased user preference on the item. $\textrm{Pr}(Y=y|U=u, do(I=i))$ adopts the conditional intervention to consider both observed and unobserved (counterfactual) interaction history, as presented in Fig. \ref{fig:mediator_ccf} (b). The derivation result of $\textrm{Pr}(Y=y|U=u,do(I=i))$ is given:
\begin{equation}
\begin{aligned}
\textrm{Pr}(Y=y \mid U=u,do(I=i)) 
\approx & \textrm{Pr}(Y=y \mid U=u,do(I=f_0(U=u, H=h))) \\
 = & \sum_{h} \textrm{Pr}(y \mid u, h, f_0(u, h))\textrm{Pr}(h\mid u)\end{aligned}
\end{equation}
It is tempting to conclude that if trained only with observed history $h$, f(U=u, I=i) would naturally degenerate to the original recommendation model $f_0(U=u, H=h)$. Therefore, Xu et al. adopt a heuristic-based approach to generate counterfactual history $h’$.


\subsection{Causal Recommendation with Confounder Structure}

There is a large volume of published studies investigating the confounding structures in recommendation since a lot of data biases widespread in recommender systems are, essentially, confounding biases mentioned in Section \ref{sec:MNAR}. Approaches to tackle confounder structures of existing literature can be categorized into four types: with back-door adjustment, with instrumental variables, with front-door adjustment, and with deep learning based intervention.
%, in which the last type accounts for the major part. 






\subsubsection{The Back-door-based Approach}

Before introducing the back-door adjustment approaches, let us brieﬂy review the definitions of back-door path and back-door criterion~\cite{imbens2015causal}. 
\begin{definition}[Back-door Path]
\label{def:backdoor_path}
Given a pair of treatment $T$ and outcome variable $Y$, a path connecting $T$ and $Y$ is a back-door path for $(T, Y)$ if it satisfies that
\begin{enumerate}
\item it is not a directed path (it contains an arrow pointing into $T$); and
\item it is not blocked (it has no collider).
\end{enumerate}
\end{definition}
Back-door path help us to identify confounders, which is the central node of a fork on a back-door path of $(T, Y)$. The following two examples will help to illustrate it~\cite{pearl2018book}. In Fig. \ref{fig:confounder_back} (a), there is one back-door path from $T$ to $Y$, $T \leftarrow A \rightarrow Y$, indicating that $A$ is the confounder. For the estimation the effect of $T$ on $Y$, we should eliminate the confounding bias by either controlling $A$ to block the back-door path or running a randomized controlled experiment. Note that $T \rightarrow B \leftarrow A \rightarrow Y$ is blocked by the collider at $B$ and, therefore, not a back-door path. In Fig. \ref{fig:confounder_back} (b), we can control for $C$ to close the back-door path $T \leftarrow B \leftarrow C \rightarrow Y$. Here we present the formal definition of the back-door criterion to deal with the confounding effects.
\begin{definition}[Back-door Criterion]
\label{def:backdoor_criterion}
Given a pair of treatment $T$ and outcome variable $Y$, a set of variables $X$ satisfied the back-door criterion if $X$ blocks all back-door paths of $(T, Y)$.
\end{definition}

Based on the Back-door Criterion, we can further derive the Back-door Adjustment Theorem, which adjusts fewer variables compared to the Causal Effect Rule (Definition \ref{def:ce_rule}).

\begin{definition}[Back-door Adjustment]
\label{def:backdoor_adjustment}
If a set of variables $X$ satisfies the back-door criterion for $T$ and $Y$, the causal effect of $T$ on $Y$ is identifiable and given by the formula:
\begin{equation}
\textrm{Pr}(Y=y \mid do(T=t)) =\sum_{x} \textrm{Pr}(Y=y \mid T=t, X=x) \textrm{Pr}(X=x),
\end{equation}
\end{definition}

\begin{figure}[t!]
	\centering
    \vspace{-3mm}
	\subfloat[]{
		%\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[height=0.12\textheight, trim=-5 0 -5 0, clip]{pic/backdoor_path0.pdf}
		%\end{minipage}
	}
	\centering
	\subfloat[]{
		%\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[height=0.12\textheight, trim=-5 0 -5 0, clip]{pic/backdoor_path1.pdf}
	}
	\centering
    \vspace{-1mm}
	\caption{Causal graphs illustrating the back-door path.}
    \vspace{-4mm}
	\label{fig:confounder_back}
\end{figure}


\begin{figure}[t!]
\centering
\vspace{-1mm}
\includegraphics[height=0.12\textheight,trim=0 0 0 0 ,clip]{pic/backdoor_ex.pdf} %l b r t
\vspace{-1mm}
\caption{A causal graph representing the relationship between recommendation ($T$), click ($Y$), consuming desire ($A$), and number of recent interactions $B$. The dotted circle indicates this variable is unobservable.}
\vspace{-1mm}
\label{fig:backdoor_ex}
\end{figure}

\begin{figure}[t!]
\centering
\vspace{-1mm}
\includegraphics[height=0.13\textheight,trim=0 0 0 0 ,clip]{pic/iv.pdf} %l b r t
\vspace{-1mm}
\caption{A causal graph showing the relationships between a sudden shock in traffic 
($Z_{i}$), total exposure of the focal product $i$ ($T_{i}$), product demand $D_i$ and $D_j$, recommendation click-through of related product $j$ from the focal product $i$($Y_{ij}$). The focal product $i$ experiences an instantaneous shock $Z_{i}$ in traffic while the product $j$ recommended shown alongside does not. $V_j$ means direct exposure of product $j$ (e.g., through search or browsing), which is not influenced by recommendation.
%The exposure of the focal product $i$, $T_{i}$, is influenced by the instantaneous shock in traffic $Z_{i}$ and the unobserved inherent demand for the product $D_i$, while the exposure of product $j$ is further broken down into direct exposure (e.g., through search or browsing), $V_j$, recommendation click-throughs from the focal product, $Y_{ij}$, and click-throughs from other products that recommend product $j$. Note that product $i$ and product $j$ are usually related products, so their demand may be driven by some common factors.
}
\vspace{-3mm}
\label{fig:sharma2015estimating}
\end{figure}






To see what this means in practice, let us look at a concrete example, as presented in Fig \ref{fig:backdoor_ex}. Suppose we need to evaluate the effect of recommendation ($T$) on user’s click behavior ($Y$) of a newly deployed recommendation strategy on an online shopping platform. However, the time-varying consuming desire ($A$) makes it difficult to compare the effect with that of the existing one. For example, users might be more willing to spend due to the proximity of holidays, resulting in a seemly better recommendation effect of the tested policy. However, the consuming desire is unmeasurable for \textit{do}-calculation. Instead, we could control for an observed variable, the number of recent interactions $B$, that fits the back-door criterion from $T$ to $Y$. Therefore, adjusting for $B$ to block the back-door path $T \leftarrow A \rightarrow B \rightarrow  Y$ will give us the true causal effect of recommendation $T$ on click $Y$, formulated as:
\begin{equation}
\begin{aligned}
\textrm{Pr}(Y=y \mid do(T=t))=\sum_{x} \textrm{Pr}(Y=y \mid T=t, B=b) \textrm{Pr}(B=b).
\end{aligned}
\vspace{-2mm}
\end{equation}





Some literature on recommendation issues with confounder structures introduces the theory of back-door criterion~\cite{huang2012exploring, sharma2015estimating, tran2021recommending, wang2021clicks}. ~\cite{huang2012exploring} utilizes the back-door criterion to verify whether or not word-of-mouth recommendations can influence users’ evaluation of the recommended items. Sharma et al.~\cite{sharma2015estimating} treat an instantaneous shock in direct traffic as an \textit{instrumental variable} to answer the counterfactual question from purely observational data: how much interaction activity would there have been on the online shopping website if recommendations were absent, and apply the back-door criterion to block the possible unobserved confounding effect between the “exposure” $T_i$ and “click” $Y_{ij}$, as Fig. \ref{fig:sharma2015estimating} shown. Besides, Tran et al.~\cite{tran2021recommending} consider the job personal recommendation issue in Disability Employment Services and present a causality-based method to tackle the problem, in which the covariate set is determined by the back-door criterion.
%It is worth mentioning that the instrumental variable method is a popular method for learning causal effects with unobserved confounders, but it seems to find little application in recommender systems because of the difficulty of finding variables that satisfy the conditions of instrumental variables.



\begin{figure}[t!]
	\centering
    \vspace{-3mm}
	\subfloat[DecRS]{
		%\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[height=0.14\textheight, trim=-15 -12 -15 -12, clip]{pic/DecRS.pdf}
		%\end{minipage}
	}
	\centering
	\subfloat[PDA]{
		%\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[height=0.14\textheight, trim=-15 0 -15 0, clip]{pic/PDA.pdf}
	}
	\centering
    \vspace{-2mm}
	\caption{(a) The causal graph used in DecRS, where $U$ and $I$ denote user and item representation, $D$ represents the user historical distribution over item groups, $G$ is the group-level user representation, and $Y$ is the prediction score. (b) The causal graph of PDA, in which $U$ and $I$ denote user and item representation, $P$ is the item popularity, and $Y$ stands for user interactions.
}
    \vspace{-3mm}
	\label{fig:backpath}
\end{figure}


A multitude of studies employ back-door adjustment to block the back-door path by directly intervening on the treatment variable~\cite{wang2021deconfounded, zhang2021causal, he2022addressing, wang2022causal, zhan2022deconfounding, rajanala2022descover, xia2023user, zhang2023leveraging, yu2023causality, tsoumas2023evaluating}. For example, Wang et al.~\cite{wang2021deconfounded} propose the framework named DecRS (Deconfounded Recommender System) to eliminate bias amplification through intervention on the user representation $U$, which removes the effect of the historical user distribution over item groups $D$ on $U$, as Fig. \ref{fig:backpath} (a) shown. Zhang et al.~\cite{zhang2021causal} propose PDA (Popularity-bias Deconfounding and Adjusting) to eliminate the effect of item popularity $P$ through intervention on the item $I$ (see Fig. \ref{fig:backpath} (b)), denoted as:
\begin{equation}
\textrm{Pr}(Y=y \mid do(U=u, I=i))=\sum_{p} \textrm{Pr}(y \mid u, i, p) \textrm{Pr}(p \mid u,i)
=\textrm{Pr}(y \mid u, i, p) \textrm{Pr}(p),
\end{equation}
where $U$ denotes the user representation and $Y$ represents interactions. $\textrm{Pr}(y \mid u, i, p)$ and $\textrm{Pr}(p)$ are learned separately. It is worth mentioning that PDA can leverage popularity bias to enhance the recommendation performance by adjusting $\textrm{Pr}(p)$ in the inference stage, which can be regarded as counterfactual inference. More recently, Zhang et al.~\cite{zhang2023leveraging} address duration bias by identifying duration time as a confounder. Subsequently, they group data samples based on watch time feedback and craft novel duration supervision labels, thereby alleviating the confounding bias.




In the above literature elaboration, we may find a series of works that accomplish the integration of SCM-based causal inference and recommender systems with a similar pattern, as shown in Fig. \ref{fig:he_type}: they first analyze the causal relationship between the variables regarding the concern issue and formulate the causal graph based on it; after theoretical analysis, a multi-task or separated structure is adopted to learn the causal effects of the variables on the potential outcome in the training phase; once the training has been completed, appropriate variables are selected to intervene during the inference stage, i.e., they are set to counterfactual values directly or indirectly, and the outcome is estimated based on applicable causal rules (e.g., backdoor adjustment, TIE, etc.) to conduct counterfactual inference. 
%A representative research is CR~\cite{wang2021clicks} by Wang et al. They focus on the clickbait issue and build the cuasal graph as Fig. \ref{fig:clickbait} (a), where $Y$, $U$, $I$, $E$, and $P$ denote the prediction score, user features, item features, exposure features, and content features (post-click), respectively. They regard that users might click the items only because they are attracted by the exposure features, and thus there exists a direct effect from the exposure features to the click behavior $E \rightarrow Y$. As a result of ignoring the effect of $E \rightarrow Y$, the clickbait issue can be mitigating by reducing this effect. After the theoretical analysis, 


%应该说明的是，有一系列工作都是利用相似模式来完成SCm框架下的因果推断与推荐系统的融合，如图3所示：他们首先分析关心问题中涉及的变量之间的关系，并绘制出因果图；在训练阶段，用多任务或者分离的结构来学习这些变量对outcome的影响；在推断阶段，在这些变量里选择合适的变量去干预，即直接或间接地将其设置为其他值，在此基础上利用合适的规则（如后门调整、TIE等）估计outcome，从而完成反事实推断。一个典型的工作是【A】，该工作formulate the clickbait issue如图4所示的因果图，其中��,��,��,��,and�� denotethepredictionscore,user features, item features, exposure features, and content features, respectively。该文认为users might click the items only because they are attracted by the exposure features，因此存在a direct effect from the exposure features to the click behavior。忽略这个作用正是clickbait的来源。。。

%如前文所言，相比PO框架，SCM框架对因果图的依赖既是他的优点，也是他的缺点。因果图可以使得变量之间的关系变得直观，但是这些关系是现在主要是依赖于专家经验给出的，这使得可能存在对结果有影响但未被观察到的变量，甚至出现过于主观、与实际不符的因果关系假设。这一点可以由现有文献中轻易得知，因为不同的文献对于同一个issue往往会提出不同的因果图。为解决以上问题，Causal discovery显得至关重要。

\subsubsection{Instrumental Variable-based Approach}
The instrumental variable (IV) method is such a powerful approach for learning causal effects with confounders that it can be done even without controlling for, or collecting data on, the confounders~\cite{pearl2018book}. The instrumental variable causally influences the outcome only through the treatment (Fig. \ref{fig:iv_ex} (a)), defined as:
\begin{definition}[Instrumental Variable]
\label{def:iv}
Given an observed variable $Z$, covariates $X$, the treatment $T$ and the outcome $Y$, $Z$ is a valid instrumental variable (IV) for the causal effect of $T \rightarrow Y$ if $Z$ satisfies~\cite{angrist1996identification}: 
\begin{enumerate}
\item $Z \dep T \mid X$; and
\item $Z \ind Y \mid do(T), X$.
\end{enumerate}
\end{definition}
%The general set up for instrumental variable is shown in Fig. \ref{fig:iv} (a), where the effect of $Z \rightarrow T$ and $Z \rightarrow T \rightarrow Y$ can be learned without interference from covariates. Therefore, $\textrm{Pr}(Y \mid do(T))$ can be estimated though:
In practice, IV is often implemented in a two-stage lease squares (2SLS) procedure.

\begin{figure}[t!]
\centering
\vspace{-1mm}
\includegraphics[height=0.50\textheight,trim=0 10 0 0 ,clip]{pic/he_type.pdf} %l b r t
\vspace{-1mm}
\caption{Separate-learning-counterfactual-inference, a common pattern of SCM-based causal inference for recommender systems, learns causal effect with a separate structure or multi-task framework and performs counterfactual inference during testing.}
\vspace{-3mm}
\label{fig:he_type}
\end{figure}

\begin{figure}[t!]
	\centering
    \vspace{-3mm}
	\subfloat[]{
		%\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[height=0.09\textheight,trim=0 0 0 0 ,clip]{pic/iv_ex.pdf}
		%\end{minipage}
	}
	\centering
	\subfloat[]{
		%\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[height=0.09\textheight,trim=0 0 0 0 ,clip]{pic/proxy_variable.pdf}
	}
	\centering
    \vspace{-2mm}
	\caption{(a) The causal graph of a general setup for instrumental variables, where $Z$ is an instrumental variable. (b) Proxy variables is easier to be satisfied compared with the instrumental variables.
}
    \vspace{-3mm}
	\label{fig:iv_ex}
\end{figure}


% Table generated from sheet 'confounder'
\begin{table*}[htbp]
  \centering
  % \renewcommand\arraystretch{1.5}
  \small  
  \vspace{-2mm}
  \caption{\normalsize Summary of recommendation models with confounder structure.}
  \vspace{-2mm}
  \setlength{\tabcolsep}{2.0mm}{
    \begin{tabular}{c|c|c|c|c}
    \toprule
    \textbf{Model} & \textbf{Causal method} & \textbf{Backbone model} & \textbf{Issue of concern} & \textbf{Year} \\
    \midrule
    ~\cite{huang2012exploring} & Back-door criterion & MF    & Effect of WOM recommendation & 2012 \\
    ~\cite{sharma2015estimating} & Back-door adjustment, IV & -     & Effect of recommendations & 2015 \\
    ~\cite{chaney2018algorithmic} & -     & MF, etc. & Feedback loop bias & 2018 \\
    DEMER~\cite{shang2019environment} & -     & \makecell{(RL)} & Unobserved confounding bias & 2019 \\
    CPR~\cite{yang2021top} & Back-door adjustment & (model-agnostic) & Data insufficiency & 2021 \\
    CauSeR~\cite{gupta2021causer}. & Back-door adjustment & SR-GNN~\cite{wu2019session} & Popularity bias in SBRSs & 2021 \\
    MCT~\cite{tran2021recommending} & Back-door criterion, CATE & (custom-designed) & Disability employment & 2021 \\
    DecRS~\cite{wang2021deconfounded} & Back-door adjustment & FM, NFM~\cite{he2017neuralfactorization} & Bias amplification & 2021 \\
    PDA~\cite{zhang2021causal} & Back-door adjustment & MF    & Popularity bias & 2021 \\
    CR~\cite{wang2021clicks} & Back-door criterion, TIE & \makecell{MMGCN~\cite{wei2019mmgcn}\\(multi-task)} & Clickbait & 2021 \\
    D2Q~\cite{zhan2022deconfounding} & Back-door adjustment & (custom-designed) & Duration bias & 2022 \\
    DeSCoVeR~\cite{rajanala2022descover} & Back-door adjustment & (custom-designed) & Venue recommendation & 2022 \\
    IV4Rec~\cite{si2022model} & IV    & DIN, NRHUB~\cite{wu2019neural} & Recommendation using search data & 2022 \\
    HCR~\cite{zhu2022mitigating} & Front-door adjustment & MMGCN & Unobserved confounding bias & 2022 \\
    DCR~\cite{he2022addressing} & Back-door adjustment & NFM   & \multicolumn{1}{c|}{Observed confounding bias} & 2023 \\
    CaDSI~\cite{wang2022causal} & Back-door adjustment & (custom-designed) & Observed confounding bias & 2023 \\
    DecUCB~\cite{xia2023user} & Back-door adjustment & (custom-designed, bandit) & Observed confounding bias & 2023 \\
    iDCF~\cite{zhang2023debiasing} & Proxy variable & MF & Unobserved confounding bias & 2023 \\
    CVRDD~\cite{tang2023counterfactual} & TIE & MLP(model-agnostic) & Duration bias & 2023 \\
    DML~\cite{zhang2023leveraging} & Back-door adjustment & MMoE & Duration bias & 2023 \\
    CGSR~\cite{yu2023causality} & Back-door adjustment & (custom-designed) & Shortcut paths in SBRSs & 2023 \\
    ~\cite{tsoumas2023evaluating} & Back-door adjustment, IPS &	(custom-designed, knowledge-based RS)& Digital agriculture & 2023 \\
    DDCE~\cite{wang2023dual}& - & (custom-designed) & Popularity bias & 2023 \\
    \bottomrule
    \end{tabular}}%
    \label{tab:confounder}%
    \begin{tablenotes}
    \small
    \item *Here, WOM stands for word-of-mouth, RL for reinforcement learning, and SBRS for session-based recommender system.
    \end{tablenotes}
    \vspace{-4mm}
\end{table*}%

\begin{figure}[t!]
\centering
\vspace{-1mm}
\includegraphics[height=0.13\textheight,trim=0 0 0 0 ,clip]{pic/IV4Rec.pdf} %l b r t
\vspace{-1mm}
\caption{The causal graph of IV4Rec, which reconstructs treatment $T$ by leveraging search queries $Z$ as instrumental variables to decompose treatment into causal part  $T^{ca}$ and non-causal part $T^{no}$ and combining them with different weights. $X$ is a set of unmeasurable confounders and $Y$ represents users’ interaction.}
\vspace{-1mm}
\label{fig:iv4rec}
\end{figure}


Though a popular tool, instrumental variable seems to find little application in recommender systems because of the difficulty of finding variables that satisfy the conditions of instrumental variables. As already cited above,  Sharma et al.~\cite{sharma2015estimating} utilize an instantaneous shock in direct traffic as an instrumental variable to evaluate the recommendation effect. Si et al.~\cite{si2022model}  propose a model-agnostic framework named IV4Rec that effectively decomposes the embedding vectors into two parts: the causal part indicating a user’s personal preference for an item, and the non-causal part merely reflects the statistical dependencies between users and items such as exposure mechanism and display position, with users’ search behaviors as the instrumental variable. More specifically, it modifies the traditional IV method, using the residual of the least square regression as the causal embedding instead of discarding it. The causal graph is illustrated in Fig. \ref{fig:iv4rec}.

Considering the stringent conditions often associated with IVs, a recent theoretical advancement~\cite{miao2023identifying} has been proposed to estimate treatment effects utilizing an auxiliary variable, which requires less restrictive prerequisites compared to IVs. An example causal diagram for auxiliary variables is visually represented in Fig. \ref{fig:iv_ex}(b), where $Z$ serves as a proxy variable for the unmeasurable confounder. Building on this theory, Zhang et al.\cite{zhang2023debiasing} developed the iDCF (identifiable deconfounder) to account for the unmeasured user’s socio-economic status $X$ by employing the user's consumption level as a proxy variable $Z$, a descendant of the unobserved confounder $X$ yet not directly causally associated with either treatment or outcomes. Furthermore, they leverage iVAE~\cite{khemakhem2020variational} to infer the conditional distribution of the latent confounder, thus resolving the Non-Identification issue encountered in ~\cite{wang2020causal}.





\subsubsection{The Front-door-based Approach}

The front-door adjustment~\cite{imbens2015causal} is another popular method for learning causal effects with unobserved confounders, in which we condition on a set of variables $K$ that satisfies the front-door criterion.

\begin{definition}[Front-door Criterion]
\label{def:frontdoor_criterion}
Given a pair of treatment $T$ and outcome variable $Y$, a set of variables $K$ is said to satisfy the front-door criterion if:
\begin{enumerate}
\item $K$ intercepts all directed paths from $T$ to $Y$;
\item there is no back-door path from $T$ to $K$; and
\item all back-door paths from $K$ to $Y$ are blocked by $T$.
\end{enumerate}
\end{definition}

A graph depicting the front-door criterion is shown in Fig. \ref{fig:frontdoor} (a). In practice, $K$ is usually the mediator of the causal effect $T \rightarrow Y$. With the help of $K$, the causal effect of $T$ on $Y$ can be calculated as follows:

\begin{definition}[Front-Door Adjustment)]
\label{def:frontdoor_adjustment}
If $K$ satisfies the front-door criterion relative to $(T, Y)$ and $\textrm{Pr}(T, Y) > 0$, then the causal effect of $T$ on $Y$ is given by the formula
\begin{equation}
\textrm{Pr}(Y \mid do(T)) =\sum_{K} \textrm{Pr}(Y \mid do(K)) \textrm{Pr}(K \mid do(T)) =\sum_{K} \textrm{Pr}(K \mid T) \sum_{T^{\prime}} \textrm{Pr}\left(Y \mid T^{\prime}, K\right) \textrm{Pr}\left(T^{\prime}\right).
\end{equation}
\end{definition}

\begin{figure}[t!]
	\centering
    \vspace{-3mm}
	\subfloat[Front-door path]{
		\centering
		\includegraphics[width=0.25\textwidth, trim=-10 -40 -10 0, clip]{pic/frontdoor0.pdf}
	}
	\centering
	\subfloat[HCR]{
		\centering
		\includegraphics[width=0.25\textwidth, trim=-10 0 -10 0, clip]{pic/frontdoor1.pdf}
	}
	\centering
    \vspace{-3mm}
	\caption{(a) A graphical model representing the front-door path, in which $T$ denotes the treatment, and $Y$ denotes outcomes. Unobserved confounders $X$ exist in the causal effect $T \rightarrow Y$, and $K$ are variables that satisfy the front-door criterion. (b) The Causal graph for illustrating the relationship in the HCR framework. $U$: user features, $X$: hidden confounders, $I$: item features affected by $X$, $Y$: post-click interactions, $M$: click behaviors. }
    \vspace{-6mm}
	\label{fig:frontdoor}
\end{figure}




Zhu et al.~\cite{zhu2022mitigating} propose HCR (Hidden Confounder Removal) framework to mitigate hidden confounding effects by front-door adjustment, in which user and item feature $U$ and $I$ are treatments, post-click user behaviors $Y$ are the concerned outcome, and the click feedback $K$ acts as the mediator that satisfies the front-door criterion, as Fig. \ref{fig:frontdoor} (b) shown. However, in real-world recommendation scenarios, confounding bias also exists in the estimation of the click feedback, which means it is not competent to perform the front-door adjustment. In fact, the front-door adjustment, like the IV method, finds little application in recommender systems because of the lack of eligible variables.

\subsubsection{Deconfounded Recommender Algorithms}
%Deep learning is a core technique in mainstream models of recommender systems. Therefore, some literature expands traditional neural network-based recommendation algorithms to deal with confounders under the inspiration of causal inference. 

Instead of directly introducing causal technique, some literature expands sheer recommendation algorithms to deal with confounders under the inspiration of analysis from the perspective of causal inference. For example, ~\cite{chaney2018algorithmic} modifies several traditional recommendation algorithms to explore the impact of algorithmic confounding, which has found that the data-algorithm feedback loop amplifies the homogenization of user behavior without corresponding gains in utility and also amplifies the impact of recommendation systems on item consumption.



Some works integrate reinforcement learning-based recommender systems with causal inference to tackle the confounding issue. For example, DEMER (deconfounded multi-agent environment reconstruction)~\cite{shang2019environment} is proposed following the generative adversarial training framework to model the hidden confounder, which affects both actions and rewards as an agent interacts with the environment and thus obstructs an effective reconstruction of the environment, by treating the hidden confounder as a hidden policy. In ~\cite{yang2021top}, user representations $U$ are considered as a confounder of the recommendation lists $T$ and users’ interactions $Y$ on recommendation lists. To alleviate this confounding bias, CPR (counterfactual personalized ranking framework) builds the recommender simulator to generate new training samples based on the causal graph. 

As for session-based recommender systems (SBRSs), Gupta et al.~\cite{gupta2021causer} propose the CauSeR (Causal Session-based Recommendations) framework to perform deconfounded training to handle popularity bias. COCO-SBRS~\cite{song2023counterfactual} adopts a self-supervised approach to pre-train a recommendation model to learn the causalities in SBRSs, so as to eliminate confounding bias and make accurate next item recommendations. In terms of GNN-based recommendations, Gas et al. infer the unobserved confounders existing in representation learning with the CVAE model~\cite{sohn2015learning} and apply it to GNN-based strategy~\cite{gao2021deconfounding}.