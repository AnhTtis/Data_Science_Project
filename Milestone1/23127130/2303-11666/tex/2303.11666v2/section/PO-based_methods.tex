\section{PO-based Methods}
\label{sec:po_based}

Many causal recommendation approaches, especially in early research, have focused on applying the potential outcome (PO) framework proposed by Donald B. Rubin ~\cite{splawa1990application, rubin1974estimating, imbens2015causal}.
%to the effect evaluation of the recommendation policy. 
These approaches primarily integrate PO-based causal inference into the optimization functions in traditional deep-learning-based methods or the reward functions in reinforcement-learning-based methods. 

Fig.~\ref{fig:classification} illustrates the strategies and objectives concerning the PO framework in the context of RS, categorizing the strategies into two main types: propensity score and causal effect. The former generally leverages estimated propensity scores from causal inference methods to adjust importance weights, while the latter concentrates on the difference between potential outcomes under treatment and control (see Definition \ref{eq:ITE}). Despite their different focuses, they are not entirely mutually exclusive. On one hand, propensity scores can be utilized to adjust the weights of samples or the weights of outcomes within causal effects. On the other hand, causal effects can be estimated in a couple of ways. One approach involves directly modeling outcomes, exemplified by fitting two separate models~\cite{radcliffe2007using, bonner2018causal} to estimate $\mathbb{E}[Y(T=1|X=x)]$ and $\mathbb{E}[Y(T=0|X=x)]$ in the CATE (refer to Equation~\ref{eq:CATE}). Alternatively, propensity score-based methods like Inverse Propensity Scoring (IPS) or Doubly Robust (DR) can be applied to weigh the potential outcome predictions.


It is essential to clarify that in this paper, models that estimate causal effects without explicitly utilizing causal structure information are classified as PO-based, whereas those explicitly incorporating causal structure information are categorized as SCM-based. 
%In this section, some related causal recommendation approaches are introduced in proper order according to the strategies shown in Figure~\ref{fig:classification}.



\subsection{Propensity Score Strategy}

Let's consider the process by which the recommendation system works, where given background variables $x \sim \textrm{Pr}(x)$, also referred to as pre-treatment variables or covariates~\cite{imbens2015causal}, (e.g., user and item features, time of the day, etc.), a recommender policy $\pi$ plays a role as a decision-making system, which makes a decision of whether to take an active treatment $t \sim \pi(t \mid x) $ (e.g., recommend an item), and the potential outcome $y \sim \textrm{Pr}(y \mid x, t) $, i.e., “reward” in the reinforcement learning context (e.g., click indicator), will be observed ~\cite{saito2022off}. For example, in online markets, information like user profile, historical consumptions, and products in the cart will be treated as context variables $x$, according to which the policy $\pi$ will produce a list of recommended items (i.e., treatment $t$), and the logged reward $y$ can be the click signal, conversions, or revenue, etc. The effectiveness of the policy $\pi$ can be evaluated through its running expected reward, formulated as:
\begin{equation}
R(\pi) :=\iiint y \textrm{Pr}(y \mid x, t) \pi(t \mid x) \textrm{Pr}(x) d x d t d y
=\mathbb{E}_{\textrm{Pr}(x) \pi(t \mid x) \textrm{Pr}(y \mid x, t)}[y].
\end{equation}

To learn the optimal policy 

\begin{equation}
\pi \in \underset{\pi \in \mathcal{\Pi}}{\arg \max } V(\pi),
\end{equation} 
where $\Pi$ means the policy class, an online A/B test will be the best choice~\cite{gomez2015netflix, kohavi2013online}, but   suffers from high expense. A substitute and common practice is offline evaluation,  by calculating an estimator $\hat{R}$ for the reward of a target policy $\pi$ using logged data $\mathcal{O}_{\pi_0}$ collected by a logging policy $\pi_0$ (which is different from $\pi$)~\cite{saito2022off}. However, like many other empirical sciences, offline evaluation is challenged with the problem of \emph{missing not at random} (MNAR). 

To address this issue, early approaches tend to predict the missing data directly~\cite{steck2010training} but have accentuated the problem of high bias~\cite{wang2019doubly, saito2021counterfactual}. Recently, many researchers have resorted to the \emph{propensity score} $e(X)$ in causality to recover the data distribution. For example, ExpoMF~\cite{liang2016modeling} first predicts the exposure matrix and then uses the exposures (i.e., propensity scores) to guide the model of the interaction matrix, which is inspired by the separation between propensity scores and potential outcomes in the PO framework. Similarly, Wang et al.~\cite{wang2018collaborative} propose SERec to integrate social exposure into collaborative filtering. A refreshing work is that Wang et al.~\cite{wang2020causal} aim to overcome the confounder issue with propensity score. They regard correlations among the interacted items as bringing indirect evidence for confounders and propose the deconfounded recommenders. They first build an exposure model to estimate the propensity score, and then use this exposure model to estimate a substitute for the unobserved confounders, conditional on which the final outcome model (specifically in ~\cite{wang2020causal}, a rating model based on matrix factorization) is trained. In addition, inspired by ~\cite{joachims2017unbiased, fang2019intervention}, Chen et al.~\cite{chen2021adapting} propose IOBM (Interactional Observation-Based Model)to estimate propensity score in interaction settings, which learns low-dimensional embeddings as a substitute for unobservable confounders. Specifically, it learns individual embeddings to capture the potential outcome information from specific exposure events. Based on individual embeddings, the interactional embeddings, which uncovers the hidden relationship among single exposure events and utilizes query context information to apply attention, are learned through the bidirectional LSTM model. Recently, the incorporation of Contrastive Learning (CL)~\cite{yu2023self, zhou2021contrastive} with propensity scores has offered new avenues to address noisy data in recommendation systems. A prominent example is the CCL (Contrastive Causal Learning) framework~\cite{zhou2023contrastive}, which innovatively employs propensity score-based sampling to generate informative positive pairs for contrastive learning tasks.

Propensity-based methods can be further divided into approaches based on inverse propensity score (IPS) and approaches based on doubly robust (DR) (Fig.\ref{fig:propensity}). One of the greatest strengths of applying propensity-based methods in RS is that most of them are unbiased and model-agnostic, simply deployed on the objective function for policy evaluation directly or for policy learning indirectly.




\subsubsection{Missing Not At Random}
\label{sec:MNAR}

In this part, we will introduce the phenomena and factors of missing not at random, to provide explanations and conclusions of challenges in recommender systems in a causal language to understand existing work better.

Recommendation algorithms often obey the missing at random (MAR)~\cite{rubin1976inference} assumption but may lead to biased prediction and suboptimal policy~\cite{little2019statistical, marlin2009collaborative}. The MAR condition essentially states that the probability that a potential outcome is missing does not depend on the value of that potential outcome and can be easily violated in recommender systems~\cite{marlin2009collaborative}. For example, on movie rating websites, movies with high ratings are less likely to be missing compared to movies with low ratings~\cite{pradel2012ranking}. The issue of missing not at random (MNAR) has been demonstrated by Marlin and Zemel ~\cite{marlin2009collaborative} and it is a phenomena stemming from \emph{selection bias} and \emph{confounding bias}~\cite{correa2019identification, wu2022opportunity}. 

\begin{figure}[t!]
    \centering
    \vspace{-3mm}
    \subfloat[User self-selection bias]{
    	\centering
    	\includegraphics[height=0.09\textheight, trim=-10 0 -10 0, clip]{pic/MNAR0.pdf}
    }
    \centering
    \subfloat[Confounding bias]{
    	\centering
    	\includegraphics[height=0.09\textheight, trim=-10 0 -10 0, clip]{pic/MNAR1.pdf}
    }
    \centering
    \vspace{-3mm}
    \caption{Causal explanation of user self-selection bias and confounding bias.}
    \vspace{-6mm}
    \label{fig:selection_confounding}
\end{figure}



Selection bias, or sampling bias, is usually discussed in the prediction task and can be further classified into model selection bias and user self-selection bias~\cite{wu2022opportunity}. For example, the case that the platform may systematically recommend pop music to younger users who may be more active on the service regardless of genre preferences~\cite{mcinerney2020counterfactual} will be regarded as model selection bias~\cite{yuan2019improving, mcinerney2020counterfactual} and can be eliminated by random recommendation. User self-selection bias~\cite{bareinboim2012controlling, elwert2014endogenous}, on the contrary, can not be removed by randomization of recommendation~\cite{correa2019identification}. It is caused by preferential exclusion of samples from the data~\cite{bareinboim2012controlling}. A typical example is a song recommender system, in which users usually rate songs they like or dislike and seldom rate what they feel neutral about~\cite{saito2020asymmetric}. Some of the most frequently discussed biases like popularity bias~\cite{zhang2021causal, wei2021model} and exposure bias~\cite{liang2016modeling, wang2018collaborative} will lead to model selection bias, while conformity bias~\cite{zhang2021causal, zheng2021disentangling} and clickbait bias~\cite{wang2021clicks} fall under user self-selection bias as a result of user preference.




Confounding bias~\cite{hernan2002causal, pearl2009causality} arises from the confounder described in Section \ref{sec:scm}, which affects both the treatment and the outcome, illuminated in Fig.\ref{fig:selection_confounding}(b). Alternatively, it can be identified if the probabilistic distribution representing the statistical association is not always equivalent to the interventional distribution, i.e., $\textrm{Pr}(y \mid t) \neq \textrm{Pr}(y \mid do(t))$~\cite{guo2020survey}. A notable example of confounding bias is that a system trained with historical user interactions may over recommend items that the user used to like, and the user’s decision (i.e., outcome) is also affected by historical interactions~\cite{wang2021deconfounded}. 




Both biases can lead to invalid estimates of causality from the data, and they are not mutually exclusive because selection bias does not explicitly involve causality. Many model selection biases, including popularity bias and exposure bias, are also confounding biases. As for user self-selection bias, the model in Fig. \ref{fig:selection_confounding} (a) gives an illustration of its causal nature in which $S$ is a variable affected by both $T$ (treatment) and $Y$ (outcome), indicating entry into the data pool~\cite{bareinboim2012controlling}. Therefore, confounding bias is significantly different from user self-selection bias from the causal perspective. The former originates from common causes, whereas the latter originates from common outcomes~\cite{elwert2014endogenous}. The former stems from the systematic bias introduced during the treatment assignment, while the latter comes from the systematic bias during the collection of units into the sample~\cite{correa2019identification}.


\begin{figure*}[t!]
\centering
\vspace{-1mm}
\includegraphics[height=0.55\textheight,trim=10 0 10 0 ,clip]{pic/propensity_illustration.pdf} %l b r t
\vspace{-3mm}
\caption{Evolutionary Timeline of Propensity Score Strategies in Recommendations.}
\vspace{-6mm}
\label{fig:propensity}
\end{figure*}



\subsubsection{Inverse Propensity Score}
\label{sec:ips}
Inverse Propensity Score (IPS)~\cite{horvitz1952generalization, rosenbaum1987model, rosenbaum1983central,little2019statistical}, also named as inverse propensity weighting (IPW), or inverse propensity of treatment weighting (IPTW), is one of the favorite counterfactual techniques and has inspired a lot of causal inference methods in RS, especially for unbiased learning~\cite{joachims2017unbiased}. Propensity score is the probability of receiving the treatment given covariates $X$, formulated as:
\begin{equation}
e_{\pi}(X) = \textrm{Pr}_{\pi}(T=1 \mid X).
\end{equation}
IPS assigns a weight $w$ to each sample:
\begin{equation}
w=\frac{t}{e(x)}+\frac{1-t}{1-e(x)},
\end{equation}
which indicates the inverse probability of receiving the \emph{observed} treatment and control. The unbiasedness of IPS can be proven~\cite{rosenbaum1987model}. More specifically, for the reward estimation of recommendation policy, IPS adjusts the distribution of background features in the logged dataset to be consistent with that during $\pi$ tests online, formulated as:
\begin{equation}
\hat{R}_{\mathrm{IPS}}\left(\pi; \mathcal{O}_{\pi_0}\right):=\frac{1}{\mathcal{O}_{\pi_0}} \sum_{k=1}^{|\mathcal{O}_{\pi_0}|} \frac{e_{\pi}(X)}{e_{\pi_0}(X)} \cdot y_k = \frac{1}{\mathcal{O}_{\pi_0}} \sum_{k=1}^{|\mathcal{O}_{\pi_0}|} \frac{\textrm{Pr}_{\pi}(T=1 \mid X)}{\textrm{Pr}_{\pi_0}(T=1 \mid X)} \cdot y_k,
\end{equation}
where we assume that only positive feedback is taken into account, and $w= \frac{e_{\pi}(X)}{e_{\pi_0}(X)} $ is the ratio of the evaluation and logged policies. Note that in most applications in RS, IPS is model-agnostic, applied to the training objective function for policy evaluation directly or for policy learning indirectly.



\begin{table*}[pt]
  \small
  \centering
  \vspace{-1mm}
  \caption{\normalsize Summary of propensity score strategies for recommendation.}
  \vspace{-2mm}
  \setlength{\tabcolsep}{1.5mm}{
    \begin{tabular}{c|c|c|c|c|c}
    \toprule
    \textbf{Category} & \textbf{Model} & \textbf{Causal method} & \textbf{Backbone model} & \textbf{Issue of concern} & \textbf{Year} \\
    \midrule
    \multicolumn{1}{c|}{\multirow{6}[2]{*}{\makecell{Approach \\ Inspired by \\Propensity\\ Score}}} & ExpoMF~\cite{liang2016modeling} & Propensity score & MF  & Exposure bias & 2016 \\
          & SERec~\cite{wang2018collaborative} & Propensity score & MF  & Social recommendation & 2018 \\
          & Dcf~\cite{wang2020causal} & Propensity score & MF  & Unobserved confounding bias  & 2020 \\
          & CNFI~\cite{zhang2021causalneural} & Propensity score & MF    & Implicit feedback & 2021 \\
          & IOBM~\cite{chen2021adapting} & Propensity score & Bi-LSTM~\cite{graves2013speech}  & Interactional observation bias & 2021 \\
          & CCL~\cite{zhou2023contrastive} & Propensity score & (custom-designed) & Unobserved confounding bias  & 2023 \\
    \midrule
    \multirow{19}[2]{*}{\makecell{Approach\\ with Inverse\\ Propensity \\Score (IPS)}} & MF-IPS~\cite{schnabel2016recommendations} & IPS, SNIPS & MF  & Selection bias & 2016 \\
          & PBM~\cite{joachims2017unbiased} & IPS   & SVM-Rank~\cite{joachims2002optimizing, joachims2006training} & Position bias & 2017 \\
          & PieceNCIS, PointNCIS~\cite{gilotte2018offline} & CIPS, SNIPS & - & Offline A/B testing & 2018 \\
          & ~\cite{mehrotra2018towards} & IPS   & (reinforcement learning) & Fairness & 2018 \\
          & Multi-IPW~\cite{zhang2020large} & IPS   & Multi-task MLP & Selection bias & 2019 \\
          & CPBM~\cite{fang2019intervention} & IPS   & SVM-Rank & Selection bias & 2019 \\
          & ULRMF,ULBPR~\cite{sato2019uplift} & IPS, SNIPS, ATE   & MF  & Uplift & 2019 \\
          & DLCE~\cite{sato2020unbiased} & CIPS  & MF  & Unobserved confounding bias  & 2020 \\
          & Rel-MF~\cite{saito2020unbiased} & CIPS  & MF  & Unobserved confounding bias  & 2020 \\
          & ~\cite{christakopoulou2020deconfounding} & IPS   & Multi-task DNN & Observed confounding bias & 2020 \\
          & RIPS~\cite{mcinerney2020counterfactual} & RIPS  & (model-agnostic) & Slate recommendation & 2020 \\
          & ACL-~\cite{xu2020adversarial} & IPS   & \makecell{(adversarial learning)} & Identifiability & 2020 \\
          & UR-IPW~\cite{zhang2021user} & SNIPS & Multi-task MLP & \makecell{Post-click revisit effect\\ \&selection bias} & 2021 \\
          & ~\cite{li2021debiasing} & IPS   & (model-agnostic) & Domain bias & 2021 \\
          & CBDF~\cite{zhang2021counterfactual} & IPS   & (reinforcement learning) & Delayed feedback & 2021 \\
          & RD\&BRD~\cite{ding2022addressing} & \makecell{IPS/DR/\\ AutoDebias~\cite{chen2021autodebias}} & MF  & Unobserved confounding bias  & 2022 \\
          & CET~\cite{cai2022hard} & IPS   & BERT  & False negative  & 2022 \\
          & CAFL~\cite{krauth2022breaking} & IPS   & MF  & Feedback loop & 2022 \\
          & RIIPS~\cite{liu2022practical} & RIIPS  & Two-tower structure  & Selection bias & 2022 \\
          & DENC~\cite{li2022causal} & IPS   & (custom-designed) & Selection bias & 2023 \\
    \midrule
    \multirow{10}[2]{*}{\makecell{Approach \\with Doubly\\ Robust}} & Propensity-free DR~\cite{yuan2019improving} & DR    & FFM~\cite{yuan2019one} & Selection bias & 2019 \\
          & DR-JL~\cite{wang2019doubly} & DR & MF & Selection bias & 2019 \\
          & Multi-DR~\cite{zhang2020large} & DR    & Multi-task MLPDNN & Selection bias & 2020 \\
          & MRDR-DL~\cite{guo2021enhanced} & MRDR  & MF  & Selection bias & 2021 \\
          & Cascade-DR~\cite{kiyohara2022doubly} & Cascade-DR & MF  & High variance of RIPS~\cite{mcinerney2020counterfactual} & 2022 \\
          & ASPIRE~\cite{mondal2022aspire} & DR, ATE    & LightGBM~\cite{ke2017lightgbm} & Uplift & 2022 \\
          & DRIB~\cite{xiao2022towards} & DR    & MF  & Unobserved confounding bias  & 2022 \\
          & DR-BIAS, DR-MSE~\cite{dai2022generalized} & DR & FM & Selection bias & 2022 \\
          & CDR~\cite{song2023cdr} & DR & MF & Selection bias & 2023 \\
          & CF-MTL~\cite{li2023should} & CATE, IPS, DR & (custom-designed) & Personalized incentive policy & 2023 \\
    \bottomrule
    \end{tabular}}%
    \label{tab:propensity}%
    \vspace{-6mm}
\end{table*}%


%A lot of IPS-based recommendation focuses on data debiasing in user interactions, mainly selection bias ~\cite{schnabel2016recommendations,saito2020unbiased,sato2020unbiased,zhang2021user,sato2021online, wu2021unbiased, li2022causal}. For example, ~\cite{schnabel2016recommendations} is a representative work adopting IPS to recommender system for the elimination of selection bias, in which the recommendation algorithm is based on matrix factorization and propensity scores are estimated via naive Bayes or logistic regression. Similarly account for selection bias, Saito et al. estimates the exposure propensity for each user-item pair~\cite{saito2020unbiased} and Sato et al. proposes the DLCE (Debiased Learning for the Causal Effect) model with IPS-based estimators to evaluating unbiased ranking uplift ~\cite{sato2020unbiased}. Unbiased IPS-based uplift is also concerned by ~\cite{sato2019uplift}. In addition, ~\cite{zhang2021user} proposes UR-IPW (User Retention Modeling with Inverse Propensity Weighting) to model revisit rate estimation accounting for the selection bias problem and ~\cite{li2021debiasing} adjusts domain weights based on IPS to reduce domain bias. Though IPS-based methods do not require an explicit analysis of the causal correlation between variables, some works~\cite{christakopoulou2020deconfounding, mcinerney2020counterfactual, ding2022addressing} still discuss causal graphs as a good guide to accurate model. For example, Ding et al.~\cite{ding2022addressing} leverage a causal graph to explain the risk of unmeasurable confounders on the accuracy of propensity estimation and propose RD (Robust Deconfounder) with the sensitivity analysis, obtaining the bound of propensity score to enhance the robustness of methods against unmeasured confounders. 

Much IPS-based recommendation focuses on data debiasing in user interactions, mainly selection bias ~\cite{schnabel2016recommendations,saito2020unbiased,sato2020unbiased,zhang2021user,sato2021online, zhang2021causalneural, wu2021unbiased, li2022causal}. For example, ~\cite{schnabel2016recommendations} is a representative work adopting IPS to recommender system for the elimination of selection bias, in which the recommendation algorithm is based on matrix factorization and propensity scores are estimated via naive Bayes or logistic regression. Similarly, Saito et al.~\cite{saito2020unbiased} estimate the exposure propensity for each user-item pair and Sato et al.~\cite{sato2020unbiased} propose the DLCE (Debiased Learning for the Causal Effect) model with IPS-based estimators to evaluating unbiased ranking uplift. Unbiased IPS-based uplift is also concerned by ~\cite{sato2019uplift}. In addition, ~\cite{zhang2021user} proposes UR-IPW (User Retention Modeling with Inverse Propensity Weighting) to model revisit rate estimation accounting for the selection bias problem and ~\cite{li2021debiasing} adjusts domain weights based on IPS to reduce domain bias. Though IPS-based methods do not require an explicit analysis of the causal correlation between variables, some works~\cite{christakopoulou2020deconfounding, mcinerney2020counterfactual, ding2022addressing} still discuss causal graphs as an excellent guide to accurate model. For example, Ding et al.~\cite{ding2022addressing} leverage a causal graph to explain the risk of unmeasurable confounders on the accuracy of propensity estimation and propose RD (Robust Deconfounder) with the sensitivity analysis, obtaining the bound of propensity score to enhance the robustness of methods against unmeasured confounders. Li et al.~\cite{li2022causal} construct the DENC (De-bias Network Confounding in Recommendation). This causal graph-based recommendation framework disentangles three determinants for the outcomes, including inherent factors, social network-based confounder and exposure, and estimates each of them with a specific component, respectively. 
%but Christakopoulou et al. still discuss the causal graph of user satisfaction and treat response rate as a confounding factor before using IPS to debias user satisfaction estimation from response bias~\cite{christakopoulou2020deconfounding}. 
By the way, there are some works~\cite{christakopoulou2020deconfounding, cai2022hard, zhang2021user} integrate multi-task models with IPS to learn propensity scores and user interactions simultaneously.





In addition to debiasing, some IPS-based methods are dedicated to addressing other issues that abound in RS~\cite{mehrotra2018towards, zhang2021counterfactual, krauth2022breaking}. For example, Mehrotra et al.~\cite{mehrotra2018towards} proposes an unbiased estimator of user satisfaction based on IPS to jointly optimize for supplier fairness and consumer relevance. Besides, the CBDF (Counterfactual Bandit with Delayed Feedback) algorithm ~\cite{zhang2021counterfactual} re-weights the observed feedback with importance sampling, which is determined by a survival model to deal with delayed feedbacks. The CAFL (causal adjustment for feedback loops) ~\cite{krauth2022breaking} extends the IPS estimator to break feedback loops. 

Despite the unbiasedness strength of IPS, the inaccurate estimation of the unknown propensity $e(x)$ or sample weight, which results in high variance~\cite{gilotte2018offline}, becomes the biggest obstacle to achieving it. To alleviate this problem, modified versions of IPS have been proposed to control variance and applied to RS, including Self Normalized IPS ~\cite{schnabel2016recommendations, zhang2021user}, Clipped IPS~\cite{saito2020unbiased, sato2020unbiased}, Reward interaction IPS~\cite{mcinerney2020counterfactual}, and Regularized per-Item IPS~\cite{liu2022practical}. Self Normalized Inverse Propensity Scoring (SNIPS)~\cite{swaminathan2015self} rescales the estimate of the original IPS without any parameters to reduce the high variance, which is:
\begin{equation}
\hat{R}_{\mathrm{SNIPS}}\left(\pi; \mathcal{O}_{\pi_0}\right):=
\left(\sum_{k=1}^{|\mathcal{O}_{\pi_0}|} \frac{e_{\pi}(X)}{e_{\pi_0}(X)}\right)^{-1}
\sum_{k=1}^{|\mathcal{O}_{\pi_0}|} \frac{e_{\pi}(X)}{e_{\pi_0}(X)} \cdot y_k,
\end{equation}
and is introduced to RS by works like ~\cite{schnabel2016recommendations} and ~\cite{zhang2021user} to alleviate selection bias. Clipped IPS (CIPS)~\cite{bottou2013counterfactual, saito2020unbiased, sato2020unbiased}, or Capped IPS, tightens the bound of the sample weight by introducing a scalar hyperparameter $\lambda_{\mathrm{CIPS}}$, formulated as:
\begin{equation}
\hat{R}_{\mathrm{CIPS}}\left(\pi; \mathcal{O}_{\pi_0}\right):=
\frac{1}{\mathcal{O}_{\pi_0}}
\sum_{k=1}^{|\mathcal{O}_{\pi_0}|} \min \left\{ \frac{e_{\pi}(X)}{e_{\pi_0}(X)} , \lambda_{\mathrm{CIPS}} \right\}\cdot y_k,
\end{equation}
which has a lower variance but gives away its unbiasedness. Expanding upon the groundwork established by NCIPS~\cite{swaminathan2015self}, which amalgamated SNIPS and CIPS, the study by Gilotte et al.~\cite{gilotte2018offline} advances PieceNCIS and PointNCIS as enhancements that utilize contextual information to refine bias modeling. McInerney et al.~\cite{mcinerney2020counterfactual} loosen the SUTVA assumption and propose Reward interaction IPS (RIPS) for sequential recommendations, which assumes a causal model in which users interact with a list of items from the top to the bottom. RIPS uses iterative normalization and lookback to estimate the average reward and achieves a better bias-variance trade-off than IPS. In addition to high variance, violation of the Unconfoundedness assumption is another challenge of utilizing IPS in RS. That is, the treatment mechanism is \emph{identifiable}~\cite{glymour2016causal, mohan2021graphical} from observed covariates due to the existence of unobserved ones, which leads to the inaccurate estimate of propensity score and the disagreement between the online and offline evaluations. To address the uncertainty brought by the identifiability issue, ~\cite{xu2020adversarial} proposes minimax empirical risk formulation, which can be converted to an adversarial game between two recommendation models via duality arguments and relaxations.

More recently, Liu et al.~\cite{liu2022practical} propose Regularized per-item IPS (RIIPS) with an additional penalty function that constrains the difference in recommended outcomes between the deployed system and the new system so that the explosion of propensity scores can be avoided.





\subsubsection{Doubly Robust}
Doubly Robust (DR)~\cite{funk2011doubly, dudik2014doubly, jiang2016doubly, wang2019doubly} is another powerful and effective causal method account for the MNAR issue. To understand DR, let us consider the two common-used approaches to mitigate against MNAR: direct method (DM)~\cite{beygelzimer2009offset} and IPS~\cite{saito2021evaluating}. The former designs a model (linear regression, deep neural network, etc.) to directly learn the missing outcomes based on the observed data, which has low variance due to the advantage of supervised learning but suffers from high bias caused by unmet IID assumptions, denoted as~\cite{saito2021evaluating}:
\begin{equation}
\hat{R}_{\mathrm{DM}}\left(\pi_{0} ; \mathcal{O}_{\pi_{0}}, \hat{y}\left(x_{k}, t\right) \right):=\frac{1}{|\mathcal{O}_{\pi_{0}}|} \sum_{k=1}^{|\mathcal{O}_{\pi_{0}}|}\textrm{Pr}_{\pi}\left(t=1 \mid x_{k}\right) \hat{y}\left(x_{k}, t\right),
\end{equation}
where $\hat{y}\left(x, t\right)$ is the estimated outcomes. The latter, though unbiased theoretically, often causes training losses to oscillate stemming from the inverse of propensity with high variance~\cite{thomas2016data}. What DR does is to combine the direct method and IPS, which takes advantage of both and overcomes their limitations:
\begin{equation}
\hat{R}_{\mathrm{DR}}\left(\pi ; \mathcal{O}_{\pi_0}, \hat{r}\right) := \hat{R}_{\mathrm{DM}}\left(\pi ; \mathcal{O}_{\pi_0}, \hat{y}\left(x_{k}, t\right) \right) + 
\frac{1}{|\mathcal{O}_{\pi_{0}}|} \sum_{k=1}^{|\mathcal{O}_{\pi_{0}}|} 
\frac{e_{\pi}(X)}{e_{\pi_0}(X)} \left(y_{k}-\hat{y}\left(x_{k}, t_{k}\right)\right). 
\end{equation}
DR uses the estimated outcomes to decrease the variance of IPS. It is also \emph{doubly robust} in that it is consistent with the policy reward value if either the propensity scores or the imputed outcomes are accurate for all user-item pairs~\cite{wang2019doubly, saito2021evaluating}. By the way, advanced versions like Switch-DR~\cite{wang2017optimal} and DRos (Doubly Robust with Optimistic Shrinkage)~\cite{su2020doubly} are proposed to further control the variance.

Based on the above advantages, DR has found an increasingly wide utilization in RSs~\cite{yuan2019improving, wang2019doubly, zhang2020large, guo2021enhanced, kiyohara2022doubly, mondal2022aspire, xiao2022towards, dai2022generalized, song2023cdr, li2023should}. Wang et al.~\cite{wang2019doubly} utilize DR for unbiased RS prediction and further propose a joint learning approach that simultaneously learns rating prediction and propensity to guarantee a low prediction inaccuracy at inference time. Yuan et al.~\cite{yuan2019improving} propose a propensity-free doubly robust method to address the issue that samples with low propensity scores are absent in the observed dataset. Zhang et al.~\cite{zhang2020large} propose Multi-DR based on a multi-task learning framework to address selection bias and data sparsity issues in CVR estimation. Gun et al.~\cite{guo2021enhanced} propose the MRDR (more robust doubly robust) estimator to further reduce the variance caused by inaccurate imputed outcomes in DR while retaining its double robustness. In addition, Kiyohara et al.~\cite{kiyohara2022doubly} expand previous RIPS to Cascade Doubly Robust estimator, which has the same user interaction assumption as RIPS. Xiao et al.~\cite{xiao2022towards} propose an information bottleneck-based approach to effectively learn the DR estimator for the estimation of recommendation uplift, with the hope of a better trade-off between the bias and variance of propensity scores. Dai et al.~\cite{dai2022generalized} learns imputation with balancing the variance and bias of DR loss. More recently, Song et al.~\cite{song2023cdr} filter imputation data through examination of their mean and variance, in order to reduce poisonous imputations that significantly deviate from the truth and impair the debiasing performance.

\subsection{Causal Effect Strategy}

%The most critical and fundamental role of causal inference is to estimate the causal effects from observational data, which has a variety of applications in real-world recommender systems. Some works are dedicated to enhancing the causal effect of a recommender policy, i.e., uplift, and therefore deploy the causal effect as a direct or indirect optimization goal for higher platform benefits. Other works introduce treatment effects to recommender systems for beyond-uplift objectives.
%$ATE$ or $CATE$ to improve representation learning and alleviate selection bias. 
% Causal inference plays an essential and foundational role in estimating causal effects from observational data. In the industry, one of the most prevalent and extensively utilized applications of causal inference is to estimate and enhance the treatment effect of a recommender policy on some customer outcome, namely uplift. 
The most critical and fundamental role of causal inference is to estimate the causal effects from observational data, which has a variety of applications in real-world recommender systems. Some works are dedicated to estimating and enhancing the treatment effect of a recommender policy on specific customer outcomes, namely uplift~\cite{gutierrez2017causal}. In such scenarios, the causal effect is typically implemented as either a direct or indirect optimization goal, aiming to maximize platform benefits. Additionally, treatment effects extend to other application areas in recommender systems, serving purposes beyond uplift.

It is crucial to highlight that within the PO framework, the causal relationships between variables are not the focal point while calculating causal effect, and all variables affecting potential outcomes except treatment will be treated as covariates.





\begin{table*}[pt]
  \small
  \centering
  % \renewcommand\arraystretch{1.5}
  \vspace{-3mm}
  \caption{\normalsize Summary of causal effect strategies for recommendation.}
  \vspace{-3mm}
  \setlength{\tabcolsep}{2.4mm}{
    \begin{tabular}{c|c|c|c|c|c}
    \toprule
    \textbf{Category} & \textbf{Model} & \textbf{Causal method} & \textbf{Backbone model} & \textbf{Issue of concern} & \textbf{Year} \\
    \midrule
    \multirow{5}[2]{*}{\makecell{Causal \\effect\\ for\\ Uplift}} & ULRMF, ULBPR~\cite{sato2019uplift} & IPS, SNIPS, ATE & MF  & \multirow{5}[2]{*}{Uplift} & 2019 \\
          & ~\cite{goldenberg2020free}  & CATE  & Xgboost~\cite{chen2016xgboost} &       & 2020 \\
          & AUUC-max~\cite{betlei2021uplift} & CATE  & \makecell{Linear\\/Wide \& Deep\\  }  &       & 2021 \\
          & CausCF~\cite{xie2021causcf} & CATE  & MF  &       & 2021 \\
          & ASPIRE~\cite{mondal2022aspire} & DR, ATE & LightGBM~\cite{ke2017lightgbm} &       & 2022 \\
    \midrule
    \multirow{6}[2]{*}{\makecell{Causal \\effect\\ beyond\\ Uplift}} & ~\cite{rosenfeld2017predicting} & ITE   & \makecell{Linear/regularized \\kernel methods} & Domain adaptation & 2017 \\
          & CausE~\cite{bonner2018causal} & ITE   & MF  & Domain adaptation & 2018 \\
          & ~\cite{mehrotra2020inferring} & TE    & \makecell{Structural \\state-space model~\cite{brodersen2015inferring}} & \makecell{Causal effect of\\a new track release} & 2020 \\
          & CACF~\cite{zhang2021causally} & ITE   & (custom-designed) & Unobserved confounding bias & 2021 \\
          & MCRec~\cite{yao2022device} & CATE  & DIN~\cite{zhou2018deep} & \makecell{Device-cloud \\recommendation} & 2022 \\
          & LRIR~\cite{tran2022most} & ITE, ATE & (custom-designed) & Disability employment & 2022 \\
    \bottomrule
    \end{tabular}}%
  \label{tab:effect}%
  \vspace{-6mm}
\end{table*}%


\subsubsection{Causal Effect for Uplift}

Uplift, denoting the causal effect of recommendations, refers to the increase in user interactions purely caused by recommendations. Typical evaluations of recommender systems regard positive user interactions as a success. However, a subset of these interactions might persist even in the absence of recommendations. This assertion is substantiated by the conclusion of Sharma et al.~\cite{sharma2015estimating}, which indicates that more than 75\% of click-throughs would still occur in the absence of recommendations. For marketing campaigns where Return on Investment (ROI) is paramount, targeting 'voluntary buyers' — individuals who would interact with or without any recommendations — is deemed unnecessary. Therefore, the industry regards uplift as a valuable metric for recommendations in expectation of higher rewards.


%Let us consider the binary treatments of a user in the two given scenarios (recommended or not) to explain the concept of uplift in the PO framework, and the user  The causal effects will fell into four groups: 

%The Two-Model approach ~\cite{radcliffe2007using, nassif2013uplift} directly models $\mathbb{E}[Y(T=1|X=x)]$ and \mathbb{E}[Y(T=0|X=x)]$ separately using the treatment group data and the control group data, respectively， and $CATE$, the difference between them, is treated as uplift~\cite{gutierrez2017causal}. This approach is simple but may suffer from accumulative error in the subtraction of two separate predictions. Other causal approaches with traditional machine learning methods for uplift estimation are proposed in ~\cite{jaskowski2012uplift, jaskowski2012uplift, radcliffe2011real}.

It is a natural application to introduce the causality concepts such as ATE and CATE for uplift modeling since the definition of uplift is a counterfactual problem and consistent with the objective of causal effect estimation~\cite{yamane2018uplift, zhang2021unified, gutierrez2017causal}. Causal approaches with traditional machine learning methods for uplift estimation include two-model approach~\cite{radcliffe2007using, nassif2013uplift}, transformed outcome~\cite{jaskowski2012uplift} and uplift trees~\cite{radcliffe2011real, rzepakowski2012decision}. Regarding recommender systems, uplift estimation on online A/B testing suffers from the high expense and large fluctuations due to user self-selection bias~\cite{sato2021online}, while uplift estimated offline is bedeviled by a wide variety of biases that could lead to MNAR. In order to deal with these issues, much of the literature has been published. Sato et al.~\cite{sato2019uplift} utilize SNIPS-based ATE to accomplish offline uplift-based evaluation. Goldenberg et al.~\cite{goldenberg2020free} leverage the Retrospective Estimation technique that relies solely on data with positive outcomes for CATE-based uplift modeling, which makes it especially suited for many recommendation scenarios where only the treatment outcomes are observable. ~\cite{betlei2021uplift} learns a model that directly optimizes an upper bound on AUUC, a popular uplift metric based on the uplift curves and unified with ATE~\cite{yamane2018uplift}. In addition,  CausCF~\cite{xie2021causcf} extends the classical MF to the tensor factorization with three dimensions—user, item, and treatment effect for better uplift performance. CF-MTL~\cite{li2023should} accounts for whether users actively accept the treatment, leading to a more granular classification of users, and then estimates the probability for each user type within a multi-task learning framework. It is worth mentioning that in the uplift modeling literature~\cite{diemert2018large,gutierrez2017causal, zhang2021unified}, there are two closely related metrics for uplift modeling, uplift and Qini curves, the latter of which is evaluated based on the ranking of conditional treatment effect estimations. 




\subsubsection{Causal Effect beyond Uplift}
%It is a natural application to introduce the concepts such as $ATE$ and $CATE$ for causal effect estimation as it is consistent with the objective of causal effect estimation. For example, early studies have explored the influence of social presence through causal effect estimation through small-scale questionaires.
%Causal effect has its natural supremacy of explainability and trusts over statistic-based techniques. For example, 
There are some other impressive recommendation works with causal effect~\cite{mehrotra2020inferring, zhang2021causally, rosenfeld2017predicting, bonner2018causal, yao2022device, tran2022most}. For example, ~\cite{mehrotra2020inferring} adapts a Bayesian model to infer the causal impact of new track releases, which may be an essential consideration in the design of music recommendation platforms. ~\cite{zhang2021causally} minimizes the distance between the traditional attention weights in the recommendation method and the ITE to reflect the true impact of the features on the interactions. ~\cite{rosenfeld2017predicting} and ~\cite{bonner2018causal} frames causal inference as a domain adaptation problem and leverages ITE with a large sample of biased data and a small sample of unbiased data to eliminate the bias problems, which are described in more detail in \ref{sec:domain_adaption}.


%~\cite{jesson2020identifying} incorporates uncertainty estimating into the design of systems for CATE inference to deal with covariate shift and the violation of the positivity assumption

\subsection{Why Potential Outcomes Framework?}

% to the effect evaluation of the recommendation policy. These methodologies primarily integrate PO-based causal inference into the optimization functions in traditional deep-learning-based methods or the reward functions in reinforcement-learning-based methods.
% PO framework从推荐系统出现至今依然流行，原因在于它与ab testing密切相关。Online ab testing通过随机实验对不同recommender policy之间的性能。具体来说，a pool of users on he platform 会被随机分为两个群体 and each of them is exposed to one of the policies~\cite{gilotte2018offline}. 实验结束后，根据收入、点击数等metrics进行比较来决定未来采用的policy。AB test之所以能生效，是因为理想的随机化使得所有同时影响treatment到effect的混杂因子全部失效，从而客观评估策略对potential outcome的影响~\cite{pearl2009causality}。（也许可以加图）但现实中的AB testing往往并不能实现理想的随机化，例如样本量太少导致样本分布与整体分布不一致。这时propensity score的方法就能重新调整样本权重，消除选择偏差。

The PO framework has maintained its popularity in the realm of recommender systems since its inception due to its close association with A/B testing. Online A/B testing evaluates the performance of two different recommender policies through randomized experiments. Specifically, a user pool on the platform is randomly divided into a treatment group and a control group, with each group being exposed to one of the policies~\cite{gilotte2018offline}. Upon completion of the experiment, metrics such as revenue and click-through rates are compared to determine the policy to be adopted for future use. As illustrated in Fig.  \ref{fig:abtest}, the efficacy of A/B testing stems from the ideal randomized controlled trial (RCT) that disables all the confounders simultaneously affecting the treatment and the outcomes, thereby leading to a pure assessment of the policy's treatment effect on potential outcomes~\cite{pearl2009causality}. In practice, however, A/B testing often fails to achieve ideal randomization due to issues such as insufficient sample sizes leading to distributions that do not match the overall population. In such cases, methods like IPS from the PO framework can adjust sample weights, thus mitigating selection biases.

\begin{figure}[t!]
	\centering
    \vspace{-4mm}
	\subfloat[The real world]{
		%\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=0.35\textheight, trim=0 5 0 0, clip]{pic/abtest_before.pdf}
		%\end{minipage}
	}
	\centering
	\subfloat[The world simulated by a randomized controlled trial]{
		%\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=0.35\textheight, trim=0 0 0 5, clip]{pic/abtest_after.pdf}
	}
	\centering
    \vspace{-3mm}
	\caption{(a) Confounders such as item popularity and user's economic status influence both the likelihood of an item being recommended by the system (i.e., the treatment) and the user's final interaction (i.e., the potential outcome). (b) An ideal randomized controlled trial (RCT) disables the effect of confounders on the receipt of treatment, allowing for an accurate estimation of the desired treatment effect.}
    \vspace{-6mm}
	\label{fig:abtest}
\end{figure}

% 但online ab test通常耗时且成本高，因此 offline A/B test被用于快速高效地估计推荐policy的效果。离线数据基于当前的推荐系统（称为logging policy）收集得到，因此我们不能直接实用目标policy在其上的预估结果，因为这些数据并不是在目标policy下收集的。instead，我们需要使用如IPS等方法调整每个样本的权重。而对于营销活动来说，除了准确的估计policy的效应，还需要估计每个用户的uplift，精准地找到营销受众。CATE-based uplift modelling可以识别出voluntary buyers and the persuadables, who would only purchase as a response to the incentive，实现营销目标。

Due to the time-intensive and costly nature of online A/B testing, offline A/B testing serves as a more expedient and cost-effective approach to estimate the efficacy of recommended policies. Offline data are accumulated using the current recommendation system, referred to as the logging policy; hence, we cannot use the direct estimation of the target policy on offline data since it was not collected under the conditions of the target policy. Instead, it is necessary to re-weight the importance of samples to align with the data distribution that would be expected under the target policy. Moreover, in the context of marketing campaigns, beyond accurately estimating the effect of a policy, it is crucial to calculate each user's uplift to precisely identify the target users. CATE-based uplift modeling is adept at distinguishing between voluntary buyers and the persuadables—who would only interact in reaction to an incentive—thus fulfilling marketing objectives.

%上述提到的政策评价指标同样可以转化到推荐算法的优化目标（如损失函数），从而实现模型优化目标与评估目标的一致性。正因为policy evaluation这项需求伴随着推荐系统的出现长久存在，且不受推荐算法技术的影响，而它本身是一个如此典型的与po framework契合的反事实问题，即在估计treatment Effect时，我们永远不知道unit在接受另一种treatment后真实outcome，才给了po framework在推荐系统中大展拳脚的舞台。

The policy evaluation methods mentioned above can also be transformed into the optimization objectives (e.g., the loss function) of recommendation algorithms, thereby aligning the model's optimization targets with evaluation goals. 

Policy evaluation is fundamentally crucial for several reasons: (1) its sustained significance since the inception of recommender systems; (2) its steadfastness amidst the development of recommendation algorithm technology shift; and (3) its archetypal alignment with the PO framework as a typical counterfactual question, where the true outcome of a unit under an alternative treatment remains perpetually indeterminate. Therefore, the PO framework seizes a prominent stage for deployment within recommender systems.
