\section{General Counterfactuals-based Methods}
\label{sec:general_counterfactual}
Some causal recommender approaches are established based on the general concept of counterfactuals, the world that does not exist but can be reasoned with some fundamental law and human intuition. In this section, we will introduce related strategies from the perspective of recommender issues they try to address, including domain adaptation, data augmentation, fairness, and explanation.

\subsection{Domain Adaptation}
\label{sec:domain_adaption}
RSs are trained and evaluated offline with the supervision of previously-collected data, which usually suffers from selection bias and confounding bias. It results in a gap between the training goal and the true recommendation objective, and, therefore, a sub-optimal recommender algorithm. To address this issue, we hope to evaluate the training policy on the unbiased data, which is collected from the randomized treatment policy. However, uniform data is always expensive and small-scale. To take full advantage of the uniform data, researchers train the recommender systems with a small amount of unbiased data and a large amount of biased data, with the hope of learning the counterfactual distribution of the biased data, which is both a counterfactual problem and a domain adaptation problem. 


~\cite{rosenfeld2017predicting} and ~\cite{bonner2018causal} train recommender policies on biased and unbiased data, and add regulation terms to the loss function so that the distance of parameters between the two policies in the inspiration of individual treatment effect is controllable. ~\cite{yuan2019improving} trains an unbiased imputation model to impute the labels of all observed and unobserved events in biased and unbiased data, and learns the final CTR model by combining the two data with the propensity-free doubly robust method. Further, ~\cite{liu2020general} propose KDCRec (Knowledge Distillation framework for Counterfactual Recommendation) in which the teacher network with unbiased data as input is used to guide the biased model via four approaches.


% Table generated from sheet 'general_counterfactual'
\begin{table*}[pt]
  \small
  \centering
  % \renewcommand\arraystretch{1.5}
  \vspace{-3mm}
  \caption{\normalsize Summary of recommendation models with general counterfactuals.}
  \vspace{-3mm}
  \setlength{\tabcolsep}{1.9mm}{
    \begin{tabular}{c|c|c|c|c}
    \toprule
    \textbf{Category} & \textbf{Model} & \textbf{Causal inference method} & \textbf{Backbone model} & \textbf{Year} \\
    \midrule
    \multirow{4}[1]{*}{\makecell{Domain\\adaptation}} & ~\cite{rosenfeld2017predicting} & ITE   & Linear/regularized kernel methods & 2017 \\
          & ~\cite{bonner2018causal} & ITE   & Matrix factorization  & 2018 \\
          & Propensity-free DR~\cite{yuan2019improving} & DR    & FFM   & 2019 \\
          & KDCRec~\cite{liu2020general} & ITE   & MF (knowledge distillation) & 2020 \\
    \midrule
    \multirow{4}[0]{*}{\makecell{Data\\augmentation}} & CF2~\cite{xiong2021counterfactual} & "Minimum" counterfactuals & (self-designed) & 2021 \\
          & CASR~\cite{wang2021counterfactual} & "Minimum" counterfactuals & NARM~\cite{li2017neural}, STAMP~\cite{liu2018stamp}, SASRec~\cite{kang2018self} & 2021 \\
          & CauseRec~\cite{zhang2021causerec} & Counterfactuals & \makecell{(self-designed, sequential recommendation)} & 2021 \\
          & POEM~\cite{liu2022modeling} & Counterfactuals & GCN   & 2022 \\
    \midrule
    \multirow{3}[0]{*}{Fairness} & ~\cite{li2021towards} & Counterfactuals & (self-designed) & 2021 \\
          & F-UCB~\cite{huang2022achieving} & Counterfactuals & UCB   & 2022 \\
          & CLOVER~\cite{wei2022comprehensive} & Counterfactuals & MELU~\cite{lee2019melu} & 2022 \\
    \midrule
    \multirow{2}[0]{*}{Explanation} & PRINCE~\cite{ghazimatin2020prince} & "Minimum" counterfactuals & HIN~\cite{shi2016survey} & 2020 \\
          & CountER~\cite{tan2021counterfactual} & "Minimum" counterfactuals & (black-box) & 2021 \\
          \bottomrule
    \end{tabular}}%
  \label{tab:fair_explain}%  
  \vspace{-6mm}
\end{table*}%

\subsection{Data Augmentation} 

Data augmentation is an uncontroversial counterfactual problem, such as answering the question: “what would be the user’s decision if a different item had been exposed?”. Therefore, some works are trying to integrate counterfactuals into the procedure of data augmentation.  

Xiong et al.~\cite{xiong2021counterfactual} generate new data samples by users’ feature-level preference for review-based recommendation. To generate more effective samples, they leverage the “minimum” idea in counterfactuals, learning the “minimum” change of the user feature-level preference that can “exactly” reverse the preference ranking of the user on a given item pair. For example, if slightly increasing the price attention of a user who had purchased an iPhone will make Xiaomi more attractive to her, this will be regarded as an effective counterfactual sample. Similarly, CASR (Counterfactual Data-Augmentation Sequential Recommendation)~\cite{wang2021counterfactual} generates the counterfactual sequence of items by “minimally” changing the user’s historical items, such that her currently interacted item can be “exactly” altered. 

The CauseRec (Counterfactual User Sequence Synthesis for Sequential Recommendation) proposed by Zhang et al.~\cite{zhang2021causerec} generates counterfactual data in a different way. It identifies indispensable and dispensable concepts in the historical behavior sequence. The former can represent a meaningful aspect of the user’s interest, while the latter indicates noisy behaviors that are less important in representing user interest. Therefore, it is reasonable to argue that replacing indispensable concepts in the original user sequence incurs a preference deviation of the original user representation, while replacing the dispensable ones still has a similar user representation, which CauseRec realizes through contrastive learning. Liu et al.~\cite{liu2022modeling} focus on the recommendation scenario where users are exposed with decision factor-based persuasion texts, i.e., persuasion factors, and generate new training samples by making simple but reasonable counterfactual assumptions about user behaviors, including:
\begin{itemize}
\vspace{-2mm}
    \item If a user clicks on an item without the existence of persuasion factors, the user will still be likely to click on it with a matching persuasion factor. 
    \item If a user does not click on an item with the existence of persuasion factors, the user will not click on it when the persuasion factor does not exist.
\vspace{-3mm}
\end{itemize}



\subsection{Fairness and Explanation}

The counterfactual technique is a natural tool for the evaluation of fairness since we can compare the outcome (ratings, recommendation lists, etc.) in the real world and in the counterfactual world in which only users’ sensitive features (e.g., gender and race) are intervened~\cite{huang2022achieving, wei2022comprehensive}. 
\begin{definition}[Counterfactual Fairness]
\vspace{-3mm}
A recommender model is counterfactually fair if, for any possible user $u$ with features $X=x$ and $Z=z$:
\begin{equation}
\textrm{Pr}(y \mid x, z) = \textrm{Pr}(y \mid x, do(z’))
\end{equation}
For any value $y$ and $z’$, where $Y$ denotes the potential outcome for user $u$. $Z$ are users’ sensitive features and $X$ are causally $Z$-independent features.
\vspace{-3mm}
\end{definition}
Based on the counterfactual fairness, Li et al. generate sensitive feature-independent user embeddings through adversary learning~\cite{li2021towards}. They train a predictor to learn the filtered embedding and an adversarial classifier to predict the sensitive features from the learned representation simultaneously. For the reinforcement learning-based recommendation, Huang et al. propose the F-UCB (fair causal bandit)~\cite{huang2022achieving}, picking arms from a subset of arms at each round in which all the arms satisfy counterfactual fairness constraint that users receive similar rewards regardless of their sensitive attributes. 

As for explanation, counterfactuals describe a dependency on the external facts that lead to certain outcomes, and thus allow researchers to reason about the behavior of a black-box algorithm~\cite{wachter2017counterfactual}. Literature on counterfactual explanation also resorts to the “minimum” idea in counterfactuals. For example, ~\cite{ghazimatin2020prince} presents PRINCE (Provider-side Interpretability with Counterfactual Evidence)  to search for a set of minimal actions performed by the user that, if removed, changes the recommendation to a different item, in a heterogeneous information network with users, items, and so on. To understand the point, consider the following example. If a user who has bought an iPhone and followed MacBook receives a recommendation about AirPods and would not have received it if she had not bought iPhone, PRINCE will regard the behavior “purchase of iPhone” as the explanation of the recommendation. Similarly, CountER (Counterfactual Explainable Recommendation) proposed by ~\cite{tan2021counterfactual} seeks the minimum changes of item features that exactly reverse the recommendation decision.