


\section{PO-based Method}
\label{sec:po_based}


Many causal recommendation approaches, especially in early research, focus on applying the potential outcome framework proposed by Donald B. Rubin ~\cite{splawa1990application, rubin1974estimating, imbens2015causal} to the effect evaluation of the recommendation policy, more specifically, on the optimization functions in traditional deep-learning-based methods and on the reward functions in reinforcement-learning-based methods. Figure~\ref{fig:classification} provides the strategies, the objectives
%, and the common RS scenario of the approaches
concerning the PO framework. As shown in Figure~\ref{fig:classification}, There are generally two strategies in the potential outcome framework for RS, i.e., inverse propensity score and causal effect. It is worth mentioning that in this survey, a model estimating causal effects without a causal graph will be regarded as a PO-based model, while a model with a causal graph will be regarded as an SCM-based one, though both frameworks involve the estimation of causal effects. In this section, some related causal recommendation approaches are introduced in proper order according to the strategies shown in Figure~\ref{fig:classification}.

\begin{figure*}[t!]
\centering
\vspace{-3mm}
\includegraphics[height=0.56\textheight,trim=140 0 0 0 ,clip]{pic/classification.pdf} %l b r t
\vspace{-1mm}
\caption{Strategies of the causal inference for recommendation.}
\vspace{-6mm}
\label{fig:classification}
\end{figure*}

\subsection{Propensity Score Strategy}

Let's consider the process by which the recommendation system works, where given background variables $x \sim \textrm{Pr}(x)$, also referred to as pre-treatment variables or covariates~\cite{imbens2015causal}, (e.g., user and item features, time of the day, etc.), a recommender policy $\pi$ plays a role as a decision-making system, which makes a decision of whether to take an active treatment $t \sim \pi(t \mid x) $ (e.g., recommend an item), and the potential outcome $y \sim \textrm{Pr}(y \mid x, t) $, i.e., “reward” in the reinforcement learning context (e.g., click indicator), will be observed ~\cite{saito2022off}. For example, in online markets, information like user profile, historical consumptions, and products in the cart will be treated as context variables $x$, according to which the policy $\pi$ will produce a list of recommended items (i.e., treatment $t$), and the logged reward $y$ can be the click signal, conversions, or revenue, etc. The effectiveness of the policy $\pi$ can be evaluated through its running expected reward, formulated as:
\begin{equation}
R(\pi) :=\iiint y \textrm{Pr}(y \mid x, t) \pi(t \mid x) \textrm{Pr}(x) d x d t d y
=\mathbb{E}_{\textrm{Pr}(x) \pi(t \mid x) \textrm{Pr}(y \mid x, t)}[y].
\end{equation}

To learn the optimal policy 

\begin{equation}
\pi \in \underset{\pi \in \mathcal{\Pi}}{\arg \max } V(\pi),
\end{equation} 
where $\Pi$ means the policy class, an online A/B test will be the best choice~\cite{gomez2015netflix, kohavi2013online}, but   suffers from high expense. A substitute and common practice is offline evaluation,  by calculating an estimator $\hat{R}$ for the reward of a target policy $\pi$ using logged data $\mathcal{O}_{\pi_0}$ collected by a logging policy $\pi_0$ (which is different from $\pi$)~\cite{saito2022off}. However, like many other empirical sciences, offline evaluation is challenged with the problem of \emph{missing not at random} (MNAR). 

To address this issue, early approaches tend to predict the missing data directly~\cite{steck2010training} but have accentuated the problem of high bias~\cite{wang2019doubly, saito2021counterfactual}. Recently, many researchers have resorted to the \emph{propensity score} $e(X)$ in causality to recover the data distribution. For example, ExpoMF~\cite{liang2016modeling} first predicts the exposure matrix and then uses the exposures (i.e., propensity scores) to guide the model of the interaction matrix, which is inspired by the separation between propensity scores and potential outcomes in the PO framework. Similarly, Wang et al.~\cite{wang2018collaborative} propose SERec to integrate social exposure into collaborative filtering. A refreshing work is that Wang et al.~\cite{wang2020causal} aim to overcome the confounder issue with propensity score. They regard correlations among the interacted items as bringing indirect evidence for confounders and propose the deconfounded recommenders. They first build an exposure model to estimate the propensity score and then use this exposure model to estimate a substitute for the unobserved confounders, conditional on which the final outcome model (specifically in ~\cite{wang2020causal}, a rating model based on matrix factorization) is trained. In addition, inspired by ~\cite{joachims2017unbiased, fang2019intervention}, Chen et al.~\cite{chen2021adapting} propose IOBM (Interactional Observation-Based Model)to estimate propensity score in interaction settings, which learns low-dimensional embeddings as a substitute for unobservable confounders . Specifically, it learns individual embeddings to capture the potential outcome information from specific exposure events. Based on individual embeddings, the interactional embeddings, which uncovers the hidden relationship among single exposure events and utilizes query context information to apply attention, are learned through the bidirectional LSTM model. 

Propensity-based methods can be further divided into approaches based on inverse propensity score (IPS) and approaches based on doubly robust (DR) (Fig.\ref{fig:propensity}). One of the greatest strengths of applying propensity-based methods in RS is that most of them are unbiased and model-agnostic, simply deployed on the objective function for policy evaluation directly or for policy learning indirectly.




\subsubsection{Missing Not At Random}
\label{sec:MNAR}

In this part, we will introduce the phenomena and factors of missing not at random, to provide explanations and conclusions of challenges in recommender systems in a causal language to understand existing work better.

Recommendation algorithms often obey the missing at random (MAR)~\cite{rubin1976inference} assumption but may lead to biased prediction and suboptimal policy~\cite{little2019statistical, marlin2009collaborative}. The MAR condition essentially states that the probability that a potential outcome is missing does not depend on the value of that potential outcome and can be easily violated in recommender systems~\cite{marlin2009collaborative}. For example, on movie rating websites, movies with high ratings are less likely to be missing compared to movies with low ratings~\cite{pradel2012ranking}. The issue of missing not at random (MNAR) has been demonstrated by Marlin and Zemel ~\cite{marlin2009collaborative} and it is a phenomena stemming from \emph{selection bias} and \emph{confounding bias}~\cite{correa2019identification, wu2022opportunity}. 

\begin{figure}[t!]
    \centering
    \vspace{-3mm}
    \subfloat[User self-selection bias]{
    	\centering
    	\includegraphics[height=0.09\textheight, trim=-10 0 -10 0, clip]{pic/MNAR0.pdf}
    }
    \centering
    \subfloat[Confounding bias]{
    	\centering
    	\includegraphics[height=0.09\textheight, trim=-10 0 -10 0, clip]{pic/MNAR1.pdf}
    }
    \centering
    \vspace{-3mm}
    \caption{Causal explanation of user self-selection bias and confounding bias.}
    \vspace{-6mm}
    \label{fig:selection_confounding}
\end{figure}



Selection bias, or sampling bias, is usually discussed in the prediction task and can be further classified into model selection bias and user self-selection bias~\cite{wu2022opportunity}. For example, the case that the platform may systematically recommend pop music to younger users who may be more active on the service regardless of genre preferences~\cite{mcinerney2020counterfactual} will be regarded as model selection bias~\cite{yuan2019improving, mcinerney2020counterfactual} and can be eliminated by random recommendation. User self-selection bias~\cite{bareinboim2012controlling, elwert2014endogenous}, on the contrary, can not be removed by randomization of recommendation~\cite{correa2019identification}. It is caused by preferential exclusion of samples from the data~\cite{bareinboim2012controlling}. A typical example is a song recommender system, in which users usually rate songs they like or dislike and seldom rate what they feel neutral about~\cite{saito2020asymmetric}. Some of the most frequently discussed biases like popularity bias~\cite{zhang2021causal, wei2021model} and exposure bias~\cite{liang2016modeling, wang2018collaborative} will lead to model selection bias, while conformity bias~\cite{zhang2021causal, zheng2021disentangling} and clickbait bias~\cite{wang2021clicks} fall under user self-selection bias as a result of user preference.




Confounding bias~\cite{hernan2002causal, pearl2009causality} arises from the confounder described in Section \ref{sec:scm}, which affects both the treatment and the outcome, illuminated in Figure\ref{fig:selection_confounding}(b). Alternatively, it can be identified if the probabilistic distribution representing the statistical association is not always equivalent to the interventional distribution, i.e., $\textrm{Pr}(y \mid t) \neq \textrm{Pr}(y \mid do(t))$~\cite{guo2020survey}. A notable example of confounding bias is that a system trained with historical user interactions may over recommend items that the user used to like, and the user’s decision (i.e., outcome) is also affected by historical interactions~\cite{wang2021deconfounded}. 




Both biases can lead to invalid estimates of causality from the data, and they are not mutually exclusive because selection bias does not explicitly involve causality. Many model selection biases, including popularity bias and exposure bias, are also confounding biases. As for user self-selection bias, the model in Fig. \ref{fig:selection_confounding} (a) gives an illustration of its causal nature in which $S$ is a variable affected by both $T$ (treatment) and $Y$ (outcome), indicating entry into the data pool~\cite{bareinboim2012controlling}. Therefore, confounding bias is significantly different from user self-selection bias from the causal perspective. The former originates from common causes, whereas the latter originates from common outcomes~\cite{elwert2014endogenous}. The former stems from the systematic bias introduced during the treatment assignment, while the latter comes from the systematic bias during the collection of units into the sample~\cite{correa2019identification}.


\begin{figure*}[t!]
\centering
\vspace{-1mm}
\includegraphics[height=0.4\textheight,trim=0 0 0 0 ,clip]{pic/propensity_illustration.pdf} %l b r t
\vspace{-3mm}
\caption{Illustration of propensity score strategies for recommendation.}
\vspace{-6mm}
\label{fig:propensity}
\end{figure*}



\subsubsection{Inverse Propensity Score}
\label{sec:ips}
Inverse Propensity Score (IPS)~\cite{horvitz1952generalization, rosenbaum1987model, rosenbaum1983central,little2019statistical}, also named as inverse propensity weighting (IPW), or inverse propensity of treatment weighting (IPTW), is one of the favorite counterfactual techniques and has inspired a lot of causal inference methods in RS, especially for unbiased learning~\cite{joachims2017unbiased}. Propensity score is the probability of receiving the treatment given covariates $X$, formulated as:
\begin{equation}
e_{\pi}(X) = \textrm{Pr}_{\pi}(T=1 \mid X).
\end{equation}
IPS assigns a weight $w$ to each sample:
\begin{equation}
w=\frac{t}{e(x)}+\frac{1-t}{1-e(x)},
\end{equation}
which indicates the inverse probability of receiving the \emph{observed} treatment and control. The unbiasedness of IPS can be proven~\cite{rosenbaum1987model}. More specifically, for the reward estimation of recommendation policy, IPS adjusts the distribution of background features in the logged dataset to be consistent with that during $\pi$ tests online, formulated as:
\begin{equation}
\hat{R}_{\mathrm{IPS}}\left(\pi; \mathcal{O}_{\pi_0}\right):=\frac{1}{\mathcal{O}_{\pi_0}} \sum_{k=1}^{|\mathcal{O}_{\pi_0}|} \frac{e_{\pi}(X)}{e_{\pi_0}(X)} \cdot y_k = \frac{1}{\mathcal{O}_{\pi_0}} \sum_{k=1}^{|\mathcal{O}_{\pi_0}|} \frac{\textrm{Pr}_{\pi}(T=1 \mid X)}{\textrm{Pr}_{\pi_0}(T=1 \mid X)} \cdot y_k,
\end{equation}
where we assume that only positive feedback is taken into account, and $w= \frac{e_{\pi}(X)}{e_{\pi_0}(X)} $ is the ratio of the evaluation and logged policies. Note that in most applications in RS, IPS is model-agnostic, applied to the training objective function for policy evaluation directly or for policy learning indirectly.



\begin{table*}[pt]
  \small
  \centering
  \vspace{-1mm}
  \caption{\normalsize Summary of propensity score strategies for recommendation.}
  \vspace{-2mm}
  \setlength{\tabcolsep}{1.5mm}{
    \begin{tabular}{c|c|c|c|c|c}
    \toprule
    \textbf{Category} & \textbf{Model} & \textbf{Causal method} & \textbf{Backbone model} & \textbf{Issue of concern} & \textbf{Year} \\
    \midrule
    \multicolumn{1}{c|}{\multirow{5}[2]{*}{\makecell{Approach \\ Inspired by \\Propensity\\ Score}}} & ExpoMF~\cite{liang2016modeling} & Propensity score & MF  & Exposure bias & 2016 \\
          & SERec~\cite{wang2018collaborative} & Propensity score & MF  & Social recommendation & 2018 \\
          & Dcf~\cite{wang2020causal} & Propensity score & MF  & Unobserved confounding bias  & 2020 \\
          & CNFI~\cite{zhang2021causalneural} & Propensity score & MF    & Implicit feedback & 2021 \\
          & IOBM~\cite{chen2021adapting} & Propensity score & Bi-LSTM~\cite{graves2013speech}  & Interactional observation bias & 2021 \\
    \midrule
    \multirow{19}[2]{*}{\makecell{Approach\\ with Inverse\\ Propensity \\Score (IPS)}} & MF-IPS~\cite{schnabel2016recommendations} & IPS, SNIPS & MF  & Selection bias & 2016 \\
          & PBM~\cite{joachims2017unbiased} & IPS   & SVM-Rank~\cite{joachims2002optimizing, joachims2006training} & Position bias & 2017 \\
          & ~\cite{mehrotra2018towards} & IPS   & (reinforcement learning) & Fairness & 2018 \\
          & Multi-IPW~\cite{zhang2020large} & IPS   & Multi-task DNN & Selection bias & 2019 \\
          & CPBM~\cite{fang2019intervention} & IPS   & SVM-Rank & Selection bias & 2019 \\
          & ULRMF,ULBPR~\cite{sato2019uplift} & IPS, SNIPS, ATE   & MF  & Uplift & 2019 \\
          & DLCE~\cite{sato2020unbiased} & CIPS  & MF  & Unobserved confounding bias  & 2020 \\
          & Rel-MF~\cite{saito2020unbiased} & CIPS  & MF  & Unobserved confounding bias  & 2020 \\
          & ~\cite{christakopoulou2020deconfounding} & IPS   & Multi-task DNN & Observed confounding bias & 2020 \\
          & RIPS~\cite{mcinerney2020counterfactual} & RIPS  & (model-agnostic) & Slate recommendation & 2020 \\
          & ACL-~\cite{xu2020adversarial} & IPS   & \makecell{(adversarial learning)} & Identifiability & 2020 \\
          & UR-IPW~\cite{zhang2021user} & SNIPS & Multi-task DNN & \makecell{Post-click revisit effect\\ \&selection bias} & 2021 \\
          & ~\cite{li2021debiasing} & IPS   & (model-agnostic) & Domain bias & 2021 \\
          & CBDF~\cite{zhang2021counterfactual} & IPS   & (reinforcement learning) & Delayed feedback & 2021 \\
          & RD\&BRD~\cite{ding2022addressing} & \makecell{IPS/DR/\\ AutoDebias~\cite{chen2021autodebias}} & MF  & Unobserved confounding bias  & 2022 \\
          & DENC~\cite{li2022causal} & IPS   & (self-designed) & Selection bias & 2022 \\
          & CET~\cite{cai2022hard} & IPS   & BERT  & False negative  & 2022 \\
          & CAFL~\cite{krauth2022breaking} & IPS   & MF  & Feedback loop & 2022 \\
          & RIIPS~\cite{liu2022practical} & RIIPS  & Two-tower structure  & Selection bias & 2022 \\
    \midrule
    \multirow{6}[2]{*}{\makecell{Approach \\with Doubly\\ Robust}} & Propensity-free DR~\cite{yuan2019improving} & DR    & FFM~\cite{yuan2019one} & Selection bias & 2019 \\
          & Multi-DR~\cite{zhang2020large} & DR    & Multi-task DNN & Selection bias & 2019 \\
          & MRDR-DL~\cite{guo2021enhanced} & MRDR  & MF  & Selection bias & 2021 \\
          & Cascade-DR~\cite{kiyohara2022doubly} & Cascade-DR & MF  & High variance of RIPS & 2022 \\
          & ASPIRE~\cite{mondal2022aspire} & DR, ATE    & LightGBM~\cite{ke2017lightgbm} & Uplift & 2022 \\
          & DRIB~\cite{xiao2022towards} & DR    & MF  & Unobserved confounding bias  & 2022 \\
    \bottomrule
    \end{tabular}}%
    \label{tab:propensity}%
    \vspace{-6mm}
\end{table*}%


%A lot of IPS-based recommendation focuses on data debiasing in user interactions, mainly selection bias ~\cite{schnabel2016recommendations,saito2020unbiased,sato2020unbiased,zhang2021user,sato2021online, wu2021unbiased, li2022causal}. For example, ~\cite{schnabel2016recommendations} is a representative work adopting IPS to recommender system for the elimination of selection bias, in which the recommendation algorithm is based on matrix factorization and propensity scores are estimated via naive Bayes or logistic regression. Similarly account for selection bias, Saito et al. estimates the exposure propensity for each user-item pair~\cite{saito2020unbiased} and Sato et al. proposes the DLCE (Debiased Learning for the Causal Effect) model with IPS-based estimators to evaluating unbiased ranking uplift ~\cite{sato2020unbiased}. Unbiased IPS-based uplift is also concerned by ~\cite{sato2019uplift}. In addition, ~\cite{zhang2021user} proposes UR-IPW (User Retention Modeling with Inverse Propensity Weighting) to model revisit rate estimation accounting for the selection bias problem and ~\cite{li2021debiasing} adjusts domain weights based on IPS to reduce domain bias. Though IPS-based methods do not require an explicit analysis of the causal correlation between variables, some works~\cite{christakopoulou2020deconfounding, mcinerney2020counterfactual, ding2022addressing} still discuss causal graphs as a good guide to accurate model. For example, Ding et al.~\cite{ding2022addressing} leverage a causal graph to explain the risk of unmeasurable confounders on the accuracy of propensity estimation and propose RD (Robust Deconfounder) with the sensitivity analysis, obtaining the bound of propensity score to enhance the robustness of methods against unmeasured confounders. 

Much IPS-based recommendation focuses on data debiasing in user interactions, mainly selection bias ~\cite{schnabel2016recommendations,saito2020unbiased,sato2020unbiased,zhang2021user,sato2021online, zhang2021causalneural, wu2021unbiased, li2022causal}. For example, ~\cite{schnabel2016recommendations} is a representative work adopting IPS to recommender system for the elimination of selection bias, in which the recommendation algorithm is based on matrix factorization and propensity scores are estimated via naive Bayes or logistic regression. Similarly, Saito et al.~\cite{saito2020unbiased} estimate the exposure propensity for each user-item pair and Sato et al.~\cite{sato2020unbiased} propose the DLCE (Debiased Learning for the Causal Effect) model with IPS-based estimators to evaluating unbiased ranking uplift. Unbiased IPS-based uplift is also concerned by ~\cite{sato2019uplift}. In addition, ~\cite{zhang2021user} proposes UR-IPW (User Retention Modeling with Inverse Propensity Weighting) to model revisit rate estimation accounting for the selection bias problem and ~\cite{li2021debiasing} adjusts domain weights based on IPS to reduce domain bias. Though IPS-based methods do not require an explicit analysis of the causal correlation between variables, some works~\cite{christakopoulou2020deconfounding, mcinerney2020counterfactual, ding2022addressing} still discuss causal graphs as an excellent guide to accurate model. For example, Ding et al.~\cite{ding2022addressing} leverage a causal graph to explain the risk of unmeasurable confounders on the accuracy of propensity estimation and propose RD (Robust Deconfounder) with the sensitivity analysis, obtaining the bound of propensity score to enhance the robustness of methods against unmeasured confounders. Li et al.~\cite{li2022causal} construct the DENC (De-bias Network Confounding in Recommendation). This causal graph-based recommendation framework disentangles three determinants for the outcomes, including inherent factors, social network-based confounder and exposure, and estimates each of them with a specific component, respectively. 
%but Christakopoulou et al. still discuss the causal graph of user satisfaction and treat response rate as a confounding factor before using IPS to debias user satisfaction estimation from response bias~\cite{christakopoulou2020deconfounding}. 
By the way, there are some works~\cite{christakopoulou2020deconfounding, cai2022hard, zhang2021user} integrate multi-task models with IPS to learn propensity scores and user interactions simultaneously.





In addition to debiasing, some IPS-based methods are dedicated to addressing other issues that abound in RS~\cite{mehrotra2018towards, zhang2021counterfactual, krauth2022breaking}. For example, Mehrotra et al.~\cite{mehrotra2018towards} proposes an unbiased estimator of user satisfaction based on IPS to jointly optimize for supplier fairness and consumer relevance. Besides, the CBDF (Counterfactual Bandit with Delayed Feedback) algorithm ~\cite{zhang2021counterfactual} re-weights the observed feedback with importance sampling, which is determined by a survival model to deal with delayed feedbacks. The CAFL (causal adjustment for feedback loops) ~\cite{krauth2022breaking} extends the IPS estimator to break feedback loops. 

Despite the unbiasedness strength of IPS, the inaccurate estimation of the unknown propensity $e(x)$ or sample weight, which results in high variance~\cite{gilotte2018offline}, becomes the biggest obstacle to achieving it. To alleviate this problem, modified versions of IPS have been proposed to control variance and applied to RS, including Self Normalized IPS ~\cite{schnabel2016recommendations, zhang2021user}, Clipped IPS~\cite{saito2020unbiased, sato2020unbiased}, Reward interaction IPS~\cite{mcinerney2020counterfactual}, and Regularized per-Item IPS~\cite{liu2022practical}. Self Normalized Inverse Propensity Scoring (SNIPS)~\cite{swaminathan2015self} rescales the estimate of the original IPS without any parameters to reduce the high variance, which is:
\begin{equation}
\hat{R}_{\mathrm{SNIPS}}\left(\pi; \mathcal{O}_{\pi_0}\right):=
\left(\sum_{k=1}^{|\mathcal{O}_{\pi_0}|} \frac{e_{\pi}(X)}{e_{\pi_0}(X)}\right)^{-1}
\sum_{k=1}^{|\mathcal{O}_{\pi_0}|} \frac{e_{\pi}(X)}{e_{\pi_0}(X)} \cdot y_k,
\end{equation}
and is introduced to RS by works like ~\cite{schnabel2016recommendations} and ~\cite{zhang2021user} to alleviate selection bias. Clipped IPS (CIPS)~\cite{saito2020unbiased, sato2020unbiased} tightens the bound of the sample weight by introducing a scalar value hyperparameter $\lambda_{\mathrm{CIPS}}$, formulated as:
\begin{equation}
\hat{R}_{\mathrm{CIPS}}\left(\pi; \mathcal{O}_{\pi_0}\right):=
\frac{1}{\mathcal{O}_{\pi_0}}
\sum_{k=1}^{|\mathcal{O}_{\pi_0}|} \min \left\{ \frac{e_{\pi}(X)}{e_{\pi_0}(X)} , \lambda_{\mathrm{CIPS}} \right\}\cdot y_k,
\end{equation}
which has a lower variance but gives away its unbiasedness. McInerney et al.~\cite{mcinerney2020counterfactual} loosen the SUTVA assumption and propose Reward interaction IPS (RIPS) for sequential recommendations, which assumes a causal model in which users interact with a list of items from the top to the bottom. RIPS uses iterative normalization and lookback to estimate the average reward and achieves a better bias-variance trade-off than IPS. In addition to high variance, violation of the Unconfoundedness assumption is another challenge of utilizing IPS in RS. That is, the treatment mechanism is \emph{identifiable}~\cite{glymour2016causal, mohan2021graphical} from observed covariates due to the existence of unobserved ones, which leads to the inaccurate estimate of propensity score and the disagreement between the online and offline evaluations. To address the uncertainty brought by the identifiability issue, ~\cite{xu2020adversarial} proposes minimax empirical risk formulation, which can be converted to an adversarial game between two recommendation models via duality arguments and relaxations.

More recently, Liu et al.~\cite{liu2022practical} propose Regularized per-item IPS (RIIPS) with an additional penalty function that constrains the difference in recommended outcomes between the deployed system and the new system so that the explosion of propensity scores can be avoided.




\subsubsection{Doubly Robust}
Doubly Robust (DR)~\cite{funk2011doubly, dudik2014doubly, jiang2016doubly, wang2019doubly} is another powerful and effective causal method account for the MNAR issue. To understand DR, let us consider the two common-used approaches to mitigate against MNAR: direct method (DM)~\cite{beygelzimer2009offset} and IPS~\cite{saito2021evaluating}. The former designs a model (linear regression, deep neural network, etc.) to directly learn the missing outcomes based on the observed data, which has low variance due to the advantage of supervised learning but suffers from high bias caused by unmet IID assumptions, denoted as~\cite{saito2021evaluating}:
\begin{equation}
\hat{R}_{\mathrm{DM}}\left(\pi_{0} ; \mathcal{O}_{\pi_{0}}, \hat{y}\left(x_{k}, t\right) \right):=\frac{1}{|\mathcal{O}_{\pi_{0}}|} \sum_{k=1}^{|\mathcal{O}_{\pi_{0}}|}\textrm{Pr}_{\pi}\left(t=1 \mid x_{k}\right) \hat{y}\left(x_{k}, t\right),
\end{equation}
where $\hat{y}\left(x, t\right)$ is the estimated outcomes. The latter, though unbiased theoretically, often causes training losses to oscillate stemming from the inverse of propensity with high variance~\cite{thomas2016data}. What DR does is to combine the direct method and IPS, which takes advantage of both and overcomes their limitations:
\begin{equation}
\hat{R}_{\mathrm{DR}}\left(\pi ; \mathcal{O}_{\pi_0}, \hat{r}\right) := \hat{R}_{\mathrm{DM}}\left(\pi ; \mathcal{O}_{\pi_0}, \hat{y}\left(x_{k}, t\right) \right) + 
\frac{1}{|\mathcal{O}_{\pi_{0}}|} \sum_{k=1}^{|\mathcal{O}_{\pi_{0}}|} 
\frac{e_{\pi}(X)}{e_{\pi_0}(X)} \left(y_{k}-\hat{y}\left(x_{k}, t_{k}\right)\right). 
\end{equation}
DR uses the estimated outcomes to decrease the variance of IPS. It is also \emph{doubly robust} in that it is consistent with the policy reward value if either the propensity scores or the imputed outcomes are accurate for all user-item pairs~\cite{wang2019doubly, saito2021evaluating}. By the way, advanced versions like Switch-DR~\cite{wang2017optimal} and DRos (Doubly Robust with Optimistic Shrinkage)~\cite{su2020doubly} are proposed to further control the variance.

Based on the above advantages, DR has found an increasingly wide utilization in RS~\cite{yuan2019improving, zhang2020large, guo2021enhanced, kiyohara2022doubly, mondal2022aspire, xiao2022towards}. Yuan et al.~\cite{yuan2019improving} propose a propensity-free doubly robust method to address the issue that samples with low propensity scores are absent in the observed dataset. Zhang et al.~\cite{zhang2020large} propose Multi-DR based on a multi-task learning framework to address selection bias and data sparsity issues in CVR estimation. Gun et al.~\cite{guo2021enhanced} propose the MRDR (more robust doubly robust) estimator to further reduce the variance caused by inaccurate imputed outcomes in DR while retaining its double robustness. In addition, Kiyohara et al.~\cite{kiyohara2022doubly} expand previous RIPS to Cascade Doubly Robust estimator, which has the same user interaction assumption as RIPS. Xiao et al.~\cite{xiao2022towards} propose an information bottleneck-based approach to effectively learn the DR estimator for the estimation of recommendation uplift, with the hope of a better trade-off between the bias and variance of propensity scores. 

\subsection{Causal Effect Strategy}

The most critical and fundamental role of causal inference is to estimate the causal effects from observational data, which has a variety of applications in real-world recommender systems. Some works are dedicated to enhancing the causal effect of a recommender policy, i.e., uplift, and therefore deploy the causal effect as a direct or indirect optimization goal for higher platform benefits. Other works introduce treatment effects to recommender systems for beyond-uplift objectives.
%$ATE$ or $CATE$ to improve representation learning and alleviate selection bias. 
Note that in the PO framework, the causal relationship between variables will not be discussed while calculating causal effect, and all variables affecting potential outcomes except treatment will be treated as covariates.



\begin{table*}[pt]
  \small
  \centering
  % \renewcommand\arraystretch{1.5}
  \vspace{-3mm}
  \caption{\normalsize Summary of causal effect strategies for recommendation.}
  \vspace{-3mm}
  \setlength{\tabcolsep}{2.4mm}{
    \begin{tabular}{c|c|c|c|c|c}
    \toprule
    \textbf{Category} & \textbf{Model} & \textbf{Causal method} & \textbf{Backbone model} & \textbf{Issue of concern} & \textbf{Year} \\
    \midrule
    \multirow{5}[2]{*}{\makecell{Causal \\effect\\ for\\ Uplift}} & ULRMF, ULBPR~\cite{sato2019uplift} & IPS, SNIPS, ATE & MF  & \multirow{5}[2]{*}{Uplift} & 2019 \\
          & ~\cite{goldenberg2020free}  & CATE  & Xgboost~\cite{chen2016xgboost} &       & 2020 \\
          & AUUC-max~\cite{betlei2021uplift} & CATE  & \makecell{Linear\\/Wide \& Deep\\  }  &       & 2021 \\
          & CausCF~\cite{xie2021causcf} & CATE  & MF  &       & 2021 \\
          & ASPIRE~\cite{mondal2022aspire} & DR, ATE & LightGBM~\cite{ke2017lightgbm} &       & 2022 \\
    \midrule
    \multirow{6}[2]{*}{\makecell{Causal \\effect\\ beyond\\ Uplift}} & ~\cite{rosenfeld2017predicting} & ITE   & \makecell{Linear/regularized \\kernel methods} & Domain adaptation & 2017 \\
          & CausE~\cite{bonner2018causal} & ITE   & MF  & Domain adaptation & 2018 \\
          & ~\cite{mehrotra2020inferring} & TE    & \makecell{Structural \\state-space model~\cite{brodersen2015inferring}} & \makecell{Causal effect of\\a new track release} & 2020 \\
          & CACF~\cite{zhang2021causally} & ITE   & (self-designed) & Unobserved confounding bias & 2021 \\
          & MCRec~\cite{yao2022device} & CATE  & DIN~\cite{zhou2018deep} & \makecell{Device-cloud \\recommendation} & 2022 \\
          & LRIR~\cite{tran2022most} & ITE, ATE & (self-designed) & Disability employment & 2022 \\
    \bottomrule
    \end{tabular}}%
  \label{tab:effect}%
  \vspace{-6mm}
\end{table*}%


\subsubsection{Causal Effect for Uplift}

Uplift, in terms of the causal effect of recommendations, refers to the increase in user interactions purely caused by recommendations. Typical evaluations of recommender systems regard the positive user interactions as a success, such as clicks, purchases, and high rates. However, these interactions may have remained even without recommendations, which can be illustrated by the conclusion of~\cite{sharma2015estimating} that more than 75\% of click-throughs would still occur in the absence of recommendations. Therefore, the industry looks favorably on the uplift as the recommendation metric in expectation of higher reward.


%Let us consider the binary treatments of a user in the two given scenarios (recommended or not) to explain the concept of uplift in the PO framework, and the user  The causal effects will fell into four groups: 

%The Two-Model approach ~\cite{radcliffe2007using, nassif2013uplift} directly models $\mathbb{E}[Y(T=1|X=x)]$ and \mathbb{E}[Y(T=0|X=x)]$ separately using the treatment group data and the control group data, respectively， and $CATE$, the difference between them, is treated as uplift~\cite{gutierrez2017causal}. This approach is simple but may suffer from accumulative error in the subtraction of two separate predictions. Other causal approaches with traditional machine learning methods for uplift estimation are proposed in ~\cite{jaskowski2012uplift, jaskowski2012uplift, radcliffe2011real}.

It is a natural application to introduce the causality concepts such as ATE and CATE for uplift modeling since the definition of uplift is a counterfactual problem and consistent with the objective of causal effect estimation~\cite{yamane2018uplift, zhang2021unified}. Causal approaches with traditional machine learning methods for uplift estimation are proposed in~\cite{radcliffe2007using, nassif2013uplift, jaskowski2012uplift, jaskowski2012uplift, radcliffe2011real, gutierrez2017causal}. Regarding recommender systems, uplift estimation on online A/B testing suffers from the high expense and large fluctuations due to user self-selection bias~\cite{sato2021online}, while uplift estimated offline is bedeviled by a wide variety of biases that could lead to MNAR. In order to deal with these issues, much of the literature has been published. Sato et al.~\cite{sato2019uplift} utilize SNIPS-based ATE to accomplish offline uplift-based evaluation. Goldenberg et al.~\cite{goldenberg2020free} leverage the Retrospective Estimation technique that relies solely on data with positive outcomes for CATE-based uplift modeling, which makes it especially suited for many recommendation scenarios where only the treatment outcomes are observable. ~\cite{betlei2021uplift} learns a model that directly optimizes an upper bound on AUUC, a popular uplift metric based on the uplift curves and unified with ATE~\cite{yamane2018uplift}. In addition,  CausCF~\cite{xie2021causcf} extends the classical MF to the tensor factorization with three dimensions—user, item, and treatment effect for better uplift performance. It is worth mentioning that in the uplift modeling literature~\cite{diemert2018large,gutierrez2017causal, zhang2021unified}, there are two closely related metrics for uplift modeling, uplift and Qini curves, the latter of which is evaluated based on the ranking of conditional treatment effect estimations. 




\subsubsection{Causal Effect beyond Uplift}
%It is a natural application to introduce the concepts such as $ATE$ and $CATE$ for causal effect estimation as it is consistent with the objective of causal effect estimation. For example, early studies have explored the influence of social presence through causal effect estimation through small-scale questionaires.
%Causal effect has its natural supremacy of explainability and trusts over statistic-based techniques. For example, 
There are some other impressive recommendation works with causal effect~\cite{mehrotra2020inferring, zhang2021causally, rosenfeld2017predicting, bonner2018causal, yao2022device, tran2022most}. For example, ~\cite{mehrotra2020inferring} adapts a Bayesian model to infer the causal impact of new track releases, which may be an essential consideration in the design of music recommendation platforms. ~\cite{zhang2021causally} minimizes the distance between the traditional attention weights in the recommendation method and the ITE to reflect the true impact of the features on the interactions. ~\cite{rosenfeld2017predicting} and ~\cite{bonner2018causal} frames causal inference as a domain adaptation problem and leverages ITE with a large sample of biased data and a small sample of unbiased data to eliminate the bias problems, which are described in more detail in \ref{sec:domain_adaption}.


%~\cite{jesson2020identifying} incorporates uncertainty estimating into the design of systems for CATE inference to deal with covariate shift and the violation of the positivity assumption