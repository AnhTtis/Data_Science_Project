\section{Foundation}
\label{sec:foundation}

In this section, background information and several important concepts of causal inference and recommender systems are introduced to facilitate readers' understanding of the inter-study of the two research fields. The notations used in this survey are listed for convenience. At the end of this section, we set up categorizations of causal recommendations.

\subsection{Causal Inference}
In this part, we will give a brief review of two representative frameworks of causal inference, including the potential outcome (PO) framework by Rubin et al.~\cite{ splawa1990application, rubin1974estimating, imbens2015causal} and the structural causal models (SCM) framework by Pearl et al.~\cite{pearl1995causal, pearl1988probabilistic, pearl2009causality}
%, to provide RS researchers with a preliminary understanding
. Note that these two frameworks are logically equivalent~\cite{ pearl2009causality}.

\subsubsection{Potential Outcome Framework}
\label{sec:po}

The Potential Outcomes Framework (aka the Neyman-Rubin Causal Model) ~\cite{plawa1990application, rubin1974estimating, imbens2015causal} is the most widely used framework across many disciplines. With a hypothetical treatment (or manipulation, intervention), the causal effect, i.e., treatment effect, is defined as the difference between the potential outcomes under treatment and control for \textit{the same unit}~\cite{imbens2015causal}.

\begin{definition}[Unit]
A unit refers to the research object in the potential framework.
\end{definition}

A unit can be a physical object, an individual, or a collection of objects or persons, such as a classroom or a market, at a particular point in time~\cite{imbens2015causal}. In recommendation research, a user-item pair will usually be defined as a unit. It should be noticed that the same physical object or person at a different time is a different unit. This is a reasonable restriction, considering the same user will make different decisions at a different time even if exposed to the same item due to factors like preference shift, mood, occasion and so on.

\begin{definition}[Treatment]
Treatment can be defined as the action applied to a unit.
\end{definition}

This paper focuses on binary treatment (e.g., recommend or not), the most common setting in the recommendation field. In practice, we refer to the more active treatment simply as the “treatment” $T=1$ and the other treatment as the “control” $T=0$.

\textbf{Potential Outcome.} For each treatment-unit pair, the potential outcome is the outcome that the treatment is applied to the unit, denoted as $Y(T=t)$ (ignoring unit). For a unit, only the potential outcome corresponding to the treatment actually taken will be observed, denominated as observed outcome, while others are referred to as counterfactual outcomes. The \textit{fundamental problem of causal inference} in PO framework is that we can never obtain both observed and counterfactual outcomes for a unit: it is impossible to realize all treatments and observe the corresponding outcomes.

\textbf{Treatment Effect/ Causal effect.} Treatment effect is represented by the difference between the potential outcomes under treatment and control for the same unit, formulated as:

\begin{equation}
\label{eq:ITE}
    \textrm{TE} = Y(T=1) - Y(T=0),
\end{equation}

where $ {Y}(T=1)$ and $ {Y}(T=0)$ are the potential treated and control outcome of the unit, respectively. Treatment effect like Equation \ref{eq:ITE} is also called \textbf{Individual Treatment Effect}. Furthermore, the treatment effect can be defined at the population and subpopulation levels. At the population level, \textbf{ Average Treatment Effect (ATE) } is the expectation of ITE over the whole population~\cite{guo2020survey}, denoted as:

\begin{equation}
\label{eq:ATE}
    \textrm{ATE} = \mathbb{E}[Y(T=1) - Y(T=0)].
\end{equation}

The ATE on the subpopulation level is often of particular interest; thus we define \textbf{Conditional Average Treatment Effect (CATE)} on the units with the same features $X=x$ as:
\begin{equation}
\label{eq:CATE}
    \textrm{CATE} = \mathbb{E}[Y(T=1|X=x) - Y(T=0|X=x)].
\end{equation}

\textbf{Assumptions.} Despite the simple definition of the causal effect, the fundamental problem in causal inference, i.e., the \textit{missing data problem}, appear to be a major obstacle to the estimation of the causal effect. Therefore, it is critical to make additional assumptions.

\begin{assumption}[SUTVA]
\label{assumpt:sutva}
The potential outcomes for any unit do not vary with the treatments assigned to other units. For each unit, there are no different forms or versions of each treatment level, which lead to different potential outcomes.
\end{assumption}

The stable unit treatment value assumption, or SUTVA~\cite{imbens2015causal} is the most fundamental assumption in causal inference, incorporating both the \textit{No Interference} idea that treatments applied to one unit do not affect the outcome for another unit and the \textit{No Hidden Variations of Treatments} concept that for each unit there is only a single version of each treatment level. The second assumption, \textit{ignorability} or \textit{unconfoundedness}~\cite{rubin1990formal}, states that treatment assignment is free from dependence on the potential outcomes.

\begin{assumption}[Unconfoundedness / Ignorability]
\label{assumpt:uncon}
Treatment assignment $W$ is independent to the potential outcomes, i.e., $T \perp Y (T = 0),Y (T = 1)|X$, also written as $\textrm{Pr}(T=1|X, Y (T = 0),Y (T = 1)) = \textrm{Pr}(T=1|X)$, where $X$ denotes the background variables.
\end{assumption}

In other words, within subpopulations defined by the values of observed background variables, or covariates, the treatment assignment is random. The ignorability assumption rules out unmeasured confounders, which causally influences both the treatment $T$ and the outcome $Y(T)$. $\textrm{Pr}(T=1|X)$ is called the \textit{propensity score}~\cite{rosenbaum1983central}. The last assumption is positivity, or overlap:

\begin{assumption}[Positivity]
\label{assumpt:pos}
$0 < \textrm{Pr}(T=t|X=x) < 1, \forall t, x.$
\end{assumption}

In large data samples, positivity requires that there are both treated and control units for all values of the covariates. In contrast to the untestable ignorability assumption~\cite{ imbens2015causal}, positivity can be tested from observed data. The combination of unconfoundedness and positivity is referred to as “\textit{strong ignorability}~\cite{rosenbaum1983central}.”


\subsubsection{Structural Causal Models Framework}
\label{sec:scm}
Structural causal models (SCM)~\cite{pearl1995causal, pearl1988probabilistic, pearl2009causality} serve as a comprehensive causality framework, which unifies graphical models, nonparametric structural equations, and counterfactual and interventional logic. The most significant advantage of SCM is its intuitive structure of real-world causal dependencies based on graphical models as well as the wise and friendly symbiosis between counterfactual and graphical methods.

\textbf{Causal Graph.} A causal graph, or a causal diagram, is usually a Bayesian network, which describes the causal relations between variables by a Directed Acyclic Graph (DAG), where the nodes represent the variables and the edges record the causal relations. Causal graphs play an essential role in the SCM framework, for they provide a vivid representation of sets of variables that are relevant to each other in any given state of knowledge, and serves as a carrier of \textit{conditional independence} relationships along the order of construction, through which we can confirm whether it satisfies the criteria such that certain causal inference methods can be applied~\cite{pearl2009causality}. 

\begin{figure}[t!]
    \centering
    \vspace{-3mm}
    \subfloat[Chain]{
    	\centering
    	\includegraphics[width=0.2\textwidth, trim=-10 0 -10 0, clip]{pic/chain.pdf}
    }
    \centering
    \subfloat[Fork]{
    	\centering
    	\includegraphics[width=0.2\textwidth, trim=-10 0 -10 0, clip]{pic/fork.pdf}
    }
    \centering
        \subfloat[Collider]{
    	\centering
    	\includegraphics[width=0.2\textwidth, trim=-10 0 -10 0, clip]{pic/collider.pdf}
    }
    \centering    
    \vspace{-3mm}
    \caption{Graphical models of three typical types of causal structures.}
    \vspace{-6mm}
    \label{fig:junction}
\end{figure}

\textbf{\textit{d}-Separation.} We first review the concept of \textit{dependency-separation} (\textit{d}-Separation) as the knowledge base for conditional independence. There are three typical causal graphs of three disjoint sets of variables, shown in Figure~\ref{fig:junction}, with the help of which we can characterize any pattern of arrows in the network. In the \textit{chain} (Figure~\ref{fig:junction}(a)), $B$ is the $mediator$ that transmits the effect of $A$ to $C$. In the \textit{fork} (Figure~\ref{fig:junction}(b)), $B$ is often called a common cause or \textit{confounder} of $A$ and $C$. A confounder will make $A$ and $C$ statistically correlated even though there is no direct causal link between them, which may give rise to a so-called spurious correlation in the application. In the \textit{collider} (Figure~\ref{fig:junction}(c)), though $A$ and $C$ are independent to begin with, conditioning on (i.e., knowing the value of) $B$ will make them dependent. A good example is three features of Hollywood actors: Talent $\rightarrow$ Celebrity $\leftarrow$ Beauty ~\cite{elwert2014endogenous}. Although beauty and talent are completely unrelated to one another in the general population, an unanticipated negative correlation is found between talent and beauty if we only focus on famous actors: a celebrity is unattractive increases our belief that he or she is talented~\cite{pearl2018book}. This negative correlation is sometimes called collider bias or the “explain-away” effect. 
A \textit{path} means a sequence of consecutive edges (of any directionality) in the graph, and we regard stopping the flow of dependency between the variables that are connected by such paths as \textit{blocked}. In the chain and fork, the path between $A$ and $C$ will be blocked by conditioning on $B$, while in the collider, any conditioning on $B$ will introduce a correlation between them. The formal definition of \textit{d}-separation or blocking is defined as follows.

\begin{definition}[\textit{d}-Separation]
\label{def:d_s}
A path is said to be d-separated (or blocked) by conditioning on a set of nodes $\mathcal{Z}$ if and only if one of the two conditions is satisfied:
\begin{enumerate}
\item The path contains a chain $A \rightarrow B \rightarrow C$ or a fork $A \leftarrow B \rightarrow C$ such that the middle node $B$ is in $\mathcal{Z}$;
\item The path contains a collider such that the middle node $B$ is not in $\mathcal{Z}$ and such that no descendant of $B$ is in $\mathcal{Z}$.
\end{enumerate}
\end{definition}

\begin{figure}[t!]
    \centering
    \vspace{-3mm}
    \subfloat[]{
    	\centering
    	\includegraphics[width=0.20\textwidth, trim=-4 0 -4 0, clip]{pic/inter0.pdf}
    }
    \centering
    \subfloat[]{
    	\centering
    	\includegraphics[width=0.20\textwidth, trim=-4 0 -4 0, clip]{pic/inter1.pdf}
    }
    \centering
    \vspace{-3mm}
    \caption{Examples of the structural equation and intervention.}
    \vspace{-7mm}
    \label{fig:inter}
\end{figure}

\textbf{Structural Equations.} Beside causal graph, structural equation is another representation of causal information, where the former is an abstraction of the latter. In its general form, a structural equation of a variable $Y$ is defined as:

\begin{equation}
\label{eq:sem}
Y=f_Y(Pa, U),
\end{equation}

where $Pa$ (connoting parents) stands for the set of variables that directly determine the value of $Y$ and where $U$ represents exogenous variables or errors (or “disturbances”) due to omitted factors. For example, the causal graph in Fig.~\ref{fig:inter}(a) is associated with the structural model as:

\begin{align}
	\label{eq:structural_example}
	\begin{cases}
	a&=\ f_A(u_A), \\
	b&=\ f_B(a, u_B), \\
	c&=\ f_C(a, b, u_C),
	\end{cases}
\end{align}

where $U_A$, $U_B$ and $U_C$ represent exogenous variables.A set of equations in the form of Equation~\ref{eq:sem} is called a \textit{structural model}; if each variable has a distinct equation in which it appears on the left-hand side, then the model is called a \textit{structural causal model}.

\textbf{Intervention.} The \textit{do-calculus} allows researchers to complete \textit{intervention}, interpreted as controlling the value of a variable, by purely mathematical means instead of by carrying out a physical experiment, which is one of the outstanding contributions of Pearl’s SCM framework. The \textit{do}-calculus involves the do-operation, like $do(T=t)$, which denotes the intervention of setting the variable $T$ to $t$, realizing by blocking the effect of $T$’s parents on $T$ and set the value of $T$ as $t$. For example, if we $do(B=b_0)$ on the model in Fig.~\ref{fig:inter}(a), Equations~\ref{eq:structural_example} will be modified as:

\begin{align}
	\label{eq:intervention}
	\begin{cases}
	a&=\ f_A(u_A), \\
	b&=\ b_0, \\
	c&=\ f_C(a, b_0, u_C), 
	\end{cases}
\end{align}
the graphical description of which is shown in Fig.~\ref{fig:inter}(b). 



It is crucial to note that $\textrm{Pr}(Y=y|do(T=t))$ and $\textrm{Pr}(Y=y|T=t)$ are not the same. For example. the fork structure (Fig.\ref{fig:junction}(b))might represent the causal mechanism that connects the number of sales at a local ice cream shop on that day ($A$), a day’s temperature in a city ($B$), and the number of violent crimes in the city on that day ($C$)~\cite{glymour2016causal}. Because both ice cream sales and violent crime are more common in hot weather, a positive correlation might be found when estimate $P(C=c|A=a)$. However, as illustrated in manipulated graphical model of Figure~\ref{fig:inter}, crime rates $C$ are independent of ice cream sales $B$, which results in a different $\textrm{Pr}(C=c|do(A=a))$ from $\textrm{Pr}(C=c|A=a)$.

Although causal graph manipulation is the most fundamentalist approach to calculating $\textrm{Pr}(Y=y|do(T=t))$, it can be challenging and even impossible in reality. Fortunately, we can estimate $\textrm{Pr}(Y=y|do(T=t))$ from observed data with the following causal effect rule:
\begin{definition}[The Causal Effect Rule]
\label{def:ce_rule}
Given a graph $G$ in which a set of variables $PA$ are designated as the parents of $T$, the causal effect of $T$ on $Y$ is given by
\begin{equation}
\label{eq:ce_rule}
\textrm{Pr}(Y=y \mid do(T=t)) 
= \sum_{x} \textrm{Pr}(Y=y \mid T=t, PA=x) \textrm{Pr}(PA=x) 
= \sum_{x} \frac{\textrm{Pr}(T=t, Y=y, PA=t)}{\textrm{Pr}(T=t \mid PA=x)}, 
\end{equation}
where $x$ ranges over all the combinations of values that the variables in $PA$ can take.
\end{definition}
The most important benefit brought by the rule is that it enables us to finish the  \textit{do}-calculus purely on passive observational data~\cite{xu2021causal}. The factor $\textrm{Pr}(T=t \mid PA=x)$ is the propensity score, and Equation \ref{eq:ce_rule} is named \textit{inverse propensity score} (Section \ref{sec:ips}) in PO framework, which partly reflects the unity of the two frameworks.

%It is crucial to note that P $(Y=y|do(X=x))$ and $P (Y=y|X=x)$ are not the same. For the above example. the chain structure might represent the causal mechanism that connects a day’s temperature in a city ($A$), the number of sales at a local ice cream shop on that day ($B$), and the number of violent crimes in the city on that day ($C$)~\cite{glymour2016causal}. A spurious correlation is that an increase in ice cream sales is correlated with an increase in violent crime—not because ice cream causes crime, but because both ice cream sales and violent crime are more common in hot weather. If we were to intervene to make ice cream sales low, we would have the graphical model shown in Figure~\ref{fig:inter}, which shows that crime rates are independent of ice cream sales. Since it is impossible to carry out the physical experiment in the real world, we intervene with the \textit{do}-calculus: 

\textbf{Counterfactuals.} Counterfactuals are employed to emphasize our wish to compare two outcomes under the exact same conditions, differing only in one aspect: the \textit{antecedent}, or \textit{hypothetical condition}~cite{glymour2016causal}. For example, in the counterfactual question “What would be the user’s interaction if the recommended items had been different?” mentioned above in the ~\ref{sec:intro}, we would like to compare the user’s interaction under the same conditions except for the recommended item. Counterfactuals, situations which are non-existent in reality, cannot be inferred by do-calculus. Fortunately, Pearl~\cite{ pearl2009causality} proposed a new set of notations: $ \textrm{Pr}(Y(T=1)|T = 0, Y = Y(T=0))$ indicates the probability of the outcome $Y(T=1)$ would be if the observed treatment value is $T=0$, given the fact that we observe $Y=Y(T=0)$ in the data.


\subsubsection{Comparison between the two frameworks}
As mentioned above, the two frameworks are equivalent logically: an assumption or a theorem can be translated to its counterpart in the other,  and a problem solved in one framework would yield the same solution in another~\cite{pearl2009causality, glymour2016causal}. For example, $\textrm{Pr}(Y = y | do(T = t))$ in the SCM is equivalent to $\textrm{Pr}(Y(T=t)=y)$ in the PO, which the regular assessment in a controlled experiment, in which the distribution of $Y$ is estimated for each level $w$ of a random variable $T$. Causal effects that are measured between the results of the counterfactual world and the real world can be estimated conveniently in both frameworks. However, there are several important differences between PO and SCM. The most significant difference is that PO does not assume the causal relations between concerned variables, while SCM makes assumptions of causal mechanisms among a set of variables or searches for ones based on some assumptions. In other words, any given PO model corresponds to multiple causal graphs in SCM. For PO, it can be a strength for PO that causal effects can be reasoned without knowing the causal model, and be a weakness either. According to the unconfoundednes assumption, all confounders should be observed to infer a correct treatment effect since the mechanism is unknown, almost impossible in practice~\cite{aliprantis2015distinction}. In contrast, in SCM, causal diagrams allow us to work with causal effects by interventions on the fewest number of variables or the observed variables as much as possible.

\subsection{Recommender Systems}

Recommender systems predict users’ preferences and proactively recommend items users might like~\cite{ricci2015recommender, zhang2019deep} to alleviate information overload. 

\subsubsection{Recommendation Techniques}

RSs are usually classified into the following three categories~\cite{adomavicius2005toward, zhang2019deep}: content-based, collaborative filtering (CF), and hybrid. Content-based recommendation learns to recommend primarily based on comparisons across items’ and users’ auxiliary information~\cite{zhang2019deep}, such as items’ human-set tags, images, texts, and users’ sex. Collaborative filtering recommender systems recommend items according to user/item historical interactions, i.e., explicit (e.g., user’s previous ratings) or implicit feedback (e.g., click behavior)~\cite{zhang2019deep}. Hybrid approaches are those that combine collaborative filtering and content-based methods.

If we review the model structure, recommender systems can generally be divided into shallow models, neural network models, and GNN-based models~\cite{gao2022graph}. Shallow models involve methods that directly calculate the similarity of interactions and CF methods with matrix factorization (MF)~\cite{koren2009matrix} or factorization machine (FM) ~\cite{rendle2010factorization}, but suffer from insufficient learning of users’ complicated interest. Neural network models are proposed to solve this issue, with the advantage of high-order feature interactions~\cite{guo2017deepfm}. For example, Wide \& Deep~\cite{cheng2016wide} jointly trains linear models and deep neural networks to combine the benefits of memorization and generalization. Deep factorization machine (DeepFM)~\cite{guo2017deepfm} combines traditional  factorization machine (FM) with multi-layer perceptrons (MLP) in parallel. Graph neural networks (GNN) adopt embedding propagation to aggregate neighborhood embedding iteratively. GNN-based methods have become the new state-of-the-art approaches in recommender systems~\cite{gao2022graph}.

\subsubsection{Notation}

Considering a general recommender system, we assume $\mathcal{O}$ and $\mathcal{O}^{-}$ denote the observed dataset and unobserved dataset. Each observed sample includes the treatment $T$, background features $X$, and an interaction label $Y$. Background features $X$, aka., covariates are usually formulated as a high dimensional sparse vector containing information such as user ID, item ID, user profile, item category, etc. The interaction label $Y$, or the outcome, can be explicit feedback (e.g., rating) or implicit feedback (such as click and watch behavior). 
%$\bm{f}_u$ and $\bm{f}_i$ denote user features and item features, respectively, which is usually a high dimensional sparse vector with multi-fields~\cite{rendle2010factorization}, such as user ID, item ID, user profile, item category, etc.

In normal circumstances, researchers prefer choosing whether to recommend as the treatment. Therefore, the observed dataset can be denoted as ${\mathcal{O}} = \{(T=1,X,Y\}|_{1}^{|\mathcal{O}|} \in \mathcal{T} \times \mathcal{X} \times \mathcal{Y}$, where $\mathcal{T}$ means the treatment space, $\mathcal{X}$ is the feature spaces, and $\mathcal{Y}$ is the label space. In general, the observed dataset is obtained with the deployed recommender policy $\pi$; thus ${\mathcal{O}}$ will be specifically expressed as ${\mathcal{O}_\pi}$ if we are concerned about the policy. Note that settings of $T, X, Y$ vary slightly according to specific work. It would be better to understand with reference to the context.

\subsection{Existing Categorizations of Causal Recommendation}
\label{sec:exist_cate}
There are several categorization criteria for causal recommender systems. For example, similar to ~\cite{yao2021survey}, Yao et.al.~\cite{wu2022opportunity} divides biases in RS into three categories from the perspective of violating what causal assumptions are adopted in the standard PO framework. 1) Position bias and conformity bias can be seen as violations of the SUTVA assumption if recommender systems do not pay enough attention to the positions of items and users’ social networks. 2) Unconfoundedness and positivity are crucial assumptions in the recoverability of the target estimated. However, the former can be violated by popularity bias, and the latter can be violated by exposure bias, both of which result in the problem of missing not at random (MNAR). 3) The final bias violates some model-specific assumptions.
%Another categorization is based on the model specification. 





According to the survey~\cite{gao2022causal}, existing work of causal recommendation can be categorized into three groups: for addressing data bias, for addressing data missing and noise, and for beyond-accuracy objectives. 1) Causal debiasing work can be further divided into several subcategories based on the specific bias, such as popularity bias, clickbait bias, and exposure bias. 2) The problem of data missing refers to the usually-discussed data sparsity issue in RS, and data noise stems from unreliable implicit signals and delayed feedback. In order to alleviate these issues, researchers use the counterfactual technique to augment insufficient data and adjust sample weights. Besides, some causal recommender systems are designed for beyond-accuracy objectives like explainability, diversity and fairness. 

~\cite{zhu2023causal} summarizes different causal inference techniques with an emphasis on debiasing, explainability promotion, and generalization improvement. 

%For better understanding, Figure~\ref{fig:exist_cate} shows the above-mentioned categorizations of previous work of causal inference for recommendation.



The three studies mentioned above are pioneering efforts in this field, and each has a distinct focus on the causal frameworks it discussed. However, this paper will systematically classify causal inference for RS from a new perspective of the employed causal approach. We regard that, although it is quite convenient for researchers, especially those in RS industry, to speed up the implementation of causal RS techniques from the perspective of what application issues they aim to address, continuous research and significant breakthroughs of in-depth integration of causal inference and recommender systems require a comprehensive understanding of causal techniques, including their strengths as well as their limitations, which our work will have a greater contribution.