\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{multirow}
\usepackage[dvipsnames,svgnames]{xcolor}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{CuNeRF: Cube-Based Neural Radiance Field for \\ Zero-Shot Medical Image Arbitrary-Scale Super Resolution}

\author{Zixuan Chen$^{1}$, Jianhuang Lai$^{1,2,3}$, Lingxiao Yang$^{1}$, Xiaohua Xie$^{1,2,3*}$\\
$^{1}$School of Computer Science and Engineering, Sun Yat-sen University, China\\
$^{2}$Guangdong Province Key Laboratory of Information Security Technology, China\\
$^{3}$Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China\\
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
   Medical image arbitrary-scale super-resolution (MIASSR) has recently gained widespread attention, aiming to super sample medical volumes at arbitrary scales via a single model.
   However, existing MIASSR methods face two major limitations: (i) reliance on high-resolution (HR) volumes and (ii) limited generalization ability, which restricts their application in various scenarios.
   To overcome these limitations, we propose Cube-based Neural Radiance Field (CuNeRF), a zero-shot MIASSR framework that can yield medical images at arbitrary scales and viewpoints in a continuous domain.
   Unlike existing MIASSR methods that fit the mapping between low-resolution (LR) and HR volumes, CuNeRF focuses on building a coordinate-intensity continuous representation from LR volumes without the need for HR references. 
   This is achieved by the proposed differentiable modules: including cube-based sampling, isotropic volume rendering, and cube-based hierarchical rendering.
   Through extensive experiments on magnetic resource imaging (MRI) and computed tomography (CT) modalities, we demonstrate that CuNeRF outperforms state-of-the-art MIASSR methods.
   CuNeRF yields better visual verisimilitude and reduces aliasing artifacts at various upsampling factors.
   Moreover, our CuNeRF does not need any LR-HR training pairs, which is more flexible and easier to be used than others.
   Our code will be publicly available soon.
   
   \end{abstract}
      %%%%%%%%% BODY TEXT
   \section{Introduction}\label{sec:intro}
   Medical imaging techniques such as computed tomography (CT) and magnetic resonance imaging (MRI) are critical tools in assisting clinical diagnosis by providing visual information. 
   However, obtaining medical slices at desired views and scales is a resource-intensive process that demands patients to expose themselves to considerable ionizing radiation while remaining immobile for a long time \cite{CT}. 
   To reduce the burden on patients, a feasible approach is to reconstruct high-quality medical volumes from low-resolution (LR) medical ones.
   
   Mainstream medical image processing methods aim to reconstruct high-resolution (HR) medical images from LR ones via medical image super-resolution (MISR) techniques. 
   Early studies employed optimization methods \cite{MISR,CSLC} and interpolation methods \cite{tricubic}.
   With the advent of deep learning, a series of methods \cite{ssp,ctsrgan,csn,gan3dmdcn,egan} have adopted convolutional neural networks to learn the mapping between LR and HR domains.
   
   Recently, medical image arbitrary-scale super-resolution (MIASSR) methods \cite{saint,miassr,uassr,ARSSR} have received widespread attention in the field of MISR.
   These methods aim to employ a single model to upsample medical volumes at arbitrary scales.
   % Based on Meta-SR \cite{meta-sr}, Peng \emph{et al.} presented SAINT \cite{saint}, which can anisotropically upsample medical volumes on the $z$-axis at arbitrary integer scales.
   % Wu \emph{et al.} proposed ArSSR \cite{ARSSR}, which continuously upsamples MRI volumes via implicit neural representation (INR) techniques.
   % Wang \emph{et al.} \cite{uassr} presented a weakly-supervised MIASSR framework, employing unpaired LR-HR volumes for training.
   Although these MIASSR methods achieve reasonable HR results, they still have two major issues: \textit{(i)} Existing MIASSR methods rely on the supervision from HR volumes, yet high-quality HR volumes are not always available;
   \textit{(ii)} These methods may be susceptible to the distribution gap between training and test data, leading to the produce non-existent details.
   
   
   \begin{figure}[!t]
     \centering
     \includegraphics[width=3.2in]{motivation.pdf}
     \caption{
       Visualization of the modeling forms between NeRF \cite{NeRF} \textbf{(a)} and \textit{CuNeRF} \textbf{(b)}.
       Visually, NeRF \emph{only} models the ray corresponding to each training pixel (\textcolor{red}{red circle}), which cannot cover the whole representation ranges, leaving some ``holes'' (\emph{i.e.}, unmodeled space within (\textcolor{LightSkyBlue}{blue cube}) between adjacent training pixels.
       To circumvent this issue, \textit{CuNeRF} samples a cube (\textcolor{Orchid}{purple cube}) centered by each training pixel, and therefore the ``holes'' are well-filled by the spatial overlaps (\textcolor{Grey}{grey cube}).
     }
     \label{fig:motivation}
     \end{figure}
   
   To address the above-mentioned limitations, we present a zero-shot MIASSR framework, which is trained on LR medical volumes without any supervision from HR ones.
   Specifically, we draw inspiration from the neural radiance field (NeRF) \cite{NeRF} to estimate the continuous representation from discrete samples (LR volumes) instead of fitting the mapping between LR and HR volumes.
   However, NeRF's modeling strategy may not be suitable for direct application on medical volumes.
   As depicted in Figure \ref{fig:motivation} \textbf{(a)}, due to the difference between medical imaging systems and conventional cameras, NeRF may remain some ``holes'' (\emph{i.e.}, unmodeled space) in the continuous fields, leading to the synthesis of grid-like artifacts (see Figure \ref{fig:examples}). 
   Detailed explaination is provided in Section \ref{sec:holes}.
   
   In this paper, we present Cube-based NeRF (CuNeRF), a zero-shot solution for MIASSR challenges.
   Specifically, we tackle NeRF's hole-forming iusses via the proposed modules: cube-based sampling, isotropic volume rendering, and cube-based hierarchical rendering.
   Since the modules are differentiable, \textit{CuNeRF} can build a continuous mapping between the coordinate and the corresponding intensity value in the training data, which is capable of generating medical slices at arbitrary scales and viewpoints in a continuous domain, as shown in Figure \ref{fig:contribution}.
   Comprehensive experiments on the MSD Brain Tumour (MRI) \cite{MSD} and KiTS19 (CT) \cite{kits19} datasets show that \textit{CuNeRF} surpasses state-of-the-art MIASSR methods in 3D MISR and achieves impressive performance in anisotropic MISR challenges.
   Figure \ref{fig:examples} shows the proposed \textit{CuNeRF} outperforms other methods in visual verisimilitude. The main contributions are summarized:
   
   \begin{itemize}
     \item To the best of our knowledge, \textit{CuNeRF} is the first zero-shot MIASSR framework that can continuously upsample medical volumes at arbitrary scales.
     
     \item We address the hole-forming issues via the proposed techniques: cube-based sampling, isotropic volume rendering, and cube-based hierarchical rendering.
   
     \item Extensive experiments on CT and MRI modalities for 3D MISR and anisotropic MISR show \textit{CuNeRF} favorably surpasses state-of-the-art MIASSR methods. 
     
   \end{itemize}
   
   
     \begin{figure}[!t]
       \centering
       \includegraphics[width=3.2in]{examples.pdf}
       \caption{
         Visual examples of 3D MISR results at $\times\!2.5$ scale between ArSSR \cite{ARSSR}, SAINT \cite{saint}, NeRF$^\dag$ \cite{NeRF} and the proposed \textit{CuNeRF} on T1-weighted brain tumor MRI volumes selected from MSD \cite{MSD} dataset.
         The heatmaps at the bottom are provided to clearly visualize the difference related to the HR image.
         PSNR and SSIM for each image are inserted, \textcolor{red}{red text} indicates the best one.
         Visually, NeRF$^\dag$ renders blurry results with severe grid-like artifacts, while ArSSR produces non-existent details (zooming in \textcolor{Orchid}{purple box}).
         By contrast, \textit{CuNeRF} renders high-quality medical images, achieving better visual verisimilitude to the HR images and reducing objectionable artifacts.
       }
       \label{fig:examples}
       \end{figure}
   
   
   
   \section{Related Works}
   In this section, we first review implicit neural representation and subsequently introduce some impressive progress in medical image super-resolution.
   Recent surveys \cite{survey,msurvey} provide a comprehensive literature review of deep learning-based super-resolution methods.
   
   \subsection{Implicit Neural Representation}
   Learning implicit neural representations (INRs) from discrete samples to form a continuous function has been a long-standing research problem in computer vision for numerous tasks.
   A recent trend in this field is to map discrete representations to coordinate-based continuous neural representations through implicit functions formed by neural networks, such as multi-layer perceptron (MLP).
   Chen \emph{et al.} \cite{LIIF} proposed a method to learn the INR of 2D images using the local implicit image function. Subsequent work \cite{videoinr} extended this \cite{LIIF} to apply in the video domain.
   Currently, most 3D view synthesis methods are based on the neural radiance fields (NeRF) \cite{NeRF} framework.
   NeRF can model a volumetric radiance field to render novel views with impressive visual quality using standard volumetric rendering \cite{rendering} and alpha compositing techniques \cite{alpha}.
   However, NeRF has a drawback of requiring massive training views and lengthy optimization iterations to learn the correct 3D geometry.
   Several follow-up works have attempted to optimize NeRF's training procedures, such as reducing the required training views \cite{pixnerf,dnerf}, accelerating convergence and rendering speed \cite{instngp}, and adapting NeRF to various domains, such as generative modeling \cite{GRAF,Giraffe}, anti-aliasing \cite{mip-nerf}, unbounded representation \cite{mip-nerf-360}, and RGB-D scene synthesis \cite{rgbd-nerf}.
   Recently, some researchers have attempted to employ INR-based methods to reconstruct high-quality medical images in both \textit{k}-space \cite{NeRP,IntraTomo,sscpn} and image space \cite{mednerf}. 
   \cite{NeRP,IntraTomo,sscpn} aim to synthesize high-quality sinograms to reconstruct sparse sampled CT from sparse views to dense views.
   \cite{mednerf} employs \cite{GRAF} to synthesize novel-view X-rays with training on multi-view X-rays.
   
   \subsection{Medical Image Super Resolution}\label{sec:related}
   Medical image super resolution (MISR) is an important task in medical image processing, which aims to reconstruct high-resolution (HR) medical slices from corresponding low-resolution (LR) ones.
   Initially, some conventional methods like \cite{MISR,CSLC} and widely-used interpolation methods like bicubic and tricubic interpolations \cite{tricubic} were employed in early researches.
   Inspired by \cite{dlsr}, recent studies have shifted their focus towards using deep learning-based super-resolution networks in the medical domain. 
   Lim \emph{et al.} \cite{edsr} employ deep learning-based super-resolution networks to upsample medical images.
   Some studies upsample each 2D LR medical slice to acquire the corresponding HR one, such as \cite{ssp,ctsrgan,csn}. On the other hand, Chen \emph{et al.} \cite{gan3dmdcn} and Wang \emph{et al.} \cite{egan} use 3D DenseNet-based networks to generate HR volumetric patches from LR ones.
   Recent researches have been focusing on medical image arbitrary-scale super-resolution (MIASSR), which aims to upsample medical slices at arbitrary scales by a single model.
   Peng \emph{et al.} \cite{saint} introduce SAINT, a method that deals with anisotropic super resolution on the $z$-axis at integer scales.
   Wu \emph{et al.} \cite{ARSSR} propose ArSSR, an INR-based method that can upsample MRI volumes at arbitrary scales in a continuous domain.
   Additionally, Wang \emph{et al.} \cite{uassr} propose a weakly-supervised framework that uses unpaired LR and HR medical volumes for supervision.
   As discussed in Section \ref{sec:intro}, MIASSR methods may be susceptible to the distribution gap between training and test data. Moreover, the availability of high-quality HR medical volumes can limit the applicability of these models in certain scenarios.
   
   
   \begin{figure*}[!t]
     \centering
     \includegraphics[width=6.8in]{overall.pdf}
     \caption{
       The overall framework of the proposed \textit{CuNeRF}.
       To synthesize a pixel (\textcolor{red}{red circle}), \textbf{(a)} \textit{CuNeRF} first uniformly samples the points $(x,y,z)$ within the cube space (\textcolor{Orchid}{purple cube}).
       Then, \textit{CuNeRF} obtains the coarse estimation (\textcolor{LightSkyBlue}{blue cube}) by feeding the sampling points into an MLP $F_{\Theta}$ to produce the corresponding pixel intensity $c$ and volume density $\sigma$.    
       \textbf{(b)} Subsequently, assuming $\sigma$ of each sampling point is only related to the distance with the target pixel, \textit{CuNeRF} computes the coarse output of the target pixel via volume integral.
       \textbf{(c)} Finally, \textit{CuNeRF} resamples the points under the PDF of coarse estimation to acquire the fine estimation (\textcolor{BurntOrange}{orange cube}) of the cube.
       The fine output is generated by the same procedures of \textbf{(b)}.
       These two rendering functions are differentiable, and thus \textit{CuNeRF} can be optimized by minimizing the rendering loss in Eq \ref{eq:render_loss}.
       We employ the fine output as the result of the target pixel.
     }
     \label{fig:overall}
   \end{figure*}
   
   \section{Preliminary: NeRF}\label{sec:NeRF}
   Neural radiance field (NeRF) \cite{NeRF} aims to build the continuous mapping from $(\mathbf{x}, \mathbf{d})$ to $(\mathbf{c}, \sigma)$, where $\mathbf{x}=(x,y,z)$ and $\mathbf{d}$ denote location and viewing direction, while $\mathbf{c}$ and $\sigma$ represent content color and volume density, respectively.
   NeRF's techniques can be summarized as follow:
   
   \noindent\textbf{Ray sampling.} NeRF first constructs the ray $\mathbf{r}(t)=\mathbf{o}+t\mathbf{d}$ that emits from the center of projection $\mathbf{o}$ and passes through the pixel along the viewing direction $\mathbf{d}$.
   Subsequently, NeRF samples $N$ points along the ray from near plane $t_{n}$ to far plane $t_{f}$ predefined.
   For each sampling point $\mathbf{r}(t_{k})$, NeRF employs a positional encoding function $\gamma(\cdot)$ to map the location $\mathbf{x}_{k}$ and view direction $\mathbf{d}$ into higher dimensional space as:
   \begin{equation}
     \gamma(\rho)=\rho\bigcup^{L-1}_{i=0}(\sin(2^{i}\rho), \cos(2^{i}\rho)),\ where\ L\in\mathbb{N}.
     \label{eq:PE}
   \end{equation}
   where $\rho$ denotes an arbitrary vector and $L$ is a hyperparameter set to $10$ as default.
   
   \noindent\textbf{Volume rendering.} The pixel color $\mathbf{C}(\mathbf{r})$ can be modeled as the integral of the corresponding ray $\mathbf{r}$ as:
   \begin{equation}
     \mathbf{C}(\mathbf{r}) = \int_{t_{n}}^{t_{f}}\frac{\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t),\mathbf{d})dt}{\exp(\int_{t_{n}}^{t}\sigma(\mathbf{r}(s))ds)},
     \label{eq:integral}
   \end{equation}
   where $\mathbf{c}(\cdot)$ and $\sigma(\cdot)$ denote the color and volume density functions.
   In practice, NeRF employs a multi-layer perceptron (MLP) $F_{\Theta}$ to estimate these two functions.
   For each sampling point $\mathbf{r}(t_{k})$, MLP $F_{\Theta}$ predicts the corresponding color $\mathbf{c}_{k}$ and volume density $\sigma_{k}$ by:
   \begin{equation}
   (\mathbf{c}_{k}, \sigma_{k}) = F_{\Theta}(\gamma(\mathbf{x}_{k}), \gamma(\mathbf{d})).
   \label{eq:MLP}
   \end{equation}
   Given the estimated results of the $N$ sampling points from $t_{n}$ to $t_{f}$, we can approximate the volume rendering integral using numerical quadrature as introduced by \cite{Max}:
   \begin{equation}
     \hat{\mathbf{C}}(\mathbf{r})=\sum^{N}_{i=1}\frac{1-\exp(-\sigma_{i}(t_{i+1}-t_{i}))}{\exp(\sum_{j=1}^{i}\sigma_{j}(t_{j+1}-t_{j}))}\mathbf{c}_{i},
     \label{eq:volume}
   \end{equation}
   where $\hat{\mathbf{C}}(\mathbf{r})$ is the predicted color of the pixel.
   
   \noindent\textbf{Hierarchical volume rendering.} NeRF also renders the scenes to refine the result by allocating samples proportionally to their expected volume distribution.
   NeRF simultaneously optimizes two MLPs, \emph{i.e.}, the coarse one $F^{c}_{\Theta}$ and the fine one $F^{f}_{\Theta}$.
   NeRF first samples $N_{c}$ points and obtain the coarse output $\hat{\mathbf{C}}_{c}(\mathbf{r})$ by Eq \ref{eq:volume}, which can be rewrited as $\hat{\mathbf{C}}_{c}(\mathbf{r})=\sum^{N_{c}}_{i=1}w_{i}\mathbf{c}_{i}$.
   A piecewise-constant PDF related to the sampling points along the ray can be produced by $\hat{w} = w_{i}/\sum^{N_{c}}_{j=1}w_{j}$.
   NeRF then samples $N_{f}$ points from this distribution by inverse transform sampling (ITS) and computes the fine outputs $\hat{\mathbf{C}}_{f}(r)$ using all $N_{c}+N_{f}$ sorted sampling points.
   Let $\mathcal{R}$ represent the batch, and these two MLPs can be optimized by the following rendering loss:
   \begin{equation}
   \mathcal{L}=\sum_{\mathbf{r}\in \mathcal{R}}\left[\|g.t. - \hat{\mathbf{C}}_{c}(\mathbf{r})\|^{2}_{2}+\|g.t. - \hat{\mathbf{C}}_{f}(\mathbf{r})\|^{2}_{2}\right],
   \end{equation}
   where $g.t.$ denotes the ground truth of the rendering pixels.
   
   \section{Method}
   In this section, we first explain our motivation, analyzing the limitations of NeRF for medical volumes.
   Subsequently, we propose cube-based NeRF (CuNeRF), a novel yet efficient method for ``zero-shot'' medical image arbitrary-scale super resolution (MIASSR), extending NeRF's application scenarios in the medical domain.
   Specifically, we first normalizes the medical volumes by volumetric normalization, and then train the model via proposed differentiable modules: cube-based sampling, isotropic volume rendering, and cube-based hierarchical rendering. 
   During training, \textit{CuNeRF} is building a coordinate-intensity continuous function whose input is a 3D location $\mathbf{x}=(x,y,z)$ and output is a pixel value $c$.
   After optimization, \textit{CuNeRF} can predict pixels at any position within the range.
   As a result, \textit{CuNeRF} achieves free-viewpoint and arbitrary-scale rendering, which can render desired medical slices (desired viewpoints and scales) by feeding the corresponding plane equations.
   Figure \ref{fig:overall} depicts the framework of \textit{CuNeRF}, and the subsequent techniques are described in the following. 
   
   \subsection{Motivation} \label{sec:holes}
   NeRF's modeling strategy may not be suitable for direct application on medical volumes.
   To explain this limitation, we provide an example of NeRF's modeling strategies applied to medical volumes in Figure \ref{fig:motivation} \textbf{(a)}.
   Visually, NeRF is trained to model the volumetric space along the ray emitted by each training pixel.
   Medical volumes only contain three orthogonal slices, which differs from multi-view photos.
   Hence, NeRF's modeling techniques cannot cover the entire fields, leaving some ``holes'' (\emph{i.e.,} unmodeled space) between adjacent training pixels, as depicted in Figure \ref{fig:motivation} \textbf{(a)}.
   Consequently, NeRF may produce sub-optimal results while rendering the contents within the holes.
   As shown in Figure \ref{fig:examples}, NeRF$^\dag$\footnote{We use the MLP depicted in Figure \ref{fig:MLP} to adapt volume data.} struggles to deal with 3D MISR issues, producing grid-like artifacts.
   Consequently, NeRF may struggle to represent high-quality HR medical volumes at arbitrary upsampled scales.
   
   To address the hole-forming issues in NeRF, we introduce cube-based sampling, which samples cubes (3D volumetric space) instead of rays (1D space) to fill the holes between adjacent training pixels by the spatial overlaps, as demonstrated in Figure \ref{fig:motivation} \textbf{(b)}.
   To adapt cube-based sampling, we further propose isotropic volume rendering and cube-based hierarchical rendering modules.
   These modules will be introduced in the following subsections.
   
   \subsection{Volumetric Normalization}
   To build the continuous representations for the given medical volumes, we first normalize the whole volumetric space $H\!\times\!W\!\times\!L$ into an $\ell_{\infty}$-norm open ball as:
   \begin{equation}
     \mathcal{B}(\hat{\mathbf{x}}_{\mathbf{o}}, \pi)=\{\hat{\mathbf{x}}:\|\hat{\mathbf{x}}-\hat{\mathbf{x}}_{\mathbf{o}}\|_{\infty}<\pi\},
   \end{equation}
   where $\hat{\mathbf{x}}_{\mathbf{o}}$ is set to $(0,0,0)$ related to the center $\mathbf{x}_{\mathbf{o}}=(\frac{H}{2}, \frac{W}{2}, \frac{L}{2})$ of the medical volume.
   To adapt the positional encoding $\gamma(\cdot)$ introduced in Eq \ref{eq:PE}, each positional coordinate $\mathbf{x}_{t}=(x_{t},y_{t},z_{t})$ within the medical volume is transformed into the field coordinate $\hat{\mathbf{x}}_{t}=(\hat{x}_{t},\hat{y}_{t},\hat{z}_{t})$. 
   The normalization function $\mathcal{N}(\cdot)$ is formulated as:
   \begin{equation}
     \hat{\mathbf{x}}_{t} =\left(\frac{2\pi(x_{t}-\frac{H}{2})}{H+2P}, \frac{2\pi(y_{t}-\frac{W}{2})}{W+2P}, \frac{2\pi(z_{t}-\frac{L}{2})}{L+2P}\right),
   \end{equation}
   where $P$ is a hyperparameter as the padding size.
   
   \subsection{Cube-based Sampling}
   Neural representation methods aim to build the continuous representation of medical volumes.
   However, NeRF suffers from hole-forming issues, which may leave some holes and thus synthesizes grid-like artifacts.
   To circumvent the hole forming in representation fields, we propose a novel sampling strategy: cube-based sampling, which samples the points within a cube (3D volumetric space).
   Specifically, for the pixel location $\hat{\mathbf{x}}_{t}$, \textit{CuNeRF} uniformly samples a set of points within the cube space $\mathcal{B}(\hat{\mathbf{x}}_{t}, \frac{l}{2})$.
   Each sampling point $\hat{\mathbf{x}}_{i}$ is chosen under the uniform distribution $\mathcal{U}$ by:
   \begin{equation}
     \hat{\mathbf{x}}_{i}\sim\mathcal{U}\left[\mathcal{B}(\hat{\mathbf{x}}_{t}, \frac{l}{2})\right], 
   \end{equation}
   where $l$ denotes the edge length of the cube.
   We employ the group of these $N$ sampling points to approximate the cube space.
   Due to the spatial overlaps between adjacent cubes, the representation fields can be well-covered by employing the proposed cube-based sampling in optimization.
   As a result, the representation field can be densely modeled with the same sampling time as NeRF.
   
   \subsection{Isotropic Volume Rendering}
   As introduced in \ref{sec:NeRF}, the pixel color related to the ray $\mathbf{r}$ is computed by an integral in Eq \ref{eq:integral}.
   Intuitively, the pixel color $\mathbf{C}(\hat{\mathbf{x}}_{t}, l)$ related to the cube space $\mathcal{B}(\hat{\mathbf{x}}_{t}, \frac{l}{2})$ can be computed by the following triple integral as:
   \begin{equation}
     \mathbf{C}(\hat{\mathbf{x}}_{t}, l)=\!\!\!\iiint\limits_{\mathcal{B}(\hat{\mathbf{x}}_{t}, \frac{l}{2})}\!\frac{\sigma(\hat{x},\hat{y},\hat{z})\mathbf{c}(\hat{x},\hat{y},\hat{z})d\hat{x}d\hat{y}d\hat{z}}{\exp(\int_{\hat{x}_{n}}^{\hat{x}}\!\int_{\hat{y}_{n}}^{\hat{y}}\!\int_{\hat{z}_{n}}^{\hat{z}}\!\sigma(x,y,z)dxdydz)},
   \end{equation}
   where $(\hat{x}_{n},\hat{y}_{n},\hat{z}_{n})=(\hat{x}_{t}-\frac{l}{2}, \hat{y}_{t}-\frac{l}{2}, \hat{z}_{t}-\frac{l}{2})$ denotes the initial location of the triple integral while $\mathbf{c}(\cdot)$ and $\sigma(\cdot)$ represent the color and volume density functions.
   However, since NeRF samples $N$ points to approximate the volume rendering integral of the ray using numerical quadrature in Eq \ref{eq:volume}, it is required to sample $N^{3}$ points to model the cube with the same density, leading to massive computational costs.
   
   Inspired by CRF \cite{crf} that assigns the nearby pixels with similar potentials, we assume the volume density $\sigma$ of each point $\hat{\mathbf{x}}$ within the cube $\mathcal{B}(\hat{\mathbf{x}}_{t}, \frac{l}{2})$ is only related to the $\ell_{p}$-norm distance $r=\|\hat{\mathbf{x}} - \hat{\mathbf{x}}_{t}\|_{p}$ between the centroid and itself.
   Hence, the volumetric distribution of the cube is isotropic towards the value of $r$. 
   The above triple integral can be converted into the spherical coordinates system and simplified by:
   \begin{equation}
     \label{eq:sim}
     \mathbf{C}(\hat{\mathbf{x}}_{t}, l) = 4\pi\int_{0}^{\hat{r}}\frac{r^{2}\sigma(\hat{\mathbf{x}}_{t}, r)c(\hat{\mathbf{x}}_{t}, r)dr}{\exp(4\pi\int_{0}^{r}s^{2}\sigma(\hat{\mathbf{x}}_{t}, s)ds)}, 
   \end{equation}
   where $\hat{r}=\|(\frac{l}{2},\frac{l}{2},\frac{l}{2})\|_{p}$ denotes the max distance of $r$ within the cube.
   The derivation detail of Eq \ref{eq:sim} is shown in the supplementary materials.
   Given $N$ sampling points by the proposed cube-based sampling, \textit{CuNeRF} first sorts these points by the distance $r$.
   Subsequently, the integral of the cube is approximated via numerical quadrature rules:
   \begin{equation}
     \hat{\mathbf{C}}(\hat{\mathbf{x}}_{t}, l)\!=\!4\pi\!\sum^{N}_{i=1}\frac{r_{i}^{2}(1-\exp(-\sigma_{i}(r_{i+1}\!-\!r_{i})))}{\exp(4\pi\sum_{j=1}^{i}r_{i}^{2}\sigma_{j}(r_{j+1}\!-\!r_{j}))}\mathbf{c}_{i},
     \label{eq:M_volume}
   \end{equation}
   where $\hat{\mathbf{C}}(\hat{\mathbf{x}}_{t}, l)$ denotes the predicted color of $\hat{\mathbf{x}}_{t}$.
   
   \begin{table*}[!t]
     \setlength{\tabcolsep}{1.7mm}
     \caption{
       Quantitative comparisons of start-of-the-arts on brain tumor MRI volumes selected from MSD \cite{MSD} dataset in terms of PSNR/SSIM metrics for 3D MISR in this table.
       \textbf{Bold} and \underline{underline} texts indicate the best and second best performance.
     }
     \label{tab:3DMISR}
   \centering
   \footnotesize
   \begin{tabular}{l|cccccccc}
   \hline
                            & $\times2$                       & $\times2.5$                        & $\times3$                          & $\times3.5$                           & $\times4$                             & $\times5$                       & $\times6$                             & $\times8$  \\\hline
   Bicubic                  & 33.75 / 0.9469      & 30.84 / 0.9271         & 30.74 / 0.9161                     & 29.19 / 0.8873                        & 28.67 / 0.8721                        & 28.14 / \underline{0.8687}      & 26.83 / 0.8521                        & 26.54 / \underline{0.8376}\\
   ArSSR \cite{ARSSR}       & \underline{36.98} / \underline{0.9690}& \underline{35.24} / \underline{0.9398}& \underline{34.69} / \underline{0.9199}& \underline{34.01} / \underline{0.8974}& \underline{33.21} / \underline{0.8910}& \underline{30.50} / 0.8624      & \underline{29.96} / \underline{0.8543}& \underline{28.43} / 0.8353 \\
   NeRF$^{\dag}$ \cite{NeRF}  & 29.33 / 0.8472                  & 27.03 / 0.8392                     & 25.98 / 0.8220                     & 25.47 / 0.8157                        & 25.12 / 0.8088                        & 24.50 / 0.7767                  & 23.45 / 0.7549                        & 22.63 / 0.7275  \\
   \textit{CuNeRF}          & \textbf{39.62} / \textbf{0.9786}      & \textbf{37.56} / \textbf{0.9441}            & \textbf{36.24} / \textbf{0.9267}& \textbf{35.81} / \textbf{0.9189}      & \textbf{35.01} / \textbf{0.9031}      & \textbf{34.73} / \textbf{0.8952}& \textbf{33.69} / \textbf{0.8800}      & \textbf{31.19} / \textbf{0.8675}   \\\hline
   \end{tabular}
   \end{table*}
   
   \subsection{Cube-based Hierarchical Rendering}
   To refine the results, \textit{CuNeRF} allocates sampling points proportionally to their expected volume distribution within the cube.
   Similar to NeRF, \textit{CuNeRF} also simultaneously optimizes the coarse and fine MLPs. 
   As obtaining the coarse output $\hat{\mathbf{C}}_{c}(\hat{\mathbf{x}}_{t}, l)$, \textit{CuNeRF} first samples $N_{f}$ numbers of $r$ using ITS.
   Subsequently, for each $r$, we use the hierarchical sampling function $\zeta_{p}(\cdot)$ to sample the corresponding sampling point:
   \begin{equation}
   \hat{\mathbf{x}}_{f}=\zeta_{p}(r, \varphi, \theta),
   \end{equation}
   where $\varphi$ and $\theta$ are the randomly sampled spherical coordinates, and $\zeta_{p}(\cdot)$ converts the $\ell_{p}$-norm spherical coordinates $(r,\varphi,\theta)$ to the Cartesian coordinates $\hat{\mathbf{x}}$.
   If $p\neq \infty$, we allow $\hat{\mathbf{x}}_{f}$ can beyond the cube space $\mathcal{B}(\hat{\mathbf{x}}_{t}, \frac{l}{2})$.
   After obtaining fine outputs $\hat{\mathbf{C}}_{f}(\hat{\mathbf{x}}_{t}, l)$ at Eq \ref{eq:M_volume} using the sorted union of $N_{c}+N_{f}$ sampling points, \textit{CuNeRF} can be optimized in the following proposed adaptive rendering loss:
   \begin{equation}
     \begin{split}
     \mathcal{L}_{A}\!=\!\!\!\!\!\sum_{\hat{\mathbf{x}}_{t}\in \mathcal{R}}\!\!\!\left[\lambda\|g.t. \!-\! \hat{\mathbf{C}}_{c}(\hat{\mathbf{x}}_{t}, l)\|^{2}_{2}\!+\!\|g.t. \!-\! \hat{\mathbf{C}}_{f}(\hat{\mathbf{x}}_{t}, l)\|^{2}_{2}\right]\!\!,
     \end{split}
     \label{eq:render_loss}
   \end{equation}
   where $\lambda=\|g.t. - \hat{\mathbf{C}}_{f}(\hat{\mathbf{x}}_{t}, l)\|^{\frac{1}{2}}$ is an adaptive regularization term to alleviate the overfitting brought by the ``coarse'' loss while $\mathcal{R}$ denotes the batch.
   
   \begin{table}[!t]
     \setlength{\tabcolsep}{1.7mm}
     \caption{
       Quantitative comparisons of start-of-the-arts on kidney tumor CT volumes selected from KiTS19 \cite{kits19} dataset in terms of PSNR/SSIM metrics for anisotropic MISR in this table.
       \textbf{Bold} and \underline{underline} texts indicate the best and second best performance.
     }
     \label{tab:ZMISR}
   \centering
   \footnotesize
   \begin{tabular}{l|ccc}
   \hline
                            & $\times2$                             & $\times4$                          & $\times6$     \\\hline%& $\times8$  \\\hline
   Bicubic                  & 37.75 / 0.9498                        & 33.76 / 0.9149                     & 31.24 / 0.8967\\%& 29.03 / 0.8572\\
   SAINT \cite{saint}       & \textbf{39.47} / \textbf{0.9782}      & \underline{36.61} / \textbf{0.9574}& \textbf{33.63} / \textbf{0.9301}\\%& 31.78 / 0.9188\\
   NeRF$^{\dag}$ \cite{NeRF}  & 36.50 / 0.9383                        & 34.14 / 0.9181                     & 31.18 / 0.9020\\%& 30.56 / 0.8748  \\
   \textit{CuNeRF}          & \underline{38.33} / \underline{0.9663}& \textbf{36.64} / \underline{0.9480}& \underline{32.93} / \underline{0.9229}\\\hline%& 32.44 / 0.9216   \\\hline
   \end{tabular}
   \end{table}
   
   \subsection{Free-Viewpoint \& Arbitrary-Scale Rendering}\label{sec:test}
   After optimization, \textit{CuNeRF} can predict the pixels corresponding to arbitrary 3D coordinates.
   Therefore, \textit{CuNeRF} can represent medical slices with free viewpoints and arbitrary scales by feeding the corresponding plane coordinates.
   Specifically, to render a medical slice with the given position $\hat{\mathbf{x}}$, viewpoint $\mathbf{d}$ and scale $\delta$, we first construct a base plane at $\hat{\mathbf{x}}_{o}$ with the sampling scale $\delta$.
   Subsequently, we employ the translation matrix $\mathcal{M}_{T}$ to move slices from $\hat{\mathbf{x}}_{o}$ to $\hat{\mathbf{x}}$.
   Finally, since the viewpoint $\mathbf{d}$ can be formulated as rotating $\phi$ degrees around a certain axis $\overset{\rightarrow}{n}$, we can obtain the corresponding rotation matrix $\mathcal{M}_{R}$ via Rodrigues' rotation formula \cite{rodrigues}.
   
   We provide some visual examples in the anonymous link.
   As shown, \textit{CuNeRF} is capable to yield high-quality medical slices at continuous-valued scales from 1.0 to 8.0.
   Moreover, \textit{CuNeRF} can synthesize medical images with a viewpoint rotating 360 degrees around an arbitrary coordinate axis $\overset{\rightarrow}{n}$.
   Consequently, compared to existing methods, \textit{CuNeRF} is capable to provide richer visual information.
   
   
   \section{Experiments}
   In this section, we conduct extensive experiments and in-depth analysis to demonstrate the superiority of our \textit{CuNeRF} in representing high-quality slices at arbitrary scales. 
   For fair comparisons, the hyperparameters and model settings are consistent in all experiments.
   
   \subsection{Experimental Details}
   \noindent\textbf{Datasets. }
   We comprehensively compare the proposed \textit{CuNeRF} and the existing advances on 80 medical volumes.
   These medical volume data consist of 2 different modalities: CT and MRI.
   More specifically, we select 40 T1-weighted brain tumor MRI volumes from the publicly available Medical Segmentation Decathlon (MSD) \cite{MSD} while we also take 40 CT volumes from the 2019 Kidney Tumor Segmentation Challenge (KiTS19) \cite{kits19} datasets, respectively.
   All MRI volumes have the same dimension of $240\!\times\!240\!\times\!155$.
   The number of CT scan slices along the $z$-axis is different, while the dimension of each slice is $512\!\times\!512$
   In experiments, all the CT volumes are resized into $512\!\times\!512\!\times\!512$.
   % Our experiments directly employ those raw medical data without transformation and resampling techniques.
   
   \noindent\textbf{Multi-Layer Perceptron Architecture. }\label{sec:MLP}
   Due to the vast difference between the medical imaging system and conventional cameras, we further reform NeRF's MLP to adapt to medical volume data.
   Figure \ref{fig:MLP} depicts MLP's architecture, where the input is a 3D location $\hat{\mathbf{x}} = (\hat{x}, \hat{y}, \hat{z})$ and the output is a 2D union of color $\mathbf{c}$ and volume density $\sigma$.
   Similar to NeRF, we first employ $\gamma(\cdot)$ to encode the input $\mathbf{x}$, and then feed it to the fully-connected network to obtain the output $(\mathbf{c}, \sigma)$.
   Thus MLP is optimized by minimizing $\mathcal{L}_{A}$.
   
   \begin{figure}[!t]
     \centering
     \includegraphics[width=3.2in]{MLP.pdf}
     \caption{
       The architecture of our multi-layer perceptron (MLP).
       For a given field coordinate $\hat{\mathbf{x}}$, it is first encoded by the positional encoding $\gamma(\cdot)$ in Eq \ref{eq:PE} as the input feature (\textcolor{LimeGreen}{green blocks}).
       Subsequently, the input features pass through 9 fully-connected layers (\textcolor{LightSkyBlue}{blue blocks}), each with $256$ channels and the ReLU activation (\textbf{black arrows}), where we concatenate (\textbf{+}) the input features to the 5th and 9th intermediate hidden layers as the skip connections.
       Finally, we downscale the feature channels from $256$ to $128$ and obtain the outputs (\textcolor[rgb]{1, 0.75, 0.25}{yellow blocks}), which contain volume density $\sigma$, and pixel value $\mathbf{c}$ with sigmoid activations (\textcolor{BurntOrange}{orange arrow}).
     }
     \label{fig:MLP}
   \end{figure}
   
   \noindent\textbf{Implementation Details. }
   \textit{CuNeRF} is implemented on top of \cite{nerf-pytorch}, a Pytorch \cite{Pytorch} re-implementation of NeRF.
   Meanwhile, our experiments are also based on Pytorch \cite{Pytorch} framework, and run on a single GeForce RTX 3090 GPU with 24G memory.
   Similar to NeRF, \textit{CuNeRF} first samples $64$ points for the coarse model $F^{c}_{\Theta}$ and feeds $192$ sampling points (the sorted union of $64$ coarse and $128$ fine samples) into the fine model $F^{f}_{\Theta}$.
   We employ the $\ell_{2}$-norm distance for isotropic volume rendering and hierarchical cubic rendering functions as default while the edge length $l$ of the cube is set to $1$ pixel distance as default.
   Hence, the hierarchical sampling function $\zeta_{2}(\cdot)$ converts $(r, \varphi, \theta)$ to $\hat{\mathbf{x}}=(\hat{x}, \hat{y}, \hat{z})$ by:
   \begin{equation}
       \begin{split}
         \hat{\mathbf{x}}=(r\sin{\varphi}\cos{\theta}, r\sin{\varphi}\sin{\theta}, r\cos{\varphi}),\\ where\ \varphi\sim\mathcal{U}[0, \pi],\ \theta\sim\mathcal{U}[0, 2\pi].
       \end{split}
   \end{equation}
   
   For training, we employ Adam \cite{adam} as the optimizer with a weight decay of $10^{-6}$ and a batch size of $2048$.
   The maximum iteration is set to $250,000$, and the learning rate is annealed logarithmically from $2\times10^{-3}$ to $2\times10^{-5}$ during the whole training time.
   
   For testing, the results are obtained by feeding all the coordinates of the given plane equations into our model (seeing details in Section \ref{sec:test}).
   Note that we do not use any pre-processing and post-processing techniques to improve our results in the following experiments.
   
   \noindent\textbf{Evaluation Metrics. }
   We use two quantitative metrics: Peak Signal-to-Noise Ratio (PSNR) and Structured Similarity Index (SSIM) \cite{ssim} to measure the image quality of different medical image super-resolution methods.
   
   \begin{table}[!t]
     \setlength{\tabcolsep}{1.4mm}
     \caption{
       Quantitative comparisons of \textit{CuNeRF} and its ablation variants on brain tumor MRI volumes selected from MSD \cite{MSD} dataset in terms of PSNR/SSIM metrics for 3D MISR in this table.    
       \textbf{Bold} text indicates the best performance.
     }
   \label{tab:abla}
   \centering
   \footnotesize
   \begin{tabular}{ccc|cccc}
   \hline
   \textit{CuS} & \textit{IVR} & $\mathcal{L}_{A}$ & $\times2$     & $\times4$     & $\times8$     \\\hline
       &     &                                     & 29.33 / 0.8472& 25.12 / 0.8088& 22.63 / 0.7275 \\
   \checkmark&     &                               & 35.82 / 0.9244& 33.29 / 0.8703& 27.77 / 0.8398 \\
   \checkmark&\checkmark&                          & 38.15 / 0.9524& 34.27 / 0.8887& 29.34 / 0.8455 \\
   \checkmark&\checkmark&\checkmark                & \textbf{39.62} / \textbf{0.9786}& \textbf{35.01} / \textbf{0.9031}& \textbf{31.19} / \textbf{0.8675}\\\hline
   \end{tabular}
   \end{table}
   
   
   \subsection{Comparison With State-of-the-art Methods}
   We compare the proposed \textit{CuNeRF} with 6 state-of-the-art methods, including 2 recent MIASSR methods: ArSSR \cite{ARSSR} and SAINT \cite{saint}, 1 conventional method: bicubic interpolation, and NeRF$^{\dag}$ \cite{NeRF}.
   As introduced in Section \ref{sec:related}, SAINT \cite{saint} can only upsample LR CT volumes on a single axis.
   For fair comparisons, as given a upsampling scale $\delta$, we evaluate these methods under the following two settings: \textit{(i)} \textbf{3D MISR.} Upsampling the donwsampled volume from $\frac{H}{\delta}\!\times\!\frac{W}{\delta}\!\times\!\frac{L}{\delta}$ to $H\!\times\!W\!\times\!L$; \textit{(i)} \textbf{Anisotropic MISR.} Upsampling the donwsampled volume from $H\!\times\!W\!\times\!\frac{L}{\delta}$ to $H\!\times\!W\!\times\!L$;
   Note that NeRF$^{\dag}$ and \textit{CuNeRF} are trained with the same experimental settings, including the same architecture of the MLP $F_{\Theta}$ introduced in Section \ref{sec:MLP}.
   
   \noindent\textbf{Quantitative Comparison. }
   We report 3D MISR and anisotropic MISR based on the increasing upsampled scales in Table \ref{tab:3DMISR} and Table \ref{tab:ZMISR}, respectively.
   As demonstrated, for the 3D MISR challenge on MRI volumes, \textit{CuNeRF} surpasses all the competitors with consistent preferable performance at various upsampling scales.
   For anisotropic MISR challenge on CT volumes, \textit{CuNeRF} achieves comparable performance to SAINT \cite{saint}.
   Note that \textit{CuNeRF} is a zero-shot framework that can represent medical volumes in a continuous domain, while SAINT \cite{saint} only deals with integer factors.
   Compared to fully-supervised MIASSR methods: ArSSR \cite{ARSSR} and SAINT \cite{saint}, the \textit{CuNeRF} is more robust at presenting large-scale medical slices and capable to deal with different modalities (CT and MRI), suggesting \textit{CuNeRF} owns broader application scenarios.
   It is also worth noting that NeRF$^{\dag}$ achieves comparable performance for anisotropic MISR but fails in 3D MISR.
   Because the rendering content of anisotropic MISR is along the ray ($z$-axis) while 3D MISR must query the contents within the holes, the experimental results of NeRF$^{\dag}$ confirm the correctness of our motivation.
   
   \begin{table}[!t]
     \setlength{\tabcolsep}{1.7mm}
     \caption{
       Quantitative comparisons of \textit{CuNeRF} under different settings on brain tumor MRI volumes selected from MSD \cite{MSD} dataset in terms of PSNR/SSIM metrics for 3D MISR in this table. 
       \textbf{Bold} and \underline{underline} texts indicate the best and second best performance.
     }
     \label{tab:diff}
     \centering
     \footnotesize
     \begin{tabular}{l|ccc}
     \hline
                                  & $\times2$                           & $\times4$                        & $\times8$ \\\hline
     \textit{CuNeRF} default.     &\underline{39.62} / \textbf{0.9786}  & \textbf{35.01} / \textbf{0.9031} & \underline{31.19} / \underline{0.8675}         \\
     \textit{CuNeRF} $p=\infty$   &35.78 / 0.9348                       & 32.85 / 0.8853                   & 29.25 / 0.8433         \\
     \textit{CuNeRF} $l=0.5$      &38.17 / 0.9621                       & \underline{35.00} / 0.9018       & \textbf{31.53} / \textbf{0.8704}         \\
     \textit{CuNeRF} $l=2$        &\textbf{39.65} / \underline{0.9723}  & 34.57 / \underline{0.9011}       & 30.85 / 0.8608          \\
     \hline
   
     \end{tabular}
   \end{table}
   
   \noindent\textbf{Visual Comparison. }
   We visualize the rendering results of \textit{CuNeRF} and other competitors on MRI (rows 1 and 2) and CT (rows 3 and 4) modalities in Figure \ref{fig:visual}.
   It can be observed that \textit{CuNeRF} well represents the medical slices at various scales.
   Compared to the exhibited methods, \textit{CuNeRF} is most similar to the ground truths, achieving better visual verisimilitude and reducing aliasing artifacts, especially in representing large-scale medical slices.
   Since NeRF$^{\dag}$ exhibits grid-like artifacts in rendering high-quality medical slices at larger-valued scales, the visualization results prove the effectiveness of \textit{CuNeRF}, which extends NeRF's capability to build high-quality continuous representation for medical volumes.
   
   \begin{figure*}[!t]
     \centering
     \includegraphics[width=6.8in]{visual.pdf}
     \caption{
       Visual comparisons between \textit{CuNeRF} and state-of-the-art methods for 3D MISR and anisotropic MISR.
       The heatmaps on the right are provided to clearly visualize the difference related to GT images.
       For better visualization, the difference maps are provided to the right of the results.
       PSNR and SSIM for each image are inserted, \textcolor{red}{red} text indicates the best one.
     }
     \label{fig:visual}
   \end{figure*}
   
   \subsection{Ablation Study}
   In this subsection, we conduct comprehensive experiments to prove the correctness of \textit{CuNeRF}'s design.
   We first carry out ablation studies to investigate the effectiveness of the proposed modules.
   Subsequently, we evaluate the \textit{CuNeRF}'s performance under different settings.
   
   \noindent\textbf{\textit{CuNeRF}'s ablation variants. }
   We evaluate against several ablations of the proposed \textit{CuNeRF} with each module: \textit{CuS}, \textit{IVR} and $\mathcal{L}_{A}$ represent cube-based sampling, isotropic volume rendering, and adaptive rendering loss, respectively.
   The baseline model here is NeRF$^\dag$.
   As reported in Table \ref{tab:abla}, the baseline model struggles to deal with 3D MISR issues (row 1), while adopting \textit{CuS} instead of ray sampling can significantly improve the performance (row 2).
   Compared to NeRF's volume rendering function, employing \textit{IVR} (row 3) can further improve the slice synthesis quality, suggesting \textit{IVR} can better estimate the volumetric distribution, reducing aliasing artifacts raised by undersampling.
   Since the coarse term of NeRF's rendering loss may affect the optimization, $\mathcal{L}_{A}$ (row 4) is able to alleviate this distraction for achieving better results.
   
   \noindent\textbf{\textit{CuNeRF} under different settings. }
   We evaluate the performance of \textit{CuNeRF} under different settings: ``$p=\infty$'' employs $\ell_{\infty}$-norm distance of $r$, ``$l=0.5$'' and ``$l=2$'' represent setting the edge length of cube to $0.5$ and $2$ pixel distance, respectively.
   The default setting of \textit{CuNeRF} is introduced in Section \ref{sec:MLP}, where $p=2$ and $l=1$.
   
   As reported in Table \ref{tab:diff}, the default setting of \textit{CuNeRF} achieves consistent outperformance at various scales.
   In contrast, employing the $\ell_{\infty}$-norm is significantly inferior to the default one, which means $\ell_{2}$-norm distance is more suitable to model the continuous representation for medical volumes.
   Meanwhile, different values of cube edge $l$ also acquire comparable performance to the default one, suggesting the proposed \textit{CuNeRF} is a parameter-insensitive method with good robustness under different experimental settings.
   
   
   \section{Conclusion}
   Medical image arbitrary-scale super resolution (MIASSR) aims to employ a single well-trained model to upsample medical slices at arbitrary scales.
   However, there are two major limitations of existing MIASSR methods: \textit{(i)} relying on HR volumes; \textit{(ii)} limited generalization ability, limiting the model application scenarios.
   In this paper, we present \textit{CuNeRF}, a zero-shot MIASSR framework that can yield medical images at arbitrary viewpoints and scales in a continuous domain.
   Instead of learning the mapping between low-resolution (LR) and high-resolution (HR) volumes, \textit{CuNeRF} models the continuous representation from LR volumes via the proposed differentiable modules: cube-based sampling, isotropic volume rendering, and cube-based hierarchical rendering.
   Extensive experiments on magnetic resource imaging (MRI) and computed tomography (CT) modalities demonstrate that CuNeRF outperforms state-of-the-art MIASSR methods, yielding better visual synthesis effects and reducing aliasing artifacts at various upsampling factors.

   \section*{Acknowledgement}
   This project is supported by the Natural Science Foundation of China (No. 62072482).
   
   {\small
   \bibliographystyle{ieee_fullname}
   \bibliography{egbib}
   }
   
   \end{document}