\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\newcommand{\myparagraph}[1]{\vspace{0pt}\noindent{\bf #1}}
\newcommand{\blue}[1]{\textcolor{blue}{{#1}}}
\newcommand{\red}[1]{\textcolor{red}{{#1}}}
\newcommand{\green}[1]{\textcolor{green}{{#1}}}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{5825} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{What Can Human Sketches Do for Object Detection?\vspace{-0.5cm}}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
\noindent We thank all reviewers for their valuable comments. Minor issues will be carefully addressed upon acceptance.

\myparagraph{Response to Reviewer [\texttt{YH6C}]}\\
\noindent \textbf{Q1: Confusion on detecting multiple similar objects:} 
Our method has two setups: (i) category-level object detection that detects all objects similar to query sketch (Fig.~\red{7}). (ii) Fine-grained object detection that detects objects with fine-grained matching (e.g., same pose, Fig.~\red{8}). Given the sketch of a ``zebra", category-level OD detects multiple ``zebras" (L697). On the other hand, fine-grained OD will detect a specific ``zebra" from a herd of ``zebras" (L746). \emph{Our limitation} is that sketch query can only represent one object at a time -- ``dog" or ``cat", not both. To detect both ``cat" and ``dog", we must pass query embeddings independently.

% Our method has two setups: (i) category-level object detection that detects all objects similar to query sketch (Fig.~\red{7}). (ii) Fine-grained object detection that detects objects with fine-grained matching (e.g., same pose) and ignores the others (Fig.~\red{8}). Given the sketch of a ``zebra", category-level OD will detect multiple ``zebras" in photos (L697). On the other hand, fine-grained OD will detect a specific ``zebra" from a herd of ``zebras" (L746). Our limitation is that sketch query can only represent one object at a time -- ``dog" or ``cat", not both. To detect both ``cat" and ``dog", we must pass the query embeddings independently. We will improve the clarity in (Sec.~\red{4.7}).

\noindent \textbf{Q2: Dealing with occluded objects:} 
We present additional examples (success and failure cases) of detecting occluded ``horse" in an image when the sketch query only shows the ``head" of the ``horse" -- thanks to our part-based object detection ability (L787). We will add more occluded examples (success and failure) and discussion upon acceptance.

\hspace{0.2cm} \includegraphics[width=0.8\linewidth]{latex/figures/occlusion.pdf}


\noindent \textbf{Q3: Confusion on multiple boxes in Fig.~\red{9}:} The multiple boxes are \emph{not} false positives. They are initial proposals (\texttt{blue boxes}) with confidence score $\omega_{k} \geq 0.7$ prior NMS to get one predicted (\texttt{yellow}) box for single object.
% The multiple boxes in Fig.~\red{7}, \red{8}, \red{9} are \emph{not} false positives. They are initial proposals (\texttt{blue boxes}) with confidence score $\omega_{k} \geq 0.7$ (Eq.~\red{7}) before using using non-maximum suppression (NMS) to get a single box for a single object marked as \texttt{yellow box} (L581).

\noindent \textbf{Q4: Extending to open-world setup:} 
Thanks! In open-world setup, a model with $C$ known classes in train set can recognise unknown classes and update the base model via incremental learning [\green{A}]. Our method already works for zero-shot in \emph{open-vocab} setup (L134) -- the model detects any queried sketch irrespective of whether it is in train set or not. We achieve this via a myriad of prototype learning, generalised SBIR (with open-vocab) with CLIP and prompt learning. We will cite [\green{B}] and clarify upon acceptance.

% Thanks! Open-world setup is defined as a model with $C$ known classes in the train set can recognise unknown classes and update the base model via incremental learning [\green{A}]. Our method already works for zero-shot in open-vocab setup (L134) -- the model can detect any queried sketch irrespective of whether it is in the training set or not. This is enabled through a combination of prototype learning and generalised SBIR (with open-vocab capabilities, L478) with CLIP and prompt learning. We will cite [\green{B}] and clarify upon acceptance.

\myparagraph{Response to Reviewer [\texttt{UztV}]}\\
\noindent \textbf{Q1: Confusion on object detection setup:}
\blue{This is clearly a confusion. We do not sketch an object with a single image in front. Instead, we draw an object from \emph{our imagination} and detect all similar instances from a gallery of photos (L105). We will clarify this upon acceptance. (i) For category-level OD, we detect all similar objects (e.g., ``zebra" in different poses) across all photos in the gallery, (ii) for fine-grained OD, we detect a specific object (e.g., ``zebra" in the same pose as sketch) from multiple photos in gallery. For more clarity, we update detection results for category-level OD (out-of-vocab setup, see L551) below.}
% \blue{This is clearly a confusion. We want to detect objects from a gallery of photos that match the query sketch. (i) For category-level OD, we detect all similar objects (e.g., ``zebras" in different poses) across all photos in the gallery, (ii) for fine-grained OD, we detect a specific object (e.g., ``zebra" with same pose as sketch) from the photos in the gallery. For more clarity, we updated detection results for category-level OD below. Note, our method is NOT limited to detecting objects in a single image. Given query sketch, we detect objects (different poses for category-level OD, and same pose for fine-grained OD) from multiple photos in gallery.}
\includegraphics[width=0.8\linewidth]{latex/figures/category-OD-gallery.pdf}\vspace{-0.1cm}

\noindent \textbf{Q2: Additional experiment on part-level detection:} A quantitative evaluation of part-level object detection is infeasible due to the lack of annotations. Nevertheless, we measure Mean Opinion Score (MOS) by asking $10$ human workers to draw $20$ part-level sketches and rate on a scale of $1$ to $5$ (bad$\rightarrow$execellent) based on their opinion of how closely the expected part was detected. We compute MOS value by taking mean$\pm$variance of all $200$ responses as $3.67 \pm 0.6$. More details on the human study, additional out-of-vocab examples (Fig. in \texttt{[UztV]}Q1) will be included in the supplementary, and release the code upon acceptance.

\noindent \textbf{Q3: Clarification on detect objects in different pose:} Our object detection has multiple setups: (i) for category-level OD, the sketch of some object O1 (``zebra") in image I1 will detect the same object O1 in a different image I2 with a \emph{different pose} (``sitting" or ``standing"). (ii) For fine-grained OD, the sketch of some object O1 in image I1 will only detect the same object O1 in a different image I2 if it has the \emph{same pose}, e.g., detect only ``zebras sitting down" amongst a herd of ``zebras".  We add more qualitative results for clarification.
\includegraphics[width=\linewidth, height=0.2\linewidth]{latex/figures/category-vs-finegrained-OD.pdf}

\noindent \textbf{Q4: Additional Ablation:} (i) \texttt{Varying prompt length} $P=\{1, 3, 5\}$ in $\{\mathbf{v}_{\mathbf{s}}, \mathbf{v}_{\mathbf{p}} \} \in \mathbb{R}^{P \times 768}$ changes $AP_{.5}$ from $16.5$, $17.1$, and $15.9$ on [\green{23}] respectively. (ii) \texttt{replacing CLIP} with VGG-based sketch encoder $\mathcal{F}_{\mathbf{s}}$ sharply drops $AP_{.5}$ to $9.1$ (iii) \texttt{Increasing tiling} from $n=\{1, \dots, 7\}$ to $n=\{1, \dots, 17\}$ reduces $AP_{.5}$ to $11.3$ due to too much noisy occlusion with $n \rightarrow 17$. More details on ablation will be added upon acceptance.

\noindent \textbf{Miscellaneous:} (i) We will release the code for clarity on training details and tiling. (ii) See \texttt{[uNLK]} Q1 for more in-depth \emph{analysis on tiling}. (iii) We will rephrase Faster-RCNN as a ``go to choice" in multiple downstream tasks. (iv) \texttt{Yellow boxes} are NOT GT (see \texttt{[YH6C]} Q3). (v) Naive fine-tuning \texttt{collapses CLIP} ($AP_{.5}$ to $3.6$ on [\green{23}]) due to small data size ($<10k$) in sketches. (vi) We will cite DETR and rephrase our statement in L153.

\myparagraph{Response to Reviewer [\texttt{uNLK}]}\\
\noindent \textbf{Q1: Analysis on robustness of tiling:}
\blue{To evaluate robustness, we generate occluded photos and measure the drop in object detection ($AP_{.5})$ on [\green{23}]. A random patch covering $\{10\%, 30\%, 50\%\}$ of GT object boundaries is masked (zero pixel values). Performance of WSDDN (w/o tiling) drops by $\{3.1, 5.2, 7.5\}$, E-WSDDN (w/ tiling) $\{1.7, 3.4, 5.7\}$, and Proposed (w/ tiling) $\{1.6, 3.3, 5.4\}$ respectively. This shows the robustness due to tiling on object detection. More details on this analysis will be added upon acceptance.}

% \noindent \textbf{Q1: How tiling helps robustness:}
% To examine the robustness of tiling we synthetically occlude $10\%, 30\%$, and $50\%$ of object regions in photos with zeros (as pixel values). Comparing WSDDN (w/o tiling), E-WSDDN (w/ tiling) and proposed (w/ tiling), the $AP_{.5}$ in [\green{23}] drop by \{$3.1$, $1.7$, $1.6$\} for $10\%$, \{$5.2$, $3.4$, $3.3$\} for $30\%$, and \{$7.5$, $5.7$, $5.4$\} for $50\%$ occlusion respectively. We observe using tiling leads to greater robustness in occluded object detection.

\noindent \textbf{Q2: Summary of contributions:} 
\blue{(i) We train an object detector from only object-level sketch-photo pairs (each photo has only one object). This is achieved via an intuitive synergy between well-known modules (CLIP, SBIR) without reinventing from ground-up. (ii) CutMix is a data augmentation tool typically used on scene-level photo. It replaces a patch in Image I1 with that from I2. On the other hand, tiling is a data generation tool that combines multiple object-level photos in SBIR to create a scene photo. This generated scene photo enables training object detection.}

% \blue{First, we introduce prompt learning for generalised SBIR to give significant improvement ($>22\%$) in CC-FGSBIR (Tab.~\red{1}). This is important since our object detector is trained using SBIR. Second, despite the technical similarity, CutMix was used as a data augmentation tool whereas we use tiling as a bridge that trains scene-level representation in object detection using object-level SBIR data only}.

\myparagraph{References:} [A] Bendale \etal "Towards $\dots$ ", CVPR 2015
[B] Joseph \etal "Towards $\dots$ detection", CVPR 2021

%%%%%%%%% REFERENCES
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

\end{document}
