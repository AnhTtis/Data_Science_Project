% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}[theorem]
\usepackage{wrapfig}
\usepackage{bbm}
\usepackage{relsize}
\usepackage{empheq}
\newcommand{\cut}[1]{}
\usepackage[accsupp]{axessibility}
\usepackage{listings}
% \usepackage{algorithm}
\usepackage{algorithm2e}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{arydshln}
\usepackage{booktabs}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\maximize}{maximize}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand{\blue}[1]{\textcolor{blue}{{#1}}}
\newcommand{\red}[1]{\textcolor{red}{{#1}}}
\newcommand{\green}[1]{\textcolor{green}{{#1}}}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{5825} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


% \definecolor{codegreen}{rgb}{0,0.6,0}
% \definecolor{codegray}{rgb}{0.5,0.5,0.5}
% \definecolor{codepurple}{rgb}{0.58,0,0.82}
% \definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% \lstdefinestyle{mystyle}{
%     backgroundcolor=\color{backcolour},   
%     commentstyle=\color{codegreen},
%     keywordstyle=\color{magenta},
%     numberstyle=\tiny\color{codegray},
%     stringstyle=\color{codepurple},
%     basicstyle=\ttfamily\footnotesize,
%     breakatwhitespace=false,         
%     breaklines=true,                 
%     captionpos=b,                    
%     keepspaces=true,                 
%     numbers=left,                    
%     numbersep=5pt,                  
%     showspaces=false,                
%     showstringspaces=false,
%     showtabs=false,                  
%     tabsize=2
% }

\usepackage{minted}
\newminted{python}{%
    % options to customize output of pythoncode
    % see section 5.3 Available options starting at page 16
}




\begin{document}
\RestyleAlgo{ruled}
%%%%%%%%% TITLE - PLEASE UPDATE
\title{What Can Human Sketches Do for Object Detection? \\ Supplemental material\vspace{-0.5cm}}

\lstset{
    basicstyle=\ttfamily,
    % language=python,
    keywordstyle=\color{blue},
    stringstyle=\color{DarkMagenta},
    commentstyle=\color{DarkGreen},
    morecomment=[l]{\%}
}

\author{Pinaki Nath Chowdhury \hspace{.2cm}  Ayan Kumar Bhunia \hspace{.2cm} Aneeshan Sain \hspace{.2cm}  Subhadeep Koley \\
Tao Xiang\hspace{.3cm}  Yi-Zhe Song \\
SketchX, CVSSP, University of Surrey, United Kingdom.  \\
{\tt\small \{p.chowdhury, a.bhunia, a.sain, s.koley, t.xiang, y.song\}@surrey.ac.uk} 
}
\maketitle

% \includegraphics[width=2\linewidth]{latex/figures/detection-supp.pdf}


% \counterwithin{figure}{section}
\renewcommand{\thefigure}{\Alph{figure}}
\renewcommand{\thesubsection}{\Alph{subsection}}
% \setcounter{figure}{8}
% \setcounter{table}{7}
% \setcounter{equation}{0}

\definecolor{commentcolor}{RGB}{110,154,155}   % define comment color
\newcommand{\PyComment}[1]{\ttfamily\textcolor{commentcolor}{\# #1}}  % add a "#" before the input text "#1"
\newcommand{\PyCode}[1]{\ttfamily\textcolor{black}{#1}} % \ttfamily is the code font


\subsection{Human Study on Part-level Object Detection}
Due to the lack of annotation, a quantitative evaluation of part-level object detection is infeasible. Nonetheless, we measure the real-world usability of our sketch-enabled object detection framework using Mean Opinion Score (MOS) by asking $10$ people to draw $20$ part-level sketches and rate from $1$ to $5$ (bad $\rightarrow$ excellent) based on their opinion of how closely the queried object part was detected. Accordingly, we obtain a MOS (mean $\pm$ variance of $200$ responses) of $3.67 \pm 0.6$. 

\subsection{Preliminary Study on Occluded Objects}
In addition to category-level, fine-grained, and part-level object detection, we further qualitatively test the generalisability of the system to detect occluded objects as:
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{latex/figures/occlusion.pdf}
    % \caption{Caption}
    \label{fig:my_label}
    \vspace{-0.5cm}
\end{figure}

While we show some successful, failed, and partially detected cases, future works can further investigate the role of sketch and foundation models like CLIP \cite{CLIP} for occluded object detection.

\subsection{Relation to Open World setup}
In open world setup, a model trained on $C$ known classes can recognise the unknown class and update the base model via incremental learning \cite{bendale2015openWorld, joseph2021OW}. Our method already works in open world setup as it detects in zero-shot, open-vocab setup, i.e., it works regardless of whether the query sketch is in the train set or not.

\subsection{Detection across Different Poses}
Our object detection has multiple setups: (i) for category-level OD, the sketch of object O1 (``zebra") in image I1 will detect the same object O1 in a different image I2 \emph{even with a different pose} (``sitting" or ``standing"). (ii) For fine-grained OD, the sketch of object O1 in image I1 will only detect the same object O1 in a different image I2 if it has the \emph{same pose}, e.g., detect only ``zebras \textit{sitting down}" amongst a herd of ``zebras".  Figure below shows qualitative results for clarity.
\includegraphics[width=\linewidth, height=0.2\linewidth]{latex/figures/category-vs-finegrained-OD.pdf}

\subsection{Additional Ablation Study}
(i) \textit{Varying prompt length} $P=\{1, 3, 5\}$ in $\{\mathbf{v}_{\mathbf{s}}, \mathbf{v}_{\mathbf{p}} \} \in \mathbb{R}^{P \times 768}$ changes $AP_{.5}$ to $16.5$, $17.1$, and $15.9$ on SketchyCOCO~\cite{sketchycoco2020} respectively. (ii) \textit{Replacing CLIP} with VGG-based sketch encoder $\mathcal{F}_{\mathbf{s}}$ sharply drops $AP_{.5}$ to $9.1$ (iii) \textit{Increasing tiling} from $n\in[1,7]$ to $n\in[1,17]$ reduces $AP_{.5}$ to $11.3$ due to high occlusion ($n\rightarrow17$).

\subsection{Robustness to Tiling}
To test robustness, we generate occluded photos by randomly masking $(10\%, 30\%, 50\%)$ of GT object boundaries with zero pixel values and measure the respective drop in accuracy ($AP_{.5})$ on \cite{sketchycoco2020}. Performance drop being less \textit{with tiling} for E-WSDDN (by $\{1.7, 3.4, 5.7\}$) or our method (by $\{1.6, 3.3, 5.4\}$), than \textit{without tiling} in WSDDN (by $\{3.1, 5.2, 7.5\}$) verifies robustness due to tiling on object detection.

\subsection{Clarification on CutMix \cite{cutmix}  vs. our Tiling}
(i) Our novelty lies in adapting well-known modules (CLIP, SBIR) to train an object detector from only object-level sketch-photo pairs (each photo has only one object) without any bounding-box annotations. (ii) Despite sharing a common technical implementation, CutMix \cite{cutmix} is a data \textit{augmentation} tool that typically replaces a patch in one \textit{existing} scene-photo with that from another. Contrarily, tiling is a data \textit{synthesis} tool that combines multiple object-level photos in the SBIR dataset to \textit{newly create} a scene photo for subsequent training.


\begin{figure*}[]
    \centering
    \includegraphics[width=\linewidth]{latex/figures/detection-supp.pdf}
    \caption{Additional qualitative results for fine-grained and part-level object detection on SketchyCOCO. Note both the \textcolor{blue}{Blue} and \textcolor{yellow}{Yellow} boxes are network predictions and not ground truth. The \textcolor{blue}{Blue} boxes are predictions from the network prior to using Non-maximum suppression (NMS) with the confidence score of the predicted box $\omega_{k} \geq 0.7$. The \textcolor{yellow}{Yellow} boxes are the resulting predictions after applying NMS with $\mathrm{IoU} \geq 0.3$}
    \label{fig:my_label}
\end{figure*}


% \cleardoublepage
%%%%%%%%% REFERENCES

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\end{document}
