% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{color, colortbl}
\usepackage{xcolor}
% \usepackage{bbold}
\usepackage{multirow}
\usepackage{makecell}
\newcommand{\keypoint}[1]{\vspace{0.01cm}\noindent\textbf{#1}\quad}
\newcommand{\cut}[1]{}
\definecolor{Gray}{gray}{0.9}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Can Sketch Based Image Retrieval Train Object Detection?}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   Our contributions are two-fold: (i) First, we propose the first generalised fine-grained sketch-based image retrieval network leveraging the zero-shot potential of CLIP via prompt learning. (ii) Second, using the generalised FG-SBIR network, we train an object detector without using scene images or bounding box annotations. We exclusively train on existing object-level sketch/photo pairs and show zero-shot potential on standard object detection datasets like MS-COCO \cite{mscoco} and PASCAL-VOC \cite{pascalVOC}. Training object detection network from a generalise CLIP-based FG-SBIR network injects zero-shot potential into our detection framework, thereby helping practical applications. * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth, height=0.5\linewidth]{example-image-a}
    \caption{Explain the Problem Setup we aim to solve}
    \label{fig:problem-setup}
\end{figure}

* \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\

\section{Related Works}
\label{sec:related-works}

\noindent \textbf{Sketch for Visual Understanding:}
Hand-drawn sketches serve as a useful query modality for visual understanding tasks that involve human perception and structural cues. Sketches not only convey a visual description \cite{hertzmann2020perception} but also exhibit artistic styles \cite{zhang2021smartshadow}. This makes sketch a vital querying modality for the creative industry like artistic image editing \cite{yang2020surgery} and animation \cite{xing2015autocomplete}. Unlike photos that are passively captured by a camera, sketches are actively drawn by humans which makes them a good visual representation \cite{pixelor} enriched with human participation. Apart from the widely explored sketch-based image retrieval \cite{bhunia2021semi, bhunia2020sketch, pang2019generalising, styleMeUp, sketchformer, liveSketch}, sketch as a query has shown potential in several vision understanding tasks like incremental learning \cite{bhunia2022incremental} image and video synthesis \cite{cusuh2022synthesis, DeepFaceVideoEditing2022}, representation learning \cite{alaniz2022primitives, clipasso}, image-inpainting \cite{xie2021inpainting}, 3D shape retrieval \cite{xu20223Dretrieval}, 3D shape modeling \cite{chowdhury20223Dsynthesis}, medical image analysis \cite{wang2022medical}, object localisation \cite{tripathi2020object, riba2021object} and segmentation \cite{hu2020segmentation, qi2022segmentation}.

While exploring sketch as a query for object detection, by Tripathi \etal \cite{tripathi2020object}, several limitations surfaced with respect to problem definition as well as architectural designs. 
Firstly, instead of fine-grained matching, sketch was used to specify object category (easier via text/keyword \cite{CLIP, gu2022open-vocab-OD}), thus \cut{wasting} missing the potential of sketch as a query in modeling fine-grained details.
% Primarily, potential of sketch as a query in modeling fine-grained details remains unused, as specifying object category via sketching is quite laborious \cite{bhunia2020sketch}, compared to a text/keyword \cite{CLIP, gu2022open-vocab-OD}.
Secondly, it requires both bounding-box and object-sketch annotation which increases annotation budget without significant improvement in performance over traditional object detection setup. 
% \textcolor{red}{Secondly, as specifying object category by sketching is laborious \cite{bhunia2020sketch}, compared to a text/keyword \cite{CLIP, gu2022open-vocab-OD}, the potential of sketch in modeling fine-grained details remains unused.}
% thus overlooking the potential of sketch in modeling fine-grained details -- a \emph{fine-grained query}. 
% sketch as a query prides in its ability to model fine-grained details \cite{styleMeUp, bhunia2022worrying, bhunia2020sketch}. 
Thirdly, due to an expensive bounding box and sketch annotation, only fewer than $50\%$ object categories in existing object detection datasets \cite{pascalVOC, mscoco} are available for training. Finally, using an early fusion strategy \cite{xu2022MML} of sketch with object detection results in recomputing object regions for each new sketch -- leading to a slower detection framework with increasing query sketches.
% \cite{tripathi2020object} employs an early fusion strategy, that uses query-sketch as a conditional input to predict object regions, per sketch-photo pair -- resulting in a slower paradigm as sketch categories increase.
In this paper, we propose a fine-grained sketch-based object detection framework that use only object-level sketch photo pairs without any bounding-box annotations for training, and is scalable with multiple fine-grained query sketches, even under zero-shot setup.

\noindent \textbf{Supervised Object Detection:} 
Object detection jointly localise and identify objects in an image. Traditional object detectors rely on large supervised object detection datasets such as PASCAL VOC \cite{pascalVOC} and MS-COCO \cite{mscoco}, containing thousands of examples per object category which are quite time-consuming to annotate, unlike our pipeline, which leverages sketch-photo pairs as object annotations. 
% have multitudes\cut{over hundreds and thousand} of time-consuming annotated examples per object category. 
Existing literature on object detection is bifurcated as: (i) fast yet less accurate single-shot \cite{yolo, yolov3, SSD, lin2017focal-loss, centerNet}; (ii) slow but more accurate two-stage object detectors \cite{rcnn, fast-rcnn, faster-rcnn, maskrcnn}. To fully exploit the fine-grained cues provided by sketch, our proposed method aligns with the two-stage detectors. Two-stage detectors predict object regions using selective search in RCNN \cite{rcnn}, ROI pooling \cite{fast-rcnn} in Fast-RCNN, and Region Proposal Network (RPN) with ROIAlign in Faster-RCNN \cite{faster-rcnn}. While there has been several attempts with sophisticated architectural modifications   \cite{law2018cornerNet, zhou2019grouping, centerNet}, Faster-RCNN \cite{faster-rcnn} still acts a a fundamental building block for several downstream tasks like scene-graph generation \cite{yang2018scenegraph}, visual grounding \cite{mouzenidis2021grounding}, and relationship prediction \cite{zhu2018relationship}. Therefore, we resort to the more traditional Faster-RCNN based two-stage pipeline.


\noindent \textbf{Weakly Supervised Object Detection (WSOD):} Collecting bounding box annotation per object category is already a time-consuming process, which is aggravated even further by Fine-grained object detection (e.g., recognising animal species). Aiming to overcome this, existing WSOD methods adopt two schools of thoughts: (a) formulate this task as a multiple instance learning (MIL) \cite{dietterich1997MIL, wsddn, li2016wsod, diba2017wsod, jie2017wsod, zhang2018wsod, tang2018wsod, shen2019wsod} problem that interpret an image as a bag of proposals or regions. The image is labeled positive, when one of the regions tightly contains the object of interest, otherwise negative if no region has it.
% whereas a negative would imply no region has it.
% {If the image is labeled negative, no region contains the object.} 
% A benefit of MIL-based approaches is its ability to detect multiple objects from different categories \cite{}.
% An image is given to a proposal generator (selective search \cite{selectivesearch}, edge boxes \cite{edgeboxes}, sliding window \cite{zhang2019freeanchor}) and backbone network \cite{alexnet, vgg, inception, he2015resnet} to get proposals and feature maps respectively. Next, feature maps and proposals are given to spatial pramid pooling (SPP) \cite{SPPNet} to generate fixed-size regions. Finally, these regions are fed to the detection head to classify and localise object instances. 
(b) resort to CAM-based methods \cite{zhou2016CAM, zhang2018cam} that use class activation maps to predict proposals. Specifically, an image is fed to a backbone network to generate feature vector from which bounding box of each class is predicted by thresholding activation maps with highest probability.
% This feature vector is given to a classifier to generate prediction scores of an image. Finally, CAM generates proposals or bounding box of each class by thresholding to segment activation maps with the highest probability of every class. 
Although CAM-based methods are faster, in this paper we use MIL-based technique as it can detect \textit{multiple instances} within the \textit{same} category \cite{wsodsurvey2022}.



% Collecting bounding box annotation per object category is time-consuming. Fine-grained object detection (e.g., recognising animal species) exacerbates this problem. Existing approaches formulate this task as multiple instance learning (MIL) \cite{dietterich1997MIL, wsddn, li2016wsod, diba2017wsod, jie2017wsod, zhang2018wsod, tang2018wsod, shen2019wsod} problem that interpret an image as bag of proposals or regions. If the image is labeled positive, then one of the regions tightly contain the object of interest. If the image is labeled negative, no region contains the object. We feed an image to a proposal generator (selective search \cite{selectivesearch}, edge boxes \cite{edgeboxes}, sliding window \cite{zhang2019freeanchor}) and backbone network \cite{alexnet, vgg, inception, he2015resnet} to get proposals and feature maps respectively. Next, feature maps and proposals are given to spatial pramid pooling (SPP) \cite{SPPNet} to generate fixed-size regions. Finally, these regions are fed to the detection head to classify and localise object instances. Instead of generating thousands of proposals as in MIL, an CAM-based methods \cite{zhou2016CAM, zhang2018cam} use class activation maps to predict proposals. Specifically, an image is fed to a backbone network to generate feature vector. This feature vector is given to a classifier to generate prediction scores of an image. Finally, CAM generates proposals or bounding box of each class by thresholding to segment activation maps with the highest probability of every class. In spite of CAM-based methods being faster, in this paper, we use MIL-based technique due to its ability in detecting multiple instances with the same category.

\noindent \textbf{Data Augmentation in Computer Vision:} Data augmentation improves the sufficiency and diversity of training data. Approaches vary from simple image rotation and flipping to more advanced techniques of image erasing \cite{gridmask} like CutOut \cite{devries2017cutout}, Hide-and-Seek \cite{hide-and-seek} and image mixing like MixUp \cite{mixup} and {CutMix} \cite{cutmix}. 
Aiming to generalise sketch-based object detection to complex scenes while training exclusively on existing object-level sketch photo pairs \cite{sketchy}, we thus employ a CutMix \cite{cutmix} like data augmentation trick -- a method that replaces removed sub-regions with a patch from another image to synthesise new images.

% In our context cutmix is quite relevant
% Out of these cutmix seems the most relevant to our cause
% CutMix \cite{cutmix} replaces removed sub-regions with a patch from another image to synthesise new images. In this paper, we employ a CutMix \cite{cutmix} like data augmentation trick, aiming to generalise sketch-based object detection to complex scenes while training exclusively on existing object-level sketch photo pairs \cite{sketchy}.


% In this paper, we employ data augmentation techniques like CutMix \cite{cutmix} that replaces removed sub-region with a patch from another image to synthesise new images. In particular, we replace the sub-regions of a scene with photos from different sketched categories. This helps learn object detection in complex scenes with multiple object instances by training from object-level sketch photo pairs.

% Data augmentation improves the sufficiency and diversity of training data. Approaches varies from simple image transformations like rotation, flipping, and cropping to the more advanced image erasing \cite{devries2017cutout, hide-and-seek, gridmask} and image mix \cite{mixup, cutmix}. Image erasing replace pixel values of one or more sub-regions in the image with constant (Cutout\cite{devries2017cutout}, Hide-and-Seek \cite{hide-and-seek}, GridMask \cite{gridmask}, FenceMask \cite{fencemask}) or random values (random erasing \cite{zheng2020random}). Image mix data combines two or more images or sub-regions into one. MixUp \cite{mixup} learns a linear relationship between mixing images from training set and their supervision signal. CutMix \cite{cutmix} replaces removed regions with a patch from another image to synthesise new images. Our proposed method is similar to CutMix \cite{cutmix} with sub-regions depicting photo of different sketched category from Sketchy dataset \cite{sketchy}. This helps the network learn complex scenes with multiple categories.

\noindent \textbf{Sketch-Based Object Representation}
Sketch with its intrinsic ability to model fine-grained visual details, makes it an ideal modality  for object retrieval, giving rise to avenues like \emph{category-level} \cite{liveSketch, yelamarthi2018sketch, doodle-to-search, sketchformer, sketchmate} and fine-grained (FG) \emph{instance-level} \cite{styleMeUp, bhunia2022worrying, bhunia2020sketch, bhunia2021semi} sketch-based image retrieval (SBIR). 
% The ability of sketch to offer inherently fine-grained visual description makes it an ideal modality for object retrieval. 
% Typically SBIR employs CNN \cite{liveSketch, doodle-to-search}, RNN \cite{sketchmate}, Transformer \cite{sketchformer}, or Vision Transformer \cite{fscoco} based deep triplet-ranking to learn joint embedding space \cite{yu2016shoe}. 
Contemporary research also explored zero-shot SBIR \cite{doodle-to-search, yelamarthi2018sketch, sain2022sketch3t}, cross-domain translation \cite{kaiyue2017cross} and approaches like reinforcement learning based on-the-fly retrieval \cite{bhunia2020sketch}, self-supervised \cite{pang2020jigsaw, vector-raster}, etc. Apart from object-level images, retrieving sketched objects from scene images was studied using graph convolutional networks \cite{liu2020scenesketcher} and optimal transport \cite{partially-does-it}. Similar to cross-category FG-SBIR \cite{kaiyue2017cross, bhunia2022adaptive}, here we explore fine-grained sketch photo association for object detection by adapting large vision-language models like CLIP \cite{CLIP} using prompt engineering \cite{zhou2022visualprompt}.

% to learn a fine-grained association using object-level sketch for fine-grained sketch-based object detection from scene images. 

% Concretely, we adapt large vision-language models like CLIP \cite{CLIP} using prompt engineering \cite{zhou2022visualprompt} to learn a cross-category FG-SBIR. Next, we distill knowledge from CLIP image-encoders for supervision.






% The ability of sketch to offer inherently fine-grained visual description makes it an ideal modality for object retrieval. This commenced avenues like \emph{category-level} \cite{liveSketch, yelamarthi2018sketch, doodle-to-search, sketchformer, sketchmate} and fine-grained (FG) \emph{instance-level} \cite{styleMeUp, bhunia2022worrying, bhunia2020sketch, bhunia2021semi} sketch-based image retrieval (SBIR). Typically SBIR employs CNN \cite{liveSketch, doodle-to-search}, RNN \cite{sketchmate}, Transformer \cite{sketchformer}, or Vision Transformer \cite{fscoco} based deep triplet-ranking based siamese networks to learn joint embedding space \cite{yu2016shoe}. Contemporary research is also directed towards zero-shot SBIR \cite{doodle-to-search, yelamarthi2018sketch, sain2022sketch3t}, binary hash-code embedding \cite{liu2017hashing, shen2018hashing}, attention mechanism \cite{deep-spatial-semantic}, cross-domain translation \cite{kaiyue2017cross}, reinforcement learning based on-the-fly retrieval \cite{bhunia2020sketch}, semi-supervised \cite{bhunia2021semi}, self-supervised \cite{pang2020jigsaw, vector-raster}, meta-learning \cite{bhunia2022adaptive}, style-agnostic retrieval \cite{styleMeUp}, etc. Apart from object-level images, retrieving sketched objects from scene images was studied using graph convolutional networks \cite{liu2020scenesketcher} and optimal transport \cite{partially-does-it}. In this paper, we show the potential of cross-category FG-SBIR \cite{kaiyue2017cross, bhunia2022adaptive} to solve object detection, a fundamental and challenging task in computer vision for a quarter century. Concretely, we adapt large vision-language models like CLIP using prompt engineering \cite{zhou2022visualprompt} to learn a cross-category FG-SBIR. Next, we formulate object detection as MIL problem and distill knowledge from CLIP image-encoders for supervision.

\section{Proposed Method}
\label{sec:proposed-method}
\keypoint{Overview} We propose a new paradigm training object detection without bounding box annotation or image-level class labels. Instead, we train an object detector by distilling knowledge from a sketch-based image retrieval. This leads to several emergent behaviors (i) fine-grained object detection -- specify a particular region-of-interest using fine-grained visual cues in sketch. (ii) category-level object detection -- specify region-of-interest of the same sketch class. (iii) part-level object detection -- instead of detecting the entire object (e.g., a ``horse"), we only detect specified parts (e.g., ``head" and ``legs" of a ``horse").
% (iv) Unlike supervised setups (SOD) that use bounding box annotation or weakly supervised setups (WSOD) that use image-level labels, we introduce extreme weakly supervised setup (EWSOD) that avoids using expensive bounding box annotation or image-level labels of downstream data distribution by exploiting zero-shot potential of CLIP and a conjugate sketch-based image retrieval task.

% \cut{We propose a new paradigm for training object detectors without using bounding box annotation or test data distribution. In particular, }
% We propose an object detection framework that can localise queried objects and has fine-grained alignment with input sketch. Firstly, we train this network without using both bounding-box annotation and hand-drawn sketch. Secondly, we begin with a pre-trained Region Proposal Network (RPN) \cite{faster-rcnn} and fine-tune the backbone feature extractor using a novel object-level cross-category FG-SBIR. Finally, we bridge the object-level and scene-level gap using a custom designed data augmentation scheme.
% We learn an object detection framework which can localise novel objects in a scene that has fine-grained alignment with a given input sketch. Importantly, we train this network without the use of bounding box annotations. To this end, we employ knowledge distillation from a pre-trained cross-category FG-SBIR teacher model to an object detection student model. In addition, to bridge the gap between FG-SBIR models trained using object-level sketch-photo pairs to sketch-guided object detection in scene images, we choose a simple yet effective data augmentation scheme following existing literature on Cut-Mix.

\subsection{Baseline Supervised Object Detection}
\vspace{-0.5cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{latex/figures/fasterRCNN.pdf}
    \vspace{-0.6cm}
    \caption{Schematic of baseline Faster-RCNN \cite{faster-rcnn}. }
    \label{fig: faster-rcnn}
    \vspace{-0.3cm}
\end{figure}
\noindent First, we briefly summarise Faster-RCNN \cite{faster-rcnn}, a baseline object detection (OD) framework that remains state-of-the-art for several downstream tasks \cite{yang2018scenegraph, mouzenidis2021grounding, zhu2018relationship}. Given a photo $\mathbf{p} \in \mathbb{R}^{H \times W \times 3}$, a VGG-16 \cite{vgg} network $\mathcal{F}(\cdot)$ predicts a feature map $f_{\mathbf{p}} \in \mathbb{R}^{H' \times W' \times 512}$. Next, a region proposal network \cite{faster-rcnn} $\mathcal{R}(\cdot)$ generates $R$ rectangular proposals $\mathbf{r} = \{ r_1, r_2, \dots, r_R \}$ with feature $r_j \in \mathbb{R}^{512}$ from the convolutional feature map $f_{\mathbf{p}}$ by sliding a small ($3 \times 3$) spatial window (with effective receptive field $228$ pixels). At each sliding-window location, $k=9$ proposals (anchor boxes) are generated, associated with $3$ scales and $3$ aspect ratio. In the first phase, the proposals $\mathbf{r}$ is fed into two sibling fully-connected layers: (i) a box-regression layer that has $4k$ outputs representing the coordinates of $k$ boxes, and (ii) a box-classification layer that outputs $2k$ scores to estimate the probability of being an object or not (``objectness" measure). In the second phase, for each object proposal $r_j \in \mathbf{r}$, a region of interest (RoI) pooling layer $\mathcal{P}(\cdot)$ extracts a fixed-length feature vector of size ($7 \times 7 \times 512$) from the convolutional feature map $f_{\mathbf{p}}$ which is fed into a sequence of fully connected (FC) layers to get $f_{\mathbf{r}_{j}} = \mathcal{P}(f_{\mathbf{p}}, {r}_{j})$, where $f_{\mathbf{r}_{j}} \in \mathbb{R}^{512}$ branches into two stream output layer: (i) $\phi_{det}: \mathbb{R}^{512} \rightarrow \mathbb{R}^{N+1}$ that produces softmax probability estimates over $N$ object class plus a catch-all ``background" class for detection, and (ii) $\phi_{reg}: \mathbb{R}^{512} \rightarrow \mathbb{R}^{4}$ that regress the refined bounding-box coordinates for the detected region. \cref{fig: faster-rcnn} presents a schematic of the data flow in our baseline Faster-RCNN \cite{faster-rcnn}. The region proposal network $\mathcal{R}$, and the RoI pool based classification and bounding box regression $\mathcal{P}$ is trained alternatively \cite{faster-rcnn}. During training, $\mathcal{R}$ and $\mathcal{P}$ use $2$- or $N$-class cross-entropy and smooth L1 \cite{fast-rcnn} loss for bounding box regression.

While seminal, baseline Faster-RCNN \cite{faster-rcnn} has some key limitation: (i) it requires expensive bounding box annotation equally distributed across all $N$-classes. However, due to dataset bias \cite{torralba2011bias} -- Zipf's law \cite{zipf-law} and social conventions \cite{hendricks2018} -- collecting sufficient bounding box annotation for less occurring classes is near impossible. We show how weakly supervised object detection paradigm \cite{OICR} resolves this dependency on requiring bounding box annotation.
% (ii) Once trained on $N$-classes, the model fails to detect $(N+1)^{th}$-class, thus limiting real-world open-set applications. (iii) It lacks the flexibility of detecting only user-specified objects of interest (e.g., detecting only `swans' standing on one leg). Therefore, this demands a further investigation on how to adapt our baseline \cite{faster-rcnn} to overcome the aforementioned limitations.

\subsection{Weakly Supervised Object Detection via MIL}\label{sec: WSOD}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{latex/figures/wsddn.pdf}
    \caption{Adapting Faster-RCNN to weakly supervised setup using multiple instance learning paradigm. In particular, we remove the bounding-box regression head $\phi_{reg}$ and only classify region features $f_{\mathbf{r}}$ into an $N$-class classification.}
    \label{fig:wsddn}
\end{figure}
% jehetu bounding box annotation nei -- ekta image e more than one object thaakte paare -- every image-level label against e amader ekta score chai jeta signify kore oei class er object ta ache ki na
% first stream and second stream -- alada kore na bole -- ek saathe bole debo
\noindent Unlike supervised object detection that requires expensive bounding box annotation, weakly supervised object detection (WSOD) trains using image-level labels -- i.e., objects of a class is present in the image or not. Due to lack of bounding box annotation, WSOD use a pre-trained region proposal network \cite{faster-rcnn} ($\mathcal{P}$), selective search \cite{selective-search}, or edge boxes \cite{edgeboxes} to generate object proposals $\mathbf{r} = \{r_1, r_2, \dots, r_R\}$


\noindent We adapt Faster-RCNN \cite{faster-rcnn} for downstream data distribution without collecting bounding box annotations. First, we pretrain a generalisable \cite{gu2022open-vocab-OD} region proposal network $\mathcal{R}$ using bounding box annotations from Visual Genome \cite{visualgenome}\footnote{Alternatively, one can use region proposal algorithms like Selective Search \cite{selectivesearch} or EdgeBoxes \cite{edgeboxes}, but it leads to lower performance.}. Next, we freeze $\mathcal{R}$ \cite{openVocab-semanticCLIP} and fine-tune the remaining modules in \cref{fig:wsddn} on downstream dataset, weakly supervised by reformulating object detection as a multiple instance learning (MIL) problem that detects positive regions from a bag of proposals. The proposal features $f_{\mathbf{r}} = \mathcal{P}(f_{\mathbf{p}}, \mathbf{r})$, where $f_{\mathbf{r}} \in \mathbb{R}^{R \times 512}$ from proposal $\mathbf{r}$ is branched into two streams $\mathbf{x}_{c} = \phi_{cls}(f_{\mathbf{r}})$, $\mathbf{x}_{d} = \phi_{det}(f_{\mathbf{r}})$, where $\mathbf{x}_{c} \in \mathbb{R}^{R \times N}$, $\mathbf{x}_{d} \in \mathbb{R}^{R \times N}$ respectively. Following \cite{OICR}, the first data stream performs classification of individual proposals $r_j \in \mathbf{r}$ into class scores as: $\sigma_{cls}(\mathbf{x}_{c}^{ij}) = \exp ( \mathbf{x}_{c}^{ij} ) / \sum_{k=1}^{N} \exp ( \mathbf{x}_{c}^{ik} )$. The second data stream scores proposals relative to one another as: $\sigma_{det}(\mathbf{x}_{d}^{ij}) = \exp ( \mathbf{x}_{d}^{ij} ) / \sum_{k=1}^{N} \exp ( \mathbf{x}_{d}^{kj} )$. The combined score is obtained by element-wise product $\mathcal{S}^{0} = \sigma_{cls}(\mathbf{x}_{c}) \odot \sigma_{det}(\mathbf{x}_{d})$, where $\mathcal{S}^{0} \in \mathbb{R}^{R \times N}$. Finally, we predict the image-level class scores ${y}_{c} = \sum_{j=1}^{R} \mathcal{S}^{0}_{jc}$ and train using multi-class cross entropy as:
\begin{equation}\label{eq:MIL}
    \mathcal{L}_{mil} = - \sum_{c=1}^{N} \{\mathbf{\hat{y}}_{c} \log {y}_{c} + (1 - \mathbf{\hat{y}}_{c}) \log (1 - {y}_{c})
\end{equation}
where, $\mathbf{\hat{y}} = \{\hat{y}_1, \hat{y}_2, \dots, \hat{y}_N \}$ are ground-truth image-level labels in downstream datasets. \cref{eq:MIL} ensures that our images may contain more than one object class but each proposals should contain a single class. 

Note, unlike Faster-RCNN (\cref{fig: faster-rcnn}), our weakly supervised setup lacks $\phi_{reg}$ that refines bounding box predictions. Hence, to predict accurate boxes, we iteratively refine \cite{iterativeText} our initial prediction $\mathcal{S}^{0}$ using $\phi_{cls}^{*}(\cdot)$ as $\mathcal{S}^{k} = \phi_{cls}^{*}(\mathcal{P}, \mathbf{r})$. The weights of $\phi_{cls}^{*}$ is iteratively updated by backpropagating instance classifier refinement loss $\mathcal{L}_{icr}$ between scores at $k^{th}$ step ($\mathcal{S}^{k}$) and a pseudo ground-truth $\mathbf{\hat{y}}^{(k)}$ from $(k-1)^{th}$ step. In particular, we assign $\mathbf{\hat{y}}^{(k)}_{j'c} = 1$ for class label $c$ in image if $j' = \arg \max_{j} \mathcal{S}^{(k-1)}_{jc}$. Proposals $r_j \in \mathbf{r}$ that has high overlap (${IoU} > 0.5$) with $r_{j'c}$ are assigned the same class label $\mathbf{\hat{y}}^{(k)}_{jc} = 1$, otherwise we label $r_j$ background class $\mathbf{\hat{y}}^{(k)}_{j(N+1)} = 1$. If an image does not have class label $c$, then for all regions $r_j \in \mathbf{r}$ are assigned $\mathbf{\hat{y}}^{(k)}_{jc} = 0$. Finally, we compute a weighted refinement loss:
\begin{equation}\label{eq:icr}
    \mathcal{L}^{k}_{icr} = - \frac{1}{R} \sum_{j=1}^{R} \sum_{c=1}^{N} \mathcal{S}^{(k-1)}_{j'c} \mathbf{\hat{y}}^{(k)}_{jc} \log \mathcal{S}^{(k-1)}_{jc}
\end{equation}
Hence, the total loss for weakly supervised object detection using MIL comprise of $\mathcal{L}_{tot} = \mathcal{L}_{mil} + \sum_{k=1}^{K} \mathcal{L}_{icr}$. While weakly supervised object detection trains using only image-level labels, it highlights two key limitations: (i) Once trained on $N$-classes, the model fails to detect $(N+1)^{th}$-class, thus limiting real-world open-set applications. (ii) It lacks the flexibility of detecting only user-specified objects of interest (e.g., detecting only `swans' standing on one leg). In the following section, we resolve these limitations by replacing image-level labels with query sketches.

\subsection{Localising Object Regions with Query Sketch}
\noindent In this section, we extend object detection from predefined $N$-classes to the open-set setup \cite{openVocab-semanticCLIP} by replacing proposal classifiers $\{ \phi_{cls}, \phi_{det}, \phi_{cls}^{*} \}$ mapping $\mathbb{R}^{R \times 512} \rightarrow \mathbb{R}^{R \times N}$ to proposal embeddings $\mathbb{R}^{R \times 512} \rightarrow \mathbb{R}^{R \times D}$ learning a $d$-dimensional feature vector as: $\mathbf{x}_{c} = \phi_{cls}(f_{\mathbf{p}}, \mathbf{r})$, $\mathbf{x}_{d} = \phi_{det}(f_{\mathbf{p}}, \mathbf{r})$, where $\mathbf{x}_{c} \in \mathbb{R}^{R \times D}$, $\mathbf{x}_{d} \in \mathbb{R}^{R \times D}$. Next, we use a pretrained (see \cref{sec: prompt-sketch}) sketch encoder $\mathcal{F}_{\mathbf{s}}$ that maps $N$ rasterised input sketches $\{ \mathbf{s}_{1}, \mathbf{s}_{2}, \dots, \mathbf{s}_{N} \}$, where $\mathbf{s}_{i} \in \mathbb{R}^{H \times W \times 3}$ to a sketch embedding $f_{\mathbf{s}} = \{ f_{\mathbf{s}}^{1}, f_{\mathbf{s}}^{2}, \dots, f_{\mathbf{s}}^{N} \}$, where $f_{\mathbf{s}} \in \mathbb{R}^{R \times D}$. Given the proposal and sketch embeddings, we compute the proposal scores ($\mathcal{S}^{0}$) as: $\mathcal{S}^{0} = \sigma_{cls}(\mathbf{x}_{c} \cdot [ f_{\mathbf{s}}^{1}, \dots, f_{\mathbf{s}}^{N} ]^{T}) \odot \sigma_{det}(\mathbf{x}_{d} \cdot [ f_{\mathbf{s}}^{1}, \dots, f_{\mathbf{s}}^{N} ]^{T})$. To compute the refined score $\mathcal{S}^{k}$ from the iterative update module $\mathcal{S}^{k}_{*} = \phi_{det}^{*}(f_{\mathbf{r}})$, where $\mathcal{S}^{k}_{*} \in \mathbb{R}^{R \times (N+1)}$ we additionally learn a ``background" embedding $bg \in \mathbb{R}^{D}$ to compute refined score as: $\mathcal{S}^{k} = \sigma_{cls}(\mathcal{S}^{k}_{*} \cdot [ f_{\mathbf{s}}^{1}, \dots, f_{\mathbf{s}}^{N}, bg ]^{T})$. Finally we train to detect sketched regions as \cref{sec: WSOD} using \cref{eq:MIL} and \cref{eq:icr}. While sketches provide flexibility to end-users and specify fine-grained regions of interest, the performance heavily depends on the sketch encoder ($\mathcal{F}_{\mathbf{s}}$) and its fine-grained alignment with patched image regions $\mathbf{r}$. This is a challenging task since $\mathcal{F}_{\mathbf{s}}$ should have fine-grained sketch-photo alignment across multiple categories \cite{pang2019generalising} and generalise to unseen open-vocabulary \cite{openVocab-semanticCLIP, gu2022open-vocab-OD} setup.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/sketchOD.pdf}
    \caption{Caption}
    \label{fig:proposed}
\end{figure}

\subsection{Prompt Learning for Cross-Category FG-SBIR}\label{sec: prompt-sketch}
\noindent First, we briefly introduce fine-grained sketch-based image retrieval (FG-SBIR). Given a sketch-photo pair represented as ($\mathbf{s}, \mathbf{p}$), we use a sketch and photo encoder $\mathcal{F}_{\mathbf{s}}$ and $\mathcal{F}_{\mathbf{p}}$ to get feature map $f_{\mathbf{s}} \in \mathbb{R}^{512}$ and $f_{\mathbf{p}} \in \mathbb{R}^{512}$. The network is trained via triplet loss with margin parameter $\mu > 0$ such that the cosine distance $\delta(\cdot)$ of sketch anchor $\mathbf{s}$ from a negative photo ($\mathbf{p}^{-}$) should increase while that from the positive photo ($\mathbf{p}^{+}$) should decrease as, $\mathcal{L}_{trip} = \max \{0, \mu + \delta(f_{\mathbf{s}}, f_{\mathbf{p}}^{+}) - \delta(f_{\mathbf{s}}, f_{\mathbf{p}}^{-}) \}$.

Next, we extend FG-SBIR to multiple categories, by exploiting the open-vocabulary potential of CLIP \cite{CLIP}. CLIP \cite{CLIP} consists of two encoders, one for image and the other for text, trained on $400M$ text-image pairs. Using the ViT-backbone \cite{ViT}, we inject\footnote{While there are multiple ways \cite{bahng2022visualPrompt} to inject visual prompts, for brevity we follow \cite{VPT} that injects prompt as an ``patch" in the first ViT layer.} a learnable sketch and image prompt $\gamma_{\mathbf{s}} \in \mathbb{R}^{768}$ and $\gamma_{\mathbf{p}} \in \mathbb{R}^{768}$ that induce the pretrained CLIP image encoder to model generalised fine-grained associations between sketch ($\mathbf{s}$) and photo ($\mathbf{p}$) across multiple categories (open vocabulary). Formally, we compute sketch and photo features using CLIP's image encoder as: $f_{\mathbf{s}} = \mathcal{F}_{clip} (\mathbf{s}, \gamma_{\mathbf{s}})$ and $f_{\mathbf{p}} = \mathcal{F}_{clip} (\mathbf{p}, \gamma_{\mathbf{p}})$ respectively. To instill category-wise discriminative knowledge for cross-category FG-SBIR, we additionally use the text-encoder in CLIP for category-level classification as: $\mathcal{L}_{cat} = \texttt{Cross\_Entropy}(\mathbf{c}_{\mathbf{s}}, \texttt{softmax}([f_{\mathbf{t}}^{1} \cdot f_{\mathbf{s}}^{T}, \dots, f_{\mathbf{t}}^{N} \cdot f_{\mathbf{s}}^{T} ]))$, where $f_{\mathbf{t}}^{i}$ is the text embedding from the text encoder in CLIP given input ``a photo of a $<$category$>$", and $\mathbf{c}_{\mathbf{s}} \in \mathbb{R}^{N}$ is a one-hot vector representing the category of sketch ($\mathbf{s}$). Hence, the generalised cross-category prompt ($\gamma_{\mathbf{s}}, \gamma_{\mathbf{p}}$) is learned as $\mathcal{L}_{pmt} = \mathcal{L}_{trip} + \mathcal{L}_{cat}$. While the pretrained $\mathcal{F}_{clip}(\cdot, \gamma_{\mathbf{s}})$ serves as the sketch encoder to train weakly supervised object detection in \cref{sec: WSOD}, the photo encoder $\mathcal{F}_{clip}(\cdot, \gamma_{\mathbf{p}})$ additionally aligns the proposal feature $f_{\mathbf{r}}$ by computing the L1 distance as: $\mathcal{L}_{kd} = ||f_{\mathbf{r}} - \mathcal{F}_{clip}(\texttt{Crop}(\mathbf{p}, \mathbf{r})) ||_{1}$ as shown in \cref{fig:proposed}.

\subsection{Bridging Object-Level and Image-Level}\label{sec: object-scene}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth, height=0.3\linewidth]{latex/figures/synth-data-sample.pdf}
    \caption{Synthetic scene-level photos created by stitching $\{k^{2} \ | \ k=1, \dots 5\}$ object-level image patches used for training.}
    \label{fig:data-augmentation}
\end{figure}
\noindent Despite the flexibility and fine-grained nature of sketches, the sketch encoder $\mathcal{F}_{clip}(\cdot, \gamma_{\mathbf{s}})$ and photo encoder $\mathcal{F}_{clip}(\cdot, \gamma_{\mathbf{p}})$ used to train our object detection network were trained on object-level sketch-photo pairs \cite{sketchy}. This creates inconsistencies for downstream applications where photos have scene-level information with multiple foreground objects and their interactions. To resolve this object/scene level inconsistency we design a data augmentation tricks similar to CutMix \cite{cutmix}. In particular, we synthesise scene-level photos by combining $k^{2}$ object-level photos from FG-SBIR datasets \cite{sketchy} by first creating an empty canvas $\mathcal{C}$ of size ($kH \times kW \times 3$). Next, $k^{2}$ photos, each of size $\mathbf{p} \in \mathbb{R}^{H \times W \times 3}$ are randomly assigned a grid as shown in \cref{fig:data-augmentation}. The sketches corresponding to the $k^{2}$ photos are labeled as positive sketch queries in \cref{sec: WSOD}. During training, we additionally sample $k^{2}$ negative sketch instances that are not present in the canvas $\mathcal{C}$. Finally, we resize $\mathcal{C}$ from $(kH \times kW \times 3) \rightarrow (H \times W \times 3)$ and fed to the object detector module $\mathcal{F}$ for training. Empirically we found $k=10$ to be optimal. Note, while the sketch encoder and photo encoder given by $\mathcal{F}_{\mathbf{s}}$ and $\mathcal{F}_{\mathbf{p}}$ are trained on object-level sketch-photo pairs, but the backbone network $\mathcal{F}$ of our object detector learns complex pseudo scene complexity due to our custom data augmentation trick.

\section{Experiments}

\keypoint{Dataset} We train our object detection network using existing cross-category FG-SBIR dataset -- Sketchy \cite{sketchy} that contains $125$ categories, each with $100$ photos. Each photo has at least $5$ sketches with fine-grained associations. To evaluate fine-grained object detection, we use SketchyCOCO \cite{sketchycoco2020} comprising of natural images in MS-COCO \cite{mscoco} with paired scene sketches. Following Liu \etal \cite{liu2020scenesketcher}, we select on $1,225$ sketch/photo pairs from SketchyCOCO \cite{sketchycoco2020} with at least one foreground sketched object. Since scene sketches in SketchyCOCO are generated from object sketches in Sketchy \cite{sketchy}, we filter the overlapping categories from Sketchy \cite{sketchy} to prevent information leak and measure zero-shot potential of CLIP-based fine-grained object detection to unseen categories. We also compare with the popular ``toy" \footnote{The potential of sketch is revealed when specifying fine-grained visual cues. For  category-level information, using text is an easier alternative.} setup of category-level sketch-guided object detection \cite{tripathi2020object}. For this, we train SBIR (used to supervise object detection) on category-level sketch/photo pairs in QuickDraw-Extended \cite{doodle-to-search} with $330k$ sketches and $204k$ photos from $110$ categories ($80$ for training and $30$ for testing). Following \cite{tripathi2020object}, we evaluate on subset of standard object detection benchmark MS-COCO \cite{mscoco} and PASCAL-VOC \cite{pascalVOC} that have {$56$} and {$20$} common categories with QuickDraw \cite{sketchrnn2018} respectively. Note, we do not use any bounding-box annotation during training.

% We use the FG-SBIR dataset Sketchy \cite{sketchy} to train our object detection models. Sketchy \cite{sketchy} comprise of 

% SketchyCOCO, SketchyScene for instance-level || QuickDraw-Scene, TU-Berline-Scene for category-level || QuickDraw-Part * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\

\keypoint{Implementation Details} Our model is implemented in PyTorch on a 11GB Nvidia RTX 2080-Ti GPU. First, we train a generalised cross-category FG-SBIR with image size ($224 \times 224$) by adapting CLIP via prompt learning \cite{VPT}. The prompts are trained with triplet loss \cite{yu2016shoe},  margin $\mu=0.3$, Adam optimiser with learning rate $0.0001$ for $60$ epochs, and batch size $64$. Our object detection pipeline is build using Detectron2 \cite{detectron2}. We use FasterRCNN \cite{faster-rcnn}, pretrained on Visual Genome \cite{visualgenome} and remove the RoIPooling \cite{fast-rcnn} and subsequent layers to keep only the pretrained backbone ResNet+FPN ($\mathcal{F}$) \cite{he2015resnet, lin2017FPN} and Region Proposal Network ($\mathcal{R}$) that generates $1000$ proposals. The model trains using SDG with batch size $8$ and initial learning rate $0.005$, multiplied by $0.1$ at $150k$ and $250k$ iterations. We train in a two-step process: (i) keeping $\mathcal{F}$ and $\mathcal{R}$ fixed, we train the newly added RoI pooling followed by FC layers ($\mathcal{P}$), classification head $\phi_{cls}$, detection head $\phi_{det}$, and refinement head $\phi_{cls}^{*}$ for $240k$ iterations. (ii) We freeze $\mathcal{R}$ and finetune the rest for $80k$ iterations. Non-maxima suppression with IoU $\geq 0.3$ is applied to get final predictions. During training, we input a canvas $\mathcal{C}$ combining $\{k^{2} \ | \ k=[1, \dots, 5] \}$ object-level photos from existing sketch/photo datasets \cite{sketchy, sketchrnn2018} with paired sketch as image-level annotations.

\keypoint{Evaluation Metric}  (i) For fine-grained object detection, we use $AP_{.5}$, $AP_{.7}$, and $mAP$ that computes the average precision (AP) at $0.5$, $0.7$, and average of $10$ intersection-over-union (IoU) steps $\{0, \dots, 1\}$ of the detected boxes with ground truth respectively. (ii) For category-level object detection, \emph{CorLoc} measure percentage of images for which the most confident predicted box has IoU $\geq 0.5$ with at least one of the ground-truth boxes for every class, and (iii) For cross-category FG-SBIR, we measures $Acc.@q$ -- percentage of sketches having true matched photo in the top-q list, and (iv) mean average precision ($mAP$), and precision condering top $200$ retrievals $P@200$ for category-level SBIR.

\subsection{Competitors}  
For object detection, we compare against, (i) supervised object detection (SOD) using both bounding box and sketch/photo annotations: \textbf{Mod-FRCNN} adapts Faster-RCNN \cite{faster-rcnn} for unseen class by concatenating query sketch feature with the RoI pooled feature and passed through a binary classifier for detection. \textbf{MatchNet} \cite{matchnet2019} extends \emph{Mod-FRCNN} using co-attention to generate region proposals conditioned on query sketch followed by squeeze-and-co-excitation to adaptively re-weight importance distribution of candidate proposals. \textbf{CoAttOD} \cite{tripathi2020object} improves upon \emph{MatchNet} by mitigating the sketch/photo domain misalignment using cross-modal attention. (ii) Weakly supervised object detection (WSOD) trains only on image-level sketch annotations without using additional bounding boxes: \textbf{WSDDN} \cite{wsddn} repurposed object detection as a region classification via multiple instance learning (MIL) paradigm. To inject query sketch, we use cross-attention with RoI pooled feature followed by a binary classifier for detection. \textbf{OICR} \cite{OICR} improves \emph{WSDDN} using an iterative MIL to refine initial predictions that improve discriminatory power of the classifier. \textbf{PCL} \cite{pcl2018} generates multiple positive instance in an image via clustering and assigning proposals the label of corresponding object class for each cluster. \textbf{ICMWSD} \cite{ren2020WSOD} addresses the problem of prior WSOD that focus on the most discriminative part of an object using context information. In particular, \emph{ICMWSD} obtains a ``dropped feature" by dropping the most discriminative parts, followed by maximising the loss of the ``dropped feature" that force the network to look at the surrounding context regions. (iii) To evaluate real-world performance on unseen image distribution (i.e., unseen dataset), we introduce a new paradigm -- extreme weakly supervised object detection (EWSOD). We adapt \textbf{$<$Method$>$} in \emph{WS-SOTA} by replacing selective search with RPN (pretrained on Visual Genome \cite{visualgenome}) to generate proposals and use object-level sketch/photo pairs (see \cref{sec: object-scene}), to evaluate on SketchyCOCO \cite{sketchycoco2020}, MS-COCO \cite{mscoco}, and PASCAL-VOC \cite{pascalVOC}, as \textbf{E-$<$Method$>$}. 

For zero-shot category-level SBIR, we compare against: \textbf{ZS-GRL} \cite{doodle-to-search} combines similar semantic information (word2vec \cite{word2vec2013}) of class labels with visual sketch information and trains over a gradient reversal layer \cite{grl2015} to reduce sketch/photo domain gap. \textbf{ZS-STT} \cite{sain2022sketch3t} adapts to unseen categories and data distribution using a slow test-time training. Although test-time training for every new category limits large-scale applications, \emph{ZS-STT} provides a good ``upper-bound" performance. For zero-shot cross-category FG-SBIR: \textbf{Cross-DG} is a SOTA domain generalisation method \cite{shankar2018} adapted to cross-category FG-SBIR \cite{pang2019generalising} using categories as domain and inter-category sketch/photo pairs as label. \textbf{CC-DG} \cite{pang2019generalising} models a universal manifold of prototypical visual sketch traits that dynamically embeds sketch/photo, to generalise to unseen categories. 

\subsection{Generalisibility of Cross-Category FG-SBIR} 
Our object detector is trained in \emph{EWSOD} setup (without using bounding box annotation, or test data distribution) using a cross-category FG-SBIR as teacher. Hence, the accuracy of teacher puts a bottleneck on the student's object detection performance. \cref{tab:generalised-SBIR} compares category-level SBIR (CL-SBIR) and cross-category FG-SBIR (CC-FGSBIR) on QuickDraw-Extended \cite{doodle-to-search} and Sketchy \cite{sketchy} respectively, with $70\%$ of all categories are used for training. To further evaluate zero-shot generalisation, we reduce the training data to $50\%$, and $30\%$ of all categories.

\keypoint{Performance Analysis} From \cref{tab:generalised-SBIR} we make the following observations: (i) with decreasing train (seen) categories, performance drops in both CL-SBIR and CC-FGSBIR. However, while \emph{ZS-GRL} and \emph{Cross-DG} drops by $xx.x\%$ and $xx.x\%$ ($70\% \rightarrow 30\%$) on CL-SBIR and CC-FGSBIR, our proposed method drops by $xx.x\%$ and $xx.x\%$ respectively. This is due to the greater zero-shot potential when using prompt-based CLIP models for sketch/photo matching. (ii) Performance drop in CC-FGSBIR is more significant $xx.x\%$ as compared to CL-SBIR $xx.x\%$. Hence, it is more difficult to discriminate unseen inter-category sketch/photo pairs than recognise a novel category. This is since CLIP was pre-trained on scene-level image/text pairs that holds greater semantic information, necessary for category-level retrieval, compared to structural information required for fine-grained matching. (iii) Performance of all competitors in CL-SBIR and CC-FGSBIR are staggeringly inferior to proposed CLIP-based approach. Such a strong SBIR is necessary to unlock training object detection in EWSOD setup.


{
\setlength{\tabcolsep}{0.6em}
\begin{table}[]
    \centering
    \footnotesize
    \caption{Quantitative performance of zero-shot category-level SBIR (CL-SBIR) and cross-category FG-SBIR (CC-FGSBIR). We compare Proposed CLIP-based SBIR method with zero-shot category-level SBIR methods like Doodle2Search (D2S) \cite{doodle-to-search}, SketchTT (STT) \cite{sain2022sketch3t}, and cross-category FGSBIR methods like CrossGrad (CG) \cite{pang2019generalising}, CC-DG (CC) \cite{pang2019generalising}.}
    \vspace{-1em}
    \begin{tabular}{ccccccc}
        \toprule
        \multirow{2}{*}{\makecell{Train\\Categories}} &  & \multicolumn{2}{c}{CL-SBIR} & & \multicolumn{2}{c}{CC-FGSBIR} \\
         &  & mAP & P@200 & & Acc.@1 & Acc.@5 \\\hline
        \multirow{2}{*}{$70\%$} & D2S & xx.x & xx.x & CG & xx.x & xx.x \\
         & STT & xx.x & xx.x & CC & xx.x & xx.x \\
        \rowcolor{Gray}
        \multicolumn{1}{c}{\textbf{Proposed}} & & xx.x & xx.x & & xx.x & xx.x \\\hline
        \multirow{2}{*}{$50\%$} & D2S & xx.x & xx.x & CG & xx.x & xx.x \\
         & STT & xx.x & xx.x & CC & xx.x & xx.x \\
        \rowcolor{Gray}
        \multicolumn{1}{c}{\textbf{Proposed}} & & xx.x & xx.x & & xx.x & xx.x \\\hline
        \multirow{2}{*}{$30\%$} & D2S & xx.x & xx.x & CG & xx.x & xx.x \\
         & STT & xx.x & xx.x & CC & xx.x & xx.x \\
         \rowcolor{Gray}
        \multicolumn{1}{c}{\textbf{Proposed}} & & xx.x & xx.x & & xx.x & xx.x \\\bottomrule
    \end{tabular}
    \label{tab:generalised-SBIR}
    \vspace{-1em}
\end{table}
}


\subsection{Category-Level Object Detection}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth, height=0.2\linewidth]{example-image-a}
    \vspace{-1em}
    
    \caption{Qualitative retrieval results for cross-category FG-SBIR.}
    \label{fig: retrieval-cross-category}
\end{figure}

We benchmark on subset of standard object detection MS-COCO \cite{mscoco} and PASCAL-VOC \cite{pascalVOC} datasets having overlapping sketch categories in QuickDraw \cite{sketchrnn2018} for category-level matching. Unlike traditional object detection that detect all instances and classes in an image, category-level object detection should detect only instances of a specific class specified by the query sketch.

\keypoint{Performance Analysis} From \cref{tab: detection-category} we observe: (i) SOD methods outperform WSOD by an average margin of $xx.x\%$. This is due to use of both bounding-box and sketch/photo annotation. (ii) Not using data distribution encountered during evaluation (EWSOD) further drops performance by $xx.x\%$ from WSOD. This highlights lack of generalisation of object detectors to shift in data distribution, thereby limiting practical deployment. (iii) Our proposed method, in spite of being trained on the challenging EWSOD setup, outperforms competitors in both EWSOD and WSOD to reach performance close to SOD -- only $xx.x\%$ gap with SOD. This shows extreme potential of training object detection with a strong CL-SBIR.

{
\setlength{\tabcolsep}{0.8em}
\begin{table}[]
 \centering
 \footnotesize
 \caption{VOC 2007 and MS-COCO detection $AP_{50}$ and \emph{CorLoc}}
 \begin{tabular}{clcccc}
     \toprule
     \multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c}{VOC 2007 \cite{pascalVOC}} & \multicolumn{2}{c}{MS-COCO \cite{mscoco}} \\
      & & $AP_{.5}$ & CorLoc & $AP_{.5}$ & CorLoc \\\hline
     \multirow{3}{*}{\rotatebox{90}{\textbf{SOD}}} & Mod-FRCNN & xx.x & xx.x & xx.x & xx.x \\
      & MatchNet & xx.x & xx.x & xx.x & xx.x \\
      & CoAttOD & xx.x & xx.x & xx.x & xx.x \\\hline
     \multirow{4}{*}{\rotatebox{90}{\textbf{WSOD}}} & WSDDN & xx.x & xx.x & xx.x & xx.x \\
      & OICR & xx.x & xx.x & xx.x & xx.x \\
      & PCL & xx.x & xx.x & xx.x & xx.x \\
      & ICMWSD & xx.x & xx.x & xx.x & xx.x \\\hline
     \multirow{4}{*}{\rotatebox{90}{\textbf{EWSOD}}} & E-WSDDN & xx.x & xx.x & xx.x & xx.x \\
      & E-OICR & xx.x & xx.x & xx.x & xx.x \\
      & E-PCL & xx.x & xx.x & xx.x & xx.x \\
      & E-ICMWSD & xx.x & xx.x & xx.x & xx.x \\
     \rowcolor{Gray}
      & {\textbf{Proposed}} & xx.x & xx.x & xx.x & xx.x \\\bottomrule
 \end{tabular}
 \label{tab: detection-category}
\end{table}
}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/category-levelOD.pdf}
    \caption{Category-Level Object Detection using query sketches with images from MS-COCO \cite{mscoco} and PASCAL-VOC \cite{pascalVOC}.}
    \label{fig: detection-category}
\end{figure}
 
\subsection{Fine-Grained Object Detection}
The real potential of sketch is unlocked when describing fine-grained visual cues. Unlike traditional object detection that detects all instances of all classes, or category-level object detection that detects all instance of a specified class, the goal of fine-grained object detection is to detect only a specific instance for the input query sketch. Due to instance-specific detection requirement, we do not use \emph{CorLoc} metric that computes over all instance in a particular class.

\keypoint{Performance Analysis} From \cref{tab: detection-finegrained} we observe: (i) SOD methods outperform WSOD and EWSOD approaches, similar to category-level object detection in \cref{tab: detection-category}. (ii) Compared to SOD, the performance of WSOD drops more for $AP_{.5} \rightarrow AP_{.7}$. This is since WSOD methods use less accurate selective search \cite{selective-search} and edge boxes \cite{edgeboxes} for region proposals compared to the more accurate RPN \cite{faster-rcnn} in SOD. (iii) Although our proposed method trained on EWSOD setup performs lower than SOD, the performance gap $xx.x\%$ is lower for fine-grained detection as compared to $xx.x\%$ in category-level detection. This shows our proposed method has better discriminibility via triplet loss, compared to competitors that injects query sketch via cross-modal attention followed by binary classification.
 
{
\setlength{\tabcolsep}{1.2em}
\begin{table}[]
    \centering
    \footnotesize
    \caption{SketchyCOCO detection fine-grained}
    \begin{tabular}{clccc}
        \toprule
        & Method & $AP_{.5}$ & $AP_{.7}$ & mAP \\\hline
         \multirow{3}{*}{\rotatebox{90}{\textbf{SOD}}} & Mod-FRCNN & xx.x & xx.x & xx.x \\
          & MatchNet & xx.x & xx.x & xx.x \\
          & CoAttOD & xx.x & xx.x & xx.x \\\hline
         \multirow{4}{*}{\rotatebox{90}{\textbf{WSOD}}} & WSDDN & xx.x & xx.x & xx.x \\
          & OICR & xx.x & xx.x & xx.x \\
          & PCL & xx.x & xx.x & xx.x \\
          & ICMWSD & xx.x & xx.x & xx.x \\\hline
         \multirow{4}{*}{\rotatebox{90}{\textbf{EWSOD}}} & E-WSDDN & xx.x & xx.x & xx.x \\
          & E-OICR & xx.x & xx.x & xx.x \\
          & E-PCL & xx.x & xx.x & xx.x \\
          & E-ICMWSD & xx.x & xx.x & xx.x \\
         \rowcolor{Gray}
          & {\textbf{Proposed}} & xx.x & xx.x & xx.x \\\bottomrule
    \end{tabular}
    \label{tab: detection-finegrained}
\end{table}
}
 
 \begin{figure}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/fine-grainedOD.pdf}
    \caption{Cross-Category Fine-Grained Object Detection using query sketches with images from SketchyCOCO \cite{sketchycoco2020}.}
    \label{fig: detection-finegrained}
\end{figure}
 
\subsection{Part-Level Object Detection}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth, height=0.5\linewidth]{latex/figures/part-levelOD.pdf}
    \caption{Unlike traditional object detection that detects an entire object (e.g., a ``cat"), sketches can specify fine-grained region-of-interest to detect a part of an object (e.g., the ``head" of a ``cat").}
    \label{fig: detection-part}
\end{figure}

Encouraged with the discriminative power of the proposed method in \cref{tab: detection-finegrained}, we go a step further and ask: Apart from uniquely detecting an instance, can we only detect a \emph{part} (e.g., only `head') of the instance? A large-scale quantitative evaluation of part-level object detection requires appropriate part-level annotation for both sketch and photo. However, existing fine-grained sketch/photo datasets like SketchyCOCO \cite{sketchycoco2020} lacks such dense annotation. Nonetheless, we conduct a small-scale qualitative study by manually editing sketches to create sketches of a single part. \cref{fig: detection-part} shows (i) our proposed method can uniquely detect the sketched `head' region of different objects. (ii) Detection performance is lower for ambiguous part sketches like `leg' (e.g., front-leg, back-leg etc.) (iii) Since detection depends on region proposals from RPN, our model fails to detect tiny sketched parts. Tiny object detection \cite{lee2022tinyOD} is a known challenge for traditional object detection \cite{faster-rcnn}. We hope future works on fine-grained object detection can resolve this by guiding the RPN towards sketched regions.
 
\subsection{Ablation}
\keypoint{Selective Search v/s Edge Boxes v/s RPN} Our proposed method use RPN, pre-trained on Visual Genome \cite{visualgenome} dataset, to generate $1000$ object proposals. However, WSOD methods primarily use selective search \cite{selective-search} or edge boxes \cite{edgeboxes} that do not require pre-training using bounding box annotation from Visual Genome \cite{visualgenome}. Hence, for a fair comparison with WSOD, we replace our RPN with selective search (\textbf{Ours-SS}), or edge boxes (\textbf{Ours-EB}) to generate $2000$ proposals. In spite of generating more proposals ($2000$ instead of $1000$ for RPN) and thereby slower inference, our proposed method still outperforms \emph{Ours-SS} by $xx.x\%$ and \emph{Ours-EB} by $xx.x\%$ on $AP_{.5}$ for fine-grained object detection.
 
\keypoint{Influence of Classifier Refinement} Similar to \emph{OICR} \cite{OICR}, we refine our initial predictions by $\phi_{cls}$ and $\phi_{det}$ with an iterative refinement head $\phi^{*}_{cls}$ for $K$ steps. Similar to \cite{OICR}, we observe $AP_{.5}$ to improve by $xx.x\%$ and $xx.x\%$ for $K=1 \rightarrow 2$ and $K=2 \rightarrow 3$ respectively. However, we notice a small drop of $xx.x\%$ for $K=3 \rightarrow 4$. Such drop with increasing iteration is typical for iterative approaches \cite{iterativetext2021}.

\keypoint{Influence of $\mathcal{L}_{kd}$ from CLIP's Photo Encoder} Distilling from CLIP's image encoder via feature distillation \cite{heofeaturedistillation2019} to the output from RoI pooled feature $f_{\mathbf{r}}$ is computationally expensive process. Each region proposal from RPN is used to crop an image region followed by encoding it via CLIP's image encoder. Hence, during training given an input image, we generate $1000$ proposals that requires passing $1000$ cropped image regions from CLIP's encoder to compute distillation loss $\mathcal{L}_{kd}$. Given the expensive compute nature of $\mathcal{L}_{kd}$ we ask, is it worth it? Removing $\mathcal{L}_{kd}$ from our proposed method leads to $xx.x\%$ drop in $AP_{.5}$. However, increasing our training iterations from $320k$ to $450k$ reduces this drop to $xx.x\%$. Hence, while $\mathcal{L}_{kd}$ helps in faster and better training, removing it can still train an objection pipeline.
 
\subsection{Limitation and Future Works} 
Introducing fine-grained object detection using sketch opens several possibilities that we do not consider. Our proposed method essentially matches generated proposals with input query sketches. We assume that each sketch represent a single object or category. However, a user might be interested in detecting complex regions (a ``dog" on the right of a ``person") with \emph{multiple} objects that have meaningful \emph{spatial} alignment. Future works can extend fine-grained object detection or semantic segmentation to complex sketches using the recently introduced FS-COCO \cite{fscoco} dataset.
 
\subsection{Conclusion}
In this work, we introduce a new paradigm to object detection -- instead of detecting all regions, how sketches can help detect specified fine-grained region of interest. To further study the practical application, we impose strict constraint on training data -- (i) without collecting additional bounding box annotation for downstream data distribution and relying on existing sketch/photo pair datasets (ii) without having access to unlabelled test-time data distribution used for evaluation. We exploit the popular generalisation potential of CLIP and extend it to sketch/photo matching using prompt learning to learn a state-of-the-art category-level SBIR and cross-category FG-SBIR models. En-armed with this new SOTA sketch/photo matching network as teacher, we train a student object detection framework. The resulting method closes in the performance gap with supervised counterpart, showing the potential of sketch for object detection.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
