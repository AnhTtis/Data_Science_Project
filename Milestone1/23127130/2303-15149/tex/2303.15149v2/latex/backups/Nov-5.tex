% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{color, colortbl}
\usepackage{xcolor}
% \usepackage{bbold}
\usepackage{multirow}
\usepackage{makecell}
\newcommand{\keypoint}[1]{\vspace{0.01cm}\noindent\textbf{#1}\quad}
\newcommand{\cut}[1]{}
\definecolor{Gray}{gray}{0.9}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Can Sketch Based Image Retrieval Train Object Detectors?}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   Our contributions are two-fold: (i) First, we propose the first generalised fine-grained sketch-based image retrieval network leveraging the zero-shot potential of CLIP via prompt learning. (ii) Second, using the generalised FG-SBIR network, we train an object detector without using scene images or bounding box annotations. We exclusively train on existing object-level sketch/photo pairs and show zero-shot potential on standard object detection datasets like MS-COCO \cite{mscoco} and PASCAL-VOC \cite{pascalVOC}. Training object detection network from a generalise CLIP-based FG-SBIR network injects zero-shot potential into our detection framework, thereby helping practical applications. * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth, height=0.5\linewidth]{example-image-a}
    \caption{Explain the Problem Setup we aim to solve}
    \label{fig:problem-setup}
\end{figure}

* \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\

\section{Related Works}
\label{sec:related-works}

\noindent \textbf{Sketch for Visual Understanding:}
Hand-drawn sketches serve as a useful query modality for visual understanding tasks that involve human perception and structural cues. Sketches not only convey a visual description \cite{hertzmann2020perception} but also exhibit artistic styles \cite{zhang2021smartshadow}. This makes sketch a vital querying modality for the creative industry like artistic image editing \cite{yang2020surgery} and animation \cite{xing2015autocomplete}. Unlike photos that are passively captured by a camera, sketches are actively drawn by humans which makes them a good visual representation \cite{pixelor} enriched with human participation. Apart from the widely explored sketch-based image retrieval \cite{bhunia2021semi, bhunia2020sketch, pang2019generalising, styleMeUp, sketchformer, liveSketch}, sketch as a query has shown potential in several vision understanding tasks like incremental learning \cite{bhunia2022incremental} image and video synthesis \cite{cusuh2022synthesis, DeepFaceVideoEditing2022}, representation learning \cite{alaniz2022primitives, clipasso}, image-inpainting \cite{xie2021inpainting}, 3D shape retrieval \cite{xu20223Dretrieval}, 3D shape modeling \cite{chowdhury20223Dsynthesis}, medical image analysis \cite{wang2022medical}, object localisation \cite{tripathi2020object, riba2021object} and segmentation \cite{hu2020segmentation, qi2022segmentation}.

While exploring sketch as a query for object detection, by Tripathi \etal \cite{tripathi2020object}, several limitations surfaced with respect to problem definition as well as architectural designs. 
Firstly, instead of fine-grained matching, sketch was used to specify object category (easier via text/keyword \cite{CLIP, gu2022open-vocab-OD}), thus \cut{wasting} missing the potential of sketch as a query in modeling fine-grained details.
% Primarily, potential of sketch as a query in modeling fine-grained details remains unused, as specifying object category via sketching is quite laborious \cite{bhunia2020sketch}, compared to a text/keyword \cite{CLIP, gu2022open-vocab-OD}.
Secondly, it requires both bounding-box and object-sketch annotation which increases annotation budget without significant improvement in performance over traditional object detection setup. 
% \textcolor{red}{Secondly, as specifying object category by sketching is laborious \cite{bhunia2020sketch}, compared to a text/keyword \cite{CLIP, gu2022open-vocab-OD}, the potential of sketch in modeling fine-grained details remains unused.}
% thus overlooking the potential of sketch in modeling fine-grained details -- a \emph{fine-grained query}. 
% sketch as a query prides in its ability to model fine-grained details \cite{styleMeUp, bhunia2022worrying, bhunia2020sketch}. 
Thirdly, due to an expensive bounding box and sketch annotation, only fewer than $50\%$ object categories in existing object detection datasets \cite{pascalVOC, mscoco} are available for training. Finally, using an early fusion strategy \cite{xu2022MML} of sketch with object detection results in recomputing object regions for each new sketch -- leading to a slower detection framework with increasing query sketches.
% \cite{tripathi2020object} employs an early fusion strategy, that uses query-sketch as a conditional input to predict object regions, per sketch-photo pair -- resulting in a slower paradigm as sketch categories increase.
In this paper, we propose a fine-grained sketch-based object detection framework that use only object-level sketch photo pairs without any bounding-box annotations for training, and is scalable with multiple fine-grained query sketches, even under zero-shot setup.

\noindent \textbf{Supervised Object Detection:} 
Object detection jointly localise and identify objects in an image. Traditional object detectors rely on large supervised object detection datasets such as PASCAL VOC \cite{pascalVOC} and MS-COCO \cite{mscoco}, containing thousands of examples per object category which are quite time-consuming to annotate, unlike our pipeline, which leverages sketch-photo pairs as object annotations. 
% have multitudes\cut{over hundreds and thousand} of time-consuming annotated examples per object category. 
Existing literature on object detection is bifurcated as: (i) fast yet less accurate single-shot \cite{yolo, yolov3, SSD, lin2017focal-loss, centerNet}; (ii) slow but more accurate two-stage object detectors \cite{rcnn, fast-rcnn, faster-rcnn, maskrcnn}. To fully exploit the fine-grained cues provided by sketch, our proposed method aligns with the two-stage detectors. Two-stage detectors predict object regions using selective search in RCNN \cite{rcnn}, ROI pooling \cite{fast-rcnn} in Fast-RCNN, and Region Proposal Network (RPN) with ROIAlign in Faster-RCNN \cite{faster-rcnn}. While there has been several attempts with sophisticated architectural modifications   \cite{law2018cornerNet, zhou2019grouping, centerNet}, Faster-RCNN \cite{faster-rcnn} still acts a a fundamental building block for several downstream tasks like scene-graph generation \cite{yang2018scenegraph}, visual grounding \cite{mouzenidis2021grounding}, and relationship prediction \cite{zhu2018relationship}. Therefore, we resort to the more traditional Faster-RCNN based two-stage pipeline.


\noindent \textbf{Weakly Supervised Object Detection (WSOD):} Collecting bounding box annotation per object category is already a time-consuming process, which is aggravated even further by Fine-grained object detection (e.g., recognising animal species). Aiming to overcome this, existing WSOD methods adopt two schools of thoughts: (a) formulate this task as a multiple instance learning (MIL) \cite{dietterich1997MIL, wsddn, li2016wsod, diba2017wsod, jie2017wsod, zhang2018wsod, tang2018wsod, shen2019wsod} problem that interpret an image as a bag of proposals or regions. The image is labeled positive, when one of the regions tightly contains the object of interest, otherwise negative if no region has it.
% whereas a negative would imply no region has it.
% {If the image is labeled negative, no region contains the object.} 
% A benefit of MIL-based approaches is its ability to detect multiple objects from different categories \cite{}.
% An image is given to a proposal generator (selective search \cite{selectivesearch}, edge boxes \cite{edgeboxes}, sliding window \cite{zhang2019freeanchor}) and backbone network \cite{alexnet, vgg, inception, he2015resnet} to get proposals and feature maps respectively. Next, feature maps and proposals are given to spatial pramid pooling (SPP) \cite{SPPNet} to generate fixed-size regions. Finally, these regions are fed to the detection head to classify and localise object instances. 
(b) resort to CAM-based methods \cite{zhou2016CAM, zhang2018cam} that use class activation maps to predict proposals. Specifically, an image is fed to a backbone network to generate feature vector from which bounding box of each class is predicted by thresholding activation maps with highest probability.
% This feature vector is given to a classifier to generate prediction scores of an image. Finally, CAM generates proposals or bounding box of each class by thresholding to segment activation maps with the highest probability of every class. 
Although CAM-based methods are faster, in this paper we use MIL-based technique as it can detect \textit{multiple instances} within the \textit{same} category \cite{wsodsurvey2022}.



% Collecting bounding box annotation per object category is time-consuming. Fine-grained object detection (e.g., recognising animal species) exacerbates this problem. Existing approaches formulate this task as multiple instance learning (MIL) \cite{dietterich1997MIL, wsddn, li2016wsod, diba2017wsod, jie2017wsod, zhang2018wsod, tang2018wsod, shen2019wsod} problem that interpret an image as bag of proposals or regions. If the image is labeled positive, then one of the regions tightly contain the object of interest. If the image is labeled negative, no region contains the object. We feed an image to a proposal generator (selective search \cite{selectivesearch}, edge boxes \cite{edgeboxes}, sliding window \cite{zhang2019freeanchor}) and backbone network \cite{alexnet, vgg, inception, he2015resnet} to get proposals and feature maps respectively. Next, feature maps and proposals are given to spatial pramid pooling (SPP) \cite{SPPNet} to generate fixed-size regions. Finally, these regions are fed to the detection head to classify and localise object instances. Instead of generating thousands of proposals as in MIL, an CAM-based methods \cite{zhou2016CAM, zhang2018cam} use class activation maps to predict proposals. Specifically, an image is fed to a backbone network to generate feature vector. This feature vector is given to a classifier to generate prediction scores of an image. Finally, CAM generates proposals or bounding box of each class by thresholding to segment activation maps with the highest probability of every class. In spite of CAM-based methods being faster, in this paper, we use MIL-based technique due to its ability in detecting multiple instances with the same category.

\noindent \textbf{Data Augmentation in Computer Vision:} Data augmentation improves the sufficiency and diversity of training data. Approaches vary from simple image rotation and flipping to more advanced techniques of image erasing \cite{gridmask} like CutOut \cite{devries2017cutout}, Hide-and-Seek \cite{hide-and-seek} and image mixing like MixUp \cite{mixup} and {CutMix} \cite{cutmix}. 
Aiming to generalise sketch-based object detection to complex scenes while training exclusively on existing object-level sketch photo pairs \cite{sketchy}, we thus employ a CutMix \cite{cutmix} like data augmentation trick -- a method that replaces removed sub-regions with a patch from another image to synthesise new images.

% In our context cutmix is quite relevant
% Out of these cutmix seems the most relevant to our cause
% CutMix \cite{cutmix} replaces removed sub-regions with a patch from another image to synthesise new images. In this paper, we employ a CutMix \cite{cutmix} like data augmentation trick, aiming to generalise sketch-based object detection to complex scenes while training exclusively on existing object-level sketch photo pairs \cite{sketchy}.


% In this paper, we employ data augmentation techniques like CutMix \cite{cutmix} that replaces removed sub-region with a patch from another image to synthesise new images. In particular, we replace the sub-regions of a scene with photos from different sketched categories. This helps learn object detection in complex scenes with multiple object instances by training from object-level sketch photo pairs.

% Data augmentation improves the sufficiency and diversity of training data. Approaches varies from simple image transformations like rotation, flipping, and cropping to the more advanced image erasing \cite{devries2017cutout, hide-and-seek, gridmask} and image mix \cite{mixup, cutmix}. Image erasing replace pixel values of one or more sub-regions in the image with constant (Cutout\cite{devries2017cutout}, Hide-and-Seek \cite{hide-and-seek}, GridMask \cite{gridmask}, FenceMask \cite{fencemask}) or random values (random erasing \cite{zheng2020random}). Image mix data combines two or more images or sub-regions into one. MixUp \cite{mixup} learns a linear relationship between mixing images from training set and their supervision signal. CutMix \cite{cutmix} replaces removed regions with a patch from another image to synthesise new images. Our proposed method is similar to CutMix \cite{cutmix} with sub-regions depicting photo of different sketched category from Sketchy dataset \cite{sketchy}. This helps the network learn complex scenes with multiple categories.

\noindent \textbf{Sketch-Based Object Representation}
Sketch with its intrinsic ability to model fine-grained visual details, makes it an ideal modality  for object retrieval, giving rise to avenues like \emph{category-level} \cite{liveSketch, yelamarthi2018sketch, doodle-to-search, sketchformer, sketchmate} and fine-grained (FG) \emph{instance-level} \cite{styleMeUp, bhunia2022worrying, bhunia2020sketch, bhunia2021semi} sketch-based image retrieval (SBIR). 
% The ability of sketch to offer inherently fine-grained visual description makes it an ideal modality for object retrieval. 
% Typically SBIR employs CNN \cite{liveSketch, doodle-to-search}, RNN \cite{sketchmate}, Transformer \cite{sketchformer}, or Vision Transformer \cite{fscoco} based deep triplet-ranking to learn joint embedding space \cite{yu2016shoe}. 
Contemporary research also explored zero-shot SBIR \cite{doodle-to-search, yelamarthi2018sketch, sain2022sketch3t}, cross-domain translation \cite{kaiyue2017cross} and approaches like reinforcement learning based on-the-fly retrieval \cite{bhunia2020sketch}, self-supervised \cite{pang2020jigsaw, vector-raster}, etc. Apart from object-level images, retrieving sketched objects from scene images was studied using graph convolutional networks \cite{liu2020scenesketcher} and optimal transport \cite{partially-does-it}. Similar to cross-category FG-SBIR \cite{kaiyue2017cross, bhunia2022adaptive}, here we explore fine-grained sketch photo association for object detection by adapting large vision-language models like CLIP \cite{CLIP} using prompt engineering \cite{zhou2022visualprompt}.

% to learn a fine-grained association using object-level sketch for fine-grained sketch-based object detection from scene images. 

% Concretely, we adapt large vision-language models like CLIP \cite{CLIP} using prompt engineering \cite{zhou2022visualprompt} to learn a cross-category FG-SBIR. Next, we distill knowledge from CLIP image-encoders for supervision.






% The ability of sketch to offer inherently fine-grained visual description makes it an ideal modality for object retrieval. This commenced avenues like \emph{category-level} \cite{liveSketch, yelamarthi2018sketch, doodle-to-search, sketchformer, sketchmate} and fine-grained (FG) \emph{instance-level} \cite{styleMeUp, bhunia2022worrying, bhunia2020sketch, bhunia2021semi} sketch-based image retrieval (SBIR). Typically SBIR employs CNN \cite{liveSketch, doodle-to-search}, RNN \cite{sketchmate}, Transformer \cite{sketchformer}, or Vision Transformer \cite{fscoco} based deep triplet-ranking based siamese networks to learn joint embedding space \cite{yu2016shoe}. Contemporary research is also directed towards zero-shot SBIR \cite{doodle-to-search, yelamarthi2018sketch, sain2022sketch3t}, binary hash-code embedding \cite{liu2017hashing, shen2018hashing}, attention mechanism \cite{deep-spatial-semantic}, cross-domain translation \cite{kaiyue2017cross}, reinforcement learning based on-the-fly retrieval \cite{bhunia2020sketch}, semi-supervised \cite{bhunia2021semi}, self-supervised \cite{pang2020jigsaw, vector-raster}, meta-learning \cite{bhunia2022adaptive}, style-agnostic retrieval \cite{styleMeUp}, etc. Apart from object-level images, retrieving sketched objects from scene images was studied using graph convolutional networks \cite{liu2020scenesketcher} and optimal transport \cite{partially-does-it}. In this paper, we show the potential of cross-category FG-SBIR \cite{kaiyue2017cross, bhunia2022adaptive} to solve object detection, a fundamental and challenging task in computer vision for a quarter century. Concretely, we adapt large vision-language models like CLIP using prompt engineering \cite{zhou2022visualprompt} to learn a cross-category FG-SBIR. Next, we formulate object detection as MIL problem and distill knowledge from CLIP image-encoders for supervision.

\section{Proposed Method}
\label{sec:proposed-method}
\keypoint{Overview} We propose a new paradigm training object detection without bounding box annotation or image-level class labels. Instead, we train an object detector by distilling knowledge from a sketch-based image retrieval. This leads to several emergent behaviors (i) fine-grained object detection -- specify a particular region-of-interest using fine-grained visual cues in sketch. (ii) category-level object detection -- specify region-of-interest of the same sketch class. (iii) part-level object detection -- instead of detecting the entire object (e.g., a ``horse"), we only detect specified parts (e.g., ``head" and ``legs" of a ``horse").
% (iv) Unlike supervised setups (SOD) that use bounding box annotation or weakly supervised setups (WSOD) that use image-level labels, we introduce extreme weakly supervised setup (EWSOD) that avoids using expensive bounding box annotation or image-level labels of downstream data distribution by exploiting zero-shot potential of CLIP and a conjugate sketch-based image retrieval task.

% \cut{We propose a new paradigm for training object detectors without using bounding box annotation or test data distribution. In particular, }
% We propose an object detection framework that can localise queried objects and has fine-grained alignment with input sketch. Firstly, we train this network without using both bounding-box annotation and hand-drawn sketch. Secondly, we begin with a pre-trained Region Proposal Network (RPN) \cite{faster-rcnn} and fine-tune the backbone feature extractor using a novel object-level cross-category FG-SBIR. Finally, we bridge the object-level and scene-level gap using a custom designed data augmentation scheme.
% We learn an object detection framework which can localise novel objects in a scene that has fine-grained alignment with a given input sketch. Importantly, we train this network without the use of bounding box annotations. To this end, we employ knowledge distillation from a pre-trained cross-category FG-SBIR teacher model to an object detection student model. In addition, to bridge the gap between FG-SBIR models trained using object-level sketch-photo pairs to sketch-guided object detection in scene images, we choose a simple yet effective data augmentation scheme following existing literature on Cut-Mix.

\subsection{Background}\label{sec: background}
Our model comprise of two key components -- an object detection and a sketch-based image retrieval module. For completeness, we give a brief background.

% \subsection{Baseline Supervised Object Detection}\label{sec: baseline-fasterRCNN}
% \vspace{-0.5cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{latex/figures/fasterRCNN.pdf}
    \vspace{-0.6cm}
    \caption{Schematic of baseline Faster-RCNN \cite{faster-rcnn}. }
    \label{fig: faster-rcnn}
    \vspace{-0.3cm}
\end{figure}

\keypoint{Baseline Supervised Object Detection:} For the sake of completeness, we briefly introduce a supervised object detection (SOD) framework, Faster-RCNN \cite{faster-rcnn}, that remains state-of-the-art \cite{yang2018scenegraph, mouzenidis2021grounding, zhu2018relationship}. Given a photo $\mathbf{p} \in \mathbb{R}^{H \times W \times 3}$, a backbone feature extractor (VGG \cite{vgg}, or ResNet \cite{he2015resnet}) $\mathcal{F}_{b}(\cdot): \mathbb{R}^{H \times W \times 3} \rightarrow \mathbb{R}^{H' \times W' \times 512}$ computes feature map $f_{\mathbf{p}} \in \mathbb{R}^{H' \times W' \times 512}$. Next, a two stage process is followed: (i) Given backbone feature map $f_{\mathbf{p}}$, a region proposal network (RPN) $\mathcal{R}: \mathbb{R}^{H' \times W' \times 512} \rightarrow \mathbb{R}^{R \times 5}$ generates rectangular boxes (i.e., proposals) $\mathbf{r} = \{r_1, \dots, r_R\}$, where $\mathbf{r} \in \mathbb{R}^{R \times 4}$ and ``objectness measure" -- a scalar $[0,1]$ probability of the box $r_j$ having an object. (ii) Using proposals $\mathbf{r} \in \mathbb{R}^{R \times 4}$ we pool the feature map $f_{\mathbf{p}}$ via RoI pooling \cite{fast-rcnn} to get intermediate feature of size $\mathbb{R}^{7 \times 7 \times 512}$, followed by a fully-connected layer (FC) to get $f_{\mathbf{r}} \in \mathbb{R}^{R \times 512}$ as, $f_{\mathbf{r}} = \mathcal{P}(f_{\mathbf{p}}, \mathbf{r})$. The patch feature $f_{\mathbf{r}}$ is branched into two streams -- first, a classification branch $\phi_{\mathrm{cls}}: \mathbb{R}^{R \times 512} \rightarrow \mathbb{R}^{R \times (|\mathcal{C}|+1)}$ outputs probability distribution (per RoI) over $\mathcal{C}$ pre-defined classes and a catch-all background class; second, a box regressions $\phi_{\mathrm{reg}}: \mathbb{R}^{R \times 512} \rightarrow \mathbb{R}^{R \times 4}$ refines initial box predictions $\mathbf{r} \in \mathbb{R}^{R \times 4}$. Although seminal, SOD requires expensive bounding box annotation, equally distributed across $\mathcal{C}$ classes. However, due to dataset bias \cite{torralba2011bias} -- Zipf's law \cite{zipf-law} and social conventions \cite{hendricks2018} -- collecting sufficient bounding box annotation for less occurring classes is near impossible. This motivated weakly supervised object detection (WSOD) that trains without using bounding box annotation.

\keypoint{Baseline SBIR Framework: } Given a sketch/photo pair ($\mathbf{s}, \mathbf{p}$ 

% First, we briefly summarise Faster-RCNN \cite{faster-rcnn}, a baseline object detection (OD) framework that remains state-of-the-art for several downstream tasks \cite{yang2018scenegraph, mouzenidis2021grounding, zhu2018relationship}. Given a photo $\mathbf{p} \in \mathbb{R}^{H \times W \times 3}$, a VGG-16 \cite{vgg} network $\mathcal{F}(\cdot)$ predicts a feature map $f_{\mathbf{p}} \in \mathbb{R}^{H' \times W' \times 512}$. Next, a region proposal network \cite{faster-rcnn} $\mathcal{R}(\cdot)$ generates $R$ rectangular proposals $\mathbf{r} = \{ r_1, r_2, \dots, r_R \}$ with feature $r_j \in \mathbb{R}^{512}$ from the convolutional feature map $f_{\mathbf{p}}$ by sliding a small ($3 \times 3$) spatial window (with effective receptive field $228$ pixels). At each sliding-window location, $k=9$ proposals (anchor boxes) are generated, associated with $3$ scales and $3$ aspect ratio. In the first phase, the proposals $\mathbf{r}$ is fed into two sibling fully-connected layers: (i) a box-regression layer that has $4k$ outputs representing the coordinates of $k$ boxes, and (ii) a box-classification layer that outputs $2k$ scores to estimate the probability of being an object or not (``objectness" measure). In the second phase, for each object proposal $r_j \in \mathbf{r}$, a region of interest (RoI) pooling layer $\mathcal{P}(\cdot)$ extracts a fixed-length feature vector of size ($7 \times 7 \times 512$) from the convolutional feature map $f_{\mathbf{p}}$ which is fed into a sequence of fully connected (FC) layers to get $f_{\mathbf{r}_{j}} = \mathcal{P}(f_{\mathbf{p}}, {r}_{j})$, where $f_{\mathbf{r}_{j}} \in \mathbb{R}^{512}$ branches into two stream output layer: (i) $\phi_{det}: \mathbb{R}^{512} \rightarrow \mathbb{R}^{N+1}$ that produces softmax probability estimates over $N$ object class plus a catch-all ``background" class for detection, and (ii) $\phi_{reg}: \mathbb{R}^{512} \rightarrow \mathbb{R}^{4}$ that regress the refined bounding-box coordinates for the detected region. \cref{fig: faster-rcnn} presents a schematic of the data flow in our baseline Faster-RCNN \cite{faster-rcnn}. The region proposal network $\mathcal{R}$, and the RoI pool based classification and bounding box regression $\mathcal{P}$ is trained alternatively \cite{faster-rcnn}. During training, $\mathcal{R}$ and $\mathcal{P}$ use $2$- or $N$-class cross-entropy and smooth L1 \cite{fast-rcnn} loss for bounding box regression.

% While seminal, baseline Faster-RCNN \cite{faster-rcnn} has some key limitation: (i) it requires expensive bounding box annotation equally distributed across all $N$-classes. However, due to dataset bias \cite{torralba2011bias} -- Zipf's law \cite{zipf-law} and social conventions \cite{hendricks2018} -- collecting sufficient bounding box annotation for less occurring classes is near impossible. We show how weakly supervised object detection paradigm \cite{OICR} resolves this dependency on requiring bounding box annotation.
% (ii) Once trained on $N$-classes, the model fails to detect $(N+1)^{th}$-class, thus limiting real-world open-set applications. (iii) It lacks the flexibility of detecting only user-specified objects of interest (e.g., detecting only `swans' standing on one leg). Therefore, this demands a further investigation on how to adapt our baseline \cite{faster-rcnn} to overcome the aforementioned limitations.

\subsection{Weakly Supervised Object Detection}\label{sec: WSOD}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{latex/figures/wsddn.pdf}
    \caption{Adapting Faster-RCNN to weakly supervised setup using multiple instance learning paradigm. In particular, we remove the bounding-box regression head $\phi_{\mathrm{reg}}$ and only classify region features $f_{\mathbf{r}}$ into an $N$-class classification.}
    \label{fig:wsddn}
\end{figure}
% jehetu bounding box annotation nei -- ekta image e more than one object thaakte paare -- every image-level label against e amader ekta score chai jeta signify kore oei class er object ta ache ki na
% first stream and second stream -- alada kore na bole -- ek saathe bole debo
To avoid collecting expensive bounding box annotation, weakly supervised object detection (WSOD) trains using image-level labels -- objects of a class label is present or not. Since we avoid using bounding box annotation, we either use a pre-trained region proposal network\footnote{Pre-trained \cite{visualgenome} RPN is highly generalisable to unseen datasets \cite{gu2022open-vocab-OD} due to its generic objective that learns to predict ``objectness" measure.} ($\mathcal{R}$) or a heuristic-based selective search \cite{selective-search} and edge boxes \cite{edgeboxes} to generate box proposals $\mathbf{r} = \{r_1, \dots, r_R\}$. The patch feature $f_{\mathbf{r}} = \mathcal{P}(f_{\mathbf{p}}, \mathbf{r})$ is branched into a classification head $x_c = \phi_{\mathrm{cls}}(f_{\mathbf{r}}) \in \mathbb{R}^{R \times (|\mathcal{C}|+1)}$ and a detection head $x_d = \phi_{\mathrm{det}}(f_{\mathbf{r}}) \in \mathbb{R}^{R \times (|\mathcal{C}|+1)}$. The classification head $\phi_{\mathrm{cls}}$ scores individuals proposals into pre-defined $\mathcal{C}$ classes and a catch-all background class as,
\begin{equation}
    \sigma_{\mathrm{cls}}(x_c^{(i, j)}) = \frac{ \exp(x_c^{(i, j)}) }{ \sum_{k=1}^{|\mathcal{C}|+1} \exp( x_c^{(i, k)}) }
\end{equation}
The detection head $\phi_{\mathrm{det}}$ measures the contribution of each patch $i$ ($r_i \in \mathbf{r}$) of being classified to class $j$ (in $\mathcal{C}+1$) as,
\begin{equation}
    \sigma_{\mathrm{det}}(x_d^{(i, j)}) = \frac{ \exp(x_d^{(i, j)}) }{ \sum_{k=1}^{R} \exp( x_d^{(k, j)}) }
\end{equation}
We train using image-level label $\mathrm{\mathbf{Y}} = [y_0, y_1, \dots, y_{|\mathcal{C}|}]^{T} \in \mathbb{R}^{(|C|+1) \times 1}$, where $y_{c} = 1$ or $0$ indicates with or without class label $c \in \mathcal{C}$. The combined score (element-wise product) of assigning a class score $\sigma_{\mathrm{cls}}$ and a patch score $\sigma_{\mathrm{det}}$ is computed as, $\omega_{0} = \sigma_{\mathrm{cls}}(x_c) \odot \sigma_{\mathrm{det}}(x_d)$. Given $\omega_{0} \in \mathbb{R}^{R \times (|\mathcal{C}|+1)}$, taking a sum over all patches $\hat{y}_{c} = \sum_{i=1}^{R} \omega_{0}^{i, c}$ gives the probability score of $c^{th}$ class. Training happens via multi-class cross entropy as,
\begin{equation}\label{eq: ws}
    \mathcal{L}_{\mathrm{ws}} = - \sum_{c=1}^{|\mathcal{C}|+1} y_{c} \log \hat{y}_c + (1 - y_c) \log (1 - \hat{y}_c)
\end{equation}
Note, unlike SOD, \cref{eq: ws} use only image-level labels that it fails to naively \emph{refine} the initial proposals $\mathbf{r} \in \mathbb{R}^{R \times 4}$. Hence, we use an iterative refinement classifier $\omega_{k} = \phi_{\mathrm{cls}}^{*}(f_{\mathbf{r}})$, where $\omega_{k} \in \mathbb{R}^{R \times (|\mathcal{C}|+1)}$ predicts a refined score of assigning a class label to patches, as shown in \cref{fig:wsddn}. The refinement classifier $\phi_{\mathrm{cls}}^{*}$ is self-supervised using pseudo scores labels $\psi_{k-1}$ from $(k-1)^{th}$ iteration as, (i) first, we compute the patches with highest scores for each class $r_{*}^{c} = \arg \max_{r} (\omega_{k-1}^{(r, c)})$. (ii) Second, all regions $r_i \in \mathbf{r}$ that have a high overlap with a top scoring patch $r_{*}^{c}$ should have the same class label $c$ as, $\psi_{k-1}^{i, c} = 1$ if $\mathrm{IoU}(r_i, r_{*}^{c}) \geq 0.5$. (iii) Third, if a region $r_i \in \mathbf{r}$ do not have a high overlap with any top scoring patch $r_{*}^{c}$, we assign it to background class $\psi_{k-1}^{r, 0} = 1$. (iv) Finally, if a class $c$ is not in image $\mathbf{p}$ we assign $\psi_{k-1}^{r, c} = 0$. We define the refinement loss as,
\begin{equation}\label{eq: ref}
    \mathcal{L}_{\mathrm{ref}}^{k} = \frac{1}{R} \sum_{i=1}^{R} \sum_{c=1}^{|\mathcal{C}|} \omega_{k-1}^{(i, j)} \ \psi_{k-1}^{(i, j)} \ \log \omega_{k}^{(i, j)}
\end{equation}
Although WSOD trains without bounding box annotation, it restricts the model to pre-defined $\mathcal{C}$ class. In the next section, we overcome this limitation of WSOD using prototype learning with sketch-based image retrieval.

% extend WSOD using prototype learning and show how sketch-based image retrieval can train an object detector.

\subsection{Localising Object Regions with Query Sketch}
We extend WSOD from a pre-defined closed-set of $\mathcal{C}$ classes using a normal classifier to the more realistic open-set recognition using prototype-based learning. Instead of mapping patch feature $f_{\mathbf{r}}$ to pre-defined scores dimensions $\mathbb{R}^{R \times 512} \rightarrow \mathbb{R}^{R \times (|\mathcal{C}|+1)}$, we modify the heads $\{\phi_{\mathrm{cls}}, \phi_{\mathrm{det}}, \phi_{\mathrm{cls}}^{*} \}$ (in \cref{sec: WSOD}) to predict embedding vectors $\{e_{\mathrm{cls}}, e_{\mathrm{det}}, e_{\mathrm{cls}}^{*} \}$ as mapping $\mathbb{R}^{R \times 512} \rightarrow \mathbb{R}^{R \times 512}$ respectively. The final scores $\{x_c, x_d, \omega_{k} \}$ are computed via matrix multiplication of embedding vectors $\{e_{\mathrm{cls}}, e_{\mathrm{det}}, e_{\mathrm{cls}}^{*} \}$ with a set support vectors of size $\mathcal{S} = [e_{\mathbf{bg}}, f_{\mathbf{s}}^{1}, f_{\mathbf{s}}^{2}, \dots, f_{\mathbf{s}}^{|\mathcal{C}|}]^{T} \in \mathbb{R}^{512 \times (|\mathcal{C}|+1)}$ representing $\mathcal{C}$ classes and catch-all background class $e_{\mathbf{bg}}$ as,
\begin{equation}\label{eq: prototype}
    x_c = e_{\mathrm{cls}} \cdot \mathcal{S}; \hspace{1em} x_d = e_{\mathrm{det}} \cdot \mathcal{S}; \hspace{1em} \omega_{k} = e_{\mathrm{cls}}^{*} \cdot \mathcal{S}
\end{equation}
% Reformulating WSOD from closed-set (\cref{eq: ws} and \cref{eq: ref}) to open-set using prototype learning (support set $\mathcal{S}$) leads to some amazing emergent properties. 
Inspired by the flexibility of human expression that sketches provide \cite{sketchrnn2018, fscoco}, we compute the support set $\mathcal{S} = [e_{\mathbf{bg}}, f_{\mathbf{s}}^{1}, f_{\mathbf{s}}^{2}, \dots, f_{\mathbf{s}}^{|\mathcal{C}|}]^{T}$ by sampling $|\mathcal{C}|$ sketch queries ($\mathbf{s}$) and computing their feature representation ($f_{\mathbf{s}}$) using a pre-trained sketch encoder $f_{\mathbf{s}}^{i} = \mathcal{F}_{\mathbf{s}}(\mathbf{s}_{i})$. The background vector $e_{\mathbf{bg}}$ learns its own embedding. Carefully choosing this sketch encoder $\mathcal{F}_{\mathbf{s}}$ leads to several properties: (i) pre-training $\mathcal{F}_{\mathbf{s}}$ on category-level SBIR results in a support set $\mathcal{S}$ where image regions $\mathbf{r}$ with same category as query sketches are detected -- category-level object detection. (ii) pre-training $\mathcal{F}_{\mathbf{s}}$ on cross-category FG-SBIR computes a support set $\mathcal{S}$ where only instance-level paired image regions $\mathbf{r}$ are detected for input query sketches -- fine-grained object detection. (iii) Extending fine-grained object detection with a generalisable (out-of-vocab) sketch encoder $\mathcal{F}_{\mathbf{s}}$ helps to detect object parts (e.g., ``head" of a ``horse") given query sketches -- part-level object detection. Using sketch ($\mathcal{F}_{\mathbf{s}}$) and photo ($\mathcal{F}_{\mathbf{p}}$) encoder, pre-trained on SBIR, as teacher we train our object detection model (student) using knowledge distillation as,
\begin{equation}\label{eq:knowledge-distillation}
    \mathcal{L}_{\mathrm{kd}} = \underbrace{\mathcal{L}_{\mathrm{ws}} + \sum_{k=1}^{K} \mathcal{L}_{\mathrm{ref}}^{k}}_{\text{logit distillation } (\mathcal{F}_{\mathbf{s}})} + \lambda \underbrace{ ||f_{\mathbf{r}} - \mathcal{F}_{\mathbf{p}}( \texttt{Crop}(\mathbf{p}, \mathbf{r}))||_{1}}_{\text{feature distillation } (\mathcal{F}_{\mathbf{p}})}
\end{equation}
where the hyperparameter $\lambda=1$. Although, in theory, a good sketch ($\mathcal{F}_{\mathbf{s}}$) and photo ($\mathcal{F}_{\mathbf{p}}$) encoder can train an object detector in WSOD setup via prototype learning in \cref{eq:knowledge-distillation}, the challenge boils down to learning a generalised (out-of-vocab) SBIR enabling category-level or fine-grained sketch/photo matching under wide variations like illumination, complex background, occlusions, etc. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/sketchOD.pdf}
    \caption{Caption}
    \label{fig:proposed}
\end{figure}

\subsection{Prompt Learning for Generalised SBIR}\label{sec: prompt-sketch}
First, we recap a baseline SBIR framework. Given a sketch/photo pair ($\mathbf{s}, \mathbf{p}$), we use a sketch/photo feature extractor to get the feature map $f_{\mathbf{s}} = \mathcal{F}_{\mathbf{s}}(\mathbf{s}) \in \mathbb{R}^{512}$ and $f_{\mathbf{p}} = \mathcal{F}_{\mathbf{p}}(\mathbf{p}) \in \mathbb{R}^{512}$. For category-level SBIR, the pair ($\mathbf{s}, \mathbf{p}$) belong to the same category, whereas fine-grained SBIR requires instance-level sketch/photo matching. For training, the cosine distance $\delta(\cdot, \cdot)$ to a sketch anchor ($\mathbf{s}$) from a negative photo ($\mathbf{p}^{-}$), denoted as $\beta^{-} = \delta(f_{\mathbf{s}}, f_{\mathbf{p}^{-}})$ should increase while that from the positive photo ($\mathbf{p}^{+}$), $\beta^{+} = \delta(f_{\mathbf{s}}, f_{\mathbf{p}^{+}})$ should decrease. Training is done via triplet loss with hyperparameter $\mu>0$ as,
\begin{equation}\label{eq: triplet}
    \mathcal{L}_{\mathrm{trip}} = \max\{0, \mu + \beta^{+} - \beta^{-} \}
\end{equation}
To extend FG-SBIR for multiple categories (cross-category FG-SBIR), we train the network using \cref{eq: triplet} with ``hard-triplets" -- where $(\mathbf{s}, \mathbf{p}^{+}, \mathbf{p}^{-})$ come from the same category, and a class discriminator loss to help separate between categories in multi-category FG-SBIR as,
\begin{equation}\label{eq: category}
    \mathcal{L}_{\mathrm{cat}} = -c_{\mathbf{q}}^{i} \log \frac{\exp(\mathcal{F}_{\mathbf{c}}(f_{\mathbf{q}}^{i}))}{\sum_{\forall j} \exp(\mathcal{F}_{\mathbf{c}}(f_{\mathbf{q}}^{j}))}
\end{equation}
where, query $\mathbf{q} = \{ \mathbf{s}, \mathbf{p} \}$, $c_{\mathbf{q}}^{i}$ represent the class label $c \in \mathcal{C}$ of $i^{th}$ sample in query $\mathbf{q}$, and $\mathcal{F}_{\mathbf{c}}: \mathbb{R}^{512} \rightarrow \mathbb{R}^{|\mathcal{C}|}$.

Although seminal, training a \emph{good enough} SBIR is essential to handle the wide variations in illuminations, complex background, occlusions, and unseen categories encountered by object detectors. Hence, to endow SBIR with high generalisation and open-vocab capabilities, we introduce  prompt learning \cite{zhou2022visualprompt} using CLIP \cite{CLIP} for SBIR (both category-level and cross-category fine-grained). CLIP \cite{CLIP} consists an image and text encoder (e.g., ViT \cite{ViT}, or ResNet \cite{he2015resnet}) trained on large $400M$ image/text pairs. This leads to a highly generalisable model that works zero-shot across multiple tasks and datasets. However, adapting CLIP for sketches is tricky since naive fine-tuning leads to model collapse. Hence, we use prompt learning, a set of $P$ learnable vector $\mathbf{v}_{\mathbf{s}} \in \mathbb{R}^{P \times 768}$ for sketch and $\mathbf{v}_{\mathbf{p}} \in \mathbb{R}^{P \times 768}$ for photo, injected into the first layer of ViT to induce CLIP to learn downstream sketch/photo distribution. Importantly, prompting CLIP has shown to preserve the desired generalisation ability \cite{zhou2022visualprompt} since the knowledge learned by CLIP is distilled into prompt's weights while keeping the ViT weights frozen. Our new sketch encoder is defined by adapting CLIP's image encoder using sketch prompt ($\mathbf{v}_{\mathbf{s}}$) as, $\mathcal{F}_{\mathbf{s}}(\cdot) = \mathcal{F}_{\mathrm{clip}}(\cdot, \mathbf{v}_{\mathbf{s}})$ and using $\mathbf{v}_{\mathbf{p}}$ for photo encoder as, $\mathcal{F}_{\mathbf{p}}(\cdot) = \mathcal{F}_{\mathrm{clip}}(\cdot, \mathbf{v}_{\mathbf{p}})$. Since ViT weights are frozen, training our CLIP-based SBIR is parameter-efficient -- we train only $\mathbf{v}_{\mathbf{s}} \in \mathbb{R}^{P \times 768}$ and $\mathbf{v}_{\mathbf{p}} \in \mathbb{R}^{P \times 768}$. This allows training with less data, and faster convergence. For category-level SBIR, ($\mathbf{v}_{\mathbf{s}}, \mathbf{v}_{\mathbf{p}}$) learns category-level inducing prompts using \cref{eq: triplet}. Learning cross-category FG-SBIR, is slightly more complicated that trains ($\mathbf{v}_{\mathbf{s}}, \mathbf{v}_{\mathbf{p}}$) using hard-triplet in \cref{eq: triplet}, and a modified class discriminative loss \cref{eq: category},
\begin{equation}
    \mathcal{L}_{\mathrm{cat}} = -c_{\mathbf{q}}^{i} \log \frac{\exp(f_{\mathbf{q}}^{i} \cdot f_{\mathbf{t}}^{i})}{\sum_{\forall j} \exp(f_{\mathbf{q}}^{i} \cdot f_{\mathbf{t}}^{j})}
\end{equation}
where, $f_{\mathbf{t}}^{i} \in \mathbb{R}^{512}$ is computed by CLIP's text encoder as, $f_{\mathbf{t}}^{i} = \mathcal{F}_{\mathrm{clip}}^{(\mathbf{t})}(``\texttt{a photo of a }[c_{\mathbf{q}}^{i}]")$ for category $c_{\mathbf{q}}^{i}$. Equipped with our novel prompt-based SBIR, we can train category-level object detection, fine-grained object detection, or part-level object detection by utilising the high generalisibility \cite{zhou2022visualprompt} of prompt learning with CLIP.

\subsection{Bridging Object-Level and Image-Level}\label{sec: object-scene}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth, height=0.3\linewidth]{latex/figures/synth-data-sample.pdf}
    \caption{Synthetic scene-level photos created by stitching $\{k^{2} \ | \ k=1, \dots 5\}$ object-level image patches used for training.}
    \label{fig:data-augmentation}
\end{figure}

Although a combination of logit distillation via sketch encoder ($\mathcal{F}_{\mathbf{s}}$), and feature distillation via photo encoder ($\mathcal{F}_{\mathbf{p}}$), could train an object detector, we still lack a crucial puzzle piece -- the training data for object detector. A naive approach may use photos from the train-set in downstream object detection datasets like SketchyCOCO \cite{sketchycoco2020} (used during evaluation) and sketched object regions. However, it defeats our motivation for real-world application where (i) we need to collect costly annotations by drawing sketches for object regions, (ii) fails to measure real-world performance on unseen datasets (i.e., cross-dataset setup \cite{bhunia2022adaptive}). Keeping real-world application in mind, we propose a novel training setup -- Extremely Weakly Supervised Object Detection (EWSOD) that avoids using expensive bounding box annotation and training on data distribution used in evaluations.

To solve this acute ``lack" of annotations, we synthesise our training data using inspirations from popular data augmentation schemes like CutMix \cite{cutmix}. In particular, we use our object-level SBIR datasets (category or fine-grained) to sample $k=\{1, \dots, 25\}$ sketch/photo pairs. While the sketches are encoded via sketch encoder ($\mathcal{F}_{\mathbf{s}}$) to generate the support set $\mathcal{S} = \{e_{\mathbf{bg}}, f_{\mathbf{s}}^{1}, \dots, f_{\mathbf{s}}^{k} \}$, with background embedding $e_{\mathbf{bg}}$, we use the sample photos to randomly tile a canvas of size ($H \times W$), as shown in \cref{fig:data-augmentation}. Such data augmentation schemes has several benefits: (i) trains an object detector using only SBIR datasets \cite{sketchy}, without the need of ``seeing" test-set data distribution to evaluate out-of-distribution generalisation (EWSOD), (ii) makes the model robustness against input corruptions and improve out-of-distribution, especially for object detection \cite{cutmix, mixup}.

\section{Experiments}

\keypoint{Dataset} We train our object detection network using existing cross-category FG-SBIR dataset -- Sketchy \cite{sketchy} that contains $125$ categories, each with $100$ photos. Each photo has at least $5$ sketches with fine-grained associations. To evaluate fine-grained object detection, we use SketchyCOCO \cite{sketchycoco2020} comprising of natural images in MS-COCO \cite{mscoco} with paired scene sketches. Following Liu \etal \cite{liu2020scenesketcher}, we select on $1,225$ sketch/photo pairs from SketchyCOCO \cite{sketchycoco2020} with at least one foreground sketched object. Since scene sketches in SketchyCOCO are generated from object sketches in Sketchy \cite{sketchy}, we filter the overlapping categories from Sketchy \cite{sketchy} to prevent information leak and measure zero-shot potential of CLIP-based fine-grained object detection to unseen categories. We also compare with the popular ``toy" \footnote{The potential of sketch is revealed when specifying fine-grained visual cues. For  category-level information, using text is an easier alternative.} setup of category-level sketch-guided object detection \cite{tripathi2020object}. For this, we train SBIR (used to supervise object detection) on category-level sketch/photo pairs in QuickDraw-Extended \cite{doodle-to-search} with $330k$ sketches and $204k$ photos from $110$ categories ($80$ for training and $30$ for testing). Following \cite{tripathi2020object}, we evaluate on subset of standard object detection benchmark MS-COCO \cite{mscoco} and PASCAL-VOC \cite{pascalVOC} that have {$56$} and {$20$} common categories with QuickDraw \cite{sketchrnn2018} respectively. Note, we do not use any bounding-box annotation during training.

% We use the FG-SBIR dataset Sketchy \cite{sketchy} to train our object detection models. Sketchy \cite{sketchy} comprise of 

% SketchyCOCO, SketchyScene for instance-level || QuickDraw-Scene, TU-Berline-Scene for category-level || QuickDraw-Part * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\

\keypoint{Implementation Details} Our model is implemented in PyTorch on a 11GB Nvidia RTX 2080-Ti GPU. First, we train a generalised cross-category FG-SBIR with image size ($224 \times 224$) by adapting CLIP via prompt learning \cite{VPT}. The prompts are trained with triplet loss \cite{yu2016shoe},  margin $\mu=0.3$, Adam optimiser with learning rate $0.0001$ for $60$ epochs, and batch size $64$. Our object detection pipeline is build using Detectron2 \cite{detectron2}. We use FasterRCNN \cite{faster-rcnn}, pretrained on Visual Genome \cite{visualgenome} and remove the RoIPooling \cite{fast-rcnn} and subsequent layers to keep only the pretrained backbone ResNet+FPN ($\mathcal{F}$) \cite{he2015resnet, lin2017FPN} and Region Proposal Network ($\mathcal{R}$) that generates $1000$ proposals. The model trains using SDG with batch size $8$ and initial learning rate $0.005$, multiplied by $0.1$ at $150k$ and $250k$ iterations. We train in a two-step process: (i) keeping $\mathcal{F}$ and $\mathcal{R}$ fixed, we train the newly added RoI pooling followed by FC layers ($\mathcal{P}$), classification head $\phi_{cls}$, detection head $\phi_{det}$, and refinement head $\phi_{cls}^{*}$ for $240k$ iterations. (ii) We freeze $\mathcal{R}$ and finetune the rest for $80k$ iterations. Non-maxima suppression with IoU $\geq 0.3$ is applied to get final predictions. During training, we input a canvas $\mathcal{C}$ combining $\{k^{2} \ | \ k=[1, \dots, 5] \}$ object-level photos from existing sketch/photo datasets \cite{sketchy, sketchrnn2018} with paired sketch as image-level annotations.

\keypoint{Evaluation Metric}  (i) For fine-grained object detection, we use $AP_{.5}$, $AP_{.7}$, and $mAP$ that computes the average precision (AP) at $0.5$, $0.7$, and average of $10$ intersection-over-union (IoU) steps $\{0, \dots, 1\}$ of the detected boxes with ground truth respectively. (ii) For category-level object detection, \emph{CorLoc} measure percentage of images for which the most confident predicted box has IoU $\geq 0.5$ with at least one of the ground-truth boxes for every class, and (iii) For cross-category FG-SBIR, we measures $Acc.@q$ -- percentage of sketches having true matched photo in the top-q list, and (iv) mean average precision ($mAP$), and precision condering top $200$ retrievals $P@200$ for category-level SBIR.

\subsection{Competitors}  
For object detection, we compare against, (i) supervised object detection (SOD) using both bounding box and sketch/photo annotations: \textbf{Mod-FRCNN} adapts Faster-RCNN \cite{faster-rcnn} for unseen class by concatenating query sketch feature with the RoI pooled feature and passed through a binary classifier for detection. \textbf{MatchNet} \cite{matchnet2019} extends \emph{Mod-FRCNN} using co-attention to generate region proposals conditioned on query sketch followed by squeeze-and-co-excitation to adaptively re-weight importance distribution of candidate proposals. \textbf{CoAttOD} \cite{tripathi2020object} improves upon \emph{MatchNet} by mitigating the sketch/photo domain misalignment using cross-modal attention. (ii) Weakly supervised object detection (WSOD) trains only on image-level sketch annotations without using additional bounding boxes: \textbf{WSDDN} \cite{wsddn} repurposed object detection as a region classification via multiple instance learning (MIL) paradigm. To inject query sketch, we use cross-attention with RoI pooled feature followed by a binary classifier for detection. \textbf{OICR} \cite{OICR} improves \emph{WSDDN} using an iterative MIL to refine initial predictions that improve discriminatory power of the classifier. \textbf{PCL} \cite{pcl2018} generates multiple positive instance in an image via clustering and assigning proposals the label of corresponding object class for each cluster. \textbf{ICMWSD} \cite{ren2020WSOD} addresses the problem of prior WSOD that focus on the most discriminative part of an object using context information. In particular, \emph{ICMWSD} obtains a ``dropped feature" by dropping the most discriminative parts, followed by maximising the loss of the ``dropped feature" that force the network to look at the surrounding context regions. (iii) To evaluate real-world performance on unseen image distribution (i.e., unseen dataset), we introduce a new paradigm -- extreme weakly supervised object detection (EWSOD). We adapt \textbf{$<$Method$>$} in \emph{WS-SOTA} by replacing selective search with RPN (pretrained on Visual Genome \cite{visualgenome}) to generate proposals and use object-level sketch/photo pairs (see \cref{sec: object-scene}), to evaluate on SketchyCOCO \cite{sketchycoco2020}, MS-COCO \cite{mscoco}, and PASCAL-VOC \cite{pascalVOC}, as \textbf{E-$<$Method$>$}. 

For zero-shot category-level SBIR, we compare against: \textbf{ZS-GRL} \cite{doodle-to-search} combines similar semantic information (word2vec \cite{word2vec2013}) of class labels with visual sketch information and trains over a gradient reversal layer \cite{grl2015} to reduce sketch/photo domain gap. \textbf{ZS-STT} \cite{sain2022sketch3t} adapts to unseen categories and data distribution using a slow test-time training. Although test-time training for every new category limits large-scale applications, \emph{ZS-STT} provides a good ``upper-bound" performance. For zero-shot cross-category FG-SBIR: \textbf{Cross-DG} is a SOTA domain generalisation method \cite{shankar2018} adapted to cross-category FG-SBIR \cite{pang2019generalising} using categories as domain and inter-category sketch/photo pairs as label. \textbf{CC-DG} \cite{pang2019generalising} models a universal manifold of prototypical visual sketch traits that dynamically embeds sketch/photo, to generalise to unseen categories. 

\subsection{Generalisibility of Cross-Category FG-SBIR} 
Our object detector is trained in \emph{EWSOD} setup (without using bounding box annotation, or test data distribution) using a cross-category FG-SBIR as teacher. Hence, the accuracy of teacher puts a bottleneck on the student's object detection performance. \cref{tab:generalised-SBIR} compares category-level SBIR (CL-SBIR) and cross-category FG-SBIR (CC-FGSBIR) on QuickDraw-Extended \cite{doodle-to-search} and Sketchy \cite{sketchy} respectively, with $70\%$ of all categories are used for training. To further evaluate zero-shot generalisation, we reduce the training data to $50\%$, and $30\%$ of all categories.

\keypoint{Performance Analysis} From \cref{tab:generalised-SBIR} we make the following observations: (i) with decreasing train (seen) categories, performance drops in both CL-SBIR and CC-FGSBIR. However, while \emph{ZS-GRL} and \emph{Cross-DG} drops by $xx.x\%$ and $xx.x\%$ ($70\% \rightarrow 30\%$) on CL-SBIR and CC-FGSBIR, our proposed method drops by $xx.x\%$ and $xx.x\%$ respectively. This is due to the greater zero-shot potential when using prompt-based CLIP models for sketch/photo matching. (ii) Performance drop in CC-FGSBIR is more significant $xx.x\%$ as compared to CL-SBIR $xx.x\%$. Hence, it is more difficult to discriminate unseen inter-category sketch/photo pairs than recognise a novel category. This is since CLIP was pre-trained on scene-level image/text pairs that holds greater semantic information, necessary for category-level retrieval, compared to structural information required for fine-grained matching. (iii) Performance of all competitors in CL-SBIR and CC-FGSBIR are staggeringly inferior to proposed CLIP-based approach. Such a strong SBIR is necessary to unlock training object detection in EWSOD setup.


{
\setlength{\tabcolsep}{0.6em}
\begin{table}[]
    \centering
    \footnotesize
    \caption{Quantitative performance of zero-shot category-level SBIR (CL-SBIR) and cross-category FG-SBIR (CC-FGSBIR). We compare Proposed CLIP-based SBIR method with zero-shot category-level SBIR methods like Doodle2Search (D2S) \cite{doodle-to-search}, SketchTT (STT) \cite{sain2022sketch3t}, and cross-category FG-SBIR methods like CrossGrad (CG) \cite{pang2019generalising}, CC-DG (CC) \cite{pang2019generalising}.}
    \vspace{-1em}
    \begin{tabular}{ccccccc}
        \toprule
        \multirow{2}{*}{\makecell{Train\\Categories}} &  & \multicolumn{2}{c}{CL-SBIR} & & \multicolumn{2}{c}{CC-FGSBIR} \\
         &  & mAP & P@200 & & Acc.@1 & Acc.@5 \\\hline
        \multirow{2}{*}{$70\%$} & D2S & xx.x & xx.x & CG & xx.x & xx.x \\
         & STT & xx.x & xx.x & CC & xx.x & xx.x \\
        \rowcolor{Gray}
        \multicolumn{1}{c}{\textbf{Proposed}} & & xx.x & xx.x & & xx.x & xx.x \\\hline
        \multirow{2}{*}{$50\%$} & D2S & xx.x & xx.x & CG & xx.x & xx.x \\
         & STT & xx.x & xx.x & CC & xx.x & xx.x \\
        \rowcolor{Gray}
        \multicolumn{1}{c}{\textbf{Proposed}} & & xx.x & xx.x & & xx.x & xx.x \\\hline
        \multirow{2}{*}{$30\%$} & D2S & xx.x & xx.x & CG & xx.x & xx.x \\
         & STT & xx.x & xx.x & CC & xx.x & xx.x \\
         \rowcolor{Gray}
        \multicolumn{1}{c}{\textbf{Proposed}} & & xx.x & xx.x & & xx.x & xx.x \\\bottomrule
    \end{tabular}
    \label{tab:generalised-SBIR}
    \vspace{-1em}
\end{table}
}


\subsection{Category-Level Object Detection}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth, height=0.2\linewidth]{example-image-a}
    \vspace{-1em}
    
    \caption{Qualitative retrieval results for cross-category FG-SBIR.}
    \label{fig: retrieval-cross-category}
\end{figure}

We benchmark on subset of standard object detection MS-COCO \cite{mscoco} and PASCAL-VOC \cite{pascalVOC} datasets having overlapping sketch categories in QuickDraw \cite{sketchrnn2018} for category-level matching. Unlike traditional object detection that detect all instances and classes in an image, category-level object detection should detect only instances of a specific class specified by the query sketch.

\keypoint{Performance Analysis} From \cref{tab: detection-category} we observe: (i) SOD methods outperform WSOD by an average margin of $xx.x\%$. This is due to use of both bounding-box and sketch/photo annotation. (ii) Not using data distribution encountered during evaluation (EWSOD) further drops performance by $xx.x\%$ from WSOD. This highlights lack of generalisation of object detectors to shift in data distribution, thereby limiting practical deployment. (iii) Our proposed method, in spite of being trained on the challenging EWSOD setup, outperforms competitors in both EWSOD and WSOD to reach performance close to SOD -- only $xx.x\%$ gap with SOD. This shows extreme potential of training object detection with a strong CL-SBIR.

{
\setlength{\tabcolsep}{0.8em}
\begin{table}[]
 \centering
 \footnotesize
 \caption{VOC 2007 and MS-COCO detection $AP_{50}$ and \emph{CorLoc}}
 \begin{tabular}{clcccc}
     \toprule
     \multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c}{VOC 2007 \cite{pascalVOC}} & \multicolumn{2}{c}{MS-COCO \cite{mscoco}} \\
      & & $AP_{.5}$ & CorLoc & $AP_{.5}$ & CorLoc \\\hline
     \multirow{3}{*}{\rotatebox{90}{\textbf{SOD}}} & Mod-FRCNN & xx.x & xx.x & xx.x & xx.x \\
      & MatchNet & xx.x & xx.x & xx.x & xx.x \\
      & CoAttOD & xx.x & xx.x & xx.x & xx.x \\\hline
     \multirow{4}{*}{\rotatebox{90}{\textbf{WSOD}}} & WSDDN & xx.x & xx.x & xx.x & xx.x \\
      & OICR & xx.x & xx.x & xx.x & xx.x \\
      & PCL & xx.x & xx.x & xx.x & xx.x \\
      & ICMWSD & xx.x & xx.x & xx.x & xx.x \\\hline
     \multirow{4}{*}{\rotatebox{90}{\textbf{EWSOD}}} & E-WSDDN & xx.x & xx.x & xx.x & xx.x \\
      & E-OICR & xx.x & xx.x & xx.x & xx.x \\
      & E-PCL & xx.x & xx.x & xx.x & xx.x \\
      & E-ICMWSD & xx.x & xx.x & xx.x & xx.x \\
     \rowcolor{Gray}
      & {\textbf{Proposed}} & xx.x & xx.x & xx.x & xx.x \\\bottomrule
 \end{tabular}
 \label{tab: detection-category}
\end{table}
}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/category-levelOD.pdf}
    \caption{Category-Level Object Detection using query sketches with images from MS-COCO \cite{mscoco} and PASCAL-VOC \cite{pascalVOC}.}
    \label{fig: detection-category}
\end{figure}
 
\subsection{Fine-Grained Object Detection}
The real potential of sketch is unlocked when describing fine-grained visual cues. Unlike traditional object detection that detects all instances of all classes, or category-level object detection that detects all instance of a specified class, the goal of fine-grained object detection is to detect only a specific instance for the input query sketch. Due to instance-specific detection requirement, we do not use \emph{CorLoc} metric that computes over all instance in a particular class.

\keypoint{Performance Analysis} From \cref{tab: detection-finegrained} we observe: (i) SOD methods outperform WSOD and EWSOD approaches, similar to category-level object detection in \cref{tab: detection-category}. (ii) Compared to SOD, the performance of WSOD drops more for $AP_{.5} \rightarrow AP_{.7}$. This is since WSOD methods use less accurate selective search \cite{selective-search} and edge boxes \cite{edgeboxes} for region proposals compared to the more accurate RPN \cite{faster-rcnn} in SOD. (iii) Although our proposed method trained on EWSOD setup performs lower than SOD, the performance gap $xx.x\%$ is lower for fine-grained detection as compared to $xx.x\%$ in category-level detection. This shows our proposed method has better discriminibility via triplet loss, compared to competitors that injects query sketch via cross-modal attention followed by binary classification.
 
{
\setlength{\tabcolsep}{1.2em}
\begin{table}[]
    \centering
    \footnotesize
    \caption{SketchyCOCO detection fine-grained}
    \begin{tabular}{clccc}
        \toprule
        & Method & $AP_{.5}$ & $AP_{.7}$ & mAP \\\hline
         \multirow{3}{*}{\rotatebox{90}{\textbf{SOD}}} & Mod-FRCNN & xx.x & xx.x & xx.x \\
          & MatchNet & xx.x & xx.x & xx.x \\
          & CoAttOD & xx.x & xx.x & xx.x \\\hline
         \multirow{4}{*}{\rotatebox{90}{\textbf{WSOD}}} & WSDDN & xx.x & xx.x & xx.x \\
          & OICR & xx.x & xx.x & xx.x \\
          & PCL & xx.x & xx.x & xx.x \\
          & ICMWSD & xx.x & xx.x & xx.x \\\hline
         \multirow{4}{*}{\rotatebox{90}{\textbf{EWSOD}}} & E-WSDDN & xx.x & xx.x & xx.x \\
          & E-OICR & xx.x & xx.x & xx.x \\
          & E-PCL & xx.x & xx.x & xx.x \\
          & E-ICMWSD & xx.x & xx.x & xx.x \\
         \rowcolor{Gray}
          & {\textbf{Proposed}} & xx.x & xx.x & xx.x \\\bottomrule
    \end{tabular}
    \label{tab: detection-finegrained}
\end{table}
}
 
 \begin{figure}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/fine-grainedOD.pdf}
    \caption{Cross-Category Fine-Grained Object Detection using query sketches with images from SketchyCOCO \cite{sketchycoco2020}.}
    \label{fig: detection-finegrained}
\end{figure}
 
\subsection{Part-Level Object Detection}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth, height=0.5\linewidth]{latex/figures/part-levelOD.pdf}
    \caption{Unlike traditional object detection that detects an entire object (e.g., a ``cat"), sketches can specify fine-grained region-of-interest to detect a part of an object (e.g., the ``head" of a ``cat").}
    \label{fig: detection-part}
\end{figure}

Encouraged with the discriminative power of the proposed method in \cref{tab: detection-finegrained}, we go a step further and ask: Apart from uniquely detecting an instance, can we only detect a \emph{part} (e.g., only `head') of the instance? A large-scale quantitative evaluation of part-level object detection requires appropriate part-level annotation for both sketch and photo. However, existing fine-grained sketch/photo datasets like SketchyCOCO \cite{sketchycoco2020} lacks such dense annotation. Nonetheless, we conduct a small-scale qualitative study by manually editing sketches to create sketches of a single part. \cref{fig: detection-part} shows (i) our proposed method can uniquely detect the sketched `head' region of different objects. (ii) Detection performance is lower for ambiguous part sketches like `leg' (e.g., front-leg, back-leg etc.) (iii) Since detection depends on region proposals from RPN, our model fails to detect tiny sketched parts. Tiny object detection \cite{lee2022tinyOD} is a known challenge for traditional object detection \cite{faster-rcnn}. We hope future works on fine-grained object detection can resolve this by guiding the RPN towards sketched regions.
 
\subsection{Ablation}
\keypoint{Selective Search v/s Edge Boxes v/s RPN} Our proposed method use RPN, pre-trained on Visual Genome \cite{visualgenome} dataset, to generate $1000$ object proposals. However, WSOD methods primarily use selective search \cite{selective-search} or edge boxes \cite{edgeboxes} that do not require pre-training using bounding box annotation from Visual Genome \cite{visualgenome}. Hence, for a fair comparison with WSOD, we replace our RPN with selective search (\textbf{Ours-SS}), or edge boxes (\textbf{Ours-EB}) to generate $2000$ proposals. In spite of generating more proposals ($2000$ instead of $1000$ for RPN) and thereby slower inference, our proposed method still outperforms \emph{Ours-SS} by $xx.x\%$ and \emph{Ours-EB} by $xx.x\%$ on $AP_{.5}$ for fine-grained object detection.
 
\keypoint{Influence of Classifier Refinement} Similar to \emph{OICR} \cite{OICR}, we refine our initial predictions by $\phi_{cls}$ and $\phi_{det}$ with an iterative refinement head $\phi^{*}_{cls}$ for $K$ steps. Similar to \cite{OICR}, we observe $AP_{.5}$ to improve by $xx.x\%$ and $xx.x\%$ for $K=1 \rightarrow 2$ and $K=2 \rightarrow 3$ respectively. However, we notice a small drop of $xx.x\%$ for $K=3 \rightarrow 4$. Such drop with increasing iteration is typical for iterative approaches \cite{iterativetext2021}.

\keypoint{Influence of $\mathcal{L}_{kd}$ from CLIP's Photo Encoder} Distilling from CLIP's image encoder via feature distillation \cite{heofeaturedistillation2019} to the output from RoI pooled feature $f_{\mathbf{r}}$ is computationally expensive process. Each region proposal from RPN is used to crop an image region followed by encoding it via CLIP's image encoder. Hence, during training given an input image, we generate $1000$ proposals that requires passing $1000$ cropped image regions from CLIP's encoder to compute distillation loss $\mathcal{L}_{kd}$. Given the expensive compute nature of $\mathcal{L}_{kd}$ we ask, is it worth it? Removing $\mathcal{L}_{kd}$ from our proposed method leads to $xx.x\%$ drop in $AP_{.5}$. However, increasing our training iterations from $320k$ to $450k$ reduces this drop to $xx.x\%$. Hence, while $\mathcal{L}_{kd}$ helps in faster and better training, removing it can still train an objection pipeline.
 
\subsection{Limitation and Future Works} 
Introducing fine-grained object detection using sketch opens several possibilities that we do not consider. Our proposed method essentially matches generated proposals with input query sketches. We assume that each sketch represent a single object or category. However, a user might be interested in detecting complex regions (a ``dog" on the right of a ``person") with \emph{multiple} objects that have meaningful \emph{spatial} alignment. Future works can extend fine-grained object detection or semantic segmentation to complex sketches using the recently introduced FS-COCO \cite{fscoco} dataset.
 
\subsection{Conclusion}
In this work, we introduce a new paradigm to object detection -- instead of detecting all regions, how sketches can help detect specified fine-grained region of interest. To further study the practical application, we impose strict constraint on training data -- (i) without collecting additional bounding box annotation for downstream data distribution and relying on existing sketch/photo pair datasets (ii) without having access to unlabelled test-time data distribution used for evaluation. We exploit the popular generalisation potential of CLIP and extend it to sketch/photo matching using prompt learning to learn a state-of-the-art category-level SBIR and cross-category FG-SBIR models. En-armed with this new SOTA sketch/photo matching network as teacher, we train a student object detection framework. The resulting method closes in the performance gap with supervised counterpart, showing the potential of sketch for object detection.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
