% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bbold}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Weakly Supervised Object Detection using Generalise Cross-Category Fine-Grained Sketch Based Image Retrieval}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth, height=0.5\linewidth]{example-image-a}
    \caption{Explain the Problem Setup we aim to solve}
    \label{fig:problem-setup}
\end{figure}

* \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\

\section{Related Works}
\label{sec:related-works}

\noindent \textbf{Sketch for Visual Understanding:}
Hand-drawn sketches serve as a useful query modality for visual understanding tasks that involve human perception and structural cues. Sketches not only convey a visual description \cite{hertzmann2020perception} but also exhibit artistic styles \cite{zhang2021smartshadow}. This makes sketch a vital querying modality for the creative industry like artistic image editing \cite{yang2020surgery} and animation \cite{xing2015autocomplete}. Unlike photos that are passively captured by a camera, sketches are actively drawn by humans which makes them a good visual representation \cite{pixelor} enriched with human participation. Apart from the widely explored sketch-based image retrieval \cite{bhunia2021semi, bhunia2020sketch, pang2019generalising, styleMeUp, sketchformer, liveSketch}, sketch as a query has shown potential in several vision understanding tasks like incremental learning \cite{bhunia2022incremental} image and video synthesis \cite{cusuh2022synthesis, DeepFaceVideoEditing2022}, representation learning \cite{alaniz2022primitives, clipasso}, image-inpainting \cite{xie2021inpainting}, 3D shape retrieval \cite{xu20223Dretrieval}, 3D shape modeling \cite{chowdhury20223Dsynthesis}, medical image analysis \cite{wang2022medical}, object localisation \cite{tripathi2020object, riba2021object} and segmentation \cite{hu2020segmentation, qi2022segmentation}.

While exploring sketch as a query for object detection, by Tripathi \etal \cite{tripathi2020object}, several limitations surfaced with respect to problem definition as well as architectural designs. 
Firstly, instead of fine-grained matching, sketch was used to specify object category (easier via text/keyword \cite{CLIP, gu2022open-vocab-OD}), thus wasting the potential of sketch as a query in modeling fine-grained details.
% Primarily, potential of sketch as a query in modeling fine-grained details remains unused, as specifying object category via sketching is quite laborious \cite{bhunia2020sketch}, compared to a text/keyword \cite{CLIP, gu2022open-vocab-OD}.
Secondly, it requires both bounding-box and object-sketch annotation which increases annotation budget without significant improvement in performance over traditional object detection setup. 
% \textcolor{red}{Secondly, as specifying object category by sketching is laborious \cite{bhunia2020sketch}, compared to a text/keyword \cite{CLIP, gu2022open-vocab-OD}, the potential of sketch in modeling fine-grained details remains unused.}
% thus overlooking the potential of sketch in modeling fine-grained details -- a \emph{fine-grained query}. 
% sketch as a query prides in its ability to model fine-grained details \cite{styleMeUp, bhunia2022worrying, bhunia2020sketch}. 
Thirdly, due to an expensive bounding box and sketch annotation, only fewer than $50\%$ object categories in existing object detection datasets \cite{pascalVOC, mscoco} are available for training. Finally, using an early fusion strategy \cite{xu2022MML} of sketch with object detection results in recomputing object regions for each new sketch -- leading to a slower detection framework with increasing query sketches.
% \cite{tripathi2020object} employs an early fusion strategy, that uses query-sketch as a conditional input to predict object regions, per sketch-photo pair -- resulting in a slower paradigm as sketch categories increase.
In this paper, we propose a fine-grained sketch-based object detection framework that use only object-level sketch photo pairs without any bounding-box annotations for training, and is scalable with multiple fine-grained query sketches, even under zero-shot setup.

\noindent \textbf{Supervised Object Detection:} 
Object detection jointly localise and identify objects in an image. Traditional object detectors rely on large supervised object detection datasets such as PASCAL VOC \cite{pascalVOC} and MS-COCO \cite{mscoco}, containing thousands of examples per object category which are quite time-consuming to annotate, unlike our pipeline, which leverages sketch-photo pairs as object annotations. 
% have multitudes\cut{over hundreds and thousand} of time-consuming annotated examples per object category. 
Existing literature on object detection is bifurcated as: (i) fast yet less accurate single-shot \cite{yolo, yolov3, SSD, lin2017focal-loss, centerNet}; (ii) slow but more accurate two-stage object detectors \cite{rcnn, fast-rcnn, faster-rcnn, maskrcnn}. To fully exploit the fine-grained cues provided by sketch, our proposed method aligns with the two-stage detectors. Two-stage detectors predict object regions using selective search in RCNN \cite{rcnn}, ROI pooling \cite{fast-rcnn} in Fast-RCNN, and Region Proposal Network (RPN) with ROIAlign in Faster-RCNN \cite{faster-rcnn}. While there has been several attempts with sophisticated architectural modifications   \cite{law2018cornerNet, zhou2019grouping, centerNet}, Faster-RCNN \cite{faster-rcnn} still acts a a fundamental building block for several downstream tasks like scene-graph generation \cite{yang2018scenegraph}, visual grounding \cite{mouzenidis2021grounding}, and relationship prediction \cite{zhu2018relationship}. Therefore, we resort to the more traditional Faster-RCNN based two-stage pipeline.


\noindent \textbf{Weakly Supervised Object Detection (WSOD):} Collecting bounding box annotation per object category is already a time-consuming process, which is aggravated even further by Fine-grained object detection (e.g., recognising animal species). Aiming to overcome this, existing WSOD methods adopt two schools of thoughts: (a) formulate this task as a multiple instance learning (MIL) \cite{dietterich1997MIL, wsddn, li2016wsod, diba2017wsod, jie2017wsod, zhang2018wsod, tang2018wsod, shen2019wsod} problem that interpret an image as a bag of proposals or regions. The image is labeled positive, when one of the regions tightly contains the object of interest, otherwise negative if no region has it.
% whereas a negative would imply no region has it.
% {If the image is labeled negative, no region contains the object.} 
% A benefit of MIL-based approaches is its ability to detect multiple objects from different categories \cite{}.
% An image is given to a proposal generator (selective search \cite{selectivesearch}, edge boxes \cite{edgeboxes}, sliding window \cite{zhang2019freeanchor}) and backbone network \cite{alexnet, vgg, inception, he2015resnet} to get proposals and feature maps respectively. Next, feature maps and proposals are given to spatial pramid pooling (SPP) \cite{SPPNet} to generate fixed-size regions. Finally, these regions are fed to the detection head to classify and localise object instances. 
(b) resort to CAM-based methods \cite{zhou2016CAM, zhang2018cam} that use class activation maps to predict proposals. Specifically, an image is fed to a backbone network to generate feature vector from which bounding box of each class is predicted by thresholding activation maps with highest probability.
% This feature vector is given to a classifier to generate prediction scores of an image. Finally, CAM generates proposals or bounding box of each class by thresholding to segment activation maps with the highest probability of every class. 
Although CAM-based methods are faster, in this paper we use MIL-based technique as it can detect \textit{multiple instances} within the \textit{same} category \cite{wsodsurvey2022}.



% Collecting bounding box annotation per object category is time-consuming. Fine-grained object detection (e.g., recognising animal species) exacerbates this problem. Existing approaches formulate this task as multiple instance learning (MIL) \cite{dietterich1997MIL, wsddn, li2016wsod, diba2017wsod, jie2017wsod, zhang2018wsod, tang2018wsod, shen2019wsod} problem that interpret an image as bag of proposals or regions. If the image is labeled positive, then one of the regions tightly contain the object of interest. If the image is labeled negative, no region contains the object. We feed an image to a proposal generator (selective search \cite{selectivesearch}, edge boxes \cite{edgeboxes}, sliding window \cite{zhang2019freeanchor}) and backbone network \cite{alexnet, vgg, inception, he2015resnet} to get proposals and feature maps respectively. Next, feature maps and proposals are given to spatial pramid pooling (SPP) \cite{SPPNet} to generate fixed-size regions. Finally, these regions are fed to the detection head to classify and localise object instances. Instead of generating thousands of proposals as in MIL, an CAM-based methods \cite{zhou2016CAM, zhang2018cam} use class activation maps to predict proposals. Specifically, an image is fed to a backbone network to generate feature vector. This feature vector is given to a classifier to generate prediction scores of an image. Finally, CAM generates proposals or bounding box of each class by thresholding to segment activation maps with the highest probability of every class. In spite of CAM-based methods being faster, in this paper, we use MIL-based technique due to its ability in detecting multiple instances with the same category.

\noindent \textbf{Data Augmentation in Computer Vision:} Data augmentation improves the sufficiency and diversity of training data. Approaches vary from simple image rotation and flipping to more advanced techniques of image erasing \cite{gridmask} like CutOut \cite{devries2017cutout}, Hide-and-Seek \cite{hide-and-seek} and image mixing like MixUp \cite{mixup} and {CutMix} \cite{cutmix}. 
Aiming to generalise sketch-based object detection to complex scenes while training exclusively on existing object-level sketch photo pairs \cite{sketchy}, we thus employ a CutMix \cite{cutmix} like data augmentation trick -- a method that replaces removed sub-regions with a patch from another image to synthesise new images.

% In our context cutmix is quite relevant
% Out of these cutmix seems the most relevant to our cause
% CutMix \cite{cutmix} replaces removed sub-regions with a patch from another image to synthesise new images. In this paper, we employ a CutMix \cite{cutmix} like data augmentation trick, aiming to generalise sketch-based object detection to complex scenes while training exclusively on existing object-level sketch photo pairs \cite{sketchy}.


% In this paper, we employ data augmentation techniques like CutMix \cite{cutmix} that replaces removed sub-region with a patch from another image to synthesise new images. In particular, we replace the sub-regions of a scene with photos from different sketched categories. This helps learn object detection in complex scenes with multiple object instances by training from object-level sketch photo pairs.

% Data augmentation improves the sufficiency and diversity of training data. Approaches varies from simple image transformations like rotation, flipping, and cropping to the more advanced image erasing \cite{devries2017cutout, hide-and-seek, gridmask} and image mix \cite{mixup, cutmix}. Image erasing replace pixel values of one or more sub-regions in the image with constant (Cutout\cite{devries2017cutout}, Hide-and-Seek \cite{hide-and-seek}, GridMask \cite{gridmask}, FenceMask \cite{fencemask}) or random values (random erasing \cite{zheng2020random}). Image mix data combines two or more images or sub-regions into one. MixUp \cite{mixup} learns a linear relationship between mixing images from training set and their supervision signal. CutMix \cite{cutmix} replaces removed regions with a patch from another image to synthesise new images. Our proposed method is similar to CutMix \cite{cutmix} with sub-regions depicting photo of different sketched category from Sketchy dataset \cite{sketchy}. This helps the network learn complex scenes with multiple categories.

\noindent \textbf{Sketch-Based Object Representation}
Sketch with its intrinsic ability to model fine-grained visual details, makes it an ideal modality  for object retrieval, giving rise to avenues like \emph{category-level} \cite{liveSketch, yelamarthi2018sketch, doodle-to-search, sketchformer, sketchmate} and fine-grained (FG) \emph{instance-level} \cite{styleMeUp, bhunia2022worrying, bhunia2020sketch, bhunia2021semi} sketch-based image retrieval (SBIR). 
% The ability of sketch to offer inherently fine-grained visual description makes it an ideal modality for object retrieval. 
% Typically SBIR employs CNN \cite{liveSketch, doodle-to-search}, RNN \cite{sketchmate}, Transformer \cite{sketchformer}, or Vision Transformer \cite{fscoco} based deep triplet-ranking to learn joint embedding space \cite{yu2016shoe}. 
Contemporary research also explored zero-shot SBIR \cite{doodle-to-search, yelamarthi2018sketch, sain2022sketch3t}, cross-domain translation \cite{kaiyue2017cross} and approaches like reinforcement learning based on-the-fly retrieval \cite{bhunia2020sketch}, self-supervised \cite{pang2020jigsaw, vector-raster}, etc. Apart from object-level images, retrieving sketched objects from scene images was studied using graph convolutional networks \cite{liu2020scenesketcher} and optimal transport \cite{partially-does-it}. Similar to cross-category FG-SBIR \cite{kaiyue2017cross, bhunia2022adaptive}, here we explore fine-grained sketch photo association for object detection by adapting large vision-language models like CLIP \cite{CLIP} using prompt engineering \cite{zhou2022visualprompt}.

% to learn a fine-grained association using object-level sketch for fine-grained sketch-based object detection from scene images. 

% Concretely, we adapt large vision-language models like CLIP \cite{CLIP} using prompt engineering \cite{zhou2022visualprompt} to learn a cross-category FG-SBIR. Next, we distill knowledge from CLIP image-encoders for supervision.






% The ability of sketch to offer inherently fine-grained visual description makes it an ideal modality for object retrieval. This commenced avenues like \emph{category-level} \cite{liveSketch, yelamarthi2018sketch, doodle-to-search, sketchformer, sketchmate} and fine-grained (FG) \emph{instance-level} \cite{styleMeUp, bhunia2022worrying, bhunia2020sketch, bhunia2021semi} sketch-based image retrieval (SBIR). Typically SBIR employs CNN \cite{liveSketch, doodle-to-search}, RNN \cite{sketchmate}, Transformer \cite{sketchformer}, or Vision Transformer \cite{fscoco} based deep triplet-ranking based siamese networks to learn joint embedding space \cite{yu2016shoe}. Contemporary research is also directed towards zero-shot SBIR \cite{doodle-to-search, yelamarthi2018sketch, sain2022sketch3t}, binary hash-code embedding \cite{liu2017hashing, shen2018hashing}, attention mechanism \cite{deep-spatial-semantic}, cross-domain translation \cite{kaiyue2017cross}, reinforcement learning based on-the-fly retrieval \cite{bhunia2020sketch}, semi-supervised \cite{bhunia2021semi}, self-supervised \cite{pang2020jigsaw, vector-raster}, meta-learning \cite{bhunia2022adaptive}, style-agnostic retrieval \cite{styleMeUp}, etc. Apart from object-level images, retrieving sketched objects from scene images was studied using graph convolutional networks \cite{liu2020scenesketcher} and optimal transport \cite{partially-does-it}. In this paper, we show the potential of cross-category FG-SBIR \cite{kaiyue2017cross, bhunia2022adaptive} to solve object detection, a fundamental and challenging task in computer vision for a quarter century. Concretely, we adapt large vision-language models like CLIP using prompt engineering \cite{zhou2022visualprompt} to learn a cross-category FG-SBIR. Next, we formulate object detection as MIL problem and distill knowledge from CLIP image-encoders for supervision.

\section{Proposed Method}
\label{sec:proposed-method}
\noindent \textbf{Overview:} We propose an object detection framework that can localise queried objects and has fine-grained alignment with input sketch. Firstly, we train this network without using both bounding-box annotation and hand-drawn sketch. Secondly, we begin with a pre-trained Region Proposal Network (RPN) \cite{faster-rcnn} and fine-tune the backbone feature extractor using a novel object-level cross-category FG-SBIR. Finally, we bridge the object-level and scene-level gap using a custom designed data augmentation scheme.
% We learn an object detection framework which can localise novel objects in a scene that has fine-grained alignment with a given input sketch. Importantly, we train this network without the use of bounding box annotations. To this end, we employ knowledge distillation from a pre-trained cross-category FG-SBIR teacher model to an object detection student model. In addition, to bridge the gap between FG-SBIR models trained using object-level sketch-photo pairs to sketch-guided object detection in scene images, we choose a simple yet effective data augmentation scheme following existing literature on Cut-Mix.

\subsection{Baseline Supervised Object Detection}
\vspace{-0.5cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{latex/figures/fasterRCNN.pdf}
    \vspace{-0.6cm}
    \caption{Schematic of baseline Faster-RCNN \cite{faster-rcnn}. }
    \label{fig: faster-rcnn}
    \vspace{-0.3cm}
\end{figure}
\noindent First, we briefly summarise Faster-RCNN \cite{faster-rcnn}, a baseline object detection (OD) framework that remains state-of-the-art for several downstream tasks \cite{yang2018scenegraph, mouzenidis2021grounding, zhu2018relationship}. Given a photo $\mathbf{p} \in \mathbb{R}^{H \times W \times 3}$, a VGG-16 \cite{vgg} network $\mathcal{F}(\cdot)$ predicts a feature map $f_{\mathbf{p}} \in \mathbb{R}^{H' \times W' \times 512}$. Next, a region proposal network \cite{faster-rcnn} $\mathcal{R}(\cdot)$ generates $R$ rectangular proposals $\mathbf{r} = \{ r_1, r_2, \dots, r_R \}$ with $r_j \in \mathbb{R}^{512}$ from the convolutional feature map $f_{\mathbf{p}}$ by sliding a small ($3 \times 3$) spatial window (with effective receptive field $228$ pixels). At each sliding-window location, $k=9$ proposals (anchor boxes) are generated, associated with $3$ scales and $3$ aspect ratio. The proposals $\mathbf{r}$ is fed into two sibling fully-connected layers: (i) a box-regression layer that has $4k$ outputs representing the coordinates of $k$ boxes, and (ii) a box-classification layer that outputs $2k$ scores to estimate the probability of being an object or not (``objectness" measure). For each object proposal $r_j \in \mathbf{r}$, a region of interest (RoI) pooling layer $\mathcal{P}(\cdot)$ extracts a fixed-length feature vector of size ($7 \times 7 \times 512$) from the convolutional feature map $f_{\mathbf{p}}$ which is fed into a sequence of fully connected (FC) layers to get $f_{\mathbf{r}_{j}} = \mathcal{P}(f_{\mathbf{p}}, {r}_{j})$, where $f_{\mathbf{r}_{j}} \in \mathbb{R}^{512}$ branches into two stream output layer: (i) $\phi_{det}: \mathbb{R}^{512} \rightarrow \mathbb{R}^{N+1}$ that produces softmax probability estimates over $N$ object class plus a catch-all ``background" class for detection, and (ii) $\phi_{reg}: \mathbb{R}^{512} \rightarrow \mathbb{R}^{4}$ that regress the refined bounding-box coordinates for the detected region. \cref{fig: faster-rcnn} presents a schematic of the data flow in our baseline Faster-RCNN \cite{faster-rcnn}. The region proposal network $\mathcal{R}$, and the RoI pool based classification and bounding box regression $\mathcal{P}$ is trained alternatively \cite{faster-rcnn}. During training, $\mathcal{R}$ and $\mathcal{P}$ use $2$- or $N$-class cross-entropy and smooth L1 \cite{fast-rcnn} loss for bounding box regression.

While seminal, baseline Faster-RCNN \cite{faster-rcnn} has some key limitation: (i) it requires expensive bounding box annotation equally distributed across all $N$-classes. However, due to dataset bias \cite{torralba2011bias} -- Zipf's law \cite{zipf-law} and social conventions \cite{hendricks2018} -- collecting sufficient bounding box annotation for less occurring classes is near impossible. We show how weakly supervised object detection paradigm \cite{OICR} resolves this dependency on requiring bounding box annotation.
% (ii) Once trained on $N$-classes, the model fails to detect $(N+1)^{th}$-class, thus limiting real-world open-set applications. (iii) It lacks the flexibility of detecting only user-specified objects of interest (e.g., detecting only `swans' standing on one leg). Therefore, this demands a further investigation on how to adapt our baseline \cite{faster-rcnn} to overcome the aforementioned limitations.

\subsection{Weakly Supervised Object Detection via MIL}\label{sec: WSOD}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{latex/figures/wsddn.pdf}
    \caption{Adapting Faster-RCNN to weakly supervised setup using multiple instance learning paradigm. In particular, we remove the bounding-box regression head $\phi_{reg}$ and only classify region features $f_{\mathbf{r}}$ into an $N$-class classification.}
    \label{fig:wsddn}
\end{figure}
\noindent We adapt Faster-RCNN \cite{faster-rcnn} for downstream data distribution without collecting bounding box annotations. First, we pretrain a generalisable \cite{gu2022open-vocab-OD} region proposal network $\mathcal{R}$ using bounding box annotations from Visual Genome \cite{visualgenome}\footnote{Alternatively, one can use region proposal algorithms like Selective Search \cite{selectivesearch} or EdgeBoxes \cite{edgeboxes}, but it leads to lower performance.}. Next, we freeze $\mathcal{R}$ \cite{openVocab-semanticCLIP} and fine-tune the remaining modules in \cref{fig:wsddn} on downstream dataset, weakly supervised by reformulating object detection as a multiple instance learning (MIL) problem that detects positive regions from a bag of proposals. The proposal features $f_{\mathbf{r}} = \mathcal{P}(f_{\mathbf{p}}, \mathbf{r})$, where $f_{\mathbf{r}} \in \mathbb{R}^{R \times 512}$ from proposal $\mathbf{r}$ is branched into two streams $\mathbf{x}_{c} = \phi_{cls}(f_{\mathbf{r}})$, $\mathbf{x}_{d} = \phi_{det}(f_{\mathbf{r}})$, where $\mathbf{x}_{c} \in \mathbb{R}^{R \times N}$, $\mathbf{x}_{d} \in \mathbb{R}^{R \times N}$ respectively. Following \cite{OICR}, the first data stream performs classification of individual proposals $r_j \in \mathbf{r}$ into class scores as: $\sigma_{cls}(\mathbf{x}_{c}^{ij}) = \exp ( \mathbf{x}_{c}^{ij} ) / \sum_{k=1}^{N} \exp ( \mathbf{x}_{c}^{ik} )$. The second data stream scores proposals relative to one another as: $\sigma_{det}(\mathbf{x}_{d}^{ij}) = \exp ( \mathbf{x}_{d}^{ij} ) / \sum_{k=1}^{N} \exp ( \mathbf{x}_{d}^{kj} )$. The combined score is obtained by element-wise product $\mathcal{S}^{0} = \sigma_{cls}(\mathbf{x}_{c}) \odot \sigma_{det}(\mathbf{x}_{d})$, where $\mathcal{S}^{0} \in \mathbb{R}^{R \times N}$. Finally, we predict the image-level class scores ${y}_{c} = \sum_{j=1}^{R} \mathcal{S}^{0}_{jc}$ and train using multi-class cross entropy as:
\begin{equation}\label{eq:MIL}
    \mathcal{L}_{mil} = - \sum_{c=1}^{N} \{\mathbf{\hat{y}}_{c} \log {y}_{c} + (1 - \mathbf{\hat{y}}_{c}) \log (1 - {y}_{c})
\end{equation}
where, $\mathbf{\hat{y}} = \{\hat{y}_1, \hat{y}_2, \dots, \hat{y}_N \}$ are ground-truth image-level labels in downstream datasets. \cref{eq:MIL} ensures that our images may contain more than one object class but each proposals should contain a single class. 

Note, unlike Faster-RCNN (\cref{fig: faster-rcnn}), our weakly supervised setup lacks $\phi_{reg}$ that refines bounding box predictions. Hence, to predict accurate boxes, we iteratively refine \cite{iterativeText} our initial prediction $\mathcal{S}^{0}$ using $\phi_{cls}^{*}(\cdot)$ as $\mathcal{S}^{k} = \phi_{cls}^{*}(\mathcal{P}, \mathbf{r})$. The weights of $\phi_{cls}^{*}$ is iteratively updated by backpropagating instance classifier refinement loss $\mathcal{L}_{icr}$ between scores at $k^{th}$ step ($\mathcal{S}^{k}$) and a pseudo ground-truth $\mathbf{\hat{y}}^{(k)}$ from $(k-1)^{th}$ step. In particular, we assign $\mathbf{\hat{y}}^{(k)}_{j'c} = 1$ for class label $c$ in image if $j' = \arg \max_{j} \mathcal{S}^{(k-1)}_{jc}$. Proposals $r_j \in \mathbf{r}$ that has high overlap (${IoU} > 0.5$) with $r_{j'c}$ are assigned the same class label $\mathbf{\hat{y}}^{(k)}_{jc} = 1$, otherwise we label $r_j$ background class $\mathbf{\hat{y}}^{(k)}_{j(N+1)} = 1$. If an image does not have class label $c$, then for all regions $r_j \in \mathbf{r}$ are assigned $\mathbf{\hat{y}}^{(k)}_{jc} = 0$. Finally, we compute a weighted refinement loss:
\begin{equation}\label{eq:icr}
    \mathcal{L}^{k}_{icr} = - \frac{1}{R} \sum_{j=1}^{R} \sum_{c=1}^{N} \mathcal{S}^{(k-1)}_{j'c} \mathbf{\hat{y}}^{(k)}_{jc} \log \mathcal{S}^{(k-1)}_{jc}
\end{equation}
Hence, the total loss for weakly supervised object detection using MIL comprise of $\mathcal{L}_{tot} = \mathcal{L}_{mil} + \sum_{k=1}^{K} \mathcal{L}_{icr}$. While weakly supervised object detection trains using only image-level labels, it highlights two key limitations: (i) Once trained on $N$-classes, the model fails to detect $(N+1)^{th}$-class, thus limiting real-world open-set applications. (ii) It lacks the flexibility of detecting only user-specified objects of interest (e.g., detecting only `swans' standing on one leg). In the following section, we resolve these limitations by replacing image-level labels with query sketches.

\subsection{Localising Object Regions with Query Sketch}
\noindent In this section, we extend object detection from predefined $N$-classes to the open-set setup \cite{openVocab-semanticCLIP} by replacing proposal classifiers $\{ \phi_{cls}, \phi_{det}, \phi_{cls}^{*} \}$ mapping $\mathbb{R}^{R \times 512} \rightarrow \mathbb{R}^{R \times N}$ to proposal embeddings $\mathbb{R}^{R \times 512} \rightarrow \mathbb{R}^{R \times D}$ learning a $d$-dimensional feature vector as: $\mathbf{x}_{c} = \phi_{cls}(f_{\mathbf{p}}, \mathbf{r})$, $\mathbf{x}_{d} = \phi_{det}(f_{\mathbf{p}}, \mathbf{r})$, where $\mathbf{x}_{c} \in \mathbb{R}^{R \times D}$, $\mathbf{x}_{d} \in \mathbb{R}^{R \times D}$. Next, we use a pretrained (see \cref{sec: prompt-sketch}) sketch encoder $\mathcal{F}_{\mathbf{s}}$ that maps $N$ rasterised input sketches $\{ \mathbf{s}_{1}, \mathbf{s}_{2}, \dots, \mathbf{s}_{N} \}$, where $\mathbf{s}_{i} \in \mathbb{R}^{H \times W \times 3}$ to a sketch embedding $f_{\mathbf{s}} = \{ f_{\mathbf{s}}^{1}, f_{\mathbf{s}}^{2}, \dots, f_{\mathbf{s}}^{N} \}$, where $f_{\mathbf{s}} \in \mathbb{R}^{R \times D}$. Given the proposal and sketch embeddings, we compute the proposal scores ($\mathcal{S}^{0}$) as: $\mathcal{S}^{0} = \sigma_{cls}(\mathbf{x}_{c} \cdot [ f_{\mathbf{s}}^{1}, \dots, f_{\mathbf{s}}^{N} ]^{T}) \odot \sigma_{det}(\mathbf{x}_{d} \cdot [ f_{\mathbf{s}}^{1}, \dots, f_{\mathbf{s}}^{N} ]^{T})$. To compute the refined score $\mathcal{S}^{k}$ from the iterative update module $\mathcal{S}^{k}_{*} = \phi_{det}^{*}(f_{\mathbf{r}})$, where $\mathcal{S}^{k}_{*} \in \mathbb{R}^{R \times (N+1)}$ we additionally learn a ``background" embedding $bg \in \mathbb{R}^{D}$ to compute refined score as: $\mathcal{S}^{k} = \sigma_{cls}(\mathcal{S}^{k}_{*} \cdot [ f_{\mathbf{s}}^{1}, \dots, f_{\mathbf{s}}^{N}, bg ]^{T})$. Finally we train to detect sketched regions as \cref{sec: WSOD} using \cref{eq:MIL} and \cref{eq:icr}. While sketches provide flexibility to end-users and specify fine-grained regions of interest, the performance heavily depends on the sketch encoder ($\mathcal{F}_{\mathbf{s}}$) and its fine-grained alignment with patched image regions $\mathbf{r}$. This is a challenging task since $\mathcal{F}_{\mathbf{s}}$ should have fine-grained sketch-photo alignment across multiple categories \cite{pang2019generalising} and generalise to unseen open-vocabulary \cite{openVocab-semanticCLIP, gu2022open-vocab-OD} setup.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth, height=0.3\linewidth]{example-image-a}
    \caption{Caption}
    \label{fig:proposed}
\end{figure}

\subsection{Generalised Cross-Category FG-SBIR}\label{sec: prompt-sketch}
\noindent First, we briefly introduce fine-grained sketch-based image retrieval (FG-SBIR). Given a sketch-photo pair represented as ($\mathbf{s}, \mathbf{p}$), we use a sketch and photo encoder $\mathcal{F}_{\mathbf{s}}$ and $\mathcal{F}_{\mathbf{p}}$ to get feature map $f_{\mathbf{s}} \in \mathbb{R}^{512}$ and $f_{\mathbf{p}} \in \mathbb{R}^{512}$. The network is trained via triplet loss with margin parameter $\mu > 0$ such that the cosine distance $\delta(\cdot)$ of sketch anchor $\mathbf{s}$ from a negative photo ($\mathbf{p}^{-}$) should increase while that from the positive photo ($\mathbf{p}^{+}$) should decrease as, $\mathcal{L}_{trip} = \max \{0, \mu + \delta(f_{\mathbf{s}}, f_{\mathbf{p}}^{+}) - \delta(f_{\mathbf{s}}, f_{\mathbf{p}}^{-}) \}$.

Next, we extend FG-SBIR to multiple categories, by exploiting the open-vocabulary potential of CLIP \cite{CLIP}. CLIP \cite{CLIP} consists of two encoders, one for image and the other for text, trained on $400M$ text-image pairs. Using the ViT-backbone \cite{ViT}, we inject\footnote{While there are multiple ways \cite{} to inject visual prompts, for brevity we follow \cite{VPT} that injects prompt as an ``patch" in the first ViT layer.} a learnable sketch and image prompt $\gamma_{\mathbf{s}} \in \mathbb{R}^{768}$ and $\gamma_{\mathbf{p}} \in \mathbb{R}^{768}$ that induce the pretrained CLIP image encoder to model generalised fine-grained associations between sketch ($\mathbf{s}$) and photo ($\mathbf{p}$) across multiple categories (open vocabulary). Formally, we compute sketch and photo features using CLIP's image encoder as: $f_{\mathbf{s}} = \mathcal{F}_{clip} (\mathbf{s}, \gamma_{\mathbf{s}})$ and $f_{\mathbf{p}} = \mathcal{F}_{clip} (\mathbf{p}, \gamma_{\mathbf{p}})$ respectively. To instill category-wise discriminative knowledge for cross-category FG-SBIR, we additionally use the text-encoder in CLIP for category-level classification as: $\mathcal{L}_{cat} = \texttt{Cross\_Entropy}(\mathbf{c}_{\mathbf{s}}, \texttt{softmax}([f_{\mathbf{t}}^{1} \cdot f_{\mathbf{s}}^{T}, \dots, f_{\mathbf{t}}^{N} \cdot f_{\mathbf{s}}^{T} ]))$, where $f_{\mathbf{t}}^{i}$ is the text embedding from the text encoder in CLIP given input ``a photo of a $<$category$>$", and $\mathbf{c}_{\mathbf{s}} \in \mathbb{R}^{N}$ is a one-hot vector representing the category of sketch ($\mathbf{s}$). Hence, the generalised cross-category prompt ($\gamma_{\mathbf{s}}, \gamma_{\mathbf{p}}$) is learned as $\mathcal{L}_{pmt} = \mathcal{L}_{trip} + \mathcal{L}_{cat}$. While the pretrained $\mathcal{F}_{clip}(\cdot, \gamma_{\mathbf{s}})$ serves as the sketch encoder to train weakly supervised object detection in \cref{sec: WSOD}, the photo encoder $\mathcal{F}_{clip}(\cdot, \gamma_{\mathbf{p}})$ additionally aligns the proposal feature $f_{\mathbf{r}}$ by computing the L1 distance as: $\mathcal{L}_{kd} = ||f_{\mathbf{r}} - \mathcal{F}_{clip}(\texttt{Crop}(\mathbf{p}, \mathbf{r})) ||_{1}$ as shown in \cref{fig:proposed}.

\subsection{Bridging Object-Level and Image-Level}
* \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ 

% \subsection{Cross-Category Generalisation for FG-SBIR}
% \label{sec:generalisation-FGSBIR}
% We briefly summarise an existing \cite{bhunia2022adaptive} cross-category FG-SBIR framework. The category specific training and testing data consists of $\mathcal{D}^{S} = \{ \mathcal{D}^{S}_{1}, \mathcal{D}^{S}_{2}, \dots, \mathcal{D}^{S}_{|N^{S}|} \ni \ |N^{S}| > 1 \}$ and $\mathcal{D}^{T} = \{ \mathcal{D}^{T}_{1}, \mathcal{D}^{T}_{2}, \dots, \mathcal{D}^{T}_{|N^{T}|} \ni \ |N^{T}| > 1 \}$, where $N^{S}$ and $N^{T}$ are disjoint sets of training and testing categories respectively, i.e., $N^{S} \cap N^{T} = \phi$. Each $i$-th category comprise of $K^{i}$ paired sketch ($x$) and photo ($y$) as $D_{i} = \{x_{j}, y_{j} \}^{K^{i}}_{j=1}$. A feature extractor $\mathcal{F}_{\theta}: \mathbb{R}^{H \times W \times C} \rightarrow \mathbb{R}^{D}$ parameterised by $\theta$ is used to get the feature map $u = \mathcal{F}_{\theta}(x)$ and $v = \mathcal{F}_{\theta}(y)$ for sketch and photo respectively. During training,  the cosine distance $\delta(\cdot, \cdot)$ of a sketch ($x^{i}$) from a negative photo ($y^{i}_{-}$) of the same $i$-th category, denoted as $\beta^{-}_{i} = \delta(u^{i}, v^{i}_{-})$ should increase while that from the positive photo ($y^{i}_{+}$), $\beta^{+}_{i} = \delta(u^{i}, v^{i}_{+})$ should decrease. Training is done via triplet loss with hyperparameter $\mu > 0$ as,
% \begin{equation}
%     \mathcal{L}_{trip} = \frac{1}{N} \sum_{i=1}^{N} \max \{0, \mu + \beta^{+}_{i} - \beta^{-}_{i} \}
% \end{equation}
% While triplet loss helps distinguish between instances of a particular category, a class discriminative objective \cite{discriminative-triplet} helps towards learning to separate between different categories in a cross-category FG-SBIR model. To this end, we add a cross-entropy loss using a classifier $\mathcal{F}_{\theta_{C}}: \mathbb{R}^{D} \rightarrow \mathbb{R}^{|N^{S}|}$ parameterised by $\theta_{C}$. Given the class label $c_{i} \in N^{S}$, the classification loss is defined as,
% \begin{equation}
%     \mathcal{L}_{C} = \frac{1}{N}\sum_{i=1}^{N} \texttt{Cross\_Entropy}(c_{i}, \texttt{softmax}(\mathcal{F}_{\theta_{C}}(u^{i})))
% \end{equation}
% Hence, we can define our final training objective as,
% \begin{equation}
%     \mathcal{L}_{ret} = \mathcal{L}_{trip} + \mathcal{L}_{C}
% \end{equation}

% \subsection{Object-Level Detection with FG-SBIR}
% The proposed object detection framework is trained via knowledge distillation from a cross-category generalised FG-SBIR model without using bounding box annotation. We use the paired sketch ($x$) and photo ($y$) dataset defined in Sec.~\ref{sec:generalisation-FGSBIR} to inject \emph{instance-level mapping} between localised photo regions and the input sketch as shown in Fig.~\ref{fig:object-level-det}. We shall adapt to scene-level complexity in the next section.
% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth, height=0.3\linewidth]{example-image-a}
%     \caption{Sample object-level detection}
%     \label{fig:object-level-det}
%     \vspace{-1em}
% \end{figure}

% Given a photo ($y^{i}_{j}$) from the $i$-th category, the object proposals $\mathbf{R} = \{ r_1, r_2, \dots, r_{M} \}$ are generated by Selective Search \cite{selective-search}. Following existing weakly supervised object detection frameworks \cite{wsddn, self-attention-wsddn}, a CNN backbone is used to extract the photo feature maps for $y^{i}_{j}$, from which the proposal feature maps are extracted respectively. Next, these proposal feature maps are fed to a Region-of-Interest (RoI) Pooling layer \cite{faster-rcnn} and two Fully-Connected (FC) layers to obtain proposal feature vectors. These proposal feature vectors are forked into two parallel classification and detection branches where each is fed to a Fully-Connected (FC) layer $f_{cls}(\cdot)$ and $f_{det}(\cdot)$, as shown in Fig.~\ref{}, to generate two vectors $e_{cls} \in \mathbb{R}^{M \times D}$ and $e_{det} \in \mathbb{R}^{M \times D}$ respectively. During training, the cosine similarity $sim(\cdot, \cdot)$ of $e_{cls}$ and $e_{det}$ from a positive sketch ($x^{i}_{+}$) should increase while that from negative sketch ($x^{i}_{-}$) should decrease.
% \begin{equation}
%     \begin{split}
%     z_{cls}(r) = [ sim(e^{r}_{cls}, u^{i}_{+}) ] \cup [sim(e^{r}_{cls}, u^{i}_{-}) \ : \ \forall \ x^{i}_{-} \in \mathcal{D}^{i} ] \\
%     z_{det}(r) = [ sim(e^{r}_{det}, u^{i}_{+}) ] \cup [sim(e^{r}_{det}, u^{i}_{-}) \ : \ \forall \ x^{i}_{-} \in \mathcal{D}^{i} ] \\
%     \end{split}
% \end{equation}
% where, $z_{cls}$ and $z_{det}$ are matrices of shape $M \times K_{i}$. While the two streams $z_{cls}$ and $z_{det}$ are remarkably similar, to interpret them as performing classification and detection, we compute $\sigma_{cls}$ and $\sigma_{det}$ by taking softmax along row-wise and column-wise respectively. The final score is computed by taking element-wise (Hadamard) product $\sigma = \sigma_{cls} \odot \sigma_{det}$ of the classification and detection similarities. Finally, the instance-level classification score for the $j$-th instance in $i$-th category is computed as,
% \begin{equation}
%     \begin{split}
%         \mathcal{L}_{sk} = -\frac{1}{NK_{i}} \sum_{i=1}^{N} \sum_{j=1}^{K_{i}} \texttt{Cross\_Entropy}(\mathbb{1}_{j}, \sum_{r=1}^{M} \sigma^{i}_{r, j})
%     \end{split}
% \end{equation}

% To make the training more efficient, we align the detection data stream ($e_{det}$) with FG-SBIR model $\mathcal{F}_{\theta}(\cdot)$ by cropping the photo region using object proposals $\mathbf{R} = \{r_1, r_2, \dots, r_{M}$ and minimising their $\mathcal{L}_{1}$ distance as,
% \begin{equation}
%     \mathcal{L}_{1} = \frac{1}{M} \sum_{r=1}^{M} || \mathcal{F}_{\theta}(\texttt{crop}(y, r)) - e^{r}_{det} ||_{1}
% \end{equation}

% Following a weakly supervised object detection setup, it lacks spatial smoothness guarantee present in supervised setups. In particular, Fast-RCNN \cite{fast-rcnn} takes region proposals that have more than $50\%$ IoU with a ground-truth bounding box as positive samples and learns to regress them into their corresponding ground-truth bounding boxes. Since our setup does not have access to ground-truth boxes, we follow a soft regularisation strategy that penalise feature map discrepancies between the highest scoring region and the region with more than $60\%$ IoU, denoted by $\mathbf{\bar{R}} = \{r_1, r_2, \dots, r_{\bar{M}} \}$. Formally,
% \begin{equation}
%     \begin{split}
%         & \hspace{1em} r^{*} = \arg \max (\sigma^{i}_{r}) \\
%         \mathcal{L}_{reg} & = \frac{1}{N} \sum_{i=1}^{N} \sum_{r=1}^{\bar{M}} \frac{1}{2} (\sigma^{i}_{r})^{2} ||e^{r^{*}}_{det} - e^{r}_{det}||_{2} \\
%     \end{split}
% \end{equation}
% Hence, we can define our resulting object-level detection loss via knowledge distillation from FG-SBIR and hyperparameters $\lambda_{1}=1$ and $\lambda_{2}=1$ as,
% \begin{equation}
%     \mathcal{L}_{obj} = \mathcal{L}_{sk} + \lambda_{1} \mathcal{L}_{1} + \lambda_{2} \mathcal{L}_{reg}
% \end{equation}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
