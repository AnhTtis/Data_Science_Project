% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{color, colortbl}
\usepackage{xcolor}
% \usepackage{bbold}
\usepackage{multirow}
\usepackage{makecell}
\newcommand{\keypoint}[1]{\vspace{0.01cm}\noindent\textbf{#1}\quad}
\newcommand{\cut}[1]{}
\definecolor{Gray}{gray}{0.9}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{5825} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{What Can Human Sketches Do for Object Detection?}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Sketches are highly expressive, inherently capturing subjective and fine-grained visual cues. The exploration of such innate properties of human sketches has however been limited to that of image retrieval. In this paper, for the first time, we cultivate the expressiveness of sketches, but for the fundamental vision task of object detection. The end result is sketch-enabled object detection framework that detects based on what \textit{you} sketch -- \textit{that} ``zerbra'' (e.g., one that is eating the grass) in a herd of zebras (instance-ware detection), and only the \textit{part} (e.g., ``head" of a ``horse") that you desire (part-aware detection). We further dictate that our model works without (i) knowing which category to expect at testing (zero-shot), and (ii) not requiring additional bounding boxes (as per supervised), and class labels (as per weakly supervised). Instead of devising a model from the ground up, we show an intuitive synergy between foundation models (e.g., CLIP) and existing sketch models build for sketch-based image retrieval (SBIR), can already elegantly solve the task -- CLIP to provide model generalisation, and SBIR to bridge the (sketch$\rightarrow$photo) gap. In particular, we first perform independent prompting on both sketch and photo branches of a SBIR model, to build highly generalisable sketch and photo encoders on the back of the generalisation ability of CLIP. We then devise a training paradigm to adapt the learned encoders for object detection, such that the region embeddings of detected boxes are aligned with the sketch and photo embeddings from SBIR. Evaluating our framework on standard object detection datasets like PASCAL-VOC and MS-COCO outperforms both supervised (SOD) and weakly supervised object detectors (WSOD) on zero-shot setups.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/sketchOD-teaser.pdf}
    \caption{We train an object detector using SBIR models. (a) First, we train a FG-SBIR model using existing sketch--photo pairs that generalise to unseen categories. (b) To train the object detector module, we tile multiple object-level photos from SBIR datasets \cite{sketchy} and use its paired sketch encoding via pre-trained sketch encoder to align region embedding of detected boxes. (c) Inclusion of sketches for object detection opens several avenues like detecting a specific object for query sketch (e.g., detect a ``horse" facing left/right) or part of an object (e.g., ``head" of a ``horse").}
    \label{fig:problem-setup}
    \vspace{-0.3cm}
\end{figure}

Sketches have been used from prehistoric times for humans to express and record ideas. The level of expressiveness they carry remains unparalleled today even in the face of language -- recall that moment that you want to resort to pen and paper (or Zoom Whiteboard) to sketch down an idea?

Sketch research has also flourished over the past decade, with a whole spectrum of works on traditional tasks such as classification and synthesis, and those more sketch-specific such as modeling visual abstraction, style transfer and continuous stroke fitting, to cute applications such as turning a sketch into a photo classifier. 

The expressiveness of sketches however has been only explored in the form of sketch-based image retrieval (SBIR), especially the fine-grained variant (FG-SBIR). Great strides have been made, with recent systems already reaching maturity for commercial adaptation -- a great testimony on how cultivating sketch expressiveness can make real impact. 

In this paper, we ask the question -- what can human sketches do for the fundamental vision tasks of object detection? The envisaged outcome is therefore a sketch-enabled object detection framework that detects based on what you sketch, i.e., how \textit{you} want to express yourself. So, sketching a ``zebra eating the grass", as shown in \cref{fig:problem-setup}, should detect ``that" zebra from a herd of zebras (instance-aware detection), \textit{and} it will also give you the freedom to be specific with parts (part-aware detection), so if the ``head" of a ``horse" is what you would rather desire, then just sketch the very head. 

Instead of devising a sketch-enabled object detection model from the ground up, we show that an intuitive synergy between foundation models (e.g., CLIP) and off-the-shelf SBIR models can already, rather elegantly, solve the problem -- CLIP to provide model generalization, and SBIR to bridge the (sketch$\rightarrow$photo) gap. In particular, we adapt CLIP to build sketch and photo encoders (branches in a common SBIR model), by learning independent prompt vectors \cite{maple2022} separately for both each modality. More specifically, during training, the learnable prompt vectors are prepended into the input sequence of the first transformer layer of CLIP's ViT backbone \cite{ViT}, while keeping the rest frozen.
% \textcolor{blue}{Fine-tuning pre-trained CLIP weights (trained on $400M$ text/image pairs) leads to model collapse. Hence, we use the parameter-efficient approach of prompt learning \cite{CoOp} that retain CLIP's generalisation potential (prevent overfitting) and adapt to downstream task.} 
As such, we inject model generalization to the learned sketch and photo distributions. Next, we devise a training paradigm to adapt the learned encoders for object detection, such that the region embeddings of detected boxes are aligned with the sketch and photo embeddings from SBIR. This allows our object detector to train \textit{without} requiring additional training photos (bounding boxes, and class labels) from auxiliary object detection datasets.


% In this paper, we set out to fundamentally change this outlook towards SBIR from being a ``niche" application to being a game changer for mainstream computer vision tasks like object detection. In particular, we show that a powerful SBIR is all you need to train an object detector. Our contributions are two-fold: (i) First, we propose the first generalised cross-category fine-grained sketch-based image retrieval (FG-SBIR) framework leveraging the out-of-vocab potential of CLIP via prompt learning. (ii) Second, using the generalised FG-SBIR network, we train an object detector without using bounding box annotation (supervised object detection), or image-level class labels (weakly supervised object detection).

% Scaling SBIR to generalise for large number of unseen categories (zero-shot cross-category FG-SBIR) is non-trivial. Apart from tackling sketch-photo domain gap in each category with a varying range of complexity \cite{bhunia2022adaptive}, cross-category FG-SBIR also needs to understanding the underlying semantics across both seen and unseen categories. To achieve this, we look at the parallel literature of foundation models \cite{romback2022stablediffusion}. Foundation models like CLIP \cite{CLIP} train on internet-scale text--image paired datasets to learn a generic visual representation that allow zero-shot transfer to variety of downstream tasks \cite{VPT}. However, fine-tuning CLIP on small scale SBIR datasets \cite{sketchy, sketchrnn2018} leads to performance collapse \cite{CLIP}. Hence, we use a parameter-efficient approach of learning a sketch-specific and photo-specific prompts \cite{zhou2022visualprompt} to induce CLIP's visual encoder for cross-category FG-SBIR \cite{pang2019generalising} and simultaneously leverage its generalisation potential \cite{CoOp}.

To make our sketch-based detector more interesting (general-purpose), we further dictate it also works in a zero-shot matter. For that, following \cite{wsddn}, we extend object detection from a pre-defined fixed-set setup to an open-vocab setup. Specifically, we replace the classification heads in object detectors with prototype learning \cite{prototype-segmentation-2021}, where the encoded query sketch features acts as the support set (or prototypes). 
% For that, we extend object detection from pre-defined fixed-set categories to a open-vocab prototype learning setup\cite{prototype-segmentation-2021}, where each category or instance-level prototype is computed using our pre-trained sketch encoder. 
Next, the model is trained under the weakly supervised object detection (WSOD) setting \cite{wsodsurvey2022}, using a multi-category cross-entropy loss over the prototypes of all possible categories or instances. However, while SBIR is trained using object-level (single object) sketch/photo pairs, object detection works on image-level (multiple categories). Hence, to train object detector using SBIR, we also need to bridge the gap between object and image-level feature. Towards this, we use a data augmentation trick that is embarrassingly simple yet highly effective for robustness towards corruption and generalisation to out-of-vocab \cite{cutmix, mixup} -- we randomly select $n=\{1, \dots, 7\}$ photos from SBIR datasets \cite{sketchrnn2018, sketchy} and arbitrarily tile them on a blank canvas (similar to CutMix \cite{cutmix}).

In summary, our contributions are (i) for the first time cultivating the expressiveness of human sketches for object detection, (ii) a sketch-based object detector that detects what you intend to express in your sketch, (iii) an object detector that is both instance-aware and part-aware, in addition to performing conventional category-level detection. (iv) a novel prompt learning setup to marry CLIP and SBIR, to build the sketch-aware detector, that works without needing bounding box annotations (as per supervised \cite{faster-rcnn, fast-rcnn}), class labels (as per weakly supervised \cite{wsddn, wsod2}), and in a zero-shot manner. (v) results that outperform both supervised (SOD) and weakly supervised object detectors (WSOD) on zero-shot setup.

% In summary, our contributions are: (i) we debunk SBIR from being a ``niche" applications to forefront of computer vision applications like object detection. (ii) We show how a powerful SBIR model is all we need to train an object detector. (iii) Towards this goal, we scale SBIR to generalise to unseen categories with the introduction of prompt learning using CLIP \cite{CLIP}. (iV) The introduction of sketch for object detection opens several new avenues like category-level object detection, fine-grained object detection, and part-level object detection. (v) The high generalisation of our SBIR used to train object detector helps the proposed framework to outperform both supervised (SOD) and weakly supervised object detectors (WSOD) on zero-shot setup.

\section{Related Works}
\label{sec:related-works}

\noindent \textbf{Sketch for Visual Understanding:}
Hand-drawn sketches serve as a useful query modality for visual understanding tasks that involve human perception and structural cues. Sketches not only convey a visual description \cite{hertzmann2020perception} but also exhibit artistic styles \cite{zhang2021smartshadow}. This makes sketch a vital querying modality for the creative industry like artistic image editing \cite{yang2020surgery} and animation \cite{xing2015autocomplete}. Unlike photos that are passively captured by a camera, sketches are actively drawn by humans which makes them a good visual representation \cite{pixelor} enriched with human participation. Apart from the widely explored sketch-based image retrieval \cite{bhunia2021semi, bhunia2020sketch, pang2019generalising, styleMeUp, sketchformer, liveSketch}, sketch as a query has shown potential in several vision understanding tasks like incremental learning \cite{bhunia2022incremental} image and video synthesis \cite{cusuh2022synthesis, DeepFaceVideoEditing2022}, representation learning \cite{alaniz2022primitives, clipasso}, image-inpainting \cite{xie2021inpainting}, 3D shape retrieval \cite{xu20223Dretrieval}, 3D shape modeling \cite{chowdhury20223Dsynthesis}, medical image analysis \cite{wang2022medical}, object localisation \cite{tripathi2020object, riba2021object} and segmentation \cite{hu2020segmentation, qi2022segmentation}.

While exploring sketch as a query for object detection, by Tripathi \etal \cite{tripathi2020object}, several limitations surfaced with respect to problem definition as well as architectural designs. 
Firstly, instead of fine-grained matching, sketch was used to specify object category (easier via text/keyword \cite{CLIP, gu2022open-vocab-OD}), thus \cut{wasting} missing the potential of sketch as a query in modeling fine-grained details.
% Primarily, potential of sketch as a query in modeling fine-grained details remains unused, as specifying object category via sketching is quite laborious \cite{bhunia2020sketch}, compared to a text/keyword \cite{CLIP, gu2022open-vocab-OD}.
Secondly, it requires both bounding-box and object-sketch annotation which increases annotation budget without significant improvement in performance over traditional object detection setup. 
% \textcolor{red}{Secondly, as specifying object category by sketching is laborious \cite{bhunia2020sketch}, compared to a text/keyword \cite{CLIP, gu2022open-vocab-OD}, the potential of sketch in modeling fine-grained details remains unused.}
% thus overlooking the potential of sketch in modeling fine-grained details -- a \emph{fine-grained query}. 
% sketch as a query prides in its ability to model fine-grained details \cite{styleMeUp, bhunia2022worrying, bhunia2020sketch}. 
Thirdly, due to an expensive bounding box and sketch annotation, only fewer than $50\%$ object categories in existing object detection datasets \cite{pascalVOC, mscoco} are available for training. Finally, using an early fusion strategy \cite{xu2022MML} of sketch with object detection results in recomputing object regions for each new sketch -- leading to a slower detection framework with increasing query sketches.
% \cite{tripathi2020object} employs an early fusion strategy, that uses query-sketch as a conditional input to predict object regions, per sketch-photo pair -- resulting in a slower paradigm as sketch categories increase.
In this paper, we propose a fine-grained sketch-based object detection framework that use only object-level sketch photo pairs without any bounding-box annotations for training, and is scalable with multiple fine-grained query sketches, even under zero-shot setup.

\noindent \textbf{Supervised Object Detection:} 
Object detection jointly localise and identify objects in an image. Traditional object detectors rely on large supervised object detection datasets such as PASCAL VOC \cite{pascalVOC} and MS-COCO \cite{mscoco}, containing thousands of examples per object category which are quite time-consuming to annotate, unlike our pipeline, which leverages sketch-photo pairs as object annotations. 
% have multitudes\cut{over hundreds and thousand} of time-consuming annotated examples per object category. 
Existing literature on object detection is bifurcated as: (i) fast yet less accurate single-shot \cite{yolo, yolov3, SSD, lin2017focal-loss, centerNet}; (ii) slow but more accurate two-stage object detectors \cite{rcnn, fast-rcnn, faster-rcnn, maskrcnn}. To fully exploit the fine-grained cues provided by sketch, our proposed method aligns with the two-stage detectors. Two-stage detectors predict object regions using selective search in RCNN \cite{rcnn}, ROI pooling \cite{fast-rcnn} in Fast-RCNN, and Region Proposal Network (RPN) with ROIAlign in Faster-RCNN \cite{faster-rcnn}. While there has been several attempts with sophisticated architectural modifications   \cite{law2018cornerNet, zhou2019grouping, centerNet}, Faster-RCNN \cite{faster-rcnn} still acts a a fundamental building block for several downstream tasks like scene-graph generation \cite{yang2018scenegraph}, visual grounding \cite{mouzenidis2021grounding}, and relationship prediction \cite{zhu2018relationship}. Therefore, we resort to the more traditional Faster-RCNN based two-stage pipeline.


\noindent \textbf{Weakly Supervised Object Detection (WSOD):} Collecting bounding box annotation per object category is already a time-consuming process, which is aggravated even further by Fine-grained object detection (e.g., recognising animal species). Aiming to overcome this, existing WSOD methods adopt two schools of thoughts: (a) formulate this task as a multiple instance learning (MIL) \cite{dietterich1997MIL, wsddn, li2016wsod, diba2017wsod, jie2017wsod, zhang2018wsod, tang2018wsod, shen2019wsod} problem that interpret an image as a bag of proposals or regions. The image is labeled positive, when one of the regions tightly contains the object of interest, otherwise negative if no region has it.
% whereas a negative would imply no region has it.
% {If the image is labeled negative, no region contains the object.} 
% A benefit of MIL-based approaches is its ability to detect multiple objects from different categories \cite{}.
% An image is given to a proposal generator (selective search \cite{selectivesearch}, edge boxes \cite{edgeboxes}, sliding window \cite{zhang2019freeanchor}) and backbone network \cite{alexnet, vgg, inception, he2015resnet} to get proposals and feature maps respectively. Next, feature maps and proposals are given to spatial pramid pooling (SPP) \cite{SPPNet} to generate fixed-size regions. Finally, these regions are fed to the detection head to classify and localise object instances. 
(b) resort to CAM-based methods \cite{zhou2016CAM, zhang2018cam} that use class activation maps to predict proposals. Specifically, an image is fed to a backbone network to generate feature vector from which bounding box of each class is predicted by thresholding activation maps with highest probability.
% This feature vector is given to a classifier to generate prediction scores of an image. Finally, CAM generates proposals or bounding box of each class by thresholding to segment activation maps with the highest probability of every class. 
Although CAM-based methods are faster, in this paper we use MIL-based technique as it can detect \textit{multiple instances} within the \textit{same} category \cite{wsodsurvey2022}.



% Collecting bounding box annotation per object category is time-consuming. Fine-grained object detection (e.g., recognising animal species) exacerbates this problem. Existing approaches formulate this task as multiple instance learning (MIL) \cite{dietterich1997MIL, wsddn, li2016wsod, diba2017wsod, jie2017wsod, zhang2018wsod, tang2018wsod, shen2019wsod} problem that interpret an image as bag of proposals or regions. If the image is labeled positive, then one of the regions tightly contain the object of interest. If the image is labeled negative, no region contains the object. We feed an image to a proposal generator (selective search \cite{selectivesearch}, edge boxes \cite{edgeboxes}, sliding window \cite{zhang2019freeanchor}) and backbone network \cite{alexnet, vgg, inception, he2015resnet} to get proposals and feature maps respectively. Next, feature maps and proposals are given to spatial pramid pooling (SPP) \cite{SPPNet} to generate fixed-size regions. Finally, these regions are fed to the detection head to classify and localise object instances. Instead of generating thousands of proposals as in MIL, an CAM-based methods \cite{zhou2016CAM, zhang2018cam} use class activation maps to predict proposals. Specifically, an image is fed to a backbone network to generate feature vector. This feature vector is given to a classifier to generate prediction scores of an image. Finally, CAM generates proposals or bounding box of each class by thresholding to segment activation maps with the highest probability of every class. In spite of CAM-based methods being faster, in this paper, we use MIL-based technique due to its ability in detecting multiple instances with the same category.

\noindent \textbf{Data Augmentation in Computer Vision:} Data augmentation improves the sufficiency and diversity of training data. Approaches vary from simple image rotation and flipping to more advanced techniques of image erasing \cite{gridmask} like CutOut \cite{devries2017cutout}, Hide-and-Seek \cite{hide-and-seek} and image mixing like MixUp \cite{mixup} and {CutMix} \cite{cutmix}. 
Aiming to generalise sketch-based object detection to complex scenes while training exclusively on existing object-level sketch photo pairs \cite{sketchy}, we thus employ a CutMix \cite{cutmix} like data augmentation trick -- a method that replaces removed sub-regions with a patch from another image to synthesise new images.

% In our context cutmix is quite relevant
% Out of these cutmix seems the most relevant to our cause
% CutMix \cite{cutmix} replaces removed sub-regions with a patch from another image to synthesise new images. In this paper, we employ a CutMix \cite{cutmix} like data augmentation trick, aiming to generalise sketch-based object detection to complex scenes while training exclusively on existing object-level sketch photo pairs \cite{sketchy}.


% In this paper, we employ data augmentation techniques like CutMix \cite{cutmix} that replaces removed sub-region with a patch from another image to synthesise new images. In particular, we replace the sub-regions of a scene with photos from different sketched categories. This helps learn object detection in complex scenes with multiple object instances by training from object-level sketch photo pairs.

% Data augmentation improves the sufficiency and diversity of training data. Approaches varies from simple image transformations like rotation, flipping, and cropping to the more advanced image erasing \cite{devries2017cutout, hide-and-seek, gridmask} and image mix \cite{mixup, cutmix}. Image erasing replace pixel values of one or more sub-regions in the image with constant (Cutout\cite{devries2017cutout}, Hide-and-Seek \cite{hide-and-seek}, GridMask \cite{gridmask}, FenceMask \cite{fencemask}) or random values (random erasing \cite{zheng2020random}). Image mix data combines two or more images or sub-regions into one. MixUp \cite{mixup} learns a linear relationship between mixing images from training set and their supervision signal. CutMix \cite{cutmix} replaces removed regions with a patch from another image to synthesise new images. Our proposed method is similar to CutMix \cite{cutmix} with sub-regions depicting photo of different sketched category from Sketchy dataset \cite{sketchy}. This helps the network learn complex scenes with multiple categories.

\noindent \textbf{Sketch-Based Object Representation}
Sketch with its intrinsic ability to model fine-grained visual details, makes it an ideal modality  for object retrieval, giving rise to avenues like \emph{category-level} \cite{liveSketch, yelamarthi2018sketch, doodle-to-search, sketchformer, sketchmate} and fine-grained (FG) \emph{instance-level} \cite{styleMeUp, bhunia2022worrying, bhunia2020sketch, bhunia2021semi} sketch-based image retrieval (SBIR). 
% The ability of sketch to offer inherently fine-grained visual description makes it an ideal modality for object retrieval. 
% Typically SBIR employs CNN \cite{liveSketch, doodle-to-search}, RNN \cite{sketchmate}, Transformer \cite{sketchformer}, or Vision Transformer \cite{fscoco} based deep triplet-ranking to learn joint embedding space \cite{yu2016shoe}. 
Contemporary research also explored zero-shot SBIR \cite{doodle-to-search, yelamarthi2018sketch, sain2022sketch3t}, cross-domain translation \cite{kaiyue2017cross} and approaches like reinforcement learning based on-the-fly retrieval \cite{bhunia2020sketch}, self-supervised \cite{pang2020jigsaw, vector-raster}, etc. Apart from object-level images, retrieving sketched objects from scene images was studied using graph convolutional networks \cite{liu2020scenesketcher} and optimal transport \cite{partially-does-it}. Similar to cross-category FG-SBIR \cite{kaiyue2017cross, bhunia2022adaptive}, here we explore fine-grained sketch photo association for object detection by adapting large vision-language models like CLIP \cite{CLIP} using prompt engineering \cite{zhou2022visualprompt}.

% to learn a fine-grained association using object-level sketch for fine-grained sketch-based object detection from scene images. 

% Concretely, we adapt large vision-language models like CLIP \cite{CLIP} using prompt engineering \cite{zhou2022visualprompt} to learn a cross-category FG-SBIR. Next, we distill knowledge from CLIP image-encoders for supervision.






% The ability of sketch to offer inherently fine-grained visual description makes it an ideal modality for object retrieval. This commenced avenues like \emph{category-level} \cite{liveSketch, yelamarthi2018sketch, doodle-to-search, sketchformer, sketchmate} and fine-grained (FG) \emph{instance-level} \cite{styleMeUp, bhunia2022worrying, bhunia2020sketch, bhunia2021semi} sketch-based image retrieval (SBIR). Typically SBIR employs CNN \cite{liveSketch, doodle-to-search}, RNN \cite{sketchmate}, Transformer \cite{sketchformer}, or Vision Transformer \cite{fscoco} based deep triplet-ranking based siamese networks to learn joint embedding space \cite{yu2016shoe}. Contemporary research is also directed towards zero-shot SBIR \cite{doodle-to-search, yelamarthi2018sketch, sain2022sketch3t}, binary hash-code embedding \cite{liu2017hashing, shen2018hashing}, attention mechanism \cite{deep-spatial-semantic}, cross-domain translation \cite{kaiyue2017cross}, reinforcement learning based on-the-fly retrieval \cite{bhunia2020sketch}, semi-supervised \cite{bhunia2021semi}, self-supervised \cite{pang2020jigsaw, vector-raster}, meta-learning \cite{bhunia2022adaptive}, style-agnostic retrieval \cite{styleMeUp}, etc. Apart from object-level images, retrieving sketched objects from scene images was studied using graph convolutional networks \cite{liu2020scenesketcher} and optimal transport \cite{partially-does-it}. In this paper, we show the potential of cross-category FG-SBIR \cite{kaiyue2017cross, bhunia2022adaptive} to solve object detection, a fundamental and challenging task in computer vision for a quarter century. Concretely, we adapt large vision-language models like CLIP using prompt engineering \cite{zhou2022visualprompt} to learn a cross-category FG-SBIR. Next, we formulate object detection as MIL problem and distill knowledge from CLIP image-encoders for supervision.

\section{Proposed Method}
\label{sec:proposed-method}
\keypoint{Overview} We propose a new paradigm training object detection without bounding box annotation or image-level class labels. Instead, we use sketch-based image retrieval for supervision. This leads to several emergent behaviors (i) fine-grained object detection -- specify focused region-of-interest using fine-grained visual cues in sketch. (ii) category-level object detection -- specify the category of detected instances via sketch. (iii) part-level object detection -- detect specified parts (e.g., ``head" and ``legs" of a ``horse").
% (iv) Unlike supervised setups (SOD) that use bounding box annotation or weakly supervised setups (WSOD) that use image-level labels, we introduce extreme weakly supervised setup (EWSOD) that avoids using expensive bounding box annotation or image-level labels of downstream data distribution by exploiting zero-shot potential of CLIP and a conjugate sketch-based image retrieval task.

% \cut{We propose a new paradigm for training object detectors without using bounding box annotation or test data distribution. In particular, }
% We propose an object detection framework that can localise queried objects and has fine-grained alignment with input sketch. Firstly, we train this network without using both bounding-box annotation and hand-drawn sketch. Secondly, we begin with a pre-trained Region Proposal Network (RPN) \cite{faster-rcnn} and fine-tune the backbone feature extractor using a novel object-level cross-category FG-SBIR. Finally, we bridge the object-level and scene-level gap using a custom designed data augmentation scheme.
% We learn an object detection framework which can localise novel objects in a scene that has fine-grained alignment with a given input sketch. Importantly, we train this network without the use of bounding box annotations. To this end, we employ knowledge distillation from a pre-trained cross-category FG-SBIR teacher model to an object detection student model. In addition, to bridge the gap between FG-SBIR models trained using object-level sketch-photo pairs to sketch-guided object detection in scene images, we choose a simple yet effective data augmentation scheme following existing literature on Cut-Mix.

\subsection{Background}\label{sec: background}
Our framework has two key modules -- Object Detection and Sketch-Based Image Retrieval (category-level and fine-grained). For completeness, we give a brief background.

\vspace{-0.3cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{latex/figures/fasterRCNN.pdf}
    \vspace{-0.4cm}
    \caption{Faster-RCNN \cite{faster-rcnn} use image encoder $\mathcal{F}_{d}$ and RPN $\mathcal{R}$ to generates box proposals. Feature maps of proposals, computed via RoI pool $\mathcal{P}$, predicts class probabilities and box regression.}
    \label{fig: faster-rcnn}
    \vspace{-0.3cm}
\end{figure}

\keypoint{Baseline Supervised Object Detection:} We briefly introduce a supervised object detection (SOD) framework, Faster-RCNN \cite{faster-rcnn}, that remains state-of-the-art \cite{yang2018scenegraph, mouzenidis2021grounding, zhu2018relationship}. Given a photo $\mathbf{p} \in \mathbb{R}^{H \times W \times 3}$, a backbone feature extractor (VGG \cite{vgg}, or ResNet \cite{he2015resnet}) $\mathcal{F}_{d}(\cdot): \mathbb{R}^{H \times W \times 3} \rightarrow \mathbb{R}^{H' \times W' \times 512}$ computes feature map $f_{\mathbf{p}} \in \mathbb{R}^{H' \times W' \times 512}$. Next, a two stage process is followed: (i) Given backbone feature map $f_{\mathbf{p}}$, a region proposal network (RPN) $\mathcal{R}: \mathbb{R}^{H' \times W' \times 512} \rightarrow \mathbb{R}^{R \times 5}$ generates rectangular boxes (i.e., proposals) $\mathbf{r} = \{r_1, \dots, r_R\}$, where $\mathbf{r} \in \mathbb{R}^{R \times 4}$ and ``objectness measure" -- a scalar $[0,1]$ probability of the box $r_j$ having an object. (ii) Using proposals $\mathbf{r} \in \mathbb{R}^{R \times 4}$ we pool the feature map $f_{\mathbf{p}}$ via RoI pool \cite{fast-rcnn} to get intermediate feature of size $\mathbb{R}^{7 \times 7 \times 512}$, followed by a fully-connected layer (FC) to get $f_{\mathbf{r}} \in \mathbb{R}^{R \times 512}$ as, $f_{\mathbf{r}} = \mathcal{P}(f_{\mathbf{p}}, \mathbf{r})$. The patch feature $f_{\mathbf{r}}$ is branched into two streams -- a classification branch $\phi_{\mathrm{cls}}: \mathbb{R}^{R \times 512} \rightarrow \mathbb{R}^{R \times (|\mathcal{C}|+1)}$ outputs probability distribution (per RoI) over $\mathcal{C}$ pre-defined classes and a catch-all background class; a box regressions $\phi_{\mathrm{reg}}: \mathbb{R}^{R \times 512} \rightarrow \mathbb{R}^{R \times 4}$ refines initial box predictions $\mathbf{r} \in \mathbb{R}^{R \times 4}$. 
% Although seminal, SOD requires expensive bounding box annotation, equally distributed across $\mathcal{C}$ classes. However, due to dataset bias \cite{torralba2011bias} -- Zipf's law \cite{zipf-law} and social conventions \cite{hendricks2018} -- collecting sufficient bounding box annotation for less occurring classes is near impossible. This motivated weakly supervised object detection (WSOD) that trains without using bounding box annotation.

\keypoint{Baseline SBIR Framework: } We recap a baseline SBIR framework. Given a sketch/photo pair ($\mathbf{s}, \mathbf{p}$), we use a sketch/photo feature extractor to get the feature map $f_{\mathbf{s}} = \mathcal{F}_{\mathbf{s}}(\mathbf{s}) \in \mathbb{R}^{512}$ and $f_{\mathbf{p}} = \mathcal{F}_{\mathbf{p}}(\mathbf{p}) \in \mathbb{R}^{512}$. Category-level SBIR requires ($\mathbf{s}, \mathbf{p}$) from the same category, whereas fine-grained SBIR requires instance-level sketch/photo matching. For training, the cosine distance $\delta(\cdot, \cdot)$ to a sketch anchor ($\mathbf{s}$) from a negative photo ($\mathbf{p}^{-}$), denoted as $\beta^{-} = \delta(f_{\mathbf{s}}, f_{\mathbf{p}^{-}})$ should increase while that from the positive photo ($\mathbf{p}^{+}$), $\beta^{+} = \delta(f_{\mathbf{s}}, f_{\mathbf{p}^{+}})$ should decrease. Training is done via triplet loss with hyperparameter $\mu>0$,
\vspace{-0.1cm}
\begin{equation}\label{eq: triplet}
    \mathcal{L}_{\mathrm{trip}} = \max\{0, \mu + \beta^{+} - \beta^{-} \}
\end{equation}
\vspace{-0.6cm}

To extend FG-SBIR across multiple categories (cross-category FG-SBIR), we train using \cref{eq: triplet} using ``hard-triplets" -- $(\mathbf{s}, \mathbf{p}^{+}, \mathbf{p}^{-})$ have same category, and a class discriminator loss across categories using cross-entropy loss,
\vspace{-0.1cm}
\begin{equation}\label{eq: category}
    \mathcal{L}_{\mathrm{cat}} = -c_{\mathbf{q}}^{i} \log \frac{\exp(\mathcal{F}_{\mathbf{c}}(f_{\mathbf{q}}^{i}))}{\sum_{\forall j} \exp(\mathcal{F}_{\mathbf{c}}(f_{\mathbf{q}}^{j}))}
\end{equation}
\vspace{-0.3cm}

\noindent where, query $\mathbf{q} = \{ \mathbf{s}, \mathbf{p} \}$, $c_{\mathbf{q}}^{i}$ represent class label of $i^{th}$ sample, $\mathcal{F}_{\mathbf{c}}: \mathbb{R}^{512} \rightarrow \mathbb{R}^{|\mathcal{C}|}$ predicts softmax probabilities.

% First, we briefly summarise Faster-RCNN \cite{faster-rcnn}, a baseline object detection (OD) framework that remains state-of-the-art for several downstream tasks \cite{yang2018scenegraph, mouzenidis2021grounding, zhu2018relationship}. Given a photo $\mathbf{p} \in \mathbb{R}^{H \times W \times 3}$, a VGG-16 \cite{vgg} network $\mathcal{F}(\cdot)$ predicts a feature map $f_{\mathbf{p}} \in \mathbb{R}^{H' \times W' \times 512}$. Next, a region proposal network \cite{faster-rcnn} $\mathcal{R}(\cdot)$ generates $R$ rectangular proposals $\mathbf{r} = \{ r_1, r_2, \dots, r_R \}$ with feature $r_j \in \mathbb{R}^{512}$ from the convolutional feature map $f_{\mathbf{p}}$ by sliding a small ($3 \times 3$) spatial window (with effective receptive field $228$ pixels). At each sliding-window location, $k=9$ proposals (anchor boxes) are generated, associated with $3$ scales and $3$ aspect ratio. In the first phase, the proposals $\mathbf{r}$ is fed into two sibling fully-connected layers: (i) a box-regression layer that has $4k$ outputs representing the coordinates of $k$ boxes, and (ii) a box-classification layer that outputs $2k$ scores to estimate the probability of being an object or not (``objectness" measure). In the second phase, for each object proposal $r_j \in \mathbf{r}$, a region of interest (RoI) pooling layer $\mathcal{P}(\cdot)$ extracts a fixed-length feature vector of size ($7 \times 7 \times 512$) from the convolutional feature map $f_{\mathbf{p}}$ which is fed into a sequence of fully connected (FC) layers to get $f_{\mathbf{r}_{j}} = \mathcal{P}(f_{\mathbf{p}}, {r}_{j})$, where $f_{\mathbf{r}_{j}} \in \mathbb{R}^{512}$ branches into two stream output layer: (i) $\phi_{det}: \mathbb{R}^{512} \rightarrow \mathbb{R}^{N+1}$ that produces softmax probability estimates over $N$ object class plus a catch-all ``background" class for detection, and (ii) $\phi_{reg}: \mathbb{R}^{512} \rightarrow \mathbb{R}^{4}$ that regress the refined bounding-box coordinates for the detected region. \cref{fig: faster-rcnn} presents a schematic of the data flow in our baseline Faster-RCNN \cite{faster-rcnn}. The region proposal network $\mathcal{R}$, and the RoI pool based classification and bounding box regression $\mathcal{P}$ is trained alternatively \cite{faster-rcnn}. During training, $\mathcal{R}$ and $\mathcal{P}$ use $2$- or $N$-class cross-entropy and smooth L1 \cite{fast-rcnn} loss for bounding box regression.

% While seminal, baseline Faster-RCNN \cite{faster-rcnn} has some key limitation: (i) it requires expensive bounding box annotation equally distributed across all $N$-classes. However, due to dataset bias \cite{torralba2011bias} -- Zipf's law \cite{zipf-law} and social conventions \cite{hendricks2018} -- collecting sufficient bounding box annotation for less occurring classes is near impossible. We show how weakly supervised object detection paradigm \cite{OICR} resolves this dependency on requiring bounding box annotation.
% (ii) Once trained on $N$-classes, the model fails to detect $(N+1)^{th}$-class, thus limiting real-world open-set applications. (iii) It lacks the flexibility of detecting only user-specified objects of interest (e.g., detecting only `swans' standing on one leg). Therefore, this demands a further investigation on how to adapt our baseline \cite{faster-rcnn} to overcome the aforementioned limitations.

\subsection{Weakly Supervised Object Detection}\label{sec: WSOD}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{latex/figures/wsddn.pdf}
    \caption{Weakly supervised setup (no bounding box) trains using image-level class labels using classification heads ($\phi_{\mathrm{cls}}$). Initial prediction $\omega_{0}$ is refined in $K$ steps with $\phi_{cls}^{*}$ to predict $\omega_{k}$.}
    \label{fig:wsddn}
    \vspace{-0.3cm}
\end{figure}
To avoid collecting expensive bounding box annotation, weakly supervised object detection (WSOD) trains using image-level class labels -- objects of a class is present or not. To avoid using bounding box annotation, we either use a pre-trained region proposal network\footnote{Pre-trained \cite{visualgenome} RPN is highly generalisable to unseen datasets \cite{gu2022open-vocab-OD} due to its generic objective that learns to predict ``objectness" measure.} ($\mathcal{R}$) or heuristic-based selective search \cite{selective-search}, or edge boxes \cite{edgeboxes} that generate box proposals $\mathbf{r} = \{r_1, \dots, r_R\}$. The patch features $f_{\mathbf{r}} = \mathcal{P}(f_{\mathbf{p}}, \mathbf{r})$ is branched into a classification head $x_c = \phi_{\mathrm{cls}}(f_{\mathbf{r}}) \in \mathbb{R}^{R \times (|\mathcal{C}|+1)}$ and a detection head $x_d = \phi_{\mathrm{det}}(f_{\mathbf{r}}) \in \mathbb{R}^{R \times (|\mathcal{C}|+1)}$. The classification head $\phi_{\mathrm{cls}}$ scores individuals proposals into pre-defined $\mathcal{C}$ classes and a catch-all background class via softmax across ($|\mathcal{C}|+1$) class labels
\vspace{-0.1cm}
\begin{equation}
    \sigma_{\mathrm{cls}}(x_c^{(i, j)}) = \frac{ \exp(x_c^{(i, j)}) }{ \sum_{k=1}^{|\mathcal{C}|+1} \exp( x_c^{(i, k)}) }
\end{equation}
The detection head $\phi_{\mathrm{det}}$ measure the contribution of each patch $i$ ($r_i \in \mathbf{r}$) of being classified to class $j$ (in $\mathcal{C}+1$), (i.e., a patch score for each class) via softmax across $R$ regions
\vspace{-0.2cm}
\begin{equation}
    \sigma_{\mathrm{det}}(x_d^{(i, j)}) = \frac{ \exp(x_d^{(i, j)}) }{ \sum_{k=1}^{R} \exp( x_d^{(k, j)}) }
\end{equation}
We train using image-level labels $\mathrm{\mathbf{Y}} = [y_0, y_1, \dots, y_{|\mathcal{C}|}]^{T} \in \mathbb{R}^{(|C|+1) \times 1}$, where $y_{c} = 1$ or $0$ indicates if instance of class $c \in \mathcal{C}$ is present in the image or not. The combined score (element-wise product) of class score $\sigma_{\mathrm{cls}}$ for each patch and a patch score for each class $\sigma_{\mathrm{det}}$ is computed as, $\omega_{0} = \sigma_{\mathrm{cls}}(x_c) \odot \sigma_{\mathrm{det}}(x_d)$. Since we only have image-level class labels, from the combined score $\omega_{0} \in \mathbb{R}^{R \times (|\mathcal{C}|+1)}$, we take the sum over all patches $\hat{y}_{c} = \sum_{i=1}^{R} \omega_{0}^{i, c}$ to get the probability of instances from the $c^{th}$ class present in the image or not. Training happens via multi-class cross entropy
\vspace{-0.2cm}
\begin{equation}\label{eq: ws}
    \mathcal{L}_{\mathrm{ws}} = - \sum_{c=1}^{|\mathcal{C}|+1} y_{c} \log \hat{y}_c + (1 - y_c) \log (1 - \hat{y}_c)
\end{equation}
Unlike SOD using bounding box annotation to refine proposals, WSOD use only image-level class labels that fails to naively refine proposals. Hence, we use an iterative refinement classifier $\omega_{k} = \phi_{\mathrm{cls}}^{*}(f_{\mathbf{r}})$, where $\omega_{k} \in \mathbb{R}^{R \times (|\mathcal{C}|+1)}$ to predict a \emph{refined} class score for each RoI, as shown in \cref{fig:wsddn}. The refinement classifier $\phi_{\mathrm{cls}}^{*}$ is supervised via pseudo scores labels ${l}_{k-1}$ from $(k-1)^{th}$ iteration as, (i) we compute the patches with highest scores in each class $r_{*}^{c} = \arg \max_{r} (\omega_{k-1}^{(r, c)})$. (ii) All regions $r_i \in \mathbf{r}$ that has high overlap with a top scoring patch $r_{*}^{c}$ should be the same class label $c$ as, ${l}_{k-1}^{i, c} = 1$ if $\mathrm{IoU}(r_i, r_{*}^{c}) \geq 0.5$. (iii) If a region $r_i \in \mathbf{r}$ has low overlap with any top scoring patch $r_{*}^{c}$, we assign it to background class $l_{k-1}^{r, 0} = 1$. (iv) If a class $c$ is not in image $\mathbf{p}$ we assign $l_{k-1}^{r, c} = 0$. The refinement loss is
\vspace{-0.1cm}
\begin{equation}\label{eq: ref}
    \mathcal{L}_{\mathrm{ref}}^{k} = \frac{1}{R} \sum_{i=1}^{R} \sum_{c=1}^{|\mathcal{C}|} \omega_{k-1}^{(i, j)} \ l_{k-1}^{(i, j)} \ \log \omega_{k}^{(i, j)}
\end{equation}
Both SOD and WSOD restricts detection to a pre-defined $\mathcal{C}$ classes. In the next section, we overcome this fixed-set limitation using prototype learning with SBIR.

\subsection{Localising Object Regions with Query Sketch}
We replace the fixed-set classifier in WSOD with scalable open-set prototype learning \cite{prototype-segmentation-2021}. We reformulate the classification heads in WSOD $\{\phi_{\mathrm{cls}}, \phi_{\mathrm{det}}, \phi_{\mathrm{cls}}^{*} \}$ (in \cref{sec: WSOD}) from $\mathbb{R}^{R \times 512} \rightarrow \mathbb{R}^{R \times (|\mathcal{C}|+1)}$ to predict embedding vectors $e = \{e_{\mathrm{cls}}, e_{\mathrm{det}}, e_{\mathrm{cls}}^{*} \}$ as $\mathbb{R}^{R \times 512} \rightarrow \mathbb{R}^{R \times 512}$ mapping. Next, we compute a support set (prototypes for each category/instance) $\mathcal{S} = [e_{\mathbf{bg}}, f_{\mathbf{s}}^{1}, f_{\mathbf{s}}^{2}, \dots, f_{\mathbf{s}}^{|\mathcal{C}|}]^{T} \in \mathbb{R}^{512 \times (|\mathcal{C}|+1)}$ by encoding query sketches $\{\mathbf{s}_{1}, \dots, \mathbf{s}_{|\mathcal{C}|} \}$ with a pre-trained sketch encoder ($\mathcal{F}_{\mathbf{s}}$) and a learned catch-all background embedding $e_{\mathbf{bg}} \in \mathbb{R}^{512}$, as shown in \cref{fig:proposed}. The scores $\{x_c, x_d, \omega_{k} \}$ (analogous to \cref{sec: WSOD}) are computed using $\mathcal{S}$ and embedding vectors $e$ of detected regions
\vspace{-0.1cm}
\begin{equation}\label{eq: prototype}
    x_c = e_{\mathrm{cls}} \cdot \mathcal{S}; \hspace{1em} x_d = e_{\mathrm{det}} \cdot \mathcal{S}; \hspace{1em} \omega_{k} = e_{\mathrm{cls}}^{*} \cdot \mathcal{S}
\end{equation}
% Inspired by the flexibility of human expression that sketches provide \cite{sketchrnn2018, fscoco}, we compute the support set $\mathcal{S} = [e_{\mathbf{bg}}, f_{\mathbf{s}}^{1}, f_{\mathbf{s}}^{2}, \dots, f_{\mathbf{s}}^{|\mathcal{C}|}]^{T}$ by sampling $|\mathcal{C}|$ sketch queries ($\mathbf{s}$) and computing their feature representation ($f_{\mathbf{s}}$) using a pre-trained sketch encoder $f_{\mathbf{s}}^{i} = \mathcal{F}_{\mathbf{s}}(\mathbf{s}_{i})$. The background vector $e_{\mathbf{bg}}$ learns its own embedding. 
Carefully choosing a sketch encoder $\mathcal{F}_{\mathbf{s}}$ leads to several properties: (i) pre-training $\mathcal{F}_{\mathbf{s}}$ on category-level SBIR computes $\mathcal{S}$ that detect regions $\mathbf{r}$ with same category as query sketches -- category-level object detection. (ii) pre-training $\mathcal{F}_{\mathbf{s}}$ on cross-category FG-SBIR computes $\mathcal{S}$ where only instance-level aligned regions $\mathbf{r}$ are detected -- fine-grained object detection. (iii) Extending fine-grained object detection with a generalisable (out-of-vocab) sketch encoder $\mathcal{F}_{\mathbf{s}}$ helps to detect object parts (e.g., ``head" of a ``horse") given query sketches -- part-level object detection. We train the object detection modules $\{\mathcal{F}_{d}, \mathcal{P}, \phi_{\mathrm{cls}}, \phi_{\mathrm{det}}, \phi_{\mathrm{cls}}^{*}\}$, using \cref{eq: ws} and \cref{eq: ref} in WSOD (\cref{sec: WSOD}).

While the sketch encoder ($\mathcal{F}_{\mathbf{s}}$) trains object detector via prototypes for each category/instance sketch, we further enhance training efficiency with additional supervision from the photo encoder ($\mathcal{F}_{\mathbf{p}}$), as shown in \cref{fig:proposed}. Specifically, we impose a $L1$-based feature matching loss (analogous to feature distillation \cite{heofeaturedistillation2019}) between patch features $f_{\mathbf{r}}$ from proposals $\mathbf{r}$ in object detector and the photo feature computed for cropped photo regions $\texttt{Crop}(\mathbf{p}, \mathbf{r})$ using pre-trained $\mathcal{F}_{\mathbf{p}}$ as, $\mathcal{L}_{\mathrm{kd}} = ||f_{\mathbf{r}} - \mathcal{F}_{\mathbf{p}}( \texttt{Crop}(\mathbf{p}, \mathbf{r}))||_{1}$. The final loss is,
\vspace{-0.2cm}
\begin{equation}\label{eq:knowledge-distillation}
    \mathcal{L}_{\mathrm{tot}} = \underbrace{\mathcal{L}_{\mathrm{ws}} + \sum_{k=1}^{K} \mathcal{L}_{\mathrm{ref}}^{k}}_{\cref{eq: ws} \text{ and } \cref{eq: ref}} + \lambda \underbrace{ ||f_{\mathbf{r}} - \mathcal{F}_{\mathbf{p}}( \texttt{Crop}(\mathbf{p}, \mathbf{r}))||_{1}}_{\mathcal{L}_{\mathrm{kd}}}
\end{equation}
where the hyperparameter $\lambda=1$. Although, in theory, we can use our baseline SBIR (in \cref{sec: background}), training object detection requires learning a generalised (out-of-vocab) SBIR for category-level or fine-grained sketch/photo matching under wide variations like illumination, complex background, occlusions, unseen categories etc. 

% Although, in theory, a good sketch ($\mathcal{F}_{\mathbf{s}}$) and photo ($\mathcal{F}_{\mathbf{p}}$) encoder can train an object detector in WSOD setup via prototype learning in \cref{eq:knowledge-distillation}, the challenge boils down to learning a generalised (out-of-vocab) SBIR enabling category-level or fine-grained sketch/photo matching under wide variations like illumination, complex background, occlusions, etc. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/sketchOD.pdf}
    \caption{The object detection modules $\{ \mathcal{F}_{d}, \mathcal{P}, \phi_{cls}, \phi_{det}, \phi_{cls}^{*} \}$ are learned using  pre-trained sketch ($\mathcal{F}_{\mathbf{s}}$) and photo ($\mathcal{F}_{\mathbf{p}}$) encoders.}
    \label{fig:proposed}
    \vspace{-0.3cm}
\end{figure}

\subsection{Prompt Learning for Generalised SBIR}\label{sec: prompt-sketch}
To train object detection using SBIR with high generalisation and open-vocab capabilities, we introduce  prompt learning \cite{zhou2022visualprompt} using CLIP \cite{CLIP} for SBIR (both category-level and cross-category fine-grained). CLIP \cite{CLIP} consists an image and text encoder (e.g., ViT \cite{ViT}, or ResNet \cite{he2015resnet}) trained on large $400M$ text/image pairs. This leads to a highly generalisable model that works zero-shot across multiple tasks and datasets. However, adapting CLIP for sketches is tricky since naive fine-tuning leads to model collapse. Hence, we use prompt learning, a set of $P$ learnable vector $\mathbf{v}_{\mathbf{s}} \in \mathbb{R}^{P \times 768}$ for sketch and $\mathbf{v}_{\mathbf{p}} \in \mathbb{R}^{P \times 768}$ for photo, injected into the first layer of ViT to induce CLIP to learn downstream sketch/photo distribution. Importantly, prompting CLIP preserves the desired generalisation ability \cite{zhou2022visualprompt} since the knowledge learned by CLIP is distilled into prompt's weights while keeping the ViT weights frozen. Our new sketch encoder is defined by adapting CLIP's image encoder using sketch prompt ($\mathbf{v}_{\mathbf{s}}$) as, $\mathcal{F}_{\mathbf{s}}(\cdot) = \mathcal{F}_{\mathrm{clip}}(\cdot, \mathbf{v}_{\mathbf{s}})$ and using $\mathbf{v}_{\mathbf{p}}$ for photo encoder as, $\mathcal{F}_{\mathbf{p}}(\cdot) = \mathcal{F}_{\mathrm{clip}}(\cdot, \mathbf{v}_{\mathbf{p}})$. Since ViT weights are frozen, training our CLIP-based SBIR is parameter-efficient -- we train only $\mathbf{v}_{\mathbf{s}} \in \mathbb{R}^{P \times 768}$ and $\mathbf{v}_{\mathbf{p}} \in \mathbb{R}^{P \times 768}$. This allows training with less data, and faster convergence. For category-level SBIR, ($\mathbf{v}_{\mathbf{s}}, \mathbf{v}_{\mathbf{p}}$) learns category inducing prompts using triplet loss (in \cref{eq: triplet}). Learning cross-category FG-SBIR, is slightly more complicated that trains ($\mathbf{v}_{\mathbf{s}}, \mathbf{v}_{\mathbf{p}}$) using hard-triplet in \cref{eq: triplet}, and a modified class discriminative loss \cref{eq: category} that additionally use CLIP's text encoder
\vspace{-0.1cm}
\begin{equation}
    \mathcal{L}_{\mathrm{cat}} = -c_{\mathbf{q}}^{i} \log \frac{\exp(f_{\mathbf{q}}^{i} \cdot f_{\mathbf{t}}^{i})}{\sum_{\forall j} \exp(f_{\mathbf{q}}^{i} \cdot f_{\mathbf{t}}^{j})}
\end{equation}
where, $f_{\mathbf{t}}^{i} \in \mathbb{R}^{512}$ is computed by CLIP's text encoder as, $f_{\mathbf{t}}^{i} = \mathcal{F}_{\mathrm{clip}}^{(\mathbf{t})}(``\texttt{a photo of a }[c_{\mathbf{q}}^{i}]")$ for category $c_{\mathbf{q}}^{i}$. Equipped with our novel prompt-based SBIR, we train open-vocab category-level object detection, fine-grained object detection, or part-level object detection.

\subsection{Bridging Object-Level and Image-Level}\label{sec: object-scene}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/synth-data-sample.pdf}
    \vspace{-0.7cm}
    
    \caption{Bridge object and image-level gap with synthetic photos by tiling $n=\{1, \dots 7\}$ object-level photos in SBIR datasets.}
    \label{fig:data-augmentation}
    \vspace{-0.5cm}
\end{figure}
While SBIR is trained using object-level (single object) sketch/photo pairs, object detection works on image-level (multiple objects) data. Hence, to train object detectors using SBIR, we need to bridge this object and image-level gap. Our solution is embarrassingly simple -- synthesise a canvas of size $(H \times W)$ by randomly tiling $n=\{1, \dots, 7\}$ object-level photos in SBIR datasets \cite{sketchrnn2018, sketchy}. Despite its simplicity, our augmentation trick, analogous to CutMix \cite{cutmix}, improves robustness against input corruptions and out-of-distribution generalisation \cite{mixup, cutmix}. The paired sketches for photos in canvas are used to construct the support set $\mathcal{S}$. Note, we train our object detector without the need to ``see" the evaluation data distribution or use any annotation (bounding box or image-level class labels). We call this setup -- extremely weakly supervised object detection (EWSOD) -- no need to ``see" the downstream data distribution.

\section{Experiments}

\keypoint{Dataset} We train our object detector using existing cross-category FG-SBIR dataset -- Sketchy \cite{sketchy} that contains $125$ categories, each with $100$ photos. Every photo in \cite{sketchy} has at least $5$ instance-level paired sketches. To evaluate fine-grained object detection, we use SketchyCOCO \cite{sketchycoco2020} comprising of natural images in MS-COCO \cite{mscoco} with instance-level paired sketches. Following Liu \etal \cite{liu2020scenesketcher}, we select $1,225$ sketch/photo pairs from SketchyCOCO \cite{sketchycoco2020} with at least one foreground sketched object. We filter the overlapping categories of in SketchyCOCO \cite{sketchycoco2020} from Sketchy \cite{sketchy} to measure true zero-shot performance. For category-level object detection, we train on category-level sketch/photo pairs in QuickDraw-Extended \cite{doodle-to-search} having $330k$ sketches and $204k$ photos from $110$ categories. Following \cite{tripathi2020object}, we evaluate on a subset of standard object detection datasets like PASCAL-VOC \cite{pascalVOC} and MS-COCO \cite{mscoco} that have $20$ and $56$ overlapping categories in QuickDraw \cite{sketchrnn2018}.

\keypoint{Implementation Details} Our model is implemented in PyTorch on a 11GB Nvidia RTX 2080-Ti GPU. First, we train a generalised cross-category FG-SBIR with image size ($224 \times 224$) by adapting CLIP with ViT \cite{ViT} backbone (ViT-B/32 weights) using prompt learning \cite{VPT}. The prompts are trained with triplet loss \cite{yu2016shoe},  margin $\mu=0.3$, Adam optimiser with learning rate $1e-4$ for $60$ epochs, and batch size $64$. Our object detection pipeline is build using Detectron2 \cite{detectron2}. We use FasterRCNN \cite{faster-rcnn}, pretrained on Visual Genome \cite{visualgenome} and remove the RoIPooling \cite{fast-rcnn} and subsequent layers to keep only the pretrained backbone ResNet+FPN ($\mathcal{F}_{d}$) \cite{he2015resnet, lin2017FPN} and Region Proposal Network ($\mathcal{R}$) that generates $1000$ proposals. An alternative is to use handcrafted region proposals like selective search \cite{selective-search}, but we observed slight performance drop. The object detector trains using SGD with batch size $8$ and initial learning rate $5e-3$, multiplied by $0.1$ at $150k$ and $250k$ iterations. We train in a two-step process: (i) keeping $\mathcal{F}_{d}$ and $\mathcal{R}$ fixed, we train the RoI pooling and FC layers ($\mathcal{P}$), classification head ($\phi_{cls}$), detection head ($\phi_{det}$), and refinement head ($\phi_{cls}^{*}$) for $240k$ iterations. (ii) We freeze only $\mathcal{R}$ and finetune all modules for $80k$ iterations. Non-maxima suppression with IoU $\geq 0.3$ is applied to get final predictions.

\keypoint{Evaluation Metric}  (i) For fine-grained object detection, we measure $AP_{.3}$, $AP_{.5}$, and $AP_{.7}$ that computes the average precision (AP) at IoU values $0.3$, $0.5$, and $0.7$. (ii) For category-level object detection, we use measure $AP_{.5}$ and \emph{CorLoc} that computes percentage of images for which the most confident predicted box has IoU $\geq 0.5$ with at least one of the ground-truth boxes for every class. (iii) For cross-category FG-SBIR, we measures $Acc.@q$ -- percentage of sketches having true matched photo in the top-q list, and (iv) mean average precision (mAP), and precision condering top $200$ retrievals P@200 for category-level SBIR.

\subsection{Competitors}  
For object detection, we compare against, (i) supervised object detection (SOD) using both bounding box in addition to sketch/photo annotations: \textbf{Mod-FRCNN} adapts Faster-RCNN \cite{faster-rcnn} for unseen class by concatenating query sketch feature with the RoI pooled feature followed by a binary classifier. \textbf{MatchNet} \cite{matchnet2019} extends \emph{Mod-FRCNN} using co-attention to generate region proposals conditioned on query sketch along with squeeze-and-co-excitation to adaptively re-weight importance distribution of candidate proposals. \textbf{CoAttOD} \cite{tripathi2020object} improves upon \emph{MatchNet} by mitigating the sketch/photo domain misalignment using cross-modal attention. (ii) Weakly supervised object detection (WSOD) trains only on image-level sketch annotations without any additional bounding boxes: \textbf{WSDDN} \cite{wsddn} repurposed object detection as a region classification via multiple instance learning (MIL) paradigm. To inject query sketch to \emph{WSDDN}, we use cross-attention with RoI pooled feature followed by a binary classifier for detection. \textbf{OICR} \cite{OICR} improves \emph{WSDDN} with an iterative MIL to refine initial predictions scores to improve discriminatory power for detection. \textbf{PCL} \cite{pcl2018} generates multiple positive instance in an image via clustering and assigning proposals to the label of corresponding object class for each cluster. \textbf{ICMWSD} \cite{ren2020WSOD} addresses the problem of prior WSOD that focus on the most discriminative part of an object using context information. In particular, \emph{ICMWSD} obtains a ``dropped feature" by dropping the most discriminative parts, followed by maximising the loss of the ``dropped feature" that force the network to look in the surrounding context regions. (iii) We adapt \textbf{$<$Method$>$} in \emph{WSOD} to \textbf{E-$<$Method$>$} that exclusively training on SBIR datasets \cite{sketchrnn2018, sketchy} by synthesising canvas with randomly tiling $n=\{1, \dots, 7\}$ object-level photos and using their paired sketches to construct the support $\mathcal{S}$. We call this setup -- extreme weakly supervised object detection (EWSOD).

For zero-shot category-level SBIR, we compare against: \textbf{GRL} \cite{doodle-to-search} combines similar semantic information (word2vec \cite{word2vec2013}) of class labels with visual sketch information and trains using a gradient reversal layer \cite{grl2015} to reduce sketch/photo domain gap. \textbf{VKD} \cite{wang2022ViTKD} is similar to ours using prototype-learning but employ selective knowledge distillation and ViT \cite{ViT} backbone. For zero-shot cross-category FG-SBIR: \textbf{CDG} is a SOTA domain generalisation method \cite{shankar2018} adapted to cross-category FG-SBIR \cite{pang2019generalising} using categories as domain and inter-category sketch/photo pairs as label. \textbf{CCD} \cite{pang2019generalising} models a universal manifold of prototypical visual sketch traits that dynamically embeds sketch/photo, to generalise to unseen categories. 

\subsection{Generalisibility of Cross-Category FG-SBIR} 
Due to the significant impact of SBIR on training object detectors, it is imperative to learn a powerful cross-category FG-SBIR that is highly generalisable. In other words, the accuracy of SBIR puts a bottleneck on object detection performance. \cref{tab:generalised-SBIR} compares category-level SBIR (CL-SBIR) and cross-category FG-SBIR (CC-FGSBIR) on QuickDraw-Extended \cite{doodle-to-search} and Sketchy \cite{sketchy} respectively, using $100\%$, $70\%$, and $50\%$ of the training set.

\keypoint{Performance Analysis} From \cref{tab:generalised-SBIR} we make the following observations: (i) with decreasing train-set categories, the performance gap (ratio of proposed / SOTA) between the proposed method versus GRL (for CL-SBIR) and CDG (for CC-FGSBIR) increases from $2.1/1.4$ at $100\%$ data to $3.0/4.2$ at $50\%$ data. This shows the high generalisation potential when using prompt-based CLIP models for sketch/photo matching. (ii) Performance gap of proposed versus SOTAs for $100\% \rightarrow 50\%$ is more significant in CC-FGSBIR as compared to CL-SBIR. Hence, it is more difficult to discriminate unseen inter-category sketch/photo pairs than recognise a novel category. (iii) Performance of all competitors in CL-SBIR and CC-FGSBIR are staggeringly inferior to proposed CLIP-based approach. Such a strong SBIR is necessary to unlock training object detection in EWSOD setup (cross-dataset and weakly supervised).


{
\setlength{\tabcolsep}{0.6em}
\begin{table}[]
    \centering
    \footnotesize
    \caption{Quantitative performance of zero-shot category-level SBIR (CL-SBIR) and cross-category FG-SBIR (CC-FGSBIR).}
    \vspace{-1em}
    \begin{tabular}{ccccccc}
        \toprule
        \multirow{2}{*}{\makecell{Train}} &  & \multicolumn{2}{c}{CL-SBIR \cite{doodle-to-search}} & & \multicolumn{2}{c}{CC-FGSBIR \cite{sketchy}} \\
         &  & mAP & P@200 & & Acc.@1 & Acc.@5 \\\hline
        \multirow{3}{*}{$100\%$} & GRL & 9.01 & 6.75 & CDG & 20.1 & 46.4 \\
         & VKD & 15.0 & 29.8 & CCD & 22.6 & 49.0 \\
        \rowcolor{Gray}
         & \multicolumn{1}{c}{\textbf{Ours}} & \textbf{18.2} & \textbf{36.1} & \textbf{Ours} & \textbf{27.6} & \textbf{59.5} \\\hline
        \multirow{3}{*}{$70\%$} & GRL & 6.3 & 5.7 & CDG & 14.6 & 39.5 \\
         & VKD & 9.4 & 17.3 & CCD & 16.3 & 41.4 \\
        \rowcolor{Gray}
         & \multicolumn{1}{c}{\textbf{Ours}} & \textbf{13.1} & \textbf{23.2} & \textbf{Ours} & \textbf{21.0} & \textbf{47.7} \\\hline
        \multirow{3}{*}{$50\%$} & GRL & 3.2 & 2.7 & CDG & 7.9 & 25.4 \\
         & VKD & 4.8 & 6.3 & CCD & 9.2 & 32.2 \\
         \rowcolor{Gray}
         & \multicolumn{1}{c}{\textbf{Ours}} & \textbf{9.6} & \textbf{11.4} & \textbf{Ours} & \textbf{14.7} & \textbf{40.1} \\\bottomrule
    \end{tabular}
    \label{tab:generalised-SBIR}
    \vspace{-0.3cm}
\end{table}
}


\subsection{Category-Level Object Detection}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/sketchOD-retrieval.pdf}
    \vspace{-2em}
    
    \caption{Qualitative retrieval results for cross-category FG-SBIR.}
    \label{fig: retrieval-cross-category}
    \vspace{-1em}
\end{figure}

We benchmark on a subset of standard object detection PASCAL-VOC \cite{pascalVOC} and MS-COCO \cite{mscoco} datasets that have overlapping categories with QuickDraw \cite{sketchrnn2018} sketches. Unlike traditional object detection that detects all instances for known classes in an image, category-level object detection specifies the category of interest by drawing a query sketch.

\keypoint{Performance Analysis} From \cref{tab: detection-category} we observe: (i) best SOD method outperform the best WSOD by an average $AP_{.5}$ margin of $1.7\%/0.1\%$ in VOC/MS-COCO. This shows that although WSOD performs less than SOD (using additional bounding box annotation), the performance gap is not significant as in prior works on seen setup using text as query \cite{gu2022open-vocab-OD, OICR, ren2020WSOD}. In other words, using sketch as labels give nearly similar performance for zero-shot setup for SOD and WSOD. (ii) EWSOD methods further drops $AP_{.5}$ performance of best WSOD method by $5.1\%/1.6\%$. This highlights the lack of generalisation of object detectors to the shift in data distribution when trained on SBIR photos and tested on VOC/MS-COCO. (iii) Despite being trained on the challenging EWSOD setup, our proposed method outperforms best SOD by $14.7/10.9$, WSOD by $16.4/11.0$, and EWSOD by $21.5/13.2$ in zero-shot setup. This shows the extreme generalisation potential of training object detetction using a strong CLIP-based SBIR.

{
\setlength{\tabcolsep}{0.8em}
\begin{table}[]
 \centering
 \footnotesize
 \caption{Quantitative performance of category-level object detection on VOC 2007 and MS-COCO using $AP_{.5}$ and \emph{CorLoc}.}
 \vspace{-1em}
 \begin{tabular}{clcccc}
     \toprule
     \multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c}{VOC 2007 \cite{pascalVOC}} & \multicolumn{2}{c}{MS-COCO \cite{mscoco}} \\
      & & $AP_{.5}$ & CorLoc & $AP_{.5}$ & CorLoc \\\hline
     \multirow{3}{*}{\rotatebox{90}{\textbf{SOD}}} & Mod-FRCNN & 30.1 & 51.2 & 7.4 & 65.8 \\
      & MatchNet & 31.4 & 51.7 & 12.4 & 68.1 \\
      & CoAttOD & 34.6 & 53.9 & 15.0 & \textbf{71.3} \\\hline
     \multirow{4}{*}{\rotatebox{90}{\textbf{WSOD}}} & WSDDN & 20.9 & 40.1 & 11.9 & 67.3 \\
      & OICR & 24.7 & 42.3 & 12.2 & 67.7 \\
      & PCL & 26.1 & 45.5 & 13.8 & 68.6 \\
      & ICMWSD & 32.9 & 52.6 & 14.9 & 69.5 \\\hline
     \multirow{4}{*}{\rotatebox{90}{\textbf{EWSOD}}} & E-WSDDN & 17.7 & 37.9 & 10.1 & 66.7 \\
      & E-OICR & 21.2 & 40.5 & 10.4 & 67.0 \\
      & E-PCL & 22.3 & 41.1 & 11.8 & 67.3 \\
      & E-ICMWSD & 27.8 & 46.3 & 12.7 & 67.9 \\
     \rowcolor{Gray}
      & {\textbf{Proposed}} & \textbf{49.3} & \textbf{69.4} & \textbf{25.9} & {70.3} \\\bottomrule
 \end{tabular}
 \label{tab: detection-category}
\end{table}
}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/category-levelOD.pdf}
    \vspace{-2em}
    
    \caption{Category-Level Object Detection using query sketches with images from MS-COCO \cite{mscoco} and PASCAL-VOC \cite{pascalVOC}.}
    \label{fig: detection-category}
    \vspace{-1em}
\end{figure}
 
\subsection{Fine-Grained Object Detection}
Unlike category-level object detection that detects all instances of sketch-specified category, the goal of fine-grained object detection is to detect only a specific instance for the input query sketch with instance-level alignment.
% Due to instance-specific detection requirement, we do not use \emph{CorLoc} metric that computes over all instance in a particular class.

\keypoint{Performance Analysis} From \cref{tab: detection-finegrained} we observe: (i) SOD methods has nearly similar performance as WSOD and drops for EWSOD, similar to category-level object detection in \cref{tab: detection-category}. (ii) Compared to SOD, the performance of WSOD drops more for $AP_{.5} \rightarrow AP_{.7}$. This is since WSOD methods use less accurate selective search \cite{selective-search} and edge boxes \cite{edgeboxes} for region proposals compared to the more accurate RPN \cite{faster-rcnn} in SOD. (iii) Our proposed method outperforms SOD, WSOD, and EWSOD in zero-shot setup, thereby proving its fine-grained generalisation.
 
{
\setlength{\tabcolsep}{1.2em}
\begin{table}[]
    \centering
    \footnotesize
    \caption{SketchyCOCO detection fine-grained}
    \begin{tabular}{clccc}
        \toprule
        & Method & $AP_{.3}$ & $AP_{.5}$ & $AP_{.7}$ \\\hline
         \multirow{3}{*}{\rotatebox{90}{\textbf{SOD}}} & Mod-FRCNN & 2.5 & 3.5 & 3.1 \\
          & MatchNet & 9.3 & 11.0 & 10.5 \\
          & CoAttOD & 10.4 & 12.1 & 11.7 \\\hline
         \multirow{4}{*}{\rotatebox{90}{\textbf{WSOD}}} & WSDDN & 8.1 & 10.2 & 9.4 \\
          & OICR & 8.9 & 10.9 & 10.0 \\
          & PCL & 9.2 & 11.5 & 10.6 \\
          & ICMWSD & 10.3 & 11.9 & 10.8 \\\hline
         \multirow{4}{*}{\rotatebox{90}{\textbf{EWSOD}}} & E-WSDDN & 6.4 & 8.5 & 7.6 \\
          & E-OICR & 7.1 & 9.1 & 8.3 \\
          & E-PCL & 7.3 & 9.4 & 8.7 \\
          & E-ICMWSD & 8.5 & 10.2 & 9.4 \\
         \rowcolor{Gray}
          & {\textbf{Proposed}} & \textbf{15.0} & \textbf{17.1} & \textbf{16.3} \\\bottomrule
    \end{tabular}
    \label{tab: detection-finegrained}
\end{table}
}
 
 \begin{figure}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/fine-grainedOD.pdf}
    \vspace{-0.7cm}
    \caption{Cross-Category Fine-Grained Object Detection using query sketches with images from SketchyCOCO \cite{sketchycoco2020}.}
    \label{fig: detection-finegrained}
    \vspace{-0.2cm}
\end{figure}
 
\subsection{Part-Level Object Detection}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth, height=0.5\linewidth]{latex/figures/part-levelOD.pdf}
    \vspace{-0.7cm}
    \caption{Unlike traditional object detection that detects an entire object (e.g., a ``cat"), sketches can specify fine-grained region-of-interest to detect a part of an object (e.g., the ``head" of a ``cat").}
    \label{fig: detection-part}
    \vspace{-0.2cm}
\end{figure}

Encouraged with the generalised fine-grained discriminative power of the proposed method in \cref{tab: detection-finegrained}, we go a step further and ask: can we only detect a \emph{part} (e.g., only `head') of an instance? Due to lack of annotation, a quantitative evaluation of part-level object detection is infeasible. Nonetheless, we conduct a small-scale qualitative study by manually editing sketches to create partial sketches of a single part (e.g., only ``head" of ``horse"). \cref{fig: detection-part} presents some results (for more see supplementary). We observe that (i) our proposed method can uniquely detect the sketched `head' region of different objects. (ii) Detection performance is lower for ambiguous part sketches like `leg' (e.g., front-leg, back-leg etc.) (iii) Since detection depends on region proposals from RPN, our model fails to detect tiny sketched parts. Tiny object detection \cite{lee2022tinyOD} is a known challenge for traditional object detection \cite{faster-rcnn}.

% A quantitative evaluation of part-level object detection  requires part-level annotation for both sketch and photo. However, existing fine-grained sketch/photo datasets like SketchyCOCO \cite{sketchycoco2020} lacks such dense annotation. Nonetheless, we conduct a small-scale qualitative study by manually editing sketches to create sketches of a single part. \cref{fig: detection-part} shows (i) our proposed method can uniquely detect the sketched `head' region of different objects. (ii) Detection performance is lower for ambiguous part sketches like `leg' (e.g., front-leg, back-leg etc.) (iii) Since detection depends on region proposals from RPN, our model fails to detect tiny sketched parts. Tiny object detection \cite{lee2022tinyOD} is a known challenge for traditional object detection \cite{faster-rcnn}. We hope future works on fine-grained object detection can resolve this by guiding the RPN towards sketched regions.
 
\subsection{Ablation}
\keypoint{Selective Search v/s Edge Boxes v/s RPN} Unlike the proposed method using pre-trained \cite{visualgenome} RPN to generate $1000$ box proposals, WSOD methods mostly use selective search \cite{selective-search} (SS) or edge boxes \cite{edgeboxes} (EB) that do not need pre-training using box annotation from visual genome \cite{visualgenome}. Hence, for a fair comparison, we replace RPN with SS/EB drops $AP_{.5}$ performance by $1.3$/$2.6$ on SketchyCOCO \cite{sketchycoco2020}.

% Our proposed method use RPN, pre-trained on Visual Genome \cite{visualgenome} dataset, to generate $1000$ object proposals. However, WSOD methods primarily use selective search \cite{selective-search} or edge boxes \cite{edgeboxes} that do not require pre-training using bounding box annotation from Visual Genome \cite{visualgenome}. Hence, for a fair comparison with WSOD, we replace our RPN with selective search (\textbf{Ours-SS}), or edge boxes (\textbf{Ours-EB}) and generate $2000$ proposals. In spite of generating more proposals ($2000$ instead of $1000$ for RPN) and thereby slower inference, our proposed method still outperforms \emph{Ours-SS} by $1.3\%$ and \emph{Ours-EB} by $2.6\%$ on $AP_{.5}$ for fine-grained object detection.
 
\keypoint{Influence of Classifier Refinement} We observe $AP_{.5}$ improve by $2.4$ and $1.1$ for $K=1 \rightarrow 2$ and $K=2 \rightarrow 3$ respectively but a small drop of $0.2$ for $K=3 \rightarrow 4$. 
% Such drop with increasing iteration is typical for iterative approaches \cite{iterativetext2021}.

\keypoint{Influence of Supervision from Photo Encoder in SBIR} Although we can train an object detector using only pre-trained sketch encoder (trained on SBIR) via prototype learning, removing supervision from the photo encoder in SBIR drops $4.5$ in $A_{.5}$ on SketchyCOCO \cite{sketchycoco2020}.

% Distilling from CLIP's image encoder via feature distillation \cite{heofeaturedistillation2019} to the output from RoI pooled feature $f_{\mathbf{r}}$ is computationally expensive process. Each region proposal from RPN is used to crop an image region followed by encoding it via CLIP's image encoder. Hence, during training given an input image, we generate $1000$ proposals that requires passing $1000$ cropped image regions from CLIP's encoder to compute distillation loss $\mathcal{L}_{kd}$. Given the expensive compute nature of $\mathcal{L}_{kd}$ we ask, is it worth it? Removing $\mathcal{L}_{kd}$ from our proposed method leads to $xx.x\%$ drop in $AP_{.5}$. However, increasing our training iterations from $320k$ to $450k$ reduces this drop to $xx.x\%$. Hence, while $\mathcal{L}_{kd}$ helps in faster and better training, removing it can still train an objection pipeline.
 
\subsection{Limitation and Future Works} 
Introducing fine-grained object detection using sketch opens several possibilities that we do not consider. Given multiple query sketches, currently we tread them as independent query embeddings. However, a user might be interested in detecting complex scenes (a ``dog" on the right of a ``person") with \emph{multiple} objects that have meaningful \emph{spatial} alignment. Future works can extend fine-grained object detection or semantic segmentation to complex sketches using the recently introduced FS-COCO \cite{fscoco} dataset.

% Our proposed method essentially matches generated proposals with input query sketches. We assume that each sketch represent a single object or category. However, a user might be interested in detecting complex regions (a ``dog" on the right of a ``person") with \emph{multiple} objects that have meaningful \emph{spatial} alignment. Future works can extend fine-grained object detection or semantic segmentation to complex sketches using the recently introduced FS-COCO \cite{fscoco} dataset.
 
\subsection{Conclusion}
% In this paper, we set out to ask if this ``niche" SBIR can be a game changer for mainstream computer vision tasks like object detection. We show how we can train an object detector using supervision from only SBIR. This is different to existing applications that use sketch as an \emph{additional} support.

In this work, we introduce a new paradigm to object detection -- instead of detecting all regions, how sketches can help detect specified fine-grained region of interest. To further study the practical application, we impose strict constraint on training data -- (i) without collecting additional bounding box annotation for downstream data distribution and relying on existing sketch/photo pair datasets (ii) without having access to unlabelled test-time data distribution used for evaluation. We exploit the popular generalisation potential of CLIP and extend it to sketch/photo matching using prompt learning to learn a state-of-the-art category-level SBIR and cross-category FG-SBIR models. En-armed with this new SOTA sketch/photo matching network as teacher, we train a student object detection framework. The resulting method closes in the performance gap with supervised counterpart, showing the potential of sketch for object detection.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
