% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bbold}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Find Me There: Weakly Supervised Object Detection with Cross-Category \hspace{5em} Fine-Grained Sketch Based Image Retrieval}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth, height=0.5\linewidth]{example-image-a}
    \caption{Explain the Problem Setup we aim to solve}
    \label{fig:problem-setup}
\end{figure}

* \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\

\section{Related Works}
\label{sec:related-works}

\noindent \textbf{Sketch for Visual Understanding:}
Hand-drawn sketches serve as a useful query modality for visual understanding tasks that is enriched with human perception and structural cues. Sketches not only convey a visual depiction \cite{hertzmann2020perception} but also exhibit artistic styles \cite{}. This make sketch a vital query modality for the creative industry like artistic image editing \cite{yang2020surgery} and animation \cite{xing2015autocomplete}. Unlike photos that is passively captured by a camera, sketches are actively drawn by humans which make it a good visual representation \cite{pixelor} enriched with human participation. Apart from the widely explored sketch-based image retrieval \cite{bhunia2021semi, bhunia2020sketch, pang2019generalising, styleMeUp, sketchformer, liveSketch}, sketch as a query shows potential in several vision understanding tasks like incremental learning \cite{bhunia2022incremental} image and video synthesis \cite{cusuh2022synthesis, DeepFaceVideoEditing2022}, representation learning \cite{alaniz2022primitives, clipasso}, image-inpainting \cite{xie2021inpainting}, 3D shape retrieval \cite{xu20223Dretrieval}, 3D shape modeling \cite{chowdhury20223Dsynthesis}, medical image analysis \cite{wang2022medical}, and object localisation \cite{tripathi2020object, riba2021object} and segmentation \cite{hu2020segmentation, qi2022segmentation}.

While sketch as query for object detection was explored by Tripathi \etal \cite{tripathi2020object}, it relied on a supervised setup that (a) requires both bounding box and  object sketch annotation. This increase the annotation budget without significant improvement in performance over traditional object detection setup. (b) Sketch as a query prides in its ability to model fine-grained details \cite{styleMeUp, bhunia2022worrying, bhunia2020sketch}. Specifying object category by drawing sketches is laborious \cite{bhunia2020sketch} in contrast to using text/keyword as a query \cite{CLIP, gu2022open-vocab-OD}, thus overlooking the fine-grained potential of sketch as query.

(b) Sketch as a query prides in its ability to model fine-grained details. However, \cite{tripathi2020object} models sketch on a category-level. Recent progress on vision-language models \cite{CLIP} for object detection \cite{gu2022open-vocab-OD} renders text a better query to specify object categories. (c) Due to its requirement of sketch and bounding box annotation, \cite{tripathi2020object} trains on less than $50\%$ object categories compared to existing datasets \cite{pascalVOC, mscoco}. (d) \cite{tripathi2020object} employs an early fusion of query sketch in the object detection pipeline that renders it slow with increasing query sketches.

% This renders the utility of drawing objects useless.

In this paper, we propose a fine-grained sketch-based object detection method that employs only sketch photo pairs for training and is scalable with multiple query sketches.



% Hand-drawn sketches are enriched with human perception cues that not only convey a realistic image but also exhibits artistic depiction styles \cite{hertzmann2020perception}. Decoding this cognitive human intelligence enables several sketch-based downstream visual understanding tasks. Sketches are useful in the creative industry like artistic image editing \cite{yang2020surgery} and animation \cite{xing2015autocomplete}. Unlike photos that passively interacts with humans, sketching actively stimulates human intelligence with pictionary-style competitive drawing games \cite{pixelor}. Set upon this fact, hand-drawn sketches can efficiently characterise a visual photo and serve as an interface for creativity and human expression. Apart from the widely explored sketch-based image retrieval \cite{bhunia2021semi, bhunia2020sketch, deep-spatial-semantic, pang2019generalising, styleMeUp, sketchformer, doodle-to-search, yelamarthi2018sketch, liveSketch, bhunia2022incremental}, sketch has shown potential in several vision understanding tasks like image and video synthesis \cite{cusuh2022synthesis, DeepFaceVideoEditing2022}, representation learning \cite{alaniz2022primitives, clipasso}, image-inpainting \cite{xie2021inpainting}, 3D shape retrieval \cite{xu20223Dretrieval}, 3D shape modeling \cite{chowdhury20223Dsynthesis}, medical image analysis \cite{wang2022medical}, and object localisation \cite{tripathi2020object, riba2021object} and segmentation \cite{hu2020segmentation, qi2022segmentation}. Sketch as query for object detection was explored in prior literature \cite{tripathi2020object} but for the supervised setup requiring bounding box annotation for each sketched region. In contrast, we use a weakly supervised setup without bounding box annotation and only employing existing SBIR datasets \cite{yu2016shoe, sketchy}. Unlike \cite{tripathi2020object}, we avoid early fusion of query sketch for proposal prediction for faster inference.

\noindent \textbf{Supervised Object Detection:} 
The task of object detection is to jointly localise and recognise objects of interest in an image. Traditional object detectors rely on large supervised object detection datasets such as PASCAL VOC \cite{pascalVOC} and MS COCO \cite{mscoco} which have over hundreds and thousand of time-cosuming annotated examples per object category. Existing literature on object detection is bifurcated into fast but less accurate single-shot \cite{yolo, yolov3, SSD, lin2017focal-loss, centerNet} and slow with higher accuracy two-stage object detectors \cite{rcnn, fast-rcnn, faster-rcnn, maskrcnn}. To fully exploit the fine-grained cues provided by sketch, our proposed method aligns with the two-stage detectors. Two-stage object detection has come a long way from using selective search \cite{rcnn}, ROI pooling \cite{fast-rcnn}, to Region Proposal Network (RPN) \cite{faster-rcnn}. Improving object representation with implicit anchors \cite{faster-rcnn} and robust keypoint estimation \cite{law2018cornerNet, zhou2019grouping, centerNet} leads to improved performance often with increased compute or supervision. Hence, we resort to the more traditional two-stage setup. First, we extract object proposals or bounding boxes susceptible to contain objects and backbone features of input image using residual network \cite{he2015resnet} or visual transformers \cite{caron2021transformers, li2022transformers, DETR}. For second stage, each object proposal is used to resample the backbone features using RoIAlign \cite{maskrcnn} or ROIPool \cite{fast-rcnn} to a fixed size pooled feature. The pooled feature is given to a classification head for object recognition and a regression head for object localisation.

% The task of object detection is to jointly localise and recognise objects of interest in an image. Traditional object detectors rely on large supervised object detection datasets such as PASCAL VOC \cite{pascalVOC} and MS COCO \cite{mscoco} which have over hundreds and thousands of time-consuming annotated examples per object category. The backbone feature extractor comprise of either ImageNet pretrained residual network \cite{he2015resnet} or visual transformers \cite{caron2021transformers, li2022transformers, DETR} that takes an input RGB image to get single \cite{fast-rcnn, faster-RCNN} or multi-scale feature maps \cite{yolov3, lin2017FPN, deformableDETR}. Multi-scale features combines high-resolution lower layers having limited semantic information with semantically-rich higher layers using a Feature Pyramid Network \cite{lin2017FPN}. Given feature maps from the backbone network, the downstream task of jointly object localisation and recognition is done using either the more accurate two-stage \cite{faster-RCNN, maskrcnn} or less accurate but faster single-stage \cite{yolo, yolov3, SSD, lin2017focal-loss, centerNet} pipeline. Two-stage object detectors like Faster-RCNN \cite{faster-RCNN} and its variants \cite{maskrcnn} feed the features to a Region Proposal Network (RPN) that extract object proposals or bounding boxes susceptible to contain objects in the first stage. For second stage, each object proposal is used to resample the backbone features using RoIAlign \cite{maskrcnn} or ROIPool \cite{fast-rcnn} to a fixed size pooled feature. The pooled feature is given to a classification head for object recognition and a regression head for object localisation. Single-stage object detectors \cite{centerNet} also known as proposal-free approaches are simpler and faster than two-stage detectors that directly predicts objects at predefined locations from backbone feature map, but at the cost of lower accuracy. Our proposed sketch-based object detector aligns towards the more accurate two-stage pipeline to leverage the potential of sketch in fine-grained object detection applications \cite{huang2022fewOD} such as animal or plant species recognition.

\noindent \textbf{Weakly Supervised Object Detection (WSOD):} Collecting bounding box annotation per object category is an already time-consuming process. Fine-grained object detection (e.g., recognising animal species) exacerbates this problem. To overcome this challenge, existing WSOD methods adopt two school of thoughts: (a) formulate this task as multiple instance learning (MIL) \cite{dietterich1997MIL, wsddn, li2016wsod, diba2017wsod, jie2017wsod, zhang2018wsod, tang2018wsod, shen2019wsod} problem that interpret an image as bag of proposals or regions. If the image is labeled positive, then one of the regions tightly contain the object of interest. If the image is labeled negative, no region contains the object. We feed an image to a proposal generator (selective search \cite{selectivesearch}, edge boxes \cite{edgeboxes}, sliding window \cite{zhang2019freeanchor}) and backbone network \cite{alexnet, vgg, inception, he2015resnet} to get proposals and feature maps respectively. Next, feature maps and proposals are given to spatial pramid pooling (SPP) \cite{SPPNet} to generate fixed-size regions. Finally, these regions are fed to the detection head to classify and localise object instances. (b) CAM-based methods \cite{zhou2016CAM, zhang2018cam} use class activation maps to predict proposals. Specifically, an image is fed to a backbone network to generate feature vector. This feature vector is given to a classifier to generate prediction scores of an image. Finally, CAM generates proposals or bounding box of each class by thresholding to segment activation maps with the highest probability of every class. In spite of CAM-based methods being faster, in this paper, we use MIL-based technique due to its ability in detecting multiple instances with the same category.



% Collecting bounding box annotation per object category is time-consuming. Fine-grained object detection (e.g., recognising animal species) exacerbates this problem. Existing approaches formulate this task as multiple instance learning (MIL) \cite{dietterich1997MIL, wsddn, li2016wsod, diba2017wsod, jie2017wsod, zhang2018wsod, tang2018wsod, shen2019wsod} problem that interpret an image as bag of proposals or regions. If the image is labeled positive, then one of the regions tightly contain the object of interest. If the image is labeled negative, no region contains the object. We feed an image to a proposal generator (selective search \cite{selectivesearch}, edge boxes \cite{edgeboxes}, sliding window \cite{zhang2019freeanchor}) and backbone network \cite{alexnet, vgg, inception, he2015resnet} to get proposals and feature maps respectively. Next, feature maps and proposals are given to spatial pramid pooling (SPP) \cite{SPPNet} to generate fixed-size regions. Finally, these regions are fed to the detection head to classify and localise object instances. Instead of generating thousands of proposals as in MIL, an CAM-based methods \cite{zhou2016CAM, zhang2018cam} use class activation maps to predict proposals. Specifically, an image is fed to a backbone network to generate feature vector. This feature vector is given to a classifier to generate prediction scores of an image. Finally, CAM generates proposals or bounding box of each class by thresholding to segment activation maps with the highest probability of every class. In spite of CAM-based methods being faster, in this paper, we use MIL-based technique due to its ability in detecting multiple instances with the same category.

\noindent \textbf{Data Augmentation in Computer Vision:} Data augmentation improves the sufficiency and diversity of training data. Approaches varies from simple image transformations like rotation, flipping, and cropping to the more advanced image erasing \cite{devries2017cutout, hide-and-seek, gridmask} and image mix \cite{mixup, cutmix}. Image erasing replace pixel values of one or more sub-regions in the image with constant (Cutout\cite{devries2017cutout}, Hide-and-Seek \cite{hide-and-seek}, GridMask \cite{gridmask}, FenceMask \cite{fencemask}) or random values (random erasing \cite{zheng2020random}). Image mix data combines two or more images or sub-regions into one. MixUp \cite{mixup} learns a linear relationship between mixing images from training set and their supervision signal. CutMix \cite{cutmix} replaces removed regions with a patch from another image to synthesise new images. Our proposed method is similar to CutMix \cite{cutmix} with sub-regions depicting photo of different sketched category from Sketchy dataset \cite{sketchy}. This helps the network learn complex scenes with multiple categories.

\noindent \textbf{Sketch-Based Object Representation} The ability of sketch to offer inherently fine-grained visual description makes it an ideal modality for object retrieval. This commenced avenues like \emph{category-level} \cite{liveSketch, yelamarthi2018sketch, doodle-to-search, sketchformer, sketchmate} and fine-grained (FG) \emph{instance-level} \cite{styleMeUp, bhunia2022worrying, bhunia2020sketch, bhunia2021semi} sketch-based image retrieval (SBIR). Typically SBIR employs CNN \cite{liveSketch, doodle-to-search}, RNN \cite{sketchmate}, Transformer \cite{sketchformer}, or Vision Transformer \cite{fscoco} based deep triplet-ranking to learn joint embedding space \cite{yu2016shoe}. Contemporary research is also directed towards zero-shot SBIR \cite{doodle-to-search, yelamarthi2018sketch, sain2022sketch3t}, cross-domain translation \cite{kaiyue2017cross}, reinforcement learning based on-the-fly retrieval \cite{bhunia2020sketch}, self-supervised \cite{pang2020jigsaw, vector-raster}, etc. Apart from object-level images, retrieving sketched objects from scene images was studied using graph convolutional networks \cite{liu2020scenesketcher} and optimal transport \cite{partially-does-it}. In this paper, we explore the potential of cross-category FG-SBIR \cite{kaiyue2017cross, bhunia2022adaptive} for object detection. Concretely, we adapt large vision-language models like CLIP \cite{CLIP} using prompt engineering \cite{zhou2022visualprompt} to learn a cross-category FG-SBIR. Next, we distill knowledge from CLIP image-encoders for supervision.






% The ability of sketch to offer inherently fine-grained visual description makes it an ideal modality for object retrieval. This commenced avenues like \emph{category-level} \cite{liveSketch, yelamarthi2018sketch, doodle-to-search, sketchformer, sketchmate} and fine-grained (FG) \emph{instance-level} \cite{styleMeUp, bhunia2022worrying, bhunia2020sketch, bhunia2021semi} sketch-based image retrieval (SBIR). Typically SBIR employs CNN \cite{liveSketch, doodle-to-search}, RNN \cite{sketchmate}, Transformer \cite{sketchformer}, or Vision Transformer \cite{fscoco} based deep triplet-ranking based siamese networks to learn joint embedding space \cite{yu2016shoe}. Contemporary research is also directed towards zero-shot SBIR \cite{doodle-to-search, yelamarthi2018sketch, sain2022sketch3t}, binary hash-code embedding \cite{liu2017hashing, shen2018hashing}, attention mechanism \cite{deep-spatial-semantic}, cross-domain translation \cite{kaiyue2017cross}, reinforcement learning based on-the-fly retrieval \cite{bhunia2020sketch}, semi-supervised \cite{bhunia2021semi}, self-supervised \cite{pang2020jigsaw, vector-raster}, meta-learning \cite{bhunia2022adaptive}, style-agnostic retrieval \cite{styleMeUp}, etc. Apart from object-level images, retrieving sketched objects from scene images was studied using graph convolutional networks \cite{liu2020scenesketcher} and optimal transport \cite{partially-does-it}. In this paper, we show the potential of cross-category FG-SBIR \cite{kaiyue2017cross, bhunia2022adaptive} to solve object detection, a fundamental and challenging task in computer vision for a quarter century. Concretely, we adapt large vision-language models like CLIP using prompt engineering \cite{zhou2022visualprompt} to learn a cross-category FG-SBIR. Next, we formulate object detection as MIL problem and distill knowledge from CLIP image-encoders for supervision.

\section{Proposed Method}
\label{sec:proposed-method}
\noindent \textbf{Overview:} We learn an object detection framework which can localise novel objects in a scene that has fine-grained alignment with a given input sketch. Importantly, we train this network without the use of bounding box annotations. To this end, we employ knowledge distillation from a pre-trained cross-category FG-SBIR teacher model to an object detection student model. In addition, to bridge the gap between FG-SBIR models trained using object-level sketch-photo pairs to sketch-guided object detection in scene images, we choose a simple yet effective data augmentation scheme following existing literature on Cut-Mix.

\subsection{Cross-Category Generalisation for FG-SBIR}
\label{sec:generalisation-FGSBIR}
We briefly summarise an existing \cite{bhunia2022adaptive} cross-category FG-SBIR framework. The category specific training and testing data consists of $\mathcal{D}^{S} = \{ \mathcal{D}^{S}_{1}, \mathcal{D}^{S}_{2}, \dots, \mathcal{D}^{S}_{|N^{S}|} \ni \ |N^{S}| > 1 \}$ and $\mathcal{D}^{T} = \{ \mathcal{D}^{T}_{1}, \mathcal{D}^{T}_{2}, \dots, \mathcal{D}^{T}_{|N^{T}|} \ni \ |N^{T}| > 1 \}$, where $N^{S}$ and $N^{T}$ are disjoint sets of training and testing categories respectively, i.e., $N^{S} \cap N^{T} = \phi$. Each $i$-th category comprise of $K^{i}$ paired sketch ($x$) and photo ($y$) as $D_{i} = \{x_{j}, y_{j} \}^{K^{i}}_{j=1}$. A feature extractor $\mathcal{F}_{\theta}: \mathbb{R}^{H \times W \times C} \rightarrow \mathbb{R}^{D}$ parameterised by $\theta$ is used to get the feature map $u = \mathcal{F}_{\theta}(x)$ and $v = \mathcal{F}_{\theta}(y)$ for sketch and photo respectively. During training,  the cosine distance $\delta(\cdot, \cdot)$ of a sketch ($x^{i}$) from a negative photo ($y^{i}_{-}$) of the same $i$-th category, denoted as $\beta^{-}_{i} = \delta(u^{i}, v^{i}_{-})$ should increase while that from the positive photo ($y^{i}_{+}$), $\beta^{+}_{i} = \delta(u^{i}, v^{i}_{+})$ should decrease. Training is done via triplet loss with hyperparameter $\mu > 0$ as,
\begin{equation}
    \mathcal{L}_{trip} = \frac{1}{N} \sum_{i=1}^{N} \max \{0, \mu + \beta^{+}_{i} - \beta^{-}_{i} \}
\end{equation}
While triplet loss helps distinguish between instances of a particular category, a class discriminative objective \cite{discriminative-triplet} helps towards learning to separate between different categories in a cross-category FG-SBIR model. To this end, we add a cross-entropy loss using a classifier $\mathcal{F}_{\theta_{C}}: \mathbb{R}^{D} \rightarrow \mathbb{R}^{|N^{S}|}$ parameterised by $\theta_{C}$. Given the class label $c_{i} \in N^{S}$, the classification loss is defined as,
\begin{equation}
    \mathcal{L}_{C} = \frac{1}{N}\sum_{i=1}^{N} \texttt{Cross\_Entropy}(c_{i}, \texttt{softmax}(\mathcal{F}_{\theta_{C}}(u^{i})))
\end{equation}
Hence, we can define our final training objective as,
\begin{equation}
    \mathcal{L}_{ret} = \mathcal{L}_{trip} + \mathcal{L}_{C}
\end{equation}

\subsection{Object-Level Detection with FG-SBIR}
The proposed object detection framework is trained via knowledge distillation from a cross-category generalised FG-SBIR model without using bounding box annotation. We use the paired sketch ($x$) and photo ($y$) dataset defined in Sec.~\ref{sec:generalisation-FGSBIR} to inject \emph{instance-level mapping} between localised photo regions and the input sketch as shown in Fig.~\ref{fig:object-level-det}. We shall adapt to scene-level complexity in the next section.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth, height=0.3\linewidth]{example-image-a}
    \caption{Sample object-level detection}
    \label{fig:object-level-det}
    \vspace{-1em}
\end{figure}

Given a photo ($y^{i}_{j}$) from the $i$-th category, the object proposals $\mathbf{R} = \{ r_1, r_2, \dots, r_{M} \}$ are generated by Selective Search \cite{selective-search}. Following existing weakly supervised object detection frameworks \cite{wsddn, self-attention-wsddn}, a CNN backbone is used to extract the photo feature maps for $y^{i}_{j}$, from which the proposal feature maps are extracted respectively. Next, these proposal feature maps are fed to a Region-of-Interest (RoI) Pooling layer \cite{faster-rcnn} and two Fully-Connected (FC) layers to obtain proposal feature vectors. These proposal feature vectors are forked into two parallel classification and detection branches where each is fed to a Fully-Connected (FC) layer $f_{cls}(\cdot)$ and $f_{det}(\cdot)$, as shown in Fig.~\ref{}, to generate two vectors $e_{cls} \in \mathbb{R}^{M \times D}$ and $e_{det} \in \mathbb{R}^{M \times D}$ respectively. During training, the cosine similarity $sim(\cdot, \cdot)$ of $e_{cls}$ and $e_{det}$ from a positive sketch ($x^{i}_{+}$) should increase while that from negative sketch ($x^{i}_{-}$) should decrease.
\begin{equation}
    \begin{split}
    z_{cls}(r) = [ sim(e^{r}_{cls}, u^{i}_{+}) ] \cup [sim(e^{r}_{cls}, u^{i}_{-}) \ : \ \forall \ x^{i}_{-} \in \mathcal{D}^{i} ] \\
    z_{det}(r) = [ sim(e^{r}_{det}, u^{i}_{+}) ] \cup [sim(e^{r}_{det}, u^{i}_{-}) \ : \ \forall \ x^{i}_{-} \in \mathcal{D}^{i} ] \\
    \end{split}
\end{equation}
where, $z_{cls}$ and $z_{det}$ are matrices of shape $M \times K_{i}$. While the two streams $z_{cls}$ and $z_{det}$ are remarkably similar, to interpret them as performing classification and detection, we compute $\sigma_{cls}$ and $\sigma_{det}$ by taking softmax along row-wise and column-wise respectively. The final score is computed by taking element-wise (Hadamard) product $\sigma = \sigma_{cls} \odot \sigma_{det}$ of the classification and detection similarities. Finally, the instance-level classification score for the $j$-th instance in $i$-th category is computed as,
\begin{equation}
    \begin{split}
        \mathcal{L}_{sk} = -\frac{1}{NK_{i}} \sum_{i=1}^{N} \sum_{j=1}^{K_{i}} \texttt{Cross\_Entropy}(\mathbb{1}_{j}, \sum_{r=1}^{M} \sigma^{i}_{r, j})
    \end{split}
\end{equation}

To make the training more efficient, we align the detection data stream ($e_{det}$) with FG-SBIR model $\mathcal{F}_{\theta}(\cdot)$ by cropping the photo region using object proposals $\mathbf{R} = \{r_1, r_2, \dots, r_{M}$ and minimising their $\mathcal{L}_{1}$ distance as,
\begin{equation}
    \mathcal{L}_{1} = \frac{1}{M} \sum_{r=1}^{M} || \mathcal{F}_{\theta}(\texttt{crop}(y, r)) - e^{r}_{det} ||_{1}
\end{equation}

Following a weakly supervised object detection setup, it lacks spatial smoothness guarantee present in supervised setups. In particular, Fast-RCNN \cite{fast-rcnn} takes region proposals that have more than $50\%$ IoU with a ground-truth bounding box as positive samples and learns to regress them into their corresponding ground-truth bounding boxes. Since our setup does not have access to ground-truth boxes, we follow a soft regularisation strategy that penalise feature map discrepancies between the highest scoring region and the region with more than $60\%$ IoU, denoted by $\mathbf{\bar{R}} = \{r_1, r_2, \dots, r_{\bar{M}} \}$. Formally,
\begin{equation}
    \begin{split}
        & \hspace{1em} r^{*} = \arg \max (\sigma^{i}_{r}) \\
        \mathcal{L}_{reg} & = \frac{1}{N} \sum_{i=1}^{N} \sum_{r=1}^{\bar{M}} \frac{1}{2} (\sigma^{i}_{r})^{2} ||e^{r^{*}}_{det} - e^{r}_{det}||_{2} \\
    \end{split}
\end{equation}
Hence, we can define our resulting object-level detection loss via knowledge distillation from FG-SBIR and hyperparameters $\lambda_{1}=1$ and $\lambda_{2}=1$ as,
\begin{equation}
    \mathcal{L}_{obj} = \mathcal{L}_{sk} + \lambda_{1} \mathcal{L}_{1} + \lambda_{2} \mathcal{L}_{reg}
\end{equation}
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
