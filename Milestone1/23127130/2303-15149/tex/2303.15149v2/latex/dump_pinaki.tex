\section{Related Works}
\label{sec:related-works}

\noindent \textbf{Sketch for Visual Understanding} Hand-drawn sketches are enriched with human perception cues that not only convey a realistic image but also exhibits artistic depiction styles \cite{hertzmann2020perception}. Decoding this cognitive human intelligence enables several sketch-based downstream visual understanding tasks. Sketches are useful in the creative industry like artistic image editing \cite{yang2020surgery} and animation \cite{xing2015autocomplete}. Unlike photos that passively interacts with humans, sketching actively stimulates human intelligence with pictionary-style competitive drawing games \cite{pixelor}. Set upon this fact, hand-drawn sketches can efficiently characterise a visual photo and serve as an interface for creativity and human expression. Apart from the widely explored sketch-based image retrieval \cite{bhunia2021semi, bhunia2020sketch, deep-spatial-semantic, pang2019generalising, styleMeUp, sketchformer, doodle-to-search, yelamarthi2018sketch, liveSketch, bhunia2022incremental}, sketch has shown potential in several vision understanding tasks like image and video synthesis \cite{cusuh2022synthesis, DeepFaceVideoEditing2022}, representation learning \cite{alaniz2022primitives, clipasso}, image-inpainting \cite{xie2021inpainting}, 3D shape retrieval \cite{xu20223Dretrieval}, 3D shape modeling \cite{chowdhury20223Dsynthesis}, medical image analysis \cite{wang2022medical}, and object localisation \cite{tripathi2020object, riba2021object} and segmentation \cite{hu2020segmentation, qi2022segmentation}. Sketch as query for object detection was explored in prior literature \cite{tripathi2020object} but for the supervised setup requiring bounding box annotation for each sketched region. In contrast, we use a weakly supervised setup without bounding box annotation and only employing existing SBIR datasets \cite{yu2016shoe, sketchy}. Unlike \cite{tripathi2020object}, we avoid early fusion of query sketch for proposal prediction for faster inference.

\noindent \textbf{Supervised Object Detection} The task of object detection is to jointly localise and recognise objects of interest in an image. Traditional object detectors rely on large supervised object detection datasets such as PASCAL VOC \cite{pascalVOC} and MS COCO \cite{mscoco} which have over hundreds and thousands of time-consuming annotated examples per object category. The backbone feature extractor comprise of either ImageNet pretrained residual network \cite{he2015resnet} or visual transformers \cite{caron2021transformers, li2022transformers, DETR} that takes an input RGB image to get single \cite{fast-rcnn, faster-RCNN} or multi-scale feature maps \cite{yolov3, lin2017FPN, deformableDETR}. Multi-scale features combines high-resolution lower layers having limited semantic information with semantically-rich higher layers using a Feature Pyramid Network \cite{lin2017FPN}. Given feature maps from the backbone network, the downstream task of jointly object localisation and recognition is done using either the more accurate two-stage \cite{faster-RCNN, maskrcnn} or less accurate but faster single-stage \cite{yolo, yolov3, SSD, lin2017focal-loss, centerNet} pipeline. Two-stage object detectors like Faster-RCNN \cite{faster-RCNN} and its variants \cite{maskrcnn} feed the features to a Region Proposal Network (RPN) that extract object proposals or bounding boxes susceptible to contain objects in the first stage. For second stage, each object proposal is used to resample the backbone features using RoIAlign \cite{maskrcnn} or ROIPool \cite{fast-rcnn} to a fixed size pooled feature. The pooled feature is given to a classification head for object recognition and a regression head for object localisation. Single-stage object detectors \cite{centerNet} also known as proposal-free approaches are simpler and faster than two-stage detectors that directly predicts objects at predefined locations from backbone feature map, but at the cost of lower accuracy. Our proposed sketch-based object detector aligns towards the more accurate two-stage pipeline to leverage the potential of sketch in fine-grained object detection applications \cite{huang2022fewOD} such as animal or plant species recognition.

\noindent \textbf{Weakly Supervised Object Detection} Collecting bounding box annotation per object category is time-consuming. Fine-grained object detection (e.g., recognising animal species) exacerbates this problem. Existing approaches formulate this task as multiple instance learning (MIL) \cite{dietterich1997MIL, wsddn, li2016wsod, diba2017wsod, jie2017wsod, zhang2018wsod, tang2018wsod, shen2019wsod} problem that interpret an image as bag of proposals or regions. If the image is labeled positive, then one of the regions tightly contain the object of interest. If the image is labeled negative, no region contains the object. We feed an image to a proposal generator (selective search \cite{selectivesearch}, edge boxes \cite{edgeboxes}, sliding window \cite{zhang2019freeanchor}) and backbone network \cite{alexnet, vgg, inception, he2015resnet} to get proposals and feature maps respectively. Next, feature maps and proposals are given to spatial pramid pooling (SPP) \cite{SPPNet} to generate fixed-size regions. Finally, these regions are fed to the detection head to classify and localise object instances. Instead of generating thousands of proposals as in MIL, an CAM-based methods \cite{zhou2016CAM, zhang2018cam} use class activation maps to predict proposals. Specifically, an image is fed to a backbone network to generate feature vector. This feature vector is given to a classifier to generate prediction scores of an image. Finally, CAM generates proposals or bounding box of each class by thresholding to segment activation maps with the highest probability of every class. In spite of CAM-based methods being faster, in this paper, we use MIL-based technique due to its ability in detecting multiple instances with the same category.

\noindent \textbf{Data Augmentation in Computer Vision} Data augmentation improves the sufficiency and diversity of training data. Approaches varies from simple image transformations like rotation, flipping, and cropping to the more advanced image erasing and image mix. Image erasing replace pixel values of one or more sub-regions in the image with constant (Cutout\cite{devries2017cutout}, Hide-and-Seek \cite{hide-and-seek}, GridMask \cite{gridmask}, FenceMask \cite{fencemask}) or random values (random erasing \cite{zheng2020random}). Image mix data combines two or more images or sub-regions into one. MixUp \cite{mixup} learns a linear relationship between mixing images from training set and their supervision signal. CutMix \cite{cutmix} replaces removed regions with a patch from another image to synthesise new images. Our proposed method is similar to \cite{cutmix} where sub-regions depicts the photo of a random sketched category in Sketchy dataset. This helps the network learn complex scenes with photos of multiple sketched categories.

\noindent \textbf{Sketch-Based Object Representation} The ability of sketch to offer inherently fine-grained visual description makes it an ideal modality for object retrieval. This commenced avenues like \emph{category-level} \cite{liveSketch, yelamarthi2018sketch, doodle-to-search, sketchformer, sketchmate} and fine-grained (FG) \emph{instance-level} \cite{styleMeUp, bhunia2022worrying, bhunia2020sketch, bhunia2021semi} sketch-based image retrieval (SBIR). Typically SBIR employs CNN \cite{liveSketch, doodle-to-search}, RNN \cite{sketchmate}, Transformer \cite{sketchformer}, or Vision Transformer \cite{fscoco} based deep triplet-ranking based siamese networks to learn joint embedding space \cite{yu2016shoe}. Contemporary research is also directed towards zero-shot SBIR \cite{doodle-to-search, yelamarthi2018sketch, sain2022sketch3t}, binary hash-code embedding \cite{liu2017hashing, shen2018hashing}, attention mechanism \cite{deep-spatial-semantic}, cross-domain translation \cite{kaiyue2017cross}, reinforcement learning based on-the-fly retrieval \cite{bhunia2020sketch}, semi-supervised \cite{bhunia2021semi}, self-supervised \cite{pang2020jigsaw, vector-raster}, meta-learning \cite{bhunia2022adaptive}, style-agnostic retrieval \cite{styleMeUp}, etc. Apart from object-level images, retrieving sketched objects from scene images was studied using graph convolutional networks \cite{liu2020scenesketcher} and optimal transport \cite{partially-does-it}. In this paper, we show the potential of cross-category FG-SBIR \cite{kaiyue2017cross, bhunia2022adaptive} to solve object detection, a fundamental and challenging task in computer vision for a quarter century. Concretely, we adapt large vision-language models like CLIP using prompt engineering \cite{zhou2022visualprompt} to learn a cross-category FG-SBIR. Next, we formulate object detection as MIL problem and distill knowledge from CLIP image-encoders for supervision.