\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\newcommand{\myparagraph}[1]{\vspace{0pt}\noindent{\bf #1}}
\newcommand{\blue}[1]{\textcolor{blue}{{#1}}}
\newcommand{\red}[1]{\textcolor{red}{{#1}}}
\newcommand{\green}[1]{\textcolor{green}{{#1}}}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{5825} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{\vspace{-2mm}What Can Human Sketches Do for Object Detection?\vspace{-4mm}}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
\noindent We thank all reviewers for their valuable comments. Minor issues will be carefully addressed upon acceptance.

\myparagraph{Response to Reviewer [\texttt{YH6C}]}\\
\noindent \textbf{1. Clarity on detecting multiple similar objects:} 
Our method has two setups: (i) category-level object detection that detects \textit{all} objects similar to query-sketch (Fig.~\red{7}). (ii) Fine-grained object detection that detects objects with fine-grained matching (e.g., same pose, Fig.~\red{8}). Given the sketch of a ``zebra", category-level OD detects multiple ``zebras" (L697), whereas, fine-grained OD will detect a specific ``zebra" out of multiple ``zebras" (L746). Our \emph{limitation} however is that query-sketch can represent only one object at a time (``dog" or ``cat"), not both. To detect both ``cat" and ``dog", we must pass query embeddings independently.

% Our method has two setups: (i) category-level object detection that detects all objects similar to query-sketch (Fig.~\red{7}). (ii) Fine-grained object detection that detects objects with fine-grained matching (e.g., same pose) and ignores the others (Fig.~\red{8}). Given the sketch of a ``zebra", category-level OD will detect multiple ``zebras" in photos (L697). On the other hand, fine-grained OD will detect a specific ``zebra" from a herd of ``zebras" (L746). Our limitation is that sketch query can only represent one object at a time -- ``dog" or ``cat", not both. To detect both ``cat" and ``dog", we must pass the query embeddings independently. We will improve the clarity in (Sec.~\red{4.7}).

\noindent \textbf{2. On occluded objects:} 
As suggested we show examples of detecting occluded ``horse" in images below where a query-sketch shows only ``head" of a ``horse". 
% -- thanks to our part-based object detection ability (L787). We will add more occluded examples and discussion upon acceptance.
Despite a few failures, most are successful thanks to our part-based OD ability (L787). We shall add more cases on acceptance.

\hspace{-2mm} \includegraphics[width=\linewidth]{latex/figures/occlusion.pdf}


\noindent \textbf{3. Multiple boxes in Fig.~\red{9}:} The multiple boxes are \emph{not} false positives. The \textit{blue} boxes are initial proposals with confidence score $\omega_{k}$ $\geq$ $0.7$ for each detected object. Using NMS thereafter predicts the \textit{yellow} box for those object(s). 
% The multiple boxes in Fig.~\red{7}, \red{8}, \red{9} are \emph{not} false positives. They are initial proposals (\texttt{blue boxes}) with confidence score $\omega_{k} \geq 0.7$ (Eq.~\red{7}) before using using non-maximum suppression (NMS) to get a single box for a single object marked as \texttt{yellow box} (L581).

\noindent \textbf{4. Extending to open-world setup:} Good point!
In open-world setup, a model trained on $C$ known classes can recognise unknown classes and update the base model via incremental learning [\green{A}]. 
Our method already works in open-world setup as it detects in zero-shot, \emph{open-vocab} setup (L134) \textit{i.e} it works regardless of the query-sketch being in train set or not. 
% Our method already detects in zero-shot, \emph{open-vocab} setup (L134) \textit{i.e} it works regardless of the query-sketch being in train set or not. 
We will cite [\green{B}] on acceptance.

% Thanks! Open-world setup is defined as a model with $C$ known classes in the train set can recognise unknown classes and update the base model via incremental learning [\green{A}]. Our method already works for zero-shot in open-vocab setup (L134) -- the model can detect any queried sketch irrespective of whether it is in the training set or not. This is enabled through a combination of prototype learning and generalised SBIR (with open-vocab capabilities, L478) with CLIP and prompt learning. We will cite [\green{B}] and clarify upon acceptance.

\myparagraph{Response to Reviewer [\texttt{UztV}]}\\
\noindent \textbf{1. Clarity on object detection setup:}
Ah, Case (A) is clearly a misunderstanding. An OD setup [\green{54}] requires querying (via \textit{sketch} in our case) on what to detect. The task is to detect \textit{all similar instances} of the query from a gallery of photos (L105), not localise (bounding box) the object from a single image -- i.e. bounding box is the output not the input. For further clarity on our OD setups, see \textbf{[YH6C]-1}. Indeed Case (B) is \textit{our} setup, which has been studied throughout (L551). We show more detection results below, as suggested, which will be added upon acceptance.
% For more clarity, we update detection results for category-level OD (out-of-vocab setup, see L551) below..

% We do not sketch an object with a single image in front. Instead, we draw an object from \emph{our imagination} and detect all similar instances from a gallery of photos (L105). We will clarify this upon acceptance. (i) For category-level OD, we detect all similar objects (e.g., ``zebra" in different poses) across all photos in the gallery, (ii) for fine-grained OD, we detect a specific object (e.g., ``zebra" in the same pose as sketch) from multiple photos in gallery. For more clarity, we update detection results for category-level OD (out-of-vocab setup, see L551) below.}
% \blue{This is clearly a misunderstanding. We do not sketch an object with a single image in front. Instead, we draw an object from \emph{our imagination} and detect all similar instances from a gallery of photos (L105). We will clarify this upon acceptance. (i) For category-level OD, we detect all similar objects (e.g., ``zebra" in different poses) across all photos in the gallery, (ii) for fine-grained OD, we detect a specific object (e.g., ``zebra" in the same pose as sketch) from multiple photos in gallery. For more clarity, we update detection results for category-level OD (out-of-vocab setup, see L551) below.}
% \blue{This is clearly a confusion. We want to detect objects from a gallery of photos that match the query-sketch. (i) For category-level OD, we detect all similar objects (e.g., ``zebras" in different poses) across all photos in the gallery, (ii) for fine-grained OD, we detect a specific object (e.g., ``zebra" with same pose as sketch) from the photos in the gallery. For more clarity, we updated detection results for category-level OD below. Note, our method is NOT limited to detecting objects in a single image. Given query-sketch, we detect objects (different poses for category-level OD, and same pose for fine-grained OD) from multiple photos in gallery.}

\hspace{-4mm} \includegraphics[width=\linewidth]{latex/figures/category-OD-gallery.pdf}\vspace{-0.1cm}

\noindent \textbf{2. Experiment on part-level detection:} Good point! A quantitative evaluation of part-level object detection is unfortunately infeasible due to lack of annotations. Instead, we put forward Mean Opinion Score (MOS) by asking $10$ people to draw $20$ part-level sketches and rate from $1$ to $5$ (bad$\rightarrow$execellent) based on their opinion of how closely the queried part was detected. Accordingly, we obtain a MOS (mean$\pm$variance of $200$ responses) of $3.67 \pm 0.6$. We shall add details on human study and release code on acceptance.

% More details on human study, and additional out-of-vocab examples will be included in the supplementary, and RELEASE the CODE upon acceptance.
% (Fig. in \texttt{[UztV]}Q1)

\noindent \textbf{3. Detecting objects in different poses:} Our object detection has multiple setups: (i) for category-level OD, the sketch of object O1 (``zebra") in image I1 will detect the same object O1 in a different image I2 \emph{even with a different pose} (``sitting" or ``standing"). (ii) For fine-grained OD, the sketch of object O1 in image I1 will only detect the same object O1 in a different image I2 if it has the \emph{same pose}, e.g., detect only ``zebras \textit{sitting down}" amongst a herd of ``zebras".  Figure below shows qualitative results for clarity.
\includegraphics[width=\linewidth, height=0.2\linewidth]{latex/figures/category-vs-finegrained-OD.pdf}

\noindent \textbf{4. Additional Ablation:} (i) \textit{Varying prompt length} $P=\{1, 3, 5\}$ in $\{\mathbf{v}_{\mathbf{s}}, \mathbf{v}_{\mathbf{p}} \} \in \mathbb{R}^{P \times 768}$ changes $AP_{.5}$ to $16.5$, $17.1$, and $15.9$ on SketchyCOCO~[\green{23}] respectively. (ii) \textit{Replacing CLIP} with VGG-based sketch encoder $\mathcal{F}_{\mathbf{s}}$ sharply drops $AP_{.5}$ to $9.1$ (iii) \textit{Increasing tiling} from $n\in[1,7]$ to $n\in[1,17]$ reduces $AP_{.5}$ to $11.3$ due to high occlusion ($n\rightarrow17$). We will add more details on acceptance.

\noindent \textbf{Miscellaneous:} (i) We will release the code for clarity on training details and tiling. (ii) See \textbf{[uNLK]-1} for in-depth \emph{analysis on tiling}. (iii) We will cite DETR, and rephrase wherever suggested (e.g. L153) on acceptance. (iv) \texttt{Yellow boxes} are NOT GT (see \textbf{[YH6C]-3}). (v) Naive fine-tuning \texttt{collapses CLIP} ($AP_{.5}$ to $3.6$ on [\green{23}]) due to small data size ($<10k$) in sketches. 


% rephrase Faster-RCNN as a ``go to choice" in multiple downstream tasks. 
% (vi) We will cite DETR and rephrase our statement in L153 on acceptance. 

\myparagraph{Response to Reviewer [\texttt{uNLK}]}\\
\noindent \textbf{1. Robustness of tiling:} {To test robustness, we generate occluded photos by randomly masking $(10\%, 30\%, 50\%)$ of GT object boundaries, with zero pixel values, and measure the respective drop in accuracy ($AP_{.5})$ on [\green{23}]. Performance drop being less \textit{with tiling} for E-WSDDN (by $\{1.7, 3.4, 5.7\}$) or our method (by $\{1.6, 3.3, 5.4\}$), than \textit{without tiling} in WSDDN (by $\{3.1, 5.2, 7.5\}$) verifies robustness due to tiling on object detection. Further details on this analysis will be added upon acceptance.}


% Performance of WSDDN (w/o tiling) drops by $\{3.1, 5.2, 7.5\}$, E-WSDDN (w/ tiling) by $\{1.7, 3.4, 5.7\}$, and Proposed (w/ tiling) by $\{1.6, 3.3, 5.4\}$ respectively. This shows the robustness due to tiling on object detection. More details on this analysis will be added upon acceptance.}

% \noindent \textbf{Q1: How tiling helps robustness:}
% To examine the robustness of tiling we synthetically occlude $10\%, 30\%$, and $50\%$ of object regions in photos with zeros (as pixel values). Comparing WSDDN (w/o tiling), E-WSDDN (w/ tiling) and proposed (w/ tiling), the $AP_{.5}$ in [\green{23}] drop by \{$3.1$, $1.7$, $1.6$\} for $10\%$, \{$5.2$, $3.4$, $3.3$\} for $30\%$, and \{$7.5$, $5.7$, $5.4$\} for $50\%$ occlusion respectively. We observe using tiling leads to greater robustness in occluded object detection.

\noindent \textbf{2. Novelty concerns:} 
(i) Our novelty lies in adapting well-known modules (CLIP, SBIR) to train an object detector from only object-level sketch-photo pairs (each photo has only one object) without any bounding-box annotations. (ii) CutMix is a data \textit{augmentation} tool that typically replaces a patch in one \textit{existing} scene-photo with that from another. Contrarily, tiling is a data \textit{synthesis} tool that combines multiple object-level photos in SBIR dataset to \textit{newly create} a scene-photo for subsequent training.

% CutMix is a data augmentation tool typically used on scene-level photos. It replaces a patch in one image with that from another. Contrarily, tiling is a data synthesis tool that combines multiple object-level photos in SBIR dataset to create a scene photo. This generated scene photo enables training object detection.

% Image I1 with that from I2
% (i) Our contribution lies in training an object detector from only object-level sketch-photo pairs (each photo has only one object) with the novelty in adapting well-known modules (CLIP, SBIR) towards our motivation, without bounding-box annotations.



% \blue{First, we introduce prompt learning for generalised SBIR to give significant improvement ($>22\%$) in CC-FGSBIR (Tab.~\red{1}). This is important since our object detector is trained using SBIR. Second, despite the technical similarity, CutMix was used as a data augmentation tool whereas we use tiling as a bridge that trains scene-level representation in object detection using object-level SBIR data only}.

\myparagraph{References:} [A] Bendale \etal ``Towards $\dots$ '', CVPR 2015
[B] Joseph \etal ``Towards $\dots$ detection'', CVPR 2021

%%%%%%%%% REFERENCES
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

\end{document}
