\documentclass[a4paper, 12pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage[colorlinks = false, pdfborder={0 0 0}]{hyperref}

\title{
    ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks\footnote{This project received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant agreement nr. 883121). We thank Fabio Melliger and Paula Moser for excellent research assistance and Elliott Ash, Juan Bermeo, Emma Hoes, Jonathan Klüser, Maria Korobeynikova, and Natalia Umansky for helpful feedback.}
}

\author{
    Fabrizio Gilardi\footnote{Department of Political Science, University of Zurich (\url{https://fabriziogilardi.org/})}
    \and
    Meysam Alizadeh\footnote{Department of Political Science, University of Zurich (\url{https://malizad.github.io/})}
    \and
    Maël Kubli\footnote{Department of Political Science, University of Zurich (\url{https://maelkubli.ch/})}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than \$0.003---about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.
\end{abstract}

\onehalfspace

\newpage

\section{Introduction}

Many NLP applications require high-quality labeled data, notably to train classifiers or evaluate the performance of unsupervised models. For example, researchers often aim to filter noisy social media data for relevance, assign texts to different topics or conceptual categories, or measure their sentiment or stance. Regardless of the specific approach used for these tasks (supervised, semi-supervised, or unsupervised), labeled data are needed to build a training set or a gold standard against which performance can be assessed. Such data may be available for high-level tasks such as semantic evaluation\footnote{e.g., \url{https://semeval.github.io/SemEval2023/tasks.html}.} and hate speech \citep{ElSherief:2021aa}, as well as, sometimes, more specific tasks such as party ideology \citep{Herrmann:2023aa}. More typically, however, researchers have to conduct original annotations to ensure that the labels match their conceptual categories \citep{Benoit:2016rt}. Until recently, two main strategies were available. First, researchers can recruit and train coders, such as research assistants. Second, they can rely on crowd-workers on platforms such as Amazon Mechanical Turk (MTurk). Often, these two strategies are used in combination: trained annotators create a relatively small gold-standard dataset, and crowd-workers are employed to increase the volume of labeled data. Each strategy has its own advantages and disadvantages. Trained annotators tend to produce high-quality data, but involve significant costs. Crowd workers are a much cheaper and more flexible option, but the quality may be insufficient, particularly for complex tasks and languages other than English. Moreover, there have been concerns that MTurk data quality has decreased \citep{Chmielewski:2020aa}, while alternative platforms such as CrowdFlower and FigureEight are no longer practicable options for academic research since they were acquired by Appen, a company that is focused on a business market.

This paper explores the potential of large language models (LLMs) for text annotation tasks, with a focus on ChatGPT, which was released in November 2022. It demonstrates that zero-shot ChatGPT classifications (that is, without any additional training) outperform MTurk annotations, at a fraction of the cost. LLMs have been shown to perform very well for a wide range of purposes, including ideological scaling \citep{Wu:2023aa}, the classification of legislative proposals \citep{Nay:2023aa}, the resolution of cognitive psychology tasks, \citep{binz2023using}, and the simulation of human samples for survey research \citep{Argyle:2023aa}. While a few studies suggested that ChatGPT might perform text annotation tasks of the kinds we have described \citep{kuzman2023chatgpt, huang2023chatgpt}, to the best of our knowledge no systematic evaluation has been conducted yet. Our analysis relies on a sample of 2,382 tweets that we collected for a previous study \citep{alizadeh2022content}. For that project, the tweets were labeled by trained annotators (research assistants) for five different tasks: relevance, stance, topics, and two kinds of frame detection. Using the same codebooks that we developed to instruct our research assistants, we submitted the tasks to ChatGPT as zero-shot classifications, as well as to crowd-workers on MTurk. We then evaluated the performance of ChatGPT against two benchmarks: (i) its accuracy, relative to that of crowd-workers, and (ii) its intercoder agreement, relative to that of crowd workers as well as of our trained annotators. 

We find that for four out of five tasks, ChatGPT's zero-shot accuracy is higher than that of MTurk. For all tasks, ChatGPT’s intercoder agreement exceeds that of both MTurk and trained annotators. Moreover, ChatGPT is significantly cheaper than MTurk: the five classification tasks cost about \$68 on ChatGPT (25,264 annotations) and \$657 on MTurk (12,632 annotations) (see Section \ref{sec:matmeth} for details). ChatGPT's per-annotation cost is therefore about \$0.003, or a third of a cent---about twenty times cheaper than MTurk, with higher quality. At this cost, it might potentially be possible to annotate entire samples, or to create large training sets for supervised learning. Based on our tests, 100,000 annotations would cost about \$300. 

While further research is needed to better understand how ChatGPT and other LLMs perform in a broader range of contexts, these results demonstrate their potential to transform how researchers conduct data annotations, and to disrupt parts of the business model of platforms such as MTurk. 

\section{Results}

We use a dataset of 2,382 tweets that we annotated manually for a previous study on the discourse around content moderation \citep{alizadeh2022content}. Specifically, we relied on trained annotators (research assistants) to construct a gold standard for five conceptual categories with different numbers of classes: relevance of tweets for the content moderation issue (relevant/irrelevant); stance regarding Section 230, a key part of US internet legislation (keep/repeal/neutral); topic identification (six classes); a first set of frames (content moderation as a problem, as a solution, or neutral); and a second set of frames (fourteen classes) (see Section \ref{sec:matmeth} for details).

We then performed these exact same classifications with ChatGPT and with crowd-workers recruited on MTurk, using the same codebook we developed for our research assistants (see Appendix \ref{sec:prompt}). For ChatGPT, we conducted four sets of annotations. To explore the effect of ChatGPT's temperature parameter, which controls the degree of randomness of the output, we conducted the annotations with the default value of 1 as well as with a value of 0.2, which implies less randomness. For each temperature value, we conducted two sets of annotations to compute ChatGPT's intercoder agreement. For MTurk, we aimed to select the best available crowd-workers, notably by filtering for workers who are classified as ``MTurk Masters'' by Amazon, who have an approval rate of over 90\%, and who are located in the US. Our procedures are described in detail in Section \ref{sec:matmeth}.

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{plot_ChatGPT.pdf}
\caption{\emph{ChatGPT zero-shot text annotation performance, compared to MTurk and trained annotators. ChatGPT's accuracy outperforms that of MTurk for four of the five tasks. ChatGPT's intercoder agreement outperforms that of both MTurk and trained annotators in all tasks. Accuracy means agreement with the trained annotators.}}
\label{fig:performance}
\end{figure}

We report ChatGPT's zero-shot performance for two different metrics: accuracy and intercoder agreement (Figure \ref{fig:performance}). Accuracy is measured as the percentage of correct annotations (using our trained annotators as a benchmark), while the intercoder agreement is computed as the percentage of tweets that were assigned the same label by two different annotators (research assistant, crowd-workers, or ChatGPT runs). Regarding accuracy, Figure \ref{fig:performance} shows that ChatGPT outperforms MTurk for four out of five tasks. Of these four tasks, in one case (relevance) ChatGPT has a slight edge but its performance is very similar to that of MTurk. In the other three cases (frames I, frames II, and stance), ChatGPT's performance is between 2.2 and 3.4 times higher than that of MTurk. Moreover, the level of accuracy of ChatGPT is, overall, more than adequate, considering the difficulty of the tasks, the number of classes, and the fact that the annotations are zero-shot. For relevance, with two classes (relevant/irrelevant), ChatGPT's accuracy is 72.8\%, while for stance it is 78.7\% with three classes (positive/negative/neutral). As the number of classes increases, accuracy decreases, although the intrinsic difficulty of the task also plays a role. Regarding intercoder agreement, Figure \ref{fig:performance} shows that ChatGPT's performance is very high, over 95\% for all tasks when the temperature parameter is set at 0.2. These values are higher than any of the human intercoder agreements, including trained annotators. Even with the default temperature value of 1, which implies more randomness, intercoder agreement consistently exceeds 84\%. The relationship between intercoder agreement and accuracy is positive, but weak (Pearson's correlation coefficient: 0.17). Although the correlation is based on just five data points, it suggests that a lower temperature value may be preferable for annotation tasks, as it seems to increase the consistency of the results without decreasing accuracy significantly.

We underscore that the test to which we subjected ChatGPT is hard. Content moderation is a complex topic, and the tasks are not a toy example. Instead, they are tasks that we conducted in the context of a previous study \citep{alizadeh2022content}, and which required considerable resources. With the exception of stance, we developed the conceptual categories for our particular research purposes, and the definitions required extensive discussions in the team. Moreover, some of the tasks involve a large number of classes, as many as fourteen for ``Frames II''. The difficulty of this task is demonstrated by the low degree of intercoder agreement among our trained annotators (about 50\%). Stance classification involved only three classes, but intercoder agreement among our trained annotators was relatively modest (78.3\%), which shows that also this task was not straightforward. However, ChatGPT still achieved an accuracy of 78.6\% for stance. We conclude that the performance of ChatGPT is impressive, particularly considering that its annotations are zero-shot.
\section{Discussion}

This paper demonstrates the potential of LLMs to transform text-annotation procedures for a variety of tasks common to many research projects. Despite the focus on a single dataset and the relatively limited number of tests, the evidence strongly suggests that LLMs may already be a superior approach compared to crowd-annotations on platforms such as MTurk. At the very least, the findings demonstrate the importance of studying the text-annotation properties and capabilities of LLMs more in depth. The following questions and steps seem particularly promising: (i) performance of ChatGPT across multiple languages; (ii) performance of ChatGPT across multiple types of text (social media, news media, legislation, speeches, etc.); (iii) implementation of few-shot learning on ChatGPT, compared with fine-tuned models such as BERT and RoBERTa; (iv) construction of semi-automated data labeling systems in which a model first learns by observing human annotations, and is then used to recommend or even automate the labeling \citep{desmond2021semi}; (v) using chain of thought prompting and other strategies to increase the performance of zero-shot reasoning \citep{kojima2022large}; (vi) and of course implementation of annotation tasks with GPT-4, as soon as availability permits. 

\section{Materials and Methods} \label{sec:matmeth}

\subsection*{Twitter Data}
We used tweets about the issue of content moderation that we collected for a previous study \citep{alizadeh2022content}. The original dataset covers all tweets containing relevant keywords and hashtags, posted between January 2020 and April 2021 (2,586,466 original tweets, 154,451 quoted retweets, and 6,991,897 retweets by 3,445,376 total unique users). In this study, we focused on a subset of 2,382 tweets that were labeled by our trained annotators.

\subsection*{Text Classification Tasks}\label{sec:annotation}
We implemented several text classification tasks: (1) \textit{relevance}: whether a tweet is about content moderation; (2) \textit{topic detection}: whether a tweet is about a set of six pre-defined topics (i.e. Section 230, Trump Ban, Complaint, Platform Policies, Twitter Support, and others); (3) \textit{stance detection}: whether a tweet is in favor of, against, or neutral about repealing Section 230 (a piece of US legislation central to content moderation); (4) \textit{general frame detection (``frames I'')}: whether a tweet contains a set of two opposing frames which we call them ``problem' and ``solution'' frames. The solution frame describes all tweets framing the issue of content moderation as a solution to other issues (e.g., hate speech). The problem frame, on the other hand, describes all tweets framing content moderation as a problem on its own as well as to other issues (e.g., free speech); (5) \textit{policy frame detection (``frames II'')}: whether a tweet contains a set of fourteen policy frames proposed in \citep{card2015media}. The full text of instruction for the five annotation tasks is presented in Appendix \ref{sec:prompt}. We used the exact same wordings for ChatGPT and crowd-sourcing annotations.

\subsection*{Human Annotation}
We trained a team of two political science graduate students to annotate tweets for all five tasks explained in Section \ref{sec:annotation}. For each task, the coders were given the same set of instructions described in Section \ref{sec:annotation} and detailed in Appendix \ref{sec:prompt}. Then, the coders were asked to independently annotate the tweets task by task. To compute the accuracy of ChatGPT and MTurk, we considered only tweets that both trained annotators agreed upon.

\subsection*{Crowd-Workers Annotation}

We employed MTurk workers to annotate the same set of tasks as for the human annotators and ChatGPT. We provided the MTurk crowd workers the same set of instructions as for the others as described in Section \ref{sec:annotation}. To maximize annotation quality, we restricted access to the tasks to workers who are classified as ``MTurk Masters'' by Amazon, who have a HIT (Human Intelligence Task) approval rate greater than 90\% with at least 50 approved HITs, and who are located in the US. Moreover, we ensured that no worker could annotate more than 20\% of the tweets for a given task. As with the trained human annotators, each tweet was annotated by two different crowd-workers.  

\subsection*{ChatGPT Annotation}

We used the ChatGPT API with the `gpt-3.5-turbo' version to classify the tweets. The annotation was conducted between March 9 and March 20, 2023. For each annotation task, we prompted ChatGPT with the corresponding annotation instruction text described in Section \ref{sec:annotation} of \textit{Material and Methods} and detailed in Appendix \ref{sec:prompt}. We intentionally avoided adding any ChatGPT-specific prompts such as `Let's think step by step' to ensure comparability between ChatGPT and MTurk crowd-workers. After testing several variations, we decided to feed tweets one by one to ChatGPT using the following prompt: ``Here's the tweet I picked, please label it as [Task Specific Instruction (e.g. `one of the topics in the instruction')].'' We set the \textit{temperature} parameter at 1 (default value) and 0.2 (which makes the output more focused and deterministic as opposed to higher values, which make the output more random) to explore differences across all tasks. For each temperature setting, we collected two responses from ChatGPT to compute the intercoder agreement. That is, we collected four ChatGPT responses for each tweet. We create a new chat session for every tweet to ensure that the ChatGPT results are not influenced by the history of annotations.

\subsection*{Evaluation Metrics}

We consider the trained human annotations as our gold standard and compare the performance of ChatGPT and MTurk crowd-workers with it. First, we compute average accuracy (i.e. percentage of correct predictions), that is, the number of correctly classified instances over the total number of cases to be classified. Second, we compared the intercoder agreement of ChatGPT, MTurk crowd-workers, and trained human annotators. To measure intercoder agreement, we computed the percentage of instances for which both annotators in a given group are reporting the same class.


\bibliography{main.bib}
\bibliographystyle{chicago}

\newpage
\appendix
%\pagenumbering{arabic}
\renewcommand{\thesection}{S\arabic{section}}

\section{Annotation Codebook}\label{sec:prompt}
\subsection{Background on content moderation (to be used for all tasks)}
For this task, you will be asked to annotate a sample of tweets about content moderation. Before describing the task, we explain what we mean by “content moderation”. 

“Content moderation” refers to the practice of screening and monitoring content posted by users on social media sites to determine if the content should be published or not, based on specific rules and guidelines. Every time someone posts something on a platform like Facebook or Twitter, that piece of content goes through a review process (“content moderation”) to ensure that it is not illegal, hateful or inappropriate and that it complies with the rules of the site. When that is not the case, that piece of content can be removed, flagged, labeled as or ‘disputed.’ 

Deciding what should be allowed on social media is not always easy. For example, many sites ban child pornography and terrorist content as it is illegal. However, things are less clear when it comes to content about the safety of vaccines or politics, for example. Even when people agree that some content should be blocked, they do not always agree about the best way to do so, how effective it is, and who should do it (the government or private companies, human moderators, or artificial intelligence).

\subsection{Task 1: Relevance}
For each tweet in the sample, follow these instructions:

\begin{enumerate}
    \item Carefully read the text of the tweet, paying close attention to details.
    \item Carefully read the text of the tweet, paying close attention to details.
\end{enumerate}
    

Tweets should be coded as RELEVANT when they directly relate to content moderation, as defined above. This includes tweets that discuss: social media platforms’ content moderation rules and practices, governments’ regulation of online content moderation, and/or mild forms of content moderation like flagging.

Tweets should be coded as IRRELEVANT if they do not refer to content moderation, as defined above, or if they are themselves examples of moderated content. This would include, for example, a Tweet by Donald Trump that Twitter has labeled as “disputed”, a tweet claiming that something is false, or a tweet containing sensitive content. Such tweets might be subject to content moderation, but are not discussing content moderation. Therefore, they should be coded as irrelevant for our purposes.

\subsection{Task 2: Problem/Solution Frames}
Content moderation can be seen from two different perspectives:

\begin{itemize}
    \item Content moderation can be seen as a PROBLEM; for example, as a restriction of free speech
    \item Content moderation can be seen as a SOLUTION; for example, as a protection from harmful speech 
\end{itemize}

For each tweet in the sample, follow these instructions:

\begin{enumerate}
    \item Carefully read the text of the tweet, paying close attention to details.
    \item Classify the tweet as describing content moderation as a problem, as a solution, or neither.
\end{enumerate}

Tweets should be classified as describing content moderation as a PROBLEM if they emphasize negative effects of content moderation, such as restrictions to free speech, or the biases that can emerge from decisions regarding what users are allowed to post.

Tweets should be classified as describing content moderation as a SOLUTION if they emphasize positive effects of content moderation, such as protecting users from various kinds of harmful content, including hate speech, misinformation, illegal adult content, or spam. 

Tweets should be classified as describing content moderation as NEUTRAL if they do not emphasize possible negative or positive effects of content moderation, for example if they simply report on the content moderation activity of social media platforms without linking them to potential advantages or disadvantages for users or stakeholders.

\subsection{Task 3: Policy Frames}

Content moderation, as described above, can be linked to various other topics, such as health, crime, or equality. 

For each tweet in the sample, follow these instructions:

\begin{enumerate}
    \item Carefully read the text of the tweet, paying close attention to details.
    \item Classify the tweet into one of the topics defined below.
\end{enumerate}

The topics are defined as follows:

\begin{itemize}
    \item ECONOMY: The costs, benefits, or monetary/financial implications of the issue (to an individual, family, community, or to the economy as a whole).
    \item Capacity and resources: The lack of or availability of physical, geographical, spatial, human, and financial resources, or the capacity of existing systems and resources to implement or carry out policy goals.
    \item MORALITY: Any perspective—or policy objective or action (including proposed action)that is compelled by religious doctrine or interpretation, duty, honor, righteousness or any other sense of ethics or social responsibility.
    \item FAIRNESS AND EQUALITY: Equality or inequality with which laws, punishment, rewards, and resources are applied or distributed among individuals or groups. Also the balance between the rights or interests of one individual or group compared to another individual or group.
    \item CONSTITUTIONALITY AND JURISPRUDENCE: The constraints imposed on or freedoms granted to individuals, government, and corporations via the Constitution, Bill of Rights and other amendments, or judicial interpretation. This deals specifically with the authority of government to regulate, and the authority of individuals/corporations to act independently of government.
    \item POLICY PRESCRIPTION AND EVALUATION: Particular policies proposed for addressing an identified problem, and figuring out if certain policies will work, or if existing policies are effective.
    \item LAW AND ORDER, CRIME AND JUSTICE: Specific policies in practice and their enforcement, incentives, and implications. Includes stories about enforcement and interpretation of laws by individuals and law enforcement, breaking laws, loopholes, fines, sentencing and punishment. Increases or reductions in crime.
    \item SECURITY AND DEFENSE: Security, threats to security, and protection of one’s person, family, in-group, nation, etc. Generally an action or a call to action that can be taken to protect the welfare of a person, group, nation sometimes from a not yet manifested threat.
    \item HEALTH AND SAFETY: Health care access and effectiveness, illness, disease, sanitation, obesity, mental health effects, prevention of or perpetuation of gun violence, infrastructure and building safety.
    \item QUALITY OF LIFE: The effects of a policy on individuals’ wealth, mobility, access to resources, happiness, social structures, ease of day-to-day routines, quality of community life, etc.
    \item CULTURAL IDENTITY: The social norms, trends, values and customs constituting culture(s), as they relate to a specific policy issue.
    \item PUBLIC OPINION: References to general social attitudes, polling and demographic information, as well as implied or actual consequences of diverging from or “getting ahead of” public opinion or polls.
    \item POLITICAL: Any political considerations surrounding an issue. Issue actions or efforts or stances that are political, such as partisan filibusters, lobbyist involvement, bipartisan efforts, deal-making and vote trading, appealing to one's base, mentions of political maneuvering. Explicit statements that a policy issue is good or bad for a particular political party.
    \item EXTERNAL REGULATION AND REPUTATION: The United States’ external relations with another nation; the external relations of one state with another; or relations between groups. This includes trade agreements and outcomes, comparisons of policy outcomes or desired policy outcomes.
    \item OTHER: Any topic that does not fit into the above categories.
\end{itemize}

\subsection{Task 4: Stance Detection}

In the context of content moderation, Section 230 is a law in the United States that protects websites and other online platforms from being held legally responsible for the content posted by their users. This means that if someone posts something illegal or harmful on a website, the website itself cannot be sued for allowing it to be posted. However, websites can still choose to moderate content and remove anything that violates their own policies.

For each tweet in the sample, follow these instructions:

\begin{enumerate}
    \item Carefully read the text of the tweet, paying close attention to details.
    \item Classify the tweet as having a positive stance towards Section 230, a negative stance, or a neutral stance.
\end{enumerate}

\subsection{Task 5: Topic Detection}

Tweets about content moderation may also discuss other related topics, such as:

\begin{enumerate}
    \item Section 230, which is a law in the United States that protects websites and other online platforms from being held legally responsible for the content posted by their users (SECTION 230).
    \item The decision by many social media platforms, such as Twitter and Facebook, to suspend Donald Trump’s account (TRUMP BAN).
    \item Requests directed to Twitter’s support account or help center (TWITTER SUPPORT).
    \item Social media platforms’ policies and practices, such as community guidelines or terms of service (PLATFORM POLICIES).
    \item Complaints about platform’s policy and practices in deplatforming and content moderation or suggestions to suspend particular accounts, or complaints about accounts being suspended or reported  (COMPLAINTS).
    \item If a text is not about the SECTION 230, COMPLAINTS, TRUMP BAN, TWITTER SUPPORT, and PLATFORM POLICIES, then it should be classified in OTHER class (OTHER).
\end{enumerate}

For each tweet in the sample, follow these instructions:

\begin{enumerate}
    \item Carefully read the text of the tweet, paying close attention to details.
    \item Please classify the following text according to topic (defined by function of the text, author’s purpose and form of the text). You can choose from the following classes: SECTION 230, TRUMP BAN, COMPLAINTS, TWITTER SUPPORT, PLATFORM POLICIES, and OTHER
\end{enumerate}



\end{document}
