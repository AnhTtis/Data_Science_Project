\documentclass[runningheads]{llncs}
\usepackage[T1]{fontenc}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{color}
\renewcommand\UrlFont{\color{blue}\rmfamily}


\newcommand{\libname}[0]{marl-jax}

\begin{document}
\title{\libname: Multi-agent Reinforcement Leaning framework for Social Generalization
}
\titlerunning{marl-jax}
\author{Kinal Mehta\inst{1} \and
Anuj Mahajan\inst{2} \and
Pawan Kumar\inst{1}}
\authorrunning{K. Mehta et al.}
\institute{IIIT Hyderabad, India \\
\email{kinal.mehta@research.iiit.ac.in}\\
\and
University of Oxford, UK\\
}

\maketitle              %
\begin{abstract}
Recent advances in Reinforcement Learning (RL) have led to many exciting applications. These advancements have been driven by improvements in both algorithms and engineering, which have resulted in faster training of RL agents.
We present \libname, a multi-agent reinforcement learning software package for training and evaluating social generalization of the agents. 
The package is designed for training a population of agents in multi-agent environments and evaluating their ability to generalize to diverse background agents. It is built on top of DeepMind's JAX ecosystem~\cite{deepmind2020jax} and leverages the RL ecosystem developed by DeepMind. Our framework \libname~is capable of working in cooperative and competitive, simultaneous-acting environments with multiple agents. The package offers an intuitive and user-friendly command-line interface for training a population and evaluating its generalization capabilities.
In conclusion, \libname~provides a valuable resource for researchers interested in exploring social generalization in the context of MARL. The open-source code for \libname~is available at: \href{https://github.com/kinalmehta/marl-jax}{https://github.com/kinalmehta/marl-jax}


\keywords{Multi-agent Reinforcement Learning  \and Zero-Shot Generalization \and General Sum Games}
\end{abstract}



\section{Introduction}
Multi-agent reinforcement learning (MARL) is an important framework for training autonomous agents that operate in dynamic environments with multiple learning agents. Many potential real-world applications require the trained agents to cooperate with humans or agents not seen during training. That is, they should be able to zero-shot generalize to novel social partners. Most of the existing MARL frameworks are either designed for cooperative MARL research or naively extend existing single-agent RL frameworks to work with multiple agents.

On the contrary, \libname~is designed specifically for multi-agent research and facilitate the training and assessment of the generalization capacities of multi-agent reinforcement learning (MARL) algorithms when facing new social partners. We utilize the functionalities of JAX~\cite{jax2018github} including autograd, vectorization through \emph{vmap}, parallel processing through \emph{pmap}, and compilation through \emph{jit}, resulting in highly optimized training for multiple agents.


\section{Related Works}
The RL community has developed several frameworks targeting various aspects such as implementation simplicity, ease of adaptation and scaling deep RL agents. In \libname, we focus on ease of experimentation and adaption for training a population of agents in multi-agent environments.

A number of libraries that concentrate on single-agent reinforcement learning have been created, such as stable-baselines3~\cite{raffinStableBaselines3ReliableReinforcement2021}, dopamine~\cite{castro18dopamine}, acme~\cite{hoffmanAcmeResearchFramework2022}, RLlib~\cite{liangRLlibAbstractionsDistributed2018}, and CleanRL~\cite{huangCleanRLHighqualitySinglefile2022}. These libraries prioritize features like modularity by providing useful abstractions, ease of use by requiring minimal code to get started, distributed training and ease of comprehension and reproducibility. Other libraries such as Reverb~\cite{cassirerReverbFrameworkExperience2021}, rlax~\cite{deepmind2020jax}, and launchpad~\cite{yangLaunchpadProgrammingModel2021} concentrate on specific components of an RL system.

For multi-agent reinforcement learning, several libraries have been developed, including PyMARL~\cite{samvelyan19smac}, epymarl~\cite{papoudakis2021epymarl}, RLlib~\cite{liangRLlibAbstractionsDistributed2018}, Mava~\cite{pretoriusMavaResearchFramework2021}, and PantheonRL~\cite{sarkarPantheonRLMARLLibrary2022}. RLlib and PantheonRL enhance existing single-agent RL algorithms to enable multi-agent training, while Mava, PyMARL, and epymarl are specifically designed for MARL but only support cooperative environments.


The advancements of Reinforcement Learning (RL) algorithms have been greatly influenced by libraries providing a range of environments. Single agent RL has been aided by libraries such as OpenAI Gym~\cite{2016openaigym} and dm-env~\cite{dm_env2019}, which established the framework for environment interactions. Multi-agent RL has been supported by SMAC~\cite{samvelyan19smac} and PettingZoo~\cite{terry2021pettingzoo}. Recently, DeepMind has contributed to the field by open-sourcing MeltingPot~\cite{agapiouMeltingPot2022}, a library for evaluating multi-agent generalization to new social partners at scale. 


\section{\libname}

Inspired by Acme~\cite{hoffmanAcmeResearchFramework2022}, we share a lot of design philosophies with it. Reverb\cite{cassirerReverbFrameworkExperience2021} is used as a data-store server for the replay buffer. Launchpad~\cite{yangLaunchpadProgrammingModel2021} is used for distributed computing. We use JAX~\cite{jax2018github} as the numerical computation backend for neural networks. 
We use \emph{dm-env} API as our environment interaction API and extend it for multi-agent environments.


\subsection{System Architecture}

We follow IMPALA-style distributed training architecture. Our system consists of three main components running in parallel as separate processes. 
\begin{itemize}
    \item \textbf{Environment Loop:} The environment loop process interacts with the environment using the available policy and adds the collected experience to the replay buffer. Multiple parallel environment loop processes are run, each with its own copy of the environment. We use CPU inference for action selection on each process. To keep the policy parameters in sync with the learner process, the parameters are periodically fetched from the learner process. The action selection step is optimized using \emph{vamp} auto-vectorization to select the action for all agents in the environment.
    \item \textbf{Learner:} The actual policy learning happens in this process. The learner fetches experience from the replay buffer and performs the optimization step on policy and value function parameters. We use \emph{pmap} to auto-scale the optimization step to multiple GPUs and \emph{vmap} based auto-vectorization to perform the optimization step for all agents in parallel.
    \item \textbf{Replay Buffer:} A separate process with reverb~\cite{cassirerReverbFrameworkExperience2021} server is used as a replay buffer. All the actors add experience to this server, and the learner process samples experience from the server and perform the optimization.
\end{itemize}

\subsection{Supported Environments}
We support two multi-agent environment suits, which consist of simultaneous acting homogeneous agents.
\begin{itemize}
    \item \textbf{Overcooked:} Overcooked~\cite{carrollUtilityLearningHumans2019overcooked} is a cooperative environment where a team of agents must cook a soup and deliver it as fast as possible. The reward is shared among all agents. A variety of different layouts are available that focus on different learning aspects. The generalization of the trained agents can be tested against human data provided by the suite.
    \item \textbf{MeltingPot:} MeltingPot~\cite{agapiouMeltingPot2022} consists of more than $50$ different environments and over $256$ unique test scenarios. Once a population of agents is trained in an environment, the generalization to novel partners can be tested on the various test scenarios provided in the suite. 
\end{itemize}
\begin{figure}
    \centering
    \includegraphics[height=96pt]{images/prisoners_dilemma_in_the_matrix__repeated_2.jpg}
    \includegraphics[width=0.5\textwidth]{images/overcooked.jpg}
    \caption{\libname~supports two major environment surits, Meltingpot~\cite{agapiouMeltingPot2022} and Overcooked~\cite{carrollUtilityLearningHumans2019overcooked}}
    \label{fig:env_supports}
\end{figure}

\subsection{Algorithms Implemented}
We currently support two major algorithms
\begin{itemize}
    \item \textbf{IMPALA:} A standard actor-critic based independent learning algorithm using V-trace~\cite{espeholtIMPALAScalableDistributed2018} for off-policy corrections.
    \item \textbf{OPRE:} Options as Responses~\cite{vezhnevetsOptionsResponsesGrounding2020} follows actor-critic based learning but its objective is specifically designed to generalize to novel partners. It is used as one of the baseline in MeltingPot~\cite{agapiouMeltingPot2022}. We are the first to provide an open-source implementation of OPRE.
\end{itemize}

\subsection{Utilities}
We provide two major utilities 1) \textit{train.py} and 2) \textit{evaluate.py}
\begin{itemize}
    \item \textbf{train.py:} The entry point for training a population of agents in the given environment
    \item \textbf{evaluate.py:} Used to evaluate the generalization performance on with various partner agents
\end{itemize}


\section{Conclusion and Future Works}
In this paper, we introduced \libname, a highly optimized package for training and evaluation of the generalization of a population of agents to novel partners. The package provides an easy-to-use utility to train and evaluate the trained agents. It also provides an open-source implementation of ORPE~\cite{vezhnevetsOptionsResponsesGrounding2020}, a MARL algorithm designed for generalization. This package is targeted for researchers working on generalization in MARL and reduces the entry barrier for new researchers in MARL generalization. As any software package, \libname~is under continuous development and, in future, aims to implement other population learning algorithms.





\bibliographystyle{splncs04}
\bibliography{mybibliography}


\end{document}
