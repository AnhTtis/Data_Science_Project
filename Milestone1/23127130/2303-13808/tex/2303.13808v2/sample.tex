\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{jmlr2e}
% \usepackage[T1]{fontenc}
% \usepackage{cite}
% \usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
% \usepackage{subcaption}
% \usepackage{textcomp}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{algorithm}
% \usepackage{algorithmic}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\libname}[0]{marl-jax}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
% \jmlrheading{23}{2022}{1-\pageref{LastPage}}{1/21; Revised 5/22}{9/22}{21-0000}{Kinal Mehta, Anuj Mahajan and Pawan Kumar}

% Short headings should be running head and authors last names

% \ShortHeadings{\libname}{Mehta, Mahajan and Kumar}
\firstpageno{1}

\begin{document}

\title{\libname: Multi-agent Reinforcement Leaning Framework}

\author{\name Kinal Mehta$^{1}$ \email kinal.mehta@research.iiit.ac.in \\
       \name Anuj Mahajan$^{2}$ \email anuj.mahajan@cs.ox.ac.uk \\
       \name Pawan Kumar$^{1}$ \email pawan.kumar@iiit.ac.in \\
       \addr $^{1}$International Institute of Information Technology Hyderabad, India \\
       \addr $^{2}$University of Oxford, Oxford, UK
}

\editor{}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
Recent advances in Reinforcement Learning (RL) have led to many exciting applications. These advancements have been driven by improvements in both algorithms and engineering, which have resulted in faster training of RL agents.
We present \libname, a multi-agent reinforcement learning software package for training and evaluating social generalization of the agents. 
The package is designed for training a population of agents in multi-agent environments and evaluating their ability to generalize to diverse background agents. It is built on top of DeepMind's JAX ecosystem~\cite{deepmind2020jax} and leverages the RL ecosystem developed by DeepMind. Our framework \libname~is capable of working in cooperative and competitive, simultaneous-acting environments with multiple agents. The package offers an intuitive and user-friendly command-line interface for training a population and evaluating its generalization capabilities.
In conclusion, \libname~provides a valuable resource for researchers interested in exploring social generalization in the context of MARL. The open-source code for \libname~is available at: \href{https://github.com/kinalmehta/marl-jax}{https://github.com/kinalmehta/marl-jax}
\end{abstract}

\begin{keywords}
  Multi-agent Reinforcement Learning, Zero-Shot Generalization, General Sum Games
\end{keywords}

\section{Introduction}
Multi-agent reinforcement learning (MARL) is an important framework for training autonomous agents that operate in dynamic environments with multiple learning agents. Many potential real-world applications require the trained agents to cooperate with humans or agents not seen during training. That is, they should be able to zero-shot generalize to novel social partners. Most of the existing MARL frameworks~\cite{samvelyan19smac, papoudakis2021epymarl, sarkarPantheonRLMARLLibrary2022, malib, hu2022marllib} are either designed for cooperative MARL research or naively extend existing single-agent RL frameworks to work with multiple agents.

On the contrary, \libname~is designed specifically for multi-agent research and facilitate the training and assessment of the generalization capacities of multi-agent reinforcement learning (MARL) algorithms when facing new social partners. We utilize the functionalities of JAX~\cite{jax2018github} including autograd, vectorization through \emph{vmap}, parallel processing through \emph{pmap}, and compilation through \emph{jit}, resulting in highly optimized training for multiple agents.


\section{Related Works}
The RL community has developed several frameworks targeting various aspects such as implementation simplicity, ease of adaptation and scaling deep RL agents. In \libname, we focus on ease of experimentation and adaption for training a population of agents in multi-agent environments.

A number of libraries that concentrate on single-agent reinforcement learning have been created, such as stable-baselines3~\cite{raffinStableBaselines3ReliableReinforcement2021}, dopamine~\cite{castro18dopamine}, acme~\cite{hoffmanAcmeResearchFramework2022}, RLlib~\cite{liangRLlibAbstractionsDistributed2018}, and CleanRL~\cite{huangCleanRLHighqualitySinglefile2022}. These libraries prioritize features like modularity by providing useful abstractions, ease of use by requiring minimal code to get started, distributed training and ease of comprehension and reproducibility. Other libraries such as Reverb~\cite{cassirerReverbFrameworkExperience2021}, rlax~\cite{deepmind2020jax}, and launchpad~\cite{yangLaunchpadProgrammingModel2021} concentrate on specific components of an RL system.

For multi-agent reinforcement learning, several libraries have been developed, including PyMARL~\cite{samvelyan19smac}, epymarl~\cite{papoudakis2021epymarl}, RLlib~\cite{liangRLlibAbstractionsDistributed2018}, Mava~\cite{pretoriusMavaResearchFramework2021}, and PantheonRL~\cite{sarkarPantheonRLMARLLibrary2022}. RLlib and PantheonRL enhance existing single-agent RL algorithms to enable multi-agent training, while Mava, PyMARL, and epymarl are specifically designed for MARL but only support cooperative environments.


The advancements of Reinforcement Learning (RL) algorithms have been greatly influenced by libraries providing a range of environments. Single agent RL has been aided by libraries such as OpenAI Gym~\cite{2016openaigym} and dm-env~\cite{dm_env2019}, which established the framework for environment interactions. Multi-agent RL has been supported by SMAC~\cite{samvelyan19smac} and PettingZoo~\cite{terry2021pettingzoo}. Recently, DeepMind has contributed to the field by open-sourcing MeltingPot~\cite{agapiouMeltingPot2022}, a library for evaluating multi-agent generalization to new social partners at scale. Similarly, efforts for measuring generalization in cooperative multi-agent settings \cite{mahajan2022generalization} are being supported by libraries like \cite{ellis2022smacv2}.

Several recent works \cite{hoffmanAcmeResearchFramework2022, huangCleanRLHighqualitySinglefile2022, deepmind2020jax, pretoriusMavaResearchFramework2021} have begun utilizing JAX \cite{jax2018github} due to its various benefits. These benefits include auto-vectorization, just-in-time compilation, and easy multi-GPU scaling. 
% Therefore, we have selected JAX as the framework for our library based on its demonstrated advantages and ability to meet the computational needs of MARL.


\section{\libname}
% \am{some representative code snippet to complemetn with these sections, space permitting}
Inspired by Acme~\cite{hoffmanAcmeResearchFramework2022}, we share a lot of design philosophies with it. Reverb~\cite{cassirerReverbFrameworkExperience2021} is used as a data-store server for the replay buffer. Launchpad~\cite{yangLaunchpadProgrammingModel2021} is used for distributed computing. We use JAX~\cite{jax2018github} as the numerical computation backend for neural networks. 
We use \emph{dm-env} API as our environment interaction API and extend it for multi-agent environments.



\subsection{System Architecture}

We implement four different training architectures


\subsubsection{Single Threaded}
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{images/marl-jax_sequential.pdf}
    \caption{Single-threaded RL training pipeline}
    \label{fig:single_process}
\end{figure}

Figure~\ref{fig:single_process} illustrates the sequential flow of operations in a single-threaded RL training process. At each step, the agent receives an observation from the environment, selects an action based on its policy, interacts with the environment, and receives a reward. The observation, action, reward, and next observation are collected to form a  batch and then used to update agent's policy and value function using gradient descent. This process continues iteratively until the desired convergence or a specified number of iterations is reached. Algorithm \ref{algo:single-threaded} describes the pseudocode for training a policy in singe-threaded manner.

\begin{algorithm}
\caption{\label{actor} Single-threaded RL training}
\begin{algorithmic}[1] 
\label{algo:single-threaded}
\REPEAT
\STATE params = initialize params for the policy and value function
\STATE sequence = [\quad]
\STATE timestep = environment.reset()
\REPEAT
\STATE actions = predict\_actions(params, timestep.observation)
\STATE new\_timestep = environment.step(actions)
\STATE sequence.append((timestep, actions))
\STATE timestep = new\_timestep
\UNTIL{not timestep.last() \textbf{or} len(sequence) $<$ MAX\_BATCH\_SIZE}
\STATE params, logs = sgd\_step(params, sequence)
\STATE logger.write(logs)
\UNTIL{actor\_steps $<$ MAX\_STEPS}
\end{algorithmic}
\end{algorithm}

\subsubsection{Synchronous Distributed}
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{images/marl-jax_synchronous.pdf}
    \caption{RL training with vectorized environment}
    \label{fig:sync_arch}
\end{figure}

The synchronous distributed architecture builds upon the single-threaded architecture by utilizing multiple environment instances running in parallel processes to collect a batch of experiences simultaneously. Each environment synchronously interacts with a common policy, generating sequences of states, actions, rewards, and next observations. This batch of sequences is used to update the policy and value function weights through gradient descent. By leveraging parallelization, the synchronous distributed architecture enables more efficient data collection and faster updates, leading to accelerated training and improved convergence in reinforcement learning. Fig.\ref{fig:sync_arch} illustrates how synchronous parallelization is achieved by running each environment instance in a separate process. The pseudocode is similar to that described in algorithm\ref{algo:single-threaded}, with the main difference being that each interaction with the environment results in a batch of data collected from environments across all processes. 


\subsubsection{IMPALA-style Asynchronous Distributed}
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{images/marl-jax_impala.pdf}
    \caption{Asynchronous distributed RL training pipeline}
    \label{fig:impala_arch}
\end{figure}

In the IMPALA-style asynchronous distributed training architecture, multiple actors run in parallel and interact with their respective environments asynchronously. Each actor collects trajectories of experience by executing its own copy of the current policy. 
The three main components running in parallel as separate processes are described below
\begin{itemize}
    \item \textbf{Environment Loop:} The environment loop process interacts with the environment using the available policy and adds the collected experience to the replay buffer. Multiple parallel environment loop processes are run, each with its own copy of the environment and policy parameters. We use CPU inference for action selection on each process. To keep the policy parameters in sync with the learner process, the parameters are periodically fetched from the learner process. The action selection step is optimized using \emph{vamp} auto-vectorization to select the action for all agents in the environment. Algorithm~\ref{algo:async_actor} shows the pseudocode for this process.
    \item \textbf{Learner:} The actual policy learning happens in this process. The learner fetches experience from the replay buffer and performs the optimization step on policy and value function parameters. We use \emph{pmap} to auto-scale the optimization step to multiple GPUs and \emph{vmap} based auto-vectorization to perform the optimization step for all agents in parallel. Algorithm~\ref{algo:async_learner} shows the pseudocode for this process.
    \item \textbf{Replay Buffer:} A separate process with reverb~\cite{cassirerReverbFrameworkExperience2021} server is used as a replay buffer. All the actors add experience to this server, and the learner process samples experience from the server to optimize for policy and value function parameters.
\end{itemize}

Figure~\ref{fig:impala_arch} illustrates the how data flows between different components enabling asynchronous experience collection and training of the RL agent. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}
\caption{\label{actor} Asynchronous Environment loop}
\begin{algorithmic}[1] 
\label{algo:async_actor}
\REPEAT
\STATE params = fetch latest params from learner
\STATE episode = [\quad]
\STATE timestep = environment.reset()
\REPEAT
\STATE actions = predict\_actions(params, timestep.observation)
\STATE new\_timestep = environment.step(actions)
\STATE episode.append((timestep, actions))
\STATE timestep = new\_timestep
\UNTIL{not timestep.last()}
\STATE replay\_buffer.write(episode)
\STATE logger.write(logs)
\UNTIL{actor\_steps $<$ MAX\_STEPS}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{\label{learner} Asynchronous Learner}
\begin{algorithmic}[1]
\label{algo:async_learner}
\REPEAT
\STATE batch = replay\_buffer.sample()
\STATE new\_params, logs = sgd\_step(params, batch)
\STATE logger.write(logs)
\UNTIL{actor\_steps $<$ MAX\_STEPS}
\end{algorithmic}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Sebulba: Asynchronous Distributed with Inference Server}
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{images/marl-jax_sebulba.pdf}
    \caption{Asynchronous distributed with Inference Server}
    \label{fig:subalba_arch}
\end{figure}

Inspired by Sebulba architecture from Podracer~\cite{hessel2021podracer} and seed-rl~\cite{espeholt2019seed}, this architecture uses a common inference server in the asynchronous distributed architecture.
When a common inference server is used in the asynchronous distributed architecture for RL training, the architecture is further enhanced to centralize the inference process. In this setup, multiple actors interact with their respective environments asynchronously, collecting trajectories of experience as before. However, instead of each actor performing its own inference, they send their collected experiences to a common inference server which usually has access to hardware-accelerator such as GPU.

By utilizing a common inference server, several advantages can be achieved. First, it reduces the computational load on the individual actors, as they no longer need to perform their own inference. This enables the actors to focus on data collection, resulting in more efficient and faster interaction with the environment.
Second, the use of a shared policy network ensures that all actors are making decisions based on the same set of parameters. This improves the consistency and stability of the training process, as it prevents any discrepancies that may arise from differences in local copies of the policy network.

Figure~\ref{fig:subalba_arch} shows the block-diagram and data-flow in asynchronous architecture with inference server. The pseudocode for learner will be exactly the same as the Algorithm~\ref{algo:async_learner} and for actor it is same as Algorithm~\ref{algo:async_actor} where the \emph{predict\_actions} functions will be a call to the inference server instead of local policy network.


\subsection{Supported Environments}
We support two multi-agent environment suits, which consist of simultaneous acting homogeneous agents.

\subsubsection{Overcooked} 

The Overcooked environment~\cite{carrollUtilityLearningHumans2019overcooked} is a popular benchmark in the field of Multi-Agent Reinforcement Learning (MARL) that simulates a cooperative cooking scenario based on the popular game \emph{Overcooked}. It provides a challenging and interactive environment where multiple agents collaborate to prepare dishes in a virtual kitchen.

In Overcooked, the goal is to efficiently work together as a team to prepare and serve a variety of meals. The agents control different characters within the kitchen and must coordinate their actions to complete tasks such as chopping ingredients, cooking, and delivering finished dishes to customers. Collaboration and coordination are essential to maximize efficiency and achieve high scores.

The environment features various elements that add complexity to the task. For example, the kitchen layout may include obstacles that require agents to navigate around, limited resources like cutting boards and stoves that need to be shared, and time-sensitive customer orders that must be fulfilled promptly. Additionally, agents need to strategize and communicate effectively to optimize their actions and avoid potential bottlenecks or collisions.

Overcooked is designed to test the ability of MARL algorithms to solve cooperative tasks in dynamic and complex environments. It challenges agents to exhibit skills such as coordination, planning, communication, and adaptive decision-making. 


\subsubsection{Melting Pot}


Melting Pot~\cite{agapiouMeltingPot2022} suite designed with the objective of evaluating generalization to novel situations and coplayers. The Melting Pot 2.0 suite consists of $50$ different environments and over $256$ unique test scenarios to evaluate the trained population of agents on broad range of topics such as social dilemmas, task partioning, resource sharing, etc

Melting Pot evaluation methodology is captured by the following equation:

\textbf{Substrate + Background Population = Scenario}
\begin{itemize}
    \item \textbf{Substrate:} The term "substrate" refers to the static or physical aspects of the environment in a simulation. It encompasses elements such as the layout of the map, the placement of objects, the rules governing their movement, and the physics involved. In essence, the substrate defines the stationary or unchanging components of the environment's dynamics. It sets the foundation and structure upon which other dynamic elements and interactions can take place. By defining the substrate, the simulation establishes the framework for how the environment behaves and provides a stable backdrop against which other entities and events can unfold.
    
    \item \textbf{Background Population:} The term "background population" refers to a group of simulated entities within a simulation that have their own agency or ability to take actions and make decisions. In other words, these entities are not passive or static; they actively participate in the simulation and contribute to its dynamics. They can interact with other entities, respond to stimuli or events, and potentially influence the overall behavior and outcomes of the simulation. The background population adds an element of realism and complexity to the simulation, making it more dynamic and reflective of real-world scenarios.
     
    \item \textbf{Scenario:} In the context of simulation or modeling, a scenario is created by combining the substrate and the background population. The substrate refers to the static or physical part of the environment, such as the layout, objects, and physics rules. On the other hand, the background population consists of simulated entities with agency, meaning they can take actions and make decisions within the simulation.

By integrating the substrate and the background population, a scenario is formed that represents a specific setting or situation within the simulation. The substrate provides the foundation, defining the physical attributes and constraints of the environment. This includes factors like the terrain, structures, objects, and their spatial arrangement. The substrate sets the stage for interactions and events to occur.

The background population adds a dynamic aspect to the scenario. These simulated entities have their own behaviors, goals, and decision-making processes. They can interact with each other, respond to stimuli or events in the environment, and potentially influence the overall dynamics of the scenario. The actions and interactions of the background population create a realistic and evolving simulation environment.

Together, the substrate and background population create a scenario that encapsulates a particular context or situation within the simulation. This scenario can be designed to simulate real-world scenarios, test hypotheses, study the behavior of complex systems, or provide a platform for experimentation and analysis. By carefully defining the substrate and background population, researchers and practitioners can create meaningful and informative scenarios that capture the intricacies of the system being studied.
\end{itemize}



\begin{figure}
    \centering
    \includegraphics[height=96pt]{images/prisoners_dilemma_in_the_matrix__repeated_2.jpg}
    \includegraphics[width=0.5\textwidth]{images/overcooked.jpg}
    \caption{\libname~supports two major environment suits, Meltingpot~\cite{agapiouMeltingPot2022} and Overcooked~\cite{carrollUtilityLearningHumans2019overcooked}}
    \label{fig:env_supports}
\end{figure}

\subsection{Algorithms Implemented}
We currently support two major algorithms
\begin{itemize}
    \item \textbf{Actor-Critic Baseline:} A standard actor-critic based independent learning algorithm using V-trace~\cite{espeholtIMPALAScalableDistributed2018} for off-policy corrections.
    \item \textbf{OPRE:} Options as Responses~\cite{vezhnevetsOptionsResponsesGrounding2020} follows actor-critic based learning but its objective is specifically designed to generalize to novel partners. It is used as one of the baseline in MeltingPot~\cite{agapiouMeltingPot2022}. We are the first to provide an open-source implementation of OPRE.
\end{itemize}


\subsection{Utilities}
We provide two major utilities 1) \textit{train.py} and 2) \textit{evaluate.py}
\begin{itemize}
    \item \textbf{train.py:} The entry point for training a population of agents in the given environment
    \item \textbf{evaluate.py:} Used to evaluate the generalization performance on with various partner agents
    \item \textbf{evaluation\_results.py:} Aggregates the evaluation results by averaging across multiple seeds and presents a table.
\end{itemize}


\section{Results}

\begin{table}[ht]
   \begin{center}  
    \begin{tabular}{|c|c|c|c|}
    \hline
     & \bfseries OPRE & \bfseries IMPALA \\
    \hline
    \bfseries Substrate & 0.00 & 0.00 \\
    \bfseries Scenario 0 & 4.91 & -7.10 \\
    \bfseries Scenario 1 & 3.79 & -2.65 \\
    \bfseries Scenario 2 & 10.52 & 5.97 \\
    \bfseries Scenario 3 & 13.52 & 11.52 \\
    \bfseries Scenario 4 & 6.60 & 0.66 \\
    \hline
    \end{tabular}
    \caption{Evaluation on Running with Scissors in the matrix}
    \label{tab:rws}
    \end{center}
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{images/results/running_with_scissors.pdf}
    \caption{Training Plot on Running with Scissors in the matrix}
    \label{fig:rws}
\end{figure}


\begin{table}[ht]
   \begin{center}  
    \begin{tabular}{|c|c|c|c|}
    \hline
     & \bfseries IMPALA & \bfseries OPRE \\
    \hline
    \bfseries Substrate & 106.85 & 38.18 \\
    \bfseries Scenario 0 & 131.00 & 59.71 \\
    \bfseries Scenario 1 & 176.54 & 114.69 \\
    \bfseries Scenario 2 & 79.58 & 27.97 \\
    \bfseries Scenario 3 & 62.80 & 41.76 \\
    \bfseries Scenario 4 & 48.63 & 38.75 \\
    \bfseries Scenario 5 & 65.82 & 47.66 \\
    \bfseries Scenario 6 & 101.83 & 40.34 \\
    \bfseries Scenario 7 & 83.33 & 49.82 \\
    \bfseries Scenario 8 & 77.75 & 32.59 \\
    \bfseries Scenario 9 & 78.41 & 74.62 \\
    \hline
    \end{tabular}
    \caption{Evaluation on Prisoners Dilemma in the Matrix}
    \label{tab:dilemma}
    \end{center}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{images/results/prisoners_dilemma_in_the_matrix.pdf}
    \caption{Training Plot on Prisoners Dilemma in the Matrix}
    \label{fig:dilemma}
\end{figure}


We evaluate our implementation in two environments to assess its performance and generalization capabilities across different types of multi-agent scenarios. The first environment, Meltingpot, encompasses a wide range of game types, including cooperative, competitive, and general-sum games. In Meltingpot, agents interact with each other to achieve various objectives, which can involve cooperation, competition, or a combination of both. This environment allows us to examine how well our implementation handles different types of interactions and strategies, evaluating its performance in cooperative, competitive, and general-sum settings.

The second environment, Overcooked, focuses specifically on cooperative multi-agent scenarios. In Overcooked, agents work together in a shared kitchen to prepare meals and serve customers. The emphasis in this environment is on effective coordination, communication, and cooperation among the agents to maximize efficiency and customer satisfaction. By evaluating our implementation in Overcooked, we can specifically assess its performance and effectiveness in cooperative multi-agent settings, where collaboration and teamwork are crucial for success.

By evaluating our implementation in both Meltingpot and Overcooked, we gain a comprehensive understanding of its performance in a range of multi-agent scenarios. This evaluation enables us to analyze how well our approach adapts to different types of interactions, strategies, and objectives, and provides valuable insights into its strengths and limitations. The findings from these evaluations contribute to advancing our understanding of multi-agent reinforcement learning and inform further research and development in this field.

\subsection{MeltingPot}
We conduct evaluations on four distinct environments from the Meltingpot-v1 and Meltingpot-v2 domains.

In Meltingpot-v1, we evaluate our approach on two environments:
\begin{itemize}
    \item \textbf{Running with Scissors in the Matrix}: The training progress is visualized in Figure~\ref{fig:rws}, and the evaluation scores across different episodes are provided in Table~\ref{tab:rws}.
    \item \textbf{Prisoners' Dilemma in the Matrix}: The training progress is depicted in Figure~\ref{fig:dilemma}, and the evaluation scores for various scenarios are presented in Table~\ref{tab:dilemma}.
\end{itemize}

In Meltingpot-v2, we assess our approach on two additional environments:
\begin{itemize}
    \item \textbf{Daycare}: The training progress is shown in Figure~\ref{fig:daycare}, and the evaluation scores for different scenarios are summarized in Table~\ref{tab:daycare}.
    \item \textbf{Externality Mushrooms Dense}: The training progress is illustrated in Figure~\ref{fig:externality_mushrooms}, and the evaluation scores for various scenarios are provided in Table~\ref{tab:externality_mushrooms}.
\end{itemize}
These evaluations allow us to analyze the performance of our approach across different environments and scenarios within the Meltingpot framework. The training plots provide insights into the learning progress, while the evaluation scores offer quantitative measures of the agent's performance in various scenarios.

\begin{table}[ht]
   \begin{center}  
    \begin{tabular}{|c|c|c|c|}
    \hline
     & \bfseries IMPALA & \bfseries OPRE \\
    \hline
    \bfseries Substrate & 65.94 & 67.83 \\
    \bfseries Scenario 0 & 0.89 & 0.33 \\
    \bfseries Scenario 1 & 109.11 & 126.00 \\
    \bfseries Scenario 2 & 0.22 & 0.00 \\
    \bfseries Scenario 3 & 154.56 & 171.33 \\
    \hline
    \end{tabular}
    \caption{Evaluation on Daycare}
    \label{tab:daycare}
    \end{center}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{images/results/daycare.pdf}
    \caption{Training Plot on Daycare}
    \label{fig:daycare}
\end{figure}


\begin{table}[ht]
   \begin{center}  
    \begin{tabular}{|c|c|c|c|}
    \hline
     & \bfseries IMPALA \\
    \hline
    \bfseries Scenario 0 & 547.70 \\
    \bfseries Scenario 1 & 13.21 \\
    \bfseries Scenario 2 & 293.02 \\
    \bfseries Scenario 3 & 38.04 \\
    \bfseries Substrate & 91.56 \\
    \hline
    \end{tabular}
    \caption{Evaluation on Externality Mushrooms}
    \label{tab:externality_mushrooms}
    \end{center}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{images/results/externality_mashrooms.pdf}
    \caption{Training Plot on Externality Mushrooms}
    \label{fig:externality_mushrooms}
\end{figure}


\subsection{Overcooked}
We evaluate the performance of the algorithms on the \emph{Cramped Room} environment from the Overcooked domain. The training progress of the algorithms is visualized in Figure~\ref{fig:cramped_room}, providing insights into their learning dynamics and convergence behavior.


\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{images/results/cramped_room.pdf}
    \caption{Training Plot on Cramped Room}
    \label{fig:cramped_room}
\end{figure}





\section{Conclusion and Future Works}
In this paper, we introduced \libname, a highly optimized package for training and evaluation of the generalization of a population of agents to novel partners. The package provides an easy-to-use utility to train and evaluate the trained agents. It also provides an open-source implementation of ORPE~\cite{vezhnevetsOptionsResponsesGrounding2020}, a MARL algorithm designed for generalization. This package is targeted for researchers working on generalization in MARL and reduces the entry barrier for new researchers in MARL generalization. As any software package, \libname~is under continuous development and, in future, aims to implement other population learning algorithms \cite{openendedlearningteam2021openended}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Acknowledgements and Disclosure of Funding should go at the end, before appendices and references

% \acks{All acknowledgements go at the end of the paper before appendices and references.
% Moreover, you are required to declare funding (financial activities supporting the
% submitted work) and competing interests (related financial activities outside the submitted work).
% More information about this disclosure can be found on the JMLR website.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

% \newpage

% \appendix
% \section{}
% \label{app:theorem}

% % Note: in this sample, the section number is hard-coded in. Following
% % proper LaTeX conventions, it should properly be coded as a reference:

% %In this appendix we prove the following theorem from
% %Section~\ref{sec:textree-generalization}:

% In this appendix we prove the following theorem from
% Section~6.2:

% \noindent
% {\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
% not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
% dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
% which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
% respective empirical mutual information values based on the sample
% $\dataset$. Then
% \[
% 	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
% \]
% with equality only if $u$ is identically 0.} \hfill\BlackBox

% \section{}

% \noindent
% {\bf Proof}. We use the notation:
% \[
% P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
% P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
% \]
% These values represent the (empirical) probabilities of $v$
% taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
% by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

% {\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


% \vskip 0.2in
\bibliography{sample}

\end{document}
