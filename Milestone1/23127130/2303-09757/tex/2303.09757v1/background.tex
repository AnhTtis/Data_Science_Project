\section{Related Work}


\begin{figure*}
    \centering
    \captionsetup[subfigure]{justification=centering}
    % 
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/gt/cityscapes}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/gt/ddad}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/gt/ua-detrac}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/gt/visdrone}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/gt/davis}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/gt/reds}
        % \caption{}
    \end{subfigure}
    \\
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/hazy/cityscapes}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/hazy/ddad}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/hazy/ua-detrac}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/hazy/visdrone}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/hazy/davis}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/hazy/reds}
        % \caption{}
    \end{subfigure}
    \\
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/transmission/cityscapes}
        \caption{Cityscapes}
    \end{subfigure}
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/transmission/ddad}
        \caption{DDAD}
    \end{subfigure}
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/transmission/ua-detrac}
        \caption{UA-DETRAC}
    \end{subfigure}
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/transmission/visdrone}
        \caption{VisDrone}
    \end{subfigure}
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/transmission/davis}
        \caption{DAVIS}
    \end{subfigure}
    \begin{subfigure}{0.147\textwidth}
        \includegraphics[width=\textwidth]{./figs/hazeworld/transmission/reds}
        \caption{REDS}
    \end{subfigure}
    \\
    % 
    % \includegraphics[width=0.9\hsize]{./figs/hazeworld/hazeworld_example}
    % 
    \vspace{-3mm}
    \caption{Example ground-truth images (the first row), synthetic hazy images (the second row), and transmission maps (the last row) in our HazeWorld dataset.}
    \label{fig:hazeworld}
    \vspace{-6mm}
\end{figure*}


\noindent \textbf{Image dehazing.}
% 
Single-image dehazing has been widely studied in computer vision and computer graphics.
% 
Early methods rely on the atmospheric scattering model and physical priors~\cite{he2010single,berman2016non}.
% 
% Please refer to~\cite{li2018benchmarking} for a more comprehensive review.
% 
Later, deep learning-based methods show superior performance by leveraging large numbers of clear/hazy images~\cite{li2018benchmarking,ancuti2020nh}.
% 
These methods either predict the components of the haze physical model~\cite{cai2016dehazenet,ren2016single,li2017aod,zhang2018densely} or directly restore the haze-free images in an image-to-image translation manner~\cite{li2018single,qu2019enhanced} using convolutional neural networks (CNNs).
% 
% Recent works propose more advanced network and module designs to improve the dehazing performance, including multi-branch aggregation~\cite{deng2019deep}, grid connection~\cite{liu2019griddehazenet}, gated fusion~\cite{chen2019gated}, channel and pixel attention~\cite{qin2020ffa}, multi-scale feature fusion~\cite{dong2020multi}, and Transformer-based architecture~\cite{guo2022image,song2023vision}.
Recent works propose more advanced network and module designs to improve the dehazing performance~\cite{deng2019deep,liu2019griddehazenet,chen2019gated,qin2020ffa,dong2020multi,liu2021synthetic,guo2022image,song2023vision}.
% \eg, attention mechanism~\cite{qin2020ffa}, multi-scale feature fusion~\cite{dong2020multi}, and Transformer~\cite{guo2022image}.
% binocular images~\cite{pang2020bidnet}, 
% In addition, some methods further boost dehazing performance by leveraging domain adaptation~\cite{shao2020domain}, knowledge distillation~\cite{hong2020distilling}, and contrastive learning~\cite{wu2021contrastive}.
% with real hazy images
% Besides, some other works focus on semantic scene understanding from the hazy images~\cite{sakaridis2018semantic,bijelic2020seeing}.
%
However, applying image dehazing methods to videos leads to discontinuous results since the temporal information is simply ignored.

% % 
% Early methods rely on the atmospheric scattering model and physical priors, \eg, dark channel prior~\cite{he2010single} and non-local prior~\cite{berman2016non}.
% % 
% % Please refer to~\cite{li2018benchmarking} for a more comprehensive review.
% % 
% Later, deep-learning-based methods show superior performance by leveraging large numbers of clear/hazy images~\cite{li2018benchmarking,ancuti2020nh}.
% % 
% DehazeNet~\cite{cai2016dehazenet} and MSCNN~\cite{ren2016single} are the pioneers to estimate transmission maps using convolutional neural networks (CNNs).
% % 
% Following works either predict the components of the haze physical model based on \cref{eq:physical model}, such as AOD-Net~\cite{li2017aod} and DCPDN~\cite{zhang2018densely}, or directly restore the haze-free images in an image-to-image translation manner~\cite{li2018single,qu2019enhanced}.
% % 
% Recent works propose more advanced network and module designs to improve the dehazing performance, including multi-branch aggregation~\cite{deng2019deep}, grid connection~\cite{liu2019griddehazenet}, gated fusion~\cite{chen2019gated}, channel and pixel attention~\cite{qin2020ffa}, multi-scale feature fusion~\cite{dong2020multi}, and Transformer-based architecture~\cite{guo2022image}.
% % 
% In addition, some methods further boost dehazing performance by leveraging binocular images~\cite{pang2020bidnet}, domain adaptation with real hazy images~\cite{shao2020domain}, knowledge distillation~\cite{hong2020distilling}, and contrastive learning~\cite{wu2021contrastive}.
% % 
% Besides, some other works focus on semantic scene understanding from the hazy images~\cite{sakaridis2018semantic,bijelic2020seeing}.


\begin{figure}
    \centering
    \captionsetup[subfigure]{justification=centering}
    % 
    \includegraphics[width=0.98\hsize]{./figs/fig_analysis}
    % 
    \vspace{-3mm}
    \caption{Dataset analysis of our HazeWorld, which contains diverse scenarios and supports various downstream evaluations.}
    \label{fig:analysis}
    \vspace{-5mm}
\end{figure}


% Dataset: Cityscapes~\cite{sakaridis2018semantic,pang2020bidnet}
\vspace{2mm}
\noindent
\textbf{Video dehazing.}
% 
Video dehazing methods leverage temporal information from the adjacent frames to enhance the restoration quality.
% 
Early methods mainly focus on post-processing to generate temporally consistent results by refining transmission maps and suppressing artifacts~\cite{kim2013optimized,chen2016robust} or joint estimating depths from videos~\cite{li2015simultaneous}.
% 
Li~\etal~\cite{li2018end} present a CNN to optimize dehazing and detection in videos end-to-end.
% 
Ren~\etal~\cite{ren2018deep} use semantic information to regularize the estimated transmission and to improve the video dehazing performance.
% 
%These methods are trained using the synthetic videos based on indoor datasets, \ie TUM RGB-D~\cite{sturm2012benchmark} and NYU-Depth V2~\cite{silberman2012indoor}.
% 
More recently, Zhang~\etal~\cite{zhang2021learning} collect a real indoor video dehazing dataset~(REVIDE) and present a confidence-guided and improved deformable network.
%
Liu~\etal~\cite{liu2022phase} design a phase-based memory network for video dehazing.
% 
Additionally, a neural compression-based method~\cite{huang2022neural} for video restoration shows better performance on REVIDE.
% 
However, these methods are mainly trained and evaluated in indoor scenes, and their performance in complex outdoor scenarios is limited.

\vspace{2mm}
\noindent
\textbf{Video alignment.}
% 
Alignment aims at obtaining spatial transformation and pixel-wise correspondence from the misaligned frames.
% 
Video restoration methods rely on explicit optical flow estimation~\cite{ranjan2017optical} to align the adjacent images/features~\cite{chan2021basicvsr,huang2022neural,yang2019frame}.
% 
Other methods~\cite{tian2020tdan,wang2019edvr} leverage deformable convolutions~\cite{dai2017deformable} to learn the offsets for feature alignment.
% 
These methods usually perform the frame-to-frame alignment.
%
More recently, attention~\cite{vaswani2017attention,dosovitskiy2020image,xia2022vision} with a large receptive field has been used together with the optical flow for feature alignment~\cite{cao2021video,lin2022flow,liang2022recurrent}.
% 
Besides, STTN~\cite{kim2018spatio} also considers multiple frames but only transforms the input images at one space-time range.

% \noindent \textbf{Vision Transformer.}
% % 
% Transformer~\cite{vaswani2017attention} involves self-attention mechanisms, and ViT~\cite{dosovitskiy2020image} is the pioneer in showing the effectiveness of Transformer in the image domain.
% % 
% Successive improvements work on building multi-scale structures and efficient attention mechanisms for dense prediction tasks~\cite{liu2021swin,wang2021pyramid,chu2021twins}.
% % 
% % These attention mechanisms include windowed attention~\cite{liu2021swin}, global subsampled attention~\cite{wang2021pyramid}, and alternative window and global subsampled attention~\cite{chu2021twins}.
% % 
% Given such success on high-level vision tasks, recent attempts adapt the attention mechanisms to low-level image and video processing.
% % 
% SwinIR~\cite{liang2021swinir} and Restormer~\cite{zamir2022restormer} involve Swin-like or specially designed Transformer layers and serve as strong baselines for image restoration.
% % 
% Video processing methods utilize transformer blocks to capture temporal dependencies in various tasks, including video super-resolution~\cite{cao2021video}, video deblurring~\cite{lin2022flow}, video inpainting~\cite{zhang2022flow}.
% % 
% Nevertheless, it is critical to leverage the haze-related priors in Transformer designing for video dehazing.
% % 

