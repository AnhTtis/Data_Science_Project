\section{HazeWorld Dataset}

Since the current video dehazing datasets are mostly collected in indoor scenes, we construct a large-scale synthetic outdoor video dehazing dataset named HazeWorld, with example frames shown in~\cref{fig:hazeworld}.
% 

\vspace{1.5mm}
\noindent
\textbf{Data collection.}
% 
The original videos of HazeWorld are from six existing datasets, \ie, Cityscapes~\cite{cordts2016cityscapes}, DDAD~\cite{guizilini20203d}, UA-DETRAC~\cite{wen2020ua}, VisDrone~\cite{zhu2021detection}, DAVIS~\cite{pont20172017}, and REDS~\cite{nah2019ntire}, resulting in 1,271 haze-free videos.
% , which contains various real-world scenarios.
% based on six existing datasets.
% 
We use the atmospheric scattering model~\cref{eq:physical model} to synthesize hazy videos.
% based on the haze-free frames
The robust video depth estimation method~\cite{kopf2021robust} is used to obtain temporally consistent depth maps.
% 
% We use the atmospheric scattering model~\cref{eq:physical model} to synthesize hazy videos, with estimated temporally consistent depth maps~\cite{kopf2021robust}.
% 
We follow~\cite{sakaridis2018semantic,bijelic2020seeing} and choose $\beta \in \{0.005,0.01,0.02,0.03\}$ to generate transmission $t$, and 
randomly select $A\in [0.75,1.0]$ for each video.
% , which is corresponding to meteorological optical ranges of around $600m$, $300m$, $150m$, and $100m$, respectively.
% The value of atmospheric light is randomly selected $A\in [0.75,1.0]$ for each video.
% 
We split 1,271 haze-free videos into training (897 videos) and testing (374 videos) sets.
%
Overall, we obtain 3,588 and 1,496 hazy synthetic videos with four $\beta$ of around 240,000 and 86,000 frames in training and testing sets, respectively.

\vspace{1.5mm}
\noindent
\textbf{Dataset analysis.}
% 
As shown in \cref{fig:analysis}, our dataset contains diverse real-world scenarios, which enables us to assess dehazing performance on various outdoor applications, such as autonomous driving~\cite{cordts2016cityscapes,guizilini20203d}, video surveillance~\cite{wen2020ua,zhu2021detection}, and photo editing~\cite{nah2019ntire}.
% 
Further, the original datasets contain the labels of multiple video and image downstream scene understanding tasks, \eg, video panoptic segmentation~\cite{kim2020video}, object segmentation~\cite{pont20172017}, depth estimation~\cite{guizilini20203d}, and image semantic segmentation~\cite{cordts2016cityscapes}. Thus we can evaluate the effectiveness of dehazing on high-level vision tasks.

% \xwhu{need some good-locking figure to show the characteristics of this dataset, such as multiple class, large number, some down-stream task labels, etc....}

% More details are provided in the supplementary material.

% City
% Cityscapes
%     Germany, France, Switzerland
% DDAD
%     USA:    ANN - Ann Arbor, MI, SF - San Francisco Bay Area, CA; DET - Detroit, MI; CAM - Cambridge, Massachusetts
%     Japan:  Japan locations: Tokyo and Odaiba.
% UA-DETRAC
%     China:  Beijing, Tianjin
% VisDrone
%     China:  14 cities
% REDS
%     Korea
    
% Location
%     France, Germany, Switzerland, China, Japan, Korea, United States

% Senario
%     Automous driving, Video surveillance, Natural, Photo editting

% # Videos
% # 540, 200, 83, 70, 117, 261

% # Frames
% # 16,200, 16,600, 8,235, 6,121, 8,198, 26,100

% Foregroud
    

% Background
%     Road, Highway, Street, Urban, Rural, Field, Yard, Wall, School, Playground, Zoo, Park, Sea, Mountain, Forest

% View
%     Vehicle mount, Bird's eye, Hand-held
