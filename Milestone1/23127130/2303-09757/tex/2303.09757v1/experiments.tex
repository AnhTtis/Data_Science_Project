\section{Experimental Results}

\input{vis/vis_main_hazeworld.tex}
\input{vis/vis_main_revide.tex}

\subsection{Settings}
% 
\noindent
\textbf{Datasets.}
% 
We evaluate the effectiveness of the proposed MAP-Net on our dataset, \ie, HazeWorld, and the widely-used REVIDE dataset~\cite{zhang2021learning}.
% 
% 3,588 training videos and 1,496 testing videos,
HazeWorld contains 3,588 training videos and 1,496 testing videos.
% , and each video contains 30 to 100 consecutive frames
% \ie, Cityscapes, UA-DETRAC, DAVIS${}_{17}$, and REDS
% We then randomly select four subsets with 2,780 hazy videos for training and use all the remaining six subsets with 1,496 hazy videos for testing.
%  
Meanwhile, REVIDE consists of 42 training videos and 6 testing videos.
% , and each video has 11 to 66 frames

\vspace{2mm}
\noindent
\textbf{Evaluation metrics.}
We utilize PSNR and SSIM to quantitatively evaluate the dehazing performance.
% of our MAP-Net and compared methods

\vspace{2mm}
\noindent
\textbf{Comparison methods.}
On HazeWorld, we compare our method against state-of-the-art methods, including ten image dehazing methods (\ie, DCP~\cite{he2010single}, AOD~\cite{li2017aod}, GDN~\cite{liu2019griddehazenet}, DM2F~\cite{deng2019deep}, FFA~\cite{qin2020ffa}, MSBDN~\cite{dong2020multi}, UHD~\cite{zheng2021ultra}, AECR~\cite{wu2021contrastive}, Dehamer~\cite{guo2022image}, and DehazeFormer~\cite{song2023vision}), and three video dehazing methods (\ie, EVD~\cite{li2018end}, VDH~\cite{ren2018deep}, and CG-IDN~\cite{lai2018learning}).
% NLD~\cite{berman2016non}, GCA-Net~\cite{chen2019gated}, DA~\cite{shao2020domain}, KDDN~\cite{hong2020distilling}, 
We also compare several video restoration methods, including FastDVD~\cite{tassano2020fastdvdnet}, EDVR~\cite{wang2019edvr}, NCFL~\cite{huang2022neural}, and BasicVSR++~\cite{chan2021basicvsr++}.
% In addition, 
On REVIDE, we compare MAP-Net with state-of-the-art methods of~\cite{zhang2021learning}. 
% ~\cite{zhang2021learning}, who's performance is reported. 

\vspace{2mm}
\noindent
\textbf{Implementation details.}
% 
We use the AdamW optimizer and the polynomial scheduler.
%  with a power of 1.0
The initial learning rate is set as 2$\times$10${}^{-4}$.
%  with a warm-up start of 1,500 iterations
The total number of iterations is 40K.
% 
The batch size is eight, and the patch size of input video frames is 256$\times$256.
% 
The weights ${\lambda}_{phy}$ and ${\lambda}_{flow}$ in \cref{eq:loss} are empirically set as 0.2 and 0.04.
% 
% More details are provided in the supplementary material.

\subsection{Comparisons with State-of-the-Art Methods}
% 
\noindent
\textbf{Quantitative comparison.}
% 
Table~\ref{tab:exp-hazeworld} summarizes the quantitative results of our network and compared methods on HazeWorld.
% 
% We also compare two image dehazing methods (\ie, AECR~\cite{wu2021contrastive} and Dehamer~\cite{guo2022image}) with the video alignment method BasicVSR++~\cite{chan2021basicvsr++}, and their results are 23.47~dB and 23.89~dB, respectively.
%
From these quantitative results, we can find that our method outperforms other baselines by a significant margin.
% 
Specifically, among all compared methods, BasicVSR++~\cite{chan2021basicvsr++} and DehazeFormer~\cite{song2023vision} achieve the best PSNR score of 26.06 and the best SSIM score of 0.9286, respectively.
% on average and on five out of six subsets
%%
More importantly, our MAP-Net has a PSNR improvement of 1.06~dB over BasicVSR++, while our method has an SSIM gain of 0.0063 over DehazeFormer.
% 
%In particular, MAP-Net outperforms state-of-the-art image dehazing methods Dehamer~\cite{guo2022image} by 4.20~dB and DehazeFormer~\cite{song2023vision} by 1.91~dB in PSNR.
% 
%Besides, MAP-Net shows a performance gain of 1.06~dB on the recent video restoration method BasicVSR++~\cite{chan2021basicvsr++}.

% 
Table~\ref{tab:exp-REVIDE} shows the PSNR and SSIM of our network and state-of-the-art methods on REVIDE.
% 
Among all compared methods, NCFL~\cite{huang2022neural} has the best PSNR (23.63~dB) and the best SSIM (0.8925).
%Compared to NCFL, 
And our method further improves the PSNR from 23.63~dB to 24.16~dB and the SSIM from 0.8925 to 0.9043.
% Our method outperforms other recent dehazing and restoration methods, \eg, the video dehazing method CG-IDN~\cite{zhang2021learning} and the video restoration method NCFL~\cite{huang2022neural} by 0.53 dB.

\input{vis/vis_main_real.tex}

\vspace{2mm}
\noindent
\textbf{Qualitative comparison.}
% 
\cref{fig:visual_ours} and \cref{fig:visual_revide} visually compare dehazed results produced by our network and state-of-the-art methods on video frames from HazeWorld and REVIDE.
% 
Apparently, compared methods often tend to introduce color distortion, darken several areas, or preserve some haze in their dehazed results, while our MAP-Net can effectively remove haze, avoid color distortion, and better recover the underlying clear frames. 
And the predicted haze-free results produced by our method are closest to ground truths shown in the last column of \cref{fig:visual_ours} and \cref{fig:visual_revide}.


%%% 
Moreover, we also compare our network against state-of-the-art methods on real-world hazy videos, and the results are shown in~\cref{fig:visual_real}.
% 
From these visual results, we can find that existing methods tend to darken many areas, or maintain some haze.
% 
Compared to these methods, our network has a higher visual quality and less color distortion; see the last column of~\cref{fig:visual_real}.
% 
% Please refer to supplementary material for more visual results.


% 
\begin{table}[t]
\caption{\textbf{Comparison of downstream effectiveness.} VPQ, mIoU, RMSE, and J\&F are metrics for panoptic segmentation, semantic segmentation, depth estimation, and object segmentation.}
\label{tab:downstream}
\vspace{-1mm}
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{lccccccc}
\hline
Method                     & Hazy   & MSBDN~\cite{dong2020multi} & Dehamer~\cite{guo2022image} & EDVR~\cite{wang2019edvr} & BasicVSR++~\cite{chan2021basicvsr++} & Ours            & GT     \\
\hline
Cityscapes-VPQ $\uparrow$  & 21.7   & 40.2                       & 43.4                        & 47.2                     & 45.9                                 & \textbf{48.5}   & 56.5   \\
Cityscapes-mIoU $\uparrow$ & 51.8   & 47.0                       & 54.1                        & 64.8                     & 63.6                                 & \textbf{66.2}   & 75.4   \\
% DDAD-RMSE $\downarrow$     & 21.208 & 14.986                     & 15.005                      & 15.255                   & 14.885                               & \textbf{14.709} & 14.363 \\
DDAD-RMSE $\downarrow$     & 21.21  & 14.99                      & 15.01                       & 15.26                    & 14.89                                & \textbf{14.71 } & 14.36 \\
DAVIS-J\&F $\uparrow$      & 76.3   & 79.2                       & 79.4                        & 79.3                     & 79.4                                 & \textbf{80.0}   & 81.3   \\
\hline
\end{tabular}
\end{adjustbox}
\vspace{-5mm}
\end{table}

\vspace{2mm}
\noindent
\textbf{Applications.}
% 
Our video dehazing method benefits several downstream applications, including video panoptic segmentation~\cite{kim2020video}, object segmentation~\cite{pont20172017}, depth estimation~\cite{guizilini20203d}, and image semantic segmentation~\cite{cordts2016cityscapes}.
%%
% We choose four different methods~\cite{kim2020video,oh2019video,guizilini20203d,zhao2017pyramid} for corresponding downstream application validation.
% For each application, we adopt the corresponding method to obtain the results on the input hazy videos, the dehazed videos, and the underlying haze-free videos.
% 
To verify this, we choose four different methods~\cite{kim2020video,oh2019video,guizilini20203d,zhao2017pyramid} for corresponding downstream application validation, and obtain results on the input hazy videos, the dehazed videos, and the underlying haze-free videos.
% 
% The results on the input hazy videos, the dehazed videos, and the underlying haze-free videos are obtained using the corresponding method.
% 
% produced by our network and compared methods
% 
Table~\ref{tab:downstream} reports the quantitative results.
% of four applications
Apparently, the dehazed videos produced by different methods improve the downstream application performance compared to the original hazy videos.
% but our method can better facilitate downstream applications than other representative dehazing methods.
Notably, our method can better facilitate downstream applications than other representative dehazing methods.
% 
% As observed in \cref{tab:downstream}, our method not only generates visually appealing results but also benefits downstream vision tasks.
% 


% 
\begin{table}[t]
\caption{\textbf{Ablation studies of our MPG and MSR modules.}}
\label{tab:ablation-components}
\centering
% \begin{adjustbox}{width=1\columnwidth}
\footnotesize
\begin{tabular}{lcccc}
\hline
         & (a)        & (b)        & (c)        & Our method \\ \hline
Basic    & \checkmark & \checkmark & \checkmark & \checkmark \\
MPG      &            & \checkmark &            & \checkmark \\
MSR      &            &            & \checkmark & \checkmark \\ \hline
PSNR     & 25.37      & 26.24      & 26.38      & \textbf{27.12}      \\
SSIM     & 0.9087     & 0.9171     & 0.9292     & \textbf{0.9349}     \\ \hline
\end{tabular}
% \end{adjustbox}
\vspace{-3mm}
\end{table}
% 


\subsection{Ablation Studies}
% 
We conduct a series of ablation studies on our HazeWorld dataset to analyze the effectiveness of major components of our network.
% 
% To avoid overfitting, a smaller validation set with 120 randomly selected videos is used for ablation.
% 

\vspace{2mm}
\noindent
\textbf{Ablation studies of two major modules.}
% 
We start by constructing a basic model (denoted as ``Basic''), which has the only scene decoder, and STDA only considers one previous frame for temporal alignment.
% 
Then, we gradually add two proposed modules, \ie, MPG and MSR. 
% , to evaluate their effectiveness
% and then we gradually add each proposed design.
%%
Table~\ref{tab:ablation-components} compares their results.
% 
Compared to ``Basic'', the MPG module and the MSR module have a PSNR improvement of 0.87~dB and 1.01~dB, respectively, due to the physical model-based priors at the MPG module, and multiple temporal haze clues at the MSR module.
%%
Moreover, combining both MPG and MSR modules together into our method can further improve our video dehazing performance. 

% \begin{table}[t]
% \caption{\textbf{Ablation studies of the prior guidance.}}
% % multi-range alignment is used.
% \label{tab:ablation-mpg}
% \centering
% % \begin{adjustbox}{width=1\columnwidth}
% \small
% \begin{tabular}{lcccc}
% \hline
% Alignment   &            & \checkmark &            & \checkmark \\
% Aggregation &            &            & \checkmark & \checkmark \\ \hline
% PSNR        & 26.38      & 26.92      & 26.61      & 27.14      \\
% SSIM        & 0.9292     & 0.9330     & 0.9326     & 0.9342     \\ \hline
% \end{tabular}
% % \end{adjustbox}
% \end{table}


\vspace{2mm}
\noindent
\textbf{Effectiveness of guidance information in our MSR module.}
% 
As shown in ~\cref{fig:overview}, our MPG module learns prior guidance features, \ie, $\mathcal{J}$ and $\mathcal{P}$, to guide the STDA block to align video frames and the GMRA block to aggregate features in our MSR module for video dehazing.
% 
Therefore, we conduct an ablation study to evaluate the effectiveness of the guidance information for the STDA block and the GMRA block, respectively.
% 
We achieve this by building three baseline networks: (1) we remove guidance information from both the STDA block and the GMRA block; (2) we only use the guidance at the STDA block; (3) we only utilize the guidance at the GMRA block.
% 
Note that we keep both the STDA block and the GMRA block in our MSR module but only remove the guidance parts during the ablation.
%
As shown in Table~\ref{tab:ablation-prior}, our video dehazing performance is reduced if we remove the prior guidance information from either the STDA block or the GMRA block.

% 
\begin{table}[t]
    \caption{\textbf{Effectiveness of the guidance at the STDA and GMRA blocks of the MSR module.}}
    \label{tab:ablation-prior}
    \vspace{-2mm}
    \centering
    \begin{adjustbox}{width=0.7\columnwidth}
    \footnotesize
    \begin{tabular}{lcccc}
    \hline
    STDA   &            & \checkmark &            & \checkmark     \\
    GMRA   &            &            & \checkmark & \checkmark     \\
    \hline
    PSNR   & 26.38      & 26.92      & 26.61      & \textbf{27.12}  \\
    \hline
    % SSIM        & 0.9292     & 0.9337     & 0.9333     & \textbf{0.9349}     \\ \hline
    \end{tabular}
    \end{adjustbox}
    \vspace{-2mm}
\end{table}
% 
\begin{table}[t]
    \caption{\textbf{Ablation studies of the number of ranges in MSR.}}
    \label{tab:ablation-msr}
    \centering
    \vspace{-2mm}

    \begin{subtable}[t]{\hsize}
    \caption{Discussion on the number of ranges.}
    % Haze guidance
    \label{tab:ablation-range}
    \centering
    \begin{adjustbox}{width=0.7\hsize}
    \footnotesize
    \begin{tabular}{lcccc}
    \hline
    \#Range  & 1       & 2       & 3 (Ours)         & 4       \\ \hline
    PSNR     & 26.24   & 26.71   & \textbf{27.12}   & 26.84   \\ \hline
    % SSIM     & 0.9171  & 0.9337  & \textbf{0.9349}  & 0.9339  \\ \hline
    \end{tabular}
    \end{adjustbox}
    \end{subtable}

    \begin{subtable}[t]{\hsize}
    \caption{Discussion on the temporal alignment.}
     % when the number of range is 3. DWA+$\mathcal{L}_{flow}$.
    \label{tab:ablation-align}
    \centering
    \begin{adjustbox}{width=0.7\hsize}
    \small
    \begin{tabular}{lcccc}
    \hline
    Manner  & 1set    & 3sets-1range  & Our method      \\ \hline
    PSNR    & 26.39   & 26.64         & \textbf{27.12}  \\ \hline
    % SSIM    & 0.9304          & 0.9178     & \textbf{0.9349} \\ 
    % & Joint & 26.57 & & 0.9207
    \end{tabular}
    \end{adjustbox}
    \end{subtable}
    \vspace{-5mm}
\end{table}
% 

%% supp?
% Moreover, we construct another network (denoted as ``ours-w/o-memory'') by removing the memory mechanism to learn the guidance features in our MPG module. It means that ``ours-w/o-memory'' utilizes only the current video frame for computing guidance features. 
% Our larger PSNR score than ``ours-w/o-memory'' demonstrates that the memory mechanism in our MPG module learns a better guidance information for the STDA block and the GMRA block, thereby obtaining a superior video dehazing performance.

\if 0
To demonstrate the effectiveness of prior guidance, we compare our method with three variants by modifying our MPG module: (1) No prior guidance is used, \ie, only a scene decoder is adopted with multi-range alignment. (2) The prior guidance is only used in STDA of MSR, \ie, the prior-guided feature from MPG is used as input for STDA. (3) The prior guidance is only used in GMRA of MSR, \ie, attention weights from the prior perspective are used for multi-range feature aggregation.
% 
As shown in~\cref{tab:ablation-mpg}(b), the prior guidance plays a critical role in our framework, which brings substantial improvement when the prior guidance is used in each part.
% 
The full version of our method achieves better results by thoroughly leveraging the prior guidance.
\fi

% 
% (Figure of two + Figure of estimated transmission)
% Number of transmission categories. {1, 16, 32, 64}, feature volume ($>$1) or not (==1). Uniform or linear increasing (appendix).
% Memory configurations. {0, }. Memory ($>$1) or not (==0)


\if 0
We further study the effectiveness of memory usage and the feature guidance integration design.
% 
\cref{tab:ablation-memory} shows that the memory information of prior features in a video can further improve the baseline that only decodes the prior from the target frame.
\fi
% 
% Besides, our feature guidance integration manner (see~\cref{sec:pfg}) improves upon the variant with simple prior and scene feature concatenation followed by convolution layers, as shown in~\cref{tab:ablation-integration}.
% , by leveraging the hidden information in transmission

\vspace{2mm}
\noindent
\textbf{Discussion on different ranges of our MSR module.}
% 
We perform two ablation study experiments on our MSR module: one is to discuss the number of space-time ranges used in our MSR module, while another experiment is to discuss how to leverage multiple adjacent frames when the number of ranges is fixed.
% 
Table~\ref{tab:ablation-range} reports PSNR scores of our video dehazing with different numbers of ranges.
% 
We can observe that our PSNR is progressively improved when we increase the number of ranges from one to three. The reason is that a larger range value involves more temporal information for aligning video frames.
%%
However, when we further increase the range value from three to four, our video dehazing performance is reduced
% since more forward iterations in STDA may introduce training difficulty.
since the weights are shared for the STDA blocks, where a single STDA needs to tackle different ranges of temporal alignment.
% 
Hence, a larger range may introduce difficulty in the flow estimation, and thus we empirically set the number of frames/ranges as three.

\if 0
% In theory, a larger number of ranges means more temporal information can be explored in alignment.
% 
However, while the performance gain is significant when the number of ranges increases from one to three, no additional benefit is observed when more ranges are adopted, \eg, four.
% 
We credit this to the issue of training difficulty.
% 
Hence, we set the number of ranges to three as default in other experiments. 
\fi

Moreover, we further discuss how to use the adjacent video frames for temporal alignment when the number of frames is fixed as three.
% 
Here, we construct two baselines: (1) the 1st baseline (denoted as ``1set'') is to change the three-set alignment in our method to only one set, which takes all three neighboring video frames.
% 
(2) the 2nd baseline (denoted as ``3sets-1range'') is constructed by only using one adjacent frame in each set, \ie, frame-by-frame alignment (three sets in total).
% two-frame
As shown in Table~\ref{tab:ablation-align}, our method has a better PSNR value than ``1set'' and ``3sets-1range'', which indicates that considering three frames at multiple ranges incurs a better video dehazing performance.
% 
%\emph{Please see supplementary material for more ablation studies of our framework.}

\if 0
merge compare our multi-range alignment method with other variants: (1) Frame-by-frame alignment~\cite{zhang2021learning,wang2019edvr} considers one adjacent frame each time.
% (2) Joint alignment~\cite{liang2022recurrent} takes the concatenated features from several adjacent frames as keys and values for attention at one time.
(2) One-range alignment uses our space-time flow sampling for multiple frames but only once and without multi-range aggregation.
% 
It can be seen from \cref{tab:ablation-multi} that our method outperforms other variants by considering: (1) temporal information hidden in consecutive frames, (2) effectiveness in designing temporal keys and values, and (3) complementary temporal haze clues from different space-time resolutions.
\fi


% \begin{table}[th!]
% \caption{\textbf{Effectiveness of the guidance at the STDA and GMRA blocks of the MSR module.}}
%     \label{tab:ablation-mpg}

%     \begin{subtable}[t]{0.9\hsize}
%     %\caption{Effectiveness of the guidance at the STDA and GMRA blocks of the MSR module.}
%     % multi-range alignment is used.
%     \label{tab:ablation-guidance}
%     \centering
%     \begin{adjustbox}{width=0.9\hsize}
%     \small
%     \begin{tabular}{lcccc}
%     \hline
%     STDA   &            & \checkmark &            & \checkmark \\
%     GMRA &            &            & \checkmark & \checkmark \\ \hline
%     PSNR        & 26.38      & 26.92      & 26.61      & \textbf{27.12}      \\ \hline
%     % SSIM        & 0.9292     & 0.9337     & 0.9333     & \textbf{0.9349}     \\ \hline
%     \end{tabular}
%     \end{adjustbox}
%     \end{subtable}

%     % \hfill
%     % \begin{subtable}[t]{0.48\hsize}
%     % \caption{Ablation studies of feature guidance.}
%     % % multi-range alignment is used.
%     % \label{tab:ablation-integration}
%     % \raggedright
%     % \begin{adjustbox}{width=\hsize}
%     % \small
%     % \begin{tabular}{lcc}
%     % \hline
%     % Integration & conv   & ours   \\ \hline
%     % PSNR        & 27.05  & \textbf{27.12}  \\
%     % SSIM        & 0.9338 & \textbf{0.9349} \\ \hline
%     % \end{tabular}
%     % \end{adjustbox}
%     % \end{subtable}
% \end{table}

% % % 
% Supplymentary
% % % 
