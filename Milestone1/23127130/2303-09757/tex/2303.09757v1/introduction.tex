\section{Introduction}
\label{sec:intro}

Haze largely degrades the visibility and contrast of the outdoor scenes, which adversely affects the performance of downstream vision tasks, such as the detection and segmentation in autonomous driving and surveillance.
% 
% Dehazing aims to restore clear images given such perceived hazy inputs.
% 
According to the atmospheric scattering model~\cite{mccartney1976optics,he2010single}, the formation of a hazy image is described as:
% 
\begin{equation}
    \label{eq:physical model}
    I(x)=J(x)t(x)+A(1-t(x)) \ ,
\end{equation}
% 
where $I, J, A, t$ denote the observed hazy image, scene radiance, atmospheric light, and transmission, respectively, and $x$ is the pixel index.
% 
% When the atmosphere is homogeneous, 
The transmission $t = e^{-\beta d(x)}$ describes the scene radiance attenuation caused by the light scattering, where $\beta$ is the scattering coefficient of the atmosphere, and $d$ denotes the scene depth.
% 
%\begin{equation}
%    \label{eq:physical model-1}
%    t(x) \ = \ e^{-\beta d(x)} \ ,
%\end{equation}
% 
%where $\beta$ is the scattering coefficient of the atmosphere, and $d$ is the scene depth.

%Previous single-image dehazing methods rely on physical priors and observed statistics~\cite{fattal2008single,he2010single,berman2016non}.
% 
%Recent CNN-based methods show superior dehazing performance by estimating the physical model-based components~\cite{cai2016dehazenet,ren2016single,li2017aod,zhang2018densely} or directly predicting the clean images~\cite{li2018single,ren2018gated,qu2019enhanced,dong2020multi}.
%, given collected hazy image datasets~\cite{li2018benchmarking,ancuti2020nh}.
% 
% However, single-image dehazing is a challenging ill-posed problem due to the ambiguity such as scene depth, thus leading to sub-optimal restoration results.
% 

\input{vis/vis_main_comparison.tex}

Video dehazing benefits from temporal clues, such as highly correlated haze thickness and lighting conditions, as well as the moving foreground objects and backgrounds.
%
% Thus, the prior temporal information from the video can reduce the ambiguity in the current frame and enhance the local details.
% 
Early deep learning-based video dehazing methods leverage temporal information by simply concatenating input frames or feature maps~\cite{ren2018deep,li2018end}.
% 
Recently, GC-IDN~\cite{zhang2021learning} proposes to use cost volume and confidence to align and aggregate temporal information.
% 
However, existing video dehazing methods suffer from several limitations.
% 
First, these approaches either obtain haze-free frames from the physical model-based component estimation~\cite{ren2018deep,li2018end} or ignore the explicit physical prior embedded in the haze imaging model~\cite{zhang2021learning}.
% in videos
The former suffers from inaccurate intermediate prediction, thus leading to error accumulation in the final results, while the latter overlooks the physical prior information, which plays an important role in haze estimation and scene recovery.
% haze-free
Second, these methods aggregate temporal information by using input/feature stacking or frame-to-frame alignment in a local sliding window, which is hard to obtain global and long-range temporal information.
% 
%Since the haze density may vary across the neighbor frames, considering temporal correspondences from several timesteps simultaneously and multiple time intervals provide more coherent and complementary hints on the haze artifact and scene radiance.


In this work, we present a novel video dehazing framework via a Multi-range temporal Alignment network with Physical prior (MAP-Net) to address the aforementioned issues.
% 
%\xwhu{Should talk about our two main contribution step by step And highlight them using First, Second!}
% 
First, we design a memory-based physical prior guidance module, which aims to inject the physical prior to help the scene radiance recovery.
% 
Specifically, we perform feature disentanglement according to the physical model with two decoders, where one estimates the transmission and atmospheric light, and the other recovers scene radiance.
% 
The feature extracted from the first decoder is leveraged as the physical haze prior, which is integrated into the second decoder for scene radiance recovery.
% 
To infer the global physical prior in a long-range video, we design a physical prior token memory that effectively encodes prior-related features into compact tokens for efficient memory reading.
% 
%\xwhu{Any relations between these two sentences?}
% 
% To achieve this, we consider the transmission prediction as a classification task and use the transmission category to represent the category-level prior feature.
% 
% This process captures non-local dependencies in one frame, which is similar to the non-local prior~\cite{berman2016non} but in the transmission category domain.
% % 
% To mitigate the misalignment issue, we first use global attention to acquire the most relevant regions in the latent features corresponding to the current ones, viewed as the reference points for the local attention queries.
% % 

Second, we introduce a multi-range scene radiance recovery module to capture space-time dependencies in multiple space-time ranges.
% 
This module first splits the adjacent frames into multiple ranges, then aligns and aggregates the corresponding recurrent range features, and finally recovers the scene radiance.
% , as the haze thickness of corresponding pixel locations may vary at different times.
% 
Unlike CG-IDN~\cite{zhang2021learning}, which aligns the adjacent features frame-by-frame, we align the features of adjacent frames into multiple sets with different ranges, which helps to explore the temporal haze clues in various time intervals.
%at once as one range feature, which effectively explores the helpful haze and scene cues hidden in long-range time intervals.
% 
We further design a space-time deformable attention to warp the features of multiple ranges to the target frame, followed by a guided multi-range complementary information aggregation.
% 
Also, we use an unsupervised flow loss to encourage the network to focus on the aligned areas and train the whole network in an end-to-end manner.

In addition, the existing learning-based video dehazing methods are mainly trained and evaluated on indoor datasets~\cite{ren2018deep,li2018end,zhang2021learning}, which suffer from performance degradation in real-world outdoor scenarios.
% 
% Further, videos captured in similar environments will restrict the generalization ability of the trained models.
% 
Thus, we construct an outdoor video dehazing benchmark dataset, HazeWorld, which has three main properties.
% 
First, it is a large-scale synthetic dataset with 3,588 training videos and 1,496 testing videos.
% 
Second, we collect videos from diverse outdoor scenarios, \eg, autonomous driving and life scenes.
% 
Third, the dataset has various downstream tasks for evaluation, such as segmentation and detection.
%
% and can be applied to evaluate the performance of various downstream applications. \xwhu{Please check this sentence is correct or not?}
%
Various experiments on both synthetic and real datasets demonstrate the effectiveness of our approach, which clearly outperforms the existing image and video dehazing methods; see Fig.~\ref{fig:comparison}.
%and achieves state-of-the-art performance.
% 
The code and dataset are publicly available at \url{https://github.com/jiaqixuac/MAP-Net}.

Our main contributions are summarized as follows:
% 
% \xwhu{TO be modified! Talk about the dataset also.}
\begin{itemize}
\itemsep0em 
  \item We present a novel framework, MAP-Net, for video dehazing. A memory-based physical prior guidance module is designed to enhance the scene radiance recovery, which encodes haze-prior-related features into long-range memory.
  % It aligns the temporal information in multiple space-time ranges and introduces the physical prior into guided scene radiance restoration.
  \item We introduce a recurrent multi-range scene radiance recovery module with the space-time deformable attention and the guided multi-range aggregation, which effectively captures long-range temporal haze and scene clues from the adjacent frames.
  \item We construct a large-scale outdoor video dehazing dataset with diverse real-world scenarios and labels for downstream task evaluation.
  \item Extensive experiments on both synthetic and real conditions demonstrate our superior performance against the recent state-of-the-art methods.
  % (more visually pleasing results in real-world video, downstream tasks)
\end{itemize}
