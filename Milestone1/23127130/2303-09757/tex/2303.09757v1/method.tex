\section{Methodology}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\hsize]{./figs/fig_overview}
    \vspace{-2.5mm}
    \caption{The overall framework of our MAP-Net for video dehazing. MAP-Net is a U-Net-like structure that mainly contains an encoder, a prior decoder, and a scene decoder. Features are processed interactively in the prior decoder and scene decoder, which jointly perform feature disentanglement. The former produces the prior guidance with a memory, and the latter recovers the scene recurrently.}
    \vspace{-3mm}
    \label{fig:overview}
\end{figure*}

\subsection{Overall Framework}
% Let $I\in \mathbb{R}^{T \times H \times W \times 3}$ be a sequence of hazy RGB input frames and $J\in \mathbb{R}^{T \times H \times W \times 3}$ be a sequence of clear RGB target frames.
% % 
% $T$, $H$, and $W$ are the frame number, height, and width, respectively.
% % 
% Video dehazing aims to restore the haze-free frames from their hazy input.

\cref{fig:overview} illustrates the overall framework of the proposed MAP-Net, which
% 
% MAP-Net restores the hazy video sequentially, which removes the haze artifacts of the current frame by leveraging the extracted radiance temporal features from previous neighboring frames and global physical prior token memory features.
% 
is a U-Net-like structure that mainly consists of an encoder, a prior decoder, and a scene decoder.
% \xwhu{Cannot find the corresponding parts in figure.}
% 
A common image backbone, \eg, ConvNeXt~\cite{liu2022convnet}, is used as the feature encoder, which extracts the multi-scale feature maps.
% , (\ie, scales of 1/4, 1/8, 1/16, 1/32), for large receptive fields and efficient computation.
%
At each scale, features are processed interactively in the prior decoder layer and scene decoder layers.
% 
The initial prior feature $\tilde{\mathcal{P}}$ and initial scene feature $\tilde{\mathcal{J}}$ are first fed into a Memory-based Physical prior Guidance (MPG) module (see~\cref{sec:MPG}), which aims to obtain the memory-enhanced prior feature $\mathcal{P}$ and the prior-guided scene feature $\mathcal{J}$.
% 
Then, $\mathcal{P}$ and $\mathcal{J}$ are fed into a Multi-range Scene radiance Recovery (MSR) module (see~\cref{sec:MSR})
, which is to obtain the feature for the haze-free scene by aligning and aggregating recurrent temporal features from the adjacent frames.
% 
%a haze feature decoder followed by a Memory-based Haze Guidance (MHG) module (see~\cref{sec:MHG}) and a radiance feature decoder based on a Multi-range Radiance Recovery (MRR) module (see~\cref{sec:MRT}).
%These two decoders jointly perform the feature disentanglement according to the physical model \cref{eq:physical model-0}.
% 
% \xwhu{It is hard to find these two decoders in figure, try to find a better way to describe them.}
% 
The prior decoder and scene decoder jointly perform feature disentanglement according to the physical model.

Specifically, the prior decoder learns the prior-related feature by predicting the transmission and atmospheric light, and the scene decoder generates the scene radiance.
% 
The intermediate components are obtained using separate prediction heads and reconstructing the hazy input via~\cref{eq:physical model}, which are supervised by a physical model disentanglement loss.
%\jqxu{Upsample, Output}
%
Moreover, pixel shuffle layers~\cite{shi2016real} are used to upsample the features in two decoders and the output from the last scene decoder layer.
% , and skip connections are used
% 
Lastly, residual prediction is used to produce the final dehazed result.

% \textbf{Loss.}
% We use the commonly used L1 loss between the dehazed sequence $\hat{J}$ and the ground-truth sequence $J$ since the robustness of L1 norm to the outliers.
% 
% \begin{equation}
%     L=\displaystyle || \hat{J}-J ||_1
% \end{equation}

\subsection{Memory-Based Physical Prior Guidance}
\label{sec:MPG}
%\vspace{-2mm}

We design a Memory-based Physical prior Guidance (MPG) module to enhance the scene recovery by encoding haze prior-related features into the long-range memory.
%
% As shown in Fig.~\ref{fig:mpg}, we split MPG module into two parts, where the first part formulates a Prior Memory Enhancement (PME) sub-module to generate the memory-enhanced prior while the second part designs a Prior Feature Guidance (PFG) sub-module to produce the final prior-guided feature, which is used to recover the scene radiance.
% 
\cref{fig:mpg} shows the architecture of MPG with three parts.
% , which contains the following three parts.
%\jqxu{(\textbf{TODO})} \jqxu{As shown in Fig.~\ref{fig:mpg}, MPG mainly produces two features, \ie, the memory-enhanced prior and the prior-guided feature, which is used to recover the scene radiance.}

% \subsubsection{Prior Memory Enhancement}
% 
% According to \cref{eq:physical model-0}, physical prior-related information, \ie, transmission and atmospheric light, helps recover the scene radiance since they describe the color distortion by haze.
% 
%MPG enhances the initial prior feature implicitly learned in the prior decoder with a long-range prior token memory and produces the prior-guided scene feature for subsequent scene recovery.
% 
%We describe the physical prior learning and transmission estimation first, which is used in the Prior Memory Enhancement (PME) module and Prior Feature Guidance (PFG) module.
% 
% Moreover, the haze information is temporally correlated for transmission distribution and atmospheric light, \eg, the global atmospheric light stays almost unchanged for a period.
% 
% Therefore, historical features in a video can provide complementary haze prior cues.

\vspace{2mm}
\noindent
\textbf{Physical prior compression.}
% 
% We omit the time and scale index in the following for simplicity.
% 
% The input features are first refined with two convolution layers to obtain $\mathcal{P}$.
% Then, to encode the prior information into $\mathcal{P}$, two heads composed of a few convolution layers are applied on top of it to predict the haze-related components, \ie, transmission and the atmospheric light.
The initial prior feature $\tilde{\mathcal{P}} \in \mathbb{R}^{H \times W \times C}$ is implicitly learned using several convolution layers on the upsampled features to predict the transmission map and atmospheric light.
%  
$H$, $W$, and $C$ denote the feature height, width, and channel size.
% respectively
%and by estimating transmission and atmospheric light, where $H,W,C$ are the feature height, width, and channel size.
% 
Since we aim to save the physical priors at different times in the memory, we need to compress the size of each prior to reduce memory space.
%
To achieve this, we first perform a discretization operation on the prior feature by using categorical classification~\cite{fu2018deep,huang2022monodtr} and then normalize the results via the Softmax function.
%
%Instead of predicting the transmission as a single continuous value at each position, we formulate the transmission estimation as a categorical classification inspired by~\cite{fu2018deep,huang2022monodtr}.
% 
Specifically, from the initial prior, we generate the transmission distribution map $\mathcal{D} \in \mathbb{R}^{H \times W \times D}$, where $D$ is the number of transmission categories.
%
%
\if 0
the transmission distribution map is obtained by a weighted summation: 
% 
\begin{equation}
    t(x)=\sum_{d=1}^{D}{\mathcal{D}_d(x) \cdot t_d},
\end{equation}
% 
where $\mathcal{D}_d(x)$ denotes the probability that the transmission value at location $x$ belongs to a transmission category $d$, and $t_d \in \{t_1, t_2, ..., t_{D}\}$ is a set of representative values that discretize the transmission interval $[0, 1]$, \eg, uniform discretization~\cite{fu2018deep} with $t_d=(d-1)/(D-1)$.
\fi
%
After that, we perform matrix multiplication between the initial prior $\tilde{\mathcal{P}}$ and the transmission distribution map $\mathcal{D}$ to obtain the value $\textbf{p} \in \mathbb{R}^{D \times C}$, which is the compressed prior token. 

\begin{figure}
    \centering
    \includegraphics[width=\hsize]{./figs/fig_mpg}
    \caption{The illustration of the Memory-based physical Prior Guidance (MPG) module, which has (a) physical prior compression, (b) memory-enhanced prior, and (c) prior feature guidance.}
    %\jqxu{\textbf{TODO} which contains three parts: (a) compresses the physical prior; (b) produces the memory-enhanced prior feature; (c) generates the prior-guided scene feature.}}
    % (a) shows the prior memory enhancement (PME) module, which produces the memory-enhanced prior feature; (b) shows the prior feature guidance (PFG) module, which generates prior-guided scene feature.
    % See~\cref{sec:MPG} for details.
    \label{fig:mpg}
    \vspace{-3mm}
\end{figure}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\hsize]{./figs/fig_msr}
    \caption{The illustration of our Multi-range Scene radiance Recovery (MSR) module. (a) MSR aligns the features of neighboring frames into multiple sets with different ranges. (b) The space-time deformable attention (STDA) block aligns the features of different ranges to the target frame. (c) The guided multi-range aggregation (GMRA) block aggregates the aligned features from multiple sets.}
    \label{fig:msr}
    \vspace{-3mm}
\end{figure*}

\vspace{2mm}
\noindent
\textbf{Memory-enhanced prior.}
After obtaining the compressed prior token $\textbf{p}$, we formulate a prior token memory by saving multiple prior tokens at different time slots.
%
Then, we obtain the feature vectors $K$ and $V$ with the dimension of $ \mathbb{R}^{ND \times C}$, where $N$ denotes the number of prior tokens.
%
Hence, we are able to record the historical haze information in video sequences.
%
To perform the interaction between the current haze information and the history information encoded in prior token memory, we adopt the attention operation to read the memory information:
\begin{equation}
	\textrm{Attention}(Q, K, V) = \textrm{softmax}(\frac{QK^T}{\sqrt{c}})V \ ,
\end{equation}
where the query $Q$ is obtained by flatting the initial prior $\tilde{\mathcal{P}}$, and $c$ is the normalization factor, which is the dimension of $Q$ and $K$.
By doing so, we obtain the final memory-enhanced prior $\mathcal{P} = \textrm{Attention}(Q, K, V)$.
 %
%PME records the historically relevant haze information in a video and enhances the prior representation of the current frame using a feature memory (\cref{fig:mhg}(a)).
% 
% Given the initial prior feature $\mathcal{P}$ and its transmission distribution $\mathcal{D}$,
%First, we encode the high-resolution prior feature map $\tilde{\mathcal{P}} \in \mathbb{R}^{H \times W \times C}$ into compact prior tokens $\textbf{p} \in \mathbb{R}^{D \times C}$ for efficient memory reading, which are the keys and values in the prior token memory.
% as $k \ll H \times W$
% 
%Specifically, the prior token $\textbf{p}_d = \frac{1}{HW} \sum_{\forall x} \tilde{\mathcal{P}}(x) \cdot \mathcal{D}_d(x)$ represents the embedding of a specific transmission category $d$, computed as the aggregation of features weighted by their category probabilities.
% 
% \begin{equation}
%     \textbf{p}_k = \frac{1}{HW} \sum_{\forall x} \mathcal{D}_k(x) \cdot \mathcal{P}(x); k = 1, ..., K.
% \end{equation}
% 
% 
% To this end, a high-dimensional prior feature map \mathbb{R}^{H \times W \times C}$ is encoded into representative prior tokens $\textbf{p} \in \mathbb{R}^{K \times C}$, which serve as keys and values in the prior token memory, with much less storage, especially when the input is high-resolution.
% 

%Then, Attention~\cite{vaswani2017attention} is used in the memory readout:
% 
\if 0
\begin{align}
    \label{eq:correlation}
    \mathcal{W} = \textrm{softmax}(\frac{qk^T}{\sqrt{c}}), \\
    \textrm{Attention}(q, k, v) = \mathcal{W}v,
\end{align}
% 
where $q,k,v,c$ are query, key, value and channel dimension, and the attention weights $\mathcal{W}$ measures the similarity between query and key.
% 
Subsequently, the memory-enhanced prior feature $\mathcal{P} = \textrm{Attention}(Q, K, V)$ is obtained,
% % 
% \begin{equation}
%     \mathcal{P}^m = \textrm{Attention}(Q, K, V),
% \end{equation}
% % 
where $Q \in \mathbb{R}^{HW \times C}$ is the query feature flattened from $\tilde{\mathcal{P}}$; $K, V \in \mathbb{R}^{ND \times C}$ are the key and value features stored in the physical prior token memory, and $N$ is the memory size.
% 
% In this regard, the temporally aggregated haze features are generated, which can be leveraged as haze priors.
\fi

% \subsubsection{Prior Feature Guidance}
% \label{sec:pfg}
% % 
% Prior feature guidance (PFG) integrates the memory-enhanced prior into the scene feature (\cref{fig:mhg}~(b)).
% % 
% We transform the prior feature map into a prior feature volume to effectively explore the 3D information for haze thickness reasoning embedded in transmission.
% % 
% In specific, the prior volume feature $\mathcal{P}^v \in \mathbb{R}^{H \times W \times D \times C}$ at location $x \in (H, W)$ is computed by the outer product~\cite{reading2021categorical} $\mathcal{P}^v(x) = \mathcal{D}(x) \otimes \mathcal{P}(x)$ of each vector pair in $\mathcal{D}(x) \in \mathbb{R}^{D \times 1}$ and $\mathcal{P}(x) \in \mathbb{R}^{C \times 1}$, $\mathcal{P}^v(x) \in \mathbb{R}^{D \times C}$.
% 
% \begin{equation}
%     \tilde{\mathcal{P}}^v(x) = \mathcal{D}(x) \otimes \mathcal{P}^m(x),
% \end{equation}
% 
% where $\tilde{\mathcal{P}}^v(x) \in \mathbb{R}^{K \times C}$ indicates the volume feature at $x$.
% 
% We obtain the final haze feature volume $\mathcal{H}^v \in \mathbb{R}^{H \times W \times K \times C}$ using a 3D convolution layer, which regularizes the haze features in 3D space.
% 
% Besides, a 3D convolution layer is performed to obtain the prior feature volume.

% The integration of prior guidance into the scene feature is achieved via cross-attention.
% % 
% The cross-attention weights are computed between the initial scene feature $\tilde{\mathcal{J}}(x) \in \mathbb{R}^{1 \times C}$ as the query and the prior volume feature $\mathcal{P}^v(x) \in \mathbb{R}^{D \times C}$ as the key along the transmission category axis $d$.
% % 
% Finally, the prior-guided scene feature $\mathcal{J}$ is acquired by aggregating the prior feature according to the attention weights, and a residual connection is adopted; see Fig~\ref{fig:mhg} (b).

\vspace{2mm}
\noindent
\textbf{Prior feature guidance.}
% 
The prior-guided scene feature $\mathcal{J}$ is obtained using several convolution layers, which take the concatenation of the memory-enhanced prior feature $\mathcal{P}$ and the initial scene feature $\tilde{\mathcal{J}}$ as input.
% 
Hence, the prior is integrated for scene recovery.

\subsection{Multi-Range Scene Radiance Recovery}
\label{sec:MSR}
% 
The Multi-range Scene radiance Recovery module (MSR) aims to capture space-time dependencies in multiple space-time ranges.
%
~\cref{fig:msr} shows the detailed structure of our MSR, which aligns the features of adjacent frames $\mathcal{J}_{[{i-1}, {i-2}, {i-3}, ...]}$ into multiple sets with different ranges, \ie, $\mathcal{J}_{\{[{i-1}], [{i-1},{i-2}], [{i-1},{i-2},{i-3}], ...\}}$, to explore the temporal haze clues in various time intervals.
%
As shown in~\cref{fig:msr}~(a), the concatenated features with different ranges are sent to the shared space-time deformable attention block (STDA), which warps the features to the target frame.
%
% Note that the parameters in STDA are shared across different ranges.
%
After that, we formulate a guided multi-range aggregation block (GMRA) to aggregate the aligned features from multiple sets with the guidance of prior features.

%Given the prior-guided feature with context-rich haze clues, MSR generates the feature for recovering the scene radiance, which captures space-time dependencies in multiple space-time ranges as shown in~\cref{fig:msr}.

% Neighboring frames share similar foreground and background contents with the current target frame, which provide critical scene radiance cues.
% However, the complementary information is limited in a local time range with a small temporal receptive field, \eg, from only the previous frame~\cite{chan2021basicvsr}.
% 
% Previous methods capture the temporal dependencies frame-to-frame in a local sliding window~\cite{zhang2021learning,wang2019edvr} or recurrently~\cite{chan2021basicvsr} with a relatively small temporal receptive field.
% 
%MSR aligns the features of neighboring frames $\mathcal{J}_{[{i-1}, {i-2}, {t-3}, ...]}$ into multiple sets with different ranges, \ie, $\mathcal{J}_{\{[{i-1}], [{i-1},{i-2}], [{i-1},{i-2},{i-3}], ...\}}$, to explore the temporal haze clues in various time intervals.
% 
%Then, we design a space-time deformable attention block to warp the features of different ranges to the target frame and aggregate the aligned features from multiple sets with the guidance of prior features.

\vspace{-1mm}
\subsubsection{Space-Time Deformable Attention}
\label{sec:stda}
% 
As shown in~\cref{fig:msr}~(b), the space-time deformable attention (STDA) block aligns the concatenated features of the adjacent frames $\mathcal{J}_r^n=\mathcal{J}_{[{i-1},...,{i-r}]} \in \mathbb{R}^{r \times H \times W \times C}$ for each range $r\in\{1,...,R\}$ towards the target frame feature $\mathcal{J}$.
The output of the STDA block is the range feature $\mathcal{J}_r \in \mathbb{R}^{H \times W \times C}$.
% \in \mathbb{R}^{H \times W \times C}
% 
%A 
Here, we further learn a space-time flow, 
which is used to capture the correspondence from the previous frames to the current frame.
% TODO
The input space-time flow of the current STDA block is $\tilde{\mathcal{O}}_{r \rightarrow i}$, which is gradually refined in this block to produce the output space-time flow $\mathcal{O}_{r \rightarrow i}$, following SPy-Net~\cite{ranjan2017optical}.
% produced from the previous STDA
% TODO
% The STDA blocks are used in multiple layers in the overall U-Net architecture. The input space-time flow of the first layer is initialized as zero, and it is trained in an end-to-end manner by using the flow loss; please see~\cref{sec:loss} for details.
% 
% Note that the space-time flow is initialized as \textit{zero} at the first STDA block, upsampled across the scale in the scene decoder, and is trained end-to-end with an extra flow loss detailed in~\cref{sec:loss}.

%is initialized from the STDA block of the previous layers in a U-Net architecture, 

%and warp the concatenated features to a feature map, which is initialized from the previous STDA block as input $\tilde{\mathcal{O}}_{r \rightarrow i}$ and adaptively refined as output $\mathcal{O}_{r \rightarrow i}$ in each STDA block. 
% \xwhu{This senence is hard to follow.}
% 
% Instead of using the frame-to-frame optical flow estimation with an extra pretrained flow estimator and additional computational overhead~\cite{chan2021basicvsr++}, we propose to
% STDA learns the correspondence offsets adaptively~\cite{dai2017deformable,xia2022vision} and warps the feature volume of consecutive frames in a space-time manner~\cite{kim2018spatio}.
% 

% \xwhu{Please give more explanations of the 3D space-time flow estimation. how to obtain this, what is the function} 
Specifically, given the concatenated features $\mathcal{J}_r^n$ and a normalized initial space-time flow $\tilde{\mathcal{O}}_{r \rightarrow i} \in [-1,1]^{H \times W \times 3}$, 
% represented by $(x,y,z)$ sampling points, where $(x,y)$ denotes the spatial sampling location and $z$ denotes the time counterpart, 
we first compute the initial aligned feature map $\tilde{\mathcal{J}}_r^a \in \mathbb{R}^{H \times W \times C}$ as follows:
% 
\begin{equation}
    \label{eq:sample}
    \tilde{\mathcal{J}}_r^a = \mathcal{S}(\mathcal{J}_r^n, \tilde{\mathcal{O}}_{r \rightarrow i}) \ ,
\end{equation}
% 
% \xwhu{What is this operation? A simple bilinear operation? How and Where} 
where $\mathcal{S}$ denotes the differentiable space-time sampling operation~\cite{kim2018spatio}.
%
Note that the third dimension of $\tilde{\mathcal{O}}_{r \rightarrow i} \in [-1,1]^{H \times W \times 3}$ is three, which means the space-time flow capture locations on both spatial domain and time slot.
% 
Then, we obtain the refined space-time flow $\mathcal{O}_{r \rightarrow i}$ by computing the flow offset residual:
% \xwhu{What is this residual, there are no defination of the residual before}:
% 
\begin{equation}
    \mathcal{O}_{r \rightarrow i} = \mathcal{F}_o([\mathcal{J}, \tilde{\mathcal{J}}_r^a, \tilde{\mathcal{O}}_{r \rightarrow i}]) + \tilde{\mathcal{O}}_{r \rightarrow i} \ ,
\end{equation}
where $\mathcal{F}_o$ is a lightweight offset network composed of convolution layers.
% , Layernorm (LN), GLEU activation, and a linear layer
% \xwhu{How to train this flow network? trained by others?} 
% sampling is Differentiable, also a flow loss for extra guidance.
% 
Finally, we obtain the aligned feature $\mathcal{J}_r^a$ using \cref{eq:sample} with $\mathcal{O}_{r \rightarrow i}$ as input instead.

Cross-attention~\cite{vaswani2017attention} is used to extract the temporal information from the aligned feature.
% 
The feature $\mathcal{J}$ is used as the query $Q = \mathcal{J} U_q$ for the target frame, and the aligned feature $\mathcal{J}_r^a$ is used as the key and value $[K, V] = \mathcal{J}_r^a U_{kv}$, where $U_q \in \mathbb{R}^{C \times C}, U_{kv} \in \mathbb{R}^{C \times 2C}$ are learnable projection matrices.
% 
Finally, the range feature $\mathcal{J}_r$ is computed as:
% 
\begin{equation}
    \mathcal{J}_r = \textrm{W-MSA} (Q, K, V) \ ,
\end{equation}
% 
where the window multi-head self attention (W-MSA)~\cite{liu2021swin} is leveraged for efficient computation; see~\cref{fig:msr}~(b).
%
Note that we further adopt the feed-forward network (FFN) to process $\mathcal{J}_r$  after the W-MSA, following~\cite{liu2021swin}. 
% 
% After that, an MLP block with depth-wise convolutions and GELU activations, layer norm, and residual connections are adopted to generate the final output; see~\cref{fig:msr}~(b).

\input{table_sotas.tex}

\vspace{-1mm}
\subsubsection{Guided Multi-Range Aggregation}
% 
The aligned features of different ranges contain their specific space-time haze clues, and GMRA aggregates multi-range features under the guidance of prior features.
% The aligned features of different ranges contain their specific temporal haze clues, and GMRA aggregates multi-range features with different space-time information under the guidance of prior features.
% with different space-time information 
% 
%The guided multi-range aggregation (GMRA) module fuses extracted information from multiple ranges with different space-time resolutions under the guidance of prior features.
%
\cref{fig:msr}~(c) shows the detailed structure, where we compute the aggregation weights from two perspectives, \ie, scene radiance and physical prior.
% 
First, the concatenated range features $\{\mathcal{J}_r\}_{r=1}^R$ are considered as the key and value, which are multiplied by the target frame query feature~$\mathcal{J}$. %, along the $r$ dimension.
% 
For each location, attention weights are computed along the range~($r$) dimension.
% For each location, attention weights from the scene perspective are computed along the range~($r$) dimension.
%
Then, the prior guidance is leveraged by computing its derived attention weights in the same way.
% in the multi-range aggregation 
In specific, we consider the prior feature $\mathcal{P}$ as query and obtain the aligned prior features $\{\mathcal{P}_r\}_{r=1}^R$ as the attention key using Eq.~(\ref{eq:sample}) by taking the prior features $\mathcal{P}_r^n=\mathcal{P}_{[{i-1},...,{i-r}]}$ and the refined space-time flow $\mathcal{O}_{r \rightarrow i}$ as the inputs.
% 
% Then, the attention weights from the prior perspective are computed accordingly.
% , which considers the prior feature $\mathcal{P}$ as query and $\{\mathcal{P}_r\}_{r=1}^R$ as key
%\jqxu{omit query and key for attention weights Here.}
%The final attention weight is the summation of the weights generated from scene and prior, which is used to compute the aggregated multi-range values. %; see~\cref{fig:msr}~(c) for .
The final attention weight is the summation of the weights generated from the scene and prior aspects, followed by a Softmax normalization function for normalization.
% 
Finally, multi-range values are aggregated by performing the final attention weight on the range features, which is further summed along the $r$ (range) dimension; see~\cref{fig:msr}~(c). 
% $\alpha$ is a learned modulation factor
% 
% An additional fusion function $\mathcal{F}_f$ is adopted for spatial refinement, which contains several convolution layers.
% and takes the concatenation of the query and aggregated values as input.

\subsection{Loss Functions}
\label{sec:loss}
%
The overall loss $\mathcal{L}$ is the summation of an output loss $\mathcal{L}_{out}$, a physical model disentanglement loss $\mathcal{L}_{phy}$, and a flow loss $\mathcal{L}_{flow}$:
% 
\begin{equation}
\label{eq:loss}
% \begin{aligned}
    \mathcal{L} = \mathcal{L}_{out}
    + {\lambda}_{phy} \mathcal{L}_{phy} + {\lambda}_{flow} \mathcal{L}_{flow},
% \end{aligned}
\end{equation}
% \sum_{s=0}^3 {2^{-s} \mathcal{L}_1(\hat{I}^s, I^s)} \\
% {\lambda}_{flow} \sum_{s=0}^3 \sum_{r=1}^R {2^{-s} \mathcal{L}_1(\hat{J}_r^s, J^s)}
% $s=0,1,2,3$ are four feature/output scales in the decoder, 
where ${\lambda}_{rec}, {\lambda}_{flow}$ are the weighting hyper-parameters.
% to balance the loss weights

The output loss $\mathcal{L}_{out}=\mathcal{L}_1(\hat{J}, J)$ supervises the final dehazed results $\hat{J}$ with the ground truth $J$.  
%
The physical model disentanglement loss $\mathcal{L}_{phy}=\sum_{s=0}^3 {2^{s-3} \mathcal{L}_1(\hat{I}_s, I_s)} + \mathcal{L}_1(\hat{J}_s, J_s)$ is to make the prior decoder and scene decoder learn the physical model-based components at each scale $s$ in the U-Net, by predicting $\hat{t}_s, \hat{A}_s$, and $\hat{J}_s$, and reconstructing input $\hat{I}_s$ using Eq.~\eqref{eq:physical model}.
% 
Moreover, to make the STDA attend to informative regions, we use an unsupervised flow loss to regularize the learned space-time flow in~\cref{sec:stda}.
% 
Specifically, the flow loss $\mathcal{L}_{flow}=\sum_{s=0}^3 \sum_{r=1}^R {2^{s-3} \mathcal{L}_1(\hat{J}_{sr}^a, J_s)}$ computes the difference between the warped image $\hat{J}_{sr}^a$ and the reference ground truth frame $J_s$ with the scale $s$ for each range $r$.
% 
The warped image is obtained by~\cref{eq:sample} with the adjacent ground truth frames and the learned space-time flow as the inputs.
% 
% $J_r^n=J_{[{i-1},...,{i-r}]} \in \mathbb{R}^{r \times H \times W \times 3}$
% $\mathcal{O}_{r \rightarrow i}^s$
