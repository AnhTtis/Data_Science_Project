

\let\thefootnote\relax\footnotetext{\textsuperscript{*}Work done during an internship at CVC.}
\let\thefootnote\relax\footnotetext{\textsuperscript{$\dagger$}Equal contribution.}

Handwritten Text Recognition (HTR) has attracted the interest of the computer vision and pattern recognition research community for decades \cite{chen2021text, bluche2017scan, souibgui2021few, kang2019convolve}. Recent HTR approaches tend to use data-hungry supervised deep learning models, which require a vast amount of annotated data, especially with the recent transformer-based methods \cite{kang2022pay,li2021trocr}. To overcome the data scarcity problem, Self-Supervised Learning (SSL) techniques are nowadays being widely used  for computer vision tasks \cite{chen2020simple,chen2021exploring,doersch2015unsupervised,grill2020bootstrap,he2022masked,he2020momentum}), and more precisely for text recognition \cite{aberdam2021sequence,souibgui2022text}. %Several works \cite{li2022dit,li2021selfdoc} have emerged from this problem, and succeeded in establishing information extraction systems, capable of capturing information, and recognizing complex patterns while requiring few amounts of labeled data. 

\begin{figure}[t]
    \centering
    \includegraphics[width = \linewidth]{images/final_treaser.pdf}
    \caption{The model continually learns representation from sequentially appearing unlabeled data. The learned representation can be used to fine-tune HTR models on all the previously learned scripts/languages.}
    \label{fig:teaser}
\end{figure}

A common assumption in the recent work on self-supervised learning is that all the training data is available at the same time during the training process. However, in many real-world scenarios, new tasks and data continuously emerge over time. Thus, the model should be able to progressively and continuously learn new tasks without forgetting the previous ones \cite{li2017learning}, and also, whenever new unlabeled data is available. In fact, and in order to handle new data, the model must be retrained on the full dataset, i.e. previous and current data, to prevent the catastrophic forgetting phenomenon \cite{mccloskey1989catastrophic} in shifted data distributions. But, retraining the model with the full dataset is impractical, costly, and even impossible when previous data is not available anymore. Indeed, in some realistic scenarios, where privacy and security are fundamental, the model is not allowed to store the full previous data. More explicitly, a time frame of data may disappear due to storage constraints or privacy issues, which requires a dynamic training process to begin upon receiving the new data.
This issue encourages researchers to develop deep learning models that are able to adapt quickly and re-learn over time. Inspired by human cognition and its capability to learn incrementally and sequentially, Continual learning (CL) investigates the ability of neural networks to continuously learn tasks in a sequential manner, either on supervised \cite{parisi2019continual,pfulb2019comprehensive}, or unsupervised fashion \cite{fernando2017pathnet,li2019learn,mallya2018packnet,rusu2016progressive}, without the catastrophic forgetting problem. One characteristic of such systems is that old ideas may be re-examined although there is no need to hold them in mind \cite{parisi2019continual}. 


Recent works \cite{gomez2022continually,cha2021co2l, fini2022self} however, focus on alleviating the problem of catastrophic forgetting in a self-supervised fashion. The aim is to learn from a continuous data stream that follows a shifted distribution of visual representations. This is useful when dealing with unsupervised sequential representations, as we are no longer bound by the cost of manual labeling data, which is particularly hard to obtain. %when a new distribution of data is introduced to the model. 
This practice is called Continual Self-Supervised Leaning (CSSL). For handwritten text recognition,
we consider continuously learning new scripts or languages, as illustrated in Figure.~\ref{fig:teaser}.
% \textcolor{blue}{In this paper, we propose to adopt continual representation learning in documents images.
Our self-supervised continual learning decouples into two stages: (1) A continual learning strategy of an encoder-decoder in a self-supervised fashion. (2) A fine-tuning stage on top of the encoder for the multi-script handwriting recognition tasks (downstream tasks). 
% To the best of our knowledge this is the first time that continual learning representation is performed in the handwritten text recognition domain. 
We state that using a representation learning stream in the continual learning paradigm is particularly advantageous for practical use cases. Since the encoder learning stage carries the majority of the computation complexity, earlier tasks can indeed be trained via the shallow output layer. Therefore, this gives the additional flexibility to add an indefinite number of classes by fine-tuning the SSL pre-trained  backbone for several downstream tasks, which is useful in practical real word scenarios. Furthermore, training an encoder with unlabeled data is beneficial for generalization performance when dealing with data scarcity, as our experiments demonstrate.
%The learning is done in two stages: SSL pre-training with unlabeled data and supervised fine-tuning with the annotated data. The goal is to learn useful representations for HTR from the unlabeled data for optimal fine-tuning, as well as not forgetting the previously learned  knowledge  when pre-training on a new task. 
For unlabeled text images, \cite{souibgui2022text} showed that a masking/recovery strategy  is more suitable for SSL than other related approaches.  Thus, this work is based on Masked Auto-Encoders (MAE) \cite{he2022masked} for pre-training. Specifically, MAE discards low-level information by masking a large portion of the image patches and enables the encoder to extract semantic information by reconstructing the pixels from a very small number of neighboring patches \cite{cao2022understand} with a lightweight decoder. 
However, the MAE original design does not take continual learning into consideration and thus can not generalize well both in the previous and current tasks. 

With the aim to incorporate representation learning within a continual learning paradigm, we retrain our model on a new language representation by utilizing knowledge from an existing representation model. Our model can retain performance on past tasks while transferring previous knowledge to be more computationally efficient when training the current task. For this purpose, we add language/script adapters \cite{houlsby2019parameter} as new parameters for learning new languages/scripts. The power of our method is that it can prevent forgetting completely and speed up the training of a new task by reusing model components of a past task.

Other continual strategies involve explicit ways to combat catastrophic forgetting. Some of them proposed the use of a buffer memory \cite{acharya2020rodeo} with compressed representations of samples of past tasks. These representations are then presented to the model while the model is trained with the current task data. In this manner, the number of images evolves by augmenting the number of tasks over time. In this work, we alternatively propose a more challenging strategy when saving images from the previous tasks. Since we use a masking/recovering pre-training, we do not need to save full images. In fact, only a few patches of the images are required for the memory replay, since our model can reconstruct the full images from only the patches. This is a clear benefit in real scenarios where the storage memory is very limited and also, where data privacy and security are crucial. Moreover, by using a language/script adapter combined with the memory replay strategy, no prior information is required at inference time, which makes our model a task-agnostic continual learning model.
% Using MAE pre-training is not only beneficial for learning robust representations, but it also allow us to save fewer ..
%mae benefits - in ssl and in training
% which is surprisingly non-studied in the pattern recognition field and, to the best of our knowledge, this is the first work that exposes the CSSL in document images. 
%In this context, authors of \cite{gomez2022continually} presented a regularisation-based technique with a distillation-based objective. They learn to project the current visual representation to the previous model representation by saving 
Overall, the main contributions of this work are the following: 
\begin{itemize}
    \item To the best of our knowledge, our work is the first application of continual learning to the field of handwritten text recognition, showing its ability to continuously learn from new scripts or languages. 
    \item An efficient continual self-supervised learning framework in terms of complexity and storing memory is proposed to address the issue of catastrophic forgetting with no need for prior knowledge at inference time. We show that adopting a scalable continual representation learning for HTR is beneficial in real word scenarios, where the model is mainly trained on unlabeled data, and only a few labeled data are made available for fine-tuning.
    % \item  A continual fine tuning is applied to generate a multi-script HTR model.
    \item Extensive experiments show the effectiveness of our approach compared to the state-of-the-art methods on continual learning in the context of multi-script/multi-lingual handwritten text recognition.
\end{itemize}


% First, we address self-supervised learning for document images and propose the use of Masked Auto-Encoders (MAE) \cite{he2022masked} for the pre-training task. 



% By replaying few patches, our model does not require storing all the samples from the previous task. This is a practical scenario in many cases where previous data is no longer available and also, where data privacy and security are crucial. 


% Motivation: 
    % Motivation for CL: CF, privacy, costly training , humans naturels learning is continual, 
    % Motivation for SSL: current Transformer requires a lot of data, most of data  is unalabled --> SSL

    % it will be intersting to build a model for both 

% solutions:
  %Alex's paper, Contrastive Learning , why does not work


% what we propose
    % better result when using masking for text recognition,  memory replay with patches requires less space 
  %  Contributions: 
           % \item
           % \item
           % \item
           % \item

