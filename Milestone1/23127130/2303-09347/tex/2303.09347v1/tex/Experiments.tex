 



In this section we evaluate our method's performance and compare it with some baselines and state-of-the-art continual learning approaches. In addition, we compare the obtained HTR accuracy when performing the fine-tuning of multi-scripts/languages. 



\begin{table*}[]
    \centering
    % \resizebox{\textwidth}{!}{
    \begin{tabular}{l|c|c|c|c|c|c|c}
     \hline
     \multirow{2}{*}{Method} & \multicolumn{2}{c|}{FT after Task 2} & \multicolumn{3}{c|}{FT after Task 3} &  \multirow{2}{*}{\# Tr. parms.} & \multirow{2}{*}{\# Samples $\uparrow$}  \\
     \cline{2-6}
     & English $\downarrow$&Italian $\downarrow$&English $\downarrow$&Italian $\downarrow$&Russian  $\downarrow$& &   \\
     \hline
     \hline
     Multilingual & 12.3 & 17.0   & 12.3 & 17.0 & {4.7}  &  68 M& --  \\  
     Monolingual  & 8.0     & 6.0    &  8.0  & 6.0  & 1.4  &  68 M& --  \\
     
     \hline
        
     Adapter& 10.6 & 5.7  & 6.1  & 5.4  & \textbf{2.8} & 10.2 M & --\\
     Distillation & 9.4      & 9.1   & 8.1  & 9.5  & 6.6  &  68 M & 3200  \\
     ER \cite{rolnick2019experience}& 19.1      & 5.8     & 14.0 & 22.0  & 5.9  & 68 M & 1920  \\
     EWC \cite{kirkpatrick2017overcoming} &  27.0     &  8.6    &  25.9  & 10.0 & 5.7 & 68 M & --  \\
     \hline
     \textbf{CSSL-MHTR}   & \textbf{5.7}  & \textbf{5.1} & \textbf{4.9}  & \textbf{5.1}  & 2.9  &  \textbf{10.2} M&  \textbf{3200}   \\
     \hline
    \end{tabular}
    % }
    \caption{Continual representation learning: evaluation of continually pre-trained models on three tasks (English followed by Italian, followed by Russian). Then, they are fine-tuned (FT) separately for recognizing English, Italian, and Russian, which correspond to the downstream HTR tasks, measured using the Character Error Rate (the lower, the better). Fine-tuning is done after learning the second task (FT after task 2) or the third task (FT after task 3). \# Tr. parms. is the number of trainable parameters in million. \# Samples indicated the possible stored samples using a fixed-size memory buffer for the memory-replay-based methods.}
    \label{tab:CL_results}
\end{table*}


\subsection{Datasets, Tasks and Metrics}

We assess the performance of our CSSL framework by sequentially learning three tasks without forgetting. Each task consists in learning the representation of a different language/script. For this, we use specific HTR datasets, as illustrated in Figure \ref{fig:datasets}. The tasks are the following:

\noindent\textbf{English}: The first task consists in recognizing English. For this task, the IAM dataset \cite{marti2002iam} was used. 

\noindent\textbf{Italian}: The second task consists in learning another language, concretely Italian language. Here we maintain the same script as English, but we have a different domain (i.e. lexicon, language model, probability distribution). For this task, the LAM dataset \cite{cascianelli2022lam} was used. 

\noindent\textbf{Russian}: The third task consists in learning a different language and script, concretely, the Russian language. This task is the most challenging because it must not only learn a new language, but also a new alphabet (a different set of character classes). The HKR \cite{nurseitov2021handwritten} was used for this task. 

For the evaluation, we use the Character Error Rate (CER) between the produced text output and the ground truth. More details on the datasets (splits and size) and the CER metric can be found in the supplementary material.


% datasets: IAM dataset\cite{marti2002iam}, LAM dataset \cite{cascianelli2022lam}, and HKR \cite{nurseitov2021handwritten}, and we use it as a backbone for pre-training tasks as follows: 
% \begin{itemize}
% \item The first task is pre-training on the English dataset.
% \item The second task consists of continuing the pre-training on the Italian dataset.
% \item and the third task 
% \end{itemize}

\begin{figure}[t]
    \centering
    \begin{tabular}{c}
        \includegraphics[width = 0.9\linewidth]{images/english.png}\\
        \includegraphics[width = 0.9\linewidth,height=0.5cm]{images/italian.jpg}\\
        \includegraphics[width = 0.9\linewidth,height=0.5cm]{images/hkr.jpg}\\
    \end{tabular}
    \caption{Three examples of the images from the different languages/scripts datasets that were used for the experiments. Up: IAM, middle: LAM and Down: HKR. It is noticeable the domain difference between the different tasks.}
    \label{fig:datasets}
\end{figure}


% \textcolor{red}{TO ADD !!!!!}

% \textcolor{red}{Task 1 is reading english. The dataset used is XXX...}

% \textcolor{red}{Task 2 is reading italian. The dataset used is XXX...}

% \textcolor{red}{Task 3 is reading russian. DIFFERENT SCRIPT!! The dataset is HKR...}

% Don't forget to mention the metric: CER.

% \textcolor{red}{!!!!!!!!!!}


\subsection{Continual Learning representations}

\subsubsection{Baselines}
The majority of CL approaches require the use of labels, which makes them inappropriate for CSSL. However, some approaches can be evaluated within our context by slightly modifying them. In order to demonstrate the effectiveness of our proposed framework, we compare it to four  continual learning methods consisting of the following: (i) Adapter: language/script adapters only are
inserted after MHSA and feedforward module in all layers of the transformer architecture. (ii) Distillation: a representation objective is optimized with a knowledge distillation loss as described in section \ref{subsection:distill}; in this method, we store only some patches of the images (iii) Experience Replay (ER) \cite{rolnick2019experience}: rehearsal-based replay method benefits from storing data (full images), which contains a lot of information about previous tasks and enables the retraining on old evaluationclasses. (iv) Elastic Weight Consolidation (EWC): We use the regularization method from \cite{kirkpatrick2017overcoming} to estimate the diagonal of the Fisher information matrix that is used to count the parameter's importance after every learned task. Details on baseline selection, implementation and tuning for CSSL can be found in the supplementary material. 

For the comparisons, we pre-train the model across languages in a multi-script fashion and then fine-tune on the respective scripts (Multilingual). Also, we perform a monolingual pre-training, and we fine-tune on separate scripts.  


\subsubsection{Comparison with Continual Learning Methods}

We begin by evaluating the quality of the learned representation within our continual learning scenario. This is reported in  Table \ref{tab:CL_results}, where we compare our method with the baselines and continual learning widely used methods presented above. The experiment consists in performing self-supervised pre-training to learn representations from different languages/scripts in a sequential manner. We begin by pre-training on English (task 1), then we continually pre-train on Italian (task 2), and then Russian (HKR) (task 3). The goal is, of course, to not forget the representation of the previously learned tasks while learning the current task. We evaluate at which extend the model prevents catastrophic forgetting by taking the encoder of the current task and fine-tuning it on the current and previously learned  tasks. The best model should provide the best results in terms of Character Error Rate (CER) on the current as well as the previously learned languages/scripts. Moreover, we evaluate the number of trainable parameters in each model to highlight the most efficient model in terms of training time. Also, we show the number of possible stored samples in the methods that use a memory buffer by fixing its size. 

\begin{figure}[t]
    \begin{center}
    \includegraphics[width =0.9 \linewidth,height=2.5cm]
    {images/plot_33.pdf}
    \end{center}
    \caption{The obtained CER of the English and Italian language using the fine-tuned models after continually pre-training on the second (Italian) and third (Russian) tasks.}
    \label{fig:plot}
\end{figure}


From Table \ref{tab:CL_results}, we notice that the replay-based approach (ER) does not help in the prevention of forgetting in CSSL. We have identified two prime reasons for this failure. First, in a supervised CL, replay-based methods benefit from the storage of labels, which contain some useful information about previous tasks and allow the model to be retrained on old classes. This is not the case in CSSL, because labels are not available.
Secondly, SSL models need more training epochs to converge, which means that the samples in the buffer are also replayed many more times. This leads to significant over-fitting on these examples and defeats consequently the purpose of the replay buffer. We also observe that ER stores fewer samples than our method because it requires storing the full images, while our approach only stores a percentage of patches from the full image. This explains why the results shown in the Distillation row are better than ER, since it stores patches instead of full images. 
% This explains the result of the row Distillation, where a better result than ER is obtained for a replay-based method that stores patches instead of full images

From the Table, we also observe that the EWC method yields unstable learning representation and does not prevent forgetting when fine-tuning the model. This means that the importance of the weights could not be accurately calculated with the Self-supervised task, i.e. the image recovery. Next, we can see that the performance of the Adapter is better than the other approaches. With the adapter, maximum prevention of forgetting is maintained since the rest of the model components are frozen.
This also brings efficient training since the number of trainable parameters becomes significantly lower than in the other approaches. However, the adaptation of the current adapter to the previous task is insufficient, which results in unsatisfactory performance. 

Moreover, we compared the results with  monolingual and multilingual pre-training methods. Although these are not continual learning methods, they can serve as references to evaluate the different models. In the monolingual method, the model is pre-trained and fine-tuned on only one script/language. This should serve as an upper bound because the model only focuses on one task. In the multilingual pre-training model, all the datasets from the three different tasks are used to train the model simultaneously. From the results, we observe that the monolingual pre-train obtains good results, while the multilingual model achieves poor results. These poor results are caused by the domain gap between the different languages and the class gap between the different scripts.  
 



Finally, it must be noted that our CSSL-MHTR model obtains the best results. Our framework demonstrates efficient forward transfer to downstream tasks, proving that its features are easier to generalize to other scripts or languages. 
This fact is demonstrated by achieving the best CER on both languages (English and Italian) after learning the second task and by achieving the best CER in English and Italian after learning the third task (Russian) as well. Furthermore, our model can prevent the catastrophic forgetting regarding the fine-tuning stage in all the script/languages. This is depicted in Figure \ref{fig:plot}. As it can be seen, our model is stable in the CER of the different languages after learning the three tasks. Here we observe that the CER of English and Italian is not only the best among all the compared approaches, but it also improves after learning more tasks (e.g. the CER for English is 5.7\% after learning Italian, but it decreases to 4.9\% after learning Russian). This can be explained by the assumption that when learning more scripts/languages in a self-supervised way, the representations are getting richer and more robust because of the newly learned domains (writing styles, fonts, backgrounds, etc.). From the Figure, we notice that this gain in representation quality in both languages is only available when using the adapter-based approaches, which confirms its utility.
% Either  retraining on two scripts or three scripts, the model can learn a more stable representation of a new script without forgetting the original one. 
In fact, the use of language adapters makes our model efficient in avoiding the forgetting problem, and also, it speeds up the training phase because it has fewer trainable parameters. Moreover, by storing only some patches of the previous tasks' images (the full image is not needed) we can have a larger number of stored samples compared to the ER method. This leads to better performance when applying knowledge distillation with our approach to adapt the current model to the previous tasks. In addition, this allows to reuse data from datasets that are not available anymore or have privacy constraints.

To summarize, with CSSL-MHTR, we can easily transfer knowledge from the original script (in our experiments, English), to learn another script/language in a sequential and continuous fashion. Also, we decrease the time and storage memory that is required to train a new script/language in a continual learning fashion. The reason is the use of a simple language adapter with frozen parameters to learn new tasks efficiently, as well as the use of knowledge distillation to adapt the current model to the previous tasks (preventing the forgetting issue). Also, the use of the masking/recovering pre-training strategy enables the model to store only a few patches instead of the full images that belongs to the previous tasks. This confirms our design choice for the addressed problem within this paper.


\subsection{Comparison with  HTR Models}

After demonstrating the suitability of our proposed approach for learning the different languages/scripts without forgetting, now we compare our model with state-of-the-art handwritten text recognition systems on the three scripts/languages. The results obtained by the models included in the analysis on the basic split are reported in Table \ref{tab:comp}. In the first column, we consider models following different kinds of architectures, including Convolutional-Recurrent Neural Network Models \cite{puigcerver2017multidimensional,cojocaru2021watch}, Transformer Networks such as \cite{kang2022pay,li2021trocr} and Fully-Convolutional Networks \cite{abdallah2020attention,bluche2017gated,coquenet2020recurrence,yousef2020origaminet}. Our CSSL-MHTR holds up on all datasets after pre-training on other scripts and obtains almost the same performance when compared to state-of-the-art approaches that use one script for monolingual training. Nevertheless, we observe a significant margin between different scripts, due to the writing style, number of characters per line and their average width, the variation between Latin and non-Latin scripts, the background of the image, etc. Anyway, our CSSL-MHTR exhibits more robustness and generalization ability when compared to other systems that were tuned to obtain the optimal performance on a particular script/language. 

\begin{table}[t]
\centering
% \resizebox{\columnwidth}{!}{
\begin{tabular}{l|c|c|c}
\hline
System                                       & English  & Italian  & Russian \\ \hline\hline
1D-LSTM\cite{puigcerver2017multidimensional} & 8.3  & 3.7  & 69.1 \\ 
CRNN \cite{cojocaru2021watch}                & 6.8  & 3.3  & -   \\ \hline
Transformer \cite{kang2022pay}               & 4.6 & 10.2 &     - \\ 
TrOCR \cite{li2021trocr}                     & 3.4  & 3.6  & -   \\ \hline
GFCN \cite{coquenet2020recurrence}           & 8.0  & 5.2  & -    \\ 
OrigamiNet \cite{yousef2020origaminet}       & 4.8  & \textbf{3.1}  & -    \\ 
Bluche \cite{bluche2017gated}                & \textbf{3.2}  &      & 22.3 \\ 
G-CNN-BGRU \cite{abdallah2020attention}  & -    & -    & 8.3 \\ \hline
\textbf{CSSL-MHTR}                                         & {4.9}  & {5.1}  & \textbf{2.9} \\ \hline
\end{tabular}
% }
\caption{Comparison with state-of-the-art approaches for HTR on multi-scripts/languages.}
\label{tab:comp}
\end{table}

%%%%%%Ã¹ nbre param adap++ vs transfer learning /storing memory: 3% in terms of ;; gega memmory vs 


\subsection{Qualitative Evaluation}





\begin{figure}[t]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ll}
         Input:& \includegraphics[width=0.85\linewidth, height=0.5cm]{images/img_949.png} \\
         GT: & was over. " Again her laughter trilled. " Marriage \\
         Monolingual: &  was over  " A\textcolor{red}{fe}in \textcolor{red}{l}er la\textcolor{red}{y}ghte\textcolor{red}{d} t\textcolor{red}{i}i\textcolor{red}{b}led. " M\textcolor{red}{oxoy}age\\
         CSSL-MHTR (t=2): & was over. " Again her laughter t\textcolor{red}{i}illed. " Marriage\\
         CSSL-MHTR (t=3): & was over. " Again her laughter tri\textcolor{red}{b}led. " Marriage \\ \hline

         % Input: &  \includegraphics[width=0.85\linewidth, height=0.5cm]{images/img_800.png} \\
         % GT: & " Yeah, " said Dowd dubiously. " 2Mebbe we 'd \\ 
         % Monolingual: &  " Yea\textcolor{red}{r}, " said Dowd d\textcolor{red}{i}biously. " \textcolor{red}{E}Meb\textcolor{red}{e}e we \textcolor{red}{"}d\\
         % CSSL-MHTR (t=2): &   " Yea\textcolor{red}{k}, " said Dowd dubiously\textcolor{red}{,} " \textcolor{red}{Er}ebbe we 'd \\
         % CSSL-MHTR (t=3): &  " Yea\textcolor{red}{k}, " said Dowd dubi\textcolor{red}{l}usly. " \textcolor{red}{B}Mebbe we 'd  \\ \hline
         Input: &  \includegraphics[width=0.85\linewidth, height=0.5cm]{images/img_951.png} \\
         GT: & " Isn't it ? " he rose and smoothed himself \\
         Monolingual: & " \textcolor{red}{t}s\textcolor{red}{a}'t it " " he rose and s\textcolor{red}{u}oothed himself \\
         CSSL-MHTR (t=2): & " Isn't it \textcolor{red}{'} " he rose and smoothed himself \\
         CSSL-MHTR (t=3): & " Isn't it ? " he rose and smoothed himself \\
    \end{tabular}}
    \caption{The output of the different models when recognizing handwritten English text images. The produced errors of each model are shown in red color.}
    \label{fig:qualitative}
\end{figure}


%%% nbre of tasks: 1 , 2 
%%



\begin{figure}[t]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ll}
         GT (English):& \includegraphics[width=0.85\linewidth, height=0.5cm]{images/reconstruct/eng_from_eng/gt.jpg} \\
         Masked (t=1):& \includegraphics[width=0.85\linewidth, height=0.5cm]{images/reconstruct/eng_from_eng/masked.jpg} \\
         Recov. (t=1):& \includegraphics[width=0.85\linewidth, height=0.5cm]{images/reconstruct/eng_from_eng/recovered.jpg} \\
         Masked (t=2):& \includegraphics[width=0.85\linewidth, height=0.5cm]{images/reconstruct/eng_from_it/masked.PNG} \\
         Recov. (t=2):& \includegraphics[width=0.85\linewidth, height=0.5cm]{images/reconstruct/eng_from_it/recovered.PNG} \\
          Masked (t=3):& \includegraphics[width=0.85\linewidth, height=0.5cm]{images/reconstruct/eng_from_ru/masked.PNG} \\
         Recov. (t=3):& \includegraphics[width=0.85\linewidth, height=0.5cm]{images/reconstruct/eng_from_ru/recovered.PNG} \\ \hline
        GT (Italian):& \includegraphics[width=0.85\linewidth, height=0.5cm]{images/reconstruct/it_from_it/gt.PNG} \\
         Masked (t=2):& \includegraphics[width=0.85\linewidth, height=0.5cm]{images/reconstruct/it_from_it/masked.PNG} \\
         Recov. (t=2):& \includegraphics[width=0.85\linewidth, height=0.5cm]{images/reconstruct/it_from_it/recovered.PNG} \\
         Masked (t=3):& \includegraphics[width=0.85\linewidth, height=0.5cm]{images/reconstruct/it_from_ru/masked.PNG} \\
         Recov. (t=3):& \includegraphics[width=0.85\linewidth, height=0.5cm]{images/reconstruct/it_from_ru/recovered.PNG} \\
    \end{tabular}}
    \caption{The recovery of masked images from English and Italian using their tasks models and the models of the next tasks.}
    \label{fig:qualitative_reco}
         

\end{figure}


For a further demonstration of the suitability of our CSSL-MHTR method for incrementally learning languages/scripts, in this subsection, we show some qualitative results. First, Figure \ref{fig:qualitative_reco} shows the reconstruction of the images for English and Italian. For each image, the model makes the masking/reconstruction after learning the current as well as the next tasks. We can notice the ability of the model at task $t$ in reconstructing an image from a previous task $t-n$ with the same performance as the model $t-n$.n Hence, it is clear that our method prevents forgetting. Next, we also show the HTR performance after fine-tuning. Here, we compare the result obtained by our model after learning the second and third tasks with the result of a monolingual pre-train model. As it can be seen in Figure \ref{fig:qualitative}, when learning more tasks, our models' HTR performance improves, with fewer errors. This is due to the robustness of the representation, which improves over tasks, since the model is trained to reconstruct images with different handwriting styles, backgrounds, scripts, etc. Not surprisingly, our model can be successfully pre-trained on all these different tasks, obtaining more robust representations. 



