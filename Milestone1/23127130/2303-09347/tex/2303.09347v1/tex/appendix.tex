% \documentclass[10pt,twocolumn,letterpaper]{article}

% \usepackage{iccv}
% \usepackage{times}
% \usepackage{epsfig}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{xspace} 
% \usepackage{multirow}
% % Include other packages here, before hyperref.

% % If you comment hyperref and then uncomment it, you should delete
% % egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% % run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% % \iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{12035} % *** Enter the ICCV Paper ID here
% \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% % Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi


% %%%%%%%%%%% NEW COMMANDS
% \newcommand{\KW}[1]{{\color{blue}{#1}}}

% %%%%%%%%%%%%%

% \begin{document}

% %%%%%%%%% TITLE
% \title{CSSL-MHTR: Continual Self-Supervised Learning for Scalable Multi-script Handwritten Text Recognition \\Supplementary Material}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

% \maketitle
% % Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi

\appendix

\section{Overview}\label{app:sec:overviwe}

CSSL-MHTR is a Continual SSL framework, aiming to continually learn representations from handwritten document images. The proposed model is then, fine-tuned on multi-script handwritten text recognition tasks. In this supplementary material, we discuss and highlight some essential insights of CSSL-MHTR, in terms of composition and configuration.


\section{Datasets and Metrics}\label{app:sec:datasets}
\subsection{Datasets}
The datasets used in the experiments are the following:
\begin{itemize}
    \item The IAM dataset \cite{marti2002iam}  contains forms of handwritten English
        text created by 657 writers. The training set comprises 747 documents (6,482 lines, 55,081 words), the validation set is composed of 116 documents (976 lines, 8,895 words) and the test set contains 336 documents (2,915 lines, 25,920 words). In addition to that, we used 40k synthetic lines images only for the first pre-trained model.   
    \item The Ludovico Antonio Muratori (LAM) dataset \cite{cascianelli2022lam} is a large line-level HTR dataset of Italian ancient manuscripts edited by a single author over 60 years. The basic split consists of  19,830 lines for training,  2,470 for validation, and 3,523 lines for testing. We used the same split in our experiments.
    \item The HKR dataset \cite{nurseitov2021handwritten} is a handwritten text dataset containing Russian Words (Areas, Cities, Villages, Settlements, Streets) written by a hundred different writers. It also incorporates the most popular words in the Republic of Kazakhstan. A few pre-processing and segmentation procedures have been developed together with the database. They contain free different handwriting forms. The database is prepared to provide training and testing sets for Kazakh and Russian word-recognition research. The database consists of more than 1400 filled forms. There are approximately 63,000 sentences, more than 715,699 symbols, and there are approximately 106,718 words. We used the same basic split for this dataset: Training (70\%), Validation (15\%), and Testing (15\%). 
 
\end{itemize}
\subsection{Evaluation Metric}

We used the standard Character Error Rate (CER) to evaluate the accuracy of text recognition. For a set of ground truths, the CER is computed using the Levenshtein distance which is the sum of the character substitutions ($Sc$), insertions ($Ic$), and deletions ($Dc$), divided by the total number of the ground truth characters ($Nc$). Formally, the CER is computed as:
\noindent
\begin{equation}
CER=\frac{Dc+Sc+Ic}{Nc}
\end{equation}





\section{Implementation Details}\label{app:sec:imp_details}

The implementation details of our CSSL-MHTR approach are summarized in Table \ref{tab:model_configs}. During the pre-training step, we deploy an encoder with 6 layers, 8 attention heads to encode the input, and 768 embedding dimensions. We used the same number of layers and attention heads for the decoder, with a dimension of 512 in the fine-tuning stage (text recognition). At masking, each input image with size 64$\times$800$\times$3 is divided into a set of patches with size 8$\times$8$\times$3. Then,  we employ random masking that grows progressively from 2.5\% to attend 40\% of the patches in the final pre-training epoch. To prevent the model from over-fitting, we used regularisation techniques such as data augmentation and early stopping. In the fine-tuning stage, we use the same encoder (pre-trained) with a different decoder of 6 layers, 8 attention heads, and an embedding dimension of 768.

 

\begin{figure*}[t]
    \centering
    \begin{tabular}{ll}
    GT (English):& \includegraphics[width = 0.5 \linewidth]{images/masking/mask2/mask_0.2/xac_3926,128gt.jpg} \\
    \\ \hline \\
    Masked (MaskRatio=0.025,epoch=1):& \includegraphics[width = 0.5 \linewidth]{images/masking/mask2/mask_0.025/xac_3926,128masked.jpg} \\            Pred (MaskRatio=0.025):& \includegraphics[width = 0.5 \linewidth]{images/masking/mask2/mask_0.025/xac_3926,128pred.jpg} \\        
    \\ \hline \\
    Masked (MaskRatio=0.15,epoch=50):& \includegraphics[width=0.5 \linewidth]{images/masking/mask2/mask_0.15/xac_3926,128masked.jpg}  \\          Predicted (MaskRatio=0.15):& \includegraphics[width=0.5 \linewidth]{images/masking/mask2/mask_0.15/xac_3926,128pred.jpg}
    \\   \\ \hline \\
    Masked (MaskRatio=0.25,epoch=90):& \includegraphics[width=0.5 \linewidth]{images/masking/mask2/mask_0.2/xac_3926,128masked.jpg}  \\ 
    Predicted (MaskRatio=0.25):& \includegraphics[width=0.5 \linewidth]{images/masking/mask2/mask_0.2/xac_3926,128pred.jpg}   
    \\   \\ \hline \\
    Masked (MaskRatio=0.4,epoch=150):& \includegraphics[width=0.5 \linewidth]{images/masking/mask2/mask_0.4/xac_3926,128masked.jpg} \\ 
    Predicted (MaskRatio=0.4):& \includegraphics[width=0.5 \linewidth]{images/masking/mask2/mask_0.4/xac_3926,128pred.jpg} 
    \\ \hline \\
    \end{tabular}
    \caption{The recovery of masked images from the English dataset with progressive mask ratio.}
    \label{fig:masking}

\end{figure*}




\begin{table}[t]
% \footnotesize
\begin{center}

\caption{\textbf{Implementation Details.} Implementation details of CSSL-MHTR during pre-training and fine-tuning.} 
\label{tab:model_configs}
\resizebox{\columnwidth}{!}{
\begin{tabular}{llll}
\hline\noalign{\smallskip}
ConFigure & Pre-Training & Fine-tuning  \\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
Optimizer & AdamW & Adam \\
Learning rate & 1.5 $e^{-4}$ & 5 $e^{-5}$ \\
Weight decay & 0.05 & 0.05 \\
Optimizer momentum & $\beta_1$, $\beta_2$=0.9, 0.95 & $\beta_1$, $\beta_2$=0.9, 0.95   \\
Batch size & 32   & 32  \\
Learning rate schedule & cosine decay & cosine decay  \\
Warmup epochs & 3 & 3  \\
%Training epochs & 150   & 600  \\
\hline
\end{tabular}}
\end{center}
\vspace{-0.5cm}
\end{table}


\subsection{Masking Strategy}
The masking strategy that was used in our model consists in starting with masking 2.5\% of the patches and incrementally increasing this masking ratio by adding 2.5\% every 10 epochs to attend 40\% as the maximum used masking ratio. This allows the model to start easily predicting the masked parts and then progressively increase the difficulty. We found that without this strategy, and by keeping a masking ratio of 40\% from the beginning, the model is unable to reconstruct the image during pre-training. In addition to that, in order to balance the patches classes, we force half of the masking to cover the text parts of the image and avoid the background pixels, while leaving the ability of the other half to select random patches (including background). In Figure \ref{fig:masking} we report an overview of the masking during different stages of pre-training.

\subsection{Memory Replay Setting}

In our approach, we used a memory replay strategy with a memory  $R_{1,2,3,...t-1}$ dedicated to storing images from the  previous datasets. When learning the task $t$, these images are presented to the model  to perform the memory replay. More precisely, at each iteration, we load a batch from the current task and a randomly chosen batch from the previous tasks to train the model. The previous task batch is used to compute the distillation loss. This is done by feeding the data batch to both models $t-1$ and $t$, then forcing the  model $t$ to behave similarly to the model $t-1$. The current task batch is fed only to the model $t$ and a reconstruction loss is computed and used to learn the current task.  

Our used memory buffer stores 3200 samples. When learning the second task, this number is fully dedicated to the first task. This means we have 3200 samples of the first task in our memory. When moving to the third task, the size of the memory buffer will be equally shared between the first and second tasks. This means the memory buffer will contain 1600 samples from the first task and 1600 from the second task. And so on when learning further tasks. 














 
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib,kai_bib}
% }


 
% \end{document}