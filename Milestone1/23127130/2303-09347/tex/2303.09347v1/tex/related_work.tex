
\subsection{Continual Learning}\label{subsec:CL-methods}



Continual learning~\cite{ring1997child,thrun1998lifelong} aims to continuously update the mastered knowledge based on prior information while preserving performance across all tasks. One of the major challenges is catastrophic forgetting~\cite{mccloskey1989catastrophic}, since relying solely on new data to update the model leads to the continuous erosion of knowledge from previous tasks. Recently, some regularization-based methods in continuous Sequence-to-Sequence models are mainly developed from the perspective of parameter update~\cite{li2022overcoming} and distillation loss~\cite{monaikul2021continual}. However, these methods usually suffer from the problem of plasticity, because they constrain the required updates of parameters specific to different tasks. Moreover, rehearsal-based methods~\cite{li2022continual, sun2019lamol} utilize a small subset of text data or intermediate features to effectively mitigate forgetting while allowing their adaptation to new tasks, with encouraging performance in various tasks. The fixed-size buffer limits the ability of the model to adequately capture the diversity of the previous tasks. Therefore, the rehearsal-based continual method faces the following two core challenges: 1) determining how to effectively select a critical subset of old data for replay from a large amount of available data, and 2) figuring out how to extract accurate and effective representations for the rehearsal data within the constraints of limited memory. 
% Therefore, addressing the problem of knowledge transfer in a continual manner is particularly important.

{Continual learning in the field of HTR can be seen as performing continual learning in sequential data.} 
To avoid the rehearsal-based strategies problems in sequence data, some approaches address the continual learning problem by reintroducing a modular approach where new parameters are added as language adapters for learning new languages \cite{houlsby2019parameter,kessler2022adapter}.  Close to our problem,  recent works \cite{lin2022continual,madaan2021rethinking,gomez2022continually,cha2021co2l, fini2022self} have attempted to address CSSL. {However, these approaches were designed for isolated classes (not sequences) using contrastive or barlow-twins losses. Contrary, \cite{souibgui2022text} showed that the use of a masking/recovery strategy is better for performing SSL on handwritten text.  
For this reason, in this work we propose a CSSL approach based on language adapters (which was shown to work better for sequential data) and knowledge distillation (with an efficient design that is compatible with the masking/recovery strategy to overcome the limited memory size by rehearsal-methods).}




%%%%%%%% we propose hybrid approach(buffer size) + adapter
%%%% adapter in regu methods

%To tackle this, regularization-based methods~\cite{Kirkpatrick3521,10.1007/978-3-319-46493-0_37} employ to constrain the variation in key neural weights. Based on the architecture-based methods~\cite{Jaehong2018,chen2015net2net}, the task-specific network architecture is dynamically added to the existing model, enabling it to accommodate the new task while maintaining its previous knowledge. In the rehearsal-based methods,~\cite{Rebuffi_2017_CVPR} stores a fixed-size buffer of samples from previous tasks and~\cite{van2018generative} synthesizes examples from previous tasks to mitigate forgetting by revisiting old information. 










 %%%%%%%%%%%%%%%%%Multiscripts HTR (genreal, ssl en htr , multscripts: domain adaptation, (incov: refaire l'app sur le total de base ---> CL the solu because is scalable 
 % HTR, SSL in htr, domain adaptation, 

\subsection{ Handwritten Text Recognition }\label{subsec:ssl-methods}

Handwritten Text Recognition aims to transform scanned/digitized handwritten documents into machine-encoded text. HTR is still a hard problem due to the high variability of writing styles, and often poor image quality (degradation). 
% Furthermore, each handwritten script has specific properties (writing direction, character shapes, ligatures, etc) making the problem even more challenging. 
Most HTR approaches are fully supervised methods based on encoder-decoder models \cite{cheng2017focusing,shi2016robust}. Some recent approaches are based on semi-supervised and self-supervised learning \cite{souibgui2021few,zhang2019sequence,souibgui2022text,aberdam2021sequence}. Regarding the unsupervised learning methods, \cite{gupta2018learning} proposes a system for text recognition that aligns the conditional distribution of the predicted strings with lexical correct strings sampled from a text database. Concerning self-supervised learning-based approaches, \cite{aberdam2021sequence} introduced a self-supervised sequence-to-sequence model that separates consecutive text features to be later used in a contrastive loss \cite{chen2020simple}. By concatenating characters and by detecting spatial strokes, \cite{zhang2022context,liu2022perceiving} focused on improving the features resulting from a contrastive loss. However, these approaches rely on large batches and a sequential definition of features, which can result in misaligned characters or n-grams seen in different words.  In \cite{souibgui2022text}, a degradation-based auto-encoder was proposed to overcome the issues of contrastive-based methods and to learn useful representation from text image restoration.  

{In addition, most existing HTR methods  address a single language/script or domain. Only few attempts address the multi-domain \cite{zhang2019sequence} and multi-script/language \cite{huang2021multiplexed, chen2020multrenets, huang2023task}. However, these latter models consist of two stages: 1) identifying the script/language and 2) training task-specific HTR models. Consequently, it is not efficient for learning consecutive tasks, because for each new task, the script/language classifier needs to be retrained, and also, a new HTR model must be trained for the new task. Moreover, these are supervised HTR/OCR models. Although this limitation is not present in existing self-supervised methods, such methods are typically designed for a specific scenario and cannot efficiently generalize to different and continual tasks. Therefore, to overcome these limitations, an efficient and scalable continual learning approach is proposed for the CSSL multi-script HTR problem.}


% Some recent works in the field of visual domain adaptation address the domain shift problem \cite{pei2018multi,yang2018domain}. 
% Nevertheless, most of the recent works use deep convolutional networks and aim to optimize the global representation by minimizing some measure of domain shift, including  MMD \cite{yang2018domain,long2015learning}, CORAL \cite{sun2016deep,zhuo2017deep} and adversarial loss \cite{ganin2015unsupervised,pei2018multi,tzeng2017adversarial}. Consequently, these methods cannot be directly used for sequence recognition, like cursive handwriting (i.e. sequence of joined characters), as the domain shift is in the characters appearance rather than the global text image.
% To address these problems, recent work \cite{zhang2019sequence} presented a sequence-to-sequence domain adaptation network, SSDAN. The method has the ability to recognize scene text, handwritten text, and mathematical expressions. However,
% this approach struggles when recognizing text with different orientations, contrast or font size variations in real scenes. \textcolor{blue}{In addition to that, the proposed model should be adjusted to better deal with various sequence domain shifts.} 




%different text orientations in the presence of contrast variations and font size variations in real-life scenes. 


%\textcolor{blue}{In summary, the lack of well-defined tasks in streaming SSL makes continual learning more challenging, as it needs to learn from data distributions that may shift over time.% In this context, we present a general framework for CSSL with higher performance, conduct large-scale experiments on three challenging scripts, thereby presenting a deeper analysis of CSSL.}
%Motivated by these approaches, in this work we tackle the self-supervised text recognition problem in a continual learning fashion.


 
%Self-supervised methods aim to learn rich visual representations by solving tasks that can be defined without human annotations. A wide variety of methods have been proposed in computer vision. Many of these approaches rely  on auto-encoding of images \cite{vincent2008extracting}. Later, many approaches were investigated for learning image representations, where image colorization \cite{zhang2016colorful},  determining relative patch positions \cite{doersch2015unsupervised} and rotation prediction \cite{gidaris2018unsupervised}, have been employed. Recent approaches \cite{chen2020simple,he2020momentum,zbontar2021barlow,caron2020unsupervised}, are based on data augmentation and contrastive learning and have addressed such issues. Most recently, a breadth of methodologies has been proposed from generative models \cite{he2022masked,bao2021beit}, to predict masked  latent representations of patches. Similar to masked language modeling, they proposed a masked image modeling task as the pre-training objective that achieves SOTA performance. For text recognition, several approaches were recently appearing basing on contrastive learning \cite{aberdam2021sequence,zhang2022context}, generative models \cite{souibgui2022text}, or combining both strategies \cite{yang2022reading}. 


% These approaches adopted a BERT-style pre-training strategy, in which authors of \cite{bao2021beit} proposed a system that first tokenizes the original image into visual tokens, then randomly masks some image patches and feeds them into the backbone Transformer. 
% \cite{zhou2021ibot} introduced a self-supervised framework that can perform masked prediction with an online tokenizer. The online tokenizer is jointly learnable with the MIM objective and dispenses with a multi-stage pipeline where the tokenizer is pre-trained beforehand. Although these works achieve good results adopting free annotation, they have not investigated the ability of SSL methods to learn continuously and adaptively. For this aim, some trends in the literature \cite{gomez2022continually,fini2022self,purushwalkam2022challenges} 
% explore a Continual Self Supervised  learning, in which unsupervised representation learning is indeed of interest for sequential learning when new data is generated on-the-fly.




%\subsection{Handwritten Text Recognition}\label{subsec:htr-methods}

%Handwritten Text Recognition  is a mature research area,  aiming to transform scanned handwritten documents into machine-encoded
%text. HTR is still a hard problem due to the high variance of writing styles, illegible handwriting and poor image quality (degradation). Furthermore, each handwritten script has specific properties (writing direction, character shapes, ligatures, etc) making the problem even more challenging. 
%Most HTR approaches are fully supervised methods based on encoder-decoder models fashion  \cite{cheng2017focusing,shi2016robust}. 
%Other approaches are based on semi-supervised and self-supervised learning \cite{souibgui2021few,zhang2019sequence,souibgui2022text,aberdam2021sequence}. Regarding the unsupervised learning methods, authors of \cite{gupta2018learning} propose a system for text recognition, that
%align the conditional distribution of strings predicted with lexical correct strings sampled from a text database. More recently, many works that focus on self-supervised learning, have been investigated. 



%For instance, \cite{aberdam2021sequence} introduced a self-supervised sequence-to-sequence model that separates consecutive text features to be later used in a contrastive loss \cite{chen2020simple}. By concatenating characters and, respectively, by detecting spatial strokes, authors from \cite{zhang2022context,liu2022perceiving} focused on  improving the features resulting from a contrastive loss. However, these approaches rely on large batches and a sequential definition of features, which can result in misaligned characters or n-grams seen in different words.  In \cite{souibgui2022text}, a degradation-based auto-encoder was proposed to overcome the contrastive-based methods issues and to learn useful representation from text image restoration.  
















