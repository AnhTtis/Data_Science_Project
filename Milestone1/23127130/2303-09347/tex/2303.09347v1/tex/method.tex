\begin{figure*}[t]
    \centering
    \includegraphics[width = \linewidth]{images/architecture.pdf}
    \caption{The continual self-supervised learning module: our method can learn different scripts/languages in a self-supervised way. At each time step $t$, only the task-specific adapter is learned and the rest of the model is frozen to overcome the catastrophic forgetting.
    Also, a set of visual patches from the data is stored in a memory buffer and the visible patches from previous tasks are loaded and replayed into the current model for a task-agnostic inference.}
    \label{fig:pre-training}
\end{figure*}


Our continual self-supervised learning based approach for handwritten text recognition comprises two phases: pre-training and fine-tuning. In this section, we introduce the proposed building blocks in both phases.

\subsection{Pre-training Module}
The overall pre-training pipeline is depicted in Figure~\ref{fig:pre-training}. In this work, the aim is to learn multiple tasks sequentially, where each task consists in recognizing a particular handwritten script or language. The purpose of each task is to learn useful representations from its unlabeled data that will be later used by the HTR model at the fine-tuning stage as the optimal starting point for learning specific or multiple scripts/languages. In this self-supervised learning strategy, the model should not forget the previous tasks when learning a new task. Hence, a continual learning strategy is proposed to achieve this goal. In what follows, we present the techniques used for both learning strategies. Then, we detail the components used within our proposed architecture.


\noindent\textbf{Self-supervised pre-training.}
As we said before, the goal of the SSL pre-training is to learn a useful representation $z$ from each unlabeled image belonging to the pre-training data. To learn this representation, we opt to use a masking/recovering strategy, as it showed success in handwritten text images compared to related approaches \cite{souibgui2022text}. As it can be seen from Figure~\ref{fig:pre-training}, for every new task with timestep $t$, the unlabeled image is first divided into a set of patches. Then, a random portion of these patches denoted by $P_m$ is masked to generate a masked image. After that, an auto-encoder model learns the recovery process by producing an unmasked image. The recovery is done by predicting the masked patches $P_{m_{pred}}$ based only on the visible patches $P_v$. The auto-encoder model is composed of a ViT encoder $\mathcal{E}$ that encodes $P_v$ into a latent representation $z$,  and a transformer decoder $\mathcal{D}$ that maps $z$ to the predicted patches $P_{m_{pred}}$. This makes the masking/recovering training process as follows:

\begin{equation}
\begin{split}
    z &= \mathcal{E}(P_v; \theta_{\mathcal{E}}) \\
    P_{m_{pred}} &= \mathcal{D}(z; \theta_{\mathcal{D}})
\end{split}
\label{eq:pre-training}
\end{equation}


\noindent\textbf{Continual Learning Module.}
As illustrated in Figure~\ref{fig:pre-training}, the different tasks are learned sequentially, where at each time-step $t$, a new script or language is processed. To avoid the catastrophic forgetting of previous tasks, two techniques are used: a task-specific adapter and a memory buffer. 

In a continual learning scenario, the goal is to learn new knowledge for the new task while preserving the previous knowledge of the model. To accomplish this, in a time step $t$, a task-specific sub-model called adapter is added to the architecture. This adapter is the only component that is trained at $t$, while freezing all the components that were trained in the time-step $t-1$. By doing this, maximum preservation of the previous knowledge is maintained (frozen) and also, the learning speed is increased since only the adapter $t$ is trained. The adapter module is added to all the encoder and decoder layers. In each layer, the adapter modules are placed after the multi-head attention and feed-forward components. Thus, they shall adapt the network to the new script/language domain. It is to note that in the first learned task, the adapter component is set to an identity mapping since there is no previous knowledge to preserve. 

By using the adapters, the model is able to adapt to new script/language domains. {But sequentially plugging the task $t$ adapter can affect the model performance on the previous tasks. So, to avoid this and improve the performance of a current adapter on the previous tasks, a memory replay strategy is used.} The idea is to create a memory buffer and store a few samples of the data belonging to the different learned tasks. At time-step $t$, the stored samples of the previous tasks $\{t-1, t-2, ..., 1\}$ are replayed to the model $t$ and its parameters are tuned to have a similar response to the model $t-1$ for this stored data. This is done by employing a knowledge distillation L2 loss between the output of the frozen previous model and the current learning model. An interesting benefit of using a masked auto-encoder for the SSL task is that we are not obliged to store full images. Therefore, we propose in this work to store only a few patches of the unlabeled images. By using those patches and the $t-1$ model, we can produce the restored image to be considered as the ground-truth for the current model. Hence, we propose an interesting solution to the problem of memory exploding when using memory buffers in continual learning. 

Next, we detail the components of our method.
%After presenting our CSSL strategies, we detail in what follows the components of our method.
% After applying CSSL, the learned representations can be used as to fine-tune different models that recognize any of the learned scripts/languages. This will be detailed in the following.


\subsection{Pre-training Module Components}
The proposed model is composed of an encoder-decoder architecture with task-specific adapter components. \\
\noindent\textbf{Encoder.} 
Our used encoder is  vanilla ViT~\cite{dosovitskiy2020image} backbone. Given an input image $I$, it is first split into a set of $N$ patches, $I^p = \{I^{p_1}, I^{p_2}, ... , I^{p_N}\}$. Then, a set of these patches are masked and the remaining visible patches are embedded with a trainable linear projection layer $E$ to obtain tokens. 
These tokens are later concatenated with their 2-D positional information embedded with $E_{pos}$ and fed to $L$ transformer blocks to map these tokens to the encoded latent representation $z_l$. These blocks are composed of $L$ layers of Multi-head Self-Attention (MSA) and a feedforward Multi-Layered Perceptron (MLP) as depicted in Figure~\ref{fig:pre-training}. Each of these blocks are preceded by a LayerNorm (LN) and followed by a residual connection and a set of adapter components. {The number of adapters depends on the number of previously learned tasks. In every learned task $t$, two adapters ${A_E^{t}}_1$ and ${A_E^{t}}_2$ and  are added  after the MHA and the MLP, respectively.} The aforementioned statement is exemplified in Eqn.~\ref{eq:encoding}
{
\begin{equation}
\begin{split}
    z_0 &=  E(I^p) + E_{pos} \\
    {z^\prime}_l &= \text{MSA}(\text{LN}(z_{l-1})) + z_{l-1} \text{, \space} l = 1, \text{ \dots L} \\
    {z^{\prime\prime}}_l &= {A_E^{t}}_1({A_E^{t-1}}_1(\text{ \dots}({A_E^{1}}_1({z^\prime}_l))))  \\ 
    {z}_l &= \text{MLP}(\text{LN}({z^{\prime\prime}}_l)) + {z^{\prime\prime}}_l \text{, \space} l = 1, \text{ \dots L} \\
     z_\mathcal{T} &= {A_E^{t}}_2({A_E^{t-1}}_2(\text{ \dots}({A_E^{1}}_2({z}_L))))  
\end{split}
\label{eq:encoding}
\end{equation}}

\noindent\textbf{Decoder.}
The decoder is a transformer reconstruction module. As in MAE \cite{he2022masked}, the encoded tokens $z_\mathcal{T}$ (Eqn.~\ref{eq:encoding}) are first concatenated with a set of mask tokens that indicated the presence of the missing patches that should be predicted. After that, positional embedding is added to the concatenated tokens and a set of transformer blocks is used to decode these tokens into patches of pixels. The output of the decoder is a set of vectors $I_{r} = \{I_{r}^{p_1}, I_{r}^{p_2}$, \dots, $I_{r}^{p_N}\}$ where each of which corresponds to a flattened patch in the predicted (reconstructed) patches that were initially masked. As in the encoder, the MSA and MLP layers are used with the task-specific adapters {${A_D^{t}}_1$ and ${A_D^{t}}_2$}.
{
\begin{equation}
\begin{split}
    {z^\prime}_l &= \text{MSA}(\text{LN}(z_{l-1})) + z_{l-1} \text{ , \space} l = 1, \text{ \dots L} \\
    {z^{\prime\prime}}_l &= {A_D^{t}}_1({A_D^{t-1}}_1(\text{ \dots}({A_D^{1}}_1({z^\prime}_l))))  \\
    {z}_l &= \text{MLP}(\text{LN}({z^\prime}_l))) + {z^\prime}_l \text{ , \space} l = 1, \text{ \dots L} \\
  I_{r} &= \text{Linear}({A_D^{t}}_2({A_D^{t-1}}_2(\text{ \dots}({A_D^{1}}_2({z}_L)))))
\end{split}
\label{eq:decoding}
\end{equation}}
\noindent\textbf{Adapter.} 
The adapter components are designed to efficiently adapt a model for a new script/language. The idea is to embed them after each MHA and feedforward layer. During pre-training, only the adapters of the current task are trained, while freezing the rest of the model. Each adapter is composed of two Linear layers, where the first Linear layer is preceded by a LayerNorm and  followed by a ReLu activation, as illustrated in Figure~\ref{fig:pre-training}.

\subsection{Pre-training Learning Objectives}\label{subsection:distill}

During the pre-training, at time-step $t$=$1$ (i.e. learning the first language/script), a reconstruction l2 loss $\mathcal{L}_{r}$ is used between the  patches that were masked  $P_m$ and the predicted ones $P_{m_{pred}}$ to make the learning objective as  $\mathcal{L}_{r} (P_m, P_{m_{pred}})$. At $t>1$, an additional distillation l2 loss $\mathcal{L}_{d}$ is used between the output of the current model and the output of the previous one ($t$-$1$) on the data loaded from the memory buffer. This makes the full pre-training learning objective as: 
\begin{equation}
\mathcal{L}_p = \mathcal{L}_{r} (P_m, P_{m_{pred}}) +  \mathcal{L}_{d} ( P_{m_{pred}}, P_{m_{pred}^{t-1}})
\label{eq:loss}
\end{equation}
% \textcolor{blue}{at each task $t >1$, the representation learning objective is optimized with a knowledge distillation loss $\mathcal{L}_{dist} (\mathcal{E} (P_{v}),\mathcal{E}_{t-1}( P_{v} ))$
%  }.

\subsection{Fine-tuning Module}
The downstream task for fine-tuning is handwritten text recognition, as an example of sequence recognition. HTR aims to transform a handwritten text image from a specific language/script into a machine-encoded form, i.e., a sequence of characters.
Let $I$ be a text image and $C_{}=\{c_{{}_1},c_{{}_2}, ..., c_{{}_N}\}$ its ground truth label which corresponds to a sequence of characters, where $N$ is the length of the text. The training is done by passing $I$ to an encoder function $\mathcal{E}$ to produce a latent representation $z$. Then, $z$ is later fed to a decoder function $\mathcal{D^{\prime}}$ multiple times to produce a sequence of characters $C_{p}=\{c_{p_1},c_{p_2}, ..., c_{p_M}\}$ that should match the ground truth label sequence. In our approach, the fine-tuning is done by initializing $\mathcal{E}$ with the learned parameters during pre-training. As shown in Figure~\ref{fig:teaser}, a pre-trained encoder can be used for multi-script fine-tuning, as the $t^{th}$ encoder can be used to train a model that recognizes any of the learned scripts/languages during the tasks $t$-1, $t$-2, ..., 1.



%  SSL (masking recovery)
%  continual (adapters + distillation)


