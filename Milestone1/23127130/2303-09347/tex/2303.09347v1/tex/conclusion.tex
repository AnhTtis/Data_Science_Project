In this paper we have presented, to the best of our knowledge, the first application of continual learning in handwritten text recognition. Our framework benefits from continual learning and self-supervised learning for multi-script/language handwritten text recognition, as an example of sequence recognition. Continual learning can be used in the self-supervised pre-training of the model to boost the performance of the downstream tasks on multiple handwritten languages/scripts. 
Our proposed CSSL-MHTR approach consists of an encoder-decoder transformer model that includes language/script adapter components and a memory replay strategy for continual self-supervised learning on handwritten text images. Extensive experimental results demonstrated the effectiveness of our approach by obtaining the best performance compared to state-of-the-art continual learning approaches. Experiments also proved our method's efficiency in terms of temporal and spatial complexity. We believe that our model can serve as a benchmark for future research in this field.

As future work, we plan to extend this approach to recognize full pages instead of segmented lines. Also, we will explore the addition of other document analysis tasks to be learned continually, for instance, layout analysis, name entity recognition and information extraction.   
