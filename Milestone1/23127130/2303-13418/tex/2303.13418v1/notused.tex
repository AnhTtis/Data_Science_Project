\begin{table}[ht!]
 \centering
 \caption{overall metrics from models created to evaluate the corpus. Hla* }
 \label{tab:resultsTFIDF-BERT}
 \begin{tabular}{c|c|c|r|r|r|r} 
  \hline

  \textbf{Model} & \textbf{One/All projects} & \textbf{Precision} & \textbf{Recall} & \textbf{F-measure} &
  \textbf{Hla} \\
  \hline

TD-IDF & S
%&	0.663 &	0.606 &	0.630 &	0.172 \\
%& 0.751 &	0.738 &	0.744 & 0.145\\
& 0.839 & 	0.799 & 	0.817 & 0.113\\

BERT & S
%&	0.663 &	0.606 &	0.630 &	0.172 \\
%& 0.751 &	0.738 &	0.744 & 0.145\\
& 0.595 & 	0.559 & 	0.568 & 0.269\\
TF-IDF & A        &	0.153   &	0.785 &	0.573 &	0.659\\
%Bert                &   0.281   &   0.637 & 0.625 & 0.619 \\
%Bert & 0.204	&		0.738 &	0.417 &	0.53\\
BERT & A & 0.219 & 0.725 & 	0.511 & 	0.593 \\
 \hline
\multicolumn{5}{l}{\scriptsize\textit{* Hla - Hamming Loss }}
 \end{tabular}
\end{table}

%\begin{table}[ht!]
% \begin{center}
%\caption{overall metrics from the model created with the all-projects dataset.}
% \label{tab:resultsAllTogether}
% \begin{tabular}{c|r|r|r|r} 
%  \hline

% \textbf{Model} &
%  \textbf{Hla*} & \textbf{Precision} & %\textbf{Recall} & \textbf{F-measure}  \\
%  \hline

%TF-IDF and RandomForest        &	0.153   &	0.785 &	0.573 &	0.659\\
%Bert                &   0.281   &   0.637 & 0.625 & 0.619 \\
%Bert & 0.204	&		0.738 &	0.417 &	0.53\\
%BERT & 0.219 & 0.725 & 	0.511 & 	0.593 %\\
% \hline
%\multicolumn{5}{l}{\scriptsize\textit{* Hla - Hamming Loss }}
%\\
% \end{tabular}
% \end{center}
%\end{table}

%%%%%%%%%%%%%%%%%%%%%%

\section{GiveMeLabeledIssues Architecture}\label{ToolDesc}

\textit{GiveMeLabeledIssues} leverages prediction models trained with closed issues linked with merged pull requests. On top of these models, we built a platform that receives user input and queries the open issues based on the users' skills and desired projects. In the following, we detail the process of building the model, the issue classification process, and the user interface that enables users to get the recommendation. \textit{GiveMeLabeledIssues} is structured in two layers: the frontend web interface\footnote{https://github.com/JoeyV55/GiveMeLabeledIssuesUI} and the backend REST API\footnote{https://github.com/JoeyV55/GiveMeLabeledIssuesAPI}.

\subsection{Model training}

\begin{figure}[!hbt]
 \centering
 \includegraphics[width=.5\textwidth] {model-training.pdf}
 \caption{The Process of Training a Model}
 \label{fig:training}
\end{figure}

In the current implementation, a model must be built for each project using data collected from project issues linked with merged pull requests. The tool maps the issue text data to the APIs used in the source code that solved the issues (Figure \ref{fig:training}). 

\subsubsection{Mining repositories}

We collected 18,482 issues and 3,129 pull requests (PRs) from JabRef, PowerToys, and Audacity projects up to November 2021. We used the GitHub REST API v3 to collect the title, body, comments, closure date, name of the files changed in the PR, and commit messages.

\subsubsection{APIs parsing}
We built a parser to process all source files from the projects to identify the APIs used in the source code affected by each pull request. In total, we found 3,686 different APIs in 3,108 source files. The parser looked for specific commands, i.e., import (Java), using (C\#), and include (C++). The parser identified all classes, including the complete namespace from each import/using/include statement.

\subsubsection{Dataset construction}
We kept only the data from issues linked with merged and closed pull requests since we needed to map issue data to the APIs that are used in the source code files changed to close the issue. To find the links between pull requests and issues, we searched for the symbol \texttt{\#issue\_number} in the pull request title and body and checked the URL associated with each link. We also filtered out issues linked to pull requests without at least one source code file (e.g., those associated only with documentation files) since they do not provide the model with content related to any API.

\subsubsection{API categorization}
We use the API domain categories defined by \citet{santos2023tell}. These 31 categories were defined by experts to encompass APIs from several projects (e.g., UI, IO, Cloud, DB, etc.---see our replication package\footnote{https://doi.org/10.5281/zenodo.7575116}. 
%After four rounds of discussions ($\approx$ 8 hours), the experts reached a consensus using a card sorting approach and defined 31 API domains. 

%Then, we parsed the APIs found in the commit source code. A similarity function receives a list of API namespaces to determine the possible domains and the percentage of agreement (e.g., sun.oracle.odbc is "DB" with 80\% confidence). The \texttt{spacy} \footnote{https://spacy.io/api/doc\#similarity} python package offers the similarity function, and \texttt{wordninja} \footnote{https://pypi.org/project/wordninja/} package splits the API namespace. Each API received the best API domain found by the similarity function. The experts manually evaluated 100 APIs and approved the categorization in 74.2\% of the cases. 25.8\% were categorized by the experts using card sorting.
%from table 8 (EMSE paper) avg of jabref, audacity and powertoys
%For the TF-IDF model, similar to a previous study \cite{vadlamani2020studying}, we converted each word to lowercase and removed URLs, source code, numbers, and punctuation. After that, we removed templates and stop-words and stemmed the words.

\subsubsection{Corpus construction}
We used the issue title and body as our corpus to train our model since they performed well in our previous analysis~\cite{santos2021can}. Similar to other studies~\cite{behl2014bug,vadlamani2020studying}, we applied TF-IDF as a technique for quantifying word importance in documents by assigning a weight to each word following the same process described in the previous work \cite{santos2021can}. TF-IDF returns a vector whose length is the number of terms used to calculate the scores. Before calculating the scores, we convert each word to lowercase and removed URLs, source code, numbers, and punctuation. After that, we remove templates and stop-words and stem the words. These TF-IDF scores are then passed to the Random Forest classifier (RF) as features for prediction. RF was chosen since it obtained the best results in previous work \cite{santos2021can}. The ground truth has a binary value (0 or 1) for each API domain, indicating whether the corresponding domain is present in the issue solution.

We also offer the option of using a BERT model in \textit{GiveMeLabeledIssues}. We created two separate CSV files to train BERT: an input binary with expert API-domain labels paired with the issue corpus and a list of the possible labels for the specific project. BERT directly labels the issue using the corpus text and lists possible labels without needing an additional classifier (such as Random Forest). 

\subsubsection{Building the model}
The BERT model was built using the Python package fast-bert\footnote{https://github.com/utterworks/fast-bert}, which builds on the Transformers\footnote{\url{https://huggingface.co/docs/transformers/index}} library for Pytorch. Before training the model, the optimal learning rate was computed using a LAMB optimizer~\cite{You2020Large}. Finally, the model was fit over 11 epochs and validated every epoch. The BERT model was trained on an NVIDIA Tesla V100 GPU with an Intel(R) Xeon(R) Gold 6132 CPU within a computing cluster.

TF-IDF and BERT models were trained and validated for every fold in a ShuffleSplit 10-fold cross-validation. Once trained, the models were hosted on the backend. The replication package contains instructions on registering a new project by running the model training pipeline that feeds the demo tool. The models can then output predictions quickly without continually retraining with each request. 
%TF-IDF and BERT were chosen primarily due to previous \cite{santos2021can} and ongoing work that uses them to label issues by API domain.

For the RandomForestClassifier (TF-IDF), the best classifier, we kept the following parameters: criterion = 'entropy,' max depth = 50, min samples leaf = 1, min samples split=3, and n estimators = 50.

\subsection{Issue Classification Process}

\textit{GiveMeLabeledIssues} classifies currently open issues for each registered project. The tool combines the title and body text and sends it to the classifier. The classifier then determines which domain labels are relevant to the gathered issues based on the inputted issue text. The labeled issues are stored in an SQLite database for future requests, recording the issue number, title, body, and domain labels outputted by the classifier.

%\begin{itemize}
 % \item The issue number
% \item The issue title and body text
% \item All the domain labels output by the classifier
%\end{itemize}

The open issues for all projects registered with \textit{GiveMeLabeledIssues} are reclassified daily to ensure that the database is up to date as issues are opened and closed. Figure \ref{fig:Classify} outlines the daily classification procedure. 

\begin{figure}[!hbt]
 \centering
 \includegraphics[width=.5\textwidth] {ClassifySteps.png}
 \caption{The Process of Classifying and Storing Issues}
 %\vspace{-10pt}
 \label{fig:Classify}
\end{figure}


\subsection{User Interface}

\textit{GiveMeLabeledIssues} outputs the labeled issues to the User Interface. The user interface is implemented using the Angular web framework. To use the tool, users provide the project name and select API-domain labels they are interested in. %the following to the front end.
%\begin{enumerate}
% \item The name of the project they would like to work on (selected from a list of projects already classified)
% \item The skills they have (User Interface, Databases, Machine Learning, etc.)
%\end{enumerate}
This information is sent to the backend REST endpoint via a GET request. The backend processes the request, recommending a set of relevant issues for the user. %The query sends the project name and labels to the backend via a GET request.

The backend REST API is implemented using the Django Rest Framework. It houses the trained TF-IDF and BERT text classification models and provides an interface to the labeled issues. When receiving the request, the backend queries the selected project for issues that match the user's skills. Once the query is completed, the backend returns the labeled issues to the user interface. Each labeled issue includes a link to the open issue page on GitHub and the issue's title, number, and applicable labels. The querying process is shown in Figure \ref{fig:OutputtingSteps}.
%that is available to work on within the selected project. The cards 

\begin{figure}[!h]
 \centering
 \includegraphics[width=.5\textwidth] {QuerySteps.png}
 \caption{The Process of Outputting Issues}
 \label{fig:OutputtingSteps}
 %\vspace{-10pt}
 \end{figure}

Figure~\ref{fig:skillSelect} shows JabRef selected as the project and ``Utility,'' ``Databases,'' ``User Interface,'' and ``Application'' as the API domains provided by the user. Figure~\ref{fig:issueResult} shows the results of this query, which displays all JabRef open issues that match those labels.


\begin{figure}[!hbt]
 \includegraphics [width=.5\textwidth] {skillSelection.png}
 \caption{Selection of a project and API domains}
 \label{fig:skillSelect}
 \end{figure}

%\vspace{-10pt}
 \begin{figure}[!hbt]
 \includegraphics [width=.5\textwidth] {threeIssues.png}
 \caption{Labeled Issues Outputted for JabRef with the Utility, Databases, User Interface, and Application skills Selected}
 \label{fig:issueResult}
 \end{figure}
%\vspace{-13pt}

\section{Evaluation}

%So far, our evaluation has been conducted in the terms of the efficiency of the models. 
We have evaluated the performance of the models used to output API-domain labels using a dataset comprised of %22,318 issues, 3,572 pull requests, 12,887 source code files, and 12,331 APIs parsed from three projects, 
%data from EMSE paper table 6.
18,482 issues, 3,129 PRs, 3,108 source code files, and 3,686 distinct APIs, chosen from active projects and diverse domains: Audacity (C++), PowerToys (C\#), and JabRef (Java).\footnote{The model training replication package is available at \url{https://zenodo.org/record/7726323#.ZA5oy-zMIeY}} 
Audacity is an audio editor, PowerToys is a set of utilities for Windows, and JabRef is an open-source bibliography manager. %and which are part of an experiment conducted in another under evaluation article. %=22231-913 table 6 MSRExt paper =4674-1075 =13107-220 =12772-441 = 3942-206
%We evaluated our approach four projects using both TF-IDF and BERT models. 
Table \ref{tab:resultsTFIDF-BERT} shows the results with Precision, Recall, F-Measure, and Hamming Loss values. %While previous studies use Precision, Recall, and F-Measure as performance indicators \cite{santos2021can}, we added hamming loss that measures the fraction of the wrong labels to the total number of labels. 
We trained models to predict API-domain labels using individual issue datasets from each project and a single dataset that combined the data from all the projects. 

%Due to privacy concerns, the %GiveMeLabeledIssues 
%demo does not allow users to access recommendations for the industry project. Thus, the demo includes only the three Open Source projects: JabRef, PowerToys, and Audacity. 

\begin{table}[]
\centering
 \caption{Overall metrics from models -averages. RF-Random Forest* Hla - Hamming Loss**}
 \label{tab:resultsTFIDF-BERT}
\begin{tabular}{|c|c|c|r|r|r|}
\hline
\begin{tabular}[c]{@{}c@{}}Model \\ averages\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{O}ne/\textbf{M}ulti \\ projects\end{tabular} & Precision & Recall & \begin{tabular}[c]{@{}c@{}}F-meas \\-ure \end{tabular}& Hla** \\ \hline
RF* TF-IDF & O & \textbf{0.839} & 0.799 & 0.817 & \textbf{0.113} \\
BERT & O & 0.595 & 0.559 & 0.568 & 0.269 \\
RF* TF-IDF & M & 0.659 & 0.785 & 0.573 & 0.153 \\
BERT & M & 0.593 & 0.725 & 0.511 & 0.219 \\ 
\citet{izadi2022predicting} & M & 0.796 & 0.776 & 0.766 & NA \\
\citet{kallis2019ticket} & M & 0.832 & \textbf{0.826} & \textbf{0.826} & NA \\
\citet{santos2021can} & O & 0.755 & 0.747 &
0.751 & 0.142 \\
\hline
\end{tabular}
%\vspace{-10pt}
\end{table}



\begin{table}[]
\centering
 \caption{Overall metrics from models - by projects. RF-Random Forest* Hla - Hamming Loss**}
 \label{tab:resultsTFIDF-BERT-projects}
\begin{tabular}{|c|c|c|r|r|r|}
\hline
\begin{tabular}[c]{@{}c@{}}Model \\ by project\end{tabular} & \begin{tabular}[c]{@{}c@{}}Project\end{tabular} & Precision & Recall & \begin{tabular}[c]{@{}c@{}}F-meas \\-ure \end{tabular}& Hla** \\ \hline
RF* TF-IDF & Audacity	&\textbf{0.872}	&\textbf{0.839}	&\textbf{0.854} & 0.103\\	
RF* TF-IDF & JabRef	&0.806	&0.782	&0.793 & 0.143\\	
RF* TF-IDF & PowerToys	&0.84	&0.776	&0.805 &\textbf{0.094}\\	
BERT & Audacity		&0.382	&0.511	&0.434 & 0.42\\	
BERT & JabRef	&0.791	&0.606	&0.686	& 0.192\\
BERT & PowerToys	&0.619	&0.643	&0.626 & 0.187\\	
\hline
\end{tabular}
%\vspace{-20pt}
\end{table}

As shown in Table~\ref{tab:resultsTFIDF-BERT}, TF-IDF overcame BERT both in the per-project analysis and for the complete dataset. The difference was quite large when using single projects. The results were closer when we used the combined dataset that included data from all projects (3,736 linked issues and pull requests). We hypothesize that the sample size influenced the classifiers' performance. This aligns with previous research on issue labeling that showed that BERT performed better than other language models for datasets larger than 5,000 issues \cite{wang2021well}. TF-IDF performs very well when the dataset is from a single project because the vocabulary used in the project is very contextual, and the frequency of terms can identify different aspects of each issue. When we include the dataset from all the projects, the performance of TF-IDF drops as the context is not unique. These results outperformed the results from the API-domain labels case study conducted by Santos et al. \cite{santos2021can}. 
%While \citet{kallis2019ticket} results for recall and F-measure overcame all the compared studies, they are limited to "bug", "enhancement", and "question" classification and the feature space with only three possible labels may benefit the approach. In comparison, our models predict many more labels, with PowerToys predicting 13 labels, Jabref 12, and Audacity 20. 
The project metrics (Table \ref{tab:resultsTFIDF-BERT-projects}) varied less than 6\% (e.g., the recall: 0.839 (Audacity) - 0.776 (PowerToys). Audacity had the best scores for all metrics except Hamming Loss. 