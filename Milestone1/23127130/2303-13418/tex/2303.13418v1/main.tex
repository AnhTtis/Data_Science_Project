\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage[numbers]{natbib}
\bibliographystyle{IEEEtranN}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
 T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{GiveMeLabeledIssues: An Open Source Issue Recommendation System\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Joseph Vargovich, Fabio Santos, Jacob Penney, Marco A. Gerosa, Igor Steinmacher}
\IEEEauthorblockA{\textit{School of Informatics, Computing, and Cyber Systems} \\
\textit{Northern Arizona University}\\
Flagstaff, United States \\
joseph\_vargovich@nau.edu, fabio\_santos@nau.edu, jacob\_penney@nau.edu, Marco.Gerosa@nau.edu, Igor.Steinmacher@nau.edu}
}

\maketitle

\begin{abstract}

Developers often struggle to navigate an Open Source Software (OSS) project's issue-tracking system and find a suitable task. Proper issue labeling can aid task selection, but current tools are limited to classifying the issues according to their type (e.g., bug, question, good first issue, feature, etc.). In contrast, this paper presents a tool (\textit{GiveMeLabeledIssues}) that mines project repositories and labels issues based on the skills required to solve them. We leverage the domain of the APIs involved in the solution (e.g., User Interface (UI), Test, Databases (DB), etc.) as a proxy for the required skills. \textit{GiveMeLabeledIssues} facilitates matching developers' skills to tasks, reducing the burden on project maintainers. The tool obtained a precision of 83.9\% when predicting the API domains involved in the issues. The replication package contains instructions on executing the tool and including new projects. A demo video is available at https://www.youtube.com/watch?v=ic2quUue7i8 

%\textit{GiveMeLabeledIssues} is a web tool that provides labeled issues according to API domains that would be required to work on the code potentially affected by that issue. API-domain labels indicate the type of skills that would be required to work on the issue.

\end{abstract}

\begin{IEEEkeywords}
Open Source Software, Machine Learning, Label, Tag, Task, Issue Tracker
\end{IEEEkeywords}

\section{Introduction}
Contributors struggle to find issues to work on in Open Source Software (OSS) projects due to difficulty determining the skills required to work on an issue \cite{steinmacher2015systematic}. Labeling issues manually can help \cite{santos2022choose}, but manual work increases maintainers' effort, and many projects end up not labeling their issues~\cite{barcomb2020managing}. 

%since communities expect new developers to find tasks or issues to work on by themselves~\cite{vonKrogh}. %However, it is common they have no clue how to do this \cite{park2009beyond}. 
%Therefore, automatic labeling of open issues is beneficial for both prospective contributors and project maintainers.

To facilitate issue labeling, several studies proposed mining software repositories to tag the issues with labels such as bug/non-bug and good-first-issue (to help newcomers find a suitable task) \cite{pingclasai2013classifying,zhu2019bug, el2020automatic, perez2021bug}. However, newcomers to a project have a variety of skill levels and these approaches do not indicate the skills needed to work on the tasks. 
%Since skill sets and interests vary across developers, such a classification would be important to aid developers when they are looking for an issue to work on.
%More recently, studies proposed using new labels \cite{kallis2019ticket}, employing a BERT text model \cite{izadi2022predicting, WANG2021107476}. Despite classifying into distinct labels, such approaches only use preexisting labels.

%In previous work~\cite{santos2021can}, we have shown that this type of label helps developers choose their tasks. 
APIs encapsulate modules with specific functionality. If we can predict APIs used to solve an issue, we can guide new contributors on what to contribute. However, since projects include thousands of different APIs, adding each API as a label would harm the user experience. To keep the number of labels manageable, we classified the APIs into domains for labeling~\cite{santos2023tell,santos2022choose}


%, because if the contributors know what types of APIs are used in the code that will be changed to solve the issue, they could choose tasks that better match their skills.} 
%We leverage the idea that APIs encapsulate modules with specific purposes (e.g., cryptography, database access, logging) and abstract the details from the implementation
 %Labeling issues with API domains would point to some of these needed skills, therefore alleviating the difficulty in selecting an issue. 
In our study, API domains represent categories of APIs such as ``UI,'' ``DB,'' ``Test,'' etc. In our previous work, we used 31 distinct API domains \cite{santos2023tell}. However, we acknowledge that the set of possible API-domain labels varies by project, as each project requires different skills and expertise. 
%API-Domains are added to the set of valid labels for a project so long as they are identified as relevant to at least one previously closed issue for that project.
In another work, Santos et al.~\cite{santos2021can} have evaluated the API domain labels in an empirical experiment with developers. The study showed that the labels enabled contributors to find OSS tasks that matched their expertise more efficiently. 
Santos et al. study showed promising results when classifying issues by API-Domain, with precision, recall, and F-measure scores of 0.755, 0.747, and 0.751, respectively. %, and which values we use as a baseline. 
%\textcolor{blue}{\citet{santos2021can} also analyzed how developers perceive the API-domain labels and found that API-domain labels help choose tasksâ€”developers considered these labels more often than the existing project labels and other pieces of information.} %Finally, high-level API groups effectively organize issue labels by eliminating the need to label individual APIs, reducing the number of labels and noise on each issue. 

Following this idea, we implemented \textit{GiveMeLabeledIssues} to classify issues for potential contributors. \textit{GiveMeLabeledIssues} is a web tool that indicates API-domain labels for open issues in registered projects. Currently, \textit{GiveMeLabeledIssues} works with three open-source projects: JabRef, PowerToys, and Audacity. The tool enables users to select a project, input their skill set (in terms of our categories), and receive a list of open issues that would require those skills. 
%Our API-domain labeling system is novel to the best of our knowledge. No other system appears to provide open source issues labeled by API domain. 

We trained the tool with the title and body text of closed issues and the APIs utilized within code that solve the issue, as gathered from pull requests. 
%The trained machine learning model is then exposed to the frontend web tool via a REST API.
%, which it uses to return labeled issues based on the project and skills the users inputted to the tool. 
We evaluated the tool using closed issues as the ground truth and found an average of 83.8\% precision and 78.5\% recall when training and testing models for individual projects. 

% We plan to run an empirical user experiment where the participants will contribute to an issue and have the progress measured. Participants will be divided into two groups: one will be exposed to an issue page with the API-domain labels added and a second group will access the same issues without them. 

%Overall, the tool aims to connect prospective contributors with new projects and tasks well suited to their skill set.

%\textit{GiveMeLabeledIssues} is a valuable mechanism for strengthening the optimizing development progress within Open Source software projects. 

\section{GiveMeLabeledIssues Architecture}\label{ToolDesc}

\textit{GiveMeLabeledIssues} leverages prediction models trained with closed issues linked with merged pull requests. On top of these models, we built a platform that receives user input and queries the open issues based on the users' skills and desired projects. In the following, we detail the process of building the model, the issue classification process, and the user interface that enables users to get the recommendation. \textit{GiveMeLabeledIssues} is structured in two layers: the frontend web interface\footnote{https://github.com/JoeyV55/GiveMeLabeledIssuesUI} and the backend REST API\footnote{https://github.com/JoeyV55/GiveMeLabeledIssuesAPI}.

\subsection{Model training}

\begin{figure}[!hbt]
 \centering
 \includegraphics[width=.5\textwidth] {model-training.pdf}
 \caption{The Process of Training a Model}
 \label{fig:training}
\end{figure}

In the current implementation, a model must be built for each project using data collected from project issues linked with merged pull requests. The tool maps the issue text data to the APIs used in the source code that solved the issues (Figure \ref{fig:training}). 

\subsubsection{Mining repositories}

We collected 18,482 issues and 3,129 pull requests (PRs) from JabRef, PowerToys, and Audacity projects up to November 2021. We used the GitHub REST API v3 to collect the title, body, comments, closure date, name of the files changed in the PR, and commit messages.

\subsubsection{APIs parsing}
We built a parser to process all source files from the projects to identify the APIs used in the source code affected by each pull request. In total, we found 3,686 different APIs in 3,108 source files. The parser looked for specific commands, i.e., import (Java), using (C\#), and include (C++). The parser identified all classes, including the complete namespace from each import/using/include statement.

\subsubsection{Dataset construction}
We kept only the data from issues linked with merged and closed pull requests since we needed to map issue data to the APIs that are used in the source code files changed to close the issue. To find the links between pull requests and issues, we searched for the symbol \texttt{\#issue\_number} in the pull request title and body and checked the URL associated with each link. We also filtered out issues linked to pull requests without at least one source code file (e.g., those associated only with documentation files) since they do not provide the model with content related to any API.

\subsubsection{API categorization}
We use the API domain categories defined by \citet{santos2023tell}. These 31 categories were defined by experts to encompass APIs from several projects (e.g., UI, IO, Cloud, DB, etc.---see our replication package\footnote{https://doi.org/10.5281/zenodo.7575116}. 
%After four rounds of discussions ($\approx$ 8 hours), the experts reached a consensus using a card sorting approach and defined 31 API domains. 

%Then, we parsed the APIs found in the commit source code. A similarity function receives a list of API namespaces to determine the possible domains and the percentage of agreement (e.g., sun.oracle.odbc is "DB" with 80\% confidence). The \texttt{spacy} \footnote{https://spacy.io/api/doc\#similarity} python package offers the similarity function, and \texttt{wordninja} \footnote{https://pypi.org/project/wordninja/} package splits the API namespace. Each API received the best API domain found by the similarity function. The experts manually evaluated 100 APIs and approved the categorization in 74.2\% of the cases. 25.8\% were categorized by the experts using card sorting.
%from table 8 (EMSE paper) avg of jabref, audacity and powertoys
%For the TF-IDF model, similar to a previous study \cite{vadlamani2020studying}, we converted each word to lowercase and removed URLs, source code, numbers, and punctuation. After that, we removed templates and stop-words and stemmed the words.

\subsubsection{Corpus construction}
We used the issue title and body as our corpus to train our model since they performed well in our previous analysis~\cite{santos2021can}. Similar to other studies~\cite{behl2014bug,vadlamani2020studying}, we applied TF-IDF as a technique for quantifying word importance in documents by assigning a weight to each word following the same process described in the previous work \cite{santos2021can}. TF-IDF returns a vector whose length is the number of terms used to calculate the scores. Before calculating the scores, we convert each word to lowercase and removed URLs, source code, numbers, and punctuation. After that, we remove templates and stop-words and stem the words. These TF-IDF scores are then passed to the Random Forest classifier (RF) as features for prediction. RF was chosen since it obtained the best results in previous work \cite{santos2021can}. The ground truth has a binary value (0 or 1) for each API domain, indicating whether the corresponding domain is present in the issue solution.

We also offer the option of using a BERT model in \textit{GiveMeLabeledIssues}. We created two separate CSV files to train BERT: an input binary with expert API-domain labels paired with the issue corpus and a list of the possible labels for the specific project. BERT directly labels the issue using the corpus text and lists possible labels without needing an additional classifier (such as Random Forest). 

\subsubsection{Building the model}
The BERT model was built using the Python package fast-bert\footnote{https://github.com/utterworks/fast-bert}, which builds on the Transformers\footnote{\url{https://huggingface.co/docs/transformers/index}} library for Pytorch. Before training the model, the optimal learning rate was computed using a LAMB optimizer~\cite{You2020Large}. Finally, the model was fit over 11 epochs and validated every epoch. The BERT model was trained on an NVIDIA Tesla V100 GPU with an Intel(R) Xeon(R) Gold 6132 CPU within a computing cluster.

TF-IDF and BERT models were trained and validated for every fold in a ShuffleSplit 10-fold cross-validation. Once trained, the models were hosted on the backend. The replication package contains instructions on registering a new project by running the model training pipeline that feeds the demo tool. The models can then output predictions quickly without continually retraining with each request. 
%TF-IDF and BERT were chosen primarily due to previous \cite{santos2021can} and ongoing work that uses them to label issues by API domain.

For the RandomForestClassifier (TF-IDF), the best classifier, we kept the following parameters: criterion = 'entropy,' max depth = 50, min samples leaf = 1, min samples split=3, and n estimators = 50.

\subsection{Issue Classification Process}

\textit{GiveMeLabeledIssues} classifies currently open issues for each registered project. The tool combines the title and body text and sends it to the classifier. The classifier then determines which domain labels are relevant to the gathered issues based on the inputted issue text. The labeled issues are stored in an SQLite database for future requests, recording the issue number, title, body, and domain labels outputted by the classifier.

%\begin{itemize}
 % \item The issue number
% \item The issue title and body text
% \item All the domain labels output by the classifier
%\end{itemize}

The open issues for all projects registered with \textit{GiveMeLabeledIssues} are reclassified daily to ensure that the database is up to date as issues are opened and closed. Figure \ref{fig:Classify} outlines the daily classification procedure. 

\begin{figure}[!hbt]
 \centering
 \includegraphics[width=.5\textwidth] {ClassifySteps.png}
 \caption{The Process of Classifying and Storing Issues}
 %\vspace{-10pt}
 \label{fig:Classify}
\end{figure}


\subsection{User Interface}

\textit{GiveMeLabeledIssues} outputs the labeled issues to the User Interface. The user interface is implemented using the Angular web framework. To use the tool, users provide the project name and select API-domain labels they are interested in. %the following to the front end.
%\begin{enumerate}
% \item The name of the project they would like to work on (selected from a list of projects already classified)
% \item The skills they have (User Interface, Databases, Machine Learning, etc.)
%\end{enumerate}
This information is sent to the backend REST endpoint via a GET request. The backend processes the request, recommending a set of relevant issues for the user. %The query sends the project name and labels to the backend via a GET request.

The backend REST API is implemented using the Django Rest Framework. It houses the trained TF-IDF and BERT text classification models and provides an interface to the labeled issues. When receiving the request, the backend queries the selected project for issues that match the user's skills. Once the query is completed, the backend returns the labeled issues to the user interface. Each labeled issue includes a link to the open issue page on GitHub and the issue's title, number, and applicable labels. The querying process is shown in Figure \ref{fig:OutputtingSteps}.
%that is available to work on within the selected project. The cards 

\begin{figure}[!h]
 \centering
 \includegraphics[width=.5\textwidth] {QuerySteps.png}
 \caption{The Process of Outputting Issues}
 \label{fig:OutputtingSteps}
 %\vspace{-10pt}
 \end{figure}

Figure~\ref{fig:skillSelect} shows JabRef selected as the project and ``Utility,'' ``Databases,'' ``User Interface,'' and ``Application'' as the API domains provided by the user. Figure~\ref{fig:issueResult} shows the results of this query, which displays all JabRef open issues that match those labels.


\begin{figure}[!hbt]
 \includegraphics [width=.5\textwidth] {skillSelection.png}
 \caption{Selection of a project and API domains}
 \label{fig:skillSelect}
 \end{figure}

%\vspace{-10pt}
 \begin{figure}[!hbt]
 \includegraphics [width=.5\textwidth] {threeIssues.png}
 \caption{Labeled Issues Outputted for JabRef with the Utility, Databases, User Interface, and Application skills Selected}
 \label{fig:issueResult}
 \end{figure}
%\vspace{-13pt}

\section{Evaluation}

%So far, our evaluation has been conducted in the terms of the efficiency of the models. 
We have evaluated the performance of the models used to output API-domain labels using a dataset comprised of %22,318 issues, 3,572 pull requests, 12,887 source code files, and 12,331 APIs parsed from three projects, 
%data from EMSE paper table 6.
18,482 issues, 3,129 PRs, 3,108 source code files, and 3,686 distinct APIs, chosen from active projects and diverse domains: Audacity (C++), PowerToys (C\#), and JabRef (Java).\footnote{The model training replication package is available at \url{https://zenodo.org/record/7726323#.ZA5oy-zMIeY}} 
Audacity is an audio editor, PowerToys is a set of utilities for Windows, and JabRef is an open-source bibliography manager. %and which are part of an experiment conducted in another under evaluation article. %=22231-913 table 6 MSRExt paper =4674-1075 =13107-220 =12772-441 = 3942-206
%We evaluated our approach four projects using both TF-IDF and BERT models. 
Table \ref{tab:resultsTFIDF-BERT} shows the results with Precision, Recall, F-Measure, and Hamming Loss values. %While previous studies use Precision, Recall, and F-Measure as performance indicators \cite{santos2021can}, we added hamming loss that measures the fraction of the wrong labels to the total number of labels. 
We trained models to predict API-domain labels using individual issue datasets from each project and a single dataset that combined the data from all the projects. 

%Due to privacy concerns, the %GiveMeLabeledIssues 
%demo does not allow users to access recommendations for the industry project. Thus, the demo includes only the three Open Source projects: JabRef, PowerToys, and Audacity. 

\begin{table}[]
\centering
 \caption{Overall metrics from models -averages. RF-Random Forest* Hla - Hamming Loss**}
 \label{tab:resultsTFIDF-BERT}
\begin{tabular}{|c|c|c|r|r|r|}
\hline
\begin{tabular}[c]{@{}c@{}}Model \\ averages\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{O}ne/\textbf{M}ulti \\ projects\end{tabular} & Precision & Recall & \begin{tabular}[c]{@{}c@{}}F-meas \\-ure \end{tabular}& Hla** \\ \hline
RF* TF-IDF & O & \textbf{0.839} & 0.799 & 0.817 & \textbf{0.113} \\
BERT & O & 0.595 & 0.559 & 0.568 & 0.269 \\
RF* TF-IDF & M & 0.659 & 0.785 & 0.573 & 0.153 \\
BERT & M & 0.593 & 0.725 & 0.511 & 0.219 \\ 
\citet{izadi2022predicting} & M & 0.796 & 0.776 & 0.766 & NA \\
\citet{kallis2019ticket} & M & 0.832 & \textbf{0.826} & \textbf{0.826} & NA \\
\citet{santos2021can} & O & 0.755 & 0.747 &
0.751 & 0.142 \\
\hline
\end{tabular}
%\vspace{-10pt}
\end{table}



\begin{table}[]
\centering
 \caption{Overall metrics from models - by projects. RF-Random Forest* Hla - Hamming Loss**}
 \label{tab:resultsTFIDF-BERT-projects}
\begin{tabular}{|c|c|c|r|r|r|}
\hline
\begin{tabular}[c]{@{}c@{}}Model \\ by project\end{tabular} & \begin{tabular}[c]{@{}c@{}}Project\end{tabular} & Precision & Recall & \begin{tabular}[c]{@{}c@{}}F-meas \\-ure \end{tabular}& Hla** \\ \hline
RF* TF-IDF & Audacity	&\textbf{0.872}	&\textbf{0.839}	&\textbf{0.854} & 0.103\\	
RF* TF-IDF & JabRef	&0.806	&0.782	&0.793 & 0.143\\	
RF* TF-IDF & PowerToys	&0.84	&0.776	&0.805 &\textbf{0.094}\\	
BERT & Audacity		&0.382	&0.511	&0.434 & 0.42\\	
BERT & JabRef	&0.791	&0.606	&0.686	& 0.192\\
BERT & PowerToys	&0.619	&0.643	&0.626 & 0.187\\	
\hline
\end{tabular}
%\vspace{-20pt}
\end{table}

As shown in Table~\ref{tab:resultsTFIDF-BERT}, TF-IDF overcame BERT both in the per-project analysis and for the complete dataset. The difference was quite large when using single projects. The results were closer when we used the combined dataset that included data from all projects (3,736 linked issues and pull requests). We hypothesize that the sample size influenced the classifiers' performance. This aligns with previous research on issue labeling that showed that BERT performed better than other language models for datasets larger than 5,000 issues \cite{wang2021well}. TF-IDF performs very well when the dataset is from a single project because the vocabulary used in the project is very contextual, and the frequency of terms can identify different aspects of each issue. When we include the dataset from all the projects, the performance of TF-IDF drops as the context is not unique. These results outperformed the results from the API-domain labels case study conducted by Santos et al. \cite{santos2021can}. 
%While \citet{kallis2019ticket} results for recall and F-measure overcame all the compared studies, they are limited to "bug", "enhancement", and "question" classification and the feature space with only three possible labels may benefit the approach. In comparison, our models predict many more labels, with PowerToys predicting 13 labels, Jabref 12, and Audacity 20. 
The project metrics (Table \ref{tab:resultsTFIDF-BERT-projects}) varied less than 6\% (e.g., the recall: 0.839 (Audacity) - 0.776 (PowerToys). Audacity had the best scores for all metrics except Hamming Loss. 

\section{Related Work}
%Finding a task to start is challenging for newcomers, according to many studies \cite{AnitaCSCW, HICCSTask}. One reason is that the contributors must have a set of skills related to the project implementation \cite{steinmacher2015systematic} and, consequently, to the available tasks. In addition, communities expect new developers to find tasks or issues to work on by themselves~\cite{vonKrogh}. However, they have no clue how to do this \cite{park2009beyond}. 
The existing literature explores strategies to help newcomers find tasks within OSS projects. Steinmacher et al.~\cite{steinmacher2016overcoming} proposed a portal to help newcomers find relevant documentation. Despite pointing the contributor to existing resources, a newcomer may have difficulty relating the documentation to the skills required to solve a task. %Another strategy reported focuses on tagging the issues on the issue tracker \cite{santos2022choose}. However, this approach requires manual input from the maintainers.

There are also several approaches designed to label issues automatically. However, most of them only try to distinguish bug reports from non-bug reports \cite{pingclasai2013classifying,zhou2016combining,xia2013tag}. \citet{zhou2016combining} built a Naive Bayes (NB) classifier to predict the priorities of bug reports. \citet{xia2013tag} tagged questions using preexisting labels. Other work \cite{el2020automatic, perez2021bug} is also restricted to existing labels while others \cite{kallis2019ticket, izadi2022predicting} proposed other labels. \citet{kallis2019ticket}, for instance, employed the textual description of the issues to classify the issues into types.

Labeling also attracted the attention of software requirement researchers. 
\citet{perez2021requirements} and \citet{quba2021software} proposed to categorize documents with functional and non-functional requirements labels to solve software engineers' problems. Non-functional requirements labels included 12 general domains like ``Performance'' or ``Availability''
\cite{quba2021software}. Such approaches use higher-level labels that would not guide contributors to choose appropriate tasks given their skills. ``Availability,'' for instance, may be related to a database, network, or a cloud module and can be challenging for a newcomer to realize where to start due to the extent of modules to analyze to find the root cause of a bug. 

APIs are also often investigated in software engineering. Recent work focuses on providing API recommendation \cite{8186224, 8478004}, giving API tips to developers \cite{8816774}, defining the skill space for APIs, developers, and projects \cite{dey2020representation} or identifying experts in libraries \cite{8816776}. The APIs usually implement a concept (database access, thread management, etc.). Knowing the API involved in a potential solution allows newcomers to pick an issue that matches their skillset. Therefore, unlike the presented related studies, our tool labels issues based on API domains~\cite{santos2021can}. %found predictions with while 
%which found 83.2\%, 82.6\% and 82.6\% and 79.6\%, 77.6\% and 76.6\% (precision, recall and F-measure). 
%85\% 84\% 70\% - 79.6 p
%88% 82% 63% - 77.6 r
%87% 83% 66% - 76.6 F
%data notebook Analysis_base_H1_H2_H3-MLSMOTE-meta-TF-IDF-BERT.ipynb
%Precision 82.2% 89.4% 78.1% = 83.2
%Recall 84.1% 76.3% 87.4% = 82.6
%F-measure 83.1% 82.3% 82.5% = 82.6

\section{Conclusion and Future Work}
\label{conclusion}

\textit{GiveMeLabeledIssues} provides OSS developers with issues that can potentially match their skill sets or interests. Such a tool may facilitate the onboarding of newcomers and alleviate the workload of project maintainers.  

Future work can explore different domain labels, such as those offered by accredited standards (i.e., ACM/IEEE/ISO/SWEBOK). %\textit{GiveMeLabeledIssues} has strong benefits for both contributors and maintainers of Open Source software projects.
%There are several additional features we would like to add to \textit{GiveMeLabeledIssues}. We would like to include additional text classification models, such as Doc2Vec \cite{kim2019multi}, in future iterations of the tool. Additionally, we will create a plug-in to directly integrate \textit{GiveMeLabeledIssues} with GitHub projects by adding labels to the issues page on the registered project repositories themselves.
As a future step to evaluate the tool's impact, we will conduct a study to receive feedback from contributors and assess how the tool influences their choices by means of controlled experiments. Future work can also incorporate the use of social features~\cite{santos2023tell} and integrate the tool into GitHub Actions, or bots \cite{kinsman2021software}.
%\textit{GiveMeLabeledIssues} will be set up to predict labels within closed issues that will then be presented to contributors. 
%After showing participants a page with issue predictions, 
%We will time participants working on issues as they complete contribution milestones, such as selecting an issue, describing the problem, and finding the: component, file, method, and lines to be updated to solve their selected issue. In addition, we will measure the correctness of each participant's solution along with their perception of the issue's complexity, their confidence in solving it, and the skills needed to address the issue. The participants will be divided into two groups, one with the additional API-domain labels on the issues page (treatment group) and the other with only the original project labels present (control group). We will determine the impact of the API domain labels provided by \textit{GiveMeLabeledIssues} by measuring the difference in performance between these two groups.


%BERT was chosen initially for GiveMeLabeledIssues as the fast-bert package \cite{fastbert} is very accommodating for this use case of outputting labels easily from a classifier that is persisted on a server. However, BERT is observed to perform best when trained with datasets that exceed 5000 issues \cite{wang2021well}. Thus, we also currently integrate TF-IDF models to handle this and achieve accurate readings. We allow the user to select their desired classifier if they would like to do so. Otherwise, the default model used is TF-IDF due to its increased performance over BERT in terms of classification metrics. The number of projects will be expanded in the future, with their own specific BERT and TF-IDF models or through the use of one (or potentially multiple) models that employ transfer learning. 

% The final item for future work is to add feedback fields and rating forms to evaluate the usefulness of \textit{GiveMeLabeledIssues}. Users will be able to rate the usefulness of issue domain labels and provide the researchers with free text feedback as well if they desire. Finally, users will also be able to mark certain issues as closed. This will prompt the backend to verify if a currently stored issue is actually open. This will alleviate the need to reclassify daily to refresh the currently open issues for each project. 

\section*{Acknowledgments}

This work is partially supported by the National Science Foundation under Grant numbers 1815503, 1900903, and 2236198. 




%\bibliographystyle{IEEEtranS} 
\bibliography{demo}

\end{document}
