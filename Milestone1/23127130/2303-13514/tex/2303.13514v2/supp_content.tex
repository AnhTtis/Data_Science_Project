

\section{Additional Results}


\noindent{\bf Qualitative Results.} In Fig.~\ref{fig:sup_qual_saor_101_1} and Fig.~\ref{fig:sup_qual_saor_101_2} we present additional qualitative results on various animal categories all generate using our SAOR models that are trained on multiple categories. 
We provide additional results showing full 360-degree predictions for multiple different categories on the project website: \url{mehmetaygun.github.io/saor}. 

\noindent{\bf Part Consistency.} We also compared SAOR's surface estimates with A-CSM~\cite{kulkarni2020acsm} in Fig.~\ref{fig:acsm}. 
Unlike A-CSM, our method does not use any 3D parts or 3D shape priors but is still able to capture finer details like discriminating left and right legs. 
A-CSM groups left and right legs as a single leg while their reference 3D template has left and right legs as a separate entity. 
Moreover, it mixes left-right consistency if the viewpoint changes. 

\noindent{\bf Without Depth.} We also demonstrate examples from a variant of our model that was trained \emph{without} using relative depth map supervision in Fig.~\ref{fig:depth_ablation}. 
We observe that this model is still capable of estimating detailed 3D shapes with accurate viewpoints and similar textures as the full model. 
However, the model trained without depth maps tends to produce wider shapes compared to the full model.
Quantitative results for our model without relative depth are available in Table~2 in the main paper. 


\noindent{\bf Limitations.} We showcase some failure cases of our method in Fig.~\ref{fig:supp_fail}. 
Our method fails when the animal is captured from the back, as there is insufficient data available from that angle in the training sets. 
Note, methods such as~\cite{wu2022magicpony} partially address this by using alternative training data that includes image sequences from video. 
Furthermore, when there is also partial visibility (\eg only the head is visible), our method produces less meaningful results as our architecture does not explicitly model occlusion.


\input{figs/acsm/acsm}
\input{figs/depth_ablation/depth_ablation}
\input{figs/sup_qual_saor_101/sup_qual_101_fig1}
\input{figs/sup_qual_saor_101/sup_qual_101_fig2}
\input{figs/supp_fail/supp_fail}



\noindent{\bf Part Ablations.} We conducted an additional ablation experiment on the number of parts used for horses. 
Results are provided in Table~\ref{tab:pck_part}. Notably, the PCK scores do not significantly vary with different numbers of parts. Therefore, for all other experiments, we used 12 parts.


\begin{table}[h]
    \centering

    \begin{tabular}{l c c c }
    \toprule
        Number of Parts & 6 & 12 & 24  \\ \midrule
        PCK & 43.8 & 44.9 & 44.1  \\ 
        \bottomrule
    \end{tabular}
    
    \vspace{-5pt}
    \caption{Keypoint transfer results on Pascal horses~\cite{everingham2015pascal} where the number of parts are varied.}
    \label{tab:pck_part}
    \vspace{-10pt}
\end{table}





\section{Additional Implementation Details}

\subsection{Data Pre-Processing}

When constructing our training datasets, we run a general-purpose animal  detector~\cite{beery2019efficient} and eliminate objects if any of the following criteria hold: i) the confidence of the detection is less than 0.8, ii) the minimum side of the bounding box is less than 32 pixels, iii) the maximum side of the bounding box is less than 128 pixels, and iv) there is no margin greater than 10 pixels on all sides of the bounding box. 

We then automatically extract segmentation masks using Segment Anything Model~\cite{kirillov2023segany} with the detected bounding box. 
We automatically estimate the relative monocular depth using the transformer-based Midas~\cite{Ranftl2021,Ranftl2022}, using their Large DPT model. 

To obtain cluster centers for the balanced sampling step in Section~3.3 in the main paper, we resize the estimated segmentation masks to $32\times32$, and cluster the 1024-dimensional vectors into 10 clusters using a Gaussian mixture model in all of our experiments. 
Visualization of cluster centers of various animals can be found in Fig.~\ref{fig:clusters}.

\input{figs/clusters/clusters}

\subsection{Architecture}

We use a ResNet-50~\cite{he2016deep} as our image encoder $f_{enc}$ in our CUB\cite{CUB} experiments and the smaller ResNet-18 in quadruped animal experiments. 
This is in contrast to much larger ViT-based backbones used in other work~\cite{wu2022magicpony}. 
We initialize these encoders from scratch, \ie no supervised or self-supervised pre-training is used.  
The architecture details are presented in the following tables: deformation network $f_{d}$ in Table~\ref{tab:arch_def}, articulation network $f_{a}$ in Table~\ref{tab:arch_art}, texture network $f_{t}$ in Table~\ref{tab:arch_text}, and pose network $f_{p}$ in Table~\ref{tab:arch_pose}. 

\begin{table}
    \centering
    \begin{tabular}{l c c c}
    \toprule
          Layer & Input & Output & Dim  \\ \midrule
          Linear (3,512) & $S^{\circ}$  & $l_{x}$ &  $N\times512$\\
          Linear (512,512) & $\phi_{im}$  & $l_{z}$&  $1\times512$ \\
          2 $\times$ Linear (512,128) &  $l_{x} + l_{z}$  & $L$ &  $N\times128$\\
          Linear (128,3) &  $l$  & $D$ &  $N\times3$\\
          \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{Architecture details of our Deformation Net $f_{d}$. 
    } 
    \label{tab:arch_def}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{l c c c}
    \toprule
          Layer & Input & Output & Dim  \\ \midrule
          Linear (3,512) & $S^{\circ}$  & $l_{x}$ &  $N\times512$\\
          Linear (512,512) & $\phi_{im}$  & $l_{z}$&  $1\times512$ \\
          Linear (512,128) &  $l_{x} + l_{z}$  & $L$ &  $N\times128$ \\
          Linear (128,128) &  $L$  & $L$ &  $N\times128$ \\
          Linear (128,K) &  $L$  & $W$ &  $N \times K$ \\
          K $\times$ Linear (512, 9) &  $\phi_{enc}$  & $\bf{\pi}$ &  $K \times 9$ \\
          \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{Architecture details of our Articulation Net $f_{a}$. $K$ is the number of parts and $N$ is the number of vertices, $\bf{\pi}$ is camera parameters. 
    }       
    \label{tab:arch_art}
\end{table}


\begin{table}
    \centering
    \begin{tabular}{l c c c}
    \toprule
          Layer & Input & Output & Dim  \\ \midrule
          Linear (512,512) & $\phi_{im}$  & $L$ &  $512\times1\times1$ \\
          Upsample  & $L$  & $L_{up}$ &  $512\times4\times4$ \\
          Upsample + Conv2D & $L_{up}$ & $L_{up}$  &  $256\times8\times8$ \\
          Upsample + Conv2D &  $L_{up}$ & $L_{up}$  &  $128\times16\times16$ \\
          Upsample + Conv2D &  $L_{up}$ & $L_{up}$  &  $64\times32\times32$ \\
          Upsample + Conv2D &  $L_{up}$ & $L_{up}$  &  $32\times64\times64$ \\
          Upsample + Conv2D &  $L_{up}$ & $L_{up}$  &  $16\times128\times128$ \\
          Conv2D & $L_{up}$ & T &  $3\times128\times128$ \\
          
          \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{Architecture details of our Texture Net $f_{t}$.} 
    \label{tab:arch_text}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{l c c c}
    \toprule
          Layer & Input & Output & Dim  \\ \midrule
          1 $\times$ Linear (512,128) & $\phi_{im}$  & $L$ &  128\\
          C $\times$ Linear (128,6) & $L$  & $\bm{r}_{p},\bm{t}_{p}$ &  128\\
          Linear (128,C) & $L$  & $\bm{\alpha}$ &  128\\
          \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{Architecture details of our Pose Net $f_{p}$. C is the number of cameras, and $\bm{\alpha}$ are the associated scores for each camera~\cite{wu2022magicpony}.} 
    \label{tab:arch_pose}
\end{table}




\subsection{3D Evaluation Details}

For 3D quantitative evaluation, we used the Animal3D dataset~\cite{xu2023animal3d}. The dataset includes pairs of input images with their corresponding 3D models, which are estimated via optimizing the SMAL~\cite{zuffi20173d} model. Moreover, the 3D models are manually verified to eliminate poorly estimated shapes. 
We used the test split of the dataset for the horse, cow, and sheep categories. As there is no global pose alignment between our predictions and the dataset, we run the ICP algorithm to align them. We optimize rotation, $R\in\mathcal{R}^{3}$, translation $T\in\mathcal{R}^{3}$, and global scale $s\in\mathcal{R}^{1}$ with the Adam optimizer~\cite{kingma2014adam} using L1 norm as our alignment objective. We also follow the same alignment steps for the MagicPony~\cite{wu2022magicpony} baseline.



\subsection{Training Losses}
Here we describe the training losses from the main paper in more detail. 
The appearance loss is a combination of an RGB and perceptual loss~\cite{zhang2018unreasonable}.
$\mathcal{L}_{appr} = \lambda_{rgb}\mathcal{L}_{rgb} + \lambda_{percp}\mathcal{L}_{percp}$. These terms are defined below,

\begin{equation}
    \mathcal{L}_{rgb} = || \sum_{i,j} I_{i,j} - \hat{I}_{i,j} ||_{2}, 
\end{equation}

\begin{equation}
    \mathcal{L}_{percp} = || \phi_{p}(I_{i,j}) - \phi_{p}(I_{i,j}) ||_{2},
\end{equation}where $\phi_{p}$ is a function that extracts features from different layers of the VGG-16~\cite{simonyan2014very} network. 

The mask loss is calculated based on the difference between the automatically generated ground truth segmentation mask $M$ and the estimated mask $\hat{M}$ derived from our predicted 3D shape, 
\begin{equation}
    \mathcal{L}_{mask} = \lambda_{mask} \sum_{i,j} ||M_{i,j} - \hat{M}_{i,j}||_{2}.
\end{equation} 
Likewise, the depth loss is computed using the automatically generated relative depth $D$ and the estimated depth $\hat{D}$ from the predicted shape,
\begin{equation}
    \mathcal{L}_{depth} = \lambda_{depth}\sum_{i,j} ||D_{i,j} - \hat{D}_{i,j}||_{2}. 
\end{equation} 

Our swap loss is a combination of the RGB and mask loss between the input image $I$ and swapped image $I^{sw}$, 
\begin{equation}
    \mathcal{L}_{swap} = \lambda_{swap}\left[ \mathcal{L}_{mask}(I, I^{sw}) + \mathcal{L}_{rgb}(I, I^{sw})\right].
\end{equation} 

Finally, we also employ part regularization on the part assignment matrix $W$ to encourage equal-sized parts,
\begin{equation}
    \mathcal{L}_{part} = \lambda_{part}\sum_{k}^{K} \left((\sum_{i}^{N} W_{i,k}) - N/K\right)^2
\end{equation} 
where $N$ is the number of vertices in the mesh and $K$ is the number of parts. 
We also apply 3D regularization on the 3D shape, $\mathcal{L}_{smooth} = \lambda_{smooth}\sum_{}LS $, where $L$ is the laplacian of shape $S$ and $\mathcal{L}_{normal}$ which is defined below,

\begin{equation}
    \mathcal{L}_{normal} = \lambda_{normal} \sum_{\bf{n_{i}},\bf{n_{j}} \in \Omega} 
    1 - \frac{\bf{n_{i}}.\bf{n_{j}}}{||\bf{n_{i}}|| . ||\bf{n_{j}}||}
\end{equation} 
Here, $n_{i}$, $n_{j}$ are normals of neighbor faces. And the smoothness regularization is defined as $\lambda_{smooth}\mathcal{L}_{smooth} = ||LV||$, where $L$ is the Laplacian operator on the vertices. The final regularization term is defined as,
\begin{align}
    \mathcal{L}_{reg} = \lambda_{part} \mathcal{L}_{part} \newline 
    + \lambda_{smooth} \mathcal{L}_{smooth} + \lambda_{normal} \mathcal{L}_{normal}.
\end{align}
We note the weights used in our experiments for each loss in Table~\ref{tab:hyperparams}.

\subsection{Training}
 
In our experiments, we trained two different models: SAOR-101 and SAOR-Birds. The bird model is trained from scratch on CUB~\cite{CUB} for 500 epochs. In the first 100 epochs we only learn deformation, and then enable articulation afterwards.

The SAOR-101 model is trained in two steps. We first train the model using only Horse data from LSUN~\cite{yu2015lsun} then finetune it on all 101 animal categories downloaded from the iNaturalist website~\cite{iNatWeb}. In a similar fashion to the SAOR-Birds model,  we only learn deformation in the first 100 epochs, then allow articulation for about 300 epochs on horse data. Finally, fine-tune the model on all categories on iNaturalist data for 150 epochs. 
We utilize Adam~\cite{kingma2014adam} with a fixed learning rate for optimizing our networks. 
We note the hyperparameters used in Table~\ref{tab:hyperparams}.

Our simplified swap loss leads to easy hyper-parameter selection compared to Unicorn~\cite{monnier2022share}. 
For instance, in their swap loss term, the following parameters need to be decided: i) feature bank size, ii) minimum and maximum viewpoint difference, and iii) number of bins to divide samples in the feature bank depending on the viewpoint. 
Moreover, they need to do multistage training where they increase the latent dimensions for the shape and texture codes to obtain similar shapes during training. 
Here the number of stages and the dimension of latent codes in each stage are also hyperparameters. 
In our method, we eliminated all of these hyperparameters. 
Moreover, as we do not use all of the hypotheses cameras to estimate loss during a forward pass as in~\cite{wu2022magicpony} and as a result of our simplified swap loss, model training is six times faster than Unicorn, as they use six cameras during training, for the same number of epochs.



\begin{table}[h]
    \centering
      \resizebox{0.75\columnwidth}{!}{
    \begin{tabular}{l c}
    \toprule
          {\bf Parameter} & {\bf Value/Range}  \\ \midrule
          \textbf{Optimization} & \\ 
          Optimizer & Adam \\
          Learning Rate & 1e-4 \\ 
          Batch Size & 96 \\
          Epochs & 500 \\
          Image Size & 128 $\times$ 128 \\ \midrule
          \textbf{Mesh} & \\ 
          Number of Vertices & 2562 \\
          Number of Faces & 5120 \\
          UV Image Size & 64 $\times$ 128 $\times$ 3 \\ 
          Number of Parts & 12 \\
          Initial Position & (0,0,0) \\ \midrule
          \textbf{Camera} &  \\
          Translation Range & (-0.5, 0.5) \\
          Azim Range & (-180,180) \\
          Elev Range & (-15, 30) \\
          Roll Range & (-30, 30) \\ 
          FOV & 30 \\
          Number of Cameras & 4 \\ \midrule
          \textbf{Loss Weights} & \\
          $\lambda_{rgb}$ & 1 \\
          $\lambda_{percp}$ & 10 \\
          $\lambda_{mask}$ & 1 \\
          $\lambda_{depth}$ & 1 \\
          $\lambda_{swap}$ & 1 \\
          $\lambda_{smooth}$ & 0.1 \\
          $\lambda_{normal}$ & 0.1 \\
          $\lambda_{part}$ & 1 \\
          $\lambda_{pose}$ & 0.05 \\
          \bottomrule
    \end{tabular}
    }
    \vspace{-5pt}
    \caption{Training hyperparameters. }  
    \vspace{-15pt}
    \label{tab:hyperparams}
\end{table}