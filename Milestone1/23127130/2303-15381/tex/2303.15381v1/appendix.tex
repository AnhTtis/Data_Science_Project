\onecolumn

\section{Appendix}
\label{sec:appendix}

\subsection{Limitations}
\label{ssec:limitations}

\textbf{Limited scales of causation}. We rely on a limited number and diversity of viewpoints for scales of causation (eight annotators at undergraduate and graduate levels at U.S. institutions of higher learning; four female, four male; five native, three non-native English speakers).

\textbf{Size of dataset}. In Table \ref{tab:datasets}, we see that \textsc{Torquestra} is slightly larger than other existing human-curated temporal and event structure reasoning datasets. Still, the question remains, how large must the corpus be to be able to enable successful learning of salient discourse-level explicit and implicit causal relations? It might be significant.  

\textbf{More and more varied test data}. A collection of 3k Wikipedia articles in a single language is a relatively small sample for testing. We note the document label class imbalance: about 33\% concern historical military conflicts. Also, labeled event types are noisy (e.g., ``The area was \underline{hit} by a storm'' is labeled \textsc{Violence-Attack}). Topic labels were likely originally assigned using similar methods as our own (tf-idf).

\textbf{Schema indeterminacy}. For better schema matching models, it is possibly although not necessarily imperative to define in clear terms what a ``schema'' is. A ``better'' schema model is likely hierarchical in terms of event semantics, although the compositional nature of events is still indeterminate, and thus also generic events (schemas). Different manners of defining schemas are possible, some of which we explore in this work: as single event types, as sequences of event types (e.g., in a causal chain), as unordered sets of event types, as causal graphs, as having predicate argument structure, etc.

\textbf{Causal relations and other possible experiments}. We note that we have not reported on experiments with the causal subrelations shown in Table \ref{tab:relations}, work we leave for the future. We also have not reported on experiments ranking causal chain salience, nor on generation results directly conditioning on $S_{\textit{causal}}$ (limiting the use of causal schema graphs to matching experiments), a promising research direction. Data for each of these proposed experiments is available as part of our release.


 \textbf{Role of structured representations in NLP}. The role of structured representations in NLP is  debatable in the deep learning era (the importance of dependency parsing, syntactic treebanks, semantic role labeling, etc.). One concern is that such human annotation is expensive. Another concern is that in many cases linguistic structure can be preempted, using raw text only for both inputs and outputs.  
 
 To add to this debate, the success of methods in causal inference is largely based on the use of graphical structures, though there has not been yet a direct link made between how to automatically acquire these structures from language text using NLP methods, a concern we hope the present work helps address.


\pagebreak

\subsection{Describing Torquestra}
\label{ssec:describing-torquestra}

\begin{figure*}[htbp]
\begin{minipage}{.48\linewidth}
\captionsetup{}
  {\includegraphics[width=215pt]{figures/torquestra-content}}
  \caption{\label{fig:torquestra-content}Count of texts for different slices of \textsc{Torquestra} by text mean length  (\#subword tokens). Existing human-engineered temporal and event structures  (in boxes with arrows) supplement our causal structures (see Fig. \ref{fig:torquestra-graphs} on right).}
\end{minipage}
\hspace{0.02\linewidth}
\begin{minipage}{.48\linewidth}
\centering
\captionsetup{}
 {\includegraphics[width=195pt,trim={1.2cm 0.6cm 1.2cm 1.2cm},clip]{figures/torquestra_graph_attributes.png}}
  \caption{\label{fig:torquestra-graphs}Complexity of human-engineered causal graphs in \textsc{Torquestra}. The \textsc{Torque} data slice (purple, leftmost) consists of  less complex causal graphs (based on number of nodes and edges) compared to \textsc{Crime} (green, rightmost).}
\end{minipage}
\end{figure*}

\textbf{Text and graph statistics}

\begin{table*}[htbp]
\centering
\small
\begin{tabular}{llllll}
\toprule
\scriptsize\textbf{Data attribute} & \scriptsize{$G_{\textit{temp-madaan}}$} & \scriptsize{$G_{\textit{temp-torque}}$} & \scriptsize{$G_{\textit{causal-torque}}$} & \scriptsize{$G_{\textit{causal-ester}}$} & \scriptsize{$G_{\textit{causal-crime}}$} \\
\midrule
\small
avg \# tokens/text &	{120.3} & {46.8}    &	{46.8}  &	 {133.4}  &	{231.3}   \\
\midrule
max \# nodes &	{} & {22.0}    &	{17.0}  &	   {19.0}  &	{36.0}   \\
mean \# nodes & {3.3} & {8.3}   &	{5.81} &	{11.6}  &	{19.5}   \\
std \# nodes &	{} & {3.2} &	{2.3} &	{3.27}  &	{5.93}   \\
\midrule
max \# edges &	{} & {102.0} &	{21.0}      &	{24.0}  &	{48.0}  \\
mean \# edges & {3.9} & {20.9} &	{5.2}    &	{13.1}  &	{23.7}  \\
std \# edges &  {} & {13.4} &	{2.9}   &	{4.3}  &	{8.6} \\
\midrule
mean \# enables & {} & {} &	{4.3}      &	{11.5}  &	{21.1} \\
mean \# blocks 	& {} & {} &	{0.9}      &	{1.6}  &	{2.6}  \\
\midrule
mean degree     &  {}   & {} &   {1.7}  &	{2.3}  &	{2.4}   \\
mean clustering & {}    & {} &	{0.04}   &	{0.08}  &	{0.07}   \\
mean transitivity & {}  & {} &	{0.05}   &	{0.09}  &	{0.07}   \\
mean sq clustering & {} & {} &	{0.02}   &	{0.05}  &	{0.03}  \\
\bottomrule
\end{tabular}
\caption{\label{tab:torquestra-statistics}
Comparison of data statistics from two temporal structure resources: $G_{\textit{temp-madaan}}$ \citep{Madaan2021a} and $G_{\textit{temp-torque}}$ \citep{Ning2020} with various slices of \textsc{Torquestra}: $G_{\textit{causal-torque}}$, $G_{\textit{causal-ester}}$ and $G_{\textit{causal-crime}}$. One important comparison here is between the number of edges between $G_{\textit{temp}}$ (mean 20.9) versus $G_{\textit{causal-torque}}$ (mean 5.2), which we could use to argue that causal structures provide a cleaner starting point for reasoning than temporal structures.
}
\end{table*}


\pagebreak

\subsection{Analysis of events in \textsc{Torquestra}}
\label{ssec:appendix-events}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.95\textwidth]{figures/torquestra-count-high-events}
\caption{\label{fig:high-level}Count of high-level hierarchical event semantic types observed in \textsc{Torquestra} texts.}
\end{figure*}


\begin{figure*}[h!]
\centering
\includegraphics[width=0.95\textwidth]{figures/torquestra-count-mid-events}
\caption{\label{fig:mid-level}Count of mid-level hierarchical event semantic types observed in \textsc{Torquestra} texts.}
\end{figure*}


\pagebreak


\subsection{Annotation}
\label{ssec:appendix-annotation}

\textbf{General annotation guidelines}. We refer to our approach of causal modeling as `participant-centered causal structure' (\S\ref{sec:definitions}). In this approach, nodes in causal graphs can be events expressed as sentences or phrases of Subject-Verb-Object form (e.g., `Alice wrote a paper'). A range of other event descriptions are possible (regarding e.g., nominalizations, modality, polarity). Nodes can also be event participants such as people and objects that directly contribute to, initiate, or disrupt the beginning, unfolding, or ending of events. `Alice made a cake' may be thus `Alice' (as causal agent) $\rightarrow$ `Alice made a cake'.

Annotators were asked to keep clearly in mind that one is not trying to model true causation, but just the apparent causation within the belief system of the speaker. Thus, causal links roughly mean ``in the opinion of the speaker there should be a causal relation between A and B'' â€” whether or not the actual causal relation (Table \ref{tab:relations}) is explicitly stated or implied somehow. This guideline helps remove the problem of reasoning about the world and allows one to frame debugging questions in terms of what the speaker might say and do, things that are much easier to discuss and evaluate. 

\textbf{Annotation quality}. Consistent labeling of graph nodes proved challenging. For free-form node labeling, annotators were asked to compose a short sentence or phrase of form \textit{Subject-Verb-Object}, e.g. `Police arrested the suspect', or `suspect arrested'. For evaluation, we selected 50 texts, asked two annotators to write short texts for nodes, counting the number that shared at least one content word. For the 325 nodes evaluated, 56\% shared 1+ tokens referring to an event or other participant. 


\textbf{Notes about salient causal chain identification}. Event salience detection is often framed as identifying the single most reportable event in an event sequence \citep{Ouyang2015}. We extend this task to be: Given a set of triples, identify a subset of triples that tell the central part of the causal story. For evaluation, dual annotations for 50 causal graphs show agreement of $\kappa=0.61$.


\textbf{Notes about data evaluation: causal relations}. For evaluation of human and machine generated causal graphs, annotators are given a text, two participants (\textbf{A} and \textbf{B}) in a directed relation, and three choices: A makes B more likely (\textsc{Enables}), less likely (\textsc{Blocks}) or has no effect, see Fig. \ref{ex:rebels-eval}. For 150 edges judged by three evaluators, average pairwise Cohen's kappa is $\kappa=0.53$ (moderate agreement). 

\begin{figure}[h!]
\centering\includegraphics[width=0.45\textwidth]{figures/rebels-eval.png}
    \caption{To evaluate a causal relation given a text and triple (rows), we used intuitive causal notions such `A makes B more likely' in place of directly assessing an \textsc{Enables} edge.}
    \label{ex:rebels-eval}
\end{figure}


 

\subsection{Notes about data evaluation: causal graph completeness}

We concede that causal graphs are difficult to fully specify for various reasons. First, causal expectations do not hold in certain contexts, implicit contributory factors related to events are typically beyond enumeration, and causal relations for events with negative polarity are easy to overlook.

\textbf{Causal expectations} for events that fail in a given context are difficult to agree on, e.g., subrelations \textsc{Without effect} and \textsc{Unknown}, $\approx$15\% of relations in \textsc{Torquestra}. In more than half of observed cases, explicit lexical cues (e.g., ``despite'' in ``The protest happened despite the rain.'') are not present.

\textbf{Implicit contributory factors} are numerous in certain kinds of text. In our case, newswire is necessarily succinct leaving a lot unsaid. Commonsense often suggests causal nodes of the kind ``and etc.,'' perhaps shorthand for ``there are many other potential contributory factors that I cannot all list, but they fit here.'' We leave the collection of this type of data for future work. 

\textbf{Notes about event polarity}. Negative event polarity is a challenge for representation. There may be a reporting bias, i.e. perhaps people less frequently speak about the causal effects of negative polarity events, e.g., `It didn't rain, so I didn't play in the mud'. Some of these semantics are captured in the \textsc{Without effects} causal subrelation. Annotators were instructed to look out for causal relations not covered by our ontology and to construct nodes including negative polarity, when possible. Additional thought and corpus studies may point to additional layers of meaning distinctions possible.


\subsection{Prompt engineering}
\label{ssec:prompts}

We construct various prompts for conditional language model generation. Using the pre-existing temporal and event structure annotations, prompts can be made concatenating raw texts with associated questions and answers about temporal and event structures. We also experiment by adding knowledge of hierarchical event types directly into texts, using dense paraphrasing \citep{Tu2022}. 


\textbf{Example prompt: text + G$_{\textit{temp}}$}. For prompts for text + G$_{temp}$, we randomly selected 2-3 temporal \textsc{Torque} questions and answers \citep{Ning2020} in the form of lists of event mentions, filtering out verbs with less eventive meaning (e.g., `was') and appended to the source text.

\begin{small}
\begin{verbatim}
TEXT: And on that basis Keating was released from prison before he was eligible for parole. 
Now the ninth US circuit court of appeals has ruled that the original appeal was flawed 
since it brought up issues that had not been raised before.\n
What event has already finished? released, ruled, brought, flawed\n
What happened before the court ruled? flawed, brought, raised, released\n
What did not happen before Keating was released? parole\n
[GEN]
\end{verbatim}
\end{small}

\textbf{Example prompt: text + V$_{\textit{event}}$} (cf. dense paraphrasing). We also add structure to input text prepending event types to event and entity mentions. We wonder: Does it \textit{help} or \textit{hinder} generation by: 1) indicating which tokens are likely associated with causal chains? and, 2) giving an explicit signal of hierarchical event semantics? We also include event types for entities and events NOT part of causal chains (to aid in learning). Results are split on this question: in some cases, it helps, in other cases, it hinders (\S\ref{sec:results}).

\begin{small}
\begin{verbatim}
TEXT: And on that basis Entity::Keating was Legal_rulings::released from Entity::prison 
before he was Scenario::eligible for parole. 
Now the Entity::ninth US circuit court of appeals has Legal_rulings::ruled that 
the original Legal_rulings::appeal was Incident::flawed 
since it brought up Scenario::issues that had not been raised before.\n
[GEN]
\end{verbatim}
\end{small}


\subsection{Model specifics}

We implemented graph generation models in PyTorch using the huggingface library. Without a limited hyperparameter search, we note generation models worked well with the following: block size between 300-600, training 12-16 epochs, learning rate=$3e^{-5}$, cosine learning rate scheduler, and Adam optimizer. For generation, we use nucleus sampling \citep{Holtzman2019} setting $p=0.90$ and temperature$=0.95$. 

To embed causal graphs, we implement a self-supervised Graph Attention Network (GAT) \citep{Velickovic2018} using PyTorch Geometric\footnote{\url{https://pytorch-geometric.readthedocs.io/en/latest/}}. Our base model uses two GAT layers with ELU activations, dropout=$0.5$, global mean pooling and a final linear layer. We trained for 2-3 epochs over the entire dataset (\textsc{Torquestra} + test data). Using the trained model, we make final embeddings for clustering experiments (test set alone); while for matching we compare the test set with \textsc{Torquestra} graphs and RESIN subgraphs.

\textbf{Masked objective function}. For the self-supervised graph modeling, we train a graph neural network using message passing to compose graph embeddings, randomly masking input nodes. In training, we hypothesize that more important nodes will contribute more to a final graph embedding, and experiment  masking nodes based on PageRank weighting\footnote{Using the NetworkX Python package, \url{https://networkx.org/}}.

\textbf{Loss function}. At each step of training, the model `sees' 5 of 10 randomly sampled masked graphs, and predicts a graph embedding. We compare the similarity of the predicted embedding for the masked graph, $y*$, with the embedding for the entire ground truth graph, $y$ and train the model to minimize an absolute value cosine similarity difference loss:
\begin{equation}
\label{eq:loss}
\mathcal{L}_{CosineSimDiff} = |1 - CosineSimilarity(y*, y)|
\end{equation}
\noindent
Cosine similarity, the scaled dot product of non-zero vectors \textbf{A} and \textbf{B}, is:

\begin{equation}
CosineSimilarity(\textbf{A}, \textbf{B}) = \frac{\textbf{A}\cdot\textbf{B}}{max(\norm{\textbf{A}}_2\norm{\textbf{B}}_2, \epsilon)} 
\end{equation}
\noindent
with $\epsilon=1e^{-8}$ to prevent division by zero. For vectors that are equal, the cosine similarity is 1, 0 when orthogonal, and -1 when opposite, so the absolute value cosine similarity difference loss (Eq. \ref{eq:loss}) approaches zero as the embedding similarity for masked graph and non-masked graph increases. In our experiments, we mostly observed loss to decrease from 1 to about 0.6 in training and validation. 


\raggedbottom
\subsection{Metrics}
\label{ssec:metrics}

For our experiments, we report a mixture of quantitative and qualitative metrics based on 2-3 human judgments. However, ground truth labels for nodes and schemas present a number of tricky issues (see Appendix \ref{ssec:limitations}). This is apparent, for example, in judgments of generated graph correctness and also in the evaluation of schema matching models. Defining schemas as a single label is likely too simple, a situation that using a set of labels does not necessarily improve.

For schema matching, we report mean average precision (MAP), measured between 0 and 1 with higher being better. For Q queries, this is defined as:
\begin{equation}
MAP = \frac{1}{Q} \sum_{q=1}^{Q} \text{avg.  precision}(q)
\end{equation}

with precision for a query $q$ defined as:
\begin{equation}
\text{avg. precision}(q) = \frac{\abs{\{\text{relevant documents}\}_q\cap{\{\text{retrieved documents}\}_q}}} {\abs{\{\text{retrieved documents}\}_q}}
\end{equation}

We consider different quantitative and qualitative judgments for relevance, including: precision of topic label, lexical overlap and structural similarity, where structural similarity can be defined in terms of shared nodes, node degree, n-nary properties, and other observable graph properties. See a cluster example sharing structural similarity in Appendix \ref{ssec:cluster}. 


\pagebreak
\subsection{Data sample}

Training examples are shorter texts from TORQUE (with temporal questions); Validation are longer texts from ESTER (with event structure questions) that \textbf{includes} the shorter TORQUE text, in \textbf{bold}.

\begin{scriptsize}
\begin{lstlisting}[escapechar=\%]
{`split': `train', 
`source': `torque', 
`@id': `train-docid_PRI19980115.2000.0186_sentid_6',
`notes': `original-698', 
`text': `%\textbf{And on that basis Keating was released from prison before he was eligible for parole. Now the ninth US circuit court of appeals has ruled \\ \indent that the original appeal was flawed since it brought up issues that had not been raised before.}%', 
`questions': 
    [`What event has already finished?',
    `What event has begun but has not finished?',  
    `What will happen in the future?', 
    `What happened after Keating was released?', 
    `What did not happen before Keating was released?', 
    `What happened before the court ruled?', 
    `What did not happen after Keating was released?', 
    `What happened after the court ruled?'], 
`answers': 
    [[`released', `ruled', `brought', `flawed'], [], [], 
    [`ruled', `flawed', `brought', `raised'], 
    [`was', `parole'], 
    [`flawed', `brought', `raised', `released'], [], []], 
`event_types': 
    {`Keating was released': `Action;Legality;Legal_rulings;Releasing', 
    `Keating was eligible for parole': `Action;Legality;Legal_rulings', 
    `issues that had not been raised before': `Scenario', 
    `court ruled that the original appeal was flawed': `Action;Legality;Legal_rulings'}, 
`noncausal_event_types': 
    {`brought': 'Action;Communication;Reporting'}, 
`causal_graph': [
    {`head': 'court ruled that the original appeal was flawed', `rel': `ENABLES', 
    `tail': 'Keating was released'}, 
    {`head': 'Keating was eligible for parole', `rel': 'BLOCKS', 
    `tail': 'Keating was released'}, 
    {`head': 'issues that had not been raised before', `rel': `ENABLES', 
    `tail': 'ruled that the original appeal was flawed'}]}

{`split': `dev', 
`source': `ester', 
`@id': `dev-docid_PRI19980115.2000.0186_sentid_6', 
`notes': `original-698', 
`text': "Former savings and loan chief, Charles Keating, is facing more legal troubles in California. 
    A federal appeals court has reinstated his state convictions for securities fraud. 
    NPR's Elaine Corey has more from San Francisco. In nineteen ninety-one Charles Keating was convicted 
    in state court of helping to defraud thousands of investors who bought high risk junk bonds 
    sold by Keating's employees at Lincoln savings and loan. The bonds became worthless when the bankrupt thrift 
    was seized by government regulators. Keating's convictions were thrown out in nineteen ninety-six on a technicality. 
    %\textbf{And on that basis Keating was released from prison before he was eligible for parole. Now the ninth US circuit court of appeals has ruled \\ \indent that the original appeal was flawed since it brought up issues that had not been raised before.}%
    That means the convictions stand, a ruling likely to send Keating's lawyers 
    back to state court where they must start over with a new appeal.", 
`questions': 
    [`Why was Mr. Keating convicted?', `Why was Mr. Keating released from prison?', 
    `What might happen as a result of the convictions being ruled to stand after the flawed appeal?'], 
`answers':
    [`defraud thousands of investors', 
    `convictions were thrown out in nineteen ninety-six on a technicality', 
    "send Keating's lawyers back to state court"], 
`event_types': `banking;crime;action;legality;legal_rulings', 
`causal_graph': 
    [{`head': `Entity::Charles Keating', `rel': `ENABLES', 
    `tail': `Keating faces legal troubles', `saliency': 0}, 
    {`head': `Entity::Charles Keating', `rel': `ENABLES', 
    `tail': `security fraud', `saliency': 0}, 
    {`head': `security fraud', `rel': `ENABLES', 
    `tail': `Keating is convicted of security fraud', `saliency': 1}, 
    {`head': "Keating's convictions dismissed", `rel': 'BLOCKS', 
    `tail': `Keating is convicted of security fraud', `saliency': 1}, 
    {`head': "Keating's convictions dismissed", `rel': `ENABLES',
    `tail': `Keating was released from prison', `saliency': 1}, 
    {`head': `technicality', `rel': `ENABLES', 
    `tail': "Keating's convictions dismissed", `saliency': 0}, 
    {`head': `bonds became worthless', `rel': `ENABLES', 
    `tail': `security fraud', `saliency': 0}, 
    {`head': `bankrupt thrift was seized', `rel': `ENABLES', 
    `tail': `bonds became worthless', `saliency': 0}, 
    {`head': `Entity:ninth US circuit court of appeals', `rel': `ENABLES', 
    `tail': "court reinstated Keating's state convictions for securities fraud", `saliency': 1}, 
    {`head': "court reinstated Keating's state convictions for securities fraud", `rel': `ENABLES', 
    `tail': `Keating faces legal troubles', `saliency': 1}, 
    {`head': "Keating's convictions dismissed", `rel': `ENABLES', 
    `tail': "court reinstated Keating's state convictions for securities fraud",  `saliency': 1}]}
\end{lstlisting}
\end{scriptsize}


\pagebreak

\section{Cluster example}
\label{ssec:cluster}

\begin{figure*}[h]
\centering
\includegraphics[width=0.70\textwidth]{figures/proto-graph.png}
\caption{\label{fig:cluster1} Example prototypical graph (cluster centroid) in an early experiment clustering \textsc{Torquestra}$_{\textit{human}}$ causal graphs.}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.80\textwidth]{figures/similar-graphs.png}
\caption{\label{fig:cluster2}Similar graphs from a cluster. The GNN matching algorithm is effective in identifying similar graph structures as well as those that share lexico-semantic similarity, cf. \textit{motif discovery} \citep{Milo2002}.}
\end{figure*}





