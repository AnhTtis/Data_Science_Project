% Please download the latest anthology.bib from
%
% http://aclweb.org/anthology/anthology.bib.gz

@article{Ammanabrolu2020,
  title={Automated Storytelling via Causal, Commonsense Plot Ordering},
  author={Ammanabrolu, Prithviraj and Cheung, Wesley and Broniec, William and Riedl, Mark O.},
  journal={CoRR},
  year={2020},
  url={https://arxiv.org/abs/2009.00829},
  volume={abs/2009.00829}
}


@inproceedings{Banerjee2005,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W05-0909",
    pages = "65--72",
}


@article{Berant2014,
abstract = {Machine reading calls for programs that read and understand text, but most current work only attempts to extract facts from redundant web-scale corpora. In this paper, we focus on a new reading comprehension task that requires complex reasoning over a single document. The input is a paragraph describing a biological process, and the goal is to answer questions that require an understanding of the relations between entities and events in the process. To answer the questions, we first predict a rich structure representing the process in the paragraph. Then, we map the question to a formal query, which is executed against the predicted structure. We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations.},
archivePrefix = {arXiv},
arxivId = {1901.03438},
author = {Berant, Jonathan and Srikumar, Vivek and Chen, Pei-Chun and Huang, Brad and Manning, Christopher D. and {Vander Linden}, Abby and Harding, Brittany and Clark, Peter},
eprint = {1901.03438},
journal = {EMNLP},
title = {{Modeling biological processes for reading comprehension}},
url = {http://arxiv.org/abs/1901.03438},
year = {2014}
}

@inproceedings{Bethard2008,
    title = "Building a Corpus of Temporal-Causal Structure",
    author = "Bethard, Steven  and
      Corvey, William  and
      Klingenstein, Sara  and
      Martin, James H.",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/229_paper.pdf",
    abstract = "While recent corpus annotation efforts cover a wide variety of semantic structures, work on temporal and causal relations is still in its early stages. Annotation efforts have typically considered either temporal relations or causal relations, but not both, and no corpora currently exist that allow the relation between temporals and causals to be examined empirically. We have annotated a corpus of 1000 event pairs for both temporal and causal relations, focusing on a relatively frequent construction in which the events are conjoined by the word and. Temporal relations were annotated using an extension of the BEFORE and AFTER scheme used in the TempEval competition, and causal relations were annotated using a scheme based on connective phrases like and as a result. The annotators achieved 81.2{\%} agreement on temporal relations and 77.8{\%} agreement on causal relations. Analysis of the resulting corpus revealed some interesting findings, for example, that over 30{\%} of CAUSAL relations do not have an underlying BEFORE relation. The corpus was also explored using machine learning methods, and while model performance exceeded all baselines, the results suggested that simple grammatical cues may be insufficient for identifying the more difficult temporal and causal relations.",
}


@inproceedings{Bonial2020,
    title = "{I}nfo{F}orager: Leveraging Semantic Search with {AMR} for {COVID}-19 Research",
    author = "Bonial, Claire  and
      Lukin, Stephanie M.  and
      Doughty, David  and
      Hill, Steven  and
      Voss, Clare",
    booktitle = "Proceedings of the Second International Workshop on Designing Meaning Representations",
    month = dec,
    year = "2020",
    address = "Barcelona Spain (online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.dmr-1.7",
    pages = "67--77",
    abstract = "This paper examines how Abstract Meaning Representation (AMR) can be utilized for finding answers to research questions in medical scientific documents, in particular, to advance the study of UV (ultraviolet) inactivation of the novel coronavirus that causes the disease COVID-19. We describe the development of a proof-of-concept prototype tool, InfoForager, which uses AMR to conduct a semantic search, targeting the meaning of the user question, and matching this to sentences in medical documents that may contain information to answer that question. This work was conducted as a sprint over a period of six weeks, and reveals both promising results and challenges in reducing the user search time relating to COVID-19 research, and in general, domain adaption of AMR for this task.",
}


@inproceedings{Brahman2021,
    title = "{``}Let Your Characters Tell Their Story{''}: A Dataset for Character-Centric Narrative Understanding",
    author = "Brahman, Faeze  and
      Huang, Meng  and
      Tafjord, Oyvind  and
      Zhao, Chao  and
      Sachan, Mrinmaya  and
      Chaturvedi, Snigdha",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.150",
    doi = "10.18653/v1/2021.findings-emnlp.150",
    pages = "1734--1752",
    abstract = "When reading a literary piece, readers often make inferences about various characters{'} roles, personalities, relationships, intents, actions, etc. While humans can readily draw upon their past experiences to build such a character-centric view of the narrative, understanding characters in narratives can be a challenging task for machines. To encourage research in this field of character-centric narrative understanding, we present LiSCU {--} a new dataset of literary pieces and their summaries paired with descriptions of characters that appear in them. We also introduce two new tasks on LiSCU: Character Identification and Character Description Generation. Our experiments with several pre-trained language models adapted for these tasks demonstrate that there is a need for better models of narrative comprehension.",
}


@article{Brown2020,
abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
archivePrefix = {arXiv},
arxivId = {2005.14165},
author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
eprint = {2005.14165},
journal = {NeurIPS},
title = {{Language models are few-shot learners}},
year = {2020}
}

@inproceedings{Chambers2008,
    title = "Unsupervised Learning of Narrative Event Chains",
    author = "Chambers, Nathanael  and
      Jurafsky, Dan",
    booktitle = "Proceedings of ACL-08: HLT",
    month = jun,
    year = "2008",
    address = "Columbus, Ohio",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P08-1090",
    pages = "789--797",
}

@article{Caselli2016,
   author = {Tommaso Caselli and Piek Vossen},
   doi = {10.18653/v1/w16-5708},
   issue = {2015},
   journal = {Proceedings of the 2nd Workshop on Computing News Storylines, ACL},
   pages = {67-72},
   title = {The Storyline Annotation and Representation Scheme (StaR): A Proposal},
   year = {2016},
}

@inproceedings{Chambers2013,
    title = "Event Schema Induction with a Probabilistic Entity-Driven Model",
    author = "Chambers, Nathanael",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1185",
    pages = "1797--1807",
}



@article{Chambers2014,
    title = "Dense Event Ordering with a Multi-Pass Architecture",
    author = "Chambers, Nathanael  and
      Cassidy, Taylor  and
      McDowell, Bill  and
      Bethard, Steven",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "2",
    year = "2014",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    pages = "273--284",
    abstract = "The past 10 years of event ordering research has focused on learning partial orderings over document events and time expressions. The most popular corpus, the TimeBank, contains a small subset of the possible ordering graph. Many evaluations follow suit by only testing certain pairs of events (e.g., only main verbs of neighboring sentences). This has led most research to focus on specific learners for partial labelings. This paper attempts to nudge the discussion from identifying some relations to all relations. We present new experiments on strongly connected event graphs that contain ∼10 times more relations per document than the TimeBank. We also describe a shift away from the single learner to a sieve-based architecture that naturally blends multiple learners into a precision-ranked cascade of sieves. Each sieve adds labels to the event graph one at a time, and earlier sieves inform later ones through transitive closure. This paper thus describes innovations in both approach and task. We experiment on the densest event graphs to date and show a 14{\%} gain over state-of-the-art.",
}

@article{Chaturvedi2017, 
    title={Unsupervised Learning of Evolving Relationships Between Literary Characters}, volume={31}, 
    abstractNote={Understanding inter-character relationships is fundamental for understanding character intentions and goals in a narrative. This paper addresses unsupervised modeling of relationships between characters. We model relationships as dynamic phenomenon, represented as evolving sequences of latent states empirically learned from data. Unlike most previous work our approach is completely unsupervised. This enables data-driven inference of inter-character relationship types beyond simple sentiment polarities, by incorporating lexical and semantic representations, and leveraging large quantities of raw text. We present three models based on rich sets of linguistic features that capture various cues about relationships. We compare these models with existing techniques and also demonstrate that relationship categories learned by our model are semantically coherent.}, number={1}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Chaturvedi, Snigdha and Iyyer, Mohit and Daume III, Hal}, year={2017}, month={Feb.} }


@inproceedings{Chen2022,
    title = "Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?",
    author = "Chen, Xilun  and
      Lakhotia, Kushal  and
      Oguz, Barlas  and
      Gupta, Anchit  and
      Lewis, Patrick  and
      Peshterliev, Stan  and
      Mehdad, Yashar  and
      Gupta, Sonal  and
      Yih, Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.19",
    pages = "250--262",
    abstract = "Despite their recent popularity and well-known advantages, dense retrievers still lag behind sparse methods such as BM25 in their ability to reliably match salient phrases and rare entities in the query and to generalize to out-of-domain data. It has been argued that this is an inherent limitation of dense models. We rebut this claim by introducing the Salient Phrase Aware Retriever (SPAR), a dense retriever with the lexical matching capacity of a sparse model. We show that a dense Lexical Model Λ can be trained to imitate a sparse one, and SPAR is built by augmenting a standard dense retriever with Λ. Empirically, SPAR shows superior performance on a range of tasks including five question answering datasets, MS MARCO passage retrieval, as well as the EntityQuestions and BEIR benchmarks for out-of-domain evaluation, exceeding the performance of state-of-the-art dense and sparse retrievers. The code and models of SPAR are available at: https://github.com/facebookresearch/dpr-scale/tree/main/spar",
}


@article{Croft2017,
author = {William Croft and Pavlina Peskova and Michael Regan},
title = {Integrating decompositional event structure into storylines},
journal = {Events and Stories in the News Workshop, Association for Computational Linguistics},
year = {2017}
}

@inproceedings{Do2011,
    title = "Minimally Supervised Event Causality Identification",
    author = "Do, Quang  and
      Chan, Yee Seng  and
      Roth, Dan",
    booktitle = "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland, UK.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D11-1027",
    pages = "294--303",
}

@inproceedings{Donatelli2018,
    title = "Annotation of Tense and Aspect Semantics for Sentential {AMR}",
    author = "Donatelli, Lucia  and
      Regan, Michael  and
      Croft, William  and
      Schneider, Nathan",
    booktitle = "Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-4912",
    pages = "96--108",
    abstract = "Although English grammar encodes a number of semantic contrasts with tense and aspect marking, these semantics are currently ignored by Abstract Meaning Representation (AMR) annotations. This paper extends sentence-level AMR to include a coarse-grained treatment of tense and aspect semantics. The proposed framework augments the representation of finite predications to include a four-way temporal distinction (event time before, up to, at, or after speech time) and several aspectual distinctions (including static vs. dynamic, habitual vs. episodic, and telic vs. atelic). This will enable AMR to be used for NLP tasks and applications that require sophisticated reasoning about time and event structure.",
}



@inproceedings{Du2022,
   abstract = {We introduce RESIN-11, a new schema-guided event extraction and prediction system that can be applied to a large variety of newsworthy scenarios. The framework consists of two parts: (1) an open-domain end-to-end multimedia multilingual information extraction system with weak-supervision and zero-shot learning-based techniques. (2) a schema matching and schema-guided event prediction system based on our curated schema library. We build a demo website 1 based on our dockerized system and schema library publicly available for installation 2. We also include a video demonstrating the system. 3},
   author = {Xinya Du and Zixuan Zhang and Sha Li and Ziqi Wang and Pengfei Yu and Hongwei Wang and Tuan Manh Lai and Xudong Lin and Iris Liu and Ben Zhou and Haoyang Wen and Manling Li and Darryl Hannan and Jie Lei and Hyounghun Kim and Rotem Dror and Haoyu Wang and Michael Regan and Qi Zeng and Qing Lyu and Charles Yu and Carl Edwards and Xiaomeng Jin and Yizhu Jiao and Ghazaleh Kazeminejad and Zhenhailong Wang and Chris Callison-Burch and Carl Vondrick and Mohit Bansal and Dan Roth and Jiawei Han and Shih-Fu Chang and Martha Palmer and Heng Ji},
   booktitle = {2022 Annual Conference of the North American Chapter of the Association for Computational Linguistics -- System Demonstration Track},
   title = {RESIN-11: Schema-guided Event Prediction for 11 Newsworthy Scenarios},
   url = {https://github.com/RESIN-KAIROS/RESIN},
   year = {2022},
}


@article{Dunietz2017,
   abstract = {This paper explores extending shallow semantic parsing beyond lexical-unit triggers, using causal relations as a test case. Semantic parsing becomes difficult in the face of the wide variety of linguistic realizations that causation can take on. We therefore base our approach on the concept of constructions from the linguistic paradigm known as Construction Grammar (CxG). In CxG, a construction is a form/function pairing that can rely on arbitrary linguistic and semantic features. Rather than codifying all aspects of each construction’s form, as some attempts to employ CxG in NLP have done, we propose methods that offload that problem to machine learning. We describe two supervised approaches for tagging causal constructions and their arguments. Both approaches combine automatically induced pattern-matching rules with statistical classifiers that learn the subtler parameters of the constructions. Our results show that these approaches are promising: they significantly outperform naïve baselines for both construction recognition and cause and effect head matches.},
   author = {Jesse Dunietz and Lori Levin and Jaime Carbonell},
   journal = {Transactions of the Association for Computational Linguistics},
   pages = {117-133},
   title = {Automatically Tagging Constructions of Causation and Their Slot-Fillers},
   volume = {5},
   year = {2017},
}



@article{Hobbs1993,
title = {Interpretation as abduction},
journal = {Artificial Intelligence},
volume = {63},
number = {1},
pages = {69-142},
year = {1993},
issn = {0004-3702},
url = {https://www.sciencedirect.com/science/article/pii/0004370293900154},
author = {Jerry R. Hobbs and Mark E. Stickel and Douglas E. Appelt and Paul Martin},
abstract = {Abduction is inference to the best explanation. In the TACITUS project at SRI we have developed an approach to abductive inference, called “weighted abduction”, that has resulted in a significant simplification of how the problem of interpreting texts is conceptualized. The interpretation of a text is the minimal explanation of why the text would be true. More precisely, to interpret a text, one must prove the logical form of the text from what is already mutually known, allowing for coercions, merging redundancies where possible, and making assumptions where necessary. It is shown how such “local pragmatics” problems as reference resolution, the interpretation of compound nominals, the resolution of syntactic ambiguity and metonymy, and schema recognition can be solved in this manner. Moreover, this approach of “interpretation as abduction” can be combined with the older view of “parsing as deduction” to produce an elegant and thorough integration of syntax, semantics, and pragmatics, one that spans the range of linguistic phenomena from phonology to discourse structure. Finally, we discuss means for making the abduction process efficient, possibilities for extending the approach to other pragmatics phenomena, and the semantics of the weights and costs in the abduction scheme.}
}



@article{Hwang2020,
   abstract = {Recent years have brought about a renewed interest in commonsense representation and reasoning in the field of natural language understanding. The development of new commonsense knowledge graphs (CSKG) has been central to these advances as their diverse facts can be used and referenced by machine learning models for tackling new and challenging tasks. At the same time, there remain questions about the quality and coverage of these resources due to the massive scale required to comprehensively encompass general commonsense knowledge. In this work, we posit that manually constructed CSKGs will never achieve the coverage necessary to be applicable in all situations encountered by NLP agents. Therefore, we propose a new evaluation framework for testing the utility of KGs based on how effectively implicit knowledge representations can be learned from them. With this new goal, we propose ATOMIC 2020, a new CSKG of general-purpose commonsense knowledge containing knowledge that is not readily available in pretrained language models. We evaluate its properties in comparison with other leading CSKGs, performing the first large-scale pairwise study of commonsense knowledge resources. Next, we show that ATOMIC 2020 is better suited for training knowledge models that can generate accurate, representative knowledge for new, unseen entities and events. Finally, through human evaluation, we show that the few-shot performance of GPT-3 (175B parameters), while impressive, remains ~12 absolute points lower than a BART-based knowledge model trained on ATOMIC 2020 despite using over 430x fewer parameters.},
   author = {Jena D. Hwang and Chandra Bhagavatula and Ronan Le Bras and Jeff Da and Keisuke Sakaguchi and Antoine Bosselut and Yejin Choi},
   journal = {AAAI},
   title = {{COMET-ATOMIC} 2020: On Symbolic and Neural Commonsense Knowledge Graphs},
   url = {http://arxiv.org/abs/2010.05953},
   year = {2020},
}

@inproceedings{Jin2022,
    title = "Event Schema Induction with Double Graph Autoencoders",
    author = "Jin, Xiaomeng  and
      Li, Manling  and
      Ji, Heng",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.147",
    doi = "10.18653/v1/2022.naacl-main.147",
    pages = "2013--2025",
    abstract = "Event schema depicts the typical structure of complex events, serving as a scaffolding to effectively analyze, predict, and possibly intervene in the ongoing events. To induce event schemas from historical events, previous work uses an event-by-event scheme, ignoring the global structure of the entire schema graph. We propose a new event schema induction framework using double graph autoencoders, which captures the global dependencies among nodes in event graphs. Specifically, we first extract the event skeleton from an event graph and design a variational directed acyclic graph (DAG) autoencoder to learn its global structure. Then we further fill in the event arguments for the skeleton, and use another Graph Convolutional Network (GCN) based autoencoder to reconstruct entity-entity relations as well as to detect coreferential entities. By performing this two-stage induction decomposition, the model can avoid reconstructing the entire graph in one step, allowing it to focus on learning global structures between events. Experimental results on three event graph datasets demonstrate that our method achieves state-of-the-art performance and induces high-quality event schemas with global consistency.",
}


@book{Newton1687,
  title={Philosophiae Naturalis Principia Mathematica},
  author={Isaac Newton},
  year={1687},
  publisher={Londini, Jussu Societatis Regiæ ac Typis Josephi Streater}
}



@inproceedings{Han2021,
    title = "{ESTER}: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations",
    author = "Han, Rujun  and
      Hsu, I-Hung  and
      Sun, Jiao  and
      Baylon, Julia  and
      Ning, Qiang  and
      Roth, Dan  and
      Peng, Nanyun",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.597",
    doi = "10.18653/v1/2021.emnlp-main.597",
    pages = "7543--7559",
    abstract = "Understanding how events are semantically related to each other is the essence of reading comprehension. Recent event-centric reading comprehension datasets focus mostly on event arguments or temporal relations. While these tasks partially evaluate machines{'} ability of narrative understanding, human-like reading comprehension requires the capability to process event-based information beyond arguments and temporal reasoning. For example, to understand causality between events, we need to infer motivation or purpose; to establish event hierarchy, we need to understand the composition of events. To facilitate these tasks, we introduce **ESTER**, a comprehensive machine reading comprehension (MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages natural language queries to reason about the five most common event semantic relations, provides more than 6K questions, and captures 10.1K event relation pairs. Experimental results show that the current SOTA systems achieve 22.1{\%}, 63.3{\%} and 83.5{\%} for token-based exact-match (**EM**), **F1** and event-based **HIT@1** scores, which are all significantly below human performances (36.0{\%}, 79.6{\%}, 100{\%} respectively), highlighting our dataset as a challenging benchmark.",
}


@inproceedings{Li2013,
author = {Li, Boyang and Lee-Urban, Stephen and Johnston, George and Riedl, Mark O.},
title = {Story Generation with Crowdsourced Plot Graphs},
year = {2013},
publisher = {AAAI Press},
abstract = {Story generation is the problem of automatically selecting a sequence of events that meet a set of criteria and can be told as a story. Story generation is knowledge-intensive; traditional story generators rely on a priori defined domain models about fictional worlds, including characters, places, and actions that can be performed. Manually authoring the domain models is costly and thus not scalable. We present a novel class of story generation system that can generate stories in an unknown domain. Our system (a) automatically learns a domain model by crowdsourcing a corpus of narrative examples and (b) generates stories by sampling from the space defined by the domain model. A large-scale evaluation shows that stories generated by our system for a previously unknown topic are comparable in quality to simple stories authored by untrained humans.},
booktitle = {Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence},
pages = {598–604},
numpages = {7},
location = {Bellevue, Washington},
series = {AAAI'13}
}


@article{Li2021,
author = {Manling Li and Sha Li and Zhenhailong Wang and Lifu Huang and Kyunghyun Cho and Heng Ji and Jiawei Han and Clare Voss},
title = {Future is not One-dimensional: Graph Modeling based Complex Event Schema Induction for Event Prediction},
journal = {EMNLP},
url = {https://aclanthology.org/2021.emnlp-main.422/},
year = {2021}
}


@article{Li2022,
author = {Manling Li and Ruochen Xu and Shuohang Wang and Luowei Zhou and Xudong Lin and Chenguang Zhu and Michael Zeng and Heng Ji and Shih-Fu Chang},
title = {CLIP-Event: Connecting Text and Images with Event Structures},
journal = {arXiv},
url = {https://arxiv.org/abs/2201.05078},
year = {2022}
}


@inproceedings{Madaan2021a,
    title = "Neural Language Modeling for Contextualized Temporal Graph Generation",
    author = "Madaan, Aman  and Yang, Yiming",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.67",
    pages = "864--881",
}

@article{Madaan2021b,
   abstract = {Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. A commonly used method in cognitive science and logic literature is to handcraft argumentation supporting inference graphs. While humans find inference graphs very useful for reasoning, constructing them at scale is difficult. In this paper, we automatically generate such inference graphs through transfer learning from another NLP task that shares the kind of reasoning that inference graphs support. Through automated metrics and human evaluation, we find that our method generates meaningful graphs for the defeasible inference task. Human accuracy on this task improves by 20\% by consulting the generated graphs. Our findings open up exciting new research avenues for cases where machine reasoning can help human reasoning. (A dataset of 230,000 influence graphs for each defeasible query is located at: https://tinyurl.com/defeasiblegraphs.)},
   author = {Aman Madaan and Dheeraj Rajagopal and Niket Tandon and Yiming Yang and Eduard Hovy},
   doi = {10.18653/v1/2021.findings-acl.456},
   pages = {5138-5147},
   title = {Could you give me a hint? Generating inference graphs for defeasible reasoning},
   publisher = {Findings of the Association for Computational Linguistics},
   year = {2021},
}

@inproceedings{Mani2006,
    title = "Machine Learning of Temporal Relations",
    author = "Mani, Inderjeet  and
      Verhagen, Marc  and
      Wellner, Ben  and
      Lee, Chong Min  and
      Pustejovsky, James",
    booktitle = "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P06-1095",
    doi = "10.3115/1220175.1220270",
    pages = "753--760",
}

@article{
Milo2002,
author = {R. Milo  and S. Shen-Orr  and S. Itzkovitz  and N. Kashtan  and D. Chklovskii  and U. Alon },
title = {Network Motifs: Simple Building Blocks of Complex Networks},
journal = {Science},
volume = {298},
number = {5594},
pages = {824-827},
year = {2002},
URL = {https://www.science.org/doi/abs/10.1126/science.298.5594.824},
eprint = {https://www.science.org/doi/pdf/10.1126/science.298.5594.824},
abstract = {Complex networks are studied across many fields of science. To uncover their structural design principles, we defined “network motifs,” patterns of interconnections occurring in complex networks at numbers that are significantly higher than those in randomized networks. We found such motifs in networks from biochemistry, neurobiology, ecology, and engineering. The motifs shared by ecological food webs were distinct from the motifs shared by the genetic networks of Escherichia coli and Saccharomyces cerevisiae or from those found in the World Wide Web. Similar motifs were found in networks that perform information processing, even though they describe elements as different as biomolecules within a cell and synaptic connections between neurons in Caenorhabditis elegans. Motifs may thus define universal classes of networks. This approach may uncover the basic building blocks of most networks.}}

@inproceedings{Ouyang2015,
    title = "Modeling Reportable Events as Turning Points in Narrative",
    author = "Ouyang, Jessica  and
      McKeown, Kathleen",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1257",
    doi = "10.18653/v1/D15-1257",
    pages = "2149--2158",
}



@inproceedings{Mirza2014,
    title = "An Analysis of Causality between Events and its Relation to Temporal Information",
    author = "Mirza, Paramita  and
      Tonelli, Sara",
    booktitle = "Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
    month = aug,
    year = "2014",
    address = "Dublin, Ireland",
    publisher = "Dublin City University and Association for Computational Linguistics",
    url = "https://aclanthology.org/C14-1198",
    pages = "2097--2106",
}


@inproceedings{Mirza2016,
    title = "{CATENA}: {CA}usal and {TE}mporal relation extraction from {NA}tural language texts",
    author = "Mirza, Paramita  and
      Tonelli, Sara",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/C16-1007",
    pages = "64--75",
    abstract = "We present CATENA, a sieve-based system to perform temporal and causal relation extraction and classification from English texts, exploiting the interaction between the temporal and the causal model. We evaluate the performance of each sieve, showing that the rule-based, the machine-learned and the reasoning components all contribute to achieving state-of-the-art performance on TempEval-3 and TimeBank-Dense data. Although causal relations are much sparser than temporal ones, the architecture and the selected features are mostly suitable to serve both tasks. The effects of the interaction between the temporal and the causal components, although limited, yield promising results and confirm the tight connection between the temporal and the causal dimension of texts.",
}

@inproceedings{Ning2018a,
    title = "Joint Reasoning for Temporal and Causal Relations",
    author = "Ning, Qiang  and
      Feng, Zhili  and
      Wu, Hao  and
      Roth, Dan",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1212",
    doi = "10.18653/v1/P18-1212",
    pages = "2278--2288",
    abstract = "Understanding temporal and causal relations between events is a fundamental natural language understanding task. Because a cause must occur earlier than its effect, temporal and causal relations are closely related and one relation often dictates the value of the other. However, limited attention has been paid to studying these two relations jointly. This paper presents a joint inference framework for them using constrained conditional models (CCMs). Specifically, we formulate the joint problem as an integer linear programming (ILP) problem, enforcing constraints that are inherent in the nature of time and causality. We show that the joint inference framework results in statistically significant improvement in the extraction of both temporal and causal relations from text.",
}


@article{Ning2018b,
   abstract = {Extracting temporal relations (before, after, overlapping, etc.) is a key aspect of understanding events described in natural language. We argue that this task would gain from the availability of a resource that provides prior knowledge in the form of the temporal order that events usually follow. This paper develops such a resource ? a probabilistic knowledge base acquired in the news domain ? by extracting temporal relations between events from the New York Times (NYT) articles over a 20-year span (1987?2007). We show that existing temporal extraction systems can be improved via this resource. As a byproduct, we also show that interesting statistics can be retrieved from this resource, which can potentially benefit other time-Aware tasks. The proposed system and resource are both publicly available1.},
   author = {Qiang Ning and Hao Wu and Haoruo Peng and Dan Roth},
   doi = {10.18653/v1/n18-1077},
   isbn = {9781948087278},
   journal = {NAACL HLT 2018 - 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
   pages = {841-851},
   title = {Improving temporal relation extraction with a globally acquired statistical resource},
   volume = {1},
   year = {2018},
}


@article{Ning2018c,
   abstract = {Existing temporal relation (TempRel) annotation schemes often have low inter-annotator agreements (IAA) even between experts, suggesting that the current annotation task needs a better definition. This paper proposes a new multi-axis modeling to better capture the temporal structure of events. In addition, we identify that event end-points are a major source of confusion in annotation, so we also propose to annotate TempRels based on start-points only. A pilot expert annotation effort using the proposed scheme shows significant improvement in IAA from the conventional 60's to 80's (Cohen's Kappa). This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each annotator. We hope that this work can foster more interesting studies towards event understanding.},
   author = {Qiang Ning and Hao Wu and Dan Roth},
   doi = {10.18653/v1/p18-1122},
   isbn = {9781948087322},
   journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
   pages = {1318-1328},
   title = {A multi-axis annotation scheme for event temporal relations},
   volume = {1},
   year = {2018},
}


@inproceedings{Ning2020,
    title = "{TORQUE}: A Reading Comprehension Dataset of Temporal Ordering Questions",
    author = "Ning, Qiang  and
      Wu, Hao  and
      Han, Rujun  and
      Peng, Nanyun  and
      Gardner, Matt  and
      Roth, Dan",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.88",
    doi = "10.18653/v1/2020.emnlp-main.88",
    pages = "1158--1172",
    abstract = "A critical part of reading is being able to understand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have practically no questions that test temporal phenomena, so systems trained on these benchmarks have no capacity to answer questions such as {``}what happened before/after [some event]?{''} We introduce TORQUE, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships. Results show that RoBERTa-large achieves an exact-match score of 51{\%} on the test set of TORQUE, about 30{\%} behind human performance.",
}

@article{OGorman2016,
abstract = {We examine the effect of industry life-cycle stages on within-industry acquisitions and capital expenditures by conglomerates and single-segment firms controlling for endogeneity of organizational form.We find greater differences in acquisitions than in capital expenditures, which are similar across organizational types. In particular, 36{\%} of the growth recorded by conglomerate segments in growth industries comes from acquisitions, versus 9{\%} for single-segment firms. In growth industries, the effect of fi- nancial dependence on acquisitions and plant openings is mitigated for conglomerate firms. Plants acquired by conglomerate firms increase in productivity. The results suggest that organizational forms' comparative advantages differ across industry conditions.},
author = {O'Gorman, Tim and Wright-Bettner, Kristin and Palmer, Martha},
doi = {10.18653/v1/w16-5706},
journal = {2nd Workshop on Computing News Storylines, ACL},
pages = {47--56},
title = {{Richer Event Description: Integrating event coreference with temporal, causal and bridging annotation}},
year = {2016}
}

@inproceedings{OGorman2018,
    title = "{AMR} Beyond the Sentence: the Multi-sentence {AMR} corpus",
    author = "O{'}Gorman, Tim  and
      Regan, Michael  and
      Griffitt, Kira  and
      Hermjakob, Ulf  and
      Knight, Kevin  and
      Palmer, Martha",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1313",
    pages = "3693--3702",
    abstract = "There are few corpora that endeavor to represent the semantic content of entire documents. We present a corpus that accomplishes one way of capturing document level semantics, by annotating coreference and similar phenomena (bridging and implicit roles) on top of gold Abstract Meaning Representations of sentence-level semantics. We present a new corpus of this annotation, with analysis of its quality, alongside a plausible baseline for comparison. It is hoped that this Multi-Sentence AMR corpus (MS-AMR) may become a feasible method for developing rich representations of document meaning, useful for tasks such as information extraction and question answering.",
}

@inproceedings{Page1999,
  title={The PageRank Citation Ranking : Bringing Order to the Web},
  author={Lawrence Page and Sergey Brin and Rajeev Motwani and Terry Winograd},
  booktitle={The Web Conference},
  year={1999}
}



@inproceedings{Pustejovsky2003,
  title={TimeML: Robust Specification of Event and Temporal Expressions in Text},
  author={James Pustejovsky and Jos{\'e} M. Casta{\~n}o and Robert Ingria and Roser Saur{\'i} and Robert J. Gaizauskas and Andrea Setzer and Graham Katz and Dragomir R. Radev},
  booktitle={Fifth International Workshop on Computational Semantics},
  year={2003}
}

@article{Radford2019,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI},
  year={2019}
}

@inproceedings{Reimers2019,
  title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
  author = "Reimers, Nils and Gurevych, Iryna",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
  month = "11",
  year = "2019",
  publisher = "Association for Computational Linguistics",
  url = "https://arxiv.org/abs/1908.10084",
}

@article{Riedl2010,
	year = 2010,
	month = {sep},
	publisher = {{AI} Access Foundation},
	volume = {39},
	pages = {217--268},
	author = {Riedl, Mark and Young, Robert},
	title = {Narrative Planning: Balancing Plot and Character},
	journal = {Journal of Artificial Intelligence Research}
}

@inproceedings{Rosenberg2007,
    title = "{V}-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure",
    author = "Rosenberg, Andrew  and
      Hirschberg, Julia",
    booktitle = "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lang. Learning ({EMNLP}-{C}o{NLL})",
    month = jun,
    year = "2007",
    address = "Prague",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D07-1043",
    pages = "410--420",
}


@inproceedings{Rozemberczki2020,
               title={{Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models}},
               author={Benedek Rozemberczki and Rik Sarkar},
               year={2020},
	       pages = {1325–1334},
	       booktitle={Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM '20)},
	       organization={ACM},
}


@inproceedings{Sakaguchi2021,
    title = "pro{S}cript: Partially Ordered Scripts Generation",
    author = "Sakaguchi, Keisuke  and
      Bhagavatula, Chandra  and
      Le Bras, Ronan  and
      Tandon, Niket  and
      Clark, Peter  and
      Choi, Yejin",
    booktitle = "Findings of the Assoc. for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.184",
    doi = "10.18653/v1/2021.findings-emnlp.184",
    pages = "2138--2149",
    abstract = "Scripts {--} prototypical event sequences describing everyday activities {--} have been shown to help understand narratives by providing expectations, resolving ambiguity, and filling in unstated information. However, to date they have proved hard to author or extract from text. In this work, we demonstrate for the first time that pre-trained neural language models can be finetuned to generate high-quality scripts, at varying levels of granularity, for a wide range of everyday scenarios (e.g., bake a cake). To do this, we collect a large (6.4k) crowdsourced partially ordered scripts (named proScript), that is substantially larger than prior datasets, and develop models that generate scripts by combining language generation and graph structure prediction. We define two complementary tasks: (i) edge prediction: given a scenario and unordered events, organize the events into a valid (possibly partial-order) script, and (ii) script generation: given only a scenario, generate events and organize them into a (possibly partial-order) script. Our experiments show that our models perform well (e.g., F1=75.7 on task (i)), illustrating a new approach to overcoming previous barriers to script collection. We also show that there is still significant room for improvement toward human level performance. Together, our tasks, dataset, and models offer a new research direction for learning script knowledge.",
}

@inproceedings{Schank1975,
author = {Schank, Roger C. and Abelson, Robert P.},
title = {Scripts, Plans, and Knowledge},
year = {1975},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We describe a theoretical system intended to facilitate the use of knowledge In an understanding system. The notion of script is introduced to account for knowledge about mundane situations. A program, SAM, is capable of using scripts to understand. The notion of plans is introduced to account for general knowledge about novel situations.},
booktitle = {Proceedings of the 4th International Joint Conference on Artificial Intelligence - Volume 1},
pages = {151–157},
numpages = {7},
location = {Tblisi, USSR},
series = {IJCAI'75}
}

@inproceedings{Sennrich2016,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@article{Talmy1988,
author = {Talmy, Leonard},
title = {Force Dynamics in Language and Cognition},
journal = {Cognitive Science},
volume = {12},
number = {1},
pages = {49-100},
abstract = {“Force dynamics” refers to a previously neglected semantic category—how entities interact with respect to force. This category includes such concepts as: the exertion of force, resistance to such exertion and the overcoming of such resistance, blockage of a force and the removal of such blockage, and so forth. Force dynamics is a generalization over the traditional linguistic notion of “causative”: it analyzes “causing” into finer primitives and sets it naturally within a framework that also includes “letting,”“hindering,”“helping,” and still further notions. Force dynamics, moreover, appears to be the semantic category that uniquely characterizes the grammatical category of modals, in both their basic and epistemic usages. In addition, on the basis of force dynamic parameters, numerous lexical items fall into systematic semantic patterns, and there exhibit parallelisms between physical and psychosocial reference. Further, from research on the relation of semantic structure to general cognitive structure, it appears that the concepts of force interaction that are encoded within language closely parallel concepts that appear both in early science and in naive physics and psychology. Overall, force dynamics thus emerges as a fundamental notional system that structures conceptual material pertaining to force interaction in a common way across a linguistic range: the physical, psychological, social, inferential, discourse, and mental-model domains of reference and conception.},
year = {1988}
}



@inproceedings{Uzzaman2013,
    title = "{S}em{E}val-2013 Task 1: {T}emp{E}val-3: Evaluating Time Expressions, Events, and Temporal Relations",
    author = "UzZaman, Naushad  and
      Llorens, Hector  and
      Derczynski, Leon  and
      Allen, James  and
      Verhagen, Marc  and
      Pustejovsky, James",
    booktitle = "2nd Joint Conference on Lexical and Computational Semantics (*{SEM})",
    month = jun,
    year = "2013",
    address = "Atlanta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S13-2001",
    pages = "1--9",
}

@article{Tu2022,
  author    = {Jingxuan Tu and
               Kyeongmin Rim and
               Eben Holderness and
               James Pustejovsky},
  title     = {Dense Paraphrasing for Textual Enrichment},
  journal   = {CoRR},
  volume    = {abs/2210.11563},
  year      = {2022},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{
  Velickovic2018,
  title="{Graph Attention Networks}",
  author={Veli{\v{c}}kovi{\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`{o}}, Pietro and Bengio, Yoshua},
  journal={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{Wang2020,
    title = "{MAVEN}: {A} {M}assive {G}eneral {D}omain {E}vent {D}etection {D}ataset",
    author = "Wang, Xiaozhi  and
      Wang, Ziqi  and
      Han, Xu  and
      Jiang, Wangyi  and
      Han, Rong  and
      Liu, Zhiyuan  and
      Li, Juanzi  and
      Li, Peng  and
      Lin, Yankai  and
      Zhou, Jie",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.129",
    doi = "10.18653/v1/2020.emnlp-main.129",
    pages = "1652--1671",
    abstract = "Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit further development of ED: (1) Data scarcity. Existing small-scale datasets are not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods. (2) Low coverage. Limited event types of existing datasets cannot well cover general-domain events, which restricts the applications of ED models. To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types. MAVEN alleviates the data scarcity problem and covers much more general event types. We reproduce the recent state-of-the-art ED models and conduct a thorough evaluation on MAVEN. The experimental results show that existing ED methods cannot achieve promising results on MAVEN as on the small datasets, which suggests that ED in the real world remains a challenging task and requires further research efforts. We also discuss further directions for general domain ED with empirical analyses. The source code and dataset can be obtained from https://github.com/THU-KEG/MAVEN-dataset.",
}


@article{Wen2021,
abstract = {We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects: (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human cu-rated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub 1 , with a demo video 2 .},
author = {Wen, Haoyang and Lin, Ying and Lai, Tuan M and Pan, Xiaoman and Li, Sha and Lin, Xudong and Zhou, Ben and Li, Manling and Wang, Haoyu and Zhang, Hongming and Yu, Xiaodong and Dong, Alexander and Wang, Zhenhailong and Fung, Yi R and Mishra, Piyush and Lyu, Qing and Sur{\'{i}}s, D{\'{i}}dac and Chen, Brian and Brown, Susan W and Palmer, Martha and Callison-burch, Chris and Vondrick, Carl and Han, Jiawei and Roth, Dan and Chang, Shih-fu and Ji, Heng},
journal = {NAACL},
title = {{RESIN : A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System}},
year = {2021}
}


@inproceedings{West2022,
    title = "Symbolic Knowledge Distillation: from General Language Models to Commonsense Models",
    author = "West, Peter  and
      Bhagavatula, Chandra  and
      Hessel, Jack  and
      Hwang, Jena  and
      Jiang, Liwei  and
      Le Bras, Ronan  and
      Lu, Ximing  and
      Welleck, Sean  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.341",
    doi = "10.18653/v1/2022.naacl-main.341",
    pages = "4602--4625",
    abstract = "The common practice for training commonsense models has gone from{--}human{--}to{--}corpus{--}to{--}machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from{--}machine{--}to{--}corpus{--}to{--}machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al. 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically{--}as text{--}in addition to the neural model. We distill only one aspect{--}the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model{'}s commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and will share our new symbolic knowledge graph and commonsense models.",
}


@inproceedings{Vallurupalli2022,
    title = "{POQ}ue: Asking Participant-specific Outcome Questions for a Deeper Understanding of Complex Events",
    author = "Vallurupalli, Sai  and
      Ghosh, Sayontan  and
      Erk, Katrin  and
      Balasubramanian, Niranjan  and
      Ferraro, Francis",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.594",
    pages = "8674--8697",
    abstract = "Knowledge about outcomes is critical for complex event understanding but is hard to acquire.We show that by pre-identifying a participant in a complex event, crowdworkers are ableto (1) infer the collective impact of salient events that make up the situation, (2) annotate the volitional engagement of participants in causing the situation, and (3) ground theoutcome of the situation in state changes of the participants. By creating a multi-step interface and a careful quality control strategy, we collect a high quality annotated dataset of8K short newswire narratives and ROCStories with high inter-annotator agreement (0.74-0.96weighted Fleiss Kappa). Our dataset, POQUe (Participant Outcome Questions), enables theexploration and development of models that address multiple aspects of semantic understanding. Experimentally, we show that current language models lag behind human performance in subtle ways through our task formulations that target abstract and specific comprehension of a complex event, its outcome, and a participant{'}s influence over the event culmination.",
}


@inproceedings{Zhou2021,
    title = "Temporal Reasoning on Implicit Events from Distant Supervision",
    author = "Zhou, Ben  and
      Richardson, Kyle  and
      Ning, Qiang  and
      Khot, Tushar  and
      Sabharwal, Ashish  and
      Roth, Dan",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.107",
    doi = "10.18653/v1/2021.naacl-main.107",
    pages = "1361--1371",
    abstract = "We propose TRACIE, a novel temporal reasoning dataset that evaluates the degree to which systems understand implicit events{---}events that are not mentioned explicitly in natural language text but can be inferred from it. This introduces a new challenge in temporal reasoning research, where prior work has focused on explicitly mentioned events. Human readers can infer implicit events via commonsense reasoning, resulting in a more comprehensive understanding of the situation and, consequently, better reasoning about time. We find, however, that state-of-the-art models struggle when predicting temporal relationships between implicit and explicit events. To address this, we propose a neuro-symbolic temporal reasoning model, SymTime, which exploits distant supervision signals from large-scale text and uses temporal rules to combine start times and durations to infer end times. SymTime outperforms strong baseline systems on TRACIE by 5{\%}, and by 11{\%} in a zero prior knowledge training setting. Our approach also generalizes to other temporal reasoning tasks, as evidenced by a gain of 1{\%}-9{\%} on MATRES, an explicit event benchmark.",
}

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{Allen1983,
author = {Allen, James F.},
title = {Maintaining Knowledge about Temporal Intervals},
year = {1983},
issue_date = {Nov. 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {11},
journal = {Commun. ACM},
month = {nov},
pages = {832–843},
numpages = {12},
keywords = {temporal interval, interval reasoning, interval representation}
}

@article{Alabdulkarim2021,  
  author = {Alabdulkarim, Amal and Li, Siyan and Peng, Xiangyu},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Automatic Story Generation: Challenges and Attempts},
  
  journal = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Ammanabrolu2021,
  
  author = {Ammanabrolu, Prithviraj and Riedl, Mark O.},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning Knowledge Graph-based World Models of Textual Environments},
  
  journal = {arXiv},

  url = {https://arxiv.org/abs/2106.09608},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@book{Croft2012,
    title = {Verbs: Aspect and Causal Structure},
    author = {William Croft},
    year = {2012},
    publisher = {Oxford University Press},
    keywords = {physics}
}

@article{Davidson1967,
  pages = {691--703},
  number = {21},
  volume = {64},
  journal = {Journal of Philosophy},
  title = {Causal Relations},
  author = {Donald Davidson},
  year = {1967},
  publisher = {Oxford Up}
}



@article {Fillmore1976,
	title = {Frame semantics and the nature of language},
	journal = {Annals of the New York Academy of Sciences: Conference on the Origin and Development of Language and Speech},
	volume = {280},
	number = {1},
	year = {1976},
	pages = {20-32},
	author = {Fillmore, Charles J.}
}

@article {Fillmore2003,
	title = {Background to {FrameNet}},
	journal = {International Journal of Lexicography},
	volume = {16.3},
	year = {2003},
	pages = {235-250},
	author = {Fillmore, Charles J. and Christopher R. Johnson and Petruck, Miriam R.L.}
}


@article{Ghosh2022,
      title={PASTA: A Dataset for Modeling Participant States in Narratives}, 
      author={Sayontan Ghosh and Mahnaz Koupaee and Isabella Chen and Francis Ferraro and Nathanael Chambers and Niranjan Balasubramanian},
      year={2022},
      eprint={2208.00329},
      journal={arXiv},
      primaryClass={cs.CL}
}


@book{Goldberg2006,
	Author = {Adele E. Goldberg},
	Publisher = {Oxford: Oxford University Press},
	Title = {Constructions at work: the nature of generalization in language},
	Year = {2006}}



@inproceedings{He2021,
title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}



@article{Hobbs2005,
abstract = {We do things in the world by exploiting our knowledge of what causes what. But in trying to reason formally about causality, there is a difficulty: to reason with certainty we need complete knowledge of all the relevant events and circumstances, whereas in everyday reasoning tasks we need a more serviceable but looser notion that does not make such demands on our knowledge. In this work the notion of 'causal complex' is introduced for a complete set of events and conditions necessary for the causal consequent to occur, and the term 'cause' is used for the makeshift, nonmonotonic notion we require for everyday tasks such as planning and language understanding. Like all interesting concepts, neither of these can be defined with necessary and sufficient conditions, but they can be more or less tightly constrained by necessary conditions or sufficient conditions. The issue of how to distinguish between what is in a causal complex from what is outside it is discussed, and within a causal complex, how to distinguish the eventualities that deserve to be called 'causes' from those that do not, in particular circumstances. One particular modal, the word 'would', is examined from the standpoint of its underlying causal content, as a linguistic motivation for this enterprise. {\textcopyright} The Author 2005. Published by Oxford University Press. All rights reserved.},
author = {Hobbs, Jerry R.},
journal = {Journal of Semantics},
number = {2},
pages = {181--209},
title = {{Toward a useful concept of causality for lexical semantics}},
volume = {22},
year = {2005}
}

@article{Holtzman2019,
  doi = {10.48550/ARXIV.1904.09751},
  
  url = {https://arxiv.org/abs/1904.09751},
  
  author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Curious Case of Neural Text Degeneration},
  
  journal = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{Kahneman2012,
  added-at = {2014-05-05T12:09:07.000+0200},
  address = {London},
  author = {Kahneman, Daniel},
  description = {Thinking, Fast and Slow},
  keywords = {chapter4},
  publisher = {Penguin Books},
  title = {Thinking, Fast and Slow},
  year = 2012
}

@article{Labov1967,
   author = "Labov, William and Waletzky, Joshua",
   title = "Narrative analysis: Oral versions of personal experience", 
   journal= "Journal of Narrative and Life History",
   year = "1967",
   volume = "7",
   number = "1-4",
   pages = "3-38",
   publisher = "John Benjamins",
   issn = "1053-6981",
   type = "Journal Article",
  }

@article{Lake2017,
   abstract = {Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
   author = {Brenden Lake and Tomer D. Ullman and Joshua B. Tenenbaum and Samuel J. Gershman},
   doi = {10.1017/S0140525X16001837},
   issn = {14691825},
   issue = {2012},
   journal = {Behavioral and Brain Sciences},
   pages = {1-58},
   pmid = {27881212},
   title = {Building machines that learn and think like people},
   volume = {40},
   year = {2017},
}



@techreport{Minsky1974,
author = {Minsky, Marvin},
title = {A Framework for Representing Knowledge},
year = {1974},
institution = {Massachusetts Institute of Technology},
address = {USA}
}

@article {Parascandola2001,
	author = {Parascandola, Mark and Weed, D L},
	title = {Causation in epidemiology},
	volume = {55},
	number = {12},
	pages = {905--912},
	year = {2001},
	doi = {10.1136/jech.55.12.905},
	publisher = {BMJ Publishing Group Ltd},
	abstract = {Causation is an essential concept in epidemiology, yet there is no single, clearly articulated definition for the discipline. From a systematic review of the literature, five categories can be delineated: production, necessary and sufficient, sufficient-component, counterfactual, and probabilistic. Strengths and weaknesses of these categories are examined in terms of proposed characteristics of a useful scientific definition of causation: it must be specific enough to distinguish causation from mere correlation, but not so narrow as to eliminate apparent causal phenomena from consideration. Two categories{\textemdash}production and counterfactual{\textemdash}are present in any definition of causation but are not themselves sufficient as definitions. The necessary and sufficient cause definition assumes that all causes are deterministic. The sufficient-component cause definition attempts to explain probabilistic phenomena via unknown component causes. Thus, on both of these views, heavy smoking can be cited as a cause of lung cancer only when the existence of unknown deterministic variables is assumed. The probabilistic definition, however, avoids these assumptions and appears to best fit the characteristics of a useful definition of causation. It is also concluded that the probabilistic definition is consistent with scientific and public health goals of epidemiology. In debates in the literature over these goals, proponents of epidemiology as pure science tend to favour a narrower deterministic notion of causation models while proponents of epidemiology as public health tend to favour a probabilistic view. The authors argue that a single definition of causation for the discipline should be and is consistent with both of these aims. It is concluded that a counterfactually-based probabilistic definition is more amenable to the quantitative tools of epidemiology, is consistent with both deterministic and probabilistic phenomena, and serves equally well for the acquisition and the application of scientific knowledge.},
	issn = {0143-005X},
	URL = {https://jech.bmj.com/content/55/12/905},
	eprint = {https://jech.bmj.com/content/55/12/905.full.pdf},
	journal = {Journal of Epidemiology \& Community Health}
}




@book{Propp1968,
  title={Morphology of the folktale, 2nd ed.},
  author={Propp, Vladimir},
  year={1968},
  publisher={University of Texas Press}
}

@book{Purves2017,
  title={Neuroscience, 6th ed.},
  author={Dale Purves and George J. Augustine and David Fitzpatrick and William C. Hall and Anthony LaMantia and Richard D. Mooney and Michael L. Platt and Leonard E. White},
  publisher={Sinauer Associates},
  year = {2017}
}

@book{Reichenbach1956,
  publisher = {Dover Publications},
  author = {Hans Reichenbach},
  title = {The Direction of Time},
  year = {1956}
}

@book{Schank1977,
  TITLE = {{Scripts, plans, goals, and understanding : an inquiry into human knowledge structures}},
  AUTHOR = {Schank, Roger C. and Abelson, Robert P.},
  PUBLISHER = {{Hillsdale, N.J. : Lawrence Erlbaum Associates}},
  PAGES = {256 pages},
  YEAR = {1977},
  KEYWORDS = {computer simulations}
}

@incollection{Schank1995,
        	title={Knowledge and memory: The real story},
        	author={Schank, Roger C. and Abelson, Robert P.},
        	year={1995},
        	pages={1-85},
        	booktitle={Knowledge and Memory: the Real Story: Advances in Social Cognition, Volume VIII},
        	publisher={Lawrence Erlbaum Associates}
        }

@article{Scholkopf2021,
   abstract = {The two fields of machine learning and graphical causality arose and are developed separately. However, there is, now, cross-pollination and increasing interest in both fields to benefit from the advances of the other. In this article, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, that is, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
   author = {Bernhard Sch{\"o}lkopf and Francesco Locatello and Stefan Bauer and Nan Rosemary Ke and Nal Kalchbrenner and Anirudh Goyal and Yoshua Bengio},
   doi = {10.1109/JPROC.2021.3058954},
   issn = {15582256},
   issue = {5},
   journal = {Proceedings of the IEEE},
   keywords = {Artificial intelligence,causality,deep learning,representation learning},
   pages = {612-634},
   title = {Toward Causal Representation Learning},
   volume = {109},
   year = {2021},
}



@inproceedings{Tan2022,
    title = "The Causal News Corpus: Annotating Causal Relations in Event Sentences from News",
    author = {Tan, Fiona Anting  and
      H{\"u}rriyeto{\u{g}}lu, Ali  and
      Caselli, Tommaso  and
      Oostdijk, Nelleke  and
      Nomoto, Tadashi  and
      Hettiarachchi, Hansi  and
      Ameer, Iqra  and
      Uca, Onur  and
      Liza, Farhana Ferdousi  and
      Hu, Tiancheng},
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.246",
    pages = "2298--2310",
    abstract = "Despite the importance of understanding causality, corpora addressing causal relations are limited. There is a discrepancy between existing annotation guidelines of event causality and conventional causality corpora that focus more on linguistics. Many guidelines restrict themselves to include only explicit relations or clause-based arguments. Therefore, we propose an annotation schema for event causality that addresses these concerns. We annotated 3,559 event sentences from protest event news with labels on whether it contains causal relations or not. Our corpus is known as the Causal News Corpus (CNC). A neural network built upon a state-of-the-art pre-trained language model performed well with 81.20{\%} F1 score on test set, and 83.46{\%} in 5-folds cross-validation. CNC is transferable across two external corpora: CausalTimeBank (CTB) and Penn Discourse Treebank (PDTB). Leveraging each of these external datasets for training, we achieved up to approximately 64{\%} F1 on the CNC test set without additional fine-tuning. CNC also served as an effective training and pre-training dataset for the two external corpora. Lastly, we demonstrate the difficulty of our task to the layman in a crowd-sourced annotation exercise. Our annotated corpus is publicly available, providing a valuable resource for causal text mining researchers.",
}

@article{Tenenbaum2011,
author = {Joshua B. Tenenbaum  and Charles Kemp  and Thomas L. Griffiths  and Noah D. Goodman },
title = {How to Grow a Mind: Statistics, Structure, and Abstraction},
journal = {Science},
volume = {331},
number = {6022},
pages = {1279-1285},
year = {2011},
doi = {10.1126/science.1192788},
URL = {https://www.science.org/doi/abs/10.1126/science.1192788},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1192788},
abstract = {In coming to understand the world—in learning concepts, acquiring language, and grasping causal relations—our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?}}

@incollection{Tversky1982, 
place={Cambridge}, 
title={Causal schemas in judgments under uncertainty}, 
DOI={10.1017/CBO9780511809477.009}, booktitle={Judgment under Uncertainty: Heuristics and Biases}, 
publisher={Cambridge University Press}, 
author={Tversky, Amos and Kahneman, Daniel}, 
editor={Kahneman, Daniel and Slovic, Paul and Tversky, Amos}, 
year={1982}, 
pages={117–128}}




@article{Rex2020,
	journal = {arXiv},
    url = {https://arxiv.org/abs/2007.03092},
	title = {Neural Subgraph Matching},
	author = {Rex, Ying and Wang, Andrew and You, Jiaxuan and Wen, Chengtao and Canedo, Arquimedes and Jure Leskovec},
	year = {2020}
}



@article{Wolff2007,
	journal = {Journal of Experimental Psychology: General},
	title = {Representing Causation},
	doi = {10.1037/0096-3445.136.1.82},
	volume = {136},
	number = {1},
	author = {Phillip Wolff},
	pages = {82--111},
	year = {2007}
}