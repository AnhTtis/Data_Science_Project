% \begin{abstract}
% We present a fully causal speech coding architecture that provides high quality at 1000 bits per second.  The method uses causal transformer-based models on the discrete residual codebook tokens from a pretrained SoundStream model that is trained for speech coding at 6 kilobits per second.  One transformer model predicts the fine SoundStream tokens from the coarse tokens, allowing transmission of fewer codes.  Another transformer model predicts the uncertainty for the next frame given the past frames, and is used to conditionally entropy code each token.  A MUSHRA subjective test was conducted and shows that the quality is comparable to reference codecs at higher bitrates. Example audio is available at \url{https://sites.google.com/corp/google.com/lowbitrateaudiolm-results}.
% \end{abstract}
\begin{abstract}
We introduce LMCodec, a causal neural speech codec that provides high quality audio at very low bitrates. The backbone of the system is a causal convolutional codec that encodes audio into a hierarchy of coarse-to-fine tokens using residual vector quantization. LMCodec trains a Transformer language model to predict the fine tokens from the coarse ones in a generative fashion, allowing for the transmission of fewer codes. A second Transformer predicts the uncertainty of the next codes given the past transmitted codes, and is used to perform conditional entropy coding. A MUSHRA subjective test was conducted and shows that the quality is comparable to reference codecs at higher bitrates. Example audio is available at  \url{https://mjenrungrot.github.io/chrome-media-audio-papers/publications/lmcodec}.
\end{abstract}
\begin{keywords}
speech coding, Transformers, self-supervised
learning, generative adversarial networks.
\end{keywords}
