\section{Evaluation}
\label{sec:evaluation}




To demonstrate the performance of our proposed method, we evaluate LMCodec using both objective and subjective evaluations. For objective evaluation, we report the accuracy of LMCodec future token prediction and objective metrics including ViSQOL \cite{chinen2020visqol}, WARP-Q \cite{jassim2021warp}, SSL-MOS \cite{cooper2022generalization}, WER, and CER together with bitrate based on the test split from the clean LibriSpeech dataset \cite{panayotov2015librispeech}.

For subjective evaluation, we perform two MUSHRA-like \cite{recommendation20151534} subjective tests to compare the audio quality with standard state-of-the-art speech codecs at medium bitrate (i.e., \SI{1}{~}kbps to \SI{12}{~}kbps) and low rate (i.e., \SI{0.5}{~}kbps to \SI{1.5}{~}kbps). The tests were conducted respectively on 91 and 94  crowd-sourced raters using headphones over 32 clean utterances from VCTK dataset \cite{Yamagishi2019CSTRVC}. Raters who did not score the reference above 80 at least 80\% of the time were discarded, as were raters who rated more than 75\% of non-reference samples 80 or above. 40  raters for the medium rate test and 33 raters for the low rate test met this requirement.

As shown in Figure \ref{fig:mushra}, the raters found that LMCodec-4/6 with 4~quantizers at \SI{1.1}{~}kbps perform significantly better than \SI{12}{~}kbps Opus. LMCodec-8/12 with 8~quantizers at \SI{2.6}{~}kbps has comparable performancce to SoundStream at \SI{6}{~}kbps. The low-rate MUSHRA test compares recent transformer neural codecs and lower bitrate SoundStream models.  The raters preferred LMCodec to the transformer models from~\cite{siahkoohi2022ultra} and SoundStream at the same rate.


\subsection{Discussion}

Table \ref{tab:accuracy} shows the accuracy of the future token prediction and the bitrate performance of LMCodec from the test split of the clean LibriSpeech \cite{panayotov2015librispeech}. For accuracy, we note that perfect accuracy means the model knows perfectly what the next tokens are. In the context of fine-level AudioLM, this suggests that the model does not necessarily need to synthesize the correct code to produce reasonable audio output. The bitrates are computed based on the future token's distributions obtained from LMCodec. For Huffman coding, we use the ground truth tokens encoded with the Huffman algorithm. Additionally, we note that the distributions of future tokens are updated every timestep based on the model, different from how other entropy codecs that may have fixed distributions operate. So, the Huffman bitrate may sometimes be lower than the bitrate derived from the entropy.

In this section, we additionally discuss some of the interesting audio effects from LMCodec. We suggest that readers listen to some of the audio samples from our model.
In particular, our model with only one quantizer is able to produce reasonable human voice with some babbling effects. The amount of babbling is reduced as the number of quantizers used in the codec increases. This suggests that there are some underlying hierarchical structure in SoundStream tokens, and the proposed codec can potentially be operating at very low bitrate, given that the coarse-to-fine prediction is accurate.

In Figure \ref{fig:audio-output}, we visualize the distribution of code prediction from the AudioLM model when the input is at the middle of a phoneme and between phonemes. We also found that the model is very confident if the audio input is the middle of the phonemes, as the language model network is able to learn underlying linguistic behavior of the utterances. On the other hand, the model has lower confidence in predicting the next token when reaching silence sections, suggesting that our proposed causal model is unable to predict future word really well. This confirms the babbling effect that we observed in the audio output from our proposed codec, which increases as we restrict the amount of information to describe each frame (e.g., by transmitting \linebreak fewer codes or dropping frames). 

Figure \ref{fig:mushra} shows the comparison of LMCodec with low-rate and medium-rate audio codecs. In particular, we find that LMCodec-4/6 performs better than SoundStream with 3 quantizers at \SI{1.5}{~}kbps but slightly worse than SoundStream with 12 quantizers at \SI{6}{~}kbps which is on par with LMCodec-8/12. We note that LMCodec-4/6 and LMCodec-8/12 are based on SoundStream with 6 and 12 quantizers respectively. Our results suggest that LMCodec effectively takes advantages from entropy coding and synthesizing reasonable fine-level codes from coarse-level codes. When comparing with SoundStream at similar rate, LMCodec essentially outperforms.


\subsection{Voice Activity Detection (VAD)}

\looseness -2
In this section, we show the performance of LMCodec applied only on audio regions with voice activity. We use an open-source RNNoise model \cite{valin2018hybrid}, which uses Mel-Frequency Cepstral Coefficients (MFCC) and outputs the probability of voice activity every \SI{10}{ms} frame size. Since the frame size of SoundStream tokens is \SI{20}{ms}, we run RNNoise on 2 consecutive 10-ms frames and define that the 20-ms SoundStream frame has a voice activity if and only if the probability that 2 consecutive frames have voice is over 0.8.

\looseness -2
Table \ref{tab:vad} shows the bitrate of LMCodec on two scenarios: (i) transmitting only voices and (ii) transmitting entire speech signals but using zero bits for non-voices. We report the  bitrate derived from the entropy and the bitrate based on Huffman coding. We note the first scenario has slightly lower bitrates as compared to bitrates from Table \ref{tab:accuracy} because the entropy for non-speech signals is usually higher than the entropy for speech signals. Additionally, the second scenario provides the lower bound estimate of bitrates when transmitting very low bits for non-voice signals similar to Opus with variable bitrate scheme.


\begin{table}
    \captionsetup{belowskip=-20pt}
    \centering
    \scriptsize
    \centerline{\begin{tabular}{lccc}
    \toprule
    $(N_\mathcal{C},N_\mathcal{F})$ & \textbf{Accuracy} & \textbf{Entropy}& \textbf{Huffman}\\
    \midrule\midrule
    % https://xm2a.corp.google.com/experiments/48089630
    $(2,1)$ & 15.5\% & \SI{534.0}{~}bps & \SI{542.5}{~}bps  \\
    % https://xm2a.corp.google.com/experiments/48088660
    $(3,1)$ & 14.3\% & \SI{837.1}{~}bps & \SI{845.7}{~}bps  \\
    % https://xm2a.corp.google.com/experiments/49313845
    $(4,2)$ & 13.1\% & \SI{1163.9}{~}bps & \SI{1173.5}{~}bps \\
    \midrule
    % https://xm2a.corp.google.com/experiments/49316968
    % new_scalable_12quantizers - 64 base_conv_depth
    $(1,11)$ & 16.1\% & \SI{262.8}{~}bps & \SI{262.6}{~}bps \\
    $(2,10)$ & 15.7\% & \SI{533.5}{~}bps & \SI{540.7}{~}bps \\
    $(3,9)$ & 14.9\% & \SI{844.6}{~}bps & \SI{847.4}{~}bps \\
    $(4,8)$ & 13.4\% & \SI{1154.2}{~}bps & \SI{1174.3}{~}bps \\
    $(6,6)$ & 11.9\% & \SI{1853.7}{~}bps & \SI{1861.2}{~}bps \\
    $(8,4)$ & 10.6\% & \SI{2561.8}{~}bps & \SI{2577.6}{~}bps \\
    $(10,2)$ & 9.7\% & \SI{3300.0}{~}bps & \SI{3324.8}{~}bps \\
    $(12,0)$ & 8.9\% & \SI{4094.5}{~}bps & \SI{4092.1}{~}bps \\
    \bottomrule
    \end{tabular}}
    \captionof{table}{Accuracy and bitrates. Bitrate without entropy coding is equivalent to \SI{500}{~}bps per quantizer (i.e., \SI{6}{~}kbps for 12 quantizers). Given the space limit, we only present the numerical results for LMCodec with 12 RVQ layers and LMCodec models shown in Figure \ref{fig:mushra}.
    \label{tab:accuracy}}
\end{table}

\begin{table}
    \captionsetup{belowskip=-13pt}
    \centering
    \setlength{\tabcolsep}{0pt} % make LaTeX figure out intercol. separation
    \scriptsize
    \begin{tabular}{@{\extracolsep{\fill}} l @{\extracolsep{0.5cm}}cccc}
         \toprule
         \multirow{2}{0.3cm}{$(N_\mathcal{C},N_\mathcal{F})$} & \multicolumn{2}{c}{Transmitting only voices} & \multicolumn{2}{c}{Transmitting non-voices with zero bits} \\
         \cmidrule(lr){2-3} \cmidrule(lr){4-5}
          & \textbf{Entropy} & \textbf{Huffman} & \textbf{Entropy} & \textbf{Huffman} \\
         \midrule \midrule
         $(2,1)$ & \SI{545.6}{~}bps & \SI{554.1}{~}bps  & \SI{303.1}{~}bps  & \SI{307.9}{~}bps \\
         $(3,1)$ & \SI{850.5}{~}bps & \SI{858.6}{~}bps  & \SI{472.1}{~}bps  & \SI{476.6}{~}bps \\
         $(4,2)$ & \SI{1165.6}{~}bps & \SI{1173.7}{~}bps & \SI{647.2}{~}bps & \SI{651.7}{~}bps  \\
        \midrule
         $(1,11)$ & \SI{268.3}{~}bps & \SI{268.7}{~}bps & \SI{149.3}{~}bps & \SI{149.5}{~}bps \\
         $(2,10)$ & \SI{523.7}{~}bps & \SI{530.0}{~}bps & \SI{290.8}{~}bps & \SI{294.3}{~}bps \\
         $(3,9)$ & \SI{816.5}{~}bps & \SI{819.1}{~}bps & \SI{453.2}{~}bps & \SI{454.7}{~}bps \\
         $(4,8)$ & \SI{1108.5}{~}bps & \SI{1129.7}{~}bps & \SI{615.2}{~}bps & \SI{627.0}{~}bps \\
         $(6,6)$ & \SI{1775.2}{~}bps & \SI{1783.6}{~}bps & \SI{985.5}{~}bps & \SI{990.2}{~}bps \\
         $(8,4)$ & \SI{2457.3}{~}bps & \SI{2471.5}{~}bps & \SI{1363.5}{~}bps & \SI{1371.4}{~}bps \\
         $(10,2)$ & \SI{3170.9}{~}bps & \SI{3196.7}{~}bps & \SI{1763.4}{~}bps & \SI{1777.8}{~}bps \\
         $(12,0)$ & \SI{3958.2}{~}bps & \SI{3951.5}{~}bps & \SI{2207.6}{~}bps & \SI{2203.9}{~}bps \\
         \bottomrule
    \end{tabular}
    \caption{Coding performance of LMCodec with VAD.}
    \label{tab:vad}
\end{table}


\subsection{Objective Evaluation}

\looseness -2
We present an objective evaluation on the audio examples from VCTK dataset \cite{Yamagishi2019CSTRVC} in Figure \ref{fig:objective_eval}. First, we demonstrate that the word error rate (WER) and character error rate (CER) are decreasing as the number of quantizers used in the LMCodec increases until around 4-6 quantizers, suggesting that the semantic content is stored in the coarse tokens. To evaluate WER and CER, we use two ASR models from AWS Transcribe service and Conformer model \cite{conformer} trained on LibriSpeech \cite{panayotov2015librispeech}. Second, ViSQOL \cite{chinen2020visqol} and WARP-Q \cite{jassim2021warp}, metrics designed for neural speech codecs, increases and decreases respectively, implying that the fine tokens are responsible for fine-grained acoustic details. Third, SSL-MOS  \cite{cooper2022generalization} shows that the overall \linebreak speech quality improves by increasing the number of quantizers.

\looseness -1
Despite neural speech codecs metrics ViSQOL and WARP-Q indicating worse performance at about 4-6 quantizers, our listening test shows very high quality audio results with small number of quantizers. This suggests that the language model of LMCodec is able to model the distribution of the fine tokens given the coarse tokens reasonably well even if the synthesized fine tokens are different from the ground truth ones. This drives metrics like ViSQOL and WARP-Q down as they primarily rely on the comparison between synthesized audio and its corresponding ground truth reference audio.

\looseness -1
When comparing LMCodec with different total number of quantizers, we first note that the upper bound performance of LMCodec with 6 quantizers is lower than the upper bound performance of LMCodec with 12 or 24 quantizers. However, LMCodec with a lower total number of quantizers reaches better performance faster than LMCodec with a higher total number of quantizers.
