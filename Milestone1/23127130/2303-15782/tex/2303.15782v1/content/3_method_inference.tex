\section{Technical Approach}
\label{sec:method}

In this section, we detail our proposed single-shot detector for articulated objects. Our method consists of two individually learned components: an encoder that predicts latent object codes as well as poses in the camera frame and a decoder that reconstructs objects in a canonical frame that can be transformed into the camera frame through the predicted pose. An overview of our approach is shown in \cref{fig:overview}.
\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{content/assets/imgs/full_diagram_low_res.pdf}
    \caption{Overview of our proposed method. We first encode a stereo image using \cite{laskey2021simnet} to predict a depth and an importance value, a pose as well as a shape and joint code for each pixel using peak detection on the depth map allows us to detect objects which then can be reconstructed given the latent code. Last, to place the objects in camera frame we transform the reconstructed point cloud using the predicted poses at the peaks. On the right side we show the position of the predicted shape codes in a t-SNE visualization of the learned shape codes for the training used as input to our single category- and joint-agnostic decoder. We additionally project each categories mean shape code and use them to reconstruct the objects at the average prismatic and revolute joint state in the training set.}
    \label{fig:overview}
    %\vspace{-0.3cm}
\end{figure*}


\subsection{Encoder}
\label{subsec:encoder}
Our encoder builds upon CenterSnap~\cite{irshad_centersnap_2022} and SimNet~\cite{laskey2021simnet}. For each pixel in our input stereo image $\inputImage$ we predict an importance scalar $\importance$, where a higher value indicates closeness to a 2D spatial center in the image of the object. The full output map of $\importance$ represents a heatmap over objects. 
Additionally, we predict a dense pixel map of canonical 6D poses for each articulated object independent of their articulation state. 
Further, we extend \cite{irshad_centersnap_2022}, which predicted a shape code $\latentShape \in \realspace^{\dimensionsShapeSpace}$, to also predict a joint code $\latentJoint \in \realspace^{\dimensionsJointSpace}$ for each pixel.  These codes can be used to predict the articulation state of the object.

Additionally, while not needed for our full pipeline, to guide the network towards important geometric object features, we also predict a semantic segmentation mask as well as 3D bounding boxes, again on a pixel-level. We use these predictions for constructing our baseline as described in \cref{subsubsec:full_pipeline_baseline}. Last, we also predict a depth map $\depthMap$. The full network architecture is given in \ifthenelse{\boolean{reftosupp}}{\cref{supp:subsec:encoder_architecture}}{Sec.~S.1.1}. % in the supplementary material.

During inference of our full pipeline, given the predicted heatmap of importance values, we use non-maximum suppression to extract peaks in the image. At each peak, we then query the feature map to get the pose, shape, and joint code. We convert our 13-dimensional pose vector to a scale value $\in\realspace$ of the canonical object frame, a position $\in\realspace^{3}$ and using \cite{bregier2021deepregression} to an orientation $\in\realspace^{3 \times 3}$ in the camera frame.
We then use the shape and joint code to reconstruct each object in its canonical object frame using our decoder. After reconstruction, we use the predicted pose to place the object in the camera frame as shown in \cref{fig:overview}. 

\subsection{Decoder}
\label{subsec:decoder}
Given a latent code, the decoder reconstructs object geometry, classifies the discrete joint type (i.e., as prismatic or revolute), and predicts the continuous joint state.
To disentangle the shape of the object from its articulation state, we split the latent code in two separate codes: a shape and a joint code. We assign the same unique shape code  $\latentShape \in \realspace^{\dimensionsShapeSpace} $ to an object instance in different articulation states, where an articulation state is expressed through its own joint code variable $\latentJoint \in \realspace^{\dimensionsJointSpace}$. We structure our decoder as two sub-decoders, one for reconstructing the geometry (\cref{subsubsec:geometry_decoder}) and the other for predicting the joint type $\jointType$ and state $\jointState$ (\cref{subsubsec:joint_decoder}). See \ifthenelse{\boolean{reftosupp}}{\cref{supp:subsec:decoder_architecture}}{Sec.~S.1.2} for a full architecture description.

\subsubsection{Geometry Decoder}
\label{subsubsec:geometry_decoder}
The geometry decoder $\geometryDecoder$ reconstructs objects based on a shape code $\latentShape$ and joint code $\latentJoint$. In principle, the approach is agnostic to the specific decoder architecture as long as it is differentiable with respect to the input latent codes. While there are many potential options such as occupancy maps \cite{mescheder2019occupancy} as adopted in \cite{jiang_ditto_2022}, we use signed distance functions (SDFs) \cite{park_deepsdf_2019} due to the proven performance in \cite{mu_a-sdf_2021, park_deepsdf_2019}.
Specifically, in the case when using SDFs as our geometry decoder, the model takes as input a point in 3D space $\spaceCoordinate$ as well as a shape  $\latentShape$ and joint code $\latentJoint$
\begin{equation}
    \geometryDecoder(\latentShape, \latentJoint, \spaceCoordinate) = \hat{\sdfValue}_{\spaceCoordinate}
\end{equation}
and predicts a value $\hat{\sdfValue}_{\spaceCoordinate}$ that indicates the distance to the surface of the object.

For faster inference, we implement a multi-level refinement procedure~\cite{irshad_2022_shapo}. We first sample query points on a coarse grid and refine them around points that have a predicted distance within half of the boundary to the next point. This step can be repeated multiple times to refine the object prediction up to a level $\objectLoD$. Eventually, we extract the surface of the objects by selecting all query points $\spaceCoordinate$ for which $ |\hat{\sdfValue}_{\spaceCoordinate}| < \sdfValueThreshold $ holds. By taking the derivative 
\begin{equation}
    \normal_{\spaceCoordinate} = \frac{\partial \geometryDecoder(\latentShape, \latentJoint, \spaceCoordinate)}{\partial \spaceCoordinate}
\end{equation}
and normalizing it we get the normal $\hat{\normal}_{\spaceCoordinate}$ at each point $\spaceCoordinate$, which can then be used to project the points onto the surface of the object with $\hat{\spaceCoordinate} = \spaceCoordinate - \hat{\sdfValue}_{\spaceCoordinate} \hat{\normal}_{\spaceCoordinate}$.

\subsubsection{Joint Decoder}
\label{subsubsec:joint_decoder}
As we represent the articulation state of the object implicitly through a joint code $\latentJoint$, we additionally introduce an articulation state decoder $\jointDecoder$ to regress a discrete joint type $\jointType = \{ \text{prismatic}, \text{revolute} \}$ and a continuous joint state $q$:
% , which is dependent on the predicted joint type:
\begin{equation}
    \jointDecoder(\latentJoint) = \hat{\jointType}, \hat{\jointState}
\end{equation} 
We use a multi-layer perceptron with 64 neurons in one hidden layer. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{content/assets/imgs/latent_joint_space_svd_with_imgs.pdf}
    \caption{Intuition for Latent Space Regularization.
    Our main idea is that the joint codes 
    % $\latentJoint^i$ and $\latentJoint^j$
    of two similarly articulated objects should be close. We define the similarity first through the joint type $\jointType$ and second through an exponential distance measure of the joint state $\jointState$. Here, the laptop (a) and the oven (b) have a revolute joint and are similarly wide open around $30\degree$. Compared to that, table (d) has a prismatic joint and thus should not be close to the revolute instances. Contrary to that, the dishwasher (c), has a revolute joint but is opened much more than the other revolute objects and therefore, should be relatively close. The visualization shows a lower dimensional projection of our learned latent joint space trained using our regularization.}
    \label{fig:latent_space_intuition}
    %\vspace{-0.3cm}
\end{figure}

\subsubsection{Backward Code Optimization}
\label{para:backward_code_optimiziation}
For the task of canonical object reconstruction from a set of SDF values at specific query points, we follow the optimization procedure from \cite{mu_a-sdf_2021} to retrieve a shape and joint code. Different from \cite{mu_a-sdf_2021}, we utilize GPU parallelization to optimize multiple code hypotheses at once and pick the best one in the end. Additionally, we do not reset the codes as done in \cite{mu_a-sdf_2021} but rather freeze them for some iterations. In early testing, we discovered that first optimizing for both codes jointly together gives a good initial guess. Freezing the joint code in a second stage of optimization helps such that the shape fits the static part and then last, freezing the shape code such that the joint code can do some fine adjustment to the articulation state of the object. To guide the gradient in the joint code space, we first transform the space using the singular value decomposition of the stacked training joint codes $\latentJoint^{\jointIndex} \+\forall\+ \jointIndex \in 1, \ldots, \jointCount$.
For further information, refer to \ifthenelse{\boolean{reftosupp}}{\cref{supp:sec:backward_optim}}{Sec.~S.2}. % in the supplementary material.
