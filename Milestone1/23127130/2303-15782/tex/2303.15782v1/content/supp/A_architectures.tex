\section{Model Architecture}
\label{supp:sec:model_architectures}
We present our model architecture for the encoder in \cref{supp:fig:encoder_architecture} and for the decoder in \cref{supp:fig:decoder_architecture}. 

\subsection{Encoder}
\label{supp:subsec:encoder_architecture}
{\parskip=0pt
Our encoder builds upon the SimNet-architecture \cite{laskey2021simnet}. The input is a stereo RGB image pair of size $\realspace^{960 \times 512 \times 3}$. Each image gets passed through a shared feature encoder network that outputs a low-dimensional feature map of size $\realspace^{128 \times 240 \times 16}$. This output is then fed into a cost volume which performs approximate stereo matching. Based on the result of the cost volume of size $\realspace^{128 \times 240 \times 32}$, a lightweight head predicts an auxiliary disparity map of size $\realspace^{128 \times 240}$.  
%The output of this stage is a low-resolution disparity image.
Parallel to that, we feed the left image through a separate RGB encoder that also predicts a feature map of size $\realspace^{128 \times 240 \times 32}$. This map, as well as the output of the cost volume, get concatenated and fed into a feature pyramid network which predicts three feature maps of sizes $\realspace^{128 \times 240 \times 32}$, $\realspace^{64 \times 120 \times 64}$, $\realspace^{32 \times 60 \times 64}$. Finally, using these features, each quantity described in \ifthenelse{\boolean{reftomain}}{\cref{subsec:encoder}}{Sec.~3.1} is predicted by its respective output head, including a segmentation mask, 3D bounding box, object pose, full resolution disparity, shape code, and joint code heads.

As in~\cite{laskey2021simnet}, although the stereo input for sim-to-real transfer has benefits for perceiving objects in harsh lighting conditions and for transparent or reflective objects, a RGB-D version 
% can be trained similar to~\cite{xie2020}.
could be trained as well (see \cref{supp:subsec:rgbd_version}).
}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{content/supp/img/simnet.pdf}
    \caption{Encoder Architecture based on \cite{laskey2021simnet}}
    \label{supp:fig:encoder_architecture}
\end{figure*}

\subsection{Decoder}
\label{supp:subsec:decoder_architecture}
Our decoder is split into two sub-decoders. A geometry decoder (see \ifthenelse{\boolean{reftomain}}{\cref{subsubsec:geometry_decoder}}{Sec~3.2.1}) based on DeepSDF \cite{park_deepsdf_2019} and a joint decoder (see \ifthenelse{\boolean{reftomain}}{\cref{subsubsec:joint_decoder}}{Sec.~3.2.2}). We detail both the architecture in the subsequent paragraphs.

\noindent \textbf{The geometry decoder} is a deep multi-layer perceptron consisting of four layers. The first layer takes a shape code $\latentShape$ and joint code $\latentJoint$ as input. Before the second and last layer, we concatenate the space coordinate $\spaceCoordinate$ for which we want to retrieve the SDF-value $\sdfValue$ with the output of the previous layer. Following the findings in \cite{mu_a-sdf_2021}, we again input the joint code $\latentJoint$ before the second last layer. For the exact feature vector dimensions see \cref{supp:fig:decoder_architecture}. As an activation function, we use ReLU for all except the last layer which uses tanh. 
Exploring exact input positions for shape code $\latentShape$, joint code $\latentJoint$, and space coordinate $\spaceCoordinate$, could be a topic of further research.

\noindent \textbf{The joint decoder} only takes a joint code $\latentJoint$ as input and feeds it through a single layer outputting a feature vector with 64 dimensions. This vector is then used to regress the articulation state, consisting of the continuous joint state $\jointState$ (no activation) and the discrete joint type $\jointType$ (Sigmoid activation).

An overview of the used loss scaling hyperparameters is given in \cref{supp:tab:decoder_scaling_training}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{content/supp/img/decoder.pdf}
    \caption{Decoder Architecture. The numbers indicate the size of the respective feature vector. Each arrow represents a layer of a multi-layer perceptron. For the geometry decoder, except for the first layer, the input to a layer always has a size of 512. The output dimensions vary depending on auxiliary inputs.}
    \label{supp:fig:decoder_architecture}
\end{figure}

\begin{table}
    \centering
    \begin{tabular}{lc}
        \toprule
        Scaling Variable & Value \\
        \midrule
        $\lossScaling_{\indexCodeRegularizer, \latentJoint, \text{pre}}$ & 0.1 \\
        $\lossScaling_{\indexJointRegularizer, \text{pre}}$ & 1.0 \\
        $\lossScaling_{\indexCodeRegularizer, \latentShape}$ & 0.0001 \\
        $\lossScaling_{\indexCodeRegularizer, \latentJoint}$ & 0.001 \\
        $\lossScaling_{\indexReconstruction}$ & 1.0 \\
        $\lossScaling_{\indexJointType}$ & 0.001 \\
        $\lossScaling_{\indexJointState}$  & 0.1 \\
        $\lossScaling_{\indexJointRegularizer}$ & 0.1 \\
        \bottomrule
    \end{tabular}
    \caption{Scaling Hyperparameters for Decoder Training.}
    \label{supp:tab:decoder_scaling_training}
\end{table}
