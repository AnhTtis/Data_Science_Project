\section{Learned Joint Code Space}
\label{supp:sec:joint_code_space}
{\parskip=0pt
In this section, we visualize and compare the resulting learned latent joint space using our in \ifthenelse{\boolean{reftomain}}{\cref{subsubsec:joint_space}}{Sec.~3.3.1} introduced regularization against naively training it. In \cref{supp:fig:latent_joint_spaces}, we visualize the learned joint codes of the training results for \textit{\ourName} and \textit{\ourName-No-Enf} from \ifthenelse{\boolean{reftomain}}{\cref{subsec:canonical_reconstruction_task}}{Sec.~4.2}. When comparing both visualizations, we can explain the worse performance of \textit{\ourName-No-Enf} in \cref{tab:decoder_results}. 

\ourName{} trained without regularization struggles to correctly align the spaces such that joint codes belonging to the same articulation state, independent of the object, are close and show a low variance. The decoder rather learns to represent the final geometry of the articulated object jointly through both codes, the shape code $\latentShape$ and joint code $\latentShape$ instead of disentangling one from the other. One could argue that this case is similar to not splitting the codes. Compared to that, when using our proposed regularization, we learn a cleaner disentanglement between the shape and the articulation state of the object. Joint codes of similar articulation states in the training set are arranged closer in the latent joint embedding and thus, the variance in the y-direction across all plots on the left side of \cref{supp:fig:latent_joint_spaces} (a) is much lower when compared to training without our regularization in \cref{supp:fig:latent_joint_spaces} (b). Moreover, two distinct clusters are visible (prismatic and revolute) whereas without \ourName{}s regularization different joint types overlap. 
}
\begin{figure*}
    \centering
     \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{content/supp/img/with_enforcement.png}
        \caption{Using \ourName{}s Regularization.}
        \label{supp:subfig:carto_latent_joint_space}
    \end{subfigure}\\
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{content/supp/img/without_enforcement.png}
        \caption{Only norm Regularization.}
    \end{subfigure}
    \caption[]{Comparison of Learned Joint Code Space. We compare the learned embedding of training joint codes when using our proposed regularization (a) against naively just regularizing the norm (b). An articulation state is expressed two-fold. First, by its form to represent the joint type. Here upside-down triangles stand for revolute and cross for prismatic joint types. Second, the form is colored by its joint state according to the scale shown on the right. In the left figure, each plot represents one component of the joint code $\latentJoint \in \realspace^{16}$. In the $i$-th plot, we plot the $i$-th component of all training joint codes on the y-axis against their associated known joint state on the x-axis. Additionally, we overlay the in 
    \ifthenelse{\boolean{reftomain}}{\cref{subsec:inverse_joint_decoder}}{Sec.~3.3.3} explained polynomial functions. In the right figure, we show a two-dimensional projection based on singular value decomposition of all training joint codes.}
    \label{supp:fig:latent_joint_spaces}
\end{figure*}
