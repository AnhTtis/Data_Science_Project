\section{Additional Experimental Results}
\label{supp:sec:additional_metric}

\input{content/assets/tables/canonical_reconstruction_full}

\begin{table*}
    \small
    \caption{Reconstruction and Articulation State Prediction Results when using A-SDF \cite{mu_a-sdf_2021} with the Proposed Test-Time-Adaptation. \textit{$^*$The joint state error mean is only reported across the revolute categories, as there is only one prismatic category.}}    
    \label{supp:tab:asdf_tta}
    \centering
    \footnotesize
    \begin{tabular}{l|ccccccc|c|c}
        \toprule
        Method & Dishwasher & Laptop & Microwave & Oven & \makecell{Refrigerator} & Table & \makecell{Washing\\Machine} & \makecell{Instance\\Mean$^*$} & \makecell{Category\\Mean$^*$} \\
        \midrule
%            \#Train & 900 & 1000 & 500 & 350 & 500 & 950 & 400 & 4600 & N/A\\
%            \#Test & 250 & 250 & 150 & 100 & 150 & 250 & 100 & 1250 & N/A\\
        Chamfer Distance ($\downarrow$) & 0.101 & 1.035 & 0.529 & 0.451 & 1.383 & 64.097 & 0.332 & 13.339 & 9.704 \\
        %\midrule
        Joint State Error ($\downarrow$) & $19.387\degree$ & $18.675\degree$ & $58.088\degree$ & $25.432\degree$ & $20.700\degree$ & $0.552\si{m}$ & $51.467\degree$ & $29.024\degree$ & $32.292\degree$ \\
        \bottomrule
    \end{tabular}
\end{table*}

In this section, we present additional metrics for our experiments. Namely, using A-SDFs proposed test time adaption as well as the Chamfer distance and joint state error for the full pipeline experiment. Also, in addition to \ifthenelse{\boolean{reftomain}}{\cref{tab:decoder_results_short}}{Tab.~2}, we report the more fine-grained category-level metrics in \cref{tab:decoder_results}.

\subsection{Canonical Reconstruction Task: A-SDF TTA}
\label{supp:subsec:a-sdf_tta}
{\parskip=0pt
In our experiments (see \ifthenelse{\boolean{reftomain}}{\cref{subsec:canonical_reconstruction_task}}{Sec.~4.2}), the proposed test time adaptation (TTA) \cite{mu_a-sdf_2021} did not prove to be stable. We report the results in \cref{supp:tab:asdf_tta}. While for some object instances TTA reduces the Chamfer distance, for the table category TTA does not prove to be robust. Additionally, the joint state error increases substantially. Both behaviors are reasonable when reflecting on the proposed TTA. When jointly optimizing the input shape code, the joint state, and network weights, the entire network will overfit to the single given geometry. Thus, it is easier to achieve a lower Chamfer distance. Whereas, the joint state variable becomes unbound from other examples and can be optimized freely, losing its meaning and therefore, potentially resulting in a high joint state error.

The proposed TTA is still promising and with further investigation into how to mitigate the aforementioned problems, it can prove to be an ideal tool for reconstructing (articulated) objects in the wild \cite{irshad_2022_shapo}.
}

\subsection{Extended Metrics for Full Pipeline}
\label{supp:subsec:extend_metrics}
{\parskip=0pt
In addition to the tabular values reported in \ifthenelse{\boolean{reftomain}}{\cref{tab:full_pipeline_synthetic}}{Tab.~3a}, we present the respective mAP curve in \cref{supp:fig:3d_metrics}. The results highlight even more that an optimization-based two-stage approach suffers from its partial input. The two counter objects, laptops and microwaves, which are free-standing and thus much more points for reconstruction are available get reconstructed much better compared to other objects. On the other hand, for these objects, the predicted rotation is much worse. This can be rooted in the fact that for all other objects, we can learn a strong prior of the rotation being roughly camera facing, whereas, for laptops and microwaves the range of the possible rotation is much higher as they are placed freely on top of the counter.

While in \ifthenelse{\boolean{reftomain}}{\cref{tab:full_pipeline_synthetic}}{Tab.~3a} and previously we only discussed the overall 3D IoU and pose error, which gives a holistic evaluation of the full pipeline, we additionally report object-centric L2-Chamfer distances (similar to \cite{irshad_centersnap_2022, irshad_2022_shapo}) multiplied by $10^3$, as well as the joint state error in \cref{supp:fig:chamfer_joint_metrics}. Since this is an object-centric evaluation and should not evaluate the detection quality, we are very forgiving in selecting our detection matches. For each scene, we calculate our spatial 2D detections and retrieve the ground-truth spatial 2D detections from the heatmap, we then match the predicted and ground truth detections by solving a linear sum assignment problem, ignoring unmatched detections (either ground-truth or predicted). For each matched detection we then reconstruct the object as before using our geometry decoder and retrieve the joint through our joint decoder. We then calculate the Chamfer distance between the predicted points and the ground-truth points and compare the joint states.

In this experiment, we observe the same trend as for 3D IoU. One major difference is that \textit{A-SDF-GT} reconstructs laptops more accurately compared to \textit{A-SDF} and \textit{\ourName} which can be attributed to laptops having the least occlusion (either through self-occlusion or other objects).
}
\begin{figure*}[b]
    \centering
    \begin{subfigure}{0.85\textwidth}
        \includegraphics[width=\textwidth]{content/supp/img/asdf_gt_mAP.pdf}
        \caption{A-SDF-GT}
    \end{subfigure}\\
    \begin{subfigure}{0.85\textwidth}
        \includegraphics[width=\textwidth]{content/supp/img/asdf_no_gt_mAP.pdf}
        \caption{A-SDF}
    \end{subfigure}\\
    \begin{subfigure}{0.85\textwidth}
        \includegraphics[width=\textwidth]{content/supp/img/ours_mAP.pdf}
        \caption{\ourName}
    \end{subfigure}
    \caption[]{Detailed metrics for the experiment presented in \ifthenelse{\boolean{reftomain}}{\cref{subsec:full_pipeline_task}}{Sec.~4.3}. We report the average precision for the 3D IoU and the pose prediction for each category in our test set as well as the mean over all instances. It can be observed that overall the mean 3D IoU is lower for \textit{\ourName{}} compared to \textit{A-SDF-GT} and \textit{A-SDF}. For \textit{A-SDF-GT} the laptop and microwave category stands out as they are mostly placed on counters and thus they are less occluded than other objects. As expected, the poses predicted by \textit{A-SDF} and \textit{\ourName{}} are similar as they both use the same pose map predicted by our encoder.}
    \label{supp:fig:3d_metrics}
\end{figure*}

\begin{figure*}[b]
    \centering
    \begin{subfigure}{0.85\textwidth}
        \includegraphics[width=\textwidth]{content/supp/img/asdf_gt_mAP_chamfer_joint.pdf}
        \caption{A-SDF-GT}
    \end{subfigure}\\
    \begin{subfigure}{0.85\textwidth}
        \includegraphics[width=\textwidth]{content/supp/img/asdf_no_gt_mAP_chamfer_joint.pdf}
        \caption{A-SDF}
    \end{subfigure}\\
    \begin{subfigure}{0.85\textwidth}
        \includegraphics[width=\textwidth]{content/supp/img/ours_mAP_chamfer_joint.pdf}
        \caption{\ourName{}}
    \end{subfigure}
    \caption[]{Additional metrics for the experiment presented in \ifthenelse{\boolean{reftomain}}{\cref{subsec:full_pipeline_task}}{Sec.~4.3}. In addition to the metrics already reported in \ifthenelse{\boolean{reftomain}}{\cref{subsec:full_pipeline_task}}{Sec.~4.3}, we report the more fine-grained object-centric Chamfer distance as well as the joint state prediction error. Both metrics show a similar trend as the more coarse 3D IoU. One can observe though, that for the laptop category \textit{A-SDF-GT} performs significantly better than all other categories. Compared to that \textit{A-SDF}, which uses a predicted segmentation masks, does not show this special behavior for laptops. As laptops are small and the segmentation mask is very thin, this gap in performance highlights potential failure cases of an optimization-based method due to imperfect segmentation masks.}
    \label{supp:fig:chamfer_joint_metrics}
\end{figure*}

\subsection{\ourName{} RGB-D Version}
\label{supp:subsec:rgbd_version}
{\parskip=0pt
In addition to the proposed stereo-RGB input version of \textit{\ourName{}}, we also evaluated and tested an RGB-D version \textit{\ourName{}-D}. 
We report quantitative results on the same synthetic dataset in \cref{supp:tab:stereo_rgbd} as well as compare the detections qualitatively in \cref{supp:fig:stereo_rgbd_comparison}. 

Quantitatively, the \textit{\ourName{}-D} performs slightly better compared to our proposed stereo RGB version. This is to be expected given that \ourName{} needs to learn the notion of depth first whereas \textit{\ourName{}-D} does not. Contrary to this observation, in our real world experiments, we do not get a single meaningful detection using the RGB-D input version (see \cref{supp:fig:stereo_rgbd_comparison}). Thus, overall, we decided for the proposed stereo version of \ourName{}.
}
\begin{table}
    \centering
    \footnotesize
    \caption{Full Scene Reconstructions Results with RGB-D Input.}
    \label{supp:tab:stereo_rgbd}
    \begin{tabular}{l|cccc}
        \toprule
            Method & IOU25 $\uparrow$ & IOU50 $\uparrow$ & $10\si{\degree}10\si{cm} \uparrow$ & $20\si{\degree}30\si{cm} \uparrow$  \\
        \midrule
            \ourName & {64.0} & {31.5} & \textbf{28.7} & {76.6} \\
            \ourName-D & \textbf{67.8} & \textbf{38.2} & {27.0} & \textbf{84.7} \\
        \bottomrule
    \end{tabular}
\end{table}%

\begin{figure*}[b]
    \centering
    \includegraphics[width=\textwidth]{content/supp/img/qualitative_comparison_stereo_rgbd.jpg}
    \caption{Stereo-RGB Image (Left) vs. RGB-D Image (Right) Input. The first row shows a successful detection and reconstruction of \ourName{} in an office kitchen environment. Second row shows a reconstruction of a cabinet. Eventhough, \ourName{} has never seen objects from this category it highlight its generalization beyond the trained categories. The third and fourth row show two failure cases of either no detection at all (third row) or a misdetection of a laptop on the kitchen counter (fourth row). CARTO with RGB-D input is not able to reconstruct any objects.}
    \label{supp:fig:stereo_rgbd_comparison}
\end{figure*}
