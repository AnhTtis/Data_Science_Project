\section{Introduction}
\label{sec:intro}

Reconstructing 3D shapes and inferring the 6D pose and sizes of objects from partially observed input observations remains a fundamental problem in computer vision with applications in Robotics~\cite{mees2019self, jiang_ditto_2022, laskey2021simnet, jiang2021synergies} and AR/VR~\cite{zhang2021holistic, irshad_2022_shapo}. This object-centric 3D scene understanding problem is challenging and under-constrained since inferring 6D pose and shape can be ambiguous without prior knowledge about the object of interest.

Previous work has shown that it is possible to perform category-level 3D shape reconstruction and 6D pose estimation in real-time~\cite{irshad_centersnap_2022}, enabling the reconstruction of complete, fine-grained 3D shapes and textures. However, there are a wide variety of real-world objects that do not have a constant shape but can be articulated according to the object's underlying kinematics. There has been great progress in articulated object tracking~\cite{weng_captra_2021, sturm_probabilistic_2011, heppert_category-independent_2022, jain_screwnet_2021} and reconstruction~\cite{jiang_ditto_2022, noguchi_watch_nodate} from a sequence of observations. However, a sequence of observations is cumbersome since it often requires prior interaction with the environment.  In contrast, object reconstruction from a single stereo image through inferring latent information about an object a priori enables both grasping and manipulation of previously unknown articulated objects.  Additionally, estimates from a single image can also serve as a good initial guess for object tracking approaches~\cite{weng_captra_2021}.

Previous approaches to articulated object reconstruction from a single observation use a two-stage approach~\cite{liu_akb-48_2022} where first objects are detected using, e.g., Mask-RCNN \cite{he2017mask}. Then, based on the detection output, object properties, e.g. part-poses and NOCS maps~\cite{wang_normalized_2019}, are predicted and the object is reconstructed using backward optimization~\cite{mu_a-sdf_2021}. Such an approach is complex, error prone, does not scale across many categories, and does not run in real-time.

To mitigate the aforementioned challenges, building on ideas from~\cite{irshad_centersnap_2022} - a single-shot approach to output complete 3D information (3D shape, 6D pose, and sizes of multiple objects) on a per-pixel manner - we present ``\ourNameFull" (\ourName{}). 
First, extending \cite{mu_a-sdf_2021}, we train a robust category- and joint-agnostic 3D decoder~\cite{fan2017point, park_deepsdf_2019, mu_a-sdf_2021} by learning disentangled latent shape and joint codes. The shape code encodes the canonical shape of the object while the joint code captures the articulation state of the object consisting of the type of articulation (e.g., prismatic or revolute) and the amount of articulation (i.e. joint state). To disentangle both codes we impose structure among our learned joint codes by proposing a physically grounded regularization term. 
Second, in combination with our stereo RGB image encoder we can do inference in a \textit{single-shot manner} to detect the objects' spatial centers, 6D poses and sizes as well as shape and joint codes. The latter two can then be used as input to our category- and joint-agnostic decoder to directly reconstruct all detected objects.

To evaluate \ourName{}, we first evaluate the reconstruction and articulation state prediction quality of our category- and joint-agnostic decoder and compare against decoders trained on a single category. In an ablation study we show the necessity of our proposed joint code regularization over naively training the joint codes. 
We then quantitatively compare our full pipeline to a two-stage approach on synthetic data and show qualitative results on a new real-world dataset.

Our main contributions can be summarized as follows:
\begin{itemize}[noitemsep, topsep=0pt]
    \item An approach for learning a shape and joint decoder jointly in a category- and joint-agnostic manner.
    \item A single shot method, which in addition to predicting 3D shapes and 6D pose, also predicts the articulation amount and type (prismatic or revolute) for each object. 
    \item Large-scale synthetic and annotated real-world evaluation data for a set of articulated objects across 7 categories. 
    \item Training and evaluation code for our method.
\end{itemize}
