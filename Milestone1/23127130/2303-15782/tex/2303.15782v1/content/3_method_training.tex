\subsection{Training Protocol}
To train \ourName{}, we first train the decoder as it provides ground truth training labels for the shape and joint code supervision of our encoder. Once shape and joint code labels are obtained for the objects in the dataset, we then train our encoder to predict the latent codes in addition to object pose. Thus, to adhere with our training procedure, we will first explain how to train our decoder and then our encoder. 
\subsubsection{Decoder}
Given a training set of $\objectCount$ objects each in $\jointCount$ articulation states, we denote the $\objectIndex$-th object in its $\jointIndex$-th articulation state as $\object_{\objectIndex, \jointIndex}$. As during training, a fixed association between each object and its latent codes is given, we can uniquely identify $\object_{\objectIndex, \jointIndex} = ( \latentShape^{\objectIndex}, \latentJoint^{\jointIndex})$ as a tuple of both. This allows us to pass the gradient all the way to the codes themselves and thus, the embedding spaces rearrange them accordingly. Similar to \cite{mu_a-sdf_2021}, during training we regularize the codes through minimizing the L2-norm
\begin{equation}
    \label{eqn:loss_code_reg}
    \lossGeneralFunction_{\indexCodeRegularizer}(\latentVariable) =
    \| \latentVariable \|,
\end{equation}
where $\latentVariable$ is either $\latentShape$ or $\latentJoint$.

{\parskip=5pt
\noindent\textbf{The geometry decoder} described in \cref{subsubsec:geometry_decoder} is trained on a set of query points $\spaceCoordinate$ close to the object surface sampled as in \cite{park_deepsdf_2019}.}
We define our reconstruction loss $\lossGeneralFunction_{\indexReconstruction}$ as in \cite{park_deepsdf_2019} but use a leaky clamping function \begin{equation}
    \leakyClamp(s|\leakyThreshold, \leakySlope) = 
    \begin{cases} 
        s & |s| \leq \leakyThreshold, \\
        \leakySlope s &  |s| > \leakyThreshold
    \end{cases}
\end{equation}
which is conceptually similar to a leaky ReLU by instead of hard clamping values above a threshold $\leakyThreshold$, we multiply it by a small factor $\leakySlope$. Initial testing revealed a more stable training. 
Our reconstruction loss at one query point $\spaceCoordinate$ is now given by:
\begin{align}
    \label{eqn:loss_reconstruction}
    & \lossGeneralFunction_{\indexReconstruction}\left( \latentShape, \latentJoint, \spaceCoordinate, \sdfValue_{\spaceCoordinate} \right) = \\
    & \quad \left| 
        \leakyClamp(
            \geometryDecoder(\latentShape, \latentJoint, \spaceCoordinate)
            | \leakyThreshold, \leakySlope
        )
        -
        \leakyClamp(
            \sdfValue_{\spaceCoordinate}
            | \leakyThreshold, \leakySlope
        )
    \right|, \nonumber
\end{align}
where $\sdfValue_{\spaceCoordinate}$ is the ground truth distance to the surface.

{\parskip=5pt
\noindent\textbf{The joint decoder} introduced in \cref{subsubsec:joint_decoder} is jointly trained with the aforementioned geometry decoder. For the joint type loss $\lossGeneralFunction_{\indexJointType}$, we use cross entropy between the predicted joint type $\hat{\jointType}$ and ground truth $\jointType$ and for the joint state loss $\lossGeneralFunction_{\indexJointState}$ the L2-norm between the predicted joint state $\hat{\jointState}$ and ground truth $\jointState$.}


{\parskip=5pt
\noindent\textbf{Joint Space Regularization}: 
\label{subsubsec:joint_space}
One core contribution of our approach is how we impose structure in our joint code space during our decoder training. Here, we enforce the same similarity of latent codes as their corresponding articulations states have. A visualization of the underlying idea is shown in \cref{fig:latent_space_intuition}.
Formally, given the joint codes $\latentJoint^{k}$ and $\latentJoint^{l}$ encoding two different articulation states $k, l \in 1, \ldots, \jointCount$, we define the similarity between them in latent space as 
\begin{equation}
    \similarityFunctionLatent \left( \latentJoint^{k}, \latentJoint^{l} \right) =
    \exp \left(- \frac{\| \latentJoint^{k} - \latentJoint^{l} \| }{\sigma} \right). % Sigma == 1
\end{equation}
Similarly, we define the respective similarity in real joint space, considering the joint types $\jointType^k$ and $\jointType^l$ and the joint states $\jointState^k$ and $\jointState^l$, through
\begin{align}
    \label{eqn:joint_sim_real}
    &\similarityFunctionReal \left( \left( \jointType^k, \jointState^k \right), \left( \jointType^l, \jointState^l \right) \right) = \\
    &\qquad\qquad \begin{cases}
        \exp \left(
            - \left( \frac{\jointState^k-\jointState^l}{\sigma_\jointType} \right)^2 
        \right) & \jointType^k = \jointType^l \\
        0 & \jointType^k \neq \jointType^l, \nonumber
    \end{cases}
\end{align}
where $\sigma_\jointType$ is a joint type specific scaling.
By minimizing the L1-norm between both similarity measurements 
\begin{align}
    \label{eqn:joint_code_reg_single}
    & \lossGeneralFunction_{\indexJointRegularizer} \left( 
        \latentJoint^{k}, \latentJoint^{l} \right) =   \\
    & \quad \left|
            \similarityFunctionLatent \left( \latentJoint^{k}, \latentJoint^{l} \right)
            -
            \similarityFunctionReal \left( \left( \jointType^k, \jointState^k \right), \left( \jointType^l, \jointState^l \right) \right)
        \right|  \nonumber
\end{align}
we enforce that the latent similarities are similarly scaled as their real similarities. 

We scale this formulation to all articulation states in the training set as described below. Calculating $\similarityFunctionReal$ can be done once in a pre-processing step for all articulation state pairs $k,l \in 1, \ldots, \jointCount$ resulting in a matrix $\similarityMatrix_\indexReal \in \realspace^{\jointCount \times \jointCount}$. Similarly, calculating all $\similarityFunctionLatent$-pairs can be efficiently implemented as a vector-vector product. We denote the resulting matrix as $\similarityMatrix_\indexLatent \in \realspace^{\jointCount \times \jointCount}$. \cref{eqn:joint_code_reg_single} now simplifies to\looseness=-1
\begin{align}
    \label{eqn:joint_code_reg}
    \lossGeneralFunction_{\indexJointRegularizer} =  
    \frac{
    \left|
        \similarityMatrix_\indexLatent
            -
        \similarityMatrix_\indexReal
    \right|}{\jointCount^2}.
\end{align}
Through this efficient calculation, optimizing this loss term comes with almost no overhead during training. This concept of similarity can be extended for arbitrary kinematic graphs.
}

{\parskip=5pt
\noindent\textbf{Pre-Training}: Before we start training our full decoder, we only optimize our joint codes $\latentJoint^{\jointIndex}\+\forall\+\jointIndex \in 1,\ldots, \jointCount$. The pre-training helps with learning the full decoder as our joint codes are already more structured and thus, it is easier to learn the shape and joint code disentanglement. 
In the pre-training, we minimize
\begin{equation}
    \lossGeneralFunction_{\text{pre}} = 
    \lossScaling_{\indexCodeRegularizer, \latentJoint, \text{pre}} \lossGeneralFunction_{\indexCodeRegularizer, \latentJoint}
    + \lossScaling_{\indexJointRegularizer, \text{pre}} \lossGeneralFunction_{\indexJointRegularizer},
\end{equation}
where $\lossGeneralFunction_{\indexCodeRegularizer, \latentJoint}$ is the default norm regularization from \cref{eqn:loss_code_reg} and $\lossGeneralFunction_{\indexJointRegularizer}$ was introduced in \cref{eqn:joint_code_reg}.
}

{\parskip=5pt
\noindent\textbf{Loss Function}: Given an object $\object_{\objectIndex, \jointIndex}$, we express our full decoder loss as} 
\begin{align}
    \label{eqn:full_loss}
    \lossGeneralFunction = &
    \lossScaling_{\indexCodeRegularizer, \latentShape} \lossGeneralFunction_{\indexCodeRegularizer, \latentShape}
    + \lossScaling_{\indexCodeRegularizer, \latentJoint} \lossGeneralFunction_{\indexCodeRegularizer, \latentJoint}
    + \lossScaling_{\indexReconstruction} \lossGeneralFunction_{\indexReconstruction} \\
    & \quad+ \lossScaling_{\indexJointType} \lossGeneralFunction_{\indexJointType}
    + \lossScaling_{\indexJointState} \lossGeneralFunction_{\indexJointState} , \nonumber
\end{align}
where $\lossGeneralFunction_{\indexCodeRegularizer, \latentVariable}$ are the shape and joint code regularization from \cref{eqn:loss_code_reg}, $\lossGeneralFunction_{\indexReconstruction}$ is the reconstruction loss introduced in \cref{eqn:loss_reconstruction}, $\lossGeneralFunction_{\indexJointType}$ and $\lossGeneralFunction_{\indexJointState}$ are the joint type and state loss.
We jointly optimize $\lossGeneralFunction$ for the latent shape and joint code as well as the network parameters of the geometry decoder and joint decoder
using ADAM \cite{Kingma2015AdamAM} for 5000 epochs.
Our new joint code regularizer loss $\lossGeneralFunction_{\indexJointRegularizer}$ introduced in \cref{eqn:joint_code_reg} is minimized at the end of each epoch separately scaled by $\lossScaling_{\indexJointRegularizer}$. All $\lossScaling$ variables are scalars to balance the different loss terms and are reported in \ifthenelse{\boolean{reftosupp}}{\cref{supp:tab:decoder_scaling_training}}{Tab.~S.1}. 


\subsubsection{Encoder}
Using \cite{laskey2021simnet} we generate a large-scale dataset in which we annotate each pixel with its respective ground truth value from the simulation as described in \cref{subsec:encoder}. For annotating the shape codes we directly use the results of our previous encoder training whereas for the joint code we use 
our inverse mapping explained in \cref{subsec:inverse_joint_decoder} to retrieve joint codes for arbitrary sampled articulation states.

\subsubsection{Inverse Joint Decoder}
\label{subsec:inverse_joint_decoder}
To solve the inverse problem, given an articulation state for which we want to retrieve a joint code, we fit polynomial functions in the learnt joint code space. With the help of this mapping, we can retrieve arbitrary joint codes which then can be combined with a shape code to reconstruct objects in novel articulation states which have not been seen during the decoder training. Additionally, the mapping provides joint code training labels for the encoder. 

We describe the full mapping as a function $\jointToCodeFunction (\jointType, \jointState) = \latentJoint$ that takes a joint type $\jointType$ and joint state $\jointState$ as input and outputs a joint code $\latentJoint$. We leverage the fact that after decoder training, we learned a joint code $\latentJoint^{\jointIndex}$ for each known training articulation state. We now define individual mappings for each joint type $\jointType$ the following way. We will treat each latent dimension $\dimensionsIndex$ separately. For each dimension $\dimensionsIndex$, we fit a polynomial function $\jointToCodeFunction^{\jointType, \dimensionsIndex} (\jointState)$ of varying degree $\polynomDegree$ through all point tuples $(\jointState^{\jointIndex}, \latentJoint^{\jointIndex}(\dimensionsIndex)) \+\forall\+ \jointIndex \in 1, \ldots, \jointCount$. 

The final function
\begin{equation}
    \jointToCodeFunction (\jointType, \jointState) = 
    \begin{bmatrix}
        \jointToCodeFunction^{\jointType, 1} (\jointState) \\
        \vdots \\
        \jointToCodeFunction^{\jointType, \dimensionsJointSpace} (\jointState)
    \end{bmatrix}
\end{equation}
is then given by evaluating the polynomials individually and stacking the results into a vector.
The exact choice of $\polynomDegree$ is not important as long as the amount of joint codes to fit to is much higher than the potential dimensions of the polynomial $\polynomDegree \ll \jointCount$. Thus, we fixed $\polynomDegree = 5$ for all of our experiments. 
A visualization of our learned latent joint space and the fitted polynomials is given in \ifthenelse{\boolean{reftosupp}}{\cref{supp:subfig:carto_latent_joint_space}}{Fig.~S.3a}.
