\section{Related Work}
\label{sec:related_work}
Related work to \ourName{} includes neural fields for reconstruction, implicit reconstructions of non-rigid objects and articulated object detection, pose estimation, and reconstruction.

\noindent\textbf{Neural Fields for Reconstruction}:
Neural fields, i.e. coordinate-based multi-layer perceptrons~\cite{xie2021neural}, have become a popular method for reconstruction in recent years. These methods encode continuous functions that model various scene properties, such as Signed Distance~\cite{park_deepsdf_2019}, radiance~\cite{mildenhall2021nerf}, and occupancy~\cite{mescheder2019occupancy}. Variations of these include hybrid discrete-continuous representations that employ an external data structure, i.e. a grid or an octree, to partition the implicit function~\cite{peng2020convolutional,zakharovroad}. The encoded shape can then be extracted via sphere tracing~\cite{liu2020dist} after querying the implicit function repeatedly. Recent advances in differential rendering have enabled learning of shapes, as well as other scene properties such as appearance, only from images and without the need for 3D supervision~\cite{mildenhall2021nerf}. Our approach falls into the paradigm of using neural fields for articulated object reconstruction and further learns a complete system for detection, pose estimation, and articulated shape reconstruction from a single observation.

\noindent\textbf{Implicit Reconstruction of Non-Rigid Objects}:
Going beyond static scenes with rigid objects,~\cite{bozic2020deepdeform} handle dynamic scenes while~\cite{su_-nerf_2021,palafox2022spams} focus on reconstructing humans by leveraging their strong shape and kinematic prior as well as the amount of readily available datasets. \cite{yang_lasr_2021} propose a general reconstruction framework to reconstruct any non-rigid entity (i.e. humans, animals, or objects) given only an RGB-video without requiring a category-specific shape template, while~\cite{lei_cadex_2022} focus on point cloud input data and split the prediction into a canonical shape and a deformation field. One downside of general reconstruction methods is that they do not leverage the rigidity and kinematic constraints of articulated objects. To explicitly use such priors,~\cite{noguchi_watch_nodate} propose a method that processes a multi-view sequence of a moving object and discovers its parts as well as their respective reconstruction and kinematic structure in an unsupervised way. Going one step further,~\cite{wei_self-supervised_nodate} learn a shape and appearance prior for each category which allows them to model accurate reconstructions of articulated objects with only 6 given views. Similarly,~\cite{tseng_cla-nerf_2022} propose a Neural Radiance Field (NeRF) based method that can reconstruct objects on a category level given some images of an object. Also leveraging a learned shape prior over objects of the same category,~\cite{mu_a-sdf_2021} reconstructs articulated objects using only a single observation by optimizing for a latent shape code; similarly to~\cite{tseng_cla-nerf_2022}, their method is only tested on revolute objects.

Our approach also represents objects through low-dimensional codes. However, by disentangling the shape from the articulation state in our code, our method becomes category- and joint-agnostic.  Other multi-category models, such as \cite{jiang_ditto_2022}, reconstruct objects given a point cloud in two different articulation states, which limits the approach to objects with a single joint; \cite{nie_structure_2022} uses many observations before and after articulation to reconstruct an object. Most similar to our disentangled representation, \cite{xu_unsupervised_2022} learns a latent space in which part-pairs are close if one part-pair can be transformed into another through a valid joint transformation.

\noindent\textbf{Articulated Object Detection, Pose Estimation and Reconstruction}: 
Work in articulated object detection and pose estimation typically first requires the detection of individual parts and their respective poses from a sequence of images demonstrating the articulation \cite{weng_captra_2021, sturm_probabilistic_2011, heppert_category-independent_2022, jain_screwnet_2021, jiang_ditto_2022} or from a single image~\cite{liu_akb-48_2022, li_category-level_2020, michel_pose_2015}.~\cite{li_category-level_2020} combines this part-level view of articulated objects with a holistic object-centric view as done for rigid objects \cite{wang_normalized_2019}. Similarly, our work predicts the poses for articulated objects in the scene in a single pass from a single stereo or RGB-D image, without the need to detect individual parts first. Most similar to us,~\cite{irshad_centersnap_2022,irshad_2022_shapo} also detect the pose, shape and scale of multiple objects from an RGB-D observation via a single-stage approach. Both methods perform category-agnostic detection of unseen object instances at test time, however, they are limited to rigid objects, with~\cite{irshad_centersnap_2022} using a point cloud decoder for shape reconstruction, while~\cite{irshad_2022_shapo} employs a latent shape and appearance prior. Our method also performs category-agnostic object detection and reconstruction, and we extend~\cite{irshad_centersnap_2022,irshad_2022_shapo} to handle articulated objects of multiple types in a single network forward pass, thus enabling fast and accurate articulate shape, pose and size estimation from a single stereo image. 


