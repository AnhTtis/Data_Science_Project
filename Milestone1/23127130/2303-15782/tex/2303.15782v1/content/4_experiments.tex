\section{Experiments}
\label{sec:experiments}
We conduct two main experiments, an object-centric canonical reconstruction task and a full scene reconstruction task. The first experiment is to evaluate the performance of our newly introduced decoder while the second experiment highlights the advantages of our single-forward pass method compared to a two-stage approach.

\subsection{Object Set}
\label{subsec:object_set}
For both experiments, we use 3D models from the PartNet-Mobility \cite{Xiang_2020_SAPIEN} object set to generate a training and test data set.

{\parskip=5pt
\noindent\textbf{Categories}:
While PartNet-Mobility provides more than 2000 objects from 46 categories, as done in previous work~\cite{mu_a-sdf_2021, heppert_category-independent_2022, jain_screwnet_2021, xue_omad_2021, li_category-level_2020} we only select a subset of all categories. From this subset of categories, we select objects with one fixed base part and one significant moving part (e.g. we filter out knobs, buttons etc.). To later create realistic room layouts, we further differentiate between three placement types for objects, stand-alone (SA), counter (C) and table-top (TT) objects. In \cref{tab:object_set} we list the number of objects per category we selected as well as the context in which they can be used.}

\begin{table}
    \centering
    % \footnotesize
    \small
    \caption{Overview of Selected Objects. We select a subset of the PartNet-Mobility \cite{Xiang_2020_SAPIEN} object set and report the amount of instances we selected per category for our training and test set. Our final object set has 92 objects instances for training and 25 for testing.}
    \label{tab:object_set}
    \begin{tabular}{llccccc}
        \toprule
        Category & \makecell{Joint\\Type} & SA & C & TT & Train & Test \\
        \midrule
        Dishwasher & Rev. & \xmark & \cmark & \xmark & 18 & 5 \\
        Laptop & Rev. & \xmark & \xmark & \cmark & 20 & 5 \\
        Microwave & Rev. & \cmark & \xmark & \cmark & 10 & 5 \\
        Oven & Rev. & \cmark & \cmark & \xmark & 7 & 3 \\
        Refrigerator & Rev. & \cmark & \cmark & \xmark & 10 & 2 \\
        Table & Pris.& \cmark  & \xmark & \xmark & 19 & 5 \\
        WashingMachine & Rev. & \cmark & \cmark & \xmark & 8 & 2 \\
        \bottomrule
    \end{tabular}
    \vspace{0.1cm}
    \\{\textit{SA = stand-alone, C = counter, TT = table-top\\Rev. = Revolute, Pris. = Prismatic}}
    \vspace{-0.3cm}
\end{table}

{\parskip=5pt
\noindent\textbf{Object Canonicalization}: \label{subsubsec:object_canonicalization}
When tackling the task of reconstructing objects in a canonical object frame, usually, objects are canonicalized such that they either fit into the unit cube or unit sphere. This helps with the stability of learning and simplifies hyperparameter tuning. This approach fails for articulated objects as their outer dimensions change depending on the joint state. Blindly rescaling an articulated object such that it fits inside the unit cube or unit sphere results in an inconsistent part scaling across different joint states. To mitigate this problem, \cite{li_category-level_2020} proposed the NAOCS-space. Following their approach, first, we bring an object in its closed state (i.e. the lower joint limit) and put it in a canonical orientation such that for all objects, Z is pointing upwards and X back, Y is given through a right-hand coordinate frame system. Different from\cite{li_category-level_2020}, we rescale and translate the closed object such that it fits in the unit cube and then backwards apply that same rescaling and translation to all objects of the same instance independent of the joint state of the object.
It is important to note that rescaling an articulated object has no impact on revolute joint states (in~$\deg$), but prismatic joint states (in~$\si{m}$) which have to be rescaled accordingly.
% Early testing showed that the canonical rescale to the unit cube helps with disentangling the shape from the scale prediction.
}
\subsection{Canonical Reconstruction Task}
\label{subsec:canonical_reconstruction_task}
In our first experiment, we evaluate how well our decoders reconstruct the object's geometry and the articulation state. Thus, the task is not to reconstruct the object in the camera frame, but simply in its canonical frame. As described in \cref{para:backward_code_optimiziation}, we will optimize the shape and joint code for each input SDF with ADAM \cite{Kingma2015AdamAM} first jointly for 400 steps, then only the shape code for 100 steps and finally, only the joint code for 100 steps.

{\parskip=5pt
\noindent\textbf{Dataset}:
To generate our dataset for the canonical reconstruction task, we first apply the aforementioned canonicalization to each object from our object set described in \cref{subsec:object_set}. Here, the placement type does not matter. Then, we sample each object in 50 joint configurations uniformly spaced within the joint limits, make the meshes watertight using \cite{DBLP:journals/corr/abs-1802-01698} and follow \cite{park_deepsdf_2019} to generate 100k ground truth SDF values. Lastly, we rescale the generated data by the largest extents across all meshes to a unit cube. As mentioned in \cref{subsubsec:object_canonicalization} we also have to rescale prismatic joint states accordingly.
While we do not consider this as a new dataset contribution, we make our generation code available to allow other researchers to adjust the selected object instances and categories and generate their own data.

{\parskip=5pt
\noindent\textbf{Baselines and Ablations}:
\label{subsubsect:canonical_reconstruction_baseline_ablations}
Throughout this experiment, we will compare against the state-of-the-art for category-level object reconstruction method \textit{A-SDF} \cite{mu_a-sdf_2021}. As \textit{A-SDF} is designed to work on a single category, first, to show that learning an implicit joint code does not have a negative impact rather than using the real joint state directly as input, we compare against \textit{A-SDF} directly by training \textit{\ourName{}} also only on a single category. 
Second, we will jointly train \textbf{\textit{}} on all categories to highlight that \textit{\ourName{}} is able to generalize to a wide variety of categories and articulations using one model.
Third, we additionally perform an ablation study to understand the importance of our similarity regularization introduced in \cref{subsubsec:joint_space}. In this ablation study, we remove the pre-training step and the post-epoch step. We call this model \textit{\ourName-No-Enf}. And fourth, we extend \textit{A-SDF} to also take the joint type as input which allows us to train it jointly on all categories.%\todo{Added this}

\noindent Please note that we neglect \textit{A-SDF}s proposed test-time adaption (TTA) technique as in real applications it would not be practical to keep network weights of all different instances encountered. Results using TTA are reported in \ifthenelse{\boolean{reftosupp}}{\cref{supp:subsec:a-sdf_tta}}{Sec.~S.5.1}. % in the supplementary.
}

{\parskip=5pt
\noindent\textbf{Metrics}:
To measure reconstruction quality, we report the bi-directional L2-Chamfer distance multiplied by 1000 between the ground truth points and the extracted points using the model's respective generation method. To quantify the articulation state prediction, we will report the joint type prediction accuracy as well as the joint state error measured in~$\deg$~or~$\si{m}$~depending on the joint type for all correctly classified joint types.}

\input{content/assets/tables/canonical_reconstruction_small}

{\parskip=5pt
\noindent\textbf{Results}: The results for the canonical reconstruction task are shown in \cref{tab:decoder_results_short}. While no method clearly outperforms the other, it is notable though that \ourName{} and \textit{A-SDF} trained on all categories are performing slightly better on average across all categories compared to our baselines. This shows that having disentangled joint and shape codes in \ourName{}
can make the reconstruction category-agnostic.}
%%%%%%%%%%%%%%%%%%%%%%%%%% Second Experiments
\subsection{Full Pipeline Task}
\label{subsec:full_pipeline_task}
In our second experiment, the full pipeline task, we want to investigate the advantages \ourName{} has over a two-stage approach. To that end, we set up two experiments, one on simulated data and one on real-world data. For the full pipeline experiment, we use the trained decoders from the previous experiment.

{\parskip=5pt
\noindent\textbf{Datasets}:
We evaluate on two datasets, quantitatively on a synthetic dataset that aligns with our synthetic training dataset and qualitatively on a newly collected real-world dataset.}

{\parskip=0pt
\textit{Synthetic Data}: For training our encoder, we use SimNet \cite{laskey2021simnet} to generate a large-scale dataset of indoor kitchen environments. We use the same articulated object instances from \cref{tab:object_set} we also used to train our decoders. Unlike the previous experiment, the placement type of the object matters here. For each randomly sampled articulated object in the scene, we randomly sample a joint state in its joint limits as well as a scale from a pre-defined scale for each category. % (see the supplementary for more details)
To get ground truth joint codes for sampled articulation states we use the proposed method in \cref{subsec:inverse_joint_decoder}. After sampling a scene, we generate noisy stereo images as well as non-noisy depth images (only used for evaluation of the baseline).
To generate our synthetic test dataset we follow the same procedure with the only exception that we use the defined test-instances.}

{\parskip=0pt
\textit{Real Data}: Additionally, we evaluate the performance on a real-world test dataset we collected. Therefore, we select two real object instances from each of the following categories: knives, laptops, refrigerators, staplers, ovens, dishwashers, microwaves as well as one storage furniture and washing machine instance. We place these instances in common household environments. For each object, four different viewpoints were collected for each of the four articulation states for the object.  We measure the real joint state and annotate the data using~\cite{Sager_2022} with orientated 3D bounding boxes. In total, we collected 263 images. For collection we used a ZED 2 stereo camera. To get depth images we use state-of-the-art, off-the-shelf learned stereo depth methods to produce highly accurate depth images \cite{shankar2022learned}.
}

{\parskip=0pt
\textit{Comparison to other Datasets}: To the best of our knowledge, the closest works to \ourName{}'s dataset are the RBO~\cite{martin-martin_rbo_2018} and BMVC~\cite{michel_pose_2015} dataset. Both datasets do not provide large-scale synthetic stereo-RGB or RGB-D images and only half of the categories with readily available 3D furniture models from PartNetMobility. For a full comparison see \ifthenelse{\boolean{reftosupp}}{\cref{tab:dataset_comparison}}{Tab.~S.2}.
}

{\parskip=5pt
\noindent\textbf{Baselines}:
\label{subsubsec:full_pipeline_baseline}
We set up two baselines using A-SDF \cite{mu_a-sdf_2021} and follow their proposed method to reconstruct objects in the camera frame. Since A-SDF assumes a given segmentation of the object as well as a pose that transforms the object from the camera frame to the object-centric frame, we will compare against two versions of A-SDF. One, where we use ground truth segmentation masks and poses which we call \textit{A-SDF-GT} and one, where we use our model to predict segmentation masks whose center we then use to query our pixel-level pose map. We call this variant simply \textit{A-SDF}.
In both cases we approximate normals using the depth image, use the segmentation masks to extract the corresponding object point cloud from the depth image, transform the point clouds into the canonical object frame using the predicted pose, create SDF values, optimize for the SDF values as done in \cref{subsubsect:canonical_reconstruction_baseline_ablations} and eventually reproject the reconstruction into the camera frame using the same transformation.
}


{\parskip=5pt
\noindent\textbf{Metrics}:
We compare our reconstructions in the camera frame using two different metrics typically used for object pose prediction \cite{wang_normalized_2019}. First, we compare the absolute error of the position and orientation by reporting the respective percentage below $10\degree10\si{cm}$ and $20\degree30\si{cm}$ combined. Second, we evaluate the average precision for various IOU-overlap thresholds~(\textbf{IOU25} and \textbf{IOU50}) between the reconstructed bounding box and the ground truth bounding box. Both metrics serve as a proxy for articulation state and reconstruction quality. For evaluation results on these more fine-grained metrics, we refer to \ifthenelse{\boolean{reftosupp}}{\cref{supp:subsec:extend_metrics}}{Sec.~S.5.1}. % the supplementary material. 
}

{\parskip=5pt
\noindent\textbf{Results}:
In \cref{tab:full_pipeline} we report results using the aforementioned metrics as well as show a speed comparison of \ourName{} against the baselines.
}
\begin{table}
    \centering
    \caption{Full Scene Reconstructions Results.}
    \label{tab:full_pipeline}
    \begin{subtable}[h]{0.45\textwidth}
        \footnotesize
        \centering
        \begin{tabular}{l|cccc}
        \toprule
            Method & IOU25 $\uparrow$ & IOU50 $\uparrow$ & $10\si{\degree}10\si{cm} \uparrow$ & $20\si{\degree}30\si{cm} \uparrow$  \\
            \midrule
            A-SDF-GT & 45.2 & 27.1 & N/A & N/A \\
            A-SDF & 33.9 & 10.4 & 27.1 & 70.8 \\
            \ourName & \textbf{64.0} & \textbf{31.5} & \textbf{28.7} & \textbf{76.6}\\
        \bottomrule
        \end{tabular}
        \caption{\footnotesize mAP Reconstruction Results.}
        \label{tab:full_pipeline_synthetic}
    \end{subtable}%
    \hfill
    \newline
    \vspace*{0.2cm}
    \newline
    \begin{subtable}[h]{0.45\textwidth}
        \footnotesize
        \centering
        \begin{tabular}{c|c|ccc|c}
            \toprule
            Method & Sample Grid & Det. & Optim. & Recon. & Total \\
            \midrule
            A-SDF & 256 & 5.390 & 21.600 & 7.836 & 64.262 \\
            \ourName & 256 & \textbf{0.264} & N/A & 0.414 & 1.092 \\
            \ourName & 128 & \textbf{0.264} & N/A  & \textbf{0.097} & \textbf{0.458} \\
            \bottomrule
        \end{tabular}
        \caption{\footnotesize Detection Speed of Approaches in [s]. We measure the speed of our approaches on a common desktop using a Nvidia Titan XP GPU. Sample grid defines how many points are sampled along each dimension. Total time assumes two detected objects. \textit{Det. = Detection time per image, Optim. = Optimiziation time per object, Recon. = Reconstruction time per object}}
        \label{tab:detection_speed}
    \end{subtable}
    \vspace{-0.4cm}
\end{table}

{\parskip=1pt
\noindent
\textit{Reconstruction}:
As visible in \cref{tab:full_pipeline_synthetic} \textit{\ourName{}} shows superior performance over both variants of \textit{A-SDF} for our full reconstruction task.
Overall the performance of all methods is lower compared to similar experiments on category-level rigid object detection \cite{wang_normalized_2019}. This can be attributed to the fact that in our kitchen scenarios we deal with heavy occlusions due to many objects being placed under the counter. Taking the occlusions into consideration, it becomes clear that for \textit{A-SDF} it is very difficult to estimate the exact extents given only a front-showing partial point cloud of the object. Compared to that, \textit{\ourName{}} benefits from its single-shot encoding step as the whole image is taken into consideration.
We show qualitative results on our synthetic and real-world data in \cref{fig:teaser} as well as in \ifthenelse{\boolean{reftosupp}}{\cref{supp:subsec:rgbd_version}}{Sec.~S.5.3} where we also compare against an RGB-D version of \ourName{}. 
}

{\parskip=1pt
\noindent
\textit{Detection Speed}:
Aside from a lower pose and bounding box error, \textit{\ourName{}} processes frames faster than \textit{A-SDF}. \cref{tab:detection_speed} shows a reduction in inference time of more than 60 times while still persevering the same level of detail.
}
