\section{Evaluation}
\label{sec:eval}

In this section, we evaluate the quality of the node level representations output by our method on both transductive and inductive settings. 

\paragraph{Datasets} For transductive learning we run RGI on 4 popular benchmarks: \textit{Amazon Computers}, \textit{Amazon Photos}, \textit{Coauthor CS} and \textit{Coauthor Physics}. Moreover, we include the large scale \textit{ogbn-arxiv} dataset. Finally, inductive learning evaluation is addressed with the challenging \textit{PPI} dataset. The statistics of the datasets are shown in Table \ref{tab:datasets}.

\paragraph{Linear evaluation} We follow the linear evaluation protocol on graphs to assess the quality of the representations as proposed in \cite{velickovic2018deep}. It consists of first fitting a GNN encoder in a fully self-supervised manner, freezing the weights of the encoder, obtaining the node-level representations and fitting a linear classifier to a downstream task without backpropagating the gradients through the encoder. For comparability with other methods, the transductive settings are evaluated in terms of accuracy of the predictions whereas for the \textit{PPI} dataset we employ the micro-average F1-score. As usual, since there is no public split for these datasets, \textit{Amazon Computers}, \textit{Amazon Photos}, \textit{Coauthor CS} and \textit{Coauthor Physics} are randomly split into train/validation/test (10\% / 10\% / 80\%). The ogbn-arxiv dataset is split with the partition provided by Open Graph Benchmark \cite{hu2021open}. To evaluate on the \textit{PPI} dataset, we employ the standard pre-defined split, which has 20 graphs to fit the model, 2 graphs for validation and another two for testing.


\subsection{Transductive learning}

\paragraph{Architecture} As in \cite{velickovic2018deep, thakoor2021bootstrapped}, among others, we fix the encoder $f_{\Theta}$ to be a $L=2$ layer GCN \cite{kipf2017semisupervised} for the Amazon and Co-authorship datasets. We have set the output dimensionality to be $512$ for all of them and the hidden, $1024$. We include batch normalization \cite{10.5555/3045118.3045167} and ReLU activation after the first layer but none of them is included after the second convolutional layer.  We also apply dropout regularization to the input graph. The architecture for the \textit{ogbn-arxiv} dataset is slightly different. Based on \cite{thakoor2021bootstrapped}, we employ a $L=3$ GCN \cite{kipf2017semisupervised} encoder, with layer normalization and ReLU activation after the first and the second layer.

To obtain the global node embeddings, we propagate the embeddings with the normalized adjacency matrix without self-loops for $K=1$ step, that is:

\begin{equation}
    \label{eq:gk_transductive}
    \mathbf{V} = g_K(\mathbf{U},\mathbf{A})= \left ( {\mathbf{D}}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}} \right ) \mathbf{U}
\end{equation}
Finally, the networks $h_{\phi}$ and $h_{\psi}$ are implemented with a two layer FCNN and ReLU activation in the hidden layer. However, no batch normalization is employed for these auxiliary networks.

\paragraph{Numerical results} Table \ref{tab:results_transductive} shows the mean accuracy of the linear evaluation protocol on the transductive graph settings. Our results are the average of 20 model weight initializations and data splits. The other results are extracted from previous reports. We can observe that RGI performs competitively in all datasets despite its simplicity and achieves state of the art in some of them. 
In except of \textit{Amazon Computers} dataset, RGI is trained for $1,000$ epochs whereas other methods such as BGRL require $10,000$ epochs \cite{thakoor2021bootstrapped}.  For instance, when BGRL is trained only for $1,000$ epochs, its performance in all datasets is dropped \cite{bielak2021graph}. Finally, as for the \textit{ogbn-arxiv} dataset, while RGI performs competitively, state-of-the-art performance is achieved by masking and augmentation-based methods, which suggest that graph data augmentations and the invariance to them are an appropriate solution for this particular dataset and task.

\begin{table}[ht]
\centering
\caption{Transductive and inductive datasets}
\label{tab:datasets}
\begin{adjustbox}{width=1\textwidth}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Name} & \textbf{Task} & \textbf{Num. Nodes} & \textbf{Num. Edges} & \textbf{Node features} & \textbf{Num. Classes} \\
\midrule
Amazon Computers & Transductive & 13,752 & 245,861 & 767 & 10\\
Amazon Photos & Transductive & 7,650 & 119,081 & 745 & 8 \\
Coauthor CS & Transductive & 18,333 & 81,894 & 6,805 & 15 \\
Coauthor Physics & Transductive & 34,493 & 247,962 & 8,415 & 5 \\
\midrule
ogbn-arxiv & Transductive & 168,343 & 1,166,243 & 128 & 40 \\
\midrule
PPI (24 graphs) & Inductive & 56,944 & 818,716 & 50 & 121 (multilabel) \\

\bottomrule
\end{tabular}
\end{adjustbox}
\vskip 0.1in
\end{table}

\begin{table}[ht]
\centering
\caption{Classification accuracies on transductive datasets averaged for 20 weight initializations.}
\label{tab:results_transductive}
\begin{adjustbox}{width=1\textwidth}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Am. Computers} & \textbf{Am. Photos} & \textbf{Co. CS} & \textbf{Co. Physics} & \textbf{Ogbn-Arxiv} \\
\midrule
Raw ft. &  73.81 $\pm$ 0.00 & 78.53 $\pm$ 0.00 & 90.37 $\pm$ 0.00 & 93.58 $\pm$ 0.00 &  55.50 $\pm$ 0.23\\
\midrule
Random-Init & 86.46 $\pm$ 0.38 & 92.08 $\pm$ 0.48 & 91.64 $\pm$ 0.29 & 93.71 $\pm$ 0.29 & 68.94 $\pm$ 0.15 \\
DGI \cite{velickovic2018deep}& 83.95 $\pm$ 0.47 & 91.61 $\pm$ 0.22 & 92.15 $\pm$ 0.63 & 94.51 $\pm$ 0.52 & 70.34 $\pm$ 0.16\\
GRACE \cite{Zhu:2020vf}& 89.53 $\pm$ 0.35 & 92.78 $\pm$ 0.45 & 91.12 $\pm$ 0.20 & OOM & 71.51 $\pm$ 0.11\\
G-BT \cite{bielak2021graph} & 88.14 $\pm$ 0.33 & 92.63 $\pm$ 0.44 & 92.95 $\pm$ 0.17 & 95.07 $\pm$ 0.17 & 70.12 $\pm$ 0.18 \\
CCA-SSG \cite{zhang2021canonical} & 88.74 $\pm$ 0.28 & 93.14 $\pm$ 0.14 & 93.31 $\pm$ 0.22 & 95.38 $\pm$ 0.06 & 71.24 $\pm$ 0.20 \\
BGRL \cite{thakoor2021bootstrapped} & 90.34 $\pm$ 0.19 & \textbf{93.17 $\pm$ 0.30} & 93.31 $\pm$ 0.13 & 95.73 $\pm$ 0.05 & \textbf{71.64 $\pm$ 0.12} \\
\midrule
RGI (\textit{ours}) & \textbf{90.45 $\pm$ 0.08} & 92.94 $\pm$ 0.09 & \textbf{93.37 $\pm$ 0.07} & \textbf{95.91 $\pm$ 0.09} & 70.33 $\pm$ 0.25 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\vskip 0.05in
\small
\footnotesize *OOM refers to out-of-memory error in a 16GB GPU.
\vskip 0.1in
\end{table}

\subsection{Inductive learning}
\label{sec:eval_inductive}

\begin{table}[ht]
\centering
\caption{Classification micro-average F1 score on PPI dataset averaged for 20 weight initializations.}
\label{tab:results_ppi}
\small
\begin{tabular}{lc}
\toprule
\textbf{Method} & \textbf{PPI} \\
\midrule  
Raw ft. & 42.20 \\
\midrule
Rdm-Init & 62.60 $\pm$ 0.20 \\
DGI \cite{velickovic2018deep} & 63.80 $\pm$ 0.20 \\
GMI \cite{peng2020graph} & 65.00 $\pm$ 0.02 \\
GRACE \cite{Zhu:2020vf} & 69.71 $\pm$ 0.17 \\
G-BT \cite{bielak2021graph} & 70.49 $\pm$ 0.19 \\
BGRL \cite{thakoor2021bootstrapped} &
70.49 $\pm$ 0.05 \\
GraphMAE \cite{hou2022graphmae}* & 63.60 $\pm$ 0.29 \\
\midrule
RGI (\textit{ours}) & \textbf{72.16 $\pm$ 0.11}\\
\bottomrule
\end{tabular}
\vskip 0.05in
\footnotesize *Results are replicated with author's official code but reducing the dimensionality to 512, as it is employed in RGI and others, as well as limiting the linear evaluation method to the one employed in RGI for a fairer comparison.
\end{table}

\paragraph{Architecture} Based on previous reports \cite{bielak2021graph, thakoor2021bootstrapped}, we implement the encoder with a $L=3$ layer GAT \cite{veličković2018graph} with ELU activation and skip connections. Both hidden and output dimensionality are set to $D=512$. Although a mean-pooling scheme would be more appropriate for inductive settings \cite{NIPS2017_5dd9db5e}, we employ the same propagation scheme than in transductive setting of Equation \ref{eq:gk_transductive}. In Section \ref{sec:ablations_propagation} we study the effect of $g_K$. As initial node features are sparse in the PPI dataset, we apply dropout regularization to the graph before propagation. The reconstruction networks $h_{\phi}$ and $h_{\psi}$ have the same architecture as in the transductive setting.

\paragraph{Numerical results} Table \ref{tab:results_ppi} shows the micro-average F1-score of the linear evaluation averaged for 20 runs. RGI outperforms the current state of the art on this challenging dataset and only requires $2,000$ epochs to reach this performance.