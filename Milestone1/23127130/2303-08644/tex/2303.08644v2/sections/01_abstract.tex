\begin{abstract}
%% Text of abstract
    Self-supervised learning is gaining considerable attention as a solution to avoid the requirement of extensive annotations in representation learning on graphs. Current algorithms are based on contrastive learning, which is computation an memory expensive, and the assumption of invariance under certain graph augmentations. However, graph transformations such as edge sampling may modify the semantics of the data so that the iinvariance assumption may be incorrect. We introduce Regularized Graph Infomax (RGI), a simple yet effective framework for node level self-supervised learning that trains a graph neural network encoder by maximizing the mutual information between output node embeddings and their propagation through the graph, which encode the nodes' local and global context, respectively. RGI do not use graph data augmentations but instead generates self-supervision signals with feature propagation, is non-contrastive and does not depend on a two branch architecture. We run RGI on both transductive and inductive settings with popular graph benchmarks and show that it can achieve state-of-the-art performance regardless of its simplicity. \footnote{The code is available at \href{https://github.com/oscar97pina/gssl-rgi}{https://github.com/oscar97pina/gssl-rgi}}
\end{abstract}

%%Graphical abstract
%\begin{graphicalabstract}
%\includegraphics{grabs}
%\end{graphicalabstract}

%%Research highlights
%\begin{highlights}
%\item Research highlight 1
%\item Research highlight 2
%\item We propose an efficient algorithm for self-supervised learning on graphs using node embedding propagation to generate the self-supervision signals rather than using graph data augmentations.
%\item The algorithm is non-contrastive, but employs variance-covariance regularization instead, which decreases the training complexity.
%\item State of the art performance in transductive and inductive graph datasets is achieved, showing the effectivenes of the method.
%\end{highlights}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

Graph neural network \sep self-supervised learning \sep graph representation learning \sep regularization

\end{keyword}