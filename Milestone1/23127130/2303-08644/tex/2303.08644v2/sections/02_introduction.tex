\section{Introduction}
\label{sec:introduction}

The primary goal of self-supervised learning is to learn meaningful representations from large amounts of unlabeled data that can be employed to efficiently fit other downstream tasks with a small amount of labeled samples. The challenge is to define an appropriate auxiliary task that leads the model to capture the relevant parts of the input data.

A popular solution within the computer vision field is to force the encoder to be invariant under certain transformations applied to the input, such as rotation and cropping, by generating two views of the same image by applying these random augmentations and maximizing the agreement between their representations. Owing to their success, these works have been adapted by the graph learning community in order to train graph neural networks encoders in a self-supervised manner \cite{chen2020big, chen2020simple, grill2020bootstrap, zbontar2021barlow}. Consequently, graph specific data augmentation techniques are required to generate the graph views. Graph transformations can focus on both node attributes and the topology of the graph. Indeed, popular choices are node attribute masking and edge sampling \cite{thakoor2021bootstrapped, zhang2021canonical, bielak2021graph, Zhu:2020vf}. However, the idea behind invariance via data augmentations roots in the fact that it is assumed that these transformations do not change the semantics of the data and the information contained about the downstream task is kept. Whereas we can understand image augmentations, it is not clear how graph transformations modify its semantics nor if they can be applied to all graph domains \cite{NEURIPS2021_ff1e68e7}. To overcome this issue, graph diffusion has been proposed to create the alternative views \cite{icml2020_1971}, and other works attempt to get rid of the augmentations by designing strategies that leverage the local neighborhood of the nodes  \cite{peng2020graph, lee2021augmentation, https://doi.org/10.48550/arxiv.2204.04874}.

In this manuscript, we introduce \textit{Regularized Graph Infomax (RGI)}, a simple yet effective self-supervised learning framework for graph structured data. RGI trains the encoder based on \textit{embedding propagation} as self supervision signals, which consists of propagating the output embeddings through the graph and using them as prediction target from the node embeddings, which leads to context-aware embeddings that do not encode unknown invariances. Additionally, variance-covariance regularization \cite{bardes2022vicreg} is used in order to avoid the collapse of the representations, which is generally more efficient than contrastive methods. Therefore, the algorithm is augmentation-free, non-contrastive, and does not require a two-branch architecture nor complex training strategies.

This document is organized as follows, in Section \ref{sec:background} we provide the necessary knowledge to fully understand the arguments and statements of the method, in Section \ref{sec:method} we detail our algorithm RGI, motivate and explain the usage of feature propagation as self-supervision signals (Section \ref{sec:method_propagation}), provide a \textit{local-global} context perspective of RGI and feature propagation (Section \ref{sec:method_locglob}), detail the loss function (Section \ref{sec:method_loss}) and show that RGI is maximizing the mutual information between the node embeddings and their propagation through the graph (Section \ref{sec:method_motiv}). Then, we perform empirical evaluation on popular graph benchmarks in Section \ref{sec:eval} and analyse the influence of different parts of the algorithm in the ablation study of Section \ref{sec:ablations}. Finally, we discuss RGI and the results in Section \ref{sec:discussion} and conclude the work.

%The main goal of self-supervised learning is to learn useful representations of high-dimensional data without relying on annotations in order to leverage them in downstream tasks. Ideally, the obtained representations should be enough to efficiently fit other tasks with a small amount of labeled data.

%In the vision domain, methods mainly focus on a maximization of the agreement between the representations of two views of an image. For instance, Deep InfoMax (DIM) \cite{hjelm2018learning} was initially proposed to maximize the mutual information (MI) between local and global views. However, methods based on data augmentation techniques have shown better performance. They are based on a two branch architecture, either symmetric or not, whose branches are fed with different augmented versions of an image obtained with random transformations such as rotation and cropping \cite{bardes2022vicreg}, \cite{chen2020big}, \cite{chen2020simple}, \cite{grill2020bootstrap}, \cite{zbontar2021barlow}. The encoder is trained to be invariant to those image distortions. 

%These schemes are also adopted for graph representation learning and successful algorithms have been extended to this domain \cite{velickovic2018deep}, \cite{sun2019infograph}, \cite{thakoor2021bootstrapped}, \cite{zhang2021canonical}, \cite{Zhu:2020vf}, \cite{icml2020_1971}. Consequently, graph specific data augmentation techniques are required to generate the graph views. Graph transformations can focus on both node attributes and the topology of the graph. Indeed, popular choices are node attribute masking and edge sampling \cite{thakoor2021bootstrapped}, \cite{zhang2021canonical}, \cite{bielak2021graph}, \cite{Zhu:2020vf}. However, the idea behind invariance via data augmentations roots in the fact that it is assumed that these transformations do not change the semantics of the data and the information contained about the downstream task is kept. Whereas we can understand image augmentations, it is not clear how graph transformations modify its semantics nor if they can be applied to all graph domains \cite{NEURIPS2021_ff1e68e7}. To overcome this issue, graph diffusion has been proposed to create the alternative views \cite{icml2020_1971}, and other works attempt to get rid of the augmentations by designing strategies that leverage the local neighborhood of the nodes  \cite{peng2020graph}, \cite{lee2021augmentation}, \cite{https://doi.org/10.48550/arxiv.2204.04874}.

%Additionally, the main challenge in multi-view scenarios is to avoid the collapse of the models, in which the encoder outputs constant representations for all inputs. Solutions can be divided into contrastive and non-contrastive. The former constructs positive and negative pairs and maximizes the similarity between components of the positive pairs while minimizing it for the elements of the negative pairs \cite{chen2020simple}, \cite{velickovic2018deep}, \cite{hjelm2018learning}, \cite{Zhu:2020vf}, \cite{icml2020_1971}. However, it has been shown that they require a large number of negative samples to efficiently work, which can be a bottleneck for scalability. Within non-contrastive methods, we can find, among others, knowledge distillation, which avoids the collapse by taking a teacher-student asymmetric architecture \cite{grill2020bootstrap}, \cite{thakoor2021bootstrapped}, and regularized methods, which spread out embedded data points by regularizing the covariance matrix of the representations \cite{bardes2022vicreg}, \cite{zbontar2021barlow}, \cite{zhang2021canonical}.

%To cope with the aforementioned challenges, we introduce \textit{RGI - Regularized Graph Infomax}, a simple yet effective self-supervised learning framework for graphs. The algorithm is augmentation-free, non-contrastive, and does not require a two-branch architecture nor complex training strategies. RGI trains an encoder maximizing the mutual information between nodes' representations output by the model and their propagation through the graph, namely local and global views, respectively. The collapse is avoided with variance-covariance regularization loss terms that attempt to maximize the entropy of the representation space.

%This paper is organized as follows, we first provide a background on mutual information (MI), the Infomax principle and the multi-view representation learning approach for images and graphs. Then, we formalize our algorithm RGI for self-supervised learning, provide intuitions behind the method and detail the objective function proposed. Then, we evaluate the method on multiple graph benchmark datasets and discuss the advantages of RGI with respect to other algorithms. The main contributions are listed below:

%\textbf{1.} We derive a loss function that attempts to maximize the MI between two views of a node by relying on the reconstruction-based bound of the MI and regularizing the entropy of the representation space via the covariance matrix.

%\textbf{2.} We define a node-level global view, particular to every node, as opposed to the graph-level view proposed in other methods \cite{velickovic2018deep}, \cite{sun2019infograph}. This is obtained by propagating the node representations through the graph. Global views are critical in our method and a simple way to obtain positive pairs and efficiently encode node's neighborhoods. Additionally, we provide intuitions on why these views are called global.

%\textbf{3.} We evaluate the algorithm on both transductive and inductive settings and show that, despite its simplicity, state-of-the-art performance is obtained in some of the benchmarks while being competitive in all of them. We also provide ablation studies on how the different designs of the method influence the performance, that demonstrate the robustness of RGI despite modifying core concepts of the algorithm.

%\textbf{4.} We provide an extensive discussion of the method addressing its advantages in terms of simplicity and effectiveness as well as its limitations.

