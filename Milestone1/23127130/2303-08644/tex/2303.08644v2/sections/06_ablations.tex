\section{Ablation study}
\label{sec:ablations}
In this section, we perform an exhaustive ablation study to evaluate the influence of the different components of RGI. Generally, results show that RGI performs robustly for different design configurations, specifically in transductive settings.

\subsection{Loss function components} 
\label{sec:ablations_loss}
RGI maximizes the mutual information between local and global node embeddings by tackling the reconstruction error between them as well as entropy regularization, which is composed by a variance and a covariance term. The final objective is defined as a weighted sum of reconstruction, variance and covariance terms by $\lambda_1$, $\lambda_2$ and $\lambda_3$, respectively. In order to measure the importance of every term, we have trained RGI by setting their corresponding weight $\lambda$ to zero and observing the difference in performance. The results are shown in Table \ref{tab:ablations_loss}. We observe that variance and covariance terms are required to avoid the collapse of the representations, specifically in the inductive setting, which is more sensitive to these parameters. Additionally, we observe that the reconstruction error is also needed to make the algorithm achieve state-of-the-art performance, validating hence the usage of propagated embeddings as supervision signals.

\begin{table}[ht]
\centering
\caption{Effect of the components of the loss function. Linear evaluation accuracy (micro-average F-Score for PPI dataset) after fitting and encoder with distinct configurations averaged for 5 weight initializations and data splits.}
\label{tab:ablations_loss}
\begin{adjustbox}{width=1\textwidth}
\small
\begin{tabular}{cccccc}
\toprule
$\mathcal{L}$ & \textbf{Am. Computers} & \textbf{Am. Photos} & \textbf{Co. CS} & \textbf{Co. Physics}  & \textbf{PPI} (F-Score) \\
\midrule
$\lambda_1=0$ & 89.78 $\pm$ 0.09 & 92.25 $\pm$ 0.20 & 93.53 $\pm$ 0.06 & 95.96 $\pm$ 0.05 & 68.83 $\pm$ 0.08 \\
$\lambda_2=0$ & 79.53 $\pm$ 0.96 & 89.62 $\pm$ 0.39 & 84.27 $\pm$ 0.39 & 92.57 $\pm$ 0.22 & 0.00 $\pm$ 0.00 \\
$\lambda_3=0$ & 85.70 $\pm$ 0.21 & 92.32 $\pm$ 0.12 & 92.51 $\pm$ 0.09 & 95.48 $\pm$ 0.07 & 49.43 $\pm$ 0.21 \\
$\lambda_1,\lambda_2,\lambda_3 \neq 0 $ & 90.45 $\pm$ 0.07 &  92.88 $\pm$ 0.13 & 93.37 $\pm$ 0.03 & 95.93 $\pm$ 0.04 & 72.25 $\pm$ 0.09 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\vskip -0.1in
\end{table}

\subsubsection{Feature propagation function}
\label{sec:ablations_propagation}
A propagation function $g_K(\mathbf{U},\mathbf{A})$ is used to create the supervision signals from the node embeddings. In this section, we evaluate different shift operators, namely the mean-propagation $\mathbf{D}^{-1}\mathbf{A}$, the normalized adjacency matrix $\mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ and the normalized Laplacian $\mathbf{I} - \mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ as well as different values of $K$ (1, 2 and 5). Note that none of them include the self-loops as opposed to most GNN propagation schemes. Table \ref{tab:ablations_propagation} shows that RGI performs robustly in all settings as long as the global propagation scheme captures the low-frequency components of the data, obtained with the adjacency matrix (either mean pooling or normalized), which is motivated by the homophily of the downstream task on the graph.

\begin{table}[ht]
\centering
\caption{Effect of global propagation scheme. Linear evaluation accuracy (micro-average F-Score for PPI dataset) for different schemes to obtain the global node embeddings propagating the local ones averaged for 5 weight initializations and data splits.}
\label{tab:ablations_propagation}
\begin{adjustbox}{width=1\textwidth}
\small
\begin{tabular}{ccccccc}
\toprule
$g(\mathbf{A})$ & $K$ & \textbf{Am. Computers} & \textbf{Am. Photos} & \textbf{Co. CS} & \textbf{Co. Physics}  & \textbf{PPI} (F-Score) \\
\midrule

$\mathbf{D}^{-1}\mathbf{A}$ & 1 & 90.52 $\pm$ 0.09 & 92.81 $\pm$ 0.14 & 93.39 $\pm$ 0.04 & 95.89 $\pm$ 0.07 & 71.29 $\pm$ 0.11 \\
$\mathbf{D}^{-1}\mathbf{A}$ & 2 & 90.43 $\pm$ 0.16 & 92.84 $\pm$ 0.15 & 93.37 $\pm$ 0.04 & 95.89 $\pm$ 0.07 & 71.28 $\pm$ 0.08 \\
$\mathbf{D}^{-1}\mathbf{A}$ & 5 & 90.41 $\pm$ 0.17 & 92.84 $\pm$ 0.08 & 93.26 $\pm$ 0.06 & 95.86 $\pm$ 0.08 & 70.06 $\pm$ 0.08 \\
\midrule
$\mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ & 1 & 90.46 $\pm$ 0.06 & 92.89 $\pm$ 0.15 & 93.37 $\pm$ 0.02 & 95.92 $\pm$ 0.04 & 72.25 $\pm$ 0.13 \\
$\mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ & 2 & 90.25 $\pm$ 0.10  & 92.69 $\pm$ 0.14 & 93.35 $\pm$ 0.02 & 95.91 $\pm$ 0.04 &  71.94 $\pm$ 0.08\\
$\mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ & 5 & 90.08 $\pm$ 0.16 & 92.75 $\pm$ 0.11 & 93.23 $\pm$ 0.05 & 95.85 $\pm$ 0.05 & 70.08 $\pm$ 0.17 \\
\midrule
$\mathbf{I} - \mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ & 1 & 84.22 $\pm$ 0.28 & 90.23 $\pm$ 0.20 & 93.01 $\pm$ 0.12 &  95.50 $\pm$ 0.11 & 61.81 $\pm$ 0.08 \\
$\mathbf{I} - \mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ & 2 & 84.58 $\pm$ 0.22 & 90.75 $\pm$ 0.25 & 92.89 $\pm$ 0.14 & 95.46 $\pm$ 0.08  & 61.35 $\pm$ 0.13\\
$\mathbf{I} - \mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ & 5 & 88.65 $\pm$ 0.16 & 92.62 $\pm$ 0.20 & 91.82 $\pm$ 0.27 & 95.53 $\pm$ 0.09 & 64.50 $\pm$ 0.12 \\

\bottomrule
\end{tabular}
\end{adjustbox}

\vskip -0.1in
\end{table}

