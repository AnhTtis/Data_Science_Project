\section{Implementation details}
\label{sec:implementation}

\begin{algorithm}[tb]
   \caption{Pseudo-code PyTorch implementation of RGI}
   \label{algo:pytorch}
\begin{algorithmic}
    \STATE \
    \STATE \PyComment{obtain local views}
    \STATE \PyCode{u = f(x, adj)}
    \STATE \PyComment{get symmetric adjacency matrix}
    \STATE \PyCode{deg = adj.sum(dim=-1)} \PyComment{adj dense and symmetric}
    \STATE \PyCode{deg = diagonal(inverse(deg).sqrt())}
    \STATE \PyCode{shift = deg @ adj @ deg}
    \STATE \PyComment{obtain global views}
    \STATE \PyCode{v = matrix\_power(shift,K) @ u}
    \STATE \PyComment{batch size and dimensionality}
    \STATE \PyCode{N, D = u.size()}
    \STATE \PyComment{reconstruction between views}
    \STATE \PyCode{v\_prime = h\_phi(u)}
    \STATE \PyCode{u\_prime = h\_psi(v)}
    \STATE \PyComment{mean - centering}
    \STATE \PyCode{u\_norm = u - u.mean(dim=0)}
    \STATE \PyCode{v\_norm = v - v.mean(dim=0)}
    \STATE \PyComment{covariance matrices}
    \STATE \PyCode{cov\_u = (u\_norm.T @ u\_norm) / (N-1) }
    \STATE \PyCode{cov\_v = (v\_norm.T @ v\_norm) / (N-1) }
    \STATE \PyComment{reconstruction loss}
    \STATE \PyCode{rec\_loss = mse\_loss(u, u\_prime) + mse\_loss(v, v\_prime)}
    \STATE \PyComment{variance loss}
    \STATE \PyCode{var\_loss = diagonal(cov\_u).add\_(-1).pow\_(2).sum() / D + \textbackslash}
    \STATE \qquad \qquad \qquad \PyCode{diagonal(cov\_v).add\_(-1).pow\_(2).sum() / D}
    \STATE \PyComment{covariance loss}
    \STATE \PyCode{cov\_loss = off\_diagonal(cov\_u).pow\_(2).sum() / D + + \textbackslash}
    \STATE \qquad \qquad \quad \PyCode{off\_diagonal(cov\_v).pow\_(2).sum() / D }
    \STATE \PyComment{total loss}
    \STATE \PyCode{loss = lambd\_1 * rec\_loss + lambd\_2 * var\_loss + lambd\_3 * cov\_loss}
    \STATE \PyComment{optimizer}
    \STATE \PyCode{loss.backward()}
    \STATE \PyCode{optimizer.step()}
\end{algorithmic}
\end{algorithm}

\paragraph{Implementation} RGI and all neural networks are implemented in PyTorch \cite{paszke2017automatic} and PyTorch Geometric \cite{Fey/Lenssen/2019}. Algorithm \ref{algo:pytorch} shows a PyTorch-like pseudo-code implementation of RGI. All our experiments are run in a single 16GB GPU. For linear evaluation, we have employed Sci-kit Learn library \cite{scikit-learn} for \textit{Amazon Computers}, \textit{Amazon Photos}, \textit{Coauthor CS} and \textit{Coauthor Physics} datasets and PyTorch for the \textit{PPI} dataset, taking the implementation from \cite{thakoor2021bootstrapped}.

\paragraph{Optimization} All models have been trained with Adam optimizer \cite{DBLP:journals/corr/KingmaB14} and a weight decay of $10^{-5}$. Additionally, we have employed a learning rate scheduler with linear warmup for $n_{warmup}$ epochs and cosine decay for the remaining $n_{epochs} - n_{warmup}$, where $n_{epochs}$ is the total number of epochs. The values of $n_{warmup}$ and $n_{epochs}$ for the different datasets can be found in Table \ref{tab:hyperparameters}. In transductive settings, we perform full-graph optimization at each gradient step. Alternatively, for the \textit{PPI} dataset, which is multi-graph, we set a batch size of 1 graph for each gradient step due to memory constraints of the GAT encoder. 

\paragraph{Node feature normalization} Input node features are normalized row-wise with the L1 norm for transductive settings whereas no normalization is applied to \textit{PPI} node features. Before fitting the linear classifier with the obtained representations, they are row-wise L2 normalized.

\paragraph{Hyperparameters} Some of the hyperparameters have been tuned with a small search whereas others have been fixed. Table \ref{tab:hyperparameters} shows the hyper-parameters settings employed to obtain the results in Table \ref{tab:results_transductive} and Table \ref{tab:results_ppi}. The search space of the optimized hyper-parameters is the following:

\begin{itemize}
    \item Propagation matrix : $\{ \mathbf{D}^{-1} \mathbf{A}, \mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}, \mathbf{I} - \mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}\}$
    \item Number of global propagation steps $(K)$ : $\{ 1, 2, 5\}$
    \item Number of training epochs $(n_{epochs})$ : $\{ 1000, 2000, 5000 \}$
    \item Learning rate $(lr)$ : $\{ 10^{-3}, 10^{-4}, 10^{-5} \}$
    \item Reconstruction loss weight $(\lambda_1)$: $\{10, 15, 20\}$
    \item Variance loss weight $(\lambda_2)$: $\{5, 10, 15\}$
    \item Covariance loss weight $(\lambda_3)$: $\{1, 5, 10\}$
    \item Dropout regularization probability input graph $(p_{input})$ : $\{ 0.0, 0.5\}$
    \item Dropout regularization probability before propagation $(p_{local})$ : $\{ 0.0, 0.5\}$  
\end{itemize}
The number of GNN layers $(L)$ and embedding size $(D)$ have been fixed according to previous reports for better comparability. The number of warmup epochs is set to be $n_{epochs} / 10$.


\begin{table}[ht]
\centering
\caption{Hyperparameters.}
\label{tab:hyperparameters}
\begin{adjustbox}{width=1\textwidth}
\small
\begin{tabular}{lccccc}
\toprule

\textbf{Method} & \textbf{Am. Computers} & \textbf{Am. Photos} & \textbf{Co. CS} & \textbf{Co. Physics} & \textbf{PPI} \\
\midrule
$L$ & 2 & 2 & 2 & 2 & 3\\
$K$ & 1 & 1 & 1 & 1 & 1\\
$D$ & 512 & 512 & 512 & 512 & 512 \\
Propagation & $\mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ & $\mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ & $\mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ & $\mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ & $\mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ \\
\midrule
$n_{epochs}$ & 5000 & 1000 & 1000 & 1000 & 2000\\
$n_{warmup}$ & 500 & 100 & 100 & 100 & 200 \\
$lr$ & $10^{-4}$ & $10^{-4}$ & $10^{-5}$ & $10^{-5}$ & $10^{-4}$ \\
\midrule
$\lambda_1$ & 10 & 10 & 20 & 20 & 15 \\
$\lambda_2$ & 5 & 5 & 15 & 15 & 10\\
$\lambda_3$ & 1 & 1 & 1 & 1 & 10\\
\midrule
$p_{input}$ & 0.5 & 0.5 & 0.5 & 0.5 & 0.0 \\
$p_{local}$ & 0.0 & 0.0 & 0.0 & 0.0 & 0.5 \\
\bottomrule
\end{tabular}

\end{adjustbox}
\end{table}


\section{Dataset details}
\label{sec:datasets}

\textbf{Amazon Computers, Amazon Photos} are extracted from the Amazon co-purchase graph \cite{10.1145/2766462.2767755} whose nodes represent products and edges encode whether two elements are usually purchased together. Node features are a vector representation of a bag-of-words from product's reviews and nodes are classified into 10 (for Computers) and 8 (for Photos) classes, given by the product category. Since there is no standard split for these datasets, we randomly split the nodes into (10/10/80\%) for train, validation and test, respectively.

\textbf{Coauthor Computer Science, Coauthor Physics} are from the Microsoft Academic Graph \cite{10.1145/2740908.2742839} from the KDD Cup 2016 challenge. Nodes represent authors, which are connected by an edge if they have co-authored a paper. Node features encode the keywords of each author's papers and authors are labeled into 15 (for CS) and 5 (for Physics) according to their most active field of study. We also use a random split into (10/10/80\%) for train, validation and test, respectively.

\textbf{PPI} is a proteint-protein interaction network \cite{Zitnik2017}. It consists of 24 independent graphs that correspond to different human tissues, whose nodes are proteins and edges represent interactions between them. Node features are biological properties and they are labeled according to the protein functions. We employ 20 graphs for training, 2 for validation and 2 for testing.