\section{Regularized graph infomax}
\label{sec:method}

In this section, we introduce the concept of feature propagation as self-supervision signals and detail our algorithm \textit{RGI - Regularized Graph Infomax} for self-supervised learning on graphs, which we show that maximizes the mutual information between the representations output by a graph neural network encoder and their propagation through the graph. The algorithm is described in Algorithm \ref{algo:rgi} and a visual illustration is shown in Figure \ref{fig:rgi}.

\begin{algorithm}[tb]
\caption{RGI}
\label{algo:rgi}
\begin{algorithmic}
   \STATE {\bfseries Input:} node features $\mathbf{X}$; adjacency matrix $\mathbf{A}$; parameters $\Theta$, $\phi$ and $\psi$; backbone $f_{\Theta}$; reconstruction networks $h_{\phi}$ and $h_{\psi}$; feature propagation function $g_K$.
   \REPEAT
   \STATE // obtain node embeddings
   \STATE $\mathbf{U} = f_{\Theta}({\mathbf{X}}, {\mathbf{A}})$
   \STATE // propagate node embeddings
   \STATE $\mathbf{V} = g_K({\mathbf{U}, \mathbf{A}})$
   \STATE // reconstruction
   \STATE $\mathbf{V}'=h_{\phi}(\mathbf{U})$
   \STATE $\mathbf{U}'=h_{\psi}(\mathbf{V})$
   \STATE // covariance matrices
   \STATE $\mathcal{C}_u= \frac{1}{N} \bar{\mathbf{U}}^T \bar{\mathbf{U}} $
   \STATE $\mathcal{C}_v     = \frac{1}{N} \bar{\mathbf{V}}^T \bar{\mathbf{V}} $
   \STATE // loss function
   \STATE $\mathcal{L}_{rec} =  \parallel \mathbf{U} - \mathbf{U}' \parallel^2_F +  \parallel \mathbf{V} - \mathbf{V}' \parallel^2_F $
   \STATE $\mathcal{L}_{var} = (1-$diag$(\mathcal{C}_u))^2 + (1-$diag$(\mathcal{C}_v))^2$
   \STATE $\mathcal{L}_{cov} = ($off-diag$(\mathcal{C}_u))^2 + ($off-diag$(\mathcal{C}_v)^2$
   \STATE $\mathcal{L} = \lambda_1 \mathcal{L}_{rec} + \lambda_2 \mathcal{L}_{var} + \lambda_3 \mathcal{L}_{cov} $
   \STATE // update parameters
   \STATE $\Theta, \phi, \psi \leftarrow \bigtriangledown_{\Theta, \phi, \psi} \mathcal{L}$
   \UNTIL{convergence}
   \STATE {\bfseries return} $\mathbf{U}$
\end{algorithmic}
\end{algorithm}

\subsection{Context and notation}
\label{sec:method_context}
Let $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ be a graph of $N$ nodes where $\mathcal{V}$ is the node set and $\mathcal{E}$ the edge set. Node attributes $X$ come from a $d$-dimensional distribution and the nodes' realizations are arranged in a matrix $\mathbf{X} = \{ \mathbf{x}_i \}_{i=1}^{N} \in \mathbb{R}^{N \times d}$, where $\mathbf{x}_i \in \mathbb{R}^{d}$ are the attributes of node $i$. $\mathcal{E}$ comes in the form of (unweighted) adjacency matrix $\mathbf{A} \in \{0,1\}^{N \times N}$, $\mathbf{A}_{ij} = 1$ if the edge $e_{ij} \in \mathcal{E}$, $0$ otherwise. The $K$-hop neighborhood of a node $i$, $\mathcal{G}_i^{(K)}$, is represented by the set of neighbors' attributes, $\mathbf{X}_i^{(K)}$ and the induced adjacency matrix $\mathbf{A}_i^{(K)}$. The same notation criterion of $X$, $\mathbf{X}$, $\mathbf{x}_i$ and $\mathbf{X}_i$ will be employed for other node hidden representations.

The goal of self-supervised learning on graphs is to fit a graph neural network encoder $f_{\Theta} : \mathbb{R}^{N \times d} \times \{0,1\}^{N \times N} \xrightarrow{} \mathbb{R}^{N \times D}$, $\mathbf{U} = f_{\Theta}(\mathcal{G}) = f_{\Theta}(\mathbf{X}, \mathbf{A})$, parametrized by $\Theta$, that obtains a $D$-dimensional vector representation $\mathbf{u}_i$ for every node of the input graph without relying on node annotations.

\subsection{Feature propagation as supervision signals}
\label{sec:method_propagation}

%As it has been previously stated, predicting future observations is a common task for self-supervision in grid domains. In this section, we explain how this idea is extended to graph data and how to efficiently implement it with embedding propagation.

%\paragraph{Predicting the next hop} RGI extends the task of predicting future observations to graphs. Based on how GNNs process graph structured data by sequentially aggregating neighborhood information, increasing the nodes' reciptive field by one hop after each convolutional layer, we leverage this notion of sequence of hops and assign a sequential component to every node $i$, $[ \mathcal{G}_i^{(1)}, \mathcal{G}_i^{(2)}, ... ]$, in order to define their future observations. Concretely, the GNN $f_{\Theta}$, which encodes the $L$-hop neighborhood, is trained to output representations $\mathbf{u}_i=f_{\Theta}( \mathcal{G}_i^{(L)} ) $ that easily predict a future hop, namely the $L+K$-hop: $\mathcal{G}_i^{(L+K)}$.

%The encoding of $\mathcal{G}_i^{(L+K)}$ could be achieved with a $(L+K)$-layer GNN, or by propagating the input features for $(L+K)$ steps. However, in real world scenarios, graph data is usually incomplete, sparse and can even do not include initial node features $\mathbf{X}$. Therefore, rather than relying on the input node features to obtain the $L+K$ hop for every node, we implement the \textit{embedding propagation} scheme. In particular, given the node embeddings $\mathbf{U} = f_{\Theta}(\mathbf{X},\mathbf{A})$, we propagate these embeddings through the graph for $K$ steps: $\mathbf{V} = g_{K} ( \mathbf{U}, \mathbf{A} )$, where $\mathbf{V} \in \mathcal{R}^{N \times D}$ is the matrix containing the resulting propagated embedding for every node and $g(\mathbf{A})$ is a non-parametric function of the graph adjacency matrix or any of its variants, such as the symmetrically normalized version. In RGI, the propagated embeddings $\mathbf{V}$ are used as supervision signals.
RGI train an encoder to maximize the mutual information between the embeddings output by the encoder, $\mathbf{U} = f_{\Theta}(\mathbf{X}, \mathbf{A})$, and their propagation through the graph, $\mathbf{V} = g_K(\mathbf{U}, \mathbf{A})$, named \textit{local} and \textit{global} node views or embeddings, respectively. 

As shown in Section \ref{sec:method_locglob}, the propagated embeddings capture the global information and structure of the graph while being particular to every node. Leveraging them as supervision signals, the model is trained to output more informative node embeddings that are aware of both their local and global context within the graph. The propagation function $g_K$ is implemented as a $K$ order polynomial of the graph adjacency matrix or any of its variants, such as the symmetrically normalized adjacency matrix, so that RGI does not rely on a two branch architecture and the computational burden to obtain the supervision signals is minimal compared to other methods.

We perform feature propagation over the embeddings $\mathbf{U}$ to create the supervision signals rather than the propagation of the raw input features $\mathbf{X}$ since real world graph data is usually incomplete and sparse. Additionally, there may be scenarios without initial node features.

\subsection{Local and global contexts}
\label{sec:method_locglob}
We name $\mathbf{U}$ and $\mathbf{V}$ as node local and global node embeddings, respectively. Note, however, that the global definition is different from the global pooling proposed in DGI \cite{velickovic2018deep}, since we define node level global embeddings rather than graph level. The reason to call $\mathbf{V}$ \textit{global} is as follows. For a target node $i$, we obtain its local embedding with a $L$-layer GNN, $\mathbf{u}_i=f_{\Theta}(\mathbf{X}_i^{(L)}, \mathbf{A}_i^{(L)})$, so that it depends on its $L$-hop neighborhood. Afterwards, its global context is computed by propagating the representations through the graph during $K$ steps, $\mathbf{v}_i=g_K(\mathbf{U}_i^{(K)}, \mathbf{A}_i^{(K)})$. Therefore, $\mathbf{v}_i$ contains information of the $(L+K)$-hop neighborhood of $i$. It is known that for a small-world network $\mathcal{G}$ of $N$ nodes, the diameter of the graph is $diam(\mathcal{G})=log(N)$. Consequently, in a small world network, $\mathbf{v}_i$ can encode global information of every node in the graph as long as $L + K \simeq log(N)$.

\subsection{Loss function}
\label{sec:method_loss}
Based on Equation \ref{eq:mi_bound}, the mean squared error is employed as loss function to maximize the mutual information. RGI includes an auxiliary network $h_{\phi}$ implemented as a multi-layer perceptron to predict the local embeddings from the global embeddings. The network is jointly trained with the encoder, but discarted afterwards.

\begin{equation}
    \label{eq:loss_rec}
    \mathcal{L}_{rec}(U|V) = \frac{1}{N} \sum_{i=1}^{N} \parallel \mathbf{u}_i - h_{\phi}( \mathbf{v}_i ) \parallel_2^2
\end{equation}

However, as the representations $U$ are not fixed, but they depend on the encoder parameters instead, only addressing the reconstruction error would lead the encoder to a collapse, in which the input is ignored and it outputs a constant representation. To alleviate this problem, we incorporate covariance matrix regularization loss terms to avoid the collapse of the representations \cite{zbontar2021barlow, bardes2022vicreg, bielak2021graph, zhang2021canonical}:

\begin{equation}
    \label{eq:loss_var}
    \mathcal{L}_{var}(U) = \frac{1}{D} \sum_{n=1}^{D} \left ( 1 - \mathcal{C}_{nn} \right )^2
\end{equation}

\begin{equation}
    \label{eq:loss_cov}
    \mathcal{L}_{cov}(U) = \frac{1}{D} \sum_{n=1}^{D} \sum_{m \neq n} \mathcal{C}_{nm}^2
\end{equation}

where $\mathcal{C}$ is the sample covariance matrix of $U$. Concretely, the former guides the diagonal elements (variances) to be close to one whereas the latter forces the non-diagonal elements (covariances) to be zero. Intuitively, maximizing the variance avoids the total collapse to a constant representation. On the other hand, covariance minimization encourages the encoder to leverage the whole capacity of the representation space rather than projecting the points to a lower dimensional subspace, also known as dimensional collapse \cite{hua2021feature}. The loss function is a weighted combination of these three terms:

 \begin{equation}
     \label{eq:loss_u}
     \mathcal{L}_u = \lambda_1 \mathcal{L}_{rec}(U|V) + \lambda_2 \mathcal{L}_{var}(U) + \lambda_3 \mathcal{L}_{cov}(U)
 \end{equation}

Where $\lambda_1, \lambda_2, \lambda_3 \in \mathbb{R}$ are weight hyperparameters. In practice, we symmetrize this loss by also predicting $\mathbf{v}_i$ from $\mathbf{u}_i$ with another auxiliary network $h_{\psi}$ and applying variance-covariance regularization to $V$.

\subsection{Mutual information maximization}
\label{sec:method_motiv}

In this section, we show that, as previously stated in Section \ref{sec:method_propagation}, RGI maximizes the mutual information between the node embeddings and their propagation.

\begin{assumption}
    \label{ass:gau_u}
    The node embeddings $\mathbf{U}$ follow a Gaussian distribution $U \sim N(\mu_u, \Sigma_u)$.
\end{assumption}

\begin{assumption}
    \label{ass:gau_v}
    The propagated embeddings $\mathbf{V}$ follow a Gaussian distribution $V \sim N(\mu_v, \Sigma_v)$.
\end{assumption}

\begin{proposition}
    \label{prop:mi_max}
    Based on Assumption \ref{ass:gau_u}, minimizing the objective $\mathcal{L}_u$ of Equation \ref{eq:loss_u} maximizes the mutual information between $U$ and $V$. 
\end{proposition}
\begin{proof}
    The proof is based on the MI lower bound of Equation \ref{eq:mi_bound}. In order to maximize the MI between $U$ and $V$, it is necessary to address both the maximization of $H(U)$ and the minimization of $\mathcal{R}(U|V)$, as the representations $U$ are not fixed and only optimizing $\mathcal{R}(U|V)$ would also affect the value of $H(U)$. The first term of the loss in Equation \ref{eq:loss_u} is the reconstruction error $\mathcal{R}(U|V)$, since our representations are continuous-valued, this is achieved with the mean-squared error. Secondly, under the Gaussianity assumption, the entropy of $U$ is proportional to the logarithm of the determinant of the covariance matrix, that is,  $H(U) \propto log(\left | \Sigma_u \right |)$. Being the logarithm an increasing function, the entropy maximization can be tackled by maximizing $\left | \Sigma_u \right |$. Although we do not have access to the true covariance matrix, it is approximated by targetting the sample covariance $\mathcal{C}$. Rather than directly maximizing the determinant, the proxy consists of maximizing the diagonal elements of the matrix while also forcing the non-diagonal elements to be close to 0.
    
    Despite the fact that the mutual information is a symmetric quantity, the approximation based on the reconstruction error and sample covariance matrix are not. Hence, the loss is symmetrized given the Assumption \ref{ass:gau_v}.
\end{proof}

%\begin{proposition}
%    \label{prop:mi_neigh} If $K=1$ and $g(\mathbf{A})=\mathbf{D}^{-1}\mathbf{A}$, RGI maximizes the expected MI between a node representation and its neighbors $\mathbb{E}_{j} \{ I(\mathbf{u}_i;\mathbf{u}_j) \}$
%\end{proposition}

%\begin{proposition}
%    If $K=1$, variance-covariance regularization on the propagated embeddings $V$ regularizes the cross-covariance matrix between 2nd order neighbors.
%\end{proposition}
%\begin{proof}
%    By definition, $\mathbf{V}=g_K(\mathbf{U} \mathbf{A})$ and when $K=1$ we can write embedding propagation as $\mathbf{V}=g(\mathbf{A})\mathbf{U}$. For simplicity, we will assume that $g(\mathbf{A})=\mathbf{A}$ and that $\mathbf{A}$ is symmetric. Variance-covariance regularization is based on the sample covariance matrix, which is obtained from the mean-centered matrix: $\hat{\mathbf{V}}=(\mathbf{I}-\frac{1}{N}\mathbf{J})\mathbf{V}$, where $\mathbf{I}, \mathbf{J} \in \mathbb{R}^{N \times N}$ are the identity matrix and a constant matrix whose entries are 1, respectively.

%    Then, the sample covariance matrix is obtained as:

%    \begin{equation}
%        \label{eq:cov_v}
%        \mathcal{C}_v = \hat{\mathbf{V}}^T \hat{\mathbf{V}} = \mathbf{V} \left (\mathbf{I}-\frac{1}{N}\mathbf{J} \right ) \mathbf{V} = \mathbf{U}^T \mathbf{A} \left ( \mathbf{I}-\frac{1}{N}\mathbf{J} \right ) \mathbf{A} \mathbf{U} = \mathbf{U}^T \left ( \mathbf{A}^2-\frac{1}{N}\mathbf{A}\mathbf{J}\mathbf{A} \right ) \mathbf{U} 
%    \end{equation}
%\end{proof}