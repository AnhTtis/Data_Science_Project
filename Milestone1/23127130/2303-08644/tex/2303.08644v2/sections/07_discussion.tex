\section{Discussion}
\label{sec:discussion}
We have presented an algorithm that achieves state-of-the-art performance even though it is much simpler in terms of theoretical interpretation, architecture and training strategy than other methods. In this section, we detail aspects of RGI and its advantages and limitations with respect to other methods.

\paragraph{Augmentation-free and non-contrastive} Graph contrastive learning is the most popular approch to avoid the collapse of the representations by relying on negative pair sampling \cite{velickovic2018deep, sun2019infograph, Zhu:2020vf, NEURIPS2021_ff1e68e7, https://doi.org/10.48550/arxiv.2204.04874}. BGRL \cite{thakoor2021bootstrapped},  CCA-SSG \cite{zhang2021canonical} and G-BT \cite{bielak2021graph}, instead, do not need negative samples. The former avoids collapse with an asymmetric architecture and the other two, regularizing the covariance matrix of the representations. In this work, we also adopt a regularized solution since it is more interpretable and naturally avoids the collapse whereas it is still an open problem to theoretically demonstrate that bootstrapped methods avoid trivial solutions. However, these methods train an encoder with the invariance via data augmentation principle. It has been stated that transformations that drop information may modify the semantics of the graph so the invariance assumption may be incorrect and not hold for all graph domains. RGI, instead, requires no transformations as simply employs feature propagation to create the supervision signals, so that it is much more intuitive and only involves graph convolution-like operations.

\paragraph{Single branch architecture} Current state of the art algorithms usually rely on a joint embedding architecture that require multiple forward passes at each training step. For example, BGRL \cite{thakoor2021bootstrapped} forwards the graph through the encoder four times at each iteration. On the contrary, RGI is much more efficient and only performs one single forward pass while achieving similar performance to BGLR and other methods.

\paragraph{Simplicity and effectiveness} Being augmentation free, requiring a single branch architecture and being non-contrastive, substantially reduce both the time and space complexity of the algorithm for self-supervised learning on graphs with respect to other methods. First of all, the complexity of contrastive methods is quadratic with respect to the number of nodes, that is $\mathcal{O}(N^2)$. Instead, covariance regularization makes the complexity be quadratic on the embedding space dimensionality $\mathcal{O}(D^2)$, which is generally much lower than the number of nodes $D << N$. Additionally, being a single branch architecture reduces the time and space complexity of computing multiple forward and backward passes through the encoder during training.

\paragraph{Evaluation protocol}
In the context of self-supervised learning of graph neural networks, while the linear evaluation protocol is commonly used to compare different methods, it is important to consider the impact of other factors such as the differences in the encoder architecture, as well as the training protocol and evaluation-related hyperparameters. In our experiments, we observed a drop in performance on GraphMAE \cite{hou2022graphmae}, which reports state-of-the-art performance in the original paper, when we re-ran the method with the same dimensionality and linear evaluation training than the majority of the other methods of the comparison. This highlights the importance of a standardized evaluation protocol for self-supervised GNNs, which can help to ensure fair comparisons between different methods and provide a more accurate assessment of their performance. Therefore, we suggest that future work in the field of self-supervised learning on graphs should consider addressing these concerns.

\paragraph{Limitations}
RGI has been shown to be an effective algorithm to learn node-level representations in a self-supervised manner. However, by definition, RGI operates on homogeneous graphs so that, in general, it could not be directly applied to heterogeneous scenarios. Although RGI requires a single branch architecture, which simplifies the training procedure with respect other methods, it comes at expense of extra depth to obtain the propagated embeddings, which increase the complexity in settings in which neighbor sampling is a must. Nonetheless, our experiments show that $K=1$ is generally the best choice as the number of steps to propagate the information through the graph, so that this issue is alleviated.