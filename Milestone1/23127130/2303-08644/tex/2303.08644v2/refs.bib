@inproceedings{
hjelm2018learning,
title={Learning deep representations by mutual information estimation and maximization},
author={R Devon Hjelm and Alex Fedorov and Samuel Lavoie-Marchildon and Karan Grewal and Phil Bachman and Adam Trischler and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bklr3j0cKX},
}
@inproceedings{
velickovic2018deep,
title="{Deep Graph Infomax}",
author={Petar Veli{\v{c}}kovi{\'{c}} and William Fedus and William L. Hamilton and Pietro Li{\`{o}} and Yoshua Bengio and R Devon Hjelm},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rklz9iAcKQ},
}
@inproceedings{bardes2022vicreg,
  author  = {Adrien Bardes and Jean Ponce and Yann LeCun},
  title   = {VICReg: Variance-Invariance-Covariance Regularization For Self-Supervised Learning},
  booktitle = {ICLR},
  year    = {2022},
}
@misc{grill2020bootstrap,
    title = {Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning},
    author = {Jean-Bastien Grill and Florian Strub and Florent Altché and Corentin Tallec and Pierre H. Richemond and Elena Buchatskaya and Carl Doersch and Bernardo Avila Pires and Zhaohan Daniel Guo and Mohammad Gheshlaghi Azar and Bilal Piot and Koray Kavukcuoglu and Rémi Munos and Michal Valko},
    year = {2020},
    eprint = {2006.07733},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}
@article{chen2020simple,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2002.05709},
  year={2020}
}
@article{chen2020big,
  title={Big Self-Supervised Models are Strong Semi-Supervised Learners},
  author={Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2006.10029},
  year={2020}
}

@inproceedings{sun2019infograph,
  title={InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization},
  author={Sun, Fan-Yun and Hoffman, Jordan and Verma, Vikas and Tang, Jian},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@misc{thakoor2021bootstrapped,
     title={Large-Scale Representation Learning on Graphs via Bootstrapping}, 
     author={Shantanu Thakoor and Corentin Tallec and Mohammad Gheshlaghi Azar and Mehdi Azabou and Eva L. Dyer and Rémi Munos and Petar Veličković and Michal Valko},
     year={2021},
     eprint={2102.06514},
     archivePrefix={arXiv},
     primaryClass={cs.LG}}

@inproceedings{zhang2021canonical,
  title={From canonical correlation analysis to self-supervised graph neural networks},
  author={Zhang, Hengrui and Wu, Qitian and Yan, Junchi and Wipf, David and Philip, S Yu},
  booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
  year={2021}
}

@inproceedings{Zhu:2020vf,
  author = {Zhu, Yanqiao and Xu, Yichen and Yu, Feng and Liu, Qiang and Wu, Shu and Wang, Liang},
  title = {{Deep Graph Contrastive Representation Learning}},
  booktitle = {ICML Workshop on Graph Representation Learning and Beyond},
  year = {2020},
  url = {http://arxiv.org/abs/2006.04131}
}

@incollection{icml2020_1971,
 author = {Hassani, Kaveh and Khasahmadi, Amir Hosein},
 booktitle = {Proceedings of International Conference on Machine Learning},
 pages = {3451--3461},
 title = {Contrastive Multi-View Representation Learning on Graphs},
 year = {2020}
}
@inproceedings{NEURIPS2021_ff1e68e7,
 author = {Xu, Dongkuan and Cheng, Wei and Luo, Dongsheng and Chen, Haifeng and Zhang, Xiang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {30414--30425},
 publisher = {Curran Associates, Inc.},
 title = {InfoGCL: Information-Aware Graph Contrastive Learning},
 url = {https://proceedings.neurips.cc/paper/2021/file/ff1e68e74c6b16a1a7b5d958b95e120c-Paper.pdf},
 volume = {34},
 year = {2021}
}
@article{zbontar2021barlow,
  title={Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  author={Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{\'e}phane},
  journal={arXiv preprint arXiv:2103.03230},
  year={2021}
}

@ARTICLE{Bell1995-je,
  title    = "An information-maximization approach to blind separation and
              blind deconvolution",
  author   = "Bell, A J and Sejnowski, T J",
  abstract = "We derive a new self-organizing learning algorithm that maximizes
              the information transferred in a network of nonlinear units. The
              algorithm does not assume any knowledge of the input
              distributions, and is defined here for the zero-noise limit.
              Under these conditions, information maximization has extra
              properties not found in the linear case (Linsker 1989). The
              nonlinearities in the transfer function are able to pick up
              higher-order moments of the input distributions and perform
              something akin to true redundancy reduction between units in the
              output representation. This enables the network to separate
              statistically independent components in the inputs: a
              higher-order generalization of principal components analysis. We
              apply the network to the source separation (or cocktail party)
              problem, successfully separating unknown mixtures of up to 10
              speakers. We also show that a variant on the network architecture
              is able to perform blind deconvolution (cancellation of unknown
              echoes and reverberation in a speech signal). Finally, we derive
              dependencies of information transfer on time delays. We suggest
              that information maximization provides a unifying framework for
              problems in ``blind'' signal processing.",
  journal  = "Neural Comput",
  volume   =  7,
  number   =  6,
  pages    = "1129--1159",
  month    =  nov,
  year     =  1995,
  address  = "United States",
  language = "en"
}

@ARTICLE{36,

  author={Linsker, R.},

  journal={Computer}, 

  title={Self-organization in a perceptual network}, 

  year={1988},

  volume={21},

  number={3},

  pages={105-117},

  doi={10.1109/2.36}}

@inproceedings{
Tschannen2020On,
title={On Mutual Information Maximization for Representation Learning},
author={Michael Tschannen and Josip Djolonga and Paul K. Rubenstein and Sylvain Gelly and Mario Lucic},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkxoh24FPH}
}

@misc{https://doi.org/10.48550/arxiv.1807.03748,
  doi = {10.48550/ARXIV.1807.03748},
  
  url = {https://arxiv.org/abs/1807.03748},
  
  author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Representation Learning with Contrastive Predictive Coding},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{hua2021feature,
  title={On Feature Decorrelation in Self-Supervised Learning},
  author={Hua, Tianyu and Wang, Wenxiao and Xue, Zihui and Wang, Yue and Ren, Sucheng and Zhao, Hang},
  journal={arXiv e-prints},
  pages={arXiv--2105},
  year={2021}
}
@inproceedings{
peng2020graph,
title="{Graph Representation Learning via Graphical Mutual Information Maximization}",
author={Peng, Zhen and Huang, Wenbing and Luo, Minnan and Zheng, Qinghua and Rong, Yu and Xu, Tingyang and Huang, Junzhou},
booktitle={Proceedings of The Web Conference},
year={2020},
doi={https://doi.org/10.1145/3366423.3380112},
}

@misc{bielak2021graph,
      title={Graph Barlow Twins: A self-supervised representation learning framework for graphs},
      author={Piotr Bielak and Tomasz Kajdanowicz and Nitesh V. Chawla},
      year={2021},
      eprint={2106.02466},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{10.5555/3045118.3045167,
author = {Ioffe, Sergey and Szegedy, Christian},
title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
year = {2015},
publisher = {JMLR.org},
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {448–456},
numpages = {9},
location = {Lille, France},
series = {ICML'15}
}

@inproceedings{NIPS2017_5dd9db5e,
 author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Inductive Representation Learning on Large Graphs},
 url = {},
 volume = {30},
 year = {2017}
}

@misc{https://doi.org/10.48550/arxiv.2204.04874,
  doi = {10.48550/ARXIV.2204.04874},
  
  url = {https://arxiv.org/abs/2204.04874},
  
  author = {Wang, Haonan and Zhang, Jieyu and Zhu, Qi and Huang, Wei},
  
  keywords = {Machine Learning (cs.LG), Social and Information Networks (cs.SI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Augmentation-Free Graph Contrastive Learning with Performance Guarantee},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{
kipf2017semisupervised,
title={Semi-Supervised Classification with Graph Convolutional Networks},
author={Thomas N. Kipf and Max Welling},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=SJU4ayYgl}
}

@inproceedings{
veličković2018graph,
title={Graph Attention Networks},
author={Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJXMpikCZ},
}

@article{lee2021augmentation,
  title={Augmentation-Free Self-Supervised Learning on Graphs},
  author={Lee, Namkyeong and Lee, Junseok and Park, Chanyoung},
  booktitle={AAAI},
  year={2022}
}

@inproceedings{Fey/Lenssen/2019,
  title={Fast Graph Representation Learning with {PyTorch Geometric}},
  author={Fey, Matthias and Lenssen, Jan E.},
  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
  year={2019},
}

@article{paszke2017automatic,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{DBLP:journals/corr/KingmaB14,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/2766462.2767755,
author = {McAuley, Julian and Targett, Christopher and Shi, Qinfeng and van den Hengel, Anton},
title = {Image-Based Recommendations on Styles and Substitutes},
year = {2015},
isbn = {9781450336215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2766462.2767755},
doi = {10.1145/2766462.2767755},
abstract = {Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.},
booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {43–52},
numpages = {10},
keywords = {visual features, metric learning, recommender systems},
location = {Santiago, Chile},
series = {SIGIR '15}
}

@inproceedings{10.1145/2740908.2742839,
author = {Sinha, Arnab and Shen, Zhihong and Song, Yang and Ma, Hao and Eide, Darrin and Hsu, Bo-June (Paul) and Wang, Kuansan},
title = {An Overview of Microsoft Academic Service (MAS) and Applications},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2742839},
doi = {10.1145/2740908.2742839},
abstract = {In this paper we describe a new release of a Web scale entity graph that serves as the backbone of Microsoft Academic Service (MAS), a major production effort with a broadened scope to the namesake vertical search engine that has been publicly available since 2008 as a research prototype. At the core of MAS is a heterogeneous entity graph comprised of six types of entities that model the scholarly activities: field of study, author, institution, paper, venue, and event. In addition to obtaining these entities from the publisher feeds as in the previous effort, we in this version include data mining results from the Web index and an in-house knowledge base from Bing, a major commercial search engine. As a result of the Bing integration, the new MAS graph sees significant increase in size, with fresh information streaming in automatically following their discoveries by the search engine. In addition, the rich entity relations included in the knowledge base provide additional signals to disambiguate and enrich the entities within and beyond the academic domain. The number of papers indexed by MAS, for instance, has grown from low tens of millions to 83 million while maintaining an above 95% accuracy based on test data sets derived from academic activities at Microsoft Research. Based on the data set, we demonstrate two scenarios in this work: a knowledge driven, highly interactive dialog that seamlessly combines reactive search and proactive suggestion experience, and a proactive heterogeneous entity recommendation.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {243–246},
numpages = {4},
keywords = {academic search, recommender systems, entity conflation},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@article{Zitnik2017,
  title={Predicting multicellular function through multi-layer tissue networks},
  author={Zitnik, Marinka and Leskovec, Jure},
  journal={Bioinformatics},
  volume={33},
  number={14},
  pages={190-198},
  year={2017}
}

@misc{https://doi.org/10.48550/arxiv.2207.10081,
  doi = {10.48550/ARXIV.2207.10081},
  
  url = {https://arxiv.org/abs/2207.10081},
  
  author = {Shwartz-Ziv, Ravid and Balestriero, Randall and LeCun, Yann},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {What Do We Maximize in Self-Supervised Learning?},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{hu2021open,
      title={Open Graph Benchmark: Datasets for Machine Learning on Graphs}, 
      author={Weihua Hu and Matthias Fey and Marinka Zitnik and Yuxiao Dong and Hongyu Ren and Bowen Liu and Michele Catasta and Jure Leskovec},
      year={2021},
      eprint={2005.00687},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{hou2022graphmae,
  title={GraphMAE: Self-Supervised Masked Graph Autoencoders},
  author={Hou, Zhenyu and Liu, Xiao and Cen, Yukuo and Dong, Yuxiao and Yang, Hongxia and Wang, Chunjie and Tang, Jie},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={594--604},
  year={2022}
}