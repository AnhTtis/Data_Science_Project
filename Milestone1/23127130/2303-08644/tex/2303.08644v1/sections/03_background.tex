\section{Background}
\subsection{Information theory for representation learning}
\paragraph{Mutual information} The mutual information (MI) between two random variables $X$ and $Y$, $I(X;Y)$ is a symmetric quantity, $I(X;Y)=I(Y;X)$, that measures how much information one variable carries about the other. Formally, it is defined as:
\begin{equation}
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
\label{mi}
\end{equation}
where $H(X)$ is the entropy of $X$ and $H(X|Y)$ is the conditional entropy of $X$ given $Y$. This quantity can be lower-bounded by the expected reconstruction error, which is usually employed in generative models:
\begin{equation}
I(X;Y) = H(X) - H(X|Y) \geq H(X) - \mathcal{R}(X|Y)
\label{mi_bound}
\end{equation}
where $\mathcal{R}(X|Y)$ is the expected reconstruction error of $X$ given $Y$. In practice, this expected error is approximated with the square loss or the cross-entropy loss.

\paragraph{InfoMax principle} The InfoMax principle \cite{36} states that a neural network can be trained in a self-supervised manner by maximizing the mutual information between its input $X$ and its output $Y$, $I(X;Y)$. To do so, \cite{Bell1995-je}  suggests that it is enough to maximize the entropy of $Y$, which, for the example of a one layer n-to-n network, is achieved by maximizing the logarithm of the jacobian of the weights. The intuition is that this quantity can be seen as the log of the volume space of $Y$ onto which the values of $X$ are mapped.

\subsection{Multi-view representation learning} 
The InfoMax principle is extended to a multi-view approach, in which rather than maximizing the MI between the input and output of the network, the agreement is maximized between the representations of two different views of the input. This scenario has two main advantages, (i) the loss is computed in the representation space, which is in general lower dimensional and avoids focusing on small details of the input and (ii) views can be defined to capture different aspects of the data \cite{Tschannen2020On}.

\textbf{Local - global MI.} Deep InfoMax \cite{hjelm2018learning} trains an encoder maximizing the average mutual information between local patches and global representations of an image. Deep Graph InfoMax \cite{velickovic2018deep} and InfoGraph \cite{sun2019infograph} extend this work to the graph domain, targetting the MI between node and graph level embeddings. The graph representation is obtained with a global pooling layer applied to the local node embeddings. In DGI, since most datasets consist of one single graph, the authors create a corrupted version of the graph by shuffling the node features and contrasting negative and positive pairs.

%\paragraph{Patch - Context MI} Contrastive Predictive Coding \cite{https://doi.org/10.48550/arxiv.1807.03748} represents images as an ordered sequence of patches, encodes every patch to a vector representation, aggregates the embeddings of the first \textit{t} patches to obtain the context vector and maximizes the MI between this context and the patch representation at position \textit{t + k} with the InfoNCE loss.
\textbf{Invariance via data augmentation.} SimCLR \cite{chen2020simple}, BYOL \cite{grill2020bootstrap}, Barlow Twins \cite{zbontar2021barlow} and VICReg \cite{bardes2022vicreg}, among others, create two augmented views of an image via data augmentation, such as image rotation and cropping, and train the encoder to be invariant to those augmentations, not necessarily with MI objectives. These works are also extended for self-supervised grapSh representation learning. For instance, GRACE \cite{Zhu:2020vf} follows a similar approach to SimCLR based on contrastive learning, BGRL \cite{thakoor2021bootstrapped} employs the same asymmetric scheme than BYOL and G-BT, \cite{bielak2021graph} directly extends Barlow Twins cross-covariance regularization objective.

\subsection{Avoiding Collapse}
Maximizing the agreement between views can lead to a total collapse in which the encoder outputs the same representation independently from the input. In order to diminish this phenomena, either architectural, regularization or training tricks can be employed:

\textbf{Contrastive learning.} Contrastive methods not only encourage the two views of the input (positive pairs) to be similar, but they also force views from different inputs (negative pairs) to be different \cite{chen2020simple}, \cite{velickovic2018deep}, \cite{hjelm2018learning}, \cite{Zhu:2020vf}, \cite{icml2020_1971}, \cite{anonymous2023localized}, \cite{anonymous2023graph}, \cite{NEURIPS2021_ff1e68e7}. They have been successfully applied to all data domains, nonetheless, their performance is highly influenced by the number of negative pairs, usually requiring many of them to work efficiently.

\textbf{Knowledge distillation.} Knowledge distiallation methods do not need negative pairs \cite{grill2020bootstrap}, \cite{thakoor2021bootstrapped}. Instead, they construct a teacher-student asymmetric architecture combined with a stop-gradient operation. Concretely, the networks are fed with two augmented views and the student network is trained to predict the output of the teacher model. Collapse is avoided by not backpropagating gradients through the teacher network to update its parameters, but setting them to be a moving average of the student's parameters.

\textbf{Covariance regularization.} Covariance regularization consists of extending the loss function by including regularization terms on the covariance matrix of the representations, forcing high variances for every feature and low co-variances \cite{bardes2022vicreg}, \cite{zbontar2021barlow}, \cite{zhang2021canonical}. Under the gaussianity assumption, these loss terms attempt to regularize the entropy of the latent space to avoid the collapse \cite{https://doi.org/10.48550/arxiv.2207.10081}.