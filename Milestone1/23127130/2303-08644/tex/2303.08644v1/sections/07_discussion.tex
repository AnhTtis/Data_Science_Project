\section{Discussion}
\label{discussion}
We have presented an algorithm that achieves state-of-the-art performance even though it is much simpler in terms of theoretical interpretation, architecture and training strategy than other methods. In this section, we detail aspects of RGI and its advantages with respect to other methods.

\textbf{Augmentation-free.} Most previous works train an encoder with the invariance via data augmentation principle \cite{thakoor2021bootstrapped}, \cite{Zhu:2020vf}, \cite{zhang2021canonical}, \cite{icml2020_1971}, \cite{NEURIPS2021_ff1e68e7}, \cite{peng2020graph}. It has been stated that transformations that drop information may modify the semantics of the graph so the invariance assumption may be incorrect and not hold for all graph domains. Other works propose alternative views leveraging the rich structure of graph data \cite{icml2020_1971}, \cite{NEURIPS2021_ff1e68e7}, \cite{peng2020graph}, \cite{lee2021augmentation}, \cite{https://doi.org/10.48550/arxiv.2204.04874}, but they also require carefully designed strategies to obtain the views. Alternatively, more recent methods also propose augmentation-free solutions \cite{anonymous2023localized}, \cite{https://doi.org/10.48550/arxiv.2204.04874} by incorporating training tricks. RGI falls into this category and require no transformations, however, it is much more intuitive and only involves graph convolution-like operations.

\textbf{Non-contrastive.} Graph contrastive learning is the most popular approch to avoid the collapse of the representations by relying on negative pair sampling \cite{velickovic2018deep}, \cite{sun2019infograph}, \cite{Zhu:2020vf}, \cite{NEURIPS2021_ff1e68e7}, \cite{https://doi.org/10.48550/arxiv.2204.04874}. BGRL \cite{thakoor2021bootstrapped} and CCA-SSG \cite{zhang2021canonical}, instead, do not need negative samples. The former avoids collapse with an asymmetric architecture and the later, regularizing the covariance matrix of the representations. In this work, we also adopt a regularized solution since it is more interpretable and naturally avoids the collapse whereas it is still an open problem to theoretically demonstrate that bootstrapped methods avoid trivial solutions.

\textbf{Single branch architecture.} Current state of the art algorithms usually rely on a joint embedding architecture that require multiple forward passes at each training step. For example, BGRL \cite{thakoor2021bootstrapped} forwards the graph through the encoder four times at each iteration. On the contrary, RGI is much more efficient and only performs one single forward pass while achieving similar performance to BGLR and other methods.

\textbf{Dropout regularization.}
Optionally, RGI can add noise to the input graph for example with dropout and edge sampling. However, this noise acts as regularization rather than data augmentation. First, we do not train the encoder to be invariant to these transformations since we only compute one forward pass. Secondly, no invariance is assumed, the loss function does not include any term to make the encoder invariant to the transformations, instead, we compute the iterations with noisy versions of the graph. Finally, node attribute masking transformation usually drops the same feature for all nodes of the graph whereas we are employing standard dropout on the input features.
