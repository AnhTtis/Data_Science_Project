\section{Results and evaluation}

\begin{table*}[t]
\caption{Statistics of the datasets employed in our experiments.}
\label{datasets}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
\textbf{Name} & \textbf{Task} & \textbf{Num. Nodes} & \textbf{Num. Edges} & \textbf{Node features} & \textbf{Num. Classes} \\
\midrule
Amazon Computers & Transductive & 13,752 & 245,861 & 767 & 10\\
Amazon Photos & Transductive & 7,650 & 119,081 & 745 & 8 \\
Coauthor CS & Transductive & 18,333 & 81,894 & 6,805 & 15 \\
Coauthor Physics & Transductive & 34,493 & 247,962 & 8,415 & 5 \\
\midrule
PPI (24 graphs) & Inductive & 56,944 & 818,716 & 50 & 121 (multilabel) \\

\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[t]
\caption{Classification accuracies on transductive datasets averaged for 20 weight initializations.}
\label{results_transductive}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Am. Computers} & \textbf{Am. Photos} & \textbf{Co. CS} & \textbf{Co. Physics} \\
\midrule
Raw ft. &  73.81 $\pm$ 0.00 & 78.53 $\pm$ 0.00 & 90.37 $\pm$ 0.00 & 93.58 $\pm$ 0.00\\
DeepWalk &  85.68 $\pm$ 0.06 & 89.44 $\pm$ 0.11 & 84.61 $\pm$ 0.22 & 91.77 $\pm$ 0.15\\
DeepWalk + feat. & 86.28 $\pm$ 0.07 & 90.05 $\pm$ 0.08 & 87.70 $\pm$ 0.04 & 94.90 $\pm$ 0.09\\
\midrule
Random-Init & 86.46 $\pm$ 0.38 & 92.08 $\pm$ 0.48 & 91.64 $\pm$ 0.29 & 93.71 $\pm$ 0.29\\
DGI \cite{velickovic2018deep}& 83.95 $\pm$ 0.47 & 91.61 $\pm$ 0.22 & 92.15 $\pm$ 0.63 & 94.51 $\pm$ 0.52\\
GMI \cite{peng2020graph} & 82.21 $\pm$ 0.31 & 90.68 $\pm$ 0.17 & OOM & OOM\\
MVGRL \cite{icml2020_1971} & 87.52 $\pm$ 0.11 & 91.74 $\pm$ 0.07 & 92.11 $\pm$ 0.12 & 95.33 $\pm$ 0.03\\
GRACE \cite{Zhu:2020vf}& 89.53 $\pm$ 0.35 & 92.78 $\pm$ 0.45 & 91.12 $\pm$ 0.20 & OOM\\
G-BT \cite{bielak2021graph} & 88.14 $\pm$ 0.33 & 92.63 $\pm$ 0.44 & 92.95 $\pm$ 0.17 & 95.07 $\pm$ 0.17 \\
CCA-SSG \cite{zhang2021canonical} & 88.74 $\pm$ 0.28 & 93.14 $\pm$ 0.14 & 93.31 $\pm$ 0.22 & 95.38 $\pm$ 0.06 \\
BGRL \cite{thakoor2021bootstrapped} & \textbf{90.34 $\pm$ 0.19} & 93.17 $\pm$ 0.30 & 93.31 $\pm$ 0.13 & 95.73 $\pm$ 0.05 \\
AFGRL \cite{lee2021augmentation} & 89.88 $\pm$ 0.37 &   93.22 $\pm$ 0.28 & 93.27 $\pm$ 0.17 & 95.69 $\pm$ 0.10 \\
AFGCL \cite{https://doi.org/10.48550/arxiv.2204.04874} & 89.68 $\pm$ 0.19 & 92.49 $\pm$ 0.31 & 91.92 $\pm$ 0.10 & 95.12 $\pm$ 0.15 \\
Local-GCL* \cite{anonymous2023localized} & 88.81 $\pm$ 0.37 & \textbf{93.25 $\pm$ 0.40} & \textbf{94.90 $\pm$ 0.19} & \textbf{96.33 $\pm$ 0.13} \\
PerturbGCL* \cite{anonymous2023graph} & 88.45 $\pm$ 0.77 & \textbf{93.62 $\pm$ 0.40} & \textbf{94.18 $\pm$ 0.09} & 95.85 $\pm$ 0.08 \\
\midrule
RGI (\textit{ours}) & \textbf{90.45 $\pm$ 0.08} & 92.94 $\pm$ 0.09 & 93.37 $\pm$ 0.07 & \textbf{95.91 $\pm$ 0.09} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table}[t]
\caption{Classification micro-average F1 score on PPI dataset averaged for 20 weight initializations.}
\label{results_ppi}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lc}
\toprule
\textbf{Method} & \textbf{PPI} \\
\midrule  
Raw ft. & 42.20 \\
\midrule
Rdm-Init & 62.60 $\pm$ 0.20 \\
DGI \cite{velickovic2018deep} & 63.80 $\pm$ 0.20 \\
GMI \cite{peng2020graph} & 65.00 $\pm$ 0.02 \\
GRACE \cite{Zhu:2020vf} & 69.71 $\pm$ 0.17 \\
G-BT \cite{bielak2021graph} & \textbf{70.49 $\pm$ 0.19} \\
BGRL \cite{thakoor2021bootstrapped} & \textbf{70.49 $\pm$ 0.05} \\
\midrule
RGI (\textit{ours}) & \textbf{72.16 $\pm$ 0.11}\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table*}[t]
\caption{Effect of loss symmetrization. Linear evaluation accuracy (micro-average F-Score for PPI dataset) after fitting and encoder with different self-supervised objective functions averaged for 5 weight initializations and data splits.}
\label{ablation_lossfn}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{cccccc}
\toprule
$\mathcal{L}$ & \textbf{Am. Computers} & \textbf{Am. Photos} & \textbf{Co. CS} & \textbf{Co. Physics}  & \textbf{PPI} (F-Score) \\
\midrule
$\mathcal{L}_u$ & 90.37 $\pm$ 0.08 & 92.09 $\pm$ 0.18 & 93.37 $\pm$ 0.08 & 95.92 $\pm$ 0.05 & 72.09 $\pm$ 0.12\\
$\mathcal{L}_v$ & 90.52 $\pm$ 0.09 & 92.86 $\pm$ 0.07 & 93.36 $\pm$ 0.05 & 95.93 $\pm$ 0.04 & 70.44 $\pm$ 0.12 \\
$\mathcal{L}_u + \mathcal{L}_v$ & 90.45 $\pm$ 0.07 &  92.88 $\pm$ 0.13 & 93.37 $\pm$ 0.03 & 95.93 $\pm$ 0.04 & 72.25 $\pm$ 0.09 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[t]
\caption{Effect of global propagation scheme. Linear evaluation accuracy (micro-average F-Score for PPI dataset) for different schemes to obtain the global views propagating the local ones averaged for 5 weight initializations and data splits.}
\label{ablation_global}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ccccccc}
\toprule
$\mathbf{S}$ & $K$ & \textbf{Am. Computers} & \textbf{Am. Photos} & \textbf{Co. CS} & \textbf{Co. Physics}  & \textbf{PPI} (F-Score) \\
\midrule

$\mathbf{D}^{-1}\mathbf{A}$ & 1 & 90.52 $\pm$ 0.09 & 92.81 $\pm$ 0.14 & 93.39 $\pm$ 0.04 & 95.89 $\pm$ 0.07 & 71.29 $\pm$ 0.11 \\
$\mathbf{D}^{-1}\mathbf{A}$ & 2 & 90.43 $\pm$ 0.16 & 92.84 $\pm$ 0.15 & 93.37 $\pm$ 0.04 & 95.89 $\pm$ 0.07 & 71.28 $\pm$ 0.08 \\
$\mathbf{D}^{-1}\mathbf{A}$ & 5 & 90.41 $\pm$ 0.17 & 92.84 $\pm$ 0.08 & 93.26 $\pm$ 0.06 & 95.86 $\pm$ 0.08 & 70.06 $\pm$ 0.08 \\
\midrule
$\mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ & 1 & 90.46 $\pm$ 0.06 & 92.89 $\pm$ 0.15 & 93.37 $\pm$ 0.02 & 95.92 $\pm$ 0.04 & 72.25 $\pm$ 0.13 \\
$\mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ & 2 & 90.25 $\pm$ 0.10  & 92.69 $\pm$ 0.14 & 93.35 $\pm$ 0.02 & 95.91 $\pm$ 0.04 &  71.94 $\pm$ 0.08\\
$\mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ & 5 & 90.08 $\pm$ 0.16 & 92.75 $\pm$ 0.11 & 93.23 $\pm$ 0.05 & 95.85 $\pm$ 0.05 & 70.08 $\pm$ 0.17 \\
\midrule
$\mathbf{I} - \mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ & 1 & 84.22 $\pm$ 0.28 & 90.23 $\pm$ 0.20 & 93.01 $\pm$ 0.12 &  95.50 $\pm$ 0.11 & 61.81 $\pm$ 0.08 \\
$\mathbf{I} - \mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ & 2 & 84.58 $\pm$ 0.22 & 90.75 $\pm$ 0.25 & 92.89 $\pm$ 0.14 & 95.46 $\pm$ 0.08  & 61.35 $\pm$ 0.13\\
$\mathbf{I} - \mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ & 5 & 88.65 $\pm$ 0.16 & 92.62 $\pm$ 0.20 & 91.82 $\pm$ 0.27 & 95.53 $\pm$ 0.09 & 64.5 $\pm$ 0.12 \\

\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[t]
\caption{Effect of dropout and edge sampling regularization. Linear evaluation accuracy (micro-average F-Score for PPI dataset) for different values of dropout and edge sampling probabilitites applied to the input graph before the GNN encoder ($p_{input}$) or to the local views before the propagation through the graph ($p_{local}$).}
\label{ablation_dropout}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ccccccc}
\toprule
$p_{input}$ & $p_{output}$ & \textbf{Am. Computers} & \textbf{Am. Photos} & \textbf{Co. CS} & \textbf{Co. Physics}  & \textbf{PPI} (F-Score) \\
\midrule
0.0 & 0.0 & 89.89 $\pm$ 0.09 & 92.66 $\pm$ 0.16 & 93.41 $\pm$ 0.03 & 95.93 $\pm$ 0.03 & 71.36 $\pm$ 0.14 \\
0.0 & 0.5 & 90.00 $\pm$ 0.16 & 92.85 $\pm$ 0.14 & 93.32 $\pm$ 0.04 & 95.88 $\pm$ 0.05 & 72.25 $\pm$ 0.09 \\
0.5 & 0.0 & 90.45 $\pm$ 0.07 & 92.90 $\pm$ 0.13 & 93.37 $\pm$ 0.03 & 95.92 $\pm$ 0.04 & 67.98 $\pm$ 0.21 \\
0.5 & 0.5 & 90.34 $\pm$ 0.08 & 92.99 $\pm$ 0.17 & 93.30 $\pm$ 0.06 & 95.88 $\pm$0.05 & 67.96 $\pm$ 0.07 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*} 
We evaluate the quality of the node level representations output by our method on both transductive and inductive settings. 

\textbf{Datasets.} For transductive learning we run RGI on 4 popular benchmarks: \textit{Amazon Computers}, \textit{Amazon Photos}, \textit{Coauthor CS} and \textit{Coauthor Physics}. Finally, inductive learning evaluation is addressed with the challenging \textit{PPI} dataset. The statistics of the datasets are shown in Table \ref{datasets}.

\textbf{Linear evaluation.} We follow the linear evaluation protocol on graphs to assess the quality of the representations as proposed in \cite{velickovic2018deep}. It consists of first fitting a GNN encoder in a fully self-supervised manner, freezing the weights of the encoder, obtaining the node-level representations and fitting a linear classifier to a downstream task without backpropagating the gradients through the encoder. For comparability with other methods, the transductive settings are evaluated in terms of accuracy of the predictions whereas for the \textit{PPI} dataset we employ the micro-average F1-score. As usual, since there is no public split for these datasets, \textit{Amazon Computers}, \textit{Amazon Photos}, \textit{Coauthor CS} and \textit{Coauthor Physics} are randomly split into train/validation/test (10\% / 10\% / 80\%). To evaluate on the \textit{PPI} dataset, we employ the standard pre-defined split, which has 20 graphs to fit the model, 2 graphs for validation and another two for testing.

\subsection{Transductive learning}

\textbf{Architecture.} As in \cite{velickovic2018deep}, \cite{thakoor2021bootstrapped}, among others, we fix the encoder $f_{\Theta}$ to be a $L=2$ layer GCN \cite{kipf2017semisupervised}. We have set the output dimensionality to be $512$ for all datasets and the hidden, $1024$. We include batch normalization \cite{10.5555/3045118.3045167} and ReLU activation after the first layer but none of them is included after the second convolutional layer. Therefore, if $\hat{\mathbf{A}} = \mathbf{A} + \mathbf{I}$, and $\hat{\mathbf{A}}_n=\hat{\mathbf{D}}^{-\frac{1}{2}} \hat{\mathbf{A}} \hat{\mathbf{D}}^{-\frac{1}{2}}$ the matrix equation of the encoder is:

\begin{equation}
\mathbf{Z} = \hat{\mathbf{A}}_n \left [ ReLU \left ( BN \left ( \hat{\mathbf{A}}_n \mathbf{X} \mathbf{W}_1 \right ) \right ) \right ] \mathbf{W}_2
\end{equation}

The node global views are obtained by propagating the node local views output by the encoder. We employ the normalized adjacency matrix as shift operator $\mathbf{S} = {\mathbf{D}}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ without self-loops and propagate for $K=1$ step. The networks $h_{\phi}$ and $h_{\psi}$ are implemented with a two layer FCNN and ReLU activation in the hidden layer. However, no batch normalization is employed for these reconstruction networks.

\textbf{Numerical results.} Table \ref{results_transductive} shows the mean accuracy of the linear evaluation protocol on the transductive graph settings. Our results are the average of 20 model weight initializations and data splits. The other results are extracted from previous reports. We can observe that RGI performs competitively in all datasets despite its simplicity and achieves state of the art in some of them. 
In except of \textit{Amazon Computers} dataset, RGI is trained for $1,000$ epochs whereas other methods such as BGRL require $10,000$ epochs \cite{thakoor2021bootstrapped}. \cite{bielak2021graph} reports the results of BGRL if training during $1,000$ epochs, which imply a drop in performance in all datasets.

\textbf{Regularizing with dropout.} Albeit not crucial, we have found it useful to add noise to the input graph to regularize the learning procedure. Concretely, we can apply dropout and edge sampling before feeding the graph neural network with the graph. However, as discussed in Section \ref{discussion}, this approach is different to invariance via data augmentation, since the only purpose of these corruptions is regularization and no invariance assumption is made. We have applied standard dropout to the node features and edge sampling to the input graph with a probability of $p_{input}=0.5$.

\subsection{Inductive learning}

\textbf{Architecture.} Based on previous reports \cite{bielak2021graph}, \cite{thakoor2021bootstrapped}, we implement the encoder with a $L=3$ layer GAT \cite{velickovic2018graph} with ELU activation and skip connections. Both hidden and output dimensionality are set to 512. Although a mean-pooling scheme would be more appropriate for inductive settings \cite{NIPS2017_5dd9db5e}, we also employ the normalized adjacency matrix as graph shift operator to obtain the global views, $\mathbf{S} = {\mathbf{D}}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$, and propagate for $K=1$ steps. In Section \ref{sec_ablation_global} we study the effect of $\mathbf{S}$ and $K$. The reconstruction networks $h_{\phi}$ and $h_{\psi}$ have the same architecture as in the transductive setting.

\textbf{Numerical results.} Table \ref{results_ppi} shows the micro-average F1-score of the linear evaluation averaged for 20 runs. RGI outperforms the current state of the art on this challenging dataset and only requires $2,000$ epochs to reach this performance.

\textbf{Sparse node features.} \textit{PPI} node features are sparse, with around 40\% of empty features, which makes neighborhood aggregation crucial for any downstream node level task. Consequently, in order to experiment with dropout regularization, we have applied the dropout and edge sampling after the GNN encoder with a probability $p_{local}=0.5$ and no dropout applied to the input $p_{input}=0$.

%\subsection{Complexity analysis}
%In this section, we provide intuitions on the time and space complexity of our algorithm compared with other methods, following the analysis of BGRL \cite{thakoor2021bootstrapped}. Let $\mathcal{G}$ to be a graph of $N$ nodes and $M$ edges, $f_{\Theta}$ a graph neural network encoder and $g_K$ a function that propagates the output of the encoder through the graph for $K$ steps. We also assume that the complexity of one propagation step through the graph is $\mathcal{O}(N+M)$. 

\subsection{Ablations}

In this section, we run an ablation analysis to evaluate the effect of the two main components of the design space of RGI: the loss function $\mathcal{L}$ and the global propagation scheme, which includes the shift operator $\mathbf{S}$ and the number of propagation steps $K$. In addition, we have experimented with the dropout regularization. Generally, results show that RGI performs robustly for different design configurations, specifically in transductive settings. 
 
\subsubsection{Loss function}
The  loss function $\mathcal{L}$ is symmetrized by applying covariance regularization to both local and global views and computing the reconstruction error on both sides. In this section, we empirically evaluate the effect of this symmetrization by training RGI with three different objectives: $\mathcal{L}_u$, $\mathcal{L}_v$ and $\mathcal{L}_u + \mathcal{L}_v$. Section \ref{ablation_lossfn} shows a performance comparison on all transductive and inductive datasets we have experimented with. Note that the metrics do not match the reported ones since in the ablations we are only averaging for 5 weights initializations and data splits. Results suggest that, in full graph transductive settings, there is no need to symmetrize the loss function and targetting either $\mathcal{L}_u$ or $\mathcal{L}_v$ is enough since the performance is statistically similar. Indeed,  targetting $\mathcal{L}_v$ also influences the variance and covariance of the local views. Nonetheless, in PPI dataset, we can observe that $\mathcal{L}_v$ objective alone does not lead to state-of-the-art performance since the variance and covariance of the local views is not influenced as in the transductive settings due to the batched, inductive nature of the problem. In general, although it may be tuned given the downstream task, $\mathcal{L}_u + \mathcal{L}_v$ performs competitively in all settings.

\subsubsection{Global propagation scheme}
\label{sec_ablation_global}
The global views are obtained by propagating the output of the encoder during $K$ steps with a graph shift operator $\mathbf{S}$, $\mathbf{V}=\mathbf{S}^K \mathbf{U}$. In this section, we evaluate different shifts operators, namely the mean-propagation $\mathbf{S}=\mathbf{D}^{-1}\mathbf{A}$, the normalized adjacency matrix $\mathbf{S}=\mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ and the normalized Laplacian $\mathbf{S}=\mathbf{I} - \mathbf{D}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ as well as different values of $K$, 1, 2 and 5. Note that none of them include the self-loops as opposed to most GNN propagation schemes. Section \ref{ablation_global} shows that RGI performs robustly in all settings as long as the global propagation scheme captures the low-frequency components of the data, obtained with the adjacency matrix (either mean pooling or normalized), which is motivated by the homophily of the downstream task on the graph. 

\subsubsection{Dropout regularization}
RGI can optionally include dropout and edge sampling regularization that can be applied to the input graph or to the graph before computing the global views. Section \ref{ablation_dropout} shows the performance with four different configurations, where the columns \textit{input} and \textit{local} are the probabilities of dropout and edge sampling to the input graph and to the graph with local views, respectively. As it is argued in Section \ref{discussion}, these corruptions act as regularization rather than data augmentation to seek invariance. The influence of the regularization on the transductive settings is minimal, whereas it boosts the performance in the batched inductive setting.
