\section{Introduction}
%Self-supervised learning is employed to learn useful representations of the data without relying on annotations with the aim of leveraging them for downstream tasks. Ideally, the obtained representations should be enough to efficiently fit other tasks with an small amount of labeled data and more basic models. Therefore, the performance of deep self-supervised methods is usually evaluated by training an encoder, freezing the weights, taking the output representations and fitting a linear model for the downstream task.

The main goal of self-supervised learning is to learn useful representations of high-dimensional data without relying on annotations in order to leverage them in downstream tasks. Ideally, the obtained representations should be enough to efficiently fit other tasks with a small amount of labeled data.

In the vision domain, methods mainly focus on a maximization of the agreement between the representations of two views of an image. For instance, Deep InfoMax (DIM) \cite{hjelm2018learning} was initially proposed to maximize the mutual information (MI) between local and global views. However, methods based on data augmentation techniques have shown better performance. They are based on a two branch architecture, either symmetric or not, whose branches are fed with different augmented versions of an image obtained with random transformations such as rotation and cropping \cite{bardes2022vicreg}, \cite{chen2020big}, \cite{chen2020simple}, \cite{grill2020bootstrap}, \cite{zbontar2021barlow}. The encoder is trained to be invariant to those image distortions. 

These schemes are also adopted for graph representation learning and successful algorithms have been extended to this domain \cite{velickovic2018deep}, \cite{sun2019infograph}, \cite{thakoor2021bootstrapped}, \cite{zhang2021canonical}, \cite{Zhu:2020vf}, \cite{icml2020_1971}. Consequently, graph specific data augmentation techniques are required to generate the graph views. Graph transformations can focus on both node attributes and the topology of the graph. Indeed, popular choices are node attribute masking and edge sampling \cite{thakoor2021bootstrapped}, \cite{zhang2021canonical}, \cite{bielak2021graph}, \cite{Zhu:2020vf}. However, the idea behind invariance via data augmentations roots in the fact that it is assumed that these transformations do not change the semantics of the data and the information contained about the downstream task is kept. Whereas we can understand image augmentations, it is not clear how graph transformations modify its semantics nor if they can be applied to all graph domains \cite{NEURIPS2021_ff1e68e7}. To overcome this issue, graph diffusion has been proposed to create the alternative views \cite{icml2020_1971}, and other works attempt to get rid of the augmentations by designing strategies that leverage the local neighborhood of the nodes  \cite{peng2020graph}, \cite{lee2021augmentation}, \cite{https://doi.org/10.48550/arxiv.2204.04874}, also seen in concurrent work \cite{anonymous2023localized}.

Additionally, the main challenge in multi-view scenarios is to avoid the collapse of the models, in which the encoder outputs constant representations for all inputs. Solutions can be divided into contrastive and non-contrastive. The former constructs positive and negative pairs and maximizes the similarity between components of the positive pairs while minimizing it for the elements of the negative pairs \cite{chen2020simple}, \cite{velickovic2018deep}, \cite{hjelm2018learning}, \cite{Zhu:2020vf}, \cite{icml2020_1971}, \cite{anonymous2023localized}. However, it has been shown that they require a large number of negative samples to efficiently work, which can be a bottleneck for scalability. Within non-contrastive methods, we can find, among others, knowledge distillation, which avoids the collapse by taking a teacher-student asymmetric architecture \cite{grill2020bootstrap}, \cite{thakoor2021bootstrapped}, and regularized methods, which spread out embedded data points by regularizing the covariance matrix of the representations \cite{bardes2022vicreg}, \cite{zbontar2021barlow}, \cite{zhang2021canonical}.

To cope with the aforementioned challenges, we introduce \textit{RGI - Regularized Graph Infomax}, a simple yet effective self-supervised learning framework for graphs. The algorithm is augmentation-free, non-contrastive, and does not require a two-branch architecture nor complex training strategies. RGI trains an encoder maximizing the mutual information between nodes' representations output by the model and their propagation through the graph, namely local and global views, respectively. The collapse is avoided with variance-covariance regularization loss terms that attempt to maximize the entropy of the representation space.

This paper is organized as follows, we first provide a background on mutual information (MI), the Infomax principle and the multi-view representation learning approach for images and graphs. Then, we formalize our algorithm RGI for self-supervised learning, provide intuitions behind the method and detail the objective function proposed. Then, we evaluate the method on multiple graph benchmark datasets and discuss the advantages of RGI with respect to other algorithms. The main contributions are listed below:

\textbf{1.} We derive a loss function that attempts to maximize the MI between two views of a node by relying on the reconstruction-based bound of the MI and regularizing the entropy of the representation space via the covariance matrix.

\textbf{2.} We define a node-level global view, particular to every node, as opposed to the graph-level view proposed in other methods \cite{velickovic2018deep}, \cite{sun2019infograph}. This is obtained by propagating the node representations through the graph and allows us to address one-to-one reconstruction.

\textbf{3.} We evaluate the algorithm on both transductive and inductive settings and show that, despite its simplicity, state-of-the-art performance is obtained in some of the benchmarks while being competitive in all of them. We also provide ablation studies on how the different designs of the method influence the performance, that demonstrate the robustness of RGI despite modifying core concepts of the algorithm.