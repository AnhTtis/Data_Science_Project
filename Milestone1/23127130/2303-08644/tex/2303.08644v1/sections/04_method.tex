\section{Regularized Graph InfoMax}
In this section, we detail our algorithm \textit{RGI - Regularized Graph Infomax} for self-supervised learning on graphs.

\begin{algorithm}[H]
\caption{RGI}
\label{grim_algo}
\begin{algorithmic}[1]
   \State {\bfseries Input:} node features $\mathbf{X}$; adjacency matrix $\mathbf{A}$; parameters $\Theta$, $\phi$ and $\psi$; backbone $f_{\Theta}$; reconstruction networks $h_{\phi}$ and $h_{\psi}$; propagation steps $K$.
   \Repeat
   \State $\mathbf{U} = f_{\Theta}({\mathbf{X}}, {\mathbf{A}})$ \Comment{obtain local views}
   \State $\mathbf{S} = {\mathbf{D}}^{-\frac{1}{2}} {\mathbf{A}} {\mathbf{D}}^{-\frac{1}{2}}$ \Comment{get symmetric adjacency matrix}
   \State $\mathbf{V} = \mathbf{S}^{K} {\mathbf{U}}$ \Comment{propagate during K steps}
   \State $\mathbf{V}'=h_{\phi}(\mathbf{U})$ \Comment{reconstruction of $V$ from $U$}
   \State $\mathbf{U}'=h_{\psi}(\mathbf{V})$ \Comment{reconstruction of $U$ from $V$}
   \State $\mathcal{C}= \frac{1}{N} \bar{\mathbf{U}}^T \bar{\mathbf{U}} $ \Comment{covariance matrix of $U$}
   \State $\Sigma     = \frac{1}{N} \bar{\mathbf{V}}^T \bar{\mathbf{V}} $ \Comment{covariance matrix of $V$}
   \State $\mathcal{L}_1 =  \parallel \mathbf{U} - \mathbf{U}' \parallel^2_F +  \parallel \mathbf{V} - \mathbf{V}' \parallel^2_F $ \Comment{reconstruction loss}
   \State $\mathcal{L}_2 = (1-$diag$(\mathcal{C}))^2 + (1-$diag$(\Sigma))^2$ \Comment{variance loss}
   \State $\mathcal{L}_3 = ($off-diag$(\mathcal{C}))^2 + ($off-diag$(\Sigma))^2$ \Comment{covariance loss}
   \State $\mathcal{L} = \lambda_1 \mathcal{L}_1 + \lambda_2 \mathcal{L}_2 + \lambda_3 \mathcal{L}_3 $ \Comment{loss function}
   \State $\Theta, \phi, \psi \leftarrow \bigtriangledown_{\Theta, \phi, \psi} \mathcal{L}$ \Comment{update parameters}
   \Until{convergence}
   \State {\bfseries return} $\mathbf{U}$
\end{algorithmic}
\end{algorithm}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/grim.png}}
\small

\caption{Visual ilustration of RGI. Given the input graph $\mathcal{G}=(\mathbf{X},\mathbf{A})$, a GNN encoder $f_{\Theta}$ extracts node features $\mathbf{U} = f_{\Theta}(\mathbf{X},\mathbf{A})$ and they are propagated through the graph during $K$ steps to obtain the nodes' global views $\mathbf{V}=\mathbf{S}^K \mathbf{U}$. RGI maximizes the MI between $\mathbf{U}$ and $\mathbf{V}$ based on the lower bound of the MI of Equation \ref{mi_bound}. To do so, two auxiliary neural networks $h_{\phi}$ and $h_{\psi}$ are trained to predict $V$ from $U$ and $U$ from $V$, respectively. Additionally, the covariance matrices of the local and global views are regularized to have large diagonal elements and off-diagonal elements close to zero, attempting to maximize the entropy of $\mathbf{U}$ and $\mathbf{U}$. The neural networks $h_{\phi}$ and $h_{\psi}$ and the global views $\mathbf{V}$ are ignored during inference.}
\end{center}
\label{grim_fig}
\vskip -0.2in
\end{figure*}

\subsection{Context and notation}
Let $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ be a graph of $N$ nodes where $\mathcal{V}$ is the node set and $\mathcal{E}$ the edge set. Node attributes $X$ come from $d$-dimensional empirical distribution, $X \sim \mathbb{P}_X$ and the nodes' realizations are arranged in a matrix $\mathbf{X} = \{ \mathbf{x}_i \}_{i=1}^{N} \in \mathbb{R}^{N \times d}$, where $\mathbf{x}_i \in \mathbb{R}^{d}$ are the attributes of node $i$. $\mathcal{E}$ comes in the form of (unweighted) adjacency matrix $\mathbf{A} \in \{0,1\}^{N \times N}$, $\mathbf{A}_{ij} = 1$ if the edge $e_{ij} \in \mathcal{E}$, $0$ otherwise. The $K$-hop neighborhood of a node $i$, $\mathcal{G}_i^{(K)}$, is represented by the set of neighbors' attributes, $\mathbf{X}_i^{(K)}$ and the induced adjacency matrix $\mathbf{A}_i^{(K)}$. The same notation criterion of $X$, $\mathbf{X}$, $\mathbf{x}_i$ and $\mathbf{X}_i$ will be employed for other node hidden representations.

The goal of graph self-supervised learning is to fit a graph neural network encoder $f_{\Theta} : \mathbb{R}^{N \times d} \times \{0,1\}^{N \times N} \xrightarrow{} \mathbb{R}^{N \times D}$, $\mathbf{U} = f_{\Theta}(\mathcal{G}) = f_{\Theta}(\mathbf{X}, \mathbf{A})$, parametrized by $\Theta$, that obtains a $D$-dimensional vector representation $\mathbf{u}_i$ for every node of the input graph $\mathcal{G}$ without relying on node annotations.

\subsection{Method}

RGI trains an encoder to maximize the MI between local node views $U$ and global ones $V$, arranged in matrices $\mathbf{U}$ and $\mathbf{V}$, respectively. Local node views $\mathbf{U}$ are the output of the $L$-layer graph neural network $f_{\Theta}$ to be trained, $\mathbf{U} = f_{\Theta}(\mathbf{X}, \mathbf{A})$. Global node views $\mathbf{V}$ are obtained by propagating these representations through the graph during $K$ steps, $\mathbf{V} = g_K(\mathbf{U}, \mathbf{S}) = \mathbf{S}^{K}\mathbf{U}$, where $\mathbf{S}$ is a graph shift operator such as the (normalized) adjacency matrix. The encoder is trained to output the representations $\mathbf{U}$ that easily predict the representations after propagation and symmetrically predicting the output of the encoder from their propagation, while also including regularization on the covariance matrices of the local and global views, $\mathcal{C}$ and $\Sigma$, respectively. The intuition is that given a node's local view, one should be able to predict its global view, which better approximates the global position of the node on the graph.  The local representations $\mathbf{U}$ are then employed for downstream tasks. Two additional fully-connected neural networks $h_{\phi}$, $h_{\psi}$ are included during training for the reconstruction between views, but they are ignored for inference. The algorithm is described in Algorithm \ref{grim_algo}.

\subsubsection{Local-global perspective}
The node views $\mathbf{V}$ are referenced as global views. This definition is different from the global pooling proposed in DGI \cite{velickovic2018deep}, since the views are node level rather than graph level. This approach has the advantage of enabling addressing one-to-one reconstruction errors since every node has its own view.

The reason to call $\mathbf{V}$ the global views is as follows. For a target node $i$, we obtain its local view with a $L$-layer GNN, $\mathbf{u}_i=f_{\Theta}(\mathbf{X}_i^{(L)}, \mathbf{A}_i^{(L)})$, so that it depends on its $L$-hop neighborhood. Afterwards, the global view is computed by propagating the representations through the graph during $K$ steps, $\mathbf{v}_i=g_K(\mathbf{U}_i^{(K)}, \mathbf{A}_i^{(K)})$. Therefore, $\mathbf{v}_i$ contains information of the $(L+K)$-hop neighborhood of $i$. It is known that for a small-world network $\mathcal{G}$ of $N$ nodes, the diameter of the graph is $diam(\mathcal{G})=log(N)$. Consequently, in a small world network, $\mathbf{v}_i$ can encode global information of every node in the graph as long as $L + K \simeq log(N)$.

\subsubsection{Loss function}
The objective promotes the predictability between views, such that one view contains as much information as possible about the other. Additionally, we incorporate covariance matrix regularization terms to avoid the collapse of the representations \cite{zbontar2021barlow}, \cite{bardes2022vicreg}, \cite{bielak2021graph}, \cite{zhang2021canonical}. Being $\bar{\mathbf{U}}$ the mean-centered version of $\mathbf{U}$ and $\mathcal{C}=\frac{1}{N}\bar{\mathbf{U}}^T \bar{\mathbf{U}}$ the covariance matrix, we define the loss:

\begin{equation}
\label{loss_u}
\begin{aligned}
    \mathcal{L}_u = {} & \frac{\lambda_1}{N} \sum_{i=1}^{N}  \parallel \mathbf{u}_i - h_{\phi}(\mathbf{v}_i) \parallel_2^2  + \frac{\lambda_2}{D} \sum_{l=1}^D \left ( 1 - \mathcal{C}_{ll} \right )^2 + \frac{\lambda_3}{D} \sum_{l=1}^D \sum_{k \neq l} \mathcal{C}_{lk}^2
\end{aligned}
\end{equation}

Where $\lambda_1, \lambda_2, \lambda_3 \in \mathbb{R}$ are weight hyperparameters. The first term of the equation is the expected prediction error of the node local views given the globals with a fully-connected neural network $h_{\phi}$, whose parameters are also updated during training. The following two terms are regularization terms on the covariance matrix of the representations. Concretely, the former guides the diagonal elements (variances) to be close to one whereas the latter forces the non-diagonal elements (covariances) to be zero.
Intuitively, maximizing the variance avoids the total collapse to a constant representation. On the other hand, covariance minimization encourages the encoder to leverage the whole capacity of the representation space rather than projecting the points to a lower dimensional subspace, also known as dimensional collapse \cite{hua2021feature}. Symmetrically, being $\bar{\mathbf{V}}$ the mean-centered version of $\mathbf{V}$ and $\Sigma=\frac{1}{N}\bar{\mathbf{V}}^T \bar{\mathbf{V}}$ the covariance matrix of the global views, we define:

\begin{equation}
\label{loss_v}
\begin{aligned}
    \mathcal{L}_v ={} & \frac{\lambda_1}{N} \sum_{i=1}^{N}  \parallel \mathbf{v}_i - h_{\psi}(\mathbf{u}_i) \parallel_2^2 + \frac{\lambda_2}{D} \sum_{l=1}^D \left ( 1 - \Sigma_{ll} \right )^2 + \frac{\lambda_3}{D} \sum_{l=1}^D \sum_{k \neq l} \Sigma_{lk}^2
\end{aligned}
\end{equation}

The final loss of the method is the sum of $\mathcal{L}_u$ and $\mathcal{L}_v$:
$\mathcal{L} = \mathcal{L}_u + \mathcal{L}_v$.