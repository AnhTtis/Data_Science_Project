\section{Background} \label{chap:background}
GPUs were originally designed for fast processing of graphics, but they have also been found to be effective accelerators for 
%***  Graphics workkloads are also parallel....  parallelizable workloads. They have been used as 
parallelizable AI and computational science workloads. 
%They have been used as General Purpose GPUs (GPGPUs) 
After their programming environments facilitated using them as General Purpose GPUs (GPGPUs)
to speed up a variety of scientific computations, they are now a key component in many of the world's largest computing clusters. The %optimization of GPU architectures and workloads is
optimization workload running on GPU architectures is thus 
important for a range of fields beyond video game graphics.

\subsection{Performance Portability}
Relative performance\footnote{performance of a code compared to peak performance of hardware} can vary greatly across different hardware platforms, even if a program functionally behaves the same on all of them. A problem configuration that runs well on one GPU may perform poorly on another GPU. When writing high-performance software, it is essential to utilize the hardware as efficiently as possible. One solution to this is to use libraries like ATLAS~\cite{clint_whaley_automated_2001}, which automatically tunes its configuration based on the executing hardware. This can avoid the manual effort of finding the optimal configuration for every hardware platform. FFTW~\cite{frigo_fftw_1998} also used a similar self-tuning approach to optimize its fast Fourier transforms. 
%**** reference Thomas Falch´s work here?
%Machine learning‐based autotuning has similarly been used to address this problem for image processing kernels \cite{falch-ml-autotuning}.
%article{https://doi.org/10.1002/cpe.4029,
%author = {Falch, Thomas L. and Elster, Anne C.},

We can measure how portable these configurations are by finding the optimal configurations for our target platforms and then examine the relative performance of these optimal configurations on other systems. We can thus analyze how sensitive the configurations are to platform changes and how large the relative performance differences are between different architectures.




\subsection{Analysing characteristics of benchmarks}
\subsubsection{Feature importance}
Feature importance in ML is a technique to identify influential features of a dataset on a model's outcome~\cite{zien_feature_2009}. Different methods compute feature importance, e.g. permutation importance, feature importances from tree-based models, LASSO, etc. These methods quantify feature importance to understand a dataset's characteristics, identify redundant/irrelevant features, and guide feature selection. The feature importance scores express the dataset's characteristics and the contribution of each feature to the model's performance.

In this study, we use Permutation Feature Importance (PFI) to evaluate feature importance. PFI measures a model's performance decrease when a feature's values are shuffled to understand the feature's importance. We calculate PFI by training a Catboost~\cite{prokhorenkova_catboost_2018} Regression model on the original dataset, shuffling each feature's values, retraining the model, and comparing the original and shuffled dataset's performance metric. The PFI score for each feature is the difference between the two. PFI helps identify important features for a model's performance and can detect multicollinearity among features to prevent overfitting and unstable models.

%Feature importance in machine learning is a technique used to determine which features of a dataset have the most influence on the outcome of a model~\cite{zien_feature_2009}. Different methods can be used to compute feature importance, such as permutation importance, feature importances from tree-based models, and other techniques like LASSO. % or Ridge. 
%These techniques can provide a quantitative measure of the importance of each feature, which can be used to understand the characteristics of a benchmark dataset, identify redundant or irrelevant features, and guide feature selection. The resulting feature importance scores can be used to express the characteristics of the benchmark in terms of which features are most informative or predictive of the target variable, and how much each feature contributes to the overall performance of the model. 
%Additionally, feature importance scores can also be used for model interpretability and to help understand how the model is making its predictions.

%In this study, we use Permutation Feature Importance (PFI) to evaluate the relative importance of individual features in our dataset. PFI is a method that measures the decrease in a model's performance when a feature's values are randomly shuffled, allowing us to understand the feature importance by estimating how much the model's performance would drop if the feature was not available. To calculate PFI, we trained a Catboost Regression model on the original dataset and made predictions using that model. We then shuffled the values of each feature in the dataset, retrained the model, and made predictions using the shuffled dataset. The difference between the performance metric on the original dataset and the performance metric on the shuffled dataset was then used as the PFI score for that feature. This process was repeated for all features in the dataset, resulting in a PFI score for each feature. This approach to feature importance analysis can be useful for identifying the features that are most important for a model's performance, which can be used for feature selection and for understanding which features are driving the model's predictions. Additionally, PFI is sensitive to the presence of multicollinearity among features, which can lead to overfitting and unstable models. The results of PFI can be used to detect such unwanted correlations among features and guide feature selection and/or regularization methods to mitigate its effects.


\subsubsection{Proportion of Centrality metric}
The proportion of centrality metric introduced by Schoonhoven et al.~\cite{schoonhoven2022benchmarking} is a way to quantify the difficulty of GPU tuning. It is based on the concept of the fitness flow graph (FFG), which contains all points in the search space and creates a directed edge to a neighboring point if the neighbor has lower fitness. This means that a random walk across the FFG mimics the behavior of a randomized first-improvement local search algorithm. The expected proportion of arrivals of each minimum then gives a metric for weighting reachability of each minimum. The likelihood of arrival per local minima is computed using the PageRank node centrality, which was originally used to determine the relevance of a webpage. The PageRank values are the values of the dominant right eigenvector of the adjacency matrix of a directed graph G, rescaled such that each column adds up to 1. The metric is a measure of difficulty, it considers how likely a certain subset of "suitably good" local minima are to be visited by a local search algorithm relative to the rest. This subset is defined by the optimal fitness and the proportion p, taking the set of nodes consisting of local minima with fitness less than $(1 + p)f_{opt}$ for minimization problems, otherwise $(1 - p)f_{opt}$.



%GPUs were originally designed for fast processing of graphics. However, in recent decades they have also been found to be excellent accelerators for sufficiently parallelizable workloads. GPUs have thus been used as General Purpose GPUs (GPGPU) to accelerate a wide range of scientific computations. 
%Many of the largest computing clusters in the world today use GPUs as an essential computational resource.
%The design and optimization of GPU workloads and architectures have therefore become important for many other subjects than video game graphics.

%\subsection{Performance Portability}
%Due to the wide range of hardware available for computers, a program will often \emph{functionally} behave the same for all compatible platforms, however the \emph{performance} can vary greatly. A configuration of a problem that runs well on a CPU might perform terribly for a GPU. When writing high-performance software, it is important that we utilize the hardware as efficiently as possible, which is why ATLAS~\cite{clint_whaley_automated_2001} was created. ATLAS is an Automatically Tuned Linear Algebra library, where the executing hardware determines the libraries' configuration. It does this by testing out various configurations to determine what configuration is optimal for this specific hardware. One way to do this is by analysing the size of the cache levels and using this data to determine optimal compute sizes. Using these automatic techniques, we can avoid the manual labor of finding the optimal configuration for every single hardware platform. FFTW~\cite{frigo_fftw_1998} used a similar concept of self-tuning to optimize its fast fourier transforms.

%\subsection{Performance Portability}
%Due to the wide range of hardware available for computers, a program will often \emph{functionally} behave the same for all compatible platforms. However, the \emph{performance} can vary greatly. A configuration of a problem that runs well on one GPU might perform terribly on another GPU of a different architecture. 
%ATLAS~\cite{clint_whaley_automated_2001} is an Automatically Tuned Linear Algebra library, that is an early well-known effort that addresses this by testing out various configurations of matrix block sizes to decide which configuration is optimal for a given system's memory hierarchy. Similar concepts are %now
%also part of Intel's matrix library MKL and the FFT library FFTW~\cite{frigo_fftw_1998}. 

%\subsection{OpenCL}
%With a wide range of architectures and different computing models it gets increasingly difficult for developers to produce programs that run well on any architecture. OpenCL~\cite{stone_opencl_2010} was therefore created as an Open standard for computation on any architecture. The goal is thus to create a programming model where the code can be written once and executed anywhere, from CPUs to GPUs and FPGAs. The programming model is quite similar to CUDA and has therefore gotten significant support from both AMD and Nvidia as an open alternative to CUDA. OpenCL is still under development and currently contains many parameters such as \emph{work group size}(similar to the size of the threadblock in CUDA), that needs to be tuned for each architecture to provide optimal performance. So OpenCL promises \emph{functional portability}, but gives no guarantees of \emph{performance portability}. 