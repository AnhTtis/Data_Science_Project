\section{Related work}\label{chap:related-work}
%Specifically in the field of autotuning the most relevant work is the benchmark suite by Petrovic et al.\cite{petrovic_benchmark_2019}, Polybench\cite{grauengray_2012}. Sund et al.\cite{sund_BAT_2021} and more.
%The Polybench authors specifically made a GPU-version, in this paper\cite{grauer-gray_2012}.
%However, the search spaces for 15 benchmarks from 116 to 725 different possible configurations. Our thresholds is that the benchmarks should have at least 1000 different configurations, ideally more than 10 000. For configuration spaces less than this it's not necessarily interesting to perform autotuning studies, as many real-life applications have much larger search spaces.

%The main issue with PolyBench being the small search space of the benchmarks. For this new version of BAT we have also reduced the number of benchmarks to those that have larger and more interesting benchmarks, as well as adding several new ones that fit our criteria.
%More generally the Collective Knowledge framework (CK) by Fursin et al.~\cite{fursin_collective_2021}, is a much more generalized framework for benchmarking and reproducibility.

%In HPO there are also benchmark suites~\cite{wang_fedhpo-b_2022}
%High-dimensional HPO benchmark suite, LassoBench~\cite{sehic_lassobench_2022}
%HPOBench~\cite{eggensperger_hpobench_2022}

%\subsection{Measuring search difficulty}
%Alpcan et al.~\cite{alpcan_can_2014} give a theoretical foundation for how one could measure the difficulty of a search problem, by encoding the minimal algorithm necessary to solve the problem in a bit-string for a universal turing machine. However, the authors give no practical implementation of such an idea, and it can therefore currently not be used in practice. 

%Some authors creating an HPO dataset for machine translation, where they emphasize how the dataset can be used the same way as the Simulated runner concept in KernelTuner and in this paper.

In the field of autotuning, the most relevant prior work includes the benchmark suite developed by Petrovi\v{c} et al.~\cite{petrovic_benchmark_2019}, Polybench-GPU~\cite{grauer-gray_auto-tuning_2012}, and Sund et al.~\cite{sund_bat_2021}. %Specifically, the authors of Polybench developed a GPU version of the suite\cite{grauer-gray_auto-tuning_2012}. 
However, these benchmark suites have issues that limit their usefulness. The benchmark in PolyBench-GPU has small search spaces, ranging from 116 to 725 different possible configurations, which would fall below our threshold for an interesting autotuning study as many real-world applications have much larger search spaces. The benchmark suite from Petrovi\v{c} et al. only supports a single autotuning framework, while the optimization parameters in the benchmark suite by Sund et al. have limited performance impact. To address this issue, we have selected benchmarks with larger and more interesting search spaces for this new version of BAT, and also added several new benchmarks that meet our criteria. These benchmarks all give significant speedups, with the performance of the optimal configurations significantly varying between different systems.

For other related works, the Collective Knowledge framework (CK) developed by Fursin et al.~\cite{fursin_collective_2021} offers a more generalized approach to benchmarking and reproducibility. In the area of Hyperparameter Optimization (HPO) several benchmark suites have been developed, such as the high-dimensional HPO benchmark suite~\cite{wang_fedhpo-b_2022}, LassoBench~\cite{sehic_lassobench_2022}, and HPOBench~\cite{eggensperger_hpobench_2022}. %Fila: define HPO?

%\subsection{Measuring search difficulty}

%Alpcan et al.~\cite{alpcan_can_2014} propose a theoretical foundation for measuring the difficulty of a search problem by encoding the minimal algorithm necessary to solve the problem in a bit-string for a universal Turing machine. However, the authors do not provide a practical implementation of this idea, and it is therefore currently not usable in practice. Additionally, some authors have created an HPO dataset for machine translation, where they highlight the potential for its use in the same way as the Simulated Runner concept in KernelTuner and the current study.

%\input{content/related-work/related-fields}
