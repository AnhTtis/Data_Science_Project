\section{Results and Discussion}\label{chap:results}

\subsection{Distribution of configurations}\label{subsec:distribution}

\begin{figure*}[ht]
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/histplot/gemm_histplot.png}
    \caption{GEMM}
    \label{fig:my_label}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/histplot/hotspot_histplot.png}
    \caption{Hotspot}
    \label{fig:hotspot-distribution}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/histplot/dedisp_histplot.png}
    \caption{Dedisp}
    \label{fig:my_label}
\end{subfigure}
\\
\begin{subfigure}{.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/histplot/convolution_histplot.png}
    \caption{Convolution}
    \label{fig:my_label}
\end{subfigure}
\begin{subfigure}{.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/histplot/pnpoly_histplot.png}
    \caption{Pnpoly}
    \label{fig:my_label}
\end{subfigure}
\begin{subfigure}{.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/histplot/nbody_histplot.png}
    \caption{Nbody}
    \label{fig:nbody-distribution}
\end{subfigure}
\begin{subfigure}{.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/histplot/expdist_histplot.png}
    \caption{Expdist}
    \label{fig:my_label}
\end{subfigure}
\caption{Performance distribution of configuration for all benchmarks on all architectures}
\label{fig:distribution}
\end{figure*}


Fig~\ref{fig:distribution} shows the distribution of configurations centered around the median performing configuration. The plot extends from the worst to the best configuration. The first thing to observe is that the distribution shapes are significantly different between the different benchmarks, but similar in shape across GPUs (this agrees with results observed in other benchmark sets~\cite{olha_exploiting_2020}). Most of the benchmarks have a high density of configurations around the median and then exponential decay toward the best-performing configurations. The Hotspot benchmark in Fig.~\ref{fig:hotspot-distribution} is an outlier among the benchmarks, with a high density around the median configurations, but a cluster of very highly performing configurations giving more than 10x speedup. The Nbody distribution in Fig.~\ref{fig:nbody-distribution} also shows a distinct high-density cluster of configurations that perform very poorly.



\subsection{Convergence towards optimum using Random Search}
Fig.~\ref{fig:convergence} shows the convergence to the optimum configuration with relative performance (y-axis) plotted against the number of function evaluations on a Symmetric Log scale (linear scale from 0 to 1). Results are from random sampling 100 times from exhaustive or partial runs, with the median of the best evaluation plotted.

%Fig.\ref{fig:convergence} shows how the convergence towards the optimum configuration, with the relative performance on the y-axis and the number of function evaluations on the x-axis. The x-axis is Symmetric Log scale with a linear scale from 0 to 1, with the final plot starting from 1.
%The results are generated by randomly sampling into the dataset of exhaustive or partial runs 100 times, and then plotting the median of the best evaluation thus far in the search. This value is then normalized with the best configuration in the entire dataset. 

We can observe that there is a significant variance in the convergence between benchmarks, while there is less difference between GPUs. %Fila: I agree with "between benchmarks", but I don't see significant variance between GPUs. Anyway, results are already similar to olha_exploiting_2020
Again the Hotspot benchmark in Fig.~\ref{fig:hotspot-convergence} is a clear outlier, with Random Search quickly approaching a performance that is close to optimal. We stipulate that this is the due to the size of the high performing cluster shown in Section~\ref{subsec:distribution}. This cluster is likely of a sufficient size such that random search can quickly find a solution in this cluster, which is then close to optimal.

There are also significant differences in how quickly the other benchmarks converge towards the optimal. Expdist in Fig.~\ref{fig:expdist-convergence} and Nbody in Fig.~\ref{fig:nbody-convergence} achieve a 90\% optimum performance after just 10 function evaluations. For Dedisp (Fig.~\ref{fig:dedisp-convergence}) and PnPoly (Fig.~\ref{fig:pnpoly-convergence}) it takes around 100 evaluations to reach the same level. We can also see here how for the RTX Titan the PnPoly benchmark shows how a single highly performing configuration can be the source of the final jump in relative performance.

Lastly Convolution (Fig.~\ref{fig:convolution-convergence}) and GEMM (Fig.~\ref{fig:gemm-convergence}) require hundreds of configurations to exceed 90\%.


\begin{figure*}[ht]
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{content/results/figures/convergence/convergence_gemm.png}  
  \caption{GEMM}
  \label{fig:gemm-convergence}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{content/results/figures/convergence/convergence_hotspot.png}
  \caption{Hotspot}
  \label{fig:hotspot-convergence}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{content/results/figures/convergence/convergence_dedisp.png}
  \caption{Dedisp}
  \label{fig:dedisp-convergence}
\end{subfigure}
\\
\begin{subfigure}{.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{content/results/figures/convergence/convergence_convolution.png}
  \caption{Convolution}
  \label{fig:convolution-convergence}
\end{subfigure}
\begin{subfigure}{.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{content/results/figures/convergence/convergence_pnpoly.png}
  \caption{PnPoly}
  \label{fig:pnpoly-convergence}
\end{subfigure}
\begin{subfigure}{.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{content/results/figures/convergence/convergence_nbody.png}  
  \caption{Nbody}
  \label{fig:nbody-convergence}
\end{subfigure}
\begin{subfigure}{.24\textwidth}
  \centering
  \includegraphics[width=\linewidth]{content/results/figures/convergence/convergence_expdist.png}  
  \caption{Expdist}
  \label{fig:expdist-convergence}
\end{subfigure}
\caption{Convergence towards optimum for all benchmarks on all architectures}
\label{fig:convergence}
\end{figure*}

\subsection{Proportion of centrality}
We are using the proportion of centrality metric proposed by Schoonhoven et al.~\cite{schoonhoven2022benchmarking} to calculate the search difficulty of the benchmarks.% We use the code provided by Schoonhoven\footnote{https://github.com/schoonhovenrichard/GPU\_benchmarking\_paper} and give our KernelTuner cache files as input.  
%Since the calculations are done by generating a graph over the entire search space, 
We did not have sufficient resources to calculate the metric for the benchmarks with the largest search spaces, incl. Hotspot, Dedisp and Expdist. The results are shown in Figure~\ref{fig:centrality}. The results indicate that local search algorithms will generally find better performing configurations on the Convolution benchmark compared with GEMM and Pnpoly, which are comparatively more difficult benchmarks under this metric. This is in contrast to the results from our Random Search results, where all three benchmarks have similar trajectories towards the optimum. 


%Fila: todo: this needs more explanation, otherwise it is hard to read the fig:facet_grid (meaning: I don't understand the graphs :-) )

\begin{figure*}[ht]
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/proportionality/prop_gemm.png}
    \caption{GEMM}
    \label{fig:my_label}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/proportionality/prop_conv.png}
    \caption{Convolution}
    \label{fig:my_label}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/proportionality/prop_pnpoly.png}
    \caption{Pnpoly}
    \label{fig:my_label}
\end{subfigure}
\caption{Proportion of centrality for all benchmarks on all architectures}
\label{fig:centrality}
\end{figure*}


\iffalse
\subsection{Notes}
I should also include performance data for how the benchmarks compare with theoretical peek for the hardware.


The convergence plot is a plot showing the current optimal relative performance compared with the theoretical maximum on the y-axis with the budget on the x-axis. 
The plot is equivalent to the empirical Cumulative Distribution Function (eCDF) of the distance between the current optimal configuration and the global optimum, as shown in Eq.~\ref{eq:eCDF-def}. 
\begin{equation}\label{eq:eCDF-def}
    \sum_{n\in[1, N]} f(opt) - \max\limits_{i\in[1, n]}f(c_i)
\end{equation}

The budget would be the number of function evaluations in this case, since we are not comparing autotuners in this case.
The reported data would be based on Random Search and include the Area Under the Curve for the entire graph, and reporting for different thresholds on the y-axis and x-axis (for different absolute values of the search space, and different percentages of the total search space on the X-axis, as well as different percentage thresholds for the performance on the Y-axis. The Y-axis data could both include "top 5\% of possible performance has been reached at X budget" and/or "top 5\% of candidates has been reached at X budget".  

Throughout our study we have evaluated 17 different parameterized benchmarks for this benchmarking suite. The core benchmarks that are presented here consist of 6 benchmarks that satisfy all of our requirements. These are benchmarks have a sufficiently large search space, and where the search difficulty is sufficiently high. The search difficulty is defined as the Expected mean of the difference between the samples found by random search and the optimal configuration as a function of the percentage of the search space explored.

Using permutation feature importance we can get insights into which features are most impactful for the performance.

Our findings show that the majority of the search parameters for the best of the existing benchmarks don't actually affect performance significantly. This is the case for the GEMM and nbody benchmark. The hotspot benchmark's search space is also drastically reduced, though since it had a very large search space to begin with, it is still an interesting space to explore.

These are the results from reducing the search space based on the feature importance.
%   nbody\_reduced: 56 (BLOCK\_SIZE, INNER\_UNROLL\_FACTO1, USE\_SOA)
%   GEMM: 54 (MDIMC, NDIMC, NDIMB, KWI)
%   TRIAD: 0 (none of them matter)
%   hotspot: 222 000 (block\_size\_x, block\_size\_y, tile\_size\_x, tile\_size\_y, temporal\_tiling\_factor)

%I could try to re-do the t-sne analysis with the reduced features, to get a better picture of their impact.

If there is a significant performance difference between the median and the best configuration, then autotuning is very beneficial for the benchmark. However, if the benchmarking difficulty is very low, then it's not very interesting to employ advanced search techniques on the benchmark, since well-performing configurations are quite common (their just not the median configuration).
\fi


\subsection{Max speedup over Median}
Fig.~\ref{fig:speedup} shows the speedup between the Median performance configuration of the search space and the best possible configuration found. While most of the benchmarks have speedups between 1.5 - 3.06x, outliers like the Hotspot benchmark have very significant speedups from 11.12 - 11.97x.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/speedup.png}
    \caption{Max speedup over Median configuration}
    \label{fig:speedup}
\end{figure}

\subsection{Performance Portability}
We can analyze the performance portability of configurations to measure the degree to which configurations are specifically optimized for each architecture. In Fig.~\ref{fig:portability} we show the relative performance compared with the optimal configuration for each architecture as the optimal configurations are transferred to the other architectures. The direction for this transfer is described by the optimal configuration for the GPU labeled in each row, being transferred to the different GPUs labeled on each of the columns.

In Fig.~\ref{fig:portability-pnpoly} we plot the portability for PnPoly. This shows that configurations are very portable between the RTX 3060 and RTX 3090, however configurations optimized for the RTX 3090 transfer poorly to the RTX Titan (58.5\% of optimal) and the 2080Ti (67.1\%). Similarly for the Convolution benchmark in Fig.~\ref{fig:portability-convolution} the optimal configuration for the RTX 3060 transfers poorly to the RTX 2080Ti (73.3\%) and RTX Titan (75.0\%).

\begin{figure*}[ht]
\iffalse
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/portability/portability_gemm.png}
    \caption{GEMM}
    \label{fig:my_label}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/portability/portability_pnpoly.png}
    \caption{Hotspot}
    \label{fig:my_label}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/portability/portability_pnpoly.png}
    \caption{Dedisp}
    \label{fig:my_label}
\end{subfigure}
\\
\fi
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/portability/portability_convolution.png}
    \caption{Convolution}
    \label{fig:portability-convolution}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/portability/portability_pnpoly.png}
    \caption{Pnpoly}
    \label{fig:portability-pnpoly}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/portability/portability_nbody.png}
    \caption{Nbody}
    \label{fig:my_label}
\end{subfigure}
\caption{Performance portability for exhaustively searched benchmarks on all architectures}
\label{fig:portability}
\end{figure*}

\begin{figure*}[ht]
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/feature_importance/gemm_features.png}
    \caption{GEMM}
    \label{fig:features-gemm}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/feature_importance/nbody_features.png}
    \caption{Nbody}
    \label{fig:features-nbody}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/feature_importance/hotspot_features.png}
    \caption{Hotspot}
    \label{fig:my_label}
\end{subfigure}
\\
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/feature_importance/convolution_features.png}
    \caption{Convolution}
    \label{fig:my_label}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/feature_importance/pnpoly_features.png}
    \caption{Pnpoly}
    \label{fig:my_label}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/feature_importance/dedisp_features.png}
    \caption{Dedisp}
    \label{fig:my_label}
\end{subfigure}
\\
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/feature_importance/expdist_features.png}
    \caption{Expdist}
    \label{fig:my_label}
\end{subfigure}
\caption{Feature importance for all benchmarks on all architectures}
\label{fig:feature-importance}
\end{figure*}

\subsection{Feature importance}
\iffalse
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{content/results/figures/r2_score.png}
    \caption{R-squared score from CatBoost for each benchmark and architecture.}
    \label{fig:model-score}
\end{figure}
\fi
To assess the importance of the different search parameters for the benchmark's objective, we train a Catboost Regression model over the dataset and analyze this model to investigate which features it finds useful for prediction. We use Permutation Feature Importance to then analyze the impact of each individual parameter on the model's predictive power.

%From Fig.~\ref{fig:model-score} we see that for the 
Training the model on the datasets the majority of the benchmarks and GPUs that use our CatBoost model is able to predict the performance of different configurations very precisely, with an R-squared score from 0.992 and upwards for all benchmarks except Convolution, where it ranges from 0.9268 to 0.9361. %With the Convolution and Dedispersion benchmarks being the only clear example where the model does not give close to perfect predictions. 


Using this model we can then generate the feature importances in Fig.~\ref{fig:feature-importance}.
We can observe that for many of the benchmarks, especially GEMM in Fig.~\ref{fig:features-gemm} and Nbody in Fig.~\ref{fig:features-nbody}, many of the parameters do not appear to have any meaningful impact on the model's predictive performance. Although the significance of these features may not extend to GPUs that are vastly different from those tested in this study, our findings are generally consistent across the various GPUs examined.


\subsection{Feature importance impact on relevant search space}

\begin{table*}[ht]
    \caption{Search space sizes of benchmarks in BAT}
    \centering
    \begin{tabular}{c|r|r|r|r|r}
        \toprule
        Benchmark & Cardinality & Constrained & Valid & Reduced & Reduce-Constrained \\
        \midrule
        PnPoly & 4 092 & 4 092 & 3 734 - 3 774 & 4 092 & 3 734 - 3 774\\ 
        Nbody & 9 408 & 1 568 & 1 568 & 112 & 70 \\ % TODO, make sure this is correct
        Convolution & 18 432 & 9 400 & 5 220 - 5 256 & 4 700 & 4 700\\
        GEMM & 82 944 & 17 956 & 17 956 & 17 956 & 17 956 \\
        Expdist & 9 732 096 & 540 000 & N/A & 144 & 96 \\
        Hotspot & 22 200 000 & 21 850 147 & N/A & 220 000 & 202 582 \\
        Dedisp & 123 863 040 & 107 011 905 & N/A & 3 870 720 & 3 327 135\\
        \bottomrule
    \end{tabular}
    \label{tab:space}
\end{table*}

Given our previous analysis we can reduce the search space of the benchmarks to only include those parameters that has at least 0.05 feature importance on any of the architectures. The results for the cardinality of the search spaces can be seen in Table~\ref{tab:space}. This gives an indication towards the size of the most interesting parts of the search space for the tested GPUs. Researchers using this benchmark can search over the full search space, but use this information to get better insight into how their models are able to concentrate on the most interesting parts of the search space. 
%Fila: we did the same in olha_exploiting_2020. Just my two cents: we should maybe state that shrinking search spaces this way may not improve the speed of search convergence (as the shape of the tuning space histogram remains the same, the probability of finding a well-performing configuration by the random searcher is the same). Also, it can be dangerous to shrink search spaces this way, as some tuning parameters can be important under conditions not observed by the experiment (for example, if some tuning parameter improves strong scaling on small inputs, but we experiment only with large inputs)

%Using the traditional approach for generating search spaces, by generating the Cartesian product, the search space for Dedisp requires around 28 GB of RAM to store. It therefore demonstrates how it is not practically feasible to generate and store extremely large search spaces through a Cartesian product. This demonstrates the importance of efficient data structures like CoT.

%On average we expect to spend about 1 second on compiling and running a kernel. Due to the large search space for Dedisp, Hotspot and Expdist it is not easy to do a brute force search of the entire search space. E.g. for Dedisp it would take an estimated 3.17 CPU years to evaluate all configurations.

\subsection{Discussion}
The results indicate the optimization parameters in our benchmarks have a significant impact on performance. While some parameters have more impact than others, the act of optimizing some parameters interact with other parameters. We observe this behavior through the Permutation Feature Importance summing up to a value much greater than 1 for many of the benchmarks. This behavior only occurs when there are significant dependencies between features. %Fila: I agree with "the act of optimizing some parameters interact with other parameters", but I don't see any data supporting this statement in the paper
Thus this provides evidence towards the need for global optimization as opposed to orthogonal search algorithms. While the importance of each parameter is generally consistent across GPU architectures, the specific values for these parameters need to be optimized for the target architecture. Our portability study shows that simply transferring the optimal configuration from one architecture to another can give as low as 58.5\% of the optimal performance, while other configurations can be ported at 99.9\% of the optimal performance. Generally this is the case for GPUs of the same family of architectures like our RTX 3060 and RTX 3090.

