\section{Introduction}\label{chap:introduction}
%Add a section on relation to original BAT paper and re+write abstract to reflect this. %Comment from Prof. Elster

%The Benchmark suite for AutoTuners (BAT) is a joint effort between several European autotuning research groups to establish a common benchmark suite for the field. This effort was partly inspired by a workshop on Generic Autotuning at the Lorentz Center in the Netherlands. The benchmark suite builds on three novel specifications that were defined by the working group members of the workshop. The first specification defines the autotuning problem, the second specification defines how to report the results of autotuning experiments, and the third specification defines the metadata and how experiments should be shared. BAT is the first implementation to integrate all of these specifications, creating an end-to-end pipeline that allows researchers to perform and share autotuning experiments with results that are unambiguous and interoperable. The interoperability and sharing of data enabled by BAT enable useful applications such as dataset analysis and reproducible autotuning experiments through simulated runs. In this paper we show how this framework provides a benchmark suite for assessing GPU kernel tuners.
% Ben: I noticed the above paragraph is commented here, I would indeed not recommend to start the introduction with that and first establish the context and explain the problem we are trying to solve. I do believe indeed that it would be nice to position this paper as a joint/community effort that originated at the workshop, so mentioning these things at some later point in the introduction is definitely a good idea.

As computers have become more advanced in recent decades, there has been a significant increase in their complexity. Central Processing Units (CPUs) still form the core of modern computers, but we have seen a growing use of accelerators like Graphics Processing Units (GPUs) and co-processors to improve efficiency and performance. These accelerators can be highly effective, but they can also make the task of optimizing programs for performance increasingly difficult.

New data centers and many of the world's top supercomputers, such as the Top500 systems, have increasingly relied on High-Performance Computing (HPC) systems that use a combination of CPUs and GPUs~\cite{heldens2020landscape}. These heterogeneous systems can make code optimization a challenging task, as the architectures of the different components change rapidly.

%**To make programs run efficiently on these systems, researchers in 
To make programs run efficiently on these systems, programmers in 
computer science and related fields spend considerable efforts in optimization~\cite{hijma2022}. However, as the complexity of these systems grows, the task of understanding how all the system components interact becomes increasingly complex. To address this, analytical methods such as compiler optimization are used to understand the systems and turn that knowledge into rules for optimizing programs. These rules can modify large chunks of code without changing the semantics of the program, but they can also contain heuristics that may not always lead to the optimal solution. %Fila I have removed "add, remove", as dead code removal is only one of the multiple optimizations done by compilers and, according to what I know about compilers, they rather modify code than add some completely new one

As architectures are constantly evolving and different, a heuristic-based rule cannot always generalize well to the entire system. In these cases, empirical methods which search for the optimal solution through trial and error can be used, this is called \textit{autotuning}. 
%, and it uses additional data to make the program more efficient by adjusting its performance. %Fila: I don't understand the second part of this sentence

One of the main consideration when developing such solutions is to hide as many architecture-based optimization details as possible from the end-user, so that the code can be used efficiently across different generations of systems. The goal is to provide easy-to-use libraries and APIs that enable developers to write code that runs well on different systems without having to understand the intricacies of each system's architecture.

In recent years, several studies have been conducted that present advancements in optimization algorithms for autotuning~\cite{schoonhoven2022benchmarking}. Despite this, comparative analysis between related works is scarce, owing to the significant manual effort required to porting a tunable kernel from one tuner another. In order to effectively study the performance of optimization algorithms for autotuning, as well as to facilitate comparisons between optimization algorithms implemented in different tuners, it is necessary to have a benchmark suite that is compatible with all of these tuners. 

Existing benchmark suites, such as %Parboil~\cite{},
Rodinia~\cite{che_rodinia_2009}, SHOC~\cite{danalis_scalable_2010}, PolyBenchGPU~\cite{grauer-gray_auto-tuning_2012}, 
%OpenDwarfs~\cite{} 
are not suitable for this purpose as they are not tunable. They typically have hardcoded thread block dimensions, but even worse, the code is often written with assumptions that the block dimensions, parallelizations, and amount of work per thread will never change. Furthermore, in many cases, the hardcoded numbers are directly dependent on the input problems, which are often unrealistically small for modern GPUs. As a result, tuning these codes requires extensive modifications.

Earlier attempts at creating tunable benchmark suites~\cite{petrovic_benchmark_2019, sund_bat_2021} are limited
%*** for _> regarding
regarding studying the effectiveness of optimization algorithms. The benchmark suite from Petrovi\v{c} et al.~\cite{petrovic_benchmark_2019} only supports one tuning framework, while the optimization parameters in Sund et al.~\cite{sund_bat_2021} have limited performance impact.
%Earlier attempts at creating tunable benchmark suites~\cite{petrovic_benchmark_2019, sund_bat_2021} are limited for studying the effectiveness of optimization algorithms. The benchmark suite from Petrovic et al.~\cite{petrovic_benchmark_2019} only supports one tuning framework, while the optimization parameters in Sund et al.~\cite{sund_bat_2021} have limited performance impact. %because 
 %the search spaces of some problems included in the suites are too small, and those suites use only one tuning framework. %optimization of the search parameters either have insignificant impact on the performance or the search spaces are unrealistically small. 
 %Fila: I disagree that tuning spaces in KTT are unrealistically small, as we can achieve close-to-optimal performance on most GPU architectures, so the spaces should be sufficient. I also think that we should stress here that BAT can compare multiple search strategies in multiple autotuners, so we don't need to port searchers to different tuners, and we can compare the whole tuning pipeline if we want.

The "BAT 2.0" benchmark suite aims to stimulate autotuning research by offering a set of tunable kernels that are representative of those used in various real-world applications, such as machine learning, image processing, astrophysics, thermal modeling, microscopy, geospatial information systems, and radio astronomy.

This benchmark suite facilitates for comparisons between optimization algorithms from different autotuners by providing a standardized problem interface for both the autotuners and benchmarks. The benchmark suite provides general configuration space and kernel handler classes providing for easy integration towards Optuna~\cite{akiba_optuna_2019}, SMAC3~\cite{hutter_sequential_2011}, KernelTuner~\cite{van_werkhoven_kernel_2019}, KTT\cite{petrovic_benchmark_2019} as well as our own basic reference tuner. This enables the study of code optimization effectiveness, search space difficulty, performance portability, and more. With the creation of this benchmark suite, researchers can now investigate key questions about code optimization, search space, and performance portability.

%This benchmark suite "BAT 2.0" is intended to be a catalyst for autotuning research in general. It consists of a number of highly-tunable kernels that are representative of kernels used in a wide range of real-world application domains including machine learning, image processing, astrophysics, integrated-circuit thermal modeling, localization microscopy, geospatial information systems and radio astronomy.

%This benchmark suite can be used to facilitate comparisons between optimization algorithms implemented in different auto-tuners. We enable this by providing a shared problem interface that autotuners and benchmarks can target. In addition, this benchmark suite can be used to study the effectiveness and interplay of different code optimizations, search space difficulty, performance portability, etc. Now that we have created this benchmark suite, it enables us to answer several research questions with regard to the effectiveness and interplay of different code optimizations, search space difficulty, and performance portability.

% You can also present this last point in this way: 
% Ben: I think the Introduction up to this point very clearly introduces the concepts of auto-tuning and explains why it is important. However, the intro does not explain yet why we need a benchmark suite for auto-tuning research, which is perhaps the most important argument to make in this introduction.
%
% I think the arguments to create this benchmark suite are as follows:

\iffalse
The rest of this paper will be organized as follows:
    The following section 
    Section~\ref{chap:background} 
    introduces general theoretical background,
    followed by an introduction to autotuning techniques (Section~\ref{sec:autotuning}).
    Section~\ref{chap:related-work} introduces and discusses related work, whereas 
    Section~\ref{sec:experimental-design} details our experimental design and Section~\ref{sec:framework} details the framework we use to conduct the experiments.
    Finally, Section~\ref{chap:results} contains our experimental results and our discussion of the data, and 
    Section~\ref{chap:conclusion} concludes the paper and gives some ideas regarding future work left to be done.
\fi

