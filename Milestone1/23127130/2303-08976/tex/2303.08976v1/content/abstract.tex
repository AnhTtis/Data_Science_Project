
\begin{abstract}
%We present a new benchmarking suite for studying the performance of optimization algorithms in modern computing systems that consist of a combination of CPUs and GPUs. The benchmarking suite consists of highly-tunable GPU kernels representative of real-world applications and enables comparisons between optimization algorithms and the study of code optimizations, search space difficulty, and performance portability. Our framework facilitates easy addition of new autotuners and benchmarks by defining a shared problem interface.

%** Need more generalize intro/motivation -- why do we care about kernel tuners and benchmarking suites?
As computing system become more complex combining CPUs and GPUs, it is becoming harder and harder for programmers to keep their codes optimized as the hardware gets updated. Autotuners try to alleviate this by hiding as many architecture-based optimization details as possible from the end-user, so that the code can be used efficiently across different generations of systems. Several autotuning frameworks have emerged, but a comparative analysis between
these related works is scarce, owing to the significant manual
effort required to port a tunable kernel from one tuner
another. 

%***We introduce a new benchmark suite for evaluating the performance of 
In this article we introduce a new benchmark suite for evaluating the performance of 
%**optimization algorithms in modern systems using GPUs. The suite 
optimization algorithms used by modern autotuners targeting GPUs. The suite 
contains tunable GPU kernels that are representative of real-world applications, allowing for comparisons between optimization algorithms and the examination of code optimization, search space difficulty, and performance portability. Our framework facilitates easy integration of new autotuners and benchmarks by defining a shared problem interface.

%**The benchmark suite is analyzed based on five characteristics: 
Our benchmark suite is evaluated based on five characteristics: 
convergence rate, local minima centrality, optimal speedup, 
%*** adding commonly used abbreviation :-)
Permutation Feature Importance (PFI), and performance portability. The results show that optimization parameters greatly impact performance and the need for global optimization. The importance of each parameter is consistent across GPU architectures, however, the specific values need to be optimized for each architecture.

%The benchmarking suite is analyzed using five key characteristics including convergence rate, local minima centrality, optimal speedup, Permutation Feature Importance, and performance portability. Results show that optimization parameters have a significant impact on performance and the need for global optimization. The importance of each parameter is consistent across GPU architectures but specific values need to be optimized for the target architecture.

Our portability study highlights the crucial importance of autotuning each application for a specific target architecture. The results reveal that simply transferring the optimal configuration from one architecture to another can result in a performance ranging from 58.5\% to 99.9\% of the optimal performance, depending on the GPU architecture. This highlights the importance of autotuning in modern computing systems and the value of our benchmark suite in facilitating the study of optimization algorithms and their effectiveness in achieving optimal performance for specific target architectures.



%Our portability study highlights the crucial importance of optimizing each application for a specific target architecture through autotuning. The study shows that simply transferring the optimal configuration from one architecture to another can result in %a decrease in 
%performance ranging from as low as 58.5\% to 99.9\% of the optimal performance, depending on the GPU architecture. This underscores the importance of autotuning in modern computing systems and the value of our benchmarking suite in facilitating the study of optimization algorithms and their effectiveness in achieving optimal performance for specific target architectures.





%The increasing complexity of modern computing systems, which often involve a combination of CPUs and GPUs, makes the task of optimizing programs for performance increasingly difficult. To address this, researchers have turned to analytical methods such as compiler optimization and autotuning, which uses trial and error to search for the optimal solution. However, existing benchmark suites are not suitable for studying the performance of these optimization algorithms, as they are often not tunable enough and often contain hardcoded assumptions about system architectures.

%To address this issue, we present a new benchmarking suite, which consists of a number of highly-tunable kernels that are representative of those used in a wide range of real-world application domains. This benchmark suite can be used to facilitate comparisons between optimization algorithms implemented in different autotuners and to study the effectiveness and interplay of different code optimizations, search space difficulty, and performance portability. The availability of this benchmarking suite enables researchers to answer important questions about the effectiveness of different optimization techniques and the performance trade-offs involved.

%The benchmarking framework for kernel tuners is an evolution of the Benchmark suite for AutoTuners project. In this work we present a framework that enables easy addition of new autotuners and benchmarks by defining a shared problem interface. Using this framework we implement a benchmarking suite that facilitates   

%We analyze the benchmarks in our benchmarking suite using five key characteristics: the convergence rate of random search towards the optimal solution, a local minima centrality metric based on PageRank, the optimal speedup compared with the median performing configuration, the Permutation Feature Importance of the search space based on a state-of-the-art regression model, and the performance portability of the optimal solution for each architecture.

%The results indicate the optimization parameters in our benchmarks have a significant impact on performance. While some parameters have more impact than others, the act of optimizing some parameters interact with other parameters. Thus providing evidence towards the need for global optimization compared with orthogonal search algorithms. While the importance of the each parameter is generally consistent across GPU architectures, the specific values for these parameters need to be optimized for the target architecture. Our portability study show that simply transfering the optimal configuration from one architecture to another can give as low as 58.5\% of the optimal performance, while other configurations can be ported at 99.9\% of the optimal performance. Generally this is the case for GPUs of the same family of archictures like our RTX 3060 and RTX 3090.

\end{abstract}
