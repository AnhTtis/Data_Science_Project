
@inproceedings{prokhorenkova_catboost_2018,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'18},
	title = {{CatBoost}: unbiased boosting with categorical features},
	shorttitle = {{CatBoost}},
	abstract = {This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to fight a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.},
	urldate = {2023-01-31},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
	month = dec,
	year = {2018},
	pages = {6639--6649},
}

@article{prokhorenkova_catboost_nodate,
	title = {{CatBoost}: unbiased boosting with categorical features},
	abstract = {This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to ﬁght a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.},
	language = {en},
	author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
}

@article{dorogush_catboost_nodate,
	title = {{CatBoost}: gradient boosting with categorical features support},
	abstract = {In this paper we present CatBoost, a new open-sourced gradient boosting library that successfully handles categorical features and outperforms existing publicly available implementations of gradient boosting in terms of quality on a set of popular publicly available datasets. The library has a GPU implementation of learning algorithm and a CPU implementation of scoring algorithm, which are signiﬁcantly faster than other gradient boosting libraries on ensembles of similar sizes.},
	language = {en},
	author = {Dorogush, Anna Veronika and Ershov, Vasily and Gulin, Andrey},
}

@article{sund_bat_2021,
	title = {{BAT}: {A} {Benchmark} suite for {AutoTuners}},
	copyright = {Copyright (c) 2021},
	issn = {1892-0721},
	shorttitle = {{BAT}},
	url = {https://ojs.bibsys.no/index.php/NIK/article/view/915},
	abstract = {the code by ?nding the best possible values for a given architecture. To our knowledge, there are currently no standardized benchmark suites for comparing and testing autotuners. Developers of autotuners thus make their own when presenting and comparing autotuners.\&nbsp;We thus present BAT, a Benchmark suite for AutoTuners with HPC-based parameterized GPU programs. CUDA programs and kernels from\&nbsp;"The Scalable Heterogeneous Computing (SHOC) Benchmark" are parameterized. BAT contains a varied selection of benchmarks of different complexity that can utilize multiple GPUs on one system, either by\&nbsp;running the same program and computations on multiple nodes, or by\&nbsp;splitting the work between nodes. BAT contains 9 di?erent HPC benchmarks that provide a large search space of autotuning parameters, and\&nbsp;are modified to suite many di?erent autotuners. BAT also includes a CLI\&nbsp;that facilitates autotuning with the benchmarks.\&nbsp;Our benchmark suite is tested with four di?erent autotuners, OpenTuner,\&nbsp;Kernel Tuner, CLTune and KTT. They di?er in setup and how they tune.\&nbsp;The impact of the di?erent benchmark parameters on the running time\&nbsp;across architectures is analyzed. Test systems used include a DGX-2,\&nbsp;IBM Power System AC922 with Tesla V100-SXM2 32 GB GPUs, an\&nbsp;RTX Titan, a GeForce GTX 980 and a server with 20 Tesla T4 GPUs.},
	language = {en},
	number = {1},
	urldate = {2021-12-10},
	journal = {Norsk IKT-konferanse for forskning og utdanning},
	author = {Sund, Ingunn and Kirkhorn, Knut A. and Tørring, Jacob O. and Elster, Anne C.},
	month = nov,
	year = {2021},
	note = {Number: 1},
	pages = {44--57},
}

@misc{noauthor_introduction_nodate,
	title = {An introduction to the set theoretical language {SETL} {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/0898122175900115?token=952A3E07F65872F0D268A657F5E543187301A0BABA552BF46012A4D776D43D13AA572E92627FE8AB2079F35B3D691E45&originRegion=us-east-1&originCreation=20230124173147},
	language = {en},
	urldate = {2023-01-24},
	doi = {10.1016/0898-1221(75)90011-5},
	note = {ISSN: 0898-1221},
}

@article{ragan-kelley_halide_2017,
	title = {Halide: decoupling algorithms from schedules for high-performance image processing},
	volume = {61},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Halide},
	url = {https://dl.acm.org/doi/10.1145/3150211},
	doi = {10.1145/3150211},
	abstract = {Writing high-performance code on modern machines requires not just locally optimizing inner loops, but globally reorganizing computations to exploit parallelism and locality—doing things such as tiling and blocking whole pipelines to fit in cache. This is especially true for image processing pipelines, where individual stages do much too little work to amortize the cost of loading and storing results to and from off-chip memory. As a result, the performance difference between a naïve implementation of a pipeline and one globally optimized for parallelism and locality is often an order of magnitude. However, using existing programming tools, writing high-performance image processing code requires sacrificing simplicity, portability, and modularity. We argue that this is because traditional programming models conflate the computations defining the algorithm with decisions about intermediate storage and the order of computation, which we call the schedule.},
	language = {en},
	number = {1},
	urldate = {2023-01-23},
	journal = {Communications of the ACM},
	author = {Ragan-Kelley, Jonathan and Adams, Andrew and Sharlet, Dillon and Barnes, Connelly and Paris, Sylvain and Levoy, Marc and Amarasinghe, Saman and Durand, Frédo},
	month = dec,
	year = {2017},
	pages = {106--115},
}

@article{noauthor_mosaic_nodate,
	title = {Mosaic: {An} {Interoperable} {Compiler} for {Tensor} {Algebra}},
	volume = {1},
	language = {en},
	number = {1},
}

@article{sundararajah_unirec_2022,
	title = {{UniRec}: a unimodular-like framework for nested recursions and loops},
	volume = {6},
	issn = {2475-1421},
	shorttitle = {{UniRec}},
	url = {https://dl.acm.org/doi/10.1145/3563333},
	doi = {10.1145/3563333},
	abstract = {KIRSHANTHAN SUNDARARAJAH, Purdue University, USA CHARITHA SAUMYA, Purdue University, USA MILIND KULKARNI, Purdue University, USA Scheduling transformations reorder operations in a program to improve locality and/or parallelism. There are mature loop transformation frameworks such as the polyhedral model for composing and applying instance-wise scheduling transformations for loop nests. In recent years, there have been efforts to build frameworks for composing and applying scheduling transformations for nested recursion and loops, but these frameworks cannot employ the full power of transformations for loop nests since they have overly-restrictive representations. This paper describes a new framework, UniRec, that not only generalizes prior frameworks for reasoning about transformations on recursion, but also generalizes the unimodular framework, and hence unifies reasoning about perfectly-nested loops and recursion. CCS Concepts: · Software and its engineering → Compilers; Recursion; Software performance.},
	language = {en},
	number = {OOPSLA2},
	urldate = {2023-01-20},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Sundararajah, Kirshanthan and Saumya, Charitha and Kulkarni, Milind},
	month = oct,
	year = {2022},
	pages = {1264--1290},
}

@article{rasch_efficient_2021,
	title = {Efficient {Auto}-{Tuning} of {Parallel} {Programs} with {Interdependent} {Tuning} {Parameters} via {Auto}-{Tuning} {Framework} ({ATF})},
	volume = {18},
	issn = {1544-3566, 1544-3973},
	url = {https://dl.acm.org/doi/10.1145/3427093},
	doi = {10.1145/3427093},
	abstract = {Auto-tuning is a popular approach to program optimization: it automatically finds good configurations of a program’s so-called tuning parameters whose values are crucial for achieving high performance for a particular parallel architecture and characteristics of input/output data. We present three new contributions of the Auto-Tuning Framework (ATF), which enable a key advantage in
              general-purpose auto-tuning
              : efficiently optimizing programs whose tuning parameters have
              interdependencies
              among them. We make the following contributions to the three main phases of general-purpose auto-tuning: (1) ATF
              generates
              the search space of interdependent tuning parameters with high performance by efficiently exploiting parameter constraints; (2) ATF
              stores
              such search spaces efficiently in memory, based on a novel chain-of-trees search space structure; (3) ATF
              explores
              these search spaces faster, by employing a multi-dimensional search strategy on its chain-of-trees search space representation. Our experiments demonstrate that, compared to the state-of-the-art, general-purpose auto-tuning frameworks, ATF substantially improves generating, storing, and exploring the search space of interdependent tuning parameters, thereby enabling an efficient overall auto-tuning process for important applications from popular domains, including stencil computations, linear algebra routines, quantum chemistry computations, and data mining algorithms.},
	language = {en},
	number = {1},
	urldate = {2023-01-19},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Rasch, Ari and Schulze, Richard and Steuwer, Michel and Gorlatch, Sergei},
	month = mar,
	year = {2021},
	pages = {1--26},
}

@inproceedings{akiba_optuna_2019,
	address = {Anchorage, AK, USA},
	series = {{KDD} '19},
	title = {Optuna: {A} {Next}-generation {Hyperparameter} {Optimization} {Framework}},
	isbn = {978-1-4503-6201-6},
	shorttitle = {Optuna},
	url = {https://doi.org/10.1145/3292500.3330701},
	doi = {10.1145/3292500.3330701},
	abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
	urldate = {2020-03-27},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
	month = jul,
	year = {2019},
	keywords = {Bayesian optimization, \_tablet\_modified, black-box optimization, hyperparameter optimization, machine learning system},
	pages = {2623--2631},
}

@article{hellsten_baco_2022,
	title = {{BaCO}: {A} {Fast} and {Portable} {Bayesian} {Compiler} {Optimization} {Framework}},
	shorttitle = {{BaCO}},
	url = {https://arxiv.org/abs/2212.11142v1},
	doi = {10.48550/arXiv.2212.11142},
	abstract = {We introduce the Bayesian Compiler Optimization framework (BaCO), a general purpose autotuner for modern compilers targeting CPUs, GPUs, and FPGAs. BaCO provides the flexibility needed to handle the requirements of modern autotuning tasks. Particularly, it deals with permutation, ordered, and continuous parameter types along with both known and unknown parameter constraints. To reason about these parameter types and efficiently deliver high-quality code, BaCO uses Bayesian optimization algorithms specialized towards the autotuning domain. We demonstrate BaCO's effectiveness on three modern compiler systems: TACO, RISE \& ELEVATE, and HPVM2FPGA for CPUs, GPUs, and FPGAs respectively. For these domains, BaCO outperforms current state-of-the-art autotuners by delivering on average 1.39x-1.89x faster code with a tiny search budget, and BaCO is able to reach expert-level performance 2.89x-8.77x faster.},
	language = {en},
	urldate = {2023-01-18},
	author = {Hellsten, Erik and Souza, Artur and Lenfers, Johannes and Lacouture, Rubens and Hsu, Olivia and Ejjeh, Adel and Kjolstad, Fredrik and Steuwer, Michel and Olukotun, Kunle and Nardi, Luigi},
	month = dec,
	year = {2022},
}

@inproceedings{alpcan_can_2014,
	title = {Can we measure the difficulty of an optimization problem?},
	doi = {10.1109/ITW.2014.6970853},
	abstract = {Can we measure the difficulty of an optimization problem? Although optimization plays a crucial role in modern science and technology, a formal framework that puts problems and solution algorithms into a broader context has not been established. This paper presents a conceptual approach which gives a positive answer to the question for a broad class of optimization problems. Adopting an information and computational perspective, the proposed framework builds upon Shannon and algorithmic information theories. As a starting point, a concrete model and definition of optimization problems is provided. Then, a formal definition of optimization difficulty is introduced which builds upon algorithmic information theory. Following an initial analysis, lower and upper bounds on optimization difficulty are established. One of the upper-bounds is closely related to Shannon information theory and black-box optimization. Finally, various computational issues and future research directions are discussed.},
	booktitle = {2014 {IEEE} {Information} {Theory} {Workshop} ({ITW} 2014)},
	author = {Alpcan, Tansu and Everitt, Tom and Hutter, Marcus},
	month = nov,
	year = {2014},
	note = {ISSN: 1662-9019},
	keywords = {Algorithm design and analysis, Complexity theory, Educational institutions, Information theory, Linear programming, Optimization, Runtime},
	pages = {356--360},
}

@article{eggensperger_hpobench_2021,
	title = {{HPOBench}: {A} {Collection} of {Reproducible} {Multi}-{Fidelity} {Benchmark} {Problems} for {HPO}},
	shorttitle = {{HPOBench}},
	url = {https://www.semanticscholar.org/paper/HPOBench%3A-A-Collection-of-Reproducible-Benchmark-Eggensperger-M%C3%BCller/4e920958066c0f91005dfa8caa7e95bd01fde619},
	abstract = {To achieve peak predictive performance, hyperparameter optimization (HPO) is a crucial component of machine learning and its applications. Over the last years, the number of efﬁcient algorithms and tools for HPO grew substantially. At the same time, the community is still lacking realistic, diverse, computationally cheap, and standardized benchmarks. This is especially the case for multi-ﬁdelity HPO methods. To close this gap, we propose HPOBench , which includes 7 existing and 5 new benchmark families, with a total of more than 100 multiﬁdelity benchmark problems. HPOBench allows to run this extendable set of multi-ﬁdelity HPO benchmarks in a reproducible way by isolating and packaging the individual benchmarks in containers. It also provides surrogate and tabular benchmarks for computationally affordable yet statistically sound evaluations. To demonstrate HPOBench ’s broad compatibility with various optimization tools, as well as its usefulness, we conduct an exemplary large-scale study evaluating 13 optimizers from 6 optimization tools. We provide HPOBench here: https: //github.com/automl/HPOBench .},
	urldate = {2023-01-11},
	journal = {ArXiv},
	author = {Eggensperger, Katharina and Müller, Philip and Mallik, Neeratyoy and Feurer, Matthias and Sass, René and Klein, Aaron and Awad, Noor H. and Lindauer, M. and Hutter, F.},
	month = sep,
	year = {2021},
}

@misc{eggensperger_hpobench_2022,
	title = {{HPOBench}: {A} {Collection} of {Reproducible} {Multi}-{Fidelity} {Benchmark} {Problems} for {HPO}},
	shorttitle = {{HPOBench}},
	url = {http://arxiv.org/abs/2109.06716},
	doi = {10.48550/arXiv.2109.06716},
	abstract = {To achieve peak predictive performance, hyperparameter optimization (HPO) is a crucial component of machine learning and its applications. Over the last years, the number of efficient algorithms and tools for HPO grew substantially. At the same time, the community is still lacking realistic, diverse, computationally cheap, and standardized benchmarks. This is especially the case for multi-fidelity HPO methods. To close this gap, we propose HPOBench, which includes 7 existing and 5 new benchmark families, with a total of more than 100 multi-fidelity benchmark problems. HPOBench allows to run this extendable set of multi-fidelity HPO benchmarks in a reproducible way by isolating and packaging the individual benchmarks in containers. It also provides surrogate and tabular benchmarks for computationally affordable yet statistically sound evaluations. To demonstrate HPOBench's broad compatibility with various optimization tools, as well as its usefulness, we conduct an exemplary large-scale study evaluating 13 optimizers from 6 optimization tools. We provide HPOBench here: https://github.com/automl/HPOBench.},
	urldate = {2023-01-11},
	publisher = {arXiv},
	author = {Eggensperger, Katharina and Müller, Philipp and Mallik, Neeratyoy and Feurer, Matthias and Sass, René and Klein, Aaron and Awad, Noor and Lindauer, Marius and Hutter, Frank},
	month = oct,
	year = {2022},
	note = {arXiv:2109.06716 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{sehic_lassobench_2022,
	title = {{LassoBench}: {A} {High}-{Dimensional} {Hyperparameter} {Optimization} {Benchmark} {Suite} for {Lasso}},
	shorttitle = {{LassoBench}},
	url = {http://arxiv.org/abs/2111.02790},
	abstract = {While Weighted Lasso sparse regression has appealing statistical guarantees that would entail a major real-world impact in nance, genomics, and brain imaging applications, it is typically scarcely adopted due to its complex high-dimensional space composed by thousands of hyperparameters. On the other hand, the latest progress with high-dimensional hyperparameter optimization (HD-HPO) methods for black-box functions demonstrates that high-dimensional applications can indeed be e ciently optimized. Despite this initial success, HD-HPO approaches are mostly applied to synthetic problems with a moderate number of dimensions, which limits its impact in scienti c and engineering applications. We propose LassoBench, the rst benchmark suite tailored for Weighted Lasso regression. LassoBench consists of benchmarks for both well-controlled synthetic setups (number of samples, noise level, ambient and e ective dimensionalities, and multiple delities) and real-world datasets, which enables the use of many avors of HPO algorithms to be studied and extended to the high-dimensional Lasso setting. We evaluate 6 state-of-the-art HPO methods and 3 Lasso baselines, and demonstrate that Bayesian optimization and evolutionary strategies can improve over the methods commonly used for sparse regression while highlighting limitations of these frameworks in very high-dimensional and noisy settings.},
	language = {en},
	urldate = {2023-01-11},
	publisher = {arXiv},
	author = {Šehić, Kenan and Gramfort, Alexandre and Salmon, Joseph and Nardi, Luigi},
	month = jun,
	year = {2022},
	note = {arXiv:2111.02790 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{wang_fedhpo-b_2022,
	title = {{FedHPO}-{B}: {A} {Benchmark} {Suite} for {Federated} {Hyperparameter} {Optimization}},
	shorttitle = {{FedHPO}-{B}},
	url = {http://arxiv.org/abs/2206.03966},
	abstract = {Hyperparameter optimization (HPO) is crucial for machine learning algorithms to achieve satisfactory performance, whose progress has been boosted by related benchmarks. Nonetheless, existing efforts in benchmarking all focus on HPO for traditional centralized learning while ignoring federated learning (FL), a promising paradigm for collaboratively learning models from dispersed data. In this paper, we ﬁrst identify some uniqueness of HPO for FL algorithms from various aspects. Due to this uniqueness, existing HPO benchmarks no longer satisfy the need to compare HPO methods in the FL setting. To facilitate the research of HPO in the FL setting, we propose and implement a benchmark suite FEDHPO-B that incorporates comprehensive FL tasks, enables efﬁcient function evaluations, and eases continuing extensions. We also conduct extensive experiments based on FEDHPO-B to benchmark a few HPO methods. We open-source FEDHPO-B at https://github.com/alibaba/FederatedScope/tree/master/benchmark/FedHPOB.},
	language = {en},
	urldate = {2023-01-11},
	publisher = {arXiv},
	author = {Wang, Zhen and Kuang, Weirui and Zhang, Ce and Ding, Bolin and Li, Yaliang},
	month = jun,
	year = {2022},
	note = {arXiv:2206.03966 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{fursin_collective_2021,
	title = {Collective knowledge: organizing research projects as a database of reusable components and portable workflows with common interfaces},
	volume = {379},
	issn = {1364-503X, 1471-2962},
	shorttitle = {Collective knowledge},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0211},
	doi = {10.1098/rsta.2020.0211},
	abstract = {This article provides the motivation and overview of the Collective Knowledge Framework (CK or cKnowledge). The CK concept is to decompose research projects into reusable components that encapsulate research artifacts and provide unified application programming interfaces (APIs), command-line interfaces (CLIs), meta descriptions and common automation actions for related artifacts. The CK framework is used to organize and manage research projects as a database of such components. Inspired by the USB ‘plug and play’ approach for hardware, CK also helps to assemble portable workflows that can automatically plug in compatible components from different users and vendors (models, datasets, frameworks, compilers, tools). Such workflows can build and run algorithms on different platforms and environments in a unified way using the customizable CK program pipeline with software detection plugins and the automatic installation of missing packages. This article presents a number of industrial projects in which the modular CK approach was successfully validated in order to automate benchmarking, auto-tuning and co-design of efficient software and hardware for machine learning and artificial intelligence in terms of speed, accuracy, energy, size and various costs. The CK framework also helped to automate the artifact evaluation process at several computer science conferences as well as to make it easier to reproduce, compare and reuse research techniques from published papers, deploy them in production, and automatically adapt them to continuously changing datasets, models and systems. The long-term goal is to accelerate innovation by connecting researchers and practitioners to share and reuse all their knowledge, best practices, artifacts, workflows and experimental results in a common, portable and reproducible format at
              cKnowledge.io
              .
            
            
              This article is part of the theme issue ‘Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification
              in silico
              ’.},
	language = {en},
	number = {2197},
	urldate = {2023-01-11},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Fursin, Grigori},
	month = may,
	year = {2021},
	pages = {rsta.2020.0211, 20200211},
}

@inproceedings{grauer-gray_auto-tuning_2012,
	address = {San Jose, CA, USA},
	title = {Auto-tuning a high-level language targeted to {GPU} codes},
	isbn = {978-1-4673-2633-9 978-1-4673-2632-2 978-1-4673-2631-5},
	url = {http://ieeexplore.ieee.org/document/6339595/},
	doi = {10.1109/InPar.2012.6339595},
	abstract = {Determining the best set of optimizations to apply to a kernel to be executed on the graphics processing unit (GPU) is a challenging problem. There are large sets of possible optimization conﬁgurations that can be applied, and many applications have multiple kernels. Each kernel may require a speciﬁc conﬁguration to achieve the best performance, and moving an application to new hardware often requires a new optimization conﬁguration for each kernel.},
	language = {en},
	urldate = {2023-01-11},
	booktitle = {2012 {Innovative} {Parallel} {Computing} ({InPar})},
	publisher = {IEEE},
	author = {Grauer-Gray, Scott and Xu, Lifan and Searles, Robert and Ayalasomayajula, Sudhee and Cavazos, John},
	month = may,
	year = {2012},
	pages = {1--10},
}

@inproceedings{zien_feature_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {Feature} {Importance} {Ranking} {Measure}},
	isbn = {978-3-642-04174-7},
	doi = {10.1007/978-3-642-04174-7_45},
	abstract = {Most accurate predictions are typically obtained by learning machines with complex feature spaces (as e.g. induced by kernels). Unfortunately, such decision rules are hardly accessible to humans and cannot easily be used to gain insights about the application domain. Therefore, one often resorts to linear models in combination with variable selection, thereby sacrificing some predictive power for presumptive interpretability. Here, we introduce the Feature Importance Ranking Measure (FIRM), which by retrospective analysis of arbitrary learning machines allows to achieve both excellent predictive performance and superior interpretation. In contrast to standard raw feature weighting, FIRM takes the underlying correlation structure of the features into account. Thereby, it is able to discover the most relevant features, even if their appearance in the training data is entirely prevented by noise. The desirable properties of FIRM are investigated analytically and illustrated in simulations.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer},
	author = {Zien, Alexander and Krämer, Nicole and Sonnenburg, Sören and Rätsch, Gunnar},
	editor = {Buntine, Wray and Grobelnik, Marko and Mladenić, Dunja and Shawe-Taylor, John},
	year = {2009},
	keywords = {Binary Feature, Feature Weighting, Importance Measure, Multiple Kernel Learn, Random Forest},
	pages = {694--709},
}

@article{blank_pymoo_2020,
	title = {Pymoo: {Multi}-{Objective} {Optimization} in {Python}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {Pymoo},
	url = {https://ieeexplore.ieee.org/document/9078759/},
	doi = {10.1109/ACCESS.2020.2990567},
	abstract = {Python has become the programming language of choice for research and industry projects related to data science, machine learning, and deep learning. Since optimization is an inherent part of these research ﬁelds, more optimization related frameworks have arisen in the past few years. Only a few of them support optimization of multiple conﬂicting objectives at a time, but do not provide comprehensive tools for a complete multi-objective optimization task. To address this issue, we have developed pymoo, a multiobjective optimization framework in Python. We provide a guide to getting started with our framework by demonstrating the implementation of an exemplary constrained multi-objective optimization scenario. Moreover, we give a high-level overview of the architecture of pymoo to show its capabilities followed by an explanation of each module and its corresponding sub-modules. The implementations in our framework are customizable and algorithms can be modiﬁed/extended by supplying custom operators. Moreover, a variety of single, multi- and many-objective test problems are provided and gradients can be retrieved by automatic differentiation out of the box. Also, pymoo addresses practical needs, such as the parallelization of function evaluations, methods to visualize low and high-dimensional spaces, and tools for multi-criteria decision making. For more information about pymoo, readers are encouraged to visit: https://pymoo.org.},
	language = {en},
	urldate = {2023-01-10},
	journal = {IEEE Access},
	author = {Blank, Julian and Deb, Kalyanmoy},
	year = {2020},
	pages = {89497--89509},
}

@article{wang_automatic_nodate,
	title = {Automatic {Optimization} of {Sparse} {Tensor} {Algebra} {Programs}},
	abstract = {In this thesis, I attempt to give some guidance on how to automatically optimize programs using a domain-specific-language (DSLs) compiler that exposes a set of scheduling commands. These DSLs have proliferated as of late, including Halide, TACO, Tiramisu and TVM, to name a few. The scheduling commands allow succinct expression of the programmer’s desire to perform certain loop transformations, such as strip-mining, tiling, collapsing and parallelization, which the compiler proceeds to carry out. I explore if we can automatically generate schedules with good performance.},
	language = {en},
	author = {Wang, Ziheng},
}

@inproceedings{robert_shaman_2020,
	address = {Berlin, Heidelberg},
	title = {{SHAMan}: {A} {Flexible} {Framework} for {Auto}-tuning {HPC} {Systems}},
	isbn = {978-3-030-68109-8},
	shorttitle = {{SHAMan}},
	url = {https://doi.org/10.1007/978-3-030-68110-4_10},
	doi = {10.1007/978-3-030-68110-4_10},
	abstract = {Modern computer components, both hardware and software, come with many tunable parameters and their parametrization can have a strong impact on their performance. Auto-tuning methods relying on black-box optimization have delivered good results for finding the optimal parametrization of complex computer systems. In this paper, we present a new optimization framework, called the Smart HPC MANager. It provides an out-of-the-box Web application to perform black-box auto-tuning of computer components running on a distributed system for an application submitted by the user. This framework integrates three state-of-the-art heuristics, as well as resampling strategies to deal with the noise due to resource sharing, and pruning strategies to speed-up the convergence process. We demonstrate a possible use-case of this framework by tuning a software I/O accelerator.},
	urldate = {2023-01-06},
	booktitle = {Modelling, {Analysis}, and {Simulation} of {Computer} and {Telecommunication} {Systems}: 28th {International} {Symposium}, {MASCOTS} 2020, {Nice}, {France}, {November} 17–19, 2020, {Revised} {Selected} {Papers}},
	publisher = {Springer-Verlag},
	author = {Robert, Sophie and Zertal, Soraya and Couvee, Philippe},
	month = nov,
	year = {2020},
	keywords = {Auto-tuning, Black-box optimization, HPC, I/O accelerators, Optimization heuristics},
	pages = {147--158},
}

@misc{tsimpourlas_benchpress_2022,
	title = {{BenchPress}: {A} {Deep} {Active} {Benchmark} {Generator}},
	shorttitle = {{BenchPress}},
	url = {http://arxiv.org/abs/2208.06555},
	abstract = {Finding the right heuristics to optimize code has always been a difficult and mostly manual task for compiler engineers. Today this task is near-impossible as hardware-software complexity has scaled up exponentially. Predictive models for compilers have recently emerged which require little human effort but are far better than humans in finding near optimal heuristics. As any machine learning technique, they are only as good as the data they are trained on but there is a severe shortage of code for training compilers. Researchers have tried to remedy this with code generation but their synthetic benchmarks, although thousands, are small, repetitive and poor in features, therefore ineffective. This indicates the shortage is of feature quality more than corpus size. It is more important than ever to develop a directed program generation approach that will produce benchmarks with valuable features for training compiler heuristics.},
	language = {en},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Tsimpourlas, Foivos and Petoumenos, Pavlos and Xu, Min and Cummins, Chris and Hazelwood, Kim and Rajan, Ajitha and Leather, Hugh},
	month = aug,
	year = {2022},
	note = {arXiv:2208.06555 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{adriaensen_automated_2022,
	title = {Automated {Dynamic} {Algorithm} {Configuration}},
	url = {http://arxiv.org/abs/2205.13881},
	doi = {10.48550/arXiv.2205.13881},
	abstract = {The performance of an algorithm often critically depends on its parameter configuration. While a variety of automated algorithm configuration methods have been proposed to relieve users from the tedious and error-prone task of manually tuning parameters, there is still a lot of untapped potential as the learned configuration is static, i.e., parameter settings remain fixed throughout the run. However, it has been shown that some algorithm parameters are best adjusted dynamically during execution, e.g., to adapt to the current part of the optimization landscape. Thus far, this is most commonly achieved through hand-crafted heuristics. A promising recent alternative is to automatically learn such dynamic parameter adaptation policies from data. In this article, we give the first comprehensive account of this new field of automated dynamic algorithm configuration (DAC), present a series of recent advances, and provide a solid foundation for future research in this field. Specifically, we (i) situate DAC in the broader historical context of AI research; (ii) formalize DAC as a computational problem; (iii) identify the methods used in prior-art to tackle this problem; (iv) conduct empirical case studies for using DAC in evolutionary optimization, AI planning, and machine learning.},
	urldate = {2022-09-22},
	publisher = {arXiv},
	author = {Adriaensen, Steven and Biedenkapp, André and Shala, Gresa and Awad, Noor and Eimer, Theresa and Lindauer, Marius and Hutter, Frank},
	month = may,
	year = {2022},
	note = {arXiv:2205.13881 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7965198},
	urldate = {2022-09-14},
}

@inproceedings{sandha_mango_2020,
	title = {Mango: {A} {Python} {Library} for {Parallel} {Hyperparameter} {Tuning}},
	shorttitle = {Mango},
	doi = {10.1109/ICASSP40776.2020.9054609},
	abstract = {Tuning hyperparameters for machine learning algorithms is a tedious task, one that is typically done manually. To enable automated hyperparameter tuning, recent works have started to use techniques based on Bayesian optimization. However, to practically enable automated tuning for large scale machine learning training pipelines, significant gaps remain in existing libraries, including lack of abstractions, fault tolerance, and flexibility to support scheduling on any distributed computing framework. To address these challenges, we present Mango, a Python library for parallel hyperparameter tuning. Mango enables the use of any distributed scheduling framework, implements intelligent parallel search strategies, and provides rich abstractions for defining complex hyperparameter search spaces that are compatible with scikit-learn. Mango is comparable in performance to Hyperopt [1], another widely used library. Mango is available open-source [2] and is currently used in production at Arm Research to provide state-of-art hyperparameter tuning capabilities.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Sandha, Sandeep Singh and Aggarwal, Mohit and Fedorov, Igor and Srivastava, Mani},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {Bayesian optimization, Hyperparameter tuning, Libraries, Optimization, Parallel Optimizer, Processor scheduling, Python library, Signal processing algorithms, Task analysis, Training, Tuning},
	pages = {3987--3991},
}

@misc{noauthor_mango_nodate,
	title = {Mango: {A} {Python} {Library} for {Parallel} {Hyperparameter} {Tuning} {\textbar} {IEEE} {Conference} {Publication} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/abstract/document/9054609},
	urldate = {2022-09-14},
}

@article{cummins_compilergym_2021,
	title = {{CompilerGym}: {Robust}, {Performant} {Compiler} {Optimization} {Environments} for {AI} {Research}},
	shorttitle = {{CompilerGym}},
	url = {http://arxiv.org/abs/2109.08267},
	abstract = {Interest in applying Artiﬁcial Intelligence (AI) techniques to compiler optimizations is increasing rapidly, but compiler research has a high entry barrier. Unlike in other domains, compiler and AI researchers do not have access to the datasets and frameworks that enable fast iteration and development of ideas, and getting started requires a signiﬁcant engineering investment. What is needed is an easy, reusable experimental infrastructure for real world compiler optimization tasks that can serve as a common benchmark for comparing techniques, and as a platform to accelerate progress in the ﬁeld. We introduce CompilerGym1, a set of environments for real world compiler optimization tasks, and a toolkit for exposing new optimization tasks to compiler researchers. CompilerGym enables anyone to experiment on production compiler optimization problems through an easy-to-use package, regardless of their experience with compilers. We build upon the popular OpenAI Gym interface enabling researchers to interact with compilers using Python and a familiar API.},
	language = {en},
	urldate = {2022-03-28},
	journal = {arXiv:2109.08267 [cs]},
	author = {Cummins, Chris and Wasti, Bram and Guo, Jiadong and Cui, Brandon and Ansel, Jason and Gomez, Sahir and Jain, Somya and Liu, Jia and Teytaud, Olivier and Steiner, Benoit and Tian, Yuandong and Leather, Hugh},
	month = dec,
	year = {2021},
	note = {arXiv: 2109.08267},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Performance, Computer Science - Programming Languages},
}

@article{yadav_distal_2022,
	title = {{DISTAL}: {The} {Distributed} {Tensor} {Algebra} {Compiler}},
	shorttitle = {{DISTAL}},
	url = {http://arxiv.org/abs/2203.08069},
	doi = {10.1145/3519939.3523437},
	abstract = {We introduce DISTAL, a compiler for dense tensor algebra that targets modern distributed and heterogeneous systems. DISTAL lets users independently describe how tensors and computation map onto target machines through separate format and scheduling languages. The combination of choices for data and computation distribution creates a large design space that includes many algorithms from both the past (e.g., Cannon’s algorithm) and the present (e.g., COSMA). DISTAL compiles a tensor algebra domain specific language to a distributed task-based runtime system and supports nodes with multi-core CPUs and multiple GPUs. Code generated by DISTAL is competitive with optimized codes for matrix multiply on 256 nodes of the Lassen supercomputer and outperforms existing systems by between 1.8x to 3.7x (with a 45.7x outlier) on higher order tensor operations.},
	language = {en},
	urldate = {2022-03-25},
	journal = {arXiv:2203.08069 [cs]},
	author = {Yadav, Rohan and Aiken, Alex and Kjolstad, Fredrik},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.08069},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Programming Languages},
}

@article{ahrens_asymptotic_2021,
	title = {An {Asymptotic} {Cost} {Model} for {Autoscheduling} {Sparse} {Tensor} {Programs}},
	url = {http://arxiv.org/abs/2111.14947},
	abstract = {While loop reordering and fusion can make big impacts on the constant-factor performance of dense tensor programs, the effects on sparse tensor programs are asymptotic, often leading to orders of magnitude performance differences in practice. Sparse tensors also introduce a choice of compressed storage formats that can have asymptotic effects. Research into sparse tensor compilers has led to simplified languages that express these tradeoffs, but the user is expected to provide a schedule that makes the decisions. This is challenging because schedulers must anticipate the interaction between sparse formats, loop structure, potential sparsity patterns, and the compiler itself. Automating this decision making process stands to finally make sparse tensor compilers accessible to end users. We present, to the best of our knowledge, the first automatic asymptotic scheduler for sparse tensor programs. We provide an approach to abstractly represent the asymptotic cost of schedules and to choose between them. We narrow down the search space to a manageably small "Pareto frontier" of asymptotically undominated kernels. We test our approach by compiling these kernels with the TACO sparse tensor compiler and comparing them with those generated with the default TACO schedules. Our results show that our approach reduces the scheduling space by orders of magnitude and that the generated kernels perform asymptotically better than those generated using the default schedules.},
	language = {en},
	urldate = {2022-03-25},
	journal = {arXiv:2111.14947 [cs]},
	author = {Ahrens, Peter and Kjolstad, Fredrik and Amarasinghe, Saman},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.14947},
	keywords = {Computer Science - Mathematical Software, Computer Science - Programming Languages},
}

@article{bik_compiler_2022,
	title = {Compiler {Support} for {Sparse} {Tensor} {Computations} in {MLIR}},
	url = {http://arxiv.org/abs/2202.04305},
	abstract = {Sparse tensors arise in problems in science, engineering, machine learning, and data analytics. Programs that operate on such tensors can exploit sparsity to reduce storage requirements and computational time. Developing and maintaining sparse software by hand, however, is a complex and error-prone task. Therefore, we propose treating sparsity as a property of tensors, not a tedious implementation task, and letting a sparse compiler generate sparse code automatically from a sparsity-agnostic deﬁnition of the computation. This paper discusses integrating this idea into MLIR.},
	language = {en},
	urldate = {2022-03-25},
	journal = {arXiv:2202.04305 [cs]},
	author = {Bik, Aart J. C. and Koanantakool, Penporn and Shpeisman, Tatiana and Vasilache, Nicolas and Zheng, Bixia and Kjolstad, Fredrik},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.04305},
	keywords = {Computer Science - Programming Languages},
}

@article{xu_copy-and-patch_2021,
	title = {Copy-and-patch compilation: a fast compilation algorithm for high-level languages and bytecode},
	volume = {5},
	issn = {2475-1421},
	shorttitle = {Copy-and-patch compilation},
	url = {https://dl.acm.org/doi/10.1145/3485513},
	doi = {10.1145/3485513},
	abstract = {HAORAN XU, Stanford University, USA FREDRIK KJOLSTAD, Stanford University, USA Fast compilation is important when compilation occurs at runtime, such as query compilers in modern database systems and WebAssembly virtual machines in modern browsers. We present copy-and-patch, an extremely fast compilation technique that also produces good quality code. It is capable of lowering both high-level languages and low-level bytecode programs to binary code, by stitching together code from a large library of binary implementation variants. We call these binary implementations stencils because they have holes where missing values must be inserted during code generation. We show how to construct a stencil library and describe the copy-and-patch algorithm that generates optimized binary code. We demonstrate two use cases of copy-and-patch: a compiler for a high-level C-like language intended for metaprogramming and a compiler for WebAssembly. Our high-level language compiler has negligible compilation cost: it produces code from an AST in less time than it takes to construct the AST. We have implemented an SQL database query compiler on top of this metaprogramming system and show that on TPC-H database benchmarks, copy-and-patch generates code two orders of magnitude faster than LLVM -O0 and three orders of magnitude faster than higher optimization levels. The generated code runs an order of magnitude faster than interpretation and 14\% faster than LLVM -O0. Our WebAssembly compiler generates code 4.9×ś6.5× faster than Liftoff, the WebAssembly baseline compiler in Google Chrome. The generated code also outperforms Liftoff’s by 39\%ś63\% on the Coremark and PolyBenchC WebAssembly benchmarks. CCS Concepts: • Software and its engineering → Just-in-time compilers; Domain specific languages.},
	language = {en},
	number = {OOPSLA},
	urldate = {2022-03-25},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Xu, Haoran and Kjolstad, Fredrik},
	month = oct,
	year = {2021},
	pages = {1--30},
}

@article{henry_compilation_2021,
	title = {Compilation of sparse array programming models},
	volume = {5},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3485505},
	doi = {10.1145/3485505},
	abstract = {This paper shows how to compile sparse array programming languages. A sparse array programming language is an array programming language that supports element-wise application, reduction, and broadcasting of arbitrary functions over dense and sparse arrays with any fill value. Such a language has great expressive power and can express sparse and dense linear and tensor algebra, functions over images, exclusion and inclusion filters, and even graph algorithms. Our compiler strategy generalizes prior work in the literature on sparse tensor algebra compilation to support any function applied to sparse arrays, instead of only addition and multiplication. To achieve this, we generalize the notion of sparse iteration spaces beyond intersections and unions. These iteration spaces are automatically derived by considering how algebraic properties annotated onto functions interact with the fill values of the arrays. We then show how to compile these iteration spaces to efficient code. When compared with two widely-used Python sparse array packages, our evaluation shows that we generate built-in sparse array library features with a performance of 1.4× to 53.7× when measured against PyData/Sparse for user-defined functions and between 0.98× and 5.53× when measured against SciPy/Sparse for sparse array slicing. Our technique outperforms PyData/Sparse by 6.58× to 70.3×, and (where applicable) performs between 0.96× and 28.9× that of a dense NumPy implementation, on end-to-end sparse array applications. We also implement graph linear algebra kernels in our system with a performance of between 0.56× and 3.50× compared to that of the hand-optimized SuiteSparse:GraphBLAS library. CCS Concepts: · Software and its engineering → Source code generation; Domain specific languages.},
	language = {en},
	number = {OOPSLA},
	urldate = {2022-03-25},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Henry, Rawn and Hsu, Olivia and Yadav, Rohan and Chou, Stephen and Olukotun, Kunle and Amarasinghe, Saman and Kjolstad, Fredrik},
	month = oct,
	year = {2021},
	pages = {1--29},
}

@article{shi_attempt_2021,
	title = {An {Attempt} to {Generate} {Code} for {Symmetric} {Tensor} {Computations}},
	url = {http://arxiv.org/abs/2110.00186},
	abstract = {This document describes an attempt to develop a compiler-based approach for computations with symmetric tensors. Given a computation and the symmetries of its input tensors, we derive formulas for random access under a storage scheme that eliminates redundancies; construct intermediate representations to describe the loop structure; and translate this information, using the taco tensor algebra compiler, into code. While we achieve a framework for reasoning about a fairly general class of symmetric computations, the resulting code is not performant when the symmetries are misaligned.},
	language = {en},
	urldate = {2022-03-25},
	journal = {arXiv:2110.00186 [cs]},
	author = {Shi, Jessica and Chou, Stephen and Kjolstad, Fredrik and Amarasinghe, Saman},
	month = sep,
	year = {2021},
	note = {arXiv: 2110.00186},
	keywords = {Computer Science - Mathematical Software, Computer Science - Programming Languages},
}

@article{liu_compiling_2021,
	title = {Compiling {Halide} {Programs} to {Push}-{Memory} {Accelerators}},
	url = {http://arxiv.org/abs/2105.12858},
	abstract = {Image processing and machine learning applications beneﬁt tremendously from hardware acceleration, but existing compilers target either FPGAs, which sacriﬁce power and performance for ﬂexible hardware, or ASICs, which rapidly become obsolete as applications change. Programmable domain-speciﬁc accelerators have emerged as a promising middle-ground between these two extremes, but such architectures have traditionally been difﬁcult compiler targets.},
	language = {en},
	urldate = {2022-03-25},
	journal = {arXiv:2105.12858 [cs]},
	author = {Liu, Qiaoyi and Huff, Dillon and Setter, Jeff and Strange, Maxwell and Feng, Kathleen and Sreedhar, Kavya and Wang, Ziheng and Zhang, Keyi and Horowitz, Mark and Raina, Priyanka and Kjolstad, Fredrik},
	month = may,
	year = {2021},
	note = {arXiv: 2105.12858},
	keywords = {Computer Science - Hardware Architecture},
}

@inproceedings{willemsen_bayesian_2021,
	title = {Bayesian {Optimization} for auto-tuning {GPU} kernels},
	doi = {10.1109/PMBS54543.2021.00017},
	abstract = {Finding optimal parameter configurations for tunable GPU kernels is a non-trivial exercise for large search spaces, even when automated. This poses an optimization task on a nonconvex search space, using an expensive to evaluate function with unknown derivative. These characteristics make a good candidate for Bayesian Optimization, which has not been applied to this problem before. However, the application of Bayesian Optimization to this problem is challenging. We demonstrate how to deal with the rough, discrete, constrained search spaces, containing invalid configurations. We introduce a novel contextual variance exploration factor, as well as new acquisition functions with improved scalability, combined with an informed acquisition function selection mechanism. By comparing the performance of our Bayesian Optimization implementation on various test cases to the existing search strategies in Kernel Tuner, as well as other Bayesian Optimization implementations, we demonstrate that our search strategies generalize well and consistently outperform other search strategies by a wide margin.},
	booktitle = {2021 {International} {Workshop} on {Performance} {Modeling}, {Benchmarking} and {Simulation} of {High} {Performance} {Computer} {Systems} ({PMBS})},
	author = {Willemsen, Floris-Jan and van Nieuwpoort, Rob and van Werkhoven, Ben},
	month = nov,
	year = {2021},
	keywords = {Bayes methods, Bayesian Optimization, Computational modeling, Convolution, GPU Computing, Graphics processing units, Optimization, Scalability, Search problems, Tuners, auto-tuning, machine learning},
	pages = {106--117},
}

@article{bjertnes_autotuning_2021,
	title = {Autotuning {CUDA}: {Applying} {NLP} {Techniques} to {LS}-{CAT}},
	copyright = {Copyright (c) 2021},
	issn = {1892-0721},
	shorttitle = {Autotuning {CUDA}},
	url = {https://ojs.bibsys.no/index.php/NIK/article/view/917},
	abstract = {The abstract relation between hardware parameters and program performance makes setting program parameters a difficult task.\&nbsp;Without autotuning, software can miss low-level optimizations, resulting in lower performance. Traditionally, time-consuming trial and error\&nbsp;search methods have been the staple of autotuning. Applying Natural\&nbsp;language processing (NLP) based machine learning (ML) methods to\&nbsp;source code as a means to perform autotuning-oriented tasks is a growing\&nbsp;topic. Earlier research has, with success, performed a range of different\&nbsp;autotuning tasks using multiple source code languages. However, most\&nbsp;of the source code data is CPU-oriented, with very little GPU code.\&nbsp;The LS-CAT (Large-Scale CUDA AutoTuning) dataset [BTE21] uses\&nbsp;CUDA GPU-based kernels and generates a dataset to perform thread-coarsening.\&nbsp;This paper implements several custom NLP-ML pipelines to evaluate\&nbsp;ML-based thread-coarsening using the LS-CAT dataset, and a custom\&nbsp;scoring function to ?nd the performance impact for any choice. Several model con?gurations were able to beat both random choice, 0.9400,\&nbsp;and only selecting the largest thread-block (1024), 0.9437. Finally, the\&nbsp;best model achieves a score of 0.9483, giving an average performance\&nbsp;increase and speedup of 0.49 percent over the largest thread-block. Implementing self-attention mechanisms proved to counteract overfitting,\&nbsp;while a multi-label based learning task outperformed other approaches.\&nbsp;Compared to previous datasets [Cum+17], the LS-CAT dataset's higher\&nbsp;thread-coarsening precision gives a more precise evaluation of the model's\&nbsp;performance. The inst2vec embedding used in earlier works was unable\&nbsp;to correctly parse the CUDA LLVM IR tokens, resulting in high data\&nbsp;loss. Approaches to addressing this, and other ideas for future work, are\&nbsp;also included.},
	language = {en},
	number = {1},
	urldate = {2022-02-13},
	journal = {Norsk IKT-konferanse for forskning og utdanning},
	author = {Bjertnes, Lars and Tørring, Jacob O. and Elster, Anne C.},
	month = nov,
	year = {2021},
	note = {Number: 1},
	pages = {72--85},
}

@article{torring_optimization_2020,
	title = {Optimization {Techniques} for {Auto}-tuning {GPUs}: {An} {Empirical} {Study} of {Image}-based {GPU} {Kernels}},
	shorttitle = {Optimization {Techniques} for {Auto}-tuning {GPUs}},
	url = {https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/2777877},
	abstract = {With advances in computing systems, especially with the complexities of modern accelerators, such as GPUs (Graphic Processing Units), it has become challenging to write efficient programs that take advantage of the available parallelism and complex memory systems. Added to that, these architectures change yearly, if not more often, further challenging how to get the desired performance out of these systems. For this reason, empirical auto-tuning has gained interest during the past decades.

While new auto-tuning algorithms are regularly presented and published, comparing these auto-tuning algorithms is a deceptively difficult task. In this thesis, a thorough empirical study of state-of-the-art auto-tuning algorithms is conducted to compare them on a range of sample sizes, benchmarks and architectures. Our work exposes that the ideal auto-tuning algorithm is heavily dependent on the sample size, and shows that no single state-of-the-art algorithm outperforms the rest for all sample sizes.

We compare Random Search (RS), Random Forest Regression (RF), AUMA, Genetic Algorithms (GA), Bayesian Optimization with Gaussian Processes (BO GP) and Bayesian Optimization with Tree-Parzen Estimators (BO TPE). Our results show that BO GP outperforms the other algorithms in most scenarios with sample sizes from 25 to 100, while GA usually outperforms the others for sample sizes 200 and beyond. For very low sample sizes like 25, BO TPE and AUMA sometimes outperforms BO GP for specific benchmarks and architectures. Our survey shows that very low samples sizes like 25, are severely underrepresented in previous research. We therefore encourage more research into algorithms for very low sample sizes.

Our study found that in general, the most advantage over RS can be gained in the lower range of sample sizes, with sample sizes from 25 to 100. As the sample sizes grows larger, the gap between model-based approaches and RS decreases. The optimal choice of a model-based algorithm for samples sizes 25, 50, 100, 200 and 400 have mean improvements over RS of 1.16x, 1.18x, 1.14x, 1.09x and 1.08x across all benchmarks and architectures.

This experimental methodology exposes a greater need to examine a wide range of factors with empirical rigorousness when presenting novel auto-tuning algorithms. We publish our results together with our full code base and all our raw data for the research community to perform further analysis. We also provide directions for important future work.},
	language = {en},
	urldate = {2022-02-13},
	author = {Tørring, Jacob Odgård},
	year = {2020},
	note = {Accepted: 2021-09-15T16:16:51Z
Publisher: NTNU},
}

@article{svela_evaluating_2020,
	title = {Evaluating multi-core graph algorithm frameworks},
	copyright = {Copyright (c) 2020 Norsk IKT-konferanse for forskning og utdanning},
	issn = {1892-0721},
	url = {https://ojs.bibsys.no/index.php/NIK/article/view/829},
	abstract = {Multi-core and GPU-based systems offer unprecedented computational power. They are, however, challenging to utilize effectively, especially when processing irregular data such as graphs. Graphs are of great interest, as they are now used to model geographic-, social- andneural networks. Several interesting programming frameworks for graph processing have therefore been developed these past few years.
In this work, we highlight the strengths and weaknesses of the Galois, GraphBLAST, Gunrock and Ligra graph frameworks through benchmarking their single source shortest path (SSSP) implementations using the SuiteSparse Matrix Collection. Tests were done on an Nvidia DGX2 system, except for Ligra, which only provides a multi-core framework. D-IrGL, built on Galois, also provided a multi-GPU option for SSSP. We also look at program size, documentation and overall ease of use.
High performance generally comes at the price of high complexity. D-IrGL shows its strength on the very largest graphs, where it achieved the best run-time, while Gunrock processed most other large sets the fastest. However, GraphBLAST, with a relatively low-complexity interface, achieves the greatest median throughput across all our test cases. This despite that its SSSP implementation size is only 1/10th of Gunrock, which for our tests has the highest peak throughput and the fastest run-time in most cases. Ligra had less computational resources available, and consequently performed worse in most cases, but it is also a very compact and easy to use framework. Futher analyses and some suggestions for future work are also included.},
	language = {en},
	number = {1},
	urldate = {2021-10-18},
	journal = {Norsk IKT-konferanse for forskning og utdanning},
	author = {Svela, Zawadi},
	month = nov,
	year = {2020},
	note = {Number: 1},
}

@misc{noauthor_no_nodate,
	title = {No. 1 (2020): {NIK} {Norsk} informatikkonferanse {\textbar} {Norsk} {IKT}-konferanse for forskning og utdanning},
	url = {https://ojs.bibsys.no/index.php/NIK/issue/view/NIK},
	urldate = {2021-10-18},
}

@article{makarova_overfitting_2021,
	title = {Overfitting in {Bayesian} {Optimization}: an empirical study and early-stopping solution},
	shorttitle = {Overfitting in {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/2104.08166},
	abstract = {Bayesian Optimization (BO) is a successful methodology to tune the hyperparameters of machine learning models. The user defines a metric of interest, such as the validation error, and BO finds the optimal hyperparameters that minimize it. However, the metric improvements on the validation set may not translate to improvements on the test set, especially when tuning models trained on small datasets. In other words, unlike conventional wisdom dictates, BO can overfit. While cross-validation can mitigate this, it comes with an increased computational cost. In this paper, we carry out the first systematic investigation of overfitting in BO and demonstrate that this issue is a serious, yet often overlooked concern in practice. We propose the first problem-adaptive and interpretable criterion to early stop BO, reducing overfitting while mitigating the cost of cross-validation. Experimental results on real-world hyperparameter optimization tasks show that our approach can substantially reduce compute time with little to no loss of test accuracy, demonstrating a practical advantage over existing techniques.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:2104.08166 [cs, stat]},
	author = {Makarova, Anastasia and Shen, Huibin and Perrone, Valerio and Klein, Aaron and Faddoul, Jean Baptiste and Krause, Andreas and Seeger, Matthias and Archambeau, Cedric},
	month = jun,
	year = {2021},
	note = {arXiv: 2104.08166},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{meyer_latency_2008,
	title = {Latency {Impact} on {Spin}-{Lock} {Algorithms} for {Modern} {Shared} {Memory} {Multiprocessors}},
	doi = {10.1109/CISIS.2008.132},
	abstract = {In 2006, John Mellor-Crummey and Michael Scott received the Dijkstra Prize in distributed computing for their 1991 paper on algorithms for scalable synchronization on shared memory multiprocessors, which included a novel spin-lock algorithm (a.k.a. MCS spin-lock) that carefully distributes spin locations in memory to lessen the impact of bandwidth limitations on spin algorithms. Their empirical work and architectural suggestions have had a major impact on how the field has viewed spin-locks. Motivated by emerging architectures with an increasing number of cores, we present an empirical study on recent shared memory architectures, including IBM P5+ and SGI ccNUMA systems. Our results show that latency will have a much greater impact on performance than bandwidth on these and future architectures with many cores and private caches. Several test cases and a tabular overview of our results are included.},
	booktitle = {2008 {International} {Conference} on {Complex}, {Intelligent} and {Software} {Intensive} {Systems}},
	author = {Meyer, Jan Christian and Elster, Anne Cathrine},
	month = mar,
	year = {2008},
	keywords = {Bandwidth, Data structures, Magnetic cores, Memory management, Program processors, Software algorithms, Timing},
	pages = {786--791},
}

@article{bergstra_random_nodate,
	title = {Random {Search} for {Hyper}-{Parameter} {Optimization}},
	abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efﬁcient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to conﬁgure neural networks and deep belief networks. Compared with neural networks conﬁgured by a pure grid search, we ﬁnd that random search over the same domain is able to ﬁnd models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search ﬁnds better models by effectively searching a larger, less promising conﬁguration space. Compared with deep belief networks conﬁgured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional conﬁguration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for conﬁguring algorithms for new data sets. Our analysis casts some light on why recent “High Throughput” methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
	language = {en},
	author = {Bergstra, James and Bengio, Yoshua},
	pages = {25},
}

@incollection{coello_sequential_2011,
	address = {Berlin, Heidelberg},
	title = {Sequential {Model}-{Based} {Optimization} for {General} {Algorithm} {Configuration}},
	volume = {6683},
	isbn = {978-3-642-25565-6 978-3-642-25566-3},
	url = {http://link.springer.com/10.1007/978-3-642-25566-3_40},
	abstract = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modiﬁed to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm conﬁguration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the ﬁrst time to general algorithm conﬁguration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm conﬁguration procedure by optimizing a local search and a tree search solver for the propositional satisﬁability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best conﬁguration approach.},
	language = {en},
	urldate = {2021-09-10},
	booktitle = {Learning and {Intelligent} {Optimization}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
	editor = {Coello, Carlos A. Coello},
	year = {2011},
	doi = {10.1007/978-3-642-25566-3_40},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {507--523},
}

@article{elster_european_2021,
	title = {The {European} {Factor}: {From} {ARM} to {Atos}},
	volume = {23},
	issn = {1558-366X},
	shorttitle = {The {European} {Factor}},
	doi = {10.1109/MCSE.2020.3044070},
	abstract = {Reports on ARM, a 30-year-old U.K.-based semiconductor Intellectual Property (IP) company that started as Advanced RISC Machines Ltd., a joint venture that included former U.K. company Acorn Computers, Apple Computers (now Apple Inc.),1 and VLSI Technology (a US company later bought by Phillips for USD 1 billion and now part of the Philips spin-off NXP Semiconductors). ARM was bought in 2016 by the Japanese conglomerate and investing company Softbank Group, which also has a main stake in US Sprint and also has significant T-Mobile stock via the merger of Sprint and T-Mobile that was completed in April 2020. In September 2020, NVIDIA announced that it is buying ARM,2 but it may take time to complete since it is running into some licensing issues regarding its subsidiary ARM-China. Unlike Intel, AMD, and Freescale, ARM does not produce any computer chips, but licenses its architecture designs and product ecosystem to other vendors.},
	number = {1},
	journal = {Computing in Science Engineering},
	author = {Elster, Anne C.},
	month = jan,
	year = {2021},
	note = {Conference Name: Computing in Science Engineering},
	keywords = {Companies, Licenses},
	pages = {102--105},
}

@misc{noauthor_european_nodate,
	title = {The {European} {Factor}: {From} {ARM} to {Atos} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/9364770},
	urldate = {2021-09-09},
}

@article{rasch_efficient_2021-1,
	title = {Efficient {Auto}-{Tuning} of {Parallel} {Programs} with {Interdependent} {Tuning} {Parameters} via {Auto}-{Tuning} {Framework} ({ATF})},
	volume = {18},
	issn = {1544-3566},
	url = {https://doi.org/10.1145/3427093},
	doi = {10.1145/3427093},
	abstract = {Auto-tuning is a popular approach to program optimization: it automatically finds good configurations of a program’s so-called tuning parameters whose values are crucial for achieving high performance for a particular parallel architecture and characteristics of input/output data. We present three new contributions of the Auto-Tuning Framework (ATF), which enable a key advantage in general-purpose auto-tuning: efficiently optimizing programs whose tuning parameters have interdependencies among them. We make the following contributions to the three main phases of general-purpose auto-tuning: (1) ATF generates the search space of interdependent tuning parameters with high performance by efficiently exploiting parameter constraints; (2) ATF stores such search spaces efficiently in memory, based on a novel chain-of-trees search space structure; (3) ATF explores these search spaces faster, by employing a multi-dimensional search strategy on its chain-of-trees search space representation. Our experiments demonstrate that, compared to the state-of-the-art, general-purpose auto-tuning frameworks, ATF substantially improves generating, storing, and exploring the search space of interdependent tuning parameters, thereby enabling an efficient overall auto-tuning process for important applications from popular domains, including stencil computations, linear algebra routines, quantum chemistry computations, and data mining algorithms.},
	number = {1},
	urldate = {2021-06-14},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Rasch, Ari and Schulze, Richard and Steuwer, Michel and Gorlatch, Sergei},
	month = jan,
	year = {2021},
	keywords = {Auto-tuning, interdependent tuning parameters, parallel programs},
	pages = {1:1--1:26},
}

@misc{ieee_ieee_nodate,
	title = {{IEEE} {Reference} {Guide}},
	shorttitle = {{IEEE} {Reference} {Guide}},
	url = {https://ieeeauthorcenter.ieee.org/wp-content/uploads/IEEE-Reference-Guide.pdf},
	language = {en},
	author = {IEEE},
}

@misc{ieee_ieee_nodate-1,
	title = {{IEEE} {Editorial} {Style} {Manual}},
	shorttitle = {{IEEE} {Editorial} {Style} {Manual}},
	url = {https://www.ieee.org/content/dam/ieee-org/ieee/web/org/conferences/style_references_manual.pdf},
	abstract = {This style manual provides editorial guidelines for IEEE Transactions, Journals, and Letters. For spelling reference, IEEE Publications uses Webster’s College Dictionary, 4th Edition. For guidance on grammar and usage not included in this manual, please consult The Chicago Manual of Style, published by the University of Chicago Press.},
	urldate = {2021-06-01},
	publisher = {IEEE},
	author = {IEEE},
}

@inproceedings{hsu_arrow_2018,
	title = {Arrow: {Low}-{Level} {Augmented} {Bayesian} {Optimization} for {Finding} the {Best} {Cloud} {VM}},
	shorttitle = {Arrow},
	doi = {10.1109/ICDCS.2018.00070},
	abstract = {With the advent of big data applications, which tend to have longer execution time, choosing the right cloud VM has significant performance and economic implications. For example, in our large-scale empirical study of 107 different workloads on three popular big data systems, we found that a wrong choice can lead to a 20 times slowdown or an increase in cost by 10 times. Bayesian optimization is a technique for optimizing expensive (black-box) functions. Previous work has only used instance-level information (such as core counts and memory size) which is not sufficient to represent the search space. In this work, we discover that this may lead to the fragility problem-either incurs high search cost or finds only the sub-optimal solution. The central insight of this paper is to use low-level performance information to augment the process of Bayesian Optimization. Our novel low-level augmented Bayesian Optimization is rarely worse than current practices and often performs much better (in 46 of 107 cases). Further, it significantly reduces the search cost in nearly half of our case studies. Based on this work, we conclude that it is often insufficient to use general-purpose off-the-shelf methods for configuring cloud instances without augmenting those methods with essential systems knowledge such as CPU utilization, working memory size and I/O wait time.},
	booktitle = {2018 {IEEE} 38th {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	author = {Hsu, Chin-Jung and Nair, Vivek and Freeh, Vincent W. and Menzies, Tim},
	month = jul,
	year = {2018},
	note = {ISSN: 2575-8411},
	keywords = {Bayes methods, Bayesian Optimization, Benchmark testing, Cloud Computing, Cloud computing, Clustering algorithms, Low-level Metrics, Machine Learning, Machine learning, Optimization, Performance Optimization, Sparks},
	pages = {660--670},
}

@article{alipourfard_cherrypick_nodate,
	title = {{CherryPick}: {Adaptively} {Unearthing} the {Best} {Cloud} {Configurations} for {Big} {Data} {Analytics}},
	abstract = {Picking the right cloud conﬁguration for recurring big data analytics jobs running in clouds is hard, because there can be tens of possible VM instance types and even more cluster sizes to pick from. Choosing poorly can signiﬁcantly degrade performance and increase the cost to run a job by 2-3x on average, and as much as 12x in the worst-case. However, it is challenging to automatically identify the best conﬁguration for a broad spectrum of applications and cloud conﬁgurations with low search cost. CherryPick is a system that leverages Bayesian Optimization to build performance models for various applications, and the models are just accurate enough to distinguish the best or close-to-the-best conﬁguration from the rest with only a few test runs. Our experiments on ﬁve analytic applications in AWS EC2 show that CherryPick has a 45-90\% chance to ﬁnd optimal conﬁgurations, otherwise near-optimal, saving up to 75\% search cost compared to existing solutions.},
	language = {en},
	author = {Alipourfard, Omid and Yu, Minlan},
	pages = {15},
}

@inproceedings{hutter_sequential_2011,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Sequential {Model}-{Based} {Optimization} for {General} {Algorithm} {Configuration}},
	isbn = {978-3-642-25566-3},
	doi = {10.1007/978-3-642-25566-3_40},
	abstract = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach.},
	language = {en},
	booktitle = {Learning and {Intelligent} {Optimization}},
	publisher = {Springer},
	author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
	editor = {Coello, Carlos A. Coello},
	year = {2011},
	keywords = {General Algorithm, Local Search, Mixed Integer Programming, Numerical Parameter, Random Forest},
	pages = {507--523},
}

@article{gelbart_bayesian_2014,
	title = {Bayesian {Optimization} with {Unknown} {Constraints}},
	url = {http://arxiv.org/abs/1403.5607},
	abstract = {Recent work on Bayesian optimization has shown its eﬀectiveness in global optimization of diﬃcult black-box objective functions. Many real-world optimization problems of interest also have constraints which are unknown a priori. In this paper, we study Bayesian optimization for constrained problems in the general case that noise may be present in the constraint functions, and the objective and constraints may be evaluated independently. We provide motivating practical examples, and present a general framework to solve such problems. We demonstrate the eﬀectiveness of our approach on optimizing the performance of online latent Dirichlet allocation subject to topic sparsity constraints, tuning a neural network given test-time memory constraints, and optimizing Hamiltonian Monte Carlo to achieve maximal eﬀectiveness in a ﬁxed time, subject to passing standard convergence diagnostics.},
	language = {en},
	urldate = {2021-05-30},
	journal = {arXiv:1403.5607 [cs, stat]},
	author = {Gelbart, Michael A. and Snoek, Jasper and Adams, Ryan P.},
	month = mar,
	year = {2014},
	note = {arXiv: 1403.5607},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{curtsinger_stabilizer_nodate,
	title = {{STABILIZER}: {Statistically} {Sound} {Performance} {Evaluation}},
	abstract = {Researchers and software developers require effective performance evaluation. Researchers must evaluate optimizations or measure overhead. Software developers use automatic performance regression tests to discover when changes improve or degrade performance. The standard methodology is to compare execution times before and after applying changes. Unfortunately, modern architectural features make this approach unsound. Statistically sound evaluation requires multiple samples to test whether one can or cannot (with high conﬁdence) reject the null hypothesis that results are the same before and after. However, caches and branch predictors make performance dependent on machine-speciﬁc parameters and the exact layout of code, stack frames, and heap objects. A single binary constitutes just one sample from the space of program layouts, regardless of the number of runs. Since compiler optimizations and code changes also alter layout, it is currently impossible to distinguish the impact of an optimization from that of its layout effects.},
	language = {en},
	author = {Curtsinger, Charlie and Berger, Emery D},
	pages = {10},
}

@article{chen_robust_nodate,
	title = {Robust benchmarking in noisy environments},
	abstract = {We propose a benchmarking strategy that is robust in the presence of timer error, OS jitter and other environmental ﬂuctuations, and is insensitive to the highly nonideal statistics produced by timing measurements. We construct a model that explains how these strongly nonideal statistics can arise from environmental ﬂuctuations, and also justiﬁes our proposed strategy. We implement this strategy in the BenchmarkTools Julia package, where it is used in production continuous integration (CI) pipelines for developing the Julia language and its ecosystem.},
	language = {en},
	author = {Chen, Jiahao and Revels, Jarrett and Edelman, Alan},
	pages = {7},
}

@misc{noauthor_intel_nodate,
	title = {Intel {P}-{State} driver},
	url = {https://www.kernel.org/doc/Documentation/cpu-freq/intel-pstate.txt},
	urldate = {2021-05-27},
}

@article{torring_autotuning_2021,
	title = {Autotuning {Benchmarking} {Techniques}: {A} {Roofline} {Model} {Case} {Study}},
	shorttitle = {Autotuning {Benchmarking} {Techniques}},
	url = {http://arxiv.org/abs/2103.08716},
	abstract = {Peak performance metrics published by vendors often do not correspond to what can be achieved in practice. It is therefore of great interest to do extensive benchmarking on core applications and library routines. Since DGEMM is one of the most used in compute-intensive numerical codes, it is typically highly vendor optimized and of great interest for empirical benchmarks. In this paper we show how to build a novel tool that autotunes the benchmarking process for the Roofline model. Our novel approach can efficiently and reliably find optimal configurations for any target hardware. Results of our tool on a range of hardware architectures and comparisons to theoretical peak performance are included. Our tool autotunes the benchmarks for the target architecture by deciding the optimal parameters through state space reductions and exhaustive search. Our core idea includes calculating the confidence interval using the variance and mean and comparing it against the current optimum solution. We can then terminate the evaluation process early if the confidence interval's maximum is lower than the current optimum solution. This dynamic approach yields a search time improvement of up to 116.33x for the DGEMM benchmarking process compared to a traditional fixed sample-size methodology. Our tool produces the same benchmarking result with an error of less than 2\% for each of the optimization techniques we apply, while providing a great reduction in search time. We compare these results against hand-tuned benchmarking parameters. Results from the memory-intensive TRIAD benchmark, and some ideas for future directions are also included.},
	urldate = {2021-05-26},
	journal = {arXiv:2103.08716 [cs]},
	author = {Tørring, Jacob Odgård and Meyer, Jan Christian and Elster, Anne C.},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.08716},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@article{bjertnes_ls-cat_2021,
	title = {{LS}-{CAT}: {A} {Large}-{Scale} {CUDA} {AutoTuning} {Dataset}},
	shorttitle = {{LS}-{CAT}},
	url = {http://arxiv.org/abs/2103.14409},
	abstract = {The effectiveness of Machine Learning (ML) methods depend on access to large suitable datasets. In this article, we present how we build the LS-CAT (Large-Scale CUDA AutoTuning) dataset sourced from GitHub for the purpose of training NLP-based ML models. Our dataset includes 19 683 CUDA kernels focused on linear algebra. In addition to the CUDA codes, our LS-CAT dataset contains 5 028 536 associated runtimes, with different combinations of kernels, block sizes and matrix sizes. The runtime are GPU benchmarks on both Nvidia GTX 980 and Nvidia T4 systems. This information creates a foundation upon which NLP-based models can find correlations between source-code features and optimal choice of thread block sizes. There are several results that can be drawn out of our LS-CAT database. E.g., our experimental results show that an optimal choice in thread block size can gain an average of 6\% for the average case. We thus also analyze how much performance increase can be achieved in general, finding that in 10\% of the cases more than 20\% performance increase can be achieved by using the optimal block. A description of current and future work is also included.},
	urldate = {2021-05-26},
	journal = {arXiv:2103.14409 [cs]},
	author = {Bjertnes, Lars and Tørring, Jacob O. and Elster, Anne C.},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.14409},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing},
}

@article{fiser_best_nodate,
	title = {{BEST} {PRACTICES} {WHEN} {BENCHMARKING} {CUDA} {APPLICATIONS}},
	language = {en},
	author = {Fiser, Bill and Jodłowski, Sebastian},
	pages = {55},
}

@misc{noauthor_nvidia_nodate,
	type = {concept},
	title = {{NVIDIA} {CUDA} {Toolkit} {Release} {Notes}},
	url = {http://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html},
	abstract = {The Release Notes for the CUDA Toolkit.},
	language = {en-us},
	urldate = {2021-05-25},
	note = {Archive Location: Release Notes},
}

@article{fekry_tuneful_2020,
	title = {Tuneful: {An} {Online} {Significance}-{Aware} {Configuration} {Tuner} for {Big} {Data} {Analytics}},
	shorttitle = {Tuneful},
	url = {http://arxiv.org/abs/2001.08002},
	abstract = {Distributed analytics engines such as Spark are a common choice for processing extremely large datasets. However, finding good configurations for these systems remains challenging, with each workload potentially requiring a different setup to run optimally. Using suboptimal configurations incurs significant extra runtime costs. \%Furthermore, Spark and similar platforms are gaining traction within data-scientists communities where awareness of such issues is relatively low. We propose Tuneful, an approach that efficiently tunes the configuration of in-memory cluster computing systems. Tuneful combines incremental Sensitivity Analysis and Bayesian optimization to identify near-optimal configurations from a high-dimensional search space, using a small number of executions. This setup allows the tuning to be done online, without any previous training. Our experimental results show that Tuneful reduces the search time for finding close-to-optimal configurations by 62{\textbackslash}\% (at the median) when compared to existing state-of-the-art techniques. This means that the amortization of the tuning cost happens significantly faster, enabling practical tuning for new classes of workloads.},
	urldate = {2021-05-13},
	journal = {arXiv:2001.08002 [cs, eess]},
	author = {Fekry, Ayat and Carata, Lucian and Pasquier, Thomas and Rice, Andrew and Hopper, Andy},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.08002},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Electrical Engineering and Systems Science - Systems and Control},
}

@article{filipovic_searching_2021,
	title = {Searching {CUDA} code autotuning spaces with hardware performance counters: data from benchmarks running on various {GPU} architectures},
	shorttitle = {Searching {CUDA} code autotuning spaces with hardware performance counters},
	url = {http://arxiv.org/abs/2102.05299},
	abstract = {We have developed several autotuning benchmarks in CUDA that take into account performance-relevant source-code parameters and reach near peak-performance on various GPU architectures. We have used them during the development and evaluation of a novel search method for tuning space proposed in [1]. With our framework Kernel Tuning Toolkit, freely available at Github, we measured computation times and hardware performance counters on several GPUs for the complete tuning spaces of ﬁve benchmarks. These data, which we provide here, might beneﬁt research of search algorithms for the tuning spaces of GPU codes or research of relation between applied code optimization, hardware performance counters, and GPU kernels’ performance.},
	language = {en},
	urldate = {2021-05-04},
	journal = {arXiv:2102.05299 [cs]},
	author = {Filipovič, Jiří and Hozzová, Jana and Nezarat, Amin and Oľha, Jaroslav and Petrovič, Filip},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.05299},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing},
}

@article{filipovic_using_2021,
	title = {Using hardware performance counters to speed up autotuning convergence on {GPUs}},
	url = {http://arxiv.org/abs/2102.05297},
	abstract = {Nowadays, GPU accelerators are commonly used to speed up general-purpose computing tasks on a variety of hardware. However, due to the diversity of GPU architectures and processed data, optimization of codes for a particular type of hardware and speciﬁc data characteristics can be extremely challenging. The autotuning of performance-relevant sourcecode parameters allows for automatic optimization of applications and keeps their performance portable. Although the autotuning process typically results in code speed-up, searching the tuning space can bring unacceptable overhead if (i) the tuning space is vast and full of poorly-performing implementations, or (ii) the autotuning process has to be repeated frequently because of changes in processed data or migration to diﬀerent hardware.},
	language = {en},
	urldate = {2021-05-04},
	journal = {arXiv:2102.05297 [cs]},
	author = {Filipovič, Jiří and Hozzová, Jana and Nezarat, Amin and Oľha, Jaroslav and Petrovič, Filip},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.05297},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Computer Science - Performance},
}

@article{rasch_efficient_2021-2,
	title = {Efficient {Auto}-{Tuning} of {Parallel} {Programs} with {Interdependent} {Tuning} {Parameters} via {Auto}-{Tuning} {Framework} ({ATF})},
	volume = {18},
	issn = {1544-3566},
	url = {https://doi.org/10.1145/3427093},
	doi = {10.1145/3427093},
	abstract = {Auto-tuning is a popular approach to program optimization: it automatically finds good configurations of a program’s so-called tuning parameters whose values are crucial for achieving high performance for a particular parallel architecture and characteristics of input/output data. We present three new contributions of the Auto-Tuning Framework (ATF), which enable a key advantage in general-purpose auto-tuning: efficiently optimizing programs whose tuning parameters have interdependencies among them. We make the following contributions to the three main phases of general-purpose auto-tuning: (1) ATF generates the search space of interdependent tuning parameters with high performance by efficiently exploiting parameter constraints; (2) ATF stores such search spaces efficiently in memory, based on a novel chain-of-trees search space structure; (3) ATF explores these search spaces faster, by employing a multi-dimensional search strategy on its chain-of-trees search space representation. Our experiments demonstrate that, compared to the state-of-the-art, general-purpose auto-tuning frameworks, ATF substantially improves generating, storing, and exploring the search space of interdependent tuning parameters, thereby enabling an efficient overall auto-tuning process for important applications from popular domains, including stencil computations, linear algebra routines, quantum chemistry computations, and data mining algorithms.},
	number = {1},
	urldate = {2021-05-04},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Rasch, Ari and Schulze, Richard and Steuwer, Michel and Gorlatch, Sergei},
	month = jan,
	year = {2021},
	keywords = {Auto-tuning, interdependent tuning parameters, parallel programs},
	pages = {1:1--1:26},
}

@inproceedings{lawson_towards_2020,
	title = {Towards automated kernel selection in machine learning systems: {A} {SYCL} case study},
	shorttitle = {Towards automated kernel selection in machine learning systems},
	doi = {10.1109/IPDPSW50202.2020.00086},
	abstract = {Automated tuning of compute kernels is a popular area of research, mainly focused on finding optimal kernel parameters for a problem with fixed input sizes. This approach is good for deploying machine learning models, where the network topology is constant, but machine learning research often involves changing network topologies and hyperparameters. Traditional kernel auto-tuning has limited impact in this case; a more general selection of kernels is required for libraries to accelerate machine learning research. In this paper we present initial results using machine learning to select kernels in a case study deploying high performance SYCL kernels in libraries that target a range of heterogeneous devices from desktop GPUs to embedded accelerators. The techniques investigated apply more generally and could similarly be integrated with other heterogeneous programming systems. By combining auto-tuning and machine learning these kernel selection processes can be deployed with little developer effort to achieve high performance on new hardware.},
	booktitle = {2020 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	author = {Lawson, John},
	month = may,
	year = {2020},
	keywords = {Auto-tuning, Computational modeling, Decision trees, GPGPU, Kernel, Libraries, Machine learning, Principal component analysis, SYCL, Tuning},
	pages = {475--478},
}

@article{hertel_sherpa_2020,
	title = {Sherpa: {Robust} hyperparameter optimization for machine learning},
	volume = {12},
	issn = {2352-7110},
	shorttitle = {Sherpa},
	url = {https://www.sciencedirect.com/science/article/pii/S2352711020303046},
	doi = {10.1016/j.softx.2020.100591},
	abstract = {Sherpa is a hyperparameter optimization library for machine learning models. It is specifically designed for problems with computationally expensive, iterative function evaluations, such as the hyperparameter tuning of deep neural networks. With Sherpa, scientists can quickly optimize hyperparameters using a variety of powerful and interchangeable algorithms. Sherpa can be run on either a single machine or in parallel on a cluster. Finally, an interactive dashboard enables users to view the progress of models as they are trained, cancel trials, and explore which hyperparameter combinations are working best. Sherpa empowers machine learning practitioners by automating the more tedious aspects of model tuning. Its source code and documentation are available at https://github.com/sherpa-ai/sherpa.},
	language = {en},
	urldate = {2021-05-04},
	journal = {SoftwareX},
	author = {Hertel, Lars and Collado, Julian and Sadowski, Peter and Ott, Jordan and Baldi, Pierre},
	month = jul,
	year = {2020},
	keywords = {Deep neural networks, Hyperparameter optimization, Machine learning},
	pages = {100591},
}

@article{bartz-beielstein_benchmarking_2020,
	title = {Benchmarking in {Optimization}: {Best} {Practice} and {Open} {Issues}},
	shorttitle = {Benchmarking in {Optimization}},
	url = {http://arxiv.org/abs/2007.03488},
	abstract = {This survey compiles ideas and recommendations from more than a dozen researchers with different backgrounds and from different institutes around the world. Promoting best practice in benchmarking is its main goal. The article discusses eight essential topics in benchmarking: clearly stated goals, well-specified problems, suitable algorithms, adequate performance measures, thoughtful analysis, effective and efficient designs, comprehensible presentations, and guaranteed reproducibility. The final goal is to provide well-accepted guidelines (rules) that might be useful for authors and reviewers. As benchmarking in optimization is an active and evolving field of research this manuscript is meant to co-evolve over time by means of periodic updates.},
	urldate = {2021-05-04},
	journal = {arXiv:2007.03488 [cs, math, stat]},
	author = {Bartz-Beielstein, Thomas and Doerr, Carola and Berg, Daan van den and Bossek, Jakob and Chandrasekaran, Sowmya and Eftimov, Tome and Fischbach, Andreas and Kerschke, Pascal and La Cava, William and Lopez-Ibanez, Manuel and Malan, Katherine M. and Moore, Jason H. and Naujoks, Boris and Orzechowski, Patryk and Volz, Vanessa and Wagner, Markus and Weise, Thomas},
	month = dec,
	year = {2020},
	note = {arXiv: 2007.03488},
	keywords = {68W50, A.1, B.8.0, Computer Science - Neural and Evolutionary Computing, Computer Science - Performance, G.1.6, G.4, I.2.8, Mathematics - Optimization and Control, Statistics - Applications},
}

@article{liaw_tune_2018,
	title = {Tune: {A} {Research} {Platform} for {Distributed} {Model} {Selection} and {Training}},
	shorttitle = {Tune},
	url = {http://arxiv.org/abs/1807.05118},
	abstract = {Modern machine learning algorithms are increasingly computationally demanding, requiring specialized hardware and distributed computation to achieve high performance in a reasonable time frame. Many hyperparameter search algorithms have been proposed for improving the efficiency of model selection, however their adaptation to the distributed compute environment is often ad-hoc. We propose Tune, a unified framework for model selection and training that provides a narrow-waist interface between training scripts and search algorithms. We show that this interface meets the requirements for a broad range of hyperparameter search algorithms, allows straightforward scaling of search to large clusters, and simplifies algorithm implementation. We demonstrate the implementation of several state-of-the-art hyperparameter search algorithms in Tune. Tune is available at http://ray.readthedocs.io/en/latest/tune.html.},
	urldate = {2021-05-04},
	journal = {arXiv:1807.05118 [cs, stat]},
	author = {Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E. and Stoica, Ion},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.05118},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{snoek_practical_2012,
	title = {Practical {Bayesian} {Optimization} of {Machine} {Learning} {Algorithms}},
	url = {http://arxiv.org/abs/1206.2944},
	abstract = {Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
	urldate = {2021-05-04},
	journal = {arXiv:1206.2944 [cs, stat]},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
	month = aug,
	year = {2012},
	note = {arXiv: 1206.2944},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{authors_gpyopt_2016,
	title = {{GPyOpt}: {A} {Bayesian} {Optimization} framework in python},
	url = {http://github.com/SheffieldML/GPyOpt},
	author = {authors, The GPyOpt},
	year = {2016},
}

@misc{noauthor_notitle_nodate,
}

@article{balandat_botorch_nodate,
	title = {{BOTORCH}: {A} {Framework} for {Efﬁcient} {Monte}-{Carlo} {Bayesian} {Optimization}},
	abstract = {Bayesian optimization provides sample-efﬁcient global optimization for a broad range of applications, including automatic machine learning, engineering, physics, and experimental design. We introduce BOTORCH, a modern programming framework for Bayesian optimization that combines Monte-Carlo (MC) acquisition functions, a novel sample average approximation optimization approach, autodifferentiation, and variance reduction techniques. BOTORCH’s modular design facilitates ﬂexible speciﬁcation and optimization of probabilistic models written in PyTorch, simplifying implementation of new acquisition functions. Our approach is backed by novel theoretical convergence results and made practical by a distinctive algorithmic foundation that leverages fast predictive distributions, hardware acceleration, and deterministic optimization. We also propose a novel “one-shot” formulation of the Knowledge Gradient, enabled by a combination of our theoretical and software contributions. In experiments, we demonstrate the improved sample efﬁciency of BOTORCH relative to other popular libraries.},
	language = {en},
	author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan},
	pages = {15},
}

@inproceedings{bergstra_hyperopt_2013,
	address = {Austin, Texas},
	title = {Hyperopt: {A} {Python} {Library} for {Optimizing} the {Hyperparameters} of {Machine} {Learning} {Algorithms}},
	shorttitle = {Hyperopt},
	url = {https://conference.scipy.org/proceedings/scipy2013/bergstra_hyperopt.html},
	doi = {10.25080/Majora-8b375195-003},
	abstract = {Sequential model-based optimization (also known as Bayesian op- These conﬁguration variables are called hyperparameters. For timization) is one of the most efﬁcient methods (per function evaluation) of function minimization. This efﬁciency makes it appropriate for optimizing the hyperparameters of machine learning algorithms that are slow to train. The Hyperopt library provides algorithms and parallelization infrastructure for performing hyperparameter optimization (model selection) in Python. This paper presents an introductory tutorial on the usage of the Hyperopt library, including the description of search spaces, minimization (in serial and parallel), and the analysis of the results collected in the course of minimization. The paper closes T with some discussion of ongoing and future work.},
	language = {en},
	urldate = {2021-05-04},
	author = {Bergstra, James and Yamins, Dan and Cox, David},
	year = {2013},
	pages = {13--19},
}

@article{knutsen_enhancing_nodate,
	title = {Enhancing {Software} {Portability} with {Hardware} {Parametrized} {Autotuning}},
	language = {en},
	author = {Knutsen, Henrik Holenbakken},
	pages = {80},
}

@inproceedings{cummins_synthesizing_2017,
	title = {Synthesizing benchmarks for predictive modeling},
	doi = {10.1109/CGO.2017.7863731},
	abstract = {Predictive modeling using machine learning is an effective method for building compiler heuristics, but there is a shortage of benchmarks. Typical machine learning experiments outside of the compilation field train over thousands or millions of examples. In machine learning for compilers, however, there are typically only a few dozen common benchmarks available. This limits the quality of learned models, as they have very sparse training data for what are often high-dimensional feature spaces. What is needed is a way to generate an unbounded number of training programs that finely cover the feature space. At the same time the generated programs must be similar to the types of programs that human developers actually write, otherwise the learning will target the wrong parts of the feature space. We mine open source repositories for program fragments and apply deep learning techniques to automatically construct models for how humans write programs. We sample these models to generate an unbounded number of runnable training programs. The quality of the programs is such that even human developers struggle to distinguish our generated programs from hand-written code. We use our generator for OpenCL programs, CLgen, to automatically synthesize thousands of programs and show that learning over these improves the performance of a state of the art predictive model by 1.27x. In addition, the fine covering of the feature space automatically exposes weaknesses in the feature design which are invisible with the sparse training examples from existing benchmark suites. Correcting these weaknesses further increases performance by 4.30x.},
	booktitle = {2017 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization} ({CGO})},
	author = {Cummins, Chris and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},
	month = feb,
	year = {2017},
	keywords = {Benchmark testing, Benchmarking, Data models, Deep Learning, GPUs, Grammar, Machine learning, OpenCL, Predictive models, Semantics, Synthetic program generation, Training},
	pages = {86--99},
}

@inproceedings{goens_case_2019,
	address = {Phoenix, AZ, USA},
	title = {A case study on machine learning for synthesizing benchmarks},
	isbn = {978-1-4503-6719-6},
	url = {http://dl.acm.org/citation.cfm?doid=3315508.3329976},
	doi = {10.1145/3315508.3329976},
	abstract = {Good benchmarks are hard to find because they require a substantial effort to keep them representative for the constantly changing challenges of a particular field. Synthetic benchmarks are a common approach to deal with this, and methods from machine learning are natural candidates for synthetic benchmark generation. In this paper we investigate the usefulness of machine learning in the prominent CLgen benchmark generator. We re-evaluate CLgen by comparing the benchmarks generated by the model with the raw data used to train it. This re-evaluation indicates that, for the use case considered, machine learning did not yield additional benefit over a simpler method using the raw data. We investigate the reasons for this and provide further insights into the challenges the problem could pose for potential future generators.},
	language = {en},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 3rd {ACM} {SIGPLAN} {International} {Workshop} on {Machine} {Learning} and {Programming} {Languages} - {MAPL} 2019},
	publisher = {ACM Press},
	author = {Goens, Andrés and Brauckmann, Alexander and Ertel, Sebastian and Cummins, Chris and Leather, Hugh and Castrillon, Jeronimo},
	year = {2019},
	pages = {38--46},
}

@inproceedings{leather_machine_2020,
	title = {Machine {Learning} in {Compilers}: {Past}, {Present} and {Future}},
	shorttitle = {Machine {Learning} in {Compilers}},
	doi = {10.1109/FDL50818.2020.9232934},
	abstract = {Writing optimising compilers is difficult. The range of programs that may be presented to the compiler is huge and the systems on which they run are complex, heterogeneous, non-deterministic, and constantly changing. The space of possible optimisations is also vast, making it very hard for compiler writers to design heuristics that take all of these considerations into account. As a result, many compiler optimisations are out of date or poorly tuned. Near the turn of the century it was first shown how compilers could be made to automatically search the optimisation space, producing programs far better optimised than previously possible, and without the need for compiler writers to worry about architecture or program specifics. The searches, though, were slow, so in the years that followed, machine learning was developed to learn heuristics from the results of previous searches so that thereafter the search could be avoided and much of the benefit could be gained in a single shot. In this paper we will give a retrospective of machine learning in compiler optimisation from its earliest inception, through some of the works that set themselves apart, to today's deep learning, finishing with our vision of the field's future.},
	booktitle = {2020 {Forum} for {Specification} and {Design} {Languages} ({FDL})},
	author = {Leather, Hugh and Cummins, Chris},
	month = sep,
	year = {2020},
	note = {ISSN: 1636-9874},
	keywords = {Deep learning, History, Neural networks, Optimization, Predictive models, Search problems, Tools, compilers, machine learning},
	pages = {1--8},
}

@article{herodotou_survey_2020,
	title = {A {Survey} on {Automatic} {Parameter} {Tuning} for {Big} {Data} {Processing} {Systems}},
	volume = {53},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3381027},
	doi = {10.1145/3381027},
	abstract = {Big data processing systems (e.g., Hadoop, Spark, Storm) contain a vast number of configuration parameters controlling parallelism, I/O behavior, memory settings, and compression. Improper parameter settings can cause significant performance degradation and stability issues. However, regular users and even expert administrators grapple with understanding and tuning them to achieve good performance. We investigate existing approaches on parameter tuning for both batch and stream data processing systems and classify them into six categories: rule-based, cost modeling, simulation-based, experiment-driven, machine learning, and adaptive tuning. We summarize the pros and cons of each approach and raise some open research problems for automatic parameter tuning.},
	number = {2},
	urldate = {2021-04-28},
	journal = {ACM Computing Surveys},
	author = {Herodotou, Herodotos and Chen, Yuxing and Lu, Jiaheng},
	month = apr,
	year = {2020},
	keywords = {MapReduce, Parameter tuning, Spark, Storm, self-tuning, stream},
	pages = {43:1--43:37},
}

@book{naono_software_2010,
	address = {New York},
	title = {Software {Automatic} {Tuning}: {From} {Concepts} to {State}-of-the-{Art} {Results}},
	isbn = {978-1-4419-6934-7},
	shorttitle = {Software {Automatic} {Tuning}},
	url = {https://www.springer.com/gp/book/9781441969347},
	abstract = {Software Automatic Tuning: From Concepts to State-of-the-Art Results Ken Naono Keita Teranishi John Cavazos Reiji Suda It is well known that carefully tuned programs run much faster than ones consisting of simply written code, and sometimes the difference of speed is more 100X. To make things more complex, well-tuned code for some machines performs badly on others. "Automatic Performance Tuning" is a technology paradigm that enables software to tune itself to its environments so that it performs well on any computer, even on computers unknown to the programmer. This book summarizes the research efforts to date and state of the art of automatic performance tuning. Software developers and researchers in the area of scientific and technical computing, optimized compilers, high performance systems software, and low-power computing will find this book to be an invaluable reference to this powerful new paradigm. •Presents the first English collaboration on the powerful, new software paradigm of Automatic Performance Tuning; •Offers a comprehensive survey of fundamental concepts and state-of-the-art results from the field; •Enables programmers to create software that will tune itself to its environments so that it performs well on any computer.},
	language = {en},
	urldate = {2021-04-28},
	publisher = {Springer-Verlag},
	editor = {Naono, Ken and Teranishi, Keita and Cavazos, John and Suda, Reiji},
	year = {2010},
	doi = {10.1007/978-1-4419-6935-4},
}

@book{naono_software_2010-1,
	title = {Software {Automatic} {Tuning}: {From} {Concepts} to {State}-of-the-{Art} {Results}},
	isbn = {978-1-4419-6935-4},
	shorttitle = {Software {Automatic} {Tuning}},
	abstract = {Automatic Performance Tuning is a new software paradigm which enables software to be high performance in any computing environment. Its methodologies have been developed over the past decade, and it is now rapidly growing in terms of its scope and applicability, as well as in its scientific knowledge and technological methods. Software developers and researchers in the area of scientific and technical computing, high performance database systems, optimized compilers, high performance systems software, and low-power computing will find this book to be an invaluable reference to this powerful new paradigm.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Naono, Ken and Teranishi, Keita and Cavazos, John and Suda, Reiji},
	month = sep,
	year = {2010},
	note = {Google-Books-ID: cmD5O83hS0QC},
	keywords = {Computers / CAD-CAM, Technology \& Engineering / Electrical, Technology \& Engineering / Electronics / Circuits / General},
}

@article{huang_survey_2020,
	title = {A {Survey} of {Automatic} {Parameter} {Tuning} {Methods} for {Metaheuristics}},
	volume = {24},
	issn = {1941-0026},
	doi = {10.1109/TEVC.2019.2921598},
	abstract = {Parameter tuning, that is, to find appropriate parameter settings (or configurations) of algorithms so that their performance is optimized, is an important task in the development and application of metaheuristics. Automating this task, i.e., developing algorithmic procedure to address parameter tuning task, is highly desired and has attracted significant attention from the researchers and practitioners. During last two decades, many automatic parameter tuning approaches have been proposed. This paper presents a comprehensive survey of automatic parameter tuning methods for metaheuristics. A new classification (or taxonomy) of automatic parameter tuning methods is introduced according to the structure of tuning methods. The existing automatic parameter tuning approaches are consequently classified into three categories: 1) simple generate-evaluate methods; 2) iterative generate-evaluate methods; and 3) high-level generate-evaluate methods. Then, these three categories of tuning methods are reviewed in sequence. In addition to the description of each tuning method, its main strengths and weaknesses are discussed, which is helpful for new researchers or practitioners to select appropriate tuning methods to use. Furthermore, some challenges and directions of this field are pointed out for further research.},
	number = {2},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Huang, Changwu and Li, Yuanxiang and Yao, Xin},
	month = apr,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Evolutionary Computation},
	keywords = {Automatic parameter tuning, Computer science, Heuristic algorithms, Measurement, Optimization, Systematics, Task analysis, Tuning, metaheuristics, parameter setting, parameter tuning},
	pages = {201--216},
}

@article{ashouri_survey_2018,
	title = {A {Survey} on {Compiler} {Autotuning} using {Machine} {Learning}},
	volume = {51},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3197978},
	doi = {10.1145/3197978},
	abstract = {Since the mid-1990s, researchers have been trying to use machine-learning-based approaches to solve a number of different compiler optimization problems. These techniques primarily enhance the quality of the obtained results and, more importantly, make it feasible to tackle two main compiler optimization problems: optimization selection (choosing which optimizations to apply) and phase-ordering (choosing the order of applying optimizations). The compiler optimization space continues to grow due to the advancement of applications, increasing number of compiler optimizations, and new target architectures. Generic optimization passes in compilers cannot fully leverage newly introduced optimizations and, therefore, cannot keep up with the pace of increasing options. This survey summarizes and classifies the recent advances in using machine learning for the compiler optimization field, particularly on the two major problems of (1) selecting the best optimizations, and (2) the phase-ordering of optimizations. The survey highlights the approaches taken so far, the obtained results, the fine-grain classification among different approaches, and finally, the influential papers of the field.},
	number = {5},
	urldate = {2021-04-28},
	journal = {ACM Computing Surveys},
	author = {Ashouri, Amir H. and Killian, William and Cavazos, John and Palermo, Gianluca and Silvano, Cristina},
	month = sep,
	year = {2018},
	keywords = {Autotuning, compilers, machine learning, optimizations, phase ordering},
	pages = {96:1--96:42},
}

@inproceedings{malakar_benchmarking_2018,
	title = {Benchmarking {Machine} {Learning} {Methods} for {Performance} {Modeling} of {Scientific} {Applications}},
	doi = {10.1109/PMBS.2018.8641686},
	abstract = {Performance modeling is an important and active area of research in high-performance computing (HPC). It helps in better job scheduling and also improves overall performance of coupled applications. Sufficiently rich analytical models are challenging to develop, however, because of interactions between different node components, network topologies, job interference, and application complexity. When analytical performance models become restrictive because of application dynamics and/or multicomponent interactions, machine-learning-based performance models can be helpful. While machine learning (ML) methods do not require underlying system or application knowledge, they are efficient in learning the unknown interactions of the application and system parameters empirically using application runs. We present a benchmark study in which we evaluate eleven machine learning methods for modeling the performance of four representative scientific applications that are irregular and with skewed domain configurations on four leadership-class HPC platforms. We assess the impact of feature engineering, size of training set, modern hardware platforms, transfer learning, extrapolation on the prediction accuracy, and training and inference times. We find that bagging, boosting, and deep neural network ML methods are promising approaches with median R2 values greater than 0.95 and these methods do not require feature engineering. We demonstrate that cross-platform performance prediction can be improved significantly using transfer learning with deep neural networks.},
	booktitle = {2018 {IEEE}/{ACM} {Performance} {Modeling}, {Benchmarking} and {Simulation} of {High} {Performance} {Computer} {Systems} ({PMBS})},
	author = {Malakar, Preeti and Balaprakash, Prasanna and Vishwanath, Venkatram and Morozov, Vitali and Kumaran, Kalyan},
	month = nov,
	year = {2018},
	keywords = {Adaptation models, Analytical models, Benchmark testing, Computational modeling, Machine learning, Neural networks, Predictive models, benchmarking, machine learning, performance modeling, transfer learning},
	pages = {33--44},
}

@article{sid-lakhdar_multitask_2019,
	title = {Multitask and {Transfer} {Learning} for {Autotuning} {Exascale} {Applications}},
	url = {http://arxiv.org/abs/1908.05792},
	abstract = {Multitask learning and transfer learning have proven to be useful in the field of machine learning when additional knowledge is available to help a prediction task. We aim at deriving methods following these paradigms for use in autotuning, where the goal is to find the optimal performance parameters of an application treated as a black-box function. We show comparative results with state-of-the-art autotuning techniques. For instance, we observe an average \$1.5x\$ improvement of the application runtime compared to the OpenTuner and HpBandSter autotuners. We explain how our approaches can be more suitable than some state-of-the-art autotuners for the tuning of any application in general and of expensive exascale applications in particular.},
	urldate = {2021-04-28},
	journal = {arXiv:1908.05792 [cs, stat]},
	author = {Sid-Lakhdar, Wissam M. and Aznaveh, Mohsen Mahmoudi and Li, Xiaoye S. and Demmel, James W.},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.05792},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wang_efficient_2020,
	title = {Efficient {Performance} {Estimation} and {Work}-{Group} {Size} {Pruning} for {OpenCL} {Kernels} on {GPUs}},
	volume = {31},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2019.2958343},
	abstract = {Graphic Processing Units (GPUs) play a vital role in state-of-the-art high-performance scientific computing realm and research work towards its performance analysis is crucial but nontrivial. Extant GPU performance models are far from practical use, while fine-grained GPU simulation requires a considerably large time cost. Moreover, massive amounts of designs with various program inputs and parameter settings pose a challenge for efficient performance estimation and tuning of parallel GPU applications. To this end, this article presents a hybrid framework for the efficient performance estimation and work-group size pruning of OpenCL workloads on GPUs. The framework contains a static module used to extract the kernel execution trace from the high-level source code and a dynamical module used to mimic the kernel execution flow to estimate the runtime performance. For the design space pruning, an extra analysis is performed to filter out the redundant work-group sizes with duplicated execution traces and inferior pipelines. The proposed framework does not require any program runs to estimate the performance and find the optimal or near-optimal designs. Experiments on four Commercial Off-The-Shelf (COTS) Nvidia GPUs show that the framework can predict the runtime performance with an average error of 17.04 percent and reduce the program design space by an average of 78.47 percent.},
	number = {5},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Wang, Xiebing and Qian, Xuehai and Knoll, Alois and Huang, Kai},
	month = may,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {Analytical models, Estimation, GPU, Graphics processing units, Hardware, Kernel, Measurement, OpenCL, Runtime, performance estimation, performance tuning, work-group size},
	pages = {1089--1106},
}

@article{yu_efficient_2020,
	title = {Efficient and {Portable} {Workgroup} {Size} {Tuning}},
	volume = {31},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2019.2937295},
	abstract = {The performance of an OpenCL program is strongly influenced by both hardware and software attributes. To achieve superior performance, developers may leverage automatic performance tuning techniques to determine the optimal parameters on the target device. Although existing approaches have shown promising tuning results in their target scenarios, other requirements such as efficiency, portability, and usability should also be considered because of the rapid growth of heterogeneous computing applications and platforms. In this paper, we re-examine the workgroup size tuning problem and propose a novel approach to meet the aforementioned requirements. We abstract the architectural details into a set of hardware parameters so that the proposed approach can be applied without the presence of target devices, which makes it more accessible to developers. The proposed approach is evaluated on 20 OpenCL kernels and six devices, including both CPUs and GPUs. Experimental results demonstrate that, with negligible overhead, our approach filters out 88.6 percent of the possible workgroup sizes on average. Among all the workgroup size candidates, the bestand worst-performing candidates can achieve average performance of 95.5 and 92.1 percent, respectively, compared with the optimal workgroup size.},
	number = {2},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Yu, Chia-Lin and Tsao, Shiao-Li},
	month = feb,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {Computational modeling, Graphics processing units, Hardware, Indexes, Kernel, OpenCL, Performance evaluation, Tuning, automatic performance tuning, microbenchmarking, workgroup size selection},
	pages = {455--469},
}

@inproceedings{ansel_opentuner_2014,
	address = {New York, NY, USA},
	series = {{PACT} '14},
	title = {{OpenTuner}: an extensible framework for program autotuning},
	isbn = {978-1-4503-2809-8},
	shorttitle = {{OpenTuner}},
	url = {https://doi.org/10.1145/2628071.2628092},
	doi = {10.1145/2628071.2628092},
	abstract = {Program autotuning has been shown to achieve better or more portable performance in a number of domains. However, autotuners themselves are rarely portable between projects, for a number of reasons: using a domain-informed search space representation is critical to achieving good results; search spaces can be intractably large and require advanced machine learning techniques; and the landscape of search spaces can vary greatly between different problems, sometimes requiring domain specific search techniques to explore efficiently. This paper introduces OpenTuner, a new open source framework for building domain-specific multi-objective program autotuners. OpenTuner supports fully-customizable configuration representations, an extensible technique representation to allow for domain-specific techniques, and an easy to use interface for communicating with the program to be autotuned. A key capability inside OpenTuner is the use of ensembles of disparate search techniques simultaneously; techniques that perform well will dynamically be allocated a larger proportion of tests. We demonstrate the efficacy and generality of OpenTuner by building autotuners for 7 distinct projects and 16 total benchmarks, showing speedups over prior techniques of these projects of up to 2.8x with little programmer effort.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 23rd international conference on {Parallel} architectures and compilation},
	publisher = {Association for Computing Machinery},
	author = {Ansel, Jason and Kamil, Shoaib and Veeramachaneni, Kalyan and Ragan-Kelley, Jonathan and Bosboom, Jeffrey and O'Reilly, Una-May and Amarasinghe, Saman},
	month = aug,
	year = {2014},
	keywords = {autotuner, optimization},
	pages = {303--316},
}

@misc{noauthor_opentuner_nodate,
	title = {{OpenTuner} {\textbar} {Proceedings} of the 23rd international conference on {Parallel} architectures and compilation},
	url = {https://dl.acm.org/doi/abs/10.1145/2628071.2628092},
	urldate = {2021-04-28},
}

@inproceedings{luszczek_search_2016,
	title = {Search {Space} {Generation} and {Pruning} {System} for {Autotuners}},
	doi = {10.1109/IPDPSW.2016.197},
	abstract = {This work tackles two simultaneous challenges faced by autotuners: the ease of describing a complex, multidimensional search space, and the speed of evaluating that space, while applying a multitude of pruning constraints. This article presents a declarative notation for describing a search space and a translation system for conversion to a standard C code for fast and multithreaded, as necessary, evaluation. The notation is Python-based and thus simple in syntax and easy to assimilate by the user interested in tuning rather than learning a new programming language. A large number of dimensions and a large number of pruning constraints may be expressed with little effort. The system is discussed in the context of autotuning the canonical matrix multiplication kernel for NVIDIA GPUs, where the search space has 15 dimensions and involves application of 10 complex pruning constrains. The speed of evaluation is compared against generators created using imperative programming style in various scripting and compiled languages.},
	booktitle = {2016 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	author = {Luszczek, Piotr and Gates, Mark and Kurzak, Jakub and Danalis, Anthony and Dongarra, Jack},
	month = may,
	year = {2016},
	keywords = {Context, Graphics processing units, Kernel, Libraries, Standards, Syntactics, Tuning, multidimensional search space enumeration, performance autotuning},
	pages = {1545--1554},
}

@inproceedings{dalibard_boat_2017,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '17},
	title = {{BOAT}: {Building} {Auto}-{Tuners} with {Structured} {Bayesian} {Optimization}},
	isbn = {978-1-4503-4913-0},
	shorttitle = {{BOAT}},
	url = {https://doi.org/10.1145/3038912.3052662},
	doi = {10.1145/3038912.3052662},
	abstract = {Due to their complexity, modern systems expose many configuration parameters which users must tune to maximize performance. Auto-tuning has emerged as an alternative in which a black-box optimizer iteratively evaluates configurations to find efficient ones. Unfortunately, for many systems, such as distributed systems, evaluating performance takes too long and the space of configurations is too large for the optimizer to converge within a reasonable time. We present BOAT, a framework which allows developers to build efficient bespoke auto-tuners for their system, in situations where generic auto-tuners fail. At BOAT's core is structured Bayesian optimization (SBO), a novel extension of the Bayesian optimization algorithm. SBO leverages contextual information provided by system developers, in the form of a probabilistic model of the system's behavior, to make informed decisions about which configurations to evaluate. In a case study, we tune the scheduling of a neural network computation on a heterogeneous cluster. Our auto-tuner converges within ten iterations. The optimized configurations outperform those found by generic auto-tuners in thirty iterations by up to 2X.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 26th {International} {Conference} on {World} {Wide} {Web}},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Dalibard, Valentin and Schaarschmidt, Michael and Yoneki, Eiko},
	month = apr,
	year = {2017},
	keywords = {auto-tuning, bayesian optimization, distributed stochastic gradient descent, distributed systems, neural networks, probabilistic programming},
	pages = {479--488},
}

@inproceedings{thiagarajan_bootstrapping_2018,
	address = {New York, NY, USA},
	series = {{ICS} '18},
	title = {Bootstrapping {Parameter} {Space} {Exploration} for {Fast} {Tuning}},
	isbn = {978-1-4503-5783-8},
	url = {https://doi.org/10.1145/3205289.3205321},
	doi = {10.1145/3205289.3205321},
	abstract = {The task of tuning parameters for optimizing performance or other metrics of interest such as energy, variability, etc. can be resource and time consuming. Presence of a large parameter space makes a comprehensive exploration infeasible. In this paper, we propose a novel bootstrap scheme, called GEIST, for parameter space exploration to find performance-optimizing configurations quickly. Our scheme represents the parameter space as a graph whose connectivity guides information propagation from known configurations. Guided by the predictions of a semi-supervised learning method over the parameter graph, GEIST is able to adaptively sample and find desirable configurations using limited results from experiments. We show the effectiveness of GEIST for selecting application input options, compiler flags, and runtime/system settings for several parallel codes including LULESH, Kripke, Hypre, and OpenAtom.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Thiagarajan, Jayaraman J. and Jain, Nikhil and Anirudh, Rushil and Gimenez, Alfredo and Sridhar, Rahul and Marathe, Aniruddha and Wang, Tao and Emani, Murali and Bhatele, Abhinav and Gamblin, Todd},
	month = jun,
	year = {2018},
	keywords = {autotuning, performance, sampling, semi-supervised learning},
	pages = {385--395},
}

@inproceedings{kloh_towards_2019,
	title = {Towards an {Autonomous} {Framework} for {HPC} {Optimization}: {Using} {Machine} {Learning} for {Energy} and {Performance} {Modeling}},
	copyright = {Copyright (c)},
	shorttitle = {Towards an {Autonomous} {Framework} for {HPC} {Optimization}},
	url = {https://sol.sbc.org.br/index.php/wscad/article/view/8689},
	doi = {10.5753/wscad.2019.8689},
	abstract = {Resumo
					Alcançar altos nı́veis de desempenho com eficiência energética se tornou um grande desafio para a computação cientı́fica de alto desempenho. Para contornar esse desafio, espera-se que os próprios requisitos do problema cientı́fico orientem a orquestração de diferentes mecanismos de economia de energia, a fim de melhorar o equilı́brio entre o consumo de energia e o desempenho das aplicações. Para isso, é proposto o desenvolvimento de um framework autonômico para fazer essa orquestração. Neste trabalho são apresentadas as pesquisas em andamento para este desenvolvimento, mais especificamente, com foco na caracterização das aplicações cientı́ficas e das tarefas de modelagem de desempenho e consumo de energia utilizando técnicas de Aprendizado de Máquina.},
	language = {en},
	urldate = {2021-04-28},
	booktitle = {Anais do {Simpósio} em {Sistemas} {Computacionais} de {Alto} {Desempenho} ({WSCAD})},
	publisher = {SBC},
	author = {Klôh, Vinícius and Gritz, Matheus and Schulze, Bruno and Ferro, Mariza},
	month = nov,
	year = {2019},
	note = {ISSN: 0000-0000},
	pages = {438--445},
}

@inproceedings{kurzak_massively_2019,
	address = {New York, NY, USA},
	series = {{ICPP} 2019},
	title = {Massively {Parallel} {Automated} {Software} {Tuning}},
	isbn = {978-1-4503-6295-5},
	url = {https://doi.org/10.1145/3337821.3337908},
	doi = {10.1145/3337821.3337908},
	abstract = {This article presents an implementation of a distributed autotuning engine developed as part of the Bench-testing OpenN Software Autotuning Infrastructure project. The system is geared towards performance optimization of computational kernels for graphics processing units, and allows for the deployment of vast autotuning sweeps to massively parallel machines. The software implements dynamic work scheduling to distributed-memory resources and takes advantage of multithreading for parallel compilation and dispatches kernel launches to multiple accelerators. This paper lays out the main design principles of the system and discusses the basic mechanics of the initial implementation. Preliminary performance results are presented, encountered challenges are discussed, and the future directions are outlined.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 48th {International} {Conference} on {Parallel} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Kurzak, Jakub and Tsai, Yaohung M. and Gates, Mark and Abdelfattah, Ahmad and Dongarra, Jack},
	month = aug,
	year = {2019},
	keywords = {automated software tuning, graphics processing unit},
	pages = {1--10},
}

@article{liu_autotuning_2019,
	title = {An {Autotuning} {Protocol} to {Rapidly} {Build} {Autotuners}},
	volume = {5},
	issn = {2329-4949},
	url = {https://doi.org/10.1145/3291527},
	doi = {10.1145/3291527},
	abstract = {Automatic performance tuning (Autotuning) is an increasingly critical tuning technique for the high portable performance of Exascale applications. However, constructing an autotuner from scratch remains a challenge, even for domain experts. In this work, we propose a performance tuning and knowledge management suite (PAK) to help rapidly build autotuners. In order to accommodate existing autotuning techniques, we present an autotuning protocol that is composed of an extractor, producer, optimizer, evaluator, and learner. To achieve modularity and reusability, we also define programming interfaces for each protocol component as the fundamental infrastructure, which provides a customizable mechanism to deploy knowledge mining in the performance database. PAK’s usability is demonstrated by studying two important computational kernels: stencil computation and sparse matrix-vector multiplication (SpMV). Our proposed autotuner based on PAK shows comparable performance and higher productivity than traditional autotuners by writing just a few tens of code using our autotuning protocol.},
	number = {2},
	urldate = {2021-04-28},
	journal = {ACM Transactions on Parallel Computing},
	author = {Liu, Junhong and Tan, Guangming and Luo, Yulong and Li, Jiajia and Mo, Zeyao and Sun, Ninghui},
	month = jan,
	year = {2019},
	keywords = {Autotuner, SpMV, knowledge database, protocol, stencil},
	pages = {9:1--9:25},
}

@article{rasch_atf_2019,
	title = {{ATF}: {A} generic directive-based auto-tuning framework},
	volume = {31},
	copyright = {© 2018 John Wiley \& Sons, Ltd.},
	issn = {1532-0634},
	shorttitle = {{ATF}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4423},
	doi = {https://doi.org/10.1002/cpe.4423},
	abstract = {We describe the Auto-Tuning Framework (ATF) — a simple-to-use, generic approach and its implementation, as a framework for automatic program optimization by choosing the most suitable values of program parameters such as the number of parallel threads, tile sizes, etc. ATF combines four major advantages over the state-of-the-art auto-tuning: i) it is generic regarding the programming language, application domain, tuning objective (eg, high performance and/or low energy consumption), and search technique; ii) it can auto-tune a broader class of applications by allowing tuning parameters to be interdependent, eg, when one parameter is divisible by another parameter; iii) it allows tuning parameters to have substantially larger ranges by implementing an optimized search space generation process; and iv) it is arguably simpler to use, eg, the ATF user prepares an application for auto-tuning by annotating its source code with simple tuning directives. We demonstrate ATF's efficacy by comparing it to the state-of-the-art auto-tuning approaches, OpenTuner and CLTune; ATF shows better tuning results with less programmer's effort.},
	language = {en},
	number = {5},
	urldate = {2021-04-28},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Rasch, Ari and Gorlatch, Sergei},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.4423},
	keywords = {CLBlast, CLTune, CUDA, GEMM, OpenCL, OpenTuner, auto-tuning, dependent tuning parameters, many-core, multi-core, multi-objective auto-tuning, tuning parameter constraints},
	pages = {e4423},
}

@article{taheri_tool_2019,
	title = {A {Tool} for {Automatically} {Suggesting} {Source}-{Code} {Optimizations} for {Complex} {GPU} {Kernels}},
	url = {http://arxiv.org/abs/1910.07776},
	abstract = {Future computing systems, from handhelds to supercomputers, will undoubtedly be more parallel and heterogeneous than todays systems to provide more performance and energy efficiency. Thus, GPUs are increasingly being used to accelerate general purpose applications, including applications with data dependent, irregular control flow and memory access patterns. However, the growing complexity, exposed memory hierarchy, incoherence, heterogeneity, and parallelism will make accelerator based systems progressively more difficult to program. In the foreseeable future, the vast majority of programmers will no longer be able to extract additional performance or energy savings from next generation systems be-cause the programming will be too difficult. Automatic performance analysis and optimization recommendation tools have the potential to avert this situation. They embody expert knowledge and make it available to software developers when needed. In this paper, we describe and evaluate such a tool.},
	urldate = {2021-04-28},
	journal = {arXiv:1910.07776 [cs]},
	author = {Taheri, Saeed and Qasem, Apan and Burtscher, Martin},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.07776},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance, Computer Science - Software Engineering},
}

@article{fernandez-fabeiro_automatic_2020,
	title = {An automatic optimizer for heterogeneous devices},
	volume = {106},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X18324294},
	doi = {10.1016/j.future.2020.01.018},
	abstract = {Codes written in a naive way seldom effectively exploit the computing resources, while writing optimized codes is usually a complex task that requires certain levels of expertise. This problem is further increased in the presence of heterogeneous devices, which present more tunable parameters than regular CPUs and high sensitivity to the optimization decisions taken. Furthermore, portability is an added concern given the wide variety of accelerators available. This paper tackles this problem adding an automatic optimizer to a library that already provides an easy and portable way to program heterogeneous devices, the Heterogeneous Programming Library (HPL). Our optimizer takes as input a simple version of a code and then tunes it for the device where it is going to be executed by performing the most usual set of optimizations applicable in heterogeneous devices. These optimizations are parametrized using a set of optimization parameters that need to be tuned for the device. The HPL library has also been equipped with an autotuner that can be used to this purpose. The effectiveness of the autotuner and the optimizer has been tested on several codes and devices. The results show that the combination of the autotuner and the optimizer make the tested codes 16 times faster on average than the original codes written by the programmer.},
	language = {en},
	urldate = {2021-04-28},
	journal = {Future Generation Computer Systems},
	author = {Fernández-Fabeiro, Jorge and Andrade, Diego and Fraguela, Basilio B. and Doallo, Ramón},
	month = may,
	year = {2020},
	keywords = {Heterogeneous systems, OpenCL, Performance portability, Performance tuning},
	pages = {572--584},
}

@article{van_werkhoven_kernel_2019,
	title = {Kernel {Tuner}: {A} search-optimizing {GPU} code auto-tuner},
	volume = {90},
	issn = {0167-739X},
	shorttitle = {Kernel {Tuner}},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X18313359},
	doi = {10.1016/j.future.2018.08.004},
	abstract = {A very common problem in GPU programming is that some combination of thread block dimensions and other code optimization parameters, like tiling or unrolling factors, results in dramatically better performance than other kernel configurations. To obtain highly-efficient kernels it is often required to search vast and discontinuous search spaces that consist of all possible combinations of values for all tunable parameters. This paper presents Kernel Tuner, an easy-to-use tool for testing and auto-tuning OpenCL, CUDA, and C kernels with support for many search optimization algorithms that accelerate the tuning process. This paper introduces the application of many new solvers and global optimization algorithms for auto-tuning GPU applications. We demonstrate that Kernel Tuner can be used in a wide range of application scenarios and drastically decreases the time spent tuning, e.g. tuning a GEMM kernel on AMD Vega Frontier Edition 71.2x faster than brute force search.},
	language = {en},
	urldate = {2021-04-28},
	journal = {Future Generation Computer Systems},
	author = {van Werkhoven, Ben},
	month = jan,
	year = {2019},
	keywords = {Auto-tuning, GPU computing, Parallel programming, Performance optimization, Software development},
	pages = {347--358},
}

@article{cummins_deep_2020,
	title = {Deep {Data} {Flow} {Analysis}},
	url = {http://arxiv.org/abs/2012.01470},
	abstract = {Compiler architects increasingly look to machine learning when building heuristics for compiler optimization. The promise of automatic heuristic design, freeing the compiler engineer from the complex interactions of program, architecture, and other optimizations, is alluring. However, most machine learning methods cannot replicate even the simplest of the abstract interpretations of data ﬂow analysis that are critical to making good optimization decisions. This must change for machine learning to become the dominant technology in compiler heuristics.},
	language = {en},
	urldate = {2021-04-20},
	journal = {arXiv:2012.01470 [cs]},
	author = {Cummins, Chris and Leather, Hugh and Fisches, Zacharias and Ben-Nun, Tal and Hoefler, Torsten and O'Boyle, Michael},
	month = nov,
	year = {2020},
	note = {arXiv: 2012.01470},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@article{leather_machine_nodate,
	title = {Machine {Learning} in {Compilers}:},
	abstract = {Writing optimising compilers is difficult. The range of programs that may be presented to the compiler is huge and the systems on which they run are complex, heterogeneous, non-deterministic, and constantly changing. The space of possible optimisations is also vast, making it very hard for compiler writers to design heuristics that take all of these considerations into account. As a result, many compiler optimisations are out of date or poorly tuned.},
	language = {en},
	author = {Leather, Hugh and Cummins, Chris},
	pages = {8},
}

@article{steiner_value_nodate,
	title = {Value {Learning} for {Throughput} {Optimizationof} {Deep} {Neural} {Networks}},
	abstract = {As the usage of machine learning techniques is becoming ubiquitous, the efﬁcient execution of neural networks is crucial to many applications. Frameworks, such as Halide and TVM, separate the algorithmic representation of the deep learning model from the schedule that determines its implementation. Finding good schedules, however, remains extremely challenging. Auto-tuning methods, which search the space of valid schedules and execute each candidate on the hardware, identify some of the best performing schedules, but the search can take hours, hampering the productivity of deep learning practitioners. What is needed is a method that achieves a similar performance without extensive search, delivering the needed efﬁciency quickly. We model the scheduling process as a sequence of optimization choices, and present a new technique to accurately predict the expected performance of a partial schedule using a LSTM over carefully engineered features that describe each DNN operator and their current scheduling choices. Leveraging these predictions we are able to make these optimization decisions greedily and, without any executions on the target hardware, rapidly identify an efﬁcient schedule.},
	language = {en},
	author = {Steiner, Benoit and Cummins, Chris and He, Horace and Leather, Hugh},
	pages = {12},
}

@article{steiner_value_2020,
	title = {Value {Function} {Based} {Performance} {Optimization} of {Deep} {Learning} {Workloads}},
	url = {http://arxiv.org/abs/2011.14486},
	abstract = {As machine learning techniques become ubiquitous, the efﬁciency of neural network implementations is becoming correspondingly paramount. Frameworks, such as Halide and TVM, separate out the algorithmic representation of the network from the schedule that determines its implementation. Finding good schedules, however, remains extremely challenging. We model this scheduling problem as a sequence of optimization choices, and present a new technique to accurately predict the expected performance of a partial schedule. By leveraging these predictions we can make these optimization decisions greedily and rapidly identify an efﬁcient schedule. This enables us to ﬁnd schedules that improve the throughput of deep neural networks by 2.6× over Halide and 1.5× over TVM. Moreover, our technique is two to three orders of magnitude faster than that of these tools, and completes in seconds instead of hours.},
	language = {en},
	urldate = {2021-04-20},
	journal = {arXiv:2011.14486 [cs]},
	author = {Steiner, Benoit and Cummins, Chris and He, Horace and Leather, Hugh},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.14486},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{kamil_auto-tuning_2010,
	title = {An auto-tuning framework for parallel multicore stencil computations},
	doi = {10.1109/IPDPS.2010.5470421},
	abstract = {Although stencil auto-tuning has shown tremendous potential in effectively utilizing architectural resources, it has hitherto been limited to single kernel instantiations; in addition, the large variety of stencil kernels used in practice makes this computation pattern difficult to assemble into a library. This work presents a stencil auto-tuning framework that significantly advances programmer productivity by automatically converting a straightforward sequential Fortran 95 stencil expression into tuned parallel implementations in Fortran, C, or CUDA, thus allowing performance portability across diverse computer architectures, including the AMD Barcelona, Intel Nehalem, Sun Victoria Falls, and the latest NVIDIA GPUs. Results show that our generalized methodology delivers significant performance gains of up to 22× speedup over the reference serial implementation. Overall we demonstrate that such domain-specific auto-tuners hold enormous promise for architectural efficiency, programmer productivity, performance portability, and algorithmic adaptability on existing and emerging multicore systems.},
	booktitle = {2010 {IEEE} {International} {Symposium} on {Parallel} {Distributed} {Processing} ({IPDPS})},
	author = {Kamil, S. and Chan, C. and Oliker, L. and Shalf, J. and Williams, S.},
	month = apr,
	year = {2010},
	note = {ISSN: 1530-2075},
	keywords = {Assembly, Computer architecture, Concurrent computing, Kernel, Libraries, Multicore processing, Performance gain, Productivity, Programming profession, Sun},
	pages = {1--12},
}

@incollection{hutchison_note_2009,
	address = {Berlin, Heidelberg},
	title = {A {Note} on {Auto}-tuning {GEMM} for {GPUs}},
	volume = {5544},
	isbn = {978-3-642-01969-2 978-3-642-01970-8},
	url = {http://link.springer.com/10.1007/978-3-642-01970-8_89},
	abstract = {The development of high performance dense linear algebra (DLA) critically depends on highly optimized BLAS, and especially on the matrix multiplication routine (GEMM). This is especially true for Graphics Processing Units (GPUs), as evidenced by recently published results on DLA for GPUs that rely on highly optimized GEMM. However, the current best GEMM performance, e.g. of up to 375 GFlop/s in single precision and of up to 75 GFlop/s in double precision arithmetic on NVIDIA’s GTX 280, is diﬃcult to achieve. The development involves extensive GPU knowledge and even backward engineering to understand some undocumented insides about the architecture that have been of key importance in the development. In this paper, we describe some GPU GEMM auto-tuning optimization techniques that allow us to keep up with changing hardware by rapidly reusing, rather than reinventing, the existing ideas. Auto-tuning, as we show in this paper, is a very practical solution where in addition to getting an easy portability, we can often get substantial speedups even on current GPUs (e.g. up to 27\% in certain cases for both single and double precision GEMMs on the GTX 280).},
	language = {en},
	urldate = {2021-04-20},
	booktitle = {Computational {Science} – {ICCS} 2009},
	publisher = {Springer Berlin Heidelberg},
	author = {Li, Yinan and Dongarra, Jack and Tomov, Stanimire},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Allen, Gabrielle and Nabrzyski, Jarosław and Seidel, Edward and van Albada, Geert Dick and Dongarra, Jack and Sloot, Peter M. A.},
	year = {2009},
	doi = {10.1007/978-3-642-01970-8_89},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {884--892},
}

@inproceedings{tiwari_scalable_2009,
	title = {A scalable auto-tuning framework for compiler optimization},
	doi = {10.1109/IPDPS.2009.5161054},
	abstract = {We describe a scalable and general-purpose framework for auto-tuning compiler-generated code. We combine Active Harmony's parallel search backend with the CHiLL compiler transformation framework to generate in parallel a set of alternative implementations of computation kernels and automatically select the one with the best-performing implementation. The resulting system achieves performance of compiler-generated code comparable to the fully automated version of the ATLAS library for the tested kernels. Performance for various kernels is 1.4 to 3.6 times faster than the native Intel compiler without search. Our search algorithm simultaneously evaluates different combinations of compiler optimizations and converges to solutions in only a few tens of search-steps.},
	booktitle = {2009 {IEEE} {International} {Symposium} on {Parallel} {Distributed} {Processing}},
	author = {Tiwari, A. and Chen, C. and Chame, J. and Hall, M. and Hollingsworth, J. K.},
	month = may,
	year = {2009},
	note = {ISSN: 1530-2075},
	keywords = {Application software, Computer architecture, Costs, Kernel, Optimizing compilers, Parallel architectures, Program processors, Programming profession, Software libraries, Tuning},
	pages = {1--12},
}

@article{ding_autotuning_nodate,
	title = {Autotuning {Algorithmic} {Choice} for {Input} {Sensitivity}},
	abstract = {A daunting challenge faced by program performance autotuning is input sensitivity, where the best autotuned conﬁguration may vary with different input sets. This paper presents a novel two-level input learning algorithm to tackle the challenge for an important class of autotuning problems, algorithmic autotuning. The new approach uses a two-level input clustering method to automatically reﬁne input grouping, feature selection, and classiﬁer construction. Its design solves a series of open issues that are particularly essential to algorithmic autotuning, including the enormous optimization space, complex inﬂuence by deep input features, high cost in feature extraction, and variable accuracy of algorithmic choices. Experimental results show that the new solution yields up to a 3x speedup over using a single conﬁguration for all inputs, and a 34x speedup over a traditional one-level method for addressing input sensitivity in program optimizations.},
	language = {en},
	author = {Ding, Yufei and Ansel, Jason and Veeramachaneni, Kalyan and Shen, Xipeng and O’Reilly, Una-May and Amarasinghe, Saman},
	pages = {12},
}

@article{balaprakash_autotuning_2018,
	title = {Autotuning in {High}-{Performance} {Computing} {Applications}},
	volume = {106},
	issn = {1558-2256},
	doi = {10.1109/JPROC.2018.2841200},
	abstract = {Autotuning refers to the automatic generation of a search space of possible implementations of a computation that are evaluated through models and/or empirical measurement to identify the most desirable implementation. Autotuning has the potential to dramatically improve the performance portability of petascale and exascale applications. To date, autotuning has been used primarily in high-performance applications through tunable libraries or previously tuned application code that is integrated directly into the application. This paper draws on the authors' extensive experience applying autotuning to high-performance applications, describing both successes and future challenges. If autotuning is to be widely used in the HPC community, researchers must address the software engineering challenges, manage configuration overheads, and continue to demonstrate significant performance gains and portability across architectures. In particular, tools that configure the application must be integrated into the application build process so that tuning can be reapplied as the application and target architectures evolve.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Balaprakash, P. and Dongarra, J. and Gamblin, T. and Hall, M. and Hollingsworth, J. K. and Norris, B. and Vuduc, R.},
	month = nov,
	year = {2018},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Computer architecture, High performance computing, High-performance computing, Performance evaluation, Programming, Runtime, Tuning, performance tuning programming systems},
	pages = {2068--2083},
}

@incollection{jarvis_roofline_2015,
	address = {Cham},
	title = {Roofline {Model} {Toolkit}: {A} {Practical} {Tool} for {Architectural} and {Program} {Analysis}},
	volume = {8966},
	isbn = {978-3-319-17247-7 978-3-319-17248-4},
	shorttitle = {Roofline {Model} {Toolkit}},
	url = {http://link.springer.com/10.1007/978-3-319-17248-4_7},
	abstract = {We present preliminary results of the Rooﬂine Toolkit for multicore, manycore, and accelerated architectures. This paper focuses on the processor architecture characterization engine, a collection of portable instrumented micro benchmarks implemented with Message Passing Interface (MPI), and OpenMP used to express thread-level parallelism. These benchmarks are specialized to quantify the behavior of different architectural features. Compared to previous work on performance characterization, these microbenchmarks focus on capturing the performance of each level of the memory hierarchy, along with thread-level parallelism, instruction-level parallelism and explicit SIMD parallelism, measured in the context of the compilers and run-time environments. We also measure sustained PCIe throughput with four GPU memory managed mechanisms. By combining results from the architecture characterization with the Rooﬂine model based solely on architectural speciﬁcations, this work oﬀers insights for performance prediction of current and future architectures and their software systems. To that end, we instrument three applications and plot their resultant performance on the corresponding Rooﬂine model when run on a Blue Gene/Q architecture.},
	language = {en},
	urldate = {2021-03-13},
	booktitle = {High {Performance} {Computing} {Systems}. {Performance} {Modeling}, {Benchmarking}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Lo, Yu Jung and Williams, Samuel and Van Straalen, Brian and Ligocki, Terry J. and Cordery, Matthew J. and Wright, Nicholas J. and Hall, Mary W. and Oliker, Leonid},
	editor = {Jarvis, Stephen A. and Wright, Steven A. and Hammond, Simon D.},
	year = {2015},
	doi = {10.1007/978-3-319-17248-4_7},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {129--148},
}

@inproceedings{ritter_learning_2020,
	address = {New Orleans, LA, USA},
	title = {Learning {Cost}-{Effective} {Sampling} {Strategies} for {Empirical} {Performance} {Modeling}},
	isbn = {978-1-72816-876-0},
	url = {https://ieeexplore.ieee.org/document/9139859/},
	doi = {10.1109/IPDPS47924.2020.00095},
	abstract = {Identifying scalability bottlenecks in parallel applications is a vital but also laborious and expensive task. Empirical performance models have proven to be helpful to ﬁnd such limitations, though they require a set of experiments in order to gain valuable insights. Therefore, the experiment design determines the quality and cost of the models. Extra-P is an empirical modeling tool that uses small-scale experiments to assess the scalability of applications. Its current version requires an exponential number of experiments per model parameter. This makes the creation of empirical performance models very expensive, and in some situations even impractical. In this paper, we propose a novel parameter-value selection heuristic, which functions as a guideline for the experiment design, leveraging sparse performance-modeling, a technique that only needs a polynomial number of experiments per model parameter. Using synthetic analysis and data from three different case studies, we show that our solution reduces the average modeling costs by about 85\% while retaining 92\% of the model accuracy.},
	language = {en},
	urldate = {2021-03-06},
	booktitle = {2020 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	publisher = {IEEE},
	author = {Ritter, Marcus and Calotoiu, Alexandru and Rinke, Sebastian and Reimann, Thorsten and Hoefler, Torsten and Wolf, Felix},
	month = may,
	year = {2020},
	pages = {884--895},
}

@article{bethel_performance_nodate,
	title = {Performance {Optimization} and {Auto}-{Tuning}},
	language = {en},
	author = {Bethel, E Wes},
	pages = {20},
}

@article{rahman_enhancing_nodate,
	title = {Enhancing {Learning}-based {Autotuning} with {Composite} and {Diagnostic} {Feature} {Vectors}},
	language = {en},
	author = {Rahman, Saami and Hay, Richard and Gutierrez, Mario A},
	pages = {2},
}

@book{meyer_performance_2012,
	title = {Performance {Modeling} of {Heterogeneous} {Systems}},
	isbn = {978-82-471-4015-4},
	url = {https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/253074},
	abstract = {As the complexity of parallel computers grows, constraints posed by the construction of larger systems require both greater, and increasingly non-linear, parameter sets to model their behavior realistically. These heterogeneous characteristics create a trade-off between the complexity and accuracy of performance models, creating challenges in utilizing them for design decisions.

In this thesis, we take a bottom-up approach to realistically model software and hardware interactions, by composing system models from simpler, linear models, which allow parts of the analysis to be automated. We associate empirically benchmarked platform performance metrics with the core elements in a variant of bulk-synchronous execution, aiming to quantify application performance, and associated potential for computation and communication overlap on SMP clusters.

The original bulk-synchronous performance model is introduced, and we identify areas of computation and communication where its abstractions impede realistic models of contemporary hardware. These are addressed independently, using experimental evidence to develop a representation collecting computation kernel characteristics and pairwise communications in matrices, to combine into a system model. As bulk-synchronous execution strongly depends on periodic, global synchronization, we develop a cost model for it by combining latency measurements with a parametric representation of signalling patterns, and experimentally verify the resulting predictions for three common algorithms.

We describe a design to implement the BSPLib programming interface, combining threads and message-passing parallelism to achieve overlap on commodity cluster platforms, implementing its one-sided communication primitives using out-of-band control messages. We augment and validate the cost model of one adapted synchronization algorithm with the corresponding bandwidth requirement, completing a framework for modeling BSPLib program performance.

Finally, we test the utility of this framework as a proof-of-concept for guiding software performance adaptations, using two cases. First, we use the latency terms to automatically generate synchronization operations, using model predictions to generate customized patterns with respect to platform topology, showing that the resulting algorithms equal or outperform the system defaults. Second, the strong scaling characteristics of a 5-point stencil code is compared for three implementations. Experiments show the performance overhead of our implementation, but also its capability for predicting program cost, including parameter values to optimize for balanced overlapping of computation and communication.},
	language = {eng},
	urldate = {2021-02-10},
	publisher = {Norges teknisk-naturvitenskapelige universitet, Fakultet for informasjonsteknologi, matematikk og elektroteknikk, Institutt for datateknikk og informasjonsvitenskap},
	author = {Meyer, Jan Christian},
	year = {2012},
	note = {Accepted: 2014-12-19T13:39:21Z},
}

@inproceedings{meyer_performance_2010,
	title = {Performance modeling of heterogeneous systems},
	doi = {10.1109/IPDPSW.2010.5470682},
	abstract = {Predicting how well applications may run on modern systems is becoming increasingly challenging. It is no longer sufficient to look at number of floating point operations and communication costs, but one also needs to model the underlying systems and how their topology, heterogeneity, system loads, etc, may impact performance. This work focuses on developing a practical model for heterogeneous computing by looking at the older BSP model, which attempts to model communication costs on homogeneous systems, and looks at how its library implementations can be extended to include a run-time system that may be useful for heterogeneous systems. Our extensions of BSPlib with MPI and GASnet mechanisms at the communication layer should provide useful tools for evaluating applications with respect to how they may run on heterogeneous systems.},
	booktitle = {2010 {IEEE} {International} {Symposium} on {Parallel} {Distributed} {Processing}, {Workshops} and {Phd} {Forum} ({IPDPSW})},
	author = {Meyer, J. C. and Elster, A. C.},
	month = apr,
	year = {2010},
	keywords = {BSP model, Computational efficiency, Computational modeling, Computer architecture, Concurrent computing, Costs, Delay, Performance analysis, Predictive models, Runtime library, Topology, bulk-synchronous parallelism model, communication costs, floating point operations, heterogeneous computing, heterogeneous systems performance modeling, library implementations, parallel programming, performance evaluation, run-time system},
	pages = {1--4},
}

@article{dagum_openmp_1998,
	title = {{OpenMP}: {An} {Industry}-{Standard} {API} for {Shared}-{Memory} {Programming}},
	volume = {5},
	issn = {1070-9924},
	shorttitle = {{OpenMP}},
	url = {https://doi.org/10.1109/99.660313},
	doi = {10.1109/99.660313},
	abstract = {The authors present a new way to achieve scalability in parallel software with OpenMP, their portable alternative to message passing. They discuss its capabilities through specific examples and comparisons with other standard parallel programming models.},
	number = {1},
	urldate = {2021-02-07},
	journal = {IEEE Computational Science \& Engineering},
	author = {Dagum, Leonardo and Menon, Ramesh},
	month = jan,
	year = {1998},
	pages = {46--55},
}

@inproceedings{georges_statistically_2007,
	address = {New York, NY, USA},
	series = {{OOPSLA} '07},
	title = {Statistically rigorous java performance evaluation},
	isbn = {978-1-59593-786-5},
	url = {https://doi.org/10.1145/1297027.1297033},
	doi = {10.1145/1297027.1297033},
	abstract = {Java performance is far from being trivial to benchmark because it is affected by various factors such as the Java application, its input, the virtual machine, the garbage collector, the heap size, etc. In addition, non-determinism at run-time causes the execution time of a Java program to differ from run to run. There are a number of sources of non-determinism such as Just-In-Time (JIT) compilation and optimization in the virtual machine (VM) driven by timer-based method sampling, thread scheduling, garbage collection, and various. There exist a wide variety of Java performance evaluation methodologies usedby researchers and benchmarkers. These methodologies differ from each other in a number of ways. Some report average performance over a number of runs of the same experiment; others report the best or second best performance observed; yet others report the worst. Some iterate the benchmark multiple times within a single VM invocation; others consider multiple VM invocations and iterate a single benchmark execution; yet others consider multiple VM invocations and iterate the benchmark multiple times. This paper shows that prevalent methodologies can be misleading, and can even lead to incorrect conclusions. The reason is that the data analysis is not statistically rigorous. In this paper, we present a survey of existing Java performance evaluation methodologies and discuss the importance of statistically rigorous data analysis for dealing with non-determinism. We advocate approaches to quantify startup as well as steady-state performance, and, in addition, we provide the JavaStats software to automatically obtain performance numbers in a rigorous manner. Although this paper focuses on Java performance evaluation, many of the issues addressed in this paper also apply to other programming languages and systems that build on a managed runtime system.},
	urldate = {2021-02-07},
	booktitle = {Proceedings of the 22nd annual {ACM} {SIGPLAN} conference on {Object}-oriented programming systems, languages and applications},
	publisher = {Association for Computing Machinery},
	author = {Georges, Andy and Buytaert, Dries and Eeckhout, Lieven},
	month = oct,
	year = {2007},
	keywords = {benchmarking, data analysis, java, methodology, statistics},
	pages = {57--76},
}

@article{kalibera_quantifying_2020,
	title = {Quantifying {Performance} {Changes} with {Effect} {Size} {Confidence} {Intervals}},
	url = {http://arxiv.org/abs/2007.10899},
	abstract = {Measuring performance \& quantifying a performance change are core evaluation techniques in programming language and systems research. Of 122 recent scientific papers, as many as 65 included experimental evaluation that quantified a performance change using a ratio of execution times. Few of these papers evaluated their results with the level of rigour that has come to be expected in other experimental sciences. The uncertainty of measured results was largely ignored. Scarcely any of the papers mentioned uncertainty in the ratio of the mean execution times, and most did not even mention uncertainty in the two means themselves. Most of the papers failed to address the non-deterministic execution of computer programs (caused by factors such as memory placement, for example), and none addressed non-deterministic compilation. It turns out that the statistical methods presented in the computer systems performance evaluation literature for the design and summary of experiments do not readily allow this either. This poses a hazard to the repeatability, reproducibility and even validity of quantitative results. Inspired by statistical methods used in other fields of science, and building on results in statistics that did not make it to introductory textbooks, we present a statistical model that allows us both to quantify uncertainty in the ratio of (execution time) means and to design experiments with a rigorous treatment of those multiple sources of non-determinism that might impact measured performance. Better still, under our framework summaries can be as simple as "system A is faster than system B by 5.5\% \${\textbackslash}pm\$ 2.5\%, with 95\% confidence", a more natural statement than those derived from typical current practice, which are often misinterpreted. November 2013},
	language = {en},
	urldate = {2021-02-04},
	journal = {arXiv:2007.10899 [cs, stat]},
	author = {Kalibera, Tomas and Jones, Richard},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.10899},
	keywords = {Computer Science - Programming Languages, D.2.8, Statistics - Methodology},
}

@article{kalibera_quantifying_2020-1,
	title = {Quantifying {Performance} {Changes} with {Effect} {Size} {Confidence} {Intervals}},
	url = {http://arxiv.org/abs/2007.10899},
	abstract = {Measuring performance \& quantifying a performance change are core evaluation techniques in programming language and systems research. Of 122 recent scientific papers, as many as 65 included experimental evaluation that quantified a performance change using a ratio of execution times. Few of these papers evaluated their results with the level of rigour that has come to be expected in other experimental sciences. The uncertainty of measured results was largely ignored. Scarcely any of the papers mentioned uncertainty in the ratio of the mean execution times, and most did not even mention uncertainty in the two means themselves. Most of the papers failed to address the non-deterministic execution of computer programs (caused by factors such as memory placement, for example), and none addressed non-deterministic compilation. It turns out that the statistical methods presented in the computer systems performance evaluation literature for the design and summary of experiments do not readily allow this either. This poses a hazard to the repeatability, reproducibility and even validity of quantitative results. Inspired by statistical methods used in other fields of science, and building on results in statistics that did not make it to introductory textbooks, we present a statistical model that allows us both to quantify uncertainty in the ratio of (execution time) means and to design experiments with a rigorous treatment of those multiple sources of non-determinism that might impact measured performance. Better still, under our framework summaries can be as simple as "system A is faster than system B by 5.5\% \${\textbackslash}pm\$ 2.5\%, with 95\% confidence", a more natural statement than those derived from typical current practice, which are often misinterpreted. November 2013},
	language = {en},
	urldate = {2021-02-04},
	journal = {arXiv:2007.10899 [cs, stat]},
	author = {Kalibera, Tomas and Jones, Richard},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.10899},
	keywords = {Computer Science - Programming Languages, D.2.8, Statistics - Methodology},
}

@article{kalibera_rigorous_nodate,
	title = {Rigorous {Benchmarking} in {Reasonable} {Time}},
	abstract = {Experimental evaluation is key to systems research. Because modern systems are complex and non-deterministic, good experimental methodology demands that researchers account for uncertainty. To obtain valid results, they are expected to run many iterations of benchmarks, invoke virtual machines (VMs) several times, or even rebuild VM or benchmark binaries more than once. All this repetition costs time to complete experiments. Currently, many evaluations give up on sufﬁcient repetition or rigorous statistical methods, or even run benchmarks only in training sizes. The results reported often lack proper variation estimates and, when a small difference between two systems is reported, some are simply unreliable.},
	language = {en},
	author = {Kalibera, Tomas and Jones, Richard},
	pages = {12},
}

@article{georges_statistically_nodate,
	title = {Statistically {Rigorous} {Java} {Performance} {Evaluation}},
	abstract = {Java performance is far from being trivial to benchmark because it is affected by various factors such as the Java application, its input, the virtual machine, the garbage collector, the heap size, etc. In addition, non-determinism at run-time causes the execution time of a Java program to differ from run to run. There are a number of sources of non-determinism such as Just-In-Time (JIT) compilation and optimization in the virtual machine (VM) driven by timerbased method sampling, thread scheduling, garbage collection, and various system effects.},
	language = {en},
	author = {Georges, Andy and Buytaert, Dries and Eeckhout, Lieven},
	pages = {20},
}

@book{kalibera_rigorous_2013,
	title = {Rigorous {Benchmarking} in {Reasonable} {Time}},
	volume = {48},
	abstract = {Experimental evaluation is key to systems research. Because modern systems are complex and non-deterministic, good experimental methodology demands that researchers account for uncertainty. To obtain valid results, they are expected to run many iterations of benchmarks, invoke virtual machines (VMs) several times, or even rebuild VM or benchmark binaries more than once. All this repetition costs time to complete experiments. Currently, many evaluations give up on sufficient repetition or rigorous statistical methods, or even run benchmarks only in training sizes. The results reported often lack proper variation estimates and, when a small difference between two systems is reported, some are simply unreliable.
In contrast, we provide a statistically rigorous methodology for repetition and summarising results that makes efficient use of experimentation time. Time efficiency comes from two key observations. First, a given benchmark on a given platform is typically prone to much less non-determinism than the common worst-case of published corner-case studies. Second, repetition is most needed where most uncertainty arises (whether between builds, between executions or between iterations). We capture experimentation cost with a novel mathematical model, which we use to identify the number of repetitions at each level of an experiment necessary and sufficient to obtain a given level of precision.
We present our methodology as a cookbook that guides researchers on the number of repetitions they should run to obtain reliable results. We also show how to present results with an effect size confidence interval. As an example, we show how to use our methodology to conduct throughput experiments with the DaCapo and SPEC CPU benchmarks on three recent platforms.},
	author = {Kalibera, Tomas and Jones, Richard},
	month = jun,
	year = {2013},
	doi = {10.1145/2464157.2464160},
	note = {Journal Abbreviation: International Symposium on Memory Management, ISMM
Publication Title: International Symposium on Memory Management, ISMM},
}

@misc{noauthor_gh_nodate,
	title = {{GH} {Archive}},
	url = {https://www.gharchive.org/},
	urldate = {2021-01-31},
}

@book{boudec_performance_2010,
	title = {Performance {Evaluation} of {Computer} and {Communication} {Systems}},
	isbn = {978-2-940222-40-7},
	abstract = {Performance evaluation is a critical stage of software- and hardware-system development that every computer engineer and scientist should master. Although complex – requiring skills in mathematics, measurement techniques and simulation – performance evaluation is primarily an art; indeed, the most difficult stage in a performance analysis is defining the approach: once you know what to do, it is less difficult to define a plan of attack with your familiar software tools. We present a set of topics, which we believe should be part of every engineer\&\#39;s intellectual toolkit. This includes the statistical exploitation of numerical results in an efficient and ethical way, for example: how to summarize variability or fairness; what transient removal in a simulation is; and how to make predictions from a time series. We also present well-known performance patterns, which helps to quickly bring the engineer to the main issues. For queuing theory, we focus on a subset of very useful results, such as operational laws. A highlight of the book is the development of Palm calculus, also called ¬ìthe importance of the viewpoint,¬î which is central to queuing theory. Indeed, this topic has so many applications to simulation and to system analysis in general that it is a very good time investment. This book began as a set of lecture notes for a course given at EPFL.},
	language = {en},
	publisher = {EPFL Press},
	author = {Boudec, Jean-Yves Le},
	month = oct,
	year = {2010},
	note = {Google-Books-ID: cpfmDAAAQBAJ},
}

@misc{noauthor_hypothesis_nodate,
	title = {hypothesis testing - {How} to choose between t-test or non-parametric test e.g. {Wilcoxon} in small samples},
	url = {https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl},
	urldate = {2021-01-23},
	journal = {Cross Validated},
}

@inproceedings{hoefler_scientific_2015,
	address = {New York, NY, USA},
	series = {{SC} '15},
	title = {Scientific benchmarking of parallel computing systems: twelve ways to tell the masses when reporting performance results},
	isbn = {978-1-4503-3723-6},
	shorttitle = {Scientific benchmarking of parallel computing systems},
	url = {https://doi.org/10.1145/2807591.2807644},
	doi = {10.1145/2807591.2807644},
	abstract = {Measuring and reporting performance of parallel computers constitutes the basis for scientific advancement of high-performance computing (HPC). Most scientific reports show performance improvements of new techniques and are thus obliged to ensure reproducibility or at least interpretability. Our investigation of a stratified sample of 120 papers across three top conferences in the field shows that the state of the practice is lacking. For example, it is often unclear if reported improvements are deterministic or observed by chance. In addition to distilling best practices from existing work, we propose statistically sound analysis and reporting techniques and simple guidelines for experimental design in parallel computing and codify them in a portable benchmarking library. We aim to improve the standards of reporting research results and initiate a discussion in the HPC field. A wide adoption of our minimal set of rules will lead to better interpretability of performance results and improve the scientific culture in HPC.},
	urldate = {2021-01-22},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Hoefler, Torsten and Belli, Roberto},
	month = nov,
	year = {2015},
	keywords = {benchmarking, data analysis, parallel computing, statistics},
	pages = {1--12},
}

@misc{noauthor_automatic_nodate,
	title = {Automatic {Benchmarking}},
	url = {https://www.overleaf.com/project/5f2914d4cae0bf0001adfd58},
	abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2021-01-18},
}

@article{sjalander_epic_2020,
	title = {{EPIC}: {An} {Energy}-{Efficient}, {High}-{Performance} {GPGPU} {Computing} {Research} {Infrastructure}},
	shorttitle = {{EPIC}},
	url = {http://arxiv.org/abs/1912.05848},
	abstract = {The pursuit of many research questions requires massive computational resources. State-of-the-art research in physical processes using simulations, the training of neural networks for deep learning, or the analysis of big data are all dependent on the availability of sufficient and performant computational resources. For such research, access to a high-performance computing infrastructure is indispensable. Many scientific workloads from such research domains are inherently parallel and can benefit from the data-parallel architecture of general purpose graphics processing units (GPGPUs). However, GPGPU resources are scarce at Norway's national infrastructure. EPIC is a GPGPU enabled computing research infrastructure at NTNU. It enables NTNU's researchers to perform experiments that otherwise would be impossible, as time-to-solution would simply take too long.},
	urldate = {2021-01-18},
	journal = {arXiv:1912.05848 [cs]},
	author = {Själander, Magnus and Jahre, Magnus and Tufte, Gunnar and Reissmann, Nico},
	month = dec,
	year = {2020},
	note = {arXiv: 1912.05848},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@inproceedings{ibrahim_roofline_2018,
	title = {Roofline {Scaling} {Trajectories}: {A} {Method} for {Parallel} {Application} and {Architectural} {Performance} {Analysis}},
	shorttitle = {Roofline {Scaling} {Trajectories}},
	doi = {10.1109/HPCS.2018.00065},
	abstract = {The end of Dennard scaling signaled a shift in HPC supercomputer architectures from systems built from single- core processor architectures to systems built from multicore and eventually manycore architectures. This transition substantially complicated performance optimization and analysis as new programming models were created, new scaling methodologies deployed, and on-chip contention became a bottleneck to performance. Existing distributed memory performance models like logP and logGP were unable to capture this contention. The Roofline model was created to address this contention and its interplay with locality. However, to date, the Roofline model has focused on full-node concurrency. In this paper, we extend the Roofline model to capture the effects of concurrency on data locality and on-chip contention. We demonstrate the value of this new technique by evaluating the NAS parallel benchmarks on both multicore and manycore architectures under both strong-and weak-scaling regimes. In order to quantify the interplay between programming model and locality, we evaluate scaling under both the OpenMP and flat MPI programming models.},
	booktitle = {2018 {International} {Conference} on {High} {Performance} {Computing} {Simulation} ({HPCS})},
	author = {Ibrahim, K. and Williams, S. and Oliker, L.},
	month = jul,
	year = {2018},
	keywords = {Computational modeling, Concurrent computing, Dennard scaling, HPC supercomputer architectures, MPI programming model, Measurement, Multicore processing, NAS parallel benchmarks, OpenMP programming model, Programming, Roofline scaling trajectories, Trajectory, application program interfaces, architectural performance analysis, concurrency (computers), data locality, distributed memory performance models, distributed memory systems, full-node concurrency, logGP, logP, mainframes, manycore architectures, message passing, multicore architectures, on-chip contention, parallel application, parallel architectures, parallel machines, parallel programming, performance evaluation, performance optimization, single- core processor architectures, software performance evaluation, strong-scaling regime, weak-scaling regimes},
	pages = {350--358},
}

@inproceedings{denoyelle_modeling_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Modeling {Large} {Compute} {Nodes} with {Heterogeneous} {Memories} with {Cache}-{Aware} {Roofline} {Model}},
	isbn = {978-3-319-72971-8},
	doi = {10.1007/978-3-319-72971-8_5},
	abstract = {In order to fulfill modern applications needs, computing systems become more powerful, heterogeneous and complex. NUMA platforms and emerging high bandwidth memories offer new opportunities for performance improvements. However they also increase hardware and software complexity, thus making application performance analysis and optimization an even harder task. The Cache-Aware Roofline Model (CARM) is an insightful, yet simple model designed to address this issue. It provides feedback on potential applications bottlenecks and shows how far is the application performance from the achievable hardware upper-bounds. However, it does not encompass NUMA systems and next generation processors with heterogeneous memories. Yet, some application bottlenecks belong to those memory subsystems, and would benefit from the CARM insights. In this paper, we fill the missing requirements to scope recent large shared memory systems with the CARM. We provide the methodology to instantiate, and validate the model on a NUMA system as well as on the latest Xeon Phi processor equiped with configurable hybrid memory. Finally, we show the model ability to exhibits several bottlenecks of such systems, which were not supported by CARM.},
	language = {en},
	booktitle = {High {Performance} {Computing} {Systems}. {Performance} {Modeling}, {Benchmarking}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Denoyelle, Nicolas and Goglin, Brice and Ilic, Aleksandar and Jeannot, Emmanuel and Sousa, Leonel},
	editor = {Jarvis, Stephen and Wright, Steven and Hammond, Simon},
	year = {2018},
	pages = {91--113},
}

@inproceedings{hill_gables_2019,
	title = {Gables: {A} {Roofline} {Model} for {Mobile} {SoCs}},
	shorttitle = {Gables},
	doi = {10.1109/HPCA.2019.00047},
	abstract = {Over a billion mobile consumer system-on-chip (SoC) chipsets ship each year. Of these, the mobile consumer market undoubtedly involving smartphones has a significant market share. Most modern smartphones comprise of advanced SoC architectures that are made up of multiple cores, GPS, and many different programmable and fixed-function accelerators connected via a complex hierarchy of interconnects with the goal of running a dozen or more critical software usecases under strict power, thermal and energy constraints. The steadily growing complexity of a modern SoC challenges hardware computer architects on how best to do early stage ideation. Late SoC design typically relies on detailed full-system simulation once the hardware is specified and accelerator software is written or ported. However, early-stage SoC design must often select accelerators before a single line of software is written. To help frame SoC thinking and guide early stage mobile SoC design, in this paper we contribute the Gables model that refines and retargets the Roofline model-designed originally for the performance and bandwidth limits of a multicore chip-to model each accelerator on a SoC, to apportion work concurrently among different accelerators (justified by our usecase analysis), and calculate a SoC performance upper bound. We evaluate the Gables model with an existing SoC and develop several extensions that allow Gables to inform early stage mobile SoC design.},
	booktitle = {2019 {IEEE} {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	author = {Hill, M. and Reddi, V. Janapa},
	month = feb,
	year = {2019},
	note = {ISSN: 2378-203X},
	keywords = {Accelerator architectures, Bandwidth, Computational modeling, Computer architecture, Fabrics, Gables model, Hardware, Mobile computing, Processor Architecture, Smart phones, SoC performance upper, Software, System-on-Chip, accelerator software, advanced SoC architectures, fixed-function accelerators, full-system simulation, hardware computer architects, integrated circuit design, mobile SoC design, mobile consumer market, multicore chip, multiple cores, multiprocessing systems, programmable accelerators, roofline model, smart phones, smartphones, system-on-chip, system-on-chip chipsets, thermal energy constraints},
	pages = {317--330},
}

@article{konstantinidis_quantitative_2017,
	title = {A quantitative roofline model for {GPU} kernel performance estimation using micro-benchmarks and hardware metric profiling},
	volume = {107},
	issn = {0743-7315},
	url = {http://www.sciencedirect.com/science/article/pii/S0743731517301247},
	doi = {10.1016/j.jpdc.2017.04.002},
	abstract = {Typically, the execution time of a kernel on a GPU is a difficult to predict measure as it depends on a wide range of factors. Performance can be limited by either memory transfer, compute throughput or other latencies. In this paper, we improve on the roofline model following a quantitative approach and present a completely automated GPU performance prediction technique. In this respect this model utilizes micro-benchmarking and profiling in a “black box” fashion as no inspection of source/binary code is required. The proposed model combines parameters in order to characterize the performance limiting factor and to estimate execution time. In addition, we propose the quadrant-split visual representation, which captures the characteristics of multiple processors in relation to a particular kernel. We performed experiments on stencil computation (red/black SOR), SGEMM and a total of 28 kernels of the Rodinia benchmark suite, using six CUDA GPUs and we showed an absolute error in predictions of 27.66\% in the average case. Furthermore, the performance model was also examined on an AMD GPU through the HIP programming environment. Prediction errors were comparable despite the significant architectural differences between different vendor GPUs.},
	language = {en},
	urldate = {2021-01-17},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Konstantinidis, Elias and Cotronis, Yiannis},
	month = sep,
	year = {2017},
	keywords = {CUDA, GPU computing, HIP, Micro-benchmarks, Performance estimation, Performance model},
	pages = {37--56},
}

@inproceedings{lo_roofline_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Roofline {Model} {Toolkit}: {A} {Practical} {Tool} for {Architectural} and {Program} {Analysis}},
	isbn = {978-3-319-17248-4},
	shorttitle = {Roofline {Model} {Toolkit}},
	doi = {10.1007/978-3-319-17248-4_7},
	abstract = {We present preliminary results of the Roofline Toolkit for multicore, manycore, and accelerated architectures. This paper focuses on the processor architecture characterization engine, a collection of portable instrumented micro benchmarks implemented with Message Passing Interface (MPI), and OpenMP used to express thread-level parallelism. These benchmarks are specialized to quantify the behavior of different architectural features. Compared to previous work on performance characterization, these microbenchmarks focus on capturing the performance of each level of the memory hierarchy, along with thread-level parallelism, instruction-level parallelism and explicit SIMD parallelism, measured in the context of the compilers and run-time environments. We also measure sustained PCIe throughput with four GPU memory managed mechanisms. By combining results from the architecture characterization with the Roofline model based solely on architectural specifications, this work offers insights for performance prediction of current and future architectures and their software systems. To that end, we instrument three applications and plot their resultant performance on the corresponding Roofline model when run on a Blue Gene/Q architecture.},
	language = {en},
	booktitle = {High {Performance} {Computing} {Systems}. {Performance} {Modeling}, {Benchmarking}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Lo, Yu Jung and Williams, Samuel and Van Straalen, Brian and Ligocki, Terry J. and Cordery, Matthew J. and Wright, Nicholas J. and Hall, Mary W. and Oliker, Leonid},
	editor = {Jarvis, Stephen A. and Wright, Steven A. and Hammond, Simon D.},
	year = {2015},
	keywords = {CUDA unified memory, Memory bandwidth, Roofline},
	pages = {129--148},
}

@article{huang_comparability_2012,
	title = {Comparability and reproducibility of biomedical data},
	volume = {14},
	doi = {10.1093/bib/bbs078},
	abstract = {With the development of novel assay technologies, biomedical experiments and analyses have gone through substantial evolution. Today, a typical experiment can simultaneously measure hundreds to thousands of individual features (e.g. genes) in dozens of biological conditions, resulting in gigabytes of data that need to be processed and analyzed. Because of the multiple steps involved in the data generation and analysis and the lack of details provided, it can be difficult for independent researchers to try to reproduce a published study. With the recent outrage following the halt of a cancer clinical trial due to the lack of reproducibility of the published study, researchers are now facing heavy pressure to ensure that their results are reproducible. Despite the global demand, too many published studies remain non-reproducible mainly due to the lack of availability of experimental protocol, data and/or computer code. Scientific discovery is an iterative process, where a published study generates new knowledge and data, resulting in new follow-up studies or clinical trials based on these results. As such, it is important for the results of a study to be quickly confirmed or discarded to avoid wasting time and money on novel projects. The availability of high-quality, reproducible data will also lead to more powerful analyses (or meta-analyses) where multiple data sets are combined to generate new knowledge. In this article, we review some of the recent developments regarding biomedical reproducibility and comparability and discuss some of the areas where the overall field could be improved.},
	journal = {Briefings in bioinformatics},
	author = {Huang, Yunda and Gottardo, Raphael},
	month = nov,
	year = {2012},
}

@article{maassen_reproducibility_2020,
	title = {Reproducibility of individual effect sizes in meta-analyses in psychology},
	volume = {15},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0233107},
	doi = {10.1371/journal.pone.0233107},
	abstract = {To determine the reproducibility of psychological meta-analyses, we investigated whether we could reproduce 500 primary study effect sizes drawn from 33 published meta-analyses based on the information given in the meta-analyses, and whether recomputations of primary study effect sizes altered the overall results of the meta-analysis. Results showed that almost half (k = 224) of all sampled primary effect sizes could not be reproduced based on the reported information in the meta-analysis, mostly because of incomplete or missing information on how effect sizes from primary studies were selected and computed. Overall, this led to small discrepancies in the computation of mean effect sizes, confidence intervals and heterogeneity estimates in 13 out of 33 meta-analyses. We provide recommendations to improve transparency in the reporting of the entire meta-analytic process, including the use of preregistration, data and workflow sharing, and explicit coding practices.},
	language = {en},
	number = {5},
	urldate = {2020-11-25},
	journal = {PLOS ONE},
	author = {Maassen, Esther and Assen, Marcel A. L. M. van and Nuijten, Michèle B. and Olsson-Collentine, Anton and Wicherts, Jelte M.},
	year = {2020},
	note = {Publisher: Public Library of Science},
	keywords = {Clinical psychology, Metaanalysis, Peer review, Psychology, Publication ethics, Reproducibility, Research reporting guidelines, Systematic reviews},
	pages = {e0233107},
}

@book{lakens_examining_2017,
	title = {Examining the {Reproducibility} of {Meta}-{Analyses} in {Psychology}: {A} {Preliminary} {Report}},
	shorttitle = {Examining the {Reproducibility} of {Meta}-{Analyses} in {Psychology}},
	abstract = {Meta-analyses are an important tool to evaluate the literature. It is essential that meta-analyses can easily be reproduced to allow researchers to evaluate the impact of subjective choices on meta-analytic effect sizes, but also to update meta-analyses as new data comes in, or as novel statistical techniques (for example to correct for publication bias) are developed. Research in medicine has revealed meta-analyses often cannot be reproduced. In this project, we examined the reproducibility of meta-analyses in psychology by reproducing twenty published meta-analyses. Reproducing published meta-analyses was surprisingly difficult. 96\% of meta-analyses published in 2013-2014 did not adhere to reporting guidelines. A third of these meta-analyses did not contain a table specifying all individual effect sizes. Five of the 20 randomly selected meta-analyses we attempted to reproduce could not be reproduced at all due to lack of access to raw data, no details about the effect sizes extracted from each study, or a lack of information about how effect sizes were coded. In the remaining meta-analyses, differences between the reported and reproduced effect size or sample size were common. We discuss a range of possible improvements, such as more clearly indicating which data were used to calculate an effect size, specifying all individual effect sizes, adding detailed information about equations that are used, and how multiple effect size estimates from the same study are combined, but also sharing raw data retrieved from original authors, or unpublished research reports. This project clearly illustrates there is a lot of room for improvement when it comes to the transparency and reproducibility of published meta-analyses.},
	author = {Lakens, Daniël and Page-Gould, Elizabeth and Assen, Marcel and Spellman, Bobbie and Schönbrodt, Felix and Hasselman, Fred and Corker, Katherine and Grange, Jim and Sharples, Amanda and Cavender, Corinne and Augusteijn, Hilde and Gerger, Heike and Locher, Cosima and Miller, Ian and Anwari, Farid and Scheel, Anne},
	month = mar,
	year = {2017},
	doi = {10.31222/osf.io/xfbjf},
}

@article{lakens_reproducibility_2016,
	title = {On the reproducibility of meta-analyses: six practical recommendations},
	volume = {4},
	issn = {2050-7283},
	shorttitle = {On the reproducibility of meta-analyses},
	url = {https://doi.org/10.1186/s40359-016-0126-3},
	doi = {10.1186/s40359-016-0126-3},
	abstract = {Meta-analyses play an important role in cumulative science by combining information across multiple studies and attempting to provide effect size estimates corrected for publication bias. Research on the reproducibility of meta-analyses reveals that errors are common, and the percentage of effect size calculations that cannot be reproduced is much higher than is desirable. Furthermore, the flexibility in inclusion criteria when performing a meta-analysis, combined with the many conflicting conclusions drawn by meta-analyses of the same set of studies performed by different researchers, has led some people to doubt whether meta-analyses can provide objective conclusions.},
	number = {1},
	urldate = {2020-11-25},
	journal = {BMC Psychology},
	author = {Lakens, Daniël and Hilgard, Joe and Staaks, Janneke},
	month = may,
	year = {2016},
	pages = {24},
}

@article{cockburn_threats_2020,
	title = {Threats of a replication crisis in empirical computer science},
	volume = {63},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3360311},
	doi = {10.1145/3360311},
	abstract = {Research replication only works if there is confidence built into the results.},
	number = {8},
	urldate = {2020-11-24},
	journal = {Communications of the ACM},
	author = {Cockburn, Andy and Dragicevic, Pierre and Besançon, Lonni and Gutwin, Carl},
	month = jul,
	year = {2020},
	pages = {70--79},
}

@misc{gutwin_threats_nodate,
	title = {Threats of a {Replication} {Crisis} in {Empirical} {Computer} {Science}},
	url = {https://cacm.acm.org/magazines/2020/8/246369-threats-of-a-replication-crisis-in-empirical-computer-science/fulltext},
	abstract = {Research replication only works if there is confidence built into the results.},
	language = {en},
	urldate = {2020-11-24},
	author = {Gutwin, Pierre Dragicevic, Lonni Besanon, Carl, Andy Cockburn},
}

@article{elliott_guide_nodate,
	title = {A guide to comparability terminology and methods},
	language = {en},
	author = {Elliott, Gill},
	pages = {20},
}

@article{barba_terminologies_2018,
	title = {Terminologies for {Reproducible} {Research}},
	url = {http://arxiv.org/abs/1802.03311},
	abstract = {Reproducible research---by its many names---has come to be regarded as a key concern across disciplines and stakeholder groups. Funding agencies and journals, professional societies and even mass media are paying attention, often focusing on the so-called "crisis" of reproducibility. One big problem keeps coming up among those seeking to tackle the issue: different groups are using terminologies in utter contradiction with each other. Looking at a broad sample of publications in different fields, we can classify their terminology via decision tree: they either, A---make no distinction between the words reproduce and replicate, or B---use them distinctly. If B, then they are commonly divided in two camps. In a spectrum of concerns that starts at a minimum standard of "same data+same methods=same results," to "new data and/or new methods in an independent study=same findings," group 1 calls the minimum standard reproduce, while group 2 calls it replicate. This direct swap of the two terms aggravates an already weighty issue. By attempting to inventory the terminologies across disciplines, I hope that some patterns will emerge to help us resolve the contradictions.},
	urldate = {2020-11-24},
	journal = {arXiv:1802.03311 [cs]},
	author = {Barba, Lorena A.},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.03311},
	keywords = {Computer Science - Digital Libraries},
}

@misc{gutwin_threats_nodate-1,
	title = {Threats of a {Replication} {Crisis} in {Empirical} {Computer} {Science}},
	url = {https://cacm.acm.org/magazines/2020/8/246369-threats-of-a-replication-crisis-in-empirical-computer-science/fulltext},
	abstract = {Research replication only works if there is confidence built into the results.},
	language = {en},
	urldate = {2020-11-19},
	author = {Gutwin, Pierre Dragicevic, Lonni Besanon, Carl, Andy Cockburn},
}

@book{bloom_taxonomy_1956,
	address = {Harlow, Essex, England},
	series = {Taxonomy of educational objectives:  {The} classification of educational goals, 1st ed},
	title = {Taxonomy of educational objectives:  {The} classification of educational goals, 1st ed},
	shorttitle = {Taxonomy of educational objectives},
	publisher = {Longman Group},
	author = {Bloom, Benjamin Samuel},
	year = {1956},
	note = {Pages: np, np},
}

@book{bloom_taxonomy_1956-1,
	address = {Harlow, Essex, England},
	series = {Taxonomy of educational objectives:  {The} classification of educational goals, 1st ed},
	title = {Taxonomy of educational objectives:  {The} classification of educational goals, 1st ed},
	shorttitle = {Taxonomy of educational objectives},
	publisher = {Longman Group},
	author = {Bloom, Benjamin Samuel},
	year = {1956},
	note = {Pages: np, np},
}

@inproceedings{auvinen_stops_2014,
	address = {New York, NY, USA},
	series = {Koli {Calling} '14},
	title = {{STOPS}: a graph-based study planning and curriculum development tool},
	isbn = {978-1-4503-3065-7},
	shorttitle = {{STOPS}},
	url = {https://doi.org/10.1145/2674683.2674689},
	doi = {10.1145/2674683.2674689},
	abstract = {STOPS (Software for Target-Oriented Personal Syllabus) is a tool that allows university students to create personal study plans and the staff to maintain and develop course contents and curriculum structure. In the system, the curriculum is modeled as a graph of learning outcomes. First, the courses are broken down to learning outcomes, i.e. what a student should know after completing a course. Second, prerequisite dependencies are defined between the outcomes. Finally, programmes are defined as goals that students can choose for their studies. Programmes consist of competence outcomes that have prerequisite dependencies from the outcomes of the courses. The resulting graph can be visualized so that students can see how the topics taught in various courses are connected to each other, and what each course contributes to the goals that the student has selected. For curriculum developers, the visualizations can reveal shortcomings such as unnecessary repetition or lacking coverage of topics in the course contents.},
	urldate = {2020-10-27},
	booktitle = {Proceedings of the 14th {Koli} {Calling} {International} {Conference} on {Computing} {Education} {Research}},
	publisher = {Association for Computing Machinery},
	author = {Auvinen, Tapio and Paavola, Juha and Hartikainen, Juha},
	month = nov,
	year = {2014},
	keywords = {curriculum design, curriculum development, curriculum mapping, curriculum visualization, study planning},
	pages = {25--34},
}

@inproceedings{herman_coordinating_2016,
	title = {Coordinating {College}-{Wide} {Instructional} {Change} {Through} {Faculty} {Communities}},
	url = {https://ebooks.asmedigitalcollection.asme.org/IMECE/proceedings/IMECE2015/57588/V015T19A021/262523},
	doi = {10.1115/IMECE2015-51549},
	language = {en},
	urldate = {2020-10-27},
	publisher = {American Society of Mechanical Engineers Digital Collection},
	author = {Herman, Geoffrey L. and Hahn, Laura and West, Matthew},
	month = mar,
	year = {2016},
}

@inproceedings{kabicher_coordinating_2009,
	title = {Coordinating {Curriculum} {Implementation} {Using} {Wiki}-supported {Graph} {Visualization}},
	doi = {10.1109/ICALT.2009.54},
	abstract = {Traditionally, curriculum specifications tend to be formal, static documents, not encouraging adaptation and evolution. In order to facilitate various activities in implementing a curriculum, such as coordinating content, meta-cognitive competences, student-centered learning outcomes, this work proposes to employ a structured, Wiki-supported space. Our experience with this approach showed an improved transparency and coordination of courses based on a structured overview, a multi-faceted specification and increased sharing among teaching staff.},
	booktitle = {2009 {Ninth} {IEEE} {International} {Conference} on {Advanced} {Learning} {Technologies}},
	author = {Kabicher, S. and Motschnig-Pitrik, R.},
	month = jul,
	year = {2009},
	note = {ISSN: 2161-377X},
	keywords = {Application software, Computer science, Education, Educational technology, Information technology, Mathematics, Qualifications, Space technology, Visualization, Web services, Wiki-supported graph visualization, computer science, computer science education, curriculum implementation, curriculum specifications, data visualisation, educational courses, formal specification, graph theory, information resources, meta-cognitive competences, multifaceted specification, student-centered learning outcomes, teaching staff},
	pages = {742--743},
}

@misc{noauthor_idun_nodate,
	title = {Idun – {High} {Performance} {Computing} {Group}},
	url = {https://www.hpc.ntnu.no/idun},
	language = {en-US},
	urldate = {2020-08-29},
}

@book{reinders_high_2015,
	title = {High {Performance} {Parallelism} {Pearls}},
	isbn = {978-0-12-802118-7},
	url = {https://linkinghub.elsevier.com/retrieve/pii/C20140017972},
	abstract = {High Performance Parallelism Pearls shows how to leverage parallelism on processors and coprocessors with the same programming – illustrating the most effective ways to better tap the computational potential of systems with Intel Xeon Phi coprocessors and Intel Xeon processors or other multicore processors. The book includes examples of successful programming efforts, drawn from across industries and domains such as chemistry, engineering, and environmental science. Each chapter in this edited work includes detailed explanations of the programming techniques used, while showing high performance results on both Intel Xeon Phi coprocessors and multicore processors. Learn from dozens of new examples and case studies illustrating "success stories" demonstrating not just the features of these powerful systems, but also how to leverage parallelism across these heterogeneous systems.},
	language = {en},
	urldate = {2020-08-13},
	publisher = {Elsevier},
	author = {Reinders, James and Jeffers, Jim},
	year = {2015},
	doi = {10.1016/C2014-0-01797-2},
}

@misc{hu_tips_2017,
	title = {Tips to {Measure} the {Performance} of {Matrix} {Multiplication} {Using} {Intel}®...},
	url = {https://www.intel.com/content/www/us/en/develop/articles/a-simple-example-to-measure-the-performance-of-an-intel-mkl-function.html},
	abstract = {Intel® Math Kernel Library (Intel® MKL) provides highly optimized and extensively threaded general matrix-matrix multiplication (GEMM) functions. In this article, we explain how to design and measure of the performance using Intel MKL SGEMM, and outline about 7 tips to help developers to perform performance tests and quickly evaluate the floating pointing computing capability (FLOPS) on a specified processor.},
	language = {en},
	urldate = {2020-08-10},
	journal = {Intel},
	author = {Hu, Ying and Story, Shane A},
	month = dec,
	year = {2017},
}

@misc{noauthor_tips_nodate,
	title = {Tips to {Measure} the {Performance} of {Matrix} {Multiplication} {Using} {Intel}®...},
	url = {https://www.intel.com/content/www/us/en/develop/articles/a-simple-example-to-measure-the-performance-of-an-intel-mkl-function.html},
	abstract = {Intel® Math Kernel Library (Intel® MKL) provides highly optimized and extensively threaded general matrix-matrix multiplication (GEMM) functions. In this article, we explain how to design and measure of the performance using Intel MKL SGEMM, and outline about 7 tips to help developers to perform performance tests and quickly evaluate the floating pointing computing capability (FLOPS) on a specified processor.},
	language = {en},
	urldate = {2020-08-26},
	journal = {Intel},
}

@misc{noauthor_improve_nodate,
	title = {Improve {Intel}® {MKL} {Performance} for {Small} {Problems}: {The} {Use} of...},
	shorttitle = {Improve {Intel}® {MKL} {Performance} for {Small} {Problems}},
	url = {https://www.intel.com/content/www/us/en/develop/articles/improve-intel-mkl-performance-for-small-problems-the-use-of-mkl-direct-call.html},
	abstract = {One of the big new features introduced in the Intel® Math Kernel Library (Intel® MKL) 11.2 is the greatly improved performanc},
	language = {en},
	urldate = {2020-08-26},
	journal = {Intel},
}

@book{efron_introduction_1993,
	address = {New York},
	series = {Monographs on statistics and applied probability},
	title = {An introduction to the bootstrap},
	isbn = {978-0-412-04231-7},
	language = {en},
	number = {57},
	publisher = {Chapman \& Hall},
	author = {Efron, Bradley and Tibshirani, Robert},
	year = {1993},
	keywords = {Bootstrap (Statistics)},
}

@inproceedings{marques_performance_2017,
	title = {Performance {Analysis} with {Cache}-{Aware} {Roofline} {Model} in {Intel} {Advisor}},
	doi = {10.1109/HPCS.2017.150},
	abstract = {The recent increase in the complexity of processor architectures imposes significant challenges when designing and optimizing the execution of real-world applications, even on general-purpose hardware. To help in this process, tools for fast and insightful visualization of architecture and application execution bottlenecks are particularly useful for computer architects and application engineers, such as the recently proposed Cache-aware Roofline Model (CARM). CARM represents an insightful architecture performance model that provides a simple and intuitive way of visually representing the limits of parallel processing on contemporary multi-core processors with complex memory hierarchy. In its recent updates, Intel Advisor integrated performance CARM into its workflow. Intel Advisor is a powerful tool that helps application developers to extract the full potential performance out of a processor architecture, by analyzing applications and providing hints on parallelization, vectorization and memory access improvements. Therefore, when coupled with CARM, Intel Advisor Roofline represents a complete analysis and visualization framework for application characterization, optimization, and development. This paper focuses on introducing the CARM analysis methodology within Intel Advisor, by also showcasing the usability of other Advisor features. For this purpose, a set of 10 applications from different benchmark suits were analyzed on the state-of-the-art hardware platform in order to uncover the most critical bottlenecks and possible optimization steps to overcome them. By following the optimization guidelines given by Intel Advisor Roofline, the performance of several application kernels was improved for up to 6.43 times when compared to the unoptimized versions.},
	booktitle = {2017 {International} {Conference} on {High} {Performance} {Computing} {Simulation} ({HPCS})},
	author = {Marques, Diogo and Duarte, Helder and Ilic, Aleksandar and Sousa, Leonel and Belenov, Roman and Thierry, Philippe and Matveev, Zakhar A.},
	month = jul,
	year = {2017},
	keywords = {Artificial intelligence, Bandwidth, CARM analysis methodology, Cache-aware Roofline Model, Computational modeling, Computer architecture, Hardware, Intel Advisor Roofline, Intel Advisor integrated performance CARM, Optimization, Tools, application kernels, cache storage, complex memory hierarchy, computer architecture, contemporary multicore processors, general-purpose hardware, insightful architecture performance model, memory access improvements, multiprocessing programs, multiprocessing systems, operating system kernels, parallel processing, processor architecture},
	pages = {898--907},
}

@misc{harrell_p-value_nodate,
	title = {P-value},
	url = {https://fharrell.com/tags/p-value/},
	abstract = {Professor of Biostatistics},
	language = {en-us},
	urldate = {2020-08-21},
	journal = {Statistical Thinking},
	author = {Harrell, Frank},
}

@article{greenland_statistical_2016,
	title = {Statistical tests, {P} values, confidence intervals, and power: a guide to misinterpretations},
	volume = {31},
	issn = {1573-7284},
	shorttitle = {Statistical tests, {P} values, confidence intervals, and power},
	url = {https://doi.org/10.1007/s10654-016-0149-3},
	doi = {10.1007/s10654-016-0149-3},
	abstract = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so—and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
	language = {en},
	number = {4},
	urldate = {2020-08-21},
	journal = {European Journal of Epidemiology},
	author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
	month = apr,
	year = {2016},
	pages = {337--350},
}

@article{welford_note_1962,
	title = {Note on a {Method} for {Calculating} {Corrected} {Sums} of {Squares} and {Products}},
	volume = {4},
	issn = {0040-1706},
	url = {https://www.jstor.org/stable/1266577},
	doi = {10.2307/1266577},
	number = {3},
	urldate = {2020-08-19},
	journal = {Technometrics},
	author = {Welford, B. P.},
	year = {1962},
	note = {Publisher: [Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]},
	pages = {419--420},
}

@article{lim_implementation_2018,
	title = {An implementation of matrix–matrix multiplication on the {Intel} {KNL} processor with {AVX}-512},
	volume = {21},
	issn = {1573-7543},
	url = {https://doi.org/10.1007/s10586-018-2810-y},
	doi = {10.1007/s10586-018-2810-y},
	abstract = {The second generation Intel Xeon Phi processor codenamed Knights Landing (KNL) have recently emerged with 2D tile mesh architecture and the Intel AVX-512 instructions. However, it is very difficult for general users to get the maximum performance from the new architecture since they are not familiar with optimal cache reuse, efficient vectorization, and assembly language. In this paper, we illustrate several developing strategies to achieve good performance with C programming language by carrying out general matrix–matrix multiplications and without the use of assembly language. Our implementation of matrix–matrix multiplication is based on blocked matrix multiplication as an optimization technique that improves data reuse. We use data prefetching, loop unrolling, and the Intel AVX-512 to optimize the blocked matrix multiplications. When we use a single core of the KNL, our implementation achieves up to 98\% of SGEMM and 99\% of DGEMM using the Intel MKL, which is the current state-of-the-art library. Our implementation of the parallel DGEMM using all 68 cores of the KNL achieves up to 90\% of DGEMM using the Intel MKL.},
	language = {en},
	number = {4},
	urldate = {2020-08-13},
	journal = {Cluster Computing},
	author = {Lim, Roktaek and Lee, Yeongha and Kim, Raehyun and Choi, Jaeyoung},
	month = dec,
	year = {2018},
	pages = {1785--1795},
}

@inproceedings{mccalpin_hpl_2018,
	address = {Dallas, Texas},
	series = {{SC} '18},
	title = {{HPL} and {DGEMM} performance variability on the {Xeon} {Platinum} 8160 processor},
	url = {https://doi.org/10.1109/SC.2018.00021},
	doi = {10.1109/SC.2018.00021},
	abstract = {Initial testing of a cluster equipped with Intel Xeon Platinum 8160 processors showed occasional slow single-node HPL benchmark performance - approximately 0.4\% of single-node results were more than 10\% slower than expected. We describe a systematic series of experiments using simplified benchmarks and hardware performance counters, showing that the increased run times were associated with increased DRAM traffic, that this was caused by increased L2 cache miss rates, and that these were caused by snoop filter evictions. These evictions resulted from associativity conflicts in the snoop filters, and were traced to pathological interactions of data physical addresses with the hash function that distributes addresses across the coherence agents on the processor. For the HPL benchmark, switching from 2MiB hugepages to 1GiB hugepages eliminated the conflict and the associated slowdowns. The snoop filter conflict was later reproduced using a simple array summation kernel, suggesting that other applications could be impacted.},
	urldate = {2020-08-13},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage}, and {Analysis}},
	publisher = {IEEE Press},
	author = {McCalpin, John D.},
	month = nov,
	year = {2018},
	pages = {1--13},
}

@misc{noauthor_john_nodate,
	title = {John {McCalpin}'s blog » {Blog} {Archive} » {SC18} paper: {HPL} and {DGEMM} performance variability on {Intel} {Xeon} {Platinum} 8160 processors},
	shorttitle = {John {McCalpin}'s blog » {Blog} {Archive} » {SC18} paper},
	url = {https://sites.utexas.edu/jdm4372/2019/01/07/sc18-paper-hpl-and-dgemm-performance-variability-on-intel-xeon-platinum-8160-processors/},
	urldate = {2020-08-13},
}

@article{welford_note_1962-1,
	title = {Note on a {Method} for {Calculating} {Corrected} {Sums} of {Squares} and {Products}},
	volume = {4},
	issn = {0040-1706},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1962.10490022},
	doi = {10.1080/00401706.1962.10490022},
	number = {3},
	urldate = {2020-08-13},
	journal = {Technometrics},
	author = {Welford, B. P.},
	month = aug,
	year = {1962},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1962.10490022},
	pages = {419--420},
}

@inproceedings{yang_empirical_2018,
	address = {Dallas, TX, USA},
	title = {An {Empirical} {Roofline} {Methodology} for {Quantitatively} {Assessing} {Performance} {Portability}},
	isbn = {978-1-72810-220-7},
	url = {https://ieeexplore.ieee.org/document/8639946/},
	doi = {10.1109/P3HPC.2018.00005},
	abstract = {System and node architectures continue to diversify to better balance on-node computation, memory capacity, memory bandwidth, interconnect bandwidth, power, and cost for speciﬁc computational workloads. For many application developers, achieving performance portability (effectively exploiting the capabilities of multiple architectures) is a desired goal. Unfortunately, dramatically different per-node performance coupled with differences in machine balance can lead to developers being unable to determine whether they have attained performance portability or simply written portable code. The Rooﬂine model provides a means of quantitatively assessing how well a given application makes use of a target platform’s computational capabilities. In this paper, we extend the Rooﬂine model so that it 1) empirically captures a more realistic set of performance bounds for CPUs and GPUs, 2) factors in the true cost of different ﬂoating-point instructions when counting FLOPs, 3) incorporates the effects of different memory access patterns, and 4) with appropriate pairing of code performance and Rooﬂine ceiling, facilitates the performance portability analysis.},
	language = {en},
	urldate = {2020-08-12},
	booktitle = {2018 {IEEE}/{ACM} {International} {Workshop} on {Performance}, {Portability} and {Productivity} in {HPC} ({P3HPC})},
	publisher = {IEEE},
	author = {Yang, Charlene and Gayatri, Rahulkumar and Kurth, Thorsten and Basu, Protonu and Ronaghi, Zahra and Adetokunbo, Adedoyin and Friesen, Brian and Cook, Brandon and Doerfler, Douglas and Oliker, Leonid and Deslippe, Jack and Williams, Samuel},
	month = nov,
	year = {2018},
	pages = {14--23},
}

@incollection{jarvis_roofline_2015-1,
	address = {Cham},
	title = {Roofline {Model} {Toolkit}: {A} {Practical} {Tool} for {Architectural} and {Program} {Analysis}},
	volume = {8966},
	isbn = {978-3-319-17247-7 978-3-319-17248-4},
	shorttitle = {Roofline {Model} {Toolkit}},
	url = {http://link.springer.com/10.1007/978-3-319-17248-4_7},
	abstract = {We present preliminary results of the Rooﬂine Toolkit for multicore, manycore, and accelerated architectures. This paper focuses on the processor architecture characterization engine, a collection of portable instrumented micro benchmarks implemented with Message Passing Interface (MPI), and OpenMP used to express thread-level parallelism. These benchmarks are specialized to quantify the behavior of different architectural features. Compared to previous work on performance characterization, these microbenchmarks focus on capturing the performance of each level of the memory hierarchy, along with thread-level parallelism, instruction-level parallelism and explicit SIMD parallelism, measured in the context of the compilers and run-time environments. We also measure sustained PCIe throughput with four GPU memory managed mechanisms. By combining results from the architecture characterization with the Rooﬂine model based solely on architectural speciﬁcations, this work oﬀers insights for performance prediction of current and future architectures and their software systems. To that end, we instrument three applications and plot their resultant performance on the corresponding Rooﬂine model when run on a Blue Gene/Q architecture.},
	language = {en},
	urldate = {2020-08-12},
	booktitle = {High {Performance} {Computing} {Systems}. {Performance} {Modeling}, {Benchmarking}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Lo, Yu Jung and Williams, Samuel and Van Straalen, Brian and Ligocki, Terry J. and Cordery, Matthew J. and Wright, Nicholas J. and Hall, Mary W. and Oliker, Leonid},
	editor = {Jarvis, Stephen A. and Wright, Steven A. and Hammond, Simon D.},
	year = {2015},
	doi = {10.1007/978-3-319-17248-4_7},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {129--148},
}

@article{marques_application-driven_2020,
	title = {Application-driven {Cache}-{Aware} {Roofline} {Model}},
	volume = {107},
	issn = {0167-739X},
	url = {http://www.sciencedirect.com/science/article/pii/S0167739X19309586},
	doi = {10.1016/j.future.2020.01.044},
	abstract = {In the coming exascale era, the complexity of modern applications and hardware resources imposes significant challenges for boosting the efficiency via execution fine-tuning. To abstract this complexity in an intuitive way, recent application analysis tools rely on insightful modeling, e.g., Intel® Advisor with Cache-aware Roofline Model. However, these approaches mainly consider the maximum architecture capabilities, which may limit their usability when characterizing real-world applications. To address this issue, a novel Cache-Aware Roofline Model for more accurate performance modeling of multi-cores is proposed, which realistically resembles application requirements. The proposed fine-grain modeling relies on micro-benchmarking to decouple the attainable performance of the micro-architecture for different utilization scenarios and for a diverse set of functional units and memory levels. Memory sub-system traffic simulation, dynamic and static analyses are also used to derive the requirements of the applications. Experimental results for a real multi-core system with an Intel server processor and for a set of 13 kernels from exascale proxy applications, show that the proposed models provide more accurate application characterization, optimization hints and bottleneck detection in comparison to the state-of-the-art models.},
	language = {en},
	urldate = {2020-08-12},
	journal = {Future Generation Computer Systems},
	author = {Marques, Diogo and Ilic, Aleksandar and Matveev, Zakhar A. and Sousa, Leonel},
	month = jun,
	year = {2020},
	keywords = {Application characterization, Cache-Aware Roofline Model, Exascale applications, Performance modeling},
	pages = {257--273},
}

@article{ilic_cache-aware_2014,
	title = {Cache-aware {Roofline} model: {Upgrading} the loft},
	volume = {13},
	issn = {1556-6056},
	shorttitle = {Cache-aware {Roofline} model},
	url = {http://ieeexplore.ieee.org/document/6506838/},
	doi = {10.1109/L-CA.2013.6},
	abstract = {The Rooﬂine model graphically represents the attainable upper bound performance of a computer architecture. This paper analyzes the original Rooﬂine model and proposes a novel approach to provide a more insightful performance modeling of modern architectures by introducing cache-awareness, thus signiﬁcantly improving the guidelines for application optimization. The proposed model was experimentally veriﬁed for different architectures by taking advantage of built-in hardware counters with a curve ﬁtness above 90\%.},
	language = {en},
	number = {1},
	urldate = {2020-08-12},
	journal = {IEEE Computer Architecture Letters},
	author = {Ilic, Aleksandar and Pratas, Frederico and Sousa, Leonel},
	month = jan,
	year = {2014},
	pages = {21--24},
}

@misc{noauthor_performance_nodate,
	title = {Performance {Benchmarks} for {Intel}® {Math} {Kernel} {Library}},
	url = {https://www.intel.com/content/www/us/en/develop/tools/math-kernel-library/benchmarks.html},
	abstract = {View performance benchmark charts for Intel® Math Kernel Library functions.},
	language = {en},
	urldate = {2020-08-10},
	journal = {Intel},
}

@techreport{williams_roofline_2009,
	title = {Roofline: {An} {Insightful} {Visual} {Performance} {Model} for {Floating}-{Point} {Programs} and {Multicore} {Architectures}},
	shorttitle = {Roofline},
	url = {http://www.osti.gov/servlets/purl/1407078/},
	abstract = {We propose an easy-to-understand, visual performance model that offers insights to programmers and architects on improving parallel software and hardware for floating point computations.},
	language = {en},
	number = {1407078},
	urldate = {2020-08-10},
	author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
	month = sep,
	year = {2009},
	doi = {10.2172/1407078},
	pages = {1407078},
}

@misc{noauthor_empirical_nodate,
	title = {Empirical {Roofline} {Tool} ({ERT})},
	url = {https://crd.lbl.gov/departments/computer-science/par/research/roofline/software/ert/},
	urldate = {2020-07-17},
}

@misc{kannan_sudarsunkannanmemlatency_2020,
	title = {{SudarsunKannan}/memlatency},
	url = {https://github.com/SudarsunKannan/memlatency},
	abstract = {Code to measure cache line size, processor cache size, and measure memory latency},
	urldate = {2020-07-17},
	author = {Kannan, Sudarsun},
	month = jul,
	year = {2020},
	note = {original-date: 2016-02-10T01:56:53Z},
}

@article{denoyelle_automatic_2016,
	title = {Automatic {Cache} {Aware} {Roofline} {Model} {Building} and {Validation} {Using} {Topology} {Detection}},
	abstract = {The ever growing complexity of high performance computing systems imposes signiﬁcant challenges to exploit as much as possible their computational and memory resources. Recently, the Cache-aware Rooﬂine Model has gained popularity due to its simplicity when modeling multi-cores with complex memory hierarchy, characterizing applications bottlenecks, and quantifying achieved or remaining improvements. In this short paper we involve hardware locality topology detection to build the Cache Aware Rooﬂine Model for modern processors in an open-source locality-aware tool. The proposed tool also includes a set of speciﬁc micro-benchmarks to assess the micro-architecture performance upper-bounds. The experimental results show that by relying on the proposed tool, it was possible to reach near-theoretical bounds of an Intel 3770K processor, thus proving the effectiveness of the modeling methodology.},
	language = {en},
	number = {1},
	author = {Denoyelle, Nicolas and Ilic, Aleksandar and Goglin, Brice and Sousa, Leonel and Jeannot, Emmanuel},
	year = {2016},
	pages = {7},
}

@misc{noauthor_c_nodate,
	title = {c - {Cache} size estimation on your system?},
	url = {https://stackoverflow.com/questions/21299488/cache-size-estimation-on-your-system},
	urldate = {2020-07-17},
	journal = {Stack Overflow},
	note = {Library Catalog: stackoverflow.com},
}

@misc{noauthor_exploring_nodate,
	title = {Exploring {GPU} performance, power and energy-efficiency bounds with {Cache}-aware {Roofline} {Modeling} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore.ieee.org/abstract/document/7975297?casa_token=kktmHUhc6lcAAAAA:x8UxgATE4gIgWBh6IE2UO0GnKIP-0Kh-ZAOGX7m7KlR6APYyA6EjUkSGgyQvhmybMKafurA},
	urldate = {2020-07-17},
}

@misc{noauthor_beyond_nodate,
	title = {Beyond the {Roofline}: {Cache}-{Aware} {Power} and {Energy}-{Efficiency} {Modeling} for {Multi}-{Cores} - {IEEE} {Journals} \& {Magazine}},
	url = {https://ieeexplore.ieee.org/abstract/document/7493653?casa_token=pzvK1CoJs9sAAAAA:k4GGboT8g1oPOjeSKfeI2EEGCP85lctx5smSPIsKmR1SsxTVyH4MsGIbbhWCZ_BUjU_l6rw},
	urldate = {2020-07-17},
}

@misc{noauthor_extending_nodate,
	title = {Extending the roofline model: {Bottleneck} analysis with microarchitectural constraints - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore.ieee.org/abstract/document/6983061},
	urldate = {2020-07-17},
}

@misc{noauthor_benchmark-based_nodate,
	title = {A benchmark-based performance model for memory-bound {HPC} applications - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore.ieee.org/abstract/document/6903790?casa_token=XZf4LcFD6_8AAAAA:WI8ViOmYg1Efqxa1lFW2Z0qHP9phENL3fylgFUg-QefXnm6EG7mfCdYcsMsnWAoSaQkpygs},
	urldate = {2020-07-17},
}

@incollection{snoek_practical_2012,
	title = {Practical {Bayesian} {Optimization} of {Machine} {Learning} {Algorithms}},
	url = {http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf},
	abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a “black art” requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm’s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
	urldate = {2020-06-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {2951--2959},
}

@article{snoek_practical_nodate,
	title = {Practical {Bayesian} {Optimization} of {Machine} {Learning} {Algorithms}},
	abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a “black art” requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm’s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
	language = {en},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
	pages = {9},
}

@article{snoek_practical_nodate-1,
	title = {Practical {Bayesian} {Optimization} of {Machine} {Learning} {Algorithms}},
	abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a “black art” requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm’s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
	language = {en},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
	pages = {9},
}

@inproceedings{arcuri_practical_2011,
	address = {Waikiki, Honolulu, HI, USA},
	title = {A practical guide for using statistical tests to assess randomized algorithms in software engineering},
	isbn = {978-1-4503-0445-0},
	url = {http://portal.acm.org/citation.cfm?doid=1985793.1985795},
	doi = {10.1145/1985793.1985795},
	abstract = {Randomized algorithms have been used to successfully address many different types of software engineering problems. This type of algorithms employ a degree of randomness as part of their logic. Randomized algorithms are useful for difﬁcult problems where a precise solution cannot be derived in a deterministic way within reasonable time. However, randomized algorithms produce different results on every run when applied to the same problem instance. It is hence important to assess the effectiveness of randomized algorithms by collecting data from a large enough number of runs. The use of rigorous statistical tests is then essential to provide support to the conclusions derived by analyzing such data. In this paper, we provide a systematic review of the use of randomized algorithms in selected software engineering venues in 2009. Its goal is not to perform a complete survey but to get a representative snapshot of current practice in software engineering research. We show that randomized algorithms are used in a signiﬁcant percentage of papers but that, in most cases, randomness is not properly accounted for. This casts doubts on the validity of most empirical results assessing randomized algorihtms. There are numerous statistical tests, based on different assumptions, and it is not always clear when and how to use these tests. We hence provide practical guidelines to support empirical research on randomized algorithms in software engineering.},
	language = {en},
	urldate = {2020-06-14},
	booktitle = {Proceeding of the 33rd international conference on {Software} engineering - {ICSE} '11},
	publisher = {ACM Press},
	author = {Arcuri, Andrea and Briand, Lionel},
	year = {2011},
	pages = {1},
}

@article{stone_opencl_2010,
	title = {{OpenCL}: {A} {Parallel} {Programming} {Standard} for {Heterogeneous} {Computing} {Systems}},
	volume = {12},
	issn = {1558-366X},
	shorttitle = {{OpenCL}},
	doi = {10.1109/MCSE.2010.69},
	abstract = {The OpenCL standard offers a common API for program execution on systems composed of different types of computational devices such as multicore CPUs, GPUs, or other accelerators.},
	number = {3},
	journal = {Computing in Science Engineering},
	author = {Stone, John E. and Gohara, David and Shi, Guochun},
	month = may,
	year = {2010},
	note = {Conference Name: Computing in Science Engineering},
	keywords = {API, Computer architecture, Computer interfaces, Concurrent computing, GPU, Hardware, High performance computing, Kernel, Microprocessors, OpenCL standard, Parallel programming, Runtime, Software standards, application program interfaces, computational devices, computer graphic equipment, coprocessors, heterogeneous computing systems, multicore CPU, parallel programming, parallel programming standard, program execution},
	pages = {66--73},
}

@inproceedings{falch_imagecl_2016,
	title = {{ImageCL}: {An} image processing language for performance portability on heterogeneous systems},
	shorttitle = {{ImageCL}},
	doi = {10.1109/HPCSim.2016.7568385},
	abstract = {Modern computer systems typically conbine multicore CPUs with accelerators like GPUs for inproved performance and energy efficiency. However, these systems suffer from poor performance portability - code tuned for one device must be retuned to achieve high performance on another. Image processing is increasing in importance, with applications ranging from seismology and medicine to Photoshop. Based on our experience with medical image processing, we propose ImageCL, a high-level domain-specific language and source-to-source compiler, targeting heterogeneous hardware. ImageCL resembles OpenCL, but abstracts away performance optimization details, allowing the programmer to focus on algorithm development, rather than performance tuning. The latter is left to our source-to-source compiler and auto-tuner. From high-level ImageCL kernels, our source-to-source compiler can generate multiple OpenCL implementations with different optimizations applied. We rely on auto-tuning rather than machine models or expert programmer knowledge to determine which optimizations to apply, making our tuning procedure highly robust. Furthermore, we can generate high performing implementations for different devices from a single source code, thereby improving performance portability. We evaluate our approach on three image processing benchmarks, on different GPU and CPU devices, and are able to outperform other state of the art solutions in several cases, achieving speedups of up to 4.57x.},
	booktitle = {2016 {International} {Conference} on {High} {Performance} {Computing} {Simulation} ({HPCS})},
	author = {Falch, Thomas L. and Elster, Anne C.},
	month = jul,
	year = {2016},
	keywords = {CPU device, GPU device, Graphics processing units, Hardware, Image processing, Kernel, OpenCL, Optimization, Performance evaluation, Pipelines, accelerators, auto-tuning, energy efficiency, graphics processing units, heterogeneous computing, heterogeneous hardware, heterogeneous systems, high-level ImageCL kernels, high-level domain-specific language, image processing, image processing language, medical image processing, multicore CPUs, performance optimization, performance portability, program compilers, source-to-source compilation, source-to-source compiler},
	pages = {562--569},
}

@article{wilkinson_history_2009,
	title = {The {History} of the {Cluster} {Heat} {Map}},
	volume = {63},
	issn = {0003-1305},
	url = {https://doi.org/10.1198/tas.2009.0033},
	doi = {10.1198/tas.2009.0033},
	abstract = {The cluster heat map is an ingenious display that simultaneously reveals row and column hierarchical cluster structure in a data matrix. It consists of a rectangular tiling, with each tile shaded on a color scale to represent the value of the corresponding element of the data matrix. The rows (columns) of the tiling are ordered such that similar rows (columns) are near each other. On the vertical and horizontal margins of the tiling are hierarchical cluster trees. This cluster heat map is a synthesis of several different graphic displays developed by statisticians over more than a century. We locate the earliest sources of this display in late 19th century publications, and trace a diverse 20th century statistical literature that provided a foundation for this most widely used of all bioinformatics displays.},
	number = {2},
	urldate = {2020-06-10},
	journal = {The American Statistician},
	author = {Wilkinson, Leland and Friendly, Michael},
	month = may,
	year = {2009},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/tas.2009.0033},
	keywords = {Cluster analysis, Heatmap, Microarray, Visualization},
	pages = {179--184},
}

@article{cureton_rank-biserial_1956,
	title = {Rank-biserial correlation},
	volume = {21},
	issn = {0033-3123, 1860-0980},
	url = {http://link.springer.com/10.1007/BF02289138},
	doi = {10.1007/BF02289138},
	language = {en},
	number = {3},
	urldate = {2020-06-10},
	journal = {Psychometrika},
	author = {Cureton, Edward E.},
	month = sep,
	year = {1956},
	pages = {287--290},
}

@article{mann_test_1947,
	title = {On a {Test} of {Whether} one of {Two} {Random} {Variables} is {Stochastically} {Larger} than the {Other}},
	doi = {10.1214/aoms/1177730491},
	abstract = {Semantic Scholar extracted view of "On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other" by Henry B. Mann et al.},
	author = {Mann, Henry B. and Whitney, D. R.},
	year = {1947},
}

@article{settles_active_nodate,
	title = {Active {Learning} {Literature} {Survey}},
	language = {en},
	author = {Settles, Burr},
	pages = {47},
}

@inproceedings{bergstra_machine_2012,
	title = {Machine learning for predictive auto-tuning with boosted regression trees},
	doi = {10.1109/InPar.2012.6339587},
	abstract = {The rapidly evolving landscape of multicore architectures makes the construction of efficient libraries a daunting task. A family of methods known collectively as “auto-tuning” has emerged to address this challenge. Two major approaches to auto-tuning are empirical and model-based: empirical autotuning is a generic but slow approach that works by measuring runtimes of candidate implementations, model-based auto-tuning predicts those runtimes using simplified abstractions designed by hand. We show that machine learning methods for non-linear regression can be used to estimate timing models from data, capturing the best of both approaches. A statistically-derived model offers the speed of a model-based approach, with the generality and simplicity of empirical auto-tuning. We validate our approach using the filterbank correlation kernel described in Pinto and Cox [2012], where we find that 0.1 seconds of hill climbing on the regression model (“predictive auto-tuning”) can achieve almost the same speed-up as is brought by minutes of empirical auto-tuning. Our approach is not specific to filterbank correlation, nor even to GPU kernel auto-tuning, and can be applied to almost any templated-code optimization problem, spanning a wide variety of problem types, kernel types, and platforms.},
	booktitle = {2012 {Innovative} {Parallel} {Computing} ({InPar})},
	author = {Bergstra, James and Pinto, Nicolas and Cox, David},
	month = may,
	year = {2012},
	keywords = {Correlation, Graphics processing unit, Instruction sets, Kernel, Libraries, Optimization, Regression tree analysis, boosted regression trees, candidate implementation runtime measurement, channel bank filters, empirical autotuning, filterbank correlation kernel, learning (artificial intelligence), machine learning, model-based approach, model-based autotuning, multicore architectures, multiprocessing systems, nonlinear regression, predictive autotuning, regression analysis, statistically-derived model, templated-code optimization problem, time 0.1 s, timing model estimation, trees (mathematics)},
	pages = {1--9},
}

@inproceedings{bergstra_machine_2012-1,
	title = {Machine learning for predictive auto-tuning with boosted regression trees},
	doi = {10.1109/InPar.2012.6339587},
	abstract = {The rapidly evolving landscape of multicore architectures makes the construction of efficient libraries a daunting task. A family of methods known collectively as “auto-tuning” has emerged to address this challenge. Two major approaches to auto-tuning are empirical and model-based: empirical autotuning is a generic but slow approach that works by measuring runtimes of candidate implementations, model-based auto-tuning predicts those runtimes using simplified abstractions designed by hand. We show that machine learning methods for non-linear regression can be used to estimate timing models from data, capturing the best of both approaches. A statistically-derived model offers the speed of a model-based approach, with the generality and simplicity of empirical auto-tuning. We validate our approach using the filterbank correlation kernel described in Pinto and Cox [2012], where we find that 0.1 seconds of hill climbing on the regression model (“predictive auto-tuning”) can achieve almost the same speed-up as is brought by minutes of empirical auto-tuning. Our approach is not specific to filterbank correlation, nor even to GPU kernel auto-tuning, and can be applied to almost any templated-code optimization problem, spanning a wide variety of problem types, kernel types, and platforms.},
	booktitle = {2012 {Innovative} {Parallel} {Computing} ({InPar})},
	author = {Bergstra, James and Pinto, Nicolas and Cox, David},
	month = may,
	year = {2012},
	keywords = {Correlation, Graphics processing unit, Instruction sets, Kernel, Libraries, Optimization, Regression tree analysis, boosted regression trees, candidate implementation runtime measurement, channel bank filters, empirical autotuning, filterbank correlation kernel, learning (artificial intelligence), machine learning, model-based approach, model-based autotuning, multicore architectures, multiprocessing systems, nonlinear regression, predictive autotuning, regression analysis, statistically-derived model, templated-code optimization problem, time 0.1 s, timing model estimation, trees (mathematics)},
	pages = {1--9},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	language = {en},
	number = {5},
	urldate = {2020-06-08},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	keywords = {Back-propagation networks, Feedforward networks, Mapping networks, Network representation capability, Sigma-Pi networks, Squashing functions, Stone-Weierstrass Theorem, Universal approximation},
	pages = {359--366},
}

@article{hornik_multilayer_1989-1,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	language = {en},
	number = {5},
	urldate = {2020-06-08},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	pages = {359--366},
}

@article{svozil_introduction_1997,
	title = {Introduction to multi-layer feed-forward neural networks},
	volume = {39},
	issn = {01697439},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169743997000610},
	doi = {10.1016/S0169-7439(97)00061-0},
	abstract = {Basic definitions concerning the multi-layer feed-forward neural networks are given. The back-propagation training algorithm is explained. Partial derivatives of the objective function with respect to the weight and threshold coefficients are derived. These derivatives are valuable for an adaptation process of the considered neural network. Training and generalisation of multi-layer feed-forward neural networks are discussed. Improvements of the standard back-propagation algorithm are reviewed. Example of the use of multi-layer feed-forward neural networks for prediction of carbon-13 NMR chemical shifts of alkanes is given. Further applications of neural networks in chemistry are reviewed. Advantages and disadvantages of multilayer feed-forward neural networks are discussed. 0 1997 Elsevier Science B.V.},
	language = {en},
	number = {1},
	urldate = {2020-06-08},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	author = {Svozil, Daniel and Kvasnicka, Vladimír and Pospichal, Jir̂í},
	month = nov,
	year = {1997},
	pages = {43--62},
}

@article{svozil_introduction_1997-1,
	title = {Introduction to multi-layer feed-forward neural networks},
	volume = {39},
	issn = {0169-7439},
	url = {http://www.sciencedirect.com/science/article/pii/S0169743997000610},
	doi = {10.1016/S0169-7439(97)00061-0},
	abstract = {Basic definitions concerning the multi-layer feed-forward neural networks are given. The back-propagation training algorithm is explained. Partial derivatives of the objective function with respect to the weight and threshold coefficients are derived. These derivatives are valuable for an adaptation process of the considered neural network. Training and generalisation of multi-layer feed-forward neural networks are discussed. Improvements of the standard back-propagation algorithm are reviewed. Example of the use of multi-layer feed-forward neural networks for prediction of carbon-13 NMR chemical shifts of alkanes is given. Further applications of neural networks in chemistry are reviewed. Advantages and disadvantages of multilayer feed-forward neural networks are discussed.},
	language = {en},
	number = {1},
	urldate = {2020-06-08},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	author = {Svozil, Daniel and Kvasnicka, Vladimír and Pospichal, Jir̂í},
	month = nov,
	year = {1997},
	keywords = {Back-propagation network, Neural networks},
	pages = {43--62},
}

@article{loh_classification_2011,
	title = {Classification and regression trees},
	volume = {1},
	copyright = {Copyright © 2011 John Wiley \& Sons, Inc.},
	issn = {1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.8},
	doi = {10.1002/widm.8},
	abstract = {Classification and regression trees are machine-learning methods for constructing prediction models from data. The models are obtained by recursively partitioning the data space and fitting a simple prediction model within each partition. As a result, the partitioning can be represented graphically as a decision tree. Classification trees are designed for dependent variables that take a finite number of unordered values, with prediction error measured in terms of misclassification cost. Regression trees are for dependent variables that take continuous or ordered discrete values, with prediction error typically measured by the squared difference between the observed and predicted values. This article gives an introduction to the subject by reviewing some widely available algorithms and comparing their capabilities, strengths, and weakness in two examples. © 2011 John Wiley \& Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 14-23 DOI: 10.1002/widm.8 This article is categorized under: Technologies {\textgreater} Classification Technologies {\textgreater} Machine Learning Technologies {\textgreater} Prediction Algorithmic Development {\textgreater} Statistics},
	language = {en},
	number = {1},
	urldate = {2020-06-08},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Loh, Wei-Yin},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.8},
	pages = {14--23},
}

@article{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2020-05-31},
	journal = {arXiv:2005.14165 [cs]},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = may,
	year = {2020},
	note = {arXiv: 2005.14165},
	keywords = {Computer Science - Computation and Language},
}

@book{ausiello_complexity_2012,
	title = {Complexity and {Approximation}: {Combinatorial} {Optimization} {Problems} and {Their} {Approximability} {Properties}},
	isbn = {978-3-642-58412-1},
	shorttitle = {Complexity and {Approximation}},
	abstract = {N COMPUTER applications we are used to live with approximation. Var I ious notions of approximation appear, in fact, in many circumstances. One notable example is the type of approximation that arises in numer ical analysis or in computational geometry from the fact that we cannot perform computations with arbitrary precision and we have to truncate the representation of real numbers. In other cases, we use to approximate com plex mathematical objects by simpler ones: for example, we sometimes represent non-linear functions by means of piecewise linear ones. The need to solve difficult optimization problems is another reason that forces us to deal with approximation. In particular, when a problem is computationally hard (i. e. , the only way we know to solve it is by making use of an algorithm that runs in exponential time), it may be practically unfeasible to try to compute the exact solution, because it might require months or years of machine time, even with the help of powerful parallel computers. In such cases, we may decide to restrict ourselves to compute a solution that, though not being an optimal one, nevertheless is close to the optimum and may be determined in polynomial time. We call this type of solution an approximate solution and the corresponding algorithm a polynomial-time approximation algorithm. Most combinatorial optimization problems of great practical relevance are, indeed, computationally intractable in the above sense. In formal terms, they are classified as Np-hard optimization problems.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Ausiello, Giorgio and Crescenzi, Pierluigi and Gambosi, Giorgio and Kann, Viggo and Marchetti-Spaccamela, Alberto and Protasi, Marco},
	month = dec,
	year = {2012},
	note = {Google-Books-ID: 8j6qCAAAQBAJ},
	keywords = {Business \& Economics / Business Mathematics, Business \& Economics / Information Management, Business \& Economics / Operations Research, Computers / Data Processing, Computers / Information Technology, Computers / Programming / Algorithms, Computers / Programming / General, Mathematics / Counting \& Numeration, Mathematics / Discrete Mathematics, Mathematics / Numerical Analysis},
}

@article{zoph_neural_2017,
	title = {Neural {Architecture} {Search} with {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1611.01578},
	abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
	urldate = {2020-05-26},
	journal = {arXiv:1611.01578 [cs]},
	author = {Zoph, Barret and Le, Quoc V.},
	month = feb,
	year = {2017},
	note = {arXiv: 1611.01578},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{danalis_scalable_2010,
	address = {Pittsburgh, Pennsylvania, USA},
	series = {{GPGPU}-3},
	title = {The {Scalable} {Heterogeneous} {Computing} ({SHOC}) benchmark suite},
	isbn = {978-1-60558-935-0},
	url = {https://doi.org/10.1145/1735688.1735702},
	doi = {10.1145/1735688.1735702},
	abstract = {Scalable heterogeneous computing systems, which are composed of a mix of compute devices, such as commodity multicore processors, graphics processors, reconfigurable processors, and others, are gaining attention as one approach to continuing performance improvement while managing the new challenge of energy efficiency. As these systems become more common, it is important to be able to compare and contrast architectural designs and programming systems in a fair and open forum. To this end, we have designed the Scalable HeterOgeneous Computing benchmark suite (SHOC). SHOC's initial focus is on systems containing graphics processing units (GPUs) and multi-core processors, and on the new OpenCL programming standard. SHOC is a spectrum of programs that test the performance and stability of these scalable heterogeneous computing systems. At the lowest level, SHOC uses microbenchmarks to assess architectural features of the system. At higher levels, SHOC uses application kernels to determine system-wide performance including many system features such as intranode and internode communication among devices. SHOC includes benchmark implementations in both OpenCL and CUDA in order to provide a comparison of these programming models.},
	urldate = {2020-05-26},
	booktitle = {Proceedings of the 3rd {Workshop} on {General}-{Purpose} {Computation} on {Graphics} {Processing} {Units}},
	publisher = {Association for Computing Machinery},
	author = {Danalis, Anthony and Marin, Gabriel and McCurdy, Collin and Meredith, Jeremy S. and Roth, Philip C. and Spafford, Kyle and Tipparaju, Vinod and Vetter, Jeffrey S.},
	month = mar,
	year = {2010},
	keywords = {GPGPU, benchmarking, graphics processors, performance},
	pages = {63--74},
}

@article{biedenkapp_dynamic_nodate,
	title = {Dynamic {Algorithm} {Conﬁguration}: {Foundation} of a {New} {Meta}-{Algorithmic} {Framework}},
	abstract = {The performance of many algorithms in the ﬁelds of hard combinatorial problem solving, machine learning or AI in general depends on parameter tuning. Automated methods have been proposed to alleviate users from the tedious and error-prone task of manually searching for performance-optimized conﬁgurations across a set of problem instances. However, there is still a lot of untapped potential through adjusting an algorithm’s parameters online since different parameter values can be optimal at different stages of the algorithm. Prior work showed that reinforcement learning is an effective approach to learn policies for online adjustments of algorithm parameters in a data-driven way. We extend that approach by formulating the resulting dynamic algorithm conﬁguration as a contextual MDP, such that RL not only learns a policy for a single instance, but across a set of instances. To lay the foundation for studying dynamic algorithm conﬁguration with RL in a controlled setting, we propose white-box benchmarks covering major aspects that make dynamic algorithm conﬁguration a hard problem in practice and study the performance of various types of conﬁguration strategies for them. On these white-box benchmarks, we show that (i) RL is a robust candidate for learning conﬁguration policies, outperforming standard parameter optimization approaches, such as classical algorithm conﬁguration; (ii) based on function approximation, RL agents can learn to generalize to new types of instances; and (iii) self-paced learning can substantially improve the performance by selecting a useful sequence of training instances automatically.},
	language = {en},
	author = {Biedenkapp, Andre and Bozkurt, H Furkan and Eimer, Theresa and Hutter, Frank and Lindauer, Marius},
	pages = {8},
}

@article{hutter_sequential_nodate,
	title = {Sequential {Model}-{Based} {Optimization} for {General} {Algorithm} {Conﬁguration} (extended version)},
	abstract = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modiﬁed to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm conﬁguration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the ﬁrst time to general algorithm conﬁguration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm conﬁguration procedure by optimizing a local search and a tree search solver for the propositional satisﬁability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best conﬁguration approach.},
	language = {en},
	author = {Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
	pages = {24},
}

@inproceedings{frigo_fftw_1998,
	title = {{FFTW}: an adaptive software architecture for the {FFT}},
	volume = {3},
	shorttitle = {{FFTW}},
	doi = {10.1109/ICASSP.1998.681704},
	abstract = {FFT literature has been mostly concerned with minimizing the number of floating-point operations performed by an algorithm. Unfortunately, on present-day microprocessors this measure is far less important than it used to be, and interactions with the processor pipeline and the memory hierarchy have a larger impact on performance. Consequently, one must know the details of a computer architecture in order to design a fast algorithm. In this paper, we propose an adaptive FFT program that tunes the computation automatically for any particular hardware. We compared our program, called FFTW, with over 40 implementations of the FFT on 7 machines. Our tests show that FFTW's self-optimizing approach usually yields significantly better performance than all other publicly available software. FFTW also compares favorably with machine-specific, vendor-optimized libraries.},
	booktitle = {Proceedings of the 1998 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}, {ICASSP} '98 ({Cat}. {No}.{98CH36181})},
	author = {Frigo, M. and Johnson, S.G.},
	month = may,
	year = {1998},
	note = {ISSN: 1520-6149},
	keywords = {Algorithm design and analysis, Automatic testing, Computer architecture, DFT, FFT, FFTW, Hardware, Microprocessors, Pipelines, Software architecture, Software libraries, Software performance, Software testing, adaptive FFT program, adaptive software architecture, adaptive systems, computer architecture, discrete Fourier transforms, fast Fourier transforms, fast algorithm, floating-point operations, mathematics computing, memory hierarchy, performance, processor pipeline, self-optimizing approach},
	pages = {1381--1384 vol.3},
}

@article{clint_whaley_automated_2001,
	series = {New {Trends} in {High} {Performance} {Computing}},
	title = {Automated empirical optimizations of software and the {ATLAS} project},
	volume = {27},
	issn = {0167-8191},
	url = {http://www.sciencedirect.com/science/article/pii/S0167819100000879},
	doi = {10.1016/S0167-8191(00)00087-9},
	abstract = {This paper describes the automatically tuned linear algebra software (ATLAS) project, as well as the fundamental principles that underly it. ATLAS is an instantiation of a new paradigm in high performance library production and maintenance, which we term automated empirical optimization of software (AEOS); this style of library management has been created in order to allow software to keep pace with the incredible rate of hardware advancement inherent in Moore's Law. ATLAS is the application of this new paradigm to linear algebra software, with the present emphasis on the basic linear algebra subprograms (BLAS), a widely used, performance-critical, linear algebra kernel library.},
	language = {en},
	number = {1},
	urldate = {2020-05-22},
	journal = {Parallel Computing},
	author = {Clint Whaley, R. and Petitet, Antoine and Dongarra, Jack J.},
	month = jan,
	year = {2001},
	keywords = {AEOS, ATLAS, BLAS, Portable performance},
	pages = {3--35},
}

@incollection{wilcoxon_individual_1992,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Individual {Comparisons} by {Ranking} {Methods}},
	isbn = {978-1-4612-4380-9},
	url = {https://doi.org/10.1007/978-1-4612-4380-9_16},
	abstract = {The comparison of two treatments generally falls into one of the following two categories: (a) we may have a number of replications for each of the two treatments, which are unpaired, or (b) we may have a number of paired comparisons leading to a series of differences, some of which may be positive and some negative. The appropriate methods for testing the significance of the differences of the means in these two cases are described in most of the textbooks on statistical methods.},
	language = {en},
	urldate = {2020-05-15},
	booktitle = {Breakthroughs in {Statistics}: {Methodology} and {Distribution}},
	publisher = {Springer},
	author = {Wilcoxon, Frank},
	editor = {Kotz, Samuel and Johnson, Norman L.},
	year = {1992},
	doi = {10.1007/978-1-4612-4380-9_16},
	keywords = {Individual Comparison, Negative Difference, Paired Comparison, Ranking Method, Serial Number},
	pages = {196--202},
}

@article{vallat_pingouin_2018,
	title = {Pingouin: statistics in {Python}},
	volume = {3},
	issn = {2475-9066},
	shorttitle = {Pingouin},
	url = {http://joss.theoj.org/papers/10.21105/joss.01026},
	doi = {10.21105/joss.01026},
	number = {31},
	urldate = {2020-05-15},
	journal = {Journal of Open Source Software},
	author = {Vallat, Raphael},
	month = nov,
	year = {2018},
	pages = {1026},
}

@article{vargha_critique_2000,
	title = {A {Critique} and {Improvement} of the "{CL}" {Common} {Language} {Effect} {Size} {Statistics} of {McGraw} and {Wong}},
	volume = {25},
	issn = {1076-9986},
	url = {https://www.jstor.org/stable/1165329},
	doi = {10.2307/1165329},
	abstract = {McGraw and Wong (1992) described an appealing index of effect size, called "CL", which measures the difference between two populations in terms of the probability that a score sampled at random from the first population will be greater than a score sampled at random from the second. McGraw and Wong introduced this "common language effect size statistic" for normal distributions and then proposed an approximate estimation for any continuous distribution. In addition, they generalized "CL" to the n-group case, the correlated samples case, and the discrete values case. In the current paper a different generalization of "CL" called the A measure of stochastic superiority, is proposed, which may be directly applied for any discrete or continuous variable that is at least ordinally scaled. Exact methods for point and interval estimation as well as the significance tests of the A = .5 hypothesis are provided. New generalizations of "CL" are provided for the multi-group and correlated samples cases.},
	number = {2},
	urldate = {2020-05-15},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Vargha, András and Delaney, Harold D.},
	year = {2000},
	note = {Publisher: [American Educational Research Association, Sage Publications, Inc., American Statistical Association]},
	pages = {101--132},
}

@article{kerby_simple_2014,
	title = {The {Simple} {Difference} {Formula}: {An} {Approach} to {Teaching} {Nonparametric} {Correlation}},
	volume = {3},
	issn = {2165-2228},
	shorttitle = {The {Simple} {Difference} {Formula}},
	url = {https://journals.sagepub.com/doi/abs/10.2466/11.IT.3.1},
	doi = {10.2466/11.IT.3.1},
	abstract = {Although teaching effect sizes is important, many statistics texts omit the topic for the Mann-Whitney U test and the Wilcoxon signed-rank test. To address this omission, this paper introduces the simple difference formula. The formula states that the correlation equals the simple difference between the proportion of favorable and unfavorable evidence; in symbols this is r = f – u. For the Mann-Whitney U, the evidence consists of pairs. For the signed-rank test, the evidence consists of rank sums. Also, the formula applies to the Binomial Effect Size Display. The formula r = f – u means that a correlation r can yield a prediction so that the proportion correct is f and the proportion incorrect is u.},
	language = {en},
	urldate = {2020-05-15},
	journal = {Comprehensive Psychology},
	author = {Kerby, Dave S.},
	month = jan,
	year = {2014},
	note = {Publisher: SAGE Publications Inc},
	pages = {11.IT.3.1},
}

@article{eggensperger_towards_nodate,
	title = {Towards an {Empirical} {Foundation} for {Assessing} {Bayesian} {Optimization} of {Hyperparameters}},
	abstract = {Progress in practical Bayesian optimization is hampered by the fact that the only available standard benchmarks are artiﬁcial test functions that are not representative of practical applications. To alleviate this problem, we introduce a library of benchmarks from the prominent application of hyperparameter optimization and use it to compare Spearmint, TPE, and SMAC, three recent Bayesian optimization methods for hyperparameter optimization.},
	language = {en},
	author = {Eggensperger, Katharina and Feurer, Matthias and Hutter, Frank and Bergstra, James and Snoek, Jasper and Hoos, Holger H and Leyton-Brown, Kevin},
	pages = {5},
}

@article{biedenkapp_dynamic_nodate-1,
	title = {Dynamic {Algorithm} {Conﬁguration}: {Foundation} of a {New} {Meta}-{Algorithmic} {Framework}},
	abstract = {The performance of many algorithms in the ﬁelds of hard combinatorial problem solving, machine learning or AI in general depends on parameter tuning. Automated methods have been proposed to alleviate users from the tedious and error-prone task of manually searching for performance-optimized conﬁgurations across a set of problem instances. However, there is still a lot of untapped potential through adjusting an algorithm’s parameters online since different parameter values can be optimal at different stages of the algorithm. Prior work showed that reinforcement learning is an effective approach to learn policies for online adjustments of algorithm parameters in a data-driven way. We extend that approach by formulating the resulting dynamic algorithm conﬁguration as a contextual MDP, such that RL not only learns a policy for a single instance, but across a set of instances. To lay the foundation for studying dynamic algorithm conﬁguration with RL in a controlled setting, we propose white-box benchmarks covering major aspects that make dynamic algorithm conﬁguration a hard problem in practice and study the performance of various types of conﬁguration strategies for them. On these white-box benchmarks, we show that (i) RL is a robust candidate for learning conﬁguration policies, outperforming standard parameter optimization approaches, such as classical algorithm conﬁguration; (ii) based on function approximation, RL agents can learn to generalize to new types of instances; and (iii) self-paced learning can substantially improve the performance by selecting a useful sequence of training instances automatically.},
	language = {en},
	author = {Biedenkapp, Andre and Bozkurt, H Furkan and Eimer, Theresa and Hutter, Frank and Lindauer, Marius},
	pages = {8},
}

@incollection{coello_sequential_2011,
	address = {Berlin, Heidelberg},
	title = {Sequential {Model}-{Based} {Optimization} for {General} {Algorithm} {Configuration}},
	volume = {6683},
	isbn = {978-3-642-25565-6 978-3-642-25566-3},
	url = {http://link.springer.com/10.1007/978-3-642-25566-3_40},
	abstract = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modiﬁed to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm conﬁguration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the ﬁrst time to general algorithm conﬁguration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm conﬁguration procedure by optimizing a local search and a tree search solver for the propositional satisﬁability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best conﬁguration approach.},
	language = {en},
	urldate = {2020-05-08},
	booktitle = {Learning and {Intelligent} {Optimization}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
	editor = {Coello, Carlos A. Coello},
	year = {2011},
	doi = {10.1007/978-3-642-25566-3_40},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {507--523},
}

@article{cummins_programl_2020,
	title = {{ProGraML}: {Graph}-based {Deep} {Learning} for {Program} {Optimization} and {Analysis}},
	shorttitle = {{ProGraML}},
	url = {http://arxiv.org/abs/2003.10536},
	abstract = {The increasing complexity of computing systems places a tremendous burden on optimizing compilers, requiring ever more accurate and aggressive optimizations. Machine learning offers significant benefits for constructing optimization heuristics but there remains a gap between what state-of-the-art methods achieve and the performance of an optimal heuristic. Closing this gap requires improvements in two key areas: a representation that accurately captures the semantics of programs, and a model architecture with sufficient expressiveness to reason about this representation. We introduce ProGraML - Program Graphs for Machine Learning - a novel graph-based program representation using a low level, language agnostic, and portable format; and machine learning models capable of performing complex downstream tasks over these graphs. The ProGraML representation is a directed attributed multigraph that captures control, data, and call relations, and summarizes instruction and operand types and ordering. Message Passing Neural Networks propagate information through this structured representation, enabling whole-program or per-vertex classification tasks. ProGraML provides a general-purpose program representation that equips learnable models to perform the types of program analysis that are fundamental to optimization. To this end, we evaluate the performance of our approach first on a suite of traditional compiler analysis tasks: control flow reachability, dominator trees, data dependencies, variable liveness, and common subexpression detection. On a benchmark dataset of 250k LLVM-IR files covering six source programming languages, ProGraML achieves an average 94.0 F1 score, significantly outperforming the state-of-the-art approaches. We then apply our approach to two high-level tasks - heterogeneous device mapping and program classification - setting new state-of-the-art performance in both.},
	urldate = {2020-04-21},
	journal = {arXiv:2003.10536 [cs, stat]},
	author = {Cummins, Chris and Fisches, Zacharias V. and Ben-Nun, Tal and Hoefler, Torsten and Leather, Hugh},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.10536},
	keywords = {Computer Science - Machine Learning, Computer Science - Performance, Computer Science - Programming Languages, Statistics - Machine Learning},
}

@article{jomaa_hyp-rl_2019,
	title = {Hyp-{RL} : {Hyperparameter} {Optimization} by {Reinforcement} {Learning}},
	shorttitle = {Hyp-{RL}},
	url = {http://arxiv.org/abs/1906.11527},
	abstract = {Hyperparameter tuning is an omnipresent problem in machine learning as it is an integral aspect of obtaining the state-of-the-art performance for any model. Most often, hyperparameters are optimized just by training a model on a grid of possible hyperparameter values and taking the one that performs best on a validation sample (grid search). More recently, methods have been introduced that build a so-called surrogate model that predicts the validation loss for a specific hyperparameter setting, model and dataset and then sequentially select the next hyperparameter to test, based on a heuristic function of the expected value and the uncertainty of the surrogate model called acquisition function (sequential model-based Bayesian optimization, SMBO). In this paper we model the hyperparameter optimization problem as a sequential decision problem, which hyperparameter to test next, and address it with reinforcement learning. This way our model does not have to rely on a heuristic acquisition function like SMBO, but can learn which hyperparameters to test next based on the subsequent reduction in validation loss they will eventually lead to, either because they yield good models themselves or because they allow the hyperparameter selection policy to build a better surrogate model that is able to choose better hyperparameters later on. Experiments on a large battery of 50 data sets demonstrate that our method outperforms the state-of-the-art approaches for hyperparameter learning.},
	urldate = {2020-04-21},
	journal = {arXiv:1906.11527 [cs, stat]},
	author = {Jomaa, Hadi S. and Grabocka, Josif and Schmidt-Thieme, Lars},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.11527},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{falkner_bohb_2018,
	title = {{BOHB}: {Robust} and {Efficient} {Hyperparameter} {Optimization} at {Scale}},
	shorttitle = {{BOHB}},
	url = {http://arxiv.org/abs/1807.01774},
	abstract = {Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based conﬁguration evaluation approaches based on random search lack guidance and do not converge to the best conﬁgurations as quickly. Here, we propose to combine the beneﬁts of both Bayesian optimization and banditbased methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal conﬁgurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.},
	language = {en},
	urldate = {2020-03-30},
	journal = {arXiv:1807.01774 [cs, stat]},
	author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.01774},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bergstra_algorithms_nodate,
	title = {Algorithms for {Hyper}-{Parameter} {Optimization}},
	abstract = {Several recent advances to the state of the art in image classiﬁcation benchmarks have come from better conﬁgurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efﬁcient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can ﬁnd better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufﬁciently efﬁcient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difﬁcult DBN learning problems from [1] and ﬁnd signiﬁcantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y{\textbar}x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
	language = {en},
	author = {Bergstra, James S and Bardenet, Rémi and Bengio, Yoshua and Kégl, Balázs},
	pages = {9},
}

@incollection{bergstra_algorithms_2011,
	title = {Algorithms for {Hyper}-{Parameter} {Optimization}},
	url = {http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf},
	urldate = {2020-03-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 24},
	publisher = {Curran Associates, Inc.},
	author = {Bergstra, James S. and Bardenet, Rémi and Bengio, Yoshua and Kégl, Balázs},
	editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
	year = {2011},
	pages = {2546--2554},
}

@article{cui_mltuner_nodate,
	title = {{MLtuner}: {System} {Support} for {Automatic} {Machine} {Learning} {Tuning}},
	abstract = {MLtuner automatically tunes settings for training tunables—such as the learning rate, the mini-batch size, and the data staleness bound—that have a signiﬁcant impact on large-scale machine learning (ML) performance. Traditionally, these tunables are set manually, which is unsurprisingly error prone and difﬁcult to do without extensive domain knowledge. MLtuner uses efﬁcient snapshotting and optimization-guided online trial-and-error to ﬁnd good initial settings as well as to re-tune settings during execution. Experiments with three real ML tasks show that MLtuner automatically enables performance within 40–178\% of having oracle knowledge of the best settings, and outperforms oracle when no single set of settings are best for the entire execution. It also signiﬁcantly outperforms most of the many feasible settings that might get used in practice.},
	language = {en},
	author = {Cui, Henggang and Ganger, Gregory R and Gibbons, Phillip B},
	pages = {25},
}

@inproceedings{yigitbasi_towards_2013,
	title = {Towards {Machine} {Learning}-{Based} {Auto}-tuning of {MapReduce}},
	doi = {10.1109/MASCOTS.2013.9},
	abstract = {MapReduce, which is the de facto programming model for large-scale distributed data processing, and its most popular implementation Hadoop have enjoyed widespread adoption in industry during the past few years. Unfortunately, from a performance point of view getting the most out of Hadoop is still a big challenge due to the large number of configuration parameters. Currently these parameters are tuned manually by trial and error, which is ineffective due to the large parameter space and the complex interactions among the parameters. Even worse, the parameters have to be re-tuned for different MapReduce applications and clusters. To make the parameter tuning process more effective, in this paper we explore machine learning-based performance models that we use to auto-tune the configuration parameters. To this end, we first evaluate several machine learning models with diverse MapReduce applications and cluster configurations, and we show that support vector regression model (SVR) has good accuracy and is also computationally efficient. We further assess our auto-tuning approach, which uses the SVR performance model, against the Starfish auto tuner, which uses a cost-based performance model. Our findings reveal that our auto-tuning approach can provide comparable or in some cases better performance improvements than Starfish with a smaller number of parameters. Finally, we propose and discuss a complete and practical end-to-end auto-tuning flow that combines our machine learning-based performance models with smart search algorithms for the effective training of the models and the effective exploration of the parameter space.},
	booktitle = {2013 {IEEE} 21st {International} {Symposium} on {Modelling}, {Analysis} and {Simulation} of {Computer} and {Telecommunication} {Systems}},
	author = {Yigitbasi, Nezih and Willke, Theodore L. and Liao, Guangdeng and Epema, Dick},
	month = aug,
	year = {2013},
	note = {ISSN: 2375-0227},
	keywords = {Accuracy, Benchmark testing, Computational modeling, Data models, Hadoop, MapReduce, SVR performance model, Training, Training data, Tuning, big data, cluster configurations, configuration parameters, cost-based performance model, de facto programming model, distributed programming, distributed systems, end-to-end autotuning flow, hadoop, large-scale distributed data processing, learning (artificial intelligence), machine learning-based autotuning approach, machine learning-based performance models, parameter tuning process, performance modeling, public domain software, regression analysis, search problems, smart search algorithms, starfish autotuner, support vector machines, support vector regression model},
	pages = {11--20},
}

@inproceedings{koch_autotune_2018,
	address = {London, United Kingdom},
	series = {{KDD} '18},
	title = {Autotune: {A} {Derivative}-free {Optimization} {Framework} for {Hyperparameter} {Tuning}},
	isbn = {978-1-4503-5552-0},
	shorttitle = {Autotune},
	url = {https://doi.org/10.1145/3219819.3219837},
	doi = {10.1145/3219819.3219837},
	abstract = {Machine learning applications often require hyperparameter tuning. The hyperparameters usually drive both the efficiency of the model training process and the resulting model quality. For hyperparameter tuning, machine learning algorithms are complex black-boxes. This creates a class of challenging optimization problems, whose objective functions tend to be nonsmooth, discontinuous, unpredictably varying in computational expense, and include continuous, categorical, and/or integer variables. Further, function evaluations can fail for a variety of reasons including numerical difficulties or hardware failures. Additionally, not all hyperparameter value combinations are compatible, which creates so called hidden constraints. Robust and efficient optimization algorithms are needed for hyperparameter tuning. In this paper we present an automated parallel derivative-free optimization framework called Autotune , which combines a number of specialized sampling and search methods that are very effective in tuning machine learning models despite these challenges. Autotune provides significantly improved models over using default hyperparameter settings with minimal user interaction on real-world applications. Given the inherent expense of training numerous candidate models, we demonstrate the effectiveness of Autotune's search methods and the efficient distributed and parallel paradigms for training and tuning models, and also discuss the resource trade-offs associated with the ability to both distribute the training process and parallelize the tuning process.},
	urldate = {2020-03-23},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Koch, Patrick and Golovidov, Oleg and Gardner, Steven and Wujek, Brett and Griffin, Joshua and Xu, Yan},
	month = jul,
	year = {2018},
	keywords = {bayesian optimization, derivative-free optimization, distributed computing system, hyperparameters, stochastic optimization},
	pages = {443--452},
}

@inproceedings{ogilvie_minimizing_2017,
	title = {Minimizing the cost of iterative compilation with active learning},
	doi = {10.1109/CGO.2017.7863744},
	abstract = {Since performance is not portable between platforms, engineers must fine-tune heuristics for each processor in turn. This is such a laborious task that high-profile compilers, supporting many architectures, cannot keep up with hardware innovation and are actually out-of-date. Iterative compilation driven by machine learning has been shown to be efficient at generating portable optimization models automatically. However, good quality models require costly, repetitive, and extensive training which greatly hinders the wide adoption of this powerful technique. In this work, we show that much of this cost is spent collecting training data, runtime measurements for different optimization decisions, which contribute little to the final heuristic. Current implementations evaluate randomly chosen, often redundant, training examples a pre-configured, almost always excessive, number of times - a large source of wasted effort. Our approach optimizes not only the selection of training examples but also the number of samples per example, independently. To evaluate, we construct 11 high-quality models which use a combination of optimization settings to predict the runtime of benchmarks from the SPAPT suite. Our novel, broadly applicable, methodology is able to reduce the training overhead by up to 26x compared to an approach with a fixed number of sample runs, transforming what is potentially months of work into days.},
	booktitle = {2017 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization} ({CGO})},
	author = {Ogilvie, William F. and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},
	month = feb,
	year = {2017},
	note = {ISSN: null},
	keywords = {Active Learning, Compilers, Hardware, Iterative Compilation, Layout, Machine Learning, Noise measurement, Optimization, Runtime, Sequential Analysis, Training, Tuning, active learning, cost minimisation, fine-tune heuristics, high-profile compilers, iterative compilation, iterative methods, learning (artificial intelligence), machine learning, microprocessor chips, optimization decisions, performance evaluation, portable optimization models, processor},
	pages = {245--256},
}

@inproceedings{ogilvie_active_2014,
	address = {Edmonton, AB, Canada},
	series = {{PACT} '14},
	title = {Active learning accelerated automatic heuristic construction for parallel program mapping},
	isbn = {978-1-4503-2809-8},
	url = {https://doi.org/10.1145/2628071.2628128},
	doi = {10.1145/2628071.2628128},
	abstract = {Building effective optimization heuristics is a challenging task which often takes developers several months if not years to complete. Predictive modelling has recently emerged as a promising solution, automatically constructing heuristics from training data, however, obtaining this data can take months per platform. This is becoming an ever more critical problem as the pace of change in architecture increases. Indeed, if no solution is found we shall be left with out of date heuristics which cannot extract the best performance from modern machines. In this work, we present a low-cost predictive modelling approach for automatic heuristic construction which significantly reduces this training overhead. Typically in supervised learning the training instances are randomly selected to evaluate regardless of how much useful information they carry, but this wastes effort on parts of the space that contribute little to the quality of the produced heuristic. Our approach, on the other hand, uses active learning to select and only focus on the most useful training examples and thus reduces the training overhead. We demonstrate this technique by automatically creating a model to determine on which device to execute four parallel programs at differing problem dimensions for a representative CPU-GPU based system. Our methodology is remarkably simple and yet effective, making it a strong candidate for wide adoption. At high levels of classification accuracy the average learning speed-up is 3x, as compared to the state-of-the-art.},
	urldate = {2020-03-02},
	booktitle = {Proceedings of the 23rd international conference on {Parallel} architectures and compilation},
	publisher = {Association for Computing Machinery},
	author = {Ogilvie, William F. and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},
	month = aug,
	year = {2014},
	keywords = {active learning, compilers, machine learning},
	pages = {481--482},
}

@inproceedings{balaprakash_active-learning-based_2013,
	title = {Active-learning-based surrogate models for empirical performance tuning},
	doi = {10.1109/CLUSTER.2013.6702683},
	abstract = {Performance models have profound impact on hardware-software codesign, architectural explorations, and performance tuning of scientific applications. Developing algebraic performance models is becoming an increasingly challenging task. In such situations, a statistical surrogate-based performance model, fitted to a small number of input-output points obtained from empirical evaluation on the target machine, provides a range of benefits. Accurate surrogates can emulate the output of the expensive empirical evaluation at new inputs and therefore can be used to test and/or aid search, compiler, and autotuning algorithms. We present an iterative parallel algorithm that builds surrogate performance models for scientific kernels and workloads on single-core and multicore and multinode architectures. We tailor to our unique parallel environment an active learning heuristic popular in the literature on the sequential design of computer experiments in order to identify the code variants whose evaluations have the best potential to improve the surrogate. We use the proposed approach in a number of case studies to illustrate its effectiveness.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Cluster} {Computing} ({CLUSTER})},
	author = {Balaprakash, Prasanna and Gramacy, Robert B. and Wild, Stefan M.},
	month = sep,
	year = {2013},
	note = {ISSN: 2168-9253},
	keywords = {Computational modeling, Correlation, Jacobian matrices, Load modeling, Tuning, active-learning-based surrogate models, algebra, algebraic performance models, architectural explorations, empirical performance tuning, hardware-software codesign, iterative methods, iterative parallel algorithm, learning (artificial intelligence), multicore architectures, multinode architectures, parallel algorithms, single-core architectures, software architecture, statistical analysis, statistical surrogate},
	pages = {1--8},
}

@inproceedings{snoek_practical_2012,
	title = {Practical {Bayesian} {Optimization} of {Machine} {Learning} {Algorithms}},
	copyright = {Jasper Snoek; Hugo Larochelle; Ryan Prescott Adams},
	url = {https://dash.harvard.edu/handle/1/11708816},
	abstract = {Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
	language = {en\_US},
	urldate = {2020-02-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan Prescott},
	year = {2012},
}

@article{snoek_practical_nodate,
	title = {Practical {Bayesian} {Optimization} of {Machine} {Learning} {Algorithms}},
	abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a “black art” requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm’s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
	language = {en},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
	pages = {9},
}

@inproceedings{snoek_practical_2012-1,
	address = {Lake Tahoe, Nevada},
	series = {{NIPS}'12},
	title = {Practical {Bayesian} optimization of machine learning algorithms},
	abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
	urldate = {2020-02-28},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 2},
	publisher = {Curran Associates Inc.},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
	month = dec,
	year = {2012},
	pages = {2951--2959},
}

@inproceedings{bergstra_hyperopt_2013,
	address = {Austin, Texas},
	title = {Hyperopt: {A} {Python} {Library} for {Optimizing} the {Hyperparameters} of {Machine} {Learning} {Algorithms}},
	shorttitle = {Hyperopt},
	url = {https://conference.scipy.org/proceedings/scipy2013/bergstra_hyperopt.html},
	doi = {10.25080/Majora-8b375195-003},
	abstract = {Sequential model-based optimization (also known as Bayesian op- These conﬁguration variables are called hyperparameters. For timization) is one of the most efﬁcient methods (per function evaluation) of function minimization. This efﬁciency makes it appropriate for optimizing the hyperparameters of machine learning algorithms that are slow to train. The Hyperopt library provides algorithms and parallelization infrastructure for performing hyperparameter optimization (model selection) in Python. This paper presents an introductory tutorial on the usage of the Hyperopt library, including the description of search spaces, minimization (in serial and parallel), and the analysis of the results collected in the course of minimization. The paper closes T with some discussion of ongoing and future work.},
	language = {en},
	urldate = {2020-02-28},
	author = {Bergstra, James and Yamins, Dan and Cox, David},
	year = {2013},
	pages = {13--19},
}

@article{sorensen_metaheuristicsmetaphor_2015,
	title = {Metaheuristics—the metaphor exposed},
	volume = {22},
	copyright = {© 2013 The Authors. International Transactions in Operational Research © 2013 International Federation of Operational Research Societies Published by John Wiley \& Sons Ltd, 9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main St, Malden, MA02148, USA.},
	issn = {1475-3995},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/itor.12001},
	doi = {10.1111/itor.12001},
	abstract = {In recent years, the field of combinatorial optimization has witnessed a true tsunami of “novel” metaheuristic methods, most of them based on a metaphor of some natural or man-made process. The behavior of virtually any species of insects, the flow of water, musicians playing together – it seems that no idea is too far-fetched to serve as inspiration to launch yet another metaheuristic. In this paper, we will argue that this line of research is threatening to lead the area of metaheuristics away from scientific rigor. We will examine the historical context that gave rise to the increasing use of metaphors as inspiration and justification for the development of new methods, discuss the reasons for the vulnerability of the metaheuristics field to this line of research, and point out its fallacies. At the same time, truly innovative research of high quality is being performed as well. We conclude the paper by discussing some of the properties of this research and by pointing out some of the most promising research avenues for the field of metaheuristics.},
	language = {en},
	number = {1},
	urldate = {2020-02-27},
	journal = {International Transactions in Operational Research},
	author = {Sörensen, Kenneth},
	year = {2015},
	keywords = {combinatorial optimization, heuristics, metaheuristics, optimization},
	pages = {3--18},
}

@article{sid-lakhdar_multitask_2019,
	title = {Multitask and {Transfer} {Learning} for {Autotuning} {Exascale} {Applications}},
	url = {http://arxiv.org/abs/1908.05792},
	abstract = {Multitask learning and transfer learning have proven to be useful in the field of machine learning when additional knowledge is available to help a prediction task. We aim at deriving methods following these paradigms for use in autotuning, where the goal is to find the optimal performance parameters of an application treated as a black-box function. We show comparative results with state-of-the-art autotuning techniques. For instance, we observe an average \$1.5x\$ improvement of the application runtime compared to the OpenTuner and HpBandSter autotuners. We explain how our approaches can be more suitable than some state-of-the-art autotuners for the tuning of any application in general and of expensive exascale applications in particular.},
	urldate = {2020-02-27},
	journal = {arXiv:1908.05792 [cs, stat]},
	author = {Sid-Lakhdar, Wissam M. and Aznaveh, Mohsen Mahmoudi and Li, Xiaoye S. and Demmel, James W.},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.05792},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{luo_fast_2015,
	address = {Newport Beach, California, USA},
	series = {{ICS} '15},
	title = {{FAST}: {A} {Fast} {Stencil} {Autotuning} {Framework} {Based} {On} {An} {Optimal}-solution {Space} {Model}},
	isbn = {978-1-4503-3559-1},
	shorttitle = {{FAST}},
	url = {https://doi.org/10.1145/2751205.2751214},
	doi = {10.1145/2751205.2751214},
	abstract = {Stencil computations comprise an important class of kernels in many scientific computing applications. As the diversity of both architectures and programming models grow, autotuning is emerging as a critical strategy for achieving portable performance across a broad range of execution contexts for stencil computations. However, costly tuning overhead is a major obstacle to its popularity. In this work, we propose a fast stencil autotuning framework FAST based on an Optimal-Solution Space (OSS) model to significantly improve tuning speed. It leverages a feature extractor that comprehensively characterizes stencil computation. Using the extracted features, FAST constructs an OSS database to train an off-line model which provides an on-line prediction. We evaluate FAST with five important stencil computation applications on both an Intel Xeon multicore CPU and an NVIDIA Tesla K20c GPU. Compared with state-of-the-art stencil autotuners like Patus and SDSL, FAST improves autotuning speed by 10-2697 times without any user annotation, while achieving comparable performance.},
	urldate = {2020-02-27},
	booktitle = {Proceedings of the 29th {ACM} on {International} {Conference} on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Luo, Yulong and Tan, Guangming and Mo, Zeyao and Sun, Ninghui},
	month = jun,
	year = {2015},
	keywords = {autotuning, oss, stencil},
	pages = {187--196},
}

@article{tan_design_2018,
	title = {Design and {Implementation} of {Adaptive} {SpMV} {Library} for {Multicore} and {Many}-{Core} {Architecture}},
	volume = {44},
	issn = {0098-3500},
	url = {https://doi.org/10.1145/3218823},
	doi = {10.1145/3218823},
	abstract = {Sparse matrix vector multiplication (SpMV) is an important computational kernel in traditional high-performance computing and emerging data-intensive applications. Previous SpMV libraries are optimized by either application-specific or architecture-specific approaches but present difficulties for use in real applications. In this work, we develop an auto-tuning system (SMATER) to bridge the gap between specific optimizations and general-purpose use. SMATER provides programmers a unified interface based on the compressed sparse row (CSR) sparse matrix format by implicitly choosing the best format and fastest implementation for any input sparse matrix during runtime. SMATER leverages a machine-learning model and retargetable back-end library to quickly predict the optimal combination. Performance parameters are extracted from 2,386 matrices in the SuiteSparse matrix collection. The experiments show that SMATER achieves good performance (up to 10 times that of the Intel Math Kernel Library (MKL) on Intel E5-2680 v3) while being portable on state-of-the-art x86 multicore processors, NVIDIA GPUs, and Intel Xeon Phi accelerators. Compared with the Intel MKL library, SMATER runs faster by more than 2.5 times on average. We further demonstrate its adaptivity in an algebraic multigrid solver from the Hypre library and report greater than 20\% performance improvement.},
	number = {4},
	urldate = {2020-02-27},
	journal = {ACM Transactions on Mathematical Software},
	author = {Tan, Guangming and Liu, Junhong and Li, Jiajia},
	month = aug,
	year = {2018},
	keywords = {Sparse matrix vector multiplication, auto-tuning, machine learning, multicore},
	pages = {46:1--46:25},
}

@inproceedings{li_smat_2013,
	address = {Seattle, Washington, USA},
	series = {{PLDI} '13},
	title = {{SMAT}: an input adaptive auto-tuner for sparse matrix-vector multiplication},
	isbn = {978-1-4503-2014-6},
	shorttitle = {{SMAT}},
	url = {https://doi.org/10.1145/2491956.2462181},
	doi = {10.1145/2491956.2462181},
	abstract = {Sparse Matrix Vector multiplication (SpMV) is an important kernel in both traditional high performance computing and emerging data-intensive applications. By far, SpMV libraries are optimized by either application-specific or architecture-specific approaches, making the libraries become too complicated to be used extensively in real applications. In this work we develop a Sparse Matrix-vector multiplication Auto-Tuning system (SMAT) to bridge the gap between specific optimizations and general-purpose usage. SMAT provides users with a unified programming interface in compressed sparse row (CSR) format and automatically determines the optimal format and implementation for any input sparse matrix at runtime. For this purpose, SMAT leverages a learning model, which is generated in an off-line stage by a machine learning method with a training set of more than 2000 matrices from the UF sparse matrix collection, to quickly predict the best combination of the matrix feature parameters. Our experiments show that SMAT achieves impressive performance of up to 51GFLOPS in single-precision and 37GFLOPS in double-precision on mainstream x86 multi-core processors, which are both more than 3 times faster than the Intel MKL library. We also demonstrate its adaptability in an algebraic multigrid solver from Hypre library with above 20\% performance improvement reported.},
	urldate = {2020-02-27},
	booktitle = {Proceedings of the 34th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Li, Jiajia and Tan, Guangming and Chen, Mingyu and Sun, Ninghui},
	month = jun,
	year = {2013},
	keywords = {SpMV, algebraic multi-grid, auto-tuning, data mining, sparse matrix-vector multiplication},
	pages = {117--126},
}

@article{liu_autotuning_2019,
	title = {An {Autotuning} {Protocol} to {Rapidly} {Build} {Autotuners}},
	volume = {5},
	issn = {2329-4949},
	url = {https://doi.org/10.1145/3291527},
	doi = {10.1145/3291527},
	abstract = {Automatic performance tuning (Autotuning) is an increasingly critical tuning technique for the high portable performance of Exascale applications. However, constructing an autotuner from scratch remains a challenge, even for domain experts. In this work, we propose a performance tuning and knowledge management suite (PAK) to help rapidly build autotuners. In order to accommodate existing autotuning techniques, we present an autotuning protocol that is composed of an extractor, producer, optimizer, evaluator, and learner. To achieve modularity and reusability, we also define programming interfaces for each protocol component as the fundamental infrastructure, which provides a customizable mechanism to deploy knowledge mining in the performance database. PAK’s usability is demonstrated by studying two important computational kernels: stencil computation and sparse matrix-vector multiplication (SpMV). Our proposed autotuner based on PAK shows comparable performance and higher productivity than traditional autotuners by writing just a few tens of code using our autotuning protocol.},
	number = {2},
	urldate = {2020-02-27},
	journal = {ACM Transactions on Parallel Computing},
	author = {Liu, Junhong and Tan, Guangming and Luo, Yulong and Li, Jiajia and Mo, Zeyao and Sun, Ninghui},
	month = jan,
	year = {2019},
	keywords = {Autotuner, SpMV, knowledge database, protocol, stencil},
	pages = {9:1--9:25},
}

@article{benkner_automatic_2014,
	title = {Automatic {Application} {Tuning} for {HPC} {Architectures} ({Dagstuhl} {Seminar} 13401)},
	volume = {3},
	issn = {2192-5283},
	url = {http://drops.dagstuhl.de/opus/volltexte/2014/4423},
	doi = {10.4230/DagRep.3.9.214},
	number = {9},
	urldate = {2020-02-27},
	journal = {Dagstuhl Reports},
	author = {Benkner, Siegfried and Franchetti, Franz and Gerndt, Hans Michael and Hollingsworth, Jeffrey K.},
	editor = {Benkner, Siegfried and Franchetti, Franz and Gerndt, Hans Michael and Hollingsworth, Jeffrey K.},
	year = {2014},
	keywords = {Parallel Computing, Performance Analysis and Tuning, Programming Tools},
	pages = {214--244},
}

@article{frigo_design_2005,
	title = {The {Design} and {Implementation} of {FFTW3}},
	volume = {93},
	issn = {1558-2256},
	doi = {10.1109/JPROC.2004.840301},
	abstract = {FFTW is an implementation of the discrete Fourier transform (DFT) that adapts to the hardware in order to maximize performance. This paper shows that such an approach can yield an implementation that is competitive with hand-optimized libraries, and describes the software structure that makes our current FFTW3 version flexible and adaptive. We further discuss a new algorithm for real-data DFTs of prime size, a new way of implementing DFTs by means of machine-specific single-instruction, multiple-data (SIMD) instructions, and how a special-purpose compiler can derive optimized implementations of the discrete cosine and sine transforms automatically from a DFT algorithm.},
	number = {2},
	journal = {Proceedings of the IEEE},
	author = {Frigo, M. and Johnson, S.G.},
	month = feb,
	year = {2005},
	keywords = {Adaptive software, DFT algorithm, Data structures, Discrete Fourier transforms, Discrete cosine transforms, Discrete transforms, FFTW3 design, FFTW3 version, Fast Fourier transforms, Fourier transform, Fourier transforms, Hardware, Hartley transform, I/O tensor, Multidimensional systems, Optimizing compilers, Software libraries, cosine transform, cosine transforms, discrete Fourier transform, discrete Fourier transforms, discrete cosine transforms, fast Fourier transform (FFT), hand optimized libraries, machine specific single instruction, mathematics computing, multiple data instructions, optimising compilers, parallel programming, sine transforms, software libraries, software structure},
	pages = {216--231},
}

@article{clint_whaley_automated_2001-1,
	series = {New {Trends} in {High} {Performance} {Computing}},
	title = {Automated empirical optimizations of software and the {ATLAS} project},
	volume = {27},
	issn = {0167-8191},
	url = {http://www.sciencedirect.com/science/article/pii/S0167819100000879},
	doi = {10.1016/S0167-8191(00)00087-9},
	abstract = {This paper describes the automatically tuned linear algebra software (ATLAS) project, as well as the fundamental principles that underly it. ATLAS is an instantiation of a new paradigm in high performance library production and maintenance, which we term automated empirical optimization of software (AEOS); this style of library management has been created in order to allow software to keep pace with the incredible rate of hardware advancement inherent in Moore's Law. ATLAS is the application of this new paradigm to linear algebra software, with the present emphasis on the basic linear algebra subprograms (BLAS), a widely used, performance-critical, linear algebra kernel library.},
	language = {en},
	number = {1},
	urldate = {2020-02-27},
	journal = {Parallel Computing},
	author = {Clint Whaley, R. and Petitet, Antoine and Dongarra, Jack J.},
	month = jan,
	year = {2001},
	keywords = {AEOS, ATLAS, BLAS, Portable performance},
	pages = {3--35},
}

@inproceedings{luszczek_search_2016,
	title = {Search {Space} {Generation} and {Pruning} {System} for {Autotuners}},
	doi = {10.1109/IPDPSW.2016.197},
	abstract = {This work tackles two simultaneous challenges faced by autotuners: the ease of describing a complex, multidimensional search space, and the speed of evaluating that space, while applying a multitude of pruning constraints. This article presents a declarative notation for describing a search space and a translation system for conversion to a standard C code for fast and multithreaded, as necessary, evaluation. The notation is Python-based and thus simple in syntax and easy to assimilate by the user interested in tuning rather than learning a new programming language. A large number of dimensions and a large number of pruning constraints may be expressed with little effort. The system is discussed in the context of autotuning the canonical matrix multiplication kernel for NVIDIA GPUs, where the search space has 15 dimensions and involves application of 10 complex pruning constrains. The speed of evaluation is compared against generators created using imperative programming style in various scripting and compiled languages.},
	booktitle = {2016 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	author = {Luszczek, Piotr and Gates, Mark and Kurzak, Jakub and Danalis, Anthony and Dongarra, Jack},
	month = may,
	year = {2016},
	note = {ISSN: null},
	keywords = {Context, Graphics processing units, Kernel, Libraries, NVIDIA GPU, Standards, Syntactics, Tuning, autotuners, autotuning, canonical matrix multiplication kernel, compiled languages, complex pruning constrains, high level languages, imperative programming style, multidimensional search space, multidimensional search space enumeration, performance autotuning, programming language, pruning constraints, pruning system, search space generation, software maintenance, standard C code, syntax, translation system},
	pages = {1545--1554},
}

@inproceedings{kurzak_massively_2019,
	address = {Kyoto, Japan},
	series = {{ICPP} 2019},
	title = {Massively {Parallel} {Automated} {Software} {Tuning}},
	isbn = {978-1-4503-6295-5},
	url = {https://doi.org/10.1145/3337821.3337908},
	doi = {10.1145/3337821.3337908},
	abstract = {This article presents an implementation of a distributed autotuning engine developed as part of the Bench-testing OpenN Software Autotuning Infrastructure project. The system is geared towards performance optimization of computational kernels for graphics processing units, and allows for the deployment of vast autotuning sweeps to massively parallel machines. The software implements dynamic work scheduling to distributed-memory resources and takes advantage of multithreading for parallel compilation and dispatches kernel launches to multiple accelerators. This paper lays out the main design principles of the system and discusses the basic mechanics of the initial implementation. Preliminary performance results are presented, encountered challenges are discussed, and the future directions are outlined.},
	urldate = {2020-02-27},
	booktitle = {Proceedings of the 48th {International} {Conference} on {Parallel} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Kurzak, Jakub and Tsai, Yaohung M. and Gates, Mark and Abdelfattah, Ahmad and Dongarra, Jack},
	month = aug,
	year = {2019},
	keywords = {automated software tuning, graphics processing unit},
	pages = {1--10},
}

@article{jia_gpu_2015,
	title = {{GPU} {Performance} and {Power} {Tuning} {Using} {Regression} {Trees}},
	volume = {12},
	issn = {1544-3566},
	url = {https://doi.org/10.1145/2736287},
	doi = {10.1145/2736287},
	abstract = {GPU performance and power tuning is difficult, requiring extensive user expertise and time-consuming trial and error. To accelerate design tuning, statistical design space exploration methods have been proposed. This article presents Starchart, a novel design space partitioning tool that uses regression trees to approach GPU tuning problems. Improving on prior work, Starchart offers more automation in identifying key design trade-offs and models design subspaces with distinctly different behaviors. Starchart achieves good model accuracy using very few random samples: less than 0.3\% of a given design space; iterative sampling can more quickly target subspaces of interest.},
	number = {2},
	urldate = {2020-02-27},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Jia, Wenhao and Garza, Elba and Shaw, Kelly A. and Martonosi, Margaret},
	month = may,
	year = {2015},
	keywords = {Design space exploration, GPGPU, decision tree, statistical modeling},
	pages = {13:1--13:26},
}

@inproceedings{vollmer_meta-programming_2015,
	address = {Vancouver, BC, Canada},
	series = {{FHPC} 2015},
	title = {Meta-programming and auto-tuning in the search for high performance {GPU} code},
	isbn = {978-1-4503-3807-3},
	url = {https://doi.org/10.1145/2808091.2808092},
	doi = {10.1145/2808091.2808092},
	abstract = {Writing high performance GPGPU code is often difficult and time-consuming, potentially requiring laborious manual tuning of low-level details. Despite these challenges, the cost in ignoring GPUs in high performance computing is increasingly large. Auto-tuning is a potential solution to the problem of tedious manual tuning. We present a framework for auto-tuning GPU kernels which are expressed in an embedded DSL, and which expose compile-time parameters for tuning. Our framework allows for kernels to be polymorphic over what search strategy will tune them, and allows search strategies to be implemented in the same meta-language as the kernel-generation code (Haskell). Further, we show how to use functional programming abstractions to enforce regular (hyper-rectangular) search spaces. We also evaluate several common search strategies on a variety of kernels, and demonstrate that the framework can tune both EDSL and ordinary CUDA code.},
	urldate = {2020-02-27},
	booktitle = {Proceedings of the 4th {ACM} {SIGPLAN} {Workshop} on {Functional} {High}-{Performance} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Vollmer, Michael and Svensson, Bo Joel and Holk, Eric and Newton, Ryan R.},
	month = aug,
	year = {2015},
	keywords = {GPUs, auto-tuning, meta-programming, parallelism},
	pages = {1--11},
}

@article{ashouri_cobayn_2016,
	title = {{COBAYN}: {Compiler} {Autotuning} {Framework} {Using} {Bayesian} {Networks}},
	volume = {13},
	issn = {1544-3566},
	shorttitle = {{COBAYN}},
	url = {https://doi.org/10.1145/2928270},
	doi = {10.1145/2928270},
	abstract = {The variety of today’s architectures forces programmers to spend a great deal of time porting and tuning application codes across different platforms. Compilers themselves need additional tuning, which has considerable complexity as the standard optimization levels, usually designed for the average case and the specific target architecture, often fail to bring the best results. This article proposes COBAYN: Compiler autotuning framework using BAYesian Networks, an approach for a compiler autotuning methodology using machine learning to speed up application performance and to reduce the cost of the compiler optimization phases. The proposed framework is based on the application characterization done dynamically by using independent microarchitecture features and Bayesian networks. The article also presents an evaluation based on using static analysis and hybrid feature collection approaches. In addition, the article compares Bayesian networks with respect to several state-of-the-art machine-learning models. Experiments were carried out on an ARM embedded platform and GCC compiler by considering two benchmark suites with 39 applications. The set of compiler configurations, selected by the model (less than 7\% of the search space), demonstrated an application performance speedup of up to 4.6 × on Polybench (1.85 × on average) and 3.1 × on cBench (1.54 × on average) with respect to standard optimization levels. Moreover, the comparison of the proposed technique with (i) random iterative compilation, (ii) machine learning--based iterative compilation, and (iii) noniterative predictive modeling techniques shows, on average, 1.2 × , 1.37 × , and 1.48 × speedup, respectively. Finally, the proposed method demonstrates 4 × and 3 × speedup, respectively, on cBench and Polybench in terms of exploration efficiency given the same quality of the solutions generated by the random iterative compilation model.},
	number = {2},
	urldate = {2020-02-27},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Ashouri, Amir Hossein and Mariani, Giovanni and Palermo, Gianluca and Park, Eunjung and Cavazos, John and Silvano, Cristina},
	month = jun,
	year = {2016},
	keywords = {Bayesian networks, design space exploration, statistical inference},
	pages = {21:1--21:25},
}

@inproceedings{dalibard_boat_2017,
	address = {Perth, Australia},
	series = {{WWW} '17},
	title = {{BOAT}: {Building} {Auto}-{Tuners} with {Structured} {Bayesian} {Optimization}},
	isbn = {978-1-4503-4913-0},
	shorttitle = {{BOAT}},
	url = {https://doi.org/10.1145/3038912.3052662},
	doi = {10.1145/3038912.3052662},
	abstract = {Due to their complexity, modern systems expose many configuration parameters which users must tune to maximize performance. Auto-tuning has emerged as an alternative in which a black-box optimizer iteratively evaluates configurations to find efficient ones. Unfortunately, for many systems, such as distributed systems, evaluating performance takes too long and the space of configurations is too large for the optimizer to converge within a reasonable time. We present BOAT, a framework which allows developers to build efficient bespoke auto-tuners for their system, in situations where generic auto-tuners fail. At BOAT's core is structured Bayesian optimization (SBO), a novel extension of the Bayesian optimization algorithm. SBO leverages contextual information provided by system developers, in the form of a probabilistic model of the system's behavior, to make informed decisions about which configurations to evaluate. In a case study, we tune the scheduling of a neural network computation on a heterogeneous cluster. Our auto-tuner converges within ten iterations. The optimized configurations outperform those found by generic auto-tuners in thirty iterations by up to 2X.},
	urldate = {2020-02-27},
	booktitle = {Proceedings of the 26th {International} {Conference} on {World} {Wide} {Web}},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Dalibard, Valentin and Schaarschmidt, Michael and Yoneki, Eiko},
	month = apr,
	year = {2017},
	keywords = {auto-tuning, bayesian optimization, distributed stochastic gradient descent, distributed systems, neural networks, probabilistic programming},
	pages = {479--488},
}

@article{eggensperger_efficient_2018,
	title = {Efficient benchmarking of algorithm configurators via model-based surrogates},
	volume = {107},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-017-5683-z},
	doi = {10.1007/s10994-017-5683-z},
	abstract = {The optimization of algorithm (hyper-)parameters is crucial for achieving peak performance across a wide range of domains, ranging from deep neural networks to solvers for hard combinatorial problems. However, the proper evaluation of new algorithm configuration (AC) procedures (or configurators) is hindered by two key hurdles. First, AC scenarios are hard to set up, including the target algorithm to be optimized and the problem instances to be solved. Second, and even more significantly, they are computationally expensive: a single configurator run involves many costly runs of the target algorithm. Here, we propose a benchmarking approach that uses surrogate scenarios, which are computationally cheap while remaining close to the original AC scenarios. These surrogate scenarios approximate the response surface corresponding to true target algorithm performance using a regression model. In our experiments, we construct and evaluate surrogate scenarios for hyperparameter optimization as well as for AC problems that involve performance optimization of solvers for hard combinatorial problems. We generalize previous work by building surrogates for AC scenarios with multiple problem instances, stochastic target algorithms and censored running time observations. We show that our surrogate scenarios capture overall important characteristics of the original AC scenarios from which they were derived, while being much easier to use and orders of magnitude cheaper to evaluate.},
	language = {en},
	number = {1},
	urldate = {2020-02-27},
	journal = {Machine Learning},
	author = {Eggensperger, Katharina and Lindauer, Marius and Hoos, Holger H. and Hutter, Frank and Leyton-Brown, Kevin},
	month = jan,
	year = {2018},
	pages = {15--41},
}

@inproceedings{chen_tvm_2018,
	title = {\{{TVM}\}: {An} {Automated} {End}-to-{End} {Optimizing} {Compiler} for {Deep} {Learning}},
	isbn = {978-1-939133-08-3},
	shorttitle = {\{{TVM}\}},
	url = {https://www.usenix.org/conference/osdi18/presentation/chen},
	language = {en},
	urldate = {2020-02-27},
	author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
	year = {2018},
	pages = {578--594},
}

@article{kirkpatrick_optimization_1983,
	title = {Optimization by {Simulated} {Annealing}},
	volume = {220},
	copyright = {© 1983},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/220/4598/671},
	doi = {10.1126/science.220.4598.671},
	abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
	language = {en},
	number = {4598},
	urldate = {2020-02-27},
	journal = {Science},
	author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
	month = may,
	year = {1983},
	pmid = {17813860},
	pages = {671--680},
}

@article{petrovic_benchmark_2019,
	title = {A {Benchmark} {Set} of {Highly}-efficient {CUDA} and {OpenCL} {Kernels} and its {Dynamic} {Autotuning} with {Kernel} {Tuning} {Toolkit}},
	url = {http://arxiv.org/abs/1910.08498},
	abstract = {In recent years, the heterogeneity of both commodity and supercomputers hardware has increased sharply. Accelerators, such as GPUs or Intel Xeon Phi co-processors, are often key to improving speed and energy eﬃciency of highly-parallel codes. However, due to the complexity of heterogeneous architectures, optimization of codes for a certain type of architecture as well as porting codes across diﬀerent architectures, while maintaining a comparable level of performance, can be extremely challenging. Addressing the challenges associated with performance optimization and performance portability, autotuning has gained a lot of interest. Autotuning of performance-relevant source-code parameters allows to automatically tune applications without hard coding optimizations and thus helps with keeping the performance portable. In this paper, we introduce a benchmark set of ten autotunable kernels for important computational problems implemented in OpenCL or CUDA. Using our Kernel Tuning Toolkit, we show that with autotuning most of the kernels reach near-peak performance on various GPUs and outperform baseline implementations on CPUs and Xeon Phis. Our evaluation also demonstrates that autotuning is key to performance portability. In addition to oﬄine tuning, we also introduce dynamic autotuning of code optimization parameters during application runtime. With dynamic tuning, the Kernel Tuning Toolkit enables applications to re-tune performance-critical kernels at runtime whenever needed, for example, when input data changes. Although it is generally believed that autotuning spaces tend to be too large to be searched during application runtime, we show that it is not necessarily the case when tuning spaces are designed rationally. Many of our kernels reach near peak-performance with moderately sized tuning spaces that can be searched at runtime with acceptable overhead. Finally we demonstrate, how dynamic performance tuning can be integrated into a real-world application from cryo-electron microscopy domain.},
	language = {en},
	urldate = {2020-02-27},
	journal = {arXiv:1910.08498 [cs]},
	author = {Petrovič, Filip and Střelák, David and Hozzová, Jana and Oľha, Jaroslav and Trembecký, Richard and Benkner, Siegfried and Filipovič, Jiří},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.08498},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@article{fernandez-fabeiro_automatic_2020,
	title = {An automatic optimizer for heterogeneous devices},
	volume = {106},
	issn = {0167-739X},
	url = {http://www.sciencedirect.com/science/article/pii/S0167739X18324294},
	doi = {10.1016/j.future.2020.01.018},
	abstract = {Codes written in a naive way seldom effectively exploit the computing resources, while writing optimized codes is usually a complex task that requires certain levels of expertise. This problem is further increased in the presence of heterogeneous devices, which present more tunable parameters than regular CPUs and high sensitivity to the optimization decisions taken. Furthermore, portability is an added concern given the wide variety of accelerators available. This paper tackles this problem adding an automatic optimizer to a library that already provides an easy and portable way to program heterogeneous devices, the Heterogeneous Programming Library (HPL). Our optimizer takes as input a simple version of a code and then tunes it for the device where it is going to be executed by performing the most usual set of optimizations applicable in heterogeneous devices. These optimizations are parametrized using a set of optimization parameters that need to be tuned for the device. The HPL library has also been equipped with an autotuner that can be used to this purpose. The effectiveness of the autotuner and the optimizer has been tested on several codes and devices. The results show that the combination of the autotuner and the optimizer make the tested codes 16 times faster on average than the original codes written by the programmer.},
	language = {en},
	urldate = {2020-02-27},
	journal = {Future Generation Computer Systems},
	author = {Fernández-Fabeiro, Jorge and Andrade, Diego and Fraguela, Basilio B. and Doallo, Ramón},
	month = may,
	year = {2020},
	keywords = {Heterogeneous systems, OpenCL, Performance portability, Performance tuning},
	pages = {572--584},
}

@inproceedings{rasch_atf_2017,
	title = {{ATF}: {A} {Generic} {Auto}-{Tuning} {Framework}},
	shorttitle = {{ATF}},
	doi = {10.1109/HPCC-SmartCity-DSS.2017.9},
	abstract = {We describe the Auto-Tuning Framework (ATF) - a novel generic approach for automatic program optimization by choosing the most suitable values of program parameters, such as number of parallel threads, tile sizes, etc. Our framework combines four advantages over the state-of-the-art autotuning: i) it is generic regarding the programming language, application domain, tuning objective (e.g., high performance and/or low energy consumption), and search technique; ii) it can auto-tune a broader class of applications by allowing tuning parameters to be interdependent, e.g., when one parameter is divisible by another parameter; iii) it allows tuning parameters with substantially larger ranges by implementing an optimized search space generation process; and iv) its interface is arguably simpler than the interfaces of current auto-tuning frameworks. We demonstrate ATF's efficacy by comparing it to the state-of-the-art auto-tuning approaches OpenTuner and CLTune, showing better tuning results with less programmer's effort.},
	booktitle = {2017 {IEEE} 19th {International} {Conference} on {High} {Performance} {Computing} and {Communications}; {IEEE} 15th {International} {Conference} on {Smart} {City}; {IEEE} 3rd {International} {Conference} on {Data} {Science} and {Systems} ({HPCC}/{SmartCity}/{DSS})},
	author = {Rasch, Ari and Haidl, Michael and Gorlatch, Sergei},
	month = dec,
	year = {2017},
	note = {ISSN: null},
	keywords = {ATF, Cost function, Energy consumption, Graphics processing units, Kernel, Runtime, Search problems, Tuning, application domain, automatic program optimization, automatic programming, configuration management, generic Auto-Tuning Framework, low energy consumption, optimisation, optimized search space generation process, parallel threads, program parameters, programming language, programming languages, search problems, search technique, tuning parameters},
	pages = {64--71},
}

@article{rasch_atf_2019,
	title = {{ATF}: {A} generic directive-based auto-tuning framework},
	volume = {31},
	copyright = {© 2018 John Wiley \& Sons, Ltd.},
	issn = {1532-0634},
	shorttitle = {{ATF}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4423},
	doi = {10.1002/cpe.4423},
	abstract = {We describe the Auto-Tuning Framework (ATF) — a simple-to-use, generic approach and its implementation, as a framework for automatic program optimization by choosing the most suitable values of program parameters such as the number of parallel threads, tile sizes, etc. ATF combines four major advantages over the state-of-the-art auto-tuning: i) it is generic regarding the programming language, application domain, tuning objective (eg, high performance and/or low energy consumption), and search technique; ii) it can auto-tune a broader class of applications by allowing tuning parameters to be interdependent, eg, when one parameter is divisible by another parameter; iii) it allows tuning parameters to have substantially larger ranges by implementing an optimized search space generation process; and iv) it is arguably simpler to use, eg, the ATF user prepares an application for auto-tuning by annotating its source code with simple tuning directives. We demonstrate ATF's efficacy by comparing it to the state-of-the-art auto-tuning approaches, OpenTuner and CLTune; ATF shows better tuning results with less programmer's effort.},
	language = {en},
	number = {5},
	urldate = {2020-02-27},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Rasch, Ari and Gorlatch, Sergei},
	year = {2019},
	keywords = {CLBlast, CLTune, CUDA, GEMM, OpenCL, OpenTuner, auto-tuning, dependent tuning parameters, many-core, multi-core, multi-objective auto-tuning, tuning parameter constraints},
	pages = {e4423},
}

@inproceedings{ansel_opentuner_2014,
	address = {Edmonton, AB, Canada},
	series = {{PACT} '14},
	title = {{OpenTuner}: an extensible framework for program autotuning},
	isbn = {978-1-4503-2809-8},
	shorttitle = {{OpenTuner}},
	url = {https://doi.org/10.1145/2628071.2628092},
	doi = {10.1145/2628071.2628092},
	abstract = {Program autotuning has been shown to achieve better or more portable performance in a number of domains. However, autotuners themselves are rarely portable between projects, for a number of reasons: using a domain-informed search space representation is critical to achieving good results; search spaces can be intractably large and require advanced machine learning techniques; and the landscape of search spaces can vary greatly between different problems, sometimes requiring domain specific search techniques to explore efficiently. This paper introduces OpenTuner, a new open source framework for building domain-specific multi-objective program autotuners. OpenTuner supports fully-customizable configuration representations, an extensible technique representation to allow for domain-specific techniques, and an easy to use interface for communicating with the program to be autotuned. A key capability inside OpenTuner is the use of ensembles of disparate search techniques simultaneously; techniques that perform well will dynamically be allocated a larger proportion of tests. We demonstrate the efficacy and generality of OpenTuner by building autotuners for 7 distinct projects and 16 total benchmarks, showing speedups over prior techniques of these projects of up to 2.8x with little programmer effort.},
	urldate = {2020-02-27},
	booktitle = {Proceedings of the 23rd international conference on {Parallel} architectures and compilation},
	publisher = {Association for Computing Machinery},
	author = {Ansel, Jason and Kamil, Shoaib and Veeramachaneni, Kalyan and Ragan-Kelley, Jonathan and Bosboom, Jeffrey and O'Reilly, Una-May and Amarasinghe, Saman},
	month = aug,
	year = {2014},
	keywords = {autotuner, optimization},
	pages = {303--316},
}

@article{hu_mirovia_2019,
	title = {Mirovia: {A} {Benchmarking} {Suite} for {Modern} {Heterogeneous} {Computing}},
	shorttitle = {Mirovia},
	url = {http://arxiv.org/abs/1906.10347},
	abstract = {This paper presents Mirovia, a benchmark suite developed for modern day heterogeneous computing. Previous benchmark suites such as Rodinia [1] and SHOC [2] are well written and have many desirable features. However, these tools were developed years ago when hardware was less powerful and software had fewer features. For example, uniﬁed memory was introduced in CUDA 6 as a new programming model and wasn’t available when Rodinia was released. Meanwhile, the increasing demand for graphics processing units (GPUs) due to the recent rise in popularity of deep neural networks (DNNs) has opened doors for many new research problems. It is essential to consider DNNs as ﬁrst-class citizens in a comprehensive benchmark suite. However, the main focus is usually limited to inference and model performance evaluation, which is not desirable for hardware architects studying for emerging platforms. Drawing inspiration from Rodinia and SHOC, Mirovia is a benchmark suite that is designed to take advantage of modern GPU architectures, while also representing a diverse set of application domains. By adopting applications from Rodinia and SHOC, and including newly written applications with special focus on DNNs, Mirovia better characterizes modern heterogeneous systems.},
	language = {en},
	urldate = {2020-02-26},
	journal = {arXiv:1906.10347 [cs]},
	author = {Hu, Bodun and Rossbach, Christopher J.},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.10347},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Computer Science - Performance},
}

@inproceedings{che_rodinia_2009,
	address = {Austin, TX, USA},
	title = {Rodinia: {A} benchmark suite for heterogeneous computing},
	isbn = {978-1-4244-5156-2},
	shorttitle = {Rodinia},
	url = {http://ieeexplore.ieee.org/document/5306797/},
	doi = {10.1109/IISWC.2009.5306797},
	abstract = {This paper presents and characterizes Rodinia, a benchmark suite for heterogeneous computing. To help architects study emerging platforms such as GPUs (Graphics Processing Units), Rodinia includes applications and kernels which target multi-core CPU and GPU platforms. The choice of applications is inspired by Berkeley’s dwarf taxonomy. Our characterization shows that the Rodinia benchmarks cover a wide range of parallel communication patterns, synchronization techniques and power consumption, and has led to some important architectural insight, such as the growing importance of memory-bandwidth limitations and the consequent importance of data layout.},
	language = {en},
	urldate = {2020-02-26},
	booktitle = {2009 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	publisher = {IEEE},
	author = {Che, Shuai and Boyer, Michael and Meng, Jiayuan and Tarjan, David and Sheaffer, Jeremy W. and Lee, Sang-Ha and Skadron, Kevin},
	month = oct,
	year = {2009},
	pages = {44--54},
}

@inproceedings{guerreiro_multi-kernel_2015,
	address = {Turku, Finland},
	title = {Multi-kernel {Auto}-{Tuning} on {GPUs}: {Performance} and {Energy}-{Aware} {Optimization}},
	isbn = {978-1-4799-8491-6},
	shorttitle = {Multi-kernel {Auto}-{Tuning} on {GPUs}},
	url = {http://ieeexplore.ieee.org/document/7092758/},
	doi = {10.1109/PDP.2015.44},
	abstract = {Prompted by their very high computational capabilities and memory bandwidth, Graphics Processing Units (GPUs) are already widely used to accelerate the execution of many scientiﬁc applications. However, programmers are still required to have a very detailed knowledge of the GPU internal architecture when tuning the kernels, in order to improve either performance or energy-efﬁciency. Moreover, different GPU devices have different characteristics, moving a kernel to a different GPU typically requires re-tuning the kernel execution, in order to efﬁciently exploit the underlying hardware. The procedure proposed in this work is based on real-time kernel proﬁling and GPU monitoring and it automatically tunes parameters from several concurrent kernels to maximize the performance or minimize the energy consumption. Experimental results on NVIDIA GPU devices with up to 4 concurrent kernels show that the proposed solution achieves near optimal conﬁgurations. Furthermore, signiﬁcant energy savings can be achieved by using the proposed energyefﬁciency auto-tunning procedure.},
	language = {en},
	urldate = {2020-02-26},
	booktitle = {2015 23rd {Euromicro} {International} {Conference} on {Parallel}, {Distributed}, and {Network}-{Based} {Processing}},
	publisher = {IEEE},
	author = {Guerreiro, Joao and Ilic, Aleksandar and Roma, Nuno and Tomas, Pedro},
	month = mar,
	year = {2015},
	pages = {438--445},
}

@inproceedings{bergstra_machine_2012-2,
	title = {Machine learning for predictive auto-tuning with boosted regression trees},
	doi = {10.1109/InPar.2012.6339587},
	abstract = {The rapidly evolving landscape of multicore architectures makes the construction of efficient libraries a daunting task. A family of methods known collectively as “auto-tuning” has emerged to address this challenge. Two major approaches to auto-tuning are empirical and model-based: empirical autotuning is a generic but slow approach that works by measuring runtimes of candidate implementations, model-based auto-tuning predicts those runtimes using simplified abstractions designed by hand. We show that machine learning methods for non-linear regression can be used to estimate timing models from data, capturing the best of both approaches. A statistically-derived model offers the speed of a model-based approach, with the generality and simplicity of empirical auto-tuning. We validate our approach using the filterbank correlation kernel described in Pinto and Cox [2012], where we find that 0.1 seconds of hill climbing on the regression model (“predictive auto-tuning”) can achieve almost the same speed-up as is brought by minutes of empirical auto-tuning. Our approach is not specific to filterbank correlation, nor even to GPU kernel auto-tuning, and can be applied to almost any templated-code optimization problem, spanning a wide variety of problem types, kernel types, and platforms.},
	booktitle = {2012 {Innovative} {Parallel} {Computing} ({InPar})},
	author = {Bergstra, James and Pinto, Nicolas and Cox, David},
	month = may,
	year = {2012},
	note = {ISSN: null},
	keywords = {Correlation, Graphics processing unit, Instruction sets, Kernel, Libraries, Optimization, Regression tree analysis, boosted regression trees, candidate implementation runtime measurement, channel bank filters, empirical autotuning, filterbank correlation kernel, learning (artificial intelligence), machine learning, model-based approach, model-based autotuning, multicore architectures, multiprocessing systems, nonlinear regression, predictive autotuning, regression analysis, statistically-derived model, templated-code optimization problem, time 0.1 s, timing model estimation, trees (mathematics)},
	pages = {1--9},
}

@article{danka_modal_2018,
	title = {{modAL}: {A} modular active learning framework for {Python}},
	shorttitle = {{modAL}},
	url = {http://arxiv.org/abs/1805.00979},
	abstract = {modAL is a modular active learning framework for Python, aimed to make active learning research and practice simpler. Its distinguishing features are (i) clear and modular object oriented design (ii) full compatibility with scikit-learn models and workflows. These features make fast prototyping and easy extensibility possible, aiding the development of real-life active learning pipelines and novel algorithms as well. modAL is fully open source, hosted on GitHub at https://github.com/cosmic-cortex/modAL. To assure code quality, extensive unit tests are provided and continuous integration is applied. In addition, a detailed documentation with several tutorials are also available for ease of use. The framework is available in PyPI and distributed under the MIT license.},
	language = {en},
	urldate = {2020-02-25},
	journal = {arXiv:1805.00979 [cs, stat]},
	author = {Danka, Tivadar and Horvath, Peter},
	month = dec,
	year = {2018},
	note = {arXiv: 1805.00979},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{garrido-merchan_dealing_2020,
	title = {Dealing with categorical and integer-valued variables in {Bayesian} {Optimization} with {Gaussian} processes},
	volume = {380},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231219315619},
	doi = {10.1016/j.neucom.2019.11.004},
	abstract = {Some optimization problems are characterized by an objective that is very expensive, that lacks an analytical expression, and whose evaluations can be contaminated by noise. Bayesian Optimization (BO) methods can be used to solve these problems efficiently. BO relies on a probabilistic model of the objective, which is typically a Gaussian process (GP). This model is used to compute an acquisition function that estimates the expected utility (for solving the optimization problem) of evaluating the objective at each potential new point. A problem with GPs is, however, that they assume real-valued input variables and cannot easily deal with categorical or integer-valued values. Common methods to account for these variables, before evaluating the objective, include assuming they are real and then using a one-hot encoding, for categorical variables, or rounding to the closest integer, for integer-valued variables. We show that this leads to suboptimal results and introduce a novel approach to tackle categorical or integer-valued input variables within the context of BO with GPs. Several synthetic and real-world experiments support our hypotheses and show that our approach outperforms the results of standard BO using GPs on problems with categorical or integer-valued input variables.},
	language = {en},
	urldate = {2020-02-24},
	journal = {Neurocomputing},
	author = {Garrido-Merchán, Eduardo C. and Hernández-Lobato, Daniel},
	month = mar,
	year = {2020},
	keywords = {Bayesian optimization, Categorical variables, Gaussian processes, Integer-valued variables, Parameter tuning},
	pages = {20--35},
}

@article{garrido-merchan_dealing_2020-1,
	title = {Dealing with categorical and integer-valued variables in {Bayesian} {Optimization} with {Gaussian} processes},
	volume = {380},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231219315619},
	doi = {10.1016/j.neucom.2019.11.004},
	abstract = {Some optimization problems are characterized by an objective that is very expensive, that lacks an analytical expression, and whose evaluations can be contaminated by noise. Bayesian Optimization (BO) methods can be used to solve these problems efficiently. BO relies on a probabilistic model of the objective, which is typically a Gaussian process (GP). This model is used to compute an acquisition function that estimates the expected utility (for solving the optimization problem) of evaluating the objective at each potential new point. A problem with GPs is, however, that they assume real-valued input variables and cannot easily deal with categorical or integer-valued values. Common methods to account for these variables, before evaluating the objective, include assuming they are real and then using a one-hot encoding, for categorical variables, or rounding to the closest integer, for integer-valued variables. We show that this leads to suboptimal results and introduce a novel approach to tackle categorical or integer-valued input variables within the context of BO with GPs. Several synthetic and real-world experiments support our hypotheses and show that our approach outperforms the results of standard BO using GPs on problems with categorical or integer-valued input variables.},
	language = {en},
	urldate = {2020-02-24},
	journal = {Neurocomputing},
	author = {Garrido-Merchán, Eduardo C. and Hernández-Lobato, Daniel},
	month = mar,
	year = {2020},
	keywords = {Bayesian optimization, Categorical variables, Gaussian processes, Integer-valued variables, Parameter tuning},
	pages = {20--35},
}

@article{garrido-merchan_dealing_2020-2,
	title = {Dealing with {Integer}-valued {Variables} in {Bayesian} {Optimization} with {Gaussian} {Processes}},
	volume = {380},
	issn = {09252312},
	url = {http://arxiv.org/abs/1706.03673},
	doi = {10.1016/j.neucom.2019.11.004},
	abstract = {Bayesian optimization (BO) methods are useful for optimizing functions that are expensive to evaluate, lack an analytical expression and whose evaluations can be contaminated by noise. These methods rely on a probabilistic model of the objective function, typically a Gaussian process (GP), upon which an acquisition function is built. This function guides the optimization process and measures the expected utility of performing an evaluation of the objective at a new point. GPs assume continous input variables. When this is not the case, such as when some of the input variables take integer values, one has to introduce extra approximations. A common approach is to round the suggested variable value to the closest integer before doing the evaluation of the objective. We show that this can lead to problems in the optimization process and describe a more principled approach to account for input variables that are integer-valued. We illustrate in both synthetic and a real experiments the utility of our approach, which signiﬁcantly improves the results of standard BO methods on problems involving integer-valued variables.},
	language = {en},
	urldate = {2020-02-24},
	journal = {Neurocomputing},
	author = {Garrido-Merchán, Eduardo C. and Hernández-Lobato, Daniel},
	month = mar,
	year = {2020},
	note = {arXiv: 1706.03673},
	keywords = {Statistics - Machine Learning},
	pages = {20--35},
}

@article{misic_optimization_2019,
	title = {Optimization of {Tree} {Ensembles}},
	url = {http://arxiv.org/abs/1705.10883},
	abstract = {Tree ensemble models such as random forests and boosted trees are among the most widely used and practically successful predictive models in applied machine learning and business analytics. Although such models have been used to make predictions based on exogenous, uncontrollable independent variables, they are increasingly being used to make predictions where the independent variables are controllable and are also decision variables. In this paper, we study the problem of tree ensemble optimization: given a tree ensemble that predicts some dependent variable using controllable independent variables, how should we set these variables so as to maximize the predicted value? We formulate the problem as a mixed-integer optimization problem. We theoretically examine the strength of our formulation, provide a hierarchy of approximate formulations with bounds on approximation quality and exploit the structure of the problem to develop two large-scale solution methods, one based on Benders decomposition and one based on iteratively generating tree split constraints. We test our methodology on real data sets, including two case studies in drug design and customized pricing, and show that our methodology can efficiently solve large-scale instances to near or full optimality, and outperforms solutions obtained by heuristic approaches. In our drug design case, we show how our approach can identify compounds that efficiently trade-off predicted performance and novelty with respect to existing, known compounds. In our customized pricing case, we show how our approach can efficiently determine optimal store-level prices under a random forest model that delivers excellent predictive accuracy.},
	language = {en},
	urldate = {2020-02-14},
	journal = {arXiv:1705.10883 [cs, math, stat]},
	author = {Mišić, Velibor V.},
	month = oct,
	year = {2019},
	note = {arXiv: 1705.10883},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{koehrsen_introductory_2018,
	title = {An {Introductory} {Example} of {Bayesian} {Optimization} in {Python} with {Hyperopt}},
	url = {https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0},
	abstract = {A hands-on example for learning the foundations of a powerful optimization framework},
	language = {en},
	urldate = {2020-02-14},
	journal = {Medium},
	author = {Koehrsen, Will},
	month = jun,
	year = {2018},
}

@article{snoek_practical_nodate-1,
	title = {Practical {Bayesian} {Optimization} of {Machine} {Learning} {Algorithms}},
	abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a “black art” requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm’s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
	language = {en},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
	pages = {9},
}

@article{wang_efficient_2020,
	title = {Efficient {Performance} {Estimation} and {Work}-{Group} {Size} {Pruning} for {OpenCL} {Kernels} on {GPUs}},
	volume = {31},
	issn = {2161-9883},
	doi = {10.1109/TPDS.2019.2958343},
	abstract = {Graphic Processing Units (GPUs) play a vital role in state-of-the-art high-performance scientific computing realm and research work towards its performance analysis is crucial but nontrivial. Extant GPU performance models are far from practical use, while fine-grained GPU simulation requires a considerably large time cost. Moreover, massive amounts of designs with various program inputs and parameter settings pose a challenge for efficient performance estimation and tuning of parallel GPU applications. To this end, this article presents a hybrid framework for the efficient performance estimation and work-group size pruning of OpenCL workloads on GPUs. The framework contains a static module used to extract the kernel execution trace from the high-level source code and a dynamical module used to mimic the kernel execution flow to estimate the runtime performance. For the design space pruning, an extra analysis is performed to filter out the redundant work-group sizes with duplicated execution traces and inferior pipelines. The proposed framework does not require any program runs to estimate the performance and find the optimal or near-optimal designs. Experiments on four Commercial Off-The-Shelf (COTS) Nvidia GPUs show that the framework can predict the runtime performance with an average error of 17.04 percent and reduce the program design space by an average of 78.47 percent.},
	number = {5},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Wang, Xiebing and Qian, Xuehai and Knoll, Alois and Huang, Kai},
	month = may,
	year = {2020},
	keywords = {Analytical models, Estimation, GPU, Graphics processing units, Hardware, Kernel, Measurement, OpenCL, Runtime, performance estimation, performance tuning, work-group size},
	pages = {1089--1106},
}

@article{van_werkhoven_kernel_2019,
	title = {Kernel {Tuner}: {A} search-optimizing {GPU} code auto-tuner},
	volume = {90},
	issn = {0167-739X},
	shorttitle = {Kernel {Tuner}},
	url = {http://www.sciencedirect.com/science/article/pii/S0167739X18313359},
	doi = {10.1016/j.future.2018.08.004},
	abstract = {A very common problem in GPU programming is that some combination of thread block dimensions and other code optimization parameters, like tiling or unrolling factors, results in dramatically better performance than other kernel configurations. To obtain highly-efficient kernels it is often required to search vast and discontinuous search spaces that consist of all possible combinations of values for all tunable parameters. This paper presents Kernel Tuner, an easy-to-use tool for testing and auto-tuning OpenCL, CUDA, and C kernels with support for many search optimization algorithms that accelerate the tuning process. This paper introduces the application of many new solvers and global optimization algorithms for auto-tuning GPU applications. We demonstrate that Kernel Tuner can be used in a wide range of application scenarios and drastically decreases the time spent tuning, e.g. tuning a GEMM kernel on AMD Vega Frontier Edition 71.2x faster than brute force search.},
	language = {en},
	urldate = {2020-02-12},
	journal = {Future Generation Computer Systems},
	author = {van Werkhoven, Ben},
	month = jan,
	year = {2019},
	keywords = {Auto-tuning, GPU computing, Parallel programming, Performance optimization, Software development},
	pages = {347--358},
}

@article{taheri_tool_2019,
	title = {A {Tool} for {Automatically} {Suggesting} {Source}-{Code} {Optimizations} for {Complex} {GPU} {Kernels}},
	url = {http://arxiv.org/abs/1910.07776},
	abstract = {Future computing systems, from handhelds to supercomputers, will undoubtedly be more parallel and heterogeneous than todays systems to provide more performance and energy efficiency. Thus, GPUs are increasingly being used to accelerate general purpose applications, including applications with data dependent, irregular control flow and memory access patterns. However, the growing complexity, exposed memory hierarchy, incoherence, heterogeneity, and parallelism will make accelerator based systems progressively more difficult to program. In the foreseeable future, the vast majority of programmers will no longer be able to extract additional performance or energy savings from next generation systems be-cause the programming will be too difficult. Automatic performance analysis and optimization recommendation tools have the potential to avert this situation. They embody expert knowledge and make it available to software developers when needed. In this paper, we describe and evaluate such a tool.},
	urldate = {2020-02-12},
	journal = {arXiv:1910.07776 [cs]},
	author = {Taheri, Saeed and Qasem, Apan and Burtscher, Martin},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.07776},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance, Computer Science - Software Engineering},
}

@article{grebhahn_predicting_2019,
	title = {Predicting {Performance} of {Software} {Configurations}: {There} is no {Silver} {Bullet}},
	shorttitle = {Predicting {Performance} of {Software} {Configurations}},
	url = {http://arxiv.org/abs/1911.12643},
	abstract = {Many software systems offer configuration options to tailor their functionality and non-functional properties (e.g., performance). Often, users are interested in the (performance-)optimal configuration, but struggle to find it, due to missing information on influences of individual configuration options and their interactions. In the past, various supervised machine-learning techniques have been used to predict the performance of all configurations and to identify the optimal one. In the literature, there is a large number of machine-learning techniques and sampling strategies to select from. It is unclear, though, to what extent they affect prediction accuracy. We have conducted a comparative study regarding the mean prediction accuracy when predicting the performance of all configurations considering 6 machine-learning techniques, 18 sampling strategies, and 6 subject software systems. We found that both the learning technique and the sampling strategy have a strong influence on prediction accuracy. We further observed that some learning techniques (e.g., random forests) outperform other learning techniques (e.g., k-nearest neighbor) in most cases. Moreover, as the prediction accuracy strongly depends on the subject system, there is no combination of a learning technique and sampling strategy that is optimal in all cases, considering the tradeoff between accuracy and measurement overhead, which is in line with the famous no-free-lunch theorem.},
	urldate = {2020-02-12},
	journal = {arXiv:1911.12643 [cs]},
	author = {Grebhahn, Alexander and Siegmund, Norbert and Apel, Sven},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.12643},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{jia_starchart_2013,
	title = {Starchart: {Hardware} and software optimization using recursive partitioning regression trees},
	shorttitle = {Starchart},
	doi = {10.1109/PACT.2013.6618822},
	abstract = {Graphics processing units (GPUs) are in increasingly wide use, but significant hurdles lie in selecting the appropriate algorithms, runtime parameter settings, and hardware configurations to achieve power and performance goals with them. Exploring hardware and software choices requires time-consuming simulations or extensive real-system measurements. While some auto-tuning support has been proposed, it is often narrow in scope and heuristic in operation. This paper proposes and evaluates a statistical analysis technique, Starchart, that partitions the GPU hardware/software tuning space by automatically discerning important inflection points in design parameter values. Unlike prior methods, Starchart can identify the best parameter choices within different regions of the space. Our tool is efficient - evaluating at most 0.3\% of the tuning space, and often much less - and is robust enough to analyze highly variable real-system measurements, not just simulation. In one case study, we use it to automatically find platform-specific parameter settings that are 6.3× faster (for AMD) and 1.3× faster (for NVIDIA) than a single general setting. We also show how power-optimized parameter settings can save 47W (26\% of total GPU power) with little performance loss. Overall, Starchart can serve as a foundation for a range of GPU compiler optimizations, auto-tuners, and programmer tools. Furthermore, because Starchart does not rely on specific GPU features, we expect it to be useful for broader CPU/GPU studies as well.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques}},
	author = {Jia, Wenhao and Shaw, Kelly A. and Martonosi, Margaret},
	month = sep,
	year = {2013},
	note = {ISSN: 1089-795X},
	keywords = {GPU, GPU compiler optimizations, GPU hardware/software tuning space, Graphics processing units, Hardware, Kernel, NVIDIA, Optimization, Power measurement, Starchart, Tuning, auto-tuners, auto-tuning, auto-tuning support, decision tree, design parameter values, design space exploration, graphics processing units, hardware configurations, hardware optimization, hardware-software codesign, platform-specific parameter settings, power-optimized parameter settings, program compilers, programmer tools, real-system measurements, recursive estimation, recursive partitioning regression trees, regression analysis, regression tree, runtime parameter settings, software optimization, statistical analysis technique, time-consuming simulations, trees (mathematics)},
	pages = {257--267},
}

@article{bergstra_random_nodate,
	title = {Random {Search} for {Hyper}-{Parameter} {Optimization}},
	abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efﬁcient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to conﬁgure neural networks and deep belief networks. Compared with neural networks conﬁgured by a pure grid search, we ﬁnd that random search over the same domain is able to ﬁnd models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search ﬁnds better models by effectively searching a larger, less promising conﬁguration space. Compared with deep belief networks conﬁgured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional conﬁguration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for conﬁguring algorithms for new data sets. Our analysis casts some light on why recent “High Throughput” methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
	language = {en},
	author = {Bergstra, James and Bengio, Yoshua},
	pages = {25},
}

@inproceedings{sourouri_towards_2017,
	address = {Denver, Colorado},
	series = {{SC} '17},
	title = {Towards fine-grained dynamic tuning of {HPC} applications on modern multi-core architectures},
	isbn = {978-1-4503-5114-0},
	url = {https://doi.org/10.1145/3126908.3126945},
	doi = {10.1145/3126908.3126945},
	abstract = {There is a consensus that exascale systems should operate within a power envelope of 20MW. Consequently, energy conservation is still considered as the most crucial constraint if such systems are to be realized. So far, most research on this topic focused on strategies such as power capping and dynamic power management. Although these approaches can reduce power consumption, we believe that they might not be sufficient to reach the exascale energy-efficiency goals. Hence, we aim to adopt techniques from embedded systems, where energy-efficiency has always been the fundamental objective. A successful energy-saving technique used in embedded systems is to integrate fine-grained autotuning with dynamic voltage and frequency scaling. In this paper, we apply a similar technique to a real-world HPC application. Our experimental results on a HPC cluster indicate that such an approach saves up to 20\% of energy compared to the baseline configuration, with negligible performance loss.},
	urldate = {2020-02-01},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Sourouri, Mohammed and Raknes, Espen Birger and Reissmann, Nico and Langguth, Johannes and Hackenberg, Daniel and Schöne, Robert and Kjeldsberg, Per Gunnar},
	month = nov,
	year = {2017},
	keywords = {autotuning, dynamic tuning, dynamic voltage and frequency scaling, energy-efficiency, high performance computing},
	pages = {1--12},
}

@misc{science_crash_2018,
	title = {Crash {Course}: {Pool}-{Based} {Sampling} in {Active} {Learning}},
	shorttitle = {Crash {Course}},
	url = {https://medium.com/@ODSC/crash-course-pool-based-sampling-in-active-learning-cb40e30d49df},
	abstract = {Active learning is a class of machine learning problems where labeled data isn’t available for supervised algorithms.},
	language = {en},
	urldate = {2020-01-30},
	journal = {Medium},
	author = {Science, ODSC-Open Data},
	month = nov,
	year = {2018},
}

@article{vinyals_matching_2017,
	title = {Matching {Networks} for {One} {Shot} {Learning}},
	url = {http://arxiv.org/abs/1606.04080},
	abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6\% to 93.2\% and from 88.0\% to 93.8\% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
	urldate = {2020-01-29},
	journal = {arXiv:1606.04080 [cs, stat]},
	author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
	month = dec,
	year = {2017},
	note = {arXiv: 1606.04080},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{snell_prototypical_2017,
	title = {Prototypical {Networks} for {Few}-shot {Learning}},
	url = {http://arxiv.org/abs/1703.05175},
	abstract = {We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
	urldate = {2020-01-29},
	journal = {arXiv:1703.05175 [cs, stat]},
	author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
	month = jun,
	year = {2017},
	note = {arXiv: 1703.05175},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{konyushkova_learning_2017,
	title = {Learning {Active} {Learning} from {Data}},
	url = {http://papers.nips.cc/paper/7010-learning-active-learning-from-data.pdf},
	urldate = {2020-01-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Konyushkova, Ksenia and Sznitman, Raphael and Fua, Pascal},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {4225--4235},
}

@article{bachman_learning_2017,
	title = {Learning {Algorithms} for {Active} {Learning}},
	url = {http://arxiv.org/abs/1708.00088},
	abstract = {We introduce a model that learns active learning algorithms via metalearning. For a distribution of related tasks, our model jointly learns: a data representation, an item selection heuristic, and a method for constructing prediction functions from labeled training sets. Our model uses the item selection heuristic to gather labeled training sets from which to construct prediction functions. Using the Omniglot and MovieLens datasets, we test our model in synthetic and practical settings.},
	language = {en},
	urldate = {2020-01-29},
	journal = {arXiv:1708.00088 [cs]},
	author = {Bachman, Philip and Sordoni, Alessandro and Trischler, Adam},
	month = jul,
	year = {2017},
	note = {arXiv: 1708.00088},
	keywords = {Computer Science - Machine Learning},
}

@article{woodward_active_2017,
	title = {Active {One}-shot {Learning}},
	url = {http://arxiv.org/abs/1702.06559},
	abstract = {Recent advances in one-shot learning have produced models that can learn from a handful of labeled examples, for passive classification and regression tasks. This paper combines reinforcement learning with one-shot learning, allowing the model to decide, during classification, which examples are worth labeling. We introduce a classification task in which a stream of images are presented and, on each time step, a decision must be made to either predict a label or pay to receive the correct label. We present a recurrent neural network based action-value function, and demonstrate its ability to learn how and when to request labels. Through the choice of reward function, the model can achieve a higher prediction accuracy than a similar model on a purely supervised task, or trade prediction accuracy for fewer label requests.},
	urldate = {2020-01-29},
	journal = {arXiv:1702.06559 [cs]},
	author = {Woodward, Mark and Finn, Chelsea},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.06559},
	keywords = {Computer Science - Machine Learning},
}

@article{ravi_meta-learning_2018,
	title = {Meta-{Learning} for {Batch} {Mode} {Active} {Learning}},
	url = {https://openreview.net/forum?id=r1PsGFJPz},
	abstract = {Active learning involves selecting unlabeled data items to label in order to best improve an existing classifier. In most applications, batch mode active learning, where a set of items is picked...},
	urldate = {2020-01-29},
	author = {Ravi, Sachin and Larochelle, Hugo},
	month = feb,
	year = {2018},
}

@article{pang_meta-learning_2018,
	title = {Meta-{Learning} {Transferable} {Active} {Learning} {Policies} by {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1806.04798},
	abstract = {Active learning (AL) aims to enable training high performance classifiers with low annotation cost by predicting which subset of unlabelled instances would be most beneficial to label. The importance of AL has motivated extensive research, proposing a wide variety of manually designed AL algorithms with diverse theoretical and intuitive motivations. In contrast to this body of research, we propose to treat active learning algorithm design as a meta-learning problem and learn the best criterion from data. We model an active learning algorithm as a deep neural network that inputs the base learner state and the unlabelled point set and predicts the best point to annotate next. Training this active query policy network with reinforcement learning, produces the best non-myopic policy for a given dataset. The key challenge in achieving a general solution to AL then becomes that of learner generalisation, particularly across heterogeneous datasets. We propose a multi-task dataset-embedding approach that allows dataset-agnostic active learners to be trained. Our evaluation shows that AL algorithms trained in this way can directly generalise across diverse problems.},
	urldate = {2020-01-29},
	journal = {arXiv:1806.04798 [cs, stat]},
	author = {Pang, Kunkun and Dong, Mingzhi and Wu, Yang and Hospedales, Timothy},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.04798},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{thiagarajan_bootstrapping_2018,
	address = {Beijing, China},
	series = {{ICS} '18},
	title = {Bootstrapping {Parameter} {Space} {Exploration} for {Fast} {Tuning}},
	isbn = {978-1-4503-5783-8},
	url = {https://doi.org/10.1145/3205289.3205321},
	doi = {10.1145/3205289.3205321},
	abstract = {The task of tuning parameters for optimizing performance or other metrics of interest such as energy, variability, etc. can be resource and time consuming. Presence of a large parameter space makes a comprehensive exploration infeasible. In this paper, we propose a novel bootstrap scheme, called GEIST, for parameter space exploration to find performance-optimizing configurations quickly. Our scheme represents the parameter space as a graph whose connectivity guides information propagation from known configurations. Guided by the predictions of a semi-supervised learning method over the parameter graph, GEIST is able to adaptively sample and find desirable configurations using limited results from experiments. We show the effectiveness of GEIST for selecting application input options, compiler flags, and runtime/system settings for several parallel codes including LULESH, Kripke, Hypre, and OpenAtom.},
	urldate = {2020-01-27},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Thiagarajan, Jayaraman J. and Jain, Nikhil and Anirudh, Rushil and Gimenez, Alfredo and Sridhar, Rahul and Marathe, Aniruddha and Wang, Tao and Emani, Murali and Bhatele, Abhinav and Gamblin, Todd},
	month = jun,
	year = {2018},
	keywords = {autotuning, performance, sampling, semi-supervised learning},
	pages = {385--395},
}

@inproceedings{malakar_benchmarking_2018,
	title = {Benchmarking {Machine} {Learning} {Methods} for {Performance} {Modeling} of {Scientific} {Applications}},
	doi = {10.1109/PMBS.2018.8641686},
	abstract = {Performance modeling is an important and active area of research in high-performance computing (HPC). It helps in better job scheduling and also improves overall performance of coupled applications. Sufficiently rich analytical models are challenging to develop, however, because of interactions between different node components, network topologies, job interference, and application complexity. When analytical performance models become restrictive because of application dynamics and/or multicomponent interactions, machine-learning-based performance models can be helpful. While machine learning (ML) methods do not require underlying system or application knowledge, they are efficient in learning the unknown interactions of the application and system parameters empirically using application runs. We present a benchmark study in which we evaluate eleven machine learning methods for modeling the performance of four representative scientific applications that are irregular and with skewed domain configurations on four leadership-class HPC platforms. We assess the impact of feature engineering, size of training set, modern hardware platforms, transfer learning, extrapolation on the prediction accuracy, and training and inference times. We find that bagging, boosting, and deep neural network ML methods are promising approaches with median R2 values greater than 0.95 and these methods do not require feature engineering. We demonstrate that cross-platform performance prediction can be improved significantly using transfer learning with deep neural networks.},
	booktitle = {2018 {IEEE}/{ACM} {Performance} {Modeling}, {Benchmarking} and {Simulation} of {High} {Performance} {Computer} {Systems} ({PMBS})},
	author = {Malakar, Preeti and Balaprakash, Prasanna and Vishwanath, Venkatram and Morozov, Vitali and Kumaran, Kalyan},
	month = nov,
	year = {2018},
	note = {ISSN: null},
	keywords = {Adaptation models, Analytical models, Benchmark testing, Computational modeling, Machine learning, Neural networks, Predictive models, analytical models, analytical performance models, application complexity, application dynamics, benchmark testing, benchmarking, cross-platform performance prediction, data analysis, deep neural network ML methods, high-performance computing, job interference, job scheduling, leadership-class HPC platforms, learning (artificial intelligence), machine learning, machine learning methods, multicomponent interactions, neural nets, parallel processing, performance modeling, representative scientific applications, transfer learning},
	pages = {33--44},
}

@inproceedings{jamshidi_transfer_2017,
	title = {Transfer learning for performance modeling of configurable systems: {An} exploratory analysis},
	shorttitle = {Transfer learning for performance modeling of configurable systems},
	doi = {10.1109/ASE.2017.8115661},
	abstract = {Modern software systems provide many configuration options which significantly influence their non-functional properties. To understand and predict the effect of configuration options, several sampling and learning strategies have been proposed, albeit often with significant cost to cover the highly dimensional configuration space. Recently, transfer learning has been applied to reduce the effort of constructing performance models by transferring knowledge about performance behavior across environments. While this line of research is promising to learn more accurate models at a lower cost, it is unclear why and when transfer learning works for performance modeling. To shed light on when it is beneficial to apply transfer learning, we conducted an empirical study on four popular software systems, varying software configurations and environmental conditions, such as hardware, workload, and software versions, to identify the key knowledge pieces that can be exploited for transfer learning. Our results show that in small environmental changes (e.g., homogeneous workload change), by applying a linear transformation to the performance model, we can understand the performance behavior of the target environment, while for severe environmental changes (e.g., drastic workload change) we can transfer only knowledge that makes sampling more efficient, e.g., by reducing the dimensionality of the configuration space.},
	booktitle = {2017 32nd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Jamshidi, Pooyan and Siegmund, Norbert and Velez, Miguel and Kästner, Christian and Patel, Akshay and Agarwal, Yuvraj},
	month = oct,
	year = {2017},
	note = {ISSN: null},
	keywords = {Analytical models, Hardware, Mobile communication, Performance analysis, Predictive models, Reliability, Software systems, configurable systems, highly dimensional configuration space, learning (artificial intelligence), performance behavior, performance model, performance modeling, program diagnostics, software configurations, software engineering, software performance evaluation, software systems, transfer learning},
	pages = {497--508},
}

@inproceedings{kloh_towards_2019,
	title = {Towards an {Autonomous} {Framework} for {HPC} {Optimization}: {Using} {Machine} {Learning} for {Energy} and {Performance} {Modeling}},
	shorttitle = {Towards an {Autonomous} {Framework} for {HPC} {Optimization}},
	url = {https://sol.sbc.org.br/index.php/wscad/article/view/8689},
	doi = {10.5753/wscad.2019.8689},
	abstract = {Performance and energy efﬁciency are now critical concerns in high performance scientiﬁc computing. It is expected that requirements of the scientiﬁc problem should guide the orchestration of different techniques of energy saving, in order to improve the balance between energy consumption and application performance. To enable this balance, we propose the development of an autonomous framework to make this orchestration and present the ongoing research to this development, more speciﬁcally, focusing in the characterization of the scientiﬁc applications and the performance modeling tasks using Machine Learning.},
	language = {en},
	urldate = {2020-01-27},
	booktitle = {Anais do {Simpósio} em {Sistemas} {Computacionais} de {Alto} {Desempenho} ({WSCAD})},
	publisher = {Sociedade Brasileira de Computação},
	author = {Klôh, Vinícius and Gritz, Matheus and Schulze, Bruno and Ferro, Mariza},
	month = nov,
	year = {2019},
	pages = {438--445},
}

@inproceedings{marathe_performance_2017,
	address = {Denver, Colorado},
	series = {{SC} '17},
	title = {Performance modeling under resource constraints using deep transfer learning},
	isbn = {978-1-4503-5114-0},
	url = {https://doi.org/10.1145/3126908.3126969},
	doi = {10.1145/3126908.3126969},
	abstract = {Tuning application parameters for optimal performance is a challenging combinatorial problem. Hence, techniques for modeling the functional relationships between various input features in the parameter space and application performance are important. We show that simple statistical inference techniques are inadequate to capture these relationships. Even with more complex ensembles of models, the minimum coverage of the parameter space required via experimental observations is still quite large. We propose a deep learning based approach that can combine information from exhaustive observations collected at a smaller scale with limited observations collected at a larger target scale. The proposed approach is able to accurately predict performance in the regimes of interest to performance analysts while outperforming many traditional techniques. In particular, our approach can identify the best performing configurations even when trained using as few as 1\% of observations at the target scale.},
	urldate = {2020-01-27},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Marathe, Aniruddha and Anirudh, Rushil and Jain, Nikhil and Bhatele, Abhinav and Thiagarajan, Jayaraman and Kailkhura, Bhavya and Yeom, Jae-Seung and Rountree, Barry and Gamblin, Todd},
	month = nov,
	year = {2017},
	keywords = {deep learning, parameter selection, performance prediction, transfer learning},
	pages = {1--12},
}

@inproceedings{sun_automated_2017,
	title = {Automated {Performance} {Modeling} {Based} on {Runtime} {Feature} {Detection} and {Machine} {Learning}},
	doi = {10.1109/ISPA/IUCC.2017.00115},
	abstract = {Automated performance modeling and performance prediction of parallel programs are highly valuable in many use cases, such as in guiding task management and job scheduling, offering insights of application behaviors, assisting resource requirement estimation, etc. The performance of parallel programs is affected by numerous factors, including but not limited to hardware, system software, applications, algorithms, and input parameters, thus an accurate performance prediction is often a challenging and daunting task. In this study, we focus on automatically predicting the execution time of parallel programs (more specifically, MPI programs) with different inputs, at different scale, and without domain knowledge. We model the correlation between the execution time and domain-independent runtime features. These features include values of variables, counters of branches, loops, and MPI communications. Through automatically instrumenting an MPI program, each execution of the program will output a feature vector and its corresponding execution time. After collecting data from executions with different inputs, a random forest machine learning approach is used to build an empirical performance model, which can predict the execution time of the program with a new input. Our experiments and analyses of three parallel programs, Graph500, GalaxSee and SMG2000, on three different systems show that our method performs well, with less than 20\% error in predictions on average.},
	booktitle = {2017 {IEEE} {International} {Symposium} on {Parallel} and {Distributed} {Processing} with {Applications} and 2017 {IEEE} {International} {Conference} on {Ubiquitous} {Computing} and {Communications} ({ISPA}/{IUCC})},
	author = {Sun, Jingwei and Zhan, Shiyan and Sun, Guangzhong and Chen, Yong},
	month = dec,
	year = {2017},
	note = {ISSN: null},
	keywords = {Analytical models, Computational modeling, Data models, Feature extraction, GalaxSee, Graph500, Instruments, MPI program, Predictive models, Runtime, SMG2000, accurate performance prediction, automated performance modeling, corresponding execution time, daunting task, domain-independent runtime features, feature vector, job scheduling, learning (artificial intelligence), machine learning, parallel computing, parallel programming, parallel programs, performance evaluation, performance modeling, runtime feature detection, task management},
	pages = {744--751},
}

@article{haj-ali_neurovectorizer_2020,
	title = {{NeuroVectorizer}: {End}-to-{End} {Vectorization} with {Deep} {Reinforcement} {Learning}},
	shorttitle = {{NeuroVectorizer}},
	url = {http://arxiv.org/abs/1909.13639},
	abstract = {One of the key challenges arising when compilers vectorize loops for today’s SIMD-compatible architectures is to decide if vectorization or interleaving is beneficial. Then, the compiler has to determine how many instructions to pack together and how many loop iterations to interleave. Compilers are designed today to use fixed-cost models that are based on heuristics to make vectorization decisions on loops. However, these models are unable to capture the data dependency, the computation graph, or the organization of instructions. Alternatively, software engineers often handwrite the vectorization factors of every loop. This, however, places a huge burden on them, since it requires prior experience and significantly increases the development time.},
	language = {en},
	urldate = {2020-01-22},
	journal = {arXiv:1909.13639 [cs]},
	author = {Haj-Ali, Ameer and Ahmed, Nesreen K. and Willke, Ted and Shao, Sophia and Asanovic, Krste and Stoica, Ion},
	month = jan,
	year = {2020},
	note = {arXiv: 1909.13639},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance, Computer Science - Programming Languages},
}

@article{s_ir2vec_2020,
	title = {{IR2Vec}: {A} {Flow} {Analysis} based {Scalable} {Infrastructure} for {Program} {Encodings}},
	shorttitle = {{IR2Vec}},
	url = {http://arxiv.org/abs/1909.06228},
	abstract = {We propose IR2Vec, a Concise and Scalable encoding infrastructure to represent programs as a distributed embedding in continuous space. This distributed embedding is obtained by combining representation learning methods with data and control flow information to capture the syntax as well as the semantics of the input programs. Our embeddings are obtained from the Intermediate Representation (IR) of the source code, and are both language as well as machine independent. The entities of the IR are modelled as relationships, and their representations are learned to form a seed embedding vocabulary. This vocabulary is used along with the flow analyses information to form a hierarchy of encodings based on various levels of program abstractions. We show the effectiveness of our methodology on a software engineering task (program classification) as well as optimization tasks (Heterogeneous device mapping and Thread coarsening). The embeddings generated by IR2Vec outperform the existing methods in all the three tasks even when using simple machine learning models. As we follow an agglomerative method of forming encodings at various levels using seed embedding vocabulary, our encoding is naturally more scalable and not data-hungry when compared to the other methods.},
	urldate = {2020-01-22},
	journal = {arXiv:1909.06228 [cs]},
	author = {S, Venkata Keerthy and Aggarwal, Rohit and Jain, Shalini and Desarkar, Maunendra Sankar and Upadrasta, Ramakrishna and Srikant, Y. N.},
	month = jan,
	year = {2020},
	note = {arXiv: 1909.06228},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@inproceedings{sung_poster_2019,
	title = {{POSTER}: {CogR}: {Exploiting} {Program} {Structures} for {Machine}-{Learning} {Based} {Runtime} {Solutions}},
	shorttitle = {{POSTER}},
	doi = {10.1109/PACT.2019.00057},
	abstract = {We propose CogR, a machine-learning based runtime solution, that enables efficient and dynamic resource scheduling and performance optimization for high-level programming interfaces on heterogeneous systems. CogR tightly combines the structural information of programs and fine-grained static and dynamic statistics into sequenced input data. This structural and value-embedded representation of programs enables CogR to accurately model the runtime behaviors of nested loop-based constructs in the high-level parallel programs. The end-to-end CogR system consists of compiler and runtime support for feature collection and input generation, a machine learning model, and a runtime scheduler with online inference and prediction. The system provides 11\% higher prediction accuracy than models simulated for prior work and improves kernel performance by 66\% compared to the baseline runtime.},
	booktitle = {2019 28th {International} {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques} ({PACT})},
	author = {Sung, Hyojin and Chen, Tong and Eichenberger, Alenxandre and O'Brien, Kevin K.},
	month = sep,
	year = {2019},
	note = {ISSN: 1089-795X},
	keywords = {Dynamic scheduling, Feature extraction, Graphics processing units, Machine learning, OpenMP, Predictive models, Runtime, Syntactics, baseline runtime, dynamic resource scheduling, dynamic statistics, end-to-end CogR system, exploiting program structures, feature collection, fine-grained static statistics, heterogeneous systems, high-level parallel programs, high-level programming interfaces, input generation, learning (artificial intelligence), machine learning, machine learning model, machine-learning based runtime solution, nested loop-based constructs, parallel programming, performance optimization, program compilers, program control structures, program diagnostics, runtime optimizations, runtime scheduler, runtime solutions, scheduling, sequenced input data},
	pages = {485--486},
}

@article{ben-nun_neural_nodate,
	title = {Neural {Code} {Comprehension}: {A} {Learnable} {Representation} of {Code} {Semantics}},
	abstract = {With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufﬁcient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we deﬁne an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel deﬁnition of contextual ﬂow for this IR, leveraging both the underlying data- and control-ﬂow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without ﬁne-tuning, a single RNN architecture and ﬁxed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classiﬁcation from raw code (104 classes), where we set a new state-of-the-art.},
	language = {en},
	author = {Ben-Nun, Tal and Jakobovits, Alice Shoshana and Hoeﬂer, Torsten},
	pages = {17},
}

@article{henkel_code_2018,
	title = {Code {Vectors}: {Understanding} {Programs} {Through} {Embedded} {Abstracted} {Symbolic} {Traces}},
	shorttitle = {Code {Vectors}},
	url = {http://arxiv.org/abs/1803.06686},
	doi = {10.1145/3236024.3236085},
	abstract = {With the rise of machine learning, there is a great deal of interest in treating programs as data to be fed to learning algorithms. However, programs do not start off in a form that is immediately amenable to most off-the-shelf learning techniques. Instead, it is necessary to transform the program to a suitable representation before a learning technique can be applied.},
	language = {en},
	urldate = {2020-01-22},
	journal = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering - ESEC/FSE 2018},
	author = {Henkel, Jordan and Lahiri, Shuvendu K. and Liblit, Ben and Reps, Thomas},
	year = {2018},
	note = {arXiv: 1803.06686},
	keywords = {Computer Science - Software Engineering},
	pages = {163--174},
}

@article{mikolov_distributed_nodate,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	abstract = {The recently introduced continuous Skip-gram model is an efﬁcient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain signiﬁcant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.},
	language = {en},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	pages = {9},
}

@article{wainakh_evaluating_2019,
	title = {Evaluating {Semantic} {Representations} of {Source} {Code}},
	url = {http://arxiv.org/abs/1910.05177},
	abstract = {Learned representations of source code enable various software developer tools, e.g., to detect bugs or to predict program properties. At the core of code representations often are word embeddings of identifier names in source code, because identifiers account for the majority of source code vocabulary and convey important semantic information. Unfortunately, there currently is no generally accepted way of evaluating the quality of word embeddings of identifiers, and current evaluations are biased toward specific downstream tasks. This paper presents IdBench, the first benchmark for evaluating to what extent word embeddings of identifiers represent semantic relatedness and similarity. The benchmark is based on thousands of ratings gathered by surveying 500 software developers. We use IdBench to evaluate state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions, as these are often used in current developer tools. Our results show that the effectiveness of embeddings varies significantly across different embedding techniques and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing embedding provides a satisfactory representation of semantic similarities, e.g., because embeddings consider identifiers with opposing meanings as similar, which may lead to fatal mistakes in downstream developer tools. IdBench provides a gold standard to guide the development of novel embeddings that address the current limitations.},
	urldate = {2020-01-22},
	journal = {arXiv:1910.05177 [cs, stat]},
	author = {Wainakh, Yaza and Rauf, Moiz and Pradel, Michael},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.05177},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Software Engineering, Statistics - Machine Learning},
}

@article{chen_literature_2019,
	title = {A {Literature} {Study} of {Embeddings} on {Source} {Code}},
	url = {http://arxiv.org/abs/1904.03061},
	abstract = {Natural language processing has improved tremendously after the success of word embedding techniques such as word2vec. Recently, the same idea has been applied on source code with encouraging results. In this survey, we aim to collect and discuss the usage of word embedding techniques on programs and source code. The articles in this survey have been collected by asking authors of related work and with an extensive search on Google Scholar. Each article is categorized into five categories: 1. embedding of tokens 2. embedding of functions or methods 3. embedding of sequences or sets of method calls 4. embedding of binary code 5. other embeddings. We also provide links to experimental data and show some remarkable visualization of code embeddings. In summary, word embedding has been successfully applied on different granularities of source code. With access to countless open-source repositories, we see a great potential of applying other data-driven natural language processing techniques on source code in the future.},
	urldate = {2020-01-22},
	journal = {arXiv:1904.03061 [cs, stat]},
	author = {Chen, Zimin and Monperrus, Martin},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.03061},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Software Engineering, Statistics - Machine Learning},
}

@inproceedings{kang_assessing_2019,
	address = {San Diego, CA, USA},
	title = {Assessing the {Generalizability} of {Code2vec} {Token} {Embeddings}},
	isbn = {978-1-72812-508-4},
	url = {https://ieeexplore.ieee.org/document/8952475/},
	doi = {10.1109/ASE.2019.00011},
	abstract = {Many Natural Language Processing (NLP) tasks, such as sentiment analysis or syntactic parsing, have beneﬁted from the development of word embedding models. In particular, regardless of the training algorithms, the learned embeddings have often been shown to be generalizable to different NLP tasks. In contrast, despite recent momentum on word embeddings for source code, the literature lacks evidence of their generalizability beyond the example task they have been trained for.},
	language = {en},
	urldate = {2020-01-22},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Kang, Hong Jin and Bissyande, Tegawende F. and Lo, David},
	month = nov,
	year = {2019},
	pages = {1--12},
}

@article{alon_code2seq_2019,
	title = {{CODE2SEQ}: {GENERATING} {SEQUENCES} {FROM} {STRUCTURED} {REPRESENTATIONS} {OF} {CODE}},
	abstract = {The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present CODE2SEQ: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples. Our model signiﬁcantly outperforms previous models that were speciﬁcally designed for programming languages, as well as state-of-the-art NMT models. An online demo of our model is available at http://code2seq.org. Our code, data and trained models are available at http://github.com/tech-srl/code2seq.},
	language = {en},
	author = {Alon, Uri and Levy, Omer and Brody, Shaked and Yahav, Eran},
	year = {2019},
	pages = {22},
}

@article{alon_code2vec_2018,
	title = {code2vec: {Learning} {Distributed} {Representations} of {Code}},
	shorttitle = {code2vec},
	url = {http://arxiv.org/abs/1803.09473},
	abstract = {We present a neural model for representing snippets of code as continuous distributed vectors ("code embeddings"). The main idea is to represent a code snippet as a single fixed-length \${\textbackslash}textit\{code vector\}\$, which can be used to predict semantic properties of the snippet. This is performed by decomposing code to a collection of paths in its abstract syntax tree, and learning the atomic representation of each path \${\textbackslash}textit\{simultaneously\}\$ with learning how to aggregate a set of them. We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 14M methods. We show that code vectors trained on this dataset can predict method names from files that were completely unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. Comparing previous techniques over the same data set, our approach obtains a relative improvement of over 75\%, being the first to successfully predict method names based on a large, cross-project, corpus. Our trained model, visualizations and vector similarities are available as an interactive online demo at http://code2vec.org. The code, data, and trained models are available at https://github.com/tech-srl/code2vec.},
	urldate = {2020-01-20},
	journal = {arXiv:1803.09473 [cs, stat]},
	author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
	month = oct,
	year = {2018},
	note = {arXiv: 1803.09473},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages, Statistics - Machine Learning},
}

@article{yu_efficient_2020,
	title = {Efficient and {Portable} {Workgroup} {Size} {Tuning}},
	volume = {31},
	issn = {2161-9883},
	doi = {10.1109/TPDS.2019.2937295},
	abstract = {The performance of an OpenCL program is strongly influenced by both hardware and software attributes. To achieve superior performance, developers may leverage automatic performance tuning techniques to determine the optimal parameters on the target device. Although existing approaches have shown promising tuning results in their target scenarios, other requirements such as efficiency, portability, and usability should also be considered because of the rapid growth of heterogeneous computing applications and platforms. In this paper, we re-examine the workgroup size tuning problem and propose a novel approach to meet the aforementioned requirements. We abstract the architectural details into a set of hardware parameters so that the proposed approach can be applied without the presence of target devices, which makes it more accessible to developers. The proposed approach is evaluated on 20 OpenCL kernels and six devices, including both CPUs and GPUs. Experimental results demonstrate that, with negligible overhead, our approach filters out 88.6 percent of the possible workgroup sizes on average. Among all the workgroup size candidates, the best- and worst-performing candidates can achieve average performance of 95.5 and 92.1 percent, respectively, compared with the optimal workgroup size.},
	number = {2},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Yu, Chia-Lin and Tsao, Shiao-Li},
	month = feb,
	year = {2020},
	keywords = {Computational modeling, Graphics processing units, Hardware, Indexes, Kernel, OpenCL, Performance evaluation, Tuning, automatic performance tuning, microbenchmarking, workgroup size selection},
	pages = {455--469},
}

@article{bernabe_portability_2019,
	title = {Portability {Study} of an {OpenCL} {Algorithm} for {Automatic} {Target} {Detection} in {Hyperspectral} {Images}},
	volume = {57},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2019.2927077},
	abstract = {In the last decades, the problem of target detection has received considerable attention in remote sensing applications. When this problem is tackled using hyperspectral images with hundreds of bands, the use of high-performance computing (HPC) is essential. One of the most popular algorithms in the hyperspectral image analysis community for this purpose is the automatic target detection and classification algorithm (ATDCA). Previous research has already investigated the mapping of ATDCA on HPC platforms such as multicore processors, graphics processing units (GPUs), and field-programmable gate arrays (FPGAs), showing impressive speedup factors (after careful fine-tuning) that allow for its exploitation in time-critical scenarios. However, the lack of standardization resulted in most implementations being too specific to a given architecture, eliminating (or at least making extremely difficult) code reusability across different platforms. In order to address this issue, we present a portability study of an implementation of ATDCA developed using the open computing language (OpenCL). We focus on cross-platform parameters such as performance, energy consumption, and code design complexity, as compared to previously developed (hand-tuned) implementations. Our portability study analyzes different strategies to expose data parallelism as well as enable the efficient exploitation of complex memory hierarchies in heterogeneous devices. We also conduct an assessment of energy consumption and discuss metrics to analyze the quality of our code. The conducted experiments-using synthetic and real hyperspectral data sets collected by the Hyperspectral Digital Imagery Collection Experiment (HYDICE) and NASA's Airborne Visible Infra-Red Imaging Spectrometer (AVIRIS)-demonstrate, for the first time in the literature, that portability across different HPC platforms can be achieved for real-time target detection in hyperspectral missions.},
	number = {11},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Bernabé, Sergio and García, Carlos and Igual, Francisco D. and Botella, Guillermo and Prieto-Matias, Manuel and Plaza, Antonio},
	month = nov,
	year = {2019},
	keywords = {ATDCA, Automatic target detection and classification algorithm (ATDCA), Computer architecture, Graphics processing units, HPC platforms, Hyperspectral imaging, Object detection, OpenCL algorithm, Performance evaluation, automatic target detection, classification algorithm, code design complexity, code quality, cross-platform parameters, energy consumption, field-programmable gate arrays, geophysical image processing, graphics processing units, high-performance computing, high-performance computing (HPC), hyperspectral digital imagery collection experiment, hyperspectral image analysis community, hyperspectral images, hyperspectral imaging, hyperspectral missions, image classification, object detection, open computing language, open computing language (OpenCL), parallel processing, portability, portability study, real-time target detection, remote sensing, remote sensing applications},
	pages = {9499--9511},
}

@misc{lanners_choosing_2019,
	title = {Choosing a {Scikit}-learn {Linear} {Regression} {Algorithm}},
	url = {https://towardsdatascience.com/choosing-a-scikit-learn-linear-regression-algorithm-dd96b48105f5},
	abstract = {A brief overview of the various Scikit-learn linear regression algorithms, and what cases they are typically most effective for. Includes…},
	language = {en},
	urldate = {2020-01-20},
	journal = {Medium},
	author = {Lanners, Quinn},
	month = jul,
	year = {2019},
}

@inproceedings{abadi_tensorflow_2016,
	title = {{TensorFlow}: {A} {System} for {Large}-{Scale} {Machine} {Learning}},
	isbn = {978-1-931971-33-1},
	shorttitle = {{TensorFlow}},
	url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
	language = {en},
	urldate = {2019-12-11},
	booktitle = {12th \{{USENIX}\} {Symposium} on {Operating} {Systems} {Design} and {Implementation} (\{{OSDI}\} 16)},
	author = {Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2016},
	pages = {265--283},
}

@inproceedings{seen_gpu_2014,
	address = {Penang, Malaysia},
	title = {{GPU} acceleration of {Runge} {Kutta}-{Fehlberg} and its comparison with {Dormand}-{Prince} method},
	url = {http://aip.scitation.org/doi/abs/10.1063/1.4887558},
	doi = {10.1063/1.4887558},
	abstract = {There is a significant reduction of processing time and speedup of performance in computer graphics with the emergence of Graphic Processing Units (GPUs). GPUs have been developed to surpass Central Processing Unit (CPU) in terms of performance and processing speed. This evolution has opened up a new area in computing and researches where highly parallel GPU has been used for non-graphical algorithms. Physical or phenomenal simulations and modelling can be accelerated through General Purpose Graphic Processing Units (GPGPU) and Compute Unified Device Architecture (CUDA) implementations. These phenomena can be represented with mathematical models in the form of Ordinary Differential Equations (ODEs) which encompasses the gist of change rate between independent and dependent variables. ODEs are numerically integrated over time in order to simulate these behaviours. The classical Runge-Kutta (RK) scheme is the common method used to numerically solve ODEs. The Runge Kutta Fehlberg (RKF) scheme has been specially developed to provide an estimate of the principal local truncation error at each step, known as embedding estimate technique. This paper delves into the implementation of RKF scheme for GPU devices and compares its result with Dorman Prince method. A pseudo code is developed to show the implementation in detail. Hence, practitioners will be able to understand the data allocation in GPU, formation of RKF kernels and the flow of data to/from GPU-CPU upon RKF kernel evaluation. The pseudo code is then written in C Language and two ODE models are executed to show the achievable speedup as compared to CPU implementation. The accuracy and efficiency of the proposed implementation method is discussed in the final section of this paper.},
	language = {en},
	urldate = {2019-10-08},
	booktitle = {{PROCEEDINGS} {OF} {THE} {21ST} {NATIONAL} {SYMPOSIUM} {ON} {MATHEMATICAL} {SCIENCES} ({SKSM21}): {Germination} of {Mathematical} {Sciences} {Education} and {Research} towards {Global} {Sustainability}},
	author = {Seen, Wo Mei and Gobithaasan, R. U. and Miura, Kenjiro T.},
	year = {2014},
	pages = {16--21},
}

@inproceedings{shamshad_deep_2019,
	title = {Deep {Ptych}: {Subsampled} {Fourier} {Ptychography} {Using} {Generative} {Priors}},
	shorttitle = {Deep {Ptych}},
	doi = {10.1109/ICASSP.2019.8682179},
	abstract = {This paper proposes a novel framework to regularize the highly ill-posed and non-linear Fourier ptychography problem using generative models. We demonstrate experimentally that our proposed algorithm, Deep Ptych, outperforms the existing Fourier ptychography techniques, in terms of quality of reconstruction and robustness against noise, using far fewer samples. We further modify the proposed approach to allow the generative model to explore solutions outside the range, leading to improved performance.},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Shamshad, Fahad and Abbas, Farwa and Ahmed, Ali},
	month = may,
	year = {2019},
	note = {ISSN: 1520-6149},
	keywords = {Cameras, Computational modeling, Deep Ptych algorithm, Fourier analysis, Fourier ptychography, Generators, Image reconstruction, Image resolution, Training, computational imaging technique, generative models, generative priors, image reconstruction, image resolution, image sampling, noise robustness, phase retrieval, reconstruction quality, resolution loss, subsampled Fourier ptychography, subsampling},
	pages = {7720--7724},
}

@article{kellman_physics-based_2019,
	title = {Physics-{Based} {Learned} {Design}: {Optimized} {Coded}-{Illumination} for {Quantitative} {Phase} {Imaging}},
	volume = {5},
	issn = {2573-0436},
	shorttitle = {Physics-{Based} {Learned} {Design}},
	doi = {10.1109/TCI.2019.2905434},
	abstract = {Coded illumination can enable quantitative phase microscopy of transparent samples with minimal hardware requirements. Intensity images are captured with different source patterns, then a nonlinear phase retrieval optimization reconstructs the image. The nonlinear nature of the processing makes optimizing the illumination pattern designs complicated. The traditional techniques for the experimental design (e.g., condition number optimization, and spectral analysis) consider only linear measurement formation models and linear reconstructions. Deep neural networks (DNNs) can efficiently represent the nonlinear process and can be optimized over via training in an end-to-end framework. However, DNNs typically require a large amount of training examples and parameters to properly learn the phase retrieval process, without making use of the known physical models. In this paper, we aim to use both our knowledge of the physics and the power of machine learning together. We propose a new data-driven approach for optimizing coded-illumination patterns for an LED array microscope for a given phase reconstruction algorithm. Our method incorporates both the physics of the measurement scheme and the nonlinearity of the reconstruction algorithm into the design problem. This enables efficient parameterization, which allows us to use only a small number of training examples to learn designs that generalize well in the experimental setting without retraining. We show experimental results for both a well-characterized phase target and mouse fibroblast cells, using coded-illumination patterns optimized for a sparsity-based phase reconstruction algorithm. Our learned design results using two measurements demonstrate similar accuracy to Fourier ptychography with 69 measurements.},
	number = {3},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Kellman, Michael R. and Bostan, Emrah and Repina, Nicole A. and Waller, Laura},
	month = sep,
	year = {2019},
	keywords = {DNN, LED array microscope, Light emitting diodes, Lighting, Microscopy, Phase imaging, Phase measurement, Physics, Training, biology computing, cellular biophysics, coded-illumination patterns, condition number optimization, data-driven approach, deep neural networks, end-to-end framework, experimental design, illumination design, illumination pattern designs, image capture, image coding, image reconstruction, intensity image capture, learning (artificial intelligence), light emitting diodes, linear measurement formation models, linear reconstructions, machine learning, measurement scheme, minimal hardware requirements, mouse fibroblast cells, neural nets, nonlinear phase retrieval optimization, nonlinear process, nonlinearity, optical microscopy, optimisation, optimized coded-illumination, parameterization, phase target, physics-based, physics-based learned design, quantitative phase imaging, quantitative phase microscopy, source patterns, sparsity-based phase reconstruction algorithm, spectral analysis, transparent samples, unrolled network},
	pages = {344--353},
}

@article{nguyen_deep_2018,
	title = {Deep learning approach for {Fourier} ptychography microscopy},
	volume = {26},
	copyright = {\&\#169; 2018 Optical Society of America},
	issn = {1094-4087},
	url = {https://www.osapublishing.org/oe/abstract.cfm?uri=oe-26-20-26470},
	doi = {10.1364/OE.26.026470},
	abstract = {Convolutional neural networks (CNNs) have gained tremendous success in solving complex inverse problems. The aim of this work is to develop a novel CNN framework to reconstruct video sequences of dynamic live cells captured using a computational microscopy technique, Fourier ptychographic microscopy (FPM). The unique feature of the FPM is its capability to reconstruct images with both wide field-of-view (FOV) and high resolution, i.e. a large space-bandwidth-product (SBP), by taking a series of low resolution intensity images. For live cell imaging, a single FPM frame contains thousands of cell samples with different morphological features. Our idea is to fully exploit the statistical information provided by these large spatial ensembles so as to make predictions in a sequential measurement, without using any additional temporal dataset. Specifically, we show that it is possible to reconstruct high-SBP dynamic cell videos by a CNN trained only on the first FPM dataset captured at the beginning of a time-series experiment. Our CNN approach reconstructs a 12800\&\#x000D7;10800 pixel phase image using only \&\#x0223C;25 seconds, a 50\&\#x000D7; speedup compared to the model-based FPM algorithm. In addition, the CNN further reduces the required number of images in each time frame by \&\#x0223C; 6\&\#x000D7;. Overall, this significantly improves the imaging throughput by reducing both the acquisition and computational times. The proposed CNN is based on the conditional generative adversarial network (cGAN) framework. We further propose a mixed loss function that combines the standard image domain loss and a weighted Fourier domain loss, which leads to improved reconstruction of the high frequency information. Additionally, we also exploit transfer learning so that our pre-trained CNN can be further optimized to image other cell types. Our technique demonstrates a promising deep learning approach to continuously monitor large live-cell populations over an extended time and gather useful spatial and temporal information with sub-cellular resolution.},
	language = {EN},
	number = {20},
	urldate = {2020-01-14},
	journal = {Optics Express},
	author = {Nguyen, Thanh and Xue, Yujia and Li, Yunzhe and Tian, Lei and Nehmetallah, George},
	month = oct,
	year = {2018},
	keywords = {Image processing, Image reconstruction, Imaging techniques, Medical imaging, Optical transfer functions, Phase noise},
	pages = {26470--26484},
}

@article{frankle_lottery_2019,
	title = {The {Lottery} {Ticket} {Hypothesis}: {Finding} {Sparse}, {Trainable} {Neural} {Networks}},
	shorttitle = {The {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/1803.03635},
	abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difﬁcult to train from the start, which would similarly improve training performance.},
	language = {en},
	urldate = {2020-01-14},
	journal = {arXiv:1803.03635 [cs]},
	author = {Frankle, Jonathan and Carbin, Michael},
	month = mar,
	year = {2019},
	note = {arXiv: 1803.03635},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{szegedy_intriguing_2014,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
	urldate = {2020-01-14},
	journal = {arXiv:1312.6199 [cs]},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	month = feb,
	year = {2014},
	note = {arXiv: 1312.6199},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{noauthor_youtube_nodate,
	title = {{YouTube}},
	url = {https://www.youtube.com/watch?v=fAd-qhKL8WA&list=PLyZk_jpQ4X_oyrmQh1HN4fmbcs5lq0KAy&index=14},
	urldate = {2020-01-05},
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {0021-9991},
	shorttitle = {Physics-informed neural networks},
	url = {http://www.sciencedirect.com/science/article/pii/S0021999118307125},
	doi = {10.1016/j.jcp.2018.10.045},
	abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
	language = {en},
	urldate = {2020-01-04},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
	month = feb,
	year = {2019},
	keywords = {Data-driven scientific computing, Machine learning, Nonlinear dynamics, Predictive modeling, Runge–Kutta methods},
	pages = {686--707},
}

@misc{nvidia_nvidia_nodate,
	title = {{NVIDIA} {Turing} {GPU} {Architecture}},
	url = {https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf},
	urldate = {2019-12-11},
	author = {Nvidia},
}

@article{young_recent_2018,
	title = {Recent {Trends} in {Deep} {Learning} {Based} {Natural} {Language} {Processing} [{Review} {Article}]},
	volume = {13},
	issn = {1556-6048},
	doi = {10.1109/MCI.2018.2840738},
	abstract = {Deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.},
	number = {3},
	journal = {IEEE Computational Intelligence Magazine},
	author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
	month = aug,
	year = {2018},
	keywords = {Computational modeling, Context modeling, Learning systems, Machine learning, NLP tasks, Natural language processing, Semantics, data structures, deep learning methods, deep learning related models, hierarchical data representations, learning (artificial intelligence), model designs, multiple processing layers, natural language processing},
	pages = {55--75},
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	abstract = {Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.},
	language = {en},
	number = {7676},
	urldate = {2019-12-11},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and Driessche, George van den and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	pages = {354--359},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {An artificial agent is developed that learns to play\&nbsp;a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a\&nbsp;performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	language = {en},
	number = {7540},
	urldate = {2019-12-11},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	pages = {529--533},
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	abstract = {A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence.},
	language = {en},
	number = {7587},
	urldate = {2019-12-11},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Driessche, George van den and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	pages = {484--489},
}

@book{hennessy_computer_2011,
	title = {Computer {Architecture}: {A} {Quantitative} {Approach}},
	isbn = {978-0-12-383873-5},
	shorttitle = {Computer {Architecture}},
	abstract = {Computer Architecture: A Quantitative Approach, Fifth Edition, explores the ways that software and technology in the cloud are accessed by digital media, such as cell phones, computers, tablets, and other mobile devices. The book, which became a part of Intel's 2012 recommended reading list for developers, covers the revolution of mobile computing. It also highlights the two most important factors in architecture today: parallelism and memory hierarchy. This fully updated edition is comprised of six chapters that follow a consistent framework: explanation of the ideas in each chapter; a crosscutting issues section, which presents how the concepts covered in one chapter connect with those given in other chapters; a putting it all together section that links these concepts by discussing how they are applied in real machine; and detailed examples of misunderstandings and architectural traps commonly encountered by developers and architects. Formulas for energy, static and dynamic power, integrated circuit costs, reliability, and availability are included. The book also covers virtual machines, SRAM and DRAM technologies, and new material on Flash memory. Other topics include the exploitation of instruction-level parallelism in high-performance processors, superscalar execution, dynamic scheduling and multithreading, vector architectures, multicore processors, and warehouse-scale computers (WSCs). There are updated case studies and completely new exercises. Additional reference appendices are available online. This book will be a valuable reference for computer architects, programmers, application developers, compiler and system software developers, computer system designers and application developers. Part of Intel's 2012 Recommended Reading List for DevelopersUpdated to cover the mobile computing revolutionEmphasizes the two most important topics in architecture today: memory hierarchy and parallelism in all its forms.Develops common themes throughout each chapter: power, performance, cost, dependability, protection, programming models, and emerging trends ("What's Next")Includes three review appendices in the printed text. Additional reference appendices are available online.Includes updated Case Studies and completely new exercises.},
	language = {en},
	publisher = {Elsevier},
	author = {Hennessy, John L. and Patterson, David A.},
	month = oct,
	year = {2011},
	keywords = {Computers / Systems Architecture / General},
}

@book{euler_institutionum_1768,
	series = {Institutionum calculi integralis},
	title = {Institutionum calculi integralis},
	url = {https://books.google.no/books?id=Vg8OAAAAQAAJ},
	number = {v. 1},
	publisher = {imp. Acad. imp. Saènt.},
	author = {Euler, L.},
	year = {1768},
}

@book{suli_introduction_2003,
	address = {Cambridge ; New York},
	title = {An introduction to numerical analysis},
	isbn = {978-0-521-81026-5 978-0-521-00794-8},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Süli, Endre and Mayers, D. F.},
	year = {2003},
	note = {OCLC: ocm50525488},
	keywords = {Numerical analysis},
}

@book{butcher_numerical_2008,
	address = {Chichester},
	edition = {2nd ed},
	title = {Numerical {Methods} for {Ordinary} {Differential} {Equations}},
	isbn = {978-0-470-72335-7 978-0-470-75375-0},
	language = {eng},
	publisher = {Wiley},
	author = {Butcher, John},
	year = {2008},
	keywords = {Differential equations, Mathematical Physics and Mathematics},
}

@book{butcher_numerical_2016,
	title = {Numerical {Methods} for {Ordinary} {Differential} {Equations}},
	isbn = {978-1-119-12150-3},
	abstract = {A new edition of this classic work, comprehensively revised to present exciting new developments in this important subject The study of numerical methods for solving ordinary differential equations is constantly developing and regenerating, and this third edition of a popular classic volume, written by one of the world’s leading experts in the field, presents an account of the subject which reflects both its historical and well-established place in computational science and its vital role as a cornerstone of modern applied mathematics. In addition to serving as a broad and comprehensive study of numerical methods for initial value problems, this book contains a special emphasis on Runge-Kutta methods by the mathematician who transformed the subject into its modern form dating from his classic 1963 and 1972 papers. A second feature is general linear methods which have now matured and grown from being a framework for a unified theory of a wide range of diverse numerical schemes to a source of new and practical algorithms in their own right. As the founder of general linear method research, John Butcher has been a leading contributor to its development; his special role is reflected in the text. The book is written in the lucid style characteristic of the author, and combines enlightening explanations with rigorous and precise analysis. In addition to these anticipated features, the book breaks new ground by including the latest results on the highly efficient G-symplectic methods which compete strongly with the well-known symplectic Runge-Kutta methods for long-term integration of conservative mechanical systems. This third edition of Numerical Methods for Ordinary Differential Equations will serve as a key text for senior undergraduate and graduate courses in numerical analysis, and is an essential resource for research workers in applied mathematics, physics and engineering.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Butcher, J. C.},
	month = aug,
	year = {2016},
	note = {Google-Books-ID: JlSvDAAAQBAJ},
	keywords = {Mathematics / General, Mathematics / Mathematical Analysis},
}

@article{lotka_contribution_1910,
	title = {Contribution to the {Theory} of {Periodic} {Reactions}},
	volume = {14},
	issn = {0092-7325, 1541-5740},
	url = {https://pubs.acs.org/doi/abs/10.1021/j150111a004},
	doi = {10.1021/j150111a004},
	language = {en},
	number = {3},
	urldate = {2019-12-09},
	journal = {The Journal of Physical Chemistry},
	author = {Lotka, Alfred J.},
	month = mar,
	year = {1910},
	pages = {271--274},
}

@article{pol_frequency_1927,
	title = {Frequency {Demultiplication}},
	volume = {120},
	copyright = {1927 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/120363a0},
	doi = {10.1038/120363a0},
	abstract = {IT is a well-known fact that when a sinusoidal E.M.F. (of the form E0 sin wt) is available, it is a relatively simple matter to design an electrical system such that alternating currents or potential differences will occur in the system, having a frequency which is a whole multiple of the applied E.M.F., e.g. 2w, 3w, etc. For example, when the E.M.F. E0, sin wt is applied to a diode-rectifier, the current in the anode circuit will include a component of double frequency, i.e. 2w. This is therefore one method of frequency multiplication. Several other methods could easily be mentioned.},
	language = {en},
	number = {3019},
	urldate = {2019-12-09},
	journal = {Nature},
	author = {Pol, Balth Van Der and Mark, J. Van Der},
	month = sep,
	year = {1927},
	pages = {363--364},
}

@inproceedings{paszke_pytorch:_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks. 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.},
	booktitle = {{NeurIPS} 2019},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas Dipl-Ing and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	note = {arXiv: 1912.01703},
	keywords = {Deep learning, Imperative programming},
}

@inproceedings{chen_neural_2018,
	address = {USA},
	series = {{NIPS}'18},
	title = {Neural {Ordinary} {Differential} {Equations}},
	url = {http://dl.acm.org/citation.cfm?id=3327757.3327764},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the {32Nd} {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	year = {2018},
	note = {event-place: Montréal, Canada},
	pages = {6572--6583},
}

@book{colella_defining_2004,
	title = {Defining software requirements for scientific computing},
	publisher = {presentation},
	author = {Colella, Phillip},
	year = {2004},
}

@article{asanovic_landscape_nodate,
	title = {The {Landscape} of {Parallel} {Computing} {Research}: {A} {View} from {Berkeley}},
	abstract = {The recent switch to parallel microprocessors is a milestone in the history of computing. Industry has laid out a roadmap for multicore designs that preserves the programming paradigm of the past via binary compatibility and cache coherence. Conventional wisdom is now to double the number of cores on a chip with each silicon generation.},
	language = {en},
	author = {Asanovíc, Krste and Bodik, Rastislav and Catanzaro, Bryan and Gebis, Joseph and Husbands, Parry and Keutzer, Kurt and Patterson, David and Plishker, William and Shalf, John and Williams, Samuel and Yelick, Katherine},
	pages = {54},
}

@article{rexroth_high_2019,
	title = {High {Performance} {Computing} for gravitational lens modeling: single vs double precision on {GPUs} and {CPUs}},
	shorttitle = {High {Performance} {Computing} for gravitational lens modeling},
	url = {http://arxiv.org/abs/1902.03252},
	abstract = {Strong gravitational lensing is a powerful probe of cosmology and the dark matter distribution. Efficient lensing software is already a necessity to fully use its potential and the performance demands will only increase with the upcoming generation of telescopes. In this paper, we study the possible impact of High Performance Computing techniques on a performance-critical part of the widely used lens modeling software LENSTOOL. We implement the algorithm once as a highly optimized CPU version and once with graphics card acceleration for a simple parametric lens model. In addition, we study the impact of finite machine precision on the lensing algorithm. While double precision is the default choice for scientific applications, we find that single precision can be sufficiently accurate for our purposes and lead to a big speedup. Therefore we develop and present a mixed precision algorithm which only uses double precision when necessary. We measure the performance of the different implementations and find that the use of High Performance Computing Techniques dramatically improves the code performance both on CPUs and GPUs. Compared to the current LENSTOOL implementation on 12 CPU cores, we obtain speedup factors of up to 170. We achieve this optimal performance by using our mixed precision algorithm on a high-end GPU which is common in modern supercomputers. We also show that these techniques reduce the energy consumption by up to 98\%. Furthermore, we demonstrate that a highly competitive speedup can be reached with consumer GPUs. While they are an order of magnitude cheaper than the high-end graphics cards, they are rarely used for scientific computations due to their low double precision performance. Our mixed precision algorithm unlocks their full potential. The consumer GPU delivers a speedup which is only a factor of four lower than the best speedup achieved by a high-end GPU.},
	urldate = {2019-12-03},
	journal = {arXiv:1902.03252 [astro-ph]},
	author = {Rexroth, Markus and Schäfer, Christoph and Fourestey, Gilles and Kneib, Jean-Paul},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.03252},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
}

@article{milroy_investigating_nodate,
	title = {Investigating the {Impact} of {Mixed} {Precision} on {Correctness} for a {Large} {Climate} {Code}},
	abstract = {Earth system models (ESMs) are computationally expensive and represent many complex processes on a wide range of scales from molecular to global. Certain ESM computations require high precision while others, such as atmospheric microphysics (e.g., precipitation) which are approximated by bulk properties, should not. As such, atmospheric microphysics models are prime candidates for conversion to single precision, which afford distinct computational and memory advantages over typical double-precision numbers. However, care must be taken as indiscriminate type casting to single precision can result in numerical instability and divergent output when applied naively. In this work we relate our experiences attempting to improve the performance of the Morrison-Gettelman microphysics package (MG2) in a popular ESM by modifying it to compute in single precision without sacriﬁcing correctness. We ﬁnd that modiﬁcation of the entire MG2 package to compute with singleprecision ﬂoats achieves a respectable performance increase but does not appear to be correct in terms of maintaining consistency with double-precision MG2. On the other hand, narrowing the scope of our conversion to a couple expensive subprograms yields more satisfying results in terms of correctness but with negligible overall performance improvement. We evaluate correctness with both an objective statistical tool and traditional approaches more familiar to climate scientists. While we are still working toward our ultimate goal of improving the performance of MG2 without negatively affecting model output, we believe that our experiences may be helpful to other groups pursuing similar goals.},
	language = {en},
	author = {Milroy, Daniel J and Baker, Allison H and Dennis, John M and Gettelman, Andrew and Hammerling, Dorit M},
	pages = {10},
}

@article{le_grand_spfp:_2013,
	title = {{SPFP}: {Speed} without compromise—{A} mixed precision model for {GPU} accelerated molecular dynamics simulations},
	volume = {184},
	issn = {0010-4655},
	shorttitle = {{SPFP}},
	url = {http://www.sciencedirect.com/science/article/pii/S0010465512003098},
	doi = {10.1016/j.cpc.2012.09.022},
	abstract = {A new precision model is proposed for the acceleration of all-atom classical molecular dynamics (MD) simulations on graphics processing units (GPUs). This precision model replaces double precision arithmetic with fixed point integer arithmetic for the accumulation of force components as compared to a previously introduced model that uses mixed single/double precision arithmetic. This significantly boosts performance on modern GPU hardware without sacrificing numerical accuracy. We present an implementation for NVIDIA GPUs of both generalized Born implicit solvent simulations as well as explicit solvent simulations using the particle mesh Ewald (PME) algorithm for long-range electrostatics using this precision model. Tests demonstrate both the performance of this implementation as well as its numerical stability for constant energy and constant temperature biomolecular MD as compared to a double precision CPU implementation and double and mixed single/double precision GPU implementations.},
	language = {en},
	number = {2},
	urldate = {2019-12-03},
	journal = {Computer Physics Communications},
	author = {Le Grand, Scott and Götz, Andreas W. and Walker, Ross C.},
	month = feb,
	year = {2013},
	keywords = {Accelerator, DPDP, Graphic processing unit, Molecular dynamics, Precision model, SPDP, SPFP, SPSP},
	pages = {374--380},
}

@inproceedings{lam_automatically_2013,
	address = {New York, NY, USA},
	series = {{ICS} '13},
	title = {Automatically {Adapting} {Programs} for {Mixed}-precision {Floating}-point {Computation}},
	isbn = {978-1-4503-2130-3},
	url = {http://doi.acm.org/10.1145/2464996.2465018},
	doi = {10.1145/2464996.2465018},
	abstract = {As scientific computation continues to scale, efficient use of floating-point arithmetic processors is critical. Lower precision allows streaming architectures to perform more operations per second and can reduce memory bandwidth pressure on all architectures. However, using a precision that is too low for a given algorithm and data set leads to inaccurate results. In this paper, we present a framework that uses binary instrumentation and modification to build mixed-precision configurations of existing binaries that were originally developed to use only double-precision. This framework allows developers to explore mixed-precision configurations without modifying their source code, and it permits autotuning of floating-point precision. We include a simple search algorithm to automate identification of code regions that can use lower precision. Our results for several benchmarks show that our framework is effective and incurs low overhead (less than 10X in most cases). In addition, we demonstrate that our tool can replicate manual conversions and suggest further optimization; in one case, we achieve a speedup of 2X.},
	urldate = {2019-12-03},
	booktitle = {Proceedings of the 27th {International} {ACM} {Conference} on {International} {Conference} on {Supercomputing}},
	publisher = {ACM},
	author = {Lam, Michael O. and Hollingsworth, Jeffrey K. and de Supinski, Bronis R. and Legendre, Matthew P.},
	year = {2013},
	note = {event-place: Eugene, Oregon, USA},
	keywords = {binary instrumentation, floating-point, mixed precision},
	pages = {369--378},
}

@article{owens_gpu_2008,
	title = {{GPU} {Computing}},
	volume = {96},
	issn = {0018-9219 1558-2256},
	url = {https://escholarship.org/uc/item/0cv1p1nc},
	doi = {10.1109/JPROC.2008.917757},
	abstract = {The graphics processing unit (GPU) has become an integral part oftoday's mainstream computing systems. Over the past six years, therehas been a marked increase in the performance and capabilities ofGPUs. The modern GPU is not only a powerful graphics engine but also ahighly-parallel programmable processor featuring peak arithmetic andmemory bandwidth that substantially outpaces its CPU counterpart. TheGPU's rapid increase in both programmability and capability hasspawned a research community that has successfully mapped a broadrange of computationally demanding, complex problems to the GPU. Thiseffort in general-purpose computing on the GPU (GPGPU), alsoknown as GPU computing, has positioned the GPU as a compellingalternative to traditional microprocessors in high-performancecomputer systems of the future. We describe the background, hardware,and programming model for GPU computing, summarize the state of theart in tools and techniques, and present four GPU computing successesin game physics and computational biophysics that deliverorder-of-magnitude performance gains over optimized CPU applications.},
	language = {en},
	number = {5},
	urldate = {2019-12-03},
	journal = {Proceedings of the IEEE},
	author = {Owens, John D. and Houston, Mike and Luebke, David and Green, Simon and Stone, John E. and Phillips, James C.},
	month = may,
	year = {2008},
	pages = {879--899},
}

@article{smistad_medical_2015,
	title = {Medical image segmentation on {GPUs} – {A} comprehensive review},
	volume = {20},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841514001819},
	doi = {10.1016/j.media.2014.10.012},
	abstract = {Segmentation of anatomical structures, from modalities like computed tomography (CT), magnetic resonance imaging (MRI) and ultrasound, is a key enabling technology for medical applications such as diagnostics, planning and guidance. More efficient implementations are necessary, as most segmentation methods are computationally expensive, and the amount of medical imaging data is growing. The increased programmability of graphic processing units (GPUs) in recent years have enabled their use in several areas. GPUs can solve large data parallel problems at a higher speed than the traditional CPU, while being more affordable and energy efficient than distributed systems. Furthermore, using a GPU enables concurrent visualization and interactive segmentation, where the user can help the algorithm to achieve a satisfactory result. This review investigates the use of GPUs to accelerate medical image segmentation methods. A set of criteria for efficient use of GPUs are defined and each segmentation method is rated accordingly. In addition, references to relevant GPU implementations and insight into GPU optimization are provided and discussed. The review concludes that most segmentation methods may benefit from GPU processing due to the methods’ data parallel structure and high thread count. However, factors such as synchronization, branch divergence and memory usage can limit the speedup.},
	language = {en},
	number = {1},
	urldate = {2019-12-02},
	journal = {Medical Image Analysis},
	author = {Smistad, Erik and Falch, Thomas L. and Bozorgi, Mohammadmehdi and Elster, Anne C. and Lindseth, Frank},
	month = feb,
	year = {2015},
	keywords = {GPU, Image, Medical, Parallel, Segmentation},
	pages = {1--18},
}

@article{de_supinski_ongoing_2018,
	title = {The {Ongoing} {Evolution} of {OpenMP}},
	volume = {106},
	issn = {1558-2256},
	doi = {10.1109/JPROC.2018.2853600},
	abstract = {This paper presents an overview of the past, present and future of the OpenMP application programming interface (API). While the API originally specified a small set of directives that guided shared memory fork-join parallelization of loops and program sections, OpenMP now provides a richer set of directives that capture a wide range of parallelization strategies that are not strictly limited to shared memory. As we look toward the future of OpenMP, we immediately see further evolution of the support for that range of parallelization strategies and the addition of direct support for debugging and performance analysis tools. Looking beyond the next major release of the specification of the OpenMP API, we expect the specification eventually to include support for more parallelization strategies and to embrace closer integration into its Fortran, C and in particular, C++ base languages, which will likely require the API to adopt additional programming abstractions.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {de Supinski, Bronis R. and Scogland, Thomas R. W. and Duran, Alejandro and Klemm, Michael and Bellido, Sergi Mateo and Olivier, Stephen L. and Terboven, Christian and Mattson, Timothy G.},
	month = nov,
	year = {2018},
	keywords = {Accelerator architectures, C language, C++ base languages, C++ languages, Complexity theory, Fortran, High performance computing, Information processing, Memory management, OpenMP API, OpenMP application programming interface, Parallel processing, Parallel programming, Program processors, application program interfaces, computer architecture, computer science, computers and information processing, debugging, memory management, multicore processing, multithreading, parallel architectures, parallel processing, parallel programming, parallelization strategies, performance analysis tools, program debugging, programming, programming abstractions, shared memory, shared memory systems},
	pages = {2004--2019},
}

@article{li_evaluating_2019,
	title = {Evaluating {Modern} {GPU} {Interconnect}: {PCIe}, {NVLink}, {NV}-{SLI}, {NVSwitch} and {GPUDirect}},
	issn = {1045-9219, 1558-2183, 2161-9883},
	shorttitle = {Evaluating {Modern} {GPU} {Interconnect}},
	url = {http://arxiv.org/abs/1903.04611},
	doi = {10.1109/TPDS.2019.2928289},
	abstract = {High performance multi-GPU computing becomes an inevitable trend due to the ever-increasing demand on computation capability in emerging domains such as deep learning, big data and planet-scale simulations. However, the lack of deep understanding on how modern GPUs can be connected and the real impact of state-of-the-art interconnect technology on multi-GPU application performance become a hurdle. In this paper, we ﬁll the gap by conducting a thorough evaluation on ﬁve latest types of modern GPU interconnects: PCIe, NVLink-V1, NVLink-V2, NVLink-SLI and NVSwitch, from six high-end servers and HPC platforms: NVIDIA P100-DGX-1, V100-DGX-1, DGX-2, OLCF’s SummitDev and Summit supercomputers, as well as an SLI-linked system with two NVIDIA Turing RTX-2080 GPUs. Based on the empirical evaluation, we have observed four new types of GPU communication network NUMA effects: three are triggered by NVLink’s topology, connectivity and routing, while one is caused by PCIe chipset design issue. These observations indicate that, for an application running in a multi-GPU node, choosing the right GPU combination can impose considerable impact on GPU communication efﬁciency, as well as the application’s overall performance. Our evaluation can be leveraged in building practical multi-GPU performance models, which are vital for GPU task allocation, scheduling and migration in a shared environment (e.g., AI cloud and HPC centers), as well as communication-oriented performance tuning.},
	language = {en},
	urldate = {2019-12-02},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Li, Ang and Song, Shuaiwen Leon and Chen, Jieyang and Li, Jiajia and Liu, Xu and Tallent, Nathan and Barker, Kevin},
	year = {2019},
	note = {arXiv: 1903.04611},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture, Computer Science - Networking and Internet Architecture, Computer Science - Performance},
	pages = {1--1},
}

@article{dongarra_high-performance_2016,
	title = {High-performance conjugate-gradient benchmark: {A} new metric for ranking high-performance computing systems},
	volume = {30},
	issn = {1094-3420, 1741-2846},
	shorttitle = {High-performance conjugate-gradient benchmark},
	url = {http://journals.sagepub.com/doi/10.1177/1094342015593158},
	doi = {10.1177/1094342015593158},
	abstract = {We describe a new high-performance conjugate-gradient (HPCG) benchmark. HPCG is composed of computations and data-access patterns commonly found in scientific applications. HPCG strives for a better correlation to existing codes from the computational science domain and to be representative of their performance. HPCG is meant to help drive the computer system design and implementation in directions that will better impact future performance improvement.},
	language = {en},
	number = {1},
	urldate = {2019-12-02},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Dongarra, Jack and Heroux, Michael A and Luszczek, Piotr},
	month = feb,
	year = {2016},
	pages = {3--10},
}

@inproceedings{kjolstad_taco:_2017,
	address = {Urbana, IL},
	title = {Taco: {A} tool to generate tensor algebra kernels},
	isbn = {978-1-5386-2684-9},
	shorttitle = {Taco},
	url = {http://ieeexplore.ieee.org/document/8115709/},
	doi = {10.1109/ASE.2017.8115709},
	abstract = {Tensor algebra is an important computational abstraction that is increasingly used in data analytics, machine learning, engineering, and the physical sciences. However, the number of tensor expressions is unbounded, which makes it hard to develop and optimize libraries. Furthermore, the tensors are often sparse (most components are zero), which means the code has to traverse compressed formats. To support programmers we have developed taco, a code generation tool that generates dense, sparse, and mixed kernels from tensor algebra expressions. This paper describes the taco web and command-line tools and discusses the beneﬁts of a code generator over a traditional library approach.},
	language = {en},
	urldate = {2019-12-02},
	booktitle = {2017 32nd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Kjolstad, Fredrik and Chou, Stephen and Lugato, David and Kamil, Shoaib and Amarasinghe, Saman},
	month = oct,
	year = {2017},
	pages = {943--948},
}

@inproceedings{li_numa-aware_2013,
	address = {New York, New York, USA},
	title = {{NUMA}-aware shared-memory collective communication for {MPI}},
	isbn = {978-1-4503-1910-2},
	url = {http://dl.acm.org/citation.cfm?doid=2493123.2462903},
	doi = {10.1145/2493123.2462903},
	abstract = {As the number of cores per node keeps increasing, it becomes increasingly important for MPI to leverage shared memory for intranode communication. This paper investigates the design and optimizations of MPI collectives for clusters of NUMA nodes. We develop performance models for collective communication using shared memory, and we develop several algorithms for various collectives. Experiments are conducted on both Xeon X5650 and Opteron 6100 InﬁniBand clusters. The measurements agree with the model and indicate that diﬀerent algorithms dominate for short vectors and long vectors. We compare our sharedmemory allreduce with several traditional MPI implementations – Open MPI, MPICH2, and MVAPICH2 – that utilize system shared memory to facilitate interprocess communication. On a 16-node Xeon cluster and 8-node Opteron cluster, our implementation achieves on average 2.5X and 2.3X speedup over MVAPICH2, respectively. Our techniques enable an eﬃcient implementation of collective operations on future multi- and manycore systems.},
	language = {en},
	urldate = {2019-12-02},
	booktitle = {Proceedings of the 22nd international symposium on {High}-performance parallel and distributed computing - {HPDC} '13},
	publisher = {ACM Press},
	author = {Li, Shigang and Hoefler, Torsten and Snir, Marc},
	year = {2013},
	pages = {85},
}

@inproceedings{feng_opencl_2012,
	address = {Boston, Massachusetts, USA},
	title = {{OpenCL} and the 13 dwarfs: a work in progress},
	isbn = {978-1-4503-1202-8},
	shorttitle = {{OpenCL} and the 13 dwarfs},
	url = {http://dl.acm.org/citation.cfm?doid=2188286.2188341},
	doi = {10.1145/2188286.2188341},
	abstract = {In the past, evaluating the architectural innovation of parallel computing devices relied on a benchmark suite based on existing programs, e.g., EEMBC or SPEC. However, with the growing ubiquity of parallel computing devices, we argue that it is unclear how best to express parallel computation, and hence, a need exists to identify a higher level of abstraction for reasoning about parallel application requirements. Therefore, the goal of this combination “Work-in-Progress and Vision” paper is to delineate application requirements in a manner that is not overly speciﬁc to individual applications or the optimizations used for certain hardware platforms, so that we can draw broader conclusions about hardware requirements. Our initial eﬀort, dubbed “OpenCL and the 13 Dwarfs” or OCD for short, realizes Berkeley’s 13 computational dwarfs of scientiﬁc computing in OpenCL, where each dwarf captures a pattern of computation and communication that is common to a class of important applications.},
	language = {en},
	urldate = {2019-12-02},
	booktitle = {Proceedings of the third joint {WOSP}/{SIPEW} international conference on {Performance} {Engineering} - {ICPE} '12},
	publisher = {ACM Press},
	author = {Feng, Wu-chun and Lin, Heshan and Scogland, Thomas and Zhang, Jing},
	year = {2012},
	pages = {291},
}

@article{baboulin_accelerating_2009,
	series = {40 {YEARS} {OF} {CPC}: {A} celebratory issue focused on quality software for high performance, grid and novel computing architectures},
	title = {Accelerating scientific computations with mixed precision algorithms},
	volume = {180},
	issn = {0010-4655},
	url = {http://www.sciencedirect.com/science/article/pii/S0010465508003846},
	doi = {10.1016/j.cpc.2008.11.005},
	abstract = {On modern architectures, the performance of 32-bit operations is often at least twice as fast as the performance of 64-bit operations. By using a combination of 32-bit and 64-bit floating point arithmetic, the performance of many dense and sparse linear algebra algorithms can be significantly enhanced while maintaining the 64-bit accuracy of the resulting solution. The approach presented here can apply not only to conventional processors but also to other technologies such as Field Programmable Gate Arrays (FPGA), Graphical Processing Units (GPU), and the STI Cell BE processor. Results on modern processor architectures and the STI Cell BE are presented.
Program summary
Program title: ITER-REF Catalogue identifier: AECO\_v1\_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AECO\_v1\_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 7211 No. of bytes in distributed program, including test data, etc.: 41 862 Distribution format: tar.gz Programming language: FORTRAN 77 Computer: desktop, server Operating system: Unix/Linux RAM: 512 Mbytes Classification: 4.8 External routines: BLAS (optional) Nature of problem: On modern architectures, the performance of 32-bit operations is often at least twice as fast as the performance of 64-bit operations. By using a combination of 32-bit and 64-bit floating point arithmetic, the performance of many dense and sparse linear algebra algorithms can be significantly enhanced while maintaining the 64-bit accuracy of the resulting solution. Solution method: Mixed precision algorithms stem from the observation that, in many cases, a single precision solution of a problem can be refined to the point where double precision accuracy is achieved. A common approach to the solution of linear systems, either dense or sparse, is to perform the LU factorization of the coefficient matrix using Gaussian elimination. First, the coefficient matrix A is factored into the product of a lower triangular matrix L and an upper triangular matrix U. Partial row pivoting is in general used to improve numerical stability resulting in a factorization PA=LU, where P is a permutation matrix. The solution for the system is achieved by first solving Ly=Pb (forward substitution) and then solving Ux=y (backward substitution). Due to round-off errors, the computed solution, x, carries a numerical error magnified by the condition number of the coefficient matrix A. In order to improve the computed solution, an iterative process can be applied, which produces a correction to the computed solution at each iteration, which then yields the method that is commonly known as the iterative refinement algorithm. Provided that the system is not too ill-conditioned, the algorithm produces a solution correct to the working precision. Running time: seconds/minutes},
	language = {en},
	number = {12},
	urldate = {2019-12-02},
	journal = {Computer Physics Communications},
	author = {Baboulin, Marc and Buttari, Alfredo and Dongarra, Jack and Kurzak, Jakub and Langou, Julie and Langou, Julien and Luszczek, Piotr and Tomov, Stanimire},
	month = dec,
	year = {2009},
	keywords = {Iterative refinement, Mixed precision, Numerical linear algebra},
	pages = {2526--2533},
}

@article{gallo_mixed-precision_2018,
	title = {Mixed-precision in-memory computing},
	volume = {1},
	copyright = {2018 The Author(s)},
	issn = {2520-1131},
	url = {https://www.nature.com/articles/s41928-018-0054-8},
	doi = {10.1038/s41928-018-0054-8},
	abstract = {A hybrid system that combines a von Neumann machine with a computational memory unit can offer both the high precision of digital computing and the energy/areal efficiency of in-memory computing, which is illustrated by accurately solving a system of 5,000 equations using 998,752 phase-change memory devices.},
	language = {en},
	number = {4},
	urldate = {2019-12-02},
	journal = {Nature Electronics},
	author = {Gallo, Manuel Le and Sebastian, Abu and Mathis, Roland and Manica, Matteo and Giefers, Heiner and Tuma, Tomas and Bekas, Costas and Curioni, Alessandro and Eleftheriou, Evangelos},
	month = apr,
	year = {2018},
	pages = {246--253},
}

@inproceedings{ahmed_scalable_2016,
	title = {Scalable {Interconnection} {Network} {Models} for {Rapid} {Performance} {Prediction} of {HPC} {Applications}},
	doi = {10.1109/HPCC-SmartCity-DSS.2016.0151},
	abstract = {Performance Prediction Toolkit (PPT) is a simulator mainly developed at Los Alamos National Laboratory to facilitate rapid and accurate performance prediction of large-scale scientific applications on existing and future HPC architectures. In this paper, we present three interconnect models for performance prediction of large-scale HPC applications. They are based on interconnect topologies widely used in HPC systems: torus, dragonfly, and fat-tree. We conduct extensive validation tests of our interconnect models, in particular, using configurations of existing HPC systems. Results show that our models provide good accuracy for predicting the network behavior. We also present a performance study of a parallel computational physics application to show that our model can accurately predict the parallel behavior of large-scale applications.},
	booktitle = {2016 {IEEE} 18th {International} {Conference} on {High} {Performance} {Computing} and {Communications}; {IEEE} 14th {International} {Conference} on {Smart} {City}; {IEEE} 2nd {International} {Conference} on {Data} {Science} and {Systems} ({HPCC}/{SmartCity}/{DSS})},
	author = {Ahmed, Kishwar and Liu, Jason and Eidenbenz, Stephan and Zerr, Joe},
	month = dec,
	year = {2016},
	note = {ISSN: null},
	keywords = {Computational modeling, Data models, HPC applications, HPC architectures, HPC system configurations, Hardware, High-performance computing, Interconnection network, Modeling and simulation, Multiprocessor interconnection, Network topology, PPT, Performance evaluation, Predictive models, dragonfly HPC systems, fat-tree HPC systems, high performance computing, interconnect models, interconnect topologies, large-scale scientific applications, multiprocessor interconnection networks, network behavior prediction, parallel computational physics, parallel processing, performance prediction toolkit, scalable interconnection network models, scientific information systems, torus HPC systems},
	pages = {1069--1078},
}

@article{kjolstad_tensor_2017,
	title = {The {Tensor} {Algebra} {Compiler}},
	volume = {1},
	issn = {2475-1421},
	url = {http://doi.acm.org/10.1145/3133901},
	doi = {10.1145/3133901},
	abstract = {Tensor algebra is a powerful tool with applications in machine learning, data analytics, engineering and the physical sciences. Tensors are often sparse and compound operations must frequently be computed in a single kernel for performance and to save memory. Programmers are left to write kernels for every operation of interest, with different mixes of dense and sparse tensors in different formats. The combinations are infinite, which makes it impossible to manually implement and optimize them all. This paper introduces the first compiler technique to automatically generate kernels for any compound tensor algebra operation on dense and sparse tensors. The technique is implemented in a C++ library called taco. Its performance is competitive with best-in-class hand-optimized kernels in popular libraries, while supporting far more tensor operations.},
	number = {OOPSLA},
	urldate = {2019-11-27},
	journal = {Proc. ACM Program. Lang.},
	author = {Kjolstad, Fredrik and Kamil, Shoaib and Chou, Stephen and Lugato, David and Amarasinghe, Saman},
	month = oct,
	year = {2017},
	keywords = {code generation, iteration graphs, linear algebra, merge lattices, parallelism, performance, sparse data structures, tensor algebra, tensors},
	pages = {77:1--77:29},
}

@inproceedings{markidis_nvidia_2018,
	title = {{NVIDIA} {Tensor} {Core} {Programmability}, {Performance} {Precision}},
	doi = {10.1109/IPDPSW.2018.00091},
	abstract = {The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called Tensor Core that performs one matrix-multiply-and-accumulate on 4x4 matrices per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta microarchitecture, provides 640 Tensor Cores with a theoretical peak performance of 125 Tflops/s in mixed precision. In this paper, we investigate current approaches to program NVIDIA Tensor Cores, their performances and the precision loss due to computation in mixed precision. Currently, NVIDIA provides three different ways of programming matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS GEMM. After experimenting with different approaches, we found that NVIDIA Tensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100 GPU, seven and three times the performance in single and half precision respectively. A WMMA implementation of batched GEMM reaches a performance of 4 Tflops/s. While precision loss due to matrix multiplication with half precision input might be critical in many HPC applications, it can be considerably reduced at the cost of increased computation. Our results indicate that HPC applications using matrix multiplications can strongly benefit from using of NVIDIA Tensor Cores.},
	booktitle = {2018 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	author = {Markidis, Stefano and Chien, Steven Wei Der and Laure, Erwin and Peng, Ivy Bo and Vetter, Jeffrey S.},
	month = may,
	year = {2018},
	note = {ISSN: null},
	keywords = {CUDA WMMA API, CUDA Warp Matrix Multiply Accumulate API, CUTLASS, Computer architecture, GEMM, GPU Programming, Graphics processing units, HPC applications, Hardware, Instruction sets, Mixed Precision, NVIDIA Tensor Core programmability, NVIDIA Tensor Cores, NVIDIA Tesla V100 accelerator, NVIDIA Volta GPU microarchitecture, Neural networks, Programming, Tensile stress, Tesla V100 GPU, Volta microarchitecture, application program interfaces, cuBLAS GEMM, graphics processing units, matrix multiplication, parallel architectures, parallel programming, programming matrix-multiply- accumulate, tensors},
	pages = {522--531},
}

@book{macdonald_international_2014,
	address = {Heidelberg ; New York ; Dordrecht ; London : Chichester, UK},
	title = {The international handbook of space technology},
	isbn = {978-3-642-41100-7},
	language = {en},
	publisher = {Springer ; Published in association with Praxis Publishing},
	editor = {Macdonald, Malcolm and Badescu, Viorel},
	year = {2014},
	keywords = {Astronautics, Handbooks, manuals, etc},
}

@article{higham_global_1991,
	title = {Global {Error} versus {Tolerance} for {Explicit} {Runge}-{Kutta} {Methods}},
	volume = {11},
	issn = {0272-4979, 1464-3642},
	url = {https://academic.oup.com/imajna/article-lookup/doi/10.1093/imanum/11.4.457},
	doi = {10.1093/imanum/11.4.457},
	language = {en},
	number = {4},
	urldate = {2019-11-18},
	journal = {IMA Journal of Numerical Analysis},
	author = {Higham, Desmond J.},
	year = {1991},
	pages = {457--480},
}

@article{shampine_practical_1986,
	title = {Some practical {Runge}-{Kutta} formulas},
	volume = {46},
	issn = {0025-5718, 1088-6842},
	url = {https://www.ams.org/mcom/1986-46-173/S0025-5718-1986-0815836-3/},
	doi = {10.1090/S0025-5718-1986-0815836-3},
	abstract = {A new selection is made of the most practical of the many explicit Runge-Kutta formulas of order 4 which have been proposed. A new formula is considered, formulas are modified to improve their quality and efficiency in agreement with improved understanding of the issues, and formulas are derived which permit interpolation. It is possible to do a lot better than the pair of Fehlberg currently regarded as "best".},
	language = {en},
	number = {173},
	urldate = {2019-11-18},
	journal = {Mathematics of Computation},
	author = {Shampine, Lawrence F.},
	year = {1986},
	pages = {135--150},
}

@article{enright_comparing_1975,
	title = {Comparing numerical methods for stiff systems of {O}.{D}.{E}:s},
	volume = {15},
	issn = {1572-9125},
	shorttitle = {Comparing numerical methods for stiff systems of {O}.{D}.{E}},
	url = {https://doi.org/10.1007/BF01932994},
	doi = {10.1007/BF01932994},
	abstract = {This paper describes a technique for comparing numerical methods that have been designed to solve stiff systems of ordinary differential equations. The basis of a fair comparison is discussed in detail. Measurements of cost and reliability are made over a collection of 25 carefully selected problems. The problems have been designed to show how certain major factors affect the performance of a method.The technique is applied to five methods, of which three turn out to be quite good, including one based on backward differentiation formulas, another on second derivative formulas, and a third on extrapolation. However, each of the three has a weakness of its own, which can be identified with particular problem characteristics.},
	language = {en},
	number = {1},
	urldate = {2019-11-18},
	journal = {BIT Numerical Mathematics},
	author = {Enright, W. H. and Hull, T. E. and Lindberg, B.},
	month = mar,
	year = {1975},
	keywords = {Computational Mathematic, Differential Equation, Fair Comparison, Ordinary Differential Equation, Problem Characteristic},
	pages = {10--48},
}

@article{dormand_new_1978,
	title = {New {Runge}-{Kutta} algorithms for numerical simulation in dynamical astronomy},
	volume = {18},
	issn = {1572-9478},
	url = {https://doi.org/10.1007/BF01230162},
	doi = {10.1007/BF01230162},
	abstract = {Some new Runge-Kutta and Runge-Kutta-Nystrom algorithms are presented for the solution of ordinary differential equations of the initial value type. The methods are compared with others in integrating the equations of motion of the two body problem and are shown to offer advantages in efficiency. It is also demonstrated that the new methods can be ‘tuned’ to achieve some measure of global error control.},
	language = {en},
	number = {3},
	urldate = {2019-11-13},
	journal = {Celestial mechanics},
	author = {Dormand, J. R. and Prince, P. J.},
	month = oct,
	year = {1978},
	keywords = {Body Problem, Differential Equation, Error Control, Global Error, Ordinary Differential Equation},
	pages = {223--232},
}

@article{dormand_families_1987,
	title = {Families of {Runge}-{Kutta}-{Nystrom} {Formulae}},
	volume = {7},
	issn = {0272-4979},
	url = {https://academic.oup.com/imajna/article/7/2/235/735233},
	doi = {10.1093/imanum/7.2.235},
	abstract = {Abstract.  Criteria to be satisfied by efficient embedded Runge-Kutta-Nystrom formulae are presented, and new families are derived. Test results indicate their},
	language = {en},
	number = {2},
	urldate = {2019-11-13},
	journal = {IMA Journal of Numerical Analysis},
	author = {Dormand, J. R. and El-Mikkawy, M. E. A. and Prince, P. J.},
	month = apr,
	year = {1987},
	pages = {235--250},
}

@article{prince_high_1981,
	title = {High order embedded {Runge}-{Kutta} formulae},
	volume = {7},
	issn = {0377-0427},
	url = {http://www.sciencedirect.com/science/article/pii/0771050X81900103},
	doi = {10.1016/0771-050X(81)90010-3},
	abstract = {The criteria to be satisfied by embedded Runge-Kutta pairs of formulae are reviewed. Two new formulae of orders 6 and 8 are presented together with tests on their efficiency relative to other high order formulae in current use.},
	language = {en},
	number = {1},
	urldate = {2019-11-13},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Prince, P. J. and Dormand, J. R.},
	month = mar,
	year = {1981},
	pages = {67--75},
}

@article{dormand_family_1980,
	title = {A family of embedded {Runge}-{Kutta} formulae},
	volume = {6},
	issn = {0377-0427},
	url = {http://www.sciencedirect.com/science/article/pii/0771050X80900133},
	doi = {10.1016/0771-050X(80)90013-3},
	abstract = {A family of embedded Runge-Kutta formulae RK5 (4) are derived. From these are presented formulae which have (a) ‘small’ principal truncation terms in the fifth order and (b) extended regions of absolute stability.},
	language = {en},
	number = {1},
	urldate = {2019-11-13},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Dormand, J. R. and Prince, P. J.},
	month = mar,
	year = {1980},
	pages = {19--26},
}

@article{byrne_stiff_1987,
	title = {Stiff {ODE} solvers: {A} review of current and coming attractions},
	volume = {70},
	issn = {0021-9991},
	shorttitle = {Stiff {ODE} solvers},
	url = {http://www.sciencedirect.com/science/article/pii/0021999187900015},
	doi = {10.1016/0021-9991(87)90001-5},
	language = {en},
	number = {1},
	urldate = {2019-11-13},
	journal = {Journal of Computational Physics},
	author = {Byrne, George D and Hindmarsh, Alan C},
	month = may,
	year = {1987},
	pages = {1--62},
}

@misc{hairer_solving_2000,
	title = {Solving {Ordinary} ,{Differential} {Equations} {I}, {Nonstiff} problems/{E}. {Hairer}, {S}. {P}. {Norsett}, {G}. {Wanner}, {Second} {Revised} {Edition} with 135 {Figures}, {Vol}.: 1},
	shorttitle = {Solving {Ordinary} ,{Differential} {Equations} {I}, {Nonstiff} problems/{E}. {Hairer}, {S}. {P}. {Norsett}, {G}. {Wanner}, {Second} {Revised} {Edition} with 135 {Figures}, {Vol}.},
	url = {https://infoscience.epfl.ch/record/103930},
	abstract = {HAIRER, E., NORSETT, S. P., WANNER, G.},
	language = {en},
	urldate = {2019-11-13},
	journal = {Infoscience},
	author = {Hairer, E.},
	year = {2000},
}

@article{hull_comparing_1972,
	title = {Comparing {Numerical} {Methods} for {Ordinary} {Differential} {Equations}},
	volume = {9},
	issn = {0036-1429},
	url = {https://epubs.siam.org/doi/abs/10.1137/0709052},
	doi = {10.1137/0709052},
	abstract = {Numerical methods for systems of first order ordinary differential equations are tested on a variety of initial value problems. The methods are compared primarily as to how well they can handle relatively routine integration steps under a variety of accuracy requirements, rather than how well they handle difficulties caused by discontinuities, stiffness, roundoff or getting started. According to criteria involving the number of function evaluations, overhead cost, and reliability, the best general-purpose method, if function evaluations are not very costly, is one due to Bulirsch and Stoer. However, when function evaluations are relatively expensive, variable-order methods based on Adams formulas are best. The overhead costs are lower for the method of Bulirsch and Stoer, but the Adams methods require considerably fewer function evaluations. Krogh’s implementation of a variable-order Adams method is the best of those tested, but one due to Gear is also very good. In general, Runge–Kutta methods are not competitive, but fourth or fifth order methods of this type are best for restricted classes of problems in which function evaluations are not very expensive and accuracy requirements are not very stringent.The problems, methods and comparison criteria are specified very carefully. One objective in doing so is to provide a rigorous conceptual basis for comparing methods. Another is to provide a useful standard for such comparisons. A program called DETEST is used to obtain the relevant statistics for any particular method.},
	number = {4},
	urldate = {2019-11-13},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Hull, T. E. and Enright, W. H. and Fellen, B. M. and Sedgwick, A. E.},
	month = dec,
	year = {1972},
	pages = {603--637},
}

@misc{honchar_neural_2019,
	title = {Neural {ODEs}: breakdown of another deep learning breakthrough},
	shorttitle = {Neural {ODEs}},
	url = {https://towardsdatascience.com/neural-odes-breakdown-of-another-deep-learning-breakthrough-3e78c7213795},
	abstract = {Hi everyone! If you’re reading this article, most probably you’re catching up with the recent advances that happen in the AI world. The…},
	language = {en},
	urldate = {2019-11-12},
	journal = {Medium},
	author = {Honchar, Alexandr},
	month = jun,
	year = {2019},
}

@inproceedings{song_slicing_2019,
	address = {Piscataway, NJ, USA},
	series = {{SESoS}-{WDES} '19},
	title = {Slicing {Executable} {System}-of-systems {Models} for {Efficient} {Statistical} {Verification}},
	url = {https://doi.org/10.1109/SESoS/WDES.2019.00011},
	doi = {10.1109/SESoS/WDES.2019.00011},
	abstract = {A System of Systems (SoS), composed of independent constituent systems, can create synergy among its systems to achieve a common goal. Many studies have used statistical model checking techniques to verify how well an SoS can achieve its goals. SoS models are usually complex and probabilistic, which makes statistical verification computationally expensive. To reduce this cost, dynamic slicing techniques can be applied to SoS models since both dynamic slicing and statistical verification focus on the models' execution samples. However, existing dynamic slicing techniques cannot guarantee executable accurate slices of SoS models when the models contain uncertainty. Therefore, we propose a hybrid slicing approach that combines dynamic backward slicing and modified observation-based slicing to produce accurate executable slices. Experimentation on the proposed technique found that the verification time was significantly reduced (47--56\%), depending on the property, while preserving the verification results.},
	urldate = {2019-10-28},
	booktitle = {Proceedings of the 7th {International} {Workshop} on {Software} {Engineering} for {Systems}-of-{Systems} and 13th {Workshop} on {Distributed} {Software} {Development}, {Software} {Ecosystems} and {Systems}-of-{Systems}},
	publisher = {IEEE Press},
	author = {Song, Jiyoung and Tørring, Jacob O. and Hyun, Sangwon and Jee, Eunkyoung and Bae, Doo-Hwan},
	year = {2019},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {model slicing, model verification, statistical model checking, system-of-systems},
	pages = {18--25},
}

@article{penney_survey_2019,
	title = {A {Survey} of {Machine} {Learning} {Applied} to {Computer} {Architecture} {Design}},
	url = {http://arxiv.org/abs/1909.12373},
	abstract = {Machine learning has enabled signiﬁcant beneﬁts in diverse ﬁelds, but, with a few exceptions, has had limited impact on computer architecture. Recent work, however, has explored broader applicability for design, optimization, and simulation. Notably, machine learning based strategies often surpass prior state-of-the-art analytical, heuristic, and human-expert approaches. This paper reviews machine learning applied system-wide to simulation and run-time optimization, and in many individual components, including memory systems, branch predictors, networks-on-chip, and GPUs. The paper further analyzes current practice to highlight useful design strategies and identify areas for future work, based on optimized implementation strategies, opportune extensions to existing work, and ambitious long term possibilities. Taken together, these strategies and techniques present a promising future for increasingly automated architectural design.},
	language = {en},
	urldate = {2019-10-22},
	journal = {arXiv:1909.12373 [cs]},
	author = {Penney, Drew D. and Chen, Lizhong},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.12373},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Hardware Architecture, Computer Science - Machine Learning},
}

@article{haas_writers_2009,
	title = {{WRITERS}’ {GROUPS} {FOR} {MA} {ESOL} {STUDENTS}: {COLLABORATIVELY} {CONSTRUCTING} {A} {MODEL} {OF} {THE} {WRITING} {PROCESS}},
	volume = {12},
	language = {en},
	author = {Haas, Sarah},
	year = {2009},
	pages = {8},
}

@misc{oh_nvidia_nodate,
	title = {The {NVIDIA} {Titan} {V} {Deep} {Learning} {Deep} {Dive}: {It}'s {All} {About} {The} {Tensor} {Cores}},
	shorttitle = {The {NVIDIA} {Titan} {V} {Deep} {Learning} {Deep} {Dive}},
	url = {https://www.anandtech.com/show/12673/titan-v-deep-learning-deep-dive},
	urldate = {2019-10-13},
	author = {Oh, Nate},
}

@book{wilkinson_rounding_1994,
	address = {New York, NY, USA},
	title = {Rounding {Errors} in {Algebraic} {Processes}},
	isbn = {978-0-486-67999-0},
	publisher = {Dover Publications, Inc.},
	author = {Wilkinson, James H.},
	year = {1994},
}

@misc{nvidia_nvidia_nodate-1,
	title = {{NVIDIA} {TESLA} {V100} {GPU} {ARCHITECTURE}},
	url = {https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf},
	author = {Nvidia},
}

@article{chen_neural_nodate,
	title = {Neural {Ordinary} {Differential} {Equations}},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing ﬂows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	language = {en},
	author = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
	pages = {13},
}

@article{niemeyer_gpu-based_2014,
	title = {{GPU}-{Based} {Parallel} {Integration} of {Large} {Numbers} of {Independent} {ODE} {Systems}},
	url = {http://arxiv.org/abs/1611.02274},
	doi = {10.1007/978-3-319-06548-9_8},
	abstract = {The task of integrating a large number of independent ODE systems arises in various scientiﬁc and engineering areas. For nonstiﬀ systems, common explicit integration algorithms can be used on GPUs, where individual GPU threads concurrently integrate independent ODEs with diﬀerent initial conditions or parameters. One example is the ﬁfth-order adaptive Runge–Kutta–Cash–Karp (RKCK) algorithm. In the case of stiﬀ ODEs, standard explicit algorithms require impractically small time-step sizes for stability reasons, and implicit algorithms are therefore commonly used instead to allow larger time steps and reduce the computational expense. However, typical high-order implicit algorithms based on backwards diﬀerentiation formulae (e.g., VODE, LSODE) involve complex logical ﬂow that causes severe thread divergence when implemented on GPUs, limiting the performance. Therefore, alternate algorithms are needed. A GPUbased Runge–Kutta–Chebyshev (RKC) algorithm can handle moderate levels of stiﬀness and performs signiﬁcantly faster than not only an equivalent CPU version but also a CPU-based implicit algorithm (VODE) based on results shown in the literature. In this chapter, we present the mathematical background, implementation details, and source code for the RKCK and RKC algorithms for use integrating large numbers of independent systems of ODEs on GPUs. In addition, brief performance comparisons are shown for each algorithm, demonstrating the potential beneﬁt of moving to GPU-based ODE integrators.},
	language = {en},
	urldate = {2019-10-08},
	journal = {arXiv:1611.02274 [physics]},
	author = {Niemeyer, Kyle E. and Sung, Chih-Jen},
	year = {2014},
	note = {arXiv: 1611.02274},
	keywords = {80A32 (Primary) 80A30, 65L04, 65L06 (Secondary), Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Mathematical Software, Physics - Computational Physics},
	pages = {159--182},
}

@article{ritschel_numerical_nodate,
	title = {Numerical {Methods} {For} {Solution} of {Differential} {Equations}},
	language = {en},
	author = {Ritschel, Tobias},
	pages = {224},
}

@article{hawick_numerical_nodate,
	title = {Numerical {Precision} and {Benchmarking} {Very}-{High}-{Order} {Integration} of {Particle} {Dynamics} on {GPU} {Accelerators}},
	abstract = {GPUs oﬀer a powerful acceleration platform for many scientiﬁc applications. Numerical integration of classical Newtonian dynamical particles often requires very high-order numerical accuracy. We assess the ﬂoatingpoint precision and performance of various GPUs for applications involving high-order time-step integration methods for particle model simulations using Nsquared interactions. We demonstrate how high-order algorithms can be expressed in Compute Uniﬁed Device Architecture (CUDA) and present some detailed benchmark data. We show the high numerical power of high-order integration methods such as Hairer’s 10th order method and relate its performance to its precision requirements.},
	language = {en},
	author = {Hawick, K A and Playne, D P and Johnson, M G B},
	pages = {7},
}

@article{harris_optimizing_nodate,
	title = {Optimizing {Parallel} {Reduction} in {CUDA}},
	language = {en},
	author = {Harris, Mark},
	pages = {38},
}

@inproceedings{garcia_adaptive_2011,
	title = {An adaptive step size {GPU} {ODE} solver for simulating the electric cardiac activity},
	abstract = {Simulation of electric cardiac activity requires the solution of a very large system of ordinary differential equations, which requires long computing times. Modern Graphic Processing Units (GPU) are powerful computing devices, which have been used to simulate electric cardiac activity. However, the numerical techniques applied were based on fixed time step. In this paper we describe an adaptive step size solver written for GPUs, and its application to simulate the behavior of a model of 300 atrial cells. Results presented in this study show that a robust adaptive step ODE solver can be implemented in a GPU. As expected, GPU implementations achieved much better performance than CPU solutions. In addition, the presented adaptive methodology achieved a computation time reduction up to a 25\% versus a fix step implementation.},
	booktitle = {2011 {Computing} in {Cardiology}},
	author = {Garcia, V. M. and Liberos, A. and Climent, A. M. and Vidal, A. and Millet, J. and González, A.},
	month = sep,
	year = {2011},
	keywords = {Accuracy, Adaptation models, Computational modeling, Differential equations, Graphics processing unit, Kernel, Mathematical model, adaptive step size GPU ODE solver, bioelectric phenomena, cardiology, cellular biophysics, differential equations, electric cardiac activity simulation, graphic processing units, graphics processing units, medical computing, ordinary differential equations, physiological models},
	pages = {233--236},
}

@article{al-omari_solving_2013,
	title = {Solving {Large} {Nonlinear} {Systems} of {First}-{Order} {Ordinary} {Differential} {Equations} {With} {Hierarchical} {Structure} {Using} {Multi}-{GPGPUs} and an {Adaptive} {Runge} {Kutta} {ODE} {Solver}},
	volume = {1},
	doi = {10.1109/ACCESS.2013.2290623},
	abstract = {The adaptive Runge-Kutta (ARK) method on multi-general-purpose graphical processing units (GPUs) is used for solving large nonlinear systems of first-order ordinary differential equations (ODEs) with over 10 000 variables describing a large genetic network in systems biology for the biological clock. To carry out the computation of the trajectory of the system, a hierarchical structure of the ODEs is exploited, and an ARK solver is implemented in compute unified device architecture/C++ (CUDA/C++) on GPUs. The result is a 75-fold speedup for calculations of 2436 independent modules within the genetic network describing clock function relative to a comparable CPU architecture. These 2436 modules span one-quarter of the entire genome of a model fungal system, Neurospora crassa. The power of a GPU can in principle be harnessed by using warp-level parallelism, instruction level parallelism or both of them. Since the ARK ODE solver is entirely sequential, we propose a new parallel processing algorithm using warp-level parallelism for solving 10 000 ODEs that belong to a large genetic network describing clock genome-level dynamics. A video is attached illustrating the general idea of the method on GPUs that can be used to provide new insights into the biological clock through single cell measurements on the clock.},
	journal = {IEEE Access},
	author = {Al-Omari, A. and Arnold, J. and Taha, T. and Schüttler, H.},
	year = {2013},
	keywords = {ARK method, ARK solver, Bioinformatics, CPU architecture, CUDA-C++, Clocks, Genomics, Graphics processing units, Instruction sets, Parallel processing, Runge-Kutta methods, adaptive Runge–Kutta integration, adaptive runge Kutta ODE solver, bioinformatics, biological clock, biological clock function, clock genome-level dynamics, compute unified device architecture, differential equations, finite element method, first-order ordinary differential equations, general-purpose graphical processing unit, graphics processing units, hierarchical structure, instruction level parallelism, large genetic network, large nonlinear systems, mathematics computing, model fungal system, multiGPGPUs, multigeneral-purpose graphical processing units, neurospora crassa, nonlinear equations, nonlinear systems, ordinary differential equation, parallel algorithms, parallel architectures, parallel processing algorithm, single cell measurements, systems biology, warp-level parallelism},
	pages = {770--777},
}

@phdthesis{lionetti_gpu_2010,
	title = {{GPU} accelerated cardiac electrophysiology},
	url = {https://escholarship.org/uc/item/69g832q7},
	abstract = {Numerical simulations of cellular membranes are useful for both basic science and increasingly for clinical diagnostic and therapeutic applications. A common bottleneck in such simulations arises from solving large highly complex stiff systems of ordinary differential equations (ODEs) thousands of times for numerous collocation points (representing cells) throughout a three -dimensional volume. For some electrophysiology simulations, over 98\% of the time is spent solving these systems of ODEs when run in serial on a single core. We have reduced the time to simulate a single heartbeat from 4.5 hours on a 48 core Opteron cluster (MPI implementation) to 12.7 minutes on a Desktop workstation equipped with a \$500 GPU accelerator that also economizes power consumption. This improvement over cluster performance transforms the simulation workflow, and at this level of performance we can realize larger scale simulations, previously only feasible on a cluster, that are needed in a clinical setting. To achieve this same performance on our Opteron cluster would theoretically require at least a 1020 core system. Thus, the GPU has effectively miniaturized the hardware requirement to perform the simulation. We also demonstrate 23x to 280x speedups across a wide spectrum of cardiac cell models running on the nVidia GTX-295 GPU compared with a multithread implementation on a 4-core Intel i7 processor. Our simulator employs a source-to-source translator that converts a higher level python description of a cell model into highly tuned and optimized CUDA source code, which is then compiled with the CUDA C compiler for execution. Our optimizations include automatic kernel partitioning and a software managed-memory cache. Our translator also removes numerical singularities introduced by using single precision arithmetic},
	language = {en},
	urldate = {2019-10-04},
	school = {UC San Diego},
	author = {Lionetti, Fred},
	year = {2010},
}

@misc{lai_multi-gpu_2019,
	type = {Research article},
	title = {A {Multi}-{GPU} {Parallel} {Algorithm} in {Hypersonic} {Flow} {Computations}},
	url = {https://www.hindawi.com/journals/mpe/2019/2053156/},
	abstract = {Computational fluid dynamics (CFD) plays an important role in the optimal design of aircraft and the analysis of complex flow mechanisms in the aerospace domain. The graphics processing unit (GPU) has a strong floating-point operation capability and a high memory bandwidth in data parallelism, which brings great opportunities for CFD. A cell-centred finite volume method is applied to solve three-dimensional compressible Navier–Stokes equations on structured meshes with an upwind AUSM},
	language = {en},
	urldate = {2019-10-02},
	journal = {Mathematical Problems in Engineering},
	author = {Lai, Jianqi and Li, Hua and Tian, Zhengyu and Zhang, Ye},
	year = {2019},
	doi = {10.1155/2019/2053156},
}

@article{guha_deepframe:_nodate,
	title = {Deepframe: {A} {Proﬁle}-driven {Compiler} for {Spatial} {Hardware} {Accelerators}},
	abstract = {Tracing code paths to form extended basic blocks is useful in many areas, compiler optimizations [1], improving instruction cache behavior [2] and custom-hardware ofﬂoading [3]. Prior work has been plagued by small traces, limited either by the overheads of dynamic proﬁling, statically available information [4], or side-exit branches [5]. In this work, we rethink what code path sequences to fuse and construct long traces for ofﬂoading to spatial accelerators, while minimizing the occurrence of side exits which limit dynamic coverage.},
	language = {en},
	author = {Guha, Apala and Vedula, Naveen and Shriraman, Arrvindh},
	pages = {14},
}

@inproceedings{shin_spectrum-based_2019,
	title = {Spectrum-{Based} {Fault} {Localization} on a {Collaboration} {Graph} of a {System}-of-{Systems}},
	doi = {10.1109/SYSOSE.2019.8753843},
	abstract = {A System-of-Systems (SoS) consists of independent and autonomous constituent systems (CSs) which collaborate to achieve an SoS goal. For SoS engineers, it is important to verify the results of the collaboration for an SoS goal. Statistical verification can be used to verify a large and complex SoS and to provide quantitative verification results. However, even when a failure of an SoS goal or a violation of a verification property is detected, it often requires a huge cost to find faults of an SoS because of the size of the SoS and the lack of information about independent CSs. In this paper, we propose a fault localization technique for an SoS to reduce the debugging cost by prioritizing suspicious entities of an SoS. This technique requires only an abstract model of the collaboration, named as a collaboration graph, which includes the presence or absence of CSs and their interactions. It extends a spectrum-based fault localization (SBFL) technique to utilize quantitative results of the statistical verification of an SoS. In the evaluation, we show that it is feasible to apply SBFL to SoS fault localization, and our approach is expected to effectively reduce the debugging space of an SoS.},
	booktitle = {2019 14th {Annual} {Conference} {System} of {Systems} {Engineering} ({SoSE})},
	author = {Shin, Y. and Hyun, S. and Baek, Y. and Bae, D.},
	month = may,
	year = {2019},
	keywords = {SoS engineers, SoS fault localization, System-of-Systems, autonomous constituent systems, collaboration graph, debugging, fault localization, formal verification, independent systems, program debugging, program testing, quantitative verification results, software fault tolerance, spectrum-based fault localization, spectrum-based fault localization technique, statistical verification, system recovery},
	pages = {358--363},
}

@article{massaroli_port-hamiltonian_2019,
	title = {Port-{Hamiltonian} {Approach} to {Neural} {Network} {Training}},
	url = {http://arxiv.org/abs/1909.02702},
	abstract = {Neural networks are discrete entities: subdivided into discrete layers and parametrized by weights which are iteratively optimized via difference equations. Recent work proposes networks with layer outputs which are no longer quantized but are solutions of an ordinary differential equation (ODE); however, these networks are still optimized via discrete methods (e.g. gradient descent). In this paper, we explore a different direction: namely, we propose a novel framework for learning in which the parameters themselves are solutions of ODEs. By viewing the optimization process as the evolution of a port-Hamiltonian system, we can ensure convergence to a minimum of the objective function. Numerical experiments have been performed to show the validity and effectiveness of the proposed methods.},
	language = {en},
	urldate = {2019-09-18},
	journal = {arXiv:1909.02702 [cs, eess, stat]},
	author = {Massaroli, Stefano and Poli, Michael and Califano, Federico and Faragasso, Angela and Park, Jinkyoo and Yamashita, Atsushi and Asama, Hajime},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.02702},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning},
}

@inproceedings{liang_correcting_2017,
	address = {New York, NY, USA},
	series = {{SC} '17},
	title = {Correcting {Soft} {Errors} {Online} in {Fast} {Fourier} {Transform}},
	isbn = {978-1-4503-5114-0},
	url = {http://doi.acm.org/10.1145/3126908.3126915},
	doi = {10.1145/3126908.3126915},
	abstract = {While many algorithm-based fault tolerance (ABFT) schemes have been proposed to detect soft errors offline in the fast Fourier transform (FFT) after computation finishes, none of the existing ABFT schemes detect soft errors online before the computation finishes. This paper presents an online ABFT scheme for FFT so that soft errors can be detected online and the corrupted computation can be terminated in a much more timely manner. We also extend our scheme to tolerate both arithmetic errors and memory errors, develop strategies to reduce its fault tolerance overhead and improve its numerical stability and fault coverage, and finally incorporate it into the widely used FFTW library - one of the today's fastest FFT software implementations. Experimental results demonstrate that: (1) the proposed online ABFT scheme introduces much lower overhead than the existing offline ABFT schemes; (2) it detects errors in a much more timely manner; and (3) it also has higher numerical stability and better fault coverage.},
	urldate = {2019-09-17},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {ACM},
	author = {Liang, Xin and Chen, Jieyang and Tao, Dingwen and Li, Sihuan and Wu, Panruo and Li, Hongbo and Ouyang, Kaiming and Liu, Yuanlai and Song, Fengguang and Chen, Zizhong},
	year = {2017},
	note = {event-place: Denver, Colorado},
	keywords = {DFT, FFT, FFTW, algorithm-based fault tolerance, soft errors},
	pages = {30:1--30:12},
}

@inproceedings{tillet_input-aware_2017,
	address = {New York, NY, USA},
	series = {{SC} '17},
	title = {Input-aware {Auto}-tuning of {Compute}-bound {HPC} {Kernels}},
	isbn = {978-1-4503-5114-0},
	url = {http://doi.acm.org/10.1145/3126908.3126939},
	doi = {10.1145/3126908.3126939},
	abstract = {Efficient implementations of HPC applications for parallel architectures generally rely on external software packages (e.g., BLAS, LAPACK, CUDNN). While these libraries provide highly optimized routines for certain characteristics of inputs (e.g., square matrices), they generally do not retain optimal performance across the wide range of problems encountered in practice. In this paper, we present an input-aware auto-tuning framework for matrix multiplications and convolutions, ISAAC, which uses predictive modeling techniques to drive highly parameterized PTX code templates towards not only hardware-, but also application-specific kernels. Numerical experiments on the NVIDIA Maxwell and Pascal architectures show up to 3x performance gains over both cuBLAS and cuDNN after only a few hours of auto-tuning.},
	urldate = {2019-09-17},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {ACM},
	author = {Tillet, Philippe and Cox, David},
	year = {2017},
	note = {event-place: Denver, Colorado},
	pages = {43:1--43:12},
}

@inproceedings{li_capes:_2017,
	address = {New York, NY, USA},
	series = {{SC} '17},
	title = {{CAPES}: {Unsupervised} {Storage} {Performance} {Tuning} {Using} {Neural} {Network}-based {Deep} {Reinforcement} {Learning}},
	isbn = {978-1-4503-5114-0},
	shorttitle = {{CAPES}},
	url = {http://doi.acm.org/10.1145/3126908.3126951},
	doi = {10.1145/3126908.3126951},
	abstract = {Parameter tuning is an important task of storage performance optimization. Current practice usually involves numerous tweak-benchmark cycles that are slow and costly. To address this issue, we developed CAPES, a model-less deep reinforcement learning-based unsupervised parameter tuning system driven by a deep neural network (DNN). It is designed to find the optimal values of tunable parameters in computer systems, from a simple client-server system to a large data center, where human tuning can be costly and often cannot achieve optimal performance. CAPES takes periodic measurements of a target computer system's state, and trains a DNN which uses Q-learning to suggest changes to the system's current parameter values. CAPES is minimally intrusive, and can be deployed into a production system to collect training data and suggest tuning actions during the system's daily operation. Evaluation of a prototype on a Lustre file system demonstrates an increase in I/O throughput up to 45\% at saturation point.},
	urldate = {2019-09-17},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {ACM},
	author = {Li, Yan and Chang, Kenneth and Bel, Oceane and Miller, Ethan L. and Long, Darrell D. E.},
	year = {2017},
	note = {event-place: Denver, Colorado},
	keywords = {deep learning, performance tuning, q-learning},
	pages = {42:1--42:14},
}

@inproceedings{carastan-santos_obtaining_2017,
	address = {New York, NY, USA},
	series = {{SC} '17},
	title = {Obtaining {Dynamic} {Scheduling} {Policies} with {Simulation} and {Machine} {Learning}},
	isbn = {978-1-4503-5114-0},
	url = {http://doi.acm.org/10.1145/3126908.3126955},
	doi = {10.1145/3126908.3126955},
	abstract = {Dynamic scheduling of tasks in large-scale HPC platforms is normally accomplished using ad-hoc heuristics, based on task characteristics, combined with some backfilling strategy. Defining heuristics that work efficiently in different scenarios is a difficult task, specially when considering the large variety of task types and platform architectures. In this work, we present a methodology based on simulation and machine learning to obtain dynamic scheduling policies. Using simulations and a workload generation model, we can determine the characteristics of tasks that lead to a reduction in the mean slowdown of tasks in an execution queue. Modeling these characteristics using a nonlinear function and applying this function to select the next task to execute in a queue improved the mean task slowdown in synthetic workloads. When applied to real workload traces from highly different machines, these functions still resulted in performance improvements, attesting the generalization capability of the obtained heuristics.},
	urldate = {2019-09-17},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {ACM},
	author = {Carastan-Santos, Danilo and de Camargo, Raphael Y.},
	year = {2017},
	note = {event-place: Denver, Colorado},
	keywords = {high performance computing, machine learning, scheduling, simulation},
	pages = {32:1--32:13},
}

@inproceedings{haidar_harnessing_2018,
	address = {Dallas, TX, USA},
	title = {Harnessing {GPU} {Tensor} {Cores} for {Fast} {FP16} {Arithmetic} to {Speed} up {Mixed}-{Precision} {Iterative} {Refinement} {Solvers}},
	isbn = {978-1-5386-8384-2},
	url = {https://ieeexplore.ieee.org/document/8665777/},
	doi = {10.1109/SC.2018.00050},
	abstract = {Low-precision ﬂoating-point arithmetic is a powerful tool for accelerating scientiﬁc computing applications, especially those in artiﬁcial intelligence. Here, we present an investigation showing that other high-performance computing (HPC) applications can also harness this power. Speciﬁcally, we use the general HPC problem, Ax = b, where A is a large dense matrix, and a double precision (FP64) solution is needed for accuracy. Our approach is based on mixed-precision (FP16→FP64) iterative reﬁnement, and we generalize and extend prior advances into a framework, for which we develop architecture-speciﬁc algorithms and highly tuned implementations. These new methods show how using half-precision Tensor Cores (FP16-TC) for the arithmetic can provide up to 4× speedup. This is due to the performance boost that the FP16-TC provide as well as to the improved accuracy over the classical FP16 arithmetic that is obtained because the GEMM accumulation occurs in FP32 arithmetic.},
	language = {en},
	urldate = {2019-09-17},
	booktitle = {{SC18}: {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE},
	author = {Haidar, Azzam and Tomov, Stanimire and Dongarra, Jack and Higham, Nicholas J.},
	month = nov,
	year = {2018},
	pages = {603--613},
}

@inproceedings{das_doomsday:_2018,
	address = {Dallas, TX, USA},
	title = {Doomsday: {Predicting} {Which} {Node} {Will} {Fail} {When} on {Supercomputers}},
	isbn = {978-1-5386-8384-2},
	shorttitle = {Doomsday},
	url = {https://ieeexplore.ieee.org/document/8665792/},
	doi = {10.1109/SC.2018.00012},
	abstract = {Predicting which node will fail and how soon remains a challenge for HPC resilience, yet may pave the way to exploiting proactive remedies before jobs fail. Not only for increasing scalability up to exascale systems but even for contemporary supercomputer architectures does it require substantial efforts to distill anomalous events from noisy raw logs. To this end, we propose a novel phrase extraction mechanism called TBP (timebased phrases) to pin-point node failures, which is unprecedented. Our study, based on real system data and statistical machine learning, demonstrates the feasibility to predict which speciﬁc node will fail in Cray systems. TBP achieves no less than 83\% recall rates with lead times as high as 2 minutes. This opens up the door for enhancing prediction lead times for supercomputing systems in general, thereby facilitating efﬁcient usage of both computing capacity and power in large scale production systems.},
	language = {en},
	urldate = {2019-09-17},
	booktitle = {{SC18}: {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE},
	author = {Das, Anwesha and Mueller, Frank and Hargrove, Paul and Roman, Eric and Baden, Scott},
	month = nov,
	year = {2018},
	pages = {108--121},
}

@inproceedings{menon_adapt:_2018,
	address = {Dallas, TX, USA},
	title = {{ADAPT}: {Algorithmic} {Differentiation} {Applied} to {Floating}-{Point} {Precision} {Tuning}},
	isbn = {978-1-5386-8384-2},
	shorttitle = {{ADAPT}},
	url = {https://ieeexplore.ieee.org/document/8665747/},
	doi = {10.1109/SC.2018.00051},
	abstract = {HPC applications use ﬂoating point arithmetic operations extensively to solve computational problems. Mixedprecision computing seeks to use the lowest precision data type that is sufﬁcient to achieve a desired accuracy, improving performance and reducing power consumption. Manually optimizing a program to use mixed precision is challenging as it not only requires extensive knowledge about the numerical behavior of the algorithm but also estimates of the rounding errors. In this work, we present ADAPT, a scalable approach for mixed-precision analysis on HPC workloads using algorithmic differentiation to provide accurate estimates about the ﬁnal output error. ADAPT provides a ﬂoating-point precision sensitivity proﬁle while incurring an overhead of only a constant multiple of the original computation irrespective of the number of variables analyzed. The sensitivity proﬁle can be used to make algorithmic choices and to develop mixed-precision conﬁgurations of a program. We evaluate ADAPT on six benchmarks and a proxy application (LULESH) and show that we are able to achieve a speedup of 1.2x on the proxy application.},
	language = {en},
	urldate = {2019-09-17},
	booktitle = {{SC18}: {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE},
	author = {Menon, Harshitha and Lam, Michael O. and Osei-Kuffuor, Daniel and Schordan, Markus and Lloyd, Scott and Mohror, Kathryn and Hittinger, Jeffrey},
	month = nov,
	year = {2018},
	pages = {614--626},
}

@article{wei_mask-cnn:_2018,
	title = {Mask-{CNN}: {Localizing} parts and selecting descriptors for fine-grained bird species categorization},
	volume = {76},
	issn = {0031-3203},
	shorttitle = {Mask-{CNN}},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320317303990},
	doi = {10.1016/j.patcog.2017.10.002},
	abstract = {Fine-grained image recognition is a challenging computer vision problem, due to the small inter-class variations caused by highly similar subordinate categories, and the large intra-class variations in poses, scales and rotations. In this paper, we prove that selecting useful deep descriptors contributes well to fine-grained image recognition. Specifically, a novel Mask-CNN model without the fully connected layers is proposed. Based on the part annotations, the proposed model consists of a fully convolutional network to both locate the discriminative parts (e.g., head and torso), and more importantly generate weighted object/part masks for selecting useful and meaningful convolutional descriptors. After that, a three-stream Mask-CNN model is built for aggregating the selected object- and part-level descriptors simultaneously. Thanks to discarding the parameter redundant fully connected layers, our Mask-CNN has a small feature dimensionality and efficient inference speed by comparing with other fine-grained approaches. Furthermore, we obtain a new state-of-the-art accuracy on two challenging fine-grained bird species categorization datasets, which validates the effectiveness of both the descriptor selection scheme and the proposed Mask-CNN model.},
	urldate = {2019-09-07},
	journal = {Pattern Recognition},
	author = {Wei, Xiu-Shen and Xie, Chen-Wei and Wu, Jianxin and Shen, Chunhua},
	month = apr,
	year = {2018},
	keywords = {Deep descriptor selection, Fine-grained image recognition, Part localization},
	pages = {704--714},
}

@inproceedings{krause_unreasonable_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {Unreasonable} {Effectiveness} of {Noisy} {Data} for {Fine}-{Grained} {Recognition}},
	isbn = {978-3-319-46487-9},
	abstract = {Current approaches for fine-grained recognition do the following: First, recruit experts to annotate a dataset of images, optionally also collecting more structured data in the form of part annotations and bounding boxes. Second, train a model utilizing this data. Toward the goal of solving fine-grained recognition, we introduce an alternative approach, leveraging free, noisy data from the web and simple, generic methods of recognition. This approach has benefits in both performance and scalability. We demonstrate its efficacy on four fine-grained datasets, greatly exceeding existing state of the art without the manual collection of even a single label, and furthermore show first results at scaling to more than 10,000 fine-grained categories. Quantitatively, we achieve top-1 accuracies of 92.3\%92.3\%92.3{\textbackslash},{\textbackslash}\% on CUB-200-2011, 85.4\%85.4\%85.4{\textbackslash},{\textbackslash}\% on Birdsnap, 93.4\%93.4\%93.4{\textbackslash},{\textbackslash}\% on FGVC-Aircraft, and 80.8\%80.8\%80.8{\textbackslash},{\textbackslash}\% on Stanford Dogs without using their annotated training sets. We compare our approach to an active learning approach for expanding fine-grained datasets.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Krause, Jonathan and Sapp, Benjamin and Howard, Andrew and Zhou, Howard and Toshev, Alexander and Duerig, Tom and Philbin, James and Fei-Fei, Li},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	pages = {301--320},
}

@inproceedings{kwan_bird_2004,
	title = {Bird classification algorithms: theory and experimental results},
	volume = {5},
	shorttitle = {Bird classification algorithms},
	doi = {10.1109/ICASSP.2004.1327104},
	abstract = {To minimize the number of birdstrikes, a common method is to use microphone arrays to monitor and identify dangerous birds near the airport or some critical locations in the airspace. However, it was recognized that the range of existing ground-based acoustic monitoring devices is only limited to a few hundred meters. Moreover, the bird classification performance in low signal-to-noise environments such as airports is not very satisfactory. This paper summarizes the development of a high performance bird classification system using a hidden Markov model (HMM) and Gaussian mixture model (GMM). Experimental results verified the classification performance.},
	booktitle = {2004 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	author = {Kwan, C. and Mei, G. and Zhao, X. and Ren, Z. and Xu, R. and Stanford, V. and Rochet, C. and Aube, J. and Ho, K. C.},
	month = may,
	year = {2004},
	keywords = {Airports, Birds, Classification algorithms, Computerized monitoring, Feature extraction, GMM, Gaussian mixture model, Gaussian processes, HMM, Hidden Markov models, Low pass filters, Microphone arrays, PCA, Principal component analysis, Sampling methods, VQ, acoustic signal processing, airports, array signal processing, bird classification algorithms, bird feature extraction, bird recognition system, bird sound monitoring system, birdstrikes, circular microphone array, dangerous birds, feature extraction, hidden Markov model, hidden Markov models, low signal-to-noise environments, principal component analysis, range limited ground-based acoustic monitoring devices, signal classification, vector quantisation},
	pages = {V--289},
}

@article{acevedo_automated_2009,
	title = {Automated classification of bird and amphibian calls using machine learning: {A} comparison of methods},
	volume = {4},
	issn = {1574-9541},
	shorttitle = {Automated classification of bird and amphibian calls using machine learning},
	url = {http://www.sciencedirect.com/science/article/pii/S1574954109000351},
	doi = {10.1016/j.ecoinf.2009.06.005},
	abstract = {We compared the ability of three machine learning algorithms (linear discriminant analysis, decision tree, and support vector machines) to automate the classification of calls of nine frogs and three bird species. In addition, we tested two ways of characterizing each call to train/test the system. Calls were characterized with four standard call variables (minimum and maximum frequencies, call duration and maximum power) or eleven variables that included three standard call variables (minimum and maximum frequencies, call duration) and a coarse representation of call structure (frequency of maximum power in eight segments of the call). A total of 10,061 isolated calls were used to train/test the system. The average true positive rates for the three methods were: 94.95\% for support vector machine (0.94\% average false positive rate), 89.20\% for decision tree (1.25\% average false positive rate) and 71.45\% for linear discriminant analysis (1.98\% average false positive rate). There was no statistical difference in classification accuracy based on 4 or 11 call variables, but this efficient data reduction technique in conjunction with the high classification accuracy of the SVM is a promising combination for automated species identification by sound. By combining automated digital recording systems with our automated classification technique, we can greatly increase the temporal and spatial coverage of biodiversity data collection.},
	number = {4},
	urldate = {2019-09-06},
	journal = {Ecological Informatics},
	author = {Acevedo, Miguel A. and Corrada-Bravo, Carlos J. and Corrada-Bravo, Héctor and Villanueva-Rivera, Luis J. and Aide, T. Mitchell},
	month = sep,
	year = {2009},
	keywords = {Amphibian calls, Bird calls, Decision tree, Linear discriminant analysis, Machine learning, Support vector machine},
	pages = {206--214},
}

@inproceedings{goens_case_2019,
	address = {Phoenix, AZ, USA},
	title = {A case study on machine learning for synthesizing benchmarks},
	isbn = {978-1-4503-6719-6},
	url = {http://dl.acm.org/citation.cfm?doid=3315508.3329976},
	doi = {10.1145/3315508.3329976},
	abstract = {Good benchmarks are hard to find because they require a substantial effort to keep them representative for the constantly changing challenges of a particular field. Synthetic benchmarks are a common approach to deal with this, and methods from machine learning are natural candidates for synthetic benchmark generation. In this paper we investigate the usefulness of machine learning in the prominent CLgen benchmark generator. We re-evaluate CLgen by comparing the benchmarks generated by the model with the raw data used to train it. This re-evaluation indicates that, for the use case considered, machine learning did not yield additional benefit over a simpler method using the raw data. We investigate the reasons for this and provide further insights into the challenges the problem could pose for potential future generators.},
	language = {en},
	urldate = {2019-09-03},
	booktitle = {Proceedings of the 3rd {ACM} {SIGPLAN} {International} {Workshop} on {Machine} {Learning} and {Programming} {Languages}  - {MAPL} 2019},
	publisher = {ACM Press},
	author = {Goens, Andrés and Brauckmann, Alexander and Ertel, Sebastian and Cummins, Chris and Leather, Hugh and Castrillon, Jeronimo},
	year = {2019},
	pages = {38--46},
}

@inproceedings{filipovic_autotuning_2017,
	address = {New York, NY, USA},
	series = {{ANDARE} '17},
	title = {Autotuning of {OpenCL} {Kernels} with {Global} {Optimizations}},
	isbn = {978-1-4503-5363-2},
	url = {http://doi.acm.org/10.1145/3152821.3152877},
	doi = {10.1145/3152821.3152877},
	abstract = {Autotuning is an important method for automatically exploring code optimizations. It may target low-level code optimizations, such as memory blocking, loop unrolling or memory prefetching, as well as high-level optimizations, such as placement of computation kernels on proper hardware devices, optimizing memory transfers between nodes or between accelerators and main memory. In this paper, we introduce an autotuning method, which extends state-of-the-art low-level tuning of OpenCL or CUDA kernels towards more complex optimizations. More precisely, we introduce a Kernel Tuning Toolkit (KTT), which implements inter-kernel global optimizations, allowing to tune parameters affecting multiple kernels or also the host code. We demonstrate on practical examples, that with global kernel optimizations we are able to explore tuning options that are not possible if kernels are tuned separately. Moreover, our tuning strategies can take into account numerical accuracy across multiple kernel invocations and search for implementations within specific numerical error bounds.},
	urldate = {2019-09-03},
	booktitle = {Proceedings of the 1st {Workshop} on {AutotuniNg} and {aDaptivity} {AppRoaches} for {Energy} {Efficient} {HPC} {Systems}},
	publisher = {ACM},
	author = {Filipovič, Jiří and Petrovič, Filip and Benkner, Siegfried},
	year = {2017},
	note = {event-place: Portland, OR, USA},
	pages = {2:1--2:6},
}

@article{cianfriglia_model-driven_2018,
	title = {A model-driven approach for a new generation of adaptive libraries},
	url = {http://arxiv.org/abs/1806.07060},
	abstract = {Efficient high-performance libraries often expose multiple tunable parameters to provide highly optimized routines. These can range from simple loop unroll factors or vector sizes all the way to algorithmic changes, given that some implementations can be more suitable for certain devices by exploiting hardware characteristics such as local memories and vector units. Traditionally, such parameters and algorithmic choices are tuned and then hard-coded for a specific architecture and for certain characteristics of the inputs. However, emerging applications are often data-driven, thus traditional approaches are not effective across the wide range of inputs and architectures used in practice. In this paper we present a new adaptive framework for data-driven applications which uses a predictive model to select the optimal algorithmic parameters by training with synthetic and real datasets. We demonstrate the effectiveness on a BLAS library and specifically on its matrix multiplication routine. We present experimental results for two GPU architectures, and show significant performance gains of up to 3x (on a high-end NVIDIA Pascal GPU) and 2.5x (on an embedded ARM Mali GPU) when compared to a traditionally optimized library.},
	language = {en},
	urldate = {2019-09-03},
	journal = {arXiv:1806.07060 [cs]},
	author = {Cianfriglia, Marco and Vella, Flavio and Nugteren, Cedric and Lokhmotov, Anton and Fursin, Grigori},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.07060},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Mathematical Software, Computer Science - Performance, Computer Science - Software Engineering},
}

@inproceedings{nugteren_clblast:_2018,
	address = {Oxford, United Kingdom},
	title = {{CLBlast}: {A} {Tuned} {OpenCL} {BLAS} {Library}},
	isbn = {978-1-4503-6439-3},
	shorttitle = {{CLBlast}},
	url = {http://dl.acm.org/citation.cfm?doid=3204919.3204924},
	doi = {10.1145/3204919.3204924},
	abstract = {This work introduces CLBlast, an open-source BLAS library providing optimized OpenCL routines to accelerate dense linear algebra for a wide variety of devices. It is targeted at machine learning and HPC applications and thus provides a fast matrix-multiplication routine (GEMM) to accelerate the core of many applications (e.g. deep learning, iterative solvers, astrophysics, computational ﬂuid dynamics, quantum chemistry). CLBlast has ﬁve main advantages over other OpenCL BLAS libraries: 1) it is optimized for and tested on a large variety of OpenCL devices including less commonly used devices such as embedded and low-power GPUs, 2) it can be explicitly tuned for speciﬁc problem-sizes on speciﬁc hardware platforms, 3) it can perform operations in half-precision ﬂoating-point FP16 saving bandwidth, time and energy, 4) it has an optional CUDA back-end, 5) and it can combine multiple operations in a single batched routine, accelerating smaller problems signiﬁcantly. This paper describes the library and demonstrates the advantages of CLBlast experimentally for diﬀerent use-cases on a wide variety of OpenCL hardware.},
	language = {en},
	urldate = {2019-09-03},
	booktitle = {Proceedings of the {International} {Workshop} on {OpenCL}  - {IWOCL} '18},
	publisher = {ACM Press},
	author = {Nugteren, Cedric},
	year = {2018},
	pages = {1--10},
}

@inproceedings{hu_deep_2018,
	address = {Gothenburg, Sweden},
	title = {Deep code comment generation},
	isbn = {978-1-4503-5714-2},
	url = {http://dl.acm.org/citation.cfm?doid=3196321.3196334},
	doi = {10.1145/3196321.3196334},
	abstract = {During software maintenance, code comments help developers comprehend programs and reduce additional time spent on reading and navigating source code. Unfortunately, these comments are often mismatched, missing or outdated in the software projects. Developers have to infer the functionality from the source code. This paper proposes a new approach named DeepCom to automatically generate code comments for Java methods. The generated comments aim to help developers understand the functionality of Java methods. DeepCom applies Natural Language Processing (NLP) techniques to learn from a large code corpus and generates comments from learned features. We use a deep neural network that analyzes structural information of Java methods for better comments generation. We conduct experiments on a large-scale Java corpus built from 9,714 open source projects from GitHub. We evaluate the experimental results on a machine translation metric. Experimental results demonstrate that our method DeepCom outperforms the state-of-the-art by a substantial margin.},
	language = {en},
	urldate = {2019-09-03},
	booktitle = {Proceedings of the 26th {Conference} on {Program} {Comprehension}  - {ICPC} '18},
	publisher = {ACM Press},
	author = {Hu, Xing and Li, Ge and Xia, Xin and Lo, David and Jin, Zhi},
	year = {2018},
	pages = {200--210},
}

@article{allamanis_learning_2017,
	title = {Learning to {Represent} {Programs} with {Graphs}},
	url = {http://arxiv.org/abs/1711.00740},
	abstract = {Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code’s known sematics. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures.},
	language = {en},
	urldate = {2019-09-03},
	journal = {arXiv:1711.00740 [cs]},
	author = {Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.00740},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@inproceedings{narayanan_deepcache:_2018,
	address = {Budapest, Hungary},
	title = {{DeepCache}: {A} {Deep} {Learning} {Based} {Framework} {For} {Content} {Caching}},
	isbn = {978-1-4503-5911-5},
	shorttitle = {{DeepCache}},
	url = {http://dl.acm.org/citation.cfm?doid=3229543.3229555},
	doi = {10.1145/3229543.3229555},
	abstract = {In this paper, we present D���C���� a novel Framework for content caching, which can signi�cantly boost cache performance. Our Framework is based on powerful deep recurrent neural network models. It comprises of two main components: i) Object Characteristics Predictor, which builds upon deep LSTM Encoder-Decoder model to predict the future characteristics of an object (such as object popularity) – to the best of our knowledge, we are the �rst to propose LSTM Encoder-Decoder model for content caching; ii) a caching policy component, which accounts for predicted information of objects to make smart caching decisions. In our thorough experiments, we show that applying D���C���� Framework to existing cache policies, such as LRU and k-LRU, signi�cantly boosts the number of cache hits.},
	language = {en},
	urldate = {2019-09-03},
	booktitle = {Proceedings of the 2018 {Workshop} on {Network} {Meets} {AI} \& {ML}  - {NetAI}'18},
	publisher = {ACM Press},
	author = {Narayanan, Arvind and Verma, Saurabh and Ramadan, Eman and Babaie, Pariya and Zhang, Zhi-Li},
	year = {2018},
	pages = {48--53},
}

@article{hashemi_learning_2018,
	title = {Learning {Memory} {Access} {Patterns}},
	url = {http://arxiv.org/abs/1803.02329},
	abstract = {The explosion in workload complexity and the recent slow-down in Moore’s law scaling call for new approaches towards efﬁcient computing. Researchers are now beginning to use recent advances in machine learning in software optimizations, augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efﬁcient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we ﬁnd that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the ﬁrst step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research.},
	language = {en},
	urldate = {2019-09-03},
	journal = {arXiv:1803.02329 [cs, stat]},
	author = {Hashemi, Milad and Swersky, Kevin and Smith, Jamie A. and Ayers, Grant and Litz, Heiner and Chang, Jichuan and Kozyrakis, Christos and Ranganathan, Parthasarathy},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.02329},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{zhang_auto-tuning_2018,
	title = {Auto-tuning {Streamed} {Applications} on {Intel} {Xeon} {Phi}},
	doi = {10.1109/IPDPS.2018.00061},
	abstract = {Many-core accelerators, as represented by the XeonPhi coprocessors and GPGPUs, allow software to exploit spatial and temporal sharing of computing resources to improve the overall system performance. To unlock this performance potential requires software to effectively partition the hardware resource to maximize the overlap between host-device communication and accelerator computation, and to match the granularity of task parallelism to the resource partition. However, determining the right resource partition and task parallelism on a per program, per dataset basis is challenging. This is because the number of possible solutions is huge, and the benefit of choosing the right solution may be large, but mistakes can seriously hurt the performance. In this paper, we present an automatic approach to determine the hardware resource partition and the task granularity for any given streamed application, targeting the Intel XeonPhi architecture. Instead of hand-crafting the heuristic for which the process will have to repeat for each hardware generation, we employ machine learning techniques to automatically learn it. We achieve this by first learning a predictive model offline using training programs; we then use the learned model to predict the resource partition and task granularity for any unseen programs at runtime. We apply our approach to 23 representative parallel applications and evaluate it on a CPU-XeonPhi mixed heterogenous many-core platform. Our approach achieves, on average, a 1.6x (upto 5.6x) speedup, which translates to 94.5\% of the performance delivered by a theoretically perfect predictor.},
	booktitle = {2018 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Zhang, P. and Fang, J. and Tang, T. and Yang, C. and Wang, Z.},
	month = may,
	year = {2018},
	keywords = {CPU-XeonPhi mixed heterogenous many-core platform, Computer architecture, GPGPU, Hardware, Heterogeneous computing, Intel XeonPhi architecture, Machine learning, Parallel processing, Parallelism, Performance Tuning, Predictive models, Runtime, Task analysis, XeonPhi coprocessors, accelerator computation, auto-tuning streamed applications, hardware resource partition, host-device communication, many-core accelerators, multiprocessing systems, parallel processing, system performance, task granularity, task parallelism},
	pages = {515--525},
}

@inproceedings{chen_adaptive_2018,
	title = {Adaptive {Optimization} of {Sparse} {Matrix}-{Vector} {Multiplication} on {Emerging} {Many}-{Core} {Architectures}},
	doi = {10.1109/HPCC/SmartCity/DSS.2018.00116},
	abstract = {Sparse matrix vector multiplication (SpMV) is one of the most common operations in scientific and high-performance applications, and is often responsible for the application performance bottleneck. While the sparse matrix representation has a significant impact on the resulting application performance, choosing the right representation typically relies on expert knowledge and trial and error. This paper provides the first comprehensive study on the impact of sparse matrix representations on two emerging many-core architectures: the Intel's Knights Landing (KNL) XeonPhi and the ARM-based FT-2000Plus (FTP). Our large-scale experiments involved over 9,500 distinct profiling runs performed on 956 sparse datasets and five mainstream SpMV representations. We show that the best sparse matrix representation depends on the underlying architecture and the program input. To help developers to choose the optimal matrix representation, we employ machine learning to develop a predictive model. Our model is first trained offline using a set of training examples. The learned model can be used to predict the best matrix representation for any unseen input for a given architecture. We show that our model delivers on average 95\% and 91\% of the best available performance on KNL and FTP respectively, and it achieves this with no runtime profiling overhead.},
	booktitle = {2018 {IEEE} 20th {International} {Conference} on {High} {Performance} {Computing} and {Communications}; {IEEE} 16th {International} {Conference} on {Smart} {City}; {IEEE} 4th {International} {Conference} on {Data} {Science} and {Systems} ({HPCC}/{SmartCity}/{DSS})},
	author = {Chen, S. and Fang, J. and Chen, D. and Xu, C. and Wang, Z.},
	month = jun,
	year = {2018},
	keywords = {ARM-based FT-2000Plus, Arrays, Predictive models, Random access memory, Sparse matrices, Sparse matrix vector multiplication, Performance optimization, Many-Cores, Performance analysis, Two dimensional displays, application performance bottleneck, high-performance applications, learning (artificial intelligence), mainstream SpMV representations, many-core architectures, mathematics computing, matrix multiplication, optimal matrix representation, sparse datasets, sparse matrices, sparse matrix representation, sparse matrix vector multiplication, sparse matrix-vector multiplication, storage management, vectors},
	pages = {649--658},
}

@article{ashouri_survey_2018,
	title = {A {Survey} on {Compiler} {Autotuning} using {Machine} {Learning}},
	volume = {51},
	issn = {03600300},
	url = {http://dl.acm.org/citation.cfm?doid=3271482.3197978},
	doi = {10.1145/3197978},
	language = {en},
	number = {5},
	urldate = {2019-09-03},
	journal = {ACM Computing Surveys},
	author = {Ashouri, Amir H. and Killian, William and Cavazos, John and Palermo, Gianluca and Silvano, Cristina},
	month = sep,
	year = {2018},
	pages = {1--42},
}

@article{allamanis_survey_2018,
	title = {A {Survey} of {Machine} {Learning} for {Big} {Code} and {Naturalness}},
	volume = {51},
	issn = {03600300},
	url = {http://dl.acm.org/citation.cfm?doid=3236632.3212695},
	doi = {10.1145/3212695},
	language = {en},
	number = {4},
	urldate = {2019-09-03},
	journal = {ACM Computing Surveys},
	author = {Allamanis, Miltiadis and Barr, Earl T. and Devanbu, Premkumar and Sutton, Charles},
	month = jul,
	year = {2018},
	pages = {1--37},
}

@article{stuecheli_ibm_2018,
	title = {The {IBM} {POWER9} {Scale} {Up} {Processor}},
	language = {en},
	author = {Stuecheli, Jeff and Starke, William},
	year = {2018},
	pages = {17},
}

@article{fuchs_quantum_2019,
	title = {Quantum {Poker} -- a pedagogical tool to learn quantum computing that is fun to play},
	url = {http://arxiv.org/abs/1908.00044},
	abstract = {Quantum computers are on the verge of becoming a commercially available reality. They represent a paradigm change to the classical computing paradigm, and the learning curve is considerably long. The creation of games is a way to ease the transition for novices. We present a game similar to the poker variant Texas hold ’em with the intention to serve as an engaging pedagogical tool to learn the basics rules of quantum computing. The diﬀerence to the classical variant is that the community cards are replaced by a quantum register that is ”randomly” initialized, and the cards for each player are replaced by quantum gates, randomly drawn from a set of available gates. Each player can create a quantum circuit with their cards, with the aim to maximize the number of 1’s that are measured in the computational basis. The basic concepts of superposition, entanglement and quantum gates are employed. We provide a proof-of-concept implementation using Qiskit [1]. A comparison of the results using a simulator and IBM machines is conducted, showing that error rates on contemporary quantum computers are still very high. Improvements on the error rates and error mitigation techniques are necessary, even for simple circuits, for the success of noisy intermediate scale quantum computers.},
	language = {en},
	urldate = {2019-08-20},
	journal = {arXiv:1908.00044 [physics, physics:quant-ph]},
	author = {Fuchs, Franz G. and Falch, Vemund and Johnsen, Christian},
	month = jul,
	year = {2019},
	note = {arXiv: 1908.00044},
	keywords = {Physics - Popular Physics, Quantum Physics},
}

@inproceedings{morales_earmo:_2018,
	title = {{EARMO}: {An} {Energy}-{Aware} {Refactoring} {Approach} for {Mobile} {Apps}},
	shorttitle = {{EARMO}},
	doi = {10.1145/3180155.3182524},
	abstract = {With millions of smartphones sold every year, the development of mobile apps has grown substantially. The battery power limitation of mobile devices has push developers and researchers to search for methods to improve the energy efficiency of mobile apps. We propose a multiobjective refactoring approach to automatically improve the architecture of mobile apps, while controlling for energy efficiency. In this extended abstract we briefly summarize our work.},
	booktitle = {2018 {IEEE}/{ACM} 40th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Morales, R. and Saborido, R. and Khomh, F. and Chicano, F. and Antoniol, G.},
	month = may,
	year = {2018},
	keywords = {Anti-patterns, Batteries, Energy consumption, Energy efficiency, Energy measurement, Mobile apps, Mobile handsets, Refactoring, Software, Software engineering},
	pages = {59--59},
}

@inproceedings{baek_meta-model_2018,
	title = {A {Meta}-{Model} for {Representing} {System}-of-{Systems} {Ontologies}},
	abstract = {A System-of-Systems (SoS) is a large-scale complex system that integrates multiple constituent systems, which have managerial and operational independence. In order to achieve higher-level common goals of an SoS, it is important to systematically integrate independent constituent systems by thoroughly analyzing and designing the target SoS as a whole. But before conducting these engineering activities, a number of various SoS stakeholders and engineers should be able to understand their SoS. In order to provide a holistic view as a common knowledge base, this paper focuses on developing a conceptual meta-model that represents SoS ontologies. By investigating several documents for Mass Casualty Incident (MCI) response systems, we identified essential objects and required features for SoS descriptions. Based on the investigation, we generalized the objects into SoS entities, and we develop a meta-model, called M2SoS (Meta-model for System-of-Systems). To design our M2SoS, we borrowed organizational concepts from meta-models for multi-agent systems, and entities and relationships are redefined to specify SoS concepts in M2SoS. Finally, M2SoS is analyzed with respect to SoS characteristics, and we evaluate if M2SoS can represent high-level ontologies for two SoS case scenarios.},
	booktitle = {2018 {IEEE}/{ACM} 6th {International} {Workshop} on {Software} {Engineering} for {Systems}-of-{Systems} ({SESoS})},
	author = {Baek, Y. and Song, J. and Shin, Y. and Park, S. and Bae, D.},
	month = may,
	year = {2018},
	keywords = {Cascading style sheets, Computer architecture, Conceptual Model, Meta-model, Model-based Software Engineering, Multi-agent systems, Object recognition, Ontologies, Ontology, Organizations, SoS Engineering, Stakeholders, System-of-Systems},
	pages = {1--7},
}

@inproceedings{magni_automatic_2014,
	address = {New York, NY, USA},
	series = {{PACT} '14},
	title = {Automatic {Optimization} of {Thread}-coarsening for {Graphics} {Processors}},
	isbn = {978-1-4503-2809-8},
	url = {http://doi.acm.org/10.1145/2628071.2628087},
	doi = {10.1145/2628071.2628087},
	abstract = {OpenCL has been designed to achieve functional portability across multi-core devices from different vendors. However, the lack of a single cross-target optimizing compiler severely limits performance portability of OpenCL programs. Programmers need to manually tune applications for each specific device, preventing effective portability. We target a compiler transformation specific for data-parallel languages: thread-coarsening and show it can improve performance across different GPU devices. We then address the problem of selecting the best value for the coarsening factor parameter, i.e., deciding how many threads to merge together. We experimentally show that this is a hard problem to solve: good configurations are difficult to find and naive coarsening in fact leads to substantial slowdowns. We propose a solution based on a machine-learning model that predicts the best coarsening factor using kernel-function static features. The model automatically specializes to the different architectures considered. We evaluate our approach on 17 benchmarks on four devices: two Nvidia GPUs and two different generations of AMD GPUs. Using our technique, we achieve speedups between 1.11X and 1.33X on average.},
	urldate = {2019-06-06},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Parallel} {Architectures} and {Compilation}},
	publisher = {ACM},
	author = {Magni, Alberto and Dubach, Christophe and O'Boyle, Michael},
	year = {2014},
	note = {event-place: Edmonton, AB, Canada},
	keywords = {opencl, optimization},
	pages = {455--466},
}

@inproceedings{grewe_portable_2013,
	address = {Shenzhen},
	title = {Portable mapping of data parallel programs to {OpenCL} for heterogeneous systems},
	isbn = {978-1-4673-5525-4 978-1-4673-5524-7},
	url = {http://ieeexplore.ieee.org/document/6494993/},
	doi = {10.1109/CGO.2013.6494993},
	abstract = {General purpose GPU based systems are highly attractive as they give potentially massive performance at little cost. Realizing such potential is challenging due to the complexity of programming. This paper presents a compiler based approach to automatically generate optimized OpenCL code from data-parallel OpenMP programs for GPUs. Such an approach brings together the beneﬁts of a clear high level language (OpenMP) and an emerging standard (OpenCL) for heterogeneous multi-cores. A key feature of our scheme is that it leverages existing transformations, especially data transformations, to improve performance on GPU architectures and uses predictive modeling to automatically determine if it is worthwhile running the OpenCL code on the GPU or OpenMP code on the multi-core host. We applied our approach to the entire NAS parallel benchmark suite and evaluated it on two distinct GPU based systems: Core i7/NVIDIA GeForce GTX 580 and Core i7/AMD Radeon 7970. We achieved average (up to) speedups of 4.51x and 4.20x (143x and 67x) respectively over a sequential baseline. This is, on average, a factor 1.63 and 1.56 times faster than a hand-coded, GPU-speciﬁc OpenCL implementation developed by independent expert programmers.},
	language = {en},
	urldate = {2019-06-06},
	booktitle = {Proceedings of the 2013 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization} ({CGO})},
	publisher = {IEEE},
	author = {Grewe, D. and {Zheng Wang} and O'Boyle, M. F. P.},
	month = feb,
	year = {2013},
	pages = {1--10},
}

@misc{han_deep_2016,
	title = {Deep {Compression}: {Compressing} {Deep} {Neural} {Network} with {Pruning}, {Trained} {Quantization} and {Huffman} {Coding}},
	shorttitle = {Deep {Compression}},
	url = {/paper/Deep-Compression%3A-Compressing-Deep-Neural-Network-Han-Mao/3d2126066c6244e05ec4d2631262252f4369d9c1},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce “deep compression”, a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35× to 49× without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9× to 13×; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35×, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49× from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3× to 4× layerwise speedup and 3× to 7× better energy efficiency.},
	language = {en},
	urldate = {2019-06-03},
	journal = {undefined},
	author = {Han, Song and Mao, Huizi and Dally, William J.},
	year = {2016},
}

@article{nair_finding_2018,
	title = {Finding {Faster} {Configurations} using {FLASH}},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/8469102/},
	doi = {10.1109/TSE.2018.2870895},
	abstract = {Finding good conﬁgurations of a software system is often challenging since the number of conﬁguration options can be large. Software engineers often make poor choices about conﬁguration or, even worse, they usually use a sub-optimal conﬁguration in production, which leads to inadequate performance. To assist engineers in ﬁnding the better conﬁguration, this article introduces FLASH, a sequential model-based method that sequentially explores the conﬁguration space by reﬂecting on the conﬁgurations evaluated so far to determine the next best conﬁguration to explore. FLASH scales up to software systems that defeat the prior state-of-the-art model-based methods in this area. FLASH runs much faster than existing methods and can solve both single-objective and multi-objective optimization problems. The central insight of this article is to use the prior knowledge of the conﬁguration space (gained from prior runs) to choose the next promising conﬁguration. This strategy reduces the effort (i.e., number of measurements) required to ﬁnd the better conﬁguration. We evaluate FLASH using 30 scenarios based on 7 software systems to demonstrate that FLASH saves effort in 100\% and 80\% of cases in single-objective and multi-objective problems respectively by up to several orders of magnitude compared to state-of-the-art techniques.},
	language = {en},
	urldate = {2019-05-31},
	journal = {IEEE Transactions on Software Engineering},
	author = {Nair, Vivek and Yu, Zhe and Menzies, Tim and Siegmund, Norbert and Apel, Sven},
	year = {2018},
	pages = {1--1},
}

@article{guo_data-efficient_2018,
	title = {Data-efficient performance learning for configurable systems},
	volume = {23},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-017-9573-6},
	doi = {10.1007/s10664-017-9573-6},
	abstract = {Many software systems today are configurable, offering customization of functionality by feature selection. Understanding how performance varies in terms of feature selection is key for selecting appropriate configurations that meet a set of given requirements. Due to a huge configuration space and the possibly high cost of performance measurement, it is usually not feasible to explore the entire configuration space of a configurable system exhaustively. It is thus a major challenge to accurately predict performance based on a small sample of measured system variants. To address this challenge, we propose a data-efficient learning approach, called DECART, that combines several techniques of machine learning and statistics for performance prediction of configurable systems. DECART builds, validates, and determines a prediction model based on an available sample of measured system variants. Empirical results on 10 real-world configurable systems demonstrate the effectiveness and practicality of DECART. In particular, DECART achieves a prediction accuracy of 90\% or higher based on a small sample, whose size is linear in the number of features. In addition, we propose a sample quality metric and introduce a quantitative analysis of the quality of a sample for performance prediction.},
	language = {en},
	number = {3},
	urldate = {2019-05-31},
	journal = {Empirical Software Engineering},
	author = {Guo, Jianmei and Yang, Dingyu and Siegmund, Norbert and Apel, Sven and Sarkar, Atrisha and Valov, Pavel and Czarnecki, Krzysztof and Wasowski, Andrzej and Yu, Huiqun},
	month = jun,
	year = {2018},
	keywords = {Configurable systems, Model selection, Parameter tuning, Performance prediction, Regression},
	pages = {1826--1867},
}

@article{guo_data-efficient_2018-1,
	title = {Data-efficient {Performance} {Learning} for {Configurable} {Systems}},
	volume = {23},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-017-9573-6},
	doi = {10.1007/s10664-017-9573-6},
	abstract = {Many software systems today are configurable, offering customization of functionality by feature selection. Understanding how performance varies in terms of feature selection is key for selecting appropriate configurations that meet a set of given requirements. Due to a huge configuration space and the possibly high cost of performance measurement, it is usually not feasible to explore the entire configuration space of a configurable system exhaustively. It is thus a major challenge to accurately predict performance based on a small sample of measured system variants. To address this challenge, we propose a data-efficient learning approach, called DECART, that combines several techniques of machine learning and statistics for performance prediction of configurable systems. DECART builds, validates, and determines a prediction model based on an available sample of measured system variants. Empirical results on 10 real-world configurable systems demonstrate the effectiveness and practicality of DECART. In particular, DECART achieves a prediction accuracy of 90\% or higher based on a small sample, whose size is linear in the number of features. In addition, we propose a sample quality metric and introduce a quantitative analysis of the quality of a sample for performance prediction.},
	number = {3},
	urldate = {2019-05-31},
	journal = {Empirical Softw. Engg.},
	author = {Guo, Jianmei and Yang, Dingyu and Siegmund, Norbert and Apel, Sven and Sarkar, Atrisha and Valov, Pavel and Czarnecki, Krzysztof and Wasowski, Andrzej and Yu, Huiqun},
	month = jun,
	year = {2018},
	keywords = {Configurable systems, Model selection, Parameter tuning, Performance prediction, Regression},
	pages = {1826--1867},
}

@article{lee_character-level_nodate,
	title = {Character-{Level} {Feature} {Extraction} with {Densely} {Connected} {Networks}},
	abstract = {Generating character-level features is an important step for achieving good results in various natural language processing tasks. To alleviate the need for human labor in generating hand-crafted features, methods that utilize neural architectures such as Convolutional Neural Network (CNN) or Recurrent Neural Network (RNN) to automatically extract such features have been proposed and have shown great results. However, CNN generates position-independent features, and RNN is slow since it needs to process the characters sequentially. In this paper, we propose a novel method of using a densely connected network to automatically extract character-level features. The proposed method does not require any language or task speciﬁc assumptions, and shows robustness and effectiveness while being faster than CNN- or RNN-based methods. Evaluating this method on three sequence labeling tasks - slot tagging, Part-of-Speech (POS) tagging, and Named-Entity Recognition (NER) - we obtain state-of-the-art performance with a 96.62 F1-score and 97.73\% accuracy on slot tagging and POS tagging, respectively, and comparable performance to the state-of-the-art 91.13 F1-score on NER.},
	language = {en},
	author = {Lee, Chanhee and Kim, Young-Bum and Lee, Dongyub and Lim, Heuiseok},
	pages = {12},
}

@article{leclair_neural_2019,
	title = {A {Neural} {Model} for {Generating} {Natural} {Language} {Summaries} of {Program} {Subroutines}},
	url = {http://arxiv.org/abs/1902.01954},
	abstract = {Source code summarization -- creating natural language descriptions of source code behavior -- is a rapidly-growing research topic with applications to automatic documentation generation, program comprehension, and software maintenance. Traditional techniques relied on heuristics and templates built manually by human experts. Recently, data-driven approaches based on neural machine translation have largely overtaken template-based systems. But nearly all of these techniques rely almost entirely on programs having good internal documentation; without clear identifier names, the models fail to create good summaries. In this paper, we present a neural model that combines words from code with code structure from an AST. Unlike previous approaches, our model processes each data source as a separate input, which allows the model to learn code structure independent of the text in code. This process helps our approach provide coherent summaries in many cases even when zero internal documentation is provided. We evaluate our technique with a dataset we created from 2.1m Java methods. We find improvement over two baseline techniques from SE literature and one from NLP literature.},
	urldate = {2019-05-31},
	journal = {arXiv:1902.01954 [cs]},
	author = {LeClair, Alexander and Jiang, Siyuan and McMillan, Collin},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.01954},
	keywords = {Computer Science - Software Engineering},
}

@article{karampatsis_maybe_2019,
	title = {Maybe {Deep} {Neural} {Networks} are the {Best} {Choice} for {Modeling} {Source} {Code}},
	url = {http://arxiv.org/abs/1903.05734},
	abstract = {Statistical language modeling techniques have successfully been applied to source code, yielding a variety of new software development tools, such as tools for code suggestion and improving readability. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. But traditional language models limit the vocabulary to a fixed set of common words. For code, this strong assumption has been shown to have a significant negative effect on predictive performance. But the open vocabulary version of the neural network language models for code have not been introduced in the literature. We present a new open-vocabulary neural language model for code that is not limited to a fixed vocabulary of identifier names. We employ a segmentation into subword units, subsequences of tokens chosen based on a compression criterion, following previous work in machine translation. Our network achieves best in class performance, outperforming even the state-of-the-art methods of Hellendoorn and Devanbu that are designed specifically to model code. Furthermore, we present a simple method for dynamically adapting the model to a new test project, resulting in increased performance. We showcase our methodology on code corpora in three different languages of over a billion tokens each, hundreds of times larger than in previous work. To our knowledge, this is the largest neural language model for code that has been reported.},
	urldate = {2019-05-31},
	journal = {arXiv:1903.05734 [cs]},
	author = {Karampatsis, Rafael-Michael and Sutton, Charles},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.05734},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{chen_improved_2017,
	title = {Improved {Neural} {Machine} {Translation} with a {Syntax}-{Aware} {Encoder} and {Decoder}},
	url = {http://arxiv.org/abs/1707.05436},
	abstract = {Most neural machine translation (NMT) models are based on the sequential encoder-decoder framework, which makes no use of syntactic information. In this paper, we improve this model by explicitly incorporating source-side syntactic trees. More specifically, we propose (1) a bidirectional tree encoder which learns both sequential and tree structured representations; (2) a tree-coverage model that lets the attention depend on the source-side syntax. Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential attentional model as well as a stronger baseline with a bottom-up tree encoder and word coverage.},
	urldate = {2019-05-31},
	journal = {arXiv:1707.05436 [cs]},
	author = {Chen, Huadong and Huang, Shujian and Chiang, David and Chen, Jiajun},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.05436},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{hellendoorn_are_2017,
	address = {Paderborn, Germany},
	title = {Are deep neural networks the best choice for modeling source code?},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106290},
	doi = {10.1145/3106237.3106290},
	abstract = {Current statistical language modeling techniques, including deeplearning based models, have proven to be quite effective for source code. We argue here that the special properties of source code can be exploited for further improvements. In this work, we enhance established language modeling approaches to handle the special challenges of modeling source code, such as: frequent changes, larger, changing vocabularies, deeply nested scopes, etc. We present a fast, nested language modeling toolkit specifically designed for software, with the ability to add \& remove text, and mix \& swap out many models. Specifically, we improve upon prior cache-modeling work and present a model with a much more expansive, multi-level notion of locality that we show to be well-suited for modeling software. We present results on varying corpora in comparison with traditional N -gram, as well as RNN, and LSTM deep-learning language models, and release all our source code for public use. Our evaluations suggest that carefully adapting N -gram models for source code can yield performance that surpasses even RNN and LSTM based deep-learning models.},
	language = {en},
	urldate = {2019-05-31},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Hellendoorn, Vincent J. and Devanbu, Premkumar},
	year = {2017},
	pages = {763--773},
}

@article{hu_deep_2018,
	title = {Deep {Code} {Comment} {Generation}},
	abstract = {During software maintenance, code comments help developers comprehend programs and reduce additional time spent on reading and navigating source code. Unfortunately, these comments are often mismatched, missing or outdated in the software projects. Developers have to infer the functionality from the source code. This paper proposes a new approach named DeepCom to automatically generate code comments for Java methods. The generated comments aim to help developers understand the functionality of Java methods. DeepCom applies Natural Language Processing (NLP) techniques to learn from a large code corpus and generates comments from learned features. We use a deep neural network that analyzes structural information of Java methods for better comments generation. We conduct experiments on a large-scale Java corpus built from 9,714 open source projects from GitHub. We evaluate the experimental results on a machine translation metric. Experimental results demonstrate that our method DeepCom outperforms the state-of-the-art by a substantial margin.},
	language = {en},
	author = {Hu, Xing and Li, Ge and Xia, Xin and Lo, David and Jin, Zhi},
	year = {2018},
	pages = {11},
}

@article{zhang_novel_nodate,
	title = {A {Novel} {Neural} {Source} {Code} {Representation} based on {Abstract} {Syntax} {Tree}},
	abstract = {Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree (AST) based neural models can better represent source code. However, the sizes of ASTs are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel AST-based Neural Network (ASTNN) for source code representation. Unlike existing models that work on entire ASTs, ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional RNN model is used to leverage the naturalness of statements and ﬁnally produce the vector representation of a code fragment. We have applied our neural network based source code representation method to two common program comprehension tasks: source code classiﬁcation and code clone detection. Experimental results on the two tasks indicate that our model is superior to state-of-the-art approaches.},
	language = {en},
	author = {Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Wang, Kaixuan and Liu, Xudong and Lab, SKLSDE},
	pages = {12},
}

@article{bhat_survey_nodate,
	title = {A {Survey} of {Machine} {Learning} and {Deep} {Learning} {Techniques} for {Compiler} {Optimization}},
	volume = {2},
	abstract = {Designing an optimized compiler is not only difficult but also very time-consuming, especially when it has to be done manually. Recent work has proven how the design and construction can be greatly simplified and improved by leveraging machine learning approaches. Most approaches employing machine learning techniques require features of the program to be specified. Selecting the best features is crucial to increase the quality of the heuristics in machine learning and deep learning. However, even having chosen the best possible heuristic may not be enough as training the machine learning model is an expensive and a repetitive task. This paper discusses the different techniques that can be utilized to optimize and further improve the quality of the heuristics chosen and the overall quality of the machine learning and deep learning models, thus improving the efficiency of the compiler. These techniques include addressing various issues including but not limited to the phase-ordering problem, multiple evaluations of a program in an iterative approach and time taken to find the optimal heuristic.},
	language = {en},
	number = {4},
	author = {Bhat, K Manasvi and Anchalia, Pratiksha P and Mohbe, Rushali and Parkavi, A},
	pages = {4},
}

@article{mukhanov_alea:_2017,
	title = {{ALEA}: {A} {Fine}-{Grained} {Energy} {Profiling} {Tool}},
	volume = {14},
	issn = {1544-3566},
	shorttitle = {{ALEA}},
	url = {http://doi.acm.org/10.1145/3050436},
	doi = {10.1145/3050436},
	abstract = {Energy efficiency is becoming increasingly important, yet few developers understand how source code changes affect the energy and power consumption of their programs. To enable them to achieve energy savings, we must associate energy consumption with software structures, especially at the fine-grained level of functions and loops. Most research in the field relies on direct power/energy measurements taken from on-board sensors or performance counters. However, this coarse granularity does not directly provide the needed fine-grained measurements. This article presents ALEA, a novel fine-grained energy profiling tool based on probabilistic analysis for fine-grained energy accounting. ALEA overcomes the limitations of coarse-grained power-sensing instruments to associate energy information effectively with source code at a fine-grained level. We demonstrate and validate that ALEA can perform accurate energy profiling at various granularity levels on two different architectures: Intel Sandy Bridge and ARM big.LITTLE. ALEA achieves a worst-case error of only 2\% for coarse-grained code structures and 6\% for fine-grained ones, with less than 1\% runtime overhead. Our use cases demonstrate that ALEA supports energy optimizations, with energy savings of up to 2.87 times for a latency-critical option pricing workload under a given power budget.},
	number = {1},
	urldate = {2019-05-30},
	journal = {ACM Trans. Archit. Code Optim.},
	author = {Mukhanov, Lev and Petoumenos, Pavlos and Wang, Zheng and Parasyris, Nikos and Nikolopoulos, Dimitrios S. and De Supinski, Bronis R. and Leather, Hugh},
	month = mar,
	year = {2017},
	keywords = {ALEA, Energy profiling, energy efficiency, power measurement, sampling},
	pages = {1:1--1:25},
}

@article{markovtsev_public_2018,
	title = {Public {Git} {Archive}: a {Big} {Code} dataset for all},
	shorttitle = {Public {Git} {Archive}},
	url = {http://arxiv.org/abs/1803.10144},
	doi = {10.1145/3196398.3196464},
	abstract = {The number of open source software projects has been growing exponentially. The major online software repository host, GitHub, has accumulated tens of millions of publicly available Git version-controlled repositories. Although the research potential enabled by the available open source code is clearly substantial, no significant large-scale open source code datasets exist. In this paper, we present the Public Git Archive -- dataset of 182,014 top-bookmarked Git repositories from GitHub. We describe the novel data retrieval pipeline to reproduce it. We also elaborate on the strategy for performing dataset updates and legal issues. The Public Git Archive occupies 3.0 TB on disk and is an order of magnitude larger than the current source code datasets. The dataset is made available through HTTP and provides the source code of the projects, the related metadata, and development history. The data retrieval pipeline employs an optimized worker queue model and an optimized archive format to efficiently store forked Git repositories, reducing the amount of data to download and persist. Public Git Archive aims to open a myriad of new opportunities for ``Big Code`` research.},
	urldate = {2019-05-23},
	journal = {Proceedings of the 15th International Conference on Mining Software Repositories  - MSR '18},
	author = {Markovtsev, Vadim and Long, Waren},
	year = {2018},
	note = {arXiv: 1803.10144},
	keywords = {Computer Science - Software Engineering},
	pages = {34--37},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2019-05-22},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{devlin_bert:_2018,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4\% (7.6\% absolute improvement), MultiNLI accuracy to 86.7 (5.6\% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5\% absolute improvement), outperforming human performance by 2.0\%.},
	urldate = {2019-05-22},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
}

@article{radford_language_nodate,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	pages = {24},
}

@inproceedings{leather_raced_2009,
	address = {New York, NY, USA},
	series = {{LCTES} '09},
	title = {Raced {Profiles}: {Efficient} {Selection} of {Competing} {Compiler} {Optimizations}},
	isbn = {978-1-60558-356-3},
	shorttitle = {Raced {Profiles}},
	url = {http://doi.acm.org/10.1145/1542452.1542460},
	doi = {10.1145/1542452.1542460},
	abstract = {Many problems in embedded compilation require one set of optimizations to be selected over another based on run time performance. Self-tuned libraries, iterative compilation and machine learning techniques all compare multiple compiled program versions. In each, program versions are timed to determine which has the best performance. The program needs to be run multiple times for each version because there is noise inherent in most performance measurements. The number of runs must be enough to compare different versions, despite the noise, but executing more than this will waste time and energy. The compiler writer must either risk taking too few runs, potentially getting incorrect results, or taking too many runs increasing the time for their experiments or reducing the number of program versions evaluated. Prior works choose constant size sampling plans where each compiled version is executed a fixed number of times without regard to the level of noise. In this paper we develop a sequential sampling plan which can automatically adapt to the experiment so that the compiler writer can have both confidence in the results and also be sure that no more runs were taken than were needed. We show that our system is able to correctly determine the best optimization settings with between 76\% and 87\% fewer runs than needed by a brute force, constant sampling size approach. We also compare our approach to JavaSTATS(10); we needed 77\% to 89\% fewer runs than it needed.},
	urldate = {2019-05-21},
	booktitle = {Proceedings of the 2009 {ACM} {SIGPLAN}/{SIGBED} {Conference} on {Languages}, {Compilers}, and {Tools} for {Embedded} {Systems}},
	publisher = {ACM},
	author = {Leather, Hugh and O'Boyle, Michael and Worton, Bruce},
	year = {2009},
	note = {event-place: Dublin, Ireland},
	keywords = {iterative compilation, statistics},
	pages = {50--59},
}

@inproceedings{nugteren_cltune:_2015,
	address = {Turin, Italy},
	title = {{CLTune}: {A} {Generic} {Auto}-{Tuner} for {OpenCL} {Kernels}},
	isbn = {978-1-4799-8670-5},
	shorttitle = {{CLTune}},
	url = {http://ieeexplore.ieee.org/document/7328205/},
	doi = {10.1109/MCSoC.2015.10},
	abstract = {This work presents CLTune, an auto-tuner for OpenCL kernels. It evaluates and tunes kernel performance of a generic, user-deﬁned search space of possible parametervalue combinations. Example parameters include the OpenCL workgroup size, vector data-types, tile sizes, and loop unrolling factors. CLTune can be used in the following scenarios: 1) when there are too many tunable parameters to explore manually, 2) when performance portability across OpenCL devices is desired, or 3) when the optimal parameters change based on input argument values (e.g. matrix dimensions). The auto-tuner is generic, easy to use, open-source, and supports multiple search strategies including simulated annealing and particle swarm optimisation. CLTune is evaluated on two GPU case-studies inspired by the recent successes in deep learning: 2D convolution and matrixmultiplication (GEMM). For 2D convolution, we demonstrate the need for auto-tuning by optimizing for different ﬁlter sizes, achieving performance on-par or better than the state-of-the-art. For matrix-multiplication, we use CLTune to explore a parameter space of more than two-hundred thousand conﬁgurations, we show the need for device-speciﬁc tuning, and outperform the clBLAS library on NVIDIA, AMD and Intel GPUs.},
	language = {en},
	urldate = {2019-05-21},
	booktitle = {2015 {IEEE} 9th {International} {Symposium} on {Embedded} {Multicore}/{Many}-core {Systems}-on-{Chip}},
	publisher = {IEEE},
	author = {Nugteren, Cedric and Codreanu, Valeriu},
	month = sep,
	year = {2015},
	pages = {195--202},
}

@article{cummins_autotuning_2015,
	title = {Autotuning {OpenCL} {Workgroup} {Size} for {Stencil} {Patterns}},
	url = {http://arxiv.org/abs/1511.02490},
	abstract = {Selecting an appropriate workgroup size is critical for the performance of OpenCL kernels, and requires knowledge of the underlying hardware, the data being operated on, and the implementation of the kernel. This makes portable performance of OpenCL programs a challenging goal, since simple heuristics and statically chosen values fail to exploit the available performance. To address this, we propose the use of machine learning-enabled autotuning to automatically predict workgroup sizes for stencil patterns on CPUs and multi-GPUs. We present three methodologies for predicting workgroup sizes. The first, using classifiers to select the optimal workgroup size. The second and third proposed methodologies employ the novel use of regressors for performing classification by predicting the runtime of kernels and the relative performance of different workgroup sizes, respectively. We evaluate the effectiveness of each technique in an empirical study of 429 combinations of architecture, kernel, and dataset, comparing an average of 629 different workgroup sizes for each. We find that autotuning provides a median 3.79x speedup over the best possible fixed workgroup size, achieving 94\% of the maximum performance.},
	urldate = {2019-05-21},
	journal = {arXiv:1511.02490 [cs]},
	author = {Cummins, Chris and Petoumenos, Pavlos and Steuwer, Michel and Leather, Hugh},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.02490},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@article{stephenson_meta_nodate,
	title = {Meta {Optimization}: {Improving} {Compiler} {Heuristics} with {Machine} {Learning}},
	abstract = {Compiler writers have crafted many heuristics over the years to approximately solve NP-hard problems eﬃciently. Finding a heuristic that performs well on a broad range of applications is a tedious and diﬃcult process. This paper introduces Meta Optimization, a methodology for automatically ﬁne-tuning compiler heuristics. Meta Optimization uses machine-learning techniques to automatically search the space of compiler heuristics. Our techniques reduce compiler design complexity by relieving compiler writers of the tedium of heuristic tuning. Our machine-learning system uses an evolutionary algorithm to automatically ﬁnd eﬀective compiler heuristics. We present promising experimental results. In one mode of operation Meta Optimization creates application-speciﬁc heuristics which often result in impressive speedups. For hyperblock formation, one optimization we present in this paper, we obtain an average speedup of 23\% (up to 73\%) for the applications in our suite. Furthermore, by evolving a compiler’s heuristic over several benchmarks, we can create eﬀective, general-purpose heuristics. The best general-purpose heuristic our system found for hyperblock formation improved performance by an average of 25\% on our training set, and 9\% on a completely unrelated test set. We demonstrate the eﬃcacy of our techniques on three diﬀerent optimizations in this paper: hyperblock formation, register allocation, and data prefetching.},
	language = {en},
	author = {Stephenson, Mark and Amarasinghe, Saman and Martin, Martin and O’Reilly, Una-May},
	pages = {14},
}

@article{falch_machine_2017,
	title = {Machine learning-based auto-tuning for enhanced performance portability of {OpenCL} applications},
	volume = {29},
	copyright = {Copyright © 2016 John Wiley \& Sons, Ltd.},
	issn = {1532-0634},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4029},
	doi = {10.1002/cpe.4029},
	abstract = {Heterogeneous computing, combining devices with different architectures such as CPUs and GPUs, is rising in popularity and promises increased performance combined with reduced energy consumption. OpenCL has been proposed as a standard for programming such systems and offers functional portability. However, it suffers from poor performance portability, because applications must be retuned for every new device. In this paper, we use machine learning-based auto-tuning to address this problem. Benchmarks are run on a random subset of the tuning parameter spaces, and the results are used to build a machine learning-based performance model. The model can then be used to find interesting subspaces for further search. We evaluate our method using five image processing benchmarks, with tuning parameter space sizes up to 2.3 M, using different input sizes, on several devices, including an Intel i7 4771 (Haswell) CPU, an Nvidia Tesla K40 GPU, and an AMD Radeon HD 7970 GPU. We compare different machine learning algorithms for the performance model. Our model achieves a mean relative error as low as 3.8\% and is able to find solutions on average only 0.29\% slower than the best configuration in some cases, evaluating less than 1.1\% of the search space. The source code of our framework is available at https://github.com/acelster/ML-autotuning.},
	language = {en},
	number = {8},
	urldate = {2019-05-19},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Falch, Thomas L. and Elster, Anne C.},
	year = {2017},
	keywords = {OpenCL, artificial neural networks, auto-tuning, heterogeneous computing, machine learning, performance portability},
	pages = {e4029},
}

@incollection{naono_efficient_2011,
	address = {New York, NY},
	title = {Efficient {Program} {Compilation} {Through} {Machine} {Learning} {Techniques}},
	isbn = {978-1-4419-6934-7 978-1-4419-6935-4},
	url = {http://link.springer.com/10.1007/978-1-4419-6935-4_19},
	abstract = {The wealth of available compiler optimizations leads to the dual problems of ﬁnding the best set of optimizations and the best heuristic parameters to tune each optimization. We describe how machine learning techniques, such as logistic regression, can be used to address these problems. We focus on decreasing the compile time for a static commercial compiler, while preserving the execution time. We show that we can speed up the compile process by at least a factor of two with almost the same generated code quality on the SPEC2000 benchmark suite, and that our logistic classiﬁer achieves the same prediction quality for non-SPEC benchmarks.},
	language = {en},
	urldate = {2019-05-19},
	booktitle = {Software {Automatic} {Tuning}},
	publisher = {Springer New York},
	author = {Pekhimenko, Gennady and Brown, Angela Demke},
	editor = {Naono, Ken and Teranishi, Keita and Cavazos, John and Suda, Reiji},
	year = {2011},
	doi = {10.1007/978-1-4419-6935-4_19},
	pages = {335--351},
}

@inproceedings{shen_rethinking_2018,
	title = {Rethinking compilers in the rise of machine learning and {AI} (keynote)},
	isbn = {978-1-4503-5644-2},
	url = {http://dl.acm.org/citation.cfm?id=3178372.3183634},
	doi = {10.1145/3178372.3183634},
	urldate = {2019-05-19},
	publisher = {ACM},
	author = {Shen, Xipeng},
	month = feb,
	year = {2018},
	pages = {1--1},
}

@inproceedings{han_use_2017,
	title = {Use of {Synthetic} {Benchmarks} for {Machine}-{Learning}-{Based} {Performance} {Auto}-{Tuning}},
	doi = {10.1109/IPDPSW.2017.108},
	abstract = {We explore the use of synthetic benchmarks for the training phase of machine-learning-based automatic performance tuning. We focus on the problem of predicting if the use of local memory on a GPU is beneficial for caching a single target array in a GPU kernel. We show that the use of only 13 real benchmarks leads to poor prediction accuracy (about to 58\%) of the 13 leave-one-out models trained using these benchmarks, even when the model features are sufficiently comprehensive. We define a metric, called the average vicinity density to measure the quality of a training set. We then use it to demonstrate that the poor accuracy of the models built with the real benchmarks is indeed because of the limited size and coverage of the training set. In contrast, the use of 90K properly generated set of synthetic benchmarks leads to significantly better accuracies, up to 87\%. These results validate our approach of using synthetic benchmarks for training machine learning models. We describe a synthetic benchmark template for the local memory optimization. We then present two approaches to using this template and a seed set of real benchmarks to generate a large number of synthetic benchmark. We also explore the impact of the number of synthetic benchmarks used in training.},
	booktitle = {2017 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	author = {Han, T. D. and Abdelrahman, T. S.},
	month = may,
	year = {2017},
	keywords = {Arrays, Benchmark testing, GPU kernel, Graphics processing units, Kernel, Optimization, Predictive models, Training, graphics processing units, learning (artificial intelligence), local memory optimization, machine-learning-based automatic performance tuning, optimisation, synthetic benchmarks},
	pages = {1350--1361},
}

@inproceedings{taylor_adaptive_2017,
	address = {New York, NY, USA},
	series = {{LCTES} 2017},
	title = {Adaptive {Optimization} for {OpenCL} {Programs} on {Embedded} {Heterogeneous} {Systems}},
	isbn = {978-1-4503-5030-3},
	url = {http://doi.acm.org/10.1145/3078633.3081040},
	doi = {10.1145/3078633.3081040},
	abstract = {Heterogeneous multi-core architectures consisting of CPUs and GPUs are commonplace in today’s embedded systems. These architectures offer potential for energy efficient computing if the application task is mapped to the right core. Realizing such potential is challenging due to the complex and evolving nature of hardware and applications. This paper presents an automatic approach to map OpenCL kernels onto heterogeneous multi-cores for a given optimization criterion – whether it is faster runtime, lower energy consumption or a trade-off between them. This is achieved by developing a machine learning based approach to predict which processor to use to run the OpenCL kernel and the host program, and at what frequency the processor should operate. Instead of hand-tuning a model for each optimization metric, we use machine learning to develop a unified framework that first automatically learns the optimization heuristic for each metric off-line, then uses the learned knowledge to schedule OpenCL kernels at runtime based on code and runtime information of the program. We apply our approach to a set of representative OpenCL benchmarks and evaluate it on an ARM big.LITTLE mobile platform. Our approach achieves over 93\% of the performance delivered by a perfect predictor.We obtain, on average, 1.2x, 1.6x, and 1.8x improvement respectively for runtime, energy consumption and the energy delay product when compared to a comparative heterogeneous-aware OpenCL task mapping scheme.},
	urldate = {2019-05-19},
	booktitle = {Proceedings of the 18th {ACM} {SIGPLAN}/{SIGBED} {Conference} on {Languages}, {Compilers}, and {Tools} for {Embedded} {Systems}},
	publisher = {ACM},
	author = {Taylor, Ben and Marco, Vicent Sanz and Wang, Zheng},
	year = {2017},
	note = {event-place: Barcelona, Spain},
	keywords = {Energy Efficiency, Heterogeneous Multi-cores, OpenCL, Predictive Modeling},
	pages = {11--20},
}

@inproceedings{rodrigues_helping_2016,
	title = {Helping {HPC} {Users} {Specify} {Job} {Memory} {Requirements} via {Machine} {Learning}},
	doi = {10.1109/HUST.2016.006},
	abstract = {Resource allocation in High Performance Computing (HPC) settings is still not easy for end-users due to the wide variety of application and environment configuration options. Users have difficulties to estimate the number of processors and amount of memory required by their jobs, select the queue and partition, and estimate when job output will be available to plan for next experiments. Apart from wasting infrastructure resources by making wrong allocation decisions, overall user response time can also be negatively impacted. Techniques that exploit batch scheduler systems to predict waiting time and runtime of user jobs have already been proposed. However, we observed that such techniques are not suitable for predicting job memory usage. In this paper we introduce a tool to help users predict their memory requirements using machine learning. We describe the integration of the tool with a batch scheduler system, discuss how batch scheduler log data can be exploited to generate memory usage predictions through machine learning, and present results of two production systems containing thousands of jobs.},
	booktitle = {2016 {Third} {International} {Workshop} on {HPC} {User} {Support} {Tools} ({HUST})},
	author = {Rodrigues, E. R. and Cunha, R. L. F. and Netto, M. A. S. and Spriggs, M.},
	month = nov,
	year = {2016},
	keywords = {Data models, Databases, HPC, Load modeling, Memory management, Predictive models, Program processors, Training, batch scheduler system, formal specification, formal verification, high performance computing, job memory requirements specification, learning (artificial intelligence), machine learning, parallel processing, resource allocation, scheduling, software tools, systems analysis, tool integration},
	pages = {6--13},
}

@inproceedings{pellegrini_automatic_2010,
	address = {New York, NY, USA},
	series = {{CF} '10},
	title = {Automatic {Tuning} of {MPI} {Runtime} {Parameter} {Settings} by {Using} {Machine} {Learning}},
	isbn = {978-1-4503-0044-5},
	url = {http://doi.acm.org/10.1145/1787275.1787310},
	doi = {10.1145/1787275.1787310},
	abstract = {MPI implementations provide several hundred runtime parameters that can be tuned for performance improvement. The ideal parameter setting does not only depend on the target multiprocessor architecture but also on the application, its problem and communicator size. This paper presents ATune, an automatic performance tuning tool that uses machine learning techniques to determine the program-specific optimal settings for a subset of the Open MPI's runtime parameters. ATune learns the behaviour of a target system by means of a training phase where several MPI benchmarks and MPI applications are run on a target architecture for varying problem and communicator sizes. For new input programs, only one run is required in order for ATune to deliver a prediction of the optimal runtime parameters values. Experiments based on the NAS Parallel Benchmarks performed on a cluster of SMP machines are shown that demonstrate the effectiveness of ATune. For these experiments, ATune derives MPI runtime parameter settings that are on average within 4\% of the maximum performance achievable on the target system resulting in a performance gain of up to 18\% with respect to the default parameter setting.},
	urldate = {2019-05-19},
	booktitle = {Proceedings of the 7th {ACM} {International} {Conference} on {Computing} {Frontiers}},
	publisher = {ACM},
	author = {Pellegrini, Simone and Fahringer, Thomas and Jordan, Herbert and Moritsch, Hans},
	year = {2010},
	note = {event-place: Bertinoro, Italy},
	keywords = {machine learning, mpi runtime parameters, tuning},
	pages = {115--116},
}

@inproceedings{falch_register_2014,
	title = {Register {Caching} for {Stencil} {Computations} on {GPUs}},
	doi = {10.1109/SYNASC.2014.70},
	abstract = {For most applications, taking full advantage of the memory system is key to achieving good performance on GPUs. In this paper, we introduce register caching, a novel idea where registers of multiple threads are combined and used as a shared, last level, manually managed cache for the contributing threads. This method is enabled by the shuffle instruction recently introduced in Nvidia's Kepler GPU architecture, which allows threads in the same warp to exchange data directly, previously only possible by going through shared memory. We evaluate our proposal with a stencil computation benchmark, achieving speedups of up to 2.04, compared to using shared memory on a GTX680 GPU. Stencil computations form the core of many scientific applications, which can therefore benefit from our proposal. Furthermore, our method is not limited to stencil computations, but is applicable to any application with a predictable memory access pattern suitable for manual caching.},
	booktitle = {2014 16th {International} {Symposium} on {Symbolic} and {Numeric} {Algorithms} for {Scientific} {Computing}},
	author = {Falch, T. L. and Elster, A. C.},
	month = sep,
	year = {2014},
	keywords = {Benchmark testing, CUDA, Caching, Computer architecture, GPU, GPU Computing, GTX680 GPU, Graphics processing units, Indexes, Instruction sets, Manuals, Nvidia Kepler GPU architecture, Register Caching, Registers, Stencil Computations, cache storage, data exchange, graphics processing unit, graphics processing units, register caching, shared memory system, shared memory systems, shuffle instruction, stencil computation},
	pages = {479--486},
}

@inproceedings{ganapathi_case_2009,
	address = {Berkeley, CA, USA},
	series = {{HotPar}'09},
	title = {A {Case} for {Machine} {Learning} to {Optimize} {Multicore} {Performance}},
	url = {http://dl.acm.org/citation.cfm?id=1855591.1855592},
	abstract = {Multicore architectures have become so complex and diverse that there is no obvious path to achieving good performance. Hundreds of code transformations, compiler flags, architectural features and optimization parameters result in a search space that can take many machinemonths to explore exhaustively. Inspired by successes in the systems community, we apply state-of-the-art machine learning techniques to explore this space more intelligently. On 7-point and 27-point stencil code, our technique takes about two hours to discover a configuration whose performance is within 1\% of and up to 18\% better than that achieved by a human expert. This factor of 2000 speedup over manual exploration of the auto-tuning parameter space enables us to explore optimizations that were previously off-limits. We believe the opportunity for using machine learning in multicore autotuning is even more promising than the successes to date in the systems literature.},
	urldate = {2019-05-19},
	booktitle = {Proceedings of the {First} {USENIX} {Conference} on {Hot} {Topics} in {Parallelism}},
	publisher = {USENIX Association},
	author = {Ganapathi, Archana and Datta, Kaushik and Fox, Armando and Patterson, David},
	year = {2009},
	note = {event-place: Berkeley, California},
	pages = {1--1},
}

@inproceedings{cummins_synthesizing_2017,
	address = {Austin, TX, USA},
	title = {Synthesizing benchmarks for predictive modeling},
	isbn = {978-1-5090-4931-8},
	url = {http://ieeexplore.ieee.org/document/7863731/},
	doi = {10.1109/CGO.2017.7863731},
	abstract = {Predictive modeling using machine learning is an effective method for building compiler heuristics, but there is a shortage of benchmarks. Typical machine learning experiments outside of the compilation ﬁeld train over thousands or millions of examples. In machine learning for compilers, however, there are typically only a few dozen common benchmarks available. This limits the quality of learned models, as they have very sparse training data for what are often high-dimensional feature spaces. What is needed is a way to generate an unbounded number of training programs that ﬁnely cover the feature space. At the same time the generated programs must be similar to the types of programs that human developers actually write, otherwise the learning will target the wrong parts of the feature space.},
	language = {en},
	urldate = {2019-05-18},
	booktitle = {2017 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization} ({CGO})},
	publisher = {IEEE},
	author = {Cummins, Chris and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},
	month = feb,
	year = {2017},
	pages = {86--99},
}

@inproceedings{girgin_feature_2008,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Feature {Discovery} in {Reinforcement} {Learning} {Using} {Genetic} {Programming}},
	isbn = {978-3-540-78671-9},
	abstract = {The goal of reinforcement learning is to find a policy that maximizes the expected reward accumulated by an agent over time based on its interactions with the environment; to this end, a function of the state of the agent has to be learned. It is often the case that states are better characterized by a set of features. However, finding a “good” set of features is generally a tedious task which requires a good domain knowledge. In this paper, we propose a genetic programming based approach for feature discovery in reinforcement learning. A population of individuals, each representing a set of features, is evolved, and individuals are evaluated by their average performance on short reinforcement learning trials. The results of experiments conducted on several benchmark problems demonstrate that the resulting features allow the agent to learn better policies in a reduced amount of episodes.},
	language = {en},
	booktitle = {Genetic {Programming}},
	publisher = {Springer Berlin Heidelberg},
	author = {Girgin, Sertan and Preux, Philippe},
	editor = {O’Neill, Michael and Vanneschi, Leonardo and Gustafson, Steven and Esparcia Alcázar, Anna Isabel and De Falco, Ivanoe and Della Cioppa, Antonio and Tarantino, Ernesto},
	year = {2008},
	keywords = {Feature Discovery, Genetic Programming, Policy Iteration, Reinforcement Learning, Reinforcement Learning Algorithm},
	pages = {218--229},
}

@inproceedings{leather_automatic_2009,
	address = {Washington, DC, USA},
	series = {{CGO} '09},
	title = {Automatic {Feature} {Generation} for {Machine} {Learning} {Based} {Optimizing} {Compilation}},
	isbn = {978-0-7695-3576-0},
	url = {http://dx.doi.org/10.1109/CGO.2009.21},
	doi = {10.1109/CGO.2009.21},
	abstract = {Recent work has shown that machine learning can automate and in some cases outperform hand crafted compiler optimizations. Central to such an approach is that machine learning techniques typically rely upon summaries or features of the program. The quality of these features is critical to the accuracy of the resulting machine learned algorithm; no machine learning method will work well with poorly chosen features. However, due to the size and complexity of programs, theoretically there are an infinite number of potential features to choose from. The compiler writer now has to expend effort in choosing the best features from this space. This paper develops a novel mechanism to automatically find those features which most improve the quality of the machine learned heuristic. The feature space is described by a grammar and is then searched with genetic programming and predictive modeling. We apply this technique to loop unrolling in GCC 4.3.1 and evaluate our approach on a Pentium 6. On a benchmark suite of 57 programs, GCC's hard-coded heuristic achieves only 3\% of the maximum performance available, while a state of the art machine learning approach with hand-coded features obtains 59\%. Our feature generation technique is able to achieve 76\% of the maximum available speedup, outperforming existing approaches.},
	urldate = {2019-05-18},
	booktitle = {Proceedings of the 7th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization}},
	publisher = {IEEE Computer Society},
	author = {Leather, Hugh and Bonilla, Edwin and O'Boyle, Michael},
	year = {2009},
	pages = {81--91},
}

@article{ashouri_survey_2018-1,
	title = {A {Survey} on {Compiler} {Autotuning} using {Machine} {Learning}},
	volume = {51},
	issn = {03600300},
	url = {http://arxiv.org/abs/1801.04405},
	doi = {10.1145/3197978},
	abstract = {Since the mid-1990s, researchers have been trying to use machine-learning based approaches to solve a number of different compiler optimization problems. These techniques primarily enhance the quality of the obtained results and, more importantly, make it feasible to tackle two main compiler optimization problems: optimization selection (choosing which optimizations to apply) and phase-ordering (choosing the order of applying optimizations). The compiler optimization space continues to grow due to the advancement of applications, increasing number of compiler optimizations, and new target architectures. Generic optimization passes in compilers cannot fully leverage newly introduced optimizations and, therefore, cannot keep up with the pace of increasing options. This survey summarizes and classifies the recent advances in using machine learning for the compiler optimization field, particularly on the two major problems of (1) selecting the best optimizations and (2) the phase-ordering of optimizations. The survey highlights the approaches taken so far, the obtained results, the fine-grain classification among different approaches and finally, the influential papers of the field.},
	number = {5},
	urldate = {2019-05-18},
	journal = {ACM Computing Surveys},
	author = {Ashouri, Amir H. and Killian, William and Cavazos, John and Palermo, Gianluca and Silvano, Cristina},
	month = sep,
	year = {2018},
	note = {arXiv: 1801.04405},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
	pages = {1--42},
}

@article{levecque_work_2017,
	title = {Work organization and mental health problems in {PhD} students},
	volume = {46},
	issn = {0048-7333},
	url = {http://www.sciencedirect.com/science/article/pii/S0048733317300422},
	doi = {https://doi.org/10.1016/j.respol.2017.02.008},
	abstract = {Research policy observers are increasingly concerned about the potential impact of current academic working conditions on mental health, particularly in PhD students. The aim of the current study is threefold. First, we assess the prevalence of mental health problems in a representative sample of PhD students in Flanders, Belgium (N=3659). Second, we compare PhD students to three other samples: (1) highly educated in the general population (N=769); (2) highly educated employees (N=592); and (3) higher education students (N=333). Third, we assess those organizational factors relating to the role of PhD students that predict mental health status. Results based on 12 mental health symptoms (GHQ-12) showed that 32\% of PhD students are at risk of having or developing a common psychiatric disorder, especially depression. This estimate was significantly higher than those obtained in the comparison groups. Organizational policies were significantly associated with the prevalence of mental health problems. Especially work-family interface, job demands and job control, the supervisor’s leadership style, team decision-making culture, and perception of a career outside academia are linked to mental health problems.},
	number = {4},
	journal = {Research Policy},
	author = {Levecque, Katia and Anseel, Frederik and Beuckelaer, Alain De and Heyden, Johan Van der and Gisle, Lydia},
	year = {2017},
	keywords = {GHQ-12, Mental health, PhD students, Psychosocial working conditions, Work organization},
	pages = {868 -- 879},
}

@article{parashar_scnn:_2017,
	title = {{SCNN}: {An} {Accelerator} for {Compressed}-sparse {Convolutional} {Neural} {Networks}},
	shorttitle = {{SCNN}},
	url = {http://arxiv.org/abs/1708.04485},
	abstract = {Convolutional Neural Networks (CNNs) have emerged as a fundamental technology for machine learning. High performance and extreme energy efficiency are critical for deployments of CNNs in a wide range of situations, especially mobile platforms such as autonomous vehicles, cameras, and electronic personal assistants. This paper introduces the Sparse CNN (SCNN) accelerator architecture, which improves performance and energy efficiency by exploiting the zero-valued weights that stem from network pruning during training and zero-valued activations that arise from the common ReLU operator applied during inference. Specifically, SCNN employs a novel dataflow that enables maintaining the sparse weights and activations in a compressed encoding, which eliminates unnecessary data transfers and reduces storage requirements. Furthermore, the SCNN dataflow facilitates efficient delivery of those weights and activations to the multiplier array, where they are extensively reused. In addition, the accumulation of multiplication products are performed in a novel accumulator array. Our results show that on contemporary neural networks, SCNN can improve both performance and energy by a factor of 2.7x and 2.3x, respectively, over a comparably provisioned dense CNN accelerator.},
	urldate = {2019-05-05},
	journal = {arXiv:1708.04485 [cs]},
	author = {Parashar, Angshuman and Rhu, Minsoo and Mukkara, Anurag and Puglielli, Antonio and Venkatesan, Rangharajan and Khailany, Brucek and Emer, Joel and Keckler, Stephen W. and Dally, William J.},
	month = may,
	year = {2017},
	note = {arXiv: 1708.04485},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{jouppi_-datacenter_2017,
	address = {New York, NY, USA},
	series = {{ISCA} '17},
	title = {In-{Datacenter} {Performance} {Analysis} of a {Tensor} {Processing} {Unit}},
	isbn = {978-1-4503-4892-8},
	url = {http://doi.acm.org/10.1145/3079856.3080246},
	doi = {10.1145/3079856.3080246},
	abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
	urldate = {2019-05-05},
	booktitle = {Proceedings of the 44th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {ACM},
	author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
	year = {2017},
	note = {event-place: Toronto, ON, Canada},
	keywords = {CNN, DNN, GPU, LSTM, MLP, RNN, TPU, TensorFlow, accelerator, deep learning, domain-specific architecture, neural network},
	pages = {1--12},
}

@inproceedings{chen_eyeriss:_2016,
	title = {Eyeriss: {A} {Spatial} {Architecture} for {Energy}-{Efficient} {Dataflow} for {Convolutional} {Neural} {Networks}},
	shorttitle = {Eyeriss},
	doi = {10.1109/ISCA.2016.40},
	abstract = {Deep convolutional neural networks (CNNs) are widely used in modern AI systems for their superior accuracy but at the cost of high computational complexity. The complexity comes from the need to simultaneously process hundreds of filters and channels in the high-dimensional convolutions, which involve a significant amount of data movement. Although highly-parallel compute paradigms, such as SIMD/SIMT, effectively address the computation requirement to achieve high throughput, energy consumption still remains high as data movement can be more expensive than computation. Accordingly, finding a dataflow that supports parallel processing with minimal data movement cost is crucial to achieving energy-efficient CNN processing without compromising accuracy. In this paper, we present a novel dataflow, called row-stationary (RS), that minimizes data movement energy consumption on a spatial architecture. This is realized by exploiting local data reuse of filter weights and feature map pixels, i.e., activations, in the high-dimensional convolutions, and minimizing data movement of partial sum accumulations. Unlike dataflows used in existing designs, which only reduce certain types of data movement, the proposed RS dataflow can adapt to different CNN shape configurations and reduces all types of data movement through maximally utilizing the processing engine (PE) local storage, direct inter-PE communication and spatial parallelism. To evaluate the energy efficiency of the different dataflows, we propose an analysis framework that compares energy cost under the same hardware area and processing parallelism constraints. Experiments using the CNN configurations of AlexNet show that the proposed RS dataflow is more energy efficient than existing dataflows in both convolutional (1.4× to 2.5×) and fully-connected layers (at least 1.3× for batch size larger than 16). The RS dataflow has also been demonstrated on a fabricated chip, which verifies our energy analysis.},
	booktitle = {2016 {ACM}/{IEEE} 43rd {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	author = {Chen, Y. and Emer, J. and Sze, V.},
	month = jun,
	year = {2016},
	keywords = {AlexNet CNN configurations, Arrays, Convolutional Neural Networks, Dataflow, Energy Efficiency, Eyeriss, PE local storage, Parallel processing, RS dataflow, Radio frequency, Random access memory, Shape, Spatial Architecture, Throughput, computational complexity, computer architecture, convolution, data flow computing, data movement energy consumption minimization, deep CNNs, deep convolutional neural networks, direct interPE communication, energy-efficient CNN processing, energy-efficient dataflow, feature map pixels, high-dimensional convolutions, local data reuse, neural nets, parallel processing, partial sum accumulations, power aware computing, processing engine local storage, row-stationary dataflow, spatial architecture, spatial parallelism},
	pages = {367--379},
}

@article{han_eie:_2016,
	title = {{EIE}: {Efficient} {Inference} {Engine} on {Compressed} {Deep} {Neural} {Network}},
	shorttitle = {{EIE}},
	url = {http://arxiv.org/abs/1602.01528},
	abstract = {State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets. While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power. Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by pruning the redundant connections and having multiple connections share the same weight. We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing. Going from DRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x; Weight sharing gives 8x; Skipping zero activations from ReLU saves another 3x. Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to CPU and GPU implementations of the same DNN without compression. EIE has a processing power of 102GOPS/s working directly on a compressed network, corresponding to 3TOPS/s on an uncompressed network, and processes FC layers of AlexNet at 1.88x10{\textasciicircum}4 frames/sec with a power dissipation of only 600mW. It is 24,000x and 3,400x more energy efficient than a CPU and GPU respectively. Compared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy efficiency and area efficiency.},
	urldate = {2019-05-05},
	journal = {arXiv:1602.01528 [cs]},
	author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.01528},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Hardware Architecture},
}

@article{park_deep_2018,
	title = {Deep {Learning} {Inference} in {Facebook} {Data} {Centers}: {Characterization}, {Performance} {Optimizations} and {Hardware} {Implications}},
	shorttitle = {Deep {Learning} {Inference} in {Facebook} {Data} {Centers}},
	url = {http://arxiv.org/abs/1811.09886},
	abstract = {The application of deep learning techniques resulted in remarkable improvement of machine learning models. In this paper provides detailed characterizations of deep learning models used in many Facebook social network services. We present computational characteristics of our models, describe high performance optimizations targeting existing systems, point out their limitations and make suggestions for the future general-purpose/accelerated inference hardware. Also, we highlight the need for better co-design of algorithms, numerics and computing platforms to address the challenges of workloads often run in data centers.},
	urldate = {2019-05-05},
	journal = {arXiv:1811.09886 [cs, stat]},
	author = {Park, Jongsoo and Naumov, Maxim and Basu, Protonu and Deng, Summer and Kalaiah, Aravind and Khudia, Daya and Law, James and Malani, Parth and Malevich, Andrey and Nadathur, Satish and Pino, Juan and Schatz, Martin and Sidorov, Alexander and Sivakumar, Viswanath and Tulloch, Andrew and Wang, Xiaodong and Wu, Yiming and Yuen, Hector and Diril, Utku and Dzhulgakov, Dmytro and Hazelwood, Kim and Jia, Bill and Jia, Yangqing and Qiao, Lin and Rao, Vijay and Rotem, Nadav and Yoo, Sungjoo and Smelyanskiy, Mikhail},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.09886},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{chen_dadiannao:_2014,
	address = {Cambridge, United Kingdom},
	title = {{DaDianNao}: {A} {Machine}-{Learning} {Supercomputer}},
	isbn = {978-1-4799-6998-2},
	shorttitle = {{DaDianNao}},
	url = {http://ieeexplore.ieee.org/document/7011421/},
	doi = {10.1109/MICRO.2014.58},
	abstract = {Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses.},
	language = {en},
	urldate = {2019-05-05},
	booktitle = {2014 47th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	publisher = {IEEE},
	author = {Chen, Yunji and Luo, Tao and Liu, Shaoli and Zhang, Shijin and He, Liqiang and Wang, Jia and Li, Ling and Chen, Tianshi and Xu, Zhiwei and Sun, Ninghui and Temam, Olivier},
	month = dec,
	year = {2014},
	pages = {609--622},
}

@article{rhu_vdnn:_2016,
	title = {{vDNN}: {Virtualized} {Deep} {Neural} {Networks} for {Scalable}, {Memory}-{Efficient} {Neural} {Network} {Design}},
	shorttitle = {{vDNN}},
	url = {http://arxiv.org/abs/1602.08124},
	abstract = {The most widely used machine learning frameworks require users to carefully tune their memory usage so that the deep neural network (DNN) ﬁts into the DRAM capacity of a GPU. This restriction hampers a researcher’s ﬂexibility to study different machine learning algorithms, forcing them to either use a less desirable network architecture or parallelize the processing across multiple GPUs. We propose a runtime memory manager that virtualizes the memory usage of DNNs such that both GPU and CPU memory can simultaneously be utilized for training larger DNNs. Our virtualized DNN (vDNN) reduces the average GPU memory usage of AlexNet by up to 89\%, OverFeat by 91\%, and GoogLeNet by 95\%, a signiﬁcant reduction in memory requirements of DNNs. Similar experiments on VGG-16, one of the deepest and memory hungry DNNs to date, demonstrate the memory-efﬁciency of our proposal. vDNN enables VGG-16 with batch size 256 (requiring 28 GB of memory) to be trained on a single NVIDIA Titan X GPU card containing 12 GB of memory, with 18\% performance loss compared to a hypothetical, oracular GPU with enough memory to hold the entire DNN.},
	language = {en},
	urldate = {2019-05-05},
	journal = {arXiv:1602.08124 [cs]},
	author = {Rhu, Minsoo and Gimelshein, Natalia and Clemons, Jason and Zulfiqar, Arslan and Keckler, Stephen W.},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.08124},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{jain_gist:_2018,
	address = {Los Angeles, CA},
	title = {Gist: {Efficient} {Data} {Encoding} for {Deep} {Neural} {Network} {Training}},
	isbn = {978-1-5386-5984-7},
	shorttitle = {Gist},
	url = {https://ieeexplore.ieee.org/document/8416872/},
	doi = {10.1109/ISCA.2018.00070},
	abstract = {Modern deep neural networks (DNNs) training typically relies on GPUs to train complex hundred-layer deep networks. A signiﬁcant problem facing both researchers and industry practitioners is that, as the networks get deeper, the available GPU main memory becomes a primary bottleneck, limiting the size of networks it can train.},
	language = {en},
	urldate = {2019-05-05},
	booktitle = {2018 {ACM}/{IEEE} 45th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	publisher = {IEEE},
	author = {Jain, Animesh and Phanishayee, Amar and Mars, Jason and Tang, Lingjia and Pekhimenko, Gennady},
	month = jun,
	year = {2018},
	pages = {776--789},
}

@article{kwon_beyond_2019,
	title = {Beyond the {Memory} {Wall}: {A} {Case} for {Memory}-centric {HPC} {System} for {Deep} {Learning}},
	shorttitle = {Beyond the {Memory} {Wall}},
	url = {http://arxiv.org/abs/1902.06468},
	abstract = {As the models and the datasets to train deep learning (DL) models scale, system architects are faced with new challenges, one of which is the memory capacity bottleneck, where the limited physical memory inside the accelerator device constrains the algorithm that can be studied. We propose a memory-centric deep learning system that can transparently expand the memory capacity available to the accelerators while also providing fast inter-device communication for parallel training. Our proposal aggregates a pool of memory modules locally within the device-side interconnect, which are decoupled from the host interface and function as a vehicle for transparent memory capacity expansion. Compared to conventional systems, our proposal achieves an average 2.8x speedup on eight DL applications and increases the system-wide memory capacity to tens of TBs.},
	urldate = {2019-05-05},
	journal = {arXiv:1902.06468 [cs]},
	author = {Kwon, Youngeun and Rhu, Minsoo},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.06468},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{hestness_beyond_2019,
	address = {New York, NY, USA},
	series = {{PPoPP} '19},
	title = {Beyond {Human}-level {Accuracy}: {Computational} {Challenges} in {Deep} {Learning}},
	isbn = {978-1-4503-6225-2},
	shorttitle = {Beyond {Human}-level {Accuracy}},
	url = {http://doi.acm.org/10.1145/3293883.3295710},
	doi = {10.1145/3293883.3295710},
	abstract = {Loading...},
	urldate = {2019-05-05},
	booktitle = {Proceedings of the 24th {Symposium} on {Principles} and {Practice} of {Parallel} {Programming}},
	publisher = {ACM},
	author = {Hestness, Joel and Ardalani, Newsha and Diamos, Gregory},
	year = {2019},
	note = {event-place: Washington, District of Columbia},
	keywords = {compute graph, compute requirements, data parallelism, deep learning, model parallelism, neural networks},
	pages = {1--14},
}

@article{rhu_maximizing_nodate,
	title = {Maximizing {SIMD} {Resource} {Utilization} in {GPGPUs} with {SIMD} {Lane} {Permutation}},
	abstract = {Current GPUs maintain high programmability by abstracting the SIMD nature of the hardware as independent concurrent threads of control with hardware responsible for generating predicate masks to utilize the SIMD hardware for diﬀerent ﬂows of control. This dynamic masking leads to poor utilization of SIMD resources when the control of different threads in the same SIMD group diverges. Prior research suggests that SIMD groups be formed dynamically by compacting a large number of threads into groups, mitigating the impact of divergence. To maintain hardware efﬁciency, however, the alignment of a thread to a SIMD lane is ﬁxed, limiting the potential for compaction. We observe that control frequently diverges in a manner that prevents compaction because of the way in which the ﬁxed alignment of threads to lanes is done. This paper presents an in-depth analysis on the causes for ineﬀective compaction. An important observation is that in many cases, control diverges because of programmatic branches, which do not depend on input data. This behavior, when combined with the default mapping of threads to lanes, severely restricts compaction. We then propose SIMD lane permutation (SLP) as an optimization to expand the applicability of compaction in such cases of lane alignment. SLP seeks to rearrange how threads are mapped to lanes to allow even programmatic branches to be compacted eﬀectively, improving SIMD utilization up to 34\% accompanied by a maximum 25\% performance boost.},
	language = {en},
	author = {Rhu, Minsoo and Erez, Mattan},
	pages = {12},
}

@inproceedings{chatterjee_managing_2014,
	title = {Managing {DRAM} {Latency} {Divergence} in {Irregular} {GPGPU} {Applications}},
	doi = {10.1109/SC.2014.16},
	abstract = {Memory controllers in modern GPUs aggressively reorder requests for high bandwidth usage, often interleaving requests from different warps. This leads to high variance in the latency of different requests issued by the threads of a warp. Since a warp in a SIMT architecture can proceed only when all of its memory requests are returned by memory, such latency divergence causes significant slowdown when running irregular GPGPU applications. To solve this issue, we propose memory scheduling mechanisms that avoid inter-warp interference in the DRAM system to reduce the average memory stall latency experienced by warps. We further reduce latency divergence through mechanisms that coordinate scheduling decisions across multiple independent memory channels. Finally we show that carefully orchestrating the memory scheduling policy can achieve low average latency for warps, without compromising bandwidth utilization. Our combined scheme yields a 10.1\% performance improvement for irregular GPGPU workloads relative to a throughput-optimized GPU memory controller.},
	booktitle = {{SC} '14: {Proceedings} of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Chatterjee, N. and O'Connor, M. and Loh, G. H. and Jayasena, N. and Balasubramonia, R.},
	month = nov,
	year = {2014},
	keywords = {Bandwidth, DRAM chips, DRAM system, Graphics processing units, Instruction sets, Memory management, Parallel processing, Random access memory, SIMT architecture, average memory stall latency, bandwidth utilization, graphics processing units, high bandwidth usage, independent memory channels, interleaving requests, interwarp interference, irregular GPGPU applications, latency divergence, memory controllers, memory requests, memory scheduling mechanisms, memory scheduling policy, scheduling decisions, storage management chips, throughput-optimized GPU memory controller},
	pages = {128--139},
}

@inproceedings{gebhart_unifying_2012,
	address = {Washington, DC, USA},
	series = {{MICRO}-45},
	title = {Unifying {Primary} {Cache}, {Scratch}, and {Register} {File} {Memories} in a {Throughput} {Processor}},
	isbn = {978-0-7695-4924-8},
	url = {https://doi.org/10.1109/MICRO.2012.18},
	doi = {10.1109/MICRO.2012.18},
	abstract = {Modern throughput processors such as GPUs employ thousands of threads to drive high-bandwidth, long-latency memory systems. These threads require substantial on-chip storage for registers, cache, and scratchpad memory. Existing designs hard-partition this local storage, fixing the capacities of these structures at design time. We evaluate modern GPU workloads and find that they have widely varying capacity needs across these different functions. Therefore, we propose a unified local memory which can dynamically change the partitioning among registers, cache, and scratchpad on a per-application basis. The tuning that this flexibility enables improves both performance and energy consumption, and broadens the scope of applications that can be efficiently executed on GPUs. Compared to a hard-partitioned design, we show that unified local memory provides a performance benefit as high as 71\% along with an energy reduction up to 33\%.},
	urldate = {2019-05-03},
	booktitle = {Proceedings of the 2012 45th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	publisher = {IEEE Computer Society},
	author = {Gebhart, Mark and Keckler, Stephen W. and Khailany, Brucek and Krashinsky, Ronny and Dally, William J.},
	year = {2012},
	note = {event-place: Vancouver, B.C., CANADA},
	pages = {96--106},
}

@inproceedings{rhu_locality-aware_2013,
	title = {A locality-aware memory hierarchy for energy-efficient {GPU} architectures},
	abstract = {As GPU's compute capabilities grow, their memory hierarchy increasingly becomes a bottleneck. Current GPU memory hierarchies use coarse-grained memory accesses to exploit spatial locality, maximize peak bandwidth, simplify control, and reduce cache meta-data storage. These coarse-grained memory accesses, however, are a poor match for emerging GPU applications with irregular control flow and memory access patterns. Meanwhile, the massive multi-threading of GPUs and the simplicity of their cache hierarchies make CPU-specific memory system enhancements ineffective for improving the performance of irregular GPU applications. We design and evaluate a locality-aware memory hierarchy for throughput processors, such as GPUs. Our proposed design retains the advantages of coarse-grained accesses for spatially and temporally local programs while permitting selective fine-grained access to memory. By adaptively adjusting the access granularity, memory bandwidth and energy are reduced for data with low spatial/temporal locality without wasting control overheads or prefetching potential for data with high spatial locality. As such, our locality-aware memory hierarchy improves GPU performance, energy-efficiency, and memory throughput for a large range of applications.},
	booktitle = {2013 46th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	author = {Rhu, M. and Sullivan, M. and Leng, J. and Erez, M.},
	month = dec,
	year = {2013},
	keywords = {Adaptive Granularity Memory, Bandwidth, CPU, Fine-Grained Memory Access, GPU, GPU memory hierarchies, GPU performance, Graphics processing units, Instruction sets, Irregular Memory Access Patterns, Memory management, Random access memory, SIMD, SIMT, Throughput, access granularity, cache hierarchies, cache meta-data storage, coarse-grained accesses, coarse-grained memory, energy efficiency, energy-efficient GPU architectures, graphics processing units, high spatial locality, irregular GPU applications, irregular control flow, locality-aware memory hierarchy, massive multithreading, memory access patterns, memory architecture, memory bandwidth, memory system enhancements, memory throughput, multi-threading, power aware computing, prefetching, selective fine-grained access, storage management, throughput processors, wasting control},
	pages = {86--98},
}

@article{narasiman_improving_2011,
	title = {Improving {GPU} performance via large warps and two-level warp scheduling},
	doi = {10.1145/2155620.2155656},
	abstract = {Due to their massive computational power, graphics processing units (GPUs) have become a popular platform for executing general purpose parallel applications. GPU programming models allow the programmer to create thousands of threads, each executing the same computing kernel. GPUs exploit this parallelism in two ways. First, threads are grouped into fixed-size SIMD batches known as warps, and second, many such warps are concurrently executed on a single GPU core. Despite these techniques, the computational resources on GPU cores are still underutilized, resulting in performance far short of what could be delivered. Two reasons for this are conditional branch instructions and stalls due to long latency operations.
 To improve GPU performance, computational resources must be more effectively utilized. To accomplish this, we propose two independent ideas: the large warp microarchitecture and two-level warp scheduling. We show that when combined, our mechanisms improve performance by 19.1\% over traditional GPU cores for a wide variety of general purpose parallel applications that heretofore have not been able to fully exploit the available resources of the GPU chip.},
	journal = {2011 44th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
	author = {Narasiman, Veynu and Shebanow, Michael and Lee, Chang Joo and Miftakhutdinov, Rustam and Mutlu, Onur and Patt, Yale N.},
	year = {2011},
	keywords = {Branch (computer science), Computational resource, Computer graphics, Graphics processing unit, Microarchitecture, Parallel computing, Programmer, SIMD, Scheduling (computing), Speedup, WARP (information security), Warp (gaming)},
	pages = {308--317},
}

@inproceedings{rhu_dual-path_2013,
	title = {The dual-path execution model for efficient {GPU} control flow},
	doi = {10.1109/HPCA.2013.6522352},
	abstract = {Current graphics processing units (GPUs) utilize the single instruction multiple thread (SIMT) execution model. With SIMT, a group of logical threads executes such that all threads in the group execute a single common instruction on a particular cycle. To enable control flow to diverge within the group of threads, GPUs partially serialize execution and follow a single control flow path at a time. The execution of the threads in the group that are not on the current path is masked. Most current GPUs rely on a hardware reconvergence stack to track the multiple concurrent paths and to choose a single path for execution. Control flow paths are pushed onto the stack when they diverge and are popped off of the stack to enable threads to reconverge and keep lane utilization high. The stack algorithm guarantees optimal reconvergence for applications with structured control flow as it traverses the structured control-flow tree depth first. The downside of using the reconvergence stack is that only a single path is followed, which does not maximize available parallelism, degrading performance in some cases. We propose a change to the stack hardware in which the execution of two different paths can be interleaved. While this is a fundamental change to the stack concept, we show how dual-path execution can be implemented with only modest changes to current hardware and that parallelism is increased without sacrificing optimal (structured) control-flow reconvergence. We perform a detailed evaluation of a set of benchmarks with divergent control flow and demonstrate that the dual-path stack architecture is much more robust compared to previous approaches for increasing path parallelism. Dual-path execution either matches the performance of the baseline single-path stack architecture or outperforms single-path execution by 14.9\% on average and by over 30\% in some cases.},
	booktitle = {2013 {IEEE} 19th {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	author = {Rhu, M. and Erez, M.},
	month = feb,
	year = {2013},
	keywords = {Computer architecture, GPU control flow, Graphics processing units, Hardware, Instruction sets, Microarchitecture, Parallel processing, Robustness, SIMT execution model, concurrency control, control-flow reconvergence stack, dual-path execution, dual-path execution model, dual-path stack architecture, graphics processing units, hardware reconvergence stack, logical threads, multi-threading, optimal reconvergence, path parallelism, performance degradation, performance evaluation, single control flow path, single instruction multiple thread execution model, single-path stack architecture, stack algorithm, stack hardware, structured control-flow tree depth first, tree searching},
	pages = {591--602},
}

@inproceedings{fung_thread_2011,
	title = {Thread block compaction for efficient {SIMT} control flow},
	doi = {10.1109/HPCA.2011.5749714},
	abstract = {Manycore accelerators such as graphics processor units (GPUs) organize processing units into single-instruction, multiple data “cores” to improve throughput per unit hardware cost. Programming models for these accelerators encourage applications to run kernels with large groups of parallel scalar threads. The hardware groups these threads into warps/wavefronts and executes them in lockstep-dubbed single-instruction, multiple-thread (SIMT) by NVIDIA. While current GPUs employ a per-warp (or per-wavefront) stack to manage divergent control flow, it incurs decreased efficiency for applications with nested, data-dependent control flow. In this paper, we propose and evaluate the benefits of extending the sharing of resources in a block of warps, already used for scratchpad memory, to exploit control flow locality among threads (where such sharing may at first seem detrimental). In our proposal, warps within a thread block share a common block-wide stack for divergence handling. At a divergent branch, threads are compacted into new warps in hardware. Our simulation results show that this compaction mechanism provides an average speedup of 22\% over a baseline per-warp, stack-based reconvergence mechanism, and 17\% versus dynamic warp formation on a set of CUDA applications that suffer significantly from control flow divergence.},
	booktitle = {2011 {IEEE} 17th {International} {Symposium} on {High} {Performance} {Computer} {Architecture}},
	author = {Fung, W. W. L. and Aamodt, T. M.},
	month = feb,
	year = {2011},
	keywords = {CUDA applications, Compaction, Graphics processing unit, Hardware, Instruction sets, Kernel, NVIDIA, Pipelines, Random access memory, SIMT control flow, computer graphic equipment, coprocessors, data dependent control flow, divergence handling, graphics processor units, manycore accelerators, multiprocessing systems, parallel architectures, parallel scalar threads, single instruction multiple data cores, stack based reconvergence mechanism, thread block compaction, warps, wavefronts},
	pages = {25--36},
}

@inproceedings{gebhart_energy-efficient_2011,
	title = {Energy-efficient mechanisms for managing thread context in throughput processors},
	doi = {10.1145/2000064.2000093},
	abstract = {Modern graphics processing units (GPUs) use a large number of hardware threads to hide both function unit and memory access latency. Extreme multithreading requires a complicated thread scheduler as well as a large register file, which is expensive to access both in terms of energy and latency. We present two complementary techniques for reducing energy on massively-threaded processors such as GPUs. First, we examine register file caching to replace accesses to the large main register file with accesses to a smaller structure containing the immediate register working set of active threads. Second, we investigate a two-level thread scheduler that maintains a small set of active threads to hide ALU and local memory access latency and a larger set of pending threads to hide main memory latency. Combined with register file caching, a two-level thread scheduler provides a further reduction in energy by limiting the allocation of temporary register cache resources to only the currently active subset of threads. We show that on average, across a variety of real world graphics and compute workloads, a 6-entry per-thread register file cache reduces the number of reads and writes to the main register file by 50\% and 59\% respectively. We further show that the active thread count can be reduced by a factor of 4 with minimal impact on performance, resulting in a 36\% reduction of register file energy.},
	booktitle = {2011 38th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	author = {Gebhart, M. and Johnson, D. R. and Tarjan, D. and Keckler, S. W. and Dally, W. J. and Lindholm, E. and Skadron, K.},
	month = jun,
	year = {2011},
	keywords = {Computer architecture, GPU, Graphics, Graphics processing unit, Instruction sets, Pipelines, Registers, energy efficient mechanisms, file caching, file register, graphics processing units, hardware threads, main memory latency, memory access latency, multi-threading, power aware computing, thread context management, thread scheduler, throughput processors},
	pages = {235--246},
}

@inproceedings{li_priority-based_2015,
	title = {Priority-based cache allocation in throughput processors},
	doi = {10.1109/HPCA.2015.7056024},
	abstract = {GPUs employ massive multithreading and fast context switching to provide high throughput and hide memory latency. Multithreading can Increase contention for various system resources, however, that may result In suboptimal utilization of shared resources. Previous research has proposed variants of throttling thread-level parallelism to reduce cache contention and improve performance. Throttling approaches can, however, lead to under-utilizing thread contexts, on-chip interconnect, and off-chip memory bandwidth. This paper proposes to tightly couple the thread scheduling mechanism with the cache management algorithms such that GPU cache pollution is minimized while off-chip memory throughput is enhanced. We propose priority-based cache allocation (PCAL) that provides preferential cache capacity to a subset of high-priority threads while simultaneously allowing lower priority threads to execute without contending for the cache. By tuning thread-level parallelism while both optimizing caching efficiency as well as other shared resource usage, PCAL builds upon previous thread throttling approaches, improving overall performance by an average 17\% with maximum 51\%.},
	booktitle = {2015 {IEEE} 21st {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	author = {Li, D. and Rhu, M. and Johnson, D. R. and O'Connor, M. and Erez, M. and Burger, D. and Fussell, D. S. and Redder, S. W.},
	month = feb,
	year = {2015},
	keywords = {Bandwidth, GPU cache pollution, Graphics processing units, Instruction sets, Multithreading, Resource management, Throughput, cache management algorithm, cache storage, context switching, graphics processing units, multi-threading, multithreading, off-chip memory bandwidth, on-chip interconnect, parallel architectures, priority-based cache allocation, thread scheduling mechanism, throttling thread-level parallelism, throughput processor},
	pages = {89--100},
}

@inproceedings{rogers_cache-conscious_2012,
	address = {Vancouver, BC, Canada},
	title = {Cache-{Conscious} {Wavefront} {Scheduling}},
	isbn = {978-1-4673-4819-5 978-0-7695-4924-8},
	url = {http://ieeexplore.ieee.org/document/6493609/},
	doi = {10.1109/MICRO.2012.16},
	abstract = {This paper studies the effects of hardware thread scheduling on cache management in GPUs. We propose Cache-Conscious Wavefront Scheduling (CCWS), an adaptive hardware mechanism that makes use of a novel intra-wavefront locality detector to capture locality that is lost by other schedulers due to excessive contention for cache capacity. In contrast to improvements in the replacement policy that can better tolerate difﬁcult access patterns, CCWS shapes the access pattern to avoid thrashing the shared L1. We show that CCWS can outperform any replacement scheme by evaluating against the Belady-optimal policy. Our evaluation demonstrates that cache efﬁciency and preservation of intra-wavefront locality become more important as GPU computing expands beyond use in high performance computing. At an estimated cost of 0.17\% total chip area, CCWS reduces the number of threads actively issued on a core when appropriate. This leads to an average 25\% fewer L1 data cache misses which results in a harmonic mean 24\% performance improvement over previously proposed scheduling policies across a diverse selection of cache-sensitive workloads.},
	language = {en},
	urldate = {2019-04-24},
	booktitle = {2012 45th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	publisher = {IEEE},
	author = {Rogers, Timothy G. and OConnor, Mike and Aamodt, Tor M.},
	month = dec,
	year = {2012},
	pages = {72--83},
}

@inproceedings{lee_cawa:_2015,
	title = {{CAWA}: {Coordinated} warp scheduling and {Cache} {Prioritization} for critical warp acceleration of {GPGPU} workloads},
	shorttitle = {{CAWA}},
	doi = {10.1145/2749469.2750418},
	abstract = {The ubiquity of graphics processing unit (GPU) architectures has made them efficient alternatives to chipmultiprocessors for parallel workloads. GPUs achieve superior performance by making use of massive multi-threading and fast context-switching to hide pipeline stalls and memory access latency. However, recent characterization results have shown that general purpose GPU (GPGPU) applications commonly encounter long stall latencies that cannot be easily hidden with the large number of concurrent threads/warps. This results in varying execution time disparity between different parallel warps, hurting the overall performance of GPUs - the warp criticality problem. To tackle the warp criticality problem, we propose a coordinated solution, criticality-aware warp acceleration (CAWA), that efficiently manages compute and memory resources to accelerate the critical warp execution. Specifically, we design (1) an instruction-based and stall-based criticality predictor to identify the critical warp in a thread-block, (2) a criticality-aware warp scheduler that preferentially allocates more time resources to the critical warp, and (3) a criticality-aware cache reuse predictor that assists critical warp acceleration by retaining latency-critical and useful cache blocks in the L1 data cache. CAWA targets to remove the significant execution time disparity in order to improve resource utilization for GPGPU workloads. Our evaluation results show that, under the proposed coordinated scheduler and cache prioritization management scheme, the performance of the GPGPU workloads can be improved by 23\% while other state-of-the-art schedulers, GTO and 2-level schedulers, improve performance by 16\% and -2\% respectively.},
	booktitle = {2015 {ACM}/{IEEE} 42nd {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	author = {Lee, S. and Arunkumar, A. and Wu, C.},
	month = jun,
	year = {2015},
	keywords = {Acceleration, CAWA, CMP, GPGPU workload, Instruction sets, Round robin, cache prioritization, chip-multiprocessor, criticality-aware warp acceleration, general purpose graphics processing unit, graphics processing units, multi-threading, multiprocessing systems, multithreading, processor scheduling, resource allocation, resource utilization, warp scheduling},
	pages = {515--527},
}

@misc{smith_nvidia_nodate,
	title = {{NVIDIA} {Posts} {Full} {GeForce} {GTX} 1070 {Specifications}: 1920 {CUDA} {Cores} {Boosting} to 1.{68GHz}},
	shorttitle = {{NVIDIA} {Posts} {Full} {GeForce} {GTX} 1070 {Specifications}},
	url = {https://www.anandtech.com/show/10336/nvidia-posts-full-geforce-gtx-1070-specs},
	urldate = {2019-03-31},
	author = {Smith, Ryan},
}

@incollection{ahnert_solving_2014,
	address = {Cham},
	title = {Solving {Ordinary} {Differential} {Equations} on {GPUs}},
	isbn = {978-3-319-06548-9},
	url = {https://doi.org/10.1007/978-3-319-06548-9_7},
	abstract = {Ordinary Differential Equations (ODEs) are a fundamental mathematical tool to model physical, biological or chemical systems, and they are widely used in engineering, economics and social sciences. Given their vast appearance, it is of crucial importance to develop efficient numerical routines for solving ODEs that employ the computational power of modern GPUs. Here, we present a high-level approach to compute numerical solutions of ODEs by developing a generic implementation of the most common algorithms and combining this with modern C++++++ libraries like VexCL and Thrust. Our approach is based on generic programming and results in highly scalable and easy-to-use source code.},
	language = {en},
	urldate = {2019-03-29},
	booktitle = {Numerical {Computations} with {GPUs}},
	publisher = {Springer International Publishing},
	author = {Ahnert, Karsten and Demidov, Denis and Mulansky, Mario},
	editor = {Kindratenko, Volodymyr},
	year = {2014},
	doi = {10.1007/978-3-319-06548-9_7},
	keywords = {Initial Value Problem, Lorenz Attractor, Lorenz System, Memory Allocation, Template Parameter},
	pages = {125--157},
}

@inproceedings{chen_adaptive_2018-1,
	title = {Adaptive {Optimization} of {Sparse} {Matrix}-{Vector} {Multiplication} on {Emerging} {Many}-{Core} {Architectures}},
	doi = {10.1109/HPCC/SmartCity/DSS.2018.00116},
	abstract = {Sparse matrix vector multiplication (SpMV) is one of the most common operations in scientific and high-performance applications, and is often responsible for the application performance bottleneck. While the sparse matrix representation has a significant impact on the resulting application performance, choosing the right representation typically relies on expert knowledge and trial and error. This paper provides the first comprehensive study on the impact of sparse matrix representations on two emerging many-core architectures: the Intel's Knights Landing (KNL) XeonPhi and the ARM-based FT-2000Plus (FTP). Our large-scale experiments involved over 9,500 distinct profiling runs performed on 956 sparse datasets and five mainstream SpMV representations. We show that the best sparse matrix representation depends on the underlying architecture and the program input. To help developers to choose the optimal matrix representation, we employ machine learning to develop a predictive model. Our model is first trained offline using a set of training examples. The learned model can be used to predict the best matrix representation for any unseen input for a given architecture. We show that our model delivers on average 95\% and 91\% of the best available performance on KNL and FTP respectively, and it achieves this with no runtime profiling overhead.},
	booktitle = {2018 {IEEE} 20th {International} {Conference} on {High} {Performance} {Computing} and {Communications}; {IEEE} 16th {International} {Conference} on {Smart} {City}; {IEEE} 4th {International} {Conference} on {Data} {Science} and {Systems} ({HPCC}/{SmartCity}/{DSS})},
	author = {Chen, S. and Fang, J. and Chen, D. and Xu, C. and Wang, Z.},
	month = jun,
	year = {2018},
	keywords = {ARM-based FT-2000Plus, Arrays, Predictive models, Random access memory, Sparse matrices, Sparse matrix vector multiplication, Performance optimization, Many-Cores, Performance analysis, Two dimensional displays, application performance bottleneck, high-performance applications, learning (artificial intelligence), mainstream SpMV representations, many-core architectures, mathematics computing, matrix multiplication, optimal matrix representation, sparse datasets, sparse matrices, sparse matrix representation, sparse matrix vector multiplication, sparse matrix-vector multiplication, storage management, vectors},
	pages = {649--658},
}

@inproceedings{cummins_end--end_2017,
	title = {End-to-{End} {Deep} {Learning} of {Optimization} {Heuristics}},
	doi = {10.1109/PACT.2017.24},
	abstract = {Accurate automatic optimization heuristics are necessary for dealing with thecomplexity and diversity of modern hardware and software. Machine learning is aproven technique for learning such heuristics, but its success is bound by thequality of the features used. These features must be hand crafted by developersthrough a combination of expert domain knowledge and trial and error. This makesthe quality of the final model directly dependent on the skill and availabletime of the system architect. Our work introduces a better way for building heuristics. We develop a deepneural network that learns heuristics over raw code, entirely without using codefeatures. The neural network simultaneously constructs appropriaterepresentations of the code and learns how best to optimize, removing the needfor manual feature creation. Further, we show that our neural nets can transferlearning from one optimization problem to another, improving the accuracy of newmodels, without the help of human experts. We compare the effectiveness of our automatically generated heuristics againstones with features hand-picked by experts. We examine two challenging tasks:predicting optimal mapping for heterogeneous parallelism and GPU threadcoarsening factors. In 89\% of the cases, the quality of our fully automaticheuristics matches or surpasses that of state-of-the-art predictive models usinghand-crafted features, providing on average 14\% and 12\% more performance withno human effort expended on designing features.},
	booktitle = {2017 26th {International} {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques} ({PACT})},
	author = {Cummins, C. and Petoumenos, P. and Wang, Z. and Leather, H.},
	month = sep,
	year = {2017},
	keywords = {Compiler Optimizations, Feature extraction, GPU threadcoarsening factors, Heterogeneous Systems, Machine Learning, Neural networks, Optimization, Optimization Heuristics, Predictive models, Runtime, Vocabulary, accurate automatic optimization heuristics, deep learning, deepneural network, end-to-end deep learning, expert domain knowledge, human experts, learning (artificial intelligence), machine learning, multi-threading, needfor manual feature creation, neural nets, optimization problem},
	pages = {219--232},
}

@article{wang_machine_2018,
	title = {Machine {Learning} in {Compiler} {Optimization}},
	volume = {106},
	issn = {0018-9219},
	doi = {10.1109/JPROC.2018.2817118},
	abstract = {In the last decade, machine-learning-based compilation has moved from an obscure research niche to a mainstream activity. In this paper, we describe the relationship between machine learning and compiler optimization and introduce the main concepts of features, models, training, and deployment. We then provide a comprehensive survey and provide a road map for the wide variety of different research areas. We conclude with a discussion on open issues in the area and potential research directions. This paper provides both an accessible introduction to the fast moving area of machine-learning-based compilation and a detailed bibliography of its main achievements.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Wang, Z. and O’Boyle, M.},
	month = nov,
	year = {2018},
	keywords = {Code optimization, Data models, Feature extraction, High performance computing, Machine learning, Optimization, Program processors, Training data, compiler, compiler optimization, learning (artificial intelligence), machine learning, machine-learning-based compilation, optimisation, program compilers, program tuning},
	pages = {1879--1901},
}

@article{bolingbroke_supercompilation_nodate,
	title = {Supercompilation by {Evaluation}},
	abstract = {Supercompilation is a technique due to Turchin [1] which allows for the construction of program optimisers that are both simple and extremely powerful. Supercompilation is capable of achieving transformations such as deforestation [2], function specialisation and constructor specialisation [3]. Inspired by Mitchell’s promising results [4], we show how the call-by-need supercompilation algorithm can be recast to be based explicitly on an evaluator, and in the process extend it to deal with recursive let expressions.},
	language = {en},
	author = {Bolingbroke, Max and Jones, Simon Peyton},
	pages = {12},
}

@inproceedings{mitchell_rethinking_2010,
	address = {New York, NY, USA},
	series = {{ICFP} '10},
	title = {Rethinking {Supercompilation}},
	isbn = {978-1-60558-794-3},
	url = {http://doi.acm.org/10.1145/1863543.1863588},
	doi = {10.1145/1863543.1863588},
	abstract = {Supercompilation is a program optimisation technique that is particularly effective at eliminating unnecessary overheads. We have designed a new supercompiler, making many novel choices, including different termination criteria and handling of let bindings. The result is a supercompiler that focuses on simplicity, compiles programs quickly and optimises programs well. We have benchmarked our supercompiler, with some programs running more than twice as fast than when compiled with GHC.},
	urldate = {2019-02-21},
	booktitle = {Proceedings of the 15th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Mitchell, Neil},
	year = {2010},
	note = {event-place: Baltimore, Maryland, USA},
	keywords = {haskell, optimisation, supercompilation},
	pages = {309--320},
}

@inproceedings{sorensen_introduction_1999,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Introduction to {Supercompilation}},
	isbn = {978-3-540-47018-2},
	abstract = {This paper gives an introduction to Turchin’s supercompiler, a program transformer for functional programs which performs optimizations beyond partial evaluation and deforestation. More precisely, the paper presents positive supercompilation.},
	language = {en},
	booktitle = {Partial {Evaluation}},
	publisher = {Springer Berlin Heidelberg},
	author = {Sørensen, Morten Heine B. and Glück, Robert},
	editor = {Hatcliff, John and Mogensen, Torben Æ and Thiemann, Peter},
	year = {1999},
	keywords = {Abstract Step, Generalization Step, Leaf Node, Partial Evaluator, Transient Reduction},
	pages = {246--270},
}

@inproceedings{sen_cute:_2005,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE}-13},
	title = {{CUTE}: {A} {Concolic} {Unit} {Testing} {Engine} for {C}},
	isbn = {978-1-59593-014-9},
	shorttitle = {{CUTE}},
	url = {http://doi.acm.org/10.1145/1081706.1081750},
	doi = {10.1145/1081706.1081750},
	abstract = {In unit testing, a program is decomposed into units which are collections of functions. A part of unit can be tested by generating inputs for a single entry function. The entry function may contain pointer arguments, in which case the inputs to the unit are memory graphs. The paper addresses the problem of automating unit testing with memory graphs as inputs. The approach used builds on previous work combining symbolic and concrete execution, and more specifically, using such a combination to generate test inputs to explore all feasible execution paths. The current work develops a method to represent and track constraints that capture the behavior of a symbolic execution of a unit with memory graphs as inputs. Moreover, an efficient constraint solver is proposed to facilitate incremental generation of such test inputs. Finally, CUTE, a tool implementing the method is described together with the results of applying CUTE to real-world examples of C code.},
	urldate = {2019-01-23},
	booktitle = {Proceedings of the 10th {European} {Software} {Engineering} {Conference} {Held} {Jointly} with 13th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Sen, Koushik and Marinov, Darko and Agha, Gul},
	year = {2005},
	keywords = {concolic testing, data structure testing, explicit path model-checking, random testing, testing C programs, unit testing},
	pages = {263--272},
}

@article{ernst_quickly_nodate,
	title = {Quickly {Detecting} {Relevant} {Program} {Invariants}},
	abstract = {Explicitly stated program invariants can help programmers by characterizing certain aspects of program execution and identifying program properties that must be preserved when modifying code. Unfortunately, these invariants are usually absent from code. Previous work showed how to dynamically detect invariants from program traces by looking for patterns in and relationships among variable values. A prototype implementation, Daikon, accurately recovered invariants from formally-speciﬁed programs, and the invariants it detected in other programs assisted programmers in a software evolution task. However, Daikon suffered from reporting too many invariants, many of which were not useful, and also failed to report some desired invariants.},
	language = {en},
	author = {Ernst, Michael D and Czeisler, Adam and Griswold, William G and Notkin, David},
	pages = {10},
}

@article{ernst_dynamically_2001,
	title = {Dynamically discovering likely program invariants to support program evolution},
	volume = {27},
	issn = {0098-5589},
	doi = {10.1109/32.908957},
	abstract = {Explicitly stated program invariants can help programmers by identifying program properties that must be preserved when modifying code. In practice, however, these invariants are usually implicit. An alternative to expecting programmers to fully annotate code with invariants is to automatically infer likely invariants from the program itself. This research focuses on dynamic techniques for discovering invariants from execution traces. This article reports three results. First, it describes techniques for dynamically discovering invariants, along with an implementation, named Daikon, that embodies these techniques. Second, it reports on the application of Daikon to two sets of target programs. In programs from Gries's work (1981) on program derivation, the system rediscovered predefined invariants. In a C program lacking explicit invariants, the system discovered invariants that assisted a software evolution task. These experiments demonstrate that, at least for small programs, invariant inference is both accurate and useful. Third, it analyzes scalability issues, such as invariant detection runtime and accuracy, as functions of test suites and program points instrumented.},
	number = {2},
	journal = {IEEE Transactions on Software Engineering},
	author = {Ernst, M. D. and Cockrell, J. and Griswold, W. G. and Notkin, D.},
	month = feb,
	year = {2001},
	keywords = {Application software, Computer Society, Daikon, Detectors, Formal specifications, Instruments, Pattern analysis, Programming profession, Runtime, Scalability, Testing, execution traces, explicitly stated program invariants, invariant inference, likely program invariants, modifying code, program derivation, program evolution, program properties, reverse engineering, scalability, small programs, software evolution, software maintenance},
	pages = {99--123},
}

@inproceedings{sun_towards_2015,
	address = {Fort Collins, CO, USA},
	title = {Towards the use of slicing techniques for an efficient invariant checking},
	isbn = {978-1-4503-3283-5},
	url = {http://dl.acm.org/citation.cfm?doid=2735386.2735926},
	doi = {10.1145/2735386.2735926},
	abstract = {In Model Driven Development (MDD), invariant checking involves determining whether a model is consistent with invariants deﬁned in a metamodel. Such checking can improve developers’ understanding of modeled aspects of complex systems and uncover structural errors in design models during the early stages of software development. General-purpose rigorous analysis tools that check invariants are likely to perform the analysis over the entire metamodel and model. Their scalability thus becomes an issue (e.g., the time used for checking can be up to several hours) with very large metamodels and models (e.g., more than 500,000 elements). In this paper we introduce model slicing within the invariant checking process, and use a slicing technique to reduce the size of checking inputs to improve the scalability of existing invariant checking tools. The evaluation we performed provides evidence that model slicing can signiﬁcantly reduce the time to perform the invariant checking while preserving the checking results.},
	language = {en},
	urldate = {2019-01-22},
	booktitle = {Companion {Proceedings} of the 14th {International} {Conference} on {Modularity} - {MODULARITY} {Companion} 2015},
	publisher = {ACM Press},
	author = {Sun, Wuliang and Combemale, Benoit and France, Robert B.},
	year = {2015},
	pages = {23--24},
}

@incollection{bloem_formula_2016,
	address = {Cham},
	title = {Formula {Slicing}: {Inductive} {Invariants} from {Preconditions}},
	volume = {10028},
	isbn = {978-3-319-49051-9 978-3-319-49052-6},
	shorttitle = {Formula {Slicing}},
	url = {http://link.springer.com/10.1007/978-3-319-49052-6_11},
	abstract = {We propose a “formula slicing” method for ﬁnding inductive invariants. It is based on the observation that many loops in the program aﬀect only a small part of the memory, and many invariants which were valid before a loop are still valid after.},
	language = {en},
	urldate = {2019-01-22},
	booktitle = {Hardware and {Software}: {Verification} and {Testing}},
	publisher = {Springer International Publishing},
	author = {Karpenkov, Egor George and Monniaux, David},
	editor = {Bloem, Roderick and Arbel, Eli},
	year = {2016},
	doi = {10.1007/978-3-319-49052-6_11},
	pages = {169--185},
}

@article{chakrabarti_cuda:_2012,
	series = {Proceedings of the {International} {Conference} on {Computational} {Science}, {ICCS} 2012},
	title = {{CUDA}: {Compiling} and optimizing for a {GPU} platform},
	volume = {9},
	issn = {1877-0509},
	shorttitle = {{CUDA}},
	url = {http://www.sciencedirect.com/science/article/pii/S1877050912003304},
	doi = {10.1016/j.procs.2012.04.209},
	abstract = {Graphics processor units (GPUs) have evolved to handle throughput oriented workloads where a large number of parallel threads must make progress. Such threads are organized around shared memory making it possible to synchronize and cooperate on shared data. Current GPUs can run tens of thousands of hardware threads and have been optimized for graphics workloads. Several high level languages have been developed to easily program the GPUs for general purpose computing problems. The use of high-level languages introduces the need for highly optimizing compilers that target the parallel GPU device. In this paper, we present our experiences in developing compilation techniques for a high level language called CUDA C. We explain the CUDA architecture and programming model and provide insights into why certain optimizations are important for achieving high performance on a GPU. In addition to classical optimizations, we present optimizations developed specifically for the CUDA architecture. We evaluate these techniques, and present performance results that show significant improvements on hundreds of kernels as well as applications.},
	urldate = {2019-01-22},
	journal = {Procedia Computer Science},
	author = {Chakrabarti, Gautam and Grover, Vinod and Aarts, Bastiaan and Kong, Xiangyun and Kudlur, Manjunath and Lin, Yuan and Marathe, Jaydeep and Murphy, Mike and Wang, Jian-Zhong},
	month = jan,
	year = {2012},
	keywords = {CUDA, GPGPU, compiler optimizations},
	pages = {1910--1919},
}

@inproceedings{cheng_simulation-directed_2008,
	title = {Simulation-{Directed} {Invariant} {Mining} for {Software} {Verification}},
	doi = {10.1109/DATE.2008.4484757},
	abstract = {With the advance of SAT solvers, transforming a software program to a prepositional formula has generated much interest for bounded model checking of software in recent years. However, reasoning at the Boolean level often may not be able to identify some key relations among the original high-level program variables. In this paper, we propose a novel framework that uses simulation-directed data mining in the original program to extract a set of high-level potential property invariants according to the dynamic execution data of the software. When these learned invariants are added as constraints to the bounded model checking instances of the software, they help to significantly reduce the search space. The simulation-directed invariant mining framework exhibits more flexibility compared to the conventional static program analysis approaches, and the experimental results showed that our approach can lead to up to an order of magnitude of speedup in software verification via bounded model checking.},
	booktitle = {2008 {Design}, {Automation} and {Test} in {Europe}},
	author = {Cheng, X. and Hsiao, M. S.},
	month = mar,
	year = {2008},
	keywords = {Analytical models, Automatic logic units, Computational modeling, Computer languages, Computer simulation, Computerized monitoring, Data mining, Embedded software, Embedded system, Hardware, bounded model checking, computability, formal verification, high-level potential property invariants, simulation-directed invariant mining, software verification},
	pages = {682--687},
}

@inproceedings{sun_perses:_2018,
	address = {Gothenburg, Sweden},
	title = {Perses: syntax-guided program reduction},
	isbn = {978-1-4503-5638-1},
	shorttitle = {Perses},
	url = {http://dl.acm.org/citation.cfm?doid=3180155.3180236},
	doi = {10.1145/3180155.3180236},
	abstract = {Given a program P that exhibits a certain property ψ (e.g., a C program that crashes GCC when it is being compiled), the goal of program reduction is to minimize P to a smaller variant P ′ that still exhibits the same property, i.e., ψ (P ′). Program reduction is important and widely demanded for testing and debugging. For example, all compiler/interpreter development projects need effective program reduction to minimize failure-inducing test programs to ease debugging. However, state-of-the-art program reduction techniques — notably Delta Debugging (DD), Hierarchical Delta Debugging (HDD), and C-Reduce — do not perform well in terms of speed (reduction time) and quality (size of reduced programs), or are highly customized for certain languages and thus lack generality. This paper presents Perses, a novel framework for effective, efficient, and general program reduction. The key insight is to exploit, in a general manner, the formal syntax of the programs under reduction and ensure that each reduction step considers only smaller, syntactically valid variants to avoid futile efforts on syntactically invalid variants. Our framework supports not only deletion (as for DD and HDD), but also general, effective program transformations. We have designed and implemented Perses, and evaluated it for two language settings: C and Java. Our evaluation results on 20 C programs triggering bugs in GCC and Clang demonstrate Perses’s strong practicality compared to the state-of-the-art: (1) smaller size — Perses’s results are respectively 2\% and 45\% in size of those from DD and HDD; and (2) shorter reduction time — Perses takes 23\% and 47\% time taken by DD and HDD respectively. Even when compared to the highly customized and optimized C-Reduce for C/C++, Perses takes only 38-60\% reduction time.},
	language = {en},
	urldate = {2019-01-21},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}  - {ICSE} '18},
	publisher = {ACM Press},
	author = {Sun, Chengnian and Li, Yuanbo and Zhang, Qirun and Gu, Tianxiao and Su, Zhendong},
	year = {2018},
	pages = {361--371},
}

@book{hermanns_multi_1999,
	title = {Multi {Terminal} {Binary} {Decision} {Diagrams} to {Represent} and {Analyse} {Continuous} {Time} {Markov} {Chains}},
	abstract = {Binary Decision Diagrams (BDDs) have gained high attention  in the context of design and verification of digital circuits. They  have successfully been employed to encode very large state spaces in an  efficient, symbolic way. Multi terminal BDDs (MTBDDs) are generalisations  of BDDs from Boolean values to values of any finite domain. In  this paper, we investigate the applicability of MTBDDs to the symbolic  representation of continuous time Markov chains, derived from high-level  formalisms, such as queueing networks or process algebras. Based on this  data structure, we discuss iterative solution algorithms to compute the  steady-state probability vector that work in a completely symbolic way.  We highlight a number of lessons learned, using a set of small examples.},
	author = {Hermanns, H. and Meyer-Kayser, J. and Siegle, M.},
	year = {1999},
}

@inproceedings{muppala_stochastic_1994,
	title = {Stochastic {Reward} {Nets} for {Reliability} {Prediction}},
	abstract = {We describe the use of stochastic Petri nets (SPNs) and stochastic reward nets (SRNs) which are SPNs augmented with the ability to specify output measures as reward-based functions, for the evaluation of reliability for complex systems. The solution of SRNs involves generation and analysis of the corresponding Markov reward model. The use of SRNs in modeling complex systems is illustrated through several interesting examples. We mention the use of the Stochastic Petri Net Package (SPNP) for the description and solution of SRN models.},
	author = {Muppala, Jogesh K. and Trivedi, Kishor S.},
	year = {1994},
	keywords = {Complex systems, Ephrin Type-B Receptor 1, human, Markov chain, Recurrent neural network, Stochastic Petri net},
}

@inproceedings{dargenio_reachability_2001,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Reachability {Analysis} of {Probabilistic} {Systems} by {Successive} {Refinements}},
	isbn = {978-3-540-44804-4},
	abstract = {We report on a novel development to model check quantitative reachability properties on Markov decision processes together with its prototype implementation. The innovation of the technique is that the analysis is performed on an abstraction of the model under analysis. Such an abstraction is significantly smaller than the original model and may safely refute or accept the required property. Otherwise, the abstraction is refined and the process repeated. As the numerical analysis necessary to determine the validity of the property is more costly than the refinement process, the technique profits from applying such numerical analysis on smaller state spaces.},
	language = {en},
	booktitle = {Process {Algebra} and {Probabilistic} {Methods}. {Performance} {Modelling} and {Verification}},
	publisher = {Springer Berlin Heidelberg},
	author = {D’Argenio, Pedro R. and Jeannet, Bertrand and Jensen, Henrik E. and Larsen, Kim G.},
	editor = {de Alfaro, Luca and Gilmore, Stephen},
	year = {2001},
	keywords = {Markov Decision Process, Model Check, Reachability Analysis, Reachable State, Simple Path},
	pages = {39--56},
}

@inproceedings{haverkort_use_2000,
	title = {On the use of model checking techniques for dependability evaluation},
	doi = {10.1109/RELDI.2000.885410},
	abstract = {Over the last two decades, many techniques have been developed to specify and evaluate Markovian dependability models. Most often, these Markovian models are automatically derived from stochastic Petri nets, stochastic process algebras or stochastic activity networks. However, whereas the model specification has become very comfortable, the specification of the dependability measures of interest most often has remained fairly cumbersome. In this paper, we show that our recently introduced logic CSL (continuous stochastic logic) provides ample means to specify state- as well as path-based dependability measures in a compact and flexible way. Moreover, due to the formal syntax and semantics of CSL, we can exploit the structure of CSL-specified dependability measures in the dependability evaluation process. Typically, the underlying Markov chains that need to be evaluated can be reduced considerably in size by this structure exploitation.},
	booktitle = {Proceedings 19th {IEEE} {Symposium} on {Reliable} {Distributed} {Systems} {SRDS}-2000},
	author = {Haverkort, B. R. and Hermanns, H. and Katoen, J.-},
	month = oct,
	year = {2000},
	keywords = {Algebra, Computer science, Equations, Linear systems, Logic, Markov chains, Markov processes, Markovian dependability models, Particle measurements, Petri nets, Power system modeling, Steady-state, Stochastic processes, algebraic specification, continuous stochastic logic, dependability evaluation, dependability measures specification, fault tolerance, formal syntax, formal verification, model checking techniques, model specification, path-based dependability measures, process algebra, reliability theory, semantics, state-based dependability measures, stochastic Petri nets, stochastic activity networks, stochastic process algebras, structure exploitation},
	pages = {228--237},
}

@inproceedings{pearson_evaluating_2017,
	title = {Evaluating and {Improving} {Fault} {Localization}},
	doi = {10.1109/ICSE.2017.62},
	abstract = {Most fault localization techniques take as input a faulty program, and produce as output a ranked list of suspicious code locations at which the program may be defective. When researchers propose a new fault localization technique, they typically evaluate it on programs with known faults. The technique is scored based on where in its output list the defective code appears. This enables the comparison of multiple fault localization techniques to determine which one is better. Previous research has evaluated fault localization techniques using artificial faults, generated either by mutation tools or manually. In other words, previous research has determined which fault localization techniques are best at finding artificial faults. However, it is not known which fault localization techniques are best at finding real faults. It is not obvious that the answer is the same, given previous work showing that artificial faults have both similarities to and differences from real faults. We performed a replication study to evaluate 10 claims in the literature that compared fault localization techniques (from the spectrum-based and mutation-based families). We used 2995 artificial faults in 6 real-world programs. Our results support 7 of the previous claims as statistically significant, but only 3 as having non-negligible effect sizes. Then, we evaluated the same 10 claims, using 310 real faults from the 6 programs. Every previous result was refuted or was statistically and practically insignificant. Our experiments show that artificial faults are not useful for predicting which fault localization techniques perform best on real faults. In light of these results, we identified a design space that includes many previously-studied fault localization techniques as well as hundreds of new techniques. We experimentally determined which factors in the design space are most important, using an overall set of 395 real faults. Then, we extended this design space with new techniques. Several of our novel techniques outperform all existing techniques, notably in terms of ranking defective code in the top-5 or top-10 reports.},
	booktitle = {2017 {IEEE}/{ACM} 39th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Pearson, S. and Campos, J. and Just, R. and Fraser, G. and Abreu, R. and Ernst, M. D. and Pang, D. and Keller, B.},
	month = may,
	year = {2017},
	keywords = {Computer bugs, Debugging, Focusing, Java, Maintenance engineering, Manuals, Tools, defective code, design space, fault localization techniques, faulty program, mutation-based families, program testing, replication study, source code (software), spectrum-based families, suspicious code locations},
	pages = {609--620},
}

@article{xu_brief_2005,
	title = {A {Brief} {Survey} of {Program} {Slicing}},
	volume = {30},
	issn = {0163-5948},
	url = {http://doi.acm.org/10.1145/1050849.1050865},
	doi = {10.1145/1050849.1050865},
	abstract = {Program slicing is a technique to extract program parts with respect to some special computation. Since Weiser first proposed the notion of slicing in 1979, hundreds of papers have been presented in this area. Tens of variants of slicing have been studied, as well as algorithms to compute them. Different notions of slicing have different properties and different applications. These notions vary from Weiser's syntax-preserving static slicing to amorphous slicing which is not syntax-preserving, and the algorithms can be based on dataflow equations, information-flow relations or dependence graphs.Slicing was first-developed to facilitate debugging, but it is then found helpful in many aspects of the software development life cycle, including program debugging, software testing, software measurement, program comprehension, software maintenance, program parallelization and so on.Over the last two decades, several surveys on program slicing have been presented. However, most of them only reviewed parts of researches on program slicing or have now been out of date. People who are interested in program slicing need more information about the up to date researches. Our survey fills this gap. In this paper, we briefly review most of existing slicing techniques including static slicing, dynamic slicing and the latest slicing techniques. We also discuss the contribution of each work and compare the major difference between them. Researches on slicing are classified by the research hot spots such that people can be kept informed of the overall program slicing researches.},
	number = {2},
	urldate = {2019-01-14},
	journal = {SIGSOFT Softw. Eng. Notes},
	author = {Xu, Baowen and Qian, Ju and Zhang, Xiaofang and Wu, Zhongqiang and Chen, Lin},
	month = mar,
	year = {2005},
	keywords = {debugging, dependence analysis, pointer analysis, program analysis, program slicing},
	pages = {1--36},
}

@article{hatcliff_slicing_2000,
	title = {Slicing {Software} for {Model} {Construction}},
	volume = {13},
	issn = {1573-0557},
	url = {https://doi.org/10.1023/A:1026599015809},
	doi = {10.1023/A:1026599015809},
	abstract = {Applying finite-state verification techniques (e.g., model checking) to software requires that program source code be translated to a finite-state transition system that safely models program behavior. Automatically checking such a transition system for a correctness property is typically very costly, thus it is necessary to reduce the size of the transition system as much as possible. In fact, it is often the case that much of a program's source code is irrelevant for verifying a given correctness property.In this paper, we apply program slicing techniques to remove automatically such irrelevant code and thus reduce the size of the corresponding transition system models. We give a simple extension of the classical slicing definition, and prove its safety with respect to model checking of linear temporal logic (LTL) formulae. We discuss how this slicing strategy fits into a general methodology for deriving effective software models using abstraction-based program specialization.},
	language = {en},
	number = {4},
	urldate = {2019-01-14},
	journal = {Higher-Order and Symbolic Computation},
	author = {Hatcliff, John and Dwyer, Matthew B. and Zheng, Hongjun},
	month = dec,
	year = {2000},
	keywords = {linear temporal logic, model-checking, program dependence graph, program slicing, software verification, state-space reduction},
	pages = {315--353},
}

@inproceedings{dwyer_evaluating_2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Evaluating the {Effectiveness} of {Slicing} for {Model} {Reduction} of {Concurrent} {Object}-{Oriented} {Programs}},
	isbn = {978-3-540-33057-8},
	abstract = {Model checking techniques have proven effective for checking a number of non-trivial concurrent object-oriented software systems. However, due to the high computational and memory costs, a variety of model reduction techniques are needed to overcome current limitations on applicability and scalability. Conventional wisdom holds that static program slicing can be an effective model reduction technique, yet anecdotal evidence is mixed, and there has been no work that has systematically studied the costs/benefits of slicing for model reduction in the context of model checking source code for realistic systems.In this paper, we present an overview of the sophisticated Indus program slicer that is capable of handling full Java and is readily applicable to interesting off-the-shelf concurrent Java programs. Using the Indus program slicer as part of the next generation of the Bandera model checking framework, we experimentally demonstrate significant benefits from using slicing as a fully automatic model reduction technique. Our experimental results consider a number of Java systems with varying structural properties, the effects of combining slicing with other well-known model reduction techniques such as partial order reductions, and the effects of slicing for different classes of properties. Our conclusions are that slicing concurrent object-oriented source code provides significant reductions that are orthogonal to a number of other reduction techniques, and that slicing should always be applied due to its automation and low computational costs.},
	language = {en},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dwyer, Matthew B. and Hatcliff, John and Hoosier, Matthew and Ranganath, Venkatesh and {Robby} and Wallentine, Todd},
	editor = {Hermanns, Holger and Palsberg, Jens},
	year = {2006},
	keywords = {Call Graph, Java Program, Model Check, Model Reduction, Predicate Abstraction},
	pages = {73--89},
}

@inproceedings{jackson_new_1994,
	address = {New York, NY, USA},
	series = {{SIGSOFT} '94},
	title = {A {New} {Model} of {Program} {Dependences} for {Reverse} {Engineering}},
	isbn = {978-0-89791-691-2},
	url = {http://doi.acm.org/10.1145/193173.195281},
	doi = {10.1145/193173.195281},
	abstract = {A dependence model for reverse engineering should treat procedures in a modular fashion and should be fine-grained, distinguishing dependences that are due to different variables. The program dependence graph (PDG) satisfies neither of these criteria. We present a new form of dependence graph that satisfies both, while retaining the advantages of the PDG: it is easy to construct and allows program slicing to be implemented as a simple graph traversal. We define 'chopping', a generalization of slicing that can express most of its variants, and show that, using our dependence graph, it produces more accurate results than algorithms based directly on the PDG.},
	urldate = {2019-01-11},
	booktitle = {Proceedings of the {2Nd} {ACM} {SIGSOFT} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Jackson, Daniel and Rollins, Eugene J.},
	year = {1994},
	keywords = {dataflow dependence, modularity, program dependence graph, program slicing, reverse engineering, specifications},
	pages = {2--10},
}

@inproceedings{canfora_software_1994,
	title = {Software salvaging based on conditions},
	doi = {10.1109/ICSM.1994.336752},
	abstract = {This paper presents algorithms for isolating reusable functions in large monolithic programs. The functions to be isolated are specified in terms of either pre-conditions or binding conditions, and these are mapped onto predicates on program's variables. Code components whose execution is triggered and/or bound by these predicates are then isolated. Each component is a candidate to implement a reusable function. The algorithms exploit a representation of the subject program in the form of a program dependence graph. This work forms part of RE/sup 2/, a research project that addresses the wider issue of software reuse. RE/sup 2/ project aims to promote the reuse of software through the exploration of reverse engineering and re-engineering techniques to identify and extract reusable software components from existing systems.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	booktitle = {Proceedings 1994 {International} {Conference} on {Software} {Maintenance}},
	author = {{Canfora} and {Cimitile} and Lucia, De and Lucca, Di},
	month = sep,
	year = {1994},
	keywords = {RE/sup 2/, Software design/development, Software fault diagnosis, Software maintenance, Software reusability, code components, large monolithic programs, program dependence graph, program diagnostics, re-engineering techniques, reusable function, reusable functions, reverse engineering, software maintenance, software reusability, software reuse, software salvaging},
	pages = {424--433},
}

@article{hall_automatic_1995,
	title = {Automatic extraction of executable program subsets by simultaneous dynamic program slicing},
	volume = {2},
	issn = {1573-7535},
	url = {https://doi.org/10.1007/BF00873408},
	doi = {10.1007/BF00873408},
	abstract = {Developers of software product families and maintainers of “legacy” software can benefit from the ability to automatically extract a correctly functioning subset of the code of a system that performs a desired subset of its behaviors. This article introduces a technique for automatic subsetting based on computing asimultaneous dynamic program slice of the code for a set of representative inputs. I show first why the naive approach (unioning traditional dynamic slices) fails, then give an abstract algorithm that succeeds in any slicing framework satisfying certain (mild) assumptions. Experiments using an implementation within the ISAT environment indicate that the algorithm consistently produces significantly smaller subsets than three competing approaches. I also discuss how to characterize the subset's correctness on inputs of interest outside the set used to compute the slice.},
	language = {en},
	number = {1},
	urldate = {2019-01-11},
	journal = {Automated Software Engineering},
	author = {Hall, Robert J.},
	month = mar,
	year = {1995},
	keywords = {program slicing, redesign, reuse, subsetting},
	pages = {33--53},
}

@inproceedings{dickinson_finding_2001,
	title = {Finding failures by cluster analysis of execution profiles},
	doi = {10.1109/ICSE.2001.919107},
	abstract = {We experimentally evaluate the effectiveness of using cluster analysis of execution profiles to find failures among the executions induced by a set of potential test cases. We compare several filtering procedures for selecting executions to evaluate for conformance to requirements. Each filtering procedure involves a choice of a sampling strategy and a clustering metric. The results suggest that filtering procedures based on clustering are more effective than simple random sampling for identifying failures in populations of operational executions, with adaptive sampling from clusters being the most effective sampling strategy. The results also suggest that clustering metrics that give extra weight to industrial profile features are most effective. Scatter plots of execution populations, produced by multidimensional scaling, are used to provide intuition for these results.},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Software} {Engineering}. {ICSE} 2001},
	author = {Dickinson, W. and Leon, D. and Fodgurski, A.},
	month = may,
	year = {2001},
	keywords = {Automatic testing, Computer science, Failure analysis, Filtering, Instruments, Multidimensional systems, Personnel, Sampling methods, Scattering, Software testing, adaptive sampling, cluster analysis, clustering metric, execution profiles, experiment, filtering procedures, multidimensional scaling, pattern clustering, program debugging, program testing, random sampling, sampling strategy, scatter plots, software failures, software testing},
	pages = {339--348},
}

@inproceedings{ottenstein_program_1984,
	address = {New York, NY, USA},
	series = {{SDE} 1},
	title = {The {Program} {Dependence} {Graph} in a {Software} {Development} {Environment}},
	isbn = {978-0-89791-131-3},
	url = {http://doi.acm.org/10.1145/800020.808263},
	doi = {10.1145/800020.808263},
	abstract = {The internal program representation chosen for a software development environment plays a critical role in the nature of that environment. A form should facilitate implementation and contribute to the responsiveness of the environment to the user. The program dependence graph (PDG) may be a suitable internal form. It allows programs to be sliced in linear time for debugging and for use by language-directed editors. The slices obtained are more accurate than those obtained with existing methods because I/O is accounted for correctly and irrelevant statements on multi-statement lines are not displayed. The PDG may be interpreted in a data driven fashion or may have highly optimized (including vectorized) code produced from it. It is amenable to incremental data flow analysis, improving response time to the user in an interactive environment and facilitating debugging through data flow anomaly detection. It may also offer a good basis for software complexity metrics, adding to the completeness of an environment based on it.},
	urldate = {2019-01-09},
	booktitle = {Proceedings of the {First} {ACM} {SIGSOFT}/{SIGPLAN} {Software} {Engineering} {Symposium} on {Practical} {Software} {Development} {Environments}},
	publisher = {ACM},
	author = {Ottenstein, Karl J. and Ottenstein, Linda M.},
	year = {1984},
	keywords = {Code optimization, Control flow, Data flow, Debugging, Internal program representation, Interpreter, Program slice, Software complexity metrics},
	pages = {177--184},
}

@article{horwitz_interprocedural_1990,
	title = {Interprocedural {Slicing} {Using} {Dependence} {Graphs}},
	volume = {12},
	issn = {0164-0925},
	url = {http://doi.acm.org/10.1145/77606.77608},
	doi = {10.1145/77606.77608},
	abstract = {The notion of a program slice, originally introduced by Mark Weiser, is useful in program debugging, automatic parallelization, and program integration. A slice of a program is taken with respect to a program point p and a variable x; the slice consists of all statements of the program that might affect the value of x at point p. This paper concerns the problem of interprocedural slicing—generating a slice of an entire program, where the slice crosses the boundaries of procedure calls. To solve this problem, we introduce a new kind of graph to represent programs, called a system dependence graph, which extends previous dependence representations to incorporate collections of procedures (with procedure calls) rather than just monolithic programs. Our main result is an algorithm for interprocedural slicing that uses the new representation. (It should be noted that our work concerns a somewhat restricted kind of slice: rather than permitting a program to b
e sliced with respect to program point p and an arbitrary variable, a slice must be taken with respect to a variable that is defined or used at p.)
The chief difficulty in interprocedural slicing is correctly accounting for the calling context of a called procedure. To handle this problem, system dependence graphs include some data dependence edges that represent transitive dependences due to the effects of procedure calls, in addition to the conventional direct-dependence edges. These edges are constructed with the aid of an auxiliary structure that represents calling and parameter-linkage relationships. This structure takes the form of an attribute grammar. The step of computing the required transitive-dependence edges is reduced to the construction of the subordinate characteristic graphs for the grammar's nonterminals.},
	number = {1},
	urldate = {2019-01-09},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {Horwitz, Susan and Reps, Thomas and Binkley, David},
	month = jan,
	year = {1990},
	pages = {26--60},
}

@inproceedings{venkatesh_semantic_1991,
	address = {New York, NY, USA},
	series = {{PLDI} '91},
	title = {The {Semantic} {Approach} to {Program} {Slicing}},
	isbn = {978-0-89791-428-4},
	url = {http://doi.acm.org/10.1145/113445.113455},
	doi = {10.1145/113445.113455},
	urldate = {2019-01-08},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 1991 {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Venkatesh, G. A.},
	year = {1991},
	pages = {107--119},
}

@inproceedings{weiser_program_1981,
	address = {Piscataway, NJ, USA},
	series = {{ICSE} '81},
	title = {Program {Slicing}},
	isbn = {978-0-89791-146-7},
	url = {http://dl.acm.org/citation.cfm?id=800078.802557},
	abstract = {Program slicing is a method used by experienced computer programmers for abstracting from programs. Starting from a subset of a program's behavior, slicing reduces that program to a minimal form which still produces that behavior. The reduced program, called a “slice”, is an independent program guaranteed to faithfully represent the original program within the domain of the specified subset of behavior. Finding a slice is in general unsolvable. A dataflow algorithm is presented for approximating slices when the behavior subset is specified as the values of a set of variables at a statement. Experimental evidence is presented that these slices are used by programmers during debugging. Experience with two automatic slicing tools is summarized. New measures of program complexity are suggested based on the organization of a program's slices.},
	urldate = {2019-01-08},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Weiser, Mark},
	year = {1981},
	keywords = {Data flow analysis, Debugging, Human factors, Program maintenance, Program metrics, Software tools},
	pages = {439--449},
}

@inproceedings{islam_porbs:_2016,
	title = {{PORBS}: {A} parallel observation-based slicer},
	shorttitle = {{PORBS}},
	doi = {10.1109/ICPC.2016.7503745},
	abstract = {This paper presents PORBS, a parallelised observation-based slicing tool. The tool itself is written in Java making it platform independent and leverages the build chain of the system being sliced to avoid the need to replicate complex compiler analysis. The target audience of PORBS is software engineers and researchers working with and on tools and techniques for software comprehension, debugging, re-engineering, and maintenance.},
	booktitle = {2016 {IEEE} 24th {International} {Conference} on {Program} {Comprehension} ({ICPC})},
	author = {Islam, S. and Binkley, D.},
	month = may,
	year = {2016},
	keywords = {Debugging, Hardware, Instruction sets, Java, Java language, Maintenance engineering, PORBS tool, Software engineering, parallel programming, parallelised observation-based slicing tool, program debugging, program slicing, software comprehension, software debugging, software maintenance, software re-engineering},
	pages = {1--3},
}

@inproceedings{binkley_orbs_2015,
	title = {{ORBS} and the limits of static slicing},
	doi = {10.1109/SCAM.2015.7335396},
	abstract = {Observation-based slicing is a recently-introduced, language-independent slicing technique based on the dependencies observable from program behaviour. Due to the well-known limits of dynamic analysis, we may only compute an under-approximation of the true observation-based slice. However, because the observation-based slice captures all possible dependence that can be observed, even such approximations can yield insight into the limitations of static slicing. For example, a static slice, S, that is strictly smaller than the corresponding observation based slice is potentially unsafe. We present the results of three sets of experiments on 12 different programs, including benchmarks and larger programs, which investigate the relationship between static and observation-based slicing. We show that, in extreme cases, observation-based slices can find the true minimal static slice, where static techniques cannot. For more typical cases, our results illustrate the potential for observation-based slicing to highlight limitations in static slicers. Finally, we report on the sensitivity of observation-based slicing to test quality.},
	booktitle = {2015 {IEEE} 15th {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation} ({SCAM})},
	author = {Binkley, D. and Gold, N. and Harman, M. and Islam, S. and Krinke, J. and Yoo, S.},
	month = sep,
	year = {2015},
	keywords = {Algorithm design and analysis, Approximation methods, Benchmark testing, Heuristic algorithms, ORBS, Performance analysis, Semantics, Trajectory, dynamic analysis, language-independent slicing technique, observation-based slice underapproximation, observation-based slicing, program behaviour, program slicing, program testing, quality testing, software quality, static slicing limits},
	pages = {1--10},
}

@inproceedings{song_sos_2017,
	title = {{SoS} {GaP} {Slicer}: {Slicing} {SoS} {Goal} and {PRISM} {Models} for {Change}-{Responsive} {Verification} of {SoS}},
	shorttitle = {{SoS} {GaP} {Slicer}},
	doi = {10.1109/APSEC.2017.63},
	abstract = {A System-of-Systems (SoS) is a collection of systems, which consists of independent constituent systems to achieve higher-level goals. As an SoS changes constantly due to external and internal factors, dynamic reconfiguration and evolutionary development must be performed effectively. To manage an SoS that has these characteristics, SoS managers and engineers need to model and verify an SoS that accommodates changes. While many researchers have proposed techniques for modeling and verifying an SoS, little attention has been paid to the verification of a continuously changing SoS. Since the size and complexity of SoSs are too great to apply existing verification techniques, an efficient verification method for dynamically changing SoSs is required. If we can detect the change-related goals of an SoS and the corresponding change-affected parts in an SoS model, we can slice the SoS model in our area of concern and verify the SoS model efficiently. This paper proposes SoS GaP slicer, a verification method that considers the dynamic reconfiguration and evolution of an SoS. Our method could save the cost of verifying the whole of an SoS model by an efficient statistical model checking for SoSs. The experimental results show the accuracy and efficiency of the proposed method.},
	booktitle = {2017 24th {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Song, J. and Baek, Y. and Jin, M. and Jee, E. and Bae, D.},
	month = dec,
	year = {2017},
	keywords = {Cascading style sheets, Complexity theory, Helicopters, Hospitals, Model checking, PRISM models, Probabilistic logic, SoS GaP slicer, SoS changes, SoS model, Synchronization, System of Systems (SoS), System of systems verification, System-of-Systems, change-responsive verification, dynamic reconfiguration, evolutionary development, independent constituent systems, model slicing, program slicing, program verification, statistical analysis, statistical model checking, systems engineering},
	pages = {546--551},
}

@inproceedings{binkley_formalizing_2004,
	title = {Formalizing executable dynamic and forward slicing},
	doi = {10.1109/SCAM.2004.13},
	abstract = {This paper uses a projection theory of slicing to formalize the definition of executable dynamic and forward program slicing. Previous definitions, when given, have been operational, and previous descriptions have been algorithmic. The projection framework is used to provide a declarative formulation in terms of the different equivalences preserved by the different forms of slicing. The analysis of dynamic slicing reveals that the slicing criterion introduced by Korel and Laski contains three inter-woven criteria. It is shown how these three conceptually distinct criteria can be disentangled to reveal two new criteria. The analysis of dynamic slicing also reveals that the subsumes relationship between static and dynamic slicing is more intricate that previous authors have claimed. Finally, the paper uses the projection theory to investigate theoretical properties of forward slicing. This is achieved by first re-formulating forward slicing to provide an executable forward slice. This definition allows for formal investigation of the relationship between forward and backward slicing},
	booktitle = {Fourth {IEEE} {International} {Workshop} on {Source} {Code} {Analysis} and {Manipulation}},
	author = {Binkley, D. and Danicic, S. and Gyimothy, T. and Harman, M. and Kiss, A. and Ouarbya, L.},
	month = sep,
	year = {2004},
	keywords = {Amorphous materials, Conferences, Educational institutions, Informatics, Testing, backward slicing, dynamic program slicing, forward program slicing, program slicing, projection slicing theory},
	pages = {43--52},
}

@article{aung_executable_nodate,
	title = {Executable {Slicing} via {Procedure} {Specialization}},
	abstract = {Although Weiser originally deﬁned a program slice to be an executable projection of a program, much of the research on slicing has focused on closure slices, which consist of the set of statements and conditions of the program that might affect the value of a given variable at a given statement or condition of interest. While closure slices can be useful, there are some contexts in which executable slices are preferable. Closure slices are not generally executable because there can be mismatches in the slice between the sets of actual parameters at different call-sites to a procedure p and the formal parameters of p.},
	language = {en},
	author = {Aung, Min and Horwitz, Susan and Joiner, Rich and Reps, Thomas},
	pages = {19},
}

@inproceedings{danicic_building_2004,
	title = {Building executable union slices using conditioned slicing},
	doi = {10.1109/WPC.2004.1311051},
	abstract = {Program slicing can be used as a support for program comprehension, because it allows a large program to be divided up into smaller slices, each of which can be understood in isolation from the rest. As such, slicing facilitates the familiar approach of 'divide and conquer'. Union slicing (the union of dynamic slices) is a useful technique for approximating a precise static slice. For program comprehension (and many other applications) it is often important that the union slice be an executable program, rather than merely a collection of statements which are relevant to the slicing criterion. This paper presents an algorithm for computing executable union slices, using conditioned slicing. A case study is used to illustrate the algorithm and how the executable union slice is preferable to the (possibly nonexecutable) union slice. The paper also shows, briefly, that the approach has wider applications than comprehension.},
	booktitle = {Proceedings. 12th {IEEE} {International} {Workshop} on {Program} {Comprehension}, 2004.},
	author = {Danicic, S. and Lucia, A. De and Harman, M.},
	month = jun,
	year = {2004},
	keywords = {Conferences, Debugging, Educational institutions, Information systems, Terminology, divide-and-conquer approach, program comprehension, program slicing, program understanding, reverse engineering, union slicing},
	pages = {89--97},
}

@article{binkley_precise_1993,
	title = {Precise {Executable} {Interprocedural} {Slices}},
	volume = {2},
	issn = {1057-4514},
	url = {http://doi.acm.org/10.1145/176454.176473},
	doi = {10.1145/176454.176473},
	abstract = {The notion of a program slice, originally introduced by Mark Weiser, is useful in program debugging, automatic parallelization, program integration, and software maintenance. A slice of a program is taken with respect to a program point p and a variable x; the slice consists of all statements of the program that might affect the value of x at point p. An interprocedural slice is a slice of an entire program, where the slice crosses the boundaries of procedure calls.
Weiser's original interprocedural-slicing algorithm produces imprecise slices that are executable programs. A recent algorithm developed by Horwitz, Reps, and Binkley produces more precise (smaller) slices by more accurately identifying those statements that might affect the values of x at point p. These slices, however, are not executable. An extension to their algorithm that produces more precise executable interprocedural slices is described together with a proof of correctness for the new algorithm.},
	number = {1-4},
	urldate = {2019-01-07},
	journal = {ACM Lett. Program. Lang. Syst.},
	author = {Binkley, David},
	month = mar,
	year = {1993},
	keywords = {control dependence, data dependence, program dependence graph, program slicing},
	pages = {31--45},
}

@article{harman_overview_2001,
	title = {An {Overview} of {Program} {Slicing}},
	volume = {2},
	doi = {10.1002/swf.41},
	abstract = {Introduction Program slicing is a technique for simplifying programs by focusing on selected aspects of semantics. The process of slicing deletes those parts of the program which can be determined to have no effect upon the semantics of interest. Slicing has applications in testing and debugging, re-engineering, program comprehension and software measurement. For example, in debugging, there is little point in the (human) debugger analysing sections of the source code which cannot have caused the bug. Slicing avoids this by removing these parts of the program, focusing attention on those parts of the program which may contain a fault. This article reviews three semantic paradigms for slicing: static, dynamic and conditioned and two syntactic paradigms: syntax-preserving and amorphous. Slicing has been applied to many software development problems including testing, reuse, maintenance and evolution. This paper describes the main forms of program slice and some of the applicati},
	journal = {Software Focus},
	author = {Harman, Mark and Hierons, Robert},
	month = dec,
	year = {2001},
}

@inproceedings{liblit_scalable_2005,
	address = {New York, NY, USA},
	series = {{PLDI} '05},
	title = {Scalable {Statistical} {Bug} {Isolation}},
	isbn = {978-1-59593-056-9},
	url = {http://doi.acm.org/10.1145/1065010.1065014},
	doi = {10.1145/1065010.1065014},
	abstract = {We present a statistical debugging algorithm that isolates bugs in programs containing multiple undiagnosed bugs. Earlier statistical algorithms that focus solely on identifying predictors that correlate with program failure perform poorly when there are multiple bugs. Our new technique separates the effects of different bugs and identifies predictors that are associated with individual bugs. These predictors reveal both the circumstances under which bugs occur as well as the frequencies of failure modes, making it easier to prioritize debugging efforts. Our algorithm is validated using several case studies, including examples in which the algorithm identified previously unknown, significant crashing bugs in widely used systems.},
	urldate = {2018-12-31},
	booktitle = {Proceedings of the 2005 {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Liblit, Ben and Naik, Mayur and Zheng, Alice X. and Aiken, Alex and Jordan, Michael I.},
	year = {2005},
	keywords = {bug isolation, feature selection, invariants, random sampling, statistical debugging},
	pages = {15--26},
}

@inproceedings{liu_sober:_2005,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE}-13},
	title = {{SOBER}: {Statistical} {Model}-based {Bug} {Localization}},
	isbn = {978-1-59593-014-9},
	shorttitle = {{SOBER}},
	url = {http://doi.acm.org/10.1145/1081706.1081753},
	doi = {10.1145/1081706.1081753},
	abstract = {Automated localization of software bugs is one of the essential issues in debugging aids. Previous studies indicated that the evaluation history of program predicates may disclose important clues about underlying bugs. In this paper, we propose a new statistical model-based approach, called SOBER, which localizes software bugs without any prior knowledge of program semantics. Unlike existing statistical debugging approaches that select predicates correlated with program failures, SOBER models evaluation patterns of predicates in both correct and incorrect runs respectively and regards a predicate as bug-relevant if its evaluation pattern in incorrect runs differs significantly from that in correct ones. SOBER features a principled quantification of the pattern difference that measures the bug-relevance of program predicates.We systematically evaluated our approach under the same setting as previous studies. The result demonstrated the power of our approach in bug localization: SOBER can help programmers locate 68 out of 130 bugs in the Siemens suite when programmers are expected to examine no more than 10\% of the code, whereas the best previously reported is 52 out of 130. Moreover, with the assistance of SOBER, we found two bugs in bc 1.06 (an arbitrary precision calculator on UNIX/Linux), one of which has never been reported before.},
	urldate = {2018-12-31},
	booktitle = {Proceedings of the 10th {European} {Software} {Engineering} {Conference} {Held} {Jointly} with 13th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Liu, Chao and Yan, Xifeng and Fei, Long and Han, Jiawei and Midkiff, Samuel P.},
	year = {2005},
	keywords = {localization metrics, statistical debugging},
	pages = {286--295},
}

@inproceedings{zeller_isolating_2002,
	title = {Isolating cause-effect chains from computer programs},
	doi = {10.1145/605466.605468},
	abstract = {Consider the execution of a failing program as a sequence of program states. Each state induces the following state, up to the failure. Which variables and values of a program state are relevant for the failure? We show how the Delta Debugging algorithm isolates the relevant variables and values by systematically narrowing the state difference between a passing run and a failing run--by assessing the outcome of altered executions to determine wether a change in the program state makes a difference in the test outcome. Applying Delta Debugging to multiple states of the program automatically reveals the cause-effect chain of the failure--that is, the variables and values that caused the failure.In a case study, our prototype implementation successfully isolated the cause-effect chain for a failure of the GNU C compiler: "Initially, the C program to be compiled contained an addition of 1.0; this caused an addition operator in the intermediate RTL representation; this caused a cycle in the RTL tree--and this caused the compiler to crash."},
	booktitle = {{SOEN}},
	author = {Zeller, Andreas},
	year = {2002},
	keywords = {Algorithm, Computer program, Delta Debugging, Failure, GNU Compiler Collection, Prototype, State (computer science)},
}

@inproceedings{abreu_evaluation_2006,
	title = {An {Evaluation} of {Similarity} {Coefficients} for {Software} {Fault} {Localization}},
	doi = {10.1109/PRDC.2006.18},
	abstract = {Automated diagnosis of software faults can improve the efficiency of the debugging process, and is therefore an important technique for the development of dependable software. In this paper we study different similarity coefficients that are applied in the context of a program spectral approach to software fault localization (single programming mistakes). The coefficients studied are taken from the systems diagnosis/automated debugging tools Pinpoint, Tarantula, and AMPLE, and from the molecular biology domain (the Ochiai coefficient). We evaluate these coefficients on the Siemens Suite of benchmark faults, and assess their effectiveness in terms of the position of the actual fault in the probability ranking of fault candidates produced by the diagnosis technique. Our experiments indicate that the Ochiai coefficient consistently outperforms the coefficients currently used by the tools mentioned. In terms of the amount of code that needs to be inspected, this coefficient improves 5\% on average over the next best technique, and up to 30\% in specific cases},
	booktitle = {2006 12th {Pacific} {Rim} {International} {Symposium} on {Dependable} {Computing} ({PRDC}'06)},
	author = {Abreu, R. and Zoeteweij, P. and Gemund, A. J. c Van},
	month = dec,
	year = {2006},
	keywords = {AMPLE, Computer bugs, Computer science, Embedded system, Fault diagnosis, Mathematics, Ochiai coefficient, Particle measurements, Pinpoint, Siemens Suite, Software debugging, Software reliability, Software testing, System testing, Tarantula, automated debugging tools, automated software fault diagnosis, benchmark faults, dependable software, fault diagnosis, molecular biology, probability, probability ranking, program debugging, program diagnostics, program spectral approach, similarity coefficients, software fault localization, software fault tolerance},
	pages = {39--46},
}

@inproceedings{reps_use_1997,
	address = {New York, NY, USA},
	series = {{ESEC} '97/{FSE}-5},
	title = {The {Use} of {Program} {Profiling} for {Software} {Maintenance} with {Applications} to the {Year} 2000 {Problem}},
	isbn = {978-3-540-63531-4},
	url = {http://dx.doi.org/10.1145/267895.267925},
	doi = {10.1145/267895.267925},
	urldate = {2018-12-31},
	booktitle = {Proceedings of the 6th {European} {SOFTWARE} {ENGINEERING} {Conference} {Held} {Jointly} with the 5th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {Springer-Verlag New York, Inc.},
	author = {Reps, Thomas and Ball, Thomas and Das, Manuvir and Larus, James},
	year = {1997},
	pages = {432--449},
}

@article{yang_incrementalizing_nodate,
	title = {Incrementalizing {MCMC} in {Probabilistic} {Programs} {Through} {Tracing} and {Slicing}},
	abstract = {Probabilistic programming enables high-level speciﬁcation of complex probabilistic models without the need for hand-built inference techniques. We present a dynamic compilation technique for increasing the efﬁciency of Markov Chain Monte Carlo (MCMC) inference on probabilistic programs. We focus on Church, a probabilistic extension of untyped call-by-value lambda calculus. MCMC in this setting is a random walk over program paths; it is extremely expensive because each iteration of MCMC requires a complete execution of the program with side computations that track random choices and probabilities. However, there exist many opportunities to remove redundant computation. In particular, we exploit the fact that only some random choices inﬂuence control ﬂow. Partially evaluating these structural choices away results in a trace that can be optimized via existing techniques from JIT compilers and incremental computation. This minimizes the amount of work done at each step in the random walk. We evaluate this technique on several representative probabilistic models, ﬁnding that we achieve orders of magnitude speedup compared with unoptimized Church MCMC. Furthermore, our technique becomes competitive with inference engines for more specialized and less expressive statistical languages.},
	language = {en},
	author = {Yang, Lingfeng and Yeh, Yi-Ting and Hanrahan, Pat and Goodman, Noah D},
	pages = {10},
}

@inproceedings{chen_metamorphic_2012,
	title = {Metamorphic {Testing}: {Applications} and {Integration} with {Other} {Methods}: {Tutorial} {Synopsis}},
	shorttitle = {Metamorphic {Testing}},
	doi = {10.1109/QSIC.2012.21},
	abstract = {In software testing, an oracle refers to a mechanism against which testers can decide whether or not outcomes of test case executions are correct. The oracle problem refers to situations when either an oracle is not available, or it is too expensive to apply. Metamorphic testing has emerged as an effective and efficient approach to alleviating the oracle problem. This article introduces the basic concepts and procedures of metamorphic testing, and gives examples to show its applications, and integration with other methods.},
	booktitle = {2012 12th {International} {Conference} on {Quality} {Software}},
	author = {Chen, T. Y. and Kuo, F. and Towey, D. and Zhou, Z. Q.},
	month = aug,
	year = {2012},
	keywords = {Debugging, Flyback transformers, Production facilities, Software, Software testing, Wireless communication, metamorphic testing, oracle problem, program testing, software testing, test case executions},
	pages = {285--288},
}

@article{xie_metamorphic_2013,
	title = {Metamorphic slice: {An} application in spectrum-based fault localization},
	volume = {55},
	issn = {0950-5849},
	shorttitle = {Metamorphic slice},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584912001759},
	doi = {10.1016/j.infsof.2012.08.008},
	abstract = {Context
Because of its simplicity and effectiveness, Spectrum-Based Fault Localization (SBFL) has been one of the popular approaches towards fault localization. It utilizes the execution result of failure or pass, and the corresponding coverage information (such as program slice) to estimate the risk of being faulty for each program entity (such as statement). However, all existing SBFL techniques assume the existence of a test oracle to determine the execution result of a test case. But, it is common that test oracles do not exist, and hence the applicability of SBFL has been severely restricted.
Objective
We aim at developing a framework that can extend the application of SBFL to the common situations where test oracles do not exist.
Method
Our approach uses a new concept of metamorphic slice resulting from the integration of metamorphic testing and program slicing. In SBFL, instead of using the program slice and the result of failure or pass for an individual test case, a metamorphic slice and the result of violation or non-violation of a metamorphic relation are used. Since we need not know the execution result for an individual test case, the existence of a test oracle is no longer a requirement to apply SBFL.
Results
An experimental study involving nine programs and three risk evaluation formulas was conducted. The results show that our proposed solution delivers a performance comparable to the performance observed by existing SBFL techniques for the situations where test oracles exist.
Conclusion
With respect to the problem that SBFL is only applicable to programs with test oracles, we propose an innovative solution. Our solution is not only intuitively appealing and conceptually feasible, but also practically effective. Consequently, test oracles are no longer mandatory for SBFL, and hence the applicability of SBFL is significantly extended.},
	number = {5},
	urldate = {2018-12-18},
	journal = {Information and Software Technology},
	author = {Xie, Xiaoyuan and Wong, W. Eric and Chen, Tsong Yueh and Xu, Baowen},
	month = may,
	year = {2013},
	keywords = {Metamorphic slice, Metamorphic testing, Slice, Spectrum-based fault localization, Test oracle},
	pages = {866--879},
}

@article{yang_generating_nodate,
	title = {Generating {Eﬃcient} {MCMC} {Kernels} from {Probabilistic} {Programs}},
	abstract = {Universal probabilistic programming languages (such as Church [6]) trade performance for abstraction: any model can be represented compactly as an arbitrary stochastic computation, but costly online analyses are required for inference. We present a technique that recovers hand-coded levels of performance from a universal probabilistic language, for the Metropolis-Hastings (MH) MCMC inference algorithm. It takes a Church program as input and traces its execution to remove computation overhead. It then analyzes the trace for each proposal, using slicing, to identify the minimal computation needed to evaluate the MH acceptance probability. Generated incremental code is much faster than a baseline implementation (up to 600x) and usually as fast as handcoded MH kernels.},
	language = {en},
	author = {Yang, Lingfeng and Hanrahan, Pat and Goodman, Noah D},
	pages = {9},
}

@inproceedings{hur_slicing_2014,
	address = {New York, NY, USA},
	series = {{PLDI} '14},
	title = {Slicing {Probabilistic} {Programs}},
	isbn = {978-1-4503-2784-8},
	url = {http://doi.acm.org/10.1145/2594291.2594303},
	doi = {10.1145/2594291.2594303},
	abstract = {Probabilistic programs use familiar notation of programming languages to specify probabilistic models. Suppose we are interested in estimating the distribution of the return expression r of a probabilistic program P. We are interested in slicing the probabilistic program P and obtaining a simpler program Sli(P) which retains only those parts of P that are relevant to estimating r, and elides those parts of P that are not relevant to estimating r. We desire that the Sli transformation be both correct and efficient. By correct, we mean that P and Sli(P) have identical estimates on r. By efficient, we mean that estimation over Sli(P) be as fast as possible. We show that the usual notion of program slicing, which traverses control and data dependencies backward from the return expression r, is unsatisfactory for probabilistic programs, since it produces incorrect slices on some programs and sub-optimal ones on others. Our key insight is that in addition to the usual notions of control dependence and data dependence that are used to slice non-probabilistic programs, a new kind of dependence called observe dependence arises naturally due to observe statements in probabilistic programs. We propose a new definition of Sli(P) which is both correct and efficient for probabilistic programs, by including observe dependence in addition to control and data dependences for computing slices. We prove correctness mathematically, and we demonstrate efficiency empirically. We show that by applying the Sli transformation as a pre-pass, we can improve the efficiency of probabilistic inference, not only in our own inference tool R2, but also in other systems for performing inference such as Church and Infer.NET.},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the 35th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Hur, Chung-Kil and Nori, Aditya V. and Rajamani, Sriram K. and Samuel, Selva},
	year = {2014},
	keywords = {Bayesian reasoning, probabilistic programming, program slicing},
	pages = {133--144},
}

@inproceedings{wong_generalizing_2011,
	address = {Lawrence, KS, USA},
	title = {Generalizing evolutionary coupling with stochastic dependencies},
	isbn = {978-1-4577-1639-3 978-1-4577-1638-6},
	url = {http://ieeexplore.ieee.org/document/6100065/},
	doi = {10.1109/ASE.2011.6100065},
	urldate = {2018-12-18},
	booktitle = {2011 26th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE} 2011)},
	publisher = {IEEE},
	author = {Wong, Sunny and Cai, Yuanfang},
	month = nov,
	year = {2011},
	pages = {293--302},
}

@article{whittaker_markov_1994,
	title = {A {Markov} chain model for statistical software testing},
	volume = {20},
	issn = {0098-5589},
	doi = {10.1109/32.328991},
	abstract = {Statistical testing of software establishes a basis for statistical inference about a software system's expected field quality. This paper describes a method for statistical testing based on a Markov chain model of software usage. The significance of the Markov chain is twofold. First, it allows test input sequences to be generated from multiple probability distributions, making it more general than many existing techniques. Analytical results associated with Markov chains facilitate informative analysis of the sequences before they are generated, indicating how the test is likely to unfold. Second, the test input sequences generated from the chain and applied to the software are themselves a stochastic model and are used to create a second Markov chain to encapsulate the history of the test, including any observed failure information. The influence of the failures is assessed through analytical computations on this chain. We also derive a stopping criterion for the testing process based on a comparison of the sequence generating properties of the two chains.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	number = {10},
	journal = {IEEE Transactions on Software Engineering},
	author = {Whittaker, J. A. and Thomason, M. G.},
	month = oct,
	year = {1994},
	keywords = {Failure analysis, History, Markov chain model, Markov processes, Performance evaluation, Probability distribution, Software quality, Software systems, Software testing, Statistical analysis, Stochastic processes, multiple probability distributions, probability, program testing, sequence generating properties, software failures, software field quality, software quality, software usage, statistical inference, statistical software testing, stochastic model, test input sequences, testing process},
	pages = {812--824},
}

@inproceedings{patrick_toolkit_2017,
	title = {A {Toolkit} for {Testing} {Stochastic} {Simulations} against {Statistical} {Oracles}},
	doi = {10.1109/ICST.2017.50},
	abstract = {Stochastic simulations are developed and employed across many fields, to advise governmental policy decisions and direct future research. Faulty simulation software can have serious consequences, but its correctness is difficult to determine due to complexity and random behaviour. Stochastic simulations may output a different result each time they are run, whereas most testing techniques are designed for programs which (for a given set of inputs) always produce the same behaviour. In this paper, we introduce a new approach towards testing stochastic simulations using statistical oracles and transition probabilities. Our approach was implemented as a toolkit, which allows the frequency of state transitions to be tested, along with their final output distribution. We evaluated our toolkit on eight simulation programs from a variety fields and found it can detect errors at least three times smaller (and in one case, over 1000 times smaller) than a conventional (tolerance threshold) approach.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Patrick, M. and Donnelly, R. and Gilligan, C. A.},
	month = mar,
	year = {2017},
	keywords = {Data models, Predator prey systems, Predictive models, Software, Stochastic processes, Testing, Time-frequency analysis, faulty simulation software, probability, program testing, software testing, statistical oracles, statistical tests, stochastic models, stochastic simulations, testing techniques, transition probabilities},
	pages = {448--453},
}

@inproceedings{kanewala_using_2013,
	title = {Using machine learning techniques to detect metamorphic relations for programs without test oracles},
	doi = {10.1109/ISSRE.2013.6698899},
	abstract = {Much software lacks test oracles, which limits automated testing. Metamorphic testing is one proposed method for automating the testing process for programs without test oracles. Unfortunately, finding the appropriate metamorphic relations required for use in metamorphic testing remains a labor intensive task, which is generally performed by a domain expert or a programmer. In this work we present a novel approach for automatically predicting metamorphic relations using machine learning techniques. Our approach uses a set of features developed using the control flow graph of a function for predicting likely metamorphic relations. We show the effectiveness of our method using a set of real world functions often used in scientific applications.},
	booktitle = {2013 {IEEE} 24th {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Kanewala, U. and Bieman, J. M.},
	month = nov,
	year = {2013},
	keywords = {Arrays, Decision trees, Feature extraction, Machine learning, Metamorphic relation, Metamorphic testing, Mutation analysis, Predictive models, Scientific software testing, Software, Software testing, Support vector machines, Test oracles, Testing, control flow graph, data flow graphs, decision trees, learning (artificial intelligence), machine learning techniques, metamorphic relation detection, metamorphic testing, mutation analysis, natural sciences computing, program testing, scientific software testing, support vector machines, testing process automation},
	pages = {1--10},
}

@inproceedings{yoo_metamorphic_2010,
	title = {Metamorphic {Testing} of {Stochastic} {Optimisation}},
	doi = {10.1109/ICSTW.2010.26},
	abstract = {Testing stochastic optimisation algorithms presents an unique challenge because of two reasons. First, these algorithms are non-testable programs, i.e. if the test oracle was known, there wouldn't have been the need for those algorithms in the first place. Second, their performance can vary depending on the problem instances they are used to solve. This paper applies the statistical metamorphic testing approach to stochastic optimisation algorithms and investigates the impact that different problem instances have on testing optimisation algorithms. The paper presents an empirical evaluation of the approach using instances of Next Release Problem (NRP). The effectiveness of the testing method is evaluated using mutation testing. The result shows that, despite the challenges from the stochastic nature of the optimisation algorithm, metamorphic testing can be effective in testing them.},
	booktitle = {2010 {Third} {International} {Conference} on {Software} {Testing}, {Verification}, and {Validation} {Workshops}},
	author = {Yoo, S.},
	month = apr,
	year = {2010},
	keywords = {Application software, Educational institutions, Genetic mutations, Life testing, Machine learning algorithms, Software algorithms, Software engineering, Software testing, Software tools, Stochastic processes, metamorphic testing, mutation testing, next release problem, program testing, search-based software engineering, software engineering, statistical analysis, statistical metamorphic testing approach, stochastic optimisation, stochastic programming, test oracle},
	pages = {192--201},
}

@article{mohapatra_distributed_2006,
	title = {Distributed dynamic slicing of {Java} programs},
	volume = {79},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121206000069},
	doi = {10.1016/j.jss.2006.01.009},
	abstract = {We propose a novel dynamic slicing technique for distributed Java programs. We first construct the intermediate representation of a distributed Java program in the form of a set of Distributed Program Dependence Graphs (DPDG). We mark and unmark the edges of the DPDG appropriately as and when dependencies arise and cease during run-time. Our algorithm can run parallely on a network of computers, with each node in the network contributing to the dynamic slice in a fully distributed fashion. Our approach does not require any trace files to be maintained. Another advantage of our approach is that a slice is available even before a request for a slice is made. This appreciably reduces the response time of slicing commands. We have implemented the algorithm in a distributed environment. The results obtained from our experiments show promise.},
	number = {12},
	urldate = {2018-12-18},
	journal = {Journal of Systems and Software},
	author = {Mohapatra, Durga P. and Kumar, Rajeev and Mall, Rajib and Kumar, D. S. and Bhasin, Mayank},
	month = dec,
	year = {2006},
	keywords = {Debugging, Distributed programming, Dynamic slicing, Java, Multithreading, Object-oriented program, Program dependence graph, Program slicing},
	pages = {1661--1678},
}

@article{zhong_kernelet:_2014,
	title = {Kernelet: {High}-{Throughput} {GPU} {Kernel} {Executions} with {Dynamic} {Slicing} and {Scheduling}},
	volume = {25},
	issn = {1045-9219},
	shorttitle = {Kernelet},
	doi = {10.1109/TPDS.2013.257},
	abstract = {Graphics processors, or GPUs, have recently been widely used as accelerators in shared environments such as clusters and clouds. In such shared environments, many kernels are submitted to GPUs from different users, and throughput is an important metric for performance and total ownership cost. Despite recently improved runtime support for concurrent GPU kernel executions, the GPU can be severely underutilized, resulting in suboptimal throughput. In this paper, we propose Kernelet, a runtime system to improve the throughput of concurrent kernel executions on the GPU. Kernelet embraces transparent memory management and PCI-e data transfer techniques, and dynamic slicing and scheduling techniques for kernel executions. With slicing, Kernelet divides a GPU kernel into multiple sub-kernels (namely slices ). Each slice has tunable occupancy to allow co-scheduling with other slices for high GPU utilization. We develop a novel Markov chain-based performance model to guide the scheduling decision. Our experimental results demonstrate up to 31 percent and 23 percent performance improvement on NVIDIA Tesla C2050 and GTX680 GPUs, respectively.},
	number = {6},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Zhong, J. and He, B.},
	month = jun,
	year = {2014},
	keywords = {GPGPU, GTX680 GPUs, Graphics processing units, Instruction sets, Kernel, Kernel slicing, Kernelet, Markov chain, Markov chain-based performance model, Markov processes, Memory management, NVIDIA Tesla C2050, Optimal scheduling, PCI-e data transfer techniques, Runtime, Throughput, concurrency control, concurrent GPU kernel executions, dynamic scheduling techniques, dynamic slicing techniques, graphics processing units, graphics processors, high-throughput GPU kernel executions, operating system kernels, performance evaluation, performance modeling, processor scheduling, program slicing, runtime support, runtime system, shared environments, storage management, suboptimal throughput, task scheduling, total ownership cost, transparent memory management},
	pages = {1522--1532},
}

@inproceedings{guderlei_statistical_2007,
	title = {Statistical {Metamorphic} {Testing} {Testing} {Programs} with {Random} {Output} by {Means} of {Statistical} {Hypothesis} {Tests} and {Metamorphic} {Testing}},
	doi = {10.1109/QSIC.2007.4385527},
	abstract = {Testing software with random output is a challenging task as the output corresponding to a given input differs from execution to execution. Therefore, the usual approaches to software testing are not applicable to randomized software. Instead, statistical hypothesis tests have been proposed for testing those applications. To apply these statistical hypothesis tests, either knowledge about the theoretical values of statistical characteristics of the program output (e. g. the mean) or a reference implementation (e. g. a legacy system) are required to apply statistical hypothesis tests. But often, both are not available. In the present paper, it is discussed how a testing method called Metamorphic Testing can be used to construct statistical hypothesis tests without knowing exact theoretical characteristics or having a reference implementation. For that purpose, two or more independent output sequences are generated by the implementation under test (IUT). Then, these sequences are compared according to the metamorphic relation using statistical hypothesis tests.},
	booktitle = {Seventh {International} {Conference} on {Quality} {Software} ({QSIC} 2007)},
	author = {Guderlei, R. and Mayer, J.},
	month = oct,
	year = {2007},
	keywords = {Collaborative software, Context modeling, Error correction, Investments, Probability, Random variables, Software quality, Software testing, Statistical analysis, Statistical distributions, implementation under test, program testing, random output, randomized software, software testing, statistical analysis, statistical hypothesis tests, statistical metamorphic testing},
	pages = {404--409},
}

@article{xu_scalable_2014,
	title = {Scalable {Runtime} {Bloat} {Detection} {Using} {Abstract} {Dynamic} {Slicing}},
	volume = {23},
	issn = {1049331X},
	url = {http://dl.acm.org/citation.cfm?doid=2628068.2560047},
	doi = {10.1145/2560047},
	language = {en},
	number = {3},
	urldate = {2018-12-17},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Xu, Guoqing and Mitchell, Nick and Arnold, Matthew and Rountev, Atanas and Schonberg, Edith and Sevitsky, Gary},
	month = jun,
	year = {2014},
	pages = {1--50},
}

@inproceedings{hatcliff_formal_1999,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Formal} {Study} of {Slicing} for {Multi}-threaded {Programs} with {JVM} {Concurrency} {Primitives}},
	isbn = {978-3-540-48294-9},
	abstract = {Previous work has shown that program slicing can be a useful step in model-checking software systems. We are interested in applying these techniques to construct models of multi-threaded Java programs. Past work does not address the concurrency primitives found in Java, nor does it provide the rigorous notions of slice correctness that are necessary for reasoning about programs with non-deterministic behaviour and potentially infinite computation traces.In this paper, we define the semantics of a simple multi-threaded language with concurrency primitives matching those found in the Java Virtual Machine, we propose a bisimulation-based notion of correctness for slicing in this setting, we identify notions of dependency that are relevant for slicing multi-threaded Java programs, and we use these dependencies to specify a program slicer for the language presented in the paper. Finally, we discuss how these dependencies can be refined to take into account common programming idioms of concurrent Java software.},
	language = {en},
	booktitle = {Static {Analysis}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hatcliff, John and Corbett, James and Dwyer, Matthew and Sokolowski, Stefan and Zheng, Hongjun},
	editor = {Cortesi, Agostino and Filé, Gilberto},
	year = {1999},
	pages = {1--18},
}

@article{ranganath_slicing_2007,
	title = {Slicing concurrent {Java} programs using {Indus} and {Kaveri}},
	volume = {9},
	issn = {1433-2779, 1433-2787},
	url = {http://link.springer.com/10.1007/s10009-007-0043-0},
	doi = {10.1007/s10009-007-0043-0},
	abstract = {Program slicing is a program analysis and trans- 1 Introduction formation technique that has been successfully used in a wide range of applications including program comprehen- 1.1 Slicing – Concepts and Applications sion, debugging, maintenance, testing, and veriﬁcation. However, there are only few fully-featured implementations of program slicing that are available for industrial applications or academic research. In particular, very little tool support exists for slicing programs written in modern object-oriented languages such as Java, C\#, or C++.},
	language = {en},
	number = {5-6},
	urldate = {2018-12-17},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Ranganath, Venkatesh Prasad and Hatcliff, John},
	month = oct,
	year = {2007},
	pages = {489--504},
}

@article{wang_dynamic_2008,
	title = {Dynamic slicing on {Java} bytecode traces},
	volume = {30},
	issn = {01640925},
	url = {http://portal.acm.org/citation.cfm?doid=1330017.1330021},
	doi = {10.1145/1330017.1330021},
	language = {en},
	number = {2},
	urldate = {2018-12-17},
	journal = {ACM Transactions on Programming Languages and Systems},
	author = {Wang, Tao and Roychoudhury, Abhik},
	month = mar,
	year = {2008},
	pages = {1--49},
}

@inproceedings{pani_property_2016,
	title = {Property {Based} {Dynamic} {Slicing} of {Object} {Oriented} {Programs}},
	abstract = {Received Jun 12 th , 2015 Revised Aug 20 th , 2015 Accepted Aug 26 th , 2015 Slicing is used for program analysis. It the process of extracting the statements of a program that are relevant to a given computation. Static slicing generates slices for all possible execution of a program helping in program understanding, verification, and maintenance and testing. Dynamic slices are smaller in size as they extract slices for a given execution of a program and helps in interactive applications like debugging and testing. With the wide spread use of object oriented software, there are many papers on Dynamic Slicing of object oriented programs but few papers only address in details about the most basic features of Object Oriented Programming that is class definition, Object creation, accessing object through reference, invoking methods of a class, polymorphism, inheritance etc. From last three decades many algorithms have been designed to slice a program with respect to the syntax of the program. The real worlds object oriented programs consist of thousands of lines of code. Traditional Syntax based slices for program variables used at many places in a program are generally large even for dynamic slices. Recently, some work has been done to get slices based on abstract/concrete properties of program variables. For smooth debugging and testing, the slice will be small if any particular property is being considered (semantics based). Most of the semantics based slicing algorithms have focused on finding static slices on the abstract properties by using SSA as intermediate representation and extract slices by storing an execution trace of a program. To the best of our knowledge generating dynamic slices based on abstract/Concrete properties of program variables is scarcely reported in literature. In this paper we present an algorithm for generating dynamic abstract slices of object oriented programs addressing all key object oriented features. Keyword:},
	author = {Pani, Santosh Kumar and Mund, G. B.},
	year = {2016},
	keywords = {Algorithm, Computation, Debugging, Intermediate representation, Keyword, Paper, Program analysis, Program comprehension, Program slicing, Source lines of code, anti-Ro (SSA) measurement},
}

@article{hammacher_design_nodate,
	title = {Design and {Implementation} of an {Efﬁcient} {Dynamic} {Slicer} for {Java}},
	language = {en},
	author = {Hammacher, Clemens},
	pages = {40},
}

@inproceedings{binkley_tree-oriented_2017,
	title = {Tree-{Oriented} vs. {Line}-{Oriented} {Observation}-{Based} {Slicing}},
	doi = {10.1109/SCAM.2017.11},
	abstract = {Observation-based slicing is a recently-introduced, language-independent slicing technique based on the dependencies observable from program behavior. The original algorithm processed traditional source code at the line-of-text level. A recent variation was developed to slice the tree-based XML representation of executable models. We ported the model slicer to source code using srcML to construct a tree-based representation of traditional source code. We present the results of a comparison of the two slicers using four experiments involving seventeen different programs, including classic benchmarks and larger production systems. The resulting slices had essentially the same size and quite often the same content. Where they differ, the use of tree structure traded an ability to remove unnecessary parts of a statement for the requirement of maintaining aspect of the code structure. Comparing the slicers finds that each has its advantages. For example, when the tree representation facilitates the deletion of large chunks of code, the tree slicer was over eight times faster. In contrast, when slicing C++ code it was over nine times slower because of the multitude of small trees created to support C++ syntax. Given the pros and cons of the two, the results suggest the value of their hybrid combination.},
	booktitle = {2017 {IEEE} 17th {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation} ({SCAM})},
	author = {Binkley, D. and Gold, N. and Islam, S. and Krinke, J. and Yoo, S.},
	month = sep,
	year = {2017},
	keywords = {C++ code, C++ language, C++ languages, C++ syntax, Heuristic algorithms, ORBS, Observational Slicing, Slicing, Software algorithms, Software packages, Trajectory, XML, XML representation, code structure, language-independent slicing technique, line-of-text level, line-oriented observation, program slicing, slicing observation, srcML, traditional source code, tree data structures, tree slicer, tree structure, tree-based XML representation},
	pages = {21--30},
}

@inproceedings{lee_hyperheuristic_2017,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Hyperheuristic {Observation} {Based} {Slicing} of {Guava}},
	isbn = {978-3-319-66299-2},
	abstract = {Observation Based Slicing is a program slicing technique that depends purely on the observation of dynamic program behaviours. It iteratively applies a deletion operator to the source code, and accepts the deletion (i.e. slices the program) if the program is observed to behave in the same was as the original with respect to the slicing criterion. While the original observation based slicing only used a single deletion operator based on deletion window, the catalogue of applicable deletion operators grew recently with the addition of deletion operators based on lexical similarity. We apply a hyperheuristic approach to the problem of selecting the best deletion operator to each program line. Empirical evaluation using four slicing criteria from Guava shows that the Hyperheuristic Observation Based Slicing (HOBBES) can significantly improve the effeciency of observation based slicing.},
	language = {en},
	booktitle = {Search {Based} {Software} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Lee, Seongmin and Yoo, Shin},
	editor = {Menzies, Tim and Petke, Justyna},
	year = {2017},
	pages = {175--180},
}

@inproceedings{gold_generalized_2017,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2017},
	title = {Generalized {Observational} {Slicing} for {Tree}-represented {Modelling} {Languages}},
	isbn = {978-1-4503-5105-8},
	url = {http://doi.acm.org/10.1145/3106237.3106304},
	doi = {10.1145/3106237.3106304},
	abstract = {Model-driven software engineering raises the abstraction level making complex systems easier to understand than if written in textual code. Nevertheless, large complicated software systems can have large models, motivating the need for slicing techniques that reduce the size of a model. We present a generalization of observation-based slicing that allows the criterion to be defined using a variety of kinds of observable behavior and does not require any complex dependence analysis. We apply our implementation of generalized observational slicing for tree-structured representations to Simulink models. The resulting slice might be the subset of the original model responsible for an observed failure or simply the sub-model semantically related to a classic slicing criterion. Unlike its predecessors, the algorithm is also capable of slicing embedded Stateflow state machines. A study of nine real-world models drawn from four different application domains demonstrates the effectiveness of our approach at dramatically reducing Simulink model sizes for realistic observation scenarios: for 9 out of 20 cases, the resulting model has fewer than 25\% of the original model's elements.},
	urldate = {2018-12-17},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Gold, Nicolas E. and Binkley, David and Harman, Mark and Islam, Syed and Krinke, Jens and Yoo, Shin},
	year = {2017},
	keywords = {MATLAB, ORBS, Observational Slicing, Simulink, Slicing},
	pages = {547--558},
}

@inproceedings{liu_tool_2009,
	title = {A {Tool} {Suite} for {Java} {Program} {Tracing} and {Feature} {Location}},
	doi = {10.1109/SNPD.2009.55},
	abstract = {In this paper, we describe a suite of two tools: Multi-Threaded Tracer (MuTT) and Exploring Traces (ET), which can be used to trace complex multi-threaded event-driven Java programs during feature location and program debugging and help the programmer to locate parts of code that are related to a specific feature. MuTT does not need instrumentation. ET can be used to browse the traces and focus on a small part of the graph presented by MuTT from the traces. We also conduct a case study on Eclipse with this tool suite and the result shows that the productivity of feature location gained by using them. The case study also demonstrates that ET can work well with MuTT.},
	booktitle = {2009 10th {ACIS} {International} {Conference} on {Software} {Engineering}, {Artificial} {Intelligences}, {Networking} and {Parallel}/{Distributed} {Computing}},
	author = {Liu, D. and Xu, S.},
	month = may,
	year = {2009},
	keywords = {Artificial intelligence, Computer science, Debugging, Eclipse, Intelligent networks, Java, Java program tracing, Multithreading, Productivity, Programming profession, Runtime, Software engineering, exploring traces, feature location, multi-threading, multithreaded event-driven Java programs, multithreaded tracer, multithreading, program debugging, program diagnostics, software tool suite, software tools, tools, trace presentation, tracing},
	pages = {469--474},
}

@inproceedings{lee_mobs:_2018,
	address = {New York, NY, USA},
	series = {{ICSE} '18},
	title = {{MOBS}: {Multi}-operator {Observation}-based {Slicing} {Using} {Lexical} {Approximation} of {Program} {Dependence}},
	isbn = {978-1-4503-5663-3},
	shorttitle = {{MOBS}},
	url = {http://doi.acm.org/10.1145/3183440.3194981},
	doi = {10.1145/3183440.3194981},
	abstract = {Observation-Based Slicing (ORBS) is a recently introduced program slicing technique based on direct observation of program semantics. Previous ORBS implementations slice a program by iteratively deleting adjacent lines of code. This paper introduces two new deletion operators based on lexical similarity. Furthermore, it presents a generalization of O RBS that can exploit multiple deletion operators: Multi-operator Observation-Based Slicing (MOBS). An empirical evaluation of MOBS using three real world Java projects finds that the use of lexical information, improves the efficiency of ORBS: MOBS can delete up to 87\% of lines while taking only about 33\% of the execution time with respect to the original ORBS.},
	urldate = {2018-12-17},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceeedings}},
	publisher = {ACM},
	author = {Lee, Seongmin and Binkley, David and Gold, Nicolas and Islam, Syed and Krinke, Jens and Yoo, Shin},
	year = {2018},
	pages = {302--303},
}

@inproceedings{beszedes_graph-less_2006,
	title = {Graph-{Less} {Dynamic} {Dependence}-{Based} {Dynamic} {Slicing} {Algorithms}},
	doi = {10.1109/SCAM.2006.17},
	abstract = {Using Dynamic Dependence Graphs is a well understood method for computing dynamic program slices. However, in its basic form, the DDG is inappropriate for practical implementation, so several alternative approaches have been proposed by researchers. In this paper, we elaborate on different methods in which the execution trace is processed and, using local definition-use information, the dependence chains are followed "on the fly" to construct the slices without actually building any graphs. Naturally, various additional data structures still need to be maintained, but these vary on the slicing scenario. Firstly, one may want to perform the slicing in a demand-driven fashion, or to compute many slices globally. Next, one may be interested either in backward or forward slices. And finally, the slices can be produced by traversing the trace either in a forward or in a backward direction. This totals eight possibilities, of which some give useful algorithms, while there are irrelevant combinations as well. In this work we investigate all of them, give the basic algorithms where appropriate and discuss on implementation experiences and perspectives.},
	booktitle = {2006 {Sixth} {IEEE} {International} {Workshop} on {Source} {Code} {Analysis} and {Manipulation}},
	author = {Beszedes, A. and Gergely, T. and Gyimothy, T.},
	month = sep,
	year = {2006},
	keywords = {Buildings, Computer aided instruction, Conferences, Data structures, Heuristic algorithms, History, Java, Program slicing, Software algorithms, Software engineering, Terminology, dynamic slicing algorithms, execution, program dependences., trace},
	pages = {21--30},
}

@inproceedings{binkley_orbs:_2014,
	address = {New York, NY, USA},
	series = {{FSE} 2014},
	title = {{ORBS}: {Language}-independent {Program} {Slicing}},
	isbn = {978-1-4503-3056-5},
	shorttitle = {{ORBS}},
	url = {http://doi.acm.org/10.1145/2635868.2635893},
	doi = {10.1145/2635868.2635893},
	abstract = {Current slicing techniques cannot handle systems written in multiple programming languages. Observation-Based Slicing (ORBS) is a language-independent slicing technique capable of slicing multi-language systems, including systems which contain (third party) binary components. A potential slice obtained through repeated statement deletion is validated by observing the behaviour of the program: if the slice and original program behave the same under the slicing criterion, the deletion is accepted. The resulting slice is similar to a dynamic slice. We evaluate five variants of ORBS on ten programs of different sizes and languages showing that it is less expensive than similar existing techniques. We also evaluate it on bash and four other systems to demonstrate feasible large-scale operation in which a parallelised ORBS needs up to 82\% less time when using four threads. The results show that an ORBS slicer is simple to construct, effective at slicing, and able to handle systems written in multiple languages without specialist analysis tools.},
	urldate = {2018-12-17},
	booktitle = {Proceedings of the {22Nd} {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Binkley, David and Gold, Nicolas and Harman, Mark and Islam, Syed and Krinke, Jens and Yoo, Shin},
	year = {2014},
	pages = {109--120},
}

@inproceedings{liu_mutt:_2009,
	title = {{MuTT}: {A} {Multi}-{Threaded} {Tracer} for {Java} {Programs}},
	shorttitle = {{MuTT}},
	doi = {10.1109/ICIS.2009.159},
	abstract = {Tracing is an important technique for program comprehension and software maintenance. Existing trace tools have some limitations, such as requiring instrumentation. This paper presents a tracer, MuTT, which can be used to collect the run-time information of multi-threaded Java programs without source code or JVM instrumentation. We also propose guidelines to use MuTT for feature location in object-oriented systems, particularly in the event-driven systems. We conduct two case studies and the result demonstrates that MuTT can provide effective tracing and can be used for feature location, especially with the help of GREP.},
	booktitle = {2009 {Eighth} {IEEE}/{ACIS} {International} {Conference} on {Computer} and {Information} {Science}},
	author = {Liu, D. and Xu, S.},
	month = jun,
	year = {2009},
	keywords = {Binary codes, Computer science, Debugging, Guidelines, Information science, Instruments, JVM instrumentation, Java, Java programs, MuTT, Programming profession, Runtime, Software maintenance, case studies, event-driven systems, feature location, java program, multi-threading, multithreaded tracer, object-oriented systems, program comprehension, run-time information, software maintenance, tracing tool},
	pages = {949--954},
}

@inproceedings{szegedi_dynamic_2005,
	title = {Dynamic slicing of {Java} bytecode programs},
	doi = {10.1109/SCAM.2005.8},
	abstract = {A forward global method for obtaining backward dynamic slices of Java bytecode programs is presented. In contrast with existing published techniques that require either a customized Java compiler (which also implies access to the source code) or bytecode instrumentation and eventual manual dependency specifications, our approach was to produce an instrumented virtual machine for Java. This approach works with programs compiled with arbitrary third party compilers and does not require access to the source code during the slicing process. However, we still retain the ability to express the slicing criterion and the resulting slice in terms of source code locations using the supplemental information present in the compiled code. Our technique also handles advanced aspects of the Java environment, such as exception handling, multithreaded execution and, to a certain degree, the execution of native machine code linked with the Java classes.},
	booktitle = {Fifth {IEEE} {International} {Workshop} on {Source} {Code} {Analysis} and {Manipulation} ({SCAM}'05)},
	author = {Szegedi, A. and Gyimothy, T.},
	month = sep,
	year = {2005},
	keywords = {Concrete, Conferences, Debugging, Instruments, Java, Java bytecode programs, Manuals, Program processors, Software engineering, Virtual machining, dynamic program slicing, program compilers, program slicing, source code locations, virtual machine},
	pages = {35--44},
}

@article{mangean_best:_2016,
	title = {{BEST}: a {Binary} {Executable} {Slicing} {Tool}},
	abstract = {We describe the implementation of Best, a tool for slicing binary code. We aim to integrate this tool in a WCET estimation framework based on model checking. In this approach, program slicing is used to abstract the program model in order to reduce the state space of the system. In this article, we also report on the results of an evaluation of the eﬃciency of the abstraction technique.},
	language = {en},
	author = {Mangean, Armel and Béchennec, Jean-Luc and Briday, Mikaël and Faucou, Sébastien},
	year = {2016},
	pages = {10},
}

@article{barr_oracle_2015,
	title = {The {Oracle} {Problem} in {Software} {Testing}: {A} {Survey}},
	volume = {41},
	issn = {0098-5589, 1939-3520},
	shorttitle = {The {Oracle} {Problem} in {Software} {Testing}},
	url = {http://ieeexplore.ieee.org/document/6963470/},
	doi = {10.1109/TSE.2014.2372785},
	abstract = {Testing involves examining the behaviour of a system in order to discover potential faults. Given an input for a system, the challenge of distinguishing the corresponding desired, correct behaviour from potentially incorrect behavior is called the “test oracle problem”. Test oracle automation is important to remove a current bottleneck that inhibits greater overall test automation. Without test oracle automation, the human has to determine whether observed behaviour is correct. The literature on test oracles has introduced techniques for oracle automation, including modelling, speciﬁcations, contract-driven development and metamorphic testing. When none of these is completely adequate, the ﬁnal source of test oracle information remains the human, who may be aware of informal speciﬁcations, expectations, norms and domain speciﬁc information that provide informal oracle guidance. All forms of test oracles, even the humble human, involve challenges of reducing cost and increasing beneﬁt. This paper provides a comprehensive survey of current approaches to the test oracle problem and an analysis of trends in this important area of software testing research and practice.},
	language = {en},
	number = {5},
	urldate = {2018-12-17},
	journal = {IEEE Transactions on Software Engineering},
	author = {Barr, Earl T. and Harman, Mark and McMinn, Phil and Shahbaz, Muzammil and Yoo, Shin},
	month = may,
	year = {2015},
	pages = {507--525},
}

@inproceedings{bhattacharya_property_2011,
	title = {Property {Driven} {Program} {Slicing} {Refinement}},
	abstract = {A slice is usually computed by analyzing how the effects of a computation are propagated through the code, i.e., by inferring dependencies. The aim of this paper is to further refine the traditional slicing technique by combining it with a static analysis in Abstract Interpretation based framework. This results into a deeper insight on the strong relation between slicing and property based dependency.},
	booktitle = {{ICSOFT}},
	author = {Bhattacharya, Sukriti and Cortesi, Agostino},
	year = {2011},
	keywords = {Abstract interpretation, Program slicing},
}

@article{bordini_property-based_2009,
	title = {Property-based {Slicing} for {Agent} {Verification}},
	volume = {19},
	issn = {0955-792X, 1465-363X},
	url = {https://academic.oup.com/logcom/article-lookup/doi/10.1093/logcom/exp029},
	doi = {10.1093/logcom/exp029},
	abstract = {Programming languages designed speciﬁcally for multi-agent systems represent a new programming paradigm that has gained popularity over recent years, with some multi-agent programming languages being used in increasingly sophisticated applications, often in critical areas. To support this, we have developed a set of tools to allow the use of model-checking techniques in the veriﬁcation of systems directly implemented in one particular language called AgentSpeak. The success of model checking as a veriﬁcation technique for large software systems is dependent partly on its use in combination with various state-space reduction techniques, an important example of which is property-based slicing. This article introduces an algorithm for property-based slicing of AgentSpeak multi-agent systems. The algorithm uses literal dependence graphs, as developed for slicing logic programs, and generates a program slice whose state space is stuttering-equivalent to that of the original program; the slicing criterion is a property in a logic with LTL operators and (shallow) BDI modalities. In addition to showing correctness and characterizing the complexity of the slicing algorithm, we apply it to an AgentSpeak program based on autonomous planetary exploration rovers, and we discuss how slicing reduces the model-checking state space. The experiment results show a signiﬁcant reduction in the state space required for model checking that agent, thus indicating that this approach can have an important impact on the future practicality of agent veriﬁcation.},
	language = {en},
	number = {6},
	urldate = {2018-12-17},
	journal = {Journal of Logic and Computation},
	author = {Bordini, R. H. and Fisher, M. and Wooldridge, M. and Visser, W.},
	month = dec,
	year = {2009},
	pages = {1385--1425},
}

@article{segura_survey_2016,
	title = {A {Survey} on {Metamorphic} {Testing}},
	volume = {42},
	doi = {10.1109/TSE.2016.2532875},
	abstract = {A test oracle determines whether a test execution reveals a fault, often by comparing the observed program output to the expected output. This is not always practical, for example when a program's input-output relation is complex and difficult to capture formally. Metamorphic testing provides an alternative, where correctness is not determined by checking an individual concrete output, but by applying a transformation to a test input and observing how the program output 'morphs' into a different one as a result. Since the introduction of such metamorphic relations in 1998, many contributions on metamorphic testing have been made, and the technique has seen successful applications in a variety of domains, ranging from web services to computer graphics. This article provides a comprehensive survey on metamorphic testing: It summarises the research results and application areas, and analyses common practice in empirical studies of metamorphic testing as well as the main open challenges.},
	journal = {IEEE Transactions on Software Engineering},
	author = {Segura, Sergio and Fraser, Gordon and Sánchez, Ana B. and Ruiz-Cortés, Antonio},
	month = sep,
	year = {2016},
	keywords = {Metamorphic Testing, Survey},
	pages = {1--1},
}

@inproceedings{gore_improved_2010,
	title = {Improved methods and measures for computing dynamic program slices in stochastic simulations},
	doi = {10.1109/WSC.2010.5679114},
	abstract = {Stochastic simulations frequently exhibit behaviors that are difficult to recreate and analyze, owing largely to the stochastics themselves, and consequent program dependency chains that can defy human reasoning capabilities. We present a novel approach called Markov Chain Execution Traces (MCETs) for efficiently representing sampled stochastic simulation execution traces and ultimately driving semiautomated analysis methods that require accurate, efficiently generated candidate execution traces. The MCET approach is evaluated, using new and established measures, against both additional novel and existing approaches for computing dynamic program slices in stochastic simulations. MCET's superior performance is established. Finally, a description of how users can apply MCETs to their own stochastic simulations and a discussion of the new analyses MCETs can enable are presented.},
	booktitle = {Proceedings of the 2010 {Winter} {Simulation} {Conference}},
	author = {Gore, R. and Reynolds, P. F.},
	month = dec,
	year = {2010},
	keywords = {Analytical models, Computational modeling, Markov chain execution traces, Markov processes, Software, Software testing, Trajectory, dynamic program slices, human reasoning capabilities, program slicing, sampled stochastic simulation execution traces, semiautomated analysis methods, stochastic processes},
	pages = {753--764},
}
