\section{Introduction}
The reliance of real-world applications on smart systems based on deep neural networks (DNNs) has been on a constant rise for several years. 
These include the applications in safety-critical systems like autonomous driving and healthcare \cite{health,auto_drive}. 
This raises concerns regarding the reliable and acceptable performance of DNNs for a diverse range of input scenarios, including the ones pertaining to consistent performance of the DNNs trained on long-tail distribution, i.e., the training datasets with a significant portion of total inputs belonging to the \textit{head} class(es) and only a small subset of inputs belonging to the \textit{tail} class(es) \cite{2022longtail}. 

% But vulnerable \cite{MLsurvey}
The concerns surrounding long-tail distribution are not ill-founded. 
Numerous available datasets, in fact, comprise of long-tail distribution. 
The MIT-BIH Arrhythmia dataset \cite{arythmia-dataset} contains a significant proportion of normal ECG samples (as opposed to ECG samples indicating arrhythmia).
The IMDB-WIKI dataset \cite{imdb-dataset} comprises of a significant proportion of Caucasian faces.
Wafer map training dataset \cite{wu2014wafer} comprises of a proportion of fault-free wafers (as opposed to faulty wafers). 
Such discrepancy in the number of inputs across different output classes is not always surprising, since the tail classes often present rare events of the real-world. 
% Long-tail distribution issue in practical Wafer map training dataset \cite{wu2014wafer} contains over $67\%$ fault-free wafers, while the remaining is distributed among multiple fault categories. 

It is not also surprising that the DNNs trained on long-tail distribution learn the patterns in head classes better than those in the tail classes due to the availability of ample input samples. 
It has also been observed that such networks also delineate a robustness bias under the influence of noise, i.e., the network is likely to correctly classify even noisy inputs from the head class(es), while the robustness of the tail class(es) against noise is only negligible. 

Orthogonally, the sensitivity of input nodes has also been found to vary \cite{bhatti2022formal}. 
While this variation comes in handy while determining the relevant input nodes for the designated task of the trained DNN \cite{input-sens1,chen2020sensitivity}, it may also pose itself as a concern for applications where a revelation of the sensitive attributes (nodes) may lead to a privacy infringement \cite{sens4fair1_dnn,sens4fair2_dnn}.

However, there is another aspect of concern for DNNs, which is inadvertently linked to those indicated  above - i.e., the (robustness) bias of the individual input nodes - which remains unrecognized in the existing literature. 
This work deals with such node bias, indicating its stealthy existence and the non-triviality of understanding its causes. 
To summarize, the novel contributions of this work are as follows: 
\begin{enumerate}
    \item Defining the concept of node (robustness) bias.
    \item Highlighting the link between robustness bias, node sensitivity, and node bias.
    \item Identifying the existence of node bias and empirically analyzing it in a network trained on a real-world Leukemia dataset.
    \item Discussing the severity of node bias with respect to long-tail distribution of the training dataset.
    \item Elucidating the open challenges pertaining to node bias, in trained DNNs.
\end{enumerate}
