\section{Conclusion and Future Work}

Past years have not only seen a rise in the use of deep neural networks (DNNs) in real-world applications, but also an awareness of the vulnerabilities of these DNNs leading to their undesirable performance. 
Among the concerns arising regarding DNNs is the inconsistent classification of these networks across the output classes, often resulting from long-tail distribution of the training dataset. 
Existing literature already highlights the (robustness) bias as a possible consequence of such distributions. %y leading from such distribution. 
However, in this work, we shed light on the bias of the DNNs beyond simply the varying classification performance across different output classes. 
To the best of our knowledge, this is the first work exposing the varying bias of input nodes, for the different output classes, for DNNs trained on long-tail distribution data.

Through our proposed framework and case study, we also explore a possible link between variance of node values in the training dataset and the impact of removing samples of head class(es) of the distribution during training. 
However, the exact cause of such node bias is still up for debate, and requires further analysis with larger datasets and attention from the research community.