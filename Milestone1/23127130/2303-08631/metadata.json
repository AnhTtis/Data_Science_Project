{
    "arxiv_id": "2303.08631",
    "paper_title": "Smoothed Q-learning",
    "authors": [
        "David Barber"
    ],
    "submission_date": "2023-03-15",
    "revised_dates": [
        "2023-03-16"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG"
    ],
    "abstract": "In Reinforcement Learning the Q-learning algorithm provably converges to the optimal solution. However, as others have demonstrated, Q-learning can also overestimate the values and thereby spend too long exploring unhelpful states. Double Q-learning is a provably convergent alternative that mitigates some of the overestimation issues, though sometimes at the expense of slower convergence. We introduce an alternative algorithm that replaces the max operation with an average, resulting also in a provably convergent off-policy algorithm which can mitigate overestimation yet retain similar convergence as standard Q-learning.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08631v1"
    ],
    "publication_venue": null
}