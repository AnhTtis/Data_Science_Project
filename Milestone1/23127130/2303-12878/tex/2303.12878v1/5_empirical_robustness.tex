%\section{Robustness with Bucket Rankings}
\subsection{Bucket Ranking - Extended Ranking Consensus}\label{sec:buckets}

Intuitively, bucket rankings are rankings with ties allowed. Formally, they can equivalently be defined as a total preorder -- \emph{i.e.} a homogeneous binary relation that satisfies transitivity and reflexivity (preorder) in which any two elements are comparable (total) -- or as a strict weak ordering -- \emph{i.e.} a strict total order over equivalence classes of items (buckets).

\begin{restatable}{definition}{defbucketrankings}\label{def:bucket_rankings}{\sc (Bucket ranking)}
A bucket order $\pi$ is a strict weak order defined by an ordered partition of $[n]$, \emph{i.e.} a sequence $(\pi^{(1)}, \dots , \pi^{(k)})$ of $k \geq 1$ pairwise disjoint non empty subsets (buckets) of $[n]$ such that: 
\begin{enumerate}
    \item[(i)] $i \prec_\pi j ~~\Leftrightarrow~~ \exists l<l' \in [k], (i,j) \in \pi^{(l)} \times \pi^{(l')}$,  
    \item[(ii)] $i \sim_\pi j ~~\Leftrightarrow~~ \exists l \in [k], (i,j) \in \pi^{(l)}\times\pi^{(l)}$,  
\end{enumerate}
We denote $\wO$ the set of bucket rankings, which is of size $\sum_{k=1}^n k! S(n,k)$\footnote{$S(n,k)$ are Stirling numbers of the second kind.} (vs $n!$ for $\pS$).
\end{restatable}
The indifference between items that bucket rankings can incorporate is an interesting feature to gain robustness, because the statistic can output alternatives between several strict orders, making it harder to attack.


\paragraph{As sets of permutations.} A bucket ranking $\pi\in\wO$ can be equivalently mapped to a subset of permutations, generated through the different ways to break ties. We say that a permutation $\sigma\in\pS$ is \emph{compatible} with a bucket ranking $\pi\in\wO$ -- denoted $\sigma\in\pi$ -- if for any $i,j\in[n]$, $\sigma(i)<\sigma(j) ~~\Leftrightarrow~~ i\prec_\pi j$ or $i\sim_\pi j$. For two bucket orders $\pi_1, \pi_2$, we say that $\pi_1$ is \emph{stricter} that $\pi_2$, denoted $\pi_1 \subseteq \pi_2$, iff for any $\sigma\in\pS, ~~\sigma\in\pi_1 \Rightarrow \sigma\in\pi_2$. 

\paragraph{As a distribution.} Being a set of permutations, a bucket order $\pi\in\wO$ can also be seen as a uniform distribution with restricted support. This point of view is particularly intuitive from a robustness perspective: a randomized output is generally harder to attack for an adversary.


\paragraph{Distances between bucket rankings.}
A key to applying the breakdown function from \cref{def:breakdown_function} to bucket orders statistics is to have a metric on $\wO$ that extends those defined on $\pS$.
To this end, we use the previous remark that weak orders are sets of rankings as well as a classical Hausdorff extension of metrics to sets. More precisely, we define:

\begin{restatable}{definition}{defnonsymmetrichausdorff}\label{def:non_symmetric_hausdorff}
    {\sc{(Non-symmetric Hausdorff)}} Let $d$ be a metric on $\pS$. The non-symmetric Hausdorff pseudoquasi-metric between two bucket rankings $\pi_1, \pi_2\in \wO$ is 
    \begin{align*}
        H^\text{\sc ns}_d(\pi_1, \pi_2) = \max_{\sigma_2 \in \pi_2} \min_{\sigma_1 \in \pi_1} d(\sigma_1, \sigma_2)  \,.      
    \end{align*}
\end{restatable}

Even though it is not a metric, $H^\text{\sc ns}_d$ is well-suited to ranking with ties. Intuitively, its lack of symmetry allows differentiating adversarial attacks whose effect is on the strict part of the bucket order (e.g. swapping two items that are strictly ordered) from those whose effect is "only" to disambiguate a tie. More precisely, if $\pi_2 \subseteq \pi_1$, then $H^\text{\sc ns}_d(\pi_1, \pi_2) = 0$. Depending on the application, one may want to focus on the first type of attacks, in which case $H^\text{\sc ns}_d$ is a suitable choice to define the breakdown function as $\varepsilon^\star_{H^\text{\sc ns}_d, p, T}$.
Otherwise, it is possible (and usual) to symmetrize the Hausdorff metric.
\begin{restatable}{definition}{defsymmetrichausdorff}\label{def:symmetric_hausdorff}
    {\sc{($1/2$-symmetric Hausdorff)}} Let $d$ be a metric on $\pS$. The $1/2$-symmetric Hausdorff metric between two bucket rankings $\pi_1, \pi_2\in \wO$ is defined by
    \begin{align*}
        H^{(1/2)}_d(\pi_1, \pi_2) = \frac{1}{2} \Big(H^\text{\sc ns}_d(\pi_1, \pi_2) + H^\text{\sc ns}_d(\pi_2, \pi_1)\Big)\,.
    \end{align*}
\end{restatable}
Usual symmetrization of the Hausdorff metric uses a maximum rather than an average \cite{fagin2006comparing}. However, under the Kendall-tau distance, the average version is computationally simpler (see \cref{app:hausdorff_kendall} for more details).

\subsection{The Breakdown Function in Ranking Data Analysis - Definition and Estimation}

\paragraph{Definition.}
Putting all the pieces together, from now on, the statistic $T : \cM_+^1(\pS) \to \wO$ 
 summarizes a distribution over $\pS$ by a bucket ranking in $\wO$. Then, we use either $H^{(NS)}_{d_\tau}(\pi_1, \pi_2)$ (see \cref{def:non_symmetric_hausdorff}) or $H^{(1/2)}_{d_\tau}(\pi_1, \pi_2)$ on $\wO$ where $d_\tau$ is the Kendall tau (see \cref{eq:kendall_tau}). Finally, the breakdown function $\varepsilon^\star_{H^{(NS)}_{d_\tau}, p, T}$ is the result of the following optimization problem
 \begin{align}
     \inf \left\{ \varepsilon > 0 \, \middle| \, \sup_{q : {\rm TV}(p,q) \leq \varepsilon} H^{(NS)}_{d_\tau}(T(p), T(q)) \geq \delta \right\}
     \label{eq:breakdown_fct_bucket}
 \end{align}

\paragraph{The Empirical Breakdown Function.}
Computing a closed-form expression for the breakdown point for any statistic $T$ and distribution $p$ is challenging in general. However, it can be estimated empirically: the extended expression of the breakdown function in \cref{eq:breakdown_fct_bucket} can be simplified so that it is the solution to the following Lagrangian-relaxed optimization problem.
\begin{equation}
    \inf_{q \in \Delta^{\frak{S}_n}} \sup_{\lambda \geq 0} 1/2 \| p-q \|_1 + \lambda(\delta - H^{(NS)}_{d_\tau}(T(p), T(q)) )
    \label{eq:lagrangian_relax_bkdwn}
\end{equation}
\paragraph{Smoothing.} As $H^{(NS)}_{d_\tau}(T(p), T(q)))$ is piece-wise constant as a function of $q$ (with a combinatorial number of pieces), Problem \eqref{eq:lagrangian_relax_bkdwn} cannot directly be solve using standard optimization techniques.
To solve this issue, we used a smoothing procedure by convolving this function with a smoothing kernel $k_\gamma$ with scale $\gamma$. Thus, after the relaxation, the optimization problem \eqref{eq:lagrangian_relax_bkdwn} becomes:
\begin{equation}
    \inf_{q \in \Delta^{\frak{S}_n}} \sup_{\lambda \geq 0} 1/2 \| p-q \|_1 + \lambda(\delta - \rho_T(p, q) ),
    \label{eq:smoothing_version_bkdwn}
\end{equation}
with 
\begin{equation}
    \begin{split}
        \rho_T(p,q) &= H^{(NS)}_{d_\tau}(T(p), T(q)) \star k_{\gamma}(q) \\
        &= \int_{u} H^{(NS)}_{d_\tau}(T(p), T(u)) \times k_{\gamma}(q-u) \text{d}u,
    \end{split}
\end{equation}
On a practical note, a simple way to build a convolution kernel $k_\gamma$ on a simplex like $\distribs$, is to use a convolution kernel $\kappa_\gamma$ on the whole euclidean space -- for instance an independent Gaussian density $\kappa_{\gamma}(x) = \frac{1}{\sqrt{ (2 \pi \gamma)^{n!} }} \exp{( -\frac{x^{\text{T}} x }{2 \gamma^2} )}$ 
-- and set $k_\gamma$ to be the density of the push-forward through a \emph{softmax} function. We denote $\varepsilon^\gamma_{p,T}(\delta)$ the limiting value of $\|p-q\|_1/2$ at the solution of \eqref{eq:smoothing_version_bkdwn}. Note the bias induced by such definition of $k_\gamma$ fades away when $\gamma$ goes to $0$ in the same way as the bias induced by the convolution.
This smoothing ensures $\rho_T$ is a continuous, differentiable function with respect to $q$. Moreover, it can easily be estimated using a Monte-Carlo sampling, using the following remark: $\rho_T(p,q) = \mathbb{E}_{u \sim k_{(p, \gamma)}} (H^{(NS)}_{d_\tau}(T(u), T(q))$.

% \paragraph{Complexity.} Solving \eqref{eq:smoothing_version_bkdwn} empirically is possible, but very costly, as the objective requires $n!$ computation just to be evaluated. However, this cost comes from computing $\|p-q\|_1$ as it is possible to reformulate $H_{d_\tau}^{(1/2)}$ so it has a much lower computational cost as states the following:
% \begin{restatable}{proposition}{propcomplexityhausdorffkendall}\label{prop:complexity_hausdorff_kendall}
% For any $\pi_1, \pi_2\in\wO$, the computation cost of $H_{d_\tau}^{\textsc{ns}}(\pi_1, \pi_2)$ and $H_{d_\tau}^{(1/2)}(\pi_1, \pi_2)$ is $\cO(n^2)$.
% \end{restatable}
% \begin{proof}
% Proof can be found in \cref{app:hausdorff_kendall}. It relies of reformulations inspired from the work of \citet{fagin2006comparing}.
% \end{proof}

\begin{comment}
As the second term in \eqref{eq:smoothing_version_bkdwn} can be computed efficiently, replacing the $\|\cdot\|_1$ budget constraint by one on pairwise marginal probabilities provides the following objective:
\begin{align}
    \label{eq:smoothing_pairwise_version_bkdwn}
    \inf_{q \in \Delta^{\frak{S}_n}} \sup_{\lambda > 0} \frac{1}{2} \| P\,\text{\scalebox{0.5}[1.0]{$-$}}\,Q \|_1 + \lambda(\delta \,\text{\scalebox{0.5}[1.0]{$-$}}\,\rho_T(p,q) ),
\end{align}
where $P\in[0,1]^{n^2}$ (resp $Q$) is the matrix of pairwise marginal probabilities of $p$ (resp $q$), defined as follows.


In the end, each Monte-Carlo sample to estimate of the objective requires $\cO(n^2 + C(T))$ computations, where $C(T)$ stands for the computational cost of the statistic $T$ and \cref{fig:proba_vs_pairwise} shows both ways to constrain the attack budget are well-correlated.


\begin{figure}[h]
\centering
\label{fig:proba_vs_pairwise}
\includegraphics[width=0.8\linewidth]{icml_submission/img/proba_vs_pairwise.png}
\caption{Relation between $TV(p_1,p_2)$ and $\|P_1-Q1\|_1$ (normalized), where distributions were generated using Plackett-Lice with random logits.}
\end{figure}
\end{comment}

\paragraph{Optimization.} When using Monte-Carlo estimation for $\rho_T$, \cref{eq:smoothing_version_bkdwn} is a stochastic saddle-point problem. To solve such problems, gradient/ascent has a rate of convergence of $\cO(t^{1/2})$ for its ergodic average ($t$ being the number of steps) \cite{Nemirovski2002}. %Recently, better-performing algorithms have also been proposed, such as the \emph{extra-gradient method} (e.g. \citet{hsieh2019}).
Our empirical optimization algorithm for computing the breakdown functions relies on stochastic gradient descent and is able to provide good approximations, as illustrated in \cref{fig:theory_exp_kemeny_maxpair}. We denote $\hat{\varepsilon}^\gamma_{p,T}(\delta) = \|p - \bar{q}_t\|_1$, where $\bar{q}_t$ is the ergodic average of the iterates $(q_s)_{s\leq t}$ obtained during the optimization.

Let's make a couple of remarks on the empirical breakdown function $\hat{\varepsilon}^\gamma_{p,T}$. First, it is a noisy estimate of $\varepsilon^\gamma_{p, T}$ as $\rho_T$ and its gradients are estimated via Monte-Carlo. Thus, the choice of $\gamma$ and $t$ should trade-off the variance of $\hat{\varepsilon}^\gamma_{p,T}$ and the bias $|\varepsilon^\gamma_{p, T}-\varepsilon^\star_{d_\tau, p, T}|$. Second, as the term $\|p - q\|_1$ is minimized in \eqref{eq:smoothing_version_bkdwn}, it is expected $\hat{\varepsilon}^\gamma_{p,T}$ over-estimates $\varepsilon^\gamma_{p, T}$.

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.95\linewidth]{icml_submission/img/theory_exp_Kemeny.pdf}
% \caption{Placeholder -- Theorical bounds and empirical values for the breakdown function of Kemeny's median.}
% \label{fig:theory_exp_kemeny}
% \end{figure}



% \subsection{Definition of Breakdown Points}

% \intodo{all this sections needs to be placed somewhere else / removed}

% In the field of ranking data, very few works have studied adversarial attacks~\cite{datar2022byzantine, agarwal2020rank,jin2018, lu2012bayesian},  (\intodo{Add refs}). In this paper, we will focus on poisoning attacks, where the attacker is allowed to modify the original distribution $p$ by a budget $\varepsilon$, in order to fool the Ranking Aggregation statistic $T$.

% \begin{definition}
%     {\sc{Adversarial Distribution}.} An adversarial distribution against the probability distribution $p \in \Delta^{\frak{S}_n}$ for statistic $T$ with budget $\varepsilon$, shortened $(p, T, \varepsilon)$-adversarial distribution, is a probability distribution $q_{P, T, \varepsilon}$ such that:
    
%     1) the budget constraint is satisfied: $\text{TV}(p, q_{P, \varepsilon}) = \frac{1}{2} \sum_{\sigma \in \frak{S}_n} |p(\sigma) - q_{P, T, \varepsilon}(\sigma)| \leq \varepsilon$, where $\text{TV}$ is the total-variation distance, and $\varepsilon \geq 0$ is the attack budget.

%     2) If possible, $q_{p, T, \varepsilon}$ fools the statistics $T$: $T(p) \neq T(q_{p, T, \varepsilon})$.
% \end{definition}

% To improve readability, whenever the context is clear, the adversarial distribution $q_{p, T, \varepsilon}$ bill be simply denoted by $q$. Note also that since the ranking probability distribution space $\Delta^{\frak{S}_n}$ is discrete and finite, we have $TV(p,q) = 1/2 \sum_{\sigma \in \frak{S}_n} | p(\sigma) - q(\sigma) | = 1/2 \| p-q \|_1  $. \\
