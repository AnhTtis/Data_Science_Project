\section{Numerical Experiments}
\label{sec:exps}

In this section, we illustrate the relevance of the statistic outputted by our Downward Merge plug-in on Kemeny's median (called our \textit{Downward Merge statistic} for short) by running several illustrative experiments for various settings and comparing with the baseline provided by the usual Kemeny's median. The code is available \href{https://github.com/RobustConsensusRanking/RobustConsensusRanking}{here}. 

\subsection{Empirical Robustness}
\label{subsec:emp_rob}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/theory_exp_kemeny_maxpair_v2.png}
    \caption{Breakdown function $\hat{\varepsilon}^{\gamma}_{p,T}(\delta)$ as a function of attack amplitude $\delta$ for a bucket distribution $p$ (almost a point mass on two neighboring rankings) with $n=4$. The plain blue line denotes the theoretical value for Kemeny's median $\varepsilon^*_{p}(\delta)$, blue crosses (resp. red dots) the empirical approximation $\hat{\varepsilon}^{\gamma}_{p,T}$ for Kemeny's median (resp. Down. Merge statistic for different thresholds $\theta$).}
    \label{fig:theory_exp_kemeny_maxpair}
\end{figure}

Our Downward Merge plug-in aims at providing a robustified statistic. To illustrate its usefulness, we ran experiments computing the approximate breakdown functions $\hat{\varepsilon}^{\gamma}_{p,T}(\delta)$ for the Kemeny's median as a baseline and our statistic when varying $\delta$. \cref{fig:theory_exp_kemeny_maxpair} shows the robustness as a function of attack amplitude $\delta$ and for a hand-picked distribution $p$ that is almost a point mass on a bucket ranking.

%When the threshold is much too small, with $t=0.0001$, \textcolor{red}{0.0001 has been removed from the plot?} one can see that the behavior of the Downward Merge statistic is similar to Kemeny's median, since the threshold does not allow the creation of any bucket order. Indeed, for the smallest values of $\delta$, the approximate robustness of the Downward Merge statistic is close to $0$, as for the Downward Merge statistic. \textcolor{red}{What was the bottom line here?}

When the threshold is set to a sensible value (here $\theta = 0.05$), the Downward Merge algorithm outputs a bucket order as a statistic: thus, the robustness increases very strongly to reach nearly optimal values even for very small values of $\delta$, which illustrates its efficiency. When $\theta=0.5$, the statistic is the bucket order regrouping all items. In this case, the statistic cannot be broken, and provide optimal values for the breakdown function. However, such a statistic does not provide any information about the distribution under analysis: its accuracy of location is very poor. Formally, the accuracy of location of a statistic $T$ is defined by its closeness (under the same metric $d$ used in its definition) to the whole ranking distribution: $AL_{d, p}(T) := \| d \|_{\infty} -\mathbb{E}_{p}(d(T(p), \Sigma))$, which is the opposite of the \textit{loss}, as simply defined by $Loss_{d,p}(T) = \mathbb{E}_{p}(d(T(p), \Sigma))$. By definition, under metric $d = d_{\tau}$, Kemeny's median has the highest accuracy of location, \textit{i.e.} the smallest loss. On the other hand, the Downward Merge statistic when $\theta=0.5$ has a very high loss, which makes it irrelevant in most cases. These observations justify the analysis of the loss/robustness tradeoff of our Downward Merge statistic compared to Kemeny's median.


\subsection{Tradeoffs between Loss and Robustness}
\label{subsec:tradeoffs_classic}

\begin{figure}
\centering
    \includegraphics[width=\linewidth]{img/tradeoffs_acc_rob_v2.png}
\caption{Loss/Robustness tradeoffs for different $p$ with $\delta=1$. Pairs of points linked by a black line denote results for Kemeny's median and Down. Merge statistics on the same distribution $p$ with $n=4$. "Buckets" are hand-picked distributions generated to be almost a point mass on a bucket order, "Uniform" (resp. "Point mass") "is an almost uniform (resp. point mass) hand-picked distribution, and "PL distribs." are random Plackett-Luce distributions.}
\label{fig:res_classic}
\end{figure}

We ran experiments for various distributions $p$ and computed the loss and the breakdown function of Kemeny's median and our Downward Merge algorithm to show the loss/robustness tradeoff for each statistic. \cref{fig:res_classic} shows the results for different choices of distribution $p$ when the number of items $n=4$, and for $\delta=1/6$ (normalized value of $\delta$ that requires at least a switch between two items to break the statistic).

The point mass (resp. the uniform) distribution represents the extreme case for which Kemeny's median is very robust (resp. not robust at all) and for which we expect no improvement from using the Downward Merge statistic. This intuition is verified in both cases, and we can see that the Downward Merge statistic yields the same results (in loss and in robustness) as Kemeny's median.

The bucket distributions (for which the gap between the probabilities for two rankings in the bucket order is respectively $0.1$ and $0.01$) represent the settings to which our Downward Merge is best suited. As expected, the improvement in robustness when using our Downward Merge statistic is high, and the increase in loss is negligible.

Finally, the Plackett Luce distributions (for which the parameters were generated randomly) represent a random setting. The results are interestingly very similar to those for the bucket distributions: the gain in robustness is high and the increase in loss is negligible. This random setting illustrates the usefulness of our Downward Merge statistic in general cases and shows that, overall, it yields a much better compromise than Kemeny's median.

\begin{comment}
\subsection{Scalable approximations}
\label{subsec:scalable_exp}

The breakdown function definition provided in \cref{eq:breakdown_fct_bucket} depends on TV distance between probability distributions $p$ and $q$ (the adversarial distribution for $p$), which needs to be computed at each step of our SGD-based optimization algorithm. These probability distributions are vectors of size $n!$, meaning that the aforementioned algorithm is overly computationally complex. A simple workaround to simplify the computation is to use remarks \cref{rk:sst_unique, rk:pairwise_simpl} and to relax the constraint from the breakdown function definition that states that the adversarial distribution must differ from $p$ by a budget measured using the TV distance. By replacing this constraint and measuring the difference between $p$ and $q$ using a norm on their respective probability matrix, for example, using $\| P-Q\|_1$, the computation is then able to run in $\text{poly}(n)$, which is much faster and allows for scalable experiments with a higher number of items $n$.

Such a relaxation induces modifying the computation of the breakdown function and thus may result in an imprecise approximation. However, the relation between the TV difference of probability distribution and the $L_1$ norm difference of the probability matrices is almost linear as illustrated by \cref{fig:proba_vs_pairwise}, meaning that we conserve a good approximation for the breakdown function.

\begin{figure}[h]
\centering
\label{fig:proba_vs_pairwise}
\includegraphics[width=0.8\linewidth]{icml_submission/img/proba_vs_pairwise.png}
\caption{Relation between $TV(p,q)$ and $\|P-Q\|_1$ (normalized).}
\end{figure}

On the same settings as in \cref{subsec:tradeoffs_classic} for a relevant distribution $p$, the tradeoffs between loss and robustness remain similar with this new pairwise relaxation, as illustrated by \cref{subfig:res_pairwise_n4}. Furthermore, \cref{subfig:res_pairwise_n8,subfig:res_pairwise_n8_delta3} also show that Maxpair statistics is also a better compromise in a more complex setting where the number of items $n = 8$ and where $\delta = 1$ or $3$. Note that in all the settings of \cref{fig:res_pairwise}, not only the robustness provided by Maxpair statistic is much higher, but also the loss is very close to that of Kemeny's median.

\begin{figure}
\centering
\begin{subfigure}{0.95\linewidth}
    \includegraphics[width=\linewidth]{icml_submission/img/pairwise_two_untied_n=4_seed=279.png}
    \caption{$n=4$}
    \label{subfig:res_pairwise_n4}
\end{subfigure}
\hfill
\begin{subfigure}{0.95\linewidth}
    \includegraphics[width=\linewidth]{icml_submission/img/pairwise_two_untied_n=8_seed=279.png}
    \caption{$n=8$}
    \label{subfig:res_pairwise_n8}
\end{subfigure}
\hfill
\begin{subfigure}{0.95\linewidth}
    \includegraphics[width=\linewidth]{icml_submission/img/pairwise_two_untied_n=8_delta=3_seed=279.png}
    \caption{$n=8, \delta=3$}
    \label{subfig:res_pairwise_n8_delta3}
\end{subfigure}
        
\caption{Loss/robustness tradeoffs for different values of $n$, with $\delta=1$ (\cref{subfig:res_pairwise_n4,subfig:res_pairwise_n8}) or $\delta=3$ (\cref{subfig:res_pairwise_n8_delta3}).}
\label{fig:res_pairwise}
\end{figure}

Overall, the experiments show that our Maxpair statistic provides a way to solve Consensus Aggregation in rankings in a scalable way.
\end{comment}



