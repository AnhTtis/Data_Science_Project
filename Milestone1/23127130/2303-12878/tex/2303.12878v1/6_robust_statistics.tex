
\section{Robust Consensus Ranking Statistics}
\label{sec:our_stats}

As proved by \cref{thm:breakdownfunctionkemeny}, the classical median statistics as defined by \eqref{eq:ranking_median} can be easily broken, which motivates defining more robust statistics, based on bucket rankings. As illustrated by \cref{fig:kemeny_on_indifference}, the weakness of median statistics comes from being ``forced" to rank all items, even those which are (almost) indistinguishable. Bucket rankings seem to be a natural solution to this problem, but \emph{what is a good way to build a bucket order statistic?}

As $H^{(NS)}_{d_\tau}$ defines a (pseudoquasi-) distance on $\wO$, we could adapt the idea of a median as in \eqref{eq:ranking_median} for bucket rankings. However, contrarily Borda medians which can be computed in a scalable way \cite{Caragiannis2013}, Hausdorff-based medians would require to optimize over $\wO$. As its cardinality is larger than $\frak{S}_n$ this problem can be more computationally challenging than Kemeny's median.

A more scalable approach is to start from a ranking median such as the Kemeny or Borda consensus and to robustify it using a plug-in method based on merging items that are close into buckets. \cref{fig:merge_limit} illustrates this idea. The left graph describes pairwise marginal probabilities for which the Kemeny consensus is $A\prec B\prec C\prec D$. Intuitively, merging either $C$ and $D$ (as $\lP(C \prec D) = 0.51)$ or $B$ and $C$ (as $\lP(B \prec C) = 0.52)$ leads to bucket rankings (i) and (ii), which will be harder to attack. However, this example also highlights that there is no unique way of merging items. For instance, if the constraint is to only merge items whose pairwise preference probability is in $[0.4, 0.6]$, it is possible to merge $B,C$ or $C,D$, but not $B,C,D$ as $\lP(B\prec D) = 0.7$: \emph{pairwise indistinguishability is not transitive}.


\begin{figure*}
\centering
\input{tikz/graphs_merge.tex}        
\caption{Left: Directed Graph that summarizes a pairwise marginal probability matrix. (i-iv) Graph representations of bucket orders that are compatible with merging items which pairwise preference probability is below 0.52 (i, ii) and below 0.7 (iii,iv).}
\label{fig:merge_limit}
\end{figure*}

\subsection{Na\"ive Merge Statistic}

% Introduce Merge algorithm
In order to formalize the latter intuition and to derive a first (na\"ive) plug-in rule, we define the pairwise preference probability between two items, which provides a relevant notion of closeness between items.
\begin{definition}
    {\sc (Pairwise probabilities)}. For $p\in\cM_+^1(\pS)$, the pairwise preference probability between items $i \text{ and } j$, denoted $P_{i,j}$, is defined for $i\neq j$ by: $P_{i,j} = \mathbb{P}_{\Sigma \sim p}(\Sigma(i) < \Sigma(j))$. By convention, $P_{ii} = 0.5$. We define the pairwise matrix of $p$ as $P:= [P_{i,j}]_{1 \leq i,j \leq n}$.
    \label{def:pairwise_matrix}
\end{definition}
Then, given a bucket ranking $\pi\in\wO$, we  formalize the notion that two buckets can be merged, with the constraint of not changing the strict order between buckets. To this end, we define $\bar{P}_{i}(\pi)$, the \emph{strongest deviation from indifference} between any two items within the $i^{\rm th}$ bucket $\pi^{(i)}$.
\begin{align}
\bar{P}_{i}(\pi) = \max \left\{\left|P_{l,l'} - 0.5\right| : (l,l')\in \pi^{(i)}\right\}
\end{align}
Then, one needs to quantify the value of $\bar{P}_{i}(\pi)$ that would result from merging bucket $i$ to bucket $j$,
\begin{align}
\bar{P}_{ij}(\pi) = \max \left\{\left|P_{l,l'} - \frac{1}{2}\right| : (l,l')\in \bigcup_{\substack{l\in[n]\\i\leq l \leq j}}\pi^{(l)}\right\}
\end{align}
Finally, given a threshold $\theta\in[0,0.5]$ on the acceptable deviation from indifference, we define the set of pairs of buckets that can be merged while keeping $\bar{P}$ below $\theta$,
\begin{align}
    \cG(\pi,\theta) = \left\{(i,j)\in [n]^2: \bar{P}_{ij}(\pi) \leq \theta\right\}
\end{align}
%A straightforward way to create ties is thus to put items in the same bucket if their pairwise probability is close to $1/2$. However, this first step needs to account for \textit{transitivity}. Intuitively, in cases where pairs of items (A,B) and (B,C) are close to each other, but not the pair (A,C), one must decide to either create a large bucket with the three items (and thus allow for transitivity) or not. We made the choice not to allow transitivity to prevent from having items too far from each other in the same bucket, and to ensure that all items in the same buckets have at most a fixed closeness.
% Moreover, inter-buckets ranking remains challenging in a general case, which can be addressed when restricting the analysis to \textit{(Strictly) Stochastically Transitive} distributions.
% \begin{definition}
%     {\sc (Stochastic transitivity)} A distribution $p\in\cM_+^1(\pS)$ is said to be stochastically transitive (ST) iif $\forall \, (i,j,k) \in [\![1,n]\!], P_{i,j} \geq 1/2$ and $P{j,k} \geq 1/2 \Rightarrow P_{i,k} \geq 1/2$. It is said to be strictly stochastically transitive if the inequalities are strict.
% \end{definition}
% When distribution $p\in\cM_+^1(\pS)$ is SST, the pairwise matrices provide a simpler way to characterize Kemeny's median:
% \begin{remark} {\sc (Uniqueness of the ranking median)} Let $p \in \Delta^{\frak{S}_n}$ be a SST distribution, and let $d_{\tau}$ be Kendall-tau distance. Then, the median $\sigma^{med}_{d_{\tau}}(p)$ is unique and can be defined as $\forall i \in [\![1, n ]\!],  \sigma^{med}_{d_{\tau}}(p)(i) = 1 + \sum_{k \neq i} \mathbb{1}(P_{i,k} < 1/2)$~ \cite{Korba2017}.
% \label{rk:sst_unique}
% \end{remark}
% This result derives from the definition of SST distributions and the following simplification remark.
% \begin{remark} {\sc Pairwise matrix simplification.} Let $d_{\tau}$ be the Kendall-tau distance, $p\in\cM_+^1(\pS)$ a distribution and $\sigma \in \frak{S}_n$ a ranking. Then $\mathbb{E}_{\Sigma \sim p}(d_{\tau}(\Sigma, \sigma)) = \sum_{i<j} P_{i,j} \mathbb{1}(\sigma(i) > \sigma(j)) + (1-P_{i,j})\mathbb{1}(\sigma(i) < \sigma(j))$~ \cite{Korba2017}.
% \label{rk:pairwise_simpl}
% \end{remark}
The first intuition is to merge buckets iteratively, starting with the most indifferent ones, as described in \cref{algo_naive_merge}.

\begin{algorithm}
\DontPrintSemicolon
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Pairwise matrix $P$, Ranking median $\sigma$, threshold $\theta \in [0, 0.5]$.}
$\pi \gets \sigma$ \tcp*{$\sigma$ as a bucket ranking}
\While{$\cG(\pi, \theta) \neq \emptyset$}{
    $(i^*, j^*) = \argmin_{(i,j)\in\cG(\pi,\theta)} \bar{P}_{ij}(\pi)$ \;
    update $\pi$ by merging all buckets between $i^*$ and $j^*$
    \vskip -2em
    \begin{flushleft}
        \begin{flalign*} 
            \begin{cases}
                \pi^{(i)} &\gets \pi^{(i)} ~~~\text{for}~ i < i^*\\
                \pi^{(i^*)} & \gets \bigcup_{l\in[n], i^*\leq l\leq j^*}\pi^{(l)}\\
                \pi^{(i - j^* + i^*)} & \gets \pi^{(i)} ~~~\text{for}~ i > j^*
            \end{cases}&&
        \end{flalign*}
    \end{flushleft}
    }
    \Output{$\pi$}
\caption{Na\"ive Merge}
\label{algo_naive_merge}
\end{algorithm}

Termination of \cref{algo_naive_merge} is guaranteed by the fact that the number of buckets in $\pi$ strictly decreases at each iteration. Then, by definition of $\cG(\pi, \theta)$, the resulting bucket ranking $\pi$ is such that any of its bucket $i$ satisfies $\bar{P}_i(\pi) \leq \theta$ -- \emph{i.e.} no two items with higher deviation than $\theta$ have been merged.
%Thus, when $p\in\cM_+^1(\pS)$ is SST, we derived the Naïve Merge algorithm to create a statistic allowing for bucket orders. Depending on a threshold $t$, a pair of items are put in the same bucket if their pairwise probability is the closest to $1/2$ and less than $1/2+t$. Then, the probability matrix is updated, and the algorithm continues recursively. Formally, the computation of the statistics is done using \cref{algo_merge}.
%A first remark on the construction of the Naïve Merge algorithm is the use of the max operator, which, as desired, prevents transitivity. 

Despite being very natural, this algorithm suffers from an important limitation: when changing the threshold $\theta$, its output only spans a limited subset of valid bucket rankings. In the example provided by \cref{fig:merge_limit}, the na\"ive merge method plugged-in on the Kemeny consensus can only output (i) and (iii). Whatever the value of $\theta$, it can never output (ii) or (iv). This limitation is induced by its outputs being a monotonic (w.r.t. to inclusion) function of $\theta$ -- \emph{i.e.} for $\theta_1 \leq \theta_2$, the resulting bucket rankings satisfy $\pi_{\theta_1} \subseteq \pi_{\theta_2}$. 


\subsection{Downward Merge Statistic}
% Introduce MaxPair algorithm

Overcoming this limitation only requires a small change in the algorithm which results in our main plug-in method named \emph{Downward Merge}, shown in \cref{algo_downward_merge}. 
Downward Merge algorithm selects the two buckets $(i^*, j^*)$ whose deviation from indifference $\bar{P}_{ij}(\pi)$ is maximal among those $\bar{P}_{ij}(\pi)\leq \theta$. \footnote{Instead of taking the most similar buckets, as in the previous statistic, we take the most different pair among those that are ``similar enough".} Then, all the buckets $l$ such that $i^*\leq l\leq j^*$ are merged. This process is repeated while there exist pairs of buckets whose deviation from indifference $\bar{P}_{ij}(\pi)\leq \theta$ and thus termination is guaranteed.

%Instead of starting by merging buckets that are the most indifferent, it starts by merging buckets which deviation from indifference is the highest (withing $\cG(\pi,\theta)$) and carries on going down. Formally, the Downward Merge algorithm is defined in \cref{algo_downward_merge}.
\begin{algorithm}
\DontPrintSemicolon
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Pairwise matrix $P$, Ranking median $\sigma$, threshold $t \in [0, 0.5]$.}
$\pi \gets \sigma$ \tcp*{$\sigma$ as a bucket ranking}
\While{$\cG(\pi, t) \neq \emptyset$}{
    $(i^*, j^*) = \argmax_{(i,j)\in\cG(\pi,t)} \bar{P}_{ij}(\pi)$ \;
    update $\pi$ by merging all buckets between $i^*$ and $j^*$
    \vskip -2em
    \begin{flushleft}
        \begin{flalign*} 
            \begin{cases}
                \pi^{(i)} &\gets \pi^{(i)} ~~~\text{for}~ i < i^*\\
                \pi^{(i^*)} & \gets \bigcup_{l\in[n], i^*\leq l\leq j^*}\pi^{(l)}\\
                \pi^{(i - j^* + i^*)} & \gets \pi^{(i)} ~~~\text{for}~ i > j^*
            \end{cases}&&
        \end{flalign*}
    \end{flushleft}
    }
    \Output{$\pi$}
\caption{Downward Merge}
\label{algo_downward_merge}
\end{algorithm}

The Downward Merge method is thus able to span a larger set of bucket orders when varying $\theta$. In the example from \cref{fig:merge_limit}, the Downward Merge method plugged-in on the Kemeny consensus can generate all four bucket rankings (i-iv) for $\theta\in \{0.51, 0.52, 0.69, 0.7)\}$.


The next experimental section illustrates the robustness improvement brought by this plug-in method over a ranking median.








