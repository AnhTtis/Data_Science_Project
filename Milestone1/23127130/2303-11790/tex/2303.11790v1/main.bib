
@article{wilkinson_fair_2016,
	title = {The {FAIR} {Guiding} {Principles} for scientific data management and stewardship},
	volume = {3},
	copyright = {2016 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201618},
	doi = {10.1038/sdata.2016.18},
	abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
	language = {en},
	number = {1},
	urldate = {2022-05-26},
	journal = {Scientific Data},
	author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’t Hoen, Peter A. C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	month = mar,
	year = {2016},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {160018},
	file = {Full Text PDF:/home/pape/Zotero/storage/EVVC366E/Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/X82UHHB3/sdata201618.html:text/html},
}

@article{moore_ome-ngff_2021,
	title = {{OME}-{NGFF}: a next-generation file format for expanding bioimaging data-access strategies},
	volume = {18},
	copyright = {2021 The Author(s)},
	issn = {1548-7105},
	shorttitle = {{OME}-{NGFF}},
	url = {https://www.nature.com/articles/s41592-021-01326-w},
	doi = {10.1038/s41592-021-01326-w},
	abstract = {The rapid pace of innovation in biological imaging and the diversity of its applications have prevented the establishment of a community-agreed standardized data format. We propose that complementing established open formats such as OME-TIFF and HDF5 with a next-generation file format such as Zarr will satisfy the majority of use cases in bioimaging. Critically, a common metadata format used in all these vessels can deliver truly findable, accessible, interoperable and reusable bioimaging data.},
	language = {en},
	number = {12},
	urldate = {2022-05-26},
	journal = {Nature Methods},
	author = {Moore, Josh and Allan, Chris and Besson, Sébastien and Burel, Jean-Marie and Diel, Erin and Gault, David and Kozlowski, Kevin and Lindner, Dominik and Linkert, Melissa and Manz, Trevor and Moore, Will and Pape, Constantin and Tischer, Christian and Swedlow, Jason R.},
	month = dec,
	year = {2021},
	note = {Number: 12
Publisher: Nature Publishing Group},
	pages = {1496--1498},
	file = {Full Text PDF:/home/pape/Zotero/storage/N75GILM6/Moore et al. - 2021 - OME-NGFF a next-generation file format for expand.pdf:application/pdf},
}

@misc{saalfeld_saalfeldlabn5_2022,
	title = {saalfeldlab/n5: n5-2.5.1},
	shorttitle = {saalfeldlab/n5},
	url = {https://zenodo.org/record/6578232},
	abstract = {Not HDF5},
	urldate = {2022-05-26},
	publisher = {Zenodo},
	author = {Saalfeld, Stephan and Pisarev, Igor and Hanslovsky, Philipp and Champion, Andrew and Rueden, Curtis and Bogovic, John and Kittisopikul, Mark and jakirkham},
	month = may,
	year = {2022},
	doi = {10.5281/zenodo.6578232},
	file = {Zenodo Snapshot:/home/pape/Zotero/storage/2R6IPZFS/6578232.html:text/html},
}

@article{pietzsch_bigdataviewer_2015,
	title = {{BigDataViewer}: visualization and processing for large image data sets},
	volume = {12},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	shorttitle = {{BigDataViewer}},
	url = {https://www.nature.com/articles/nmeth.3392},
	doi = {10.1038/nmeth.3392},
	language = {en},
	number = {6},
	urldate = {2022-05-26},
	journal = {Nature Methods},
	author = {Pietzsch, Tobias and Saalfeld, Stephan and Preibisch, Stephan and Tomancak, Pavel},
	month = jun,
	year = {2015},
	note = {Number: 6
Publisher: Nature Publishing Group},
	pages = {481--483},
	file = {Full Text PDF:/home/pape/Zotero/storage/3E94JAW2/Pietzsch et al. - 2015 - BigDataViewer visualization and processing for la.pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/GUMNKZZM/nmeth.html:text/html},
}

@misc{noauthor_imglib2generic_nodate,
	title = {{ImgLib2}—generic image processing in {Java} {\textbar} {Bioinformatics} {\textbar} {Oxford} {Academic}},
	url = {https://academic.oup.com/bioinformatics/article/28/22/3009/240540?login=false},
	urldate = {2022-05-26},
}

@misc{noauthor_fiji_nodate,
	title = {Fiji: an open-source platform for biological-image analysis {\textbar} {Nature} {Methods}},
	url = {https://www.nature.com/articles/nmeth.2019},
	urldate = {2022-05-26},
	file = {Fiji\: an open-source platform for biological-image analysis | Nature Methods:/home/pape/Zotero/storage/T7J5989Y/nmeth.html:text/html},
}

@article{vergara_whole-body_2021,
	title = {Whole-body integration of gene expression and single-cell morphology},
	volume = {184},
	issn = {0092-8674, 1097-4172},
	url = {https://www.cell.com/cell/abstract/S0092-8674(21)00876-X},
	doi = {10.1016/j.cell.2021.07.017},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}p{\textgreater}Animal bodies are composed of cell types with unique expression programs that implement their distinct locations, shapes, structures, and functions. Based on these properties, cell types assemble into specific tissues and organs. To systematically explore the link between cell-type-specific gene expression and morphology, we registered an expression atlas to a whole-body electron microscopy volume of the nereid \textit{Platynereis dumerilii}. Automated segmentation of cells and nuclei identifies major cell classes and establishes a link between gene activation, chromatin topography, and nuclear size. Clustering of segmented cells according to gene expression reveals spatially coherent tissues. In the brain, genetically defined groups of neurons match ganglionic nuclei with coherent projections. Besides interneurons, we uncover sensory-neurosecretory cells in the nereid mushroom bodies, which thus qualify as sensory organs. They furthermore resemble the vertebrate telencephalon by molecular anatomy. We provide an integrated browser as a Fiji plugin for remote exploration of all available multimodal datasets.{\textless}/p{\textgreater}},
	language = {English},
	number = {18},
	urldate = {2022-05-26},
	journal = {Cell},
	author = {Vergara, Hernando M. and Pape, Constantin and Meechan, Kimberly I. and Zinchenko, Valentyna and Genoud, Christel and Wanner, Adrian A. and Mutemi, Kevin Nzumbi and Titze, Benjamin and Templin, Rachel M. and Bertucci, Paola Y. and Simakov, Oleg and Dürichen, Wiebke and Machado, Pedro and Savage, Emily L. and Schermelleh, Lothar and Schwab, Yannick and Friedrich, Rainer W. and Kreshuk, Anna and Tischer, Christian and Arendt, Detlev},
	month = sep,
	year = {2021},
	pmid = {34380046},
	note = {Publisher: Elsevier},
	keywords = {mobie},
	pages = {4819--4837.e22},
	file = {Full Text PDF:/home/pape/Zotero/storage/KXEYUQKM/Vergara et al. - 2021 - Whole-body integration of gene expression and sing.pdf:application/pdf},
}

@article{pape_microscopy-based_2021,
	title = {Microscopy-based assay for semi-quantitative detection of {SARS}-{CoV}-2 specific antibodies in human sera},
	volume = {43},
	issn = {1521-1878},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bies.202000257},
	doi = {10.1002/bies.202000257},
	abstract = {Emergence of the novel pathogenic coronavirus SARS-CoV-2 and its rapid pandemic spread presents challenges that demand immediate attention. Here, we describe the development of a semi-quantitative high-content microscopy-based assay for detection of three major classes (IgG, IgA, and IgM) of SARS-CoV-2 specific antibodies in human samples. The possibility to detect antibodies against the entire viral proteome together with a robust semi-automated image analysis workflow resulted in specific, sensitive and unbiased assay that complements the portfolio of SARS-CoV-2 serological assays. Sensitive, specific and quantitative serological assays are urgently needed for a better understanding of humoral immune response against the virus as a basis for developing public health strategies to control viral spread. The procedure described here has been used for clinical studies and provides a general framework for the application of quantitative high-throughput microscopy to rapidly develop serological assays for emerging virus infections.},
	language = {en},
	number = {3},
	urldate = {2022-05-26},
	journal = {BioEssays},
	author = {Pape, Constantin and Remme, Roman and Wolny, Adrian and Olberg, Sylvia and Wolf, Steffen and Cerrone, Lorenzo and Cortese, Mirko and Klaus, Severina and Lucic, Bojana and Ullrich, Stephanie and Anders-Össwein, Maria and Wolf, Stefanie and Cerikan, Berati and Neufeldt, Christopher J. and Ganter, Markus and Schnitzler, Paul and Merle, Uta and Lusic, Marina and Boulant, Steeve and Stanifer, Megan and Bartenschlager, Ralf and Hamprecht, Fred A. and Kreshuk, Anna and Tischer, Christian and Kräusslich, Hans-Georg and Müller, Barbara and Laketa, Vibor},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bies.202000257},
	keywords = {mobie},
	pages = {2000257},
	file = {Full Text PDF:/home/pape/Zotero/storage/UQFF822G/Pape et al. - 2021 - Microscopy-based assay for semi-quantitative detec.pdf:application/pdf},
}

@article{cortese_integrative_2020,
	title = {Integrative {Imaging} {Reveals} {SARS}-{CoV}-2-{Induced} {Reshaping} of {Subcellular} {Morphologies}},
	volume = {28},
	issn = {1931-3128},
	url = {https://www.cell.com/cell-host-microbe/abstract/S1931-3128(20)30620-X},
	doi = {10.1016/j.chom.2020.11.003},
	language = {English},
	number = {6},
	urldate = {2022-05-26},
	journal = {Cell Host \& Microbe},
	author = {Cortese, Mirko and Lee, Ji-Young and Cerikan, Berati and Neufeldt, Christopher J. and Oorschot, Viola M. J. and Köhrer, Sebastian and Hennies, Julian and Schieber, Nicole L. and Ronchi, Paolo and Mizzon, Giulia and Romero-Brey, Inés and Santarella-Mellwig, Rachel and Schorb, Martin and Boermel, Mandy and Mocaer, Karel and Beckwith, Marianne S. and Templin, Rachel M. and Gross, Viktoriia and Pape, Constantin and Tischer, Christian and Frankish, Jamie and Horvat, Natalie K. and Laketa, Vibor and Stanifer, Megan and Boulant, Steeve and Ruggieri, Alessia and Chatel-Chaix, Laurent and Schwab, Yannick and Bartenschlager, Ralf},
	month = dec,
	year = {2020},
	pmid = {33245857},
	note = {Publisher: Elsevier},
	keywords = {mobie},
	pages = {853--866.e5},
	file = {Full Text PDF:/home/pape/Zotero/storage/SNUPZH6P/Cortese et al. - 2020 - Integrative Imaging Reveals SARS-CoV-2-Induced Res.pdf:application/pdf},
}

@article{saalfeld_catmaid_2009,
	title = {{CATMAID}: collaborative annotation toolkit for massive amounts of image data},
	volume = {25},
	issn = {1367-4803},
	shorttitle = {{CATMAID}},
	url = {https://doi.org/10.1093/bioinformatics/btp266},
	doi = {10.1093/bioinformatics/btp266},
	abstract = {Summary: High-resolution, three-dimensional (3D) imaging of large biological specimens generates massive image datasets that are difficult to navigate, annotate and share effectively. Inspired by online mapping applications like GoogleMaps™, we developed a decentralized web interface that allows seamless navigation of arbitrarily large image stacks. Our interface provides means for online, collaborative annotation of the biological image data and seamless sharing of regions of interest by bookmarking. The CATMAID interface enables synchronized navigation through multiple registered datasets even at vastly different scales such as in comparisons between optical and electron microscopy.Availability:http://fly.mpi-cbg.de/catmaidContact:tomancak@mpi-cbg.de},
	number = {15},
	urldate = {2022-05-26},
	journal = {Bioinformatics},
	author = {Saalfeld, Stephan and Cardona, Albert and Hartenstein, Volker and Tomančák, Pavel},
	month = aug,
	year = {2009},
	pages = {1984--1986},
	file = {Full Text PDF:/home/pape/Zotero/storage/3WMW75MG/Saalfeld et al. - 2009 - CATMAID collaborative annotation toolkit for mass.pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/IQRG5XTV/210794.html:text/html},
}

@misc{noauthor_webknossos_nodate,
	title = {{webKnossos}: efficient online {3D} data annotation for connectomics {\textbar} {Nature} {Methods}},
	url = {https://www.nature.com/articles/nmeth.4331},
	urldate = {2022-05-26},
}

@article{manz_viv_2022,
	title = {Viv: multiscale visualization of high-resolution multiplexed bioimaging data on the web},
	volume = {19},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {Viv},
	url = {https://www.nature.com/articles/s41592-022-01482-7},
	doi = {10.1038/s41592-022-01482-7},
	language = {en},
	number = {5},
	urldate = {2022-05-26},
	journal = {Nature Methods},
	author = {Manz, Trevor and Gold, Ilan and Patterson, Nathan Heath and McCallum, Chuck and Keller, Mark S. and Herr, Bruce W. and Börner, Katy and Spraggins, Jeffrey M. and Gehlenborg, Nils},
	month = may,
	year = {2022},
	note = {Number: 5
Publisher: Nature Publishing Group},
	pages = {515--516},
	file = {Full Text PDF:/home/pape/Zotero/storage/RIASJTXF/Manz et al. - 2022 - Viv multiscale visualization of high-resolution m.pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/VIYEDL6E/s41592-022-01482-7.html:text/html},
}

@misc{maitin-shepard_googleneuroglancer_2021,
	title = {google/neuroglancer:},
	shorttitle = {google/neuroglancer},
	url = {https://zenodo.org/record/5573294},
	abstract = {WebGL-based viewer for volumetric data},
	urldate = {2022-05-26},
	publisher = {Zenodo},
	author = {Maitin-Shepard, Jeremy and Baden, Alex and Silversmith, William and Perlman, Eric and Collman, Forrest and Blakely, Tim and Funke, Jan and Jordan, Chris and Falk, Ben and Kemnitz, Nico and tingzhao and Roat, Chris and Castro, Manuel and Jagannathan, Sridhar and moenigin and Clements, Jody and Hoag, Austin and Katz, Bill and Parsons, Dave and Wu, Jingpeng and Kamentsky, Lee and Chervakov, Pavel and Hubbard, Philip and Berg, Stuart and Hoffer, John and Halageri, Akhilesh and Machacek, Christian and Mader, Kevin and Roeder, Lutz and Li, Peter H.},
	month = oct,
	year = {2021},
	doi = {10.5281/zenodo.5573294},
	file = {Zenodo Snapshot:/home/pape/Zotero/storage/DSSI44XU/5573294.html:text/html},
}

@article{hartmann_image-based_2020,
	title = {An image-based data-driven analysis of cellular architecture in a developing tissue},
	volume = {9},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.55913},
	doi = {10.7554/eLife.55913},
	abstract = {Quantitative microscopy is becoming increasingly crucial in efforts to disentangle the complexity of organogenesis, yet adoption of the potent new toolbox provided by modern data science has been slow, primarily because it is often not directly applicable to developmental imaging data. We tackle this issue with a newly developed algorithm that uses point cloud-based morphometry to unpack the rich information encoded in 3D image data into a straightforward numerical representation. This enabled us to employ data science tools, including machine learning, to analyze and integrate cell morphology, intracellular organization, gene expression and annotated contextual knowledge. We apply these techniques to construct and explore a quantitative atlas of cellular architecture for the zebrafish posterior lateral line primordium, an experimentally tractable model of complex self-organized organogenesis. In doing so, we are able to retrieve both previously established and novel biologically relevant patterns, demonstrating the potential of our data-driven approach.},
	urldate = {2022-05-26},
	journal = {eLife},
	author = {Hartmann, Jonas and Wong, Mie and Gallo, Elisa and Gilmour, Darren},
	editor = {Bronner, Marianne E and Solnica-Krezel, Lilianna and Chitnis, Ajay B},
	month = jun,
	year = {2020},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {mobie},
	pages = {e55913},
	file = {Full Text PDF:/home/pape/Zotero/storage/9TIC3ZAC/Hartmann et al. - 2020 - An image-based data-driven analysis of cellular ar.pdf:application/pdf},
}

@misc{noauthor_profiling_nodate,
	title = {Profiling cellular diversity in sponges informs animal cell type and nervous system evolution},
	url = {https://www.science.org/doi/10.1126/science.abj2949},
	urldate = {2022-05-26},
	keywords = {mobie},
}

@techreport{ayuso-jimeno_identifying_2021,
	title = {Identifying long-range synaptic inputs using genetically encoded labels and volume electron microscopy},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2021.12.13.472405v1},
	abstract = {Enzymes that facilitate the local deposition of electron dense reaction products have been widely used as labels in electron microscopy (EM). Peroxidases, in particular, can efficiently metabolize 3,3’-diaminobenzidine tetrahydrochloride hydrate (DAB) to produce precipitates with high contrast under EM following heavy metal staining, and can be genetically encoded to facilitate the labeling of specific cell-types or organelles. Nevertheless, the peroxidase/DAB method has so far not been reported to work in combination with 3D volume EM techniques (e.g. Serial blockface electron microscopy, SBEM; Focused ion beam electron microscopy, FIBSEM) because the surfactant treatment needed for efficient reagent penetration disrupts tissue ultrastructure and because these methods require the deposition of large amounts of heavy metals that can obscure DAB precipitates. However, a recently described peroxidase with enhanced enzymatic activity (dAPEX2) appears to successfully deposit EM-visible DAB products in thick tissue without surfactant treatment. Here we demonstrate that multiplexed dAPEX2/DAB tagging is compatible with both FIBSEM and SBEM volume EM approaches and use them to map long-range genetically identified synaptic inputs from the anterior cingulate cortex to the periaqueductal gray in the mouse brain.},
	language = {en},
	urldate = {2022-05-26},
	institution = {bioRxiv},
	author = {Ayuso-Jimeno, Irene P. and Ronchi, Paolo and Wang, Tianzi and Gallori, Catherine and Gross, Cornelius T.},
	month = dec,
	year = {2021},
	doi = {10.1101/2021.12.13.472405},
	note = {Section: New Results
Type: article},
	keywords = {mobie},
	pages = {2021.12.13.472405},
	file = {Full Text PDF:/home/pape/Zotero/storage/F2DFM9XY/Ayuso-Jimeno et al. - 2021 - Identifying long-range synaptic inputs using genet.pdf:application/pdf},
}

@article{wolny_accurate_2020,
	title = {Accurate and versatile {3D} segmentation of plant tissues at cellular resolution},
	volume = {9},
	copyright = {All rights reserved},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.57613},
	doi = {10.7554/eLife.57613},
	abstract = {Quantitative analysis of plant and animal morphogenesis requires accurate segmentation of individual cells in volumetric images of growing organs. In the last years, deep learning has provided robust automated algorithms that approach human performance, with applications to bio-image analysis now starting to emerge. Here, we present PlantSeg, a pipeline for volumetric segmentation of plant tissues into cells. PlantSeg employs a convolutional neural network to predict cell boundaries and graph partitioning to segment cells based on the neural network predictions. PlantSeg was trained on fixed and live plant organs imaged with confocal and light sheet microscopes. PlantSeg delivers accurate results and generalizes well across different tissues, scales, acquisition settings even on non plant samples. We present results of PlantSeg applications in diverse developmental contexts. PlantSeg is free and open-source, with both a command line and a user-friendly graphical interface.},
	urldate = {2022-05-26},
	journal = {eLife},
	author = {Wolny, Adrian and Cerrone, Lorenzo and Vijayan, Athul and Tofanelli, Rachele and Barro, Amaya Vilches and Louveaux, Marion and Wenzl, Christian and Strauss, Sören and Wilson-Sánchez, David and Lymbouridou, Rena and Steigleder, Susanne S and Pape, Constantin and Bailoni, Alberto and Duran-Nebreda, Salva and Bassel, George W and Lohmann, Jan U and Tsiantis, Miltos and Hamprecht, Fred A and Schneitz, Kay and Maizel, Alexis and Kreshuk, Anna},
	editor = {Hardtke, Christian S and Bergmann, Dominique C and Bergmann, Dominique C and Graeff, Moritz},
	month = jul,
	year = {2020},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {mobie},
	pages = {e57613},
	file = {Full Text PDF:/home/pape/Zotero/storage/34QLTJLI/Wolny et al. - 2020 - Accurate and versatile 3D segmentation of plant ti.pdf:application/pdf},
}

@article{uwizeye_morphological_2021,
	title = {Morphological bases of phytoplankton energy management and physiological responses unveiled by {3D} subcellular imaging},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-21314-0},
	doi = {10.1038/s41467-021-21314-0},
	abstract = {Eukaryotic phytoplankton have a small global biomass but play major roles in primary production and climate. Despite improved understanding of phytoplankton diversity and evolution, we largely ignore the cellular bases of their environmental plasticity. By comparative 3D morphometric analysis across seven distant phytoplankton taxa, we observe constant volume occupancy by the main organelles and preserved volumetric ratios between plastids and mitochondria. We hypothesise that phytoplankton subcellular topology is modulated by energy-management constraints. Consistent with this, shifting the diatom Phaeodactylum from low to high light enhances photosynthesis and respiration, increases cell-volume occupancy by mitochondria and the plastid CO2-fixing pyrenoid, and boosts plastid-mitochondria contacts. Changes in organelle architectures and interactions also accompany Nannochloropsis acclimation to different trophic lifestyles, along with respiratory and photosynthetic responses. By revealing evolutionarily-conserved topologies of energy-managing organelles, and their role in phytoplankton acclimation, this work deciphers phytoplankton responses at subcellular scales.},
	language = {en},
	number = {1},
	urldate = {2022-05-26},
	journal = {Nature Communications},
	author = {Uwizeye, Clarisse and Decelle, Johan and Jouneau, Pierre-Henri and Flori, Serena and Gallet, Benoit and Keck, Jean-Baptiste and Bo, Davide Dal and Moriscot, Christine and Seydoux, Claire and Chevalier, Fabien and Schieber, Nicole L. and Templin, Rachel and Allorent, Guillaume and Courtois, Florence and Curien, Gilles and Schwab, Yannick and Schoehn, Guy and Zeeman, Samuel C. and Falconet, Denis and Finazzi, Giovanni},
	month = feb,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {mobie},
	pages = {1049},
	file = {Full Text PDF:/home/pape/Zotero/storage/RY4SVVPT/Uwizeye et al. - 2021 - Morphological bases of phytoplankton energy manage.pdf:application/pdf},
}

@article{klein_elastix_2010,
	title = {elastix: {A} {Toolbox} for {Intensity}-{Based} {Medical} {Image} {Registration}},
	volume = {29},
	issn = {1558-254X},
	shorttitle = {elastix},
	doi = {10.1109/TMI.2009.2035616},
	abstract = {Medical image registration is an important task in medical image processing. It refers to the process of aligning data sets, possibly from different modalities (e.g., magnetic resonance and computed tomography), different time points (e.g., follow-up scans), and/or different subjects (in case of population studies). A large number of methods for image registration are described in the literature. Unfortunately, there is not one method that works for all applications. We have therefore developed elastix, a publicly available computer program for intensity-based medical image registration. The software consists of a collection of algorithms that are commonly used to solve medical image registration problems. The modular design of elastix allows the user to quickly configure, test, and compare different registration methods for a specific application. The command-line interface enables automated processing of large numbers of data sets, by means of scripting. The usage of elastix for comparing different registration methods is illustrated with three example experiments, in which individual components of the registration method are varied.},
	number = {1},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Klein, Stefan and Staring, Marius and Murphy, Keelin and Viergever, Max A. and Pluim, Josien P. W.},
	month = jan,
	year = {2010},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	pages = {196--205},
}

@article{paul-gilloteaux_ec-clem_2017,
	title = {{eC}-{CLEM}: flexible multidimensional registration software for correlative microscopies},
	volume = {14},
	copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	shorttitle = {{eC}-{CLEM}},
	url = {https://www.nature.com/articles/nmeth.4170},
	doi = {10.1038/nmeth.4170},
	language = {en},
	number = {2},
	urldate = {2022-05-26},
	journal = {Nature Methods},
	author = {Paul-Gilloteaux, Perrine and Heiligenstein, Xavier and Belle, Martin and Domart, Marie-Charlotte and Larijani, Banafshe and Collinson, Lucy and Raposo, Graça and Salamero, Jean},
	month = feb,
	year = {2017},
	note = {Number: 2
Publisher: Nature Publishing Group},
	pages = {102--103},
	file = {Full Text PDF:/home/pape/Zotero/storage/ZE9UWJP9/Paul-Gilloteaux et al. - 2017 - eC-CLEM flexible multidimensional registration so.pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/C8L3KH4G/nmeth.html:text/html},
}

@article{schorb_software_2019,
	title = {Software tools for automated transmission electron microscopy},
	volume = {16},
	copyright = {2019 Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-019-0396-9},
	doi = {10.1038/s41592-019-0396-9},
	abstract = {The demand for high-throughput data collection in electron microscopy is increasing for applications in structural and cellular biology. Here we present a combination of software tools that enable automated acquisition guided by image analysis for a variety of transmission electron microscopy acquisition schemes. SerialEM controls microscopes and detectors and can trigger automated tasks at multiple positions with high flexibility. Py-EM interfaces with SerialEM to enact specimen-specific image-analysis pipelines that enable feedback microscopy. As example applications, we demonstrate dose reduction in cryo-electron microscopy experiments, fully automated acquisition of every cell in a plastic section and automated targeting on serial sections for 3D volume imaging across multiple grids.},
	language = {en},
	number = {6},
	urldate = {2022-05-26},
	journal = {Nature Methods},
	author = {Schorb, Martin and Haberbosch, Isabella and Hagen, Wim J. H. and Schwab, Yannick and Mastronarde, David N.},
	month = jun,
	year = {2019},
	note = {Number: 6
Publisher: Nature Publishing Group},
	pages = {471--477},
	file = {Full Text PDF:/home/pape/Zotero/storage/2RVGFXLW/Schorb et al. - 2019 - Software tools for automated transmission electron.pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/4EQQTJ5W/s41592-019-0396-9.html:text/html},
}

@article{held_cellcognition_2010,
	title = {{CellCognition}: time-resolved phenotype annotation in high-throughput live cell imaging},
	volume = {7},
	copyright = {2010 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	shorttitle = {{CellCognition}},
	url = {https://www.nature.com/articles/nmeth.1486},
	doi = {10.1038/nmeth.1486},
	abstract = {Incorporation of time information into the annotation of distinct biological states in automated fluorescence time-lapse live-cell imaging of complex cellular dynamics reduces both classification noise and confusion between cell states with similar morphology. A computational framework for achieving this is implemented in the open-source software package CellCognition.},
	language = {en},
	number = {9},
	urldate = {2022-05-26},
	journal = {Nature Methods},
	author = {Held, Michael and Schmitz, Michael H. A. and Fischer, Bernd and Walter, Thomas and Neumann, Beate and Olma, Michael H. and Peter, Matthias and Ellenberg, Jan and Gerlich, Daniel W.},
	month = sep,
	year = {2010},
	note = {Number: 9
Publisher: Nature Publishing Group},
	pages = {747--754},
	file = {Full Text PDF:/home/pape/Zotero/storage/D9WFEHTZ/Held et al. - 2010 - CellCognition time-resolved phenotype annotation .pdf:application/pdf},
}

@inproceedings{bogovic_robust_2016,
	title = {Robust registration of calcium images by learned contrast synthesis},
	doi = {10.1109/ISBI.2016.7493463},
	abstract = {Multi-modal image registration is a challenging task that is vital to fuse complementary signals for subsequent analyses. Despite much research into cost functions addressing this challenge, there exist cases in which these are ineffective. In this work, we show that (1) this is true for the registration of in-vivo Drosophila brain volumes visualizing genetically encoded calcium indicators to an nc82 atlas and (2) that machine learning based contrast synthesis can yield improvements. More specifically, the number of subjects for which the registration outright failed was greatly reduced (from 40\% to 15\%) by using a synthesized image.},
	booktitle = {2016 {IEEE} 13th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Bogovic, John A. and Hanslovsky, Philipp and Wong, Allan and Saalfeld, Stephan},
	month = apr,
	year = {2016},
	note = {ISSN: 1945-8452},
	pages = {1123--1126},
}

@article{schmid_high-level_2010,
	title = {A high-level {3D} visualization {API} for {Java} and {ImageJ}},
	volume = {11},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/1471-2105-11-274},
	doi = {10.1186/1471-2105-11-274},
	abstract = {Current imaging methods such as Magnetic Resonance Imaging (MRI), Confocal microscopy, Electron Microscopy (EM) or Selective Plane Illumination Microscopy (SPIM) yield three-dimensional (3D) data sets in need of appropriate computational methods for their analysis. The reconstruction, segmentation and registration are best approached from the 3D representation of the data set.},
	number = {1},
	urldate = {2022-05-26},
	journal = {BMC Bioinformatics},
	author = {Schmid, Benjamin and Schindelin, Johannes and Cardona, Albert and Longair, Mark and Heisenberg, Martin},
	month = may,
	year = {2010},
	pages = {274},
	file = {Full Text PDF:/home/pape/Zotero/storage/LGHCMJZK/Schmid et al. - 2010 - A high-level 3D visualization API for Java and Ima.pdf:application/pdf},
}

@article{hartley_bioimage_2022,
	title = {The {BioImage} {Archive} – {Building} a {Home} for {Life}-{Sciences} {Microscopy} {Data}},
	issn = {0022-2836},
	url = {https://www.sciencedirect.com/science/article/pii/S0022283622000791},
	doi = {10.1016/j.jmb.2022.167505},
	abstract = {Despite the huge impact of data resources in genomics and structural biology, until now there has been no central archive for biological data for all imaging modalities. The BioImage Archive is a new data resource at the European Bioinformatics Institute (EMBL-EBI) designed to fill this gap. In its initial development BioImage Archive accepts bioimaging data associated with publications, in any format, from any imaging modality from the molecular to the organism scale, excluding medical imaging. The BioImage Archive will ensure reproducibility of published studies that derive results from image data and reduce duplication of effort. Most importantly, the BioImage Archive will help scientists to generate new insights through reuse of existing data to answer new biological questions, and provision of training, testing and benchmarking data for development of tools for image analysis. The archive is available at https://www.ebi.ac.uk/bioimage-archive/.},
	language = {en},
	urldate = {2022-05-26},
	journal = {Journal of Molecular Biology},
	author = {Hartley, Matthew and Kleywegt, Gerard J. and Patwardhan, Ardan and Sarkans, Ugis and Swedlow, Jason R. and Brazma, Alvis},
	month = feb,
	year = {2022},
	pages = {167505},
}

@article{heinrich_whole-cell_2021,
	title = {Whole-cell organelle segmentation in volume electron microscopy},
	volume = {599},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03977-3},
	doi = {10.1038/s41586-021-03977-3},
	abstract = {Cells contain hundreds of organelles and macromolecular assemblies. Obtaining a complete understanding of their intricate organization requires the nanometre-level, three-dimensional reconstruction of whole cells, which is only feasible with robust and scalable automatic methods. Here, to support the development of such methods, we annotated up to 35 different cellular organelle classes—ranging from endoplasmic reticulum to microtubules to ribosomes—in diverse sample volumes from multiple cell types imaged at a near-isotropic resolution of 4 nm per voxel with focused ion beam scanning electron microscopy (FIB-SEM)1. We trained deep learning architectures to segment these structures in 4 nm and 8 nm per voxel FIB-SEM volumes, validated their performance and showed that automatic reconstructions can be used to directly quantify previously inaccessible metrics including spatial interactions between cellular components. We also show that such reconstructions can be used to automatically register light and electron microscopy images for correlative studies. We have created an open data and open-source web repository, ‘OpenOrganelle’, to share the data, computer code and trained models, which will enable scientists everywhere to query and further improve automatic reconstruction of these datasets.},
	language = {en},
	number = {7883},
	urldate = {2022-05-26},
	journal = {Nature},
	author = {Heinrich, Larissa and Bennett, Davis and Ackerman, David and Park, Woohyun and Bogovic, John and Eckstein, Nils and Petruncio, Alyson and Clements, Jody and Pang, Song and Xu, C. Shan and Funke, Jan and Korff, Wyatt and Hess, Harald F. and Lippincott-Schwartz, Jennifer and Saalfeld, Stephan and Weigel, Aubrey V.},
	month = nov,
	year = {2021},
	note = {Number: 7883
Publisher: Nature Publishing Group},
	pages = {141--146},
	file = {Full Text PDF:/home/pape/Zotero/storage/SVRGU6PQ/Heinrich et al. - 2021 - Whole-cell organelle segmentation in volume electr.pdf:application/pdf},
}

@article{de_chaumont_icy_2012,
	title = {Icy: an open bioimage informatics platform for extended reproducible research},
	volume = {9},
	copyright = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	shorttitle = {Icy},
	url = {https://www.nature.com/articles/nmeth.2075},
	doi = {10.1038/nmeth.2075},
	abstract = {Icy is a collaborative platform for biological image analysis that extends reproducible research principles by facilitating and stimulating the contribution and sharing of algorithm-based tools and protocols between researchers.},
	language = {en},
	number = {7},
	urldate = {2022-05-26},
	journal = {Nature Methods},
	author = {de Chaumont, Fabrice and Dallongeville, Stéphane and Chenouard, Nicolas and Hervé, Nicolas and Pop, Sorin and Provoost, Thomas and Meas-Yedid, Vannary and Pankajakshan, Praveen and Lecomte, Timothée and Le Montagner, Yoann and Lagache, Thibault and Dufour, Alexandre and Olivo-Marin, Jean-Christophe},
	month = jul,
	year = {2012},
	note = {Number: 7
Publisher: Nature Publishing Group},
	pages = {690--696},
	file = {Full Text PDF:/home/pape/Zotero/storage/9C7GNG8N/de Chaumont et al. - 2012 - Icy an open bioimage informatics platform for ext.pdf:application/pdf},
}

@misc{noauthor_napari_nodate,
	title = {napari — napari},
	url = {https://napari.org/},
	urldate = {2022-05-26},
}

@article{horl_bigstitcher_2019,
	title = {{BigStitcher}: reconstructing high-resolution image datasets of cleared and expanded samples},
	volume = {16},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {{BigStitcher}},
	url = {https://www.nature.com/articles/s41592-019-0501-0},
	doi = {10.1038/s41592-019-0501-0},
	abstract = {Light-sheet imaging of cleared and expanded samples creates terabyte-sized datasets that consist of many unaligned three-dimensional image tiles, which must be reconstructed before analysis. We developed the BigStitcher software to address this challenge. BigStitcher enables interactive visualization, fast and precise alignment, spatially resolved quality estimation, real-time fusion and deconvolution of dual-illumination, multitile, multiview datasets. The software also compensates for optical effects, thereby improving accuracy and enabling subsequent biological analysis.},
	language = {en},
	number = {9},
	urldate = {2022-05-26},
	journal = {Nature Methods},
	author = {Hörl, David and Rojas Rusak, Fabio and Preusser, Friedrich and Tillberg, Paul and Randel, Nadine and Chhetri, Raghav K. and Cardona, Albert and Keller, Philipp J. and Harz, Hartmann and Leonhardt, Heinrich and Treier, Mathias and Preibisch, Stephan},
	month = sep,
	year = {2019},
	note = {Number: 9
Publisher: Nature Publishing Group},
	pages = {870--874},
	file = {Full Text PDF:/home/pape/Zotero/storage/9P4QQPKU/Hörl et al. - 2019 - BigStitcher reconstructing high-resolution image .pdf:application/pdf},
}

@article{linkert_metadata_2010,
	title = {Metadata matters: access to image data in the real world},
	volume = {189},
	issn = {0021-9525},
	shorttitle = {Metadata matters},
	url = {https://doi.org/10.1083/jcb.201004104},
	doi = {10.1083/jcb.201004104},
	abstract = {Data sharing is important in the biological sciences to prevent duplication of effort, to promote scientific integrity, and to facilitate and disseminate scientific discovery. Sharing requires centralized repositories, and submission to and utility of these resources require common data formats. This is particularly challenging for multidimensional microscopy image data, which are acquired from a variety of platforms with a myriad of proprietary file formats (PFFs). In this paper, we describe an open standard format that we have developed for microscopy image data. We call on the community to use open image data standards and to insist that all imaging platforms support these file formats. This will build the foundation for an open image data repository.},
	number = {5},
	urldate = {2022-05-26},
	journal = {Journal of Cell Biology},
	author = {Linkert, Melissa and Rueden, Curtis T. and Allan, Chris and Burel, Jean-Marie and Moore, Will and Patterson, Andrew and Loranger, Brian and Moore, Josh and Neves, Carlos and MacDonald, Donald and Tarkowska, Aleksandra and Sticco, Caitlin and Hill, Emma and Rossner, Mike and Eliceiri, Kevin W. and Swedlow, Jason R.},
	month = may,
	year = {2010},
	pages = {777--782},
	file = {Full Text PDF:/home/pape/Zotero/storage/NIL9BBJ4/Linkert et al. - 2010 - Metadata matters access to image data in the real.pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/WHFGUPF8/Metadata-matters-access-to-image-data-in-the-real.html:text/html},
}

@article{glasbey_colour_2007,
	title = {Colour displays for categorical images},
	volume = {32},
	issn = {1520-6378},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/col.20327},
	doi = {10.1002/col.20327},
	abstract = {We propose a method for identifying a set of colours for displaying 2D and 3D categorical images when the categories are unordered labels. The principle is to find maximally distinct sets of colours. We either generate colours sequentially, to maximize the dissimilarity or distance between a new colour and the set of colours already chosen, or use a simulated annealing algorithm to find a set of colours of specified size. In both cases, we use a Euclidean metric on the perceptual colour space, CIELAB, to specify distances. © 2007 Wiley Periodicals, Inc. Col Res Appl, 32, 304–309, 2007},
	language = {en},
	number = {4},
	urldate = {2022-05-26},
	journal = {Color Research \& Application},
	author = {Glasbey, Chris and van der Heijden, Gerie and Toh, Vivian F. K. and Gray, Alision},
	year = {2007},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/col.20327},
	pages = {304--309},
	file = {Accepted Version:/home/pape/Zotero/storage/4TTZVBER/Glasbey et al. - 2007 - Colour displays for categorical images.pdf:application/pdf},
}

@article{kukulski_correlated_2011,
	title = {Correlated fluorescence and {3D} electron microscopy with high sensitivity and spatial precision},
	volume = {192},
	issn = {0021-9525},
	url = {https://doi.org/10.1083/jcb.201009037},
	doi = {10.1083/jcb.201009037},
	abstract = {Correlative electron and fluorescence microscopy has the potential to elucidate the ultrastructural details of dynamic and rare cellular events, but has been limited by low precision and sensitivity. Here we present a method for direct mapping of signals originating from ∼20 fluorescent protein molecules to 3D electron tomograms with a precision of less than 100 nm. We demonstrate that this method can be used to identify individual HIV particles bound to mammalian cell surfaces. We also apply the method to image microtubule end structures bound to mal3p in fission yeast, and demonstrate that growing microtubule plus-ends are flared in vivo. We localize Rvs167 to endocytic sites in budding yeast, and show that scission takes place halfway through a 10-s time period during which amphiphysins are bound to the vesicle neck. This new technique opens the door for direct correlation of fluorescence and electron microscopy to visualize cellular processes at the ultrastructural scale.},
	number = {1},
	urldate = {2022-05-26},
	journal = {Journal of Cell Biology},
	author = {Kukulski, Wanda and Schorb, Martin and Welsch, Sonja and Picco, Andrea and Kaksonen, Marko and Briggs, John A.G.},
	month = jan,
	year = {2011},
	pages = {111--119},
	file = {Full Text PDF:/home/pape/Zotero/storage/GZ6XLKE5/Kukulski et al. - 2011 - Correlated fluorescence and 3D electron microscopy.pdf:application/pdf},
}

@article{lohoff_integration_2022,
	title = {Integration of spatial and single-cell transcriptomic data elucidates mouse organogenesis},
	volume = {40},
	copyright = {2021 The Author(s)},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-021-01006-2},
	doi = {10.1038/s41587-021-01006-2},
	abstract = {Molecular profiling of single cells has advanced our knowledge of the molecular basis of development. However, current approaches mostly rely on dissociating cells from tissues, thereby losing the crucial spatial context of regulatory processes. Here, we apply an image-based single-cell transcriptomics method, sequential fluorescence in situ hybridization (seqFISH), to detect mRNAs for 387 target genes in tissue sections of mouse embryos at the 8–12 somite stage. By integrating spatial context and multiplexed transcriptional measurements with two single-cell transcriptome atlases, we characterize cell types across the embryo and demonstrate that spatially resolved expression of genes not profiled by seqFISH can be imputed. We use this high-resolution spatial map to characterize fundamental steps in the patterning of the midbrain–hindbrain boundary (MHB) and the developing gut tube. We uncover axes of cell differentiation that are not apparent from single-cell RNA-sequencing (scRNA-seq) data, such as early dorsal–ventral separation of esophageal and tracheal progenitor populations in the gut tube. Our method provides an approach for studying cell fate decisions in complex tissues and development.},
	language = {en},
	number = {1},
	urldate = {2022-11-15},
	journal = {Nature Biotechnology},
	author = {Lohoff, T. and Ghazanfar, S. and Missarova, A. and Koulena, N. and Pierson, N. and Griffiths, J. A. and Bardot, E. S. and Eng, C.-H. L. and Tyser, R. C. V. and Argelaguet, R. and Guibentif, C. and Srinivas, S. and Briscoe, J. and Simons, B. D. and Hadjantonakis, A.-K. and Göttgens, B. and Reik, W. and Nichols, J. and Cai, L. and Marioni, J. C.},
	month = jan,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {mobie},
	pages = {74--85},
	file = {Full Text PDF:/home/pape/Zotero/storage/79ZFQHZS/Lohoff et al. - 2022 - Integration of spatial and single-cell transcripto.pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/3FQ9WV8E/s41587-021-01006-2.html:text/html},
}

@article{williams_image_2017,
	title = {Image {Data} {Resource}: a bioimage data integration and publication platform},
	volume = {14},
	copyright = {2017 The Author(s)},
	issn = {1548-7105},
	shorttitle = {Image {Data} {Resource}},
	url = {https://www.nature.com/articles/nmeth.4326},
	doi = {10.1038/nmeth.4326},
	abstract = {This Resource describes the Image Data Resource (IDR), a prototype online system for biological image data that links experimental and analytic data across multiple data sets and promotes image data sharing and reanalysis.},
	language = {en},
	number = {8},
	urldate = {2022-11-15},
	journal = {Nature Methods},
	author = {Williams, Eleanor and Moore, Josh and Li, Simon W. and Rustici, Gabriella and Tarkowska, Aleksandra and Chessel, Anatole and Leo, Simone and Antal, Bálint and Ferguson, Richard K. and Sarkans, Ugis and Brazma, Alvis and Carazo Salas, Rafael E. and Swedlow, Jason R.},
	month = aug,
	year = {2017},
	note = {Number: 8
Publisher: Nature Publishing Group},
	pages = {775--781},
	file = {Full Text PDF:/home/pape/Zotero/storage/H5WV7ICH/Williams et al. - 2017 - Image Data Resource a bioimage data integration a.pdf:application/pdf},
}

@misc{emmanuel_saliencenet_2022,
	title = {{SalienceNet}: an unsupervised {Image}-to-{Image} translation method for nuclei saliency enhancement in microscopy images},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	shorttitle = {{SalienceNet}},
	url = {https://www.biorxiv.org/content/10.1101/2022.10.27.514030v2},
	doi = {10.1101/2022.10.27.514030},
	abstract = {Automatic segmentation of nuclei in low-light microscopy images remains a difficult task, especially for high-throughput experiments where need for automation is strong. Low saliency of nuclei with respect to the background, variability of their intensity together with low signal-to-noise ratio in these images constitute a major challenge for mainstream algorithms of nuclei segmentation. In this work we introduce SalienceNet, an unsupervised deep learning-based method that uses the style transfer properties of cycleGAN to transform low saliency images into high saliency images, thus enabling accurate segmentation by downstream analysis methods, and that without need for any parameter tuning. We have acquired a novel dataset of organoid images with soSPIM, a microscopy technique that enables the acquisition of images in low-light conditions. Our experiments show that SalienceNet increased the saliency of these images up to the desired level. Moreover, we evaluated the impact of SalienceNet on segmentation for both Otsu thresholding and StarDist and have shown that enhancing nuclei with SalienceNet improved segmentation results using Otsu thresholding by 30\% and using StarDist by 26\% in terms of IOU when compared to segmentation of non-enhanced images. Together these results show that SalienceNet can be used as a common preprocessing step to automate nuclei segmentation pipelines for low-light microscopy images.},
	language = {en},
	urldate = {2022-11-23},
	publisher = {bioRxiv},
	author = {Emmanuel, Bouilhol and Lefevre, Edgar and Barry, Thierno and Levet, Florian and Beghin, Anne and Viasnoff, Virgile and Galindo, Xareni and Galland, Rémi and Sibarita, Jean-Baptiste and Nikolski, Macha},
	month = oct,
	year = {2022},
	note = {Pages: 2022.10.27.514030
Section: New Results},
	file = {Full Text PDF:/home/pape/Zotero/storage/JCJYX2BJ/Emmanuel et al. - 2022 - SalienceNet an unsupervised Image-to-Image transl.pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/KGX8UQ7N/2022.10.27.html:text/html},
}

@misc{dorkenwald_multi-layered_2022,
	title = {Multi-{Layered} {Maps} of {Neuropil} with {Segmentation}-{Guided} {Contrastive} {Learning}},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.03.29.486320v2},
	doi = {10.1101/2022.03.29.486320},
	abstract = {Maps of the nervous system that identify individual cells along with their type, subcellular components, and connectivity have the potential to reveal fundamental organizational principles of neural circuits. Volumetric nanometer-resolution imaging of brain tissue provides the raw data needed to build such maps, but inferring all the relevant cellular and subcellular annotation layers is challenging. Here, we present Segmentation-Guided Contrastive Learning of Representations (“SegCLR”), a self-supervised machine learning technique that produces highly informative representations of cells directly from 3d electron microscope imagery and segmentations. When applied to volumes of human and mouse cerebral cortex, SegCLR enabled the classification of cellular subcompartments (axon, dendrite, soma, astrocytic process) with 4,000-fold less labeled data compared to fully supervised approaches. Surprisingly, SegCLR also enabled inference of cell types (neurons, glia, and subtypes of each) from fragments with lengths as small as 10 micrometers, a task that can be difficult for humans to perform and whose feasibility greatly enhances the utility of imaging portions of brains in which many neuron fragments terminate at a volume boundary. These predictions were further augmented via Gaussian process uncertainty estimation to enable analyses restricted to high confidence subsets of the data. Finally, SegCLR enabled detailed exploration of layer-5 pyramidal cell subtypes and automated large-scale statistical analysis of upstream and downstream synaptic partners in mouse visual cortex.},
	language = {en},
	urldate = {2022-11-23},
	publisher = {bioRxiv},
	author = {Dorkenwald, Sven and Li, Peter H. and Januszewski, Michał and Berger, Daniel R. and Maitin-Shepard, Jeremy and Bodor, Agnes L. and Collman, Forrest and Schneider-Mizell, Casey M. and Costa, Nuno Maçarico da and Lichtman, Jeff W. and Jain, Viren},
	month = nov,
	year = {2022},
	note = {Pages: 2022.03.29.486320
Section: New Results},
	file = {Full Text PDF:/home/pape/Zotero/storage/HMP87EWV/Dorkenwald et al. - 2022 - Multi-Layered Maps of Neuropil with Segmentation-G.pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/VG8KRCEE/2022.03.29.html:text/html},
}

@article{christiansen_silico_2018,
	title = {In {Silico} {Labeling}: {Predicting} {Fluorescent} {Labels} in {Unlabeled} {Images}},
	volume = {173},
	issn = {0092-8674},
	shorttitle = {In {Silico} {Labeling}},
	url = {https://www.sciencedirect.com/science/article/pii/S0092867418303647},
	doi = {10.1016/j.cell.2018.03.040},
	abstract = {Microscopy is a central method in life sciences. Many popular methods, such as antibody labeling, are used to add physical fluorescent labels to specific cellular constituents. However, these approaches have significant drawbacks, including inconsistency; limitations in the number of simultaneous labels because of spectral overlap; and necessary perturbations of the experiment, such as fixing the cells, to generate the measurement. Here, we show that a computational machine-learning approach, which we call “in silico labeling” (ISL), reliably predicts some fluorescent labels from transmitted-light images of unlabeled fixed or live biological samples. ISL predicts a range of labels, such as those for nuclei, cell type (e.g., neural), and cell state (e.g., cell death). Because prediction happens in silico, the method is consistent, is not limited by spectral overlap, and does not disturb the experiment. ISL generates biological measurements that would otherwise be problematic or impossible to acquire.},
	language = {en},
	number = {3},
	urldate = {2022-11-23},
	journal = {Cell},
	author = {Christiansen, Eric M. and Yang, Samuel J. and Ando, D. Michael and Javaherian, Ashkan and Skibinski, Gaia and Lipnick, Scott and Mount, Elliot and O’Neil, Alison and Shah, Kevan and Lee, Alicia K. and Goyal, Piyush and Fedus, William and Poplin, Ryan and Esteva, Andre and Berndl, Marc and Rubin, Lee L. and Nelson, Philip and Finkbeiner, Steven},
	month = apr,
	year = {2018},
	keywords = {cancer, computer vision, deep learning, machine learning, microscopy, neuroscience, stem cells},
	pages = {792--803.e19},
	file = {ScienceDirect Full Text PDF:/home/pape/Zotero/storage/VD3N272B/Christiansen et al. - 2018 - In Silico Labeling Predicting Fluorescent Labels .pdf:application/pdf},
}

@article{lee_deephcs_2021,
	title = {{DeepHCS}++: {Bright}-field to fluorescence microscopy image conversion using multi-task learning with adversarial losses for label-free high-content screening},
	volume = {70},
	issn = {1361-8415},
	shorttitle = {{DeepHCS}++},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521000414},
	doi = {10.1016/j.media.2021.101995},
	abstract = {In this paper, we propose a novel microscopy image translation method for transforming a bright-field microscopy image into three different fluorescence images to observe the apoptosis, nuclei, and cytoplasm of cells, which visualize dead cells, nuclei of cells, and cytoplasm of cells, respectively. These biomarkers are commonly used in high-content drug screening to analyze drug response. The main contribution of the proposed work is the automatic generation of three fluorescence images from a conventional bright-field image; this can greatly reduce the time-consuming and laborious tissue preparation process and improve throughput of the screening process. Our proposed method uses only a single bright-field image and the corresponding fluorescence images as a set of image pairs for training an end-to-end deep convolutional neural network. By leveraging deep convolutional neural networks with a set of image pairs of bright-field and corresponding fluorescence images, our proposed method can produce synthetic fluorescence images comparable to real fluorescence microscopy images with high accuracy. Our proposed model uses multi-task learning with adversarial losses to generate more accurate and realistic microscopy images. We assess the efficacy of the proposed method using real bright-field and fluorescence microscopy image datasets from patient-driven samples of a glioblastoma, and validate the method’s accuracy with various quality metrics including cell number correlation (CNC), peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), cell viability correlation (CVC), error maps, and R2 correlation.},
	language = {en},
	urldate = {2022-11-23},
	journal = {Medical Image Analysis},
	author = {Lee, Gyuhyun and Oh, Jeong-Woo and Her, Nam-Gu and Jeong, Won-Ki},
	month = may,
	year = {2021},
	keywords = {Apoptosis, Bright-field microscopy, Cytoplasm, DAPI, Deep learning, Fluorescence microscopy, High-content screening, Precision medicine},
	pages = {101995},
}

@article{ounkomol_label-free_2018,
	title = {Label-free prediction of three-dimensional fluorescence images from transmitted-light microscopy},
	volume = {15},
	copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-018-0111-2},
	doi = {10.1038/s41592-018-0111-2},
	abstract = {Understanding cells as integrated systems is central to modern biology. Although fluorescence microscopy can resolve subcellular structure in living cells, it is expensive, is slow, and can damage cells. We present a label-free method for predicting three-dimensional fluorescence directly from transmitted-light images and demonstrate that it can be used to generate multi-structure, integrated images. The method can also predict immunofluorescence (IF) from electron micrograph (EM) inputs, extending the potential applications.},
	language = {en},
	number = {11},
	urldate = {2022-11-23},
	journal = {Nature Methods},
	author = {Ounkomol, Chawin and Seshamani, Sharmishtaa and Maleckar, Mary M. and Collman, Forrest and Johnson, Gregory R.},
	month = nov,
	year = {2018},
	note = {Number: 11
Publisher: Nature Publishing Group},
	keywords = {Cellular imaging, Computational models, Image processing, Machine learning, Organelles},
	pages = {917--920},
	file = {Full Text PDF:/home/pape/Zotero/storage/XJICKMKM/Ounkomol et al. - 2018 - Label-free prediction of three-dimensional fluores.pdf:application/pdf},
}

@article{rivenson_phasestain_2019,
	title = {{PhaseStain}: the digital staining of label-free quantitative phase microscopy images using deep learning},
	volume = {8},
	copyright = {2019 The Author(s)},
	issn = {2047-7538},
	shorttitle = {{PhaseStain}},
	url = {https://www.nature.com/articles/s41377-019-0129-y},
	doi = {10.1038/s41377-019-0129-y},
	abstract = {Using a deep neural network, we demonstrate a digital staining technique, which we term PhaseStain, to transform the quantitative phase images (QPI) of label-free tissue sections into images that are equivalent to the brightfield microscopy images of the same samples that are histologically stained. Through pairs of image data (QPI and the corresponding brightfield images, acquired after staining), we train a generative adversarial network and demonstrate the effectiveness of this virtual-staining approach using sections of human skin, kidney, and liver tissue, matching the brightfield microscopy images of the same samples stained with Hematoxylin and Eosin, Jones’ stain, and Masson’s trichrome stain, respectively. This digital-staining framework may further strengthen various uses of label-free QPI techniques in pathology applications and biomedical research in general, by eliminating the need for histological staining, reducing sample preparation related costs and saving time. Our results provide a powerful example of some of the unique opportunities created by data-driven image transformations enabled by deep learning.},
	language = {en},
	number = {1},
	urldate = {2022-11-23},
	journal = {Light: Science \& Applications},
	author = {Rivenson, Yair and Liu, Tairan and Wei, Zhensong and Zhang, Yibo and de Haan, Kevin and Ozcan, Aydogan},
	month = feb,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Imaging and sensing, Microscopy},
	pages = {23},
	file = {Full Text PDF:/home/pape/Zotero/storage/W2JWVCPI/Rivenson et al. - 2019 - PhaseStain the digital staining of label-free qua.pdf:application/pdf},
}

@inproceedings{hoyer_daformer_2022,
	title = {{DAFormer}: {Improving} {Network} {Architectures} and {Training} {Strategies} for {Domain}-{Adaptive} {Semantic} {Segmentation}},
	shorttitle = {{DAFormer}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Hoyer_DAFormer_Improving_Network_Architectures_and_Training_Strategies_for_Domain-Adaptive_Semantic_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-11-23},
	author = {Hoyer, Lukas and Dai, Dengxin and Van Gool, Luc},
	year = {2022},
	pages = {9924--9935},
	file = {Full Text PDF:/home/pape/Zotero/storage/IUG3MA2V/Hoyer et al. - 2022 - DAFormer Improving Network Architectures and Trai.pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/7UGFH28J/Hoyer_DAFormer_Improving_Network_Architectures_and_Training_Strategies_for_Domain-Adaptive_Sema.html:text/html},
}

@misc{kohl_probabilistic_2019,
	title = {A {Probabilistic} {U}-{Net} for {Segmentation} of {Ambiguous} {Images}},
	abstract = {Many real-world vision problems suffer from inherent ambiguities. In clinical applications for example, it might not be clear from a CT scan alone which particular region is cancer tissue. Therefore a group of graders typically produces a set of diverse but plausible segmentations. We consider the task of learning a distribution over segmentations given an input. To this end we propose a generative segmentation model based on a combination of a U-Net with a conditional variational autoencoder that is capable of efficiently producing an unlimited number of plausible hypotheses. We show on a lung abnormalities segmentation task and on a Cityscapes segmentation task that our model reproduces the possible segmentation variants as well as the frequencies with which they occur, doing so significantly better than published approaches. These models could have a high impact in real-world applications, such as being used as clinical decision-making algorithms accounting for multiple plausible semantic segmentation hypotheses to provide possible diagnoses and recommend further actions to resolve the present ambiguities.},
	urldate = {2022-11-23},
	publisher = {arXiv},
	author = {Kohl, Simon A. A. and Romera-Paredes, Bernardino and Meyer, Clemens and De Fauw, Jeffrey and Ledsam, Joseph R. and Maier-Hein, Klaus H. and Eslami, S. M. Ali and Rezende, Danilo Jimenez and Ronneberger, Olaf},
	month = jan,
	year = {2019},
	note = {arXiv:1806.05034 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/IGS89YFX/Kohl et al. - 2019 - A Probabilistic U-Net for Segmentation of Ambiguou.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/ZSTLJ8AD/1806.html:text/html},
}

@article{kohl2019hierarchical,
  title={A hierarchical probabilistic u-net for modeling multi-scale ambiguities},
  author={Kohl, Simon AA and Romera-Paredes, Bernardino and Maier-Hein, Klaus H and Rezende, Danilo Jimenez and Eslami, SM and Kohli, Pushmeet and Zisserman, Andrew and Ronneberger, Olaf},
  journal={arXiv preprint arXiv:1905.13077},
  year={2019}
}

@article{kendall2015bayesian,
  title={Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding},
  author={Kendall, Alex and Badrinarayanan, Vijay and Cipolla, Roberto},
  journal={arXiv preprint arXiv:1511.02680},
  year={2015}
}

@inproceedings{sedai2019uncertainty,
  title={Uncertainty guided semi-supervised segmentation of retinal layers in OCT images},
  author={Sedai, Suman and Antony, Bhavna and Rai, Ravneet and Jones, Katie and Ishikawa, Hiroshi and Schuman, Joel and Gadi, Wollstein and Garnavi, Rahil},
  booktitle={Medical Image Computing and Computer Assisted Intervention--MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13--17, 2019, Proceedings, Part I 22},
  pages={282--290},
  year={2019},
  organization={Springer}
}

@misc{rumberger_panoptic_2022,
	title = {Panoptic segmentation with highly imbalanced semantic labels},
	url = {http://arxiv.org/abs/2203.11692},
	doi = {10.48550/arXiv.2203.11692},
	abstract = {We describe here the panoptic segmentation method we devised for our participation in the CoNIC: Colon Nuclei Identification and Counting Challenge at ISBI 2022. Key features of our method are a weighted loss specifically engineered for semantic segmentation of highly imbalanced cell types, and a state-of-the art nuclei instance segmentation model, which we combine in a Hovernet-like architecture.},
	urldate = {2022-11-23},
	publisher = {arXiv},
	author = {Rumberger, Josef Lorenz and Baumann, Elias and Hirsch, Peter and Janowczyk, Andrew and Zlobec, Inti and Kainmueller, Dagmar},
	month = apr,
	year = {2022},
	note = {arXiv:2203.11692 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/2E6RERX9/Rumberger et al. - 2022 - Panoptic segmentation with highly imbalanced seman.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/AEIXF3HN/2203.html:text/html},
}

@article{lessmann_iterative_2019,
	title = {Iterative fully convolutional neural networks for automatic vertebra segmentation and identification},
	volume = {53},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841518305905},
	doi = {10.1016/j.media.2019.02.005},
	abstract = {Precise segmentation and anatomical identification of the vertebrae provides the basis for automatic analysis of the spine, such as detection of vertebral compression fractures or other abnormalities. Most dedicated spine CT and MR scans as well as scans of the chest, abdomen or neck cover only part of the spine. Segmentation and identification should therefore not rely on the visibility of certain vertebrae or a certain number of vertebrae. We propose an iterative instance segmentation approach that uses a fully convolutional neural network to segment and label vertebrae one after the other, independently of the number of visible vertebrae. This instance-by-instance segmentation is enabled by combining the network with a memory component that retains information about already segmented vertebrae. The network iteratively analyzes image patches, using information from both image and memory to search for the next vertebra. To efficiently traverse the image, we include the prior knowledge that the vertebrae are always located next to each other, which is used to follow the vertebral column. The network concurrently performs multiple tasks, which are segmentation of a vertebra, regression of its anatomical label and prediction whether the vertebra is completely visible in the image, which allows to exclude incompletely visible vertebrae from further analyses. The predicted anatomical labels of the individual vertebrae are additionally refined with a maximum likelihood approach, choosing the overall most likely labeling if all detected vertebrae are taken into account. This method was evaluated with five diverse datasets, including multiple modalities (CT and MR), various fields of view and coverages of different sections of the spine, and a particularly challenging set of low-dose chest CT scans. For vertebra segmentation, the average Dice score was 94.9 ± 2.1\% with an average absolute symmetric surface distance of 0.2 ± 10.1mm. The anatomical identification had an accuracy of 93\%, corresponding to a single case with mislabeled vertebrae. Vertebrae were classified as completely or incompletely visible with an accuracy of 97\%. The proposed iterative segmentation method compares favorably with state-of-the-art methods and is fast, flexible and generalizable.},
	language = {en},
	urldate = {2022-11-23},
	journal = {Medical Image Analysis},
	author = {Lessmann, Nikolas and van Ginneken, Bram and de Jong, Pim A. and Išgum, Ivana},
	month = apr,
	year = {2019},
	keywords = {Deep learning, Iterative instance segmentation, Vertebra identification, Vertebra segmentation},
	pages = {142--155},
	file = {Submitted Version:/home/pape/Zotero/storage/MFHSZJE4/Lessmann et al. - 2019 - Iterative fully convolutional neural networks for .pdf:application/pdf},
}

@misc{araslanov_self-supervised_2021,
	title = {Self-supervised {Augmentation} {Consistency} for {Adapting} {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/2105.00097},
	doi = {10.48550/arXiv.2105.00097},
	abstract = {We propose an approach to domain adaptation for semantic segmentation that is both practical and highly accurate. In contrast to previous work, we abandon the use of computationally involved adversarial objectives, network ensembles and style transfer. Instead, we employ standard data augmentation techniques \$-\$ photometric noise, flipping and scaling \$-\$ and ensure consistency of the semantic predictions across these image transformations. We develop this principle in a lightweight self-supervised framework trained on co-evolving pseudo labels without the need for cumbersome extra training rounds. Simple in training from a practitioner's standpoint, our approach is remarkably effective. We achieve significant improvements of the state-of-the-art segmentation accuracy after adaptation, consistent both across different choices of the backbone architecture and adaptation scenarios.},
	urldate = {2022-11-23},
	publisher = {arXiv},
	author = {Araslanov, Nikita and Roth, Stefan},
	month = apr,
	year = {2021},
	note = {arXiv:2105.00097 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/2LNPBLWP/Araslanov and Roth - 2021 - Self-supervised Augmentation Consistency for Adapt.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/ZHB429Y2/2105.html:text/html},
}

@misc{shi_adversarial_2022,
	title = {Adversarial {Masking} for {Self}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2201.13100},
	doi = {10.48550/arXiv.2201.13100},
	abstract = {We propose ADIOS, a masked image model (MIM) framework for self-supervised learning, which simultaneously learns a masking function and an image encoder using an adversarial objective. The image encoder is trained to minimise the distance between representations of the original and that of a masked image. The masking function, conversely, aims at maximising this distance. ADIOS consistently improves on state-of-the-art self-supervised learning (SSL) methods on a variety of tasks and datasets -- including classification on ImageNet100 and STL10, transfer learning on CIFAR10/100, Flowers102 and iNaturalist, as well as robustness evaluated on the backgrounds challenge (Xiao et al., 2021) -- while generating semantically meaningful masks. Unlike modern MIM models such as MAE, BEiT and iBOT, ADIOS does not rely on the image-patch tokenisation construction of Vision Transformers, and can be implemented with convolutional backbones. We further demonstrate that the masks learned by ADIOS are more effective in improving representation learning of SSL methods than masking schemes used in popular MIM models. Code is available at https://github.com/YugeTen/adios.},
	urldate = {2022-11-23},
	publisher = {arXiv},
	author = {Shi, Yuge and Siddharth, N. and Torr, Philip H. S. and Kosiorek, Adam R.},
	month = jul,
	year = {2022},
	note = {arXiv:2201.13100 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/FQN7PC2C/Shi et al. - 2022 - Adversarial Masking for Self-Supervised Learning.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/NFVEP7WX/2201.html:text/html},
}

@inproceedings{khalid_deepcens_2021,
	address = {Shenzhen, China},
	title = {{DeepCeNS}: {An} end-to-end {Pipeline} for {Cell} and {Nucleus} {Segmentation} in {Microscopic} {Images}},
	isbn = {978-1-66543-900-8},
	shorttitle = {{DeepCeNS}},
	url = {https://ieeexplore.ieee.org/document/9533624/},
	doi = {10.1109/IJCNN52387.2021.9533624},
	abstract = {With the evolution of deep learning in the past decade, more biomedical related problems that seemed strenuous, are now feasible. The introduction of U-net and Mask RCNN architectures has paved a way for many object detection and segmentation tasks in numerous applications ranging from security to biomedical applications. In the cell biology domain, light microscopy imaging provides a cheap and accessible source of raw data to study biological phenomena. By leveraging such data and deep learning techniques, human diseases can be easily diagnosed and the process of treatment development can be greatly expedited. In microscopic imaging, accurate segmentation of individual cells is a crucial step to allow better insight into cellular heterogeneity. To address the aforementioned challenges, DeepCeNS is proposed in this paper to detect and segment cells and nucleus in microscopic images. We have used EVICAN2 dataset which contains microscopic images from a variety of microscopes having numerous cell cultures, to evaluate the proposed pipeline. DeepCeNS outperforms EVICAN-MRCNN by a signiﬁcant margin on the EVICAN2 dataset.},
	language = {en},
	urldate = {2022-11-23},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Khalid, Nabeel and Munir, Mohsin and Edlund, Christoffer and Jackson, Timothy R and Trygg, Johan and Sjogren, Rickard and Dengel, Andreas and Ahmed, Sheraz},
	month = jul,
	year = {2021},
	keywords = {livecell},
	pages = {1--8},
	file = {Khalid et al. - 2021 - DeepCeNS An end-to-end Pipeline for Cell and Nucl.pdf:/home/pape/Zotero/storage/KQ3HDIC6/Khalid et al. - 2021 - DeepCeNS An end-to-end Pipeline for Cell and Nucl.pdf:application/pdf},
}

@misc{cheng_boxteacher_2022,
	title = {{BoxTeacher}: {Exploring} {High}-{Quality} {Pseudo} {Labels} for {Weakly} {Supervised} {Instance} {Segmentation}},
	shorttitle = {{BoxTeacher}},
	url = {http://arxiv.org/abs/2210.05174},
	doi = {10.48550/arXiv.2210.05174},
	abstract = {Labeling objects with pixel-wise segmentation requires a huge amount of human labor compared to bounding boxes. Most existing methods for weakly supervised instance segmentation focus on designing heuristic losses with priors from bounding boxes. While, we find that box-supervised methods can produce some fine segmentation masks and we wonder whether the detectors could learn from these fine masks while ignoring low-quality masks. To answer this question, we present BoxTeacher, an efficient and end-to-end training framework for high-performance weakly supervised instance segmentation, which leverages a sophisticated teacher to generate high-quality masks as pseudo labels. Considering the massive noisy masks hurt the training, we present a mask-aware confidence score to estimate the quality of pseudo masks, and propose the noise-aware pixel loss and noise-reduced affinity loss to adaptively optimize the student with pseudo masks. Extensive experiments can demonstrate effectiveness of the proposed BoxTeacher. Without bells and whistles, BoxTeacher remarkably achieves \$34.4\$ mask AP and \$35.4\$ mask AP with ResNet-50 and ResNet-101 respectively on the challenging MS-COCO dataset, which outperforms the previous state-of-the-art methods by a significant margin. The code and models are available at https://github.com/hustvl/BoxTeacher.},
	urldate = {2022-11-23},
	publisher = {arXiv},
	author = {Cheng, Tianheng and Wang, Xinggang and Chen, Shaoyu and Zhang, Qian and Liu, Wenyu},
	month = oct,
	year = {2022},
	note = {arXiv:2210.05174 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/CTREP636/Cheng et al. - 2022 - BoxTeacher Exploring High-Quality Pseudo Labels f.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/MDER26WU/2210.html:text/html},
}

@misc{kohl_hierarchical_2019,
	title = {A {Hierarchical} {Probabilistic} {U}-{Net} for {Modeling} {Multi}-{Scale} {Ambiguities}},
	url = {http://arxiv.org/abs/1905.13077},
	doi = {10.48550/arXiv.1905.13077},
	abstract = {Medical imaging only indirectly measures the molecular identity of the tissue within each voxel, which often produces only ambiguous image evidence for target measures of interest, like semantic segmentation. This diversity and the variations of plausible interpretations are often specific to given image regions and may thus manifest on various scales, spanning all the way from the pixel to the image level. In order to learn a flexible distribution that can account for multiple scales of variations, we propose the Hierarchical Probabilistic U-Net, a segmentation network with a conditional variational auto-encoder (cVAE) that uses a hierarchical latent space decomposition. We show that this model formulation enables sampling and reconstruction of segmenations with high fidelity, i.e. with finely resolved detail, while providing the flexibility to learn complex structured distributions across scales. We demonstrate these abilities on the task of segmenting ambiguous medical scans as well as on instance segmentation of neurobiological and natural images. Our model automatically separates independent factors across scales, an inductive bias that we deem beneficial in structured output prediction tasks beyond segmentation.},
	urldate = {2022-11-23},
	publisher = {arXiv},
	author = {Kohl, Simon A. A. and Romera-Paredes, Bernardino and Maier-Hein, Klaus H. and Rezende, Danilo Jimenez and Eslami, S. M. Ali and Kohli, Pushmeet and Zisserman, Andrew and Ronneberger, Olaf},
	month = may,
	year = {2019},
	note = {arXiv:1905.13077 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/NYCSR6FI/Kohl et al. - 2019 - A Hierarchical Probabilistic U-Net for Modeling Mu.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/XJGRGZ4S/1905.html:text/html},
}

@article{lequyer_fast_2022,
	title = {A fast blind zero-shot denoiser},
	volume = {4},
	copyright = {2022 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-022-00547-8},
	doi = {10.1038/s42256-022-00547-8},
	abstract = {Image noise is a common problem in light microscopy. This is particularly true in real-time live-cell imaging applications in which long-term cell viability necessitates low-light conditions. Modern denoisers are typically trained on a representative dataset, sometimes consisting of just unpaired noisy shots. However, when data are acquired in real time to track dynamic cellular processes, it is not always practical nor economical to generate these training sets. Recently, denoisers have emerged that allow us to denoise single images without a training set or knowledge about the underlying noise. But such methods are currently too slow to be integrated into imaging pipelines that require rapid, real-time hardware feedback. Here we present Noise2Fast, which can overcome these limitations. Noise2Fast uses a novel downsampling technique we refer to as ‘chequerboard downsampling’. This allows us to train on a discrete 4-image training set, while convergence can be monitored using the original noisy image. We show that Noise2Fast is faster than all similar methods with only a small drop in accuracy compared to the gold standard. We integrate Noise2Fast into real-time multi-modal imaging applications and demonstrate its broad applicability to diverse imaging and analysis pipelines.},
	language = {en},
	number = {11},
	urldate = {2022-11-23},
	journal = {Nature Machine Intelligence},
	author = {Lequyer, Jason and Philip, Reuben and Sharma, Amit and Hsu, Wen-Hsin and Pelletier, Laurence},
	month = nov,
	year = {2022},
	note = {Number: 11
Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning, Super-resolution microscopy},
	pages = {953--963},
	file = {Full Text PDF:/home/pape/Zotero/storage/VDCMCPPT/Lequyer et al. - 2022 - A fast blind zero-shot denoiser.pdf:application/pdf},
}

@article{robitaille_self-supervised_2022,
	title = {Self-supervised machine learning for live cell imagery segmentation},
	volume = {5},
	copyright = {2022 This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply},
	issn = {2399-3642},
	url = {https://www.nature.com/articles/s42003-022-04117-x},
	doi = {10.1038/s42003-022-04117-x},
	abstract = {Segmenting single cells is a necessary process for extracting quantitative data from biological microscopy imagery. The past decade has seen the advent of machine learning (ML) methods to aid in this process, the overwhelming majority of which fall under supervised learning (SL) which requires vast libraries of pre-processed, human-annotated labels to train the ML algorithms. Such SL pre-processing is labor intensive, can introduce bias, varies between end-users, and has yet to be shown capable of robust models to be effectively utilized throughout the greater cell biology community. Here, to address this pre-processing problem, we offer a self-supervised learning (SSL) approach that utilizes cellular motion between consecutive images to self-train a ML classifier, enabling cell and background segmentation without the need for adjustable parameters or curated imagery. By leveraging motion, we achieve accurate segmentation that trains itself directly on end-user data, is independent of optical modality, outperforms contemporary SL methods, and does so in a completely automated fashion—thus eliminating end-user variability and bias. To the best of our knowledge, this SSL algorithm represents a first of its kind effort and has appealing features that make it an ideal segmentation tool candidate for the broader cell biology research community.},
	language = {en},
	number = {1},
	urldate = {2022-11-23},
	journal = {Communications Biology},
	author = {Robitaille, Michael C. and Byers, Jeff M. and Christodoulides, Joseph A. and Raphael, Marc P.},
	month = nov,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Cellular imaging, Machine learning, livecell, Optical imaging},
	pages = {1--8},
	file = {Full Text PDF:/home/pape/Zotero/storage/DKCBM8QR/Robitaille et al. - 2022 - Self-supervised machine learning for live cell ima.pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/Z5JXHX5L/s42003-022-04117-x.html:text/html},
}

@misc{yang_lite_2021,
	title = {Lite {Vision} {Transformer} with {Enhanced} {Self}-{Attention}},
	url = {http://arxiv.org/abs/2112.10809},
	doi = {10.48550/arXiv.2112.10809},
	abstract = {Despite the impressive representation capacity of vision transformer models, current light-weight vision transformer models still suffer from inconsistent and incorrect dense predictions at local regions. We suspect that the power of their self-attention mechanism is limited in shallower and thinner networks. We propose Lite Vision Transformer (LVT), a novel light-weight transformer network with two enhanced self-attention mechanisms to improve the model performances for mobile deployment. For the low-level features, we introduce Convolutional Self-Attention (CSA). Unlike previous approaches of merging convolution and self-attention, CSA introduces local self-attention into the convolution within a kernel of size 3x3 to enrich low-level features in the first stage of LVT. For the high-level features, we propose Recursive Atrous Self-Attention (RASA), which utilizes the multi-scale context when calculating the similarity map and a recursive mechanism to increase the representation capability with marginal extra parameter cost. The superiority of LVT is demonstrated on ImageNet recognition, ADE20K semantic segmentation, and COCO panoptic segmentation. The code is made publicly available.},
	urldate = {2022-11-23},
	publisher = {arXiv},
	author = {Yang, Chenglin and Wang, Yilin and Zhang, Jianming and Zhang, He and Wei, Zijun and Lin, Zhe and Yuille, Alan},
	month = dec,
	year = {2021},
	note = {arXiv:2112.10809 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/6J7KC6DB/Yang et al. - 2021 - Lite Vision Transformer with Enhanced Self-Attenti.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/MB56CHK6/2112.html:text/html},
}

@article{maddalena_artificial_2022,
	title = {Artificial {Intelligence} for {Cell} {Segmentation}, {Event} {Detection}, and {Tracking} for {Label}-{Free} {Microscopy} {Imaging}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1999-4893},
	url = {https://www.mdpi.com/1999-4893/15/9/313},
	doi = {10.3390/a15090313},
	abstract = {Background: Time-lapse microscopy imaging is a key approach for an increasing number of biological and biomedical studies to observe the dynamic behavior of cells over time which helps quantify important data, such as the number of cells and their sizes, shapes, and dynamic interactions across time. Label-free imaging is an essential strategy for such studies as it ensures that native cell behavior remains uninfluenced by the recording process. Computer vision and machine/deep learning approaches have made significant progress in this area. Methods: In this review, we present an overview of methods, software, data, and evaluation metrics for the automatic analysis of label-free microscopy imaging. We aim to provide the interested reader with a unique source of information, with links for further detailed information. Results: We review the most recent methods for cell segmentation, event detection, and tracking. Moreover, we provide lists of publicly available software and datasets. Finally, we summarize the metrics most frequently adopted for evaluating the methods under exam. Conclusions: We provide hints on open challenges and future research directions.},
	language = {en},
	number = {9},
	urldate = {2022-11-23},
	journal = {Algorithms},
	author = {Maddalena, Lucia and Antonelli, Laura and Albu, Alexandra and Hada, Aroj and Guarracino, Mario Rosario},
	month = sep,
	year = {2022},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, machine learning, livecell, artificial intelligence, cell classification, cell event detection, cell segmentation, cell tracking, label-free microscopy},
	pages = {313},
	file = {Full Text PDF:/home/pape/Zotero/storage/H8WBWXI9/Maddalena et al. - 2022 - Artificial Intelligence for Cell Segmentation, Eve.pdf:application/pdf},
}

@article{mukherjee_domain_2022,
	title = {Domain {Adapted} {Multi}-task {Learning} {For} {Segmenting} {Amoeboid} {Cells} in {Microscopy}},
	volume = {PP},
	issn = {1558-254X},
	doi = {10.1109/TMI.2022.3203022},
	abstract = {The method proposed in this paper is a robust combination of multi-task learning and unsupervised domain adaptation for segmenting amoeboid cells in microscopy. A highlight of this work is the manner in which the model's hyperparameters are estimated. The detriments of ad-hoc parameter estimation are well known, but this issue remains largely unaddressed in the context of CNN-based segmentation. Using a novel min-max formulation of the segmentation cost function our proposed method analytically estimates the model's hyperparameters, while simultaneously learning the CNN weights during training. This end-to-end framework provides a consolidated mechanism to harness the potential of multi-task learning to isolate and segment clustered cells from low contrast brightfield images, and it simultaneously leverages deep domain adaptation to segment fluorescent cells without explicit pixel-level re-annotation of the data. Experimental validations on multi-cellular images strongly suggest the effectiveness of the proposed technique, and our quantitative results show at least 15\% and 10\% improvement in cell segmentation on brightfield and fluorescence images respectively compared to contemporary supervised segmentation methods.},
	language = {eng},
	journal = {IEEE transactions on medical imaging},
	author = {Mukherjee, Suvadip and Sarkar, Rituparna and Manich, Maria and Labruyere, Elisabeth and Olivo-Marin, Jean-Christophe},
	month = aug,
	year = {2022},
	pmid = {36044485},
}

@misc{ngoc_buythedips_2022,
	title = {{BuyTheDips}: {PathLoss} for improved topology-preserving deep learning-based image segmentation},
	shorttitle = {{BuyTheDips}},
	url = {http://arxiv.org/abs/2207.11446},
	doi = {10.48550/arXiv.2207.11446},
	abstract = {Capturing the global topology of an image is essential for proposing an accurate segmentation of its domain. However, most of existing segmentation methods do not preserve the initial topology of the given input, which is detrimental for numerous downstream object-based tasks. This is all the more true for deep learning models which most work at local scales. In this paper, we propose a new topology-preserving deep image segmentation method which relies on a new leakage loss: the Pathloss. Our method is an extension of the BALoss [1], in which we want to improve the leakage detection for better recovering the closeness property of the image segmentation. This loss allows us to correctly localize and fix the critical points (a leakage in the boundaries) that could occur in the predictions, and is based on a shortest-path search algorithm. This way, loss minimization enforces connectivity only where it is necessary and finally provides a good localization of the boundaries of the objects in the image. Moreover, according to our research, our Pathloss learns to preserve stronger elongated structure compared to methods without using topology-preserving loss. Training with our topological loss function, our method outperforms state-of-the-art topology-aware methods on two representative datasets of different natures: Electron Microscopy and Historical Map.},
	urldate = {2022-11-23},
	publisher = {arXiv},
	author = {Ngoc, Minh On Vu and Chen, Yizi and Boutry, Nicolas and Fabrizio, Jonathan and Mallet, Clement},
	month = aug,
	year = {2022},
	note = {arXiv:2207.11446 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/pape/Zotero/storage/CIXZCKL6/2207.html:text/html},
}

@article{zhou_machine_2023,
	title = {A machine learning pipeline for membrane segmentation of cryo-electron tomograms},
	volume = {66},
	issn = {1877-7503},
	url = {https://www.sciencedirect.com/science/article/pii/S1877750322002630},
	doi = {10.1016/j.jocs.2022.101904},
	abstract = {We describe how to use several machine learning techniques organized in a learning pipeline to segment and identify cell membrane structures from cryo electron tomograms. These tomograms are difficult to analyze with traditional segmentation tools. The learning pipeline in our approach starts from supervised learning via a special convolutional neural network trained with simulated data. It continues with semi-supervised reinforcement learning and/or a region merging technique that tries to piece together disconnected components belonging to the same membrane structure. A parametric or non-parametric fitting procedure is then used to enhance the segmentation results and quantify uncertainties in the fitting. Domain knowledge is used in generating the training data for the neural network and in guiding the fitting procedure through the use of appropriately chosen priors and constraints. We demonstrate that the approach proposed here works well for extracting membrane surfaces in two real tomogram datasets.},
	language = {en},
	urldate = {2022-12-01},
	journal = {Journal of Computational Science},
	author = {Zhou, Li and Yang, Chao and Gao, Weiguo and Perciano, Talita and Davies, Karen M. and Sauter, Nicholas K.},
	month = jan,
	year = {2023},
	keywords = {Machine learning, Gaussian process, Image segmentation, Membrane structure, Reinforcement learning, Uncertainty quantification},
	pages = {101904},
	file = {ScienceDirect Full Text PDF:/home/pape/Zotero/storage/MRAIZY2J/Zhou et al. - 2023 - A machine learning pipeline for membrane segmentat.pdf:application/pdf;ScienceDirect Snapshot:/home/pape/Zotero/storage/U5P8HSJU/S1877750322002630.html:text/html},
}

@article{zhou_one-shot_2021,
	title = {One-{Shot} {Learning} {With} {Attention}-{Guided} {Segmentation} in {Cryo}-{Electron} {Tomography}},
	volume = {7},
	issn = {2296-889X},
	url = {https://www.frontiersin.org/articles/10.3389/fmolb.2020.613347},
	abstract = {Cryo-electron Tomography (cryo-ET) generates 3D visualization of cellular organization that allows biologists to analyze cellular structures in a near-native state with nano resolution. Recently, deep learning methods have demonstrated promising performance in classification and segmentation of macromolecule structures captured by cryo-ET, but training individual deep learning models requires large amounts of manually labeled and segmented data from previously observed classes. To perform classification and segmentation in the wild (i.e., with limited training data and with unseen classes), novel deep learning model needs to be developed to classify and segment unseen macromolecules captured by cryo-ET. In this paper, we develop a one-shot learning framework, called cryo-ET one-shot network (COS-Net), for simultaneous classification of macromolecular structure and generation of the voxel-level 3D segmentation, using only one training sample per class. Our experimental results on 22 macromolecule classes demonstrated that our COS-Net could efficiently classify macromolecular structures with small amounts of samples and produce accurate 3D segmentation at the same time.},
	urldate = {2022-12-01},
	journal = {Frontiers in Molecular Biosciences},
	author = {Zhou, Bo and Yu, Haisu and Zeng, Xiangrui and Yang, Xiaoyan and Zhang, Jing and Xu, Min},
	year = {2021},
	file = {Full Text PDF:/home/pape/Zotero/storage/HI7S2HW4/Zhou et al. - 2021 - One-Shot Learning With Attention-Guided Segmentati.pdf:application/pdf},
}

@misc{noauthor_biology_nodate,
	title = {Biology},
	url = {https://www.jove.com/t/64435/deep-learning-based-segmentation-of-cryo-electron-tomograms},
	abstract = {Jove},
	language = {en},
	urldate = {2022-12-01},
	file = {Snapshot:/home/pape/Zotero/storage/2XLITFYK/deep-learning-based-segmentation-of-cryo-electron-tomograms.html:text/html},
}

@article{chen_convolutional_2017,
	title = {Convolutional neural networks for automated annotation of cellular cryo-electron tomograms},
	volume = {14},
	copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.4405},
	doi = {10.1038/nmeth.4405},
	abstract = {An algorithm and software tool automating the annotation of subcellular features in cryo-electron tomography data is presented.},
	language = {en},
	number = {10},
	urldate = {2022-12-01},
	journal = {Nature Methods},
	author = {Chen, Muyuan and Dai, Wei and Sun, Stella Y. and Jonasch, Darius and He, Cynthia Y. and Schmid, Michael F. and Chiu, Wah and Ludtke, Steven J.},
	month = oct,
	year = {2017},
	note = {Number: 10
Publisher: Nature Publishing Group},
	keywords = {Cellular imaging, Image processing, Cryoelectron microscopy, Software},
	pages = {983--985},
	file = {Full Text PDF:/home/pape/Zotero/storage/VTZG7HHD/Chen et al. - 2017 - Convolutional neural networks for automated annota.pdf:application/pdf},
}

@article{lamm_membrain_2022,
	title = {{MemBrain}: {A} deep learning-aided pipeline for detection of membrane proteins in {Cryo}-electron tomograms},
	volume = {224},
	issn = {0169-2607},
	shorttitle = {{MemBrain}},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260722003728},
	doi = {10.1016/j.cmpb.2022.106990},
	abstract = {Background and Objective
Cryo-electron tomography (cryo-ET) is an imaging technique that enables 3D visualization of the native cellular environment at sub-nanometer resolution, providing unpreceded insights into the molecular organization of cells. However, cryo-electron tomograms suffer from low signal-to-noise ratios and anisotropic resolution, which makes subsequent image analysis challenging. In particular, the efficient detection of membrane-embedded proteins is a problem still lacking satisfactory solutions.
Methods
We present MemBrain – a new deep learning-aided pipeline that automatically detects membrane-bound protein complexes in cryo-electron tomograms. After subvolumes are sampled along a segmented membrane, each subvolume is assigned a score using a convolutional neural network (CNN), and protein positions are extracted by a clustering algorithm. Incorporating rotational subvolume normalization and using a tiny receptive field simplify the task of protein detection and thus facilitate the network training.
Results
MemBrain requires only a small quantity of training labels and achieves excellent performance with only a single annotated membrane (F1 score: 0.88). A detailed evaluation shows that our fully trained pipeline outperforms existing classical computer vision-based and CNN-based approaches by a large margin (F1 score: 0.92 vs. max. 0.63). Furthermore, in addition to protein center positions, MemBrain can determine protein orientations, which has not been implemented by any existing CNN-based method to date. We also show that a pre-trained MemBrain program generalizes to tomograms acquired using different cryo-ET methods and depicting different types of cells.
Conclusions
MemBrain is a powerful and annotation-efficient tool for the detection of membrane protein complexes in cryo-ET data, with the potential to be used in a wide range of biological studies. It is generalizable to various kinds of tomograms, making it possible to use pretrained models for different tasks. Its efficiency in terms of required annotations also allows rapid training and fine-tuning of models. The corresponding code, pretrained models, and instructions for operating the MemBrain program can be found at: https://github.com/CellArchLab/MemBrain.},
	language = {en},
	urldate = {2022-12-01},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Lamm, Lorenz and Righetto, Ricardo D. and Wietrzynski, Wojciech and Pöge, Matthias and Martinez-Sanchez, Antonio and Peng, Tingying and Engel, Benjamin D.},
	month = sep,
	year = {2022},
	keywords = {deep learning, annotation-efficient, Cryo-electron tomography, membrane protein, particle picking, protein localization},
	pages = {106990},
	file = {ScienceDirect Full Text PDF:/home/pape/Zotero/storage/3SWVCFP7/Lamm et al. - 2022 - MemBrain A deep learning-aided pipeline for detec.pdf:application/pdf},
}

@article{moebel_deep_2021,
	title = {Deep learning improves macromolecule identification in {3D} cellular cryo-electron tomograms},
	volume = {18},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-021-01275-4},
	doi = {10.1038/s41592-021-01275-4},
	abstract = {Cryogenic electron tomography (cryo-ET) visualizes the 3D spatial distribution of macromolecules at nanometer resolution inside native cells. However, automated identification of macromolecules inside cellular tomograms is challenged by noise and reconstruction artifacts, as well as the presence of many molecular species in the crowded volumes. Here, we present DeepFinder, a computational procedure that uses artificial neural networks to simultaneously localize multiple classes of macromolecules. Once trained, the inference stage of DeepFinder is faster than template matching and performs better than other competitive deep learning methods at identifying macromolecules of various sizes in both synthetic and experimental datasets. On cellular cryo-ET data, DeepFinder localized membrane-bound and cytosolic ribosomes (roughly 3.2 MDa), ribulose 1,5-bisphosphate carboxylase–oxygenase (roughly 560 kDa soluble complex) and photosystem II (roughly 550 kDa membrane complex) with an accuracy comparable to expert-supervised ground truth annotations. DeepFinder is therefore a promising algorithm for the semiautomated analysis of a wide range of molecular targets in cellular tomograms.},
	language = {en},
	number = {11},
	urldate = {2022-12-01},
	journal = {Nature Methods},
	author = {Moebel, Emmanuel and Martinez-Sanchez, Antonio and Lamm, Lorenz and Righetto, Ricardo D. and Wietrzynski, Wojciech and Albert, Sahradha and Larivière, Damien and Fourmentin, Eric and Pfeffer, Stefan and Ortiz, Julio and Baumeister, Wolfgang and Peng, Tingying and Engel, Benjamin D. and Kervrann, Charles},
	month = nov,
	year = {2021},
	note = {Number: 11
Publisher: Nature Publishing Group},
	keywords = {Image processing, Cryoelectron microscopy},
	pages = {1386--1394},
	file = {Full Text PDF:/home/pape/Zotero/storage/LTZ52EQA/Moebel et al. - 2021 - Deep learning improves macromolecule identificatio.pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/IHE7NUIT/s41592-021-01275-4.html:text/html},
}

@article{zhong_cryodrgn_2021,
	title = {{CryoDRGN}: reconstruction of heterogeneous cryo-{EM} structures using neural networks},
	volume = {18},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {{CryoDRGN}},
	url = {https://www.nature.com/articles/s41592-020-01049-4},
	doi = {10.1038/s41592-020-01049-4},
	abstract = {Cryo-electron microscopy (cryo-EM) single-particle analysis has proven powerful in determining the structures of rigid macromolecules. However, many imaged protein complexes exhibit conformational and compositional heterogeneity that poses a major challenge to existing three-dimensional reconstruction methods. Here, we present cryoDRGN, an algorithm that leverages the representation power of deep neural networks to directly reconstruct continuous distributions of 3D density maps and map per-particle heterogeneity of single-particle cryo-EM datasets. Using cryoDRGN, we uncovered residual heterogeneity in high-resolution datasets of the 80S ribosome and the RAG complex, revealed a new structural state of the assembling 50S ribosome, and visualized large-scale continuous motions of a spliceosome complex. CryoDRGN contains interactive tools to visualize a dataset’s distribution of per-particle variability, generate density maps for exploratory analysis, extract particle subsets for use with other tools and generate trajectories to visualize molecular motions. CryoDRGN is open-source software freely available at http://cryodrgn.csail.mit.edu.},
	language = {en},
	number = {2},
	urldate = {2022-12-01},
	journal = {Nature Methods},
	author = {Zhong, Ellen D. and Bepler, Tristan and Berger, Bonnie and Davis, Joseph H.},
	month = feb,
	year = {2021},
	note = {Number: 2
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Cryoelectron microscopy},
	pages = {176--185},
	file = {Full Text PDF:/home/pape/Zotero/storage/H59CAUHM/Zhong et al. - 2021 - CryoDRGN reconstruction of heterogeneous cryo-EM .pdf:application/pdf},
}

@article{de_maesschalck_mahalanobis_2000,
	title = {The {Mahalanobis} distance},
	volume = {50},
	issn = {0169-7439},
	url = {https://www.sciencedirect.com/science/article/pii/S0169743999000477},
	doi = {10.1016/S0169-7439(99)00047-7},
	abstract = {The theory of many multivariate chemometrical methods is based on the measurement of distances. The Mahalanobis distance (MD), in the original and principal component (PC) space, will be examined and interpreted in relation with the Euclidean distance (ED). Techniques based on the MD and applied in different fields of chemometrics such as in multivariate calibration, pattern recognition and process control are explained and discussed.},
	language = {en},
	number = {1},
	urldate = {2022-12-01},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	author = {De Maesschalck, R. and Jouan-Rimbaud, D. and Massart, D. L.},
	month = jan,
	year = {2000},
	keywords = {Euclidean distance, Mahalanobis distance, Principal components},
	pages = {1--18},
	file = {ScienceDirect Full Text PDF:/home/pape/Zotero/storage/JI3SHBDU/De Maesschalck et al. - 2000 - The Mahalanobis distance.pdf:application/pdf},
}

@article{helm_large-scale_2021,
	title = {A large-scale nanoscopy and biochemistry analysis of postsynaptic dendritic spines},
	volume = {24},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-021-00874-w},
	doi = {10.1038/s41593-021-00874-w},
	abstract = {Dendritic spines, the postsynaptic compartments of excitatory neurotransmission, have different shapes classified from ‘stubby’ to ‘mushroom-like’. Whereas mushroom spines are essential for adult brain function, stubby spines disappear during brain maturation. It is still unclear whether and how they differ in protein composition. To address this, we combined electron microscopy and quantitative biochemistry with super-resolution microscopy to annotate more than 47,000 spines for more than 100 synaptic targets. Surprisingly, mushroom and stubby spines have similar average protein copy numbers and topologies. However, an analysis of the correlation of each protein to the postsynaptic density mass, used as a marker of synaptic strength, showed substantially more significant results for the mushroom spines. Secretion and trafficking proteins correlated particularly poorly to the strength of stubby spines. This suggests that stubby spines are less likely to adequately respond to dynamic changes in synaptic transmission than mushroom spines, which possibly explains their loss during brain maturation.},
	language = {en},
	number = {8},
	urldate = {2022-12-01},
	journal = {Nature Neuroscience},
	author = {Helm, Martin S. and Dankovich, Tal M. and Mandad, Sunit and Rammner, Burkhard and Jähne, Sebastian and Salimi, Vanessa and Koerbs, Christina and Leibrandt, Richard and Urlaub, Henning and Schikorski, Thomas and Rizzoli, Silvio O.},
	month = aug,
	year = {2021},
	note = {Number: 8
Publisher: Nature Publishing Group},
	keywords = {Cellular neuroscience, Neurodegeneration, Synaptic transmission},
	pages = {1151--1162},
	file = {Full Text PDF:/home/pape/Zotero/storage/QQEGWXXS/Helm et al. - 2021 - A large-scale nanoscopy and biochemistry analysis .pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/FUQ5JV4P/s41593-021-00874-w.html:text/html},
}

@article{wilhelm_composition_2014,
	title = {Composition of isolated synaptic boutons reveals the amounts of vesicle trafficking proteins},
	volume = {344},
	url = {https://www.science.org/doi/10.1126/science.1252884},
	doi = {10.1126/science.1252884},
	abstract = {Synaptic vesicle recycling has long served as a model for the general mechanisms of cellular trafficking. We used an integrative approach, combining quantitative immunoblotting and mass spectrometry to determine protein numbers; electron microscopy to measure organelle numbers, sizes, and positions; and super-resolution fluorescence microscopy to localize the proteins. Using these data, we generated a three-dimensional model of an “average” synapse, displaying 300,000 proteins in atomic detail. The copy numbers of proteins involved in the same step of synaptic vesicle recycling correlated closely. In contrast, copy numbers varied over more than three orders of magnitude between steps, from about 150 copies for the endosomal fusion proteins to more than 20,000 for the exocytotic ones.},
	number = {6187},
	urldate = {2022-12-01},
	journal = {Science},
	author = {Wilhelm, Benjamin G. and Mandad, Sunit and Truckenbrodt, Sven and Kröhnert, Katharina and Schäfer, Christina and Rammner, Burkhard and Koo, Seong Joo and Claßen, Gala A. and Krauss, Michael and Haucke, Volker and Urlaub, Henning and Rizzoli, Silvio O.},
	month = may,
	year = {2014},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1023--1028},
	file = {Full Text PDF:/home/pape/Zotero/storage/4PZHXNS7/Wilhelm et al. - 2014 - Composition of isolated synaptic boutons reveals t.pdf:application/pdf},
}

@misc{chen_deeplab_2017,
	title = {{DeepLab}: {Semantic} {Image} {Segmentation} with {Deep} {Convolutional} {Nets}, {Atrous} {Convolution}, and {Fully} {Connected} {CRFs}},
	shorttitle = {{DeepLab}},
	url = {http://arxiv.org/abs/1606.00915},
	doi = {10.48550/arXiv.1606.00915},
	abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7\% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	urldate = {2022-12-04},
	publisher = {arXiv},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = may,
	year = {2017},
	note = {arXiv:1606.00915 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/JA4ZKKI8/Chen et al. - 2017 - DeepLab Semantic Image Segmentation with Deep Con.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/SVERPMJM/1606.html:text/html},
}

@misc{chen_encoder-decoder_2018,
	title = {Encoder-{Decoder} with {Atrous} {Separable} {Convolution} for {Semantic} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1802.02611},
	doi = {10.48550/arXiv.1802.02611},
	abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0{\textbackslash}\% and 82.1{\textbackslash}\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at {\textbackslash}url\{https://github.com/tensorflow/models/tree/master/research/deeplab\}.},
	urldate = {2022-12-04},
	publisher = {arXiv},
	author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	month = aug,
	year = {2018},
	note = {arXiv:1802.02611 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/U763DYDT/Chen et al. - 2018 - Encoder-Decoder with Atrous Separable Convolution .pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/GCAF4Y48/1802.html:text/html},
}

@misc{chen_rethinking_2017,
	title = {Rethinking {Atrous} {Convolution} for {Semantic} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1706.05587},
	doi = {10.48550/arXiv.1706.05587},
	abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
	urldate = {2022-12-04},
	publisher = {arXiv},
	author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	month = dec,
	year = {2017},
	note = {arXiv:1706.05587 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/6JW5KZYK/Chen et al. - 2017 - Rethinking Atrous Convolution for Semantic Image S.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/KE7RTGA9/1706.html:text/html},
}

@article{imig_ultrastructural_2020,
	title = {Ultrastructural {Imaging} of {Activity}-{Dependent} {Synaptic} {Membrane}-{Trafficking} {Events} in {Cultured} {Brain} {Slices}},
	volume = {108},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627320307042},
	doi = {10.1016/j.neuron.2020.09.004},
	abstract = {Electron microscopy can resolve synapse ultrastructure with nanometer precision, but the capture of time-resolved, activity-dependent synaptic membrane-trafficking events has remained challenging, particularly in functionally distinct synapses in a tissue context. We present a method that combines optogenetic stimulation-coupled cryofixation (“flash-and-freeze”) and electron microscopy to visualize membrane trafficking events and synapse-state-specific changes in presynaptic vesicle organization with high spatiotemporal resolution in synapses of cultured mouse brain tissue. With our experimental workflow, electrophysiological and “flash-and-freeze” electron microscopy experiments can be performed under identical conditions in artificial cerebrospinal fluid alone, without the addition of external cryoprotectants, which are otherwise needed to allow adequate tissue preservation upon freezing. Using this approach, we reveal depletion of docked vesicles and resolve compensatory membrane recycling events at individual presynaptic active zones at hippocampal mossy fiber synapses upon sustained stimulation.},
	language = {en},
	number = {5},
	urldate = {2022-12-04},
	journal = {Neuron},
	author = {Imig, Cordelia and López-Murcia, Francisco José and Maus, Lydia and García-Plaza, Inés Hojas and Mortensen, Lena Sünke and Schwark, Manuela and Schwarze, Valentin and Angibaud, Julie and Nägerl, U. Valentin and Taschenberger, Holger and Brose, Nils and Cooper, Benjamin H.},
	month = dec,
	year = {2020},
	keywords = {active zone, Electron microscopy, Electron tomography, endocytosis, exocytosis, Flash-and-freeze, High-pressure freezing, optogenetics, synapse, synaptic vesicle},
	pages = {843--860.e8},
	file = {ScienceDirect Full Text PDF:/home/pape/Zotero/storage/XFQ5QXSP/Imig et al. - 2020 - Ultrastructural Imaging of Activity-Dependent Syna.pdf:application/pdf;ScienceDirect Snapshot:/home/pape/Zotero/storage/4WGLWRS9/S0896627320307042.html:text/html},
}

@article{tawfik_synaptotagmin-7_2021,
	title = {Synaptotagmin-7 places dense-core vesicles at the cell membrane to promote {Munc13}-2- and {Ca2}+-dependent priming},
	volume = {10},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.64527},
	doi = {10.7554/eLife.64527},
	abstract = {Synaptotagmins confer calcium-dependence to the exocytosis of secretory vesicles, but how coexpressed synaptotagmins interact remains unclear. We find that synaptotagmin-1 and synaptotagmin-7 when present alone act as standalone fast and slow Ca2+-sensors for vesicle fusion in mouse chromaffin cells. When present together, synaptotagmin-1 and synaptotagmin-7 are found in largely non-overlapping clusters on dense-core vesicles. Synaptotagmin-7 stimulates Ca2+-dependent vesicle priming and inhibits depriming, and it promotes ubMunc13-2- and phorbolester-dependent priming, especially at low resting calcium concentrations. The priming effect of synaptotagmin-7 increases the number of vesicles fusing via synaptotagmin-1, while negatively affecting their fusion speed, indicating both synergistic and competitive interactions between synaptotagmins. Synaptotagmin-7 places vesicles in close membrane apposition ({\textless}6 nm); without it, vesicles accumulate out of reach of the fusion complex (20–40 nm). We suggest that a synaptotagmin-7-dependent movement toward the membrane is involved in Munc13-2/phorbolester/Ca2+-dependent priming as a prelude to fast and slow exocytosis triggering.},
	urldate = {2022-12-04},
	journal = {eLife},
	author = {Tawfik, Bassam and Martins, Joana S and Houy, Sébastien and Imig, Cordelia and Pinheiro, Paulo S and Wojcik, Sonja M and Brose, Nils and Cooper, Benjamin H and Sørensen, Jakob Balslev},
	editor = {Brunger, Axel T and Westbrook, Gary L},
	month = mar,
	year = {2021},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {capacitance measurements, chromaffin cell, neurotransmitter release, SNARE-proteins, synaptotagmin-7, vesicle priming},
	pages = {e64527},
	file = {Full Text PDF:/home/pape/Zotero/storage/TJZTSCX6/Tawfik et al. - 2021 - Synaptotagmin-7 places dense-core vesicles at the .pdf:application/pdf},
}

@article{michanski_mapping_2019,
	title = {Mapping developmental maturation of inner hair cell ribbon synapses in the apical mouse cochlea},
	volume = {116},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.1812029116},
	doi = {10.1073/pnas.1812029116},
	abstract = {Ribbon synapses of cochlear inner hair cells (IHCs) undergo molecular assembly and extensive functional and structural maturation before hearing onset. Here, we characterized the nanostructure of IHC synapses from late prenatal mouse embryo stages (embryonic days 14–18) into adulthood [postnatal day (P)48] using electron microscopy and tomography as well as optical nanoscopy of apical turn organs of Corti. We find that synaptic ribbon precursors arrive at presynaptic active zones (AZs) after afferent contacts have been established. These ribbon precursors contain the proteins RIBEYE and piccolino, tether synaptic vesicles and their delivery likely involves active, microtubule-based transport pathways. Synaptic contacts undergo a maturational transformation from multiple small to one single, large AZ. This maturation is characterized by the fusion of ribbon precursors with membrane-anchored ribbons that also appear to fuse with each other. Such fusion events are most frequently encountered around P12 and hence, coincide with hearing onset in mice. Thus, these events likely underlie the morphological and functional maturation of the AZ. Moreover, the postsynaptic densities appear to undergo a similar refinement alongside presynaptic maturation. Blockwise addition of ribbon material by fusion as found during AZ maturation might represent a general mechanism for modulating ribbon size.},
	number = {13},
	urldate = {2022-12-04},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Michanski, Susann and Smaluch, Katharina and Steyer, Anna Maria and Chakrabarti, Rituparna and Setz, Cristian and Oestreicher, David and Fischer, Christian and Möbius, Wiebke and Moser, Tobias and Vogl, Christian and Wichmann, Carolin},
	month = mar,
	year = {2019},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {6415--6424},
	file = {Full Text PDF:/home/pape/Zotero/storage/KYRRZCLP/Michanski et al. - 2019 - Mapping developmental maturation of inner hair cel.pdf:application/pdf},
}

@misc{zhang_how_2022,
	title = {How {Does} {SimSiam} {Avoid} {Collapse} {Without} {Negative} {Samples}? {A} {Unified} {Understanding} with {Self}-supervised {Contrastive} {Learning}},
	shorttitle = {How {Does} {SimSiam} {Avoid} {Collapse} {Without} {Negative} {Samples}?},
	url = {http://arxiv.org/abs/2203.16262},
	doi = {10.48550/arXiv.2203.16262},
	abstract = {To avoid collapse in self-supervised learning (SSL), a contrastive loss is widely used but often requires a large number of negative samples. Without negative samples yet achieving competitive performance, a recent work has attracted significant attention for providing a minimalist simple Siamese (SimSiam) method to avoid collapse. However, the reason for how it avoids collapse without negative samples remains not fully clear and our investigation starts by revisiting the explanatory claims in the original SimSiam. After refuting their claims, we introduce vector decomposition for analyzing the collapse based on the gradient analysis of the \$l\_2\$-normalized representation vector. This yields a unified perspective on how negative samples and SimSiam alleviate collapse. Such a unified perspective comes timely for understanding the recent progress in SSL.},
	urldate = {2022-12-04},
	publisher = {arXiv},
	author = {Zhang, Chaoning and Zhang, Kang and Zhang, Chenshuang and Pham, Trung X. and Yoo, Chang D. and Kweon, In So},
	month = mar,
	year = {2022},
	note = {arXiv:2203.16262 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/8LNEK24S/Zhang et al. - 2022 - How Does SimSiam Avoid Collapse Without Negative S.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/GPCY8QJD/2203.html:text/html},
}

@misc{chen_exploring_2020,
	title = {Exploring {Simple} {Siamese} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2011.10566},
	doi = {10.48550/arXiv.2011.10566},
	abstract = {Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our "SimSiam" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.},
	urldate = {2022-12-04},
	publisher = {arXiv},
	author = {Chen, Xinlei and He, Kaiming},
	month = nov,
	year = {2020},
	note = {arXiv:2011.10566 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/8VQJVUGX/Chen and He - 2020 - Exploring Simple Siamese Representation Learning.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/TUFDYGIJ/2011.html:text/html},
}

@article{isensee_nnu-net_2021,
	title = {{nnU}-{Net}: a self-configuring method for deep learning-based biomedical image segmentation},
	volume = {18},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {{nnU}-{Net}},
	url = {https://www.nature.com/articles/s41592-020-01008-z},
	doi = {10.1038/s41592-020-01008-z},
	abstract = {Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.},
	language = {en},
	number = {2},
	urldate = {2022-12-05},
	journal = {Nature Methods},
	author = {Isensee, Fabian and Jaeger, Paul F. and Kohl, Simon A. A. and Petersen, Jens and Maier-Hein, Klaus H.},
	month = feb,
	year = {2021},
	note = {Number: 2
Publisher: Nature Publishing Group},
	keywords = {Image processing, Translational research},
	pages = {203--211},
	file = {Full Text PDF:/home/pape/Zotero/storage/LF5JF37E/Isensee et al. - 2021 - nnU-Net a self-configuring method for deep learni.pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/T2BKXHJ7/s41592-020-01008-z.html:text/html},
}

@misc{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	doi = {10.48550/arXiv.2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2022-12-05},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2103.00020 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/PK6W67WY/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/559D6TJV/2103.html:text/html},
}

@misc{lin_focal_2018,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://arxiv.org/abs/1708.02002},
	doi = {10.48550/arXiv.1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	urldate = {2022-12-07},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = feb,
	year = {2018},
	note = {arXiv:1708.02002 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/RQERWWGL/Lin et al. - 2018 - Focal Loss for Dense Object Detection.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/3Z2Y2XMP/1708.html:text/html},
}

@article{gallusser_deep_2022,
	title = {Deep neural network automated segmentation of cellular structures in volume electron microscopy},
	volume = {222},
	issn = {0021-9525},
	url = {https://doi.org/10.1083/jcb.202208005},
	doi = {10.1083/jcb.202208005},
	abstract = {Volume electron microscopy is an important imaging modality in contemporary cell biology. Identification of intracellular structures is a laborious process limiting the effective use of this potentially powerful tool. We resolved this bottleneck with automated segmentation of intracellular substructures in electron microscopy (ASEM), a new pipeline to train a convolutional neural network to detect structures of a wide range in size and complexity. We obtained dedicated models for each structure based on a small number of sparsely annotated ground truth images from only one or two cells. Model generalization was improved with a rapid, computationally effective strategy to refine a trained model by including a few additional annotations. We identified mitochondria, Golgi apparatus, endoplasmic reticulum, nuclear pore complexes, caveolae, clathrin-coated pits, and vesicles imaged by focused ion beam scanning electron microscopy. We uncovered a wide range of membrane–nuclear pore diameters within a single cell and derived morphological metrics from clathrin-coated pits and vesicles, consistent with the classical constant-growth assembly model.},
	number = {2},
	urldate = {2022-12-09},
	journal = {Journal of Cell Biology},
	author = {Gallusser, Benjamin and Maltese, Giorgio and Di Caprio, Giuseppe and Vadakkan, Tegy John and Sanyal, Anwesha and Somerville, Elliott and Sahasrabudhe, Mihir and O’Connor, Justin and Weigert, Martin and Kirchhausen, Tom},
	month = dec,
	year = {2022},
	pages = {e202208005},
	file = {Snapshot:/home/pape/Zotero/storage/HG8NJCFZ/Deep-neural-network-automated-segmentation-of.html:text/html;Submitted Version:/home/pape/Zotero/storage/REQEMT9X/Gallusser et al. - 2022 - Deep neural network automated segmentation of cell.pdf:application/pdf},
}

@article{heinrich_whole-cell_2021-1,
	title = {Whole-cell organelle segmentation in volume electron microscopy},
	volume = {599},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03977-3},
	doi = {10.1038/s41586-021-03977-3},
	abstract = {Cells contain hundreds of organelles and macromolecular assemblies. Obtaining a complete understanding of their intricate organization requires the nanometre-level, three-dimensional reconstruction of whole cells, which is only feasible with robust and scalable automatic methods. Here, to support the development of such methods, we annotated up to 35 different cellular organelle classes—ranging from endoplasmic reticulum to microtubules to ribosomes—in diverse sample volumes from multiple cell types imaged at a near-isotropic resolution of 4 nm per voxel with focused ion beam scanning electron microscopy (FIB-SEM)1. We trained deep learning architectures to segment these structures in 4 nm and 8 nm per voxel FIB-SEM volumes, validated their performance and showed that automatic reconstructions can be used to directly quantify previously inaccessible metrics including spatial interactions between cellular components. We also show that such reconstructions can be used to automatically register light and electron microscopy images for correlative studies. We have created an open data and open-source web repository, ‘OpenOrganelle’, to share the data, computer code and trained models, which will enable scientists everywhere to query and further improve automatic reconstruction of these datasets.},
	language = {en},
	number = {7883},
	urldate = {2022-12-09},
	journal = {Nature},
	author = {Heinrich, Larissa and Bennett, Davis and Ackerman, David and Park, Woohyun and Bogovic, John and Eckstein, Nils and Petruncio, Alyson and Clements, Jody and Pang, Song and Xu, C. Shan and Funke, Jan and Korff, Wyatt and Hess, Harald F. and Lippincott-Schwartz, Jennifer and Saalfeld, Stephan and Weigel, Aubrey V.},
	month = nov,
	year = {2021},
	note = {Number: 7883
Publisher: Nature Publishing Group},
	keywords = {Cellular imaging, Image processing, Machine learning, Organelles, Data mining},
	pages = {141--146},
	file = {Full Text PDF:/home/pape/Zotero/storage/MZ8VL8VD/Heinrich et al. - 2021 - Whole-cell organelle segmentation in volume electr.pdf:application/pdf},
}

@misc{baevski_data2vec_2022,
	title = {data2vec: {A} {General} {Framework} for {Self}-supervised {Learning} in {Speech}, {Vision} and {Language}},
	shorttitle = {data2vec},
	url = {http://arxiv.org/abs/2202.03555},
	doi = {10.48550/arXiv.2202.03555},
	abstract = {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.},
	urldate = {2022-12-15},
	publisher = {arXiv},
	author = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
	month = oct,
	year = {2022},
	note = {arXiv:2202.03555 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/5P6MUW4U/Baevski et al. - 2022 - data2vec A General Framework for Self-supervised .pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/BRAD7VX8/2202.html:text/html},
}

@misc{mahendran_understanding_2014,
	title = {Understanding {Deep} {Image} {Representations} by {Inverting} {Them}},
	url = {http://arxiv.org/abs/1412.0035},
	doi = {10.48550/arXiv.1412.0035},
	abstract = {Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
	urldate = {2022-12-15},
	publisher = {arXiv},
	author = {Mahendran, Aravindh and Vedaldi, Andrea},
	month = nov,
	year = {2014},
	note = {arXiv:1412.0035 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/GRGJ4NJJ/Mahendran and Vedaldi - 2014 - Understanding Deep Image Representations by Invert.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/DKHC3PP5/1412.html:text/html},
}

@misc{yosinski_understanding_2015,
	title = {Understanding {Neural} {Networks} {Through} {Deep} {Visualization}},
	url = {http://arxiv.org/abs/1506.06579},
	doi = {10.48550/arXiv.1506.06579},
	abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
	urldate = {2022-12-15},
	publisher = {arXiv},
	author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
	month = jun,
	year = {2015},
	note = {arXiv:1506.06579 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/SLJ7QN88/Yosinski et al. - 2015 - Understanding Neural Networks Through Deep Visuali.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/AYDWQ4UV/1506.html:text/html},
}

@inproceedings{cadena_diverse_2018,
	title = {Diverse feature visualizations reveal invariances in early layers of deep neural networks},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Santiago_Cadena_Diverse_feature_visualizations_ECCV_2018_paper.html},
	urldate = {2022-12-15},
	author = {Cadena, Santiago A. and Weis, Marissa A. and Gatys, Leon A. and Bethge, Matthias and Ecker, Alexander S.},
	year = {2018},
	pages = {217--232},
	file = {Full Text PDF:/home/pape/Zotero/storage/GLS5NT7W/Cadena et al. - 2018 - Diverse feature visualizations reveal invariances .pdf:application/pdf},
}

@misc{dai_diagnosing_2019,
	title = {Diagnosing and {Enhancing} {VAE} {Models}},
	url = {http://arxiv.org/abs/1903.05789},
	doi = {10.48550/arXiv.1903.05789},
	abstract = {Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples. In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true. We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning. Quantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture. A shorter version of this work will appear in the ICLR 2019 conference proceedings (Dai and Wipf, 2019). The code for our model is available at https://github.com/daib13/ TwoStageVAE.},
	urldate = {2022-12-16},
	publisher = {arXiv},
	author = {Dai, Bin and Wipf, David},
	month = oct,
	year = {2019},
	note = {arXiv:1903.05789 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/FMW3K8WK/Dai and Wipf - 2019 - Diagnosing and Enhancing VAE Models.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/FNVPVEL5/1903.html:text/html},
}

@article{berthelot2019remixmatch,
  title={Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring},
  author={Berthelot, David and Carlini, Nicholas and Cubuk, Ekin D and Kurakin, Alex and Sohn, Kihyuk and Zhang, Han and Raffel, Colin},
  journal={arXiv preprint arXiv:1911.09785},
  year={2019}
}


@misc{tarvainen_mean_2018,
	title = {Mean teachers are better role models: {Weight}-averaged consistency targets improve semi-supervised deep learning results},
	shorttitle = {Mean teachers are better role models},
	abstract = {The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35\% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55\% to 6.28\%, and on ImageNet 2012 with 10\% of the labels from 35.24\% to 9.11\%.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Tarvainen, Antti and Valpola, Harri},
	month = apr,
	year = {2018},
	note = {arXiv:1703.01780 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/DAW7LVJI/Tarvainen and Valpola - 2018 - Mean teachers are better role models Weight-avera.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/X8V67246/1703.html:text/html},
}

@article{xing_adversarial_2019,
	title = {Adversarial {Domain} {Adaptation} and {Pseudo}-{Labeling} for {Cross}-{Modality} {Microscopy} {Image} {Quantification}},
	volume = {11764},
	doi = {10.1007/978-3-030-32239-7_82},
	abstract = {Cell or nucleus quantification has recently achieved state-of-the-art performance by using convolutional neural networks (CNNs). In general, training CNNs requires a large amount of annotated microscopy image data, which is prohibitively expensive or even impossible to obtain in some applications. Additionally, when applying a deep supervised model to new datasets, it is common to annotate individual cells in those target datasets for model re-training or fine-tuning, leading to low-throughput image analysis. In this paperSSS, we propose a novel adversarial domain adaptation method for cell/nucleus quantification across multimodality microscopy image data. Specifically, we learn a fully convolutional network detector with task-specific cycle-consistent adversarial learning, which conducts pixel-level adaptation between source and target domains and completes a cell/nucleus detection task. Then we generate pseudo-labels on target training data using the detector trained with adapted source images and further fine-tune the detector towards the target domain to boost the performance. We evaluate the proposed method on multiple cross-modality microscopy image datasets and obtain a significant improvement in cell/nucleus detection compared to the reference baselines and a recent state-of-the-art deep domain adaptation approach. In addition, our method is very competitive with the fully supervised models trained with all real target training labels.},
	language = {eng},
	journal = {Medical image computing and computer-assisted intervention: MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
	author = {Xing, Fuyong and Bennett, Tell and Ghosh, Debashis},
	month = oct,
	year = {2019},
	pmid = {31825019},
	pmcid = {PMC6903918},
	pages = {740--749},
	file = {Accepted Version:/home/pape/Zotero/storage/SSHRPJXU/Xing et al. - 2019 - Adversarial Domain Adaptation and Pseudo-Labeling .pdf:application/pdf},
}

@misc{choi_pseudo-labeling_2019,
	title = {Pseudo-{Labeling} {Curriculum} for {Unsupervised} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1908.00262},
	doi = {10.48550/arXiv.1908.00262},
	abstract = {To learn target discriminative representations, using pseudo-labels is a simple yet effective approach for unsupervised domain adaptation. However, the existence of false pseudo-labels, which may have a detrimental influence on learning target representations, remains a major challenge. To overcome this issue, we propose a pseudo-labeling curriculum based on a density-based clustering algorithm. Since samples with high density values are more likely to have correct pseudo-labels, we leverage these subsets to train our target network at the early stage, and utilize data subsets with low density values at the later stage. We can progressively improve the capability of our network to generate pseudo-labels, and thus these target samples with pseudo-labels are effective for training our model. Moreover, we present a clustering constraint to enhance the discriminative power of the learned target features. Our approach achieves state-of-the-art performance on three benchmarks: Office-31, imageCLEF-DA, and Office-Home.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Choi, Jaehoon and Jeong, Minki and Kim, Taekyung and Kim, Changick},
	month = aug,
	year = {2019},
	note = {arXiv:1908.00262 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/V85D3NRQ/Choi et al. - 2019 - Pseudo-Labeling Curriculum for Unsupervised Domain.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/J2EBMRJ6/1908.html:text/html},
}

@misc{zou_domain_2018,
	title = {Domain {Adaptation} for {Semantic} {Segmentation} via {Class}-{Balanced} {Self}-{Training}},
	abstract = {Recent deep networks achieved state of the art performance on a variety of semantic segmentation tasks. Despite such progress, these models often face challenges in real world `wild tasks' where large difference between labeled training/source data and unseen test/target data exists. In particular, such difference is often referred to as `domain gap', and could cause significantly decreased performance which cannot be easily remedied by further increasing the representation power. Unsupervised domain adaptation (UDA) seeks to overcome such problem without target domain labels. In this paper, we propose a novel UDA framework based on an iterative self-training procedure, where the problem is formulated as latent variable loss minimization, and can be solved by alternatively generating pseudo labels on target data and re-training the model with these labels. On top of self-training, we also propose a novel class-balanced self-training framework to avoid the gradual dominance of large classes on pseudo-label generation, and introduce spatial priors to refine generated labels. Comprehensive experiments show that the proposed methods achieve state of the art semantic segmentation performance under multiple major UDA settings.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Zou, Yang and Yu, Zhiding and Kumar, B. V. K. Vijaya and Wang, Jinsong},
	month = oct,
	year = {2018},
	note = {arXiv:1810.07911 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/RTHVXNVE/Zou et al. - 2018 - Domain Adaptation for Semantic Segmentation via Cl.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/HYVF6PFY/1810.html:text/html},
}

@inproceedings{NIPS2015_8d55a249,
 author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning Structured Output Representation using Deep Conditional Generative Models},
 volume = {28},
 year = {2015}
}

@article{jaeger2014two,
  title={Two public chest X-ray datasets for computer-aided screening of pulmonary diseases},
  author={Jaeger, Stefan and Candemir, Sema and Antani, Sameer and W{\'a}ng, Y{\`\i}-Xi{\'a}ng J and Lu, Pu-Xuan and Thoma, George},
  journal={Quantitative imaging in medicine and surgery},
  volume={4},
  number={6},
  pages={475},
  year={2014},
  publisher={AME Publications}
}

@article{shiraishi2000development,
  title={Development of a digital image database for chest radiographs with and without a lung nodule: receiver operating characteristic analysis of radiologists' detection of pulmonary nodules},
  author={Shiraishi, Junji and Katsuragawa, Shigehiko and Ikezoe, Junpei and Matsumoto, Tsuneo and Kobayashi, Takeshi and Komatsu, Ken-ichi and Matsui, Mitate and Fujita, Hiroshi and Kodera, Yoshie and Doi, Kunio},
  journal={American Journal of Roentgenology},
  volume={174},
  number={1},
  pages={71--74},
  year={2000},
  publisher={Am Roentgen Ray Soc}
}


@inproceedings{tang2019xlsor,
  title={Xlsor: A robust and accurate lung segmentor on chest x-rays using criss-cross attention and customized radiorealistic abnormalities generation},
  author={Tang, You-Bao and Tang, Yu-Xing and Xiao, Jing and Summers, Ronald M},
  booktitle={International Conference on Medical Imaging with Deep Learning},
  pages={457--467},
  year={2019},
  organization={PMLR}
}

@inproceedings{yun2019cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6023--6032},
  year={2019}
}


@article{edlund2021livecell,
  title={LIVECell—A large-scale dataset for label-free live cell segmentation},
  author={Edlund, Christoffer and Jackson, Timothy R and Khalid, Nabeel and Bevan, Nicola and Dale, Timothy and Dengel, Andreas and Ahmed, Sheraz and Trygg, Johan and Sj{\"o}gren, Rickard},
  journal={Nature methods},
  volume={18},
  number={9},
  pages={1038--1045},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

@inproceedings {wei2020mitoem,
    title={MitoEM Dataset: Large-Scale 3D Mitochondria Instance Segmentation from EM Images},
    author={Wei, Donglai and Lin, Zudi and Franco-Barranco, Daniel and Wendt, Nils and Liu, Xingyu and Yin, Wenjie and Huang, Xin and Gupta, Aarush and Jang, Won-Dong and Wang, Xueying and others},
    booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
    pages={66--76},
    year={2020},
    organization={Springer}
}

@article{mekuvc2020automatic,
  title={Automatic segmentation of mitochondria and endolysosomes in volumetric electron microscopy data},
  author={Meku{\v{c}}, Manca {\v{Z}}erovnik and Bohak, Ciril and Hudoklin, Samo and Kim, Byeong Hak and Kim, Min Young and Marolt, Matija and others},
  journal={Computers in biology and medicine},
  volume={119},
  pages={103693},
  year={2020},
  publisher={Elsevier}
}

@article{gerhard2013segmented,
  title={Segmented anisotropic ssTEM dataset of neural tissue},
  author={Gerhard, Stephan and Funke, Jan and Martel, Julien and Cardona, Albert and Fetter, Richard},
  journal={figshare},
  pages={0--0},
  year={2013},
  publisher={figshare}
}


@inproceedings{lucchi2012structured,
  title={Structured Image Segmentation using Kernelized},
  author={Lucchi, Aur{\'e}lien and Li, Yunpeng and Smith, Kevin and Fua, Pascal},
  booktitle={Computer Vision--ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part II},
  volume={7573},
  pages={400},
  year={2012},
  organization={Springer}
}


@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@misc{mei_instance_2020,
	title = {Instance {Adaptive} {Self}-{Training} for {Unsupervised} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/2008.12197},
	doi = {10.48550/arXiv.2008.12197},
	abstract = {The divergence between labeled training data and unlabeled testing data is a significant challenge for recent deep learning models. Unsupervised domain adaptation (UDA) attempts to solve such a problem. Recent works show that self-training is a powerful approach to UDA. However, existing methods have difficulty in balancing scalability and performance. In this paper, we propose an instance adaptive self-training framework for UDA on the task of semantic segmentation. To effectively improve the quality of pseudo-labels, we develop a novel pseudo-label generation strategy with an instance adaptive selector. Besides, we propose the region-guided regularization to smooth the pseudo-label region and sharpen the non-pseudo-label region. Our method is so concise and efficient that it is easy to be generalized to other unsupervised domain adaptation methods. Experiments on 'GTA5 to Cityscapes' and 'SYNTHIA to Cityscapes' demonstrate the superior performance of our approach compared with the state-of-the-art methods.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Mei, Ke and Zhu, Chuang and Zou, Jiaqi and Zhang, Shanghang},
	month = aug,
	year = {2020},
	note = {arXiv:2008.12197 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/QFDHJA69/Mei et al. - 2020 - Instance Adaptive Self-Training for Unsupervised D.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/WCY8TEJ5/2008.html:text/html},
}

@misc{chen_semi-supervised_2021,
	title = {Semi-supervised {Domain} {Adaptation} based on {Dual}-level {Domain} {Mixing} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/2103.04705},
	doi = {10.48550/arXiv.2103.04705},
	abstract = {Data-driven based approaches, in spite of great success in many tasks, have poor generalization when applied to unseen image domains, and require expensive cost of annotation especially for dense pixel prediction tasks such as semantic segmentation. Recently, both unsupervised domain adaptation (UDA) from large amounts of synthetic data and semi-supervised learning (SSL) with small set of labeled data have been studied to alleviate this issue. However, there is still a large gap on performance compared to their supervised counterparts. We focus on a more practical setting of semi-supervised domain adaptation (SSDA) where both a small set of labeled target data and large amounts of labeled source data are available. To address the task of SSDA, a novel framework based on dual-level domain mixing is proposed. The proposed framework consists of three stages. First, two kinds of data mixing methods are proposed to reduce domain gap in both region-level and sample-level respectively. We can obtain two complementary domain-mixed teachers based on dual-level mixed data from holistic and partial views respectively. Then, a student model is learned by distilling knowledge from these two teachers. Finally, pseudo labels of unlabeled data are generated in a self-training manner for another few rounds of teachers training. Extensive experimental results have demonstrated the effectiveness of our proposed framework on synthetic-to-real semantic segmentation benchmarks.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Chen, Shuaijun and Jia, Xu and He, Jianzhong and Shi, Yongjie and Liu, Jianzhuang},
	month = mar,
	year = {2021},
	note = {arXiv:2103.04705 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/VFZ3C4HH/Chen et al. - 2021 - Semi-supervised Domain Adaptation based on Dual-le.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/UAWRWGJH/2103.html:text/html},
}

@misc{wang_recurrent_2019,
	title = {Recurrent {U}-{Net} for {Resource}-{Constrained} {Segmentation}},
	url = {http://arxiv.org/abs/1906.04913},
	doi = {10.48550/arXiv.1906.04913},
	abstract = {State-of-the-art segmentation methods rely on very deep networks that are not always easy to train without very large training datasets and tend to be relatively slow to run on standard GPUs. In this paper, we introduce a novel recurrent U-Net architecture that preserves the compactness of the original U-Net, while substantially increasing its performance to the point where it outperforms the state of the art on several benchmarks. We will demonstrate its effectiveness for several tasks, including hand segmentation, retina vessel segmentation, and road segmentation. We also introduce a large-scale dataset for hand segmentation.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Wang, Wei and Yu, Kaicheng and Hugonot, Joachim and Fua, Pascal and Salzmann, Mathieu},
	month = jun,
	year = {2019},
	note = {arXiv:1906.04913 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/RN7LZJDE/Wang et al. - 2019 - Recurrent U-Net for Resource-Constrained Segmentat.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/6CEIMTEW/1906.html:text/html},
}

@incollection{schmidt_cell_2018,
	title = {Cell {Detection} with {Star}-convex {Polygons}},
	volume = {11071},
	url = {http://arxiv.org/abs/1806.03535},
	abstract = {Automatic detection and segmentation of cells and nuclei in microscopy images is important for many biological applications. Recent successful learning-based approaches include per-pixel cell segmentation with subsequent pixel grouping, or localization of bounding boxes with subsequent shape refinement. In situations of crowded cells, these can be prone to segmentation errors, such as falsely merging bordering cells or suppressing valid cell instances due to the poor approximation with bounding boxes. To overcome these issues, we propose to localize cell nuclei via star-convex polygons, which are a much better shape representation as compared to bounding boxes and thus do not need shape refinement. To that end, we train a convolutional neural network that predicts for every pixel a polygon for the cell instance at that position. We demonstrate the merits of our approach on two synthetic datasets and one challenging dataset of diverse fluorescence microscopy images.},
	urldate = {2022-12-18},
	author = {Schmidt, Uwe and Weigert, Martin and Broaddus, Coleman and Myers, Gene},
	year = {2018},
	doi = {10.1007/978-3-030-00934-2_30},
	note = {arXiv:1806.03535 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {265--273},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/PY6ABTGC/Schmidt et al. - 2018 - Cell Detection with Star-convex Polygons.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/Q3BZ77V2/1806.html:text/html},
}

@inproceedings{weigert_star-convex_2020,
	title = {Star-convex {Polyhedra} for {3D} {Object} {Detection} and {Segmentation} in {Microscopy}},
	url = {http://arxiv.org/abs/1908.03636},
	doi = {10.1109/WACV45572.2020.9093435},
	abstract = {Accurate detection and segmentation of cell nuclei in volumetric (3D) fluorescence microscopy datasets is an important step in many biomedical research projects. Although many automated methods for these tasks exist, they often struggle for images with low signal-to-noise ratios and/or dense packing of nuclei. It was recently shown for 2D microscopy images that these issues can be alleviated by training a neural network to directly predict a suitable shape representation (star-convex polygon) for cell nuclei. In this paper, we adopt and extend this approach to 3D volumes by using star-convex polyhedra to represent cell nuclei and similar shapes. To that end, we overcome the challenges of 1) finding parameter-efficient star-convex polyhedra representations that can faithfully describe cell nuclei shapes, 2) adapting to anisotropic voxel sizes often found in fluorescence microscopy datasets, and 3) efficiently computing intersections between pairs of star-convex polyhedra (required for non-maximum suppression). Although our approach is quite general, since star-convex polyhedra include common shapes like bounding boxes and spheres as special cases, our focus is on accurate detection and segmentation of cell nuclei. Finally, we demonstrate on two challenging datasets that our approach (StarDist-3D) leads to superior results when compared to classical and deep learning based methods.},
	urldate = {2022-12-18},
	booktitle = {2020 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Weigert, Martin and Schmidt, Uwe and Haase, Robert and Sugawara, Ko and Myers, Gene},
	month = mar,
	year = {2020},
	note = {arXiv:1908.03636 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {3655--3662},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/SZMG8NYC/Weigert et al. - 2020 - Star-convex Polyhedra for 3D Object Detection and .pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/QSP39CRQ/1908.html:text/html},
}

@misc{de_brabandere_semantic_2017,
	title = {Semantic {Instance} {Segmentation} with a {Discriminative} {Loss} {Function}},
	url = {http://arxiv.org/abs/1708.02551},
	doi = {10.48550/arXiv.1708.02551},
	abstract = {Semantic instance segmentation remains a challenging task. In this work we propose to tackle the problem with a discriminative loss function, operating at the pixel level, that encourages a convolutional network to produce a representation of the image that can easily be clustered into instances with a simple post-processing step. The loss function encourages the network to map each pixel to a point in feature space so that pixels belonging to the same instance lie close together while different instances are separated by a wide margin. Our approach of combining an off-the-shelf network with a principled loss function inspired by a metric learning objective is conceptually simple and distinct from recent efforts in instance segmentation. In contrast to previous works, our method does not rely on object proposals or recurrent mechanisms. A key contribution of our work is to demonstrate that such a simple setup without bells and whistles is effective and can perform on par with more complex methods. Moreover, we show that it does not suffer from some of the limitations of the popular detect-and-segment approaches. We achieve competitive performance on the Cityscapes and CVPPP leaf segmentation benchmarks.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {De Brabandere, Bert and Neven, Davy and Van Gool, Luc},
	month = aug,
	year = {2017},
	note = {arXiv:1708.02551 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/FLPT7FFV/De Brabandere et al. - 2017 - Semantic Instance Segmentation with a Discriminati.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/ZNLL35TE/1708.html:text/html},
}

@misc{shi_inconsistency-aware_2021,
	title = {Inconsistency-aware {Uncertainty} {Estimation} for {Semi}-supervised {Medical} {Image} {Segmentation}},
	abstract = {In semi-supervised medical image segmentation, most previous works draw on the common assumption that higher entropy means higher uncertainty. In this paper, we investigate a novel method of estimating uncertainty. We observe that, when assigned different misclassification costs in a certain degree, if the segmentation result of a pixel becomes inconsistent, this pixel shows a relative uncertainty in its segmentation. Therefore, we present a new semi-supervised segmentation model, namely, conservative-radical network (CoraNet in short) based on our uncertainty estimation and separate self-training strategy. In particular, our CoraNet model consists of three major components: a conservative-radical module (CRM), a certain region segmentation network (C-SN), and an uncertain region segmentation network (UC-SN) that could be alternatively trained in an end-to-end manner. We have extensively evaluated our method on various segmentation tasks with publicly available benchmark datasets, including CT pancreas, MR endocardium, and MR multi-structures segmentation on the ACDC dataset. Compared with the current state of the art, our CoraNet has demonstrated superior performance. In addition, we have also analyzed its connection with and difference from conventional methods of uncertainty estimation in semi-supervised medical image segmentation.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Shi, Yinghuan and Zhang, Jian and Ling, Tong and Lu, Jiwen and Zheng, Yefeng and Yu, Qian and Qi, Lei and Gao, Yang},
	month = oct,
	year = {2021},
	note = {arXiv:2110.08762 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/TZDF2A64/Shi et al. - 2021 - Inconsistency-aware Uncertainty Estimation for Sem.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/ER5QC5IS/2110.html:text/html},
}

@misc{yu_uncertainty-aware_2019,
	title = {Uncertainty-aware {Self}-ensembling {Model} for {Semi}-supervised {3D} {Left} {Atrium} {Segmentation}},
	abstract = {Training deep convolutional neural networks usually requires a large amount of labeled data. However, it is expensive and time-consuming to annotate data for medical image segmentation tasks. In this paper, we present a novel uncertainty-aware semi-supervised framework for left atrium segmentation from 3D MR images. Our framework can effectively leverage the unlabeled data by encouraging consistent predictions of the same input under different perturbations. Concretely, the framework consists of a student model and a teacher model, and the student model learns from the teacher model by minimizing a segmentation loss and a consistency loss with respect to the targets of the teacher model. We design a novel uncertainty-aware scheme to enable the student model to gradually learn from the meaningful and reliable targets by exploiting the uncertainty information. Experiments show that our method achieves high performance gains by incorporating the unlabeled data. Our method outperforms the state-of-the-art semi-supervised methods, demonstrating the potential of our framework for the challenging semi-supervised problems.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Yu, Lequan and Wang, Shujun and Li, Xiaomeng and Fu, Chi-Wing and Heng, Pheng-Ann},
	month = jul,
	year = {2019},
	note = {arXiv:1907.07034 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/6CNRXSBU/Yu et al. - 2019 - Uncertainty-aware Self-ensembling Model for Semi-s.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/S5K5956C/1907.html:text/html},
}

@article{turner_reconstruction_2022,
	title = {Reconstruction of neocortex: {Organelles}, compartments, cells, circuits, and activity},
	volume = {185},
	issn = {0092-8674},
	shorttitle = {Reconstruction of neocortex},
	url = {https://www.sciencedirect.com/science/article/pii/S0092867422001349},
	doi = {10.1016/j.cell.2022.01.023},
	abstract = {We assembled a semi-automated reconstruction of L2/3 mouse primary visual cortex from ∼250 × 140 × 90 μm3 of electron microscopic images, including pyramidal and non-pyramidal neurons, astrocytes, microglia, oligodendrocytes and precursors, pericytes, vasculature, nuclei, mitochondria, and synapses. Visual responses of a subset of pyramidal cells are included. The data are publicly available, along with tools for programmatic and three-dimensional interactive access. Brief vignettes illustrate the breadth of potential applications relating structure to function in cortical circuits and neuronal cell biology. Mitochondria and synapse organization are characterized as a function of path length from the soma. Pyramidal connectivity motif frequencies are predicted accurately using a configuration model of random graphs. Pyramidal cells receiving more connections from nearby cells exhibit stronger and more reliable visual responses. Sample code shows data access and analysis.},
	language = {en},
	number = {6},
	urldate = {2022-12-18},
	journal = {Cell},
	author = {Turner, Nicholas L. and Macrina, Thomas and Bae, J. Alexander and Yang, Runzhe and Wilson, Alyssa M. and Schneider-Mizell, Casey and Lee, Kisuk and Lu, Ran and Wu, Jingpeng and Bodor, Agnes L. and Bleckert, Adam A. and Brittain, Derrick and Froudarakis, Emmanouil and Dorkenwald, Sven and Collman, Forrest and Kemnitz, Nico and Ih, Dodam and Silversmith, William M. and Zung, Jonathan and Zlateski, Aleksandar and Tartavull, Ignacio and Yu, Szi-chieh and Popovych, Sergiy and Mu, Shang and Wong, William and Jordan, Chris S. and Castro, Manuel and Buchanan, JoAnn and Bumbarger, Daniel J. and Takeno, Marc and Torres, Russel and Mahalingam, Gayathri and Elabbady, Leila and Li, Yang and Cobos, Erick and Zhou, Pengcheng and Suckow, Shelby and Becker, Lynne and Paninski, Liam and Polleux, Franck and Reimer, Jacob and Tolias, Andreas S. and Reid, R. Clay and da Costa, Nuno Maçarico and Seung, H. Sebastian},
	month = mar,
	year = {2022},
	keywords = {3D reconstruction, calcium imaging, cortex, electron microscopy, inhibitory cell, mitochondria, mouse, pyramidal cell, synaptic connectivity, visual cortex},
	pages = {1082--1100.e24},
}

@article{payer_segmenting_2019,
	title = {Segmenting and tracking cell instances with cosine embeddings and recurrent hourglass networks},
	volume = {57},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S136184151930057X},
	doi = {10.1016/j.media.2019.06.015},
	abstract = {Differently to semantic segmentation, instance segmentation assigns unique labels to each individual instance of the same object class. In this work, we propose a novel recurrent fully convolutional network architecture for tracking such instance segmentations over time, which is highly relevant, e.g., in biomedical applications involving cell growth and migration. Our network architecture incorporates convolutional gated recurrent units (ConvGRU) into a stacked hourglass network to utilize temporal information, e.g., from microscopy videos. Moreover, we train our network with a novel embedding loss based on cosine similarities, such that the network predicts unique embeddings for every instance throughout videos, even in the presence of dynamic structural changes due to mitosis of cells. To create the final tracked instance segmentations, the pixel-wise embeddings are clustered among subsequent video frames by using the mean shift algorithm. After showing the performance of the instance segmentation on a static in-house dataset of muscle fibers from H\&E-stained microscopy images, we also evaluate our proposed recurrent stacked hourglass network regarding instance segmentation and tracking performance on six datasets from the ISBI celltracking challenge, where it delivers state-of-the-art results.},
	language = {en},
	urldate = {2022-12-18},
	journal = {Medical Image Analysis},
	author = {Payer, Christian and Štern, Darko and Feiner, Marlies and Bischof, Horst and Urschler, Martin},
	month = oct,
	year = {2019},
	keywords = {Cell, Embeddings, Instances, Recurrent, Segmentation, Tracking, Video},
	pages = {106--119},
	file = {ScienceDirect Full Text PDF:/home/pape/Zotero/storage/FGMJAXSM/Payer et al. - 2019 - Segmenting and tracking cell instances with cosine.pdf:application/pdf},
}

@misc{payer_instance_2018,
	title = {Instance {Segmentation} and {Tracking} with {Cosine} {Embeddings} and {Recurrent} {Hourglass} {Networks}},
	url = {http://arxiv.org/abs/1806.02070},
	doi = {10.48550/arXiv.1806.02070},
	abstract = {Different to semantic segmentation, instance segmentation assigns unique labels to each individual instance of the same class. In this work, we propose a novel recurrent fully convolutional network architecture for tracking such instance segmentations over time. The network architecture incorporates convolutional gated recurrent units (ConvGRU) into a stacked hourglass network to utilize temporal video information. Furthermore, we train the network with a novel embedding loss based on cosine similarities, such that the network predicts unique embeddings for every instance throughout videos. Afterwards, these embeddings are clustered among subsequent video frames to create the final tracked instance segmentations. We evaluate the recurrent hourglass network by segmenting left ventricles in MR videos of the heart, where it outperforms a network that does not incorporate video information. Furthermore, we show applicability of the cosine embedding loss for segmenting leaf instances on still images of plants. Finally, we evaluate the framework for instance segmentation and tracking on six datasets of the ISBI celltracking challenge, where it shows state-of-the-art performance.},
	urldate = {2022-12-20},
	publisher = {arXiv},
	author = {Payer, Christian and Štern, Darko and Neff, Thomas and Bischof, Horst and Urschler, Martin},
	month = jul,
	year = {2018},
	note = {arXiv:1806.02070 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/GAMUYIKU/Payer et al. - 2018 - Instance Segmentation and Tracking with Cosine Emb.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/IF952YJU/1806.html:text/html},
}

@inproceedings{heinrich_synaptic_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Synaptic {Cleft} {Segmentation} in {Non}-isotropic {Volume} {Electron} {Microscopy} of the {Complete} {Drosophila} {Brain}},
	isbn = {978-3-030-00934-2},
	doi = {10.1007/978-3-030-00934-2_36},
	abstract = {Neural circuit reconstruction at single synapse resolution is increasingly recognized as crucially important to decipher the function of biological nervous systems. Volume electron microscopy in serial transmission or scanning mode has been demonstrated to provide the necessary resolution to segment or trace all neurites and to annotate all synaptic connections.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2018},
	publisher = {Springer International Publishing},
	author = {Heinrich, Larissa and Funke, Jan and Pape, Constantin and Nunez-Iglesias, Juan and Saalfeld, Stephan},
	editor = {Frangi, Alejandro F. and Schnabel, Julia A. and Davatzikos, Christos and Alberola-López, Carlos and Fichtinger, Gabor},
	year = {2018},
	pages = {317--325},
	file = {Submitted Version:/home/pape/Zotero/storage/TA2HFBEX/Heinrich et al. - 2018 - Synaptic Cleft Segmentation in Non-isotropic Volum.pdf:application/pdf},
}

@inproceedings{turner_synaptic_2020,
	title = {Synaptic {Partner} {Assignment} {Using} {Attentional} {Voxel} {Association} {Networks}},
	doi = {10.1109/ISBI45749.2020.9098489},
	abstract = {Connectomics aims to recover a complete set of synaptic connections within a dataset imaged by volume electron microscopy. Many systems have been proposed for locating synapses, and recent research has included a way to identify the synaptic partners that communicate at a synaptic cleft. We reframe the problem of identifying synaptic partners as directly generating the mask of the synaptic partners from a given cleft. We train a convolutional network to perform this task. The network takes the local image context and a binary mask representing a single cleft as input. It is trained to produce two binary output masks: one which labels the voxels of the presynaptic partner within the input image, and another similar labeling for the postsynaptic partner. The cleft mask acts as an attentional gating signal for the network. We find that an implementation of this approach performs well on a dataset of mouse somatosensory cortex, and evaluate it as part of a combined system to predict both clefts and connections.},
	booktitle = {2020 {IEEE} 17th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Turner, Nicholas L. and Lee, Kisuk and Lu, Ran and Wu, Jingpeng and Ih, Dodam and Seung, H. Sebastian},
	month = apr,
	year = {2020},
	note = {ISSN: 1945-8452},
	keywords = {Machine learning, Image segmentation, Brain, Connectivity Analysis, Machine Learning, Mice, Microscopy - Electron, Neurons, Pattern Recognition and classification, Synapses, Task analysis, Training},
	pages = {1--5},
	file = {Submitted Version:/home/pape/Zotero/storage/4YI5FBAK/Turner et al. - 2020 - Synaptic Partner Assignment Using Attentional Voxe.pdf:application/pdf},
}

@article{buhmann_automatic_2021,
	title = {Automatic detection of synaptic partners in a whole-brain {Drosophila} electron microscopy data set},
	volume = {18},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-021-01183-7},
	doi = {10.1038/s41592-021-01183-7},
	abstract = {We develop an automatic method for synaptic partner identification in insect brains and use it to predict synaptic partners in a whole-brain electron microscopy dataset of the fruit fly. The predictions can be used to infer a connectivity graph with high accuracy, thus allowing fast identification of neural pathways. To facilitate circuit reconstruction using our results, we develop CIRCUITMAP, a user interface add-on for the circuit annotation tool CATMAID.},
	language = {en},
	number = {7},
	urldate = {2022-12-22},
	journal = {Nature Methods},
	author = {Buhmann, Julia and Sheridan, Arlo and Malin-Mayor, Caroline and Schlegel, Philipp and Gerhard, Stephan and Kazimiers, Tom and Krause, Renate and Nguyen, Tri M. and Heinrich, Larissa and Lee, Wei-Chung Allen and Wilson, Rachel and Saalfeld, Stephan and Jefferis, Gregory S. X. E. and Bock, Davi D. and Turaga, Srinivas C. and Cook, Matthew and Funke, Jan},
	month = jul,
	year = {2021},
	note = {Number: 7
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Software, Databases, Neuroscience},
	pages = {771--774},
	file = {Full Text PDF:/home/pape/Zotero/storage/42DJ7JMK/Buhmann et al. - 2021 - Automatic detection of synaptic partners in a whol.pdf:application/pdf},
}

@article{wagner_sphire-cryolo_2019,
	title = {{SPHIRE}-{crYOLO} is a fast and accurate fully automated particle picker for cryo-{EM}},
	volume = {2},
	copyright = {2019 The Author(s)},
	issn = {2399-3642},
	url = {https://www.nature.com/articles/s42003-019-0437-z},
	doi = {10.1038/s42003-019-0437-z},
	abstract = {Selecting particles from digital micrographs is an essential step in single-particle electron cryomicroscopy (cryo-EM). As manual selection of complete datasets—typically comprising thousands of particles—is a tedious and time-consuming process, numerous automatic particle pickers have been developed. However, non-ideal datasets pose a challenge to particle picking. Here we present the particle picking software crYOLO which is based on the deep-learning object detection system You Only Look Once (YOLO). After training the network with 200–2500 particles per dataset it automatically recognizes particles with high recall and precision while reaching a speed of up to five micrographs per second. Further, we present a general crYOLO network able to pick from previously unseen datasets, allowing for completely automated on-the-fly cryo-EM data preprocessing during data acquisition. crYOLO is available as a standalone program under http://sphire.mpg.de/and is distributed as part of the image processing workflow in SPHIRE.},
	language = {en},
	number = {1},
	urldate = {2022-12-22},
	journal = {Communications Biology},
	author = {Wagner, Thorsten and Merino, Felipe and Stabrin, Markus and Moriya, Toshio and Antoni, Claudia and Apelbaum, Amir and Hagel, Philine and Sitsel, Oleg and Raisch, Tobias and Prumbaum, Daniel and Quentin, Dennis and Roderer, Daniel and Tacke, Sebastian and Siebolds, Birte and Schubert, Evelyn and Shaikh, Tanvir R. and Lill, Pascal and Gatsogiannis, Christos and Raunser, Stefan},
	month = jun,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Cryoelectron microscopy, Data processing},
	pages = {1--13},
	file = {Full Text PDF:/home/pape/Zotero/storage/W68AJ5ZH/Wagner et al. - 2019 - SPHIRE-crYOLO is a fast and accurate fully automat.pdf:application/pdf},
}

@misc{teresa_convolutional_2022,
	title = {Convolutional networks for supervised mining of molecular patterns within cellular context},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.04.12.488077v1},
	doi = {10.1101/2022.04.12.488077},
	abstract = {Cryo-electron tomograms capture a wealth of structural information on the molecular constituents of cells and tissues. We present DeePiCt (Deep Picker in Context), an open-source deep-learning framework for supervised structure segmentation and macromolecular complex localization in cellular cryo-electron tomography. To train and benchmark DeePiCt on experimental data, we comprehensively annotated 20 tomograms of Schizosaccharomyces pombe for ribosomes, fatty acid synthases, membranes, nuclear pore complexes, organelles and cytosol. By comparing our method to state-of-the-art approaches on this dataset, we show its unique ability to identify low-abundance and low-density complexes. We use DeePiCt to study compositionally-distinct subpopulations of cellular ribosomes, with emphasis on their contextual association with mitochondria and the endoplasmic reticulum. Finally, by applying pre-trained networks to a HeLa cell dataset, we demonstrate that DeePiCt achieves high-quality predictions in unseen datasets from different biological species in a matter of minutes. The comprehensively annotated experimental data and pre-trained networks are provided for immediate exploitation by the community.},
	language = {en},
	urldate = {2022-12-22},
	publisher = {bioRxiv},
	author = {Teresa, Irene de and Goetz, Sara K. and Mattausch, Alexander and Stojanovska, Frosina and Zimmerli, Christian E. and Toro-Nahuelpan, Mauricio and Cheng, Dorothy W. C. and Tollervey, Fergus and Pape, Constantin and Beck, Martin and Kreshuk, Anna and Mahamid, Julia and Zaugg, Judith},
	month = apr,
	year = {2022},
	note = {Pages: 2022.04.12.488077
Section: New Results},
	file = {Full Text PDF:/home/pape/Zotero/storage/6CV2GMWA/Teresa et al. - 2022 - Convolutional networks for supervised mining of mo.pdf:application/pdf},
}

@inproceedings{roels_domain_2019,
	title = {Domain {Adaptive} {Segmentation} {In} {Volume} {Electron} {Microscopy} {Imaging}},
	doi = {10.1109/ISBI.2019.8759383},
	abstract = {In the last years, automated segmentation has become a necessary tool for volume electron microscopy (EM) imaging. So far, the best performing techniques have been largely based on fully supervised encoder-decoder CNNs, requiring a substantial amount of annotated images. Domain Adaptation (DA) aims to alleviate the annotation burden by `adapting' the networks trained on existing groundtruth data (source domain) to work on a different (target) domain with as little additional annotation as possible. Most DA research is focused on the classification task, whereas volume EM segmentation remains rather unexplored. In this work, we extend recently proposed classification DA techniques to an encoder-decoder layout and propose a novel method that adds a reconstruction decoder to the classical encoder-decoder segmentation in order to align source and target encoder features. The method has been validated on the task of segmenting mitochondria in EM volumes. We have performed DA from brain EM images to HeLa cells and from isotropic FIB/SEM volumes to anisotropic TEM volumes. In all cases, the proposed method has outperformed the extended classification DA techniques and the finetuning baseline. An implementation of our work can be found on https://github.com/JorisRoels/domain-adaptive-segmentation.},
	booktitle = {2019 {IEEE} 16th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2019)},
	author = {Roels, Joris and Hennies, Julian and Saeys, Yvan and Philips, Wilfried and Kreshuk, Anna},
	month = apr,
	year = {2019},
	note = {ISSN: 1945-8452},
	keywords = {Image segmentation, Electron microscopy, Training, Biomedical imaging, Decoding, domain adaptation, Feature extraction, segmentation},
	pages = {1519--1522},
	file = {Full Text:/home/pape/Zotero/storage/WDIM78YY/Roels et al. - 2019 - Domain Adaptive Segmentation In Volume Electron Mi.pdf:application/pdf},
}

@article{matskevych_shallow_2022,
	title = {From {Shallow} to {Deep}: {Exploiting} {Feature}-{Based} {Classifiers} for {Domain} {Adaptation} in {Semantic} {Segmentation}},
	volume = {4},
	issn = {2624-9898},
	shorttitle = {From {Shallow} to {Deep}},
	url = {https://www.frontiersin.org/articles/10.3389/fcomp.2022.805166},
	abstract = {The remarkable performance of Convolutional Neural Networks on image segmentation tasks comes at the cost of a large amount of pixelwise annotated images that have to be segmented for training. In contrast, feature-based learning methods, such as the Random Forest, require little training data, but rarely reach the segmentation accuracy of CNNs. This work bridges the two approaches in a transfer learning setting. We show that a CNN can be trained to correct the errors of the Random Forest in the source domain and then be applied to correct such errors in the target domain without retraining, as the domain shift between the Random Forest predictions is much smaller than between the raw data. By leveraging a few brushstrokes as annotations in the target domain, the method can deliver segmentations that are sufficiently accurate to act as pseudo-labels for target-domain CNN training. We demonstrate the performance of the method on several datasets with the challenging tasks of mitochondria, membrane and nuclear segmentation. It yields excellent performance compared to microscopy domain adaptation baselines, especially when a significant domain shift is involved.},
	urldate = {2022-12-22},
	journal = {Frontiers in Computer Science},
	author = {Matskevych, Alex and Wolny, Adrian and Pape, Constantin and Kreshuk, Anna},
	year = {2022},
	file = {Full Text PDF:/home/pape/Zotero/storage/RDXZFRLC/Matskevych et al. - 2022 - From Shallow to Deep Exploiting Feature-Based Cla.pdf:application/pdf},
}

@misc{he_masked_2021,
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	url = {http://arxiv.org/abs/2111.06377},
	doi = {10.48550/arXiv.2111.06377},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
	urldate = {2022-12-22},
	publisher = {arXiv},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
	month = dec,
	year = {2021},
	note = {arXiv:2111.06377 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/H6F45JVP/He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/AWP2NLW7/2111.html:text/html},
}

@misc{he_momentum_2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1911.05722},
	doi = {10.48550/arXiv.1911.05722},
	abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	urldate = {2022-12-22},
	publisher = {arXiv},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	month = mar,
	year = {2020},
	note = {arXiv:1911.05722 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/R2IG6X8Y/He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/NY6UYHH9/1911.html:text/html},
}

@misc{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	doi = {10.48550/arXiv.2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2022-12-22},
	publisher = {arXiv},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv:2002.05709 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/WE36GVT4/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/STA3ZN6Y/2002.html:text/html},
}

@article{conrad_cem500k_2021,
	title = {{CEM500K}, a large-scale heterogeneous unlabeled cellular electron microscopy image dataset for deep learning},
	volume = {10},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.65894},
	doi = {10.7554/eLife.65894},
	abstract = {Automated segmentation of cellular electron microscopy (EM) datasets remains a challenge. Supervised deep learning (DL) methods that rely on region-of-interest (ROI) annotations yield models that fail to generalize to unrelated datasets. Newer unsupervised DL algorithms require relevant pre-training images, however, pre-training on currently available EM datasets is computationally expensive and shows little value for unseen biological contexts, as these datasets are large and homogeneous. To address this issue, we present CEM500K, a nimble 25 GB dataset of 0.5 × 106 unique 2D cellular EM images curated from nearly 600 three-dimensional (3D) and 10,000 two-dimensional (2D) images from {\textgreater}100 unrelated imaging projects. We show that models pre-trained on CEM500K learn features that are biologically relevant and resilient to meaningful image augmentations. Critically, we evaluate transfer learning from these pre-trained models on six publicly available and one newly derived benchmark segmentation task and report state-of-the-art results on each. We release the CEM500K dataset, pre-trained models and curation pipeline for model building and further expansion by the EM community. Data and code are available at https://www.ebi.ac.uk/pdbe/emdb/empiar/entry/10592/ and https://git.io/JLLTz.},
	urldate = {2022-12-22},
	journal = {eLife},
	author = {Conrad, Ryan and Narayan, Kedar},
	editor = {Grigorieff, Nikolaus and Akhmanova, Anna and Grigorieff, Nikolaus and Saalfeld, Stephan},
	month = apr,
	year = {2021},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {deep learning, electron microscopy, segmentation, image dataset, neural network, vEM},
	pages = {e65894},
	file = {Full Text PDF:/home/pape/Zotero/storage/HL9XNX6S/Conrad and Narayan - 2021 - CEM500K, a large-scale heterogeneous unlabeled cel.pdf:application/pdf},
}

@misc{zinchenko_morphofeatures_2022,
	title = {{MorphoFeatures}: unsupervised exploration of cell types, tissues and organs in volume electron microscopy},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{MorphoFeatures}},
	url = {https://www.biorxiv.org/content/10.1101/2022.05.07.490949v1},
	doi = {10.1101/2022.05.07.490949},
	abstract = {Electron microscopy (EM) provides a uniquely detailed view of cellular morphology, including organelles and fine subcellular ultrastructure. While the acquisition and (semi-)automatic segmentation of multicellular EM volumes is now becoming routine, large-scale analysis remains severely limited by the lack of generally applicable pipelines for automatic extraction of comprehensive morphological descriptors. Here, we present a novel unsupervised method for learning cellular morphology features directly from 3D EM data: a convolutional neural network delivers a representation of cells by shape and ultrastructure. Applied to the full volume of an entire three-segmented worm of the annelid Platynereis dumerilii, it yields a visually consistent grouping of cells supported by specific gene expression profiles. Integration of features across spatial neighbours can retrieve tissues and organs, revealing, for example, a detailed organization of the animal foregut. We envision that the unbiased nature of the proposed morphological descriptors will enable rapid exploration of very different biological questions in large EM volumes, greatly increasing the impact of these invaluable, but costly resources.},
	language = {en},
	urldate = {2022-12-22},
	publisher = {bioRxiv},
	author = {Zinchenko, Valentyna and Hugger, Johannes and Uhlmann, Virginie and Arendt, Detlev and Kreshuk, Anna},
	month = may,
	year = {2022},
	note = {Pages: 2022.05.07.490949
Section: New Results},
	file = {Full Text PDF:/home/pape/Zotero/storage/UIKQARJH/Zinchenko et al. - 2022 - MorphoFeatures unsupervised exploration of cell t.pdf:application/pdf},
}

@misc{dorkenwald_multi-layered_2022-1,
	title = {Multi-{Layered} {Maps} of {Neuropil} with {Segmentation}-{Guided} {Contrastive} {Learning}},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.03.29.486320v2},
	doi = {10.1101/2022.03.29.486320},
	abstract = {Maps of the nervous system that identify individual cells along with their type, subcellular components, and connectivity have the potential to reveal fundamental organizational principles of neural circuits. Volumetric nanometer-resolution imaging of brain tissue provides the raw data needed to build such maps, but inferring all the relevant cellular and subcellular annotation layers is challenging. Here, we present Segmentation-Guided Contrastive Learning of Representations (“SegCLR”), a self-supervised machine learning technique that produces highly informative representations of cells directly from 3d electron microscope imagery and segmentations. When applied to volumes of human and mouse cerebral cortex, SegCLR enabled the classification of cellular subcompartments (axon, dendrite, soma, astrocytic process) with 4,000-fold less labeled data compared to fully supervised approaches. Surprisingly, SegCLR also enabled inference of cell types (neurons, glia, and subtypes of each) from fragments with lengths as small as 10 micrometers, a task that can be difficult for humans to perform and whose feasibility greatly enhances the utility of imaging portions of brains in which many neuron fragments terminate at a volume boundary. These predictions were further augmented via Gaussian process uncertainty estimation to enable analyses restricted to high confidence subsets of the data. Finally, SegCLR enabled detailed exploration of layer-5 pyramidal cell subtypes and automated large-scale statistical analysis of upstream and downstream synaptic partners in mouse visual cortex.},
	language = {en},
	urldate = {2022-12-22},
	publisher = {bioRxiv},
	author = {Dorkenwald, Sven and Li, Peter H. and Januszewski, Michał and Berger, Daniel R. and Maitin-Shepard, Jeremy and Bodor, Agnes L. and Collman, Forrest and Schneider-Mizell, Casey M. and Costa, Nuno Maçarico da and Lichtman, Jeff W. and Jain, Viren},
	month = nov,
	year = {2022},
	note = {Pages: 2022.03.29.486320
Section: New Results},
	file = {Full Text PDF:/home/pape/Zotero/storage/XSX3JII3/Dorkenwald et al. - 2022 - Multi-Layered Maps of Neuropil with Segmentation-G.pdf:application/pdf},
}

@article{beier_multicut_2017,
	title = {Multicut brings automated neurite segmentation closer to human performance},
	volume = {14},
	copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.4151},
	doi = {10.1038/nmeth.4151},
	language = {en},
	number = {2},
	urldate = {2022-12-22},
	journal = {Nature Methods},
	author = {Beier, Thorsten and Pape, Constantin and Rahaman, Nasim and Prange, Timo and Berg, Stuart and Bock, Davi D. and Cardona, Albert and Knott, Graham W. and Plaza, Stephen M. and Scheffer, Louis K. and Koethe, Ullrich and Kreshuk, Anna and Hamprecht, Fred A.},
	month = feb,
	year = {2017},
	note = {Number: 2
Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning, Software, Neuroscience, Scanning electron microscopy},
	pages = {101--102},
	file = {Full Text PDF:/home/pape/Zotero/storage/WRI3BK5A/Beier et al. - 2017 - Multicut brings automated neurite segmentation clo.pdf:application/pdf},
}

@inproceedings{pape_solving_2017,
	title = {Solving {Large} {Multicut} {Problems} for {Connectomics} via {Domain} {Decomposition}},
	url = {https://openaccess.thecvf.com/content_ICCV_2017_workshops/w1/html/Pape_Solving_Large_Multicut_ICCV_2017_paper.html},
	urldate = {2022-12-22},
	author = {Pape, Constantin and Beier, Thorsten and Li, Peter and Jain, Viren and Bock, Davi D. and Kreshuk, Anna},
	year = {2017},
	pages = {1--10},
	file = {Full Text PDF:/home/pape/Zotero/storage/5S923DQM/Pape et al. - 2017 - Solving Large Multicut Problems for Connectomics v.pdf:application/pdf},
}

@article{pape_leveraging_2019,
	title = {Leveraging domain knowledge to improve microscopy image segmentation with lifted multicuts},
	journal = {Frontiers in Computer Science},
	author = {Pape, Constantin and Matskevych, Alex and Wolny, Adrian and Hennies, Julian and Mizzon, Giulia and Louveaux, Marion and Musser, Jacob and Maizel, Alexis and Arendt, Detlev and Kreshuk, Anna},
	year = {2019},
	note = {Publisher: Frontiers},
	pages = {6},
}

@article{musser_profiling_2021,
	title = {Profiling cellular diversity in sponges informs animal cell type and nervous system evolution},
	volume = {374},
	url = {https://www.science.org/doi/10.1126/science.abj2949},
	doi = {10.1126/science.abj2949},
	abstract = {The evolutionary origin of metazoan cell types such as neurons and muscles is not known. Using whole-body single-cell RNA sequencing in a sponge, an animal without nervous system and musculature, we identified 18 distinct cell types. These include nitric oxide–sensitive contractile pinacocytes, amoeboid phagocytes, and secretory neuroid cells that reside in close contact with digestive choanocytes that express scaffolding and receptor proteins. Visualizing neuroid cells by correlative x-ray and electron microscopy revealed secretory vesicles and cellular projections enwrapping choanocyte microvilli and cilia. Our data show a communication system that is organized around sponge digestive chambers, using conserved modules that became incorporated into the pre- and postsynapse in the nervous systems of other animals.},
	number = {6568},
	urldate = {2022-12-22},
	journal = {Science},
	author = {Musser, Jacob M. and Schippers, Klaske J. and Nickel, Michael and Mizzon, Giulia and Kohn, Andrea B. and Pape, Constantin and Ronchi, Paolo and Papadopoulos, Nikolaos and Tarashansky, Alexander J. and Hammel, Jörg U. and Wolf, Florian and Liang, Cong and Hernández-Plaza, Ana and Cantalapiedra, Carlos P. and Achim, Kaia and Schieber, Nicole L. and Pan, Leslie and Ruperti, Fabian and Francis, Warren R. and Vargas, Sergio and Kling, Svenja and Renkert, Maike and Polikarpov, Maxim and Bourenkov, Gleb and Feuda, Roberto and Gaspar, Imre and Burkhardt, Pawel and Wang, Bo and Bork, Peer and Beck, Martin and Schneider, Thomas R. and Kreshuk, Anna and Wörheide, Gert and Huerta-Cepas, Jaime and Schwab, Yannick and Moroz, Leonid L. and Arendt, Detlev},
	month = nov,
	year = {2021},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {717--723},
	file = {Full Text PDF:/home/pape/Zotero/storage/R8VGNQU7/Musser et al. - 2021 - Profiling cellular diversity in sponges informs an.pdf:application/pdf},
}

@inproceedings{wolny_sparse_2022,
	title = {Sparse {Object}-{Level} {Supervision} for {Instance} {Segmentation} {With} {Pixel} {Embeddings}},
	url = {},
	language = {en},
	urldate = {2022-12-22},
	author = {Wolny, Adrian and Yu, Qin and Pape, Constantin and Kreshuk, Anna},
	year = {2022},
	pages = {4402--4411},
	file = {Full Text PDF:/home/pape/Zotero/storage/3FII48N5/Wolny et al. - 2022 - Sparse Object-Level Supervision for Instance Segme.pdf:application/pdf},
}

@misc{ouyang_bioimage_2022,
	title = {{BioImage} {Model} {Zoo}: {A} {Community}-{Driven} {Resource} for {Accessible} {Deep} {Learning} in {BioImage} {Analysis}},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	shorttitle = {{BioImage} {Model} {Zoo}},
	url = {https://www.biorxiv.org/content/10.1101/2022.06.07.495102v1},
	doi = {10.1101/2022.06.07.495102},
	abstract = {Deep learning-based approaches are revolutionizing imaging-driven scientific research. However, the accessibility and reproducibility of deep learning-based workflows for imaging scientists remain far from sufficient. Several tools have recently risen to the challenge of democratizing deep learning by providing user-friendly interfaces to analyze new data with pre-trained or fine-tuned models. Still, few of the existing pre-trained models are interoperable between these tools, critically restricting a model’s overall utility and the possibility of validating and reproducing scientific analyses. Here, we present the BioImage Model Zoo (https://bioimage.io): a community-driven, fully open resource where standardized pre-trained models can be shared, explored, tested, and downloaded for further adaptation or direct deployment in multiple end user-facing tools (e.g., ilastik, deepImageJ, QuPath, StarDist, ImJoy, ZeroCostDL4Mic, CSBDeep). To enable everyone to contribute and consume the Zoo resources, we provide a model standard to enable cross-compatibility, a rich list of example models and practical use-cases, developer tools, documentation, and the accompanying infrastructure for model upload, download and testing. Our contribution aims to lay the groundwork to make deep learning methods for microscopy imaging findable, accessible, interoperable, and reusable (FAIR) across software tools and platforms.},
	language = {en},
	urldate = {2022-12-22},
	publisher = {bioRxiv},
	author = {Ouyang, Wei and Beuttenmueller, Fynn and Gómez-de-Mariscal, Estibaliz and Pape, Constantin and Burke, Tom and Garcia-López-de-Haro, Carlos and Russell, Craig and Moya-Sans, Lucía and de-la-Torre-Gutiérrez, Cristina and Schmidt, Deborah and Kutra, Dominik and Novikov, Maksim and Weigert, Martin and Schmidt, Uwe and Bankhead, Peter and Jacquemet, Guillaume and Sage, Daniel and Henriques, Ricardo and Muñoz-Barrutia, Arrate and Lundberg, Emma and Jug, Florian and Kreshuk, Anna},
	month = jun,
	year = {2022},
	note = {Pages: 2022.06.07.495102
Section: New Results},
	file = {Full Text PDF:/home/pape/Zotero/storage/B2EKTDTK/Ouyang et al. - 2022 - BioImage Model Zoo A Community-Driven Resource fo.pdf:application/pdf},
}

@article{martinez-sanchez_template-free_2020,
	title = {Template-free detection and classification of membrane-bound complexes in cryo-electron tomograms},
	volume = {17},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-019-0675-5},
	doi = {10.1038/s41592-019-0675-5},
	abstract = {With faithful sample preservation and direct imaging of fully hydrated biological material, cryo-electron tomography provides an accurate representation of molecular architecture of cells. However, detection and precise localization of macromolecular complexes within cellular environments is aggravated by the presence of many molecular species and molecular crowding. We developed a template-free image processing procedure for accurate tracing of complex networks of densities in cryo-electron tomograms, a comprehensive and automated detection of heterogeneous membrane-bound complexes and an unsupervised classification (PySeg). Applications to intact cells and isolated endoplasmic reticulum (ER) allowed us to detect and classify small protein complexes. This classification provided sufficiently homogeneous particle sets and initial references to allow subsequent de novo subtomogram averaging. Spatial distribution analysis showed that ER complexes have different localization patterns forming nanodomains. Therefore, this procedure allows a comprehensive detection and structural analysis of complexes in situ.},
	language = {en},
	number = {2},
	urldate = {2022-12-22},
	journal = {Nature Methods},
	author = {Martinez-Sanchez, Antonio and Kochovski, Zdravko and Laugks, Ulrike and Meyer zum Alten Borgloh, Johannes and Chakraborty, Saikat and Pfeffer, Stefan and Baumeister, Wolfgang and Lučić, Vladan},
	month = feb,
	year = {2020},
	note = {Number: 2
Publisher: Nature Publishing Group},
	keywords = {Image processing, Software, Cryoelectron tomography, Membrane proteins},
	pages = {209--216},
	file = {Full Text PDF:/home/pape/Zotero/storage/K5T48AUG/Martinez-Sanchez et al. - 2020 - Template-free detection and classification of memb.pdf:application/pdf},
}

@inproceedings{campello_density-based_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Density-{Based} {Clustering} {Based} on {Hierarchical} {Density} {Estimates}},
	isbn = {978-3-642-37456-2},
	doi = {10.1007/978-3-642-37456-2_14},
	abstract = {We propose a theoretically and practically improved density-based, hierarchical clustering method, providing a clustering hierarchy from which a simplified tree of significant clusters can be constructed. For obtaining a “flat” partition consisting of only the most significant clusters (possibly corresponding to different density thresholds), we propose a novel cluster stability measure, formalize the problem of maximizing the overall stability of selected clusters, and formulate an algorithm that computes an optimal solution to this problem. We demonstrate that our approach outperforms the current, state-of-the-art, density-based clustering methods on a wide variety of real world data.},
	language = {en},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer},
	author = {Campello, Ricardo J. G. B. and Moulavi, Davoud and Sander, Joerg},
	editor = {Pei, Jian and Tseng, Vincent S. and Cao, Longbing and Motoda, Hiroshi and Xu, Guandong},
	year = {2013},
	keywords = {Cluster Tree, Core Object, Density Threshold, Hierarchical Cluster Method, Minimum Span Tree},
	pages = {160--172},
}

@article{jumper_highly_2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2022-12-22},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Number: 7873
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Computational biophysics, Protein structure predictions, Structural biology},
	pages = {583--589},
	file = {Full Text PDF:/home/pape/Zotero/storage/HHD4JQBQ/Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf:application/pdf},
}

@misc{baevski_efficient_2022,
	title = {Efficient {Self}-supervised {Learning} with {Contextualized} {Target} {Representations} for {Vision}, {Speech} and {Language}},
	url = {http://arxiv.org/abs/2212.07525},
	doi = {10.48550/arXiv.2212.07525},
	abstract = {Current self-supervised learning algorithms are often modality-specific and require large amounts of computational resources. To address these issues, we increase the training efficiency of data2vec, a learning objective that generalizes across several modalities. We do not encode masked tokens, use a fast convolutional decoder and amortize the effort to build teacher representations. data2vec 2.0 benefits from the rich contextualized target representations introduced in data2vec which enable a fast self-supervised learner. Experiments on ImageNet-1K image classification show that data2vec 2.0 matches the accuracy of Masked Autoencoders in 16.4x lower pre-training time, on Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x less time, and on GLUE natural language understanding it matches a retrained RoBERTa model in half the time. Trading some speed for accuracy results in ImageNet-1K top-1 accuracy of 86.8{\textbackslash}\% with a ViT-L model trained for 150 epochs.},
	urldate = {2022-12-22},
	publisher = {arXiv},
	author = {Baevski, Alexei and Babu, Arun and Hsu, Wei-Ning and Auli, Michael},
	month = dec,
	year = {2022},
	note = {arXiv:2212.07525 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/8C3C8JUZ/Baevski et al. - 2022 - Efficient Self-supervised Learning with Contextual.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/EEUIIDSP/2212.html:text/html},
}

@inproceedings{lee_pseudo-label_2013,
	title = {Pseudo-{Label} : {The} {Simple} and {Efficient} {Semi}-{Supervised} {Learning} {Method} for {Deep} {Neural} {Networks}},
	shorttitle = {Pseudo-{Label}},
	url = {https://www.semanticscholar.org/paper/Pseudo-Label-%3A-The-Simple-and-Efficient-Learning-Lee/798d9840d2439a0e5d47bcf5d164aa46d5e7dc26},
	abstract = {We propose the simple and efficient method of semi-supervised learning for deep neural networks. Basically, the proposed network is trained in a supervised fashion with labeled and unlabeled data simultaneously. For unlabeled data, Pseudo-Labels, just picking up the class which has the maximum predicted probability, are used as if they were true labels. This is in effect equivalent to Entropy Regularization. It favors a low-density separation between classes, a commonly assumed prior for semi-supervised learning. With Denoising Auto-Encoder and Dropout, this simple method outperforms conventional methods for semi-supervised learning with very small labeled data on the MNIST handwritten digit dataset.},
	urldate = {2022-12-24},
	author = {Lee, Dong-Hyun},
	year = {2013},
}

@misc{hassani_dilated_2022,
	title = {Dilated {Neighborhood} {Attention} {Transformer}},
	url = {http://arxiv.org/abs/2209.15001},
	doi = {10.48550/arXiv.2209.15001},
	abstract = {Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.5\% box AP in COCO object detection, 1.3\% mask AP in COCO instance segmentation, and 1.1\% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.2 PQ) and ADE20K (48.5 PQ), and instance segmentation model on Cityscapes (44.5 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.2 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data). We open-source our project.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Hassani, Ali and Shi, Humphrey},
	month = nov,
	year = {2022},
	note = {arXiv:2209.15001 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/69SX5Q2L/Hassani and Shi - 2022 - Dilated Neighborhood Attention Transformer.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/TQYGM8ES/2209.html:text/html},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/ENFFJK7H/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/INNBLNRP/2010.html:text/html},
}

@misc{liu_swin_2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	shorttitle = {Swin {Transformer}},
	url = {http://arxiv.org/abs/2103.14030},
	doi = {10.48550/arXiv.2103.14030},
	abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	month = aug,
	year = {2021},
	note = {arXiv:2103.14030 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/LRFSCDMN/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/UJRBVUM7/2103.html:text/html},
}

@misc{hassani_neighborhood_2022,
	title = {Neighborhood {Attention} {Transformer}},
	url = {http://arxiv.org/abs/2204.07143},
	doi = {10.48550/arXiv.2204.07143},
	abstract = {We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40\% faster than Swin's WSA while using up to 25\% less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2\% top-1 accuracy on ImageNet, 51.4\% mAP on MS-COCO and 48.4\% mIoU on ADE20K, which is 1.9\% ImageNet accuracy, 1.0\% COCO mAP, and 2.6\% ADE20K mIoU improvement over a Swin model with similar size. To support more research based on sliding-window attention, we open source our project and release our checkpoints at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer .},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Hassani, Ali and Walton, Steven and Li, Jiachen and Li, Shen and Shi, Humphrey},
	month = nov,
	year = {2022},
	note = {arXiv:2204.07143 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/APBRDBYF/Hassani et al. - 2022 - Neighborhood Attention Transformer.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/HH59FAMN/2204.html:text/html},
}

@article{xie_microscopy_2018,
	title = {Microscopy cell counting and detection with fully convolutional regression networks},
	volume = {6},
	issn = {2168-1163},
	url = {https://doi.org/10.1080/21681163.2016.1149104},
	doi = {10.1080/21681163.2016.1149104},
	abstract = {This paper concerns automated cell counting and detection in microscopy images. The approach we take is to use convolutional neural networks (CNNs) to regress a cell spatial density map across the image. This is applicable to situations where traditional single-cell segmentation-based methods do not work well due to cell clumping or overlaps. We make the following contributions: (i) we develop and compare architectures for two fully convolutional regression networks (FCRNs) for this task; (ii) since the networks are fully convolutional, they can predict a density map for an input image of arbitrary size, and we exploit this to improve efficiency by end-to-end training on image patches; (iii) we show that FCRNs trained entirely on synthetic data are able to give excellent predictions on microscopy images from real biological experiments without fine-tuning, and that the performance can be further improved by fine-tuning on these real images. Finally, (iv) by inverting feature representations, we show to what extent the information from an input image has been encoded by feature responses in different layers. We set a new state-of-the-art performance for cell counting on standard synthetic image benchmarks and show that the FCRNs trained entirely with synthetic data can generalise well to real microscopy images both for cell counting and detections for the case of overlapping cells.},
	number = {3},
	urldate = {2022-12-24},
	journal = {Computer Methods in Biomechanics and Biomedical Engineering: Imaging \& Visualization},
	author = {Xie, Weidi and Noble, J. Alison and Zisserman, Andrew},
	month = may,
	year = {2018},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/21681163.2016.1149104},
	keywords = {cell counting, cell detection, fully convolutional regression networks, inverting feature representations, Microscopy image analysis},
	pages = {283--292},
	file = {Submitted Version:/home/pape/Zotero/storage/SAHCUHT7/Xie et al. - 2018 - Microscopy cell counting and detection with fully .pdf:application/pdf},
}

@misc{tao_hierarchical_2020,
	title = {Hierarchical {Multi}-{Scale} {Attention} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/2005.10821},
	doi = {10.48550/arXiv.2005.10821},
	abstract = {Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling. In this work, we present an attention-based approach to combining multi-scale predictions. We show that predictions at certain scales are better at resolving particular failures modes, and that the network learns to favor those scales for such cases in order to generate better predictions. Our attention mechanism is hierarchical, which enables it to be roughly 4x more memory efficient to train than other recent approaches. In addition to enabling faster training, this allows us to train with larger crop sizes which leads to greater model accuracy. We demonstrate the result of our method on two datasets: Cityscapes and Mapillary Vistas. For Cityscapes, which has a large number of weakly labelled images, we also leverage auto-labelling to improve generalization. Using our approach we achieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and Cityscapes (85.1 IOU test).},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Tao, Andrew and Sapra, Karan and Catanzaro, Bryan},
	month = may,
	year = {2020},
	note = {arXiv:2005.10821 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/N35ANNYH/Tao et al. - 2020 - Hierarchical Multi-Scale Attention for Semantic Se.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/A4ZFPX84/2005.html:text/html},
}

@misc{conrad_instance_2022,
	title = {Instance segmentation of mitochondria in electron microscopy images with a generalist deep learning model},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.03.17.484806v2},
	doi = {10.1101/2022.03.17.484806},
	abstract = {Mitochondria are extremely pleomorphic organelles. Automatically annotating each one accurately and precisely in any 2D or volume electron microscopy (EM) image is an unsolved computational challenge. Current deep learning-based approaches train models on images that provide limited cellular contexts, precluding generality. To address this, we amassed a highly heterogeneous ∼1.5 x 106 image 2D unlabeled cellular EM dataset, and segmented ∼135,000 mitochondrial instances therein. MitoNet, a model trained on these resources, performs well on challenging benchmarks and on previously unseen volume EM datasets containing tens of thousands of mitochondria. We release a new Python package and napari plugin, empanada, to rapidly run inference, visualize, and proofread instance segmentations.},
	language = {en},
	urldate = {2022-12-24},
	publisher = {bioRxiv},
	author = {Conrad, Ryan and Narayan, Kedar},
	month = may,
	year = {2022},
	note = {Pages: 2022.03.17.484806
Section: New Results},
	file = {Full Text PDF:/home/pape/Zotero/storage/VR5YHJUJ/Conrad and Narayan - 2022 - Instance segmentation of mitochondria in electron .pdf:application/pdf},
}

@article{malin-mayor_automated_2022,
	title = {Automated reconstruction of whole-embryo cell lineages by learning from sparse annotations},
	copyright = {2022 The Author(s)},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-022-01427-7},
	doi = {10.1038/s41587-022-01427-7},
	abstract = {We present a method to automatically identify and track nuclei in time-lapse microscopy recordings of entire developing embryos. The method combines deep learning and global optimization. On a mouse dataset, it reconstructs 75.8\% of cell lineages spanning 1 h, as compared to 31.8\% for the competing method. Our approach improves understanding of where and when cell fate decisions are made in developing embryos, tissues, and organs.},
	language = {en},
	urldate = {2022-12-24},
	journal = {Nature Biotechnology},
	author = {Malin-Mayor, Caroline and Hirsch, Peter and Guignard, Leo and McDole, Katie and Wan, Yinan and Lemon, William C. and Kainmueller, Dagmar and Keller, Philipp J. and Preibisch, Stephan and Funke, Jan},
	month = sep,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning, Cell lineage},
	pages = {1--6},
	file = {Full Text PDF:/home/pape/Zotero/storage/G2IDC9Z6/Malin-Mayor et al. - 2022 - Automated reconstruction of whole-embryo cell line.pdf:application/pdf},
}

@article{steyer_pathology_2020,
	title = {Pathology of myelinated axons in the {PLP}-deficient mouse model of spastic paraplegia type 2 revealed by volume imaging using focused ion beam-scanning electron microscopy},
	volume = {210},
	issn = {1047-8477},
	url = {https://www.sciencedirect.com/science/article/pii/S1047847720300587},
	doi = {10.1016/j.jsb.2020.107492},
	abstract = {Advances in electron microscopy including improved imaging techniques and state-of-the-art detectors facilitate imaging of larger tissue volumes with electron microscopic resolution. In combination with genetic tools for the generation of mouse mutants this allows assessing the three-dimensional (3D) characteristics of pathological features in disease models. Here we revisited the axonal pathology in the central nervous system of a mouse model of spastic paraplegia type 2, the Plp−/Y mouse. Although PLP is a bona fide myelin protein, the major hallmark of the disease in both SPG2 patients and mouse models are axonal swellings comprising accumulations of numerous organelles including mitochondria, gradually leading to irreversible axonal loss. To assess the number and morphology of axonal mitochondria and the overall myelin preservation we evaluated two sample preparation techniques, chemical fixation or high-pressure freezing and freeze substitution, with respect to the objective of 3D visualization. Both methods allowed visualizing distribution and morphological details of axonal mitochondria. In Plp−/Y mice the number of mitochondria is 2-fold increased along the entire axonal length. Mitochondria are also found in the excessive organelle accumulations within axonal swellings. In addition, organelle accumulations were detected within the myelin sheath and the inner tongue. We find that 3D electron microscopy is required for a comprehensive understanding of the size, content and frequency of axonal swellings, the hallmarks of axonal pathology.},
	language = {en},
	number = {2},
	urldate = {2022-12-24},
	journal = {Journal of Structural Biology},
	author = {Steyer, Anna M. and Ruhwedel, Torben and Nardis, Christos and Werner, Hauke B. and Nave, Klaus-Armin and Möbius, Wiebke},
	month = may,
	year = {2020},
	keywords = {High-pressure freezing, mouse, Axonal swellings, Central nervous system (CNS), Focused Ion Beam-Scanning Electron Microscopy (FIB-SEM), Myelin, Spastic paraplegia type 2 (SPG2)},
	pages = {107492},
	file = {ScienceDirect Full Text PDF:/home/pape/Zotero/storage/LCYR6GDE/Steyer et al. - 2020 - Pathology of myelinated axons in the PLP-deficient.pdf:application/pdf},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/FEDEDZVT/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/VKAA97NW/1412.html:text/html},
}

@misc{casser_fast_2020,
	title = {Fast {Mitochondria} {Detection} for {Connectomics}},
	url = {http://arxiv.org/abs/1812.06024},
	doi = {10.48550/arXiv.1812.06024},
	abstract = {High-resolution connectomics data allows for the identification of dysfunctional mitochondria which are linked to a variety of diseases such as autism or bipolar. However, manual analysis is not feasible since datasets can be petabytes in size. We present a fully automatic mitochondria detector based on a modified U-Net architecture that yields high accuracy and fast processing times. We evaluate our method on multiple real-world connectomics datasets, including an improved version of the EPFL mitochondria benchmark. Our results show an Jaccard index of up to 0.90 with inference times lower than 16ms for a 512x512px image tile. This speed is faster than the acquisition speed of modern electron microscopes, enabling mitochondria detection in real-time. Our detector ranks first for real-time detection when compared to previous works and data, results, and code are openly available.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Casser, Vincent and Kang, Kai and Pfister, Hanspeter and Haehn, Daniel},
	month = jun,
	year = {2020},
	note = {arXiv:1812.06024 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/B5EK9NS5/Casser et al. - 2020 - Fast Mitochondria Detection for Connectomics.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/W2LQ2RYG/1812.html:text/html},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/PTZTHXSX/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/F8BBY5VM/1312.html:text/html},
}

@misc{doersch_unsupervised_2016,
	title = {Unsupervised {Visual} {Representation} {Learning} by {Context} {Prediction}},
	url = {http://arxiv.org/abs/1505.05192},
	doi = {10.48550/arXiv.1505.05192},
	abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
	month = jan,
	year = {2016},
	note = {arXiv:1505.05192 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/KNJY3VS9/Doersch et al. - 2016 - Unsupervised Visual Representation Learning by Con.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/ADG9GRFN/1505.html:text/html},
}

@misc{henaff_data-efficient_2020,
	title = {Data-{Efficient} {Image} {Recognition} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1905.09272},
	doi = {10.48550/arXiv.1905.09272},
	abstract = {Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Hénaff, Olivier J. and Srinivas, Aravind and De Fauw, Jeffrey and Razavi, Ali and Doersch, Carl and Eslami, S. M. Ali and Oord, Aaron van den},
	month = jul,
	year = {2020},
	note = {arXiv:1905.09272 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/8RBIAVQX/Hénaff et al. - 2020 - Data-Efficient Image Recognition with Contrastive .pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/E5QK9M7E/1905.html:text/html},
}

@misc{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	doi = {10.48550/arXiv.1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv:1807.03748 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/CW8Z57X6/Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/8V36ARDR/1807.html:text/html},
}

@misc{xie_self-training_2020,
	title = {Self-training with {Noisy} {Student} improves {ImageNet} classification},
	url = {http://arxiv.org/abs/1911.04252},
	doi = {10.48550/arXiv.1911.04252},
	abstract = {We present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4\% top-1 accuracy on ImageNet, which is 2.0\% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0\% to 83.7\%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. Noisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet. Code is available at https://github.com/google-research/noisystudent.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
	month = jun,
	year = {2020},
	note = {arXiv:1911.04252 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/U5IIZ6FY/Xie et al. - 2020 - Self-training with Noisy Student improves ImageNet.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/RUKQV7YD/1911.html:text/html},
}

@misc{bao_classification_2018,
	title = {Classification from {Pairwise} {Similarity} and {Unlabeled} {Data}},
	url = {http://arxiv.org/abs/1802.04381},
	doi = {10.48550/arXiv.1802.04381},
	abstract = {Supervised learning needs a huge amount of labeled data, which can be a big bottleneck under the situation where there is a privacy concern or labeling cost is high. To overcome this problem, we propose a new weakly-supervised learning setting where only similar (S) data pairs (two examples belong to the same class) and unlabeled (U) data points are needed instead of fully labeled data, which is called SU classification. We show that an unbiased estimator of the classification risk can be obtained only from SU data, and the estimation error of its empirical risk minimizer achieves the optimal parametric convergence rate. Finally, we demonstrate the effectiveness of the proposed method through experiments.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Bao, Han and Niu, Gang and Sugiyama, Masashi},
	month = aug,
	year = {2018},
	note = {arXiv:1802.04381 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/3Q37ZNLB/Bao et al. - 2018 - Classification from Pairwise Similarity and Unlabe.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/R6IURK7S/1802.html:text/html},
}

@misc{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2105.05233},
	doi = {10.48550/arXiv.2105.05233},
	abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\${\textbackslash}times\$128, 4.59 on ImageNet 256\${\textbackslash}times\$256, and 7.72 on ImageNet 512\${\textbackslash}times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\${\textbackslash}times\$256 and 3.85 on ImageNet 512\${\textbackslash}times\$512. We release our code at https://github.com/openai/guided-diffusion},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	month = jun,
	year = {2021},
	note = {arXiv:2105.05233 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/A399BVPU/Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/JSVJ8S2G/2105.html:text/html},
}

@misc{oord_pixel_2016,
	title = {Pixel {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1601.06759},
	doi = {10.48550/arXiv.1601.06759},
	abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Kalchbrenner, Nal and Kavukcuoglu, Koray},
	month = aug,
	year = {2016},
	note = {arXiv:1601.06759 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/7VFGYA39/Oord et al. - 2016 - Pixel Recurrent Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/D97JSA7D/1601.html:text/html},
}

@misc{lehtinen_noise2noise_2018,
	title = {{Noise2Noise}: {Learning} {Image} {Restoration} without {Clean} {Data}},
	shorttitle = {{Noise2Noise}},
	url = {http://arxiv.org/abs/1803.04189},
	doi = {10.48550/arXiv.1803.04189},
	abstract = {We apply basic statistical reasoning to signal reconstruction by machine learning -- learning to map corrupted observations to clean signals -- with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans -- all corrupted by different processes -- based on noisy data only.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Lehtinen, Jaakko and Munkberg, Jacob and Hasselgren, Jon and Laine, Samuli and Karras, Tero and Aittala, Miika and Aila, Timo},
	month = oct,
	year = {2018},
	note = {arXiv:1803.04189 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/9H25E4VC/Lehtinen et al. - 2018 - Noise2Noise Learning Image Restoration without Cl.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/2A7GARZT/1803.html:text/html},
}

@article{weigert_content-aware_2018,
	title = {Content-aware image restoration: pushing the limits of fluorescence microscopy},
	volume = {15},
	copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {Content-aware image restoration},
	url = {https://www.nature.com/articles/s41592-018-0216-7},
	doi = {10.1038/s41592-018-0216-7},
	abstract = {Fluorescence microscopy is a key driver of discoveries in the life sciences, with observable phenomena being limited by the optics of the microscope, the chemistry of the fluorophores, and the maximum photon exposure tolerated by the sample. These limits necessitate trade-offs between imaging speed, spatial resolution, light exposure, and imaging depth. In this work we show how content-aware image restoration based on deep learning extends the range of biological phenomena observable by microscopy. We demonstrate on eight concrete examples how microscopy images can be restored even if 60-fold fewer photons are used during acquisition, how near isotropic resolution can be achieved with up to tenfold under-sampling along the axial direction, and how tubular and granular structures smaller than the diffraction limit can be resolved at 20-times-higher frame rates compared to state-of-the-art methods. All developed image restoration methods are freely available as open source software in Python, FIJI, and KNIME.},
	language = {en},
	number = {12},
	urldate = {2022-12-24},
	journal = {Nature Methods},
	author = {Weigert, Martin and Schmidt, Uwe and Boothe, Tobias and Müller, Andreas and Dibrov, Alexandr and Jain, Akanksha and Wilhelm, Benjamin and Schmidt, Deborah and Broaddus, Coleman and Culley, Siân and Rocha-Martins, Mauricio and Segovia-Miranda, Fabián and Norden, Caren and Henriques, Ricardo and Zerial, Marino and Solimena, Michele and Rink, Jochen and Tomancak, Pavel and Royer, Loic and Jug, Florian and Myers, Eugene W.},
	month = dec,
	year = {2018},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning, Microscopy, Software},
	pages = {1090--1097},
	file = {Full Text PDF:/home/pape/Zotero/storage/U9YHS72K/Weigert et al. - 2018 - Content-aware image restoration pushing the limit.pdf:application/pdf},
}

@article{ershov_trackmate_2022,
	title = {{TrackMate} 7: integrating state-of-the-art segmentation algorithms into tracking pipelines},
	volume = {19},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {{TrackMate} 7},
	url = {https://www.nature.com/articles/s41592-022-01507-1},
	doi = {10.1038/s41592-022-01507-1},
	abstract = {TrackMate is an automated tracking software used to analyze bioimages and is distributed as a Fiji plugin. Here, we introduce a new version of TrackMate. TrackMate 7 is built to address the broad spectrum of modern challenges researchers face by integrating state-of-the-art segmentation algorithms into tracking pipelines. We illustrate qualitatively and quantitatively that these new capabilities function effectively across a wide range of bio-imaging experiments.},
	language = {en},
	number = {7},
	urldate = {2022-12-24},
	journal = {Nature Methods},
	author = {Ershov, Dmitry and Phan, Minh-Son and Pylvänäinen, Joanna W. and Rigaud, Stéphane U. and Le Blanc, Laure and Charles-Orszag, Arthur and Conway, James R. W. and Laine, Romain F. and Roy, Nathan H. and Bonazzi, Daria and Duménil, Guillaume and Jacquemet, Guillaume and Tinevez, Jean-Yves},
	month = jul,
	year = {2022},
	note = {Number: 7
Publisher: Nature Publishing Group},
	keywords = {Image processing, Software},
	pages = {829--832},
	file = {Full Text PDF:/home/pape/Zotero/storage/MYZXVV76/Ershov et al. - 2022 - TrackMate 7 integrating state-of-the-art segmenta.pdf:application/pdf},
}

@article{pachitariu_cellpose_2022,
	title = {Cellpose 2.0: how to train your own model},
	volume = {19},
	copyright = {2022 The Author(s)},
	issn = {1548-7105},
	shorttitle = {Cellpose 2.0},
	url = {https://www.nature.com/articles/s41592-022-01663-4},
	doi = {10.1038/s41592-022-01663-4},
	abstract = {Pretrained neural network models for biological segmentation can provide good out-of-the-box results for many image types. However, such models do not allow users to adapt the segmentation style to their specific needs and can perform suboptimally for test images that are very different from the training images. Here we introduce Cellpose 2.0, a new package that includes an ensemble of diverse pretrained models as well as a human-in-the-loop pipeline for rapid prototyping of new custom models. We show that models pretrained on the Cellpose dataset can be fine-tuned with only 500–1,000 user-annotated regions of interest (ROI) to perform nearly as well as models trained on entire datasets with up to 200,000 ROI. A human-in-the-loop approach further reduced the required user annotation to 100–200 ROI, while maintaining high-quality segmentations. We provide software tools such as an annotation graphical user interface, a model zoo and a human-in-the-loop pipeline to facilitate the adoption of Cellpose 2.0.},
	language = {en},
	number = {12},
	urldate = {2022-12-24},
	journal = {Nature Methods},
	author = {Pachitariu, Marius and Stringer, Carsen},
	month = dec,
	year = {2022},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Image processing, Computational platforms and environments},
	pages = {1634--1641},
	file = {Full Text PDF:/home/pape/Zotero/storage/9WK7F7E9/Pachitariu and Stringer - 2022 - Cellpose 2.0 how to train your own model.pdf:application/pdf},
}

@misc{long_fully_2015,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1411.4038},
	doi = {10.48550/arXiv.1411.4038},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	month = mar,
	year = {2015},
	note = {arXiv:1411.4038 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/EXS5UGLA/Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/AXLF2T4P/1411.html:text/html},
}

@misc{lin_microsoft_2015,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	doi = {10.48550/arXiv.1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = feb,
	year = {2015},
	note = {arXiv:1405.0312 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/P5BML5VQ/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/ADIGEWHP/1405.html:text/html},
}

@misc{lin_microsoft_2015-1,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	doi = {10.48550/arXiv.1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = feb,
	year = {2015},
	note = {arXiv:1405.0312 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/NWZBZTE8/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/L2MSCDAK/1405.html:text/html},
}

@misc{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/YMKGBV6T/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/Z5YUP4I2/1512.html:text/html},
}

@misc{yun_cutmix_2019,
	title = {{CutMix}: {Regularization} {Strategy} to {Train} {Strong} {Classifiers} with {Localizable} {Features}},
	shorttitle = {{CutMix}},
	url = {http://arxiv.org/abs/1905.04899},
	doi = {10.48550/arXiv.1905.04899},
	abstract = {Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefficiency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch .},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
	month = aug,
	year = {2019},
	note = {arXiv:1905.04899 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/X6JRGE42/Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Cl.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/V9HEBLRC/1905.html:text/html},
}

@misc{zhang_mixup_2018,
	title = {mixup: {Beyond} {Empirical} {Risk} {Minimization}},
	shorttitle = {mixup},
	url = {http://arxiv.org/abs/1710.09412},
	doi = {10.48550/arXiv.1710.09412},
	abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
	urldate = {2022-12-24},
	publisher = {arXiv},
	author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
	month = apr,
	year = {2018},
	note = {arXiv:1710.09412 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/U7VT4CI2/Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/6HQ84GHN/1710.html:text/html},
}

@misc{guo_deep_2020,
	title = {Deep {Learning} for {3D} {Point} {Clouds}: {A} {Survey}},
	shorttitle = {Deep {Learning} for {3D} {Point} {Clouds}},
	url = {http://arxiv.org/abs/1912.12033},
	doi = {10.48550/arXiv.1912.12033},
	abstract = {Point cloud learning has lately attracted increasing attention due to its wide applications in many areas, such as computer vision, autonomous driving, and robotics. As a dominating technique in AI, deep learning has been successfully used to solve various 2D vision problems. However, deep learning on point clouds is still in its infancy due to the unique challenges faced by the processing of point clouds with deep neural networks. Recently, deep learning on point clouds has become even thriving, with numerous methods being proposed to address different problems in this area. To stimulate future research, this paper presents a comprehensive review of recent progress in deep learning methods for point clouds. It covers three major tasks, including 3D shape classification, 3D object detection and tracking, and 3D point cloud segmentation. It also presents comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions.},
	urldate = {2022-12-28},
	publisher = {arXiv},
	author = {Guo, Yulan and Wang, Hanyun and Hu, Qingyong and Liu, Hao and Liu, Li and Bennamoun, Mohammed},
	month = jun,
	year = {2020},
	note = {arXiv:1912.12033 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/KV3VJG4I/Guo et al. - 2020 - Deep Learning for 3D Point Clouds A Survey.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/S3JF6I9V/1912.html:text/html},
}

@misc{qi_pointnet_2017,
	title = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	shorttitle = {{PointNet}},
	url = {http://arxiv.org/abs/1612.00593},
	doi = {10.48550/arXiv.1612.00593},
	abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	urldate = {2022-12-28},
	publisher = {arXiv},
	author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
	month = apr,
	year = {2017},
	note = {arXiv:1612.00593 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/3JJY9W2C/Qi et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/ACCTMY4W/1612.html:text/html},
}

@misc{qi_pointnet_2017-1,
	title = {{PointNet}++: {Deep} {Hierarchical} {Feature} {Learning} on {Point} {Sets} in a {Metric} {Space}},
	shorttitle = {{PointNet}++},
	url = {http://arxiv.org/abs/1706.02413},
	doi = {10.48550/arXiv.1706.02413},
	abstract = {Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
	urldate = {2022-12-28},
	publisher = {arXiv},
	author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
	month = jun,
	year = {2017},
	note = {arXiv:1706.02413 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/L3DL8J8X/Qi et al. - 2017 - PointNet++ Deep Hierarchical Feature Learning on .pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/QG9RGC6J/1706.html:text/html},
}

@misc{hu_randla-net_2020,
	title = {{RandLA}-{Net}: {Efficient} {Semantic} {Segmentation} of {Large}-{Scale} {Point} {Clouds}},
	shorttitle = {{RandLA}-{Net}},
	url = {http://arxiv.org/abs/1911.11236},
	doi = {10.48550/arXiv.1911.11236},
	abstract = {We study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Extensive experiments show that our RandLA-Net can process 1 million points in a single pass with up to 200X faster than existing approaches. Moreover, our RandLA-Net clearly surpasses state-of-the-art approaches for semantic segmentation on two large-scale benchmarks Semantic3D and SemanticKITTI.},
	urldate = {2022-12-28},
	publisher = {arXiv},
	author = {Hu, Qingyong and Yang, Bo and Xie, Linhai and Rosa, Stefano and Guo, Yulan and Wang, Zhihua and Trigoni, Niki and Markham, Andrew},
	month = may,
	year = {2020},
	note = {arXiv:1911.11236 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/U3W5GUJB/Hu et al. - 2020 - RandLA-Net Efficient Semantic Segmentation of Lar.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/KBIKRJ99/1911.html:text/html},
}

@article{guo_pct_2021,
	title = {{PCT}: {Point} cloud transformer},
	volume = {7},
	issn = {2096-0433, 2096-0662},
	shorttitle = {{PCT}},
	url = {http://arxiv.org/abs/2012.09688},
	doi = {10.1007/s41095-021-0229-5},
	abstract = {The irregular domain and lack of ordering make it challenging to design deep neural networks for point cloud processing. This paper presents a novel framework named Point Cloud Transformer(PCT) for point cloud learning. PCT is based on Transformer, which achieves huge success in natural language processing and displays great potential in image processing. It is inherently permutation invariant for processing a sequence of points, making it well-suited for point cloud learning. To better capture local context within the point cloud, we enhance input embedding with the support of farthest point sampling and nearest neighbor search. Extensive experiments demonstrate that the PCT achieves the state-of-the-art performance on shape classification, part segmentation and normal estimation tasks.},
	number = {2},
	urldate = {2022-12-28},
	journal = {Computational Visual Media},
	author = {Guo, Meng-Hao and Cai, Jun-Xiong and Liu, Zheng-Ning and Mu, Tai-Jiang and Martin, Ralph R. and Hu, Shi-Min},
	month = jun,
	year = {2021},
	note = {arXiv:2012.09688 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {187--199},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/MEZ2TCSB/Guo et al. - 2021 - PCT Point cloud transformer.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/4VWIPBPF/2012.html:text/html},
}

@misc{xie_pointcontrast_2020,
	title = {{PointContrast}: {Unsupervised} {Pre}-training for {3D} {Point} {Cloud} {Understanding}},
	shorttitle = {{PointContrast}},
	url = {http://arxiv.org/abs/2007.10985},
	doi = {10.48550/arXiv.2007.10985},
	abstract = {Arguably one of the top success stories of deep learning is transfer learning. The finding that pre-training a network on a rich source set (eg., ImageNet) can help boost performance once fine-tuned on a usually much smaller target set, has been instrumental to many applications in language and vision. Yet, very little is known about its usefulness in 3D point cloud understanding. We see this as an opportunity considering the effort required for annotating data in 3D. In this work, we aim at facilitating research on 3D representation learning. Different from previous works, we focus on high-level scene understanding tasks. To this end, we select a suite of diverse datasets and tasks to measure the effect of unsupervised pre-training on a large source set of 3D scenes. Our findings are extremely encouraging: using a unified triplet of architecture, source dataset, and contrastive loss for pre-training, we achieve improvement over recent best results in segmentation and detection across 6 different benchmarks for indoor and outdoor, real and synthetic datasets -- demonstrating that the learned representation can generalize across domains. Furthermore, the improvement was similar to supervised pre-training, suggesting that future efforts should favor scaling data collection over more detailed annotation. We hope these findings will encourage more research on unsupervised pretext task design for 3D deep learning.},
	urldate = {2022-12-28},
	publisher = {arXiv},
	author = {Xie, Saining and Gu, Jiatao and Guo, Demi and Qi, Charles R. and Guibas, Leonidas J. and Litany, Or},
	month = nov,
	year = {2020},
	note = {arXiv:2007.10985 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/CZLHM7W4/Xie et al. - 2020 - PointContrast Unsupervised Pre-training for 3D Po.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/LYY9QYJ8/2007.html:text/html},
}

@misc{tejankar_isd_2021,
	title = {{ISD}: {Self}-{Supervised} {Learning} by {Iterative} {Similarity} {Distillation}},
	shorttitle = {{ISD}},
	url = {http://arxiv.org/abs/2012.09259},
	doi = {10.48550/arXiv.2012.09259},
	abstract = {Recently, contrastive learning has achieved great results in self-supervised learning, where the main idea is to push two augmentations of an image (positive pairs) closer compared to other random images (negative pairs). We argue that not all random images are equal. Hence, we introduce a self supervised learning algorithm where we use a soft similarity for the negative images rather than a binary distinction between positive and negative pairs. We iteratively distill a slowly evolving teacher model to the student model by capturing the similarity of a query image to some random images and transferring that knowledge to the student. We argue that our method is less constrained compared to recent contrastive learning methods, so it can learn better features. Specifically, our method should handle unbalanced and unlabeled data better than existing contrastive learning methods, because the randomly chosen negative set might include many samples that are semantically similar to the query image. In this case, our method labels them as highly similar while standard contrastive methods label them as negative pairs. Our method achieves comparable results to the state-of-the-art models. We also show that our method performs better in the settings where the unlabeled data is unbalanced. Our code is available here: https://github.com/UMBCvision/ISD.},
	urldate = {2022-12-28},
	publisher = {arXiv},
	author = {Tejankar, Ajinkya and Koohpayegani, Soroush Abbasi and Pillai, Vipin and Favaro, Paolo and Pirsiavash, Hamed},
	month = sep,
	year = {2021},
	note = {arXiv:2012.09259 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/4KZW2RQW/Tejankar et al. - 2021 - ISD Self-Supervised Learning by Iterative Similar.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/LASNG5B8/2012.html:text/html},
}

@misc{carreira_quo_2018,
	title = {Quo {Vadis}, {Action} {Recognition}? {A} {New} {Model} and the {Kinetics} {Dataset}},
	shorttitle = {Quo {Vadis}, {Action} {Recognition}?},
	url = {http://arxiv.org/abs/1705.07750},
	doi = {10.48550/arXiv.1705.07750},
	abstract = {The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9\% on HMDB-51 and 98.0\% on UCF-101.},
	urldate = {2022-12-29},
	publisher = {arXiv},
	author = {Carreira, Joao and Zisserman, Andrew},
	month = feb,
	year = {2018},
	note = {arXiv:1705.07750 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/K474NIYH/Carreira and Zisserman - 2018 - Quo Vadis, Action Recognition A New Model and the.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/M3SMLD3U/1705.html:text/html},
}

@article{ji_3d_2013,
	title = {{3D} {Convolutional} {Neural} {Networks} for {Human} {Action} {Recognition}},
	volume = {35},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2012.59},
	abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.},
	number = {1},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
	month = jan,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Deep learning, Feature extraction, 3D convolution, action recognition, Computational modeling, Computer architecture, convolutional neural networks, Kernel, model combination, Solid modeling, Three dimensional displays, Videos},
	pages = {221--231},
	file = {IEEE Xplore Abstract Record:/home/pape/Zotero/storage/7TWPGBIX/6165309.html:text/html},
}

@misc{wu_towards_2021,
	title = {Towards {Long}-{Form} {Video} {Understanding}},
	url = {http://arxiv.org/abs/2106.11310},
	doi = {10.48550/arXiv.2106.11310},
	abstract = {Our world offers a never-ending stream of visual stimuli, yet today's vision systems only accurately recognize patterns within a few seconds. These systems understand the present, but fail to contextualize it in past or future events. In this paper, we study long-form video understanding. We introduce a framework for modeling long-form videos and develop evaluation protocols on large-scale datasets. We show that existing state-of-the-art short-term models are limited for long-form tasks. A novel object-centric transformer-based video recognition architecture performs significantly better on 7 diverse tasks. It also outperforms comparable state-of-the-art on the AVA dataset.},
	urldate = {2022-12-29},
	publisher = {arXiv},
	author = {Wu, Chao-Yuan and Krähenbühl, Philipp},
	month = jun,
	year = {2021},
	note = {arXiv:2106.11310 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/WZUPD5XX/Wu and Krähenbühl - 2021 - Towards Long-Form Video Understanding.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/PNILBX9U/2106.html:text/html},
}

@article{luo_multiple_2021,
	title = {Multiple object tracking: {A} literature review},
	volume = {293},
	issn = {0004-3702},
	shorttitle = {Multiple object tracking},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370220301958},
	doi = {10.1016/j.artint.2020.103448},
	abstract = {Multiple Object Tracking (MOT) has gained increasing attention due to its academic and commercial potential. Although different approaches have been proposed to tackle this problem, it still remains challenging due to factors like abrupt appearance changes and severe object occlusions. In this work, we contribute the first comprehensive and most recent review on this problem. We inspect the recent advances in various aspects and propose some interesting directions for future research. To the best of our knowledge, there has not been any extensive review on this topic in the community. We endeavor to provide a thorough review on the development of this problem in recent decades. The main contributions of this review are fourfold: 1) Key aspects in an MOT system, including formulation, categorization, key principles, evaluation of MOT are discussed; 2) Instead of enumerating individual works, we discuss existing approaches according to various aspects, in each of which methods are divided into different groups and each group is discussed in detail for the principles, advances and drawbacks; 3) We examine experiments of existing publications and summarize results on popular datasets to provide quantitative and comprehensive comparisons. By analyzing the results from different perspectives, we have verified some basic agreements in the field; and 4) We provide a discussion about issues of MOT research, as well as some interesting directions which will become potential research effort in the future.},
	language = {en},
	urldate = {2022-12-29},
	journal = {Artificial Intelligence},
	author = {Luo, Wenhan and Xing, Junliang and Milan, Anton and Zhang, Xiaoqin and Liu, Wei and Kim, Tae-Kyun},
	month = apr,
	year = {2021},
	keywords = {Data association, Multi-object tracking, Survey},
	pages = {103448},
	file = {ScienceDirect Snapshot:/home/pape/Zotero/storage/YBS86HAT/S0004370220301958.html:text/html;Submitted Version:/home/pape/Zotero/storage/ZB47TAST/Luo et al. - 2021 - Multiple object tracking A literature review.pdf:application/pdf},
}

@article{oprea_review_2022,
	title = {A {Review} on {Deep} {Learning} {Techniques} for {Video} {Prediction}},
	volume = {44},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {http://arxiv.org/abs/2004.05214},
	doi = {10.1109/TPAMI.2020.3045007},
	abstract = {The ability to predict, anticipate and reason about future outcomes is a key component of intelligent decision-making systems. In light of the success of deep learning in computer vision, deep-learning-based video prediction emerged as a promising research direction. Defined as a self-supervised learning task, video prediction represents a suitable framework for representation learning, as it demonstrated potential capabilities for extracting meaningful representations of the underlying patterns in natural videos. Motivated by the increasing interest in this task, we provide a review on the deep learning methods for prediction in video sequences. We firstly define the video prediction fundamentals, as well as mandatory background concepts and the most used datasets. Next, we carefully analyze existing video prediction models organized according to a proposed taxonomy, highlighting their contributions and their significance in the field. The summary of the datasets and methods is accompanied with experimental results that facilitate the assessment of the state of the art on a quantitative basis. The paper is summarized by drawing some general conclusions, identifying open research challenges and by pointing out future research directions.},
	number = {6},
	urldate = {2022-12-29},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Oprea, Sergiu and Martinez-Gonzalez, Pablo and Garcia-Garcia, Alberto and Castro-Vargas, John Alejandro and Orts-Escolano, Sergio and Garcia-Rodriguez, Jose and Argyros, Antonis},
	month = jun,
	year = {2022},
	note = {arXiv:2004.05214 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	pages = {2806--2826},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/MG5SLGPQ/Oprea et al. - 2022 - A Review on Deep Learning Techniques for Video Pre.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/C7IK2X5W/2004.html:text/html},
}

@misc{simonyan_two-stream_2014,
	title = {Two-{Stream} {Convolutional} {Networks} for {Action} {Recognition} in {Videos}},
	url = {http://arxiv.org/abs/1406.2199},
	doi = {10.48550/arXiv.1406.2199},
	abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
	urldate = {2022-12-29},
	publisher = {arXiv},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = nov,
	year = {2014},
	note = {arXiv:1406.2199 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/C3MQNI3Y/Simonyan and Zisserman - 2014 - Two-Stream Convolutional Networks for Action Recog.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/3HNF4QFQ/1406.html:text/html},
}

@misc{neimark_video_2021,
	title = {Video {Transformer} {Network}},
	url = {http://arxiv.org/abs/2102.00719},
	doi = {10.48550/arXiv.2102.00719},
	abstract = {This paper presents VTN, a transformer-based framework for video recognition. Inspired by recent developments in vision transformers, we ditch the standard approach in video action recognition that relies on 3D ConvNets and introduce a method that classifies actions by attending to the entire video sequence information. Our approach is generic and builds on top of any given 2D spatial network. In terms of wall runtime, it trains \$16.1{\textbackslash}times\$ faster and runs \$5.1{\textbackslash}times\$ faster during inference while maintaining competitive accuracy compared to other state-of-the-art methods. It enables whole video analysis, via a single end-to-end pass, while requiring \$1.5{\textbackslash}times\$ fewer GFLOPs. We report competitive results on Kinetics-400 and present an ablation study of VTN properties and the trade-off between accuracy and inference speed. We hope our approach will serve as a new baseline and start a fresh line of research in the video recognition domain. Code and models are available at: https://github.com/bomri/SlowFast/blob/master/projects/vtn/README.md},
	urldate = {2022-12-29},
	publisher = {arXiv},
	author = {Neimark, Daniel and Bar, Omri and Zohar, Maya and Asselmann, Dotan},
	month = aug,
	year = {2021},
	note = {arXiv:2102.00719 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/228693EZ/Neimark et al. - 2021 - Video Transformer Network.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/PTDQ222I/2102.html:text/html},
}

@article{luo_multiple_2021-1,
	title = {Multiple object tracking: {A} literature review},
	volume = {293},
	issn = {0004-3702},
	shorttitle = {Multiple object tracking},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370220301958},
	doi = {10.1016/j.artint.2020.103448},
	abstract = {Multiple Object Tracking (MOT) has gained increasing attention due to its academic and commercial potential. Although different approaches have been proposed to tackle this problem, it still remains challenging due to factors like abrupt appearance changes and severe object occlusions. In this work, we contribute the first comprehensive and most recent review on this problem. We inspect the recent advances in various aspects and propose some interesting directions for future research. To the best of our knowledge, there has not been any extensive review on this topic in the community. We endeavor to provide a thorough review on the development of this problem in recent decades. The main contributions of this review are fourfold: 1) Key aspects in an MOT system, including formulation, categorization, key principles, evaluation of MOT are discussed; 2) Instead of enumerating individual works, we discuss existing approaches according to various aspects, in each of which methods are divided into different groups and each group is discussed in detail for the principles, advances and drawbacks; 3) We examine experiments of existing publications and summarize results on popular datasets to provide quantitative and comprehensive comparisons. By analyzing the results from different perspectives, we have verified some basic agreements in the field; and 4) We provide a discussion about issues of MOT research, as well as some interesting directions which will become potential research effort in the future.},
	language = {en},
	urldate = {2022-12-29},
	journal = {Artificial Intelligence},
	author = {Luo, Wenhan and Xing, Junliang and Milan, Anton and Zhang, Xiaoqin and Liu, Wei and Kim, Tae-Kyun},
	month = apr,
	year = {2021},
	keywords = {Data association, Multi-object tracking, Survey},
	pages = {103448},
	file = {ScienceDirect Snapshot:/home/pape/Zotero/storage/AMALRETJ/S0004370220301958.html:text/html;Submitted Version:/home/pape/Zotero/storage/T9RICL4R/Luo et al. - 2021 - Multiple object tracking A literature review.pdf:application/pdf},
}

@misc{wang_fast_2019,
	title = {Fast {Online} {Object} {Tracking} and {Segmentation}: {A} {Unifying} {Approach}},
	shorttitle = {Fast {Online} {Object} {Tracking} and {Segmentation}},
	url = {http://arxiv.org/abs/1812.05050},
	doi = {10.48550/arXiv.1812.05050},
	abstract = {In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 55 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state of the art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017. The project website is http://www.robots.ox.ac.uk/{\textasciitilde}qwang/SiamMask.},
	urldate = {2022-12-29},
	publisher = {arXiv},
	author = {Wang, Qiang and Zhang, Li and Bertinetto, Luca and Hu, Weiming and Torr, Philip H. S.},
	month = may,
	year = {2019},
	note = {arXiv:1812.05050 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/JK6DUB26/Wang et al. - 2019 - Fast Online Object Tracking and Segmentation A Un.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/KELMSZWZ/1812.html:text/html},
}

@inproceedings{li_high_2018,
	title = {High {Performance} {Visual} {Tracking} {With} {Siamese} {Region} {Proposal} {Network}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Li_High_Performance_Visual_CVPR_2018_paper.html},
	urldate = {2022-12-29},
	author = {Li, Bo and Yan, Junjie and Wu, Wei and Zhu, Zheng and Hu, Xiaolin},
	year = {2018},
	pages = {8971--8980},
	file = {Full Text PDF:/home/pape/Zotero/storage/N5GZZR5Y/Li et al. - 2018 - High Performance Visual Tracking With Siamese Regi.pdf:application/pdf},
}

@misc{li_siamrpn_2018,
	title = {{SiamRPN}++: {Evolution} of {Siamese} {Visual} {Tracking} with {Very} {Deep} {Networks}},
	shorttitle = {{SiamRPN}++},
	url = {http://arxiv.org/abs/1812.11703},
	doi = {10.48550/arXiv.1812.11703},
	abstract = {Siamese network based trackers formulate tracking as convolutional feature cross-correlation between target template and searching region. However, Siamese trackers still have accuracy gap compared with state-of-the-art algorithms and they cannot take advantage of feature from deep networks, such as ResNet-50 or deeper. In this work we prove the core reason comes from the lack of strict translation invariance. By comprehensive theoretical analysis and experimental validations, we break this restriction through a simple yet effective spatial aware sampling strategy and successfully train a ResNet-driven Siamese tracker with significant performance gain. Moreover, we propose a new model architecture to perform depth-wise and layer-wise aggregations, which not only further improves the accuracy but also reduces the model size. We conduct extensive ablation studies to demonstrate the effectiveness of the proposed tracker, which obtains currently the best results on four large tracking benchmarks, including OTB2015, VOT2018, UAV123, and LaSOT. Our model will be released to facilitate further studies based on this problem.},
	urldate = {2022-12-29},
	publisher = {arXiv},
	author = {Li, Bo and Wu, Wei and Wang, Qiang and Zhang, Fangyi and Xing, Junliang and Yan, Junjie},
	month = dec,
	year = {2018},
	note = {arXiv:1812.11703 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/YYH4NDF9/Li et al. - 2018 - SiamRPN++ Evolution of Siamese Visual Tracking wi.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/TGIMMJCQ/1812.html:text/html},
}

@misc{zhu_distractor-aware_2018,
	title = {Distractor-aware {Siamese} {Networks} for {Visual} {Object} {Tracking}},
	url = {http://arxiv.org/abs/1808.06048},
	doi = {10.48550/arXiv.1808.06048},
	abstract = {Recently, Siamese networks have drawn great attention in visual tracking community because of their balanced accuracy and speed. However, features used in most Siamese tracking approaches can only discriminate foreground from the non-semantic backgrounds. The semantic backgrounds are always considered as distractors, which hinders the robustness of Siamese trackers. In this paper, we focus on learning distractor-aware Siamese networks for accurate and long-term tracking. To this end, features used in traditional Siamese trackers are analyzed at first. We observe that the imbalanced distribution of training data makes the learned features less discriminative. During the off-line training phase, an effective sampling strategy is introduced to control this distribution and make the model focus on the semantic distractors. During inference, a novel distractor-aware module is designed to perform incremental learning, which can effectively transfer the general embedding to the current video domain. In addition, we extend the proposed approach for long-term tracking by introducing a simple yet effective local-to-global search region strategy. Extensive experiments on benchmarks show that our approach significantly outperforms the state-of-the-arts, yielding 9.6\% relative gain in VOT2016 dataset and 35.9\% relative gain in UAV20L dataset. The proposed tracker can perform at 160 FPS on short-term benchmarks and 110 FPS on long-term benchmarks.},
	urldate = {2022-12-29},
	publisher = {arXiv},
	author = {Zhu, Zheng and Wang, Qiang and Li, Bo and Wu, Wei and Yan, Junjie and Hu, Weiming},
	month = aug,
	year = {2018},
	note = {arXiv:1808.06048 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/WWIQSU9S/Zhu et al. - 2018 - Distractor-aware Siamese Networks for Visual Objec.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/J9J6VII3/1808.html:text/html},
}

@misc{bertinetto_fully-convolutional_2021,
	title = {Fully-{Convolutional} {Siamese} {Networks} for {Object} {Tracking}},
	url = {http://arxiv.org/abs/1606.09549},
	doi = {10.48550/arXiv.1606.09549},
	abstract = {The problem of arbitrary object tracking has traditionally been tackled by learning a model of the object's appearance exclusively online, using as sole training data the video itself. Despite the success of these methods, their online-only approach inherently limits the richness of the model they can learn. Recently, several attempts have been made to exploit the expressive power of deep convolutional networks. However, when the object to track is not known beforehand, it is necessary to perform Stochastic Gradient Descent online to adapt the weights of the network, severely compromising the speed of the system. In this paper we equip a basic tracking algorithm with a novel fully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset for object detection in video. Our tracker operates at frame-rates beyond real-time and, despite its extreme simplicity, achieves state-of-the-art performance in multiple benchmarks.},
	urldate = {2022-12-29},
	publisher = {arXiv},
	author = {Bertinetto, Luca and Valmadre, Jack and Henriques, João F. and Vedaldi, Andrea and Torr, Philip H. S.},
	month = dec,
	year = {2021},
	note = {arXiv:1606.09549 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/XSHTQWTU/Bertinetto et al. - 2021 - Fully-Convolutional Siamese Networks for Object Tr.pdf:application/pdf},
}

@article{matskevych2022shallow,
  title={From shallow to deep: exploiting feature-based classifiers for domain adaptation in semantic segmentation},
  author={Matskevych, Alex and Wolny, Adrian and Pape, Constantin and Kreshuk, Anna},
  journal={Front. Comput. Sci},
  volume={4},
  year={2022}
}


@misc{sohn_fixmatch_2020,
	title = {{FixMatch}: {Simplifying} {Semi}-{Supervised} {Learning} with {Consistency} and {Confidence}},
	shorttitle = {{FixMatch}},
	abstract = {Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93\% accuracy on CIFAR-10 with 250 labels and 88.61\% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at https://github.com/google-research/fixmatch.},
	urldate = {2023-01-03},
	publisher = {arXiv},
	author = {Sohn, Kihyuk and Berthelot, David and Li, Chun-Liang and Zhang, Zizhao and Carlini, Nicholas and Cubuk, Ekin D. and Kurakin, Alex and Zhang, Han and Raffel, Colin},
	month = nov,
	year = {2020},
	note = {arXiv:2001.07685 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/HJPHQLFG/Sohn et al. - 2020 - FixMatch Simplifying Semi-Supervised Learning wit.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/ILURHLI3/2001.html:text/html},
}

@article{seibold_reference-guided_2022,
	title = {Reference-guided {Pseudo}-{Label} {Generation} for {Medical} {Semantic} {Segmentation}},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	url = {http://arxiv.org/abs/2112.00735},
	doi = {10.1609/aaai.v36i2.20114},
	abstract = {Producing densely annotated data is a difficult and tedious task for medical imaging applications. To address this problem, we propose a novel approach to generate supervision for semi-supervised semantic segmentation. We argue that visually similar regions between labeled and unlabeled images likely contain the same semantics and therefore should share their label. Following this thought, we use a small number of labeled images as reference material and match pixels in an unlabeled image to the semantics of the best fitting pixel in a reference set. This way, we avoid pitfalls such as confirmation bias, common in purely prediction-based pseudo-labeling. Since our method does not require any architectural changes or accompanying networks, one can easily insert it into existing frameworks. We achieve the same performance as a standard fully supervised model on X-ray anatomy segmentation, albeit 95\% fewer labeled images. Aside from an in-depth analysis of different aspects of our proposed method, we further demonstrate the effectiveness of our reference-guided learning paradigm by comparing our approach against existing methods for retinal fluid segmentation with competitive performance as we improve upon recent work by up to 15\% mean IoU.},
	number = {2},
	urldate = {2023-01-03},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Seibold, Constantin and Reiß, Simon and Kleesiek, Jens and Stiefelhagen, Rainer},
	month = jun,
	year = {2022},
	note = {arXiv:2112.00735 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Artificial Intelligence, 68T07, 68T45, I.5.4},
	pages = {2171--2179},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/9MZH3EXA/Seibold et al. - 2022 - Reference-guided Pseudo-Label Generation for Medic.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/V8APECP7/2112.html:text/html},
}

@misc{zou_pseudoseg_2021,
	title = {{PseudoSeg}: {Designing} {Pseudo} {Labels} for {Semantic} {Segmentation}},
	shorttitle = {{PseudoSeg}},
	url = {http://arxiv.org/abs/2010.09713},
	doi = {10.48550/arXiv.2010.09713},
	abstract = {Recent advances in semi-supervised learning (SSL) demonstrate that a combination of consistency regularization and pseudo-labeling can effectively improve image classification accuracy in the low-data regime. Compared to classification, semantic segmentation tasks require much more intensive labeling costs. Thus, these tasks greatly benefit from data-efficient training methods. However, structured outputs in segmentation render particular difficulties (e.g., designing pseudo-labeling and augmentation) to apply existing SSL strategies. To address this problem, we present a simple and novel re-design of pseudo-labeling to generate well-calibrated structured pseudo labels for training with unlabeled or weakly-labeled data. Our proposed pseudo-labeling strategy is network structure agnostic to apply in a one-stage consistency training framework. We demonstrate the effectiveness of the proposed pseudo-labeling strategy in both low-data and high-data regimes. Extensive experiments have validated that pseudo labels generated from wisely fusing diverse sources and strong data augmentation are crucial to consistency training for segmentation. The source code is available at https://github.com/googleinterns/wss.},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Zou, Yuliang and Zhang, Zizhao and Zhang, Han and Li, Chun-Liang and Bian, Xiao and Huang, Jia-Bin and Pfister, Tomas},
	month = mar,
	year = {2021},
	note = {arXiv:2010.09713 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/MIXMKUKK/Zou et al. - 2021 - PseudoSeg Designing Pseudo Labels for Semantic Se.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/TTI7AT3U/2010.html:text/html},
}

@misc{ben-haim_graph_2022,
	title = {Graph {Neural} {Network} for {Cell} {Tracking} in {Microscopy} {Videos}},
	url = {http://arxiv.org/abs/2202.04731},
	doi = {10.48550/arXiv.2202.04731},
	abstract = {We present a novel graph neural network (GNN) approach for cell tracking in high-throughput microscopy videos. By modeling the entire time-lapse sequence as a direct graph where cell instances are represented by its nodes and their associations by its edges, we extract the entire set of cell trajectories by looking for the maximal paths in the graph. This is accomplished by several key contributions incorporated into an end-to-end deep learning framework. We exploit a deep metric learning algorithm to extract cell feature vectors that distinguish between instances of different biological cells and assemble same cell instances. We introduce a new GNN block type which enables a mutual update of node and edge feature vectors, thus facilitating the underlying message passing process. The message passing concept, whose extent is determined by the number of GNN blocks, is of fundamental importance as it enables the `flow' of information between nodes and edges much behind their neighbors in consecutive frames. Finally, we solve an edge classification problem and use the identified active edges to construct the cells' tracks and lineage trees. We demonstrate the strengths of the proposed cell tracking approach by applying it to 2D and 3D datasets of different cell types, imaging setups, and experimental conditions. We show that our framework outperforms current state-of-the-art methods on most of the evaluated datasets. The code is available at our repository: https://github.com/talbenha/cell-tracker-gnn.},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Ben-Haim, Tal and Raviv, Tammy Riklin},
	month = jul,
	year = {2022},
	note = {arXiv:2202.04731 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/8R7KZATN/Ben-Haim and Raviv - 2022 - Graph Neural Network for Cell Tracking in Microsco.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/TQSKBZJ4/2202.html:text/html},
}

@misc{xie_propagate_2021,
	title = {Propagate {Yourself}: {Exploring} {Pixel}-{Level} {Consistency} for {Unsupervised} {Visual} {Representation} {Learning}},
	shorttitle = {Propagate {Yourself}},
	url = {http://arxiv.org/abs/2011.10043},
	doi = {10.48550/arXiv.2011.10043},
	abstract = {Contrastive learning methods for unsupervised visual representation learning have reached remarkable levels of transfer performance. We argue that the power of contrastive learning has yet to be fully unleashed, as current methods are trained only on instance-level pretext tasks, leading to representations that may be sub-optimal for downstream tasks requiring dense pixel predictions. In this paper, we introduce pixel-level pretext tasks for learning dense feature representations. The first task directly applies contrastive learning at the pixel level. We additionally propose a pixel-to-propagation consistency task that produces better results, even surpassing the state-of-the-art approaches by a large margin. Specifically, it achieves 60.2 AP, 41.4 / 40.5 mAP and 77.2 mIoU when transferred to Pascal VOC object detection (C4), COCO object detection (FPN / C4) and Cityscapes semantic segmentation using a ResNet-50 backbone network, which are 2.6 AP, 0.8 / 1.0 mAP and 1.0 mIoU better than the previous best methods built on instance-level contrastive learning. Moreover, the pixel-level pretext tasks are found to be effective for pre-training not only regular backbone networks but also head networks used for dense downstream tasks, and are complementary to instance-level contrastive methods. These results demonstrate the strong potential of defining pretext tasks at the pixel level, and suggest a new path forward in unsupervised visual representation learning. Code is available at {\textbackslash}url\{https://github.com/zdaxie/PixPro\}.},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Xie, Zhenda and Lin, Yutong and Zhang, Zheng and Cao, Yue and Lin, Stephen and Hu, Han},
	month = mar,
	year = {2021},
	note = {arXiv:2011.10043 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/8WVT2ZDJ/Xie et al. - 2021 - Propagate Yourself Exploring Pixel-Level Consiste.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/XN6D35N4/2011.html:text/html},
}

@misc{mensink_factors_2021,
	title = {Factors of {Influence} for {Transfer} {Learning} across {Diverse} {Appearance} {Domains} and {Task} {Types}},
	url = {http://arxiv.org/abs/2103.13318},
	doi = {10.48550/arXiv.2103.13318},
	abstract = {Transfer learning enables to re-use knowledge learned on a source task to help learning a target task. A simple form of transfer learning is common in current state-of-the-art computer vision models, i.e. pre-training a model for image classification on the ILSVRC dataset, and then fine-tune on any target task. However, previous systematic studies of transfer learning have been limited and the circumstances in which it is expected to work are not fully understood. In this paper we carry out an extensive experimental exploration of transfer learning across vastly different image domains (consumer photos, autonomous driving, aerial imagery, underwater, indoor scenes, synthetic, close-ups) and task types (semantic segmentation, object detection, depth estimation, keypoint detection). Importantly, these are all complex, structured output tasks types relevant to modern computer vision applications. In total we carry out over 2000 transfer learning experiments, including many where the source and target come from different image domains, task types, or both. We systematically analyze these experiments to understand the impact of image domain, task type, and dataset size on transfer learning performance. Our study leads to several insights and concrete recommendations: (1) for most tasks there exists a source which significantly outperforms ILSVRC'12 pre-training; (2) the image domain is the most important factor for achieving positive transfer; (3) the source dataset should {\textbackslash}emph\{include\} the image domain of the target dataset to achieve best results; (4) at the same time, we observe only small negative effects when the image domain of the source task is much broader than that of the target; (5) transfer across task types can be beneficial, but its success is heavily dependent on both the source and target task types.},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Mensink, Thomas and Uijlings, Jasper and Kuznetsova, Alina and Gygli, Michael and Ferrari, Vittorio},
	month = nov,
	year = {2021},
	note = {arXiv:2103.13318 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/MU4PW6I2/Mensink et al. - 2021 - Factors of Influence for Transfer Learning across .pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/YV96FFCZ/2103.html:text/html},
}

@misc{zhang_cyclemix_2022,
	title = {{CycleMix}: {A} {Holistic} {Strategy} for {Medical} {Image} {Segmentation} from {Scribble} {Supervision}},
	shorttitle = {{CycleMix}},
	url = {http://arxiv.org/abs/2203.01475},
	doi = {10.48550/arXiv.2203.01475},
	abstract = {Curating a large set of fully annotated training data can be costly, especially for the tasks of medical image segmentation. Scribble, a weaker form of annotation, is more obtainable in practice, but training segmentation models from limited supervision of scribbles is still challenging. To address the difficulties, we propose a new framework for scribble learning-based medical image segmentation, which is composed of mix augmentation and cycle consistency and thus is referred to as CycleMix. For augmentation of supervision, CycleMix adopts the mixup strategy with a dedicated design of random occlusion, to perform increments and decrements of scribbles. For regularization of supervision, CycleMix intensifies the training objective with consistency losses to penalize inconsistent segmentation, which results in significant improvement of segmentation performance. Results on two open datasets, i.e., ACDC and MSCMRseg, showed that the proposed method achieved exhilarating performance, demonstrating comparable or even better accuracy than the fully-supervised methods. The code and expert-made scribble annotations for MSCMRseg are publicly available at https://github.com/BWGZK/CycleMix.},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Zhang, Ke and Zhuang, Xiahai},
	month = mar,
	year = {2022},
	note = {arXiv:2203.01475 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/7X4QWLFI/Zhang and Zhuang - 2022 - CycleMix A Holistic Strategy for Medical Image Se.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/UE73YIR5/2203.html:text/html},
}

@misc{lee_scribble2label_2020,
	title = {{Scribble2Label}: {Scribble}-{Supervised} {Cell} {Segmentation} via {Self}-{Generating} {Pseudo}-{Labels} with {Consistency}},
	shorttitle = {{Scribble2Label}},
	url = {http://arxiv.org/abs/2006.12890},
	doi = {10.48550/arXiv.2006.12890},
	abstract = {Segmentation is a fundamental process in microscopic cell image analysis. With the advent of recent advances in deep learning, more accurate and high-throughput cell segmentation has become feasible. However, most existing deep learning-based cell segmentation algorithms require fully annotated ground-truth cell labels, which are time-consuming and labor-intensive to generate. In this paper, we introduce Scribble2Label, a novel weakly-supervised cell segmentation framework that exploits only a handful of scribble annotations without full segmentation labels. The core idea is to combine pseudo-labeling and label filtering to generate reliable labels from weak supervision. For this, we leverage the consistency of predictions by iteratively averaging the predictions to improve pseudo labels. We demonstrate the performance of Scribble2Label by comparing it to several state-of-the-art cell segmentation methods with various cell image modalities, including bright-field, fluorescence, and electron microscopy. We also show that our method performs robustly across different levels of scribble details, which confirms that only a few scribble annotations are required in real-use cases.},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Lee, Hyeonsoo and Jeong, Won-Ki},
	month = jun,
	year = {2020},
	note = {arXiv:2006.12890 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/HALTGM7W/Lee and Jeong - 2020 - Scribble2Label Scribble-Supervised Cell Segmentat.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/N2WYES8I/2006.html:text/html},
}

@article{gao_segmentation_2022,
	title = {Segmentation only uses sparse annotations: {Unified} weakly and semi-supervised learning in medical images},
	volume = {80},
	issn = {1361-8415},
	shorttitle = {Segmentation only uses sparse annotations},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841522001621},
	doi = {10.1016/j.media.2022.102515},
	abstract = {Since segmentation labeling is usually time-consuming and annotating medical images requires professional expertise, it is laborious to obtain a large-scale, high-quality annotated segmentation dataset. We propose a novel weakly- and semi-supervised framework named SOUSA (Segmentation Only Uses Sparse Annotations), aiming at learning from a small set of sparse annotated data and a large amount of unlabeled data. The proposed framework contains a teacher model and a student model. The student model is weakly supervised by scribbles and a Geodesic distance map derived from scribbles. Meanwhile, a large amount of unlabeled data with various perturbations are fed to student and teacher models. The consistency of their output predictions is imposed by Mean Square Error (MSE) loss and a carefully designed Multi-angle Projection Reconstruction (MPR) loss. Extensive experiments are conducted to demonstrate the robustness and generalization ability of our proposed method. Results show that our method outperforms weakly- and semi-supervised state-of-the-art methods on multiple datasets. Furthermore, our method achieves a competitive performance with some fully supervised methods with dense annotation when the size of the dataset is limited.},
	language = {en},
	urldate = {2023-01-06},
	journal = {Medical Image Analysis},
	author = {Gao, Feng and Hu, Minhao and Zhong, Min-Er and Feng, Shixiang and Tian, Xuwei and Meng, Xiaochun and Ni-jia-ti, Ma-yi-di-li and Huang, Zeping and Lv, Minyi and Song, Tao and Zhang, Xiaofan and Zou, Xiaoguang and Wu, Xiaojian},
	month = aug,
	year = {2022},
	keywords = {Medical image, Semantic segmentation, Semi-supervised learning, Weakly supervised learning},
	pages = {102515},
}

@misc{singhal_large_2022,
	title = {Large {Language} {Models} {Encode} {Clinical} {Knowledge}},
	url = {http://arxiv.org/abs/2212.13138},
	doi = {10.48550/arXiv.2212.13138},
	abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but the quality bar for medical and clinical applications is high. Today, attempts to assess models' clinical knowledge typically rely on automated evaluations on limited benchmarks. There is no standard to evaluate model predictions and reasoning across a breadth of tasks. To address this, we present MultiMedQA, a benchmark combining six existing open question answering datasets spanning professional medical exams, research, and consumer queries; and HealthSearchQA, a new free-response dataset of medical questions searched online. We propose a framework for human evaluation of model answers along multiple axes including factuality, precision, possible harm, and bias. In addition, we evaluate PaLM (a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA, MedMCQA, PubMedQA, MMLU clinical topics), including 67.6\% accuracy on MedQA (US Medical License Exam questions), surpassing prior state-of-the-art by over 17\%. However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve this we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, recall of knowledge, and medical reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal important limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLM models for clinical applications.},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly, Chris and Scharli, Nathaneal and Chowdhery, Aakanksha and Mansfield, Philip and Arcas, Blaise Aguera y and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Chou, Katherine and Gottweis, Juraj and Tomasev, Nenad and Liu, Yun and Rajkomar, Alvin and Barral, Joelle and Semturs, Christopher and Karthikesalingam, Alan and Natarajan, Vivek},
	month = dec,
	year = {2022},
	note = {arXiv:2212.13138 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/6C24MAHX/Singhal et al. - 2022 - Large Language Models Encode Clinical Knowledge.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/4WBM8ZDH/2212.html:text/html},
}

@article{viana_integrated_2023,
	title = {Integrated intracellular organization and its variations in human {iPS} cells},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-022-05563-7},
	doi = {10.1038/s41586-022-05563-7},
	abstract = {Understanding how a subset of expressed genes dictates cellular phenotype is a considerable challenge owing to the large numbers of molecules involved, their combinatorics and the plethora of cellular behaviours that they determine1,2. Here we reduced this complexity by focusing on cellular organization—a key readout and driver of cell behaviour3,4—at the level of major cellular structures that represent distinct organelles and functional machines, and generated the WTC-11 hiPSC Single-Cell Image Dataset v1, which contains more than 200,000 live cells in 3D, spanning 25 key cellular structures. The scale and quality of this dataset permitted the creation of a generalizable analysis framework to convert raw image data of cells and their structures into dimensionally reduced, quantitative measurements that can be interpreted by humans, and to facilitate data exploration. This framework embraces the vast cell-to-cell variability that is observed within a normal population, facilitates the integration of cell-by-cell structural data and allows quantitative analyses of distinct, separable aspects of organization within and across different cell populations. We found that the integrated intracellular organization of interphase cells was robust to the wide range of variation in cell shape in the population; that the average locations of some structures became polarized in cells at the edges of colonies while maintaining the ‘wiring’ of their interactions with other structures; and that, by contrast, changes in the location of structures during early mitotic reorganization were accompanied by changes in their wiring.},
	language = {en},
	urldate = {2023-01-06},
	journal = {Nature},
	author = {Viana, Matheus P. and Chen, Jianxu and Knijnenburg, Theo A. and Vasan, Ritvik and Yan, Calysta and Arakaki, Joy E. and Bailey, Matte and Berry, Ben and Borensztejn, Antoine and Brown, Eva M. and Carlson, Sara and Cass, Julie A. and Chaudhuri, Basudev and Cordes Metzler, Kimberly R. and Coston, Mackenzie E. and Crabtree, Zach J. and Davidson, Steve and DeLizo, Colette M. and Dhaka, Shailja and Dinh, Stephanie Q. and Do, Thao P. and Domingus, Justin and Donovan-Maiye, Rory M. and Ferrante, Alexandra J. and Foster, Tyler J. and Frick, Christopher L. and Fujioka, Griffin and Fuqua, Margaret A. and Gehring, Jamie L. and Gerbin, Kaytlyn A. and Grancharova, Tanya and Gregor, Benjamin W. and Harrylock, Lisa J. and Haupt, Amanda and Hendershott, Melissa C. and Hookway, Caroline and Horwitz, Alan R. and Hughes, H. Christopher and Isaac, Eric J. and Johnson, Gregory R. and Kim, Brian and Leonard, Andrew N. and Leung, Winnie W. and Lucas, Jordan J. and Ludmann, Susan A. and Lyons, Blair M. and Malik, Haseeb and McGregor, Ryan and Medrash, Gabe E. and Meharry, Sean L. and Mitcham, Kevin and Mueller, Irina A. and Murphy-Stevens, Timothy L. and Nath, Aditya and Nelson, Angelique M. and Oluoch, Sandra A. and Paleologu, Luana and Popiel, T. Alexander and Riel-Mehan, Megan M. and Roberts, Brock and Schaefbauer, Lisa M. and Schwarzl, Magdalena and Sherman, Jamie and Slaton, Sylvain and Sluzewski, M. Filip and Smith, Jacqueline E. and Sul, Youngmee and Swain-Bowden, Madison J. and Tang, W. Joyce and Thirstrup, Derek J. and Toloudis, Daniel M. and Tucker, Andrew P. and Valencia, Veronica and Wiegraebe, Winfried and Wijeratna, Thushara and Yang, Ruian and Zaunbrecher, Rebecca J. and Labitigan, Ramon Lorenzo D. and Sanborn, Adrian L. and Johnson, Graham T. and Gunawardane, Ruwanthi N. and Gaudreault, Nathalie and Theriot, Julie A. and Rafelski, Susanne M.},
	month = jan,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cellular imaging, Image processing, Organelles, Induced pluripotent stem cells, Robustness},
	pages = {1--10},
	file = {Full Text PDF:/home/pape/Zotero/storage/FRH6KLAW/Viana et al. - 2023 - Integrated intracellular organization and its vari.pdf:application/pdf},
}

@misc{celik_biological_2022,
	title = {Biological {Cartography}: {Building} and {Benchmarking} {Representations} of {Life}},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Biological {Cartography}},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.09.519400v1},
	doi = {10.1101/2022.12.09.519400},
	abstract = {The continued scaling of genetic perturbation technologies combined with high-dimensional assays (microscopy and RNA-sequencing) has enabled genome-scale reverse-genetics experiments that go beyond single-endpoint measurements of growth or lethality. Datasets emerging from these experiments can be combined to construct “maps of biology”, in which perturbation readouts are placed in unified, relatable embedding spaces to capture known biological relationships and discover new ones. Construction of maps involves many technical choices in both experimental and computational protocols, motivating the design of benchmark procedures by which to evaluate map quality in a systematic, unbiased manner.
In this work, we propose a framework for the steps involved in map building and demonstrate key classes of benchmarks to assess the quality of a map. We describe univariate benchmarks assessing perturbation quality and multivariate benchmarks assessing recovery of known biological relationships from large-scale public data sources. We demonstrate the application and interpretation of these benchmarks through example maps of scRNA-seq and phenomic imaging data.},
	language = {en},
	urldate = {2023-01-06},
	publisher = {bioRxiv},
	author = {Celik, Safiye and Hütter, Jan-Christian and Carlos, Sandra Melo and Lazar, Nathan H. and Mohan, Rahul and Tillinghast, Conor and Biancalani, Tommaso and Fay, Marta and Earnshaw, Berton A. and Haque, Imran S.},
	month = dec,
	year = {2022},
	note = {Pages: 2022.12.09.519400
Section: New Results},
	file = {Full Text PDF:/home/pape/Zotero/storage/4KUUQYUG/Celik et al. - 2022 - Biological Cartography Building and Benchmarking .pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/2INAI284/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/AWZEQBZT/1810.html:text/html},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/HAVAU4X7/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/9TXHEL2S/1706.html:text/html},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/IPFY6YDH/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/X94CA2KQ/2005.html:text/html},
}

@misc{chowdhery_palm_2022,
	title = {{PaLM}: {Scaling} {Language} {Modeling} with {Pathways}},
	shorttitle = {{PaLM}},
	url = {http://arxiv.org/abs/2204.02311},
	doi = {10.48550/arXiv.2204.02311},
	abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	month = oct,
	year = {2022},
	note = {arXiv:2204.02311 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/WHSQDJJB/Chowdhery et al. - 2022 - PaLM Scaling Language Modeling with Pathways.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/C7LG7BP2/2204.html:text/html},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	doi = {10.48550/arXiv.1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/8SBMFJ9X/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/TZWCUXS4/1907.html:text/html},
}

@misc{stanton_does_2021,
	title = {Does {Knowledge} {Distillation} {Really} {Work}?},
	url = {http://arxiv.org/abs/2106.05945},
	doi = {10.48550/arXiv.2106.05945},
	abstract = {Knowledge distillation is a popular technique for training a small student network to emulate a larger teacher model, such as an ensemble of networks. We show that while knowledge distillation can improve student generalization, it does not typically work as it is commonly understood: there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student has the capacity to perfectly match the teacher. We identify difficulties in optimization as a key reason for why the student is unable to match the teacher. We also show how the details of the dataset used for distillation play a role in how closely the student matches the teacher -- and that more closely matching the teacher paradoxically does not always lead to better student generalization.},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Stanton, Samuel and Izmailov, Pavel and Kirichenko, Polina and Alemi, Alexander A. and Wilson, Andrew Gordon},
	month = dec,
	year = {2021},
	note = {arXiv:2106.05945 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/WBJKCIYJ/Stanton et al. - 2021 - Does Knowledge Distillation Really Work.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/PXKQH7A3/2106.html:text/html},
}

@misc{dai_semi-supervised_2015,
	title = {Semi-supervised {Sequence} {Learning}},
	url = {http://arxiv.org/abs/1511.01432},
	doi = {10.48550/arXiv.1511.01432},
	abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
	urldate = {2023-01-08},
	publisher = {arXiv},
	author = {Dai, Andrew M. and Le, Quoc V.},
	month = nov,
	year = {2015},
	note = {arXiv:1511.01432 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/AWL5P764/Dai and Le - 2015 - Semi-supervised Sequence Learning.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/Y6KW4N9T/1511.html:text/html},
}

@article{mukherjee_domain_2023,
	title = {Domain {Adapted} {Multitask} {Learning} for {Segmenting} {Amoeboid} {Cells} in {Microscopy}},
	volume = {42},
	issn = {1558-254X},
	doi = {10.1109/TMI.2022.3203022},
	abstract = {The method proposed in this paper is a robust combination of multi-task learning and unsupervised domain adaptation for segmenting amoeboid cells in microscopy. A highlight of this work is the manner in which the model’s hyperparameters are estimated. The detriments of ad-hoc parameter estimation are well known, but this issue remains largely unaddressed in the context of CNN-based segmentation. Using a novel min-max formulation of the segmentation cost function our proposed method analytically estimates the model’s hyperparameters, while simultaneously learning the CNN weights during training. This end-to-end framework provides a consolidated mechanism to harness the potential of multi-task learning to isolate and segment clustered cells from low contrast brightfield images, and it simultaneously leverages deep domain adaptation to segment fluorescent cells without explicit pixel-level re- annotation of the data. Experimental validations on multi-cellular images strongly suggest the effectiveness of the proposed technique, and our quantitative results show at least 15\% and 10\% improvement in cell segmentation on brightfield and fluorescence images respectively compared to contemporary supervised segmentation methods.},
	number = {1},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Mukherjee, Suvadip and Sarkar, Rituparna and Manich, Maria and Labruyère, Elisabeth and Olivo-Marin, Jean-Christophe},
	month = jan,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {deep learning, microscopy, Microscopy, Image segmentation, Task analysis, Training, domain adaptation, Adaptation models, cell biology, Cell segmentation, Imaging, multi-task learning, Multitasking},
	pages = {42--54},
	file = {IEEE Xplore Abstract Record:/home/pape/Zotero/storage/CJ2YCRD2/9870864.html:text/html},
}

@article{krentzel_deep_2023,
	title = {Deep learning in image-based phenotypic drug discovery},
	issn = {0962-8924},
	url = {https://www.sciencedirect.com/science/article/pii/S0962892422002628},
	doi = {10.1016/j.tcb.2022.11.011},
	abstract = {Modern drug discovery approaches often use high-content imaging to systematically study the effect on cells of large libraries of chemical compounds. By automatically screening thousands or millions of images to identify specific drug-induced cellular phenotypes, for example, altered cellular morphology, these approaches can reveal ‘hit’ compounds offering therapeutic promise. In the past few years, artificial intelligence (AI) methods based on deep learning (DL) [a family of machine learning (ML) techniques] have disrupted virtually all image analysis tasks, from image classification to segmentation. These powerful methods also promise to impact drug discovery by accelerating the identification of effective drugs and their modes of action. In this review, we highlight applications and adaptations of ML, especially DL methods for cell-based phenotypic drug discovery (PDD).},
	language = {en},
	urldate = {2023-01-13},
	journal = {Trends in Cell Biology},
	author = {Krentzel, Daniel and Shorte, Spencer L. and Zimmer, Christophe},
	month = jan,
	year = {2023},
	keywords = {deep learning, machine learning, cellular assays, high-content screening, phenotypic drug discovery},
	file = {ScienceDirect Full Text PDF:/home/pape/Zotero/storage/E77GZSHJ/Krentzel et al. - 2023 - Deep learning in image-based phenotypic drug disco.pdf:application/pdf;ScienceDirect Snapshot:/home/pape/Zotero/storage/LTAHQCFU/S0962892422002628.html:text/html},
}

@misc{cazorla_svetlana_2023,
	title = {Svetlana: a {Supervised} {Segmentation} {Classifier} for {Napari}},
	shorttitle = {Svetlana},
	url = {https://hal.science/hal-03927879},
	abstract = {We present Svetlana (SuperVised sEgmenTation cLAssifier for NapAri), an open-source Napari plugin dedicated to the manual or automatic classification of segmentation results. A few recent software have made it possible to automatically segment complex 2D and 3D objects such as cells in biology with unrivaled performance. However, the subsequent analysis of the results is oftentimes inaccessible to non-specialists. The Svetlana plugin aims at going one step further, by allowing end-users to label the segmented objects and to pick, train and run arbitrary neural network classifiers. The resulting network can then be used for the quantitative analysis of biophysical phenoma. We showcase its performance through challenging problems in 2D and 3D. Comparisons with random forest classifiers, which are the only easily available alternative to date, show significant advantages for the proposed approach.},
	language = {en},
	urldate = {2023-01-13},
	author = {Cazorla, Clément and Weiss, Pierre and Morin, Renaud},
	month = jan,
	year = {2023},
	file = {Full Text PDF:/home/pape/Zotero/storage/4RTNXS3G/Cazorla et al. - 2023 - Svetlana a Supervised Segmentation Classifier for.pdf:application/pdf},
}

@misc{heckenbach_deep_2021,
	title = {Deep {Learning} {Shows} {Cellular} {Senescence} {Is} a {Barrier} to {Cancer} {Development}},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2021.03.18.435987v1},
	doi = {10.1101/2021.03.18.435987},
	abstract = {Cellular senescence is a critical component of aging and many age-related diseases, but understanding its role in human health is challenging in part due to the lack of exclusive or universal markers. Using neural networks, we achieve high accuracy in predicting senescence state and type from the nuclear morphology of DAPI-stained human fibroblasts, murine astrocytes and fibroblasts derived from premature aging diseases in vitro. After generalizing this approach, the predictor recognizes an increasing rate of senescent cells with age in H\&E-stained murine liver tissue and human dermal biopsies. Evaluating corresponding medical records reveals that individuals with increased senescent cells have a significantly decreased rate of malignant neoplasms, lending support for the protective role of senescence in limiting cancer development. In sum, we introduce a novel predictor of cellular senescence and apply it to diagnostic medical images, indicating cancer occurs more frequently for those with a lower rate of senescence.},
	language = {en},
	urldate = {2023-01-13},
	publisher = {bioRxiv},
	author = {Heckenbach, Indra and Ezra, Michael Ben and Mkrtchyan, Garik V. and Madsen, Jakob Sture and Nielsen, Malte Hasle and Oró, Denise and Mortensen, Laust and Verdin, Eric and Westendorp, Rudi and Scheibye-Knudsen, Morten},
	month = mar,
	year = {2021},
	note = {Pages: 2021.03.18.435987
Section: New Results},
	file = {Full Text PDF:/home/pape/Zotero/storage/AJ84584Q/Heckenbach et al. - 2021 - Deep Learning Shows Cellular Senescence Is a Barri.pdf:application/pdf},
}

@misc{noauthor_reconstructing_nodate,
	title = {Reconstructing cell cycle and disease progression using deep learning {\textbar} {Nature} {Communications}},
	url = {https://www.nature.com/articles/s41467-017-00623-3},
	urldate = {2023-01-13},
}

@inproceedings{bewley_simple_2016,
	title = {Simple {Online} and {Realtime} {Tracking}},
	url = {http://arxiv.org/abs/1602.00763},
	doi = {10.1109/ICIP.2016.7533003},
	abstract = {This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9\%. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.},
	urldate = {2023-01-18},
	booktitle = {2016 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},
	month = sep,
	year = {2016},
	note = {arXiv:1602.00763 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {3464--3468},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/UZ8PWHVV/Bewley et al. - 2016 - Simple Online and Realtime Tracking.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/9FZAT4VE/1602.html:text/html},
}

@misc{wojke_simple_2017,
	title = {Simple {Online} and {Realtime} {Tracking} with a {Deep} {Association} {Metric}},
	url = {http://arxiv.org/abs/1703.07402},
	doi = {10.48550/arXiv.1703.07402},
	abstract = {Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a large-scale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45\%, achieving overall competitive performance at high frame rates.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},
	month = mar,
	year = {2017},
	note = {arXiv:1703.07402 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/2BPMJA7B/Wojke et al. - 2017 - Simple Online and Realtime Tracking with a Deep As.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/ZB2ED9NX/1703.html:text/html},
}

@misc{du_strongsort_2022,
	title = {{StrongSORT}: {Make} {DeepSORT} {Great} {Again}},
	shorttitle = {{StrongSORT}},
	url = {http://arxiv.org/abs/2202.13514},
	doi = {10.48550/arXiv.2202.13514},
	abstract = {Existing Multi-Object Tracking (MOT) methods can be roughly classified as tracking-by-detection and joint-detection-association paradigms. Although the latter has elicited more attention and demonstrates comparable performance relative to the former, we claim that the tracking-by-detection paradigm is still the optimal solution in terms of tracking accuracy. In this paper, we revisit the classic tracker DeepSORT and upgrade it from various aspects, i.e., detection, embedding and association. The resulting tracker, called StrongSORT, sets new HOTA and IDF1 records on MOT17 and MOT20. We also present two lightweight and plug-and-play algorithms to further refine the tracking results. Firstly, an appearance-free link model (AFLink) is proposed to associate short tracklets into complete trajectories. To the best of our knowledge, this is the first global link model without appearance information. Secondly, we propose Gaussian-smoothed interpolation (GSI) to compensate for missing detections. Instead of ignoring motion information like linear interpolation, GSI is based on the Gaussian process regression algorithm and can achieve more accurate localizations. Moreover, AFLink and GSI can be plugged into various trackers with a negligible extra computational cost (591.9 and 140.9 Hz, respectively, on MOT17). By integrating StrongSORT with the two algorithms, the final tracker StrongSORT++ ranks first on MOT17 and MOT20 in terms of HOTA and IDF1 metrics and surpasses the second-place one by 1.3 - 2.2. Code will be released soon.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Du, Yunhao and Song, Yang and Yang, Bo and Zhao, Yanyun},
	month = feb,
	year = {2022},
	note = {arXiv:2202.13514 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/29SYCE4K/Du et al. - 2022 - StrongSORT Make DeepSORT Great Again.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/6LSP2SIJ/2202.html:text/html},
}

@misc{wang_towards_2020,
	title = {Towards {Real}-{Time} {Multi}-{Object} {Tracking}},
	url = {http://arxiv.org/abs/1909.12605},
	doi = {10.48550/arXiv.1909.12605},
	abstract = {Modern multiple object tracking (MOT) systems usually follow the {\textbackslash}emph\{tracking-by-detection\} paradigm. It has 1) a detection model for target localization and 2) an appearance embedding model for data association. Having the two models separately executed might lead to efficiency problems, as the running time is simply a sum of the two steps without investigating potential structures that can be shared between them. Existing research efforts on real-time MOT usually focus on the association step, so they are essentially real-time association methods but not real-time MOT system. In this paper, we propose an MOT system that allows target detection and appearance embedding to be learned in a shared model. Specifically, we incorporate the appearance embedding model into a single-shot detector, such that the model can simultaneously output detections and the corresponding embeddings. We further propose a simple and fast association method that works in conjunction with the joint model. In both components the computation cost is significantly reduced compared with former MOT systems, resulting in a neat and fast baseline for future follow-ups on real-time MOT algorithm design. To our knowledge, this work reports the first (near) real-time MOT system, with a running speed of 22 to 40 FPS depending on the input resolution. Meanwhile, its tracking accuracy is comparable to the state-of-the-art trackers embodying separate detection and embedding (SDE) learning (\$64.4{\textbackslash}\%\$ MOTA {\textbackslash}vs \$66.1{\textbackslash}\%\$ MOTA on MOT-16 challenge). Code and models are available at {\textbackslash}url\{https://github.com/Zhongdao/Towards-Realtime-MOT\}.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Wang, Zhongdao and Zheng, Liang and Liu, Yixuan and Li, Yali and Wang, Shengjin},
	month = jul,
	year = {2020},
	note = {arXiv:1909.12605 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/GIAZ5Q45/Wang et al. - 2020 - Towards Real-Time Multi-Object Tracking.pdf:application/pdf},
}

@article{ciaparrone_deep_2020,
	title = {Deep {Learning} in {Video} {Multi}-{Object} {Tracking}: {A} {Survey}},
	volume = {381},
	issn = {09252312},
	shorttitle = {Deep {Learning} in {Video} {Multi}-{Object} {Tracking}},
	url = {http://arxiv.org/abs/1907.12740},
	doi = {10.1016/j.neucom.2019.11.023},
	abstract = {The problem of Multiple Object Tracking (MOT) consists in following the trajectory of different objects in a sequence, usually a video. In recent years, with the rise of Deep Learning, the algorithms that provide a solution to this problem have benefited from the representational power of deep models. This paper provides a comprehensive survey on works that employ Deep Learning models to solve the task of MOT on single-camera videos. Four main steps in MOT algorithms are identified, and an in-depth review of how Deep Learning was employed in each one of these stages is presented. A complete experimental comparison of the presented works on the three MOTChallenge datasets is also provided, identifying a number of similarities among the top-performing methods and presenting some possible future research directions.},
	urldate = {2023-01-18},
	journal = {Neurocomputing},
	author = {Ciaparrone, Gioele and Sánchez, Francisco Luque and Tabik, Siham and Troiano, Luigi and Tagliaferri, Roberto and Herrera, Francisco},
	month = mar,
	year = {2020},
	note = {arXiv:1907.12740 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {61--88},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/9Y8DKPFX/Ciaparrone et al. - 2020 - Deep Learning in Video Multi-Object Tracking A Su.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/PXXP6YCN/1907.html:text/html},
}

@article{guo_review_2022,
	title = {A {Review} of {Deep} {Learning}-{Based} {Visual} {Multi}-{Object} {Tracking} {Algorithms} for {Autonomous} {Driving}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/21/10741},
	doi = {10.3390/app122110741},
	abstract = {Multi-target tracking, a high-level vision job in computer vision, is crucial to understanding autonomous driving surroundings. Numerous top-notch multi-object tracking algorithms have evolved in recent years as a result of deep learning’s outstanding performance in the field of visual object tracking. There have been a number of evaluations on individual sub-problems, but none that cover the challenges, datasets, and algorithms associated with visual multi-object tracking in autonomous driving scenarios. In this research, we present an exhaustive study of algorithms in the field of visual multi-object tracking over the last ten years, based on a systematic review approach. The algorithm is broken down into three groups based on its structure: methods for tracking by detection (TBD), joint detection and tracking (JDT), and Transformer-based tracking. The research reveals that the TBD algorithm has a straightforward structure, however the correlation between its individual sub-modules is not very strong. To track multiple objects, the JDT technique combines multi-module joint learning with a deep network framework. Transformer-based algorithms have been explored over the past two years, and they have benefits in numerous assessment indicators, as well as tremendous research potential in the area of multi-object tracking. Theoretical support for algorithmic research in adjacent disciplines is provided by this paper. Additionally, the approach we discuss, which uses merely monocular cameras rather than sophisticated sensor fusion, is anticipated to pave the way for the quick creation of safe and affordable autonomous driving systems.},
	language = {en},
	number = {21},
	urldate = {2023-01-18},
	journal = {Applied Sciences},
	author = {Guo, Shuman and Wang, Shichang and Yang, Zhenzhong and Wang, Lijun and Zhang, Huawei and Guo, Pengyan and Gao, Yuguo and Guo, Junkai},
	month = jan,
	year = {2022},
	note = {Number: 21
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, autonomous driving, transformer, visual multi-object tracking},
	pages = {10741},
	file = {Full Text PDF:/home/pape/Zotero/storage/EBECIL23/Guo et al. - 2022 - A Review of Deep Learning-Based Visual Multi-Objec.pdf:application/pdf},
}

@article{powell_deeporganoid_2022,
	title = {{deepOrganoid}: {A} brightfield cell viability model for screening matrix-embedded organoids},
	volume = {27},
	issn = {2472-5552, 2472-5560},
	shorttitle = {{deepOrganoid}},
	url = {https://slas-discovery.org/article/S2472-5552(22)12520-7/fulltext#articleInformation},
	doi = {10.1016/j.slasd.2022.03.004},
	language = {English},
	number = {3},
	urldate = {2023-01-20},
	journal = {SLAS Discovery},
	author = {Powell, Reid T. and Moussalli, Micheline J. and Guo, Lei and Bae, Goeun and Singh, Pankaj and Stephan, Clifford and Shureiqi, Imad and Davies, Peter J.},
	month = apr,
	year = {2022},
	pmid = {35314378},
	note = {Publisher: Elsevier},
	pages = {175--184},
	file = {Full Text PDF:/home/pape/Zotero/storage/XTGI97T6/Powell et al. - 2022 - deepOrganoid A brightfield cell viability model f.pdf:application/pdf},
}

@article{hradecka_segmentation_2023,
	title = {Segmentation and {Tracking} of {Mammary} {Epithelial} {Organoids} in {Brightfield} {Microscopy}},
	volume = {42},
	issn = {1558-254X},
	doi = {10.1109/TMI.2022.3210714},
	abstract = {We present an automated and deep-learning-based workflow to quantitatively analyze the spatiotemporal development of mammary epithelial organoids in two-dimensional time-lapse (2D+t) sequences acquired using a brightfield microscope at high resolution. It involves a convolutional neural network (U-Net), purposely trained using computer-generated bioimage data created by a conditional generative adversarial network (pix2pixHD), to infer semantic segmentation, adaptive morphological filtering to identify organoid instances, and a shape-similarity-constrained, instance-segmentation-correcting tracking procedure to reliably cherry-pick the organoid instances of interest in time. By validating it using real 2D+t sequences of mouse mammary epithelial organoids of morphologically different phenotypes, we clearly demonstrate that the workflow achieves reliable segmentation and tracking performance, providing a reproducible and laborless alternative to manual analyses of the acquired bioimage data.},
	number = {1},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Hradecká, Lucia and Wiesner, David and Sumbal, Jakub and Koledova, Zuzana Sumbalova and Maška, Martin},
	month = jan,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {deep learning, Microscopy, Mice, Bioinformatics, brightfield microscopy, image synthesis, Manuals, Organoid segmentation, organoid tracking, Periodic structures, Pipelines, Shape},
	pages = {281--290},
}

@article{borten_automated_2018,
	title = {Automated brightfield morphometry of {3D} organoid populations by {OrganoSeg}},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-017-18815-8},
	doi = {10.1038/s41598-017-18815-8},
	abstract = {Spheroid and organoid cultures are powerful in vitro models for biology, but size and shape diversity within the culture is largely ignored. To streamline morphometric profiling, we developed OrganoSeg, an open-source software that integrates segmentation, filtering, and analysis for archived brightfield images of 3D culture. OrganoSeg is more accurate and flexible than existing platforms, and we illustrate its potential by stratifying 5167 breast-cancer spheroid and 5743 colon and colorectal-cancer organoid morphologies. Organoid transcripts grouped by morphometric signature heterogeneity were enriched for biological processes not prominent in the original RNA sequencing data. OrganoSeg enables complete, objective quantification of brightfield phenotypes, which may give insight into the molecular and multicellular mechanisms of organoid regulation.},
	language = {en},
	number = {1},
	urldate = {2023-01-20},
	journal = {Scientific Reports},
	author = {Borten, Michael A. and Bajikar, Sameer S. and Sasaki, Nobuo and Clevers, Hans and Janes, Kevin A.},
	month = mar,
	year = {2018},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Software, Cell biology},
	pages = {5319},
	file = {Full Text PDF:/home/pape/Zotero/storage/K55SM9U5/Borten et al. - 2018 - Automated brightfield morphometry of 3D organoid p.pdf:application/pdf},
}

@article{matthews_organoid_2022,
	title = {{OrganoID}: {A} versatile deep learning platform for tracking and analysis of single-organoid dynamics},
	volume = {18},
	issn = {1553-7358},
	shorttitle = {{OrganoID}},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010584},
	doi = {10.1371/journal.pcbi.1010584},
	abstract = {Organoids have immense potential as ex vivo disease models for drug discovery and personalized drug screening. Dynamic changes in individual organoid morphology, number, and size can indicate important drug responses. However, these metrics are difficult and labor-intensive to obtain for high-throughput image datasets. Here, we present OrganoID, a robust image analysis platform that automatically recognizes, labels, and tracks single organoids, pixel-by-pixel, in brightfield and phase-contrast microscopy experiments. The platform was trained on images of pancreatic cancer organoids and validated on separate images of pancreatic, lung, colon, and adenoid cystic carcinoma organoids, which showed excellent agreement with manual measurements of organoid count (95\%) and size (97\%) without any parameter adjustments. Single-organoid tracking accuracy remained above 89\% over a four-day time-lapse microscopy study. Automated single-organoid morphology analysis of a chemotherapy dose-response experiment identified strong dose effect sizes on organoid circularity, solidity, and eccentricity. OrganoID enables straightforward, detailed, and accurate image analysis to accelerate the use of organoids in high-throughput, data-intensive biomedical applications.},
	language = {en},
	number = {11},
	urldate = {2023-01-20},
	journal = {PLOS Computational Biology},
	author = {Matthews, Jonathan M. and Schuster, Brooke and Kashaf, Sara Saheb and Liu, Ping and Ben-Yishay, Rakefet and Ishay-Ronen, Dana and Izumchenko, Evgeny and Shen, Le and Weber, Christopher R. and Bielski, Margaret and Kupfer, Sonia S. and Bilgic, Mustafa and Rzhetsky, Andrey and Tay, Savaş},
	month = nov,
	year = {2022},
	note = {Publisher: Public Library of Science},
	keywords = {Bright field microscopy, Fluorescence imaging, Gastrointestinal tract, Image analysis, Neural networks, Organoids, Phase contrast microscopy, Pulmonary imaging},
	pages = {e1010584},
	file = {Full Text PDF:/home/pape/Zotero/storage/28CVSMUR/Matthews et al. - 2022 - OrganoID A versatile deep learning platform for t.pdf:application/pdf},
}

@misc{xie_segformer_2021,
	title = {{SegFormer}: {Simple} and {Efficient} {Design} for {Semantic} {Segmentation} with {Transformers}},
	shorttitle = {{SegFormer}},
	url = {http://arxiv.org/abs/2105.15203},
	doi = {10.48550/arXiv.2105.15203},
	abstract = {We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3\% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2\% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0\% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M. and Luo, Ping},
	month = oct,
	year = {2021},
	note = {arXiv:2105.15203 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/93B6VNF9/Xie et al. - 2021 - SegFormer Simple and Efficient Design for Semanti.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/F8R2RTFV/2105.html:text/html},
}

@misc{lu_pretrained_2021,
	title = {Pretrained {Transformers} as {Universal} {Computation} {Engines}},
	url = {http://arxiv.org/abs/2103.05247},
	doi = {10.48550/arXiv.2103.05247},
	abstract = {We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks.},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
	month = jun,
	year = {2021},
	note = {arXiv:2103.05247 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/A3EFYRHN/Lu et al. - 2021 - Pretrained Transformers as Universal Computation E.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/WYNIVXSL/2103.html:text/html},
}

@misc{shaib_expansion_2022,
	title = {Expansion microscopy at one nanometer resolution},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.08.03.502284v1},
	doi = {10.1101/2022.08.03.502284},
	abstract = {Fluorescence imaging is one of the most versatile and widely-used tools in biology1. Although techniques to overcome the diffraction barrier were introduced more than two decades ago, and the nominal attainable resolution kept improving to reach single-digit nm2,3, fluorescence microscopy still fails to image the morphology of single proteins or small molecular complexes, either purified or in a cellular context4,5. Here we report a solution to this problem, in the form of one-nanometer expansion (ONE) microscopy. We combined the 10-fold axial expansion of the specimen (1000-fold by volume) with a fluorescence fluctuation analysis6,7 to achieve resolutions down to 1 nm or better. We have successfully applied ONE microscopy to image cultured cells, tissues, viral particles, molecular complexes and single proteins. At the cellular level, using immunostaining, our technology revealed detailed nanoscale arrangements of synaptic proteins, including a quasi-regular organisation of PSD95 clusters. At the single molecule level, upon main chain fluorescent labelling, we could visualise the shape of individual membrane and soluble proteins. Moreover, conformational changes undergone by the {\textasciitilde}17 kDa protein calmodulin upon Ca2+ binding were readily observable. We could also image and classify molecular aggregates in cerebrospinal fluid samples from Parkinson’s Disease (PD) patients, which represents a promising new development towards an improved PD diagnosis. ONE microscopy is compatible with conventional microscopes and can be performed with the software we provide here as a free, open-source package. This technology bridges the gap between high-resolution structural biology techniques and light microscopy, and provides a new avenue for discoveries in biology and medicine.},
	language = {en},
	urldate = {2023-01-22},
	publisher = {bioRxiv},
	author = {Shaib, Ali H. and Chouaib, Abed Alrahman and Imani, Vanessa and Chowdhury, Rajdeep and Georgiev, Svilen Veselinov and Mougios, Nikolaos and Monga, Mehar and Reshetniak, Sofiia and Mihaylov, Daniel and Chen, Han and Fatehbasharzad, Parisa and Crzan, Dagmar and Saal, Kim-Ann and Trenkwalder, Claudia and Mollenhauer, Brit and Outeiro, Tiago F. and Preobraschenski, Julia and Becherer, Ute and Moser, Tobias and Boyden, Edward S. and Aricescu, A. Radu and Sauer, Markus and Opazo, Felipe and Rizzoli, Silvio O.},
	month = aug,
	year = {2022},
	note = {Pages: 2022.08.03.502284
Section: New Results},
	file = {Full Text PDF:/home/pape/Zotero/storage/5RTCDV7Y/Shaib et al. - 2022 - Expansion microscopy at one nanometer resolution.pdf:application/pdf},
}

@article{turk_promise_2020,
	title = {The promise and the challenges of cryo-electron tomography},
	volume = {594},
	copyright = {© 2020 The Authors. FEBS Letters published by John Wiley \& Sons Ltd on behalf of Federation of European Biochemical Societies},
	issn = {1873-3468},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1873-3468.13948},
	doi = {10.1002/1873-3468.13948},
	abstract = {Structural biologists have traditionally approached cellular complexity in a reductionist manner in which the cellular molecular components are fractionated and purified before being studied individually. This ‘divide and conquer’ approach has been highly successful. However, awareness has grown in recent years that biological functions can rarely be attributed to individual macromolecules. Most cellular functions arise from their concerted action, and there is thus a need for methods enabling structural studies performed in situ, ideally in unperturbed cellular environments. Cryo-electron tomography (Cryo-ET) combines the power of 3D molecular-level imaging with the best structural preservation that is physically possible to achieve. Thus, it has a unique potential to reveal the supramolecular architecture or ‘molecular sociology’ of cells and to discover the unexpected. Here, we review state-of-the-art Cryo-ET workflows, provide examples of biological applications, and discuss what is needed to realize the full potential of Cryo-ET.},
	language = {en},
	number = {20},
	urldate = {2023-01-24},
	journal = {FEBS Letters},
	author = {Turk, Martin and Baumeister, Wolfgang},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/1873-3468.13948},
	keywords = {cellular structural biology, correlative light-electron microscopy, cryo-electron tomography, image processing workflow, sample preparation workflows, structural biology in situ},
	pages = {3243--3261},
	file = {Full Text PDF:/home/pape/Zotero/storage/G32B86DS/Turk and Baumeister - 2020 - The promise and the challenges of cryo-electron to.pdf:application/pdf},
}

@misc{hatamizadeh_swin_2022,
	title = {Swin {UNETR}: {Swin} {Transformers} for {Semantic} {Segmentation} of {Brain} {Tumors} in {MRI} {Images}},
	shorttitle = {Swin {UNETR}},
	url = {http://arxiv.org/abs/2201.01266},
	doi = {10.48550/arXiv.2201.01266},
	abstract = {Semantic segmentation of brain tumors is a fundamental medical image analysis task involving multiple MRI imaging modalities that can assist clinicians in diagnosing the patient and successively studying the progression of the malignant entity. In recent years, Fully Convolutional Neural Networks (FCNNs) approaches have become the de facto standard for 3D medical image segmentation. The popular "U-shaped" network architecture has achieved state-of-the-art performance benchmarks on different 2D and 3D semantic segmentation tasks and across various imaging modalities. However, due to the limited kernel size of convolution layers in FCNNs, their performance of modeling long-range information is sub-optimal, and this can lead to deficiencies in the segmentation of tumors with variable sizes. On the other hand, transformer models have demonstrated excellent capabilities in capturing such long-range information in multiple domains, including natural language processing and computer vision. Inspired by the success of vision transformers and their variants, we propose a novel segmentation model termed Swin UNEt TRansformers (Swin UNETR). Specifically, the task of 3D brain tumor semantic segmentation is reformulated as a sequence to sequence prediction problem wherein multi-modal input data is projected into a 1D sequence of embedding and used as an input to a hierarchical Swin transformer as the encoder. The swin transformer encoder extracts features at five different resolutions by utilizing shifted windows for computing self-attention and is connected to an FCNN-based decoder at each resolution via skip connections. We have participated in BraTS 2021 segmentation challenge, and our proposed model ranks among the top-performing approaches in the validation phase. Code: https://monai.io/research/swin-unetr},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {Hatamizadeh, Ali and Nath, Vishwesh and Tang, Yucheng and Yang, Dong and Roth, Holger and Xu, Daguang},
	month = jan,
	year = {2022},
	note = {arXiv:2201.01266 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/5U6Y5FSB/Hatamizadeh et al. - 2022 - Swin UNETR Swin Transformers for Semantic Segment.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/7PSL9W75/2201.html:text/html},
}

@misc{hatamizadeh_unetr_2021,
	title = {{UNETR}: {Transformers} for {3D} {Medical} {Image} {Segmentation}},
	shorttitle = {{UNETR}},
	url = {http://arxiv.org/abs/2103.10504},
	doi = {10.48550/arXiv.2103.10504},
	abstract = {Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful "U-shaped" network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard. Code: https://monai.io/research/unetr},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {Hatamizadeh, Ali and Tang, Yucheng and Nath, Vishwesh and Yang, Dong and Myronenko, Andriy and Landman, Bennett and Roth, Holger and Xu, Daguang},
	month = oct,
	year = {2021},
	note = {arXiv:2103.10504 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/IZ6HQNIR/Hatamizadeh et al. - 2021 - UNETR Transformers for 3D Medical Image Segmentat.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/C2LPDXP2/2103.html:text/html},
}

@article{de_teresa-trueba_convolutional_2023,
	title = {Convolutional networks for supervised mining of molecular patterns within cellular context},
	copyright = {2023 The Author(s)},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-022-01746-2},
	doi = {10.1038/s41592-022-01746-2},
	abstract = {Cryo-electron tomograms capture a wealth of structural information on the molecular constituents of cells and tissues. We present DeePiCt (deep picker in context), an open-source deep-learning framework for supervised segmentation and macromolecular complex localization in cryo-electron tomography. To train and benchmark DeePiCt on experimental data, we comprehensively annotated 20 tomograms of Schizosaccharomyces pombe for ribosomes, fatty acid synthases, membranes, nuclear pore complexes, organelles, and cytosol. By comparing DeePiCt to state-of-the-art approaches on this dataset, we show its unique ability to identify low-abundance and low-density complexes. We use DeePiCt to study compositionally distinct subpopulations of cellular ribosomes, with emphasis on their contextual association with mitochondria and the endoplasmic reticulum. Finally, applying pre-trained networks to a HeLa cell tomogram demonstrates that DeePiCt achieves high-quality predictions in unseen datasets from different biological species in a matter of minutes. The comprehensively annotated experimental data and pre-trained networks are provided for immediate use by the community.},
	language = {en},
	urldate = {2023-01-27},
	journal = {Nature Methods},
	author = {de Teresa-Trueba, Irene and Goetz, Sara K. and Mattausch, Alexander and Stojanovska, Frosina and Zimmerli, Christian E. and Toro-Nahuelpan, Mauricio and Cheng, Dorothy W. C. and Tollervey, Fergus and Pape, Constantin and Beck, Martin and Diz-Muñoz, Alba and Kreshuk, Anna and Mahamid, Julia and Zaugg, Judith B.},
	month = jan,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Image processing, Data mining, Protein structure predictions, Molecular imaging},
	pages = {1--11},
	file = {Full Text PDF:/home/pape/Zotero/storage/AX3PJ4SW/de Teresa-Trueba et al. - 2023 - Convolutional networks for supervised mining of mo.pdf:application/pdf},
}

@article{madani_large_2023,
	title = {Large language models generate functional protein sequences across diverse families},
	copyright = {2023 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-022-01618-2},
	doi = {10.1038/s41587-022-01618-2},
	abstract = {Deep-learning language models have shown promise in various biotechnological applications, including protein design and engineering. Here we describe ProGen, a language model that can generate protein sequences with a predictable function across large protein families, akin to generating grammatically and semantically correct natural language sentences on diverse topics. The model was trained on 280 million protein sequences from {\textgreater}19,000 families and is augmented with control tags specifying protein properties. ProGen can be further fine-tuned to curated sequences and tags to improve controllable generation performance of proteins from families with sufficient homologous samples. Artificial proteins fine-tuned to five distinct lysozyme families showed similar catalytic efficiencies as natural lysozymes, with sequence identity to natural proteins as low as 31.4\%. ProGen is readily adapted to diverse protein families, as we demonstrate with chorismate mutase and malate dehydrogenase.},
	language = {en},
	urldate = {2023-01-27},
	journal = {Nature Biotechnology},
	author = {Madani, Ali and Krause, Ben and Greene, Eric R. and Subramanian, Subu and Mohr, Benjamin P. and Holton, James M. and Olmos, Jose Luis and Xiong, Caiming and Sun, Zachary Z. and Socher, Richard and Fraser, James S. and Naik, Nikhil},
	month = jan,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Machine learning, Enzymes, Proteomics},
	pages = {1--8},
	file = {Full Text PDF:/home/pape/Zotero/storage/TRT6PTAY/Madani et al. - 2023 - Large language models generate functional protein .pdf:application/pdf},
}

@misc{lin_evolutionary-scale_2022,
	title = {Evolutionary-scale prediction of atomic level protein structure with a language model},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.07.20.500902v3},
	doi = {10.1101/2022.07.20.500902},
	abstract = {Artificial intelligence has the potential to open insight into the structure of proteins at the scale of evolution. It has only recently been possible to extend protein structure prediction to two hundred million cataloged proteins. Characterizing the structures of the exponentially growing billions of protein sequences revealed by large scale gene sequencing experiments would necessitate a break-through in the speed of folding. Here we show that direct inference of structure from primary sequence using a large language model enables an order of magnitude speed-up in high resolution structure prediction. Leveraging the insight that language models learn evolutionary patterns across millions of sequences, we train models up to 15B parameters, the largest language model of proteins to date. As the language models are scaled they learn information that enables prediction of the three-dimensional structure of a protein at the resolution of individual atoms. This results in prediction that is up to 60x faster than state-of-the-art while maintaining resolution and accuracy. Building on this, we present the ESM Metage-nomic Atlas. This is the first large-scale structural characterization of metagenomic proteins, with more than 617 million structures. The atlas reveals more than 225 million high confidence predictions, including millions whose structures are novel in comparison with experimentally determined structures, giving an unprecedented view into the vast breadth and diversity of the structures of some of the least understood proteins on earth.},
	language = {en},
	urldate = {2023-01-27},
	publisher = {bioRxiv},
	author = {Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and Verkuil, Robert and Kabeli, Ori and Shmueli, Yaniv and Costa, Allan dos Santos and Fazel-Zarandi, Maryam and Sercu, Tom and Candido, Salvatore and Rives, Alexander},
	month = dec,
	year = {2022},
	note = {Pages: 2022.07.20.500902
Section: New Results},
	file = {Full Text PDF:/home/pape/Zotero/storage/7RDKVB4U/Lin et al. - 2022 - Evolutionary-scale prediction of atomic level prot.pdf:application/pdf},
}

@misc{defazio_learning-rate-free_2023,
	title = {Learning-{Rate}-{Free} {Learning} by {D}-{Adaptation}},
	url = {http://arxiv.org/abs/2301.07733},
	doi = {10.48550/arXiv.2301.07733},
	abstract = {The speed of gradient descent for convex Lipschitz functions is highly dependent on the choice of learning rate. Setting the learning rate to achieve the optimal convergence rate requires knowing the distance D from the initial point to the solution set. In this work, we describe a single-loop method, with no back-tracking or line searches, which does not require knowledge of \$D\$ yet asymptotically achieves the optimal rate of convergence for the complexity class of convex Lipschitz functions. Our approach is the first parameter-free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems. Our method is practical, efficient and requires no additional function value or gradient evaluations each step. An open-source implementation is available (https://github.com/facebookresearch/dadaptation).},
	urldate = {2023-01-27},
	publisher = {arXiv},
	author = {Defazio, Aaron and Mishchenko, Konstantin},
	month = jan,
	year = {2023},
	note = {arXiv:2301.07733 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/DNXDWXRS/Defazio and Mishchenko - 2023 - Learning-Rate-Free Learning by D-Adaptation.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/76NSI56X/2301.html:text/html},
}

@article{mei_radimagenet_2022,
	title = {{RadImageNet}: {An} {Open} {Radiologic} {Deep} {Learning} {Research} {Dataset} for {Effective} {Transfer} {Learning}},
	volume = {4},
	shorttitle = {{RadImageNet}},
	url = {https://pubs.rsna.org/doi/10.1148/ryai.210315},
	doi = {10.1148/ryai.210315},
	abstract = {Purpose

To demonstrate the value of pretraining with millions of radiologic images compared with ImageNet photographic images on downstream medical applications when using transfer learning.

Materials and Methods

This retrospective study included patients who underwent a radiologic study between 2005 and 2020 at an outpatient imaging facility. Key images and associated labels from the studies were retrospectively extracted from the original study interpretation. These images were used for RadImageNet model training with random weight initiation. The RadImageNet models were compared with ImageNet models using the area under the receiver operating characteristic curve (AUC) for eight classification tasks and using Dice scores for two segmentation problems.

Results

The RadImageNet database consists of 1.35 million annotated medical images in 131 872 patients who underwent CT, MRI, and US for musculoskeletal, neurologic, oncologic, gastrointestinal, endocrine, abdominal, and pulmonary pathologic conditions. For transfer learning tasks on small datasets—thyroid nodules (US), breast masses (US), anterior cruciate ligament injuries (MRI), and meniscal tears (MRI)—the RadImageNet models demonstrated a significant advantage (P {\textless} .001) to ImageNet models (9.4\%, 4.0\%, 4.8\%, and 4.5\% AUC improvements, respectively). For larger datasets—pneumonia (chest radiography), COVID-19 (CT), SARS-CoV-2 (CT), and intracranial hemorrhage (CT)—the RadImageNet models also illustrated improved AUC (P {\textless} .001) by 1.9\%, 6.1\%, 1.7\%, and 0.9\%, respectively. Additionally, lesion localizations of the RadImageNet models were improved by 64.6\% and 16.4\% on thyroid and breast US datasets, respectively.

Conclusion

RadImageNet pretrained models demonstrated better interpretability compared with ImageNet models, especially for smaller radiologic datasets.

Keywords: CT, MR Imaging, US, Head/Neck, Thorax, Brain/Brain Stem, Evidence-based Medicine, Computer Applications–General (Informatics)

Supplemental material is available for this article.

Published under a CC BY 4.0 license.

See also the commentary by Cadrin-Chênevert in this issue.},
	number = {5},
	urldate = {2023-01-27},
	journal = {Radiology: Artificial Intelligence},
	author = {Mei, Xueyan and Liu, Zelong and Robson, Philip M. and Marinelli, Brett and Huang, Mingqian and Doshi, Amish and Jacobi, Adam and Cao, Chendi and Link, Katherine E. and Yang, Thomas and Wang, Ying and Greenspan, Hayit and Deyer, Timothy and Fayad, Zahi A. and Yang, Yang},
	month = sep,
	year = {2022},
	note = {Publisher: Radiological Society of North America},
	pages = {e210315},
	file = {Full Text PDF:/home/pape/Zotero/storage/NURIV5JT/Mei et al. - 2022 - RadImageNet An Open Radiologic Deep Learning Rese.pdf:application/pdf},
}

@article{luo_biogpt_2022,
	title = {{BioGPT}: {Generative} {Pre}-trained {Transformer} for {Biomedical} {Text} {Generation} and {Mining}},
	volume = {23},
	issn = {1467-5463, 1477-4054},
	shorttitle = {{BioGPT}},
	url = {http://arxiv.org/abs/2210.10341},
	doi = {10.1093/bib/bbac409},
	abstract = {Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e., BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98\%, 38.42\% and 40.76\% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks respectively, and 78.2\% accuracy on PubMedQA, creating a new record. Our larger model BioGPT-Large achieves 81.0\% on PubMedQA. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms. Code is available at https://github.com/microsoft/BioGPT.},
	number = {6},
	urldate = {2023-01-28},
	journal = {Briefings in Bioinformatics},
	author = {Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
	month = nov,
	year = {2022},
	note = {arXiv:2210.10341 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {bbac409},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/A992WATI/Luo et al. - 2022 - BioGPT Generative Pre-trained Transformer for Bio.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/8AHBJ2DL/2210.html:text/html},
}

@misc{dalla-torre_nucleotide_2023,
	title = {The {Nucleotide} {Transformer}: {Building} and {Evaluating} {Robust} {Foundation} {Models} for {Human} {Genomics}},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	shorttitle = {The {Nucleotide} {Transformer}},
	url = {https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1},
	doi = {10.1101/2023.01.11.523679},
	abstract = {Closing the gap between measurable genetic information and observable traits is a longstanding challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learnings between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, integrating information from 3,202 diverse human genomes, as well as 850 genomes from a wide range of species, including model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the representations alone match or outperform specialized methods on 11 of 18 prediction tasks, and up to 15 after fine-tuning. Despite no supervision, the transformer models learnt to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model representations alone can improve the prioritization of functional genetic variants. The training and application of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence alone.},
	language = {en},
	urldate = {2023-01-28},
	publisher = {bioRxiv},
	author = {Dalla-Torre, Hugo and Gonzalez, Liam and Revilla, Javier Mendoza and Carranza, Nicolas Lopez and Grzywaczewski, Adam Henryk and Oteri, Francesco and Dallago, Christian and Trop, Evan and Sirelkhatim, Hassan and Richard, Guillaume and Skwark, Marcin and Beguir, Karim and Lopez, Marie and Pierrot, Thomas},
	month = jan,
	year = {2023},
	note = {Pages: 2023.01.11.523679
Section: New Results},
	file = {Full Text PDF:/home/pape/Zotero/storage/HWGRRC4S/Dalla-Torre et al. - 2023 - The Nucleotide Transformer Building and Evaluatin.pdf:application/pdf},
}

@article{guo_pct_2021-1,
	title = {{PCT}: {Point} cloud transformer},
	volume = {7},
	issn = {2096-0433, 2096-0662},
	shorttitle = {{PCT}},
	url = {http://arxiv.org/abs/2012.09688},
	doi = {10.1007/s41095-021-0229-5},
	abstract = {The irregular domain and lack of ordering make it challenging to design deep neural networks for point cloud processing. This paper presents a novel framework named Point Cloud Transformer(PCT) for point cloud learning. PCT is based on Transformer, which achieves huge success in natural language processing and displays great potential in image processing. It is inherently permutation invariant for processing a sequence of points, making it well-suited for point cloud learning. To better capture local context within the point cloud, we enhance input embedding with the support of farthest point sampling and nearest neighbor search. Extensive experiments demonstrate that the PCT achieves the state-of-the-art performance on shape classification, part segmentation and normal estimation tasks.},
	number = {2},
	urldate = {2023-01-29},
	journal = {Computational Visual Media},
	author = {Guo, Meng-Hao and Cai, Jun-Xiong and Liu, Zheng-Ning and Mu, Tai-Jiang and Martin, Ralph R. and Hu, Shi-Min},
	month = jun,
	year = {2021},
	note = {arXiv:2012.09688 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {187--199},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/3IIVGEBE/Guo et al. - 2021 - PCT Point cloud transformer.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/4F2K6GMM/2012.html:text/html},
}

@misc{lu_pretrained_2021-1,
	title = {Pretrained {Transformers} as {Universal} {Computation} {Engines}},
	url = {http://arxiv.org/abs/2103.05247},
	doi = {10.48550/arXiv.2103.05247},
	abstract = {We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks.},
	urldate = {2023-01-29},
	publisher = {arXiv},
	author = {Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
	month = jun,
	year = {2021},
	note = {arXiv:2103.05247 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/YYK7UVRB/Lu et al. - 2021 - Pretrained Transformers as Universal Computation E.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/TN6BJLC3/2103.html:text/html},
}

@article{bray_cell_2016,
	title = {Cell {Painting}, a high-content image-based assay for morphological profiling using multiplexed fluorescent dyes},
	volume = {11},
	copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1750-2799},
	url = {https://www.nature.com/articles/nprot.2016.105},
	doi = {10.1038/nprot.2016.105},
	abstract = {Cell Painting is a high-content screening assay that uses multiplexed fluorescent dyes for image-based profiling of ∼1,500 morphological features. Image analysis with CellProfiler automatically identifies and extracts data from individual cells.},
	language = {en},
	number = {9},
	urldate = {2023-02-05},
	journal = {Nature Protocols},
	author = {Bray, Mark-Anthony and Singh, Shantanu and Han, Han and Davis, Chadwick T. and Borgeson, Blake and Hartland, Cathy and Kost-Alimova, Maria and Gustafsdottir, Sigrun M. and Gibson, Christopher C. and Carpenter, Anne E.},
	month = sep,
	year = {2016},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Image processing, Fluorescence imaging, High-throughput screening, Phenotypic screening},
	pages = {1757--1774},
	file = {Full Text PDF:/home/pape/Zotero/storage/SDPDDBJC/Bray et al. - 2016 - Cell Painting, a high-content image-based assay fo.pdf:application/pdf},
}

@article{godinez_multi-scale_2017,
	title = {A multi-scale convolutional neural network for phenotyping high-content cellular images},
	volume = {33},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btx069},
	doi = {10.1093/bioinformatics/btx069},
	abstract = {Identifying phenotypes based on high-content cellular images is challenging. Conventional image analysis pipelines for phenotype identification comprise multiple independent steps, with each step requiring method customization and adjustment of multiple parameters. Here, we present an approach based on a multi-scale convolutional neural network (M-CNN) that classifies, in a single cohesive step, cellular images into phenotypes by using directly and solely the images’ pixel intensity values. The only parameters in the approach are the weights of the neural network, which are automatically optimized based on training images. The approach requires no a priori knowledge or manual customization, and is applicable to single- or multi-channel images displaying single or multiple cells. We evaluated the classification performance of the approach on eight diverse benchmark datasets. The approach yielded overall a higher classification accuracy compared with state-of-the-art results, including those of other deep CNN architectures. In addition to using the network to simply obtain a yes-or-no prediction for a given phenotype, we use the probability outputs calculated by the network to quantitatively describe the phenotypes. This study shows that these probability values correlate with chemical treatment concentrations. This finding validates further our approach and enables chemical treatment potency estimation via CNNs.The network specifications and solver definitions are provided in Supplementary Software 1.Supplementary data are available at Bioinformatics online.},
	number = {13},
	urldate = {2023-02-05},
	journal = {Bioinformatics},
	author = {Godinez, William J and Hossain, Imtiaz and Lazic, Stanley E and Davies, John W and Zhang, Xian},
	month = jul,
	year = {2017},
	pages = {2010--2019},
	file = {Full Text PDF:/home/pape/Zotero/storage/G6GFECH2/Godinez et al. - 2017 - A multi-scale convolutional neural network for phe.pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/22K2GGJV/2997285.html:text/html},
}

@misc{caron_deep_2019,
	title = {Deep {Clustering} for {Unsupervised} {Learning} of {Visual} {Features}},
	url = {http://arxiv.org/abs/1807.05520},
	doi = {10.48550/arXiv.1807.05520},
	abstract = {Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks.},
	urldate = {2023-02-05},
	publisher = {arXiv},
	author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
	month = mar,
	year = {2019},
	note = {arXiv:1807.05520 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/CP27IVZM/Caron et al. - 2019 - Deep Clustering for Unsupervised Learning of Visua.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/NMEXNLDI/1807.html:text/html},
}

@incollection{perakis_contrastive_2021,
	title = {Contrastive {Learning} of {Single}-{Cell} {Phenotypic} {Representations} for {Treatment} {Classification}},
	volume = {12966},
	url = {http://arxiv.org/abs/2103.16670},
	abstract = {Learning robust representations to discriminate cell phenotypes based on microscopy images is important for drug discovery. Drug development efforts typically analyse thousands of cell images to screen for potential treatments. Early works focus on creating hand-engineered features from these images or learn such features with deep neural networks in a fully or weakly-supervised framework. Both require prior knowledge or labelled datasets. Therefore, subsequent works propose unsupervised approaches based on generative models to learn these representations. Recently, representations learned with self-supervised contrastive loss-based methods have yielded state-of-the-art results on various imaging tasks compared to earlier unsupervised approaches. In this work, we leverage a contrastive learning framework to learn appropriate representations from single-cell fluorescent microscopy images for the task of Mechanism-of-Action classification. The proposed work is evaluated on the annotated BBBC021 dataset, and we obtain state-of-the-art results in NSC, NCSB and drop metrics for an unsupervised approach. We observe an improvement of 10\% in NCSB accuracy and 11\% in NSC-NSCB drop over the previously best unsupervised method. Moreover, the performance of our unsupervised approach ties with the best supervised approach. Additionally, we observe that our framework performs well even without post-processing, unlike earlier methods. With this, we conclude that one can learn robust cell representations with contrastive learning.},
	urldate = {2023-02-05},
	author = {Perakis, Alexis and Gorji, Ali and Jain, Samriddhi and Chaitanya, Krishna and Rizza, Simone and Konukoglu, Ender},
	year = {2021},
	doi = {10.1007/978-3-030-87589-3_58},
	note = {arXiv:2103.16670 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	pages = {565--575},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/8PCPV867/Perakis et al. - 2021 - Contrastive Learning of Single-Cell Phenotypic Rep.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/VDMJ7YDE/2103.html:text/html},
}

@article{driehuis_establishment_2020,
	title = {Establishment of patient-derived cancer organoids for drug-screening applications},
	volume = {15},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1750-2799},
	url = {https://www.nature.com/articles/s41596-020-0379-4},
	doi = {10.1038/s41596-020-0379-4},
	abstract = {Adult stem cell–based organoid technology is a versatile tool for the generation and long-term maintenance of near-native 3D epithelial tissues in vitro. The generation of cancer organoids from primary patient material enables a range of therapeutic agents to be tested in the resulting organoid cultures. Patient-derived cancer organoids therefore hold great promise for personalized medicine. Here, we provide an overview of the protocols used by different groups to establish organoids from various epithelial tissues and cancers, plus the different protocols subsequently used to test the in vitro therapy sensitivity of these patient-derived organoids. We also provide an in-depth protocol for the generation of head and neck squamous cell carcinoma organoids and their subsequent use in semi-automated therapy screens. Establishment of organoids and subsequent screening can be performed within 3 months, although this timeline is highly dependent on a.o. starting material and the number of therapies tested. The protocol provided may serve as a reference to successfully establish organoids from other cancer types and perform drug screenings thereof.},
	language = {en},
	number = {10},
	urldate = {2023-02-05},
	journal = {Nature Protocols},
	author = {Driehuis, Else and Kretzschmar, Kai and Clevers, Hans},
	month = oct,
	year = {2020},
	note = {Number: 10
Publisher: Nature Publishing Group},
	keywords = {Cancer models, Drug screening, Oral cancer, Tissue culture},
	pages = {3380--3409},
	file = {Full Text PDF:/home/pape/Zotero/storage/S4NH828P/Driehuis et al. - 2020 - Establishment of patient-derived cancer organoids .pdf:application/pdf},
}

@misc{feinberg_sketchy_2023,
	title = {Sketchy: {Memory}-efficient {Adaptive} {Regularization} with {Frequent} {Directions}},
	shorttitle = {Sketchy},
	url = {http://arxiv.org/abs/2302.03764},
	doi = {10.48550/arXiv.2302.03764},
	abstract = {Adaptive regularization methods that exploit more than the diagonal entries exhibit state of the art performance for many tasks, but can be prohibitive in terms of memory and running time. We find the spectra of the Kronecker-factored gradient covariance matrix in deep learning (DL) training tasks are concentrated on a small leading eigenspace that changes throughout training, motivating a low-rank sketching approach. We describe a generic method for reducing memory and compute requirements of maintaining a matrix preconditioner using the Frequent Directions (FD) sketch. Our technique allows interpolation between resource requirements and the degradation in regret guarantees with rank \$k\$: in the online convex optimization (OCO) setting over dimension \$d\$, we match full-matrix \$d{\textasciicircum}2\$ memory regret using only \$dk\$ memory up to additive error in the bottom \$d-k\$ eigenvalues of the gradient covariance. Further, we show extensions of our work to Shampoo, placing the method on the memory-quality Pareto frontier of several large scale benchmarks.},
	urldate = {2023-02-11},
	publisher = {arXiv},
	author = {Feinberg, Vladimir and Chen, Xinyi and Sun, Y. Jennifer and Anil, Rohan and Hazan, Elad},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03764 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/5UN8RCSS/Feinberg et al. - 2023 - Sketchy Memory-efficient Adaptive Regularization .pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/9VXVRNHL/2302.html:text/html},
}

@misc{yu_coca_2022,
	title = {{CoCa}: {Contrastive} {Captioners} are {Image}-{Text} {Foundation} {Models}},
	shorttitle = {{CoCa}},
	url = {http://arxiv.org/abs/2205.01917},
	doi = {10.48550/arXiv.2205.01917},
	abstract = {Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3\% zero-shot top-1 accuracy, 90.6\% with a frozen encoder and learned classification head, and new state-of-the-art 91.0\% top-1 accuracy on ImageNet with a finetuned encoder.},
	urldate = {2023-02-11},
	publisher = {arXiv},
	author = {Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
	month = jun,
	year = {2022},
	note = {arXiv:2205.01917 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/JJLMSSWY/Yu et al. - 2022 - CoCa Contrastive Captioners are Image-Text Founda.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/XDSYUP9P/2205.html:text/html},
}

@misc{wang_dense_2021,
	title = {Dense {Contrastive} {Learning} for {Self}-{Supervised} {Visual} {Pre}-{Training}},
	url = {http://arxiv.org/abs/2011.09157},
	doi = {10.48550/arXiv.2011.09157},
	abstract = {To date, most existing self-supervised learning methods are designed and optimized for image classification. These pre-trained models can be sub-optimal for dense prediction tasks due to the discrepancy between image-level prediction and pixel-level prediction. To fill this gap, we aim to design an effective, dense self-supervised learning method that directly works at the level of pixels (or local features) by taking into account the correspondence between local features. We present dense contrastive learning, which implements self-supervised learning by optimizing a pairwise contrastive (dis)similarity loss at the pixel level between two views of input images. Compared to the baseline method MoCo-v2, our method introduces negligible computation overhead (only {\textless}1\% slower), but demonstrates consistently superior performance when transferring to downstream dense prediction tasks including object detection, semantic segmentation and instance segmentation; and outperforms the state-of-the-art methods by a large margin. Specifically, over the strong MoCo-v2 baseline, our method achieves significant improvements of 2.0\% AP on PASCAL VOC object detection, 1.1\% AP on COCO object detection, 0.9\% AP on COCO instance segmentation, 3.0\% mIoU on PASCAL VOC semantic segmentation and 1.8\% mIoU on Cityscapes semantic segmentation. Code is available at: https://git.io/AdelaiDet},
	urldate = {2023-02-11},
	publisher = {arXiv},
	author = {Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},
	month = apr,
	year = {2021},
	note = {arXiv:2011.09157 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/AS56575X/Wang et al. - 2021 - Dense Contrastive Learning for Self-Supervised Vis.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/95TY9KGI/2011.html:text/html},
}

@misc{yuan_florence_2021,
	title = {Florence: {A} {New} {Foundation} {Model} for {Computer} {Vision}},
	shorttitle = {Florence},
	url = {http://arxiv.org/abs/2111.11432},
	doi = {10.48550/arXiv.2111.11432},
	abstract = {Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.},
	urldate = {2023-02-11},
	publisher = {arXiv},
	author = {Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and Liu, Ce and Liu, Mengchen and Liu, Zicheng and Lu, Yumao and Shi, Yu and Wang, Lijuan and Wang, Jianfeng and Xiao, Bin and Xiao, Zhen and Yang, Jianwei and Zeng, Michael and Zhou, Luowei and Zhang, Pengchuan},
	month = nov,
	year = {2021},
	note = {arXiv:2111.11432 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/IFZVD5ZA/Yuan et al. - 2021 - Florence A New Foundation Model for Computer Visi.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/V2U8T6NB/2111.html:text/html},
}

@inproceedings{van_gansbeke_unsupervised_2021,
	title = {Unsupervised {Semantic} {Segmentation} by {Contrasting} {Object} {Mask} {Proposals}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Van_Gansbeke_Unsupervised_Semantic_Segmentation_by_Contrasting_Object_Mask_Proposals_ICCV_2021_paper.html?ref=https://coder.social},
	language = {en},
	urldate = {2023-02-11},
	author = {Van Gansbeke, Wouter and Vandenhende, Simon and Georgoulis, Stamatios and Van Gool, Luc},
	year = {2021},
	pages = {10052--10062},
	file = {Full Text PDF:/home/pape/Zotero/storage/EM8AKT4H/Van Gansbeke et al. - 2021 - Unsupervised Semantic Segmentation by Contrasting .pdf:application/pdf},
}

@inproceedings{wang_exploring_2021,
	title = {Exploring {Cross}-{Image} {Pixel} {Contrast} for {Semantic} {Segmentation}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Exploring_Cross-Image_Pixel_Contrast_for_Semantic_Segmentation_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-02-11},
	author = {Wang, Wenguan and Zhou, Tianfei and Yu, Fisher and Dai, Jifeng and Konukoglu, Ender and Van Gool, Luc},
	year = {2021},
	pages = {7303--7313},
	file = {Full Text PDF:/home/pape/Zotero/storage/BK2LGHR9/Wang et al. - 2021 - Exploring Cross-Image Pixel Contrast for Semantic .pdf:application/pdf},
}

@misc{hoyer_mic_2022,
	title = {{MIC}: {Masked} {Image} {Consistency} for {Context}-{Enhanced} {Domain} {Adaptation}},
	shorttitle = {{MIC}},
	url = {http://arxiv.org/abs/2212.01322},
	doi = {10.48550/arXiv.2212.01322},
	abstract = {In unsupervised domain adaptation (UDA), a model trained on source data (e.g. synthetic) is adapted to target data (e.g. real-world) without access to target annotation. Most previous UDA methods struggle with classes that have a similar visual appearance on the target domain as no ground truth is available to learn the slight appearance differences. To address this problem, we propose a Masked Image Consistency (MIC) module to enhance UDA by learning spatial context relations of the target domain as additional clues for robust visual recognition. MIC enforces the consistency between predictions of masked target images, where random patches are withheld, and pseudo-labels that are generated based on the complete image by an exponential moving average teacher. To minimize the consistency loss, the network has to learn to infer the predictions of the masked regions from their context. Due to its simple and universal concept, MIC can be integrated into various UDA methods across different visual recognition tasks such as image classification, semantic segmentation, and object detection. MIC significantly improves the state-of-the-art performance across the different recognition tasks for synthetic-to-real, day-to-nighttime, and clear-to-adverse-weather UDA. For instance, MIC achieves an unprecedented UDA performance of 75.9 mIoU and 92.8\% on GTA-to-Cityscapes and VisDA-2017, respectively, which corresponds to an improvement of +2.1 and +3.0 percent points over the previous state of the art. The implementation is available at https://github.com/lhoyer/MIC.},
	urldate = {2023-02-11},
	publisher = {arXiv},
	author = {Hoyer, Lukas and Dai, Dengxin and Wang, Haoran and Van Gool, Luc},
	month = dec,
	year = {2022},
	note = {arXiv:2212.01322 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/N9JUT2HM/Hoyer et al. - 2022 - MIC Masked Image Consistency for Context-Enhanced.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/F6TRZUC9/2212.html:text/html},
}

@article{wang_looking_2022,
	title = {Looking {Beyond} {Single} {Images} for {Weakly} {Supervised} {Semantic} {Segmentation} {Learning}},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2022.3168530},
	abstract = {This article studies the problem of learning weakly supervised semantic segmentation (WSSS) from image-level supervision only. Rather than previous efforts that primarily focus on intra-image information, we address the value of cross-image semantic relations for comprehensive object pattern mining. To achieve this, two neural co-attentions are incorporated into the classifier to complimentarily capture cross-image semantic similarities and differences. In particular, given a pair of training images, one co-attention enforces the classifier to recognize the common semantics from co-attentive objects, while the other one, called contrastive co-attention, drives the classifier to identify the unique semantics from the rest, unshared objects. This helps the classifier discover more object patterns and better ground semantics in image regions. More importantly, our algorithm provides a unified framework that handles well different WSSS settings, i.e., learning WSSS with (1) precise image-level supervision only, (2) extra simple single-label data, and (3) extra noisy web data. Without bells and whistles, it sets new state-of-the-arts on all these settings. Moreover, our approach ranked 1 st place in the WSSS Track of CVPR2020 LID Challenge. The extensive experimental results demonstrate well the efficacy and high utility of our method.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wang, Wenguan and Sun, Guolei and Van Gool, Luc},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Image segmentation, Training, Birds, Co-Attention, Cross-Image Semantic Relation, Location awareness, Noise measurement, Semantic Segmentation, Semantics, Training data, Weakly Supervised Learning},
	pages = {1--1},
}

@misc{li_exploring_2022,
	title = {Exploring {Plain} {Vision} {Transformer} {Backbones} for {Object} {Detection}},
	url = {http://arxiv.org/abs/2203.16527},
	doi = {10.48550/arXiv.2203.16527},
	abstract = {We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP\_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors. Code for ViTDet is available in Detectron2.},
	urldate = {2023-02-11},
	publisher = {arXiv},
	author = {Li, Yanghao and Mao, Hanzi and Girshick, Ross and He, Kaiming},
	month = jun,
	year = {2022},
	note = {arXiv:2203.16527 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/LF5HUWE6/Li et al. - 2022 - Exploring Plain Vision Transformer Backbones for O.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/GUBP5DWN/2203.html:text/html},
}

@inproceedings{van_gansbeke_revisiting_2021,
	title = {Revisiting {Contrastive} {Methods} for {Unsupervised} {Learning} of {Visual} {Representations}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/8757150decbd89b0f5442ca3db4d0e0e-Abstract.html},
	abstract = {Contrastive self-supervised learning has outperformed supervised pretraining on many downstream tasks like segmentation and object detection. However, current methods are still primarily applied to curated datasets like ImageNet. In this paper, we first study how biases in the dataset affect existing methods. Our results show that an approach like MoCo works surprisingly well across: (i) object- versus scene-centric, (ii) uniform versus long-tailed and (iii) general versus domain-specific datasets. Second, given the generality of the approach, we try to realize further gains with minor modifications. We show that learning additional invariances - through the use of multi-scale cropping, stronger augmentations and nearest neighbors - improves the representations. Finally, we observe that MoCo learns spatially structured representations when trained with a multi-crop strategy. The representations can be used for semantic segment retrieval and video instance segmentation without finetuning. Moreover, the results are on par with specialized models. We hope this work will serve as a useful study for other researchers.},
	urldate = {2023-02-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Van Gansbeke, Wouter and Vandenhende, Simon and Georgoulis, Stamatios and Gool, Luc V},
	year = {2021},
	pages = {16238--16250},
	file = {Full Text PDF:/home/pape/Zotero/storage/MLBTHX27/Van Gansbeke et al. - 2021 - Revisiting Contrastive Methods for Unsupervised Le.pdf:application/pdf},
}

@misc{abbas_clusterfug_2023,
	title = {{ClusterFuG}: {Clustering} {Fully} connected {Graphs} by {Multicut}},
	shorttitle = {{ClusterFuG}},
	url = {http://arxiv.org/abs/2301.12159},
	doi = {10.48550/arXiv.2301.12159},
	abstract = {We propose a graph clustering formulation based on multicut (a.k.a. weighted correlation clustering) on the complete graph. Our formulation does not need specification of the graph topology as in the original sparse formulation of multicut, making our approach simpler and potentially better performing. In contrast to unweighted correlation clustering we allow for a more expressive weighted cost structure. In dense multicut, the clustering objective is given in a factorized form as inner products of node feature vectors. This allows for an efficient formulation and inference in contrast to multicut/weighted correlation clustering, which has at least quadratic representation and computation complexity when working on the complete graph. We show how to rewrite classical greedy algorithms for multicut in our dense setting and how to modify them for greater efficiency and solution quality. In particular, our algorithms scale to graphs with tens of thousands of nodes. Empirical evidence on instance segmentation on Cityscapes and clustering of ImageNet datasets shows the merits of our approach.},
	urldate = {2023-02-11},
	publisher = {arXiv},
	author = {Abbas, Ahmed and Swoboda, Paul},
	month = jan,
	year = {2023},
	note = {arXiv:2301.12159 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/4EIWZWGK/Abbas and Swoboda - 2023 - ClusterFuG Clustering Fully connected Graphs by M.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/Y529IU56/2301.html:text/html},
}

@misc{du_weakly_2022,
	title = {Weakly {Supervised} {Semantic} {Segmentation} by {Pixel}-to-{Prototype} {Contrast}},
	url = {http://arxiv.org/abs/2110.07110},
	doi = {10.48550/arXiv.2110.07110},
	abstract = {Though image-level weakly supervised semantic segmentation (WSSS) has achieved great progress with Class Activation Maps (CAMs) as the cornerstone, the large supervision gap between classification and segmentation still hampers the model to generate more complete and precise pseudo masks for segmentation. In this study, we propose weakly-supervised pixel-to-prototype contrast that can provide pixel-level supervisory signals to narrow the gap. Guided by two intuitive priors, our method is executed across different views and within per single view of an image, aiming to impose cross-view feature semantic consistency regularization and facilitate intra(inter)-class compactness(dispersion) of the feature space. Our method can be seamlessly incorporated into existing WSSS models without any changes to the base networks and does not incur any extra inference burden. Extensive experiments manifest that our method consistently improves two strong baselines by large margins, demonstrating the effectiveness. Specifically, built on top of SEAM, we improve the initial seed mIoU on PASCAL VOC 2012 from 55.4\% to 61.5\%. Moreover, armed with our method, we increase the segmentation mIoU of EPS from 70.8\% to 73.6\%, achieving new state-of-the-art.},
	urldate = {2023-02-11},
	publisher = {arXiv},
	author = {Du, Ye and Fu, Zehua and Liu, Qingjie and Wang, Yunhong},
	month = mar,
	year = {2022},
	note = {arXiv:2110.07110 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/TQ8JVDGY/Du et al. - 2022 - Weakly Supervised Semantic Segmentation by Pixel-t.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/BZ3S9SCN/2110.html:text/html},
}

@article{ouyang_deep_2018,
	title = {Deep learning massively accelerates super-resolution localization microscopy},
	volume = {36},
	copyright = {2018 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/nbt.4106},
	doi = {10.1038/nbt.4106},
	abstract = {Accelerating PALM/STORM microscopy with deep learning allows super-resolution imaging of {\textgreater}1,000 cells in a few hours.},
	language = {en},
	number = {5},
	urldate = {2023-02-12},
	journal = {Nature Biotechnology},
	author = {Ouyang, Wei and Aristov, Andrey and Lelek, Mickaël and Hao, Xian and Zimmer, Christophe},
	month = may,
	year = {2018},
	note = {Number: 5
Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning, Microscopy, Super-resolution microscopy, Fluorescence imaging},
	pages = {460--468},
	file = {Full Text PDF:/home/pape/Zotero/storage/9R2VJBSZ/Ouyang et al. - 2018 - Deep learning massively accelerates super-resoluti.pdf:application/pdf},
}

@article{nehme_deep-storm_2018,
	title = {Deep-{STORM}: super-resolution single-molecule microscopy by deep learning},
	volume = {5},
	copyright = {© 2018 Optical Society of America},
	issn = {2334-2536},
	shorttitle = {Deep-{STORM}},
	url = {https://opg.optica.org/optica/abstract.cfm?uri=optica-5-4-458},
	doi = {10.1364/OPTICA.5.000458},
	abstract = {We present an ultrafast, precise, parameter-free method, which we term Deep-STORM, for obtaining super-resolution images from stochastically blinking emitters, such as fluorescent molecules used for localization microscopy. Deep-STORM uses a deep convolutional neural network that can be trained on simulated data or experimental measurements, both of which are demonstrated. The method achieves state-of-the-art resolution under challenging signal-to-noise conditions and high emitter densities and is significantly faster than existing approaches. Additionally, no prior information on the shape of the underlying structure is required, making the method applicable to any blinking dataset. We validate our approach by super-resolution image reconstruction of simulated and experimentally obtained data.},
	language = {EN},
	number = {4},
	urldate = {2023-02-12},
	journal = {Optica},
	author = {Nehme, Elias and Weiss, Lucien E. and Michaeli, Tomer and Shechtman, Yoav},
	month = apr,
	year = {2018},
	note = {Publisher: Optica Publishing Group},
	keywords = {Image processing, Neural networks, Diffraction limit, High numerical aperture optics, Image enhancement, Image reconstruction},
	pages = {458--464},
	file = {Full Text PDF:/home/pape/Zotero/storage/GHGZF3Z5/Nehme et al. - 2018 - Deep-STORM super-resolution single-molecule micro.pdf:application/pdf},
}

@article{speiser_deep_2021,
	title = {Deep learning enables fast and dense single-molecule localization with high accuracy},
	volume = {18},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-021-01236-x},
	doi = {10.1038/s41592-021-01236-x},
	abstract = {Single-molecule localization microscopy (SMLM) has had remarkable success in imaging cellular structures with nanometer resolution, but standard analysis algorithms require sparse emitters, which limits imaging speed and labeling density. Here, we overcome this major limitation using deep learning. We developed DECODE (deep context dependent), a computational tool that can localize single emitters at high density in three dimensions with highest accuracy for a large range of imaging modalities and conditions. In a public software benchmark competition, it outperformed all other fitters on 12 out of 12 datasets when comparing both detection accuracy and localization error, often by a substantial margin. DECODE allowed us to acquire fast dynamic live-cell SMLM data with reduced light exposure and to image microtubules at ultra-high labeling density. Packaged for simple installation and use, DECODE will enable many laboratories to reduce imaging times and increase localization density in SMLM.},
	language = {en},
	number = {9},
	urldate = {2023-02-12},
	journal = {Nature Methods},
	author = {Speiser, Artur and Müller, Lucas-Raphael and Hoess, Philipp and Matti, Ulf and Obara, Christopher J. and Legant, Wesley R. and Kreshuk, Anna and Macke, Jakob H. and Ries, Jonas and Turaga, Srinivas C.},
	month = sep,
	year = {2021},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Super-resolution microscopy, Software, Fluorescence imaging},
	pages = {1082--1090},
	file = {Full Text PDF:/home/pape/Zotero/storage/A7FJGWLD/Speiser et al. - 2021 - Deep learning enables fast and dense single-molecu.pdf:application/pdf},
}

@article{xu_actin_2013,
	title = {Actin, {Spectrin}, and {Associated} {Proteins} {Form} a {Periodic} {Cytoskeletal} {Structure} in {Axons}},
	volume = {339},
	url = {https://www.science.org/doi/10.1126/science.1232251},
	doi = {10.1126/science.1232251},
	abstract = {Actin and spectrin play important roles in neurons, but their organization in axons and dendrites remains unclear. We used stochastic optical reconstruction microscopy to study the organization of actin, spectrin, and associated proteins in neurons. Actin formed ringlike structures that wrapped around the circumference of axons and were evenly spaced along axonal shafts with a periodicity of {\textasciitilde}180 to 190 nanometers. This periodic structure was not observed in dendrites, which instead contained long actin filaments running along dendritic shafts. Adducin, an actin-capping protein, colocalized with the actin rings. Spectrin exhibited periodic structures alternating with those of actin and adducin, and the distance between adjacent actin-adducin rings was comparable to the length of a spectrin tetramer. Sodium channels in axons were distributed in a periodic pattern coordinated with the underlying actin-spectrin–based cytoskeleton.},
	number = {6118},
	urldate = {2023-02-12},
	journal = {Science},
	author = {Xu, Ke and Zhong, Guisheng and Zhuang, Xiaowei},
	month = jan,
	year = {2013},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {452--456},
	file = {Full Text PDF:/home/pape/Zotero/storage/G8PKS43C/Xu et al. - 2013 - Actin, Spectrin, and Associated Proteins Form a Pe.pdf:application/pdf},
}

@article{tang_trans-synaptic_2016,
	title = {A trans-synaptic nanocolumn aligns neurotransmitter release to receptors},
	volume = {536},
	copyright = {2016 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature19058},
	doi = {10.1038/nature19058},
	abstract = {Synaptic vesicle fusion, as evoked by action potentials, is confined to presynaptic protein nanoclusters, which are closely aligned with concentrated postsynaptic receptors and their scaffolding proteins—an organization termed a ‘nanocolumn’.},
	language = {en},
	number = {7615},
	urldate = {2023-02-12},
	journal = {Nature},
	author = {Tang, Ai-Hui and Chen, Haiwen and Li, Tuo P. and Metzbower, Sarah R. and MacGillavry, Harold D. and Blanpied, Thomas A.},
	month = aug,
	year = {2016},
	note = {Number: 7615
Publisher: Nature Publishing Group},
	keywords = {Super-resolution microscopy, Cellular neuroscience, Synaptic transmission, Brain, Exocytosis},
	pages = {210--214},
	file = {Full Text PDF:/home/pape/Zotero/storage/UIA2SHEB/Tang et al. - 2016 - A trans-synaptic nanocolumn aligns neurotransmitte.pdf:application/pdf},
}

@article{bepler_topaz-denoise_2020,
	title = {Topaz-{Denoise}: general deep denoising models for {cryoEM} and {cryoET}},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	shorttitle = {Topaz-{Denoise}},
	url = {https://www.nature.com/articles/s41467-020-18952-1},
	doi = {10.1038/s41467-020-18952-1},
	abstract = {Cryo-electron microscopy (cryoEM) is becoming the preferred method for resolving protein structures. Low signal-to-noise ratio (SNR) in cryoEM images reduces the confidence and throughput of structure determination during several steps of data processing, resulting in impediments such as missing particle orientations. Denoising cryoEM images can not only improve downstream analysis but also accelerate the time-consuming data collection process by allowing lower electron dose micrographs to be used for analysis. Here, we present Topaz-Denoise, a deep learning method for reliably and rapidly increasing the SNR of cryoEM images and cryoET tomograms. By training on a dataset composed of thousands of micrographs collected across a wide range of imaging conditions, we are able to learn models capturing the complexity of the cryoEM image formation process. The general model we present is able to denoise new datasets without additional training. Denoising with this model improves micrograph interpretability and allows us to solve 3D single particle structures of clustered protocadherin, an elongated particle with previously elusive views. We then show that low dose collection, enabled by Topaz-Denoise, improves downstream analysis in addition to reducing data collection time. We also present a general 3D denoising model for cryoET. Topaz-Denoise and pre-trained general models are now included in Topaz. We expect that Topaz-Denoise will be of broad utility to the cryoEM community for improving micrograph and tomogram interpretability and accelerating analysis.},
	language = {en},
	number = {1},
	urldate = {2023-02-12},
	journal = {Nature Communications},
	author = {Bepler, Tristan and Kelley, Kotaro and Noble, Alex J. and Berger, Bonnie},
	month = oct,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Cryoelectron microscopy},
	pages = {5208},
	file = {Full Text PDF:/home/pape/Zotero/storage/5JK4EDBP/Bepler et al. - 2020 - Topaz-Denoise general deep denoising models for c.pdf:application/pdf},
}

@article{lamm_membrain_2022-1,
	title = {{MemBrain}: {A} deep learning-aided pipeline for detection of membrane proteins in {Cryo}-electron tomograms},
	volume = {224},
	issn = {0169-2607},
	shorttitle = {{MemBrain}},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260722003728},
	doi = {10.1016/j.cmpb.2022.106990},
	abstract = {Background and Objective
Cryo-electron tomography (cryo-ET) is an imaging technique that enables 3D visualization of the native cellular environment at sub-nanometer resolution, providing unpreceded insights into the molecular organization of cells. However, cryo-electron tomograms suffer from low signal-to-noise ratios and anisotropic resolution, which makes subsequent image analysis challenging. In particular, the efficient detection of membrane-embedded proteins is a problem still lacking satisfactory solutions.
Methods
We present MemBrain – a new deep learning-aided pipeline that automatically detects membrane-bound protein complexes in cryo-electron tomograms. After subvolumes are sampled along a segmented membrane, each subvolume is assigned a score using a convolutional neural network (CNN), and protein positions are extracted by a clustering algorithm. Incorporating rotational subvolume normalization and using a tiny receptive field simplify the task of protein detection and thus facilitate the network training.
Results
MemBrain requires only a small quantity of training labels and achieves excellent performance with only a single annotated membrane (F1 score: 0.88). A detailed evaluation shows that our fully trained pipeline outperforms existing classical computer vision-based and CNN-based approaches by a large margin (F1 score: 0.92 vs. max. 0.63). Furthermore, in addition to protein center positions, MemBrain can determine protein orientations, which has not been implemented by any existing CNN-based method to date. We also show that a pre-trained MemBrain program generalizes to tomograms acquired using different cryo-ET methods and depicting different types of cells.
Conclusions
MemBrain is a powerful and annotation-efficient tool for the detection of membrane protein complexes in cryo-ET data, with the potential to be used in a wide range of biological studies. It is generalizable to various kinds of tomograms, making it possible to use pretrained models for different tasks. Its efficiency in terms of required annotations also allows rapid training and fine-tuning of models. The corresponding code, pretrained models, and instructions for operating the MemBrain program can be found at: https://github.com/CellArchLab/MemBrain.},
	language = {en},
	urldate = {2023-02-12},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Lamm, Lorenz and Righetto, Ricardo D. and Wietrzynski, Wojciech and Pöge, Matthias and Martinez-Sanchez, Antonio and Peng, Tingying and Engel, Benjamin D.},
	month = sep,
	year = {2022},
	keywords = {deep learning, annotation-efficient, Cryo-electron tomography, membrane protein, particle picking, protein localization},
	pages = {106990},
	file = {ScienceDirect Full Text PDF:/home/pape/Zotero/storage/D863MB2U/Lamm et al. - 2022 - MemBrain A deep learning-aided pipeline for detec.pdf:application/pdf},
}

@article{liu_isotropic_2022,
	title = {Isotropic reconstruction for electron tomography with deep learning},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-33957-8},
	doi = {10.1038/s41467-022-33957-8},
	abstract = {Cryogenic electron tomography (cryoET) allows visualization of cellular structures in situ. However, anisotropic resolution arising from the intrinsic “missing-wedge” problem has presented major challenges in visualization and interpretation of tomograms. Here, we have developed IsoNet, a deep learning-based software package that iteratively reconstructs the missing-wedge information and increases signal-to-noise ratio, using the knowledge learned from raw tomograms. Without the need for sub-tomogram averaging, IsoNet generates tomograms with significantly reduced resolution anisotropy. Applications of IsoNet to three representative types of cryoET data demonstrate greatly improved structural interpretability: resolving lattice defects in immature HIV particles, establishing architecture of the paraflagellar rod in Eukaryotic flagella, and identifying heptagon-containing clathrin cages inside a neuronal synapse of cultured cells. Therefore, by overcoming two fundamental limitations of cryoET, IsoNet enables functional interpretation of cellular tomograms without sub-tomogram averaging. Its application to high-resolution cellular tomograms should also help identify differently oriented complexes of the same kind for sub-tomogram averaging.},
	language = {en},
	number = {1},
	urldate = {2023-02-12},
	journal = {Nature Communications},
	author = {Liu, Yun-Tao and Zhang, Heng and Wang, Hui and Tao, Chang-Lu and Bi, Guo-Qiang and Zhou, Z. Hong},
	month = oct,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Software, Cryoelectron tomography},
	pages = {6482},
	file = {Full Text PDF:/home/pape/Zotero/storage/6XP4J2RE/Liu et al. - 2022 - Isotropic reconstruction for electron tomography w.pdf:application/pdf},
}

@inproceedings{buchholz_cryo-care_2019,
	title = {Cryo-{CARE}: {Content}-{Aware} {Image} {Restoration} for {Cryo}-{Transmission} {Electron} {Microscopy} {Data}},
	shorttitle = {Cryo-{CARE}},
	doi = {10.1109/ISBI.2019.8759519},
	abstract = {Multiple approaches to use deep learning for image restoration have recently been proposed. Training such approaches requires well registered pairs of high and low quality images. While this is easily achievable for many imaging modalities, e.g. fluorescence light microscopy, for others it is not. Cryo-transmission electron microscopy (cryo-TEM) could profoundly benefit from improved denoising methods, unfortunately it is one of the latter. Here we show how recent advances in network training for image restoration tasks, i.e. denoising, can be applied to cryo-TEM data. We describe our proposed method and show how it can be applied to single cryo-TEM projections and whole cryo-tomographic image volumes. Our proposed restoration method dramatically increases contrast in cryo-TEM images, which improves the interpretability of the acquired data. Furthermore we show that automated downstream processing on restored image data, demonstrated on a dense segmentation task, leads to improved results.},
	booktitle = {2019 {IEEE} 16th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2019)},
	author = {Buchholz, Tim-Oliver and Jordan, Mareike and Pigino, Gaia and Jug, Florian},
	month = apr,
	year = {2019},
	note = {ISSN: 1945-8452},
	keywords = {deep learning, Electron microscopy, Training, Image reconstruction, cryo-electron microscopy, denoising, image restoration, Image restoration, Motion pictures, Tomography},
	pages = {502--506},
}

@misc{lee_clinical_2023,
	title = {Clinical {Decision} {Transformer}: {Intended} {Treatment} {Recommendation} through {Goal} {Prompting}},
	shorttitle = {Clinical {Decision} {Transformer}},
	url = {http://arxiv.org/abs/2302.00612},
	doi = {10.48550/arXiv.2302.00612},
	abstract = {With recent achievements in tasks requiring context awareness, foundation models have been adopted to treat large-scale data from electronic health record (EHR) systems. However, previous clinical recommender systems based on foundation models have a limited purpose of imitating clinicians' behavior and do not directly consider a problem of missing values. In this paper, we propose Clinical Decision Transformer (CDT), a recommender system that generates a sequence of medications to reach a desired range of clinical states given as goal prompts. For this, we conducted goal-conditioned sequencing, which generated a subsequence of treatment history with prepended future goal state, and trained the CDT to model sequential medications required to reach that goal state. For contextual embedding over intra-admission and inter-admissions, we adopted a GPT-based architecture with an admission-wise attention mask and column embedding. In an experiment, we extracted a diabetes dataset from an EHR system, which contained treatment histories of 4788 patients. We observed that the CDT achieved the intended treatment effect according to goal prompt ranges (e.g., NormalA1c, LowerA1c, and HigherA1c), contrary to the case with behavior cloning. To the best of our knowledge, this is the first study to explore clinical recommendations from the perspective of goal prompting. See https://clinical-decision-transformer.github.io for code and additional information.},
	urldate = {2023-02-12},
	publisher = {arXiv},
	author = {Lee, Seunghyun and Lee, Da Young and Im, Sujeong and Kim, Nan Hee and Park, Sung-Min},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00612 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/7ZLFYY4I/Lee et al. - 2023 - Clinical Decision Transformer Intended Treatment .pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/N6ZBDHHF/2302.html:text/html},
}

@misc{cubuk_randaugment_2019,
	title = {{RandAugment}: {Practical} automated data augmentation with a reduced search space},
	shorttitle = {{RandAugment}},
	url = {http://arxiv.org/abs/1909.13719},
	doi = {10.48550/arXiv.1909.13719},
	abstract = {Recent work has shown that data augmentation has the potential to significantly improve the generalization of deep learning models. Recently, automated augmentation strategies have led to state-of-the-art results in image classification and object detection. While these strategies were optimized for improving validation accuracy, they also led to state-of-the-art results in semi-supervised learning and improved robustness to common corruptions of images. An obstacle to a large-scale adoption of these methods is a separate search phase which increases the training complexity and may substantially increase the computational cost. Additionally, due to the separate search phase, these approaches are unable to adjust the regularization strength based on model or dataset size. Automated augmentation policies are often found by training small models on small datasets and subsequently applied to train larger models. In this work, we remove both of these obstacles. RandAugment has a significantly reduced search space which allows it to be trained on the target task with no need for a separate proxy task. Furthermore, due to the parameterization, the regularization strength may be tailored to different model and dataset sizes. RandAugment can be used uniformly across different tasks and datasets and works out of the box, matching or surpassing all previous automated augmentation approaches on CIFAR-10/100, SVHN, and ImageNet. On the ImageNet dataset we achieve 85.0\% accuracy, a 0.6\% increase over the previous state-of-the-art and 1.0\% increase over baseline augmentation. On object detection, RandAugment leads to 1.0-1.3\% improvement over baseline augmentation, and is within 0.3\% mAP of AutoAugment on COCO. Finally, due to its interpretable hyperparameter, RandAugment may be used to investigate the role of data augmentation with varying model and dataset size. Code is available online.},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Cubuk, Ekin D. and Zoph, Barret and Shlens, Jonathon and Le, Quoc V.},
	month = nov,
	year = {2019},
	note = {arXiv:1909.13719 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/XRCT3D75/Cubuk et al. - 2019 - RandAugment Practical automated data augmentation.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/XEW9J25Z/1909.html:text/html},
}

@misc{cubuk_autoaugment_2019,
	title = {{AutoAugment}: {Learning} {Augmentation} {Policies} from {Data}},
	shorttitle = {{AutoAugment}},
	url = {http://arxiv.org/abs/1805.09501},
	doi = {10.48550/arXiv.1805.09501},
	abstract = {Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5\% which is 0.4\% better than the previous record of 83.1\%. On CIFAR-10, we achieve an error rate of 1.5\%, which is 0.6\% better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
	month = apr,
	year = {2019},
	note = {arXiv:1805.09501 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/W92J74GY/Cubuk et al. - 2019 - AutoAugment Learning Augmentation Policies from D.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/KS5F5P76/1805.html:text/html},
}

@article{hyun_development_2022,
	title = {Development of {Deep}-{Learning}-{Based} {Single}-{Molecule} {Localization} {Image} {Analysis}},
	volume = {23},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9266576/},
	doi = {10.3390/ijms23136896},
	abstract = {Recent developments in super-resolution fluorescence microscopic techniques (SRM) have allowed for nanoscale imaging that greatly facilitates our understanding of nanostructures. However, the performance of single-molecule localization microscopy (SMLM) ...},
	language = {en},
	number = {13},
	urldate = {2023-02-14},
	journal = {International Journal of Molecular Sciences},
	author = {Hyun, Yoonsuk and Kim, Doory},
	month = jul,
	year = {2022},
	pmid = {35805897},
	note = {Publisher: Multidisciplinary Digital Publishing Institute  (MDPI)},
	file = {Full Text:/home/pape/Zotero/storage/9LVVX7AI/Hyun and Kim - 2022 - Development of Deep-Learning-Based Single-Molecule.pdf:application/pdf;Snapshot:/home/pape/Zotero/storage/FSBZNAWL/ijms-23-06896-t001.html:text/html},
}

@article{hyun_development_2022-1,
	title = {Development of {Deep}-{Learning}-{Based} {Single}-{Molecule} {Localization} {Image} {Analysis}},
	volume = {23},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1422-0067},
	url = {https://www.mdpi.com/1422-0067/23/13/6896},
	doi = {10.3390/ijms23136896},
	abstract = {Recent developments in super-resolution fluorescence microscopic techniques (SRM) have allowed for nanoscale imaging that greatly facilitates our understanding of nanostructures. However, the performance of single-molecule localization microscopy (SMLM) is significantly restricted by the image analysis method, as the final super-resolution image is reconstructed from identified localizations through computational analysis. With recent advancements in deep learning, many researchers have employed deep learning-based algorithms to analyze SMLM image data. This review discusses recent developments in deep-learning-based SMLM image analysis, including the limitations of existing fitting algorithms and how the quality of SMLM images can be improved through deep learning. Finally, we address possible future applications of deep learning methods for SMLM imaging.},
	language = {en},
	number = {13},
	urldate = {2023-02-14},
	journal = {International Journal of Molecular Sciences},
	author = {Hyun, Yoonsuk and Kim, Doory},
	month = jan,
	year = {2022},
	note = {Number: 13
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {computer vision, deep learning, single-molecule localization microscopy, super-resolution microscopy},
	pages = {6896},
	file = {Full Text PDF:/home/pape/Zotero/storage/BUIN7UBH/Hyun and Kim - 2022 - Development of Deep-Learning-Based Single-Molecule.pdf:application/pdf},
}

@article{gomez-de-mariscal_deep-learning-based_2019,
	title = {Deep-{Learning}-{Based} {Segmentation} of {Small} {Extracellular} {Vesicles} in {Transmission} {Electron} {Microscopy} {Images}},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-49431-3},
	doi = {10.1038/s41598-019-49431-3},
	abstract = {Small extracellular vesicles (sEVs) are cell-derived vesicles of nanoscale size ({\textasciitilde}30–200 nm) that function as conveyors of information between cells, reflecting the cell of their origin and its physiological condition in their content. Valuable information on the shape and even on the composition of individual sEVs can be recorded using transmission electron microscopy (TEM). Unfortunately, sample preparation for TEM image acquisition is a complex procedure, which often leads to noisy images and renders automatic quantification of sEVs an extremely difficult task. We present a completely deep-learning-based pipeline for the segmentation of sEVs in TEM images. Our method applies a residual convolutional neural network to obtain fine masks and use the Radon transform for splitting clustered sEVs. Using three manually annotated datasets that cover a natural variability typical for sEV studies, we show that the proposed method outperforms two different state-of-the-art approaches in terms of detection and segmentation performance. Furthermore, the diameter and roundness of the segmented vesicles are estimated with an error of less than 10\%, which supports the high potential of our method in biological applications.},
	language = {en},
	number = {1},
	urldate = {2023-02-14},
	journal = {Scientific Reports},
	author = {Gómez-de-Mariscal, Estibaliz and Maška, Martin and Kotrbová, Anna and Pospíchalová, Vendula and Matula, Pavel and Muñoz-Barrutia, Arrate},
	month = sep,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Image processing, Extracellular signalling molecules},
	pages = {13211},
	file = {Full Text PDF:/home/pape/Zotero/storage/43BNU36F/Gómez-de-Mariscal et al. - 2019 - Deep-Learning-Based Segmentation of Small Extracel.pdf:application/pdf},
}

@article{imbrosci_automated_2022,
	title = {Automated {Detection} and {Localization} of {Synaptic} {Vesicles} in {Electron} {Microscopy} {Images}},
	volume = {9},
	copyright = {Copyright © 2022 Imbrosci et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International license, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
	issn = {2373-2822},
	url = {https://www.eneuro.org/content/9/1/ENEURO.0400-20.2021},
	doi = {10.1523/ENEURO.0400-20.2021},
	abstract = {Information transfer and integration in the brain occurs at chemical synapses and is mediated by the fusion of synaptic vesicles filled with neurotransmitter. Synaptic vesicle dynamic spatial organization regulates synaptic transmission as well as synaptic plasticity. Because of their small size, synaptic vesicles require electron microscopy (EM) for their imaging, and their analysis is conducted manually. The manual annotation and segmentation of the hundreds to thousands of synaptic vesicles, is highly time consuming and limits the throughput of data collection. To overcome this limitation, we built an algorithm, mainly relying on convolutional neural networks (CNNs), capable of automatically detecting and localizing synaptic vesicles in electron micrographs. The algorithm was trained on murine synapses but we show that it works well on synapses from different species, ranging from zebrafish to human, and from different preparations. As output, we provide the vesicle count and coordinates, the nearest neighbor distance (nnd) and the estimate of the vesicles area. We also provide a graphical user interface (GUI) to guide users through image analysis, result visualization, and manual proof-reading. The application of our algorithm is especially recommended for images produced by transmission EM. Since this type of imaging is used routinely to investigate presynaptic terminals, our solution will likely be of interest for numerous research groups.},
	language = {en},
	number = {1},
	urldate = {2023-02-14},
	journal = {eNeuro},
	author = {Imbrosci, Barbara and Schmitz, Dietmar and Orlando, Marta},
	month = jan,
	year = {2022},
	pmid = {34983830},
	note = {Publisher: Society for Neuroscience
Section: Research Article: Methods/New Tools},
	keywords = {machine learning, synaptic vesicle, convolutional neural networks, automated detection, image analysis},
	file = {Full Text PDF:/home/pape/Zotero/storage/N8RDTTPM/Imbrosci et al. - 2022 - Automated Detection and Localization of Synaptic V.pdf:application/pdf},
}

@article{kaltdorf_fiji_2017,
	title = {{FIJI} {Macro} {3D} {ART} {VeSElecT}: {3D} {Automated} {Reconstruction} {Tool} for {Vesicle} {Structures} of {Electron} {Tomograms}},
	volume = {13},
	issn = {1553-7358},
	shorttitle = {{FIJI} {Macro} {3D} {ART} {VeSElecT}},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005317},
	doi = {10.1371/journal.pcbi.1005317},
	abstract = {Automatic image reconstruction is critical to cope with steadily increasing data from advanced microscopy. We describe here the Fiji macro 3D ART VeSElecT which we developed to study synaptic vesicles in electron tomograms. We apply this tool to quantify vesicle properties (i) in embryonic Danio rerio 4 and 8 days past fertilization (dpf) and (ii) to compare Caenorhabditis elegans N2 neuromuscular junctions (NMJ) wild-type and its septin mutant (unc-59(e261)). We demonstrate development-specific and mutant-specific changes in synaptic vesicle pools in both models. We confirm the functionality of our macro by applying our 3D ART VeSElecT on zebrafish NMJ showing smaller vesicles in 8 dpf embryos then 4 dpf, which was validated by manual reconstruction of the vesicle pool. Furthermore, we analyze the impact of C. elegans septin mutant unc-59(e261) on vesicle pool formation and vesicle size. Automated vesicle registration and characterization was implemented in Fiji as two macros (registration and measurement). This flexible arrangement allows in particular reducing false positives by an optional manual revision step. Preprocessing and contrast enhancement work on image-stacks of 1nm/pixel in x and y direction. Semi-automated cell selection was integrated. 3D ART VeSElecT removes interfering components, detects vesicles by 3D segmentation and calculates vesicle volume and diameter (spherical approximation, inner/outer diameter). Results are collected in color using the RoiManager plugin including the possibility of manual removal of non-matching confounder vesicles. Detailed evaluation considered performance (detected vesicles) and specificity (true vesicles) as well as precision and recall. We furthermore show gain in segmentation and morphological filtering compared to learning based methods and a large time gain compared to manual segmentation. 3D ART VeSElecT shows small error rates and its speed gain can be up to 68 times faster in comparison to manual annotation. Both automatic and semi-automatic modes are explained including a tutorial.},
	language = {en},
	number = {1},
	urldate = {2023-02-14},
	journal = {PLOS Computational Biology},
	author = {Kaltdorf, Kristin Verena and Schulze, Katja and Helmprobst, Frederik and Kollmannsberger, Philip and Dandekar, Thomas and Stigloher, Christian},
	month = jan,
	year = {2017},
	note = {Publisher: Public Library of Science},
	keywords = {Synapses, Caenorhabditis elegans, Computer software, Neuromuscular junctions, Septins, Synaptic vesicles, Vesicles, Zebrafish},
	pages = {e1005317},
	file = {Full Text PDF:/home/pape/Zotero/storage/EKN93RI2/Kaltdorf et al. - 2017 - FIJI Macro 3D ART VeSElecT 3D Automated Reconstru.pdf:application/pdf},
}

@article{tao_differentiation_2018,
	title = {Differentiation and {Characterization} of {Excitatory} and {Inhibitory} {Synapses} by {Cryo}-electron {Tomography} and {Correlative} {Microscopy}},
	volume = {38},
	copyright = {Copyright © 2018 Tao, Liu et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution License Creative Commons Attribution 4.0 International, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/38/6/1493},
	doi = {10.1523/JNEUROSCI.1548-17.2017},
	abstract = {As key functional units in neural circuits, different types of neuronal synapses play distinct roles in brain information processing, learning, and memory. Synaptic abnormalities are believed to underlie various neurological and psychiatric disorders. Here, by combining cryo-electron tomography and cryo-correlative light and electron microscopy, we distinguished intact excitatory and inhibitory synapses of cultured hippocampal neurons, and visualized the in situ 3D organization of synaptic organelles and macromolecules in their native state. Quantitative analyses of {\textgreater}100 synaptic tomograms reveal that excitatory synapses contain a mesh-like postsynaptic density (PSD) with thickness ranging from 20 to 50 nm. In contrast, the PSD in inhibitory synapses assumes a thin sheet-like structure ∼12 nm from the postsynaptic membrane. On the presynaptic side, spherical synaptic vesicles (SVs) of 25–60 nm diameter and discus-shaped ellipsoidal SVs of various sizes coexist in both synaptic types, with more ellipsoidal ones in inhibitory synapses. High-resolution tomograms obtained using a Volta phase plate and electron filtering and counting reveal glutamate receptor-like and GABAA receptor-like structures that interact with putative scaffolding and adhesion molecules, reflecting details of receptor anchoring and PSD organization. These results provide an updated view of the ultrastructure of excitatory and inhibitory synapses, and demonstrate the potential of our approach to gain insight into the organizational principles of cellular architecture underlying distinct synaptic functions.
SIGNIFICANCE STATEMENT To understand functional properties of neuronal synapses, it is desirable to analyze their structure at molecular resolution. We have developed an integrative approach combining cryo-electron tomography and correlative fluorescence microscopy to visualize 3D ultrastructural features of intact excitatory and inhibitory synapses in their native state. Our approach shows that inhibitory synapses contain uniform thin sheet-like postsynaptic densities (PSDs), while excitatory synapses contain previously known mesh-like PSDs. We discovered “discus-shaped” ellipsoidal synaptic vesicles, and their distributions along with regular spherical vesicles in synaptic types are characterized. High-resolution tomograms further allowed identification of putative neurotransmitter receptors and their heterogeneous interaction with synaptic scaffolding proteins. The specificity and resolution of our approach enables precise in situ analysis of ultrastructural organization underlying distinct synaptic functions.},
	language = {en},
	number = {6},
	urldate = {2023-02-14},
	journal = {Journal of Neuroscience},
	author = {Tao, Chang-Lu and Liu, Yun-Tao and Sun, Rong and Zhang, Bin and Qi, Lei and Shivakoti, Sakar and Tian, Chong-Li and Zhang, Peijun and Lau, Pak-Ming and Zhou, Z. Hong and Bi, Guo-Qiang},
	month = feb,
	year = {2018},
	pmid = {29311144},
	note = {Publisher: Society for Neuroscience
Section: Research Articles},
	keywords = {synaptic vesicle, cryo-electron tomography, correlative light and electron microscopy, neurotransmitter receptor, postsynaptic density, synaptic ultrastructure},
	pages = {1493--1510},
	file = {Full Text PDF:/home/pape/Zotero/storage/D24VK47Q/Tao et al. - 2018 - Differentiation and Characterization of Excitatory.pdf:application/pdf},
}

@article{peddie_volume_2022,
	title = {Volume electron microscopy},
	volume = {2},
	copyright = {2022 Crown},
	issn = {2662-8449},
	url = {https://www.nature.com/articles/s43586-022-00131-9},
	doi = {10.1038/s43586-022-00131-9},
	abstract = {Life exists in three dimensions, but until the turn of the century most electron microscopy methods provided only 2D image data. Recently, electron microscopy techniques capable of delving deep into the structure of cells and tissues have emerged, collectively called volume electron microscopy (vEM). Developments in vEM have been dubbed a quiet revolution as the field evolved from established transmission and scanning electron microscopy techniques, so early publications largely focused on the bioscience applications rather than the underlying technological breakthroughs. However, with an explosion in the uptake of vEM across the biosciences and fast-paced advances in volume, resolution, throughput and ease of use, it is timely to introduce the field to new audiences. In this Primer, we introduce the different vEM imaging modalities, the specialized sample processing and image analysis pipelines that accompany each modality and the types of information revealed in the data. We showcase key applications in the biosciences where vEM has helped make breakthrough discoveries and consider limitations and future directions. We aim to show new users how vEM can support discovery science in their own research fields and inspire broader uptake of the technology, finally allowing its full adoption into mainstream biological imaging.},
	language = {en},
	number = {1},
	urldate = {2023-02-15},
	journal = {Nature Reviews Methods Primers},
	author = {Peddie, Christopher J. and Genoud, Christel and Kreshuk, Anna and Meechan, Kimberly and Micheva, Kristina D. and Narayan, Kedar and Pape, Constantin and Parton, Robert G. and Schieber, Nicole L. and Schwab, Yannick and Titze, Benjamin and Verkade, Paul and Weigel, Aubrey and Collinson, Lucy M.},
	month = jul,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Microscopy, Electron microscopy},
	pages = {1--23},
}

@misc{zhuang_survey_2023,
	title = {A {Survey} on {Efficient} {Training} of {Transformers}},
	url = {http://arxiv.org/abs/2302.01107},
	doi = {10.48550/arXiv.2302.01107},
	abstract = {Recent advances in Transformers have come with a huge requirement on computing resources, highlighting the importance of developing efficient training techniques to make Transformer training faster, at lower cost, and to higher accuracy by the efficient use of computation and memory resources. This survey provides the first systematic overview of the efficient training of Transformers, covering the recent progress in acceleration arithmetic and hardware, with a focus on the former. We analyze and compare methods that save computation and memory costs for intermediate tensors during training, together with techniques on hardware/algorithm co-design. We finally discuss challenges and promising areas for future research.},
	urldate = {2023-02-16},
	publisher = {arXiv},
	author = {Zhuang, Bohan and Liu, Jing and Pan, Zizheng and He, Haoyu and Weng, Yuetian and Shen, Chunhua},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01107 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/ILK38VFH/Zhuang et al. - 2023 - A Survey on Efficient Training of Transformers.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/PVTEZG2K/2302.html:text/html},
}

@misc{berthelot_adamatch_2022,
	title = {{AdaMatch}: {A} {Unified} {Approach} to {Semi}-{Supervised} {Learning} and {Domain} {Adaptation}},
	shorttitle = {{AdaMatch}},
	abstract = {We extend semi-supervised learning to the problem of domain adaptation to learn significantly higher-accuracy models that train on one data distribution and test on a different one. With the goal of generality, we introduce AdaMatch, a method that unifies the tasks of unsupervised domain adaptation (UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation (SSDA). In an extensive experimental study, we compare its behavior with respective state-of-the-art techniques from SSL, SSDA, and UDA on vision classification tasks. We find AdaMatch either matches or significantly exceeds the state-of-the-art in each case using the same hyper-parameters regardless of the dataset or task. For example, AdaMatch nearly doubles the accuracy compared to that of the prior state-of-the-art on the UDA task for DomainNet and even exceeds the accuracy of the prior state-of-the-art obtained with pre-training by 6.4\% when AdaMatch is trained completely from scratch. Furthermore, by providing AdaMatch with just one labeled example per class from the target domain (i.e., the SSDA setting), we increase the target accuracy by an additional 6.1\%, and with 5 labeled examples, by 13.6\%.},
	urldate = {2023-02-16},
	publisher = {arXiv},
	author = {Berthelot, David and Roelofs, Rebecca and Sohn, Kihyuk and Carlini, Nicholas and Kurakin, Alex},
	month = mar,
	year = {2022},
	note = {arXiv:2106.04732 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/4FHLCMYE/Berthelot et al. - 2022 - AdaMatch A Unified Approach to Semi-Supervised Le.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/2V8PXMJ7/2106.html:text/html},
}

@misc{dehghani_scaling_2023,
	title = {Scaling {Vision} {Transformers} to 22 {Billion} {Parameters}},
	url = {http://arxiv.org/abs/2302.05442},
	doi = {10.48550/arXiv.2302.05442},
	abstract = {The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for "LLM-like" scaling in vision, and provides key steps towards getting there.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and Jenatton, Rodolphe and Beyer, Lucas and Tschannen, Michael and Arnab, Anurag and Wang, Xiao and Riquelme, Carlos and Minderer, Matthias and Puigcerver, Joan and Evci, Utku and Kumar, Manoj and van Steenkiste, Sjoerd and Elsayed, Gamaleldin F. and Mahendran, Aravindh and Yu, Fisher and Oliver, Avital and Huot, Fantine and Bastings, Jasmijn and Collier, Mark Patrick and Gritsenko, Alexey and Birodkar, Vighnesh and Vasconcelos, Cristina and Tay, Yi and Mensink, Thomas and Kolesnikov, Alexander and Pavetić, Filip and Tran, Dustin and Kipf, Thomas and Lučić, Mario and Zhai, Xiaohua and Keysers, Daniel and Harmsen, Jeremiah and Houlsby, Neil},
	month = feb,
	year = {2023},
	note = {arXiv:2302.05442 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/NXT8A4L8/Dehghani et al. - 2023 - Scaling Vision Transformers to 22 Billion Paramete.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/VFMQ35K6/2302.html:text/html},
}

@article{durand_machine_2018,
	title = {A machine learning approach for online automated optimization of super-resolution optical microscopy},
	volume = {9},
	copyright = {2018 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-018-07668-y},
	doi = {10.1038/s41467-018-07668-y},
	abstract = {Traditional approaches for finding well-performing parameterizations of complex imaging systems, such as super-resolution microscopes rely on an extensive exploration phase over the illumination and acquisition settings, prior to the imaging task. This strategy suffers from several issues: it requires a large amount of parameter configurations to be evaluated, it leads to discrepancies between well-performing parameters in the exploration phase and imaging task, and it results in a waste of time and resources given that optimization and final imaging tasks are conducted separately. Here we show that a fully automated, machine learning-based system can conduct imaging parameter optimization toward a trade-off between several objectives, simultaneously to the imaging task. Its potential is highlighted on various imaging tasks, such as live-cell and multicolor imaging and multimodal optimization. This online optimization routine can be integrated to various imaging systems to increase accessibility, optimize performance and improve overall imaging quality.},
	language = {en},
	number = {1},
	urldate = {2023-02-17},
	journal = {Nature Communications},
	author = {Durand, Audrey and Wiesner, Theresa and Gardner, Marc-André and Robitaille, Louis-Émile and Bilodeau, Anthony and Gagné, Christian and De Koninck, Paul and Lavoie-Cardinal, Flavie},
	month = dec,
	year = {2018},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Super-resolution microscopy},
	pages = {5247},
	file = {Full Text PDF:/home/pape/Zotero/storage/LRRQIZLW/Durand et al. - 2018 - A machine learning approach for online automated o.pdf:application/pdf},
}

@misc{geirhos_imagenet-trained_2022,
	title = {{ImageNet}-trained {CNNs} are biased towards texture; increasing shape bias improves accuracy and robustness},
	url = {http://arxiv.org/abs/1811.12231},
	doi = {10.48550/arXiv.1811.12231},
	abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on "Stylized-ImageNet", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
	month = nov,
	year = {2022},
	note = {arXiv:1811.12231 [cs, q-bio, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/ML6TKRZV/Geirhos et al. - 2022 - ImageNet-trained CNNs are biased towards texture\; .pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/RJLC233W/1811.html:text/html},
}

@misc{hendrycks_benchmarking_2019,
	title = {Benchmarking {Neural} {Network} {Robustness} to {Common} {Corruptions} and {Perturbations}},
	url = {http://arxiv.org/abs/1903.12261},
	doi = {10.48550/arXiv.1903.12261},
	abstract = {In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Dietterich, Thomas},
	month = mar,
	year = {2019},
	note = {arXiv:1903.12261 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/BTT2F6DQ/Hendrycks and Dietterich - 2019 - Benchmarking Neural Network Robustness to Common C.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/7W5A7ZQ8/1903.html:text/html},
}

@misc{recht_imagenet_2019,
	title = {Do {ImageNet} {Classifiers} {Generalize} to {ImageNet}?},
	url = {http://arxiv.org/abs/1902.10811},
	doi = {10.48550/arXiv.1902.10811},
	abstract = {We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3\% - 15\% on CIFAR-10 and 11\% - 14\% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly "harder" images than those found in the original test sets.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
	month = jun,
	year = {2019},
	note = {arXiv:1902.10811 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/ZXUNE35C/Recht et al. - 2019 - Do ImageNet Classifiers Generalize to ImageNet.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/QXNIZS7Y/1902.html:text/html},
}

@misc{villars_dextrusion_2023,
	title = {{DeXtrusion}: {Automatic} recognition of epithelial cell extrusion through machine learning in vivo},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{DeXtrusion}},
	url = {https://www.biorxiv.org/content/10.1101/2023.02.16.528845v1},
	doi = {10.1101/2023.02.16.528845},
	abstract = {Epithelial cell death is highly prevalent during development and in adult tissues. It plays an essential role in the regulation of tissue size, shape, and turnover. Cell elimination relies on the concerted remodelling of cell junctions, so-called cell extrusion, which allows the seamless expulsion of dying cells. The dissection of the regulatory mechanism giving rise to a certain number and pattern of cell death was so far limited by our capacity to generate high-throughput quantitative data on cell death/extrusion number and distribution in various perturbed backgrounds. Indeed, quantitative studies of cell death rely so far on manual detection of cell extrusion events or through tedious systematic error-free segmentation and cell tracking. Recently, deep learning was used to automatically detect cell death and cell division in cell culture mostly using transmission light microscopy. However, so far, no method was developed for fluorescent images and confocal microscopy, which constitute most datasets in embryonic epithelia. Here, we devised DeXtrusion, a pipeline for automatic detection of cell extrusion/cell death events in larges movies of epithelia marked with cell contour and based on recurrent neural networks. The pipeline, initially trained on large movies of the Drosophila pupal notum marked with fluorescent E-cadherin, is easily trainable, provides fast and accurate extrusion/cell death predictions in a large range of imaging conditions, and can also detect other cellular events such as cell division or cell differentiation. It also performs well on other epithelial tissues with markers of cell junctions with reasonable retraining.},
	language = {en},
	urldate = {2023-02-17},
	publisher = {bioRxiv},
	author = {Villars, Alexis and Letort, Gaelle and Valon, Leo and Levayer, Romain},
	month = feb,
	year = {2023},
	note = {Pages: 2023.02.16.528845
Section: New Results},
	file = {Full Text PDF:/home/pape/Zotero/storage/HWHGFBFP/Villars et al. - 2023 - DeXtrusion Automatic recognition of epithelial ce.pdf:application/pdf},
}

@misc{amatriain_transformer_2023,
	title = {Transformer models: an introduction and catalog},
	shorttitle = {Transformer models},
	url = {http://arxiv.org/abs/2302.07730},
	doi = {10.48550/arXiv.2302.07730},
	abstract = {In the past few years we have seen the meteoric appearance of dozens of models of the Transformer family, all of which have funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovation in Transformer models.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Amatriain, Xavier},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07730 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/DSUXCTQB/Amatriain - 2023 - Transformer models an introduction and catalog.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/ZCMG8A7K/2302.html:text/html},
}

@misc{yang_revisiting_2022,
	title = {Revisiting {Weak}-to-{Strong} {Consistency} in {Semi}-{Supervised} {Semantic} {Segmentation}},
	abstract = {In this work, we revisit the weak-to-strong consistency framework, popularized by FixMatch from semi-supervised classification, where the prediction of a weakly perturbed image serves as supervision for its strongly perturbed version. Intriguingly, we observe that such a simple pipeline already achieves competitive results against recent advanced works, when transferred to our segmentation scenario. Its success heavily relies on the manual design of strong data augmentations, however, which may be limited and inadequate to explore a broader perturbation space. Motivated by this, we propose an auxiliary feature perturbation stream as a supplement, leading to an expanded perturbation space. On the other, to sufficiently probe original image-level augmentations, we present a dual-stream perturbation technique, enabling two strong views to be simultaneously guided by a common weak view. Consequently, our overall Unified Dual-Stream Perturbations approach (UniMatch) surpasses all existing methods significantly across all evaluation protocols on the Pascal, Cityscapes, and COCO benchmarks. We also demonstrate the superiority of our method in remote sensing interpretation and medical image analysis. Code is available at https://github.com/LiheYoung/UniMatch.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Yang, Lihe and Qi, Lei and Feng, Litong and Zhang, Wayne and Shi, Yinghuan},
	month = aug,
	year = {2022},
	note = {arXiv:2208.09910 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/GLA56HL9/Yang et al. - 2022 - Revisiting Weak-to-Strong Consistency in Semi-Supe.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/4E9HPACK/2208.html:text/html},
}

@misc{krishna_downstream_2022,
	title = {Downstream {Datasets} {Make} {Surprisingly} {Good} {Pretraining} {Corpora}},
	url = {http://arxiv.org/abs/2209.14389},
	doi = {10.48550/arXiv.2209.14389},
	abstract = {For most natural language processing tasks, the dominant practice is to finetune large pretrained transformer models (e.g., BERT) using smaller downstream datasets. Despite the success of this approach, it remains unclear to what extent these gains are attributable to the massive background corpora employed for pretraining versus to the pretraining objectives themselves. This paper introduces a large-scale study of self-pretraining, where the same (downstream) training data is used for both pretraining and finetuning. In experiments addressing both ELECTRA and RoBERTa models and 10 distinct downstream datasets, we observe that self-pretraining rivals standard pretraining on the BookWiki corpus (despite using around \$10{\textbackslash}times\$--\$500{\textbackslash}times\$ less data), outperforming the latter on \$7\$ and \$5\$ datasets, respectively. Surprisingly, these task-specific pretrained models often perform well on other tasks, including the GLUE benchmark. Our results suggest that in many scenarios, performance gains attributable to pretraining are driven primarily by the pretraining objective itself and are not always attributable to the incorporation of massive datasets. These findings are especially relevant in light of concerns about intellectual property and offensive content in web-scale pretraining data.},
	urldate = {2023-02-18},
	publisher = {arXiv},
	author = {Krishna, Kundan and Garg, Saurabh and Bigham, Jeffrey P. and Lipton, Zachary C.},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14389 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/K3WVTKN8/Krishna et al. - 2022 - Downstream Datasets Make Surprisingly Good Pretrai.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/BU2PCKEX/2209.html:text/html},
}

@article{weigert_content-aware_2018-1,
	title = {Content-aware image restoration: pushing the limits of fluorescence microscopy},
	volume = {15},
	copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {Content-aware image restoration},
	url = {https://www.nature.com/articles/s41592-018-0216-7},
	doi = {10.1038/s41592-018-0216-7},
	abstract = {Fluorescence microscopy is a key driver of discoveries in the life sciences, with observable phenomena being limited by the optics of the microscope, the chemistry of the fluorophores, and the maximum photon exposure tolerated by the sample. These limits necessitate trade-offs between imaging speed, spatial resolution, light exposure, and imaging depth. In this work we show how content-aware image restoration based on deep learning extends the range of biological phenomena observable by microscopy. We demonstrate on eight concrete examples how microscopy images can be restored even if 60-fold fewer photons are used during acquisition, how near isotropic resolution can be achieved with up to tenfold under-sampling along the axial direction, and how tubular and granular structures smaller than the diffraction limit can be resolved at 20-times-higher frame rates compared to state-of-the-art methods. All developed image restoration methods are freely available as open source software in Python, FIJI, and KNIME.},
	language = {en},
	number = {12},
	urldate = {2023-02-21},
	journal = {Nature Methods},
	author = {Weigert, Martin and Schmidt, Uwe and Boothe, Tobias and Müller, Andreas and Dibrov, Alexandr and Jain, Akanksha and Wilhelm, Benjamin and Schmidt, Deborah and Broaddus, Coleman and Culley, Siân and Rocha-Martins, Mauricio and Segovia-Miranda, Fabián and Norden, Caren and Henriques, Ricardo and Zerial, Marino and Solimena, Michele and Rink, Jochen and Tomancak, Pavel and Royer, Loic and Jug, Florian and Myers, Eugene W.},
	month = dec,
	year = {2018},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning, Microscopy, Software},
	pages = {1090--1097},
	file = {Full Text PDF:/home/pape/Zotero/storage/D56CZXHV/Weigert et al. - 2018 - Content-aware image restoration pushing the limit.pdf:application/pdf},
}

@misc{krull_noise2void_2019,
	title = {{Noise2Void} - {Learning} {Denoising} from {Single} {Noisy} {Images}},
	url = {http://arxiv.org/abs/1811.10980},
	doi = {10.48550/arXiv.1811.10980},
	abstract = {The field of image denoising is currently dominated by discriminative deep learning methods that are trained on pairs of noisy input and clean target images. Recently it has been shown that such methods can also be trained without clean targets. Instead, independent pairs of noisy images can be used, in an approach known as Noise2Noise (N2N). Here, we introduce Noise2Void (N2V), a training scheme that takes this idea one step further. It does not require noisy image pairs, nor clean target images. Consequently, N2V allows us to train directly on the body of data to be denoised and can therefore be applied when other methods cannot. Especially interesting is the application to biomedical image data, where the acquisition of training targets, clean or noisy, is frequently not possible. We compare the performance of N2V to approaches that have either clean target images and/or noisy image pairs available. Intuitively, N2V cannot be expected to outperform methods that have more information available during training. Still, we observe that the denoising performance of Noise2Void drops in moderation and compares favorably to training-free denoising methods.},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Krull, Alexander and Buchholz, Tim-Oliver and Jug, Florian},
	month = apr,
	year = {2019},
	note = {arXiv:1811.10980 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/QVWCCISE/Krull et al. - 2019 - Noise2Void - Learning Denoising from Single Noisy .pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/BCFDZP5Z/1811.html:text/html},
}

@article{krull_probabilistic_2020,
	title = {Probabilistic {Noise2Void}: {Unsupervised} {Content}-{Aware} {Denoising}},
	volume = {2},
	issn = {2624-9898},
	shorttitle = {Probabilistic {Noise2Void}},
	url = {https://www.frontiersin.org/articles/10.3389/fcomp.2020.00005},
	abstract = {Today, Convolutional Neural Networks (CNNs) are the leading method for image denoising. They are traditionally trained on pairs of images, which are often hard to obtain for practical applications. This motivates self-supervised training methods, such as Noise2Void (N2V) that operate on single noisy images. Self-supervised methods are, unfortunately, not competitive with models trained on image pairs. Here, we present Probabilistic Noise2Void (PN2V), a method to train CNNs to predict per-pixel intensity distributions. Combining these with a suitable description of the noise, we obtain a complete probabilistic model for the noisy observations and true signal in every pixel. We evaluate PN2V on publicly available microscopy datasets, under a broad range of noise regimes, and achieve competitive results with respect to supervised state-of-the-art methods.},
	urldate = {2023-02-21},
	journal = {Frontiers in Computer Science},
	author = {Krull, Alexander and Vičar, Tomáš and Prakash, Mangal and Lalit, Manan and Jug, Florian},
	year = {2020},
	file = {Full Text PDF:/home/pape/Zotero/storage/4ZR8X27Q/Krull et al. - 2020 - Probabilistic Noise2Void Unsupervised Content-Awa.pdf:application/pdf},
}

@inproceedings{prakash_fully_2022,
	title = {Fully {Unsupervised} {Diversity} {Denoising} with {Convolutional} {Variational} {Autoencoders}},
	url = {https://openreview.net/forum?id=agHLCOBM5jP},
	abstract = {Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.},
	language = {en},
	urldate = {2023-02-21},
	author = {Prakash, Mangal and Krull, Alexander and Jug, Florian},
	month = feb,
	year = {2022},
	file = {Full Text PDF:/home/pape/Zotero/storage/C4GP88HD/Prakash et al. - 2022 - Fully Unsupervised Diversity Denoising with Convol.pdf:application/pdf},
}

@article{mathis_deeplabcut_2018,
	title = {{DeepLabCut}: markerless pose estimation of user-defined body parts with deep learning},
	volume = {21},
	copyright = {2018 The Author(s)},
	issn = {1546-1726},
	shorttitle = {{DeepLabCut}},
	url = {https://www.nature.com/articles/s41593-018-0209-y},
	doi = {10.1038/s41593-018-0209-y},
	abstract = {Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, but markers are intrusive, and the number and location of the markers must be determined a priori. Here we present an efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors. Remarkably, even when only a small number of frames are labeled ({\textasciitilde}200), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy.},
	language = {en},
	number = {9},
	urldate = {2023-02-21},
	journal = {Nature Neuroscience},
	author = {Mathis, Alexander and Mamidanna, Pranav and Cury, Kevin M. and Abe, Taiga and Murthy, Venkatesh N. and Mathis, Mackenzie Weygandt and Bethge, Matthias},
	month = sep,
	year = {2018},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Behavioural methods, Computational neuroscience},
	pages = {1281--1289},
	file = {Full Text PDF:/home/pape/Zotero/storage/G4KUNXT5/Mathis et al. - 2018 - DeepLabCut markerless pose estimation of user-def.pdf:application/pdf},
}

@article{nath_using_2019,
	title = {Using {DeepLabCut} for {3D} markerless pose estimation across species and behaviors},
	volume = {14},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1750-2799},
	url = {https://www.nature.com/articles/s41596-019-0176-0},
	doi = {10.1038/s41596-019-0176-0},
	abstract = {Noninvasive behavioral tracking of animals during experiments is critical to many scientific pursuits. Extracting the poses of animals without using markers is often essential to measuring behavioral effects in biomechanics, genetics, ethology, and neuroscience. However, extracting detailed poses without markers in dynamically changing backgrounds has been challenging. We recently introduced an open-source toolbox called DeepLabCut that builds on a state-of-the-art human pose-estimation algorithm to allow a user to train a deep neural network with limited training data to precisely track user-defined features that match human labeling accuracy. Here, we provide an updated toolbox, developed as a Python package, that includes new features such as graphical user interfaces (GUIs), performance improvements, and active-learning-based network refinement. We provide a step-by-step procedure for using DeepLabCut that guides the user in creating a tailored, reusable analysis pipeline with a graphical processing unit (GPU) in 1–12 h (depending on frame size). Additionally, we provide Docker environments and Jupyter Notebooks that can be run on cloud resources such as Google Colaboratory.},
	language = {en},
	number = {7},
	urldate = {2023-02-21},
	journal = {Nature Protocols},
	author = {Nath, Tanmay and Mathis, Alexander and Chen, An Chi and Patel, Amir and Bethge, Matthias and Mathis, Mackenzie Weygandt},
	month = jul,
	year = {2019},
	note = {Number: 7
Publisher: Nature Publishing Group},
	keywords = {Software, Computational platforms and environments, Behavioural methods, Learning algorithms},
	pages = {2152--2176},
	file = {Full Text PDF:/home/pape/Zotero/storage/UZ4DEKFI/Nath et al. - 2019 - Using DeepLabCut for 3D markerless pose estimation.pdf:application/pdf},
}

@article{mayr_deeptox_2016,
	title = {{DeepTox}: {Toxicity} {Prediction} using {Deep} {Learning}},
	volume = {3},
	issn = {2296-665X},
	shorttitle = {{DeepTox}},
	url = {https://www.frontiersin.org/articles/10.3389/fenvs.2015.00080},
	abstract = {The Tox21 Data Challenge has been the largest effort of the scientific community to compare computational methods for toxicity prediction. This challenge comprised 12,000 environmental chemicals and drugs which were measured for 12 different toxic effects by specifically designed assays. We participated in this challenge to assess the performance of Deep Learning in computational toxicity prediction. Deep Learning has already revolutionized image processing, speech recognition, and language understanding but has not yet been applied to computational toxicity. Deep Learning is founded on novel algorithms and architectures for artificial neural networks together with the recent availability of very fast computers and massive datasets. It discovers multiple levels of distributed representations of the input, with higher levels representing more abstract concepts. We hypothesized that the construction of a hierarchy of chemical features gives Deep Learning the edge over other toxicity prediction methods. Furthermore, Deep Learning naturally enables multi-task learning, that is, learning of all toxic effects in one neural network and thereby learning of highly informative chemical features. In order to utilize Deep Learning for toxicity prediction, we have developed the DeepTox pipeline. First, DeepTox normalizes the chemical representations of the compounds. Then it computes a large number of chemical descriptors that are used as input to machine learning methods. In its next step, DeepTox trains models, evaluates them, and combines the best of them to ensembles. Finally, DeepTox predicts the toxicity of new compounds. In the Tox21 Data Challenge, DeepTox had the highest performance of all computational methods winning the grand challenge, the nuclear receptor panel, the stress response panel, and six single assays (teams “Bioinf@JKU”). We found that Deep Learning excelled in toxicity prediction and outperformed many other computational approaches like naive Bayes, support vector machines, and random forests.},
	urldate = {2023-02-21},
	journal = {Frontiers in Environmental Science},
	author = {Mayr, Andreas and Klambauer, Günter and Unterthiner, Thomas and Hochreiter, Sepp},
	year = {2016},
	file = {Full Text PDF:/home/pape/Zotero/storage/VT2C5QGP/Mayr et al. - 2016 - DeepTox Toxicity Prediction using Deep Learning.pdf:application/pdf},
}

@article{jimenez-luna_drug_2020,
	title = {Drug discovery with explainable artificial intelligence},
	volume = {2},
	copyright = {2020 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-00236-4},
	doi = {10.1038/s42256-020-00236-4},
	abstract = {Deep learning bears promise for drug discovery, including advanced image analysis, prediction of molecular structure and function, and automated generation of innovative chemical entities with bespoke properties. Despite the growing number of successful prospective applications, the underlying mathematical models often remain elusive to interpretation by the human mind. There is a demand for ‘explainable’ deep learning methods to address the need for a new narrative of the machine language of the molecular sciences. This Review summarizes the most prominent algorithmic concepts of explainable artificial intelligence, and forecasts future opportunities, potential applications as well as several remaining challenges. We also hope it encourages additional efforts towards the development and acceptance of explainable artificial intelligence techniques.},
	language = {en},
	number = {10},
	urldate = {2023-02-21},
	journal = {Nature Machine Intelligence},
	author = {Jiménez-Luna, José and Grisoni, Francesca and Schneider, Gisbert},
	month = oct,
	year = {2020},
	note = {Number: 10
Publisher: Nature Publishing Group},
	keywords = {Cheminformatics, Computational science, Drug discovery and development},
	pages = {573--584},
	file = {Full Text PDF:/home/pape/Zotero/storage/D7IYS663/Jiménez-Luna et al. - 2020 - Drug discovery with explainable artificial intelli.pdf:application/pdf},
}

@misc{berthelot_remixmatch_2020,
	title = {{ReMixMatch}: {Semi}-{Supervised} {Learning} with {Distribution} {Alignment} and {Augmentation} {Anchoring}},
	shorttitle = {{ReMixMatch}},
	abstract = {We improve the recently-proposed "MixMatch" semi-supervised learning algorithm by introducing two new techniques: distribution alignment and augmentation anchoring. Distribution alignment encourages the marginal distribution of predictions on unlabeled data to be close to the marginal distribution of ground-truth labels. Augmentation anchoring feeds multiple strongly augmented versions of an input into the model and encourages each output to be close to the prediction for a weakly-augmented version of the same input. To produce strong augmentations, we propose a variant of AutoAugment which learns the augmentation policy while the model is being trained. Our new algorithm, dubbed ReMixMatch, is significantly more data-efficient than prior work, requiring between \$5{\textbackslash}times\$ and \$16{\textbackslash}times\$ less data to reach the same accuracy. For example, on CIFAR-10 with 250 labeled examples we reach \$93.73{\textbackslash}\%\$ accuracy (compared to MixMatch's accuracy of \$93.58{\textbackslash}\%\$ with \$4\{,\}000\$ examples) and a median accuracy of \$84.92{\textbackslash}\%\$ with just four labels per class. We make our code and data open-source at https://github.com/google-research/remixmatch.},
	urldate = {2023-02-23},
	publisher = {arXiv},
	author = {Berthelot, David and Carlini, Nicholas and Cubuk, Ekin D. and Kurakin, Alex and Sohn, Kihyuk and Zhang, Han and Raffel, Colin},
	month = feb,
	year = {2020},
	note = {arXiv:1911.09785 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/DWHLKH46/Berthelot et al. - 2020 - ReMixMatch Semi-Supervised Learning with Distribu.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/CJBSTSPE/1911.html:text/html},
}

@misc{jain_oneformer_2022,
	title = {{OneFormer}: {One} {Transformer} to {Rule} {Universal} {Image} {Segmentation}},
	shorttitle = {{OneFormer}},
	url = {http://arxiv.org/abs/2211.06220},
	doi = {10.48550/arXiv.2211.06220},
	abstract = {Universal Image Segmentation is not a new concept. Past attempts to unify image segmentation in the last decades include scene parsing, panoptic segmentation, and, more recently, new panoptic architectures. However, such panoptic architectures do not truly unify image segmentation because they need to be trained individually on the semantic, instance, or panoptic segmentation to achieve the best performance. Ideally, a truly universal framework should be trained only once and achieve SOTA performance across all three image segmentation tasks. To that end, we propose OneFormer, a universal image segmentation framework that unifies segmentation with a multi-task train-once design. We first propose a task-conditioned joint training strategy that enables training on ground truths of each domain (semantic, instance, and panoptic segmentation) within a single multi-task training process. Secondly, we introduce a task token to condition our model on the task at hand, making our model task-dynamic to support multi-task training and inference. Thirdly, we propose using a query-text contrastive loss during training to establish better inter-task and inter-class distinctions. Notably, our single OneFormer model outperforms specialized Mask2Former models across all three segmentation tasks on ADE20k, CityScapes, and COCO, despite the latter being trained on each of the three tasks individually with three times the resources. With new ConvNeXt and DiNAT backbones, we observe even more performance improvement. We believe OneFormer is a significant step towards making image segmentation more universal and accessible. To support further research, we open-source our code and models at https://github.com/SHI-Labs/OneFormer},
	urldate = {2023-02-23},
	publisher = {arXiv},
	author = {Jain, Jitesh and Li, Jiachen and Chiu, MangTik and Hassani, Ali and Orlov, Nikita and Shi, Humphrey},
	month = dec,
	year = {2022},
	note = {arXiv:2211.06220 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/86CX6D5I/Jain et al. - 2022 - OneFormer One Transformer to Rule Universal Image.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/ZHPVU367/2211.html:text/html},
}

@misc{amatriain_transformer_2023-1,
	title = {Transformer models: an introduction and catalog},
	shorttitle = {Transformer models},
	url = {http://arxiv.org/abs/2302.07730},
	doi = {10.48550/arXiv.2302.07730},
	abstract = {In the past few years we have seen the meteoric appearance of dozens of models of the Transformer family, all of which have funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovation in Transformer models.},
	urldate = {2023-02-23},
	publisher = {arXiv},
	author = {Amatriain, Xavier},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07730 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/NB9DEJS2/Amatriain - 2023 - Transformer models an introduction and catalog.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/AT49FL39/2302.html:text/html},
}

@misc{shirokikh_first_2020,
	title = {First {U}-{Net} {Layers} {Contain} {More} {Domain} {Specific} {Information} {Than} {The} {Last} {Ones}},
	url = {http://arxiv.org/abs/2008.07357},
	doi = {10.48550/arXiv.2008.07357},
	abstract = {MRI scans appearance significantly depends on scanning protocols and, consequently, the data-collection institution. These variations between clinical sites result in dramatic drops of CNN segmentation quality on unseen domains. Many of the recently proposed MRI domain adaptation methods operate with the last CNN layers to suppress domain shift. At the same time, the core manifestation of MRI variability is a considerable diversity of image intensities. We hypothesize that these differences can be eliminated by modifying the first layers rather than the last ones. To validate this simple idea, we conducted a set of experiments with brain MRI scans from six domains. Our results demonstrate that 1) domain-shift may deteriorate the quality even for a simple brain extraction segmentation task (surface Dice Score drops from 0.85-0.89 even to 0.09); 2) fine-tuning of the first layers significantly outperforms fine-tuning of the last layers in almost all supervised domain adaptation setups. Moreover, fine-tuning of the first layers is a better strategy than fine-tuning of the whole network, if the amount of annotated data from the new domain is strictly limited.},
	urldate = {2023-02-23},
	publisher = {arXiv},
	author = {Shirokikh, Boris and Zakazov, Ivan and Chernyavskiy, Alexey and Fedulova, Irina and Belyaev, Mikhail},
	month = aug,
	year = {2020},
	note = {arXiv:2008.07357 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/pape/Zotero/storage/PIKXWKYP/Shirokikh et al. - 2020 - First U-Net Layers Contain More Domain Specific In.pdf:application/pdf;arXiv.org Snapshot:/home/pape/Zotero/storage/X8HETI9H/2008.html:text/html},
}

@misc{smith_active_2023,
	title = {Active mesh and neural network pipeline for cell aggregate segmentation},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.02.17.528925v1},
	doi = {10.1101/2023.02.17.528925},
	abstract = {Segmenting cells within cellular aggregates in 3D is a growing challenge in cell biology, due to improvements in capacity and accuracy of microscopy techniques. Here we describe a pipeline to segment images of cell aggregates in 3D. The pipeline combines neural network segmentations with active meshes. We apply our segmentation method to cultured mouse mammary duct organoids imaged over 24 hours with oblique plane microscopy, a high-throughput light-sheet fluorescence microscopy technique. We show that our method can also be applied to images of mouse embryonic stem cells imaged with a spinning disc microscope. We segment individual cells based on nuclei and cell membrane fluorescent markers, and track cells over time. We describe metrics to quantify the quality of the automated segmentation. Our segmentation pipeline involves a Fiji plugin which implement active meshes deformation and allows a user to create training data, automatically obtain segmentation meshes from original image data or neural network prediction, and manually curate segmentation data to identify and correct mistakes. Our active meshes-based approach facilitates segmentation postprocessing, correction, and integration with neural network prediction.
Statement of significance In vitro culture of organ-like structures derived from stem cells, so-called organoids, allows to image tissue morphogenetic processes with high temporal and spatial resolution. Three-dimensional segmentation of cell shape in timelapse movies of these developing organoids is however a significant challenge. In this work, we propose an image analysis pipeline for cell aggregates that combines deep learning with active contour segmentations. This combination offers a flexible and efficient way to segment three-dimensional cell images, which we illustrate with by segmenting datasets of growing mammary gland organoids and mouse embryonic stem cells.},
	language = {en},
	urldate = {2023-02-23},
	publisher = {bioRxiv},
	author = {Smith, Matthew B. and Sparks, Hugh and Almagro, Jorge and Chaigne, Agathe and Behrens, Axel and Dunsby, Chris and Salbreux, Guillaume},
	month = feb,
	year = {2023},
	note = {Pages: 2023.02.17.528925
Section: New Results},
	file = {Full Text PDF:/home/pape/Zotero/storage/DNL7FP3A/Smith et al. - 2023 - Active mesh and neural network pipeline for cell a.pdf:application/pdf},
}

@article{yeh_novo_2023,
	title = {De novo design of luciferases using deep learning},
	volume = {614},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-05696-3},
	doi = {10.1038/s41586-023-05696-3},
	abstract = {De novo enzyme design has sought to introduce active sites and substrate-binding pockets that are predicted to catalyse a reaction of interest into geometrically compatible native scaffolds1,2, but has been limited by a lack of suitable protein structures and the complexity of native protein sequence–structure relationships. Here we describe a deep-learning-based ‘family-wide hallucination’ approach that generates large numbers of idealized protein structures containing diverse pocket shapes and designed sequences that encode them. We use these scaffolds to design artificial luciferases that selectively catalyse the oxidative chemiluminescence of the synthetic luciferin substrates diphenylterazine3 and 2-deoxycoelenterazine. The designed active sites position an arginine guanidinium group adjacent to an anion that develops during the reaction in a binding pocket with high shape complementarity. For both luciferin substrates, we obtain designed luciferases with high selectivity; the most active of these is a small (13.9 kDa) and thermostable (with a melting temperature higher than 95 °C) enzyme that has a catalytic efficiency on diphenylterazine (kcat/Km = 106 M−1 s−1) comparable to that of native luciferases, but a much higher substrate specificity. The creation of highly active and specific biocatalysts from scratch with broad applications in biomedicine is a key milestone for computational enzyme design, and our approach should enable generation of a wide range of luciferases and other enzymes.},
	language = {en},
	number = {7949},
	urldate = {2023-02-25},
	journal = {Nature},
	author = {Yeh, Andy Hsien-Wei and Norn, Christoffer and Kipnis, Yakov and Tischer, Doug and Pellock, Samuel J. and Evans, Declan and Ma, Pengchen and Lee, Gyu Rie and Zhang, Jason Z. and Anishchenko, Ivan and Coventry, Brian and Cao, Longxing and Dauparas, Justas and Halabiya, Samer and DeWitt, Michelle and Carter, Lauren and Houk, K. N. and Baker, David},
	month = feb,
	year = {2023},
	note = {Number: 7949
Publisher: Nature Publishing Group},
	keywords = {Enzymes, Protein design, Sensors and probes},
	pages = {774--780},
	file = {Full Text PDF:/home/pape/Zotero/storage/ZLB7NP9S/Yeh et al. - 2023 - De novo design of luciferases using deep learning.pdf:application/pdf},
}

@article{baek_accurate_2021,
	title = {Accurate prediction of protein structures and interactions using a three-track neural network},
	volume = {373},
	url = {https://www.science.org/doi/10.1126/science.abj8754},
	doi = {10.1126/science.abj8754},
	abstract = {DeepMind presented notably accurate predictions at the recent 14th Critical Assessment of Structure Prediction (CASP14) conference. We explored network architectures that incorporate related ideas and obtained the best performance with a three-track network in which information at the one-dimensional (1D) sequence level, the 2D distance map level, and the 3D coordinate level is successively transformed and integrated. The three-track network produces structure predictions with accuracies approaching those of DeepMind in CASP14, enables the rapid solution of challenging x-ray crystallography and cryo–electron microscopy structure modeling problems, and provides insights into the functions of proteins of currently unknown structure. The network also enables rapid generation of accurate protein-protein complex models from sequence information alone, short-circuiting traditional approaches that require modeling of individual subunits followed by docking. We make the method available to the scientific community to speed biological research.},
	number = {6557},
	urldate = {2023-02-26},
	journal = {Science},
	author = {Baek, Minkyung and DiMaio, Frank and Anishchenko, Ivan and Dauparas, Justas and Ovchinnikov, Sergey and Lee, Gyu Rie and Wang, Jue and Cong, Qian and Kinch, Lisa N. and Schaeffer, R. Dustin and Millán, Claudia and Park, Hahnbeom and Adams, Carson and Glassman, Caleb R. and DeGiovanni, Andy and Pereira, Jose H. and Rodrigues, Andria V. and van Dijk, Alberdina A. and Ebrecht, Ana C. and Opperman, Diederik J. and Sagmeister, Theo and Buhlheller, Christoph and Pavkov-Keller, Tea and Rathinaswamy, Manoj K. and Dalwadi, Udit and Yip, Calvin K. and Burke, John E. and Garcia, K. Christopher and Grishin, Nick V. and Adams, Paul D. and Read, Randy J. and Baker, David},
	month = aug,
	year = {2021},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {871--876},
	file = {Accepted Version:/home/pape/Zotero/storage/RZT9H53G/Baek et al. - 2021 - Accurate prediction of protein structures and inte.pdf:application/pdf},
}
