\section{Definition of Terms} \label{app_definitions}

We address the problem of unsupervised domain adaptation (UDA). This problem is closely related to supervised domain adaptation (SDA) and semi-supervised learning (SSL). In SSL problems both labeled and unlabeled training data is given, with the goal to learn a "better" model using all available data compared to using fully supervised learning on just the labeled data. Both labeled and unlabeled data are from the same domain (same data distribution). In the case of UDA a source domain with labels and a target domain without labels is given. Source and target domain have different data distributions, corresponding e.g. to different imaging devices or different experimental conditions in biomedical applications. The goal of UDA is to train a model that solves the same task (e.g. cell segmentation) on the target domain as on the source domain. The case of SDA is very similar, but the target domain is partially labeled (i.e. annotations are provided for a subset of the target samples).
This discussion shows that all three settings are similar, with the distinction being the data distributions for labeled and unlabeled data:
In SSL both labeled and unlabeled data come from the same data distribution and in UDA they come from two different data distributions (source and target data distribution). In SDA the labeled data comes from both source and target distribution (usually with the fraction of target data being significantly smaller) and the unlabeled data comes only from the target distribution. Consequently self-training methods can be generalized to all three learning problems, as has recently been demonstrated by \textit{AdaMatch} \cite{berthelot_adamatch_2022}. Using the same recipe, our method is applicable to SSL and SDA as well; in fact we apply it in a SSL fashion in our proof-of-concept for instance segmentation (see App.~\ref{app_instance_seg}). 

We make use of self-training with pseudo-labels to address UDA. These terms are sometimes used with slightly different meanings in the literature.  
Here, we use self-training to describe methods that use a version of the model being trained to generate predictions on unlabeled data, which are then used as targets in an unsupervised loss function to again train the model. This can be understood as a student-teacher set-up, with the teacher being a version of the student (e.g. through EMA of weights or weight sharing). We use the term pseudo-labeling to describe the process of transforming the teacher predictions into targets for the unsupervised loss function, e.g. by post-processing or filtering (masking or weighting) them. Note that the literature sometimes distinguish between pseudo-labeling when the likeliest prediction is used as hard target in the unsupervised loss, and consistency regularization when the softmax output (more generally output after the last activation) is used as target. See for example \url{https://lilianweng.github.io/posts/2021-12-05-semi-supervised/} for an in-depth discussion. However, this distinction is minor in practice and can be incorporated in the pseudo-labeling post-processing in our formulation (the function $p$ in Eq.~\ref{eq_pseudolabels}). Hence, we do not make this distinction throughout the paper. 

\section{Probabilistic Domain Adaptation: Training Strategies} \label{app_pseudocode}

We implement two different approaches to domain adaptation: \textit{joint} training (model is trained jointly on labeled source and unlabeled target data, using a supervised and unsupervised loss function) and \textit{separate} training (model is first pre-trained on the labeled source data, using only the supervised loss, and then fine-tuned on the unlabeled target data, using only the unsupervised loss). Here, we show pseudo-code for the two training routines, for \textit{joint} training in Alg.~\ref{alg_1} and for \textit{separate} training in Alg.~\ref{alg_2}. For simplicity we omit validation, which is performed using the target data in both cases. Here, we use the same loss function $l$ for both the supervised and unsupervised loss.

\begin{algorithm}[h]
\SetAlgoLined
\KwIn{Labeled training data \{($x_s$, $y_s$)\}, unlabeled training data \{$x_u$\}, teacher and student models $s$, $t$, number of iterations $N$}
Initialize parameters of $s$ and $t$\;
\For{$i \gets 1$ to $N$}{
Sample mini-batch ($x_s^i$, $y_s^i$) and $x_u^i$\;
Compute supervised loss $L_s = l(s(x_s^i), y_s^i)$\;
Sample augmentations $\tau_s$ and $\tau_t$\;
Compute pseudo-labels $\hat{y} = t(\tau_t(x_u))$\;
Compute unsupervised loss $L_u = l(f(s(\tau_s(x_s)), \hat{y}))$\;
Compute gradients, update parameters of $s$ based on $L_s + L_u$\;
Update parameters of $t$ from $s$;
}
\caption{Pseudo code for the joint training strategy.}
\label{alg_1}
\end{algorithm}
\vspace{-1.0cm}
\begin{algorithm}[h]
\SetAlgoLined
\KwIn{Unlabeled training data \{$x_u$\}, pre-trained model $s$, number of iterations $N$}
Copy model $t$ from $s$\;
\For{$i \gets 1$ to $N$}{
Sample mini-batch $x_u^i$\;
Sample augmentations $\tau_s$ and $\tau_t$\;
Compute pseudo-labels $\hat{y} = t(\tau_t(x_u))$\;
Compute unsupervised loss $L_u = l(f(s(\tau_s(x_s)), \hat{y}))$\;
Compute gradients, update parameters of $s$ based on $L_u$\;
Update parameters of $t$ from $s$;
}
\caption{Pseudo code for the separate training strategy. (Only the adaptation stage on the target domain; source training follows regular supervised learning.)}
\label{alg_2}
\end{algorithm}

The two self-training approaches we implement, \textit{MeanTeacher} and \textit{AdaMatch} correspond to different choices for the teacher and student augmentations $\tau_s$ and $\tau_t$ as well as the teacher update scheme. For \textit{MeanTeacher} both $\tau_s$ and $\tau_t$ are sampled from a distribution of weak augmentations and the weights of $t$ are the EMA of $s$. For \textit{FixMatch} $\tau_s$ is sampled from a distribution of strong augmentations and $\tau_t$ from a distribution of strong augmentations, $s$ and $t$ share weights. The different pseudo-label filtering approaches are realized by different choices for $f$, where \textit{consensus masking} corresponds to only computing gradients for pixels that have a value of 1 in the consensus response (see Eq.~\ref{eq_consensus}), \textit{consensus weighting} corresponds to weighting the loss by the consensus response. In the case of no filtering $f$ is the identity.

\section{Implementation} \label{app_method_details}

We use the same UNet and PUNet architecture for all experiments, using an encoder-decoder architecture following the respective implementations of \cite{ronneberger2015u} and \cite{kohl_probabilistic_2019}. We increase the number of channels from 64 to 128, 256 and 512 in the encoder, and decrease it accordingly in the decoder. We use 2D segmentation network, hence both architectures make use of 2d convolutions, 2d max-pooling and 2d upsampling operations. The UNet is trained using the Dice Error (1. - Dice Score) as loss function. For the PUNet we use a similar formulation for the loss function as in \cite{kohl_probabilistic_2019}, but use the Dice Error for the reconstruction term in stead of the cross entropy. We use a dimension of 6 for the latent space predicted by the prior and posterior net of the PUNet. We use the Adam optimizer, relying on the default PyTorch parameter settings, except for the learning rate, and we use the \textit{ReduceLROnPlateau} learning rate scheduler. For \textit{joint} and source model trainings we train for 100k iteration, for the second stage of \textit{separate} trainings we train for 10k iterations.
We use different patch shapes, batch sizes and learning rates depending on the dataset and method; these values were determined by exploratory experiments.
For LiveCELL:
\begin{itemize}
    \item $UNet$: patch shape: (256, 256); batch size: 4; learning rate: 1e-4
    \item $PUNet$: patch shape: (512, 512); batch size: 4; learning rate: 1e-5
    \item $PUNet_{trg}$, $MT_s$: patch shape: (512, 512); batch size: 2; learning rate: 1e-5
    \item $FM_s$: patch shape: (256, 256); batch size: 2; learning rate: 1e-7
    \item $FM_j$, $MT_j$: patch shape: (256, 256); batch size: 2; learning rate: 1e-5
\end{itemize}
For mitochondria segmentation in EM:
\begin{itemize}
    \item $UNet$, $PUNet$: patch shape: (512, 512); batch size: 2; learning rate: 1e-5
    \item $MT_s$: (256, 256); batch size: 2; learning rate: 1e-5
\end{itemize}
For lung segmentation in X-Ray:
\begin{itemize}
    \item $UNet$: patch shape: (256, 256); batch size: 2; learning rate: 1e-4
    \item $PUNet$, $MT_s$, $MT_j$: patch shape: (256, 256); batch size: 2; learning rate: 1e-5
\end{itemize}

We use gaussian blurring and additive gaussian noise (applied randomly with a probability of 0.25, and with augmentations parameters also sampled from a distribution) as weak augmentations, and gaussian blurring, additive gaussian noise and random contrast adjustments (applied randomly with a probability of 0.5, and sampling from a wider range compared to the weak augmentations) as strong augmentations.

Our implementation is based on PyTorch. We use the PUNet implementation from \url{https://github.com/stefanknegt/Probabilistic-Unet-Pytorch}. All our code is available on github at \url{https://github.com/computational-cell-analytics/Probabilistic-Domain-Adaptation}. Please refer to the README for instructions on how to run and install it.

\section{Datasets} \label{app_dataset_details}

\paragraph{LiveCELL Dataset}

We use the LiveCELL dataset from \cite{edlund2021livecell}. This dataset contains about 5000 phase contrast microscopy images with instance segmentation ground-truth and predefined train-, test-, and validation-splits. We binarize the instance segmentation ground-truth to obtain a semantic segmentation problem. The dataset contains images of 8 different cell lines: A172, BT474, BV2, Huh7, MCF7, SHSY5Y, SkBr3 and SKOV3.
These cell lines show significant difference in appearance and morphology of cells as well as spatial distribution such as cell density and cell clustering. Hence, we treat all 8 cell types as different domains, and study the adaptation from one cell line as source domain to the seven other target domains for all 8 cell lines. The columns in Tab.~\ref{tab1} show the average dice score for one source applied to the seven target domains.

\paragraph{Mitochondria EM Segmentation}

For mitochondria segmentation in EM we use the dataset of \cite{wei2020mitoem} as source dataset. This dataset contains two EM volumes, one of human neural tissue, the other of rat neural tissue, imaged with scanning EM. Each volume contains 400 images with instance annotations for training, and 100 images with instance annotations for testing. We binarize the instance segmentation ground-truth to obtain a semantic segmentation problem. We study domain adaptation with \cite{wei2020mitoem} as source for two different target datasets. The first is \textit{Lucchi} \cite{lucchi2012structured}, which contains two volumes of tissue from the murine hippocampus imaged with FIBSEM that both contain mitochondria instance annotations. We use one of the volumes for training the domain adaptation methods (either via joint or separate training), and the other for evaluation. And \textit{VNC} \cite{gerhard2013segmented}, which contains two volumes from the ventral nerve cord of a fruit fly, imaged with serial section transmission EM. Only one of the two volumes contains instance annotation, it is used for evaluation, the other does not, it is used for training the domain adaptation methods (which does not require labels).

\paragraph{Lung X-Ray Segmentation}

For the lung segmentation task we use four different datasets of chest radiographs, following the experiment set-up of \cite{tang2019xlsor}. The datasets are: \textit{NIH}, which contains chest X-Ray (CXR) images with various severity of lung diseases, \textit{Montgomery} \cite{jaeger2014two}, which contains images of patients with and without tuberculosis, and \textit{JSRT} \cite{shiraishi2000development}, which contains images of patients with and without lung nodules. The \textit{JSRT} dataset is split into two subsets: \textit{JSRT1} with normal CXR images (60 images, 50 train, 10 test), and \textit{JSRT2} with inverted CXR images (247 images, 199 train, 48 test). The \textit{NIH} dataset contains 100 images (we use 80 for training and 20 for testing) and the \textit{Montgomery} dataset contains 138 images (113 are used trainining, 25 for testing) respectively. All datasets contain binary lung annotations; we discard additional annotations in the case of \textit{JSRT2}.
We treat each dataset as a separate domain, and perform domain adaptation for all pairs of domains.

\section{Livecell Segmentation Results} \label{app_livecell_results}

\begin{figure}[hb]
    \centering
    \includegraphics[width=\textwidth]{figures/fig-livecell-appendix.png}
    \caption{Qualtiative results for livecell segmentation: left image shows the source domain with labels, right shows the target domain, including predictions from four methods, PUNet trained on source, adapted to target, and two self-training approaches.}
    \label{fig_livecell_extended}
\end{figure}
%\vspace{-0.4cm}

\section{Extension to instance segmentation} \label{app_instance_seg}

We implement a proof-of-concept extension fo our approach to instance segmentation, using data from the NeurIPS Cell Segmentation Challenge (\url{https://neurips22-cellseg.grand-challenge.org/}).
This dataset contains 1,000 training images with instance segmentation annotations, and 1,500 additional images without annotations. The images come from 4 different microscopy domains (Brightfield, Fluorescence, Phase-contrast, Differential inference contrast). % The instance segmentation results for an additional 101 images can be evaluated against private segmentation annotations by submitting the results online.

We adapt our approach to instance segmentation by predicting 2 channels: the cell foreground (same as in the LiveCELL experiments) and the cell boundaries. To obtain an instance segmentation we use a seeded watershed algorithm, using the boundary predictions as height map and connected components of the foreground predictions subtracted by boundary probabilities as seeds. We train a PUNet fully supervised on the data with labels, and compare it to predictions from all $MT_j$ variations ($MT_j$, $MT_j^w$, $MT_j^m$). Note that we train in a \emph{semi-supervised} fashion here, and train the model jointly on all labeled and unlabeled data, without taking the different domains into account.

We have only studied the results qualitatively, using additional (unlabeled) validation data provided by the challenge. We have seen a qualitative improvement of the results through semi-supervised training, in particular $MT_j^w$ shows promise. We compare its predicions to the baseline PUNet in Fig.~\ref{fig_neurips} for two example images from the held-out set.

\begin{figure}[hb]
    \centering
    \includegraphics[width=\textwidth]{figures/fig_neurips.png}
    \caption{Instance segmentation results obtained from PUNet compared with $MT_j^w$ on two different images from the NeurIPS challenge held-out set. We observe that the semi-supervised model recovers more cells for the differential intensity contrast images (top row) and suffers less wrong merges (incorrect joining into one object of two or more cells) for the phase-contrast images (bottom row).}
    \label{fig_neurips}
\end{figure}

% Scores:
% Mean F1 Scores for:
% PUNet - 0.6368
% Challenge Baseline (by Organizers) - 0.5482
% nnUNet - 0.3965
% First AdaMT submission - 0.5786 
% Second AdaMT submission - 0.6291