\section{Theoretical Preliminaries}

This work compares multiple methods for modeling the latent space in a geometrically meaningful way. Before the specific details of these methods can be explained, it is important to provide the reader with a general theoretical background. This will focus on the basics of variational Bayes and variational auto-encoders, basic Riemannian geometry and the theory underlying steerable group convolutional networks.

\subsection{Riemannian Geometry}


% \citeauthor{arvanitidis2017latent} for example, argue that to increase robustness, we should consider infinitesimal distances along the data manifold in the input space. This would imply that mathematically, the latent space has to be seen as an instance of curved space instead of a Euclidean one. 

As mentioned in the background (ref section), a real, smooth manifold is a space that is locally similar to a linear space. Riemannian geometry allows for defining notions of angles, distances, and volume on such spaces by endowing the manifold with a \textit{Riemannian Metric}. The manifold can then be considered as a \textit{Riemannian manifold}.
We define an $m$-dimensional Riemannian manifold embedded in an ambient Euclidean space $\mathcal{X} = \mathbf{R}^d$ and endowed with a \textit{Riemannian metric} $\mathbf{G} \triangleq (\mathbf{G}_{\mathbf{x}})_{\mathbf{x} \in \mathcal{M}}$ to be a smooth curved space $(\mathcal{M},G)$. 
For every point on the manifold $\mathcal{M}$, there exists a tangent vector $\mathbf{v}\in \mathcal{X}$ that is tangent to $\mathcal{M}$ at $\mathbf{x}$ iff there exists a smooth curve $\gamma:[0,1] \mapsto \mathcal{M}$ such that $\gamma(0)=\mathbf{x}$ and $\dot{\gamma}(0)=\mathbf{v}$.
The velocities of all such curves through $\mathbf{x}$ form the \emph{tangent space} $\mathcal{T}_{\mathbf{x}}\mathcal{M}=\{ \dot{\gamma} (0) \,|\, \gamma:\mathbf{R}\mapsto\mathcal{M} \text{ is smooth around $0$ and } \gamma(0)=\mathbf{x}\}$, which has the same dimensionality as the manifold. The tangent space can be viewed as the collection of all the different ways in which the points on the manifold can be passed. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{images/theoretical_background/Manifold_Example.png}
    \caption{Schematic example of a 2-D manifold $\mathcal{M}$ and its tangent space $\mathcal{M}_x \mathcal{T}$ at point $x$. The geodesic $\gamma(t)$ starts at $x$ and goes in the direction $\mathbf{v}$.}
    \label{fig:my_label}
\end{figure}

The Riemannian metric $G(\cdot)$ then equips each point $\mathbf{x}$ on the manifold with an inner product in the tangent space $\mathcal{T}_{\mathbf{x}}\mathcal{M}$, \textit{e}.\textit{g}. $\langle \mathbf{u}, \mathbf{v} \rangle_x = \mathbf{u} ^T \mathbf{G}_{\mathbf{x}} \mathbf{v}$. 
This induces a norm $\|\mathbf{u}\|_\mathbf{x}\,,\forall \mathbf{u} \in \mathcal{T}_{\mathbf{x}}\mathcal{M}$ locally defining the geometry of the manifold. Given these local notions, we can not only compute local angles, lengths and areas, but also derive global quantities by integrating over local properties. We can thus compute the length of any curve on the manifold $\gamma : [0,1] \rightarrow \mathcal{M}$, with $\gamma(0) = \mathbf{x}$ and $\gamma(1) = \mathbf{y}$ as the integral of its speed: $\len(\gamma) = \int_{0}^1 \|\dot{\gamma}(t)\|_{\gamma(t)}dt$.
The notion of length leads to a natural notion of distance by taking the infimum over all lengths of such curves, giving the \emph{gobal Riemannian distance} on $\mathcal{M}$, $d(\mathbf{x},\mathbf{y})=\inf_{\gamma}\len(\gamma)$. The constant speed length that minimizes the distance of a curve between two points, is called a \emph{geodesic} on $\mathcal{M}$. 

Should I also talk a little bit about exponential maps?


% Endowing $\mathcal{M}$ with the global Riemannian distance consequently defines a metric space $\left(\mathcal{M}, d_{\mathcal{M}}\right)$. 
%  The concept of moving along a "straight" curve with constant velocity is given by the exponential map. In particular, there is a unique unit speed geodesic $\gamma$ satisfying $\gamma(0)=z$ with initial tangent vector $\gamma^{\prime}(0)=v$. The corresponding exponential map is then defined by $\exp _z(v)=\gamma(1)$, as illustrated on Figure 2 . The logarithm map is the inverse $\log _x=\exp _x^{-1}: \mathcal{M} \rightarrow \mathcal{T}_z \mathcal{M}$. For geodesically complete manifolds, such as the Poincaré ball, $\exp _z$ is well-defined on the full tangent space $\mathcal{T}_x \mathcal{M}$ for all $z \in \mathcal{M}$.
% By the celebrated Picard Lindelöf theorem~\cite{coddington1955theory}, given any $(\x,\v)\in\TM$, there exists a unique \emph{maximal}\footnote{\emph{maximal} refers to the fact that the curve is as long as possible.} geodesic $\geo$ such that $\geo(0)=\x$ and $\dgeo(0)=\v$. Hence, we can define a unique diffeomorphism or \emph{exponential map}, sending $\x$ to the endpoint of the geodesic: $\Exp\pd(\v)=\geo(1)$. Note that the geodesic is not the only way to move away from $\x$ in the direction of $\v$ on $\Man$. In fact, any continuously differentiable, smooth map $\Rx:\TxM\mapsto \Man$ whose directional derivative along $\v$ is identity, \ie $\mathrm{D} \Rx(\zero)[\v]=\v$ and $\Rx(\zero) = \x$ allows for moving on the manifold in a given direction $\v$. Such $\Rx$, called \emph{retraction}, constitutes the basic building block of any on-manifold optimizer. In addition to those we also speak of a \emph{tangent space projector} $\Pi_{\x}:\Amb\mapsto\TxM$ able to project any \emph{ambient} vector to the tangent space at $\x$.

\subsection{Variational Autoencoders}
A Variational Autoencoder (VAE) \citep{maxkingma2013auto} is a neural network architecture that aims at learning a parameterized probability distribution $p_{\theta }$ describing the input data $x$'s true distribution $P(x)$. To do so, we assume that the input data can be characterized by a lower-dimensional latent distribution $z$. The marginal likelihood can then be written as \begin{align} p_\theta(x) = \int p_\theta(x|z)q_{prior}(z)dz\end{align} where $q_{prior}(z)dz$ is a prior distribution over the latent variables, that in case of the vanilla VAE is chosen as a standard
normal distribution. Unfortunately, computing $p_\theta(x)$ involves the posterior $p_\theta(z|x)$, which is computationally expensive and often intractable.
We therefore introduce an approximation $q_\phi(z|x)$ of the true posterior, which is computed by a neural network: the encoder. We can then train a variational autoencoder, consisting of the encoder, which computes the approximate posterior and the decoder, which computes the conditional likelihood $p_\theta(x|z)$.

Within the variational autoencoder framework, the encoder and decoder are optimized in a joint setting. For the decoder, the loss is defined as the reconstruction error between the input and generated output. Oftentimes, cross entropy or mean squared error is used for this. Meanwhile, for the encoder, we want the distance between the true posterior $p_\theta(z|x)$ and the approximate posterior  $q_\phi(z|x)$ to be as small as possible. For this we can use the Kullbeck-Leibler divergence between two distributions
\begin{align}
    \text{KL}(q_{\boldsymbol{\mathbf{\phi}}}(\boldsymbol{\mathbf{z}}\mid \boldsymbol{\mathbf{x}})\,\,||\,\,p_{\boldsymbol{\mathbf{\theta}}}(\boldsymbol{\mathbf{z}}\mid \boldsymbol{\mathbf{x}}))
    &= \mathbb{E}_{q_\phi} \big[ \log q_\phi(\mathbf{z}) \big] - \mathbb{E}_{q_\phi} \big[ \log p_\theta(\mathbf{z} | \mathbf{x}) \big]\\
    &= \mathbb{E}_{q_\phi} \big[ \log q_\phi(\mathbf{z}) \big] - \mathbb{E}_{q_\phi} \bigg[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z}) }{p_\theta(\mathbf{x})} \bigg]\\
    &= \mathbb{E}_{q_\phi} \big[ \log q_\phi(\mathbf{z}) \big] - \mathbb{E}_{q_\phi} \big[ \log p_\theta(\mathbf{x}, \mathbf{z}) - \log p_\theta(\mathbf{x}) \big]\\
    &= \mathbb{E}_{q_\phi} \big[ \log q_\phi(\mathbf{z}) - \log p_\theta(\mathbf{x}, \mathbf{z}) \big] + \mathbb{E}_{q_\phi} \big[ \log p_\theta(\mathbf{x}) \big]\\
    &= \mathbb{E}_{q_\phi} \big[ \log q_\phi(\mathbf{z}) - \log p_\theta(\mathbf{x}, \mathbf{z}) \big] + \underbrace{\log p_\theta(\mathbf{x})}_{\text{intractable}}
\end{align}
However, as can be seen when we rewrite the equation, we still have the intractable evidence term $\log p_\theta(\mathbf{x})$. We therefore introduce a lower bound of the log likelihood using Jensen’s inequality. 
\begin{align}
    \log p(x) &=\log \int_{\mathbf{z}} p_\theta(\mathbf{x}, \mathbf{z}) \\
    &=\log \int_{\mathbf{z}} p_\theta(\mathbf{x}, \mathbf{z}) \frac{q_\phi(\mathbf{z})}{q_\phi(\mathbf{z})} \\
    &=\log \left(\mathbb{E}_{q_\phi}\left[\frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z})}\right]\right) \\
    & \geq \underbrace{\mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{x}, \mathbf{z})]-\mathbb{E}_{q_\phi}[\log q_\phi(\mathbf{z})]}_\text{ELBO} 
\end{align}
This lower bound is called the Evidence Lower BOund (ELBO) \citep{maxkingma2013auto}. Using the ELBO, we can rewrite the KL divergence
\begin{align}
\mathbb{KL}(q_\phi(\mathbf{z}) \| p_\theta(\mathbf{z} \mid \mathbf{x})) &=\mathbb{E}_{q_\phi}\left[\log \frac{q_\phi(\mathbf{z})}{p_\theta(\mathbf{z} \mid \mathbf{x})}\right] \\
&=\mathbb{E}_{q_\phi}[\log q_\phi(\mathbf{z})]-\mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{z} \mid \mathbf{x})] \\
&=\mathbb{E}_{q_\phi}[\log q_\phi(\mathbf{z})]-\mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{z}, \mathbf{x})]+\log p_\theta(\mathbf{x}) \\
&=-\left(\mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{z}, \mathbf{x})]-\mathbb{E}_{q_\phi}[\log q_\phi(\mathbf{z})]\right)+\log p_\theta(\mathbf{x})
\end{align}
such that is consists of the log marginal probability of $\mathbf{x}$ and the negative ELBO. Because $\log p_\theta(\mathbf{x})$ does not depend on $q$, minimizing the KL divergence is equal to maximizing the ELBO. Now, all that remains is to solve the problem of the random sampling operation from $\mathbf{z}$ not being differentiable. \citeauthor{maxkingma2013auto} propose to solve this by using the \textit{reparameterization trick}, which suggests that instead some noise $\epsilon$ is sampled from a unit Gaussian distribution. We can then add the mean to this noise term and multiply it by the variance to arrive at a mean and variance as would have been directly sampled from the latent distribution, but that allows for backpropagation through the neural network. 

% https://mpatacchiola.github.io/blog/2021/01/25/intro-variational-inference.html



\subsection{Group Convolutional Neural Networks}

Besides comparing the different geometric latent spaces, we also experiment with learning representations that are orientation-invariant. The rotation invariance is achieved by extending the VAE model encoder and decoder networks from regular convolutional neural networks to Group-Convolutional Neural Networks (G-CNNs) \cite{cohen2016group}. G-CNNs generally consist of three main elements that set it apart from a regular CNN: a lifting convolution, a group convolution and a projection operation. The lifting convolution disentangles the original image by transforming the features in the image for each $h \in H$, where $H$ is the order of the cyclic group used for the network. Next, these lifted layers are convolved with a special group kernel in the group convolution layers of the network, similar to how feature maps are convolved with kernels in regular CNN convolutional layers. Finally, the projection operation reduces the dimensionality of the batch, channels and shape, by performing a projection with an operation that is invariant to the group action. This can for example be summing, maxing, or averaging. We, therefore, obtain an output representation that is invariant to the group action, while also having the same dimensionality as a regular CNN would have had. 
        
One possible downside of the above framework is that the values of the feature maps are computed and stored on each element of the group. The computational complexity of the model, therefore, scales with the order of the group that is used, thus making it inefficient for creating a network that is equivariant to groups with infinite elements. \cite{cohen2016steerable} therefore propose a more general framework through \textit{steerable} CNNs. Steerable CNNs still use groups, but instead of storing the value of a feature map on each group element, the model uses harmonic analysis to store the Fourier transform of this feature map. This is done in $\mathbb{R}^{d \times d}$, where $d$ is the collection of functions obtained by the Fourier transform. This allows the model to efficiently store up to a finite number of frequencies. After the group convolutions are performed, the Fourier functions can be transformed back to their original representation, resulting in the network output being comparable to that of a regular CNN again. 

This section could be a bit more precise on the theoeretics, needs more math.
% Since different variants of G-CNNs allow for different kinds of equivariance, we test three different types. The first network that is implemented is a C8 -referring to the cyclic group $C8$- steerable network, meaning that it is equivariant to rotations of multiples of 45 degrees