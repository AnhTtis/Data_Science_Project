\chapter{Experimental Set-up}\label{experiments}
Our goal is to evaluate whether the geometrically-inspired VAEs introduced in Section \ref{section:method}, can be used to model the progression of BE in a more meaningful way than regular VAEs. We therefore test all models on a variety of tasks, meant to evaluate the quality of the learned representations in both a quantitative and qualitative way. We conduct experiments with reconstruction loss, classifiability of latent vectors, random generative sampling quality, and the ability to generate smooth interpolations. This section will detail the exact setup for conducting these experiments. Section \ref{subsec:data} will start by giving an overview of what datasets were used, how these were obtained, and how the data was processed. Next, Section \ref{subsec:experiments} will give a detailed overview of the model architectures, hyperparameter choice and training procedure. Finally, Section \ref{ex:evaluation} will describe how we will evaluate the quality of the learned representations and motivate the choice for these experiments. 

\section{Data}\label{subsec:data}
% Analogous to a real-life histopathologist, 
To test whether the proposed methods can learn to model the progression of BE in an unsupervised setting, we train them on histopathological image data containing instances of the four different progression stages. The sources and format of this data are discussed in Subsection \ref{datasets}, and the processing of the data is discussed in Subsection \ref{preprocessing}.

    \subsection{Datasets}\label{datasets}
    We train our models on digitized H\&E-stained endoscopic biopsies. Four separate datasets containing such scans are obtained. The \textbf{ASL} and \textbf{RBE} datasets consist of annotated biopsies from archived BE screenings from the Amsterdam University Medical Centre (AMC). Furthermore, the \textbf{LANS} dataset consists of cases from the Dutch expert board of esophageal cancer (Landelijk Adviesorgaan Neoplasie Slokdarm). As mentioned before, the interobserver variability between pathologists can be very high, which could make the annotations made by the individual pathologists of the ASL, RBE and LANS datasets less reliable. A study by \citet{bolero} therefore set out to study this variability and create a dataset with more reliable annotations. This dataset, \textbf{BOLERO} (Barrett mOlecuLar ExpeRt cOnsensus), contains biopsies that were each assessed by a review panel of 4 expert BE pathologists, with a total test panel of 51 pathologists from over twenty countries. The study found that pathologists overall had a good concordance for distinguishing non-dysplastic and dysplastic BE, with agreeability rates for NDBE and HGD being almost 80\%, but that the distinction between LGD and HGD was harder to make, leading to a rate of 40 \% for LGD. 

    \begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{images/background/biopsy_example_vivien.png}
        \caption{Example of a scanned endoscopic biopsy.}
        \label{fig:biopsy}
    \end{figure}
    
    Combining these four datasets leads to a total number of 934 biopsies from 324 patients. All biopsies are stored in a format known as a Whole-Slide Image (WSI). Whole-slide imaging is also known as virtual microscopy and stores highly precise scans of a glass slides, containing up to four biopsies from the same patient, at multiple microscopic magnification levels ranging from $5\times$ to $40\times$ \cite{li2022comprehensive}. An example of one such biopsy at the lowest magnification level is shown in Figure \ref{fig:biopsy}. 
    Pathologists examine these images with specialized software and divide the entire biopsy into designated regions of a specific type of cell tissue. For the current dataset, these are \textit{background}, where no cell tissue is present, \textit{stroma}, which is connective tissue between cells, and the four classes used for diagnosing BE: \textit{squamous epithelium} \textit{Non-Dysplastic Barrett's Esophagus} (NDBE), \textit{Low-Grade Dysplastic Barrett's esophagus} (LGD) and \textit{High-Grade Dysplastic Barrett's esophagus} (HGD). For these last four categories, the annotations are made at glandular level by manually tracing around tissue regions. Except for the BOLERO dataset, which was annotated by panels of multiple pathologists, all annotations are made by a single expert-level histopathologist and are in accordance with case-level diagnoses of biopsies made by a review panel. 



    \subsection{Preprocessing the dataset}\label{preprocessing}
    As can be observed in Figure \ref{fig:biopsy}, a biopsy contains multiple types of tissue and many fine-grained details. Whole-slide images composed of up to five biopsies therefore contain an enormous amount of data. Files of this format are often larger than 1 GB and typically contain more than $100.000 \times 100.000$ pixels. Hence, it is essential to preprocess the data to allow deep learning models to perform computations with it. Most studies working with WSI files choose to divide the image into smaller standard image format patches. Multiple approaches to determining these patches exist, however, by far the most common method is to simply divide the WSI uniformly into smaller patches of the same size \cite{li2022comprehensive}.
    
    \subsubsection{Determining Patch Area}
    Following the uniform slicing approach, a number of important choices remain to be made in regard to what information is contained within the patches. One such choice is the patch size, which is closely interlaced with the microscopic magnification level. Ideally, we want to create patches that, when individually processed by the model, contain enough information for the model to be able to recognize certain stages of BE. This means that a somewhat larger area of the WSI should be covered in each patch, such that general changes in cell morphology can be observed. However, we are limited in the image resolution of the patches we create, as VAEs generally work with somewhat smaller image sizes \citep{thambawita2021impact}. Additionally, we found that hyperspherical model types tend to be less numerically stable than normal-type VAEs, and that larger patches increase the already heavier computational power required for roto-equivariant model types. Since we want to equally compare performance across all different models, we choose to have a resolution of $64 \times 64$ for each image patch, allowing all models to easily process them. Within this patch size, we then want to contain as much information about the cell tissue as possible. We show $64 \times 64$ patches of $5\times$ and $10\times$, the lowest possible magnification levels, to an expert-pathologist and biomedical scientist, who both indicate that $10\times$ patches, unlike $5\times$, do not contain enough context to accurately make a classification. To illustrate this, four examples of patches with different magnification levels are shown in Figure \ref{fig:magnifications}. For $5\times$ magnification, some patches still do not cover enough of the biopsy to make a perfect classification, but for many patches, clear cases of squamous NDBE, LGD and HGD type tissue can be observed. We therefore choose to work with the $5\times$ magnification patches. 

    \begin{figure}
    \centering
      \subfloat[a][$5\times$]{\includegraphics[width=0.15\textwidth]{images/background/102.png}} \quad
      \subfloat[b][$10\times$]{\includegraphics[width=0.15\textwidth]{images/background/71.png} \label{fig:b}}
      \quad
      \subfloat[c][$5\times$]{\includegraphics[width=0.15\textwidth]{images/background/472.png}} \quad
      \subfloat[d][$10\times$]{\includegraphics[width=0.15\textwidth]{images/background/133.png} \label{fig:10xa}}
    \caption{Comparison between different magnification levels. All patches are of size 64$\times$64 and were taken from the same biopsy.} \label{fig:magnifications}
    \end{figure}

    \subsubsection{Discarding Low-Context Patches}
    To further increase the amount of context present in the patches, we choose to only include patches in our final dataset that contain a high enough threshold of relevant tissue. By relevant tissue, we mean belonging to either the squamous, NDBE, LGD or HGD classes, meaning that background and stroma are excluded. This design choice was determined in accordance with a histopathologist, who argued that pathologists disregard the background and stroma  classes when determining BE. As such, only patches containing 50\% or more data annotated as belonging to the relevant classes are selected. This threshold was determined heuristically by experimenting with several threshold values. A number much higher than 0.5 was found to lead to a too significant reduction in the total amount of patches included in the final dataset, while a lower threshold resulted in a large number of low-context patches that were difficult for a histopathologist to classify. With a threshold of 0.5, an image resolution of $64 \times 64$ and a magnification level of $5\times$, we create a dataset containing a total of 75.661 patches. 

    \subsubsection{Computing Labels and Balancing the Dataset}
    Although this work studies unsupervised representation learning, we nevertheless want to obtain labels for the individual patches as well. The reason for this is twofold. First, biopsies are known to contain a large amount of squamous and NDBE-type tissue, and less LGD and HGD. Because we want the model to be trained on all different instances of the progression stage equally, there is a need to balance the training dataset. We therefore require patches to be labeled in order to know which ones to include in the training set. Second, as will be discussed more in-depth in Section \ref{ex:evaluation}, we conduct experiments into the classifiability of the learned representation vectors. To evaluate the experiments, we compute classification accuracy, a metric which requires ground truth labels. Hence, labels are computed for every individual patch. 
    
    These labels are computed according to annotations made by the pathologist, which are contained in a mask file accompanying each WSI. The format of these mask files is such that all pixels within a traced region designated by the pathologist to belong to a certain class, are assigned an integer value corresponding to that class, i.e., $0$ for background, $1$ for stroma, $2$ for squamous, and so on. This results in an image file consisting of only six possible pixel values, that encode the pathologist's annotations in a way that can easily be processed. In all four different datasets, such a mask file is provided for the WSIs. When dividing the WSIs into smaller patches, we therefore also divide the corresponding mask files into the same format. 

    We then convert these masks into scalar labels, so that they can be used to evaluate our models' performance. To do so, we choose to take the most dominant class, or in other words the most commonly occurring pixel value within a patch, as its label class. In doing so, we discover a class imbalance in our dataset: 16.1\% of the patches are labeled as squamous, only 11.9\% as HGD and 11.5\% as LGD, while a great majority of 60.5\% are labeled as NDBE. Because we want our model to be exposed to instances of classes equally during training, we decide to stratify the dataset. For each class, we take the 8000 patches with the highest dominance of the corresponding class pixel, leading to a balanced dataset with 32.000 patches that contain the most clear cases obtainable within each class. 

    For our model pipeline, we choose to use the patches belonging to the ASL, RBE and LANS datasets as training data and those belonging to BOLERO as test data. In this way, we expose the model to a variety of unlabeled training data and evaluate it on an unseen dataset with far more reliable labels as compared to the other datasets. We can therefore fairly asses both the generalization capabilities of our models, and determine how well they pick up on the different classes during training. We also designate 10\% of our training set as validation dataset.



\section{Models and Training}\label{subsec:experiments}
% \subsection{Model Architectures}


    In Section \ref{section:method}, we explained a number of different concepts that could improve the structuring of the VAE's learned latent space. These were the Riemannian latent space of RHVAE, and the hyperspherical latent space of \svae. Moreover, we described how the hyperspherical VAE could be extended to be roto-equivariant, as proposed by \citet{vadgama2022kendall}. Finally, we proposed a modification of the hyperspherical setup to a non-variational autoencoder framework. 
    
    To test which model provides the best latent structuring, we compare RHVAE and \svae to a baseline vanilla VAE, which we will refer to as \nvae, where $\mathcal{N}$ stands for the Gaussian distribution. Secondly, to assess the possible benefits of the roto-equivariant \svae, we build and compare such equivariant versions of \svae and \nvae, as detailed in Subsection \ref{subsec:ksvae}. Finally, to test the validity of our own proposed spherical autoencoder framework (\sae), we create non-variational autoencoder variants of both equivariant and non-equivariant hyperspherical and normal-type models. 
    Unfortunately, experiments for RHVAE could not be conducted, as the mode failed to generalize to large datasets. This will be further elaborated on in Subsection \ref{failed}. The remaining sections will focus on the other models described above, and we will conduct experiments for a total of eight different models: \nvae, \nae, equivariant \nvae, equivariant \nae, \svae, \sae, equivariant \svae and equivariant \sae.

    % \subsection{Model Training}\label{ex:model_training}

    \subsection{Model Architectures}
    In order to compare the different models fairly, we follow an almost identical architecture for all models. As \citeauthor{lafarge2020orientation} demonstrate good results on histopathological data with their vanilla and roto-equivariant VAEs, we base our architecture on theirs. A detailed schematic overview of the architecture can be found in Appendix \ref{ap:architectures}. In general, the base encoder architecture consists of three ConNeXt blocks \cite{liu2022convnet}, containing a convolution layer with kernel size 5, followed by a layer normalization, a pointwise convolution, a GELU activation, and another pointwise convolution. Each block is followed by a max pooling operation with kernel size 2. The decoder mirrors this structure, swapping out the convolutional blocks for their transposed versions and replacing the pooling operations with upsampling operations. We furthermore pad our input images to be of size 68 $\times$ 68, to ensure that all convolutions can be computed.
    
    In case of the non-equivariant variational versions of \nvae and \svae, the encoder outputs the parameters for the relevant posterior distribution: a vector $\mu$ and an isotropic scalar $\sigma$ or $\kappa$. Additionally, for spherical models, the softmax operation is applied over $kappa$ and a value of 1 is added, ensuring a minimum variance of 1. 
    Besides the distribution parameters, a third parameter coding for the orientation of the input $\mathbf{R}$ is also learned in the equivariant setting. Moreover, in the equivariant framework, all convolutional layers are replaced by their group variants and the remaining layers are adapted for use with the extended channel format. We choose to follow \citeauthor{vadgama2022kendall} and use the $C_8$ cyclic permutation group for all group convolutions, leading to a model that is equivariant to eight rotations covering $SO$(2), or rotations of angles of 45 degrees.    
    
    For the normal non-variational autoencoder type models, \nae and equivariant \nae, we do not learn the log variance parameter $\sigma$ and instead take the learned vector $\mu$ directly as our latent encoding $\mathbf{z}$. In the case of the spherical autoencoder model types \sae and equivariant \sae, as described in \ref{subsec:sae}, we do not omit the concentration parameter $kappa$ completely, but instead fix it to a value of 1000, which was heuristically determined to fully limit the vMF distribution's variability. 


    \subsection{Model Hyperparameters} \label{ex:hyperparams}
    As the dimensionality of the latent space can have great influence on the models' output, we test a range of different latent dimension sizes. \citeauthor{lafarge2020orientation} use a dimensionality of $M=64$, so we choose to vary around this number, leading to a total of eight different sizes $M \in (3, 8, 16, 32, 64, 128, 256, 512)$. It is worth noting that we choose a multiple of 8 because of the $C_8$ group, which also limits the latent dimension of equivariant type models to have a minimum size of 8. Although we cannot use a latent dimension size lower than 8 for equivariant models, we nonetheless chose to include 3 in our range of sizes because of its property of being visualizable. This allows us to visually inspect the latent space and verify that the proposed methods are indeed following a different geometric structuring of the manifold. 
    
    \paragraph{Avoiding Numerical Instability} Another important factor in choosing the latent space sizes, is that spherical variational model types can become numerically unstable in higher dimensions \cite{davidson2018hyperspherical}. We found that for latent dimensions of 32 and higher, the model started outputting NaN (Not a Number) values, and the spherical variational models could not be trained. \citeauthor{davidson2018hyperspherical} propose this can in part be explained by the \textit{vanishing surface problem}. Considering the equation for the surface of a hypersphere
    \begin{align}
        \mathcal{S}(m-1) = r^m\frac{2(\pi^{m/2})}{\Gamma(m/2)},
    \end{align}
    it can be observed that in the infinite limit of dimension $m$, the surface area of the hypersphere approaches 0. The authors moreover already observe this vanishing surface problem in dimensions sizes higher than 20. This observation corresponds to our preliminary experiments, as a dimension of 16 could still be trained, while 32 unfortunately failed.
    We observed that the same does not hold for spherical autoencoder models, however, suggesting that the concentration parameter $\kappa$ plays an important role in the numerical instability. 
    We therefore implement a way to limit the minimum value for $\kappa$, which normally can range between 1 and 10000. Using $\kappa=100$ as our minimum value, we are able to train equivariant and non-equivariant spherical VAEs up to a dimension size of 256, albeit at the cost of somewhat limiting the expressivity of the model. The latter will be taken into account when analyzing the results. 


    \subsection{Training Details}    
    We then train all our models for 500 epochs, with a batch size of 128. We employ the Adam optimizer with a learning rate of 0.0005 and use a cosine annealing learning rate scheduler. Preliminary experiments reveal that all models can reach convergence with these settings. Prior to training, input images are padded to size 68 $\times$ 68 and normalized. Moreover, as mentioned in the previous section, we use MSE loss for all models as reconstruction loss. Since our equivariant models, due to rotations during training, lose out on information about the outer edges of an image, we choose to disregard these areas when computing the reconstruction loss.
    Furthermore, we determine regularization loss by computing KL divergence for variational model types and add this to the reconstruction loss.     

   
\section{Evaluation of Proposed Methods}\label{ex:evaluation}
    
    In this study, we seek to learn how providing different forms of geometric structure to a VAE's latent space can make representations of histopathological data more meaningful. This leads to the question of what is actually meant by a \textit{meaningful} representation. In this section, we therefore aim to provide the reader with an overview of the conducted experiments and motivate why they inform us about the quality of the learned representations. For quantitative evaluation, these experiments include reconstruction error and classifiability of latent representations. Furthermore, for qualitative evaluation, we experiment with random generative sampling and interpolation quality.  
    
    \paragraph{Reconstruction Error} 
    In all autoencoder frameworks, an image is compressed into a lower-dimensional representation by the encoder network and consequently reconstructed from this representation by the decoder. It naturally follows then, that the quality of the representation has a direct influence on the reconstruction. The more relevant features a model learns to encode in the latent space, and conversely the more irrelevant data it learns to discard in its encoding, the better the decoder will be able to create a reconstruction. We therefore quantitatively evaluate performance by reporting the reconstruction loss, which for all models is calculated using the MSE loss. During the reconstruction experiments, we subtract the outer edges of each image from the loss for both equivariant and non-equivariant models,  so that the area to be reconstructed is equal across all model types, and the losses can be compared fairly. After training, we apply each model to the unseen Bolero test set. A lower loss value means a reconstruction closer to the input image and indicates a more meaningful and informative latent representation. 
    
    \paragraph{Classification of Latent Representations}
    In the field of supervised learning of histopathology data, a CNN classification network receives image patches and learns to directly map them to their correct classes. Many different approaches to this problem have been proposed, but general performance is good, with many proposed methods reaching 80-90\% accuracy scores \cite{de2018survey}. 
    It would therefore be interesting to replace the input of this type of classifier model with the latent representations learned by our VAEs. In this way, we can directly measure how accuracy scores are affected by differently-structured latent representations. If, for example, spherical type models improve the structuring of the latent space in a meaningful way, for instance by an improved clustering ability over the vanilla VAE, then this would naturally lead to a higher ease of classification and a better classification accuracy. 

    We therefore train models for classifying both representation vectors and original image patches.     
    For our latent representation classifier, we use a simple MLP with three linear layers. Latent vectors encoded by a pre-trained model are forwarded through the model with a hidden dimension of 64 and mapped to a vector containing four outputs, of which the values code for the four class probabilities. We train this model for 500 epochs, use Cross Entropy to compute loss, and report accuracy scores for all models and latent dimension sizes. 
    Furthermore, to compare the accuracy scores of the representations to those of patches, we create a convolutional neural network that follows the same architecture as the VAE encoders described earlier. Both a regular convolutional network and an equivariant group-convolutional network are tested. We take the encoder network architectures of both variants and add an MLP head identical to the one used for the representation classification. The models are then trained in an identical setting to the representation classifiers, allowing for a fair comparison of classification ability between representations and image patches. 
    
    \paragraph{Random Generative Sampling}
    Besides the quantitative evaluation methods, we moreover evaluate the method in a qualitative way. One way to achieve this is to randomly sample new images from each of the models' latent spaces and visually determine their quality. We generate new images by sampling from the prior for all models except the autoencoder types, for which we simply decode a vector from a uniformly random location on the latent space. Sampling from a number of locations on the latent space can provide insights into how such a manifold space is structured. If, for example, there are many low-density areas in the space, then that will be reflected in the quality of samples originating from such an area. Conversely, if many random samples are high quality, this would suggest that the latent space is well-structured. 
    
    Besides general qualitative analysis, the generative sampling experiments will also be used to assess the ability of \sae and equivariant \sae models to be used as a generative model. Ordinary AEs will for most samples result in nonsensical images, and because of this are not considered a generative model. Examining images generated by \sae can thereby demonstrate to which capacity it can be used as a generative model. 
    
    \paragraph{Interpolations}
    Finally, we examine the models' ability to create smooth, realistic interpolations. Similarly to generative sampling, when interpolating, we sample and decode vectors from the latent space. However, unlike above, we do not sample from a random location on the latent space, but sample along the shortest path between two images in the latent space. For normal-type models, this means a linear path, while for Riemannian and spherical-type models, we sample along the geodesic. In both \cite{chadebec2020geometryaware} and \cite{davidson2018hyperspherical}, interpolations were used to demonstrate the improved latent structuring of the geometric VAE models. Interpolations appeared smooth and  were able to retain more realistic structure of objects. Ideally, we would want to be able to observe similar realistic variations in the morphological structure of cells for the current work. Such an interpolation would be an indication of a well-structured latent space, that is able to capture elements about the progression of healthy tissue to Barrett's esophagus.
    
    % \textcolor{red}{Maybe add how we compute the geodesic? This is easy in case of hyperspheres, but not for Riemannian (requires another neural network).}


    \section{Motivation for Not Continuing with RHVAE }\label{failed}
    Unfortunately, we found that it was impossible to train RHVAE with our full dataset. As described in Section \ref{subsec:rhvae}, the Riemannian metric is computed by a neural network, and this network is parameterized by the data points. The model complexity therefore scales with the size of the dataset, which in our case includes 32.000 data points. Such a size lead to infeasible computation times.
    
    When conducting initial experiments with RHVAE, the default settings as provided by the authors were used. This means that the encoder and decoder networks, and the network for learning the Riemannian metric, all consist of two linear layers, separated by a ReLU activation. The hyperparameters required for computing the metric in RHVAE are also kept at default, leading to a temperature of $T=1.5$, and a regularization parameter of $\lambda=0.01$.    
    Multiple dataset sizes were tried out, but we found that a dataset of 1000 points already increased training time by such an amount that it became infeasible to compute. 
    Attempts were made to reduce this by increasing the amount of data points included in one centroid, leading to a lower number of centroids in total. This somewhat increased the number of training points that were able to be used for training, however, it still could not come close to the magnitude of the 30.000 patches that are available. Experiments were also done with different neural network architectures, both for the Riemannian network as well as the encoder and decoder networks, and multiple different latent dimension sizes were tested. However, decreasing the complexity of these models did not have an effect on the infeasibility of the computation time. 
    
    To gain insight into the general performance of RHVAE for pathological images, we then experimented with smaller dataset sizes and trained the model with datasets of 100 and 400 data points. Examples of reconstructions made by these models are shown in Figure \ref{fig:rhvae_recon}. In both dataset sizes, we see that reconstructions are often not true to the original. Although it would be expected that reconstructions of an insufficiently trained model would appear as a less sharp version of their originals, here instead we see instances of reconstructions containing a completely different tissue pattern than their original input. 
    \begin{figure}[H]
    \centering
      \subfloat[a][Dataset size = 100]{\includegraphics[width=0.6\textwidth]{images/experiments/rhvae/reconstructions_100.png}}\\
      \subfloat[b][Dataset size = 400]{\includegraphics[width=0.6\textwidth]{images/experiments/rhvae/reconstructions_400.png} \label{fig:recon400}}
    \caption{Reconstructions made by RHVAE for different dataset sizes. The top row shows the original patches and the bottom row the reconstructions.} \label{fig:rhvae_recon}
    \end{figure}     
    \begin{figure}
    \centering
      \subfloat[b][Dataset size = 100]{\includegraphics[width=0.7\textwidth]{images/experiments/rhvae/generations_N100.png} \label{clust}} \\
      \subfloat[b][Dataset size = 400]{\includegraphics[width=0.7\textwidth]{images/experiments/rhvae/generations_400.png} \label{logdet}}
    \caption{Randomly generated images from the latent space learned by RHVAE for models trained with different dataset sizes. } \label{fig:rhvae_gen}
    \end{figure}
    It can also be observed that some structures appear multiple times, which becomes especially apparent in Subfigure \ref{fig:recon400}. These results suggest that the model might be overfitting and is inappropriately applying the patterns that it has learned to the reconstructed patches.   
    Another indication of overfitting becomes apparent from Figure \ref{fig:rhvae_gen}, which shows some examples of randomly generated images. For the model trained with 100 data points especially, the images look somewhat realistic, but the same structures are being repeated over different generated images. For a dataset size of 400, this problem becomes less apparent, but the generated image quality suffers. 

    Finally, Figure \ref{fig:rhvae_clusters} shows a visualization of a 3-dimensional latent space learned by RHVAE. Subfigure \ref{clust} shows the four different classes visualized as differently colored points. It can be seen that some clustering exists, with clusters for squamous and high-grade dysplasia becoming the most apparent out of the four progression stages. This is in line with the nature of the data, as these two classes are also known for being the least difficult to differentiate in histopathology as well \cite{bolero}. Furthermore  Subfigure \ref{logdet} shows the same latent space, but points are colored in accordance with the value of the log determinant of the metric tensor $\sqrt{\det(\mathbf{G}(z)}$, which corresponds to the volume element of the learned manifold. 
    \begin{figure}[H]
    \centering
      \subfloat[a][Training images belonging to different classes visualized in the latent space.]{\includegraphics[width=0.48\textwidth]{images/experiments/rhvae/riemancluster.png} \label{fig:b}} \quad
      \subfloat[b][The geodesic follows the most informed path, visualized here by $\sqrt{\det(\mathbf{G}(z)}$.]{\includegraphics[width=0.48\textwidth]{images/experiments/rhvae/geodesic_bar.png} \label{fig:b}}
    \caption{Two visualizations of training points embedded in the latent space learned by RHVAE with \textit{M=3}. } \label{fig:rhvae_clusters}
    \end{figure}    
    A lower value indicates a higher density of data points and therefore a more informed region within the manifold space. A geodesic between two random data points is calculated through the Riemannian metric and also plotted in red. It can be seen that this path does not appear as a straight line, which would be the case for a Euclidean distance, but rather as a curve. Moreover, this curve follows the direction of the highest density, creating a path that passes through the most informed region of the manifold space. 

    Although the clustering ability to some extent indicates the suitability of RHVAE for the current pathological use case, the earlier reconstructions and generations indicate that the model has a tendency to overfit for very small dataset sizes, and fails to generalize well for slightly larger datasets. This, combined with the severe limitation of dataset size, lead us to decide to not run further experiments on RHVAE and instead focus efforts on the remaining models. 

