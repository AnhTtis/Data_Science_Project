\chapter{Discussion}\label{discussion}
In this final section, we will discuss the results reported in Section \ref{results} and provide a conclusion for the experiments. Section \ref{analysis} will include the analysis of the results, Section \ref{future} will discuss some limitations and possibilities for future work, and Section \ref{conclusion} will summarize this work and provide the concluding remarks. 

\section{Analysis of Results}\label{analysis}
\vspace{-0.4cm}
% \subsection{Hyperspherical VAE}
To determine the potential of spherical models for learning meaningful histopathological representations, we first start by discussing the experiments for non-equivariant variational autoencoders and later discuss the effects of adding rotation-disentanglement to these models. Consequently, special attention is given to the non-variational framework, and we discuss the results for our own generative \sae model. 

\subsection{Evaluation of Spherical and Normal-Type VAEs}

\paragraph{Reconstruction Error} To start, during the reconstruction experiments, we observed that spherical models produce better quality reconstructions than normal-type VAEs in lower latent dimension sizes, but not in higher dimensions. This is in line with the work of \citeauthor{davidson2018hyperspherical}, as the authors there also describe that their model is limited in higher dimension sizes. In lower dimensions, the uniform prior allows for a natural form of encoding and clustering, and the model is able to use the entire compact space of the hyperspherical surface. This improves performance over normal-type models that use a Gaussian prior, as is reflected in the reconstruction losses. 
However, as dimensionality increases, the surface area starts to vanish, which limits the possible space for the model to encode points on, as was described in subsection \ref{ex:hyperparams}. Although this numerical instability initially limited the model to latent dimensions sizes of less than 20, we found that restricting the concentration parameter $\kappa$ had a positive effect on stability, allowing the spherical VAEs to be trained up to and including a latent size of 256. Nevertheless, it should be noted that, as $\kappa$ is fixed for all dimensions, the expressivity of the spherical VAE naturally decreases the higher the dimensionality becomes. Therefore, combining our modification of $\kappa$ with the already restricted nature of the vMF distribution in high dimensions could potentially decrease model expressivity by a great amount compared to normal-type models. These two limitations of spherical VAEs could explain how normal-type VAEs outperform the spherical models for higher dimension sizes in the reconstruction loss results.

\paragraph{Classifiability} A similar trend was observed during the classification experiments. Here, spherical VAEs outperformed normal types in the lower dimensions 3-32, but not in the higher range of 64-256. On average, the highest scores on the latent representation classification task were not far behind those of the CNN directly classifying the patches. The \nvae model, for instance, reached 0.454 for a latent dimension of 64, while the CNN model reached 0.450 in these same settings. This is perhaps surprising, as state-of-the-art deep learning for the classification of Barrett's esophagus has often reached 0.8 to 0.9 accuracy on test datasets \cite{barretcnn1, cnn2}. One reason as to why the current CNNs do not reach this level of performance may be the size of the patches, or more specifically, the amount of information present in them. Unlike VAEs, CNN classifiers generally use a much larger patch size, such as 512$\times$512 with  a magnification of 10$\times$, which provides both more detail and more information. In comparison, the patch size used in the current experiments, 64$\times$64 with  a magnification of 5$\times$, provides four times less the amount of context. This can be an essential factor in determining the classifiability of the patches, as well as their encoded latent representations. Although not all the required information needed to reach a high accuracy score may be present in the patches, the classification experiments do show that there is a minimal loss of information between the patches and their lower-dimensional representations, suggesting that the models are able to learn an encoding that is meaningful within these settings. 

\paragraph{Generative Quality}
Furthermore, comparing the spherical VAEs to the normal VAEs for the qualitative experiments, we see a clear improvement across the entire tested dimensionality range. When using \svae and \nvae to randomly generate new images, the normal VAE produces noisy images from a dimension size of 32 upwards, while the spherical model produces realistic output up until the largest dimension size. 
The lackluster quality of the normal-type VAE in higher dimensions might possibly be attributed to the fixing of the variance parameter to an isotropic scalar value. This was done to be able to compare the architecture to the spherical model, and to fairly assess the effects of roto-equivariance on the model. However, it may also lead to decreased model expressivity in higher-dimensional settings. Further experiments would be needed to confirm this hypothesis.
For \svae, we observed that as dimensionality increases, general shapes in the generated images start to become more fine-grained. In the highest dimension ranges, \svae generates images that contain much more detail than lower-dimensional images, but the generations also contain relatively more blurriness, less clear structures and less color variation than the reconstructions for those dimension sizes. Conversely, images generated by \svae latent dimensions 16 and 32, do resemble their corresponding reconstructions to a greater extent. The reduction of generation quality in higher dimensions can therefore again be explained due to the decreased model expressivity, and the decreasing surface area to encode the images on in that dimensionality range. 
The decreased performance in high dimensions of spherical models does not become apparent during the interpolation experiments. There, across all dimensionalities, \svae shows a slight improvement in interpolation quality over \nvae, that is mainly noticeable in the better retainment of shape and structure.

\subsection{The Effects of Rotation-Disentanglement}

\paragraph{Rotation-Disentanglement Improves Allover Performance}
Another interesting finding is that the addition of roto-equivariance to the spherical setting generally increased performance across all experiments. Especially in medium to high range of dimension sizes (from 16 upwards), equivariant models improved reconstruction loss, classification accuracy and quality of generated images and interpolations over their not roto-equivariant versions. A similar trend was also observed for the \nvae, but the effect of equivariance did not become as apparent as in the hyperspherical case. It is possible that the positive effects of spherical and roto-equivariant techniques amplify each other. The roto-equivariance allows the model to learn a rotation-disentangled representation, and because of the absence of unnecessary information in this representation, it becomes easier to encode on the latent space. The improved structuring of this space in the hyperspherical setting could then improve the quality of the representations even further, leading to an improved benefit of the roto-equivariance in spherical settings compared to normal settings. 
% \newpage
\paragraph{Spherical VAEs learn a more meaningful representation in lower dimensions}
Considering the abovementioned improved performance across all experiments for spherical VAEs, and their potential to be even further improved by the addition of roto-equivariance, the results suggest that spherical models learn representations that are more meaningful than those of the vanilla VAE, but that they may be better used in low-dimensional settings. 
However, this brings us to the bottleneck central to this work. Spherical models are limited to a lower dimension range, but the histopathological image patches that these model are applied to, are too complex to be accurately encoded and recovered using a lower latent dimension. This became apparent from for instance Figure \ref{fig:reconstructions}. It was observed that even the highest latent dimension sizes still do not recover the original image completely accurately, suggesting that the latent dimension should at least be 512 or higher to obtain a good reconstruction. Additionally, it is likely that classifications of both representations as well as direct patch classification will improve by using patches that cover a larger biopsy area. However, the additional context and detail could make it even harder for the models to learn good encodings in low-dimensional settings, therefore also requiring a high latent dimension size. With the current setting for \svae, such a larger patch size would be very difficult to process. 

\subsection{Spherical Autoencoders}
We therefore introduced spherical autoencoders as a possible alternative to \svae, and included non-variational versions of each model in the experiments. As mentioned in \ref{ex:hyperparams}, spherical autoencoders did not show the same numerical instability as variational spherical autoencoders and were therefore applicable to higher dimensionalities. Moreover, autoencoders are known to produce sharper reconstructions than variational models, suggesting their potential for improving quantitative results. 

\paragraph{Autoencoders Obtain Lower Reconstruction Errors}
For the reconstruction experiments, it was shown that the autoencoder version of every model, both equivariant and non-equivariant, achieved a lower reconstruction error on the test set in higher dimensions. This was especially apparent for the spherical models. The observations are therefore in line with earlier work stating that variational autoencoders generally produce less sharp reconstructions than their non-variational counterparts. Moreover, the classification experiments showed no notable reduction in the classifiability of the learned latent representations. Although autoencoders showed an improvement in the quantitative experiments, they did not compare to the spherical variational models during the qualitative analysis of generated images. Here, it became apparent that autoencoders are often not considered as a generative model, as the models could only produce non-realistic noisy images in the majority of the tested dimensionality range. Interpolations generated by the spherical autoencoder did however show a vast improvement of quality over the vanilla autoencoder model, suggesting that the spherical structuring of the latent space provided a benefit to the learned representations in the non-variational case as well.  

\paragraph{Spread Loss Improves the Quality of \sae's Generations}
Therefore, the spread loss function was introduced and applied to the spherical autoencoder, and examples of generated images were provided for this modified architecture. In these experiments, it was shown that the addition of spread loss substantially impacted the quality of the generated images in a positive way. Where spherical autoencoders could only generate noise before, the sampled images now resembled those generated by \svae. We suspect that in the original \sae, images were often encoded close together, in a relatively small area on the hyperspherical surface, and that the amount of unutilized space only grew in larger dimensions. This would explain why randomly sampled images appeared as noise: the greater the dimension of the latent hypersphere, the more likely it would become that random images were sampled from an empty, uninformed location on the surface. 

Furthermore, this would also be in line with the interpolation experiments, in which we showed that non-modified \sae models, contrary to generative random sampling, produced good-quality interpolations over all dimensions. The encoded points all being clustered in a specific area of the hypersphere would mean they are close together and allow for smooth interpolations. Finally, a similar phenomenon was observed in the visualizations of the hyperspherical latent space in 3D, as we could see that the encoded points covered mainly one side of the sphere; an effect that would only increase the higher the latent size becomes according to this hypothesis. 
Encouraging the encodings to be spread uniformly over the sphere through the spread loss, which in the variational setting is accomplished by the prior, would therefore decrease the amount of empty surface area. This in turn is then hypothesized to lead to the improved random sampling observed in Subsection \ref{results:spreadloss}, but also the more unstable, less smooth interpolations that sometimes occurred. 

\paragraph{Quantitative Results with Spread Loss are Comparable to Regular \sae}
Quantitative experiments were also conducted on models with spread loss, and these showed no substantial negative effects of the modified loss function on reconstruction loss or classification accuracy. Even more so, the spread loss model demonstrated an improved accuracy in dimensions 256 and 512, which may perhaps be explained by the spread making it easier for a classifier to differentiate the otherwise closely clustered points. 
Although this work only provides an initial step, the experiments with spread loss show a potential for the proposed \sae to be used as a generative model, which would make it a promising alternative to \svae in higher dimensions.

\vspace{-0.1cm}
\section{Limitations and Future Work} \label{future}
\vspace{-0.1cm}
As mentioned in the discussion of the results, one of the greatest limitations of this work was the fact that the models, especially \svae, had trouble performing in higher dimensions, while the patches required higher latent dimensions to be accurately encoded and decoded. Future work could therefore focus on either one of these limiting factors, or a combination of both. 
\vspace{-0.1cm}
\subsection{Experimenting with Patches}
\paragraph{Patch Size}
In regards to improving the ability of the models to process the images, we could for instance experiment with different sizes and magnification levels of patches. \citeauthor{lafarge2020orientation} for example, use a high magnification level, showing only one cell per image patch, and are able to successfully use \nvae and the roto-equivariant \nvae to create high-quality reconstructions using a latent dimension size of 64. While our goal of modeling cell progression on a higher, tissue-based level would require a patch to contain more context than just one cell, the results of \citeauthor{lafarge2020orientation} do show that a higher magnification level, or rather a higher resolution image, might be easier for VAEs to reconstruct. 
This was also confirmed in earlier work by for example \cite{thambawita2021impact}.
We could therefore start experimenting with using a magnification level of 10$\times$ and a patch size of 128$\times$128. This would correspond to the same amount of biopsy area per patch as in the original experiments, but with a higher resolution. Although initial experiments showed that patches of this size were more difficult for the model to computationally process, experimenting with hyperparameters such as batch size and learning rate might alleviate this issue. Experimenting with such patches could show if the higher resolution does indeed have a positive effect on reconstruction quality, and if so, if a lower latent dimension size would be sufficient. 
Following \citeauthor{thambawita2021impact}, classification of patches might also be improved by using a higher image resolution, as a size of 512$\times$512 with a 10$\times$ magnification seems to be the standard \cite{cnn2, thambawita2021impact}. Further experiments could clarify if the models can be extended to process even larger patch sizes.

\paragraph{Patch Labels}
Besides patch size, classification might also be improved by considering alternative labeling systems. In this work, labels were assigned based on the most dominant pixel value present in a patch, and this was the only technique that was considered. Alternative labeling methods exist that do not consider the dominant pixel, but the value of the pixel in the middle of a patch \cite{li2022comprehensive}. It would be interesting to determine the effects of another labeling method on the assigned patch classes. For example, currently over 60\% of the patches were labeled as NDBE, which lead us to discard part of these patches from the training dataset. Another labeling technique could produce a different, more balanced distribution of classes, and although the results showed no obvious signs of a shortage of training data, this would result in an additional number of patches that could be included in the dataset. Furthermore, as a side note, it is also possible that the Vienna criteria class system in general is not a good fit for the data. As VAEs learn in an unsupervised manner, there is no guarantee that they cluster the encodings on the latent space in the four classes used in this work. Classification into such categories might in that case be difficult. The Vienna criteria have been used in clinical practice and have been used to train successful deep-learning classifiers, so it is probable that the class system is a satisfactory fit for the problem of modeling BE progression. However, should research on latent representation learning for BE advance, then it would be desirable that the utility of the class system is tested, for example by training a model such as K-means clustering on the latent space and seeing what the optimal number of clusters is. 

\subsection{Improving Models}
\paragraph{Improving \svae in Higher Dimensions}
On the other hand, future research could also focus on improving \svae in higher dimensions. An extension of the original paper by \citeauthor{davidson2018hyperspherical} was introduced that aimed to increase the expressivity of the model in higher dimensions by introducing additional concentration parameters \cite{https://doi.org/10.48550/arxiv.1910.02912}. This reduced the limiting effects of $\kappa$ being fixed for all dimensionalities. Such a method could also be implemented for the current use case, however, the problem of numerical instability would still persist. We took a first step towards showing the potential of \sae as a generative model and a more stable alternative to \svae in higher dimensions. Although results showed promise, we believe many experiments would need to be conducted before \sae can be called a true competitor to \svae. First and foremost, \sae should be applied to a simpler problem so that a proof of concept can be made. A first step would therefore be to apply the model to for example the MNIST dataset. If results remain promising on this dataset, extensions to more complicated datasets can be made. In a histopathological setting, it would for example be interesting to test both \sae and \svae on a simpler progression modeling task that requires a lower dimension to accurately reconstruct and is easier to cluster and classify, such as healthy tissue to dysplasia or to cancer. Not only could we better test the potential of spherical models for modeling morphological tissue changes in general, but we would also better be able to study the possible limitations and benefits of spherical autoencoders. 

\paragraph{Extending RHVAE to Larger Datasets}

Finally, experiments with the Riemannian model RHVAE were halted by the model being unable to scale to larger training datasets, and we instead focused our efforts on the spherical models for the further part of this work. The concept of Riemannian latent space structuring itself could still be an interesting alternative to \svae however. For example, we showed that in initial experiments with default settings, the model was already able to create latent space in which data points belonging to squamous tissue and high-graded dysplasia were clustered. Moreover, we also observed that the quality of reconstructions and generated samples was closer to that of the original input images, showing more intricate tissue patterns than spherical and normal-type models. These preliminary results indicate that the Riemannian latent space modeling could be a good choice for histopathological data and should be investigated in future work. The main hurdle to overcome in that case, is to make it feasible for the Riemannian VAE to process a large dataset becomes. One possible approach to accomplish this is by exploring alternative methods of learning the Riemannian metric, such that is not parameterized by all data points. Some ideas regarding efficient metric learning were for example proposed by \citet{louis2019riemannian} and \citet{gruffaz2021learning}. Alternatively, the learning of the metric might at all be unnecessary if we can directly compute it in a computationally efficient way, such as attempted by \citet{jeong2021efficient} or \citet{fan2022efficient}.


\section{Conclusion}\label{conclusion}
% \textcolor{red}{To do.}

In this work, we studied how the latent space of a VAE could be geometrically structured such that it models the progression of healthy tissue to Barrettâ€™s esophagus. A number of different techniques were researched, most notably RHVAE, which structures the latent space as a Riemannian manifold with a metric learned from the data, and \svae, which structures the manifold as a hypersphere on which the beneficial uniform prior can be imposed. Additionally, an extension of \svae to a rotation-equivariant group-based architecture was studied. 
We then asked which of the different manifold topologies, regular Euclidean, Riemannian, or hyperspherical, was the best choice to model disease progression by evaluating the models on four different tasks: reconstruction of input, classifiability of latent representations, quality of randomly generated images, and quality of interpolations. We found that RHVAE unfortunately could not be tested on the current dataset, as the model was limited to only small-scale datasets. 

Between the vanilla \nvae and hyperspherical \svae, the results showed that spherical models had improved performance over vanilla VAEs, especially in lower-dimensional settings. Here, they obtained better reconstruction errors than normal-type VAEs and their representations were shown to be easier for a simple linear model to classify. Moreover, they consistently generated high-quality image samples, even in higher dimensions where other models could only generate noise. Furthermore, their interpolations were more true to real-life changes in cell tissue. 
Additionally, the roto-equivariant version of the spherical models improved their performance even more across all evaluated tasks, which further established their potential for learning meaningful representations. 

However, we also showed that the spherical VAEs were unable to perform in higher dimensions, showing limited expressivity and severe numerical instability issues. This posed a problem for histopathological images, as they contain many fine-grained details and are hard to reconstruct in low dimensions accurately.
We therefore investigated non-variational spherical autoencoders and showed that they achieved lower reconstruction errors than variational models, and were less numerically unstable, allowing them to reach the higher dimensions the histopathological images require. We furthermore proposed to solve their inability to generate realistic random images by introducing spread loss and showed that this custom loss function could be used to significantly improve the quality of generations by spherical autoencoders, while having minimal to no impact on reconstruction quality or classifiability. 

Even with these improved results, a certain degree of blurriness remained in the reconstructions, and generated samples of all model types were still not on par with the quality of the original patches. Although we demonstrated several benefits of geometric-inspired VAEs over regular VAEs, the results suggest that the latent space learned by these models does not yet have the desired data-supporting structure to be able to model the progression of BE, and that some steps toward this goal remain to be made. On a short-term basis, experiments with different preprocessing of patches could make it easier for models to represent their features in the latent space. In the longer term, it would be desirable to look into alternative geometric-inspired models that do not suffer from the same issues as \svae in higher dimensions. Although more experiments would be needed to determine its full potential, the initial results described in this work suggest that our \sae model could potentially form one such promising alternative to \svae, both in this histopathological setting and in other applications. 