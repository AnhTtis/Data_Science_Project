\chapter{Methodology}\label{section:method}

This section explains the theoretical details of the models used in the experiments. These are the regular vanilla variational autoencoder (VAE), the Riemannian Hamiltonian variational autoencoder (RH-VAE), the spherical variational autoencoder (\svae) and the roto-equivariant Variational Auto-Encoder (KS-VAE).

\section{Variational Autoencoder} \label{subsec:vae}
At the basis of all models used in this work lies the autoencoder model. 
The general framework of an autoencoder consists of two neural networks: an encoder that encodes an input image $x$ into a lower-dimensional latent representation $z$, and a decoder that decodes the latent representation into a reconstruction $\hat{x}$, with the aim of minimizing the error between the original image and its reconstruction. The Variational Autoencoder (VAE) \citep{maxkingma2013auto} is a generative version of the original autoencoder, that instead of learning the latent representation $z$ directly, learns a distribution describing each data point, from which the latent representation is sampled (see Figure \ref{autoencoders}). The aim of the VAE is therefore to learn a parameterized probability distribution $p_{\theta }$ describing the input data $x$'s true distribution $P(x)$. To do so, we assume that the input data can be characterized by a lower-dimensional latent distribution $z$. The marginal likelihood can then be written as \begin{align} p_\theta(x) = \int p_\theta(x|z)q_{prior}(z)dz\end{align} where $q_{prior}(z)dz$ is a prior distribution over the latent variables, that in case of the vanilla VAE is chosen as a standard normal Gaussian distribution. 
Unfortunately, computing $p_\theta(x)$ involves the posterior $p_\theta(z|x)$, which is computationally expensive and often intractable.
We therefore introduce an approximation $q_\phi(z|x)$ of the true posterior, which is computed by a neural network: the encoder. We can then train a variational autoencoder, consisting of the encoder, which computes the approximate posterior and the decoder, which computes the conditional likelihood $p_\theta(x|z)$.

Within the variational autoencoder framework, the encoder and decoder are optimized in a joint setting. To find the posterior distribution $q_\phi(z|x)$ that best approximates the true posterior $p_\theta(z|x)$, we can use the \textit{Kullback-Leibler} divergence, which measures the difference between two probability distributions. Ideally, we would want to minimize this term, which is given by

\begin{align}
    \text{KL}(q_{\boldsymbol{\mathbf{\phi}}}(\boldsymbol{\mathbf{z}}\mid \boldsymbol{\mathbf{x}})\,\,||\,\,p_{\boldsymbol{\mathbf{\theta}}}(\boldsymbol{\mathbf{z}}\mid \boldsymbol{\mathbf{x}}))
    &= \mathbb{E}_{q_\phi} \big[ \log q_\phi(\mathbf{z}) \big] - \mathbb{E}_{q_\phi} \big[ \log p_\theta(\mathbf{z} | \mathbf{x}) \big]\\
    &= \mathbb{E}_{q_\phi} \big[ \log q_\phi(\mathbf{z}) \big] - \mathbb{E}_{q_\phi} \bigg[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z}) }{p_\theta(\mathbf{x})} \bigg]\\
    &= \mathbb{E}_{q_\phi} \big[ \log q_\phi(\mathbf{z}) \big] - \mathbb{E}_{q_\phi} \big[ \log p_\theta(\mathbf{x}, \mathbf{z}) - \log p_\theta(\mathbf{x}) \big]\\
    &= \mathbb{E}_{q_\phi} \big[ \log q_\phi(\mathbf{z}) - \log p_\theta(\mathbf{x}, \mathbf{z}) \big] + \mathbb{E}_{q_\phi} \big[ \log p_\theta(\mathbf{x}) \big]\\
    &= \mathbb{E}_{q_\phi} \big[ \log q_\phi(\mathbf{z}) - \log p_\theta(\mathbf{x}, \mathbf{z}) \big] + \underbrace{\log p_\theta(\mathbf{x})}_{\text{intractable}}.
\end{align}

% \begin{figure}[h]
%     \centering
%     \subfloat[\centering Autoencoder]{{\includegraphics[width=0.43\textwidth]{images/method/ae.png} }}%
%     \qquad
%     \subfloat[\centering Variational Autoencoder]{{\includegraphics[width=0.5\textwidth]{images/method/vae} }}%
%     \caption{Schematic view of autoencoder and variational autoencoder architectures. The encoder either learns to map the input vector $\mathbf{x}$ to a latent vector $\mathbf{z}$ directly (AE), or learns the parameters of a distribution describing $\mathbf{x}$, from which $\mathbf{z}$ is then sampled (VAE). The decoder in both cases learns to most accurately reconstruct the original input ($\mathbf{\hat{x}}$) from $\mathbf{z}$.}
%     \label{autoencoders}
% \end{figure}
\begin{figure}[h]
    \centering
    \subfloat[\centering Autoencoder]{{\includegraphics[width=0.36\textwidth]{images/method/ae_colored.png} }}%
    \quad \quad \quad
    \subfloat[\centering Variational Autoencoder]{{\includegraphics[width=0.5\textwidth]{images/method/vae_colored1.png} }}%
    \caption{Schematic view of autoencoder and variational autoencoder architectures. The encoder either learns to map the input vector $\mathbf{x}$ to a latent vector $\mathbf{z}$ directly (AE), or learns the parameters of a distribution describing $\mathbf{x}$, from which $\mathbf{z}$ is then sampled (VAE). The decoder in both cases learns to most accurately reconstruct the original input ($\mathbf{\hat{x}}$) from $\mathbf{z}$.}
    \label{autoencoders}
\end{figure}

However, as can be seen when we rewrite the equation, we still have the intractable evidence term $\log p_\theta(\mathbf{x})$. We therefore introduce a lower bound of the log-likelihood using Jensen’s inequality. 
\begin{align}
    \log p_\theta (x) &=\log \int_{\mathbf{z}} p_\theta(\mathbf{x}, \mathbf{z}) \\
    &=\log \int_{\mathbf{z}} p_\theta(\mathbf{x}, \mathbf{z}) \frac{q_\phi(\mathbf{z})}{q_\phi(\mathbf{z})} \\
    &=\log \left(\mathbb{E}_{q_\phi}\left[\frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z})}\right]\right) \\
    & \geq \underbrace{\mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{x}, \mathbf{z})]-\mathbb{E}_{q_\phi}[\log q_\phi(\mathbf{z})]}_\text{ELBO} 
\end{align}
This lower bound is called the Evidence Lower BOund (ELBO) \citep{maxkingma2013auto}. Because the evidence is a constant, maximizing the ELBO amounts to minimizing the KL divergence. The ELBO therefore forms the key to variational inference: instead of finding our optimal distribution q by minimizing the KL divergence, requiring us to calculate the intractable evidence term, we find it by maximizing ELBO, which is a tractable operation. We can therefore use the ELBO as our model's loss function. 
To arrive at our final loss function, we rearrange the ELBO term into the following expression

\begin{align}
    \text{ELBO} &= \mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{x}, \mathbf{z})]-\mathbb{E}_{q_\phi}[\log q_\phi(\mathbf{z})] \\
    &= \mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{x}, \mathbf{z}) - \log q_\phi(\mathbf{z})] \\
    &= \mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{x}| \mathbf{z}) + \log p_\theta(\mathbf{z})
    - \log q_\phi(\mathbf{z})] \\
    &= -\mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{x}| \mathbf{z}) - \mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{z}) - \log q_\phi(\mathbf{z})] \\
    &= -\mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{x}| \mathbf{z})] - \text{KL} \left(q_{\boldsymbol{\mathbf{\phi}}}\left(\boldsymbol{\mathbf{z}}\mid \boldsymbol{\mathbf{x}})\,\,||\,\,p_{\boldsymbol{\mathbf{\theta}}}(\boldsymbol{\mathbf{z}}\right)\right),
\end{align}

which consists of a regularization term $\mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{x}| \mathbf{z})]$ and a KL divergence 
term $\text{KL} \left(q_{\boldsymbol{\mathbf{\phi}}}\left(\boldsymbol{\mathbf{z}}\mid \boldsymbol{\mathbf{x}})\,\,||\,\,p_{\boldsymbol{\mathbf{\theta}}}(\boldsymbol{\mathbf{z}}\right)\right)$. The expectation term is also called the \textit{reconstruction loss}. It pushes the model to most accurately reconstruct an image from its encoded latent representation, such that the difference between decoding a sampled latent vector from the learned distribution is as small as possible. Meanwhile, the KL divergence term is also called the \textit{regularization loss}, as it pushes the approximate posterior to more closely resemble the prior distribution. When choosing a normal Gaussian distribution for both the prior and approximate posterior, as is the standard for VAEs, the prior enforces the posterior probability mass to have spread like a Gaussian, therefore adding a form of regularization to the model. Moreover, for a Gaussian prior and posterior, the KL term reduces to a closed-form formula, making computations more efficient. 



Now, all that remains is to solve the problem of the random sampling operation from $\mathbf{z}$ not being differentiable. \citeauthor{maxkingma2013auto} propose to solve this by using the \textit{reparameterization trick}, which suggests that instead of sampling $\mathbf{z}$ directly, some noise $\epsilon$ is sampled from a unit Gaussian distribution. We can then add the learned mean parameter $\mu$ to this noise term and multiply it by the variance $\sigma$ to arrive at a mean and variance as would have been directly sampled from the latent distribution, while still allowing for backpropagation through the neural network. 



% https://mpatacchiola.github.io/blog/2021/01/25/intro-variational-inference.html
% --------------------------------------------
% --------RIEMANNIAN RHVAE--------------------
% --------------------------------------------

\section{Riemannian Variational Autoencoder}\label{subsec:rhvae}
As discussed in Section \ref{bg:dst}, the vanilla variational autoencoder suffers from a distortion in the latent space  as a consequence of the Euclidean manifold assumption and Gaussian prior, which makes geometric notions such as distance unreliable in this space. One way of remedying this problem is to use metrics defined in the non-distorted input space instead, and mapping them to the manifold space. Such a mapping is possible by endowing the manifold with a Riemannian metric. A model that not only does this, but also \textit{learns} a fitting metric from the input data, is the Riemannian Hamiltonian VAE. These qualities make it a promising technique for accurately representing relationships between points and modelling BE progression.

The following section describes the details of RHVAE. Before this can be discussed however, it is important to give the reader a short overview of the basics of Riemannian geometry.

    \subsection{Basics of Riemannian Geometry}
        As discussed earlier, a real, smooth manifold is a space that is locally similar to a linear space. Riemannian geometry allows for defining notions of angles, distances, and volume on such spaces by endowing the manifold with a \textit{Riemannian Metric}. The manifold can then be considered as a \textit{Riemannian manifold}.
        We define an $m$-dimensional Riemannian manifold embedded in an ambient Euclidean space $\mathcal{X} = \mathbf{R}^d$ and endowed with a \textit{Riemannian metric} $\mathbf{G} \triangleq (\mathbf{G}_{\mathbf{x}})_{\mathbf{x} \in \mathcal{M}}$ to be a smooth curved space $(\mathcal{M},G)$. 
        For every point on the manifold $\mathcal{M}$, there exists a tangent vector $\mathbf{v}\in \mathcal{X}$ that is tangent to $\mathcal{M}$ at $\mathbf{x}$ iff there exists a smooth curve $\gamma:[0,1] \mapsto \mathcal{M}$ such that $\gamma(0)=\mathbf{x}$ and $\dot{\gamma}(0)=\mathbf{v}$.
        The velocities of all such curves through $\mathbf{x}$ form the \emph{tangent space} $\mathcal{T}_{\mathbf{x}}\mathcal{M}=\{ \dot{\gamma} (0) \,|\, \gamma:\mathbf{R}\mapsto\mathcal{M} \text{ is smooth around $0$ and } \gamma(0)=\mathbf{x}\}$, which has the same dimensionality as the manifold. The tangent space can be viewed as the collection of all the different ways in which the points on the manifold can be passed. 
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.4\textwidth]{images/method/Manifold_Example.png}
            \caption{Schematic example of a 2-D manifold $\mathcal{M}$ and its tangent space $\mathcal{M}_x \mathcal{T}$ at point $x$. The geodesic $\gamma(t)$ starts at $x$ and goes in the direction $\mathbf{v}$.}
            \label{fig:my_label}
        \end{figure}
        
        The Riemannian metric $G(\cdot)$ then equips each point $\mathbf{x}$ on the manifold with an inner product in the tangent space $\mathcal{T}_{\mathbf{x}}\mathcal{M}$, \textit{e}.\textit{g}. $\langle \mathbf{u}, \mathbf{v} \rangle_x = \mathbf{u} ^T \mathbf{G}_{\mathbf{x}} \mathbf{v}$. 
        This induces a norm $\|\mathbf{u}\|_\mathbf{x}\,,\forall \mathbf{u} \in \mathcal{T}_{\mathbf{x}}\mathcal{M}$ locally defining the geometry of the manifold. Given these local notions, we can not only compute local angles, lengths, and areas, but also derive global quantities by integrating over local properties. We can thus compute the length of any curve on the manifold $\gamma : [0,1] \rightarrow \mathcal{M}$, with $\gamma(0) = \mathbf{x}$ and $\gamma(1) = \mathbf{y}$ as the integral of its speed: $\ell(\gamma) = \int_{0}^1 \|\dot{\gamma}(t)\|_{\gamma(t)}dt$.
        The notion of length leads to a natural notion of distance by taking the infimum over all lengths of such curves, giving the \emph{gobal Riemannian distance} on $\mathcal{M}$, $d(\mathbf{x},\mathbf{y})=\inf_{\gamma}\ell(\gamma)$. The constant speed-length that minimizes the distance of a curve between two points is called a \emph{geodesic} on $\mathcal{M}$. VAEs can generate images along such a geodesic path, providing a more geometry-aware alternative to the vanilla VAE's linear interpolations.

           
    \subsection{Riemannian Hamiltonian Variational Autoencoder}
    \citeauthor{chadebec2020geometryaware} propose the Riemannian Hamiltonian VAE (RHVAE), which assumes the latent space to be structured as a Riemannian manifold $\mathcal{M}=\left(\mathbb{R}^{d}, \mathbf{G}\right)$ with $\mathbf{G}$ being the Riemannian metric, as described above. RHVAE attempts to exploit this assumed Riemannian structuring by introducing two main extensions of the vanilla VAE. First, to replace the regular Gaussian posterior distribution with a geometrically-informed posterior through the use of Riemannian Hamiltonian dynamics. Secondly, to find an appropriate Riemannian metric for this space, by learning it with a neural network. 
    
    \subsubsection{Learning the Riemannian Metric}
    As mentioned in section \ref{bg:rie}, while the choice of Riemannian metric is crucial to defining the manifold space, the computation of many proposed metrics involves the Jacobian, which is difficult and expensive to compute. In the RHVAE framework, the metric is therefore proposed to be learned directly from the data. This parameterized metric is defined as follows

    \begin{align}\label{eq:riemanmetric}
            \mathbf{G}^{-1}(z)=\sum_{i=1}^{N} L_{\psi_{i}} L_{\psi_{i}}^{\top} \exp \left(-\frac{\left\|z-c_{i}\right\|_{2}^{2}}{T^{2}}\right)+\lambda I_{d},    
    \end{align}

    
    
    where $N$ is the number of observed data points, $L_{\psi_{i}}$ are parameterized lower triangular matrices with positive diagonal coefficients learned from the data through neural networks, $c_{i}$ are centroids corresponding to the mean of the encoded distributions for every data point, 
    $T$ is a temperature parameter that scales the metric close to the centroids and $\lambda$ is a regularization factor, which allows for scaling the Riemannian volume element further away from the data. The above-defined metric is smooth, differentiable, and allows for computing geodesics easily, which is useful for creating informed interpolations along the geodesic curve on the manifold. 
    % The shape of this metric is very powerful since we have access to a closed-form expression of the inverse metric tensor which is usually useful to compute shortest paths (through the exponential map). Moreover, this metric is very smooth, differentiable everywhere and allows scaling the Riemannian volume element $\sqrt{\operatorname{det} \mathbf{G}(z)}$ far from the data very easily through the regularization factor $\lambda$.

    Training of the metric learning model is done jointly with training the rest of the RHVAE network. Just as with a regular VAE, an input image is encoded by the encoder network, which learns the parameters of a normal Gaussian distribution $\mathcal{N}(\mu, \sigma^2)$. Simultaneously, the metric network learns to map the input image to the lower triangular matrix $L_{\psi_{i}}$, allowing us to compute the Riemannian metric. These serve as input for a sampler, called the RHMC (Riemmanian Hamiltonian Monte-Carlo) sampler, from which a latent vector $z$ defined on the manifold  $z \in \mathcal{M}$ is sampled. The RHMC sampler thus essentially enriches the Gaussian approximate posterior function to be more aware of the underlying geometry of the manifold. 

    \subsubsection{A Geometrically-Aware Posterior through the RHMC Sampler}
    % For RHVAE, we assume that the latent space is the Riemannian manifold $\mathcal{M}=\left(\mathbb{R}^{d}, \mathbf{G}\right)$ with $\mathbf{G}$ being the Riemannian metric. 
    
    % Building upon the Hamiltonian VAE (HVAE) [52], we propose to exploit the assumed Riemannian structure of the latent space by using Riemannian Hamiltonian dynamics [74] instead. The main goal remains the same and consists in using the Riemannian Hamiltonian Monte Carlo (RHMC) sampler to be able to enrich the variational posterior $q_{\phi}(z \mid x)$ such that it targets the true (unknown) posterior $p_{\theta}(z \mid x)$ while exploiting the properties of Riemannian manifolds.
    Given the Riemannian manifold $\mathcal{M}=\left(\mathbb{R}^{d}, \mathbf{G}\right)$ with our metric $\mathbf{G}$, we want to sample our latent $z$ from a distribution that is informed about the geometry of the Riemannian latent space. We therefore want to obtain this target distribution $p_{\text {target }}(z)$ through the Riemannian Hamiltonian dynamics of the RHMC sampler. 
    The core of this sampling process revolves around the concept of seeing the VAE as an energy-based model, where $z$ is seen as the position of a traveling particle in $\mathcal{M}$. We also sample a random variable $v$, which represents the velocity of this particle. Following the view of the $z$ as a particle on a manifold, we aim to essentially simulate the evolution of the traveling particle towards the target density $p_{\text {target }}(z)$ using a Markov Chain.
    We first define the potential energy $U(z)$ and kinetic energy $K(z, v)$ as   

    \begin{align}
    U(z) & =-\log p_{\text {target }}(z) \\
    K(v, z) & =\frac{1}{2}\left[\log \left((2 \pi)^{d}|\mathbf{G}(z)|\right)+v^{\top} \mathbf{G}^{-1}(z) v\right] ,
    \intertext{which together give the Hamiltonian}
    H(z,v) &= U(z) + K(v,z) .
    \end{align}
    
    This Hamiltonian equation is integrated in every step of the Markov chain, which allows us to preserve  the target density and make sure that the chain eventually converges to the stationary target distribution. 
    This essentially creates a flow that is informed both by the target distribution and by the latent space geometry thanks to the Riemannian metric $\mathbf{G}$. The approximate posterior distribution is guided by this flow, leading to better variational posterior estimates.


% -------------------------------------------------------------------------------------------------------
% --------------HYPERSPHERICAL---------------------------------------------------------------------------
% -------------------------------------------------------------------------------------------------------


\section{Hyperspherical Variational Autoencoder}\label{subsec:svae}
Another approach to solving the distortion of the latent space is to structure it as a hyperspherical manifold and assume a uniform prior. 
One of the discussed issues with vanilla variational autoencoders is that the Gaussian prior tends to concentrate points in a cluster around the center of the distribution's probability mass. In the case of multi-class data, this can become problematic, as separate clusters in the latent space will also be pulled towards the origin and therefore become difficult to separate. In an ideal case, we would still have a prior that regularizes the approximate posterior, but that does not enforce the encoded points to be at the center of the probability mass. The probability distribution that does exactly this is the uniform prior. Instead of concentrating points in one location, it spreads them over the latent space. However, the vanilla VAE's Gaussian posterior means that our latent space corresponds to a Euclidean hyperplane, a space on which the uniform prior is not well-defined. 

\subsubsection{Replacing the Gaussian by the von Mises-Fisher Distribution}
By assuming a \textit{hyperspherical} posterior, however, our latent manifold becomes a compact space on which it is possible to define a uniform prior. This is why \svae uses a von Mises-Fisher (vMF) distribution instead of the Gaussian posterior of the vanilla VAE. The vMF distribution is often considered analogous to the Gaussian distribution on a hypersphere of dimensionality $m$. Similarly to the Gaussian, it is parameterized by a mean direction $\mu \in \mathbb{R}^{m}$, but instead of variance, the vMF is parameterized by a concentration parameter around the mean $\kappa \in \mathbb{R}_{\geq 0}$. The parameters $\mu$ and $\kappa$ are called the mean direction and concentration parameter, respectively. The greater the value of $\kappa$, the higher the concentration of the distribution around the mean direction $\mu$. The distribution is unimodal for $\kappa > 0$ and is uniform on the sphere for $\kappa$ = 0. 
The probability density function of the vMF distribution for a random unit vector $\mathbf{z}$ is then defined as

\begin{align}
q(\mathbf{z} \mid \mu, \kappa) & =\frac{\kappa^{m / 2-1}}{(2 \pi)^{m / 2} \mathcal{I}_{m / 2-1}(\kappa)} \exp \left(\kappa \mu^{T} \mathbf{z}\right),
\end{align}

where the mean direction $\mu$ is a unit vector $(\|\mu\| = 1)$ and
% $\|\mu\|^{2}=1, \mathcal{C}_{m}(\kappa)$ is the normalizing constant, and 
$\mathcal{I}_{n}(\kappa)$ denotes the modified Bessel function of the first kind at order $n = (m/2-1)$.

For the special case of $\kappa=0$, the vMF represents a Uniform distribution on the $(m - 1)$-dimensional hypersphere $U\left(\mathcal{S}^{m-1}\right)$. This fact allows us to place the desired uniform prior over the hyperspherical latent space. To incorporate the newly chosen prior and posterior distribution, the KL divergence term to be optimized needs to be rewritten to

\begin{align}
    K L\left(\operatorname{vMF}(\mu, \kappa) \| U\left(\mathcal{S}^{m-1}\right)\right) &= \kappa \cdot \frac{\mathcal{I}_{m / 2}(k)}{\mathcal{I}_{m / 2-1}(k)}+\log \mathcal{C}_{m}(\kappa)-\log \left(\frac{2\left(\pi^{m / 2}\right)}{\Gamma(m / 2)}\right)^{-1},
\end{align}

Using the above as our regularization loss and Mean Squared Error (MSE) as reconstruction loss, we have defined a loss function of the hyperspherical VAE. 
% Notice that since the KL term does not depend on $\mu$, this is only optimized in the reconstruction term. The above expression cannot be handled by automatic differentiation packages because of the modified Bessel function in $\mathcal{C}_{m}(\kappa)$. Thus, to optimize this term we derive the gradient with respect to the 
% concentration parameter $\nabla_{\kappa} K L\left(\operatorname{vMF}(\mu, \kappa) \| U\left(S^{m-1}\right)\right)$ :
% $$
% \begin{align}
% & \frac{1}{2} k\left(\frac{\mathcal{I}_{m / 2+1}(k)}{\mathcal{I}_{m / 2-1}(k)}+\right. \\
% &\left.\quad-\frac{\mathcal{I}_{m / 2}(k)\left(\mathcal{I}_{m / 2-2}(k)+\mathcal{I}_{m / 2}(k)\right)}{\mathcal{I}_{m / 2-1}(k)^{2}}+1\right)
% \end{align}
% $$
% where the modified Bessel functions can be computed without numerical instabilities using the exponentially scaled modified Bessel function.
\subsubsection{Sampling from the von Mises-Fisher Distribution}
Consequently, we need to define a way to sample from the posterior distribution. Sampling from a vMF is not as trivial as from a normal Gaussian distribution, but can be achieved with an algorithm involving an acceptance-rejection scheme, based on \cite{ulrich1984computer} and further defined by \cite{davidson2018hyperspherical}. The entire algorithm for sampling from the vMF distribution is shown in Algorithm \ref{vmf_algorithm}. It consists of sampling a random scalar $\omega$ from $g(\omega \mid \kappa, m) \propto \exp (\kappa \omega)\left(1-\omega^{2}\right)^{(m-3) / 2}, \quad \omega \in[-1,1]$ using an acceptance-rejection scheme. We then sample a random vector $\mathbf{v}$ from the uniform distribution on the sphere. 
Having sampled these independently, we can define a vector $\mathbf{z}' = \left(\omega ;\left(\sqrt{1-\omega^{2}}\right) \mathbf{v}^{\top}\right)^{\top}$. The next step is to construct a Householder reflection matrix $H$, defined as $H=\mathrm{I}-2\mathbf{hh}^T$, where $H=\mathrm{I}$ is the identity matrix and $\mathbf{h} = \frac{\mathbf{e}_{1} - \mu}{\| \mathbf{e}_{1}-\mu \|}$, with modal vector $\mathbf{e}_{1}=$ $(1,0, \cdots, 0)$. Applying this Householder transform to $\mathbf{z}'$ essentially reflects it across the hyperplane that lies between $\mu$ and $\mathbf{e}_1$, resulting in $\mathbf{z} = H\mathbf{z}'$, a direction vector sampled from the vMF distribution. 

\begin{algorithm}
\begin{algorithmic}[1]
\State \textbf{input}: dimension $m$, mean $\mu$, concentration $\kappa$ 
\State Acceptance-rejection sampling:  $\omega \sim g(\omega \mid \kappa, m) \propto \exp (\omega \kappa)\left(1-\omega^{2}\right)^{\frac{1}{2}(m-3)}$ 
\State Sample $\mathbf{v}$ from Uniform distribution: $\mathbf{v} \sim U\left(\mathcal{S}^{m-2}\right)$
\State Householder transform: $\mathbf{z}^{\prime} \leftarrow\left(\omega ;\left(\sqrt{1-\omega^{2}}\right) \mathbf{v}^{\top}\right)^{\top}$
$H \leftarrow$ Householder $\left(\mathbf{e}_{1}, \mu\right)$
\State \Return: $H \mathbf{z}^{\prime}$
\end{algorithmic}
\caption{vMF Sampling}
\label{vmf_algorithm}
\end{algorithm}

The gradient for this sampling procedure can be computed using the reparameterization trick for acceptance-rejection sampling schemes as proposed by \cite{naesseth2017reparameterization} and further defined for the vMF distribution by \cite{davidson2018hyperspherical}. 

% \textcolor{red}{Should I also explain the reparameterization trick for vMFs? It's quite extensive and math heavy. I don't think so, only need sampling procedure because I refer to downsides of numerical instability later.}

    
\section{The Hyperspherical Autoencoder}\label{subsec:sae}
    % Novel idea: increasing the kappa resultls in an autencoder that still follows a hyperpshereical latnet space structuring. We have therefore created a model that retains the benefits of the autoencoder (sharper images), while still also having a structured latent space! Spread loss then makes up for lack of uniform prior
    
    Besides the benefits of the hyperspherical VAE as previously described in the literature by the likes of \cite{davidson2018hyperspherical}, there is another, to our knowledge previously unexplored benefit to the hyperspherical set-up. Namely, we can disregard the variational framework and turn our model into a hyperspherical autoencoder, providing a number of possible benefits.   
    As mentioned,  for the vMF distribution, the greater the value of $\kappa$, the higher the concentration of the distribution around the mean direction $\mu$. 
    This implies that in the limit, as $\kappa \rightarrow \infty$, the probability density will tend to a point mass distribution. We can leverage this fact to use high values of $\kappa$ to effectively turn the variational autoencoder into a regular non-variational autoencoder. The reasons for wanting to do so are twofold. First of all, autoencoders do not require a sampling procedure, which not only makes training the model faster and less computationally expensive, but also circumvents a significant problem present in the sampling procedure as detailed in Algorithm \ref{vmf_algorithm}; namely that the acceptance-rejection scheme becomes highly numerically unstable in higher dimensions \cite{davidson2018hyperspherical}. 
    
    Moreover, autoencoders tend to reconstruct sharper images than VAEs \cite{kovenko2020comprehensive}. As the images used in this project contain a great amount of detail and may be difficult for any model to reconstruct, the increased sharpness of the autoencoder over its variational variant would be a beneficial property indeed. Regular vanilla autoencoders however, are not a generative model.
    The vanilla VAE regularizes the latent space to follow a Gaussian distribution, creating a dense latent space from which we can sample realistic variations of the original input images. However, autoencoders have no such restrictions on the latent vector. The lack of structuring leads to discontinuities in the latent space that don’t result in smooth transitions between encoded points. Decoding a randomly picked vector from the latent space will accordingly likely result in a nonsensical image. 

    \subsection{Hyperspherical Autoencoders as Generative Model}
    However, we can use the hyperspherical nature of the latent space to give the autoencoder generative abilities. In the novel proposed hyperspherical autoencoder (\sae) framework, we still have a von Mises-Fisher distribution, but instead of learning the parameters $\mu$ and $\kappa$, we fix $\kappa$ to a very high value, thereby effectively turning the probability mass into a concentrated peak. Taking the mean of such a “distribution” therefore becomes equal to learning a latent vector $\mathbf{z}$ directly, with the only difference being that $\mathbf{z}$ is still constrained to be on the hypersphere. Having obtained our $\mathbf{z}$ through this method, we avoid expensive and possibly unstable sampling operations and are able to directly decode the representation to a reconstructed image. 
    

    \subsubsection{Spread Loss}
    Having defined an autoencoder with a hyperspherical latent space, we further provide structure to the latent space by introducing \textbf{spread loss}, a custom loss function that encourages points to be spread evenly over the surface of the hypersphere. We hypothesize that for spherical autoencoders specifically, such a loss will enforce a form of regularization that in case of \svae is achieved through KL divergence with a uniform prior.
    In order to achieve such a uniform spread, we maximize the distance between every pair of encoded points on the hypersphere. This distance between two points on a sphere can be computed as

    \begin{align}
        d(z_1, z_2) =   \arccos \left( \frac{\langle z_1, z_2 \rangle}{\|z_1\|\cdot\|z_2\|}\right).
    \end{align}

    However, We can observe that the arccos in this equation function is actually monotonous. When $z_1=z_2$, the inner product $\langle z_1, z_2 \rangle = 1$ and $arccos = 0$. Conversely, if $z_1$ and $z_2$ are as far away from each other on the sphere as possible, their inner product $\langle z_1, z_2 \rangle = -1$ and $arccos = \pi$. Moreover, because all encodings are unitary, we can discard the denominator of the fraction in this equation. Maximizing the distance between two points would therefore be equal to minimizing their inner product. The loss function can then be defined as
    \begin{align}
        L_\text{spread} = \sum^N_{i,j=1} -\mathbf{z}_i^T \mathbf{z}_j,
    \end{align}
    or the sum over all inner products between $N$ vectors. 

    To test the validity of this approach, we perform initial experiments in which we visualize a 3-dimensional latent space for \sae without and with spread loss implemented, as shown in Figure \ref{fig:spread}.
    \begin{figure}[H]
      \centering
      \subfloat[\sae without spread loss.]{\includegraphics[width=0.38\textwidth]{images/experiments/latent_space/latent1.png}} \quad \quad \quad
      \subfloat[\sae with with spread loss.]{\includegraphics[width=0.4\textwidth]{images/experiments/latent_space/latent2.png} \label{fig:b}}
      \caption{Visualization of Latent Space for model \sae with $M=3$ without and with spread loss. The same batch of 200 images was encoded by both models, and different image classes are visualized with different colored points. Although not entirely evenly spaced, the points encoded by the model trained with spread loss cover a significantly larger area of the sphere.} \label{fig:spread}
    \end{figure}

    In this figure, we can see that implementing spread loss has a clear effect on the distribution of encoded images over the latent space. While in the regular \sae, the points are mainly concentrated on one side and the upper half of the sphere, with spread loss, those points are more evenly distributed over the sphere surface, leaving less significant gaps in the informedness of the latent space.     
    We therefore predict that with this loss, a structured and uniformly informed latent space is obtained, which will allow us to use the proposed \sae as a generative model, while still retaining the benefits of autoencoders, such as stability and sharpness of reconstructions. 


% -------------------------------------------------------------------------------------------------------
%  Equivariant Stuff
% -------------------------------------------------------------------------------------------------------

\section{Roto-Equivariant Variational Autoencoder}\label{subsec:ksvae}
    Besides comparing the different geometric latent spaces, we also experiment with learning representations that are orientation-disentangled. CNNs are, by design, equivariant to translation. This means that translating the input will also transform the learned representation accordingly. The same does however not hold for orientation information, causing identical patches in different orientations to result in different learned representation vectors. As biopsies can be scanned in any arbitrary orientation, this redundant information thus becomes entangled in the learned latent space, possibly making the representations harder for the model to process. It would therefore be beneficial to remove this rotation information and create roto-disentangled representations.  
    
    \citeauthor{lafarge2020orientation} propose that learning such representations relies on two factors: extending the encoder and decoder networks to be \textit{group-structured}, which makes the network equivariant to rotations, and then leveraging this structure to separate oriented and non-oriented features in the latent space, which results in disentangled representations. The following section will describe how this is accomplished, providing some necessary mathematical preliminaries of group theory, explaining the concepts of group-convolutional neural networks, and describing how these concepts can be used to achieve roto-disentangled representations. 

    \subsection{Group Convolutional Neural Networks}

    \subsubsection{Group Theory} A group $G$ is a set of elements performing a \textit{group operation}, that together satisfy properties of closure, associativity, presence of the identity element, and that each have an inverse \cite{herstein1991topics}. When such a group operates on a set, this is called the \textit{group action}. Formally, a group action of $G$ on a domain $X$ is defined as a mapping in which every group element is associated with some element in $X$, such that the mapping from $G$ to the permutation group of $X$ is a homomorphism. Such a domain is known as the \textit{G-space}. Each group element can be represented as a matrix that acts on, or transforms, an element of the G-space, a mapping
    also known as the \textit{group representation} $\rho$. Multiple types of group representations exist. For this work, most important are the trivial representation, which maps any vector to itself, and the regular representation, which maps all the axes in a representation space to another basis \cite{weiler2019general}. 

    \subsubsection{Regular Group-Convolutional Neural Networks}
    In theory, regular convolutions can be generalized to such a group framework by viewing the convolutional kernel as the G-space on which elements of the group of translations ($\mathrm{T}$) act. Considering a neural network's convolutional layers in this way, allows us to more easily see how group convolutions can extend the model to be equivariant to rotations as well. Instead of the group $\mathrm{T}$, we simply extend to the finite subgroup $SE(2, N)$ of the continuous translation-rotation group $SE(2)$, where $N$ is the cyclic permutation order. Hence, we can achieve roto-translational equivariance by extending the VAE model encoder and decoder networks from regular convolutional neural networks to Group-Convolutional Neural Networks (G-CNNs) \cite{cohen2016group}. 
    
    G-CNNs generally consist of three main elements that set them apart from a regular CNN: a lifting convolution, the group convolutions, and a projection operation. The lifting convolution discretizes the orientation axis of an image by transforming the image features for every rotation angle $\frac{2\pi}{N} n $, with $n \in \{0, \dots, N-1\}$. 
    The internal feature maps can be treated as \textit{SE}($2$) images $F \in \mathbb{L}_{2}[SE(2,N)]$. Next, these are convolved with image kernels in the group convolution layers of the network, preserving the channels and ensuring equivariance under the action of the $SE(2, N)$ group. Hence, in both group lifting and convolutional layers, information about orientation and translation is preserved. Our goal however, is to obtain an invariant representation, which requires a final projection layer. This layer performs a projection with an operation that is invariant to the group action, such as summing, maxing, or averaging, resulting in a representation from which orientation and translation-information is lost.
        
    \subsubsection{Steerable Group CNNs} 
    One possible downside of the above framework is that the values of the group convolutional feature maps are computed and stored on each element of the group. The computational complexity of the model thus scales with the order of the group that is used. \cite{cohen2016steerable} therefore propose a more general framework through \textit{steerable} G-CNNs. 
    Steerable G-CNNs apply do not learn a signal directly, as is the case for G-CNNs, but instead learn to describe it through functions decomposed by a \textit{Fourier transform}. In our case this is a transform of signals over the orthogonal $SO(2)$ group, which a subgroup of $SE(2)$ that concerns only continuous rotations and no translations. By applying the Fourier transform, steerable group convolutions are expanded to the co-domain,  instead of to an additional axis in the domain as is the case for regular group convolutions. The functions, or feature vectors, in the resulting \textit{feature fields} can be interpreted as Fourier coefficients. The transformation laws of these fields are determined by the group representation type that is associated with it. 
%     Every steerable feature field is associated with corresponding group representation type, that essentially specifies their transformation behavior under transformations of the inpu a transformation law determined by its type ρ. ρ is a group
% % representation (defined in Section 2.1) that specifies how the d channels are combined together (Weiler
% % and Cesa, 2019), e.g. a trivial, regular or quotient representation.
This not only allows for more efficient memory storage, but presents a more precise way of describing signals than in regular G-CNNs.
    


\subsection{Learning Roto-Disentangled Representations}


    Having obtained encoder and decoder networks that are roto-translational equivariant, we can partition the latent space to encode both isotropic and oriented image features, thereby creating disentangled representations. Following \cite{vadgama2022kendall}, we choose to work with the more efficient steerable G-CNN type network as explained above. These networks, like regular G-CNNs, contains three key layers: a lifting layer, which requires the trivial representation type as input in order to lift the input image's feature space, a series of regular representation type steerable convolution layers, and a projection layer.   

    We then have our steerable encoder model learn three different quantities: a latent mean descriptor $\mu$ that is equivariant under the actions of the group $SE(2,N)$, a pose or orientation corresponding to this representation $\mathbf{R}$, and an invariant scalar parameter that either corresponds to $\kappa$ in the case of the hyperspherical framework, or $\sigma$ in the case of the regular Gaussian posterior. Because the architecture is equivariant, rotating the network's input results in a transformation of both the mean descriptor $\mu$, as well as the estimated pose $\mathbf{R}$ via a representation of the group, whereas the predicted parameter $\kappa$ or $\sigma$ stays invariant. 
    The network's equivariance allows us to essentially undo the rotation of the mean descriptor and orient it to a canonical pose via the mapping $\mu_0 = \rho(\mathbf{R}^{-1}) \mu$, with $\rho(R)$ a group-representation\footnote{SO(2) group representations generalize the notation of rotation to vectors other than the usual 2D vectors} of SO(2).
    Thus, our method obtains an invariant descriptor $\mu_0$ that represents a whole equivalence class of images that are just rotated copies of one another. The main learning objective is thus to learn a probability distribution on this equivalence class, which is done as usual through variational inference. For the decoding process, we sample a vector $\hat{z}_0$ from this distribution, map it to its corresponding learned pose $\mathbf{R}$, and feed it through the equivariant decoder network. This results in a reconstructed image that has the same orientation as the original input image.
    




