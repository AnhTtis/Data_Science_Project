In this section, we prove a lower bound of $\Omega(\sqrt{KT})$ for the multi-task structure described in Section \ref{sec-multi-task}. To reiterate, we have that $K = q M$, where $M \geq 1$ is the number of sections each representing a bandit game, and $q \geq 2$ is the number of arms in each section. We will index the arms according to the section they belong to and their order therein. In other words, $\mathcal{A} = \{a_{i,j}: i\in[M], j\in[q]\}$. The structure of the policy space can be described as follows:
\begin{equation*}
    \Theta = \left\{ \theta \in \Delta^M_K: \forall i \in [M], \sum_{j=1}^q \theta(a_{i,j}) = \frac{1}{M} \right\},
\end{equation*}
where $\Delta^M_K$ is the set of uniform distributions (over $K$ arms) that are supported on only $M$ arms. We will overload the notation and denote by $a_{i,\theta}$ (which belongs to $\{a_{i,j}\}_{j=1}^q$) the arm that is played by policy $\theta$ in section $i$ (i.e. we have that $\theta(a_{i,\theta}) = \frac{1}{M}$).

\begin{theorem} \label{low:multi:thm}
Suppose that the policy and arm spaces conform to the multi-task structure. 
Then for any algorithm and $T \geq \frac{K}{4\log(4/3)}$, there exists a sequence of losses such that
\begin{equation*}
    R_T \geq \frac{1}{18} \sqrt{K T}.
\end{equation*}
\end{theorem}
\begin{proof} 
    We construct an environment $\mu_\theta$ for each policy $\theta$ such that for $a \in \mathcal{A}$, $\mu_{\theta}(a) = \frac{1}{2} - \Delta\mathbb{I}\{a \in U(\theta)\}$, where $U(\theta)$ is the support of $\theta$ and $0 < \Delta < \frac{1}{2}$ is to be tuned later. Moreover, we will also use the following variations of each environment. For $i \in [M]$, let $\mu_\theta^{-i}$ be an environment such that for $a \in \mathcal{A}$, 
    \[ \mu_\theta^{-i}(a) = 
    \begin{cases}
    \frac{1}{2} , & \text{if } a \in \{a_{i,j}\}_{j=1}^q\\
    \mu_{\theta}(a), & \text{otherwise.}
    \end{cases}\]

    For a policy $\theta$, we have that
    \begin{align*}
         R_T(\mu_\theta) &= \mathbb{E}_{\mu_\theta}  \sum_{t=1}^T \sum_{a\in \mathcal{A}} (\theta_t(a) - \theta(a)) \mu_\theta(a) \\ 
         &= \Delta \E\nolimits_{\mu_\theta}\sum_{t=1}^T \sum_{i=1}^M (\theta(a_{i,\theta}) - \theta_t(a_{i,\theta}))  \\
         &= \frac{\Delta}{M} \E\nolimits_{\mu_\theta}\sum_{t=1}^T \sum_{i=1}^M (1 - \mathbb{I}\{a_{i,\theta_t} = a_{i,\theta}\}) \\
         &= \frac{\Delta}{M} \sum_{i=1}^M  (T - N_{\mu_\theta}(i,\theta;T)\}),
    \end{align*}
    where for an environment $\mu$, a policy $\theta$, and a section $i \in [M]$, $N_{\mu}(i,\theta; T) \coloneqq \E_{\mu}\left[\sum_{t=1}^T  \mathbb{I}\{a_{i,\theta_t} = a_{i,\theta}\} \right]$. In words, this counts the expected number of times (under $\mu$) that the chosen policy agrees with $\theta$ in section $i$. Next, we use that, for any $i \in [M]$, $N_{\mu_\theta}(i,\theta;T) - N_{\mu^{-i}_\theta}(i,\theta;T) \leq T \tv(P_{\mu^{-i}_\theta},P_{\mu_\theta})$ together with Pinsker's inequality
    to get that
    \begin{equation} \label{low:multi:reg-kl}
        R_T(\mu_\theta) \geq \frac{\Delta}{M} \sum_{i=1}^M  \bigg(T - N_{\mu^{-i}_\theta}(i,\theta;T) - T\sqrt{\frac{1}{2}D(P_{\mu^{-i}_\theta},P_{\mu_\theta})}\bigg).
    \end{equation}
    As for the KL-divergence term, we apply Lemma \ref{low:kl-decomp}:
    \begin{align*}
         &D(P_{\mu^{-i}_\theta},P_{\mu_\theta})\\
         &\quad= \sum_{\theta' \in \Theta} N_{\mu^{-i}_\theta}(\theta';T) \sum_{a\in\mathcal{A}} \theta'(a) d(\mu^{-i}_\theta(a),\mu_\theta(a)) \\
         &\quad= \sum_{\theta' \in \Theta} N_{\mu^{-i}_\theta}(\theta';T)  \theta'(a_{i,\theta}) d(\mu^{-i}_\theta(a_{i,\theta}),\mu_\theta(a_{i,\theta})) \\
         &\quad= \frac{1}{M}\sum_{\theta' \in \Theta} N_{\mu^{-i}_\theta}(\theta';T)  \mathbb{I}\{a_{i,\theta'} = a_{i,\theta}\} d\left(\frac{1}{2},\frac{1}{2}-\Delta\right) \\
         &\quad\leq \frac{c \Delta^2}{M}\sum_{\theta' \in \Theta} N_{\mu^{-i}_\theta}(\theta';T)  \mathbb{I}\{a_{i,\theta'} = a_{i,\theta}\} \\
         &\quad= \frac{c \Delta^2}{M} \sum_{\theta' \in \Theta: a_{i,\theta'} = a_{i,\theta}} N_{\mu^{-i}_\theta}(\theta';T) \\
         &\quad= \frac{c \Delta^2}{M}   \E\nolimits_{\mu^{-i}_\theta}\sum_{t=1}^T \sum_{\theta' \in \Theta: a_{i,\theta'} = a_{i,\theta}} \mathbb{I}\{\theta_t = \theta'\} \\
         &\quad= \frac{c \Delta^2}{M}   \E\nolimits_{\mu^{-i}_\theta}\sum_{t=1}^T  \mathbb{I}\{a_{i,\theta_t} = a_{i,\theta}\}\\
         &\quad= \frac{c \Delta^2}{M} N_{\mu^{-i}_\theta}(i,\theta;T),  
    \end{align*}
    where the second equality holds since $a_{i,\theta}$ is the only arm that does not have the same mean loss in the two environments, and the inequality holds for $\Delta \leq \frac{1}{4}$ and $c = 8\log{\frac{4}{3}}$. Plugging back into \eqref{low:multi:reg-kl}, we get that
    \begin{align} 
        \nonumber &R_T(\mu_\theta) \\ \label{low:multi:reg-single}
        &\quad\geq \frac{\Delta}{M} \sum_{i=1}^M  \bigg(T - N_{\mu^{-i}_\theta}(i,\theta;T) - T\Delta\sqrt{\frac{c}{2 M} N_{\mu^{-i}_\theta}(i,\theta;T)}\bigg).
    \end{align}
    For what follows, we introduce an extra bit of notation. For $i \in [M]$ and $\theta \in \Theta$, define $F(i,\theta;T) = N_{\mu^{-i}_\theta}(i,\theta;T) + T\Delta\sqrt{\frac{c}{2 M} N_{\mu^{-i}_\theta}(i,\theta;T)}$. Moreover, let $\sim_i$ denote an equivalence relation on the policy set such that for $\theta$, $\theta' \in \Theta$,
    \begin{equation*}
        \theta \sim_i \theta' \iff \forall s \in [M]\backslash\{i\}, a_{s, \theta} = a_{s, \theta'}.
    \end{equation*}
    In words, two policies are equivalent according to $\sim_i$ if they agree everywhere outside section $i$. Denote the set of all equivalence classes of $\sim_i$ by $\Theta / \sim_i$, which contains $q^{M-1}$ classes, each containing $q$ policies corresponding to the possible arm choices in section $i$. Notice that if $\theta \sim_i \theta'$ then $\mu_{\theta}^{-i}$ is the same as $\mu_{\theta'}^{-i}$, and we will thus refer to either of the two environments using the equivalence class to which the two policies belong: $\mu_{[\theta]}^{-i}$.
    Now, with $i$ still referring to a fixed section, we have that
    \begin{align*}
        \sum_{\theta \in \Theta} N_{\mu^{-i}_\theta}(i,\theta;T) &= \sum_{W \in \Theta/\sim_i}
        \sum_{\theta \in W} N_{\mu^{-i}_\theta}(i,\theta;T) \\
        &= \sum_{W \in \Theta/\sim_i}
        \sum_{\theta \in W} N_{\mu^{-i}_W}(i,\theta;T) \\
        &= \sum_{W \in \Theta/\sim_i}
         \E\nolimits_{\mu^{-i}_W}\sum_{t=1}^T \underbrace{\sum_{\theta \in W} \mathbb{I}\{a_{i,\theta^t} = a_{i,\theta}\}}_{=1} \\
         &= q^{M-1} T = q^{M} \frac{T}{q}.
    \end{align*}
    On the other hand,
    \begin{align*}
        \sum_{\theta \in \Theta} \sqrt{  N_{\mu^{-i}_\theta}(i,\theta;T)} &\leq \sqrt{\sum_{\theta \in \Theta} 1^2} \sqrt{\sum_{\theta \in \Theta}  N_{\mu^{-i}_\theta}(i,\theta;T)}  \\
        &= \sqrt{q^M} \sqrt{q^{M} \frac{T}{q}} = q^{M} \sqrt{\frac{T}{q}}.
    \end{align*}
    Thus, we have that
    \begin{align*}
        \sum_{\theta \in \Theta} F(i,\theta;T) \leq q^{M} T\bigg( \frac{1}{q} + \Delta\sqrt{\frac{c T}{2 q M} } \bigg).
    \end{align*}
    Hence, 
    \begin{align*} 
        \sup_\mu R_T(\mu) 
        &\geq \frac{1}{|\Theta|} \sum_{\theta \in \Theta}  R_T(\mu_\theta) \\
        &\geq \frac{1}{|\Theta|} \sum_{\theta \in \Theta}  \frac{\Delta}{M} \sum_{i=1}^M  (T - F(i,\theta;T)) \\
        &\geq \frac{\Delta}{M} \sum_{i=1}^M   \bigg (T - \frac{1}{|\Theta|}  q^{M} T\bigg( \frac{1}{q} + \Delta\sqrt{\frac{c T}{2 q M} } \bigg)\bigg) \\
        &= \Delta T   \bigg (1 -   \frac{1}{q} - \Delta\sqrt{\frac{c T}{2 K} } \bigg) \\
        &\stackrel{q\geq2}{\geq} \Delta T\left(\frac{1}{2} - \Delta\sqrt{\frac{cT}{2K}} \right). 
    \end{align*}
    Plugging $\Delta = \frac{1}{4} \sqrt{\frac{2K}{cT}}$ into the previous display proves the theorem after observing that $\frac{1}{16}\sqrt{\frac{2}{c}} \geq \frac{1}{18}$. Lastly, notice that the condition imposed on $T$ ensures that indeed $\Delta \leq \frac{1}{4}$.
\end{proof}