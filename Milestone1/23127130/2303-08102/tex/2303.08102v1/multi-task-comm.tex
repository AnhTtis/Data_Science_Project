In the type of structure referred to in the theorem, the arms are divided into $M$ sections
(where $\frac{K}{M}=q$) and each policy is a uniform distributions supported over $M$ arms such that its support contains an arm from each section. When the policy set contains all such policies 
($(\frac{K}{M})^M$ in total),
this problem becomes equivalent to playing $M$ bandit problems simultaneously with the choice of policy at each round dictating an arm choice at each game. The distinction is that only the loss of one such arm is observed, while the player 
%suffers
aims to minimize
the average regret of the $M$ games. This type of structure
(albeit with a different type of feedback)
is commonly used to prove lower bounds for combinatorial bandits, see \cite{comb_audibert} for example. 
An adaptation of the proof of Theorem 5 in \cite{comb_audibert} to our case (see Appendix \ref{appendix-multi-task-proof}) leads
to a lower bound of $\Omega(\sqrt{K T})$, from which the theorem follows by using that $\mathcal{S}^*(\Theta) = \mathcal{S}(\Theta)=\frac{K}{M}$ and that $N=(\frac{K}{M})^M$. For what concerns the bound of Theorem~\ref{cvxhull-thm}, we have that
%$\tau^*$ is the uniform distribution (with full support) and 
$D^*(\Theta)=\log(\frac{K}{M})$. Thus, Theorems \ref{upper:alt-losses:thm} and \ref{cvxhull-thm} provide the same bound since $\mathcal{S}^*(\Theta) \log N = K D^*(\Theta)$. 