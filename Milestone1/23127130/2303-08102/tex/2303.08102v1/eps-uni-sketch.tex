%The proof is very similar to that of Theorem \ref{low:flower:thm}, so we only sketch the differences. We  consider $K$ environments\footnote{We recall that for this case $N=K$.} $\{\mu_{\theta}\}_{\theta \in \Theta}$ such that for $\mu_{\theta}$ and arm $j$, $\mu_{\theta}(j) = \frac{1}{2} - \Delta\mathbb{I}\{j=a_\theta\}$.
%Similar to before, we can show that\footnote{Environment $\mu_0$ is defined as in the proof of Theorem \ref{low:flower:thm}.} 
%    \begin{align*}
%        R_T(\mu_\theta)
%        &\geq \Delta \epsilon \bigg(T - N_{\mu_0}(\theta; T) - T \sqrt{\frac{1}{2}D(P_{\mu_0},P_{\mu_\theta})}\bigg).
%    \end{align*}
%An important difference w.r.t. the construction of Theorem~\ref{low:flower:thm} is that in the latter, all policies except $\theta$ are blind to the difference between $\mu_\theta$ and $\mu_0$, while this is not true here. Indeed, it can be seen, via Lemma \ref{low:kl-decomp}, that $D(P_{\mu_0},P_{\mu_\theta})$ is of order:
%\begin{align*}
%    & \Delta^2 \bigg(\sum_{\theta' \in \Theta \backslash \theta} N_{\mu_0}(\theta'; T) \theta'(a_\theta) +   N_{\mu_0}(\theta; T) \theta(a_\theta)\bigg) \\
%    &= \Delta^2 \bigg(T \frac{1-\epsilon}{K} +   N_{\mu_0}(\theta; T) \epsilon\bigg).
%\end{align*}
%When averaging the quantity in the brackets over all policies, as is done in the last passage in the proof of Theorem \ref{low:flower:thm}, we end up with just $\frac{T}{K}$, which does not depend on $\epsilon$. 
%This explains why the bound is of order $\epsilon\sqrt{KT}$ instead of the bigger $\sqrt{\epsilon KT}$ which would have matched the $\mathcal{S}^*$ bound of Theorem \ref{upper:alt-losses:thm}. This should not come as a surprise as we have demonstrated that Algorithm \ref{osmd} enjoys a better regret bound in this case. It is interesting to see if a matching lower bound could be proved.