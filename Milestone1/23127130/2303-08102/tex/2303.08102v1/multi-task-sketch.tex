In the type of structure referred to in the theorem, the arms are divided into $M$ sections
%\footnote{This construction can also be done with $M=1$, although this case is not interesting as it is equivalent to a standard bandits problem.}
(where $\frac{K}{M}=q$) and each policy is a uniform distributions supported over $M$ arms such that its support contains an arm from each section. When the policy set contains all possible $(\frac{K}{M})^M$ policies, this problem becomes equivalent to playing $M$ bandit problems simultaneously with the choice of policy at each round dictating an arm choice at each game. The distinction is that only the loss of one such arm is observed, while the player suffers the average regret of the $M$ games. This structure is commonly used to prove lower bounds for combinatorial bandits, see \cite{comb_audibert} for example. A straightforward adaptation of the proof of Theorem 5 in \cite{comb_audibert} to our case\footnote{In the setting of Theorem 5 in \cite{comb_audibert}, the scale of the incurred losses is $M$, while for us it is $1$, which is why their bound has an extra factor of $M$. The other main difference is the feedback model, which matters in the proof when bounding the KL-divergence between two environments, which, for us, can be easily bounded via Lemma \ref{low:kl-decomp} yielding a similar result.}
leads to a lower bound of $\Omega(\sqrt{K T})$, from which the theorem follows by using that $\mathcal{S}^*(\Theta) = \mathcal{S}(\Theta)=\frac{K}{M}$ and that $N=(\frac{K}{M})^M$. For what concerns the bound of Theorem~\ref{cvxhull-thm}, we have that
%$\tau^*$ is the uniform distribution (with full support) and 
$D^*(\Theta)=\log(\frac{K}{M})$. Thus, Theorems \ref{upper:alt-losses:thm} and \ref{cvxhull-thm} provide the same bound since $\mathcal{S}^*(\Theta) \log N = K D^*(\Theta)$. 