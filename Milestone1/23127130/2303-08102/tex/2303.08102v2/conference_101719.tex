\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{hyperref}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{thmtools}
%\usepackage{bm}
\usepackage{algorithm} 
\usepackage[noend]{algpseudocode}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{cor}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{exmp}{Example}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{macros2}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\hypersetup{
    colorlinks,
    linkcolor={black},
    citecolor={black},
    urlcolor={black}
}

\begin{document}

\title{
 Information-Theoretic Regret Bounds \\ for Bandits with Fixed Expert Advice
}

\author{\IEEEauthorblockN{Khaled Eldowa\IEEEauthorrefmark{1}, Nicolò Cesa-Bianchi\IEEEauthorrefmark{1}, Alberto Maria Metelli\IEEEauthorrefmark{2}, Marcello Restelli\IEEEauthorrefmark{2}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Università degli Studi di Milano, Milan, Italy}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Politecnico di Milano, Milan, Italy}
\IEEEauthorblockA{\{khaled.eldowa, nicolo.cesa-bianchi\}@unimi.it, \{albertomaria.metelli, marcello.restelli\}@polimi.it}
}






\maketitle

\begin{abstract}
We investigate the problem of bandits with expert advice when the experts are fixed and known distributions over the actions. Improving on previous analyses, we show that the regret in this setting is controlled by information-theoretic quantities that measure the similarity between experts. In some natural special cases, this allows us to obtain the first regret bound for EXP4 that
can get arbitrarily close to zero if the experts are similar enough.
While for a different algorithm, we provide another bound that describes the similarity between the experts in terms of the KL-divergence, and we show that this bound can be smaller than the one of EXP4 in some cases.  
Additionally, we provide lower bounds for certain classes of experts showing that the algorithms we analyzed are nearly optimal in some cases.
\end{abstract}

%\begin{IEEEkeywords}
%\end{IEEEkeywords}

\section{Introduction}
\input{introv2.tex}

\section{Problem Formulation}
We consider a non-stochastic multi-armed bandits problem with a finite action set $\mathcal{A} = [K]$ containing $K$ actions, and a (fixed) policy set $\Theta \in \Delta_{K-1}$ consisting of $N$ probability distributions over the actions. Here $\Delta_{K-1}$ denotes the probability simplex in $\mathbb{R}^K$ and, for a policy $\theta \in \Theta$ and $j \in [K]$, $\theta(j)$ is the probability with which policy $\theta$ picks action $j$. We additionally assume that each arm is in the support of at least one policy.
With a time horizon of $T$ rounds, an instance of the problem is characterized by an unknown sequence of loss vectors $(\ell_t)_{t=1}^T$, where $\ell_t(j) \in [0,1]$, for $j \in [K]$, denotes the loss assigned to action $j$ at round $t \in [T]$. A decision maker interacts with the environment as follows: at each round $t$, the decision maker selects a policy $\theta_t \in \Theta$; an action $A_t \in [K]$ is then sampled from $\theta_t$; the decision maker subsequently suffers the loss $\ell_t(A_t)$ and observes the pair $\big(A_t,\ell_t(A_t)\big)$. With a slight abuse of notation we denote by $\ell_t(\theta)$ the expected loss (at round $t$) given that policy $\theta$ was selected; that is, $\ell_t(\theta) = \sum_{j=1}^K \theta(j) \ell_t(j)$. The objective is to minimize the regret, which we define as follows:
\begin{equation*}
    R_T = \E \sum_{t=1}^T \ell_t(\theta_t) - \sum_{t=1}^T \ell_t(\theta^*),
\end{equation*}
where $\theta^* \in \argmin_{\theta \in \Theta} \sum_{t=1}^T \ell_t(\theta)$ and the expectation is over the internal randomization of the player.  

\section{EXP4 Regret Analysis}
It is possible to show\footnote{See also Theorem 18.3 in \cite{lattimore2020bandit}.} that the EXP4 algorithm with a suitable tuning of the learning rate satisfies a similar regret bound to the one proven for the algorithm developed in \cite{McMahanS09}. In our setting, this bound is $\sqrt{2T\mathcal{S}(\Theta)\log N}$, where
$
    \mathcal{S}(\Theta) = \sum_{j=1}^K \max_{\theta \in \Theta} \theta(j)
$
is a notion of similarity for the policy set. It is easy to see that $\mathcal{S}(\Theta) \leq \min\{K,N\}$, and thus, it can be interpreted as the effective ``number'' of policies.

\begin{algorithm} [t]
    \caption{EXP4 With Fixed Expert Advice}
    \label{exp4}
    \begin{algorithmic}[1]
        \State \textbf{Input:} $K$, $\Theta$, $\eta$
        \State \textbf{Initialize:} $\forall \theta \in \Theta$, $\hat{\ell}_0(\theta)=0$
        \For{$t=1,\dotsc,T$}
            \State Draw $\theta_t \sim P_t$, where $P_{t}(\theta) = \frac{\exp(-\eta\sum_{s=0}^{t-1}\hat{\ell}_s(\theta))}{\sum_{\xi \in \Theta}  \exp(-\eta\sum_{s=0}^{t-1}\hat{\ell}_s(\xi))}$
            \State Draw $A_t \sim \theta_t$, and observe loss $\ell_t (A_t)$
            \State $\forall \theta \in \Theta$, set $\hat{\ell}_t(\theta) = \frac{\theta(A_t)}{\sum_{\xi \in \Theta} P_{t} (\xi) \xi(A_t)} \ell_t(A_t)$
        \EndFor
    \end{algorithmic}
\end{algorithm}

\subsection{An Information-Theoretic Interpretation of \texorpdfstring{$\mathcal{S}(\Theta)$}{S(Theta)}} \label{sec:S-interp}
An alternative characterization of the policy set similarity can be derived by observing that $\mathcal{S}(\Theta) = 1 + \mathcal{TV}(\Theta)$, where for any ordering of the policies $(\theta_i)_{i=1}^N$ we define:
\begin{equation}\label{tv}
    \mathcal{TV}(\Theta) = \sum_{i=2}^N \sum_{j:\theta_i(j) > \theta_{[i-1]}(j)} (\theta_i(j)-\theta_{[i-1]}(j)),
\end{equation}
where $\theta_{[i-1]}(j) = \max_{\theta \in \{\theta_1,\cdots,\theta_{i-1}\}}\theta(j)$, and the second sum is the upper variation of the signed measure $\theta_i-\theta_{[i-1]}$.
$\mathcal{TV}(\Theta)$ can be seen as a generalization of the total variation distance to describe the overall divergence of the policy set. Indeed, it is easy to see that when $\Theta=\{\theta_1, \theta_2\}$,  $\mathcal{TV}(\Theta)$ reduces to the total variation between the two policies:
\[
\tv(\theta_1,\theta_2) = \sum_{j: \theta_2(j) > \theta_1(j)} (\theta_2(j)-\theta_1(j))~.
\]
Moreover, an upper bound on $\mathcal{TV}(\Theta)$ can be derived by noting that for any $\tau \in \Delta_{K-1}$ we have:
\begin{equation*}
    \mathcal{TV}(\Theta) = \mathcal{S}(\Theta) - 1 = \sum_{\theta \in \Theta} \sum_{j \in B(\theta)} (\theta(j) - \tau(j)),
\end{equation*}
where $(B(\theta))_{\theta \in \Theta}$ is any partition of $[K]$ such that for  $j \in B(\theta)$ we have $\theta \in \argmax_{\theta'\in \Theta} \theta'(j)$. Then, it follows that:
\begin{equation}\label{tv-bound}
         \mathcal{TV}(\Theta) \leq \min_{\tau \in \Delta_{K-1}} \sum_{\theta \in \Theta} \tv(\theta,\tau).
\end{equation}
It should be noted that this bound can be loose. For instance, if the policy set is partitioned into clusters of similar policies, then it is not hard to see that the right-hand side of \eqref{tv-bound} will be wasteful compared to \eqref{tv}. It is also noteworthy that quantities related to $\mathcal{S}(\Theta)$ are used when studying the minimax risk in statistical estimation problems. In particular, Theorem II.1 in \cite{minimax-risk} can be used to derive upper bounds similar to \eqref{tv-bound} in terms of any $f$-divergence, though often in implicit form.

\subsection{An Improved Bound}
Nevertheless, the stated regret bound scales with $\mathcal{S}(\Theta)$, not with $\mathcal{TV}(\Theta)$. The main limitation is that 
$\mathcal{S}(\Theta) \geq 1$, no matter how close $\mathcal{TV}(\Theta)$ is to zero. Thus, the bound $\sqrt{2T\mathcal{S}(\Theta)\log N}$ is never smaller than $\sqrt{2T\log N}$ regardless of the structure. One might wonder if this is necessary.
The following theorem provides the first regret bound for EXP4 that can get arbitrarily close to zero if the policies are similar enough. Our bound depends on the key quantity:
\[
        \mathcal{S}^*(\Theta) = \sum_{j=1}^K \left(\max_{\theta \in \Theta} \theta(j) - \min_{\theta' \in \Theta} \theta'(j)\right),
\]
that is easily seen to satisfy
$\mathcal{TV}(\Theta) \le \mathcal{S}^*(\Theta) \le \mathcal{S}(\Theta)$.
\begin{theorem} \label{upper:alt-losses:thm}
    Algorithm \ref{exp4} run with $\eta = \sqrt{ \frac{2\log N}{T \mathcal{S}^*(\Theta)}}$ satisfies
$
        R_T \leq \sqrt{2 T \mathcal{S}^*(\Theta) \log N}.
$
\end{theorem}
%
\input{exp4-proof.tex}
%
In general, $\mathcal{S}^*(\Theta)$ is not guaranteed to be strictly smaller $\mathcal{S}(\Theta)$; they can be equal in some cases regardless of how small $\mathcal{TV}(\Theta)$ is. In other cases, however, there can be an improvement. To see this, note that for any $\tau \in \Delta_{K-1}$,
\begin{equation*}
    \mathcal{S}^*(\Theta) = \sum_{\theta \in \Theta} \sum_{j \in B(\theta)} (\theta(j) - \tau(j)) + \sum_{j \in B'(\theta)} (\tau(j)-\theta(j)),
\end{equation*} 
where $(B'(\theta))_{\theta \in \Theta}$ is a partition of $[K]$ such that for every $j \in B'(\theta)$ we have $\theta \in \argmin_{\theta'\in \Theta} \theta'(j)$. Then, analogously to~\eqref{tv-bound}, we have that:
\begin{equation*} 
    \mathcal{S}^*(\Theta) \leq 2\min_{\tau \in \Delta_{K-1}} \sum_{\theta \in \Theta} \tv(\theta,\tau).
\end{equation*}
Like \eqref{tv-bound}, this bound can be loose, but it serves to indicate that if $\min_{\tau \in \Delta_{K-1}} \sum_{\theta \in \Theta} \tv(\theta,\tau)$ is small (it can get arbitrarily so), $\mathcal{S}^*(\Theta)$ is guaranteed to be of at most the same order.

\subsection{Examples}
In the following, we compare the quantities $\mathcal{S}(\Theta)$, $\mathcal{S}^*(\Theta)$, and $\mathcal{TV}(\Theta)$ for a selection of policy set structures. 
\begin{exmp}[Two Policies] \label{example-two}
    As we have seen before, for the two policies case, i.e., $\Theta = \{\theta_1,\theta_2\}$, $\mathcal{S}(\Theta) = 1 + D_{TV}(\theta_1,\theta_2)$. Whereas
$
    \mathcal{S}^*(\Theta) = \sum_{j=1}^K |\theta_1(j) - \theta_2(j)| = 2 D_{TV}(\theta_1,\theta_2).
$
\end{exmp}
The next two examples concern the case in which each policy is a uniform distribution over a support of $M\leq K$ arms. In this scenario, we get that\footnote{Recall that we assume that an arm is in the support of at least one policy.} $\mathcal{S}(\Theta) = \frac{K}{M}$, while $\mathcal{S}^*(\Theta)$ depends on the number of arms common to all policies.

\begin{exmp}[Radially Symmetric Uniform Policies] \label{example-flower}
    Consider a structure where the intersection of the supports of any pair of policies is the same.\footnote{This means that any arm is either in the support of all policies or exclusively in the support of a single one.} 
    Let $V \leq M$ be the number of arms common to all policies, we have that $\mathcal{S}(\Theta) = \frac{N(M-V)+V}{M}$, that is bounded from below by $1$, while $\mathcal{S}^*(\Theta)=N\frac{M-V}{M}$ and $\mathcal{TV}(\Theta)=(N-1)\frac{M-V}{M}$ are not.\footnote{Note that $\frac{M-V}{M}$ is the total variation distance between any two policies.}
    \end{exmp}

\begin{exmp}[Failure of $\mathcal{S}^*$] \label{example-stripes}    
    On the other hand, if $K=M+1$, and the policy set contains all possible $M$-supported uniform policies, then $\mathcal{S}(\Theta) = \frac{M+1}{M}$ which approaches $1$ as $M$ increases, thus $\mathcal{TV}(\Theta)=\frac{1}{M}$ approaches $0$. However, $\mathcal{S}^*(\Theta)$ is always equal to $\mathcal{S}(\Theta)$ since for each arm $j$, $\min_{\theta' \in \Theta} \theta'(j) = 0$. 
\end{exmp}

\begin{exmp}[$\epsilon$-Uniform Policies] \label{example-epsilon}
    If $N=K$ and each policy $\theta$ is associated (one-to-one) with an arm $a_\theta$ so that, for an arm $j$, $\theta(j) = \frac{1-\epsilon}{K} + \epsilon \mathbb{I}\{j=a_\theta\}$, where $0 \leq \epsilon \leq 1$.
    Then, $\mathcal{S}(\Theta) = \epsilon K + 1 - \epsilon$, while $\mathcal{S}^*(\Theta) = \epsilon K$ and $\mathcal{TV}(\Theta) = \epsilon (K-1)$.
\end{exmp}

\section{An Alternative Approach} \label{sec:osmd}
Since we can randomize our policy choice at each round, we can interpret our setting as a bandits problem in which the player has to randomize over the actions choosing a distribution from 
(and also competing with)
the convex hull $\mathrm{co}(\Theta)$ of the available policies. A simple approach, outlined in Algorithm~\ref{osmd}, is to adapt the Online Stochastic Mirror Descent (OSMD) interpretation of EXP3 \cite{adversarial} to our setting. The main distinction is that we need to project onto $\mathrm{co}(\Theta)$ at each step. Denote by $D(P,Q)$ the KL-divergence between distributions $P$ and $Q$, and for $\tau \in \Delta_{K-1}$, define $D(\Theta||\tau)=\max_{\theta\in\Theta} D(\theta, \tau)$. 
The following regret bound for Algorithm~\ref{cvxhull-thm} uses a notion of the ``width'' of $\Theta$ in terms of the KL-divergence defined by:\footnote{Note that the minimum value can only be attained in $\mathrm{co}(\Theta)$; see Theorem 11.6.1 in \cite{cover2006}.}
\[
    D^*(\Theta) =  \min_{\tau\in\Delta_{K-1}} D(\Theta||\tau)~.
\]
\begin{theorem} \label{cvxhull-thm}
Algorithm \ref{osmd} run with
\[
\tau^* \in \argmin_{\tau\in\Delta_{K-1}} D(\Theta||\tau) \quad\text{and}\quad \eta=\sqrt{ \frac{2D^*(\Theta)}{T K}}
\]
satisfies
$
       R_T \leq \sqrt{2TD^*(\Theta)K}
$.
\end{theorem}
\input{cvx-hull-proof.tex}
It can be shown that $D^*(\Theta)\leq \log N$. Moreover, if the policy set is symmetric, in the sense that the KL-divergence between any policy and the uniform mixture is the same, then $D^*(\Theta)$ (attained at the uniform mixture) coincides with the Jensen-Shannon divergence~\cite{jensen-shannon} (or the information radius) of the policy set.

\begin{algorithm} [thb]
    \caption{OSMD on the Convex Hull of Policies}
    \label{osmd}
    \begin{algorithmic}[1]
        \State \textbf{Input:} $K$, $\Theta$, $\eta$, $\tau \in \text{co}(\Theta): \tau(j)>0 \: \forall j \in [K]$
        \State \textbf{Initialize:} $x_1=\tau$
        \For{$t=1,\dotsc,T$}
            \State Pick distribution $P_t$ on $\Theta$ such that $\sum_{\theta \in \Theta} P_t(\theta) \theta = x_t$
            \State Draw $\theta_t \sim P_t$, then $A_t \sim \theta_t$, and observe loss $\ell_t (A_t)$
            \State $\forall j \in [K]$, set $\hat{\ell}_t(j) = \frac{\mathbb{I}\{j=A_t\}}{x_t(j)} \ell_t(A_t)$
            \State Update $x_{t+1} = \argmin_{x \in \text{co}(\Theta)} \eta \langle x, \hat{\ell}_t \rangle + D(x, x_t)$
        \EndFor
    \end{algorithmic}
\end{algorithm}
Algorithm~\ref{exp4} can also be seen as an instance of OSMD with the negative entropy regularizer. The main difference is that the decision space is the entire probability simplex over the policies. Hence, the regularization in Algorithm~\ref{exp4} favors exploring uniformly over the policies, whereas it favors exploring uniformly over actions in Algorithm~\ref{osmd}. Analysis-wise (compare in particular \eqref{upper:alt-losses:hedge} and \eqref{cvxhull-omd}), Algorithm~\ref{exp4} takes advantage of the similarity between policies to reduce the variance of their loss estimates (compared to $K$), whereas Algorithm~\ref{osmd} takes advantage of the (possibly) limited size of $\text{co}(\Theta)$ to reduce the bias term (compared to $\log N$).

Consider the $\epsilon$-uniform structure of Example \ref{example-epsilon}.
%$\tau^*$ is the uniform distribution, and thus, 
$D^*(\Theta)$ in this case is given by
\begin{equation*} 
    \frac{K-1}{K} (1-\epsilon) \log(1-\epsilon) + \frac{1+\epsilon(K-1)}{K} \log(1+\epsilon(K-1)).
\end{equation*}
If we now compare $K D^*(\Theta)$ and $\epsilon K \log K$, 
we see that both are equal when $\epsilon\in\{0,1\}$. However, the former is strictly convex for $\epsilon\in (0,1)$, while the latter is linear. Thus, in this case, the bound of Theorem \ref{cvxhull-thm} is better than that of Theorem \ref{upper:alt-losses:thm}.
However, if we consider the structure of Example \ref{example-flower}, we have that
%$\tau^*$ is the uniform mixture of the policies, and 
$D^*(\Theta) = \frac{M-V}{M} \log N$. Thus, the bound of Theorem \ref{cvxhull-thm} becomes $\sqrt{2T\frac{M-V}{M}K\log N}$ which is worse than the $\sqrt{2T\frac{M-V}{M}N\log N}$ bound of Theorem \ref{upper:alt-losses:thm} since $K \geq N$.

\section{Lower Bounds}
In this section, we prove minimax lower bounds for specific classes of policy sets and contrast them with the regret bounds discussed thus far. More precisely, with a fixed policy set $\Theta$, we prove lower bounds on $\inf_\pi \sup_{(\ell_t)_{t=1}^T} R_T$, 
where $\pi$ is the player's strategy. To this end, we will consider a class of stochastic environments, each identified by the vector $\mu \in [0,1]^K$ such that, for $j \in [K]$ and any $t$, $\ell_t(j)$ is drawn from a Bernoulli distribution with mean $\mu(j)$. For any $t \leq T$, let $H_t = (\theta_1, A_1, \ell_1(A_1), \dotsc, \theta_t, A_t, \ell_t(A_t))$ be the interaction history up to round $T$. Each environment $\mu$ (together with strategy $\pi$) induces a probability distribution $P_\mu$ on $H_T$. Define: 
\begin{equation*}
    R_T(\mu) = \max_{\theta^* \in \Theta} \mathbb{E}_\mu  \sum_{t=1}^T \sum_{j=1}^K (\theta_t(j) - \theta^*(j)) \mu(j),
\end{equation*}
where the subscript in $\mathbb{E}_\mu$ emphasizes the dependence on $P_\mu$. 
%In the following, we will use the fact that a lower bound on $\sup_\mu R_T(\mu)$ that is independent of $\pi$ is also a lower bound on $\inf_\pi \sup_{(\ell_t)_{t=1}^T} R_T$. 
In the following, we will prove lower bounds on $\sup_\mu R_T(\mu)$ that hold for any algorithm. 
We will rely on the following lemma, which is an immediate extension of a standard result (see Lemma 15.1 in \cite{lattimore2020bandit}) to our setting.
\begin{lemma} \label{low:kl-decomp}
Fix a strategy $\pi$, a policy set $\Theta$, and a horizon $T$; and let $\mu$ and $\mu'$ be two environments. Then, 
\begin{equation*}
    D(P_\mu, P_{\mu'}) = \sum\nolimits_{\theta \in \Theta} N_\mu(\theta; T) \sum\nolimits_j \theta(j) d(\mu(j),\mu'(j)),
\end{equation*}
where $N_\mu(\theta; T) = \E_\mu\sum_{t=1}^T\mathbb{I}\{\theta_t=\theta\}$, and $d(a,b)$ is the KL-divergence between two Bernoulli distributions with means $a$ and $b$.
\end{lemma} 


\subsection{Radially Symmetric Uniform Policies}
The first lower bound concerns the case discussed in Example \ref{example-flower}. The construction of the lower bound (which is an adaptation of the standard approach in \cite{adversarial}) leverages the fact that the policies are uniform and equidistant.
\begin{theorem} \label{low:flower:thm}
If $\Theta$ conforms to the structure of Example \ref{example-flower} such that $M > V$. Then for any algorithm and $T \geq \frac{N M}{4 \log(4/3)(M-V)}$, there exists a sequence of losses such that
$
    R_T \geq \frac{1}{18} \sqrt{N\frac{M-V}{M} T}.
$
\end{theorem}
Since $\mathcal{S}^*(\Theta)=N\frac{M-V}{M}$, it follows that the bound of Theorem \ref{upper:alt-losses:thm} is optimal in this case, up to a logarithmic factor. 
\input{flower-proof.tex}

\subsection{\texorpdfstring{$\epsilon$}{Epsilon}-Uniform Policies}
The next bound concerns the $\epsilon$-Uniform structure of Example \ref{example-epsilon}. While all policies are still equidistant, this case does not enjoy the peculiar discrete structure of Example~\ref{example-flower}.
\begin{theorem} \label{thm:eps-uniform}
    If $\Theta$ conforms to the $\epsilon$-uniform structure of Example \ref{example-epsilon}. Then for $T \geq \frac{K}{4\log(4/3)}$ and any algorithm, there exists a sequence of losses such that
    $
    R_T \geq \frac{1}{18}\epsilon\sqrt{KT}.
    $
\end{theorem}
The proof, see Appendix \ref{appendix-eps-uni-proof}, is similar to that of Theorem~\ref{low:flower:thm} apart from the fact that all policies contribute to the KL-divergence between any two environments (see Lemma~\ref{low:kl-decomp}) 
since all policies have full support.
Notice that the bound is of order $\sqrt{\epsilon^2KT}$ instead of $\sqrt{\epsilon KT} = \sqrt{\mathcal{S}^*(\Theta)T}$, which would have nearly matched the bound of Theorem~\ref{upper:alt-losses:thm}. This was expected since we have shown that Algorithm~\ref{osmd} enjoys a better regret bound in this case. It is interesting to see if a matching lower bound could be proved.  

\subsection{The Two Policies Case}
For the two policies case, we can prove a lower bound of order $\sqrt{H^2(\theta_1,\theta_2)T}$ as asserted by the following theorem, where $H^2(\theta_1,\theta_2)=\frac{1}{2}\sum_j (\sqrt{\theta_1(j)}-\sqrt{\theta_2(j)})^2$ is the squared Hellinger distance. Relative to the total variation, we have that in general:
$
    \frac{1}{2}D^2_{TV}(\theta_1,\theta_2) \leq H^2(\theta_1,\theta_2) \leq D_{TV}(\theta_1,\theta_2).
$ 
%Note that when the two policies conform to the structure of Example~\ref{example-flower}, $H^2(\theta_1,\theta_2)$ and $\tv(\theta_1,\theta_2)$ coincide.

\begin{theorem} \label{thm:H2}
    Assume that $\Theta=\{\theta_1,\theta_2\}$ and $H^2(\theta_1,\theta_2) > 0$. Then for any algorithm and $T \geq \frac{1}{8\log(4/3)H^2(\theta_1,\theta_2)}$, there exists a sequence of losses such that
    $
    R_T \geq \frac{1}{13}\sqrt{H^2(\theta_1,\theta_2)T}.$
\end{theorem}

\input{two-policies-sketch.tex}
%\input{two-policies-proof.tex}

The squared Hellinger distance can be related to other measures of divergence. For instance, it is of the same order as the Jensen-Shannon divergence and the triangular discrimination \cite{topsoe, H2-JSD}. Thus, the bound of Theorem \ref{thm:H2} can be stated, up to small constants, in terms of these measures as well. 

\subsection{A Matching Lower Bound for a Class of Policy Sets} \label{sec-multi-task}
Lastly, we provide a lower bound that almost matches both Theorems  \ref{upper:alt-losses:thm} and \ref{cvxhull-thm} for a certain class of policy sets that we discuss shortly. This bound is analogous to the $\sqrt{KT\log N/\log K}$ lower bound proved in \cite{expertslowerbound}. However, unlike \cite{expertslowerbound}, we rely on \textit{fixed} sets of \textit{stochastic} policies.
\begin{theorem}
\label{th:worst-case-lower}
For any integer $q\geq2$, there exists a problem structure where $K\geq2$, $\mathcal{S}^*(\Theta) = q$, and $\mathcal{S}^*(\Theta) \log N = K D^*(\Theta)$; such that any algorithm, for sufficiently large $T$, suffers $\Omega\left(\sqrt{\mathcal{S}^*(\Theta) T \frac{\log N}{\log \mathcal{S}^*(\Theta)}}\right)$ regret.
%Moreover, when $K\geq4$, the policy set in question can be chosen such that $\mathcal{S}^*(\Theta) < \min(K,N)$.
\end{theorem}
%\input{multi-task-sketch.tex}
\input{multi-task-comm.tex}

\section{Conclusion}
We analyzed two algorithms providing regret bounds that depend on information-theoretic quantities describing the policy set. We proved lower bounds for certain classes of policy sets highlighting instances where our regret bounds are nearly matched. Nevertheless, it remains to be seen if better guarantees can be proved in cases like Example \ref{example-stripes} where $\mathcal{S}^*(\Theta)\geq1$
even if $\mathcal{TV}(\Theta)$ can be smaller. It is also interesting to see what are the optimal rates for cases like Example~\ref{example-epsilon} where the bound of Theorem~\ref{upper:alt-losses:thm} is suboptimal, as we learned in Section~\ref{sec:osmd}, even if $\mathcal{S}^*(\Theta)$ is of the same order as $\mathcal{TV}(\Theta)$. Another direction is 
%to see if it is possible to adapt
to investigate the possibility of adapting
to the policy set structure if the distributions are not known beforehand.
  

%\section*{Acknowledgment}

\bibliographystyle{IEEEtran}
\bibliography{IEEE}

\input{appendix.tex}





\end{document}
