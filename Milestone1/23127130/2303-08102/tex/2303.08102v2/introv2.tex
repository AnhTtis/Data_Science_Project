Bandits with expert advice (see, e.g., \cite{lattimore2020bandit}) is a well-known variant of the non-stochastic bandits problem in which, at the beginning of each round, $N$ experts each make a recommendation to the learner in the form of a distribution over the $K$ available actions. The algorithm EXP4 \cite{adversarial} solves this problem with a regret against the best expert bounded by $\sqrt{2TK\log N}$, where $T$ is the horizon. %%
When $N \gg K$, this bound shows the ability of EXP4 to leverage the structure of the problem, as opposed to running a bandit algorithm over the $N$ experts achieving a bound of $\sqrt{TN}$. An almost matching lower bound of order $\sqrt{TK\log N/\log K}$ was proved in \cite{expertslowerbound} (for deterministic experts).  %%
In this work, we study a variant of bandits with expert advice in which the distributions recommended by the experts are \emph{fixed} and \emph{known}.
In the following, we will use the term \emph{policies} to denote these fixed experts.
Our goal is to determine the best possible dependence of the regret on the structure of the policy set $\Theta$ irrespective of the assigned sequence of losses. 

This problem is closely related to linear bandits \cite{linear-bandits} (with finite decision sets), where 
the structure of the decision/policy set can be provably leveraged. %%%
Our problem can also be viewed as a non-stochastic version of \textit{bandits with mediator feedback} \cite{metelli2021policy}, where the learner's access to actions is mediated by the fixed policy set.
When losses are stochastic rather than being adversarial,
regret bounds were proved in \cite{metelli2021policy} that scale with the largest pair-wise exponentiated 2-RÃ©nyi divergence\footnote{This divergence is related to the chi-squared divergence. Note that pairwise, these divergences can be infinite in non-trivial cases, see Example~\ref{example-flower}.} between the policies in the context of policy-based reinforcement learning. Comparable bounds were also proved in \cite{chi-squared} in the setting of contextual bandits.
In our setting, where losses are adversarially generated, the best known bound is $\sqrt{2T\mathcal{S}(\Theta)\log N}$ from \cite{McMahanS09}, where $\mathcal{S}(\Theta) \le \min\{K,N\}$ %%
is a notion describing the similarity between policies, see Section \ref{sec:S-interp} for its definition and a new information-theoretic interpretation.
Since $\mathcal{S}(\Theta) \ge 1$ for all $\Theta$, this bound cannot get arbitrarily small no matter how similar the policies are, and becomes vacuous when the policies are identical. %%%%%%%%

Our first contribution (Theorem~\ref{upper:alt-losses:thm}) is a new regret bound for EXP4 of the form $\sqrt{2T\mathcal{S}^*(\Theta)\log N}$, where $\mathcal{S}^*(\Theta)$ is a new index of similarity between policies that is never larger than $\mathcal{S}(\Theta)$ and reduces to twice the total variation distance when $N = |\Theta| = 2$. In particular, we show that $\mathcal{S}^*(\Theta)$ can indeed become arbitrarily small, depending on the policy set.
Note that such guarantees cannot be obtained only as a consequence of the reduced range of the losses caused by the similarity of the policies, see \cite{lattimore-lowerbounds}.
Additionally, we show in Theorem~\ref{cvxhull-thm} an algorithm whose regret is bounded by $\sqrt{2 T K D^*(\Theta)}$, where $D^*(\Theta)$ is a notion of the ``width'' of $\Theta$ in terms of the KL-divergence that reduces to the information radius when $\Theta$ is symmetric. This bound is never worse than the EXP4 bound $\sqrt{2TK\log N}$. Moreover, we construct sets $\Theta$ where $K D^*(\Theta) < \mathcal{S}^*(\Theta) \log N$. 
Finally, we prove lower bounds for a number of policy set structures and contrast them with the upper bounds we derived. 
We illustrate, in particular, some examples where the bounds are nearly matching.