% \usepackage{times}
% \usepackage{latexsym}
% \usepackage[T1]{fontenc}
% \usepackage[utf8]{inputenc}

% % This is not strictly necessary, and may be commented out,
% % but it will improve the layout of the manuscript,
% % and will typically save some space.
% \usepackage{microtype}
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage{multicol}
% \usepackage{subcaption}
% % \usepackage{subfigure}
% \usepackage[linesnumbered,ruled]{algorithm2e}
% \usepackage{mathtools,amssymb,mathrsfs}
% \usepackage{microtype}
% \usepackage{enumitem}
% \begin{document}

\appendix

\section{Pooling Matrix in the Note-pooling Embedding Layer}
\label{appendix:pool_mat}

We have note embedding $\mathbf{e}_{note} \in \mathbb{R}^{N \times d}$ ($d$ is the embedding dimension) and alignment matrix $\mathbf{M} \in \{0, 1\}^{L \times N}$. 
The non-overlapped mean-pooling can be calculated as follows.
%
\begin{align*}
\mathbf{W} &= \mathbf{M} / \text{sum}(\mathbf{M}, \text{dim}=-1, \text{keepdim}=\text{True}) \\
\mathbf{e}_{md} &= \mathbf{W} * \mathbf{e}_{note}
\end{align*}
%
where $/$ is element-wise division and $*$ is matrix multiplication. 
By leveraging \texttt{gather} and \texttt{scatter} operations, the non-overlapped mean-pooing can even be computed in batch.

\section{Analysis of Adaptive Grouping Loss}
\label{appendix:group_loss}

By the definition of the adaptive grouping loss, we only need to analyze the following term.
%
\begin{equation*}
    \left| K(j) - (1 - R(j)) - \Delta_j \right|
\end{equation*}

If $K(j) > \Delta_j$ in the forward pass, we have $K(j) - \Delta_j \geq 1$ because they are both positive integers. 
In order to encourage the loss to become smaller, $1 - R(j)=\sum_{k=1}^{K(j)-1}\alpha_j^k$ should become larger. 
In other words, the optimization will push $\sum_{k=1}^{K(j)-1}\alpha_j^k$ to be larger towards $K(j) - \Delta_j$. 
Note that the theoretical upper bound of $\sum_{k=1}^{K(j)-1}\alpha_j^k$ is $K(j)- 1$, which is larger or equal to $K(j)- \Delta_j$. 
Thus, this optimization is possible and it will meet the following condition during optimization.
%
\begin{equation*}
    \sum_{k=1}^{K(j)-1}\alpha_j^k \geq 1 - \epsilon .
\end{equation*}
%
By definition of $K(j)$, we have the following conclusion.
%
\begin{equation*}
    K(j)^{\text{new}} = \arg\min_{K}\left\{\sum_{k=1}^K \alpha_j^k \geq 1 - \epsilon\right\} \leq K(j) - 1
\end{equation*}
%

If $K(j) < \Delta_j$, a similar analysis can be derived. 
$\sum_{k=1}^{K(j)-1}\alpha_j^k\rightarrow0$ should be encouraged to purse a smaller loss. 
It implies if the $K(j)$-th halting probability doesn't satisfy the condition $\alpha_j^{K(j)} \geq 1-\epsilon$, the $K(j)^{\text{new}}$ will have an increasing trend.
However, if $\alpha_j^{K(j)} \geq 1 - \epsilon$, the optimization will be stuck. 
We may optimize $\left| K(j) - \left(1 - R(j) + \alpha_j^{K(j)}\right) - \Delta_j \right|$, \emph{i.e.}, $\left| K(j) - \sum_{k=1}^{K(j)} \alpha_j^{k} - \Delta_j \right|$. 
In practice, we found this is a rare case and the will completely disappear after several epochs. 
So we adopt the unified adaptive grouping loss.

If $K(j) = \Delta_j$, it means we can safely remove this term in the loss.


\section{Scheduler of Curriculum Learning}
\label{appendix:bt_cl}

The down sampling ratio of back translation data starts at 1.00 and decrease to 0.01 at the half of total training epochs.
The sampling ratio of annotation data starts at 20.00 for upsampling and decrease to 5.00 at the end of total training epochs.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/backtrans_curriculum.pdf}
    \caption{An illustration of how we use back translation data together with annotated data in co-translation training. ``bt'' represent data from back translation data augmentation and ``at'' represent data from annotation.}
    \label{fig:bt_curriculum}
\end{figure}

%\section{Limitations and Future Work}
%Future endeavors may lie in mining and utilizing song translation data from richer sources and more languages. 
%Besides, 
\section{Post-processing In Inference}
\label{appendix:infer}

In order to generate scores and singing voice in line with musical rules, we add some rule-based post-processing to the alignment predictions for more tolerance. 
For cases where total number of aligned notes is larger than the number of notes in the melody, we simply truncate the predicted number of aligned notes from the last token to the first or from the first token to the last. 
For cases of fewer number of predicted notes, we add the number of difference all to the last token.


\section{Data Annotation and Human Evaluation}
\label{appendix:data}

Annotators are students who major in music, vocal singing or relevant specialty. 
They all speak bilingual languages with Chinese and English, so they are also qualified for translation quality evaluation. 
For data annotation and human evaluation, each person gets reasonably paid according to the individual workload.
The annotation guidance and evaluation guidance can be found in supplement materials. 
Figure \ref{fig:eval_page} is an example of visual front-end interface for human evaluation. The pipeline for data annotation is shown in Figure \ref{fig:da_pipeline}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\textwidth]{appendix_figures/MOS_eval_page.pdf}
    \caption{An example of evaluation front-end interface for human evaluation.}
    \label{fig:eval_page}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\textwidth]{appendix_figures/da_pipeline.pdf}
    \caption{An overview of our propdata annosed otation pipeline.}
    \label{fig:da_pipeline}
\end{figure}

\section{Chinese-to-English Song Translation Evaluation}
\label{appendix:zh-en}

Here we show the MOS-S and MOS-Q for Chinese-to-English song translation for reference. 
Lack of open data set for English singing voice synthesis caused the bad quality of synthesized English singing voice in inference. 
So we have to use the Chinese SVS system to synthesize the English songs. 
According to the feedback from annotators, this gap influence their feeling about translation results to some extent. As is shown in Table \ref{tab:zh_en_subjective}, results significantly drop compared to those of En$\rightarrow$Zh.
So we leave this to appendix part for reference.

\begin{table}[htbp]
    \centering
    %\setlength{\tabcolsep}{4pt}
    \begin{tabular}{l|c|c}
    \hline
    & MOS-S & MOS-Q \\
    \hline
    & Zh$\rightarrow$En & Zh$\rightarrow$En\\
    \hline
    Human Ref. & 4.05 $\pm$ 0.08 & 4.06 $\pm$ 0.07\\
    \hline
    GagaST & 3.04 $\pm$ 0.02 & 3.08 $\pm$ 0.03\\
    \hline
    \modelname-cls  & 3.03 $\pm$ 0.02 & 3.10 $\pm$ 0.01\\
    ~~~ only bt & 3.11 $\pm$ 0.04& 3.15 $\pm$ 0.03\\
    ~~~ w/o bt & 3.27 $\pm$ 0.04 & 3.20 $\pm$ 0.02\\
    \hline
    \modelname  & 3.39 $\pm$ 0.03& 3.61 $\pm$ 0.05\\
    ~~~ only bt & 3.39 $\pm$ 0.05 & 3.60 $\pm$ 0.02\\
    ~~~ w/o bt  & 3.29 $\pm$ 0.05 & 3.32 $\pm$ 0.02\\
    % \midrule
    % \modelname~w/o bt  & & & & & & \\
    % \modelname~only bt & & & & & & \\
    % \modelname~+ bt  & & & & & & \\
    \hline
    \end{tabular}
    \caption{The  Mean  Opinion  Score singability~(MOS-S) and overall quality~(MOS-Q) for Zh$\rightarrow$En samples with 95\% confidence intervals.}
    %带*的结果仅供参考
    \label{tab:zh_en_subjective}
\end{table}

\begin{comment}
\section{Ethics Statement}
\subsection{For every submission}
\subsubsection{Did you discuss the \textit{limitations} of your work?}
Yes. Appendix D.

\subsubsection{Did you discuss any potential \textit{risks} of your work?}
Yes. Appendix E.

\subsubsection{Do the abstract and introduction summarize the paper’s main claims?}
Yes. Abstract.

\subsection{Did you use or create \textit{scientific artifacts}?}
Yes. 

If yes:
\subsubsection{Did you cite the creators of artifacts you used?}
Yes. Section 4.1.

\subsubsection{Did you discuss the \textit{license or terms} for use and/or distribution of any artifacts?}
Yes. Section 4.1 and Appendix E.

\subsubsection{Did you discuss if your use of existing artifact(s) was consistent with their \textit{intended use}, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?}
Yes. Appendix E.

\subsubsection{Did you discuss the steps taken to check whether the data that was collected/used contains any \textit{information that names or uniquely identifies individual people} or \textit{offensive content}, and the steps taken to protect / anonymize it?}
Yes. The participants are asked to use bogus names in the data collection process, which means we do not know who sang these recordings.

\subsubsection{Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?}
Yes. Section 4.1.

\subsubsection{Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created?}
Yes. Section 4.1.

\subsection{Did you run \textit{computational experiments}?} 
Yes. 

If yes:

\subsubsection{Did you report the \textit{number of parameters} in the models used, the \textit{total computational budget} (e.g., GPU hours), and \textit{computing infrastructure} used?}
Yes. Section 4.1.

\subsubsection{Did you discuss the experimental setup, including \textit{hyperparameter search} and \textit{best-found hyperparameter} values?}
Yes. Section 4.1.

\subsubsection{Did you report \textit{descriptive statistics} about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?}
Yes. Section 4.2.

\subsubsection{If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?}
Yes. Section 4.1.

\subsection{Did you use \textit{human annotators} (e.g., crowdworkers) or \textit{research with human subjects}?}  If you answer {\bf Yes}, provide the section number; if you answer {\bf No}, you can skip the rest of this section. \\[0.3cm]
Yes.

If yes:

\subsubsection{Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?}
Yes. Appendix C.

\subsubsection{Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such \textit{payment is adequate} given the participants’ demographic (e.g., country of residence)?}
No. Trade secret.

\subsubsection{Did you discuss whether and how \textit{consent} was obtained from people whose data you're using/curating (e.g., did your instructions explain how the data would be used)?}
Yes. Section 4.1 and our code project.

\subsubsection{Was the data collection protocol \textit{approved (or determined exempt)} by an ethics review board?}
N/A. 

\subsubsection{Did you report the basic demographic and geographic characteristics of the \textit{annotator} population that is the source of the data?}
No. They're completely anonymous, and we don't know.

\end{comment}


% \end{document}