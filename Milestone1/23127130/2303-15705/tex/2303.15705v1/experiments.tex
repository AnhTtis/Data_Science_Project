\section{Experiments}
In this section, we describe the experiment setup, results and analysis on Chinese$\leftrightarrow$English song translation.

\begin{table*}[t]
    \centering
    %\setlength{\tabcolsep}{4pt}
    \begin{tabular}{l|c|c|c|c|c|c}
    \hline
    \multirow{2}{*}{Models} & \multicolumn{2}{c|}{MOS-T} & \multicolumn{2}{c|}{MOS-S} & \multicolumn{2}{c}{MOS-Q} \\
    \cline{2-7}
    & En$\rightarrow$Zh & Zh$\rightarrow$En & En$\rightarrow$Zh & Zh$\rightarrow$En$^\dagger$ & En$\rightarrow$Zh & Zh$\rightarrow$En$^\dagger$ \\
    \hline\hline
    Human Ref. & 3.83 $\pm$ 0.06 & 4.11 $\pm$ 0.05 & 3.92 $\pm$ 0.07& \multirow{8}{*}{\diagbox[height=40pt, width=0.05\textwidth]{}{}} & 3.90 $\pm$ 0.06 &\multirow{8}{*}{\diagbox[height=40pt, width=0.05\textwidth]{}{}}\\
    \cline{1-4} \cline{6-6}
    GagaST & 3.66 $\pm$ 0.06 & 3.72 $\pm$ 0.05 & 3.49 $\pm$ 0.10 & & 3.65 $\pm$ 0.05 & \\
    \cline{1-4} \cline{6-6}
    \modelname-cls  & 3.66 $\pm$ 0.05& 3.79 $\pm$ 0.05 & 3.58 $\pm$ 0.07& & 3.62 $\pm$ 0.05& \\
    ~~~ only bt & 3.69 $\pm$ 0.05 & 3.80 $\pm$ 0.04 & 3.53 $\pm$ 0.09 & & 3.63 $\pm$ 0.05&\\
    ~~~ w/o bt & 3.64 $\pm$ 0.05 & 3.30 $\pm$ 0.05 & 2.16 $\pm$ 0.05 & & 3.14 $\pm$ 0.04 &\\
    \cline{1-4} \cline{6-6}
    \modelname  & 3.71 $\pm$ 0.05& 3.85 $\pm$ 0.05 & 3.68 $\pm$ 0.05&  & 3.69 $\pm$ 0.04&\\
    ~~~ only bt & 3.71 $\pm$ 0.05 & 3.80 $\pm$ 0.05 & 3.58 $\pm$ 0.07 & & 3.65 $\pm$ 0.04&\\
    ~~~ w/o bt  & 3.69 $\pm$ 0.05 & 3.28 $\pm$ 0.04 & 3.63 $\pm$ 0.07 & & 3.67 $\pm$ 0.04&\\
    % \midrule
    % \modelname~w/o bt  & & & & & & \\
    % \modelname~only bt & & & & & & \\
    % \modelname~+ bt  & & & & & & \\
    \hline
    \end{tabular}
    \caption{The Mean Opinion Score in translation intelligibility and naturalness~(MOS-T), singability~(MOS-S) and overall quality~(MOS-Q) with 95\% confidence intervals. The translation direction with $^\dagger$ means that audio samples of the translated song for evaluation are generated with the voice synthesis model that is not trained for that target language. So those results are presented in Appendix \ref{appendix:zh-en} and for reference only.}
    %带*的结果仅供参考
    \label{tab:subjective}
\end{table*}

\subsection{Experimental Settings}

\subsubsection*{Data Sets}

Since there is no publicly available data set with high quality parallel lyrics translation and lyrics-melody alignments, we collect and annotate a data set PopCV (Pop songs with Cover Version) containing both Chinese songs with their English cover version and English ones with Chinese cover version. 
Since there are no industry standards or published precedence in annotating such a data set, we design an annotation procedure which is time-saving and easy for annotators to carry out. 
First, we collect the score sheet files of songs from score websites\footnote{\scriptsize{\url{https://www.musescore.com} and \url{https://wwww.midishow.com}}}. 
Then the annotators add lyrics to notes according to how songs are sung in the original and its cover version as conventions\footnote{\scriptsize{\url{https://lilypond.org} and \url{https://musescore.org/howto}}} suggest. 
We then export the annotated music score files in \texttt{.musicxml} format and automatically extract lyrics and their aligned notes. 
Please refer to Appendix \ref{appendix:data} for details. 

% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=0.99\textwidth]{figures/data_annotation.pdf}
%     \caption{An illustration of data collection and annotation process.}
%     \label{fig:data_anno}
% \end{figure*}
For the data used in back-translation, we use LMD\footnote{\scriptsize{\url{https://github.com/yy1lab/Lyrics-Conditioned-Neural-Melody-Generation}}}~\citep{LMD} for English songs with alignments to melody and a data set crawled from Changba App for Chinese songs. 
We first pre-train two lyrics translation models with length control, one in each direction, and then translate the above two data sets.
The translated lyrics are one-on-one aligned to the notes. 
Two sets of back-translated data are used for training only while testing is done on real data with human annotations. 
An overview of the data is in Table \ref{tab:dataset_stat}. 
We will release the code and the human annotated data set upon acceptance. 

\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{l|c|c|c|c}
    \hline
         & Lang & Songs & Lyrics & Source\\
    \hline
     LMD & En & \diagbox[]{}{} & 152,991 & Back Translation\\
    \hline
     Changba & Zh & \diagbox[]{}{} & 542,034 & Back Translation\\
    \hline
     PopCV & En,Zh & 79 & 2,959 & Annotation\\
    \hline
     testset & En,Zh & 25 & 629 & Annotation\\
    \hline
    \end{tabular}
    \caption{Statistics of datasets in our experiments}
    \label{tab:dataset_stat}
\end{table}

\subsubsection*{Evaluation Metrics}

The most convincing evaluation of how our model works is whether the translated songs can be sung, understood, and, most importantly, enjoyed. 
Thus, we follow \citet{songmass}~and show annotators the resulting score of the song with translated lyrics. 
To verify the singability in the end-to-end manner, we additionally use an open-source Chinese singing voice synthesis (SVS)  model~\citep{diffsinger} to supply the annotators with an actual audio rendition of the songs for more intuitive feeling.

We randomly select 20 verses from the test set and show the music sheets and synthesized singing voice~(see Appendix \ref{appendix:data}) of each translated verse to five annotators.
For automatic evaluations, we use sacreBLEU\footnote{\scriptsize{\url{https://github.com/mjpost/sacrebleu}}}.  
For translation intelligibility, naturalness, singability and overall quality evaluation, we use mean opinion score~(MOS) in human evaluations, referred to MOS-T, MOS-S and MOS-Q. 
In evaluating the alignments, the traditional AER does not apply here because in addition to machine-produced alignments, the target translation is also machine-produced. 
Instead, we propose an Alignment Score (AS) that calculates the weighted intersection over ground truth (IOG) of the empirical probability density between the predicted and the true alignments:
%改成重合对应音符数的期望值和真实音符数的期望值之间的比值
%
\begin{equation}
    % \text{AS} = \frac{\sum_{k} \min(\text{freq}_{pred}^k, \text{freq}_{gt}^k) * k)}{\sum_{k} \text{freq}_{gt}^k * k }
    \text{AS} = \frac{\sum_{k}\min(\text{freq}_{pred}^k/F_{pred}, \text{freq}_{gt}^k/F_{gt}) * k)}{\sum_{k} (\text{freq}_{gt}^k/F_{gt}) * k) } 
\end{equation}
%
where $k$ represents the number of aligned notes, and $F = \sum_{k} \text{freq}^k$. 

\begin{figure*}[t]
    \centering
% \subfigure[GT]{
%     \includegraphics[width=0.23\textwidth,clip=true]{figures/align_hist.pdf}
% }
\subfigure[GagaST]{
    \includegraphics[width=0.31\textwidth,clip=true]{figures/gagast_align_hist.pdf}
}
\subfigure[\modelname-cls]{
    \includegraphics[width=0.31\textwidth,clip=true]{figures/baseline_align_hist.pdf}
}
\subfigure[\modelname]{
    \includegraphics[width=0.31\textwidth,clip=true]{figures/ltag_align_hist.pdf}
}
\caption{The overlapped histograms of ground truth alignments and predicted alignments on En$\rightarrow$Zh test set.}
\label{fig:align_hist}
\end{figure*}

\subsubsection*{Model Configurations}

The token embeddings of the Transformer encoder and decoder have dimension of 256 and are shared.
In the note-pooling embedding layer, the size of the lookup table for MIDI pitch and duration type are set to 128 and 31. 
The halting hyper-parameter epsilon $\epsilon$ for the adaptive grouping process is 0.05. 
$w_j$ is 5 when $\Delta_j > 1$, and $\beta$ is 0.8 in the joint loss $L_{joint}$. 
The beam size during decoding is 5.

The \modelname~model is pre-trained on the WMT data and the crawled lyrics data, including the parallel and the monolingual corpora.
% , with 4 Tesla A100 GPUs and 20480 max tokens per batch. 
% For the co-translation training, we use one Tesla V100 GPU with 4096 max tokens per batch.
The sampling ratio scheduling of augmented data and annotated data are described in Appendix \ref{appendix:bt_cl}. 

For voice synthesis, we convert the Chinese lyrics into phonemes by \emph{pypinyin}~\citep{ren2020deepsinger} and set the hop size and frame size to 128 and 512 for the sample rate of 24kHz. 
Pitch inputs to the SVS model are all re-tuned to the range between $A3$ and $C5$ in C major. 
Besides, we apply some post-processing in inference to generate scores and singing voice for more tolerance (Appendix \ref{appendix:infer}). 


\begin{table}[tbp]
    \centering
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{l|c|c|c|c}
    \hline
    \multirow{2}{*}{Models} & \multicolumn{2}{c|}{BLEU$\uparrow$} & \multicolumn{2}{c}{AS. $\uparrow$}\\
    \cline{2-5}
    & En$\rightarrow$Zh & Zh$\rightarrow$En & En$\rightarrow$Zh & Zh$\rightarrow$En \\
    \hline\hline
    GagaST & 11.87 & 5.67 & 0.701 & 0.468\\
    \hline
    \modelname-cls & 14.21 & 10.01 & 0.827 & 0.555\\
    ~~~ only bt  & 15.54 & 10.21 & 0.709 & 0.667\\
    ~~~ w/o bt & 13.73 & 8.26 & 0.704 & 0.490 \\
    \hline
    \modelname & 16.02* & \textbf{10.68} & \textbf{0.923} & \textbf{0.781} \\
    ~~~ only bt  & \textbf{16.27} & 10.26* & 0.880* & 0.718* \\ 
    ~~~ w/o bt & 14.12 & 7.86 & 0.845 & 0.710\\
    ~~~ w/o $\mathbf{e}_{align}$  & 15.16 & 9.24 & 0.852 & 0.703\\
    \hline
    \end{tabular}
    \caption{The sacreBLEU and Alignment Score on both translation directions. * means the second highest result within the row.}
    \label{tab:objective}
\end{table}

\subsection{Main Results}
We compare \modelname~with two baseline systems. 
One is the GagaST system~\cite{gagast}, which focuses on the tonal aspect of Chinese. 
The other one is a variation of our model. This variation uses a transformer-layer based classifier (\modelname-cls) instead of our alignment decoder to predict the number of aligned notes. The maximum number of aligned notes is 30, the same as allowed maximum $K(j)$ in alignment decoder. 
Besides, we show results from the human reference.
\subsubsection{Translation Evaluation}

We first report the human evaluation metrics (MOS-T) on both Chinese-to-English~(Zh$\rightarrow$En) and English-to-Chinese~(En$\rightarrow$Zh) song translation tasks in Table \ref{tab:subjective}. 
\modelname~generally gains improvements among all systems while the gap between different systems and settings is not obvious. 
It's partly because the lyrics translation by professionals is usually free translation rather than literal translation. 
A missing word in different slices can cause negative, neutral or even positive effect. 
Only obvious semantic deviations or grammatical mistakes lead to certain score decrease. 
As discussed in MOS-T, automatic metric BLEU may not be a good criterion to compare the machine translation and free translation for lyrics. 
But we still present the BLEU results in Table \ref{tab:objective}. 
We can see that our proposed system \modelname~significantly outperforms the recent baseline GagaST by a large margin on both translation directions. 
As to the variant model \modelname-cls, the \modelname~is still slightly better. 

%需要解释BLEU值 only 高，加了标注数据有所下降
\begin{figure*}[t]
    \centering
\subfigure[Source and Reference. Left: En$\rightarrow$Zh. Right: Zh$\rightarrow$En]{
    \includegraphics[width=0.55\textwidth,clip=true]{figures/exp_en_1.pdf}
    \includegraphics[width=0.43\textwidth,clip=true]{figures/exp_zh_2.pdf}
}
\subfigure[GagaST]{
    \includegraphics[width=0.55\textwidth,clip=true]{figures/exp_gagast_zh_1.pdf}
    \includegraphics[width=0.44\textwidth,clip=true]{figures/exp_gagast_en_2.pdf}
}
\subfigure[\modelname-cls]{
    \includegraphics[width=0.55\textwidth,clip=true]{figures/exp_baseline_zh_1.pdf}
    \includegraphics[width=0.44\textwidth,clip=true]{figures/exp_baseline_en_2.pdf}
}
\subfigure[\modelname]{
    \includegraphics[width=0.55\textwidth,clip=true]{figures/exp_LTAG_zh_1.pdf}
    \includegraphics[width=0.44\textwidth,clip=true]{figures/exp_LTAG_en_2.pdf}
}
\caption{Example scores of the source, reference and the translation for ``Is love I can be sure of'' in \textit{Will You Love Me Tomorrow} and ``t\={a} hu\.{i} y\v{o}u du\={o} x\.{i}ng y\.{u}n'' in \textit{Xi\v{a}o X\.{i}ng Y\.{u}n} from three systems.}
\label{fig:score_analysis}
\end{figure*}


\subsubsection{Lyrics-Melody Alignment Evaluation}

As for lyrics-melody alignment quality, we report the human evaluation metrics (MOS-S) on en-zh translation direction. 
In Table \ref{tab:subjective}, \modelname~considerably outperforms other systems, especially better than the GagaST with simple length control decoding. 
Notably, the variant version \modelname~cls performs worse than other systems, which indicates that more flexible alignments between lyrics and melody bring listening enjoyment to audience when it's reasonable enough. 
Otherwise, the flexibility may be counterproductive.  
We also evaluate the alignment quality by using the histograms of the number of aligned notes in Figure \ref{fig:align_hist}. 
In Table \ref{tab:objective}, we calculate the Alignment Score between the histograms of each system and the true histograms. 
The histograms show that the distribution of alignments generated by \modelname~resemble those of the true alignments while ``GagaST'' lacks variety by providing only one-on-one alignments between the lyrics and melody. 
In conclusion, both results demonstrate that the adaptive grouping method shows significant advantage over the length control or simple classifier in predicting reasonable alignment between the translated lyrics and melody. 

In Table \ref{tab:subjective}, MOS-Q mainly reflects the overall intelligibility, naturalness, singability and beauty of the song translation. 
Since the translation and alignment quality both contribute to the final result, the difference between methods seem less visible. 
But considering the 95\% confidence, we can conclude that the \modelname~still ranks best.

\subsection{Ablation Study and Analysis}

We first conduct ablation experiments to study the effects of back translation data with various settings. 
In Table \ref{tab:subjective}, we have the following findings for \modelname~and \modelname-cls. 
(1) Since the back-translation data is obviously larger than the real annotation data, there is almost no difference if only back-translation data is used for training. 
This enables the possibility of training our model in unsupervised way. 
(2) If only the limited supervised data is used, the performance apparently becomes worse. 
(3) \modelname~is consistently better than \modelname-cls in all ablation experiments. 
In addition, we verify the importance of the novel alignment embedding $\mathbf{e}_{align}$ by removing it from the note-pooling embedding layer and alignment decoder, and observe a non-negligible decrease on both BLEU and AS.

% As the sampling ratio of back-translation data goes down, the alignment quality increases while translation quality holds stable.
% \begin{table}[htbp]
%     \centering
%     \begin{tabular}{|c|c|c|}
%     \toprule
%     &  \multicolumn{2}{c|}{Co-translation Failure Rate}\\
%     \cmidrule{2-3}
%          & en-zh & zh-en \\
%     \midrule
%     GagaST & 0.0\% & 0.0\% \\
%     \midrule
%     TF+cls & 5.12\% & 4.23\% \\
%     \modelname & 6.78\% & 9.16\% \\
%     \bottomrule
%     \end{tabular}
%     \caption{Rates of translation failure of different systems.}
%     \label{tab:tarns_error}
% \end{table}

Some case studies in Figure \ref{fig:score_analysis} suggest that, when the tokens in lyrics fall into one-to-many alignments, GagaST usually provides inappropriate lyrics translation or even decodes non-vocal tokens such as comma to meet the length constraint. 
It will hurt both the translation quality and the singability of the translated lyrics. 
In contrast, the simple classifier following transformer layers is enough for flexible alignments. 
However, our evaluation results indicate our light weighted alignment decoder is capable of providing delicate alignments between tokens and notes.



