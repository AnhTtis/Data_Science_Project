In this section, we do experiments to explore each application's thread scalability, bandwidth consumption, prefetcher sensitivity, without co-running. 

\begin{figure*}
    \centering
    %\quad
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{./figure/scale_powergraph.pdf}
        \caption{Powergraph}
        \label{fig:scale_powergraph}
    \end{subfigure}  
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{./figure/scale_gemini.pdf}
        \caption{Gemini}
        \label{fig:scale_gemini}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{./figure/scale_cntk.pdf}
        \caption{CNTK}
        \label{fig:scale_cntk}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{./figure/scale_parsec.pdf}
        \caption{PARSEC}
        \label{fig:scale_parsec}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{./figure/scale_spec.pdf}
        \caption{SPEC CPU2017}
        \label{fig:scale_spec}
    \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{./figure/scale_hpc.pdf}
        \caption{HPC}
        \label{fig:scale_hpc}
    \end{subfigure}
    \caption{Normalized speed up for different number of threads of each application.}
    \vspace{-5mm}
\end{figure*}


\subsection{Thread Scalability}
\label{sec:threadscale}
To evaluate each application's thread scalability, we run each application on this machine exclusively with fixed input size. Each application's thread number increases linearly from 1 to 8. 
%Each thread occupies one physical core. 
The application's execution time is measured to calculate the speedup among different thread counts. 
For application with a non-negligible data preprocessing phase, we only measure the runtime of the execution phase. 
For instance, using 4 threads to preprocess the \textit{friendster} graph in GeminiGraph can take approximately 420s, while executing the SSSP application atop it only takes 48.6s. Similar phenomenon can be found in PowerGraph as well.  
Such preprocessing time is not included in the execution time, as this is a one-time cost and it can be amortized with multiple runs.
%the graph applications take a friendster graph as input (1.8 billion edges) However, for graph applications with huge input data size, the preprocessing time could be very large. So we treat the processing time as the execution time.  

As Figure \ref{fig:scale_powergraph} shows, the PowerGraph applications' scalability is good except P-SSSP: P-CC and P-PR scale linearly up to $6.7\times$, P-SSSP shows speedup less than $2\times$. The low scalability of P-SSSP is caused by the unrealistic assumption that all graph edges have identical weight. Its low scalability is observed in~\cite{shuangicpp} as well. GeminiGraph applications scale well(shown in Figure \ref{fig:scale_gemini}): all the applications can get more than $4\times$ scaling factors, and the maximum is up to $7\times$. G-PR, G-CC and G-BC grow faster when the number of threads is less than 4, and then their scale factor growth slow down. As shown in Figure~\ref{fig:bandwidth_eachapp}, these three applications' bandwidth are relatively higher and nearly saturated at 4-thread configuration. 
Compared to other graph applications, the speedup of G-SSSP on this scale-up test is less sharp. 
%This phenomenon is not due to the bandwidth bottleneck, but due to the nature of SSSP's irregular memory access pattern~\cite{burtscher2012,neil2014}. 
This phenomenon is due to the nature of G-SSSP's irregular memory access pattern~\cite{burtscher2012,neil2014} rather than the bandwidth bottleneck. Moreover, CNTK applications scalability varies a lot(Figure \ref{fig:scale_cntk}): CIFAR, MNIST and LSTM have good scalability up to $6.3\times$, while ATIS has no scalability. We use the Intel Vtune to profile ATIS and find that ATIS spends 80\% of total CPU cycles on \texttt{kmp\_hyper\_barrier\_release} function (synchronization bound), when its thread number is above 2. However, this function only costs 28\% execution cycles in 2-thread case. 

PARSEC applications have good scalability with more than $4\times$ speedup, especially  blackscholes and freqmine's speedup are nearly $8\times$. Furthermore, SPEC CPU2017 applications' scalability results are plotted in Figure \ref{fig:scale_spec}. CactuBSSN, nab and deepsjeng have perfect linear scalability. However, fotonik3d scales poorly after $4$ threads. Since fotonik3d is a memory intensive application, 4 fotonik3d instances cause memory bandwidth to reach its limit in practice. Therefore, the scalability of fotonik3d is relatively low. Finally, Figure \ref{fig:scale_hpc} shows HPC application's scalability: lulesh scales well, and IRSmk and AMG2006 saturates after 6 threads and 4 threads, respectively. AMG2006 has three different phases, where the first two phases leverage single thread to process input data. In its last phase, it generates intensive memory accesses and yields a medium scalability.


Based on each application's thread scalability performance, we characterize all applications into three categories: low scalability, saturate scalability, and high scalability. Table \ref{tab:thread_scale_cat} presents the thread scalability category for each application. 
Most graph processing applications (data analytics) have high scalability. For CNTK (machine learning) and HPC workloads, the scalability highly depends on the structure of the application. For  SPEC CPU2017 and PARSEC benchmarks, they either saturated around 4-thread or yields high scalability.
The information provided by our analysis can help choose the right configuration for execution without wasting hardware resources in the cloud environment.


\begin{table}[t]
\centering
\caption{Thread scalability characterization result.}
\label{tab:thread_scale_cat}
\begin{tabular}{|c|c|c|c|}
\hline
Suite & Low & Medium & High \\ \hline
Powergraph & P-SSSP & - & P-CC, P-PR \\ \hline
Gemini & - & G-SSSP & \begin{tabular}[c]{@{}c@{}}G-CC,G-BC,\\ G-PR,G-BFS\end{tabular} \\ \hline
CNTK & ATIS & CIFAR, LSTM & MNIST \\ \hline
PARSEC & - & \begin{tabular}[c]{@{}c@{}}streamcluster,\\ blackscholes\end{tabular} & swaptions,freqmine \\ \hline
SPEC CPU2017 & - & \begin{tabular}[c]{@{}c@{}}fotonik3d, \\ deepsjeng,cpuxalan\end{tabular} & \begin{tabular}[c]{@{}c@{}}mcf, nab,\\ cactuBSSN\end{tabular} \\ \hline
HPC & AMG2006 & IRSmk & lulesh \\ \hline
\end{tabular}
\vspace{-5mm}
\end{table}


\begin{figure}[t]
  \centering
  \includegraphics[width=3.5in,trim = 0mm 0mm 0mm 0mm, clip=true]	   
   {./figure/bandwidth_eachapp.pdf}
  \caption{Memory bandwidth of each application.}
  \label{fig:bandwidth_eachapp}
  \vspace{-5mm}
\end{figure}

\subsection{Bandwidth Analysis}

Figure \ref{fig:bandwidth_eachapp} shows each application's bandwidth consumption range with different number of threads, measured by Intel PCM 2.8. Due to the page limit, we only show bandwidth consumption for three configurations: 1-thread, 4-thread, 8-thread, where 1-thread yields the minimal bandwidth usage and 8-thread produces the maximal one.



Similar to the runtime measurement, for graph applications, the bandwidth information is measured in every execution phase. GeminiGraph applications consume higher bandwidth than PowerGraph applications, as GeminiGraph leverages thread-level work stealing technique and achieves better data locality due to the chunking partitioning scheme. Streamcluster in PARSEC, IRSmk and AMG2006 in HPC, fotonik3d and mcf in SPEC CPU2017 consume a larger amount of bandwidth compared to other applications in the same domain. Since we measure bandwidth consumption of each application with different thread configurations, we can verify the correlation between the bandwidth consumption and thread counts. Other than the graph applications discussed in Section~\ref{sec:threadscale}, we find that streamcluster, IRSmk, fotonik3d and CIFAR's bandwidth consumption growth becomes much slower as thread counts scales up from 4 to 8. The scalability of these benchmarks also saturates after 4-thread. We conclude that these benchmarks don't benefit from the increase in thread resources after 4 threads, as they are bounded by other resources, like the memory bandwidth. Workloads, like ATIS, blackscholes, freqmine, swaptions, xalancbmk, deepsjeng and nab, have extremely low bandwidth consumption. 

\subsection{Prefetcher Sensitivity}

Next we study each application's sensitivity to hardware prefetchers, as prefetcher is one of the most effective techniques for data intensive applications. Workloads with regular memory access pattern can benefit a lot from hardware prefetchers. Such applications usually take a lot of bandwidth resource and potentially degrade other co-running application's performance. This sensitivity study can help us identify such applications.

For Sandy Bridge machine,  there are four different hardware prefetchers: 
\begin{itemize}
\item L2 hardware prefetcher: fetches additional cache lines into L2 cache.
\item L2 adjacent cache line prefetcher: fetches successive cache lines.
\item L1-data cache prefetcher: fetches the next cache line into L1-D cache.
\item L1-data cache IP prefetcher: fetches additional cache lines determined by instruction point of previous load history.
\end{itemize}
For each core, there is a Model Specific Register (MSR) to control hardware prefetchers. Each prefetcher can be activated or deactivated by setting the corresponding MSR bit. Such MSR bits can be found on Ivy Bridge, Haswell, Broadwell processors as well~\cite{intelmanual}.

\begin{figure}[t]
  \centering
  \includegraphics[width=3.5in,trim = 0mm 0mm 0mm 0mm, clip=true]	     
   {./figure/prefetch_sensitivity.pdf}
  \caption{Prefetch sensitivity: Slowdown if prefetcher is turned off}
  \label{fig:prefetch_sensitivity}
  \vspace{-5mm}
\end{figure}
We fix each application's thread-count and use 4-thread with prefetchers enabled as our baseline. 
%We measure the execution time of each applciation with 4 threads, when all prefetcher are tuned on or not. 
Each application's execution time with all prefetchers activated is normalized to the execution time with prefetchers deactivated. Results are plotted in Figure 4.
In GeminiGraph and PowerGraph, all graph applications do not benefit from L1/L2 cache prefetchers, as their memory access patterns are not regular enough. Similar to graph applications, machine learning applications in CNTK are not sensitive to prefethers. However, the root cause can be different, as the memory bandwidth consumption of graph applications is $2.45\times$ higher than CNTK applications'. Therefore, applications like ATIS is very robust to the availability of prefetchers. Such applications are unlikely to harm other consolidating application.

\begin{figure*}[t]
  \centering
%  \includegraphics[height=0.425\textheight,width=1\textwidth,trim = 50mm 0mm 50mm 0mm, clip=true]     
\includegraphics[height=0.425\textheight, width=0.85\textwidth, clip=true]
   {./figure/corun_heatmap_colorful_blackwhite.pdf}
  \caption{Normalized execution time of co-running two applications on the shared machine (foreground application on y-axis and background application on x-axis).}
  \label{fig:corun_heatmap}
  \vspace{-4mm}
\end{figure*}

The streamcluster in PARSEC,  HPC applications, and fotonik3d in SPEC CPU2017 are very sensitive to prefetchers, as their performance is slowed down by 1.18$\times$. All these benchmarks have a regular memory access pattern, and consume significant amount of bandwidth. These applications can potentially cause LLC and memory contention problems in the co-running case. 






