In this section, we provide an overview of the experimental platform, describe the applications in detail, and present two toolsets used for bandwidth measurement and bottleneck detection.

\subsection{Platform Configuration}
Our experiments are performed on a Supermicro 8047R-TRF+ server with 8-core Intel Xeon E5-4650 processor clocked at 2.7GHz. Each core has a private 32K L1 instruction Cache, 32K L1 data Cache and 256K L2 Cache. All cores share a 20MB L3 Cache and 64 GB DRAM. The operation system on this machine is CentOS 7.

Similar to the server configuration in warehouse-scale computing~\cite{tacc1,tacc2,amazonht}, we disable the Hyper-Threading (HT) technology of the machine, as HT can reduce certain application's performance. %Furthermore, to isolate application performance from other sharing resources, e.g. L2 Cache, we binded each application to specific cores exclusively. 
Furthermore, to minimize the effects (e.g. context switch) on applications performance, we binded each application to specific cores exclusively. Figure \ref{fig:experiment_setup} shows the binding configurations for the co-running experiments. All applications are configured with 4 threads. Each application takes 4 physical cores exclusively. With such configuration, the only shared resources are the LLC and the memory subsystem.  



\begin{figure}[t]
  \centering
  \includegraphics[width=3in]	       {./figure/ExperimentSetup.pdf}
  \caption{Experiment Setup}
  \label{fig:experiment_setup}
  \vspace{-5mm}
\end{figure}

\subsection{Workload Description}

We form our workload set using a wide range of applications from 5 application domains: Graph Processing, Deep Learning, SPEC CPU2017, PARSEC and HPC. Table \ref{tab:benchmark_suite} summarizes the chosen applications. Details of each application suite are summarized as follows:
%covering 4 categories: Graph Processing, Deep Learning, SPEC CPU2017, PARSEC.
\paragraph{\textbf{PowerGraph}} PowerGraph~\cite{powergraph} is one of the earliest graph systems that is designed for data mining and analytics. It is implemented in the classic bulk synchronization manner. Techniques used in it have been widely adopted by many other graph frameworks~\cite{powerswitch,powerlyra,cube,shuangsc,shuangicpp}. 

\paragraph{\textbf{GeminiGraph}} GeminiGraph~\cite{geminigraph} is one of the state-of-the-art graph processing frameworks with near-linear scalability. This framework emphasizes on optimizing computation performance via thread-level partitioning and balancing. We choose 5 representative data analytics applications implemented atop it, which include Pagerank (PR), single source shortest path (SSSP), breadth first search (BFS), betweenness centrality (BC), and connected component (CC). We use the \textit{friendster} graph~\cite{snapnets} with 65.6 million vertices and 1.8 billion edges in both GeminiGraph and PowerGraph.

\paragraph{\textbf{CNTK}} CNTK (short for The Microsoft Cognitive Toolkit)~\cite{cntk} is an open-source and commercial grade toolkit of deep learning applications. We deploy 3 applications from this toolkit. ConvNet is an image recognition workload with CIFAR and MNIST datasets. LSTM is a Long Short Term Memory network using CMU-AN4 data input. ATIS a natural language processing workload, which leverages the data from the air travel information system. For these deep learning applications, only the training phase is used for performance measurement.

\paragraph{\textbf{PARSEC}} The PARSEC benchmark suite~\cite{parsec} is used to represent parallel real-world applications. We use the largest input sets that are designed for native execution. Four benchmarks are chosen from this suite, which are blackscholes, freqmine, swaptions, and streamcluster. 
%These PARSEC benchmarks are resilient to performance degradation of intra-application cache-sharing~\cite{zhang2010}. 

\paragraph{\textbf{SPEC CPU2017}} The SPEC CPU2017 benchmark suite is recently released to represent CPU/memory-intensive applications. It is designed to mimic the real-world applications. Using the subseting methodology suggested by Phansalkar et al.~\cite{Phansalkar2007}, we obtain the performance data (published on SPEC website~\cite{spec2017}) submitted by industries and choose 6 benchmarks to represent SPEC CPU2017 suite. CactuBSSN, nab, and fotonik3d are floating-point benchmarks. Xalancbmk, mcf, and deepsjeng are integer benchmarks. To achieve the effect of parallel execution (SPEC rate), multiple copies of each workload are executed simultaneously. 

\paragraph{\textbf{HPC}} Three benchmarks published by Lawrence Livermore National Laboratory are chosen: AMG2006, IRSmk and lulesh. AMG2006 is a parallel algebraic multi-grid solver for linear systems. IRSmk is a parallel nested do-loops that perform a matrix multiply. Lulesh solves the Sedov blast wave problem for one material in 3D.

\iffalse
\begin{itemize}
\item \textbf{Gemini}, a state of the art distributed graph processing system, which has good computation performance scalability. We choose 5 representative applications built on it. We use friendster dataset as input graph data of these 5 applications.
\item \textbf{PowerGraph}, another commercialized distributed graph processing system. We include 4 representative applications, with the wikipedia graph dataset as input.
\item \textbf{SPEC CPU2017}, it includes compute intensive applications to measure system's performance. 6 of them are chosen with ref input size.
\item \textbf{CNTK}, short for The Microsoft Cognitive Toolkit, an open-source and commercial-grade toolkit, which trains and tests deep learning algorithms.
\item \textbf{PARSEC}, short for the Princeton Application Repository for Shared-Memory Computers, contains multi-threaded programs focusing on emerging workloads.
\item \textbf{HPC Workloads}, it contains several memory access pattern representative HPC workloads.
\end{itemize}
\fi


Other than these real-world applications, we leverage two mini-benchmarks to study specific memory system features such as bandwidth. The two mini-benchmarks are Bandit\cite{haoxu2017} and Stream \cite{stream1995}, both of which are memory stressing benchmarks. The Bandit program continuously issues memory requests, where all requests result in cache misses. Every memory access in Bandit conflicts with its previous one in caches, so such access goes to the main memory. Stream program streams a large amount of data from memory. It has a very regular memory access pattern. Hence, hardware prefetchers help load consecutive cache lines from memory, which results in extremely high memory bandwidth consumption.

\begin{table}[t]
\centering
\caption{Applications chosen for each Application Suite}
\label{tab:benchmark_suite}
\begin{tabular}{|c|c|}
\hline
Benchmark Suite & Benchmarks \\ \hline
GeminiGraph & PR, BFS, BC, SSSP, CC \\ \hline
PowerGraph & PR, SSSP, CC \\ \hline
CNTK & ConvNet-CIFAR/MNIST, LSTM-AN4, ATIS\\ \hline
PARSEC & freqmine, streamcluster, swaptions, blackscholes \\ \hline
HPC & lulesh, amg2006, IRSmk \\ \hline
SPEC CPU2017 &  mcf, fotonik3d, deepsjeng \\ 
& nab, xalancbmk, cactuBSSN \\ 
\hline
mini-benchmarks & Bandit, McCalpin Stream \\ \hline
\end{tabular}
\end{table}



\subsection{Performance Measurement}
To measure the bandwidth consumption at runtime, we adopted Intel Processor Counter Monitor (PCM) 2.8~\cite{pcm} to read MSR registers. PCM provides a set of tools based on its API to monitor performance and energy metrics of Intel processors. We used pcm-memory to measure bandwidth at 10 seconds granularity with extremely low overhead.
In addition, we used Intel VTune Amplifier 2017~\cite{vtune} to find performance bottleneck of problematic co-running pairs. VTune can provide hardware event-based sampling analysis result, with a very low overhead. Beside, it also can attribute event sampling results to specific hot spots.  





