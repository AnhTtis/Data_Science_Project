\pdfoutput=1

\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

\input{math_commands.tex}

\usepackage{hyperref}

\usepackage{url}
\usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
\title{Adapting to the Low-Resource Double-Bind: Investigating Low-Compute Methods on Low-resource African Languages}

% EXPERIMENTAL SETUP (by Chris Emezue) https://docs.google.com/document/d/1rW6TZ4Z2zsrPHd1xKAoEGW5CYZqnFX_YwseFQLEtpoY/edit#

% ORIGINAL TEMPLATE: https://www.overleaf.com/latex/templates/template-africanlp-workshop-at-iclr-2022/cvbbssrkxjpb 
% website: https://sites.google.com/view/africanlp2023/home?pli=1 

% EXAMPLE OF A PREVIOUS WIP SUBMISSION https://openreview.net/pdf?id=B0GEqcGV8-5

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{\forall, Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

\author{\normalsize $\forall$*, Colin Leong$^{1}$, Herumb Shandilya$^*$, Bonaventure F. P. Dossou $^{2,3,4,14}$, Atnafu Lambebo Tonja$^{5}$,\\ 
\textbf{\normalsize Joel Mathew$^{6}$,  Abdul-Hakeem Omotayo $^{7}$, Oreen Yousuf$^*$, Zainab Akinjobi $^{8}$, }\\
\textbf{\normalsize  Chris Chinenye Emezue $^{9,14}$, Shamsudeen Muhammad  $^{10}$, Steven Kolawole $^{11}$,}\\
\textbf{\normalsize Younwoo Choi $^{12}$, Tosin Adewumi $^{13}$} \\\\
\footnotesize
$^*$Masakhane NLP, $^1$University of Dayton, $^2$Center for Intelligent Machines, McGill University, $^3$Mila Quebec AI Institute,  \\ \footnotesize $^4$Lelapa AI, $^5$Instituto Politécnico Nacional,
$^6$USC Information Sciences Institute, $^7$University of California, Davis \\ \footnotesize $^8$New Mexico State University, $^9$ Technical University of Munich, $^{10}$University of Porto, $^{11}$ML Collective, 
\\ \footnotesize $^{12}$University of Toronto, $^{13}$ML Group, Luleå University of Technology,$^{14}$Lanfrica.
 \\
 % $^\dagger$ \small{These authors contributed equally to this work}
 \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

 \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\maketitle
\makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or\forall\or\dagger\fi}}
\makeatother

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{To represent the whole Masakhane community.}
\renewcommand{\thefootnote}{\arabic{footnote}}


\begin{abstract}
Many natural language processing (NLP) tasks make use of massively pre-trained language models, which are computationally expensive. However, access to high computational resources added to the issue of data scarcity of African languages constitutes a real barrier to research experiments on these languages. In this work, we explore the applicability of low-compute approaches such as language adapters in the context of this \textit{low-resource double-bind}. We intend to answer the following question: do language adapters allow those who are doubly bound by data and compute to practically build useful models? Through fine-tuning experiments on African languages, we evaluate their effectiveness as cost-effective approaches to low-resource African NLP. Using solely free compute resources, our results show that language adapters achieve comparable performances to massive pre-trained language models which are heavy on computational resources. This opens the door to further experimentation and exploration on full-extent of language adapters capacities.
% We intend to evaluate various methods for pre-training and fine-tuning the resulting models on African-language benchmarks to quantify what performance improvements can be achieved under \textit{double-bind} conditions. We will also investigate using Phylogeny-based training, combining data from related language families. We will analyze performance improvements with respect to compute and data constraints and release resulting code and models as well as empirically determined recommendations for low-resource and low-compute settings. 
\end{abstract}

% Our results on Masakhane NER show that Adapters are [good/bad/good if you do x] because you can train them fast and iterate quickly, and then try out some other data, and even with only xyz amount of data you can still achieve such and such improvements. We combine this with the use of phylogeny-based methods, showing that it is possible to do all of this with no budget and still achieve such and such further improvements. We haven't done this all yet but it's WIP.


\section{Introduction and Motivation: Adapting to The Low-resource Double-Bind Problem}

\cite{ahia-etal-2021-low-resource} coined the term \textit{low-resource double-bind} to describe the \textbf{co-occurrence of
data limitations and compute resource constraints}. Especially in the African setting, this double limitation often occurs  because most people do not have access to compute resources like GPUs and TPUs to construct different research projects that require more and more computational resources. The other limitation is the availability of datasets: African languages account for a small fraction
of available language resources, and NLP
research rarely considers them \citep{nekoto2020participatory}. This has a big impact on researchers working on different NLP tasks for African languages.  
% when there are data limitations and compute limitations.
%this is often the experience: if one is limited by data, it is likely one will be limited by compute. 
In this study, we embrace this double limitation and investigate computationally efficient methods under low-data and compute conditions that will enable the researchers to work on different NLP tasks for African languages without being limited to the dataset and computational resources. We seek to answer if, and how, these can be used to build useful models for different NLP tasks. We focus specifically on training language and task adapters \cite{pfeiffer2020AdapterHub} and evaluating them on downstream Named Entity Recognition (NER) tasks.
% Some people have tried things. 
% Talk about how it used to be that you would pretrain a model like mBERT on 100 languages, and extending that to 101 languages is a huge pain. 

\subsection{Unique Challenges for Double-Bind Model Training}
Training under the double-bind scenario introduces a number of unique challenges. For example, using free compute limits the size of the \textit{model}, \textit{dataset}; and it leads to the length of training. Resource limits may cause training runs to timeout, putting wall-clock limits on training.

\subsection{Language Adapters, Task Adapters and AdapterHub}
Fine-tuning all or a majority of a pre-trained model needs a lot of computational resources and also it depends on the availability of the dataset for the specific task which is the problem in the African context.
% at a high cost in terms of memory and compute demands. 
 \citet{pfeiffer2020AdapterHub} introduced AdapterHub, which is a central repository for pre-trained adapter modules. \textit{Adapters} refers to a set of newly introduced weights, typically within the layers of a transformer model. Adapters provide an alternative to fully fine-tuning the model for each downstream task while maintaining performance. Adapters also have the benefit of requiring as little as 1MB of storage space per task \citep{pmlr-v97-houlsby19a}. 
 % which are small blocks within pre-trained Transformer models that add only a small number of trainable parameters per task, or per \textit{language} \citep{pmlr-v97-houlsby19a}. 
 Rather than pre-training a large language model, a \textit{language adapter} can be pre-trained for each language.
% \footnote{\url{https://github.com/google-research/bert/blob/a9ba4b8d7704c1ae18d1b28c56c0430d41407eb1/multilingual.md}}
Adapter modules are parameter-efficient, sustainable, and achieve near SOTA results on low-resource and cross-lingual tasks \citep{pmlr-v97-houlsby19a, he-etal-2021-effectiveness}.
In this work, we leverage Adapter-based tuning for African languages because it has been shown to mitigate forgetting issues better than fine-tuning, as it yields representations with less deviation from those generated by the original pre-training \citep{he-etal-2021-effectiveness}.  Moreover, Adapter fine-tuning does not take a lot of time because of its lightweight nature: we believe this will be one of the main advantages for people who are highly limited with computational resources.
% who do not have computing resources can run their experiments in freely available environments like Google Colab and Kaggle etc.      

% \subsection{Unique challenges for the double-bind scenario}

% % Maximum model size

% % Batch size constraints

% % Dataset size constraints

% % Wall-clock time constraints (disconnection)

% \subsection{Low-Data methods take a lot of compute}



% \subsection{Low-Compute methods}

\section{Current Status and Results: Community Model Training Using Free Resources}
% Describe the Experimental Setup. We can copy/paste things from  \href{https://docs.google.com/document/d/1rW6TZ4Z2zsrPHd1xKAoEGW5CYZqnFX_YwseFQLEtpoY/edit#}{The Experimental Setup Document}

We ran a collaborative project, where community volunteers used free resources (i.e., Google Colab) to pre-train language adapters for several African languages, then used those language adapters to fine-tune on the MasakhaNER 1.0 and 2.0 datasets \citep{adelani-etal-2021-masakhaner, adelani-etal-2022-masakhaner}, to determine how much benefit the language adapters would provide. We used Weights and Biases \citep{wandb} for experiment tracking and analysis. Community Members were told not to use any computing resources that would not be available to a graduate student in Africa with limited funds. %(e.g., Kigali, Uganda). 

\subsection{Initial Experiments: Pre-train Language Adapters and Fine-tune on NER}

For our initial experiments, we concentrated on creating a pipeline to evaluate monolingual language adapters. We used default settings taken from AdapterHub examples, and pre-trained monolingual language-adapter modules using \textbf{\textit{roberta-base}} as the base model. Each language adapter was then used to perform downstream NER tasks on the respective language.
% TODO: MAFAND dataset description
Each language adapter is pre-trained using the \textbf{\textit{MAFAND-MT}} dataset, the largest MT benchmark dataset for African languages in the news domain, covering 21 languages \citep{adelani-etal-2022-thousand}. We used target-language sentences to pre-train monolingual language adapters%For pre-training monolingual language adapters, we simply took the target-language sentences and concatenated them into .txt files, the format expected for language adapter pre-training with the MLM task.
, and finetuning on NER was performed using both MasakhaNER (1.0 and 2.0) datasets. To compare and evaluate the performance using language adapters in a downstream task, we conducted a baseline experiment by fine-tuning roberta-base pre-trained language model. Information about dataset splits has been presented and detailed in Table \ref{tab:lang-data}.

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[h]
\small
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllccccc}
\hline
Language (ISO) &
  Family &
  Region &
  \multicolumn{2}{l}{\textbf{Language Adapter Data}} &
  \multicolumn{3}{l}{\textbf{NER Finetuning Data}} \\ 
  &&&Train&Dev&Train&Dev&Test\\\hline
  
Amharic (amh)         & Afro-Asiatic-­Ethio-Semitic & East            & 1037 &  899                      & 1750   &250                     & 500                        \\
Fon (fon)             & Niger-Congo-Volta-Niger      & West            & 2637                         & 1227                        & 4343 & 621 & 1240                       \\
Hausa (hau)           & Afro-Asiatic-Chadic        & West    & 5865  &1300 & 1903&2072 & 545 \\
Igbo (ibo)            & Niger-Congo-Volta-Niger      & West            & 6998 &1500                        & 2233 &319& 638 \\
Kinyarwanda (kin)     & Niger-Congo-Bantu            & East & 1006  & 460&2110&301&604 \\
Luganda (lug)         & Niger-Congo-Bantu            & East            & 4075                         &1500& 2003   &200                     & 401                        \\
Nigerian-Pidgin (pcm) & English Creole               & West& 4790 & 1484& 2100&300&600                        \\
Swahili (swa)& Niger-Congo-Bantu & East \& Central & 30782 & 1791 &2104 &300& 602 \\
Akan/Twi (twi)        & Niger-Congo-Kwa              & West& 3337 &  1284           & 4240                        &605& 1211                       \\
Wolof (wol)           & Niger-Congo-Senegambia       & West & 3360  &1506& 1871 & 267&536 \\
Yorùbá (yor)          & Niger-Congo-Volta-Niger      & West & 6644  &1544& 2124&303                        & 608                        \\
Zulu (zul)            & Niger-Congo-Bantu            & South           & 3500 &1239                        & 5848     &836                   & 1670                       \\ \hline
\end{tabular}%
} 
\caption{Languages with ISO 639-2 Code. Language adapter training data was taken from the MAFAND dataset. NER fine-tuning and Evaluation data was taken from the MasakhaNER and MasakhaNER 2.0 datasets.}
\label{tab:lang-data}
\end{table}


% \section{Current Results}

% We present preliminary language adapter results.

%We did language adapter results but we don't have without-adapter yet, that's future work. Also in future work we will do other models such as multilingual-bert, and compare with Masakhane NER paper. 




% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table*}[h]
\centering
\begin{tabular}{lcccc}
\hline
\textbf{Language}  &\multicolumn{4}{c}{\textbf{F1 - Score}}   \\ 
&\multicolumn{2}{l}{Baseline NER}  &\multicolumn{2}{l}{Adapter NER}\\
& Dev & Test & Dev & Test \\ \hline
Amharic     &0.32&\textbf{0.34}    & 0.29                         & 0.27                            \\
Fon           &0.83&0.79  & 0.82&\textbf{0.80}  \\
Hausa         &0.90&\textbf{0.85}     & 0.85                        & 0.79                            \\
Igbo           &0.84&\textbf{0.79}    & 0.65  & 0.69                              \\
Kinyarwanda    &0.79&\textbf{0.68}    & 0.64  &0.60\\
Luganda        &0.67&\textbf{0.73}  & 0.64 &0.70 \\
Nigerian-Pidgin   &0.90&\textbf{0.87}& 0.89  &0.83\\
Swahili        &0.84&\textbf{0.81}   & 0.81  & 0.78     \\
Akan/Twi      &0.77&\textbf{0.75}   & 0.75   & 0.73 \\
Wolof        &0.70&\textbf{0.57}     &0.68    & 0.56    \\
Yorùbá  &0.66&\textbf{0.71} & 0.65 & 0.68    \\
Zulu       &0.78&\textbf{0.83}   & 0.76    & 0.80                             \\ \hline
Average       &0.75&\textbf{0.72}   & 0.72    & \textit{\underline{0.69}}                             \\ \hline
\end{tabular}%

\caption{Results for averaged eval and predict F1 scores by language.}
\label{tab:results}
\end{table*}

% Credit to David Adelani for suggesting this. 

% TODO: cite the phylogeny paper

\section{Results and Future works}
In this section, we discuss the experimental results of our approach and future works.
\subsection{Adapters Enable Rapid Iteration}
In Table \ref{tab:results}, we present the results of two experiments, across 12 African Languages:
\begin{itemize}
    \item Baseline NER: setup where \textit{roberta-base} has been used to directly perform NER downstream (finetuning and evaluation) using MasakhaNER 1.0 and 2.0.
    \item Adapter NER: setup where we first of all trained a language adapter based on \textit{roberta-base}, then used the latter to perform downstream NER task.
\end{itemize}

Our initial results (Table \ref{tab:results}) with language adapters show comparable average performance to \textit{roberta-base} finetuned on the NER downstream task: this demonstrates that it is indeed feasible to train a monolingual language adapter in African low-resource settings, only with free computational resources while achieving comparable performance to massive pre-trained language model which requires a lot of computational resources.
% We look forward to even more comprehensive findings. We were able to easily track variables such as loss, learning rate, and GPU usage \& memory allocation with Weights and Biases.

Given how rapidly adapters can be trained and re-trained, it is possible to conduct rapid and iterative experiments. Therefore, there are many experiments we further wish to explore:
\begin{itemize}
    \item Extending experiments to Other Base Models: We will extend our experiments to several massive multilingual pre-trained language models like XLM-R \citep{conneau-etal-2020-unsupervised}, but also to Afro-centric language models like AfroLM \citep{dossou-etal-2022-afrolm}, AfriBERTa \citep{ogueji-etal-2021-small}, and AfroXLMR \citep{alabi-etal-2022-adapting}. This will allow direct comparison with previous benchmarks on MasakhaNER datasets.
    \item Alleviating Low-Data Issues with Phylogeny-based Methods: We will analyze how phylogeny-based adapter training affects performance. \citet{faisal-anastasopoulos-2022-phylogeny} carried this task out on other, non-African, low-resource languages. As training individual language adapters have proved to be time-efficient, we hope that training adapters on multiple, linguistically similar, languages yields better results.
    \item Quantifying Performance Improvement Tradeoffs vs Dataset Size: Given how quickly adapters can be trained, we can determine ratios of optimal data size to maximize performance. Power usage would have to be analyzed in this context as well.
    \item Other low-compute methods: In addition to the AdapterHub paradigm, we wish to comparatively analyze other low-compute methods such as \citet{cramming_https://doi.org/10.48550/arxiv.2212.14034}
\end{itemize} 

\section{Conclusion}
We built a pipeline for analyzing low-compute methods on low-resource languages. Initial results suggest that language adapter modules can be quickly and easily trained on entirely free resources such as Google Colab, opening the door to further experimentation and exploration. We hope to conduct further rounds of experiments and release both trained models and best practices for practical training of models when constrained by both low-data and low-compute.

% More baselines
% different models
% also try cramming
% also try Phylogeny-based methods, aka add in related langauge data, but of course we're constrained by memory and hard drive
% basically that. 
% \section{Submission of papers to AfricaNLP workshop at ICLR2023}
% See the workshop website for more instructions:\\ \url{https://sites.google.com/view/africanlp2023/home}

% If your paper is ultimately accepted, the statement {\tt
%   {\textbackslash}iclrfinalcopy} should be inserted to adjust the
% format to the camera ready requirements.

% \section{General formatting instructions}
% \label{gen_inst}

% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).
% Use 10~point type with a vertical spacing of 11~points. Times New Roman is the
% preferred typeface throughout. Paragraphs are separated by 1/2~line space,
% with no indentation.

% Paper title is 17~point, in small caps and left-aligned.
% All pages should start at 1~inch (6~picas) from the top of the page.

% Authors' names are
% set in boldface, and each name is placed above its corresponding
% address. The lead author's name is to be listed first, and
% the co-authors' names are set to follow. Authors sharing the
% same address can be on the same line.

% Please pay special attention to the instructions in section \ref{others}
% regarding figures, tables, acknowledgments, and references.


% There will be a strict upper limit of 8 pages for the main text of the initial submission, with unlimited additional pages for citations. Note that the upper page limit differs from last year!Authors may use as many pages of appendices (after the bibliography) as they wish, but reviewers are not required to read these. During the rebuttal phase and for the camera ready version, authors are allowed one additional page for the main text, for a strict upper limit of 9 pages.


\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

% \appendix
% \section{Appendix}
% You may include other additional sections here.

\end{document}
