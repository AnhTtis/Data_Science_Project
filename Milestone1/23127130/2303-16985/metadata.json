{
    "arxiv_id": "2303.16985",
    "paper_title": "Adapting to the Low-Resource Double-Bind: Investigating Low-Compute Methods on Low-Resource African Languages",
    "authors": [
        "Colin Leong",
        "Herumb Shandilya",
        "Bonaventure F. P. Dossou",
        "Atnafu Lambebo Tonja",
        "Joel Mathew",
        "Abdul-Hakeem Omotayo",
        "Oreen Yousuf",
        "Zainab Akinjobi",
        "Chris Chinenye Emezue",
        "Shamsudeen Muhammad",
        "Steven Kolawole",
        "Younwoo Choi",
        "Tosin Adewumi"
    ],
    "submission_date": "2023-03-29",
    "revised_dates": [
        "2023-03-31"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL",
        "cs.AI"
    ],
    "abstract": "Many natural language processing (NLP) tasks make use of massively pre-trained language models, which are computationally expensive. However, access to high computational resources added to the issue of data scarcity of African languages constitutes a real barrier to research experiments on these languages. In this work, we explore the applicability of low-compute approaches such as language adapters in the context of this low-resource double-bind. We intend to answer the following question: do language adapters allow those who are doubly bound by data and compute to practically build useful models? Through fine-tuning experiments on African languages, we evaluate their effectiveness as cost-effective approaches to low-resource African NLP. Using solely free compute resources, our results show that language adapters achieve comparable performances to massive pre-trained language models which are heavy on computational resources. This opens the door to further experimentation and exploration on full-extent of language adapters capacities.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.16985v1"
    ],
    "publication_venue": "Accepted to AfricaNLP workshop at ICLR2023"
}