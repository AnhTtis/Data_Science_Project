@article{azhand_algorithm_2021,
  title = {Algorithm Based on One Monocular Video Delivers Highly Valid and Reliable Gait Parameters},
  author = {Azhand, Arash and Rabe, Sophie and M{\"u}ller, Swantje and Sattler, Igor and {Heimann-Steinert}, Anika},
  year = {2021},
  month = jul,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {14065},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-93530-z},
  abstract = {Despite its paramount importance for manifold use cases (e.g., in the health care industry, sports, rehabilitation and fitness assessment), sufficiently valid and reliable gait parameter measurement is still limited to high-tech gait laboratories mostly. Here, we demonstrate the excellent validity and test\textendash retest repeatability of a novel gait assessment system which is built upon modern convolutional neural networks to extract three-dimensional skeleton joints from monocular frontal-view videos of walking humans. The validity study is based on a comparison to the GAITRite pressure-sensitive walkway system. All measured gait parameters (gait speed, cadence, step length and step time) showed excellent concurrent validity for multiple walk trials at normal and fast gait speeds. The test\textendash retest-repeatability is on the same level as the GAITRite system. In conclusion, we are convinced that our results can pave the way for cost, space and operationally effective gait analysis in broad mainstream applications. Most sensor-based systems are costly, must be operated by extensively trained personnel (e.g., motion capture systems) or\textemdash even if not quite as costly\textemdash still possess considerable complexity (e.g., wearable sensors). In contrast, a video sufficient for the assessment method presented here can be obtained by anyone, without much training, via a smartphone camera.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Diagnostic markers,Predictive markers,read},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational biology and bioinformatics;Diagnostic markers;Predictive markers Subject\_term\_id: computational-biology-and-bioinformatics;diagnostic-markers;predictive-markers},
  file = {/Users/jcotton81/Zotero/storage/GVHQ8LXV/Azhand et al. - 2021 - Algorithm based on one monocular video delivers hi.pdf;/Users/jcotton81/Zotero/storage/HH7XZ68C/s41598-021-93530-z.html}
}

@article{carse_minimal_2021,
  title = {Minimal Clinically Important Difference in Walking Velocity, Gait Profile Score and Two Minute Walk Test for Individuals with Lower Limb Amputation},
  author = {Carse, Bruce and Scott, Helen and {Davie-Smith}, Fiona and Brady, Laura and Colvin, John},
  year = {2021},
  month = jul,
  journal = {Gait \& Posture},
  volume = {88},
  pages = {221--224},
  issn = {1879-2219},
  doi = {10.1016/j.gaitpost.2021.06.001},
  abstract = {BACKGROUND: Individuals with lower limb amputation are routinely assessed with a variety outcome measures, however there is a lack of published data to indicate minimal clinically important differences (MCID) for many of these outcome measures. Three such important gait-specific outcome measures include walking velocity, gait profile score (GPS) and the two minute walk test (2MWT). RESEARCH QUESTION: Determine the MCIDs for walking velocity, GPS and 2MWT for individuals with lower limb amputation. METHODS: Walking velocity and GPS (n = 60), and 2MWT (n = 119) data for individuals with unilateral transfemoral or knee disarticulation were identified retrospectively from a database held at the study centre. An anchor-based method was used with Medicare functional classification level (MFCL) acting as the impairment-related criterion, and a least-squares linear regression approach was used to calculate the gradient required for a change between MFCL levels. RESULTS: An increase of 0.21 m/s (95 \% CI: 0.13,0.29) for walking velocity, a reduction of 1.7\textdegree{} (95 \% CI: -2.449,-1.097) for GPS and an increase of 37.2 m (95 \% CI: 28.8,45.5) for 2MWT were found to correspond to an increase in MFCL of one level. Walking velocity, GPS and 2MWT correlated with MFCL with R2 values of 0.333, 0.322 and 0.398 respectively (p {$<$} 0.00001). The authors propose that 0.21 m/s for walking velocity, 1.7\textdegree{} for GPS and 37.2 m for 2MWT be used as MCID values for individuals with lower limb amputation. SIGNIFICANCE: The results of this study can be used to help both researchers and clinicians to objectively evaluate if interventions for individuals with lower limb amputation are effective.},
  langid = {english},
  pmid = {34119776},
  keywords = {Aged,Amputation,Amputees,Artificial Limbs,Gait,Gait profile score,Humans,Lower Extremity,Lower limb amputation,Medicare,Minimal clinically important difference,Minimal Clinically Important Difference,Prosthetic,Retrospective Studies,United States,Walk Test,Walking,Walking velocity}
}

@article{cotton_kinematic_2020,
  title = {Kinematic {{Tracking}} of {{Rehabilitation Patients With Markerless Pose Estimation Fused}} with {{Wearable Inertial Sensors}}},
  author = {Cotton, R. James},
  year = {2020},
  journal = {IEEE 15th International Conference on Automatic Face \& Gesture Recognition},
  file = {/Users/jcotton81/Zotero/storage/GZJNYR34/Cotton - 2020 - Kinematic Tracking of Rehabilitation Patients With.pdf}
}

@article{cotton_posepipe_2022,
  title = {{{PosePipe}}: {{Open-Source Human Pose Estimation Pipeline}} for {{Clinical Research}}},
  shorttitle = {{{PosePipe}}},
  author = {Cotton, R. James},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.08792 [cs, q-bio]},
  eprint = {2203.08792},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio}
}

@inproceedings{cotton_transforming_2022,
  title = {Transforming {{Gait}}: {{Video-Based Spatiotemporal Gait Analysis}}},
  shorttitle = {Transforming {{Gait}}},
  booktitle = {2022 44th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} \& {{Biology Society}} ({{EMBC}})},
  author = {Cotton, R. James and McClerklin, Emoonah and Cimorelli, Anthony and Patel, Ankit and Karakostas, Tasos},
  year = {2022},
  month = jul,
  pages = {115--120},
  issn = {2694-0604},
  doi = {10.1109/EMBC48229.2022.9871036},
  abstract = {Human pose estimation from monocular video is a rapidly advancing field that offers great promise to human movement science and rehabilitation. This potential is tempered by the smaller body of work ensuring the outputs are clinically meaningful and properly calibrated. Gait analysis, typically performed in a dedicated lab, produces precise measurements including kinematics and step timing. Using over 7000 monocular video from an instrumented gait analysis lab, we trained a neural network to map 3D joint trajectories and the height of individuals onto interpretable biomechanical outputs including gait cycle timing and sagittal plane joint kinematics and spatiotemporal trajectories. This task specific layer produces accurate estimates of the timing of foot contact and foot off events. After parsing the kinematic outputs into individual gait cycles, it also enables accurate cycle-by-cycle estimates of cadence, step time, double and single support time, walking speed and step length.},
  keywords = {Biomechanics,Computer Science - Computer Vision and Pattern Recognition,Kinematics,Neural networks,Pose estimation,Quantitative Biology - Quantitative Methods,Three-dimensional displays,Timing,Trajectory},
  file = {/Users/jcotton81/Zotero/storage/FPZJFXZV/Transforming_Gait_Video-Based_Spatiotemporal_Gait_Analysis.pdf;/Users/jcotton81/Zotero/storage/K3VQ69E3/Cotton et al. - 2022 - Transforming Gait Video-Based Spatiotemporal Gait.pdf;/Users/jcotton81/Zotero/storage/3LMP5LBQ/2203.html;/Users/jcotton81/Zotero/storage/UJBMVUVA/9871036.html}
}

@inproceedings{cotton_wearable_2019,
  title = {Wearable {{Monitoring}} of {{Joint Angle}} and {{Muscle Activity}}},
  booktitle = {2019 {{IEEE}} 16th {{International Conference}} on {{Rehabilitation Robotics}} ({{ICORR}})},
  author = {Cotton, R. James and Rogers, John},
  year = {2019},
  month = jun,
  volume = {2019},
  pages = {258--263},
  publisher = {{IEEE}},
  issn = {1945-7901},
  doi = {10.1109/ICORR.2019.8779538},
  abstract = {Simultaneous tracking of muscle activity and joint rotation is of significant interest in rehabilitation, but gold-standard methods with optical motion tracking and wireless electromyography recording typically restricts this to the laboratory setting. There has been significant progress using wear-able inertial measurement units (IMUs) for motion tracking, but there are no systems that can easily be deployed to home and provide simultaneous electromyography. We addressed this gap by developing a flexible, wearable, Bluetooth-connected sensor that records both IMU and EMG activity. The sensor runs an efficient quaternion-based complementary filter that estimates the sensor orientation while correcting for estimate drift and constraining magnetometer estimates to only influence heading. The difference in two sensor orientations is used to estimate the joint angle, which can be further improved with joint axis estimation. We demonstrate successful tracking of joint angle and muscle activity in a home environment with just the sensors and a smartphone.},
  isbn = {978-1-72812-755-2},
  pmid = {31374639},
  file = {/Users/jcotton81/Zotero/storage/NGSZKC68/08779538-2.pdf}
}

@misc{gong_posetriplet_2022,
  title = {{{PoseTriplet}}: {{Co-evolving 3D Human Pose Estimation}}, {{Imitation}}, and {{Hallucination}} under {{Self-supervision}}},
  shorttitle = {{{PoseTriplet}}},
  author = {Gong, Kehong and Li, Bingbing and Zhang, Jianfeng and Wang, Tao and Huang, Jing and Mi, Michael Bi and Feng, Jiashi and Wang, Xinchao},
  year = {2022},
  month = mar,
  number = {arXiv:2203.15625},
  eprint = {2203.15625},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Existing self-supervised 3D human pose estimation schemes have largely relied on weak supervisions like consistency loss to guide the learning, which, inevitably, leads to inferior results in real-world scenarios with unseen poses. In this paper, we propose a novel self-supervised approach that allows us to explicitly generate 2D-3D pose pairs for augmenting supervision, through a self-enhancing dual-loop learning framework. This is made possible via introducing a reinforcement-learning-based imitator, which is learned jointly with a pose estimator alongside a pose hallucinator; the three components form two loops during the training process, complementing and strengthening one another. Specifically, the pose estimator transforms an input 2D pose sequence to a low-fidelity 3D output, which is then enhanced by the imitator that enforces physical constraints. The refined 3D poses are subsequently fed to the hallucinator for producing even more diverse data, which are, in turn, strengthened by the imitator and further utilized to train the pose estimator. Such a co-evolution scheme, in practice, enables training a pose estimator on self-generated motion data without relying on any given 3D data. Extensive experiments across various benchmarks demonstrate that our approach yields encouraging results significantly outperforming the state of the art and, in some cases, even on par with results of fully-supervised methods. Notably, it achieves 89.1\% 3D PCK on MPI-INF-3DHP under self-supervised cross-dataset evaluation setup, improving upon the previous best self-supervised methods by 8.6\%. Code can be found at: https://github.com/Garfield-kh/PoseTriplet},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read},
  file = {/Users/jcotton81/Zotero/storage/VC45CK8E/Gong et al. - 2022 - PoseTriplet Co-evolving 3D Human Pose Estimation,.pdf;/Users/jcotton81/Zotero/storage/DUJ9D52H/2203.html}
}

@article{guo_toward_2019,
  title = {Toward {{Fairness}} in {{AI}} for {{People}} with {{Disabilities}}: {{A Research Roadmap}}},
  author = {Guo, Anhong and Kamar, Ece and Vaughan, Jennifer Wortman and Wallach, Hanna and Morris, Meredith Ringel},
  year = {2019},
  month = jul,
  journal = {arXiv},
  publisher = {{arXiv}},
  abstract = {AI technologies have the potential to dramatically impact the lives of people with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for many state-of-the-art AI systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities. However, widely deployed AI systems may not work properly for PWD, or worse, may actively discriminate against them. These considerations regarding fairness in AI for PWD have thus far received little attention. In this position paper, we identify potential areas of concern regarding how several AI technology categories may impact particular disability constituencies if care is not taken in their design, development, and testing. We intend for this risk assessment of how various classes of AI might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms.},
  keywords = {Accessibility,AI bias,AI fairness,Artificial intelligence,Data,Disability,Ethical AI,Inclusion,Machine learning},
  file = {/Users/jcotton81/Zotero/storage/MW8TYMDN/full-text.pdf}
}

@article{hamacher_kinematic_2011,
  title = {Kinematic Measures for Assessing Gait Stability in Elderly Individuals: A Systematic Review},
  shorttitle = {Kinematic Measures for Assessing Gait Stability in Elderly Individuals},
  author = {Hamacher, D. and Singh, N. B. and Van Die{\"e}n, J. H. and Heller, M. O. and Taylor, W. R.},
  year = {2011},
  month = dec,
  journal = {Journal of the Royal Society, Interface},
  volume = {8},
  number = {65},
  pages = {1682--1698},
  issn = {1742-5662},
  doi = {10.1098/rsif.2011.0416},
  abstract = {Falls not only present a considerable health threat, but the resulting treatment and loss of working days also place a heavy economic burden on society. Gait instability is a major fall risk factor, particularly in geriatric patients, and walking is one of the most frequent dynamic activities of daily living. To allow preventive strategies to become effective, it is therefore imperative to identify individuals with an unstable gait. Assessment of dynamic stability and gait variability via biomechanical measures of foot kinematics provides a viable option for quantitative evaluation of gait stability, but the ability of these methods to predict falls has generally not been assessed. Although various methods for assessing gait stability exist, their sensitivity and applicability in a clinical setting, as well as their cost-effectiveness, need verification. The objective of this systematic review was therefore to evaluate the sensitivity of biomechanical measures that quantify gait stability among elderly individuals and to evaluate the cost of measurement instrumentation required for application in a clinical setting. To assess gait stability, a comparative effect size (Cohen's d) analysis of variability and dynamic stability of foot trajectories during level walking was performed on 29 of an initial yield of 9889 articles from four electronic databases. The results of this survey demonstrate that linear variability of temporal measures of swing and stance was most capable of distinguishing between fallers and non-fallers, whereas step width and stride velocity prove more capable of discriminating between old versus young (OY) adults. In addition, while orbital stability measures (Floquet multipliers) applied to gait have been shown to distinguish between both elderly fallers and non-fallers as well as between young and old adults, local stability measures ({$\lambda$}s) have been able to distinguish between young and old adults. Both linear and nonlinear measures of foot time series during gait seem to hold predictive ability in distinguishing healthy from fall-prone elderly adults. In conclusion, biomechanical measurements offer promise for identifying individuals at risk of falling and can be obtained with relatively low-cost tools. Incorporation of the most promising measures in combined retrospective and prospective studies for understanding fall risk and designing preventive strategies is warranted.},
  langid = {english},
  pmcid = {PMC3203491},
  pmid = {21880615},
  keywords = {Accidental Falls,Activities of Daily Living,Aged,Aging,Algorithms,Biomechanical Phenomena,Female,Gait,Humans,Male,Postural Balance,read,Risk,Risk Factors,Self-Help Devices,Walking},
  file = {/Users/jcotton81/Zotero/storage/HXEBRBBF/Hamacher et al. - 2011 - Kinematic measures for assessing gait stability in.pdf}
}

@article{kanko_assessment_2021,
  title = {Assessment of Spatiotemporal Gait Parameters Using a Deep Learning Algorithm-Based Markerless Motion Capture System},
  author = {Kanko, Robert M. and Laende, Elise K. and Strutzenberger, Gerda and Brown, Marcus and Selbie, W. Scott and DePaul, Vincent and Scott, Stephen H. and Deluzio, Kevin J.},
  year = {2021},
  month = jun,
  journal = {Journal of Biomechanics},
  volume = {122},
  pages = {110414},
  issn = {1873-2380},
  doi = {10.1016/j.jbiomech.2021.110414},
  abstract = {Spatiotemporal parameters can characterize the gait patterns of individuals, allowing assessment of their health status and detection of clinically meaningful changes in their gait. Video-based markerless motion capture is a user-friendly, inexpensive, and widely applicable technology that could reduce the barriers to measuring spatiotemporal gait parameters in clinical and more diverse settings. Two studies were performed to determine whether gait parameters measured using markerless motion capture demonstrate concurrent validity with those measured using marker-based motion capture and a pressure-sensitive gait mat. For the first study, thirty healthy young adults performed treadmill gait at self-selected speeds while marker-based motion capture and synchronized video data were recorded simultaneously. For the second study, twenty-five healthy young adults performed over-ground gait at self-selected speeds while footfalls were recorded using a gait mat and synchronized video data were recorded simultaneously. Kinematic heel-strike and toe-off gait events were used to identify the same gait cycles between systems. Nine spatiotemporal gait parameters were measured by each system and directly compared between systems. Measurements were compared using Bland-Altman methods, mean differences, Pearson correlation coefficients, and intraclass correlation coefficients. The results indicate that markerless measurements of spatiotemporal gait parameters have good to excellent agreement with marker-based motion capture and gait mat systems, except for stance time and double limb support time relative to both systems and stride width relative to the gait mat. These findings indicate that markerless motion capture can adequately measure spatiotemporal gait parameters of healthy young adults during treadmill and over-ground gait.},
  langid = {english},
  pmid = {33915475},
  keywords = {Algorithms,Biomechanical Phenomena,Deep learning,Deep Learning,Gait,Gait analysis,Gait mat,Humans,Markerless motion capture,Reproducibility of Results,Spatiotemporal parameters,Walking,Young Adult},
  file = {/Users/jcotton81/Zotero/storage/WG6HPQD9/Kanko et al. - 2021 - Assessment of spatiotemporal gait parameters using.pdf}
}

@misc{kanko_concurrent_2020,
  title = {Concurrent Assessment of Gait Kinematics Using Marker-Based and Markerless Motion Capture},
  author = {Kanko, Robert and Laende, Elise and Davis, Elysia and Selbie, W. Scott and Deluzio, Kevin J.},
  year = {2020},
  month = dec,
  pages = {2020.12.10.420075},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.12.10.420075},
  abstract = {Kinematic analysis is a useful and widespread tool used in research and clinical biomechanics for the estimation of human pose and the quantification of human movement. Common marker-based optical motion capture systems are expensive, time intensive, and require highly trained operators to obtain kinematic data. Markerless motion capture systems offer an alternative method for the measurement of kinematic data with several practical benefits. This work compared the kinematics of human gait measured using a deep learning algorithm-based markerless motion capture system to those of a common marker-based motion capture system. Thirty healthy adult participants walked on a treadmill while data were simultaneously recorded using eight video cameras (markerless) and seven infrared optical motion capture cameras (marker-based). Video data were processed using markerless motion capture software, marker-based data were processed using marker-based capture software, and both sets of data were compared. The average root mean square distance (RMSD) between corresponding joints was less than 3 cm for all joints except the hip, which was 4.1 cm. Lower limb segment angles indicated pose estimates from both systems were very similar, with RMSD of less than 6\textdegree{} for all segment angles except those that represent rotations about the long axis of the segment. Lower limb joint angles captured similar patterns for flexion/extension at all joints, ab/adduction at the knee and hip, and toe-in/toe-out at the ankle. These findings demonstrate markerless motion capture can measure similar 3D kinematics to those from marker-based systems.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/jcotton81/Zotero/storage/UV3INQC9/Kanko et al. - 2020 - Concurrent assessment of gait kinematics using mar.pdf;/Users/jcotton81/Zotero/storage/5348YHRV/2020.12.10.420075v1.html}
}

@article{kanko_inter-session_2021,
  title = {Inter-Session Repeatability of Markerless Motion Capture Gait Kinematics},
  author = {Kanko, Robert M. and Laende, Elise and Selbie, W. Scott and Deluzio, Kevin J.},
  year = {2021},
  month = may,
  journal = {Journal of Biomechanics},
  volume = {121},
  pages = {110422},
  issn = {1873-2380},
  doi = {10.1016/j.jbiomech.2021.110422},
  abstract = {The clinical uptake and influence of gait analysis has been hindered by inherent limitations of marker-based motion capture systems, which have long been the standard method for the collection of gait data including kinematics. Markerless motion capture offers an alternative method for the collection of gait kinematics that presents several practical benefits over marker-based systems. This work aimed to determine the reliability of lower limb gait kinematics from video based markerless motion capture using an established experimental protocol for testing reliability. Eight healthy adult participants performed three sessions of five over-ground walking trials in their own self-selected clothing, separated by an average of 8.5\,days, while eight synchronized and calibrated cameras recorded video. Three-dimensional pose estimates from the video data were used to compute lower limb joint angles. Inter-session variability, inter-trial variability, and the variability ratio were used to assess the reliability of the gait kinematics. Compared to repeatability studies based on marker-based motion capture, inter-trial variability was slightly greater than previously reported for some angles, with an average across all joint angles of 2.5\textdegree. Inter-session variability was smaller on average than all previously reported values, with an average across all joint angles of 2.8\textdegree. Variability ratios were all smaller than those previously reported with an average of 1.1, indicating that the multi-session protocol increased the total variability of joint angles by 10\% of the inter-trial variability. These results indicate that gait kinematics can be reliably measured using markerless motion capture.},
  langid = {english},
  pmid = {33873117},
  keywords = {Adult,Biomechanical Phenomena,Deep learning,Gait,Gait analysis,Humans,Kinematics,Markerless motion capture,Motion,Repeatability,Reproducibility of Results,Walking},
  file = {/Users/jcotton81/Zotero/storage/NRTFYA3T/Kanko et al. - 2021 - Inter-session repeatability of markerless motion c.pdf}
}

@article{kidzinski_deep_2020,
  title = {Deep Neural Networks Enable Quantitative Movement Analysis Using Single-Camera Videos},
  author = {Kidzi{\'n}ski, {\L}ukasz and Yang, Bryan and Hicks, Jennifer L. and Rajagopal, Apoorva and Delp, Scott L. and Schwartz, Michael H.},
  year = {2020},
  month = dec,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {1--10},
  publisher = {{Nature Research}},
  issn = {20411723},
  doi = {10.1038/s41467-020-17807-z},
  abstract = {Many neurological and musculoskeletal diseases impair movement, which limits people's function and social participation. Quantitative assessment of motion is critical to medical decision-making but is currently possible only with expensive motion capture systems and highly trained personnel. Here, we present a method for predicting clinically relevant motion parameters from an ordinary video of a patient. Our machine learning models predict parameters include walking speed (r = 0.73), cadence (r = 0.79), knee flexion angle at maximum extension (r = 0.83), and Gait Deviation Index (GDI), a comprehensive metric of gait impairment (r = 0.75). These correlation values approach the theoretical limits for accuracy imposed by natural variability in these metrics within our patient population. Our methods for quantifying gait pathology with commodity cameras increase access to quantitative motion analysis in clinics and at home and enable researchers to conduct large-scale studies of neurological and musculoskeletal disorders.},
  pmid = {32792511},
  keywords = {Data processing,Diagnostic markers,Machine learning,Movement disorders},
  file = {/Users/jcotton81/Zotero/storage/D9DGLXZG/full-text.pdf}
}

@article{kwolek_calibrated_2019,
  title = {Calibrated and Synchronized Multi-View Video and Motion Capture Dataset for Evaluation of Gait Recognition},
  author = {Kwolek, Bogdan and Michalczuk, Agnieszka and Krzeszowski, Tomasz and Switonski, Adam and Josinski, Henryk and Wojciechowski, Konrad},
  year = {2019},
  month = nov,
  journal = {Multimedia Tools and Applications},
  volume = {78},
  number = {22},
  pages = {32437--32465},
  issn = {1573-7721},
  doi = {10.1007/s11042-019-07945-y},
  abstract = {We introduce synchronized and calibrated multi-view video and motion capture dataset for motion analysis and gait identification. The 3D gait dataset consists of 166 data sequences with 32 people. In 128 data sequences, each of 32 individuals was dressed in his/her clothes, in 24 data sequences, 6 of 32 performers changed clothes, and in 14 data sequences, 7 of the performers had a backpack on his/her back. In a single recording session, every performer walked from right to left, then from left to right, and afterwards on the diagonal from upper-right to bottom-left and from bottom-left to upper-right corner of a rectangular scene. We demonstrate that a baseline algorithm achieves promising results in a challenging scenario, in which gallery/training data were collected in walks perpendicular/facing to the cameras, whereas the probe/testing data were collected in diagonal walks. We compare performances of biometric gait recognition that were achieved on marker-less and marker-based 3D data. We present recognition performances, which were achieved by a convolutional neural network and classic classifiers operating on gait signatures obtained by multilinear principal component analysis. The availability of synchronized multi-view image sequences with 3D locations of body markers creates a number of possibilities for extraction of discriminative gait signatures. The gait data are available at http://bytom.pja.edu.pl/projekty/hm-gpjatk/.},
  langid = {english},
  keywords = {Biometrics,Covariate factors,Gait recognition,Markerless 3D tracking},
  file = {/Users/jcotton81/Zotero/storage/NRGGNCJQ/Kwolek et al. - 2019 - Calibrated and synchronized multi-view video and m.pdf}
}

@inproceedings{lin_microsoft_2014,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {740--755},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-10602-1_48},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  isbn = {978-3-319-10602-1},
  langid = {english},
  keywords = {Common Object,Object Category,Object Detection,Object Instance,Scene Understanding},
  file = {/Users/jcotton81/Zotero/storage/FNKJSPGV/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf}
}

@article{liu_graph_2020,
  title = {A {{Graph Attention Spatio-temporal Convolutional Network}} for {{3D Human Pose Estimation}} in {{Video}}},
  author = {Liu, Junfa and Rojas, Juan and Liang, Zhijun and Li, Yihui and Guan, Yisheng},
  year = {2020},
  month = mar,
  abstract = {Spatio-temporal information is key to resolve occlusion and depth ambiguity in 3D pose estimation. Previous methods have focused on either temporal contexts or local-to-global architectures that embed fixed-length spatio-temporal information. To date, there have not been effective proposals to simultaneously and flexibly capture varying spatio-temporal sequences and effectively achieves real-time 3D pose estimation. In this work, we improve the learning of kinematic constraints in the human skeleton: posture, local kinematic connections, and symmetry by modeling local and global spatial information via attention mechanisms. To adapt to single- and multi-frame estimation, the dilated temporal model is employed to process varying skeleton sequences. Also, importantly, we carefully design the interleaving of spatial semantics with temporal dependencies to achieve a synergistic effect. To this end, we propose a simple yet effective graph attention spatio-temporal convolutional network (GAST-Net) that comprises of interleaved temporal convolutional and graph attention blocks. Experiments on two challenging benchmark datasets (Human3.6M and HumanEva-I) and YouTube videos demonstrate that our approach effectively mitigates depth ambiguity and self-occlusion, generalizes to half upper body estimation, and achieves competitive performance on 2D-to-3D video pose estimation. Code, video, and supplementary information is available at: \textbackslash href\{http://www.juanrojas.net/gast/\}\{http://www.juanrojas.net/gast/\}},
  langid = {english},
  keywords = {read},
  file = {/Users/jcotton81/Zotero/storage/5DWKRP3A/full-text.pdf;/Users/jcotton81/Zotero/storage/QQ5RT3H2/Liu et al. - 2020 - A Graph Attention Spatio-temporal Convolutional Ne.pdf;/Users/jcotton81/Zotero/storage/Y6WEB3C5/2003.html}
}

@article{liu_recent_2022,
  title = {Recent {{Advances}} of {{Monocular 2D}} and {{3D Human Pose Estimation}}: {{A Deep Learning Perspective}}},
  shorttitle = {Recent {{Advances}} of {{Monocular 2D}} and {{3D Human Pose Estimation}}},
  author = {Liu, Wu and Mei, Tao},
  year = {2022},
  month = mar,
  journal = {ACM Computing Surveys},
  pages = {3524497},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3524497},
  abstract = {Estimation of the human pose from a monocular camera has been an emerging research topic in the computer vision community with many applications. Recently, benefiting from the deep learning technologies, a significant amount of research efforts have advanced the monocular human pose estimation both in 2D and 3D areas. Although there have been some works to summarize different approaches, it still remains challenging for researchers to have an in-depth view of how these approaches work from 2D to 3D. In this paper, we provide a comprehensive and holistic 2D-to-3D perspective to tackle this problem. Firstly, we comprehensively summarize the 2D and 3D representations of human body. Then we summarize the mainstream and milestone approaches for these human body presentations since the year 2014 under unified frameworks. Especially, we provide insightful analyses for the intrinsic connections and methods evolution from 2D to 3D pose estimation. Furthermore, we analyze the solutions for challenging cases, such as the lack of data, the inherent ambiguity between 2D and 3D, and the complex multi-person scenarios. Next, we summarize the benchmarks, evaluation metrics, and the quantitative performance of popular approaches. Finally, we discuss the challenges and give deep thinking of promising directions for future research. We believe this survey will provide the readers (researchers, engineers, developers, etc.) with a deep and insightful understanding of monocular human pose estimation. CCS Concepts: \textbullet{} General and Reference \textrightarrow{} Surveys and Overviews; \textbullet{} Computing Methodologies \textrightarrow{} Artificial Intelligence - Computer Vision.},
  langid = {english},
  file = {/Users/jcotton81/Zotero/storage/LNGY2EY3/Liu and Mei - 2022 - Recent Advances of Monocular 2D and 3D Human Pose .pdf}
}

@article{lonini_video-based_2022,
  title = {Video-{{Based Pose Estimation}} for {{Gait Analysis}} in {{Stroke Survivors}} during {{Clinical Assessments}}: {{A Proof-of-Concept Study}}},
  shorttitle = {Video-{{Based Pose Estimation}} for {{Gait Analysis}} in {{Stroke Survivors}} during {{Clinical Assessments}}},
  author = {Lonini, Luca and Moon, Yaejin and Embry, Kyle and Cotton, R. James and McKenzie, Kelly and Jenz, Sophia and Jayaraman, Arun},
  year = {2022},
  journal = {Digital Biomarkers},
  volume = {6},
  number = {1},
  pages = {9--18},
  publisher = {{Karger Publishers}},
  issn = {, 2504-110X},
  doi = {10.1159/000520732},
  abstract = {Recent advancements in deep learning have produced significant progress in markerless human pose estimation, making it possible to estimate human kinematics from single camera videos without the need for reflective markers and specialized labs equipped with motion capture systems. Such algorithms have the potential to enable the quantification of clinical metrics from videos recorded with a handheld camera. Here we used DeepLabCut, an open-source framework for markerless pose estimation, to fine-tune a deep network to track 5 body keypoints (hip, knee, ankle, heel, and toe) in 82 below-waist videos of 8 patients with stroke performing overground walking during clinical assessments. We trained the pose estimation model by labeling the keypoints in 2 frames per video and then trained a convolutional neural network to estimate 5 clinically relevant gait parameters (cadence, double support time, swing time, stance time, and walking speed) from the trajectory of these keypoints. These results were then compared to those obtained from a clinical system for gait analysis (GAITRite\textregistered, CIR Systems). Absolute accuracy (mean error) and precision (standard deviation of error) for swing, stance, and double support time were within 0.04 {$\pm$} 0.11 s; Pearson's correlation with the reference system was moderate for swing times (\emph{r} = 0.4\textendash 0.66), but stronger for stance and double support time (\emph{r} = 0.93\textendash 0.95). Cadence mean error was -0.25 steps/min {$\pm$} 3.9 steps/min (\emph{r} = 0.97), while walking speed mean error was -0.02 {$\pm$} 0.11 m/s (\emph{r} = 0.92). These preliminary results suggest that single camera videos and pose estimation models based on deep networks could be used to quantify clinically relevant gait metrics in individuals poststroke, even while using assistive devices in uncontrolled environments. Such development opens the door to applications for gait analysis both inside and outside of clinical settings, without the need of sophisticated equipment.},
  langid = {english},
  file = {/Users/jcotton81/Zotero/storage/ECGP72YP/Lonini et al. - 2022 - Video-Based Pose Estimation for Gait Analysis in S.pdf;/Users/jcotton81/Zotero/storage/KM4CCX5H/520732.html}
}

@article{mathis_deeplabcut:_2018,
  title = {{{DeepLabCut}}: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning},
  author = {Mathis, Alexander and Mamidanna, Pranav and Cury, Kevin M. and Abe, Taiga and Murthy, Venkatesh N. and Mathis, Mackenzie Weygandt and Bethge, Matthias},
  year = {2018},
  journal = {Nature Neuroscience 2018},
  pages = {1},
  issn = {1097-6256},
  doi = {10.1038/s41593-018-0209-y},
  abstract = {Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, but markers are intrusive, and the number and location of the markers must be determined a priori. Here we present an efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors. Remarkably, even when only a small number of frames are labeled (\textasciitilde 200), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy.}
}

@article{mcguirk_feasibility_2022,
  title = {Feasibility of {{Markerless Motion Capture}} for {{Three-Dimensional Gait Assessment}} in {{Community Settings}}},
  author = {McGuirk, Theresa E. and Perry, Elliott S. and Sihanath, Wandasun B. and Riazati, Sherveen and Patten, Carolynn},
  year = {2022},
  journal = {Frontiers in Human Neuroscience},
  volume = {16},
  issn = {1662-5161},
  abstract = {Three-dimensional (3D) kinematic analysis of gait holds potential as a digital biomarker to identify neuropathologies, monitor disease progression, and provide a high-resolution outcome measure to monitor neurorehabilitation efficacy by characterizing the mechanisms underlying gait impairments. There is a need for 3D motion capture technologies accessible to community, clinical, and rehabilitation settings. Image-based markerless motion capture (MLMC) using neural network-based deep learning algorithms shows promise as an accessible technology in these settings. In this study, we assessed the feasibility of implementing 3D MLMC technology outside the traditional laboratory environment to evaluate its potential as a tool for outcomes assessment in neurorehabilitation. A sample population of 166 individuals aged 9\textendash 87 years (mean 43.7, S.D. 20.4) of varied health history were evaluated at six different locations in the community over a 3-month period. Participants walked overground at self-selected (SS) and fastest comfortable (FC) speeds. Feasibility measures considered the expansion, implementation, and practicality of this MLMC system. A subset of the sample population (46 individuals) walked over a pressure-sensitive walkway (PSW) concurrently with MLMC to assess agreement of the spatiotemporal gait parameters measured between the two systems. Twelve spatiotemporal parameters were compared using mean differences, Bland-Altman analysis, and intraclass correlation coefficients for agreement (ICC2,1) and consistency (ICC3,1). All measures showed good to excellent agreement between MLMC and the PSW system with cadence, speed, step length, step time, stride length, and stride time showing strong similarity. Furthermore, this information can inform the development of rehabilitation strategies targeting gait dysfunction. These first experiments provide evidence for feasibility of using MLMC in community and clinical practice environments to acquire robust 3D kinematic data from a diverse population. This foundational work enables future investigation with MLMC especially its use as a digital biomarker of disease progression and rehabilitation outcome.},
  file = {/Users/jcotton81/Zotero/storage/6MCHMDJF/McGuirk et al. - 2022 - Feasibility of Markerless Motion Capture for Three.pdf}
}

@misc{mmpose_contributors_openmmlab_2020,
  title = {{{OpenMMLab Pose Estimation Toolbox}} and {{Benchmark}}},
  author = {MMPose Contributors},
  year = {2020}
}

@article{moore_core_2018,
  title = {A {{Core Set}} of {{Outcome Measures}} for {{Adults With Neurologic Conditions Undergoing Rehabilitation}}: {{A CLINICAL PRACTICE GUIDELINE}}},
  shorttitle = {A {{Core Set}} of {{Outcome Measures}} for {{Adults With Neurologic Conditions Undergoing Rehabilitation}}},
  author = {Moore, Jennifer L. and Potter, Kirsten and Blankshain, Kathleen and Kaplan, Sandra L. and O'Dwyer, Linda C. and Sullivan, Jane E.},
  year = {2018},
  month = jul,
  journal = {Journal of Neurologic Physical Therapy},
  volume = {42},
  number = {3},
  pages = {174--220},
  issn = {1557-0576},
  doi = {10.1097/NPT.0000000000000229},
  abstract = {Background:~         Use of outcome measures (OMs) in adult neurologic physical therapy is essential for monitoring changes in a patient's status over time, quantifying observations and patient-reported function, enhancing communication, and increasing the efficiency of patient care. OMs also provide a mechanism to compare patient and organizational outcomes, examine intervention effectiveness, and generate new knowledge. This clinical practice guideline (CPG) examined the literature related to OMs of balance, gait, transfers, and patient-stated goals to identify a core set of OMs for use across adults with neurologic conditions and practice settings.         Methods:~         To determine the scope of this CPG, surveys were conducted to assess the needs and priorities of consumers and physical therapists. OMs were identified through recommendations of the Academy of Neurologic Physical Therapy's Evidence Database to Guide Effectiveness task forces. A systematic review of the literature on the OMs was conducted and additional OMs were identified; the literature search was repeated on these measures. Articles meeting the inclusion criteria were critically appraised by 2 reviewers using a modified version of the COnsensus-based Standards for the selection of health Measurement INstruments. (COSMIN) checklist. Methodological quality and the strength of statistical results were determined. To be recommended for the core set, the OMs needed to demonstrate excellent psychometric properties in high-quality studies across neurologic conditions.         Results/Discussion:~         Based on survey results, the CPG focuses on OMs that have acceptable clinical utility and can be used to assess change over time in a patient's balance, gait, transfers, and patient-stated goals. Strong, level I evidence supports the use of the Berg Balance Scale to assess changes in static and dynamic sitting and standing balance and the Activities-specific Balance Confidence Scale to assess changes in balance confidence. Strong to moderate evidence supports the use of the Functional Gait Assessment to assess changes in dynamic balance while walking, the 10 meter Walk Test to assess changes in gait speed, and the 6-Minute Walk Test to assess changes in walking distance. Best practice evidence supports the use of the 5 Times Sit-to-Stand to assess sit to standing transfers. Evidence was insufficient to support use of a specific OM to assess patient-stated goals across adult neurologic conditions. Physical therapists should discuss the OM results with patients and collaboratively decide how the results should inform the plan of care.         Disclaimer:~         The recommendations included in this CPG are intended as a guide for clinicians, patients, educators, and researchers to improve rehabilitation care and its impact on adults with neurologic conditions. The contents of this CPG were developed with support from the APTA and the Academy of Neurologic Physical Therapy (ANPT). The Guideline Development Group (GDG) used a rigorous review process and was able to freely express its findings and recommendations without influence from the APTA or the ANPT. The authors declare no competing interest.         Video Abstract available for more insights from the authors (see Video, Supplemental Digital Content 1, available at: https://links.lww.com/JNPT/A214.},
  langid = {american},
  file = {/Users/jcotton81/Zotero/storage/3K63WS47/Moore et al. - 2018 - A Core Set of Outcome Measures for Adults With Neu.pdf;/Users/jcotton81/Zotero/storage/ECUXP8QJ/A_Core_Set_of_Outcome_Measures_for_Adults_With.10.html}
}

@article{muhammad_review_2022,
  title = {A Review of {{3D}} Human Body Pose Estimation and Mesh Recovery},
  author = {Muhammad, Zaka-Ud-Din and Huang, Zhangjin and Khan, Rashid},
  year = {2022},
  month = aug,
  journal = {Digital Signal Processing},
  volume = {128},
  pages = {103628},
  issn = {1051-2004},
  doi = {10.1016/j.dsp.2022.103628},
  abstract = {3D human body pose estimation and mesh recovery refer to the approximation of body parts and joint locations and their recovery into a 3D model to visualize the characteristics of the target object from input imaging data. Recent advancements in body pose, shape estimation, and corresponding mesh reconstruction of a body in motion in the real-world environment, specifically from the single monocular imaging resources have attracted huge attraction for daily life applications. Overall, this is a difficult task due to various reasons, including size variations in the complex structure of the human body and data availability for training a model to meet the application's criteria. We surveyed the most recent (2018-2021) CVPR, ICCV, and ECCV state-of-the-art published articles to highlight the importance of real-world applications and recent developments in this area of study. We focus on the development approaches (parametric (template-based) and non-parametric (template-free)), output (Naked, Clothed, and Body in Motion Mesh), used datasets, evaluation metrics, and performance comparisons from referenced articles.},
  langid = {english},
  keywords = {3D human body,Human mesh recovery,Human motion recovery,Human shape estimation,Pose estimation},
  file = {/Users/jcotton81/Zotero/storage/7D3PGULW/S1051200422002457.html}
}

@article{needham_development_2022,
  title = {The {{Development}} and {{Evaluation}} of a {{Fully Automated Markerless Motion Capture Workflow}}},
  author = {Needham, Laurie and Evans, Murray and Wade, Logan and Cosker, Darren P. and Polly McGuigan, M. and Bilzon, James L. and Colyer, Steffi L.},
  year = {2022},
  month = oct,
  journal = {Journal of Biomechanics},
  pages = {111338},
  issn = {0021-9290},
  doi = {10.1016/j.jbiomech.2022.111338},
  abstract = {This study presented a fully automated deep learning based markerless motion capture workflow and evaluated its performance against marker-based motion capture during overground running, walking and counter movement jumping. Multi-view high speed (200 Hz) image data were collected concurrently with marker-based motion capture (criterion data), permitting a direct comparison between methods. Lower limb kinematic data for 15 participants were computed using 2D pose estimation, our 3D fusion process and OpenSim based inverse kinematics modelling. Results demonstrated high levels of agreement for lower limb joint angles, with mean differences ranging ``0.1\textdegree{} - 10.5\textdegree{} for hip (3 DoF) joint rotations, and 0.7\textdegree{} - 3.9\textdegree{} for knee (1 DoF) and ankle (2 DoF) rotations.. These differences generally fall within the documented uncertainties of marker-based motion capture, suggesting that our markerless approach could be used for appropriate biomechanics applications. We used an open-source, modular and customisable workflow, allowing for integration with other popular biomechanics tools such as OpenSim. By developing open-source tools, we hope to facilitate the democratisation of markerless motion capture technology and encourage the transparent development of markerless methods. This presents exciting opportunities for biomechanics researchers and practitioners to capture large amounts of high quality, ecologically valid data both in the laboratory and in the wild.},
  langid = {english},
  keywords = {Biomechanics,Computer vision,Deep learning,Inverse kinematics,Pose estimation,read,Validation},
  file = {/Users/jcotton81/Zotero/storage/MF75WLMC/1-s2.0-S0021929022003797-main.pdf;/Users/jcotton81/Zotero/storage/ERJC6G9U/S0021929022003797.html}
}

@article{noauthor_imported_nodate,
  title = {Imported from {{https://www.nejm.org/doi/full/10.1056/NEJMoa1803588}}},
  doi = {10.1056/NEJM_2018.379.ISSUE-13;PAGEGROUP:STRING:PUBLICATION}
}

@article{pagnon_pose2sim_2021,
  title = {{{Pose2Sim}}: {{An End-to-End Workflow}} for {{3D Markerless Sports Kinematics}}\textemdash{{Part}} 1: {{Robustness}}},
  shorttitle = {{{Pose2Sim}}},
  author = {Pagnon, David and Domalain, Mathieu and Reveret, Lionel},
  year = {2021},
  month = sep,
  journal = {Sensors},
  volume = {21},
  number = {19},
  pages = {6530},
  issn = {1424-8220},
  doi = {10.3390/s21196530},
  abstract = {Being able to capture relevant information about elite athletes' movement ``in the wild'' is challenging, especially because reference marker-based approaches hinder natural movement and are highly sensitive to environmental conditions. We propose Pose2Sim, a markerless kinematics workflow that uses OpenPose 2D pose detections from multiple views as inputs, identifies the person of interest, robustly triangulates joint coordinates from calibrated cameras, and feeds those to a 3D inverse kinematic full-body OpenSim model in order to compute biomechanically congruent joint angles. We assessed the robustness of this workflow when facing simulated challenging conditions: (Im) degrades image quality (11-pixel Gaussian blur and 0.5 gamma compression); (4c) uses few cameras (4 vs. 8); and (Cal) introduces calibration errors (1 cm vs. perfect calibration). Three physical activities were investigated: walking, running, and cycling. When averaged over all joint angles, stride-to-stride standard deviations lay between 1.7{$\smwhtcircle$} and 3.2{$\smwhtcircle$} for all conditions and tasks, and mean absolute errors (compared to the reference condition\textemdash Ref) ranged between 0.35{$\smwhtcircle$} and 1.6{$\smwhtcircle$}. For walking, errors in the sagittal plane were: 1.5{$\smwhtcircle$}, 0.90{$\smwhtcircle$}, 0.19{$\smwhtcircle$} for (Im), (4c), and (Cal), respectively. In conclusion, Pose2Sim provides a simple and robust markerless kinematics analysis from a network of calibrated cameras.},
  langid = {english},
  keywords = {to read},
  file = {/Users/jcotton81/Zotero/storage/V9QJYIXF/Pagnon et al. - 2021 - Pose2Sim An End-to-End Workflow for 3D Markerless.pdf}
}

@article{perera_meaningful_2006,
  title = {Meaningful {{Change}} and {{Responsiveness}} in {{Common Physical Performance Measures}} in {{Older Adults}}},
  shorttitle = {Meaningful {{Change}} and {{Responsiveness}} in {{Common Physical Performance Measures}} in {{Older Adults}}},
  author = {Perera, Subashan and Mody, Samir H. and Woodman, Richard C. and Studenski, Stephanie A.},
  year = {2006},
  month = may,
  journal = {Journal of the American Geriatrics Society},
  volume = {54},
  number = {5},
  pages = {743--749},
  issn = {00028614, 15325415},
  doi = {10.1111/j.1532-5415.2006.00701.x},
  abstract = {OBJECTIVES: To estimate the magnitude of small meaningful and substantial individual change in physical performance measures and evaluate their responsiveness. DESIGN: Secondary data analyses using distribution- and anchor-based methods to determine meaningful change. SETTING: Secondary analysis of data from an observational study and clinical trials of community-dwelling older people and subacute stroke survivors. PARTICIPANTS: Older adults with mobility disabilities in a strength training trial (n 5 100), subacute stroke survivors in an intervention trial (n 5 100), and a prospective cohort of community-dwelling older people (n 5 492). MEASUREMENTS: Gait speed, Short Physical Performance Battery (SPPB), 6-minute-walk distance (6MWD), and self-reported mobility. RESULTS: Most small meaningful change estimates ranged from 0.04 to 0.06 m/s for gait speed, 0.27 to 0.55 points for SPPB, and 19 to 22 m for 6MWD. Most substantial change estimates ranged from 0.08 to 0.14 m/s for gait speed, 0.99 to 1.34 points for SPPB, and 47 to 49 m for 6MWD. Based on responsiveness indices, per-group sample sizes for clinical trials ranged from 13 to 42 for substantial change and 71 to 161 for small meaningful change. CONCLUSION: Best initial estimates of small meaningful change are near 0.05 m/s for gait speed, 0.5 points for SPPB, and 20 m for 6MWD and of substantial change are near 0.10 m/s for gait speed, 1.0 point for SPPB, and 50 m for 6MWD. For clinical use, substantial change in these measures and small change in gait speed and 6MWD, but not SPPB, are detectable. For research use, these measures yield feasible sample sizes for detecting meaningful change. J Am Geriatr Soc 54:743\textendash 749, 2006.},
  langid = {english},
  file = {/Users/jcotton81/Zotero/storage/3LNA84H8/Perera et al. - 2006 - Meaningful Change and Responsiveness in Common Phy.pdf}
}

@article{picerno_25_2017,
  title = {25 Years of Lower Limb Joint Kinematics by Using Inertial and Magnetic Sensors: {{A}} Review of Methodological Approaches},
  author = {Picerno, Pietro},
  year = {2017},
  month = jan,
  journal = {Gait and Posture},
  volume = {51},
  pages = {239--246},
  publisher = {{Elsevier B.V.}},
  issn = {18792219},
  doi = {10.1016/j.gaitpost.2016.11.008},
  abstract = {Joint kinematics is typically limited to the laboratory environment, and the restricted volume of capture may vitiate the execution of the motor tasks under analysis. Conversely, clinicians often require the analysis of motor acts in non-standard environments and for long periods of time, such as in ambulatory settings or during daily life activities. The miniaturisation of motion sensors and electronic components, generally associated with wireless communications technology, has opened up a new perspective: movement analysis can be carried out outside the laboratory and at a relatively lower cost. Wearable inertial measurement units (embedding 3D accelerometers and gyroscopes), eventually associated with magnetometers, allow one to estimate segment orientation and joint angular kinematics by exploiting the laws governing the motion of a rotating rigid body. The first study which formalised the problem of the estimate of joint kinematics using inertial sensors dates back to 1990. Since then, a variety of methods have been presented over the past 25 years for the estimate of 2D and 3D joint kinematics by using inertial and magnetic sensors. The aim of the present review is to describe these approaches from a purely methodological point of view to provide the reader with a comprehensive understanding of all the instrumental, computational and methodological issues related to the estimate of joint kinematics when using such sensor technology.},
  keywords = {Accelerometers,Gyroscopes,Joint kinematics,Methodological approach,Wearable inertial sensors},
  file = {/Users/jcotton81/Zotero/storage/ABU99D4B/1-s2.0-S0966636216306373-main (1).pdf}
}

@misc{pierzchlewicz_multi-hypothesis_2022,
  title = {Multi-Hypothesis {{3D}} Human Pose Estimation Metrics Favor Miscalibrated Distributions},
  author = {Pierzchlewicz, Pawe{\l} A. and Cotton, R. James and Bashiri, Mohammad and Sinz, Fabian H.},
  year = {2022},
  month = oct,
  number = {arXiv:2210.11179},
  eprint = {2210.11179},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.11179},
  abstract = {Due to depth ambiguities and occlusions, lifting 2D poses to 3D is a highly ill-posed problem. Well-calibrated distributions of possible poses can make these ambiguities explicit and preserve the resulting uncertainty for downstream tasks. This study shows that previous attempts, which account for these ambiguities via multiple hypotheses generation, produce miscalibrated distributions. We identify that miscalibration can be attributed to the use of sample-based metrics such as minMPJPE. In a series of simulations, we show that minimizing minMPJPE, as commonly done, should converge to the correct mean prediction. However, it fails to correctly capture the uncertainty, thus resulting in a miscalibrated distribution. To mitigate this problem, we propose an accurate and well-calibrated model called Conditional Graph Normalizing Flow (cGNFs). Our model is structured such that a single cGNF can estimate both conditional and marginal densities within the same model - effectively solving a zero-shot density estimation problem. We evaluate cGNF on the Human\textasciitilde 3.6M dataset and show that cGNF provides a well-calibrated distribution estimate while being close to state-of-the-art in terms of overall minMPJPE. Furthermore, cGNF outperforms previous methods on occluded joints while it remains well-calibrated.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/jcotton81/Zotero/storage/JDFFSP6Z/Pierzchlewicz et al. - 2022 - Multi-hypothesis 3D human pose estimation metrics .pdf;/Users/jcotton81/Zotero/storage/YLFWJ3HI/2210.html}
}

@article{prasanth_wearable_2021,
  title = {Wearable {{Sensor-Based Real-Time Gait Detection}}: {{A Systematic Review}}},
  shorttitle = {Wearable {{Sensor-Based Real-Time Gait Detection}}},
  author = {Prasanth, Hari and Caban, Miroslav and Keller, Urs and Courtine, Gr{\'e}goire and Ijspeert, Auke and Vallery, Heike and {von Zitzewitz}, Joachim},
  year = {2021},
  month = apr,
  journal = {Sensors (Basel, Switzerland)},
  volume = {21},
  number = {8},
  pages = {2727},
  issn = {1424-8220},
  doi = {10.3390/s21082727},
  abstract = {Gait analysis has traditionally been carried out in a laboratory environment using expensive equipment, but, recently, reliable, affordable, and wearable sensors have enabled integration into clinical applications as well as use during activities of daily living. Real-time gait analysis is key to the development of gait rehabilitation techniques and assistive devices such as neuroprostheses. This article presents a systematic review of wearable sensors and techniques used in real-time gait analysis, and their application to pathological gait. From four major scientific databases, we identified 1262 articles of which 113 were analyzed in full-text. We found that heel strike and toe off are the most sought-after gait events. Inertial measurement units (IMU) are the most widely used wearable sensors and the shank and foot are the preferred placements. Insole pressure sensors are the most common sensors for ground-truth validation for IMU-based gait detection. Rule-based techniques relying on threshold or peak detection are the most widely used gait detection method. The heterogeneity of evaluation criteria prevented quantitative performance comparison of all methods. Although most studies predicted that the proposed methods would work on pathological gait, less than one third were validated on such data. Clinical applications of gait detection algorithms were considered, and we recommend a combination of IMU and rule-based methods as an optimal solution.},
  langid = {english},
  pmcid = {PMC8069962},
  pmid = {33924403},
  keywords = {Activities of Daily Living,assistive device,Biomechanical Phenomena,Gait,gait analysis,Gait Analysis,gait rehabilitation,Humans,inertial measurement unit,insole pressure sensors,pathological gait,real-time gait detection,Wearable Electronic Devices,wearable sensor},
  file = {/Users/jcotton81/Zotero/storage/FGVETSDV/Prasanth et al. - 2021 - Wearable Sensor-Based Real-Time Gait Detection A .pdf}
}

@article{rast_systematic_2020,
  title = {Systematic Review on the Application of Wearable Inertial Sensors to Quantify Everyday Life Motor Activity in People with Mobility Impairments},
  author = {Rast, Fabian Marcel and Labruy{\`e}re, Rob},
  year = {2020},
  month = nov,
  journal = {Journal of NeuroEngineering and Rehabilitation},
  volume = {17},
  number = {1},
  pages = {148},
  issn = {1743-0003},
  doi = {10.1186/s12984-020-00779-y},
  abstract = {Recent advances in wearable sensor technologies enable objective and long-term monitoring of motor activities in a patient's habitual environment. People with mobility impairments require appropriate data processing algorithms that deal with their altered movement patterns and determine clinically meaningful outcome measures. Over the years, a large variety of algorithms have been published and this review provides an overview of their outcome measures, the concepts of the algorithms, the type and placement of required sensors as well as the investigated patient populations and measurement properties.},
  keywords = {Accelerometer,Activities of daily living,Algorithms,Disabled persons,Gyroscope,Inertial measurement unit,Machine learning,Patients,Pattern recognition,Rehabilitation},
  file = {/Users/jcotton81/Zotero/storage/4D6ILITE/Rast and Labruyre - 2020 - Systematic review on the application of wearable i.pdf;/Users/jcotton81/Zotero/storage/3NL2B634/s12984-020-00779-y.html}
}

@book{richard_whittles_2012,
  title = {Whittle's {{Gait Analysis}} - 5th {{Edition}}},
  author = {Richard, Jim and Levine, David and Whittle, Michael},
  year = {2012},
  publisher = {{Elsevier}},
  file = {/Users/jcotton81/Zotero/storage/JNE2LSXZ/978-0-7020-4265-2.html}
}

@article{shi_motionet_2020,
  title = {{{MotioNet}}: {{3D Human Motion Reconstruction}} from {{Monocular Video}} with {{Skeleton Consistency}}},
  author = {Shi, Mingyi and Aberman, Kfir and Aristidou, Andreas and Komura, Taku and Lischinski, Dani and {Cohen-Or}, Daniel and Chen, Baoquan},
  year = {2020},
  month = jun,
  journal = {ACM Transactions on Graphics},
  volume = {40},
  number = {1},
  publisher = {{Association for Computing Machinery}},
  abstract = {We introduce MotioNet, a deep neural network that directly reconstructs the motion of a 3D human skeleton from monocular video.While previous methods rely on either rigging or inverse kinematics (IK) to associate a consistent skeleton with temporally coherent joint rotations, our method is the first data-driven approach that directly outputs a kinematic skeleton, which is a complete, commonly used, motion representation. At the crux of our approach lies a deep neural network with embedded kinematic priors, which decomposes sequences of 2D joint positions into two separate attributes: a single, symmetric, skeleton, encoded by bone lengths, and a sequence of 3D joint rotations associated with global root positions and foot contact labels. These attributes are fed into an integrated forward kinematics (FK) layer that outputs 3D positions, which are compared to a ground truth. In addition, an adversarial loss is applied to the velocities of the recovered rotations, to ensure that they lie on the manifold of natural joint rotations. The key advantage of our approach is that it learns to infer natural joint rotations directly from the training data, rather than assuming an underlying model, or inferring them from joint positions using a data-agnostic IK solver. We show that enforcing a single consistent skeleton along with temporally coherent joint rotations constrains the solution space, leading to a more robust handling of self-occlusions and depth ambiguities.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,I.4.5,motion analysis,motion capturing,Pose estimation},
  file = {/Users/jcotton81/Zotero/storage/DTQQ6E2V/full-text.pdf;/Users/jcotton81/Zotero/storage/I4R4SU2K/Shi et al. - MotioNet 3D Human Motion Reconstruction from Mono.pdf;/Users/jcotton81/Zotero/storage/Q2KT6MBS/Shi et al. - 2020 - MotioNet 3D Human Motion Reconstruction from Mono.pdf;/Users/jcotton81/Zotero/storage/EVB6IJVS/2006.html}
}

@article{shimada_physcap_2020,
  title = {{{PhysCap}}: {{Physically Plausible Monocular 3D Motion Capture}} in {{Real Time}}},
  shorttitle = {{{PhysCap}}},
  author = {Shimada, Soshi and Golyanik, Vladislav and Xu, Weipeng and Theobalt, Christian},
  year = {2020},
  month = dec,
  journal = {arXiv:2008.08880 [cs]},
  eprint = {2008.08880},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Marker-less 3D human motion capture from a single colour camera has seen significant progress. However, it is a very challenging and severely ill-posed problem. In consequence, even the most accurate state-of-the-art approaches have significant limitations. Purely kinematic formulations on the basis of individual joints or skeletons, and the frequent frame-wise reconstruction in state-of-the-art methods greatly limit 3D accuracy and temporal stability compared to multi-view or marker-based motion capture. Further, captured 3D poses are often physically incorrect and biomechanically implausible, or exhibit implausible environment interactions (floor penetration, foot skating, unnatural body leaning and strong shifting in depth), which is problematic for any use case in computer graphics. We, therefore, present PhysCap, the first algorithm for physically plausible, real-time and marker-less human 3D motion capture with a single colour camera at 25 fps. Our algorithm first captures 3D human poses purely kinematically. To this end, a CNN infers 2D and 3D joint positions, and subsequently, an inverse kinematics step finds space-time coherent joint angles and global 3D pose. Next, these kinematic reconstructions are used as constraints in a real-time physics-based pose optimiser that accounts for environment constraints (e.g., collision handling and floor placement), gravity, and biophysical plausibility of human postures. Our approach employs a combination of ground reaction force and residual force for plausible root control, and uses a trained neural network to detect foot contact events in images. Our method captures physically plausible and temporally stable global 3D human motion, without physically implausible postures, floor penetrations or foot skating, from video in real time and in general scenes. The video is available at http://gvv.mpi-inf.mpg.de/projects/PhysCap},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/jcotton81/Zotero/storage/3UU4X9SK/Shimada et al. - 2020 - PhysCap Physically Plausible Monocular 3D Motion .pdf;/Users/jcotton81/Zotero/storage/VMLSC9BS/2008.html}
}

@article{stenum_two-dimensional_2021,
  title = {Two-Dimensional Video-Based Analysis of Human Gait Using Pose Estimation},
  author = {Stenum, Jan and Rossi, Cristina and Roemmich, Ryan T.},
  year = {2021},
  month = apr,
  journal = {PLOS Computational Biology},
  volume = {17},
  number = {4},
  pages = {e1008935},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008935},
  abstract = {Human gait analysis is often conducted in clinical and basic research, but many common approaches (e.g., three-dimensional motion capture, wearables) are expensive, immobile, data-limited, and require expertise. Recent advances in video-based pose estimation suggest potential for gait analysis using two-dimensional video collected from readily accessible devices (e.g., smartphones). To date, several studies have extracted features of human gait using markerless pose estimation. However, we currently lack evaluation of video-based approaches using a dataset of human gait for a wide range of gait parameters on a stride-by-stride basis and a workflow for performing gait analysis from video. Here, we compared spatiotemporal and sagittal kinematic gait parameters measured with OpenPose (open-source video-based human pose estimation) against simultaneously recorded three-dimensional motion capture from overground walking of healthy adults. When assessing all individual steps in the walking bouts, we observed mean absolute errors between motion capture and OpenPose of 0.02 s for temporal gait parameters (i.e., step time, stance time, swing time and double support time) and 0.049 m for step lengths. Accuracy improved when spatiotemporal gait parameters were calculated as individual participant mean values: mean absolute error was 0.01 s for temporal gait parameters and 0.018 m for step lengths. The greatest difference in gait speed between motion capture and OpenPose was less than 0.10 m s-1. Mean absolute error of sagittal plane hip, knee and ankle angles between motion capture and OpenPose were 4.0\textdegree, 5.6\textdegree{} and 7.4\textdegree. Our analysis workflow is freely available, involves minimal user input, and does not require prior gait analysis expertise. Finally, we offer suggestions and considerations for future applications of pose estimation for human gait analysis.},
  langid = {english},
  keywords = {Ankles,gait analysis,Gait analysis,Hip,kinematics,Kinematics,Knees,marker-less motion capture,OpenPose Subject Terms:,pose estimation,Skeletal joints,Video recording,Walking},
  file = {/Users/jcotton81/Zotero/storage/68CX3VVT/2020.07.24.218776v1.full.pdf;/Users/jcotton81/Zotero/storage/NY24WI38/Stenum et al. - 2021 - Two-dimensional video-based analysis of human gait.pdf;/Users/jcotton81/Zotero/storage/PZM97ARU/article.html}
}

@article{sun_deep_2019,
  title = {Deep {{High-Resolution Representation Learning}} for {{Human Pose Estimation}}},
  author = {Sun, Ke and Xiao, Bin and Liu, Dong and Wang, Jingdong},
  year = {2019},
  month = jun,
  journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume = {2019-June},
  pages = {5686--5696},
  publisher = {{IEEE Computer Society}},
  issn = {10636919},
  doi = {10.1109/CVPR.2019.00584},
  abstract = {In this paper, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted key-point heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. In addition, we show the superiority of our network in pose tracking on the PoseTrack dataset. The code and models have been publicly available at https://github.com/leoxiaobin/ deep-high-resolution-net.pytorch.},
  isbn = {9781728132938},
  keywords = {2d pose,And Body Pose,Face,Gesture,pose},
  file = {/Users/jcotton81/Zotero/storage/739MLDGR/Sun et al. - 2019 - Deep High-Resolution Representation Learning for Human Pose Estimation.pdf}
}

@article{trewin_ai_2018,
  title = {{{AI Fairness}} for {{People}} with {{Disabilities}}: {{Point}} of {{View}}},
  shorttitle = {{{AI Fairness}} for {{People}} with {{Disabilities}}},
  author = {Trewin, Shari},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.10670 [cs]},
  eprint = {1811.10670},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We consider how fair treatment in society for people with disabilities might be impacted by the rise in the use of artificial intelligence, and especially machine learning methods. We argue that fairness for people with disabilities is different to fairness for other protected attributes such as age, gender or race. One major difference is the extreme diversity of ways disabilities manifest, and people adapt. Secondly, disability information is highly sensitive and not always shared, precisely because of the potential for discrimination. Given these differences, we explore definitions of fairness and how well they work in the disability space. Finally, we suggest ways of approaching fairness for people with disabilities in AI applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
  file = {/Users/jcotton81/Zotero/storage/RLXMNAKL/Trewin - 2018 - AI Fairness for People with Disabilities Point of.pdf;/Users/jcotton81/Zotero/storage/UAGR66MG/1811.html}
}

@misc{uhlrich_opencap_2022,
  title = {{{OpenCap}}: {{3D}} Human Movement Dynamics from Smartphone Videos},
  shorttitle = {{{OpenCap}}},
  author = {Uhlrich, Scott D. and Falisse, Antoine and Kidzi{\'n}ski, {\L}ukasz and Muccini, Julie and Ko, Michael and Chaudhari, Akshay S. and Hicks, Jennifer L. and Delp, Scott L.},
  year = {2022},
  month = jul,
  pages = {2022.07.07.499061},
  institution = {{bioRxiv}},
  doi = {10.1101/2022.07.07.499061},
  abstract = {Measures of human movement dynamics can predict outcomes like injury risk or musculoskeletal disease progression. However, these measures are rarely quantified in clinical practice due to the prohibitive cost, time, and expertise required. Here we present and validate OpenCap, an open-source platform for computing movement dynamics using videos captured from smartphones. OpenCap's web application enables users to collect synchronous videos and visualize movement data that is automatically processed in the cloud, thereby eliminating the need for specialized hardware, software, and expertise. We show that OpenCap accurately predicts dynamic measures, like muscle activations, joint loads, and joint moments, which can be used to screen for disease risk, evaluate intervention efficacy, assess between-group movement differences, and inform rehabilitation decisions. Additionally, we demonstrate OpenCap's practical utility through a 100-subject field study, where a clinician using OpenCap estimated movement dynamics 25 times faster than a laboratory-based approach at less than 1\% of the cost. By democratizing access to human movement analysis, OpenCap can accelerate the incorporation of biomechanical metrics into large-scale research studies, clinical trials, and clinical practice.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid = {english},
  file = {/Users/jcotton81/Zotero/storage/QMLT9GGX/Uhlrich et al. - 2022 - OpenCap 3D human movement dynamics from smartphon.pdf;/Users/jcotton81/Zotero/storage/Y4GMJW9E/2022.07.07.html}
}

@article{weygers_inertial_2020,
  title = {Inertial {{Sensor-Based Lower Limb Joint Kinematics}}: {{A Methodological Systematic Review}}},
  author = {Weygers, Ive and Kok, Manon and Konings, Marco and Hallez, Hans and De Vroey, Henri and Claeys, Kurt},
  year = {2020},
  month = jan,
  journal = {Sensors},
  volume = {20},
  number = {3},
  pages = {673},
  publisher = {{MDPI AG}},
  issn = {1424-8220},
  doi = {10.3390/s20030673},
  abstract = {{$<$}p{$>$}The use of inertial measurement units (IMUs) has gained popularity for the estimation of lower limb kinematics. However, implementations in clinical practice are still lacking. The aim of this review is twofold\textemdash to evaluate the methodological requirements for IMU-based joint kinematic estimation to be applicable in a clinical setting, and to suggest future research directions. Studies within the PubMed, Web Of Science and EMBASE databases were screened for eligibility, based on the following inclusion criteria: (1) studies must include a methodological description of how kinematic variables were obtained for the lower limb, (2) kinematic data must have been acquired by means of IMUs, (3) studies must have validated the implemented method against a golden standard reference system. Information on study characteristics, signal processing characteristics and study results was assessed and discussed. This review shows that methods for lower limb joint kinematics are inherently application dependent. Sensor restrictions are generally compensated with biomechanically inspired assumptions and prior information. Awareness of the possible adaptations in the IMU-based kinematic estimates by incorporating such prior information and assumptions is necessary, before drawing clinical decisions. Future research should focus on alternative validation methods, subject-specific IMU-based biomechanical joint models and disturbed movement patterns in real-world settings.{$<$}/p{$>$}},
  keywords = {Inertial measurement unit,Lower quadrant,Movement analysis,Outside laboratory,Sensor fusion},
  file = {/Users/jcotton81/Zotero/storage/KJZQ8SPF/full-text.pdf}
}

@article{wojke_simple_2017,
  title = {Simple {{Online}} and {{Realtime Tracking}} with a {{Deep Association Metric}}},
  author = {Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.07402 [cs]},
  eprint = {1703.07402},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a large-scale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45\%, achieving overall competitive performance at high frame rates.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/jcotton81/Zotero/storage/DUDI6F3B/Wojke et al. - 2017 - Simple Online and Realtime Tracking with a Deep As.pdf;/Users/jcotton81/Zotero/storage/IQH9X2QY/1703.html}
}

@article{xie_physics-based_2021,
  title = {Physics-Based {{Human Motion Estimation}} and {{Synthesis}} from {{Videos}}},
  author = {Xie, Kevin and Wang, Tingwu and Iqbal, Umar and Guo, Yunrong and Fidler, Sanja and Shkurti, Florian},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.09913 [cs]},
  eprint = {2109.09913},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Human motion synthesis is an important problem with applications in graphics, gaming and simulation environments for robotics. Existing methods require accurate motion capture data for training, which is costly to obtain. Instead, we propose a framework for training generative models of physically plausible human motion directly from monocular RGB videos, which are much more widely available. At the core of our method is a novel optimization formulation that corrects imperfect image-based pose estimations by enforcing physics constraints and reasons about contacts in a differentiable way. This optimization yields corrected 3D poses and motions, as well as their corresponding contact forces. Results show that our physically-corrected motions significantly outperform prior work on pose estimation. We can then use these to train a generative model to synthesize future motion. We demonstrate both qualitatively and quantitatively significantly improved motion estimation, synthesis quality and physical plausibility achieved by our method on the large scale Human3.6m dataset \textbackslash cite\{h36m\_pami\} as compared to prior kinematic and physics-based methods. By enabling learning of motion synthesis from video, our method paves the way for large-scale, realistic and diverse motion synthesis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read},
  file = {/Users/jcotton81/Zotero/storage/VFQIH7MZ/Xie et al. - 2021 - Physics-based Human Motion Estimation and Synthesi.pdf;/Users/jcotton81/Zotero/storage/4786K9MC/2109.html;/Users/jcotton81/Zotero/storage/PI5EAQVI/2109.html}
}

@misc{yatsenko_datajoint_2015,
  title = {{{DataJoint}}: Managing Big Scientific Data Using {{MATLAB}} or {{Python}}},
  shorttitle = {{{DataJoint}}},
  author = {Yatsenko, Dimitri and Reimer, Jacob and Ecker, Alexander S. and Walker, Edgar Y. and Sinz, Fabian and Berens, Philipp and Hoenselaar, Andreas and Cotton, R. James and Siapas, Athanassios S. and Tolias, Andreas S.},
  year = {2015},
  month = nov,
  pages = {031658},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/031658},
  abstract = {The rise of big data in modern research poses serious challenges for data management: Large and intricate datasets from diverse instrumentation must be precisely aligned, annotated, and processed in a variety of ways to extract new insights. While high levels of data integrity are expected, research teams have diverse backgrounds, are geographically dispersed, and rarely possess a primary interest in data science. Here we describe DataJoint, an open-source toolbox designed for manipulating and processing scientific data under the relational data model. Designed for scientists who need a flexible and expressive database language with few basic concepts and operations, DataJoint facilitates multiuser access, efficient queries, and distributed computing. With implementations in both MATLAB and Python, DataJoint is not limited to particular file formats, acquisition systems, or data modalities and can be quickly adapted to new experimental designs. DataJoint and related resources are available at http://datajoint.github.com.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2015, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/jcotton81/Zotero/storage/HRMABBAV/Yatsenko et al. - 2015 - DataJoint managing big scientific data using MATL.pdf;/Users/jcotton81/Zotero/storage/VDUVN2ZU/031658v1.html}
}

@inproceedings{yuan_simpoe_2021,
  title = {{{SimPoE}}: {{Simulated Character Control}} for {{3D Human Pose Estimation}}},
  shorttitle = {{{SimPoE}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yuan, Ye and Wei, Shih-En and Simon, Tomas and Kitani, Kris and Saragih, Jason},
  year = {2021},
  month = jun,
  pages = {7155--7165},
  issn = {2575-7075},
  doi = {10.1109/CVPR46437.2021.00708},
  abstract = {Accurate estimation of 3D human motion from monocular video requires modeling both kinematics (body motion without physical forces) and dynamics (motion with physical forces). To demonstrate this, we present SimPoE, a Simulation-based approach for 3D human Pose Estimation, which integrates image-based kinematic inference and physics-based dynamics modeling. SimPoE learns a policy that takes as input the current-frame pose estimate and the next image frame to control a physically-simulated character to output the next-frame pose estimate. The policy contains a learnable kinematic pose refinement unit that uses 2D keypoints to iteratively refine its kinematic pose estimate of the next frame. Based on this refined kinematic pose, the policy learns to compute dynamics-based control (e.g., joint torques) of the character to advance the current-frame pose estimate to the pose estimate of the next frame. This design couples the kinematic pose refinement unit with the dynamics-based control generation unit, which are learned jointly with reinforcement learning to achieve accurate and physically-plausible pose estimation. Furthermore, we propose a meta-control mechanism that dynamically adjusts the character's dynamics parameters based on the character state to attain more accurate pose estimates. Experiments on large-scale motion datasets demonstrate that our approach establishes the new state of the art in pose accuracy while ensuring physical plausibility.},
  keywords = {Computer vision,Dynamics,Kinematics,Pose estimation,Reinforcement learning,Solid modeling,Three-dimensional displays},
  file = {/Users/jcotton81/Zotero/storage/B97R9UP9/Yuan et al. - 2021 - SimPoE Simulated Character Control for 3D Human P.pdf;/Users/jcotton81/Zotero/storage/BWXZYKAH/Yuan et al. - 2021 - SimPoE Simulated Character Control for 3D Human P.pdf;/Users/jcotton81/Zotero/storage/KLW2RVXK/Yuan et al. - SimPoE Simulated Character Control for 3D Human P.pdf;/Users/jcotton81/Zotero/storage/NQKV453Q/2104.html;/Users/jcotton81/Zotero/storage/P33FU4JD/9578708.html}
}

@inproceedings{zhang_distribution-aware_2020,
  title = {Distribution-{{Aware Coordinate Representation}} for {{Human Pose Estimation}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhang, Feng and Zhu, Xiatian and Dai, Hanbin and Ye, Mao and Zhu, Ce},
  year = {2020},
  month = jun,
  pages = {7091--7100},
  issn = {2575-7075},
  doi = {10.1109/CVPR42600.2020.00712},
  abstract = {While being the de facto standard coordinate representation for human pose estimation, heatmap has not been investigated in-depth. This work fills this gap. For the first time, we find that the process of decoding the predicted heatmaps into the final joint coordinates in the original image space is surprisingly significant for the performance. We further probe the design limitations of the standard coordinate decoding method, and propose a more principled distributionaware decoding method. Also, we improve the standard coordinate encoding process (i.e. transforming ground-truth coordinates to heatmaps) by generating unbiased/accurate heatmaps. Taking the two together, we formulate a novel Distribution-Aware coordinate Representation of Keypoints (DARK) method. Serving as a model-agnostic plug-in, DARK brings about significant performance boost to existing human pose estimation models. Extensive experiments show that DARK yields the best results on two common benchmarks, MPII and COCO. Besides, DARK achieves the 2nd place entry in the ICCV 2019 COCO Keypoints Challenge. The code is available online.},
  keywords = {Decoding,Encoding,Image resolution,Pose estimation,Space heating,Standards},
  file = {/Users/jcotton81/Zotero/storage/QAD4I5IF/Zhang et al. - 2020 - Distribution-Aware Coordinate Representation for H.pdf;/Users/jcotton81/Zotero/storage/WPUPDRN9/full-text.pdf;/Users/jcotton81/Zotero/storage/JK3VBCMI/9157744.html}
}

@techreport{zheng_deep_2020,
  title = {Deep {{Learning-Based Human Pose Estimation}}: {{A Survey}}},
  author = {Zheng, Ce and Wu, Wenhan and Yang, Taojiannan and Zhu, Sijie and Chen, Chen and Liu, Ruixu and Shen, Ju and Kehtarnavaz, Nasser and Shah, Mubarak},
  year = {2020},
  month = dec,
  number = {6},
  pages = {663--676},
}

@article{nakano_evaluation_2020,
  title = {Evaluation of {{3D Markerless Motion Capture Accuracy Using OpenPose With Multiple Video Cameras}}},
  author = {Nakano, Nobuyasu and Sakura, Tetsuro and Ueda, Kazuhiro and Omura, Leon and Kimura, Arata and Iino, Yoichi and Fukashiro, Senshi and Yoshioka, Shinsuke},
  year = {2020},
  journal = {Frontiers in Sports and Active Living},
  volume = {2},
  eprint = {33345042},
  eprinttype = {pmid},
  pages = {50},
  issn = {2624-9367},
  doi = {10.3389/fspor.2020.00050},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7739760/},
}

@book{krajbich_atlas_2016,
  title = {Atlas of {{Amputations}} and {{Limb Deficiencies}}: {{Surgical}}, {{Prosthetic}}, and {{Rehabilitation Principles}}},
  shorttitle = {Atlas of {{Amputations}} and {{Limb Deficiencies}}},
  author = {Krajbich, Joseph Ivan and Pinzur, Michael S. and Potter, Benjamin K. and Stevens, Phillip M.},
  year = {2016},
  eprint = {NkN2xwEACAAJ},
  eprinttype = {googlebooks},
  publisher = {{American Academy of Orthopaedic Surgeons}},
  isbn = {978-1-62552-437-9},
  langid = {english},
  pagetotal = {1}
}

@Article{klopfer-kramer_gait_2020,
   Author="Klpfer-Krmer, I.  and Brand, A.  and Wackerle, H.  and Mig, J.  and Krger, I.  and Augat, P. ",
   Title="{{G}ait analysis - {A}vailable platforms for outcome assessment}",
   Journal="Injury",
   Year="2020",
   Volume="51 Suppl 2",
   Pages="S90-S96",
   Month="May"
}

@article{fang_alphapose_2022,
  author = {Fang, Hao-Shu and Li, Jiefeng and Tang, Hongyang and Xu, Chao and Zhu, Haoyi and Xiu, Yuliang and Li, Yong-Lu and Lu, Cewu},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title = {AlphaPose: Whole-Body Regional Multi-Person Pose Estimation and Tracking in Real-Time},
  year = {2022}
}

@book{Hartley_Zisserman_2003, title={Multiple View Geometry in Computer Vision}, ISBN={978-0-521-54051-3}, publisher={Cambridge University Press}, author={Hartley, Richard and Zisserman, Andrew}, year={2003}, language={en} }

@unpublished{roy_triangulation_2022,
  title = {On {{Triangulation}} as a {{Form}} of {{Self-Supervision}} for {{3D Human Pose Estimation}}},
  author = {Roy, Soumava Kumar and Citraro, Leonardo and Honari, Sina and Fua, Pascal},
  date = {2022-06-28},
  number = {arXiv:2203.15865},
  eprint = {2203.15865},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.15865},
  url = {http://arxiv.org/abs/2203.15865}
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{opencv_library, 
    author = {Bradski, G.}, 
    citeulike-article-id = {2236121}, 
    journal = {Dr. Dobb's Journal of Software Tools}, 
    keywords = {bibtex-import}, 
    posted-at = {2008-01-15 19:21:54}, 
    priority = {4}, 
    title = {{The OpenCV Library}}, 
    year = {2000} 
}

@misc{cao_openpose_2019,
  title = {{{OpenPose}}: {{Realtime Multi-Person 2D Pose Estimation}} Using {{Part Affinity Fields}}},
  shorttitle = {{{OpenPose}}},
  author = {Cao, Zhe and Hidalgo, Gines and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  date = {2019-05-30},
  number = {arXiv:1812.08008},
  eprint = {1812.08008},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1812.08008},
  url = {http://arxiv.org/abs/1812.08008},
}

@Misc{easymocap,  
    title = {EasyMoCap - Make human motion capture easier.},
    howpublished = {Github},  
    year = {2021},
    url = {https://github.com/zju3dv/EasyMocap}
}

@inproceedings{dong2021fast,
  title={Fast and Robust Multi-Person 3D Pose Estimation and Tracking from Multiple Views},
  author={Dong, Junting and Fang, Qi and Jiang, Wen and Yang, Yurou and Bao, Hujun and Zhou, Xiaowei},
  booktitle={T-PAMI},
  year={2021}
}

@article{cimorelli_portable_2022,
  title = {Portable In-Clinic Video-Based Gait Analysis: Validation Study on Prosthetic Users},
  author = {Cimorelli, Anthony and Patel, Ankit and Karakostas, Tasos and Cotton, R. James},
  date = {2022},
  journaltitle = {medRxiv},
  shortjournal = {medRxiv},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2022.11.10.22282089},
}

@unpublished{iskakov_learnable_2019,
  title = {Learnable {{Triangulation}} of {{Human Pose}}},
  author = {Iskakov, Karim and Burkov, Egor and Lempitsky, Victor and Malkov, Yury},
  date = {2019-05-14},
  number = {arXiv:1905.05754},
  eprint = {1905.05754},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.05754}
}

@inproceedings{jin_whole_2020,
  title={Whole-Body Human Pose Estimation in the Wild},
  author={Jin, Sheng and Xu, Lumin and Xu, Jin and Wang, Can and Liu, Wentao and Qian, Chen and Ouyang, Wanli and Luo, Ping},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},    
  year={2020}
}

@article{tang_comparison_2022,
  title = {Comparison of {{Lower Extremity Joint Moment}} and {{Power Estimated}} by {{Markerless}} and {{Marker-Based Systems}} during {{Treadmill Running}}},
  author = {Tang, Hui and Pan, Jiahao and Munkasy, Barry and Duffy, Kim and Li, Li},
  year = {2022},
  month = oct,
  journal = {Bioengineering},
  volume = {9},
  number = {10},
  pages = {574},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2306-5354},
  doi = {10.3390/bioengineering9100574},
}

@article{karashchuk_anipose_2020,
  title = {Anipose: A Toolkit for Robust Markerless {{3D}} Pose Estimation},
  author = {Karashchuk, Pierre and Rupp, Katie L. and Dickinson, Evyn S. and Sanders, Elischa and Azim, Eiman and Brunton, Bingni W. and Tuthill, John C.},
  date = {2020-05-29},
  journaltitle = {bioRxiv},
  pages = {2020.05.26.117325},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.05.26.117325}
}

@article{bilney_concurrent_2003,
  title = {Concurrent Related Validity of the {{GAITRite}} Walkway System for Quantification of the Spatial and Temporal Parameters of Gait},
  author = {Bilney, Belinda and Morris, Meg and Webster, Kate},
  date = {2003-02},
  journaltitle = {Gait \& Posture},
  shortjournal = {Gait Posture},
  volume = {17},
  number = {1},
  eprint = {12535728},
  eprinttype = {pmid},
  pages = {68--74},
  issn = {0966-6362},
  doi = {10.1016/s0966-6362(02)00053-x}
}

@article{mcdonough_validity_2001,
  title = {The Validity and Reliability of the {{GAITRite}} System's Measurements: {{A}} Preliminary Evaluation},
  shorttitle = {The Validity and Reliability of the {{GAITRite}} System's Measurements},
  author = {McDonough, A. L. and Batavia, M. and Chen, F. C. and Kwon, S. and Ziai, J.},
  date = {2001-03},
  journaltitle = {Archives of Physical Medicine and Rehabilitation},
  shortjournal = {Arch Phys Med Rehabil},
  volume = {82},
  number = {3},
  eprint = {11245768},
  eprinttype = {pmid},
  pages = {419--425},
  issn = {0003-9993},
  doi = {10.1053/apmr.2001.19778}
}

@misc{wang_direct_2021,
  title = {Direct {{Multi-view Multi-person 3D Pose Estimation}}},
  author = {Wang, Tao and Zhang, Jianfeng and Cai, Yujun and Yan, Shuicheng and Feng, Jiashi},
  date = {2021-11-27},
  number = {arXiv:2111.04076},
  eprint = {2111.04076},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.04076},
  url = {http://arxiv.org/abs/2111.04076},
  urldate = {2022-08-17}
}

@inproceedings{dong_shape-aware_2021,
  title = {Shape-Aware {{Multi-Person Pose Estimation}} from {{Multi-View Images}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Dong, Zijian and Song, Jie and Chen, Xu and Guo, Chen and Hilliges, Otmar},
  date = {2021-10},
  pages = {11138--11148},
  publisher = {{IEEE}},
  location = {{Montreal, QC, Canada}},
  doi = {10.1109/ICCV48922.2021.01097},
  url = {https://ieeexplore.ieee.org/document/9711390/},
  urldate = {2022-08-17}
}

@article{wu_isb_2002,
  title = {{{ISB}} Recommendation on Definitions of Joint Coordinate System of Various Joints for the Reporting of Human Joint Motion - {{Part I}}: {{Ankle}}, Hip, and Spine},
  author = {Wu, Ge and Siegler, Sorin and Allard, Paul and Kirtley, Chris and Leardini, Alberto and Rosenbaum, Dieter and Whittle, Mike and D'Lima, Darryl D. and Cristofolini, Luca and Witte, Hartmut and Schmid, Oskar and Stokes, Ian},
  date = {2002},
  journaltitle = {Journal of Biomechanics},
  volume = {35},
  number = {4},
  eprint = {11934426},
  eprinttype = {pmid},
  pages = {543--548},
  publisher = {{Elsevier Ltd}},
  issn = {00219290},
  doi = {10.1016/S0021-9290(01)00222-6},
}

@article{wu_isb_2005,
  title = {{{ISB}} Recommendation on Definitions of Joint Coordinate Systems of Various Joints for the Reporting of Human Joint Motion--{{Part II}}: Shoulder, Elbow, Wrist and Hand.},
  author = {Wu, Ge and van der Helm, Frans C T and Veeger, H E J DirkJan and Makhsous, Mohsen and Van Roy, Peter and Anglin, Carolyn and Nagels, Jochem and Karduna, Andrew R and McQuade, Kevin and Wang, Xuguang and Werner, Frederick W and Buchholz, Bryan and {International Society of Biomechanics}},
  options = {useprefix=true},
  date = {2005-05},
  journaltitle = {Journal of biomechanics},
  volume = {38},
  number = {5},
  eprint = {15844264},
  eprinttype = {pmid},
  pages = {981--992},
  issn = {0021-9290},
}

@misc{gao_nerf_2022,
  title = {{{NeRF}}: {{Neural Radiance Field}} in {{3D Vision}}, {{A Comprehensive Review}}},
  shorttitle = {{{NeRF}}},
  author = {Gao, Kyle and Gao, Yina and He, Hongjie and Lu, Dening and Xu, Linlin and Li, Jonathan},
  date = {2022-12-18},
  number = {arXiv:2210.00379},
  eprint = {2210.00379},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.00379},
  urldate = {2022-12-25},
}

@article{vaswani_attention_2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017},
  eprint = {1000303116},
  eprinttype = {pmid},
  issn = {0140-525X},
  doi = {10.1017/S0140525X16001837},
  url = {http://arxiv.org/abs/1706.03762}
}

@software{matthis_jonathan_samir_2022_7233714,
  author       = {Matthis, Jonathan Samir and
                  Cherian, Aaron},
  title        = {{FreeMoCap: A free, open source markerless motion 
                   capture system}},
  month        = jul,
  year         = 2022,
  note         = {{If you use this software, please cite it as below. 
                   ---  Website: https://freemocap.org --- Github:
                   https://github.com/freemocap/freemocap ---
                   Updates: https://twitter.com/freemocap --- Discord
                   Community: https://discord.gg/SgdnzbHDTG ---
                   Supported in part by: NIH R00 EY028229-04  issued
                   to JSM}},
  publisher    = {Zenodo},
  version      = {0.0.54},
  doi          = {10.5281/zenodo.7233714},
  url          = {https://doi.org/10.5281/zenodo.7233714}
}

@misc{cotton_improved_2023,
  doi = {10.48550/ARXIV.2303.02413},  
  url = {https://arxiv.org/abs/2303.02413},
  author = {Cotton, R. James and Cimorelli, Anthony and Shah, Kunal and Anarwala, Shawana and Uhlrich, Scott and Karakostas, Tasos},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Improved Trajectory Reconstruction for Markerless Pose Estimation},  
  publisher = {arXiv},
  year = {2023},
}

@misc{sarandi_learning_2022,
  title = {Learning {{3D Human Pose Estimation}} from {{Dozens}} of {{Datasets}} Using a {{Geometry-Aware Autoencoder}} to {{Bridge Between Skeleton Formats}}},
  author = {Srndi, Istvn and Hermans, Alexander and Leibe, Bastian},
  date = {2022-12-29},
  number = {arXiv:2212.14474},
  eprint = {2212.14474},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.14474},
  url = {http://arxiv.org/abs/2212.14474},}

@article{rapczynski_baseline_2021,
  title = {A {{Baseline}} for {{Cross-Database 3D Human Pose Estimation}}},
  author = {Rapczyski, Micha and Werner, Philipp and Handrich, Sebastian and Al-Hamadi, Ayoub},
  date = {2021-05-28},
  journaltitle = {Sensors (Basel, Switzerland)},
  shortjournal = {Sensors (Basel)},
  volume = {21},
  number = {11},
  eprint = {34071704},
  eprinttype = {pmid},
  pages = {3769},
  issn = {1424-8220},
  doi = {10.3390/s21113769},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8198914/}
}

@article{needham_accuracy_2021,
  title = {The Accuracy of Several Pose Estimation Methods for {{3D}} Joint Centre Localisation},
  author = {Needham, Laurie and Evans, Murray and Cosker, Darren P. and Wade, Logan and McGuigan, Polly M. and Bilzon, James L. and Colyer, Steffi L.},
  date = {2021-10-19},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {11},
  number = {1},
  pages = {20673},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-00212-x},
  url = {https://www.nature.com/articles/s41598-021-00212-x},
}

@article{ghorbani_movi_2021,
  title = {{{MoVi}}: {{A}} Large Multi-Purpose Human Motion and Video Dataset},
  shorttitle = {{{MoVi}}},
  author = {Ghorbani, Saeed and Mahdaviani, Kimia and Thaler, Anne and Kording, Konrad and Cook, Douglas James and Blohm, Gunnar and Troje, Nikolaus F.},
  date = {2021-06-17},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {16},
  number = {6},
  pages = {e0253157},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0253157},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0253157},
  urldate = {2023-02-20},
}



@book{winter_biomechanics_2009,
  title = {Biomechanics and {{Motor Control}} of {{Human Movement}}},
  author = {Winter, David A},
  date = {2009},
  edition = {4},
  publisher = {{Wiley}},
}

@article{werling_rapid_2022,
  title = {Rapid Bilevel Optimization to Concurrently Solve Musculoskeletal Scaling, Marker Registration, and Inverse Kinematic Problems for Human Motion Reconstruction},
  author = {Werling, Keenon and Raitor, Michael and Stingel, Jon and Hicks, Jennifer L. and Collins, Steve and Delp, Scott L. and Liu, C. Karen},
  date = {2022-08-23},
  pages = {2022.08.22.504896},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.08.22.504896},
  url = {https://www.biorxiv.org/content/10.1101/2022.08.22.504896v1},
}

@misc{nimblephysics,
  author = {Keenon Werling},
  title = {nimblephysics},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/keenon/nimblephysics}},
}

@article{lee_dart_2018,
  title = {{{DART}}: {{Dynamic Animation}} and {{Robotics Toolkit}}},
  shorttitle = {{{DART}}},
  author = {Lee, Jeongseok and Grey, Michael X. and Ha, Sehoon and Kunz, Tobias and Jain, Sumit and Ye, Yuting and Srinivasa, Siddhartha S. and Stilman, Mike and Liu, C. Karen},
  date = {2018-02-06},
  journaltitle = {Journal of Open Source Software},
  volume = {3},
  number = {22},
  pages = {500},
  issn = {2475-9066},
  doi = {10.21105/joss.00500},
  url = {https://joss.theoj.org/papers/10.21105/joss.00500},
  urldate = {2023-02-16},
  abstract = {Lee et al., (2018). DART: Dynamic Animation and Robotics Toolkit. Journal of Open Source Software, 3(22), 500, https://doi.org/10.21105/joss.00500},
  langid = {english},
}

@article{rajagopal_full-body_2016,
  title = {Full-{{Body Musculoskeletal Model}} for {{Muscle-Driven Simulation}} of {{Human Gait}}},
  author = {Rajagopal, Apoorva and Dembia, Christopher L. and DeMers, Matthew S. and Delp, Denny D. and Hicks, Jennifer L. and Delp, Scott L.},
  date = {2016},
  journaltitle = {IEEE Transactions on Biomedical Engineering},
  volume = {63},
  number = {10},
  eprint = {27392337},
  eprinttype = {pmid},
  pages = {2068--2079},
  issn = {15582531},
  doi = {10.1109/TBME.2016.2586891},
}

@article{ravi2020pytorch3d,
    author = {Nikhila Ravi and Jeremy Reizenstein and David Novotny and Taylor Gordon
                  and Wan-Yen Lo and Justin Johnson and Georgia Gkioxari},
    title = {Accelerating 3D Deep Learning with PyTorch3D},
    journal = {arXiv:2007.08501},
    year = {2020},
}

@software{trimesh,
	author = {{Dawson-Haggerty et al.}},
	title = {trimesh},
	url = {https://trimsh.org/},
	version = {3.2.0},
	year = {2019},
}

@article{sarandi_metrabs_2021,
  title = {{{MeTRAbs}}: {{Metric-Scale Truncation-Robust Heatmaps}} for {{Absolute 3D Human Pose Estimation}}},
  shorttitle = {{{MeTRAbs}}},
  author = {Srndi, Istvn and Linder, Timm and Arras, Kai O. and Leibe, Bastian},
  date = {2021-01},
  journaltitle = {IEEE Transactions on Biometrics, Behavior, and Identity Science},
  shortjournal = {IEEE Trans. Biom. Behav. Identity Sci.},
  volume = {3},
  number = {1},
  eprint = {2007.07227},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {16--30},
  issn = {2637-6407},
  doi = {10.1109/TBIOM.2020.3037257},
  url = {http://arxiv.org/abs/2007.07227}
}

@ARTICLE{mcfarland_musculoskeletal_2022,
  author={McFarland, Daniel C. and Binder-Markey, Benjamin I. and Nichols, Jennifer A. and Wohlman, Sarah J. and de Bruin, Marije and M. Murray, Wendy},
  journal={IEEE Transactions on Biomedical Engineering}, 
  title={A Musculoskeletal Model of the Hand and Wrist Capable of Simulating Functional Tasks}, 
  year={2022},
  volume={},
  number={},
  pages={1-12},
  doi={10.1109/TBME.2022.3217722}}

@online{song_markerless_2023,
  title = {Markerless Motion Capture Estimates of Lower Extremity Kinematics and Kinetics Are Comparable to Marker-Based across 8 Movements},
  author = {Song, Ke and Hullfish, Todd J. and Silva, Rodrigo Scattone and Silbernagel, Karin Gravare and Baxter, Josh R.},
  date = {2023-02-22},
  eprinttype = {bioRxiv},
  eprintclass = {New Results},
  pages = {2023.02.21.526496},
  doi = {10.1101/2023.02.21.526496},
  url = {https://www.biorxiv.org/content/10.1101/2023.02.21.526496v1},
  urldate = {2023-02-23}
}
