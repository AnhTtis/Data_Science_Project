%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

% \documentclass[preprint,12pt,authoryear]{elsarticle}
\documentclass[preprint,3p,authoryear]{elsarticle} % 1 colum % 리뷰단계는 1colm 권장한다고 저널홈페이지에 명시됨
% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle} % 2 column option

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem*{prop}{Proposition}
\newtheorem*{remark}{Remark}
\usepackage{kotex}
\usepackage[pdftex]{color}
\usepackage[font=footnotesize,labelfont=bf]{caption}
% \usepackage[font=footnotesize,labelfont=bf]{subcaption}
\usepackage{subcaption}
\usepackage{caption}
% \captionsetup{compatibility=false}
\usepackage{multirow}
\usepackage{floatrow}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xfrac}


\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{nicematrix}
\usepackage{tikz}
\usetikzlibrary{tikzmark,calc}
\newsavebox{\imagebox}

%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
\usepackage{lineno}
% \linenumbers
\journal{Engineering Applications of Artificial Intelligence}

\begin{document}

\begin{frontmatter}

\title{Towards Quantifying Calibrated Uncertainty via Deep Ensembles \\
in Multi-output Regression Task}
% \title{Towards Quantifying Calibrated Uncertainty via Deep Ensembles in Missile Performance Regression Tasks}

\author[1]{Sunwoong Yang}\corref{cor1}
\ead{sunwoongy@gmail.com}

\author[1]{Kwanjung Yee}\corref{cor2}
\ead{kjyee@snu.ac.kr}

\cortext[cor2]{Corresponding author}

\address[1]{Department of Aerospace Engineering, Seoul National University, Seoul, 08826, Republic of Korea}


\begin{abstract} 
% \begin{linenumbers}

Deep ensemble is a simple and straightforward approach for approximating Bayesian inference and has been successfully applied to many classification tasks. This study aims to comprehensively investigate this approach in the multi-output regression task to predict the aerodynamic performance of a missile configuration. By scrutinizing the effect of the number of neural networks used in the ensemble, an obvious trend toward underconfidence in estimated uncertainty is observed. In this context, we propose the deep ensemble framework that applies the post-hoc calibration method, and its improved uncertainty quantification performance is demonstrated. It is compared with Gaussian process regression, the most prevalent model for uncertainty quantification in engineering, and is proven to have superior performance in terms of regression accuracy, reliability of estimated uncertainty, and training efficiency. Finally, the impact of the suggested framework on the results of Bayesian optimization is examined, showing that whether or not the deep ensemble is calibrated can result in completely different exploration characteristics. This framework can be seamlessly applied and extended to any regression task, as no special assumptions have been made for the specific problem used in this study.
% \end{linenumbers}
\end{abstract}

\begin{keyword}
Predictive uncertainty \sep Deep ensemble \sep Uncertainty calibration \sep Bayesian optimization
\end{keyword}


\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{sec:intro}

We are entering an era of high performance computing technologies that have enabled engineers to efficiently obtain vast amounts of data, known as big data. According to the increase of available datasets, numerous data-driven approaches have been investigated to extract physical insights from them. The most fundamental, but most widely used one, is to train a regression model (also known as a surrogate model) with a given dataset and then leverage it to predict quantities of interest (QoIs) \citep{jeong2005efficient, nikolopoulos2022non, yang2022inverse}. This simple approach can be extended to a variety of applications, from exploration during the design optimization process \citep{yang2022design} to the prediction of high-dimensional fields by reduced order modeling \citep{kang2022pof}. Furthermore, from the perspective that the regression model can facilitate the realization of digital twins by replacing the demanding simulations required within its process \citep{vanderhorn2021digital}, its potential seems boundless.

However, such impacts cannot be fully achieved by the regression model alone. In real-world engineering problems, \textit{knowing what it does not know and therefore improving interpretability} is an indispensable issue. In the decision-making process based on the regression model, engineers should consider the predictive uncertainty derived from insufficient train data and imperfect regression model \citep{zhang2022uncertainty}. Otherwise, blind faith in regression models, especially during risk assessment and management procedures, can lead to unexpected and therefore disastrous outcomes. The most common approach to deal with this issue is to perform Bayesian optimization, which is also known as efficient global optimization in engineering fields \citep{jones1998efficient, yang2022design, chae2010helicopter, kanazaki2007multi}. Briefly, it aims to reduce model uncertainty by iteratively updating the model based on the acquisition function \citep{snoek2012practical, shin2020deep, shimoyama2013updating}, which contains uncertainty information (Fig. \ref{fig:EGO}). Since the Bayesian optimization process requires uncertainty quantification (UQ), whether the model provides uncertainty over its prediction in a quantified manner is the key criterion for engineers in determining which type of regression model to use.

\begin{figure*}[htb!]
    \centering
    
        % \includegraphics[width=.9\textwidth]{figures/EGO_whole_AI.png}
        \includegraphics[width=.9\textwidth]{figures/EGO_whole_AI.jpg}
        
    \caption{Flowchart of Bayesian optimization.}
    \label{fig:EGO}
\end{figure*} 

Gaussian process regression (GPR)---also known as Kriging---is one of the most widely used regression models capable of UQ in various engineering fields \citep{rhode2020non, wang2022dynamic, yang2020surrogate, quirante2018hybrid, quirante2016optimization, kessler2019global, zhong2019operation, yang2022comment, sugimura2009kriging, park2022multi, munoz2023gaussian}. GPR allows engineers to distinguish which predictions are unreliable by providing predictive uncertainty, and it has become the most famous regression model for Bayesian optimization \citep{jeong2005efficient, yang2022design, namura2016efficient, shimoyama2013updating}. However, GPR is notorious for its time complexity of $O(n^3)$ and memory complexity of $O(n^2)$, where $n$ denotes the dataset size \citep{cheng2017variational, wang2017time}. Even in multi-output regression tasks, since a GPR is trained for each output independently, the required training time increases linearly with the output dimension, and the correlations between outputs become completely ignored \citep{wang2015gaussian}.  

In this regard, Bayesian neural networks (BNNs) \citep{mackay1992information, mackay1995probable, fernandez2023physics} can be effective alternatives for the following reasons: 1) universal approximation capability \citep{hornik1989multilayer, barron1993universal}; 2) scalability to large datasets due to mini-batch training \citep{meng2021multi}; 3) multi-output prediction only with a single regression model. Since BNNs aim to learn probability distributions of the model parameters based on Bayesian inference, they can estimate the uncertainty of their prediction, whereas traditional neural networks (NNs) only provide point estimates. However, their additional model parameters slow down the convergence of the training \citep{zhang2019quantifying} and require significant modifications to the framework of conventional NNs, resulting in knotty and cumbersome training algorithms \citep{gal2016uncertainty, lakshminarayanan2017simple, fernandez2022uncertainty}. Such computational inefficiency and complexity prevent BNNs from being one of the options for engineers who value practicality and are not familiar with Bayesian formalism.

Recently, simple but scalable methods for approximating Bayesian inference have garnered the attention of engineers. Among them, deep ensembles (DE) \citep{lakshminarayanan2017simple} and MC-dropout \citep{gal2016dropout, deruyttere2021giving} require only a few modifications to vanilla NNs, indicating their applicability in engineering fields. However, MC-dropout is out of our focus due to its controversial issues of whether or not it is Bayesian inference \citep{osband2016risk, hron2017variational, hron2018variational, folgoc2021mc}; see \ref{sec:app_MCD}. DE, an approach to quantify the predictive uncertainty by leveraging ensembles of NNs, was proposed by \citet{lakshminarayanan2017simple}. Their idea is so simple and straightforward that it just trains multiple NNs in parallel with the same training dataset. Despite its simplicity, several researchers have discovered that the DE approach provides not only accurate predictions, but also reliable, robust, and pragmatically useful uncertainty estimates on a variety of datasets and architectures, even on out-of-distribution (OOD) examples \citep{gustafsson2020evaluating,fort2019deep,ovadia2019can,ashukha2020pitfalls}. Finally, it has come to be treated as the ``gold standard for accurate and well-calibrated predictive distributions'' \citep{wilson2020bayesian}.

However, most previous studies have focused on verifying whether DE accurately estimates uncertainty in classification tasks \citep{lakshminarayanan2017simple, rahaman2021uncertainty, wu2021should, fort2019deep, ovadia2019can, ashukha2020pitfalls}. Its comprehensive validation has not been conducted in multi-output regression tasks, which are the most common problems in practical engineering disciplines. For example, \citet{de2021bayesian} and \citet{pawar2022multi} each utilized DE for tailless aircraft range optimization and boundary layer flow prediction tasks, respectively, without any validation of the estimated uncertainty in their problems. In this sense, our research focuses on a thorough validation of the DE approach in multi-output regression tasks, while comparing it with GPR, both in terms of regression accuracy and reliability of the estimated uncertainty. In particular, we seek to overcome the limitations of existing studies that blindly adopted the number of NNs used in DE without sufficient explanation of their effects \citep{pocevivciute2022generalisation, ovadia2019can, ilg2018uncertainty, de2021bayesian, linmans2020efficient, rahaman2021uncertainty, de2021bayesian, egele2022autodeuq, maulik2023quantifying}. Finally, a tendency of the quantified uncertainty to become underconfident with the number of NNs is observed and a practical calibration method is proposed to be applied. The corresponding effects are verified quantitatively with two uncertainty evaluation criteria, and their impact on Bayesian optimization is also investigated. The main contributions of this work can be summarized as follows:
\begin{enumerate}

    \item First attempt to validate DE approach in the multi-output regression task.
    
    \item The effect of the number of NNs used for DE is comprehensively investigated and two different criteria are utilized for rigorous validation of its uncertainty quality.
 
    \item Accordingly, an increasing trend of underconfidence with increasing number of NNs is first empirically observed in the regression task and its analytical explanation is derived.
    
    \item A simple post-hoc calibration method is applied to DE models for the correction of unsatisfactory uncertainty quality and its effectiveness is verified both qualitatively and quantitatively.
    
    \item The impact of the proposed calibration method on Bayesian optimization is examined: the next query candidates before and after calibration are compared.

    \item Throughout the above procedures, GPR---the most well-known UQ model---is compared with DE, and the effectiveness of DE over GPR is confirmed.

    
\end{enumerate}

The rest of this paper is organized as follows. In Section \ref{sec:DE}, the background on how to implement DE and evaluate its uncertainty quality is described. In Section \ref{sec:DE_missile}, the application of DE to a multi-output regression task in aerospace engineering is elaborated. It provides a thorough validation of DE models compared to GPR models, both in terms of prediction accuracy and uncertainty quality. In Section \ref{sec:calib_results}, a simple post-hoc calibration method is applied and its effects on uncertainty quality and Bayesian optimization are investigated. Finally, in Section \ref{sec:conclusion}, the conclusion and future work of this study are presented.

\clearpage
\section{Implementation and evaluation of DE}
\label{sec:DE}

DE was first proposed by \citet{lakshminarayanan2017simple} for the simple and scalable estimation of predictive uncertainty using NNs. They not only suggested a novel pipeline, but also the proper scoring rule for its training. Although the DE concept can be regarded as a simple extension of the NNs (leveraging multiple NNs), DE has received little attention in the engineering disciplines despite its considerable reputation in the field of computer science, due to the lack of previous papers providing comprehensive and friendly explanations of its algorithm. To bridge this academic gap, this section is dedicated to providing a detailed description of the DE methodology and its validation. First, we briefly introduce the NNs (Section \ref{sec:DE_NN}) before moving on to DE. Then, the background of how to implement DE (Section \ref{sec:DE_DE}) and how to evaluate its uncertainty quality (Section \ref{sec:DE_UQ}) is described.

\subsection{\label{sec:DE_NN} Neural networks (NNs)}

NNs have attracted engineers from all disciplines for the following reasons: 1) universal approximation capability \citep{hornik1989multilayer, barron1993universal}; 2) scalability to large datasets owing to their mini-batch training \citep{meng2021multi}; and 3) multi-output prediction using only a single regression model. In this section, the brief theoretical background of these NNs is provided.

The information received from the input layer of NNs is propagated to the output layer through the feed-forward process. In this process, the information is transferred from the input layer to the output layer through an affine transformation, as follows:
\begin{equation}\
\label{eq:affine}
y = Wx + b,
\end{equation}
where $x$ is a vector of nodes in the input layer, $W$ and $b$ are the weight matrix and bias vector between the input and output layers, respectively, and $y$ is a vector of nodes in the output layer. However, since $x$ and $y$ are linearly correlated in Eq. \ref{eq:affine}, nonlinearity between them cannot be modeled regardless of the number of hidden layers embedded between the input and output layers. In this context, the activation function is introduced. It transforms the output of NNs: when nonlinear activation functions are imposed on each layer, nonlinear modeling becomes available. There are numerous nonlinear activation functions, and among them, the LeakyReLU function is represented as follows \citep{maas2013rectifier}:
\begin{equation}\
\label{eq:act}
    f(x)= 
\begin{cases}
    x,& \text{if } x \ge 0\\
    ax,              & \text{otherwise}
\end{cases}
\end{equation}
where $a$ stands for a small nonzero gradient (0.01 in this study). The NN model then applies an activation function (Eq. \ref{eq:act}) to the output of the previous layer to represent nonlinear behavior, and the correspondingly transformed output value is used as the input for the next layer. From the input layer to the output layer, this process is repeated through the hidden layers, which is called feed-forward. 

However, only with the feed-forward process, it is obvious that the final output cannot satisfy the intended accuracy, since there is no algorithm to adaptively train the parameters of the NN ($W$ and $b$). Accordingly, the training algorithm for adjusting them by minimizing the loss function has been introduced under the name of backpropagation \citep{rumelhart1986learning}: the loss function is set as the difference between the predicted values from NN and the desired target values, so that their values become similar as the training progresses. Specifically, the minimization of the loss function by backpropagation is realized using gradient descent optimization techniques, and to name a few, there are Adagrad \citep{duchi2011adaptive}, RMSprop \citep{tieleman2012lecture}, and Adam \citep{kingma2014adam}. Among them, Adam has been widely used due to its advantages in dealing with sparse gradients and non-stationary objectives, combining the strengths of Adagrad and RMSprop \citep{kingma2014adam}. As feed-forward (feeding information from input to output) and backpropagation with gradient descent (updating model parameters) are repeated iteratively, the loss function will decrease to the desired level and training will stop accordingly. The resulting NN model with the converged weights and biases then serves as a regression model in that predictions can be made in near real-time with the feed-forward operation. Since there are numerous papers describing the NN model, only the key points are presented here. For more details on NNs, see \citet{goodfellow2016deep}.

\subsection{\label{sec:DE_DE} Deep ensembles (DE)}

The NNs described above are often referred to as ``overconfident'' due to the lack of quantified uncertainty. In this context, DE can be an alternative for those interested in UQ. As mentioned earlier, the main concept of DE can be regarded as an ensemble of NNs. However, there is a simple but important point to remark: while the vanilla NN only outputs QoIs as a scalar quantity $\mu(x)$, the NN leveraged for DE outputs QoIs as a Gaussian distribution $N\bigl(\mu(x),\sigma^2(x)\bigr)$. More specifically, NN for DE assumes that the QoIs are sampled from $N\bigl(\mu(x),\sigma^2(x)\bigr)$, and accordingly it aims to output information about this distribution, $\mu(x)$ and $\sigma^2(x)$. Here, $\mu(x)$ refers to the estimated/predicted value and $\sigma^2(x)$ refers to the estimated/predicted variance. Note that the estimated variance $\sigma^2(x)$ indicates the aleatory uncertainty (uncertainty due to inherent noise in the training data) about the estimated value \citep{solopchuk2021active, laves2021recalibration}. With this special NN structure, the number of final nodes is doubled, since it outputs not only the standard outputs, $\mu(x)$, but also the uncertainty about them, $\sigma^2(x)$. As it outputs auxiliary information, we refer to this type of NN as an auxiliary NN. 

The DE concept adopts this auxiliary NN architecture for the following reason: the proper scoring rule cannot be applied with the vanilla NN structure. The scoring rule here indicates the criterion for estimating the quality of predictive uncertainty \citep{gneiting2007strictly}. \citet{lakshminarayanan2017simple} focused on the fact that with vanilla NN, which only outputs $\mu(x)$, the loss function for its training would be the mean squared error (MSE) as follows:
\begin{equation}\
\label{eq:MSE}
% \sum\limits_{n=1}^N(y_n-\mu_n(x))^2 % N 있는 버전
\mathrm{MSE}=\bigl(y-\mu(x)\bigr)^2
\end{equation}
and therefore the information about the predictive uncertainty is completely discarded during its training process. Accordingly, they suggested using an auxiliary NN that also outputs $\sigma^2(x)$ to enable the use of the proper scoring rule, negative log-likelihood (NLL). NLL is the standard metric for the quality of probabilistic models \citep{hastie2009elements}:

\begin{equation}\
\label{eq:NLL}
% \mathrm{NLL}=-\mathrm{log}(p_{\theta}(y|x))=\sum\limits_{n=1}^N(\cfrac{\mathrm{log}{\sigma^2_n(x)}}{2}+\cfrac{(y-\mu(x)_n)^2}{2\sigma^2_n(x)}+\cfrac{\mathrm{log}2\pi}{2}), % N 있는 버전
\mathrm{NLL}=-\mathrm{log}\bigl(p_{\theta}(y|x)\bigr)=\cfrac{\mathrm{log}{\sigma^2(x)}}{2}+\cfrac{\bigl(y-\mu(x)\bigr)^2}{2\sigma^2(x)}+\cfrac{\mathrm{log}2\pi}{2}
\end{equation}
This proper scoring rule, NLL, allows the following intuitive interpretations \citep{kendall2017uncertainties, guo2017calibration}. 1) When some training points have high MSE $\bigl(y-\mu(x)\bigr)^2$, the effect of the term $\tfrac{\bigl(y-\mu(x)\bigr)^2}{2\sigma^2(x)}$ becomes relatively dominant to the term $\tfrac{\mathrm{log}{\sigma^2(x)}}{2}$. Therefore, the model is trained to output high denominator value, $\sigma^2(x)$, over the corresponding points to reduce the NLL. 2) At training points with low MSE, the term $\tfrac{\mathrm{log}{\sigma^2(x)}}{2}$ becomes relatively dominant and therefore the model is encouraged to output low $\sigma^2(x)$ over the corresponding points. In summary, the training algorithm for auxiliary NN based on the NLL scoring rule allows learning accurate predictive uncertainty by estimating high uncertainty where the prediction error is actually high and low uncertainty where the prediction error is actually low. It is important to note that this cannot be achieved using the MSE loss function in a vanilla NN.

However, using a single auxiliary NN can only estimate the aleatory uncertainty arising from the inherent noise in the training data. To also quantify the epistemic uncertainty (uncertainty arising from the model parameters due to insufficient training data), one should go one step further. In this sense, \citet{lakshminarayanan2017simple} proposed to use an ensemble of auxiliary NNs, called deep ensembles (DE). Its main concept is to quantify aleatory and epistemic uncertainties altogether just by leveraging multiple NNs. Specifically, they aimed to capture the epistemic uncertainty by using the multiple auxiliary NNs trained on the identical dataset, of which the overall training procedure is summarized in Algorithm \ref{alg:train}. There are two notable points herein: first, the random initialization of the model parameters of the NNs in line 2; second, the random shuffling of the training dataset due to mini-batches in line 5. These two factors are considered to be the main reasons why the individual NNs in the ensemble can be trained with sufficient diversity \citep{lakshminarayanan2017simple}. For more details on this property, see \citet{fort2019deep}, which scrutinized the effects of random initialization in the DE approach and also investigated the decoupled effects of random initialization and random shuffling.

\begin{algorithm}[htb!]
\caption{Training procedure of DE}\label{alg:train}
    \begin{algorithmic}[1]
%        \State Split the whole dataset into train/calibration/test datasets.
        \State Split the train dataset $X$ into $J$ mini-batches.
        \State Randomly initializes model parameters of the $M$ auxiliary NNs and set training epochs.
        \For{$i=1:M$} \Comment{Loop for NN (parallelizable)}
            \For{epochs} \label{epochs} \Comment{Loop for epoch}
                \For{$j=1:J$} \Comment{Loop for mini-batch}
                    \State{$\mu_{ij}, \sigma^2_{ij}=$ NN$_{i}(x_{ij})$} \Comment{Feed-forward with mini-batch $x_{j}$}
                    \State{$\mathcal{L}_{ij}=$NLL$(y_{ij}, \mu_{ij}, \sigma^2_{ij})$} \Comment{Calculate NLL}
                    \State{$\theta_{i}=\theta_{i}-{learning\,rate}*{\delta{\mathcal{L}_{ij}}}/{\delta{\theta}}$} \Comment{Update model parameters of NN$_{i}$}
                \EndFor 
            \EndFor
        \EndFor
    \end{algorithmic}
\end{algorithm}

To see how the ensemble of auxiliary NNs trained in Algorithm \ref{alg:train} estimates two types of uncertainty, let $\mu_{i}(x)$ and ${\sigma}^2_{i}(x)$ be the predictive mean and predictive variance output by the $i$th individual NN. With this notation, the predicted probabilities of $y$ from the $i$th NN can be expressed as $N\bigl(\mu_{i}(x),\sigma^2_{i}(x)\bigr)$, indicating that there are multiple Gaussian distributions according to each NN in the ensemble. \citet{lakshminarayanan2017simple} suggested approximating the final probability of the output as a mixture of Gaussian probabilities as follows:
\begin{equation}\
\label{eq:mixture_m}
\hat{\mu}=\cfrac{1}{M}\sum\limits_{i=1}^M{\mu_{i}},
\end{equation}

\begin{equation}\
\label{eq:mixture_s}
\begin{aligned}
\underbrace{\hat{\sigma}^2}_{\substack{\text{predictive}\\ \text{uncertainty}}}
& =\cfrac{1}{M}\sum\limits_{i=1}^M{\sigma^2_{i}}
+ (\cfrac{1}{M}\sum\limits_{i=1}^M{\mu^2_{i}}-\hat{\mu}^2) \\
& = \underbrace{E(\sigma_i)}_{\substack{\text{aleatory}\\
\text{uncertainty}}} + \underbrace{Var(\mu_i)}_{\substack{\text{epistemic}\\ \text{uncertainty}}} \\
% & = \underbrace{\cfrac{1}{M}\sum\limits_{j=1}^M(\sigma^2_{ij})}_{\text{aleatoric uncertainty}}+\underbrace{\cfrac{1}{M}\sum\limits_{j=1}^M(\mu^2_{ij})-\hat{\mu}^2_{i}}_{\text{epistemic uncertainty}} \\
\end{aligned}
\end{equation}
where $M$ is the number of auxiliary NNs used for the ensemble. Accordingly, the final predictive value of DE is $\hat{\mu}$ and the final predictive uncertainty is $\hat{\sigma}^2$. As can be seen in Eq. \ref{eq:mixture_s}, the predictive uncertainty can be decomposed into aleatory and epistemic uncertainty; see \citet{scalia2020evaluating} and \citet{hu2021learning} for more details. It should be noted that no additional training algorithm is required after the training of auxiliary NNs in Algorithm \ref{alg:train}: only the mixture process of already trained NNs as in Eq. \ref{eq:mixture_m} and Eq. \ref{eq:mixture_s} is required. The overall flowchart of DE from the training of auxiliary NNs to the final prediction is schematically shown in Fig. \ref{fig:DE_struct}.

\begin{figure*}[htb!]
    \centering
    
        \includegraphics[width=.7\textwidth]{figures/DE_struct_v2.png}
        
    \caption{Flowchart of DE approach.}
    \label{fig:DE_struct}
\end{figure*} 

\subsection{\label{sec:DE_UQ} Uncertainty quality evaluation}

Section \ref{sec:DE_DE} demonstrated how the DE approach estimates predictive uncertainty. However, the feasibility of UQ alone cannot satisfy engineers who are interested in the uncertainty of predictions: the reliability of the estimated uncertainty should also be evaluated. Since the existing studies quantifying predictive uncertainty via GPR in engineering disciplines have underestimated the importance of this point, this study also aims to address this niche market. In this regard, the main objective of this section is to demonstrate the two different approaches for evaluating the quality of the estimated predictive uncertainty. The following methods can be used for any regression model capable of UQ, including GPR and DE.

\subsubsection{\label{sec:AUCE} AUCE}

The area under the calibration error curve (AUCE) is the most commonly used criterion to evaluate the quality of uncertainty \citep{kuleshov2018accurate, gustafsson2020evaluating}. The main purpose of this metric is to verify whether the confidence interval (CI) estimated by the model is accurate in practice, and Fig. \ref{fig:AUCE_info} schematically shows the details of its concept. In Fig. \ref{fig:AUCEa}, the CI labeled ``Well-calibrated 60\% CI'' contains the 60\% of the test dataset (6 out of 10 points), where test dataset indicates the dataset used to verify the quality of the estimated uncertainty. Therefore, the corresponding model can be considered well-calibrated in that its 60\% CI actually contains 60\% of the test data. On the other hand, when the 60\% CI actually contains more than 60\% of the dataset (8 out of 10 points), the model is judged to be underconfident, and it corresponds to the ``Underconfident 60\% CI'' case: that is, the model is underconfident about its prediction, so it overestimates its CI. Otherwise, when less than 60\% of the dataset is contained (4 out of 10 points, ``Overconfident 60\% CI'' case), the model is judged to be overconfident: overconfident about its prediction, so it estimates a narrower CI than reality. 

\begin{figure*}[htb!]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/CI_compar_a.png}
        % \caption{Comparison of 60\% CIs}\label{fig:AUCEa}
        \caption{}\label{fig:AUCEa}
    \end{subfigure}
    \hspace{0.0\columnwidth}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/CI_compar_b.png}
        % \caption{Comparison of CI-based reliability plot}\label{fig:AUCEb}
        \caption{}\label{fig:AUCEb}
    \end{subfigure}\hspace{0.05\textwidth}
    \caption{(a) Illustration of well-calibrated/miscalibrated models: $60\%$ CI of the well-calibrated model contains $60\%$ of the test data, whereas that of the underconfident and overconfident model contains $80\%$ and $40\%$ of the data, respectively. (b) Illustration of CI-based reliability plot.}
    \label{fig:AUCE_info}
\end{figure*} 

This discrepancy between CI estimated by the model and the actual data contained in this interval can be visually inspected using the CI-based reliability plot in Fig. \ref{fig:AUCEb}. It intuitively compares the predicted CI from the model (x-axis) and the observed CI measured with the test dataset (y-axis). For further explanation, the same situation as in Fig. \ref{fig:AUCEa} is assumed again. The underconfident case corresponds to P1 (x=0.6, y=0.8): the predicted 60\% CI actually corresponds to the observed 80\% CI, since 8 out of 10 points are included. The well-calibrated case is P2 (x=0.6, y=0.6): predicted 60\% CI actually contains 60\% of the points. Finally, P3 (x=0.6, y=0.4) corresponds to the overconfident case: the predicted 60\% CI actually contains 40\% of the calibration points. In this respect, the $y=x$ line can be considered an ideally well-calibrated model, as it predicts CI that is identical to the observed CI in practice. The procedure for plotting the CI-based reliability plot is summarized in Algorithm \ref{alg:CI_relia}.

\begin{algorithm}[htb!]
\caption{Procedure for CI-based reliability plot}\label{alg:CI_relia}
\begin{algorithmic}[1]
\State Prepare the test dataset $X$ (with input $x$ and output $y$).
\State Define candidates of CI to be investigated: $P = \{p_1,p_2,...,p_K\}$.
\State {$D = \varnothing$} \Comment{Initialize dataset $D$ to be plotted as y-axis}
\For{$i = 1 : K$} \Comment{Loop for $P$}
    \State {$count = 0$} \Comment{Initialize $count$}
    \State Find $Q(\dfrac{p_i+1}{2}|\mu,\sigma^2)$, which is $\dfrac{p_i+1}{2}$ quantile of $N(\mu,\sigma^2)$.
    \For{$j = 1 : length(X)$} \Comment{Loop for $X$}
        % \State Find percent point function of Gaussian distribution $N(\mu,\sigma^2)$, $\Phi^{-1}(p|\mu,\sigma^2)$.
        % \State Find quantile function of Gaussian distribution $N(\mu,\sigma^2)$: $Q(p|\mu,\sigma^2)$.
        \If{$-Q\bigl(\dfrac{p_i+1}{2}|\mu(x_j),\sigma^2(x_j)\bigr) \leq y_j \leq Q\bigl(\dfrac{p_i+1}{2}|\mu(x_j),\sigma^2(x_j)\bigr)$} %Cumulative distribution function-cdf
            \State $count += 1$ \Comment{Increase $count$ if test data is within the estimated CI}
        \EndIf
    \EndFor
    \State $\hat{p} = count / length(X)$ \Comment{Calculate observed CI}
    \State $D = D \cup \hat{p}$ \Comment{Append $\hat{p}$ to $D$}
\EndFor
\State Plot CI-based reliability plot: x-axis with $P$ and y-axis with $D$.
\end{algorithmic}
\end{algorithm}

With this reliability plot, AUCE, the metric for the quality of the evaluated uncertainty can be obtained. Specifically, it is measured as the area between the ideal $y=x$ line and the CI-based reliability plot of the model (for example, the AUCE of the underconfident model in Fig. \ref{fig:AUCEb} corresponds to the hatched area) and can be expressed as the following equation \citep{gustafsson2020evaluating}:

\begin{equation}\
\label{eq:AUCE}
\mathrm{AUCE} = \frac{1}{K}\sum\limits_{i=1}^K\lvert\hat{p}-p_i\rvert
\end{equation}
where $K$ refers to the number of CI candidates as in Algorithm \ref{alg:CI_relia}. By definition, a low AUCE value indicates that the predictive uncertainty quantified by the model is accurate (or well-calibrated). For more details on AUCE, see \citet{naeini2015obtaining}, \citet{gustafsson2020evaluating}, and \citet{scalia2020evaluating}.

\subsubsection{\label{sec:ENCE} ENCE}

Despite its reputation as a metric of uncertainty quality, AUCE has a critical shortcoming in that it only considers the average over the entire test dataset rather than individuals as mentioned by \citet{levi2022evaluating}. Moreover, they analytically and empirically elaborated that AUCE can be zero even in the case where the predicted distribution is statistically independent from that of the ground truth. In this context, they proposed a novel approach to evaluate the quality of uncertainty, the expected normalized calibration error (ENCE), relatively state-of-the-art metric. It was first proposed based on the intuitive assumption: for the well-calibrated model, the estimated uncertainty $\sigma^2(x)$ will be equal to $\bigl(y-\mu(x)\bigr)^2$, MSE. This condition can be expressed mathematically as follows, implying that a higher estimated variance should correspond to a higher expected MSE \citep{phan2018calibrating}:
\begin{equation}\
\label{eq:ENCE_int}
\mathbb{E}_{x,y}[\bigl(y-\mu(x)\bigr)^2 | \sigma^2(x)] = \sigma^2(x)
\end{equation}
The above Eq. \ref{eq:ENCE_int} indicates that the ideally (perfectly) well-calibrated model will have an expected error, $\bigl(y-\mu(x)\bigr)^2$, exactly identical to predictive uncertainty, $\sigma^2(x)$. In this sense, whether the model is well-calibrated can be visually inspected using the error-based reliability plot \citep{scalia2020evaluating,levi2022evaluating}: x-axis as root mean squared error (RMSE), $y-\mu(x)$, and y-axis as root of the mean variance (RMV), $\sigma(x)$. Fig. \ref{fig:ENCE_info} illustrates it, and by its definition in Eq. \ref{eq:ENCE_int}, $y=x$ line indicates the ideally calibrated model. The procedure for its plotting is summarized in Algorithm \ref{alg:err_relia}. 

\begin{figure*}[htb!]
    \centering
            \includegraphics[width=.4\textwidth]{figures/CI_compar_c.png}
    \caption{Illustration of error-based reliability plot. Underconfident model overestimates RMV relative to RMSE, while overconfident model underestimates RMV. The ideal model estimates the equivalent RMV and RMSE as the $y=x$ black dashed line.}
    \label{fig:ENCE_info}
\end{figure*} 

\begin{algorithm}[htb!]
\caption{Procedure for error-based reliability plot}\label{alg:err_relia}
\begin{algorithmic}[1]
\State Prepare the test dataset $X$ (with input $x$ and output $y$).
\State Sort $X$ according to $y$ values.
\State Define the number of bins: $B$ (assume $B$ divides $length(X)$).
\State Divide sorted $X$ into $B$ bins, $\tilde{X}=\{\tilde{X}_1,\tilde{X}_2,...,\tilde{X}_B\}$, such that each $\tilde{X}_i$ has the same size of $length(X)/B$.
\State {$D_{RMSE} = \varnothing$} \Comment{Initialize dataset $D_{RMSE}$ to be plotted as x-axis}
\State {$D_{RMV} = \varnothing$} \Comment{Initialize dataset $D_{RMV}$ to be plotted as y-axis}
\For{$i = 1 : B$} \Comment{Loop for $\tilde{X}$}
    % \State $D_{RMSE} = D_{RMSE} \cup \sqrt{(y(\tilde{X}_i) - \mu(\tilde{X}_i))^2}$ \Comment{Append RMSE to $D_{RMSE}$}
    % \State $D_{RMV} = D_{RMV} \cup \sqrt{\sigma^2(\tilde{X}_i)}$ \Comment{Append RMV to $D_{RMV}$}
    \State $D_{RMSE} = D_{RMSE} \cup \sqrt{\dfrac{1}{\lvert \tilde{X}_i \rvert}\sum\limits_{x \in \tilde{X}_i} {\bigl(y(x) - \mu(x)\bigr)^2}}$ \Comment{Append RMSE to $D_{RMSE}$}
    \State $D_{RMV} = D_{RMV} \cup \sqrt{\dfrac{1}{\lvert \tilde{X}_i \rvert}\sum\limits_{x \in \tilde{X}_i} \sigma^2(x)}$ \Comment{Append RMV to $D_{RMV}$}
\EndFor
\State Plot error-based reliability plot: x-axis with $D_{RMSE}$ and y-axis with $D_{RMV}$.
\end{algorithmic}
\end{algorithm}

\clearpage
Then, the area between the ideal $y=x$ line and the error-based reliability plot can be calculated. The normalized version of this value refers to ENCE, the second uncertainty quality metric, and is as follows: 
\begin{equation}\
\label{eq:ENCE}
\mathrm{ENCE} = \frac{1}{B}\sum\limits_{i=1}^B \frac{{\lvert\mathrm{RMV}(i)-\mathrm{RMSE}(i)\rvert}}{\mathrm{RMV}(i)}
\end{equation}
where $B$ indicates the number of bins in Algorithm \ref{alg:err_relia}. Therefore, the ENCE of the underconfident model in Fig. \ref{fig:ENCE_info} can be calculated as the hatched area divided by RMV. As with AUCE, the lower the ENCE value, the better the model is calibrated. 

\subsection{\label{sec:Calib} Uncertainty calibration: STD scaling}

When the estimated uncertainty from the model is inaccurate in terms of AUCE (Section \ref{sec:AUCE}) and ENCE (Section \ref{sec:ENCE}), there are several uncertainty calibration techniques. To name a few, histogram binning \citep{zadrozny2001obtaining}, isotonic regression \citep{zadrozny2002transforming}, and temperature scaling \citep{guo2017calibration}. The first two methods are non-parametric approaches, so the number of used parameters depends on the size of the training dataset. In contrast, temperature scaling is a parametric approach that requires a fixed number of parameters. 

Since this study focuses on the applications of UQ methods to the engineering disciplines, where practicality is highly sought, the simple temperature scaling method is adopted. To be more specific, standard deviation (STD) scaling is used which is the extension of temperature scaling to the regression tasks \citep{levi2022evaluating}. STD scaling only needs to find the scalar parameter $s$, which is multiplied to the standard deviation originally estimated by DE model, $\hat{\sigma}$. The value of $s$ used in the calibration is determined to minimize the NLL as follows:

\begin{equation}\
\label{eq:NLL_cal}
s=\underset{s}{\operatorname{argmin}}(\cfrac{\mathrm{log}\bigl({s\hat{\sigma}(x)}\bigr)^2}{2}+\cfrac{\bigl(y-\hat{\mu}(x)\bigr)^2}{2\bigl(s\hat{\sigma}(x)\bigr)^2}+\cfrac{\mathrm{log}2\pi}{2}),
\end{equation}
Note that the above equation is the simple modification of Eq. \ref{eq:NLL}, where $\sigma(x)$ is replaced by $s\hat{\sigma}(x)$. This calibration procedure is completely separate from the training procedure of DE; it is performed after the mixture step in Fig. \ref{fig:DE_struct}, so it is called the post-hoc or post-process calibration method. Also, the model parameters (e.g., weights and biases in the NN model) do not change at all during this calibration process. The intuitive explanation for the STD scaling method is as follows: if the estimated uncertainty from the trained model $\bigl(\hat{\sigma}(x)\bigr)$ is not well-calibrated, the calibrated version of the uncertainty $s\hat{\sigma}(x)$ can be used instead. As the purpose of this calibration is only to correct the estimated uncertainty, it is also important to note that only the output $\hat{\sigma}(x)$ of the DE changes, while the predictive value $\hat{\mu}(x)$ remains unchanged. The process of STD calibration is summarized in Algorithm \ref{alg:calib}. For the calibration, it is important to note that a calibration dataset separate from the train/test dataset should be utilized to ensure calibration generalization \citep{levi2022evaluating}. In multi-output regression tasks, each output of the DE can be calibrated independently using a number of parameters $s$ equal to the dimensionality of the QoI (this is implemented by the for-loop in line 3 of Algorithm \ref{alg:calib}). In conclusion, a simple and straightforward STD calibration method, which only requires tuning the scalar parameters without modifying the already trained NNs, is adopted for uncertainty calibration in this study.

\begin{algorithm}[htb!]
\caption{STD calibration procedure}\label{alg:calib}
\begin{algorithmic}[1]
\State Prepare calibration dataset $X$ (with input $x$ and output $y$).
\State Define candidates of scaling factor: $S$
% \State $S' = \varnothing$ \Comment{Initialize a set of final scaling factors}
\For {$i = 1 : length(y)$} \Comment{Loop for output dimension of DE}
    % \For {$j = 1 : K$} \Comment{Loop for $s$}
    \State $s_i=\underset{s \in S}{\operatorname{argmin}}(\cfrac{\mathrm{log}{\bigl(s\hat{\sigma_i}(x)\bigr)^2}}{2}+\cfrac{\bigl(y_i-\hat{\mu_i}(x)\bigr)^2}{2\bigl(s\hat{\sigma_i}(x)\bigr)^2}+\cfrac{\mathrm{log}2\pi}{2})$
    % \State $S' = S' \cup s_i$
\EndFor
% \State Utilize obtained $S'$ to calibrate uncertainty over $i$th output, $y_i$, such as $s_i\sigma_i$ in lieu of $\sigma_i$.
\State Utilize $s_i$ to calibrate estimated uncertainty over $i$th output \Comment{E.g., use $s_i\hat{\sigma_i}$ in lieu of $\hat{\sigma_i}$.}
\end{algorithmic}
\end{algorithm}

\section{Application of DE to aerodynamic performance regression task}
\label{sec:DE_missile}

In this section, the DE method is applied to a multi-output regression task. Specifically, it is leveraged to predict six aerodynamic coefficients of the specific missile configuration under varying flow conditions. The main objective of this section is to verify the applicability of the DE model to the real engineering regression problem, since there is no study that comprehensively validates its performance in the multi-output regression task. In this regard, both the regression and uncertainty estimation performances of DE are scrutinized, and the effect of $M$ (the number of NNs for the ensemble) is also investigated.

\subsection{\label{sec:pre_missile} Data preparation and training details}

As mentioned above, the prediction of six aerodynamic coefficients for the fixed missile configuration is selected as the engineering problem in this study. The missile configuration is selected as ``Configuration 1'' in the NASA TM-2005-213541 report \citep{allen2005aerodynamics}, which has a small nose, four long strakes, and four tails. Then Missile Datcom \citep{blake1998missile}, the low-fidelity semi-empirical solver, is employed to obtain six aerodynamic coefficients of this configuration. The input parameters of the solver are selected as five flow conditions: $Ma$, $\phi$ (roll angle), $\delta{p}$ (pitch control fin deflection angle), $\delta{r}$ (roll control fin deflection angle), and $AoA$. The corresponding outputs are the following six aerodynamic coefficients $C_{NF}$ (normal force coefficient), $C_{AF}$ (axial force coefficient), $C_{PM}$ (pitching moment coefficient), $C_{RM}$ (rolling moment coefficient), $C_{YM}$ (yawing moment coefficient), and $C_{SF}$ (side force coefficient). Finally, a total of 9,800 points are sampled from the input space using the full-factorial technique, and these are split 8:1:1 for the train:test:calibration dataset. The train dataset is used to train DE and GPR models in this section, the test dataset is used to evaluate regression and UQ performance in Sections \ref{sec:pred_results} and \ref{sec:UQ_results}, and the calibration dataset is used to perform STD calibration in Section \ref{sec:UQ_calib}.

Since the training dataset is obtained, the next step is to determine the hyperparameters of the auxiliary NN used for DE model. The hyperparameter tuning is performed based on a grid search, and the parameters considered in this tuning are chosen as number of layers, number of nodes, and size of mini-batch. The other hyperparameters such as optimizer algorithm, initial learning rate, and total epochs are set to Adam, $10^{-3}$, and 13,000, respectively. The results of the tuning can be found in \ref{sec:app_hyp}. Finally, considering both NLL and RMSE, an auxiliary NN with 7 hidden layers and 128 nodes has been chosen for the ensemble, and the size of the mini-batch has been set to 512. Then, different values of the hyperparameter $M$ are examined: 2, 4, 8, and 16 are considered, and the DE model corresponding to each $M$ is referred to as DE-2, DE-4, DE-8, and DE-16 throughout this manuscript. In addition to multiple DE models, GPR is also trained to compare with them in terms of effectiveness. Therefore, an RBF kernel-based GPR model with the number of optimizer restarts as 5 and a nugget \citep{yang2023multi} value of $10^{-2}$ is set up. Finally, the training procedure of these models is completed, and the required training times using Intel(R) Xeon(R) CPU @ 2.20GHz are as follows: 17.8 hours for GPR, 2.4 hours for DE-2, 5 hours for DE-4, 9.7 hours for DE-8, and 19.4 hours for DE-16.


\subsection{\label{sec:pred_results} Evaluation of regression performance}

In this section, the regression performances of all trained models are presented using a test dataset that is not used in model training. Before going into details, DE-2 (which required the least training time among the DE models) is compared with GPR to highlight the efficiency of the DE models. Fig. \ref{fig:kde_plot} shows the results of kernel density estimation (KDE), which demonstrates the generalization performance of the models by visualizing the distributions of the test data in terms of NLL and RMSE (those of all six QoIs are averaged to be shown in this figure). For both criteria, the obvious superiority of DE-2 can be identified: most of the test data is concentrated in the lower error region in DE-2. More specifically, the KDE of NLL shows that the density peak of DE-2 represented by a star with long dashed line is located at NLL of -4.5, while that of GPR is located at -1.5. When it comes to RMSE, the peak of DE-2 is at RMSE of 0.003 while GPR is at 0.012. The medians of the error metrics are also shown as circles with dotted lines. For both metrics, those of DE-2 are much lower than those of GPR, indicating that DE-2 performs better than GPR overall. The most interesting point here is that although DE-2 requires only 13\% of the training time of GPR, it achieves superior regression accuracy. 

\begin{figure}[htb!]
    \centering
    
        \includegraphics[width=.45\columnwidth]{figures/kde_plt.png}
        
    \caption{Comparison of regression accuracy between GPR and DE-2: kernel density estimation (KDE) of test dataset with respect to NLL and RMSE (averaged values of all six QoIs). The stars and circles represent the maximum and median points of each model, respectively.}
    \label{fig:kde_plot}
\end{figure} 

Fig. \ref{fig:pred_compar} provides the comprehensive results of the regression performance. Fig. \ref{fig:pred_compar_a} shows the NLL results of all trained models with respect to the six aerodynamic QoIs, and their averaged NLL is also shown at the right end. Throughout all QoIs, GPR shows inferior regression accuracy than all other DE models. The results on NLL could be expected as each NN in the DE model is trained to minimize NLL. However, the results on RMSE in Fig. \ref{fig:pred_compar_b} are highly inspiring: they also achieve higher regression accuracy even in terms of RMSE. Considering that numerous engineers use RMSE to evaluate regression models, the fact that the average RMSE of DE models is less than half that of GPR is quite encouraging. Also, assuming that the performance of DE-5 will be between DE-4 and DE-8, the results in Fig. \ref{fig:pred_compar} indicate that $M=5$ as suggested by \citet{lakshminarayanan2017simple} is sufficient for DE, at least in terms of predictive accuracy.

\begin{figure*}[htb!]
    \centering
    \begin{subfigure}[h]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pred_NLL.png}
        \caption{}\label{fig:pred_compar_a}
    \end{subfigure}
    \vfill
    \begin{subfigure}[h]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pred_RMSE.png}
        \caption{}\label{fig:pred_compar_b}
    \end{subfigure}
    \caption{Comparison of regression accuracy between GPR and all DE models: comprehensive results in terms of all aerodynamic QoIs. (a) NLL, (b) RMSE.}
    \label{fig:pred_compar}
\end{figure*} 


\clearpage
\subsection{\label{sec:UQ_results} Evaluation of UQ performance}

This section examines the quality of the predictive uncertainty, using AUCE and ENCE criteria for the quantitative investigation. For this purpose, reliability plots should be drawn first, using the test dataset split in Section \ref{sec:pre_missile} (dataset size of 980). Also, as in Algorithm \ref{alg:CI_relia}, CI-based reliability plots require the set of CI candidates ($P$) and error-based reliability plots in Algorithm \ref{alg:err_relia} need the number of bins ($B$). In this study, $P = \{0.1, 0.2, ..., 0.9\}$ and $B = 20$ are chosen.

Fig. \ref{fig:reliab_GPR} shows the results of GPR, and it appears that GPR has a satisfactory uncertainty quality with respect to the CI-based reliability plot (Fig. \ref{fig:reliab_GPR_a}), while the error-based plot (Fig. \ref{fig:reliab_GPR_b}) shows relatively poor quality. In the latter plot, since the estimated variances (RMV on the y-axis) are underestimated compared to the actual errors (RMSE on the x-axis), it can be inferred that GPR is trained to be ``overconfident'': it is overconfident itself, so it underestimates its uncertainty.

\begin{figure}[htb!]
    \centering
    \begin{subfigure}[h]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/under_GPR.png}
        \caption{}\label{fig:reliab_GPR_a}
    \end{subfigure}
    \hspace{0.0\columnwidth}
    \begin{subfigure}[h]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/err_GPR.png}
        \caption{}\label{fig:reliab_GPR_b}
    \end{subfigure}
    \caption{Reliability plots of GPR: (a) CI-based reliability plot, (b) Error-based reliability plot.}\label{fig:reliab_GPR}
\end{figure} 

The results of the DE models are then shown in Fig. \ref{fig:reliab_DE_bef}. Note that unlike GPR in Fig. \ref{fig:reliab_GPR}, only the results of $C_{SF}$ are visualized to highlight the differences between DE models: comprehensive results can be found in Fig. \ref{fig:reliab_DE_comprehensive} in \ref{sec:app_UQ_bef}. For DE-2, the CI-based plot (Fig. \ref{fig:reliab_DE_bef_a}) shows inferior performance to that of GPR, while the error-based plot (Fig. \ref{fig:reliab_DE_bef_b}) shows much higher quality. Meanwhile, a notable trend is observed along the increase of $M$ values: as $M$ increases, the uncertainty quality with respect to both reliability plots apparently degrades. More specifically, both types of plots move upward away from the $y=x$ ideal line as $M$ increases, indicating that DE models tend to become ``underconfident''. Considering that DE-16 requires about 8 times as much training time as DE-2, it can be confirmed that using large $M$ values for the ensemble does not necessarily lead to better results, but rather the opposite in terms of uncertainty quality. In this context,  assuming that the performance of DE-5 will be between DE-4 and DE-8, it can be inferred that using $M=5$ as suggested by \citet{lakshminarayanan2017simple} does not guarantee sufficient UQ quality in this case. In fact, the insight behind this underconfident tendency when ensembling networks in classification tasks can be found in \citet{rahaman2021uncertainty}, while the corresponding tendency in regression has not been proven. Accordingly, in the next section we provide the mathematical explanation for this underconfident tendency in regression tasks.

\begin{figure}[htb!]
    \centering
    \begin{subfigure}[h]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/under_DE_simple_bef.png}
        \caption{}\label{fig:reliab_DE_bef_a}
    \end{subfigure}
    \hspace{0.0\columnwidth}
    \begin{subfigure}[h]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/err_DE_simple_bef.png}
        \caption{}\label{fig:reliab_DE_bef_b}
    \end{subfigure}
    \caption{Reliability plots of DE: for simplicity, only the $C_{SF}$ results of different DE models are shown. (a) CI-based reliability plot, (b) Error-based reliability plot. In (b), to clearly show the tendency of UQ quality to decrease with increasing $M$, the linear regression model of the scatter points of each DE model is shown as a dashed line with the corresponding color.}\label{fig:reliab_DE_bef}
\end{figure} 


\clearpage
\subsection{\label{sec:math} Theoretical derivation: underconfidence of DE in regression tasks}

This section is for the mathematical derivation of why the ensemble of individual NNs becomes underconfident, as discovered in the previous section. For this purpose, the deviation from calibration (DC) score is introduced as in \citet{rahaman2021uncertainty}; their work focused only on classification tasks, so their DC score consisted of the Brier score and the entropic term. Meanwhile, since our work focuses on the regression task, we adopted the different DC score consisting of the MSE and predictive variance. In this context, the following proposition and its proof can be considered as one of the contributions of this paper.

\begin{prop}
When DC score is defined as follows,

\begin{equation}
% DC(\mu, \sigma) \equiv \mathbb{E}[(\mu - \hat{\mu})^2 - \hat{\sigma}^2]
DC(\mu, \sigma) \equiv (y - {\mu})^2 - {\sigma}^2
\end{equation}

DC score of the ensemble becomes less than or equal to the averaged DC score of the individual NNs.

\begin{equation}
\label{eq:proposition}
DC(\hat{\mu}, \hat{\sigma}) \leq \cfrac{1}{M} \sum\limits_{i=1}^MDC(\mu_i, \sigma_i)
\end{equation}

\end{prop}

\begin{proof}
The averaged DC score of the individual NNs (right-hand side of the Eq. \ref{eq:proposition}) can be expressed as:

\begin{equation}
\begin{aligned}[b]
\cfrac{1}{M} \sum\limits_{i=1}^M {DC(\mu_i, \sigma_i)}
& = \cfrac{1}{M} \sum\limits_{i=1}^M (y^2 - 2y{\mu_i} + {\mu_i}^2 - {\sigma_i}^2) \\
& = y^2 - 2y\hat{\mu} + \cfrac{1}{M} \sum\limits_{i=1}^M {{\mu_i}^2} - \cfrac{1}{M} \sum\limits_{i=1}^M {{\sigma_i}^2} \\
& = (y^2 - 2y\hat{\mu} + \hat{\mu}^2 - \hat{\sigma}^2) + (\cfrac{1}{M} \sum\limits_{i=1}^M {{\mu_i}^2} - \hat{\mu}^2) + (\hat{\sigma}^2 - \cfrac{1}{M} \sum\limits_{i=1}^M {{\sigma_i}^2}) \\
& = \underbrace{(y^2 - 2y\hat{\mu} + \hat{\mu}^2 - \hat{\sigma}^2)}_{= DC(\hat{\mu}, \hat{\sigma})} + 2 \underbrace{(\cfrac{1}{M} \sum\limits_{i=1}^M {{\mu_i}^2} - \hat{\mu}^2)}_{= Var(\mu_i)} \quad (\because \text{Eq. \ref{eq:mixture_s}})
\end{aligned}
\end{equation}
 % && (\because \text{Eq. \ref{eq:mixture_s}})
Hence,

\begin{equation}
\begin{aligned}
DC(\hat{\mu}, \hat{\sigma}) 
= \cfrac{1}{M} \sum\limits_{i=1}^MDC(\mu_i, \sigma_i) - \underbrace{2 \cdot Var(\mu_i)}_{\geq 0} \\
\end{aligned}
\end{equation}

\end{proof}

\begin{remark}
The DC used in the above proposition indicates the degree of calibration. When DC equals 0, it means that the estimated uncertainty $\sigma^2$ exactly matches the MSE, $(y - {\mu})^2$. If $DC<0$, the uncertainty is overestimated compared to the MSE, which is an underconfident case. Therefore, the proposition that the DC score decreases after ensembling has mathematically explained the underconfidence of DE models observed in Section \ref{sec:UQ_results}.  
\end{remark}

\clearpage
\section{\label{sec:calib_results} DE models with STD calibration}

The underconfidence tendency of DE models in regression tasks is observed and explained in previous sections. This section suggests the use of post-hoc STD calibration to mitigate this undesirable tendency and examines its effects.

\subsection{\label{sec:UQ_calib} STD calibration of DE models}

The results in Section \ref{sec:UQ_results} implied that although numerous previous studies argued that the DE models are well-calibrated, it is not always the case: even in this simple multi-output regression task in an engineering discipline. In this context, we proposed to apply the STD calibration method to the trained DE models. As in Algorithm \ref{alg:calib}, STD calibration is simple and practical in that it requires only a single for-loop, leveraging the already trained models without additional training. This property makes it reasonable to use the corresponding technique in our paper, which focuses on the application of DE to engineering fields where practicality is highly desired.

Algorithm \ref{alg:calib} first requires a set of candidates for scaling factors, $S$. Since the scaling factor of 1 corresponds to the case without calibration, the candidates $s$ are set around 1. Accordingly, $s=10^x$ are chosen as candidates, where $x$ are 100 uniformly distributed points from -2 to 0.18, so that the resulting range of scaling factors to explore is from 0.01 to 1.5. Note that with $s$ less than 1, underconfident models that overestimate the standard deviations (uncertainty) can be calibrated. Finally, the STD calibration is performed using calibration dataset split in Section \ref{sec:pre_missile} (dataset size of 980) and the optimized scaling factors for each DE model with respect to each output (QoI) are summarized in Table \ref{tab:cal_results}. The STD calibration for all models is performed within 60 seconds, which is negligible compared to their training time.

\renewcommand{\arraystretch}{1.1}
\begin{table}[htb!]
\caption{Optimized scaling factors for STD calibration.}\label{tab:cal_results}
    \begin{tabular*}{0.95\columnwidth}{@{\extracolsep{\fill}}lccccccc}
        \hline 
        \multirow{2}{*}{Methods} & \multicolumn{7}{c}{Optimized scaling factors} \\ 
        \cline{2-8}
        & $C_{NF}$ & $C_{AF}$ & $C_{PM}$ & $C_{RM}$ & $C_{YM}$ & $C_{SF}$ & \textbf{Avg} \\ \hline
        DE-2 & 0.549 & 1.061 & 0.608 & 1.009 & 0.824 & 0.578 & \textbf{0.771}\\
        DE-4 & 0.385 & 0.405 & 0.284 & 0.472 & 0.257 & 0.270 & \textbf{0.345}\\
        DE-8 & 0.147 & 0.270 & 0.133 & 0.270 & 0.155 & 0.140 & \textbf{0.186}\\
        DE-16 & 0.103 & 0.199 & 0.088 & 0.189 & 0.120  & 0.108 & \textbf{0.135}\\ \hline
    \end{tabular*}
\end{table}

Herein, the scaling factors for all six aerodynamic coefficients and their average value in each model are presented. The most notable point is that almost all $s$ values are less than 1 and they decrease as $M$ increases: see the bold values in Table \ref{tab:cal_results} to confirm their average trend. Taken together with the results from Section \ref{sec:UQ_results} that DE models overestimate their $\sigma^2$ (become underconfident) as $M$ increases, one might expect optimized $s<1$ to mitigate this underconfident tendency. And Fig. \ref{fig:reliab_DE_aft} proves that this actually happens: reliability plots of the DE models after STD calibration are drawn with test dataset. Note that the calibration dataset used during the STD calibration should not be reused in this validation process for generalization purposes. When compared to the previous plots in Fig. \ref{fig:reliab_DE_bef}, the obvious improvement due to the calibration technique can be observed. See \ref{sec:app_UQ_aft} for comprehensive results on the calibration effects with respect to all six QoIs.

\begin{figure}[htb!]
    \centering
    \begin{subfigure}[h]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/under_DE_simple_aft.png}
        \caption{}\label{fig:reliab_DE_aft_a}
    \end{subfigure}
    \hspace{0.0\columnwidth}
    \begin{subfigure}[h]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/err_DE_simple_aft.png}
        \caption{}\label{fig:reliab_DE_aft_b}
    \end{subfigure}
    \caption{Reliability plots of DE after STD calibration: (a) CI-based reliability plot, (b) Error-based reliability plot. The noticeable effects of STD calibration can be found when compared with the corresponding figure before STD calibration, Fig. \ref{fig:reliab_DE_bef}.}
    \label{fig:reliab_DE_aft}
\end{figure} 

Then, the quantitative effects of the calibration in terms of AUCE and ENCE will be analyzed, and from now on DE before and after calibration will be referred to as DE-bef and DE-aft, respectively. The AUCE and ENCE of the GPR will also be presented for the comparison, but please note that the GPR can be considered inherently STD-calibrated since its training algorithm already aims to minimize NLL as in the STD calibration process. This means that the GPR does not require additional STD calibration for a fair comparison with DE-aft because it can be seen as having already undergone STD calibration. Finally, the results are summarized in Fig. \ref{fig:slope_plot}. It consists of the sub-figures, where the row indicates each UQ metric, the column indicates each QoI, and the x-axis in each sub-figure indicates whether the DE undergoes STD calibration (as explained, GPR metrics have a constant value along the x-axis regardless of the STD calibration). Before the calibration, the AUCE (upper row) of DE models was overwhelmed by that of GPR. However, the STD calibration completely changes this situation: AUCE of all DE models for all aerodynamic QoIs decreases dramatically. Especially in the cases $C_{NF}$, $C_{YM}$ and $C_{SF}$, DE-aft clearly outperforms GPR. The significant improvement over ENCE (lower row) can also be verified. DE was already better than GPR without calibration, but the gap widens even more. Overall, the increasing trends of AUCE and ENCE for DE-bef with $M$ value are clearly shown as expected in Fig. \ref{fig:reliab_DE_bef}. In summary, vanilla DE outperformed GPR in terms of training efficiency and regression accuracy, but not in terms of quality of estimated uncertainty. However, when used with a simple post-hoc STD calibration (which requires negligible additional post-processing time), DE demonstrated its strong potential as an alternative to GPR in terms of training time, prediction accuracy, and UQ quality.

\begin{figure*}[htb!]
    \centering
        \includegraphics[width=0.95\textwidth]{figures/slope_plt.png}
    \caption{AUCE and ENCE of DE models before and after STD calibration. Those of GPR are also shown for comparison.}
    \label{fig:slope_plot}
\end{figure*} 

\clearpage
\subsection{\label{sec:UQ_BO} Effects of STD calibration on Bayesian optimization}

Since the scaling factors are optimized to have values less than 1 during the STD calibration process (Table \ref{tab:cal_results}), it is obvious that the overall predictive uncertainty of DE models would decrease. To provide a more intuitive understanding of the practical implications of calibration, this section extends DE models to Bayesian optimization. Specifically, the importance of calibration in DE is highlighted by contrasting the next query candidates obtained from Bayesian optimization before and after STD calibration.

Before moving on to Bayesian optimization, CIs of the 68\% confidence level predicted by DE-16 model are shown in Fig. \ref{fig:OOD} to visually understand the impact of calibration. Only one input variable, $AoA$, is used for the illustration. And its value is standardized to distinguish between its ID (in-distribution) region and the OOD (out-of-distribution) region: in Fig. \ref{fig:OOD}, the ID region is defined as the area containing 95$\%$ of the train data, while the OOD region is the remaining area. Overall, both results---those obtained before and after STD calibration---show diverging CIs in OOD and relatively narrow CIs in ID for all six QoIs. However, as expected from the scaling factors less than 1, the CIs from the DE-aft models become significantly narrower than those from the DE-bef models, indicating that these discrepancies will lead to differences in the subsequent Bayesian optimization process.


\begin{figure*}[htb!]
    \centering
        \includegraphics[width=0.8\textwidth]{figures/OOD_div.png}
    \caption{CIs of $68\%$ confidence level predicted by DE-16: comparison between before and after STD calibration.}
    \label{fig:OOD}
\end{figure*} 

A test problem is adopted to practically investigate their effects on Bayesian optimization. First, the multi-objective Bayesian optimization problem is defined: the maximization of both $C_{NF}$ and $C_{AF}$ within five varying input parameters ($Ma$, $\phi$, $\delta{p}$, $\delta{r}$, and $AoA$). These optimizations, coupled with the expected improvement (EI) acquisition function, are performed separately for DE-bef and DE-aft models. The former searches for the maximum EI point where EI is calculated from the uncertainty quantified by the DE-bef model, while the latter does so using the uncertainty quantified by DE-aft. Since the purpose of this section is to provide a brief insight into the impact of STD calibration on Bayesian optimization, only the first iteration is executed. To find the Pareto solutions of $EI(C_{NF})$ and $EI(C_{AF})$, the non-dominated sorting genetic algorithm-\uppercase\expandafter{\romannumeral2} (NSGA-\uppercase\expandafter{\romannumeral2}) in the Python package pymoo is utilized \citep{blank2020pymoo, yang2022inverse}. Finally, the obtained Pareto solutions from the first iteration are shown in Fig. \ref{fig:EIopt_a}. Since the uncertainty estimated by DE-bef and DE-aft are different as shown in Fig. \ref{fig:OOD}, the Pareto solutions of $EI(C_{NF})$ and $EI(C_{AF})$ are also different: EI values of both QoIs after calibration are much smaller than those before calibration.

In Bayesian optimization, however, the most valuable information to the user is not the EI value itself (Fig. \ref{fig:EIopt_a}). More important are the values of the input variable sets (Fig. \ref{fig:EIopt_b}) obtained from the EI Pareto solutions, since they are the next query candidates, the main purpose of implementing Bayesian optimization. Additional experiments/simulations will be performed on these candidate queries, indicating that their selection has a significant impact on the efficiency of the iterative Bayesian optimization process. If inappropriate candidates are obtained due to inaccurate UQ and therefore inaccurate EI calculation, the convergence of Bayesian optimization will be severely degraded. In this sense, the parallel coordinates plot (PCP) in Fig. \ref{fig:EIopt_b} shows how the first query candidates in Bayesian optimization can vary due to the STD calibration in the DE model. This PCP has five vertical lines corresponding to each input variable, and the y-axis indicates their standardized values. Each red/blue line represents each point of the Pareto solutions in Fig. \ref{fig:EIopt_a}. Comparing them, large variations are found especially in the input variable $\delta{r}$. That is, Bayesian optimization coupled with DE-bef discourages exploration of the variable $\delta{r}$, while DE-aft encourages exploration within $\delta{r}$. As it can be regarded that the input variable sets of the next query from DE-aft are more accurate (Section \ref{sec:UQ_calib}), the incorrect exploration trend of DE-bef will have significant side effects on the efficiency of the Bayesian optimization process. In conclusion, whether the DE is calibrated by STD calibration or not can result in completely different exploration characteristics when extended to Bayesian optimization, warning against blind application of vanilla DE models to Bayesian optimization in regression tasks.

\begin{figure*}[htb!]
    \centering
    \begin{subfigure}[h]{0.45\textwidth}
        \centering
            \includegraphics[height=0.55\linewidth]{figures/pareto.png}
        \subcaption{Pareto solutions obtained from multi-objective EI optimization: comparison before and after calibration}
        \label{fig:EIopt_a}
    \end{subfigure}% \qquad
    \hspace{0.05\columnwidth}
    \begin{subfigure}[h]{0.45\textwidth}
        \centering
            \includegraphics[height=0.55\linewidth]{figures/pcp.png}
        \subcaption{PCP of design variables in Pareto solutions: comparison before and after calibration}
        \label{fig:EIopt_b}
    \end{subfigure}
    \caption{Effects of STD calibration for DE models on Bayesian optimization results.}
    \label{fig:EIopt}
\end{figure*} 

\section{Conclusion}
\label{sec:conclusion}
This study comprehensively investigated the state-of-the-art approximate Bayesian inference approach, DE. It is applied to the multi-output regression task, which is the most common task in the engineering fields: a simple test case is adopted where aerodynamic QoIs of the specific missile configuration are predicted under varying flow conditions. DE models with different numbers of NNs are trained and then examined in the following order. First, their regression performance and the quality of estimated uncertainty are scrutinized while being compared with GPR. Then, a simple post-hoc STD calibration method is proposed to be applied to miscalibrated DE models. Finally, the effectiveness of the calibration on DE is highlighted by the improvement of two UQ quality criteria and the difference in Bayesian optimization results before and after calibration. The key findings of our study can be summarized as follows:

\begin{enumerate}

    \item The effect of the number of NNs used in ensemble, $M$, is comprehensively investigated in the simple multi-output regression task. For regression accuracy, DE models show superior performance to GPR in terms of RMSE and NLL, while showing indistinguishable differences among themselves. For UQ quality, however, they show the obvious trend toward underconfidence as $M$ increases, both in terms of AUCE and ENCE criteria. The mathematical proof of why DE tends to be miscalibrated in regression tasks is also derived.
    
    \item The post-hoc STD calibration method, which simply modifies the estimated uncertainty from DE, is proposed to be applied to miscalibrated DE models. Finally, the reliability of the UQ performance is dramatically improved after calibration for both AUCE and ENCE, also surpassing that of GPR.
    
    \item The impact of the calibration approach on the results of Bayesian optimization is examined. Finally, whether or not the DE is calibrated via STD calibration can result in completely different exploration characteristics when extended to Bayesian optimization, which cautions against blindly applying vanilla DE models to Bayesian optimization in regression tasks.
    
    \item We have demonstrated that by applying a simple post-hoc STD calibration technique that requires negligible additional post-processing time, DE models can have enormous potential compared to GPR, which is the most commonly used regression model for UQ in engineering. These results are summarized in Table \ref{tab:app_public}, where the DE-2 model after STD calibration outperforms GPR in terms of regression performance ($-150\%$ NLL $\&$ $-67\%$ RMSE), reliability of UQ ($-32\%$ AUCE $\&$ $-82\%$ ENCE), and training efficiency ($-86\%$ training time).
    
\end{enumerate}

% https://tex.stackexchange.com/questions/167366/combining-multirow-and-multicolumn
\renewcommand{\arraystretch}{1.3}
\begin{table}[htb!]
\caption{Comprehensive comparison between GPR and DE-2}\label{tab:app_public}
    \begin{NiceTabular*}{0.8\columnwidth}{@{\extracolsep{\fill}}lc!{\qquad}ccc}
        \cline{1-5}
        \Block[c]{2-2}{Metrics} && \Block{2-1}{GPR} & \Block{1-2}{DE-2} & \\ \cline{4-5}
        & & & Before calibration & After calibration \\ \cline{1-5}
        \Block{2-1}{Regression} & NLL & -1.657 ($-\%$) & -4.145 ($\downarrow\textbf{150}\%$) & -4.145 ($\downarrow\textbf{150}\%$)\\ \cline{2-5}
        & RMSE & 0.039 ($-\%$) & 0.013 ($\downarrow\textbf{67}\%$) & 0.013 ($\downarrow\textbf{67}\%$) \\ \cline{1-5}
        \Block{2-1}{UQ} & AUCE & 0.050 ($-\%$) & 0.076 ($\uparrow52\%$) & 0.034 ($\downarrow\textbf{32}\%$)\\ \cline{2-5}
        & ENCE & 0.905 ($-\%$) & 0.206 ($\downarrow77\%$) & 0.159 ($\downarrow\textbf{82}\%$)\\ \cline{1-5}
        \Block[c]{1-2}{Training time [s]} && 64080 ($-\%$) & 8640 ($\downarrow\textbf{87}\%$) & 8640+30 ($\downarrow\textbf{86}\%$) \\ \cline{1-5}                
\end{NiceTabular*}
\end{table}

The presented DE framework, which quantifies reliable predictive uncertainty, has great promise in two engineering applications. First, DE with STD calibration can replace the most common regression model, GPR, owing to its following advantages: more scalable to large datasets, higher regression accuracy, and last but not least, more reliable uncertainty estimation. Second, DE with STD calibration can be leveraged in Bayesian optimization by ensuring a reliable balance between exploitation and exploration due to its trustworthy UQ performance. Although the application of this framework has been demystified using the simple multi-output regression task, it can be easily applied and extended to numerous engineering disciplines since no special assumptions have been made for this specific problem. For future work, a more comprehensive investigation of DE models will be conducted, such as their scalability to other engineering regression problems. In addition, they will be applied to the entire Bayesian optimization framework and its comprehensive results will be examined, not only the first iteration as in this study.

\clearpage
\section*{CRediT authorship contribution statement}
\textbf{S. Yang}: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Writing – original draft, Writing – review \& editing, Visualization.
\textbf{K. Yee}: Supervision, Funding acquisition.

\section*{Declaration of competing interest}
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

\section*{Data availability}
Data will be made available on request.

\section*{Acknowledgments}
This work was supported by Grant UE191109CD from Agency for Defense Development and Defense Acquisition Program Administration. Also, the authors especially thank Shinkyu Jeong and Seungmin Yoo at Kyunghee University for providing the training dataset of the current study.

\clearpage
\appendix

\section{Controversial issues on MC-dropout}
\label{sec:app_MCD}

\citet{osband2016risk} pointed out that what MC-dropout (MCD) estimates is a risk, not an uncertainty, and also emphasized the pitfalls of MCD when used as a naive tool for estimating uncertainty. Moreover, its algorithm does not perform adequately even in very simple examples \citep{osband2016deep, pearce2018bayesian}, and its posterior samples are often too spiky to provide a reliable predictive uncertainty trend \citep{gal2016dropout, gal2016uncertainty, osband2016deep, riquelme2018deep, zhang2019quantifying}. Since Bayesian optimization uses the predictive uncertainty estimated by MCD during its process, MCD is considered inappropriate for Bayesian optimization in this manuscript.


\section{Hyperparameter tuning results in Section \ref{sec:pre_missile}}
\label{sec:app_hyp}

The results of the hyperparameter tuning performed in Section \ref{sec:pre_missile} are shown in Table \ref{tab:hyp_results}. Three hyperparameters are used: the number of hidden layers ($N_{layer}\in\{3, 5, 7\}$), the number of nodes in each hidden layer ($N_{node}\in\{32, 64, 128\}$), and the size of the mini-batch ($N_{batch}\in\{512, 1024, 2048\}$). The corresponding regression performance in terms of NLL and RMSE of all hyperparameter combinations are shown, and the best one is selected to be 7 hidden layers with 128 nodes and a mini-batch size of 512.

\renewcommand{\arraystretch}{1.1}
\begin{table}[H]
\caption{Results of hyperparameter tuning of the auxiliary NN used in the DE model.}\label{tab:hyp_results}
    \begin{tabular*}{0.45\columnwidth}{@{\extracolsep{\fill}}ccccc}
        \cline{1-5}
        % \begin{tabular}[c]{@{}c@{}}Layer\\$\#$\end{tabular} & \begin{tabular}[c]{@{}c@{}}Node\\$\#$\end{tabular} & \begin{tabular}{@{}c@{}}Mini-batch\\size\end{tabular} & \begin{tabular}[c]{@{}c@{}}NLL\end{tabular} & \begin{tabular}{@{}c@{}}RMSE\end{tabular}\\ \cline{1-5}
        \begin{tabular}[c]{@{}c@{}}$N_{layer}$\end{tabular} & \begin{tabular}[c]{@{}c@{}}$N_{node}$\end{tabular} & \begin{tabular}{@{}c@{}}$N_{batch}$\end{tabular} & \begin{tabular}[c]{@{}c@{}}NLL\end{tabular} & \begin{tabular}{@{}c@{}}RMSE\end{tabular}\\ \cline{1-5}
        % \begin{tabular}[c]{@{}l@{}}Mini-batch\\size\end{tabular} & NLL & RMSE  \\ \hline
        % \multirow{2}{*}{Layer $\#$} & \multirow{2}{*}{Node $\#$} & \multirow{2}{*}{Mini-batch size} & \multirow{2}{*}{NLL} & \multirow{2}{*}{RMSE}  \\ 
        % &&&& \\ \hline
        \multirow{9}{*}{3} & \multirow{3}{*}{32} & 512 & -2.38 & 0.18 \\ \cline{3-5}
         &  & 1024 & -2.25 & 0.21 \\ \cline{3-5}
         &  & 2048 & -2.15 & 0.18 \\ \cline{2-5}
         & \multirow{3}{*}{64} & 512 & -2.95 & 0.09 \\ \cline{3-5}
         &  & 1024 & -2.77 & 0.08 \\ \cline{3-5}
         &  & 2048 & -2.59 & 0.09 \\ \cline{2-5}
         & \multirow{3}{*}{128} & 512 & -3.61 & 0.04 \\ \cline{3-5}
         &  & 1024 & -3.18 & 0.04 \\ \cline{3-5}
         &  & 2048 & -2.83 & 0.05 \\ \hline
        \multirow{9}{*}{5} & \multirow{3}{*}{32} & 512 & -2.57 & 0.12 \\ \cline{3-5}
         &  & 1024 & -2.28 & 0.13 \\ \cline{3-5}
         &  & 2048 & -2.16 & 0.14 \\ \cline{2-5}
         & \multirow{3}{*}{64} & 512 & -0.18 & 0.07 \\ \cline{3-5}
         &  & 1024 & -2.68 & 0.07 \\ \cline{3-5}
         &  & 2048 & -1.35 & 0.08 \\ \cline{2-5}
         & \multirow{3}{*}{128} & 512 & -3.03 & 0.03 \\ \cline{3-5}
         &  & 1024 & -3.47 & 0.03 \\ \cline{3-5}
         &  & 2048 & -1.64 & 0.11 \\ \hline
        \multirow{9}{*}{\textbf{7}} & \multirow{3}{*}{32} & 512 & -2.51 & 0.11 \\ \cline{3-5}
         &  & 1024 & -2.38 & 0.15 \\ \cline{3-5}
         &  & 2048 & -2.25 & 0.10 \\ \cline{2-5}
         & \multirow{3}{*}{64} & 512 & -2.98 & 0.05 \\ \cline{3-5}
         &  & 1024 & -2.88 & 0.05 \\ \cline{3-5}
         &  & 2048 & -2.64 & 0.06 \\ \cline{2-5}
         & \multirow{3}{*}{\textbf{128}} & \textbf{512} & \textbf{-3.68} & \textbf{0.02} \\ \cline{3-5}
         &  & 1024 & -3.36 & 0.02 \\ \cline{3-5}
         &  & 2048 & -3.21 & 0.04 \\ \cline{1-5}
    \end{tabular*}
\end{table}

\clearpage
\section{Additional results in Section \ref{sec:UQ_results}}
\label{sec:app_UQ_bef}

Fig. \ref{fig:reliab_DE_bef} in Section \ref{sec:UQ_results} shows the reliability plots with respect to only one QoI, $C_{SF}$. In this section, more comprehensive results are provided. For each vanilla DE model, all six QoIs are shown with different colors. Again, the trend of underconfidence as $M$ increases can be seen from DE-2 to DE-16.

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[h]{0.8\textwidth}
        \centering
        \includegraphics[height=0.18\textheight]{figures/under_DE2.png}
        \hspace{0.0\columnwidth}
        \includegraphics[height=0.18\textheight]{figures/err_DE2.png}
        \caption{DE-2}\label{fig:comp_NN_a}
    \end{subfigure}
    \vfill
    \begin{subfigure}[h]{0.8\textwidth}
        \centering
        \includegraphics[height=0.18\textheight]{figures/under_DE4.png}
        \hspace{0.0\columnwidth}
        \includegraphics[height=0.18\textheight]{figures/err_DE4.png}
        \caption{DE-4}\label{fig:comp_NN_b}
    \end{subfigure}
    \vfill
    \begin{subfigure}[h]{0.8\textwidth}
        \centering
        \includegraphics[height=0.18\textheight]{figures/under_DE8.png}
        \hspace{0.0\columnwidth}
        \includegraphics[height=0.18\textheight]{figures/err_DE8.png}
        \caption{DE-8}\label{fig:comp_NN_c}
    \end{subfigure}
    \vfill
    \begin{subfigure}[h]{0.8\textwidth}
        \centering
        \includegraphics[height=0.18\textheight]{figures/under_DE16.png}
        \hspace{0.0\columnwidth}
        \includegraphics[height=0.18\textheight]{figures/err_DE16.png}
        \caption{DE-16}\label{fig:comp_NN_d}
    \end{subfigure}
    \caption{Reliability plots of vanilla DE models: (left) CI-based reliability plots, (right) error-based reliability plots.}
    \label{fig:reliab_DE_comprehensive}
\end{figure*}

\clearpage
\section{Additional results in Section \ref{sec:UQ_calib}}
\label{sec:app_UQ_aft}

In \ref{sec:app_UQ_bef}, the reliability plots of DE models before STD calibration (vanilla DE models) are shown; this section shows the results after STD calibration. It is shown that all DE models become well-calibrated after calibration, even for the DE-16 model, which was the most miscalibrated DE model.

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[h]{0.8\textwidth}
        \centering
        \includegraphics[height=0.18\textheight]{figures/NLLcal_DE2.png}
        \hspace{0.0\columnwidth}
        \includegraphics[height=0.18\textheight]{figures/err_NLL_DE2.png}
        \caption{DE-2}\label{fig:calcomp_NN_a}
    \end{subfigure}
    \vfill
    \begin{subfigure}[h]{0.8\textwidth}
        \centering
        \includegraphics[height=0.18\textheight]{figures/NLLcal_DE4.png}
        \hspace{0.0\columnwidth}
        \includegraphics[height=0.18\textheight]{figures/err_NLL_DE4.png}
        \caption{DE-4}\label{fig:calcomp_NN_b}
    \end{subfigure}
    \vfill
    \begin{subfigure}[h]{0.8\textwidth}
        \centering
        \includegraphics[height=0.18\textheight]{figures/NLLcal_DE8.png}
        \hspace{0.0\columnwidth}
        \includegraphics[height=0.18\textheight]{figures/err_NLL_DE8.png}
        \caption{DE-8}\label{fig:calcomp_NN_c}
    \end{subfigure}
    \vfill
    \begin{subfigure}[h]{0.8\textwidth}
        \centering
        \includegraphics[height=0.18\textheight]{figures/NLLcal_DE16.png}
        \hspace{0.0\columnwidth}
        \includegraphics[height=0.18\textheight]{figures/err_NLL_DE16.png}
        \caption{DE-16}\label{fig:calcomp_NN_d}
    \end{subfigure}
    \caption{Reliability plots of DE models after STD calibration: (left) CI-based reliability plots, (right) error-based reliability plots.}
    \label{fig:reliab_DE_cal}
\end{figure*}



\bibliographystyle{elsarticle-harv} 
\bibliography{cas-refs}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

% \begin{thebibliography}{00}

% %% \bibitem[Author(year)]{label}
% %% Text of bibliographic item

% \bibitem[ ()]{}

% \end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.
