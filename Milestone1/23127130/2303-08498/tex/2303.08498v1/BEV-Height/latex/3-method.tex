\section{Method}
\input{BEV-Height/latex/fig/histogram-depth-height}
\input{BEV-Height/latex/fig/correlation-depth-height}
% In this section, we introduce our proposed \name{} in detail. 
We first give a brief problem definition of camera-only 3D object detection on roadside. We then analyze the downside of predicting depth that is widely-adopted in current camera-only methods and show the benefit of using height instead. Subsequently, we present our framework in detail. 
% and the generalization assessments under the camera's external parameters disturbed setting in Sec.~\ref{sec:problem_definition}. 
% Then, By analyzing the training data, we justify the superiority of height estimation over the depth in constructing 3D features from the roadside view in Sec.~\ref{sec:delving_into_the_height}. Subsequently, we give an overview of the whole framework and clarify our key contributions including height network and the projection process from 2D to 3D in Sec.~\ref{sec:BEVHeight}.
\input{BEV-Height/latex/fig/framework}

\subsection{Problem Definition}
\label{sec:problem_definition}
In this work, we would like to detect a three-dimensional bounding box of given foreground objects of interest. Formally, we are given the image $I\in R^{H\times W\times 3}$ from the roadside cameras, whose extrinsic matrix $E\in R^{3\times 4}$ and intrinsic matrix $K\in R^{3\times 3}$ can be obtained via camera calibration. 
We seek to precisely detect the 3D bounding boxes of objects on the image. We denote all bounding boxes of this image as $B=\left\{B_1,B_2,…,B_n\right\}$, and the output of detector as $\hat{B}$. 
% represents the annotations for each image used in the training stage. The detection results are defined
% as $\hat{B}_{ego}=\left\{\hat{B}_1,\hat{B}_2,…,\hat{B}_n\right\}$. 
Each 3D bounding box $B_{i}$ can be formulated as a vector with 7 degrees of freedom:
\begin{equation}
    \hat{B}_{i} = \left(x, y, z, l, w, h, \theta \right)
    \label{con:eq1}
\end{equation}
where $\left(x,y,z \right)$ is the location of each 3D bounding box . $\left(l,w,h \right)$ denotes the cuboid's length, width and height respectively. $\theta$ represents the yaw angle of each instance with respect to one specific axis. Specifically, a camera-only 3D object detector $F_{Det}$ can be defined as follows:
\begin{equation}
    \hat{B}_{ego} = F_{Det}\left(I_{cam}\right)
    \label{con:eq2}
\end{equation}
As a common assumption in autonomous driving, we assume the camera pose parameters $E$ and $K$ are known after the initial installation. In the roadside perception domain, people usually rely on multiple cameras installed at different locations to enlarge the perception range. This naturally encourages adopting those multi-view perception methods though the feature maps are not aligned geologically. Note that, although there are certain roadside units are equipped with other sensors, we focus on camera-only settings in this work for generalization purpose. 
% \KY{try to link why we focus on bev methods.}
% Meanwhile, 
% We expect the 3D object detector to maintain detection accuracy when the camera's posture changes inevitably and is calibrated.
% \ky{}

\subsection{Comparing the depth and height}
% \subsection{Delving into the Height}
\label{sec:delving_into_the_height}
As discussed before, state-of-the-art BEV camera-only methods first project the features into the bird's eye view space to reduce the z-axis dimension, then let the network learn implicitly \cite{liu2022petr, liu2022petrv2, li2022bevformer} or explicitly \cite{huang2021bevdet, li2022bevdepth, li2022bevstereo} about the 3D location information. Motivated by previous approaches in RGB-D recognition, one naive approach is to leverage the per-pixel depth as a location encoding.
In \cref{fig:histogram-depth-height}~(a), current methods firstly use an encoder to transform the original image into 2D feature maps. After predicting the per-pixel depth of the feature map, each pixel feature can be lifted into 3D space and zipped in the BEV feature space by voxel pooling techniques. 

However, we discover that using depth may be sub-optimal under the face-forwarding camera settings in autonomous driving scenarios. 
Specifically, we leverage the LiDAR point clouds of the DAIR-V2X-I~\cite{yu2022dair} dataset, where we first project these points to the images, to plot the histogram of per-pixel depth in \cref{fig:histogram-depth-height}~(b). We can observe a large range from 0 to 200 meters. By contrast, we plot the histogram of the per-pixel height to the ground and clearly observe the height ranges from -1 to 2m respectively, 
% We hypothesize this is easier for the network to predict.
which is easier for the network to predict.
But in practice, the predicted height can't be employed directly to the pinhole camera model like depth. 
How to achieve the projection from 2D to 3D effectively through height has not been explored.
% We ingeniously designed a virtual coordinate system and a reference plane (refer \cref{alg:algorithm} for details), which help us achieve the projection from 2D to 3D effectively. 


% To illustrate the benefits of the distribution of height to over that of depth, we compare the depth and height from the ground distribution of annotated instances from DAIR-V2X-I\cite{yu2022dair} Dataset in two aspects: (1) Histogram Distribution. (2) the distribution under the extrinsic disturbance.

% \mypara{Histogram Distribution.} Based on the 3D annotations of DAIR-V2X-I dataset, we analyze the histogram distribution of their depth and the height from the ground. As shown in \cref{fig:three}, height and depth approximately follow a normal distribution. The variance of height distribution is less than one ten-thousandth of that of the depth, which indicates that height estimation is more straightforward than depth.

\mypara{Analysis when extrinsic parameter changes.}
In \cref{fig:five}~(a), we provide an visual example of extrinsic disturbance.
To show that predicting height is superior to depth, we plot the scatter graph to show the correlation between the object's row coordinates on the image and its depth and height. Each plot represent an instance. As shown in \cref{fig:five}~(b). we observe that objects with smaller depths have a smaller $v$ value. However, suppose the extrinsic parameter changes; we plot the same metric in blue and observe that these values are drastically different from the clean setting. In that case, i.e., there is only a small overlap between the clean and noisy settings. We believe this is why the depth-based methods perform poorly when external parameters change. On the contrary, as observed in \cref{fig:five}~(c), the distribution remains similar regardless of the external parameter changes, i.e. the overlap between orange and blue dots is large. This motivates us to consider using height instead of depth. However, unlike depth that can be directly lifted to the 3D space via camera model, directly predicting height will not work to recover the 3D coordinate. Later, we present a novel height-based projection module to address this issue.


% we visualize the correlation between the image row coordinates of the object and its corresponding depth and height in the form of a scatter diagram in \cref{fig:five}. As shown, after adjusting the roll and pitch angle, the depth distribution only slightly overlaps before, while the height distribution has a much larger intersection. This phenomenon indicates that the correlation between the object's position on the image and its height from the ground is less affected by the roll and pitch perturbations. Thus, the height-based BEV detection method is less susceptible to the camera's external parameter perturbations.

\subsection{BEVHeight}
\label{sec:BEVHeight}

\mypara {Overall Architecture.}
As shown in Fig.\ref{fig:framework},  our proposed BEVHeight framework consists of five main stages.
% Image-view Encoder, HeightNet, $2D\rightarrow 3D$ projector, Voxel Pooling, and 3D Detection Head. 
 The image-view encoder that is composed of a 2D backbone and an FPN module aims to extract the 2D high-dimensional multi-scale image features $F^{2d} \in R^{C_F\times \frac{H}{16} \times \frac{W}{16}}$ given an image $I\in R^{3\times H \times W}$ in roadside view, where $C_F$ denotes the channel number. $H$ and $W$ represent the input image's height and width, respectively.
 The HeightNet is responsible for predicting the bins-like distribution of height from the ground $H^{pred} \in R^{C_H \times \frac{H}{16} \times \frac{W}{16}}$ and the context features $F^{context}\in R^{C_c \times \frac{H}{16}\times \frac{W}{16}}$ based on the image fractures $F^{2d}$, where $C_H$ stands for the number of height bins, $C_c$ denotes the channels of the context features. The fused features $F^{fused}$ that combines image context and height distribution is generated using Eq.~\ref{con:eq3}.
 The height-based $2D\rightarrow 3D$ projector pushes the fused features $F^{fused}$ into the 3D wedge-shaped features $F^{wedge} \in R^{X \times Y \times Z \times C_{c}}$ based on the predicted bins-like height distribution $H^{pred}$. See Algorithm \cref{alg:algorithm} for more details.
 Voxel Pooling transforms the 3D wedge-shaped features into the BEV features $F^{bev}$ along the height direction. 
 3D detection head firstly encodes the BEV features with convolution layers, And then predicts the 3D bounding box consisting of location $\left(x, y, z\right)$, dimension$\left(l, w, h\right)$, and orientation $\theta$.

\begin{equation}
    \begin{aligned}
        F^{fused} = F^{context}\otimes H^{pred},\\
        F^{fused} \in R^{C_c \times C_H \times \frac{H}{16} \times \frac{W}{16}}
    \end{aligned}
    \label{con:eq3}
\end{equation}

\mypara {HeightNet.}
% Our HeightNet follows the general design in BEVDepth~\cite{li2022bevdepth},
Motivated by the DepthNet in BEVDepth~\cite{li2022bevdepth}, we leverage a Squeeze-and-Excitation layer to generate the context features  $F^{context}$ from the 2D image features $F^{2d}$. Concretely, we stack multiple residual blocks~\cite{he2016deep} to increase the representation power and then use a deformable convolution layer~\cite{XizhouZhu2018DeformableCV} to predict the per-pixel height. We denote this height module as $H^{pred}$. To facilitate the optimization process, we translate the regression task to use one-hot encoding, i.e. discretizing the height into various height bins. The output of this module is  $h\in R^{C_H\times 1\times 1}$. Moreover, previous depth discretization strategies~\cite{HuanFu2018DeepOR,YunleiTang2020Center3DCM} are generally fixed and thus not suitable for roadside height predictions. To this end, we present an
% However, these methods' discretization bins are fixed and can't be adaptively adjusted according to the concerned height ranges in roadside scenarios. In this way, we adopt an
dynamic discretization as follow:
\begin{equation}
    h_i =  \lfloor{N \times \sqrt[\alpha]{ \frac{h-h_{min}}{h_{max}-h_{min}}}}\rfloor,
    \label{con:eq4}
\end{equation}
where $h$ represents the continuous height value from the ground, $h_{min}$ and $h_{max}$ represent the start and end of the height range. $N$ is the number of height bins, and $h_i$ denotes the value of $i-th$ height bin. $H$ is the height of the roadside camera from the ground. $\alpha$ is the hype parameter to control the concentration of height bins. See the supplementary material for more details.
% \KY{But honestly, i do not think this paragraph make sense by showing the sampling. Consider show a failure case of previous strategies and ours. otherwise I would suggest to combine this with previous height net. }

% whe context branch that 
% consists of a SE-like layer are used to generate the context features $F^{context}$ from the 2D image features $F^{2d}$. 
% The height layers composed of 
% stacked multiple Residual Blocks~\cite{he2016deep} and a Deformable Conv ~\cite{XizhouZhu2018DeformableCV} are responsible for predicting the height distribution $H^{pred}$. Each pixel value on the height distribution map is represented as $h\in R^{C_H\times 1\times 1}$, this means the probability distribution belongs to the pixel's discrete height bins. 


\input{BEV-Height/latex/table/alg}
% \input{BEV-Height/latex/fig/2d3d}

\mypara{Height-based 2D-3D projection module.} 
Unlike the ``lift'' step in previous depth-based methods, one cannot recover the 3D location with only height information. To this end, we design a novel 2D to 3D projection module to push the fused features $F^{fused} \in R^{C_H \times C_c \times \frac{H}{16} \times \frac{W}{16}}$ into the wedge-shaped volume feature $F^{wedge} \in R^{X\times Y\times Z\times C_{c}}$ in the ego coordinate system.
% The 2D to 3D projector is applied to push the fused features $F^{fused} \in R^{C_H \times C_c \times \frac{H}{16} \times \frac{W}{16}}$ into the wedge-shaped volume feature $F^{wedge} \in R^{X\times Y\times Z\times C_{c}}$ in the ego coordinate system. 
% \ky{Unlike the} ``lift" step in previous depth-based methods, the height-fused features  $F^{fused}$ need ingenious transformations. 
As illustrated in \cref{fig:framework} and \cref{alg:algorithm}, we design a virtual coordinate system, with the origin coinciding with that of the camera coordinate system and the Y-axis perpendicular to the ground, and a special reference plane parallel to the image plane with a fixed distance 1. 

For each point $p_{image} = (u, v)$ in the image plane, we first choose the associated point $p_{ref}$ in the reference plane $plane_{ref}$, whose depth is naturally 1, i.e, $d_{ref}=1$. Thus we can project $p_{ref}$ from the uvd space to the camera coordinate through the camera's intrinsic matrix:
\begin{equation}
P_{ref}^{cam}=K^{-1} d_{ref} [u,v,1]^T = K^{-1} [u,v,1]^T . 
\end{equation}
Further, it can be transformed to the virtual coordinate to get $P_{ref}^{virt.}$ with the transformation matrix $T_{cam}^{virt.}$: 
\begin{equation}
    P_{ref}^{virt.} = T_{cam}^{virt.} P_{ref}^{cam}.
\end{equation}
Now we can know the point $p_{ref}$ in our virtual coordinate is $P_{ref}^{virt.}$.
% $P_{ref}^{virt.} = (x_{ref}^{virt.}, y_{ref}^{virt.}, z_{ref}^{virt.})$. 
Suppose the $i-th$ value in height bins relative to the ground for point $p_{image}$ is $h_i$ and the height from the origin of the virtual coordinate system to the ground is $H$. Based on similar triangle theory, we can have the $i-th$ projected 3D point in height virtual coordinate for $p_{image}$:
\begin{equation}
P_{i}^{virt.}=\frac{H-h_i}{y_{ref}^{virt.}} P_{ref}^{virt.} .
\end{equation}
Finally, we transform the $P_{i}^{virt.}$ to the ego-car space:
\begin{equation}
P_{i}^{ego}=T_{virt.}^{ego} P_{i}^{virt.} .
\end{equation}

% Summarize the above process, we can finish the 2D to 3D projection through:
In summary, the contribution of our module is in two-fold: i) we design a virtual coordinate system that leverages the height from the HeightNet; ii) we adopt a reference plane to simplify the computation by setting a constant depth to 1.  We formulate the height-based 2D-3D projection as follow:
\begin{equation}
    P_{i}^{ego}=T_{virt.}^{ego} \frac{H-h_i}{y_{ref}^{virt.}} T_{cam}^{virt.} K^{-1} [u,v,1]^T.
\end{equation}

% The arrows in \cref{fig:2dto3d} and the loop in \cref{alg:algorithm} also illustrate this projection is well-designed and efficient.

% Rethinking this process, there are two key elements worth aftertaste again: first one is the virtual coordinate system, which allows the HeightNet to directly predict the height with small variance relative to the ground; Another one is the reference plane, which simplifies the whole processing and expression through constant depth of 1.
% The overall process of 2D to 3D projector is depicted in \cref{alg:algorithm}.


% \mypara{Height discretization.}
% \KY{Should we group this with height net?}
% The height discretization can be performed with uniform discretization (UD) with a fixed bin size, spacing-increasing discretization (SID)~\cite{HuanFu2018DeepOR} with increasing bin sizes in logspace, or linear-increasing discretization (LID) ~\cite{YunleiTang2020Center3DCM} with linearly increasing bin sizes.
% The above four height discretization techniques are visualized in Fig. \ref{fig:seven}.
% Height supervision comes from the object-level annotations $B_{ego}$ and the optional ground-truth $H^{gt}$ derived from point clouds. \tao{this sentence mean?} \KY{I think we should remove this.}