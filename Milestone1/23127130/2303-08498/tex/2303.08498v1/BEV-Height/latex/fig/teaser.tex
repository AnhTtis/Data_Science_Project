%figure1

\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
\centering
\captionsetup{type=figure}
\vspace{-0.38cm}
% \input{fig/teaser.tex}
\begin{center}
\includegraphics[width=0.99\textwidth]{BEV-Height/figures/teaser_intro3_rectify.pdf}
\end{center}
\setlength{\belowcaptionskip}{-0.005cm} 
\captionof{figure}{
\textbf{(a)} To produce 3D bounding boxes out of a monocular image, state-of-the-art methods firstly predict the per-pixel depth either explicitly or implicitly to determine the 3D location of foreground objects with the background. However, when we plot the per-pixel depth on the image, we notice that the differences between points on the car roof and surrounding ground quickly shrink when the car moves away from the camera, making it sub-optimal to optimize especially for far objects. \textbf{(b)} On the contrary, we plot the per-pixel height to the ground and observe that such difference remains agnostic regardless of the distance, and visually is superior for the network to detect objects. However, one cannot directly regress the 3D location by solely predicting the height.  \textbf{(c)} To this end, we propose a novel framework, \name{} to address this issue. Empirical results reveal that our method surpasses the best method by a margin of 4.85\% on clean settings and over 26.88\% on noisy settings.
% Without loss of generality, we mark two point, one on the car roof, another on the nearest ground plane, and denote the depth of camera center to these points as $d$ and $d'$. 
% \textbf{(a)} We visualize per-pixel depth and height of road-side cameras. State-of-the-art  To effectively differentiate the point on the vehicle and nearest ground, 
% % state-of-the-art methods rely on depth as a supervision, i.e. to first predict the depth then recover the 3D geometry. 
% state-of-the-art methods rely on implicitly or explicity depth, i.e. to first predict the depth then recover the 3D geometry. 
% However, the depth difference will drastically shrink to zero when the distance between the vehicle and camera increases. This potentially is sub-optimal to . Motivated by the residual learning regime, we propose a novel framework that predicts height instead of depth, where the difference between car and ground remains unchanged regardless the distance. As analyzed later, our framework significantly ease the optimization process and is more robust. \textbf{(c)} 
}
\label{fig:teaser}
\end{center}%
}]


% \begin{figure*}[!t]
% 	\centering
% 	\includegraphics[width=\textwidth]{BEV-Height/figures/intro.pdf}
% 	\caption{\textbf{(a)} Visualize per-pixel depth and height of road-side cameras. \textbf{(b)} To effectively differentiate the point on the vehicle and nearest ground, state-of-the-art methods rely on depth as a supervision, i.e. to first predict the depth then recover the 3D geometry. However, the depth difference will drastically shrink to zero when the distance between the vehicle and camera increases, thus is sub-optimal to learn. On the contrary, we propose a novel framework to regress height, where the difference between car and ground remains agnostic to the distance and is more robust. \textbf{(c)} Empirical results reveal that our method surpass previous best method by a margin of 3\% on clean setting and over 20\% on noisy settings. }
% \label{fig:teaser}
% \end{figure*}


% %figure1
% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=8.5cm]{BEV-Height/figures/Teaser.pdf}
% 	\caption{\textbf{The comparison of depth and height distribution.}}
% \label{fig:one}
% \end{figure}

% %figure2
% \begin{figure}[htbp]
% 	\centering
% 	\includegraphics[width=8.5cm]{BEV-Height/figures/perspective_principle.pdf}
% 	\caption{\textbf{The comparison of depth and height distribution.}}
% \label{fig:two}
% \end{figure}
