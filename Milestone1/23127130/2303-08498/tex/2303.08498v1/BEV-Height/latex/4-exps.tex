\section{Experiments}
We briefly introduce the experiment settings and two benchmark datasets in road-side perception domain. We then compare our proposed \name{} with state-of-the-art methods under clean and noisy camera settings. We ablate our methods in detail and discuss the limitations. 
% In this section, we first introduce our experimental setups. Then, we conduct comprehensive experiments to validate the effects of our \name{} on two large-scale roadside datasets, DAIR-V2X~\cite{yu2022dair} and Rope3D~\cite{ye2022rope3d}. Finally, we also give an analysis on the popular vehicle perception dataset, nuScenes~\cite{caesar2020nuscenes}.

\subsection{Datasets}
\mypara{DAIR-V2X.} Yu \etal~\cite{yu2022dair} introduces a large-scale, multi-modality dataset. As the original dataset contains images from vehicles and roadside units, this benchmark consists of three tracks to simulate different scenarios. Here, we focus on the DAIR-V2X-I, which only contains the images from mounted cameras to study roadside perception. Specifically, DAIR-V2X-I contains around ten thousand images, where 50\%, 20\% and 30\% images are split into train, validation and testing respectively. However, up to now, the testing examples are not yet published, we evaluate the results on the validation set. We follow the benchmark to use the average perception of the bounding box as in KITTI~\cite{geiger2012we}.

% \mypara{DAIR-V2X~\cite{yu2022dair}} is a large-scale, multi-modality vehicle-infrastructure collaborative dataset for 3D object detection, which is composed of three sub-datasets: DAIR-V2X-C, DAIR-V2X-I, DAIR-V2X-V. DAIR-V2X-C containing sensors from both vehicle and roadside is for the vehicle-infrastructure cooperative 3D object detection. DAIR-V2X-V including sensors on the vehicle is for the 3D object detection in autonomous driving. DAIR-V2X-I with only sensors on infrastructure, is designed for the roadside perception task. In this paper, we conduct experiments based on the DAIR-V2X-I. DAIR-V2X-I contains 10084 samples, which are further divided into train/val/test according to 5:2:3 respectively. Since the test set is not released yet, all experiments are trained on the train set and evaluated on val set. The evaluation metric is average precision (AP) as used in the KITTI\cite{geiger2012we} dataset.

% Rope3D
\mypara{Rope3D\cite{ye2022rope3d}.} 
There is another recent large-scale benchmark named Rope3D. It contains over 500k images with three-dimensional bounding boxes from seventeen intersections. Here, we follow the proposed homologous setting to use 70\% of the images as training, and the remaining as testing. Note that, all images are randomly sampled. For validation metrics, we leverage the AP$_{\text{3D}{|\text{R40}}}$~\cite{simonelli2019disentangling} and the Rope$_\text{score}$, which is a consolidated metric of the 3D AP and other similarities metrics, such as average area similarity.

% is a large-scale dataset dedicated to vision-based roadside 3D object detection. This dataset provides 50009 frames of images together with 3D annotations. All images are sampled from seventeen intersections and split into the train/val set according to 7:3 under the Homologous setting. The proposed AP$_{\text{3D}{|\text{R40}}}$ and Rope$_\text{score}$ are used as the metrics.

% \mypara{nuScenes\cite{HolgerCaesar2019nuScenesAM}} is a large-scale autonomous-driving dataset for 3D detection, consisting of 700, 150 and 150 scenes for training, validation, and testing, respectively. 
% Each frame contains one point cloud and six calibrated images that cover 360 fields-of-view. 
% metric
% For 3D detection, the main metrics are mean Average Precision (mAP) and nuScenes detection score (NDS). 
% The mAP is defined by the BEV center distance with thresholds of {0.5m, 1m, 2m, 4m}, instead of the IoUs of bounding boxes. 
% NDS is a consolidated metric of mAP and other metric scores, such as average translation error and average scale error.

\subsection{Experimental Settings}
For architecture details, we use ResNet-101\cite{he2016deep} as image-view encoder in results compared with state-of-the-art and ResNet-50 for other ablation studies.
The input resolution is in (864, 1536). For data augmentation, we follow \cite{li2022bevdepth} to use random scaling and rotation in the 2D space only. All methods are trained for 150 epochs with AdamW optimzer~\cite{loshchilov2017adamw}, where the initial learning rate is set to $2e-4$.
% We use ResNet\cite{he2016deep} as the image backbone. For DAIR-V2X-I and Rope3D datasets, the image size is processed to 864x1536; we adopt random translation, random scaling, and random rotation as image data augmentations. No BEV data augmentations are applied. All results are trained for 120 epochs with AdamW optimizer and learning rate set to 2e-4. 

\input{BEV-Height/latex/table/dair_v2.tex}

\subsection{Comparing with state-of-the-art}
\mypara{Results on the original benchmark.} On DAIR-V2X-I setting, we compare our BEVHeight with other state-of-the-art methods like ImvoxelNet~\cite{rukhovich2022imvoxelnet}, BEVFormer~\cite{li2022bevformer}, BEVDepth~\cite{li2022bevdepth} on DAIR-V2X-I val set. Some results of LiDAR-based and multimodal methods reproduced by the original DAIR-V2X~\cite{yu2022dair} benchmark are also displayed.
As can be seen from Tab.~\ref{dair_sota_2}, the proposed \name{} surpasses state-of-the-art methods by a significant margin of 2.19\%, 5.87\% and 4.61\% in vehicle, pedestrian and cyclist categories respectively.

% \mypara{Rope3D val set.}
On Rope3D dataset, we also compare our BEVHeight with other leading BEV methods, such as BEVFormer~\cite{li2022bevformer} and BEVDepth~\cite{li2022bevdepth}. Some results of the monocular 3D object detectors are revised by adapting the ground plane. As shown in Tab.~\ref{tab_performance_overall},  
we can see that our method outperforms all BEV and monocular methods listed in the table. In addition, under the same configuration, our BEVHeight outperforms the BEVDepth by $4.97\%$ / $4.02\%$, $3.91\%$ / $3.06\%$ on AP$_{\text{3D}{|\text{R40}}}$ and Rope$_\text{score}$  for car and big vehicle respectively.

\input{BEV-Height/latex/table/rope3d}
% 地面方程咋用的

\mypara{Results on noisy extrinsic parameters.}
% To verify the robustness of our BEVHeight when the camera's extrinsic matrix is changed inevitably. 
In the realistic world, camera parameters frequently change for various reasons. Here we evaluate the performance of our framework in such noisy settings. We follow \cite{yu2022benchmarking} to simulate the scenarios that external parameters are changed. Specifically, we introduce a random rotational offset in normal distribution $N(0, 1.67)$ along the roll and pitch directions as the mounting points usually remain unchanged.  
% \KY{degree of what?}

% This situation often occurs during the maintenance of roadside cameras. In this case, the camera's extrinsic matrix will differ from its previous state when the labeled data is collected. The generalization to the camera's mount position disturbance is a great challenge for the existing methods.
During the evaluation, we add the rotational offset along roll and pitch directions to the original extrinsic matrix. The image is then applied with rotation and translation operations to ensure the calibration relationship between the new external reference and the new image. Examples are given in Sec.~\ref{sec:visualization_results}.
As shown in Tab.~\ref{dair_robust}, the performance of the existing methods degrades significantly when the camera's extrinsic matrix is changed. Take $\text{Vehicle}_{(IoU=0.5)}$ for example, the accuracy of BEVFormer~\cite{li2022bevformer} drops from 50.73\% to 16.35\%. The decline of BEVDepth~\cite{li2022bevdepth} is from 60.75\% to 9.48\%, which is pronounced. Compared with the above methods, Our BEVHeight maintains 51.77\% from the original 63.49\%, which surprises the BEVDepth by 42.29\% on vehicle category.

\mypara{Visualization Results.}
\input{BEV-Height/latex/fig/visualization}
\label{sec:visualization_results}
As shown in Fig.~\ref{fig:visualization}, we present the results of BEVDepth~\cite{li2022bevdepth} and our BEVHeight in the image view and BEV space, respectively. The above two models are not applied with data augmentations in the training phase. From the samples in (a), we can see that the predictions of BEVHeight fit more closely to the ground truth than that of BEVDepth. As for the results in (b), under the disturbance of roll angle, there is a remarkable offset to the far side relative to the ground truth in BEVDepth detections. In contrast, the results of our method are still keeping the correct position with ground truth.  Moreover, referring to the predictions in (c), BEVDepth can hardly identify far objects, but our method can still detect the instance in the middle and long-distance ranges and maintain a high IoU with the ground truth.

\input{BEV-Height/latex/table/disturb}
\input{BEV-Height/latex/table/discretization}
\subsection{Ablation Study}
% \mypara{Robust to Extrinsic Disturbance.}

\mypara{Dynamic Discretization.}
Experiments in Tab.~\ref{dair_discretization} show the detection accuracy improvement 0.3\% - s1.5.0\% when our dynamic discritization is applied instead of uniform discretization(UD).
The performance when hype-parameter $\alpha$ is set to 2.0 suppresses that of 1.5 in most cases, which signifies that hype-parameter $\alpha$ is necessary to achieve the most appropriate discretization.

\mypara{Analysis on Point Cloud Supervision.}
\input{BEV-Height/latex/table/pc_sup}
To verify the effectiveness of point cloud supervision in roadside scenes, we conduct ablation experiments on both BEVDepth~\cite{li2022bevdepth} and our method. As shown in Tab.~\ref{pc_sup}, BEVDepth with point cloud supervision is slightly lower than that without supervision. As for our BEVHeight, although there is a slight improvement under the IoU=0.5 condition, the overall gain is not apparent. This can be explained by the fact that the background in roadside scenarios is stable. These background point clouds fail to provide adequate supervised information and increase the difficulty of model fitting.
% there is only a slight improvement under IoU=0.5. We speculate this is because the camera is fixed in roadside scenarios, thus the majority of the pixels belong to the background and have relatively fixed depth or height values. The network can learn them well even without ground truth as supervision.
% We speculate that this is due to the fact that the background in roadside scenarios is stable. 
% These background point clouds remain unchanged and fail to provide adequate supervised information
% and increase the difficulty of model fitting.
% 分析，depth的问题，猜测不同路口混合训练的影响，加了监督更加hard让网络拟合了

% \mypara{Analysis on point cloud supervision.}

\mypara{Analysis on Distance Error.}
To provide a qualitative analysis of depth and height estimations, we convert depth and height to the distance between the predicted object's center and the camera’s coordinate origin, as is shown in Fig.~\ref{fig:distance_correlation}.  Compared with the distance error triggered by depth estimation in BEVDepth\cite{li2022bevdepth}, the height estimation in our BEVHeight introduces less error, which illustrates the superiority of height estimation over the depth estimation in the roadside scenario.
\input{BEV-Height/latex/fig/distance_correlation}

\mypara{Latency.}
As shown in Tab.~\ref{latency_rebuttal}, we benchmark the runtime of BEVHeight and BEVDepth. With an image size of 864x1536, BEVDepth runs at 14.7 FPS with a latency of 68ms, while ours runs at 16.1 FPS with 62ms, which is around 5\% faster. It is due to the depth range (1$\sim$104m) being much larger than height (-1$\sim$1m), thus ours has 90 height bins that less than 206 depth ones,
leading to a slimmer regression head and fewer pseudo  points for voxel pooling. It evidences the superiority of predicting height instead of depth and advocates the efficiency of our method.
\input{BEV-Height/latex/table/latency_rebuttal.tex}

\mypara{Limitations and Analysis.}
Though the motivation of our work is to address the challenges in the roadside scenarios, we nonetheless benchmark our methods on nuScenes to study the effectiveness. Here, the input resolution is set to (256, 704). We follow the setting of BEVDepth, i.e. the training lasts for 24 epochs. Note that, we did not use other tricks such as class-balanced grouping and sampling~(CBGS) strategy~\cite{zhu2019cbgs}, exponential moving average or multi-frame fusion. 
In \cref{tab:nus}, we observe that our method falls behind the BEVDepth by around 0.02 in mAP metrics. This shows that our method has limited performance on ego-vehicle settings. 

% \Tao{256x704 128x128 pc sup}
% \KY{to finish}
% For the nuScenes\cite{HolgerCaesar2019nuScenesAM} dataset, the input image is scaled to 256x704; we adopt the same dataset augmentations on image-view and BEV features as in BEVDepth\cite{li2022bevdepth}. All experiments are trained for 24 epochs without using the CBGS strategy, EMA, and multi-frame fusion.

\input{BEV-Height/latex/table/nus}

Firstly, our method does \emph{not} assume the ground-plane is fixed, and it is not the reason why our method cannot surpass the depth-based one on ego-vehicle settings. To verify, we collect around 13 thousand sequences from the camera mounted on a moving truck with a ground height of 3.14m, and annotate the 3D object box following nuScenes. As shown in Tab.~\ref{ddd_rebuttal} We observe that our BEVHeight again surpasses the depth-based state-of-the-art by a large margin, evidences the performance is affected by the camera height but not time-varying ground plane and it can work on ego-vehicle settings.
We visualize three cameras observing the same object and analyze the detection error in Fig.~\ref{fig:versatility_analysis}: (a) shows when the height prediction is equal to the ground-truth, detection is perfect for all cameras; (b) if not, for the same height prediction error, the distance between the predicted point and ground-truth is inversely proportional to the camera ground height. This is why BEVHeight achieves on-par performance on nuScenes but quickly surpasses BEVDepth~\cite{li2022bevdepth} when the camera height only increases less than 1 meter.
\input{BEV-Height/latex/table/ddd_rebuttal.tex}
\input{BEV-Height/latex/fig/versatility_analysis_rebuttal.tex}

\mypara{Contributions.}
Theoretically, our proposed height-based pipeline entails: i) representation agnostic to distance, as visualized in Fig.~\ref{fig:teaser}, ii) friendly prediction owing to centralized distribution as displayed in Fig.~\ref{fig:histogram-depth-height}, iii) robustness against extrinsic disturbance as illustrated in Fig.~\ref{fig:five}. Technically, we design a novel HeightNet and the projection module with less computational cost. Experimentally, experiments on various datasets and multiple depth-based detectors show the superiority of our method in both accuracy and latency.