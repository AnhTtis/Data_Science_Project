%%%%%%%%% ABSTRACT
\begin{abstract}
% Perception is one of the most critical tasks in autonomous driving, where the mainstream methods only rely on ego-vehicle sensors, such as cameras, to recognize the surrounding worlds. However, these sensors are limited to seeing objects within a fixed radius of the vehicle center. With the fast growth of intelligent infrastructures, roadside camera units have the potential to see the world beyond the visual range. 
While most recent autonomous driving system focuses on developing perception methods on ego-vehicle sensors, people tend to overlook an alternative approach to leverage intelligent roadside cameras to extend the perception ability beyond the visual range. 
% However, unlike ego-vehicle cameras, these roadside ones suffer from various challenges. For example, the installation position and camera poses are usually inconsistent even on the same road, which greatly challenges the robustness of the perception method. 
We discover that the state-of-the-art vision-centric bird's eye view detection methods have inferior performances on roadside cameras. 
This is because these methods mainly focus on recovering the depth regarding the camera center, where the depth difference between the car and the ground quickly shrinks while the distance increases. In this paper, we propose a simple yet effective approach, dubbed BEVHeight, to address this issue. In essence, instead of predicting the pixel-wise depth, we regress the height to the ground to achieve a distance-agnostic formulation to ease the optimization process of camera-only perception methods. On popular 3D detection benchmarks of roadside cameras, our method surpasses all previous vision-centric methods by a significant margin. The code is available at {\url{https://github.com/ADLab-AutoDrive/BEVHeight}}.
% \href{https://github.com/ADLab-AutoDrive/BEVHeight}{here}.

\comment{
   Overlooked roadside perception has great potential to solve the issues of over-occlusion and long-range perception, which is believed to facilitate more intelligent and safer autonomous driving. Vision-centric BEV perception perceives inherent merits in fusioning sensors from adjacent road side units (RSU). However, the existing BEV perception methods mainly aim at autonomous driving and are dedicated to achieving higher accuracy. The generalization challenge caused by various cameras with ambiguous mounting positions and inconsistent intrinsic parameters on RSU is disregarded. In this paper, we find that the height information perpendicular to the ground has three benefits from the perspective of road side unit: 1) Centralized distribution is conducive to improving accuracy 2) Not depending on perspective principle makes it robust to intrinsic perturbation, 3) Weakly correlated with image area make it not easily affected by ambiguous mounting viewpoint. Motivated by the conclusions above, we propose a novel height-based BEV framework for roadside perception, dubbed BEVHeight, whose PV-BEV transformation relies on the implicit height estimation, thus achieving higher accuracy and stronger generalization on the roadside. Experiments on the Rope3D and DAIR-V2X 3D object detection benchmark show that our framework surpasses the state-of-the-art methods under the standard settings. Under the camera's internal and external parameters disturbed setting, our method yields the best robustness.
  }
\end{abstract}
% 对于自动驾驶，感知很重要，现在关注于车辆自身的感知，忽视了路端的优势（遮挡/远距离）；（为什么路端）
% 最近新兴的车端BEV感知在路端也可以发挥他的优点（传感器融合）；（为什么BEV）
% 但是路端传感器的复杂环境（安装位置/内外参类型）对流行的视觉BEV感知方法的精度和泛化性带来了挑战；（有什么问题）
% 我们发现了问题在于现在的最好的sota BEV方法依赖于准确的depth估计，但是在路端depth分布差异大/传感器内外参差异大（问题）
% 于是我们提出了BEV-height，巧妙的选择了height估计，对比depth，其优点（height分布集中/透视原理？）（解决问题）
% 取得了很好的精度和泛化性结果（结果）
