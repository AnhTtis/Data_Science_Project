
%%%%%%%%% BODY TEXT
% \vspace{-0.3cm}
\section{Introduction}
\label{sec:intro}
The rising tide of autonomous driving vehicles draws vast research attention to many 3D perception tasks, of which 3D object detection plays a critical role.  
While most recent works tend to only rely on ego-vehicle sensors, there are certain downsides of this line of work that hinders the perception capability under given scenarios. For example, as the mounting position of cameras is relatively close to the ground, obstacles can be easily occluded by other vehicles to cause severe crash damage. To this end, people have started to develop perception systems that leverage intelligent units on the roadside, such as cameras, to address such occlusion issue and enlarge  perception range so to increase the response time in case of danger \cite{ye2022rope3d, yu2022dair, song2022efficient, huang2022rd,cui2022coopernaut, xu2022v2x}. To facilitate future research, there are two large-scale benchmark datasets~\cite{yu2022dair, ye2022rope3d} of various roadside cameras and provide an evaluation of certain baseline methods. 

% \KY{say something about the current vision-centric bird's eye view method} 
% Thanks to the two recently published benchmarks, XX, YY, 
Recently, people discover that, in contrast of directly projecting the 2D images into a 3D space, leveraging a bird's eye view~(BEV) feature space that reduce the Z-axis degree of freedom can significantly improve the perception performance of vision centric system. One line of recent approach, which constitutes the state-of-the-art in camera only method, is to generate implicitly or explicitly the depth for each pixel to ease the optimization process of bounding box regression. However, as shown in \cref{fig:teaser}, we visualize the per-pixel depth of an example roadside image and notice a phenomenon. Consider two points, one on the roof of a car and another on the nearest ground. If we measure the depth of these points to camera center, namely $d$ and $d'$ respectively, the difference between these depth $d-d'$ would drastically decrease when the car moves away from the camera. We conjecture this leads to two potential downsides: i) unlike the autonomous vehicle that has a consistent camera pose, roadside ones usually have different camera pose parameters across the datasets, which makes regressing depth hard; ii) depth prediction is very sensitive to the change of extrinsic parameter, where it happens quite often in the real world. 

On the contrary, we notice that the height to the ground is consistent regardless of the distance between car and camera center. To this end, we propose a novel framework to predict the per-pixel height instead of depth, dubbed \name{}. Specifically, our method firstly predicts categorical height distribution for each pixel to project rich contextual feature information to the appropriate height interval in wedgy voxel space. Followed by a voxel pooling operation and a detection head to get the final output detections. Besides, we propose a hyperparameter-adjustable height sampling strategy. Note that our framework does not depend on explicit supervision like point clouds. 

% \KY{to complete after experiment is finalized} Through extensive experiments on DAIR-V2X and Rope3D, our method achieves xxxx over yyy ....
We conduct extensive experiments on two popular large-scale benchmarks for roadside camera perception, DAIR-V2X~\cite{yu2022dair} and Rope3D~\cite{ye2022rope3d}. On traditional settings where there is no disruption to the cameras, our \name{} achieves the state-of-the-art performance and surpass all previous methods, regardless of monocular 3D detectors or recent bird's eye view methods by a margin of 5\%. In realistic scenarios, the extrinsic parameters of these roadside units can be subject to changes due to various reasons, such as maintenance and wind blows. We simulate these scenarios following \cite{yu2022benchmarking} and observes a severe performance drop of the BEVDepth, from 41.97\% to 5.49\%. Compared to these methods, we showcase the benefit of predicting the height instead of depth and achieve 26.88\% improvement over the BEVDepth~\cite{li2022bevdepth}, which further evidences the robustness of our method.

% As there is an emerging trend of leveraging bird's eye view feature space to improve the perception results, one naive approach is to adopt the commonly adopted BEV methods on monocular cameras on these benchmarks. 
% However, this line of work makes two assumptions: i) 


% \comment{
% Autonomous driving faces great perception challenges, such as over-occlusion and long-range perception. It has been widely agreed that roadside perception can effectively make up for these long-tail problems in perception based on its god's-eye perspective, which makes it possible to achieve safer and more practical autonomous driving.

% As a result of various field-of-view requirements and inevitable installation errors, cameras mounted on roadside units (RSU) have ambiguous mounting positions and inconsistent intrinsic parameters. This way, vision-based roadside perception methods should generalize to different combinations of internal and external camera parameters. 



% Vision-centric BEV perception, due to its natural representation of the world, is friendly to fusion cameras on different roadside platforms. However, the existing BEV perception methods mainly aim at autonomous driving and are dedicated to achieving higher accuracy, neglecting the generalization performance when the same model is applied to cameras with different specifications.


% We found experimentally that the depth-based method is less than 10\% of the original performance under internal and external parameter perturbation. This is because the depth estimation is based on the camera perspective principle, and the depth information distribution shows a large near and small far pattern, as shown in \cref{fig:two}. On the one hand, the box size of closed car A and remote car B negatively correlates with their depth. The scaling relationship between box size and depth depends on the camera's focal length, and the model implicitly learns this relationship and focal length. Consequently, the depth-based method trained with data from long-focus cameras is difficult to generalize to short-focus camera images. On the other hand, the depth distribution on the image is non-uniformity. As shown in \cref{fig:two}, the A and B areas focus on different depth intervals in training. A disturbance of external camera parameters may cause the object in area A to be shifted to area B on the image. However, area B can hardly accurately predict this object's depth in the inference. In summary, the depth-based approach cannot effectively cope with the generalization problem caused by various cameras with ambiguous mounting positions and inconsistent intrinsic parameters on RSU.

% In this paper, we empirically find that height perpendicular to the ground in the roadside view has three significant advantages over depth:
% \begin{itemize}
% \item	Height perpendicular to the ground is in a more centralized distribution, which helps reduce training challenges and improve detection performance.
% \item The distribution of height does not follow the perspective phenomenon of large near and small far, which makes it robust to intrinsic perturbation.
% \item Height distribution presents uniformity on the image; areas like A and B focus on the same height intervals in training. Subsequently, extrinsic disturbance imposes less impact on height estimation.
% \end{itemize}

% Motivated by the insights above, a novel BEV-Height model for camera-only roadside 3D object detection is introduced in this work. Specifically, BEV-Height firstly predicts categorical height distribution for each pixel to project rich contextual feature information to the appropriate height interval in wedgy voxel space. Followed by a voxel pooling operation and a detection head to get the final output detections. Besides, we propose a hyperparameter-adjustable height sampling strategy and a supervised training strategy with point cloud to improve the height distribution estimation further.

% We conduct extensive experiments on DAIR-V2X and Rope3D datasets. BEV-Height ranks first among other methods on camera-only 3D object detection benchmarks. Under the camera's internal and external parameters disturbed setting, our method surpasses BEVDepth by almost 500\% in terms of detection accuracy.

% In summary, our main contribution can be summarized as follow: 
% i) We systematically analyze the advantages of the distribution of height perpendicular to the ground over depth distribution in roadside scenes. Based on this analysis, we introduce a new BEV-Height framework for camera-only roadside 3D object detection. 
% ii) Extensive experiments on the DAIR-V2X and Rope3D datasets show that the proposed BEV-Height achieves state-of-the-art, demonstrating the effectiveness of our method. More importantly, our approach maintains the best robustness under the camera's internal and external parameters disturbed setting.
% }