% \begin{table*}[ht]
%  \centering\addtolength{\tabcolsep}{-0.6pt}
%  \resizebox{0.8\textwidth}{!}{
%  \begin{tabularx}{1.0\textwidth}{l|c|ccc|ccc|ccc}
%   \toprule
%  \multirow{3}{*}{Method} &  
%  \multirow{3}{*}{Modality}  
%  & \multicolumn{3}{c|}{$\text{Vehicle}_{(IoU=0.5)}$} & \multicolumn{3}{c|}{$\text{Pedestrian}_{(IoU=0.25)}$} & \multicolumn{3}{c}{$\text{Cyclist}_{(IoU=0.25)}$} \\
%     \cmidrule(r){3-11}
%      &  & Easy & Mid & Hard & Easy & Mid & Hard & Easy & Mid & Hard  \\
% \midrule

% PointPillars~\cite{lang2019pointpillars} & PointCloud &63.07 & 54.00 & 54.01 & 38.53 & 37.20 & 37.28 & 38.46 & 22.60 & 22.49 \\
% SECOND~\cite{yan2018second} & PointCloud &71.47 & 53.99 & 54.00 & 55.16 & 52.49 & 52.52 & 54.68 & 31.05 & 31.19 \\
% MVXNet~\cite{Sindagi2019MVX} & Image+PointCloud &71.04 & 53.71 & 53.76 & 55.83 & 54.45 & 54.40 & 54.05 & 30.79 & 31.06 \\
% \midrule
% ImvoxelNet~\cite{rukhovich2022imvoxelnet} &Image & 44.78 & 37.58 & 37.55 & 6.81 & 6.746 & 6.73 & 21.06 & 13.57 & 13.17 \\
% BEVFormer-R101$\ast$~\cite{li2022bevformer} & Image 	&	61.37&	50.73&	50.73&	16.89&	15.82&	15.95	&22.16&	22.13&	22.06\\
% BEVDepth-R101$\ast$~\cite{li2022bevdepth}&	Image 	&	76.01&	64.11&	64.18&	24.32&	24.96&	24.84	&46.45&	45.56&	45.69	\\

% \midrule
% BEVHeight-R101(Ours) & Image &	79.12&	67.95&	67.04&	29.85&	29.31&	29.07	&51.55&	51.39&	50.91\\
%     \bottomrule
%   \end{tabularx}
%   }
%   \caption{\textbf{Comparison on the DAIR-V2X-I val set.}}
%   \label{dair_sota}
% \end{table*}


\begin{table}[t]
 \scriptsize\centering\addtolength{\tabcolsep}{-4.2pt}
\caption{\textbf{Comparing with the state-of-the-art on the DAIR-V2X-I val set.} Here, we report the results of three types of objects, vehicle~(veh.), pedestrian~(ped.) and cyclist~(cyc.). Each object is categorized into three settings according to the difficulty defined in ~\cite{yu2022dair}. First, recent BEVDepth surpasses the previous best by a large margin, showing that using bird's-eye-view indeed helps in roadside scenarios. Our method outperforms the BEVDepth by over 3\% in average precision and constitutes state-of-the-art. It is surprising to see that our method outperforms those relying on LiDAR modality.
}
% \vspace{-0.2cm}

 \begin{tabularx}{1.0\linewidth}{l|c|ccc|ccc|ccc}
  \toprule
 \multirow{3}{*}{Method} &  
 \multirow{3}{*}{M}  
 & \multicolumn{3}{c|}{$\text{Veh.}_{(IoU=0.5)}$} & \multicolumn{3}{c|}{$\text{Ped.}_{(IoU=0.25)}$} & \multicolumn{3}{c}{$\text{Cyc.}_{(IoU=0.25)}$} \\
    \cmidrule(r){3-11}
     &  & Easy & Mid & Hard & Easy & Mid & Hard & Easy & Mid & Hard  \\
\midrule
PointPillars~\cite{lang2019pointpillars} & L &63.07 & 54.00 & 54.01 & 38.53 & 37.20 & 37.28 & 38.46 & 22.60 & 22.49 \\
SECOND~\cite{yan2018second} & L &71.47 & 53.99 & 54.00 & 55.16 & 52.49 & 52.52 & 54.68 & 31.05 & 31.19 \\
MVXNet~\cite{Sindagi2019MVX} & LC &71.04 & 53.71 & 53.76 & 55.83 & 54.45 & 54.40 & 54.05 & 30.79 & 31.06 \\
\midrule
ImvoxelNet~\cite{rukhovich2022imvoxelnet} &C & 44.78 & 37.58 & 37.55 & 6.81 & 6.746 & 6.73 & 21.06 & 13.57 & 13.17 \\
BEVFormer~\cite{li2022bevformer} & C 	&	61.37&	50.73&	50.73&	16.89&	15.82&	15.95	&22.16&	22.13&	22.06\\
BEVDepth~\cite{li2022bevdepth}&	C 	&	
75.50&	63.58&	63.67&	34.95&	33.42&	33.27& 55.67&	55.47&	55.34\\
\midrule
 \rowcolor{cyan!30} BEVHeight & C &	
 \textbf{77.78}&	\textbf{65.77}&	\textbf{65.85}&	\textbf{41.22}&	\textbf{39.29}&	\textbf{39.46}	&\textbf{60.23}&	\textbf{60.08}&	\textbf{60.54}\\
    \bottomrule
\multicolumn{8}{l}{\scriptsize{M, L, C denotes modality, LiDAR, camera respectively.}}
  \end{tabularx}
  \vspace{-0.5cm}
  \label{dair_sota_2}
\end{table}




