\section{Related Work}
\mypara{Roadside Perception.}
Concurrent perception efforts for autonomous driving are mainly limited to the ego vehicle~\cite{caesar2020nuscenes, sun2020waymo}. While the roadside perception, which comparatively has a longer perceptual range and more robustness to occlusion and long-time event prediction, is mainly under-explored. Recently, some pioneers have present roadside datasets~\cite{ye2022rope3d, yu2022dair}, hoping to facilitate the 3D perception tasks in roadside scenarios. Compared with the vehicle perceptual system, which only observes surroundings in a short distance, the roadside cameras, mounted on poles a few meters above the ground, can provide long-range perception. However, the cameras mounted on roadside units have ambiguous mounting positions and 
%inconsistent intrinsic 
variable extrinsic parameters, which bring critical challenges to current perception models. In this paper, we take the advances and challenges of roadside cameras into account, and design an efficient and robust roadside perception framework, \name{}.


% \mypara{Vision-based Multi-View BEV perception.}
%  single-camera setting and multi-camera setting
\mypara{Vision Centric BEV Perception.}
Recent vision-centric works predict objects in 3D space, which is very suitable for applying multi-view feature aggregation under BEV for autonomous driving. Popular methods can be divided into transformer-based and depth-based schema. 
Following DETR3D~\cite{wang2022detr3d}, transformer-based detectors design a set of object queries~\cite{liu2022petr, liu2022petrv2, jiang2022polarformer, Chen2022PolarPF, Saha2022TranslatingII} or BEV grid queries\cite{li2022bevformer}, then perform the view transformation through cross-attention between queries and image features. 
Following LSS~\cite{philion2020lift}, depth-based methods ~\cite{reading2021cadnn, huang2021bevdet, huang2022bevdet4d} explicitly predict the depth distribution and use it to construct the 3D volumetric feature. Followup works introduce depth supervision from the LiDAR sensors ~\cite{li2022bevdepth} or multi-view stereo techniques ~\cite{wang2022sts, li2022bevstereo, park2022solofusion} to improve the depth estimation accuracy and achieve state-of-the-art performance. 
Additionally, transformer-based detectors' implicit 3D location information also can benefit from accurate depth cues. Inspired by~\cite{zhang2022monodetr,huang2022monodtr}, CrossDTR~\cite{tseng2022crossdtr} proposed depth-guided transformers, which compose depth-aware embedding from depth maps and are supervised by ground truth depth maps to enhance performance.
However, when applying these methods to roadside perception, the bonus of accurate depth information fades. As the complex mounting positions and variable extrinsic parameters of the roadside cameras, predicting depth from them is difficult. In this work, our \name{} utilizes the height estimation to achieve state-of-the-art performance and best robustness of roadside 3D object detection.
