
%================================================================================
\section{Experiments}
\label{sec:experiments}
%================================================================================

%--------------------------------------------------------------------------------
\begin{table}[t]
    \centering
    \small
    \begin{tabular}{l c rrrr c rr}
        \toprule
         && \multicolumn{4}{c}{\textbf{Standard setting}} && \multicolumn{2}{c}{\textbf{$\sothree$ generalization}}\\
         Environment && {BCQ} & {CQL} & {Diffuser}& \eqd (ours) && {Diffuser}& \eqd (ours) \\
         \cmidrule{1-1} \cmidrule{3-6} \cmidrule{8-9}
         Navigation && -- & -- & \bestresult{94.9}{3.9} & \bestresult{95.1}{3.4} && \result{5.6}{4.4} & \bestresult{83.3}{3.5} \\
         \cmidrule{1-1} \cmidrule{3-6} \cmidrule{8-9}
         Unconditional && $\hphantom{0}0.0$ & $24.4$            & \result{59.7}{2.6} & \bestresult{68.7}{2.5} && \result{38.7}{2.3} & \bestresult{69.0}{2.7} \\
         Conditional && $\hphantom{0}0.0$ & $\hphantom{0}0.0$   & \result{46.0}{3.4} & \bestresult{52.0}{3.6} && \result{16.7}{2.0} & \bestresult{35.9}{3.5} \\
         Rearrangement && $\hphantom{0}0.0$ & $\hphantom{0}0.0$ & \bestresult{49.2}{3.3} & \bestresult{47.2}{3.9} && \result{17.8}{2.3} & \bestresult{45.0}{3.6} \\
         Average && $\hphantom{0}0.0$ & $\hphantom{0}8.1$       & \result{51.6}{1.8} & \bestresult{56.0}{2.0} && \result{24.4}{1.3} & \bestresult{50.0}{1.9} \\
        \bottomrule
    \end{tabular}
    \vspace{5pt}
    \caption{Performance on navigation tasks and block stacking problems with a Kuka robot. We report normalized cumulative rewards, showing the mean and standard errors over 100 episodes. Results consistent with the best results within the errors are bold. BCQ and CQL results are taken from \citet{janner2022planning}; for Diffuser, we show our reproduction using their codebase. \textbf{Left}: Models trained on the standard datasets. \textbf{Right}: $\sothree$ generalization experiments, with training data restricted to specific spatial orientations such that the agent encounters previously unseen states at test time.}
    \label{tab:standard_dataset_results}
\end{table}
%--------------------------------------------------------------------------------

We demonstrate the effectiveness of incorporating symmetries as a powerful inductive bias in the Diffuser algorithm with experiments in two environments. The first environment is a $3$D navigation task, in which an agent needs to navigate a number of obstacles to reach a goal state. Rewards are awarded based on the distance to the goal at each step, with penalties for collisions with obstacles. The position of the obstacles and the goal state are different in each episode and part of the observation. For simplicity, the actions directly control the acceleration of the agent; both the agent and the obstacles are spherical.
Please see Fig.~\ref{fig:edgi_sketch} for a schematic representation of this task and
% Appendix~B
Appendix~\ref{app:pointmass}
for more details and the reward structure for this task.

In our remaining experiments, the agent controls a simulated Kuka robotic arm interacting with four blocks on a table. We use 
a benchmark environment introduced by \citet{janner2022planning}, which specifies three different tasks: an unconditional block stacking task, a conditional block stacking task where the stacking order is specified, and a rearrangement problem, in which the stacking order has to be changed in a particular way.
For both environments, we train on offline trajectory datasets of roughly $10^5$ (navigation) or $10^4$ (manipulation) trajectories. We describe the setup in detail in
% Appendix~C.
Appendix~\ref{app:kuka}.

\xhdr{Algorithms}
We train our \eqd on the offline dataset and use conditional sampling to plan the next actions. For the conditional and rearrangement tasks in the Kuka environment, we use classifier guidance following \citet{janner2022planning}.

As our main baseline, we compare our results to the (non-equivariant) Diffuser model~\citep{janner2022planning}. In addition to a straightforward model, we consider a version trained with $\sothree$ data augmentation. We also compare two model-based RL baselines reported by~\citet{janner2022planning}, BCQ \citep{fujimoto2019off} and CQL \citep{kumar2020conservative}. To study the benefits of the symmetry groups in isolation, we construct two EDGI variations: one is equivariant with respect to $\sethree$, but not $\permutation$; while the other is equivariant with respect to $\permutation$, but not $\sethree$. Both are equivariant to temporal translations, just like \eqd and the baseline Diffuser.


\xhdr{Task performance}
We report the results on both navigation and object tasks in Tab.~\ref{tab:standard_dataset_results}. For each environment, we evaluate $100$ episodes and report the average reward and standard error for each method.
In both environments and across all tasks, \eqd performs as well as or better than the Diffuser baseline when using the full training set. In the navigation task, achieving a good performance for the baseline required substantially increasing the model's capacity compared to the hyperparameters used in \citet{janner2022planning}. On the Kuka environment, both diffusion-based methods clearly outperform the BCQ and CQL baselines.

\xhdr{Sample efficiency}
We study \eqd's sample efficiency by training models on subsets of the training data. The results in Fig.~\ref{fig:sample_efficiency} show that \eqd achieves just as strong rewards in the Kuka environment when training with only on $10\%$ of the training data, and on the navigation task even when training on only $0.01\%$ if the training data. The Diffuser baseline is much less sample-efficient. 
Training the Diffuser model with data augmentation partially closes the gap, but \eqd still maintains an edge.
Our results provide evidence for the benefits of the inductive bias of equivariant models and matches similar observations in other works for using symmetries in an RL context \citep{Van_der_Pol2020-mm, Walters2020-iz, Mondal2021-hu, rezaei2022continuous, Deac2023-tg}.

\xhdr{Effects of individual symmetries}
In the left panel of Fig.~\ref{fig:sample_efficiency}, we also show results for \eqd{} variations that are only equivariant with respect to $\sethree$, but not $\permutation$, or vice versa. Both partially equivariant methods perform better than the Diffuser baseline, but not as well as the \eqd model equivariant to the full product group $\fullgroup$.
This confirms that the more of the symmetry of a problem we take into account in designing an architecture, the bigger the benefits in sample efficiency can be.

\xhdr{Group generalization}
Finally, we demonstrate that equivariance improves generalization across the $\sothree$ symmetry group. On both environments, we train \eqd and Diffuser models on restricted offline datasets in which all trajectories are oriented in a particular way. In particular, in the navigation environment, we only use training data that navigates towards a goal location with $x = 0$. In the robotic manipulation tasks, we only use training trajectories where the red block is in a position with $x = 0$ at the beginning of the episode. We test all agents on the original environment, where they encounter goal positions and block configurations unseen during training.
We show results for these experiments in Tab.~\ref{tab:standard_dataset_results}. The original Diffuser performs substantially worse, showing its limited capabilities to generalize to the new setting. In contrast, the performance of \eqd is robust to this domain shift, confirming that equivariance helps in generalizing across the symmetry group.

%--------------------------------------------------------------------------------
\begin{figure*}[t]
    \centering%
    \includegraphics[width=0.49\linewidth]{figures/pointmass_v2_sample_efficiency_rebuttal.pdf}%
    \includegraphics[width=0.49\linewidth]{figures/kuka_sample_efficiency.pdf}%
    \caption{Average reward as a function of training dataset size for \eqd and Diffuser. \textbf{Left}: navigation environment. \textbf{Right}: Kuka object maniplation, averaged over the three tasks.}
    \label{fig:sample_efficiency}
\end{figure*}
%--------------------------------------------------------------------------------
