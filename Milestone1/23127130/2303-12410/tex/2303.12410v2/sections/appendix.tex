
\appendix


%================================================================================
\section{Group theory}
\label{app:group_theory_intro}
%================================================================================

In this appendix, we provide a basic introduction to Lie groups with an emphasis on the Lie group $\sethree$ which is used in the main paper. 

A Lie group is both a group and a smooth manifold. A manifold is a topological space and thus a Lie group is a topological group that is formally defined below. 

\begin{definition}
A topological group is a topological space $G$ equipped with continuous maps that allow group composition $\circ: G \times G \to G$ and group inverses $(\circ)^{-1}: G \to G$.  
\end{definition}
Now let $g \in G$ be an element of the topological group and consider the map $\bar{g}: G \to G$ given by $\bar{g} (g) = g \circ g'$. If $g \circ g_1 = g \circ g_2$ then $g_1 = g_2$ and thus $\bar{g}$ is injective and for all $g \in G$, and there exists an element---i.e. $g^{-1} \circ g$ such that $\bar{g} (g^{-1} \circ g) = g \circ g^{-1} \circ g' = g'$. Thus, $\bar{g}$ is a bijection. 

In a topological space $\bar{g}$ and $\bar{g}^{-1}$ are continuous. Thus, $\bar{g}$ is a homomorphism. This means if $U \subset G $ is open 1.) $ g \circ U$ is open 2.) $ U \circ g$ is open and inversion is also a homomorphism so $U$ is open if and only if $U^{-1}$ is open. A topological space $\mathcal{X}$ is said to be \emph{homogenous} if $\forall x,y \in \mathcal{X}$ there exists a map $h_{x,y}: \mathcal{X} \to \mathcal{X}$ that is a homomorphism such that $h_{x,y}(x) = y$. Note that a topological group is always homogenous. In words, a homogenous space for a topological group means any group element is reachable by a suitable group homomorphism.

\xhdr{Matrix Lie groups}
A group of special interest is the group of $n \times n$ invertible matrices with entries in $\mathbb{R}$, $\text{GL}_n(\mathbb{R})$. This is a topological group equipped with Euclidean topology. The subgroup $\text{SL}_n(\mathbb{R}) \subset \text{GL}_n(\mathbb{R})$ is a closed subgroup with the property that $\det = 1$. As an example $\sethree \subset \text{E}(3)$ satisfies this definition and is the group of $3\rm D$ Euclidean symmetries that include rotations and translations, but not reflections.  

A matrix Lie group is a closed subgroup of $\text{GL}_{n}(\mathbb{R})$.\footnote{The field can also be complex $\mathbb{C}$ to allow invertible complex matrices $\text{GL}_n(\mathbb{C})$.} This means $\text{SL}_n(\mathbb{R}) $ is a matrix Lie group as are $\text{O}(n)$ and $\text{SO}(n)$. Moreover, $\text{O}(n)$ and $\text{SO}(n)$ are compact. Finally, it is important to note that for matrix Lie groups the exponential and logarithmic maps correspond to the matrix exponential and matrix logarithm. Both of these are infinite power series and have precise connections to the representation theory of matrix Lie groups; namely, the Lie algebra associated with the Lie groups. While we pause further discussion on representation theory here the interested reader is encouraged to read \citet{hall2013lie}. 

\xhdr{$\sethree$ as a matrix Lie group}
The group of Euclidean symmetries in $3 \rm D$ is known as the $\text{E}(3)$. A closed subgroup of this group is the special Euclidean group, $\sethree$, and corresponds to rotations and translations in $3 \rm D$. Commonly, elements of $\sethree$ can be used to represent rigid body transformations in $3$ dimensions. This is because $\sethree \cong \sothree \ltimes \R^3$ and thus can be written as,

\begin{equation}
    \sethree = 
    \left \{ 
    \begin{pmatrix}
       R & x \\
        0 & 1 
    \end{pmatrix}
     : R \in \sothree, x \in (\R^3, +)
     \right \}
\end{equation}
Represented by this $4 \times 4$ matrix and with the group operation defined by matrix multiplication, this group can be seen as a subgroup of the general linear group $\mathrm{GL}_4(\mathbb{R})$.

\xhdr{$\sothree$ as a matrix Lie group}
The group of $3 \rm D$ rotations is $\sothree$. It is a compact matrix Lie group where each element $R \in \sothree$ is a rotation. To represent $R$ there are various possible choices. The most familiar of them is perhaps $R \in \R^{3 \times 3}$ is as a $3 \times 3$ rotation matrix.

An alternative parameterization is the \emph{rotation vector}. Such a parametrization exploits the Lie algebra of $\sothree$ which are skew-symmetric matrices $\mathfrak{g}$. Each element of the Lie algebra can be distinctively associated with a vector $\boldsymbol{\omega} \in \R^3$. Given any $\mathbf{v} \in \R^3$, it's related by $\mathfrak{g} \mathbf{v} = \boldsymbol{\omega} \times \mathbf{v}$, with the symbol $\times$ representing the cross product. The length of this vector, symbolized as $\omega = || \boldsymbol{\omega} ||$, represents the rotation angle, while the unit direction, expressed as $e_{\boldsymbol{\omega}} = \frac{\boldsymbol{\omega}}{|| \boldsymbol{\omega} ||}$, indicates the rotation axis.

Yet another parametrization of rotations can be achieved using \emph{quaternions}.
A quaternion, $ q $, is often represented as $ q = w + xi + yj + zk $ where $ w, x, y, $ and $ z $ are real numbers and $ i, j, $ and $ k $ are mutually orthogonal imaginary units vectors. The unit vectors obey the following rule:
$ i^2 = j^2 = k^2 = ijk = -1$. A subset of quaternions, known as rotation quaternions, can be used to represent $3 \rm D$ rotations. These specific quaternions have magnitude $1$ and can also be thought of as an ordered pair $ (w, v) $ where $ w $ is a scalar and $ v = (x, y, z) $ is a $3 \rm D$ vector. The scalar part $ w $ can be thought of as $ \cos(\omega/2) $ and the vector part $ v $ as $n \sin(\omega/2)$, where $ \omega $ is the angle of rotation and $n$ is the unit vector along the axis of rotation.

Finally, to rotate a point $p$ in $3 \rm D$ space using a quaternion $q$, the point can be represented as a pure quaternion $ p' = 0 + xi + yj + zk $. The rotated point $ p''$ is then given by: $p'' = q \cdot p' \cdot q^* $.
Where $ q^* $ is the conjugate of $ q $---i.e. if $ q = w + xi + yj + zk $, then $ q^* = w - xi - yj - zk $.

%================================================================================
\section{Architecture details}
\label{app:algo}
%================================================================================

On a high level, \eqd follows Diffuser~\citep{janner2022planning}. In the following, we will describe the key difference: our $\fullgroup$-equivariant architecture for the diffusion model.

\xhdr{Overall architecture}
We illustrate the architecture in
% Fig.~2 in the main paper.
% Fig.~\ref{fig:architecture}.
After converting the input data in our internal representation (see
% Sec.~3.2 in the main paper),
Sec.~\ref{sec:eqd_diffusion}),
the data is processed with an equivariant $U$-net with four levels.
At each level, we process the hidden state with two residual standard blocks, before downsampling (in the downward pass) or upsampling (in the upward pass). 

\xhdr{Residual standard block}
The main processing unit of our architecture processes the current hidden state with an equivariant block consisting of a temporal layer, an object layer, a normalization layer, and a geometric layer. In parallel, the context information (an embedding of diffusion time and a conditioning mask) is processed with a context block. The hidden state is added to the output of the context block and processes with another equivariant block. Finally, we process the data with a linear attention layer over time. This whole pipeline consists of an equivariant block, a context block, and another equivariant block is residual (the inputs are added to the outputs).

\xhdr{Temporal layers}
Temporal layers consist of one-dimensional convolutions without bias along the time dimension. We use a kernel size of 5.

\xhdr{Normalization layers}
We use a simple equivariant normalization layer that for each batch element rescales the entire tensor $w_{toc}$ to unit variance. This is essentially an equivariant version of LayerNorm. The difference is that our normalization layer does not shift the inputs to zero means, as that would break equivariance with respect to $\sothree$.

\xhdr{Geometric layers}
In the geometric layers, the input state is split into scalar and vector components. The vector components are linearly transformed to reduce the number of channels to 16. We then construct all $\sothree$ invariants from these 16 vectors by taking pairwise inner products and concatenating them with the scalar inputs. This set of scalars is processed with two MLPs, each consisting of two hidden layers and ReLU nonlinearities. The MLPs output the scalar outputs and coefficients for a linear map between the vector inputs and the vector outputs, respectively. Finally, there is a residual connection that adds the scalar and vector inputs to the outputs.

\xhdr{Linear attention over time}
To match the architecture used by \citet{janner2022planning} as closely as possible, we follow their choice of adding another residual linear attention over time at the end of each level in the U-net. We make the linear attention mechanism equivariant by computing the attention weights as 

\xhdr{Context blocks}
The embeddings of diffusion time and conditioning information are processed with a Mish nonlinearity and a linear layer, like in \citet{janner2022planning}. Finally, we embed them in our internal representation by zero-padding the resulting tensor.

\xhdr{Upsampling and downsampling}
During the downsampling path, there is a final temporal layer that implements temporal downsampling and increases the number of channels by a factor of two. Conversely, during the upsampling path, we use a temporal layer for temporal upsampling and a reduction of the number of channels.

\xhdr{Equivariance}
We now demonstrate the equivariance of the \eqd architecture explicitly. For concreteness, we focus on the geometric layers, as they are the most novel, and on both $\sethree$ transformations and permutations. Similar arguments can be made for the other layers and for equivariance with respect to temporal translations.

Let $w \in \mathbb{R}^{n \times H \times c \times 4}$ be data in our internal representation, such that the entries $w_{toc}$ decompose into SO(3) scalars $s_{toc}$ and SO(3) vectors $v_{toc}$. Let $S(w_{to})$ be the set of all scalars and all pairwise $SO(3)$ inner products between the vectors $v_{to}$, as defined in Eq.~\eqref{eq:invariant_scalars}.
The outputs of the geometric layer are then $f(w)_{toc} = (\phi( S(w_{to}) ), \sum_{c'} \psi( S(w_{to}) ) v_{toc'} )$.

First, consider what happens under permutations of the objects, $w_{toc} \to w_{t\pi(o)c}$ for a permutation $\pi \in S_n$. We have $f(\pi \cdot w)_{toc} = (\phi( S(w_{t\pi(o)}) ), \sum_{c'} \psi( S(w_{t\pi(o)}) ) v_{t\pi(o)c'} ) = f(w)_{t \pi(o) c} = (\pi \cdot f(w))_{toc}$. Thus, because this layer “leaves the object dimension untouched”, it is equivariant with respect to object permutations.

Next, consider the behavior under spatial transformations. Like most (S)E(3)-equivariant architectures, we deal with translations through canonicalization, defining all coordinates with respect to the center of mass or the robot base, as applicable. This means we only have to analyze the behavior under rotations.

Let $R \in \mathrm{SO(3)}$, such that $R \cdot w = R \cdot (s, v) = (s, R \cdot v)$. By definition, orthogonal matrices leave the inner product invariant, thus $S(R \cdot w) = S(w)$. The geometric layer applied to rotated inputs then gives $f(R \cdot w)_{toc} = (\phi( S(R \cdot w_{to}) ), \sum_{c'} \psi( S(w_{to}) ) R \cdot v_{toc'} ) = (\phi( S(w_{to}) ), R \cdot \sum_{c'} \psi( S(w_{to}) ) v_{toc'} ) = (R \cdot f(w))_{toc}$. Hence the geometric layer is equivariant with respect to SE(3).



%================================================================================
\section{Navigation experiments}
\label{app:pointmass}
%================================================================================

We introduce a new navigation environment. The scene consists of a spherical agent navigating a plane populated with a goal state and $n = 10$ spherical obstacles. At the beginning of every episode, the agent position, agent velocity, obstacle positions, and goal position are initialized randomly (in a rotation-invariant way). We simulate the environment dynamics with PyBullet~\citep{coumans2019}.

\xhdr{Offline dataset}
To obtain expert trajectories, we train a TD3~\citep{fujimoto2018addressing} agent in the implementation by \citet{raffin2021baselines3} for $10^7$ steps with default hyperparameters on this environment. We generate $10^5$ trajectories for our offline dataset.

\xhdr{State}
The state contains the agent position, agent velocity, goal position, and obstacle positions.

\xhdr{Actions}
The action space is two-dimensional and specifies a force acting on the agent.

\xhdr{Rewards}
At each time step, the agent receives a reward equal to the negative Euclidean distance to the goal state. In addition, a penalty of $-0.1$ is added to the reward if the agent touches any of the obstacles. Finally, there is an additional control cost equal to $-10^3$ times the force acting on the agent. We affinely normalize the rewards such that a normalized reward of $0$ corresponds to that achieved by a random policy and a normalized reward of $100$ corresponds to the expert policy.


%================================================================================
\section{Kuka experiments}
\label{app:kuka}
%================================================================================

%--------------------------------------------------------------------------------
\begin{figure*}[t]
    \centering%
    \includegraphics[width=0.24\textwidth,clip=true,trim=100px 150px 100px 100px]{figures/frame_0.png}\,%
    \includegraphics[width=0.24\textwidth,clip=true,trim=100px 150px 100px 100px]{figures/frame_2.png}\,%
    \includegraphics[width=0.24\textwidth,clip=true,trim=100px 150px 100px 100px]{figures/frame_3.png}\,%
    \includegraphics[width=0.24\textwidth,clip=true,trim=100px 150px 100px 100px]{figures/frame_4.png}%
    \caption{Rendering of the robotic manipulation environment. We show four frames from a single trajectory solving the unconditional block stacking task.}%
    \label{fig:kuka_env}
\end{figure*}
%--------------------------------------------------------------------------------

We use the object manipulation environments and tasks from \citet{janner2022planning}, please see that work for details on the environment. In our experiments, we consider three tasks: unconditional stacking, conditional stacking, and block rearrangement. Figure~\ref{fig:kuka_env} visualizes the unconditional block stacking task. For a fair comparison, we re-implement the Diffuser algorithm while making bug fixes in the codebase of \citet{janner2022planning}, which mainly included properly resetting the environment. 

\xhdr{State}
\label{app:kuka_state}
We experiment with two parameterizations of the Kuka environment state. For the Diffuser baseline, we use the original 39-dimensional parameterization from \citet{janner2022planning}.

For our \eqd, we need to parameterize the system in terms of $\fullgroup$ representations. We, therefore, describe the robot and block orientations with $\sothree$ vectors as follows. Originally, the robot state is specified through a collection of joint angles. One of these encodes the rotation of the base along the vertical $z$-axis. We choose to represent this angle as a $\rho_1$ vector in the $xy$-plane. In addition, we add the gravity direction (the $z$-axis itself) as another $\rho_1$ vector, which is also the normal direction of the table on which the objects rest. Combined, these vectors define the pose of the base of the robot arm.
Rotating gravity direction, and the robot and object pose by $\sothree$ can be interpreted as a passive coordinate transformation, or as an active rotation of the entire scene, including gravity. As the laws of physics are invariant to this transformation, this is a valid symmetry of the problem.

The $n$ objects can be translated and rotated. Their pose is thus given by a translation $t \in \R^3$ and rotation in $r \in \sothree$ relative to a reference pose. The translation transforms by a global rotation $g \in \sothree$ as a vector via representation $\rho_1$.
The rotational pose transforms by left multiplication $r \mapsto gr$. The $\sothree$ pose is not a Euclidean space, but a non-trivial manifold. Even though diffusion on manifolds is possible \cite{De_Bortoli2022-fe,Huang2022-ra}, we simplify the problem by embedding the pose in a Euclidean space. This is done by picking the first two columns of the pose rotation matrix $r \in \sothree$. These columns each transform again as a vector with representation $\rho_1$.
This forms an equivariant embedding $\iota: \sothree \hookrightarrow \R^{2 \times 3}$, whose image is two orthogonal 3-vectors of unit norm. Via the Gram-Schmidt procedure, we can define an equivariant map $\pi: \R^{2 \times 3} \to \sothree$ (defined almost everywhere), that is a left inverse to the embedding: $\pi \circ \iota = \text{id}_\sothree$.
Combining with the translation, the roto-translational pose of each object is thus embedded as three $\rho_1$ vectors.

We also tested the performance of the baseline Diffuser method on this reparameterization of the state but found worse results.

\xhdr{Hyperparameters}
\label{app:kuka_hyperparams}
We also follow the choices of \citet{janner2022planning}, except that we experiment with a linear noise schedule as an alternative to the cosine schedule they use. For each model and each dataset, we train the diffusion model with both noise schedules and report the better of the two results.
