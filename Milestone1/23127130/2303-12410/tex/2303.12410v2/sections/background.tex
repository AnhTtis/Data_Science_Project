%================================================================================
\section{Background}
\label{sec:background}
%================================================================================

\xhdr{Equivariant deep learning}
Equivariant networks directly encode the symmetries described by a group $G$ in their architecture. For the purposes of this paper, we are interested in the symmetries of 3D space, which include translations and rotations and are described by the special Euclidean group $\sethree$, discrete-time translations $\mathbb{Z}$, and object permutations, which are defined using the symmetric group of $n$ elements $\mathbb{S}_n$. We now recall the main definition of equivariance and invariance of functions.

\begin{definition}
A function $f: \mathcal{X} \to \mathcal{Y}$ is called $G$-equivariant if $g \cdot f(x) = f(g \cdot x)$ for all $g \in G$ and $x \in \mathcal{X}$. Here $\mathcal{X}$ and $\mathcal{Y}$ are input and output spaces that carry a $G$ action denoted by $\cdot$. The function $f$ is called $G$-invariant if the group action in $\mathcal{Y}$ is trivial, $g \cdot y = y$. 
\end{definition}

We will focus on $\mathcal{X} = \R^n$, $\mathcal{Y} = \R^m$, and linear group actions or representations, which are group homomorphisms $\rho : G \to \mathrm{GL}(\mathbb{R}^k)$.
Examples include rotation and permutation matrices.
For a more comprehensive introduction to group and representation theory, we direct the interested reader to Appendix \ref{app:group_theory_intro} or \citet{esteves2020theoretical} and \citet{bronstein2021geometric}.

For generative modeling, we seek to model $G$-invariant densities. As proven in \citep{kohler2020equivariant, bose2021equivariant,papamakarios2021normalizing}, given a $G$-invariant prior density it is sufficient to construct a $G$-equivariant map to reach the desired $G$-invariant target density. 
In Sec.~\ref{sec:equivariant_diffuser}, we design $G$-equivariant diffusion architectures to model a distribution of trajectories that are known to be symmetric with respect to the product group $\fullgroup$.

\xhdr{Diffusion models}
Diffusion models~\citep{sohl2015deep} are latent variable models that generate data by iteratively inverting a diffusion process.
This diffusion process starts from a clean data sample $x \sim q(x_0)$ and progressively injects noise for $i \in [T]$ steps until the distribution is pure noise. The reverse, generative process takes a sample from a noise distribution and denoises it by progressively adding back structure, until we return to a sample that resembles being drawn from the empirical data distribution $p(x)$.

In diffusion models, it is customary to choose a parameter-free diffusion process (\eg  Gaussian noise with fixed variance). Specifically, we may define $q(x_{t} | x_{t-1})$ as the forward diffusion distribution modeled as a Gaussian centered around the sample at timestep $x_{t-1}$:
$q(x_{t} | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t}x_{t-1}, \beta_t I)$,
where $\beta_t$ is a known variance schedule. The reverse generative process is learnable and can be parametrized using another distribution $p_{\theta}(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_{\theta}(x_t, t), \sigma^2_t I)$, and the constraint that the terminal marginal at time $T$ is a standard Gaussian---i.e. $p(x_T) = \mathcal{N}(0, I)$. The generative process can be learned by maximizing a variational lower bound on the marginal likelihood.
In practice, instead of predicting the mean of the noisy data, it is convenient to predict the noise level $\epsilon_t$ directly \citep{ho2020denoising}. Furthermore, to perform low-temperature sampling in diffusion models it is possible to leverage a pre-trained classifier to guide the generation process \citep{dhariwal2021diffusion}. To do so we can modify the diffusion score by including the gradient of the log-likelihood of the classifier $\bar{\epsilon}_{\theta}(x_t, t, y) = \epsilon_{\theta}(x_t, t) - \lambda \sigma_t \nabla_{x_t} \log p(y | x_t) $, where $\lambda$ is the guidance weight and $y$ is the label. 

%--------------------------------------------------------------------------------
\xhdr{Trajectory optimization with diffusion}
We are interested in modeling systems that are governed by discrete-time dynamics of a state $s_{h+1} = f(s_h, a_h)$, given the state $s_h$ and action $a_h$ taken at timestep $h$. The goal in trajectory optimization is then to find a sequence of actions $\mathbf{a}^*_{0:H}$ that maximizes an objective (reward) $\mathcal{J}$ which factorizes over per-timestep rewards $r(s_h, a_h)$. Formally, this corresponds to the optimization problem
$\mathbf{a}^*_{0:H} = \arg \max_{a_{0:H}} \mathcal{J}(s_0, a_{0:H}) = \arg \max_{a_{0:H}} \sum_{h=0}^H r(s_h, a_h)$,
where $H$ is the planning horizon and $\tau = (s_0, a_0, \dots, s_H, a_H)$ denotes the trajectory. 

A practical method to solve this optimization problem is to unify the problem of learning a model of the state transition dynamics and the problem of planning with this model into a single generative modeling problem. \Citet{janner2022planning} propose to train a diffusion model on offline trajectory data consisting of state-action pairs, learning a density $p_{\theta}(\tau)$. Planning can then be phrased as a conditional sampling problem: finding the distribution $\Tilde{p}_{\theta}(\tau) \propto p_{\theta}(\tau) c(\tau)$ over trajectories $\tau$ where $c(\tau)$ encodes constraints on the trajectories and specifies the task for instance as a reward function. Diffusion models allow conditioning in a way similar to inpainting in generative image modeling, and test-time reward maximization in analogy to classifier-based guidance.
