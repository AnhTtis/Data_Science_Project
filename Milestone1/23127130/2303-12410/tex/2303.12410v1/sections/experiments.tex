
%================================================================================
\section{Experiments}
\vspace{-5pt}
\label{sec:experiments}
%================================================================================

\looseness=-1
We demonstrate the effectiveness of incorporating symmetries as a powerful inductive bias in the Diffuser algorithm with experiments in two environments. The first environment is a $3$D navigation task, in which an agent needs to navigate a number of obstacles to reach a goal state. Rewards are awarded based on the distance to the goal at each step, with penalties for collisions with obstacles. The position of the obstacles and the goal state are different in each episode and part of the observation. For simplicity, the actions directly control the acceleration of the agent and we use identical spherical obstacles.
Please see Fig.~\ref{fig:edgi_sketch} for a schematic representation of this task and Appendix \ref{app:pointmass} for more details and the reward structure for this task.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{l c rrrr c rr}
        \toprule
         && \multicolumn{4}{c}{\textbf{Standard setting}} && \multicolumn{2}{c}{\textbf{$\sothree$ generalization}}\\
         Environment && {BCQ} & {CQL} & {Diffuser}& \eqd (ours) && {Diffuser}& \eqd (ours) \\
         \cmidrule{1-1} \cmidrule{3-6} \cmidrule{8-9}
         Navigation && -- & -- & \bestresult{94.9}{3.9} & \bestresult{95.1}{3.4} && \result{5.6}{4.4} & \bestresult{83.3}{3.5} \\
         \cmidrule{1-1} \cmidrule{3-6} \cmidrule{8-9}
         Unconditional && $\hphantom{0}0.0$ & $24.4$& \bestresult{61.3}{2.7} & \bestresult{62.0}{2.1} && \result{39.3}{2.5} & \bestresult{59.9}{2.4} \\
         Conditional && $\hphantom{0}0.0$ & $\hphantom{0}0.0$ & \bestresult{52.3}{3.5} &  \result{45.8}{4.3} && \result{17.7}{2.3} & \bestresult{37.9}{5.8} \\
         Rearrangement && $\hphantom{0}0.0$ & $\hphantom{0}0.0$ & \bestresult{54.0}{3.5} & \bestresult{53.0}{3.5} && \result{20.3}{2.7} & \bestresult{48.8}{3.6}\\
         Average && $\hphantom{0}0.0$ & $\hphantom{0}8.1$ & \bestresult{55.9}{1.9} &  \bestresult{53.6}{2.0} && \result{25.8}{1.4} & \bestresult{48.9}{2.4} \\
        \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{Performance on navigation tasks and block stacking problems with a Kuka robot. We report normalized cumulative rewards, showing the mean and standard errors over 100 episodes. Results consistent with the best results within the errors are bold. BCQ and CQL results are taken from \citet{janner2022planning}; for Diffuser, we show our reproduction using their codebase. \textbf{Left}: Models trained on the standard datasets. \textbf{Right}: $\sothree$ generalization experiments, with training data restricted to specific spatial orientations such that the agent encounters previously unseen states at test time.}
    \label{tab:standard_dataset_results}
    \vspace{-10pt}
\end{table}

In our remaining experiments, the agent controls a simulated Kuka robotic arm interacting with four blocks on a table. Following \citet{janner2022planning}, we consider three different tasks: an unconditional block stacking task, a conditional block stacking task where the stacking order is specified, and a rearrangement problem, in which the stacking order has to be changed in a particular way.
For both environments, we generate an offline trajectory dataset of roughly $10^5$ (navigation) or $10^5$ (manipulation) trajectories. We describe the setup in detail in Appendix \ref{app:kuka}.

\looseness=-1
\xhdr{Algorithms}
We train our \eqd on the offline dataset and use conditional sampling to plan the next actions. For the conditional and rearrangement tasks in the Kuka environment, we also use classifier guidance following \citet{janner2022planning}.
As our main baseline, we compare our results to the (non-equivariant) Diffuser model~\citep{janner2022planning}. We also compare two model-based RL baselines reported by~\citep{janner2022planning}, BCQ \citep{fujimoto2019off} and CQL \citep{kumar2020conservative}.

\looseness=-1
\xhdr{Task performance}
We report the results on both navigation and object tasks in Tab.~\ref{tab:standard_dataset_results}. For each environment, we evaluate $100$ episodes and report the average reward and standard error for each method. In the navigation task, the baseline diffuser fails to solve the problem, even after substantially increasing the model's capacity compared to the hyperparemeters used in \citet{janner2022planning}. \eqd achieves a substantially better performance.  On the Kuka environment, we find that \eqd achieves rewards comparable with the original Diffuser model within the error bars and both methods clearly outperform the BCQ and CQL baselines.

\looseness=-1
\xhdr{Sample efficiency}
Next, we study the sample efficiency by training \eqd and Diffuser models on small subsets of the training data. The results in Fig.~\ref{fig:sample_efficiency} show that our \eqd model achieves reasonable rewards in both environments even when training with only on $0.1\%$ of the training data, while the baseline Diffuser struggles in this setting. This provides evidence for the benefits of the inductive bias of equivariant models and matches similar observations in other works for using symmetries in an RL context \citep{Van_der_Pol2020-mm, Walters2020-iz, Mondal2021-hu, rezaei2022continuous, Deac2023-tg}.

\xhdr{Group generalization}
Finally, we demonstrate that equivariance improves generalization across the $\sothree$ symmetry group. On both environments, we train \eqd and Diffuser models on restricted offline datasets in which all trajectories are oriented in a particular way. In particular, in the navigation environment, we only use training data that navigates towards a goal location with $x = 0$. In the robotic manipulation tasks, we only use training trajectories where the red block is in a position with $x = 0$ at the beginning of the episode. We test all agents on the original environment, where they encounter goal positions and block configurations unseen during training. We show results for these experiments in Tab.~\ref{tab:standard_dataset_results}. The original Diffuser performs substantially worse, showing its limited capabilities to generalize to the new setting. In contrast, the performance of \eqd is robust to this domain shift, confirming that equivariance helps in generalizing across the symmetry group.

%--------------------------------------------------------------------------------
\begin{figure*}[t]
    \centering%
    \includegraphics[width=0.49\linewidth]{figures/pointmass_sample_efficiency.pdf}%
    \includegraphics[width=0.49\linewidth]{figures/kuka_sample_efficiency.pdf}%
    \vspace{-6pt}
    \caption{Average reward as a function of training dataset size for \eqd and Diffuser. \textbf{Left}: navigation environment. \textbf{Right}: Kuka object maniplation, averaged over the three tasks.}
    \label{fig:sample_efficiency}
    \vspace{-12pt}
\end{figure*}
%--------------------------------------------------------------------------------
