%================================================================================
\section{Related work}
\vspace{-5pt}
\label{sec:related}
%================================================================================

\looseness=-1
Our work builds on two foundational lines of research: framing planning as a generative modeling problem and equivariant deep learning. 
The closest work to ours is the original Diffuser paper~\citep{janner2022planning} which we used as a baseline. Concurrent to our work, Diffuser was extended by \citet{ajay2022conditional} who used a separate inverse dynamics model and classifier-free guidance. The key novelty of our work is that we make this approach aware of the symmetry structure of planning problems through a new $\fullgroup$-equivariant denoising network yielding an invariant distribution over trajectories while allowing for soft symmetry breaking as dictated by the task.

\xhdr{Equivariant deep learning}
Baking in symmetries into deep learning architectures was first studied in the work of \citep{cohen2016group} for geometric transformations, and the DeepSet architecture for permutations \citep{zaheer2017deep}. Followup work to group convolutional networks focused on both spherical geometry \citep{cohen2018spherical} and building kernels using irreducible group representations \citep{cohen2016steerable,weiler2019general,cesa2021program}. For symmetries of the 3D space---\ie subgroups of $\textrm{E}(3)$---a dominant paradigm is to use the message passing framework \citep{gilmer2017neural} along with geometric quantities like positions, velocities, and relative angles \citep{satorras2021n,schutt2021equivariant,batatia2022mace}. 

\xhdr{Equivariance in RL}
The role of symmetries has also been explored in reinforcement learning problems with a body of work focusing on symmetries of the joint state-action space of an MDP \citep{Van_der_Pol2020-mm,Walters2020-iz,Mondal2021-hu,Muglich2022-id,wang2022so,wang2022robot,Cetin2022,rezaei2022continuous}. More recently, model-based approaches---like our proposed \eqd---have also benefited from increased data efficiency through the use of symmetries of the environment~\citep{Deac2023-tg}.

\xhdr{Equivariant generative models}
Early efforts in learning invariant densities using generative models utilized the continuous normalizing flow (CNF) framework. A variety of works imbued symmetries by designing equivariant vector fields
\citep{kohler2020equivariant, rezende2015variational, bose2021equivariant}. As flow-based models enjoy exact density estimation, their application is a natural fit for applications in theoretical physics \citep{boyda2020sampling, kanwar2020equivariant} and modeling equivariant densities on manifolds \citep{katsman2021equivariant}. 
Other promising approaches to CNFs include equivariant score matching \citep{De_Bortoli2022-fe} and diffusion models \citep{hoogeboom2022equivariant, xu2022geodiff,igashov2022equivariant}. Our proposed \eqd model extends the latter category to the product group $\fullgroup$ and increases flexibility with respect to the data representations.
