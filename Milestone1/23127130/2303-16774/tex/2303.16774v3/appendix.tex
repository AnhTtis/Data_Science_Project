% Appendices
\rev{
\subsection{Proof of Theorem~\ref{thm:strong-dob-average}}\label{app:sec:strong-dob-average}

Using Eqs.~\eqref{eq:bindex-strong-k} and \eqref{eq:contribution} one can derive 
Eq.~\eqref{eq:bindex-strong} as a weighted average of local balance:
\begin{equation}
\begin{split}
    R(\beta)
    &= \frac{\tr\mathbf{W}(\mathbf{A}, \beta)}{\tr\mathbf{W}(|\mathbf{A}|, \beta)} \\
    &= \frac{1}{\tr\mathbf{W}(|\mathbf{A}|, \beta)}
    \sum_k\frac{\beta^k}{k!}\tr\mathbf{A}^k \\
    &= \sum_k\frac{\beta^k}{k!} 
    \times \frac{\tr|\mathbf{A}|^k}{\tr\mathbf{W}(|\mathbf{A}|, \beta)}
    \times \frac{\tr\mathbf{A}^k}{\tr|\mathbf{A}|^k} \\
    &= \sum_kC_k(\beta)R_k
\end{split}
\end{equation}
Now, the above result can be rewritten in terms of Eqs. \eqref{eq:dob-strong} and \eqref{eq:dob-strong-k}:
\begin{equation}
\begin{split}
    R(\beta) 
    &= \sum_kC_k(\beta)R_k \\
    2B(\beta) - 1
    &= \sum_kC_k(\beta)(2B_k -1) \\
    &= 2\sum_kC_k(\beta)B_k - \overbrace{\sum_kC_k(\beta)}^{=1}
\end{split}
\end{equation}
The last term is equal to $1$ thanks to the normalization property of the contribution scores.
Thus, after some straightforward algebra, we have that:
\begin{equation}
    B(\beta) = \sum_kC_k(\beta)B_k
\end{equation}
which ends the proof. \qedsymbol
}

\newpage
\rev{
\subsection{Weak DoB as a weighted average}\label{app:sec:weak-dob-average}

\begin{theorem}\label{thm:weak-dob-average}
    Let $G$ be a signed graph, $\beta > 0$ a resolution parameter and 
    $2 \leq k = k_{min}, \ldots, k_{\max}$ a sequence of consecutive integers. Then:
    \begin{equation*}
        W(\beta) = \sum_{k}C_k(\beta)W_k
    \end{equation*}
\end{theorem}

\begin{proof}\label{proof:weak-dob-average}
We first rewrite Eq.~\eqref{eq:dob-weak} in terms of Eq.~\ref{eq:weakly-unbalanced-walks-k}
and then follow with a few simple transformations to get the final result:
\begin{equation}    
\begin{split}
    W(\beta) 
    &= 1 - \frac{\tr\mathbf{V}(\mathbf{A}, \beta)}{\tr\mathbf{W}(|\mathbf{A}|, \beta)} \\
    &= 1 - \frac{\sum_k\frac{\beta^k}{k!}\tr\mathbf{V}_k(\mathbf{A})}{\tr\mathbf{W}(|\mathbf{A}|, \beta)} \\
    &= 1 - \sum_k\frac{\beta^k}{k!}\tr\mathbf{V}_k(\mathbf{A})\frac{1}{\tr\mathbf{W}(|\mathbf{A},\beta)} 
    \times \frac{\tr|\mathbf{A}|^k}{\tr|\mathbf{A}|^k} \\
    &= 1 - \sum_k\overbrace{\frac{\beta^k}{k!}\frac{\tr|\mathbf{A}|^k}{\tr\mathbf{W}(|\mathbf{A}|,\beta}}
    ^{C_k(\beta)}\frac{\tr\mathbf{V}_k(\mathbf{A})}{\tr|\mathbf{A}|^k} \\
    &= \sum_kC_k(\beta)\left(1 - \frac{\tr\mathbf{V}_k(\mathbf{A})}{\tr|\mathbf{A}|^k}\right) \\
    &= \sum_kC_k(\beta)W_k
\end{split}
\end{equation}
where in the second last equality we used the fact that $\sum_kC_k(\beta) = 1$.
\end{proof}
}

\rev{
\newpage
\subsection{Accuracy of numerical approximations}\label{app:sec:numerical}

MSB uses two different numerical approximations to attain high computational efficiency.
The first approximation happens when truncating power series to include only the terms of orders
$k_{\min}, \ldots, k_{\max}$. However, this approximation introduces no significant error by design,
as LP ensures that higher order terms have very low and monotonically decreasing contributions to the 
overall DoB calculations. Thus, as long as enough terms are included, 
and typically about a dozen or two is enough,
the truncation introduces no noticeable error. In principle, the lowest number of terms necessary
for attaining a given cumulative contribution score can be determined easily by inspecting the contribution
profile. However, here we used a simple rule-of-thumb and in all cases, unless specified otherwise,
used $k_{\max} = 30$, which is typically more than enough (see Fig.~\ref{app:fig:accuracy}A).

\begin{figure}[htb!]
\centering
\includegraphics[width=\textwidth]{figs/accuracy}
\caption{
    Effects of MSB approximations assessed using bill co-sponsorship network
    from the U.S. Senate during 114th Congress ($|V| = 100$, $|E| = 3696$)
    as well as its randomized counterparts based on Erdős–Rényi model and configuration
    model~\cite{newmanNetworks2018}.
    \textbf{(A)}~Contribution profiles are clearly almost identical for the original and
    randomized networks. Crucially, at $\beta = \beta_{\max}$ almost all of the cumulative
    contribution score is driven by leading low order terms, meaning that higher order terms
    (roughly $k > 10$) can be safely omitted. The inset plot presents the same data on log-log scale
    in order to better display the tail of the distribution.
    \textbf{(B)}~Errors of DoB measures based on leading eigenvalues calculated relative
    to values obtained using full spectra. Errors are typically low even for $m = 1$
    and in all cases quickly decrease as $m$ increases. Moreover, in most of the cases
    they are lower for the real network 
    (as compared to its randomized counterparts based on the Erdős–Rényi and configuration models), 
    which is consistent with the fact that errors should be lower for networks with heterogeneous
    distributions of eigenvalues.
}
\label{app:fig:accuracy}
\end{figure}

The last approximation happens when only $m$ leading eigenpairs from the both ends of the spectrum
are used. This allows for solving the corresponding eigenproblems and running other downstream 
calculations much faster. Moreover, as discussed in Sec.~\ref{app:sec:analytic}, this approximation
is optimal and can be highly accurate, especially for real-world networks with heterogeneous spectra.
Fig.~\ref{app:fig:accuracy}B provides an empirical support for this claim.
}

\newpage
\subsection{Analytic functions of real symmetric matrices}\label{app:sec:analytic}

\rev{
Let $\mathbf{X} \in \mathbb{R}^{n \times n}$ be a real symmetric matrix and 
$f: \mathbb{R}^{n \times n} \to \mathbb{R}^{n \times n}$ an analytic function defined over
the field of real square matrices. 
Then, $f\left(\mathbf{X}\right) = \mathbf{Q}f\left(\mathbf{\Lambda}\right)\mathbf{Q}^\top$,
where $\mathbf{\Lambda}$ is a real diagonal matrix with eigenvalues of $\mathbf{X}$ 
satisfying $|\lambda_1| \geq |\lambda_2| \geq \ldots \geq |\lambda_n|$
and the columns of $\mathbf{Q}$ are the corresponding eigenvectors. 
This implies that:
\begin{align}
    f\left(\mathbf{X}\right)_{ij} &= \sum_{r=1}^n \mathbf{Q}_{ir}f(\lambda_r)\mathbf{Q}_{jr}
    \label{eq:methods:fXij}\\
    \tr{}f\left(\mathbf{X}\right) &= \sum_{r=1}^n f(\lambda_r)
    \label{eq:methods:trfX}
\end{align}
In particular, $k$th power is given by $\mathbf{X}^k = \mathbf{Q}\mathbf{\Lambda}^k\mathbf{Q}^\top$
and exponential by $e^{\mathbf{X}} = \mathbf{Q}e^{\mathbf{\Lambda}}\mathbf{Q}^\top$.

Note that Eqs. \eqref{eq:methods:fXij} and \eqref{eq:methods:trfX} can be approximated using only
$m$ leading eigenvalues, which allows for efficient computations.
In particular, as a consequence of Eckart-Young low-rank approximation 
theorem~\cite{eckartApproximationOneMatrix1936},
the error when reconstructing $\mathbf{Y} = f(\mathbf{X})$ based on $m$ leading eigenvalues and eigenvectors,
provided that $\mathbf{X}$ is symmetric and $f$ is analytic, is:
\begin{equation}\label{eq:methods:fX-error}
    \norm{\mathbf{Y} - \mathbf{\hat{Y}}}_F
    = \sqrt{\sum_{i=m+1}^n f(\lambda_i)^2}
\end{equation}
where $\mathbf{\hat{Y}}$ is the reconstructed matrix, and $\norm{\cdot}_F$ is Frobenius norm.
This approximation produces a matrix minimizing the error across all rank $m$ matrices and therefore
is optimal. Moreover, it is clear that if $|f(x)|$ is increasing the approximation is more accurate
for networks with heterogeneous spectra, or when some eigenvalues are much larger (in absolute value)
than others, which is a frequent property of real-world networks.
}

\newpage
\subsection{Computational complexity}\label{app:sec:efficiency}

Below are computation times for global, local and node-wise DoB measures for the three large
networks studied in this paper (Epinions, Slashdot and Wikipedia). Performance was assessed
using a laptop with AMD Ryzen 9 5900HX CPU and 16Gb of RAM. As evident in Fig.~\ref{app:fig:efficiency},
all running times were arguably short. Global DoB and balance profiles were calculated in about
1 second or much less. Node-wise measures (for all nodes) were calculated in no more than 
16 seconds (in the case of the largest network). All results include both the time needed for solving 
the eigenproblem(s), which can be cached and re-used in multiple computations, as well as any downstream
computations using eigenvalues and eigenvectors. Furthermore, in all cases computation times seem to 
scale with respect to $m$ in a very similar fashion with an average slope coefficient (in log-log scale)
of about $0.61$. This indicates that, at least for relatively low values of $m$, MSB computation times
are only moderately (sub-linearly) affected when increasing the number of used leading eigenpairs.

\begin{figure}[htb!]
\centering
\includegraphics[width=.85\textwidth]{figs/efficiency}
\caption{
Running times of global, local and node-wise DoB measures (both strong and weak).
Lines correspond to median times (over 10 repetitions) and bounds to 1st and 9th deciles.
}
\label{app:fig:efficiency}
\end{figure}

\newpage
\subsection{Descriptive statistics for the U.S. Congress co-sponsorship networks}
\label{app:sec:congress}

\begin{table}[ht!]
\footnotesize
\begin{threeparttable}
\begin{tabular}{ll|rrrrrrrrr}
\toprule
Chamber & Congress & $f_R$ & $f_D$ & $|V|$ & $|E|$ & $f_+$ & $B$ & $W$ & $\bar{d}$ & $d_{\text{cv}}$ \\
\midrule
\multirow[t]{21}{*}{House} & 93 & 0.43 & 0.56 & 446 & 18083 & 0.56 & 0.71 & 0.94 & 81.09 & 0.73 \\
 & 94 & 0.33 & 0.67 & 445 & 19503 & 0.58 & 0.71 & 0.95 & 87.65 & 0.70 \\
 & 95 & 0.34 & 0.66 & 444 & 21133 & 0.59 & 0.68 & 0.96 & 95.19 & 0.60 \\
 & 96 & 0.38 & 0.62 & 442 & 51081 & 0.84 & 0.55 & 1.00 & 231.14 & 0.33 \\
 & 97 & 0.45 & 0.54 & 447 & 49364 & 0.82 & 0.57 & 1.00 & 220.87 & 0.35 \\
 & 98 & 0.39 & 0.61 & 444 & 48721 & 0.75 & 0.59 & 0.99 & 219.46 & 0.31 \\
 & 99 & 0.42 & 0.57 & 443 & 49764 & 0.72 & 0.61 & 0.98 & 224.67 & 0.29 \\
 & 100 & 0.42 & 0.58 & 446 & 50688 & 0.73 & 0.62 & 0.99 & 227.30 & 0.29 \\
 & 101 & 0.42 & 0.58 & 449 & 56231 & 0.70 & 0.62 & 0.98 & 250.47 & 0.25 \\
 & 102 & 0.40 & 0.60 & 447 & 58067 & 0.68 & 0.63 & 0.98 & 259.81 & 0.25 \\
 & 103 & 0.42 & 0.58 & 446 & 59092 & 0.68 & 0.70 & 0.98 & 264.99 & 0.23 \\
 & 104 & 0.53 & 0.46 & 445 & 62154 & 0.72 & 0.78 & 1.00 & 279.34 & 0.25 \\
 & 105 & 0.52 & 0.48 & 449 & 66701 & 0.69 & 0.80 & 1.00 & 297.11 & 0.23 \\
 & 106 & 0.51 & 0.49 & 442 & 63652 & 0.67 & 0.83 & 0.99 & 288.02 & 0.24 \\
 & 107 & 0.51 & 0.48 & 447 & 63851 & 0.68 & 0.84 & 0.99 & 285.69 & 0.24 \\
 & 108 & 0.52 & 0.47 & 444 & 66277 & 0.67 & 0.84 & 0.99 & 298.55 & 0.24 \\
 & 109 & 0.53 & 0.47 & 445 & 66700 & 0.68 & 0.82 & 1.00 & 299.78 & 0.24 \\
 & 110 & 0.46 & 0.54 & 452 & 70923 & 0.67 & 0.82 & 0.99 & 313.82 & 0.20 \\
 & 111 & 0.41 & 0.59 & 451 & 70160 & 0.68 & 0.77 & 0.99 & 311.13 & 0.21 \\
 & 112 & 0.54 & 0.45 & 450 & 77872 & 0.66 & 0.82 & 1.00 & 346.10 & 0.18 \\
 & 113 & 0.53 & 0.47 & 447 & 75771 & 0.64 & 0.86 & 1.00 & 339.02 & 0.18 \\
 & 114 & 0.56 & 0.44 & 446 & 75180 & 0.65 & 0.86 & 1.00 & 337.13 & 0.19 \\
 \midrule
\multirow[t]{21}{*}{Senate} & 93 & 0.42 & 0.56 & 101 & 2439 & 0.76 & 0.62 & 0.99 & 48.30 & 0.35 \\
 & 94 & 0.37 & 0.61 & 100 & 2432 & 0.79 & 0.63 & 1.00 & 48.64 & 0.35 \\
 & 95 & 0.37 & 0.62 & 104 & 2336 & 0.81 & 0.57 & 1.00 & 44.92 & 0.39 \\
 & 96 & 0.41 & 0.58 & 101 & 2275 & 0.82 & 0.55 & 1.00 & 45.05 & 0.39 \\
 & 97 & 0.52 & 0.47 & 101 & 2073 & 0.79 & 0.60 & 0.99 & 41.05 & 0.37 \\
 & 98 & 0.53 & 0.47 & 101 & 2194 & 0.76 & 0.58 & 0.99 & 43.45 & 0.33 \\
 & 99 & 0.52 & 0.48 & 101 & 2177 & 0.75 & 0.61 & 0.99 & 43.11 & 0.33 \\
 & 100 & 0.46 & 0.54 & 101 & 2143 & 0.72 & 0.60 & 0.99 & 42.44 & 0.36 \\
 & 101 & 0.44 & 0.54 & 101 & 2445 & 0.68 & 0.63 & 0.98 & 48.42 & 0.31 \\
 & 102 & 0.42 & 0.56 & 102 & 2479 & 0.71 & 0.64 & 0.99 & 48.61 & 0.31 \\
 & 103 & 0.44 & 0.54 & 101 & 2257 & 0.72 & 0.70 & 0.99 & 44.69 & 0.35 \\
 & 104 & 0.52 & 0.46 & 102 & 2324 & 0.74 & 0.81 & 1.00 & 45.57 & 0.38 \\
 & 105 & 0.53 & 0.45 & 100 & 3002 & 0.70 & 0.81 & 1.00 & 60.04 & 0.27 \\
 & 106 & 0.53 & 0.45 & 102 & 2930 & 0.72 & 0.78 & 0.99 & 57.45 & 0.25 \\
 & 107 & 0.48 & 0.50 & 101 & 2522 & 0.73 & 0.70 & 0.99 & 49.94 & 0.30 \\
 & 108 & 0.50 & 0.48 & 100 & 2387 & 0.74 & 0.79 & 0.99 & 47.74 & 0.28 \\
 & 109 & 0.53 & 0.45 & 101 & 2823 & 0.73 & 0.82 & 0.99 & 55.90 & 0.25 \\
 & 110 & 0.49 & 0.49 & 102 & 2779 & 0.70 & 0.85 & 0.99 & 54.49 & 0.30 \\
 & 111 & 0.39 & 0.59 & 109 & 3645 & 0.74 & 0.68 & 1.00 & 66.88 & 0.27 \\
 & 112 & 0.48 & 0.50 & 101 & 3914 & 0.69 & 0.78 & 1.00 & 77.50 & 0.20 \\
 & 113 & 0.44 & 0.54 & 105 & 3932 & 0.65 & 0.86 & 1.00 & 74.90 & 0.24 \\
 & 114 & 0.54 & 0.44 & 100 & 3696 & 0.61 & 0.96 & 1.00 & 73.92 & 0.20 \\
 \bottomrule
\end{tabular}
\begin{tablenotes}
    \footnotesize
    \item $f_R, f_D$ -- fraction of Republicans/Democrats 
    (may not sum up to 1 due to the presence of other parties and/or independents)
    \item $|V|, |E|$ -- number of nodes/edges
    \item $f_+$ -- fraction of positive edges
    \item $B, W$ -- strong/weak degree of balance for $\beta \coloneqq \beta_{\max}$
    \item $\bar{d}, d_{\text{cv}}$ -- average degree and coefficient of variation of degree distribution
\end{tablenotes}
\end{threeparttable}
\end{table}
