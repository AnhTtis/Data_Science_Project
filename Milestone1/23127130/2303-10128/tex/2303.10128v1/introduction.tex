It has been argued that linguistic universals are a myth \parencite{Evans2009a}, but this neglects the statistical regularities that the quantitative linguistic community has been investigating for many decades. A salient case is Zipf's law of abbreviation, the tendency of more frequent words to be shorter \parencite{Zipf1949a}. 
%, that is a robust statistical pattern of languages. 
It holds across language families \parencite{Piantadosi2011a,Bentz2016a,Levshina2022a,Meylan2021a, Koplenig2022a}, writing systems \parencite{Wang2015a,Sanada2008a} and modalities \parencite{Boerstell2016a,Torre2019a}, and also when word length in characters is replaced by word duration in time \parencite{Hernandez2019a}. 
Furthermore, the number of species where a parallel of this law has been confirmed is growing over time \parencite{Semple2021a} 
% although exceptions have been found \cite{Ferrer2012a,Safryghin2022a,Blink,otherauthorsinbiblio} 
\footnote{The interested reader can check the latest discoveries on this law in \href{https://cqllab.upc.edu/biblio/laws/}{Bibliography on laws of language outside human language}.}. In language sciences, research on the law of abbreviation in languages measures word length in discrete units (e.g., characters) whereas, in biology, research on the law in other species typically uses duration in time. Here, we aim to reduce the gulf that separates these two traditions by promoting research on the law of abbreviation on word durations.


G. K. Zipf believed that the law of abbreviation constituted {\em indirect} evidence of the minimization of the cost of using words \parencite{Zipf1949a}. At present, Zipf's view is supported by standard information theory and its extensions: the main argument is that the minimization of $L$, the mean word length, that is indeed a simplification of Zipf's cost function\footnote{He  referred to the cost function as ``minimum equation'' \parencite{Zipf1949a}}, leads to the law of abbreviation \parencite{Ferrer2012d,Ferrer2019c}. In the language of information theory, the minimization of mean word length is known as compression. In the language of quantitative linguistics, $L$ 
is the average length of tokens from a repertoire of $n$ types, that is defined as 
\begin{equation}
L = \sum_{i=1}^n p_i l_i,
\label{eq:mean_type_length}    
\end{equation}
where $p_i$ and $l_i$ are, respectively, the probability and the length of the $i$-th type.
In practical applications, $L$ is calculated replacing $p_i$ by the relative frequency of a type, that is
$$p_i = f_i/ T,$$ 
where $f_i$ is the absolute frequency of a type and $T$ is the total number of tokens, i.e.
$$T = \sum_{i=1}^n f_i.$$
This leads to a definition of $L$ that is 
$$L = \frac{1}{T} \sum_{i=1}^n f_i l_i.$$

At present, the mathematical link between the law of abbreviation and compression has been established under the assumption that words are coded optimaly so as to minimize $L$. If words are coded optimaly, the correlation between the frequency of a word and its duration cannot be positive \parencite{Ferrer2019c}. Thus, a lack of correlation between the frequency of a word and its duration does not imply absence of compression. Furthermore, it is not a warranted assumption that languages code words optimaly. Therefore, an approach to find {\em direct} evidence of compression getting rid of the assumption of optimal coding is required.  


As a first approach, one could compare the value of $L$ of a language against $L_{max}$, the maximum value that $L$ could achieve in this language. The larger the gap between $L$ and $L_{max}$, the higher the level of compression in the language. However, the problem is that $L_{max}$ can be infinite {\em a priori}. To fix that problem, one could restrict $L_{max}$ to be finite but then this raises the question of what should be the finite value of $L_{max}$ and why. For these reasons, here we resort to the notion of random baseline, an imaginary line that we use for comparison against the actual value of $L$, that is defined assuming some random mapping of word types into strings. 
In previous research, the random baseline was defined by the average word length resulting from a shuffling of the current length/duration of types \parencite{Ferrer2012d,Heesen2019a} so as to check if $L$ was smaller than expected by chance in that random mapping \parencite{Ferrer2012d,Heesen2019a}. Critically, an exact method to compute the random baseline, namely the expected word length in these shufflings, is missing.


The remainder of the article is organized as follows. In \autoref{sec:baselines}, we introduce the definition of $L_r$, the random baseline, that we will use to explore direct evidence of compression. In particular, we derive a simple formula for $L_r$ that will simplify future research on compression in natural communication systems.
In \autoref{sec:material} and \autoref{sec:methodology}, we present, respectively, the materials and methods that will be used to provide further evidence of compression and the law of abbreviation in real languages with emphasis on word durations. In \autoref{sec:methodology}, we present a new unsupervised method to exclude words with foreign characters in line with good practices for research on linguistic laws and communicative efficiency \parencite{Meylan2021a}.
In \autoref{sec:results}, we show that the law of abbreviation holds without exceptions in a wide sample of languages, independently of the unit of measurement of word length, namely characters or duration in time, %Only a word of caution is needed for Panjabi, whose weaker support for the law is likely due to undersampling.
providing further indirect evidence of compression in languages.
In addition, the random baseline indicates that word lengths are systematically below chance, across linguistic families and writing systems, independently of the unit of measurement (length in characters or duration in time), providing direct evidence of compression.  
Finally, in \autoref{sec:discussion}, we discuss the findings in relation to the potential universality of the law of abbreviation and the universality of compression in languages. We also make proposals for future research. 
