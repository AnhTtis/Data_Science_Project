All the code used to produce the results is available in the repository of the article  \footnote{
In the \textit{code} folder of \url{\repository}.}.

\subsection{The units of length}

\subsubsection{Duration}

The duration of a word for a given language is estimated by computing the median duration in seconds across all its occurrences in utterances in the CV corpus. 
All words with equal orthographic form are assumed to be the same type. The median is preferred over the mean as it is less sensitive to outliers (that may be produced by forced alignment errors) and better suited to deal with heavy-tailed distributions \parencite{Hernandez2019a}. Given the oral nature of the data, we do expect to observe some variation in the duration of words, 
% given
due to differences between individuals, and variation within a single individual. This is more generally in line with speakers acting as complex dynamical systems \parencite{Kello2010a}. For these reasons, median duration is preferred for research on the law of abbreviation in acoustic units \parencite{Torre2019a,Watson2020a}. 
% Average duration is still included in certain analyses as a robustness check. 

\subsubsection{Length in characters}

Word length in characters is measured by counting every Unicode UTF-8 character present in a word. Special characters such as ``='' were removed. Characters with stress accents are considered as different from their non-stressed counterpart (e.g. ``a'' and ``à'' are considered separate characters). Following best practices from \parencite{Meylan2021a}, characters were always kept in UTF-8. 

\subsubsection{Length in strokes}

Japanese Kanji and Chinese Hanzi were turned into strokes using the \textit{cihai} Python library\footnote{\url{https://github.com/cihai/cihai}}. 
In Japanese characters other than Kanji, namely Japanese Kana, the number of strokes in printed versus hand-written modality can differ (Chinese Hanzi and Japanese Kanji have the same number of strokes in printed version or hand-written version). Here we counted the number of strokes in printed form.
% In addition, %Japanese Kana was turned into handwritten strokes using a hand-crafted correspondence table
% since Japanese Kana is not part of the CJK unified character system a hand-crafted correspondence table was used.
Japanese Kana were converted into printed strokes by using a hand-crafted correspondence table, since Kana is not part of the CJK unified character system.
This table was created by us and checked by a native linguist (S. Komori from Chubu University, Japan). It is available in the repository of the article \footnote{In the \textit{data/other} folder of \url{\repository}.}.

In case of discrepancies on the number of strokes for a given character, the most typical printed version was chosen.

\subsubsection{Length in Pinyin and Romaji}

\begin{CJK*}{UTF8}{ipxm}
Chinese Pinyin was obtained using the \textit{cihai} package as above, while the Japanese Romaji was obtained with the \textit{cutlet} Python library\footnote{\url{https://github.com/polm/cutlet}}. The latter uses Kunrei-shiki  romanization (since it is the one used officially by the government of Japan) and the spelling of foreign words 
is obtained in its native reading
(e.g. ``カレー'' is romanized as ``karee'' instead of ``curry''). There are some particularities with the  romanization of Kanji characters by \textit{cutlet}. For example, in the case of the word ``year'' (年), it chose the reading of ``Nen'' instead of ``Tosi'', which would be the expected one.

A more systematic issue with Japanese romanization is that it does not provide means to indicate pitch accents, which are implicitly present in Kanji. For example, ``日本'' ``Ni$\uparrow$hon'' (``Japan'') is romanized as simply ``Nihon''. Therefore, the alphabet size of romanized Japanese is smaller than it should be, compared to other languages where, as stated before, stress accents are counted as distinctive features of characters.
\end{CJK*}

\subsection{Tokenization} 

Tokenization is already given in each dataset and we borrow it for our analyses. 
Thus tokenization methods are not uniform for CV and PUD and are not guaranteed to be uniform among languages even within each of these datasets.

\subsection{Filtering of tokens}

Examining our datasets, we noticed that in some text files there was a considerable number of unusual character strings, as well as foreign words (written in different scripts). These need to be filtered out in order to obtain a ``clean'' set of word types.
To this end we filter out tokens following a two step procedure:
\begin{enumerate}
    \item 
    {\em Mandatory elementary filtering}. This filter consists of:
    \begin{itemize}
    \item 
    {\em Common filtering}. In essence, it consists of the original tokenization and the removal of tokens containing digits. In each collection, the original tokenizer yields tokens that may contain certain punctuation marks. 
%    Removal of certain punctuation and digits (common filtering).
%    We remove tokens that contain ASCII punctuation with some exceptions and digits. 
Due to the nature of the CV dataset, the bulk of punctuation was already removed via the Montreal Forced Aligner with some exceptions. For instance, single quotation (in particular ``''') is a punctuation sign that is kept within a word token in CV, as it is necessary for the formation of clitics in multiple languages, such as in English or French. In PUD, as a part of UD, contractions are split into two word types. ``can't'' is split into ``ca'' ``n't'' (in CV ``can't'' would remain as just one token). In both collections, words containing ASCII digits are removed because they do not reflect phonemic length and can be seen as another writing system.

    \item
    {\em Specific filtering.} In case of the PUD collection, we excluded all tokens with Part-of-Speech (POS) tag `PUNCT'. In case of the CV collection, we removed tokens tagged as <unk> or null tokens, namely tokens that either could not be read or that represent pauses.
    \item
    {\em Lowercasing.} Every character is lowercased. In the case of CV, this is already given by the Montreal Forced Aligner, while in the case of PUD, tokens are lowercased by means of the {\em spaCy} python package\footnote{\url{https://spacy.io/}}.
    \end{itemize}
    
%    This filtering consists of removing tokens that contain ASCII punctuation and digits. In addition, in case of PUD collection, we excluded all tokens with POS tag 'PUNCT'.\textcolor{red} {In the case of CV collection, we removed tokens tagged as <unk> or null tokens, namely tokens that either could not be understood and pauses. Due to the nature of the CV dataset, punctuation was already removed via the Montreal forced aligner. We do not consider "'" to be a punctuation sign, as it is necessary for the formation of clitics in multiple languages, such as in English or French.
    
    \item
    {\em Optional filtering}. This is a new method that is applied after the previous filter and described in \autoref{sec:filter}. 
\end{enumerate}


\subsection{A new method to filter out unusual characters}
\label{sec:filter}

It has been pointed out that ``chunk'' words and loanwords can distort the results of quantitative analyses of word lengths \parencite{Meylan2021a}. Indeed, especially the files of the Common Voice Corpus feature a considerable number of word tokens which do not consist of characters belonging to the primary alphabet of the respective writing system. \textcite{Meylan2021a} proposed to use dictionaries to exclude such anomalous words. However, this is not feasible for our multilingual datasets, as loanword dictionaries are not available for this large number of diverse languages (\autoref{tab:coll_summary_pud} and \autoref{tab:coll_summary_cv}). The Intercontinental Dictionary Series\footnote{\url{https://ids.clld.org/}}, for example, contains only around half of the languages in our analysis, so it is not applicable to many of them. Hence, this approach would lead to a non-uniform treatment of different languages and texts. Selecting a matched set of semantic concepts across languages using a lexical database is also infeasible due to similar reasons. 

Against this backdrop, we decided to develop an unsupervised method to filter out words which contain highly unusual characters. For a given language, the method starts 
by assuming that the strings (after the mandatory filtering illustrated above) contain characters of two types: characters of the working/primary alphabet as well as other characters. We hypothesize that the latter are much less frequent than the former. Following this rationale, we apply the $k$-means algorithm of the {\em Ckmeans R} package\footnote{\url{https://cran.r-project.org/web/packages/Ckmeans.1d.dp/index.html}} to split the set of characters into the two groups based on the logarithm of the frequency of the characters\footnote{The motivation for taking logarithms of frequencies is three-fold: First, this brings observations closer together. Note that the $k$-means algorithm prefers high-density areas. Second, this transforms the frequencies into a measure of surprisal, following standard information theory \parencite{Shannon1948}. Third, manual inspection suggests that the logarithmic transformation is required to produce an accurate split.}. 
To maximize the power of the clustering method, we use the exact method with $k=2$ for one dimension instead of the customary approximate method.
We then keep the high frequency cluster as the real working alphabet and filter out the word tokens that contain characters not belonging to this high frequency cluster. 
   

% \textcolor{red}{For this purpose, we filter the characters of the languages to retain only those that are used frequently enough to be considered part of the working alphabet, even if they may not be part of the official alphabet, rather, existing entirely within foreign words. In fact, these might have become common enough so as to warp the working vocabulary.}

% Inspired by the fact that our datasets, specially Common Voice, had many word tokens that did not contain characters of the working alphabet or primary alphabet of the writing system of the language, 
% In order to generate our working alphabets, we decided to develop an unsupervised method to solve the problem of filtering \textcolor{red}{characters belonging to the language's working vocabulary and characters not belonging to it, based entirely on the analyzed text itself.} % even when dictionaries are not available, 

% A critical issue for researching on an ensemble of languages is the large and diverse nature in terms of linguistic families and writing systems.\textcolor{red}{ This diversity difficultates the application of unified methods, as what one language might need, will distort other languages. Due to this, we consider a method that does not take language specifics into account}.

We illustrate the power of the method by showing working alphabets that are obtained on CV, that is the noisiest one of the collections. 

In English, the working alphabet is defined by the 26 English letters and quotation marks (``''', ``’''). These quotation marks are used often in clitics, and as such are correctly identified as part of the encoding, since, for example, ``can't'' and ``cant'' are different words in meaning, with ``can't'' meaning ``can not'', while ``cant'' is a statement on a religious or moral subject that is not believed by the person making the statement, with the differentiating feature being the ``'''. Therefore, the working alphabet becomes 5 vowels (``a'', ``e'', ``i'', ``o'', ``u''), 21 consonants (``b'', ``c'', ``d'', ``f'', ``g'', ``h'', ``j'', ``k'', ``l'', ``m'', ``n'', ``p'', ``q'', ``r'', ``s'', ``t'', ``v'', ``w'', ``x'', ``y'', ``z'') and 2 kinds of quotation marks (``''', ``’'').

%\textcolor{red}{Ramon's concerns: in filter 1 we say that we remove punctuation marks but then we find "'", "’". Is that consistent with filter 1? Should we restrict the removal of punctuation marks in step 1?}

In Russian, the working alphabet comprises 9 vowels ( ``\foreignlanguage{russian}{а}'',   ``\foreignlanguage{russian}{о}'', ``\foreignlanguage{russian}{у}'', ``\foreignlanguage{russian}{ы}'', ``\foreignlanguage{russian}{э}'',  ``\foreignlanguage{russian}{я}'', ``\foreignlanguage{russian}{ю}'', ``\foreignlanguage{russian}{и}'', ``\foreignlanguage{russian}{е}''),
%\textcolor{red}{we don't have vowel "\foreignlanguage{russian}{ё}" since it's not present in our data}
a semivowel / consonant 
``\foreignlanguage{russian}{й}'',
20 consonants ( 
``\foreignlanguage{russian}{б}'', ``\foreignlanguage{russian}{в}'',  ``\foreignlanguage{russian}{г}'', ``\foreignlanguage{russian}{д}'',  ``\foreignlanguage{russian}{ж}'', ``\foreignlanguage{russian}{з}'',
``\foreignlanguage{russian}{к}'', ``\foreignlanguage{russian}{л}'', ``\foreignlanguage{russian}{м}'', ``\foreignlanguage{russian}{н}'', ``\foreignlanguage{russian}{п}'', ``\foreignlanguage{russian}{р}'', ``\foreignlanguage{russian}{с}'', ``\foreignlanguage{russian}{т}'', ``\foreignlanguage{russian}{ф}'', ``\foreignlanguage{russian}{х}'', ``\foreignlanguage{russian}{ц}'', ``\foreignlanguage{russian}{ч}'', ``\foreignlanguage{russian}{ш}'', ``\foreignlanguage{russian}{щ}'')
and 2 modifier letters 
(``\foreignlanguage{russian}{ъ}'', ``\foreignlanguage{russian}{ь}'').

In Italian, it comprises 5 vowels (``a'', ``e'', ``i'', ``o'', ``u''), 21 consonants (``b'', ``c'', ``d'', ``f'', ``g'', ``h'', ``j'', ``k'', ``l'', ``m'', ``n'', ``p'', ``q'', ``r'', ``s'', ``t'', ``v'', ``w'', ``x'' , ``y'', ``z'') and 6 instances of the 5 vowels containing a diacritic mark (``à'', ``è'', ``é'', ``ì'', ``ò'', ``ù''). %\textcolor{red}{ we don't have í î ó ú}

The unsupervised filter method filter is not applied to Chinese, Japanese and Korean as, given their nature, this would exclude letters that actually belong to the real alphabet. % Nevertheless, the CJK is applied to them to exclude foreign characters.
In \autoref{app:no_filter} we analyze the impact of the optional filter and provide arguments for not applying the unsupervised filter to these languages. 
As a compensation, strings that contain non-CJK characters are filtered out in Chinese and Japanese as a part of the optional filter. In Korean, only a few characters are not proper Hangul and thus such a complementary filtering is not necessary.

\subsection{Immediate constituents in writing systems}
\label{sec:immediate_constituents}

% To avoid that these two languages are hence over-represented in statistical analyses on the correlation between scores and certain parameters at the level of languages (\autoref{sec:other_desirable_properties}), we will use only their immediate constituents (namely original characters) for simplicity. The results based on original characters are hence compared with those for the other languages.  
When measuring word length in written languages, we are using \textit{immediate constituents} of written words. In Romance languages, the immediate constituents are letters of the alphabet, which are a proxy for phonemes. For syllabic writing systems (as Chinese in our dataset), these are characters that correspond to syllables. 
In addition, for Chinese and Japanese,  we are considering two other possible units for word length, which are not immediate constituents, but alternative ways of measuring word lengths which could provide useful insights: strokes and letters in Latin script romanizations. That means that for each of these languages words are unfolded into three systems, one for each unit of encoding (original characters, strokes, romanized letters/characters). In the hierarchy from words to other units, only the original characters are immediate constituents. 

\subsection{Statistical testing}

\subsubsection{Correlation}

When measuring the association between two variables, we  use both Pearson correlation and Kendall correlation \parencite{Conover1999a}. Note that the traditional view of Pearson being a measure of linear association and Kendall correlation being a measure of monotonic non-linear association has been challenged \parencite{vandenJeuvel2022a}. 

\subsubsection{How to test for the law of abbreviation}

We used a left-sided correlation test to verify the presence of the law of abbreviation. In a purely exploratory or atheoretic exploration, one should use a two-sided test. In an exploration guided by theory, namely regarding the law of abbreviation as a manifestation of compression, the test should be left-sided as theory predicts that $\tau(p,l)$ cannot be positive in case of optimal coding \parencite{Ferrer2019c}. 

\subsubsection{How to test for compression}


In the context of the null hypothesis of a random mapping of type probabilities into type lengths, testing that compression (minimization of $L$) has some effect on actual word lengths is easy because $L$ is a linear function of $r$, the Pearson correlation between word length and word probability (Appendix \ref{app:theory}). In particular,
$$L = a r + L_r,$$
where $a = (n-1)s_p s_l$, being $n$ the number of types and $s_p$ and $s_l$, respectively the standard deviation of type probabilities and type lengths.
In such random mappings, $L_r$, $s_p$ and $s_l$ remain constant and then testing if $r$ is significantly small is equivalent to testing if $L$ is significantly small (notice $a \geq 0$). 


\subsubsection{Controlling for multiple testing}

When performing multiple correlation tests at the same time, it becomes easier to reject the null hypothesis simply by chance. To address this problem we used a Holm-Bonferroni correction to \textit{p}-values\footnote{\url{https://stat.ethz.ch/R-manual/R-devel/library/stats/html/p.adjust.html}}. We applied the correction when checking the law of abbreviation in the languages of a collection, so as to exclude the possibility that the law of abbreviation is found many times simply because we are testing it in many languages. 
