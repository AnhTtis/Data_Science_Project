\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}
    

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}
\hypersetup{
  colorlinks,
  linkcolor={red!50!black},
  citecolor={blue!50!black},
%   urlcolor={blue!80!black}
  urlcolor={blue!50!black}
}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{shapes,snakes,backgrounds,arrows}

% PERSONAL PACKAGES
\usepackage{upgreek}
\usepackage{pbox}
\usepackage[inline]{enumitem}
\usepackage{algorithm,algorithmic}
% \usepackage[ruled]{algorithm2e}
\usepackage{xspace}
\usepackage{amsthm,amsmath}
\usepackage{graphicx}
% \usepackage{ulem}
\usepackage{aliascnt}
\usepackage{cleveref}
\usepackage{autonum}
\usepackage{wrapfig}
\usepackage{subfig}

% MACROS
\input{def}

\renewcommand{\arraystretch}{1.2} % stretching the row height

\newcommand{\dsbmalgo}{
\STATE{\textbf{Input:} Joint distribution $\Pi_{0,T}^0$, tractable bridge $\Qbb_{|0,T}$, number of outer iterations $N \in \nset$.}
\STATE{Let $\Pi^0 = \Pi_{0,T}^0 \Qbb_{|0,T}$.}
\FOR{$n \in \{0, \dots, N-1\}$}
  \STATE Learn $v_{\phi^\star}$ using \eqref{eq:loss_function_backward} with $\Pi = \Pi^{2n}$. 
  \STATE Let $\Mbb^{2n+1}$ be given by \eqref{eq:approximate_markovian_proj_backward}. 
  \STATE Let $\Pi^{2n+1} = \Mbb^{2n+1}_{0,T} \Qbb_{|0,T}$.
  \STATE Learn $v_{\theta^\star}$ using \eqref{eq:loss_function} with $\Pi = \Pi^{2n+1}$. 
  \STATE Let $\Mbb^{2n+2}$ be given by \eqref{eq:approximate_markovian_proj_forward}. 
  \STATE Let $\Pi^{2n+2} = \Mbb^{2n+2}_{0,T} \Qbb_{|0,T}$.
  \ENDFOR
  \STATE \textbf{Output:} $v_{\theta^\star}$, $v_{\phi^\star}$
}


\title{Diffusion Schr\"odinger Bridge Matching}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
    Yuyang Shi\thanks{Equal contribution.} \\
    University of Oxford
\And
    Valentin De Bortoli\footnotemark[1] \\
    ENS ULM
\And
    Andrew Campbell \\
    University of Oxford
\And
    Arnaud Doucet \\
    University of Oxford
}

\makeatother
\begin{document}

\maketitle

\begin{abstract}
  Solving transport problems, i.e. finding a map transporting one given
  distribution to another, has numerous applications in machine learning. Novel
  mass transport methods motivated by generative modeling have recently been
  proposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models
  (FMMs) implement such a transport through a Stochastic Differential Equation
  (SDE) or an Ordinary Differential Equation (ODE). However, while it is
  desirable in many applications to approximate the deterministic dynamic
  Optimal Transport (OT) map which admits attractive properties, DDMs and FMMs
  are not guaranteed to provide transports close to the OT map. In contrast,
  Schr\"odinger bridges (SBs) compute stochastic dynamic mappings which recover
  entropy-regularized versions of OT. Unfortunately, existing numerical methods approximating SBs either scale poorly with dimension or accumulate errors across iterations. In this work, we introduce Iterative Markovian
  Fitting (IMF), a new methodology for solving SB problems, and Diffusion
  Schr\"odinger Bridge Matching (DSBM), a novel numerical algorithm for
  computing IMF iterates.  DSBM significantly improves over previous SB numerics
  and recovers as special/limiting cases various recent transport methods. We
  demonstrate the performance of DSBM on a variety of problems.
\end{abstract}


\section{Introduction}
Mass transport problems are ubiquitous in machine learning
\citep{peyre2019computational}.
For discrete measures, the Optimal Transport (OT) map can be computed exactly but is 
computationally intensive. In a landmark paper, \cite{cuturi2013sinkhorn} showed that an entropy-regularized version of OT can be computed more efficiently
using the Sinkhorn algorithm \citep{sinkhorn1967diagonal}. This has enabled the use of OT techniques in a variety of applications ranging from biology \citep{bunne2022proximal} to shape correspondence \citep{feydy2017optimal}. 
However, applications involving high-dimensional continuous distributions and/or large datasets remain challenging for these techniques.

One of such data-rich applications is generative modeling, a central transport problem in machine learning which requires designing a deterministic or stochastic mapping transporting a
reference ``noise'' distribution to the data distribution. For example,
Generative Adversarial Networks \citep{goodfellow2014generative} define a static,
deterministic transport map, while Denoising Diffusion Models (DDMs)
\citep{song2020score,ho2020denoising} build a dynamic, stochastic transport map
by simulating a Stochastic Differential Equation (SDE), whose drift is learned
using score matching \citep{hyvarinen2005estimation,vincent2011connection}. 
The excellent performances of DDMs have motivated
recent developments of Bridge Matching and Flow Matching models, which are
dynamic transport maps using SDEs
\citep{song2020denoising,peluchettinon,liu2022rectified,albergo2023stochastic}
or ODEs
\citep{albergo2022building,heitz2023iterative,lipman2022flow,liu2022flow}. 
Compared to DDMs, Bridge and Flow Matching methods
do not rely on a forward ``noising'' diffusion converging
to the reference distribution in infinite time, and are also more generally applicable as
they can approximate transport maps between two general distributions based on their samples. Nonetheless, these transport maps are not necessarily close to the OT
map minimizing the Wasserstein-2 metric,
which is appealing for its many attractive properties
\citep{peyre2019computational,villani2009optimal}.
 
In contrast, the \schro Bridge (SB) problem is a dynamic
version of entropy-regularized OT
\citep{follmer1988random,leonard2014survey}. 
The SB is the finite-time diffusion which admits
as initial and terminal distributions the two distributions of interest and is
the closest in Kullback--Leibler divergence to a reference diffusion. Numerous methods to approximate SBs numerically have been proposed, see e.g. \citep{bernton2019schr,chen2016entropic,finlay2020learning,caluya2021wasserstein,pavon2018data}, but these techniques tend to be restricted to low-dimensional settings. Recently, novel techniques using diffusion-based ideas have been proposed in \citep{debortoli2021diffusion,vargas2021solving,chen2021likelihood} based on Iterative 
Proportional Fitting (IPF)
\citep{fortet1940resolution,kullback1968probability,ruschendorf1993note}, a continuous state-space extension of the Sinkhorn algorithm \citep{essid2019traversing}.  
These approaches have been shown to scale better empirically, 
 but numerical errors tend to accumulate over
iterations \citep{fernandes2021shooting}. 

\begin{figure}
\centering
\begin{minipage}[b]{0.31\textwidth}
    \scalebox{0.7}{
    \begin{tikzpicture}
        \node [draw, rounded corners=25pt, very thick, olive, minimum width=6cm, minimum height=3.6cm, align=center] at (1,0.3){};
        
        \node [olive, align=center] at (-1.1,1.6){\textbf{DSBM}};
        
        \node [draw, ellipse, very thick, purple, minimum width=5cm, minimum height=2cm, align=center] at (1,0.2){Bridge Matching\\\\\\\\};
        \node [draw, ellipse, very thick, blue, minimum width=2cm, minimum height=1cm, align=center] at   (0,0)   {Denoising \\Diffusion};
        \node [draw, ellipse, very thick, red, minimum width=2cm, minimum height=1cm, align=center] at    (2,0)   {Flow \\ Matching};
    \end{tikzpicture}
    }
    \caption{Relationship between DSBM and existing methods. }
    \label{fig:venn_diagram}
\end{minipage}
\hfill
\begin{minipage}[b]{0.67\textwidth} 
  \begin{tabular}[c]{c||c|c}
    & Sets for alternating projections & Preserved properties \\
    \hline \hline 
    IPF & $\Pbb_0 = \pi_0$; $\Pbb_T = \pi_T$  & $\calM$, $\calR(\Qbb)$   \\
    \hline
    IMF &$\calM$; $\calR(\Qbb)$ & $\Pbb_0 = \pi_0$, $\Pbb_T = \pi_T$  \\ 
    \hline 
  \end{tabular}
  
  \captionof{table}{Comparison between Iterative Markovian Fitting (IMF) and
    Iterative Proportional Fitting (IPF). The \schro Bridge is the \emph{unique} $\Pbb$
    s.t. $\Pbb_0 = \pi_0$, $\Pbb_T = \pi_T$, $\Pbb\in\calM$,
    $\Pbb\in\calR(\Qbb)$ simultaneously by
    \Cref{prop:schro-markov-recip}. $\calM$ is the space of (regular)
    Markov measures and $\calR(\Qbb)$ the
    space of reciprocal measures of $\Qbb$.
    }
  \label{tab:diff}
\end{minipage}
\end{figure}


In this paper, our contributions are three-fold. First, we introduce Iterative
Markovian Fitting (IMF), a new procedure to compute SBs which alternates between
projecting on the space of \emph{Markov processes} and on the \emph{reciprocal class}, i.e. the measures which
have the same bridge as the reference measure of SB  \citep{leonard2014reciprocal}. 
We establish various theoretical results for IMF. Contrary to IPF, the IMF iterates always preserve the initial and terminal distributions. The differences between IPF and IMF are presented in \Cref{tab:diff}. 
Second, we propose Diffusion Schr\"odinger Bridge Matching (DSBM), a novel
algorithm approximating numerically 
the SB solution derived from IMF. 
DSBM requires at
each iteration solving a simple regression problem in the spirit of
Bridge and Flow Matching,
and does not suffer from the time-discretization and 
``forgetting'' issues of previous DSB techniques
\citep{debortoli2021diffusion,vargas2021solving,chen2021likelihood}. 
Finally, we demonstrate the performance of DSBM on a variety of transport tasks.\footnote{Code can be found at \url{https://github.com/yuyang-shi/dsbm-pytorch}.}
 
\textbf{Notations.} We denote by $\pathmeas$, the space of \emph{path
measures}, i.e. $\pathmeas=\mathcal{P}(\rmC(\ccint{0,T}, \rset^d))$ where
$T > 0$. 
The subset of \emph{Markov} path measures associated
with an SDE of the form
$\rmd \bfX_t = v_t(\bfX_t) \rmd t + \sigma_t \rmd \bfB_t$, with $\sigma,v$
locally Lipschitz, is denoted $\calM$.  For any
$\Qbb \in \calM$, the \emph{reciprocal class}  of $\Qbb$
is denoted $\calR(\Qbb)$, see
\Cref{def:reciprocal_projection}. We also denote $\Qbb_t$ its marginal
distribution at time $t$, $\Qbb_{s,t}$ the joint distribution at times $s$ and
$t$, $\Qbb_{s|t}$ the conditional distribution at time $s$ given state at
time $t$, and $\Qbb_{|0,T} \in \pathmeas$ its \emph{diffusion bridge}. Unless specified otherwise, all gradient operators $\nabla$ are
w.r.t. the variable $x_t$ with time index $t$. Let $(\msx, \mathcal{X})$ and $(\mathsf{Y}, \mathcal{Y})$ be probability spaces. Given a Markov kernel $\mathrm{K}: \ \msx \times \mathcal{Y} \to [0,1]$ and a probability measure $\mu$ defined on $\mathcal{X}$, we write $\mu \mathrm{K}$ the probability measure on $\mathcal{Y}$ such that for any $\mathsf{A} \in \mathcal{Y}$ we have $\mu \mathrm{K}(\mathsf{A}) = \int_{\msx} \mathrm{K}(x, \mathsf{A}) \rmd \mu(x)$. 
In particular, for any joint distribution $\Pi_{0,T}$ over $\rset^d \times \rset^d$, we denote the \emph{mixture of bridges} measure as $\Pi = \Pi_{0,T} \Qbb_{|0,T} \in \pathmeas$, which is short for $\Pi(\cdot) = \int_{\rset^d \times \rset^d} \Qbb_{|0,T}(\cdot|x_0, x_T) \Pi_{0,T}(\rmd x_0, \rmd x_T)$. 

\section{Dynamic Mass Transport Techniques}

\subsection{Denoising Diffusion and Bridge Matching Models}

Denoising Diffusion Models \citep{song2020score,ho2020denoising} are a popular
class of generative models. They define a forward noising process $\Qbb \in \calM$
using the SDE
$\rmd \bfX_t = - \tfrac{1}{2} \bfX_t \rmd t + \rmd \bfB_t$ on the time-interval $[0,T]$,
where $\bfX_0 \in \rset^d$ is drawn from the data distribution $\pi_0$
and $(\bfB_t)_{t\in[0,T]}$ is a $d$-dimensional Brownian motion. This
diffusion\footnote{This is known as the Ornstein-Uhlenbeck (OU) process or VPSDE \citep{song2020score}. } 
converges towards the standard Gaussian distribution
$\mathrm{N}(0, \Id)$ as $T\to\infty$. A generative model is given by its
\textit{time-reversal} $(\bfY_t)_{t\in[0,T]}=(\bfX_{T-t})_{t\in[0,T]}$, where
$\bfY_0 \sim \Qbb_T$ and
$\rmd \bfY_t = \{\tfrac{1}{2} \bfY_t + \nabla \log \Qbb_{T-t}(\bfY_t)\} \rmd t +
\rmd \bfB_t$
\citep{anderson1982reverse,haussmann1986time}.  In practice, $(\bfY_t)_{t \in \ccint{0,T}}$
is initialized with $\bfY_0 \sim \pi_T = \mathrm{N}(0, \Id)$, and the \emph{Stein
  score}
$\nabla \log \Qbb_t(x_t)=\CPELigne{\Qbb_{0|t}}{\nabla \log
  \Qbb_{t|0}(\bfX_{t}|\bfX_0)}{\bfX_{t}=x_t}$ is approximated using a neural
network $s_\theta(t,x_t)$
minimizing the 
\emph{denoising score matching} loss
$\expeLigne{\Qbb_{0,t}}{\normLigne{ \nabla \log \Qbb_{t|0}(\bfX_{t}|\bfX_0) -
    s_\theta(t,\bfX_t)}^2}$.  

An alternative to considering the time-reversal of a forward noising process is to
``build bridges'' between the two distributions and learn a \emph{mimicking} diffusion process. This approach generalizes DDMs and allows for more
flexible choices of sampling processes. We call this framework \emph{Bridge Matching} and adopt a presentation similar to 
\cite{peluchettinon,liu2022let}, where $\pi_T$ is the data distribution.\footnote{To keep notations consistent with existing works, $\pi_0$ is the data distribution in the context of DDM and SB, whereas $\pi_T$ is the data distribution in Bridge Matching. However, both SB and Bridge Matching methods allow transfer between arbitrary distributions $\pi_0,\pi_T$, so this distinction is not important.}
We denote $\Qbb \in \calM$ the path measure associated with the following process
\begin{equation}
   \rmd \bfX_t = f_t(\bfX_t) \rmd t + \sigma_t \rmd \bfB_t, \qquad \bfX_0 \sim \Qbb_0.
   \label{eq:reference_measure}
\end{equation}
Consider now the distribution of this process pinned down at an initial and terminal
point $x_0,x_T$, denoted $\Qbb_{|0,T}(\cdot|x_0,x_T)$. Under mild assumptions,
the \emph{pinned} process $\Qbb_{|0,T}(\cdot|x_0,x_T)$ is a \emph{diffusion bridge} and is given by 
\begin{equation}
  \rmd \bfX_t^{0,T} = \{f_t(\bfX_t^{0,T}) + \sigma_t^2 \nabla \log \Qbb_{T|t}(x_T|\bfX_t^{0,T})\} \rmd t + \sigma_t \rmd \bfB_t, \qquad \bfX_0^{0,T} = x_0,
  \label{eq:bridge_forward}
\end{equation}
which satisfies $\bfX_T^{0,T}=x_T$ 
using Doob $h$-transform theory
\citep{rogers2000diffusions}. Next, we define an independent coupling
$\Pi_{0,T}=\pi_0 \otimes \pi_T$, and let
$\Pi = \Pi_{0,T} \Qbb_{|0,T}$. This path measure $\Pi$ is a
\textit{mixture of bridges}.  
We aim to find a Markov diffusion 
$\rmd \bfY_t = \{f_t(\bfY_t) + v_t(\bfY_t)\}\rmd t + \sigma_t \rmd \bfB_t$ on $[0,T]$ 
which admits the same marginals as $\Pi$; i.e. for any
$t \in \ccint{0,T}$, $\bfY_t \sim \Pi_{t}$, so $\bfY_T \sim \pi_T$.  For such $v_t$,
a
generative model for sampling data distribution $\pi_T$ is obtained by simulating $(\bfY_t)_{t\in [0,T]}$. 
It can be verified that indeed $\bfY_t \sim \Pi_{t}$ for $v^\star_t(x_t) = \sigma_t^2 \CPELigne{\Pi_{T|t}}{ \nabla \log \Qbb_{T|t}(\bfX_{T}|\bfX_t)}{\bfX_{t}=x_t}$.
We present the theory behind this idea more formally using Markovian projections in \Cref{subsec:markovian_projection}. 
In practice, we do not have access to
$v^\star_t$ and it is learned using neural networks with regression loss
\begin{equation}
    \label{eq:bridge_matching_loss}
    \expeLigne{\Pi_{t,T}}{\normLigne{\sigma_t^2 \nabla \log \Qbb_{T|t}(\bfX_{T}|\bfX_t) - v_\theta(t,\bfX_t)}^2}.
\end{equation}
For $f_t = 0$ and $\sigma_t=\sigma$, $\Qbb_{|0,T}$ is a \emph{Brownian Bridge} and we have
\begin{equation}
  \label{eq:interpolation_stochastic}
  \bfX_t^{0,T} = \tfrac{t}{T} x_T + (1- \tfrac{t}{T}) x_0 + \sigma_t (\bfB_t - \tfrac{t}{T}\bfB_T) , \quad \rmd \bfX_t^{0,T} = \{(x_T - \bfX_t^{0,T})/(T-t)\} \rmd t + \sigma_t \rmd \bfB_t,
\end{equation}
with $(\bfB_t - \tfrac{t}{T}\bfB_T) \sim \mathrm{N}(0, t(1 - \tfrac{t}{T})~\Id)$. The regression loss \eqref{eq:bridge_matching_loss} associated with \eqref{eq:interpolation_stochastic} is given by 
\begin{equation}
    \label{eq:bridge_matching_loss_brownian}
    \expeLigne{\Pi_{t,T}}{\normLigne{(\bfX_T - \bfX_t)/(T-t) - v_\theta(t,\bfX_t)}^2}.
\end{equation}
Letting $\sigma \to 0$, we recover Flow Matching models (see \Cref{subsec:fm_relationship_appendix} for further details).


\subsection{Schr\"odinger Bridges and Optimal Transport}
The Schr\"odinger Bridge (SB) problem \citep{schrodinger1932theorie} consists in finding a path measure $\PSB \in \pathmeas$
such that
\begin{equation}
  \PSB = \argmin_{\Pbb} \ensembleLigne{\KLLigne{\Pbb}{\Qbb}}{\Pbb_0 = \pi_0, \ \Pbb_T = \pi_T} ,
  \label{eq:schrodinger_bridge}  
\end{equation}
where $\Qbb \in \pathmeas$ is a reference path measure.  In what follows, we
consider $\Qbb$ defined by the diffusion process \eqref{eq:reference_measure}
which is Markov, and without loss of generality, we assume $\Qbb_0=\pi_0$.
Hence $\PSB$ is the path measure closest to $\Qbb$ in terms of Kullback--Leibler
divergence which satisfies the initial and terminal constraints $\PSB_0 = \pi_0$
and $\PSB_T = \pi_T$.  

Another crucial property of $\PSB$ is that it can also be defined as a mixture of bridges
$\PSB = \PiSB_{0,T} \Qbb_{|0,T}$, where
$\PiSB_{0,T} = \argmin_{\Pi_{0,T}}
\ensembleLigne{\KLLigne{\Pi_{0,T}}{\Qbb_{0,T}}}{\Pi_0 = \pi_0, \ \Pi_T = \pi_T}$
is the solution of the \emph{static} SB problem \citep{leonard2014survey}. In
particular, for $\Qbb$ associated with $(\sigma \bfB_t)_{t \in \ccint{0,T}}$ we have 
\begin{equation}
\PiSB_{0,T} = \argmin_{\Pi_{0,T}} \ensembleLigne{\mathbb{E}_{\Pi_{0,T}}[||\bfX_0-\bfX_T||^2-2\sigma^2 T~\textup{H}(\Pi_{0,T})}{\Pi_0 = \pi_0, \ \Pi_T = \pi_T},
\end{equation}
where $\textup{H}(\mu)$
denotes the
entropy, i.e. $\PiSB_{0,T}$ is the solution of the entropy-regularized OT problem. 
In this case, the SB can also be obtained theoretically by solving the following problem \citep{dai1991stochastic}
\begin{equation}\label{eq:BenamouBrenierSBOT}
  \textstyle
v_{\textup{SB}}=\argmin_{v} \ensembleLigne{\int^T_0 \mathbb{E}_{\P_t}[||v(t,\bfX_t)||^2]\rmd t}{\rmd \bfX_t=v(t,\bfX_t)\rmd t+ \sigma\rmd \bfB_t, \,~\P_0=\pi_0,~ \P_T=\pi_T}.
\end{equation}
Then $\PSB$ is given by the SDE with drift $v_{\textup{SB}}$ initialized with $\bfX_0 \sim \pi_0$. 
For $\sigma=0$, we recover the classical OT problem and the Benamou-Brenier formula \citep{benamou2000computational}. 

A common approach to solve \eqref{eq:schrodinger_bridge} is the
Iterative Proportional Fitting (IPF) method 
\citep{fortet1940resolution,kullback1968probability,ruschendorf1995convergence} defining a sequence of path measures $(\tilde{\Pbb}^{n})_{n \in \nset}$ where
\begin{equation}
  \label{eq:sinkhorn}
  \hspace{0.1cm} \tilde{\Pbb}^{2n+1} = \argmin_{\tilde{\Pbb}} \ensembleLigne{\KLLigne{\tilde{\Pbb}}{\tilde{\Pbb}^{2n}}}{\tilde{\Pbb}_T = \pi_T} , \ \tilde{\Pbb}^{2n+2} = \argmin_{\tilde{\Pbb}} \ensembleLigne{\KLLigne{\tilde{\Pbb}}{\tilde{\Pbb}^{2n+1}}}{\tilde{\Pbb}_0 = \pi_0} , \hspace{-0.2cm}
\end{equation}
with initialization $\tilde{\Pbb}^0 = \Qbb$. This procedure alternates between
projections on the set of path measures with given initial distribution $\pi_0$ and terminal distribution $\pi_T$. It can be shown \citep{debortoli2021diffusion}
that $(\tilde{\Pbb}^n)_{n \in \nset}$ are associated with diffusions and
that for any $n \in \nset$, $\tilde{\Pbb}^{2n+1}$ is the time-reversal of
$\tilde{\Pbb}^{2n}$ with initialization $\pi_T$, and $\tilde{\Pbb}^{2n+2}$ is the time-reversal
of $\tilde{\Pbb}^{2n+1}$ with initialization $\pi_0$. 
Leveraging this property, \cite{debortoli2021diffusion} proposed Diffusion
Schr\"odinger Bridge (DSB), an algorithm which learns the time-reversals
iteratively.  In particular, DDMs can be seen as the first iteration of DSB.

\section{Iterative Markovian Fitting}
\label{sec:iterative-markovian-fitting}

\subsection{Markovian Projection and Reciprocal Projection}
\label{subsec:markovian_projection}

\textbf{Markovian Projection.} Projecting on Markov measures is a key ingredient in our methodology and
in the Bridge Matching framework. This concept was introduced multiple times in
the literature \citep{gyongy1986mimicking,peluchettinon,liu2022let}. 
In particular, we focus on Markovian projection of path measures given by a mixture of bridges
$\Pi = \Pi_{0,T} \Qbb_{|0,T} \in \pathmeas$.

\begin{definition}
  \label{def:markovian_proj}
  Assume that $\Qbb$ is given by \eqref{eq:reference_measure} and that for any
  $(x_0,x_T) \in \rset^d$, $\Qbb_{|0,T}(\cdot|x_0,x_T)$ is associated with
  $(\bfX_t^{0,T})_{t \in \ccint{0,T}}$ given by
  $\rmd \bfX_t^{0,T} = \{f_t(\bfX_t^{0,T}) + \sigma_t^2 \nabla \log
  \Qbb_{T|t}(x_T|\bfX_t^{0,T})\} \rmd t + \sigma_t \rmd \bfB_t$, with
  $\sigma: \ \ccint{0,T} \to \ooint{0,+\infty}$. Then, when it is well-defined,
  we introduce the \emph{Markovian projection} of $\Pi$,
  $\Mbb^\star = \projM(\Pi) \in \calM$, which is associated with the SDE
  \begin{equation}
    \rmd \bfX^\star_t = \{f_t(\bfX^\star_t) + v_t^\star(\bfX^\star_t)\}\rmd t + \sigma_t \rmd \bfB_t, \qquad v_t^\star(x_t) =  \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t=x_t}.
  \end{equation}  
\end{definition}


Note that in our definition $\sigma_t>0$ so $\nabla \log \Qbb_{T|t}(x_T|x_t)$ is
well-defined, but Flow Matching can be recovered as the \emph{deterministic}
case in the limit $\sigma_t=\sigma \to 0$. 
In the following proposition, we show that the Markovian projection is indeed a
projection for the \textit{reverse} Kullback--Leibler divergence, and that it
preserves marginals of $\Pi_t$. 

\begin{proposition}
  \label{prop:markovian-projection}
  Assume that $\sigma_t > 0$. Let $\Mbb^\star = \projM(\Pi)$. Then, under mild assumptions, we have
  \begin{align}    
    &\textstyle \Mbb^\star = \argmin_{\Mbb} \ensembleLigne{\KLLigne{\Pi}{\Mbb}}{\Mbb \in \calM} , \\
    & \textstyle \KLLigne{\Pi}{\Mbb^\star} = \tfrac{1}{2} \int_0^T \expeLigne{\Pi_{0,t}}{\normLigne{\sigma_t^2 \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t} - v_t^\star(\bfX_t)}^2}/\sigma_t^2 \rmd t  .
  \end{align}
  In addition, we have that for any $t \in \ccint{0,T}$, $\Mbb^\star_t = \Pi_t$. In particular, $\Mbb^\star_T = \Pi_T$.
\end{proposition}

\textbf{Reciprocal Projection.} While the Markovian projection
ensures that the obtained  measure is Markov, the associated \emph{bridge} measure is
not preserved in general, i.e.
$\projM(\Pi)_{|0,T} \neq \Pi_{|0,T} = \Qbb_{|0,T}$. 
Measures with same bridge as $\Qbb$ are said to be in its \emph{reciprocal class}
\citep{leonard2014reciprocal}. 

\begin{definition}
  $\Pi \in \pathmeas$ is in the reciprocal class $\calR(\Qbb)$ of $\Qbb \in \calM$ if
  $\Pi = \Pi_{0,T} \Qbb_{|0,T}$. 
  We define the
  \emph{reciprocal projection} of $\Pbb \in \pathmeas$ as
  $\Pi^\star = \projR{\Qbb}(\Pbb) =  \Pbb_{0,T} \Qbb_{|0,T} $.
  \label{def:reciprocal_projection}
\end{definition}

Similarly to \Cref{prop:markovian-projection}, we have the following
result, which justifies the term reciprocal projection.

\begin{proposition}
  \label{prop:reciprocal-projection}
  Let $\Pbb \in \pathmeas$, $\Pi^\star = \projR{\Qbb}(\Pbb)$. Then, $\Pi^\star = \argmin_{\Pi} \ensembleLigne{\KLLigne{\Pbb}{\Pi}}{ \Pi \in \calR(\Qbb)}$.
\end{proposition}

The reciprocal projection $\Pi^\star$ of a Markov path measure $\Mbb$
does not preserve the Markov property in general. In fact, the Schr\"odinger
Bridge is the \emph{unique} path measure which satisfies the initial and
terminal conditions, is Markov and is in the reciprocal class of $\Qbb$, see
\citep{leonard2014survey}. 

\begin{proposition}
  \label{prop:schro-markov-recip}
  Let $\Pbb$ be a Markov measure in the reciprocal class of $\Qbb$ such
  that $\Pbb_0 = \pi_0$, $\Pbb_T = \pi_T$. Then, under 
  assumptions on $\Qbb$, $\pi_0$ and $\pi_T$, $\Pbb$ is unique and is equal to the Schr\"odinger Bridge $\PSB$.
\end{proposition}



\subsection{Iterative Markovian Fitting}
\label{sec:iter-mark-fitt}
Based on \Cref{prop:schro-markov-recip}, we propose a novel methodology called
\textit{Iterative Markovian Fitting} (IMF) to solve Schr\"odinger Bridges.  We
consider a sequence $(\Pbb^n)_{n \in \nset}$ such that
\begin{equation}  
  \label{eq:imp}
  \Pbb^{2n+1} = \projM(\Pbb^{2n}) , \qquad \Pbb^{2n+2} = \projR{\Qbb}(\Pbb^{2n+1}) ,
\end{equation}
with $\Pbb^0$ such that $\Pbb^0_0 = \pi_0$, $\Pbb^0_T=\pi_T$ and
$\Pbb^0 \in \calR(\Qbb)$.  These updates correspond to alternatively performing
Markovian projections and reciprocal projections. 

Combining \Cref{prop:markovian-projection} and
\Cref{def:reciprocal_projection}, we get that for any $n \in \nset$,
$\Pbb^n_0 = \pi_0$ and $\Pbb^n_T = \pi_T$.
This property is in contrast to the IPF
algorithm \eqref{eq:sinkhorn} for which the marginals at the initial and final times are
\textit{not} preserved.  
We highlight this duality between IPF \eqref{eq:sinkhorn} and IMF \eqref{eq:imp} in \Cref{tab:diff}.

We conclude this section with a theoretical analysis of IMF.  First, we
start by showing a Pythagorean theorem for both the Markovian projection and the
reciprocal projection. 

\begin{lemma}
  \label{lemma:pythagorean_theorem}
  Under mild assumptions, 
  if $\Mbb \in \calM$, $\Pi \in \calR(\Qbb)$ and $\KLLigne{\Pi}{\Mbb} < +\infty$, we have
  \begin{equation}
    \KLLigne{\Pi}{\Mbb} = \KLLigne{\Pi}{\projM(\Pi)} +  \KLLigne{\projM(\Pi)}{\Mbb} . 
  \end{equation}
  If $\KLLigne{\Mbb}{\Pi} < +\infty$, we have 
  \begin{equation}
    \KLLigne{\Mbb}{\Pi} = \KLLigne{\Mbb}{\projR{\Qbb}(\Mbb)} +  \KLLigne{\projR{\Qbb}(\Mbb)}{\Pi} . 
  \end{equation}  
\end{lemma}


Using \Cref{lemma:pythagorean_theorem}, we have the following proposition.

\begin{proposition}
  \label{prop:convergence_marginals}
  Under mild assumptions, we have
  $\lim_{n \to +\infty} \KLLigne{\Pbb^n}{\Pbb^{n+1}} = 0$. 
  If $\Pstar$ is a fixed point of $(\Pbb^n)_{n \in \nset}$, then $\Pstar$
  is the Schr\"odinger Bridge $\PSB$.
\end{proposition}

Hence, for the IMF
sequence $(\Pbb^n)_{n \in \nset}$, the Markov path measures $(\Pbb^{2n+1})_{n \in \nset}$ are getting closer
to the reciprocal class, while the reciprocal path measures
$(\Pbb^{2n+2})_{n \in \nset}$ are getting closer to the set of Markov measures.
\Cref{prop:convergence_marginals} should be compared with \cite[Proposition
2.1, Equation (2.16)]{ruschendorf1995convergence} which shows that, for the IPF
sequence $(\tilde{\Pbb}^n)_{n \in \nset}$, we have
$\lim_{n \to +\infty} \KLLigne{\tilde{\Pbb}^{n+1}}{\tilde{\Pbb}^n} = 0$. This result is similar 
to \Cref{prop:convergence_marginals} but for the \emph{forward}
Kullback--Leibler divergence. 

\section{Diffusion \schro Bridge Matching}
\label{sec:pract-meth}
In this section, we present  Diffusion \schro Bridge Matching (DSBM), a practical
algorithm for solving the SB problem obtained by combining the IMF procedure
with Bridge Matching. 

\textbf{Iterative Markovian Fitting in practice. }
IMF alternatively projects on the Markov class $\calM$ and
the reciprocal class $\calR(\Qbb)$. We denote $\Mbb^{n+1} = \Pbb^{2n+1} \in \calM$ and $\Pi^n = \Pbb^{2n} \in \calR(\Qbb)$. Assuming we know how to sample from the
bridge $\Qbb_{|0,T}$ given the initial and terminal conditions, sampling from
the reciprocal projection $\projR{\Qbb}(\Mbb)$ is simple: First, sample $(\bfX_0,\bfX_T)$ from the joint distribution
$\Mbb_{0,T}$.\footnote{In practice, we sample the SDE associated with $\Mbb$ and
  save a batch of joint samples $(\bfX_0, \bfX_T)$. This is similar to the
  \emph{trajectory caching} procedure in \cite{debortoli2021diffusion}, but we
  only retain initial and final samples.} Then, sample from the bridge
$\Qbb_{|0,T}(\cdot|\bfX_0,\bfX_T)$.
The bottleneck of IMF is in the computation of Markovian projections. By
\Cref{def:markovian_proj}, $\Mbb^\star=\projM(\Pi)$ is associated with the process
\begin{equation}
  \rmd \bfX_t = \{ f_t(\bfX_t) + \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t}\} \rmd t + \sigma_t \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 . 
\end{equation}
By Proposition \ref{prop:markovian-projection}, we can learn
$\Mbb^\star$ using $\Mbb^{\theta^\star}$ given by 
\begin{align}
&  \label{eq:approximate_markovian_proj_forward}
  \rmd \bfX_t = \{ f_t(\bfX_t) +  v_{\theta^\star}(t, \bfX_t) \} \rmd t + \sigma_t \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 , \\
  \label{eq:loss_function}
 & \textstyle \theta^\star = \argmin_\theta \ensembleLigne{\int_0^T \expeLigne{\Pi_{t,T}}{\normLigne{\sigma_t^2 \nabla
    \log \Qbb_{T|t}(\bfX_T|\bfX_t) - v_\theta(t,\bfX_t)}^2}/\sigma_t^2 \rmd t }{\theta \in \Theta},
\end{align}
where $\ensembleLigne{v_\theta}{\theta \in \Theta}$ is a parametric family of
functions, usually given by a neural network. The optimal $v_{\theta^\star}(t, x_t) = \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log
    \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t = x_t}$ 
for any
$t \in \ccint{0,T}$ and $x_t \in \rset^d$. 

With the above two procedures for computing $\projR{\Qbb}(\Mbb)$ and $\projM(\Pi)$, we can
now describe a numerical method implementing IMF \eqref{eq:imp}. Let $\Pi^0=   \Pi_{0,T}^0 \Qbb_{|0,T}$ where
$\Pi^0_0 = \pi_0$, $\Pi^0_T = \pi_T$. 
Learn $\Mbb^1 \approx \projM(\Pi^0)$ given by
\eqref{eq:approximate_markovian_proj_forward} with $v_{\theta^\star}$ given by 
\eqref{eq:loss_function}.  Next, sample from $\Pi^1 = \projR{\Qbb}(\Mbb^1)= \Mbb^1_{0,T} \Qbb_{|0,T}$ by
sampling from $\Mbb^1_{0,T}$ and reconstructing the bridge $\Qbb_{|0,T}$. 
We iterate the process to obtain a sequence $(\Pi^n,\Mbb^{n+1})_{n \in \nset}$.
In practice, this algorithm performs poorly (see \Cref{fig:gaussian_ipf_convergence}), since the approximate
minimization \eqref{eq:loss_function} for computing $\Mbb^{n+1}$
may not admit $\Mbb^{n+1}_T = \pi_T$ exactly as in \Cref{prop:markovian-projection}. 
Instead, we incur a bias between $\Mbb^{n+1}_T$ and $\pi_T$ which accumulates for each $n \in \nset$.

To mitigate this problem, we alternate between a \emph{forward} Markovian
projection and a \emph{backward} Markovian projection. This procedure is
justified by the following proposition.
\begin{proposition}
  \label{prop:forward_backward_markov_proj}
  Assume that $\Pi=\Pi_{0,T} \Qbb_{|0,T}$ with $\Qbb$ associated with
  $\rmd \bfX_t = f_t(\bfX_t) \rmd t + \sigma_t \rmd \bfB_t$. Under mild
  conditions, the Markovian projection $\Mbb^\star = \projM(\Pi)$ is associated with both 
  \begin{align}
    &\label{eq:forward_markov_proj}
    \hspace{-0.5cm} \rmd \bfX_t = \{f_t(\bfX_t) + \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t} \} \rmd t + \sigma_t \rmd \bfB_t , \quad \bfX_0 \sim \Pi_0 , \\
    & \label{eq:backward_markov_proj}
    \hspace{-0.5cm} \rmd \bfY_t = \{-f_{T-t}(\bfY_t) + \sigma_{T-t}^2 \CPELigne{\Pi_{0|T-t}}{\nabla \log \Qbb_{T-t|0}(\bfY_t|\bfY_T)}{\bfY_t} \} \rmd t + \sigma_{T-t} \rmd \bfB_t, \bfY_0 \sim \Pi_T.\hspace{-0.5cm} 
  \end{align}
  \vspace{-0.5cm}
\end{proposition}
In \Cref{prop:forward_backward_markov_proj}, \eqref{eq:forward_markov_proj} is the definition of the Markovian
projection, see \Cref{def:markovian_proj}. However,
\eqref{eq:backward_markov_proj} is an equivalent representation as a \emph{time-reversal}. 
In practice, $(\bfY_t)_{t \in \ccint{0,T}}$ is approximated with
\begin{align}
&  \label{eq:approximate_markovian_proj_backward}
  \rmd \bfY_t = \{ -f_{T-t}(\bfY_t) + v_{\phi^\star}(T-t, \bfY_t) \} \rmd t + \sigma_{T-t} \rmd \bfB_t , \qquad \bfY_0 \sim \pi_T , \\ 
  \label{eq:loss_function_backward}
 & \textstyle \phi^\star = \argmin_\phi \ensembleLigne{\int_0^T \expeLigne{\Pi_{0,t}}{\normLigne{\sigma_t^2 \nabla
    \log \Qbb_{t|0}(\bfX_t|\bfX_0) - v_\phi(t,\bfX_t)}^2} /\sigma_t^2 \rmd t }{\phi \in \Phi} .
\end{align}
The optimal 
  $v_{\phi^\star}(t, x_t) = \sigma_t^2 \CPELigne{\Pi_{0|t}}{\nabla \log
    \Qbb_{t|0}(\bfX_t|\bfX_0)}{\bfX_t = x_t}$ for any
$t \in \ccint{0,T}$ and $x_t \in \rset^d$.


\begin{wrapfigure}{l}{0.6\textwidth}
\begin{minipage}{0.6\textwidth}
\vspace{-0.8cm}
\begin{algorithm}[H]
\caption{Diffusion Schr\"odinger Bridge Matching}
\label{alg:DSBM_general}
\begin{algorithmic}[1]
\dsbmalgo
\end{algorithmic}
\end{algorithm}
\vspace{-1cm}
\end{minipage}
\end{wrapfigure}


Note that $\bfX_0 \sim \pi_0$ in the forward projection, while $\bfY_0 \sim \pi_T$ in the backward projection. Therefore, using the backward projection removes the bias on $\pi_T$
accumulated from the forward projection. 
Leveraging the time-symmetry of the Markovian projection 
 and alternating between \eqref{eq:approximate_markovian_proj_backward} and
\eqref{eq:approximate_markovian_proj_forward} yields the DSBM methodology summarized in \Cref{alg:DSBM_general}. 

It is also possible to learn
\emph{both} the forward and backward processes at each step, 
and enforce that 
the backward and forward processes match. We explore this in \Cref{sec:joint-train-forw}. 


\textbf{Initialization coupling.} 
We now relate \Cref{alg:DSBM_general} to the classical IPF and practical algorithms such as DSB
\citep{debortoli2021diffusion}. 
Instead of initializing DSBM with $\Pi^0_{0,T}$ given by a coupling between $\pi_0, \pi_T$, 
if we initialize it by $\Pi^0_{0,T}=\Q_{0,T}$
where $\Q_0=\pi_0$ and $\Q_{T|0}$ is given by the reference process
defined in \eqref{eq:reference_measure}, 
then DSBM also recovers the IPF iterates used in DSB.

 \begin{proposition}
    \label{prop:dsb_dsbm}
   Suppose the families of functions $\ensembleLigne{v_\theta}{\theta \in \Theta}$ and
  $\ensembleLigne{v_\phi}{\phi \in \Phi}$ are rich enough so that they can model the optimal vector fields. 
    Let $(\Pi^n, \Mbb^{n+1})_{n \in \nset}$ be the
    optimal DSBM sequence in \Cref{alg:DSBM_general} initialized with  $\Pi^0_{0,T}=\Q_{0,T}$, and let
    $(\tilde{\Pbb}^n)_{n \in \nset}$ be the optimal DSB sequence given by the
    IPF iterates in \eqref{eq:sinkhorn}. Then for any $n \in \nset, n \geq 1$, we have
    $\Mbb^n = \tilde{\Pbb}^n$.
 \end{proposition}

We will thus call DSBM-IPF, the DSBM algorithm initialized with the joint distribution given by the forward reference process $\Pi_{0,T}^0=\Q_{0,T}$; and DSBM-IMF,
the DSBM algorithm initialized with an independent coupling
$\Pi_{0,T}^0 = \pi_0 \otimes \pi_T$. 
However, the training procedure of DSBM-IPF is very different from the one of 
\citep{debortoli2021diffusion,chen2021likelihood}. In existing works, $\tilde{\Pbb}^{n+1}$ is obtained as the time-reversal of
$\tilde{\Pbb}^{n}$
which requires full trajectories from $\tilde{\Pbb}^{n}$, see e.g. \cite[Proposition 6]{debortoli2021diffusion}.
In contrast, in \Cref{alg:DSBM_general} we only use the \emph{coupling} $\Mbb^{n}_{0,T}$ to create the bridge measure
$\Pi^{n} = \Mbb^{n}_{0,T} \Qbb_{|0,T} $. By doing so,
\begin{enumerate*}[label=(\roman*)]   
\item the losses \eqref{eq:loss_function} and
\eqref{eq:loss_function_backward} can be easily evaluated at any time $t \in \ccint{0,T}$;
\item the \emph{trajectory caching} procedure in DSBM is more computationally and memory efficient; 
\item while every IPF iteration $\tilde{\Pbb}^n$ is also supposed to be in $\calR(\Qbb)$, in practice one can observe a
\emph{forgetting} of the bridge $\Qbb_{|0,T}$
\citep{fernandes2021shooting}. In DSBM, this effect is countered by explicit projections on the reciprocal class.
\end{enumerate*}
See \Cref{sec:dsbm_vs_dsb_appendix} for more details. 

\section{Related Work}
\label{sec:related_work}
\textbf{Markovian projection and Bridge Matching.} The concept of Markovian projection has been
rediscovered multiple times \citep{krylov1984once, gyongy1986mimicking, dupire1994pricing}. 
In the machine learning context, this was first proposed by
\cite{peluchettinon} to define Bridge Matching models. 
More recently,
\citet{liu2022let} derived theoretical properties of the Markovian projection in \Cref{prop:markovian-projection}, first part of \Cref{lemma:pythagorean_theorem}, 
and applied Bridge Matching for learning data on discrete and constrained domains. 

\textbf{Bridge and Flow Matching.}
Flow Matching corresponds to deterministic bridges with deterministic samplers (ODEs) and has been under active study 
\citep{liu2022flow,liu2022rectified,lipman2022flow,albergo2022building,heitz2023iterative,pooladian2023multisample,tong2023conditional}.
Denoising Diffusion Implicit Models (DDIM) \citep{song2020denoising} can also be
formulated as a discrete-time version of Flow Matching, see \cite{liu2022flow}.
These models have been extended to the Riemannian setting by
\cite{chen2023riemannian}. 
Recently, \cite{albergo2023stochastic} studied the influence
of stochasticity in the bridge, through the concept of stochastic interpolants.
\cite{liu2023I2SB,delbracio2023inversion} used Bridge Matching to perform
image restoration tasks and noted benefits of stochasticity empirically. 
Closely related to our work
is the Rectified Flow algorithm of \cite{liu2022flow}, which corresponds to an
iterative Flow Matching procedure in order to improve the straightness
of the flow and thus eases its simulation. DSBM-IMF is closest to
Rectified Flow, which can be seen as the deterministic limiting case as
$\sigma\to0$.  However, there are a few important 
differences. Most notably, we adopt the SDE approach 
which is crucial for the validity of \Cref{prop:schro-markov-recip} as well as for the empirical performance of DSBM. We discuss further distinctions between DSBM and Rectified Flow in \Cref{subsec:dsbm_vs_rf_appendix}. 


\textbf{Diffusion Schr\"odinger Bridge. } Schr\"odinger Bridges
\citep{schrodinger1932theorie} are ubiquitous in probability theory
\citep{leonard2014survey} and stochastic control \citep{dai1991stochastic,chen2020optimal}. 
More recently, they have been used for generative modeling: \cite{debortoli2021diffusion} introduced the
DSB algorithm and \cite{vargas2021solving,chen2021likelihood} introduced similar algorithms. The case of Dirac delta terminal distribution was investigated by \cite{wang2021deepschro}. 
These methods were later extended to solve conditional simulation and more general control problems
\citep{shi2022conditional,thornton2022riemannian,liu2022deep,chen2023deep,tamir2023transport}. 
In \cite{somnath2023aligned}, SBs are learned using one Bridge
Matching iteration, assuming access to the true Schr\"odinger static coupling. 
Our proposed method DSBM-IPF is closest to DSB, 
but with improved continous-time training and projections on the reciprocal
class which mitigate two limitations of DSB. 
Concurrently with our work, \cite{peluchetti2023diffusion} independently introduced the DSBM-IMF approach (named IDBM therein).

\vspace{-0.1cm}
\section{Experiments}
\label{sec:experiments}


\begin{figure}[b]
\centering
\setlength{\tabcolsep}{5pt}
\renewcommand\arraystretch{0.8}
\vspace{-0.3cm}
\scalebox{0.64}{
    \begin{tabular}{ccccc}
    \toprule 
     & \multicolumn{4}{c}{\textit{2-Wasserstein (Euler 20 steps)}}\tabularnewline
    \cmidrule{2-5} \cmidrule{3-5} \cmidrule{4-5} \cmidrule{5-5} 
    \textit{Dataset} & moons & scurve & 8gaussians & moons-8gaussians\tabularnewline
    \midrule
    \midrule 
    DSBM-IPF & 0.140\textpm 0.006 & 0.140\textpm 0.024 & 0.315\textpm 0.079 & \textit{0.812\textpm 0.092}\tabularnewline
    \midrule 
    DSBM-IMF & 0.144\textpm 0.024 & 0.145\textpm 0.037 & 0.338\textpm 0.091 & 0.838\textpm 0.098\tabularnewline
    \midrule 
    DSB & 0.190\textpm 0.049 & 0.272\textpm 0.065 & 0.411\textpm 0.084 & 0.987\textpm 0.324\tabularnewline
    \midrule 
    FM & 0.212\textpm 0.025 & 0.161\textpm 0.033 & 0.351\textpm 0.066 & -\tabularnewline
    \midrule 
    CFM & 0.215\textpm 0.028 & 0.171\textpm 0.023 & 0.370\textpm 0.049 & 1.285\textpm 0.314\tabularnewline
    \midrule 
    RF & \textit{0.129\textpm 0.022} & \textit{0.126\textpm 0.019} & \textit{0.267\textpm 0.041} & 1.522\textpm 0.304\tabularnewline
    \midrule 
    OT-CFM & \textbf{0.111\textpm 0.005} & \textbf{0.102\textpm 0.013} & \textbf{0.253\textpm 0.040} & \textbf{0.716\textpm 0.187}\tabularnewline
    \bottomrule
    \end{tabular}
}
\scalebox{0.64}{
    \begin{tabular}{ccccc}
    \toprule 
    \multicolumn{4}{c}{\textit{Path energy}}\tabularnewline
    \midrule
    moons & scurve & 8gaussians & moons-8gaussians\tabularnewline
    \midrule
    \midrule 
    1.598\textpm 0.034 & 2.110\textpm 0.059 & \emph{14.91\textpm 0.310} & 42.16\textpm 1.026\tabularnewline
    \midrule 
    1.580\textpm 0.036 & 2.092\textpm 0.053 & \textbf{14.81\textpm 0.255} & 41.00\textpm 1.495\tabularnewline
    \midrule 
    - & - & - & - \tabularnewline
    \midrule 
    2.227\textpm 0.056 & 2.950\textpm 0.074 & 18.12\textpm 0.416 & -\tabularnewline
    \midrule 
    2.391\textpm 0.043 & 3.071\textpm 0.026 & 18.00\textpm 0.090 & 116.5\textpm 2.633\tabularnewline
    \midrule 
    \textit{1.185\textpm 0.052} & \textit{1.633\textpm 0.074} & \textit{14.84\textpm 0.441} & \textit{37.61\textpm 3.906}\tabularnewline
    \midrule 
    \textbf{1.178\textpm 0.020} & \textbf{1.577\textpm 0.036} & 15.10\textpm 0.215 & \textbf{30.50\textpm 0.626}\tabularnewline
    \bottomrule
    \end{tabular}
}
\captionof{table}{Sampling quality as measured by 2-Wasserstein distance and path energy for the 2D experiments. $\pm1$ SD over 5 seeds. Best values are in bold and second best are italicized. }
\label{tab:2d_result}

\vspace{-0.25cm}
\centering
\includegraphics[width=0.9\textwidth,trim=100 0 100 0]{Figs/2d_sigma/transport.png}
\captionof{figure}{Learned SB probability flow between two mixtures of Gaussians (green $\to$ yellow).}
\label{fig:2d_sigma_paths}
\vspace{-0.3cm}
\end{figure}


\textbf{2D Experiments.} We first show our proposed methods can learn lower kinetic energy transport maps and generate correct samples in some 2D examples. 
We compare our method DSBM with DSB \citep{debortoli2021diffusion}, Flow Matching (FM) \citep{lipman2022flow}, Conditional Flow Matching (CFM), OT-CFM \citep{tong2023conditional}, and Rectified Flow (RF) \citep{liu2022flow}. 
OT-CFM directly uses a sample-based OT solver for mini-batches \citep{fatras2021minibatch,flamary2021pot}, see also \cite{pooladian2023multisample,stromme2023sampling}, whereas the rest of the methods do not use OT solvers. DSB and DSBM learn the EOT map as the solution of the diffusion process.

In \Cref{tab:2d_result}, we show the 2-Wasserstein distance between the true and generated samples, as well as the integrated path energy defined as $\smash{\E[\int_{0}^T || v(t, \bfZ_t) ||^2 \rmd t]}$ where $v$ is the learned drift along the ODE trajectory $\bfZ_t$. 
For direct comparability, we report for DSBM using its probability flow ODE, see \Cref{subsec:prob_flow_ode_appendix}. 
Lower path energies represent shorter (and potentially easier to integrate) trajectories.
We find that in this low dimensional setting, OT-CFM performs the best by utilizing OT solvers, but DSBM outperforms FM and CFM when OT solvers are not used.
Further, whilst DSBM solves the same SB problems as DSB, DSBM outperforms DSB on all datasets, suggesting DSBM solves the SB problem with higher accuracy.
Finally, we find Rectified Flow achieves lower sampling error than DSBM except for the \textit{moons-8gaussians} task, for which DSBM is significantly more accurate. Since RF can be informally seen as DSBM in the case $\sigma \rightarrow 0$, this suggests the optimal $\sigma$ varies for each task and between generative modeling and general transfer tasks.  Figure \ref{fig:2d_sigma_paths} visualizes how $\sigma$ affects the straightness and sample quality of learned transport maps between two mixture distributions.


\textbf{High-Dimensional Gaussian Experiment.} 
We next perform the Gaussian transport experiment in \cite{debortoli2021diffusion} with dimension $d=50$ to verify the scalability of our proposed approach. The true SB can be computed analytically in this case \citep{bunne2022gaussian}. In \Cref{fig:gaussian_ipf_convergence}, we plot the convergence of the learned mean $\E[\bfX_0]$, 
variance $\mathrm{Var}(\bfX_0)$,
and covariance $\mathrm{Cov}(\bfX_0, \bfX_T)$ between times $0,T$. 
We also consider RF and a baseline IMF-b, which both perform IMF numerically but only in the backward direction. All methods converge approximately to the correct mean. However, the variance estimates become inaccurate for RF and IMF-b. Among SB methods, DSB and IMF-b also gave inaccurate SB covariance estimates as the number of iteration increases. On the other hand, DSBM does not suffer from this issue. 
In \Cref{tab:gaussian_avg_kl}, we further quantify the accuracy and compare with SB-CFM \citep{tong2023conditional} by computing the KL divergence between the marginal distributions of the learned process $\P_t$ and the true SB $\PSB_t$. Our proposed methods achieve similar KL divergence as SB-CFM in dimension $d=5$, but are much more accurate in higher dimensions.

\begin{figure}
\centering
\begin{minipage}[b]{.66\textwidth}
    \centering
    \includegraphics[trim={3 0 1.2 0}, clip, height=2.65cm]{Figs/Gaussian/dim50_a0.1_mlp_large_10_10000_mean_tueplots_nomarker.pdf}
    \includegraphics[trim={1.2 0 1.2 0}, clip, height=2.65cm]{Figs/Gaussian/dim50_a0.1_mlp_large_10_10000_var_tueplots_nomarker.pdf}
    \includegraphics[trim={1.2 0 1.2 0}, clip, height=2.65cm]{Figs/Gaussian/dim50_a0.1_mlp_large_10_10000_cov_tueplots_nomarker.pdf}
    \caption{Convergence of Gaussian experiment in $d=50$. }  
    \label{fig:gaussian_ipf_convergence}
\end{minipage}
\hspace{0.3cm}
\begin{minipage}[b]{.3\textwidth}
    \centering
    {\setlength{\tabcolsep}{2pt}
    \scalebox{0.64}{
    \begin{tabular}{cccc}
    \toprule 
    KL $\times {10}^{-3}$ & $d=5$ & $d=20$ & $d=50$\tabularnewline
    \midrule
    \midrule 
    DSB & 3.26\textpm 1.60 & 13.0\textpm 3.49 & 32.8\textpm 1.28\tabularnewline
    \midrule 
    SB-CFM & 1.45\textpm 0.73 & 12.3\textpm 1.47 & 49.4\textpm 3.91\tabularnewline
    \midrule 
    DSBM-IPF & \textbf{1.23\textpm 0.23} & \textbf{4.42\textpm 0.76} & \textbf{8.75\textpm 0.87}\tabularnewline
    \midrule 
    DSBM-IMF & \emph{1.34\textpm 0.51} & \emph{5.05\textpm 0.95} & \emph{9.76\textpm 1.67}\tabularnewline
    \bottomrule
    \end{tabular}
    }}
    \captionof{table}{Average $\KL(\P_t|\PSB_t)$ at 21 uniformly spaced $t$. 
    }
    \label{tab:gaussian_avg_kl}
\end{minipage} 


\centering
\begin{minipage}[b]{.65\textwidth}
    \centering
    \subfloat[OT-CFM]{
    \includegraphics[width=0.28\linewidth]{Figs/MNIST_EMNIST/CFM/OTCFM250000b.png} 
    \label{fig:mnist_backward_samples_otcfm}
    }
    \subfloat[DSB]{
    \includegraphics[width=0.28\linewidth]{Figs/MNIST_EMNIST/DSB/DSB20b.png} 
    \label{fig:mnist_backward_samples_dsb20}
    }
    \subfloat[DSBM-IPF]{
    \includegraphics[width=0.28\linewidth]{Figs/MNIST_EMNIST/DSBM/DSBM50b.png} 
    }
    \caption{Samples of MNIST digits transferred from letters. }
    \label{fig:mnist_backward_samples}
\end{minipage}
\quad
\begin{minipage}[b]{.28\textwidth}
    \centering
    \includegraphics[width=\linewidth]{Figs/MNIST_EMNIST/mnist_emnist_fid_tueplots_new.pdf}
    \caption{FID vs iteration. }
    \label{fig:mnist_fid}
\end{minipage} 
\vspace{-.6cm}
\end{figure}

\textbf{MNIST, EMNIST transfer.} We test our method for domain transfer between MNIST digits and EMNIST letters as in \cite{debortoli2021diffusion}. We compare DSBM as a direct substitute of DSB, and also with Bridge Matching (BM) \citep{peluchettinon,liu2022let}, CFM, OT-CFM and RF. 
We plot some output samples from different algorithms in \Cref{fig:mnist_backward_samples} and the convergence of FID score in \Cref{fig:mnist_fid}. 
We find that OT-CFM becomes less applicable in higher dimensions and produces samples of worse quality (\Cref{fig:mnist_backward_samples_otcfm}).
On the other hand, image quality deteriorates during training of DSB and RF.  DSBM achieves higher quality samples visually, and does not suffer from  deterioration. 
It is also about $30\%$ more efficient than DSB in terms of runtime.


\textbf{CelebA transfer.}
Next, we evaluate and perform some ablations of our method on a transfer task on the CelebA $64\times64$ dataset. We
consider the images given by the tokens \texttt{male}/\texttt{old} and
\texttt{female}/\texttt{young}. 
In Figures \ref{fig:visual_sigma} and \ref{fig:fid_sigma}, we show that as
$\sigma$ increases, the quality of the images (as measured by the FID score) increases until $\sigma$ is too high, but the alignment (as measured by LPIPS) between the generated image and the original sample decreases. 
Additionally, we investigate the dependency between $\sigma$ and
image dimension in \Cref{fig:sigma_dim}. 
In particular, 
for the same $\sigma =1$, the outputs of DSBM for CelebA $128 \times 128$ are better aligned with the original data than for CelebA $64 \times 64$. 
This is in agreement with the observations of
\cite{chen2023importance,hoogeboom2023simple} that the \emph{noise schedule}
in diffusion models should scale with the resolution.

\begin{figure}
\centering
\pgfplotsset{every tick label/.append style={font=\scriptsize}}
\begin{minipage}[b]{.35\linewidth}
  \centering
  \includegraphics[trim={1 67 265 133}, clip, width=.18\linewidth]{img_celeba/init.png}
  \includegraphics[trim={1 67 265 133}, clip, width=.18\linewidth]{img_celeba/sigma_0.01.png}
  \includegraphics[trim={1 67 265 133}, clip, width=.18\linewidth]{img_celeba/sigma_0.1.png}
  \includegraphics[trim={1 67 265 133}, clip, width=.18\linewidth]{img_celeba/sigma_1.png}
  \includegraphics[trim={1 67 265 133}, clip, width=.18\linewidth]{img_celeba/sigma_10.png}
  \captionof{figure}{Left to right: initial and
    generated samples ($64\times64$) obtained after 20 DSBM-IMF iterations for
    $\sigma^2\in \{0.01, 0.1, 1, 10\}$.}
  \label{fig:visual_sigma}
\end{minipage}
\hspace{0.2cm}
\begin{minipage}[b]{.31\linewidth}
\centering
\begin{tikzpicture}
\begin{semilogxaxis}[axis y line*=left, width=0.85\linewidth, x post scale=1.5, minor tick style={draw=none}, xtick={0.01, 0.1, 1, 10}]
  \addplot[blue, line width=1pt, mark=*] table [x=sigma, y=fid, col sep=comma] {fid.csv};
\end{semilogxaxis}
\begin{semilogxaxis}[axis y line*=right, width=0.85\linewidth, , x post scale=1.5, y tick label style={
        /pgf/number format/.cd,
            fixed,
            fixed zerofill,
            precision=2,
        /tikz/.cd
    }, minor tick style={draw=none}, xtick={0.01, 0.1, 1, 10}]
  \addplot[red, line width=1pt, mark=*] table [x=sigma, y=lpip, col sep=comma] {lpip.csv};
 \end{semilogxaxis}
\end{tikzpicture} 
\captionof{figure}{FID (blue) and LPIPS (red) scores (lower is better for both) as we vary $\sigma^2$.}
\label{fig:fid_sigma}
\end{minipage}
\hspace{0.1cm}
\begin{minipage}[b]{.27\linewidth}
  \centering
  \includegraphics[width=0.8\linewidth]{img_celeba/sigma_1_dim_study.pdf}
  \includegraphics[width=0.8\linewidth]{img_celeba/generated_128_female_dim_study_downscale.pdf}
  \includegraphics[width=0.8\linewidth]{img_celeba/init_128_male_dim_study_downscale.pdf}
  \captionof{figure}{\label{fig:sigma_dim} Top to bottom: DSBM ($\sigma=1$)
    $64 \times 64$;  
    $128 \times 128$; original images.}
\end{minipage}
\vspace{-0.3cm}
\end{figure}


\textbf{Unpaired Fluid Flows Downscaling.} Finally, we apply DSBM to perform downscaling of geophysical fluid dynamics, i.e. super-resolution of low resolution spatial data. We use the dataset in \citep{bischoff2023unpaired}, which consists of unpaired low ($64 \times 64$) and high ($512 \times 512$) resolution fields. As shown in \Cref{fig:downscaler_vis}, DSBM is able to learn high resolution reconstructions by only slightly noising the low resolution input. In contrast, \cite{bischoff2023unpaired} use two diffusion models in forward and backward directions (Diffusion-fb) based on \cite{meng2022sdedit}, which improves over the Random baseline. 
\Cref{fig:downscaler_l2} shows that DSBM-IPF and DSBM-IMF achieve much lower $\ell_2$ distances for all frequency classes in the dataset than Diffusion-fb (and thus Random), indicating DSBM is able to reconstruct high resolution fields consistent with the low resolution source. 
\begin{figure}
\centering
\begin{minipage}[b]{.52\linewidth}
    \centering
    \subfloat[]{
    \includegraphics[width=1.5cm]{Figs/Downscaler/k=2/dsbm-ipf/clip_im_grid_start.png} 
    \includegraphics[width=1.5cm]{Figs/Downscaler/k=2/dsbm-ipf/clip_im_grid_15.png}
    \includegraphics[width=1.5cm]{Figs/Downscaler/k=2/dsbm-ipf/clip_im_grid_last.png} 
    }
    \subfloat[]{
    \includegraphics[width=1.5cm]{Figs/Downscaler/k=2/dsbm-ipf/clip_im_grid_data_x.png}
    }
    \caption{(a) Left to right: source low resolution sample, intermediate state and final reconstruction of DSBM-IPF; (b) an unpaired high resolution sample.   }
    \label{fig:downscaler_vis}
\end{minipage}
\hfill
\begin{minipage}[b]{.435\linewidth}
    \centering
    \includegraphics[width=0.9\linewidth]{Figs/Downscaler/downscaler_l2_tueplots.pdf}
    \caption{$\ell_2$ distance between low resolution source and high resolution reconstructed fields. }
    \label{fig:downscaler_l2}
\end{minipage}
\vspace{-0.5cm}
\end{figure}

\vspace{-0.1cm}
\section{Discussion}
In this work, we introduce IMF, a new methodology for learning \schro Bridges.
IMF is an alternative to the classical IPF and can be interpreted as its dual. 
Building on this new framework, we present two practical algorithms, DSBM-IPF and DSBM-IMF, for learning SBs. 
These algorithms mitigate the time-discretization and bias accumulation issues of existing methods.
However, DSBM still has some limitations. First, our results suggest DSBM is most effective for solving general transport problems. For generative modeling, we only find minor improvements compared to Bridge and Flow Matching on CIFAR-10 (see \Cref{subsec:cifar10_appendix}). Second, while DSBM is more efficient than DSB, it still requires sampling from the learned process during the caching step.
Finally, the EOT problem becomes more difficult to solve numerically for small values of $\sigma$. 


In future work, we would like to 
further investigate the differences between DSBM-IMF and DSBM-IPF.
IMF also appears useful for developing a better understanding of the Rectified Flow algorithm \citep{liu2022flow}, as IMF minimizes a clear objective \eqref{eq:schrodinger_bridge} and Rectified Flow can be seen as a limiting case of it. Finally, Rectified Flow has also been extended to solve OT problems with general convex costs by \cite{liu2022rectified}, and it would be interesting to derive a SB version of this extension. 

\newpage

\bibliographystyle{apalike}
\bibliography{bib}

\newpage
\appendix 

\section*{Outline of the Appendix} 
In \Cref{sec:related_work_appendix}, we first clarify the relationship
between different methods in the existing literature and our proposed DSBM framework.  In
\Cref{sec:bridge_design_space_appendix}, we focus on the family of linear SDEs, and draw a link between the
parameterization of bridges in this paper and the stochastic interpolant in
\cite{albergo2023stochastic}.  In \Cref{sec:proofs_appendix}, we give proofs for
results in the main text.  
In \Cref{sec:imf_gaussian_appendix}, we present additional theoretical results for IMF in the Gaussian case. 
In \Cref{sec:discrete_time_mp_appendix}, we derive the discrete-time version of
Markovian projection.  In \Cref{sec:dsbm_vs_dsb_appendix}, we explain
the benefits of DSBM compared to DSB in more detail.  In
\Cref{sec:joint-train-forw}, we describe a method for learning the forward and
backward processes jointly and propose a consistency loss between the forward
and backward processes.  In \Cref{sec:additional_details_appendix}, we present additional methodological
details, including the probability flow ODE associated with the learned SB process, and a
practical scaling of the loss function to reduce variance, similar to
standard Denoising Diffusion Models.  In \Cref{sec:exp_detail_appendix}, we give
further details for all experiments and additional experimental results.
Finally, we discuss broader impacts of our work in \Cref{sec:broader_impact_appendix}. 


\section{Discussion of Existing Works}
\label{sec:related_work_appendix}
\subsection{Bridge Matching and Flow Matching Models}
\label{subsec:fm_relationship_appendix}
In this section, we clarify the relationship between variants of 
Flow Matching and show that they are equivalent under some
conditions. We follow the nomenclature of \cite{tong2023conditional}. We refer
to the algorithm originally proposed in \cite{lipman2022flow} using linear
probability paths and described in \cite[Section 4.1]{tong2023conditional} as
Flow Matching (FM), and the algorithm proposed in \cite[Section
4.2]{tong2023conditional} as Conditional Flow Matching (CFM).  There is a small
constant parameter $\sigma_\text{min}$ in both algorithms, which controls the
smoothing of the modeled distribution. We consider the case
$\sigma_\text{min}=0$. Then CFM recovers exactly the 1st iteration
of Rectified Flow \citep{liu2022flow}. Furthermore, FM, CFM and the 1st iteration of
Rectified Flow are all equivalent when performing generative modeling with a
standard Gaussian $\pi_0$. 
We refer to them collectively
as Flow Matching models (FMMs) as they only differ in the smoothing method. 
We also present them all under the Bridge Matching framework. 
These models can also be interpreted in the context
of \emph{stochastic interpolants}
\citep{albergo2022building,albergo2023stochastic}. 
Finally, we present
recent applications of Bridge Matching and show that some of the objectives in
\cite{somnath2023aligned,liu2023I2SB,delbracio2023inversion} are identical.

\paragraph{Flow Matching and Conditional Flow Matching.} In Flow Matching (FM),
the objective \cite[Equation (21)]{lipman2022flow} is
\begin{equation}
  \label{eq:flow_matching_objective}
    \expeLigne{\Pi_{t,T}}{\normLigne{(\bfX_T - \bfX_t)/(T-t) - v_\theta(t,\bfX_t)}^2},
\end{equation}
where $\Pi_{t,T}$ is given by $\pi_T(\bfX_T) \mathrm{N}(\bfX_t;\frac{t}{T}\bfX_T, {(1 - \frac{t}{T})}^2)$.

In Conditional Flow Matching (CFM),
$\bfX_t^{0,T} = \frac{t}{T}\bfX_T + (1-\frac{t}{T})\bfX_0$, with
$\bfX_0 \sim \mathrm{N}(0, \Id)$ and the objective \cite[Equation
(16)]{tong2023conditional} is given by
\begin{equation}
  \label{eq:loss_tong}
    \expeLigne{\Pi_{0,T}}{\normLigne{(\bfX_T - \bfX_0)/T - v_\theta(t,\bfX_t^{0,T})}^2}.
\end{equation}
This is the same as \cite[Equation (1)]{liu2022flow}. Furthermore, $(\bfX_T - \bfX_0)/T=(1-\frac{t}{T})(\bfX_T - \bfX_0)/(T-t)=(\bfX_T - \bfX_t^{0,T})/(T-t)$, so the CFM objective is equivalent to 
\begin{equation}
    \label{eq:cfm_objective}
    \expeLigne{\Pi_{t,T}}{\normLigne{(\bfX_T - \bfX_t^{0,T})/(T-t) - v_\theta(t,\bfX_t^{0,T})}^2}.
  \end{equation}

  The optimal $v_\theta(t,x_t)=(\CPELigne{\Pi_{T|t}}{\bfX_T}{\bfX_t=x_t} - x_t)/(T-t)$. 
  In the case of generative modeling, $\pi_0$ is a standard Gaussian
  distribution and $\Pi_{0,T}$ is given by
  $\mathrm{N}(\bfX_0;0, \Id) \pi_T(\bfX_T)$. Thus, $\Pi_{t,T}$ is also given by
  $\pi_T(\bfX_T) \mathrm{N}(\bfX_t^{0,T};\frac{t}{T}\bfX_T, {(1 -
    \frac{t}{T})}^2)$. Therefore, the FM \citep{lipman2022flow} and CFM
  \citep{tong2023conditional} objectives are exactly the same. However, CFM is
  also applicable when $\pi_0$ is not Gaussian distributed, so CFM is a
  generalized version of FM\footnote{In the case $\sigma_\text{min}>0$, FM and
    CFM are indeed different in how smoothing is performed, and we refer to
    \cite{tong2023conditional} for a more detailed analysis.}.

  \paragraph{Stochastic Interpolant.} In
  \citep{albergo2022building,albergo2023stochastic}, the
  concept of stochastic interpolant is introduced. In \cite{albergo2022building}, the
  interpolation is deterministic (not necessarily linear), of the form
  $I_t(x_0,x_T) = \alpha(t) x_0 + \beta(t) x_T$, while in
  \cite{albergo2023stochastic}, the interpolation is stochastic given by 
  $I_t(x_0,x_T,z) = \alpha(t) x_0 + \beta(t) x_T + \gamma(t) \bfZ$ for  $\bfZ \sim \mathrm{N}(0, \Id)$. In
  \cite{albergo2022building}, an ODE is learned and the associated velocity
  field $v_\theta$ is obtained by minimizing the following objective
  \cite[Equation (9)]{albergo2022building}
  \begin{equation}
    \expeLigne{\Pi_{0,T}}{\normLigne{\partial_t I_t(\bfX_0, \bfX_T) - v_\theta(t,\bfX_t^{0,T})}^2}.
  \end{equation}
  Hence, if $I_t(x_0,x_T) = \tfrac{t}{T} x_0 + (1 - \tfrac{t}{T}) x_T$, we
  recover \eqref{eq:loss_tong}. 

  \paragraph{Link with Bridge Matching.} When $\Qbb$ is associated with the Brownian motion 
  $(\sigma \bfB_t)_{t \in \ccint{0,T}}$ and $\sigma \to 0$ in Bridge
  Matching, we recover the same objective
  \eqref{eq:bridge_matching_loss_brownian} as the Flow Matching objective
  \eqref{eq:cfm_objective}, since
  $\nabla \log \Qbb_{T|t}(\bfX_{T}|\bfX_t)=(\bfX_T - \bfX_t)/(\sigma^2
  (T-t))$.  Bridge Matching can also be applied to general distributions
  $\pi_0,\pi_T$; i.e. $\pi_0$ does not have to be restricted to a Gaussian. Therefore, Bridge Matching is a generalized version of Flow Matching, see
  also \cite[Equation (10)]{liu2022let}. 

  \paragraph{Inverse problems and interpolation.} 
  \cite{somnath2023aligned} and \cite{liu2023I2SB} present Bridge Matching algorithms 
  between aligned data $(\bfX_0,\bfX_T) \sim \Pi_{0,T}$. The objectives \cite[Equation (8)]{somnath2023aligned} and \cite[Equation (12)]{liu2023I2SB} are equivalent to the Bridge Matching objective
  \eqref{eq:bridge_matching_loss}.
  The main difference
  between \cite{liu2023I2SB} and \cite{somnath2023aligned} resides in the choice
  of $\Pi_{0,T}$. In the case of \cite{somnath2023aligned}, this choice is
  motivated by the access to \emph{aligned data} with applications in biology assuming they are distributed as the true Schr\"odinger static coupling, i.e. $\Pi_{0,T}=\PiSB_{0,T}$.
  In the case of \cite{liu2023I2SB}, $\Pi_{0,T}$ corresponds to a pairing between
  clean and corrupted images, e.g. with $\Pi_0=\pi_0$ the distribution of clean images and $\Pi_T=\pi_T$ 
  the distribution of corrupted images obtained from the clean images using the degradation kernel $\Pi_{T|0}$.

  Finally, in \cite[Equation (5)]{delbracio2023inversion} the authors consider a
  reconstruction process of the form
  \begin{equation}
    \label{eq:generative_T_to_zero}
    \rmd \bfX_t = (\CPELigne{\Pi_{0|t}}{\bfX_0}{\bfX_t} - \bfX_t)/t \rmd t , \qquad \bfX_T \sim \Pi_T ,
  \end{equation}
  where here we have replaced $F(x_t,t)$ by
  $\CPELigne{\Pi_{0|t}}{\bfX_0}{\bfX_t=x_t}$. This is justified if the
  $\| \cdot \|_p$ norm in \cite[Equation (4)]{delbracio2023inversion} is
  replaced by $\| \cdot \|_2^2$ (or any Bregman Loss Function, see
  \cite{banerjee2005optimality}).  In \cite{delbracio2023inversion}, $\Pi_{0,T}$
  corresponds to the joint distribution of clean and corrupted images as in
  \cite{liu2023I2SB}. Exchanging the role of $\Pi_0$ and
  $\Pi_T$, \eqref{eq:generative_T_to_zero} can be rewritten equivalently as
    \begin{equation}
    \label{eq:generative_zero_to_T}
    \rmd \bfX_t = (\CPELigne{\Pi_{T|t}}{\bfX_T}{\bfX_t} - \bfX_t)/(T-t) \rmd t , \qquad \bfX_0 \sim \Pi_0 . 
  \end{equation}
  We thus obtain the optimal Flow Matching vector field $v_\theta(t,x_t)=(\CPELigne{\Pi_{T|t}}{\bfX_T}{\bfX_t=x_t} - x_t)/(T-t)$ in \eqref{eq:cfm_objective}. 
  Note that \cite{delbracio2023inversion} also incorporates a stochastic version
  of their objective \cite[Equation
  (7)]{delbracio2023inversion}. It remains an open question whether this objective can be understood as a special instance of the Bridge Matching framework.


\subsection{On DSBM and Existing Works}
In this section, we show that the DSBM framework recovers the above existing algorithms for different choices of bridges $\Qbb_{|0,T}$ and couplings $\Pi_{0,T}^0$ in \Cref{alg:DSBM_general}. For the independent coupling
$\Pi_{0,T}^0 = \pi_0 \otimes \pi_T$ and Brownian bridge $\Qbb_{|0,T}$  \eqref{eq:interpolation_stochastic} 
with diffusion parameter $\sigma_t=\sigma$, 
the loss function \eqref{eq:loss_function} recovers the Brownian Bridge Matching loss \eqref{eq:bridge_matching_loss_brownian}.
Letting $\sigma \to 0$, we recover Flow Matching 
\citep{lipman2022flow}. In this case, further iterations repeating lines 7-9 in \Cref{alg:DSBM_general} (with only forward projections) recover Rectified Flow
\citep{liu2022flow}. 
If the coupling $\Pi_{0,T}^0$ is given by
an estimation of the OT map between $\pi_0$ and $\pi_T$, then the first
iteration recovers OT-CFM \citep{tong2023conditional, pooladian2023multisample}. Finally, for general bridges $\Qbb_{|0,T}$, if we are given the optimal Schr\"odinger Bridge
static coupling $\Pi_{0,T}^0=\PiSB_{0,T}$, then the DSBM procedure converges in one
iteration and we recover \cite{somnath2023aligned}. 


\subsection{DSBM and Rectified Flow}
\label{subsec:dsbm_vs_rf_appendix}
We discuss the differences in more detail between our proposed DSBM method and Rectified Flow.  
Both Rectified Flow and DSBM are general frameworks for building transport maps between two general distributions $\pi_0, \pi_T$. However, there are a few important theoretical and practical differences.  
Firstly, we adopt the SDE approach as opposed to the ODE approach
in Rectified Flow. This distinction is crucial in theory, as
\Cref{prop:schro-markov-recip}, which guarantees the uniqueness of the
characterization of SB, is valid only when $\sigma_t>0$. Consequently, Rectified
Flow is not guaranteed to converge to the dynamic optimal transport solution (see e.g.\ counterexample in \cite{liu2022rectified}). In
a following work, \cite{liu2022rectified} established formal connections between
Rectified Flow and OT when restricting the class of vector fields to gradient
fields. In DSBM, the connection to OT is obtained by considering its
entropy-regularized version.  
Furthermore, by adopting the SDE approach, we observe significant improvements of sample quality in our experiments when performing transport between two general distributions. 
This is in line with the theoretical analysis in \cite{albergo2023stochastic}. 
On the other hand, while Bridge Matching also achieves high sample quality using the SDE approach, the transported samples are much more dissimilar to the input data (see e.g.\ Figures \ref{fig:mnist_backward_trajectory}, \ref{fig:mnist_forward_trajectory}, \ref{fig:downscaler_vis_others}). 
Lastly, Rectified Flow also performs Markovian
projections iteratively, but only in the forward direction. Consequently, the
bias in the learned marginals $\Pbb_T^n$ is accumulated and cannot be corrected in later
iterations, i.e. the first iteration of RF will achieve the most accurate marginal
$\Pbb_T^1$. Subsequent iterations can improve the straightness of the flow, but at the cost of sampling
accuracy of $\Pbb_T^n$.  We observe in practice that this becomes particularly problematic if the first iteration of Rectified Flow (which is equivalent to CFM) fails to provide a good transport and learn an accurate $\Pbb_T^1$, 
e.g. in the case of \textit{moons-8gaussians} (\Cref{tab:2d_result}),
Gaussian transport (\Cref{fig:gaussian_ipf_convergence}), 
and MNIST, EMNIST transfer (\Cref{fig:mnist_fid} and \Cref{fig:mnist_backward_samples_comparison}). 
As Rectified Flow cannot recover from this issue, we observe the accuracy of $\Pbb_T^n$ only deteriorates in further iterations as $n$ increases. 
In our methodology, we leverage \Cref{prop:forward_backward_markov_proj} to perform
forward and backward Bridge Matching, and we observe that the marginal accuracy
is able to improve with iteration.

\section{The Design Space of Brownian Bridges}
\label{sec:bridge_design_space_appendix}

\subsection{Relationship to Stochastic Interpolants}
\paragraph{From stochastic interpolants to Brownian bridges.} In this section,
we draw a link between our parameterization of bridges and the one used in
\cite{albergo2023stochastic}. In \cite{albergo2023stochastic}, a stochastic
interpolant is defined as
\begin{equation}
  \label{eq:ode_integral}
  \bfX_t = \bar{\alpha}_t x_0 + \bar{\beta}_t x_T + \bar{\gamma}_t \bfZ ,
\end{equation}
where $\bfZ \sim \mathrm{N}(0, \Id)$. Since their methodology and analysis
mainly relies on the probability flow, they work with \eqref{eq:ode_integral},
which is easier to analyse. In our setting, as we deal mostly with diffusions, it
is natural to parameterize Brownian bridges as follows
\begin{equation}
  \label{eq:sde_diff}
  \rmd \bfX_t =\{ -\alpha_t \bfX_t + \beta_t x_T \} \rmd t + \gamma_t \rmd \bfB_t . 
\end{equation}
The goal of this section is to derive explicit formulas between the parameters
$\bar{\alpha}_t$, $\bar{\beta}_t$ and $\bar{\gamma}_t$ of \eqref{eq:ode_integral} and
the parameters $\alpha_t$, $\beta_t$ and $\gamma_t$ of
\eqref{eq:sde_diff}. Consider $(\bfX_t)_{t \in \ccint{0,T}}$ given by
\eqref{eq:sde_diff}. We have that for any $t \in \ccint{0,T}$
\begin{equation}
  \textstyle \bfX_t = \exp[-A_t] x_0 + \int_0^t \beta_s \exp[A_s - A_t] \rmd s x_T  + \int_0^t \gamma_s \exp[A_s - A_t] \rmd \bfB_s ,
\end{equation}
where $A_t = \int_0^t \alpha_s \rmd s$. Therefore, we have that
\begin{equation}
  \label{eq:identification_design_dos}
  \textstyle \bar{\alpha}_t = \exp[-\int_0^t \alpha_s \rmd s] , \qquad \bar{\beta}_t = \int_0^t \beta_s \exp[-\int_s^t \alpha_u \rmd u] \rmd s , \qquad \bar{\gamma}_t^2 = \int_0^t \gamma_s^2 \exp[-2\int_s^t \alpha_u \rmd u] \rmd s ,
\end{equation}
\begin{equation}
  \label{eq:identificiation_design}
  \alpha_t = -\tfrac{\bar{\alpha}_t'}{\bar{\alpha}_t} , \qquad \beta_t = \bar{\beta}_t' + \bar{\beta}_t \alpha_t , \qquad \gamma_t^2 = (\bar{\gamma}_t^2)' + 2 \bar{\gamma}_t^2 \alpha_t  = 2 \bar{\gamma}_t \bar{\gamma}_t' + 2 \bar{\gamma}_t^2 \alpha_t .
\end{equation}
Using this relationship, we get that the Markovian projection, see \Cref{def:markovian_proj}, is given by
  \begin{equation}
    \rmd \bfX_t^{\star} = f_t^\star(\bfX_t) \rmd t + \gamma_t \rmd \bfB_t , \qquad f_t^\star(x_t) = \CPELigne{\Pi_{T|t}}{-\alpha_t \bfX_t + \beta_t x_T}{\bfX_t = x_t}.
  \end{equation}  
We have that
\begin{align}
  f_t^\star(x_t) &= \CPELigne{\Pi_{T|t}}{-\alpha_t \bfX_t + \beta_t x_T}{\bfX_t = x_t} \\
  &= \CPELigne{\Pi_{0,T|t}}{-\alpha_t( \bar{\alpha}_t x_0 + \bar{\beta}_t x_T + \bar{\gamma}_t \bfZ) + \beta_t x_T}{\bfX_t = x_t} .
\end{align}
Using \eqref{eq:identificiation_design}, we get that
\begin{equation}
  f_t^\star(x_t) = \CPELigne{\Pi_{0,T|t}}{\bar{\alpha}_t' x_0 + \bar{\beta}_t' x_T + \tfrac{\bar{\alpha}_t' \bar{\gamma}_t}{\bar{\alpha}_t}\bfZ}{\bfX_t=x_t}.
\end{equation}
In \cite{albergo2023stochastic}, it is shown that
$\nabla \log \Mbb^\star_t(x_t) = -\CPELigne{\Pi_{0,T|t}}{\bfZ}{\bfX_t
  =x_t}/\bar{\gamma}_t$, where $\Mbb^\star$ is the Markovian projection.  The
probability flow associated with $(\bfX^\star_t)_{t \in \ccint{0,T}}$ is given
by
\begin{align}
  \rmd \bfZ_t^\star &= \{f_t^\star( \bfZ_t^\star) - \tfrac{\gamma_t^2}{2} \nabla \log \Mbb^\star_t( \bfZ_t^\star) \} \rmd t  \\
                    &= \{\CPELigne{\Pi_{0,T|t}}{\bar{\alpha}_t' x_0 + \bar{\beta}_t' x_T + (-\alpha_t \bar{\gamma}_t + \tfrac{\gamma_t^2}{2\bar{\gamma}_t})\bfZ}{\bfX_t=\bfZ_t^\star} \} \rmd t \\
                      &= \{\CPELigne{\Pi_{0,T|t}}{\bar{\alpha}_t' x_0 + \bar{\beta}_t' x_T + \bar{\gamma}_t' \bfZ}{\bfX_t=\bfZ_t^\star} \} \rmd t .
\end{align}
Hence, we recover \cite[Theorem 2.6]{albergo2023stochastic}.

\paragraph{Non-Markov path measures.} A natural question is whether
\eqref{eq:sde_diff} arises as the bridge measure of some \emph{Markov}
measure. For instance, if $\Qbb$ is associated with
$(x_0 + \bfB_t)_{t \in \ccint{0,T}}$, then pinning the process at $x_T$ at
time $T$, we get that the associated bridge measure $\Qbb_{|0,T}$ is given by
\begin{equation}
  \rmd \bfX_t^{0,T} = (x_T - \bfX_t)/(T-t) \rmd t + \rmd \bfB_t . 
\end{equation}
Therefore, we recover \eqref{eq:sde_diff} with
$\alpha_t = \beta_t = \tfrac{1}{T-t}$ and $\gamma_t = 1$. Using
\eqref{eq:identification_design_dos}, we get that
$\bar{\alpha}_t = 1 - \tfrac{t}{T}$, $\bar{\beta}_t = \tfrac{t}{T}$ and
$\bar{\gamma}_t^2 = (T-t)t/T$. We recover \eqref{eq:interpolation_stochastic},
upon noting that $\bfB_t - \tfrac{t}{T}\bfB_T$ is Gaussian with zero mean and
variance $(T-t)t/T$.

More generally, we consider a Markov measure $\Qbb$ associated with
$(\bfX_t)_{t \in \ccint{0,T}}$ such that
\begin{equation}
  \rmd \bfX_t = -a_t \bfX_t \rmd t + c_t \rmd \bfB_t , \qquad \bfX_0 = x_0 .
\end{equation}
We now derive the associated bridge measure $\Qbb_{|0,T}$:
\begin{equation}
  \textstyle \bfX_T = \exp[-\Lambda_T + \Lambda_t] \bfX_t   + \int_t^T c_s \exp[\Lambda_s - \Lambda_T] \rmd \bfB_s ,
\end{equation}
with $\Lambda_t = \int_0^t a_s \rmd s$.
We have that
\begin{align}
  \textstyle c_t^2 \nabla_{x_t} \log \Qbb_{T|t}(x_T|x_t)&\textstyle = (c_t^2 \exp[\Lambda_t - \Lambda_T] / \int_t^T c_s^2 \exp[2(\Lambda_s - \Lambda_T)] \rmd s) x_T  \\
  & \qquad \textstyle - (c_t^2 \exp[2(\Lambda_t - \Lambda_T)] / \int_t^T c_s^2 \exp[2(\Lambda_s - \Lambda_T)] \rmd s) x_t . 
\end{align}
Therefore, combining this result and \eqref{eq:bridge_forward}, we get that $\Qbb_{|0,T}$ is associated with
\begin{align}
  &\textstyle \alpha_t = a_t + c_t^2 \exp[-2\int_t^Ta_s \rmd s ] / \int_t^T c_s^2\exp[-2\int_s^Ta_u \rmd u ] \rmd s , \\
    &\textstyle \beta_t = c_t^2 \exp[-\int_t^Ta_s \rmd s ] / \int_t^T c_s^2\exp[-2\int_s^Ta_u \rmd u ] \rmd s , \qquad \gamma_t = c_t . 
\end{align}
In that case $(a_t,c_t)_{t \in \ccint{0,T}}$ entirely parameterize
$(\alpha_t, \beta_t, \gamma_t)_{t \in \ccint{0,T}}$. Hence, in the
Ornstein-Uhlenbeck setting, if $\Qbb_{|0,T}$ is the bridge of a Markov measure,
it is fully parameterized by two functions while in the non-Markov setting it is
parameterized by three functions.

In this paper, we present our framework in the Markovian setting as the
Schr\"odinger Bridge problem is usually defined with respect to Markov reference
measures. However, our methodology could be extended in a straightforward fashion
to the non-Markovian setting. This would allow for a further exploration of the
design space of DSBM.


\subsection{Linear SDE and Bridge Matching}
 
In this section, we study further the diffusion bridge of linear SDEs. 
Arbitrary Markov measures can be chosen to
build bridges; however, we want to be able to compute some representations of
the bridge in an explicit way. More precisely, denoting
$(\bfX^{0,T}_t)_{t \in \ccint{0,T}}$ the diffusion bridge with $x_0, x_T$ the initial and final
  condition, we want to have access to the following:
\begin{itemize}
\item \emph{integral sampler}: we want to have a formula to sample $\bfX^{0,T}_t$ for any
  $t \in \ccint{0,T}$ without having to run a stochastic process forward or
  backward. 
\item \emph{forward sampler}: we want to have a forward SDE for $\bfX^{0,T}_t$ with explicit
  coefficients, which might depend on $x_T$, running in a forward fashion terminating at $x_T$.
\item \emph{backward sampler}: we want to have a backward SDE for $\bfY^{0,T}_t=\bfX^{0,T}_{T-t}$ with explicit
  coefficients, which might depend on $x_0$, running in a backward fashion terminating at $x_0$.
\end{itemize}

We focus on \emph{linear SDEs} of the form
$\rmd \bfX_t = -\alpha \beta_t \bfX_t \rmd t + \sigma \beta_t^{1/2} \rmd \bfB_t$, which
are particularly amenable, where $(\beta_t)_{t \in \ccint{0,T}}$ is a schedule
with $\beta \in \rmC(\ccint{0,T}, \ooint{0,+\infty})$. 

\subsubsection{Brownian motion} 
First, we consider the Brownian motion setting and
let $(\bfX_t)_{t \in \ccint{0,T}}$ be associated with $\Qbb$ with $\rmd \bfX_t = \beta_t^{1/2} \rmd \bfB_t$. We
consider $(\bfX^{0,T}_t)_{t \in \ccint{0,T}}$ conditioned at both ends $\bfX^{0,T}_0 = x_0$ and
$\bfX^{0,T}_T = x_T$. First, using \cite[Theorem 3.3]{barczy2013representations}, we
have that for any $t \in \ccint{0,T}$
\begin{equation}
  \bfX^{0,T}_t = \tfrac{R(t,T)}{R(0,T)} x_0 + \tfrac{R(0,t)}{R(0,T)} (x_T  - \bfX_T) + \bfX_t ,
\end{equation}
with $R(s,t) = \int_s^t \beta_u \rmd u = \sigma^2 (B_t-B_s)$, where for any
$t \in \ccint{0,T}$, $B_t = \int_0^t \beta_s \rmd s$. Therefore, we get that
\begin{equation}
  \label{eq:integral_sampler_brownian}
  \bfX^{0,T}_t = (1 - \tfrac{B(t)}{B(T)}) x_0 + \tfrac{B(t)}{B(T)} (x_T  - \bfX_T) + \bfX_t .
\end{equation}
\eqref{eq:integral_sampler_brownian} defines the \emph{integral sampler}. In addition, using
\cite[Theorem 3.2]{barczy2013representations}, we have
\begin{equation}
  \bfX^{0,T}_0 = x_0 , \qquad \rmd \bfX^{0,T}_t = \{-\sigma^2 \beta_t /\gamma(t,T) \bfX^{0,T}_t + \sigma^2 \beta_t /\gamma(t,T) x_T\} \rmd t + \sigma \beta_t^{1/2} \rmd \bfB_t ,
\end{equation}
where $\gamma(s,t) = \sigma^2(B(t)-B(s))$. Therefore, we get that
\begin{equation}
  \label{eq:forward_sampler_brownian}
  \bfX^{0,T}_0 = x_0 , \qquad \rmd \bfX^{0,T}_t = \{-\tfrac{\beta_t}{B(T)-B(t)} \bfX^{0,T}_t + \tfrac{\beta_t}{B(T)-B(t)} x_T\} \rmd t + \sigma \beta_t^{1/2} \rmd \bfB_t .
\end{equation}
\eqref{eq:forward_sampler_brownian} defines the \emph{forward sampler}. Finally, we derive the
\emph{backward sampler} by considering the time-reversal of the forward
unconditional process (initialized at $x_0$). Following \cite{haussmann1986time},
\begin{equation}
  \label{eq:time_reversal}
  \bfY^{0,T}_0 = x_T , \qquad \rmd \bfY^{0,T}_t = \sigma^2 \beta_{T-t} \nabla \log \Qbb_{T-t|0}(\bfY^{0,T}_t|x_0) \rmd t + \sigma \beta_{T-t}^{1/2} \rmd \bfB_t . 
\end{equation}
In addition, we have that
\begin{equation}
  \bfX_t = x_0 + \sigma B(t)^{1/2} \vareps_t , \qquad \vareps_t \sim \mathrm{N}(0, \Id) .  
\end{equation}
Hence, we get that for any $t \in \ccint{0,T}$ and $x \in \rset^d$
\begin{equation}
  \nabla \log \Qbb_{t|0}(x|x_0) = - (x - x_0)/ (\sigma^2 B(t)) . 
\end{equation}
Combining this result and \eqref{eq:time_reversal}, we get
\begin{equation}
  \label{eq:backward_sampler_brownian}
  \bfY^{0,T}_0 = x_T , \qquad \rmd \bfY^{0,T}_t = \{ -\tfrac{ \beta_{T-t}}{B(T-t)} \bfY^{0,T}_t + \tfrac{\beta_{T-t}}{B(T-t)} x_0 \} \rmd t  + \sigma \beta_{T-t}^{1/2} \rmd \bfB_t . 
\end{equation}
Combining \eqref{eq:integral_sampler_brownian},
\eqref{eq:forward_sampler_brownian} and \eqref{eq:backward_sampler_brownian}, we get
\begin{align}
  & \bfX^{0,T}_t = \lambda_t x_0 +\varphi_t (x_T  - \bfX_T) + \bfX_t . \\
  & \bfX^{0,T}_0 = x_0 , \qquad \rmd \bfX^{0,T}_t = \{\kappa_t^f \bfX^{0,T}_t + \Psi_t^f x_T\} \rmd t + \sigma \beta_t^{1/2} \rmd \bfB_t , \\
  & \bfY^{0,T}_0 = x_T , \qquad \rmd \bfY^{0,T}_t = \kappa_{T-t}^b \bfY^{0,T}_t + \Psi_{T-t}^b  x_0 \} \rmd t  + \sigma \beta_{T-t}^{1/2} \rmd \bfB_t ,
\end{align}
with
\begin{align}
  &\lambda_t = 1 - \tfrac{B(t)}{B(T)} , \qquad \varphi_t = \tfrac{B(t)}{B(T)} , \\
  &\kappa_t^f = -\tfrac{\beta_t}{B(T) - B(t)} , \qquad \Psi_t^f = \tfrac{\beta_t}{B(T) - B(t)} , \\
  &\kappa_t^b = -\tfrac{\beta_t}{B(t)} , \qquad \Psi_t^b = \tfrac{\beta_t}{B(t)} .
\end{align}

\subsubsection{Ornstein-Uhlenbeck}
Second, we consider the Ornstein-Uhlenbeck setting and let
$(\bfX_t)_{t \in \ccint{0,T}}$ with
$\rmd \bfX_t = -\alpha \beta_t \bfX_t \rmd t + \sigma \beta_t^{1/2} \rmd \bfB_t$, with
$\alpha \neq 0$. We consider $(\bfX^{0,T}_t)_{t \in \ccint{0,T}}$, the stochastic
process $(\bfX_t)_{t \in \ccint{0,T}}$ conditioned at both ends $\bfX^{0,T}_0 = x_0$
and $\bfX^{0,T}_T = x_T$. First, using \cite[Theorem 3.3]{barczy2013representations},
we have that for any $t \in \ccint{0,T}$
\begin{equation}
  \bfX^{0,T}_t = \tfrac{R(t,T)}{R(0,T)} x_0 + \tfrac{R(0,t)}{R(0,T)} (x_T  - \bfX_T) + \bfX_t ,
\end{equation}
with $R(s,t) = \exp[\alpha(B(t) - B(s))] \gamma(s,t)$, with
$\gamma(s,t) = \int_s^t \sigma^2 \beta(u) \exp[-2\alpha (B(t) - B(u))] \rmd
u$. In particular, we have
\begin{equation}
  \label{eq:R_gamma}
  \gamma(s,t) = \tfrac{\sigma^2}{2\alpha}(1 - \exp[-2\alpha(B(t) - B(s))] , \qquad R(s,t) = \tfrac{\sigma^2}{\alpha} \sinh(\alpha(B(t) - B(s))). 
\end{equation}
Therefore, we get that
\begin{equation}
  \label{eq:integral_sampler_ou}
  \bfX^{0,T}_t = \tfrac{\sinh(\alpha(B(T) - B(t))}{\sinh(\alpha B(T))} x_0 + \tfrac{\sinh(\alpha B(t))}{\sinh(\alpha B(T))} (x_T  - \bfX_T) + \bfX_t .
\end{equation}
\eqref{eq:integral_sampler_ou} defines the \emph{integral sampler}. In addition, using
\cite[Theorem 3.2]{barczy2013representations} and \eqref{eq:R_gamma}, we have
$\bfX^{0,T}_0 = x_0$ and
\begin{align}
  \rmd \bfX^{0,T}_t &= \{-\alpha\beta_t \bfX_t -\tfrac{\sigma^2 \beta_t \exp[-2\alpha(B(T) - B(t))]}{\gamma(t,T)} \bfX^{0,T}_t + \tfrac{\sigma^2 \beta_t \exp[-\alpha(B(T) - B(t))]}{\gamma(t,T)} x_T\} \rmd t + \sigma \beta_t^{1/2} \rmd \bfB_t \\
              &= \{-\alpha\beta_t \bfX_t -  \tfrac{2\alpha \beta_t }{\exp[2\alpha(B(T) - B(t))] - 1} \bfX^{0,T}_t + \tfrac{2\alpha \beta_t }{\exp[-\alpha(B(T) - B(t))] - \exp[-\alpha(B(T) - B(t))]} x_T\} \rmd t + \sigma \beta_t^{1/2} \rmd \bfB_t \\
              &= \{-\alpha\beta_t \tfrac{\exp[2\alpha(B(T) - B(t))] + 1}{\exp[2\alpha(B(T) - B(t))] - 1} \bfX^{0,T}_t + \tfrac{2\alpha \beta_t }{\exp[-\alpha(B(T) - B(t))] - \exp[-\alpha(B(T) - B(t))]} x_T\} \rmd t + \sigma \beta_t^{1/2} \rmd \bfB_t \\
    &= \{-\alpha \beta_t\coth(\alpha (B(T) - B(t))) \bfX^{0,T}_t + \alpha \beta_t \csch(\alpha (B(T) - B(t))) x_T \} \rmd t + \sigma \beta_t^{1/2} \rmd \bfB_t .
\end{align}
In the formula, $\coth$ is the hyperbolic cotangent function defined as $\coth(x)=\frac{1}{\tanh(x)}=\frac{\cosh(x)}{\sinh(x)}$ and $\csch$ is the hyperbolic cosecant function defined as $\csch(x)=\frac{1}{\sinh(x)}$. Combining this result and \eqref{eq:R_gamma}, we get that 
\begin{equation}
  \label{eq:forward_sampler_ou} 
    \bfX^{0,T}_0 = x_0 , \ \rmd \bfX^{0,T}_t = \{-\alpha \beta_t\coth(\alpha (B(T) - B(t))) \bfX^{0,T}_t + \alpha \beta_t \csch(\alpha (B(T) - B(t))) x_T \} \rmd t + \sigma \beta_t^{1/2} \rmd \bfB_t . 
\end{equation}
\eqref{eq:forward_sampler_ou} defines the \emph{forward sampler}. Finally, we derive the
\emph{backward sampler} by considering the time-reversal of the forward
unconditional process (initialized at $x_0$). Following \cite{haussmann1986time},
\begin{equation}
  \label{eq:time_reversal_ou}
  \bfY^{0,T}_0 = x_T , \qquad \rmd \bfY^{0,T}_t = \{ \alpha \beta_{T-t} \bfY^{0,T}_t  + \sigma^2 \beta_{T-t} \nabla \log \Qbb_{T-t|0}(\bfY^{0,T}_t|x_0)\} \rmd t + \sigma \beta_{T-t}^{1/2} \rmd \bfB_t . 
\end{equation}
In addition, we have that
\begin{equation}
  \bfX_t = \exp[-\alpha B(t)] x_0 + \tfrac{\sigma}{\sqrt{2\alpha}} (1 - \exp[-2\alpha B(t)])^{1/2}  \vareps_t , \qquad \vareps_t \sim \mathrm{N}(0, \Id) .  
\end{equation}
Hence, we get that for any $t \in \ccint{0,T}$ and $x \in \rset^d$
\begin{equation}
  \nabla \log \Qbb_{t|0}(x|x_0) = - 2\alpha (x - \exp[-\alpha B(t)]x_0)/ (\sigma^2 (1 - \exp[-2\alpha B(t)])) . 
\end{equation}
Combining this result and \eqref{eq:time_reversal_ou}, we get $\bfY^{0,T}_0 = x_T$ and 
\begin{align}
  \rmd \bfY^{0,T}_t &= \{ \alpha \beta_{T-t} \bfY^{0,T}_t - \tfrac{2\alpha \beta_{T-t}}{1 - \exp[-\alpha B(T-t)]}\bfY^{0,T}_t  + \tfrac{2\alpha \beta_{T-t}\exp[-\alpha B(T-t)]}{1 - \exp[-2\alpha B(T-t)]} x_0 \} \rmd t  + \sigma \beta_{T-t}^{1/2} \rmd \bfB_t \\
              &= \{ -\alpha \beta_{T-t} \tfrac{1 + \exp[-2\alpha B(T-t)]}{1 - \exp[-2\alpha B(T-t)]}\bfY^{0,T}_t  + \tfrac{2\alpha \beta_{T-t}}{\exp[\alpha B(T-t)] - \exp[-\alpha B(T-t)]} x_0 \} \rmd t  + \sigma \beta_{T-t}^{1/2} \rmd \bfB_t \\
  &= \{-\alpha \beta_{T-t}\coth(\alpha B(T-t)) \bfY^{0,T}_t + \alpha \beta_{T-t} \csch(\alpha B(T-t)) x_T \} \rmd t + \sigma \beta_{T-t}^{1/2} \rmd \bfB_t . 
\end{align}
Therefore 
\begin{equation}
  \label{eq:backward_sampler_ou} 
  \bfY^{0,T}_0 = x_T , \ \rmd \bfY^{0,T}_t = \{-\alpha \beta_{T-t}\coth(\alpha B(T-t)) \bfY^{0,T}_t + \alpha \beta_{T-t} \csch(\alpha B(T-t)) x_T \} \rmd t + \sigma \beta_{T-t}^{1/2} \rmd \bfB_t . 
\end{equation}
Combining \eqref{eq:integral_sampler_ou},
\eqref{eq:forward_sampler_ou} and \eqref{eq:backward_sampler_ou}, we get
\begin{align}
  & \bfX^{0,T}_t = \lambda_t x_0 +\varphi_t (x_T  - \bfX_T) + \bfX_t . \\
  & \bfX^{0,T}_0 = x_0 , \qquad \rmd \bfX^{0,T}_t = \{\kappa_t^f \bfX^{0,T}_t + \Psi_t^f x_T\} \rmd t + \sigma \beta_t^{1/2} \rmd \bfB_t , \\
  & \bfY^{0,T}_0 = x_T , \qquad \rmd \bfY^{0,T}_t = \{\kappa_{T-t}^b \bfY^{0,T}_t + \Psi_{T-t}^b  x_0 \} \rmd t  + \sigma \beta_{T-t}^{1/2} \rmd \bfB_t ,
\end{align}
with
\begin{align}
  &\lambda_t = \tfrac{\sinh(\alpha(B(T) - B(t))}{\sinh(\alpha B(T))} , \qquad \varphi_t =  \tfrac{\sinh(\alpha B(t))}{\sinh(\alpha B(T))} , \\
  &\kappa_t^f = -\alpha \beta_{t}\coth(\alpha (B(T)-B(t))) , \qquad \Psi_t^f = \alpha \beta_{t}\csch(\alpha (B(T)-B(t))) , \\
  &\kappa_t^b = -\alpha \beta_{t}\coth(\alpha B(t)) , \qquad \Psi_t^b = \alpha \beta_{t}\csch(\alpha B(t)) .
\end{align}
Using that $\tanh(x) \sim x$ and $\sinh(x) \sim x$ for $x \to 0$, we recover the
Brownian motion setting by letting $\alpha \to 0$. Note that
\cite{albergo2023stochastic} show that given an \emph{integral sampler}, which
does not necessarily comes from a Markovian process, a \emph{forward sampler}
with the same marginals can be defined, although it does not necessarily
satisfies the fact that the \emph{paths} have the same distribution. 


\section{Proofs}
\label{sec:proofs_appendix}

\subsection{Proof of \Cref{prop:markovian-projection}}

We refer the reader to \cite{chung2006markov,rogers2000diffusions} for an introduction to Doob
$h$-transform. Our theoretical treatment of the Doob $h$-transform closely
follows \cite{palmowski2002technique}.

First, we introduce the \emph{infinitesimal generator} $\mathcal{A}$ given for
any $f \in \rmC_c^\infty(\ccint{0,T} \times \rset^d, \rset)$,
$t \in \ccint{0,T}$ and $x \in \rset^d$ by
\begin{equation}
  \label{eq:infinitesimal_generator_Q}
  \mathcal{A}f(t,x) = \langle f_t(x), \nabla f(t, x) \rangle + \tfrac{\sigma_t^2}{2} \Delta f(t,x) + \partial_t f(t,x) . 
\end{equation}
The following assumption ensures that the diffusion associated with $\Qbb$ as well as its Markovian projections are well-defined.

\begin{assumption}
  \label{assum:nice_diffusion}
  $f$, $\sigma$ and
  $(t, x_t) \mapsto \CPELigne{\Pi_{T|t}}{\nabla \log
    \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t=x_t}$ are locally Lipschitz and there
  exist $C > 0 $, $\psi \in \rmC(\ccint{0,T}, \rset_+)$ such that for any
  $t \in \ccint{0,T}$ and $x_0, x_t \in \rset^d$, we have
 \begin{align}
    &\normLigne{f_t(x_t)} \leq C(1 + \normLigne{x_t}) , \qquad C \geq \sigma_t \geq 1/C , \\
        &\normLigne{\CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t=x_t}} \leq C\psi(t)(1 + \normLigne{x_t}) .
  \end{align}
\end{assumption}

We consider the following assumption, which will ensure that we can apply Doob
$h$-transform techniques.

\begin{assumption}
  \label{assum:good_function_palmowski}
  For any $x_0 \in \rset^d$, $\Pi_{T|0}$ is absolutely continuous
  w.r.t. $\Qbb_{T|0}$.  For any $x_0 \in \rset^d$, let $\varphi_{T|0}$ be given
  for any $x_T \in \rset^d$ by
  $\varphi_{T|0}(x_T|x_0) = \rmd \Pi_{T|0}(x_T|x_0) / \rmd
  \Qbb_{T|0}(x_T|x_0)$ and assume that for any $x_0 \in \rset^d$,
  $x_T \mapsto \varphi_{T|0}(x_T|x_0)$ is bounded. For any $x_0 \in \rset^d$,
  let $\varphi_{t|0}$ given for any $x_t \in \rset^d$ and $t \in \ccint{0,T}$ by
  \begin{equation}
    \label{eq:def_h_transform}
    \textstyle
    \varphi_{t|0}(x_t|x_0) = \int_{\rset^d} \varphi_{T|0}(x_T|x_0) \rmd \Qbb_{T|t} (x_T|x_t) .
  \end{equation}
  Finally, we assume that for any $x_0 \in \rset^d$,
  $(t, x_t) \mapsto 1/\varphi_{t|0}(x_t|x_0)$ and
  $(t, x_t) \mapsto \mathcal{A} \varphi_{t|0}(x_t|x_0)$ are bounded.
\end{assumption}

This means that for any $x_0 \in \rset^d$, $(t, x_t) \mapsto \varphi_t(x_t|x_0)$
is a \emph{good function} in the sense of \cite[Proposition
3.2]{palmowski2002technique}.  Note here that these assumptions could be relaxed
on a case-by-case basis. We leave this study for future work.

The following lemma is a direct consequence of
\Cref{assum:good_function_palmowski} and \eqref{eq:def_h_transform}. It ensures
that the $h$-function $\varphi_{t|0}$ satisfies the backward Kolmogorov
equation.

\begin{lemma}
  \label{lemma:backward_kolmogorov}
  Assume \tref{assum:good_function_palmowski}. Then,
  $\varphi \in \rmC^{1,2}(\coint{0,T} \times \rset^d, \rset)$ and
  $\mathcal{A} \varphi_{|0}=0$.
\end{lemma}

Using \eqref{eq:infinitesimal_generator_Q}, we have that for any
$x_0 \in \rset^d$ and $f \in \rmC^\infty_c(\ccint{0,T} \times \rset^d, \rset)$,
$t \in \ccint{0,T}$ and $x_t \in \rset^d$
\begin{equation}
  \label{eq:doob_h_generator}
  ( \mathcal{A}(f \varphi_{|0}) - f \mathcal{A} \varphi_{|0})(t,x_t) / \varphi_{|0}(t,x_t) = \mathcal{A} f(t,x_t) + \sigma_t^2 \langle \nabla f(t,x_t) , \nabla \log \varphi_{t|0}(x_t|x_0) \rangle . 
\end{equation}

Finally, we
consider the following assumption, which will ensure that the Doob $h$-transform
is well-defined. 

\begin{assumption}
  \label{assum:doob_h_defined}
  For any $x_0 \in \rset^d$, there exists $C \geq 0$ such that for
  any $t \in \ccint{0,T}$ and $x_t \in \rset^d$,
  $\normLigne{\nabla \log \varphi_{t|0}(x_t|x_0)} \leq C(1 +\normLigne{x_0}+\normLigne{x_t})$. 
\end{assumption}


We are now ready to state and prove \Cref{prop:markovian-projection}. Note that
the Markovian projection is defined in \Cref{def:markovian_proj}. Finally, we
define $\calM$ the space of path measures such that
$\Pbb \in \calM$ if $\Pbb$ is associated with
$\rmd \bfX_t = \{f_t(\bfX_t) + v_t(\bfX_t) \} \rmd t + \sigma_t \rmd \bfB_t$,
with $\sigma,v$ locally Lipschitz. This restriction of Markov measures allows
us to apply the entropic version of the Girsanov theorem
\citep{leonard2012girsanov}. It has no impact on our methodology.

\begin{proposition*}
  Assume \tref{assum:nice_diffusion}, \tref{assum:good_function_palmowski},
  \tref{assum:doob_h_defined}. Let $\Mbb^\star = \projM(\Pi)$. Then, 
  \begin{align}    
    &\textstyle \Mbb^\star = \argmin_{\Mbb} \ensembleLigne{\KLLigne{\Pi}{\Mbb}}{\Mbb \in \calM} , \\
    & \textstyle \KLLigne{\Pi}{\Mbb^\star} = \tfrac{1}{2} \int_0^T \expeLigne{\Pi_{0,t}}{\normLigne{\sigma_t^2 \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t} - v_t^\star}^2}/\sigma_t^2 \rmd t  .
  \end{align}
  In addition, we have that for any $t \in \ccint{0,T}$, $\Mbb^\star_t = \Pi_t$. In particular, $\Mbb^\star_T = \Pi_T$.
\end{proposition*}


\begin{proof} 
  First, we recall that $\Pi$ is given by $\Pi = \Qbb \varphi_{0,T}$ with
  $\varphi_{0,T} = \tfrac{\rmd \Pi_{0,T}}{\rmd \Qbb_{0,T}}$. In particular, we
  have $\Pi_{|0} = \Qbb_{|0} \varphi_{T|0}$, where
  $\varphi_{T|0} = \tfrac{\rmd \Pi_{T|0}}{\rmd \Qbb_{T|0}}$. Therefore, using
  \Cref{lemma:backward_kolmogorov}, \cite[Lemma 3.1, Lemma
  4.1]{palmowski2002technique}, the remark following \cite[Lemma
  4.1]{palmowski2002technique}, \tref{assum:nice_diffusion},
  \tref{assum:good_function_palmowski} and \tref{assum:doob_h_defined}, we get
  that $\Pi_{|0}$ is Markov and associated with the distribution of
  $(\bfX_t)_{t \in \ccint{0,T}}$ given for any $t \in \ccint{0,T}$ by 
  \begin{equation}
    \label{eq:non_markov}
    \textstyle \bfX_t = \int_0^t \{ f_s(\bfX_s) + \sigma_s^2 \nabla \log \varphi_{s|0}(\bfX_s|\bfX_0) \} \rmd s + \int_0^t \sigma_s \rmd \bfB_s ,
  \end{equation}
  where for any $t \in \ccint{0,T}$, $x_0, x_t \in \rset^d$ we recall that 
  \begin{equation}
    \label{eq:varphi_int_def}
    \textstyle \varphi_{t|0}(x_t|x_0) = \int_{\rset^d} \varphi_{T|0}(x_T|x_0) \rmd \Qbb_{T|t}(x_T|x_t) . 
  \end{equation}
  First, we have that for any $t \in \ccint{0,T}$, $x_t, x_0 \in \rset^d$
  \begin{equation}
    \textstyle \Qbb_{t|0}(x_t|x_0) \varphi_{t|0}(x_t|x_0) = \int_{\rset^d} \Qbb_{t|0,T}(x_t|x_T,x_0) \rmd \Pi_{T|0}(x_T|x_0) = \Pi_{t|0}(x_t|x_0) . 
  \end{equation}
  Therefore, we get that for any $t \in \ccint{0,T}$ and $x_t, x_0 \in \rset^d$
  \begin{equation}
    \label{eq:varphi_integrate}
    \varphi_{t|0}(x_t|x_0) = \tfrac{\rmd \Pi_{t|0}(x_t|x_0)}{\rmd \Qbb_{t|0}(x_t|x_0)} . 
  \end{equation}
    In addition, we have the following identity for any $t \in \ccint{0,T}$, $x_0, x_t, x_T \in \rset^d$
  \begin{equation}
    \Qbb_{T|0}(x_T|x_0) \Qbb_{t|0,T}(x_t|x_0,x_T) = \Qbb_{t|0}(x_t|x_0) \Qbb_{T|t}(x_T|x_t) .
  \end{equation}
  Using \eqref{eq:varphi_int_def}, this result and \eqref{eq:varphi_integrate},
  we get that for any $t \in \ccint{0,T}$ and $x_0,x_t \in \rset^d$
  \begin{align}
    \textstyle \nabla \log \varphi_{t|0}(x_t|x_0) &\textstyle= \int_{\rset^d} \tfrac{\Pi_{T|0}(x_T|x_0) \Qbb_{T|t}(x_T|x_t)}{\Qbb_{T|0}(x_T|x_0) \varphi_{t|0}(x_t|x_0)} \nabla \log \Qbb_{T|t}(x_T|x_t) \rmd x_T \\
                                                  &\textstyle= \int_{\rset^d} \tfrac{\Pi_{T|0}(x_T|x_0) \Qbb_{t|0,T}(x_t|x_0,x_T)}{\Qbb_{t|0}(x_t|x_0) \varphi_{t|0}(x_t|x_0)} \nabla \log \Qbb_{T|t}(x_T|x_t) \rmd x_T \\
    &\textstyle= \int_{\rset^d} \tfrac{\Pi_{t,T|0}(x_t,x_T|x_0)}{\Pi_{t|0}(x_t|x_0)} \nabla \log \Qbb_{T|t}(x_T|x_t) \rmd x_T \\    
                                                  &= \textstyle  \int_{\rset^d} \nabla \log \Qbb_{T|t}(x_T|x_t) \rmd \Pi_{T|t,0}(x_T|x_t,x_0) . 
  \end{align}
  Hence, combining this result and \eqref{eq:non_markov}, we get 
  \begin{equation}
    \label{eq:non_markov_2}
    \textstyle \bfX_t = \int_0^t \{ f_s(\bfX_s) + \sigma_s^2 \CPELigne{\Pi_{T|t, 0}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t, \bfX_0} \} \rmd s + \int_0^t \sigma_s \rmd \bfB_s .
  \end{equation}
  Let $\Mbb$ be Markov defined by
  $\rmd \bfX_t = \{f_t(\bfX_t) + v_t(\bfX_t) \} \rmd t + \sigma_t \rmd \bfB_t$,
  such that $\KLLigne{\Pi}{\Mbb} < +\infty$ with $\sigma,v$ locally
  Lipschitz. Using \cite[Theorem 2.3]{leonard2012girsanov}, we get that
  \begin{equation}
    \textstyle \KLLigne{\Pi}{\Mbb} = \tfrac{1}{2} \int_0^T \expeLigne{\Pi_{0,t}}{\| \sigma_t^2\CPELigne{\Pi_{T|t, 0}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t, \bfX_0} - v_t(\bfX_t) \|^2} / \sigma_t^2 \rmd t . 
  \end{equation}
  In addition, we have that for any $t \in \ccint{0,T}$,
  \begin{align}
    &\expeLigne{\Pi_{0,t}}{\| \sigma_t^2\CPELigne{\Pi_{T|t, 0}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t, \bfX_0} - v_t(\bfX_t)\|^2} \\
    & \geq \expeLigne{\Pi_{0,t}}{\| \sigma_t^2\CPELigne{\Pi_{T|t, 0}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t, \bfX_0} - v^\star_t(\bfX_t) \|^2} ,
  \end{align}
  where
$v^\star_t(x_t) = \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log
    \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t=x_t}$ which concludes the first part of
  the proof. For the second part of the proof, we show that for any
  $t \in \ccint{0,T}$, we have $\Mbb^\star_t = \Pi_t$. First, we have that $\Mbb^\star_t$
  and $\Pi_t$ satisfy the same Fokker-Planck equation, see \cite[Theorem
  2]{peluchettinon} for instance. We conclude by uniqueness of the solutions of
  the Fokker-Planck equation under \tref{assum:nice_diffusion} and
  \tref{assum:doob_h_defined}, see \cite{bogachev2021uniqueness} for instance.
\end{proof}



\subsection{Proof of \Cref{prop:reciprocal-projection}}
\begin{proof}
  By the additive property of KL divergence \citep{leonard2014some},
  $\KLLigne{\Pbb}{\Pi}=\KLLigne{\Pbb_{0,T}}{\Pi_{0,T}} +
  \expeLigne{\Pbb_{0,T}}{\KLLigne{\Pbb_{|0,T}}{\Pi_{|0,T}}}$. Restricting
  $\Pi_{|0,T}=\Qbridge$ directly gives that the KL minimizer is $\Pi^\star$
  with $\Pi^\star_{0,T}=\Pbb_{0,T}$, and thus
  $\Pi^\star=\P_{0,T} \Qbridge$ which we recall is short for 
  $\Pi^\star(\cdot) = \int_{\rset^d \times \rset^d} \Qbb_{|0,T}(\cdot|x_0, x_T) \P_{0,T}(\rmd x_0, \rmd x_T)$. 
\end{proof}


\subsection{Proof of \Cref{prop:schro-markov-recip}}

This result is a direct consequence of \cite[Theorem 2.12]{leonard2014survey},
which we recall here for completeness.

\begin{proposition*}
  Assume
  that $\Qbb \in \calM$, 
  that $\Qbb_0 = \Qbb_T = \bar{\Qbb}$, that for any
  $x_0, x_T \in \rset^d$,
  $\rmd \Qbb_{0,T} / \rmd (\bar{\Qbb} \otimes \bar{\Qbb})(x_0,x_T) \geq
  \exp[-A(x_0) - A(x_T)]$ with $A \geq 0$ measurable,
  $\int_{\rset^d \times \rset^d} \exp[-B(x_0) - B(x_T)] \rmd \Qbb(x_0,x_T) <
  +\infty$ with $B \geq 0$ measurable.
  Assume that there exists
  $t_0 \in \ooint{0,T}$ and $\msx$ measurable such that $\Qbb_{t_0}(\msx) > 0$
  and for all $x \in \msx$,
  $\Qbb_{0,T} \ll \Qbb_{0,T|t_0}(\cdot|\bfX_{t_0} =x)$.  In addition, assume
  that $\KLLigne{\pi_0}{\bar{\Qbb}} <+\infty$,
  $\KLLigne{\pi_T}{\bar{\Qbb}} <+\infty$,
  $\int_{\rset^d} (A+B)(x_0) \rmd \pi_0(x_0) < +\infty$,
  $\int_{\rset^d} (A+B)(x_T) \rmd \pi_T(x_T) < +\infty$.
  
  Then
  there exists a unique Schr\"odinger Bridge $\PSB$. In addition let $\Pbb$ be a
  Markov measure in the reciprocal class of $\Qbb$ such that
  $\Pbb_0 = \pi_0$ and $\Pbb_T = \pi_T$. Assume that
  $\KLLigne{\Pbb}{\Qbb} < +\infty$.  Then $\Pbb$ is the unique  Schr\"odinger Bridge $\PSB$.
\end{proposition*}

\begin{proof}
  The first part of the proof is a consequence of \cite[Theorem
  2.12(a)]{leonard2014survey}. The second part is a consequence of \cite[Theorem
  2.12(b)]{leonard2014survey} and \cite[Theorem 2.14]{leonard2014reciprocal}.
\end{proof}


\subsection{Proof of \Cref{lemma:pythagorean_theorem}}

\begin{lemma*}
  Let $\Mbb \in \calM$ and $\Pi \in \calR(\Qbb)$ and assume \tref{assum:nice_diffusion},
  \tref{assum:good_function_palmowski}, \tref{assum:doob_h_defined}. If $\KLLigne{\Pi}{\Mbb}< +\infty$ and
  $\KLLigne{\projM(\Pi)}{\Mbb}< +\infty$ we have
  \begin{equation}
    \label{eq:pythagoran_one}
    \KLLigne{\Pi}{\Mbb} = \KLLigne{\Pi}{\projM(\Pi)} +  \KLLigne{\projM(\Pi)}{\Mbb} . 
  \end{equation}
  For any $\Pbb \in \pathmeas$, if $\KLLigne{\Pbb}{\Pi} < +\infty$, we have 
  \begin{equation}
    \label{eq:pythagoran_two}
    \KLLigne{\Pbb}{\Pi} = \KLLigne{\Pbb}{\projR{\Qbb}(\Pbb)} +  \KLLigne{\projR{\Qbb}(\Pbb)}{\Pi} . 
  \end{equation}  
\end{lemma*}


\begin{proof}
  We start with the proof of \eqref{eq:pythagoran_one}. Similarly to
  \Cref{prop:markovian-projection}, where we have
  $\Mbb \in \calM$ to ensure that we can apply \cite[Theorem
  2.3]{leonard2012girsanov}, we get
  \begin{equation}
    \textstyle \KL(\Pi | \Mbb) = \tfrac{1}{2} \int_0^T \expeLigne{\Pi_{0,t}}{\normLigne{v_t(\bfX_t) - \sigma_t^2 \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t}}^2} / \sigma_t^2 \rmd t .
  \end{equation}
  In addition, we have
  \begin{equation}
    \textstyle \KL(\projM(\Pi) | \Mbb) = \tfrac{1}{2} \int_0^T \expeLigne{\Pi_{t}}{\normLigne{v_t(\bfX_t) - \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t}}^2} / \sigma_t^2 \rmd t .
  \end{equation}
  Finally, using \Cref{prop:markovian-projection}, we have that
  \begin{align}
    & \textstyle \KLLigne{\Pi}{\projM(\Pi)} \\
    &  \textstyle = \tfrac{1}{2} \int_0^T \expeLigne{\Pi_{0,t}}{\normLigne{ \sigma_t^2 \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t} - \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_t} }^2} / \sigma_t^2 \rmd t  \\
    &  \textstyle =  \tfrac{1}{2} \int_0^T (\expeLigne{\Pi_{0,t}}{\normLigne{ \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t}}^2} - \expeLigne{\Pi_{t}}{\normLigne{ \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_t} }^2}) \sigma_t^2 \rmd t  .    
  \end{align}
  Using this result, we have
  \begin{align}
    &2 \textstyle \KLLigne{\Pi}{\projM(\Pi)} + 2\KLLigne{\projM(\Pi)}{\Mbb}  \\
    & = \textstyle \int_0^T (\expeLigne{\Pi_{0,t}}{\normLigne{ \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t}}^2} - \expeLigne{\Pi_{t}}{\normLigne{\CPELigne{\Pi_{T|t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_t} }^2}) \sigma_t^2 \rmd t \\
    & \textstyle + \int_0^T \expeLigne{\Pi_{t}}{\normLigne{v_t(\bfX_t) / \sigma_t^2 - \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t}}^2} \sigma_t^2 \rmd t \\
    & = \textstyle \int_0^T (\expeLigne{\Pi_{0,t}}{\normLigne{ \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t}}^2} - \expeLigne{\Pi_{t}}{\normLigne{\CPELigne{\Pi_{T|t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_t} }^2}) \sigma_t^2 \rmd t \\
    & \textstyle + \int_0^T (\expeLigne{\Pi_{t}}{\normLigne{v_t(\bfX_t) / \sigma_t^2}^2} + \expeLigne{\Pi_{t}}{\normLigne{\CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t}}^2}) \sigma_t^2 \rmd t \\
    & \textstyle - 2 \int_0^T \expeLigne{\Pi_{t}}{\langle v_t(\bfX_t) / \sigma_t^2, \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t} \rangle} \sigma_t^2 \rmd t  \\
 & = \textstyle \int_0^T \expeLigne{\Pi_{0,t}}{\normLigne{ \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t}}^2} \sigma_t^2 \rmd t  + \int_0^T \expeLigne{\Pi_{t}}{\normLigne{v_t(\bfX_t) / \sigma_t^2}^2} \sigma_t^2 \rmd t \\
    & \textstyle - 2 \int_0^T \expeLigne{\Pi_{t}}{\langle v_t(\bfX_t) / \sigma_t^2, \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t} \rangle} \sigma_t^2 \rmd t  \\
 & = \textstyle \int_0^T \expeLigne{\Pi_{0,t}}{\normLigne{ \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t}}^2} \sigma_t^2 \rmd t  + \int_0^T \expeLigne{\Pi_{t}}{\normLigne{v_t(\bfX_t) / \sigma_t^2}^2} \sigma_t^2 \rmd t \\
& \textstyle - 2 \int_0^T \expeLigne{\Pi_{0,t}}{\langle v_t(\bfX_t) / \sigma_t^2, \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t} \rangle} \sigma_t^2 \rmd t = 2 \KLLigne{\Pi}{\Mbb} ,       
  \end{align}
  which concludes the first part of the proof.

  For \eqref{eq:pythagoran_two}, define $\Pistar = \projR{\Qbb}(\Pbb) = \Pbb_{0,T} \Qbb_{|0,T}$. Using
  \cite[Equation 2.6]{csiszar1975divergence}, we have
  \begin{align}
    \KLLigne{\Pbb}{\Pi} &= \textstyle \KLLigne{\Pbb}{\Pistar} + \int_{\calC} \log(\tfrac{\rmd \Pistar}{\rmd \Pi}(\omega)) \rmd \Pbb(\omega) \\
    &\textstyle = \KLLigne{\Pbb}{\Pistar} + \int_{\rset^d \times \rset^d} \log(\tfrac{\rmd \Pistar_{0,T}}{\rmd \Pi_{0,T}}(x_0,x_1)) \rmd \Pbb_{0,T}(x_0,x_1) \\
    &\textstyle = \KLLigne{\Pbb}{\Pistar} + \int_{\rset^d \times \rset^d} \log(\tfrac{\rmd \Pistar_{0,T}}{\rmd \Pi_{0,T}}(x_0,x_1)) \rmd \Pistar_{0,T}(x_0,x_1)  = \KLLigne{\Pbb}{\Pistar} + \KLLigne{\Pistar}{\Pi} , 
  \end{align}
  which concludes the proof.
\end{proof}


\subsection{Proof of \Cref{prop:convergence_marginals}}

\begin{proposition*}
  Assume that the conditions of \Cref{prop:schro-markov-recip} and
  \Cref{lemma:pythagorean_theorem} apply for $\Pbb^n$ for every
  $n \in \nset$ and for the \schro Bridge $\PSB$,
  we have
  $\lim_{n \to +\infty} \KLLigne{\Pbb^n}{\Pbb^{n+1}} = 0$. 
  If $\Pstar$ is a fixed point of $(\Pbb^n)_{n \in \nset}$, then $\Pstar$
  is the Schr\"odinger Bridge.
\end{proposition*}

\begin{proof}
  The second part of the proof is a direct application of
  \Cref{prop:schro-markov-recip}, since in that case $\Pstar$ is Markov and
  reciprocal and admits $\pi_0,\pi_T$ as marginals. 
  For the first part of the proof, we follow the technique of
  \cite{ruschendorf1995convergence} but for the \emph{reverse} Kullback--Leibler
  divergence. Applying \Cref{lemma:pythagorean_theorem}, we get for any $N \in \nset$
  \begin{equation}
    \textstyle \KLLigne{\Pbb^0}{\Pstar} = \KLLigne{\Pbb^0}{\Pbb^1} + \KLLigne{\Pbb^1}{\Pstar} = \sum_{i=0}^{N} \KLLigne{\Pbb^i}{\Pbb^{i+1}} + \KLLigne{\Pbb^{N+1}}{\Pstar} ,
    \end{equation}
    which concludes the proof.
\end{proof}


\subsection{Proof of \Cref{prop:forward_backward_markov_proj}}
\begin{proof}
The proof is similar to the one of \Cref{prop:markovian-projection}.
\end{proof}

In particular, the time-reversal of $\Qbb_{|0,T}(\cdot|x_0,x_T)$ is associated with
\begin{equation}
  \rmd \bfY_t^{0,T} = \{-f_{T-t}(\bfY_t^{0,T}) + \sigma_{T-t}^2 \nabla \log \Qbb_{T-t|0}(\bfY_t^{0,T}|x_0)\} \rmd t + \sigma_{T-t} \rmd \bfB_t, \qquad \bfY_0^{0,T} = x_T.
  \label{eq:bridge_backward}
\end{equation}
One can view both \eqref{eq:forward_markov_proj} \eqref{eq:backward_markov_proj} as SDEs with drift defined as the conditional expectation of the drift of \eqref{eq:bridge_forward} \eqref{eq:bridge_backward} under $\Pi_{0,T|t}$ in the forward and backward directions respectively. 

\subsection{Proof of \Cref{prop:dsb_dsbm}}
\begin{proof}
We proceed by induction. 
Firstly, for $\Pi^0_{0,T}=\Q_{0,T}$, $\Pi^0=\tilde{\Pbb}^0=\Qbb$ at initialization. 
We can also define $\Mbb^0=\Qbb$, such that $\Mbb^0=\tilde{\Pbb}^0$ and $\Pi^n=\projR{\Qbb}(\Mbb^n)$ for all $n \in \nset$. 
By \cite[Section 3.5]{debortoli2021diffusion}, the optimal DSB sequence $\tilde{\Pbb}^n$ is Markov and $\tilde{\Pbb}^n = \tilde{\Pbb}_{0,T}^n \Qbridge$, where $\tilde{\Pbb}_{0,T}^n$ is the IPF sequence of the static SB problem. In other words, $\tilde{\Pbb}^n \in \calM \cap \calR(\Qbb) $. 

Suppose $\Mbb^{2n+1}=\tilde{\Pbb}^{2n+1}$. By definition, $\Mbb^{2n+2}_0=\tilde{\Pbb}^{2n+2}_0=\pi_0$, i.e. both forward processes are initialized at $\pi_0$. In DSB, by \cite{debortoli2021diffusion}, $\tilde{\Pbb}^{2n+2}$ is defined as the time-reversal of $\tilde{\Pbb}^{2n+1}$, such that $\tilde{\Pbb}^{2n+2}_{|0}=\tilde{\Pbb}^{2n+1}_{|0}$. Hence, $\tilde{\Pbb}^{2n+2} = \pi_0 \tilde{\Pbb}^{2n+1}_{|0}$. 

In DSBM, we first perform reciprocal projection $\Pi^{2n+1} = \projR{\Qbb}{(\Mbb^{2n+1})} = \Mbb_{0,T}^{2n+1} \Qbridge$. Since $\Mbb^{2n+1}=\tilde{\Pbb}^{2n+1} \in \calR(\Qbb)$, however, we have that $\Pi^{2n+1}=\Mbb^{2n+1}$. Furthermore, since $\Mbb^{2n+1}=\tilde{\Pbb}^{2n+1} \in \calM$, $\projM{(\Pi^{2n+1})}=\projM{(\Mbb^{2n+1})}=\Mbb^{2n+1}$. Thus, $\Mbb^{2n+2}$ given by \eqref{eq:approximate_markovian_proj_forward} is such that $\Mbb^{2n+2}_0=\pi_0$ and $\Mbb_{|0}^{2n+2}=\projM{(\Pi^{2n+1})}_{|0}=\Mbb_{|0}^{2n+1}$. We conclude that $\Mbb^{2n+2}= \pi_0 \Mbb_{|0}^{2n+1} = \tilde{\Pbb}^{2n+2}$. Similar arguments holds for the the reverse projection \eqref{eq:approximate_markovian_proj_backward}. Therefore, $\Mbb^n = \tilde{\Pbb}^n$ for all $n \in \nset$. 
\end{proof}



\subsection{The set of Markov measures is not convex}

The result of \Cref{lemma:pythagorean_theorem} should be compared with the information geometry result of
\cite[Theorem 2.2]{csiszar1975divergence}, which states that if $\calC$ is a convex set and $\Pbb \in \calC$, then under mild
conditions,
$\KLLigne{\Pbb}{\Qbb} = \KLLigne{\Pbb}{\proj_{\calC}(\Qbb)} +
\KLLigne{\proj_{\calC}(\Qbb)}{\Qbb}$, where
$\proj_{\calC}(\Qbb) = \argmin_{\Pbb} \ensembleLigne{\KLLigne{\Pbb}{\Qbb}}{\Pbb \in
  \calC}$ is the projection of $\Qbb$ on $\calC$. Note that, contrary to \Cref{lemma:pythagorean_theorem},
\cite[Theorem 2.2]{csiszar1975divergence} is given for the \emph{forward}
Kullback--Leibler divergence whereas \Cref{lemma:pythagorean_theorem} is given for the \emph{reverse} KL divergence. In addition, \cite[Theorem
2.2]{csiszar1975divergence} requires the projection set $\calC$ to be convex
which is not satisfied for the space of Markov measures $\calM$.
We give a simple
counter-example proving that the set of Markov measures is not convex.  

Let $p_1(x_0,x_1,x_2) = p_1(x_0)p_1(x_1|x_0)p_1(x_2|x_1)$ and
$p_2(x_0,x_1,x_2) = p_2(x_0)p_2(x_1|x_0)p_2(x_2|x_1)$ on $\{0,1\}^3$ such that
\begin{equation}
  p_1(x_0 = 1) = \alpha_0, \qquad p_1(x_1=1|x_0) = \alpha_1 , \qquad p_1(x_2=1|x_1) = \alpha_2 . 
\end{equation}
Additionally, we set 
\begin{equation}
  p_2(x_0 = 1) = \beta_0, \qquad p_2(x_1=1|x_0) = \beta_1 , \qquad p_2(x_2=1|x_1) = \beta_2 . 
\end{equation}
Finally, we set $q = (1/2)p_1 + (1/2)p_2$.  Consider
$q(x_2=1|x_1=1,x_0=1) = q(x_2=1,x_1=1,x_0=1)/q(x_1=1,x_0=1)$ and
$q(x_2=1|x_1=1) = q(x_2=1,x_1=1)/q(x_1=1)$. Let
\begin{align}
  \Delta &=  4[q(x_2=1,x_1=1,x_0=1)q(x_1=1) - q(x_2=1,x_1=1)q(x_1=1,x_0=1)] \\
         &= (\alpha_0\alpha_1\alpha_2 + \beta_0 \beta_1 \beta_2)(\alpha_1 + \beta_1) - (\alpha_1\alpha_2 + \beta_1\beta_2)(\alpha_0\alpha_1 + \beta_0\beta_1) \\
         &= \alpha_0 \alpha_1 \beta_1 \alpha_2 +\beta_0 \alpha_1 \beta_1 \beta_2 - \beta_0 \alpha_1 \beta_1 \alpha_2 - \alpha_0 \alpha_1 \beta_1 \beta_2 \\
         &=\alpha_1\beta_1 \beta_2(\beta_0 - \alpha_0) + \alpha_1\beta_1\alpha_2 (\alpha_0 - \beta_0) \\
  &=\alpha_1\beta_1(\beta_0 - \alpha_0)(\beta_2 - \alpha_2) .
\end{align}
$q$ is Markov if and only if $\Delta =0$. Therefore $q$ is not Markov as soon as
$\alpha_0 \neq \beta_0$ and $\alpha_2 \neq \beta_2$.

\section{Convergence of IMF in the Gaussian setting}
\label{sec:imf_gaussian_appendix}
In this section, we study the IMF in an one-dimensional Gaussian case. We consider $T=1$,
$\Pi_0 = \Pi_1 = \mathrm{N}(0, (1/2\beta^2))$ and $\Qbb$ associated with
$(\sigma \bfB_t)_{t \in \ccint{0,1}}$ where $\sigma >0$. In what follows, we let
\begin{equation}
  \label{eq:matrix_sigma_0}
  \Sigma^0 = (1/2\beta^2) \left(
      \begin{matrix}
        1 & c^2 \\
        c^2 & 1
      \end{matrix}
    \right) ,
  \end{equation}
  where $c \in \ccint{0,1}$.  We also denote
  $\bar{\sigma}^2 = 2 \sigma^2 \beta^2 $.  We start with the following result
  which gives an explicit expression of some marginals of the reciprocal
  projection. 

  \begin{lemma}
    \label{lemma:joint_gaussian}
    Let $\Pi_{0,1}^0 = \mathrm{N}(0, \Sigma^0)$ with $\Sigma^0$ given by
    \eqref{eq:matrix_sigma_0}. Let
    $\Pi^0 = \Pi_{0,1}^0 \Qbb_{|0,1}$. For any $t \in \ccint{0,1}$, we
    have that $\Pi_{0,1,t}^0 = \mathrm{N}(0, \Sigma)$ with
    \begin{equation}
      \Sigma = (1/2\beta^2) \left( \begin{matrix}
        1 & c^2 & a_k^t\\
                        c^2 & 1 & a_k^{1-t} \\
                        a_k^t & a_k^{1-t} & b_k^t 
      \end{matrix} \right) ,
  \end{equation}
  where we have
  \begin{equation}
    a_k^t = 1-t + tc^2 , \qquad b_k^t = 1 + t(1-t)(2(c^2-1)+\bar{\sigma}^2) .
  \end{equation}
\end{lemma}

\begin{proof}
  Let $t \in \ccint{0,1}$ and $\bfX_t^{0,1} \sim \Pi^0_{t|0,1} = \Qbb_{t|0,1}$.  Using
  \cite[Theorem 3.3]{barczy2013representations}, we get that
  \begin{equation}
    \bfX_t^{0,1} = (1-t)\bfX_0 + t \bfX_1 + \sigma (t (1-t))^{1/2} \bfZ , 
  \end{equation}
  with $\bfZ \sim \mathrm{N}(0, \Id)$ independent from $(\bfX_0, \bfX_1)$. Hence, we get that $\expeLigne{}{\bfX_t^{0,1}} = 0$, and 
  \begin{equation}
    \mathrm{Cov}(\bfX_0, \bfX_t^{0,1}) = \expeLigne{}{\bfX_t^{0,1} \bfX_0} = (1-t)/(2 \beta^2) + t c^2 / (2\beta^2) = a_k^t/ (2\beta^2). 
  \end{equation}
  Similarly, we get that $\mathrm{Cov}(\bfX_1, \bfX_t^{0,1}) = a_k^{1-t} / (2\beta^2)$. Finally, we get that
  \begin{align}
    \mathrm{Var}(\bfX_t^{0,1}) &= \expeLigne{}{((1-t)\bfX_0 + t \bfX_1)^2} + \sigma^2 t(1-t) \\
                         &= (1-t)^2/(2\beta^2) + 2t(1-t)c^2/(2\beta^2) + t^2 / (2\beta^2) + \bar{\sigma}^2t(1-t)/(2\beta^2) \\
                         &=  (1 - 2t + 2t^2 + 2t(1-t)c^2 + \bar{\sigma}^2t(1-t))/(2\beta^2) \\
    &=(1 + t(1-t)(2(c^2-1) + \bar{\sigma}^2)) / (2 \beta^2) ,
  \end{align}
  which concludes the proof.
\end{proof}

Leveraging \Cref{lemma:joint_gaussian}, we can give an explicit expression of
the drift term in the Markovian projection.

\begin{lemma}
  Let $\Pi_{0,1}^0 = \mathrm{N}(0, \Sigma^0)$ with $\Sigma^0$ given by
  \eqref{eq:matrix_sigma_0}. Let $\Pi^0 = \Pi_{0,1}^0 \Qbb_{|0,1}$.  For
  any $t \in \ccint{0,1}$ and $x_t \in \rset^d$, we have that
  \begin{equation}
    \sigma^2 \mathbb{E}_{\Pi_{1|t}^0}[\nabla \log \Qbb_{1|t} | \bfX_t = x_t] = \tfrac{(1-2t)(c^2-1) - \bar{\sigma}^2t}{1+t(1-t)(2(c^2-1) + \bar{\sigma}^2)} x_t . 
  \end{equation}
  Hence, the Markovian projection of $\Pi^0$, denoted $\Mbb^1$ is associated
  with $(\bfX_t)_{t \in \ccint{0,1}}$ with
  \begin{equation}
    \label{eq:sde_projection}
    \bfX_0 \sim \Pi_0, \qquad \rmd \bfX_t = \tfrac{(1-2t)(c^2-1) - \bar{\sigma}^2t}{1+t(1-t)(2(c^2-1) + \bar{\sigma}^2)} \bfX_t + \sigma \rmd \bfB_t . 
  \end{equation}
\end{lemma}

\begin{proof}
  Using \cite[Theorem 3.2]{barczy2013representations}, we get that $\Qbb_{|0,1}$
  is associated with
  \begin{equation}
    \label{eq:bridge_sde}
  \rmd \bfX_t^{0,1} = \sigma^2 \nabla \log \Qbb_{1|t}(x_1|\bfX_t) \rmd t + \sigma \rmd \bfB_t , \qquad \bfX_0^{0,1} = x_0 ,  
  \end{equation}
  where for any $t \in \coint{0,1}$, we have and $x_t \in \rset^d$,
  $\sigma^2 \nabla \log \Qbb_{1|t}(x_1|x_t) = (x_1 - x_t)/(1-t)$. Therefore, we
  get that $\Mbb^1$ is associated with $(\bfX_t)_{t \in \ccint{0,1}}$ such that
  \begin{equation}
    \label{eq:mixture_bridge}
  \rmd \bfX_t = \mathbb{E}_{\Pi^0_{1|t}}[\sigma^2 \nabla \log \Qbb_{1|t}(\bfX_1|\bfX_t)|\bfX_t] \rmd t + \sigma \rmd \bfB_t , \qquad \bfX_0 = x_0 .
\end{equation}
Therefore, we get that
  \begin{equation}
    \label{eq:mixture_bridge}
  \rmd \bfX_t = (\mathbb{E}_{\Pi^0_{1|t}}[\bfX_1|\bfX_t] - x_t)/(1-t) \rmd t + \sigma \rmd \bfB_t , \qquad \bfX_0 = x_0 .
\end{equation}
Using \Cref{lemma:joint_gaussian}, we have that for any $t \in \ccint{0,1}$ and
$x_t \in \rset^d$,
$\mathbb{E}_{\Pi^0_{1|t}}[\bfX_1|\bfX_t=x_t] = a_k^{1-t} / b_k^t x_t$.
In addition, we have for any $t \in \ccint{0,1}$
\begin{align}
  a_k^{1-t}/b_k^t - 1 &= (t + (1-t)c^2 - 1 - t(1-t)(2(c^2-1) + \bar{\sigma}^2)) / (1 + t(1-t)(2(c^2-1) + \bar{\sigma}^2)) \\
                      &= ((1-t)(c^2-1) - t(1-t)(2(c^2-1) + \bar{\sigma}^2)) / (1 + t(1-t)(2(c^2-1) + \bar{\sigma}^2)) \\
                      &= (1-t)((c^2-1) - t(2(c^2-1) + \bar{\sigma}^2)) / (1 + t(1-t)(2(c^2-1) + \bar{\sigma}^2)) \\
  &= (1-t)((1-2t)(c^2-1) - t\bar{\sigma}^2)) / (1 + t(1-t)(2(c^2-1) + \bar{\sigma}^2)) ,
\end{align}
which concludes the proof.
\end{proof}

Note that since $\sigma > 0$ and $c^2 \in \ccint{0,1}$, we get that for any
$t \in \ccint{0,1}$, $1 +t(1-t)(2(c^2-1) + \bar{\sigma}^2) > 0$ and therefore
the drift is well-defined, smooth and sublinear. In particular,
\eqref{eq:sde_projection} admits a unique strong solution.
In what follows, we denote $G : \ \ccint{0,1} \times \ccint{0,1} \to \rset$ given for any $t \in \ccint{0,1}$ by
\begin{equation}
\textstyle  G(t,c^2) = \int_0^t \tfrac{(1-2s)(c^2-1) - \bar{\sigma}^2s}{1+s(1-s)(2(c^2-1) + \bar{\sigma}^2)} \rmd s  .
\end{equation}

We have the following useful lemma.

\begin{lemma}
  Let $c \in \ccint{0,1}$, $\bar{\sigma} > 0$ and $p = 2c^2 + \bar{\sigma}^2$.
  We distinguish three cases:
  \begin{enumerate}[label=(\alph*), wide]
  \item If $p < 2$, then
    \begin{equation}
      G(1,c^2) = -\bar{\sigma}^2(4 - p^2)^{-1/2} \tan^{-1}((4-p^2)^{1/2}/p) . 
    \end{equation}
  \item If $p = 2$, then
    \begin{equation}
      G(1,c^2) = -\bar{\sigma}^2 / 2 . 
    \end{equation}
  \item If $p > 2$, then
    \begin{equation}
      G(1,c^2) = -\bar{\sigma}^2  (p^2 - 4)^{-1/2} \tanh^{-1}((p^2 - 4)^{1/2}/p) . 
    \end{equation}        
  \end{enumerate}
\end{lemma}

This lemma is useful combined with the following proposition, which gives the analytical update formula of Gaussian IMF covariances as a function of $G(1,c^2)$. 

\begin{proposition}
   \label{prop:imf_iterate_gaussian}
   Let $\Pi^0_{0,1} = \mathrm{N}(0, \Sigma^0)$ with $\Sigma^0$ given by
   \eqref{eq:matrix_sigma_0}. Then $\Pi^1_{0,1} = \mathrm{N}(0, \Sigma^1)$ with
   \begin{equation}
       \Sigma^1 = (1/2\beta^2) \left(
      \begin{matrix}
        1 & c_1^2 \\
        c_1^2 & 1
      \end{matrix}
    \right) , \qquad c_1^2 = f(c_0^2) , 
  \end{equation}
  with $f: \ \ccint{0,1} \to \ccint{0,1}$ given for any $c \in \ccint{0,1}$ by
  \begin{equation}
    f(c^2) = \exp[G(1,c^2)] . 
  \end{equation}
\end{proposition}

\begin{proof}
  We have that $\bfX_1 = \exp[G(1,c^2)] \bfX_0 + \bfM_1$, where $\bfM_1$ is a
  Gaussian random variable with zero mean independent from $\bfX_0$. Therefore,
  we get that $\mathrm{Cov}(\bfX_0, \bfX_1) = \exp[G(1,c^2)]/(2 \beta^2)$. In
  addition, we have that $\expeLigne{}{\bfX_1^2} = \expeLigne{}{\bfX_0^2} = 1/2\beta^2$,
  which concludes the proof. 
\end{proof}


Iterating the procedure in \Cref{prop:imf_iterate_gaussian}, we obtain a sequence of IMF covariances $(c_n^2)_{n \in \nset}$ satisfying $c_{n+1}^2=f(c_n^2)$. 
Finally, we show that this iterative procedure recovers the true SB coupling $\PiSB_{0,1}=\mathrm{N}(0, \Sigma^\textup{SB})$ as a fixed point. 
The formula of $\Sigma^\textup{SB}$ is given e.g.\ in  \cite[Equation (2)]{bunne2022gaussian} which we use below. 
\begin{proposition}
    Let $\PiSB_{0,1}=\mathrm{N}(0, \Sigma^\textup{SB})$ be the true static SB solution, with 
   \begin{equation}
       \Sigma^\textup{SB} = (1/2\beta^2) \left(
      \begin{matrix}
        1 & c_\textup{SB}^2 \\
        c_\textup{SB}^2 & 1
      \end{matrix}
    \right) , \qquad  c_\textup{SB}^2 = \frac{1}{2}(\sqrt{4+\bar{\sigma}^4} -\bar{\sigma}^2). 
  \end{equation}
  Then $\PiSB_{0,1}$ is a fixed point of the iterative procedure in \Cref{prop:imf_iterate_gaussian}, i.e. $f(c_\textup{SB}^2)=c_\textup{SB}^2$.
\end{proposition}
\begin{proof}
    By straightforward calculations, $p_\textup{SB}=2c_\textup{SB}^2 + \bar{\sigma}^2 = \sqrt{4+\bar{\sigma}^4}$. If $\bar{\sigma}=0$, $p_\textup{SB}=2$ and thus $G(1,c_\textup{SB}^2)= -\bar{\sigma}^2 / 2=0$. Hence $f(c_\textup{SB}^2) = \exp (G(1,c_\textup{SB}^2)) = 1 =c_\textup{SB}^2$. If $\bar{\sigma}>0$, we have $p_\textup{SB} > 2$ and $G(1,c_\textup{SB}^2)=-\tanh^{-1}(\bar{\sigma}^2/\sqrt{4+\bar{\sigma}^4})$. Hence, $f(c_\textup{SB}^2) = \exp (G(1,c_\textup{SB}^2)) = \frac{1}{2}(\sqrt{4+\bar{\sigma}^4} -\bar{\sigma}^2) =c_\textup{SB}^2$. We thus correctly recover the true static SB solution $\PiSB_{0,1}$ as fixed point of IMF. 
\end{proof}

We visualize the convergence of this fixed point procedure for a variety of parameter settings in \Cref{fig:analytic_convergence_appendix}. The convergence appears to be very fast in only two or three iterations. 

\begin{figure}[h]
    \centering
    \subfloat[$\bar{\sigma}=0.1, c_0^2=0$]{\includegraphics[width=0.23\linewidth]{Figs/analytic/analytic_0.1_0.pdf}}\hfill
    \subfloat[$\bar{\sigma}=1, c_0^2=0$]{\includegraphics[width=0.23\linewidth]{Figs/analytic/analytic_1_0.pdf}}\hfill
    \subfloat[$\bar{\sigma}=1, c_0^2=1$]{\includegraphics[width=0.23\linewidth]{Figs/analytic/analytic_1_1.pdf}}\hfill
    \subfloat[$\bar{\sigma}=5, c_0^2=1$]{\includegraphics[width=0.23\linewidth]{Figs/analytic/analytic_5_1.pdf}}
    \caption{Convergence of IMF in the analytic case given by \Cref{prop:imf_iterate_gaussian}. }
    \label{fig:analytic_convergence_appendix}
\end{figure}

\section{Discrete-Time Markovian Projection}
\label{sec:discrete_time_mp_appendix}
We derive in this section a discrete-time version of the Markovian
projection and show that, in some limiting case, we recover the continuous-time
projection. In the discrete case, we let \begin{align}
  \label{eq:def_q_discret}
  \textstyle
  \pi(x_{0:N}) &\textstyle=\pi(x_0, x_N) \prod_{k=0}^{N-1} q_{k+1|0,k,N}(x_{k+1}|x_0, x_k, x_N) \\
  &\textstyle= \pi(x_0, x_N)\prod_{k=0}^{N-2} q_{k+1|k,N}(x_{k+1}|x_k, x_N)  .
\end{align}
We consider a Markovian measure $p$ given by
$p(x_{0:N}) = p(x_0) \prod_{k=0}^{N-1} p_{k+1|k}(x_{k+1} | x_k)$.  Now let us
compute $\KLLigne{\pi}{p}$. We have
\begin{align}
  \KLLigne{\pi(x_{0:N})}{p(x_{0:N})} &= \textstyle \sum_{k=0}^{N-2} \int_{\rset^d \times \rset^d} \KLLigne{q_{k+1|k,N}}{p_{k+1|k}} \pi_{k,N}(x_k,x_N) \rmd x_k \rmd x_N \\
  & \qquad \textstyle + \KLLigne{\pi_0}{p_0} + \int_{\rset^d} \KLLigne{\pi(x_N|x_0)}{p(x_{N}|x_{N-1})} \pi(x_0, x_{N-1}) \rmd x_0 \rmd x_{N-1} .
\end{align}
In what follows, we denote
\begin{align}
  & \mathcal{L}_0 = \KLLigne{\pi_0}{p_0} , \textstyle  \mathcal{L}_{N} = \int_{\rset^d} \KLLigne{\pi(x_N|x_0)}{p(x_{N}|x_{N-1})} \pi(x_0, x_{N-1}) \rmd x_0 \rmd x_{N-1},  \\
  & \textstyle  \mathcal{L}_{k+1} = \int_{\rset^d \times \rset^d} \KLLigne{q_{k+1|k,N}}{p_{k+1|k}} \pi_{k,N}(x_k,x_N) \rmd x_k \rmd x_N , \end{align}
We have the following proposition.
\begin{proposition}
  The minimizer $p_{k+1|k}$ of $\mathcal{L}_{k+1}$ is given by
  \begin{equation}
    \label{eq:definition_discrete}
    \textstyle p_{k+1|k}(x_{k+1} | x_k) = \int_{\rset^d} q_{k+1|k,N}(x_{k+1}|x_k, x_N)  \pi_{N|k}(x_N|x_k) \rmd x_N .
  \end{equation}
  If $p_0 = q_0$, then  for any $k \in \{0, \dots, N-1\}$,
  $p_k = \pi_k$.  In addition, assume that
  $p_{k+1|k}(x_{k+1}|x_k) = \exp[-\normLigne{x_{k+1} - x_k - \gamma
    f(x_k)}^2/(2\gamma)] / (2 \uppi \gamma)^{-d/2}$ and
  $ q_{k+1|k,N}(x_{k+1}|x_k, x_N) = \exp[-\normLigne{x_{k+1} - x_k - \gamma
    f(x_k, x_N)}^2/(2\gamma)] / (2 \uppi \gamma)^{d/2}$.  Finally, assume that
  $\normLigne{x_{k+1} - x_k} \leq \gamma^{1/2}$.  Then, we have that
  \begin{equation}
    \label{eq:limit_continuous}
    \textstyle f(x_k) = \int_{\rset^d} f(x_k, x_N) \pi(x_N|x_k) \rmd x_N + o(\gamma^{1/2}).
  \end{equation}
\end{proposition}

\begin{proof}
  The proofs of \eqref{eq:definition_discrete} and \eqref{eq:limit_continuous}
  are straightforward and left to the reader. We now prove that if $p_0 = \pi_0$, then for any
  $k \in \{1, \dots, N\}$, $p_k = \pi_k$. First, we have that for any
  $k \in \{0, \dots, N-1\}$,
  \begin{equation}
    \pi(x_k, x_{k+1},x_N) = \pi(x_k,x_N)q(x_{k+1}|x_k,x_N) . 
  \end{equation}
  Assume now that $p_k=\pi_k$, then we have
  \begin{align}
    p_{k+1}(x_{k+1}) &\textstyle = \int_{\rset^d} p_k(x_k) p_{k+1|k}(x_{k+1}|x_k) \rmd x_k\\
     &= \textstyle \int_{\rset^d \times \rset^d} p_k(x_k) q_{k+1|k,N}(x_{k+1}|x_k,x_N) \pi_{N|k}(x_N|x_k) \rmd x_k \rmd x_N \\
     &= \textstyle \int_{\rset^d \times \rset^d} \pi_k(x_k) q_{k+1|k,N}(x_{k+1}|x_k,x_N) \pi_{N|k}(x_N|x_k) \rmd x_k \rmd x_N \\
      &= \textstyle \int_{\rset^d \times \rset^d} \pi_{k,k+1,N}(x_k,x_{k+1},x_N) \rmd x_k \rmd x_N = \pi_{k+1}(x_{k+1}),
  \end{align}
  which concludes the proof.
\end{proof}

In particular, in the previous proposition, if
$f(x_k, x_N) = \nabla \log q(x_N|x_k)$, i.e. we have a discretization of the
bridge then
$f(x_k) = \int_{\rset^d} \nabla \log q(x_N|x_K) \pi(x_N|x_k) \rmd x_N$, which
recovers the Markovian projection in continuous-time.

\section{Comparing DSBM-IPF and DSB}
\label{sec:dsbm_vs_dsb_appendix}
We analyze further the differences between DSBM-IPF proposed here and DSB proposed in \cite{debortoli2021diffusion} and related algorithms in \cite{vargas2021solving,chen2021likelihood}. All algorithms solve the SB problem using the IPF iterates. However, DSB-type algorithms solve for the IPF iterates using \emph{time-reversals}, whereas DSBM solves for the iterates using \emph{Markovian} and \emph{reciprocal projections}. A comparison between these two methodologies is made in \Cref{sec:pract-meth}. 

We investigate here further benefit (iii) of DSBM in \Cref{sec:pract-meth}, i.e. the benefit of explicitly projecting onto the reciprocal class of $\Qbb$. 
Intuitively speaking, we directly incorporate the reference measure $\Q$ in the training procedure as our inductive bias. 
More formally, suppose we have at the current IPF iteration $\Mbb^{2n}$ (Markov diffusion in the forward direction) and want to learn $\Mbb^{2n+1}$ (Markov diffusion in the backward direction). Due to training error and the \emph{forgetting} issue \citep{fernandes2021shooting}, however, $\Mbb^{2n}$ no longer has the correct bridge $\Qbridge$. 
Now suppose we first perform IMF for $\Mbb^{2n}$ and learn $\Mbb^{2n,\star}$ in the forward direction. That is to say, we repeat alternative reciprocal and Markovian projections and obtain a sequence $(\Mbb^{2n,m})_{m \in \nset}$ in the forward direction converging to $\Mbb^{2n,\star}$. Then $\Mbb^{2n,\star}$ now has the correct bridge $\Qbridge$ by \Cref{prop:convergence_marginals}, since $\Mbb^{2n,\star}$ is the SB between $\Mbb_0^{2n}$ and $\Mbb_T^{2n}$. Theoretically, $\Mbb_t^{2n}=\Mbb_t^{2n,\star}$ for $t=0,T$, but due to training error accumulating it may be that $\Mbb_T^{2n} \neq \Mbb_T^{2n,\star}$. However, $\Mbb_0^{2n}=\Mbb_0^{2n,\star}$, since $\Mbb^{2n,\star}$ is in the forward direction and starting from samples from $\pi_0$. As a result, we can obtain a Markov forward diffusion $\Mbb^{2n,\star}$, which has $\Mbb_0^{2n,\star}=\pi_0$ and the correct bridge $\Qbridge$. These are the same set of properties that the reference measure $\Q$ has. As a result, replacing $\Q$ with $\Mbb^{2n,\star}$ in \eqref{eq:schrodinger_bridge} results in the same SB solution. 
Consequently, now continuing the IPF iterations from $\Mbb_0^{2n,\star}$, it is as if we restart IPF afresh using a modified SB problem 
\begin{equation}
    \PSB = \argmin \ensembleLigne{\KLLigne{\Pbb}{\Mbb^{2n,\star}}}{\Pbb_0 = \pi_0, \ \Pbb_T = \pi_T}.\label{eq:SB_modified}
\end{equation} 
If $\Mbb^{2n,\star}$ is closer than $\Q$ to $\Mbb^{\star}$ in the sense of KL divergence, then we obtain a better initialization of the IPF procedure. 
As proposed in \Cref{alg:DSBM_general}, DSBM performs the Markovian and reciprocal projection only once before switching between the forward and backward directions. However, it is still beneficial compared to DSB with less bias accumulation in the bridge. 


Algorithmically, one main difference between DSB-type algorithms and DSBM due to the above distinction occurs in the trajectory caching step. In DSB, a fixed discretization of SDE needs to be chosen, and all intermediate samples from the discretized Euler-Maruyama simulation of the SDE need to be saved. Furthermore, a second set of drift evaluation needs to be performed for all datapoints in the trajectory  \citep[Equations (12), (13)]{debortoli2021diffusion}. The IPML algorithm in \cite{vargas2021solving} is also similar to DSB, but Gaussian processes are used to fit the drifts of forward and backward SDEs instead of neural networks. In \cite{chen2021likelihood}, the implicit score matching loss is used instead, but all intermediate points in the SDE trajectory also need to be saved. On the contrary, DSBM does not require intermediate samples during trajectory caching and only retains the joint samples at times $0,T$. Then, the intermediate trajectories are reconstructed using the reference bridge $\Qbb_{|0,T}$.


\section{Joint Training of Forward and Backward Processes}
\label{sec:joint-train-forw}

We recall below the DSBM algorithm given in \Cref{alg:DSBM_general}.

\begin{algorithm}[H]
\caption{Diffusion Schr\"odinger Bridge Matching}
\begin{algorithmic}[1]
\dsbmalgo
\end{algorithmic}
\end{algorithm}


Our main observation comes from \Cref{prop:forward_backward_markov_proj}. In
particular, under mild assumptions, we have that the Markovian projection
$\Mbb = \projM(\Pi)$ is associated with 
  \begin{align}
    &
    \rmd \bfX_t = \{f_t(\bfX_t) + \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t} \} \rmd t + \sigma_t \rmd \bfB_t , \quad \bfX_0 \sim \pi_0 ,  \\
    &
    \rmd \bfY_t = \{-f_{T-t}(\bfY_t) + \sigma_{T-t}^2 \CPELigne{\Pi_{0|T-t}}{\nabla \log \Qbb_{T-t|0}(\bfY_t|\bfY_T)}{\bfY_t} \} \rmd t + \sigma_{T-t} \rmd \bfB_t , \quad \bfY_0 \sim \pi_T . 
  \end{align}
Considering the following losses, 
  \begin{align}
    &\textstyle \theta^\star = \argmin_\theta \ensembleLigne{\int_0^T \expeLigne{\Pi_{t,T}}{\normLigne{\sigma_t^2 \nabla
    \log \Qbb_{T|t}(\bfX_T|\bfX_t) - v_\theta(t,\bfX_t)}^2}/\sigma_t^2 \rmd t }{\theta \in \Theta} , \label{eq:loss_forward_appendix} \\
    &\textstyle \phi^\star = \argmin_\phi \ensembleLigne{\int_0^T \expeLigne{\Pi_{t,0}}{\normLigne{\sigma_t^2 \nabla
    \log \Qbb_{t|0}(\bfX_t|\bfX_0) - v_\phi(t,\bfX_t)}^2} /\sigma_t^2 \rmd t }{\phi \in \Phi} . \label{eq:loss_backward_appendix}
  \end{align}
  If the families of functions $\ensembleLigne{v_\theta}{\theta \in \Theta}$ and
  $\ensembleLigne{v_\phi}{\theta \in \Phi}$ are rich enough, we have for any
  $t \in \ccint{0,T}$ and $x_t \in \rset^d$,
  $v_{\theta^\star}(t, x_t) = \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log
    \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t = x_t}$ and
  $v_{\phi^\star}(t, x_t) = \sigma_t^2 \CPELigne{\Pi_{0|t}}{\nabla \log
    \Qbb_{t|0}(\bfX_t|\bfX_0)}{\bfX_t = x_t}$. In practice, this means that the
  Markovian projection can be computed in a forward \emph{or} backward fashion
  equivalently.

  Therefore, given a coupling $\Pi = \Pi^{2n}$, we can update \emph{both} $v_\theta$
  and $v_\phi$. This means that we train the forward and backward model
  \emph{jointly}. We then consider $\Mbb^{2n+1}_b$ associated with
  \eqref{eq:approximate_markovian_proj_backward} and $\Mbb^{2n+1}_f$ associated
  with \eqref{eq:approximate_markovian_proj_forward}. Note that if the families
  of functions $\ensembleLigne{v_\theta}{\theta \in \Theta}$ and
  $\ensembleLigne{v_\phi}{\theta \in \Phi}$ are rich enough then
  $\Mbb^{2n+1}_f = \Mbb^{2n+1}_b$.


  \paragraph{Mixture from forward and backward.} Once we have obtained both the
  forward update and the backward update, our next task is to define the new
  mixture of bridge $\Pi^{2n+1}$. In \Cref{alg:DSBM_general}, since we train
  only the \emph{backward} model $\Mbb^{2n+1}=\Mbb^{2n+1}_b$, we define
  $\Pi^{2n+1} = \Mbb_{0,T}^{2n+1} \Qbb_{|0,T}$. In the case of
  \emph{joint training}, we have access to $\Mbb^{2n+1}_b$ and
  $\Mbb^{2n+1}_f$. One way to define a new mixture of bridge is to compute
  $\Pi^{2n+1} = \tfrac{1}{2}(\Mbb_{b, 0,T}^{2n+1} \Qbb_{|0,T} + 
  \Mbb_{f, 0,T}^{2n+1}) \Qbb_{|0,T}$. This choice ensures that in the case
  where $\Mbb^{2n+1}_f = \Mbb^{2n+1}_b$ we have
  \begin{equation}
    \textstyle \Pi^{2n+1} = \Mbb_{f, 0,T}^{2n+1} \Qbb_{|0,T} = \Mbb_{b, 0,T}^{2n+1}  \Qbb_{|0,T}.
  \end{equation}
  It also ensures that all the steps in the \emph{joint} DSBM training
  algorithms are symmetric. We leave the study of an optimal combination of
  $\Mbb_f^{2n+1}$ and $\Mbb_b^{2n+1}$ for future work.

  \paragraph{Consistency loss.} In addition to the losses
  \eqref{eq:loss_forward_appendix} and \eqref{eq:loss_backward_appendix}, we
  also consider an additional \emph{consistency} loss. A similar idea
  was explored in \cite{song2022applying}. In DSB
  \citep{debortoli2021diffusion,chen2021likelihood} and DSBM, see
  \Cref{alg:DSBM_general}, the processes parameterized by $v_\theta$ (forward)
  and $v_\phi$ backward are identical \emph{only at equilibrium}. Thus imposing the forward and the backward processes match at each step of DSB or DSBM would lead to some bias. However, this is not the case in the
  \emph{joint training} setting. Indeed, in that case, we have
  $\Mbb^{2n+1}_f = \Mbb^{2n+1}_b$ if the families are rich enough. Therefore, we get that
  \begin{equation}
    \label{eq:approximate_markovian_proj_backward_appendix}
    \rmd \bfY_t = \{ -f_{T-t}(\bfY_t) + v_{\phi}(T-t, \bfY_t) \} \rmd t + \sigma_{T-t} \rmd \bfB_t , \qquad \bfY_0 \sim \pi_T , 
  \end{equation}
  is the time reversal of 
  \begin{equation}
    \label{eq:approximate_markovian_proj_forward_appendix}
    \rmd \bfX_t = \{ f_t(\bfX_t) +  v_{\theta}(t, \bfX_t) \} \rmd t + \sigma_t \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 .
  \end{equation}
  Computing the time-reversal of \eqref{eq:approximate_markovian_proj_backward_appendix}, we have
  \begin{equation}
    \label{eq:approximate_markovian_proj_backward_backward_appendix}
    \rmd \bfX_t = \{ f_t(\bfX_t) -  v_{\phi}(t, \bfX_t) + \sigma_t^2 \nabla \log \Pi^{2n}_t(\bfX_t) \} \rmd t + \sigma_t \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 .
  \end{equation}
  Identifying \eqref{eq:approximate_markovian_proj_backward_backward_appendix} and \eqref{eq:approximate_markovian_proj_forward_appendix}, we get that for any $t \in \ccint{0,T}$ and $x_t \in \rset^d$
  \begin{equation}
    \label{eq:identity_score}
    v_\theta(t, x_t) = -  v_{\phi}(t, x_t) + \sigma_t^2 \nabla \log \Pi^{2n}_t(x_t) . 
  \end{equation}
  We highlight that letting $\sigma_t \to 0$ for any $t \in \ccint{0,T}$, we get
  that $v_\theta = -v_\phi$, which confirms that the time-reversal of an ODE is
  simply given by flipping the sign of the velocity. Therefore, we propose the
  following loss which links the parameters $\theta$ and $\phi$
  \begin{equation}
    \textstyle \mathcal{L}_{\mathrm{cons}}(\theta, \phi) = \int_0^T \expeLigne{\Pi_t^{2n}}{\normLigne{v_\theta(t, \bfX_t) +   v_{\phi}(t, \bfX_t) - \sigma_t^2 \nabla \log \Pi^{2n}_t(\bfX_t)}^2} /\sigma_t^2 \rmd t . 
  \end{equation}
  Leveraging tools from implicit score matching \citep{hyvarinen2005estimation} and the divergence theorem, we get that 
  \begin{equation}
    \label{eq:consistency_loss}
    \textstyle \mathcal{L}_{\mathrm{cons}}(\theta, \phi) = \int_0^T \expeLigne{\Pi_t^{2n}}{\normLigne{v_\theta(t, \bfX_t) +   v_{\phi}(t, \bfX_t)}^2 /\sigma_t^2  + 2 \mathrm{div}(v_\theta(t, \bfX_t) +   v_{\phi}(t, \bfX_t)) } \rmd t + C , 
  \end{equation}
  where $C \geq 0$ is a constant which does not depend on $\theta$ and $\phi$.
  Alternatively, \eqref{eq:identity_score} shows that 
    \begin{equation}
      \nabla \log \Pi_t^{2n}(x_t) = \E_{\Pi_{T|t}^{2n}}[\nabla \log \Qbb_{T|t}|\bfX_t = x_t] + \E_{\Pi_{0|t}^{2n}}[\nabla \log \Qbb_{t|0}|\bfX_t = x_t] . 
    \end{equation}
  We thus also have the following denoising score matching consistency loss
  \begin{equation}
    \label{eq:consistency_loss_dsm}
    \textstyle \mathcal{L}_{\mathrm{cons}}(\theta, \phi) = \int_0^T \expeLigne{\Pi_{0,t,T}^{2n}}{\normLigne{v_\theta(t,\bfX_t) + v_\phi(t,\bfX_t) - \sigma_t^2 ( \nabla
    \log \Qbb_{T|t}(\bfX_T|\bfX_t) + \nabla \log \Qbb_{t|0}(\bfX_t|\bfX_0))}^2} /\sigma_t^2 \rmd t  , 
  \end{equation}
  

  The advantage of this DSM loss is that it does not rely on any divergence computation.
  
  Below, we recall the two losses used to estimate the Markovian projection
  \eqref{eq:loss_forward_appendix} and \eqref{eq:loss_backward_appendix}
  \begin{align}    
        &\textstyle \mathcal{L}(\theta) = \int_0^T \expeLigne{\Pi_{t,T}}{\normLigne{\sigma_t^2 \nabla
    \log \Qbb_{T|t}(\bfX_T|\bfX_t) - v_\theta(t,\bfX_t)}^2} /\sigma_t^2 \rmd t  ,  \label{eq:loss_forward_appendix_comp} \\
    & \textstyle \mathcal{L}(\phi) = \int_0^T \expeLigne{\Pi_{t,0}}{\normLigne{\sigma_t^2 \nabla
    \log \Qbb_{t|0}(\bfX_t|\bfX_0) - v_\phi(t,\bfX_t)}^2} /\sigma_t^2 \rmd t  . \label{eq:loss_backward_appendix_comp}
  \end{align}
  The complete loss we consider in the joint training of the algorithm is of the form
  \begin{equation}
    \label{eq:total_loss}
    \mathcal{L}_\lambda(\theta, \phi) = \mathcal{L}(\theta) + \mathcal{L}(\phi) + \lambda \mathcal{L}_{\mathrm{cons}}(\theta, \phi) ,
  \end{equation}
  where $\lambda > 0$ is an additional regularization parameter. We now
  state a version of DSBM which performs \emph{joint training} in
  \Cref{alg:joint_training}. 

  \begin{algorithm}[H]
    \caption{Diffusion Schr\"odinger Bridge Matching (Joint Training)}
    \label{alg:joint_training}
\begin{algorithmic}[1]
\STATE{\textbf{Input:} Coupling $\Pi_{0,T}^0$, tractable bridge $\Qbb_{|0,T}$, $N \in \nset$}
\STATE{Let $\Pi^0 = \Pi_{0,T}^0 \Qbb_{|0,T}$.}
\FOR{$n \in \{0, \dots, N-1\}$}
\STATE Learn $v_{\phi^\star}, v_{\theta^\star}$ using \eqref{eq:total_loss} with $\Pi = \Pi^{n}$.
  \STATE Let $\Mbb^{n+1}_f$ be given by \eqref{eq:approximate_markovian_proj_forward}. 
  \STATE Let $\Mbb^{n+1}_b$ be given by \eqref{eq:approximate_markovian_proj_backward}. 
  \STATE Let $\Mbb^{n+1} = \tfrac{1}{2}(\Mbb^{n+1}_f + \Mbb^{n+1}_b)$. 
  \STATE Let $\Pi^{n+1} = \Mbb^{n+1}_{0,T} \Qbb_{|0,T} $. 
 \ENDFOR
  \STATE \textbf{Output:} $v_{\theta^\star}$, $v_{\phi^\star}$
\end{algorithmic}
\end{algorithm}

\section{Additional Methodological Details}
\label{sec:additional_details_appendix}
\subsection{Probability Flow ODE}
\label{subsec:prob_flow_ode_appendix}
At equilibrium of DSBM, we have that
$(\bfY_t)_{t \in \ccint{0,T}}$ given by
\eqref{eq:approximate_markovian_proj_backward} is the time reversal of
$(\bfX_t)_{t \in \ccint{0,T}}$ given by
\eqref{eq:approximate_markovian_proj_forward} and are both associated with the
Schr\"odinger Bridge path measure. As a result, we have that
$v_{\phi^\star}(t,x) = -v_{\theta^\star}(t,x) + \sigma_t^2 \nabla \log
\PSB_t(x)$. Hence, a probability flow $(\bfZ^\star_t)_{t \in \ccint{0,T}}$ such that
$\mathrm{Law}(\bfZ^\star_t) = \PSB_t$ for any $t \in \ccint{0,T}$ is given by
 \begin{equation}
   \label{eq:probability_flow}
   \rmd \bfZ^\star_t = \{f_t(\bfZ^\star_t) + \tfrac{1}{2}[v_{\theta^\star}(t, \bfZ^\star_t) - v_{\phi^\star}(t, \bfZ^\star_t)]\} \rmd t , \qquad \bfZ^\star_0 \sim \pi_0 . 
 \end{equation}
See also \cite{debortoli2021diffusion,chen2021likelihood} for derivation of this result. 
Note however that the path measure induced by $(\bfZ^\star_t)_{t \in \ccint{0,T}}$ does not correspond to $\PSB$; in particular,
$(\bfZ^\star_0, \bfZ^\star_T)$ is \emph{not} an entropic OT plan. However,
since for any $t \in \ccint{0,T}$, $\bfZ^\star_t$ has marginal distribution
$\PSB_t$, we
can compute the log-likelihood of the model
\citep{song2020score,huang2021variational}.

\subsection{Loss Scaling}
\label{subsec:loss_scaling_appendix}
Similar to the loss weighting in standard diffusion models \citep{song2020score,ho2020denoising}, we derive a similar weighting to reduce the variance of our objective. We focus on the forward direction of Markovian projection in this case, and the backward case can be derived similarly. 
Our forward loss in the DSBM framework is given by \eqref{eq:loss_function}, where the inner expectation is given by  
\begin{equation}
    \expeLigne{\Pi_{t,T}}{\normLigne{\sigma_t^2 \nabla
    \log \Qbb_{T|t}(\bfX_T|\bfX_t) - v_\theta(t,\bfX_t)}^2}. 
\end{equation}
Letting $\Qbb_{|0,T}$ be a Brownian bridge with diffusion parameter $\sigma$ and assuming $T=1$, this becomes
\begin{equation}
    \expeLigne{(\mathbf{X}_0, \mathbf{X}_1) \sim \Pi_{0,1}, \bfZ \sim \mathcal{N}(0, \Id)}{\normLigne{ \mathbf{X}_1 - \mathbf{X}_0 - \sigma \sqrt{t/(1-t)} \bfZ - v_\theta(t, \bfX_t^{0,T})}^2}
\end{equation}
with $\bfX_t^{0,T} = t \mathbf{X}_1 + (1-t) \mathbf{X}_0 + \sigma \sqrt{t(1-t)} \bfZ$. When $t \approx 1$, we see that the regression target is dominated by the noise term $\sigma \sqrt{t/(1-t)} \bfZ$ which needs to be predicted based on information contained within $\bfX_t^{0,T}$. 
The loss will have an approximate scale of $\sigma^2 t/(1-t)$ when $t\approx 1$ which will be very large. To avoid these large values affecting gradient descent, we can downweight the loss by $1 + \sigma^2 t/(1-t)$ (we can add $1$ to effectively cause no loss scaling when $t$ is close to $0$)
\begin{equation}
    (1 + \sigma^2 t/(1-t))^{-1} \expeLigne{(\mathbf{X}_0, \mathbf{X}_1) \sim \Pi_{0,1}, \bfZ \sim \mathcal{N}(0, \Id)}{\normLigne{ \mathbf{X}_1 - \mathbf{X}_0 - \sigma \sqrt{t/(1-t)} \bfZ - v_\theta(t, \bfX_t^{0,T})}^2}. 
\end{equation}
Similar arguments can be applied to the backward loss \eqref{eq:loss_function_backward}
\begin{align}
    &\expeLigne{\Pi_{t,0}}{\normLigne{\sigma_t^2 \nabla
    \log \Qbb_{t|0}(\bfX_t|\bfX_0) - v_\phi(t,\bfX_t)}^2}\\
    &=\expeLigne{(\mathbf{X}_0, \mathbf{X}_1) \sim \Pi_{0,1}, \bfZ \sim \mathcal{N}(0, \Id)}{\normLigne{ \mathbf{X}_0 - \mathbf{X}_1 - \sigma \sqrt{(1-t)/t} \bfZ - v_\phi(t, \bfX_t^{0,T})}^2} ,
\end{align}
which we then downweight by $1 + \sigma^2 (1-t)/t$
\begin{equation}
    (1 + \sigma^2 (1-t)/t)^{-1} \expeLigne{(\mathbf{X}_0, \mathbf{X}_1) \sim \Pi_{0,1}, \bfZ \sim \mathcal{N}(0, \Id)}{\normLigne{ \mathbf{X}_0 - \mathbf{X}_1 - \sigma \sqrt{(1-t)/t} \bfZ - v_\phi(t, \bfX_t^{0,T})}^2}.
\end{equation}





\section{Experiments}
\label{sec:exp_detail_appendix}
In this section, we present further details of the experiment setups as well as further experiment results. 
In all experiments, we use Brownian motion for the reference measure $\Qbb$ with corresponding Brownian bridge \eqref{eq:interpolation_stochastic} and $T=1$. 
We use the Adam optimizer with learning rate ${10}^{-4}$ and SiLU activations unless specified otherwise. The experiments are run on computing clusters with a mixture of both CPU and GPU resources. 

\subsection{2D Experiments}
For the 2D experiments, we closely follow \cite{tong2023conditional} and the released code at \url{https://github.com/atong01/conditional-flow-matching} (code released under MIT license), and use the same synthetic datasets and the 2-Wasserstein distance between the test set and samples simulated using probability flow ODE as the evaluation metric. However, we use 10000 samples in the test set since we find the 2-Wasserstein distance can vary greatly with only 1000 samples (which can be as high as 0.3 even between two set of samples both drawn from the ground truth distribution). We use a simple MLP with 3 hidden layers and 256 hidden units to parameterize the forward and backward drift networks. We use batch size 128 and 20 diffusion steps with uniform schedule at sampling time. Each outer iteration is trained for 10000 steps and we train for 20 outer iterations. For flow methods, we train for 200000 steps in total. For Table \ref{tab:2d_result} we use $\sigma_t=1$ in all cases, except the \textit{moons-8gaussians} dataset where we use $\sigma_t=5$.
Note that FM cannot be used for the \textit{moons-8gaussians} task since it requires a Gaussian source, but CFM is applicable. 
The experiments are run using 1 CPU and take approximately 200 minutes (for both training and testing). 

In \Cref{tab:2d_additional_wasserstein2}, we additionally compare to the SB-CFM method \citep{tong2023conditional} which utilizes mini-batch Sinkhorn solvers to define an approximate Schr\"odinger Bridge
static coupling $\tilde{\Pi}^\textup{SB}_{0,T} \approx \PiSB_{0,T}$. We can also utilize this idea in the DSBM-IMF framework, which corresponds to using the initialization coupling $\Pi_{0,T}^0=\tilde{\Pi}^\textup{SB}_{0,T}$ in \Cref{alg:DSBM_general}. This coupling also satisfies $\tilde{\Pi}^\textup{SB}_0=\pi_0$ and $\tilde{\Pi}^\textup{SB}_T=\pi_T$ but can provide a better initialization than the independent coupling $\Pi_{0,T}^0=\pi_0 \otimes \pi_T$. We name this approach DSBM-IMF+. As the initial coupling is more optimal in DSBM-IMF+, we reduce the number of outer iterations to 4 and train for 50000 steps in each outer iteration. The results in \Cref{tab:2d_additional_wasserstein2} show that DSBM-IMF+ can achieve lower sampling error than DSBM-IPF and DSBM-IMF. It also performs better than SB-CFM on 3 of the datasets and achieve lower path energy on all datasets. 

\subsubsection{Variance of the reference measure $\mathbb{Q}$}
We comment further on the effect of $\sigma_t$ in the reference path measure $\mathbb{Q}$. We assume a time-homogeneous $\sigma_t=\sigma$ for simplicity. In \Cref{fig:2d_sigma_paths}, we vary $\sigma$ and visualize the learned transport for a \textit{3gaussians} problem of transporting between two Gaussian mixtures. In Table \ref{tab:2d_sigma_metrics_wasserstein2} we show the 2-Wasserstein distance between the test set and generated samples for this \textit{3gaussians} problem as well as the \textit{moons-8gaussians} problem. We find that large values of $\sigma$ result in increasingly curved transport paths, and correspondingly reduced performance when $\sigma$ is excessively large. Conversely, we also find reduced performance when $\sigma$ is excessively small. 
We conjecture this is due to increased optimization difficulty and bias accumulation. 
Firstly, the EOT problem becomes more difficult to solve as $\sigma$ is taken to $0$, which would require a higher number of outer iterations.
Further, the introduction of noise also decreases optimization difficulty by smoothing the intermediate marginals between the two terminal distributions. The benefit of setting $\sigma >0$ and using a stochastic sampler was also observed in \cite{albergo2023stochastic,delbracio2023inversion}. Finally, we conjecture setting $\sigma >0$ could also increase the diversity of sampled couplings and may alleviate some bias accumulation issues in the outer iterations. When $\sigma = 0$, these issues result in the artifacts observed in the transferred samples (marked using yellow points) in \Cref{fig:2d_sigma_paths}. The appropriate value for $\sigma$ depends on the spatial scaling of the problem as shown in Table \ref{tab:2d_sigma_metrics_wasserstein2}, where the optimum $\sigma$ is larger for the larger scale \textit{moons-8gaussians} problem. 



\begin{table}
    \centering
    \setlength{\tabcolsep}{5pt}
    \scalebox{0.64}{
    \begin{tabular}{ccccc}
    \toprule 
     & \multicolumn{4}{c}{\textit{2-Wasserstein (Euler 20 steps)}}\tabularnewline
    \cmidrule{2-5} \cmidrule{3-5} \cmidrule{4-5} \cmidrule{5-5} 
    \textit{Dataset} & moons & scurve & 8gaussians & moons-8gaussians\tabularnewline
    \midrule
    \midrule 
    DSBM-IPF & 0.140\textpm 0.006 & 0.140\textpm 0.024 & 0.315\textpm 0.079 & \textit{0.812\textpm 0.092}\tabularnewline
    \midrule 
    DSBM-IMF & 0.144\textpm 0.024 & 0.145\textpm 0.037 & 0.338\textpm 0.091 & 0.838\textpm 0.098\tabularnewline
    \midrule 
    DSB & 0.190\textpm 0.049 & 0.272\textpm 0.065 & 0.411\textpm 0.084 & 0.987\textpm 0.324\tabularnewline
    \midrule 
    \midrule 
    SB-CFM & \textit{0.129\textpm 0.024} & \textit{0.136\textpm 0.030} & \textbf{0.238\textpm 0.044} & 0.843\textpm 0.079\tabularnewline
    \midrule 
    DSBM-IMF+ & \textbf{0.123\textpm 0.014} & \textbf{0.130\textpm 0.025} & \textit{0.276\textpm 0.030} & \textbf{0.802\textpm 0.172}\tabularnewline
    \bottomrule
    \end{tabular}
    }
    \scalebox{0.64}{
    \begin{tabular}{ccccc}
    \toprule 
    \multicolumn{4}{c}{\textit{Path energy}}\tabularnewline
    \midrule
    moons & scurve & 8gaussians & moons-8gaussians\tabularnewline
    \midrule
    \midrule 
    1.598\textpm 0.034 & \textit{2.110\textpm 0.059} & 14.91\textpm 0.310 & 42.16\textpm 1.026\tabularnewline
    \midrule 
    \textbf{1.580\textpm 0.036} & \textbf{2.092\textpm 0.053} & \textbf{14.81\textpm 0.255} & \textbf{41.00\textpm 1.495}\tabularnewline
    \midrule 
    - & - & - & - \tabularnewline
    \midrule 
    \midrule 
    1.649\textpm 0.035 & 2.144\textpm 0.044 & 15.08\textpm 0.209 & 45.69\textpm 0.661 \tabularnewline
    \midrule 
    \textit{1.594\textpm 0.043} & 2.116\textpm 0.018 & \textit{14.88\textpm 0.252} & \textit{41.09\textpm 1.206} \tabularnewline
    \bottomrule
    \end{tabular}
    }
    \vspace{0.25cm}
    \caption{Sampling quality as measured by 2-Wasserstein distance and path energy for all SB methods in the 2D experiments. $\pm1$ SD over 5 seeds. Best values are in bold and second best are italicized. We additionally test SB-CFM and DSBM-IMF+ (both use mini-batch Sinkhorn solvers). }
    \label{tab:2d_additional_wasserstein2}


    \centering
    \scalebox{0.7}{
    \begin{tabular}{cccc}
    \toprule 
    \multicolumn{2}{c}{3gaussians} & \multicolumn{2}{c}{moons-8gaussians} \tabularnewline
    \midrule
    $\sigma$ & \textit{2-Wasserstein} & $\sigma$ & \textit{2-Wasserstein} \tabularnewline
    \midrule
    \midrule 
    $\sigma=0$ & 0.646\textpm 0.028 & $\sigma=0$ & 1.459\textpm 0.008\tabularnewline
    \midrule 
    $\sigma=0.1$ & 0.724\textpm 0.039  & $\sigma=1.0$ & 1.285\textpm 0.346 \tabularnewline
    \midrule 
    $\sigma=0.3$ & 0.546\textpm 0.169 & $\sigma=2.0$ & \emph{0.916\textpm 0.292}   \tabularnewline
    \midrule 
    $\sigma=1.0$ & \textbf{0.439\textpm 0.072} & $\sigma=4.0$ & \textbf{0.818\textpm 0.249} \tabularnewline
    \midrule
    $\sigma=3.0$ & \emph{0.543\textpm 0.078}  & $\sigma=8.0$ & 0.989\textpm 0.179  \tabularnewline
    \bottomrule
    \end{tabular}
    }
    \vspace{0.25cm}
    \caption{2-Wasserstein distance for varying value of $\sigma$ used in the DSBM-IMF method.}
    \label{tab:2d_sigma_metrics_wasserstein2}
    \vspace{-0.6cm}
\end{table}


\subsection{Gaussian Experiment}
Similar to the 2D experiments, we use a simple MLP with 2 hidden layers and 256 hidden units to parameterize the forward and backward drift networks. This is a smaller network compared to the ``large'' network in \cite{debortoli2021diffusion}. We use batch size 128 and 20 diffusion sampling steps with uniform time schedule at inference time. Each outer iteration is trained for 10000 steps and we train for 20 outer iterations. The experiments are also run using 1 CPU, and for the case $d=50$ finish in approximately 200 minutes. We note that DSB, IMF-b, and DSBM methods all solve for the \schro Bridge transport, whereas Rectified Flow does not and so is not plotted on the covariance convergence plot in \Cref{fig:gaussian_ipf_convergence} since it is not comparable. 

For \Cref{tab:gaussian_avg_kl}, we assume the marginals of the learned process $\P_t$ are also independently Gaussian distributed in each dimension. We thus estimate the KL divergence from the sample mean and variance of each dimension of $\P_t$ using the analytic KL formula between Gaussian distributions. 


\subsection{MNIST Transfer Experiment} \label{subsec:mnist_appendix}
We follow \cite{debortoli2021diffusion} closely for the setup of this experiment. We use the set of first 5 letters of EMNIST (A-E, a-e) such that both domains have 10 classes in total. We use the same U-Net architecture, batch size 128 and 30 diffusion sampling steps. The network size is approximately 6.6 million parameters. Each outer iteration is trained for 5000 steps. We refresh the cache dataloader 
every 1000 steps with 10000 new samples. 
Contrary to \cite{debortoli2021diffusion}, in our experiments we find that we obtain better sampling quality for both DSB and DSBM using a uniform noising schedule. We simply choose $\sigma_t=1$ for all $t$ and $T=1$ in our experiments. 

For DSBM-IPF, we train for at most 50 outer iterations (i.e. 250000 total number of steps). For DSBM-IMF and Rectified Flow, since their first iteration corresponds to Bridge Matching and Flow Matching respectively, we first pretrain the forward and backward networks for 100000 steps using the Bridge Matching or Flow Matching losses. DSBM-IMF then switches to iterative training with 5000 steps per outer iteration, the same as DSBM-IPF. For Rectified Flow, we train for 50000 steps per outer iteration. The experiments are performed using 2 GPUs and take approximately one day. 


We provide further experiment results in Figures \ref{fig:mnist_backward_samples_comparison}, \ref{fig:mnist_backward_trajectory} and \ref{fig:mnist_forward_trajectory}. 
In \Cref{fig:mnist_backward_samples_comparison}, we show samples generated using different algorithms and at different points of convergence. Samples generated using CFM and Rectified Flow with 2 rectified steps in (a) and (b) appear to be less clear and identifiable. OT-CFM improves upon CFM slightly in (c), but many samples still appear to be unclear. 
For DSB, the algorithm has not converged after 10 iterations, and many samples in (d) still appear to be letters. After 20 iterations, there are still letter-like samples in \Cref{fig:mnist_backward_samples_dsb20}, and the digit classes also appear to be unbalanced with many instances of digit `0'. After 30 iterations, however, the sample quality of DSB becomes very poor in (e). On the other hand, as shown in (f)-(j), we observe that DSBM-IPF converges faster than DSB, with more accurate samples even in iterations 10 and 20, and the sample quality continues to improve until the end of training after 50 iterations. 

We present some additional trajectory samples at the end of training in Figures \ref{fig:mnist_backward_trajectory} and \ref{fig:mnist_forward_trajectory} in both the forward and backward directions. We observe DSBM is able to transfer samples across the two domains faithfully, and the output samples preserve interesting similarities compared to the input, whereas Bridge Matching preserves much less similarity. The Mean Squared Distance (MSD) between the initial and final samples also confirm that DSBM methods transfer more closely to the original inputs, and DSBM-IMF further achieves the best FID score out of the three methods. 

\begin{figure}
    \centering

    \subfloat[CFM]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/CFM/CFM250000b.png} 
    }
    \subfloat[Rectified Flow @ 3]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/RF/2RF.png} 
    \label{fig:mnist_backward_samples_rf_appendix}
    }
    \subfloat[OT-CFM]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/CFM/OTCFM250000b.png} 
    } 
    \subfloat[DSB @ 10]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/DSB/DSB10b.png} 
    }
    \subfloat[DSB @ 30]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/DSB/DSB30b.png} 
    }


    \subfloat[DSBM-IPF @ 10]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/DSBM/DSBM10b.png} 
    }
    \subfloat[DSBM-IPF @ 20]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/DSBM/DSBM20b.png} 
    }
    \subfloat[DSBM-IPF @ 30]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/DSBM/DSBM30b.png} 
    }
    \subfloat[DSBM-IPF @ 40]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/DSBM/DSBM40b.png} 
    }
    \subfloat[DSBM-IPF @ 50]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/DSBM/DSBM50b.png} 
    }
    \caption{Samples of MNIST digits transferred from the EMNIST letters using different methods. @ indicates the progressed number of outer iterations $n+1$. }
    \label{fig:mnist_backward_samples_comparison}
    \vspace{-0.2cm}
\end{figure}


\begin{figure}
    \centering
    \subfloat[Bridge Matching]{
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/BM/b/im_grid_0.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/BM/b/im_grid_10.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/BM/b/im_grid_20.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/BM/b/im_grid_30.png} 
    \hfill
    \raisebox{0.1\linewidth}{
    \small
    \begin{tabular}{c|c}
        FID & 17.14 \\
        MSD & 0.579
    \end{tabular}
    }
    }
    
    \subfloat[DSBM-IPF]{
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM/b/im_grid_0.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM/b/im_grid_10.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM/b/im_grid_20.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM/b/im_grid_30.png} 
    \hfill
    \raisebox{0.1\linewidth}{
    \small
    \begin{tabular}{c|c}
        FID & 15.27 \\
        MSD & \textbf{0.354}
    \end{tabular}
    }
    }
    
    \subfloat[DSBM-IMF]{
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM-IMF/b/im_grid_0.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM-IMF/b/im_grid_10.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM-IMF/b/im_grid_20.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM-IMF/b/im_grid_30.png} 
    \hfill
    \raisebox{0.1\linewidth}{
    \small
    \begin{tabular}{c|c}
        FID & \textbf{10.59} \\
        MSD & 0.375
    \end{tabular}
    }
    }
    \caption{Left: EMNIST to MNIST sample trajectory with 30 diffusion steps at $t=0,1/3,2/3,1$. Right: FID score of final samples, and Mean Squared Distance between initial and final samples. }
    
    \label{fig:mnist_backward_trajectory}

    \centering
    \subfloat[Bridge Matching]{
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/BM/f/im_grid_0.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/BM/f/im_grid_10.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/BM/f/im_grid_20.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/BM/f/im_grid_30.png} 
    \hfill
    \raisebox{0.1\linewidth}{
    \small
    \begin{tabular}{c|c}
        FID & 9.402 \\
        MSD & 0.561
    \end{tabular}
    }
    }
    
    \subfloat[DSBM-IPF]{
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM/f/im_grid_0.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM/f/im_grid_10.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM/f/im_grid_20.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM/f/im_grid_30.png} 
    \hfill
    \raisebox{0.1\linewidth}{
    \small
    \begin{tabular}{c|c}
        FID & 13.59 \\
        MSD & \textbf{0.359}
    \end{tabular}
    }
    }
    
    \subfloat[DSBM-IMF]{
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM-IMF/f/im_grid_0.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM-IMF/f/im_grid_10.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM-IMF/f/im_grid_20.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM-IMF/f/im_grid_30.png} 
    \hfill
    \raisebox{0.1\linewidth}{
    \small
    \begin{tabular}{c|c}
        FID & \textbf{8.990} \\
        MSD & 0.375
    \end{tabular}
    }
    }
    \caption{Left: MNIST to EMNIST sample trajectory with 30 diffusion steps at $t=0,1/3,2/3,1$. Right: FID score of final samples, and Mean Squared Distance between initial and final samples. }
    \label{fig:mnist_forward_trajectory}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.48\linewidth]{img_celeba/init_128_male.png}  \hfill
  \includegraphics[width=0.48\linewidth]{img_celeba/generated_128_female.png} 
  \vspace{.3cm}
  \includegraphics[width=0.48\linewidth]{img_celeba/init_128_female.png}  \hfill
  \includegraphics[width=0.48\linewidth]{img_celeba/generated_128_male.png}  
  \caption{Transfer results between images given by the tokens   \texttt{female}/\texttt{young} and \texttt{male}/\texttt{old}. Top row: original images (left) and generated images (right). Bottom row: original images (left) and generated images (right).}
  \label{fig:celeba_128}
\end{figure}

\subsection{CelebA Transfer Experiment}
For CelebA \citep{liu2015faceattributes}, we test DSBM on resized $64\times64$
and $128\times128$ resolution images.  We evaluate the dependency of the results
with respect to $\sigma >0$ on CelebA $64 \times 64$. We also showcase the
scalibility of our method by training DSBM on CelebA $128 \times 128$. In both
cases, the dataset is split between \texttt{male/old} and
\texttt{female/young}. We gather 20000 samples of each class on which we perform
classical data augmentation such as horizontal flipping. 

For our ablation study w.r.t. the value of $\sigma > 0$, we run DSBM for 20
iterations (note that the loss and the quality was still improving after 20
DSBM iterations but by stopping the runs early we were allowed to draw
comparisons with more values of $\sigma$). 
We use a U-Net architecture with 4 resolution levels and 2 residual blocks per resolution level. The batch size is fixed to 64 and the
EMA rate is fixed to 0.999. 
We refresh the cache dataloader 
every 500 steps with 10000 new samples. 
The SDE sampler is chosen to be the modified Euler-Maruyama
sampler, see \cite{heng2021simulating} for instance, with a  constant schedule for
the stepsizes. We use 100 sampling steps
at inference time and to refresh the cache. For each outer DSBM iteration we
train the model for 20000 iterations. 


We provide additional transfer results in resolution $128\times128$ in
\Cref{fig:celeba_128}. We do not change the training setting for this
experiment. 


\subsection{CIFAR-10 Generative Modeling Experiment}
\label{subsec:cifar10_appendix}
We also test our method in the standard generative modeling framework on the CIFAR-10 dataset. We again use a U-Net architecture with 4 resolution levels and 2 residual blocks per resolution level. The network size is approximately 39.6 million parameters. The batch size is fixed to 128 and the
EMA rate is fixed to 0.9999. The AdamW optimizer is used for this task. 
For DSBM-IMF and Rectified Flow, again we first pretrain the networks using the Bridge Matching or Flow Matching losses, then switch to DSBM-IMF or RF training with 100000 steps per outer iteration. We find that 1 or 2 additional outer iterations appear sufficiently effective on this task, and additional outer iterations can cause sample quality to drop. For our main experiment, the pretraining stage ran for approximately 6 days, and DSBM-IMF ran for approximately 4 additional days using 4 V100 GPUs. 

The best results during training for each method are reported in \Cref{tab:fid_cifar10_appendix} and \Cref{fig:fid_nfe_cifar10_appendix}, where we compute the FID score between 50000 samples in the CIFAR-10 training set and 50000 generated samples following standard practice for this task. We observe that DSBM-IMF can clearly improve upon Bridge Matching at the same value of $\sigma$, which suggests that further outer iterations in DSBM is beneficial for improving sample quality. This is contrary to Rectified Flow, which causes the FID score to worsen compared to Flow Matching after only 1 rectified iteration. However, as $\sigma$ increases, we observe the FID score worsens for both Bridge Matching and DSBM as more stochasticity is introduced in the sampler. The best result of DSBM-IMF is obtained using $\sigma^2=0.2$ and is slightly better than FM (i.e. with $\sigma^2=0$) using 100 Euler steps. On the other hand, using the dopri5 ODE solver, FM achieves a FID of 4.055 with on average 148 integration steps. 
In \Cref{fig:fid_nfe_cifar10_appendix}, we observe that both RF and DSBM-IMF are very effective in improving sampling quality at low number of diffusion steps, i.e. low number of function evaluations (NFEs), compared to Bridge and Flow Matching as well as OT-CFM which improves upon CFM slightly. DSBM-IMF also achieves lower FID score than RF as the NFEs are taken higher. 
Additional strategies such as distillation and fast SDE solvers can also be useful for improving few-step sampling quality further. 


\begin{figure}
    \centering
    \begin{minipage}[b]{.47\linewidth}
        \centering
        \scalebox{0.8}{
        \begin{tabular}{ccc}
            \toprule
            $\sigma^2$ & FM & RF \\
            \midrule
            0 & 4.931 & 6.010 \\
            \midrule
            \midrule
            $\sigma^2$  & BM & DSBM-IMF \\
            \midrule
            0.2 & 5.427 & \textbf{4.511} \\
            \midrule
            0.5 & 8.274 & 6.896 \\
            \midrule
            1 & 12.749 & 9.881 \\
            \bottomrule
        \end{tabular}
        }
    
        \captionof{table}{FID results on the CIFAR-10 train set using 100 Euler(-Maruyama) steps.}
        \label{tab:fid_cifar10_appendix}
    \end{minipage}
    \qquad
    \begin{minipage}[b]{.42\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{Figs/CIFAR10/cifar10_fid.pdf}
        \caption{FID vs number of diffusion steps (NFE) on CIFAR-10. }
        \label{fig:fid_nfe_cifar10_appendix}
    \end{minipage}
    \vspace{-0.4cm}
\end{figure}

\subsection{Fluid Flows Experiment}
We use the fluid flows dataset from \cite{bischoff2023unpaired} available at \url{https://github.com/CliMA/CliMADatasets.jl} (code released under MIT license). The dataset consists of unpaired low ($64 \times 64$) and high ($512 \times 512$) resolution fields, as well as a context field with local information for the high resolution field. The data fields consist of two channels representing supersaturation and vorticity. The context field is dependent on the wavenumber $k_x=k_y \in \{1,2,4,8,16\}$, which specifies the frequency of the saturation specific humidity modulation. We follow \cite{bischoff2023unpaired} for data processing and network architecture, which is given by a U-Net with an additional spatial mean bypass network given by an MLP. 
The network size is approximately 11.3 million parameters in total. We train using batch size 4, learning rate $2 \times {10}^{-4}$, for 5000 steps per outer iteration and $N=12$ outer iterations. 
We refresh the cache dataloader 
every 2500 steps with 1250 new samples. 
The training takes approximately 20 hours on a single RTX GPU. We used $\sigma^2=0.3$ and sample using 30 diffusion steps without finetuning these parameters. 
For the Diffusion-fb method in \cite{bischoff2023unpaired}, we use the released code available at \url{https://github.com/CliMA/diffusion-bridge-downscaling} (code released under Apache-2.0 license) without modifying any parameters. 

We visualize intermediate and final reconstruction samples for different algorithms in \Cref{fig:downscaler_vis_others}. We see that DSBM-IPF and DSBM-IMF provide consistent samples with the low resolution source, whereas  Diffusion-fb and Bridge Matching produce dissimilar samples. 
We also follow \cite{bischoff2023unpaired} for a more refined statistical analysis in Figures \ref{fig:downscaler_pixel_kde_appendix}, \ref{fig:downscaler_spectra_appendix}, \ref{fig:downscaler_mean_kde_appendix}. 
DSBM-IMF achieves comparable performance as Diffusion-fb in terms of these statistical profiles, and can be comparatively more accurate e.g.\ in the tails of the distributions in \Cref{fig:downscaler_pixel_kde_appendix}, and for the case $k_x=k_y=4$ in \Cref{fig:downscaler_spectra_appendix} for which the power spectrum of supersaturation is correctly captured by DSBM-IMF but not by other methods. Comparing this analysis with \Cref{fig:downscaler_l2}, DSBM-IMF is also significantly more accurate in terms of conditional consistency than Diffusion-fb. On the other hand, DSBM-IPF appears less accurate in terms of these unconditional statistics than Diffusion-fb and DSBM-IMF, but achieves lower $\ell_2$ distances from the input sources in \Cref{fig:downscaler_l2}. This suggests that DSBM-IPF and DSBM-IMF exhibit different empirical biases before convergence, and DSBM-IMF is more preferable when the accuracy of the samples are important. This is in line with IMF theory as the marginals $\pi_0, \pi_T$ are preserved in IMF but not in IPF. 


\begin{figure}
    \centering
    \subfloat[Low-res]{
    \includegraphics[width=1.4cm]{Figs/Downscaler/k=2/dsbm-ipf/clip_im_grid_start.png}
    }
    \subfloat[Diffusion-fb]{
    \includegraphics[width=1.4cm]{Figs/Downscaler/k=2/diffusion-fb/clip_init_x_reverse_2.0_1.png}
    \includegraphics[width=1.4cm]{Figs/Downscaler/k=2/diffusion-fb/clip_target_samples_2.0_1.png} 
    }
    \subfloat[Bridge Matching]{
    \includegraphics[width=1.4cm]{Figs/Downscaler/k=2/bm/clip_im_grid_15.png}
    \includegraphics[width=1.4cm]{Figs/Downscaler/k=2/bm/clip_im_grid_last.png} 
    }
    \subfloat[DSBM-IPF]{
    \includegraphics[width=1.4cm]{Figs/Downscaler/k=2/dsbm-ipf/clip_im_grid_15.png}
    \includegraphics[width=1.4cm]{Figs/Downscaler/k=2/dsbm-ipf/clip_im_grid_last.png} 
    }
    \subfloat[DSBM-IMF]{
    \includegraphics[width=1.4cm]{Figs/Downscaler/k=2/dsbm-imf/clip_im_grid_15.png}
    \includegraphics[width=1.4cm]{Figs/Downscaler/k=2/dsbm-imf/clip_im_grid_last.png} 
    }
    \caption{(a) Source low resolution sample; (b)(c)(d)(e) intermediate state and final reconstruction of each algorithm, for wavenumber $k_x=k_y=2$. }
    \label{fig:downscaler_vis_others}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figs/Downscaler/kdes_ch1.pdf}

    \includegraphics[width=\linewidth]{Figs/Downscaler/kdes_ch2.pdf}
    \caption{KDE estimates of values in supersaturation and vorticity fields.}
    \label{fig:downscaler_pixel_kde_appendix}

    \centering
    \includegraphics[width=\linewidth]{Figs/Downscaler/spectra_ch1.pdf}

    \includegraphics[width=\linewidth]{Figs/Downscaler/spectra_ch2.pdf}
    \caption{Spectral density estimates of supersaturation and vorticity fields.}
    \label{fig:downscaler_spectra_appendix}

    \centering
    
    \includegraphics[width=\linewidth]{Figs/Downscaler/mean_kdes_ch1.pdf}

    \caption{KDE estimates of spatial means of the supersaturation field. The shaded areas denote 99\% confidence interval obtained using 10000 bootstrap samples. }
    \label{fig:downscaler_mean_kde_appendix}

    
\end{figure}

\section{Broader Impact}
\label{sec:broader_impact_appendix}
Our work focuses on theoretical and methodological research and is intended to bring closer the fields of generative modeling and optimal transport. 
It can be useful for learning transport maps between general distributions with high accuracy and high scalability, which can have useful applications in machine learning, but also natural science areas such as physics, biology and geosciences in which optimal transport maps with theoretical guarantees are appealing. Our fluid flows experiment demonstrates such potentials. However, as is the case for generative models as a whole, intentional malicious use could cause detrimental societal impacts. 


\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
