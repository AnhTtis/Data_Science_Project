\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
% \usepackage{neurips_2022}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}
    

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}
\hypersetup{
  colorlinks,
  linkcolor={red!50!black},
  citecolor={blue!50!black},
%   urlcolor={blue!80!black}
  urlcolor={blue!50!black}
}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{tikz}
\usetikzlibrary{shapes,snakes,backgrounds,arrows}

% PERSONAL PACKAGES
\usepackage{upgreek}
\usepackage{pbox}
\usepackage[inline]{enumitem}
\usepackage{algorithm,algorithmic}
% \usepackage[ruled]{algorithm2e}
\usepackage{xspace}
\usepackage{amsthm,amsmath}
\usepackage{graphicx}
% \usepackage{ulem}
\usepackage{aliascnt}
\usepackage{cleveref}
\usepackage{autonum}
\usepackage{wrapfig}
\usepackage{subfig}

% MACROS
\input{def}

\renewcommand{\arraystretch}{1.2} % stretching the row height

\newcommand{\dsbmalgo}{
\STATE{\textbf{Input:} Joint distribution $\Pi_{0,T}^0$, tractable bridge $\Qbb_{|0,T}$, number of outer iterations $N \in \nset$.}
\STATE{Let $\Pi^0 = \int \Qbb_{|0,T} \rmd \Pi_{0,T}^0$.}
\FOR{$n \in \{0, \dots, N-1\}$}
  \STATE Learn $v_{\phi^\star}$ using \eqref{eq:loss_function_backward} with $\Pi = \Pi^{2n}$. 
  \STATE Let $\Mbb^{2n+1}$ be given by \eqref{eq:approximate_markovian_proj_backward}. 
  \STATE Let $\Pi^{2n+1} = \int \Qbb_{|0,T} \rmd \Mbb^{2n+1}_{0,T}$.
  \STATE Learn $v_{\theta^\star}$ using \eqref{eq:loss_function} with $\Pi = \Pi^{2n+1}$. 
  \STATE Let $\Mbb^{2n+2}$ be given by \eqref{eq:approximate_markovian_proj_forward}. 
  \STATE Let $\Pi^{2n+2} = \int \Qbb_{|0,T} \rmd \Mbb^{2n+2}_{0,T}$.
  \ENDFOR
  \STATE \textbf{Output:} $v_{\theta^\star}$, $v_{\phi^\star}$
}


\title{Diffusion Schr\"odinger Bridge Matching}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
    Yuyang Shi\thanks{Equal contribution.} \\
    University of Oxford
\And
    Valentin De Bortoli\footnotemark[1] \\
    ENS ULM
\And
    Andrew Campbell \\
    University of Oxford
\And
    Arnaud Doucet \\
    University of Oxford
}

\makeatother
\begin{document}

\maketitle

\begin{abstract}
  Solving transport problems, i.e. finding a map transporting one given
  distribution to another, has numerous applications in machine learning. Novel
  mass transport methods motivated by generative modeling have recently been
  proposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models
  (FMMs) implement such a transport through a Stochastic Differential Equation
  (SDE) or an Ordinary Differential Equation (ODE). However, while it is
  desirable in many applications to approximate the deterministic dynamic
  Optimal Transport (OT) map which admits attractive properties, DDMs and FMMs
  are not guaranteed to provide transports close to the OT map. In contrast,
  Schr\"odinger bridges (SBs) compute stochastic dynamic mappings which recover
  entropy-regularized versions of OT. Unfortunately, existing numerical methods approximating SBs either scale poorly with dimension or accumulate errors across iterations. In this work, we introduce Iterative Markovian
  Fitting, a new methodology for solving SB problems, and Diffusion
  Schr\"odinger Bridge Matching (DSBM), a novel numerical algorithm for
  computing IMF iterates.  DSBM significantly improves over previous SB numerics
  and recovers as special/limiting cases various recent transport methods. We
  demonstrate the performance of DSBM on a variety of problems.
\end{abstract}


\section{Introduction}
Mass transport problems are ubiquitous in machine learning
\citep{peyre2019computational} with applications ranging from biology
\citep{bunne2022proximal} to shape correspondence \citep{feydy2017optimal}. For
discrete measures, the Optimal Transport (OT) map can be computed exactly but is too
computationally intensive. In a landmark paper, \cite{cuturi2013sinkhorn} has
shown that an entropy-regularized version of OT can be computed efficiently
using the Sinkhorn algorithm \citep{sinkhorn1967diagonal}. This has enabled the 
implementation of OT techniques in a wide range of applications; see
e.g. \cite{benamou2015iterative,courty2017joint,caron2020unsupervised}. However,
while these techniques have been successful, they do not provide
state-of-the-art performance for applications where high dimensional and/or
continuous distributions are involved such as generative modeling.

Generative modeling is indeed a central machine learning transport problem which
requires designing a deterministic or stochastic mapping transporting a
reference ``noise'' distribution to the data distribution. For example,
Generative Adversarial Networks \citep{goodfellow2014generative} define a static
deterministic transport map while Denoising Diffusion Models (DDMs)
\citep{song2020score,ho2020denoising} build a dynamic stochastic transport map
by simulating a Stochastic Differential Equation (SDE) whose drift is learned
using score matching techniques 
\citep{hyvarinen2005estimation,vincent2011connection}. This denoising diffusion
is the time-reversal of a forward ``noising'' diffusion initialized at the data
distribution which transports the data approximately to a unit Gaussian
distribution. The popularity and excellent performance of DDMs have motivated
the development of novel Bridge Matching and Flow Matching Models which are
dynamic transport techniques building stochastic maps using SDEs
\citep{song2020denoising,peluchettinon,liu2022rectified,albergo2023stochastic}
or deterministic maps using ODEs
\citep{albergo2022building,heitz2023iterative,lipman2022flow,liu2022flow} whose
drifts are approximated by solving a simple regression problem. These techniques
not only do not require introducing a forward ``noising'' diffusion converging
to a reference distribution, but are also more generally applicable than DDMs as
they can approximate a transport map between any two distributions one can
sample from by constructing ``bridges'' between them. Nonetheless, these dynamic
transport maps are not necessarily close to the dynamic Optimal Transport (OT)
map minimizing the Wasserstein-2 metric between the two distributions of
interest, which is appealing for its many attractive theoretical properties
\citep{peyre2019computational,villani2009optimal}.

Unfortunately the dynamic OT map is intractable in realistic scenarios and, to
the best of our knowledge, there is no available numerical techniques scaling to
high dimensions. In contrast, the Schr\"odinger Bridge (SB) problem is a dynamic
version of an entropy-regularized version of OT
\citep{follmer1988random,leonard2014survey}. 
The SB is the finite-time diffusion which is
the closest in Kullback--Leibler divergence to a reference diffusion and admits
as initial and terminal distributions the two distributions of interest. Numerous methods to approximate SBs numerically have been proposed, see e.g. \citep{bernton2019schr,chen2016entropic,finlay2020learning,caluya2021wasserstein,pavon2018data}, but these techniques tend to be restricted to low-dimensional settings. Recently novel techniques based on Iterative 
Proportional Fitting (IPF)
\citep{fortet1940resolution,kullback1968probability,ruschendorf1993note}, a continuous state-space extension of the Sinkhorn algorithm
 \citep{essid2019traversing}, have been proposed in \citep{debortoli2021diffusion,vargas2021solving,chen2021likelihood}. In these contributions, the IPF iterates are approximated using diffusion-based ideas. These approaches scale better empirically than previous techniques but numerical errors tend to accumulate over
iterations \citep{fernandes2021shooting}. 

\begin{figure}
\centering
\begin{minipage}[b]{0.32\textwidth}
    \scalebox{0.7}{
    \begin{tikzpicture}
        \node [draw, rounded corners=25pt, very thick, olive, minimum width=6cm, minimum height=3.6cm, align=center] at (1,0.3){};
        
        \node [olive, align=center] at (-1.1,1.6){\textit{\textbf{DSBM}}};
        
        \node [draw, ellipse, very thick, purple, minimum width=5cm, minimum height=2cm, align=center] at (1,0.2){Bridge Matching\\\\\\\\};
        \node [draw, ellipse, very thick, blue, minimum width=2cm, minimum height=1cm, align=center] at   (0,0)   {Denoising \\Diffusion};
        \node [draw, ellipse, very thick, red, minimum width=2cm, minimum height=1cm, align=center] at    (2,0)   {Flow \\ Matching};
    \end{tikzpicture}
    }
    \caption{Relationship between DSBM and existing methods. }
    \label{fig:venn_diagram}
\end{minipage}
\hfill
\begin{minipage}[b]{0.67\textwidth} 
  \begin{tabular}[c]{c||c|c}
    & Sets for alternating projections & Preserved properties \\
    \hline \hline 
    IPF & $\Pbb_0 = \pi_0$; $\Pbb_T = \pi_T$  & $\calM$, $\calR(\Qbb)$   \\
    \hline
    IMF &$\calM$; $\calR(\Qbb)$ & $\Pbb_0 = \pi_0$, $\Pbb_T = \pi_T$  \\ 
    \hline 
  \end{tabular}
  
  \captionof{table}{Comparison between Iterative Markovian Fitting (IMF) and
    Iterative Proportional Fitting (IPF). The \schro Bridge is the unique $\Pbb$
    s.t. $\Pbb_0 = \pi_0$, $\Pbb_T = \pi_T$, $\Pbb\in\calM$,
    $\Pbb\in\calR(\Qbb)$ simultaneously by
    \Cref{prop:schro-markov-recip}. $\calM$ is the space of (regular)
    Markov measures and $\calR(\Qbb)$ the
    space of reciprocal measures of $\Qbb$.
    }
  \label{tab:diff}
\end{minipage}
\end{figure}


In this paper, our contributions are three-fold. First, we introduce Iterative
Markovian Fitting (IMF), a new procedure to compute SBs which alternates between
projecting on the space of Markov processes and projecting on the \emph{reciprocal class} of
the reference measure, i.e. the measures which
have the same bridge as the reference measure of SB  \citep{leonard2014reciprocal}. 
Contrary to IPF, the IMF iterates always preserve the initial and terminal distributions.  The relationship between IPF and IMF is
presented in \Cref{tab:diff}. We establish various theoretical results for
IMF. 
Second, we propose Diffusion Schr\"odinger Bridge Matching (DSBM), a novel
algorithm approximating numerically 
the SB solution derived from IMF. 
This algorithm requires at
each iteration to solve a simple regression problem in the spirit of
\citep{peluchettinon,liu2022rectified,albergo2022building,lipman2022flow,liu2022flow} and does not suffer from the time-discretization and the
``forgetting'' issues of previous techniques
\citep{debortoli2021diffusion,vargas2021solving,chen2021likelihood}. We show that DSBM also sheds light on various recent dynamic deterministic and stochastic transport algorithms that it recovers as special/limiting cases. Finally,
 we demonstrate the performance of DSBM on a variety of transport tasks.
 
 \textbf{Notations.} We denote by $\pathmeas$, the space of \emph{path
   measures}, i.e. $\pathmeas=\mathcal{P}(\rmC(\ccint{0,T}, \rset^d))$ where
 $T > 0$. 
 The subset of \emph{Markov} path measures associated
 with an SDE of the form
 $\rmd \bfX_t = v_t(\bfX_t) \rmd t + \sigma_t \rmd \bfB_t$, with $\sigma,v$
 locally Lipschitz, is denoted $\calM$.  For any
 $\Qbb \in \calM$, the \emph{reciprocal class}  of $\Qbb$
 is denoted $\calR(\Qbb)$, see
 \Cref{def:reciprocal_projection}. We also denote $\Qbb_t$ its marginal
 distribution at time $t$, $\Qbb_{s,t}$ the joint distribution at times $s$ and
 $t$ and $\Qbb_{s|t}$ the conditional distribution at time $s$ given state at
 time $t$. Unless specified otherwise, all gradient operators $\nabla$ are
 w.r.t. the variable $x_t$ with time index $t$.


\section{Dynamic Mass Transport Techniques}

We begin by presenting Denoising Diffusion Models and Bridge Matching approaches. We then turn to their links with the Schr\"odinger Bridge problem. 

\subsection{Denoising Diffusion and Bridge Matching}

Denoising Diffusion Models \citep{song2020score,ho2020denoising} are a popular
class of generative models. We start with a forward noising process $\Qbb \in \calM$
associated with the SDE,
$\rmd \bfX_t = - \tfrac{1}{2} \bfX_t \rmd t + \rmd \bfB_t$, on the time-interval
$[0,T]$, where $\bfX_0 \in \rset^d$ is drawn from the data distribution $\pi_0$
and $(\bfB_t)_{t\in[0,T]}$ is a multivariate standard Brownian motion. This
diffusion, known as the Ornstein-Uhlenbeck (OU) process\footnote{This is
  referred to as variance preserving SDE (VPSDE) in \cite{song2020score}. For ease of presentation, we omit the time rescaling.},
converges towards the standard multivariate Gaussian distribution
$\mathrm{N}(0, \Id)$ as $T\to\infty$. A generative model is given by the
\textit{time-reversal} $(\bfY_t)_{t\in[0,T]}=(\bfX_{T-t})_{t\in[0,T]}$, where
$\bfY_0 \sim \Qbb_T$ and
$\rmd \bfY_t = \{\tfrac{1}{2} \bfY_t + \nabla \log \Qbb_{T-t}(\bfY_t)\} \rmd t +
\rmd \bfB_t$
\citep{anderson1982reverse,haussmann1986time} for
$ \Qbb_{t}=\textup{Law}(\bfX_t)$.  In practice, $(\bfY_t)_{t \in \ccint{0,T}}$
is initialized with $\bfY_0 \sim \pi_T = \mathrm{N}(0, \Id)$ and the \emph{Stein
  score}
$\nabla \log \Qbb_t(x_t)=\CPELigne{\Qbb_{0|t}}{\nabla \log
  \Qbb_{t|0}(\bfX_{t}|\bfX_0)}{\bfX_{t}=x_t}$ is approximated by a neural
network $s_\theta(t,x_t)$ whose parameters are obtained by minimizing the
regression objective
$\expeLigne{\Qbb_{0,t}}{\normLigne{ \nabla \log \Qbb_{t|0}(\bfX_{t}|\bfX_0) -
    s_\theta(t,\bfX_t)}^2}$, see  \cite{vincent2011connection}.  

An alternative to considering the time-reversal of a forward noising process is to
build bridges between the two boundary distributions and learn a \emph{mimicking} diffusion process. This approach generalizes DDMs and allows for more
flexible choices of sampling processes. We call this framework \emph{Bridge Matching} and adopt a presentation similar to 
\cite{peluchettinon,liu2022let}, where $\pi_T$ is the data distribution\footnote{To keep notations consistent with existing works, $\pi_0$ is the data distribution in the context of DDM and SB, whereas $\pi_T$ is the data distribution in FMM. However, both SB and FMM allow transfer between general distributions $\pi_0,\pi_T$, so this distinction is not important.}. 
We denote $\Qbb \in \calM$ the distribution of the following process
\begin{equation}
   \rmd \bfX_t = f_t(\bfX_t) \rmd t + \sigma_t \rmd \bfB_t, \qquad \bfX_0 \sim \Qbb_0.
   \label{eq:reference_measure}
\end{equation}
Consider now the law of this process pinned down at an initial and terminal
point $x_0,x_T$, denoted $\Qbb_{|0,T}(\cdot|x_0,x_T)$. Under mild assumptions,
the \emph{conditioned} process $\Qbb_{|0,T}(\cdot|x_0,x_T)$ is associated with
the SDE
\begin{equation}
  \rmd \bfX_t^{0,T} = \{f_t(\bfX_t^{0,T}) + \sigma_t^2 \nabla \log \Qbb_{T|t}(x_T|\bfX_t^{0,T})\} \rmd t + \sigma_t \rmd \bfB_t, \qquad \bfX_0^{0,T} = x_0,
  \label{eq:bridge_forward}
\end{equation}
which satisfies $\bfX_T^{0,T}=x_T$ by 
using results from the Doob $h$-transform theory
\citep{rogers2000diffusions}. Next we define an independent coupling
$\Pi_{0,T}=\pi_0 \otimes \pi_T$ between $\pi_0,\pi_T$, and let
$\Pi = \int \Qbb_{|0,T} \rmd \Pi_{0,T}$. This path measure $\Pi$ is a
\textit{mixture of bridges}, where the bridge measure $\Qbb_{|0,T}$ is
integrated at its end points over the coupling $\Pi_{0,T}$.  Note that while the
bridge $\Qbb_{|0,T}$ is Markov, the mixture $\Pi$ is usually not a Markov
process.  We aim to find a Markov diffusion $(\bfY_t)_{t \in \ccint{0,T}}$
defined by
$\rmd \bfY_t = \{f_t(\bfY_t) + v_t(\bfY_t)\}\rmd t + \sigma_t \rmd \bfB_t$,
$\bfY_0 \sim \pi_0$, which admits the same marginals as $\Pi$; i.e. for any
$t \in \ccint{0,T}$, $\bfY_t \sim \Pi_{t}$, and in particular
$\bfY_T \sim \pi_T$.  For such a velocity field $v_t$,
a generative model for sampling data distribution $\pi_T$ is given by
\begin{enumerate*}[label=(\roman*)]
\item sampling $\bfY_0 \sim \pi_0 = \mathrm{N}(0, \Id)$, 
\item simulating $\rmd \bfY_t = \{f_{t}(\bfY_t) + v_t(\bfY_t)\} \rmd t + \sigma_t \rmd \bfB_t$.
\end{enumerate*}
It can be shown that indeed $\bfY_t \sim \Pi_{t}$ holds true for $v^\star_t(x_t) = \sigma_t^2 \CPELigne{\Pi_{T|t}}{ \nabla \log \Qbb_{T|t}(\bfX_{T}|\bfX_t)}{\bfX_{t}=x_t}$.
We present the theory behind this idea more formally using Markovian projections
in \Cref{sec:markovian_projection}. In practice, we do not have access to
$v^\star_t$ and it is learned using neural networks with a regression loss
\begin{equation}
    \label{eq:bridge_matching_loss}
    \expeLigne{\Pi_{t,T}}{\normLigne{\sigma_t^2 \nabla \log \Qbb_{T|t}(\bfX_{T}|\bfX_t) - v_\theta(t,\bfX_t)}^2}.
\end{equation}
In the case where $f_t = 0$ and $\sigma_t=\sigma$, $\Qbb_{|0,T}$ is a \emph{Brownian Bridge} and we have
\begin{equation}
  \label{eq:interpolation_stochastic}
  \bfX_t^{0,T} = \tfrac{t}{T} x_T + (1- \tfrac{t}{T}) x_0 + \sigma_t (\bfB_t - \tfrac{t}{T}\bfB_T) , \quad \rmd \bfX_t^{0,T} = \{(x_T - \bfX_t^{0,T})/(T-t)\} \rmd t + \sigma_t \rmd \bfB_t,
\end{equation}
with $(\bfB_t - \tfrac{t}{T}\bfB_T) \sim \mathrm{N}(0, t(1 - \tfrac{t}{T})~\Id)$. The regression loss associated with \eqref{eq:interpolation_stochastic} is given by 
\begin{equation}
    \label{eq:bridge_matching_loss_brownian}
    \expeLigne{\Pi_{t,T}}{\normLigne{(\bfX_T - \bfX_t)/(T-t) - v_\theta(t,\bfX_t)}^2}.
\end{equation}
Letting $\sigma \to 0$, we recover Flow Matching Models\footnote{We clarify this
  further as well as the relationship between different Flow Matching Models in
  \Cref{sec:fm_relationship_appendix}. }. 
  More generally, while the Markovian projection
introduced in \Cref{sec:iterative-markovian-fitting} and the new DSBM
methodology presented in \Cref{sec:pract-meth} are stated in a \emph{stochastic}
context, since they rely on stochastic tools such as the Girsanov
theorem, deterministic counterparts can be obtained by letting $\sigma \to 0$.


\subsection{Optimal Transport and Schr\"odinger Bridges}
The static optimal transport (OT) map is defined as the coupling between $\pi_0$ and $\pi_T$ minimizing the Wasserstein-2 metric, i.e.
\begin{equation}
\Pi^{\textup{OT}}_{0,T}=\argmin_{\Pi_{0,T}} \ensembleLigne{\mathbb{E}_{\Pi_{0,T}}[||\bfX_0-\bfX_T||^2]}{\Pi_0=\pi_0, \ \Pi_T=\pi_T},
\end{equation}
i.e. it is the probability coupling with marginals $\pi_0$ and $\pi_T$ minimizing the expected square Euclidean distance between $\bfX_0$ and $\bfX_T$. A famous result by \cite{benamou2000computational} is that, under mild assumptions on $\pi_0, \pi_T$, the OT map can be obtained by solving a \emph{dynamic} transport problem
\begin{equation}\label{eq:BenamouBrenierOT}
  \textstyle
v_{\textup{OT}}=\argmin_{v} \ensembleLigne{\int^T_0 \mathbb{E}_{\Pbb_t}[||v(t,\bfX_t)||^2]\rmd t}{\rmd \bfX_t=v(t,\bfX_t)\rmd t,~\Pbb_0=\pi_0,~ \Pbb_T=\pi_T} ,
\end{equation}
for $\Pbb_t=\textup{Law}(\bfX_t)$; i.e. $v_{\textup{OT}}$ is the velocity field minimizing the time integral of the kinetic energy among all vector fields such that the ODE initialized at $\bfX_0 \sim \pi_0$ yields $\bfX_T \sim \pi_T$. The OT map is then obtained by sampling $\bfX_0 \sim \pi_0$ followed by $\rmd \bfX_t=v_{\textup{OT}}(t,\bfX_t)\rmd t$. The joint samples at the initial and final times satisfy $(\bfX_0,\bfX_T) \sim \Pi^{\textup{OT}}_{0,T}$.

  
 The Schr\"odinger Bridge (SB) problem \citep{schrodinger1932theorie} is a dynamic
version of an Entropy-regularized Optimal Transport (EOT) problem
\citep{mikami2008optimal,leonard2014survey,chen2020optimal}. It consists in finding a path measure $\PSB \in \pathmeas$
such that
\begin{equation}
  \PSB = \argmin_{\Pbb} \ensembleLigne{\KLLigne{\Pbb}{\Qbb}}{\Pbb_0 = \pi_0, \ \Pbb_T = \pi_T} ,
  \label{eq:schrodinger_bridge}  
\end{equation}
where $\Qbb \in \pathmeas$ is a reference path measure.  In what follows, we
consider $\Qbb$ defined by the diffusion process \eqref{eq:reference_measure}
which is Markov, and without loss of generality, we assume $\Qbb_0=\pi_0$.
Hence $\PSB$ is the path measure closest to $\Qbb$ in terms of Kullback--Leibler
divergence which satisfies the initial and terminal constraints $\PSB_0 = \pi_0$
and $\PSB_T = \pi_T$.  Note that we also have
$\PSB = \int \Qbb_{|0,T} \rmd \PiSB_{0,T}$ where
$\PiSB_{0,T} = \argmin_{\Pi_{0,T}}
\ensembleLigne{\KLLigne{\Pi_{0,T}}{\Qbb_{0,T}}}{\Pi_0 = \pi_0, \ \Pi_T = \pi_T}$
is the solution of the \emph{static} SB problem \citep{leonard2014survey}. In
particular, for $\Qbb$ a multivariate Brownian of standard deviation $\sigma$,
\begin{equation}
\PiSB_{0,T} = \argmin_{\Pi_{0,T}} \ensembleLigne{\mathbb{E}_{\Pi_{0,T}}[||\bfX_0-\bfX_T||^2-2\sigma^2 T~\textup{H}(\Pi_{0,T})}{\Pi_0 = \pi_0, \ \Pi_T = \pi_T},
\end{equation}
where $\textup{H}(\mu)=-\int \log (\rmd \mu / \rmd \Leb) \rmd \mu$ denotes the
entropy, i.e. $\PiSB_{0,T}$ is an entropy-regularized OT plan. This transport
can also obtained theoretically by solving the following problem related
to \eqref{eq:BenamouBrenierOT}
\begin{equation}\label{eq:BenamouBrenierSBOT}
  \textstyle
v_{\textup{SB}}=\argmin_{v} \ensembleLigne{\int^T_0 \mathbb{E}_{\P_t}[||v(t,\bfX_t)||^2]\rmd t}{\rmd \bfX_t=v(t,\bfX_t)\rmd t+ \sigma\rmd \bfB_t, \,~\P_0=\pi_0,~ \P_T=\pi_T}.
\end{equation}
Then the SDE with drift $v_{\textup{SB}}$ initialized with $\bfX_0 \sim \pi_0$
is such that $(\bfX_0,\bfX_T)\sim \PiSB_{0,T}$.
A common approach to solve \eqref{eq:schrodinger_bridge} is the
Iterative Proportional Fitting (IPF) method 
\citep{fortet1940resolution,kullback1968probability,ruschendorf1995convergence} defining a sequence of path measures $(\Pbb^n)_{n \in \nset}$ where
\begin{equation}
  \label{eq:sinkhorn}
  \Pbb^{2n+1} = \argmin_{\Pbb} \ensembleLigne{\KLLigne{\Pbb}{\Pbb^{2n}}}{\Pbb_T = \pi_T} , \ \Pbb^{2n+2} = \argmin_{\Pbb} \ensembleLigne{\KLLigne{\Pbb}{\Pbb^{2n+1}}}{\Pbb_0 = \pi_0} ,
\end{equation}
with initialization $\Pbb^0 = \Qbb$. This procedure alternates between
projections on the set of path measures with given initial distribution $\pi_0$ and the
set of path measures with given terminal distribution $\pi_T$. It can be shown \citep{debortoli2021diffusion}
that the distributions $(\Pbb^n)_{n \in \nset}$ are associated with diffusions and
that for any $n \in \nset$, $\Pbb^{2n+1}$ is the time-reversal of
$\Pbb^{2n}$ with initialization $\pi_T$ and $\Pbb^{2n+2}$ is the time-reversal
of $\Pbb^{2n+1}$ with initialization $\pi_0$. 
Leveraging this property, \cite{debortoli2021diffusion} propose Diffusion
Schr\"odinger Bridge (DSB), an algorithm which learns the time-reversals
iteratively using neural networks.  In particular, DDMs can be seen as the first iteration of DSB.


\section{Iterative Markovian Fitting}
\label{sec:iterative-markovian-fitting}

\subsection{Markovian Projection and Reciprocal Projection}
\label{sec:markovian_projection}

\paragraph{Markovian Projection.} 
\textit{Markovian projection} is a key ingredient in our methodology and
in the Bridge Matching framework. This concept was introduced multiple times in
the literature \citep{gyongy1986mimicking,peluchettinon,liu2022let}. 
In particular, we focus on Markovian projection of path measures given by a mixture of bridges
$\Pi = \int \Qbb_{|0,T} \rmd \Pi_{0,T} \in \pathmeas$.

\begin{definition}
  \label{def:markovian_proj}
  Assume that $\Qbb$ is given by \eqref{eq:reference_measure} and that for any
  $(x_0,x_T) \in \rset^d$, $\Qbb_{|0,T}(\cdot|x_0,x_T)$ is associated with
  $(\bfX_t^{0,T})_{t \in \ccint{0,T}}$ given by
  $\rmd \bfX_t^{0,T} = \{f_t(\bfX_t^{0,T}) + \sigma_t^2 \nabla \log
  \Qbb_{T|t}(x_T|\bfX_t^{0,T})\} \rmd t + \sigma_t \rmd \bfB_t$, with
  $\sigma: \ \ccint{0,T} \to \ooint{0,+\infty}$. Then, when it is well-defined,
  we introduce the \emph{Markovian projection} of $\Pi$,
  $\Mbb^\star = \projM(\Pi) \in \calM$, associated with the SDE
  \begin{equation}
    \rmd \bfX^\star_t = \{f_t(\bfX^\star_t) + v_t^\star(\bfX^\star_t)\}\rmd t + \sigma_t \rmd \bfB_t, \qquad v_t^\star(x_t) =  \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t=x_t}.
  \end{equation}  
\end{definition}


Note that in our definition $\sigma_t>0$ so $\nabla \log \Qbb_{T|t}(x_T|x_t)$ is
well-defined, but Flow Matching can be recovered as the \emph{deterministic}
case in the limit $\sigma_t=\sigma \to 0$. For simplicity, we also assume that
$\Qbb$ is Markov but \Cref{def:markovian_proj} can be extended to the
non-Markovian setting. We use the term \emph{Markovian projection} since
$\Mbb^\star$ is Markovian (as it is given by a It\^o diffusion), while $\Pi$
might not be.
In the following proposition, we show that the Markovian projection is indeed a
projection for the \textit{reverse} Kullback--Leibler divergence, and that it
preserves marginals of $\Pi_t$. 

\begin{proposition}
  \label{prop:markovian-projection}
  Assume that $\sigma_t > 0$. Let $\Mbb^\star = \projM(\Pi)$. Then, under mild assumptions, we have
  \begin{align}    
    &\textstyle \Mbb^\star = \argmin_{\Mbb} \ensembleLigne{\KLLigne{\Pi}{\Mbb}}{\Mbb \in \calM} , \\
    & \textstyle \KLLigne{\Pi}{\Mbb^\star} = \tfrac{1}{2} \int_0^T \expeLigne{\Pi_{0,t}}{\normLigne{\sigma_t^2 \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t} - v_t^\star(\bfX_t)}^2}/\sigma_t^2 \rmd t  .
  \end{align}
  In addition, we have that for any $t \in \ccint{0,T}$, $\Mbb^\star_t = \Pi_t$. In particular, $\Mbb^\star_T = \Pi_T$.
\end{proposition}

\paragraph{Reciprocal Projection.} While the Markovian projection $\projM$
ensures that the obtained  measure is Markov, the associated \emph{bridge} measure is
not preserved in general, i.e.
$\projM(\Pi)_{|0,T} \neq \Pi_{|0,T} = \Qbb_{|0,T}$. 
Measures with same bridge as $\Qbb$ are said to be in its \emph{reciprocal class}
\citep{leonard2014reciprocal}. 

\begin{definition}
  $\Pi \in \pathmeas$ is in the reciprocal class of $\Qbb \in \calM$ if
  $\Pi = \int \Qbb_{|0,T} \rmd \Pi_{0,T} $. 
  We denote by $\calR(\Qbb)$ the reciprocal class of $\Qbb$. 
  We define the
  \emph{reciprocal projection} of $\Pbb \in \pathmeas$ as
  $\Pi^\star = \projR{\Qbb}(\Pbb) = \int \Qbb_{|0,T} \rmd \Pbb_{0,T} $.
  \label{def:reciprocal_projection}
\end{definition}

Similarly to \Cref{prop:markovian-projection}, we have the following
result, which justifies the term reciprocal projection.

\begin{proposition}
  \label{prop:reciprocal-projection}
  Let $\Pbb \in \pathmeas$, $\Pi^\star = \projR{\Qbb}(\Pbb)$. Then, $\Pi^\star = \argmin_{\Pi} \ensembleLigne{\KLLigne{\Pbb}{\Pi}}{ \Pi \in \calR(\Qbb)}$.
\end{proposition}

The reciprocal projection $\Pi^\star$ of a Markov path measure $\Mbb$
does not preserve the Markov property in general. In fact, the Schr\"odinger
Bridge is the \emph{unique} path measure which satisfies the initial and
terminal conditions, is Markov and is in the reciprocal class of $\Qbb$, see
\citep{leonard2014survey}. The following proposition makes this link explicit.

\begin{proposition}
  \label{prop:schro-markov-recip}
  Let $\Pbb$ be a Markov measure in the reciprocal class of $\Qbb$ such
  that $\Pbb_0 = \pi_0$, $\Pbb_T = \pi_T$. Then, under 
  assumptions on $\Qbb$, $\pi_0$ and $\pi_T$, $\Pbb$ is unique and is equal to the Schr\"odinger Bridge $\PSB$.
\end{proposition}



\subsection{Iterative Markovian Fitting}
\label{sec:iter-mark-fitt}

Based on \Cref{prop:schro-markov-recip}, we propose a novel methodology called
\textit{Iterative Markovian Fitting} (IMF) to solve Schr\"odinger Bridges.  We
consider a sequence $(\Pbb^n)_{n \in \nset}$ such that
\begin{equation}  
  \label{eq:imp}
  \Pbb^{2n+1} = \projM(\Pbb^{2n}) , \qquad \Pbb^{2n+2} = \projR{\Qbb}(\Pbb^{2n+1}) ,
\end{equation}
with $\Pbb^0$ such that $\Pbb^0_0 = \pi_0$, $\Pbb^0_T=\pi_T$ and
$\Pbb^0 \in \calR(\Qbb)$.  These updates correspond to alternatively performing
Markovian projections and reciprocal projections. Note that for any
$n \in \nset$, $\Pbb^{2n+1}$ is Markov and $\Pbb^{2n}$ is in the reciprocal
class of $\Qbb$. 


Combining \Cref{prop:markovian-projection} and
\Cref{def:reciprocal_projection}, we get that for any $n \in \nset$,
$\Pbb^n_0 = \pi_0$ and $\Pbb^n_T = \pi_T$.
This last property is in contrast with the sequence generated by the IPF
algorithm \eqref{eq:sinkhorn} for which the marginals at the initial and final times are
\textit{not} preserved.  Here the marginals are preserved, but we iteratively
project on the set of Markov measures and the reciprocal class of $\Qbb$. We
highlight this duality between IPF \eqref{eq:sinkhorn} and IMF \eqref{eq:imp} in \Cref{tab:diff}.


We conclude this section with a preliminary theoretical analysis of IMF.  First, we
start by showing a Pythagorean theorem for both the Markovian projection and the
reciprocal projection. 

\begin{lemma}
  \label{lemma:pythagorean_theorem}
  Under mild assumptions, 
  if $\Mbb \in \calM$, $\Pi \in \calR(\Qbb)$ and $\KLLigne{\Pi}{\Mbb} < +\infty$, we have
  \begin{equation}
    \KLLigne{\Pi}{\Mbb} = \KLLigne{\Pi}{\projM(\Pi)} +  \KLLigne{\projM(\Pi)}{\Mbb} . 
  \end{equation}
  If $\KLLigne{\Mbb}{\Pi} < +\infty$, we have 
  \begin{equation}
    \KLLigne{\Mbb}{\Pi} = \KLLigne{\Mbb}{\projR{\Qbb}(\Mbb)} +  \KLLigne{\projR{\Qbb}(\Mbb)}{\Pi} . 
  \end{equation}  
\end{lemma}


This result should be compared with the information geometry result of
\cite[Theorem 2.2]{csiszar1975divergence}, which states that if $\calC$ is a convex set and $\Pbb \in \calC$, then under mild
conditions,
$\KLLigne{\Pbb}{\Qbb} = \KLLigne{\Pbb}{\proj_{\calC}(\Qbb)} +
\KLLigne{\proj_{\calC}(\Qbb)}{\Qbb}$, where
$\proj_{\calC}(\Qbb) = \argmin_{\Pbb} \ensembleLigne{\KLLigne{\Pbb}{\Qbb}}{\Pbb \in
  \calC}$ is the projection of $\Qbb$ on $\calC$. Note that, contrary to \Cref{lemma:pythagorean_theorem},
\cite[Theorem 2.2]{csiszar1975divergence} is given for the \emph{forward}
Kullback--Leibler divergence whereas \Cref{lemma:pythagorean_theorem} is given for the \emph{reverse} KL divergence. In addition, \cite[Theorem
2.2]{csiszar1975divergence} requires the projection set $\calC$ to be convex
which is not satisfied for the space of Markov measures $\calM$, see \Cref{sec:set-markov-measures}.
Using \Cref{lemma:pythagorean_theorem}, we have the following proposition.

\begin{proposition}
  \label{prop:convergence_marginals}
  Under mild assumptions, we have
  $\lim_{n \to +\infty} \KLLigne{\Pbb^n}{\Pbb^{n+1}} = 0$. 
  If $\Pstar$ is a fixed point of $(\Pbb^n)_{n \in \nset}$, then $\Pstar$
  is the Schr\"odinger Bridge $\PSB$.
\end{proposition}

Hence, the Markov path measures $(\Pbb^{2n+1})_{n \in \nset}$ are getting closer
to the reciprocal class, while the reciprocal path measures
$(\Pbb^{2n+2})_{n \in \nset}$ are getting closer to the set of Markov measures.
\Cref{prop:convergence_marginals} should be compared with \cite[Proposition
2.1, Equation (2.16)]{ruschendorf1995convergence} which shows that, for the IPF
sequence $(\Pbb^n)_{n \in \nset}$, we have
$\lim_{n \to +\infty} \KLLigne{\Pbb^{n+1}}{\Pbb^n} = 0$. This is a result similar
to \Cref{prop:convergence_marginals} but for the \emph{forward}
Kullback--Leibler divergence. We leave the general study of the convergence of
$(\Pbb^n)_{n \in \nset}$ for future work.

\section{Diffusion \schro Bridge Matching}
\label{sec:pract-meth}
In this section, we present  Diffusion \schro Bridge Matching (DSBM), a practical
algorithm for solving the SB problem obtained by combining the IMF procedure
with Bridge Matching. Before presenting DSBM, we recall that the IMF sequence is
given by $(\Pbb^n)_{n \in \nset}$ in \eqref{eq:imp}. We further denote
$\Mbb^{n+1} = \Pbb^{2n+1} \in \calM$ and $\Pi^n = \Pbb^{2n} \in \calR(\Qbb)$. 
 
\paragraph{Iterative Markovian Fitting in practice. }
IMF alternatively projects on the set of Markov path measures $\calM$ and
on the reciprocal class of $\Qbb$. Assuming we know how to sample from the
bridge $\Qbb_{|0,T}$ given the initial and terminal conditions, sampling from
the reciprocal projection $\projR{\Qbb}(\Mbb)$ is simple and goes as
follows. First, sample $(\bfX_0,\bfX_T)$ from the joint distribution
$\Mbb_{0,T}$\footnote{In practice, we sample the SDE associated with $\Mbb$ and
  save a batch of joint samples $(\bfX_0, \bfX_T)$. This is similar to the
  \emph{trajectory caching} procedure in \cite{debortoli2021diffusion}, but we
  only retain initial and final samples.}. Then, sample from the bridge
$\Qbb_{|0,T}(\cdot|\bfX_0,\bfX_T)$.

The bottleneck of IMF is in the computation of the Markovian projections $\Mbb^\star=\projM(\Pi)$. Using
\Cref{def:markovian_proj}, we get that $\Mbb^\star=\projM(\Pi)$ is associated with the process
\begin{equation}
  \rmd \bfX_t = \{ f_t(\bfX_t) + \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t}\} \rmd t + \sigma_t \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 . 
\end{equation}
By Proposition \ref{prop:markovian-projection}, we can learn
$\Mbb^\star$ using $\Mbb^{\theta^\star}$ given by 
\begin{align}
&  \label{eq:approximate_markovian_proj_forward}
  \rmd \bfX_t = \{ f_t(\bfX_t) +  v_{\theta^\star}(t, \bfX_t) \} \rmd t + \sigma_t \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 , \\
  \label{eq:loss_function}
 & \textstyle \theta^\star = \argmin_\theta \ensembleLigne{\int_0^T \expeLigne{\Pi_{t,T}}{\normLigne{\sigma_t^2 \nabla
    \log \Qbb_{T|t}(\bfX_T|\bfX_t) - v_\theta(t,\bfX_t)}^2}/\sigma_t^2 \rmd t }{\theta \in \Theta} ,
\end{align}
and $\ensembleLigne{v_\theta}{\theta \in \Theta}$ is a parametric family of
functions, usually given by a neural network. Suppose the family of functions $\ensembleLigne{v_\theta}{\theta \in \Theta}$ is rich enough, we have for any
$t \in \ccint{0,T}$ and $x_t \in \rset^d$, the optimal 
  $v_{\theta^\star}(t, x_t) = \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log
    \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t = x_t}$. 

With the above two procedures for computing $\projR{\Qbb}(\Mbb)$ and $\projM(\Pi)$, we can
now describe a numerical method implementing IMF. Let $\Pi^0=\int \Qbb_{|0,T} \rmd \Pi_{0,T}^0$ be such that
$\Pi^0_0 = \pi_0$, $\Pi^0_T = \pi_T$. 
Following \eqref{eq:imp}, let $\Mbb^1 \approx \projM(\Pi^0)$ given by
\eqref{eq:approximate_markovian_proj_forward} with $v_{\theta^\star}$ learned using
\eqref{eq:loss_function}.  Next, sample from $\Pi^1 = \projR{\Qbb}(\Mbb^1)$ by
sampling from $\Mbb^1_{0,T}$ and reconstructing the bridge $\Qbb_{|0,T}$. The
Markov measure $\Mbb^2 \approx \projM(\Pi^1)$ is obtained similarly to
$\Mbb^1$. We iterate the process to obtain a sequence $(\Pi^n,\Mbb^{n+1})_{n \in \nset}$.
In practice, this algorithm performs poorly, since due to the approximate
minimization \eqref{eq:loss_function} to compute $\Mbb^{n+1}$, 
we do not have $\Mbb^{n+1}_T = \pi_T$ as in \Cref{prop:markovian-projection}. 
Instead, we incur a bias for each $n \in \nset$, and this bias between $\Mbb^{n+1}_T$ and $\pi_T$ accumulates.

To mitigate this problem, we alternate between a \emph{forward} Markovian
projection and a \emph{backward} Markovian projection. This procedure is
justified by the following proposition.
\begin{proposition}
  \label{prop:forward_backward_markov_proj}
  Assume that $\Pi=\int \Qbb_{|0,T} \rmd \Pi_{0,T} $ with $\Qbb$ associated with
  $\rmd \bfX_t = f_t(\bfX_t) \rmd t + \sigma_t \rmd \bfB_t$. Under mild
  conditions, the Markovian projection $\Mbb^\star = \projM(\Pi)$ is associated with both 
  \begin{align}
    &\label{eq:forward_markov_proj}
    \rmd \bfX_t = \{f_t(\bfX_t) + \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t} \} \rmd t + \sigma_t \rmd \bfB_t , \quad \bfX_0 \sim \Pi_0 , \\
    & \label{eq:backward_markov_proj}
    \rmd \bfY_t = \{-f_{T-t}(\bfY_t) + \sigma_{T-t}^2 \CPELigne{\Pi_{0|T-t}}{\nabla \log \Qbb_{T-t|0}(\bfY_t|\bfY_T)}{\bfY_t} \} \rmd t + \sigma_{T-t} \rmd \bfB_t , \ \bfY_0 \sim \Pi_T . 
  \end{align}
  \vspace{-0.5cm}
\end{proposition}
In \Cref{prop:forward_backward_markov_proj}, \eqref{eq:forward_markov_proj} is the definition of the Markovian
projection, see \Cref{def:markovian_proj}. However,
\eqref{eq:backward_markov_proj} is an equivalent representation as a \emph{time-reversal}. 
In practice, $(\bfY_t)_{t \in \ccint{0,T}}$ is approximated with
\begin{align}
&  \label{eq:approximate_markovian_proj_backward}
  \rmd \bfY_t = \{ -f_{T-t}(\bfY_t) + v_{\phi^\star}(T-t, \bfY_t) \} \rmd t + \sigma_{T-t} \rmd \bfB_t , \qquad \bfY_0 \sim \pi_T , \\ 
  \label{eq:loss_function_backward}
 & \textstyle \phi^\star = \argmin_\phi \ensembleLigne{\int_0^T \expeLigne{\Pi_{0,t}}{\normLigne{\sigma_t^2 \nabla
    \log \Qbb_{t|0}(\bfX_t|\bfX_0) - v_\phi(t,\bfX_t)}^2} /\sigma_t^2 \rmd t }{\phi \in \Phi} .
\end{align}
Suppose the family of functions $\ensembleLigne{v_\phi}{\phi \in \Phi}$ is rich enough, we have for any
$t \in \ccint{0,T}$ and $x_t \in \rset^d$, the optimal 
  $v_{\phi^\star}(t, x_t) = \sigma_t^2 \CPELigne{\Pi_{0|t}}{\nabla \log
    \Qbb_{t|0}(\bfX_t|\bfX_0)}{\bfX_t = x_t}$.

Note that in the forward projection
$\bfX_0 \sim \pi_0$, while in the backward projection $\bfY_0 \sim
\pi_T$. Therefore, using the backward projection removes the bias on $\pi_T$
accumulated from the previous iterations. 
Alternating between \eqref{eq:approximate_markovian_proj_backward} and
\eqref{eq:approximate_markovian_proj_forward} yields the Diffusion Schr\"odinger
Bridge Matching (DSBM) methodology summarized in \Cref{alg:DSBM_general}. 


\begin{wrapfigure}{l}{0.6\textwidth}
\begin{minipage}{0.6\textwidth}
\vspace{-0.8cm}
\begin{algorithm}[H]
\caption{Diffusion Schr\"odinger Bridge Matching}
\label{alg:DSBM_general}
\begin{algorithmic}[1]
\dsbmalgo
\end{algorithmic}
\end{algorithm}
\vspace{-1cm}
\end{minipage}
\end{wrapfigure}

\Cref{alg:DSBM_general} leverages the time-symmetry of the Markovian projection by alternating between the backward 
and forward Markovian projections. 
However, it is possible to learn
\emph{both} the forward and backward processes at each step. 
This also allows us to consider a \emph{consistency} loss which enforces that
the backward and forward processes match, see \Cref{sec:joint-train-forw}. 

In the next paragraph, we relate \Cref{alg:DSBM_general} to the 
DSB algorithm
\citep{debortoli2021diffusion} which relies on the classical IPF. 


\paragraph{Initialization coupling.} 
If instead of initializing DSBM by a coupling between $\pi_0, \pi_T$, we initialize it by $\Pi^0_{0,T}=\Q_{0,T}$
 where $\Q_0=\pi_0$ and $\Q_{T} \neq \pi_T$\footnote{If $\Q_{T}$ were equal to $\pi_T$, then the SB would be given by $\Qbb$.} is given by the reference process defined in \eqref{eq:reference_measure}, 
 then DSBM recovers the IPF iterates used in DSB. In particular, we have the following identification.

 \begin{proposition}
    \label{prop:dsb_dsbm}
   Suppose the families of functions $\ensembleLigne{v_\theta}{\theta \in \Theta}$ and
  $\ensembleLigne{v_\phi}{\phi \in \Phi}$ are rich enough so that they can model the optimal vector fields. 
    Let $(\Pi^n, \Mbb^{n+1})_{n \in \nset}$ be the
    optimal DSBM sequence in \Cref{alg:DSBM_general} initialized with  $\Pi^0_{0,T}=\Q_{0,T}$, and let
    $(\tilde{\Pbb}^n)_{n \in \nset}$ be the optimal DSB sequence given by the
    IPF iterates in \eqref{eq:sinkhorn}. Then for any $n \in \nset, n \geq 1$, we have
    $\Mbb^n = \tilde{\Pbb}^n$.
 \end{proposition}

 The training procedure in DSBM is very different from the one in DSB
 \citep{debortoli2021diffusion,chen2021likelihood}. In existing works, for any
 $n \in \nset$, $\tilde{\Pbb}^{2n+1}$ is obtained as the time-reversal of
 $\tilde{\Pbb}^{2n}$, see \cite[Proposition 6]{debortoli2021diffusion}. More
 precisely, for any $n \in \nset$ we have that $\tilde{\Pbb}^{2n+1}$ is
 associated with
 $\rmd \bfY_t^{2n+1} = b_{T-t}^n(\bfY_t^{2n+1}) \rmd t + \rmd \bfB_t$,
 $\bfY_0 \sim \pi_T$ and $\tilde{\Pbb}^{2n+2}$ with
 $\rmd \bfX_t^{2n+2} = f_t^{n+1}(\bfX_t^{2n+2}) \rmd t + \rmd \bfB_t$,
 $\bfX_0 \sim \pi_0$, where $b_t^n = -f_t^n + \nabla \log \tilde{\Pbb}_t^{2n}$
 and $f_t^{n+1} = -b_t^n + \nabla \log \tilde{\Pbb}_t^{2n+1}$. The drifts
 $b_t^n$ and $f_t^n$ are estimated using the \emph{drift-matching loss}. This
 requires sampling time-discretized trajectories from $\tilde{\Pbb}^{2n}$ in
 order to estimate $b_t^n$. In contrast, in \Cref{alg:DSBM_general} we do not
 use the full trajectory of $\Mbb^{2n}$ to learn $\Mbb^{2n+1}$. More precisely,
 we only use the \emph{coupling} $\Mbb^{2n}_{0,T}$ to create the bridge measure
 $\Pi^{2n} = \int \Qbb_{|0,T} \rmd \Mbb^{2n}_{0,T} $. By doing so,
 \begin{enumerate*}[label=(\roman*)]   
 \item  the losses \eqref{eq:loss_function} and
   \eqref{eq:loss_function_backward} can be evaluated for any time $t \in \ccint{0,T}$, without relying on a fixed \emph{time-discretization} of the process; 
 \item setting $\Pi^{2n} = \int \Qbb_{|0,T} \rmd \Mbb^{2n}_{0,T} $ projects $\Mbb^{2n}$ on
   the reciprocal class of $\Qbb$, see \Cref{def:reciprocal_projection}. While
   every iteration $\tilde{\Pbb}^n$ in the DSB algorithm is supposed to be in
   the reciprocal class of $\Qbb$, in practice one can observe a
   \emph{forgetting} of the bridge $\Qbb_{|0,T}$
   \citep{fernandes2021shooting}. In DSBM, this effect is countered by considering
   explicit projections on the reciprocal class;
   \item the \emph{trajectory caching} procedure in DSBM is more computationally and memory efficient, since the Bridge Matching objective does not require evaluating the drift at two different timesteps as in DSB (thus only requiring half NFEs), nor saving the entire SDE trajectory in memory. 
 \end{enumerate*}
 We explain further the benefits of DSBM in \Cref{sec:dsbm_vs_dsb_appendix}.

 We call DSBM-IPF, the DSBM algorithm applied with the coupling given by the
 forward reference process $\Pi_{0,T}^0=\Q_{0,T}$. Similarly, we call DSBM-IMF,
 the DSBM algorithm applied with an initial independent coupling
 $\Pi_{0,T}^0 = \pi_0 \otimes \pi_T$. 
 We experiment with both methods and leave extensive empirical comparison
 between the two as future
 work. 

\paragraph{On DSBM and existing algorithms.}
Different choices of bridges $\Qbb_{|0,T}$ and couplings $\Pi_{0,T}^0$ in \Cref{alg:DSBM_general} recover existing algorithms. For the independent coupling
$\Pi_{0,T}^0 = \pi_0 \otimes \pi_T$ and Brownian bridge $\Qbb_{|0,T}$ 
with diffusion parameter $\sigma_t=\sigma$ given by \eqref{eq:interpolation_stochastic}, we get that
\eqref{eq:loss_function} recovers the Brownian Bridge Matching loss \eqref{eq:bridge_matching_loss_brownian}.
Letting $\sigma \to 0$, we recover Flow Matching 
\citep{lipman2022flow}. In this case, further iterations repeating lines 7-9 in \Cref{alg:DSBM_general} recover the Rectified Flow algorithm
\citep{liu2022flow}. Therefore, existing Flow Matching and Rectified Flow
approaches can be seen as special limiting cases of DSBM with only forward projections. If the coupling $\Pi_{0,T}^0$ is given by
an estimation of the OT map, then the first
iteration recovers the OT-CFM model
\citep{tong2023conditional}. Finally for general bridges $\Qbb_{|0,T}$, if instead we are given the optimal Schr\"odinger Bridge
static coupling $\Pi_{0,T}^0=\PiSB_{0,T}$, then the DSBM procedure converges in one
iteration and we recover \cite{somnath2023aligned}. See \Cref{sec:related_work} for further discussions. 

\paragraph{Probability flow ODE.} At equilibrium of DSBM, we have that
$(\bfY_t)_{t \in \ccint{0,T}}$ given by
\eqref{eq:approximate_markovian_proj_backward} is the time reversal of
$(\bfX_t)_{t \in \ccint{0,T}}$ given by
\eqref{eq:approximate_markovian_proj_forward} and are both associated with the
Schr\"odinger Bridge path measure. As a result, we have that
$v_{\phi^\star}(t,x) = -v_{\theta^\star}(t,x) + \sigma_t^2 \nabla \log
\PSB_t(x)$. Hence, a probability flow $(\bfZ_t)_{t \in \ccint{0,T}}$ such that
$\mathrm{Law}(\bfZ_t) = \PSB_t$ for any $t \in \ccint{0,T}$ is given by
 \begin{equation}
   \label{eq:probability_flow}
   \rmd \bfZ_t = \{f_t(\bfZ_t) + \tfrac{1}{2}[v_{\theta^\star}(t, \bfZ_t) - v_{\phi^\star}(t, \bfZ_t)]\} \rmd t , \qquad \bfZ_0 \sim \pi_0 . 
 \end{equation}
 Note that the path measure induced by $(\bfZ_t)_{t \in \ccint{0,T}}$ does not correspond to $\PSB$; in particular,
 $(\bfZ_0, \bfZ_T)$ is \emph{not} an entropic optimal transport plan. However,
since for any $t \in \ccint{0,T}$, $\bfZ_t$ has marginal distribution
 $\PSB_t$, we
 can compute the log-likelihood of the model
\citep{song2020score,huang2021variational}.




\section{Related Work}
\label{sec:related_work}
\paragraph{Markovian projection and Bridge Matching. } The concept of Markovian projection has been
rediscovered multiple times. It was first introduced in the probability literature by \cite{gyongy1986mimicking}
to mimick non-Markovian processes, following on a work by
\citet{krylov1984once}. This idea was introduced in mathematical finance
community by \citet{dupire1994pricing}.
In the machine learning context, this was first proposed by
\cite{peluchettinon} to define Bridge Matching models. 
More recently,
\citet{liu2022let} established Kullback--Leibler divergence
properties of the Markovian projection presented in \Cref{prop:markovian-projection}, the first part of \Cref{lemma:pythagorean_theorem},
and applied Bridge Matching for learning data on discrete and constrained domains. 

\paragraph{Bridge and Flow Matching. }
Flow Matching Models correspond to deterministic bridges with deterministic samplers (ODEs) and have been under active study 
\citep{liu2022flow,liu2022rectified,lipman2022flow,albergo2022building,tong2023conditional,heitz2023iterative}.
Denoising Diffusion Implicit Models (DDIM) \citep{song2020denoising} can also be
thought of as a discrete-time version of Flow Matching, see \cite{liu2022flow}.
These models were extended to the Riemannian setting by
\cite{chen2023riemannian}. 
Recently, \cite{albergo2023stochastic} studied the influence
of stochasticity in the sampler and the bridge, through the concept of stochastic interpolants\footnote{Stochastic interpolants do not require the reference measure $\Qbb$ to be Markov. We discuss the differences between our frameworks in \Cref{sec:bridge_design_space_appendix}.}. 
\cite{liu2023I2SB,delbracio2023inversion} used Bridge Matching to perform
image restoration tasks and noted benefits of stochasticity empirically. 
Closely related to our work
is the Rectified Flow algorithm of \cite{liu2022flow}, which corresponds to an
iterative Flow Matching procedure in order to improve the straightness
of the flow and thus ease its simulation. DSBM-IMF is closest to
Rectified Flow, which can be seen as the deterministic limiting case as
$\sigma\to0$.  However, there are a few important 
differences.  Firstly, we adopt the SDE approach as opposed to the ODE approach
in Rectified Flow. This distinction is crucial in theory, as
\Cref{prop:schro-markov-recip}, which guarantees the uniqueness of the
characterization of SB, is valid only when $\sigma_t>0$. Consequently, Rectified
Flow is not guaranteed to converge to the dynamic optimal transport solution. In
a following work \cite{liu2022rectified} established formal connections between
Rectified Flow and OT when restricting the class of vector fields to gradient
fields. In DSBM, the connection to OT is obtained by considering its
entropy-regularized version.  Secondly, Rectified Flow also performs Markovian
projections iteratively, but only in the forward direction. Consequently, the
bias in the marginal $\Pbb_T^n$ is accumulated and cannot be corrected in later
iterations, i.e. the first iteration will achieve the most accurate marginal
$\Pbb_T^1$. Subsequent iterations can improve the straightness of the flow and
thus requires few discretization steps, but at the cost of the optimal sampling
accuracy.  We observe in practice that this becomes particularly problematic if
the first iteration of Rectified Flow fails to provide a good transport.  In our
methodology, we leverage \Cref{prop:forward_backward_markov_proj} to perform
forward and backward Bridge Matching, and we observe that the marginal accuracy
is able to improve with each iteration.

\paragraph{Diffusion Schr\"odinger Bridge. } Schr\"odinger Bridges
\citep{schrodinger1932theorie} are ubiquitous in probability theory
\citep{leonard2014survey} and stochastic control
\citep{dai1991stochastic,chen2020optimal}.  
More recently, they have been used in machine learning to solve generative
modeling problems.  In particular, \cite{debortoli2021diffusion} proposed the
DSB algorithm, and \cite{vargas2021solving,chen2021likelihood} proposed similar
related algorithms.  The case where one of the terminal distributions is a Dirac
delta mass is investigated in \cite{wang2021deepschro}.  These methods were
later extended to solve conditional simulation problems
\citep{shi2022conditional} and more general control problems
\citep{thornton2022riemannian,liu2022deep,chen2023deep,tamir2023transport}.  In
\cite{somnath2023aligned}, Schr\"odinger Bridges are trained using one Bridge
Matching iteration, assuming access to the Schr\"odinger static coupling. Our
proposed method DSBM-IPF most closely resembles
DSB, 
but with improved continous-time training and projections on the reciprocal
class which mitigate two limitations of DSB.

\section{Experiments}
\label{sec:experiments}

\subsection{2D Experiments}

We first show our proposed methods can learn lower kinetic energy transport maps and generate correct samples in some 2D examples. 
We compare our method DSBM with DSB \citep{debortoli2021diffusion}, Flow Matching (FM) \citep{lipman2022flow}, Conditional Flow Matching (CFM), OT-CFM \citep{tong2023conditional}, and Rectified Flow (RF) \citep{liu2022flow}. OT-CFM directly uses sample-based Optimal Transport solver \citep{flamary2021pot} to approximate the solution of the dynamic OT problem, whereas the rest of the methods do not use OT solvers. DSB and our proposed method learn the EOT map implicitly as the solution of the diffusion process. Note that FM cannot be used for the \textit{moons-8gaussians} task since it requires a Gaussian source, but CFM is applicable. For our method, we use Brownian motion for the reference measure $\Qbb$ with Brownian bridge \eqref{eq:interpolation_stochastic} and $\sigma_t=1$ in all cases, except the \textit{moons-8gaussians} dataset where we use $\sigma_t=5$.

In \Cref{tab:2d_result}, we show the 2-Wasserstein distance between the test set
and the generated samples when using 20 discretization steps, as well as the
average path energy using each method. The path energy is defined by
$\E[\int_{0}^T || v(t, \bfZ_t) ||^2 \rmd t]$ where $v$ is the learned drift along
the ODE trajectory $\bfZ_t$. Lower path energies represent shorter (and
potentially easier to integrate) trajectories. For the 2D experiments, we
observe that OT-CFM performs the best by directly utilizing OT solvers. On the
other hand, DSBM and Rectified Flow achieve lower 2-Wasserstein distance and
path energy among methods that do not use specialized solvers. Comparing DSBM
and Rectified Flow, we observe that Rectified Flow achieves lower sampling error
under 20 steps except for the \textit{moons-8gaussians} dataset, 
for which DSBM is significantly more accurate. 
Since RF can be seen as the
deterministic limit case of DSBM, this suggests that the optimal amount of
stochasticity may depend on the task. 
Compared to FM and CFM, DSBM provides
consistent improvements over both sampling quality and path energy. Furthermore,
while DSBM solves the same SB problems as DSB, DSBM outperforms DSB in terms of
2-Wasserstein distance on all datasets at the same sampling budget, which
suggests that DSBM solves the SB problem with higher accuracy.  


\begin{table}
\centering
\setlength{\tabcolsep}{5pt}
\caption{Sampling quality as measured by 2-Wasserstein distance and path energy for the 2D experiments. $\pm$ one standard deviation computed over 5 seeds. Best values are in bold and second best are italicized. }

\subfloat[]{%
\scalebox{0.65}{
    \begin{tabular}{ccccc}
    \toprule 
     & \multicolumn{4}{c}{\textit{2-Wasserstein (Euler 20 steps)}}\tabularnewline
    \cmidrule{2-5} \cmidrule{3-5} \cmidrule{4-5} \cmidrule{5-5} 
     & moons & scurve & 8gaussians & moons-8gaussians\tabularnewline
    \midrule
    \midrule 
    DSBM-IPF & 0.140\textpm 0.006 & 0.140\textpm 0.024 & 0.315\textpm 0.079 & \textit{0.812\textpm 0.092}\tabularnewline
    \midrule 
    DSBM-IMF & 0.144\textpm 0.024 & 0.145\textpm 0.037 & 0.338\textpm 0.091 & 0.838\textpm 0.098\tabularnewline
    \midrule 
    DSB & 0.190\textpm 0.049 & 0.272\textpm 0.065 & 0.411\textpm 0.084 & 0.987\textpm 0.324\tabularnewline
    \midrule 
    FM & 0.212\textpm 0.025 & 0.161\textpm 0.033 & 0.351\textpm 0.066 & -\tabularnewline
    \midrule 
    CFM & 0.215\textpm 0.028 & 0.171\textpm 0.023 & 0.370\textpm 0.049 & 1.285\textpm 0.314\tabularnewline
    \midrule 
    RF & \textit{0.129\textpm 0.022} & \textit{0.126\textpm 0.019} & \textit{0.267\textpm 0.041} & 1.522\textpm 0.304\tabularnewline
    \midrule 
    OT-CFM & \textbf{0.111\textpm 0.005} & \textbf{0.102\textpm 0.013} & \textbf{0.253\textpm 0.040} & \textbf{0.716\textpm 0.187}\tabularnewline
    \bottomrule
    \end{tabular}
}
}
\subfloat[]{%
\scalebox{0.65}{
    \begin{tabular}{ccccc}
    \toprule 
    \multicolumn{4}{c}{\textit{Path energy}}\tabularnewline
    \midrule
    moons & scurve & 8gaussians & moons-8gaussians\tabularnewline
    \midrule
    \midrule 
    1.598\textpm 0.034 & 2.110\textpm 0.059 & \emph{14.91\textpm 0.310} & 42.16\textpm 1.026\tabularnewline
    \midrule 
    1.580\textpm 0.036 & 2.092\textpm 0.053 & \textbf{14.81\textpm 0.255} & 41.00\textpm 1.495\tabularnewline
    \midrule 
    - & - & - & - \tabularnewline
    \midrule 
    2.227\textpm 0.056 & 2.950\textpm 0.074 & 18.12\textpm 0.416 & -\tabularnewline
    \midrule 
    2.391\textpm 0.043 & 3.071\textpm 0.026 & 18.00\textpm 0.090 & 116.5\textpm 2.633\tabularnewline
    \midrule 
    \textit{1.185\textpm 0.052} & \textit{1.633\textpm 0.074} & \textit{14.84\textpm 0.441} & \textit{37.61\textpm 3.906}\tabularnewline
    \midrule 
    \textbf{1.178\textpm 0.020} & \textbf{1.577\textpm 0.036} & 15.10\textpm 0.215 & \textbf{30.50\textpm 0.626}\tabularnewline
    \bottomrule
    \end{tabular}
}
}
\label{tab:2d_result}
\end{table}


\subsubsection{Variance of the reference measure $\mathbb{Q}$}
We now investigate the effect of the value of $\sigma_t$ used for the reference path measure $\mathbb{Q}$. We assume a time-homogeneous $\sigma_t=\sigma$ value for simplicity. In Figures \ref{fig:2d_sigma_samples} and \ref{fig:2d_sigma_paths}, we vary $\sigma$ and visualize the learned transport for the \textit{3gaussians} problem of transporting between two Gaussian mixtures. In Table \ref{tab:2d_sigma_metrics_wasserstein2} we show the 2-Wasserstein distance between the test set and generated samples for this \textit{3gaussians} problem as well as the \textit{moons-8gaussians} problem. We find that large values of $\sigma$ result in increasingly curved transport paths, and correspondingly reduced performance when $\sigma$ is excessively large. Conversely, we also find reduced performance when $\sigma$ is excessively small. This is due to increased optimization difficulty and bias accumulation. 
Firstly, the number of outer steps required to converge to the Schr\"odinger Bridge is proportional to the KL divergence between the reference process and the Schr\"odinger Bridge itself. This KL divergence increases as $\sigma$ is taken to $0$ thus making the outer optimization convergence more difficult.
Further, the introduction of a small amount of noise also decreases optimization difficulty by smoothing the intermediate marginals between the two terminal distributions. Finally, a larger $\sigma$ value can also increase the diversity of sampled couplings and may alleviate some bias accumulation issues during outer iterations. When $\sigma = 0$, these issues result in the artifacts observed in Figure \ref{fig:2d_sigma_samples}. The appropriate value for $\sigma$ depends on the spatial scaling of the problem as shown in Table \ref{tab:2d_sigma_metrics_wasserstein2} where the optimum $\sigma$ is larger for the larger scale \textit{moons-8gaussians} problem. The benefit of setting $\sigma >0$ and using a stochastic sampler was also observed in \cite{albergo2023stochastic,delbracio2023inversion}.


\begin{figure}
    \centering
    \vspace{-0.8cm}
    \includegraphics[width=\textwidth,trim=100 0 100 0]{Figs/2d_sigma/genvsgt.png}
    \caption{Samples from final marginal of learned SB process (yellow) versus ground truth (purple).}
    \label{fig:2d_sigma_samples}
    \vspace{-0.2cm}
    \includegraphics[width=\textwidth,trim=100 0 100 0]{Figs/2d_sigma/transport.png}
    \caption{Trajectories of ODE integration of the learned SB probability flow.}
    \label{fig:2d_sigma_paths}
\end{figure}

\begin{table}
    \caption{2-Wasserstein distance for varying value of $\sigma$ used in the DSBM-IMF method.}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{cccc}
    \toprule 
    \multicolumn{2}{c}{3gaussians} & \multicolumn{2}{c}{moons-8gaussians} \tabularnewline
    \midrule
    $\sigma$ & \textit{2-Wasserstein} & $\sigma$ & \textit{2-Wasserstein} \tabularnewline
    \midrule
    \midrule 
    $\sigma=0$ & 0.646\textpm 0.028 & $\sigma=0$ & 1.459\textpm 0.008\tabularnewline
    \midrule 
    $\sigma=0.1$ & 0.724\textpm 0.039  & $\sigma=1.0$ & 1.285\textpm 0.346 \tabularnewline
    \midrule 
    $\sigma=0.3$ & 0.546\textpm 0.169 & $\sigma=2.0$ & \emph{0.916\textpm 0.292}   \tabularnewline
    \midrule 
    $\sigma=1.0$ & \textbf{0.439\textpm 0.072} & $\sigma=4.0$ & \textbf{0.818\textpm 0.249} \tabularnewline
    \midrule
    $\sigma=3.0$ & \emph{0.543\textpm 0.078}  & $\sigma=8.0$ & 0.989\textpm 0.179  \tabularnewline
    \bottomrule
    \end{tabular}
    }
    
    \label{tab:2d_sigma_metrics_wasserstein2}
\end{table}




\subsection{High-Dimensional Synthetic Experiment}
We next perform the Gaussian transport experiment in \cite{debortoli2021diffusion} with dimension $d=50$ to verify the scalability of our proposed approach. The true SB can be computed analytically in this case \citep{debortoli2021diffusion,bunne2022gaussian}. In \Cref{fig:gaussian_ipf_convergence}, we plot the convergence of DSB and our proposed methods. We also consider a baseline IMF-b, which performs Iterative Markovian Fitting numerically but only in the backward direction. All methods converge approximately to the mean of ground truth distribution $\pi_0$. However, the variance estimate becomes increasingly inaccurate for IMF-b as the number of iterations increases. On the other hand, our proposed methods do not suffer from this issue, which demonstrates the usefulness of alternating forward and backward training. Furthermore, for the covariance between times $0,1$ under the true SB $\PSB$, our proposed methods converge close to the ground truth covariance, whereas DSB and IMF-b both worsen from the optimum. In particular, DSB loses accuracy due to accumulation of error in its iterative time-reversal procedure as noted in \cite{debortoli2021diffusion}. Our proposed methods are the most accurate across all three metrics and avoided the deterioration of learned bridges. 

In \Cref{tab:gaussian_avg_kl}, we further quantify the accuracy of the learned SB by computing the KL divergence between the marginal distributions of the learned processes $\P_t$ and the true SB marginal $\PSB_t$. We also compare with SB-CFM \citep{tong2023conditional} using this setup at the marginal level. Our proposed methods achieve similar KL divergence as SB-CFM in dimension $d=5$, but are much more accurate in higher dimensions compared to both DSB and SB-CFM. This highlights the challenge of learning SB in high dimensions. On the other hand, the error of DSBM scales approximately linearly with the number of dimensions, which suggests that our proposed method is more scalable. 



\begin{figure}
    \centering
    \includegraphics[width=.325\linewidth]{Figs/Gaussian/dim50_a0.1_mlp_large_10_10000_mean.png}
    \includegraphics[width=.325\linewidth]{Figs/Gaussian/dim50_a0.1_mlp_large_10_10000_var.png}
    \includegraphics[width=.325\linewidth]{Figs/Gaussian/dim50_a0.1_mlp_large_10_10000_cov.png}
    \caption{Convergence of Gaussian experiment in dimension $d=50$. $\pm$ one standard deviation computed over 5 seeds is shaded. }
    \label{fig:gaussian_ipf_convergence}
\end{figure}

\begin{table}
    \captionof{table}{Average $\KL(\P_t|\PSB_t)$ at 21 uniformly spaced values of $t\in[0,T]$ for the Gaussian experiment in dimensions $d=5,20,50$. $\pm$ one standard deviation computed over 5 seeds. Best values are in bold and second best are italicized. }
    \centering
    
    \scalebox{0.8}{
    \begin{tabular}{cccc}
    \toprule 
    KL $\times {10}^{-3}$ & $d=5$ & $d=20$ & $d=50$\tabularnewline
    \midrule
    \midrule 
    DSB & 3.26\textpm 1.60 & 13.0\textpm 3.49 & 32.8\textpm 1.28\tabularnewline
    \midrule 
    SB-CFM & 1.45\textpm 0.73 & 12.3\textpm 1.47 & 49.4\textpm 3.91\tabularnewline
    \midrule 
    DSBM-IPF & \textbf{1.23\textpm 0.23} & \textbf{4.42\textpm 0.76} & \textbf{8.75\textpm 0.87}\tabularnewline
    \midrule 
    DSBM-IMF & \emph{1.34\textpm 0.51} & \emph{5.05\textpm 0.95} & \emph{9.76\textpm 1.67}\tabularnewline
    \bottomrule
    \end{tabular}
    }
    
    \label{tab:gaussian_avg_kl}
\end{table}

\subsection{Image Experiment}

Finally, we test our method on image domain transfer between MNIST digits and EMNIST letters as in \cite{debortoli2021diffusion}. We compare our method DSBM-IPF as a direct substitute of DSB, and also with CFM, OT-CFM and Rectified Flow. Similar to \cite{debortoli2021diffusion}, we use the set of first 5 letters of EMNIST in upper and lower cases (A-E, a-e) such that both domains have 10 classes in total. Both DSB and DSBM integrate the learned SDE with 30 Euler--Maruyama integration steps, whereas we use 100 Euler steps for all flow methods. 

We plot some output samples from each algorithm in \Cref{fig:mnist_backward_samples} and the convergence of FID score in \Cref{fig:mnist_fid}, and additional experimental results are included in \Cref{subsec:mnist_appendix}. We find that in this high-dimensional transfer example, 
OT-CFM produces samples of worse quality 
(\Cref{fig:mnist_backward_samples_otcfm}). 
This suggests that OT-CFM becomes less and less applicable in higher dimensions. 
Since Rectified Flow cannot improve training errors in CFM, its performance is also worse than CFM and OT-CFM on this task (\Cref{fig:mnist_backward_samples_rf_appendix}). 
On the other hand, DSB appears to generate clearer samples after 20 iterations as shown in \Cref{fig:mnist_backward_samples_dsb20}. However, DSB's FID score deteriorates after 10 iterations, and eventually becomes exceedingly high after 30 iterations. Our method DSBM achieves higher quality samples visually, and does not suffer from  deterioration. We train DSBM for 50 outer iterations, and do not observe deterioration of FID score.
Our method is also about $30\%$ more efficient than DSB in terms of runtime due to computational savings discussed in \Cref{sec:pract-meth} and \Cref{sec:dsbm_vs_dsb_appendix}.


\begin{figure}

\centering
\begin{minipage}[b]{.65\textwidth}
    \centering
    \subfloat[OT-CFM]{
    \includegraphics[width=0.3\linewidth]{Figs/MNIST_EMNIST/CFM/OTCFM250000b.png} 
    \label{fig:mnist_backward_samples_otcfm}
    }
    \subfloat[DSB Iteration 20]{
    \includegraphics[width=0.3\linewidth]{Figs/MNIST_EMNIST/DSB/DSB20b.png} 
    \label{fig:mnist_backward_samples_dsb20}
    }
    \subfloat[DSBM Iteration 50]{
    \includegraphics[width=0.3\linewidth]{Figs/MNIST_EMNIST/DSBM/DSBM50b.png} 
    }
    \caption{Samples of MNIST digits transferred from letters. }
    \label{fig:mnist_backward_samples}
\end{minipage}
\quad
\begin{minipage}[b]{.27\textwidth}
    \centering
    \includegraphics[width=\linewidth]{Figs/MNIST_EMNIST/mnist_emnist_fid.png}
    \caption{FID vs iteration. }
    \label{fig:mnist_fid}
\end{minipage} 

\end{figure}

\section{Discussion}

In this work, we propose IMF, a novel methodology for learning \schro Bridges motivated by recent progress in Flow Matching \citep{liu2022flow,lipman2022flow}. IMF is an alternative to the classical IPF and can be interpreted as its dual, with alternative projections on the class of Markov and reciprocal processes. Building on IPF and the novel IMF methodologies, we present two practical algorithms, DSBM-IPF and DSBM-IMF, to compute SBs. The DSBM framework mitigates the time-discretization and accumulation of bias issues in existing methods such as DSB \citep{debortoli2021diffusion} as demonstrated in our experiments.
In future work, we would like to further investigate the theoretical properties of DSBM. Another interesting direction would be to compare DSBM-IMF vs DSBM-IPF, and
investigate whether one could propose better initial couplings scalable to high dimensions to further accelerate the convergence of DSBM.
We believe IMF might also be useful to develop a better understanding of the Rectified Flow procedure \citep{liu2022flow}, as IMF minimizes a clear objective and Rectified Flow is informally a limiting case of it. Finally, the Rectified Flow procedure has been extended to solve OT problems with general convex costs by \cite{liu2022rectified}, and it would be interesting to derive a SB version of this approach. 

\newpage

\bibliographystyle{apalike}
\bibliography{bib}

\newpage
\appendix 

\section*{Outline of the Appendix} 
In \Cref{sec:fm_relationship_appendix}, we first clarify the relationship
between different Flow Matching Models in the existing literature and show that
Bridge Matching generalizes Flow Matching.  In
\Cref{sec:bridge_design_space_appendix}, we draw a link between the
parameterization of bridges in this paper and the stochastic interpolant in
\cite{albergo2023stochastic}.  In \Cref{sec:proofs_appendix}, we give proofs for
results in the main text.  In \Cref{sec:set-markov-measures}, we give a simple
counter-example proving that the set of Markov measures is not convex.  In
\Cref{sec:discrete_time_mp_appendix}, we derive a discrete-time version of
Markovian projection.  In \Cref{sec:dsbm_vs_dsb_appendix}, we explain further
the benefits of DSBM compared to DSB.  In
\Cref{sec:joint-train-forw}, we describe a method for learning the forward and
backward processes jointly and propose a consistency loss between the forward
and backward processes.  In \Cref{sec:loss_scaling_appendix}, we present a
practical scaling of the loss function to reduce the variance, similar to
standard Denoising Diffusion Models.  In \Cref{sec:exp_detail_appendix}, we give
further details for all experiments and some additional experimental results.



\section{Bridge Matching and Flow Matching Models}
\label{sec:fm_relationship_appendix}
In this section, we clarify the relationship between variants of Bridge Matching
and Flow Matching and show that they are equivalent under some
conditions. We follow the nomenclature of \cite{tong2023conditional}. We refer
to the algorithm originally proposed in \cite{lipman2022flow} using linear
probability paths and described in \cite[Section 4.1]{tong2023conditional} as
Flow Matching (FM), and the algorithm proposed in \cite[Section
4.2]{tong2023conditional} as Conditional Flow Matching (CFM).  There is a small
constant parameter $\sigma_\text{min}$ in both algorithms, which controls the
smoothing of the modeled distribution. We consider the case
$\sigma_\text{min}=0$. Then we show that CFM recovers exactly the 1st iteration
of Rectified Flow \citep{liu2022flow}. Furthermore, FM, CFM and the 1st iteration of
Rectified Flow are all equivalent when performing generative modeling with a
standard Gaussian $\pi_0$. 
We refer to them collectively
as Flow Matching Models (FMMs) as they only differ in the smoothing method. 
We also present them all under the Bridge Matching framework. 
These models can also be interpreted in the context
of \emph{stochastic interpolants}
\citep{albergo2022building,albergo2023stochastic}. 
Finally, we present
recent applications of Bridge Matching and show that some of the objectives in
\cite{somnath2023aligned,liu2023I2SB,delbracio2023inversion} are identical.

\paragraph{Flow Matching and Conditional Flow Matching.} In Flow Matching (FM),
the objective \cite[Equation (21)]{lipman2022flow} is
\begin{equation}
  \label{eq:flow_matching_objective}
    \expeLigne{\Pi_{t,T}}{\normLigne{(\bfX_T - \bfX_t)/(T-t) - v_\theta(t,\bfX_t)}^2},
\end{equation}
where $\Pi_{t,T}$ is given by $\pi_T(\bfX_T) \mathrm{N}(\bfX_t;\frac{t}{T}\bfX_T, {(1 - \frac{t}{T})}^2)$.

In Conditional Flow Matching (CFM),
$\bfX_t^{0,T} = \frac{t}{T}\bfX_T + (1-\frac{t}{T})\bfX_0$, with
$\bfX_0 \sim \mathrm{N}(0, \Id)$ and the objective \cite[Equation
(16)]{tong2023conditional} is given by
\begin{equation}
  \label{eq:loss_tong}
    \expeLigne{\Pi_{0,T}}{\normLigne{(\bfX_T - \bfX_0)/T - v_\theta(t,\bfX_t^{0,T})}^2}.
\end{equation}
This is the same as \cite[Equation (1)]{liu2022flow}. Furthermore, $(\bfX_T - \bfX_0)/T=(1-\frac{t}{T})(\bfX_T - \bfX_0)/(T-t)=(\bfX_T - \bfX_t^{0,T})/(T-t)$, so the CFM objective is equivalent to 
\begin{equation}
    \label{eq:cfm_objective}
    \expeLigne{\Pi_{t,T}}{\normLigne{(\bfX_T - \bfX_t^{0,T})/(T-t) - v_\theta(t,\bfX_t^{0,T})}^2}.
  \end{equation}
  
  In the case of generative modeling, $\pi_0$ is a standard Gaussian
  distribution and $\Pi_{0,T}$ is given by
  $\mathrm{N}(\bfX_0;0, \Id) \pi_T(\bfX_T)$. Thus, $\Pi_{t,T}$ is also given by
  $\pi_T(\bfX_T) \mathrm{N}(\bfX_t^{0,T};\frac{t}{T}\bfX_T, {(1 -
    \frac{t}{T})}^2)$. Therefore, the FM \citep{lipman2022flow} and CFM
  \citep{tong2023conditional} objectives are exactly the same. However, CFM is
  also applicable when $\pi_0$ is not Gaussian distributed, so CFM is a
  generalized version of FM\footnote{In the case $\sigma_\text{min}>0$, FM and
    CFM are indeed different in how smoothing is performed, and we refer to
    \cite{tong2023conditional} for a more detailed analysis.}.

  \paragraph{Stochastic Interpolant.} In
  \citep{albergo2022building,albergo2023stochastic}, the
  concept of stochastic interpolant is introduced. In \cite{albergo2022building}, the
  interpolation is deterministic (not necessarily linear), of the form
  $I_t(x_0,x_T) = \alpha(t) x_0 + \beta(t) x_T$, while in
  \cite{albergo2023stochastic}, the interpolation is stochastic given by 
  $I_t(x_0,x_T,z) = \alpha(t) x_0 + \beta(t) x_T + \gamma(t) \bfZ$ for  $\bfZ \sim \mathrm{N}(0, \Id)$. In
  \cite{albergo2022building}, an ODE is learned and the associated velocity
  field $v_\theta$ is obtained by minimizing the following objective
  \cite[Equation (9)]{albergo2022building}
  \begin{equation}
    \expeLigne{\Pi_{0,T}}{\normLigne{\partial_t I_t(\bfX_0, \bfX_T) - v_\theta(t,\bfX_t^{0,T})}^2}.
  \end{equation}
  Hence, if $I_t(x_0,x_T) = \tfrac{t}{T} x_0 + (1 - \tfrac{t}{T}) x_T$, we
  recover \eqref{eq:loss_tong}. 

  \paragraph{Link with Bridge Matching.} When $\sigma \to 0$ in Bridge
  Matching, we recover the same objective
  \eqref{eq:bridge_matching_loss_brownian} as the CFM objective
  \eqref{eq:cfm_objective}, since
  $\nabla \log \Qbb_{T|t}(\bfX_{T}|\bfX_t)=(\bfX_T - \bfX_t)/(\sigma^2
  (T-t))$.  Bridge Matching can also be applied to general distributions
  $\pi_0,\pi_T$; i.e. $\pi_0$ does not have to be restricted to a Gaussian. Therefore, Bridge Matching is a generalized version of CFM, see
  also \cite[Equation (10)]{liu2022let}. 

  \paragraph{Inverse problems and interpolation.} 
  \cite{somnath2023aligned} present a bridge matching algorithm to interpolate
  between aligned data $(\bfX_0,\bfX_T) \sim \Pi_{0,T}$. The objective in \cite[Equation (8)]{somnath2023aligned} is identical
  (setting $g_t = 1$) to the FM objective or the Bridge Matching objective
  \eqref{eq:interpolation_stochastic}. In the case $g_t = 1$, the measure
  $\Qbb_{|0,T}$ is associated with
  $\rmd \bfX_t^{0,T} = (x_T - \bfX_t^{0,T})/(T-t) \rmd t + \rmd \bfB_t$, which
  is identical to the second part of
  \eqref{eq:interpolation_stochastic}. Setting $\sigma_t = 1$, we recover the
  same objective in \cite{liu2023I2SB}. The measure $\Qbb_{|0,T}$ is given by
  \cite[Proposition 3.3]{liu2023I2SB} which, in the case $g_t =1$, is identical
  to the second part of \eqref{eq:interpolation_stochastic}. The main difference
  between \cite{liu2023I2SB} and \cite{somnath2023aligned} resides in the choice
  of $\Pi_{0,T}$. In the case of \cite{somnath2023aligned}, this choice is
  motivated by the access to \emph{aligned data} with applications to biology assuming they are distributed as the true Schr\"odinger static coupling, i.e. $\Pi_{0,T}=\PiSB_{0,T}$,
  whereas in \cite{liu2023I2SB}, $\Pi_{0,T}$ corresponds to a pairing between
  clean and corrupted images.

  Finally, in \cite[Equation (5)]{delbracio2023inversion} the authors consider a
  reconstruction process of the form
  \begin{equation}
    \label{eq:generative_T_to_zero}
    \rmd \bfX_t = (\CPELigne{\Pi_{0|t}}{\bfX_0}{\bfX_t} - \bfX_t)/t \rmd t , \qquad \bfX_T \sim \Pi_T ,
  \end{equation}
  where here we have replaced $F(x_t,t)$ by
  $\CPELigne{\Pi_{0|t}}{\bfX_0}{\bfX_t=x_t}$. This is justified if the
  $\| \cdot \|_p$ norm in \cite[Equation (4)]{delbracio2023inversion} is
  replaced by $\| \cdot \|_2^2$ (or any Bregman Loss Function, see
  \cite{banerjee2005optimality}).  In \cite{delbracio2023inversion}, $\Pi_{0,T}$
  corresponds to the joint distribution of clean and corrupted images as in
  \cite{liu2023I2SB}. $\Pi_0=\pi_0$ is the distribution of clean images and $\Pi_T=\pi_T$  is
  the distribution of corrupted images. Exchanging the role of $\Pi_0$ and
  $\Pi_T$, \eqref{eq:generative_T_to_zero} can be rewritten as
    \begin{equation}
    \label{eq:generative_zero_to_T}
    \rmd \bfX_t = (\CPELigne{\Pi_{T|t}}{\bfX_T}{\bfX_t} - \bfX_t)/(T-t) \rmd t , \qquad \bfX_0 \sim \Pi_0 . 
  \end{equation}
  Equivalently, we obtain the optimal CFM vector field in \eqref{eq:cfm_objective}. 
  Note that \cite{delbracio2023inversion} also incorporates a stochastic version
  of their objective \cite[Equation
  (7)]{delbracio2023inversion}. It remains an open question whether this objective can be understood as a special instance of the bridge matching framework.


\section{The Design Space of Brownian Bridges}
\label{sec:bridge_design_space_appendix}


\paragraph{From stochastic interpolants to Brownian bridges.} In this section,
we draw a link between our parameterization of bridges and the one used in
\cite{albergo2023stochastic}. In \cite{albergo2023stochastic}, a stochastic
interpolant is defined as
\begin{equation}
  \label{eq:ode_integral}
  \bfX_t = \bar{\alpha}_t x_0 + \bar{\beta}_t x_T + \bar{\gamma}_t \bfZ ,
\end{equation}
where $\bfZ \sim \mathrm{N}(0, \Id)$. Since their methodology and analysis
mainly relies on the probability flow, they work with \eqref{eq:ode_integral},
which is easier to analyse. In our setting, as we deal mostly with diffusions, it
is natural to parameterize Brownian bridges as follows
\begin{equation}
  \label{eq:sde_diff}
  \rmd \bfX_t =\{ -\alpha_t \bfX_t + \beta_t x_T \} \rmd t + \gamma_t \rmd \bfB_t . 
\end{equation}
The goal of this section is to derive explicit formulas between the parameters
$\bar{\alpha}_t$, $\bar{\beta}_t$ and $\bar{\gamma}_t$ of \eqref{eq:ode_integral} and
the parameters $\alpha_t$, $\beta_t$ and $\gamma_t$ of
\eqref{eq:sde_diff}. Consider $(\bfX_t)_{t \in \ccint{0,T}}$ given by
\eqref{eq:sde_diff}. We have that for any $t \in \ccint{0,T}$
\begin{equation}
  \textstyle \bfX_t = \exp[-A_t] x_0 + \int_0^t \beta_s \exp[A_s - A_t] \rmd s x_T  + \int_0^t \gamma_s \exp[A_s - A_t] \rmd \bfB_s ,
\end{equation}
where $A_t = \int_0^t \alpha_s \rmd s$. Therefore, we have that
\begin{equation}
  \label{eq:identification_design_dos}
  \textstyle \bar{\alpha}_t = \exp[-\int_0^t \alpha_s \rmd s] , \qquad \bar{\beta}_t = \int_0^t \beta_s \exp[-\int_s^t \alpha_u \rmd u] \rmd s , \qquad \bar{\gamma}_t^2 = \int_0^t \gamma_s^2 \exp[-2\int_s^t \alpha_u \rmd u] \rmd s ,
\end{equation}
\begin{equation}
  \label{eq:identificiation_design}
  \alpha_t = -\tfrac{\bar{\alpha}_t'}{\bar{\alpha}_t} , \qquad \beta_t = \bar{\beta}_t' + \bar{\beta}_t \alpha_t , \qquad \gamma_t^2 = (\bar{\gamma}_t^2)' + 2 \bar{\gamma}_t^2 \alpha_t  = 2 \bar{\gamma}_t \bar{\gamma}_t' + 2 \bar{\gamma}_t^2 \alpha_t .
\end{equation}
Using this relationship, we get that the Markovian projection, see \Cref{def:markovian_proj}, is given by
  \begin{equation}
    \rmd \bfX_t^{\star} = f_t^\star(\bfX_t) \rmd t + \gamma_t \rmd \bfB_t , \qquad f_t^\star(x_t) = \CPELigne{\Pi_{T|t}}{-\alpha_t \bfX_t + \beta_t x_T}{\bfX_t = x_t}.
  \end{equation}  
We have that
\begin{align}
  f_t^\star(x_t) &= \CPELigne{\Pi_{T|t}}{-\alpha_t \bfX_t + \beta_t x_T}{\bfX_t = x_t} \\
  &= \CPELigne{\Pi_{0,T|t}}{-\alpha_t( \bar{\alpha}_t x_0 + \bar{\beta}_t x_T + \bar{\gamma}_t \bfZ) + \beta_t x_T}{\bfX_t = x_t} .
\end{align}
Using \eqref{eq:identificiation_design}, we get that
\begin{equation}
  f_t^\star(x_t) = \CPELigne{\Pi_{0,T|t}}{\bar{\alpha}_t' x_0 + \bar{\beta}_t' x_T + \tfrac{\bar{\alpha}_t' \bar{\gamma}_t}{\bar{\alpha}_t}\bfZ}{\bfX_t=x_t}.
\end{equation}
In \cite{albergo2023stochastic}, it is shown that
$\nabla \log \Mbb^\star_t(x_t) = -\CPELigne{\Pi_{0,T|t}}{\bfZ}{\bfX_t
  =x_t}/\bar{\gamma}_t$, where $\Mbb^\star$ is the Markovian projection.  The
probability flow associated with $(\bfX^\star_t)_{t \in \ccint{0,T}}$ is given
by
\begin{align}
  \rmd \bfZ_t^\star &= \{f_t^\star( \bfZ_t^\star) - \tfrac{\gamma_t^2}{2} \nabla \log p_t( \bfZ_t^\star) \} \rmd t  \\
                    &= \{\CPELigne{\Pi_{0,T|t}}{\bar{\alpha}_t' x_0 + \bar{\beta}_t' x_T + (-\alpha_t \bar{\gamma}_t + \tfrac{\gamma_t^2}{2\bar{\gamma}_t})\bfZ}{\bfX_t=\bfZ_t^\star} \} \rmd t \\
                      &= \{\CPELigne{\Pi_{0,T|t}}{\bar{\alpha}_t' x_0 + \bar{\beta}_t' x_T + \bar{\gamma}_t' \bfZ}{\bfX_t=\bfZ_t^\star} \} \rmd t .
\end{align}
Hence, we recover \cite[Theorem 2.6]{albergo2023stochastic}.

\paragraph{Non-Markov path measures.} A natural question is whether
\eqref{eq:sde_diff} arises as the bridge measure of some \emph{Markov}
measure. For instance, if $\Qbb$ is associated with
$(x_0 + \bfB_t)_{t \in \ccint{0,T}}$, then pinning the process at $x_T$ at
time $T$, we get that the associated bridge measure $\Qbb_{|0,T}$ is given by
\begin{equation}
  \rmd \bfX_t^{0,T} = (x_T - \bfX_t)/(T-t) \rmd t + \rmd \bfB_t . 
\end{equation}
Therefore, we recover \eqref{eq:sde_diff} with
$\alpha_t = \beta_t = \tfrac{1}{T-t}$ and $\gamma_t = 1$. Using
\eqref{eq:identification_design_dos}, we get that
$\bar{\alpha}_t = 1 - \tfrac{t}{T}$, $\bar{\beta}_t = \tfrac{t}{T}$ and
$\bar{\gamma}_t^2 = (T-t)t/T$. We recover \eqref{eq:interpolation_stochastic},
upon noting that $\bfB_t - \tfrac{t}{T}\bfB_T$ is Gaussian with zero mean and
variance $(T-t)t/T$.

More generally, we consider a Markov measure $\Qbb$ associated with
$(\bfX_t)_{t \in \ccint{0,T}}$ such that
\begin{equation}
  \rmd \bfX_t = -a_t \bfX_t \rmd t + c_t \rmd \bfB_t , \qquad \bfX_0 = x_0 .
\end{equation}
We now derive the associated bridge measure $\Qbb_{|0,T}$:
\begin{equation}
  \textstyle \bfX_T = \exp[-\Lambda_T + \Lambda_t] \bfX_t   + \int_t^T c_s \exp[\Lambda_s - \Lambda_T] \rmd \bfB_s ,
\end{equation}
with $\Lambda_t = \int_0^t a_s \rmd s$.
We have that
\begin{align}
  \textstyle c_t^2 \nabla_{x_t} \log \Qbb_{T|t}(x_T|x_t)&\textstyle = (c_t^2 \exp[\Lambda_t - \Lambda_T] / \int_t^T c_s^2 \exp[2(\Lambda_s - \Lambda_T)] \rmd s) x_T  \\
  & \qquad \textstyle - (c_t^2 \exp[2(\Lambda_t - \Lambda_T)] / \int_t^T c_s^2 \exp[2(\Lambda_s - \Lambda_T)] \rmd s) x_t . 
\end{align}
Therefore, combining this result and \eqref{eq:bridge_forward}, we get that $\Qbb_{|0,T}$ is associated with
\begin{align}
  &\textstyle \alpha_t = a_t + c_t^2 \exp[-2\int_t^Ta_s \rmd s ] / \int_t^T c_s^2\exp[-2\int_s^Ta_u \rmd u ] \rmd s , \\
    &\textstyle \beta_t = c_t^2 \exp[-\int_t^Ta_s \rmd s ] / \int_t^T c_s^2\exp[-2\int_s^Ta_u \rmd u ] \rmd s , \qquad \gamma_t = c_t . 
\end{align}
In that case $(a_t,c_t)_{t \in \ccint{0,T}}$ entirely parameterize
$(\alpha_t, \beta_t, \gamma_t)_{t \in \ccint{0,T}}$. Hence, in the
Ornstein-Uhlenbeck setting, if $\Qbb_{|0,T}$ is the bridge of a Markov measure,
it is fully parameterized by two functions while in the non-Markov setting it is
parameterized by three functions.

In this paper, we present our framework in the Markovian setting as the
Schr\"odinger Bridge problem is usually defined with respect to Markov reference
measures. However, our methodology could be extended in a straightforward fashion
to the non-Markovian setting. This would allow for a further exploration of the
design space of DSBM.
  
\section{Proofs}
\label{sec:proofs_appendix}

\subsection{Proof of \Cref{prop:markovian-projection}}

We refer the reader to \cite{chung2006markov,rogers2000diffusions} for an introduction to Doob
$h$-transform. Our theoretical treatment of the Doob $h$-transform closely
follows \cite{palmowski2002technique}.

First, we introduce the \emph{infinitesimal generator} $\mathcal{A}$ given for
any $f \in \rmC_c^\infty(\ccint{0,T} \times \rset^d, \rset)$,
$t \in \ccint{0,T}$ and $x \in \rset^d$ by
\begin{equation}
  \label{eq:infinitesimal_generator_Q}
  \mathcal{A}f(t,x) = \langle f_t(x), \nabla f(t, x) \rangle + \tfrac{\sigma_t^2}{2} \Delta f(t,x) + \partial_t f(t,x) . 
\end{equation}
The following assumption ensures that the diffusion associated with $\Qbb$ as well as its Markovian projections are well-defined.

\begin{assumption}
  \label{assum:nice_diffusion}
  $f$, $\sigma$ and
  $(t, x_t) \mapsto \CPELigne{\Pi_{T|t}}{\nabla \log
    \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t=x_t}$ are locally Lipschitz and there
  exist $C > 0 $, $\psi \in \rmC(\ccint{0,T}, \rset_+)$ such that for any
  $t \in \ccint{0,T}$ and $x_0, x_t \in \rset^d$, we have
 \begin{align}
    &\normLigne{f_t(x_t)} \leq C(1 + \normLigne{x_t}) , \qquad C \geq \sigma_t \geq 1/C , \\
        &\normLigne{\CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t=x_t}} \leq C\psi(t)(1 + \normLigne{x_t}) .
  \end{align}
\end{assumption}

We consider the following assumption, which will ensure that we can apply Doob
$h$-transform techniques.

\begin{assumption}
  \label{assum:good_function_palmowski}
  For any $x_0 \in \rset^d$, $\Pi_{T|0}$ is absolutely continuous
  w.r.t. $\Qbb_{T|0}$.  For any $x_0 \in \rset^d$, let $\varphi_{T|0}$ be given
  for any $x_T \in \rset^d$ by
  $\varphi_{T|0}(x_T|x_0) = \rmd \Pi_{T|0}(x_T|x_0) / \rmd
  \Qbb_{T|0}(x_T|x_0)$ and assume that for any $x_0 \in \rset^d$,
  $x_T \mapsto \varphi_{T|0}(x_T|x_0)$ is bounded. For any $x_0 \in \rset^d$,
  let $\varphi_{t|0}$ given for any $x_t \in \rset^d$ and $t \in \ccint{0,T}$ by
  \begin{equation}
    \label{eq:def_h_transform}
    \textstyle
    \varphi_{t|0}(x_t|x_0) = \int_{\rset^d} \varphi_{T|0}(x_T|x_0) \rmd \Qbb_{T|t} (x_T|x_t) .
  \end{equation}
  Finally, we assume that for any $x_0 \in \rset^d$,
  $(t, x_t) \mapsto 1/\varphi_{t|0}(x_t|x_0)$ and
  $(t, x_t) \mapsto \mathcal{A} \varphi_{t|0}(x_t|x_0)$ are bounded.
\end{assumption}

This means that for any $x_0 \in \rset^d$, $(t, x_t) \mapsto \varphi_t(x_t|x_0)$
is a \emph{good function} in the sense of \cite[Proposition
3.2]{palmowski2002technique}.  Note here that these assumptions could be relaxed
on a case-by-case basis. We leave this study for future work.

The following lemma, is a direct consequence of
\Cref{assum:good_function_palmowski} and \eqref{eq:def_h_transform}. It ensures
that the $h$-function $\varphi_{t|0}$ satisfies the backward Kolmogorov
equation.

\begin{lemma}
  \label{lemma:backward_kolmogorov}
  Assume \tref{assum:good_function_palmowski}. Then,
  $\varphi \in \rmC^{1,2}(\coint{0,T} \times \rset^d, \rset)$ and
  $\mathcal{A} \varphi_{|0}=0$.
\end{lemma}

Using \eqref{eq:infinitesimal_generator_Q}, we have that for any
$x_0 \in \rset^d$ and $f \in \rmC^\infty_c(\ccint{0,T} \times \rset^d, \rset)$,
$t \in \ccint{0,T}$ and $x_t \in \rset^d$
\begin{equation}
  \label{eq:doob_h_generator}
  ( \mathcal{A}(f \varphi_{|0}) - f \mathcal{A} \varphi_{|0})(t,x_t) / \varphi_{|0}(t,x_t) = \mathcal{A} f(t,x_t) + \sigma_t^2 \langle \nabla f(t,x_t) , \nabla \log \varphi_{t|0}(x_t|x_0) \rangle . 
\end{equation}

Finally, we
consider the following assumption, which will ensure that the Doob $h$-transform
is well-defined. 

\begin{assumption}
  \label{assum:doob_h_defined}
  For any $x_0 \in \rset^d$, there exists $C \geq 0$ such that for
  any $t \in \ccint{0,T}$ and $x_t \in \rset^d$,
  $\normLigne{\nabla \log \varphi_{t|0}(x_t|x_0)} \leq C(1 +\normLigne{x_0}+\normLigne{x_t})$. 
\end{assumption}


We are now ready to state and prove \Cref{prop:markovian-projection}. Note that
the Markovian projection is defined in \Cref{def:markovian_proj}. Finally, we
define $\calM$ the space of path measures such that
$\Pbb \in \calM$ if $\Pbb$ is associated with
$\rmd \bfX_t = \{f_t(\bfX_t) + v_t(\bfX_t) \} \rmd t + \sigma_t \rmd \bfB_t$,
with $\sigma,v$ locally Lipschitz. This restriction of Markov measures allows
us to apply the entropic version of the Girsanov theorem
\citep{leonard2012girsanov}. It has no impact on our methodology.

\begin{proposition}
  \label{prop:markovian-projection_appendix}
  Assume \tref{assum:nice_diffusion}, \tref{assum:good_function_palmowski},
  \tref{assum:doob_h_defined}. Let $\Mbb^\star = \projM(\Pi)$. Then, 
  \begin{align}    
    &\textstyle \Mbb^\star = \argmin_{\Mbb} \ensembleLigne{\KLLigne{\Pi}{\Mbb}}{\Mbb \in \calM} , \\
    & \textstyle \KLLigne{\Pi}{\Mbb^\star} = \tfrac{1}{2} \int_0^T \expeLigne{\Pi_{0,t}}{\normLigne{\sigma_t^2 \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t} - v_t^\star}^2}/\sigma_t^2 \rmd t  .
  \end{align}
  In addition, we have that for any $t \in \ccint{0,T}$, $\Mbb^\star_t = \Pi_t$. In particular, $\Mbb^\star_T = \Pi_T$.
\end{proposition}


\begin{proof}
  First, we recall that $\Pi$ is given by $\Pi = \Qbb \varphi_{0,T}$ with
  $\varphi_{0,T} = \tfrac{\rmd \Pi_{0,T}}{\rmd \Qbb_{0,T}}$. In particular, we
  have $\Pi_{|0} = \Qbb_{|0} \varphi_{T|0}$, where
  $\varphi_{T|0} = \tfrac{\rmd \Pi_{T|0}}{\rmd \Qbb_{T|0}}$. Therefore, using
  \Cref{lemma:backward_kolmogorov}, \cite[Lemma 3.1, Lemma
  4.1]{palmowski2002technique}, the remark following \cite[Lemma
  4.1]{palmowski2002technique}, \tref{assum:nice_diffusion},
  \tref{assum:good_function_palmowski} and \tref{assum:doob_h_defined}, we get
  that $\Pi_{|0}$ is Markov and associated with the distribution of
  $(\bfX_t)_{t \in \ccint{0,T}}$ given for any $t \in \ccint{0,T}$ by 
  \begin{equation}
    \label{eq:non_markov}
    \textstyle \bfX_t = \int_0^t \{ f_s(\bfX_s) + \sigma_s^2 \nabla \log \varphi_{s|0}(\bfX_s|\bfX_0) \} \rmd s + \int_0^t \sigma_s \rmd \bfB_s ,
  \end{equation}
  where for any $t \in \ccint{0,T}$, $x_0, x_t \in \rset^d$ we recall that 
  \begin{equation}
    \label{eq:varphi_int_def}
    \textstyle \varphi_{t|0}(x_t|x_0) = \int_{\rset^d} \varphi_{T|0}(x_T|x_0) \rmd \Qbb_{T|t}(x_T|x_t) . 
  \end{equation}
  First, we have that for any $t \in \ccint{0,T}$, $x_t, x_0 \in \rset^d$
  \begin{equation}
    \textstyle \Qbb_{t|0}(x_t|x_0) \varphi_{t|0}(x_t|x_0) = \int_{\rset^d} \Qbb_{t|0,T}(x_t|x_T,x_0) \rmd \Pi_{T|0}(x_T|x_0) = \Pi_{t|0}(x_t|x_0) . 
  \end{equation}
  Therefore, we get that for any $t \in \ccint{0,T}$ and $x_t, x_0 \in \rset^d$
  \begin{equation}
    \label{eq:varphi_integrate}
    \varphi_{t|0}(x_t|x_0) = \tfrac{\rmd \Pi_{t|0}(x_t|x_0)}{\rmd \Qbb_{t|0}(x_t|x_0)} . 
  \end{equation}
    In addition, we have the following identity for any $t \in \ccint{0,T}$, $x_0, x_t, x_T \in \rset^d$
  \begin{equation}
    \Qbb_{T|0}(x_T|x_0) \Qbb_{t|0,T}(x_t|x_0,x_T) = \Qbb_{t|0}(x_t|x_0) \Qbb_{T|t}(x_T|x_t) .
  \end{equation}
  Using \eqref{eq:varphi_int_def}, this result and \eqref{eq:varphi_integrate},
  we get that for any $t \in \ccint{0,T}$ and $x_0,x_t \in \rset^d$
  \begin{align}
    \textstyle \nabla \log \varphi_{t|0}(x_t|x_0) &\textstyle= \int_{\rset^d} \tfrac{\Pi_{T|0}(x_T|x_0) \Qbb_{T|t}(x_T|x_t)}{\Qbb_{T|0}(x_T|x_0) \varphi_{t|0}(x_t|x_0)} \nabla \log \Qbb_{T|t}(x_T|x_t) \rmd x_T \\
                                                  &\textstyle= \int_{\rset^d} \tfrac{\Pi_{T|0}(x_T|x_0) \Qbb_{t|0,T}(x_t|x_0,x_T)}{\Qbb_{t|0}(x_t|x_0) \varphi_{t|0}(x_t|x_0)} \nabla \log \Qbb_{T|t}(x_T|x_t) \rmd x_T \\
    &\textstyle= \int_{\rset^d} \tfrac{\Pi_{t,T|0}(x_t,x_T|x_0)}{\Pi_{t|0}(x_t|x_0)} \nabla \log \Qbb_{T|t}(x_T|x_t) \rmd x_T \\    
                                                  &= \textstyle  \int_{\rset^d} \nabla \log \Qbb_{T|t}(x_T|x_t) \rmd \Pi_{T|t,0}(x_T|x_t,x_0) . 
  \end{align}
  Hence, combining this result and \eqref{eq:non_markov}, we get 
  \begin{equation}
    \label{eq:non_markov_2}
    \textstyle \bfX_t = \int_0^t \{ f_s(\bfX_s) + \sigma_s^2 \CPELigne{\Pi_{T|t, 0}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t, \bfX_0} \} \rmd s + \int_0^t \sigma_s \rmd \bfB_s .
  \end{equation}
  Let $\Mbb$ be Markov defined by
  $\rmd \bfX_t = \{f_t(\bfX_t) + v_t(\bfX_t) \} \rmd t + \sigma_t \rmd \bfB_t$,
  such that $\KLLigne{\Pi}{\Mbb} < +\infty$ with $\sigma,v$ locally
  Lipschitz. Using \cite[Theorem 2.3]{leonard2012girsanov}, we get that
  \begin{equation}
    \textstyle \KLLigne{\Pi}{\Mbb} = \tfrac{1}{2} \int_0^T \expeLigne{\Pi_{0,t}}{\| \sigma_t^2\CPELigne{\Pi_{T|t, 0}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t, \bfX_0} - v_t(\bfX_t) \|^2} / \sigma_t^2 \rmd t . 
  \end{equation}
  In addition, we have that for any $t \in \ccint{0,T}$,
  \begin{align}
    &\expeLigne{\Pi_{0,t}}{\| \sigma_t^2\CPELigne{\Pi_{T|t, 0}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t, \bfX_0} - v_t(\bfX_t)\|^2} \\
    & \geq \expeLigne{\Pi_{0,t}}{\| \sigma_t^2\CPELigne{\Pi_{T|t, 0}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t, \bfX_0} - v^\star_t(\bfX_t) \|^2} ,
  \end{align}
  where
$v^\star_t(x_t) = \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log
    \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t=x_t}$ which concludes the first part of
  the proof. For the second part of the proof, we show that for any
  $t \in \ccint{0,T}$, we have $\Mbb^\star_t = \Pi_t$. First, we have that $\Mbb^\star_t$
  and $\Pi_t$ satisfy the same Fokker-Planck equation, see \cite[Theorem
  2]{peluchettinon} for instance. We conclude by uniqueness of the solutions of
  the Fokker-Planck equation under \tref{assum:nice_diffusion} and
  \tref{assum:doob_h_defined}, see \cite{bogachev2021uniqueness} for instance.
\end{proof}


\subsection{Proof of \Cref{prop:reciprocal-projection}}
\begin{proof}
  By the additive property of KL divergence \citep{leonard2014some},
  $\KLLigne{\Pbb}{\Pi}=\KLLigne{\Pbb_{0,T}}{\Pi_{0,T}} +
  \expeLigne{\Pbb_{0,T}}{\KLLigne{\Pbb_{|0,T}}{\Pi_{|0,T}}}$. Restricting
  $\Pi_{|0,T}=\Qbridge$ directly gives that the KL minimizer is $\Pi^\star$
  with $\Pi^\star_{0,T}=\Pbb_{0,T}$, and thus
  $\Pi^\star=\int \Qbridge \rmd \P_{0,T}$.
\end{proof}


\subsection{Proof of \Cref{prop:schro-markov-recip}}

This result is a direct consequence of \cite[Theorem 2.12]{leonard2014survey},
which we recall here for completeness.

\begin{proposition}
  Assume
  that $\Qbb \in \calM$, 
  that $\Qbb_0 = \Qbb_T = \bar{\Qbb}$, that for any
  $x_0, x_T \in \rset^d$,
  $\rmd \Qbb_{0,T} / \rmd (\bar{\Qbb} \otimes \bar{\Qbb})(x_0,x_T) \geq
  \exp[-A(x_0) - A(x_T)]$ with $A \geq 0$ measurable,
  $\int_{\rset^d \times \rset^d} \exp[-B(x_0) - B(x_T)] \rmd \Qbb(x_0,x_T) <
  +\infty$ with $B \geq 0$ measurable.
  Assume that there exists
  $t_0 \in \ooint{0,T}$ and $\msx$ measurable such that $\Qbb_{t_0}(\msx) > 0$
  and for all $x \in \msx$,
  $\Qbb_{0,T} \ll \Qbb_{0,T|t_0}(\cdot|\bfX_{t_0} =x)$.  In addition, assume
  that $\KLLigne{\pi_0}{\bar{\Qbb}} <+\infty$,
  $\KLLigne{\pi_T}{\bar{\Qbb}} <+\infty$,
  $\int_{\rset^d} (A+B)(x_0) \rmd \pi_0(x_0) < +\infty$,
  $\int_{\rset^d} (A+B)(x_T) \rmd \pi_T(x_T) < +\infty$.
  
  Then
  there exists a unique Schr\"odinger Bridge $\PSB$. In addition let $\Pbb$ be a
  Markov measure in the reciprocal class of $\Qbb$ such that
  $\Pbb_0 = \pi_0$ and $\Pbb_T = \pi_T$. Assume that
  $\KLLigne{\Pbb}{\Qbb} < +\infty$.  Then $\Pbb$ is the unique  Schr\"odinger Bridge $\PSB$.
\end{proposition}

\begin{proof}
  The first part of the proof is a consequence of \cite[Theorem
  2.12(a)]{leonard2014survey}. The second part is a consequence of \cite[Theorem
  2.12(b)]{leonard2014survey} and \cite[Theorem 2.14]{leonard2014reciprocal}.
\end{proof}


\subsection{Proof of \Cref{lemma:pythagorean_theorem}}

\begin{lemma}
  \label{lemma:pythagorean_theorem_appendix}
  Let $\Mbb \in \calM$ and $\Pi \in \calR(\Qbb)$ and assume \tref{assum:nice_diffusion},
  \tref{assum:good_function_palmowski}, \tref{assum:doob_h_defined}. If $\KLLigne{\Pi}{\Mbb}< +\infty$ and
  $\KLLigne{\projM(\Pi)}{\Mbb}< +\infty$ we have
  \begin{equation}
    \label{eq:pythagoran_one}
    \KLLigne{\Pi}{\Mbb} = \KLLigne{\Pi}{\projM(\Pi)} +  \KLLigne{\projM(\Pi)}{\Mbb} . 
  \end{equation}
  For any $\Pbb \in \pathmeas$, if $\KLLigne{\Pbb}{\Pi} < +\infty$, we have 
  \begin{equation}
    \label{eq:pythagoran_two}
    \KLLigne{\Pbb}{\Pi} = \KLLigne{\Pbb}{\projR{\Qbb}(\Pbb)} +  \KLLigne{\projR{\Qbb}(\Pbb)}{\Pi} . 
  \end{equation}  
\end{lemma}


\begin{proof}
  We start with the proof of \eqref{eq:pythagoran_one}. Similarly to
  \Cref{prop:markovian-projection}, where we have
  $\Mbb \in \calM$ to ensure that we can apply \cite[Theorem
  2.3]{leonard2012girsanov}, we get
  \begin{equation}
    \textstyle \KL(\Pi | \Mbb) = \tfrac{1}{2} \int_0^T \expeLigne{\Pi_{0,t}}{\normLigne{v_t(\bfX_t) - \sigma_t^2 \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t}}^2} / \sigma_t^2 \rmd t .
  \end{equation}
  In addition, we have
  \begin{equation}
    \textstyle \KL(\projM(\Pi) | \Mbb) = \tfrac{1}{2} \int_0^T \expeLigne{\Pi_{t}}{\normLigne{v_t(\bfX_t) - \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t}}^2} / \sigma_t^2 \rmd t .
  \end{equation}
  Finally, using \Cref{prop:markovian-projection_appendix}, we have that
  \begin{align}
    & \textstyle \KLLigne{\Pi}{\projM(\Pi)} \\
    &  \textstyle = \tfrac{1}{2} \int_0^T \expeLigne{\Pi_{0,t}}{\normLigne{ \sigma_t^2 \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t} - \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_t} }^2} / \sigma_t^2 \rmd t  \\
    &  \textstyle =  \tfrac{1}{2} \int_0^T (\expeLigne{\Pi_{0,t}}{\normLigne{ \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t}}^2} - \expeLigne{\Pi_{t}}{\normLigne{ \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_t} }^2}) \sigma_t^2 \rmd t  .    
  \end{align}
  Using this result, we have
  \begin{align}
    &2 \textstyle \KLLigne{\Pi}{\projM(\Pi)} + 2\KLLigne{\projM(\Pi)}{\Mbb}  \\
    & = \textstyle \int_0^T (\expeLigne{\Pi_{0,t}}{\normLigne{ \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t}}^2} - \expeLigne{\Pi_{t}}{\normLigne{\CPELigne{\Pi_{T|t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_t} }^2}) \sigma_t^2 \rmd t \\
    & \textstyle + \int_0^T \expeLigne{\Pi_{t}}{\normLigne{v_t(\bfX_t) / \sigma_t^2 - \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t}}^2} \sigma_t^2 \rmd t \\
    & = \textstyle \int_0^T (\expeLigne{\Pi_{0,t}}{\normLigne{ \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t}}^2} - \expeLigne{\Pi_{t}}{\normLigne{\CPELigne{\Pi_{T|t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_t} }^2}) \sigma_t^2 \rmd t \\
    & \textstyle + \int_0^T (\expeLigne{\Pi_{t}}{\normLigne{v_t(\bfX_t) / \sigma_t^2}^2} + \expeLigne{\Pi_{t}}{\normLigne{\CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t}}^2}) \sigma_t^2 \rmd t \\
    & \textstyle - 2 \int_0^T \expeLigne{\Pi_{t}}{\langle v_t(\bfX_t) / \sigma_t^2, \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t} \rangle} \sigma_t^2 \rmd t  \\
 & = \textstyle \int_0^T \expeLigne{\Pi_{0,t}}{\normLigne{ \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t}}^2} \sigma_t^2 \rmd t  + \int_0^T \expeLigne{\Pi_{t}}{\normLigne{v_t(\bfX_t) / \sigma_t^2}^2} \sigma_t^2 \rmd t \\
    & \textstyle - 2 \int_0^T \expeLigne{\Pi_{t}}{\langle v_t(\bfX_t) / \sigma_t^2, \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t} \rangle} \sigma_t^2 \rmd t  \\
 & = \textstyle \int_0^T \expeLigne{\Pi_{0,t}}{\normLigne{ \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t}}^2} \sigma_t^2 \rmd t  + \int_0^T \expeLigne{\Pi_{t}}{\normLigne{v_t(\bfX_t) / \sigma_t^2}^2} \sigma_t^2 \rmd t \\
& \textstyle - 2 \int_0^T \expeLigne{\Pi_{0,t}}{\langle v_t(\bfX_t) / \sigma_t^2, \CPELigne{\Pi_{T|0,t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_0, \bfX_t} \rangle} \sigma_t^2 \rmd t = 2 \KLLigne{\Pi}{\Mbb} ,       
  \end{align}
  which concludes the first part of the proof.

  For \eqref{eq:pythagoran_two}, define $\Pistar = \projR{\Qbb}(\Pbb) = \int \Qbb_{|0,T} \rmd \Pbb_{0,T}$. Using
  \cite[Equation 2.6]{csiszar1975divergence}, we have
  \begin{align}
    \KLLigne{\Pbb}{\Pi} &= \textstyle \KLLigne{\Pbb}{\Pistar} + \int_{\calC} \log(\tfrac{\rmd \Pistar}{\rmd \Pi}(\omega)) \rmd \Pbb(\omega) \\
    &\textstyle = \KLLigne{\Pbb}{\Pistar} + \int_{\rset^d \times \rset^d} \log(\tfrac{\rmd \Pistar_{0,T}}{\rmd \Pi_{0,T}}(x_0,x_1)) \rmd \Pbb_{0,T}(x_0,x_1) \\
    &\textstyle = \KLLigne{\Pbb}{\Pistar} + \int_{\rset^d \times \rset^d} \log(\tfrac{\rmd \Pistar_{0,T}}{\rmd \Pi_{0,T}}(x_0,x_1)) \rmd \Pistar_{0,T}(x_0,x_1)  = \KLLigne{\Pbb}{\Pistar} + \KLLigne{\Pistar}{\Pi} , 
  \end{align}
  which concludes the proof.
\end{proof}


\subsection{Proof of \Cref{prop:convergence_marginals}}

\begin{proposition}
  \label{prop:convergence_marginals_appendix}
  Assume that the conditions of \Cref{prop:schro-markov-recip} and
  \Cref{lemma:pythagorean_theorem} apply for $\Pbb^n$ for every
  $n \in \nset$ and for the \schro Bridge $\PSB$,
  we have
  $\lim_{n \to +\infty} \KLLigne{\Pbb^n}{\Pbb^{n+1}} = 0$. 
  If $\Pstar$ is a fixed point of $(\Pbb^n)_{n \in \nset}$, then $\Pstar$
  is the Schr\"odinger Bridge.
\end{proposition}

\begin{proof}
  The second part of the proof is a direct application of
  \Cref{prop:schro-markov-recip}, since in that case $\Pstar$ is Markov and
  reciprocal and admits $\pi_0,\pi_T$ as marginals. 
  For the first part of the proof, we follow the technique of
  \cite{ruschendorf1995convergence} but for the \emph{reverse} Kullback--Leibler
  divergence. Applying \Cref{lemma:pythagorean_theorem}, we get for any $N \in \nset$
  \begin{equation}
    \textstyle \KLLigne{\Pbb^0}{\Pstar} = \KLLigne{\Pbb^0}{\Pbb^1} + \KLLigne{\Pbb^1}{\Pstar} = \sum_{i=0}^{N} \KLLigne{\Pbb^i}{\Pbb^{i+1}} + \KLLigne{\Pbb^{N+1}}{\Pstar} ,
    \end{equation}
    which concludes the proof.
\end{proof}


\subsection{Proof of \Cref{prop:forward_backward_markov_proj}}
\begin{proof}
The proof is similar to the one of \Cref{prop:markovian-projection}.
\end{proof}

In particular, the time-reversal of $\Qbb_{|0,T}(\cdot|x_0,x_T)$ is associated with
\begin{equation}
  \rmd \bfY_t^{0,T} = \{-f_{T-t}(\bfY_t^{0,T}) + \sigma_{T-t}^2 \nabla \log \Qbb_{T-t|0}(\bfY_t^{0,T}|x_0)\} \rmd t + \sigma_{T-t} \rmd \bfB_t, \qquad \bfY_0^{0,T} = x_T.
  \label{eq:bridge_backward}
\end{equation}
One can view both \eqref{eq:forward_markov_proj} \eqref{eq:backward_markov_proj} as SDEs with drift defined as the conditional expectation of the drift of \eqref{eq:bridge_forward} \eqref{eq:bridge_backward} under $\Pi_{0,T|t}$ in the forward and backward directions respectively. 

\subsection{Proof of \Cref{prop:dsb_dsbm}}
\begin{proof}
We proceed by induction. 
Firstly, for $\Pi^0_{0,T}=\Q_{0,T}$, $\Pi^0=\tilde{\Pbb}^0=\Qbb$ at initialization. 
We can also define $\Mbb^0=\Qbb$, such that $\Mbb^0=\tilde{\Pbb}^0$ and $\Pi^n=\projR{\Qbb}(\Mbb^n)$ for all $n \in \nset$. 
By \cite[Section 3.5]{debortoli2021diffusion}, the optimal DSB sequence $\tilde{\Pbb}^n$ is Markov and $\tilde{\Pbb}^n = \int \Qbridge \rmd \tilde{\Pbb}_{0,T}^n$, where $\tilde{\Pbb}_{0,T}^n$ is the IPF sequence of the static SB problem. In other words, $\tilde{\Pbb}^n \in \calM \cap \calR(\Qbb) $. 

Suppose $\Mbb^{2n+1}=\tilde{\Pbb}^{2n+1}$. By definition, $\Mbb^{2n+2}_0=\tilde{\Pbb}^{2n+2}_0=\pi_0$, i.e. both forward processes are initialized at $\pi_0$. In DSB, by \cite{debortoli2021diffusion}, $\tilde{\Pbb}^{2n+2}$ is defined as the time-reversal of $\tilde{\Pbb}^{2n+1}$, such that $\tilde{\Pbb}^{2n+2}_{|0}=\tilde{\Pbb}^{2n+1}_{|0}$. Hence, $\tilde{\Pbb}^{2n+2} = \int \tilde{\Pbb}^{2n+1}_{|0} \rmd \pi_0$. 

In DSBM, we first perform reciprocal projection $\Pi^{2n+1} = \projR{\Qbb}{(\Mbb^{2n+1})} = \int \Qbridge \rmd \Mbb_{0,T}^{2n+1}$. Since $\Mbb^{2n+1}=\tilde{\Pbb}^{2n+1} \in \calR(\Qbb)$, however, we have that $\Pi^{2n+1}=\Mbb^{2n+1}$. Furthermore, since $\Mbb^{2n+1}=\tilde{\Pbb}^{2n+1} \in \calM$, $\projM{(\Pi^{2n+1})}=\projM{(\Mbb^{2n+1})}=\Mbb^{2n+1}$. Thus, $\Mbb^{2n+2}$ given by \eqref{eq:approximate_markovian_proj_forward} is such that $\Mbb^{2n+2}_0=\pi_0$ and $\Mbb_{|0}^{2n+2}=\projM{(\Pi^{2n+1})}_{|0}=\Mbb_{|0}^{2n+1}$. We conclude that $\Mbb^{2n+2}=\int \Mbb_{|0}^{2n+1} \rmd \pi_0 = \tilde{\Pbb}^{2n+2}$. Similar arguments holds for the the reverse projection \eqref{eq:approximate_markovian_proj_backward}. Therefore, $\Mbb^n = \tilde{\Pbb}^n$ for all $n \in \nset$. 
\end{proof}



\section{The set of Markov measures is not convex}
\label{sec:set-markov-measures}

Let $p_1(x_0,x_1,x_2) = p_1(x_0)p_1(x_1|x_0)p_1(x_2|x_1)$ and
$p_2(x_0,x_1,x_2) = p_2(x_0)p_2(x_1|x_0)p_2(x_2|x_1)$ on $\{0,1\}^3$ such that
\begin{equation}
  p_1(x_0 = 1) = \alpha_0, \qquad p_1(x_1=1|x_0) = \alpha_1 , \qquad p_1(x_2=1|x_1) = \alpha_2 . 
\end{equation}
Additionally, we set 
\begin{equation}
  p_2(x_0 = 1) = \beta_0, \qquad p_2(x_1=1|x_0) = \beta_1 , \qquad p_2(x_2=1|x_1) = \beta_2 . 
\end{equation}
Finally, we set $q = (1/2)p_1 + (1/2)p_2$.  Consider
$q(x_2=1|x_1=1,x_0=1) = q(x_2=1,x_1=1,x_0=1)/q(x_1=1,x_0=1)$ and
$q(x_2=1|x_1=1) = q(x_2=1,x_1=1)/q(x_1=1)$. Let
\begin{align}
  \Delta &=  4[q(x_2=1,x_1=1,x_0=1)q(x_1=1) - q(x_2=1,x_1=1)q(x_1=1,x_0=1)] \\
         &= (\alpha_0\alpha_1\alpha_2 + \beta_0 \beta_1 \beta_2)(\alpha_1 + \beta_1) - (\alpha_1\alpha_2 + \beta_1\beta_2)(\alpha_0\alpha_1 + \beta_0\beta_1) \\
         &= \alpha_0 \alpha_1 \beta_1 \alpha_2 +\beta_0 \alpha_1 \beta_1 \beta_2 - \beta_0 \alpha_1 \beta_1 \alpha_2 - \alpha_0 \alpha_1 \beta_1 \beta_2 \\
         &=\alpha_1\beta_1 \beta_2(\beta_0 - \alpha_0) + \alpha_1\beta_1\alpha_2 (\alpha_0 - \beta_0) \\
  &=\alpha_1\beta_1(\beta_0 - \alpha_0)(\beta_2 - \alpha_2) .
\end{align}
$q$ is Markov if and only if $\Delta =0$. Therefore $q$ is not Markov as soon as
$\alpha_0 \neq \beta_0$ and $\alpha_2 \neq \beta_2$.


\section{Discrete-Time Markovian Projection}
\label{sec:discrete_time_mp_appendix}
We derive in this section a discrete-time version of the Markovian
projection and show that, in some limiting case, we recover the continuous-time
projection. In the discrete case, we let \begin{align}
  \label{eq:def_q_discret}
  \textstyle
  \pi(x_{0:N}) &\textstyle=\pi(x_0, x_N) \prod_{k=0}^{N-1} q_{k+1|0,k,N}(x_{k+1}|x_0, x_k, x_N) \\
  &\textstyle= \pi(x_0, x_N)\prod_{k=0}^{N-2} q_{k+1|k,N}(x_{k+1}|x_k, x_N)  .
\end{align}
We consider a Markovian measure $p$ given by
$p(x_{0:N}) = p(x_0) \prod_{k=0}^{N-1} p_{k+1|k}(x_{k+1} | x_k)$.  Now let us
compute $\KLLigne{\pi}{p}$. We have
\begin{align}
  \KLLigne{\pi(x_{0:N})}{p(x_{0:N})} &= \textstyle \sum_{k=0}^{N-2} \int_{\rset^d \times \rset^d} \KLLigne{q_{k+1|k,N}}{p_{k+1|k}} \pi_{k,N}(x_k,x_N) \rmd x_k \rmd x_N \\
  & \qquad \textstyle + \KLLigne{\pi_0}{p_0} + \int_{\rset^d} \KLLigne{\pi(x_N|x_0)}{p(x_{N}|x_{N-1})} \pi(x_0, x_{N-1}) \rmd x_0 \rmd x_{N-1} .
\end{align}
In what follows, we denote
\begin{align}
  & \mathcal{L}_0 = \KLLigne{\pi_0}{p_0} , \textstyle  \mathcal{L}_{N} = \int_{\rset^d} \KLLigne{\pi(x_N|x_0)}{p(x_{N}|x_{N-1})} \pi(x_0, x_{N-1}) \rmd x_0 \rmd x_{N-1},  \\
  & \textstyle  \mathcal{L}_{k+1} = \int_{\rset^d \times \rset^d} \KLLigne{q_{k+1|k,N}}{p_{k+1|k}} \pi_{k,N}(x_k,x_N) \rmd x_k \rmd x_N , \end{align}
We have the following proposition.
\begin{proposition}
  The minimizer $p_{k+1|k}$ of $\mathcal{L}_{k+1}$ is given by
  \begin{equation}
    \label{eq:definition_discrete}
    \textstyle p_{k+1|k}(x_{k+1} | x_k) = \int_{\rset^d} q_{k+1|k,N}(x_{k+1}|x_k, x_N)  \pi_{N|k}(x_N|x_k) \rmd x_N .
  \end{equation}
  If $p_0 = q_0$, then  for any $k \in \{0, \dots, N-1\}$,
  $p_k = \pi_k$.  In addition, assume that
  $p_{k+1|k}(x_{k+1}|x_k) = \exp[-\normLigne{x_{k+1} - x_k - \gamma
    f(x_k)}^2/(2\gamma)] / (2 \uppi \gamma)^{-d/2}$ and
  $ q_{k+1|k,N}(x_{k+1}|x_k, x_N) = \exp[-\normLigne{x_{k+1} - x_k - \gamma
    f(x_k, x_N)}^2/(2\gamma)] / (2 \uppi \gamma)^{d/2}$.  Finally, assume that
  $\normLigne{x_{k+1} - x_k} \leq \gamma^{1/2}$.  Then, we have that
  \begin{equation}
    \label{eq:limit_continuous}
    \textstyle f(x_k) = \int_{\rset^d} f(x_k, x_N) \pi(x_N|x_k) \rmd x_N + o(\gamma^{1/2}).
  \end{equation}
\end{proposition}

\begin{proof}
  The proofs of \eqref{eq:definition_discrete} and \eqref{eq:limit_continuous}
  are straightforward and left to the reader. We now prove that if $p_0 = \pi_0$, then for any
  $k \in \{1, \dots, N\}$, $p_k = \pi_k$. First, we have that for any
  $k \in \{0, \dots, N-1\}$,
  \begin{equation}
    \pi(x_k, x_{k+1},x_N) = \pi(x_k,x_N)q(x_{k+1}|x_k,x_N) . 
  \end{equation}
  Assume now that $p_k=\pi_k$, then we have
  \begin{align}
    p_{k+1}(x_{k+1}) &\textstyle = \int_{\rset^d} p_k(x_k) p_{k+1|k}(x_{k+1}|x_k) \rmd x_k\\
     &= \textstyle \int_{\rset^d \times \rset^d} p_k(x_k) q_{k+1|k,N}(x_{k+1}|x_k,x_N) \pi_{N|k}(x_N|x_k) \rmd x_k \rmd x_N \\
     &= \textstyle \int_{\rset^d \times \rset^d} \pi_k(x_k) q_{k+1|k,N}(x_{k+1}|x_k,x_N) \pi_{N|k}(x_N|x_k) \rmd x_k \rmd x_N \\
      &= \textstyle \int_{\rset^d \times \rset^d} \pi_{k,k+1,N}(x_k,x_{k+1},x_N) \rmd x_k \rmd x_N = \pi_{k+1}(x_{k+1}),
  \end{align}
  which concludes the proof.
\end{proof}

In particular, in the previous proposition, if
$f(x_k, x_N) = \nabla \log q(x_N|x_k)$, i.e. we have a discretization of the
bridge then
$f(x_k) = \int_{\rset^d} \nabla \log q(x_N|x_K) \pi(x_N|x_k) \rmd x_N$, which
recovers the Markovian projection in continuous-time.

\section{Comparing DSBM-IPF and DSB}
\label{sec:dsbm_vs_dsb_appendix}
We analyze further the differences between DSBM-IPF proposed here and DSB proposed in \cite{debortoli2021diffusion}. Both algorithms solve the SB problem using the IPF iterates. 
Algorithmically, the difference occurs in the trajectory caching step. In DSB, a fixed discretization of SDE needs to be chosen, and all intermediate samples from the discretized Euler-Maruyama simulation of the SDE need to be saved. Furthermore, a second set of drift evaluation needs to be performed for all datapoints in the trajectory  \citep[Equations (12), (13)]{debortoli2021diffusion}. On the contrary, DSBM-IPF discards all intermediate samples during trajectory caching and only retains the joint samples at times $0,T$. Then, the intermediate trajectories are reconstructed using the reference bridge $\Qbb_{|0,T}$.

In addition to the benefits of continuous-time training of SB and lower computational and memory costs than DSB as discussed in the main text, we investigate here further the benefit of explicitly projecting onto the reciprocal class of $\Qbb$. 
Intuitively speaking, we directly incorporate the reference measure $\Q$ in the training procedure as our inductive bias. 
More formally, suppose we have at the current IPF iteration $\Mbb^{2n}$ (Markov diffusion in the forward direction) and want to learn $\Mbb^{2n+1}$ (Markov diffusion in the backward direction). Due to training error accumulating, however, $\Mbb^{2n}$ no longer has the correct bridge $\Qbridge$. 
Now suppose we first perform IMF for $\Mbb^{2n}$ and learn $\Mbb^{2n,\star}$ in the forward direction. That is to say, we repeat alternative reciprocal and Markovian projections and obtain a sequence $(\Mbb^{2n,m})_{m \in \nset}$ in the forward direction converging to $\Mbb^{2n,\star}$. Then $\Mbb^{2n,\star}$ now has the correct bridge $\Qbridge$ by \Cref{prop:convergence_marginals}, since $\Mbb^{2n,\star}$ is the SB between $\Mbb_0^{2n}$ and $\Mbb_T^{2n}$. Theoretically, $\Mbb_t^{2n}=\Mbb_t^{2n,\star}$ for $t=0,T$, but due to training error accumulating it may be that $\Mbb_T^{2n} \neq \Mbb_T^{2n,\star}$. However, $\Mbb_0^{2n}=\Mbb_0^{2n,\star}$, since $\Mbb^{2n,\star}$ is in the forward direction and starting from samples from $\pi_0$. As a result, we can obtain a Markov forward diffusion $\Mbb^{2n,\star}$, which has $\Mbb_0^{2n,\star}=\pi_0$ and the correct bridge $\Qbridge$. These are the same set of properties that the reference measure $\Q$ has. As a result, replacing $\Q$ with $\Mbb^{2n,\star}$ in \eqref{eq:schrodinger_bridge} results in the same SB solution. 
Consequently, now continuing the IPF iterations from $\Mbb_0^{2n,\star}$, it is as if we restart IPF afresh using a modified SB problem 
\begin{equation}
    \PSB = \argmin \ensembleLigne{\KLLigne{\Pbb}{\Mbb^{2n,\star}}}{\Pbb_0 = \pi_0, \ \Pbb_T = \pi_T}.\label{eq:SB_modified}
\end{equation} 
If $\Mbb^{2n,\star}$ is closer than $\Q$ to $\Mbb^{\star}$ in the sense of KL divergence, then we obtain a better initialization of the IPF procedure. 

As proposed in \Cref{alg:DSBM_general}, DSBM performs the Markovian and reciprocal projection only once before switching between the forward and backward directions. However, it is still beneficial compared to DSB with less bias accumulation in the bridge. 


\section{Joint Training of Forward and Backward Processes}
\label{sec:joint-train-forw}

We recall below the DSBM algorithm given in \Cref{alg:DSBM_general}.

\begin{algorithm}[H]
\caption{Diffusion Schr\"odinger Bridge Matching}
\begin{algorithmic}[1]
\dsbmalgo
\end{algorithmic}
\end{algorithm}


Our main observation comes from \Cref{prop:forward_backward_markov_proj}. In
particular, under mild assumptions, we have that the Markovian projection
$\Mbb = \projM(\Pi)$ is associated with 
  \begin{align}
    &
    \rmd \bfX_t = \{f_t(\bfX_t) + \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t} \} \rmd t + \sigma_t \rmd \bfB_t , \quad \bfX_0 \sim \pi_0 ,  \\
    &
    \rmd \bfY_t = \{-f_{T-t}(\bfY_t) + \sigma_{T-t}^2 \CPELigne{\Pi_{0|T-t}}{\nabla \log \Qbb_{T-t|0}(\bfY_t|\bfY_T)}{\bfY_t} \} \rmd t + \sigma_{T-t} \rmd \bfB_t , \quad \bfY_0 \sim \pi_T . 
  \end{align}
Considering the following losses, 
  \begin{align}
    &\textstyle \theta^\star = \argmin_\theta \ensembleLigne{\int_0^T \expeLigne{\Pi_{t,T}}{\normLigne{\sigma_t^2 \nabla
    \log \Qbb_{T|t}(\bfX_T|\bfX_t) - v_\theta(t,\bfX_t)}^2}/\sigma_t^2 \rmd t }{\theta \in \Theta} , \label{eq:loss_forward_appendix} \\
    &\textstyle \phi^\star = \argmin_\phi \ensembleLigne{\int_0^T \expeLigne{\Pi_{t,0}}{\normLigne{\sigma_t^2 \nabla
    \log \Qbb_{t|0}(\bfX_t|\bfX_0) - v_\phi(t,\bfX_t)}^2} /\sigma_t^2 \rmd t }{\phi \in \Phi} . \label{eq:loss_backward_appendix}
  \end{align}
  If the families of functions $\ensembleLigne{v_\theta}{\theta \in \Theta}$ and
  $\ensembleLigne{v_\phi}{\theta \in \Phi}$ are rich enough, we have for any
  $t \in \ccint{0,T}$ and $x_t \in \rset^d$,
  $v_{\theta^\star}(t, x_t) = \sigma_t^2 \CPELigne{\Pi_{T|t}}{\nabla \log
    \Qbb_{T|t}(\bfX_T|\bfX_t)}{\bfX_t = x_t}$ and
  $v_{\phi^\star}(t, x_t) = \sigma_t^2 \CPELigne{\Pi_{0|t}}{\nabla \log
    \Qbb_{t|0}(\bfX_t|\bfX_0)}{\bfX_t = x_t}$. In practice, this means that the
  Markovian projection can be computed in a forward \emph{or} backward fashion
  equivalently.

  Therefore, given a coupling $\Pi = \Pi^{2n}$, we can update \emph{both} $v_\theta$
  and $v_\phi$. This means that we train the forward and backward model
  \emph{jointly}. We then consider $\Mbb^{2n+1}_b$ associated with
  \eqref{eq:approximate_markovian_proj_backward} and $\Mbb^{2n+1}_f$ associated
  with \eqref{eq:approximate_markovian_proj_forward}. Note that if the families
  of functions $\ensembleLigne{v_\theta}{\theta \in \Theta}$ and
  $\ensembleLigne{v_\phi}{\theta \in \Phi}$ are rich enough then
  $\Mbb^{2n+1}_f = \Mbb^{2n+1}_b$.


  \paragraph{Mixture from forward and backward.} Once we have obtained both the
  forward update and the backward update, our next task is to define the new
  mixture of bridge $\Pi^{2n+1}$. In \Cref{alg:DSBM_general}, since we train
  only the \emph{backward} model $\Mbb^{2n+1}=\Mbb^{2n+1}_b$, we define
  $\Pi^{2n+1} = \int \Qbb_{|0,T} \rmd \Mbb_{0,T}^{2n+1}$. In the case of
  \emph{joint training}, we have access to $\Mbb^{2n+1}_b$ and
  $\Mbb^{2n+1}_f$. One way to define a new mixture of bridge is to compute
  $\Pi^{2n+1} = \tfrac{1}{2}(\int \Qbb_{|0,T} \rmd \Mbb_{b, 0,T}^{2n+1} + \int
  \Qbb_{|0,T} \rmd \Mbb_{f, 0,T}^{2n+1})$. This choice ensures that in the case
  where $\Mbb^{2n+1}_f = \Mbb^{2n+1}_b$ we have
  \begin{equation}
    \textstyle \Pi^{2n+1} = \int \Qbb_{|0,T} \rmd \Mbb_{f, 0,T}^{2n+1} = \int \Qbb_{|0,T} \rmd \Mbb_{b, 0,T}^{2n+1} .
  \end{equation}
  It also ensures that all the steps in the \emph{joint} DSBM training
  algorithms are symmetric. We leave the study of an optimal combination of
  $\Mbb_f^{2n+1}$ and $\Mbb_b^{2n+1}$ for future work.

  \paragraph{Consistency loss.} In addition to the losses
  \eqref{eq:loss_forward_appendix} and \eqref{eq:loss_backward_appendix}, we
  also consider an additional \emph{consistency} loss. A similar idea
  was explored in \cite{song2022applying}. In DSB
  \citep{debortoli2021diffusion,chen2021likelihood} and DSBM, see
  \Cref{alg:DSBM_general}, the processes parameterized by $v_\theta$ (forward)
  and $v_\phi$ backward are identical \emph{only at equilibrium}. Thus imposing the forward and the backward processes match at each step of DSB or DSBM would lead to some bias. However, this is not the case in the
  \emph{joint training} setting. Indeed, in that case, we have
  $\Mbb^{2n+1}_f = \Mbb^{2n+1}_b$ if the families are rich enough. Therefore, we get that
  \begin{equation}
    \label{eq:approximate_markovian_proj_backward_appendix}
    \rmd \bfY_t = \{ -f_{T-t}(\bfY_t) + v_{\phi}(T-t, \bfY_t) \} \rmd t + \sigma_{T-t} \rmd \bfB_t , \qquad \bfY_0 \sim \pi_T , 
  \end{equation}
  is the time reversal of 
  \begin{equation}
    \label{eq:approximate_markovian_proj_forward_appendix}
    \rmd \bfX_t = \{ f_t(\bfX_t) +  v_{\theta}(t, \bfX_t) \} \rmd t + \sigma_t \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 .
  \end{equation}
  Computing the time-reversal of \eqref{eq:approximate_markovian_proj_backward_appendix}, we have
  \begin{equation}
    \label{eq:approximate_markovian_proj_backward_backward_appendix}
    \rmd \bfX_t = \{ f_t(\bfX_t) -  v_{\phi}(t, \bfX_t) + \sigma_t^2 \nabla \log \Pi^{2n}_t(\bfX_t) \} \rmd t + \sigma_t \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 .
  \end{equation}
  Identifying \eqref{eq:approximate_markovian_proj_backward_backward_appendix} and \eqref{eq:approximate_markovian_proj_forward_appendix}, we get that for any $t \in \ccint{0,T}$ and $x_t \in \rset^d$
  \begin{equation}
    v_\theta(t, x_t) = -  v_{\phi}(t, x_t) + \sigma_t^2 \nabla \log \Pi^{2n}_t(x_t) . 
  \end{equation}
  We highlight that letting $\sigma_t \to 0$ for any $t \in \ccint{0,T}$, we get
  that $v_\theta = -v_\phi$, which confirms that the time-reversal of an ODE is
  simply given by flipping the sign of the velocity. Therefore, we propose the
  following loss which links the parameters $\theta$ and $\phi$
  \begin{equation}
    \textstyle \mathcal{L}_{\mathrm{cons}}(\theta, \phi) = \int_0^T \expeLigne{\Pi_t^{2n}}{\normLigne{v_\theta(t, \bfX_t) +   v_{\phi}(t, \bfX_t) - \sigma_t^2 \nabla \log \Pi^{2n}_t(\bfX_t)}^2} /\sigma_t^2 \rmd t . 
  \end{equation}
  Leveraging tools from score matching \citep{hyvarinen2005estimation} and the divergence theorem, we get that 
  \begin{equation}
    \textstyle \mathcal{L}_{\mathrm{cons}}(\theta, \phi) = \int_0^T \expeLigne{\Pi_t^{2n}}{\normLigne{v_\theta(t, \bfX_t) +   v_{\phi}(t, \bfX_t)}^2 /\sigma_t^2  + 2 \mathrm{div}(v_\theta(t, \bfX_t) +   v_{\phi}(t, \bfX_t)) } \rmd t + C , 
  \end{equation}
  where $C \geq 0$ is a constant which does not depend on $\theta$ and $\phi$.
  Below, we recall the two losses used to estimate the Markovian projection
  \eqref{eq:loss_forward_appendix} and \eqref{eq:loss_backward_appendix}
  \begin{align}
        &\textstyle \mathcal{L}(\theta) = \int_0^T \expeLigne{\Pi_{t,T}}{\normLigne{\sigma_t^2 \nabla
    \log \Qbb_{T|t}(\bfX_T|\bfX_t) - v_\theta(t,\bfX_t)}^2} /\sigma_t^2 \rmd t  ,  \label{eq:loss_forward_appendix_comp} \\
    & \textstyle \mathcal{L}(\phi) = \int_0^T \expeLigne{\Pi_{t,0}}{\normLigne{\sigma_t^2 \nabla
    \log \Qbb_{t|0}(\bfX_t|\bfX_0) - v_\phi(t,\bfX_t)}^2} /\sigma_t^2 \rmd t  . \label{eq:loss_backward_appendix_comp}
  \end{align}
  The complete loss we consider in the joint training of the algorithm is of the form
  \begin{equation}
    \label{eq:total_loss}
    \mathcal{L}_\lambda(\theta, \phi) = \mathcal{L}(\theta) + \mathcal{L}(\phi) + \lambda \mathcal{L}_{\mathrm{cons}}(\theta, \phi) ,
  \end{equation}
  where $\lambda > 0$ is an additional regularization parameter. We are now
  ready to state the full \emph{joint training} version of DSBM, in
  \Cref{alg:joint_training}. 

  \begin{algorithm}[H]
    \caption{Diffusion Schr\"odinger Bridge Matching (Joint Training)}
    \label{alg:joint_training}
\begin{algorithmic}[1]
\STATE{\textbf{Input:} Coupling $\Pi_{0,T}^0$, tractable bridge $\Qbb_{|0,T}$, $N \in \nset$}
\STATE{Let $\Pi^0 = \int \Qbb_{|0,T} \rmd \Pi_{0,T}^0$.}
\FOR{$n \in \{0, \dots, N-1\}$}
\STATE Learn $v_{\phi^\star}, v_{\theta^\star}$ using \eqref{eq:total_loss} with $\Pi = \Pi^{n}$.
  \STATE Let $\Mbb^{n+1}_f$ be given by \eqref{eq:approximate_markovian_proj_forward}. 
  \STATE Let $\Mbb^{n+1}_b$ be given by \eqref{eq:approximate_markovian_proj_backward}. 
  \STATE Let $\Mbb^{n+1} = \tfrac{1}{2}(\Mbb^{n+1}_f + \Mbb^{n+1}_b)$. 
  \STATE Let $\Pi^{n+1} = \int \Qbb_{|0,T} \rmd \Mbb^{n+1}_{0,T} $. 
 \ENDFOR
  \STATE \textbf{Output:} $v_{\theta^\star}$, $v_{\phi^\star}$
\end{algorithmic}
\end{algorithm}


\section{Loss Scaling}
\label{sec:loss_scaling_appendix}
Similar to the loss weighting in standard diffusion models \citep{song2020score,ho2020denoising}, we derive a similar weighting to reduce the variance of our objective. We focus on the forward direction of Markovian projection in this case, and the backward case can be derived similarly. 
Our forward loss in the DSBM framework is given by \eqref{eq:loss_function}, where the inner expectation is given by  
\begin{equation}
    \expeLigne{\Pi_{t,T}}{\normLigne{\sigma_t^2 \nabla
    \log \Qbb_{T|t}(\bfX_T|\bfX_t) - v_\theta(t,\bfX_t)}^2}. 
\end{equation}
Letting $\Qbb_{|0,T}$ be a Brownian bridge with diffusion parameter $\sigma$ and assuming $T=1$, this becomes
\begin{equation}
    \expeLigne{(\mathbf{X}_0, \mathbf{X}_1) \sim \Pi_{0,1}, \bfZ \sim \mathcal{N}(0, \Id)}{\normLigne{ \mathbf{X}_1 - \mathbf{X}_0 - \sigma \sqrt{t/(1-t)} \bfZ - v_\theta(t, \bfX_t^{0,T})}^2}
\end{equation}
with $\bfX_t^{0,T} = t \mathbf{X}_1 + (1-t) \mathbf{X}_0 + \sigma \sqrt{t(1-t)} \bfZ$. When $t \approx 1$, we see that the regression target is dominated by the noise term $\sigma \sqrt{t/(1-t)} \bfZ$ which needs to be predicted based on information contained within $\bfX_t^{0,T}$. 
The loss will have an approximate scale of $\sigma^2 t/(1-t)$ when $t\approx 1$ which will be very large. To avoid these large values affecting gradient descent, we can downweight the loss by $1 + \sigma^2 t/(1-t)$ (we can add $1$ to effectively cause no loss scaling when $t$ is close to $0$).
\begin{equation}
    (1 + \sigma^2 t/(1-t))^{-1} \expeLigne{(\mathbf{X}_0, \mathbf{X}_1) \sim \Pi_{0,1}, \bfZ \sim \mathcal{N}(0, \Id)}{\normLigne{ \mathbf{X}_1 - \mathbf{X}_0 - \sigma \sqrt{t/(1-t)} \bfZ - v_\theta(t, \bfX_t^{0,T})}^2}. 
\end{equation}
Similar arguments can be applied to the backward loss \eqref{eq:loss_function_backward}
\begin{align}
    &\expeLigne{\Pi_{t,0}}{\normLigne{\sigma_t^2 \nabla
    \log \Qbb_{t|0}(\bfX_t|\bfX_0) - v_\phi(t,\bfX_t)}^2}\\
    &=\expeLigne{(\mathbf{X}_0, \mathbf{X}_1) \sim \Pi_{0,1}, \bfZ \sim \mathcal{N}(0, \Id)}{\normLigne{ \mathbf{X}_0 - \mathbf{X}_1 - \sigma \sqrt{(1-t)/t} \bfZ - v_\phi(t, \bfX_t^{0,T})}^2} ,
\end{align}
which we then downweight by $1 + \sigma^2 (1-t)/t$
\begin{equation}
    (1 + \sigma^2 (1-t)/t)^{-1} \expeLigne{(\mathbf{X}_0, \mathbf{X}_1) \sim \Pi_{0,1}, \bfZ \sim \mathcal{N}(0, \Id)}{\normLigne{ \mathbf{X}_0 - \mathbf{X}_1 - \sigma \sqrt{(1-t)/t} \bfZ - v_\phi(t, \bfX_t^{0,T})}^2}.
\end{equation}


\section{Experiment Details}
\label{sec:exp_detail_appendix}
In this section, we present further details of the experiment setups as well as further experiment results. In all experiments, we use Brownian motion for the reference measure $\Qbb$ with corresponding Brownian bridge \eqref{eq:interpolation_stochastic}, the Adam optimizer with learning rate ${10}^{-4}$, SiLU activation, and $T=1$. 


\subsection{2D Experiments}
For the 2D experiments, we follow \cite{tong2023conditional} closely and use the same datasets and 2-Wasserstein distance between the test set and samples simulated using probability flow ODE as the evaluation metric. However, we use 10000 samples in the test set since we find the 2-Wasserstein distance can vary greatly with only 1000 samples (which can be as high as 0.3 even between two set of samples both drawn from the ground truth distribution). We use a simple MLP with 3 hidden layers and 256 hidden units to parameterize the forward and backward drift networks. We use batch size 128 and 20 diffusion steps. Each outer iteration is trained for 10000 steps and we train for 20 outer iterations. For flow methods, we train for 200000 steps in total. For Table \ref{tab:2d_result} we use $\sigma_t=1$ in all cases, except the moons-8gaussians dataset where we use $\sigma_t=5$.




\subsection{Gaussian Experiment}
Similar to the 2D experiments, we use a simple MLP with 2 hidden layers and 256 hidden units to parameterize the forward and backward drift networks. This is a smaller network compared to the ``large'' network in \cite{debortoli2021diffusion}. We use batch size 128 and 20 diffusion steps. Each outer iteration is trained for 10000 steps and we train for 20 outer iterations.

For \Cref{tab:gaussian_avg_kl}, we assume the marginals of the learned process $\P_t$ are also independently Gaussian distributed in each dimension. We thus estimate the KL divergence using the sample mean and variance of each dimension of $\P_t$ and the analytic KL expression between Gaussian distributions. 





\subsection{Image Experiment} \label{subsec:mnist_appendix}
We follow \cite{debortoli2021diffusion} closely for the setup of this experiment. We use the same U-Net architecture, batch size 128 and 30 diffusion steps. Each outer iteration is trained for 5000 steps. We train for at most 50 outer iterations (i.e. 250000 total number of steps). 

Contrary to \cite{debortoli2021diffusion}, in our experiments we find that we can improve the sampling quality of both DSB and DSBM by choosing a uniform noising schedule. We simply choose $\sigma_t=1$ for all $t$ and $T=1$ in our experiments. 

We provide further experiment results in Figures \ref{fig:mnist_backward_samples_comparison}, \ref{fig:mnist_backward_trajectory} and \ref{fig:mnist_forward_trajectory}. 
In \Cref{fig:mnist_backward_samples_comparison}, we show samples generated using different algorithms and at different points of convergence. Samples generated using CFM and 2-Rectified Flow with 2 rectified steps in (a) and (b) appear to be less clear and identifiable. OT-CFM improves upon CFM slightly in (c), but many samples still appear to be unclear. 
For DSB, the algorithm has not converged after 10 iterations, and many samples in (d) still appear to be letters. After 20 iterations, there are still letter-like samples, in particular many `C' in \Cref{fig:mnist_backward_samples_dsb20}, and the digit classes also appear to be unbalanced with many instances of digit `0'. After 30 iterations, however, the sample quality of DSB becomes very poor in (e). On the other hand, as shown in (f)-(j), we observe that DSBM converges faster than DSB, with more accurate samples even in iterations 10 and 20. At the end of training, DSBM generates samples of highest quality after 50 iterations. 

We present some additional trajectory samples at the end of training using DSBM in Figures \ref{fig:mnist_backward_trajectory} and \ref{fig:mnist_forward_trajectory} in both the forward and backward directions. We observe DSBM is able to transfer samples across the two domains faithfully, and the output samples preserve interesting similarities compared to the input. 

\begin{figure}
    \centering

    \subfloat[CFM]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/CFM/CFM250000b.png} 
    }
    \subfloat[2-Rectified Flow]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/RF/2RF.png} 
    \label{fig:mnist_backward_samples_rf_appendix}
    }
    \subfloat[OT-CFM]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/CFM/OTCFM250000b.png} 
    } 
    \subfloat[DSB Iteration 10]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/DSB/DSB10b.png} 
    }
    \subfloat[DSB Iteration 30]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/DSB/DSB30b.png} 
    }


    \subfloat[DSBM Iteration 10]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/DSBM/DSBM10b.png} 
    }
    \subfloat[DSBM Iteration 20]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/DSBM/DSBM20b.png} 
    }
    \subfloat[DSBM Iteration 30]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/DSBM/DSBM30b.png} 
    }
    \subfloat[DSBM Iteration 40]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/DSBM/DSBM40b.png} 
    }
    \subfloat[DSBM Iteration 50]{
    \includegraphics[width=0.19\linewidth]{Figs/MNIST_EMNIST/DSBM/DSBM50b.png} 
    }
    \caption{Samples of MNIST digits transferred from the EMNIST letters using different methods.}
    \label{fig:mnist_backward_samples_comparison}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM/b/im_grid_0.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM/b/im_grid_10.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM/b/im_grid_20.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM/b/im_grid_30.png} 
    \caption{EMNIST to MNIST sample trajectory of DSBM with 30 diffusion steps at iteration 50 and $t=0,1/3,2/3,1$. }
    \label{fig:mnist_backward_trajectory}
% \end{figure}

% \begin{figure}
    \centering
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM/f/im_grid_0.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM/f/im_grid_10.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM/f/im_grid_20.png} 
    \hfill
    \includegraphics[width=0.2\linewidth]{Figs/MNIST_EMNIST/DSBM/f/im_grid_30.png} 
    \caption{MNIST to EMNIST sample trajectory of DSBM with 30 diffusion steps at iteration 50 and $t=0,1/3,2/3,1$. }
    \label{fig:mnist_forward_trajectory}
\end{figure}


\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
