\section{introduction}

% 1. importance of TKGs and reasoning on TKGs. 
% 2. low resource languages, main main idea.
% 3. relations and limitations of current works.
% 4. summarize our solutions and contributions.

Temporal Knowledge Graphs (TKGs)~\cite{YAGO,ICEWS18,WIKI,acekg} characterize temporally evolving events, where each event, represented as ({\em subject}, {\em relation}, {\em object}), is associated with temporal information ({\em time}), e.g., ({\em Macron}, {\em reelected}, {\em French president}, {\em 2022}). TKGs has facilitated various knowledge-intensive Web applications with timeliness, such as question answering~\cite{KBQA}, product recommendation~\cite{RippleNet,TKG4Rec,TKG4Rec2,RETE}, and social event forecasting~\cite{KG4Social,DyDiff-VAE,andgan,belief,misinfo,polarization}. 

As new events are continually emerging, modern TKGs are still far from being complete. Conventionally, the TKG construction process relies primarily on information extraction from unstructured corpus~\cite{WIKI,YAGO, EventKG}, which necessitates extensive manual annotations to keep up with changing events. For instance, the recent transition from Trump to Biden as the President of the United States has not been reflected in many TKGs, highlighting the need for timely updates. This spurs research on temporal knowledge graph reasoning to automate evolving events prediction over time~\cite{TA-DistMult,Know-Evolve,Renet,RE-GCN}. Unfortunately, the problem of TKG incompleteness is particularly pronounced in low-resource languages, where it is unable to collect enough corpus and annotations to support robust TKG construction. This results in suboptimal reasoning performance and distinctly unsatisfying accuracy in predicting recent and future events.

% whose performance can degrade significantly in low-resource language TKGs that suffer from severe incompleteness over time. 
% \jingfeng{why don't people  study cross-lingual TKG previously, (i.e. use language alignment to improve TKG). Is it really helpful intuitively to use high resource language to help TKGC? For instance, is it enough to use static langauge-alignment to help KGC, ignoring the temporal information? Are those langauge-alignment changing across time?}



\begin{figure}
    \centering
    \includegraphics[width = 1.0\linewidth]{fig/task.pdf}
    \caption{An illustrative example of cross-lingual reasoning on TKGs. 1) We aim to transfer knowledge from English TKG to Japanese TKG, where the English version provides more complete information; 2) Cross-lingual alignments only cover a small ratio of entities, e.g., Apple Inc; 3) Cross-lingual alignments can be noisy and misleading, e.g., A city called Ventura is linked to new macOS Ventura at $t_2$, introducing noise for reasoning in Japanese.}
    \label{fig:illustration}
    %\vspace{-6mm}
\end{figure}

Inspired by the incompleteness issue facing low-resource languages in constructing TKGs, we introduce a novel task named Cross-Lingual Temporal Knowledge Graph Reasoning (as shown in Figure~\ref{fig:illustration}). This task aims to alleviate the reliance on supervision for TKGs in low-resource languages (referred to as the target language) by transferring temporal knowledge from high-resource languages (referred to as the source language)~\footnote{In this paper, for the sake of brevity, we interchangeably use the terms high-resource/low-resource and source/target.}. In contrast, all the existing efforts are either limited to reasoning in monolingual TKGs (usually high-resource languages, e.g., English)~\cite{TA-DistMult,Know-Evolve,Renet,RE-GCN}, or multilingual static KGs~\cite{KEnS,AlignKGC,SS-AGA}. To the best of our knowledge, cross-lingual TKG reasoning that transfers temporal knowledge between TKGs has not been investigated. 

%Motivated by this, we study a new task named {\em cross-lingual temporal knowledge graph reasoning} as shown in Figure~\ref{fig:illustration}, to alleviate the heavy dependence on supervision for any resource-poor language TKGs by distilling the temporal knowledge from resource-rich ones. Differently, all the existing efforts are either limited to reasoning in monolingual (usually high-resource languages, e.g., English) temporal KGs~\cite{TA-DistMult,Know-Evolve,Renet,RE-GCN}, or multilingual static KG~\cite{KEnS,AlignKGC,SS-AGA}, but neglecting the reasoning in a both temporal and cross-lingual manner that highly requires capturing time-evolving patterns and language discrepancy. To the best of our knowledge, this problem, regarding how to transfer cross-lingual knowledge between TKGs, has still not been formally investigated. 

% Unlike conventional TKG reasoning, 
The fulfillment of this task poses tremendous challenges in two aspects: 1) \textbf{Scarcity of cross-lingual alignment}: as the informative bridge of two separate TKGs, cross-lingual alignment is imperative for cross-lingual knowledge transfer~\cite{AlignKGC,KEnS,SS-AGA}. However, obtaining alignments between languages is a time-consuming and resource-intensive process that heavily relies on human annotations. The transfer of knowledge through a limited number of alignments is often insufficient to fully enhance the TKG in the target language. 2) \textbf{Temporal knowledge discrepancy}: the information associated with two aligned entities is not necessarily identical, especially with regards to temporal patterns. Utilizing a rough approach to equate the aligned entities at all times can result in the transfer of misleading knowledge and negatively impact performance. This becomes more pronounced when the alignments are noisy and unreliable. For example, at the time step $t_2$, a new event about operating system ``{\it Ventura}'' from Apple company occurs in the source English TKG, and meanwhile there is a noisy aligned entity ``{\it Ventura city}'' in the target Japanese TKG. Directly pulling those two entities at this point, can inevitably introduce  noise and fail to predict a set of related events in the target TKG. Therefore, it is crucial to dynamically regulate the alignment strength of each local graph structure over time in order to maximize the effectiveness of cross-lingual knowledge distillation.

% Pulling those entities together cannot augment information in target languages. Small alignment strength is beneficial in the unreliable alignment cases, otherwise the misleading knowledge transferring can even hurt the performance.

% Moreover, in a case that the alignments are not fully reliable, directly pulling the two aligned entities together 


% optimally dynamic alignment strength
% {\em Optimal alignment strength to maximize the benefits of knowledge distillation is difficult to obtain, especially in the temporal manner.} 
% In practical, although the aligned entities can share similar information, they may still differ in other perspectives, including but not limited to frequency, interactions, and temporal patterns. How to adjust the alignment strength (i.e., the distance constrains of the aligned entities in the uni-space) accordingly for different entities at different time is unclear. \zheng{Ruijie TODO: add Ventura case}Moreover, in a case that the alignments are not fully reliable, directly pulling the two aligned entities together can even hurt the performance.



% scarcity of hinders the efficient
% knowledge transfer across languages. 
% {\em Transferring knowledge through a small set of alignments is hard to augment information for all entities.} 

% Aligning the same entities across languages rely heavily on manual labeling or rule-based inference~\cite{EA1,EA2,EA3,selfKG}, which is too time-consuming and impractical to obtain the alignments covering most of the entities in target language. 

% In this paper, we study how to boost the TKG reasoning performance in low-resource languages by explicitly increasing the completeness of those TKGs in history. Instead of improving the underlying information extraction techniques in low-data regime, we propose a new task called {\em Cross-lingual Temporal Knowledge Graph Reasoning}, motivated by the facts that there exists common or complementary knowledge shared by the TKGs in different languages under similar topics. The new task aims to facilitate TKG reasoning in low-resource languages (target languages) by distilling knowledge from a corresponding TKG in high-resource language (source language)  through a small set of entity alignments as bridges~\footnote{In this paper, we interchangeably use the terminology high-resource/low-resource and source/target for briety.}. Figure~\ref{fig:illustration} provides an illustrative example of the proposed task.


% Unfortunately, recent breakthroughs in temporal knowledge graph reasoning model~\cite{TA-DistMult,Know-Evolve,Renet,RE-GCN} highly rely on the completeness of the TKGs, especially for the most recent events. 

% However, the completeness of TKGs varies a lot across different languages, even under similar topics. Conventionally, the TKG construction process relies primarily on information extraction techniques built on the unstructured corpus~\cite{WIKI,YAGO, EventKG}. Therefore, the amount of corpus and human annotations in different languages significantly influence the quality of the corresponding TKGs . 
% Therefore, automatically completing/updating TKGs has been attracting enormous interests in recently years, which aims to predict recent/future events on TKGs based on historical events~\cite{TA-DistMult,Know-Evolve,Renet,RE-GCN}, namely temporal knowledge graph reasoning~\footnote{Broadly speaking, TKG reasoning includes interpolation to predict historical events and extrapolation to predict future events. In this paper, we refer to extrapolation task as TKG reasoning, since it is more vital for time-sensitive downstream tasks.}.


% For languages with large-scale and carefully labeled corpus (we refer to as high-resource languages, e.g., English), the constructed TKGs are more comprehensive than TKGs in other languages that lack the high-quality corpus (we refer to as low-resource languages, e.g., Spanish, Slovene, Danish, etc). Such completeness discrepancy leads to distinctly uneven TKG reasoning performances in different languages, which in turn affects the quality of service of the downstream applications. 


% Compared with the traditional TKG reasoning task, the new task imposes non-trivial challenges. An intuitive solution is to construct a unified graph including two TKGs in both source and target languages, and the knowledge distillation can be fulfilled by pulling the aligned entities from two languages close to each other in the uni-space~\cite{AlignKGC,KEnS}. However, there are still two challenges to be addressed. 

% \zheng{Ruijie TODO, Place this part to related works.}
% Existing works in related areas fail to address the aforementioned challenges. Monolingual reasoning methods on static/temporal knowledge graphs~\cite{TransE,TranR,ComplEX,RotatE,TA-DistMult,Know-Evolve,Renet,RE-GCN} is incapable of the desired knowledge transferring due to the insufficient alignment modeling. Although they can be extended on the cross-lingual scenario by viewing the alignments as a new relation on the merged TKGs, the limited amount of alignments prevent them from augmenting information for most of the entities. Entity alignment methods on KGs~\cite{EA1,EA2,EA3,EA4,EA5,selfKG} can automatically enlarge the alignments by  predicting the correspondence between the two TGs. But most of them, if not all, require the relatively even completeness of two TGs to capture the structural similarities, which can not be satisfied in our case, as target TKGs are far from complete. Some recent works start to study the multilingual TK reasoning on static graphs~\cite{AlignKGC,KEnS,SS-AGA}, which similarly aim to extract knowledge from several source KGs to boost the reasoning performance in the target KG, while they still require a sufficient amount of cross-lingual alignments and totally ignore the temporal perspective in our task.

% to facilitate temporal knowledge graph reasoning in low-resource languages. 
% increase the TKG connection and target TKG capacity
% In light of the mutual benefits, we iteratively generate pseudo alignment pairs and pseudo temporal events to address the co-existing scarcity issue in both cross-lingual alignment and target TKGs. 


In this paper, we propose a novel Mutually-paced Knowledge Distillation (\model) framework, where a teacher network learns more enriched temporal knowledge and reasoning skills from the source TKG to facilitate the learning of a student network in the low-data target one. The knowledge transfer is enabled via an alignment module, which estimates entity correspondence across languages based on temporal patterns. Firstly, to alleviate the limited language alignments (\textbf{Challenge \#1}), such a knowledge distillation process is mutually paced over time. This means, on one hand, we encourage the mutually interactive learning between the teacher and student. Concretely, the alignment module between the teacher and the student learns to generate pseudo alignment between TKGs to maximally expand the upper bound of knowledge transfer. And subsequently, it empowers the student to encode more informative knowledge in target TKG, which can in turn boost the alignment module to explore more reasonable alignments as the bridge across TKGs. One the other hand, inspired by self-paced learning~\cite{spl-1,spl-2}, we make the generations as a progressively easy-to-hard process over time. We start from generating reliable pseudo data with high confidence. As time goes by, we then gradually increase the generation amount by relieving the restriction over time. Secondly, to inhibit the temporal knowledge mismatch (\textbf{Challenge \#2}), the attention module can estimate the graph alignment strength distribution over time. This is achieved by a temporal cross-lingual attention in terms of the local graph structure and temporal-evolving patterns of aligned entities. As such, it can dynamically control the negative effect and suppress noise  propagation from the source TKG. Moreover, we provide a theoretical convergence guarantee for the training objective on both initial ground-truth data and pseudo data. To evaluate \model, we conduct extensive experiments of 12 cross-lingual TKG transfer tasks in multilingual EventKG dataset~\cite{EventKG}. Our empirical results show that the \model method outperforms state-of-the-art baselines in both with and without alignment noise settings, where only $20\%$ of temporal events in the target KG and $10\%$ of cross-lingual alignments are preserved.

% To validate the effectiveness of \model, we conduct extensive experiments of 12 cross-lingual TKG transfer tasks in multilingual EventKG benchmark dataset~\cite{EventKG} . Our experimental results empirically demonstrate the superiority of the \model method over state-of-the-art baselines, ranging from static KG embedding~\cite{TransE,TransR,DistMult,RotatE}, temporal KG reasoning~\cite{TA-DistMult,Renet,RE-GCN} to multilingual KG completion~\cite{KEnS,AlignKGC,SS-AGA}, in both with and without alignment noise settings. We further conduct comprehensive ablation and hyperparameter studies to validate the effectiveness of each design choices. Moreover, we provide theoretical analysis of convergence guarantee for the training objective on both initial groundtruth data and pseudo generative data.



To sum up, our contributions are three-fold:

\begin{itemize}[leftmargin = 15pt]
    \item \textbf{Problem formulation}: We propose the cross-lingual temporal knowledge graph reasoning task, to boost the temporal reasoning performance in target TKG by transferring knowledge from source TKG;
    \item \textbf{Novel framework}: We propose a novel \model framework, which enables the mutually-paced learning between the teacher and student networks, to promote both pseudo alignments and knowledge transfer reliability. Besides, \model involves a dynamic alignment estimation across TKGs that inhibits the influence of temporal knowledge discrepancy.
    \item \textbf{Extensive evaluations}: Empirically, extensive experiments on 12 cross-lingual TKG transfer tasks in multilingual EventKG benchmark dataset demonstrate the effectiveness of \model.
\end{itemize}
% pseudo data generation technique to progressively enhance the training data. The generated pseudo alignments can help the training of the representation modules by the knowledge distillation, and in turn adding pseudo events in the target TKG can improves alignment module by providing high-quality representations. 




% interactively
% TKGs in a source language and a target language are represented by a teacher representation module and a student one into a uni-space, respectively. 
% The knowledge distillation is enabled by a cross-lingual alignment module which pulls the aligned entities close to each other and push other entities far away. 
% To address the challenge caused by the scarcity of cross-lingual alignment, 

