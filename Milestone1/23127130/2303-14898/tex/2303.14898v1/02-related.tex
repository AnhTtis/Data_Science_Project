\section{Related Work}
%\zheng{Ruijie TODO, Place this part to related works.}
% Existing works in related areas fail to address the aforementioned challenges. Monolingual reasoning methods on static/temporal knowledge graphs~\cite{TransE,TranR,ComplEX,RotatE,TA-DistMult,Know-Evolve,Renet,RE-GCN} is incapable of the desired knowledge transferring due to the insufficient alignment modeling. Although they can be extended on the cross-lingual scenario by viewing the alignments as a new relation on the merged TKGs, the limited amount of alignments prevent them from augmenting information for most of the entities. Entity alignment methods on KGs~\cite{EA1,EA2,EA3,EA4,EA5,selfKG} can automatically enlarge the alignments by  predicting the correspondence between the two TGs. But most of them, if not all, require the relatively even completeness of two TGs to capture the structural similarities, which can not be satisfied in our case, as target TKGs are far from complete. Some recent works start to study the multilingual TK reasoning on static graphs~\cite{AlignKGC,KEnS,SS-AGA}, which similarly aim to extract knowledge from several source KGs to boost the reasoning performance in the target KG, while they still require a sufficient amount of seed alignments and totally ignore the temporal perspective in our task.

\noindent \textbf{Knowledge Graph Reasoning}.
Knowledge graph reasoning aims to predict missing facts to automatically complete KGs~\cite{DBPedia,WIKI,YAGO,acekg}. It is mostly formulated as measuring the correctness of factual samples and negative samples by specially designed score functions~\cite{TransE,RotatE,factKG,multiKG}. Recently, reasoning on temporal KGs attracts a lot of interests from the community~\cite{Know-Evolve,TA-DistMult,Renet,RE-GCN}. Compared with static KG reasoning task, the main challenge lies in how to incorporate time information. Several embedding-based methods have been proposed. They encode time-dependent information of entities and relations by decoupling embeddings into static component and time-varying component~\cite{TKGC_TE,GoelAAAI2020,metatkgr}, utilizing recurrent neural networks (RNNs) to adaptively learn the dynamic evolution from historical fact sequence~\cite{Renet,RE-GCN}, or learning a sequence of evolving representations from discrete knowledge graph snapshots~\cite{TKGC-ODE,DBKGE,Renet,RE-GCN}. However, all of the existing temporal KG reasoning models aim to extrapolate future facts based on relatively complete TKGs in high-resource languages, and how to boost reasoning performance for TKGs in low-resource languages through cross-lingual alignments is largely under-explored.

\noindent \textbf{Multilingual KG Reasoning}.
Entity alignment methods on KGs~\cite{EA1,DKGA,bright,selfKG,multi-network} can automatically enlarge the alignments by  predicting the correspondence between the two KGs. But most of them, if not all, require the relatively even completeness of two KGs to capture the structural similarities, which can not be satisfied in our case, as target TKGs are far from complete. Inspired by recent transfer learning advances~\cite{pan2009survey,li2018hierarchical,li2019exploiting,li2019sal,li2020learn,juan2022disentangle}, some recent works start to study the multilingual KG reasoning on static graphs~\cite{AlignKGC,KEnS,SS-AGA}, which aim to extract knowledge from several source KGs to boost the reasoning performance in the target KG, while they still require a sufficient amount of seed alignments and totally ignore the temporal information in our task.  We extend this line of works on TKGs, where transferring temporal knowledge is more complex.

%Multilingual KG reasoning (KGR) are extensions of monolingual KGR that consider knowledge transferring across KGs in different languages with the use of limited seed alignment~\cite{KEnS,SS-AGA}. Earlier works focus mainly on entity alignment task which aims to predict the correspondence of entity pairs given a set of seed alignments~\cite{EA1,EA2,EA3,EA4,EA5,selfKG}. For our case that source graphs are complete and target ones are largely incomplete, they fail to capture structural similarities, leading to unsatisfied alignment results. \cite{KEnS} starts to directly improve KGR performance on static KGs given a set of seed alignment, and proposes an ensemble-based approach for the task. \cite{SS-AGA} views alignments as new edge type and employ a relation-aware GNN with learnable attention weight to model the influence of the aligned entities.~\cite{pan2009survey,li2017end,li2018hierarchical,li2019exploiting,li2019sal,li2020learn,juan2022disentangle}

\noindent \textbf{Self-training}.
% polish here as self-training is NOT for a transfer learning strategy... and add the references elsewhere.
Self-training is one of the learning strategies that addresses data scarcity issue by fully utilizing abundant unlabeled data~\cite{ST,ST1,li2021queaco}. Recent works start to study self-training strategy for graph data, as GNNs typically require large amount of data labeling~\cite{ST-GCN,CaGCN-st,Shift}. In summary, most efforts are put on node classification problem, where node labels are largely unavailable. We focus on utilizing self-training technique to deal with link scarcity (events + alignments), which is also a bottleneck for improving the performance on graphs.



