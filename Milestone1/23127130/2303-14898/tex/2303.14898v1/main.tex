%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.

%\documentclass[sigconf,anonymous=true, review]{acmart}
\documentclass[sigconf]{acmart}

\makeatletter
\def\@ACM@checkaffil{% Only warnings
    \if@ACM@instpresent\else
    \ClassWarningNoLine{\@classname}{No institution present for an affiliation}%
    \fi
    \if@ACM@citypresent\else
    \ClassWarningNoLine{\@classname}{No city present for an affiliation}%
    \fi
    \if@ACM@countrypresent\else
        \ClassWarningNoLine{\@classname}{No country present for an affiliation}%
    \fi
}
\makeatother

\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amsfonts}
\newcommand{\cmark}{\checkmark}%
\newcommand{\xmark}{\text{\sffamily X}}%
\usepackage{color}
\usepackage{colortbl}
\definecolor{mygray}{gray}{.9}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\newcommand{\nop}[1]{}

\newcommand{\tarek}[1]{{\color{red}[Tarek: #1]}}
\newcommand{\ruijie}[1]{{\color{blue}[Ruijie: #1]}}
\newcommand{\chao}[1]{{\color{magenta}[Chao: #1]}}
\newcommand{\zheng}[1]{{\color{teal}[Zheng: #1]}}
\newcommand{\jingfeng}[1]{{\color{green}[Jingfeng: #1]}}
\newcommand{\tianyu}[1]{{\color{violet}[Tianyu: #1]}}

\newcommand{\model}{{MP-KD}\xspace }

\settopmatter{printacmref=true}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\copyrightyear{2023} 
\acmYear{2023} 
\setcopyright{acmlicensed}\acmConference[WWW '23]{Proceedings of the ACM Web Conference 2023}{April 30-May 4, 2023}{Austin, TX, USA}
\acmBooktitle{Proceedings of the ACM Web Conference 2023 (WWW '23), April 30-May 4, 2023, Austin, TX, USA}
\acmPrice{15.00}
\acmDOI{10.1145/3543507.3583407}
\acmISBN{978-1-4503-9416-1/23/04}

% Authors, replace the red X's with your assigned DOI string during the rightsreview eform process.
% \settopmatter{printacmref=false} % Removes citation information below abstract
% \renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
% \pagestyle{plain} % removes running headers
%\renewcommand\footnotetextcopyrightpermission[1]{}

\begin{document}
\title{Mutually-paced Knowledge Distillation for Cross-lingual Temporal Knowledge Graph Reasoning}
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Ruijie Wang, Zheng Li$^{2\dagger}$, Jingfeng Yang$^{2}$, Tianyu Cao$^{2}$, Chao Zhang$^{3}$, \\Bing Yin$^{2}$ and Tarek Abdelzaher$^{1\dagger}$}
\thanks{$^{\dagger}$Corresponding authors in UIUC and Amazon.com Inc}
\affiliation{%
  \institution{$^{1}$Department of Computer Science, University of Illinois Urbana-Champaign,~$^{2}$Amazon.com Inc}
  \institution{$^{3}$School of Computational Science and Engineering, Georgia Institute of Technology}
}

\email{{ruijiew2, zaher}@illinois.edu,\space chaozhang@gatech.edu}
\email{{amzzhe, jingfe, caoty, alexbyin}@amazon.com}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010187.10010193</concept_id>
       <concept_desc>Computing methodologies~Temporal reasoning</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   % <concept>
   %     <concept_id>10010147.10010257</concept_id>
   %     <concept_desc>Computing methodologies~Machine learning</concept_desc>
   %     <concept_significance>500</concept_significance>
   %     </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Temporal reasoning}
%\ccsdesc[500]{Computing methodologies~Machine learning}

\keywords{Temporal Knowledge Graph, Cross-lingual Transfer, Knowledge Distillation, Self-training}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Wang, et al.}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{abstract}
This paper investigates cross-lingual temporal knowledge graph reasoning problem, which aims to facilitate reasoning on Temporal Knowledge Graphs (TKGs) in low-resource languages by transfering knowledge from TKGs in high-resource ones. The cross-lingual distillation ability across TKGs becomes increasingly crucial, in light of the unsatisfying performance of existing reasoning methods on those severely incomplete TKGs, especially in low-resource languages. However, it poses tremendous challenges in two aspects. First, the cross-lingual alignments, which serve as bridges for knowledge transfer, are usually too scarce to transfer sufficient knowledge between two TKGs. Second, temporal knowledge discrepancy of the aligned entities, especially when alignments are unreliable, can mislead the knowledge distillation process. We correspondingly propose a mutually-paced knowledge distillation model \model, where a teacher network trained on a source TKG can guide the training of a student network on target TKGs with an alignment module. Concretely, to deal with the scarcity issue, \model generates pseudo alignments between TKGs based on the temporal information extracted by our representation module. To maximize the efficacy of knowledge transfer and control the noise caused by the temporal knowledge discrepancy, we enhance \model with a temporal cross-lingual attention mechanism to dynamically estimate the alignment strength. The two procedures are mutually paced along with model training. Extensive experiments on twelve cross-lingual TKG transfer tasks in the EventKG benchmark demonstrate the effectiveness of the proposed \model method. 

% compared with ten baselines from three related areas, in both scarce and noisy settings.

%This paper investigates a practical but underexplored problem, namely cross-lingual temporal knowledge graph reasoning. It aims to facilitate reasoning on Temporal Knowledge Graphs (TKGs) in low-resource languages, by distilling knowledge from TKGs in high-resource ones, through a few cross-lingual entity alignments. This new task is important in applications that require timely knowledge from low-resource language TKGs, in light of the unsatisfying performance of existing reasoning methods on those severely incomplete TKGs. However, it poses tremendous challenges in two aspects. First, cross-lingual alignments are usually too scarce to transfer sufficient knowledge to TKGs in low-resource languages. Second, temporal knowledge discrepancy of the aligned entities, especially when alignments are unreliable, can mislead knowledge distillation. We correspondingly propose a mutually-paced knowledge distillation model \model, where a teacher network trained on a source TKG can guide the training of a student network on target TKGs with an alignment module. Concretely, to deal with the co-existing scarcity issue, \model alternatively generates pseudo alignments between TKGs and pseudo events in the target TKG, where two generation procedures are mutually paced along with model training. To maximize the efficacy of knowledge transfer and control the noise caused by knowledge mismatch, we enhance \model with a temporal cross-lingual attention mechanism to dynamically estimate the alignment strength. Extensive experiments on twelve cross-lingual TKG transfer tasks in the EventKG benchmark  demonstrate the effectiveness of the proposed \model method. 
% compared with ten baselines from three related areas, in both scarce and noisy settings.

 %describe time-varying events in real-world. Predicting future events on TKGs, namely temporal knowledge graph reasoning, is important to various knowledge-intensive applications. However, most existing TKG reasoning methods yield limited performance in  low-resource languages due to the scarcity of TKGs. We study cross-lingual temporal knowledge reasoning, which aims to facilitate low-resource temporal reasoning by transferring knowledge from high-resource TKGs to low-resource ones with only a small set of cross-lingual alignments. Transferring knowledge for most entities in low-resource TKGs through such limited alignments is non-trivial. It is also challenging to determine the suitable alignment strength adaptively given the temporal knowledge discrepancy and even unreliable alignments. In light of these challenges, we propose a mutually-paced knowledge distillation model \model, which progressively generates pseudo data to deal with data scarcity for both cross-lingual alignment and  low-resource TKG reasoning. During training, we iteratively generate pseudo alignments to expand the cross-lingual connection, as well as pseudo temporal events to facilitate student model training in low-resource languages.  Our alignment module is learned to dynamically control the alignment strength for different entities at different time, thereby maximizing the benefits of knowledge transferring. Moreover, our theoretical analysis provides convergence guarantee on the new training objective on both groundtruth data and pseudo data. Extensive experiments on 12 language pairs of EventKG data demonstrate the superiority of the proposed model compared with ten baselines from three related areas, in both scarce and noisy settings.


\end{abstract}
\maketitle
\input{01-introduction.tex}
\input{03-definition.tex}
\input{04-method.tex}
\input{05-experiment.tex}
\input{02-related.tex}
\input{06-conclusion.tex}

%\newpage
%\bibliographystyle{ACM-Reference-Format}
\bibliographystyle{plain}
\bibliography{main}
\newpage
\input{07-appendix}
\end{document}
