\begin{center}
  \textbf{\LARGE Supplementary Material}
\end{center}

\begin{center}
  \textbf{\large Is Forgetting Less a Good Inductive Bias for Forward Transfer?}
\end{center}

\section{Experimental Details}
\label{app:exp-detail}
\subsection{Datasets}

We describe the details of the datasets used in this paper below: 

\paragraph{CIFAR-10. } 
%
The CIFAR-10 dataset~\citep{krizhevsky2009learning} consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. 
%
There are 50,000 training images and 10,000 test images. We reserve 10,000 training images as the validation data. 
%
So the training set used has 40,000 images. 
%
We split the CIFAR-10 dataset into 5 disjoint subsets to create the Split CIFAR-10 benchmark. 
%
Split CIFAR-10 has 5 tasks corresponding to the 5 disjoint subsets and each task has 2 classes. 
%
During training, we apply random cropping and random horizontal flip to the training images.  
%

\paragraph{CIFAR-100. } 
%
The CIFAR-100 dataset~\citep{krizhevsky2009learning} is just like the CIFAR-10, except it has 100 classes containing 600 images each. 
%
There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. 
%
Each image comes with a ``fine'' label (the class to which it belongs) and a ``coars'' label (the superclass to which it belongs). 
%
We reserve 10,000 training images as the validation data. So the training set used has 40,000 images. 
%
We use the CIFAR-100 dataset to create two benchmarks Split CIFAR-100 and CIFAR-100 Superclasses. 
%
The split CIFAR-100 benchmark is created by splitting the CIFAR-100 dataset into 20 disjoint subsets corresponding to 20 tasks. Each task in Split CIFAR-100 has 5 classes. 
%
The CIFAR-100 Superclasses benchmark is created by splitting the CIFAR-100 dataset into 5 disjoint subsets corresponding to 5 tasks. 
%
Each task in CIFAR-100 Superclasses has 20 classes from 20 superclasses respectively. 
%
For both Split CIFAR-100 and CIFAR-100 Superclasses benchmarks, during training, we apply random cropping and random horizontal flipping data augmentations to the training images.  
%

\paragraph{CLEAR. } 
%
CLEAR~\citep{lin2021clear} is the first continual image classification benchmark dataset with a natural temporal evolution of visual concepts in the real world that spans a decade (2005-2014). 
%
It contains two continual learning benchmark CLEAR10 and CLEAR100. 
%
The original CLEAR10 has 33,000 training images and 5,500 test images with 10 tasks and 11 classes (including a BACKGROUND class). 
%
We Remove the BACKGROUND class and reserve 5,000 training images as the validation data. 
%
So the training set used has 25,000 images and the test set used has 5,000 images. 
%
The original CLEAR100 has 99,963 training images and 50,000 test images with 10 tasks and 100 classes. 
%
We reserve 19,991 training images as the validation data. 
%
So the training set used has 79,972 images. 
%
Each task in CLEAR10 (or CLEAR100) contains images from a certain year (2005-2014). 
%
For both CLEAR10 and CLEAR100 benchmarks, we resize the images to $224 \times 224$ and use random cropping and random horizontal flipping data augmentations during training. 
%

\paragraph{ImageNet. } 
%
ILSVRC 2012, commonly known as ``ImageNet''~\citep{russakovsky2015imagenet} is a large scale image dataset with 1,000 classes organized according to the WordNet hierarchy. 
%
It has 1,281,167 training images and 50,000 validation images with labels. It also has 100,000 test images but without labels. 
%
We use the validation images as the test set and reserve 300,000 training images (300 images per class) as the validation set. 
%
So the training set used contains 981,167 images. We split the ImageNet dataset into 100 disjoint subsets corresponding to 100 tasks. Each task has 10 classes. 
%
During training, we use random cropping and random horizontal flipping data augmentations. 
%
To reduce computational cost, we resize the images to $64 \times 64$. 
%

\subsection{Baselines} 
\label{app:baselines}

%
We consider the following approaches for leaning on a sequence of tasks $T_1, \dots, T_N$. 
%
Except for the independent baseline, all other baselines reuse the model (\emph{i.e.}) continue training the same model used for the previous tasks. 
%
Specifically, when learning on the task $T_{i}$ ($i>1$), the continual learning method will use the model $f_{i-1}$ learned on the previous task $T_{i-1}$ as an initialization for the model $f_i$, which we call \textit{model reusing}. 
%
For the first task $T_1$ training, the continual learning method will initialize the model by an initial model $f_0$ (either a random intialization or a pre-trained model). 
%

\paragraph{Independent (IND). } 
%
The IND baseline learns on each task independently. That is when learning on the task $T_i$, it will train the model from an initial model (either a random initialization or a pre-trained model) using the Empirical Risk Minimization (ERM) objective: 
%
\begin{align}
\label{obj:erm}
    \min_{f_i} \mathbb{E}_{(x,y,t)\sim \mathcal{D}_i} \ell_i(f_i(x, t), y)
\end{align}

%
We use the IND baseline as a reference to see how well we can learn on a task without learning on other tasks in the task sequence.  
%

\paragraph{Finetuning (FT). } 
%
Finetuning is a simple baseline for continual learning. It trains a single model on all the tasks in a sequence. 
%
When training the model $f_i$ on the current task $T_i$, it doesn't use the data from the previous tasks. 
%
It uses the ERM objective~(\eqref{obj:erm}) to train the model $f_i$ on the current task $T_i$ only. 
%

\paragraph{Linear-Probing-Finetuning (LP-FT). } 
%
Linear-Probing-Finetuning~\citep{kumar2021fine} is the same as the Finetuning baseline except that before each task training, we first learn a task-specific classifier $\classifier$ for the task via linear probing and then train both the feature extractor $\representation$ and the classifier $\classifier$ on the task.
%
That is when learning on the current task $T_i$, we have a two-stage training process. In the first stage, we train the classifier $\classifier_i$ while fixing the feature extractor $\representation_i$ via the ERM training objective: 
%
\begin{align}
    \min_{\classifier_i} \mathbb{E}_{(x,y,t)\sim \mathcal{D}_i} \ell_i(f_i(x, t), y; \classifier_i, \representation_i)
\end{align}

%
In the second stage, we train both the classifier $\classifier_i$ and the feature extractor $\representation_i$ via the ERM training objective~(\eqref{obj:erm}). We only use LP-FT in the setting where the initial model $f_0$ is a pre-trained model.
%

\paragraph{Multitask (MT). } 
%
The Multitask baseline trains the model on the data from both the current task and the previous tasks when learning on the current task $T_i$. It uses the following multitask training objective: 
%

\begin{align}
\label{obj:multitask}
    \min_{\representation_i, \{\classifier_j\}_{j=1}^i} \mathbb{E}_{(x,y,t)\sim \mathcal{D}_i} \ell_i(f_i(x, t), y; \classifier_i, \representation_i) + \lambda \cdot \mathbb{E}_{1\leq j<i} \mathbb{E}_{(x, y, t)\sim \mathcal{D}_j} \ell_j(f_j(x, t), y; \classifier_j, \representation_i)
\end{align}

%
It trains a shared feature extractor $\representation_i$ and task-specific classifiers $\{\classifier_j\}_{j=1}^i$ for the tasks $T_1, \dots, T_i$. The hyperparameter $\lambda$ is chosen from the set $\{1.0, 0.1, 0.01\}$ based on the average learning accuracy across tasks on the validation set.  
%

\paragraph{Experience Replay (ER). } 
%
The ER baseline uses a replay buffer $\mathcal{M}=\cup_{i=1}^{N-1} \mathcal{M}_i$ when learning on the sequence of tasks, where $\mathcal{M}_i$ stores examples from the task $T_i$. 
%
In our work, we restrict the replay buffer $\mathcal{M}$ to store only $m$ examples per class from each task. The training objective used by ER is: 
%

\begin{align}
\label{obj:er}
    \min_{\representation_i, \{\classifier_j\}_{j=1}^i} \mathbb{E}_{(x,y,t)\sim \mathcal{D}_i} \ell_i(f_i(x, t), y; \classifier_i, \representation_i) + \mathbb{E}_{1\leq j<i} \mathbb{E}_{(x, y, t)\sim \mathcal{M}_j} \ell_j(f_j(x, t), y; \classifier_j, \representation_i)
\end{align}

\paragraph{AGEM. } 
%
AGEM is a continual learning method proposed by \citeauthor{chaudhry2018efficient}. 
%
Similar to ER, AGEM also uses a replay buffer (or an episodic memory) $\mathcal{M}=\cup_{i=1}^{N-1} \mathcal{M}_i$, where $\mathcal{M}_i$ stores only $m$ examples per class from the task $T_i$. 
%
While learning the task $T_i$, the training objective of AGEM is: 
%

\begin{align}
\label{obj:agem}
     & \min_{f_i} \quad  \mathbb{E}_{(x,y,t)\sim \mathcal{D}_i} \ell_i(f_i(x, t), y) \\
\label{obj:agem-constraint}
    \text{s.t.} \quad &  \mathbb{E}_{(x,y,t)\sim \mathcal{M}_{1}^{i-1}} \ell_t(f_i(x, t), y) \leq \mathbb{E}_{(x,y,t)\sim \mathcal{M}_{1}^{i-1}} \ell_t(f_{i-1}(x, t), y) 
\end{align}

where $\mathcal{M}_{1}^{i-1} = \cup_{j=1}^{i-1} \mathcal{M}_j$. The corresponding optimization problem is: 

\begin{align}
    \min_{\tilde{g}} \frac{1}{2} \| g-\tilde{g} \|_2^2 \quad \text{s.t.} \quad \tilde{g}^T g_{ref} \geq 0
\end{align}

%
where $g$ is a gradient computed using a batch randomly sampled from the current task to solve the objective~(\eqref{obj:agem}), $g_{ref}$ is a gradient computed using a batch randomly sampled from the episodic memory $\mathcal{M}_{1}^{i-1}$, and $\tilde{g}$ is a projected gradient that we will use to update the model. 
%
When the gradient $g$ violates the constraint~(\eqref{obj:agem-constraint}), it is projected via: 
%

\begin{align}
    \tilde{g} = g - \frac{g^T g_{ref}}{g^T_{ref}g_{ref}}g_{ref}
\end{align}

\paragraph{FOMAML. } 
%
MAML is a meta-learning approach proposed by~\citeauthor{finn2017model}. 
%
Since there are some differences in the meta-learning setting and the continual learning setting (e.g. meta-learning algorithms assume that there is a task distribution where we can sample tasks from it while in the continual learning setting, we don't have such a task distribution), we cannot directly use the MAML algorithm proposed in \cite{finn2017model}. 
%
We then modify MAML such that it can be used in the continual learning setting. 
%
While learning on the task $T_i$, the training objective we want to solve is: 
%

\begin{align}
\label{obj:fomaml}
    \min_{f_{i}} \mathbb{E}_{B \sim \mathcal{D}_i} \ell_i(B; f_i) + \lambda \cdot \mathbb{E}_{j \in [i]} \mathbb{E}_{B^{\textrm{in}}_{j,1}, \dots, B^{\textrm{in}}_{j,b}, B^{\textrm{out}}_{j} \sim \mathcal{D}_j} \ell_j(B^{\textrm{out}}_{j}; f^{(b)}_{i,j})
\end{align}

where $[i]=\{1,2,\dots, i\}$, $B \sim \mathcal{D}_i$ means sampling a batch $B$ from $\mathcal{D}_i$ and 
\begin{align}
    f^{(b)}_{i,j}=U_b(B^{\textrm{in}}_{j,1}, \dots, B^{\textrm{in}}_{j,b}; f_{i})
\end{align}

Here, $U_b(B_1, \dots, B_b; f)$ is a model obtained by applying $b$ gradient update steps on the model $f$ using $b$ batches $B_1, \dots, B_b$. If we use standard SGD for $U_b$ and the learning rate is $\alpha$, then we have 
\begin{align}
    f_{i,j}^{(0)} =  f_{i}, \quad
    f^{(q)}_{i,j} =  f^{(q-1)}_{i,j}-\alpha \cdot \nabla_{f^{(q-1)}_{i,j}} \ell_j(B^{\textrm{in}}_{j,q}; f^{(q-1)}_{i,j}), \quad q=1,\dots,b
\end{align}

%
The training objective aims to find a model such that it achieves small error on the current task $T_i$ and after several gradient update steps on some batches from a seen task $T_j$ ($j\in [i]$), the updated model can achieve small error on other batches from the task $T_j$. 
%
So we want to find a model that can enable knowledge transfer between different batches from the same task. 
%

%
Solving the objective~(\eqref{obj:fomaml}) requires computing the second-order gradients, which might be expensive. 
%
However, we can use the idea of first-order MAML (FOMAML) proposed in~\cite{finn2017model}, which ignores the second derivative terms, to solve the objective. 
%
The algorithm of FOMAML is presented in Algorithm~\ref{alg:fomaml}. 
%
In our experiments, we simply set $\alpha=\beta$ and $c=1$. On Split ImageNet, we set $b=1$ while on other benchmarks, we set $b=2$. 
%

\begin{algorithm*}[htb]
  \small
  \caption{\textsc{\small First-Order MAML (FOMAML)}}
  \begin{algorithmic}[1]
    \REQUIRE A model $f_{i-1}$ after training on the previous task $T_{i-1}$, a learning rate $\alpha$ for inner-update, a learning rate $\beta$ for outer-update, the number of previous tasks $c$ used for each training step, the number of gradient update steps $b$ for the inner-update and the number of training steps $n$ for the outer-update.  
    \STATE $f_i \gets f_{i-1}$
    \STATE Randomly sample a batch $B$ from the current task $T_i$, i.e., $B \sim \mathcal{D}_i$
    \STATE $G \gets \nabla_{f_i} \ell_i(B; f_i)$ 
    \FOR{$p=1,2,\dots,n$}
        \STATE Randomly select $c$ indices from the set $\{1, 2, \dots, i-1\}$ without replacement as a set $I_{p}$.
        \STATE $I \gets I_{p} \cup \{ i \}$
        \FOR{$j \in I$}
            \STATE $f_{i,j} \gets f_{i}$
            \FOR{$q=1,2,\dots,b$}
                \STATE Randomly sample a batch $B^{\textrm{in}}_{j,q}$ from $\mathcal{D}_j$.
                \STATE Apply an inner-update step: $f^{(q)}_{i,j} \gets f^{(q-1)}_{i,j}-\alpha \cdot \nabla_{f^{(q-1)}_{i,j}} \ell_j(B^{\textrm{in}}_{j,q}; f^{(q-1)}_{i,j})$. 
            \ENDFOR
            \STATE Randomly sample a batch $B^{\textrm{out}}_j$ from $D_j$. 
            \STATE $G \gets G + \nabla_{f^{(q)}_{i,j}} \ell_j(B^{\textrm{out}}_j; f^{(q)}_{i,j})$
        \ENDFOR
        
        % \STATE Randomly select a previous task $T_j$ and sample a batch $B^{\textrm{prev}}_{\textrm{out}}$ from $\mathmal{M}_j^{(i)}$.
        \STATE Apply an outer-update step: 
        $ f_{i} \gets f_{i}- \beta \cdot G$
    \ENDFOR
    \RETURN $f_{i}$.
  \end{algorithmic}
  \label{alg:fomaml}
\end{algorithm*}

\subsection{Architecture and Training Details}
\label{app:arch-training-details}

\paragraph{Architecture. } 
%
We use ResNet50~\citep{he2016deep} architecture as the feature extractor $\representation$ on all benchmarks by default. 
%
On CLEAR10 and CLEAR100, we use a single classification head $\classifier$ that is shared by all the tasks (single-head architecture) while on other benchmarks, we use a separate classification head $\classifier_i$ for each task $T_i$ (multi-head architecture). 
%

\paragraph{Continual Learning Training Details. } 
%
We use Stochastic Gradient Decent (SGD) for training models. We use cosine learning rate scheduling~\citep{loshchilov2016sgdr} to adjust the learning rate during training. 
%
Suppose the base learning rate is $r$ and the number of training steps for each task is $n$. Then for each task training, at training step $t$, the learning rate for the SGD update is $r \cdot \cos{\frac{t\pi}{2n}}$. 
%
For LP-FT, we use a base learning rate of $0.001$ while for other baselines, we use a base learning rate of $0.01$. 
%
on split CIFAR-10, split CIFAR-100 and CIFAR-100 superclasses, we use a batch size of $64$. On CLEAR10 and CLEAR100, we use a batch size of $128$. 
%
On Split ImageNet, we use a batch size of $256$. When using a random initialization as the initial model $f_0$, on split CIFAR-10, split CIFAR-100 and CIFAR-100 superclasses, we train the model for $50$ epochs per task while on CLEAR10, CLEAR100 and Split ImageNet, we train the model for $100$ epochs per task. 
%
When using a pre-trained model as the initial model $f_0$, on all benchmarks, we train the model for $20$ epochs per task as we found it sufficient for training convergence. For LP-FT, we perform the linear probing for $10$ epochs per task. These training hyper-parameters are chosen based on the average learning accuracy across tasks on the validation set. 
%

\paragraph{K-Shot Linear Probing Training Details. } 
%
We use Stochastic Gradient Decent (SGD) with a fixed learning rate of $0.01$ for linear probing. 
%
We train the classifier head $\hat{\classifier}$ for $100$ epochs on the k-shot dataset $\mathcal{S}^k_{j+1}$ as we found it sufficient for training convergence. 
%
On CLEAR100, the set of values we consider for $k$ is $\{5, 10, 20, 40\}$ while on other benchmarks, the set of values we consider for $k$ is $\{5, 10, 20, 100\}$. 
%
For each $k$, we use a batch size of $\min(k\cdot c, 50)$, where $c$ is the number of classes in the task. 
%
We don't apply any data augmentations to the training images during the k-shot linear probing. 
%

\subsection{Hyper-parameters Selection}
In this section, we discuss how we select the hyper-parameters.

\paragraph{Continual Learning Training. } For different baselines, the shared hyper-parameters are the batch size, the learning rate and the number of training epochs. We do not tune the batch size, but set it to be a fixed number for each benchmark. For the learning rate and the number of training epochs, we choose them based on the average learning accuracy across tasks on the validation data. The range of the learning rate that we consider is $\{0.1, 0.01, 0.001, 0.0001\}$. We found that setting the learning rate to be $0.01$ leads to the best average learning accuracy for all the baselines except LP-FT. For LP-FT, we found that setting the learning rate to be $0.001$ leads to better average learning accuracy. We set the number of training epochs to be a sufficiently large number such that the average learning accuracy doesn't improve as we increase the number of epochs further.  For all the baselines, we pick a fixed number of training epochs such that all methods converge for each benchmark setting. 

\paragraph{K-shot Linear Probing Training. } The hyper-parameters are the batch size, the learning rate and the number of training epochs. For each $k$, we just set the batch size to be $\min(k\cdot c, 50)$ and do not tune it. Note that K-shot linear probing is a convex optimization problem. Thus, the number of training epochs will not affect the results across baselines as long as we train for a sufficient number of epochs. We found that $100$ epochs were more than sufficient for all the baselines to converge for K-shot linear probing. Also, the learning rate will not affect the results much as long as we pick a reasonable one. Therefore, we simply fix the learning rate to be $0.01$.

\section{Additional Experimental Results} 

\subsection{Comparing Forgetting and Forward Transfer}
\label{app:compare-fgt-fwt}
In the main paper, we give results for comparing forgetting and forward transfer on some benchmarks. In this section, we provide additional results for comparing forgetting and forward transfer on other benchmarks.  \Figref{fig:cifar100_clear10_split_imagenet_fgt_fwt} shows the results where we use a random initialization and \Figref{fig:cifar100_clear10_clear100_fgt_fwt_pretrain} shows the results where we use a pre-trained model. We can see that the claims made in~\Secref{sec:results} still hold here. 

\begin{figure}[t]
\centering
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/cifar100_superclasses_Random_init_avg_forgetting.pdf}
      % \caption{\small Average Forgetting ($\downarrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/cifar100_superclasses_Random_init_avg_fwt.pdf}
      % \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear10_Random_init_avg_forgetting.pdf}
    %   \caption{\small Average Forgetting ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear10_Random_init_avg_fwt.pdf}
    %   \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_imagenet_Random_init_avg_forgetting.pdf}
      \caption{\small Average Forgetting ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_imagenet_Random_init_avg_fwt.pdf}
      \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}\hfill

% \negspace{-1mm}
\caption{\small Comparing average forgetting with average forward transfer for different continual learning methods using random initialization on the CIFAR-100 Superclasses, CLEAR10 and Split ImageNet benchmarks. }
\label{fig:cifar100_clear10_split_imagenet_fgt_fwt}
% \negspace{-3mm}
\end{figure}


\begin{figure}[t]
\centering
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/cifar100_superclasses_Pre-train_avg_forgetting.pdf}
      % \caption{\small Average Forgetting ($\downarrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/cifar100_superclasses_Pre-train_avg_fwt.pdf}
      % \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/cifar100_superclasses_imagenet_eval.pdf}
      % \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear10_Pre-train_avg_forgetting.pdf}
      % \caption{\small Average Forgetting ($\downarrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear10_Pre-train_avg_fwt.pdf}
      % \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear10_imagenet_eval.pdf}
      % \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear100_Pre-train_avg_forgetting.pdf}
      \caption{\small $\avgfgt$ ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear100_Pre-train_avg_fwt.pdf}
      \caption{\small $\avgfwt$ ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear100_imagenet_eval.pdf}
      \caption{\small Upstream Accuracy ($\uparrow$ better)}
\end{subfigure}

% \negspace{-1mm}
\caption{\small Comparing average forgetting with average forward transfer for different continual learning methods that train the model from a pre-trained ImageNet model on the CIFAR-100 Superclasses, CLEAR10 and CLEAR100 benchmarks. We also show the accuracy of the models on the upsteam ImageNet data. Since CIFAR-100 images have different image resolution than that of ImageNet images, we need to resize the ImageNet test images from $224 \times 224$ to $32\times 32$ in order to get meaningful accuracy of the models trained on the CIFAR-100 Superclasses benchmark on the upstream ImageNet data. On CLEAR10 and CLEAR100 benchmarks, since their images have the same image resolution as the ImageNet images, we don't need to resize the ImageNet test images when evaluating the upstream accuracy. }
\label{fig:cifar100_clear10_clear100_fgt_fwt_pretrain}
% \negspace{-3mm}
\end{figure}


\subsection{Evaluating Average Accuracy and Average Learning Accuracy}
\label{app:eval-avg-acc}
%
In this section, we report results for traditional continual learning metrics Average Accuracy and Average Learning Accuracy. 
%
The Average Accuracy is defined as: 

\begin{equation*}
    \avgacc = \frac{1}{N} \sum_{j=1}^{N} \acc(N, j) .
\end{equation*}

While the Average Learning Accuracy is defined as: 

\begin{equation*}
    \avglacc = \frac{1}{N} \sum_{j=1}^{N} \acc(j, j) .
\end{equation*}

The results are reported in~\Tabref{tab:avg-accuracy-results}.

\begin{table*}[htb]
    \centering
    % \begin{adjustbox}{width=\columnwidth,center}
		\begin{tabular}{l|l|cc|cc}
			\toprule
			  \multirow{2}{0.12\linewidth}{Dataset} &  \multirow{2}{0.08\linewidth}{Method} &   \multicolumn{2}{c|}{Random Init} & \multicolumn{2}{c}{Pretrain}  \\ \cline{3-6}
			  & & $\avgacc$ & $\avglacc$  &  $\avgacc$ & $\avglacc$  \\ \hline
			 \multirow{6}{0.12\linewidth}{Split CIFAR-10}
			 & FT & 62.69 $\pm$ 5.24 & 91.38 $\pm$ 1.44 & 67.56 $\pm$ 14.62 & 95.75 $\pm$ 1.02 \\
            & LP-FT & - & - & 92.64 $\pm$ 1.15 & 95.46 $\pm$ 0.13 \\
            & ER (m=50) & 85.56 $\pm$ 2.46 & 91.70 $\pm$ 1.53 & 90.41 $\pm$ 1.26 & 95.90 $\pm$ 0.35 \\
            & AGEM (m=50) & 81.74 $\pm$ 3.14 & 91.59 $\pm$ 1.14 & 80.22 $\pm$ 5.51 & 95.88 $\pm$ 1.09 \\
            & MT & 89.70 $\pm$ 5.37 & 92.68 $\pm$ 0.80 & 91.48 $\pm$ 4.32 & 96.14 $\pm$ 0.08 \\
            & FOMAML & 92.77 $\pm$ 0.40 & 93.45 $\pm$ 0.71 & 94.61 $\pm$ 0.50 & 96.06 $\pm$ 0.45 \\ \hline
			  \multirow{6}{0.12\linewidth}{Split CIFAR-100} 
			 & FT & 57.47 $\pm$ 3.11 & 81.31 $\pm$ 0.93 & 67.96 $\pm$ 1.90 & 89.64 $\pm$ 0.79 \\
            & LP-FT & - & -  & 85.33 $\pm$ 0.59 & 90.27 $\pm$ 0.32 \\
            & ER (m=20) & 72.91 $\pm$ 1.19 & 81.00 $\pm$ 0.42 & 81.51 $\pm$ 1.00 & 89.82 $\pm$ 0.20 \\
            & AGEM (m=20) & 64.02 $\pm$ 0.84 & 81.35 $\pm$ 0.75 & 71.77 $\pm$ 1.59 & 89.24 $\pm$ 1.34 \\
            & MT & 73.90 $\pm$ 5.08 & 82.15 $\pm$ 0.36 & 82.74 $\pm$ 5.11 & 90.33 $\pm$ 0.38 \\
            & FOMAML & 79.79 $\pm$ 0.57 & 82.36 $\pm$ 0.57 & 85.36 $\pm$ 1.13 & 89.63 $\pm$ 0.64 \\ \hline
			  \multirow{6}{0.12\linewidth}{CIFAR100 Superclasses} 
			 & FT & 56.76 $\pm$ 0.94 & 68.92 $\pm$ 0.78 & 70.25 $\pm$ 0.78 & 81.55 $\pm$ 0.38 \\
            & LP-FT & - & - & 78.88 $\pm$ 0.47 & 81.22 $\pm$ 0.53 \\
            & ER (m=5) & 60.43 $\pm$ 2.37 & 68.63 $\pm$ 1.60 & 72.43 $\pm$ 0.99 & 81.36 $\pm$ 0.25 \\
            & AGEM (m=5) & 59.28 $\pm$ 0.52 & 69.42 $\pm$ 0.67 & 71.79 $\pm$ 0.79 & 81.60 $\pm$ 0.25 \\
            & MT & 68.43 $\pm$ 3.50 & 69.54 $\pm$ 1.37 & 77.28 $\pm$ 3.19 & 81.77 $\pm$ 0.18 \\
            & FOMAML & 71.48 $\pm$ 1.82 & 69.86 $\pm$ 2.00 & 80.11 $\pm$ 0.45 & 81.03 $\pm$ 0.26 \\ \hline
			  \multirow{6}{0.12\linewidth}{CLEAR10} 
			& FT & 71.18 $\pm$ 0.42 & 70.19 $\pm$ 0.19 & 93.64 $\pm$ 0.48 & 93.87 $\pm$ 0.21 \\
            & LP-FT & - & - & 95.27 $\pm$ 0.05 & 94.70 $\pm$ 0.11 \\
            & ER (m=10) & 72.92 $\pm$ 0.63 & 71.16 $\pm$ 0.24 & 94.12 $\pm$ 0.22 & 94.15 $\pm$ 0.27 \\
            & AGEM (m=10) & 71.76 $\pm$ 0.67 & 70.22 $\pm$ 0.75 & 93.81 $\pm$ 0.27 & 94.00 $\pm$ 0.13 \\
            & MT & 77.68 $\pm$ 0.68 & 73.69 $\pm$ 0.54 & 95.04 $\pm$ 0.64 & 94.53 $\pm$ 0.26 \\
            & FOMAML & 78.51 $\pm$ 0.47 & 74.41 $\pm$ 0.47 & 95.16 $\pm$ 0.22 & 94.63 $\pm$ 0.24 \\ \hline
			  \multirow{6}{0.12\linewidth}{CLEAR100} 
			 & FT & 52.38 $\pm$ 0.21 & 47.27 $\pm$ 0.25 & 86.47 $\pm$ 0.10 & 86.29 $\pm$ 0.12 \\
            & LP-FT & - & - & 89.56 $\pm$ 0.04 & 88.22 $\pm$ 0.06 \\
            & ER (m=5) & 53.16 $\pm$ 0.47 & 46.99 $\pm$ 0.40 & 87.25 $\pm$ 0.18 & 86.49 $\pm$ 0.07 \\
            & AGEM (m=5) & 52.35 $\pm$ 0.25 & 47.25 $\pm$ 0.41 & 86.56 $\pm$ 0.16 & 86.26 $\pm$ 0.18 \\
            & MT & 59.58 $\pm$ 1.56 & 50.94 $\pm$ 0.55 & 89.56 $\pm$ 0.12 & 87.96 $\pm$ 0.13 \\
            & FOMAML & 60.99 $\pm$ 0.54 & 52.09 $\pm$ 0.67 & 89.42 $\pm$ 0.12 & 87.78 $\pm$ 0.06 \\ \hline
            \multirow{6}{0.12\linewidth}{Split ImageNet} 
			 & FT & 13.42 $\pm$ 0.54 & 72.19 $\pm$ 0.41 & - & - \\
            & ER (m=10) & 43.42 $\pm$ 3.50 & 69.71 $\pm$ 0.40 & - & - \\
            & AGEM (m=10) & 16.82 $\pm$ 1.30 & 71.71 $\pm$ 0.47 & - & - \\
            & MT & 54.68 $\pm$ 4.13 & 73.58 $\pm$ 0.50 & - & - \\
            & FOMAML & 59.73 $\pm$ 4.58 & 71.85 $\pm$ 1.28 & - & - \\ 
			\bottomrule
		\end{tabular}
% 	\end{adjustbox}
	\caption[]{\small Results for average accuracy $\avgacc$ and average learning accuracy $\avglacc$. All numbers are percentages. }
	\label{tab:avg-accuracy-results}
\end{table*}

\subsection{Ablation Study on the Model Architecture}
\label{app:ablation-arch}
We want to see whether our claims about forgetting and forward transfer also hold when we use a different architecture. Thus, on split CIFAR-10, split CIFAR-100, and CIFAR-100 Superclasses benchmarks, we also report results in \Figref{fig:cifar10_cifar100_resnet18} for using ResNet18 as the model architecture. We only show results for using random initialization since we don't have pre-trained ResNet18 model on ImageNet. From the results, we can see that our claim that less forgetting is a good inductive bias for forward transfer still holds. 

\begin{figure}[t]
\centering
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar10_Random_init_resnet18_avg_forgetting.pdf}
      % \caption{\small Average Forgetting ($\downarrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar10_Random_init_resnet18_avg_fwt.pdf}
      % \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar100_Random_init_resnet18_avg_forgetting.pdf}
    %   \caption{\small Average Forgetting ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar100_Random_init_resnet18_avg_fwt.pdf}
    %   \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/cifar100_superclasses_Random_init_resnet18_avg_forgetting.pdf}
      \caption{\small Average Forgetting ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/cifar100_superclasses_Random_init_resnet18_avg_fwt.pdf}
      \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}\hfill

% \negspace{-1mm}
\caption{\small Comparing average forgetting with average forward transfer for different continual learning methods using random initialization on the Split CIFAR-10, Split CIFAR-100 and CIFAR-100 Superclasses benchmarks. Here, we use ResNet18 as the model architecture. }
\label{fig:cifar10_cifar100_resnet18}
% \negspace{-3mm}
\end{figure}

\subsection{Correlation Between Average Forgetting and Average Feature Diversity Score}
\label{app:corr-forgetting-feature-score}
In order to aggregate the metrics across different approaches and to see a global trend between forgetting and feature diversity, we compute the Spearman rank correlation between the $\avgfgt$ and $\avgfdiv$.
%
Table \ref{tab:spearman-correlation-feature-score-results} shows the correlation values for randomly initialized models.
%
From the results, we can see that for randomly initialized models, $\avgfgt$ and $\avgfdiv$ generally have positive correlations. 

\begin{table*}[h!]
    \centering
    % \begin{adjustbox}{width=\columnwidth,center}
		\begin{tabular}{l|c}
			\toprule
			 Dataset &  Spearman Correlation  \\ \hline
            Split CIFAR-10 & 0.33 (0.10)  \\
            Split CIFAR-100 & 0.31 (0.13) \\
            CIFAR100 Superclasses  & 0.38 (0.06)   \\
            CLEAR10  & 0.46 (0.02) \\
            CLEAR100 & 0.80  \\
            Split ImageNet & 0.81   \\
            \bottomrule
		\end{tabular}
% 	\end{adjustbox}
	\caption[]{\small  Spearman correlation between $\avgfgt$ and $\avgfdiv$, which computes the correlation over different settings (different training methods and random runs). Here, we use random initialization as the initial model. $p$-values are shown in parenthesis if greater than or equal to $0.01$.}
	\label{tab:spearman-correlation-feature-score-results}
\end{table*}

\subsection{forward transfer through k-shot fine-tuning}
\label{app:fwt-fine-tuning}

We also evaluate forward transfer through k-shot fine-tuning (i.e., we fine-tune the entire model including the feature extractor $\representation_j$ and the classifier $\hat{\classifier}$ on the k-shot samples $\sample_{j+1}^k$). The training hyper-parameters are the same as those of k-shot linear probing, except that to avoid over-fitting while fine-tuning the whole network, we perform cross-validation for the learning rate and the number of training epochs using the validation set. The learning rate is chosen from the set $\{0.01, 0.001 \}$ while the number of training epochs is chosen from the set $\{ 10, 50, 100 \}$. When using random initialization, the results on the Split CIFAR-10, Split CIFAR-100, and CIFAR-100 Superclasses benchmarks are shown in~\Figref{fig:cifar_fgt_fwt_full_finetune} while the results on the CLEAR10, CLEAR100 and Split ImageNet benchmarks are shown in ~\Figref{fig:clear_imagenet_fgt_fwt_full_finetune}. When using a pre-trained model as initialization, the results on the Split CIFAR-10, Split CIFAR-100, CIFAR-100 Superclasses, CLEAR10 and CLEAR100 benchmarks are shown in ~\Figref{fig:fgt_fwt_pretrain_full_finetune}. 

In order to aggregate the metrics across different approaches and to see a global trend between forgetting and forward transfer, we compute the Spearman rank correlation between the $\avgfgt$ and $\avgfwt$ for the k-shot fine-tuning evaluation. Table \ref{tab:spearman-correlation-full-finetune-results} shows the correlation values for different values of `k' for both randomly initialized and pre-trained models. It can be seen from the table that most of the entries are above $0.5$ and statistically significant ($p < 0.01$) showing that reducing forgetting improves the forward transfer across the board.

From the results, we can see that less forgetting generally leads to better forward transfer. Thus, our claim that less forgetting is a good inductive bias for forward transfer still holds. 

\begin{figure}[t]
\centering
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar10_Random_init_avg_forgetting.pdf}
      % \caption{\small Average Forgetting ($\downarrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar10_Random_init_avg_fwt_full_finetune.pdf}
      % \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar100_Random_init_avg_forgetting.pdf}
    %   \caption{\small Average Forgetting ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar100_Random_init_avg_fwt_full_finetune.pdf}
    %   \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/cifar100_superclasses_Random_init_avg_forgetting.pdf}
      \caption{\small Average Forgetting ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/cifar100_superclasses_Random_init_avg_fwt_full_finetune.pdf}
      \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}\hfill

% \negspace{-1mm}
\caption{\small Comparing average forgetting with average forward transfer for different continual learning methods using random initialization on the Split CIFAR-10, Split CIFAR-100 and CIFAR-100 Superclasses benchmarks. Here, we evaluate the forward transfer through k-shot fine-tuning. }
\label{fig:cifar_fgt_fwt_full_finetune}
% \negspace{-3mm}
\end{figure}

\begin{figure}[t]
\centering
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear10_Random_init_avg_forgetting.pdf}
      % \caption{\small Average Forgetting ($\downarrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear10_Random_init_avg_fwt_full_finetune.pdf}
      % \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear100_Random_init_avg_forgetting.pdf}
    %   \caption{\small Average Forgetting ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear100_Random_init_avg_fwt_full_finetune.pdf}
    %   \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_imagenet_Random_init_avg_forgetting.pdf}
      \caption{\small Average Forgetting ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_imagenet_Random_init_avg_fwt_full_finetune.pdf}
      \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}\hfill

% \negspace{-1mm}
\caption{\small Comparing average forgetting with average forward transfer for different continual learning methods using random initialization on the CLEAR10, CLEAR100 and Split ImageNet benchmarks. Here, we evaluate the forward transfer through k-shot fine-tuning.}
\label{fig:clear_imagenet_fgt_fwt_full_finetune}
% \negspace{-3mm}
\end{figure}

\begin{figure}[t]
\centering
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar10_Pre-train_avg_forgetting.pdf}
      % \caption{\small Average Forgetting ($\downarrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar10_Pre-train_avg_fwt_full_finetune.pdf}
      % \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar10_imagenet_eval.pdf}
      % \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar100_Pre-train_avg_forgetting.pdf}
      % \caption{\small Average Forgetting ($\downarrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar100_Pre-train_avg_fwt_full_finetune.pdf}
      % \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar100_imagenet_eval.pdf}
      % \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/cifar100_superclasses_Pre-train_avg_forgetting.pdf}
    %   \caption{\small $\avgfgt$ ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/cifar100_superclasses_Pre-train_avg_fwt_full_finetune.pdf}
    %   \caption{\small $\avgfwt$ ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/cifar100_superclasses_imagenet_eval.pdf}
    %   \caption{\small Upstream Accuracy ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear10_Pre-train_avg_forgetting.pdf}
    %   \caption{\small $\avgfgt$ ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear10_Pre-train_avg_fwt_full_finetune.pdf}
    %   \caption{\small $\avgfwt$ ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear10_imagenet_eval.pdf}
    %   \caption{\small Upstream Accuracy ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear100_Pre-train_avg_forgetting.pdf}
      \caption{\small $\avgfgt$ ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear100_Pre-train_avg_fwt_full_finetune.pdf}
      \caption{\small $\avgfwt$ ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear100_imagenet_eval.pdf}
      \caption{\small Upstream Accuracy ($\uparrow$ better)}
\end{subfigure}

% \negspace{-1mm}
\caption{\small Comparing average forgetting with average forward transfer for different continual learning methods that train the model from a pre-trained ImageNet model on the Split CIFAR-10, Split CIFAR-100, CIFAR-100 Superclasses, CLEAR10 and CLEAR100 benchmarks. We also show the accuracy of the models on the upsteam ImageNet data. Here, we evaluate the forward transfer through k-shot fine-tuning. }
\label{fig:fgt_fwt_pretrain_full_finetune}
% \negspace{-3mm}
\end{figure}

\begin{table*}[htb]
    \centering
    \begin{adjustbox}{width=\columnwidth,center}
		\begin{tabular}{l|ccc|ccc}
			\toprule
			  \multirow{2}{0.12\linewidth}{Dataset} &  \multicolumn{3}{c|}{Random Init} & \multicolumn{3}{c}{Pretrain}  \\ \cline{2-7}
			  & $k=5$ & $k=10$ & $k=20$  & $k=5$ & $k=10$ & $k=20$  \\ \hline
            Split CIFAR-10 & $0.49$ & $0.5$ & $0.49$ & $0.64$ & $0.57$ & $0.51$  \\
            Split CIFAR-100 & $0.92$ & $0.93$ & $0.83$ & $0.87$ & $0.88$ & $0.86$  \\
            CIFAR100 Superclasses & $0.16$ ($0.40$) & $0.24$ ($0.20$) & $0.28$ ($0.14$) & $0.28$ ($0.13$) & $0.33$ ($0.07$) & $0.38$ ($0.04$)  \\
            CLEAR10 & $0.68$ & $0.68$ & $0.68$ & $0.18$ ($0.34$) & $0.25$ ($0.18$) & $0.53$  \\
            CLEAR100 & $0.6$ & $0.59$ & $0.61$ & $0.87$ & $0.87$ & $0.83$  \\
            Split ImageNet & $0.86$ & $0.81$ & $0.83$ & - & - & -  \\
            \bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption[]{\small  Spearman correlation between $\avgfgt$ and $\avgfwtfull$ for different $k$, which computes the correlation over different settings (different training methods and random runs). Here, $\avgfwtfull$ is defined like $\avgfwt$, but instead of using k-shot linear probing for evaluation, we use k-shot fine-tuning evaluation (i.e., fine-tuning the entire model). $p$-values are shown in parenthesis if greater than or equal to $0.01$.}
	\label{tab:spearman-correlation-full-finetune-results}
\end{table*}

\subsection{Ablation Study on the replay buffer size}

For the Experience Replay (ER) baseline, we perform experiments on the Split CIFAR-100 and CIFAR-100 Superclasses benchmarks to study the effect of the replay buffer size on the $\avgfgt$ and $\avgfwt$ metrics. The results when using random initialization are shown in~\Figref{fig:cifar_fgt_fwt_er_ablation} while the results when using a pre-trained model as initialization are shown in~\Figref{fig:cifar_fgt_fwt_pretrain_er_ablation}. From the results, we can see that increasing $m$ usually leads to less forgetting and thus more forward transfer. Therefore, our claim that less forgetting is a good inductive bias for forward transfer still holds. 


\begin{figure}[t]
\centering
% \begin{subfigure}{.5\textwidth}
%       \centering
%       \includegraphics[width=.99\linewidth]{figs/split_cifar10_Random_init_avg_forgetting_er_ablation.pdf}
%       % \caption{\small Average Forgetting ($\downarrow$ better)}
% \end{subfigure}\hfill
% \begin{subfigure}{.5\textwidth}
%       \centering
%       \includegraphics[width=.99\linewidth]{figs/split_cifar10_Random_init_avg_fwt_er_ablation.pdf}
%       % \caption{\small Average Forward Transfer ($\uparrow$ better)}
% \end{subfigure}

\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar100_Random_init_avg_forgetting_er_ablation.pdf}
    %   \caption{\small Average Forgetting ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar100_Random_init_avg_fwt_er_ablation.pdf}
    %   \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/cifar100_superclasses_Random_init_avg_forgetting_er_ablation.pdf}
      \caption{\small Average Forgetting ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/cifar100_superclasses_Random_init_avg_fwt_er_ablation.pdf}
      \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}\hfill

% \negspace{-1mm}
\caption{\small Comparing average forgetting with average forward transfer for the ER method with different replay buffer sizes using random initialization on the Split CIFAR-100 and CIFAR-100 Superclasses benchmarks.} 
\label{fig:cifar_fgt_fwt_er_ablation}
% \negspace{-3mm}
\end{figure}

\begin{figure}[t]
\centering
% \begin{subfigure}{.5\textwidth}
%       \centering
%       \includegraphics[width=.99\linewidth]{figs/split_cifar10_Pre-train_avg_forgetting_er_ablation.pdf}
%       % \caption{\small Average Forgetting ($\downarrow$ better)}
% \end{subfigure}\hfill
% \begin{subfigure}{.5\textwidth}
%       \centering
%       \includegraphics[width=.99\linewidth]{figs/split_cifar10_Pre-train_avg_fwt_er_ablation.pdf}
%       % \caption{\small Average Forward Transfer ($\uparrow$ better)}
% \end{subfigure}

\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar100_Pre-train_avg_forgetting_er_ablation.pdf}
    %   \caption{\small Average Forgetting ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar100_Pre-train_avg_fwt_er_ablation.pdf}
    %   \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/cifar100_superclasses_Pre-train_avg_forgetting_er_ablation.pdf}
      \caption{\small Average Forgetting ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/cifar100_superclasses_Pre-train_avg_fwt_er_ablation.pdf}
      \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}\hfill

% \negspace{-1mm}
\caption{\small Comparing average forgetting with average forward transfer for the ER method with different replay buffer sizes that trains the model from a pre-trained ImageNet model on the Split CIFAR-100 and CIFAR-100 Superclasses benchmarks.} 
\label{fig:cifar_fgt_fwt_pretrain_er_ablation}
% \negspace{-3mm}
\end{figure}

\subsection{Results for EWC and Vanilla L2 Regularization}

In this section, we provide some results for the EWC method~\citep{Kirkpatrick2016EWC} and the vanilla L2 regularization (a variant of EWC where the fisher information matrix is replaced with an identity matrix) using ResNet18 as the model architecture with random initialization on the Split CIFAR-10 benchmark. For $\lambda$ in EWC, we consider the range $\{10, 50, 100, 200\}$ and select the best one based on the performance on the validation data. For $\lambda$ in vanilla L2 regularization, we consider the range $\{10, 1, 0.1, 0.01\}$ and select the best one based on the performance on the validation data. Experimenting with EWC on larger models and longer benchmarks is computationally very expensive. The comparison of EWC with FT and vanilla L2 regularization is given in~\Tabref{tab:ewc-results}. It can be seen from the table that less forgetting leads to better forward transfer. Thus, our claim that less forgetting is a good inductive bias for forward transfer still holds.  


\begin{table*}[t]
    \centering
    \begin{adjustbox}{width=\columnwidth,center}
		\begin{tabular}{l|cccc}
			\toprule
			   Method &  $\avgfgt \uparrow$ & $\avgfwt$ (k=10) & $\avgfwt$ (k=20) & $\avgfwt$ (k=100) \\ \hline
             FT & -32.06 $\pm$ 4.27  & 71.57 $\pm$ 1.87 & 74.89 $\pm$ 2.72 & 79.14 $\pm$ 2.06  \\
             Vanilla L2 Reg. ($\lambda=0.01$) & -26.41 $\pm$ 3.02 & 72.68 $\pm$ 2.28 & 76.58 $\pm$ 2.73 & 80.88 $\pm$ 1.92 \\
             %EWC ($\lambda=10$) & -29.54 $\pm$ 2.23  73.11 $\pm$ 2.78 & 76.41 $\pm$ 2.95 & 81.27 $\pm$ 2.15 \\
             EWC ($\lambda=100$) & -21.99 $\pm$ 3.51 & 74.27 $\pm$ 2.09 & 77.00 $\pm$ 2.03 & 81.98 $\pm$ 0.93 \\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption[]{\small Results for EWC and Vanilla L2 Regularization using ResNet18 as the model architecture with random initialization on the Split CIFAR-10 benchmark. $\lambda$ is a hyper-parameter that controls the regularization strength of EWC and Vanilla L2 Regularization.  The numbers are percentages. }
	\label{tab:ewc-results}
\end{table*}


\section{Discussion} \label{sec:discussion}

\subsection{Relatedness of Tasks and our conclusions}

We note here that task relatedness bears significant effect on the relationship between forgetting and forward transfer.
%
The benchmarks that we considered in this work either have very similar tasks (CLEAR10/100), where the same classes are observed over a 10 years period, or unrelated tasks (Split CIFAR10/100, ImageNet), where disjoint classes are observed in each task.
%
In both cases, less forgetting improved the forward transfer, although for more similar tasks the improvement is more significant as intuitively expected.
%
We did not observe that ``unrelatedness'' of tasks leads to negative transfer.
%
However, if tasks were negatively related to begin with then less forgetting would intuitively lead to negative transfer.
%
But in our experience negatively related tasks are very rare and in practical machine learning systems many tasks can learn from each other (which is the basis of transfer learning, multitask learning, etc.).
%
We would like to emphasize that the point of the paper is precisely to show that when tasks are somewhat related, and observed in a continual setting, less forgetting improves forward transfer.
%
It is in this setting that some of the previous works~\citep{hadsell2020embracing, wolczyk2021continual} concluded that less forgetting does not improve end-to-end forward transfer.
%
We show here that even on such tasks less forgetting improves the representational measure of forward transfer. 
