\section{Related Works} \label{sec:related_works}

Continual Learning (also known as Life-long Learning)~\citep{ring1994continual,thrun1995lifelong} aims to learn a model on a sequence of tasks that has good performance on all the tasks observed so far.
%
However, SGD training, relying on IID assumption of data, tends to result in a degraded performance on older tasks, when the model is updated on new tasks.
%
This phenomenon is known as catastrophic forgetting~\citep{McCloskey1989CatastrophicII,DBLP:journals/corr/GoodfellowMDCB13} and it has been a main focus of continual learning research.
%
There are several methods that have been proposed to alleviate catastrophic forgetting, ranging from regularization-based approaches~\citep{Kirkpatrick2016EWC,aljundi2017memory,nguyen2017variational,Zenke2017Continual}, to methods based on episodic memory~\citep{lopez2017gradient,chaudhry2018efficient,aljundi2019online,hayes2018memory,riemer2018learning,rolnick18,ameyaGdumb2020} to the algorithms based on parameter isolation~\citep{yoon2018lifelong,PackNet,SuperMaskInSuperPosition,mirzadeh2021linear,farajtabar2020orthogonal}. 
%
Besides the algorithmic innovations to reduce catastrophic forgetting, recently some works looked at the role of training regimes~\citep{mirzadeh_understanding_continual} and network architectures \citep{WideNNs-CL,mirzadeh2022architecture} for understanding the catastrophic forgetting phenomenon. 

While a learner that reduces catastrophic forgetting tries to preserve the knowledge of the past tasks, often what is more important is to utilize the accrued knowledge to learn new tasks more efficiently, a phenomenon known as forward transfer ~\citep{lopez2017gradient, chaudhry2018efficient}.
%
In most existing works, the forward transfer to a task is measured as the learning accuracy of the task \emph{after training on the task is finished}.
%
\citet{hadsell2020embracing} and \citet{wolczyk2021continual} argued that continual learning methods that avoid catastrophic forgetting do not improve the forward transfer, in fact, sometimes the catastrophic forgetting is reduced at the expense of the forward transfer (as measured by the learning accuracy).
%
This begs the question whether reducing catastrophic forgetting is a good objective for continual learning research or should the community shift focus on the forward transfer as there seems to be a tug of war between the two?

Contrary to previous work, here, we take an auxiliary evaluation perspective to forward transfer where instead of asking whether reducing forgetting on previous tasks, during training on the current task, improves the current task learning, we ask whether a learner that has less forgetting on previous tasks, results in network representations that can quickly be adapted to new tasks?
%
We argue that this mode of measuring forward transfer decouples the notion of transfer from the restricted updates on the current task employed by a continual learner to avoid forgetting on previous tasks.
%
To the best of our knowledge, most similar to our work is \cite{javed2019meta,beaulieu2020learning} who also looked at the network representations in the context of continual learning.
%
But they took a converse perspective -- arguing that learning transferable representations via meta-learning alleviates catastrophic forgetting.

%A lot of methods have been proposed to address the catastrophic forgetting issue in continual learning. They are mainly from three categories: (1) model weights regularization based approaches like EWC~\citep{kirkpatrick2017overcoming}; (2) replaying the past knowledge like Dark Experience Replay~\citep{buzzega2020dark}; (3) gradient projection based approaches like GEM~\citep{lopez2017gradient} and A-GEM~\citep{chaudhryRRE19}. 

%\paragraph{Transfer Learning. } Transfer learning aims to transfer knowledge from a source task to a target task. Usually, the source task has abundant data while the target task has limited data. We first train a model on the source task and then fine-tune it on the target task. A typical example is pre-training a model on the ImageNet dataset and then fine-tune it on downstream tasks~\citep{kolesnikov2020big}. 



