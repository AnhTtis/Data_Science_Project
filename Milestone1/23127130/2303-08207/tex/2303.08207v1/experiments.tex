\section{Experiments \& Results} \label{sec:experiments}

\subsection{Setup}

We now briefly describe the experimental setup including the benchmarks, approaches and training details. More details can be found in Appendix~\ref{app:exp-detail}.
%
After the experimental details, we provide the main results of the paper.  


\paragraph{Benchmarks} 

\begin{itemize}
    \item \textbf{Split CIFAR-10}: We split CIFAR-10 dataset~\citep{krizhevsky2009learning} into 5 disjoint subsets corresponding to 5 tasks. Each task has 2 classes. 
    \item \textbf{Split CIFAR-100}: We split CIFAR-100 dataset~\citep{krizhevsky2009learning} into 20 disjoint subsets corresponding to 20 tasks. Each task has 5 classes. 
    \item \textbf{CIFAR-100 Superclasses}: We split CIFAR-100 dataset into 5 disjoint subsets corresponding to 5 tasks. Each task has 20 classes from 20 superclasses in CIFAR-100 respectively.
    \item \textbf{CLEAR}: This is a continual image classification benchmark by \citet{lin2021clear}, built from YFCC100M~\citep{thomee2016yfcc100m} images, containing the evolution of object categories from years 2005-2014. There are 10 tasks each containing images in chronological order from years (2005-2014). We consider both CLEAR10 (consisting of 10 object classes) and CLEAR100 (consisting of 100 object classes) variants of the benchmark. 
    \item \textbf{Split ImageNet}: We split ImageNet~\citep{russakovsky2015imagenet} dataset into 100 disjoint subsets corresponding to 100 tasks. Each task has 10 classes. 
\end{itemize}

For all the benchmarks, except split ImageNet, we considered continual learning from a randomly initialized model as well as from a pre-trained ImageNet model.
%
For split ImageNet, we only considered continual learning from a randomly initialized model. 

\paragraph{Approaches}\footnote{The EWC~\citep{Kirkpatrick2016EWC} results are in Appendix~\Tabref{tab:ewc-results}.}

Below we describe the approaches considered in this work.
%
Except for the independent baseline, all other baselines reuse the model \emph{i.e.} continue training the same model used for the previous tasks. 

\begin{itemize}
    \item \textbf{Independent (IND)}: Trains a model from an initial model (either random initialized or pre-trained) on each task independently. 
    
    \item \textbf{Finetuning (FT}): Trains a single model on all the tasks in a sequence, one task at a time. 
    
    \item \textbf{Linear-Probing-Finetuning (LP-FT}): LP-FT~\citep{kumar2021fine} is the same as FT except that before each task training, we first learn a task-specific classifier $\classifier$ for the task via linear probing and then train both the feature extractor $\representation$ and the classifier $\classifier$ on the task to reduce the feature drift.
    
    \item \textbf{Multitask (MT)}: Trains the model on the data from both the current and previous tasks using the multitask training objective~(\eqref{obj:multitask} in Appendix). The data of previous tasks is used as an auxiliary loss while learning on the current task. 
    
    \item \textbf{Experience Replay (ER)}: Uses a replay buffer $\mathcal{M}=\cup_{i=1}^{N-1} \mathcal{M}_i$ when learning on the task sequence, where $\mathcal{M}_i$ stores $m$ examples per class from the task $T_i$. It trains the model on the data from both the current task and the replay buffer when learning on the current task using an ER training objective~(\eqref{obj:er} in Appendix) \citep{chaudhry2019tiny}. There are two main differences between MT and ER: (1) MT uses all the data from the previous tasks while ER only uses limited data from the previous tasks; (2) MT chooses the coefficient for the auxiliary loss via cross-validation while ER always set it to be $1$. 
    
    \item \textbf{AGEM}: Projects the gradient when doing the SGD updates so that the average loss on the data from the episodic memory does not increase \citep{chaudhry2018efficient}. The episodic memory stores $m$ examples per class from each task.
    
    \item \textbf{FOMAML}: First-order MAML (FOMAML) is a meta-learning approach proposed by~\citeauthor{finn2017model}. We modify FOMAML such that it can be used in the continual learning setting. Similar to MT, FOMAML uses all the data from the previous tasks when learning the current task. The training objective of FOMAML aims to enable knowledge transfer between different batches from the same task. The learning algorithm for FOMAML is provided in Appendix~\ref{app:baselines}. 
    
    
\end{itemize}

\paragraph{Architecture and Training details. }  
%
We use ResNet50~\citep{he2016deep} architecture as the feature extractor $\representation$ on all benchmarks.
%
On CLEAR10 and CLEAR100, we use a single classification head $\classifier$ that is shared by all the tasks (single-head architecture) while on other benchmarks, we use a separate classification head $\classifier_i$ for each task $T_i$ (multi-head architecture).
%
We use SGD to update the model parameters and use cosine learning rate scheduling~\citep{loshchilov2016sgdr} to adjust the learning rate during training. For LP-FT, we use a base learning rate of $0.001$ while for other baselines, we use a base learning rate of $0.01$. 
%
When using a random initialization as the initial model $f_0$, on split CIFAR-10, split CIFAR-100 and CIFAR-100 superclasses, we train the model for $50$ epochs per task while on CLEAR10, CLEAR100 and Split ImageNet, we train the model for $100$ epochs per task. 
%
When using a pre-trained model as the initial model $f_0$, on all benchmarks, we train the model for $20$ epochs per task as we found it sufficient for training convergence.
%
For CLEAR10 and CLEAR100, the results are averaged over 5 different runs with different random seeds each corresponding to a different network initialization, where the task order is fixed. 
%
For other benchmarks, the results are averaged over 5 different runs, where each run corresponds to a different random ordering of tasks.  
%
The results are reported as averages and 95\% confidence interval estimates of these 5 runs.
%
For k-shot linear probing, we use SGD with a fixed learning rate of $0.01$.
%
We train the classifier head $\hat{\classifier}$ for $100$ epochs on the k-shot dataset $\mathcal{S}^k_{j+1}$ as we found it sufficient for training convergence.
%
On CLEAR100, we consider $k \in \{5, 10, 20, 40\}$ while on other benchmarks, we consider $k \in \{5, 10, 20, 100\}$.

\subsection{Results} \label{sec:results}

\subsubsection*{Less forgetful representations transfer better}

\begin{figure}[t]
\centering
\begin{subfigure}{.48\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar10_Random_init_avg_forgetting.pdf}
      % \caption{\small Average Forgetting ($\downarrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.48\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar10_Random_init_avg_fwt.pdf}
      % \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.48\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear100_Random_init_avg_forgetting.pdf}
      \caption{\small Average Forgetting ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.48\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/clear100_Random_init_avg_fwt.pdf}
      \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}\hfill

\negspace{-1mm}
\caption{\small Comparing average forgetting with average forward transfer for different continual learning methods using random initialization on the Split CIFAR-10 and CLEAR100 benchmarks. }
\label{fig:split_cifar10_clear100_fgt_fwt}
\negspace{-3mm}
\end{figure}

We assess the compatibility between forgetting and transferability through $\avgfgt$ and $\avgfwt$ metrics described in \Secref{sec:problem_setup}.
%
\twoFigref{fig:split_cifar100_fgt_fwt}{fig:split_cifar10_clear100_fgt_fwt} show these two metrics for Split CIFAR-100, Split-CIFAR10 and CLEAR100, respectively when the continual learning experience begins from a \textbf{randomly initialized model} (the comparison on the other benchmarks is provided in the Appendix~\ref{app:compare-fgt-fwt}).
%
It can be seen from the figures that if a model has less average forgetting, the corresponding model representations have a better K-shot forward transfer.
%
For example, on all the three benchmarks visualized in the figures, FOMAML and MT tend to have the least amount of average forgetting.
%
Consequently, the $\avgfwt$ of these two baselines is higher compared to all the other baselines, for all the values of $k$ considered in this work.
%
Note that the ranking of other methods in terms of correspondence between forgetting and forward transfer is roughly maintained as well.
%
This shows that \emph{when continual learning experience begins from a randomly initialized model, retaining the knowledge of the past tasks or forgetting less on those tasks is a good inductive bias for forward transfer.}

Recently, \citet{mehta2021empirical} showed that pre-trained models tend to forget less, compared to randomly initialized models, when trained on a sequence of tasks.
%
We build upon this observation and ask if forgetting less on both the upstream (pre-trained) task, and downstream tasks improve the transferability of the representations?
%
\Figref{fig:split_cifar10_cifar100_fgt_fwt_pretrain} shows the comparison between forgetting (left) and forward transfer (middle) on Split CIFAR-10 and Split CIFAR-100 when the continual learning experience begins from a pre-trained model (the comparison on the other benchmarks is provided in the Appendix~\ref{app:compare-fgt-fwt}).
%
It can be seen from the figure that except for LP-FT, less forgetting is a good indicator of a better forward transfer. 
%
In order to understand, why LP-FT has a better forward transfer, compared to FOMAML and MT, despite having higher forgetting on the continual learning benchmark at hand, we evaluate the continually updated representations on the upstream data (test set of ImageNet).
%
The evaluation results are given in the right plot of \Figref{fig:split_cifar10_cifar100_fgt_fwt_pretrain}.
%
From the plot, it can be seen that LP-FT has retained better upstream performance (relatively speaking) compared to the other baselines. 
%
This follows our general thesis that \emph{retaining `previous knowledge', evidenced here by the past performance on both the upstream and downstream tasks, is a good inductive bias for forward transfer.}
%
If instead of freezing the representations and just updating the classifier, we finetune the whole model in the auxiliary evaluation, the less forgetful representations still transfer better (refer to Appendix~\ref{app:fwt-fine-tuning}).
%We also provide the results for finetuning the whole model during the k-shot evaluation, instead of freezing the feature extractor and updating the classifier, in Appednix~\ref{app:fwt-fine-tuning}. Our main conclusions also hold for the full finetuning.


\begin{figure}[t]
\centering
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar10_Pre-train_avg_forgetting.pdf}
      % \caption{\small Average Forgetting ($\downarrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar10_Pre-train_avg_fwt.pdf}
      % \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar10_imagenet_eval.pdf}
      % \caption{\small Average Forward Transfer ($\uparrow$ better)}
\end{subfigure}

\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar100_Pre-train_avg_forgetting.pdf}
      \caption{\small $\avgfgt$ ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar100_Pre-train_avg_fwt.pdf}
      \caption{\small $\avgfwt$ ($\uparrow$ better)}
\end{subfigure}\hfill
\begin{subfigure}{.33\textwidth}
      \centering
      \includegraphics[width=.99\linewidth]{figs/split_cifar100_imagenet_eval.pdf}
      \caption{\small Upstream Accuracy ($\uparrow$ better)}
\end{subfigure}

\negspace{-1mm}
\caption{\small Comparing average forgetting with average forward transfer for different continual learning methods that train the model from a pre-trained ImageNet model on the Split CIFAR-10 and Split CIFAR-100 benchmarks. We also show the accuracy of the models on the upsteam ImageNet data. Since CIFAR-10 and CIFAR-100 images have different image resolution than that of ImageNet images, we need to resize the ImageNet test images from $224 \times 224$ to $32\times 32$ in order to get meaningful accuracy of the models trained on the Split CIFAR-10 and Split CIFAR-100 benchmarks on the upstream ImageNet data (although the accuracy of the pre-trained model on the resized ImageNet test images is significantly reduced). }
\label{fig:split_cifar10_cifar100_fgt_fwt_pretrain}
\negspace{-3mm}
\end{figure}

In order to aggregate the metrics across different methods and to see a global trend between forgetting and forward transfer, we compute the Spearman rank correlation between $\avgfgt$ and $\avgfwt$.
%
Table \ref{tab:spearman-correlation-results} shows the correlation values for different values of `k' for both randomly initialized and pre-trained models.
%
It can be seen from the table that most of the entries are above $0.5$ and statistically significant ($p < 0.01$) showing that reducing forgetting improves the forward transfer across the board.

\begin{table*}[h!]
    \centering
    \begin{adjustbox}{width=\columnwidth,center}
		\begin{tabular}{l|ccc|ccc}
			\toprule
			  \multirow{2}{0.12\linewidth}{Dataset} &  \multicolumn{3}{c|}{Random Init} & \multicolumn{3}{c}{Pretrain}  \\ \cline{2-7}
			  & $k=5$ & $k=10$ & $k=20$  & $k=5$ & $k=10$ & $k=20$  \\ \hline
            Split CIFAR-10 & $0.64$ & $0.56$ & $0.65$ & $0.68$ & $0.66$ & $0.58$  \\
            Split CIFAR-100 & $0.92$ & $0.88$ & $0.91$ & $0.86$ & $0.86$ & $0.88$  \\
            CIFAR100 Superclasses & $0.3$ ($0.11$) & $0.4$ ($0.03$) & $0.4$ ($0.03$) & $0.16$ ($0.40$) & $0.46$ ($0.01$) & $0.62$  \\
            CLEAR10 & $0.7$ & $0.65$ & $0.73$ & $0.43$ ($0.02$) & $0.37$ ($0.04$) & $0.64$  \\
            CLEAR100 & $0.58$ & $0.59$ & $0.53$ & $0.83$ & $0.8$ & $0.81$  \\
            Split ImageNet & $0.85$ & $0.85$ & $0.79$ & - & - & -  \\
            \bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption[]{\small  Spearman correlation between $\avgfgt$ and $\avgfwt$ for different $k$, which computes the correlation over different settings (different training methods and random runs). $p$-values are shown in parenthesis if greater than or equal to $0.01$.}
	\label{tab:spearman-correlation-results}
\end{table*}

\subsubsection*{Less forgetful representations are more diverse}

\begin{table*}[t]
    \centering
    \begin{adjustbox}{width=0.85\columnwidth,center}
		\begin{tabular}{l|l|cc|cc}
			\toprule
			  \multirow{2}{0.12\linewidth}{Dataset} &  \multirow{2}{0.08\linewidth}{Method} &   \multicolumn{2}{c}{Random Init} & \multicolumn{2}{c}{Pre-trained}   \\ \cline{3-6}
			  & & $\avgfgt \uparrow$ & $\avgfdiv \uparrow$ &   $\avgfgt \uparrow$ & $\avgfdiv \uparrow$ \\ \hline
			 \multirow{6}{0.12\linewidth}{Split CIFAR-10}
            & FT & -28.18 $\pm$ 2.97 & 35.59 $\pm$ 10.52 & -29.01 $\pm$ 7.97 & 60.18 $\pm$ 36.35 \\
            & LP-FT & - & - & -3.39 $\pm$ 1.06 & {\bf 171.41} $\pm$ 13.41 \\
            & ER (m=50) & -9.18 $\pm$ 1.50 & 37.33 $\pm$ 14.66 & -7.15 $\pm$ 1.97 & 66.18 $\pm$ 35.74 \\
            & AGEM (m=50) & -13.77 $\pm$ 2.38 & 35.79 $\pm$ 16.34 & -19.26 $\pm$ 5.01 & 60.77 $\pm$ 41.80 \\
            & MT & -3.88 $\pm$ 5.86 & 36.88 $\pm$ 13.21 & -4.83 $\pm$ 5.56 & 86.88 $\pm$ 21.82 \\
            & FOMAML & {\bf -0.75} $\pm$ 1.39 & {\bf 45.52} $\pm$ 7.82 & -1.40 $\pm$ 0.61 & 65.26 $\pm$ 10.36 \\ \hline
			  \multirow{6}{0.12\linewidth}{Split CIFAR-100} 
    		& FT & -25.83 $\pm$ 2.43 & 224.27 $\pm$ 3.63 & -24.33 $\pm$ 4.19 & 263.31 $\pm$ 27.46 \\
            & LP-FT & - & - & -4.46 $\pm$ 0.46 & {\bf 332.10} $\pm$ 2.97 \\
            & ER (m=20) & -9.44 $\pm$ 1.11 & 225.95 $\pm$ 2.38 & -9.19 $\pm$ 0.28 & 281.31 $\pm$ 3.59 \\
            & AGEM (m=20) & -18.70 $\pm$ 1.00 & 224.46 $\pm$ 2.93 & -20.05 $\pm$ 3.12 & 260.01 $\pm$ 20.32 \\
            & MT & -9.35 $\pm$ 4.96 & 225.33 $\pm$ 4.62 & -7.93 $\pm$ 4.04 & 277.14 $\pm$ 8.31 \\
            & FOMAML & {\bf -3.05} $\pm$ 0.98 & 225.87 $\pm$ 5.31 & -4.40 $\pm$ 0.20 & 271.56 $\pm$ 7.45 \\ \hline
			  \multirow{6}{0.12\linewidth}{CIFAR-100 Superclasses} 
    		 & FT & -14.45 $\pm$ 1.02 & 458.73 $\pm$ 12.99 & -13.51 $\pm$ 0.56 & 599.29 $\pm$ 13.65 \\
            & LP-FT & - & - & -2.66 $\pm$ 0.53 & {\bf 702.43} $\pm$ 4.10 \\
            & ER (m=5) & -11.33 $\pm$ 1.79 & 463.78 $\pm$ 7.86 & -11.36 $\pm$ 1.44 & 600.23 $\pm$ 23.86 \\
            & AGEM (m=5) & -12.28 $\pm$ 0.84 & 459.65 $\pm$ 14.52 & -12.11 $\pm$ 0.76 & 594.70 $\pm$ 27.51 \\
            & MT & -1.30 $\pm$ 4.02 & 465.47 $\pm$ 7.84 & -5.50 $\pm$ 3.65 & 601.38 $\pm$ 16.92 \\
            & FOMAML & {\bf 1.99} $\pm$ 0.76 & {\bf 470.27} $\pm$ 5.17 & -1.24 $\pm$ 0.44 & 620.66 $\pm$ 10.34 \\ \hline
			  \multirow{6}{0.12\linewidth}{CLEAR10} 
			 & FT & 0.93 $\pm$ 1.01 & 76.72 $\pm$ 1.70 & 0.14 $\pm$ 0.42 & 265.72 $\pm$ 1.08 \\
            & LP-FT & - & - & 0.87 $\pm$ 0.11 & {\bf 281.78} $\pm$ 0.34 \\
            & ER (m=10) & 1.79 $\pm$ 0.24 & 76.76 $\pm$ 1.62 & -0.05 $\pm$ 0.23 & 263.89 $\pm$ 0.82 \\
            & AGEM (m=10) & 2.03 $\pm$ 0.86 & 76.00 $\pm$ 1.79 & -0.01 $\pm$ 0.19 & 266.36 $\pm$ 1.02 \\
            & MT & {\bf 4.49} $\pm$ 0.99 & {\bf 79.01} $\pm$ 1.41 & 0.77 $\pm$ 0.51 & 265.25 $\pm$ 1.49 \\
            & FOMAML & {\bf 4.49} $\pm$ 0.56 & 77.98 $\pm$ 1.27 & 0.84 $\pm$ 0.34 & 262.88 $\pm$ 1.28 \\ \hline
			  \multirow{6}{0.12\linewidth}{CLEAR100} 
            & FT & 5.06 $\pm$ 0.29 & 179.47 $\pm$ 1.01 & -0.03 $\pm$ 0.13 & 441.22 $\pm$ 0.50 \\
            & LP-FT & - & - & 1.52 $\pm$ 0.07 & {\bf 488.94} $\pm$ 0.71 \\
            & ER (m=5) & 5.53 $\pm$ 0.24 & 181.39 $\pm$ 1.56 & 0.34 $\pm$ 0.18 & 440.48 $\pm$ 0.79 \\
            & AGEM (m=5) & 4.94 $\pm$ 0.36 & 179.12 $\pm$ 1.03 & 0.04 $\pm$ 0.14 & 441.26 $\pm$ 0.27 \\
            & MT & 8.65 $\pm$ 0.96 & {\bf 186.16} $\pm$ 3.31 & 1.56 $\pm$ 0.15 & 444.38 $\pm$ 0.41 \\
            & FOMAML & {\bf 9.21} $\pm$ 0.12 & 184.38 $\pm$ 1.58 & 1.55 $\pm$ 0.16 & 440.30 $\pm$ 0.71 \\ \hline
             \multirow{5}{0.12\linewidth}{Split ImageNet} 
            & FT & -54.62 $\pm$ 2.26 & 271.98 $\pm$ 0.96 & - & - \\
            & ER (m=10) & -27.56 $\pm$ 0.63 & 289.18 $\pm$ 3.99 & - & - \\
            & AGEM (m=10) & -50.53 $\pm$ 1.58 & 274.07 $\pm$ 1.20 & - & - \\
            & MT & -16.79 $\pm$ 0.69 & 274.88 $\pm$ 2.13 & - & - \\
            & FOMAML & {\bf -10.58} $\pm$ 0.69 & {\bf 305.63} $\pm$ 5.59 & - & - \\ 
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption[]{\small Comparing $\avgfgt$ with $\avgfdiv$. The numbers for $\avgfgt$ are percentages. {\bf Bold} numbers are superior results. }
	\label{tab:feature-score-results}
	\negspace{-5mm}
\end{table*}


We now look at what makes the less forgetful representations amenable for better forward transfer.
%
We hypothesize that less forgetful representations maintain more diversity and discrimination in the features making it easy to learn a classifier head on top leading to better forward transfer.
%
To measure this diversity of representations, we look at the feature diversity score $\avgfdiv$, as defined in \Eqref{eq:fdiv}, and compare it with the average forgetting score $\avgfgt$.
%
\Tabref{tab:feature-score-results} shows the results on different benchmarks and approaches.
%
It can be seen from the table that, for randomly initialized models, less forgetting, evidenced by higher $\avgfgt$ score, generally leads to representations that have higher $\avgfdiv$ score.
%
Similarly, on pre-trained models, methods with lower overall forgetting between the upstream and downstream tasks, such as LP-FT, leads to the highest $\avgfdiv$ score.
%
These results suggest that less forgetful representations tend to be more diverse and discriminative. 

