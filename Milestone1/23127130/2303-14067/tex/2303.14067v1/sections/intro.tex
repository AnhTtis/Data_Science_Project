\section{INTRODUCTION}
We envision autonomous systems that can perceive and perform tasks across large, building-scale spaces \cite{hawes2017}, \cite{veloso2015}, \cite{khandelwal2017} to serve needs across society, such as care-taking tasks in assisted living facilities and supply chain tasks in warehouses. In order to be effective, such systems must infer the objects present in the environment as well as predict the outcomes of actions afforded \cite{gibson1977} by these objects. In essence, robots need to perceive actions that are currently afforded by the environment, and not just the objects to be acted upon. For example, observing a cup should inform the system of an optimal location to achieve the action ``Grasp Cup‚Äù. In small enough workspaces, a robot can simply look for the objects required for the task, but as the environment grows this becomes infeasible. 

We are inspired by the idea that despite the aforementioned challenges, there is structure to human environments. Buildings, in most cases, are designed for efficient task completion by humans as objects and actions of similar types of usually in the vicinity of each other. For example, brooms, mops, and vacuums are likely to be in the closet whereas spoons, cups, and plates are likely to be in the kitchen. Moreover, we acknowledge the inherent structure of task execution due to the sequentiality of multi-step actions. While some affordances are inherent in certain object classes (i.e. \textit{Grasp} a cup, \textit{Open} a door, etc.), others have structured criteria, or preconditions, which must be met before being executed. For example, a cup must be full and near a container in order to \textit{Pour} the contents of the cup. Semantic frames, as elaborated in further sections, explicitly describe these relations and have been used in previous works to ground natural language commands in robot actions \cite{thomas2012}. Recently, the community has explored using Transformer-based models to ground natural language commands \cite{saycan} \cite{cliport} \cite{vemprala2023chatgpt}. While these models have shown impressive high-level reasoning capabilities, they often lack the physical intuition necessary to ground their output in feasible robot actions. 

Three core characteristics of semantic frames\cite{thomas2012} \cite{baker1998} \cite{ruppenhofer2016} motivate our exploration of their use as a representation to bring together language, action, and perception. First, they are evoked by a verb phrase such that we can directly parse natural language commands into semantic frames. Second, they explicitly define the objects necessary for execution. Last, they define the preconditions necessary before execution can begin and postconditions of the state after execution.  

In order to efficiently execute semantic frames, we require a model which can localize the frames location conditioned on observations of the environment.  Semantic perception of individual objects in large environments has been explored previously in the context of object search and generalized notions of object permanence \cite{zeng2020}.  We are now able to extend these ideas to consider  perception of afforded actions through inference over semantic frame representations.

In this paper, we introduce \seal (SEAL) which casts the affordance execution problem into a graphical model which accounts for object-affordance, state-affordance, and affordance-affordance relations. Additionally, we propose the Semantic Frame Mapping (SeFM) algorithm for perception of afforded actions in the context of task-level reasoning for mobile manipulation robot.  We consider SeFM as one possible algorithm for the broader SEAL problem. SeFM is a nonparametric particle-based inference method for maintaining belief over a finite set of semantic frames which represent the locations of actins afforded to the robot. We introduce and validate the SEAL model in a simulated apartment using ROS Gazebo. Next, we compare SeFM to a Transformer-based model on a multitude of household tasks using a simulated mobile manipulator and find that using SeFM leads to a higher success rate. Finally, we empower a real Fetch robot to execute tasks using SeFM. 
