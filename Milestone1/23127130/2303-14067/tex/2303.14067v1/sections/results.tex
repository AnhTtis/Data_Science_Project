\section{RESULTS}
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/gazebo_column.png}
    \caption{SeFM Inference in Gazebo Apartment Setting. Object's room-level priors (Red=Cup, Blue=Spoon) and ground truth locations (Diamond=Spoon, Star=Cup) shown in top right. Resulting particle distributions shown in A,B,C,D. A\&B validate context potential as Stir Cup density (Green) shifts from close to spoon (A) to cup (B) depending on whether robot is currently grasping a spoon. C\&D validate measurement potential as distributions converge when objects are observed.}
    \label{fig:gazebo_results}
\end{figure}
\subsection{Inference}
\begin{figure*}
    \includegraphics[width=\linewidth]{figures/iTHOR_demo.pdf}
    \caption{SeFM implemented in iTHOR simulation tasked with ``Put Vase in Safe". Top row shows topdown view of environment with Robot circled in blue, Vase location marked with teal diamond, and Safe marked with green cross. Bottom row shows the distribution at various timesteps throughout the episode. Beginning with initial, uniform distribution, particle weights are updated according to Algorithm \ref{algo:inference}.}
    \label{fig:iTHOR}
\end{figure*}
\label{sec:gazebo_inf}
We begin by studying the effectiveness of SEAL to model the location of affordances in a simulated apartment environment using ROS Gazebo. Figure \ref{fig:gazebo_results} shows our experimental setup. The shaded rectangles represent room-level priors explicitly given to the agent a priori. The agent does not know the ground truth \textit{Spoon} and \textit{Cup} locations a priori, but can observe them while exploring the environment. Particles are uniformly initialized throughout the map for each frame and object class. We choose to maintain belief over the semantic frames ``\textit{Grasp} Spoon", ``\textit{Grasp} Cup" and ``\textit{Stir} Cup". Where ``\textit{Stir} Cup" refers to the action of grasping a spoon then using that spoon to stir the contents of a cup.

To explore SEAL's effectiveness under partial observability, we keep the agent at a fixed position where none of the objects are observable. Figure \ref{fig:gazebo_results} A-B show the resulting particle distributions after 20 belief update iterations. In Figure \ref{fig:gazebo_results}A, the agent is initialized with empty grippers whereas in \ref{fig:gazebo_results}B the agent is initialized already grasping a spoon. The effect of this change is reflected in the final distribution of ``\textit{Stir} Cup". Without a spoon (\ref{fig:gazebo_results}A), we maintain belief near likely locations of a spoon. Furthermore, since we sum over all frame elements, we maintain relatively higher density in the region where both a spoon and cup are likely to be. When the agent has a spoon (\ref{fig:gazebo_results}B), the density of ``\textit{Stir} Cup" shifts to locations where cups are likely to be, since the measurement potential between spoon and ``\textit{Stir} Cup" is now 0. This result affirms that SEAL can accurately condition semantic frame locations $f_t$ on the robot-state information vector $x_t$ through the context potential $\phi_{c,\mathcal{B}(R_{ij})}(f^i_t, f^j_t, x_t)$. 

To incorporate observations from the environment, we have the agent follow a predefined trajectory through the environment. We assume the agent has a 5m observable range. Figures \ref{fig:gazebo_results} C-D display the particle distributions after the agent has finished navigation. Here we follow the same initialization routine as mentioned above (i.e. C is initialized without a spoon and D is). By incorporating observations, the final distributions converge to observed objects and show the same sensitivity to initial conditions as in \ref{fig:gazebo_results}A-B. This suggests that the measurement potential $\phi_{m,\mathcal{B}(R_{ik}|x_t)}$ is effective in the convergence of frame locations when frame elements are observed. We later incorporate this into an active search algorithm in simulated and real robots.

\subsection{Task Execution}
\label{sec:iTHOR}
Now we explore the utility these distributions are when a robot is tasked with executing a semantic frame. Experiments are conducted using a mobile manipulator in the iTHOR simulation environment \cite{ai2thor} using tasks from the ALFRED benchmark \cite{ALFRED20}. ALFRED is a public benchmark used to evaluate the ability to ground natural language commands for everyday household tasks. Tasks in ALFRED are commanded using a natural language sentence, domains are kitchens, bathrooms, and living rooms and contain actions with irreversible state changes. To apply SEAL to this, we slightly alter our inference method to now reason over robot poses that allow for interaction with an affordance rather than the affordance location itself. Additionally, in this case, we no longer use room-level priors for objects since the operating domain is single-room. When given a task, we first parse the task into a set of semantic frames. Next, the agent uses our particle-based inference method over SEAL to infer the poses at which a semantic frame can be executed, and, finally, navigates to and executes the action primitive defined in the semantic frame. Figure \ref{fig:iTHOR} shows the progression of distributions for the semantic frame ``Put Vase in Safe".

We choose a subset of 50 trials from each of the following experiment groups in ALFRED: Look at, Pick-Place, Pick-Stack-Place, and Pick-Heat-Place and refer the reader to the original work for task descriptions. We define 6 semantic frames (Pick, Place, Slice, Open, Close and Heat) grounded in singular action events that can be called in the iTHOR simulator. We compare SeFM to a method similar to SayCan \cite{saycan} in which a Large Language Model is queried to provide the next robot action conditioned on current state and commanded goal. In this experiment, we query GPT-3 using OpenAI's API client. Further, we use the same affordance scoring algorithm available in the public SayCan implementation based on object detection rather then a learned value function. 

\begin{figure}
    \label{fig:results}
    \includegraphics[width=\columnwidth]{figures/iTHOR_results.png}
    \caption{Success rate of SeFM (Blue) and SayCan (Red) across each task group. Human performance shown as green dashed line.} 
\end{figure}
Figure 4 shows the success rate of each algorithm across the 4 aforementioned task groups. Success rate here is defined as the percentage of trials which completed all the required actions in the correct order; partial completion of a task counts as a failure. We note that SeFM does significantly better than SayCan, especially in multi-step tasks. Empirically, we found that GPT-3 will often propose actions which are not yet afforded to the robot. For example, when heating an object GPT-3 will suggest ``Turn on Microwave" prior to suggesting ``Close Microwave" leading to failure. Because semantic frames explicitly encode these preconditions, SeFM does not struggle with this. Further, we found that Saycan does poorly when a required object is not immediately observable by the robot. This could be alleviated by improving the affordance scoring function, but that requires additional training data. As shown in section \ref{sec:gazebo_inf}, SeFM maintains an informed belief of affordance locations even without observing the necessary objects. This ability allows for the robot to search its environment efficiently for necessary objects, boosting performance here.

\subsection{Real Robot}
Finally, we implement SeFM on a Fetch mobile manipulator. We start by creating a 2D occupancy map of the operating environment and annotate said map with known regions (i.e. "Lab", "Hallway", "Kitchenette", etc.). Robot-state information, $x_t$, maintains knowledge of the pose of the robot in the map frame, a history of semantic frames the robot has previously executed, and the name of the object currently in the gripper. For our observations, we use a pretrained YOLOv7 \cite{wang2022yolov7} network finetuned on real world images of objects from the YCB object dataset \cite{Calli2015}. To determine navigation goals, we use a similar method to \cite{zeng2020} which fits a Bayesian Gaussian Mixture Model to the distribution and chooses a pose which allows the robot to observe the mean of the resulting Gaussian. Action policies (Pick, Place, etc.) are manually engineered using MoveIt!. We task the robot with tasks similar to those described in Section \ref{sec:iTHOR} -- excluding Pick-Heat-Place due to Fetch's inability to operate a microwave. 10 trials are conducted for each experiment group. We achieve success rates of 80\%, 60\% and 20\% for Look at, Pick-Place, and Pick-Stack-Place, respectively. We note that a majority of failures came from errors during manipulation not inference or navigation. 