\section{RELATED WORK}

\subsection{Generalizable Task Execution}
Generalized task execution has garnered much attention in the community as robotic perception and manipulation capabilities have improved. Recent works have demonstrated the ability to learn task-specific manipulation policies from RGB-D observations of the workspace \cite{shridhar2021} \cite{huang2022}. Though, in those works, they assume the environment is fully observable. Some attention has been given to operating in partially observable domains, but has met limited success due to challenges in perceiving the necessary objects for a task \cite{Inoue2023}. Other works have attempted to overcome the challenges imposed by human environments by utilizing a hybrid planning framework in which an online probabilistic semantic representation of the environment is passed to an offline task planner \cite{wang2022}, or by restricting the action space to unstructured (i.e. atomic) actions that have a uniform likelihood throughout the environment \cite{sarch2022tidee}. Recent methods have explored the use of Large Language Models (LLM) as planners \cite{saycan}, \cite{vemprala2023chatgpt}. While LLM do show promising results in reasoning over high-level goals, they struggle to ground their output in robot actions, even with appropriate prompting. Additionally, LLM cannot inherently estimate whether an action is afforded in the current scene, so in \cite{saycan} the authors train value function offline mapping RGB images of the state to executability and \cite{vemprala2023chatgpt} they restrict the output to API function names which are always actionable. Our proposed method, SEAL, addresses these challenges by formulating affordance execution as a search problem. Estimating executability is no long necessary as we can simply cross-reference the defined preconditions with the known state of the world. Partial observability is overcome through actively searching the environment for necessary frame elements. 

\subsection{Semantic Frames}
Semantic frames \cite{thomas2012},\cite{ruppenhofer2016} describe affordances, complete with actors, objects, preconditions, and results. A semantic frame is said to be evoked by a particular verb clause making them good representations of actions due to their implicit ability to generalize task description across variations in environment, object instances, and even request phrases. “Get Roger a coffee” and “Bring Roger a coffee” evoke the same semantic frame: ``Bring \{\textit{object}\} to \{\textit{recipient}\}". Formally, a semantic frame, $f$,  is defined as $f = (O, P, A)$,  where $O$, $P$, and $A$ are the sets of frame elements (objects), preconditions, and robot actions encoded in the frame, respectively. Semantic frames also have a notion of postconditions --- how the state transitions given a successful frame execution --- which can be logically sequenced to generate task plans for high level goals. Previous work \cite{thomas2012} \cite{ruppenhofer2016} has shown the ability to ground natural language commands into robot actions by parsing commands into semantic frames. In \cite{thomas2012}, $A$ consisted only of locomotion actions. In this work, we expand the set of possible actions to include manipulation and move toward performing complex tasks across large building-wide spaces. Because it is now necessary to interact with objects, a new problem of semantic frame localization is introduced. 

\subsection{Conditional Random Fields}
Conditional Random Fields (CRF) \cite{Sutton2012} \cite{lafferty2001} are a class of statistical modeling methods introduced in machine learning for sequence labelling problems. In essence, CRFs are an extension of Hidden Markov Models \cite{Rabiner1986} with the ability to incorporate complex, higher-order dependencies among the input features. CRFs are particularly useful when the outputs are correlated and depend not only on the current input but also on the context of neighboring inputs. A factor graph is a probabilistic graphical model where the nodes represent variables and edges represent the conditional dependencies between variables. In the case of CRFs, the variables are inputs and outputs, and the edges are the dependencies neighboring input features and outputs. Prior work \cite{zeng2020} has shown that casting the object search problem into a CRF factor graph can lead to performance gains in partially observable, real-world environments without the need for strong assumptions about static landmarks as in \cite{kollar2009} \cite{kunze2014} \cite{toris2017}.

