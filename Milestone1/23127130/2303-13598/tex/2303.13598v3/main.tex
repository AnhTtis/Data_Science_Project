%% Template for the submission to:
%%   The Annals of Statistics [AOS]
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% In this template, the places where you   %%
%% need to fill in your information are     %%
%% indicated by '???'.                      %%
%%                                          %%
%% Please do not use \input{...} to include %%
%% other tex files. Submit your LaTeX       %%
%% manuscript as one .tex document.         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[aos]{imsart}

%% Packages
\usepackage{amsfonts,amssymb,amsmath,amsthm,amscd,bbm,enumitem}
\usepackage[mathscr]{euscript} %use for \N for Normal distribution, closer to the original mathcal
\usepackage[round]{natbib}
\usepackage{color,graphicx,hyperref}

%\usepackage{caption,subcaption}

\usepackage{apptools}
\AtAppendix{\counterwithin{lemma}{section}}
\AtAppendix{\counterwithin{theorem}{section}}
\AtAppendix{\counterwithin{assumption}{section}}
\AtAppendix{\counterwithin{example}{section}}

\startlocaldefs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Uncomment next line to change            %%
%% the type of equation numbering           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Axiom, Claim, Corollary, Hypothesis, %%
%% Lemma, Theorem, Proposition              %%
%% use \theoremstyle{plain}                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{lemma}{Lemma}
\newtheorem{statement}{Statement}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}
\renewcommand\theassumption{\Alph{assumption}}

\newtheorem*{assumption-BW}{Assumption BW}
\newtheorem*{assumption-MD}{Assumption MD}
\newtheorem*{assumption-MR}{Assumption MR}
\newtheorem*{assumption-MRC}{Assumption MRC}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{remark}                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%% MATH OPERATORS
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\tr}{tr}
\DeclareMathOperator*{\sign}{sign}


%%%%%%%% BASIC MATH NOTATION
\newcommand{\1}{\mathbbm{1}}
\renewcommand\P{\mathbbm{P}}
\newcommand{\E}{\mathbbm{E}}
\newcommand{\V}{\mathbbm{V}}
\newcommand{\Cov}{\mathbbm{C}\mathrm{ov}}
\newcommand\R{\mathbb{R}}
\newcommand\N{\mathbb{N}}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

%%%%%%% NOTATION
\newcommand{\G}{\mathbbm{G}}
\newcommand\x{\mathsf{x}}
\newcommand\q{\mathfrak{q}}
\newcommand{\s}{\mathfrak{s}}
\newcommand{\T}{\top}
\newcommand{\GCM}{\mathsf{GCM}}
\newcommand{\LSC}{\mathsf{LSC}}
\newcommand{\lsc}{\mathsf{lsc}}
\newcommand{\conv}{\mathsf{conv}}

\begin{document}

\begin{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Bootstrap-Assisted Inference for Generalized Grenander-type Estimators} 
%\title{A sample article title with some additional note\thanksref{T1}}
\runtitle{Bootstrap-Assisted Inference for Monotone Estimators}
%\thankstext{T1}{}

\begin{aug}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Only one address is permitted per author. %%
%% Only division, organization and e-mail is %%
%% included in the address.                  %%
%% Additional information can be included in %%
%% the Acknowledgments section if necessary. %%
%% ORCID can be inserted by command:         %%
%% \orcid{0000-0000-0000-0000}               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author[A]{\fnms{Matias D.}~\snm{Cattaneo}\ead[label=e1]{cattaneo@princeton.edu}},
\author[B]{\fnms{Michael}~\snm{Jansson}\ead[label=e2]{mjansson@econ.berkeley.edu}}
\and
\author[C]{\fnms{Kenichi}~\snm{Nagasawa}\ead[label=e3]{kenichi.nagasawa@warwick.ac.uk}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Addresses                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\address[A]{Department of Operations Research and Financial Engineering,
Princeton University\printead[presep={,\ }]{e1}}

\address[B]{Department of Economics,
University of California at Berkeley\printead[presep={,\ }]{e2}}

\address[C]{Department of Economics,
University of Warwick\printead[presep={,\ }]{e3}}
\end{aug}

\begin{abstract}

\citet{Westling-Carone_2020_AoS} proposed a framework for studying the large sample distributional properties of generalized Grenander-type estimators, a versatile class of nonparametric estimators of monotone functions. The limiting distribution of those estimators is representable as the left derivative of the greatest convex minorant of a Gaussian process whose monomial mean can be of unknown order (when the degree of flatness of the function of interest is unknown). The standard nonparametric bootstrap is unable to consistently approximate the large sample distribution of the generalized Grenander-type estimators even if the monomial order of the mean is known, making statistical inference a challenging endeavour in applications. To address this inferential problem, we present a bootstrap-assisted inference procedure for generalized Grenander-type estimators. The procedure relies on a carefully crafted, yet automatic, transformation of the estimator. Moreover, our proposed method can be made ``flatness robust'' in the sense that it can be made adaptive to the (possibly unknown) degree of flatness of the function of interest. The method requires only the consistent estimation of a single scalar quantity, for which we propose an automatic procedure based on numerical derivative estimation and the generalized jackknife. Under random sampling, our inference method can be implemented using a computationally attractive exchangeable bootstrap procedure. We illustrate our methods with examples and we also provide a small simulation study. The development of formal results is made possible by some technical results that may be of independent interest.
    
\end{abstract}

\begin{keyword}[class=MSC]
\kwd[Primary ]{62G09}
\kwd{62G20}
\kwd[; secondary ]{62G07}
\kwd{62G08}
\end{keyword}

\begin{keyword}
\kwd{Monotone estimation}
\kwd{bootstrapping}
\kwd{robust inference}
\end{keyword}

\end{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please use \tableofcontents for articles %%
%% with 50 pages and more                   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Monotone function estimators have received renewed attention in statistics, biostatistics, econometrics, machine learning, and other data science disciplines. See \citet{Groeneboom-Jongbloed_2014_Book,Groeneboom-Jongbloed_2018_SS} for a textbook introduction and a review article, respectively, the latter being published in a special issue devoted to nonparametric inference under shape constraints. More recently, \citet{Westling-Carone_2020_AoS} expanded the scope and applicability of monotone function estimators by embedding many such estimators in a unified framework of generalized Grenander-type estimators. Estimation problems covered by \citeauthor{Westling-Carone_2020_AoS}'s \citeyearpar{Westling-Carone_2020_AoS} general theory include many practically relevant examples such as monotone density, regression and hazard estimation, possibly with censoring and/or covariate adjustment.

The large sample theory developed by \citet{Westling-Carone_2020_AoS} offers a general distributional approximation involving the left derivative of the Greatest Convex Minorant (GCM) of a Gaussian process whose mean and covariance kernel depend on unknown functions. Furthermore, both the convergence rate of the estimator and the shape of the mean appearing in the representation of the limiting distribution depend on whether the unknown function of interest exhibits certain degeneracies. For these reasons, the large sample distributional approximation for generalized Grenander-type estimators can be difficult to employ in practice for inference purposes. In their concluding remarks, \citet{Westling-Carone_2020_AoS} recognize these limitations and pose the question of whether it would be possible to employ bootstrap-assisted methods to conduct automatic/robust statistical inference within their framework.

As is well documented, the standard nonparametric bootstrap does not provide a valid distributional approximation for the generalized Grenander-type estimators \citep*{Kosorok_2008_BookCh,Sen-Banerjee-Woodroofe_2010_AoS}. This fact has led scholars to rely on other bootstrap schemes such as subsampling \citep{Politis-Romano_1994_AoS}, the smoothed bootstrap \citep{Kosorok_2008_BookCh,Sen-Banerjee-Woodroofe_2010_AoS}, the $m$-out-of-$n$ bootstrap \citep{Sen-Banerjee-Woodroofe_2010_AoS,Lee-Yang_2020_AoS}, or the numerical bootstrap \citep{Hong-Li_2020_AoS}. See also \citet{Cavaliere-Georgiev_2020_ECMA}, and references therein, for some related recent results. Those approaches could in principle be used to construct bootstrap-based inference methods for (some members of the class of) generalized Grenander-type estimators, but they all would require employing specific regularized multidimensional bootstrap distributions or related quantities involving multiple smoothing and tuning parameters, rendering those approaches potentially difficult to implement in practice. Furthermore, those methods would not be robust to unknown degeneracies determining the convergence rate and shape of the limiting distribution without additional modifications. For example, subsampling methods require knowledge of the precise convergence rate of the statistic, or estimation thereof, as a preliminary step \citep*{Politis-Romano-Wolf_1999_Book}. Another resampling approach was recently proposed by \citet{kuchibhotla-Balakrishnan-Wasserman_2024_JRSSB} and \citet{mallick2024new}, which offers valid confidence intervals under an approximate symmetry condition of the limiting distribution of the estimator.

We complement existing methods by introducing a novel bootstrap-assisted inference approach that restores validity of bootstrap methods by reshaping of the ingredients of the generalized Grenander-type estimator. Our approach is motivated by a constructive interpretation of the source of the bootstrap failure. As a by-product, our interpretation explicitly isolates the role of unknown degeneracies determining the precise form of the limiting distribution, a feature of the interpretation which allow us to develop an automatic inference method that is robust to such degeneracies, ultimately resulting in a more robust bootstrap-assisted inference approach. In the case of random sampling, we show that our method can be implemented using a computationally attractive exchangeable bootstrap procedure. For completeness, we also discuss implementation issues, offering a fully automatic (i.e., data-driven) valid inference method for generalized Grenander-type estimators. Some of the ideas underlying our approach are similar to ideas used in \citet*{Cattaneo-Jansson-Nagasawa_2020_ECMA}, where we introduced a bootstrap-based distributional approximation for $M$-estimators with possibly non-standard \citet{Chernoff_1964_AISM}-type asymptotic distributions \citep{Kim-Pollard_1990_AoS,Seo-Otsu_2018_AoS}. Generalized Grenander-type estimators are not $M$-estimators, however, and as further explained in the next paragraph the analysis of generalized Grenander-type estimators turns out to necessitate the development of technical tools that play no role in the analysis of $M$-estimators.

Although valid distributional approximations for monotone estimators can be obtained in a variety of ways \citep{Groeneboom-Jongbloed_2014_Book}, by far the most common approach is to employ the \textit{switch relation} \citep{Groeneboom_1985_BookCh} to re-express the cumulative distribution function (cdf) of the suitably normalized monotone estimator in terms of a probability statement about the maximizer of a certain stochastic process whose large sample properties can in turn be analyzed by employing standard empirical process methods \citep{vanderVaart-Wellner_1996_Book}. From a technical perspective, this approach requires (at least) two ingredients in order to be successful, namely results establishing (i) validity of the switch relation and (ii) continuity of the limiting cdf of the maximizer of a stochastic process. In the process of developing our main results, we shed new light on both (i) and (ii). First, we show by example that \citeauthor{Westling-Carone_2020_AoS}'s \citeyearpar[Supplement]{Westling-Carone_2020_AoS} generalization of the switch relation is incomplete as stated and then propose a modification. Our modification of \citeauthor{Westling-Carone_2020_AoS}'s \citeyearpar[Supplement]{Westling-Carone_2020_AoS} Lemma 1 shows that the conclusion of that lemma is valid once an additional assumption is made. The additional assumption seems very mild, being satisfied in all examples considered by \citet{Westling-Carone_2020_AoS} and all other examples of which we are aware, including some new examples we consider in this paper. Second, we present a lemma establishing continuity of the cdf of the maximizer of a Gaussian process, a result which can in turn be used to establish continuity of the cdf of the suitably normalized generalized Grenander-type estimator. Interestingly, although these continuity properties are important when deriving limiting distributions with the help of the switch relation and justifying bootstrap-type inference procedures, respectively, it would appear that explicit statements of them are unavailable in the existing literature. (A prominent exception is the one where the limiting distribution is a scaled Chernoff distribution, which is known to be absolutely continuous.) For further details on (i) and (ii), see Appendix \ref{[Section] Appendix - Technical Results}.

In the remainder of this introductory section we outline key notation and definitions used throughout the paper. Section \ref{[Section] Setup} then recalls the setup of \citet{Westling-Carone_2020_AoS} and presents a version of their main distributional approximation result for generalized Grenander-type estimators. Section \ref{[Section] Bootstrap-based Distributional Approximation} contains our main results about bootstrap-assisted distributional approximations, while Section \ref{[Section] Implementation} discusses implementation issues, including tuning parameter selection and a computationally attractive weighted bootstrap procedure. Section \ref{[Section] Examples} illustrates our general theory by means of prominent examples, while Section \ref{[Section] Simulations} reports numerical results from a small-scale simulation experiment. Appendix \ref{[Section] Appendix - Technical Results} reports the two technical results alluded to in the previous paragraph. All proofs and other technical details are given in the supplemental appendix \citep{Cattaneo-Jansson-Nagasawa_2024_supp}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NOTATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Notation and Definitions}

For an interval $I \subseteq \R$ and a function $f : I \to \R$, $\GCM_I(f)$ denotes its greatest convex minorant (on $I$) and if $f$ is non-decreasing and right-continuous, then $f^-$ denotes its generalized inverse; that is, $f^-(x) = \inf\{u\in I : f(u) \geq x\}$, where the dependence of $f^-$ on $I$ has been suppressed (and where the infimum of the empty set is $\sup I$). Also, if $f$ is non-decreasing and right-continuous, we write $f(x-) = \lim_{\eta \downarrow 0}f(x - \eta)$ and $f^-(y+) = \lim_{\eta \downarrow 0}f^-(y + \eta)$. Assuming the relevant derivatives exist, $\partial^q$ denotes the $q$th partial derivative (operator) and $\partial_-$ denotes the left derivative (operator). In addition, $f\circ g$ denotes the composition of $f$ and $g$; that is, $(f\circ g)(x) = f(g(x))$. Finally, for $a,b \in \R$, we write $a \land b = \min(a,b), a \lor b = \max(a,b)$, and $a^+ = \max(a,0)$, and $\lfloor a \rfloor$ denotes the integer part of $a$.

Limits are taken as $n \to \infty$, unless otherwise stated. For two (possibly) random sequences $\{a_n\}$ and $\{b_n\}$, $a_n = O_\P(b_n)$ is shorthand for $\limsup_{\epsilon \to \infty}\limsup_n\P[|a_n / b_n| \geq \epsilon] = 0$, $a_n = o_\P(b_n)$ is shorthand for $\limsup_{\epsilon \to 0}\limsup_n\P[|a_n / b_n| \geq \epsilon] = 0$, and the subscript ``$\P$'' on ``$O$'' and ``$o$'' is often omitted when $\{a_n\}$ and $\{b_n\}$ are non-random. We use $\to_\P$ to denote convergence in probability and $\rightsquigarrow$ to denote weak convergence, where, for a stochastic process indexed by $\R$, convergence is in the topology of uniform convergence on compacta. When analyzing the bootstrap, $\P^*_n$ denotes the probability measure under the bootstrap distribution conditional on the original data and $\rightsquigarrow_\P$ denotes weak convergence in probability conditionally on the original data. For more details, see \citet{vanderVaart-Wellner_1996_Book}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SETUP
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Setup}\label{[Section] Setup}

Our setup is that of \citet{Westling-Carone_2020_AoS}. The goal is to conduct inference on $\theta_0(\x)$, where, for some interval $I \subseteq \R$, $\theta_0$ is non-decreasing on $I$ and $\x$ is an interior point of $I$. Assuming it is well defined, the function $\Theta_0$ given by
\[\Theta_0(x) = \int_{\inf I}^x \theta_0(v) dv\]
is convex on $I$ and therefore enjoys the property that if $\theta_0$ is left-continuous at $\x$, then
\[\theta_0(\x) = \partial_- \GCM_I(\Theta_0)(\x).\]
An estimator of $\theta_0(\x)$ obtained by replacing $\Theta_0$ and $I$ in the preceding display with estimators is said to be of the Grenander-type, a canonical example of this class of estimators being the celebrated \citet{Grenander_1956_SAJ} estimator of a non-decreasing density.

\begin{example}[Monotone Density Estimation]\label{[Example] Monotone Density Estimation}
Suppose $X_1,\dots,X_n$ are i.i.d. copies of a continuously distributed non-negative random variable $X$ whose density $f_0$ is non-decreasing on its support $[0,u_0],$ where $u_0$ is (possibly) unknown. For $\x \in (0,u_0)$, the \citet{Grenander_1956_SAJ} estimator of $f_0(\x)$ is
\[\widehat{f}_n(\x) = \partial_- \GCM_{[0,\widehat{u}_n]}(\widehat{F}_n)(\x),\]
where $\widehat{u}_n = \max_{1 \leq i \leq n} X_i \lor \x$ and where $\widehat{F}_n(x) = n^{-1} \sum_{i=1}^n \1(X_i \leq x)$ is the empirical cdf. Section \ref{[Subsection] Examples: Monotone Density Estimation} and the supplemental appendix discuss more general monotone density estimators allowing for censoring and/or covariate adjustment \citep[e.g.,][]{vanderlaan-Robins_2003_Book}.
\end{example}

To define the class of generalized Grenander-type estimators, let $\psi_0 = \theta_0 \circ \Phi_0^-$, where $\Phi_0$ is non-negative, non-decreasing, and continuous on $I$. Defining
\[\Gamma_0 = \Psi_0 \circ \Phi_0, \qquad \Psi_0(x) = \int_0^x \psi_0(v) dv,\]
and assuming that $\Phi_0(x) < \Phi_0(\x) < u_0$ for every $x < \x$, we have
\begin{equation}\label{[Representation] theta0}
\theta_0(\x) = \partial_- \GCM_{[0,u_0]} (\Gamma_0 \circ \Phi_0^-) \circ \Phi_0(\x)
\end{equation}
whenever $\theta_0$ is left-continuous at $\x$. In the terminology of \citet{Westling-Carone_2020_AoS}, an estimator of $\theta_0(\x)$ is of the Generalized Grenander-type if it is obtained by replacing $\Gamma_0$, $\Phi_0$, and $u_0$ in the preceding display with estimators $\widehat{\Gamma}_n,\widehat{\Phi}_n$, and $\widehat{u}_n$ (say); to be specific, an estimator of the Generalized Grenander-type is of the form
\[\widehat{\theta}_n(\x) = \partial_- \GCM_{[0,\widehat{u}_n]}(\widehat{\Gamma}_n \circ \widehat{\Phi}_n^-) \circ \widehat{\Phi}_n(\x),\]
where $\widehat{\Phi}_n$ is non-negative, non-decreasing, and right-continuous. Of course, Grenander-type estimators are of the generalized Grenander-type (with $\widehat{\Phi}_n$ equal to the identity mapping) whenever the associated estimator of $I$ is of the form $[0,\widehat{u}_n]$, but the class of generalized Grenander-type estimators contains many important estimators that are not of Grenander-type, a canonical example being the celebrated isotonic regression estimator of \citet{Brunk_1958_AMS}.

\begin{example}[Monotone Regression Estimation]\label{[Example] Monotone Regression Estimation}
Suppose $(Y_1,X_1),\dots,(Y_n,X_n)$ are i.i.d. copies of $(Y,X)$, where $X$ is a continuously distributed random variable and where the regression function $\mu_0(x) = \E(Y|X = x)$ is non-decreasing. For $\x$ in the interior of the support of $X$, the \citet{Brunk_1958_AMS} estimator of $\mu_0(\x)$ is
\[\widehat{\mu}_n(\x) = \partial_-\GCM_{[0,1]}(\widehat{\Gamma}_n \circ \widehat{F}_n^-) \circ \widehat{F}_n(\x),\]
where $\widehat{\Gamma}_n(x) = n^{-1} \sum_{i=1}^n Y_i \1(X_i \leq x)$ and where $\widehat{F}_n(x) = n^{-1} \sum_{i=1}^n \1(X_i \leq x)$ is an estimator of $F_0$, the cdf of $X$. Section \ref{[Subsection] Examples: Monotone Regression Estimation} discusses more general monotone regression estimators allowing for covariate adjustment \citep*[e.g.,][]{Westling-Gilbert-Carone_2020_JRRSB}.
\end{example}

Under regularity conditions, if $\widehat{\theta}_n(\x)$ is of the generalized Grenander-type, then its rate of convergence is governed by the flatness of $\theta_0$ around $\x$, as measured by the characteristic exponent
\[\q = \min\{j \in \N: \partial^j \theta_0(\x) \neq 0\},\]
where $\N$ is the set of positive integers. (When $\theta_0$ is non-decreasing and suitably smooth, $\q$ is necessarily an odd integer.) To be specific, \citet[Theorem 3]{Westling-Carone_2020_AoS} gave conditions under which
\begin{equation}\label{[Asymptotics] thetahat}
r_n(\widehat{\theta}_n(\x) - \theta_0(\x)) \rightsquigarrow \frac{1}{\partial\Phi_0(\x)} \partial_- \GCM_\R(\mathcal{G}_\x + \mathcal{M}_\x^\q)(0), \qquad r_n = n^{\q / (1 + 2 \q)},
\end{equation}
where $\mathcal{G}_\x$ is a scalar multiple of two-sided Brownian motion and where $\mathcal{M}_\x^\q$ is a monomial given by
\begin{equation}\label{[Definition] M_x,q}
\mathcal{M}_\x^\q(v) = \frac{\partial^\q \theta_0(\x) \partial \Phi_0(\x)}{(\q + 1)!} v^{\q + 1}.
\end{equation}
In addition to governing the rate of convergence, the characteristic exponent $\q$ also governs the shape of $\mathcal{M}_\x^\q$. On the other hand, and as the notation suggests, the covariance kernel of $\mathcal{G}_\x$ does not depend on $\q$. If $\q = 1$, then the distribution of $\partial_- \GCM_\R(\mathcal{G}_\x + \mathcal{M}_\x^\q)(0)$ is a scaled Chernoff distribution. More generally, the distribution of $\partial_- \GCM_\R(\mathcal{G}_\x + \mathcal{M}_\x^\q)(0)$ is a scaled Chernoff-type distribution \citep[in the terminology of][]{Han-Kato_2022_AoAP}.

Among other things, the following assumption guarantees validity of the representation (\ref{[Representation] theta0}) and ensures existence of the right hand side of (\ref{[Definition] M_x,q}).

\begin{assumption}\label{Assumption A}
For some $\delta > 0$ and some $\s \geq \q$, the following are satisfied:
\begin{enumerate}[label=(A\arabic*)]
\item $I \subseteq \R$ is an interval and $I_\x^\delta = \{x \in \R : |x - \x| \leq \delta\} \subseteq I$.

\item \label{Assumption A: theta0} $\theta_0$ is non-decreasing and bounded on $I$, and $\left\lfloor \s \right\rfloor$ times continuously differentiable on $I_\x^\delta$ with
\[\sup_{x,x' \in I_\x^\delta}\frac{|\partial^{\left\lfloor \s \right\rfloor} \theta_0(x) - \partial^{\left\lfloor \s \right\rfloor} \theta_0(x')|}{|x - x'|^{\s - \left\lfloor \s \right\rfloor}} < \infty.\]

\item \label{Assumption A: Phi0}
 $\Phi_0$ is non-negative, non-decreasing, continuous, and bounded on $I$, and $\left\lfloor \s \right\rfloor - \q + 1$ times continuously differentiable on $I_\x^\delta$ with $\partial\Phi_0(\x) \neq 0$ and
\[\sup_{x,x' \in I_\x^\delta} \frac{|\partial^{\left\lfloor \s \right\rfloor - \q + 1} \Phi_0(x) - \partial^{\left\lfloor \s \right\rfloor - \q + 1}\Phi_0(x')|}{|x - x'|^{\s - \left\lfloor \s \right\rfloor}} < \infty.\]

\end{enumerate}
\end{assumption}

\section{Bootstrap-Assisted Distributional Approximation}\label{[Section] Bootstrap-based Distributional Approximation}

The distributional approximation \eqref{[Asymptotics] thetahat} motivates a plug-in procedure for statistical inference: with consistent estimators of  $\mathcal{M}_\x^\q$ and the covariance kernel of $\mathcal{G}_\x$, the limiting law in \eqref{[Asymptotics] thetahat} can be simulated. This approach requires characterizing the covariance kernel explicitly, and then forming a consistent estimator based on preliminary non-parametric smoothing methods (in addition to the other preliminary nonparametric estimators needed). Also, simulating from the limiting law requires specifying or estimating the unknown characteristic exponent $\q$. As a consequence, although conceptually feasible, the plug-in approach is often computationally cumbersome to employ in practice.

Since resampling methods have the potential to provide automatic and robust distributional approximations, it seems natural to consider such methods as an alternative to plug-in methods. Letting $(\widehat{\Gamma}_n^*,\widehat{\Phi}_n^*,\widehat{u}_n^*)$ denote a generic (not necessarily nonparametric) bootstrap analog of $(\widehat{\Gamma}_n,\widehat{\Phi}_n,\widehat{u}_n)$ and assuming that $\widehat{\Phi}_n^*$ is non-decreasing and right-continuous, the associated bootstrap analog of $\widehat{\theta}_n(\x)$ is
\[\widehat{\theta}_n^*(\x) = \partial_- \GCM_{[0,\widehat{u}_n^*]}(\widehat{\Gamma}_n^* \circ \widehat{\Phi}_n^{*-}) \circ \widehat{\Phi}_n^*(\x).\]

As is well documented for the classical Grenander estimator \citep[e.g.][]{Kosorok_2008_BookCh,Sen-Banerjee-Woodroofe_2010_AoS}, the bootstrap analog of (\ref{[Asymptotics] thetahat}) does not (necessarily) hold when $(\widehat{\Gamma}_n^*,\widehat{\Phi}_n^*,\widehat{u}_n^*)$ is obtained by means of the nonparametric bootstrap. In other words,
\[r_n (\widehat{\theta}_n^*(\x) - \widehat{\theta}_n(\x)) \not \rightsquigarrow_\P \frac{1}{\partial\Phi_0(\x)} \partial_- \GCM_\R(\mathcal{G}_\x + \mathcal{M}_\x^\q)(0)\]
in general; that is, the bootstrap is inconsistent in general. (A precise statement is provided in Theorem \ref{[Theorem]: Bootstrap inconsistency} in Appendix \ref{[Subsection] Appendix - Bootstrap Inconsistency}.) It turns out, however, that under plausible conditions on $(\widehat{\Gamma}_n^*,\widehat{\Phi}_n^*,\widehat{u}_n^*)$, a valid bootstrap-based distributional approximation can be obtained by employing
\[\widetilde{\theta}_n^*(\x) = \partial_- \GCM_{[0,\widehat{u}_n^*]}(\widetilde{\Gamma}_n^* \circ \widehat{\Phi}_n^{*-}) \circ \widehat{\Phi}_n^*(\x),\]
where, for some judiciously chosen $\widetilde{M}_{\x,n}$,
\[\widetilde{\Gamma}_n^*(x) = \widehat{\Gamma}_n^*(x) - \widehat{\Gamma}_n(x) + \widehat{\theta}_n(\x) \widehat{\Phi}_n(x) + \widetilde{M}_{\x,n}(x - \x).\]
(As the notation suggests, and as further discussed below, a defining property of $\widetilde{M}_{\x,n}$ is that a suitably re-scaled version of it can be interpreted as an estimator of $\mathcal{M}_\x^\q$.)

\subsection{Heuristics}\label{[Subsection] Heuristics}

To explain the source of the bootstrap failure and motivate the functional form of $\widetilde{\Gamma}_n^*$, it is useful to begin by sketching the derivation of (\ref{[Asymptotics] thetahat}). For any $t \in \R$ and under mild conditions on $\widehat{\Gamma}_n$ and $\widehat{\Phi}_n$, it follows from the so-called (generalized) switch relation (e.g., Appendix \ref{[Subsection] Appendix - Generalized Switch Relation}) that
\[\P \left[r_n (\widehat{\theta}_n(\x) - \theta_0(\x)) \leq t\right] = \P \left[\argmin_{v \in \widehat{V}_{\x,n}^\q} \{\widehat{G}_{\x,n}^\q(v) + M_{\x,n}^\q(v) - t \widehat{L}_{\x,n}^\q(v)\} \geq \widehat{Z}_{\x,n}^\q \right],\]
where $\widehat{V}_{\x,n}^\q$ is a ``large'' subset of $\R$, $\widehat{G}_{\x,n}^\q$ is a stochastic process given by

\begin{align*}
\widehat{G}_{\x,n}^\q(v) &= \sqrt{n a_n} [\widehat{\Gamma}_n(\x + v a_n^{-1}) - \widehat{\Gamma}_n(\x) - \Gamma_0(\x + v a_n^{-1}) + \Gamma_0(\x)]\\
&- \theta_0(\x) \sqrt{n a_n} [\widehat{\Phi}_n(\x + v a_n^{-1}) - \widehat{\Phi}_n(\x) - \Phi_0(\x + v a_n^{-1}) + \Phi_0(\x)], \quad a_n = n^{1 / (1 + 2 \q)},
\end{align*}
$M_{\x,n}^\q$ is a smooth non-random function, $\widehat{L}_{\x,n}^\q$ can be interpreted as an estimator of the linear function $\mathcal{L}_\x$ given by $\mathcal{L}_\x(v) = v \partial \Phi_0(\x)$,
and where $\widehat{Z}_{\x,n}^\q$ is zero when $\widehat{\Phi}_n$ is the identity mapping and can be interpreted as an ``estimator'' of zero more generally. (The exact definitions of $\widehat{V}_{\x,n}^\q,M_{\x,n}^\q,\widehat{L}_{\x,n}^\q$, and $\widehat{Z}_{\x,n}^\q$ are given in Appendix \ref{[Subsection] Appendix - Omitted Formulas}.) In the above and the sequel, the superscript $\q$ indicates that the superscripted object is defined on a ``localized'' domain. 

Under mild conditions, $\widehat{G}_{\x,n}^\q$ converges weakly to a scalar multiple of two-sided Brownian motion $\mathcal{G}_\x$ (say), while $M_{\x,n}^\q,\widehat{L}_{\x,n}^\q$, and $\widehat{Z}_{\x,n}^\q$ converge in probability to $\mathcal{M}_\x^\q,\mathcal{L}_\x$, and zero, respectively. In other words, we would expect that
\begin{equation}\label{[Asymptotics] Ghat,Lhat,Zhat}        
(\widehat{G}_{\x,n}^\q,\widehat{L}_{\x,n}^\q,\widehat{Z}_{\x,n}^\q) \rightsquigarrow (\mathcal{G}_\x,\mathcal{L}_\x,0),
\end{equation}
and that
\begin{equation}\label{[Asymptotics] M}
M_{\x,n}^\q \rightsquigarrow \mathcal{M}_\x^\q.
\end{equation}
It therefore stands to reason that under mild additional conditions (including suitable convergence of $\widehat{V}_{\x,n}^\q$ to $\R$), we have
\begin{align*}
\lim_{n \to \infty} \P \left[r_n (\widehat{\theta}_n(\x) - \theta_0(\x)) \leq t\right] &= \P \left[\argmin_{v \in \R}\{\mathcal{G}_\x(v) + \mathcal{M}_\x^\q(v) - t \mathcal{L}_\x(v)\} \geq 0 \right]\\
&= \P \left[\frac{1}{\partial \Phi_0(\x)} \partial_- \GCM_\R(\mathcal{G}_\x + \mathcal{M}_\x^\q)(0) \leq t\right],
\end{align*}
where the first equality follows from an $\argmax$ continuous mapping theorem, and the second equality is obtained by another application of the switch relation.

Next, consider the bootstrap analog of $\widehat{\theta}_n(\x)$. For any $t \in \R$, the switch relation gives
\[\P_n^*\left[r_n (\widehat{\theta}_n^*(\x) - \widehat{\theta}_n(\x)) \leq t\right] = \P_n^*\left[\argmin_{v \in \widehat{V}_{\x,n}^{\q,*}}\{\widehat{G}_{\x,n}^{\q,*}(v) + \widehat{M}_{\x,n}^\q(v) - t \widehat{L}_{\x,n}^{\q,*}(v)\} \geq \widehat{Z}_{\x,n}^{\q,*} \right],\]
where
\begin{align*}
\widehat{G}_{\x,n}^{\q,*}(v) &= \sqrt{n a_n} [\widehat{\Gamma}_n^*(\x + v a_n^{-1}) - \widehat{\Gamma}_n^*(\x) - \widehat{\Gamma}_n(\x + v a_n^{-1}) + \widehat{\Gamma}_n(\x)] \\
&-\widehat{\theta}_n(\x) \sqrt{n a_n} [\widehat{\Phi}_n^*(\x + v a_n^{-1}) - \widehat{\Phi}_n^*(\x) -\widehat{\Phi}_n(\x + v a_n^{-1}) + \widehat{\Phi}_n(\x)],
\end{align*}
and where $\widehat{V}_{\x,n}^{\q,*},\widehat{M}_{\x,n}^\q,\widehat{L}_{\x,n}^{\q,*}$, and $\widehat{Z}_{\x,n}^{\q,*}$ are bootstrap analogs of $\widehat{V}_{\x,n}^\q,M_{\x,n}^\q,\widehat{L}_{\x,n}^\q$, and $\widehat{Z}_{\x,n}^\q$, respectively. (The exact definitions of $\widehat{V}_{\x,n}^{\q,*},\widehat{M}_{\x,n}^\q,\widehat{L}_{\x,n}^{\q,*}$, and $\widehat{Z}_{\x,n}^{\q,*}$ are given in Appendix \ref{[Subsection] Appendix - Omitted Formulas}.) If (\ref{[Asymptotics] Ghat,Lhat,Zhat}) holds, then the following bootstrap counterpart thereof can also be expected to hold:
\begin{equation}\label{[Asymptotics] Ghatstar,Lhatstar,Zhatstar}
(\widehat{G}_{\x,n}^{\q,*},\widehat{L}_{\x,n}^{\q,*},\widehat{Z}_{\x,n}^{\q,*}) \rightsquigarrow_\P (\mathcal{G}_\x,\mathcal{L}_\x,0).
\end{equation}
On the other hand, unlike $M_{\x,n}^\q$ the process $\widehat{M}_{\x,n}^\q$ is (random and) non-smooth, so the bootstrap counterpart of (\ref{[Asymptotics] M}) typically fails, implying in turn that the bootstrap is inconsistent; for details, see Theorem \ref{[Theorem]: Bootstrap inconsistency} (stated in Appendix \ref{[Subsection] Appendix - Bootstrap Inconsistency}) and its proof.

In other words, the sole source of the bootstrap inconsistency is the failure of $\widehat{M}_{\x,n}^\q$ to replicate the salient properties of $M_{\x,n}^\q$. Recognizing this, our proposed estimator $\widetilde{\theta}_n^*(\x)$ has been carefully constructed so that it differs from $\widehat{\theta}_n^*(\x)$ only in terms of the implied estimator of $\mathcal{M}_\x^\q$. To be specific, $\widetilde{\theta}_n^*(\x)$ is similar to $\widehat{\theta}_n^*(\x)$ insofar as it satisfies the following switch relation: For any $t\in\R$,
\[\P_n^* \left[r_n (\widetilde{\theta}_n^*(\x) -\widehat{\theta}_n(\x)) \leq t \right] = \P_n^* \left[\argmin_{v \in \widehat{V}_{\x,n}^{\q,*}} \{\widehat{G}_{\x,n}^{\q,*}(v) + \widetilde{M}_{\x,n}^\q(v) - t \widehat{L}_{\x,n}^{\q,*}(v)\} \geq \widehat{Z}_{\x,n}^{\q,*}\right],\]
where $\widetilde{M}_{\x,n}^\q$ is the following transformation of $\widetilde{M}_{\x,n}$:
\[\widetilde{M}_{\x,n}^\q(v) = \sqrt{n a_n} \widetilde{M}_{\x,n}(v a_n^{-1}).\]
As a consequence, if (\ref{[Asymptotics] Ghatstar,Lhatstar,Zhatstar}) holds and if $\widetilde{M}_{\x,n}^\q \rightsquigarrow_\P \mathcal{M}_\x^\q$, then it stands to reason that under mild additional conditions $\widetilde{\theta}_n^*(\x)$ satisfies the following bootstrap counterpart of (\ref{[Asymptotics] thetahat}):
\begin{equation}\label{[Asymptotics] thetatildestar}
r_n (\widetilde{\theta}_n^*(\x) - \widehat{\theta}_n(\x)) \rightsquigarrow_\P \frac{1}{\partial\Phi_0(\x)} \partial_- \GCM_\R (\mathcal{G}_\x + \mathcal{M}_\x^\q)(0).
\end{equation}

In words, our proposed bootstrap-assisted inference procedure can be explained as follows: we employ the bootstrap to approximate the ``fluctuations'' of the process $\mathcal{G}_\x$, while we use the estimator $\widetilde{M}_{\x,n}^\q$ to approximate $\mathcal{M}_\x^\q$. It is an interesting question whether our approach offers provably better distributional approximations than the plug-in procedure mentioned previously, but it is beyond the scope of this paper to answer that question.

\subsection{Main result}\label{[Subsection] Main result}

Our heuristic derivation of (\ref{[Asymptotics] thetahat}) can be made rigorous by providing conditions under which four properties hold. First, the switch relation(s) must be valid. Second, the convergence properties (\ref{[Asymptotics] Ghat,Lhat,Zhat}) and (\ref{[Asymptotics] M}) must hold. Third, to use (\ref{[Asymptotics] Ghat,Lhat,Zhat}) and (\ref{[Asymptotics] M}) to obtain the result
\[\argmin_{v \in \widehat{V}_{\x,n}^\q} \{\widehat{G}_{\x,n}^\q(v) + M_{\x,n}^\q(v) - t \widehat{L}_{\x,n}^\q(v)\} \rightsquigarrow \argmin_{v \in \R} \{\mathcal{G}_\x(v) + \mathcal{M}_\x^\q(v) - t \mathcal{L}_\x(v)\}\]
with the help of a suitable $\argmax$ continuous mapping theorem, tightness of the left hand side in the previous display must hold. Finally, to furthermore obtain the conclusion
\begin{align*}
&\lim_{n \to \infty} \P \left[\argmin_{v \in \widehat{V}_{\x,n}^\q} \{\widehat{G}_{\x,n}^\q(v) + M_{\x,n}^\q(v) - t \widehat{L}_{\x,n}^\q(v)\} \geq \widehat{Z}_{\x,n}^\q \right] \\
&= \P \left[\argmin_{v \in \R}\{\mathcal{G}_\x(v) + \mathcal{M}_\x^\q(v) - t \mathcal{L}_\x(v)\} \geq 0 \right],
\end{align*}
the cdf of $\argmin_{v \in \R}\{\mathcal{G}_\x(v) + \mathcal{M}_\x^\q(v) - t \mathcal{L}_\x(v)\}$ must be continuous at zero.

Conditions under which the second and third properties hold can be formulated with the help of well known empirical process results. We base our formulations on \citet{vanderVaart-Wellner_1996_Book} and employ the generalized $\argmax$ continuous mapping theorem of \citet{Cox_2022}, which applies also to settings where the feasibility set varies with $n$. The first and fourth properties, on the other hand, seem more difficult to verify. Regarding the first property, it turns out that the generalization of the switch relation employed by \citet{Westling-Carone_2020_AoS} requires additional conditions in order to be valid. To address this concern, we present a modified generalized switch relation whose assumptions include a condition not present in Lemma 1 of \citet[Supplement]{Westling-Carone_2020_AoS}. Thankfully, the condition in question seems very mild and having imposed it we are able to preserve the main implication of Lemma 1 of \citet[Supplement]{Westling-Carone_2020_AoS}; for details, see Lemma \ref{[Lemma] Generalized Switch Relation} in Appendix \ref{[Subsection] Appendix - Generalized Switch Relation}. In the special case where $\q = 1$, the fourth property follows from well known properties of the Chernoff distribution. More generally, however, we are unaware of existing results guaranteeing the requisite continuity property when $\q\neq1$, but fortunately it turns out that the continuity property of interest can be established; for details, see Lemma \ref{[Lemma] Continuity of argmax} in Appendix \ref{[Subsection] Appendix - Continuity of Limiting Distribution}.

The following assumption collects the conditions under which our verification of the four above-mentioned properties will proceed.

\begin{assumption}\label{Assumption B}
For some $\delta>0$, the following are satisfied:
\begin{enumerate}[label=(B\arabic*)]

\item \label{Assumption B: weak convergence}
$\widehat{G}_{\x,n}^\q \rightsquigarrow \mathcal{G}_\x$ and $\widehat{G}_{\x,n}^{\q,*} \rightsquigarrow_\P \mathcal{G}_\x$, where $\mathcal{G}_\x$ is a centered Gaussian process whose covariance function is $\mathcal{C}_\x(s,t) = C(|s| \land |t|) \1(\sign(s)=\sign(t))$ for some $C>0$. 
    
\item \label{Assumption B: moment bound} 
There exist $\beta < \q+1$ and events $\mathcal{A}_n$ with $\lim_{n \to \infty}\P [\mathcal{A}_n] = 1$,
\[\sup_{V \in [1,a_n \delta]} \E \left[V^{-\beta} \sup_{|v| \in [V,2V]} |\widehat{G}_{\x,n}^\q(v)|\1_{\mathcal{A}_n}\right] = O(1)\]
and
\[\sup_{V \in [1,a_n \delta]} \E \left[V^{-\beta} \sup_{|v| \in [V,2V]} |\widehat{G}_{\x,n}^{\q,*}(v)|\1_{\mathcal{A}_n}\right] = O(1).\]

\item \label{Assumption B: uniform consistency Gamma}
$\sup_{x \in I}|\widehat{\Gamma}_n(x) - \Gamma_0(x)| = o_\P(1)$ and $\sup_{x \in I}|\widehat{\Gamma}_n^*(x) - \widehat{\Gamma}_n(x)| = o_\P(1)$.

\item \label{Assumption B: uniform convergence Phi}
$\sup_{x \in I} |\widehat{\Phi}_n(x) - \Phi_0(x)| = o_\P(1)$ and $\sup_{x \in I} |\widehat{\Phi}_n^*(x) - \widehat{\Phi}_n(x)| = o_\P(1).$
In addition, 
\[a_n \sup_{x \in I_\x^\delta} |\widehat{\Phi}_n(x) - \Phi_0(x)| = o_\P(1) \quad \text{and} \quad a_n \sup_{x \in I_\x^\delta} |\widehat{\Phi}_n^*(x) - \widehat{\Phi}_n(x)| = o_\P(1).\]

\item \label{Assumption B: uhat}
For some $u_0 > \Phi_0(\x)$, $\widehat{u}_n \geq u_0 + o_\P(1)$ and $\widehat{u}_n^* \geq \widehat{u}_n + o_\P(1)$.

\item \label{Assumption B: Phi uhat}
$\widehat{\Phi}_n$ and $\widehat{\Phi}_n^*$ are non-negative, non-decreasing, and right-continuous on $I$. In addition, $\{0,\widehat{u}_n\} \subseteq \widehat{\Phi}_n(I)$ and $\{0,\widehat{u}_n^*\} \subseteq \widehat{\Phi}_n^*(I)$. Also, $\widehat{\Phi}_n(I) \cap [0,\widehat{u}_n]$ and $\widehat{\Phi}_n^*(I) \cap [0,\widehat{u}_n^*]$ are closed. 

\item \label{Assumption B: jump of Phi hat}
\[\sqrt{n a_n} \sup_{x \in I_\x^\delta} |\widehat{\Phi}_n (x) - \widehat{\Phi}_n (x-)| = o_\P(1) \quad \text{and} \quad \sqrt{n a_n} \sup_{x \in I_\x^\delta} |\widehat{\Phi}_n^* (x) - \widehat{\Phi}_n^* (x-)| = o_\P(1).\]

\end{enumerate}    
\end{assumption}

Verification of the bootstrap parts of Assumption \ref{Assumption B} will be discussed in Section \ref{[Subsection] Bootstrapping} below. When combined with Assumption \ref{Assumption A}, Assumption \ref{Assumption B} suffices in order to establish (\ref{[Asymptotics] thetahat}) and (\ref{[Asymptotics] Ghatstar,Lhatstar,Zhatstar}). In addition, (\ref{[Asymptotics] thetatildestar}) can be shown to hold if $\widetilde{M}_{\x,n}$ satisfies the following

\begin{assumption}\label{Assumption C}
$\widetilde{M}_{\x,n}^\q \rightsquigarrow_\P \mathcal{M}_\x^\q$ and, for some $c>0$ and every $K>0$,
\[\liminf_{n \to \infty} \P \left[\inf_{|v| > K^{-1}} \widetilde{M}_{\x,n}(v) \geq c K^{-(\q + 1)} \right] = 1.\]
\end{assumption}

Moreover, continuity of the cdf of $\partial_- \GCM_\R (\mathcal{G}_\x + \mathcal{M}_\x^\q)(0)$ can be shown with the help of the arguments used to show that the cdf of $\argmin_{v \in \R} \{\mathcal{G}_\x(v) + \mathcal{M}_\x^\q(v) - t \mathcal{L}_\x(v)\}$ is continuous at zero. As a consequence, we obtain the following result.

\begin{theorem}\label{[Theorem] Main result}
Suppose Assumptions \ref{Assumption A}, \ref{Assumption B}, and \ref{Assumption C} are satisfied. Then (\ref{[Asymptotics] thetahat}) and (\ref{[Asymptotics] thetatildestar}) hold, and \begin{equation}\label{Bootstrap consistency}
\sup_{t \in \R} \left|\P_n^* \left[\widetilde{\theta}_n^*(\x) - \widehat{\theta}_n(\x) \leq t \right] - \P \left[\widehat{\theta}_n(\x) - \theta_0(\x) \leq t \right]\right| = o_\P(1).
\end{equation}
\end{theorem}

In an attempt to emphasize the rate-adaptive nature of the consistency property enjoyed by the bootstrap-based distributional approximation based on $\widetilde{\theta}_n^*(\x)$, the formulation (\ref{Bootstrap consistency}) deliberately omits the rate term $r_n$ present in (\ref{[Asymptotics] thetahat}) and (\ref{[Asymptotics] thetatildestar}). Theorem \ref{[Theorem] Main result} has immediate implications for inference. For instance, it follows from (\ref{[Asymptotics] thetahat}) and (\ref{Bootstrap consistency}) that for any $\alpha \in (0,1)$, we have
\[\lim_{n \to \infty} \P[\theta_0(\x) \in \mathsf{CI}_{1 - \alpha,n}(\x)] = 1 - \alpha,\]
where, defining $Q_{a,n}^*(\x) = \inf\{Q \in \R : \P_n^* [\widetilde{\theta}_n^*(\x) - \widehat{\theta}_n(\x) \leq Q] \geq a\}$,
\[\mathsf{CI}_{1 - \alpha,n}(\x) = \left[\widehat{\theta}_n(\x) - Q_{1-\alpha/2,n}^*(\x)~,~\widehat{\theta}_n(\x) - Q_{\alpha/2,n}^*(\x)\right]\]
is the (nominal) level $1 - \alpha$ bootstrap confidence interval for $\theta_0$ based on the ``percentile method'' \citep[in the terminology of][]{vanderVaart_1998_Book}.

\section{Implementation}\label{[Section] Implementation}

Suppose Assumption \ref{Assumption A} is satisfied and suppose $(\widehat{\Gamma}_n,\widehat{\Phi}_n,\widehat{u}_n)$ satisfies the non-bootstrap parts of Assumption \ref{Assumption B}. Then, in order to compute the estimator $\widetilde{\theta}_n^*(\x)$ upon which our proposed bootstrap-assisted distributional approximation is based, two implementation issues must be addressed, namely the choice/construction of $\widetilde{M}_{\x,n}$ and $(\widehat{\Gamma}_n^*,\widehat{\Phi}_n^*,\widehat{u}_n^*)$, respectively. Section \ref{[Subsection] Mean function estimation} demonstrates the plausibility of Assumption \ref{Assumption C} by exhibiting estimators $\widetilde{M}_{\x,n}$ satisfying it under Assumptions \ref{Assumption A} and \ref{Assumption D}, the latter being a high-level condition that typically holds whenever the non-bootstrap parts of Assumption \ref{Assumption B} hold. Then, Section \ref{[Subsection] Bootstrapping} exhibits easy-to-compute $(\widehat{\Gamma}_n^*,\widehat{\Phi}_n^*,\widehat{u}_n^*)$ satisfying the bootstrap parts of Assumption \ref{Assumption B} under a random sampling assumption and other mild conditions.

\subsection{Mean Function Estimation}\label{[Subsection] Mean function estimation}

The ease with which an $\widetilde{M}_{\x,n}$ satisfying Assumption \ref{Assumption C} can be constructed depends in particular on whether $\q$ is known. To facilitate the subsequent discussion, let
$\mathcal{D}_0(\x) = 0$ and for $j = 1,\dots,\lfloor \s \rfloor$, define (recursively)
\[\mathcal{D}_{j}(\x) = \lim_{x \to \x} \frac{\Upsilon_0(x) - \Upsilon_0(\x) - \sum_{k = 0}^{j-1} [(k + 1)!]^{-1} \mathcal{D}_k(\x) (x - \x)^{k + 1}}{(x - \x)^{j + 1}}, \quad \Upsilon_0 = \Gamma_0 - \theta_0(\x) \Phi_0.\]
Noting that $\partial \Gamma_0(x) = \theta_0(x) \partial \Phi_0(x)$ for $x$ near $\x$, we have $\mathcal{D}_0(\x) = \dots = \mathcal{D}_{\q - 1}(\x) = 0$ and
\[\mathcal{D}_j(\x) = \frac{1}{(j + 1)!} \sum_{k = \q}^{j}\binom{j}{k} \partial^{k} \theta_0(\x) \partial^{j + 1 - k} \Phi_0(\x), \qquad j = \q,\dots,\left\lfloor \s \right\rfloor.\]
In particular, $\mathcal{D}_\q(\x) = \partial^\q \theta_0(\x) \partial \Phi_0(\x) / (\q + 1)!$ and therefore $\mathcal{M}_\x^\q(v) = \mathcal{D}_\q(\x)v^{\q + 1}$.

\subsubsection{Mean Function Estimation with Known \texorpdfstring{$\q$}{TEXT}}
First, consider the (simpler) case where $\q$ is known. In this case, if
\begin{equation}\label{Consistency condition}
\mathcal{\widetilde{D}}_{\q,n}(\x) \to_\P \mathcal{D}_\q(\x),
\end{equation}
then Assumption \ref{Assumption C} holds when
\begin{equation}\label{eq: simple mean function estimator}
\widetilde{M}_{\x,n}(x) = \mathcal{\widetilde{D}}_{\q,n}(\x) x^{\q + 1}.
\end{equation}

Example-specific estimators $\mathcal{\widetilde{D}}_{\q,n}(\x)$ satisfying the consistency requirement (\ref{Consistency condition}) may be readily available. For instance, in the case of the \citet{Grenander_1956_SAJ} estimator described in Example \ref{[Example] Monotone Density Estimation}, we have $\mathcal{D}_\q(\x) = \partial^\q f_0(\x) / (\q + 1)!$, a consistent estimator of which can be based on any consistent estimator of $\partial^\q f_0(\x)$, such as a standard kernel estimator or, if the evaluation point $\x$ is near the boundary of the support of $X$, boundary adaptive versions thereof.

More generic estimators are also available. For specificity, the remainder of this section focuses on estimators of $\mathcal{D}_{j}(\x)$ obtained by applying numerical derivative-type operators to the following (possibly) non-smooth estimator of $\Upsilon_0$:
\[\widehat{\Upsilon}_n = \widehat{\Gamma}_n - \widehat{\theta}_n(\x) \widehat{\Phi}_n.\]
The fact that $\mathcal{D}_j(\x) = 0$ for $j < \q$ implies that $\mathcal{D}_\q(\x)$ admits the ``monomial approximation'' representation
\[\mathcal{D}_\q(\x) = \lim_{\epsilon \to 0} \left\{\epsilon^{-(\q + 1)}[\Upsilon_0(\x + \epsilon) - \Upsilon_0(\x)]\right\},\]
motivating the estimator
\[\mathcal{\widetilde{D}}_{\q,n}^{\mathtt{MA}}(\x) = \epsilon_n^{-(\q + 1)}[\widehat{\Upsilon}_n(\x + \epsilon_n) - \widehat{\Upsilon}_n(\x)],\]
where $\epsilon_n > 0$ is a tuning parameter. Similarly, the generic ``forward difference'' representation
\[\mathcal{D}_\q(\x) = \lim_{\epsilon \to 0} \left\{\epsilon^{-(\q + 1)} \sum_{k = 1}^{\q + 1}(-1)^{k + \q + 1}\binom{\q + 1}{k}[\Upsilon_0(\x + k \epsilon) - \Upsilon_0(\x)]\right\}\]
motivates the estimator
\[\mathcal{\widetilde{D}}_{\q,n}^{\mathtt{FD}}(\x) = \epsilon_n^{-(\q + 1)} \sum_{k = 1}^{\q + 1} (-1)^{k + \q + 1}\binom{\q + 1}{k}[\widehat{\Upsilon}_n(\x + k \epsilon_n) - \widehat{\Upsilon}_n(\x)].\]

For $\eta > 0$, let
\begin{align*}
\widehat{G}_{\x,n}(v;\eta)  &= \sqrt{n \eta^{-1}} [\widehat{\Gamma}_n(\x + v \eta) - \widehat{\Gamma}_n(\x) - \Gamma_0(\x + v \eta) + \Gamma_0(\x)]\\
&- \theta_0(\x) \sqrt{n \eta^{-1}} [\widehat{\Phi}_n(\x + v \eta) - \widehat{\Phi}_n(\x) - \Phi_0(\x + v \eta) + \Phi_0(\x)],
\end{align*}
and
\[\widehat{R}_{\x,n}(v;\eta) = \eta^{-1}[\widehat{\Phi}_n(\x + v\eta) - \widehat{\Phi}_n(\x) - \Phi_0(\x + v \eta ) +\Phi_0(\x)].\]
Using this notation, the first part of \ref{Assumption B: weak convergence} can be restated as
\[\widehat{G}_{\x,n}(\cdot;a_n^{-1}) \rightsquigarrow \mathcal{G}_\x\]
and the second displayed condition of \ref{Assumption B: uniform convergence Phi} implies that
\[\widehat{R}_{\x,n}(\cdot;a_n^{-1}) = o_\P(1).\]
In the preceding displays, one can typically replace $a_n^{-1} = n^{-1 / (1 + 2 \q)}$ by any $\eta_n > 0$ with $\eta_n = o(1)$ and $a_n^{-1} \eta_n^{-1} = O(1)$. As a consequence, validity of the following assumption usually follows as a by-product of the arguments used to justify \ref{Assumption B: weak convergence} and \ref{Assumption B: uniform convergence Phi}.

\begin{assumption}\label{Assumption D}
For every $\eta_n > 0$ with $\eta_n = o(1)$ and $a_n^{-1} \eta_n^{-1} = O(1)$,
\[\widehat{G}_{\x,n}(1;\eta_n) = O_\P(1) \qquad \text{and} \qquad \widehat{R}_{\x,n}(1;\eta_n) = O_\P(1).\]
\end{assumption}

In turn, Assumption \ref{Assumption D} is useful for the purposes of analyzing $\mathcal{\widetilde{D}}_{j,n}^{\mathtt{MA}}$, and $\mathcal{\widetilde{D}}_{j,n}^{\mathtt{FD}}$.

\begin{lemma}\label{[Lemma] Mean function estimation with known q}
Suppose Assumptions \ref{Assumption A} and \ref{Assumption D} are satisfied, that $r_n (\widehat{\theta}_n(\x) - \theta_0(\x)) = O_\P(1)$, and that $\epsilon_n \to 0$ and $n \epsilon_n^{1 + 2 \q} \to \infty$. If ${\mathcal{\widetilde{D}}_{\q,n}} \in \{\mathcal{\widetilde{D}}_{\q,n}^{\mathtt{MA}} ,\mathcal{\widetilde{D}}_{\q,n}^{\mathtt{FD}}\}$, then
(\ref{Consistency condition}) is satisfied and Assumption \ref{Assumption C} holds with $\widetilde{M}_{\x,n}$ in \eqref{eq: simple mean function estimator}. 
\end{lemma}

\subsubsection{Mean Function Estimation: Adaptation to Unknown Degeneracy}

Next, consider the somewhat more complicated case where $\q$ is unknown. In this case, we are able to obtain a positive result under the (additional) assumption that there are known integers $\bar\q$ and $\underline{\s}$ for which $\q \leq \bar \q \leq \underline{\s} < \s$. As the notation suggests, $\bar\q$ and $\underline{\s}$ are upper and lower bounds on $\q$ and $\s$, respectively. The stated condition furthermore implies that $\s > \q$, a stronger smoothness condition than the condition $\s \geq \q$ of Assumption \ref{Assumption A}.

For any $\bar\q \geq \q$, noting that $\mathcal{D}_{j}(\x) = 0$ for $j < \q$ and that $\mathcal{D}_\q(\x) > 0$ with $\q$ an odd integer, $\mathcal{M}_\x^\q$ can be written as
\[\mathcal{M}_\x^\q(v) = \sum_{\ell = 1}^{\left\lfloor(\bar\q + 1) / 2 \right\rfloor} \1(2 \ell - 1 \leq \q) \mathcal{D}_{2 \ell - 1}(\x)^+ v^{2 \ell}.\]
Dropping the indicator function term from each summand, we obtain
\[\mathcal{\bar{M}}_\x(v) = \sum_{\ell = 1}^{\left\lfloor (\bar\q + 1) / 2 \right\rfloor} \mathcal{D}_{2 \ell - 1}(\x)^+ v^{2 \ell}.\]
The majorant $\mathcal{\bar{M}}_\x$ is an ``adaptive'' approximation to $\mathcal{M}_\x^\q$ in the sense that it does not depend on $\q$, yet satisfies the local approximation property
\[\mathcal{\bar{M}^\q}_\x \rightsquigarrow \mathcal{M}_\x^\q, \qquad \mathcal{\bar{M}^\q}_\x(v) = \sqrt{n a_n} \mathcal{\bar{M}}_\x(v a_n^{-1}).\]
Moreover, the following (``global'') positivity property automatically holds: for some $c > 0$,
\[\inf_{|v| > K^{-1}} \mathcal{\bar{M}}_\x(v) > c K^{-(\q + 1)} \qquad \text{for every } K > 0.\]
As a consequence, it seems plausible that a ``plug-in'' estimator of $\mathcal{\bar{M}}_\x$ would satisfy Assumption \ref{Assumption C} under reasonable conditions. Indeed, if
\begin{equation}\label{Rate of convergence condition}
a_n^{\q - (2 \ell - 1)} (\mathcal{\widetilde{D}}_{2 \ell - 1,n}(\x) - \mathcal{D}_{2 \ell - 1}(\x)) = o_\P(1), \qquad \ell = 1,\dots,\left\lfloor (\bar\q + 1) / 2 \right\rfloor,
\end{equation}
then Assumption \ref{Assumption C} is satisfied by
\begin{equation}\label{eq: adaptive mean function estimator}
\widetilde{M}_{\x,n}(v) = \sum_{\ell = 1}^{\left\lfloor (\bar\q + 1) / 2 \right\rfloor } \mathcal{\widetilde{D}}_{2 \ell - 1,n}(\x)^+ v^{2\ell}.
\end{equation}

For ``small'' $\ell$ (namely, for $2 \ell - 1 < \q$), the precision requirement (\ref{Rate of convergence condition}) is stronger than consistency, but fortunately it turns out that the requirement can be met as long as $\s$ admits an integer $\underline{\s}$ satisfying the condition $\bar\q \leq \underline{\s} < \s$. To describe estimators satisfying \eqref{Rate of convergence condition} under this condition, let the constants $c_1 ,\dots, c_{\underline{\s} + 1}$ be such that invertibility holds on the part of the $(\underline{\s} + 1) \times (\underline{\s}+1)$ matrix with element $(k,p)$ given by $c_{k}^p$ and let the defining property of $\{\lambda_{j}^{\mathtt{BR}}(k):1 \leq 
 k \leq \underline{\s} + 1\}$ be
\[\sum_{k = 1}^{\underline{\s} + 1}\lambda_{j}^{\mathtt{BR}}(k) c_k^{p} = \1(p = j + 1), \qquad p = 1,\dots,\underline{\s} + 1.\]
Then, for any $j = 1,\dots,\underline{\s}$, the ``bias-reduced'' estimator
\[\mathcal{\widetilde{D}}_{j,n}^{\mathtt{BR}}(\x) = \epsilon_n^{-(j + 1)} \sum_{k = 1}^{\underline{\s} + 1} \lambda_{j}^{\mathtt{BR}}(k)[\widehat{\Upsilon}_n(\x + c_k \epsilon_n) - \widehat{\Upsilon}_n(\x)]\]
is motivated by the fact that as $\epsilon\to0$,
\[\mathcal{D}_{j}(\x) = \epsilon^{-(j + 1)}\sum_{k = 1}^{\underline{\s} + 1} \lambda_{j}^{\mathtt{BR}}(k)[\Upsilon_0(\x + c_k \epsilon) - \Upsilon_0(\x)] + O(\epsilon^{\min(\underline{\s} + 1,\s) - j})\]
when $\s > j$. Relative to $\mathcal{\widetilde{D}}_{j,n}^{\mathtt{MA}}(\x)$ and $\mathcal{\widetilde{D}}_{j,n}^{\mathtt{FD}}(\x)$, this is a distinguishing feature of $\mathcal{\widetilde{D}}_{j,n}^{\mathtt{BR}}(\x)$ and as it turns out this feature will enable us to formulate sufficient conditions for (\ref{Rate of convergence condition}).

\begin{lemma}\label{[Lemma] Mean function estimation unknown q}
Suppose Assumptions \ref{Assumption A} and \ref{Assumption D} are satisfied, that $r_n (\widehat{\theta}_n(\x) - \theta_0(\x)) = O_\P(1)$, and that $\epsilon_n \to 0$. If $\q \leq \bar\q \leq \underline{\s} < \s$, then
\[a_n^{\q - j} (\mathcal{\widetilde{D}}_{j,n}^{\mathtt{BR}}(\x) -\mathcal{D}_{j}(\x)) = O(a_n^{\q - j} \epsilon_n^{\min(\underline{\s} + 1,\s) - j}) + O_{\P}[(a_n \epsilon_n)^{-(j + 1/2)} + (a_n \epsilon_n)^{-j}]\]
for $j = 1,\dots,\underline{\s}$, implying in particular that if also
\[n \epsilon_n^{(1 + 2 \bar \q) \min(\underline{\s},\s - 1)/(\bar \q - 1)} \to 0 \quad \text{and} \quad n \epsilon_n^{1 + 2 \bar \q} \to \infty,\]
then (\ref{Rate of convergence condition}) is satisfied and Assumption \ref{Assumption C} holds with $\widetilde{M}_{\x,n}$ in \eqref{eq: adaptive mean function estimator} if $\mathcal{\widetilde{D}}_{2\ell-1,n}=\mathcal{\widetilde{D}}_{2\ell-1,n}^{\mathtt{BR}}$.
\end{lemma}

As alluded to previously, the ability of $\mathcal{\widetilde{D}}^{\mathtt{BR}}$ to satisfy (\ref{Rate of convergence condition}) is attributable in large part to its bias properties. In an attempt to highlight this, the first display of the lemma gives a stochastic expansion wherein the $O(a_n^{\q-j}\epsilon_n^{\min(\underline{\s}+1,\s)-j})$ term is a (possibly) negligible bias term. For $\mathcal{\widetilde{D}}\in\{\mathcal{\widetilde{D}}^{\mathtt{MA}},\mathcal{\widetilde{D}}^{\mathtt{FD}}\}$, the analogous stochastic expansions are of the form
\[a_n^{\q - j} (\mathcal{\widetilde{D}}_{j,n}(\x) - \mathcal{D}_{j}(\x)) = O(a_n^{\q - j}\epsilon_n^{\q - j}) + O_{\P}[(a_n \epsilon_n)^{-(j + 1/2)} + (a_n \epsilon_n)^{-j}],\]
the $O(a_n^{\q - j} \epsilon_n^{\q - j})$ term also being a bias term. When $a_n \epsilon_n \to \infty$ (as is required for the ``noise'' term in the stochastic expansion to be $o_\P(1)$), this bias term is non-negligible and the estimators $\mathcal{\widetilde{D}}^{\mathtt{MA}}$ and $\mathcal{\widetilde{D}}^{\mathtt{FD}}$ therefore do not satisfy (\ref{Rate of convergence condition}).

Under additional assumptions (including $\s \geq \underline{\s} + 1$ and additional smoothness on $\Phi_0$), $\mathcal{\widetilde{D}}^{\mathtt{BR}}$ admits a Nagar-type mean squared error (MSE) expansion that can be used to select $\epsilon_n. $ The resulting approximate MSE formula is of the form
\[\epsilon_n^{2(\underline{\s} + 1 - j)} \mathsf{B}_{j}^{\mathtt{BR}}(\x)^{2} + \frac{1}{n\epsilon_n^{1 + 2 j}} \mathsf{V}_{j}^{\mathtt{BR}}(\x),\]
where the bias constant is
\[\mathsf{B}_{j}^{\mathtt{BR}}(\x) = \mathcal{D}_{\underline{\s} + 1}(\x)\sum_{k = 1}^{\underline{\s} + 1} \lambda_{j}^{\mathtt{BR}}(k) c_k^{\underline{\s} + 2}\]
and the variance constant is
\[\mathsf{V}_{j}^{\mathtt{BR}}(\x) = \sum_{k = 1}^{\underline{\s} + 1}\sum_{l = 1}^{\underline{\s} + 1} \lambda_{j}^{\mathtt{BR}}(k) \lambda_{j}^{\mathtt{BR}}(l) \mathcal{C}_\x(c_k,c_l);\]
for details, see Section SA.1.7 in the Supplemental Appendix \citep{Cattaneo-Jansson-Nagasawa_2024_supp}.
Assuming $\mathsf{B}_{j}^{\mathtt{BR}}(\x)\neq0$, the approximate MSE is minimized by
\[\epsilon_{j,n}^{\mathtt{BR}}(\x) = \left(\frac{1 + 2 j}{2(\underline{\s} + 1 - j)} \frac{\mathsf{V}_{j}^{\mathtt{BR}}(\x)}{\mathsf{B}_{j}^{\mathtt{BR}}(\x)^{2}}\right)^{1/(3 + 2 \underline{\s})} n^{-1/(3 + 2 \underline{\s})},\]
a feasible version of which can be constructed by replacing $\mathcal{D}_{\underline{\s}+1}(\x)$ and $\mathcal{C}_\x$ with estimators in the expressions for $\mathsf{B}_{j}^{\mathtt{BR}}(\x)$ and $\mathsf{V}_{j}^{\mathtt{BR}}(\x)$, respectively.

\subsection{Bootstrapping}\label{[Subsection] Bootstrapping}

Suppose inference is to be based on a random sample $\mathbf{Z}_1,\dots,\mathbf{Z}_n$ from the distribution of some $\mathbf{Z.}$ In all examples of which we are aware, the bootstrap parts of Assumption \ref{Assumption B} are satisfied when $(\widehat{\Gamma}_n^*,\widehat{\Phi}_n^*,\widehat{u}_n^*)$ is given by the nonparametric bootstrap analog of $(\widehat{\Gamma}_n,\widehat{\Phi}_n,\widehat{u}_n)$. Nevertheless, computationally simpler alternatives are often available and in what follows we will present one such alternative. To motivate our proposal, it is instructive to begin by revisiting Example \ref{[Example] Monotone Density Estimation}.

\setcounter{example}{0}
\begin{example}[Monotone Density Estimation, continued]
In this example, $\mathbf{Z}=X$. Moreover, defining $\gamma_0(x;\mathbf{z})=\1(\mathbf{z}\leq x)$ and $\phi_0(x;\mathbf{z})=x$, we have the representations
\[\Gamma_0(x)=\E[\gamma_0(x;\mathbf{Z})]\qquad\text{and} \qquad \Phi_0(x)=\E[\phi_0(x;\mathbf{Z})],\]
and the estimators $\widehat{\Gamma}_n$ and $\widehat{\Phi}_n$ are linear in the sense that they are of the form
\[\widehat{\Gamma}_n(x)=\frac{1}{n}\sum_{i=1}^n\gamma_0(x;\mathbf{Z}_i)\qquad\text{and}\qquad\widehat{\Phi}_n(x)=\frac{1}{n}\sum_{i=1}^n\phi_0(x;\mathbf{Z}_i),\]
respectively. Finally, $\widehat{u}_n = \max_{1\leq i\leq n}\mathbf{Z}_i \lor \x$.

Letting $\mathbf{Z}_{1,n}^*,\dots,\mathbf{Z}_{n,n}^*$ be a random sample from the empirical distribution of $\mathbf{Z}_1,\dots,\mathbf{Z}_n$, $\widehat{u}_n^* = \max_{1 \leq i \leq n}\mathbf{Z}_{i,n}^* \lor \x$ is the nonparametric bootstrap analog of $\widehat{u}_n$, while
\[\widehat{\Gamma}_n^*(x)=\frac{1}{n}\sum_{i = 1}^n\gamma_0(x;\mathbf{Z}_{i,n}^*) \qquad \text{and} \qquad \widehat{\Phi}_n^*(x) = \frac{1}{n} \sum_{i = 1}^n\phi_0(x;\mathbf{Z}_{i,n}^*)\]
are the nonparametric bootstrap analogs of $\widehat{\Gamma}_n$ and $\widehat{\Phi}_n$, respectively.

In the case of $\widehat{u}_n$, the alternative bootstrap analog $\widehat{u}_n^*=\widehat{u}_n$ is computationally trivial and automatically satisfies the bootstrap parts of \ref{Assumption B: uhat} and \ref{Assumption B: Phi uhat}. As for $\widehat{\Gamma}_n$ and $\widehat{\Phi}_n$, their nonparametric bootstrap analogs admit the weighted bootstrap representations
\[\widehat{\Gamma}_n^*(x)=\frac{1}{n}\sum_{i=1}^nW_{i,n}\gamma_0(x;\mathbf{Z}_i)\qquad\text{and}\qquad\widehat{\Phi}_n^*(x)=\frac{1}{n}\sum_{i=1}^nW_{i,n}\phi_0(x;\mathbf{Z}_i),\]
where, conditionally on $\mathbf{Z}_1,\dots,\mathbf{Z}_n$, $(W_{1,n},\dots,W_{n,n})$ is multinomial distributed with probabilities $(n^{-1},\dots,n^{-1})$ and number of trials $n$. (In turn, the weighted bootstrap interpretation of the nonparametric bootstrap analog of $(\widehat{\Gamma}_n,\widehat{\Phi}_n)$ in this example is interesting partly because it can be used to embed the nonparametric bootstrap in a class of bootstraps also containing the Bayesian bootstrap and the wild bootstrap.)
\end{example}
\setcounter{example}{2}

Looking beyond Example \ref{[Example] Monotone Density Estimation}, finding a computationally trivial $\widehat{u}_n^*$ satisfying the bootstrap part of \ref{Assumption B: uhat} and \ref{Assumption B: Phi uhat} is usually straightforward. On the other hand, although a weighted bootstrap interpretation of the nonparametric bootstrap version of the estimators $\widehat{\Gamma}_n$ and $\widehat{\Phi}_n$ is available whenever they are linear (e.g., in Example \ref{[Example] Monotone Regression Estimation}), there is no shortage of examples for which linearity does not hold. Nevertheless, the weighted bootstrap representation of the nonparametric bootstrap in Example \ref{[Example] Monotone Density Estimation} turns out be useful for our purposes, as it is suggestive of computationally attractive alternatives to the nonparametric bootstrap in more complicated examples.

When the non-bootstrap part of \ref{Assumption B: weak convergence} holds, the estimators $\widehat{\Gamma}_n$ and $\widehat{\Phi}_n$ are typically asymptotically linear in the sense that they admit (possibly) unknown functions $\gamma_0$ and $\phi_0$ (satisfying $\Gamma_0(x)=\E[\gamma_0(x;\mathbf{Z})]$ and $\Phi_0(x)=\E[\phi_0(x;\mathbf{Z})]$, respectively) for which the approximations
\[\widehat{\Gamma}_n(x)\approx\bar{\Gamma}_n(x)=\frac{1}{n}\sum_{i=1}^n \gamma_0(x;\mathbf{Z}_i)\qquad\text{and}\qquad\widehat{\Phi}_n(x)\approx\bar{\Phi}_n(x)=\frac{1}{n}\sum_{i=1}^n\phi_0(x;\mathbf{Z}_i)\]
are suitably accurate. Assuming also that $\gamma_0$ and $\phi_0$ admit sufficiently well behaved estimators $\widehat{\gamma}_n$ and $\widehat{\phi}_n$ (say), it then stands to reason that the salient properties of $\widehat{\Gamma}_n$ and $\widehat{\Phi}_n$ are well approximated by those of the easy-to-compute exchangeable bootstrap-type pair
\[\widehat{\Gamma}_n^*(x) = \frac{1}{n} \sum_{i = 1}^nW_{i,n} \widehat{\gamma}_n(x;\mathbf{Z}_i) \qquad \text{and} \qquad \widehat{\Phi}_n^*(x) = \frac{1}{n} \sum_{i = 1}^n W_{i,n} \widehat{\phi}_n(x;\mathbf{Z}_i),\]
where $W_{1,n},\dots,W_{n,n}$ denote exchangeable random variable (independent of $\mathbf{Z}_1,\dots,\mathbf{Z}_n$).

To give a precise statement, let
\[\psi_x(v;\mathbf{z}) = \gamma_0(x + v;\mathbf{z}) - \gamma_0(x;\mathbf{z}) - \theta_0(x) [\phi_0(x + v;\mathbf{z}) -\phi_0(x;\mathbf{z})]\]
and define
\[\check{\Gamma}_n(x) = \frac{1}{n} \sum_{i = 1}^n \widehat{\gamma}_n(x;\mathbf{Z}_i) \qquad \text{and} \qquad  \check{\Phi}_n(x) = \frac{1}{n} \sum_{i = 1}^n \widehat{\phi}_n(x;\mathbf{Z}_i).\]
In addition, for any function class $\mathfrak{F}$, let $N_{U}(\varepsilon,\mathfrak{F})$ denote the associated uniform covering numbers relative to $L_{2};$ that is, for any $\varepsilon>0$, let
\[N_{U}(\varepsilon,\mathfrak{F}) = \sup_{Q} N(\varepsilon \left\| \bar{F}\right\|_{Q,2},\mathfrak{F},L_{2}(Q)),\]
where $\bar{F}$ is the minimal envelope function of $\mathfrak{F}$, $\left\|\cdot\right\|_{Q,2}$ is the $L_{2}(Q)$ norm, $N(\cdot)$ is the covering number, and the supremum is over all discrete probability measure $Q$ with $\left\|\bar{F}\right\|_{Q,2} > 0$.

\begin{assumption}\label{Assumption E}
For some $\delta > 0$, the following are satisfied:
\begin{enumerate}[label=(E\arabic*)]
    \item \label{Assumption E: iid} $\mathbf{Z}_1,\dots,\mathbf{Z}_n,$ are independent and identically distributed.
    
    \item \label{Assumption E: bootstrap weights}$W_{1,n},\dots,W_{n,n}$ are exchangeable random variables independent of $\mathbf{Z}_1,\dots ,\mathbf{Z}_n$, $\widehat{\gamma}_n$, and $\widehat{\phi}_n$.
    In addition, for some $\mathfrak{r}>(4\q+2)/(2\q-1)$,
    \[\frac{1}{n}\sum_{i=1}^nW_{i,n}=1,\quad\frac{1}{n}\sum_{i=1}^n    (W_{i,n}-1)^{2}\to_\P1,\quad\text{and}\quad\E[|W_{1,n}|^{\mathfrak{r}}]=O(1).\]
    
    \item \label{Assumption E: Gamma hat}$\sup_{x\in I}|\widehat{\Gamma}_n(x)-\bar{\Gamma}_n(x)|=o_{\P}(1)$ and
    \[\frac{1}{n}\sum_{i=1}^n\sup_{x\in I}|\widehat{\gamma}_n(x;\mathbf{Z}_i)-\gamma_0(x;\mathbf{Z}_i)|^2=o_{\P}(1).\]
    Also,
    \begin{equation*}
            \frac{a_n}{n}\sum_{i=1}^n\sup_{x \in I_{\x}^{\delta}}\left|\widehat{\gamma}_n(x;\mathbf{Z}_i) - \gamma_0(x;\mathbf{Z}_i) \right|^2 =o_{\P}(1),
        \end{equation*} 
        and there exist $\beta_\gamma \in [1/2,\q+1)$ and random variables $A_{\gamma,n}=o_{\P}(1)$ and $B_{\gamma,n}=o_{\P}(a_n^{\beta_\gamma})$ such that, for $V \in (0,2\delta]$,
        \begin{equation*}
            \sqrt{na_n}\sup_{|v|\leq V } \left|\widehat{\Gamma}_n(\x+v ) -\widehat{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+v ) + \bar{\Gamma}_n(\x)\right|  \leq  A_{\gamma,n}+  V^{\beta_\gamma}B_{\gamma,n} ,
        \end{equation*}
        \begin{equation*}
            \sqrt{na_n}\sup_{|v|\leq V} \left|\check{\Gamma}_n(\x+v) -\check{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+v ) + \bar{\Gamma}_n(\x)\right| \leq A_{\gamma,n}+  V^{\beta_\gamma}B_{\gamma,n}.
        \end{equation*}

        In addition, for some $\rho_\gamma\in (0,2)$,
        \begin{equation*}
            \limsup_{\varepsilon\downarrow 0}\frac{\log N_{U}(\varepsilon,\mathfrak{F}_{\gamma})}{\varepsilon^{-\rho_\gamma}} < \infty,\quad\E[\bar{F}_{\gamma}(\mathbf{Z})^2]<\infty, \quad \limsup_{\varepsilon\downarrow 0} \frac{\log N_{U}(\varepsilon,\widehat{\mathfrak{F}}_{\gamma,n})}{\varepsilon^{-\rho_\gamma}} = O_{\P}(1),
        \end{equation*}
        where $\mathfrak{F}_{\gamma}=\{\gamma_0(x;\cdot):x\in I\}$, $\bar{F}_{\gamma}$ is its minimal envelope, and $\widehat{\mathfrak{F}}_{\gamma,n}=\{\widehat{\gamma}_n(x;\cdot):x\in I\}$.
        
        Also,
        \begin{equation*}
            \limsup_{\eta\downarrow 0} \frac{\E[\bar{D}_{\gamma}^{\eta}(\mathbf{Z})^2+\bar{D}_{\gamma}^{\eta}(\mathbf{Z})^4]}{\eta} <\infty,
        \end{equation*}
        where $\bar{D}_{\gamma}^{\eta}$ is the minimal envelope of $\{\gamma_0(x;\cdot)-\gamma_0(\x;\cdot):x\in I_\x^\eta\}$.

    \item \label{Assumption E: Phi hat}
    $\sup_{x\in I}|\widehat{\Phi}_n(x)-\bar{\Phi}_n(x)|=o_{\P}(1)$ and
    \[\frac{1}{n}\sum_{i=1}^n\sup_{x\in I}|\widehat{\phi}_n(x;\mathbf{Z}_i)-\phi_0(x;\mathbf{Z}_i)|^2=o_{\P}(1).\] 
    Also, $a_n |\widehat{\Phi}_n(\x)-\bar{\Phi}_n(\x)|=o_{\P}(1),a_n |\check{\Phi}_n(\x)-\bar{\Phi}_n(\x)|=o_{\P}(1)$,
    \begin{equation*}
        \frac{a_n}{n}\sum_{i=1}^n\sup_{x \in I_{\x}^{\delta}}\left|\widehat{\phi}_n(x;\mathbf{Z}_i) -\phi_0(x;\mathbf{Z}_i) \right|^2 =o_{\P}(1),
    \end{equation*}
    and there exist $\beta_\phi \in [1/2,\q]$ and random variables $A_{\phi,n}=o_{\P}(1)$ and $B_{\phi,n}=o_{\P}(a_n^{\beta_\phi})$ such that, for $V \in (0,2\delta]$,
    \begin{equation*}
        \sqrt{na_n} \sup_{|v|\leq V}\left|\widehat{\Phi}_n(\x+v) -\widehat{\Phi}_n(\x) - \bar{\Phi}_n(\x+v) + \bar{\Phi}_n(\x)\right|\leq  A_{\phi,n}+  V^{\beta_\phi}B_{\phi,n} ,
        \end{equation*}
    \begin{equation*}
        \sqrt{na_n} \sup_{|v|\leq V}\left|\check{\Phi}_n(\x+v) -\check{\Phi}_n(\x) - \bar{\Phi}_n(\x+v) + \bar{\Phi}_n(\x)\right|\leq  A_{\phi,n}+  V^{\beta_\phi}B_{\phi,n}.
    \end{equation*}
     
        In addition, for some $\rho_\phi\in (0,2)$,
        \begin{equation*}
            \limsup_{\varepsilon\downarrow 0}\frac{\log N_{U}(\varepsilon,\mathfrak{F}_{\phi})}{\varepsilon^{-\rho_\phi}} < \infty,\quad\E[\bar{F}_{\phi}(\mathbf{Z})^2]<\infty, \quad \limsup_{\varepsilon\downarrow 0} \frac{\log N_{U}(\eta,\widehat{\mathfrak{F}}_{\phi,n})}{\varepsilon^{-\rho_\phi}} = O_{\P}(1),
        \end{equation*}
        where $\mathfrak{F}_{\phi}=\{\phi_0(x;\cdot):x\in I\}$, $\bar{F}_{\phi}$ is its minimal envelope, and $\widehat{\mathfrak{F}}_{\phi,n}=\{\widehat{\phi}_n(x;\cdot):x\in I\}$.
        
        Also,
        \begin{equation*}
            \limsup_{\eta\downarrow 0} \frac{\E[\bar{D}_{\phi}^{\eta}(\mathbf{Z})^2+\bar{D}_{\phi}^{\eta}(\mathbf{Z})^4]}{\eta} <\infty,
        \end{equation*}
        where $\bar{D}_{\phi}^{\eta}$ is the minimal envelope of $\{\phi_0(x;\cdot)-\phi_0(\x;\cdot):x\in I_\x^\eta\}$.
    
    \item \label{Assumption E: covariance kernel} For every $\eta_n>0$ with $a_n\eta_n=O(1)$,
    \[\sup_{v,v'\in[-\eta_n,\eta_n]}\frac{\E [|\psi_\x(v;\mathbf{Z})-\psi_\x(v';\mathbf{Z})|]}{|v-v'|}=O(1)\]
    and, for all $v,u\in\R$, and for some $\mathcal{C}_\x$ (as defined in \ref{Assumption B: weak convergence}),
    \[\frac{\E[\psi_\x(\eta_n v;\mathbf{Z})\psi_{\x}(\eta_n u;\mathbf{Z})]}{\eta_n}\to\mathcal{C}_{\x}(v,u).\]
\end{enumerate}

\end{assumption}

\begin{lemma}
\label{[Lemma] Bootstrapping}Suppose Assumptions \ref{Assumption A} and \ref{Assumption E} are satisfied. Then \ref{Assumption B: weak convergence}-\ref{Assumption B: uniform convergence Phi} are satisfied. 
\end{lemma}

\section{Examples}\label{[Section] Examples}

We apply our main results to two distinct sets of examples, both previously analyzed in \citet{Westling-Carone_2020_AoS} and \citet{Westling-Gilbert-Carone_2020_JRRSB}, and references therein. In the supplemental appendix, we also consider two other set of examples, namely monotone hazard estimation \citep{Huang-Wellner_1995_SJS} and monotone distribution estimation \citep{vanderVaart-vanderLaan_2006_IJB}. All the proofs are found in the supplemental appendix. 

For future reference, we collect the conditions on the bootstrap weights in an assumption.
\begin{assumption-BW}\label{Assumption BW}
    The bootstrap weights $W_{1,n},\dots,W_{n,n}$ are exchangeable , independent of the random sample $\mathbf{Z}_1,\dots,\mathbf{Z}_n$, and satisfy, for some $\mathfrak{r}>6$,
    \begin{equation*}
        \frac{1}{n}\sum_{i=1}^nW_{i,n}=1,\qquad\frac{1}{n}\sum_{i=1}^n(W_{i,n}-1)^{2}\to_\P1,\qquad\text{and}\qquad\E[|W_{1,n}|^{\mathfrak{r}}]=O(1).
    \end{equation*}
\end{assumption-BW}

\subsection{Monotone Density Estimation}\label{[Subsection] Examples: Monotone Density Estimation}

Consider the problem of estimating the density of a non-negative, continuously distributed random variable, possibly with censoring. Let $\mathbf{Z}_1,\dots,\mathbf{Z}_n$ be $i.i.d.$ copies of $\mathbf{Z}=(\check{X},\Delta)'$, with $\check{X} = X \land C$ and $\Delta=\1(X\leq C)$.
Assuming that $f_0$, the density of $X$, (exists and) is non-decreasing on $I=[0,u_0]$ where $u_0$ is some point in the support of $X$, the parameter of interest is $\theta_0(\x)$ for some $\x\in(0,u_0)$, where $\theta_0=f_0$. This setup generalizes Example \ref{[Example] Monotone Density Estimation}. We take $\Phi_0(x)=x$ and therefore have $\Gamma_0=F_0$, where $F_0$ is the cdf of $X$.


The canonical case of no censoring (i.e.,\ $\P[C \geq X]=1$) has been considered in Example \ref{[Example] Monotone Density Estimation}. Assumptions under which this example is covered by our general theory is given in the following result.
\begin{corollary}\label{[Corollary] monotone density without censoring}
    Let $u_0$ be the supremum of the support of $X$. Suppose $\x$ is in the interior of $I$, $\theta_0$ satisfies \ref{Assumption A: theta0}, and Assumption BW holds. Then, Assumptions \ref{Assumption A}, \ref{Assumption B}, and \ref{Assumption D} hold with 
    \begin{equation*}
        \widehat{\Gamma}_n(x) = \frac{1}{n}\sum_{i=1}^n \1(X_i\leq x),\quad \widehat{\Gamma}_n^*(x) = \frac{1}{n}\sum_{i=1}^n W_{i,n}\1(X_i\leq x),
    \end{equation*}
    \begin{equation*}
        \widehat{\Phi}_n(x) = \widehat{\Phi}_n^*(x) = x, \quad \widehat{u}_n = \widehat{u}_n^* = \max_{1\leq i\leq n}X_i \lor \x,
    \end{equation*}
\begin{equation*}
    \mathcal{C}_\x(s,t) = f_0(\x) (|s| \land |t|) \1(\sign(s)=\sign(t)), \quad \mathcal{D}_\q(\x) = \frac{\partial^\q f_0(\x)}{(\q+1)!}.
\end{equation*}
\end{corollary}

Under the assumptions of Corollary \ref{[Corollary] monotone density without censoring}, it follows from Theorem \ref{[Theorem] Main result} that the bootstrap consistency result \eqref{Bootstrap consistency} holds for any $\widetilde{M}_{\x,n}$ satisfying Assumption \ref{Assumption C}. In particular, any pointwise consistent estimator of $\partial^\q f_0(\x)$ could be used to estimate $\mathcal{D}_\q(\x)$. Alternatively, since Assumption \ref{Assumption D} also holds, $\widetilde{M}_{\x,n}$ in \eqref{eq: simple mean function estimator} with $\widetilde{\mathcal{D}}_{\q,n} \in \{ \widetilde{\mathcal{D}}_{\q,n}^{\mathtt{MA}},\widetilde{\mathcal{D}}_{\q,n}^{\mathtt{FD}},\widetilde{\mathcal{D}}_{\q,n}^{\mathtt{BR}}\}$ can also be used, provided that $\epsilon_n \to 0$ and $n\epsilon_n^{1+2\q}\to \infty$.

Suppose now that censoring occurs completely at random; that is, suppose $X\indep C$ \citep[e.g.,][and references therein]{Huang-Wellner_1995_SJS}.
Let $\widehat{S}_n$ and $\widehat{G}_n$ be the Kaplan-Meier estimators of the survival functions $S_0(\cdot)=\P[X>\cdot]$ and $G_0(\cdot)=\P[C>\cdot]$, and define $\widehat{F}_n=1-\widehat{S}_n$ and $\widehat{\Lambda}_n(x) = \int_0^x \widehat{S}_n(u)^{-1}d\widehat{F}_n(u)$. In anticipation of the following result, we note that $\Gamma_0(x)=F_0(x)=\E[\gamma_0(x;\mathbf{Z})]$, where
\[\gamma_0(x;\mathbf{Z})= F_0(x)+S_0(x)\left[\frac{\Delta\1(\check{X}\leq x) }{S_0(\check{X})G_0(\check{X})} - \int_0^{\check{X} \land x}\frac{\Lambda_0(du)}{S_0(u)G_0(u)}\right],\]
with $\Lambda_0(x) = \int_0^x S_0(u)^{-1}dF_0(u)$.
\begin{corollary}\label{[Corollary] monotone density independent right-censoring}
    Let $u_0$ be an interior point in the support of $X$. Suppose $\x$ is in the interior of $I$, $\theta_0$ satisfies \ref{Assumption A: theta0}, $C\indep X$, $C$ is absolutely continuous on $I$ with bounded density, and $S_0(u_0)G_0(u_0) > 0$. Then, Assumptions \ref{Assumption A}, \ref{Assumption B}, and \ref{Assumption D} hold with 
    \begin{equation*}
        \widehat{\Gamma}_n(x)= \widehat{F}_n(x),\quad \widehat{\Gamma}_n^*(x)= \frac{1}{n}\sum_{i=1}^n W_{i,n}\widehat{\gamma}_n(x;\mathbf{Z}_i)
    \end{equation*}
    \begin{equation*}
        \widehat{\gamma}_n(x;\mathbf{Z}) = \widehat{F}_n(x) + \widehat{S}_n(x)\left[\frac{\Delta\1(\check{X}\leq x)}{\widehat{S}_n(\check{X})\widehat{G}_n(\check{X})} - \int_0^{\check{X} \land x} \frac{d\widehat{\Lambda}_n(u) }{\widehat{S}_n(u)\widehat{G}_n(u)} \right],
\end{equation*}
    \begin{equation*}
        \widehat{\Phi}_n(x)=\widehat{\Phi}_n^*(x)=x,\quad \widehat{u}_n=\widehat{u}_n^*=u_0,
    \end{equation*}
\begin{equation*}
    \mathcal{C}_\x(s,t) = \frac{f_0(\x)}{G_0(\x)}(|s| \land |t|) \1(\sign(s)=\sign(t)), \quad \mathcal{D}_\q(\x) = \frac{\partial^\q f_0(\x)}{(\q+1)!}.
\end{equation*}
\end{corollary}
\noindent Theorem \ref{[Theorem] Main result} implies that inference based on our proposed bootstrap-assisted method is asymptotically valid for this example as well. Additionally, Assumption \ref{Assumption D} also holds under the above conditions, and we can use one of the numerical derivative estimators discussed in Section \ref{[Subsection] Mean function estimation}. 

In the supplemental appendix, we also consider estimation of monotone density under conditionally independent right-censoring, where covariates are present. We do not discuss this example here because it involves additional notation and technicalities.


\subsection{Monotone Regression Estimation}\label{[Subsection] Examples: Monotone Regression Estimation}

Consider now the problem of regression estimation, possibly with covariate adjustment. We assume that $\mathbf{Z}_1,\dots,\mathbf{Z}_n$ are $i.i.d.$ copies of $\mathbf{Z}=(Y,X,\mathbf{A}')'$, where $X$ is continuous on its support $I$ with density $f_0$ and cdf $F_0$. Defining $\mu_0(x,\mathbf{a})=\E[Y|X=x,\mathbf{A}=\mathbf{a}]$, the parameter of interest is $\theta_0(\x)$ for some $\x$ in the interior of $I$, where $\theta_0(\cdot)=\E[\mu_0(\cdot,\mathbf{A})]$ is assumed to be non-decreasing on $I$. (If there are no covariates $\mathbf{A}$, then $\theta_0(\cdot)=\mu_0(\cdot)=\E[Y|X=\cdot]$.) We take $\Phi_0=F_0$ and set $u_0=1$. 

The classical monotone regression estimator has been considered in Example \ref{[Example] Monotone Regression Estimation}. Assumptions under which this example is covered by our general theory is given in the following result. To state it, let $\varepsilon=Y-\E[Y|X]$ and define $\sigma_0^2(x) = \E[\varepsilon^2|X=x]$.
\begin{corollary}\label{[Corollary] classical monotone regression}
    Suppose that $\x$ is in the interior of $I$, $\theta_0$ satisfies \ref{Assumption A: theta0}, $\Phi_0$ satisfies \ref{Assumption A: Phi0}, and Assumption BW holds. If $\sigma_0^2(x)$ is continuous and positive at $\x$, $\E[\varepsilon^2]<\infty$, and if $\sup_{x \in I_\x^\delta}\E[\varepsilon^4|X=x]<\infty$ for some $\delta >0$, then Assumptions \ref{Assumption A}, \ref{Assumption B}, and \ref{Assumption D} hold with 
    \begin{equation*}
        \widehat{\Gamma}_n(x)= \frac{1}{n}\sum_{i=1}^n Y_i\1(X_i\leq x),\quad \widehat{\Gamma}_n^*(x)= \frac{1}{n}\sum_{i=1}^n W_{i,n}Y_i\1(X_i\leq x),
    \end{equation*}
    \begin{equation*}
        \widehat{\Phi}_n(x)=\frac{1}{n}\sum_{i=1}^n \1(X_i\leq x),\quad \widehat{\Phi}_n^*(x)=\frac{1}{n}\sum_{i=1}^n W_{i,n}\1(X_i\leq x),\quad \widehat{u}_n=\widehat{u}_n^*=1,
    \end{equation*}
\begin{equation*}
    \mathcal{C}_\x(s,t) = f_0(\x) \sigma_0^2(\x) (|s| \land |t|) \1(\sign(s)=\sign(t)),\qquad \mathcal{D}_\q(\x) = \frac{f_0(\x)\partial^\q\mu_0(\x)}{(\q+1)!}.
\end{equation*}
\end{corollary}%
\noindent Under the assumptions of Corollary \ref{[Corollary] classical monotone regression}, it follows from Theorem \ref{[Theorem] Main result} that the bootstrap consistency result \eqref{Bootstrap consistency} holds for any $\widetilde{M}_{\x,n}$ satisfying Assumption \ref{Assumption C}. In this example, any pointwise consistent estimators of $f_0(\x)$ and $\partial^\q \mu_0(\x)$ could be used to estimate $\mathcal{D}_\q(\x)$. Alternatively, instead of using two distinct estimators, we can use the numerical derivative-type estimators since Assumption \ref{Assumption D} also holds.

Next, consider the case of monotone regression estimation with covariate-adjustment.
Introducing covariates $\mathbf{A}$, let $f_{X|\mathbf{A}}(\cdot|\mathbf{a})$ denote the conditional Lebesgue density of $X$ given $\mathbf{A}=\mathbf{a}$ and define $g_0(x,\mathbf{a})=f_{X|\mathbf{A}}(x|\mathbf{a})/f_0(x)$. Noting that $\Gamma_0(x)=\E[\gamma_0(x;\mathbf{Z})]$, where
\[\gamma_0(x;\mathbf{Z})= \1(X\leq x) \left[\frac{Y-\mu_0(X,\mathbf{A})}{g_0(X,\mathbf{A})} - \theta_0(X)\right],\]
we assume that $\widehat{\Gamma}_n$ is of the form $\widehat{\Gamma}_n(x)= n^{-1}\sum_{i=1}^n \widehat{\gamma}_n(x;\mathbf{Z}_i)$, where
\[\widehat{\gamma}_n(x;\mathbf{Z})= \1(X\leq x)\left[\frac{Y-\widehat{\mu}_n(X,\mathbf{A})}{\widehat{g}_n(X,\mathbf{A})} + \frac{1}{n}\sum_{j=1}^n \widehat{\mu}_n(X,\mathbf{A}_j)\right],\]
with $\widehat{\mu}_n$ and $\widehat{g}_n$ being preliminary estimators of $\mu_0$ and $g_0$ based on $\mathbf{Z}_1,\dots,\mathbf{Z}_n$. As discussed by \cite{Westling-Gilbert-Carone_2020_JRRSB}, there are a number of candidates for $\widehat{\mu}_n$ and $\widehat{g}_n$. To accommodate a relatively wide class of estimators, the following assumption employs high-level conditions on $\widehat{\mu}_n$ and $\widehat{g}_n$. To state the assumption, let $\varepsilon=Y-\E[Y|X,\mathbf{A}]$ and define $\sigma_0^2(x,\mathbf{a})=\E[\varepsilon^2|X=x,\mathbf{A}=\mathbf{a}]$.

\begin{assumption-MRC}\label{Assumption MRC}
    For some $\delta>0$, the following are satisfied:
    \begin{enumerate}[label=\normalfont(\roman*),noitemsep,itemindent=*]
        \item[(i)] $\E[g_0(\x,\mathbf{A})^{-1}\sigma_0^2(\x,\mathbf{A})]>0$, $\E[\varepsilon^2]<\infty$, and $\sup_{x \in I_\x^\delta}\E[\varepsilon^4|X=x]<\infty$.
        
        \item[(ii)] $f_{X|\mathbf{A}}$ is bounded and $g_0$ is bounded away from zero.

        \item[(iii)] 
        There exist random variables $A_n=o_{\P}(1)$ and $B_n=O_\P(a_n^{1/2})$ such that
        \[\sqrt{na_n}\sup_{\vert v\vert\leq  V} \vert\widehat{\Gamma}_n(\x+v)-\widehat{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+v)+\bar{\Gamma}_n(\x)\vert\leq A_n + V B_n, \qquad V\in (0,2\delta].\]
        In addition,
        \[\frac{a_n}{n}\sum_{i=1}^n|\widehat{\mu}_n(X_i,\mathbf{A}_i)-\mu_0(X_i,\mathbf{A}_i)|^2=o_{\P}(1),\]
        \[\frac{a_n}{n^2}\sum_{i=1}^n\sum_{j=1}^n|\widehat{\mu}_n(X_i,\mathbf{A}_j)-\mu_0(X_i,\mathbf{A}_j)|^2=o_{\P}(1),\]
        and
        \[\frac{a_n}{n}\sum_{i=1}^n\varepsilon_i^2|\widehat{g}_n(X_i,\mathbf{A}_i)-g_0(X_i,\mathbf{A}_i)|^2=o_{\P}(1).\]
        
        \item[(iv)] $\E[\bar{\mu}(\mathbf{A})^2]<\infty$, where
        \[\bar{\mu}(\mathbf{A})=\sup_{|x-x'|\leq\delta}\frac{|\mu_0(x,\mathbf{A})-\mu_0(x',\mathbf{A})|}{|x-x'|}.\]
        
        \item[(v)] $\E[\bar{\sigma}^2(\mathbf{A})]<\infty$, where, for some function $\omega$ with $\lim_{\eta\downarrow0}\omega(\eta)=0$,
        \[\left|\frac{\sigma_0^2(x,\mathbf{A})f_0(x)}{g_0(x,\mathbf{A})} -\frac{\sigma_0^2(\x,\mathbf{A})f_0(\x)}{g_0(\x,\mathbf{A})}\right| \leq\omega(|x-\x|) \bar{\sigma}^2(\mathbf{A}), \qquad x \in I_\x^\delta.\]
    \end{enumerate} 
\end{assumption-MRC}
Part (iii) of the assumption contains high-level conditions. In the supplemental appendix, we provide primitive sufficient conditions using nonparametric estimators based on sample splitting.

\begin{corollary}\label{[Corollary] monotone regression with covariates}
Suppose that $\x$ is in the interior of $I$, $\theta_0$ satisfies \ref{Assumption A: theta0}, $\Phi_0$ satisfies \ref{Assumption A: Phi0}, and Assumption BW holds. If Assumption MRC holds, then Assumptions \ref{Assumption A} and \ref{Assumption B} hold with 
    \begin{equation*}
        \widehat{\Gamma}_n^*(x)= \frac{1}{n}\sum_{i=1}^n W_{i,n}\widehat{\gamma}_n(x;\mathbf{Z}_i),
    \end{equation*}
    \begin{equation*}
        \widehat{\Phi}_n(x)=\frac{1}{n}\sum_{i=1}^n \1(X_i\leq x),\quad \widehat{\Phi}_n^*(x)=\frac{1}{n}\sum_{i=1}^n W_{i,n}\1(X_i\leq x),\quad \widehat{u}_n=\widehat{u}_n^*=1,
    \end{equation*}
    \begin{equation*}
    \mathcal{C}_\x(s,t) = f_0(\x) \E\left[\frac{\sigma_0^2(\x,\mathbf{A})}{g_0(\x,\mathbf{A})}\right] (|s| \land |t|) \1(\sign(s)=\sign(t)),\quad \mathcal{D}_\q(\x) = \frac{f_0(\x)\partial^\q\theta_0(\x)}{(\q+1)!}.
\end{equation*}
\end{corollary}
\noindent As a consequence, if $\widetilde{M}_{\x,n}$ satisfies Assumption \ref{Assumption C}, then the ``reshaped'' bootstrap estimator $\widetilde{\theta}_n^*(\x)=\partial_-\GCM_{[0,1]}(\widetilde{\Gamma}_n^*\circ\widehat{\Phi}_n^{*-})\circ\widehat{\Phi}_n^*(\x)$ gives a bootstrap-assisted distributional approximation satisfying \eqref{Bootstrap consistency}. In this example, the covariance kernel contains $\E[g_0(\x,\mathbf{A})^{-1}\sigma_0^2(\x,\mathbf{A})]$ and the plug-in procedure discussed in Section \ref{[Section] Bootstrap-based Distributional Approximation} would involve estimating this nuisance parameter, which in turn requires estimation of $\sigma_0^2(\x,\cdot)$. When $\mathbf{A}$ is of moderate dimension, this would introduce a challenging estimation problem on top of estimating $\mu_0$ and $g_0$. Our bootstrap-assisted procedure circumvents estimation of this additional nuisance parameter.

\section{Simulations}\label{[Section] Simulations}

We consider the canonical case of monotone regression (i.e., Example \ref{[Example] Monotone Regression Estimation}). We estimate the non-decreasing regression function $\theta_0(\cdot)=\E[Y|X=\cdot]$ at an interior point $\x$ using a random sample of observations. Three distinct data generating processes (DGPs) are considered. For each DGP, we let $X$ be a uniform $(0,1)$ random variable and we let $Y=\theta_0(X)+\sigma_0(X)\widetilde{\varepsilon}$, where $\widetilde{\varepsilon}$ is a standard normal random variable independent of $X$. DGP 1 sets $\theta_0(x)=2\exp(x-0.5)$ and $\sigma_0=1$, DGP 2 sets $\theta_0(x)=2\exp(x-0.5)$ and $\sigma_0=\exp(x)$, and DGP 3 sets $\theta_0(x)=24\exp(X-0.5) -24(X-0.5) - 12(X-0.5)^2$ and $\sigma_0=0.1$. In all three cases, we set $\x=0.5$. The second DGP exhibits heteroskedastic regression errors, and the third DGP features a regression function whose first derivative equals zero at the evaluation point $\x$; that is, DGP 3 exhibits a degree of degeneracy and if inference is conducted without knowledge of the fact that $\q=3$, then this feature makes the inference problem more challenging. 

The Monte Carlo experiment employs a sample size $n=1,000$ with $2,000$ bootstrap replications and $4,000$ simulations, and compares four types of bootstrap-based inference procedures: the standard non-parametric bootstrap, $m$-out-of-$n$ bootstrap, the ``plug-in method'' which simulates the limit law with estimated nuisance parameters, and our proposed bootstrap-assisted inference method implemented using the bias-reduced numerical derivative estimator discussed in Section \ref{[Subsection] Mean function estimation}. For the plug-in method, we estimate the covariance kernel and the nuisance parameter $\partial \Phi_0(\x)$ using local linear kernel estimators, and we estimate the mean function by the same numerical derivative estimator as for our bootstrap-assisted procedure. For $m$-out-of-$n$ bootstrap and the plug-in method, researchers need to specify the characteristic exponent $\q$, and we use the true (infeasible) value. For the numerical derivative estimator, we developed a rule of thumb for the step size $\epsilon_n$ to operationalize the procedure. For further details, see Section SA.6 of \cite{Cattaneo-Jansson-Nagasawa_2024_supp}. For our proposed method, we report results for three implementations: (i) (infeasible) procedure using the true value of $\mathcal{D}_0(\x)$ (referred to as ``oracle''), (ii) implementation using the numerical derivative estimator with a correct specification of $\q$ (referred to as ``known $\q$''), and (iii) ``robust'' implementation only assuming $\q\in\{1, 3\}$ (referred to as ``robust''). 

Table \ref{TABLE: isoreg} presents the numerical results. We report empirical coverage for nominal $95\%$ confidence intervals and their average interval length. For all of the DGPs considered, our proposed bootstrap-assisted inference method leads to confidence intervals with excellent empirical coverage and average interval length. The infeasible ``oracle'' procedure attains empirical coverage very close to the nominal $95\%$, which aligns with our theoretical results, and feasible procedures using the numerical derivative estimators perform almost identical to the infeasible oracle version. 

In DGPs 1 and 2, our procedures outperform both the standard non-parametric bootstrap (which is inconsistent) and the $m$-out-of-$n$ bootstrap (which is consistent) in empirical coverage and average length. In DGP 3, the $m$-out-of-$n$ with the subsample size $m=\lceil n^{1/2}\rceil$ performs comparable to our procedure, but two caveats should be noted. First, the $m$-out-of-$n$ bootstrap performance is sensitive to the choice of the subsample size. Therefore, to operationalize the $m$-out-of-$n$ bootstrap procedure, one needs to develop a reliable procedure to choose the subsample size. Another caveat, arguably more important in this context, is that the $m$-out-of-$n$ bootstrap procedure requires the knowledge of the convergence rate of the estimator. In our simulation, we use the true convergence rate for $m$-out-of-$n$ bootstrap, but in practice, one needs to assume or estimate from data the convergence rate. Since the convergence rate and the limit distribution of the generalized Grenander-type estimator crucially hinge on unknown $\q$, this feature of the $m$-out-of-$n$ bootstrap may be unappealing.
In contrast, our proposed bootstrap-based procedure only requires specifying an upper bound on $\q$ and it automatically adapts to the unknown convergence rate.

The plug-in method performs on par with our method, which is not unexpected because (i) the covariance kernel in this setting takes a relatively simple form and estimating it is not challenging with sample size $n=1,000$ and (ii) we use the true $\q$ to simulate the limit law. In other examples where covariates are present, estimating covariance kernels typically involves nonparametric estimation of additional, possibly high-dimensional, nuisance functions. Our proposed method reduces the number of nuisance parameters that need to be estimated. Moreover, the good performance of the plug-in method for DGP 3 crucially depends on correctly specifying $\q$, whereas our ``robust'' procedure only requires specifying an upper bound on $\q$, and this is another desirable feature of our proposed method.

\begin{table}\renewcommand{\arraystretch}{1.2}
	\caption{Simulations, Monotone Regression Estimator, 95\% Confidence Intervals.}
	{\resizebox{\columnwidth}{!}{ \label{TABLE: isoreg}\input{table_isoreg.txt} } }%\footnotesize\hspace{.5in} 
	\raggedright Notes:
	(i) Panel \textbf{Standard} refers to standard non-parametric bootstrap, Panel \textbf{m-out-of-n} refers to $m$-out-of-$n$ non-parametric bootstrap with subsample $m$, Panel \textbf{Plug-in} refers to the plug-in method, Panel \textbf{Reshaped} refers to our proposed bootstrap-assisted procedure. \newline
	(ii) Columns ``$\tilde{\mathcal{D}}_{1,n}$'' and ``$\tilde{\mathcal{D}}_{3,n}$'' report the averages of the estimated $\mathcal{D}_{1},\mathcal{D}_3$ across simulations, and Columns ``Coverage'' and ``Length'' report empirical coverage and average length of bootstrap-based $95\%$ percentile confidence intervals, respectively.\newline
	(iii) ``Oracle'' corresponds to the infeasible version of our proposed procedure using the true value of $\mathcal{D}_\q$, ``ND known $\q$'' corresponds to our proposed procedure using the bias-reduced numerical derivative estimator with a correct specification of $\q$, and ``ND robust'' corresponds to our proposed procedure only assuming $\q\in \{1,3\}$. The step size choice for the numerical derivative estimator is described in the supplemental appendix.\newline 
	(iv) The sample size is $1,000$, the number of bootstrap iterations is $2,000$, and the number of Monte Carlo simulations is $4,000$.
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Appendix                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{appendix}

\section{Technical Results and Omitted Details}\label{[Section] Appendix - Technical Results}

This appendix collects several technical results and details omitted from the main paper to improve exposition. First, in Section \ref{[Subsection] Appendix - Generalized Switch Relation} we present a corrected version of the generalization of the switch relation stated by \citet[Supplement]{Westling-Carone_2020_AoS}. Second, in Section \ref{[Subsection] Appendix - Continuity of Limiting Distribution} we present a lemma that can be used to establish continuity of the cdf of the maximizer of a Gaussian process whose covariance kernel is that of two-sided Brownian motion. Both lemmas are used in the proof of Theorem \ref{[Theorem] Main result} and may be of independent interest as well. Third, in Section \ref{[Subsection] Appendix - Bootstrap Inconsistency} we establish inconsistency of the plug-in nonparametric bootstrap for a large class of Generalized Grenander-type Estimators, following early work by \cite{Kosorok_2008_BookCh} and \cite{Sen-Banerjee-Woodroofe_2010_AoS}. Finally, in Section \ref{[Subsection] Appendix - Omitted Formulas} we report omitted formulas from Section \ref{[Subsection] Heuristics}.  

\subsection{Generalized Switch Relation}\label{[Subsection] Appendix - Generalized Switch Relation}
For a real-valued function $f$ defined on a set $X \subset \R$, $\argmax_{x \in X} \{f(x)\}$ denotes the (possibly empty) set of maximizers of $f$ over $X$. If $\argmax_{x \in X} \{f(x)\}$ is non-empty and contains a largest element, we denote that element by $\max \argmax_{x \in X} \{f(x)\}$.

In their analysis of generalized Grenander-type estimators, \cite{Westling-Carone_2020_AoS} relied on a generalization of the \textit{switch relation} \citep{Groeneboom_1985_BookCh}. Their generalized switch relation is given in Lemma 1 of \citet[Supplement]{Westling-Carone_2020_AoS}. For the purposes of comparing it with Lemma \ref{[Lemma] Generalized Switch Relation} below, it is convenient to restate that lemma as follows:

\begin{description}
\item[Statement GSR \citep{Westling-Carone_2020_AoS}.] \emph{Let $\Phi$ and $\Gamma$ be real-valued functions defined on an interval $I \subseteq \R$ and suppose that $\Phi$ is non-decreasing and right-continuous. Fix $l,u$ in $\Phi(I)$ with $l < u$ and let $\theta = \partial_- \GCM_{[l,u]}(\Gamma \circ \Phi^-) \circ \Phi$. If $I$ is closed, $\Phi(I) \subseteq [l,u]$, and if $\Gamma$ and $\Gamma \circ \Phi^-$ are lower semi-continuous, then}
\begin{equation}\label{Westling and Carone Lemma 1}
\theta(\x) > t \quad \iff \quad \sup \argmax_{x \in \Phi^-([l,u])} \{t \Phi(x) - \Gamma(x)\} < \Phi^-(\Phi(\x))
\end{equation}
\emph{for any $t \in \R$ and any $\x \in I$ with $\Phi(\x) \in (l,u)$}.
\end{description}

The main purpose of the following example is to show that without further restrictions, the $\argmax$ in \eqref{Westling and Carone Lemma 1} can be empty and the relation \eqref{Westling and Carone Lemma 1} can be violated (if we interpret $\sup \argmax_{x \in X} \{f(x)\}$ as $\inf X$ when $\argmax_{x \in X} \{f(x)\}$ is empty). In other words, Statement GSR can fail to hold.

\begin{example}
Let $I = [l,u] = [0,1], \Gamma(x) = \gamma x$ (for some $\gamma \in \R$), and let
\begin{equation*}
\Phi(x)=
\begin{cases}
x & \text{if } 0 \leq x < 1/2\\
1 & \text{if } 1/2 \leq x \leq 1.
\end{cases}
\end{equation*}
Then
\begin{equation*}
t \Phi(x) - \Gamma(x) =
\begin{cases}
(t - \gamma) x & \text{if } 0 \leq x < 1/2\\
t - \gamma/2 & \text{if } 1/2 \leq x \leq 1,
\end{cases}
\end{equation*}
so $\argmax_{x \in \Phi^-([l,u])}\{t \Phi(x) - \Gamma(x)\}$ is empty when $0 > t > \gamma$.
	
In particular, if $0 > t > \gamma$ and if $\x \in (0,1/2)$, then
\[\sup\argmax_{x \in \Phi^-([l,u])}\{t \Phi(x) - \Gamma(x)\} = 0 < \x = \Phi^-(\Phi(\x)),\]
whereas $\theta(\x) = \partial_- \GCM_{[l,u]} (\Gamma \circ \Phi^-) \circ \Phi(\x) = \gamma < t,$ so \eqref{Westling and Carone Lemma 1} is violated.
	
For future reference, we note that $\Phi(I) = \Phi(I) \cap [l,u] = [0,1/2) \cup \{1\}$ in this example.
\end{example}

The problems highlighted by the example are attributable to the fact that $\Phi(I) \cap [l,u]$ is not closed. Fortunately, it turns out that one can obtain a result in the spirit of \eqref{Westling and Carone Lemma 1} as long as $\Phi(I) \cap [l,u]$ is closed. The following lemma gives a precise statement.

\begin{lemma}[Generalized Switch Relation]\label{[Lemma] Generalized Switch Relation} Let $\Phi$ and $\Gamma$ be real-valued functions defined on an interval $I \subseteq \R$ and suppose that $\Phi$ is non-decreasing and right-continuous. Fix $l,u \in \Phi(I)$ with $l < u$ and let $\theta = \partial_- \GCM_{[l,u]} (\Gamma \circ \Phi^-) \circ \Phi$. If $\Phi(I) \cap [l,u]$ is closed, then
\begin{equation}\label{Generalized Switch Relation}
\theta(\x) >t \quad \iff \quad \max \argmax_{x \in \Phi^-([l,u])}\{t\Phi(x) - \LSC_{\Phi}({\Gamma})(x)\} < \Phi^-(\Phi(\x))
\end{equation}
for any $t \in \R$ and any $\x \in I$ with $\Phi(\x) \in (l,u)$, where
\[\LSC_{\Phi} ({\Gamma})(x) = \LSC (\Gamma \circ \Phi^-) \circ \Phi(x),\]
and where $\LSC (\cdot)$ denotes the greatest lower semi-continuous minorant.
\end{lemma}

The assumptions of the lemma seem mild. In particular, the assumption that $\Phi(I) \cap [l,u]$ is closed is satisfied not only in the examples of Section \ref{[Section] Examples}, but in all examples of which we are aware. 

The lemma does not assume lower semi-continuity of $\Gamma$ or $\Gamma \circ \Phi^-$. Instead, the conclusion involves the function $\LSC_{\Phi} ({\Gamma})$. As the notation suggests, $\LSC_{\Phi} (\cdot) = \LSC (\cdot)$ when $\Phi$ is the identity. More generally, $\LSC_{\Phi}(\cdot) = \LSC (\cdot)$ when $\Phi$ is continuous and strictly increasing and whether or not these properties hold it follows from Lemma 4.3 of \cite{vanderVaart-vanderLaan_2006_IJB} that $\LSC_{\Phi}({\Gamma})$ admits the following characterization:
\[\LSC_{\Phi} ({\Gamma})(x) = \liminf_{y \to \Phi(x)} (\Gamma \circ \Phi^-) (y).\]

\subsection{Continuity of argmax}\label{[Subsection] Appendix - Continuity of Limiting Distribution}

Let $\{\G(v) : v \in \R\}$ be a Gaussian process with $\E[\G(v)]=\mu(v)$ and $\Cov[\G(v),\G(u)]=C(|v|\wedge |u|)\1(\sign(v)=\sign(u))$ for some $C>0$, i.e.\ $\G$ is a scalar multiple of two-sided Brownian motion with mean shift. 
Under conditions on $\mu$ stated below, there exists a unique maximizer of $\G(v)$ over $v \in \R$ with probability one \citep{Kim-Pollard_1990_AoS}. We complement this known fact with a result showing that the cdf of $\argmax_{v \in \R} \{\G(v)\}$ is continuous.

\begin{lemma}\label{[Lemma] Continuity of argmax}
Suppose that for each $v \in \R$, $\lim_{\delta \downarrow 0}[\mu(v + \delta) -\mu(v)] / \sqrt{\delta} = 0$ and that $\limsup_{|v| \to \infty} \mu(v) / |v|^{c} = - \infty $ for some $c > 1/2$. Then $x \mapsto \P[\argmax_{v \in \R} \{\G(v)\} \leq x]$ is continuous.
\end{lemma}

Under Assumptions \ref{Assumption A} and \ref{Assumption B} of the main text, $-\mathcal{M}_\x^\q + t \mathcal{L}_\x$ satisfies the hypothesis of Lemma \ref{[Lemma] Continuity of argmax} for any $t \in \R$. It therefore follows from the lemma that the function
\[ x \mapsto \P \left[\argmin_{v \in \R} \{\mathcal{G}_\x(v) + \mathcal{M}_\x^\q(v) - t\mathcal{L}_\x(v)\} \geq x \right]\]
is continuous at $x = 0$ for any $t \in \R$. We utilize that fact in our proof of \eqref{[Asymptotics] thetahat} and note in passing that most of the existing literature on monotone function estimators seems to implicitly utilize a similar continuity result.

\subsection{Bootstrap Inconsistency}\label{[Subsection] Appendix - Bootstrap Inconsistency}

Employing the exchangeable bootstrap setup introduced in Section \ref{[Subsection] Bootstrapping}, we show the inconsistency of the plug-in bootstrap-based distributional approximation. More precisely, we consider the ``na\"ive'' (plug-in) bootstrap estimator
\[\widehat{\theta}_n^* (\x) = \partial_- \GCM_{[0,\widehat{u}_n^*]} (\widehat{\Gamma}_n^* \circ \widehat{\Phi}_n^{*-}) \circ \widehat{\Phi}_n^* (\x)\]
with
\[\widehat{\Gamma}_n^*(x) = \frac{1}{n} \sum_{i = 1}^n W_{i,n} \widehat{\gamma}_n(x;\mathbf{Z}_i), \qquad \widehat{\Phi}_n^*(x) = \frac{1}{n}\sum_{i=1}^n W_{i,n}\widehat{\phi}_n(x;\mathbf{Z}_i),\]
and establish the following negative result.

\begin{theorem}\label{[Theorem]: Bootstrap inconsistency}
Suppose Assumptions \ref{Assumption A}, \ref{Assumption B: uhat}-\ref{Assumption B: jump of Phi hat}, and \ref{Assumption E} hold. Then
\[\sup_{t \in \R}\left| \P_n^* \left[\widehat{\theta}_n^*(\x) - \widehat{\theta}_n(\x) \leq t\right] - \P \left[\widehat{\theta}_n(\x) - \theta_0(\x) \leq t\right] \right| \not = o_\P(1).\]
\end{theorem}

Theorem \ref{[Theorem]: Bootstrap inconsistency} accommodates exchangeable bootstrap schemes and a wide class of generalized Grenander-type estimators and contains as a special case the the well-known fact that the nonparametric bootstrap approximation to the distribution of the classical Grenander estimator is inconsistent \cite[e.g.,][]{Kosorok_2008_BookCh,Sen-Banerjee-Woodroofe_2010_AoS}.

\subsection{Omitted Formulas}\label{[Subsection] Appendix - Omitted Formulas}

The objects $\widehat{V}_{\x,n}^\q,M_{\x,n}^\q,\widehat{L}_{\x,n}^\q$, and $\widehat{Z}_{\x,n}^\q$ appearing in Section \ref{[Subsection] Heuristics} are defined as follows:
\[\widehat{V}_{\x,n}^\q = \{a_n (x-\x): x \in \widehat{\Phi}_n^-([0,\widehat{u}^*_n])\},\]
\[M_{\x,n}^\q(v) = \sqrt{n a_n} [\Gamma_0(\x + v a_n^{-1}) - \Gamma_0(\x)]-\theta_0(\x)\sqrt{na_n}[\Phi_0(\x+va_n^{-1})-\Phi_0(\x)],\]
and
\[\widehat{L}_{\x,n}^\q(v) = a_n [\widehat{\Phi}_n(\x + v a_n^{-1}) -\widehat{\Phi}_n(\x)], \qquad \widehat{Z}_{\x,n}^\q = a_n [(\widehat{\Phi}_n^- \circ \widehat{\Phi}_n)(\x) - \x].\]

Similarly, the bootstrap analogs of $\widehat{V}_{\x,n}^\q,M_{\x,n}^\q,\widehat{L}_{\x,n}^\q$, and $\widehat{Z}_{\x,n}^\q$ are given by
\[\widehat{V}_{\x,n}^{\q,*} = \{a_n (x-\x): x \in \widehat{\Phi}_n^{*-}([0,\widehat{u}^*_n])\},\]
\[\widehat{M}_{\x,n}^\q(v) = \sqrt{n a_n} [\widehat{\Gamma}_n(\x + v a_n^{-1}) - \widehat{\Gamma}_n(\x)] - \widehat{\theta}_n(\x) \sqrt{n a_n} [\widehat{\Phi}_n(\x + v a_n^{-1}) - \widehat{\Phi}_n(\x)],\]
and
\[\widehat{L}_{\x,n}^{\q,*}(v) = a_n [\widehat{\Phi}_n^*(\x + v a_n^{-1}) - \widehat{\Phi}_n^*(\x)], \qquad \widehat{Z}_{\x,n}^{\q,*} = a_n [(\widehat{\Phi}_n^{*-} \circ \widehat{\Phi}_n^*)(\x) - \x],\]
respectively.
\end{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Support information, if any,             %%
%% should be provided in the                %%
%% Acknowledgements section.                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{acks}[Acknowledgments]
We are grateful to Boris Hanin, Jason Klusowski, Whitney Newey, and seminar participants at various institutions for their comments. We specially thank Marco Carone and Ted Westling for insightful discussions. The co-Editor, an Associate Editor, and two reviewers offered detailed suggestions that improved our paper.
\end{acks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Funding information, if any,             %%
%% should be provided in the                %%
%% funding section.                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{funding}
Cattaneo gratefully acknowledges financial support from the National Science Foundation through grants SES-1947805 and DMS-2210561, and Jansson gratefully acknowledges financial support from the National Science Foundation through grant SES-1947662.
\end{funding}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Supplementary Material, including data   %%
%% sets and code, should be provided in     %%
%% {supplement} environment with title      %%
%% and short description. It cannot be      %%
%% available exclusively as external link.  %%
%% All Supplementary Material must be       %%
%% available to the reader on Project       %%
%% Euclid with the published article.       %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{supplement}
\stitle{Supplement to ``Bootstrap-Assisted Infernce for Generalized Grenander-Type Estimators.''}
\sdescription{The supplementary material contains proofs of Theorems \ref{[Theorem] Main result} and \ref{[Theorem]: Bootstrap inconsistency}, Corollaries \ref{[Corollary] monotone density without censoring}-\ref{[Corollary] monotone regression with covariates}, and Lemmas \ref{[Lemma] Mean function estimation with known q}-\ref{[Lemma] Bootstrapping} and \ref{[Lemma] Generalized Switch Relation}-\ref{[Lemma] Continuity of argmax}. It discusses additional examples that our main results cover.}
\end{supplement}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  imsart-???.bst  will be used to                        %%
%%  create a .BBL file for submission.                     %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%  MR numbers will be added by VTeX.                      %%
%%                                                         %%
%%  Use \cite{...} to cite references in text.             %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% if your bibliography is in bibtex format, uncomment commands:
\bibliographystyle{imsart-nameyear.bst} % Style BST file (imsart-number.bst or imsart-nameyear.bst)
\bibliography{CJN_2024_AOS--bib}

%% or include bibliography directly:
% \begin{thebibliography}{}
% \bibitem{b1}
% \end{thebibliography}

\end{document}