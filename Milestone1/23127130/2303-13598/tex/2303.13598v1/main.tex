\documentclass[11pt]{article}
\usepackage{amsfonts,amssymb,amsmath,amsthm,amscd,bbm,enumitem}
\usepackage[mathscr]{euscript} %use for \N for Normal distribution, closer to the original mathcal
\usepackage[round]{natbib}
\usepackage[onehalfspacing]{setspace}
\usepackage{color,graphicx,caption,subcaption,hyperref}
\hypersetup{pdfborder = {0 0 0},colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue}
\usepackage[margin=1in]{geometry}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{apptools}
\AtAppendix{\counterwithin{lemma}{section}}
\AtAppendix{\counterwithin{assumption}{section}}

\newtheorem{lemma}{Lemma}
\newtheorem{statement}{Statement}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}
\renewcommand\theassumption{\Alph{assumption}}

%%%%%%%% MATH OPERATORS
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\tr}{tr}

%%%%%%%% BASIC MATH NOTATION
\newcommand{\1}{\mathbbm{1}}
\renewcommand{\P}{\mathbbm{P}}
\newcommand{\E}{\mathbbm{E}}
\newcommand{\V}{\mathbbm{V}}
\newcommand{\cov}{\mathbbm{C}\mathrm{ov}}
\newcommand{\G}{\mathbbm{G}}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

%%%%%%% NOTATION
\newcommand{\x}{\mathsf{x}}
\newcommand{\q}{\mathfrak{q}}
\newcommand{\s}{\mathfrak{s}}
\newcommand{\I}{\mathbbm{1}}
\newcommand{\T}{\top}
\newcommand{\GCM}{\mathsf{GCM}}
\newcommand{\LSC}{\mathsf{LSC}}
\newcommand{\lsc}{\mathsf{lsc}}
\newcommand{\conv}{\mathsf{conv}}

\begin{document}

\title{Bootstrap-Assisted Inference for Generalized Grenander-type Estimators\thanks{We are grateful to Boris Hanin, Jason Klusowski, Whitney Newey, and seminar participants at various institutions for their comments. We specially thank Marco Carone and Ted Westling for insightful discussions. Cattaneo gratefully acknowledges financial support from the National Science Foundation through grant SES-1947805, and Jansson gratefully acknowledges financial support from the National Science Foundation through grant SES-1947662.}}

\author{
    Matias D. Cattaneo\footnote{Department of Operations Research and Financial Engineering, Princeton University.}\and
    Michael Jansson\footnote{Department of Economics, University of California at Berkeley.}\and
    Kenichi Nagasawa\footnote{Department of Economics, University of Warwick.}
}
\maketitle

\setcounter{page}{0}\thispagestyle{empty}

\begin{abstract}
    \citet{Westling-Carone_2020_AoS} proposed a framework for studying the large sample distributional properties of generalized Grenander-type estimators, a versatile class of nonparametric estimators of monotone functions. The limiting distribution of those estimators is representable as the left derivative of the greatest convex minorant of a Gaussian process whose covariance kernel can be complicated and whose monomial mean can be of unknown order (when the degree of flatness of the function of interest is unknown). The standard nonparametric bootstrap is unable to consistently approximate the large sample distribution of the generalized Grenander-type estimators even if the monomial order of the mean is known, making statistical inference a challenging endeavour in applications. To address this inferential problem, we present a bootstrap-assisted inference procedure for generalized Grenander-type estimators. The procedure relies on a carefully crafted, yet automatic, transformation of the estimator. Moreover, our proposed method can be made ``flatness robust" in the sense that it can be made adaptive to the (possibly unknown) degree of flatness of the function of interest. The method requires only the consistent estimation of a single scalar quantity, for which we propose an automatic procedure based on numerical derivative estimation and the generalized jackknife. Under random sampling, our inference method can be implemented using a computationally attractive exchangeable bootstrap procedure. We illustrate our methods with examples and we also provide a small simulation study. The development of formal results is made possible by some technical results that may be of independent interest.
\end{abstract}

\textbf{Keywords}: Monotone estimation, bootstrapping, robust inference.

%\doublespacing
\clearpage
%\setlength{\abovedisplayskip}{5pt}
%\setlength{\belowdisplayskip}{5pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Monotone function estimators have received renewed attention in statistics, biostatistics, econometrics, machine learning, and other data science disciplines. See \citet{Groeneboom-Jongbloed_2014_Book,Groeneboom-Jongbloed_2018_SS} for a textbook introduction and a review article, respectively, the latter being published in a special issue devoted to nonparametric inference under shape constraints. More recently, \citet{Westling-Carone_2020_AoS} expanded the scope and applicability of monotone function estimators by embedding many such estimators in a unified framework of so-called generalized Grenander-type estimators. Estimation problems covered by \citeauthor{Westling-Carone_2020_AoS}'s \citeyearpar{Westling-Carone_2020_AoS} general theory contains many practically relevant examples such as monotone density, regression and hazard estimation, possibly with censoring and/or covariate adjustment.

The large sample theory developed by \citet{Westling-Carone_2020_AoS} offers a general distributional approximation involving the left derivative of the Greatest Convex Minorant (GCM) of a Gaussian process whose mean and covariance kernel depend on unknown functions. Furthermore, both the convergence rate of the estimator and the shape of the mean appearing in the representation of the limiting distribution depend on whether the unknown function of interest exhibits certain degeneracies. For these reasons, the large sample distributional approximation for generalized Grenander-type estimators can be difficult to employ in practice for inference purposes. In their concluding remarks, \citet{Westling-Carone_2020_AoS} recognize these limitations and pose the question of whether it would be possible to employ bootstrap-assisted methods to conduct automatic/robust statistical inference within their framework.

As is well documented, the standard nonparametric bootstrap does not provide a valid distributional approximation for the generalized Grenander-type estimators \citep*{Kosorok_2008_BookCh,Sen-Banerjee-Woodroofe_2010_AoS}. This fact has led scholars to rely on other bootstrap schemes such as subsampling \citep{Politis-Romano_1994_AoS}, the smoothed bootstrap \citep{Kosorok_2008_BookCh,Sen-Banerjee-Woodroofe_2010_AoS}, the $m$-out-of-$n$ bootstrap \citep{Sen-Banerjee-Woodroofe_2010_AoS,Lee-Yang_2020_AoS}, or the numerical bootstrap \citep{Hong-Li_2020_AoS}. See also \citet{Cavaliere-Georgiev_2020_ECMA}, and references therein, for some related recent results. Those approaches could in principle be used to construct bootstrap-based inference methods for (some members of the class of) generalized Grenander-type estimators, but they all would require employing specific regularized multidimensional bootstrap distributions or related quantities involving multiple smoothing and tuning parameters, rendering those approaches potentially difficult to implement in practice. Furthermore, those methods would not be robust to unknown degeneracies determining the convergence rate and shape of the limiting distribution without additional modifications. For example, subsampling methods require knowledge of the precise convergence rate of the statistic, or estimation thereof, as a preliminary step \citep*{Politis-Romano-Wolf_1999_Book}.

We complement existing methods by introducing a novel bootstrap-assisted inference approach that restores validity of bootstrap methods by reshaping of the ingredients of the generalized Grenander-type estimator. Our approach is motivated by a constructive interpretation of the source of the failure of the nonparametric bootstrap. As a by-product, our interpretation explicitly isolates the role of unknown degeneracies determining the precise form of the limiting distribution, a feature of the interpretation which allow us to develop an automatic inference method that is robust to such degeneracies, ultimately resulting in a more robust bootstrap-assisted inference approach. In the case of random sampling, we show that our method can be implemented using a computationally attractive exchangeable bootstrap procedure. For completeness, we also discuss implementation issues, offering a fully automatic (i.e., data-driven) valid inference method for generalized Grenander-type estimators. Some of the ideas underlying our approach are similar to ideas used in \citet*{Cattaneo-Jansson-Nagasawa_2020_ECMA}, where we introduced a bootstrap-based distributional approximation for $M$-estimators with possibly non-standard \citet{Chernoff_1964_AISM}-type asymptotic distributions \citep{Kim-Pollard_1990_AoS,Seo-Otsu_2018_AoS}. Generalized Grenander-type estimators are not $M$-estimators, however, and as further explained in the next paragraph the analysis of generalized Grenander-type estimators turns out to necessitate the development of technical tools that play no role in the analysis of $M$-estimators.

Although valid distributional approximations for monotone estimators can be obtained in a variety of ways \citep{Groeneboom-Jongbloed_2014_Book}, by far the most common approach is to employ the so-called \textit{switch relation} \citep{Groeneboom_1985_BookCh} to re-express the cumulative distribution function (cdf) of the suitably normalized monotone estimator in terms of a probability statement about the maximizer of a certain stochastic process whose large sample properties can in turn be analyzed by employing standard empirical process methods \citep{vanderVaart-Wellner_1996_Book}. From a technical perspective, this approach requires (at least) two ingredients in order to be successful, namely results establishing (i) validity of the switch relation and (ii) continuity of the limiting cdf of the maximizer of a stochastic process. In the process of developing our main results, we shed new light on both (i) and (ii). First, we show by example that \citeauthor{Westling-Carone_2020_AoS}'s \citeyearpar[Supplement]{Westling-Carone_2020_AoS} generalization of the switch relation is incomplete as stated and then propose a modification. Our modification of \citeauthor{Westling-Carone_2020_AoS}'s \citeyearpar[Supplement]{Westling-Carone_2020_AoS} Lemma 1 shows that the conclusion of that lemma is valid once two additional assumptions are made.
The additional assumptions seem very mild, being satisfied in all examples considered by \citet{Westling-Carone_2020_AoS} and all other examples of which we are aware, including some new examples we consider in this paper. Second, we present a lemma establishing continuity of the cdf of the maximizer of a Gaussian process, a result which can in turn be used to establish continuity of the cdf of the suitably normalized generalized Grenander-type estimator. Interestingly, although these continuity properties are important when deriving limiting distributions with the help of the switch relation and justifying bootstrap-type inference procedures, respectively, it would appear that explicit statements of them are unavailable in the existing literature. (A prominent exception is the one where the limiting distribution is a scaled Chernoff distribution, which is known to be absolutely continuous.) For further details on (i) and (ii), see Appendix \ref{[Section] Appendix - Technical Results}.

In the remainder of this introductory section we outline key notation and definitions used throughout the paper. Section \ref{[Section] Setup} then recalls the setup of \citet{Westling-Carone_2020_AoS} and presents a version of their main distributional approximation result for generalized Grenander-type estimators. Section \ref{[Section] Bootstrap-based Distributional Approximation} contains our main results about bootstrap-assisted distributional approximations, while Section \ref{[Section] Implementation} discusses implementation issues, including tuning parameter selection and a computationally attractive weighted bootstrap procedure. Section \ref{[Section] Examples} illustrates our general theory by means of prominent examples, while Section \ref{[Section] Simulations} reports numerical results from a small-scale simulation experiment. Appendix \ref{[Section] Appendix - Technical Results} reports the two technical results alluded to in the previous paragraph. All proofs and other technical derivations, as well as regularity conditions for the specific examples, are given in the supplemental appendix.

\subsection{Notation and Definitions}
For a function $f$ defined on an interval $I\subseteq\mathbb{R}$, $\GCM_I(f)$ denotes its greatest convex minorant (on $I$) and if $f$ is non-decreasing, then $f^-$ denotes its generalized inverse (on the convex hull of $f(I)$); that is, $f^-(x) = \inf\{u\in I : f(u) \geq x\}$, where the dependence of $f^-$ on $I$ has been suppressed. Assuming the relevant derivatives exist, $\partial^q$ denotes the $q$th partial derivative (operator) and $\partial_-$ denotes the left derivative (operator). In addition, $f\circ g$ denotes the composition of $f$ and $g$; that is, $(f\circ g)(x) = f(g(x))$.

Limits are taken as $n\to\infty$, unless otherwise stated. For two (possibly) random sequences $\{a_n\}$ and $\{b_n\}$, %$a_n\lesssim b_n$ denotes $\limsup_n|a_n/b_n|$ is finite, 
$a_n=O_\P(b_n)$ is shorthand for $\limsup_{\epsilon\to\infty}\limsup_n\P[|a_n/b_n|\geq\epsilon]=0$, $a_n=o_\P(b_n)$ is shorthand for $\limsup_{\epsilon\to 0}\limsup_n\P[|a_n/b_n|\geq\epsilon]=0$, and the subscript ``$\P$'' on ``$O$'' and ``$o$'' is often omitted when $\{a_n\}$ and $\{b_n\}$ are non-random. We use $\rightsquigarrow$ to denote weak convergence, where, for a stochastic process indexed by $\mathbb{R}$, convergence is in the topology of uniform convergence on compacta. When analyzing the bootstrap, $\P^*_n$ denotes the probability measure under the bootstrap distribution conditional on the original data and $\rightsquigarrow_{\P}$ denotes weak convergence in probability conditionally on the original data. For more details, see \citet{vanderVaart-Wellner_1996_Book}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SETUP
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Setup}\label{[Section] Setup}

Our setup is that of \citet{Westling-Carone_2020_AoS}. The goal is to conduct inference on $\theta_0(\x)$, where, for some interval $I\subseteq\mathbb{R}$, $\theta_0$ is non-decreasing on $I$ and $\x$ is an interior point of $I$. Assuming it is well defined, the
function $\Theta_0$ given by
\[\Theta_0(x)=\int_{\inf I}^{x}\theta_0(v)dv\]
is convex on $I$ and therefore enjoys the property that if $\theta_0$ is left continuous at $\x$, then
\begin{equation}\label{[Representation] theta0}
    \theta_0(\x)=\partial_{-}\GCM_{I}(\Theta_0)(\x).
\end{equation}
An estimator of $\theta_0(\x)$ obtained by replacing $\Theta_0$ and $I$ in the preceding display with estimators is said to be of the Grenander-type, a canonical example of this class of estimators being the celebrated \citet{Grenander_1956_SAJ} estimator of a non-decreasing density.

\begin{example}[Monotone Density Estimation] \label{[Example] Monotone Density Estimation}
    Suppose $X_1,\ldots,X_n$ are i.i.d. copies of a continuously distributed non-negative random variable $X$ whose density $f_0$ is non-decreasing on $[0,u_0],$ where $u_0$ is (possibly) unknown. For $\x\in(0,u_0)$, the \citet{Grenander_1956_SAJ} estimator of $f_0(\x)$ is
    \[\widehat{f}_n(\x)=\partial_{-}\GCM_{[0,\widehat{u}_n]}(\widehat{F}_n)(\x),\]
    where $\widehat{u}_n=\max(\max_{1\leq i\leq n}X_{i},\x)$ and where $\widehat{F}_n(x)=n^{-1}\sum_{i=1}^{n}\I(X_{i}\leq x)$ is the empirical cdf. Section \ref{[Subsection] Examples: Monotone Density Estimation} presents novel bootstrap-based inference methods for monotone
    density estimators possibly allowing for censoring and/or covariate adjustment \citep[e.g.,][]{vanderlaan-Robins_2003_Book}.
\end{example}

To define the class of generalized Grenander-type estimators, let $\psi_0=\theta_0\circ\Phi_0^{-}$, where $\Phi_0$ is non-negative, non-decreasing, and right continuous on $I$. Defining
\[\Gamma_0=\Psi_0\circ\Phi_0,\qquad\Psi_0(x)=\int_0^{x}\psi_0(v)dv,\]
and assuming that $\Phi_0(x)<\Phi_0(\x)<u_0$ for every $x<\x$, we have
\begin{equation*}
    \theta_0(\x)=\partial_{-}\GCM_{[0,u_0]}(\Gamma_0\circ\Phi_0^{-})\circ\Phi_0(\x)\label{[Representation] theta}
\end{equation*}
whenever $\theta_0$ is left continuous at $\x$. In the terminology of \citet{Westling-Carone_2020_AoS}, an estimator of $\theta_0(\x)$ is of the Generalized Grenander-type if it is obtained by replacing $\Gamma_0$, $\Phi_0$, and $u_0$ in the preceding display with estimators $\widehat{\Gamma}_n,\widehat{\Phi}_n$, and $\widehat{u}_n$ (say); that is, an estimator of Generalized Grenander-type is of the form
\begin{equation*}
    \widehat{\theta}_n(\x)=\partial_{-}\GCM_{[0,\widehat{u}_n]}(\widehat{\Gamma}_n\circ\widehat{\Phi}_n^{-})\circ\widehat{\Phi}_n(\x).
\end{equation*}
Of course, Grenander-type estimators are of generalized Grenander-type (with $\widehat{\Phi}_n$ equal to the identity mapping) whenever the associated estimator of $I$ is of the form $[0,\widehat{u}_n]$, but the class of generalized Grenander-type estimators contains many important estimators that are not of Grenander-type, a canonical example being the celebrated isotonic regression estimator of
\citet{Brunk_1958_AMS}.

\begin{example}[Monotone Regression Estimation] \label{[Example] Monotone Regression Estimation}
Suppose $(Y_1,X_1),\ldots,(Y_n,X_n)$ are i.i.d. copies of $(Y,X)$, where $X$ is a continuously distributed random variable and where the regression function $\mu_0(x)=\E(Y|X=x)$ is non-decreasing. For $\x$ in the interior of the support of $X$, the \citet{Brunk_1958_AMS} estimator of $\mu_0(\x)$ is
\begin{equation*}
    \widehat{\mu}_n(\x)=\partial_{-}\GCM_{[0,1]}(\widehat{\Gamma}_n\circ\widehat{F}_n^{-})\circ\widehat{F}_n(\x),
\end{equation*}
where $\widehat{\Gamma}_n(x)=n^{-1}\sum_{i=1}^{n}Y_{i}\I(X_{i}\leq x)$ and $\widehat{F}_n(x)=n^{-1}\sum_{i=1}^{n}\I(X_{i}\leq x)$. Section \ref{[Subsection] Examples: Monotone Regression Estimation} presents novel bootstrap-based inference methods for monotone regression estimators possibly allowing for covariate adjustment \citep*[e.g.,][]{Westling-Gilbert-Carone_2020_JRRSB}.
\end{example}

Under regularity conditions, the rate of convergence of the generalized Grenander-type estimator $\widehat{\theta}_n(\x)$ is governed by the flatness of $\theta_0$ around $\x$, as measured by the characteristic exponent
\begin{equation}\label{[Definition] Characteristic exponent}
    \q=\min\{j\in\mathbb{N}:\partial^{j}\theta_0(\x)\neq0\}.
\end{equation}
(When $\theta_0$ is non-decreasing and suitably smooth, $\q$ is necessarily an odd integer.) To be specific, \citet[Theorem 3]{Westling-Carone_2020_AoS} gave conditions under which
\begin{equation}\label{[Asymptotics] thetahat}
    r_n(\widehat{\theta}_n(\x)-\theta_0(\x))\rightsquigarrow\frac{1}{\partial\Phi_0(\x)}\partial_{-}\GCM_{\mathbb{R}}(\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q})(0), \qquad
    r_n=n^{\q/(1+2\q)},
\end{equation}
where $\mathcal{G}_{\x}$ is a zero mean Gaussian process and where $\mathcal{M}_{\x}^{\q}$ is a monomial (of order $\q+1$) given by
\begin{equation}\label{[Definition] M_x,q}
    \mathcal{M}_{\x}^{\q}(v)=\frac{\partial^{\q}\theta_0(\x)\partial\Phi_0(\x)}{(\q+1)!}v^{\q+1}.
\end{equation}
In addition to governing the rate of convergence, the characteristic exponent $\q$ also governs the shape of $\mathcal{M}_{\x}^{\q}$. On the other hand, and as the notation suggests, the covariance kernel of $\mathcal{G}_{\x}$ does not depend on $\q$. If $\q=1$ and if $\mathcal{G}_{\x}$ is a two-sided Brownian motion, then the distribution of $\partial_{-}\GCM_{\mathbb{R}}(\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q})(0)$ is a scaled Chernoff distribution. More generally, the distribution of $\partial_{-}\GCM_{\mathbb{R}}(\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q})(0)$ is a scaled Chernoff-type distribution \citep[in the terminology of][]{Han-Kato_2022_AoAP} whenever $\mathcal{G}_{\x}$ is a two-sided Brownian motion, but for more complicated $\mathcal{G}_{\x}$ the asymptotic distribution of $r_n(\widehat{\theta}_n(\x)-\theta_0(\x))$ need not belong to scale family of distributions.

Among other things, the following assumption guarantees validity of the representation (\ref{[Representation] theta0}) and ensures existence of the right hand side of (\ref{[Definition] M_x,q}).

\begin{assumption}\label{Assumption A}
For some $\delta>0$, some $\s\geq1$, and some $\q\in\mathbb{N}$, the following are satisfied:
\begin{description}
    \item[(A1)] $I\subseteq\mathbb{R}$ is an interval and $I_{\x}^{\delta}=\{x:|x-\x|\leq\delta\}\subseteq I$.

    \item[(A2)] $\theta_0$ is non-decreasing on $I$.

    Also, $\theta_0$ is $\left\lfloor \s\right\rfloor $ times continuously differentiable on $I_{\x}^{\delta}$ with
%    \item[(A2)] $\theta_0:I\to\mathbb{R}$ is non-decreasing. In addition, $\theta_0$ is $\left\lfloor \s\right\rfloor $ times continuously differentiable on $I_{\x}^{\delta}$ with
    \[\sup_{x,x'\in I_{\x}^{\delta}}\frac{|\partial^{\left\lfloor\s\right\rfloor }\theta_0(x)-\partial^{\left\lfloor \s \right\rfloor}\theta_0(x')|}{|x-x'|^{\s-\left\lfloor \s\right\rfloor }}<\infty.
    \]
    In addition, $\mathfrak{q\leq\left\lfloor \s\right\rfloor }$, where $\q$ is the characteristic exponent defined in (\ref{[Definition] Characteristic exponent}).

    \item[(A3)] $\Phi_0$ is non-negative, non-decreasing, and right continuous on $I$.
    
    Also, $\Phi_0$ is $\left\lfloor \s \right\rfloor -\q+1$ times continuously differentiable on $I_{\x}^{\delta}$ with $\partial\Phi_0(\x)\neq0$ and
%    \item[(A3)] $\Phi_0:I\to[0,u_0]$ is non-decreasing and right continuous. In addition, $\Phi_0$ is $\left\lfloor \s \right\rfloor -\q+1$ times continuously differentiable on $I_{\x}^{\delta}$ with $\partial\Phi_0(\x)\neq0$ and
    \[\sup_{x,x'\in I_{\x}^{\delta}}\frac{|\partial^{\left\lfloor \s \right\rfloor-\q+1}\Phi_0(x)-\partial^{\left\lfloor\s\right\rfloor-\q+1}\Phi_0(x')|}{|x-x'|^{\s-\left\lfloor \s\right\rfloor }}<\infty.
    \]
\end{description}
\end{assumption}

\section{Bootstrap-Assisted Distributional Approximation}\label{[Section] Bootstrap-based Distributional Approximation}

Letting $(\widehat{\Gamma}_n^*,\widehat{\Phi}_n^*,\widehat{u}_n^*)$ denote a bootstrap analog of $(\widehat{\Gamma}_n,\widehat{\Phi}_n,\widehat{u}_n)$, the associated bootstrap analog of $\widehat{\theta}_n(\x)$ is
\begin{equation*}
    \widehat{\theta}_n^*(\x)=\partial_{-}\GCM_{[0,\widehat{u}_n^*]}(\widehat{\Gamma}_n^*\circ(\widehat{\Phi}_n^*)^{-})\circ\widehat{\Phi}_n^*(\x).
\end{equation*}

As is well documented \citep[e.g.][]{Kosorok_2008_BookCh,Sen-Banerjee-Woodroofe_2010_AoS}, the following bootstrap analog of (\ref{[Asymptotics] thetahat}) does not (necessarily) hold when $(\widehat{\Gamma}_n^*,\widehat{\Phi}_n^*,\widehat{u}_n^*)$ is obtained by means of the nonparametric bootstrap:
\begin{equation*}
    r_n(\widehat{\theta}_n^*(\x)-\widehat{\theta}_n(\x))\rightsquigarrow_{\P}\frac{1}{\partial\Phi_0(\x)}\partial_{-}\GCM_{\mathbb{R}}(\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q})(0).
\end{equation*}
In other words, the bootstrap is inconsistent in general. It turns out, however, that under plausible conditions on $(\widehat{\Gamma}_n^*,\widehat{\Phi}_n^*,\widehat{u}_n^*)$ a valid bootstrap-based distributional can be obtained by employing
\begin{equation*}
    \widetilde{\theta}_n^*(\x)=\partial_{-}\GCM_{[0,\widehat{u}_n^*]}(\widetilde{\Gamma}_n^*\circ(\widehat{\Phi}_n^*)^{-})\circ\widehat{\Phi}_n^*(\x),
\end{equation*}
where, for some judiciously chosen $\widetilde{M}_{\x,n}$, $\widetilde{\Gamma}_n^*$ is given by
\begin{equation*}
    \widetilde{\Gamma}_n^*(x)=\widehat{\Gamma}_n^*(x)-\widehat{\Gamma}_n(x)+\widehat{\theta}_n(\x)\widehat{\Phi}_n(x)+\widetilde{M}_{\x,n}(x-\x).
\end{equation*}
(As the notation suggests, a suitably re-scaled $\widetilde{M}_{\x,n}$ can be interpreted as an estimator of $\mathcal{M}_{\x}^{\q}$.)

\subsection{Heuristics}\label{[Subsection] Heuristics}

To explain the source of the bootstrap failure and motivate the functional form of $\widetilde{\Gamma}_n^*$, it is useful to begin by sketching the derivation of (\ref{[Asymptotics] thetahat}). Let $a_n=n^{1/(1+2\q)}$ and define
\begin{align*}
    \widehat{G}_{\x,n}^{\q}(v)
    & =\sqrt{na_n}[\widehat{\Gamma}_n(\x+va_n^{-1})-\widehat{\Gamma}_n(\x)-\Gamma_0(\x+va_n^{-1})+\Gamma_0(\x)]\\
    & -\theta_0(\x)\sqrt{na_n}[\widehat{\Phi}_n(\x+va_n^{-1})-\widehat{\Phi}_n(\x)-\Phi_0(\x+va_n^{-1})+\Phi_0(\x)],
\end{align*}
\[M_{\x,n}^{\q}(v)=\sqrt{na_n}[\Gamma_0(\x+va_n^{-1})-\Gamma_0(\x)]-\theta_0(\x)\sqrt{na_n}[\Phi_0(\x+va_n^{-1})-\Phi_0(\x)],
\]
\[\widehat{L}_{\x,n}^{\q}(v)=a_n[\widehat{\Phi}_n(\x+va_n^{-1})-\widehat{\Phi}_n(\x)],
\]
and
\[\widehat{Z}_{\x,n}^{\q}=a_n[\widehat{\Phi}_n^{-}\circ\widehat{\Phi}_n(\x)-\x].
\]
For any $t\in\mathbb{R}$, assuming validity of the so-called (generalized) switch relation, we have
\[\P\left[ r_n(\widehat{\theta}_n(\x)-\theta_0(\x))\leq t\right] =\P\left[ \argmin_{v\in\widehat{V}_{\x,n}^{\q}}\{\widehat{G}_{\x,n}^{\q}(v)+M_{\x,n}^{\q}(v)-t\widehat{L}_{\x,n}^{\q}(v)\}-\widehat{Z}_{\x,n}^{\q}\geq0\right],
\]
where $\widehat{V}_{\x,n}^{\q}=\{a_n(x-\mathsf{x)}:x\in \widehat{\Phi}_n^{-}([0,\widehat{u}_n])\}$.\footnote{In the preceding display, we (tacitly) assume for simplicity that $\widehat{\Gamma}_n$ is lower semi-continuous and that the minimizer is unique. Departures from these assumptions can be handled by replacing $\widehat{\Gamma}_n$ with its greatest lower semi-continuous minorant and by working with the largest minimizer, respectively; for details, see Appendix \ref{[Subsection] Appendix - Generalized Switch Relation}. A similar remark applies to subsequent displays obtained with the help of the switch relation.} Assuming moreover that
\begin{equation}\label{[Asymptotics] Ghat,Lhat,Zhat}        
(\widehat{G}_{\x,n}^{\q},\widehat{L}_{\x,n}^{\q},\widehat{Z}_{\x,n}^{\q})\rightsquigarrow(\mathcal{G}_{\x},\mathcal{L}_{\x},0),\qquad\mathcal{L}_{\x}(v)=v\partial\Phi_0(\x),
\end{equation}
and
\begin{equation}\label{[Asymptotics] M}
M_{\x,n}^{\q}\rightsquigarrow\mathcal{M}_{\x}^{\q},
\end{equation}
it therefore stands to reason that under mild additional conditions we have
\begin{align*}
    \lim_{n\to\infty}\P\left[ r_n(\widehat{\theta}_n(\x)-\theta_0(\x))\leq t\right]
    & =\P\left[\argmin_{v\in\mathbb{R}}\{\mathcal{G}_{\x}(v)+\mathcal{M}_{\x}^{\q}(v)-t\mathcal{L}_{\x}(v)\}\geq0\right]\\
    & =\P\left[\frac{1}{\partial\Phi_0(\x)}\partial_{-}\GCM_{\mathbb{R}}(\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q})(0)\leq t\right],
\end{align*}
the first equality following from the argmax continuous mapping theorem and the second equality being obtained by another application of the switch relation.

Next, let the natural bootstrap analogs of $\widehat{G}_{\x,n}^{\q}$, $M_{\x,n}^{\q}$, $\widehat{L}_{\x,n}^{\q}$, and $\widehat{Z}_{\x,n}^{\q}$ be given by
\begin{align*}
    \widehat{G}_{\x,n}^{\q,*}(v)
    & =\sqrt{na_n}[\widehat{\Gamma}_n^*(\x+va_n^{-1})-\widehat{\Gamma}_n^*(\x)-\widehat{\Gamma}_n(\x+va_n^{-1})+\widehat{\Gamma}_n(\x)]\\
    & -\widehat{\theta}_n(\x)\sqrt{na_n}[\widehat{\Phi}_n^*(\x+va_n^{-1})-\widehat{\Phi}_n^*(\x)-\widehat{\Phi}_n(\x+va_n^{-1})+\widehat{\Phi}_n(\x)],
\end{align*}
\[\widehat{M}_{\x,n}^{\q}(v)=\sqrt{na_n}[\widehat{\Gamma}_n(\x+va_n^{-1})-\widehat{\Gamma}_n(\x)]-\widehat{\theta}_n(\x)\sqrt{na_n}[\widehat{\Phi}_n(\x+va_n^{-1})-\widehat{\Phi}_n(\x)],
\]
\[\widehat{L}_{\x,n}^{\q,*}(v)=a_n[\widehat{\Phi}_n^*(\x+va_n^{-1})-\widehat{\Phi}_n^*(\x)],
\]
and
\[\widehat{Z}_{\x,n\x}^{\q,*}=a_n[(\widehat{\Phi}_n^*)^{-}\circ\widehat{\Phi}_n^*(\x)-\x],
\]
respectively. For every $t\in\mathbb{R}$, it follows from the switch relation that these objects satisfy
\[\P_n^*\left[r_n(\widehat{\theta}_n^*(\x)-\widehat{\theta}_n(\x))\leq t\right] =\P_n^*\left[\argmin_{v\in\widehat{V}_{\x,n}^{\q,*}}\{\widehat{G}_{\x,n}^{\q,*}(v)+\widehat{M}_{\x,n}^{\q}(v)-t\widehat{L}_{\x,n}^{\q,*}(v)\}-\widehat{Z}_{\x,n}^{\q,*}\geq0\right],
\]
where $\widehat{V}_{\x,n}^{\q,*}=\{a_n(x-\x):x\in (\widehat{\Phi}_n^*)^{-}([0,\widehat{u}_n^*])\}$. If (\ref{[Asymptotics] Ghat,Lhat,Zhat}) holds, then the following bootstrap counterpart thereof can also be expected to hold provided that $(\widehat{\Gamma}_n^*,\widehat{\Phi}_n^*,\widehat{u}_n^*)$ is chosen appropriately:
\begin{equation}\label{[Asymptotics] Ghatstar,Lhatstar,Zhatstar}
    (\widehat{G}_{\x,n}^{\q,*},\widehat{L}_{\x,n}^{\q,*},\widehat{Z}_{\x,n}^{\q,*})\rightsquigarrow_{\P}(\mathcal{G}_{\x},\mathcal{L}_{\x},0).
\end{equation}
On the other hand, the process $\widehat{M}_{\x,n}^{\q}$ typically does not inherit the smoothness of $M_{\x,n}^{\q}$ and the bootstrap counterpart of (\ref{[Asymptotics] M}) typically fails \citep[e.g.,][]{Sen-Banerjee-Woodroofe_2010_AoS}, implying in turn that the bootstrap is inconsistent.

By construction, the estimator $\widetilde{\theta}_n^*(\x)$ is similar to $\widehat{\theta}_n^*(\x)$ insofar as it satisfies the following switch relation: For every $t\in\mathbb{R}$,
\[\P_n^*\left[r_n(\widetilde{\theta}_n^*(\x)-\widehat{\theta}_n(\x))\leq t\right] =\P_n^*\left[\argmin_{v\in\widehat{V}_{\x,n}^{\q,*}}\{\widehat{G}_{\x,n}^{\q,*}(v)+\widetilde{M}_{\x,n}^{\q}(v)-t\widehat{L}_{\x,n}^{\q,*}(v)\}-\widehat{Z}_{\x,n}^{\q,*}\geq0\right],
\]
where
\[\widetilde{M}_{\x,n}^{\q}(v)=\sqrt{na_n}\widetilde{M}_{\x,n}(va_n^{-1}).
\]
As a consequence, if (\ref{[Asymptotics] Ghatstar,Lhatstar,Zhatstar}) holds and if $\widetilde{M}_{\x,n}^{\q}\rightsquigarrow_{\P}\mathcal{M}_{\x}^{\q}$, then it stands to reason that under mild additional conditions $\widetilde{\theta}_n^*(\x)$ satisfies the following bootstrap counterpart of (\ref{[Asymptotics] thetahat}):
\begin{equation}\label{[Asymptotics] thetatildestar}
    r_n(\widetilde{\theta}_n^*(\x)-\widehat{\theta}_n(\x))\rightsquigarrow_{\P}\frac{1}{\partial\Phi_0(\x)}\partial_{-}\GCM_{\mathbb{R}}(\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q})(0).
\end{equation}

\subsection{Main result}\label{[Subsection] Main result}

Our heuristic derivation of (\ref{[Asymptotics] thetahat}) can be made rigorous by providing conditions under which four properties hold. First, the switch relation(s) must be valid. Second, the convergence properties (\ref{[Asymptotics] Ghat,Lhat,Zhat}) and (\ref{[Asymptotics] M}) must hold. Third, to use (\ref{[Asymptotics] Ghat,Lhat,Zhat}) and (\ref{[Asymptotics] M}) to obtain the result
\[\argmin_{v\in\widehat{V}_{\x,n}^{\q}}\{\widehat{G}_{\x,n}^{\q}(v)+M_{\x,n}^{\q}(v)-t\widehat{L}_{\x,n}^{\q}(v)\}-\widehat{Z}_{\x,n}^{\q}\rightsquigarrow\argmin_{v\in\mathbb{R}}\{\mathcal{G}_{\x}(v)+\mathcal{M}_{\x}^{\q}(v)-t\mathcal{L}_{\x}(v)\}
\]
with the help of the argmax continuous mapping theorem, tightness of the left hand side in the previous display must hold. Finally, to furthermore obtain the conclusion
\begin{align*}
    & \lim_{n\to\infty}\P\left[\argmin_{v\in\widehat{V}_{\x,n}^{\q}}\{\widehat{G}_{\x,n}^{\q}(v)+M_{\x,n}^{\q}(v)-t\widehat{L}_{\x,n}^{\q}(v)\}-\widehat{Z}_{\x,n}^{\q}\geq0\right]\\
    & =\P\left[\argmin_{v\in\mathbb{R}}\{\mathcal{G}_{\x}(v)+\mathcal{M}_{\x}^{\q}(v)-t\mathcal{L}_{\x}(v)\}\geq0\right],
\end{align*}
the cdf of $\argmin_{v\in\mathbb{R}}\{\mathcal{G}_{\x}(v)+\mathcal{M}_{\x}^{\q}(v)-t\mathcal{L}
_{\x}(v)\}$ must be continuous at zero.

Conditions under which the second and third properties hold can be formulated with the help of well known empirical process results. For concreteness, we base our formulations on \citet{vanderVaart-Wellner_1996_Book}. The first and fourth properties, on the other hand, seem more difficult to verify. Regarding the first property, it turns out that the generalization of the switch relation employed by \citet{Westling-Carone_2020_AoS} requires additional conditions in order to be valid. To address this concern about the generalized switch relation, we present a modification whose assumptions include two conditions not present in Lemma 1 of \citet[Supplement]{Westling-Carone_2020_AoS}. Thankfully, the assumptions in question seem very mild and having made these assumptions we are able to preserve the main implication of Lemma 1 of \citet[Supplement]{Westling-Carone_2020_AoS}. For details, see Lemma \ref{[Lemma] Generalized Switch Relation} in the appendix. In the special case where $\q=1$ and $\mathcal{G}_{\x}$ is a two-sided Brownian motion, the fourth property follows from well known properties of the Chernoff distribution. More generally, however, we are unaware of existing results guaranteeing the requisite continuity property when $\q\neq1$ and/or $\mathcal{G}_{\x}$ is not a two-sided Brownian motion, but fortunately it turns out that the continuity property of interest admits simple sufficient conditions (namely, \textbf{(B4)} in Assumption \ref{Assumption B}). For details, see Lemma \ref{[Lemma] Continuity of argmax} in the appendix.

%Regarding the first property , although various statements of versions of the switch relation can be found in the literature, we are unaware of a version of the result which can handle reasonably general examples of $\widehat{\Phi}_n$ and $\widehat{\Phi}_n^*$. In particular, it turns out that the version of the switch relation employed by \cite{Westling-Carone_2020_AoS} is incorrect as stated, but thankfully it turns out that a seemingly mild extra condition (namely, closedness of the range of $\widehat{\Phi}_n$ and $\widehat{\Phi}_n^*$) is enough to establish/restore validity of the switch relation stated in Lemma 1 in the supplemental appendix of \cite{Westling-Carone_2020_AoS}. For details, see Lemma \ref{[Lemma] Generalized Switching Lemma} in the appendix. In the special case where $\q=1$ and $\mathcal{G}_{\x}$ is a two-sided Brownian motion, the fourth property follows from well known properties of the Chernoff distribution. More generally, however, we are unaware of existing results guaranteeing the requisite continuity property when $\q\neq1$ and/or $\mathcal{G}_{\x}$ is not a two-sided Brownian motion, but fortunately it turns out that the continuity property of interest admits simple sufficient conditions (namely, \textbf{(B4)} in Assumption \ref{Assumption B}). For details, see Lemma \ref{[Lemma] Continuity AD} in the appendix.

The following assumption collects the conditions under which our verification of the four above-mentioned properties will proceed.

\begin{assumption}\label{Assumption B}
For the same $\q$ as in Assumption \ref{Assumption A}, the following are satisfied:
\begin{description}
    \item[(B1)] \label{Assumption B: weak convergence}
    $\widehat{G}_{\x,n}^{\q}\rightsquigarrow\mathcal{G}_{\x}$ and $\widehat{G}_{\x,n}^{\q,*}\rightsquigarrow_{\P}\mathcal{G}_{\x}$, where $\widehat{G}_{\x,n}^{\q}$ and $\widehat{G}_{\x,n}^{\q,*}$ are defined as in Section \ref{[Subsection] Heuristics} and where $\mathcal{G}_{\x}$ is a zero mean Gaussian process.

    \item[(B2)] \label{Assumption B: uniform consistency Gamma}
    $\sup_{x\in I}|\widehat{\Gamma}_n(x)-\Gamma_0(x)|=o_{\P}(1)$ and $\sup_{x\in I}|\widehat{\Gamma}_n^*(x)-\widehat{\Gamma}_n(x)|=o_{\P}(1)$.

    \item[(B3)] \label{Assumption B: uniform convergence Phi}
    $\widehat{\Phi}_n$ and $\widehat{\Phi}_n^*$ are non-negative, non-decreasing, and right continuous on $I$.
%    \item[(B3)] \label{Assumption B: uniform convergence Phi}
%    $\widehat{\Phi}_n:I\to[0,\widehat{u}_n]$ and $\widehat{\Phi}_n^*:I\to[0,\widehat{u}_n^*]$ are non-decreasing and right continuous.
    
%    In addition, $\widehat{\Phi}_n$ and $\widehat{\Phi}_n^*$ and their generalized inverses have closed range.
    
    Also, $\sup_{x\in I}|\widehat{\Phi}_n(x)-\Phi_0(x)|=o_{\P}(1)$, $\sup_{x\in I}|\widehat{\Phi}_n^*(x)-\widehat{\Phi}_n(x)|=o_{\P}(1)$, and, for every $K>0$,
    \[a_n\sup_{|v|\leq K}|\widehat{\Phi}_n(\x+va_n^{-1})-\Phi_0(\x+va_n^{-1})|=o_{\P}(1)
    \]
    and
    \[a_n\sup_{|v|\leq K}|\widehat{\Phi}_n^*(\x+va_n^{-1})-\widehat{\Phi}_n(\x+va_n^{-1})|=o_{\P}(1).\]

    \item[(B4)] For every $v,v'\in\mathbb{R}$, the covariance kernel $\mathcal{C}_{\x}$ of $\mathcal{G}_{\x}$ satisfies
    \[\mathcal{C}_{\x}(v+v',v+v')-\mathcal{C}_{\x}(v+v',v')-\mathcal{C}_{\x}(v',v+v')+\mathcal{C}_{\x}(v',v')=\mathcal{C}_{\x}(v,v)\]
    and
    \[\mathcal{C}_{\x}(v\tau,v'\tau)=\mathcal{C}_{\x}(v,v')\tau\qquad\text{for every } \tau\geq0.\]
    Also, $\mathcal{C}_{\x}(1,1)>0$ and $\lim_{\delta\downarrow0}\mathcal{C}_{\x}(1,\delta)/\sqrt{\delta}=0$.

    \item[(B5)] For some $u_0>\Phi_0(\x)$, $\widehat{u}_n \geq u_0+o_{\P}(1)$ and $\widehat{u}_n^* \geq \widehat{u}_n+o_{\P}(1)$.

    Also, $\{0,\widehat{u}_n\} \subseteq \widehat{\Phi}_n(I)$ and $\{0,\widehat{u}_n^*\} \subseteq \widehat{\Phi}_n^*(I)$.
    
    In addition, $\widehat{\Phi}_n^{-}([0,\widehat{u}_n]), (\widehat{\Phi}_n^*)^{-}([0,\widehat{u}_n^*]),\widehat{\Phi}_n(I) \cap [0,\widehat{u}_n]$, and $\widehat{\Phi}_n^*(I) \cap [0,\widehat{u}_n^*]$ are closed. 
    
\end{description}
\end{assumption}

Verification of the bootstrap parts of Assumption \ref{Assumption B} will be discussed in Section \ref{[Subsection] Bootstrapping} below. When combined with Assumption \ref{Assumption A}, Assumption \ref{Assumption B} suffices in order to establish (\ref{[Asymptotics] thetahat}) and (\ref{[Asymptotics] Ghatstar,Lhatstar,Zhatstar}). In addition, (\ref{[Asymptotics] thetatildestar}) can be shown to hold if $\widetilde{M}_{\x,n}$ satisfies the following

\begin{assumption}\label{Assumption C}
For the same $\q$ as in Assumption \ref{Assumption A}, $\widetilde{M}_{\x,n}^{\q}\rightsquigarrow_{\P}\mathcal{M}_{\x}^{\q}$ and, for every $K>0$,
\[\liminf_{\delta\downarrow0}\liminf_{n\to\infty}\P\left[\inf_{|v|>K^{-1}}\widetilde{M}_{\x,n}(v)\geq\delta\right]=1.\]
\end{assumption}

Moreover, the arguments used to show that the cdf of $\argmin_{v\in\mathbb{R}}\{\mathcal{G}_{\x}(v)+\mathcal{M}_{\x}^{\q}(v)-t\mathcal{L}_{\x}(v)\}$ is continuous at zero can be used to also establish continuity of the cdf of $\partial_{-}\GCM_{\mathbb{R}}(\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q})(0)$. As a consequence, we obtain the following result.

\begin{theorem}\label{[Theorem] Main result}
    Suppose Assumptions \ref{Assumption A}, \ref{Assumption B}, and \ref{Assumption C} are satisfied. Then (\ref{[Asymptotics] thetahat}) and (\ref{[Asymptotics] thetatildestar}) hold. In addition, \begin{equation}\label{Bootstrap consistency}
        \sup_{t\in\mathbb{R}}\left\vert \P_n^*\left[\widetilde{\theta}_n^*(\x)-\widehat{\theta}_n(\x)\leq t\right]-\P\left[\widehat{\theta}_n(\x)-\theta_0(\x)\leq t\right]\right\vert = o_{\P}(1).
\end{equation}
\end{theorem}

In an attempt to emphasize the rate-adaptive nature of the consistency property enjoyed by the bootstrap-based distributional approximation based on $\widetilde{\theta}_n^*(\x)$, the formulation (\ref{Bootstrap consistency}) deliberately omits the rate term $r_n$ present in (\ref{[Asymptotics] thetahat}) and (\ref{[Asymptotics] thetatildestar}). Theorem \ref{[Theorem] Main result} has immediate implications for inference. For instance, it follows from (\ref{[Asymptotics] thetahat}) and (\ref{Bootstrap consistency}) that for any $\alpha\in(0,1)$, we have
\[\lim_{n\to\infty}\P[\theta_0(\x)\in\mathsf{CI}_{1-\alpha,n}(\x)]=1-\alpha,\]
where, defining $Q_{a,n}^*(\x)=\inf\{Q\in\mathbb{R}:\P_n^*[\widetilde{\theta}_n^*(\x)-\widehat{\theta}_n(\x)\leq Q]\geq a\}$,
\[\mathsf{CI}_{1-\alpha,n}(\x)=\left[ \widehat{\theta}_n(\x)-Q_{1-\alpha/2,n}^*(\x)~,~\widehat{\theta}_n(\x)-Q_{\alpha/2,n}^*(\x)\right]\]
is the (nominal) level $1-\alpha$ bootstrap confidence interval for $\theta_0$ based on the ``percentile method" \citep[in the terminology of][]{vanderVaart_1998_Book}.

\section{Implementation}\label{[Section] Implementation}

Suppose Assumption \ref{Assumption A} is satisfied and suppose the triple $(\widehat{\Gamma}_n,\widehat{\Phi}_n,\widehat{u}_n)$ satisfies the non-bootstrap parts of
Assumption \ref{Assumption B}. Then, in order to compute the estimator $\widetilde{\theta}_n^*(\x)$ upon which our proposed bootstrap-based distributional approximation is based, two implementational issues must be addressed, namely the choice/construction of $\widetilde{M}_{\x,n}$ and $(\widehat{\Gamma}_n^*,\widehat{\Phi}_n^*,\widehat{u}_n^*)$, respectively. Section \ref{[Subsection] Mean function estimation} demonstrates the plausibility of Assumption \ref{Assumption C} by exhibiting estimators $\widetilde{M}_{\x,n}$ satisfying it under Assumptions \ref{Assumption A} and \ref{Assumption D}, the latter being a high-level condition that typicaly holds whenever the non-bootstrap parts of Assumption \ref{Assumption B} hold. Then, Section \ref{[Subsection] Bootstrapping} exhibits easy-to-compute $(\widehat{\Gamma}_n^*,\widehat{\Phi}_n^*,\widehat{u}_n^*)$ satisfying the bootstrap parts of Assumption \ref{Assumption B} under a random sampling assumption and other mild conditions.

\subsection{Mean Function Estimation}\label{[Subsection] Mean function estimation}

The ease with which an $\widetilde{M}_{\x,n}$ satisfying Assumption \ref{Assumption C} can be
constructed depends on whether $\q$ is known. To facilitate the discussion, for $j=1,\ldots,\left\lfloor \s\right\rfloor $, let
\[\mathcal{D}_{j}(\x)=\frac{\partial^{j+1}\Upsilon_0(\x)}{(j+1)!},\qquad \Upsilon_0(x)=\Gamma_0(x)-\theta_0(\x)\Phi_0(x).\]
As defined, $\Upsilon_0$ satisfies $\partial \Upsilon_0(\x)=\ldots=\partial^{\q}\Upsilon_0(\x)=0$ and
\[\partial^{j+1}\Upsilon_0(\x)=\sum_{k=\q}^{j}\binom{j}{k}\partial^{k}\theta_0(\x)\partial^{j+1-k}\Phi_0(\x), \qquad j=\q,\ldots,\left\lfloor \s\right\rfloor,\]
implying in particular that $\partial^{\q+1}\Upsilon_0(\x)=\partial^{\q}\theta_0(\x)\partial\Phi_0(\x)$ and therefore $\mathcal{M}_{\x}^{\q}(v)=\mathcal{D}_{\q}(\x)v^{\q+1}$.

First, consider the (simpler) case where $\q$ is known. In this case, if
\begin{equation}\label{Consistency condition}
    \mathcal{\widetilde{D}}_{\q,n}(\x)\to_{\P}\mathcal{D}_{\q}(\x),
\end{equation}
then Assumption \ref{Assumption C} holds when
\[\widetilde{M}_{\x,n}(v)=\mathcal{\widetilde{D}}_{\q,n}(\x)v^{\q+1}.\]
Examples of estimators $\mathcal{\widetilde{D}}_{\q,n}(\x)$ satisfying the consistency requirement (\ref{Consistency condition}) will be
given below.

Next, consider the somewhat more complicated case where $\q$ is unknown, but assumed to satisfy $\q\leq\overline{\q}$ for some known integer $\overline{\q}\geq3$. Noting that $\mathcal{D}_{j}(\x)=0$ for $j<\q$, $\mathcal{D}_{\q}(\x)>0$, and that $\q$ is necessarily an odd integer, $\mathcal{M}_{\x}^{\q}$ can be written as
\[\mathcal{M}_{\x}^{\q}(v)=\sum_{\ell=1}^{\left\lfloor(\overline{\q}+1)/2\right\rfloor}\I(2\ell\leq\q)\max(\mathcal{D}_{2\ell-1}(\x),0)v^{2\ell}.\]
Dropping the indicator function term from each summand, we obtain
\[\mathcal{\bar{M}}_{\x}(v)=\sum_{\ell=1}^{\left\lfloor (\overline{\q}+1)/2\right\rfloor }\max(\mathcal{D}_{2\ell-1}(\x),0)v^{2\ell}.\]
The majorant $\mathcal{\bar{M}}_{\x}$ is an ``adaptive" approximation to $\mathcal{M}_{\x}^{\q}$ in the sense that it does not depend on $\q$, yet satisfies the local approximation property
\[\mathcal{\bar{M}}_{\x,n}^{\q}\rightsquigarrow\mathcal{M}_{\x}^{\q},\qquad\mathcal{\bar{M}}_{\x,n}^{\q}(v)=\sqrt{na_n}\mathcal{\bar{M}}_{\x}(va_n^{-1}).\]
Moreover, the following (``global") positivity property automatically holds:
\[\inf_{|v|>K^{-1}}\mathcal{\bar{M}}_{\x}(v)>0, \qquad \text{for every } K>0.\]
As a consequence, it seems plausible that a ``plug-in" estimator of $\mathcal{\bar{M}}_{\x}$ would satisfy Assumption \ref{Assumption C} under reasonable conditions. Indeed, if
\begin{equation}\label{Rate of convergence condition}
    a_n^{\q-(2\ell-1)}(\mathcal{\widetilde{D}}_{2\ell-1,n}(\x)-\mathcal{D}_{2\ell-1}(\x))=o_{\P}(1),\qquad\ell=1,\ldots,\left\lfloor (\overline{\q}+1)/2\right\rfloor,
\end{equation}
then Assumption \ref{Assumption C} is satisfied by
\[\widetilde{M}_{\x,n}(v)=\sum_{\ell=1}^{\left\lfloor (\overline{\q}+1)/2\right\rfloor }\max(\mathcal{\widetilde{D}}_{2\ell-1,n}(\x),0)v^{2\ell}.\]
For ``small" $\ell$ (namely, for $2\ell-1<\q$), the precision requirement (\ref{Rate of convergence condition}) is stronger than consistency, but fortunately it turns out that the requirement can be met as long as $\s$ is larger than $\overline{\q}$.

Example-specific estimators $\mathcal{\widetilde{D}}_{\q,n}(\x)$ satisfying the consistency requirement (\ref{Consistency condition}) are sometimes readily available. For instance, in the case of the \citet{Grenander_1956_SAJ} estimator (i.e., in Example \ref{[Example] Monotone Density Estimation}), we have $\mathcal{D}_{\q}(\x)=\partial^{\q}f_0(\x)/(\q+1)!$, a consistent estimator of which can be based on any consistent estimator of
$\partial^{\q}f_0(\x)$, such as a standard kernel estimator or, if the evaluation point $\x$ is near the boundary of the support of $X$, boundary adaptive versions thereof.

More generic estimators are also available. For specificity, the remainder of this section focuses on estimators of $\mathcal{D}_{j}(\x)$ obtained by applying numerical derivative-type operators to the following (possibly) non-smooth estimator of $\Upsilon_0$:
\[\widehat{\Upsilon}_n(x)=\widehat{\Gamma}_n(x)-\widehat{\theta}_n(\x)\widehat{\Phi}_n(x).\]
For $j=1,\ldots,\q$, the fact that $\partial \Upsilon_0(\x)=\ldots=\partial^{\q}\Upsilon_0(\x)=0$ implies that $\mathcal{D}_{j}(\x)$ admits the ``monomial approximation" representation
\[\mathcal{D}_{j}(\x)=\lim_{\epsilon\to0}\left\{\epsilon^{-(j+1)}[\Upsilon_0(\x+\epsilon)-\Upsilon_0(\x)]\right\},\]
motivating the estimator
\[\mathcal{\widetilde{D}}_{j,n}^{\mathtt{MA}}(\x)=\epsilon_n^{-(j+1)}[\widehat{\Upsilon}_n(\x+\epsilon_n)-\widehat{\Upsilon}_n(\x)],\]
where $\epsilon_n>0$ is a (small) tuning parameter. Similarly, for any $j=1,\ldots,\left\lfloor \s\right\rfloor $, the generic ``forward difference" representation
\[\mathcal{D}_{j}(\x)=\lim_{\epsilon\to0}\left\{\epsilon^{-(j+1)}\sum_{k=1}^{j+1}(-1)^{k+j+1}\binom{j+1}{k}[\Upsilon_0(\x+k\epsilon)-\Upsilon_0(\x)]\right\}\]
motivates the estimator
\[\mathcal{\widetilde{D}}_{j,n}^{\mathtt{FD}}(\x)=\epsilon_n^{-(j+1)}\sum_{k=1}^{j+1}(-1)^{k+j+1}\binom{j+1}{k}[\widehat{\Upsilon}_n(\x+k\epsilon_n)-\widehat{\Upsilon}_n(\x)].\]
Finally, to define an estimator with (possibly) superior bias properties, suppose $\s$ admits a known integer $\underline{\s}$ satisfying $\overline{\q}\leq \underline{\s}\leq\s$, let $\{c_k\in\mathbb{R}:1\leq k\leq \underline{\s}+1\}$ be such that the matrix $[c_{k}^p]_{1\leq k\leq \underline{\s}+1,1\leq p\leq \underline{\s}+1}$ is invertible, and let the defining property of $\{\lambda_{j}^{\mathtt{BR}}(k):1\leq k\leq\underline{\s}+1\}$ be
\[\sum_{k=1}^{\underline{\s}+1}\lambda_{j}^{\mathtt{BR}}(k)c_k^{p}=\I(p=j+1),\qquad p=1,\ldots,\underline{\s}+1.\]
Then, for any $j=1,\ldots,\underline{\s}$, the ``bias-reduced" estimator
\[\mathcal{\widetilde{D}}_{j,n}^{\mathtt{BR}}(\x)=\epsilon_n^{-(j+1)}\sum_{k=1}^{\underline{\s}+1}\lambda_{j}^{\mathtt{BR}}(k)[\widehat{\Upsilon}_n(\x+c_k\epsilon_n)-\widehat{\Upsilon}_n(\x)]\]
is motivated by the fact that as $\epsilon\to0$, the error of the approximation
\[\mathcal{D}_{j}(\x)\approx\epsilon^{-(j+1)}\sum_{k=1}^{\underline{\s}+1}\lambda_{j}^{\mathtt{BR}}(k)[\Upsilon_0(\x+c_k\epsilon)-\Upsilon_0(\x)]\]
is of order $\epsilon^{\min(\underline{\s}+1,\s)-j}$ when $\s>j$. Relative to $\mathcal{\widetilde{D}}_{j,n}^{\mathtt{MA}}(\x)$ and $\mathcal{\widetilde{D}}_{j,n}^{\mathtt{FD}}(\x)$, this is a distinguishing feature of $\mathcal{\widetilde{D}}_{j,n}^{\mathtt{BR}}(\x)$ and as it turns out this feature will enable us to formulate sufficient conditions for (\ref{Rate of convergence condition}).

For $\delta>0$, let
\begin{align*}
    \widehat{G}_{\x,n}(v;\delta)  & =\sqrt{n\delta^{-1}}[\widehat{\Gamma}_n(\x+v\delta)-\widehat{\Gamma}_n(\x)-\Gamma_0(\x+v\delta)+\Gamma_0(\x)]\\
    & -\theta_0(\x)\sqrt{n\delta^{-1}}[\widehat{\Phi}_n(\x+v\delta)-\widehat{\Phi}_n(\x)-\Phi_0(\x+v\delta)+\Phi_0(\x)],
\end{align*}
and
\[\widehat{R}_{\x,n}(v;\delta)=\delta^{-1}[\widehat{\Phi}_n(\x+v\delta)-\widehat{\Phi}_n(\x)-\Phi_0(\x+v\delta)+\Phi_0(\x)].\]
Using this notation, the first part of \textbf{(B1)} and the first displayed condition of \textbf{(B3)} can be restated as
\[\widehat{G}_{\x,n}(\cdot;a_n^{-1})\rightsquigarrow\mathcal{G}_{\x}\]
and
\[\sup_{|v|\leq K}|\widehat{R}_{\x,n}(v;a_n^{-1})|=o_{\P}(1) \qquad \text{for every } K>0,\]
respectively. In the displayed results, one can typically replace $a_n^{-1}=n^{-1/(1+2\q)}$ by any $\delta_n>0$ with $\delta_n=o(1)$ and $a_n^{-1}\delta_n
^{-1}=O(1)$. As a consequence, validity of the following assumption usually follows as a by-product of the arguments used to justify \textbf{(B1)} and \textbf{(B3)}. (An illustration of this phenomenon is provided by Lemma \ref{[Lemma] Bootstrapping} below.)

\begin{assumption}\label{Assumption D}
For the same $\q$ as in Assumption \ref{Assumption A} and for every $\delta_n>0$ with $\delta_n=o(1)$ and $a_n^{-1}\delta_n
^{-1}=O(1)$,
\[\widehat{G}_{\x,n}(1;\delta_n)=O_{\P}(1) \qquad\text{and}\qquad \widehat{R}_{\x,n}(1;\delta_n)=o_{\P}(1).\]
\end{assumption}

In turn, Assumption \ref{Assumption D} is useful for the purposes of analyzing $\mathcal{\widetilde{D}}_{j,n}^{\mathtt{MA}},\mathcal{\widetilde{D}}_{j,n}^{\mathtt{FD}}$, and $\mathcal{\widetilde{D}}_{j,n}^{\mathtt{BR}}$.

\begin{lemma}\label{[Lemma] Mean function estimation}
    Suppose Assumptions \ref{Assumption A} and \ref{Assumption D} are satisfied and that $r_n(\widehat{\theta}_n(\x)-\theta_0(\x))=O_{\P}(1)$. If $\epsilon_n\to0$ and if $a_n\epsilon_n\to\infty$, then
    \[\mathcal{\widetilde{D}}_{\q,n}(\x)\to_{\P}\mathcal{D}_{\q}(\x),\qquad\mathcal{\widetilde{D}}_{\q,n}\in\{\mathcal{\widetilde{D}}_{\q,n}^{\mathtt{MA}},\mathcal{\widetilde{D}}_{\q,n}^{\mathtt{FD}},\mathcal{\widetilde{D}}_{\q,n}^{\mathtt{BR}}\}\]
    and
    \[a_n^{\q-j}(\mathcal{\widetilde{D}}_{j,n}^{\mathtt{BR}}(\x)-\mathcal{D}_{j}(\x))=O(a_n^{\q-j}\epsilon_n^{\min(\underline{\s}+1,\s)-j})+o_{\P}(1), \qquad j=1,\ldots,\underline{\s}.\]
    In particular, if $3\leq\overline{\q}<\s$, then (\ref{Rate of convergence condition}) is satisfied by $\mathcal{\widetilde{D}}_{2\ell-1,n}=\mathcal{\widetilde{D}}_{2\ell-1,n}^{\mathtt{BR}}$ if $n\epsilon_n^{(1+2\overline{\q})\min(\underline{\s},\s-1)/(\overline{\q}-1)}\to0$ and if $n\epsilon_n^{1+2\overline{\q}}\to\infty$.
\end{lemma}

As alluded to previously, the ability of $\mathcal{\widetilde{D}}^{\mathtt{BR}}$ to satisfy (\ref{Rate of convergence condition}) is attributable in large part to its bias properties. In an attempt to highlight this, the second display of the lemma gives a stochastic expansion wherein the $O(a_n^{\q-j}\epsilon_n^{\min(\underline{\s}+1,\s)-j})$ term is a (possibly) negligible bias term. For $\mathcal{\widetilde{D}}\in\{\mathcal{\widetilde{D}}^{\mathtt{MA}},\mathcal{\widetilde{D}}^{\mathtt{FD}}\}$, the analogous stochastic expansions are of the form
\[a_n^{\q-j}(\mathcal{\widetilde{D}}_{j,n}(\x)-\mathcal{D}_{j}(\x))=O(a_n^{\q-j}\epsilon_n^{\q-j})+o_{\P}(1),\qquad j=1,\ldots,\q-1,\]
the $O(a_n^{\q-j}\epsilon_n^{\q-j})$ term also being a bias term. When $a_n\epsilon_n\to\infty$ (as is required for the ``noise" term in the stochastic expansion to be $o_{\P}(1)$), this bias term is non-negligible and the estimators $\mathcal{\widetilde{D}}^{\mathtt{MA}}$ and $\mathcal{\widetilde{D}}^{\mathtt{FD}}$ therefore do not satisfy (\ref{Rate of convergence condition}).

Under additional assumptions (including $\s\geq\underline{\s}+1$ and additional smoothness on $\Phi_0$), $\mathcal{\widetilde{D}}^{\mathtt{BR}}$ admits a Nagar-type mean squared error (MSE) expansion that can be used to select $\epsilon_n. $ The resulting approximate MSE formula is of the form
\[\epsilon_n^{2(\underline{\s}+1-j)}\mathsf{B}_{j}^{\mathtt{BR}}(\x)^{2}+\frac{1}{n\epsilon_n^{1+2j}}\mathsf{V}_{j}^{\mathtt{BR}}(\x),\]
where the bias constant is
\[\mathsf{B}_{j}^{\mathtt{BR}}(\x)=\mathcal{D}_{\underline{\s}+1}(\x)\sum_{k=1}^{\underline{\s}+1}\lambda_{j}^{\mathtt{BR}}(k)c_k^{\underline{\s}+2}\]
and the variance constant is
\[\mathsf{V}_{j}^{\mathtt{BR}}(\x)=\sum_{k=1}^{\underline{\s}+1}\sum_{l=1}^{\underline{\s}+1}\lambda_{j}^{\mathtt{BR}}(k)\lambda_{j}^{\mathtt{BR}}(l)\mathcal{C}_{\x}(k,l).\]
For details, see Section SA.5.4 in the supplemental appendix.
Assuming $\mathsf{B}_{j}^{\mathtt{BR}}(\x)\neq0$, the approximate MSE is minimized by
\[\epsilon_{j,n}^{\mathtt{BR}}(\x)=\left(\frac{1+2j}{2(\underline{\s}+1-j)}\frac{\mathsf{V}_{j}^{\mathtt{BR}}(\x)}{\mathsf{B}_{j}^{\mathtt{BR}}(\x)^{2}}\right)^{1/(3+2\underline{\s})}n^{-1/(3+2\underline{\s})},\]
a feasible version of which can be constructed by replacing $\mathcal{D}_{\underline{\s}+1}(\x)$ and $\mathcal{C}_{\x}$ with estimators in the expressions for $\mathsf{B}_{j}^{\mathtt{BR}}(\x)$ and $\mathsf{V}_{j}^{\mathtt{BR}}(\x)$, respectively.

\setcounter{example}{0}
\begin{example}[Monotone Density Estimation, continued]
    In this example,
    \[\mathcal{D}_{\underline{\s}+1}(\x)=\frac{1}{(\underline{\s}+2)!}\partial^{\underline{\s}+1}f_0(\x)\qquad\text{and\qquad}\mathcal{C}_{\x}(k,l)=f_0(\x)\min(k,l),\]
    so a feasible version of $\epsilon_{j,n}^{\mathtt{BR}}(\x)$ can be based on estimators of $\partial^{\underline{\s}+1}f_0(\x)$ and $f_0$. One option is to obtain consistent estimators by employing standard nonparametric techniques. Alternatively, a Silverman-style approach would obtain a feasible version of $\epsilon_{j,n}^{\mathtt{BR}}(\x)$ by working with a suitable reference distribution.
\end{example}

\setcounter{example}{1}
\begin{example}[Monotone Regression Estimation, continued]
    In this example,
    \[\mathcal{D}_{\underline{\s}+1}(\x)=\frac{1}{(\underline{\s}+2)!} \sum_{k=\q}^{\underline{\s}+1}\binom{\underline{\s}+1}{k}\partial^{k}\mu_0(\x)\partial^{\underline{\s}+1-k}f_0(\x)\qquad\text{and\qquad}\mathcal{C}_{\x}(k,l)=\sigma_0^{2}(\x)f_0(\x)\min(k,l),\]
    where $f_0$ is the Lebesgue density of $X$ and where $\sigma_0^{2}(x)=\mathbb{V}(Y|X=x)$. Again, one can obtain consistent estimators of the unknown components of $\mathcal{D}_{\underline{\s}+1}(\x)$ and $\mathcal{C}_{\x}(k,l)$ by employing standard nonparametric techniques or one can adopt a Silverman-style and obtain a feasible version of $\epsilon_{j,n}^{\mathtt{BR}}(\x)$ by working with a suitable reference model.
\end{example}

\subsection{Bootstrapping\label{[Subsection] Bootstrapping}}

Suppose inference is to be based on a random sample $\mathbf{Z}_1
,\ldots,\mathbf{Z}_n$ from the distribution of some $\mathbf{Z.}$ In all
examples of which we are aware, the bootstrap parts of Assumption \ref{Assumption B} are
satisfied when $(\widehat{\Gamma}_n^*,\widehat{\Phi}_n^*,\widehat{u}
_n^*)$ is given by the nonparametric bootstrap analog of $(\widehat{\Gamma
}_n,\widehat{\Phi}_n,\widehat{u}_n)$. Nevertheless, computationally simpler
alternatives are often available and in what follows we will present one such
alternative. To motivate our proposal, it is instructive to begin by
revisiting Example \ref{[Example] Monotone Density Estimation}.

\setcounter{example}{0}
\begin{example}[Monotone Density Estimation, continued]
In this example, $\mathbf{Z}=X$. Moreover, defining $\gamma_0(x;\mathbf{z})=\I(\mathbf{z}\leq x)$ and $\phi_0(x;\mathbf{z})=x$, we have the representations
\[
\Gamma_0(x)=\E[\gamma_0(x;\mathbf{Z})]\qquad\text{and}\qquad
\Phi_0(x)=\mathbb{E[}\phi_0(x;\mathbf{Z})],
\]
and the estimators $\widehat{\Gamma}_n$ and $\widehat{\Phi}_n$ are linear in the
sense that they are of the form%
\[
\widehat{\Gamma}_n(x)=\frac{1}{n}\sum_{i=1}^{n}\gamma_0(x;\mathbf{Z}
_{i})\qquad\text{and}\qquad\widehat{\Phi}_n(x)=\frac{1}{n}\sum_{i=1}^{n}\phi
_0(x;\mathbf{Z}_{i}),
\]
respectively. Finally, $\widehat{u}_n=\max(\max_{1\leq i\leq n}\mathbf{Z}
_{i},\x)$.

Letting $\mathbf{Z}_{1,n}^*,\ldots,\mathbf{Z}_{n,n}^*$ denote a
random sample from the empirical distribution of $\mathbf{Z}_1
,\ldots,\mathbf{Z}_n$, the nonparametric bootstrap analog $\widehat{u}_n$ is
given by $\widehat{u}_n^*=\max(\max_{1\leq i\leq n}\mathbf{Z}_{i,n}^*,\x)$, while%
\[
\widehat{\Gamma}_n^*(x)=\frac{1}{n}\sum_{i=1}^{n}\gamma_0(x;\mathbf{Z}
_{i,n}^*)\qquad\text{and}\qquad\widehat{\Phi}_n^*(x)=\frac{1}{n}
\sum_{i=1}^{n}\phi_0(x;\mathbf{Z}_{i,n}^*)
\]
are the nonparametric bootstrap analogs of $\widehat{\Gamma}_n$ and $\widehat{\Phi
}_n$, respectively.

In the case of $\widehat{u}_n$, the alternative bootstrap analog $\widehat{u}
_n^*=\widehat{u}_n$ is computationally trivial and automatically
satisfies the bootstrap part of \textbf{(B5)}. As for $\widehat{\Gamma}_n$ and
$\widehat{\Phi}_n$, their nonparametric bootstrap analogs admit the weighted
bootstrap representations%
\[
\widehat{\Gamma}_n^*(x)=\frac{1}{n}\sum_{i=1}^{n}W_{i,n}\gamma
_0(x;\mathbf{Z}_{i})\qquad\text{and}\qquad\widehat{\Phi}_n^*(x)=\frac
{1}{n}\sum_{i=1}^{n}W_{i,n}\phi_0(x;\mathbf{Z}_{i}),
\]
where, conditionally on $\mathbf{Z}_1,\ldots,\mathbf{Z}_n$, $(W_{1,n}
,\ldots,W_{n,n})$ is multinomially distributed with probabilities
$(n^{-1},\ldots,n^{-1})$ and number of trials $n$. (In turn, the weighted bootstrap
interpretation of the nonparametric bootstrap analog of $(\widehat{\Gamma}
_n,\widehat{\Phi}_n)$ in this example is interesting partly because it can be
used to embed the nonparametric bootstrap in a class of bootstraps also
containing the Bayesian bootstrap and the wild bootstrap.)\bigskip
\end{example}

Looking beyond Example \ref{[Example] Monotone Regression Estimation}, finding a computationally trivial $\widehat{u}_n^*$ satisfying the bootstrap part of \textbf{(B5)} is usually straightforward. On the
other hand, although a weighted bootstrap interpretation of the nonparametric
bootstrap version of the estimators $\widehat{\Gamma}_n$ and $\widehat{\Phi}_n$ is available whenever they are linear (e.g., in Example \ref{[Example] Monotone Regression Estimation}), there is no shortage of
examples for which linearity does not hold. Nevertheless, the weighted
bootstrap representation of the nonparametric bootstrap in Example \ref{[Example] Monotone Density Estimation} turns out
be useful for our purposes, as it is suggestive of computationally attractive
alternatives to the nonparametric bootstrap in more complicated examples.

When the non-bootstrap part of \textbf{(B1)} holds, the estimators $\widehat{\Gamma}_n$
and $\widehat{\Phi}_n$ are typically asymptotically linear in the sense that
they admit (possibly) unknown functions $\gamma_0$ and $\phi_0$
(satisfying $\Gamma_0(x)=\E[\gamma_0(x;\mathbf{Z})]$ and $\Phi
_0(x)=\mathbb{E[}\phi_0(x;\mathbf{Z})]$, respectively) for which the the
approximations
\[
\widehat{\Gamma}_n(x)\approx\bar{\Gamma}_n(x)=\frac{1}{n}\sum_{i=1}^{n}
\gamma_0(x;\mathbf{Z}_{i})\qquad\text{and}\qquad\widehat{\Phi}_n(x)\approx
\bar{\Phi}_n(x)=\frac{1}{n}\sum_{i=1}^{n}\phi_0(x;\mathbf{Z}_{i})
\]
are suitably accurate. Assuming also that $\gamma_0$ and $\phi_0$ admit
sufficiently well behaved estimators $\widehat{\gamma}_n$ and $\widehat{\phi}_n$
(say), it then stands to reason that the salient properties of $\widehat{\Gamma
}_n$ and $\widehat{\Phi}_n$ are well approximated by those of the
easy-to-compute exchangeable bootstrap-type pair%
\[
\widehat{\Gamma}_n^*(x)=\frac{1}{n}\sum_{i=1}^{n}W_{i,n}\widehat{\gamma}
_n(x;\mathbf{Z}_{i})\qquad\text{and}\qquad\widehat{\Phi}_n^*(x)=\frac
{1}{n}\sum_{i=1}^{n}W_{i,n}\widehat{\phi}_n(x;\mathbf{Z}_{i}),
\]
where $W_{1,n},\ldots,W_{n,n}$ denote exhangeable random variable (independent
of $\mathbf{Z}_1,\ldots,\mathbf{Z}_n$).

To give a precise statement, let
\[
\psi_{\x}(v;\mathbf{z})=\gamma_0(\x+v;\mathbf{z})-\gamma
_0(\x;\mathbf{z})-\theta_0(\x)[\phi_0(\x%
+v;\mathbf{z})-\phi_0(\x;\mathbf{z})]
\]
and define
\[
\bar{\Gamma}_n^*(x)=\frac{1}{n}\sum_{i=1}^{n}W_{i,n}\gamma
_0(x;\mathbf{Z}_{i})\qquad\text{and}\qquad\bar{\Phi}_n^*(x)=\frac
{1}{n}\sum_{i=1}^{n}W_{i,n}\phi_0(x;\mathbf{Z}_{i}).
\]
In addition, for any function class $\mathfrak{F}$, let $N_{U}(\varepsilon
,\mathfrak{F})$ denote the associated uniform covering numbers relative to
$L_{2};$ that is, for any $\varepsilon>0$, let%
\[
N_{U}(\varepsilon,\mathfrak{F})=\sup_{Q}N(\varepsilon\left\Vert \bar
{F}\right\Vert _{Q,2},\mathfrak{F},L_{2}(Q)),
\]
where $\bar{F}$ is the minimal envelope function of $\mathfrak{F}$,
$\left\Vert \cdot\right\Vert _{Q,2}$ is the $L_{2}(Q)$ norm, $N(\cdot)$ is the
covering number, and where the supremum is over all discrete probability
measure $Q$ with $\left\Vert \bar{F}\right\Vert _{Q,2}>0$.

\begin{assumption}\label{Assumption E}
For the same $\q$ as in Assumption \ref{Assumption A}, the
following are satisfied:
\begin{description}
%\begin{enumerate}
    \item[(E1)] $\mathbf{Z}_1,\ldots,\mathbf{Z}_n,$ are independent and identically distributed.

    \item[(E2)] $W_{1,n},\ldots,W_{n,n}$ are exchangeable random variables independent of $\mathbf{Z}_1,\ldots ,\mathbf{Z}_n$, $\widehat{\gamma}_n$, and $\widehat{\phi}_n$.
    
    Also, for some $\mathfrak{r}>(4\q+2)/(2\q-1)$,
    \[\frac{1}{n}\sum_{i=1}^{n}W_{i,n}=1,\qquad\frac{1}{n}\sum_{i=1}^{n}
    (W_{i,n}-1)^{2}\to_{\P}1,\qquad\text{and}\qquad\mathbb{E[}|W_{1,n}|^{\mathfrak{r}}]=O(1).
    \]


    \item[(E3)] $\sup_{x\in I}|\widehat{\Gamma}_n(x)-\bar{\Gamma}_n(x)|=o_{\mathbb{P}}(1)$,  $n^{-1}\sum_{i=1}^n\sup_{x\in I}|\widehat{\gamma}_n(x;\mathbf{Z}_i)-\gamma_0(x;\mathbf{Z}_i)|^2=o_{\mathbb{P}}(1)$, 
    and, for every $K>0$,
        \begin{equation*}
            \sqrt{na_n} \sup_{|v|\leq K}\big|\widehat{\Gamma}_n(\x+va_n^{-1}) -\widehat{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+va_n^{-1}) + \bar{\Gamma}_n(\x)\big|=o_{\mathbb{P}}(1)
        \end{equation*}
        and
        \begin{equation*}
            \frac{a_n}{n}\sum_{i=1}^n\sup_{|v|\leq K}\big|\widehat{\gamma}_n(\x+va_n^{-1};\mathbf{Z}_i) -\widehat{\gamma}_n(\x;\mathbf{Z}_i) -\gamma_0(\x+va_n^{-1};\mathbf{Z}_i) + \gamma_0(\x;\mathbf{Z}_i) \big|^2 =o_{\mathbb{P}}(1).
        \end{equation*}
        In addition, for some $V_\gamma\in (0,2)$,
        \begin{equation*}
            \limsup_{\varepsilon\downarrow 0}\frac{\log N_{U}(\varepsilon,\mathfrak{F}_{\gamma})}{\varepsilon^{-V_\gamma}} < \infty,\qquad\mathbb{E}[\bar{F}_{\gamma}(\mathbf{Z})^2]<\infty, \qquad \limsup_{\varepsilon\downarrow 0} \frac{\log N_{U}(\varepsilon,\widehat{\mathfrak{F}}_{\gamma,n})}{\varepsilon^{-V_\gamma}} = O_{\mathbb{P}}(1),
        \end{equation*}
        where $\mathfrak{F}_{\gamma}=\{\gamma_0(x;\cdot):x\in I\}$, $\bar{F}_{\gamma}$ is its minimal envelope, and $\widehat{\mathfrak{F}}_{\gamma,n}=\{\widehat{\gamma}_n(x;\cdot):x\in I\}$.
        
        Also,
        \begin{equation*}
            \limsup_{\delta\downarrow 0} \frac{\mathbb{E}[\bar{D}_{\gamma}^{\delta}(\mathbf{Z})^2+\bar{D}_{\gamma}^{\delta}(\mathbf{Z})^4]}{\delta} <\infty,
        \end{equation*}
        where $\bar{D}_{\gamma}^{\delta}$ is the minimal envelope of $\{\gamma_0(x;\cdot)-\gamma_0(\x;\cdot):x\in I_{\x}^{\delta}\}$.

    \item[(E4)] $\widehat{\Phi}_n$ and $\widehat{\Phi}_n^*$ are non-negative, non-decreasing, and right continuous on $I$.
%    \item[(E4)] $\widehat{\Phi}_n:I\to[0,\widehat{u}_n]$ and $\widehat{\Phi}_n^*:I\to[0,\widehat{u}_n^*]$ are non-decreasing and right continuous.
    
%    In addition, $\widehat{\Phi}_n$ and $\widehat{\Phi}_n^*$ and their generalized inverses have closed range.
        
    Also, $\sup_{x\in I}|\widehat{\Phi}_n(x)-\bar{\Phi}_n(x)|=o_{\mathbb{P}}(1)$, $n^{-1}\sum_{i=1}^n\sup_{x\in I}|\widehat{\phi}_n(x;\mathbf{Z}_i)-\phi_0(x;\mathbf{Z}_i)|^2=o_{\mathbb{P}}(1)$, $a_n|\widehat{\Phi}_n(\x)-\bar{\Phi}_n(\x)|=o_{\mathbb{P}}(1)$, and, for every $K>0$,
    \begin{equation*}
        \sqrt{na_n} \sup_{|v|\leq K}\big|\widehat{\Phi}_n(\x+va_n^{-1}) -\widehat{\Phi}_n(\x) - \bar{\Phi}_n(\x+va_n^{-1}) + \bar{\Phi}_n(\x)\big|=o_{\mathbb{P}}(1)
        \end{equation*}
        and
    \begin{equation*}
        \frac{a_n}{n}\sum_{i=1}^n\sup_{|v|\leq K}\big|\widehat{\phi}_n(\x+va_n^{-1};\mathbf{Z}_i) -\widehat{\phi}_n(\x;\mathbf{Z}_i) -\phi_0(\x+va_n^{-1};\mathbf{Z}_i) + \phi_0(\x;\mathbf{Z}_i) \big|^2 =o_{\mathbb{P}}(1).
    \end{equation*}
        In addition, for some $V_\phi\in (0,2)$,
        \begin{equation*}
            \limsup_{\varepsilon\downarrow 0}\frac{\log N_{U}(\varepsilon,\mathfrak{F}_{\phi})}{\varepsilon^{-V_\phi}} < \infty,\qquad\mathbb{E}[\bar{F}_{\phi}(\mathbf{Z})^2]<\infty, \qquad \limsup_{\varepsilon\downarrow 0} \frac{\log N_{U}(\varepsilon,\widehat{\mathfrak{F}}_{\phi,n})}{\varepsilon^{-V_\phi}} = O_{\mathbb{P}}(1),
        \end{equation*}
        where $\mathfrak{F}_{\phi}=\{\phi_0(x;\cdot):x\in I\}$, $\bar{F}_{\phi}$ is its minimal envelope, and $\widehat{\mathfrak{F}}_{\phi,n}=\{\widehat{\phi}_n(x;\cdot):x\in I\}$.
        
        Also,
        \begin{equation*}
            \limsup_{\delta\downarrow 0} \frac{\mathbb{E}[\bar{D}_{\phi}^{\delta}(\mathbf{Z})^2+\bar{D}_{\phi}^{\delta}(\mathbf{Z})^4]}{\delta} <\infty,
        \end{equation*}
        where $\bar{D}_{\phi}^{\delta}$ is the minimal envelope of $\{\phi_0(x;\cdot)-\phi_0(\x;\cdot):x\in I_{\x}^{\delta}\}$.

    \item[(E5)] For every $\delta_n>0$ with $a_n\delta_n=O(1)$,%
    \[\sup_{v,v'\in[-\delta_n,\delta_n]}\frac{\E
    [|\psi_{\x}(v;\mathbf{Z})-\psi_{\x}(v';\mathbf{Z}
    )|]}{|v-v'|}=O(1)\]
    and, for all $v,v'\in\mathbb{R}$, and for some $\mathcal{C}_{\x}$,%
    \[\frac{\E[\psi_{\x}(v\delta_n;\mathbf{Z})\psi_{\x%
    }(v'\delta_n;\mathbf{Z})]}{\delta_n}\to\mathcal{C}_{\x%
    }(v,v').
    \]
\end{description}
\end{assumption}

\begin{lemma}
\label{[Lemma] Bootstrapping}Suppose Assumptions \ref{Assumption A} and \ref{Assumption E} are satisfied. Then \textbf{(B1)-(B3)} are satisfied. If also
\[
\sqrt{n\delta_n^{-1}}[\widehat{\Gamma}_n(\x+\delta_n)-\widehat{\Gamma
}_n(\x)-\bar{\Gamma}_n(\x+\delta_n)+\bar{\Gamma}
_n(\x)]=O_{\P}(1)
\]
and
\[
\sqrt{n\delta_n^{-1}}[\widehat{\Phi}_n(\x+\delta_n)-\widehat{\Phi
}_n(\x)-\bar{\Phi}_n(\x+\delta_n)+\bar{\Phi}
_n(\x)]=O_{\P}(1)
\]
for every $\delta_n>0$ with $\delta_n=o(1)$ and $a_n^{-1}\delta_n
^{-1}=O(1)$, then Assumption \ref{Assumption D} is satisfied.
\end{lemma}

If Lemma \ref{[Lemma] Bootstrapping} is used to verify \textbf{(B1)-(B3)}, then \textbf{(B4)}
can usually be verified with minimal additional effort. In fact, the second
displayed part of \textbf{(B4)} is implied by the second displayed part of \textbf{(E5)} and the first displayed part of \textbf{(B4)} is implied by the following locally uniform (with
respect to $x$) strengthening of the second displayed part of \textbf{(E5)}:%
\[
\sup_{x\in I_{\x}^{\delta_n}}\left\vert \frac{\E[\psi
_{x}(v\delta_n;\mathbf{Z})\psi_{x}(v'\delta_n;\mathbf{Z})]}{\delta_n
}-\mathcal{C}_{\x}(v,v')\right\vert \to0.
\]
Moreover, it is usually not difficult to verify that $\mathcal{C}_{\x}$
also satisfies both the non-degeneracy condition $\mathcal{C}_{\x}(1,1)>0$ and
the (H\"{o}lder-type) continuity condition $\lim_{\delta\downarrow
0}\mathcal{C}_{\x}(1,\delta)/\sqrt{\delta}=0$.

\section{Examples}\label{[Section] Examples}

We apply our main results to two distinct sets of examples, both previously analyzed in \citet{Westling-Carone_2020_AoS} and \citet{Westling-Gilbert-Carone_2020_JRRSB}, and references therein. In the supplemental appendix, we also consider two other set of examples: Monotone Hazard Estimation \citep{Huang-Wellner_1995_SJS} and Monotone Distribution Estimation \citep{vanderVaart-vanderLaan_2006_IJB}. To conserve space, this section only offers an overview of our main results for each of the examples. Precise regularity conditions are stated in the supplemental appendix.

%The examples considered rely on the same basic setup. Let $\mathbf{Z}_i=(Y_i,\check{X}_i,\Delta_i,\mathbf{A}_i)'$, $i=1,2,\dots,n$, be an observed random sample with $\check{X}_i=\min\{X_i,C_i\}$, $\Delta_i=\1(X_i\leq C_i)$, $\mathbf{A}_i$ denoting additional covariates. If $\P[C_i\geq X_i]=1$, then there is no (right) censoring and $\check{X}_i=X_i$. Assuming that $F_0(x)=\P[X_i\leq x]$ is absolutely continuous, $f_0(x)$ denotes the Lebesgue density of $X$. Letting $\mu_0(x)=\E[Y_i|X_i=x]$, the examples consider monotone estimation of $f_0(\x)$ and $\mu_0(\x)$, respectively, under various assumptions related to censoring and covariate-adjustment.

\subsection{Monotone Density Estimation}\label{[Subsection] Examples: Monotone Density Estimation}

As a first set of examples, consider the problem of estimating the density of a non-negative, continuously distributed random variable, possibly with censoring and covariate-adjustment. Let $\mathbf{Z}_1,\dots,\mathbf{Z}_n$ be $i.i.d.$ copies of $\mathbf{Z}=(\check{X},\Delta,\mathbf{A}')'$, with $\check{X}=\min(X,C)$, $\Delta=\1(X\leq C)$, and $\mathbf{A}$ denoting additional covariates. Assuming that $f_0$, the density of $X$, (exists and) is non-decreasing on $I=[0,u_0]$, the parameter of interest is $\theta_0(\x)=f_0(\x)$ for some $\x\in(0,u_0)$.

Throughout, we set $\Phi_0(x) = \widehat{\Phi}_n(x) = \widehat{\Phi}_n^*(x) = x$ and $\widehat{u}_n = \widehat{u}_n^* = \max({\max_{1\leq i \leq n} X_i,\x)}$. It remains to specify $\widehat{\Gamma}_n$ and $\widehat{\Gamma}_n^*$.

The canonical case of no censoring (i.e.,\ $\P[C \geq X]=1$) has been considered in Example \ref{[Example] Monotone Density Estimation}. For that case, $\widehat{\Gamma}_n(x)=n^{-1}\sum_{i=1}^n \1 (X_i\leq x)$, an exchangeable bootstrap analog of which is given by $\widehat{\Gamma}_n^*(x) = n^{-1}\sum_{i=1}^{n} W_{i,n}\1(X_i\leq x)$. Assuming $f_0$ is $\q$ times differentiable at $\x$ for $\q\geq 1$ with the first $(\q-1)$ derivatives vanishing and the $\q$th derivative positive, Assumptions \ref{Assumption A} and \ref{Assumption B} are easily verified under mild regularity conditions. As a consequence, Theorem \ref{[Theorem] Main result} implies that the bootstrap-based distributional approximation \eqref{[Asymptotics] thetahat} holds for any $\widetilde{M}_{\x,n}$ satisfying Assumption \ref{Assumption C}. In particular, $\mathcal{D}_{\q}(\x) = {\partial^\q f_0(\x)}/{(\q+1)!}$ in this example, so any pointwise consistent estimator of $\partial^\q f_0(\x)$ could be used to estimate $\mathcal{D}_{\q}(\x)$. Alternatively, since Assumption \ref{Assumption D} also holds, $\widetilde{M}_{\x,n}$ with one of $\widetilde{\mathcal{D}}_{\q,n}^{\mathtt{MA}},\widetilde{\mathcal{D}}_{\q,n}^{\mathtt{FD}},\widetilde{\mathcal{D}}_{\q,n}^{\mathtt{BR}}$ can also be used, provided that $\epsilon_n \to 0$ and $n\epsilon_n^{1+2\q}\to \infty$.

Next, suppose that censoring occurs completely at random; that is, suppose $X\indep C$. (See \citet{Huang-Wellner_1995_SJS}, and references therein.) In this case, we take $\widehat{\Gamma}_n(x) = 1 - \widehat{S}_n(x)$, where $\widehat{S}_n$ denotes an estimator of the survival function $S_0(x)=\mathbb{P}[X>x]$ such as the Kaplan-Meier estimator. Letting $\widehat{\Gamma}_n^*$ be the natural bootstrap analog of $\widehat{\Gamma}_n$,  the conclusions from the previous paragraph remain valid if $\P[C>c]$ is continuous, $S_0(u_0)\P[C>u_0]>0$, and if other regularity conditions hold. Therefore, if $\widetilde{M}_{\x,n}$ satisfies Assumption \ref{Assumption C}, then the ``reshaped'' bootstrap estimator $\widetilde{\theta}_n^*(\x)= \partial_{-} \mathrm{GCM}_{[0,\widehat{u}_n^*]} (\widetilde{\Gamma}_n^*)(\x)$ gives a bootstrap-based distributional approximation satifying \eqref{Bootstrap consistency}.

Finally, consider the case of censoring at random; that is, suppose $X\indep C|\mathbf{A}$. (See \citet{vanderlaan-Robins_2003_Book}, \citet{Zeng_2004_AoS}, and references therein.) Now we set
\[\widehat{\Gamma}_n(x) 
  =  \frac{1}{n}\sum_{i=1}^n \widehat{F}_n(x|\mathbf{A}_i) + \widehat{S}_n(x|\mathbf{A}_i)
            \Big[\frac{\Delta_i\1(\check{X}_i\leq x)}{\widehat{S}_n(\check{X}_i|\mathbf{A}_i)\widehat{G}_n(\check{X}_i|\mathbf{A}_i)}  - \int_0^{\min\{\check{X}_i, x\}} \frac{d\widehat{\Lambda}_n(u|\mathbf{A}_i)}{\widehat{S}_n(u|\mathbf{A}_i)\widehat{G}_n(u|\mathbf{A}_i)}\Big],
\]
where $\widehat{F}_n(x|\mathbf{A})=1-\widehat{S}_n(x|\mathbf{A})$, $\widehat{S}_n(x|\mathbf{A})$ and $\widehat{G}_n(c|\mathbf{A})$ denote preliminary estimates of the conditional survival functions $S_0(x|\mathbf{A})=\P[X>x|\mathbf{A}]$ and $G_0(c|\mathbf{A})=\P[C>c|\mathbf{A}]$, respectively, and $\widehat{\Lambda}_n(u|\mathbf{A})$ is the conditional cumulative hazard function that corresponds to $\widehat{S}_n(u|\mathbf{A})$. Letting 
\[
\widehat{\gamma}_n(x;\mathbf{Z}_i)= \widehat{F}_n(x|\mathbf{A}_i)+ \widehat{S}_n(x|\mathbf{A}_i)\Big[\frac{\Delta_i\1(\check{X}_i\leq x)}{\widehat{S}_n(\check{X}_i|\mathbf{A}_i)\widehat{G}_n(\check{X}_i|\mathbf{A}_i)} - \int_0^{\min\{\check{X}_i, x\}} \frac{d\widehat{\Lambda}_n(u|\mathbf{A}_i)}{\widehat{S}_n(u|\mathbf{A}_i)\widehat{G}_n(u|\mathbf{A}_i)}\Big]
\]
a bootstrap analog of $\widehat{\Gamma}_n$ is given by $\widehat{\Gamma}_n^*(x)  = n^{-1}\sum_{i=1}^n W_{i,n}\widehat{\gamma}_n(x;\mathbf{Z}_i),$
where we employ the original first-step estimates $\widehat{S}_n(x|\mathbf{A}_i)$ and $\widehat{G}_n(c|\mathbf{A}_i)$. This case also fits into the setting of Section \ref{[Subsection] Bootstrapping}, with the estimator $\widehat{\gamma}_n(x;\mathbf{Z}_i)$ defined above. Under regularity conditions stated in the supplemental appendix and if $\widetilde{M}_{\x,n}$ is one of the numerical derivative-based estimators discussed in Section \ref{[Subsection] Mean function estimation}, then the ``reshaped'' bootstrap estimator $\widetilde{\theta}_n^*(\x)= \partial_{-} \mathrm{GCM}_{[0,\widehat{u}_n^*]} (\widetilde{\Gamma}_n^*)(\x)$ gives a bootstrap-assisted distributional approximation satisfying \eqref{Bootstrap consistency}.

\subsection{Monotone Regression Estimation}\label{[Subsection] Examples: Monotone Regression Estimation}

As a second pair of examples, consider the problem of regression estimation, possibly with additional covariate adjustment. In these examples we abstract from censoring and assume that
$\mathbf{Z}_1,\dots,\mathbf{Z}_n$ are $i.i.d.$ copies of $\mathbf{Z}=(Y,X,\mathbf{A}')'$. Defining $\mu_0(x|a)=\E[Y|X=x,\mathbf{A}=a]$, the parameter of interest is $\theta_0(\x)=\E[\mu_0(\x|\mathbf{A})]$, where $\x$ is in the interior of $I$, the support of $X$. (If there are no covariates $\mathbf{A}$, then $\theta_0(\x)=\mu_0(\x)=\E[Y|X=\x]$.) It is assumed that $\theta_0$ is non-decreasing on $I$.

With $\Phi_0$ equal to the cdf of $X$, we can set $u_0=\widehat{u}_n=\widehat{u}_n^*=1$ and natural choices of $\widehat{\Phi}_n$ and $\widehat{\Phi}_n^*$ are given by $\widehat{\Phi}_n(x) =n^{-1}\sum_{i=1}^n \1(X_i\leq x)$ and $\widehat{\Phi}_n^*(x) =n^{-1}\sum_{i=1}^n W_{i,n}\1(X_i\leq x)$, respectively. It remains to specify $\widehat{\Gamma}_n$ and $\widehat{\Gamma}_n^*$.

The classical monotone regression estimator has been considered in Example \ref{[Example] Monotone Regression Estimation}. For that estimator, $\widehat{\Gamma}_n(x) = n^{-1}\sum_{i=1}^n Y_i\1(X_i\leq x)$, an exchangeable bootstrap analog of which is given by $\widehat{\Gamma}_n^*(x) = n^{-1}\sum_{i=1}^n W_{i,n}Y_i\1(X_i\leq x)$. Assuming $f_0(\x)$ is positive and that $\mu_0$ is $\q$ times differentiable at $\x$ for $\q\geq 1$ with the first $(\q-1)$ derivatives vanishing and the $\q$th derivative positive, Assumptions \ref{Assumption A} and \ref{Assumption B} are easily verified under mild regularity conditions. As a consequence, Theorem \ref{[Theorem] Main result} implies that the bootstrap-based distributional approximation \eqref{[Asymptotics] thetahat} holds for any $\widetilde{M}_{\x,n}$ satisfying Assumption \ref{Assumption C}. In this example, $\mathcal{D}_{\q}(\x)= {f_0(\x)\partial^\q \mu(\x)}/{(\q+1)!}$, so any pointwise consistent estimators of $f_0(\x)$ and $\partial^\q \mu_0(\x)$ could be used to estimate $\mathcal{D}_{\q}(\x)$. Alternatively, instead of using two distinct estimators, we can use the numerical derivative-type estimators since Assumption \ref{Assumption D} also holds.

Next, consider the case of monotone regression estimation with covariate-adjustment. (See \citet{Westling-Gilbert-Carone_2020_JRRSB}.)
We take
\[\widehat{\Gamma}_n(x)= \frac{1}{n}\sum_{i=1}^n \1(X_i\leq x)\Big[ \frac{Y_i-\widehat{\mu}_n(X_i|\mathbf{A}_i)}{\widehat{g}_n(X_i|\mathbf{A}_i)} + \frac{1}{n}\sum_{j=1}^n \widehat{\mu}_n(X_i|\mathbf{A}_j)\Big],
\]
where $\widehat{\mu}_n(x|\mathbf{a})$ and $\widehat{g}_n(x|\mathbf{a})$ are preliminary estimators of $\mu_0(x|\mathbf{a})$ and $g_0(x|\mathbf{a})=f_0(x|\mathbf{a})/f_0(x)$, respectively, with $f_0(x|\mathbf{a})$ denoting the conditional density of $X$ given $\mathbf{A}$. A bootstrap analog of $\widehat{\Gamma}_n$ is given by $\widehat{\Gamma}_n^*(x)  = n^{-1}\sum_{i=1}^n W_{i,n}\widehat{\gamma}_n(x;\mathbf{Z}_i)$, where \[\widehat{\gamma}_n(x;\mathbf{Z}_i)= \1(X_i\leq x)\Big[ \frac{Y_i-\widehat{\mu}_n(X_i|\mathbf{A}_i)}{\widehat{g}_n(X_i|\mathbf{A}_i)} + \frac{1}{n}\sum_{j=1}^n \widehat{\mu}_n(X_i|\mathbf{A}_j)\Big].\]
Under regularity conditions stated in the supplemental appendix and if $\widetilde{M}_{\x,n}$ is one of the numerical derivative-based estimators discussed in Section \ref{[Subsection] Mean function estimation}, then the ``reshaped'' bootstrap estimator $\widetilde{\theta}_n^*(\x)=\partial_{-}\GCM_{[0,\widehat{u}_n^*]}(\widetilde{\Gamma}_n^*\circ(\widehat{\Phi}_n^*)^{-})\circ\widehat{\Phi}_n^*(\x)$ gives a bootstrap-assisted distributional approximation satisfying \eqref{Bootstrap consistency}.

\section{Simulations}\label{[Section] Simulations}

We consider the canonical case of isotonic regression estimation. We estimate the non-decreasing regression function $\theta_0(\x)=\mathbb{E}[Y|X=\x]$ at an interior point $\x=0.5$ using a random sample of observations, where three distinct data generating processes (DGPs) are considered. To describe the DGPs, let $X$ be a uniform $(0,1)$ random variable and $\varepsilon$ be a standard normal random variable with $X$ and $\varepsilon$ being statistically independent. Model 1 corresponds to $Y=2\exp(X-0.5) + \varepsilon$, Model 2 corresponds to $Y=2(X-0.5) + \exp(X)\varepsilon$, and Model 3 corresponds to $Y=24\exp(X-0.5) -24(X-0.5) - 12(X-0.5)^2 + 0.1\varepsilon$. The second DGP exhibits heteroscedastic regression errors, and the third model features the regression function whose first derivative equals zero at the evaluation point $\x$ ($\q=3$). DGP 3 exhibits a degree of degeneracy, and if inference is conducted without the knowledge of $\q=3$ (which is what happens in practice), this feature makes the inference problem more challenging. 

The Monte Carlo experiment employs a sample size $n=1,000$ with $B=2,000$ bootstrap replications and $S=4,000$ simulations, and compares three types of bootstrap-based inference procedures: the standard non-parametric bootstrap, $m$-out-of-$n$ bootstrap, and our proposed bootstrap-assisted inference method implemented using the bias-reduced numerical derivative estimator discussed in Section \ref{[Subsection] Mean function estimation}. For the numerical derivative estimator, we developed a rule of thumb for the step size $\epsilon_n$ to operationalize the procedure. See the supplemental appendix for details. For our proposed method, we report results for three implementations: (i) (infeasible) procedure using the true value of $\mathcal{D}_0(\x)$ (referred to as ``oracle''), (ii) implementation using the numerical derivative estimator with a correct specification of $\q$ (referred to as ``known $\q$''), and (iii) ``robust'' implementation only assuming $\q\in\{1, 3\}$ (referred to as ``robust''). 

Table \ref{TABLE: isoreg} presents the numerical results. We report empirical coverage for nominal $95\%$ confidence intervals and their average interval length. In all the DGPs considered, our proposed bootstrap-assisted inference method leads to confidence intervals with excellent empirical coverage and average interval length. The infeasible ``oracle'' procedure attains empirical coverage very close to the nominal $95\%$, which aligns with our theoretical results, and feasible procedures using the numerical derivative estimators perform almost identical to the infeasible oracle version. In DGP 1 and 2, our procedures outperform both the standard non-parametric bootstrap (which is inconsistent) and the $m$-out-of-$n$ bootstrap (which is consistent) in empirical coverage and average length. In DGP 3, the $m$-out-of-$n$ with the subsample size $m=\lceil n^{1/2}\rceil$ performs comparable to our procedure, but two caveats should be noted. First, the $m$-out-of-$n$ bootstrap performance is sensitive to the choice of the subsample size. Therefore, to operationalize the $m$-out-of-$n$ bootstrap procedure, one needs to develop a reliable procedure to choose the subsample size. Another caveat, arguably more important in this context, is that the $m$-out-of-$n$ bootstrap procedure requires the knowledge of the convergence rate of the estimator. In our simulation, we use the true convergence rate for $m$-out-of-$n$ bootstrap, but in practice, one needs to assume or estimate from data the convergence rate. Since the convergence rate and the limit distribution of the generalized Grenander-type estimator crucially hinge on unknown $\q$, this feature of the $m$-out-of-$n$ bootstrap may be unappealing.
In contrast, our proposed bootstrap-based procedure only requires specifying an upper bound on $\q$ and it automaticall adapts to the unknown convergence rate.

\appendix
\renewcommand{\theequation}{A.\arabic{equation}}
\renewcommand{\theexample}{A.\arabic{example}}
\numberwithin{equation}{section}
\setcounter{example}{0}

\section{Technical Results}\label{[Section] Appendix - Technical Results}

This appendix presents two technical results. First, in Section \ref{[Subsection] Appendix - Generalized Switch Relation} we present a corrected version of the generalization of the switch relation stated by \citet[Supplement]{Westling-Carone_2020_AoS}. Second, in Section \ref{[Subsection] Appendix - Continuity of Limiting Distribution} we present a lemma that can be used to establish continuity of the cdf of the maximizer of a Gaussian process. Both lemmas are used in the proof of Theorem \ref{[Theorem] Main result} and may be of independent interest as well. 

%This appendix presents two technical results. First, in Section \ref{[Subsection] Appendix - Generalized Switching Lemma} we discuss the Generalized Switching Lemma of \citet[Supplement]{Westling-Carone_2020_AoS}, demonstrating by example that the lemma is incorrect and presenting a corrected version of it. The corrected lemma includes an additional regularity condition, but also removes other unnecessary conditions. Second, in Section \ref{[Subsection] Appendix - Continuity of Limiting Distribution} we present a lemma that can be used to establish continuity of the cdf of the maximizer of a Gaussian process. Both lemmas are used in the paper and may be of general interest as well. 

\subsection{Generalized Switch Relation}\label{[Subsection] Appendix - Generalized Switch Relation}

In their analysis of generalized Grenander-type estimators, \cite{Westling-Carone_2020_AoS} relied on a generalization of the \textit{switch relation} \citep{Groeneboom_1985_BookCh}. Their generalized switch relation is given in Lemma 1 of the Supplement to \cite{Westling-Carone_2020_AoS}. For the purposes of comparing it with Lemma \ref{[Lemma] Generalized Switch Relation} below, it is convenient to restate that lemma as follows:

\begin{description}
\item[Statement GSR \citep{Westling-Carone_2020_AoS}.] \emph{Let $\Phi$ and $\Gamma$ be real-valued functions defined on an interval $I\subseteq\mathbb{R}$ and suppose that $\Phi$ is non-decreasing and right continuous. Fix $l,u$ in $\Phi(I)$ with $l<u$ and let $\theta=\partial_{-}\GCM_{[l,u]}(\Gamma\circ\Phi^{-})\circ\Phi$. If $I$ is closed, $\Phi(I)\subseteq [l,u]$, and if $\Gamma$ and $\Gamma\circ\Phi^{-}$ are lower semi-continuous, then}
    \begin{equation}\label{Westling and Carone Lemma 1}
        \theta(\x)>t\quad\iff\quad \sup\argmax_{x\in \Phi^{-}([l,u])}\{t\Phi(x) - \Gamma(x)\}<\Phi^{-}(\Phi(\x))
    \end{equation}
\emph{for any $t\in\mathbb{R}$ and any $\x\in I$ with $\Phi(\x)\in(l,u)$}.
\end{description}

%\begin{statement}[Lemma 1 in the Supplement to \cite{Westling-Carone_2020_AoS} restated in our notation]
%    Let $\Phi$ and $\Gamma$ be functions from a closed interval $I\subseteq\mathbb{R}$ to $\mathbb{R}$, where $\Phi$ is non-decreasing and c\'{a}dl\'{a}g, $\Gamma$ and $\Gamma\circ\Phi^{-}$ are lower semi-continuous, and $\{a,b\}\subset \Phi(I)\subset [a,b]$.
%    Let $\theta=\partial_{-}\GCM_{[a,b]}(\Gamma\circ\Phi^{-})\circ\Phi$. Then, for any $c\in\mathbb{R}$ and $\x\in I$ with $\Phi(\x)\in(a,b)$,
%    \begin{equation}\label{Westling and Carone lemma 1}
%        \theta(\x)>c\quad\iff\quad \sup\argmax_{v\in I^{\star}}\{c\Phi(v) - \Gamma(v)\}<\Phi^{-}(\Phi(\x))
%    \end{equation}
%    where $I^{\star}= I\cap\Phi^{-}([a,b]) = \{x\in I: x=\Phi^{-}(u),u\in [a,b]\}$.
%\end{statement}

The main purpose of the following two examples is to show that without further restrictions, the $\argmax$ in \eqref{Westling and Carone Lemma 1} can be empty and the relation \eqref{Westling and Carone Lemma 1} can be violated. In other words, Statement GSR can fail to hold.

\begin{example}
Let $I=[l,u]=[0,1], \Gamma(x)=\gamma x$ (for some $\gamma \in \mathbb{R}$), and let
\begin{equation*}
\Phi(x)=
    \begin{cases}
        x & \text{if } 0\leq x<1/2\\
        1 & \text{if } 1/2\leq x \leq 1.
    \end{cases}
\end{equation*}
Then
\begin{equation*}
    t\Phi(x)-\Gamma(x)=
    \begin{cases}
        (t-\gamma)x & \text{if } 0\leq x<1/2\\
        t-\gamma/2 & \text{if } 1/2\leq x \leq 1,
    \end{cases}
\end{equation*}
so $\argmax_{x\in \Phi^{-}([l,u])}\{t\Phi(x) - \Gamma(x)\}$ is empty when $0>t>\gamma$.

In particular, if $0>t>\gamma$ and if $\x \in (0,1/2)$, then
\begin{equation*}
    \sup\argmax_{x\in \Phi^{-}([l,u])}\{t\Phi(x) - \Gamma(x)\}=0<\x=\Phi^{-}(\Phi(\x)),
\end{equation*}
whereas $\theta(\x)=\partial_{-}\GCM_{[l,u]}(\Gamma\circ\Phi^{-})\circ\Phi(\x)=\gamma<t,$ so \eqref{Westling and Carone Lemma 1} is violated.
    
In this example, the problems are attributable to the fact that although $\Phi^{-}([l,u])=[0,1/2]$ is closed, $\Phi(I)=\Phi(I) \cap [l,u]=[0,1/2)\cup \{1\}$ is not.
\end{example}

\begin{example}
Let $I=[l,u]=[0,1]$, $\Gamma(x)=\gamma x$ (for some $\gamma \in \mathbb{R}$), and let
\begin{equation*}
    \Phi(x)=
    \begin{cases}
        x & \text{if } 0\leq x<1/2\\
        1/2 & \text{if } 1/2\leq x<3/4\\
        2x-1 & \text{if } 3/4\leq x\leq 1.
    \end{cases}
\end{equation*}
Then
\begin{equation*}
    t\Phi(x)-\Gamma(x)=
    \begin{cases}
        (t-\gamma)x & \text{if } 0\leq x<1/2\\
        t/2 - \gamma x & \text{if } 1/2\leq x<3/4\\
        (2t-\gamma)x-t & \text{if } 3/4\leq x\leq 1,
    \end{cases}
\end{equation*}
so $\argmax_{x\in \Phi^{-}([l,u])}\{t\Phi(x) - \Gamma(x)\}$ is empty when $0>\gamma /2>t>\gamma$.

In particular, if $0>\gamma /2>t>\gamma$ and if $\x \in (0,3/4)$, then
\begin{equation*}
    \sup\argmax_{x\in \Phi^{-}([l,u])}\{t\Phi(x) - \Gamma(x)\}=0<\min(\x,1/2)=\Phi^{-}(\Phi(\x)),
\end{equation*}
whereas $\theta(\x)=\partial_{-}\GCM_{[l,u]}(\Gamma\circ\Phi^{-})\circ\Phi (\x)=3\gamma/2<t,$ so \eqref{Westling and Carone Lemma 1} is violated.

In this example, the problems are attributable to the fact that although $\Phi(I)=\Phi(I) \cap [l,u]=[0,1]$ is closed, $\Phi^{-}([l,u])=[0,1/2]\cup (3/4,1]$ is not.
\end{example}

As alluded to in these examples, it turns out that the problems are attributable to the fact that either $\Phi(I) \cap [l,u]$ or $\Phi^{-}([l,u])$ is not closed. That this is so is a consequence of the following lemma, which shows that one can indeed obtain a result in the spirit of \eqref{Westling and Carone Lemma 1} as long as $\Phi(I) \cap [l,u]$ and $\Phi^{-}([l,u])$ are closed.

%As stated, the above lemma is incorrect. There are (at least) two ways in which the conclusion can fail. First, the maximization problem $\max_{v\in I^{\star}}\{c\Phi(v)-\Gamma(v)\}$ may not have a solution. Furthermore, even when a solution exists, the statement \eqref{Westling and Carone lemma 1} can fail. In both cases, the problems can arise when $c<0$ and are attributable to the fact that $c\Phi$ may not be upper semi-continuous when $c<0$.

\begin{lemma}[Generalized Switch Relation]\label{[Lemma] Generalized Switch Relation} Let $\Phi$ and $\Gamma$ be real-valued functions defined on an interval $I\subseteq\mathbb{R}$ and suppose that $\Phi$ is non-decreasing and right continuous. Fix $l,u\in\Phi(I)$ with $l<u$ and let $\theta=\partial_{-}\GCM_{[l,u]}(\Gamma\circ\Phi^{-})\circ\Phi$. If $\Phi^{-}([l,u])$ and $\Phi(I) \cap [l,u]$ are closed, then
    \begin{equation}\label{Generalized Switch Relation}
        \theta(\x)>t\quad\iff\quad \sup\argmax_{x\in \Phi^{-}([l,u])}\{t\Phi(x) - \underline{\Gamma}(x)\}<\Phi^{-}(\Phi(\x))
    \end{equation}
for any $t\in\mathbb{R}$ and any $\x\in I$ with $\Phi(\x)\in(l,u)$, where $\underline{\Gamma}$ is the greatest lower semi-continuous minorant of $\Gamma$ on $\Phi^{-}([l,u]).$
\end{lemma}

The assumptions of the lemma seem very mild. In particular, the assumption that $\Phi(I) \cap [l,u]$ and $\Phi^{-}([l,u])$ are closed is satisfied not only in the examples of Section \ref{[Section] Examples}, but in all examples of which we are aware.

%To illustrate what can happen when $c<0$, let $I=[0,1]$, $a=0$, $b=1$, and define
%\begin{equation*}
%    \Phi(v)=\begin{cases}
%        0.5v &\text{if } 0\leq v<0.8\\
%        v & \text{if }0.8\leq v \leq 1
%    \end{cases}
%\end{equation*}
%and
%\begin{equation*}
%    \Gamma(v) = \begin{cases}
%        -v &\text{if } 0\leq v<0.8\\
%        4v-4&\text{if } 0.8\leq v\leq 1.
%    \end{cases}
%\end{equation*}
%Then, 
%\begin{equation*}
%   \Gamma(\Phi^{-}(u)) = 
%    \begin{cases}
%        -2u &\text{if } 0\leq u<0.4\\
%        - 0.8 &\text{if } 0.4\leq u< 0.8\\
%        4u-4 &\text{if } 0.8\leq u\leq 1
%   \end{cases}
%\end{equation*}
%and 
%\begin{equation*}
%    \theta(x) = 
%    \begin{cases}
%        -2 &\text{if } 0\leq x < 0.8\\
%        0 &\text{if } x=0.8\\
%        4 &\text{if } 0.8< x\leq 1.
%    \end{cases}
%\end{equation*}
%(See also Figure \ref{[Figure] Example}.) Note that $\Phi$ is non-decreasing and c\'{a}dl\'{a}g, $\Gamma$ and $\Gamma\circ\Phi^{-}$ are continuous, and $\{a,b\}\subset\Phi(I)\subset[a,b]$. As a consequence, the assumptions of Statement 1 are satisfied.
%
%Now consider the case when $c\in (-2,0)$. Then
%\begin{equation*}
%    c\Phi(v) - \Gamma(v) = (1+c0.5)v\mathbbm{1}\{0\leq v<0.8\} + (4+(c-4)v)\mathbbm{1}\{0.8\leq v\leq 1\}
%\end{equation*}
%and $\argmax_{v\in [0,1]} \{c\Phi(v)-\Gamma(v)\}$ is empty.

%Next, consider the case $c=-2$ and $\x=0.8$. Then
%\begin{equation*}
%    c\Phi(v) - \Gamma(v) = 0\mathbbm{1}\{0\leq v<0.8\} + (4-6v)\mathbbm{1}\{0.8\leq v\leq 1\}
%\end{equation*}
%and $\argmax_{v\in [0,1]}\{c\Phi(v) - \Gamma(v)\} = [0,0.8)$ so $\sup \argmax_{v\in [0,1]}\{c\Phi(v) - \Gamma(v)\}=0.8=\Phi^{-}(\Phi(\x))$, but $\theta(\x)=0>c$ and the statement \eqref{Westling and Carone lemma 1} is therefore incorrect. As already mentioned, the failure of the switch relation in the above example is due to $c\Phi-\Gamma$ not necessarily being upper semi-continuous when $c<0$. 
%
%There is another subtle issue regarding the generalized switching relationship when the composition $\Gamma\circ\Phi^{-}$ is not lower semi-continuous. In this situation, a natural approach is to replace $\Gamma\circ\Phi^{-}$ with its lower semi-continuous minorant, which does not alter the estimator $\widehat{\theta}_n(\x)$ \cite[c.f.,\ Lemma 4.2 of][]{vanderVaart-vanderLaan_2006_IJB}. However, the generalized switching relationship may not hold. To illustrate this point, consider the following example: $\Gamma$ as defined above and 
%\begin{equation*}
%    \Phi_2(x) = \begin{cases}
%        x & \text{if } 0\leq x\leq 0.5\\
%       0.5 & \text{if } x \in 0.5 < x\leq 0.8 \\
%        2.5x - 1.5 & \text{if } 0.8<x\leq 1.
%    \end{cases}
%\end{equation*}
%Note $\Phi_2$ is continuous and non-decreasing, $\Phi_2(I)=[0,1]$, and $I^{\star}=\Phi_2^{-}([0,1])=[0,0.5]\cup (0.8,1]$. $I^{\star}$ not being closed causes some issues with the generalized switching relation. Specifically,
%\begin{equation*}
%    \argmax_{x\in I^{\star}}\big\{c\Phi_2(x) - \Gamma(x)\big\}=
%    \begin{cases}
%        \{0\} &\text{if } c \leq -1.6\\
%        \emptyset &\text{if } c\in (-1.6,1.6)\\
%        (0.8,1]&\text{if } c=1.6\\
%        \{1\} &\text{if }c > 1.6
%    \end{cases}
%\end{equation*}
%and $\partial_{-}\GCM_{[0,1]}(\Gamma\circ\Phi_2^{-})(\Phi_2(x))=-1.6\mathbbm{1}\{x\in (0,0.8]\} +1.6\mathbbm{1}\{x\in (0.8,1]\}$. Thus, the statement \eqref{Westling and Carone lemma 1} fails (e.g.,\ $\x=0.5$ and $c=-1.6$).
%
%
%The problems highlighted by our examples can be avoided by imposing additional restrictions on the function $\Phi$. To be specific, it suffices to assume that the ranges of $\Phi$ and $\Phi^{-}$ are closed. This condition is satisfied in all the applications we consider. The proof of the following lemma is given in the supplemental appendix.
%
%\begin{lemma}[Generalized Switching Lemma]\label{[Lemma] Generalized Switching Lemma} 
%	Let $\Gamma$ and $\Phi$ be real-valued functions defined on an interval $I\subseteq \mathbb{R}$, where $\Phi$ is nondecreasing and right continuous, and $\Phi(I)$ is closed. Fix $l<u$ with $l,u\in\Phi(I)$. Let $\psi=\partial_{-}\mathrm{GCM}_{[l,u]}(\Gamma \circ \Phi^{-})$ and $\theta=\psi\circ \Phi$. If the set $\Phi^{-}([l,u])$ is closed, then, for any $t\in \mathbb{R}$ and $\x\in I$ with $ \Phi(\mathsf{x}) \in (l,u)$,
%	\begin{equation*}
%	   \theta(\mathsf{x}) > t \quad \iff \quad \sup \argmax_{x\in I^{\star} } \big\{ t \Phi(x) - \lsc_{I^{\star}}(\Gamma)(x) \big\} < \Phi^{-}\big(\Phi(\mathsf{x})\big)
%	\end{equation*}
%	where $I^{\star}:=\Phi^{-}([l,u]) =\{x\in I: \Phi(x) \in [l,u]\}$.
%\end{lemma}

%\begin{figure}[h]\caption{Plots of functions}\label{[Figure] Example}
%	\sbox0{
%	\centering
%	\begin{tikzpicture}
%	\begin{axis}[
%	domain=0:1, xmax=1.05, width=0.35\textwidth
%	]
%	\addplot[color=black, line width=1.5pt,domain=0:0.8]{-x};
%	\addplot[color=black, line width=1.5pt,domain=0.8:1]{4*x-4};
	%\draw  (80,800) circle (2pt);
	%\filldraw  (80,0) circle (2pt);
%	\end{axis}
%	\end{tikzpicture}}%
%   \sbox1{ \centering
%	\begin{tikzpicture}
%	\begin{axis}[
%	domain=0:1, xmax=1.05, width=0.35\textwidth
%	]
%	\addplot[color=black, line width=1.5pt,domain=0:0.8]{x/2};
%	%\addplot[color=red, line width=0.5pt,domain=0:1]{0.72};
%	\addplot[color=black, line width=1.5pt,domain=0.8:1]{x};
%	%\draw[red] (80,0) -- (100,800); 
%	%\draw  (80,20) circle (2pt);
%	\filldraw  (80,800) circle (2pt);
%	\draw  (80,400) circle (2pt);
%	\end{axis}
%	\end{tikzpicture}}%
%   \sbox2{\centering
%	\begin{tikzpicture}
%	\begin{axis}[
%	domain=0:1, xmax=1.05, width=0.35\textwidth
%	]
%	\addplot[color=black, line width=1.5pt,domain=0:0.4]{-2*x};
%	\addplot[color=black, line width=1.5pt,domain=0.4:0.8]{-0.8};
%	\addplot[color=black, line width=1.5pt,domain=0.8:1]{4*x-4};
%	\end{axis}
%	\end{tikzpicture}}
% \sbox3{ \centering
%	\begin{tikzpicture}
%	\begin{axis}[
%	domain=0:1, xmax=1.05, width=0.35\textwidth
%	]
%	\addplot[color=black, line width=1.5pt,domain=0:0.5]{x};
%	\addplot[color=black, line width=1.5pt,domain=0.5:0.8]{0.5};
%	\addplot[color=black, line width=1.5pt,domain=0.8:1]{2.5*x-1.5};
	%\draw[red] (80,0) -- (100,800); 
	%\draw  (80,20) circle (2pt);
%	\end{axis}
%	\end{tikzpicture}}%
%   \sbox4{ \centering
%	\begin{tikzpicture}
%	\begin{axis}[
%	domain=0:1, xmax=1.05, width=0.35\textwidth
%	]
%	\addplot[color=black, line width=1.5pt,domain=0:0.5]{-x};
	%\addplot[color=black, line width=0.5pt,domain=0.5:0.8]{0.5};
%	\addplot[color=black, line width=1.5pt,domain=0.5:1]{1.6*x-1.6};
%	\filldraw  (50,300) circle (2pt);
%	\draw  (50,0) circle (2pt);
%	\end{axis}
%	\end{tikzpicture}}%
% \sbox5{ \centering
%	\begin{tikzpicture}
%	\begin{axis}[
%	domain=0:1, xmax=1.05, width=0.35\textwidth
%	]
%	\addplot[color=black, line width=1.5pt,domain=0:0.5]{-1.6*x};
%	%\addplot[color=black, line width=0.5pt,domain=0.5:0.8]{0.5};
%	\addplot[color=black, line width=1.5pt,domain=0.5:1]{1.6*x-1.6};
	%\filldraw  (50,300) circle (2pt);
	%\draw  (50,0) circle (2pt);
%	\end{axis}
%	\end{tikzpicture}}%
% \begin{subfigure}[b]{\wd0}
%\caption{$\Gamma$}
%\usebox0
%\end{subfigure}\hfill
%\begin{subfigure}[b]{\wd1}
%\caption{$\Phi$}
%\usebox1
%\end{subfigure}\hfill
%\begin{subfigure}[b]{\wd2}
%\caption{$\Gamma\circ \Phi^{-}$}
%\usebox2
%\end{subfigure}
%\begin{subfigure}[b]{\wd3}
%\caption{$\Phi_2$}
%\usebox3
%\end{subfigure}
%\begin{subfigure}[b]{\wd4}
%\caption{$\Gamma\circ\Phi_2^{-}$}
%\usebox4
%\end{subfigure}
%\begin{subfigure}[b]{\wd5}
%\caption{$\GCM_{[0,1]}(\Gamma\circ\Phi_2^{-})$}
%\usebox5
%\end{subfigure}
%\end{figure}

\subsection{Continuity of argmax}\label{[Subsection] Appendix - Continuity of Limiting Distribution}

Let $\{\mathbb{G}(v):v\in\mathbb{R}\}$ be a Gaussian process with mean function $\mu$, covariance kernel $\mathcal{K}$, and continuous sample paths.
Under conditions on $\mu$ and $\mathcal{K}$ stated below, there exists a unique maximizer of $\mathbb{G}(v)$ over $v\in\mathbb{R}$ with probability one \citep{Kim-Pollard_1990_AoS}. We complement this known fact with a result showing that the cdf of $\argmax_{v\in\mathbb{R}}\mathbb{G}(v)$ is continuous.

\begin{assumption}\label{[Assumption] Continuity of argmax - Covariance kernel}
    For every $ \tau> 0$ and every $v,v'\in\mathbb{R}$, $\mathcal{K}(v\tau ,v'\tau )=\mathcal{K}(v,v')\tau$ and
        \[\mathcal{K}(v+v',v+v')-\mathcal{K}(v+v',v')-\mathcal{K}(v',v+v')+\mathcal{K}(v',v')=\mathcal{K}(v,v).\]
    In addition, $\mathcal{K}(1,1)>0$, and $\lim_{\delta\downarrow0}\mathcal{K}(1,\delta)/\sqrt{\delta}=0$.
\end{assumption}
\begin{assumption}\label{[Assumption] Continuity of argmax - Mean function}
    For some $c>1$, $\limsup_{|v|\to\infty}\mu(v) |v|^{-c} = - \infty$.
\end{assumption}
\begin{lemma}\label{[Lemma] Continuity of argmax}
    Suppose that Assumptions \ref{[Assumption] Continuity of argmax - Covariance kernel} and \ref{[Assumption] Continuity of argmax - Mean function} hold. Then $x \mapsto \P[\argmax_{v\in\mathbb{R}}\{\mathbb{G}(v)\}\leq x]$ is continuous.
\end{lemma}
Under Assumptions \ref{Assumption A} and \ref{Assumption B} of the main text, $\mathcal{C}_{\x}$ satisfies Assumption \ref{[Assumption] Continuity of argmax - Covariance kernel} and $-\mathcal{M}_{\x}^{\q}+t\mathcal{L}_{\x}$ satisfies Assumption \ref{[Assumption] Continuity of argmax - Mean function} for any $t\in\mathbb{R}$. It therefore follows from the lemma that the function
\begin{equation*}
    x\mapsto \mathbb{P}\bigg[ \argmin_{v\in\mathbb{R}}\{\mathcal{G}_{\x}(v) + \mathcal{M}_{\x}^{\q}(v) - t\mathcal{L}_{\x}(v)\}\geq x \bigg]
\end{equation*}
is continuous at $x=0$ for any $t\in\mathbb{R}$. We utilize that fact in our proof of \eqref{[Asymptotics] thetahat} and note in passing that most of the existing literature on monotone function estimators seems to implicitly utilize a similar continuity result.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\onehalfspacing

\bibliographystyle{jasa}
\bibliography{CJN_2023_Monotone--bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Tables
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage

\begin{table}\renewcommand{\arraystretch}{1.2}
\caption{Simulations, Isotonic Regression Estimator, 95\% Confidence Intervals.}
{\resizebox{\columnwidth}{!}{ 
\label{TABLE: isoreg}\input{table_isoreg.txt} } }
\footnotesize\hspace{.5in} Notes:\newline 
(i) Panel \textbf{Standard} refers to standard non-parametric bootstrap, Panel \textbf{m-out-of-n} refers to $m$-out-of-$n$ non-parametric bootstrap with subsample $m$, Panel \textbf{Reshaped} refers to our proposed bootstrap-assisted procedure.  \newline
(ii) Columns ``$\tilde{\mathcal{D}}_{1,n}$'' and ``$\tilde{\mathcal{D}}_{3,n}$'' report the averages of the estimated $\mathcal{D}_{1},\mathcal{D}_3$ across simulations, and Columns ``Coverage'' and ``Length'' report empirical coverage and average length of bootstrap-based $95\%$ percentile confidence intervals, respectively.\newline
(iii) ``Oracle'' corresponds to the infeasible version of our proposed procedure using the true value of $\mathcal{D}_{\q}$, ``ND known $\q$'' corresponds to our proposed procedure using the bias-reduced numerical derivative estimator with a correct specification of $\q$, and ``ND robust'' corresponds to our proposed procedure only assuming $\q\in \{1,3\}$. The step size choice for the numerical derivative estimator is described in the supplemental appendix.\newline
(iv) The sample size is $1,000$, the number of bootstrap iterations is $2,000$, and the number of Monte Carlo simulations is $4,000$.
\end{table}

%\begin{table}\renewcommand{\arraystretch}{1.2}
%\caption{Simulations, Isotonic Density Estimator, 95\% Confidence Intervals.}\label{TABLE: isodensity_table}
%\subfloat[$n=1,000$, $S=2,000$, $B=2,000$]{\resizebox{\textwidth}{!}{\input{table_isodensity.txt}}}\\
%\footnotesize\hspace{.5in} Notes:\newline
%(i) Panel \textbf{Standard} refers to standard non-parametric bootstrap, Panel \textbf{m-out-of-n} refers to $m$-out-of-$n$ non-parametric bootstrap with subsample $m$, Panel \textbf{Plug-in:} $\widetilde{Q}^\mathtt{PI}_n$ refers to our proposed bootstrap-based implemented using the example-specific plug-in mean function estimator, and Panel \textbf{Num Deriv:} $\widetilde{Q}^\mathtt{ND}_n$ refers to our proposed bootstrap-based implemented using the generic numerical derivative mean function estimator.\newline
%(ii) Column ``$h$, $\epsilon$'' reports tuning parameter value used or average across simulations when estimated, and Columns ``Coverage'' and ``Length'' report empirical coverage and average length of bootstrap-based $95\%$ percentile confidence intervals, respectively.\newline
%(iii) $h_\mathtt{MSE}$ and $\epsilon_\mathtt{MSE}$ correspond to the simulation MSE-optimal choices, $h_\mathtt{AMSE}$ and $\epsilon_\mathtt{AMSE}$ correspond to the AMSE-optimal choices, and $\widehat{h}_\mathtt{AMSE}$ and $\widehat{\epsilon}_\mathtt{AMSE}$ correspond to the ROT feasible implementation of $\widehat{h}_\mathtt{AMSE}$ and $\widehat{\epsilon}_\mathtt{AMSE}$ described in the supplemental appendix.
%\end{table}

\end{document}