\documentclass[11pt]{article}
\usepackage{amsfonts,amssymb,amsmath,amsthm,amscd,dsfont,bbm,enumitem}
\usepackage[mathscr]{euscript} %use for \N for Normal distribution, closer to the original mathcal
\usepackage[round]{natbib}
\usepackage[onehalfspacing]{setspace}
\usepackage{color,graphicx,subcaption,hyperref}
\hypersetup{pdfborder = {0 0 0},colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue}
\usepackage[margin=1in]{geometry}

\newtheorem{assumption}{Assumption}
\renewcommand{\theassumption}{SA-\arabic{assumption}}
\newtheorem{lemma}{Lemma}
\renewcommand{\thelemma}{SA-\arabic{lemma}}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\renewcommand{\thetheorem}{SA-\arabic{theorem}}

\renewcommand{\thesection}{SA.\arabic{section}}\setcounter{section}{0}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}\setcounter{subsection}{0}
\renewcommand{\theequation}{SA.\arabic{equation}}\setcounter{equation}{0}

\usepackage{tocloft} % http://ctan.org/pkg/tocloft
\setlength{\cftsecnumwidth}{4em} % Set length of number width in ToC for \section
\setlength{\cftsubsecnumwidth}{4em} % Set length of number width in ToC for \subsection
\setlength{\cftsubsubsecnumwidth}{4em} % Set length of number width in ToC for \subsubsection

\renewcommand{\P}{\mathbbm{P}}
\newcommand{\E}{\mathbbm{E}}
\newcommand{\V}{\mathbbm{V}}
\newcommand{\cov}{\mathbbm{C}\mathrm{ov}}
\newcommand{\1}{\mathbbm{1}}

\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

%%%%%%% NOTATION
\newcommand{\x}{\mathsf{x}}
\newcommand{\q}{\mathfrak{q}}
\newcommand{\s}{\mathfrak{s}}
\newcommand{\GCM}{\mathsf{GCM}}
\newcommand{\lsc}{\mathsf{lsc}}

\setcounter{tocdepth}{2}

\begin{document}
	
\title{Bootstrap-Assisted Inference for Generalized Grenander-type Estimators\thanks{Cattaneo gratefully acknowledges financial support from the National Science Foundation through grant SES-1947805, and Jansson gratefully acknowledges financial support from the National Science Foundation through grant SES-1947662.}\\
Supplemental Appendix
}

\author{
	Matias D. Cattaneo\footnote{Department of Operations Research and Financial Engineering, Princeton University.}\and
	Michael Jansson\footnote{Department of Economics, University of California at Berkeley.}\and
	Kenichi Nagasawa\footnote{Department of Economics, University of Warwick.}
}
\maketitle
\setcounter{page}{0}\thispagestyle{empty}

%\begin{abstract}
%	This document provides all proofs of the results.
%\end{abstract}

\clearpage
\tableofcontents

\clearpage
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\section{Generalized Grenander-type estimators}
As discussed in the main paper, the parameter of interest $\theta_0(\x)$ is characterized by $\theta_0(\x) = \partial_{-}\GCM_{[0,u_0]}(\Gamma_0\circ \Phi_0^{-})\circ\Phi_0(\x)$ where $\partial_{-}$ denotes left-differentiation operator, $\GCM_{J}(\cdot)$ is the greatest convex minorant operator over an interval $J$, and $\Gamma_0,\Phi_0$ are some real-valued functions.
Let $\widehat{\Phi}_n,\widehat{\Gamma}_n$ be estimators of $\Gamma_0,\Phi_0$, and $\widehat{u}_n$ be a sample counterpart of $u_0$. Then, a generalized Grenander-type estimator takes the form
\begin{equation*}
	\widehat{\theta}_n(\x) = \partial_{-}\GCM_{[0,\widehat{u}_n]}\big(\widehat{\Gamma}_n\circ\widehat{\Phi}_n^{-}\big) \circ \widehat{\Phi}_n(\x).
\end{equation*}
Below we analyze the limiting distribution of $\widehat{\theta}_n(\x)$ and its bootstrap-based distributional approximation.

\subsection{Asymptotic distribution}\label{Appendix Section Asymptotic Distribution}

Let $\q=\min\{j\in\mathbb{N}:\partial^{j}\theta_{0}(\x)\neq0\}$ be the characteristic exponent of $\theta_0(\x)$ and $\mathcal{M}_{\x}^{\q}(v)=\frac{\partial^{\q}\theta_{0}(\x)\partial\Phi_{0}(\x)}{(\q+1)!}v^{\q+1}$.
\begin{assumption}\label{Appendix Assumption A}
For some $\delta > 0 $, $\s \geq 1$, and $\q\in\mathbb{N}$,
\begin{enumerate}[label=\normalfont(\arabic*),noitemsep,itemindent=*]
\item $I\subseteq \mathbb{R}$ is an interval and $I_{\x}^{\delta}:=\{x:|x-\x|\leq\delta\}\subseteq I$.
\item \label{Appendix Assumption A theta0}
	$\theta_0:I\to\mathbb{R}$ is non-decreasing. In addition, $\theta_0(x)$ is $\lfloor\s\rfloor$-times continuously differentiable on $I_{\x}^{\delta}$ with
	\begin{equation*}
	    \sup_{x\neq x'\in I_{\x}^{\delta}} \frac{|\partial^{\lfloor\s\rfloor}\theta_0(x)-\partial^{\lfloor\s\rfloor}\theta_0(x')|}{|x-x'|^{\s-\lfloor\s\rfloor}} <\infty. 
	\end{equation*}
	Also, $\q\leq\lfloor\s\rfloor$.

    \item \label{Appendix Assumption A Phi0}
    $\Phi_0:I\to [0,u_0]$ is non-decreasing and right-continuous. In addition, $\Phi_0(x)$ is $(\lfloor\s\rfloor-\q+1)$-times continuously differentiable on $I_{\x}^{\delta}$ with $\partial \Phi_0(\x)\neq 0$ and
	\begin{equation*}
	    \sup_{x\neq x'\in I_{\x}^{\delta}} \frac{|\partial^{\lfloor\s\rfloor-\q+1}\theta_0(x)-\partial^{\lfloor\s\rfloor-\q+1}\theta_0(x')|}{|x-x'|^{\s-\lfloor\s\rfloor}} <\infty. 
	\end{equation*}
\end{enumerate}
\end{assumption}
Define $a_n=n^{1/(2\q+1)}$ and
\begin{align*}
    \widehat{G}_{\x,n}^{\q}(v) &= \sqrt{na_{n}}[\widehat{\Gamma}_{n}(\x+va_{n}^{-1})-\widehat{\Gamma}_{n}(\x)-\Gamma_{0}(\x+va_{n}^{-1})+\Gamma_{0}(\x)]\\
    &\quad -\theta_{0}(\x)\sqrt{na_{n}}[\widehat{\Phi}_{n}(\x+va_{n}^{-1})-\widehat{\Phi}_{n}(\x)-\Phi_{0}(\x+va_{n}^{-1})+\Phi_{0}(\x)].
\end{align*}
\begin{assumption}\label{Appendix Assumption B non-bootstrap}\hspace{1pt}
\begin{enumerate}[label=\normalfont(\arabic*),noitemsep,itemindent=*]   \item \label{Appendix Assumption B non-bootstrap Weak Convergence} 
	$\widehat{G}_{\x,n}^{\q}\leadsto \mathcal{G}_{\x}$ where $\mathcal{G}_{\x}$ is a mean-zero Gaussian process with covariance kernel $\mathcal{C}_\x$.

	
	\item \label{Appendix Assumption B non-bootstrap Uniform Consistency}
	$\sup_{x\in I} |\widehat{\Gamma}_n(x)- \Gamma_0(x)|=o_{\P}(1)$.

	\item \label{Appendix Assumption B non-bootstrap Uniform Convergence Rate}
	$\widehat{\Phi}_n$ is non-decreasing and right-continuous. 
	For any $K >0$, 
	\begin{equation*}
	   a_n \sup_{|v|\leq K}\big|\widehat{\Phi}_n(\x+va_n^{-1})-\Phi_0(\x+va_n^{-1}) \big| = o_\P(1).
	\end{equation*}
	Also, $\sup_{x\in I} |\widehat{\Phi}_n(x) - \Phi_0(x)|=o_{\P}(1)$.
	
	\item \label{Appendix Assumption B non-bootstrap Covariance Kernel} 
	For every $s,t\in\mathbb{R}$,
	\begin{equation*}
	    \mathcal{C}_{\x}(s+t,s+t)-\mathcal{C}_{\x}(s+t,s)-\mathcal{C}_{\x}(s,s+t)+\mathcal{C}_{\x}(s,s)=\mathcal{C}_{\x}(t,t)
	\end{equation*}
	and
	\begin{equation*}
	    \mathcal{C}_{\x}(s\tau ,t\tau )=\mathcal{C}_{\x}(s,t)\tau \qquad\text{for every } \tau\geq 0.
	\end{equation*}
	In addition, $\mathcal{C}_{\x}(1,1)>0$ and $\lim_{\delta\downarrow0}\mathcal{C}_{\x}(1,\delta)/\sqrt{\delta}=0$.
	   
	
	\item \label{Appendix Assumption B non-bootstrap Support Consistency}
    For some $u_0>\Phi_0(\x)$, $\widehat{u}_n \geq u_0+o_{\P}(1)$.

    Also, $\{0,\widehat{u}_n\} \subseteq \widehat{\Phi}_n(I)$, and $\widehat{\Phi}_n^{-}([0,\widehat{u}_n])$ and $\widehat{\Phi}_n(I) \cap [0,\widehat{u}_n]$ are closed.
\end{enumerate}
\end{assumption}


\begin{theorem}\label{Appendix Theorem: Asymptotic Distribution - High-level}
	Under Assumptions \ref{Appendix Assumption A} and \ref{Appendix Assumption B non-bootstrap},
	\begin{equation*}
		r_n(\widehat{\theta}_n(\mathsf{x}) - \theta_0(\mathsf{x}) ) \leadsto (\partial \Phi_0(\mathsf{x}))^{-1} \partial_{-}\GCM_{\mathbb{R}}\{\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q}\}(0). 
	\end{equation*}
\end{theorem}
where $r_n=n^{\frac{\q}{2\q+1}}$.

\subsection{Continuity of the limiting distribution}
The proof of Thereom \ref{Appendix Theorem: Asymptotic Distribution - High-level} relies on the continuity of the distribution function
\begin{equation*}
    x\mapsto \mathbb{P}\Big[ \argmax_{v\in\mathbb{R}} \Big\{\mathcal{G}_{\x}(v) - \mathcal{M}_{\x}^{\q}(v) + zv \Big\}\leq x \Big]
\end{equation*}
at $x=0$ for each $z\in\mathbb{R}$. To show the continuity of the distribution function, we present a general lemma. Let $\{\mathbb{G}(s):s\in\mathbb{R}\}$ be a Gaussian process with mean function $\mu$ and covariance kernel $\mathcal{K}$.
\begin{lemma}\label{Appendix Lemma Continuity Distribution Function}
    The Gaussian process $\{\mathbb{G}(s):s\in\mathbb{R}\}$ has continuous sample paths, for every $ \tau> 0$, $s,t\in\mathbb{R}$, $\mathcal{K}(s\tau ,t\tau )=\mathcal{K}(s,t)\tau$ and $\mathcal{K}(s+t,s+t)-\mathcal{K}(s+t,s)-\mathcal{K}(s,s+t)+\mathcal{K}(s,s)=\mathcal{K}(t,t)$, $\mathcal{K}(1,1)>0$, and $\lim_{\delta\downarrow0}\mathcal{K}(1,\delta)/\sqrt{\delta}=0$. Also, $\limsup_{|s|\to\infty}\mu(s) /|s|^{c} = - \infty$ for some $c>1$ and $\lim_{\tau \downarrow}|\mu(x+\tau)-\mu(x)|/\sqrt{\tau}=0$ for each $x\in\mathbb{R}$. Then, a unique maximizer of $\mathbb{G}(s)$ exists with probability one, and the distribution function 
    \begin{equation*}
        x\mapsto\mathbb{P}\Big[\argmax_{s\in\mathbb{R}}\mathbb{G}(s)\leq x\Big]
    \end{equation*}
    is continuous.
\end{lemma}


\subsection{Bootstrap approximation}
Let $(\widehat{\Gamma}_{n}^{\ast}(x),\widehat{\Phi}_{n}^{\ast}(x),\widehat{u}_n^*)$ be the bootstrap version of $(\widehat{\Gamma}_n(x), \widehat{\Phi}_n(x),\widehat{u}_n)$, and 
\begin{equation*}
    \widetilde{\Gamma}_{n}^{\ast}(x)=\widehat{\Gamma}_{n}^{\ast}(x)-\widehat{\Gamma}_{n}(x)+\widehat{\theta}_{n}(\x)\widehat{\Phi}_{n}(x)+\widetilde{M}_{\x,n}(x-\x)
\end{equation*}
where $\widetilde{M}_{\x,n}(v)$ is an estimator of $\mathcal{M}_{\x}^{\q}$ as discussed in the main paper.
Our bootstrap distributional approximation is based on the estimator
\begin{equation*}
    \widetilde{\theta}_n(\x) = \partial_{-}\GCM_{[0,\widehat{u}_n^*]}\big(\widetilde{\Gamma}_{n}^{\ast}\circ(\widehat{\Gamma}_n^{\ast})^{-}\big)\circ \widehat{\Gamma}_n^{\ast}(\x).
\end{equation*}
Define
\begin{align*}
    \widehat{G}_{\x,n}^{\q,\ast}(v)
    & =\sqrt{na_{n}}[\widehat{\Gamma}_{n}^{\ast}(\x+va_{n}^{-1})-\widehat{\Gamma}_{n}^{\ast}(\x)-\widehat{\Gamma}_{n}(\x+va_{n}^{-1})+\widehat{\Gamma}_{n}(\x)]\\
    & -\widehat{\theta}_{n}(\x)\sqrt{na_{n}}[\widehat{\Phi}_{n}^{\ast}(\x+va_{n}^{-1})-\widehat{\Phi}_{n}^{\ast}(\x)-\widehat{\Phi}_{n}(\x+va_{n}^{-1})+\widehat{\Phi}_{n}(\x)].
\end{align*}
We impose the following conditions to analyze the bootstrap procedure.
\begin{assumption}\label{Appendix Assumption B Bootstrap Approximation} \hspace{1pt}
\begin{enumerate}[label=(\arabic*),itemindent=*]
	\item \label{Appendix Assumption B: Bootstrap Weak Convergence}
	$\widehat{G}_{n,\x}^{\q,*}\leadsto_{\mathbb{P}}\mathcal{G}_{\x}$.
	
	\item \label{Appendix Assumption B: Bootstrap Gamma_hat}
	$\sup_{x\in I}|\widehat{\Gamma}_n^*(x)-\widehat{\Gamma}_n(x)| = o_{\mathbb{P}}(1)$.
	
	\item \label{Appendix Assumption B: Bootstrap Phi_hat}
	$\widehat{\Phi}_n^*$ is non-decreasing and right-continuous.
	For any $K >0$, 
	\begin{equation*}
	   a_n \sup_{|v|\leq K}\big|\widehat{\Phi}_n^*(\x+va_n^{-1})-\widehat{\Phi}_n(\x+va_n^{-1})\big| = o_\P(1).
	\end{equation*}
	Also, $\sup_{x\in I}|\widehat{\Phi}_n^*(x)-\widehat{\Phi}_n(x)| = o_{\mathbb{P}}(1)$.
	
	\item \label{Appendix Assumption B: Boot uhat}
	For the same $\widehat{u}_n$ in Assumption \ref{Appendix Assumption B non-bootstrap}, $\widehat{u}_n^* \geq \widehat{u}_n+o_{\P}(1)$.

    Also, $\{0,\widehat{u}_n^*\} \subseteq \widehat{\Phi}_n^*(I)$, and $(\widehat{\Phi}_n^*)^{-}([0,\widehat{u}_n^*])$ and $\widehat{\Phi}_n^*(I) \cap [0,\widehat{u}_n^*]$ are closed.
\end{enumerate}
\end{assumption}
Define 
\begin{equation*}
    \widetilde{M}_{\x,n}^{\q}(v) = \sqrt{na_n}\widetilde{M}_{\x,n}(va_n^{-1}).
\end{equation*}

\begin{assumption}\label{Appendix Assumption: mean function estimation} 
    $\widetilde{M}_{\x,n}^{\q} \leadsto_{\mathbb{P}} \mathcal{M}_{\x}^{\q}$ and for every $K>0$,
    \begin{equation*}
	    \liminf_{\delta\downarrow 0}\liminf_{n\to\infty} \mathbb{P}\Big[ \inf_{|v|>K^{-1}}\widetilde{M}_{\x,n}(v)\geq \delta \Big]=1\quad \text{ for every }K>0.
	\end{equation*}
\end{assumption}

\begin{theorem}\label{Appendix Theorem: Bootstrap Approximation - High-level}
	Suppose Assumptions \ref{Appendix Assumption A}, \ref{Appendix Assumption B non-bootstrap}, \ref{Appendix Assumption B Bootstrap Approximation}, and \ref{Appendix Assumption: mean function estimation} hold.
	Then,
	\begin{equation*}
		r_n(\widetilde{\theta}_n^*(\mathsf{x}) - \widehat{\theta}_n(\mathsf{x}) ) \leadsto_{\P} (\partial \Phi_0(\mathsf{x}))^{-1} \partial_{-}\GCM_{\mathbb{R}}\{\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q}\}(0). 
	\end{equation*}
 where $r_n=n^{\frac{\q}{2\q+1}}$.
\end{theorem}
By Lemma \ref{Appendix Lemma Continuity Distribution Function}, the limit distribution is continuous, and thus, we have
\begin{equation*}
    \sup_{t\in\mathbb{R}}\Big|\mathbb{P}_n^*\Big[\widetilde{\theta}_n^*(\mathsf{x}) - \widehat{\theta}_n(\mathsf{x})\leq t\Big] - \mathbb{P}\Big[\widehat{\theta}_n(\x)-\theta_0(\x)\leq t\Big] \Big| = o_{\mathbb{P}}(1)
\end{equation*}
where $\mathbb{P}_n^*$ is the bootstrap probability measure conditional on the data.
\section{Implementation}

\subsection{Mean function estimation}
Here we consider a construction of $\widetilde{M}_{\x,n}^{\q}$ and proivde a set of sufficient conditions implying Assumption \ref{Appendix Assumption: mean function estimation}.

For $j=1,\dots,\lfloor\s\rfloor$, let
\begin{equation*}
    \mathcal{D}_j(\x) = \frac{\partial^{j+1}\Upsilon_0(\x)}{(j+1)!},\qquad \Upsilon_0(x) = \Gamma_0(x) - \theta_0(\x)\Phi_0(x).
\end{equation*}
Under Assumption \ref{Appendix Assumption A}, $\mathcal{M}_{\x}^{\q}(v) = \mathcal{D}_{\q}(\x)v^{\q+1}$.

In the main paper, we considered the following estimators of $\mathcal{D}_{j}(\x)$:
\begin{equation*}
    \widetilde{\mathcal{D}}^{\mathtt{MA}}_{j,n}(\x) = \epsilon_n^{-(j+1)}[\widehat{\Upsilon}_n(\x+\epsilon_n) - \widehat{\Upsilon}_n(\x) ],
\end{equation*}
\begin{equation*}
    \widetilde{\mathcal{D}}^{\mathtt{FD}}_{j,n}(\x) = \epsilon_n^{-(j+1)}\sum_{k=1}^{j+1}(-1)^{k+j+1}\binom{j+1}{k}[\widehat{\Upsilon}_n(\x+k\epsilon_n) - \widehat{\Upsilon}_n(\x) ],
\end{equation*}
\begin{equation*}
    \widetilde{\mathcal{D}}^{\mathtt{BR}}_{j,n}(\x) = \epsilon_n^{-(j+1)} \sum_{k=1}^{\underline{\s}+1}\lambda_j^{\mathtt{BR}}(k)[\widehat{\Upsilon}_n(\x+c_k\epsilon_n) - \widehat{\Upsilon}_n(\x) ],
\end{equation*}
where $\epsilon_n >0$ is a vanishing sequence of tuning parameters, $\widehat{\Upsilon}_n(x) = \widehat{\Gamma}_n(x) -\widehat{\theta}_n(\x)\widehat{\Phi}_n(x)$, the integer $\underline{\s}$ is chosen by a researcher and assumed to satisfy $\underline{\s}\leq \s$, $\{c_k:1\leq k\leq \underline{\s}+1\}$ are user chosen constants, and the scalars $\{\lambda_j^{\mathtt{BR}}(k):1\leq k\leq \underline{\s}+1\}$ are defined by the property
\begin{equation*}
    \sum_{k=1}^{\underline{\s}+1} \lambda_j^{\mathtt{BR}}(k) c_k^p=\mathbbm{1}\{p=j+1\},\qquad p=1,\dots,\underline{\s}+1.
\end{equation*}
To analyze properties of the above numerical derivative estimators, for $\delta > 0$, define
\begin{align*}
    \widehat{G}_{\x,n}(v;\delta) &= \sqrt{n\delta^{-1}} \big[ \widehat{\Gamma}_n(\x+v\delta)-\widehat{\Gamma}_n(\x) - \Gamma_0(\x+v\delta)+\Gamma_0(\x)\big]\\
    &-\theta_0(\x) \sqrt{n\delta^{-1}} \big[ \widehat{\Phi}_n(\x+v\delta)-\widehat{\Phi}_n(\x) - \Phi_0(\x+v\delta)+\Phi_0(\x)\big],
\end{align*}
and
\begin{equation*}
    \widehat{R}_{\x,n}(v;\delta) =\delta^{-1}\big[\widehat{\Phi}_n(\x+v\delta)-\widehat{\Phi}_n(\x) - \Phi_0(\x+v\delta)+\Phi_0(\x)\big].
\end{equation*}

\begin{assumption}\label{Appendix Assumption D}
    For the same $\q$ as in Assumption \ref{Appendix Assumption A} and for every $\delta_n>0$ with $\delta_n=o(1)$ and $a_n^{-1}\delta_n^{-1}=O(1)$,
    \begin{equation*}
        \widehat{G}_{\x,n}(1;\delta_n) = O_{\mathbb{P}}(1)\qquad\text{and}\qquad \widehat{R}_{\x,n}(1;\delta_n) = o_{\mathbb{P}}(1).
    \end{equation*}
\end{assumption}

\begin{lemma}\label{Appendix Lemma: numerical derivative estimators}
    Suppose Assumptions \ref{Appendix Assumption A} and \ref{Appendix Assumption D} are satisfied and that $r_n(\widehat{\theta}_n(\x)-\theta_0(\x))=O_{\mathbb{P}}(1)$. If $\epsilon_n=o(1)$ and $a_n^{-1}\epsilon_n^{-1}=o(1)$, then
    \begin{equation*}
        \widetilde{\mathcal{D}}_{\q,n}(\x)\to_{\mathbb{P}} \mathcal{D}_{\q}(\x),\qquad \widetilde{\mathcal{D}}_{\q,n} \in \{\widetilde{\mathcal{D}}_{\q,n}^{\mathtt{MA}},\widetilde{\mathcal{D}}_{\q,n}^{\mathtt{FD}}, \widetilde{\mathcal{D}}_{\q,n}^{\mathtt{BR}}\}
    \end{equation*}
    and
    \begin{equation*}
        a_n^{\q-j} \big( \widetilde{\mathcal{D}}_{j,n}^{\mathtt{BR}}-\mathcal{D}_{j}(\x) \big) = O\big(a_n^{\q-j}\epsilon_n^{\min(\underline{\s}+1,\s)-j} \big) + o_{\mathbb{P}}(1),\qquad j=1,\dots,\s.
    \end{equation*}
    In particular, if $3\leq \bar{\q} < \s$, then 
    \begin{equation}\label{Appendix eq: ND convergence rate lemma}
        a_n^{\q-(2\ell-1)}\big(\widetilde{\mathcal{D}}_{j,n}^{\mathtt{BR}}-\mathcal{D}_{j}(\x)\big) = o_{\mathbb{P}}(1),\qquad \ell=1,\dots,\lfloor(\bar{\q}+1)/2\rfloor,
    \end{equation}
    holds provided that $n\epsilon_n^{1+2\bar{\q}\min(\underline{\s},\s-1)/(\bar{\q}-1)}\to 0$ and $n\epsilon_n^{1+2\bar{\q}}\to\infty$.
\end{lemma}


\subsection{Bootstrap}\label{Appendix section bootstrap primitive conditions}
In the main paper, we considered how to construct bootstrap estimators $(\widehat{\Gamma}_n^{\ast},\widehat{\Phi}_n^{\ast})$ and provided primitive conditions that can be used to verify Assumption \ref{Appendix Assumption B Bootstrap Approximation}.
Specifically, we assumed that the non-bootstrap estimators admit large-sample approximations
\begin{equation*}
    \widehat{\Gamma}_n(x) \approx \bar{\Gamma}_n(x) = \frac{1}{n}\sum_{i=1}^n \gamma_0(x;\mathbf{Z}_i)\qquad\text{and}\qquad \widehat{\Phi}_n(x) \approx \bar{\Phi}_n(x) = \frac{1}{n}\sum_{i=1}^n \phi_0(x;\mathbf{Z}_i).
\end{equation*}
Denoting estimators of $\gamma_0$ and $\phi_0$ by $\widehat{\gamma}_n$ and $\widehat{\phi}_n$, we consider the bootstrap estimators 
\begin{equation*}
    \widehat{\Gamma}_n^{\ast}(x) = \frac{1}{n}\sum_{i=1}^n W_{i,n}\widehat{\gamma}_n(x;\mathbf{Z}_i)\qquad\text{and}\qquad \widehat{\Phi}_n^{\ast}(x)=\frac{1}{n}\sum_{i=1}^n W_{i,n}\widehat{\phi}_n(x;\mathbf{Z}_i),
\end{equation*}
where $W_{1,n},\dots,W_{n,n}$ denote exchangeable random variables, independent of $(\mathbf{Z}_1,\dots,\mathbf{Z}_n,\widehat{\gamma}_n,\widehat{\phi}_n)$.

Let
\begin{equation*}
    \psi_{\x}(v;\mathbf{z}) = \gamma_0(\x+v;\mathbf{z}) - \gamma_0(\x;\mathbf{z}) -\theta_0(\x)[\phi_0(\x+v;\mathbf{z})-\phi_0(\x;\mathbf{z})],
\end{equation*}
and
\begin{equation*}
    \bar{\Gamma}_n^{\ast}(x)=\frac{1}{n}\sum_{i=1}^nW_{i,n}\gamma_0(x;\mathbf{Z}_i)\qquad\text{and}\qquad\bar{\Phi}_n^{\ast}(x) = \frac{1}{n}\sum_{i=1}^n W_{i,n}\phi_0(x;\mathbf{Z}_i).
\end{equation*}
For any class of functions $\mathfrak{F}$, let $N_{U}(\varepsilon,\mathfrak{F})$ denote the associated uniform covering number relative to $L_2$; that is, for any $\varepsilon>0$, let
\begin{equation*}
    N_{U}(\varepsilon,\mathfrak{F}) = \sup_{Q}N(\varepsilon\|\bar{F}\|_{Q,2},\mathfrak{F},L_2(Q)),
\end{equation*}
where $\bar{F}$ is the minimal envelope function of $\mathfrak{F}$, $\|\cdot\|_{Q,2}$ is the $L_2(Q)$ norm, $N(\cdot)$ is the covering number, and the supermum is taken over every discrete probability measure $Q$ with $\|\bar{F}\|_{Q,2}>0$.

\begin{assumption}\label{Appendix Assumption E}
    For the same $\q$ as in Assumption \ref{Appendix Assumption A}, the following are satisfied:
    \begin{enumerate}[label=\normalfont(\arabic*),noitemsep,itemindent=*]
        \item \label{Appendix Assumption E: iid sample}
        $\mathbf{Z}_1,\mathbf{Z}_2,\dots$ are independent and identifcally distributed.

        \item \label{Appendix Assumption E: bootstrap weights}
        For each $n\in\mathbb{N}$, $W_{1,n},\dots,W_{n,n}$ are exchangeable random variables independent of $\mathbf{Z}_1,\dots,\mathbf{Z}_n,\widehat{\gamma}_n,\widehat{\phi}_n$. In addition, for some $\mathfrak{r}>(4\q+2)/(2\q-1)$,
        \begin{equation*}
            \frac{1}{n}\sum_{i=1}^n W_{i,n}=1,\qquad\frac{1}{n}\sum_{i=1}^n(W_{i,n}-1)^2\to_{\mathbb{P}}1,\qquad\text{and}\qquad\mathbb{E}[|W_{1,n}|^{\mathfrak{r}}] =O(1).
        \end{equation*}
        
        \item \label{Appendix Assumption E: gamma hat}
        $\sup_{x\in I}|\widehat{\Gamma}_n(x)-\bar{\Gamma}_n(x)|=o_{\mathbb{P}}(1)$ and $\frac{1}{n}\sum_{i=1}^n\sup_{x\in I}|\widehat{\gamma}_n(x;\mathbf{Z}_i)-\gamma_0(x;\mathbf{Z}_i)|^2=o_{\mathbb{P}}(1)$. For every $K>0$,
        \begin{equation*}
            \sqrt{na_n} \sup_{|v|\leq K}\big|\widehat{\Gamma}_n(\x+va_n^{-1}) -\widehat{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+va_n^{-1}) + \bar{\Gamma}_n(\x)\big|=o_{\mathbb{P}}(1),
        \end{equation*}
        and
        \begin{equation*}
            a_n \frac{1}{n}\sum_{i=1}^n\sup_{|v|\leq K}\big|\widehat{\gamma}_n(\x+va_n^{-1};\mathbf{Z}_i) -\widehat{\gamma}_n(\x;\mathbf{Z}_i) -\gamma_0(\x+va_n^{-1};\mathbf{Z}_i) + \gamma_0(\x;\mathbf{Z}_i) \big|^2 =o_{\mathbb{P}}(1).
        \end{equation*}
        In addition, for some $V\in (0,2)$,
        \begin{equation*}
            \limsup_{\varepsilon\downarrow 0}\frac{\log N_{U}(\varepsilon,\mathfrak{F}_{\gamma})}{\varepsilon^{-V}} < \infty,\qquad\mathbb{E}[\bar{F}_{\gamma}(\mathbf{Z})^2]<\infty, \qquad \limsup_{\varepsilon\downarrow 0} \frac{\log N_{U}(\varepsilon,\hat{\mathfrak{F}}_{\gamma,n})}{\varepsilon^{-V}} = O_{\mathbb{P}}(1),
        \end{equation*}
        where $\mathfrak{F}_{\gamma}=\{\gamma_0(x;\cdot):x\in I\}$, $\bar{F}_{\gamma}$ is its minimal envelope, and $\hat{\mathfrak{F}}_{\gamma,n}=\{\widehat{\gamma}_n(x;\cdot):x\in I\}$. Also,
        \begin{equation*}
            \limsup_{\delta\downarrow 0} \frac{\mathbb{E}[\bar{D}_{\gamma}^{\delta}(\mathbf{Z})^2+\bar{D}_{\gamma}^{\delta}(\mathbf{Z})^4]}{\delta} <\infty,
        \end{equation*}
        where $\bar{D}_{\gamma}^{\delta}$ is the minimal envelope of $\{\gamma_0(x;\cdot)-\gamma_0(\x;\cdot):x\in I_{\x}^{\delta}\}$.

        \item \label{Appendix Assumption E: phi hat}
        $\widehat{\Phi}_n,\widehat{\Phi}_n^{\ast}$ are non-decreasing and right-continuous on $I$. 
        
        $\sup_{x\in I}|\widehat{\Phi}_n(x)-\bar{\Phi}_n(x)|=o_{\mathbb{P}}(1)$, $\frac{1}{n}\sum_{i=1}^n\sup_{x\in I}|\widehat{\phi}_n(x;\mathbf{Z}_i)-\phi_0(x;\mathbf{Z}_i)|^2=o_{\mathbb{P}}(1)$, and $a_n|\widehat{\Phi}_n(\x)-\bar{\Phi}_n(\x)|=o_{\mathbb{P}}(1)$. For every $K>0$,
        \begin{equation*}
            \sqrt{na_n} \sup_{|v|\leq K}\big|\widehat{\Phi}_n(\x+va_n^{-1}) -\widehat{\Phi}_n(\x) - \bar{\Phi}_n(\x+va_n^{-1}) + \bar{\Phi}_n(\x)\big|=o_{\mathbb{P}}(1),
        \end{equation*}
        and
        \begin{equation*}
            a_n \frac{1}{n}\sum_{i=1}^n\sup_{|v|\leq K}\big|\widehat{\phi}_n(\x+va_n^{-1};\mathbf{Z}_i) -\widehat{\phi}_n(\x;\mathbf{Z}_i) -\phi_0(\x+va_n^{-1};\mathbf{Z}_i) + \phi_0(\x;\mathbf{Z}_i) \big|^2 =o_{\mathbb{P}}(1).
        \end{equation*}
        In addition, for some $V\in (0,2)$,
        \begin{equation*}
            \limsup_{\varepsilon\downarrow 0}\frac{\log N_{U}(\varepsilon,\mathfrak{F}_{\phi})}{\varepsilon^{-V}} < \infty,\qquad\mathbb{E}[\bar{F}_{\phi}(\mathbf{Z})^2]<\infty, \qquad \limsup_{\varepsilon\downarrow 0} \frac{\log N_{U}(\varepsilon,\hat{\mathfrak{F}}_{\phi,n})}{\varepsilon^{-V}} = O_{\mathbb{P}}(1),
        \end{equation*}
        where $\mathfrak{F}_{\phi}=\{\phi_0(x;\cdot):x\in I\}$, $\bar{F}_{\phi}$ is its minimal envelope, and $\hat{\mathfrak{F}}_{\phi,n}=\{\widehat{\phi}_n(x;\cdot):x\in I\}$. Also,
        \begin{equation*}
            \limsup_{\delta\downarrow 0} \frac{\mathbb{E}[\bar{D}_{\phi}^{\delta}(\mathbf{Z})^2+\bar{D}_{\phi}^{\delta}(\mathbf{Z})^4]}{\delta} <\infty,
        \end{equation*}
        where $\bar{D}_{\phi}^{\delta}$ is the minimal envelope of $\{\phi_0(x;\cdot)-\phi_0(\x;\cdot):x\in I_{\x}^{\delta}\}$.

        \item \label{Appendix Assumption E: covariance kernel}
        For every $\delta_n>0$ with $a_n\delta_n=O(1)$,
        \begin{equation*}
            \sup_{v\neq v'\in[-\delta_n,\delta_n]}\frac{\mathbb{E}[|\psi_{\x}(v;\mathbf{Z})-\psi_{\x}(v';\mathbf{Z})|]}{|v-v'|} = O(1)
        \end{equation*}
        and for all $s,t\in\mathbb{R}$, and for some $\mathcal{C}_{\x}$,
        \begin{equation*}
            \frac{\mathbb{E}[\psi_{\x}(s\delta_n;\mathbf{Z})\psi_{\x}(t\delta_n;\mathbf{Z})]}{\delta_n} \to \mathcal{C}_{\x}(s,t).
        \end{equation*}
    \end{enumerate}
\end{assumption}
\begin{lemma}\label{Appendix Lemma: Bootstrap primitive conditions}
    Suppose Assumptions \ref{Appendix Assumption A} and \ref{Appendix Assumption E} are satisfied. Then, Assumption \ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Weak Convergence}-\ref{Appendix Assumption B non-bootstrap Uniform Convergence Rate} and Assumption \ref{Appendix Assumption B Bootstrap Approximation} \ref{Appendix Assumption B: Bootstrap Weak Convergence}-\ref{Appendix Assumption B: Bootstrap Phi_hat} are satisfied. If also
    \begin{equation*}
        \sqrt{n\delta_n^{-1}} \big[\widehat{\Gamma}_n(\x+\delta_n) -\widehat{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+\delta_n)+\bar{\Gamma}_n(\x)\big] = O_{\mathbb{P}}(1)
    \end{equation*}
    and
    \begin{equation*}
        \sqrt{n\delta_n^{-1}} \big[\widehat{\Phi}_n(\x+\delta_n) -\widehat{\Phi}_n(\x) - \bar{\Phi}_n(\x+\delta_n)+\bar{\Phi}_n(\x)\big] = O_{\mathbb{P}}(1)
    \end{equation*}
    for every $\delta_n>0$ with $\delta_n=o(1)$ and $a_n^{-1}\delta_n^{-1}=O(1)$, then Assumption \ref{Appendix Assumption D} is satisfied.
\end{lemma}




\section{Bootstrap inconsistency}
In this section, we formally show the inconsistency of bootstrap distribution approximations.
Consider the ``na\"ive'' bootstrap estimator
\begin{equation*}
	\widehat{\theta}_n^*(\mathsf{x}) = \big(\partial_{-} \GCM_{[0,\widehat{u}_n^*]}\big(\widehat{\Gamma}_n^*\circ (\widehat{\Phi}_n^*)^{-} \big) \big) \circ \widehat{\Phi}_n^*(\mathsf{x})
\end{equation*}
where
\begin{equation*}
	\widehat{\Gamma}_n^*(x) = \frac{1}{n}\sum_{i=1}^n W_{i,n} \widehat{\gamma}_n(x;\mathbf{Z}_i), \qquad \widehat{\Phi}_n^*(x) = \frac{1}{n}\sum_{i=1}^n W_{i,n}\widehat{\phi}_n(x;\mathbf{Z}_i).
\end{equation*}
\begin{theorem}\label{Appendix Theorem: Bootstrap inconsistency}
    Suppose Assumptions \ref{Appendix Assumption A}, \ref{Appendix Assumption B non-bootstrap}, and \ref{Appendix Assumption B Bootstrap Approximation} hold.
    Then, $n^{\frac{\q}{2\q+1}}(\widehat{\theta}_n^*(\mathsf{x})-\widehat{\theta}_n(\x))\not\leadsto_{\mathbb{P}}(\partial\Phi_0(\x))^{-1}\partial_{-}\GCM_{\mathbb{R}}(\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q})(0)$ i.e.,\ the bootstrap approximation fails.
\end{theorem}
This theorem implies the well-known result of the bootstrap inconsistency for Grenander estimator \cite[e.g.,][]{Kosorok_2008_BookCh,Sen-Banerjee-Woodroofe_2010_AoS}.
Our result accommodates exchangeable bootstrap schemes and a wide class of generalized Grenander-type estimators.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Examples}\label{Appendix Section: Examples}
Let $\mathbf{Z}_i=(Y_i,\check{X}_i,\Delta_i,\mathbf{A}_i)'$, $i=1,2,\dots,n$, be an observed random sample with $\check{X}_i=\min\{X_i,C_i\}$, $\Delta_i=\1(X_i\leq C_i)$, and $\mathbf{A}_i$ denoting additional covariates. Assuming that $F_0(x)=\P[X_i\leq x]$ is absolutely continuous on $I$, a Lebesgue density function is denoted by $f_0(x)=\partial F_0(x)$. If $\P[C_i\geq X_i]=1$, then there is no (right) censoring and $\check{X}_i=X_i$. Other basic quantities of interest are the survival function $S_0(x)=\P[X_i> x]$ and the mean function $\mu_0(X_i)=\E[Y_i|X_i]$, as well as their conditional on $\mathbf{A}_i$ analogues $S_0(x|\mathbf{A}_i)=\P[X_i> x|\mathbf{A}_i]$ and $\mu_0(X_i,\mathbf{A}_i)=\E[Y_i|X_i,\mathbf{A}_i]$. $\q$ is as defined in Assumption \ref{Appendix Assumption A}.
Recall that we write $\partial^{\ell} g(x)$ for the $\ell$-th derivative of a smooth function $g$ and we use the convention $\partial^0g(x)=g(x)$.

The examples below consider monotone estimation of $f_0(\x)$, $\mu_0(\x)$, $f_0(\x)/S_0(\x)$ and $F_0(\x)$, under various assumptions related to censoring and covariate-adjustment.
For each example, we provide a set of primitive conditions that imply Assumptions \ref{Appendix Assumption A}, \ref{Appendix Assumption B non-bootstrap}, \ref{Appendix Assumption B Bootstrap Approximation}, and \ref{Appendix Assumption D}. For brevity, we only describe the covariance kernel and the mean function of the limiting Gaussian process, and do not repeat the conclusions of Theorems \ref{Appendix Theorem: Asymptotic Distribution - High-level} and \ref{Appendix Theorem: Bootstrap Approximation - High-level}.

\subsection{Monotone density function}\label{Appendix Section: Example -- Monotone Density Estimation}
In this example, the parameter of interest is the density of $X$ at a point $\x$ i.e.,\ $\theta_0(\x)=f_0(\x)$.
We have $\Gamma_0(x) = F_0(x)$ and $\Phi_0(x)=x$. We take $\widehat{\Phi}_n(x)=\widehat{\Phi}_n^*(x)=x$. Also, for simplicity, we take $u_0$ as given.
\subsubsection{No censoring}
First, consider the canonical case of no censoring: $\P[C_i\geq u_0]=1$. The classical \citet{Grenander_1956_SAJ} estimator sets $\widehat{\Gamma}_{n}(x) = \frac{1}{n}\sum_{i=1}^{n} \1(X_i\leq x)$. The exchangeable bootstrap analogue is $\widehat{\Gamma}_{n}^*(x) = \frac{1}{n}\sum_{i=1}^{n} W_{i,n}\1(X_i\leq x)$. 

To analyze this example, we impose the following conditions.
\paragraph{Assumption \thesubsubsection} 
\begin{enumerate}[label=\normalfont(\arabic*),noitemsep,itemindent=*]
	\item The Lebesgue density $f_0$ of $X$ is non-decreasing on $I=[0,u_0]$ and $\mathsf{x}$ is in the interior of $I$.
	
	\item The density $f_0$ satisfies Assumption \ref{Appendix Assumption A} \ref{Appendix Assumption A theta0}.
\end{enumerate}
Under this assumption, the limit distribution of the Grenander estimator is characterized by
\begin{equation*}
    \mathcal{C}_{\x}(s,t) = f_0(\x) \min\{|s|,|t|\}\1\{\mathrm{sign}(s)=\mathrm{sign}(t)\}, \quad \mathcal{D}_{\q}(\x) = \frac{\partial^\q f_0(\x)}{(1+\q)!}. 
\end{equation*}

\subsubsection{Independent right-censoring}\label{Appendix Section: Example monotone density CAR}
Next, suppose that censoring occurs completely at random: $X_i\indep C_i$. \cite{Huang-Wellner_1995_SJS} analyzed this example and related problems. In this case, we take $\widehat{\Gamma}_{n}(x) = 1 - \widehat{S}_n(x)$, where $\widehat{S}_n$ denotes an estimator of the survival function $S_0(x)$. For concreteness, let $\widehat{S}_n$ be the Kaplan-Meier estimator. For bootstrap, one possibility is to use the non-parametric bootstrap to resample the Kaplan-Meier estimator. Another approach is to use our framework in Section \ref{Appendix section bootstrap primitive conditions}. For this purpose, let
\begin{equation*}
    \widehat{\gamma}_n(x;\mathbf{Z}) = \widehat{F}_n(x) + \widehat{S}_n(x)\bigg[\frac{\Delta\mathbbm{1}(\check{X}\leq x)}{\widehat{S}_n(\check{X})\widehat{G}_n(\check{X})} - \int_0^{\check{X}\wedge x} \frac{d\widehat{\Lambda}_n(u) }{\widehat{S}_n(u)\widehat{G}_n(u)} \bigg] 
\end{equation*}
where $\widehat{F}_n=1-\widehat{S}_n$, $\widehat{G}_n$ is the Kaplan Meier estimator for $G_0$, and $\widehat{\Lambda}_n$ is the cumulative hazard function associated with $\widehat{S}_n$ i.e.,\ $\widehat{\Lambda}_n(x) = \int_0^x \widehat{S}_n(u)^{-1}d\widehat{F}_n(u)$. Then, the bootstrap objective function is $\widehat{\Gamma}_{n}^*=\sum_{i=1}^nW_{i,n}\widehat{\gamma}_n(x;\mathbf{Z}_i)$.

To analyze this example, we impose the following conditions.
\paragraph{Assumption \thesubsubsection} 
\begin{enumerate}[label=\normalfont(\arabic*),noitemsep,itemindent=*]
	\item The Lebesgue density $f_0$ of $X$ is non-decreasing on $I=[0,u_0]$ and $\mathsf{x}$ is in the interior of $I$.
	
	\item $X\indep C$.
	
	\item The density $f_0$ satisfies Assumption \ref{Appendix Assumption A} \ref{Appendix Assumption A theta0}. $G_0(c) = \P[C_i>c]$ is continuous on $I$, and $S_0(u_0)G_0(u_0) > 0$.
\end{enumerate}
The last condition imposes that we set the interval $I$ to be a strict subset of the support of $X$.
The covariance kernel and the mean function in this setting have the form
\begin{equation*}
    \mathcal{C}_{\x}(s,t) = \frac{f_0(\x)}{G_0(\x)}\min\{|s|,|t|\}\mathbbm{1}\{\mathrm{sign}(x)=\mathrm{sign}(t)\},\quad \mathcal{D}_{\q}(\x) = \frac{\partial^\q f_0(\x)}{(1+\q)!}. 
\end{equation*}

\subsubsection{Conditionally independent right-censoring}\label{Appendix Section: Example monotone density conditionally independent}
We consider the case of censoring at random: $X_i\indep C_i|\mathbf{A}_i$. See \cite{vanderlaan-Robins_2003_Book,Zeng_2004_AoS} and references therein for existing analysis of this problem. We set 
\[ \widehat{\gamma}_n(x;\mathbf{Z})
  = \widehat{F}_n(x|\mathbf{A}) + \widehat{S}_n(x|\mathbf{A})
            \bigg[\frac{\Delta\1(\check{X}\leq x)}{\widehat{S}_n(\check{X}|\mathbf{A})\widehat{G}_n(\check{X}|\mathbf{A})} 
                 - \int_0^{\check{X}\wedge x} \frac{d\widehat{\Lambda}_n(u|\mathbf{A})}{\widehat{S}_n(u|\mathbf{A})\widehat{G}_n(u|\mathbf{A})}\bigg],
\]
where $\widehat{F}_n=1-\widehat{S}_n$, $\widehat{S}_n(x|\mathbf{A}),\widehat{G}_n(c|\mathbf{A})$ denote preliminary estimates of the conditional survival functions $S_0(x|\mathbf{A})=\P[X>x|\mathbf{A}]$, $G_0(c|\mathbf{A})=\P[C>c|\mathbf{A}]$, respectively, and $\widehat{\Lambda}_n(x|\mathbf{A}) =\int_0^x\frac{\widehat{F}_n(du|\mathbf{A})}{\widehat{S}_n(u|\mathbf{A})}$. Using this function, we define $\widehat{\Gamma}_n(x) = \frac{1}{n}\sum_{i=1}^n \widehat{\gamma}_n(x;\mathbf{Z}_i)$ and $\widehat{\Gamma}_n^{\ast}(x) = \frac{1}{n}\sum_{i=1}^nW_{i,n} \widehat{\gamma}_n(x;\mathbf{Z}_i)$.
Notice that we employ the original first-step estimates $\widehat{S}_n,\widehat{G}_n,\widehat{\Lambda}_n$ in the bootstrap objective function, so it is not necessary to bootstrap these preliminary estimators.

\paragraph{Assumption \thesubsubsection}
Let $\mathfrak{S}_n$, $\mathfrak{G}_n$ be sequences of function classes that contain $S_0(\cdot|\cdot)$, $G_0(\cdot|\cdot)$, respectively. 
\begin{enumerate}[label=\normalfont(\arabic*),noitemsep,itemindent=*]
	\item The Lebesgue density $f_0$ of $X$ is non-decreasing on $I=[0,u_0]$ and $\mathsf{x}$ is in the interior of $I$.
	
	\item $X\indep C |A$ and the density $f_0$ satisfies Assumption \ref{Appendix Assumption A} \ref{Appendix Assumption A theta0}.
    
	\item \label{Appendix Example: monotone density conditionally independent function classes} For each $S\in\mathfrak{S}_n$, $x\mapsto S(x|\mathbf{A})$ is non-increasing almost surely, and $\{S(x|\cdot):x\in I\}$ is a VC-subgraph class with the VC index bounded by a fixed constant. For all $S\in\mathfrak{S}_n$, $G\in \mathfrak{G}_n$, $0<c\leq S,G\leq C < \infty $. 

    %\item \label{Appendix Example: monotone density conditionally independent covering number}     For a mapping $(\tilde{X},\mathbf{A})\mapsto f(\tilde{X},\mathbf{A})$, let $\|f\|_{\infty,Q} = (Q \sup_{x\in I}|f(x,\cdot)|^2)^{1/2}$ where $Q$ is a measure on the support of $\mathbf{A}$.    For $\mathfrak{F}\in \{\{S(x|\cdot):x \in I, S\in \mathfrak{S}_n\},\{\Lambda(x|\cdot):x \in I, \Lambda\in \mathfrak{L}_n\},\mathfrak{S}_n,\mathfrak{G}_n,\mathfrak{L}_n\}$, let $\bar{F}$ be an envelope of $\mathfrak{F}$, and we assume $\limsup_{\varepsilon\downarrow 0}\log \sup_{Q} N(\varepsilon \|\bar{F}\|_{\infty,Q},\mathfrak{F},\|\cdot\|_{\infty,Q})\varepsilon^V<\infty$ for some $V\in (0,2)$ where the supremum is over discrete measures.

    \item \label{Appendix Example: monotone density conditionally independent high-level}
    For $K>0$, $\sqrt{na_n}\sup_{|v|\leq K}|\widehat{\Gamma}_n(\x+va_n^{-1})-\widehat{\Gamma}_n(\x)-\Gamma_0(\x+va_n^{-1})+\Gamma_0(\x)|=o_{\mathbb{P}}(1)$.
	
	\item
	With probability approaching one, $\widehat{S}_n\in\mathfrak{S}_n$ and $\widehat{G}_{n}\in\mathfrak{G}_n$. $a_n\frac{1}{n}\sum_{i=1}^n\sup_{x\in I}|\widehat{S}_n(x|\mathbf{A}_i)-S_0(x|\mathbf{A}_i)|^2=o_{\mathbb{P}}(1)$, and $a_n\frac{1}{n}\sum_{i=1}^n\sup_{x\in I}|\widehat{G}_n(x|\mathbf{A}_i)-G_0(x|\mathbf{A}_i)|^2=o_{\mathbb{P}}(1)$.
 
	\item The conditional distribution of $X$ given $\mathbf{A}$ has bounded Lebesgue density $f_{X|A}$, $\mathbb{E}[\frac{f_{X|A}(\x|\mathbf{A})}{G_0(\x|\mathbf{A})}]>0$, and there are real-valued functions $B,\omega$ such that $\mathbb{E}[B(\mathbf{A})]<\infty$, $\lim_{\delta\downarrow0}\omega(\delta)=0$, and for $|x-\x|$ sufficiently small, $|\frac{f_{X|A}(x|\mathbf{A})}{S_0(x|\mathbf{A})G_0(x|\mathbf{A})}-\frac{f_{X|A}(\x|\mathbf{A})}{S_0(\x|\mathbf{A})G_0(\x|\mathbf{A})}|\leq \omega(|x-\x|)B(\mathbf{A})$. 
\end{enumerate}
The condition \ref{Appendix Example: monotone density conditionally independent high-level} is high-level, and there are a few different approaches to verify them. See \cite{Westling-Carone_2020_AoS} for details.
The covariance kernel and the mean function are
\begin{equation*}
    \mathcal{C}_{\x}(s,t) = \mathbb{E}\Big[\frac{f_{X|A}(\x|\mathbf{A})}{G_0(\x|\mathbf{A})}\Big] \min\{|s|,|t|\}\1\{\mathrm{sign}(s)=\mathrm{sign}(t)\}, \quad \mathcal{D}_{\q}(\x) = \frac{\partial^\q f_0(\x)}{(1+\q)!}. 
\end{equation*}

\subsection{Monotone regression function}
The parameter of interest is the conditional mean function $\theta_0(\x)=\mu_0(\x)$ for the classical case and $\theta_0(\x)=\mathbb{E}[\mu_0(\x,\mathbf{A})]$ if covariates $\mathbf{A}$ are available. There is no censoring in this example. If $X\indep \mathbf{A}$, the two objects coincide, but they are not the same in general. We have $\Gamma_0(x) = \mathbb{E}[Y\mathbbm{1}\{X\leq x\}]$, $\Phi_0(x)=F_0(x)$, $I$ is the support of $X$, and $u_0=1$. 

\subsubsection{Classical case}\label{Appendix Section: Example monotone regression classical case}
First consider the case without covariates. The classical isotonic regression estimator of \cite{Ayer-Brunk-Ewing-Reid-Silverman_1955_AMS} sets $\widehat{\Gamma}_n(x) = \frac{1}{n}\sum_{i=1}^n Y_i\mathbbm{1}\{X_i\leq x\}$ and $\widehat{\Phi}_n(x) = \frac{1}{n}\sum_{i=1}^n \mathbbm{1}\{X_i\leq x\}$. The bootstrap analogue is $\widehat{\Gamma}_n^*(x) = \frac{1}{n}\sum_{i=1}^n W_{i,n}Y_i\mathbbm{1}\{X_i\leq x\}$ and $\widehat{\Phi}_n(x) = \frac{1}{n}\sum_{i=1}^n W_{i,n}\mathbbm{1}\{X_i\leq x\}$.

\paragraph{Assumption \thesubsubsection} 
Let $\varepsilon=Y-\mathbb{E}[Y|X]$.
\begin{enumerate}[label=\normalfont(\arabic*),noitemsep,itemindent=*]
	\item The regression function $\mu_0$ is non-decreasing on $I$ and $\mathsf{x}$ is in the interior of $I$.
	
	\item $\mu_0$ satisfies Assumption \ref{Appendix Assumption A} \ref{Appendix Assumption A theta0}, and the cdf $F_0$ satisfies Assumption \ref{Appendix Assumption A} \ref{Appendix Assumption A Phi0}.

    \item $\mathbb{E}[Y^2]<\infty$, $\sup_{|x-\x|\leq \eta}\mathbb{E}[\varepsilon^4|X=x]<\infty$ for some $\eta >0$, and $\sigma_0^2(x) = \mathbb{E}[\varepsilon^2|X=x]$ is continuous and positive at $\x$.
\end{enumerate}
The covariance kernel and the mean function are
\begin{equation*}
    \mathcal{C}_{\x}(s,t) = f_0(\x) \sigma_0^2(\x) \min\{|s|,|t|\}\1\{\mathrm{sign}(s)=\mathrm{sign}(t)\},\qquad \mathcal{D}_{\q}(\x) = \frac{f_0(\x)\partial^{\q}\mu_0(\x)}{(1+\q)!}.
\end{equation*}

\subsubsection{With covariates}\label{Appendix Section: Example monotone regression with covariates}
Now we consider a setting with covariates $\mathbf{A}$. A leading application of this framework in causal inference is discussed in \cite{Westling-Gilbert-Carone_2020_JRRSB}. The parameter of interest is $\theta_0(\x) = \mathbb{E}[\mathbb{E}[Y|X=\x,\mathbf{A}]]$. For $\widehat{\Phi}_n$, we set $\phi_0(x;\mathbf{Z})=\mathbbm{1}\{X\leq x\}$, and for $\widehat{\Gamma}_n$, given a random sample $\{Y_i,X_i,\mathbf{A}_i\}_{i=1}^n$, we use
\begin{equation*}
    \widehat{\gamma}_n(x;\mathbf{Z}) = \mathbbm{1}\{X\leq x\}\bigg[\frac{Y-\widehat{\mu}_n(X,\mathbf{A})}{\widehat{g}_n(X,\mathbf{A})} + \frac{1}{n}\sum_{j=1}^n \widehat{\mu}_n(X,\mathbf{A}_j)\bigg]
\end{equation*}
where $\widehat{\mu}_n(X,\mathbf{A}),\widehat{g}_n(X,\mathbf{A})$ are preliminary estimates of $\mu_0(X,\mathbf{A})$ and $g_0(X,\mathbf{A})=\frac{f_{X|A}(X,\mathbf{A})}{f_0(X)}$, respectively, and $f_{X|A}$ is the conditional Lebesgue density of $X$ given $\mathbf{A}$.
\paragraph{Assumption \thesubsubsection} 
Define $\varepsilon=Y-\mathbb{E}[Y|X,\mathbf{A}]$ and $\sigma_0^2(X,\mathbf{A})=\mathbb{E}[\varepsilon^2|X,\mathbf{A}]$. Let $\eta >0$ be some fixed number.
\begin{enumerate}[label=\normalfont(\arabic*),noitemsep,itemindent=*]
	\item $I$ is compact, the mapping $x\mapsto \mathbb{E}[\mu_0(x,\mathbf{A})]$ is non-decreasing on $I$, and $\mathsf{x}$ is in the interior of $I$.
 
	\item $\theta_0$ satisfies Assumption \ref{Appendix Assumption A} \ref{Appendix Assumption A theta0}. The conditional distribution of $X$ given $\mathbf{A}$ has a bounded Lebesgue density $f_{X|A}$, and there is $c>0$ such that $g_0(X,\mathbf{A})=\frac{f_{X|A}(X,\mathbf{A})}{f_X(X)}\geq c$ with probability one. The cdf $F_0$ satisfies Assumption \ref{Appendix Assumption A} \ref{Appendix Assumption A Phi0}.

    \item \label{Appendix Example: monotone regression with covariates high-level}
    For $K>0$, $\sqrt{na_n}\sup_{|v|\leq K}|\widehat{\Gamma}_n(\x+va_n^{-1})-\widehat{\Gamma}_n(\x)-\Gamma_0(\x+va_n^{-1})+\Gamma_0(\x)|=o_{\mathbb{P}}(1)$.

    \item \label{Appendix Example: monotone regression with covariates first stage}
    $a_n\frac{1}{n}\sum_{i=1}^n|\widehat{\mu}_n(X_i,\mathbf{A}_i)-\mu_0(X_i,\mathbf{A}_i)|^2=o_{\mathbb{P}}(1)$, $a_n\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n|\widehat{\mu}_n(X_i,\mathbf{A}_j)-\mu_0(X_i,\mathbf{A}_j)|^2=o_{\mathbb{P}}(1)$, $a_n\frac{1}{n}\sum_{i=1}^n\varepsilon_i^2|\widehat{g}_n(X_i,\mathbf{A}_i)-g_0(X_i,\mathbf{A}_i)|^2=o_{\mathbb{P}}(1)$.

    \item \label{Appendix Example: monotone regression with covariates mu Lipschitz}
    There exists a real-valued function $\bar{\mu}$ such that $|\mu_0(x_1,\mathbf{A})-\mu_0(x_2,\mathbf{A})|\leq |x_1-x_2|\bar{\mu}(\mathbf{A})$ for $|x_1-x_2|\leq \eta$ and $\mathbb{E}[\bar{\mu}(\mathbf{A})^2]<\infty$.

    \item $\mathbb{E}[\varepsilon^2]<\infty$, $\sup_{|x-\x|\leq \eta}\mathbb{E}[\varepsilon^4|X=x]<\infty$, $\mathbb{E}[\frac{\sigma_0^2(\x,\mathbf{A})}{g_0(\x,\mathbf{A})}]>0$, and there are real-valued functions $B,\omega$ such that $\mathbb{E}[B(\mathbf{A})]<\infty$, $\lim_{\delta\downarrow0}\omega(\delta)=0$, and for $|x-\x|\leq \eta$, $|\frac{\sigma_0^2(x,\mathbf{A})f_{X|A}(x|\mathbf{A})}{g_0(x,\mathbf{A})^2} -\frac{\sigma_0^2(\x,\mathbf{A})f_{X|A}(\x|\mathbf{A})}{g_0(\x,\mathbf{A})^2}| \leq\omega(|x-\x|) B(\mathbf{A})$.
\end{enumerate}
The condition \ref{Appendix Example: monotone regression with covariates high-level} is high-level, and there are different possibilities to verify them. See \cite{Westling-Carone_2020_AoS,Westling-Gilbert-Carone_2020_JRRSB} for details.
The covariance kernel and the mean function are
\begin{equation*}
    \mathcal{C}_{\x}(s,t) = f_0(\x) \mathbb{E}\bigg[\frac{\sigma_0^2(\x,\mathbf{A})}{g_0(\x,\mathbf{A})}\bigg] \min\{|s|,|t|\}\1\{\mathrm{sign}(s)=\mathrm{sign}(t)\},\qquad \mathcal{D}_{\q}(\x) = \frac{f_0(\x)\partial^{\q}\theta_0(\x)}{(1+\q)!}.
\end{equation*}


\subsection{Monotone hazard function}
The parameter of interest is the hazard function of $X$, $\theta_0(\x)=f_0(\x)/S_0(\x)$. As pointed out by \cite{Westling-Carone_2020_AoS}, the function $\Gamma_0$ takes the form $\Gamma_0(x) = \int_0^x \frac{f_0(u)}{S_0(u)}\Phi_0(du)$, and by taking $\Phi_0(x)=\int_0^xS_0(u)du$, $\Gamma_0(x) =F_0(x)$. Since $\Gamma_0$ is identical to the monotone density case, we can leverage the analysis done for the monotone density in Section \ref{Appendix Section: Example -- Monotone Density Estimation}. The interval $I$ equals $[0,u_0^{\mathtt{MD}}]$ where $u_0^{\mathtt{MD}}$ is $u_0$ in the monotone density example. The $u_0$ for the monotone hazard function estimation is $u_0=\Phi_0(u_0^{\mathtt{MD}})$, and we can take $\widehat{u}_n=\widehat{\Phi}_n(u_0^{\mathtt{MD}})$.

\subsubsection{Independent right-censoring}
Consider the case of completely random censoring i.e.,\ $X\indep C$. We take $\widehat{\Gamma}_n(x) =1-\widehat{S}_n(x)$, where $\widehat{S}_n$ is the Kaplan-Meier estimator, and $\widehat{\Phi}_n(x) = \int_0^x \widehat{S}_n(u)du$.
Using the same $\widehat{\gamma}_n(x)$ function as in Section \ref{Appendix Section: Example monotone density CAR}, the bootstrap analogues are defined by $\widehat{\Gamma}_n^*(x)=\frac{1}{n}\sum_{i=1}^nW_{i,n}\widehat{\gamma}_n(x;\mathbf{Z}_i)$ and $\widehat{\Phi}_n^*(x)=\int_0^x [1-\widehat{\Gamma}_n^*(u)]du = \frac{1}{n}\sum_{i=1}^n W_{i,n} \widehat{\phi}_n(x;\mathbf{Z}_i)$ where $\widehat{\phi}_n(x;\mathbf{Z})=x-\int_0^x \widehat{\gamma}_n(u;\mathbf{Z})du$.
\paragraph{Assumption \thesubsubsection} 
\begin{enumerate}[label=\normalfont(\arabic*),noitemsep,itemindent=*]
	\item Assumption \ref{Appendix Section: Example monotone density CAR} holds.
    \item $\sqrt{na_n} \sup_{|v|\leq K}|\widehat{\Phi}_n(\x+va_n^{-1}) -\widehat{\Phi}_n(\x) - \bar{\Phi}_n(\x+va_n^{-1}) + \bar{\Phi}_n(\x)|=o_{\mathbb{P}}(1)$.
\end{enumerate}
The second condition is high-level, and similar to verifying the condition $\sqrt{na_n} \sup_{|v|\leq K}|\widehat{\Gamma}_n(\x+va_n^{-1}) -\widehat{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+va_n^{-1}) + \bar{\Gamma}_n(\x)\big|=o_{\mathbb{P}}(1)$ in the monotone density case, specific structures of the estimators facilitates the analysis. Alternatively, one may assume $n^{\frac{\q}{2\q+1}}[\widehat{\Gamma}_n(\x)-\bar{\Gamma}_n(\x)]=o_{\mathbb{P}}(1)$, which is similar to the condition assumed in Theorem 7 of \cite{Westling-Carone_2020_AoS}.

The covariance kernel and the mean function in this example have the form
\begin{equation*}
    \mathcal{C}_{\x}(s,t) = \frac{f_0(\x)}{G_0(\x)}\min\{|s|,|t|\}\mathbbm{1}\{\mathrm{sign}(x)=\mathrm{sign}(t)\},\quad \mathcal{D}_{\q}(\x) = \frac{S_0(\x)\partial^\q f_0(\x)}{(1+\q)!}.
\end{equation*}

\subsubsection{Conditionally independent right-censoring}
Now suppose $X\indep C|\mathbf{A}$ i.e.,\ conditionally independent censoring. Using the same $\widehat{\gamma}_n$ function as in Section \ref{Appendix Section: Example monotone density conditionally independent}, we set $\widehat{\Gamma}_n(x)=\frac{1}{n}\sum_{i=1}^n\widehat{\gamma}_n(x;\mathbf{Z}_i)$ and $\widehat{\Phi}_n(x)=\int_0^x [1-\widehat{\Gamma}_n(u)]du = \frac{1}{n}\sum_{i=1}^n \widehat{\phi}_n(x;\mathbf{Z}_i)$ where $\widehat{\phi}_n(x;\mathbf{Z})=x-\int_0^x \widehat{\gamma}_n(u;\mathbf{Z})du$. The bootstrap analogues are $\widehat{\Gamma}_n(x) = \frac{1}{n}\sum_{i=1}^nW_{i,n}\widehat{\gamma}_n(x;\mathbf{Z}_i)$ and $\widehat{\Gamma}_n(x) = \frac{1}{n}\sum_{i=1}^nW_{i,n}\widehat{\phi}_n(x;\mathbf{Z}_i)$. 
\paragraph{Assumption \thesubsubsection} 
\begin{enumerate}[label=\normalfont(\arabic*),noitemsep,itemindent=*]
	\item Assumption \ref{Appendix Section: Example monotone density conditionally independent} holds.
    \item $\sqrt{na_n} \sup_{|v|\leq K}|\widehat{\Phi}_n(\x+va_n^{-1}) -\widehat{\Phi}_n(\x) - \bar{\Phi}_n(\x+va_n^{-1}) + \bar{\Phi}_n(\x)|=o_{\mathbb{P}}(1)$.
\end{enumerate}
The covariance kernel and the mean function take the form
\begin{equation*}
    \mathcal{C}_{\x}(s,t) = \mathbb{E}\Big[\frac{f_{X|A}(\x|\mathbf{A})}{G_0(\x|\mathbf{A})}\Big] \min\{|s|,|t|\}\1\{\mathrm{sign}(s)=\mathrm{sign}(t)\}, \quad \mathcal{D}_{\q}(\x) = \frac{S_0(\x)\partial^\q f_0(\x)}{(1+\q)!}. 
\end{equation*}

\subsection{Distribution function estimation with current status data}\label{Appendix Section: Example current status}
The parameter of interest is the cdf of $X$ at $\x$, $\theta_0(\x)=F_0(\x)$. In this example, we do not observe $\tilde{X}=X\wedge C$. Instead, the observation consists of $\mathbf{Z}_i=(\Delta_i,C_i,\mathbf{A}_i)$. This setup is often referred to as current status data.
Let $H_0(x)=\mathbb{P}[C\leq x]$ be the cdf of the censoring time $C$. We can use $\Gamma_0(x) = \int_0^x F_0(u) H_0(du)$ and $\Phi_0(x)=H_0(x)$. The interval $I$ is the support of $X$ and $u_0=1$.
The structure of the estimation problem turns out to be identical to the one for the monotone regression example, and we can leverage the common structure.

\subsubsection{Independent right-censoring}
First we consider the case of completely at random censoring $X\indep C$. See \cite{Groeneboom-Wellner_1992_Book} for existing analysis. We set $\gamma_0(x;\mathbf{Z})=\Delta \mathbbm{1}\{C\leq x\}$ and $\phi_0(x;\mathbf{Z})=\mathbbm{1}\{C\leq x\}$. Thus, $\widehat{\Gamma}_n,\widehat{\Phi}_n,\widehat{\Gamma}_n^*,\widehat{\Phi}_n^*$ are defined using $\gamma_0$ and $\phi_0$. Note that if the notation is mapped by $(\Delta,C)\leftrightarrow (Y,X)$, then these functions are identical to those of the classical monotone regression problem (Section \ref{Appendix Section: Example monotone regression classical case}). Thus, the following assumptions are identical to Assumption \ref{Appendix Section: Example monotone regression classical case} up to notation and some changes due to boundedness of $\Delta$.
\paragraph{Assumption \thesubsubsection} 
Let $\varepsilon=\Delta-\mathbb{E}[\Delta|C]$.
\begin{enumerate}[label=\normalfont(\arabic*),noitemsep,itemindent=*]
	\item The distribution function $F_0$ is non-decreasing on $I$ and $\mathsf{x}$ is in the interior of $I$.
	
	\item $F_0$ satisfies Assumption \ref{Appendix Assumption A} \ref{Appendix Assumption A theta0}, and the cdf $H_0$ satisfies Assumption \ref{Appendix Assumption A} \ref{Appendix Assumption A Phi0}.
    \item $\sigma_0^2(x) = \mathbb{E}[\varepsilon^2|C=x]$ is continuous and positive at $\x$.
\end{enumerate}
By the second assumption, $H_0$ is $\lfloor \s\rfloor-\q +1$ times continuously differentiable on $I_{\x}^{\delta}$, and we write $h_0(x)=\partial H_0(x)$.
The covariance kernel and the mean function are
\begin{equation*}
    \mathcal{C}_{\x}(s,t) = h_0(\x) \sigma_0^2(\x) \min\{|s|,|t|\}\1\{\mathrm{sign}(s)=\mathrm{sign}(t)\},\qquad \mathcal{D}_{\q}(\x) = \frac{h_0(\x)\partial^{\q}F_0(\x)}{(1+\q)!}.
\end{equation*}

\subsubsection{Conditionally independent right-censoring}
We consider the case where right-censoring is conditionally independent i.e.,\ $X\indep C|\mathbf{A}$. \cite{vanderVaart-vanderLaan_2006_IJB} analyzed this example as well as settings with time-varying covariates. We are focusing on time-invariant covariates.
Define $F_0(C,\mathbf{A})=\mathbb{E}[\Delta|C,\mathbf{A}]$ and $g_0(C,\mathbf{A})=\frac{h_{C|A}(C|\mathbf{A})}{h_0(C)}$ where $h_{C|A}$ is the conditional density of $C$ given $\mathbf{A}$ and $h_0$ is the marginal density of $C$. We set $\phi_0(x;\mathbf{Z})=\mathbbm{1}\{C\leq x\}$ and
\begin{equation*}
    \widehat{\gamma}_n(x;\mathbf{Z})=\mathbbm{1}\{C\leq x\}\bigg[\frac{\Delta-\widehat{F}_n(C,\mathbf{A})}{\widehat{g}_n(C,\mathbf{A})} + \frac{1}{n}\sum_{j=1}^n\widehat{F}_n(C,\mathbf{A}_j)\bigg]
\end{equation*}
where $\widehat{F}_n(c,\mathbf{a})$ and $\widehat{g}_n(c,\mathbf{a})$ are preliminary estimators for $F_0(c,\mathbf{a})$ and $g_0(c,\mathbf{a})$, respectively.
Similarly to the censoring completely at random case, with the change in the notation (i.e.,\ $(\Delta, C)\leftrightarrow (Y,X)$), the setup is identical to that of the monotone regression with covariates (Section \ref{Appendix Section: Example monotone regression with covariates}). The following assumption is identical to Assumption \ref{Appendix Section: Example monotone regression with covariates} up to notation and some changes due to boundedness of $\Delta$.

\paragraph{Assumption \thesubsubsection} 
Define $\varepsilon=\Delta-\mathbb{E}[\Delta|C,\mathbf{A}]$ and $\sigma_0^2(C,\mathbf{A})=\mathbb{E}[\varepsilon^2|C,\mathbf{A}]$. Let $\eta >0$ be some fixed number.
\begin{enumerate}[label=\normalfont(\arabic*),noitemsep,itemindent=*]
	\item $I$ is compact, $F_0$ is non-decreasing on $I$, and $\mathsf{x}$ is in the interior of $I$.
 
	\item $\theta_0=F_0$ satisfies Assumption \ref{Appendix Assumption A} \ref{Appendix Assumption A theta0}. The conditional distribution of $C$ given $\mathbf{A}$ has a bounded Lebesgue density $h_{C|A}$, and there is $c>0$ such that $g_0(C,\mathbf{A})\geq c$ with probability one. $\Phi_0=H_0$ satisfies Assumption \ref{Appendix Assumption A} \ref{Appendix Assumption A Phi0}.

    \item For $K>0$, $\sqrt{na_n}\sup_{|v|\leq K}|\widehat{\Gamma}_n(\x+va_n^{-1})-\widehat{\Gamma}_n(\x)-\Gamma_0(\x+va_n^{-1})+\Gamma_0(\x)|=o_{\mathbb{P}}(1)$.

    \item $a_n\frac{1}{n}\sum_{i=1}^n|\widehat{F}_n(C_i,\mathbf{A}_i)-F_0(C_i,\mathbf{A}_i)|^2=o_{\mathbb{P}}(1)$, $a_n\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n|\widehat{F}_n(C_i,\mathbf{A}_j)-F_0(C_i,\mathbf{A}_j)|^2=o_{\mathbb{P}}(1)$, $a_n\frac{1}{n}\sum_{i=1}^n|\widehat{g}_n(C_i,\mathbf{A}_i)-g_0(C_i,\mathbf{A}_i)|^2=o_{\mathbb{P}}(1)$.

    \item There exists a real-valued function $\bar{F}$ such that $|F_0(c_1,\mathbf{A})-F_0(c_2,\mathbf{A})|\leq |c_1-c_2|\bar{F}(\mathbf{A})$ for $|c_1-c_2|\leq \eta$ and $\mathbb{E}[\bar{F}(\mathbf{A})^2]<\infty$.

    \item $\mathbb{E}[\frac{\sigma_0^2(\x,\mathbf{A})}{g_0(\x,\mathbf{A})}]>0$, and there are real-valued functions $B,\omega$ such that $\mathbb{E}[B(\mathbf{A})]<\infty$, $\lim_{\delta\downarrow0}\omega(\delta)=0$, and for $|x-\x|\leq \eta$, $|\frac{\sigma_0^2(x,\mathbf{A})h_{C|A}(x|\mathbf{A})}{g_0(x,\mathbf{A})^2} -\frac{\sigma_0^2(\x,\mathbf{A})h_{C|A}(\x|\mathbf{A})}{g_0(\x,\mathbf{A})^2}| \leq\omega(|x-\x|) B(\mathbf{A})$.
\end{enumerate}
The covariance kernel and the mean function are
\begin{equation*}
    \mathcal{C}_{\x}(s,t) = h_0(\x) \mathbb{E}\bigg[\frac{\sigma_0^2(\x,\mathbf{A})}{g_0(\x,\mathbf{A})}\bigg] \min\{|s|,|t|\}\1\{\mathrm{sign}(s)=\mathrm{sign}(t)\},\qquad \mathcal{D}_{\q}(\x) = \frac{h_0(\x)\partial^{\q}F_0(\x)}{(1+\q)!}.
\end{equation*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proofs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proofs}\label{Appendix Section: Proofs}

\subsection{Switch relations}
For a real-valued function $f$, let $\lsc_J(f)$ be the lower semi-continuous minorant on a set $J$. For a real-valued function defined on a metric space, Lemma 4.3 of \cite{vanderVaart-vanderLaan_2006_IJB} states that $\lsc_J(f)(s) = \liminf_{t\to s:t\in J} f(t)$. 

A key technical tool to analyze generalized Grenander-type estimators is the following switching relationship.
\begin{lemma}\label{Appendix Lemma: generalized switching}
	Let $\Gamma$ and $\Phi$ be real-valued functions defined on an interval $I\subseteq \mathbb{R}$, where $\Phi$ is nondecreasing and right continuous. Fix $l<u$ with $l,u\in\Phi(I)$. Let $\psi=\partial_{-}\mathrm{GCM}_{[l,u]}(\Gamma \circ \Phi^{-})$ and $\theta=\psi\circ \Phi$. If $\Phi(I)\cap[l,u]$ and $\Phi^{-}([l,u])$ are closed, then, for any $t\in \mathbb{R}$ and $\x\in I$ with $ \Phi(\mathsf{x}) \in (l,u)$,
	\begin{equation*}
	   \theta(\mathsf{x}) > t \quad \iff \quad \sup \argmax_{x\in I^{\star} } \big\{ t \Phi(x) - \lsc_{I^{\star}}(\Gamma)(x) \big\} < \Phi^{-}\big(\Phi(\mathsf{x})\big)
	\end{equation*}
	where $I^{\star}:=\Phi^{-}([l,u]) =\{x\in I: \Phi(x) \in [l,u]\}$.
\end{lemma}
\begin{proof}
    In the sequel, $\lsc(\Gamma\circ\Phi^{-})\equiv \lsc_{[l,u]}(\Gamma\circ\Phi^{-})$ and $\lsc(\Gamma)\equiv \lsc_{I^{\star}}(\Gamma)$.
    
    Lemma 4.2 of \cite{vanderVaart-vanderLaan_2006_IJB} states that $\GCM_{[l,u]}(\lsc(\Gamma\circ\Phi^{-})) = \GCM_{[l,u]}(\Gamma\circ\Phi^{-})$ on the interior of $[l,u]$.
    Since $\Phi(\x)$ is in the interior of $[l,u]$, we have $\theta(\x) = \partial_{-} \GCM_{[l,u]}(\lsc(\Gamma\circ\Phi^{-})) (\Phi(\x))$.

    Now, by upper semi-continuity of $-(\lsc(\Gamma\circ \Phi^{-}))$, Lemma 4.1 of \cite{vanderVaart-vanderLaan_2006_IJB} implies
	\begin{equation*}
	\theta(\x) > t \quad \iff \quad \sup \argmax_{y\in [l,u]}\big\{ ty -\lsc(\Gamma\circ\Phi^{-})(y)  \big\} < \Phi(\x).
	\end{equation*}
	Thus, it suffices to show
	\begin{equation}\label{Appendix eq: switch lemma}
	   \sup \argmax_{x\in I^{\star} } \big\{ t \Phi(x) - \lsc(\Gamma) (x) \big\} < \Phi^{-}\big(\Phi(\mathsf{x})\big)  \iff \sup \argmax_{y\in [l,u]}\big\{ ty -\lsc(\Gamma\circ\Phi^{-})  (y)  \big\} < \Phi(\x).
	\end{equation} 
    For $x\in I^{\star}$, $\Gamma(x) = \Gamma(\Phi^{-}(y))$ with $y=\Phi(x)$, and $\liminf_{s\to x:s\in I^{\star}} \Gamma(s)\geq \liminf_{v\to y:v\in [l,u]}\Gamma(\Phi^{-}(v))$. Also, by closedness of $\Phi^{-}([l,u])$, either $\Phi^{-}(y+):=\lim_{\eta\downarrow0}\Phi^{-}(y+\eta)=\Phi^{-}(y)$ or $\Phi(\Phi^{-}(y+)) >y$. Therefore, $\liminf_{v\to y:v\in[l,u]}\Gamma(\Phi^{-}(v))\geq \liminf_{s\to x:s\in I^{\star}} \Gamma(s)$. Then, for $x\in I^{\star}$ and $y=\Phi(x)$,
    \begin{equation}\label{Appendix eq: siwtch lemma constrained maxmization}
        t\Phi(x)-\lsc(\Gamma)(x)  =  t \Phi(x) - \liminf_{s\to x: s\in I^{\star}} \Gamma(s) = ty - \liminf_{v \to y: v\in [l,u] } \Gamma(\Phi^{-}(v))  = ty - \lsc(\Gamma\circ\Phi^{-})(y).
    \end{equation} 
	
	Let $y^{\star}=\sup\argmax_{y\in [l,u]}\{ty -\lsc(\Gamma\circ\Phi^{-})(y)\}$. 	
	First we show $y^{\star}\in \Phi(I)$.
	The case $\Phi(I)\supseteq[l,u]$ is obvious, so assume $\Phi(I)\not\supset [l,u]$.
	For contradiction, suppose $y^{\star}\not\in \Phi(I)$.
	Since $\Phi(I)$ is closed and $y^{\star}\not\in\{l,u\}$, there exists $\eta >0$ such that $[y^{\star}-\eta,y^{\star}+\eta]\subset [l,u]$ and $[y^{\star}-\eta,y^{\star}+\eta] \cap \Phi(I)=\emptyset$.
	On the interval $[y^{\star}-\eta,y^{\star}+\eta]$, $\Phi^{-}$ is constant, and thus, $\lsc(\Gamma\circ\Phi^{-})=\Gamma\circ\Phi^{-}$ is constant on $ [y^{\star}-\eta/2,y^{\star}+\eta/2]$, which contradicts the definition of $y^{\star}=\sup\argmax_{y\in [l,u]}\{ty-\lsc(\Gamma\circ\Phi^{-})(y)\}$.
	Therefore, $y^{\star}\in \Phi(I)$.

    Since $y^{\star}\in \Phi(I)$, $\Phi(\Phi^{-}(y^{\star}))=y^{\star}$. By \eqref{Appendix eq: siwtch lemma constrained maxmization}, $t\Phi(\Phi^{-}(y^{\star}))-\lsc(\Gamma)(\Phi^{-}(y^{\star}))=ty^{\star}-\lsc(\Gamma\circ\Phi^{-})(y^{\star})$, and thus, $\Phi^{-}(y^{\star})\in \argmax_{x\in I^{\star}}\{t\Phi(x)-\lsc(\Gamma)(x)\}$. Also, for $\hat{x}\in \argmax_{x\in I^{\star}} \{t\Phi(x) - \lsc(\Gamma)(x)\}$, $\hat{x}\leq \Phi^{-}(y^{\star})$ as shown below. 
	Then, $\Phi^{-}(y^{\star})=\sup\argmax_{x\in I^{\star}}\{t\Phi(x)-\lsc(\Gamma)(x)\}$.
	
	Now, the LHS of \eqref{Appendix eq: switch lemma} implying the RHS follows from the non-decreasing property of $\Phi^{-}$. If $y^{\star} < u$, $y^{\star}\in \Phi(I)$ implies $\Phi^{-}(y^{\star})<\Phi^{-}(y^{\star}+\epsilon)$ for any $\epsilon>0$, and thus, the RHS of \eqref{Appendix eq: switch lemma} implies the LHS. If $y^{\star} = u$, \eqref{Appendix eq: switch lemma} follows from $u\in \Phi(I)$ and $\Phi(\x)<u$.
	
	
	It remains to show $\hat{x}\leq \Phi^{-}(y^{\star})$ for $\hat{x}\in \argmax_{x\in I^{\star}} \{t\Phi(x) - \lsc(\Gamma)(x)\}$.
	By \eqref{Appendix eq: siwtch lemma constrained maxmization} and $\Phi^{-}(\Phi(\hat{x}))=\hat{x}$, \[t\Phi(\hat{x})-\lsc(\Gamma\circ\Phi^{-})(\Phi(\hat{x}))=t\Phi(\hat{x})-\lsc(\Gamma)(\hat{x})=t\Phi(\Phi^{-}(y^{\star}))-\lsc(\Gamma)(\Phi^{-}(y^{\star}))\] where the second equalty uses $\Phi^{-}(y^{\star})$ being a maximizer of $x \in I^{\star} \mapsto t \Phi(x)-\lsc(\Gamma)(x)$. Thus, $\Phi(\hat{x}) \in \argmax_{y\in [l,u]}\{ty-\lsc(\Gamma\circ\Phi^{-})(y)\}$.
	Then, $\Phi(\hat{x})\leq y^{\star}$ because $y^{\star}$ is the largest element.
	Now, if $\Phi(\hat{x}) < y^{\star}$, then $\hat{x}<\Phi^{-}(y^{\star})$.
	If $\Phi(\hat{x})=y^{\star}$, then $\Phi^{-}(y^{\star})=\Phi^{-}(\Phi(\hat{x}))=\hat{x}$, and thus, $\hat{x}\leq \Phi^{-}(y^{\star})$ holds.
\end{proof}
For the sake of completeness, we state the following version of a switching lemma.
\begin{lemma}\label{Appendix Lemma: switching lemma for R}
    Let $\Gamma:\mathbb{R}\to\mathbb{R}$ be a bounded, lower semi-continuous function such that $vt-\Gamma(t)$ has a unique maximum and satisfies $\lim_{|v|\to\infty}\Gamma(v)/|v|= \infty$.
    Then, for any $x,t\in\mathbb{R}$,
    \begin{equation*}
        \partial_{-}\GCM_{\mathbb{R}} (\Gamma)(x) > t\qquad \iff\qquad \argmax_{v\in\mathbb{R}}\big\{vt - \Gamma(v) \} < x.
    \end{equation*}
\end{lemma}
\begin{proof}
    Lemma A.1.\ of \cite{Sen-Banerjee-Woodroofe_2010_AoS} implies that there exists $c>|x|$ such that $\GCM_{\mathbb{R}} (\Gamma)=\GCM_{[-M,M]} (\Gamma)$ for $M\geq c$.
    For fixed $t$, there exists some $K>c$ such that $-\Gamma(0) > vt -\Gamma(v)$ for $|v| > K$. Thus, the above display is equivalent to 
    \begin{equation*}
        \partial_{-}\GCM_{[-K,K]} (\Gamma)(x) > t\qquad \iff\qquad \argmax_{v\in[-K,K]}\big\{vt - \Gamma(v) \} < x
    \end{equation*}
    and this statement follows from Lemma 4.1 of \cite{vanderVaart-vanderLaan_2006_IJB}.
\end{proof}

\subsection{Proof of Lemma \ref{Appendix Lemma Continuity Distribution Function}}
First we show that a maximizer of $\mathbb{G}(s)$ over $s\in\mathbb{R}$ exists and is unique with probability one. We follow the proof strategy of \cite{Kim-Pollard_1990_AoS}.
Let $\widetilde{\mathbb{G}}(s) = \mathbb{G}(s)-\mu(s)$ be the centered process. If we can show that for $c>1$ in the hypothesis,
\begin{equation}\label{Appendix eq: BC lemma}
    \mathbb{P}\bigg[\limsup_{|s|\to\infty}\frac{\widetilde{\mathbb{G}}(s)}{|s|^{c}} >\eta \bigg] = 0\qquad \text{for any } \eta>0,
\end{equation}
then $\mathbb{G}(s)\to -\infty$ as $|s|\to\infty$ with probability one by $\limsup_{|s|\to\infty}\mu(s)/|s|^c\to-\infty$. Then, by continuous sample paths, a maximizer $\mathbb{G}(s)$ exists. Since $\mathcal{K}(s,s)+\mathcal{K}(t,t)-2\mathcal{K}(s,t)=\mathcal{K}(s-t,s-t)>0$ for $s\neq t$, Lemma 2.6 of \cite{Kim-Pollard_1990_AoS} implies that this maximizer is unique with probability one.
It remains to show \eqref{Appendix eq: BC lemma}. Using the property $\mathcal{K}(s\tau,t\tau)=\tau\mathcal{K}(s,t)$,
\begin{align*}
    \sum_{k=2}^{\infty} \mathbb{P}\bigg[ \sup_{k-1\leq |s|\leq k} \frac{\widetilde{\mathbb{G}}(s)}{|s|^c} > \eta\bigg] &\leq \sum_{k=2}^{\infty} \mathbb{P}\bigg[ \sup_{|s|\leq k} \widetilde{\mathbb{G}}(s) > |k-1|^c\eta\bigg]\\
    &\leq \sum_{k=2}^{\infty} \mathbb{P}\bigg[ \sup_{|s|\leq 1} \widetilde{\mathbb{G}}(s) > \frac{|k-1|^c}{\sqrt{k}}\eta\bigg]\\
    &\leq \mathbb{E}\Big[\sup_{|s|\leq 1} \widetilde{\mathbb{G}}(s)^{2}\Big] \eta^{-2}\sum_{k=2}^{\infty}   k^{1-2c} < \infty.
\end{align*}
Then, the Borel-Cantelli lemma implies the desired result. 

To show the continuity of the distribution function $x\mapsto \mathbb{P}[\argmax_{s\in \mathbb{R}}\mathbb{G}(s)\leq x]$, it suffices to show $\mathbb{P}[\argmax_{s\in \mathbb{R}}\mathbb{G}(s)= x]=0$ for $x\in\mathbb{R}$.
Fix $x\in\mathbb{R}$ and define
\begin{equation*}
	\tilde{Z}(s) = \frac{\mathbb{G}(s) - \mathbb{G}(x) }{\sqrt{\mathcal{K}(s,s) + \mathcal{K}(x,x) - 2\mathcal{K}(s,x)}}, \qquad s\neq x 
\end{equation*}
and $\tilde{Z}(x)=0$.
Note $\mathcal{K}(s,s) + \mathcal{K}(x,x) - 2\mathcal{K}(s,x)=\mathcal{K}(s-x,s-x)$.
Then, $\max_{s\in\mathbb{R} }\tilde{Z}(s)\geq 0$, and
\begin{equation*}
	\mathbb{P}\Big[\argmax_{s\in \mathbb{R}}\mathbb{G}(s)=x\Big] = \mathbb{P}\Big[\max_{s\in\mathbb{R}}\tilde{Z}(s) \leq 0 \Big]
\end{equation*}
and the last probability is bounded by $\mathbb{P}[\max_{s\in\mathcal{S}}\tilde{Z}(s) \leq 0]$ with any subset $\mathcal{S}\subset \mathbb{R}$.
In the sequel, we construct a suitable subset $\mathcal{S}$ to show that the probability can be made arbitrarily small.
In particular, for given $\varepsilon\in (0,1)$ and $N\in\mathbb{N}$, we pick $N$ points $\{s_{1,N}^{\varepsilon},\dots,s_{N,N}^{\varepsilon}\}=:\mathcal{S}_{N}^{\varepsilon}$ such that
\begin{equation}\label{Appendix eq: mean_bound}
	\mathbb{E}\tilde{Z}(s_{i,N}^{\varepsilon})\geq -\varepsilon \quad \text{ for every } 1\leq i \leq N
\end{equation}
and
\begin{equation}\label{Appendix eq: cov_bound}
	\big|\mathrm{Cov}(\tilde{Z}\big(s_{i,N}^{\varepsilon}),\tilde{Z}(s_{j,N}^{\varepsilon})\big)\big|\leq \varepsilon \quad \text{ for every } 1\leq i \leq N.
\end{equation}
Then, using the fact that normal random vectors converge in distribution when their means and variances converge, we have
\begin{equation*}
	\mathbb{P}\Big[\max_{s\in\mathbb{R}}\tilde{Z}(s) \leq 0 \Big] \leq \liminf_{\varepsilon\downarrow 0}\mathbb{P}\Big[\max_{s\in\mathcal{S}_{N}^{\varepsilon}}\tilde{Z}(s) \leq 0 \Big] \leq \bigg| \int_{-\infty}^{0}\frac{\exp(-x^2/2)}{\sqrt{2\pi}}dx\bigg|^N=2^{-N}.
\end{equation*}
By taking $N$ large, we can make the left-hand side probability arbitrarily small.

Using the properties of $\mathcal{K}$ and of $\lim_{t\downarrow 0}[\mu(x+t)-\mu(x)]/\sqrt{t}=0$, there exists $\bar{\tau}_{\varepsilon,1}\in (0,1)$ such that
\begin{equation*}
	\mathbb{E}[\tilde{Z}( x+\tau )] = \frac{\mu(x+\tau ) -\mu(x)}{\sqrt{\mathcal{K}(\tau ,\tau )}}  =  \frac{[\mu(x+\tau )-\mu(x)]/\sqrt{\tau}}{\sqrt{\mathcal{K}(1,1)}} > -\varepsilon,\qquad \forall\ \tau\in (0,\bar{\tau}_{\varepsilon,1}).
\end{equation*}
Also, for $0<\tau_j<\tau_i<1$, using condition $\mathcal{K}(s\tau ,t\tau )=\mathcal{K}(s,t)\tau$,
\begin{equation*}
	\mathrm{Cov}\big(\tilde{Z}(x+\tau_i),\tilde{Z}(x+\tau_j)\big) = \frac{\mathcal{K}(\tau_i,\tau_j)}{\sqrt{\mathcal{K}(\tau_i,\tau_i)\mathcal{K}(\tau_j,\tau_j)}} = \frac{\mathcal{K}(1,\tau_j/\tau_i)/\sqrt{\tau_j/\tau_i} }{\mathcal{K}(1,1)} 
\end{equation*}
and $\lim_{\delta \downarrow 0}\mathcal{K}(s,s\delta )/\sqrt{\delta}=0$ implies that there exists $\bar{\tau}_{\varepsilon,2}\in (0,1)$ such that for all $\tau_j/\tau_i \in (0,\bar{\tau}_{\varepsilon,2})$, $|\mathrm{Cov}(\tilde{Z}(x+\tau_i),\tilde{Z}(x+\tau_j))|\leq \varepsilon$.
Now, let $s_{i,N}^{\varepsilon} = x+\bar{\tau}_{\varepsilon}^{i}/2$ where $\bar{\tau}_{\varepsilon}=\min\{\bar{\tau}_{\varepsilon,1},\bar{\tau}_{\varepsilon,2}\}$.
This choice of $\{s_{i,N}^{\varepsilon}\}_{i=1}^N$ indeed satisfies \eqref{Appendix eq: mean_bound} and \eqref{Appendix eq: cov_bound}.\hfill\qed

\subsection{Proof of Theorem \ref{Appendix Theorem: Asymptotic Distribution - High-level}}
Let $r_n=n^{\frac{\q}{2\q+1}}$ and $\widehat{I}_n=\widehat{\Phi}_n^{-}([0,\widehat{u}_n])$.
By Lemma \ref{Appendix Lemma: generalized switching} and change of variables,
\begin{align*}
	\mathbb{P}[r_n(\widehat{\theta}_n(\mathsf{x}) - \theta_0(\mathsf{x}) ) > t ] &= \mathbb{P}\bigg[\sup\argmax_{v\in \widehat{I}_n} \big\{ [\theta_0(\mathsf{x}) +t r_n^{-1}]\widehat{\Phi}_n(v) - \lsc_{\widehat{I}_n}(\widehat{\Gamma}_n)(v)   \big\} < \widehat{\Phi}_n^{-}\big(\widehat{\Phi}_n(\mathsf{x}) \big) \bigg]\\
    &= \mathbb{P}\bigg[\sup\argmin_{v\in \widehat{V}_{\x,n}^{\q}} \big\{ \lsc_{\widehat{V}_{\x,n}^{\q}}(\widehat{G}_{\x,n}^{\q} + M_{\x,n}^{\q})(v)   -t \widehat{L}_{\x,n}^{\q}(v)    \big\} < \widehat{Z}_{\x,n}^{\q} \bigg]
\end{align*}
where 
\begin{align*}
    \widehat{G}_{\x,n}^{\q}(v) &= \sqrt{na_n}[\widehat{\Gamma}_n(\x + v a_n^{-1}) - \widehat{\Gamma}_n(\x) -\Gamma_0(\x + v a_n^{-1}) + \Gamma_0(\x) ] \\
    &\qquad - \theta_0(\x) \sqrt{na_n}[\widehat{\Phi}_n(\x + v a_n^{-1})-\widehat{\Phi}_n(\x) - \Phi_0(\x+va_n^{-1}) + \Phi_0(\x)]\\
    M_{\x,n}^{\q}(v)&=\sqrt{na_{n}}[\Gamma_{0}(\x+va_{n}^{-1})-\Gamma_{0}(\x)]-\theta_{0}(\x)\sqrt{na_{n}}[\Phi_{0}(\x+va_{n}^{-1})-\Phi_{0}(\x)]\\
    \widehat{L}_{\x,n}^{\q}(v)&=a_{n}[\widehat{\Phi}_{n}(\x+va_{n}^{-1})-\widehat{\Phi}_{n}(\x)]\\
    \widehat{Z}_{\x,n}^{\q}&=a_{n}[\widehat{\Phi}_{n}^{-}(\widehat{\Phi}_{n}(\x))-\x]
\end{align*}
and $\widehat{V}_{\x,n}^{\q}=\{a_{n}(x-\mathsf{x)}:x\in \widehat{\Phi}_{n}^{-}([0,\widehat{u}_{n}])\}$ as defined in the main text. Note that $\widehat{\Phi}_n$ is continuous on $\widehat{I}_n$ because $\widehat{\Phi}_n(I)\cap [0,\widehat{u}_n]$ is closed.

We let $\lsc_{\widehat{V}_{\x,n}^{\q}}(\widehat{G}_{\x,n}^{\q}+M_{\x,n}^{\q})(v) = \widehat{G}_{\x,n}^{\q}(v)+M_{\x,n}^{\q}(v)$ if $v\not\in \widehat{V}_{\x,n}^{\q}$. With $V_n = \{a_n(x-\x):x\in I\}$, we have
\begin{equation*}
    \lsc_{V_n}(\widehat{G}_{\x,n}^{\q}+M_{\x,n}^{\q})(v) \leq \lsc_{\widehat{V}_{\x,n}^{\q}}(\widehat{G}_{\x,n}^{\q}+M_{\x,n}^{\q})(v) \leq \widehat{G}_{\x,n}^{\q}(v)+M_{\x,n}^{\q}(v)\quad\forall \ v\in V_n, \quad \text{almost surely}.
\end{equation*}
For any compact interval $K$, Lemma 4.4 of \cite{vanderVaart-vanderLaan_2006_IJB} implies that if $\widehat{G}_{\x,n}^{\q} + M_{\x,n}^{\q}\leadsto \mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q}$, then $\lsc_{K}(\widehat{G}_{\x,n}^{\q}+M_{\x,n}^{\q})\leadsto \mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q}$ as $\mathcal{G}_{\x}$ has continuous sample paths with probability one and $\mathcal{M}_{\x}^{\q}$ is continuous. Thus, for our purpose, it is without loss of generality to look at $\widehat{G}_{\x,n}^{\q}+M_{\x,n}^{\q}$ in place of $\lsc_{\widehat{V}_{\x,n}^{\q}}(\widehat{G}_{\x,n}^{\q}+M_{\x,n}^{\q})$.

By Assumption \ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Weak Convergence} and \ref{Appendix Assumption B non-bootstrap Uniform Convergence Rate}, $\widehat{G}_{\x,n}^{\q}\leadsto\mathcal{G}_{\x}$, $\sup_{|v|\leq C}|\widehat{L}_{\x,n}^{\q}(v)-L_{\x}(v)|=o_{\mathbb{P}}(1)$ with $L_{\x}(v)=\partial\Phi_0(\x)v$, and $\widehat{Z}_{\x,n}^{\q}=o_{\mathbb{P}}(1)$.

For $M_{\x,n}^{\q}$ term, the function $\Gamma_0(\mathsf{x}+u)-\theta_0(\mathsf{x})\Phi_0(\mathsf{x}+u) -\Gamma_0(\mathsf{x}) + \theta_0(\mathsf{x})\Phi_0(\mathsf{x})$ converges to $0$ as $u\to 0$.
The derivative equals $[\theta_0(\mathsf{x}+u)-\theta_0(\mathsf{x})] \partial \Phi_0(\mathsf{x}+u)$, and $|\theta_0(\mathsf{x}+u)-\theta_0(\mathsf{x})|\partial \Phi_0(\x+u)/|u|^{\q}\to \partial\Phi_0(\mathsf{x})\partial^{\q}\theta_0(\mathsf{x})/\q!$.
Now by L'H\^{o}pital's rule,
\begin{align*}
	\lim_{u\to 0}\frac{\Gamma_0(\mathsf{x}+u)-\theta_0(\mathsf{x})\Phi_0(\mathsf{x}+u) -\Gamma_0(\mathsf{x}) + \theta_0(\mathsf{x})\Phi_0(\mathsf{x})}{|u|^{\q+1}} &= \lim_{u\to 0}\frac{|\theta_0(\mathsf{x}+u)-\theta_0(\mathsf{x})| \partial \Phi_0(\x+u)}{(\q+1)|u|^{\q} } \\
	&= \frac{\partial^{\q}\theta_0(\mathsf{x})\partial\Phi_0(\mathsf{x})}{(\q+1)!} .
\end{align*}
Thus, $M_{\x,n}^{\q}\to \mathcal{M}_{\x}^{\q}$ uniformly on compacta.

We apply an argmax continuous mapping theorem \citep[e.g.,\ Theorem 3.2.2][]{vanderVaart-Wellner_1996_Book} to prove
\begin{equation*}
	\sup\argmin_{v\in \widehat{V}_{\x,n}^{\q}} \big\{ \widehat{G}_{\x,n}^{\q}(v) + \widehat{M}_{\x,n}^{\q}(v)  -t \widehat{L}_{\x,n}^{\q}(v)    \big\} \leadsto \argmin_{v\in \mathbb{R}} \Big\{\mathcal{G}_{\mathsf{x}} (v) +\mathcal{M}_{\x}^{\q}(v)  - t \partial \Phi_0(\mathsf{x}) v \Big\},
\end{equation*}
which together with Lemma \ref{Appendix Lemma Continuity Distribution Function} implies
\begin{equation*}
    \mathbb{P}[r_n(\widehat{\theta}_n(\mathsf{x}) - \theta_0(\mathsf{x}) ) > t ] \to  \mathbb{P} \Big[ \argmin_{v\in \mathbb{R}} \Big\{\mathcal{G}_{\mathsf{x}} (v) + \mathcal{M}_{\x}^{\q}(v)  - t \partial \Phi_0(\mathsf{x}) v \Big\} < 0\Big] .
\end{equation*}
That the limit distribution equals $\P[(\partial\Phi_0(\x))^{-1}\partial_{-}\GCM_{\mathbb{R}}\{\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q}\}(0)> t]$ follows from Lemma \ref{Appendix Lemma: switching lemma for R}.
To handle the $n$-varying domains, we use a modified version of argmax continuous mapping theorem by \cite{Cox_2022}.
To apply the continuous mapping theorem, let $\hat{v}_n=\sup\argmin_{v\in \widehat{V}_{\x,n}^{\q}} [\widehat{G}_{\x,n}^{\q}(v) + M_{\x,n}^{\q}(v)  -t \widehat{L}_{\x,n}^{\q}(v)]$, and we need to verify (i) $ \hat{v}_n=O_{\mathbb{P}}(1)$, (ii) $\widehat{V}_{\x,n}^{\q}$ converges to $\mathbb{R}$ in the Painlev\'{e}-Kuratowski sense (see \cite{Cox_2022} for the defintion), and (iii) the limit process is continuous and has a unique maximum with probability one.
The limit process is continuous by the property of the covariance kernel, and the uniqueness of a maximizer follows as in the proof of Lemma \ref{Appendix Lemma Continuity Distribution Function}.


To show $\hat{v}_n=O_{\P}(1)$, let $\widehat{\x}_n=\hat{v}_na_n^{-1}+\x$, that is, $\widehat{\x}_n = \sup\argmax_{v\in \widehat{I}_n} \{ [\theta_0(\mathsf{x}) +t r_n^{-1}]\widehat{\Phi}_n(v) - \widehat{\Gamma}_n(v) \}$. Also, define $\widetilde{\x}_n=\widehat{\Phi}_n^{-}(\widehat{\Phi}_n(\x))$.
Since $a_n (\widetilde{\x}_n-\x)=o_{\P}(1)$, it suffices to show $a_n (\widehat{\x}_n-\widetilde{\x}_n)=O_{\mathbb{P}}(1)$.

First we show $|\widehat{\x}_n-\widetilde{\x}_n|=o_{\P}(1)$. For any $\eta >0$, 
\begin{align*}
    \P[ |\widehat{\x}_n-\widetilde{\x}_n|>\eta ] &\leq \P\bigg[ \sup_{v\in\widehat{I}_n: |v-\widetilde{\x}_n|>\eta}\big\{ [\theta_0(\mathsf{x}) +t r_n^{-1}][\widehat{\Phi}_n(v) - \widehat{\Phi}_n(\widetilde{\x}_n)] - [\widehat{\Gamma}_n(v) -\widehat{\Gamma}_n(\widetilde{\x}_n) ] \big\} \geq 0  \bigg]\\
    &\leq \P\bigg[ \sup_{v\in I: |v-\x|>\frac{\eta}{2}}\big\{ [\theta_0(\mathsf{x}) +t r_n^{-1}][\widehat{\Phi}_n(v) - \widehat{\Phi}_n(\widetilde{\x}_n)] - [\widehat{\Gamma}_n(v) -\widehat{\Gamma}_n(\widetilde{\x}_n) ] \big\} \geq 0  \bigg] + o(1).
\end{align*}
The random function inside the bracket converges in probability (uniformly over $x\in I$) to $\theta_0(\mathsf{x}) [\Phi_0(v) - \Phi_0(\x)] - [\Gamma_0(v) -\Gamma_0(\x) ]$, which is uniquely maximized at $v=\mathsf{x}$, and $\sup_{v\in I: |v-\x|>\frac{\eta}{2}}\{\theta_0(\mathsf{x}) [\Phi_0(v) - \Phi_0(\x)] - [\Gamma_0(v) -\Gamma_0(\x) ]\} <0$. Thus, $|\widehat{\x}_n-\widetilde{\x}_n|=o_{\P}(1)$ holds.

Following the argument similar to Theorem 3.2.5 of \cite{vanderVaart-Wellner_1996_Book} and using $[\theta_0(\mathsf{x}) +t r_n^{-1}][\widehat{\Phi}_n(\widetilde{\x}_n) -\widehat{\Phi}_n(\x)] -[ \widehat{\Gamma}_n(\widetilde{\x}_n)-\widehat{\Gamma}_n(\x)]=o_{\P}(a_n^{-(1+\q)})$, it suffices to bound for any small $\eta >0$ and sufficiently large $M>0$
\begin{align*}
	& \sum_{ j \geq M, 2^j\leq \eta a_n} \mathbb{P} \bigg[ \sup_{2^{j-1} <a_n|v|\leq 2^{j}} - \widehat{\Gamma}_n(\mathsf{x}+v) +\widehat{\Gamma}_n(\mathsf{x})+ [\theta_0(\mathsf{x})+t r_n^{-1}]	[ \widehat{\Phi}_n(\mathsf{x}+v) -\widehat{\Phi}_n(\mathsf{x})] \geq   o_{\mathbb{P}}(a_n^{-(1+\q)})  \bigg]  \\
	&\leq \sum_{ j \geq M,2^j\leq \eta a_n } \mathbb{P} \bigg[ \sup_{a_n|v|\leq 2^{j}}  -\widehat{G}_{n,\x}^{\q}(v a_n) + t a_n	[ \Phi_0(\mathsf{x}+v) -\Phi_0(v)]  \geq  c 2^{(j-1)(\q+1)} +o_{\mathbb{P}}(1)    \bigg]
\end{align*}
where we use $-\Gamma_0(\mathsf{x}+v)+\Gamma_0(\mathsf{x}) + \theta_0(\mathsf{x})[\Phi_0(\mathsf{x}+v)-\Phi_0(\mathsf{x})]\leq - c|v|^{\q+1}$ for some $c>0$ and $|v|$ close to zero and  $a_n\sup_{|v-\x|\leq\eta}|\widehat{\Phi}_n(v)-\Phi_0(v)|=o_{\mathbb{P}}(1)$.

With some abuse of notation, we analyze the above probability by replacing $\widehat{G}_{n,\x}^{\q}$ with $\mathcal{G}_{\x} + o_{\mathbb{P}}(1)$, which is possible by Dudley's representation theorem \citep[e.g.,\ Theorem 2.2 of][]{Kim-Pollard_1990_AoS}.
Then,
\begin{align*}
	&\sum_{ j \geq M,2^j\leq \eta a_n } \mathbb{P}\bigg[ \sup_{a_n|v|\leq 2^{j}} - \mathcal{G}_{\mathsf{x}}(v a_n) + t a_n	[ \Phi_0(\mathsf{x}+v) -\Phi_0(v)]  \geq  c 2^{(j-1)(\q+1)}  -o_{\mathbb{P}}(1)    \bigg] \\
	&\leq C \sum_{j\geq M,2^j\leq \eta a_n} \frac{\mathbb{E}\Big[\sup_{|v|\leq 2^j} |\mathcal{G}_{\mathsf{x}}(v)| \Big] + a_n \sup_{|v|\leq 2^ja_n^{-1}} |\Phi_0(\mathsf{x}+v) - \Phi_0(\mathsf{x})|}{2^{j(\q+1)}}    \\
	&\leq C\sum_{j\geq M,2^j\leq \eta a_n}  2^{-j(\q+1)} \big[ 2^{j/2} + 2^{j}  \big] 
\end{align*}
where the last inequality uses a maximal inequality for Gaussian processes \citep[e.g.,\ Corollary 2.2.8 of][]{vanderVaart-Wellner_1996_Book} and the property of the covariance kernel $|\mathcal{C}_{\x}(s,s)+ \mathcal{C}_{\x}(t,t) -2\mathcal{C}_{\x}(s,t)|=|s-t| \mathcal{C}_{\x}(1,1)$. 
Thus, taking $M$ large enough makes the sum of the probabilities arbitrary small.

It remains to show $\widehat{V}_{\x,n}^{\q}$ converges to $\mathbb{R}$ in the Painlev\'{e}-Kuratowski sense. By Assumption \ref{Appendix Assumption A}, $\Phi_0$ is strictly increasing and continuously differentiable on a neighborhood of $\x$. Also, $a_n\sup_{|v|\leq K}|\widehat{\Phi}_n(\x+va_n^{-1}) -\Phi_0(\x+va_n^{-1})|=o_{\mathbb{P}}(1)$ implies that 
\begin{equation*}
    \sup_{|v|\leq K}\Big|a_n\big[\widehat{\Phi}_n^{-}(\Phi_0(\x+va_n^{-1}))- \Phi_0^{-}(\Phi_0(\x+va_n^{-1}) ) \big]\Big|=\sup_{|v|\leq K}\Big|a_n[\widehat{\Phi}_n^{-}(\Phi_0(\x+va_n^{-1}))-\x  ]- v\Big|
\end{equation*}
is $o_{\mathbb{P}}(1)$.
Then, every $b\in\mathbb{R}$ belongs to the Painlev\'{e}-Kuratowski limit of $\widehat{V}_{\x,n}^{\q}$.
\hfill\qed 




\subsection{Proof of Theorem \ref{Appendix Theorem: Bootstrap Approximation - High-level}}
The argument is analogous to the proof of Theorem \ref{Appendix Theorem: Asymptotic Distribution - High-level} with appropriate changes for bootstrap.

Let $\widehat{G}_{\x,n}^{\q,\ast}(v)=\sqrt{na_{n}}[\widehat{\Gamma}_{n}^{\ast}(\x+va_{n}^{-1})-\widehat{\Gamma}_{n}^{\ast}(\x)-\widehat{\Gamma}_{n}(\x+va_{n}^{-1})+\widehat{\Gamma}_{n}(\x)] -\widehat{\theta}_{n}(\x)\sqrt{na_{n}}[\widehat{\Phi}_{n}^{\ast}(\x+va_{n}^{-1})-\widehat{\Phi}_{n}^{\ast}(\x)-\widehat{\Phi}_{n}(\x+va_{n}^{-1})+\widehat{\Phi}_{n}(\x)]$, $\widehat{L}_{\x,n}^{\q,\ast}(v)=a_{n}[\widehat{\Phi}_{n}^{\ast}(\x+va_{n}^{-1})-\widehat{\Phi}_{n}^{\ast}(\x)]$, $\widehat{Z}_{\x,n\x}^{\q,\ast}=a_{n}[(\widehat{\Phi}_{n}^{\ast})^{-}(\widehat{\Phi}_{n}^{\ast}(\x))-\x]$, $\widehat{I}_n^{\ast}=(\widehat{\Phi}_{n}^{\ast})^{-}([0,\widehat{u}_{n}^{\ast}])$, and $\widehat{V}_{\x,n}^{\q,\ast}=\{a_{n}(x-\mathsf{x)}:x\in \widehat{I}_n^{\ast}\}$.
Then,
\begin{align*}
	\mathbb{P}[r_n(\widetilde{\theta}_n^*(\mathsf{x}) - \widehat{\theta}_n(\mathsf{x}) ) > t ] &= \mathbb{P}\Big[ \sup\argmax_{v\in \widehat{I}_{n}^{\ast}} \big\{ [\widehat{\theta}(\x) + t r_n^{-1}] \widehat{\Phi}_n^*(v) -  \lsc_{\widehat{I}_n^*} (\widetilde{\Gamma}_n^*) (v) \big\} < (\widehat{\Phi}_n^*)^{-}(\widehat{\Phi}_n^{\ast}(\x)) \Big]\\
    &= \mathbb{P}\Big[\sup\argmin_{v\in \widehat{V}_{\x,n}^{\q,\ast}} \big\{ \lsc_{\widehat{V}_{\x,n}^{\q,\ast}} (\widehat{G}_{n,\x}^{\q,*}+ \widetilde{M}_{\x,n}^{\q})(v)  - t \widehat{L}_{\x,n}^{\q,*}(v)  \big\} < \widehat{Z}_{\x,n\x}^{\q,\ast}  \Big]
\end{align*}
where the first equality uses Lemma \ref{Appendix Lemma: generalized switching} and the second equality uses changes of variables and continuity of $\widehat{\Phi}_n^*$ on $\widehat{I}_n^{\ast}$. With the same argument used for the proof of Theorem \ref{Appendix Theorem: Asymptotic Distribution - High-level}, it is without loss of generality to look at $\widehat{G}_{n,\x}^{\q,*} +\widetilde{M}_{\x,n}^{\q}$ in place of $\lsc_{\widehat{V}_{\x,n}^{\q,\ast}} (\widehat{G}_{n,\x}^{\q,*}+\widetilde{M}_{\x,n}^{\q})$.  By the hypothesis,
\begin{equation*}
   \widehat{G}_{n,\x}^{\q,*}  + \widetilde{M}_{\x,n}^{\q} \leadsto_{\P} \mathcal{G}_{\x} + \mathcal{M}_{\x}^{\q}.
\end{equation*}
For the other term, $a_n\sup_{|v|\leq K}|\widehat{\Phi}_n(\x+va_n^{-1})-\Phi_0(\x+va_n^{-1})|  = o_\P(1)$ and $a_n\sup_{|v|\leq \eta}|\widehat{\Phi}_n^*(\x+va_n^{-1})-\widehat{\Phi}_n(\x+va_n^{-1})|  = o_\P(1)$ imply $\sup_{|v|\leq K} |\widehat{L}_{\x,n}^{\q,*}(v) - a_n[\Phi_0(\x+va_n^{-1}) - \Phi_0(\x)] |= o_{\P}(1)$ and $\widehat{L}_{\x,n}^{\q,*}(v)$ converges to $\partial\Phi_0(\x)v$ uniformly on compacta. Also, $a_{n}[(\widehat{\Phi}_{n}^{\ast})^{-}(\widehat{\Phi}_{n}^{\ast}(\x))-\x]=o_{\mathbb{P}}(1)$.

It remains to verify the hypothesis of the argmax continuous mapping theorem.
Let $\widehat{x}_n^*=\argmax_{v\in \widehat{I}_n^*}\{[\widehat{\theta}_n(\x)+tr_n^{-1}]\widehat{\Phi}_n^*(v) - \widetilde{\Gamma}_n^*(v)\}$ and we check $a_n(\widehat{\x}_n^*-\widetilde{\x}_n^*)=O_{\P}(1)$ with $\widetilde{\x}_n^*=(\widehat{\Phi}_n^*)^{-}(\widehat{\Phi}_n^*(\x))$. With $\widehat{\Gamma}_{n,0}^*(v) = \widehat{\Gamma}_{n}^*(v)-\widehat{\Gamma}_n(v)$ and $\widehat{\Phi}_{n,0}^*(v) = \widehat{\Phi}_{n}^*(v)-\widehat{\Phi}_n(v)$,
\begin{align*}
    \P\big[ |\widehat{\x}_n^*-\widetilde{\x}_n^*| > \eta \big] 
    &\leq \P\bigg[ \sup_{ |v-\widetilde{\x}_n^*|>\eta}\big\{ \widehat{\theta}_n(\x)[\widehat{\Phi}_{n,0}^*(v) - \widehat{\Phi}_{n,0}^*(\widetilde{\x}_{n}^*)] - [\widehat{\Gamma}_{n,0}^*(v) - \widehat{\Gamma}_{n,0}^*(\widetilde{\x}_{n}^*)]\\
    &\qquad\qquad+t r_n^{-1}[\widehat{\Phi}_{n}^*(v) - \widehat{\Phi}_{n}^*(\widetilde{\x}_n^*)]  - \widetilde{M}_{\x,n}(v-\x) + \widetilde{M}_{\x,n}(\widetilde{x}_n^*-\x) \big\} \geq 0  \bigg]\\
    &\leq  \P\bigg[ \sup_{ |v-\x|>\eta/2}\big\{  - \widetilde{M}_{\x,n}(v-\x) + \widetilde{M}_{\x,n}(\widetilde{x}_n^*-\x) \big\} \geq -o_{\mathbb{P}}(1)  \bigg] + o(1)
\end{align*}
where for some $\delta >0$, $-\widetilde{M}_{\x,n}(v-\x)\leq -\delta$ for all $|v-\x|>\eta/2$ with probability approaching one by Assumption \ref{Appendix Assumption: mean function estimation} and $\widetilde{M}_{\x,n}(\widetilde{x}_n^*-\x)= (na_n)^{-1/2}[ \widetilde{M}_{\x,n}^{\q}(a_n(\widetilde{x}_n^*-\x))\mp\mathcal{M}_{\x}^{\q}(a_n(\widetilde{x}_n^*-\x))]=o_{\mathbb{P}}(1)$ by $a_n(\widetilde{x}_n^*-\x)=o_{\mathbb{P}}(1)$ and $\widetilde{M}_{\x,n}^{\q}\leadsto \mathcal{M}_{\x}^{\q}$. Thus, the majorant probability goes to zero.

Now, to show $a_n(\widehat{\x}_n^*-\widetilde{\x}_n^*)=O_{\P}(1)$, arguing as in the proof of Theorem \ref{Appendix Theorem: Asymptotic Distribution - High-level}, it suffices to bound for any small $\eta >0$ and sufficiently large $M>0$
\begin{align*}
	& \sum_{ j \geq M, 2^j\leq \eta a_n} \mathbb{P}^* \bigg[  \sup_{2^{j-1} < a_n|v|\leq 2^{j}}  \widehat{G}_{n,\x}^{\q,*}(v a_n) + t a_n	[ \widehat{\Phi}_n^*(\x+v) -\widehat{\Phi}_n^*(\x)] + \widetilde{M}_{\x,n}^{\q}(va_n)  \geq  o_{\mathbb{P}}(1)    \bigg] \\
	&\leq \sum_{ j \geq M, 2^j\leq \eta a_n} \mathbb{P}^* \bigg[  \sup_{a_n|v|\leq 2^{j}}  \widehat{G}_{n,\x}^{\q,*}(v a_n) + t a_n	[ \Phi_0(\x+v) -\Phi_0(\x)]    \geq c2^{j-1} +   o_{\mathbb{P}}(1)    \bigg]
\end{align*}
where we used $\sup_{|v|\leq \eta }|\widetilde{M}_{\x,n}^{\q}(v)-\mathcal{M}_{\x}^{\q}(v)|=o_{\P}(1)$ and $\sup_{2^{j-1}<a_n|v|\leq 2^j }\mathcal{M}_{\x}^{\q}(a_n v) = -2^{j-1}\mathcal{D}_{\q}(\x)$.
Using the weak convergence in probability of $ \widehat{G}_{n,\x}^{\q,*}(v)$ to invoke the representation theorem, we can show $a_n(\widehat{x}_n^{\star}-\x_0)=O_{\mathbb{P}}(1)$. The set convergence of the domain follows from the argument similar to the one used in the proof of Theorem \ref{Appendix Theorem: Asymptotic Distribution - High-level}.\qed 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Numerical derivative estimators
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of Lemma \ref{Appendix Lemma: numerical derivative estimators}}
\paragraph{Monomial approximation estimator}
\begin{align}
\label{Appendix eq: ND main term}
	\widetilde{\mathcal{D}}_{\q,n}^{\mathtt{MA}}(\x) &= \epsilon_n^{-(\q+1)}[\Gamma_0(\mathsf{x}+\epsilon_n) - \Gamma_0(\mathsf{x}) - \theta_0(\mathsf{x}) \{\Phi_0(\mathsf{x}+\epsilon_n) - \Phi_0(\mathsf{x})\} ] \\
	\label{Appendix eq: ND remainder1}
	&\quad + \epsilon_n^{-(\q+1/2)}n^{-1/2}\widehat{G}_{\x,n}(1;\epsilon_n) \\
\label{Appendix eq: ND remainder2}
	&\quad - \epsilon_n^{-\q}[\widehat{\theta}_n(\mathsf{x}) -\theta_0(\mathsf{x})] \widehat{R}_{\x,n}(1;\epsilon_n) \\
	\label{Appendix eq: ND remainder3}
	&\quad -\epsilon_n^{-(\q+1)}[\widehat{\theta}_n(\x)-\theta_0(\x)] [\Phi_0(\x+\epsilon_n)-\Phi_0(\x)].
\end{align}
The term \eqref{Appendix eq: ND main term} converges to $\frac{\partial\Phi_0(\x)\partial^{\q}\theta_0(\x)}{(1+\q)!}$ as argued in the proof of Theorem \ref{Appendix Theorem: Asymptotic Distribution - High-level}. The term \eqref{Appendix eq: ND remainder1} is $O_{\mathbb{P}}( [n\epsilon_n^{1+2\q}]^{-1/2} ) = o_{\mathbb{P}}(1)$, and the term \eqref{Appendix eq: ND remainder2} and \eqref{Appendix eq: ND remainder3} are $O_{\mathbb{P}}( n^{-\frac{\q}{2\q+1}} \epsilon_n^{-\q}) = o_{\mathbb{P}}(1)$ by $\epsilon_n^{2\q+1}n\to\infty$.
\paragraph{Forward difference estimator}
\begin{align}
	\label{Appendix eq: FD main term}
	\widetilde{\mathcal{D}}_{\q,n}^{\mathtt{FD}}(\x) &= \epsilon_n^{-(\q+1)}\sum_{k=1}^{\q+1}(-1)^{k+\q+1}\binom{\q+1}{k} [\Upsilon_0(\mathsf{x}+k\epsilon_n) - \Upsilon_0(\mathsf{x}) ] \\
	\label{Appendix eq: FD remainder1}
	&\quad + \epsilon_n^{-(\q+1/2)}n^{-1/2}\sum_{k=1}^{\q+1}(-1)^{k+\q+1}\binom{\q+1}{k} \widehat{G}_{\x,n}(k;\epsilon_n) \\
	\label{Appendix eq: FD remainder2}
	&\quad - \epsilon_n^{-\q}[\widehat{\theta}_n(\mathsf{x}) -\theta_0(\mathsf{x})] \sum_{k=1}^{\q+1}(-1)^{k+\q+1}\binom{\q+1}{k}\widehat{R}_{\x,n}(k;\epsilon_n) \\
	\label{Appendix eq: FD remainder3}
	&\quad -\epsilon_n^{-(\q+1)}[\widehat{\theta}_n(\x)-\theta_0(\x)]\sum_{k=1}^{\q+1}(-1)^{k+\q+1}\binom{\q+1}{k} [\Phi_0(\x+k\epsilon_n)-\Phi_0(\x)].
\end{align}
\eqref{Appendix eq: FD main term} converges to $\partial^{\q+1}\Upsilon_0(\x) = \partial\Phi_0(\x) \partial^{\q}\theta_0(\x)/(\q+1)!$ by the standard forward difference formula. For \eqref{Appendix eq: FD remainder1}-\eqref{Appendix eq: FD remainder3}, they are $o_{\mathbb{P}}(1)$ by the same argument as above for each $k$.
\paragraph{Bias-reduced estimator}
\begin{align}
	\label{Appendix eq: BR main term}
	\widetilde{\mathcal{D}}_{j,n}^{\mathtt{BR}}(\x) &= \epsilon_n^{-(j+1)}\sum_{k=1}^{\underline{\s}+1}\lambda_j^{\mathtt{BR}}(k) [\Upsilon_0(\mathsf{x}+c_k\epsilon_n) - \Upsilon_0(\mathsf{x}) ] \\
	\label{Appendix eq: BR remainder1}
	&\quad + \epsilon_n^{-(j+1/2)}n^{-1/2}\sum_{k=1}^{\underline{\s}+1}\lambda_j^{\mathtt{BR}}(k) \widehat{G}_{\x,n}(c_k;\epsilon_n) \\
	\label{Appendix eq: BR remainder2}
	&\quad - \epsilon_n^{-j}[\widehat{\theta}_n(\mathsf{x}) -\theta_0(\mathsf{x})] \sum_{k=1}^{\underline{\s}+1}\lambda_j^{\mathtt{BR}}(k)\widehat{R}_{\x,n}(c_k;\epsilon_n) \\
	\label{Appendix eq: BR remainder3}
	&\quad -\epsilon_n^{-(j+1)}[\widehat{\theta}_n(\x)-\theta_0(\x)]\sum_{k=1}^{\underline{\s}+1}\lambda_j^{\mathtt{BR}}(k) [\Phi_0(\x+c_k\epsilon_n)-\Phi_0(\x)].
\end{align}
\eqref{Appendix eq: BR remainder1} is $O_{\mathbb{P}}(\epsilon_n^{-(j+1/2)}n^{-1/2})$, \eqref{Appendix eq: BR remainder2} is $o_{\mathbb{P}}(\epsilon_n^{-j}a_n^{-\q})$, and \eqref{Appendix eq: BR remainder3} is $O_{\mathbb{P}}(\epsilon_n^{-j}a_n^{-\q})$. Note $\epsilon_n^{-(j+1/2)}n^{-1/2}= \epsilon_n^{-(j+1/2)} a_n^{-1/2} a_n^{-\q}$. For \eqref{Appendix eq: BR main term}, by the definition of $\{\lambda_j^{\mathtt{BR}}(k):k=1,\dots,\underline{\s}\}$,  
\begin{equation*}
	\epsilon_n^{-(j+1)}\sum_{k=1}^{\underline{\s}+1}\lambda_j^{\mathtt{BR}}(k) [\Upsilon_0(\mathsf{x}+c_k\epsilon_n) - \Upsilon_0(\mathsf{x}) ] =  \frac{\partial^{j+1}\Upsilon_0(\x)}{(j+1)!} + O\big(\epsilon_n^{\min\{\s,\underline{\s}+1\}-j}\big) 
\end{equation*}
and the first part of the lemma follows. For the second part, note
\begin{equation*}
	a_n^{\q-j}\big( \widetilde{\mathcal{D}}_{j,n}^{\mathtt{BR}}(\x) - \mathcal{D}_j(\x)\big) = O\big( a_n^{\q-j}\epsilon_n^{\min\{\s,\underline{\s}+1\}-j}\big) + O_{\mathbb{P}}\Big( [n\epsilon_n^{2\q+1}]^{-\frac{j}{2\q+1}}\Big)
\end{equation*}
and it is $o_{\mathbb{P}}(1)$ for every $(j,\q)\in\{1,\dots,\bar{q}\}^2$ if $n\epsilon_n^{(1+2\bar{\q})\min\{\s-1,\underline{\s}\}/(\bar{\q}-1)}\to 0$ and $n\epsilon_n^{2\bar{\q}+1}\to\infty$.
\hfill\qed 

\subsubsection{Higher-order expansion of the bias-reduced estimator}\label{Appendix Section: bias-reduced ND higher-order terms}
Here, we additionally assume that $\theta_0$ is $(\underline{\s}+1)$-times continusly differentiable and $\Phi_0$ is $(\underline{\s}+2)$-times continuously differentiable on $I_{\x}^{\delta}$ for some $\delta >0$. Then,
\begin{align*}
    \epsilon_n^{-(j+1)} &\sum_{k=1}^{\underline{\s}+1}\lambda_j^{\mathtt{BR}}(k) [\Upsilon_0(\mathsf{x}+c_k\epsilon_n) - \Upsilon_0(\mathsf{x}) ] - \frac{\partial^{j+1} \Upsilon_0(\x)}{(j+1)!} \\
    &=\epsilon_n^{\underline{\s}+1-j} \frac{\partial^{\underline{\s}+2}\Upsilon_0(\x)}{(\underline{\s}+2)!}\sum_{k=1}^{\underline{\s}+1} \lambda_j^{\mathtt{BR}}(k) c_k^{\underline{\s}+2} + o(\epsilon_n^{\underline{\s}+1-j} ).
\end{align*}
Also,
\begin{equation*}
    \epsilon_n^{-(j+1)}\sum_{k=1}^{\underline{\s}+1}\lambda_j^{\mathtt{BR}}(k) [\Phi_0(\x+c_k\epsilon_n)-\Phi_0(\x)] =  \frac{\partial^{j+1}\Phi_0(\x)}{(j+1)!} + O(\epsilon_n^{\underline{\s}+1-j}),
\end{equation*}
and
\begin{equation*}
    \eqref{Appendix eq: BR remainder3} = -[\widehat{\theta}_n(\x)-\theta_0(\x)] \frac{\partial^{j+1}\Phi_0(\x)}{(j+1)!} + O\big(\epsilon_n^{\underline{\s}+1-j} n^{-\frac{\q}{2\q+1}}\big).
\end{equation*}
The first term is independent of $\epsilon_n$, so we can treat \eqref{Appendix eq: BR remainder3} as a higher-order term (of smaller magnitude than \eqref{Appendix eq: BR main term}).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Primitive conditions for bootstrap approximation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of Lemma \ref{Appendix Lemma: Bootstrap primitive conditions}}
We verify that Assumptions \ref{Appendix Assumption A} and \ref{Appendix Assumption E} imply Assumptions \ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B Bootstrap Approximation}-\ref{Appendix Assumption B non-bootstrap Uniform Convergence Rate} and Assumption \ref{Appendix Assumption B Bootstrap Approximation} \ref{Appendix Assumption B: Bootstrap Weak Convergence}-\ref{Appendix Assumption B: Bootstrap Phi_hat}. 
Verifying Assumption \ref{Appendix Assumption D} is straightforward.

\subsubsection{Assumption  \ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Weak Convergence}}
By Assumption \ref{Appendix Assumption E} \ref{Appendix Assumption E: gamma hat}-\ref{Appendix Assumption E: phi hat},
\begin{equation*}
	\widehat{G}_{\x,n}^{\q}(v) = \sqrt{\frac{a_n}{n}} \sum_{i=1}^n \big\{\psi_{\x}(va_n^{-1};\mathbf{Z}_i) - \mathbb{E}[\psi_{\x}(va_n^{-1};\mathbf{Z}) ]  \big\}  + o_{\mathbb{P}}(1) 
\end{equation*}
where the remainder term is uniformly small over $\{v:|v|\leq K\}$ for any fixed $K>0$. Letting $\bar{\psi}_{\x,n}(v;\mathbf{Z}_i) = \sqrt{a_n}\psi_{\x}(va_n^{-1};\mathbf{Z}_i) $, we want to prove that the empirical process of $\{\bar{\psi}_{\x,n}(v;\cdot):|v|\leq K\}$ weakly converges to $\mathcal{G}_{\x}$. We verify finite-dimensional weak convergence and stochastic equicontinuity. 

Letting $\delta_n = Ka_n^{-1}$,
\begin{equation*}
	n^{-1}\mathbb{E}[|\bar{\psi}_{\x,n}(v;\mathbf{Z})|^4] \leq C n^{-1} a_n^2 \mathbb{E}[ \bar{D}_{\gamma}^{\delta_n} (\mathbf{Z})^4 +   \bar{D}_{\phi}^{\delta_n} (\mathbf{Z})^4] = o(1)
\end{equation*}
for all $|v|\leq K$. Also, convergence of the covariance kernel is imposed in Assumption \ref{Appendix Assumption E} \ref{Appendix Assumption E: covariance kernel}. Thus, the Lyapunov central limit theorem implies the finite-dimensional convergence. 

For stochastic equicontinuity, following the argument of \citet[Lemma 4.6]{Kim-Pollard_1990_AoS} and using $a_n\mathbb{E}[\bar{D}_{\gamma}^{\delta_n} (\mathbf{Z})^2 +   \bar{D}_{\phi}^{\delta_n} (\mathbf{Z})^2]=O(1)$, it suffices to show $\sup_{|v-s|\leq \eta_n,|v|\vee |s|\leq C} \frac{1}{n}\sum_{i=1}^n |\bar{\psi}_{\x,n}(v;\mathbf{Z}_i)-\bar{\psi}_{\x,n}(s;\mathbf{Z}_i)|^2=o_{\mathbb{P}}(1)$ for any $\eta_n=o(1)$.  For a constant $C>0$,
\begin{align*}
	&\sup_{|v-s|\leq \eta_n,|v|\vee |s|\leq K}\frac{1}{n}\sum_{i=1}^n |\bar{\psi}_{\x,n}(v;\mathbf{Z}_i)-\bar{\psi}_{\x,n}(s;\mathbf{Z}_i)|^2\\
	& \leq \frac{1}{n}\sum_{i=1}^n a_n [\bar{D}_{\gamma}^{\delta_n} (\mathbf{Z}_i)^2 +   \bar{D}_{\phi}^{\delta_n} (\mathbf{Z}_i)^2] \mathbbm{1}\{\bar{D}_{\gamma}^{\delta_n} (\mathbf{Z}_i) +   \bar{D}_{\phi}^{\delta_n} (\mathbf{Z}_i)> C\} \\
	&\quad + a_n C\sup_{|v-s|\leq \eta_n,|v|\vee |s|\leq K} \mathbb{E} \big[|\psi_{\x}(va_n^{-1};\mathbf{Z})-\psi_{\x}(sa_n^{-1};\mathbf{Z})| \big] \\
	&\quad +C a_n \sup_{|v-s|\leq \eta_n,|v|\vee |s|\leq K}\frac{1}{n}\sum_{i=1}^n  \big\{|\psi_{\x}(va_n^{-1};\mathbf{Z}_i)-\psi_{\x}(sa_n^{-1};\mathbf{Z}_i)| -\mathbb{E} \big[|\psi_{\x}(va_n^{-1};\mathbf{Z})-\psi_{\x}(sa_n^{-1};\mathbf{Z})| \big]\big\}
\end{align*}
where the first term after the inequality can be made arbitrarily small by making $C$ large using $a_n \mathbb{E}[ \bar{D}_{\gamma}^{\delta_n} (\mathbf{Z})^4 +   \bar{D}_{\phi}^{\delta_n} (\mathbf{Z})^4] = O(1)$. The second term is $o_{\mathbb{P}}(1)$ by Assumption \ref{Appendix Assumption E} \ref{Appendix Assumption E: covariance kernel}. Finally, the third term is $O_{\mathbb{P}}( \sqrt{a_n/n} )$ using Theorem 4.2 of \cite{Pollard_1989_SS}. 


\subsubsection{Assumptions  \ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Uniform Consistency}-\ref{Appendix Assumption B non-bootstrap Uniform Convergence Rate}}
For Assumption  \ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Uniform Consistency},
\begin{equation*}
    \sup_{x\in I}\big|\widehat{\Gamma}_n(x)-\Gamma_0(x)\big|\leq \sup_{x\in I}\big|\widehat{\Gamma}_n(x)-\bar{\Gamma}_n(x)\big| + \sup_{x\in I}\big|\bar{\Gamma}_n(x)-\Gamma_0(x)\big|
\end{equation*}
where the first term after the inequality is assumed to be $o_{\mathbb{P}}(1)$ and the second term is $o_{\mathbb{P}}(1)$ by standard empirical process arguments.
The identical argument implies $\sup_{x\in I}|\widehat{\Phi}_n(x)-\Phi_0(x)|=o_{\mathbb{P}}(1)$.
For $a_n \sup_{|v|\leq K}|\widehat{\Phi}_n(\x+va_n^{-1})-\Phi_0(\x+va_n^{-1})|=o_{\mathbb{P}}(1)$,
\begin{align*}
    \sup_{|v|\leq K}\big|\widehat{\Phi}_n(\x+va_n^{-1})-\Phi_0(\x+va_n^{-1})\big| &\leq \sup_{|v|\leq K}\big|\widehat{\Phi}_n(\x+va_n^{-1})-\widehat{\Phi}_n(\x)-\bar{\Phi}_n(\x+va_n^{-1})+\bar{\Phi}_n(\x)\big|\\
    &\quad + \sup_{|v|\leq K}\big|\bar{\Phi}_n(\x+va_n^{-1})-\bar{\Phi}_n(\x)-\Phi_0(\x+va_n^{-1})+\Phi_0(\x)\big|\\
    &\quad + \big|\widehat{\Phi}_n(\x)-\bar{\Phi}_n(\x)\big| + \big|\bar{\Phi}_n(\x)-\Phi_0(\x)\big|
\end{align*}
where the first term after the inequality is $o_{\mathbb{P}}((na_n)^{-1/2})=o_{\mathbb{P}}(a_n^{-1})$ and $|\widehat{\Phi}_n(\x)-\bar{\Phi}_n(\x)|=o_{\mathbb{P}}(a_n^{-1})$. The remaining terms are also $o_{\mathbb{P}}(a_n^{-1})$ by standard arguments.


\subsubsection{Assumption \ref{Appendix Assumption B Bootstrap Approximation} \ref{Appendix Assumption B: Bootstrap Weak Convergence}}
First we posit 
\begin{align}\nonumber
		&\sqrt{na_{n}}\sup_{|v|\leq K}|\widehat{\Gamma}_{n}^{\ast }(\mathsf{x}+va_{n}^{-1})-\widehat{\Gamma}_{n}^{\ast }(\mathsf{x})-\bar{\Gamma}_{n}^{\ast }(\mathsf{x}+va_{n}^{-1})+\bar{\Gamma}_{n}^{\ast }(\mathsf{x})|=o_{\mathbb{P}}(1),\\
  \label{Appendix eq: high-level bootstrap approximation}
  &\sqrt{na_{n}}\sup_{|v|\leq K}|\widehat{\Phi}_{n}^{\ast }(\mathsf{x}+va_{n}^{-1})-\widehat{\Phi}_{n}^{\ast }(\mathsf{x})-\bar{\Phi}_{n}^{\ast }(\mathsf{x}+va_{n}^{-1})+\bar{\Phi}_{n}^{\ast }(\mathsf{x})|=o_{\mathbb{P}}(1),
\end{align}
which follows from the hypothesis of the lemma as shown below.
By the above display,
\begin{equation*}
	 	\widehat{G}_{\x,n}^{\q,*}(v) = \sqrt{\frac{a_n}{n}} \sum_{i=1}^n W_{i,n} \Big\{\psi_{\x}(va_n^{-1};\mathbf{Z}_i) - \frac{1}{n}\sum_{j=1}^n\psi_{\x}(va_n^{-1};\mathbf{Z}_j)   \Big\}   + o_{\mathbb{P}}(1)
\end{equation*}
where we use $\sqrt{\frac{a_n}{n}} \sum_{i=1}^nW_{i,n} \{\phi_0(\x+va_n^{-1};\mathbf{Z}_i) - \phi_0(\x;\mathbf{Z}_i) - \frac{1}{n}\sum_{j=1}^n[\phi_0(\x+va_n^{-1};\mathbf{Z}_j) - \phi_0(\x;\mathbf{Z}_j)]\}=O_{\mathbb{P}}(1)$, and $\widehat{\theta}_n(\x)\to_{\mathbb{P}}\theta_0(\x)$.
Let $\widehat{\psi}_{\x,n}(va_n^{-1};\mathbf{Z})=  \sqrt{a_n}[\psi_{\x}(va_n^{-1};\mathbf{Z}) - \frac{1}{n}\sum_{j=1}^n\psi_{\x}(va_n^{-1};\mathbf{Z}_j)]$ and to prove the finite-dimensional convergence, we apply Lemma 3.6.15 of \cite{vanderVaart-Wellner_1996_Book}. Assumption \ref{Appendix Assumption E} \ref{Appendix Assumption E: bootstrap weights} implies $\frac{1}{n}\sum_{i=1}^n(W_{i,n}-1)^2\to_{\P}1$ and $n^{-1}\max_{1\leq i \leq n}W_{i,n}^2=o_{\mathbb{P}}(1)$. Since
\begin{align*}
	\frac{1}{n}\sum_{i=1}^n \widehat{\psi}_{\x,n}(va_n^{-1};\mathbf{Z}_i)\widehat{\psi}_{\x,n}(ua_n^{-1};\mathbf{Z}_i) &= a_n \bigg[\frac{1}{n}\sum_{i=1}^n \psi_{\x}(va_n^{-1};\mathbf{Z}_i)\psi_{\x}(ua_n^{-1};\mathbf{Z}_i) \\
	&\qquad - \frac{1}{n}\sum_{i=1}^n \psi_{\x}(va_n^{-1};\mathbf{Z}_i)\frac{1}{n}\sum_{i=1}^n \psi_{\x}(ua_n^{-1};\mathbf{Z}_i) \bigg]
\end{align*}
and $\sup_{|v|\leq\delta } \psi_{\x}(v;\mathbf{Z})\leq \bar{D}_{\gamma}^{\delta}(\mathbf{Z}) + |\theta_0(\x)|\bar{D}_{\phi}^{\delta}(\mathbf{Z})$, for any $v,u\in\mathbb{R}$,
\begin{equation*}
	\frac{1}{n}\sum_{i=1}^n \widehat{\psi}_{\x,n}(va_n^{-1};\mathbf{Z}_i)\widehat{\psi}_{\x,n}(ua_n^{-1};\mathbf{Z}_i) - a_n\mathbb{E}[\psi_{\x}(va_n^{-1};\mathbf{Z})\psi_{\x}(ua_n^{-1};\mathbf{Z})] =o_{\mathbb{P}}(1).
\end{equation*}
Also, $\frac{1}{n}\sum_{i=1}^n \widehat{\psi}_{\x,n}^4(va_n^{-1};\mathbf{Z}_i)=O_{\mathbb{P}}(1)$ and we verified the hypothesis of the lemma.

For stochastic equicontinuity, Lemma 3.6.7 of \cite{vanderVaart-Wellner_1996_Book} implies that for any $n_0 \in \{1,\dots, n\}$,
\begin{align*}
	&\mathbb{E}\bigg[ \sup_{|v-u|\leq \eta_n, |v|\vee |u|\leq K} \bigg| \frac{1}{\sqrt{n}} \sum_{i=1}^n W_{i,n}\big[  \widehat{\psi}_{\x,n}(va_n^{-1};\mathbf{Z}_i)-\widehat{\psi}_{\x,n}(ua_n^{-1};\mathbf{Z}_i)\big]\bigg| \Big| \{\mathbf{Z}_i\}_{i=1}^n\bigg] \\
	&\leq C \frac{\sqrt{a_n}}{n}\sum_{i=1}^n \big[\bar{D}_{\gamma}^{\delta_n}(\mathbf{Z}_i) + \bar{D}_{\phi}^{\delta_n}(\mathbf{Z}_i)\big] (n_0-1) \mathbb{E}\max_{1\leq i \leq n} |W_{i,n}| n^{-1/2}\\
	&\quad + C \max_{n_0\leq k \leq n} \mathbb{E}\bigg[ \sup_{|v-u|\leq \eta_n, |v|\vee |u|\leq K} \bigg| \frac{1}{\sqrt{k}} \sum_{i=n_0}^k \big[  \widehat{\psi}_{\x,n}(va_n^{-1};\mathbf{Z}_{R_i})-\widehat{\psi}_{\x,n}(ua_n^{-1};\mathbf{Z}_{R_i})\big]\bigg| \Big| \{\mathbf{Z}_i\}_{i=1}^n\bigg]
\end{align*}
where $(R_1,\dots,R_n)$ is uniformly distributed on the set of all permutations of $\{1,\dots,n\}$, independent of $\{\mathbf{Z}_i\}_{i=1}^n$. Choose $n_0$ such that $n^{1/2-1/\mathfrak{r}}/n_0\to \infty$ and $n_0/a_n\to \infty$, which is possible by $\mathfrak{r}>(4\q+2)/(2\q-1)$. Following the argument of \citet[Theorem 3.6.13]{vanderVaart-Wellner_1996_Book}, it suffices to bound
\begin{equation*}
	\max_{n_0\leq k \leq n} \mathbb{E}^* \sup_{|v-u|\leq \eta_n, |v|\vee |u|\leq K} \bigg| \frac{1}{\sqrt{k}} \sum_{i=1}^k \big[  \widehat{\psi}_{\x,n}(va_n^{-1};\mathbf{Z}_i^*)-\widehat{\psi}_{\x,n}(ua_n^{-1};\mathbf{Z}_i^*)\big]\bigg| 
\end{equation*}
where $\{\mathbf{Z}_i^*\}_{i=1}^k$ denotes a random sample from the empirical CDF and $\mathbb{E}^*$ is the expectation under this empirical bootstrap law. Following the argument of \citet[Lemma 4.6]{Kim-Pollard_1990_AoS}, it suffices to show
\begin{equation*}
	\max_{n_0\leq k \leq n}\mathbb{E}^*  \sup_{|v-u|\leq \eta_n, |v|\vee |u|\leq K}  \frac{1}{k} \sum_{i=1}^k \big| \widehat{\psi}_{\x,n}(va_n^{-1};\mathbf{Z}_i^*)-\widehat{\psi}_{\x,n}(ua_n^{-1};\mathbf{Z}_i^*)\big|^2 = o_{\mathbb{P}}(1).
\end{equation*}
\begin{align*}
	&\mathbb{E}^*  \sup_{|v-u|\leq \eta_n, |v|\vee |u|\leq K}  \frac{1}{k} \sum_{i=1}^k \big| \widehat{\psi}_{\x,n}(va_n^{-1};\mathbf{Z}_i^*)-\widehat{\psi}_{\x,n}(ua_n^{-1};\mathbf{Z}_i^*)\big|^2\\
	&\leq  a_n  \frac{1}{n}\sum_{i=1}^n  \big[ \bar{D}_{\gamma}^{\delta_n}(\mathbf{Z}_i)^2 + \bar{D}_{\phi}^{\delta_n}(\mathbf{Z}_i)^2\big]\mathbbm{1}\{\bar{D}_{\gamma}^{\delta_n}(\mathbf{Z}_i) + \bar{D}_{\phi}^{\delta_n}(\mathbf{Z}_i) > C\} \\
	&\quad + C   a_n\sup_{|v-u|\leq \eta_n, |v|\vee |u|\leq K} \frac{1}{n}\sum_{i=1}^n \big| \psi_{\x}(va_n^{-1};\mathbf{Z}_i)-\psi_{\x}(ua_n^{-1};\mathbf{Z}_i)\big| \\
	&\quad +  C   a_n\mathbb{E}^* \sup_{|v-u|\leq \eta_n, |v|\vee |u|\leq K} \frac{1}{k} \sum_{i=1}^k \big| \psi_{\x}(va_n^{-1};\mathbf{Z}_i^*)-\psi_{\x}(ua_n^{-1};\mathbf{Z}_i^*)\big|\\
	&\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad - \mathbb{E}^*\big[\big| \psi_{\x}(va_n^{-1};\mathbf{Z}^*)-\psi_{\x}(ua_n^{-1};\mathbf{Z}^*)\big|\big].
\end{align*}
The first term after the inequality does not depend on $k$ and its expectation can be made arbitrarily small by taking $C$ sufficiently large. The second term is independent of $k$ and we can handle this term by adding and subtracting the expectation inside the summation. For the third term, applying Theorem 4.2 of \cite{Pollard_1989_SS} again, it is bounded by a constant multiple of
\begin{equation*}
	a_n k^{-1/2}  \bigg(\frac{1}{n}\sum_{i=1}^n \big[ \bar{D}_{\gamma}^{\delta_n}(\mathbf{Z}_i)^2 + \bar{D}_{\phi}^{\delta_n}(\mathbf{Z}_i)^2\big] \bigg)^{1/2} = O_{\mathbb{P}}\big(  \sqrt{a_n/k} \big),
\end{equation*}
which is $o_{\mathbb{P}}(1)$ by the choice of $n_0$.

\paragraph{Verifying \eqref{Appendix eq: high-level bootstrap approximation}}
We focus on the first display.
By adding and subtracting the bootstrap means,
\begin{align*}
	&\widehat{\Gamma}_n^*(\x + va_n^{-1}) -\widehat{\Gamma}_n^*(\x) - \bar{\Gamma}_n^*(\x+va_n^{-1})+ \bar{\Gamma}_n^*(\x)\\
	&= \frac{1}{n}\sum_{i=1}^n (W_{i,n}-1)[\widehat{\gamma}(\x+va_n^{-1};\mathbf{Z}_i)-\widehat{\gamma}(\x;\mathbf{Z}_i) - \gamma_0(\x+va_n^{-1};\mathbf{Z}_i)+\gamma_0(\x;\mathbf{Z}_i)]\\
	&\quad + \check{\Gamma}_n(\x+va_n^{-1}) - \check{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+va_n^{-1}) + \bar{\Gamma}_n(\x)
\end{align*}
where $\check{\Gamma}_n(x) = \frac{1}{n}\sum_{i=1}^n \widehat{\gamma}(x;\mathbf{Z}_i)$.
Let
\begin{align*}
	\widetilde{\gamma}_n(v;\mathbf{Z}) &= \widehat{\gamma}(\x+va_n^{-1};\mathbf{Z}_i)-\widehat{\gamma}(\x;\mathbf{Z}_i) - \gamma_0(\x+va_n^{-1};\mathbf{Z}_i)+\gamma_0(\x;\mathbf{Z}_i)\\
 &\qquad - \check{\Gamma}_n(\x+va_n^{-1}) + \check{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+va_n^{-1}) + \bar{\Gamma}_n(\x).
\end{align*}
Lemma 3.6.7 of \cite{vanderVaart-Wellner_1996_Book} implies
\begin{align}
\nonumber
	&\sqrt{na_n} \mathbb{E}\bigg[ \sup_{ |v|\leq K}  \bigg|\frac{1}{n}\sum_{i=1}^n W_{i,n}\widetilde{\gamma}_n(v;\mathbf{Z}_i) \bigg| \Big\vert \{\mathbf{Z}_i\}_{i=1}^n \bigg]\\
 \nonumber
	&\leq C \frac{\sqrt{a_n}}{n} \sum_{i=1}^n \sup_{ |v|\leq K} |\widetilde{\gamma}_n(v;\mathbf{Z}_i) | (n_0-1) \mathbb{E}[\max_{1\leq i \leq n}|W_{i,n}|] n^{-1/2}\\
 \nonumber
	&\quad + C\sqrt{a_n} \max_{n_0\leq k\leq n} \mathbb{E}\bigg[ \sup_{ |v|\leq K}\bigg|\frac{1}{\sqrt{k}}\sum_{i=n_0}^k \widetilde{\gamma}_n(v;\mathbf{Z}_{R_i})   \bigg|  \Big\vert \{\mathbf{Z}_i\}_{i=1}^n \bigg] \\
 \label{Appendix eq: high-level bootstrap approximation remainder 1}
	&\leq C  \frac{\sqrt{a_n}}{n} \sum_{i=1}^n \sup_{ |v|\leq K} |\widetilde{\gamma}_n(v;\mathbf{Z}_i) | \frac{n_0 n^{\mathfrak{r}}}{\sqrt{n}} \\
 \label{Appendix eq: high-level bootstrap approximation remainder 2}
	&\quad +  C \sqrt{a_n}\max_{n_0\leq k\leq n} \mathbb{E}^* \sup_{ |v|\leq K} \bigg|\frac{1}{\sqrt{k}}\sum_{i=1}^k \widetilde{\gamma}_n(v;\mathbf{Z}_i^*)  \bigg|.
\end{align}
For \eqref{Appendix eq: high-level bootstrap approximation remainder 1},
\begin{align*}
	& \frac{\sqrt{a_n}}{n} \sum_{i=1}^n \sup_{ |v|\leq K} |\widetilde{\gamma}_n(v;\mathbf{Z}_i) |\\
	&\leq \frac{\sqrt{a_n}}{n} \sum_{i=1}^n \sup_{ |v|\leq K} |\widehat{\gamma}(\x+va_n^{-1};\mathbf{Z}_i)-\widehat{\gamma}(\x;\mathbf{Z}_i) -\gamma_0(\x+va_n^{-1};\mathbf{Z}_i) + \gamma_0(\x;\mathbf{Z}_i) | \\
	&\quad + \sqrt{a_n} \sup_{ |v|\leq K} \big|\check{\Gamma}_n(\x+va_n^{-1}) - \check{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+va_n^{-1}) + \bar{\Gamma}_n(\x)\big|
\end{align*}
and both terms are $o_{\mathbb{P}}(1)$ by $a_n\frac{1}{n}\sum_{i=1}^n \sup_{|v|\leq K}|\widehat{\gamma}(\x+va_n^{-1};\mathbf{Z}_i)-\widehat{\gamma}(\x;\mathbf{Z}_i)-\gamma_0(\x+va_n^{-1};\mathbf{Z}_i)+\gamma_0(\x;\mathbf{Z}_i)|^2=o_{\mathbb{P}}(1)$.
For \eqref{Appendix eq: high-level bootstrap approximation remainder 2}, Corollary 4.3 of \cite{Pollard_1989_SS} implies
\begin{equation*}
	\mathbb{E}^* \sup_{ |v|\leq K} \bigg|\frac{1}{\sqrt{k}}\sum_{i=1}^k \widetilde{\gamma}_n(v;\mathbf{Z}_i^*)  \bigg| \leq \frac{1}{n}\sum_{i=1}^n\sup_{ |v|\leq K} |\widehat{\gamma}(\x+va_n^{-1};\mathbf{Z}_i)-\widehat{\gamma}(\x;\mathbf{Z}_i) -\gamma_0(\x+va_n^{-1};\mathbf{Z}_i) + \gamma_0(\x;\mathbf{Z}_i) |^2
\end{equation*}
and this term is $o_{\mathbb{P}}(a_n^{-1})$ by  Assumption \ref{Appendix Assumption E} \ref{Appendix Assumption E: gamma hat}.


\subsubsection{Assumption \ref{Appendix Assumption B Bootstrap Approximation} \ref{Appendix Assumption B: Bootstrap Gamma_hat}}
\begin{equation*}
    \sup_{x\in I}\big|\widehat{\Gamma}_n^*(x)-\widehat{\Gamma}_n(x)\big| \leq \sup_{x\in I}\big|\widehat{\Gamma}_n^*(x)-\bar{\Gamma}_n^*(x)\big| + \sup_{x\in I}\big|\bar{\Gamma}_n^*(x)-\bar{\Gamma}_n(x)\big|+ \sup_{x\in I}\big|\bar{\Gamma}_n(x)-\widehat{\Gamma}_n(x)\big|
\end{equation*}
where the last term is $o_{\mathbb{P}}(1)$ by the hypothesis. For $\widehat{\Gamma}_n^*(x)-\bar{\Gamma}_n^*(x)$,
\begin{align*}
    \sup_{x\in I}\big|\widehat{\Gamma}_n^*(x)-\bar{\Gamma}_n^*(x)\big| &\leq \frac{1}{n}\sum_{i=1}^n |W_{i,n}| \sup_{x\in I} \big| \widehat{\gamma}_n(x;\mathbf{Z}_i) - \gamma_0(x;\mathbf{Z}_i) \big| \\
    &\leq \sqrt{\frac{1}{n}\sum_{i=1}^n |W_{i,n}|^2 \frac{1}{n}\sum_{i=1}^n  \sup_{x\in I} \big| \widehat{\gamma}_n(x;\mathbf{Z}_i) - \gamma_0(x;\mathbf{Z}_i) \big|^2 }
\end{align*}
and the last term is $o_{\mathbb{P}}(1)$ by the hypothesis. For $\bar{\Gamma}_n^*(x)-\bar{\Gamma}_n(x)$, Lemma 3.6.7 of \cite{vanderVaart-Wellner_1996_Book} implies
\begin{align*}
    \mathbb{E}\Big[\sup_{x\in I}\big|\bar{\Gamma}_n^*(x)-\bar{\Gamma}_n(x)\big|\Big\vert \{\mathbf{Z}_i\}_{i=1}^n\Big] &\leq C \frac{1}{n}\sum_{i=1}^n \sup_{x\in I}|\gamma_0(x;\mathbf{Z}_i)| \mathbb{E}[\max_{1\leq i \leq n}|W_{i,n}|] n^{-1/2} \\
    &\quad + C n^{-1/2} \max_{\lfloor\sqrt{n}\rfloor\leq k\leq n} \mathbb{E}\bigg[\sup_{x\in I}\Big| \frac{1}{\sqrt{k}}\sum_{i=\lfloor\sqrt{n}\rfloor }^k \bar{\gamma}_n(x;\mathbf{Z}_{R_i}) \Big| \bigg\vert \{\mathbf{Z}_i\}_{i=1}^n\bigg]
\end{align*}
where $\bar{\gamma}_n(x;\mathbf{z}) =\gamma_0(x;\mathbf{z}) - \frac{1}{n}\sum_{i=1}^n \gamma_0(x;\mathbf{Z}_i)$ and $(R_1,\dots,R_n)$ is uniformly distributed on the set of all permutations of $\{1,\dots,n\}$, independent of $\{\mathbf{Z}_i\}_{i=1}^n$. By the same argument as for verifying Assumption \ref{Appendix Assumption B Bootstrap Approximation} \ref{Appendix Assumption B: Bootstrap Weak Convergence}, it suffices to show
\begin{equation*}
    n^{-1/2} \max_{\lfloor\sqrt{n}\rfloor\leq k\leq n}\mathbb{E}^*\sup_{x\in I}\bigg|\frac{1}{\sqrt{k}}\sum_{i=1}^k \bar{\gamma}_n(x;\mathbf{Z}_i^*)\bigg| = o_{\mathbb{P}}(1)
\end{equation*}
where $\{\mathbf{Z}_i^*\}_{i=1}^k$ denotes a random sample from the empirical CDF and $\mathbb{E}^*$ is the expectation under this empirical bootstrap law. Corollary 4.3 of \cite{Pollard_1989_SS} implies the desired result.

\subsubsection{Assumption \ref{Appendix Assumption B Bootstrap Approximation} \ref{Appendix Assumption B: Bootstrap Phi_hat}}
$\sup_{x\in I}|\widehat{\Phi}_n^*(x)-\widehat{\Phi}_n(x)|=o_{\mathbb{P}}(1)$ follows from the same argument as for $\widehat{\Gamma}_n^*$. 
\begin{align*}
    a_n\sup_{|v|\leq K}\big|\widehat{\Phi}_n^*(\x+va_n^{-1})-\widehat{\Phi}_n&(\x+va_n^{-1})\big| \leq a_n\big|\widehat{\Phi}_n^*(\x)-\widehat{\Phi}_n(\x)\big|  \\
    &+  a_n\sup_{|v|\leq K}\big|\widehat{\Phi}_n^*(\x+va_n^{-1})- \widehat{\Phi}_n^*(\x) -\widehat{\Phi}_n(\x+va_n^{-1})+\widehat{\Phi}_n(\x)\big|
\end{align*}
The first term after the inequality is $o_{\mathbb{P}}(1)$ by the hypothesis. The second term is $o_{\mathbb{P}}(1)$ by the stochastic equicontinuity argument for Assumption \ref{Appendix Assumption B Bootstrap Approximation} \ref{Appendix Assumption B: Bootstrap Weak Convergence}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bootstrap inconsistency
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of Theorem \ref{Appendix Theorem: Bootstrap inconsistency}}
The proof closely follows \cite{Kosorok_2008_BookCh}.
Let $r_n=n^{\frac{\q}{2\q+1}}$.
Suppose for contradiction that the bootstrap approximation is consistent i.e.,
\begin{equation*}
	r_n\big( \widehat{\theta}_n^*(\mathsf{x})-\widehat{\theta}_n(\mathsf{x}) \big) \leadsto_{\mathbb{P}}  (\partial \Phi_0(\mathsf{x}))^{-1} \partial_{-}\GCM_{\mathbb{R}}\{\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q}\}(0).
\end{equation*}
Then, by Theorem 2.2 of \cite{Kosorok_2008_BookCh}, we have 
\begin{equation}\label{eqBootFail:contradiction}
	r_n\big( \widehat{\theta}_n^*(\mathsf{x}) - \theta_0(\mathsf{x}) \big) \leadsto \sqrt{2} (\partial \Phi_0(\mathsf{x}))^{-1} \partial_{-}\GCM_{\mathbb{R}}\{\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q}\}(0)
\end{equation}
where the convergence in distribution is unconditional.

Now, using the switching lemma, $\mathbb{P}[r_n\big( \widehat{\theta}_n^*(\mathsf{x}) - \theta_0(\mathsf{x})\big)  > t]$ equals
\begin{equation*}
	\mathbb{P}\Big[ \sup\argmax_{v\in \widehat{I}_n} \big\{ - \widehat{\Gamma}_n^*(v) +  [\theta_0(\mathsf{x}) + r_n^{-1}t]\widehat{\Phi}_n^*(v) \big\} < (\widehat{\Phi}_n^*)^{-}\big( \widehat{\Phi}_n^*(\mathsf{x}) \big) \Big] 
\end{equation*}
and to characterize the limiting distribution of $r_n(\widehat{\theta}_n^*(\mathsf{x}) - \theta_0(\mathsf{x}))$, it suffices to look at
\begin{align}
\label{eqBootFail:mean_zero}
	&  -  \frac{a_n^{\q+1}}{n}\sum_{i=1}^n \bar{W}_{i,n} \big\{\widehat{\gamma}_n(\mathsf{x}+va_n^{-1};\mathbf{Z}_i)  - \widehat{\gamma}_n(\mathsf{x};\mathbf{Z}_i)   - \theta_0(\mathsf{x}) \{\widehat{\phi}_n(\mathsf{x}+va_n^{-1};\mathbf{Z}_i) - \widehat{\phi}_n(\x;\mathbf{Z}_i) \} \big\}  \\
\label{eqBootFail:boot_mean}
	& - a_n^{\q+1}  \big[ \widehat{\Gamma}_n(\mathsf{x}+va_n^{-1}) -\widehat{\Gamma}_n(\mathsf{x}) - \theta_0(\mathsf{x})\{ \widehat{\Phi}_n(\mathsf{x}+va_n^{-1}) -\widehat{\Phi}_n(\mathsf{x})\}  \big] \\
\label{eqBootFail:linear}
	& + a_n t \big[\widehat{\Phi}_n^*(\mathsf{x}+va_n^{-1})  - \widehat{\Phi}_n^*(\mathsf{x}) \big]
\end{align}
where $\bar{W}_{i,n}=W_{i,n}-1$.
The term \eqref{eqBootFail:mean_zero} conditionally weakly converge to $-\mathcal{G}_{x}$ on compacta and the term \eqref{eqBootFail:linear} converges in probability to $t \partial \Phi_0(\mathsf{x})$.
The term \eqref{eqBootFail:boot_mean} weakly converges to $ -\mathcal{G}_{x} (v) - \mathcal{M}_{\x}^{\q}(v)$ unconditionally.
Thus, 
\begin{equation*}
	\mathbb{P} \big[ r_n\big( \widehat{\theta}_n^*(\mathsf{x}) - \theta_0(\mathsf{x}) \big) >t\big] \to \mathbb{P}\Big[ \argmax_{v\in \mathbb{R}} \Big\{ -\sqrt{2} \mathcal{G}_{\mathsf{x}}(v) - \mathcal{M}_{\x}^{\q}(v)  + t \partial\Phi_0(\mathsf{x}) v \Big\} < 0 \Big] .
\end{equation*}
Note $\mathcal{G}_{\mathsf{x}}(a v)=_d \sqrt{|a|} \mathcal{G}_{\mathsf{x}}(v)$, and using the change of variable $v=u2^{\frac{1}{2\q+1}}$, the limit distribution equals
\begin{align*}
	&\mathbb{P}\Big[   2^{\frac{1}{2\q+1}} \argmax_{u\in \mathbb{R}} \Big\{ -  \mathcal{G}_{\mathsf{x}}(u) - \mathcal{M}_{\x}^{\q}(v) +  2^{-\frac{\q}{2\q+1}} t \partial\Phi_0(\mathsf{x})u   \Big\} < 0 \Big] \\
	&= \mathbb{P}\Big[   2^{\frac{\q}{2\q+1}} (\partial\Phi_0(\mathsf{x}))^{-1}  \partial_{-} \GCM_{\mathbb{R}} \{\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q}\}(0) >  t  \Big].
\end{align*} 
Thus,
\begin{equation*}
	r_n\big( \widehat{\theta}_n^*(\mathsf{x}) - \theta_0(\mathsf{x}) \big) \leadsto 2^{\frac{\q}{2\q+1}} (\partial \Phi_0(\mathsf{x}))^{-1} \partial_{-}\GCM_{\mathbb{R}}\{\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q}\}(0)
\end{equation*}
and this limit contradicts with \eqref{eqBootFail:contradiction} as $2^{\frac{\q}{2\q+1}} \neq \sqrt{2}$, proving that the bootstrap estimator $\widehat{\theta}_n^*(\x)$ fails to approximate the limit distribution. \hfill$\qedsymbol$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Sufficient conditions for examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Verifying conditions in examples}\label{Appendix Section: verifying conditions - examples}
In this section, we demonstrate that our general theory is easily applicable to the examples considered in Section \ref{Appendix Section: Examples}. For this purpose, one should verify Assumptions \ref{Appendix Assumption A}, \ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Covariance Kernel}-\ref{Appendix Assumption B non-bootstrap Support Consistency}, and \ref{Appendix Assumption E} for each example. Then, Lemma \ref{Appendix Lemma: Bootstrap primitive conditions} implies that our general results (Theorems \ref{Appendix Theorem: Asymptotic Distribution - High-level} and \ref{Appendix Theorem: Bootstrap Approximation - High-level}) apply.
Since it is straightforward to check Assumptions \ref{Appendix Assumption A}, \ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Support Consistency}, and \ref{Appendix Assumption E} \ref{Appendix Assumption E: iid sample}-\ref{Appendix Assumption E: bootstrap weights}, we focus on \ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Covariance Kernel} and \ref{Appendix Assumption E} \ref{Appendix Assumption E: gamma hat}-\ref{Appendix Assumption E: covariance kernel}.

When $\gamma_0$ is known (i.e.,\ no preliminary estimations are needed), then Assumption \ref{Appendix Assumption E} \ref{Appendix Assumption E: gamma hat} reduces to: for some $V\in (0,2)$, 
\begin{equation}\label{Appendix Eq: simpler D gamma hat}
    \limsup_{\varepsilon\downarrow0}\frac{\log N_{U}(\varepsilon,\mathfrak{F}_{\gamma})}{\varepsilon^{-V}}<\infty,\qquad \mathbb{E}[\bar{F}_{\gamma}(\mathbf{Z})^2] <\infty,\qquad \limsup_{\delta\downarrow0}\frac{\mathbb{E}[\bar{D}_{\gamma}^{\delta}(\mathbf{Z})^2+\bar{D}_{\gamma}^{\delta}(\mathbf{Z})^4]}{\delta}<\infty.
\end{equation}
 An identical remark applies to $\phi_0$ and Assumption \ref{Appendix Assumption E} \ref{Appendix Assumption E: phi hat}.

In addition, as remarked in the main paper after Lemma 2, the second display of \ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Covariance Kernel} follows from the second display of \ref{Appendix Assumption E} \ref{Appendix Assumption E: covariance kernel}, and the first display of \ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Covariance Kernel} follows from 
 \begin{equation}\label{Appendix eq: uniform version of SA-5 (5)}
     \lim_{n\to\infty} \delta_n^{-1}\mathbb{E}[\psi_{\x_n}(s\delta_n;\mathbf{Z})\psi_{\x_n}(t\delta_n;\mathbf{Z})] = \mathcal{C}_{\x}(s,t)  
 \end{equation}
 for $a_n\delta_n=O(1)$ and any $\x_n\to \x$. To see the second claim,
 \begin{align*}
     &\delta_n^{-1} \big\{ \mathbb{E}[\psi_{\x}( (s+t)\delta_n;\mathbf{Z})\psi_{\x}((s+t)\delta_n;\mathbf{Z})] - 2\mathbb{E}[\psi_{\x}( s\delta_n;\mathbf{Z})\psi_{\x}((s+t)\delta_n;\mathbf{Z})] \\
     &\qquad + \mathbb{E}[\psi_{\x}( s\delta_n;\mathbf{Z})\psi_{\x}(s\delta_n;\mathbf{Z})] \big\} \to \mathcal{C}_{\x}(s+t,s+t) -  \mathcal{C}_{\x}(s+t,s) -  \mathcal{C}_{\x}(s,s+t) + \mathcal{C}_{\x}(s,s)
 \end{align*}
 and at the same time, setting $\x_n=\x+s\delta_n$,
 \begin{align*}
     &\delta_n^{-1} \big\{ \mathbb{E}[\psi_{\x}( (s+t)\delta_n;\mathbf{Z})\psi_{\x}((s+t)\delta_n;\mathbf{Z})] - 2\mathbb{E}[\psi_{\x}( s\delta_n;\mathbf{Z})\psi_{\x}((s+t)\delta_n;\mathbf{Z})] \\
     &\qquad + \mathbb{E}[\psi_{\x}( s\delta_n;\mathbf{Z})\psi_{\x}(s\delta_n;\mathbf{Z})] \big\}\\
     &= \delta_n^{-1} \big\{ \mathbb{E}[ \{\psi_{\x_n}( t\delta_n;\mathbf{Z})-\psi_{\x_n}( -s\delta_n;\mathbf{Z})\}\{\psi_{\x_n}(t\delta_n;\mathbf{Z})-\psi_{\x_n}( -s\delta_n;\mathbf{Z})\}]  \\
     &\qquad + 2\mathbb{E}[\psi_{\x_n}( -s\delta_n;\mathbf{Z})\{\psi_{\x_n}(t\delta_n;\mathbf{Z})-\psi_{\x_n}( -s\delta_n;\mathbf{Z})\}] + \mathbb{E}[\psi_{\x_n}( -s\delta_n;\mathbf{Z})\psi_{\x_n}( -s\delta_n;\mathbf{Z})] \big\} \\
     &= \delta_n^{-1} \mathbb{E}[ \psi_{\x_n}( t\delta_n;\mathbf{Z})\psi_{\x_n}(t\delta_n;\mathbf{Z})]\to \mathcal{C}_{\x}(t,t) 
 \end{align*}
and thus, $\mathcal{C}_{\x}(s+t,s+t) -  \mathcal{C}_{\x}(s+t,s) -  \mathcal{C}_{\x}(s,s+t) + \mathcal{C}_{\x}(s,s)=\mathcal{C}_{\x}(t,t)$ holds.
Thus, for the two displays in \ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Covariance Kernel}, it suffices to check \ref{Appendix Assumption E} \ref{Appendix Assumption E: covariance kernel} and \eqref{Appendix eq: uniform version of SA-5 (5)}.
 
\subsection{Monotone density function}
For monotone density estimation, $\Phi_0$ is the identity map, so Assumption \ref{Appendix Assumption E} \ref{Appendix Assumption E: phi hat} holds trivially.
\subsubsection{No censoring}
The covariance kernel is 
\begin{equation*}
    \mathcal{C}_{\x}(s,t) = f_0(\x)\min\{|s|,|t|\} \mathbbm{1}\{\mathrm{sign}(s)=\mathrm{sign}(t)\}.
\end{equation*}
\paragraph{\ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Covariance Kernel}}
It is clear that $\mathcal{C}_{\x}(1,1)>0$ from $f_0(\x)>0$. Also, $\mathcal{C}_{\x}(1,\delta)/\sqrt{\delta}=f_0(\x) \sqrt{\delta} \mathbbm{1}\{\delta >0 \}$ for $|\delta| <1$ and $\limsup_{\delta\downarrow0}\mathcal{C}_{\x}(1,\delta)/\sqrt{\delta}=0$ holds. The remaining conditions follow from verifying \ref{Appendix Assumption E} \ref{Appendix Assumption E: covariance kernel} below.
\paragraph{\ref{Appendix Assumption E} \ref{Appendix Assumption E: gamma hat}}
In this example, $\gamma_0(x;\mathbf{Z})=\mathbbm{1}\{X\leq x\}$ is known, so it suffices to verify \eqref{Appendix Eq: simpler D gamma hat}. The uniform covering number of $\{\mathbbm{1}\{\cdot\leq x\} :x\in\mathbb{R}\}$ grows linearly, and an envelope function can be taken to be $1$. For an envelope function of $\{\mathbbm{1}\{\cdot\leq x\}-\mathbbm{1}\{\cdot\leq \x\} : |x-\x|\leq \delta \}$, we can take $\mathbbm{1}\{-\delta +\x\leq \cdot\leq \x+ \delta \}$ and the moment bound is satisfied as $\mathbb{E}[\mathbbm{1}\{-\delta +\x\leq X\leq \x+ \delta \}]\leq C \delta$.

\paragraph{\ref{Appendix Assumption E} \ref{Appendix Assumption E: covariance kernel}}
Here $\psi_{\x}(v;\mathbf{Z})=\mathbbm{1}\{X\leq \x+v\}-\mathbbm{1}\{X\leq\x\} -f_0(\x)v$. Then,
\begin{equation*}
    \frac{\mathbb{E}[|\psi_{\x}(v;\mathbf{Z}) - \psi_{\x}(v';\mathbf{Z})|]}{|v-v'|}\leq \frac{\mathbb{E}[\mathbbm{1}\{\x+\min\{v,v'\}<X\leq \x+\max\{v,v'\}\}]}{|v-v'|} +f_0(\x) \leq C.
\end{equation*}
Also, $\psi_{\x_n}(s\delta_n;\mathbf{Z})=\mathbbm{1}\{\min\{\x_n,\x_n+s\delta_n\}< X\leq \max\{\x_n,\x_n+s\delta_n\} \} - f_0(\x) s\delta_n$ and
\begin{align*}
    \psi_{\x_n}(s\delta_n;\mathbf{Z})\psi_{\x_n}(t\delta_n;\mathbf{Z})&= \mathbbm{1}\{ \x_n < X\leq \x_n + \delta_n\min\{s,t\} \}\mathbbm{1}\{s>0,t>0\}\\
    &\quad + \mathbbm{1}\{ \x_n + \max\{s,t\}< X\leq \x_n \}\mathbbm{1}\{s<0,t<0\} \\
    &\quad -\mathbbm{1}\{\min\{\x_n,\x_n+s\delta_n\}< X\leq \max\{\x_n,\x_n+s\delta_n\} \} f_0(\x) t\delta_n\\
    &\quad -\mathbbm{1}\{\min\{\x_n,\x_n+t\delta_n\}< X\leq \max\{\x_n,\x_n+t\delta_n\} \} f_0(\x) s\delta_n\\
    &\quad +f_0(\x)^2 st \delta_n^2.
\end{align*}
Then, for any $s,t\in\mathbb{R}$ and $\x_n\to\x$, using continuity of $f_0$ at $\x$,
\begin{align*}
    \delta_n^{-1}\mathbb{E}[\psi_{\x_n}(s\delta_n;\mathbf{Z})\psi_{\x_n}(t\delta_n;\mathbf{Z})] &= \delta_n^{-1} \int_{\x_n}^{\x_n + \delta_n \min\{s,t\} } f_0(u)du \mathbbm{1}\{s>0,t>0\} \\
    &\quad +\delta_n^{-1} \int_{\x_n + \delta_n \max\{s,t\} }^{\x_n} f_0(u)du \mathbbm{1}\{s<0,t<0\} + o(1)\\
    &= f_0(\x) [\min\{s,t\}\mathbbm{1}\{s>0,t>0\} - \max\{s,t\}\mathbbm{1}\{s<0,t<0\}] + o(1)
\end{align*}
and by $\min\{s,t\}\mathbbm{1}\{s>0,t>0\} - \max\{s,t\}\mathbbm{1}\{s<0,t<0\}= \min\{|s|,|t|\}\mathbbm{1}\{\mathrm{sign}(s)=\mathrm{sign}(t)\}$, the desired result holds.

\subsubsection{Independent censoring}
Let
\begin{equation*}
    \gamma_0(x;\mathbf{Z}) = \Gamma_0(x) + S_0(x)\bigg[ \frac{\mathbbm{1}\{\tilde{X}\leq x\}\Delta}{S_0(\tilde{X})G_0(\tilde{X})} -\int_0^{\tilde{X}\wedge x} \frac{\Lambda_0(du)}{S_0(u)G_0(u)} \bigg].
\end{equation*}
The covariance kernel is
\begin{equation*}
    \mathcal{C}_{\x}(s,t) = \frac{f_0(\x)}{G_0(\x)} \min\{|s|,|t|\}\1\{\mathrm{sign}(s)=\mathrm{sign}(t)\}. 
\end{equation*}

\paragraph{\ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Covariance Kernel}}
 $\mathcal{C}_{\x}(1,1)>0$ follows from $f_0(\x)>0$. $\lim_{\delta\downarrow0}\mathcal{C}_{\x}(1,\delta)/\sqrt{\delta}=0$ follows from the same computation as in the no censoring case. The remaining conditions follow from verifying \ref{Appendix Assumption E} \ref{Appendix Assumption E: covariance kernel} below.

 
\paragraph{\ref{Appendix Assumption E} \ref{Appendix Assumption E: gamma hat}}
We have $\widehat{\Gamma}_n=1-\widehat{S}_n$ with $\widehat{S}_n$ the Kaplan-Meier estimator.
By Theorem 1 of \cite{Lo-Singh_1986_PTRF},
\begin{equation*}
    \sup_{x\in I}\bigg|\widehat{\Gamma}_n(x) -\frac{1}{n}\sum_{i=1}^n \gamma_0(x;\mathbf{Z}) \bigg| = O_{\mathbb{P}}\Big( \Big|\frac{\log n}{n}\Big|^{3/4}\Big).
\end{equation*}
Since $\sqrt{n a_n} = n^{\frac{\q+1}{2\q+1}}\leq n^{2/3}$ for $\q\geq 1$, $\sup_{x}|\widehat{\Gamma}_n(x)-\Gamma_0(x)|=o_{\mathbb{P}}(1)$ and $\sqrt{na}\sup_{|v|\leq K}|\widehat{\Gamma}_n(\x+va_n^{-1})-\widehat{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+va_n^{-1})+\bar{\Gamma}_n(\x)|=o_{\mathbb{P}}(1)$ hold.

We have
\begin{align*}
    \widehat{\gamma}_n(x;\mathbf{Z}) - \gamma_0(x;\mathbf{Z}) &= \widehat{F}_n(x) -F_0(x) + \big[\widehat{S}_n(x)-S_0(x)\big]\bigg[ \frac{\mathbbm{1}\{\tilde{X}\leq x\}\Delta}{\widehat{S}_n(\tilde{X})\widehat{G}_n(\tilde{X})} -\int_0^{\tilde{X}\wedge x} \frac{\widehat{\Lambda}_n(du)}{\widehat{S}_n(u)\widehat{G}_n(u)} \bigg] \\
    &\quad + S_0(x)\mathbbm{1}\{\tilde{X}\leq x\}\Delta\frac{S_0(\tilde{X})G_0(\tilde{X})-\widehat{S}_n(\tilde{X})\widehat{G}_n(\tilde{X})}{S_0(\tilde{X})G_0(\tilde{X})\widehat{S}_n(\tilde{X})\widehat{G}_n(\tilde{X})}\\
    &\quad - S_0(x) \int_0^{\tilde{X}\wedge x} \frac{S_0(u)G_0(u)-\widehat{S}_n(u)\widehat{G}_n(u)}{S_0(u)G_0(u)\widehat{S}_n(u)\widehat{G}_n(u)}\widehat{\Lambda}_n(du) \\
    &\quad - S_0(x)\int_0^{\tilde{X}\wedge x} \frac{[\widehat{\Lambda}_n-\Lambda_0](du)}{S_0(u)G_0(u)}.
\end{align*}
Using $S_0(u_0)G_0(u_0)>0$, $\sqrt{n}\sup_{x\in I}|\widehat{S}_n(x)-S_0(x)|=O_{\mathbb{P}}(1)$, $\sqrt{n}\sup_{x\in I}|\widehat{G}_n(x)-G_0(x)|=O_{\mathbb{P}}(1)$, and $\sqrt{n}\sup_{x\in I}|\widehat{\Lambda}_n(x)-\Lambda_0(x)|=O_{\mathbb{P}}(1)$, we have $\sqrt{n}\max_{1\leq i\leq n}\sup_{x\in I}|\widehat{\gamma}_n(x;\mathbf{Z}_i)-\gamma_0(x;\mathbf{Z}_i)|=o_{\mathbb{P}}(1)$, which in turn implies
\begin{equation*}
    a_n \frac{1}{n}\sum_{i=1}^n \sup_{|v|\leq K}\big|\widehat{\gamma}_n(\x+va_n^{-1};\mathbf{Z}_i)-\widehat{\gamma}_n(\x;\mathbf{Z}_i)-\gamma_0(\x+va_n^{-1};\mathbf{Z}_i)+\gamma_0(\x;\mathbf{Z}_i)\big|^2=o_{\mathbb{P}}(1).
\end{equation*}

For the function class $\mathfrak{F}_{\gamma}$, we can take $\bar{F}_{\gamma}(\mathbf{Z}) = 1 + [S_0(u_0)G_0(u_0)]^{-1}[1 + \Lambda_0(u_0)]$ as a constant envelope. 
For the function class $\{S_0(x):x\in I\}$, given $m\in\mathbbm{N}$, there exists $\{x_1,\dots, x_{m+1}\}\subset I$ such that $\sup_{x\in I}\min_{l=1,\dots,m+1}|S_0(x_l)-S_0(x)|\leq 1/m$, which implies the uniform covering number is bounded by a linear function. The covering numbers of $\{\mathbbm{1}\{\cdot\leq s\}:s\in I\}$ and $\{\int_0^{\cdot\wedge s}[S_0(u)G_0(u)]^{-1}\Lambda_0(du):s\in I\}$ are also bounded by a linear function. By Lemma 5.1 of \cite{vanderVaart-vanderLaan_2006_IJB}, $\limsup_{\varepsilon\downarrow 0}\log N_U(\varepsilon,\mathfrak{F}_{\gamma})\varepsilon^V < \infty$ holds for $V\in (0,2)$.

Now consider the uniform covering number of $\hat{\mathfrak{F}}_{\gamma}$. Given a realization of $(\widehat{S}_n,\widehat{G}_n)$, the mapping $x\mapsto\int_0^{x\wedge s}[\widehat{S}_n(u)\widehat{G}_n(u)]^{-1}\widehat{\Lambda}_n(du)$ is a composition of $x\mapsto x\wedge s$ and $x\mapsto \int_0^x [\widehat{S}_n(u)\widehat{G}_n(u)]^{-1}\widehat{\Lambda}_n(du)$. The latter mapping is monotone, and the first mapping is a VC-subgraph class, and Lemma 2.6.18 of \cite{vanderVaart-Wellner_1996_Book} implies $\{\int_0^{\cdot\wedge s}[\widehat{S}_n(u)\widehat{G}_n(u)]^{-1}\widehat{\Lambda}_n(du):s\in I\}$ is a VC-subgraph class. Note that since $S_0,G_0$ are bounded away from zero, $\widehat{S}_n,\widehat{G}_n$ are bounded away from zero with probability approaching one. Thus, $\limsup_{\varepsilon\downarrow 0}\log N_U(\varepsilon,\hat{\mathfrak{F}}_{\gamma})\varepsilon^V = O_{\mathbb{P}}(1)$ holds.

For $s\leq t\in I$,
\begin{equation*}
    |\gamma_0(s;\mathbf{Z})-\gamma_0(t;\mathbf{Z})|\leq C|F_0(s)-F_0(t)| + C|\mathbbm{1}\{\tilde{X}\leq s\}-\mathbbm{1}\{\tilde{X}\leq t\}|\Delta + \int_{\tilde{X}\wedge s}^{\tilde{X}\wedge t}\frac{\Lambda_0(du)}{S_0(du)G_0(du)}
\end{equation*}
and we can take $D_{\gamma}^{\delta}(\mathbf{Z})$ to be a constant multiple of $\sup_{|s|\leq \delta}|F_0(\x +s)-F_0(\x)| + \Delta \mathbbm{1}\{|\tilde{X}-\x|\leq \delta\} + \int_{ x-\delta}^{x+\delta}\Lambda_0(du)/S_0(u)G_0(u)$. For $\delta >0$ small enough,
\begin{equation*}
    \mathbb{E}[D_{\gamma}^{\delta}(\mathbf{Z})^2 + D_{\gamma}^{\delta}(\mathbf{Z})^4] \leq C f_0(\x+\delta) \delta.
\end{equation*}

\paragraph{\ref{Appendix Assumption E} \ref{Appendix Assumption E: covariance kernel}}
We have
\begin{equation*}
    \psi_{\x}(v;\mathbf{Z}) = S_0(\x)\frac{(\mathbbm{1}\{\tilde{X}\leq \x +v\}-\mathbbm{1}\{\tilde{X}\leq \x\})\Delta}{S_0(\tilde{X})G_0(\tilde{X})}  + O(|v|)
\end{equation*}
where $O(|v|)$ is uniformly over small enough $|v|$. Since
\begin{equation*}
    \mathbb{E}[|\mathbbm{1}\{\tilde{X}\leq \x+v\} - \mathbbm{1}\{\tilde{X}\leq \x+v'\}|\Delta]=\int_{\x + v\wedge v'}^{\x+v\vee v'} G_0(u) f_0(u)du \leq C|v-v'|,
\end{equation*}
the first display in \ref{Appendix Assumption E: covariance kernel} is satisfied. For the covariance kernel,
\begin{align*}
    &\mathbb{E}[\psi_{\x_n}(s\delta_n;\mathbf{Z})\psi_{\x_n}(t\delta_n;\mathbf{Z})] \\
    &= S_0(\x_n)^{2}\bigg( \int_{\x_n}^{\x_n + \delta_n\min\{s,t\}}\frac{f_0(u)}{S_0(u)^2G_0(u)}du  \mathbbm{1}\{s>0,t>0\}\\
    &\qquad\qquad+\int_{\x_n + \delta_n\max\{s,t\}}^{\x_n}\frac{f_0(u)}{S_0(u)^2G_0(u)}du\mathbbm{1}\{s<0,t<0\} \bigg)+O(\delta_n^2)\\
    &= \frac{S_0(\x_n)^{2}f_0(\x)}{S_0(\x)^2G_0(\x)} \min\{s,t\}\delta_n \mathbbm{1}\{s>0,t>0\} \\
    &\quad - \frac{S_0(\x_n)^{2}f_0(\x)}{S_0(\x)^2G_0(\x)} \max\{s,t\}\delta_n \mathbbm{1}\{s<0,t<0\} + o(\delta_n)
\end{align*}
where the last equality uses continuity of $(S_0,G_0,f_0)$ at $\x$ i.e.,\ $\int_{\x_n}^{\x_n +\delta_n} [\frac{f_0(u)}{S_0(u)^2G_0(u)}-\frac{f_0(\x)}{S_0(\x)^2G_0(\x)}]du =o(1)\delta_n$. Thus, \eqref{Appendix eq: uniform version of SA-5 (5)} holds.

\subsubsection{Conditionally independent case}
Let
\begin{equation*}
    \gamma_0(x;\mathbf{Z}) = F_0(x|\mathbf{A}) + S_0(x|\mathbf{A}) \bigg[ \frac{\Delta\mathbbm{1}\{\tilde{X}\leq x\}}{S_0(\tilde{X}|\mathbf{A})G_0(\tilde{X}|\mathbf{A})} - \int_0^{\tilde{X}\wedge x}\frac{\Lambda_0(du|\mathbf{A})}{S_0(u|\mathbf{A})G_0(u|\mathbf{A})}\bigg]
\end{equation*}
where $F_0(x|A)=1-S_0(x|A)$.
The covariance kernel is
\begin{equation*}
    \mathcal{C}_{\x}(s,t) = \mathbb{E}\Big[\frac{f_{X|A}(\x|A)}{G_0(\x|A)}\Big] \min\{|s|,|t|\}\1\{\mathrm{sign}(s)=\mathrm{sign}(t)\}.    
\end{equation*}


\paragraph{\ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Covariance Kernel}}
$\mathcal{C}_{\x}(1,1)>0$ follows from $\mathbb{E}[f_{X|A}(\x|A)/G_0(\x|A)]>0$. $\lim_{\delta\downarrow0}\mathcal{C}_{\x}(1,\delta)/\sqrt{\delta}=0$ follows from the same computation as in the no censoring case. The remaining conditions follow from verifying \ref{Appendix Assumption E} \ref{Appendix Assumption E: covariance kernel} below.

\paragraph{\ref{Appendix Assumption E} \ref{Appendix Assumption E: gamma hat}}
Since $\widehat{\Lambda}_n(x|\mathbf{A})=\int_0^x\frac{\widehat{F}_n(du|\mathbf{A})}{\widehat{S}_n(u|\mathbf{A})}$, 
\begin{equation*}
    |\widehat{\Lambda}_n(x|\mathbf{A}) - \Lambda_0(x|\mathbf{A})|\leq  \sup_{u\in I} |\widehat{S}_n(u|\mathbf{A})^{-1} - S_0(u|\mathbf{A})^{-1}| \widehat{F}_n(x|\mathbf{A})
\end{equation*}
and $a_n \frac{1}{n}\sum_{i=1}^n\sup_{x \in I} |\widehat{\Lambda}_n(x|\mathbf{A}_i) - \Lambda_0(x|\mathbf{A}_i)|^2=o_{\mathbb{P}}(1)$ holds. Using
\begin{align*}
    |\widehat{\gamma}_n(x;\mathbf{Z}) -\gamma_0(x;\mathbf{Z})|&\leq C \sup_{x\in I}|\widehat{S}_n(x|\mathbf{A})-S_0(x|\mathbf{A})| + C \sup_{x\in I}|\widehat{G}_n(x|\mathbf{A})-G_0(x|\mathbf{A})|\\
    &\quad +C\sup_{x\in I}|\widehat{\Lambda}_n(x|\mathbf{A})-\Lambda_0(x|\mathbf{A})|,
\end{align*}
$\frac{1}{n}\sum_{i=1}^n\sup_{x\in I}|\widehat{\gamma}_n(x;\mathbf{Z}_i)-\gamma_0(x;\mathbf{Z}_i)|^2=o_{\mathbb{P}}(1)$ and $a_n \frac{1}{n}\sum_{i=1}^n\sup_{|v|\leq K}\big|\widehat{\gamma}_n(\x+va_n^{-1};\mathbf{Z}_i) -\widehat{\gamma}_n(\x;\mathbf{Z}_i) -\gamma_0(\x+va_n^{-1};\mathbf{Z}_i) + \gamma_0(\x;\mathbf{Z}_i) \big|^2 =o_{\mathbb{P}}(1)$ hold.

For uniform covering numbers, the class $\{S(x|\cdot):x\in I\}$ is assumed to be a VC-subgraph class. For $\{\int_0^{\cdot\wedge x} \frac{\Lambda(du|\cdot)}{S(u|\cdot)G(u|\cdot)} : x\in I\}$ with $(S,G)\in \mathfrak{S}_n\times \mathfrak{G}_n$ and $\Lambda(x|\mathbf{A}) = \int_0^xS(du|\mathbf{A})/S(u|\mathbf{A})$,
\begin{equation*}
    \bigg|\int_0^{\tilde{X}\wedge x_1}\frac{\Lambda(du|\mathbf{A})}{S(u|\mathbf{A})G(u|\mathbf{A})} - \int_0^{\tilde{X}\wedge x_2}\frac{\Lambda(du|\mathbf{A})}{S(u|\mathbf{A})G(u|\mathbf{A})}\bigg| \leq \frac{|S(x_1|\mathbf{A})-S(x_2|\mathbf{A})|}{\inf_{u\in I} S(u|\mathbf{A})^2 G(u|\mathbf{A})}
\end{equation*}
and the class $\{\int_0^{\cdot\wedge x} \frac{\Lambda(du|\cdot)}{S(u|\cdot)G(u|\cdot)} : x\in I\}$ has a desired uniform coverig number bound by Lemma 5.1 of \cite{vanderVaart-vanderLaan_2006_IJB}.

For $\x + v\in I$,
\begin{align*}
    |\gamma_0(\x+v;\mathbf{Z}) -\gamma_0(\x;\mathbf{Z})|&\leq C |S_0(\x+v|\mathbf{A})-S_0(\x|\mathbf{A})| + C\Delta |\mathbbm{1}\{\tilde{X}\leq \x +v\} - \mathbbm{1}\{\tilde{X}\leq \x\}|
\end{align*}
and using $1-S_0(x|\cdot) =\int_0^x f_{X|A}(u|\cdot) du$ with $f_{X|A}$ being bounded, we can take
\begin{equation*}
    \bar{D}_{\gamma}^{\delta}(\mathbf{Z}) =  C\Delta \mathbbm{1}\{\x-\delta \leq \tilde{X}\leq \x +\delta\} + C\delta .
\end{equation*}

\paragraph{\ref{Appendix Assumption E} \ref{Appendix Assumption E: covariance kernel}}
We have
\begin{equation*}
    \psi_{\x}(v;\mathbf{Z}) = S_0(\x|\mathbf{A})\frac{(\mathbbm{1}\{\tilde{X}\leq \x +v\}-\mathbbm{1}\{\tilde{X}\leq \x\})\Delta}{S_0(\tilde{X}|\mathbf{A})G_0(\tilde{X}|\mathbf{A})}  + O(|v|)
\end{equation*}
and the first display follows as in the independent censoring case. For the covariance kernel,
\begin{align*}
    \mathbb{E}[\psi_{\x_n}(s\delta_n;\mathbf{Z})\psi_{\x_n}(t\delta_n;\mathbf{Z})] &= \mathbb{E}\bigg[S_0(\x_n|\mathbf{A})^2 \int_{\x_n}^{\x_n +\delta_n s\wedge t} \frac{f_{X|A}(u|\mathbf{A})}{S_0(u|\mathbf{A})^2G_0(u|\mathbf{A})}du \mathbbm{1}\{s>0,t>0\} \bigg]\\
    &+ \mathbb{E}\bigg[S_0(\x_n|\mathbf{A})^2 \int_{\x_n +\delta_n s\vee t}^{\x_n} \frac{f_{X|A}(u|\mathbf{A})}{S_0(u|\mathbf{A})^2G_0(u|\mathbf{A})}du \mathbbm{1}\{s<0,t<0\} \bigg] + O(\delta_n^2)
\end{align*}
and $\delta_n^{-1}\mathbb{E}[\psi_{\x_n}(s\delta_n;\mathbf{Z})\psi_{\x_n}(t\delta_n;\mathbf{Z})]$ converges to $\mathbb{E}[\frac{f_{X|A}(\x|\mathbf{A})}{G_0(\x|\mathbf{A})}] |s|\wedge|t| \mathbbm{1}\{\mathrm{sign}(s)=\mathrm{sign}(t)\}$.

\subsection{Monotone regression function}\label{Appendix Section: Verifying example monotone regression}

\subsubsection{Classical case}
The covariance kernel is
\begin{equation*}
    \mathcal{C}_{\x}(s,t) = f_0(\x) \sigma_0^2(\x) \min\{|s|,|t|\}\1\{\mathrm{sign}(s)=\mathrm{sign}(t)\}.
\end{equation*}

\paragraph{\ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Covariance Kernel}}
$\mathcal{C}_{\x}(1,1)>0$ follows from $f_0(\x) \sigma_0^2(\x)>0$. $\lim_{\delta\downarrow0}\mathcal{C}_{\x}(1,\delta)/\sqrt{\delta}=0$ follows from the same computation as in the monotone density estimation. The remaining conditions follow from verifying \ref{Appendix Assumption E} \ref{Appendix Assumption E: covariance kernel} below.


\paragraph{\ref{Appendix Assumption E} \ref{Appendix Assumption E: gamma hat}}
In this example, $\gamma_0(x;\mathbf{Z})=Y\mathbbm{1}\{X\leq x\}$ is known, so it suffices to verify \eqref{Appendix Eq: simpler D gamma hat}. The uniform covering number bound is straightforward as $\{\mathbbm{1}\{\cdot\leq x\}:x\in\mathbb{R}\}$ is a VC-subgraph class. An envelope function is $|Y|$, whose second moment is finite. For $x\in I_{\x}^{\delta}$, $|\gamma_0(x;\mathbf{Z})-\gamma_0(\x;\mathbf{Z})|\leq |Y| \mathbbm{1}\{\x-\delta \leq X\leq \x +\delta\}$, which we can take as $\bar{D}_{\gamma}^{\delta}(\mathbf{Z})$. Then, for $j=2,4$,
\begin{equation*}
    \mathbb{E}[\bar{D}_{\gamma}^{\delta}(\mathbf{Z})^j] \leq 2^{j-1} \int_{\x-\delta}^{\x+\delta} \big(|\mu_0(x)|^j +\mathbb{E}[\varepsilon^j|X=x]\big) f_0(x)dx \leq C \delta
\end{equation*}
and the desired bound holds.

\paragraph{\ref{Appendix Assumption E} \ref{Appendix Assumption E: phi hat}}
$\widehat{\Phi}_n(x),\widehat{\Phi}_n^*(x)$ are step functions, and the sets $\widehat{\Phi}_n(I),\widehat{\Phi}_n^*(I),\widehat{\Phi}_n^{-}([0,1]),(\widehat{\Phi}_n^*)^{-}([0,1])$ are finite and thus closed.
$\phi_0(x;\mathbf{Z})=\mathbbm{1}\{X\leq x\}$ is known, so it suffices to verify the analogue of \eqref{Appendix Eq: simpler D gamma hat}. The argument is the same as for checking \ref{Appendix Assumption E} \ref{Appendix Assumption E: gamma hat} in monotone density estimation with no censoring.


\paragraph{\ref{Appendix Assumption E} \ref{Appendix Assumption E: covariance kernel}}
We have
\begin{equation*}
    \psi_{\x}(v;\mathbf{Z}) = \varepsilon(\mathbbm{1}\{X\leq \x +v\}-\mathbbm{1}\{X\leq \x\})  + (\mu_0(X)-\mu_0(\x)) (\mathbbm{1}\{X\leq \x +v\}-\mathbbm{1}\{X\leq \x\}).
\end{equation*}
Then,
\begin{equation*}
    \mathbb{E}[|\psi_{\x}(v;\mathbf{Z})-\psi_{\x}(v';\mathbf{Z})|]\leq \int_{\x+v \wedge v'}^{\x + v\vee v'} [\sigma_0(x) +|\mu_0(x)-\mu_0(\x)|] f_0(x)dx \leq C |v-v'|
\end{equation*}
and the first display holds. 
For the covariance kernel, note $|(\mu_0(X)-\mu_0(\x_n))(\mathbbm{1}\{X\leq \x_n+v\}-\mathbbm{1}\{X\leq \x_n\}|\leq |v| \sup_{|x-\x|\leq 2\delta} |\partial\mu_0(x)|$ for $|x_n-\x|\vee |v|\leq \delta$ for $\delta<0$ small enough. Then,
\begin{align*}
    &\mathbb{E}[\psi_{\x_n}(s\delta_n;\mathbf{Z})\psi_{\x_n}(t\delta_n;\mathbf{Z})]\\
    &= \mathbb{E}[\varepsilon^2(\mathbbm{1}\{X\leq \x_n +s\delta_n\}-\mathbbm{1}\{X\leq \x_n\})(\mathbbm{1}\{X\leq \x_n +t\delta_n\}-\mathbbm{1}\{X\leq \x_n\}) ] + O(\delta_n^2)\\
    &= \int_{\x_n}^{\x_n+\delta_n s\wedge t} \sigma_0^2(x)f_0(x)dx\mathbbm{1}\{s>0,t>0\} + \int^{\x_n}_{\x_n+\delta_n s\vee t} \sigma_0^2(x)f_0(x)dx\mathbbm{1}\{s<0,t<0\} + O(\delta_n^2)
\end{align*}
and
\begin{align*}
   \delta_n^{-1}\mathbb{E}[\psi_{\x_n}(s\delta_n;\mathbf{Z})\psi_{\x_n}(t\delta_n;\mathbf{Z})] &\to \sigma_0^2(\x)f_0(\x) \big( s\wedge t\mathbbm{1}\{s>0,t>0\} - s\vee t \mathbbm{1}\{s<0,t<0\} \big)\\
   &= \sigma_0^2(\x)f_0(\x) |s|\wedge |t| \mathbbm{1}\{\mathrm{sign}(s)=\mathrm{sign}(t)\}
\end{align*}
as desired.



\subsubsection{With covariates}
Let
\begin{equation*}
    \gamma_0(x;\mathbf{Z}) = \mathbbm{1}\{X\leq x\} \bigg[\frac{\varepsilon}{g_0(X,\mathbf{A})} + \theta_0(X)\bigg].
\end{equation*}
The covariance kernel is
\begin{equation*}
    \mathcal{C}_{\x}(s,t) = f_0(\x) \mathbb{E}\bigg[\frac{\sigma_0^2(\x,\mathbf{A})}{g_0(\x,\mathbf{A})}\bigg] \min\{|s|,|t|\}\1\{\mathrm{sign}(s)=\mathrm{sign}(t)\}.
\end{equation*}
\paragraph{\ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Covariance Kernel}}
$\mathcal{C}_{\x}(1,1)>0$ follows from $f_0(\x) \mathbb{E}[\frac{\sigma_0^2(\x,\mathbf{A})}{g_0(\x,\mathbf{A})}] >0$. $\lim_{\delta\downarrow0}\mathcal{C}_{\x}(1,\delta)/\sqrt{\delta}=0$ follows from the same computation as in the monotone density estimation. The remaining conditions follow from verifying \ref{Appendix Assumption E} \ref{Appendix Assumption E: covariance kernel} below.


\paragraph{\ref{Appendix Assumption E} \ref{Appendix Assumption E: gamma hat}}
\begin{align*}
    |\widehat{\gamma}_n(x;\mathbf{Z}) - \gamma_0(x;\mathbf{Z})|\leq \mathbbm{1}\{X\leq & x\} \bigg[|\varepsilon|\Big| \widehat{g}_n(X,\mathbf{A})^{-1}-g_0(X,\mathbf{A})^{-1}\Big| + \frac{|\widehat{\mu}_n(X,\mathbf{A})-\mu_0(X,\mathbf{A})|}{\widehat{g}_n(X,\mathbf{A})}\\
    & + \frac{1}{n}\sum_{j=1}^n |\widehat{\mu}_n(X,\mathbf{A}_j) - \mu_0(X,\mathbf{A}_j)|+ \bigg|\frac{1}{n}\sum_{j=1}^n \mu_0(X,\mathbf{A}_j) - \theta_0(X)\bigg| \bigg].
\end{align*}
The last sum is bounded by $\sup_{x\in I}|\frac{1}{n}\sum_{j=1}^n\mu_0(x,\mathbf{A}_j)-\theta_0(x)|$, and this object is $O_{\mathbb{P}}(n^{-1/2})$: to see this claim, first note that Assumption \ref{Appendix Section: Example monotone regression with covariates} \ref{Appendix Example: monotone regression with covariates mu Lipschitz} and Theorem 2.7.11 of \cite{vanderVaart-Wellner_1996_Book} imply $\limsup_{\epsilon\downarrow0}\log N_U(\epsilon,\{\mu(x,\cdot):x\in I\}) \epsilon^V <\infty$ for some $V\in (0,2)$ and Theorem 4.2 of \cite{Pollard_1989_SS} implies $\sup_{x\in I}|\frac{1}{n}\sum_{j=1}^n\mu_0(x,\mathbf{A}_j)-\theta_0(x)|=O_{\mathbb{P}}(n^{-1/2})$.
Then, $a_n \frac{1}{n}\sum_{i=1}^n \sup_{x\in I}|\widehat{\gamma}_n(x;\mathbf{Z})-\gamma_0(x;\mathbf{Z})|^2=o_{\mathbb{P}}(1)$ holds.

The uniform covering numbers of $\mathfrak{F}_{\gamma},\hat{\mathfrak{F}}_{\gamma,n}$ are the same order as for $\{\mathbbm{1}\{\cdot\leq x\}:x\in I\}$. For $x \in I_{\x}^{\delta}$, $|\gamma_0(x;\mathbf{Z})-\gamma_0(\x;\mathbf{Z})|\leq \mathbbm{1}\{\x-\delta\leq X\leq \x+\delta\}(|\varepsilon| c^{-1}+\theta_0(\x+\delta) )$. Then, $\limsup_{\delta\downarrow0}\mathbb{E}[\bar{D}_{\gamma}^{\delta}(\mathbf{Z})^j]\delta^{-1} <\infty$ holds for $j=2,4$.

\paragraph{\ref{Appendix Assumption E} \ref{Appendix Assumption E: phi hat}}
$\phi_0(x;\mathbf{Z})=\mathbbm{1}\{X\leq x\}$ is known and the same as in the classical case, so the same argument applies.


\paragraph{\ref{Appendix Assumption E} \ref{Appendix Assumption E: covariance kernel}}
We have
\begin{equation*}
    \psi_{\x}(v;\mathbf{Z}) = (\mathbbm{1}\{X\leq \x+v\}-\mathbbm{1}\{X\leq \x\})\bigg[\frac{\varepsilon}{g_0(X,\mathbf{A})} + \theta_0(X)-\theta_0(\x) \bigg].
\end{equation*}
Then, for $v,v'\in [-\delta,\delta]$ with sufficiently small $\delta>0$,
\begin{align*}
    |\psi_{\x}(v;\mathbf{Z}) - \psi_{\x}(v';\mathbf{Z})|&\leq |\mathbbm{1}\{X\leq \x+ v\}-\mathbbm{1}\{X\leq \x+ v'\}|\big( c^{-1} |\varepsilon| + |X-\x| \sup_{x\in I_{\x}^{\delta}}|\partial \theta_0(x)|\big) 
\end{align*}
and $\sup_{v\neq v'\in [-\delta_n,\delta_n]}\mathbb{E}[|\psi_{\x}(v;\mathbf{Z}) - \psi_{\x}(v';\mathbf{Z})|]/|v-v'|=O(1)$ holds.

For $s\delta_n$ small enough, $\psi_{\x}(s\delta_n;\mathbf{Z})= (\mathbbm{1}\{X\leq \x+ s\delta_n\}-\mathbbm{1}\{X\leq \x\})\varepsilon g_0(X,\mathbf{A})^{-1} + O(\delta_n)$ and
\begin{align*}
    &\mathbb{E}[\psi_{\x}(s\delta_n;\mathbf{Z})\psi_{\x}(t\delta_n;\mathbf{Z})] \\
    &= \mathbb{E}\bigg[\frac{\sigma_0^2(X,\mathbf{A})}{g_0(X,\mathbf{A})^2}(\mathbbm{1}\{X\leq \x+ s\delta_n\}-\mathbbm{1}\{X\leq \x\})(\mathbbm{1}\{X\leq \x+ t\delta_n\}-\mathbbm{1}\{X\leq \x\}) \bigg] + O(\delta_n^2)
\end{align*}
and
\begin{align*}
    & \mathbb{E}\bigg[\frac{\sigma_0^2(X,\mathbf{A})}{g_0(X,\mathbf{A})^2}(\mathbbm{1}\{X\leq \x+ s\delta_n\}-\mathbbm{1}\{X\leq \x\})(\mathbbm{1}\{X\leq \x+ t\delta_n\}-\mathbbm{1}\{X\leq \x\}) \bigg]\\
    &= \mathbb{E} \bigg[\int_{\x}^{\x+\delta_n s\wedge t} \frac{\sigma_0^2(x,\mathbf{A})}{g_0(x,\mathbf{A})^2} f_{X|A}(x|\mathbf{A})dx \bigg]\mathbbm{1}\{s>0,t>0\}\\
    &\quad + \mathbb{E} \bigg[\int_{\x+\delta_n s\vee t}^{\x} \frac{\sigma_0^2(x,\mathbf{A})}{g_0(x,\mathbf{A})^2} f_{X|A}(x|\mathbf{A})dx \bigg]\mathbbm{1}\{s<0,t<0\}\\
    &= \mathbb{E}\bigg[\frac{\sigma_0^2(\x,\mathbf{A})}{g_0(\x,\mathbf{A})^2}f_{X|A}(\x|\mathbf{A})\bigg]\Big(\delta_n s\wedge t \mathbbm{1}\{s>0,t>0\} - \delta_n s\vee t \mathbbm{1}\{s<0,t<0\} \Big) + o(\delta_n).
\end{align*}
Since $\frac{f_{X|A}(x|\mathbf{A})}{g_0(x,\mathbf{A})^2} = \frac{f_0(x)}{g_0(x,\mathbf{A})}$, we have 
\begin{equation*}
    \delta_n^{-1}\mathbb{E}[\psi_{\x}(s\delta_n;\mathbf{Z})\psi_{\x}(t\delta_n;\mathbf{Z})] \to f_0(\x) \mathbb{E}\bigg[\frac{\sigma_0^2(\x,\mathbf{A})}{g_0(\x,\mathbf{A})}\bigg]\Big( s\wedge t \mathbbm{1}\{s>0,t>0\} -  s\vee t \mathbbm{1}\{s<0,t<0\} \Big)
\end{equation*}
as desired.

\subsection{Monotone hazard function}
For both cases, we use the same $\widehat{\gamma}_n$ function and assumptions as in the corresponding monotone density setting. Also, the covariance kernels are the same as in the monotone density case. Thus, \ref{Appendix Assumption E} \ref{Appendix Assumption E: gamma hat} and part of \ref{Appendix Assumption B non-bootstrap} \ref{Appendix Assumption B non-bootstrap Covariance Kernel} follow from the same argument. We focus on \ref{Appendix Assumption E} \ref{Appendix Assumption E: phi hat} and \ref{Appendix Assumption E: covariance kernel}.

In the sequel, $\gamma_0$ denotes the function defined for the corresponding monotone density example. That is, for the independent right-censoring case,
\begin{equation*}
    \gamma_0(x;\mathbf{Z}) = F_0(x) + S_0(x)\bigg[\frac{\mathbbm{1}\{\tilde{X}\leq x\}\Delta}{S_0(\tilde{X})G_0(\tilde{X})}-\int_0^{\tilde{X}\wedge x}\frac{\Lambda_0(du)}{S_0(u)G_0(u)}\bigg]
\end{equation*}
and for the conditionally independent case,
\begin{equation*}
    \gamma_0(x;\mathbf{Z}) = F_0(x|\mathbf{A}) + S_0(x|\mathbf{A})\bigg[\frac{\mathbbm{1}\{\tilde{X}\leq x\}\Delta}{S_0(\tilde{X}|\mathbf{A})G_0(\tilde{X}|\mathbf{A})}-\int_0^{\tilde{X}\wedge x}\frac{\Lambda_0(du|\mathbf{A})}{S_0(u|\mathbf{A})G_0(u|\mathbf{A})}\bigg].
\end{equation*}
The same remark applies to $\widehat{\gamma}_n$.

\paragraph{\ref{Appendix Assumption E} \ref{Appendix Assumption E: phi hat}}
Since $\widehat{\Phi}_n,\widehat{\Phi}_n^*$ are integrals with non-negative integrands, they are non-decreasing and continuous. The closedness of range follows from continuity and $I$ being a compact interval. For closedness of $\widehat{\Phi}_n^{-}([0,\widehat{u}_n])$ and $(\widehat{\Phi}_n^*)^{-}([0,\widehat{u}_n])$, the integrands of $\widehat{\Phi}_n,\widehat{\Phi}_n^*$ are non-increasing and non-negative, and thus, $\widehat{\Phi}_n^{-}([0,\widehat{u}_n])$ and $(\widehat{\Phi}_n^*)^{-}([0,\widehat{u}_n])$ are closed intervals.
Note that
\begin{equation*}
    \widehat{\phi}_n(x;\mathbf{Z}) -\phi_0(x;\mathbf{Z})= -\int_0^x [\widehat{\gamma}_n(u;\mathbf{Z})-\gamma_0(u;\mathbf{Z})]du
\end{equation*}
and thus, $\frac{1}{n}\sum_{i=1}^n\sup_{x\in I}|\widehat{\phi}_n(x;\mathbf{Z})-\phi_0(x;\mathbf{Z})|^2=o_{\mathbb{P}}(1)$ and $a_n\frac{1}{n}\sum_{i=1}^n \sup_{|v|\leq K}|\widehat{\phi}_n(\x+va_n^{-1};\mathbf{Z}_i)-\widehat{\phi}_n(\x;\mathbf{Z}_i)-\phi_0(\x+va_n^{-1};\mathbf{Z}_i)+\phi_0(\x;\mathbf{Z}_i)|=o_{\mathbb{P}}(1)$ follow from the analogous conditions on $\widehat{\gamma}_n$.
To check $\sup_{x\in I}|\widehat{\Phi}_n(x)-\Phi_0(x)|=o_{\mathbb{P}}(1)$, note $\sup_{x\in I}|\frac{1}{n}\sum_{i=1}^n\phi_0(x;\mathbf{Z}_i)-\Phi_0(x)|=o_{\mathbb{P}}(1)$ follows from Glivenko-Cantelli, where $\phi_0(x;\mathbf{Z})=x - \int_0^x\gamma_0(u;\mathbf{Z}) du$ and
\begin{equation*}
    \sup_{x\in I}\bigg|\frac{1}{n}\sum_{i=1}^n \big[\widehat{\phi}_n(x;\mathbf{Z}_i)-\phi_0(x;\mathbf{Z}_i)\big]\bigg|\leq \frac{1}{n}\sum_{i=1}^n \sup_{x\in I}|\widehat{\gamma}_n(x;\mathbf{Z}_i)-\gamma_0(x;\mathbf{Z}_i)| = o_{\mathbb{P}}(1),
\end{equation*}
where the last equality follows from $\frac{1}{n}\sum_{i=1}^n\sup_{x\in I}|\widehat{\gamma}_n(x;\mathbf{Z}_i)-\gamma_0(x;\mathbf{Z}_i)|^2=o_{\mathbb{P}}(1)$. Now $\sup_{x\in I}|\widehat{\Phi}_n(x)-\Phi_0(x)|=o_{\mathbb{P}}(1)$ follows by the triangle inequality.

The conditions on the uniform covering number hold because $\gamma_0$ and $\widehat{\gamma}_n$ are bounded (for $\widehat{\gamma}_n$, with probability approaching one) and thus $|\phi_0(x_1;\mathbf{Z})-\phi_0(x_2;\mathbf{Z})|\leq C |x_1-x_2|$
and $|\widehat{\phi}_n(x_1;\mathbf{Z})-\widehat{\phi}_n(x_2;\mathbf{Z})|\leq C |x_1-x_2|$ with probability approaching one. By this Lipschitz property, the condition on $\bar{D}_{\phi}^{\delta}(\mathbf{Z})$ also holds.

\paragraph{\ref{Appendix Assumption E} \ref{Appendix Assumption E: covariance kernel}}
Let $\psi_{\x}^{\mathtt{MD}}(v;\mathbf{Z})=\gamma_0(\x +v;\mathbf{Z})-\gamma_0(\x;\mathbf{Z}) - \theta_0(\x)v$ be the $\psi_{\x}$ function for the monotone density. Then, for $x$ sufficiently close to $\x$ and $|v|$ small enough,
\begin{align*}
    \psi_{x}(v;\mathbf{z}) &\overset{def}{=}\gamma_0(x+v;\mathbf{z})-\gamma_0(x;\mathbf{z}) -\theta_0(x)[\phi_0(x+v;\mathbf{z})-\phi_0(x;\mathbf{z})]\\
    &= \psi_{x}^{\mathtt{MD}}(v;\mathbf{Z}) + \theta_0(x) \int_{x}^{x+v} \gamma_0(u;\mathbf{Z}) du = \psi_{x}^{\mathtt{MD}}(v;\mathbf{Z}) + O(|v|).
\end{align*}
Then, the same argument as in the monotone density case implies the desired result.

\subsection{Distribution function estimation with current status data}
As noted in Section \ref{Appendix Section: Example current status}, by mapping the notation $(\Delta, C)\leftrightarrow (Y,X)$, the arguments in Section \ref{Appendix Section: Verifying example monotone regression} directly apply to the generalized Grenander-type estimators considered in Section \ref{Appendix Section: Example current status}.

\section{Rule-of-Thumb Step Size Selection}
Here we develop a rule-of-thumb procedure to choose a step size for the bias-reduced numerical derivative estimator in the context of isotonic regression without covariates. Specifically, we consider the numerical derivative estimator
\begin{equation*}
    \widetilde{\mathcal{D}}_{j,n}^{\mathtt{BR}}(\x) = \epsilon_n^{-(j+1)}\sum_{k=1}^{\underline{\s}+1} \lambda_j^{\mathtt{BR}}(k)[\widehat{\Upsilon}_n(\x+c_k\epsilon_n)-\widehat{\Upsilon}_n(\x)]
\end{equation*}
with $\underline{\s}=3$, $c_1=1,c_2=-1,c_3=2,c_4=-2$. Then,
\begin{equation*}
    \lambda_1^{\mathtt{BR}}(1)=\frac{2}{3}=\lambda_1^{\mathtt{BR}}(2),\quad \lambda_1^{\mathtt{BR}}(3)=-\frac{1}{24}=\lambda_1^{\mathtt{BR}}(4),
\end{equation*}
\begin{equation*}
    \lambda_3^{\mathtt{BR}}(1)=-\frac{1}{6}=\lambda_3^{\mathtt{BR}}(2),\quad \lambda_3^{\mathtt{BR}}(3)=\frac{1}{24}=\lambda_3^{\mathtt{BR}}(4).
\end{equation*}
We use the (asymptotic) MSE-optimal step size discussed in the main paper. See also \ref{Appendix Section: bias-reduced ND higher-order terms}. Yet, with the choice of $c_k$'s, part of the bias constant $\sum_{k=1}^{\underline{\s}+1}\lambda_j^{\mathtt{BR}}(k)c_k^{\underline{s}+2}$ equals zero, and we need to turn to the next leading term of the bias, which is
\begin{equation*}
    \epsilon_n^{\underline{\s}+2-j} \frac{\partial^{\underline{\s}+3}\Upsilon_0(\x)}{(\underline{\s}+3)!} \sum_{k=1}^{\underline{\s}+1}\lambda_j^{\mathtt{BR}}(k)c_k^{\underline{s}+3}.
\end{equation*}
Then, letting $\mathsf{B}_j^{\mathtt{BR}}(\x) = \frac{\partial^{6}\Upsilon_0(\x)}{6!} \sum_{k=1}^{4}\lambda_j^{\mathtt{BR}}(k)c_k^{6}$, the MSE-optmal step size is
\begin{equation*}
    \epsilon_{j,n}^{\mathtt{BR}} = \bigg(\frac{(2j+1)\mathsf{V}_j^{\mathtt{BR}}(\x) }{2(5-j)\mathsf{B}_j^{\mathtt{BR}}(\x)^2}\bigg)^{1/11} n^{-1/11}.
\end{equation*}
The bias and variance constants depend on unknown features of the data generating process. Specifically, $\mathsf{B}_j^{\mathtt{BR}}(\x)$ depends on the regression function $\theta_0$, the Lebesgue density of $X$, and their derivatives at $X=\x$ while $\mathsf{V}_j^{\mathtt{BR}}(\x)$ is determined by the density of $X$ and the conditional variance of the regression error $\varepsilon=Y-\theta_0(X)$ at $X=\x$. To operationalize the construction of the step size, we posit a simple parametric model:
\begin{equation*}
    \mathbb{E}[Y|X] = \gamma_0 + \sum_{k=1}^5 \gamma_k (X-\x_0)^k, \quad X\sim\mathrm{Normal}(\mu,\sigma^2)
\end{equation*}
where $\{\gamma_0,\gamma_1,\gamma_2,\gamma_4,\gamma_4,\gamma_5,\mu,\sigma\}$ are parameters to be estimated. Once we estimate the parameters of this reference model, we can construct a rule-of-thumb step size $\epsilon_{j,n}^{\mathtt{ROT}}$ by replacing $\mathsf{B}_j^{\mathtt{BR}}(\x)$ and $\mathsf{V}_j^{\mathtt{BR}}(\x)$ with their estimates. Note that although the bias and variance constant estimators may not be consistent for the true $\mathsf{B}_j^{\mathtt{BR}}(\x)$ and $\mathsf{V}_j^{\mathtt{BR}}(\x)$, the rate of $\epsilon_{j,n}^{\mathtt{ROT}}$ is MSE-optimal, and the numerical derivative estimator converges to $\mathcal{D}_j(\x)$ sufficiently fast to satisfy \eqref{Appendix eq: ND convergence rate lemma}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{jasa}
\bibliography{CJN_2023_Monotone--bib}


\end{document}