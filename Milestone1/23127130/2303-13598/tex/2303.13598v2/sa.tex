%% Template for the submission to:
%%   The Annals of Statistics [AOS]
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% In this template, the places where you   %%
%% need to fill in your information are     %%
%% indicated by '???'.                      %%
%%                                          %%
%% Please do not use \input{...} to include %%
%% other tex files. Submit your LaTeX       %%
%% manuscript as one .tex document.         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[aos]{imsart}

%% Packages
\usepackage{amsfonts,amssymb,amsmath,amsthm,amscd,bbm,enumitem}
\usepackage[mathscr]{euscript} %use for \N for Normal distribution, closer to the original mathcal
\usepackage[round]{natbib}
\usepackage{color,graphicx,hyperref}

\usepackage{filecontents}

%\usepackage{caption,subcaption}

\usepackage{apptools}
\AtAppendix{\counterwithin{lemma}{section}}
\AtAppendix{\counterwithin{theorem}{section}}
\AtAppendix{\counterwithin{assumption}{section}}

\startlocaldefs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Uncomment next line to change            %%
%% the type of equation numbering           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Axiom, Claim, Corollary, Hypothesis, %%
%% Lemma, Theorem, Proposition              %%
%% use \theoremstyle{plain}                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{assumption}{Assumption}
\renewcommand{\theassumption}{SA-\arabic{assumption}}
\newtheorem{lemma}{Lemma}
\renewcommand{\thelemma}{SA-\arabic{lemma}}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\renewcommand{\thetheorem}{SA-\arabic{theorem}}
\newtheorem{corollary}{Corollary}
\renewcommand{\thecorollary}{SA-\arabic{corollary}}

\renewcommand{\thesection}{SA.\arabic{section}}\setcounter{section}{0}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}\setcounter{subsection}{0}
\renewcommand{\theequation}{SA.\arabic{equation}}\setcounter{equation}{0}

\usepackage{tocloft} % http://ctan.org/pkg/tocloft
\setlength{\cftsecnumwidth}{4em} % Set length of number width in ToC for \section
\setlength{\cftsubsecnumwidth}{4em} % Set length of number width in ToC for \subsection
\setlength{\cftsubsubsecnumwidth}{4em} % Set length of number width in ToC for \subsubsection

\setcounter{tocdepth}{2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{remark}                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%% MATH OPERATORS
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\tr}{tr}

%%%%%%%% BASIC MATH NOTATION
\newcommand{\1}{\mathbbm{1}}
\renewcommand{\P}{\mathbbm{P}}
\newcommand{\E}{\mathbbm{E}}
\newcommand{\V}{\mathbbm{V}}
\newcommand{\Cov}{\mathbbm{C}\mathrm{ov}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

%%%%%%% NOTATION
\newcommand{\G}{\mathbbm{G}}
\newcommand{\x}{\mathsf{x}}
\newcommand{\q}{\mathfrak{q}}
\newcommand{\s}{\mathfrak{s}}
\newcommand{\I}{\1}
\newcommand{\T}{\top}
\newcommand{\GCM}{\mathsf{GCM}}
\newcommand{\LSC}{\mathsf{LSC}}
\newcommand{\lsc}{\mathsf{lsc}}
\newcommand{\conv}{\mathsf{conv}}

% cross-referencing from the main paper
\usepackage{xr}
\makeatletter
\newcommand*{\addFileDependency}[1]{% argument=file name and extension
\typeout{(#1)}% latexmk will find this if $recorder=0
% however, in that case, it will ignore #1 if it is a .aux or 
% .pdf file etc and it exists! If it doesn't exist, it will appear 
% in the list of dependents regardless)
%
% Write the following if you want it to appear in \listfiles 
% --- although not really necessary and latexmk doesn't use this
%
\@addtofilelist{#1}
%
% latexmk will find this message if #1 doesn't exist (yet)
\IfFileExists{#1}{}{\typeout{No file #1.}}
}\makeatother

\newcommand*{\myexternaldocument}[2][]{% 
\externaldocument[#1]{#2}%     
\addFileDependency{#2.tex}%    
\addFileDependency{#2.aux}% 
} 

%\newcommand*{\myexternaldocument}[1]{%
%\externaldocument{#1}%
%\addFileDependency{#1.tex}%
%\addFileDependency{#1.aux}%
%}
%------------End of helper code--------------
\myexternaldocument[M-]{main}


\begin{document}

\begin{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Supplement to ``Bootstrap-Assisted Inference for Generalized Grenander-type Estimators''%\thanks{Cattaneo gratefully acknowledges financial support from the National Science Foundation through grant SES-1947805, and Jansson gratefully acknowledges financial support from the National Science Foundation through grant SES-1947662.}
}
\runtitle{Bootstrap-Assisted Inference for Monotone Estimators}
%\thankstext{T1}{}

\begin{aug}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Only one address is permitted per author. %%
%% Only division, organization and e-mail is %%
%% included in the address.                  %%
%% Additional information can be included in %%
%% the Acknowledgments section if necessary. %%
%% ORCID can be inserted by command:         %%
%% \orcid{0000-0000-0000-0000}               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author[A]{\fnms{Matias D.}~\snm{Cattaneo}\ead[label=e1]{cattaneo@princeton.edu}},
\author[B]{\fnms{Michael}~\snm{Jansson}\ead[label=e2]{mjansson@econ.berkeley.edu}}
\and
\author[C]{\fnms{Kenichi}~\snm{Nagasawa}\ead[label=e3]{kenichi.nagasawa@warwick.ac.uk}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Addresses                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\address[A]{Department of Operations Research and Financial Engineering,
Princeton University\printead[presep={,\ }]{e1}}

\address[B]{Department of Economics,
University of California at Berkeley\printead[presep={,\ }]{e2}}

\address[C]{Department of Economics,
University of Warwick\printead[presep={,\ }]{e3}}
\end{aug}


\begin{keyword}[class=MSC]
\kwd[Primary ]{62G09}
\kwd{62G20}
\kwd[; secondary ]{62G07}
\kwd{62G08}
\end{keyword}

\begin{keyword}
\kwd{Monotone estimation}
\kwd{bootstrapping}
\kwd{robust inference}
\end{keyword}

\end{frontmatter}	

%\begin{abstract}
%	This document provides all proofs of the results.
%\end{abstract}

\tableofcontents

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PROOFS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proofs}

\subsection{Proof of Lemma \ref{M-[Lemma] Generalized Switch Relation}}

Lemma 4.2 of \cite{vanderVaart-vanderLaan_2006_IJB} implies that, on $(l,u)$,
\[\GCM_{[l,u]}(\Gamma\circ\Phi^{-}) = \GCM_{[l,u]}(\LSC (\Gamma\circ\Phi^{-})).\]
Since $\Phi(\x) \in (l,u)$, we therefore have
\[\theta(\x) = \partial_{-} \GCM_{[l,u]}(\Gamma\circ\Phi^{-}) \circ \Phi(\x) = \partial_{-} \GCM_{[l,u]}(\LSC (\Gamma\circ\Phi^{-})) \circ \Phi(\x),\]
so Lemma 4.1 of \cite{vanderVaart-vanderLaan_2006_IJB} implies that
\[\theta(\x) > t \quad \iff \quad y^{\star} < \Phi(\x),\]
where
\[y^{\star} = \max\argmax_{y \in [l,u]}\left\{ty -\LSC(\Gamma\circ\Phi^{-})(y)\right\}.\]

Suppose $y^{\star} \in \Phi(I)$. Then $y^{\star} = \Phi(x^{\star})$, where
\[x^{\star} = \Phi^{-}(y^{\star}) \in \argmax_{x \in \Phi^{-}([l,u])} \left\{t\Phi(x) - \LSC_{\Phi}(\Gamma)(x)\right\}.\]
In particular,
\[\theta(\x) > t \quad \iff \quad y^{\star} = \Phi(x^{\star}) < \Phi(\x) \quad \iff \quad x^{\star} < \Phi^{-}(\Phi(\x)),\]
where, in fact,
\[x^{\star} = \max\argmax_{x \in \Phi^{-}([l,u])} \left\{t\Phi(x) -\LSC_{\Phi}(\Gamma)(x)\right\},\]
because if
\[x^{\star} < x' \in \argmax_{x \in \Phi^{-}([l,u])} \left\{t\Phi(x) -\LSC_{\Phi}(\Gamma)(x)\right\},\]
then, contradicting the definition of $y^{\star}$, we have
\[y' = \Phi(x') \in \argmax_{y\in [l,u]}\left\{ty -\LSC(\Gamma\circ\Phi^{-})(y)\right\} \qquad \text { and } \qquad y' > y^{\star}.\]
The proof can therefore be completed by showing that $y^{\star}\in \Phi(I)$.

If $\Phi(I) \supseteq [l,u]$, then there is nothing to show, so suppose $\Phi(I) \not\supseteq [l,u]$. If $y \in \Phi(I)^c \cap [l,u]$, then, since $\Phi(I) \cap [l,u]$ is closed and $l,u \in \Phi(I)$, we have $[y-\eta,y+\eta] \cap \Phi(I) = \emptyset$ for some $\eta > 0$ with $[y-\eta,y+\eta] \subset [l,u]$. Therefore, the function $\LSC (\Gamma\circ\Phi^{-})$ in constant on the interval $[y-\eta/2,y+\eta/2]$, implying in particular that
\[y^{\star}  =\max\argmax_{y' \in [l,u]}\left\{ty' -\LSC (\Gamma\circ\Phi^{-})(y')\right\} \neq y. \qed\]
	
\subsection{Proof of Lemma \ref{M-[Lemma] Continuity of argmax}}
We begin by adapting the arguments of \citet[pp. 196-198]{Kim-Pollard_1990_AoS} to show that a maximizer of $\G(v)$ over $v \in \R$ exists and is unique with probability one. Let $\widetilde{\G}(v) = \G(v) - \mu(v)$ be the centered process and suppose that, for the same $c>1/2$ as in Assumption \ref{M-[Assumption] Continuity of argmax - Mean function},
\begin{equation}\label{Appendix eq: BC lemma}
\P\left[\limsup_{|v|\to\infty}\frac{\widetilde{\G}(v)}{|v|^{c}} > \eta \right] = 0 \qquad \text{for any } \eta > 0.
\end{equation}
Then, with probability one, $\G(v) \to -\infty$ as $|v| \to \infty$, implying in turn that a maximizer of $\G(v)$ exists (because sample paths are continuous). Also, since
\[\V[\G(v) - \G(v')] = \mathcal{K}(v,v) + \mathcal{K}(v',v')-2\mathcal{K}(v,v-v') = \mathcal{K}(v-v',v-v') > 0\]
for $v \neq v'$, Lemma 2.6 of \cite{Kim-Pollard_1990_AoS} implies that this maximizer is unique with probability one. In turn, \eqref{Appendix eq: BC lemma} follows from the Borel-Cantelli lemma because
\begin{align*}
\sum_{k = 2}^{\infty} \P\left[\sup_{k - 1 \leq |v| \leq k} \frac{\widetilde{\G}(v)}{|v|^c} > \eta\right] 
&\leq \sum_{k = 2}^{\infty} \P\left[\sup_{|v| \leq k} \widetilde{\G}(v) > (k / 2)^c \eta\right]\\
&= \sum_{k = 2}^{\infty} \P\left[\sup_{|v| \leq 1} \widetilde{\G}(v) > k^{c - 1/2} 2^{-c} \eta\right]\\
&\leq \frac{2^{4c/(2c-1)} \E\left[\sup_{|v| \leq 1} |\widetilde{\G}(v)|^{4c/(2c-1)}\right]}{\eta^{4c/(2c-1)}}\sum_{k = 2}^{\infty}k^{-2} < \infty,
\end{align*}
where the equality uses the rescaling property $\mathcal{K}(v\tau,v'\tau) = \tau\mathcal{K}(v,v')$ and where the last inequality uses \citet[][Corollary 4.7]{Jain-Marcus_1978_BookCh}.

To show continuity of the function $x \mapsto 
 \P[\argmax_{v \in \R}\G(v) \leq x]$, it suffices to show that $\P[\argmax_{v \in \R}\G(v) = x] = 0$ for every $x \in \R$.
Fix $x \in \R$ and define $\tilde{Z}(x) = 0$ and
\[\tilde{Z}(v) = \frac{\G(v) - \G(x) }{\sqrt{\mathcal{K}(v-x,v-x)}}, \qquad v \neq x.\]
Then $\max_{v \in \R}\tilde{Z}(v) \geq 0$ and, for any set $\mathcal{V} \subset \R$,
\begin{equation}\label{Appendix eq: probability bound with subset}
\P\left[\argmax_{v \in \R}\G(v) = x\right] = \P\left[\max_{v \in \R}\tilde{Z}(v) \leq 0 \right] \leq \P\left[\max_{v\in\mathcal{V}}\tilde{Z}(v) \leq 0\right].
\end{equation}
In the sequel, we show that the majorant in \eqref{Appendix eq: probability bound with subset} can be made arbitrarily small by choice of $\mathcal{V}$.
In particular, for $\varepsilon \in (0,1)$ and $N \in \N$, we construct $v_{1,N}^{\varepsilon},\dots,v_{N,N}^{\varepsilon}$ such that
\begin{equation}\label{Appendix eq: mean_bound}
\E\left[\tilde{Z}(v_{i,N}^{\varepsilon})\right] \geq -\varepsilon \quad \text{ for every } 1 \leq i \leq N,
\end{equation}
and
\begin{equation}\label{Appendix eq: cov_bound}
\left|\Cov\left(\tilde{Z}\big(v_{i,N}^{\varepsilon}),\tilde{Z}(v_{j,N}^{\varepsilon})\right)\right| \leq \varepsilon \quad \text{ for every } 1 \leq i < j \leq N.
\end{equation}
Defining $\mathcal{V}_{N}^{\varepsilon} = \{v_{1,N}^{\varepsilon},\dots,v_{N,N}^{\varepsilon}\}$, we therefore have
\[\P\left[\max_{v \in \R}\tilde{Z}(v) \leq 0 \right] \leq \liminf_{\varepsilon \downarrow 0}\P\left[\max_{v \in \mathcal{V}_{N}^{\varepsilon}}\tilde{Z}(v) \leq 0 \right] \leq \left| \int_{-\infty}^{0}\frac{\exp(-x^2/2)}{\sqrt{2\pi}}dx\right|^N = 2^{-N},\]
where the second inequality uses the fact that convergence of means and covariances of normal random vectors implies convergence in distribution. Since $N$ is arbitrary, the left-hand side in the preceding display is zero. Letting $\varepsilon \in (0,1)$ and $N \in \N$ be given, the proof can therefore be completed by exhibiting $\{v_{i,N}^{\varepsilon}\}_{i=1}^N$ satisfying \eqref{Appendix eq: mean_bound} and \eqref{Appendix eq: cov_bound}.

Because $\mathcal{K}(\tau,\tau) = \tau \mathcal{K}(1,1)$ and $\lim_{\tau \downarrow 0}[\mu(x + \tau) - \mu(x)]/\sqrt{\tau} = 0$, there exists $\bar{\tau}_\varepsilon^{'} > 0$ such that
\[\E\left[\tilde{Z}(x + \tau)\right] = \frac{\mu(x + \tau) - \mu(x)}{\sqrt{\mathcal{K}(\tau,\tau)}}  =  \frac{[\mu(x + \tau ) -\mu(x)]/\sqrt{\tau}}{\sqrt{\mathcal{K}(1,1)}} \geq -\varepsilon\]
for every $\tau \in (0,\bar{\tau}_\varepsilon^{'})$. Also, because $\mathcal{K}(\tau_i,\tau_j) = \tau_i\mathcal{K}(1,\tau_j/\tau_i)$ and $\lim_{\tau \downarrow 0}\mathcal{K}(1,\tau )/\sqrt{\tau} = 0$, there exists $\bar{\tau}_\varepsilon^{''} > 0$ such that
\[\Cov\left(\tilde{Z}(x + \tau_i),\tilde{Z}(x + \tau_j)\right) = \frac{\mathcal{K}(\tau_i,\tau_j)}{\sqrt{\mathcal{K}(\tau_i,\tau_i)\mathcal{K}(\tau_j,\tau_j)}} = \frac{\mathcal{K}(1,\tau_j/\tau_i)/\sqrt{\tau_j/\tau_i} }{\mathcal{K}(1,1)} \in [-\varepsilon,\varepsilon]\]
for all $\tau_i,\tau_j > 0$ with $\tau_j/\tau_i < \bar{\tau}_\varepsilon^{''}$. Now, if $v_{i,N}^{\varepsilon} = x+\bar{\tau}_{\varepsilon}^{i}/2$ for some  $\bar{\tau}_{\varepsilon} < \min\{\bar{\tau}_\varepsilon^{'},\bar{\tau}_\varepsilon^{''},1\}$, then $\{v_{i,N}^{\varepsilon}\}_{i=1}^N$ satisfies \eqref{Appendix eq: mean_bound} and \eqref{Appendix eq: cov_bound}.\qed

\subsection{Technical Lemmas}

In preparation for the proof of Theorem \ref{M-[Theorem] Main result}, this section presents six technical lemmas. The first lemma is a switching lemma, which will be used when characterizing the limiting distributions obtained in Theorem \ref{M-[Theorem] Main result}.

\begin{lemma}\label{[Appendix Lemma] Switching lemma for R}
Let $\Gamma: \R \to \R$ be a lower semi-continuous function that is bounded from below and satisfies $\lim_{|v| \to \infty}\Gamma(v) / |v| = \infty$. Then, for any $\x,t \in \R$,
\[\partial_{-}\GCM_{\R}(\Gamma)(\x) > t \qquad \iff \qquad \max \argmax_{v \in \R}\left\{vt - \Gamma(v) \right\} < \x.\]
\end{lemma}

\begin{proof}
Because $\lim_{|v| \to \infty}\Gamma(v) / |v| = \infty$, there exists a $K > |\x|$ such that if $|v| \geq K$, then $vt - \Gamma(v) < -\Gamma(0)$, implying in particular that
\[\argmax_{v \in \R}\left\{vt - \Gamma(v)\right\} = \argmax_{v \in [-c,c]}\left\{vt - \Gamma(v)\right\}\]
for every $c \geq K$. Also, by Lemma A.1.\ of \cite{Sen-Banerjee-Woodroofe_2010_AoS} there exists a $c > K$ such that $\GCM_{\R}(\Gamma) = \GCM_{[-c,c]}(\Gamma)$ on $[-K,K]$, implying in particular that
\[\partial_{-}\GCM_{\R}(\Gamma)(\x) = \partial_{-}\GCM_{[-c,c]}(\Gamma)(\x).\]
For any such $c$, the conclusion of the lemma is equivalent to the statement
\[\partial_{-}\GCM_{[-c,c]}(\Gamma)(\x) > t \qquad \iff \qquad \max \argmax_{v \in [-c,c]}\left\{vt - \Gamma(v)\right\} < \x,\]
whose validity follows from Lemma 4.1 of \cite{vanderVaart-vanderLaan_2006_IJB}.
\end{proof}

The proof of Theorem \ref{M-[Theorem] Main result} furthermore employs various approximations to functionals of the form $\LSC_\Phi(f)$. The approximations in question are obtained using Lemmas \ref{[Appendix Lemma] LSC of Gamma_0 (global)}, \ref{[Appendix Lemma] LSC of Phi (global)}, \ref{[Appendix Lemma] LSC of f (local)}, and \ref{[Appendix Lemma] LSC of Phi (local)}. In all cases, the approximations are based on the representation
\begin{equation}\label{Appendix eq: LSC representation}
\LSC_\Phi(f)(x) = \liminf_{\epsilon \downarrow 0} \inf_{x' \in \mathcal{X}_\Phi^\epsilon(x)}f(x'),
\end{equation}
where $\mathcal{X}_\Phi^\epsilon(x) = \left(\Phi^-(\Phi(x) - \epsilon),\Phi^-(\Phi(x))\right] \cup \left(\Phi^-(\Phi(x)+),\Phi^-(\Phi(x) + \epsilon) \right)$.

The following lemma uses (\ref{Appendix eq: LSC representation}) and the special structure of $\Gamma_0$ to obtain a simple ``global'' bound on the error of the approximation $\LSC_{\Phi}(\Gamma_0) \approx \Gamma_0$.

\begin{lemma}\label{[Appendix Lemma] LSC of Gamma_0 (global)}
Suppose Assumption \ref{M-Assumption A} holds and suppose $\Phi$ is non-decreasing and right-continuous on $I$. Then, for every $x \in I$,
\[\left|\LSC_{\Phi}(\Gamma_0\big)(x) - \Gamma_0(x)\right| \leq 2 \left(\sup_{x' \in I}|\theta_0 (x')|\right) \left(\sup_{x' \in I}|\Phi(x') - \Phi_0 (x')|\right).\]
\end{lemma}

\begin{proof}
By (\ref{Appendix eq: LSC representation}) and continuity of $\Gamma_0$,
\[\LSC_{\Phi}(\Gamma_0)(x) = \min \left[\Gamma_0(\Phi^-(\Phi(x))),\Gamma_0(\Phi^-(\Phi(x)+))\right],\]
while, by Assumption \ref{M-Assumption A},
\[|\Gamma_0(x')-\Gamma_0(x)| \leq \left(\sup_{x'' \in I}|\theta_0 (x'')| \right) |\Phi_0(x') - \Phi_0(x)|.\]
Now, using $\Phi\circ\Phi^-\circ\Phi = \Phi$,
\begin{align*}
|\Phi_0(\Phi^-(\Phi(x))) - \Phi_0(x)| &= |\Phi_0(\Phi^-(\Phi(x))) - \Phi(\Phi^-(\Phi(x))) + \Phi(x) - \Phi_0(x)| \\
&\leq 2\left(\sup_{x' \in I}|\Phi(x') - \Phi_0 (x')|\right).    
\end{align*}
Also,
\[|\Phi_0(\Phi^-(\Phi(x)+)) - \Phi_0(x)| \leq 2\left(\sup_{x' \in I}|\Phi(x') - \Phi_0 (x')|\right)\]
because, for every $\eta > 0$,
\begin{align*}
0 &\leq \Phi_0(\Phi^-(\Phi(x)+)) - \Phi_0(x) \leq \Phi_0(\Phi^-(\Phi(x) + \eta)) - \Phi_0(x) \\
&\leq \Phi_0\left(\Phi^-\left(\Phi_0(x) + \sup_{x' \in I}|\Phi(x') - \Phi_0 (x')| + \eta\right)\right) - \Phi_0(x) \\
&\leq \Phi_0\left(\Phi_0^-\left(\Phi_0(x)+2\sup_{x' \in I}|\Phi(x') - \Phi_0 (x')|+\eta\right)\right) - \Phi_0(x) \\
&\leq 2 \sup_{x' \in I}|\Phi(x') - \Phi_0 (x')| + \eta,
\end{align*}
where the last inequality uses continuity of $\Phi_0$.
\end{proof}

A simple ``global'' bound on the error of the approximation $\LSC_{\Phi}(f) \approx f$ is available also in the important special case where $f$ is proportional to $\Phi$.

\begin{lemma}\label{[Appendix Lemma] LSC of Phi (global)}
Suppose Assumption \ref{M-Assumption A} holds and suppose $\Phi$ is non-decreasing and right-continuous on $I$. Then, for every $x \in I$ and every $\theta \in \R$,
\[|\LSC_{\Phi}\big(\theta \Phi\big)(x) - \theta \Phi(x)| \leq |\theta| \left(\sup_{x' \in I}|\Phi(x') - \Phi(x'-)|\right).\]
\end{lemma}

\begin{proof}
First, if $\theta > 0$, then the result follows from the fact that, by (\ref{Appendix eq: LSC representation}),
\[\LSC_{\Phi}\big(\theta \Phi\big)(x) = \theta \Phi(\Phi^-(\Phi(x))-) \leq \theta \Phi(\Phi^-(\Phi(x))) = \theta \Phi(x),\]
where
\[\Phi(x) = \Phi(\Phi^-(\Phi(x))) \leq \Phi(\Phi^-(\Phi(x))-) + \sup_{x' \in I}|\Phi(x') - \Phi(x'-)|.\]

Next, if $\theta < 0$, then the result follows from the fact that, by (\ref{Appendix eq: LSC representation}),
\[\LSC_{\Phi}\big(\theta \Phi\big)(x) = \theta \Phi(\Phi^-(\Phi(x)+)) \leq \theta \Phi(\Phi^-(\Phi(x))) = \theta \Phi(x),\]
where
\begin{align*}
\Phi(\Phi^-(\Phi(x)+)) &\leq \liminf_{\eta \downarrow 0} \Phi(\Phi^-(\Phi(x) + \eta)-) + \sup_{x' \in I}|\Phi(x') - \Phi(x'-)| \\
&\leq \Phi(x)+ \sup_{x' \in I}|\Phi(x') - \Phi(x'-)|.
\end{align*}
\end{proof}

Next, we give a ``local'' approximation to $\Phi^- \circ \Phi$. That approximation will later be used in combination with (\ref{Appendix eq: LSC representation}) to obtain ``local'' approximations to $\LSC_\Phi(f)$, but the approximation is also useful in its own right and we therefore state it as a separate lemma.
\begin{lemma}\label{[Appendix Lemma] Phi- circ Phi}
Suppose Assumption \ref{M-Assumption A} holds and suppose $\Phi$ is non-decreasing and right-continuous on $I$. Also, suppose
\[\frac{2 \sup_{x' \in I_{\x}^{\delta + \epsilon}}|\Phi(x') - \Phi_0(x')|}{\inf_{x' \in I_{\x}^{\delta + \epsilon}}\partial\Phi_0(x')} < \epsilon\]
for some $\delta,\epsilon > 0$ with $I_{\x}^{\delta + \epsilon} \subseteq I$. Then, for every $x \in I_{\x}^{\delta}$, 
\[|\Phi^-(\Phi(x)) - x| < \epsilon \qquad \text{and} \qquad |\Phi^-(\Phi(x)+) - x| < \epsilon.\]
\end{lemma}

\begin{proof}
First, suppose $|\Phi^-(\Phi(x))-x| \geq \epsilon$. Then $\Phi^-(\Phi(x)) \leq x - \epsilon$, implying in particular that $\Phi(x-\epsilon) = \Phi(x)$ and therefore also
\[\left[\Phi(x - \epsilon) - \Phi_0(x - \epsilon)\right] - \left[\Phi(x) - \Phi_0(x)\right] = \Phi_0(x) - \Phi_0(x - \epsilon).\]
Now, if $x \in I_{\x}^{\delta}$, then $[x-\epsilon,x] \subseteq I_{\x}^{\delta + \epsilon}$, so
\[\Phi_0(x) - \Phi_0(x - \epsilon) \geq \epsilon \left(\inf_{x' \in I_{\x}^{\delta + \epsilon}} \partial \Phi_0(x')\right) > 2 \left(\sup_{x' \in I_{\x}^{\delta + \epsilon}}|\Phi(x') - \Phi_0(x')|\right),\]
whereas
\[\bigg| \left[\Phi(x - \epsilon) - \Phi_0(x - \epsilon)\right] - \left[\Phi(x) - \Phi_0(x)\right] \bigg| \leq 2 \left(\sup_{x' \in I_{\x}^{\delta + \epsilon}}|\Phi(x') - \Phi_0(x')|\right).\]
In other words, $x \not \in I_{\x}^{\delta}$.

Next, suppose $|\Phi^-(\Phi(x)+) - x| \geq \epsilon$. Then, for every $\eta,\eta' > 0$, $\Phi^-(\Phi(x) + \eta) \geq x + \epsilon$ and therefore
\[\Phi(x + \epsilon - \eta')-\sup_{x' \in I_{\x}^{\delta + \epsilon}}|\Phi(x') - \Phi_0(x')| - \eta < \Phi(x)-\sup_{x' \in I_{\x}^{\delta + \epsilon}}|\Phi(x') - \Phi_0(x')|,\]
where, for $x \in I_{\x}^{\delta}$,
\[\Phi(x) - \sup_{x' \in I_{\x}^{\delta + \epsilon}}|\Phi(x') - \Phi_0(x')| \leq \Phi_0(x),\]
whereas
\begin{align*}
&\liminf_{\eta,\eta' \downarrow 0}\left[\Phi(x + \epsilon - \eta') - \sup_{x' \in I_{\x}^{\delta + \epsilon}}|\Phi(x') - \Phi_0(x')| - \eta \right]\\
&\geq \liminf_{\eta' \downarrow 0}\left[\Phi_0(x + \epsilon - \eta') - 2 \sup_{x' \in I_{\x}^{\delta + \epsilon}}|\Phi(x') - \Phi_0(x')| \right]\\
&\geq \Phi_0(x) + \left(\inf_{x' \in I_{\x}^{\delta + \epsilon}} \partial \Phi_0(x')\right) \liminf_{\eta' \downarrow 0}\left[\epsilon - \frac{2 \sup_{x' \in I_{\x}^{\delta + \epsilon}}|\Phi(x') - \Phi_0(x')|}{\inf_{x' \in I_{\x}^{\delta + \epsilon}} \partial \Phi_0 x')} - \eta'\right] > \Phi_0(x).\\
\end{align*}
In other words, $x \not \in I_{\x}^{\delta}$.
\end{proof}

Next, we obtain two ``local'' approximations to $\LSC_\Phi(f)$. The first of these is a generic approximation obtained by simply combining (\ref{Appendix eq: LSC representation}) and Lemma \ref{[Appendix Lemma] Phi- circ Phi}, but for later reference we state the result as a separate lemma.

\begin{lemma}\label{[Appendix Lemma] LSC of f (local)}
Suppose the assumptions of Lemma \ref{[Appendix Lemma] Phi- circ Phi} hold. Then, for every $x \in I_{\x}^{\delta}$ and every $f : I \to \R$,
\[\inf_{|x' - x| \leq \epsilon}f(x') \leq \LSC_{\Phi}(f)(x) \leq \sup_{|x' - x| \leq \epsilon}f(x')\]
and
\[|\LSC_{\Phi}(f)(x) - f(x)| \leq \sup_{|x' - x| \leq \epsilon} |f(x') - f(x)|.\]
\end{lemma}

The final lemma is concerned with the special case where $f$ is proportional to $\Phi$. In that case, the following ``local'' analog of Lemma \ref{[Appendix Lemma] LSC of Phi (global)} shows that the bound(s) obtained in Lemma \ref{[Appendix Lemma] LSC of f (local)} can be improved under mild conditions on $\Phi$.

\begin{lemma}\label{[Appendix Lemma] LSC of Phi (local)}
Suppose the assumptions of Lemma \ref{[Appendix Lemma] Phi- circ Phi} hold. Then, for every $x \in I_{\x}^{\delta}$ and every $\theta \in \R$,
\[|\LSC_{\Phi}\big(\theta \Phi\big)(x) - \theta \Phi(x)| \leq |\theta| \left(\sup_{x' \in I_{\x}^{\delta + \epsilon}}|\Phi(x') - \Phi(x'-)|\right).\]
\end{lemma}

\begin{proof}
First, if $\theta > 0$, then the result follows from the fact that, by (\ref{Appendix eq: LSC representation}),
\[\LSC_{\Phi}\big(\theta \Phi\big)(x) = \theta \Phi(\Phi^-(\Phi(x))-) \leq \theta \Phi(\Phi^-(\Phi(x))) = \theta \Phi(x),\]
where, if $x \in I_{\x}^{\delta}$, then $\Phi^-(\Phi(x)) \in I_{\x}^{\delta + \epsilon}$ by Lemma \ref{[Appendix Lemma] Phi- circ Phi}, and therefore
\[\Phi(x) = \Phi(\Phi^-(\Phi(x))) \leq \Phi(\Phi^-(\Phi(x))-) + \sup_{x' \in I_{\x}^{\delta + \epsilon}}|\Phi(x') - \Phi(x'-)|.\]

Next, if $\theta < 0$, then the result follows from the fact that, by (\ref{Appendix eq: LSC representation}),
\[\LSC_{\Phi}\big(\theta \Phi\big)(x) = \theta \Phi(\Phi^-(\Phi(x)+)) \leq \theta \Phi(\Phi^-(\Phi(x))) = \theta \Phi(x),\]
where, if $x \in I_{\x}^{\delta}$, then $\Phi^-(\Phi(x)+) \in I_{\x}^{\delta + \epsilon}$ by Lemma \ref{[Appendix Lemma] Phi- circ Phi}, and therefore
\begin{align*}
\Phi(\Phi^-(\Phi(x)+)) &\leq \liminf_{\eta \downarrow 0} \Phi(\Phi^-(\Phi(x) + \eta)-) + \sup_{x' \in I_{\x}^{\delta + \epsilon}}|\Phi(x') - \Phi(x'-)| \\
&\leq \Phi(x)+ \sup_{x' \in I_{\x}^{\delta + \epsilon}}|\Phi(x') - \Phi(x'-)|.
\end{align*}
\end{proof}

\subsection{Proof of Theorem \ref{M-[Theorem] Main result}}

\paragraph*{Proof of \eqref{M-[Asymptotics] thetahat}}
Let $t \in \R$ be given. By Lemma \ref{M-[Lemma] Generalized Switch Relation} and change of variables,
\begin{align*}
&\P\left[r_n(\widehat{\theta}_n(\x) - \theta_0(\x) ) > t \right] \\
&= \P\left[\max\argmax_{x \in \widehat{\Phi}_n^{-}([0,\widehat{u}_n])} \left\{ [\theta_0(\x) + t r_n^{-1}] \widehat{\Phi}_n(x) - \LSC_{\widehat{\Phi}_n}\big(\widehat{\Gamma}_n\big)(x) \right\} < \widehat{\Phi}_n^{-} \circ \widehat{\Phi}_n (\x) \right]\\
&= \P\left[\max\argmin_{v \in \widehat{V}_{\x,n}^{\q}} \widehat{H}_{\x,n}^{\q}(v;t) < \widehat{Z}_{\x,n}^{\q} \right],
\end{align*}
where
\[\widehat{V}_{\x,n}^{\q} = \left\{a_n (x - \x) : x \in \widehat{\Phi}_{n}^{-}([0,\widehat{u}_{n}])\right\},\]
\[\widehat{H}_{\x,n}^{\q}(v;t) = \LSC_{\widehat{L}_{\x,n}^{\q}}
\left(\widehat{G}_{\x,n}^{\q} + M_{\x,n}^{\q} + r_n \theta_0(\x) \widehat{L}_{\x,n}^{\q} \right)(v)  - [r_n \theta_0(\x) + t] \widehat{L}_{\x,n}^{\q}(v)\]
and
\[\widehat{Z}_{\x,n}^{\q} = a_n \left[\widehat{\Phi}_{n}^{-} \circ \widehat{\Phi}_{n}(\x) - \x \right],\]
with
\begin{align*}
\widehat{G}_{\x,n}^{\q}(v) &= \sqrt{n a_n} \left[\widehat{\Gamma}_n(\x + v a_n^{-1}) - \widehat{\Gamma}_n(\x) - \Gamma_0(\x + v a_n^{-1}) + \Gamma_0(\x) \right] \\
&\qquad - \theta_0(\x) \sqrt{n a_n} \left[\widehat{\Phi}_n(\x + v a_n^{-1})-\widehat{\Phi}_n(\x) - \Phi_0(\x + v a_n^{-1}) + \Phi_0(\x)\right],\\
M_{\x,n}^{\q}(v) &= \sqrt{n a_n} \left[\Gamma_{0}(\x + v a_n^{-1}) - \Gamma_{0}(\x)\right] - \theta_{0}(\x) \sqrt{n a_n} \left[\Phi_{0}(\x + v a_n^{-1})-\Phi_{0}(\x)\right],\\
\widehat{L}_{\x,n}^{\q}(v) &= a_n \left[\widehat{\Phi}_{n}(\x + v a_n^{-1}) - \widehat{\Phi}_{n}(\x)\right].
\end{align*}
By \ref{M-Assumption B: uniform convergence Phi} and Lemma \ref{[Appendix Lemma] Phi- circ Phi}, $\widehat{Z}_{\x,n}^{\q} = o_{\P}(1)$. Suppose also that
\begin{equation}\label{Appendix eq: argmax convergence}
\max\argmin_{v \in \widehat{V}_{\x,n}^{\q}} \widehat{H}_{\x,n}^{\q}(v;t) \leadsto \argmin_{v \in \R} \mathcal{H}_{\x}^{\q} (v;t),
\end{equation}
where $\mathcal{H}_{\x}^{\q}(v;t) = \mathcal{G}_{\x}(v) + \mathcal{M}_{\x}^{\q}(v)  - t \partial \Phi_0(\x) v$. Then
\begin{align*}
\P\left[r_n (\widehat{\theta}_n(\x) - \theta_0(\x)) > t \right] &= \P\left[\max\argmin_{v \in \widehat{V}_{\x,n}^{\q}} \widehat{H}_{\x,n}^{\q}(v;t) < \widehat{Z}_{\x,n}^{\q} \right]\\
&\to  \P\left[\argmin_{v \in \R} \mathcal{H}_{\x}^{\q}(v;t) < 0\right]\\
&= \P\left[\frac{1}{\partial\Phi_0(\x)}\partial_{-}\GCM_{\R}(\mathcal{G}_{\x} + \mathcal{M}_{\x}^{\q}(0) > t \right],
\end{align*}
where the second line uses Lemma \ref{M-[Lemma] Continuity of argmax} and where the last equality uses Lemma \ref{[Appendix Lemma] Switching lemma for R}. The proof of \eqref{M-[Asymptotics] thetahat} can therefore be completed by showing \eqref{Appendix eq: argmax convergence}.

We shall do so by means of the argmax continuous mapping theorem of \citet{Cox_2022}. To be specific, using that theorem it can be shown that \eqref{Appendix eq: argmax convergence} holds if
\begin{equation}\label{Appendix eq: vhat tight}
\widehat{v}_n(t) = \max\argmin_{v \in \widehat{V}_{\x,n}^{\q}} \widehat{H}_{\x,n}^{\q}(v;t) = O_{\P}(1)
\end{equation}
and if
\begin{equation}\label{Appendix eq: Hhat convergence}
\widehat{H}_{\x,n}^{\q}(\cdot;t) \leadsto \mathcal{H}_{\x}^{\q} (\cdot;t).
\end{equation}

We begin by showing \eqref{Appendix eq: Hhat convergence}. First, by \ref{M-Assumption B: weak convergence}, $\widehat{G}_{\x,n}^{\q} \leadsto \mathcal{G}_{\x}$. Also, by \ref{M-Assumption A: theta0} and \ref{M-Assumption A: Phi0}, as $u \to 0$,
\begin{align*}
\frac{\Gamma_0(\x+u) - \theta_0(\x) \Phi_0(\x + u) -\Gamma_0(\x) + \theta_0(\x) \Phi_0(\x)}{u^{\q+1}} &\to \lim_{u \to 0}\frac{\left[\theta_0(\x + u)-\theta_0(\x)\right] \partial \Phi_0(\x + u)}{(\q+1) u^{\q}} \\
&= \frac{\partial^{\q} \theta_0(\x) \partial \Phi_0(\x)}{(\q+1)!},
\end{align*}
where the first equality uses L'H\^{o}pital's rule and
\[\partial \left[\Gamma_0(\x + u)-\theta_0(\x) \Phi_0(\x + u) -\Gamma_0(\x) + \theta_0(\x) \Phi_0(\x)\right] = \left[\theta_0(\x + u)-\theta_0(\x)\right] \partial \Phi_0(\x + u).\]
As a consequence, $M_{\x,n}^{\q} \leadsto \mathcal{M}_{\x}^{\q}$. Moreover, $\widehat{L}_{\x,n}^{\q} - L_{\x,n}^{\q} \leadsto 0$ by \ref{M-Assumption B: uniform convergence Phi} and $L_{\x,n}^{\q} \leadsto \mathcal{L}_{\x}$ by \ref{M-Assumption A: Phi0}, where  
$L_{\x,n}^{\q}(v) = a_n \left[\Phi_0(\x + v a_n^{-1}) - \Phi_0(\x)\right]$. 
In particular, $\widehat{L}_{\x,n}^{\q} \leadsto \mathcal{L}_{\x}$ and therefore
\[(\widehat{G}_{\x,n}^{\q},M_{\x,n}^{\q},\widehat{L}_{\x,n}^{\q}) \leadsto (\mathcal{G}_{\x},\mathcal{M}_{\x}^{\q},\mathcal{L}_{\x}).\]

Because $\widehat{G}_{\x,n}^{\q}+M_{\x,n}^{\q}$ is asymptotically equicontinuous,
\[\LSC_{\widehat{L}_{\x,n}^{\q}}\left(\widehat{G}_{\x,n}^{\q} + M_{\x,n}^{\q}\right) - \left(\widehat{G}_{\x,n}^{\q} + M_{\x,n}^{\q}\right) \leadsto 0\]
by \ref{M-Assumption B: uniform convergence Phi} and Lemma \ref{[Appendix Lemma] LSC of f (local)}. Also, by \ref{M-Assumption B: jump of Phi hat} and Lemma \ref{[Appendix Lemma] LSC of Phi (local)},
\[\LSC_{\widehat{L}_{\x,n}^{\q}}\left(r_n \theta_0(\x)  \widehat{L}_{\x,n}^{\q}\right) - \left(r_n \theta_0(\x) \widehat{L}_{\x,n}^{\q}\right) \leadsto 0.\]
The result \eqref{Appendix eq: Hhat convergence} follows from the three preceding displays and the fact that, on $\widehat{V}_{\x,n}^{\q}$,
\begin{align*}
    0 &\geq \LSC_{\widehat{L}_{\x,n}^{\q}}\left(\widehat{G}_{\x,n}^{\q} + M_{\x,n}^{\q} + r_n \theta_0(\x) \widehat{L}_{\x,n}^{\q}\right) - \left(\widehat{G}_{\x,n}^{\q} + M_{\x,n}^{\q} + r_n \theta_0(\x) \widehat{L}_{\x,n}^{\q}\right) \\
    &\geq \LSC_{\widehat{L}_{\x,n}^{\q}}\left(\widehat{G}_{\x,n}^{\q} + M_{\x,n}^{\q}\right) - \left(\widehat{G}_{\x,n}^{\q} + M_{\x,n}^{\q}\right) + \LSC_{\widehat{L}_{\x,n}^{\q}}\left(r_n \theta_0(\x) \widehat{L}_{\x,n}^{\q}\right) - \left(r_n \theta_0(\x) \widehat{L}_{\x,n}^{\q}\right).    
\end{align*}

Next, to show \eqref{Appendix eq: vhat tight}, we first define $\theta_n(\x;t) = \theta_0(\x) + t r_n^{-1}$ and note that
\[\widehat{v}_n(t) = a_n \left[\max\argmax_{x \in \widehat{\Phi}_n^{-}([0,\widehat{u}_n])} \left\{\theta_n(\x;t) \widehat{\Phi}_n(x) - \LSC_{\widehat{\Phi}_n}\left(\widehat{\Gamma}_n\right)(x)\right\} - \x\right].\]
Now, if $|\widehat{v}_n(t)| > a_n \delta > 0,$ then
\[\sup_{x \not \in I_{\x}^{\delta}} \left\{\theta_n(\x;t) \widehat{\Phi}_n(x) - \LSC_{\widehat{\Phi}_n} \left(\widehat{\Gamma}_n\right)(x) \right\} \geq \theta_n(\x;t) \widehat{\Phi}_n(\x) - \LSC_{\widehat{\Phi}_n}\left(\widehat{\Gamma}_n\right)(\x),\]
where $|\theta_n(\x;t) - \theta_0(\x)| = O(r_n^{-1}) = o(1)$, and, by \ref{M-Assumption B: uniform convergence Phi},
\[\sup_{x \in I}\left|\widehat{\Phi}_n(x) - \Phi_0(x)\right| = o_{\P}(1).\]
Also, using \ref{M-Assumption B: uniform consistency Gamma}, \ref{M-Assumption B: uniform convergence Phi}, and Lemma \ref{[Appendix Lemma] LSC of Gamma_0 (global)},
\begin{align*}
&\sup_{x \in I}\left|\LSC_{\widehat{\Phi}_n}\left(\widehat{\Gamma}_n\right)(x) - \Gamma_0(x)\right| \\
&\leq \sup_{x \in I} \left|\LSC_{\widehat{\Phi}_n}\left(\widehat{\Gamma}_n\right)(x) - \LSC_{\widehat{\Phi}_n}\left(\Gamma_0\right)(x)\right| + \sup_{x \in I}\left|\LSC_{\widehat{\Phi}_n}\left(\Gamma_0\right)(x) - \Gamma_0(x)\right| \\
&\leq \sup_{x \in I} \left|\widehat{\Gamma}_n(x) - \Gamma_0(x)\right| + 2 \left(\sup_{x \in I}|\theta_0 (x)|\right) \left(\sup_{x \in I}|\widehat{\Phi}_n(x) - \Phi_0(x)|\right) = o_{\P}(1).
\end{align*}
As a consequence, $\widehat{v}_n(t) = o_\P(a_n)$: For any $\delta > 0$,
\begin{align*}
&\P\left[|\widehat{v}_n(t)| > a_n \delta \right] \\
&\leq \P\left[\sup_{x \not \in I_{\x}^{\delta}} \left\{\theta_0(\x) \Phi_0(x) - \Gamma_0(x) \right\} \geq \theta_0(\x) \Phi_0(\x) - \Gamma_0(\x) + o_{\P}(1)\right] = o(1),
\end{align*}
where the equality uses the fact, noted by \citet[Supplement, proof of Lemma 3]{Westling-Carone_2020_AoS}, that the function  $v \mapsto \theta_0(\x) \Phi_0(v) - \Gamma_0(v)$ is unimodal and maximized at $v = \x$.

Next, defining $\widehat{V}_{\x,n}^{\q}(j) = \{v\in \widehat{V}_{\x,n}^{\q} : 2^j < |v| \leq 2^{j+1}\}$ and using $\widehat{v}_n(t) = o_{\P}(a_n)$, we have, for any $K$, any positive $\delta'$, and any sequence of events $\{\mathcal{A}_n'\}$ with $\lim_{n \to \infty}\P[\mathcal{A}_n'] = 1$,
\begin{align*}
\limsup_{n \to \infty}\P\left[|\widehat{v}_n(t)| > 2^K\right] &\leq \limsup_{n \to \infty}\sum_{j \geq K : 2^j \leq a_n \delta'} \P\left[2^j < |\widehat{v}_n(t)| \leq 2^{j+1} \cap \mathcal{A}_n'\right] \\
&\leq \limsup_{n \to \infty} \sum_{j \geq K : 2^j \leq a_n \delta'} \P\left[\inf_{v \in \widehat{V}_{\x,n}^{\q}(j)} \widehat{H}_{\x,n}^{\q}(v;t) \leq \widehat{H}_{\x,n}^{\q}(0;t) \cap \mathcal{A}_n' \right].
\end{align*}
The proof of \eqref{Appendix eq: vhat tight} can therefore be completed by showing that the majorant side in the display can be made arbitrarily small by choice of $K$, $\delta'$, and $\{\mathcal{A}_n'\}$.

To do so, we begin by analyzing each term in the basic bound 
\begin{align*}
\widehat{H}_{\x,n}^{\q}(v;t) - \widehat{H}_{\x,n}^{\q}(0;t) &\geq \LSC_{\widehat{L}_{\x,n}^{\q}}\left(\widehat{G}_{\x,n}^{\q}\right)(v) + \LSC_{\widehat{L}_{\x,n}^{\q}}\left(M_{\x,n}^{\q}\right)(v) - t \widehat{L}_{\x,n}^{\q}(v) \\
&+ \LSC_{\widehat{L}_{\x,n}^{\q}}\left(r_n \theta_0(\x) \widehat{L}_{\x,n}^{\q}\right)(v) - r_n \theta_0(\x) \widehat{L}_{\x,n}^{\q} (v) - \widehat{H}_{\x,n}^{\q}(0;t).
\end{align*}
Because $\widehat{H}_{\x,n}^{\q}(0;t) \leadsto \mathcal{H}_{\x}^{\q}(0;t) = 0$ and because, by \ref{M-Assumption B: jump of Phi hat} and Lemma \ref{[Appendix Lemma] LSC of Phi (local)}, there is a positive $\delta'$ such that
\[\sup_{|v|\leq a_n \delta'}\left|\LSC_{\widehat{L}_{\x,n}^{\q}}\left(r_n\theta_0(\x) \widehat{L}_{\x,n}^{\q}\right)(v) - r_n\theta_0(\x) \widehat{L}_{\x,n}^{\q} (v)\right| = o_{\P}(1),\]
we may assume that, on $\{\mathcal{A}_n'\}$ and for some $C_0$,
\[\sup_{|v| \leq a_n \delta'}\left|\LSC_{\widehat{L}_{\x,n}^{\q}}\left(r_n \theta_0(\x) \widehat{L}_{\x,n}^{\q}\right)(v) - r_n \theta_0(\x) \widehat{L}_{\x,n}^{\q}(v) - \widehat{H}_{\x,n}^{\q}(0;t)\right| \leq C_0.\]
Also, because, by \ref{M-Assumption B: uniform convergence Phi} and \ref{M-Assumption A: Phi0}, there is a positive $\delta'$ such that
\[\sup_{|v| \leq a_n \delta'}\left|\widehat{L}_{\x,n}^{\q}(v) - L_{\x,n}^{\q} (v)\right| = o_{\P}(1) \qquad \text{and} \qquad \sup_{1 \leq |v| \leq a_n \delta'}\left|\frac{L_{\x,n}^{\q}(v)}{\mathcal{L}_{\x} (v)}\right| < \infty,\]
we may assume that, on $\{\mathcal{A}_n'\}$ and for some $C_L$,
\[\sup_{1 \leq |v| \leq a_n \delta'}\left|\frac{\widehat{L}_{\x,n}^{\q}(v)}{v}\right| \leq C_L. \]
Next, by \ref{M-Assumption B: uniform convergence Phi} and Lemma \ref{[Appendix Lemma] LSC of f (local)}, with probability approaching one,
\[\LSC_{\widehat{L}_{\x,n}^{\q}}\left(M_{\x,n}^{\q}\right)(v) \geq \inf_{|v|/2 \leq |v'| \leq 2|v|} M_{\x,n}^{\q}(v') \qquad \text{for every } |v| \geq 2,\]
while, by \ref{M-Assumption A: theta0} and \ref{M-Assumption A: Phi0}, there is a positive $\delta'$ such that
\[\inf_{1 \leq |v| \leq \eta a_n \delta'}\frac{M_{\x,n}^{\q}(v)}{\mathcal{M}_{\x}^{\q} (v)} > 0.\]
We may therefore assume that, on $\{\mathcal{A}_n'\}$ and for some positive $C_M$,
\[\inf_{2 \leq |v| \leq a_n \delta'}\frac{\LSC_{\widehat{L}_{\x,n}^{\q}}\left(M_{\x,n}^{\q}\right)(v)}{v^{\q+1}} \geq C_M. \]
Finally, by \ref{M-Assumption B: uniform convergence Phi} and Lemma \ref{[Appendix Lemma] LSC of f (local)}, with probability approaching one,
\[\left|\LSC_{\widehat{L}_{\x,n}^{\q}}\left(\widehat{G}_{\x,n}^{\q}\right)(v)\right| \leq \sup_{|v| / 2 \leq |v'| \leq 2 |v|} \left|\widehat{G}_{\x,n}^{\q}(v')\right| \qquad \text{for every } |v| \geq 2,\]
and we may therefore assume that, on $\{\mathcal{A}_n'\}$,
\[\sup_{v \in \widehat{V}_{\x,n}^{\q}(j)}\left|\LSC_{\widehat{L}_{\x,n}^{\q}}\left(\widehat{G}_{\x,n}^{\q}\right)(v)\right| \leq \sup_{v \in V_2(j)} \left|\widehat{G}_{\x,n}^{\q}(v)\right| \qquad \text{for every } j \geq 2 \text{ with } 2^j \leq a_n \delta',\]
where $V_\eta(j) = \{v \in \R: \eta^{-1}2^j \leq |v| \leq \eta 2^{j+1}\}$.

As a consequence, by the Markov inequality,
\begin{align*}
&\P\left[\inf_{v \in \widehat{V}_{\x,n}^{\q}(j)} \widehat{H}_{\x,n}^{\q}(v;t) \leq \widehat{H}_{\x,n}^{\q}(0;t) \cap \mathcal{A}_n' \right] \\
&\leq \P\left[\sup_{v \in V_2(j)} \left|\widehat{G}_{\x,n}^{\q}(v)\right| \geq \inf_{v \in V_1(j)} \left[C_M v^{\q+1} - C_L |v| - C_0\right] \cap \mathcal{A}_n' \right] \\
&\leq \frac{\E\left[\sup_{v \in V_2(j)} \left|\widehat{G}_{\x,n}^{\q}(v)\right| \I_{\mathcal{A}_n'} \right]}{\inf_{v \in V_1(j)} \left[C_M v^{\q+1} - C_L |v| - C_0\right]} \qquad \text{for every } j \geq 2 \text{ with } 2^j \leq a_n \delta',
\end{align*}
where, by \ref{M-Assumption B: uniform convergence Phi}, we may assume that, for some $C_G$,
\[\E\left[\sup_{v \in V_2(j)} \left| \widehat{G}_{\x,n}^{\q}(v)\right| \I_{\mathcal{A}_n'} \right] \leq \sum_{j-1 \leq j' \leq j+1} \E\left[\sup_{v \in V_1(j)} \left|\widehat{G}_{\x,n}^{\q}(v)\right| \I_{\mathcal{A}_n'} \right] \leq C_G 2^{j \beta},\]
and where, for all sufficiently large $j$,
\[\inf_{v \in V_1(j)} \left[C_M v^{\q+1} - C_L |v| - C_0\right] \geq \frac{1}{2} C_M 2^{j(\q+1)}.\]
In other words, for large $K$,
\[\limsup_{n \to \infty} \sum_{j \geq K : 2^j \leq a_n \delta'} \P\left[\inf_{v \in \widehat{V}_{\x,n}^{\q}(j)} \widehat{H}_{\x,n}^{\q}(v;t) \leq \widehat{H}_{\x,n}^{\q}(0;t) \cap \mathcal{A}_n' \right] \leq \frac{2 C_G}{C_M}\sum_{j \geq K} 2^{j[\beta - (\q + 1)]},\]
which can be made arbitrarily small by choice of $K$.

\paragraph*{Proof of \eqref{M-[Asymptotics] thetatildestar}}
We proceed as in the proof of \eqref{M-[Asymptotics] thetahat}. Let $t \in \R$ be given. By Lemma \ref{M-[Lemma] Generalized Switch Relation} and change of variables,
\begin{align*}
&\P_n^*\left[r_n(\widetilde{\theta}_n^*(\x) - \widehat{\theta}_n(\x) ) > t \right] \\
&= \P_n^*\left[\max\argmax_{x \in \widehat{\Phi}_n^{*-}([0,\widehat{u}_n^*])} \left\{[\widehat{\theta}_n(\x) + t r_n^{-1}] \widehat{\Phi}_n^*(x) - \LSC_{\widehat{\Phi}_n^*}\big(\widetilde{\Gamma}_n^*\big)(x) \right\} < \widehat{\Phi}_n^{*-} \circ \widehat{\Phi}_n^*(\x) \right]\\
&= \P\left[\max\argmin_{v \in \widehat{V}_{\x,n}^{\q,*}} \widehat{H}_{\x,n}^{\q,*}(v;t) < \widehat{Z}_{\x,n}^{\q,*} \right],
\end{align*}
where
\[\widehat{V}_{\x,n}^{\q,*} = \left\{a_n (x - \x) : x \in \widehat{\Phi}_{n}^{*-}([0,\widehat{u}_{n}^*])\right\},\]
\[\widehat{H}_{\x,n}^{\q,*}(v;t) = \LSC_{\widehat{L}_{\x,n}^{\q,*}}
\left(\widehat{G}_{\x,n}^{\q,*} + \widetilde{M}_{\x,n}^{\q} + r_n \widehat{\theta}_n(\x) \widehat{L}_{\x,n}^{\q,*} \right)(v)  - [r_n \widehat{\theta}_n(\x) + t] \widehat{L}_{\x,n}^{\q,*}(v)\]
and
\[\widehat{Z}_{\x,n}^{\q,*} = a_n \left[\widehat{\Phi}_{n}^{*-} \circ \widehat{\Phi}_{n}^*(\x) - \x \right],\]
with
\begin{align*}
\widehat{G}_{\x,n}^{\q,*}(v) &= \sqrt{n a_n} \left[\widehat{\Gamma}_n^*(\x + v a_n^{-1}) - \widehat{\Gamma}_n^*(\x) - \widehat{\Gamma}_n(\x + v a_n^{-1}) + \widehat{\Gamma}_n(\x) \right] \\
&\qquad - \widehat{\theta}_n(\x) \sqrt{n a_n} \left[\widehat{\Phi}_n^*(\x + v a_n^{-1}) - \widehat{\Phi}_n^*(\x) - \widehat{\Phi}_n(\x + v a_n^{-1}) + \widehat{\Phi}_n(\x)\right],\\
\widetilde{M}_{\x,n}^{\q}(v) &= \sqrt{n a_n} \widetilde{M}_{\x,n}(v a_n^{-1}), \qquad \widehat{L}_{\x,n}^{\q,*}(v) = a_n \left[\widehat{\Phi}_{n}^*(\x + v a_n^{-1}) - \widehat{\Phi}_{n}^*(\x)\right].
\end{align*}
By \ref{M-Assumption B: uniform convergence Phi} and Lemma \ref{[Appendix Lemma] Phi- circ Phi}, $\widehat{Z}_{\x,n}^{\q,*} = o_{\P}(1)$. Suppose also that
\begin{equation}\label{Appendix eq: bootstrap argmax convergence}
\max\argmin_{v \in \widehat{V}_{\x,n}^{\q,*}} \widehat{H}_{\x,n}^{\q,*}(v;t) \leadsto_\P \argmin_{v \in \R} \mathcal{H}_{\x}^{\q} (v;t).
\end{equation}
Then, as in the proof of \eqref{M-[Asymptotics] thetahat},
\[\P_n^*\left[r_n(\widetilde{\theta}_n^*(\x) - \widehat{\theta}_n(\x) ) > t \right] \to_\P \P\left[\frac{1}{\partial\Phi_0(\x)}\partial_{-}\GCM_{\R}(\mathcal{G}_{\x} + \mathcal{M}_{\x}^{\q}(0) > t \right].\]
The proof of \eqref{M-[Asymptotics] thetatildestar} can therefore be completed by showing \eqref{Appendix eq: bootstrap argmax convergence}.

We shall do so by showing that
\begin{equation}\label{Appendix eq: vhatstar tight}
\widehat{v}_n^*(t) = \max\argmin_{v \in \widehat{V}_{\x,n}^{\q,*}} \widehat{H}_{\x,n}^{\q,*}(v;t) = O_{\P}(1)
\end{equation}
and
\begin{equation}\label{Appendix eq: Hhatstar convergence}
\widehat{H}_{\x,n}^{\q,*}(\cdot;t) \leadsto_\P \mathcal{H}_{\x}^{\q} (\cdot;t).
\end{equation}

We begin by showing \eqref{Appendix eq: Hhatstar convergence}. First, by \ref{M-Assumption B: weak convergence}, $\widehat{G}_{\x,n}^{\q,*} \leadsto_\P \mathcal{G}_{\x}$. Also, by Assumption \ref{M-Assumption C}, $\widetilde{M}_{\x,n}^{\q,*} \leadsto_\P \mathcal{M}_{\x}^{\q}$. Moreover, $\widehat{L}_{\x,n}^{\q,*} - \widehat{L}_{\x,n}^{\q} \leadsto_\P 0$ by \ref{M-Assumption B: uniform convergence Phi}, where, as shown in the proof of (\ref{Appendix eq: argmax convergence}), $\widehat{L}_{\x,n}^{\q} \leadsto \mathcal{L}_{\x}$. In particular, $\widehat{L}_{\x,n}^{\q,*} \leadsto_\P \mathcal{L}_{\x}$ and therefore
\[(\widehat{G}_{\x,n}^{\q,*},\widetilde{M}_{\x,n}^{\q},\widehat{L}_{\x,n}^{\q,*}) \leadsto_\P (\mathcal{G}_{\x}^{\q},\mathcal{M}_{\x}^{\q},\mathcal{L}_{\x}).\]

Because $\widehat{G}_{\x,n}^{\q,*} + \widetilde{M}_{\x,n}^{\q}$ is asymptotically equicontinuous,
\[\LSC_{\widehat{L}_{\x,n}^{\q,*}}\left(\widehat{G}_{\x,n}^{\q,*} + \widetilde{M}_{\x,n}^{\q}\right) - \left(\widehat{G}_{\x,n}^{\q,*} + \widetilde{M}_{\x,n}^{\q}\right) \leadsto_\P 0\]
by \ref{M-Assumption B: uniform convergence Phi} and Lemma \ref{[Appendix Lemma] LSC of f (local)}. Also, by \ref{M-Assumption B: jump of Phi hat} and Lemma \ref{[Appendix Lemma] LSC of Phi (local)},
\[\LSC_{\widehat{L}_{\x,n}^{\q,*}}\left(r_n \widehat{\theta}_n(\x)  \widehat{L}_{\x,n}^{\q,*}\right) - \left(r_n \widehat{\theta}_n(\x) \widehat{L}_{\x,n}^{\q,*}\right) \leadsto_\P 0.\]
The result \eqref{Appendix eq: Hhatstar convergence} follows from the three preceding displays and the fact that, on $\widehat{V}_{\x,n}^{\q,*}$,
\begin{align*}
    0 &\geq \LSC_{\widehat{L}_{\x,n}^{\q,*}}\left(\widehat{G}_{\x,n}^{\q,*} + \widetilde{M}_{\x,n}^{\q} + r_n \widehat{\theta}_n(\x) \widehat{L}_{\x,n}^{\q,*}\right) - \left(\widehat{G}_{\x,n}^{\q,*} + \widetilde{M}_{\x,n}^{\q} + r_n \widehat{\theta}_n(\x) ,\widehat{L}_{\x,n}^{\q,*}\right) \\
    &\geq \LSC_{\widehat{L}_{\x,n}^{\q,*}}\left(\widehat{G}_{\x,n}^{\q,\,*} + \widetilde{M}_{\x,n}^{\q}\right) - \left(\widehat{G}_{\x,n}^{\q,*} + \widetilde{M}_{\x,n}^{\q}\right) \\
    &\qquad + \LSC_{\widehat{L}_{\x,n}^{\q,*}}\left(r_n \widehat{\theta}_n(\x) \widehat{L}_{\x,n}^{\q,*}\right) - \left(r_n \widehat{\theta}_n(\x) \widehat{L}_{\x,n}^{\q,*}\right).    
\end{align*}

Next, to show \eqref{Appendix eq: vhatstar tight}, we first define $\widehat{\theta}_n(\x;t) = \widehat{\theta}_n(\x) + t r_n^{-1}$ and note that
\[\widehat{v}_n^*(t) = a_n \left[\max\argmax_{x \in \widehat{\Phi}_n^{*-}([0,\widehat{u}_n^*])} \left\{\widehat{\theta}_n(\x;t) \widehat{\Phi}_n^*(x) - \LSC_{\widehat{\Phi}_n^*}\left(\widetilde{\Gamma}_n^*\right)(x)\right\} - \x\right].\]
Now, if $|\widehat{v}_n^*(t)| > a_n \delta > 0,$ then
\[\sup_{x \not \in I_{\x}^{\delta}} \left\{\widehat{\theta}_n(\x;t) \widehat{\Phi}_n^*(x) - \LSC_{\widehat{\Phi}_n^*} \left(\widetilde{\Gamma}_n^*\right)(x) \right\} \geq \widehat{\theta}_n(\x;t) \widehat{\Phi}_n^*(\x) - \LSC_{\widehat{\Phi}_n^*}\left(\widetilde{\Gamma}_n^*\right)(\x),\]
where $|\widehat{\theta}_n(\x;t) - \theta_0(\x)| = O_\P(r_n^{-1}) = o_\P(1)$, and, by \ref{M-Assumption B: uniform convergence Phi},
\[\sup_{x \in I}\left|\widehat{\Phi}_n^*(x) - \Phi_0(x)\right| = o_{\P}(1).\]
Therefore, defining $\widehat{\x}_n^* = \widehat{\Phi}_n^{*-}(\widehat{\Phi}_n^{*}(\x)) = \x + o_\P(1)$ and using (\ref{Appendix eq: LSC representation}),
\begin{align*}
&\widehat{\theta}_n(\x;t) \widehat{\Phi}_n^*(\x) - \LSC_{\widehat{\Phi}_n^*}\left(\widetilde{\Gamma}_n^*\right)(\x) \\
&\leq \widehat{\theta}_n(\x;t) \widehat{\Phi}_n^*(\x) - \widetilde{\Gamma}_n^*(\widehat{\x}_n^*) \\
&= \widehat{\theta}_n(\x;t) \widehat{\Phi}_n^*(\x) - \widehat{\theta}_n(\x) \widehat{\Phi}_n(\widehat{\x}_n^*) - \widehat{\Gamma}_n^*(\widehat{\x}_n^*) + \widehat{\Gamma}_n(\widehat{\x}_n^*) - \widetilde{M}_{\x,n}^\q(\widehat{Z}_n^{\q,*}) / \sqrt{n a_n} = o_\P(1),  
\end{align*}
where the last equality uses \ref{M-Assumption B: uniform consistency Gamma}, $\widehat{Z}_n^{\q,*} = o_\P(1)$, and Assumption \ref{M-Assumption C}. Also, using \ref{M-Assumption B: uniform consistency Gamma}, \ref{M-Assumption B: uniform convergence Phi}, and Assumption \ref{M-Assumption C}, we have, uniformly in $x \not \in I_\x^\delta$ and for some $c > 0$,
\begin{align*}
\widetilde{\Gamma}_n^*(x) - \widehat{\theta}_n(\x) \widehat{\Phi}_n^*(x) &= \widehat{\Gamma}_n^*(x) - \widehat{\Gamma}_n(x) - \widehat{\theta}_n(\x) \left(\widehat{\Phi}_n^*(x) - \widehat{\Phi}_n(x)\right) + \widetilde{M}_n(x - \x) \\
&\geq c \delta^{\q + 1} + o_\P(1),
\end{align*}
and therefore, by Lemma \ref{[Appendix Lemma] LSC of Phi (global)},
\[\sup_{x \not \in I_{\x}^{\delta}} \left\{\widehat{\theta}_n(\x;t) \widehat{\Phi}_n^*(x) - \LSC_{\widehat{\Phi}_n^*} \left(\widetilde{\Gamma}_n^*\right)(x) \right\} \leq - c \delta^{\q + 1} + o_\P(1).\]

As a consequence, $\widehat{v}_n^*(t) = o_\P(a_n)$: For any $\delta > 0$,
\[\P\left[|\widehat{v}_n^*(t)| > a_n \delta \right] \leq \P\left[- c \delta^{\q + 1} \geq o_{\P}(1)\right] = o(1).\]

Next, defining $\widehat{V}_{\x,n}^{\q,*}(j) = \{v\in \widehat{V}_{\x,n}^{\q,*} : 2^j < |v| \leq 2^{j+1}\}$ and using $\widehat{v}_n^*(t) = o_{\P}(a_n)$, we have, for any $K$, any positive $\delta'$, and any sequence of events $\{\mathcal{A}_n'\}$ with $\lim_{n \to \infty}\P[\mathcal{A}_n'] = 1$,
\begin{align*}
\limsup_{n \to \infty}\P\left[|\widehat{v}_n^*(t)| > 2^K\right] &\leq \limsup_{n \to \infty}\sum_{j \geq K : 2^j \leq a_n \delta'} \P\left[2^j < |\widehat{v}_n^*(t)| \leq 2^{j+1} \cap \mathcal{A}_n'\right] \\
&\leq \limsup_{n \to \infty} \sum_{j \geq K : 2^j \leq a_n \delta'} \P\left[\inf_{v \in \widehat{V}_{\x,n}^{\q,*}(j)} \widehat{H}_{\x,n}^{\q,*}(v;t) \leq \widehat{H}_{\x,n}^{\q,*}(0;t) \cap \mathcal{A}_n' \right].
\end{align*}
The proof of \eqref{Appendix eq: vhatstar tight} can therefore be completed by showing that the majorant side in the display can be made arbitrarily small by choice of $K$, $\delta'$, and $\{\mathcal{A}_n'\}$.

To do so, we begin by analyzing each term in the basic bound 
\begin{align*}
\widehat{H}_{\x,n}^{\q,*}(v;t) - \widehat{H}_{\x,n}^{\q,*}(0;t) &\geq \LSC_{\widehat{L}_{\x,n}^{\q,*}}\left(\widehat{G}_{\x,n}^{\q,*}\right)(v) + \LSC_{\widehat{L}_{\x,n}^{\q,*}}\left(\widetilde{M}_{\x,n}^{\q}\right)(v) - t \widehat{L}_{\x,n}^{\q,*}(v) \\
&+ \LSC_{\widehat{L}_{\x,n}^{\q,*}}\left(r_n \widehat{\theta}_n(\x) \widehat{L}_{\x,n}^{\q,*}\right)(v) - r_n \widehat{\theta}_n(\x) \widehat{L}_{\x,n}^{\q,*} (v) - \widehat{H}_{\x,n}^{\q,*}(0;t).
\end{align*}
Because $\widehat{H}_{\x,n}^{\q,*}(0;t) \leadsto_\P \mathcal{H}_{\x}^{\q}(0;t) = 0$ and because, by \ref{M-Assumption B: jump of Phi hat} and Lemma \ref{[Appendix Lemma] LSC of Phi (local)}, there is a positive $\delta'$ such that
\[\sup_{|v|\leq a_n \delta'}\left|\LSC_{\widehat{L}_{\x,n}^{\q,*}}\left(r_n \widehat{\theta}_n(\x) \widehat{L}_{\x,n}^{\q,*}\right)(v) - r_n \widehat{\theta}_n(\x) \widehat{L}_{\x,n}^{\q,*} (v)\right| = o_{\P}(1),\]
we may assume that, on $\{\mathcal{A}_n'\}$ and for some $C_0$,
\[\sup_{|v| \leq a_n \delta'}\left|\LSC_{\widehat{L}_{\x,n}^{\q,*}}\left(r_n \widehat{\theta}_n(\x) \widehat{L}_{\x,n}^{\q,*}\right)(v) - r_n \widehat{\theta}_n(\x) \widehat{L}_{\x,n}^{\q,*}(v) - \widehat{H}_{\x,n}^{\q,*}(0;t)\right| \leq C_0.\]
Also, because, by \ref{M-Assumption B: uniform convergence Phi} and \ref{M-Assumption A: Phi0}, there is a positive $\delta'$ such that
\[\sup_{|v| \leq a_n \delta'}\left|\widehat{L}_{\x,n}^{\q,*}(v) - L_{\x,n}^{\q} (v)\right| = o_{\P}(1) \qquad \text{and} \qquad \sup_{1 \leq |v| \leq a_n \delta'}\left|\frac{L_{\x,n}^{\q}(v)}{\mathcal{L}_{\x} (v)}\right| < \infty,\]
we may assume that, on $\{\mathcal{A}_n'\}$ and for some $C_L$,
\[\sup_{1 \leq |v| \leq a_n \delta'}\left|\frac{\widehat{L}_{\x,n}^{\q,*}(v)}{v}\right| \leq C_L. \]
Next, by \ref{M-Assumption B: uniform convergence Phi} and Lemma \ref{[Appendix Lemma] LSC of f (local)}, with probability approaching one,
\[\LSC_{\widehat{L}_{\x,n}^{\q,*}}\left(\widetilde{M}_{\x,n}^{\q}\right)(v) \geq \inf_{|v|/2 \leq |v'| \leq 2|v|} \widetilde{M}_{\x,n}^{\q}(v') \qquad \text{for every } |v| \geq 2,\]
while, by Assumption \ref{M-Assumption C}, there is a positive $c$ such that, with probability approaching one,
\[\inf_{|v| \geq 1}\frac{\widetilde{M}_{\x,n}^{\q}(v)}{v^{\q + 1}} > c.\]
We may therefore assume that, on $\{\mathcal{A}_n'\}$ and for some positive $C_M$,
\[\inf_{2 \leq |v| \leq a_n \delta'}\frac{\LSC_{\widehat{L}_{\x,n}^{\q,*}}\left(\widetilde{M}_{\x,n}^{\q}\right)(v)}{v^{\q+1}} \geq C_M. \]
Finally, by \ref{M-Assumption B: uniform convergence Phi} and Lemma \ref{[Appendix Lemma] LSC of f (local)}, with probability approaching one,
\[\left|\LSC_{\widehat{L}_{\x,n}^{\q,*}}\left(\widehat{G}_{\x,n}^{\q,*}\right)(v)\right| \leq \sup_{|v| / 2 \leq |v'| \leq 2 |v|} \left|\widehat{G}_{\x,n}^{\q,*}(v')\right| \qquad \text{for every } |v| \geq 2,\]
and we may therefore assume that, on $\{\mathcal{A}_n'\}$,
\[\sup_{v \in \widehat{V}_{\x,n}^{\q,*}(j)}\left|\LSC_{\widehat{L}_{\x,n}^{\q,*}}\left(\widehat{G}_{\x,n}^{\q,*}\right)(v)\right| \leq \sup_{v \in V_2(j)} \left|\widehat{G}_{\x,n}^{\q,*}(v)\right| \qquad \text{for every } j \geq 2 \text{ with } 2^j \leq a_n \delta'.\]

As a consequence, by the Markov inequality,
\begin{align*}
&\P\left[\inf_{v \in \widehat{V}_{\x,n}^{\q,*}(j)} \widehat{H}_{\x,n}^{\q,*}(v;t) \leq \widehat{H}_{\x,n}^{\q,*}(0;t) \cap \mathcal{A}_n' \right] \\
&\leq \P\left[\sup_{v \in V_2(j)} \left|\widehat{G}_{\x,n}^{\q,*}(v)\right| \geq \inf_{v \in V_1(j)} \left[C_M v^{\q+1} - C_L |v| - C_0\right] \cap \mathcal{A}_n' \right] \\
&\leq \frac{\E\left[\sup_{v \in V_2(j)} \left|\widehat{G}_{\x,n}^{\q,*}(v)\right| \I_{\mathcal{A}_n'} \right]}{\inf_{v \in V_1(j)} \left[C_M v^{\q+1} - C_L |v| - C_0\right]} \qquad \text{for every } j \geq 2 \text{ with } 2^j \leq a_n \delta',
\end{align*}
where, by \ref{M-Assumption B: uniform convergence Phi}, we may assume that, for some $C_G$,
\[\E\left[\sup_{v \in V_2(j)} \left| \widehat{G}_{\x,n}^{\q,*}(v)\right| \I_{\mathcal{A}_n'} \right] \leq \sum_{j-1 \leq j' \leq j+1} \E\left[\sup_{v \in V_1(j)} \left|\widehat{G}_{\x,n}^{\q,*}(v)\right| \I_{\mathcal{A}_n'} \right] \leq C_G 2^{j \beta},\]
and where, for all sufficiently large $j$,
\[\inf_{v \in V_1(j)} \left[C_M v^{\q+1} - C_L |v| - C_0\right] \geq \frac{1}{2} C_M 2^{j(\q+1)}.\]
In other words, for large $K$,
\[\limsup_{n \to \infty} \sum_{j \geq K : 2^j \leq a_n \delta'} \P\left[\inf_{v \in \widehat{V}_{\x,n}^{\q,*}(j)} \widehat{H}_{\x,n}^{\q,*}(v;t) \leq \widehat{H}_{\x,n}^{\q,*}(0;t) \cap \mathcal{A}_n' \right] \leq \frac{2 C_G}{C_M}\sum_{j \geq K} 2^{j[\beta - (\q + 1)]},\]
which can be made arbitrarily small by choice of $K$.

\paragraph*{Proof of \eqref{M-Bootstrap consistency}} The bootstrap consistency result \eqref{M-Bootstrap consistency} follows from \eqref{M-[Asymptotics] thetahat}, \eqref{M-[Asymptotics] thetatildestar}, Polya's theorem, and the fact that, by Lemma \ref{M-[Lemma] Continuity of argmax},  the limiting distribution in \eqref{M-[Asymptotics] thetahat} and \eqref{M-[Asymptotics] thetatildestar} has a continuous cdf. \qed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Numerical derivative estimators
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of Lemma \ref{M-[Lemma] Mean function estimation with known q}}

For the monomial approximation estimator, we have
\begin{align*}
\widetilde{\mathcal{D}}_{\q,n}^{\mathtt{MA}}(\x) &= \epsilon_n^{-(\q + 1)}[\Gamma_0(\x + \epsilon_n) - \Gamma_0(\x) - \theta_0(\x) \{\Phi_0(\x + \epsilon_n) - \Phi_0(\x)\} ] \\
&\quad + \epsilon_n^{-(\q + 1/2)}n^{-1/2} \widehat{G}_{\x,n}(1;\epsilon_n) \\
&\quad - \epsilon_n^{-\q}[\widehat{\theta}_n(\x) -\theta_0(\x)] \widehat{R}_{\x,n}(1;\epsilon_n) \\
&\quad -\epsilon_n^{-(\q + 1)}[\widehat{\theta}_n(\x) - \theta_0(\x)] [\Phi_0(\x + \epsilon_n) - \Phi_0(\x)] \\
&= \frac{\partial^{\q} \theta_0(\x) \partial \Phi_0(\x)}{(\q + 1)!} + o(1) + O_{\P}[(n \epsilon_n^{1 + 2 \q})^{-1/2} + (n \epsilon_n^{1 + 2 \q})^{-\q / (1 + 2 \q)}] \\
&= \frac{\partial^{\q} \theta_0(\x) \partial \Phi_0(\x)}{(\q + 1)!} + o_\P(1),
\end{align*}
where the second equality uses $\epsilon_n \to 0$ and the last equality uses $n \epsilon_n^{1 + 2 \q} \to \infty$.

Similarly, for the forward difference estimator, we have
\begin{align*}
\widetilde{\mathcal{D}}_{\q,n}^{\mathtt{FD}}(\x) &= \epsilon_n^{-(\q+1)}\sum_{k = 1}^{\q + 1} (-1)^{k + \q + 1} \binom{\q + 1}{k} [\Upsilon_0(\x + k \epsilon_n) - \Upsilon_0(\x) ] \\
&\quad + \epsilon_n^{-(\q + 1/2)} n^{- 1/2}\sum_{k = 1}^{\q + 1}(-1)^{k + \q + 1} \binom{\q + 1}{k} \widehat{G}_{\x,n}(k;\epsilon_n) \\
&\quad - \epsilon_n^{-\q}[\widehat{\theta}_n(\x) - \theta_0(\x)] \sum_{k = 1}^{\q + 1} (-1)^{k + \q + 1}\binom{\q + 1}{k} \widehat{R}_{\x,n}(k;\epsilon_n) \\
&\quad -\epsilon_n^{-(\q + 1)}[\widehat{\theta}_n(\x) - \theta_0(\x)] \sum_{k = 1}^{\q + 1} (-1)^{k + \q + 1} \binom{\q + 1}{k} [\Phi_0(\x + k \epsilon_n) - \Phi_0(\x)] \\
&= \frac{\partial^{\q} \theta_0(\x) \partial \Phi_0(\x)}{(\q + 1)!} + o(1) + O_{\P}[(n \epsilon_n^{1 + 2 \q})^{-1/2} + (n \epsilon_n^{1 + 2 \q})^{-\q / (1 + 2 \q)}] \\
&= \frac{\partial^{\q} \theta_0(\x) \partial \Phi_0(\x)}{(\q + 1)!} + o_\P(1). \qed
\end{align*}

\subsection{Proof of Lemma \ref{M-[Lemma] Mean function estimation unknown q}}

Proceeding as in the proof of Lemma \ref{M-[Lemma] Mean function estimation with known q}, we have
\begin{align*}
\widetilde{\mathcal{D}}_{j,n}^{\mathtt{BR}}(\x) &= \epsilon_n^{-(j + 1)} \sum_{k = 1}^{\underline{\s} + 1} \lambda_j^{\mathtt{BR}}(k) [\Upsilon_0(\x + c_k \epsilon_n) - \Upsilon_0(\x)] \\
&\quad + \epsilon_n^{-(j + 1/2)} n^{-1/2} \sum_{k = 1}^{\underline{\s} + 1}\lambda_j^{\mathtt{BR}}(k) \widehat{G}_{\x,n}(c_k;\epsilon_n) \\
&\quad - \epsilon_n^{-j}[\widehat{\theta}_n(\x) -\theta_0(\x)] \sum_{k = 1}^{\underline{\s} + 1} \lambda_j^{\mathtt{BR}}(k)\widehat{R}_{\x,n}(c_k;\epsilon_n) \\
&\quad -\epsilon_n^{-(j + 1)}[\widehat{\theta}_n(\x)-\theta_0(\x)] \sum_{k = 1}^{\underline{\s} + 1} \lambda_j^{\mathtt{BR}}(k) [\Phi_0(\x + c_k \epsilon_n) - \Phi_0(\x)] \\
&= \frac{\partial^{j + 1} \Upsilon_0(\x)}{(j + 1)!} + O(\epsilon_n^{\min \{\s,\underline{\s} + 1\} - j}) + O_{\P}(\epsilon_n^{-(j + 1/2)} n^{-1/2} + \epsilon_n^{-j} a_n^{-\q}) \\
&= \mathcal{D}_j(\x) + O(\epsilon_n^{\min \{\s,\underline{\s} + 1\} - j}) + a_n^{j - \q} O_{\P}[(a_n \epsilon_n)^{-(j + 1/2)} + (a_n \epsilon_n)^{-j}],
\end{align*}
where the second equality uses $\epsilon_n \to 0$ and the defining property of $\{\lambda_j^{\mathtt{BR}}(k) : k = 1,\dots,\underline{\s}\}$.

The second part of the lemma follows from the fact that if
\[n \epsilon_n^{(1 + 2 \bar \q) \min(\underline{\s},\s - 1)/(\bar \q - 1)} \to 0 \quad \text{and} \quad n \epsilon_n^{1 + 2 \bar \q} \to \infty,\]
then
\[a_n^{\q - j} \epsilon_n^{\min\{\s,\underline{\s} + 1\} - j} \to 0 \quad \text{and} \quad a_n \epsilon_n \to \infty.\]
\qed 

\subsection{Higher-order expansion of the bias-reduced estimator }\label{Appendix Section: higher-order expansion}

In addition to the assumptions of Lemma \ref{M-[Lemma] Mean function estimation unknown q}, suppose that $\widehat{R}_{\x,n}(1;\eta_n) = O_{\mathbb{P}}(a_n^{-1/2})$ for $a_n^{-1}\eta_n^{-1}=O(1)$ and that, for some $\delta > 0$, $\theta_0$ is $(\underline{\s} + 1)$-times continuously differentiable and $\Phi_0$ is $(\underline{\s} + 2)$-times continuously differentiable on $I_{\x}^{\delta}$. Then, the first term in the stochastic expansion of $\widetilde{\mathcal{D}}_{j,n}^{\mathtt{BR}}(\x)$ satisfies
\begin{align*}
\epsilon_n^{-(j + 1)} &\sum_{k = 1}^{\underline{\s} + 1}\lambda_j^{\mathtt{BR}}(k) [\Upsilon_0(\x + c_k \epsilon_n) - \Upsilon_0(\x) ] - \frac{\partial^{j + 1} \Upsilon_0(\x)}{(j + 1)!} \\
&= \epsilon_n^{\underline{\s} + 1 - j} \frac{\partial^{\underline{\s} + 2} \Upsilon_0(\x)}{(\underline{\s} + 2)!} \sum_{k = 1}^{\underline{\s} + 1} \lambda_j^{\mathtt{BR}}(k) c_k^{\underline{\s} + 2} + o(\epsilon_n^{\underline{\s} + 1 - j}) =\epsilon_n^{\underline{\s} + 1 - j} \mathsf{B}_{j}^{\mathtt{BR}}(\x) + o(\epsilon_n^{\underline{\s} + 1 - j}).
\end{align*}
Also, the approximate variance of
\[ \epsilon_n^{-(j + 1/2)} n^{-1/2} \sum_{k = 1}^{\underline{\s} + 1}\lambda_j^{\mathtt{BR}}(k) \widehat{G}_{\x,n}(c_k;\epsilon_n)\]
is
\[ \frac{1}{n \epsilon_n^{1 + 2 j}} \sum_{k = 1}^{\underline{\s} + 1}\sum_{l = 1}^{\underline{\s} + 1} \lambda_{j}^{\mathtt{BR}}(k) \lambda_{j}^{\mathtt{BR}}(l) \mathcal{C}_\x(c_k,c_l) = \frac{1}{n \epsilon_n^{1 + 2 j}} \mathsf{V}_{j}^{\mathtt{BR}}(\x).\]
Finally, the third term in the stochastic expansion of $\widetilde{\mathcal{D}}_{j,n}^{\mathtt{BR}}(\x)$ is asymptotically negligible under the condition that $\widehat{R}_{\x,n}(1;\eta_n) = O_{\mathbb{P}}(a_n^{-1/2})$ for $a_n^{-1}\eta_n^{-1}=O(1)$, while the fourth term exhibits only a higher-order dependence on $\epsilon_n$ (relative to the dependence exhibited by the first two terms).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Primitive conditions for bootstrap approximation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of Lemma \ref{M-[Lemma] Bootstrapping}}

We verify that Assumptions \ref{M-Assumption A} and \ref{M-Assumption E} imply Assumptions \ref{M-Assumption B: weak convergence}-\ref{M-Assumption B: uniform convergence Phi}. Define
\[\bar{\Gamma}_n^*(x) = \frac{1}{n} \sum_{i = 1}^nW_{i,n} \gamma_0(x;\mathbf{Z}_i) \qquad \text{and} \qquad \bar{\Phi}_n^*(x) = \frac{1}{n} \sum_{i = 1}^nW_{i,n} \phi_0(x;\mathbf{Z}_i).\]
\subsubsection*{Verifying Assumption  \ref{M-Assumption B: weak convergence}}

\paragraph*{Non-bootstrap weak convergence}
We first prove $\widehat{G}_{\x,n}^{\q}\rightsquigarrow \mathcal{G}_{\x}$. 
By Assumption \ref{M-Assumption E: Gamma hat}-\ref{M-Assumption E: Phi hat}, 
\begin{equation*}
    \sqrt{n a_n} \sup_{\vert v\vert\leq K}\left\vert\widehat{\Gamma}_n(\x+va_n^{-1}) -\widehat{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+va_n^{-1}) + \bar{\Gamma}_n(\x)\right\vert =  o_{\P}(1), 
\end{equation*}
\begin{equation*}
    \sqrt{n a_n} \sup_{\vert v\vert\leq K}\left\vert\widehat{\Phi}_n(\x+va_n^{-1}) -\widehat{\Phi}_n(\x) - \bar{\Phi}_n(\x+va_n^{-1}) + \bar{\Phi}_n(\x)\right\vert = o_{\P}(1), 
\end{equation*}
for each $K>0$, and thus,
\begin{equation*}
	\sup_{\vert v\vert\leq K}\left\vert\widehat{G}_{\x,n}^{\q}(v) - \sqrt{\frac{a_n}{n}} \sum_{i=1}^n \big\{\psi_{\x}(va_n^{-1};\mathbf{Z}_i) - \E[\psi_{\x}(va_n^{-1};\mathbf{Z}) ]  \big\}\right\vert  = o_{\P}(1).
\end{equation*}
Letting $\bar{\psi}_{\x,n}(v;\mathbf{Z}_i) = \sqrt{a_n}\psi_{\x}(va_n^{-1};\mathbf{Z}_i) $, we want to prove that the empirical process of $\{\bar{\psi}_{\x,n}(v;\cdot):|v|\leq K\}$ weakly converges to $\mathcal{G}_{\x}$. We verify finite-dimensional weak convergence and stochastic equicontinuity. 

Letting $\eta_n = Ka_n^{-1}$, 
\begin{equation*}
	n^{-1}\sup_{|v|\leq K} \E[|\bar{\psi}_{\x,n}(v;\mathbf{Z})|^4] \leq (1+|\theta_0(\x)|)^4 n^{-1} a_n^2 \E[ \bar{D}_{\gamma}^{\eta_n} (\mathbf{Z})^4 +   \bar{D}_{\phi}^{\eta_n} (\mathbf{Z})^4] = o(1).
\end{equation*}
Also, convergence of the covariance kernel is imposed in Assumption \ref{M-Assumption E: covariance kernel}. Thus, the Lyapunov central limit theorem implies the finite-dimensional convergence. 

For stochastic equicontinuity, following the argument of \citet[Lemma 4.6]{Kim-Pollard_1990_AoS} and using $a_n\E[\bar{D}_{\gamma}^{\eta_n} (\mathbf{Z})^2 +   \bar{D}_{\phi}^{\eta_n} (\mathbf{Z})^2]=O(1)$, it suffices to show \\ $\sup_{|v-s|\leq \epsilon_n,|v|\lor |s|\leq K} \frac{1}{n}\sum_{i=1}^n |\bar{\psi}_{\x,n}(v;\mathbf{Z}_i)-\bar{\psi}_{\x,n}(s;\mathbf{Z}_i)|^2=o_{\P}(1)$ for any $\epsilon_n=o(1)$. For a constant $M>0$,
\begin{align*}
	&\sup_{|v-s|\leq \epsilon_n,|v|\lor |s|\leq K}\frac{1}{n}\sum_{i=1}^n |\bar{\psi}_{\x,n}(v;\mathbf{Z}_i)-\bar{\psi}_{\x,n}(s;\mathbf{Z}_i)|^2\\
	& \leq 4(1+|\theta_0(\x)|)^2\frac{1}{n}\sum_{i=1}^n a_n [\bar{D}_{\gamma}^{\eta_n} (\mathbf{Z}_i)^2 +   \bar{D}_{\phi}^{\eta_n} (\mathbf{Z}_i)^2] \1\{\bar{D}_{\gamma}^{\eta_n} (\mathbf{Z}_i) +   \bar{D}_{\phi}^{\eta_n} (\mathbf{Z}_i)> M\} \\
	& + a_n M\sup_{|v-s|\leq \epsilon_n,|v|\lor |s|\leq K} \E \big[|\psi_{\x}(va_n^{-1};\mathbf{Z})-\psi_{\x}(sa_n^{-1};\mathbf{Z})| \big] \\
	& +M a_n \sup_{\substack{|v-s|\leq \epsilon_n\\ |v|\lor |s|\leq K}}\frac{1}{n}\sum_{i=1}^n  \big\{|\psi_{\x}(va_n^{-1};\mathbf{Z}_i)-\psi_{\x}(sa_n^{-1};\mathbf{Z}_i)| -\E \big[|\psi_{\x}(va_n^{-1};\mathbf{Z})-\psi_{\x}(sa_n^{-1};\mathbf{Z})| \big]\big\}
\end{align*}
where the first term after the inequality can be made arbitrarily small by making $M$ large using $a_n \E[ \bar{D}_{\gamma}^{\eta_n} (\mathbf{Z})^4 +   \bar{D}_{\phi}^{\eta_n} (\mathbf{Z})^4] = O(1)$. The second term is $o_{\P}(1)$ by Assumption \ref{M-Assumption E: covariance kernel}. Finally, the third term is $O_{\P}( \sqrt{a_n/n} )$ using Theorem 4.2 of \cite{Pollard_1989_SS}. 

\paragraph*{Bootstrap weak convergence}
We next prove $\widehat{G}_{\x,n}^{\q}\rightsquigarrow_{\P}\mathcal{G}_{\x}$.
First we posit 
\begin{align}\nonumber
	&\sqrt{n a_n}\sup_{|v|\leq K}|\widehat{\Gamma}_{n}^{* }(\x+va_n^{-1})-\widehat{\Gamma}_{n}^{* }(\x)-\bar{\Gamma}_{n}^{* }(\x+va_n^{-1})+\bar{\Gamma}_{n}^{* }(\x)|=o_{\P}(1),\\
  \label{Appendix eq: high-level bootstrap approximation}
    &\sqrt{n a_n}\sup_{|v|\leq K}|\widehat{\Phi}_{n}^{* }(\x+va_n^{-1})-\widehat{\Phi}_{n}^{* }(\x)-\bar{\Phi}_{n}^{* }(\x+va_n^{-1})+\bar{\Phi}_{n}^{* }(\x)|=o_{\P}(1),
\end{align}
which follows from the hypothesis of the lemma as shown below.
By the above display,
\begin{equation*}
	 \sup_{\vert v\vert\leq K}\left\vert\widehat{G}_{\x,n}^{\q,*}(v)-\sqrt{\frac{a_n}{n}} \sum_{i=1}^n W_{i,n} \Big\{\psi_{\x}(va_n^{-1};\mathbf{Z}_i) - \frac{1}{n}\sum_{j=1}^n\psi_{\x}(va_n^{-1};\mathbf{Z}_j)   \Big\} \right\vert  = o_{\P}(1)
\end{equation*}
where we use $\sqrt{\frac{a_n}{n}} \sum_{i=1}^nW_{i,n} \{\phi_0(\x+va_n^{-1};\mathbf{Z}_i) - \phi_0(\x;\mathbf{Z}_i) - \frac{1}{n}\sum_{j=1}^n[\phi_0(\x+va_n^{-1};\mathbf{Z}_j) - \phi_0(\x;\mathbf{Z}_j)]\}=O_{\P}(1)$ uniformly over $\vert v\vert \leq K$, and $\widehat{\theta}_n(\x)\to_{\P}\theta_0(\x)$.
Let $\widehat{\psi}_{\x,n}(v;\mathbf{Z})=  \sqrt{a_n}[\psi_{\x}(v a_n^{-1};\mathbf{Z}) - \frac{1}{n}\sum_{j=1}^n\psi_{\x}(va_n^{-1};\mathbf{Z}_j)]$ and to prove the finite-dimensional convergence, we apply Lemma 3.6.15 of \cite{vanderVaart-Wellner_1996_Book}. Assumption \ref{M-Assumption E: bootstrap weights} implies $\frac{1}{n}\sum_{i=1}^n(W_{i,n}-1)^2\to_{\P}1$ and $n^{-1}\max_{1\leq i \leq n}W_{i,n}^2=o_{\P}(1)$. Since
\begin{align*}
	\frac{1}{n}\sum_{i=1}^n \widehat{\psi}_{\x,n}(v;\mathbf{Z}_i)\widehat{\psi}_{\x,n}(u;\mathbf{Z}_i) &= a_n \bigg[\frac{1}{n}\sum_{i=1}^n \psi_{\x}(v;\mathbf{Z}_i)\psi_{\x}(u;\mathbf{Z}_i) \\
	&\qquad - \frac{1}{n}\sum_{i=1}^n \psi_{\x}(v;\mathbf{Z}_i)\frac{1}{n}\sum_{i=1}^n \psi_{\x}(u;\mathbf{Z}_i) \bigg]
\end{align*}
and $\sup_{|v|\leq\eta } |\psi_{\x}(v;\mathbf{Z})|\leq \bar{D}_{\gamma}^{\eta}(\mathbf{Z}) + |\theta_0(\x)|\bar{D}_{\phi}^{\eta}(\mathbf{Z})$, for any $v,u\in\R$,
\begin{equation*}
	\frac{1}{n}\sum_{i=1}^n \widehat{\psi}_{\x,n}(v;\mathbf{Z}_i)\widehat{\psi}_{\x,n}(u;\mathbf{Z}_i) - a_n\E[\psi_{\x}(va_n^{-1};\mathbf{Z})\psi_{\x}(ua_n^{-1};\mathbf{Z})] =o_{\P}(1).
\end{equation*}
Also, $\frac{1}{n}\sum_{i=1}^n \widehat{\psi}_{\x,n}^4(v;\mathbf{Z}_i)=O_{\P}(1)$ and we verified the hypothesis of the lemma.

For stochastic equicontinuity, let $\epsilon_n=o(1)$ and $\eta_n=K a_n^{-1}$. Lemma 3.6.7 of \cite{vanderVaart-Wellner_1996_Book} implies that for any $n_0 \in \{1,\dots, n\}$, there is a fixed constant $C>0$ such that
\begin{align*}
	&\E\bigg[ \sup_{|v-u|\leq \epsilon_n, |v|\lor |u|\leq K} \bigg| \frac{1}{\sqrt{n}} \sum_{i=1}^n W_{i,n}\big[  \widehat{\psi}_{\x,n}(v;\mathbf{Z}_i)-\widehat{\psi}_{\x,n}(u;\mathbf{Z}_i)\big]\bigg| \Big| \{\mathbf{Z}_i\}_{i=1}^n\bigg] \\
	&\leq C \frac{\sqrt{a_n}}{n}\sum_{i=1}^n \big[\bar{D}_{\gamma}^{\eta_n}(\mathbf{Z}_i) + \bar{D}_{\phi}^{\eta_n}(\mathbf{Z}_i)\big] (n_0-1) \E\max_{1\leq i \leq n} |W_{i,n}| n^{-1/2}\\
	& + C \max_{n_0\leq k \leq n} \E\bigg[ \sup_{|v-u|\leq \epsilon_n, |v|\lor |u|\leq K} \bigg| \frac{1}{\sqrt{k}} \sum_{i=n_0}^k \big[  \widehat{\psi}_{\x,n}(va_n^{-1};\mathbf{Z}_{R_i})-\widehat{\psi}_{\x,n}(ua_n^{-1};\mathbf{Z}_{R_i})\big]\bigg| \Big| \{\mathbf{Z}_i\}_{i=1}^n\bigg]
\end{align*}
where $(R_1,\dots,R_n)$ is uniformly distributed on the set of all permutations of $\{1,\dots,n\}$, independent of $\{\mathbf{Z}_i\}_{i=1}^n$. Choose $n_0$ such that $n^{1/2-1/\mathfrak{r}}/n_0\to \infty$ and $n_0/a_n\to \infty$ (which is possible by $\mathfrak{r}>(4\q+2)/(2\q-1)$), and the first term after the inequality in the above display is $o_{\mathbb{P}}(1)$. For the second term, following the argument of \citet[Theorem 3.6.13]{vanderVaart-Wellner_1996_Book}, it suffices to bound
\begin{equation*}
	\max_{n_0\leq k \leq n} \E^* \sup_{|v-u|\leq \epsilon_n, |v|\lor |u|\leq K} \bigg| \frac{1}{\sqrt{k}} \sum_{i=1}^k \big[  \widehat{\psi}_{\x,n}(v;\mathbf{Z}_i^*)-\widehat{\psi}_{\x,n}(u;\mathbf{Z}_i^*)\big]\bigg| 
\end{equation*}
where $\{\mathbf{Z}_i^*\}_{i=1}^k$ denotes a random sample from the empirical cdf and $\E^*$ is the expectation under this empirical bootstrap law. Following the argument of \citet[Lemma 4.6]{Kim-Pollard_1990_AoS}, it suffices to show
\begin{equation*}
	\max_{n_0\leq k \leq n}\E^*  \sup_{|v-u|\leq \epsilon_n, |v|\lor |u|\leq K}  \frac{1}{k} \sum_{i=1}^k \big| \widehat{\psi}_{\x,n}(v;\mathbf{Z}_i^*)-\widehat{\psi}_{\x,n}(u;\mathbf{Z}_i^*)\big|^2 = o_{\P}(1).
\end{equation*} 
For $k\in \{n_0,\dots,n\}$ and $M>0$,
\begin{align*}
	&\E^*  \sup_{|v-u|\leq \epsilon_n, |v|\lor |u|\leq K}  \frac{1}{k} \sum_{i=1}^k \big| \widehat{\psi}_{\x,n}(v;\mathbf{Z}_i^*)-\widehat{\psi}_{\x,n}(u;\mathbf{Z}_i^*)\big|^2\\
	&\leq 2(1+|\theta_0(\x)|)^2 a_n  \frac{1}{n}\sum_{i=1}^n  \big[ \bar{D}_{\gamma}^{\eta_n}(\mathbf{Z}_i)^2 + \bar{D}_{\phi}^{\eta_n}(\mathbf{Z}_i)^2\big]\1\{\bar{D}_{\gamma}^{\eta_n}(\mathbf{Z}_i) + \bar{D}_{\phi}^{\eta_n}(\mathbf{Z}_i) > M-o_{\mathbb{P}}(1)\} \\
	&\quad + 2 M   a_n\sup_{|v-u|\leq \epsilon_n, |v|\lor |u|\leq K} \frac{1}{n}\sum_{i=1}^n \big| \psi_{\x}(va_n^{-1};\mathbf{Z}_i)-\psi_{\x}(ua_n^{-1};\mathbf{Z}_i)\big| \\
	&\quad + 2 M   a_n\E^* \sup_{|v-u|\leq \epsilon_n, |v|\lor |u|\leq K} \frac{1}{k} \sum_{i=1}^k \big| \psi_{\x}(va_n^{-1};\mathbf{Z}_i^*)-\psi_{\x}(ua_n^{-1};\mathbf{Z}_i^*)\big|\\
	&\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad - \E^*\big[\big| \psi_{\x}(va_n^{-1};\mathbf{Z}^*)-\psi_{\x}(ua_n^{-1};\mathbf{Z}^*)\big|\big].
\end{align*}
The first term after the inequality does not depend on $k$ and its expectation can be made arbitrarily small by taking $M$ sufficiently large. The second term is independent of $k$ and we can handle this term by adding and subtracting the expectation inside the summation. For the third term, applying Theorem 4.2 of \cite{Pollard_1989_SS} again, it is bounded by a constant multiple of
\begin{equation*}
	a_n k^{-1/2}  \bigg(\frac{1}{n}\sum_{i=1}^n \big[ \bar{D}_{\gamma}^{\eta_n}(\mathbf{Z}_i)^2 + \bar{D}_{\phi}^{\eta_n}(\mathbf{Z}_i)^2\big] \bigg)^{1/2} = O_{\P}\big(  \sqrt{a_n/k} \big),
\end{equation*}
which is $o_{\P}(1)$ by the choice of $n_0$.

\paragraph*{Verifying \eqref{Appendix eq: high-level bootstrap approximation}}
We focus on the first display.
By adding and subtracting the bootstrap means,
\begin{align*}
	&\widehat{\Gamma}_n^*(\x + va_n^{-1}) -\widehat{\Gamma}_n^*(\x) - \bar{\Gamma}_n^*(\x+va_n^{-1})+ \bar{\Gamma}_n^*(\x)\\
	&= \frac{1}{n}\sum_{i=1}^n W_{i,n} \Breve{\gamma}_n(v;\mathbf{Z}) + \big[ \check{\Gamma}_n(\x+va_n^{-1}) - \check{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+va_n^{-1}) + \bar{\Gamma}_n(\x)\big]
\end{align*}
where 
\begin{align*}
	\Breve{\gamma}_n(v;\mathbf{Z}) &= \widehat{\gamma}(\x+va_n^{-1};\mathbf{Z}_i)-\widehat{\gamma}(\x;\mathbf{Z}_i) - \gamma_0(\x+va_n^{-1};\mathbf{Z}_i)+\gamma_0(\x;\mathbf{Z}_i)\\
    &\qquad - \check{\Gamma}_n(\x+va_n^{-1}) + \check{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+va_n^{-1}) + \bar{\Gamma}_n(\x).
\end{align*}
By Assumption \ref{M-Assumption E: Gamma hat}, $\sqrt{n a_n} \sup_{|v|\leq K} \vert \check{\Gamma}_n(\x+va_n^{-1}) - \check{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+va_n^{-1}) + \bar{\Gamma}_n(\x) \vert = o_{\P}(1)$.
Identical to above, Lemma 3.6.7 and the argument in Theorem 3.6.13 of \cite{vanderVaart-Wellner_1996_Book} imply that for some fixed $C>0$,
\begin{align}
\nonumber
	&\sqrt{n a_n} \E\bigg[ \sup_{ |v|\leq K}  \bigg|\frac{1}{n}\sum_{i=1}^n W_{i,n}\Breve{\gamma}_n(v;\mathbf{Z}_i) \bigg| \Big\vert \{\mathbf{Z}_i\}_{i=1}^n \bigg]\\
 \label{Appendix eq: high-level bootstrap approximation remainder 1}
	&\leq C  \frac{\sqrt{a_n}}{n} \sum_{i=1}^n \sup_{ |v|\leq K} |\Breve{\gamma}_n(v;\mathbf{Z}_i) | \frac{n_0 n^{\mathfrak{r}}}{\sqrt{n}} 
	 +  C \sqrt{a_n}\max_{n_0\leq k\leq n} \E^* \sup_{ |v|\leq K} \bigg|\frac{1}{\sqrt{k}}\sum_{i=1}^k \Breve{\gamma}_n(v;\mathbf{Z}_i^*)  \bigg|.
\end{align}
For the first term in the last line,
\begin{align*}
	& \frac{\sqrt{a_n}}{n} \sum_{i=1}^n \sup_{ |v|\leq K} |\Breve{\gamma}_n(v;\mathbf{Z}_i) |\\
	&\leq \frac{\sqrt{a_n}}{n} \sum_{i=1}^n \sup_{ |v|\leq K} |\widehat{\gamma}(\x+va_n^{-1};\mathbf{Z}_i)-\widehat{\gamma}(\x;\mathbf{Z}_i) -\gamma_0(\x+va_n^{-1};\mathbf{Z}_i) + \gamma_0(\x;\mathbf{Z}_i) | \\
	&\quad + \sqrt{a_n} \sup_{ |v|\leq K} \big|\check{\Gamma}_n(\x+va_n^{-1}) - \check{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+va_n^{-1}) + \bar{\Gamma}_n(\x)\big|
\end{align*}
and both terms are $o_{\P}(1)$ by Assumption \ref{M-Assumption E: Gamma hat}.
For the second term in \eqref{Appendix eq: high-level bootstrap approximation remainder 1}, Corollary 4.3 of \cite{Pollard_1989_SS} implies that for some fixed $C>0$,
\begin{align*}
	&\E^*\left[ \sup_{ |v|\leq K} \bigg|\frac{1}{\sqrt{k}}\sum_{i=1}^k \Breve{\gamma}_n(v;\mathbf{Z}_i^*)  \bigg| \right]^2 \\
    &\leq C \frac{1}{n}\sum_{i=1}^n\sup_{ |v|\leq K} |\widehat{\gamma}(\x+va_n^{-1};\mathbf{Z}_i)-\widehat{\gamma}(\x;\mathbf{Z}_i) -\gamma_0(\x+va_n^{-1};\mathbf{Z}_i) + \gamma_0(\x;\mathbf{Z}_i) |^2
\end{align*}
and this term is $o_{\P}(a_n^{-1})$ by  Assumption  \ref{M-Assumption E: Gamma hat}.

\subsubsection*{Verifying Assumption \ref{M-Assumption B: moment bound}}
Let
\begin{align*}
    \bar{G}_{\x,n}^{\q}(v) &= \sqrt{n a_n}\big[ \bar{\Gamma}_n(\x +v a_n^{-1} ) - \bar{\Gamma}_n(\x) - \Gamma_0(\x+v a_n^{-1}) + \Gamma_0(\x) \big]  \\
    &\quad  -\theta_0(\x) \sqrt{n a_n}\big[\bar{\Phi}_n(\x +v a_n^{-1} ) - \bar{\Phi}_n(\x) - \Phi_0(\x+v a_n^{-1}) + \Phi_0(\x) \big].
\end{align*}
From the definition,
\begin{align*}
    \widehat{G}_{\x,n}^{\q}(v) - \bar{G}_{\x,n}^{\q}(v) &= \sqrt{n a_n}\big[\widehat{\Gamma}_n(\x+va_n^{-1}) -\widehat{\Gamma}_n(\x) - \bar{\Gamma}_n(\x +va_n^{-1} ) + \bar{\Gamma}_n(\x) \big] \\
    &\quad - \theta_0(\x) \sqrt{n a_n}\big[\widehat{\Phi}_n(\x+va_n^{-1}) -\widehat{\Phi}_n(\x) - \bar{\Phi}_n(\x +va_n^{-1} ) + \bar{\Phi}_n(\x) \big] 
\end{align*}
For the two terms after the equality, Assumption \ref{M-Assumption E: Gamma hat} and \ref{M-Assumption E: Phi hat} imply that for $V \in [1,a_n \delta]$,
\[\sqrt{n a_n}\sup_{|v|\in [V,2V]}\vert\widehat{\Gamma}_n(\x+v a_n^{-1}) -\widehat{\Gamma}_n(\x) - \bar{\Gamma}_n(\x +v a_n^{-1}) + \bar{\Gamma}_n(\x) \vert \leq (2V)^{\beta} a_n^{-\beta}B_n+A_n\]
and
\[\sqrt{n a_n}\sup_{|v|\in [V,2V]}\vert\widehat{\Phi}_n(\x+v a_n^{-1}) -\widehat{\Phi}_n(\x) - \bar{\Phi}_n(\x +v a_n^{-1}) + \bar{\Phi}_n(\x) \vert \leq (2V)^{\beta} a_n^{-\beta}B_n+A_n,\]
where $\beta=\max(\beta_\gamma,\beta_\phi),A_n=\max(A_{\gamma,n},A_{\phi,n})$, and $B_n=\max(B_{\gamma,n},B_{\phi,n})$. Since $A_n=o_{\mathbb{P}}(1)$, $a_n^{-\beta}B_n=o_{\mathbb{P}}(1)$, and $A_n$ and $B_n$ are independent of $V$, there exists $\eta_n'=o(1)$ such that
\begin{equation*}
    \lim_{n\to\infty} \P\left[\bigcap_{V  \in [1,a_n\delta]} \left\{ \sup_{\vert v\vert\in [V,2V]}\vert\widehat{G}_{\x,n}^{\q}(v) - \bar{G}_{\x,n}^{\q}(v) \vert \leq [V^{\beta}+1]\eta_n' \right\}\right] =1
\end{equation*}
holds, and we take the event in the display to be $\mathcal{A}_n$. Also, $\E[\sup_{\vert v\vert\leq V}\vert\bar{G}_{\x,n}^{\q}(v)\vert]\leq C \sqrt{V}$ for $V \in (0,a_n\delta]$ follows from Corollary 4.3 of \cite{Pollard_1989_SS} and $\limsup_{\eta \downarrow 0}\E[\bar{D}_{\gamma}^{\eta}(\mathbf{Z})^2/\eta] + \E[\bar{D}_{\phi}^{\eta}(\mathbf{Z})^2/\eta] < \infty$. Then, $\E[\sup_{\vert v\vert \in [V,2V]}\vert \widehat{G}_{\x,n}^{\q}(v)\vert\1_{\mathcal{A}_n}]\leq C \sqrt{V} + [V^{\beta}+1]\eta_n'$ holds, which implies
\begin{equation*}
    \sup_{V\in[1,a_n\delta]}\mathbb{E}\left[V^{-\beta} \sup_{|v|\in [V,2V] } \vert \widehat{G}_{\x,n}^{\q}(v)\vert \1_{\mathcal{A}_n}\right] = O(1).
\end{equation*}

For the bootstrap counterpart, let
\begin{align*}
    \bar{G}_{\x,n}^{\q,*}(v) &= \sqrt{n a_n}\big[ \bar{\Gamma}_n^*(\x +v a_n^{-1} ) - \bar{\Gamma}_n^*(\x) - \bar{\Gamma}_n(\x +v a_n^{-1} ) + \bar{\Gamma}_n(\x) \big]  \\
    &\quad  -\theta_0(\x) \sqrt{n a_n}\big[\bar{\Phi}_n^*(\x +v a_n^{-1} ) - \bar{\Phi}_n^*(\x) - \bar{\Phi}_n(\x +v a_n^{-1} ) + \bar{\Phi}_n(\x) \big]
\end{align*}
and we have
\begin{align}
\label{Appendix eq: verifying bootstrap rate of convergence}
    &\widehat{G}_{\x,n}^{\q,*}(v) - \bar{G}_{\x,n}^{\q,*}(v) \\
\nonumber
    &= \sqrt{n a_n}\big[\widehat{\Gamma}_n^*(\x+va_n^{-1}) -\widehat{\Gamma}_n^*(\x) - \bar{\Gamma}_n^*(\x +va_n^{-1}) + \bar{\Gamma}_n^*(\x) \big] \\
\nonumber
    & - \widehat{\theta}_n(\x)\sqrt{n a_n}\big[\widehat{\Phi}_n^*(\x+va_n^{-1}) -\widehat{\Phi}_n^*(\x) - \bar{\Phi}_n^*(\x +va_n^{-1}) + \bar{\Phi}_n^*(\x) \big] \\
\nonumber
    & + \sqrt{n a_n}\big[ \bar{\Gamma}_n(\x +va_n^{-1}) - \bar{\Gamma}_n(\x) - \widehat{\Gamma}_n(\x+va_n^{-1}) + \widehat{\Gamma}_n(\x) \big] \\
\nonumber
    & - \widehat{\theta}_n(\x)\sqrt{n a_n}\big[ \bar{\Phi}_n(\x +va_n^{-1}) - \bar{\Phi}_n(\x) - \widehat{\Phi}_n(\x+va_n^{-1}) + \widehat{\Phi}_n(\x) \big] \\
\nonumber
    & +\sqrt{a_n}[\widehat{\theta}_n(\x) - \theta_0(\x)] \sqrt{n} \big[\bar{\Phi}_n^*(\x + va_n^{-1}) - \bar{\Phi}_n^*(\x) - \bar{\Phi}_n(\x +va_n^{-1}) + \bar{\Phi}_n(\x) \big].
\end{align}
The last term is $o_{\P}(1)$ uniformly over $\vert v\vert \leq a_n \delta$ as $\sqrt{a_n}[\widehat{\theta}_n(\x) - \theta_0(\x)]=o_{\P}(1)$ and $\sqrt{n} \sup_{\vert v\vert\leq \delta}\vert\bar{\Phi}_n^*(\x + v) - \bar{\Phi}_n(\x +v ) \vert=O_{\P}(1)$.
For the first term after the equality in \eqref{Appendix eq: verifying bootstrap rate of convergence},
\begin{align*}
    & \sqrt{na_n}\big[\widehat{\Gamma}_n^*(\x+va_n^{-1}) -\widehat{\Gamma}_n^*(\x) - \bar{\Gamma}_n^*(\x +v a_n^{-1}) + \bar{\Gamma}_n^*(\x) \big]\\
    &= \frac{1}{\sqrt{n}}\sum_{i=1}^n W_{i,n}\Breve{\gamma}_n(va_n^{-1};\mathbf{Z}_i) + \sqrt{na_n} \big[\check{\Gamma}_n(\x+va_n^{-1}) - \check{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+va_n^{-1}) + \bar{\Gamma}_n(\x)\big]
\end{align*}
where $\Breve{\gamma}_n(v;\mathbf{Z}) = \sqrt{a_n}\{\widehat{\gamma}_n(\x+v;\mathbf{Z}) - \widehat{\gamma}_n(\x;\mathbf{Z}) - \gamma_0(\x+v;\mathbf{Z}) + \gamma_0(\x;\mathbf{Z}) - [\check{\Gamma}_n(\x+v) - \check{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+v) + \bar{\Gamma}_n(\x)]\}$ and the  second part satisfies $\sqrt{n a_n}\sup_{|v|\leq V } \vert \check{\Gamma}_n(\x+va_n^{-1}) - \check{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+va_n^{-1}) + \bar{\Gamma}_n(\x)\vert =[1+ V^{\beta}]o_{\mathbb{P}}(1)$ uniformly over $V\in [1,a_n\delta]$ by Assumption \ref{M-Assumption E: Gamma hat}. For $\frac{1}{\sqrt{n}}\sum_{i=1}^n W_{i,n}\Breve{\gamma}_n(va_n^{-1};\mathbf{Z}_i)$, we apply Lemma 3.6.7 of \cite{vanderVaart-Wellner_1996_Book} as we did above to verify Assumption \ref{M-Assumption B: weak convergence}: for $n_0\in \{1,\dots, n\}$,
\begin{align}\nonumber
    &\E\left[ \sup_{\vert v\vert\leq \delta} \left\vert \frac{1}{\sqrt{n}}\sum_{i=1}^nW_{i,n}\Breve{\gamma}_n(v;\mathbf{Z}_i) \right\vert \bigg\vert \{\mathbf{Z}_i\}_{i=1}^n \right] \\
    \label{Appendix eq: verifying bootstrap moment bound}
    &\leq C \frac{1}{n}\sum_{i=1}^n \sup_{\vert v\vert \leq \delta} \vert\Breve{\gamma}_n(v;\mathbf{Z}_i)\vert \frac{n_0}{\sqrt{n}} \E\Big[\max_{1\leq i\leq n} \vert W_{i,n}\vert\Big] + C \max_{n_0\leq k\leq n}\E^* \sup_{\vert v\vert\leq \delta} \bigg\vert \frac{1}{\sqrt{k}} \sum_{i=1}^k \Breve{\gamma}_n(v;\mathbf{Z}_i^*)\bigg\vert
\end{align}
where $C>0$ is a fixed constant, $\E^*$ is the expectation under the empirical cdf measure, and $\{\mathbf{Z}_i^*\}_{i=1}^n$ is a random sample from the empirical cdf. Let $n_0$ be a diverging sequence (dependent on $n$) such that $n_0 n^{\mathfrak{r}}\sqrt{a_n/n} =o(1)$. The first term in \eqref{Appendix eq: verifying bootstrap moment bound} is bounded by \begin{equation*}
    C\frac{1}{n}\sum_{i=1}^n \sup_{\vert x-\x\vert\leq \delta} \vert \widehat{\gamma}_n(x;\mathbf{Z}_i) - \gamma_0(x;\mathbf{Z}_i) \vert + C\sup_{\vert v\vert\leq \delta } \vert \check{\Gamma}_n(\x+v) - \bar{\Gamma}_n(\x+v)  \vert =o_{\P}(1).
\end{equation*}
By Corollary 4.3 of \cite{Pollard_1989_SS}, the second term in \eqref{Appendix eq: verifying bootstrap moment bound} is bounded by (up to a constant)
\begin{equation*}
    \sqrt{a_n}\bigg\vert \frac{1}{n}\sum_{i=1}^n \sup_{x\in I_{\x}^{\delta} } \vert \widehat{\gamma}_n(x;\mathbf{Z}_i)- \gamma_0(x;\mathbf{Z}_i) \vert^2\bigg\vert^{1/2} 
\end{equation*}
which is $o_{\mathbb{P}}(1)$ by Assumption \ref{M-Assumption E: Gamma hat}.
Then, there exists a sequence of random variables $A_n'=o_{\mathbb{P}}(1)$ such that for $V\in [1,a_n\delta]$,
\begin{equation*}
    \sqrt{n a_n}\sup_{|v|\leq [V,2V]} \Big\vert\widehat{\Gamma}_n^*(\x+va_n^{-1}) -\widehat{\Gamma}_n^*(\x) - \bar{\Gamma}_n^*(\x +va_n^{-1}) + \bar{\Gamma}_n^*(\x)\Big\vert \leq  [V^{\beta}+1]A_n',
\end{equation*}
and by identical arguments, an analogous bound holds for the second term after the inequality in \eqref{Appendix eq: verifying bootstrap rate of convergence}. Then, there exists $\eta_n'=o(1)$ and events $\mathcal{A}_n$ such that $\lim_{n\to\infty}\mathbb{P}[\mathcal{A}_n]=1$ and
\begin{equation*}
    \mathbb{E}\left[\sup_{|v|\in [V,2V]} \big\vert\widehat{G}_{\x,n}^{\q,*}(v) - \bar{G}_{\x,n}^{\q,*}(v)\big\vert\1_{\mathcal{A}_n}\right]\leq [V^{\beta}+1]\eta_n'
\end{equation*}
for any $V\in [1,a_n\delta]$, where bounds for the third and fourth terms after the equality in \eqref{Appendix eq: verifying bootstrap rate of convergence} were derived in the non-bootstrap case. Finally, the bound $\E[\sup_{\vert v\vert\in [V,2V] }\vert\bar{G}_{\x,n}^{\q,*}(v)\vert]\leq C\sqrt{V}$ holds by a similar argument to the stochastic equicontinuity, and the desired result follows.

\subsubsection*{Verifying Assumptions  \ref{M-Assumption B: uniform consistency Gamma}-\ref{M-Assumption B: uniform convergence Phi}}
\begin{equation*}
    \sup_{x\in I}\big|\widehat{\Gamma}_n(x)-\Gamma_0(x)\big|\leq \sup_{x\in I}\big|\widehat{\Gamma}_n(x)-\bar{\Gamma}_n(x)\big| + \sup_{x\in I}\big|\bar{\Gamma}_n(x)-\Gamma_0(x)\big|
\end{equation*}
where the first term after the inequality is assumed to be $o_{\P}(1)$ and the second term is $o_{\P}(1)$ by standard arguments.
The identical argument implies $\sup_{x\in I}|\widehat{\Phi}_n(x)-\Phi_0(x)|=o_{\P}(1)$. By adding and subtracting,
\begin{align*}
    \sup_{x\in I_{\x}^{\delta}} \big\vert \widehat{\Phi}_n(x) - \Phi_0(x) \big\vert &\leq \sup_{x\in I_{\x}^{\delta}} \big\vert \widehat{\Phi}_n(x) - \widehat{\Phi}_n(\x) - \bar{\Phi}_n(x) +\bar{\Phi}_n(\x) \big\vert \\
    &\quad +\sup_{x\in I_{\x}^{\delta}}\big\vert \bar{\Phi}_n(x)-\Phi_0(x) \big\vert + \big\vert \widehat{\Phi}_n(\x) - \bar{\Phi}_n(\x) \big\vert 
\end{align*}
where the last two terms are $o_{\mathbb{P}}(a_n^{-1})$. Assumption \ref{M-Assumption E: Phi hat} implies
\begin{equation*}
    \sup_{x\in I_{\x}^{\delta}} \big\vert \widehat{\Phi}_n(x) - \widehat{\Phi}_n(\x) - \bar{\Phi}_n(x) +\bar{\Phi}_n(\x) \big\vert\leq \big(na_n\big)^{-1/2}[ A_{\phi,n}+ \delta^{\beta_\phi} B_{\phi,n}]  = o_{\mathbb{P}}(a_n^{-1})
\end{equation*}
where the last equality uses $B_{\phi,n}=o_{\mathbb{P}}(a_n^{\beta_\phi})$ and $\beta_\phi\leq \q$.


Now we look at the bootstrap objects. For $\widehat{\Gamma}_n^*$,
\begin{equation*}
    \sup_{x\in I}\big|\widehat{\Gamma}_n^*(x)-\widehat{\Gamma}_n(x)\big| \leq \sup_{x\in I}\big|\widehat{\Gamma}_n^*(x)-\bar{\Gamma}_n^*(x)\big| + \sup_{x\in I}\big|\bar{\Gamma}_n^*(x)-\bar{\Gamma}_n(x)\big|+ \sup_{x\in I}\big|\bar{\Gamma}_n(x)-\widehat{\Gamma}_n(x)\big|
\end{equation*}
where the last term is $o_{\P}(1)$ by the hypothesis. For $\widehat{\Gamma}_n^*(x)-\bar{\Gamma}_n^*(x)$,
\begin{align*}
    \sup_{x\in I}\big|\widehat{\Gamma}_n^*(x)-\bar{\Gamma}_n^*(x)\big| &\leq \frac{1}{n}\sum_{i=1}^n |W_{i,n}| \sup_{x\in I} \big| \widehat{\gamma}_n(x;\mathbf{Z}_i) - \gamma_0(x;\mathbf{Z}_i) \big| \\
    &\leq \sqrt{\frac{1}{n}\sum_{i=1}^n |W_{i,n}|^2 \frac{1}{n}\sum_{i=1}^n  \sup_{x\in I} \big| \widehat{\gamma}_n(x;\mathbf{Z}_i) - \gamma_0(x;\mathbf{Z}_i) \big|^2 }
\end{align*}
and the last term is $o_{\P}(1)$ by the hypothesis. For $\bar{\Gamma}_n^*(x)-\bar{\Gamma}_n(x)$, using Lemma 3.6.7 of \cite{vanderVaart-Wellner_1996_Book} and the same argument as for verifying Assumption \ref{M-Assumption B: weak convergence}, it suffices to show
\begin{equation*}
    n^{-1/2} \max_{\lfloor\sqrt{n}\rfloor\leq k\leq n}\E^*\sup_{x\in I}\bigg|\frac{1}{\sqrt{k}}\sum_{i=1}^k \bar{\gamma}_n(x;\mathbf{Z}_i^*)\bigg| = o_{\P}(1)
\end{equation*}
where $\{\mathbf{Z}_i^*\}_{i=1}^k$ denotes a random sample from the empirical cdf and $\E^*$ is the expectation under this empirical bootstrap law. Corollary 4.3 of \cite{Pollard_1989_SS} implies the desired result.

For $\widehat{\Phi}_n^*$, $\sup_{x\in I}|\widehat{\Phi}_n^*(x)-\widehat{\Phi}_n(x)|=o_{\P}(1)$ follows from the same argument as for $\widehat{\Gamma}_n^*$. For $a_n \sup_{x\in I_{\x}^{\delta}}|\widehat{\Phi}_n^*(x) -\widehat{\Phi}_n(x)|=o_{\mathbb{P}}(1)$,
\begin{align*}
    \sup_{x\in I_{\x}^{\delta}}\big|\widehat{\Phi}_n^*(x) -\widehat{\Phi}_n(x)\big| &\leq \sup_{x\in I_{\x}^{\delta}}\big|\widehat{\Phi}_n^*(x) -\bar{\Phi}_n^*(x)\big| +\sup_{x\in I_{\x}^{\delta}}\big|\bar{\Phi}_n^*(x) - \bar{\Phi}_n(x)\big| \\
    &\quad + \sup_{x\in I_{\x}^{\delta}}\big| \bar{\Phi}_n(x)-\Phi_0(x)\big| + \sup_{x\in I_{\x}^{\delta}}\big| \widehat{\Phi}_n(x)-\Phi_0(x)\big|
\end{align*}
where the last term is $o_{\mathbb{P}}(a_n^{-1})$ as shown above and the second and third terms after the inequality are $O_{\mathbb{P}}(n^{-1/2})$ by standard arguments. For the remaining term,
\begin{align*}
    \sup_{x\in I_{\x}^{\delta}}\big|\widehat{\Phi}_n^*(x) -\bar{\Phi}_n^*(x)\big| &\leq \sup_{x\in I_{\x}^{\delta}}\bigg|\frac{1}{n}\sum_{i=1}^n W_{i,n}\Breve{\phi}_n(x;\mathbf{Z}_i)\bigg| + \big|\check{\Phi}_n(\x)-\bar{\Phi}_n(\x)\big| \\
    &\quad + \sup_{x\in I_{\x}^{\delta}} \big|\check{\Phi}_n(x)-\check{\Phi}_n(\x)-\bar{\Phi}_n(x)+\bar{\Phi}_n(\x)\big|
\end{align*}
where $\Breve{\phi}_n(x;\mathbf{Z}) = \widehat{\phi}_n(x;\mathbf{Z})-\phi_0(x;\mathbf{Z}) - [\check{\Phi}_n(x)-\bar{\Phi}_n(x)]$. The last two terms are $o_{\mathbb{P}}(a_n^{-1})$ by Assumption \ref{M-Assumption E: Phi hat}. Using Lemma 3.6.7 of \cite{vanderVaart-Wellner_1996_Book} and the argument similar to above, the remaining term is $o_{\mathbb{P}}(a_n^{-1})$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bootstrap inconsistency
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of Theorem \ref{M-[Theorem]: Bootstrap inconsistency}}
The proof is by contradiction and follows \cite{Kosorok_2008_BookCh}. We omit some details in cases where the arguments are almost identical to those for Theorem \ref{M-[Theorem] Main result} and Lemma \ref{M-[Lemma] Bootstrapping}.

Suppose that the bootstrap approximation is consistent; that is, suppose
\begin{equation*}
	r_n\big( \widehat{\theta}_n^*(\x)-\widehat{\theta}_n(\x) \big) \leadsto_{\P}  Y, \qquad Y=(\partial \Phi_0(\x))^{-1} \partial_{-}\GCM_{\R}\{\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q}\}(0).
\end{equation*}
Then, by Theorem 2.2 of \cite{Kosorok_2008_BookCh}, we have 
\begin{equation}\label{eqBootFail:contradiction}
	r_n\big( \widehat{\theta}_n^*(\x) - \theta_0(\x) \big) \leadsto Y_1 + Y_2 =_d \sqrt{2} Y
\end{equation}
where $=_d$ denotes the distributional equality, $Y_1$ and $Y_2$ are independent copies of $Y$, and where the convergence in distribution is unconditional.

Using the switching lemma, $\P[r_n\big( \widehat{\theta}_n^*(\x) - \theta_0(\x)\big)  > t]$ equals
\begin{equation*}
	\P\bigg[ \max\argmax_{x\in \widehat{\Phi}_n^{*-}([0,\widehat{u}_n^*]) } \Big\{   [\theta_0(\x) + r_n^{-1}t]\widehat{\Phi}_n^*(x) - \LSC_{\widehat{\Phi}_n^{*}}\big(\widehat{\Gamma}_n^*\big)(x) \Big\} < \widehat{\Phi}_n^{*-} \circ \widehat{\Phi}_n^*(\x) \bigg].
\end{equation*}
By the arguments used in the proof of Theorem \ref{M-[Theorem] Main result}, to characterize the limiting distribution of $r_n(\widehat{\theta}_n^*(\x) - \theta_0(\x))$, it suffices to look at
\begin{equation*}
    -\check{G}_{\x,n}^{\q,*}(v) - \check{M}_{\x,n}^{\q}(v) + t\widehat{L}_{\x,n}^{\q,*}(v)
\end{equation*}
where 
\begin{align*}
    \check{G}_{\x,n}^{\q,*}(v) &= \sqrt{na_n} \big[ \widehat{\Gamma}_n^*(\x+va_n^{-1})  - \widehat{\Gamma}_n^*(\x) - \check{\Gamma}_n (\x+va_n^{-1}) + \check{\Gamma}_n (\x) \big] \\
    &\quad - \theta_0(\x)\sqrt{na_n} \big[ \widehat{\Phi}_n^*(\x+va_n^{-1}) - \widehat{\Phi}_n^*(\x)-\check{\Phi}_n(\x+va_n^{-1}) + \check{\Phi}_n(\x) \big]
\end{align*}
and
\begin{equation*}
    \check{M}_{\x,n}^{\q}(v)=\sqrt{n a_n}  \big[ \check{\Gamma}_n(\x+va_n^{-1}) -\check{\Gamma}_n(\x) - \theta_0(\x)\{ \check{\Phi}_n(\x+va_n^{-1}) -\check{\Phi}_n(\x)\}  \big].
\end{equation*}
It can be shown that $\check{G}_{\x,n}^{\q,*} \leadsto_\P \mathcal{G}_{\x}, \widehat{L}_{\x,n}^{\q,*} \leadsto \mathcal{L}_{\x}$, and that $\check{M}_{\x,n}^{\q} \leadsto \mathcal{G}_{\x} + \mathcal{M}_{\x}^{\q}$. Thus, 
\begin{equation*}
	\P \big[ r_n\big( \widehat{\theta}_n^*(\x) - \theta_0(\x) \big) >t\big] \to \P\Big[ \argmin_{v\in \R} \Big\{ \mathcal{G}_{\x,1}(v) + \mathcal{G}_{\x,2}(v) + \mathcal{M}_{\x}^{\q}(v)  - t \partial\Phi_0(\x) v \Big\} < 0 \Big] 
\end{equation*}
where $\mathcal{G}_{\x,1}$ and $\mathcal{G}_{\x,2}$ are independent copies of $\mathcal{G}_{\x}$.
Noting that $\mathcal{G}_{\x}(a v)=_d \sqrt{|a|} \mathcal{G}_{\x}(v)$ and using the change of variable $v=u2^{\frac{1}{2\q+1}}$, the limit distribution equals
\begin{align*}
	&\P\Big[   2^{\frac{1}{2\q+1}} \argmin_{u\in \R} \Big\{   \mathcal{G}_{\x}(u) + \mathcal{M}_{\x}^{\q}(u) -  2^{-\frac{\q}{2\q+1}} t \partial\Phi_0(\x)u   \Big\} < 0 \Big] \\
	&= \P\Big[   2^{\frac{\q}{2\q+1}} (\partial\Phi_0(\x))^{-1}  \partial_{-} \GCM_{\R} \{\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q}\}(0) >  t  \Big].
\end{align*} 
As a consequence,
\begin{equation*}
	r_n\big( \widehat{\theta}_n^*(\x) - \theta_0(\x) \big) \leadsto 2^{\frac{\q}{2\q+1}} (\partial \Phi_0(\x))^{-1} \partial_{-}\GCM_{\R}\{\mathcal{G}_{\x}+\mathcal{M}_{\x}^{\q}\}(0),
\end{equation*}
contradicting \eqref{eqBootFail:contradiction} because $2^{\frac{\q}{2\q+1}} \neq \sqrt{2}$.

In other words, the bootstrap estimator $\widehat{\theta}_n^*(\x)$ fails to approximate the limit distribution. \hfill$\qedsymbol$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Remarks on verifying conditions in applications}\label{Appendix: section remark}

Below we verify the hypothesis of Theorem \ref{M-[Theorem] Main result} for various examples. For this purpose, one should verify Assumptions \ref{M-Assumption A}, \ref{M-Assumption B: covariance kernel}-\ref{M-Assumption B: jump of Phi hat}, and \ref{M-Assumption E} since Assumption \ref{M-Assumption E} implies \ref{M-Assumption B: weak convergence}-\ref{M-Assumption B: uniform convergence Phi} by Lemma \ref{M-[Lemma] Bootstrapping}.

When $\gamma_0$ is known, it is natural to take $\widehat{\Gamma}_n=\check{\Gamma}_n=\bar{\Gamma}_n$, in which case \ref{M-Assumption E: Gamma hat} reduces to the requirement that, for some $\rho_{\gamma}\in (0,2)$, 
\begin{equation}\label{Appendix Eq: simpler D gamma hat}
    \limsup_{\varepsilon\downarrow0}\frac{\log N_{U}(\varepsilon,\mathfrak{F}_{\gamma})}{\varepsilon^{-\rho_{\gamma}}}<\infty,\quad \E[\bar{F}_{\gamma}(\mathbf{Z})^2] <\infty,\quad \limsup_{\eta\downarrow0}\frac{\E[\bar{D}_{\gamma}^{\eta}(\mathbf{Z})^2+\bar{D}_{\gamma}^{\eta}(\mathbf{Z})^4]}{\eta}<\infty.
\end{equation} 
An identical remark applies to $\phi_0$ and \ref{M-Assumption E: Phi hat}.

In addition, as remarked in the main paper after Lemma \ref{M-[Lemma] Bootstrapping}, the second display of \ref{M-Assumption B: covariance kernel} follows from the second display of \ref{M-Assumption E: covariance kernel}, and the first display of \ref{M-Assumption B: covariance kernel} follows from 
 \begin{equation}\label{Appendix eq: uniform version of SA-5 (5)}
     \lim_{n\to\infty} \eta_n^{-1}\E[\psi_{\x_n}(s\eta_n;\mathbf{Z})\psi_{\x_n}(t\eta_n;\mathbf{Z})] = \mathcal{C}_{\x}(s,t)  
 \end{equation}
 for $a_n\eta_n=O(1)$ and any $\x_n\to \x$. To see the second claim,
 \begin{align*}
     &\eta_n^{-1} \big\{ \E[\psi_{\x}( (s+t)\eta_n;\mathbf{Z})\psi_{\x}((s+t)\eta_n;\mathbf{Z})] - 2\E[\psi_{\x}( s\eta_n;\mathbf{Z})\psi_{\x}((s+t)\eta_n;\mathbf{Z})] \\
     &\qquad + \E[\psi_{\x}( s\eta_n;\mathbf{Z})\psi_{\x}(s\eta_n;\mathbf{Z})] \big\} \to \mathcal{C}_{\x}(s+t,s+t) -  \mathcal{C}_{\x}(s+t,s) -  \mathcal{C}_{\x}(s,s+t) + \mathcal{C}_{\x}(s,s)
 \end{align*}
 and at the same time, setting $\x_n=\x+s\eta_n$,
 \begin{align*}
     &\eta_n^{-1} \big\{ \E[\psi_{\x}( (s+t)\eta_n;\mathbf{Z})\psi_{\x}((s+t)\eta_n;\mathbf{Z})] - 2\E[\psi_{\x}( s\eta_n;\mathbf{Z})\psi_{\x}((s+t)\eta_n;\mathbf{Z})] \\
     &\qquad + \E[\psi_{\x}( s\eta_n;\mathbf{Z})\psi_{\x}(s\eta_n;\mathbf{Z})] \big\}\\
     &= \eta_n^{-1} \big\{ \E[ \{\psi_{\x_n}( t\eta_n;\mathbf{Z})-\psi_{\x_n}( -s\eta_n;\mathbf{Z})\}\{\psi_{\x_n}(t\eta_n;\mathbf{Z})-\psi_{\x_n}( -s\eta_n;\mathbf{Z})\}]  \\
     &\qquad + 2\E[\psi_{\x_n}( -s\eta_n;\mathbf{Z})\{\psi_{\x_n}(t\eta_n;\mathbf{Z})-\psi_{\x_n}( -s\eta_n;\mathbf{Z})\}] + \E[\psi_{\x_n}( -s\eta_n;\mathbf{Z})\psi_{\x_n}( -s\eta_n;\mathbf{Z})] \big\} \\
     &= \eta_n^{-1} \E[ \psi_{\x_n}( t\eta_n;\mathbf{Z})\psi_{\x_n}(t\eta_n;\mathbf{Z})]\to \mathcal{C}_{\x}(t,t) 
 \end{align*}
and thus, $\mathcal{C}_{\x}(s+t,s+t) -  \mathcal{C}_{\x}(s+t,s) -  \mathcal{C}_{\x}(s,s+t) + \mathcal{C}_{\x}(s,s)=\mathcal{C}_{\x}(t,t)$ holds.
Thus, for the two displays in \ref{M-Assumption B: covariance kernel}, it suffices to check \ref{M-Assumption E: covariance kernel} and \eqref{Appendix eq: uniform version of SA-5 (5)}.

\subsection{Proof of Corollary \ref{M-[Corollary] monotone density without censoring}}
Assumption \ref{M-Assumption A} and \ref{M-Assumption E: iid}-\ref{M-Assumption E: bootstrap weights} follow from the hypothesis. 

\paragraph*{\ref{M-Assumption E: Gamma hat}}
In this example, $\gamma_0(x;\mathbf{Z})=\1\{X\leq x\}$ is known, so it suffices to verify \eqref{Appendix Eq: simpler D gamma hat}. The uniform covering number of $\{\1\{\cdot\leq x\} :x\in\R\}$ grows linearly, and an envelope function can be taken to be $1$. For an envelope function of $\{\1\{\cdot\leq x\}-\1\{\cdot\leq \x\} : |x-\x|\leq \eta \}$, we can take $\1\{-\eta +\x\leq \cdot\leq \x+ \eta \}$ and the moment bound is satisfied as $\E[\1\{-\eta +\x\leq X\leq \x+ \eta \}]\leq C \eta$.\smallskip
\newline 
\ref{M-Assumption E: Phi hat} trivially holds as $\widehat{\Phi}_n(x)=\widehat{\Phi}_n^*(x)=x$.

\paragraph*{\ref{M-Assumption E: covariance kernel}}
Here $\psi_{\x}(v;\mathbf{Z})=\1\{X\leq \x+v\}-\1\{X\leq\x\} -\Phi_0(\x)v$. Then,
\begin{equation*}
    \frac{\E[|\psi_{\x}(v;\mathbf{Z}) - \psi_{\x}(v';\mathbf{Z})|]}{|v-v'|}\leq \frac{\E[\1\{\x + v \land v'<X\leq \x + v \lor v'\}]}{|v-v'|} +f_0(\x) \leq C.
\end{equation*}
Also, $\psi_{\x_n}(s\eta_n;\mathbf{Z})=\1\{\x_n \land (\x_n+s\eta_n)\}< X\leq \x_n \lor (\x_n+s\eta_n) \} - f_0(\x) s\eta_n$ and
\begin{align*}
    \psi_{\x_n}(s\eta_n;\mathbf{Z})\psi_{\x_n}(t\eta_n;\mathbf{Z})&= \1\{ \x_n < X\leq \x_n + \eta_n (s \land t) \}\1\{s>0,t>0\}\\
    &\quad + \1\{ \x_n + (s \lor t) < X\leq \x_n \}\1\{s<0,t<0\} \\
    &\quad - \1\{\x_n \land (\x_n+s\eta_n) < X\leq \x_n \lor (\x_n+s\eta_n) \} f_0(\x_n) t\eta_n\\
    &\quad - \1\{\x_n \land (\x_n+t\eta_n) < X\leq \x_n \lor (\x_n+t\eta_n) \} f_0(\x_n) s\eta_n\\
    &\quad +f_0(\x_n)^2 st \eta_n^2.
\end{align*}
Then, for any $s,t\in\R$ and $\x_n\to\x$, using continuity of $f_0$ at $\x$,
\begin{align*}
    &\eta_n^{-1}\E[\psi_{\x_n}(s\eta_n;\mathbf{Z})\psi_{\x_n}(t\eta_n;\mathbf{Z})]\\ &= \eta_n^{-1} \int_{\x_n}^{\x_n + \eta_n (s \land t)} f_0(u)du \1\{s>0,t>0\}  \\
    &\quad +\eta_n^{-1} \int_{\x_n + \eta_n (s \lor t)}^{\x_n} f_0(u)du \1\{s<0,t<0\} + o(1)\\
    &= f_0(\x) [(s \land t)\1\{s>0,t>0\} - (s \lor t)\1\{s<0,t<0\}] + o(1) \\
    &= f_0(\x) (|s| \land |t|)\1\{\mathrm{sign}(s)=\mathrm{sign}(t)\} + o(1).
\end{align*}

\paragraph*{\ref{M-Assumption B: covariance kernel}} 
It is clear that $\mathcal{C}_{\x}(1,1)>0$ from $f_0(\x)>0$. Also, $\mathcal{C}_{\x}(1,\eta)/\sqrt{\eta}=f_0(\x) \sqrt{\eta} \1\{\eta >0 \}$ for $|\eta| <1$ and $\limsup_{\eta\downarrow0}\mathcal{C}_{\x}(1,\eta)/\sqrt{\eta}=0$ holds. The remaining conditions follow from verifying \ref{M-Assumption E: covariance kernel} above.\smallskip
\newline
\ref{M-Assumption B: uhat} holds since $\widehat{u}_n=\widehat{u}_n^*$ converges in probability to $u_0$, the supremum of the support of $X$ by i.i.d.\ assumption.\smallskip
\newline
\ref{M-Assumption B: Phi uhat} and \ref{M-Assumption B: jump of Phi hat} hold trivially since $\widehat{\Phi}_n$ and $\widehat{\Phi}_n^*$ are the identity map.\smallskip
\newline
Assumption \ref{M-Assumption D} follows from \ref{M-Assumption E: Gamma hat} and empirical process theory arguments.


\subsection{Proof of Corollary \ref{M-[Corollary] monotone density independent right-censoring}}
\label{Appendix subsection: proof of Theorem monotone density independent right-censoring}
Assumption \ref{M-Assumption A} and \ref{M-Assumption E: iid}-\ref{M-Assumption E: bootstrap weights} follow from the hypothesis. 





 
\paragraph*{\ref{M-Assumption E: Gamma hat}}
We have $\widehat{\Gamma}_n=1-\widehat{S}_n$ with $\widehat{S}_n$ the Kaplan-Meier estimator.
By Theorem 1 of \cite{Lo-Singh_1986_PTRF},
\begin{equation*}
    \sup_{x\in I}\bigg|\widehat{\Gamma}_n(x) -\frac{1}{n}\sum_{i=1}^n \gamma_0(x;\mathbf{Z}) \bigg| = O_{\P}\Big( \Big|\frac{\log n}{n}\Big|^{3/4}\Big).
\end{equation*}
Since $\sqrt{n a_n} \leq n^{2/3}$ for $\q\geq 1$, $\sup_{x \in I}|\widehat{\Gamma}_n(x)-\Gamma_0(x)|=o_{\P}(1)$ and $\sqrt{na}\sup_{|v|\leq \delta}|\widehat{\Gamma}_n(\x+v)-\widehat{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+v)+\bar{\Gamma}_n(\x)|=o_{\P}(1)$ hold. 

We have
\begin{align*}
    &\widehat{\gamma}_n(x;\mathbf{Z}) - \gamma_0(x;\mathbf{Z}) \\
    &= \widehat{F}_n(x) -F_0(x) + \big[\widehat{S}_n(x)-S_0(x)\big]\bigg[ \frac{\1\{\check{X}\leq x\}\Delta}{\widehat{S}_n(\check{X})\widehat{G}_n(\check{X})} -\int_0^{\check{X}\land x} \frac{\widehat{\Lambda}_n(du)}{\widehat{S}_n(u)\widehat{G}_n(u)} \bigg] \\
    &\quad + S_0(x)\1\{\check{X}\leq x\}\Delta\frac{S_0(\check{X})G_0(\check{X})-\widehat{S}_n(\check{X})\widehat{G}_n(\check{X})}{S_0(\check{X})G_0(\check{X})\widehat{S}_n(\check{X})\widehat{G}_n(\check{X})}\\
    &\quad - S_0(x) \int_0^{\check{X}\land x} \frac{S_0(u)G_0(u)-\widehat{S}_n(u)\widehat{G}_n(u)}{S_0(u)G_0(u)\widehat{S}_n(u)\widehat{G}_n(u)}\widehat{\Lambda}_n(du)  - S_0(x)\int_0^{\check{X}\land x} \frac{[\widehat{\Lambda}_n-\Lambda_0](du)}{S_0(u)G_0(u)}.
\end{align*}
By $S_0(u_0)G_0(u_0)>0$, we have $\sqrt{n}\sup_{x\in I}|\widehat{S}_n(x)-S_0(x)|=O_{\P}(1)$, $\sqrt{n}\sup_{x\in I}|\widehat{G}_n(x)-G_0(x)|=O_{\P}(1)$, and $\sqrt{n}\sup_{x\in I}|\widehat{\Lambda}_n(x)-\Lambda_0(x)|=O_{\P}(1)$, which in turn implies
\begin{equation*}
    \frac{1}{n}\sum_{i=1}^n \sup_{x\in I}\big|\widehat{\gamma}_n(x;\mathbf{Z}_i)-\gamma_0(x;\mathbf{Z}_i)\big|^2=o_{\P}(1),\ a_n \frac{1}{n}\sum_{i=1}^n \sup_{x\in I_{\x}^{\delta}}\big|\widehat{\gamma}_n(x;\mathbf{Z}_i)-\gamma_0(x;\mathbf{Z}_i)\big|^2=o_{\P}(1).
\end{equation*}

Let $\delta = \min\{\x, (u_0-\x)\}/4$, $R_{1n}(v)=\vert \widehat{F}_n(\x+v)-\widehat{F}_n(\x) - F_0(\x+v)+F_0(\x)\vert$, $R_{2n}=\vert \widehat{F}_n(\x)-F_0(\x)\vert$, $R_{3n}= \sup_{x\in I}\vert [\widehat{S}_n(x)\widehat{G}_n(x)]^{-1} - [S_0(x)G_0(x)]^{-1} \vert$, and $R_{4n}(x_1,x_2) = \vert\int_{x_1}^{x_2} \frac{\widehat{\Lambda}_n(du)}{\widehat{S}_n(u)\widehat{G}_n(u)} -\int_{x_1}^{x_2} \frac{\Lambda_0(du)}{S_0(u)G_0(u)}\vert $. For $|v|\leq \delta$,
\begin{align*}
    &\big\vert\check{\Gamma}_n(\x+v)-\check{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+v) + \bar{\Gamma}_n(\x)\big\vert \\
    &\leq R_{1n}(v) \left[1 + \frac{1 + \widehat{\Lambda}_n(x) }{\widehat{S}_n(x)\widehat{G}_n(x)}\right] + R_{2n}\big[\widehat{S}_n(\x-\delta)\widehat{G}_n(\x-\delta) \big]^{-1} \frac{1}{n}\sum_{i=1}^n \1\{|X_i-\x|\leq v \}\\
    &+ R_{2n} \int_{\x-v}^{\x+v} \frac{\widehat{\Lambda}_n(du)}{\widehat{S}_n(u)\widehat{G}_n(u)} + \sup_{x\in I_{\x}^{\delta}} f_0(x)\vert v \vert \bigg[R_{3n} +  \frac{1}{n}\sum_{i=1}^n R_{4n}(0,\check{X}_i\land (\x+v) )   \bigg] \\
    & +  R_{3n} \frac{1}{n}\sum_{i=1}^n \1\{|X_i-\x|\leq v \} + \frac{1}{n}\sum_{i=1}^n R_{4n}(\check{X}_i\land\x,\check{X}_i\land (\x+v) ).
\end{align*}
As noted above, $\sup_{|v|\leq \delta}R_{1n}(v)=o_{\mathbb{P}}( (na_n)^{-1/2})$, $R_{2n}=O_{\mathbb{P}}(n^{-1/2})$, and $R_{3n}=O_{\mathbb{P}}(n^{-1/2})$ by $S_0(u_0)G_0(u_0)>0$.
Also, uniformly over $V\in (0, 2\delta]$, $\sup_{\vert v\vert\leq V}\vert \frac{1}{n}\sum_{i=1}^n  (\1\{X_i\leq \x +v\} - \1\{X_i\leq \x\} )\vert \leq C V + O_{\P}(n^{-1/2})$. If 
\begin{equation}\label{Appendix: Example monotone density right-censoring R3 1}
    \sup_{|v|\leq \delta}\frac{1}{n}\sum_{i=1}^n R_{4n}(0,\check{X}_i\land (\x+v) )=O_{\mathbb{P}}(n^{-1/2})
\end{equation}
and 
\begin{equation}\label{Appendix: Example monotone density right-censoring R3 2}
    \frac{1}{n}\sum_{i=1}^n R_{4n}(\check{X}_i\land\x,\check{X}_i\land (\x+v) ) \leq |v| O_{\mathbb{P}}(n^{-1/2}) + o_{\mathbb{P}}\Big(( na_n)^{-1/2} \Big)
\end{equation}
uniformly over $|v|\leq 2\delta$, then there exists random variables $A_n=o_{\mathbb{P}}(1)$ and $B_n=O_{\mathbb{P}}(\sqrt{a_n})$ independent of $v$ such that for $V\in (0, 2\delta]$,
\begin{equation*}
    \sqrt{na_n} \sup_{|v|\leq V} \big\vert\check{\Gamma}_n(\x+v)-\check{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+v) + \bar{\Gamma}_n(\x)\big\vert \leq A_n + V B_n
\end{equation*}
i.e.,\ $\beta_{\gamma}=1$ in the notation of \ref{M-Assumption E: Gamma hat}. To show \eqref{Appendix: Example monotone density right-censoring R3 1} and \eqref{Appendix: Example monotone density right-censoring R3 2}, for $x_1,x_2\in I$,
\begin{equation*}
    R_{4n}(x_1,x_2) \leq R_{3n}\big\vert \widehat{\Lambda}_n(x_2)- \widehat{\Lambda}_n(x_1)\big\vert  + \left\vert \int_{x_1}^{x_2}\frac{[\widehat{\Lambda}_n-\Lambda_0](du)}{S_0(u)G_0(u)}\right\vert.
\end{equation*}
Let $J_0(u)= 1/[S_0(u)G_0(u)]$ and integration by parts implies
\begin{align*}
    \int_{x_1}^{x_2} \frac{[\widehat{\Lambda}_n-\Lambda_0](du)}{S_0(u)G_0(u)} &=  \frac{\widehat{\Lambda}_n(x_2)-\Lambda_0(x_2)}{S_0(x_2)G_0(x_2)} - \frac{\widehat{\Lambda}_n(x_1)-\Lambda_0(x_1)}{S_0(x_1)G_0(x_1)} - \int_{x_1}^{x_2} [\widehat{\Lambda}_n(u)-\Lambda_0(u)]J_0(du) \\
    &= [\widehat{\Lambda}_n(x_2)-\Lambda_0(x_2)]\left[\frac{1}{S_0(x_2)G_0(x_2)}-\frac{1}{S_0(x_1)G_0(x_1)}\right] \\
    & + \frac{\widehat{\Lambda}_n(x_2)-\Lambda_0(x_2) -\widehat{\Lambda}_n(x_1)+\Lambda_0(x_1)}{S_0(x_1)G_0(x_1)}  - \int_{x_1}^{x_2} [\widehat{\Lambda}_n(u)-\Lambda_0(u)]J_0(du).
\end{align*}
The first term after the second equality is bounded by $O_{\P}(n^{-1/2}) \vert x_2-x_1\vert$. For the second term, Theorem 1 of \cite{Burketal} implies that on a suitable probability space there exists a sequence of standard Brownian motion $W_n$ such that $\sqrt{a_n}\sup_{\vert x_1-x_2\vert\leq v} \vert \sqrt{n}[\widehat{\Lambda}_n(x_1)-\Lambda_0(x_2) -\widehat{\Lambda}_n(x_1)+\Lambda_0(x_2)] - W_n(d(x_1))+W_n(d(x_2))\vert = o_{\P}(1)$, where $d(x)=\int_0^x \frac{F_0(du)}{S_0(u)^2G_0(u)}$. By Theorem 3.2 of \cite{Pollard_1989_SS}, there is some fixed constant $C>0$ such that $\E[\sup_{\vert x_1-x_2\vert\leq v}\vert W_n(d(x_1))-W_n(d(x_2))\vert]\leq C v$. Finally, $\vert \int_{x_1}^{x_2} [\widehat{\Lambda}_n(u)-\Lambda_0(u)]J_0(du)\vert\leq \sup_{x'\in [x_1,x_2]}\vert\widehat{\Lambda}_n(x')-\Lambda_0(x')\vert [J_0(x_2)-J_0(x_1)]\leq O_{\P}(n^{-1/2}) \vert x_2-x_1\vert$. Thus, \eqref{Appendix: Example monotone density right-censoring R3 1} and \eqref{Appendix: Example monotone density right-censoring R3 2} hold.

For the function class $\mathfrak{F}_{\gamma}$, we can take $\bar{F}_{\gamma}(\mathbf{Z}) = 1 + [S_0(u_0)G_0(u_0)]^{-1}[1 + \Lambda_0(u_0)]$ as a constant envelope. 
For the function class $\{S_0(x):x\in I\}$, given $m\in\mathbbm{N}$, there exists $\{x_1,\dots, x_{m+1}\}\subset I$ such that $\sup_{x\in I}\min_{l=1,\dots,m+1}|S_0(x_l)-S_0(x)|\leq 1/m$, which implies the uniform covering number is bounded by a linear function. The covering numbers of $\{\1\{\cdot\leq s\}:s\in I\}$ and $\{\int_0^{\cdot\land s}[S_0(u)G_0(u)]^{-1}\Lambda_0(du):s\in I\}$ are also bounded by a linear function. By Lemma 5.1 of \cite{vanderVaart-vanderLaan_2006_IJB}, there exists $\rho\in (0,2)$ such that $\limsup_{\eta\downarrow 0}\log N_U(\eta,\mathfrak{F}_{\gamma})\eta^{\rho} < \infty$ holds.

Now consider the uniform covering number of $\hat{\mathfrak{F}}_{\gamma}$. Given a realization of $(\widehat{S}_n,\widehat{G}_n)$, the mapping $x\mapsto\int_0^{x\land s}[\widehat{S}_n(u)\widehat{G}_n(u)]^{-1}\widehat{\Lambda}_n(du)$ is a composition of $x\mapsto x\land s$ and $x\mapsto \int_0^x [\widehat{S}_n(u)\widehat{G}_n(u)]^{-1}\widehat{\Lambda}_n(du)$. The latter mapping is monotone, and the first mapping is a VC-subgraph class, and Lemma 2.6.18 of \cite{vanderVaart-Wellner_1996_Book} implies $\{\int_0^{\cdot\land s}[\widehat{S}_n(u)\widehat{G}_n(u)]^{-1}\widehat{\Lambda}_n(du):s\in I\}$ is a VC-subgraph class. Note that since $S_0,G_0$ are bounded away from zero, $\widehat{S}_n,\widehat{G}_n$ are bounded away from zero with probability approaching one. Thus, for some $\rho\in (0,2)$, $\limsup_{\eta\downarrow 0}\log N_U(\eta,\hat{\mathfrak{F}}_{\gamma})\eta^{\rho} = O_{\P}(1)$ holds.

For $s\leq t\in I$,
\begin{equation*}
    |\gamma_0(s;\mathbf{Z})-\gamma_0(t;\mathbf{Z})|\leq C|F_0(s)-F_0(t)| + C|\1\{\check{X}\leq s\}-\1\{\check{X}\leq t\}|\Delta + \int_{\check{X}\land s}^{\check{X}\land t}\frac{\Lambda_0(du)}{S_0(du)G_0(du)}
\end{equation*}
and we can take $D_{\gamma}^{\eta}(\mathbf{Z})$ to be a constant multiple of $\sup_{|s|\leq \eta}|F_0(\x +s)-F_0(\x)| + \Delta \1\{|\check{X}-\x|\leq \eta\} + \int_{ x-\eta}^{x+\eta}\Lambda_0(du)/S_0(u)G_0(u)$. For $\eta >0$ small enough, there is some fixed $C>0$ with
\begin{equation*}
    \E[D_{\gamma}^{\eta}(\mathbf{Z})^2 + D_{\gamma}^{\eta}(\mathbf{Z})^4] \leq C f_0(\x+\eta) \eta.
\end{equation*}
\ref{M-Assumption E: Phi hat} trivially holds as $\widehat{\Phi}_n(x)=\widehat{\Phi}_n^*(x)=x$.

\paragraph*{\ref{M-Assumption E: covariance kernel}}
We have
\begin{equation*}
    \psi_{\x}(v;\mathbf{Z}) = S_0(\x)\frac{(\1\{\check{X}\leq \x +v\}-\1\{\check{X}\leq \x\})\Delta}{S_0(\check{X})G_0(\check{X})}  + O(|v|)
\end{equation*}
where $O(|v|)$ is uniformly over small enough $|v|$. Since
\begin{equation*}
    \E[|\1\{\check{X}\leq \x+v\} - \1\{\check{X}\leq \x+v'\}|\Delta]=\int_{\x + v\land v'}^{\x+v\lor v'} G_0(u) f_0(u)du \leq C|v-v'|,
\end{equation*}
the first display in \ref{M-Assumption E: covariance kernel} is satisfied. For the covariance kernel,
\begin{align*}
    &\E[\psi_{\x_n}(s\eta_n;\mathbf{Z})\psi_{\x_n}(t\eta_n;\mathbf{Z})] \\
    &= S_0(\x_n)^{2}\bigg( \int_{\x_n}^{\x_n + \eta_n(s \land t)}\frac{f_0(u)}{S_0(u)^2G_0(u)}du  \1\{s>0,t>0\}\\
    &\qquad\qquad+\int_{\x_n + \eta_n(s \lor t)}^{\x_n}\frac{f_0(u)}{S_0(u)^2G_0(u)}du\1\{s<0,t<0\} \bigg)+O(\eta_n^2)\\
    &= \frac{S_0(\x_n)^{2}f_0(\x)}{S_0(\x)^2G_0(\x)} (s \land t)\eta_n \1\{s>0,t>0\} \\
    &\quad - \frac{S_0(\x_n)^{2}f_0(\x)}{S_0(\x)^2G_0(\x)} (s \lor t)\eta_n \1\{s<0,t<0\} + o(\eta_n)
\end{align*}
where the last equality uses continuity of $(S_0,G_0,f_0)$ at $\x$ i.e.,\ $\int_{\x_n}^{\x_n +\eta_n} [\frac{f_0(u)}{S_0(u)^2G_0(u)}-\frac{f_0(\x)}{S_0(\x)^2G_0(\x)}]du =o(1)\eta_n$. Thus, \eqref{Appendix eq: uniform version of SA-5 (5)} holds.

\paragraph*{\ref{M-Assumption B: covariance kernel}}
 $\mathcal{C}_{\x}(1,1)>0$ follows from $f_0(\x)>0$. $\lim_{\eta\downarrow0}\mathcal{C}_{\x}(1,\eta)/\sqrt{\eta}=0$ follows from the same computation as in the no censoring case. The remaining conditions follow from verifying \ref{M-Assumption E: covariance kernel} above.\smallskip
\newline
\ref{M-Assumption B: uhat}, \ref{M-Assumption B: Phi uhat}, and \ref{M-Assumption B: jump of Phi hat} hold since in this example, $\widehat{u}_n=\widehat{u}_n^*=u_0$ and $\widehat{\Phi}_n,\widehat{\Phi}_n^*$ are the identity map.

\paragraph*{Assumption \ref{M-Assumption D}}
As noted when verifying \ref{M-Assumption E: Gamma hat}, $\widehat{G}_n(1;\eta_n)=o_{\mathbb{P}}(1)$ for any $\eta_n=o(1)$ with $a_n^{-1}\eta_n^{-1}=O(1)$. $\widehat{\Phi}_n=\Phi_0$ is the identity map and the desired result holds.

\subsection{Proof of Corollary \ref{M-[Corollary] classical monotone regression}}\label{Appendix subsection: proof theorem classical monotone regression}
Assumption \ref{M-Assumption A} and \ref{M-Assumption E: iid}-\ref{M-Assumption E: bootstrap weights} follow from the hypothesis.




\paragraph*{\ref{M-Assumption E: Gamma hat}}
In this example, $\gamma_0(x;\mathbf{Z})=Y\1\{X\leq x\}$ is known, so it suffices to verify \eqref{Appendix Eq: simpler D gamma hat}. The uniform covering number bound is straightforward as $\{\1\{\cdot\leq x\}:x\in\R\}$ is a VC-subgraph class. An envelope function is $|Y|$, whose second moment is finite. For $x\in I_{\x}^{\eta}$, $|\gamma_0(x;\mathbf{Z})-\gamma_0(\x;\mathbf{Z})|\leq |Y| \1\{\x-\eta \leq X\leq \x +\eta\}$, which we can take as $\bar{D}_{\gamma}^{\eta}(\mathbf{Z})$. Then, for $j=2,4$,
\begin{equation*}
    \E[\bar{D}_{\gamma}^{\eta}(\mathbf{Z})^j] \leq 2^{j-1} \int_{\x-\eta}^{\x+\eta} \big(|\mu_0(x)|^j +\E[\varepsilon^j|X=x]\big) f_0(x)dx \leq C \eta
\end{equation*}
and the desired bound holds.

\paragraph*{\ref{M-Assumption E: Phi hat}}
$\phi_0(x;\mathbf{Z})=\1\{X\leq x\}$ is known, so it suffices to verify the analogue of \eqref{Appendix Eq: simpler D gamma hat}. The argument is the same as for checking \ref{M-Assumption E: Gamma hat} in monotone density estimation with no censoring.


\paragraph*{\ref{M-Assumption E: covariance kernel}}
We have
\begin{equation*}
    \psi_{\x}(v;\mathbf{Z}) = \varepsilon(\1\{X\leq \x +v\}-\1\{X\leq \x\})  + (\mu_0(X)-\mu_0(\x)) (\1\{X\leq \x +v\}-\1\{X\leq \x\}).
\end{equation*}
Then,
\begin{equation*}
    \E[|\psi_{\x}(v;\mathbf{Z})-\psi_{\x}(v';\mathbf{Z})|]\leq \int_{\x+v \land v'}^{\x + v\lor v'} [\sigma_0(x) +|\mu_0(x)-\mu_0(\x)|] f_0(x)dx \leq C |v-v'|
\end{equation*}
and the first display holds. 
For the covariance kernel, note $|(\mu_0(X)-\mu_0(\x_n))(\1\{X\leq \x_n+v\}-\1\{X\leq \x_n\}|\leq |v| \sup_{|x-\x|\leq 2\eta} |\partial\mu_0(x)|$ for $|x_n-\x|\lor |v|\leq \eta$ for $\eta>0$ small enough. Then,
\begin{align*}
    &\E[\psi_{\x_n}(s\eta_n;\mathbf{Z})\psi_{\x_n}(t\eta_n;\mathbf{Z})]\\
    &= \E[\varepsilon^2(\1\{X\leq \x_n +s\eta_n\}-\1\{X\leq \x_n\})(\1\{X\leq \x_n +t\eta_n\}-\1\{X\leq \x_n\}) ] + O(\eta_n^2)\\
    &= \int_{\x_n}^{\x_n+\eta_n (s\land t)} \sigma_0^2(x)f_0(x)dx\1\{s>0,t>0\} \\
    &\quad + \int^{\x_n}_{\x_n+\eta_n (s\lor t)} \sigma_0^2(x)f_0(x)dx\1\{s<0,t<0\} + O(\eta_n^2)
\end{align*}
and
\begin{align*}
   &\eta_n^{-1}\E[\psi_{\x_n}(s\eta_n;\mathbf{Z})\psi_{\x_n}(t\eta_n;\mathbf{Z})] \\
   &\to \sigma_0^2(\x)f_0(\x) \big[ (s\land t) \1\{s>0,t>0\} - (s\lor t) \1\{s<0,t<0\} \big]\\
   &= \sigma_0^2(\x)f_0(\x) (|s|\land |t|) \1\{\mathrm{sign}(s)=\mathrm{sign}(t)\},
\end{align*}
as desired.


\paragraph*{\ref{M-Assumption B: covariance kernel}}
$\mathcal{C}_{\x}(1,1)>0$ follows from $f_0(\x) \sigma_0^2(\x)>0$. $\lim_{\eta\downarrow0}\mathcal{C}_{\x}(1,\eta)/\sqrt{\eta}=0$ follows from the same computation as in the monotone density estimation. The remaining conditions follow from verifying \ref{M-Assumption E: covariance kernel} above.\smallskip
\newline
\ref{M-Assumption B: uhat} trivially holds since $\widehat{u}_n=\widehat{u}_n^*=1$ in this example.


\paragraph*{\ref{M-Assumption B: Phi uhat}}
$\widehat{\Phi}_n,\widehat{\Phi}_n^*$ are (empirical) cdfs, so they are non-negative, non-decreasing, and right-continuous. $\{0,\widehat{u}_n\}\subset \widehat{\Phi}_n(I)$ and $\{0,\widehat{u}_n^*\}\subset \widehat{\Phi}_n^*(I)$ hold as $\widehat{u}_n=\widehat{u}_n^*=1$, $\widehat{\Phi}_n(\min_iX_i-)=0=\widehat{\Phi}_n^*(\min_iX_i-)$, and $\widehat{\Phi}_n(\max_iX_i)=1=\widehat{\Phi}_n^*(\max_iX_i)$. The sets $\widehat{\Phi}_n(I),\widehat{\Phi}_n^*(I)$ are finite and thus closed.

\paragraph*{\ref{M-Assumption B: jump of Phi hat}}
With probability one, all $X_i$'s are distinct. If $x$ is one of $X_i$'s, $\widehat{\Phi}_n^*( x ) - \widehat{\Phi}_n^*( x- ) = n^{-1}W_{j,n}$ for some $j$, and $\widehat{\Phi}_n^*( x ) - \widehat{\Phi}_n^*( x- ) =0$ otherwise.  Given $\E|W_{1,n}|^{\mathfrak{r}}<\infty$, $n^{-1}\max_{1\leq i\leq n}|W_{i,n}|=o_{\P}(n^{-5/6})$, which implies $\sqrt{na_n}\sup_{x\in I}|\widehat{\Phi}_n^*( x ) - \widehat{\Phi}_n^*( x- ) |=o_{\mathbb{P}}(1)$. The argument for $\widehat{\Phi}_n$ is similar, but simpler.
\smallskip
\newline
Assumption \ref{M-Assumption D} follows from \ref{M-Assumption E: Gamma hat}-\ref{M-Assumption E: Phi hat} and empirical process theory arguments.

\subsection{Proof of Corollary \ref{M-[Corollary] monotone regression with covariates}}\label{Appendix subsection: proof theorem monotone regression with covariates}
Assumption \ref{M-Assumption A} and \ref{M-Assumption E: iid}-\ref{M-Assumption E: bootstrap weights} follow from the hypothesis.




\paragraph*{\ref{M-Assumption E: Gamma hat}}
In this example, $\check{\Gamma}_n=\widehat{\Gamma}_n$.
\begin{align*}
    |\widehat{\gamma}_n(x;\mathbf{Z}) - \gamma_0(x;\mathbf{Z})|&\leq \1\{X\leq  x\} \bigg[|\varepsilon|\Big| \widehat{g}_n(X,\mathbf{A})^{-1}-g_0(X,\mathbf{A})^{-1}\Big| + \frac{|\widehat{\mu}_n(X,\mathbf{A})-\mu_0(X,\mathbf{A})|}{\widehat{g}_n(X,\mathbf{A})}\\
    & + \frac{1}{n}\sum_{j=1}^n |\widehat{\mu}_n(X,\mathbf{A}_j) - \mu_0(X,\mathbf{A}_j)|+ \bigg|\frac{1}{n}\sum_{j=1}^n \mu_0(X,\mathbf{A}_j) - \theta_0(X)\bigg| \bigg].
\end{align*}
The last sum is bounded by $\sup_{x\in I}|\frac{1}{n}\sum_{j=1}^n\mu_0(x,\mathbf{A}_j)-\theta_0(x)|$, and this object is $O_{\P}(n^{-1/2})$: to see this claim, first note that Assumption MRC (iv) and Theorem 2.7.11 of \cite{vanderVaart-Wellner_1996_Book} imply $\limsup_{\epsilon\downarrow0}\log N_U(\epsilon,\{\mu(x,\cdot):x\in I\}) \epsilon^V <\infty$ for some $V\in (0,2)$ and Theorem 4.2 of \cite{Pollard_1989_SS} implies $\sup_{x\in I}|\frac{1}{n}\sum_{j=1}^n\mu_0(x,\mathbf{A}_j)-\theta_0(x)|=O_{\P}(n^{-1/2})$.
Together with Assumption MRC (iii), $a_n \frac{1}{n}\sum_{i=1}^n \sup_{x\in I}|\widehat{\gamma}_n(x;\mathbf{Z})-\gamma_0(x;\mathbf{Z})|^2=o_{\P}(1)$ holds.

By Assumption MRC (iii), uniformly over $V\in (0,2\delta]$
\begin{equation*}
    \sqrt{na_n}\sup_{|v|\leq V}\big\vert\widehat{\Gamma}_n(\x+v)-\widehat{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+v)+\bar{\Gamma}_n(\x)\big\vert \leq o_{\mathbb{P}}(1) + O_{\mathbb{P}}(\sqrt{a_n}) V
\end{equation*}
and the desired inequality holds.

The uniform covering numbers of $\mathfrak{F}_{\gamma},\hat{\mathfrak{F}}_{\gamma,n}$ are the same order as for $\{\1\{\cdot\leq x\}:x\in I\}$. For $x \in I_{\x}^{\eta}$, $|\gamma_0(x;\mathbf{Z})-\gamma_0(\x;\mathbf{Z})|\leq \1\{\x-\eta\leq X\leq \x+\eta\}(|\varepsilon| c^{-1}+\theta_0(\x+\eta) )$. Then, $\limsup_{\eta\downarrow0}\E[\bar{D}_{\gamma}^{\eta}(\mathbf{Z})^j]\eta^{-1} <\infty$ holds for $j=2,4$.

\paragraph*{\ref{M-Assumption E: Phi hat}}
$\phi_0(x;\mathbf{Z})=\1\{X\leq x\}$ is known and the same as in the classical case, so the same argument applies.


\paragraph*{\ref{M-Assumption E: covariance kernel}}
We have
\begin{equation*}
    \psi_{\x}(v;\mathbf{Z}) = (\1\{X\leq \x+v\}-\1\{X\leq \x\})\bigg[\frac{\varepsilon}{g_0(X,\mathbf{A})} + \theta_0(X)-\theta_0(\x) \bigg].
\end{equation*}
Then, for $v,v'\in [-\eta,\eta]$ with sufficiently small $\eta>0$,
\begin{align*}
    |\psi_{\x}(v;\mathbf{Z}) - \psi_{\x}(v';\mathbf{Z})|&\leq |\1\{X\leq \x+ v\}-\1\{X\leq \x+ v'\}|\big( c^{-1} |\varepsilon| + |X-\x| \sup_{x\in I_{\x}^{\eta}}|\partial \theta_0(x)|\big) 
\end{align*}
and $\sup_{v\neq v'\in [-\eta_n,\eta_n]}\E[|\psi_{\x}(v;\mathbf{Z}) - \psi_{\x}(v';\mathbf{Z})|]/|v-v'|=O(1)$ holds.

For $s\eta_n$ small enough, $\psi_{\x}(s\eta_n;\mathbf{Z})= (\1\{X\leq \x+ s\eta_n\}-\1\{X\leq \x\})\varepsilon g_0(X,\mathbf{A})^{-1} + O(\eta_n)$ and
\begin{align*}
    &\E[\psi_{\x}(s\eta_n;\mathbf{Z})\psi_{\x}(t\eta_n;\mathbf{Z})] \\
    &= \E\bigg[\frac{\sigma_0^2(X,\mathbf{A})}{g_0(X,\mathbf{A})^2}(\1\{X\leq \x+ s\eta_n\}-\1\{X\leq \x\})(\1\{X\leq \x+ t\eta_n\}-\1\{X\leq \x\}) \bigg] + O(\eta_n^2)
\end{align*}
and
\begin{align*}
    & \E\bigg[\frac{\sigma_0^2(X,\mathbf{A})}{g_0(X,\mathbf{A})^2}(\1\{X\leq \x+ s\eta_n\}-\1\{X\leq \x\})(\1\{X\leq \x+ t\eta_n\}-\1\{X\leq \x\}) \bigg]\\
    &= \E \bigg[\int_{\x}^{\x+\eta_n (s\land t)} \frac{\sigma_0^2(x,\mathbf{A})}{g_0(x,\mathbf{A})^2} f_{X|\mathbf{A}}(x|\mathbf{A})dx \bigg]\1\{s>0,t>0\}\\
    &\quad + \E \bigg[\int_{\x+\eta_n (s\lor t)}^{\x} \frac{\sigma_0^2(x,\mathbf{A})}{g_0(x,\mathbf{A})^2} f_{X|\mathbf{A}}(x|\mathbf{A})dx \bigg]\1\{s<0,t<0\}\\
    &= \E\bigg[\frac{\sigma_0^2(\x,\mathbf{A})}{g_0(\x,\mathbf{A})^2}f_{X|\mathbf{A}}(\x|\mathbf{A})\bigg]\Big(\eta_n (s\land t) \1\{s>0,t>0\} - \eta_n (s\lor t) \1\{s<0,t<0\} \Big) + o(\eta_n).
\end{align*}
Since $\frac{f_{X|\mathbf{A}}(x|\mathbf{A})}{g_0(x,\mathbf{A})^2} = \frac{f_0(x)}{g_0(x,\mathbf{A})}$, we have 
\begin{equation*}
    \eta_n^{-1}\E[\psi_{\x}(s\eta_n;\mathbf{Z})\psi_{\x}(t\eta_n;\mathbf{Z})] \to f_0(\x) \E\bigg[\frac{\sigma_0^2(\x,\mathbf{A})}{g_0(\x,\mathbf{A})}\bigg]\Big( (s\land t) \1\{s,t>0\} -  (s\lor t) \1\{s,t<0\} \Big),
\end{equation*}
as desired.

\paragraph*{\ref{M-Assumption B: covariance kernel}}
$\mathcal{C}_{\x}(1,1)>0$ follows from $f_0(\x) \E[\frac{\sigma_0^2(\x,\mathbf{A})}{g_0(\x,\mathbf{A})}] >0$. $\lim_{\eta\downarrow0}\mathcal{C}_{\x}(1,\eta)/\sqrt{\eta}=0$ follows from the same computation as in the monotone density estimation. The remaining conditions follow from verifying \ref{M-Assumption E: covariance kernel} above. \smallskip
\newline
\paragraph*{\ref{M-Assumption B: uhat} \ref{M-Assumption B: Phi uhat} \ref{M-Assumption B: jump of Phi hat}} Verifying these conditions is the same as in the classical monotone regression case.

\section{Primitive sufficient conditions for Assumption MRC (iii)}

Here we provide primitive sufficient conditions for Assumption MRC (iii) by focusing on specific estimators $\widehat{\mu}_n$ and $\widehat{g}_n$. As discussed by \cite{Westling-Gilbert-Carone_2020_JRRSB}, cross-fitting avoids restrictions on uniform entropy, allowing for a large class of flexible preliminary estimators. Here we use sample splitting to simplify exposition, but the proposed procedure can be straightforwardly modified for cross-fitting.

Suppose there is a separate random sample $\tilde{\mathbf{Z}}_1,\dots,\tilde{\mathbf{Z}}_n$ drawn from the distribution of $\mathbf{Z}$, which is independent of $\mathbf{Z}_1,\dots,\mathbf{Z}_n$. Preliminary estimators $\widehat{\mu}_n$ and $\widehat{g}_n$ are constructed from $\tilde{\mathbf{Z}}_1,\dots,\tilde{\mathbf{Z}}_n$.
For concreteness, we consider a partitioning-based least squares estimator $\widehat{\mu}_n$ \citep{Cattaneo-Farrell-Feng_2020_AoS} and local polynomial kernel-based estimators $\widehat{f}_{X|\mathbf{A},n}(x|\mathbf{a})$ and $\widehat{f}_n(x)$ of $f_{X|\mathbf{A}}(x|\mathbf{a})$ and $f_0(x)$ \citep*{Cattaneo-Jansson-Ma_2020_JASA,Cattaneo-Chandak-Jansson-Ma_2024}, from which we construct $\widehat{g}_n(x,\mathbf{a})=\widehat{f}_{X|\mathbf{A},n}(x|\mathbf{a})/\widehat{f}_n(x)$.  

Let $d=\mathrm{dim}(\mathbf{A})$. For simplicity, suppose the support of $(X,\mathbf{A}')'$ equals $[0,1]^{1+d}$. Let $\mathbf{p}(x,\mathbf{a})$ be a $k_n$-dimensional vector of bounded basis functions of order $m$ on $\mathcal{S}$ which are locally supported  e.g.,\ splines \cite[see][for details and examples of basis functions]{Cattaneo-Farrell-Feng_2020_AoS}. We consider the estimator 
\begin{equation*}
    \widehat{\mu}_n(x,\mathbf{a}) = \mathbf{p}(x,\mathbf{a})'\bigg(\sum_{i=1}^n\mathbf{p}(\tilde{X}_i,\tilde{\mathbf{A}}_i) \mathbf{p}(\tilde{X}_i,\tilde{\mathbf{A}}_i)'\bigg)^{-1} \sum_{i=1}^n \mathbf{p}(\tilde{X}_i,\tilde{\mathbf{A}}_i)\tilde{Y}_i.
\end{equation*}
For the estimator of $f_{X|\mathbf{A}}(x\vert\mathbf{a})$, letting $\widehat{F}_{X|\mathbf{A},n}(\cdot\vert \mathbf{a})$ be an estimator of $\P[X\leq \cdot \vert \mathbf{A}=\mathbf{a}]$ specified below, $\widehat{f}_{X|\mathbf{A},n}(x|\mathbf{a})$ is obtained by local polynomial regression:
\begin{equation*}
    \widehat{f}_{X|\mathbf{A},n}(x|\mathbf{a}) = \mathbf{e}_2'\widehat{\boldsymbol{\beta}}(x|\mathbf{a}),\quad \widehat{\boldsymbol{\beta}}(x|\mathbf{a}) = \argmin_{\mathbf{u}\in\R^{\mathfrak{p}_1+1}}\sum_{i=1}^n\Big(\widehat{F}_{X|\mathbf{A},n}(\tilde{X}_i|\mathbf{a}) - \mathbf{q}_1(\tilde{X}_i-x)'\mathbf{u}\Big)^2K_h(\tilde{X}_i-x) 
\end{equation*}
where $\mathfrak{p}_1\geq 1$ is the order of the polynomial basis $\mathbf{q}_1(x)=(1,x/1!, x^2/2!,\dots,x^{\mathfrak{p}_1}/\mathfrak{p}_1!)'$, $\mathbf{e}_l$ is the conformable unit vector whose $l$th element is unity, and $K_h(x)=K(x/h)/h$ for some kernel function $K$ and some positive bandwidth $h$. The estimator $\widehat{F}_{X|\mathbf{A},n}(x|\mathbf{a})$ is constructed via local polynomial regression of order $\mathfrak{p}_2=\mathfrak{p}_1-1$:
\begin{equation*}
    \widehat{F}_{X|\mathbf{A},n}(x|\mathbf{a}) =\mathbf{e}_1'\widehat{\boldsymbol{\gamma}}(x|\mathbf{a}),\quad \widehat{\boldsymbol{\gamma}}(x|\mathbf{a}) = \argmin_{\mathbf{v}\in\R^{k_{\mathfrak{p}_2}}}\sum_{i=1}^n \big(\1\{\tilde{X}_i\leq x\} - \mathbf{q}_2(\tilde{\mathbf{A}}_i-\mathbf{a})'\mathbf{v}\big)^2L_h(\tilde{\mathbf{A}}_i-\mathbf{a})
\end{equation*}
where, using standard multi-index notation, $\mathbf{q}_2(\mathbf{a})$ denotes the $k_{\mathfrak{p}_2}$-dimensional vector collecting the polynomials $\mathbf{a}^{\mathbf{m}}/\mathbf{m}!$ for $0\leq \vert \mathbf{m}\vert\leq \mathfrak{p}_2$ with $\mathbf{a}^{\mathbf{m}} = a_1^{m_1}a_2^{m_2}\dots a_d^{m_d}$, $\vert\mathbf{m}\vert=\sum_{j=1}^dm_j$, and $k_{\mathfrak{p}_2}=\frac{(d+\mathfrak{p}_2)!}{d!\mathfrak{p}_2!}+1$, and $L_h(\mathbf{a})=L(\mathbf{a}/h)/h^d$ for $L(\mathbf{a})=\prod_{j=1}^dK(a_j)$ i.e.,\ product kernel. 
The estimator $\widehat{f}_n(x)$ is constructed in a similar manner. First, the empirical cdf $\widehat{F}_n$ of $\{\tilde{X}_i\}$ is constructed and then $\widehat{f}_n(x)$ is formed via local polynomial regression:
\begin{equation*}
    \widehat{f}_n(x)=\mathbf{e}_2'\widehat{\boldsymbol{\delta}}_n(x),\quad \widehat{\boldsymbol{\delta}}_n(x)= \argmin_{\mathbf{w}\in\R^{\mathfrak{p}_1}+1} \sum_{i=1}^n\Big(\widehat{F}_n(\tilde{X}_i)-\mathbf{q}_1(\tilde{X}_i-x)'\mathbf{w}\Big)^2K_b(\tilde{X}_i-x)
\end{equation*}
where $b>0$ is some bandwidth.

Now we state sufficient conditions for Assumption MRC (ii) based on the partitioning-based series estimator $\widehat{\mu}_n$ and the kernel-based estimator $\widehat{g}_n$.
\begin{description}
    \item[Primitive Conditions MRC]
    \item[(i)] $\tilde{\mathbf{Z}}_1,\dots,\tilde{\mathbf{Z}}_n$ are independent of $\mathbf{Z}_1,\dots,\mathbf{Z}_n$.
    
    \item[(ii)] The support of $(X,\mathbf{A}')'$ is $[0,1]^{1+d}$, and the distribution of $(X,\mathbf{A}')'$ is absolutely continuous. The Lebesgue density of $(X,\mathbf{A}')'$ and the conditional variance of $Y$ given $(X,\mathbf{A}')'$ are bounded away from zero and continuous on $[0,1]^{1+d}$. $\mu_0$ is $(m+1)$-times continuously differentiable on $[0,1]^{1+d}$.

    \item[(iii)] The vector of basis functions $p$ satisfies Assumptions 2, 3, and 4 of \cite{Cattaneo-Farrell-Feng_2020_AoS}.

    \item[(iv)] $f_{X|\mathbf{A}}(x|\mathbf{a})$ and $f_0(x)$ are $\mathfrak{p}_1$-times continuously differentiable in $x$, and $f_{X|\mathbf{A}}(x|\mathbf{a})$ is $\mathfrak{p}_1$-times continuously differentiable in $\mathbf{a}$.

    \item[(v)] $K$ is a symmetric, Lipschitz continuous probability density function supported on $[-1,1]$.
\end{description}
As verified by \cite{Cattaneo-Farrell-Feng_2020_AoS}, (iii) holds for widely used local basis functions such as splines and wavelets.

\begin{lemma}
    Suppose $\tilde{\mathbf{Z}}_1,\dots,\tilde{\mathbf{Z}}_n$ is a random sample drawn from the distribution of $\mathbf{Z}$ and Primitive Conditions MRC hold. In addition, with $\tau_n= n^{-1}\log n$, $k_n=O(\tau_n^{-\frac{d+1}{2m+d+1}})$, $h=O(\tau_n^{\frac{d+1}{2\mathfrak{p}_1+d+1}})$, $b=O(\tau_n^{\frac{1}{2\mathfrak{p}_1+1}})$, $\frac{m}{2m+d+1}+\frac{\mathfrak{p}_1}{2\mathfrak{p}_1+d+1} \geq \frac{1}{2}$, and $\min\{\frac{2m}{2m+d+1},\frac{2\mathfrak{p}_1}{2\mathfrak{p}_1+d+1} \} > \frac{1}{2\q+1}$. Then, $\widehat{\mu}_n$ and $\widehat{g}_n$ described above and $\widehat{\Gamma}_n$ based on the $\widehat{\mu}_n,\widehat{g}_n$ satisfy Assumption MRC (iii) with $\delta = \min\{\x,1-\x\}/4$. In particular,
    \begin{equation*}
        \sqrt{n} \sup_{|v|\leq V}\big\vert \widehat{\Gamma}_n(\x+v) - \widehat{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+v) +\bar{\Gamma}_n(\x)\big\vert \leq V O_{\mathbb{P}}(1) + o_{\mathbb{P}}(a_n^{-1})
    \end{equation*}
    uniformly over $V\in (0, 2\delta]$.
\end{lemma}
\begin{proof}
    By Theorem 4.3 of \cite{Cattaneo-Farrell-Feng_2020_AoS},
    \begin{equation*}
        \sup_{(x,\mathbf{a})\in [0,1]^{1+d}} \vert \widehat{\mu}_n(x,\mathbf{a})-\mu_0(x,\mathbf{a})\vert = O_{\P}\bigg( \sqrt{\frac{k_n\log n}{n}} + k_n^{-\frac{m}{d+1}}\bigg)
    \end{equation*}
    and by Theorem 1 of \citet*{Cattaneo-Chandak-Jansson-Ma_2024},
    \begin{equation*}
        \sup_{(x,\mathbf{a})\in [0,1]^{1+d}}\vert\widehat{f}_{X|\mathbf{A},n}(x|\mathbf{a})- f_{X|\mathbf{A}}(x|\mathbf{a})\vert = O_{\P}\bigg( \sqrt{\frac{\log n}{n h^{1+d}}} + h^{\mathfrak{p}_1}\bigg).
    \end{equation*}
    Also, one can show
    \begin{equation*}
        \sup_{x\in [0,1]}\vert\widehat{f}_n(x) -f_0(x)\vert = O_{\P}\bigg( \sqrt{\frac{\log n}{n b}} + b^{\mathfrak{p}_1}\bigg).
    \end{equation*}
    Then, with the specified rate of $k_n,h,b$,
    \begin{equation*}
        \sup_{(x,\mathbf{a})\in [0,1]^{1+d}} \vert \widehat{\mu}_n(x,\mathbf{a})-\mu_0(x,\mathbf{a})\vert = O_{\P}\Big( \tau_n^{\frac{m}{2m+d+1}} \Big),
    \end{equation*}
    \begin{equation*}
        \sup_{(x,\mathbf{a})\in [0,1]^{1+d}} \vert \widehat{g}_n(x,\mathbf{a})-g_0(x,\mathbf{a})\vert = O_{\P}\Big( \tau_n^{\frac{\mathfrak{p}_1}{2\mathfrak{p}_1+d+1}} \Big).
    \end{equation*}
    Since $\min\{\frac{2m}{2m+d+1},\frac{2\mathfrak{p}_1}{2\mathfrak{p}_1+d+1} \} > \frac{1}{2\q+1}$, it follows $a_n\frac{1}{n}\sum_{i=1}^n|\widehat{\mu}_n(X_i,\mathbf{A}_i)-\mu_0(X_i,\mathbf{A}_i)|^2=o_{\P}(1)$, $a_n\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n|\widehat{\mu}_n(X_i,\mathbf{A}_j)-\mu_0(X_i,\mathbf{A}_j)|^2=o_{\P}(1)$, and $a_n\frac{1}{n}\sum_{i=1}^n\varepsilon_i^2|\widehat{g}_n(X_i,\mathbf{A}_i)-g_0(X_i,\mathbf{A}_i)|^2=o_{\P}(1)$. Also, by $\frac{m}{2m+d+1}+\frac{\mathfrak{p}_1}{2\mathfrak{p}_1+d+1} \geq \frac{1}{2}$,
    \begin{align}\label{Appendix: Primitive Condition Covariate Adjusted Regression sqrt N rate}
        \sup_{(x,\mathbf{a})\in [0,1]^{1+d}} \vert \widehat{\mu}_n(x,\mathbf{a})-\mu_0(x,\mathbf{a})\vert \sup_{(x,\mathbf{a})\in [0,1]^{1+d}} \vert \widehat{g}_n(x,\mathbf{a})-g_0(x,\mathbf{a})\vert = O_{\P}\Big( n^{-1/2}\Big).
    \end{align}
    
    Decompose $\widehat{\gamma}_n$ into $\widehat{\gamma}_{1,n}$ and $\widehat{\gamma}_{2,n}$ where
    \begin{equation*}
        \widehat{\gamma}_{1,n}(x;\mathbf{Z}) = \1\{X\leq x\} \frac{Y-\widehat{\mu}_n(X,\mathbf{A})}{\widehat{g}_n(X,\mathbf{A})},\quad \widehat{\gamma}_{2,n}(x;\mathbf{Z}) = \1\{X\leq x\} \frac{1}{n}\sum_{j=1}^n\widehat{\mu}_n(X,\mathbf{A}_j)
    \end{equation*}
    and let $\widehat{\Gamma}_{k,n}(x)=\frac{1}{n}\sum_{i=1}^n \widehat{\gamma}_{1,n}(x;\mathbf{Z}_i)$ for $k=1,2$. Define $\gamma_{k,0},\bar{\Gamma}_{k,n}$ $k=1,2$ in the same manner. 
    
    Letting $\tilde{\mathfrak{Z}}_n$ be the $\sigma$-field generated by $\tilde{\mathbf{Z}}_1,\dots,\tilde{\mathbf{Z}}_n$,
    \begin{align*}
        &\V\Big[ \widehat{\Gamma}_{1,n}(\x+v) - \widehat{\Gamma}_{1,n}(\x) - \bar{\Gamma}_{1,n}(\x+v) + \bar{\Gamma}_{1,n}(\x) \Big\vert \tilde{\mathfrak{Z}}_n\Big] \\
        &\leq n^{-1} \E\left[\left. \big(\1\{X\leq\x+v\} -\1\{X\leq \x\} \big)^2 \bigg(\frac{Y-\widehat{\mu}_n(X,\mathbf{A})}{\widehat{g}_n(X,\mathbf{A})} - \frac{Y-\mu_0(X,\mathbf{A})}{g_0(X,\mathbf{A})}\bigg)^2 \right\vert\tilde{\mathfrak{Z}}_n\right] \\
        &\leq n^{-1} C \E\big[\big\vert\1\{X\leq\x+v\} -\1\{X\leq \x\} \big\vert \varepsilon^2\big] \sup_{\vert x-\x\vert\leq \vert v\vert}\sup_{\mathbf{a}\in [0,1]^d} \vert \widehat{g}_n(x,\mathbf{a})-g_0(x,\mathbf{a})\vert^2 \\
        &\quad + n^{-1} C \E\big[\big\vert\1\{X\leq\x+v\} -\1\{X\leq \x\} \big\vert \big]\sup_{\vert x-\x\vert\leq \vert v\vert}\sup_{\mathbf{a}\in [0,1]^d} \vert \widehat{\mu}_n(x,\mathbf{a})-\mu_0(x,\mathbf{a})\vert^2\\
        &= o_{\P}\big( (n a_n)^{-1} \big)
    \end{align*}
    where the inequalities hold with probability one. 
    
    Note $\widehat{\Gamma}_{2,n}(x) = \frac{1}{n^2}\sum_{1\leq i\neq j\leq n} \1\{X_i\leq x\}\widehat{\mu}_n(X_i,\mathbf{A}_j) + O_{\P}(n^{-1})$, and
    \begin{align*}
        &\V\left[ \left.\frac{1}{n(n-1)}\sum_{1\leq i\neq j\leq n} \big(\1\{X_i\leq \x+v\}-\1\{X_i\leq \x\}\big)\big(\widehat{\mu}_n(X_i,\mathbf{A}_j) -\mu_0(X_i,\mathbf{A}_j)\big) \right\vert \tilde{\mathfrak{Z}}_n\right] \\
        &\leq n^{-1}C \vert v\vert \sup_{\vert x-\x\vert\leq \vert v\vert}\sup_{\mathbf{a}\in [0,1]^d} \vert \widehat{\mu}_n(x,\mathbf{a})-\mu_0(x,\mathbf{a})\vert^2 \\
        &\quad + C n^{-2}\vert v\vert \sup_{\vert x-\x\vert\leq \vert v\vert}\sup_{\mathbf{a}\in [0,1]^d} \vert \widehat{\mu}_n(x,\mathbf{a})-\mu_0(x,\mathbf{a})\vert^2 =  o_{\P}\big((n a_n)^{-1}\big)
    \end{align*}
    where we use Hoeffding decomposition and the inequality holds with probability one.

    To complete the proof, it suffices to show that there is a sequence of random variables $A_n'=O_{\P}(1)$ such that for $V \in (0,a_n\delta]$,
    \begin{equation*}
        \sqrt{n } \sup_{\vert v\vert \leq V}\left\vert  \E\big[\widehat{\Gamma}_{n}(\x+v) - \widehat{\Gamma}_{n}(\x) - \bar{\Gamma}_{n}(\x+v) + \bar{\Gamma}_{n}(\x)\big\vert \tilde{\mathfrak{Z}}_n\big] \right\vert \leq  A_n' V.
    \end{equation*}
    With $f_{\mathbf{A}}(\mathbf{a})$ denoting the Lebesgue density of $\mathbf{A}$,
    \begin{align*}
        &\E\big[\widehat{\Gamma}_{n}(\x+v) - \widehat{\Gamma}_{n}(\x) \big\vert \tilde{\mathfrak{Z}}_n\big] \\
        &= \int \int \big(\1\{u\leq \x+v\}-\1\{u\leq \x\}\big)  \frac{g_0(u,\mathbf{a})}{\widehat{g}_n(u,\mathbf{a})}\big[\mu_0(u,\mathbf{a})-\widehat{\mu}_n(u,\mathbf{a})\big]f_0(u)du f_{\mathbf{A}}(\mathbf{a})d\mathbf{a} \\
        &\qquad+ \int \big(\1\{u\leq \x+v\}-\1\{u\leq \x\}\big)\int\widehat{\mu}_n(u,\mathbf{a})f_{\mathbf{A}}(\mathbf{a})d\mathbf{a} du\\
        & \E\big[\bar{\Gamma}_{n}(\x+v) - \bar{\Gamma}_{n}(\x)\big\vert \tilde{\mathfrak{Z}}_n\big]\\
        &=\int\big(\1\{u\leq \x+v\}-\1\{u\leq \x\}\big)\int \mu_0(u,\mathbf{a})f_{\mathbf{A}}(\mathbf{a})d\mathbf{a}f_0(u)du.
    \end{align*}
    Then, for $v\geq 0$,
    \begin{align*}
        &\E\big[\widehat{\Gamma}_{n}(\x+v) - \widehat{\Gamma}_{n}(\x) - \bar{\Gamma}_{n}(\x+v) + \bar{\Gamma}_{n}(\x)\big\vert \tilde{\mathfrak{Z}}_n\big]\\
        &= \int_{\x\leq u\leq \x +v} \big[\widehat{\mu}_n(u,\mathbf{a})-\mu_0(u,\mathbf{a})\big] \Big[1-\frac{g_0(u,\mathbf{a})}{\widehat{g}_n(u,\mathbf{a})}\Big]f_0(u) f_{\mathbf{A}}(\mathbf{a})d(u,\mathbf{a}).
    \end{align*}
    A similar expression holds for $v<0$. Then, for some fixed $C>0$,
    \begin{align*}
        &\left\vert \E\big[\widehat{\Gamma}_{n}(\x+v) - \widehat{\Gamma}_{n}(\x) - \bar{\Gamma}_{n}(\x+v) + \bar{\Gamma}_{n}(\x)\big\vert \tilde{\mathfrak{Z}}_n\big]\right\vert \\
        &\leq C \vert v\vert \sup_{\vert x-\x\vert\leq \vert v\vert}\sup_{\mathbf{a}\in [0,1]^d} \vert \widehat{\mu}_n(x,\mathbf{a})-\mu_0(x,\mathbf{a})\vert \sup_{\vert x-\x\vert\leq \vert v\vert}\sup_{\mathbf{a}\in [0,1]^d} \vert \widehat{g}_n(x,\mathbf{a})-g_0(x,\mathbf{a})\vert
    \end{align*}
    and the desired result follows from \eqref{Appendix: Primitive Condition Covariate Adjusted Regression sqrt N rate}.
\end{proof}



\section{Additional example: monotone density function with conditionally independent right-censoring}


\label{Appendix Section: Example monotone density conditionally independent}
We introduce covariates $\mathbf{A}$ and consider the case of censoring at random: $X \indep C|\mathbf{A}$. See \cite{vanderlaan-Robins_2003_Book,Zeng_2004_AoS} and references therein for existing analysis of this problem. We have
\begin{equation*}
        \gamma_0(x;\mathbf{Z}) = F_0(x|\mathbf{A}) + S_0(x|\mathbf{A}) \bigg[ \frac{\Delta\1\{\check{X}\leq x\}}{S_0(\check{X}|\mathbf{A})G_0(\check{X}|\mathbf{A})} - \int_0^{\check{X}\land x}\frac{\Lambda_0(du|\mathbf{A})}{S_0(u|\mathbf{A})G_0(u|\mathbf{A})}\bigg]
\end{equation*}
where $F_0(x|A)=1-S_0(x|A)$, $S_0(x|\mathbf{A})=\P[X>x|\mathbf{A}]$, $G_0(c|\mathbf{A})=\P[C>c|\mathbf{A}]$, and $\Lambda_0(x|\mathbf{A}) = \int_0^{x} \frac{f_0(u|\mathbf{A})}{S_0(u|\mathbf{A})}du$ with $f_0$ being the Lebesgue density of $X$.
Denote by $\widehat{S}_n(\cdot|\cdot)$, $\widehat{G}_n(\cdot|\cdot)$, $\widehat{\Lambda}_n(\cdot|\cdot)$ preliminary estimates of $S_0,G_0,\Lambda_0$, respectively.


\paragraph*{Assumption \thesection}\label{Appendix Assumption: Monotone Density with Conditional Independent Censoring}
Let $\mathfrak{S}_n$, $\mathfrak{G}_n,\mathfrak{L}_n$ be sequences of function classes that contain $S_0(\cdot|\cdot)$, $G_0(\cdot|\cdot)$, $\Lambda_0(\cdot|\cdot)$, respectively. 
\begin{enumerate}[label=\normalfont(\roman*),noitemsep,itemindent=*]
	\item $\x$ is in the interior of $I=[0,u_0]$, $X\indep C |A$, and $\theta_0=f_0$ satisfies Assumption \ref{M-Assumption A: theta0}.
    
	\item \label{Appendix Example: monotone density conditionally independent function classes} There exist $c,c_1,c_2>0,\rho_{\gamma}\in (0,2)$such that for $n\geq 1$, for any $S\in\mathfrak{S}_n$, $G\in \mathfrak{G}_n$, and $\Lambda \in \mathfrak{L}_n$, the following hold: $\log N_U(\varepsilon,\{S(x|\cdot):x\in I\}) \leq c \varepsilon^{-\rho_{\gamma}}$ for $\varepsilon\in (0,1)$, where $N_U$ is as defined in Section 4.2 of the main paper, and $c_1\leq S(x|\mathbf{A})\leq c_2$, $c_1\leq G(x|\mathbf{A})\leq c_2$ for $x\in I$, and $\Lambda(u_0|\mathbf{A})\leq c_2$ with probability one. 


    \item \label{Appendix Example: monotone density conditionally independent high-level}
    There exist $\delta>0, \beta_{\gamma} \in [1/2,2)$ such that for $V \in (0,2\delta]$, $\sqrt{n a_n}\sup_{|v|\leq V}|\widehat{\Gamma}_n(\x+v)-\widehat{\Gamma}_n(\x)-\bar{\Gamma}_n(\x+v)+\bar{\Gamma}_n(\x)|\leq o_{\P}(1) +V^{\beta_{\gamma}}o_{\mathbb{P}}(a_n^{\beta_{\gamma}})$ where $o_{\P}$ terms do not depend on $V$.
    
	\item
	With probability approaching one, $\widehat{S}_n\in\mathfrak{S}_n$, $\widehat{G}_{n}\in\mathfrak{G}_n$, $\widehat{\Lambda}_n\in\mathfrak{L}_n$. For $(\widehat{h}_n,h_0) \in \{(\widehat{S}_n,S_0),(\widehat{G}_n,G_0),(\widehat{\Lambda}_n,\Lambda_0) \}$,
    \begin{equation*}
        a_n\frac{1}{n}\sum_{i=1}^n\sup_{x\in I}|\widehat{h}_n(x|\mathbf{A}_i)-h_0(x|\mathbf{A}_i)|^2=o_{\P}(1).
    \end{equation*}
 
	\item The conditional distribution of $X$ given $\mathbf{A}$ has bounded Lebesgue density $f_{X|\mathbf{A}}$, $\E[\frac{f_{X|\mathbf{A}}(\x|\mathbf{A})}{G_0(\x|\mathbf{A})}]>0$, and there are real-valued functions $B,\omega$ such that $\E[B(\mathbf{A})]<\infty$, $\lim_{\eta\downarrow0}\omega(\eta)=0$, and for $|x-\x|$ sufficiently small, $|\frac{f_{X|\mathbf{A}}(x|\mathbf{A})}{S_0(x|\mathbf{A})G_0(x|\mathbf{A})}-\frac{f_{X|\mathbf{A}}(\x|\mathbf{A})}{S_0(\x|\mathbf{A})G_0(\x|\mathbf{A})}|\leq \omega(|x-\x|)B(\mathbf{A})$. 
\end{enumerate}
The condition \ref{Appendix Example: monotone density conditionally independent high-level} is high-level, and there are a few different approaches to verify them. See \cite{Westling-Carone_2020_AoS} for details.
\begin{corollary}\label{Appendix Corollary: Monotone Density with Conditional Independent Censoring}
    Under Assumption \ref{Appendix Assumption: Monotone Density with Conditional Independent Censoring}, Assumptions \ref{M-Assumption A} and \ref{M-Assumption B} hold with
    \begin{equation*}
        \widehat{\Gamma}_n(x) = \frac{1}{n}\sum_{i=1}^n \widehat{\gamma}_n(x;\mathbf{Z}_i),\quad  \widehat{\Gamma}_n^{*}(x) = \frac{1}{n}\sum_{i=1}^nW_{i,n} \widehat{\gamma}_n(x;\mathbf{Z}_i)
    \end{equation*}
    \[ \widehat{\gamma}_n(x;\mathbf{Z})
  = \widehat{F}_n(x|\mathbf{A}) + \widehat{S}_n(x|\mathbf{A})
    \bigg[\frac{\Delta\1(\check{X}\leq x)}{\widehat{S}_n(\check{X}|\mathbf{A})\widehat{G}_n(\check{X}|\mathbf{A})} 
    - \int_0^{\check{X}\land x} \frac{\widehat{\Lambda}_n(du|\mathbf{A})}{\widehat{S}_n(u|\mathbf{A})\widehat{G}_n(u|\mathbf{A})}\bigg],
\]
where $\widehat{F}_n=1-\widehat{S}_n$, 
    \begin{equation*}
        \widehat{\Phi}_n(x)=\widehat{\Phi}_n^*(x)=x,\quad \widehat{u}_n=\widehat{u}_n^*=u_0,
    \end{equation*}
    \begin{equation*}
        \mathcal{C}_{\x}(s,t) = \E\Big[\frac{f_{X|\mathbf{A}}(\x|\mathbf{A})}{G_0(\x|\mathbf{A})}\Big] (|s| \land |t|) \1\{\mathrm{sign}(s)=\mathrm{sign}(t)\}, \quad \mathcal{D}_{\q}(\x) = \frac{\partial^\q f_0(\x)}{(\q+1)!}. 
\end{equation*}
\end{corollary}

\subsection{Proof of Corollary \ref{Appendix Corollary: Monotone Density with Conditional Independent Censoring}}
\label{Appendix Section: verification monotone density conditionally independent cernsoring}
In this example, $\widehat{\Phi}_n(x)=\widehat{\Phi}_n^*(x)=x=\Phi_0(x)$. Assumptions \ref{M-Assumption A} and \ref{M-Assumption E: iid}-\ref{M-Assumption E: bootstrap weights} follow from the hypothesis.
\paragraph*{\ref{M-Assumption E: Gamma hat}}
In this example, $\check{\Gamma}_n=\widehat{\Gamma}_n$. For $x\in I$,
\begin{align*}
    &\widehat{\gamma}_n(x;\mathbf{Z}) -\gamma_0(x;\mathbf{Z}) = \widehat{F}_n(x|\mathbf{A}) - F_0(x|\mathbf{A}) \\
    &\quad + \big[\widehat{S}_n(x|\mathbf{A})-S_0(x|\mathbf{A})\big]\bigg[\frac{\Delta\1(\check{X}\leq x)}{\widehat{S}_n(\check{X}|\mathbf{A})\widehat{G}_n(\check{X}|\mathbf{A})} 
    - \int_0^{\check{X}\land x} \frac{\widehat{\Lambda}_n(du|\mathbf{A})}{\widehat{S}_n(u|\mathbf{A})\widehat{G}_n(u|\mathbf{A})}\bigg] \\
    &\quad + S_0(x|\mathbf{A})\Delta \1\{\check{X}\leq x\} \Big[\big(\widehat{S}_n(\check{X}|\mathbf{A})\widehat{G}_n(\check{X}|\mathbf{A})\big)^{-1} - \big(S_0(\check{X}|\mathbf{A})G_0(\check{X}|\mathbf{A})\big)^{-1} \Big] \\
    &\quad - S_0(x|\mathbf{A}) \bigg[\int_0^{\check{X}\land x} \frac{\widehat{\Lambda}_n(du|\mathbf{A})}{\widehat{S}_n(u|\mathbf{A})\widehat{G}_n(u|\mathbf{A})} - \int_0^{\check{X}\land x}\frac{\Lambda_0(du|\mathbf{A})}{S_0(u|\mathbf{A})G_0(u|\mathbf{A})} \bigg]
\end{align*}
and
\begin{align*}
    &\left\vert\int_0^{\check{X}\land x} \frac{\widehat{\Lambda}_n(du|\mathbf{A})}{\widehat{S}_n(u|\mathbf{A})\widehat{G}_n(u|\mathbf{A})} - \int_0^{\check{X}\land x}\frac{\Lambda_0(du|\mathbf{A})}{S_0(u|\mathbf{A})G_0(u|\mathbf{A})} \right\vert \\
    &\leq \sup_{x'\in I} \Big\vert \big(\widehat{S}_n(x'|\mathbf{A})\widehat{G}_n(x'|\mathbf{A})\big)^{-1} - \big(S_0(x'|\mathbf{A})G_0(x'|\mathbf{A})\big)^{-1} \Big\vert \widehat{\Lambda}_n(u_0|\mathbf{A}) \\
    & \quad + \left\vert \frac{\widehat{\Lambda}_n(\check{X}\land x|\mathbf{A})-\Lambda_0(\check{X}\land x|\mathbf{A})}{S_0(\check{X}\land x|\mathbf{A})G_0(\check{X}\land x|\mathbf{A})} \right\vert + \left\vert \frac{\widehat{\Lambda}_n(0|\mathbf{A})-\Lambda_0(0|\mathbf{A})}{S_0(0|\mathbf{A})G_0(0|\mathbf{A})} \right\vert \\
    &\quad + \left\vert \int_0^{\check{X}\land x} [\widehat{\Lambda}_n(u|\mathbf{A})-\Lambda_0(u|\mathbf{A})]J_0(du|\mathbf{A})  \right\vert
\end{align*}
using integration by parts, where $J_0(u|\mathbf{a}) = [S_0(u|\mathbf{a})G_0(u|\mathbf{a})]^{-1}$. Thus, there is a fixed $C>0$ such that
\begin{align*}
    |\widehat{\gamma}_n(x;\mathbf{Z}) -\gamma_0(x;\mathbf{Z})|&\leq C \Big[ \sup_{x\in I}|\widehat{S}_n(x|\mathbf{A})-S_0(x|\mathbf{A})| + \sup_{x\in I}|\widehat{G}_n(x|\mathbf{A})-G_0(x|\mathbf{A})|\\
    &\qquad +\sup_{x\in I}|\widehat{\Lambda}_n(x|\mathbf{A})-\Lambda_0(x|\mathbf{A})| \Big],
\end{align*}
From the hypothesis, 
\begin{equation*}
    \frac{1}{n}\sum_{i=1}^n\sup_{x\in I}|\widehat{\gamma}_n(x;\mathbf{Z}_i)-\gamma_0(x;\mathbf{Z}_i)|^2=o_{\P}(1),\ a_n \frac{1}{n}\sum_{i=1}^n\sup_{x\in I_{\x}^{\delta}}\big|\widehat{\gamma}_n(x;\mathbf{Z}_i)  -\gamma_0(x;\mathbf{Z}_i) \big|^2 =o_{\P}(1)
\end{equation*}
follow.


For uniform covering numbers, it suffices to show that each of $\{S(x|\cdot):x\in I\}$, $\{\1\{\cdot\leq x\}:x\in I\}$, and $\{\int_0^{\cdot\land x} \frac{\Lambda(du|\cdot)}{S(u|\cdot)G(u|\cdot)} : x\in I\}$ has an appropriate bound on the uniform covering number by Lemma 5.1 of \cite{vanderVaart-vanderLaan_2006_IJB} (see examples after the lemma). For $\{\int_0^{\cdot\land x} \frac{\Lambda(du|\cdot)}{S(u|\cdot)G(u|\cdot)} : x\in I\}$ with $(S,G,\Lambda)\in \mathfrak{S}_n\times \mathfrak{G}_n\times\mathfrak{L}_n$, the mapping $x\mapsto \int_0^x \frac{\Lambda(du|\cdot)}{S(u|\cdot)G(u|\cdot)}$ is monotone (by the non-decreasing property of $\Lambda$ and $S,G\geq c_1 >0 $) and Lemma 2.6.18 of \cite{vanderVaart-Wellner_1996_Book} implies the desired result.


There is a fixed $C>0$ such that for $x\in I_{\x}^{\eta}$,
\begin{align*}
    |\gamma_0(x;\mathbf{Z}) -\gamma_0(\x;\mathbf{Z})|&\leq C \Big[|S_0(x|\mathbf{A})-S_0(\x|\mathbf{A})| + C\Delta |\1\{\check{X}\leq x\} - \1\{\check{X}\leq \x\}| \Big]
\end{align*}
and using $1-S_0(x|\cdot) =\int_0^x f_{X|\mathbf{A}}(u|\cdot) du$ with $f_{X|\mathbf{A}}$ being bounded, we can take
\begin{equation*}
    \bar{D}_{\gamma}^{\eta}(\mathbf{Z}) =  C\Delta \1\{\x-\eta \leq \check{X}\leq \x +\eta\} + C\eta,
\end{equation*}
which satisfies the desired bound condition.

\paragraph*{\ref{M-Assumption E: covariance kernel}}
We have
\begin{equation*}
    \psi_{\x}(v;\mathbf{Z}) = S_0(\x|\mathbf{A})\frac{(\1\{\check{X}\leq \x +v\}-\1\{\check{X}\leq \x\})\Delta}{S_0(\check{X}|\mathbf{A})G_0(\check{X}|\mathbf{A})}  + O(|v|)
\end{equation*}
and the first display follows as in the independent censoring case. For the covariance kernel,
\begin{align*}
    \E[\psi_{\x_n}(s\eta_n;\mathbf{Z})\psi_{\x_n}&(t\eta_n;\mathbf{Z})] = \E\bigg[S_0(\x_n|\mathbf{A})^2 \int_{\x_n}^{\x_n +\eta_n (s\land t)} \frac{f_{X|\mathbf{A}}(u|\mathbf{A})}{S_0(u|\mathbf{A})^2G_0(u|\mathbf{A})}du \1\{s,t>0\} \bigg]\\
    &+ \E\bigg[S_0(\x_n|\mathbf{A})^2 \int_{\x_n +\eta_n (s\lor t)}^{\x_n} \frac{f_{X|\mathbf{A}}(u|\mathbf{A})}{S_0(u|\mathbf{A})^2G_0(u|\mathbf{A})}du \1\{s,t<0\} \bigg] + O(\eta_n^2)
\end{align*}
and $\eta_n^{-1}\E[\psi_{\x_n}(s\eta_n;\mathbf{Z})\psi_{\x_n}(t\eta_n;\mathbf{Z})]$ converges to $\E[\frac{f_{X|\mathbf{A}}(\x|\mathbf{A})}{G_0(\x|\mathbf{A})}] (|s|\land|t|) \1\{\mathrm{sign}(s)=\mathrm{sign}(t)\}$.

\paragraph*{\ref{M-Assumption B: covariance kernel}}
$\mathcal{C}_{\x}(1,1)>0$ follows from $\E[f_{X|\mathbf{A}}(\x|A)/G_0(\x|A)]>0$. $\lim_{\eta\downarrow0}\mathcal{C}_{\x}(1,\eta)/\sqrt{\eta}=0$ follows from the same computation as in the no censoring case. The remaining conditions follow from verifying \ref{M-Assumption E: covariance kernel}.
\smallskip
\newline
\ref{M-Assumption B: uhat}, \ref{M-Assumption B: Phi uhat}, and \ref{M-Assumption B: jump of Phi hat} hold since in this example, $\widehat{u}_n=\widehat{u}_n^*=u_0$ and $\widehat{\Phi}_n,\widehat{\Phi}_n^*$ are the identity map.



\section{Additional example: monotone hazard function}
Let $X$ be a non-negative random variable, $f_0$ be its Lebesgue density, and $S_0(x)=\mathbb{P}[X>x]$ be its survival function. We consider the parameter of estimating the hazard function of $X$, $\theta_0(\x)=f_0(\x)/S_0(\x)$, with possible right-censoring as in the monotone density function example. Observations $\mathbf{Z}_1,\dots,\mathbf{Z}_n$ come from a random sample of $\mathbf{Z}=(\check{X},\Delta)'$ where $\check{X}=\min\{X,C\}$ and $\Delta=\1\{X\leq C\}$, $C$ being a random censoring time. As pointed out by \cite{Westling-Carone_2020_AoS}, with strictly increasing $\Phi_0$, the function $\Gamma_0$ takes the form $\Gamma_0(x) = \int_0^x \frac{f_0(u)}{S_0(u)}\Phi_0(du)$, and by taking $\Phi_0(x)=\int_0^xS_0(u)du$, $\Gamma_0(x) =F_0(x)=\mathbb{P}[X\leq x]$. Since $\Gamma_0$ is identical to the monotone density case with the choice $\Phi_0=\int_0^xS_0(u)du$, we can leverage the analysis for the monotone density. The interval $I$ equals $[0,u_0^{\mathtt{MD}}]$ where $u_0^{\mathtt{MD}}$ is $u_0$ in the monotone density example. The $u_0$ for the monotone hazard function estimation is $u_0=\Phi_0(u_0^{\mathtt{MD}})$.


Consider the case of completely random censoring i.e.,\ $X\indep C$. As in the setup for Corollary \ref{M-[Corollary] monotone density independent right-censoring}, let $\widehat{S}_n(x)$ be the Kaplan-Meier estimator for $S_0(x)=1-F_0(x)=\mathbb{P}[X> x]$, $\widehat{F}_n=1-\widehat{S}_n$, and $\widehat{G}_n$ be the Kaplan-Meier estimator for $G_0(x)=\mathbb{P}[C>x]$. Also,
\begin{equation*}
    \gamma_0(x;\mathbf{Z}) = F_0(x) + S_0(x)\left[\frac{\Delta\1\{\check{X}\leq x\}}{S_0(\check{X})G_0(\check{X})} - \int_0^{\check{X}\land x}\frac{\Lambda_0(du)}{S_0(u)G_0(u)} \right]
\end{equation*}
and $\phi_0(x;\mathbf{Z}) =x-\int_0^x \gamma_0(u;\mathbf{Z})du$.
\begin{corollary}\label{Appendix Corollary: Monotone Hazard Function Independent Right-Censoring}
    Suppose that the hypothesis of Corollary \ref{M-[Corollary] monotone density independent right-censoring} and Assumption BW hold. Then, Assumptions \ref{M-Assumption A} and \ref{M-Assumption B} hold with
    \begin{equation*}
        \widehat{\Gamma}_n(x)=1-\widehat{S}_n(x),\quad \widehat{\Gamma}_n^*(x)=\frac{1}{n}\sum_{i=1}^nW_{i,n}\widehat{\gamma}_n(x;\mathbf{Z}_i),
    \end{equation*}
    \begin{equation*}
        \widehat{\gamma}_n(x;\mathbf{Z}) = \widehat{F}_n(x) + \widehat{S}_n(x)\left[\frac{\Delta\1\{\check{X}\leq x\}}{\widehat{S}_n(\check{X})\widehat{G}_n(\check{X})} - \int_0^{\check{X}\land x} \frac{\widehat{\Lambda}_n(du)}{\widehat{S}_n(u)\widehat{G}_n(u)}\right],
    \end{equation*}
    \begin{equation*}
        \widehat{\Phi}_n(x)= \int_0^x \widehat{F}_n(u) du,\quad \widehat{\Phi}_n^*(x)=\int_0^x [1-\widehat{\Gamma}_n^*(u)]du = \frac{1}{n}\sum_{i=1}^n W_{i,n} \widehat{\phi}_n(x;\mathbf{Z}_i),
    \end{equation*}
    \begin{equation*}
        \widehat{\phi}_n(x;\mathbf{Z}) = x-\int_0^x \widehat{\gamma}_n(u;\mathbf{Z})du,\quad \widehat{u}_n=\widehat{u}_n^*=\widehat{\Phi}_n(u_0^{\mathtt{MD}}),
    \end{equation*}
    \begin{equation*}
        \mathcal{C}_{\x}(s,t) = \frac{f_0(\x)}{G_0(\x)} (|s| \land |t|) \1\{\mathrm{sign}(s)=\mathrm{sign}(t)\},\quad \mathcal{D}_{\q}(\x) = \frac{S_0(\x)\partial^\q f_0(\x)}{(\q+1)!}.
\end{equation*}
\end{corollary}

\subsection{Proof of Corollary \ref{Appendix Corollary: Monotone Hazard Function Independent Right-Censoring}}

We use the same $\widehat{\gamma}_n$ function and assumptions as in the monotone density setting. Also, the covariance kernels are the same as in the monotone density case. Thus, \ref{M-Assumption E: Gamma hat} and part of \ref{M-Assumption B: covariance kernel} follow from the same argument. We focus on \ref{M-Assumption E: Phi hat}-\ref{M-Assumption E: covariance kernel} and \ref{M-Assumption B: uhat}-\ref{M-Assumption B: jump of Phi hat}.

\paragraph*{\ref{M-Assumption E: Phi hat}}
Since
\begin{equation*}
    \widehat{\phi}_n(x;\mathbf{Z}) -\phi_0(x;\mathbf{Z})= -\int_0^x [\widehat{\gamma}_n(u;\mathbf{Z})-\gamma_0(u;\mathbf{Z})]du,
\end{equation*}
\begin{equation*}
    \frac{1}{n}\sum_{i=1}^n\sup_{x\in I}|\widehat{\phi}_n(x;\mathbf{Z}_i)-\phi_0(x;\mathbf{Z}_i)|^2=o_{\P}(1),\ a_n\frac{1}{n}\sum_{i=1}^n \sup_{x\in I_{\x}^{\delta}}|\widehat{\phi}_n(x;\mathbf{Z}_i)-\phi_0(x;\mathbf{Z}_i)|^2=o_{\P}(1)
\end{equation*}
follow from $a_n\frac{1}{n}\sum_{i=1}^n\sup_{x\in I}\vert\widehat{\gamma}_n(x;\mathbf{Z}_i) - \gamma_0(x;\mathbf{Z}_i)\vert^2$, which was verified in Section \ref{Appendix subsection: proof of Theorem monotone density independent right-censoring}.
To check $\sup_{x\in I}|\widehat{\Phi}_n(x)-\Phi_0(x)|=o_{\P}(1)$, $\sup_{x\in I}|\frac{1}{n}\sum_{i=1}^n\phi_0(x;\mathbf{Z}_i)-\Phi_0(x)|=o_{\P}(1)$ follows from Glivenko-Cantelli, and
\begin{equation*}
    \sup_{x\in I}\bigg|\frac{1}{n}\sum_{i=1}^n \big[\widehat{\phi}_n(x;\mathbf{Z}_i)-\phi_0(x;\mathbf{Z}_i)\big]\bigg|\leq \frac{1}{n}\sum_{i=1}^n \sup_{x\in I}|\widehat{\gamma}_n(x;\mathbf{Z}_i)-\gamma_0(x;\mathbf{Z}_i)| = o_{\P}(1),
\end{equation*}
where the last equality follows from $\frac{1}{n}\sum_{i=1}^n\sup_{x\in I}|\widehat{\gamma}_n(x;\mathbf{Z}_i)-\gamma_0(x;\mathbf{Z}_i)|^2=o_{\P}(1)$. Now $\sup_{x\in I}|\widehat{\Phi}_n(x)-\Phi_0(x)|=o_{\P}(1)$ follows by the triangle inequality.

For $|v|\leq V$
\begin{align*}
    \big\vert\widehat{\Phi}_n(\x + v) -& \widehat{\Phi}_n(\x) - \bar{\Phi}_n(\x+v) + \bar{\Phi}_n(\x) \big\vert = \left\vert\int_{\x-v}^{\x+v} [\widehat{\Gamma}_n(u)-\bar{\Gamma}_n(u)]du \right\vert  \\
    &\leq 2\vert V\vert \Big( \sup_{\vert v\vert\leq \vert V\vert } \vert \widehat{\Gamma}_n(\x+v)-\widehat{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+v) + \bar{\Gamma}_n(\x)\vert + \vert \widehat{\Gamma}_n(\x)-\bar{\Gamma}_n(\x)\vert \Big)
\end{align*}
Using the argument in Section \ref{Appendix subsection: proof of Theorem monotone density independent right-censoring}, we can bound $\sup_{\vert v\vert\leq \vert V\vert } \vert \widehat{\Gamma}_n(\x+v)-\widehat{\Gamma}_n(\x) - \bar{\Gamma}_n(\x+v) + \bar{\Gamma}_n(\x)\vert$. Then, $\sqrt{n} \vert \widehat{\Gamma}_n(\x)-\bar{\Gamma}_n(\x)\vert=O_{\P}(1)$ implies
\begin{equation*}
    \sqrt{n a_n}\sup_{|v|\leq V} \big\vert  \widehat{\Phi}_n(\x + v) -\widehat{\Phi}_n(\x) - \bar{\Phi}_n(\x+v) + \bar{\Phi}_n(\x)\big\vert \leq o_{\mathbb{P}}(1) + V O_{\mathbb{P}}\big(\sqrt{a_n}\big)
\end{equation*}
uniformly over $V\in (0,2\delta]$.
Theorem 1 of \cite{Lo-Singh_1986_PTRF} implies $\sqrt{n a_n}\sup_{x\in I}\vert \check{\Phi}_n(x ) - \check{\Phi}_n(\x ) - \bar{\Phi}_n(x)+\bar{\Phi}_n(\x )  \vert =o_{\P}(1)$.

The conditions on the uniform covering number hold because $\gamma_0$ and $\widehat{\gamma}_n$ are bounded (for $\widehat{\gamma}_n$, with probability approaching one) and thus $|\phi_0(x_1;\mathbf{Z})-\phi_0(x_2;\mathbf{Z})|\leq C |x_1-x_2|$
and $|\widehat{\phi}_n(x_1;\mathbf{Z})-\widehat{\phi}_n(x_2;\mathbf{Z})|\leq C |x_1-x_2|$ with probability approaching one. By this Lipschitz property, the condition on $\bar{D}_{\phi}^{\eta}(\mathbf{Z})$ also holds.

\paragraph*{\ref{M-Assumption E: covariance kernel}}
Let $\psi_{\x}^{\mathtt{MD}}(v;\mathbf{Z})=\gamma_0(\x +v;\mathbf{Z})-\gamma_0(\x;\mathbf{Z}) - \theta_0(\x)v$ be the $\psi_{\x}$ function for the monotone density. Then, for $x$ sufficiently close to $\x$ and $|v|$ small enough,
\begin{align*}
    \psi_{x}(v;\mathbf{z}) &=\gamma_0(x+v;\mathbf{z})-\gamma_0(x;\mathbf{z}) -\theta_0(x)[\phi_0(x+v;\mathbf{z})-\phi_0(x;\mathbf{z})]\\
    &= \psi_{x}^{\mathtt{MD}}(v;\mathbf{Z}) + \theta_0(x) \int_{x}^{x+v} \gamma_0(u;\mathbf{Z}) du = \psi_{x}^{\mathtt{MD}}(v;\mathbf{Z}) + O(|v|).
\end{align*}
Then, the same argument as in the monotone density case implies the desired result.\smallskip
\newline
\ref{M-Assumption B: uhat} follows from consistency of $\widehat{\Phi}_n$ and $\widehat{\Phi}_n^*$.

\paragraph*{\ref{M-Assumption B: Phi uhat}}
$\widehat{\Phi}_n(x)=\int_0^x \widehat{F}_n(u)du,\widehat{\Phi}_n^*(x)=\int_0^x 1-\widehat{\Gamma}_n^*(u)du$ are non-negative since $\widehat{F}_n\geq 0$ and $1-\widehat{\Gamma}_n^*\geq 0$ with probability approaching one. This property also implies the non-decreasing property as  $\widehat{\Phi}_n,\widehat{\Phi}_n^*$ are integrals. The continuity property also follows from the integral representation.
By definition, $\widehat{\Phi}_n(0)=0=\widehat{\Phi}_n^*(0)$ and $\widehat{\Phi}_n(u_0^{\mathtt{MD}})=\widehat{u}_n=\widehat{u}_n^*=\widehat{\Phi}_n^*(u_0^{\mathtt{MD}})$ with $I=[0,u_0^{\mathtt{MD}}]$.
The closedness of the range follows from continuity and $I$ being a compact interval.\smallskip
\newline
\ref{M-Assumption B: jump of Phi hat} follows from continuity of $\widehat{\Phi}_n$ and $\widehat{\Phi}_n^*$.

\section{Additional example: distribution function estimation with current status data}\label{Appendix Section: Example current status}
We consider the problem of estimating the cdf of $X$ at $\x$, $\theta_0(\x)=F_0(\x)$. Observations $\mathbf{Z}_1,\dots,\mathbf{Z}_n$ come from a random sample of $\mathbf{Z}=(\Delta,C,\mathbf{A}')'$ where $\Delta=\1\{X\leq C\}$, $C$ is a random censoring time, and $\mathbf{A}$ is a vector of covariates. In this example, we do not observe $\check{X}=X\land C$. Instead, we observe the censoring time and whether the observation was censored. This setup is often referred to as current status data.
Let $H_0(x)=\P[C\leq x]$ be the cdf of $C$. We can use $\Gamma_0(x) = \int_0^x F_0(u) H_0(du)$ and $\Phi_0(x)=H_0(x)$. The interval $I$ is the support of $X$ and $u_0=1$. We also assume $H_0$ admits a Lebesgue density $h_0$.
The structure of the estimation problem turns out to be identical to the one for the monotone regression example, and we can leverage the common structure.

\subsection{Independent right-censoring}
First we consider the case of completely at random censoring $X\indep C$. See \cite{Groeneboom-Wellner_1992_Book} for existing analysis. In this exaple, we do not use covariates $\mathbf{A}$. We set $\gamma_0(x;\mathbf{Z})=\Delta \1\{C\leq x\}$ and $\phi_0(x;\mathbf{Z})=\1\{C\leq x\}$. Note that if the notation is mapped by $(\Delta,C)\leftrightarrow (Y,X)$, then these functions are identical to those of the classical monotone regression problem (Corollary \ref{M-[Corollary] classical monotone regression}). Thus, the following result is identical to Corollary \ref{M-[Corollary] classical monotone regression}, up to notation and some changes due to boundedness of $\Delta$.
\begin{corollary}\label{Appendix Corollary: Current Status Data Completely Independent Case}
    Let $\varepsilon=\Delta-\E[\Delta|C]$ and $\x$ be an interior point of $I$. Suppose that Assumption BW holds, $\theta_0=F_0$ satisfies Assumption \ref{M-Assumption A: theta0}, the cdf $\Phi_0=H_0$ satisfies Assumption \ref{M-Assumption A: Phi0}, and $\sigma_0^2(x) = \E[\varepsilon^2|C=x]$ is continuous and positive at $\x$. Then  Assumptions \ref{M-Assumption A} and \ref{M-Assumption B} hold with
    \begin{equation*}
        \widehat{\Gamma}_n(x) = \frac{1}{n}\sum_{i=1}^n \Delta \1\{C\leq x\},\quad \widehat{\Gamma}_n^*(x) = \frac{1}{n}\sum_{i=1}^n W_{i,n}\Delta \1\{C\leq x\},
    \end{equation*}
    \begin{equation*}
        \widehat{\Phi}_n(x) = \frac{1}{n}\sum_{i=1}^n  \1\{C\leq x\},\quad \widehat{\Phi}_n^*(x) = \frac{1}{n}\sum_{i=1}^n W_{i,n}\1\{C\leq x\},\quad \widehat{u}_n=\widehat{u}_n^*=1,
    \end{equation*}
    \begin{equation*}
        \mathcal{C}_{\x}(s,t) = h_0(\x) \sigma_0^2(\x) (|s| \land |t|)\1\{\mathrm{sign}(s)=\mathrm{sign}(t)\},\qquad \mathcal{D}_{\q}(\x) = \frac{h_0(\x)\partial^{\q}F_0(\x)}{(\q+1)!}.
\end{equation*}
\end{corollary}

\subsection{Conditionally independent right-censoring}

We consider the case where right-censoring is conditionally independent i.e.,\ $X\indep C|\mathbf{A}$. \cite{vanderVaart-vanderLaan_2006_IJB} analyzed this example as well as settings with time-varying covariates. We are focusing on time-invariant covariates.
Define $F_0(C,\mathbf{A})=\E[\Delta|C,\mathbf{A}]$ and $g_0(C,\mathbf{A})=\frac{h_{C|\mathbf{A}}(C|\mathbf{A})}{h_0(C)}$ where $h_{C|\mathbf{A}}$ is the conditional density of $C$ given $\mathbf{A}$ and $h_0$ is the marginal density of $C$. 
Let $\widehat{F}_n(c,\mathbf{a})$ and $\widehat{g}_n(c,\mathbf{a})$ be preliminary estimators for $F_0(c,\mathbf{a})$ and $g_0(c,\mathbf{a})$, respectively.
 
Identical to the censoring completely at random case, with appropriate changes in the notation (i.e.,\ $(\Delta, C)\leftrightarrow (Y,X)$), the setup is equivalent to that of the monotone regression with covariates. 

\paragraph*{Assumption \thesubsection} 
Let $\varepsilon=\Delta-\E[\Delta|C,\mathbf{A}]$, $\sigma_0^2(C,\mathbf{A})=\E[\varepsilon^2|C,\mathbf{A}]$, and $\eta >0$ be some fixed number.
\begin{enumerate}[label=\normalfont(\roman*),noitemsep,itemindent=*]
	\item $\x$ is in the interior of $I$, $\theta_0=F_0$ satisfies \ref{M-Assumption A: theta0}, and $\Phi_0=H_0$ satisfies \ref{M-Assumption A: Phi0}.

    \item The conditional distribution of $C$ given $\mathbf{A}$ has a bounded Lebesgue density $h_{C|A}$, and there is $c>0$ such that $g_0(C,\mathbf{A})\geq c$ with probability one.

    \item There exist $\delta>0$ and random variables $A_n=o_{\mathbb{P}}(a_n^{-1/2})$, $B_n=O_{\mathbb{P}}(1)$ such that $\sqrt{n}\sup_{|v|\leq V}|\widehat{\Gamma}_n(\x+v)-\widehat{\Gamma}_n(\x)-\Gamma_0(\x+v)+\Gamma_0(\x)|\leq A_n +B_nV$ for $V\in (0,2\delta]$. For each $n\geq 1$, $(\mathbf{Z}_1,\dots,\mathbf{Z}_n,\widehat{F}_n,\widehat{g}_n)\indep (W_{1,n},\dots, W_{n,n})$. Also, $a_n\frac{1}{n}\sum_{i=1}^n|\widehat{F}_n(C_i,\mathbf{A}_i)-F_0(C_i,\mathbf{A}_i)|^2=o_{\P}(1)$, $a_n\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n|\widehat{F}_n(C_i,\mathbf{A}_j)-F_0(C_i,\mathbf{A}_j)|^2=o_{\P}(1)$, $a_n\frac{1}{n}\sum_{i=1}^n|\widehat{g}_n(C_i,\mathbf{A}_i)-g_0(C_i,\mathbf{A}_i)|^2=o_{\P}(1)$.

    \item There exists a real-valued function $\bar{F}$ such that $|F_0(c_1,\mathbf{A})-F_0(c_2,\mathbf{A})|\leq |c_1-c_2|\bar{F}(\mathbf{A})$ for $|c_1-c_2|\leq \eta$ and $\E[\bar{F}(\mathbf{A})^2]<\infty$.

    \item $\E[\frac{\sigma_0^2(\x,\mathbf{A})}{g_0(\x,\mathbf{A})}]>0$, and there are real-valued functions $B,\omega$ such that $\E[B(\mathbf{A})]<\infty$, $\lim_{\eta\downarrow0}\omega(\eta)=0$, and for $|x-\x|\leq \eta$, $|\frac{\sigma_0^2(x,\mathbf{A})h_{C|A}(x|\mathbf{A})}{g_0(x,\mathbf{A})^2} -\frac{\sigma_0^2(\x,\mathbf{A})h_{C|A}(\x|\mathbf{A})}{g_0(\x,\mathbf{A})^2}| \leq\omega(|x-\x|) B(\mathbf{A})$.
\end{enumerate}

\begin{corollary}\label{Appendix Corollary: Current Status Data Conditionally Independence}
    Under Assumption \thesubsection and Assumption BW, Assumptions \ref{M-Assumption A} and \ref{M-Assumption B} hold with
    \begin{equation*}
        \widehat{\Gamma}_n(x)= \frac{1}{n}\sum_{i=1}^n \widehat{\gamma}_n(x;\mathbf{Z}_i),\quad  \widehat{\Gamma}_n^*(x)= \frac{1}{n}\sum_{i=1}^nW_{i,n} \widehat{\gamma}_n(x;\mathbf{Z}_i),
    \end{equation*}
    \begin{equation*}
    \widehat{\gamma}_n(x;\mathbf{Z})=\1\{C\leq x\}\bigg[\frac{\Delta-\widehat{F}_n(C,\mathbf{A})}{\widehat{g}_n(C,\mathbf{A})} + \frac{1}{n}\sum_{j=1}^n\widehat{F}_n(C,\mathbf{A}_j)\bigg],
\end{equation*}
\begin{equation*}
    \widehat{\Phi}_n(x) = \frac{1}{n}\sum_{i=1}^n \1\{C_i\leq x\},\quad \widehat{\Phi}_n^*(x) = \frac{1}{n}\sum_{i=1}^nW_{i,n} \1\{C_i\leq x\},\quad \widehat{u}_n=\widehat{u}_n^*=1,
\end{equation*}
\begin{equation*}
    \mathcal{C}_{\x}(s,t) = h_0(\x) \E\bigg[\frac{\sigma_0^2(\x,\mathbf{A})}{g_0(\x,\mathbf{A})}\bigg] (|s| \land |t|) \1\{\mathrm{sign}(s)=\mathrm{sign}(t)\},\qquad \mathcal{D}_{\q}(\x) = \frac{h_0(\x)\partial^{\q}F_0(\x)}{(\q+1)!}.
\end{equation*}
\end{corollary}

\subsection{Proof of Corollaries \ref{Appendix Corollary: Current Status Data Completely Independent Case} and \ref{Appendix Corollary: Current Status Data Conditionally Independence}}

As noted above, by mapping the notation $(\Delta, C)\leftrightarrow (Y,X)$, the arguments in Sections \ref{Appendix subsection: proof theorem classical monotone regression} and \ref{Appendix subsection: proof theorem monotone regression with covariates} directly apply to the current status estimators.

\section{Rule-of-thumb step size selection}

Here we develop a rule-of-thumb procedure to choose a step size for the bias-reduced numerical derivative estimator in the context of isotonic regression without covariates. Specifically, we consider the numerical derivative estimator
\begin{equation*}
    \widetilde{\mathcal{D}}_{j,n}^{\mathtt{BR}}(\x) = \epsilon_n^{-(j+1)}\sum_{k=1}^{\underline{\s}+1} \lambda_j^{\mathtt{BR}}(k)[\widehat{\Upsilon}_n(\x+c_k\epsilon_n)-\widehat{\Upsilon}_n(\x)]
\end{equation*}
with $\underline{\s}=3$, $c_1=1,c_2=-1,c_3=2,c_4=-2$. Then,
\begin{equation*}
    \lambda_1^{\mathtt{BR}}(1)=\frac{2}{3}=\lambda_1^{\mathtt{BR}}(2),\quad \lambda_1^{\mathtt{BR}}(3)=-\frac{1}{24}=\lambda_1^{\mathtt{BR}}(4),
\end{equation*}
\begin{equation*}
    \lambda_3^{\mathtt{BR}}(1)=-\frac{1}{6}=\lambda_3^{\mathtt{BR}}(2),\quad \lambda_3^{\mathtt{BR}}(3)=\frac{1}{24}=\lambda_3^{\mathtt{BR}}(4).
\end{equation*}
We use the (asymptotic) MSE-optimal step size discussed in the main paper. See also \ref{Appendix Section: higher-order expansion}. Yet, with the choice of $c_k$'s, part of the bias constant $\sum_{k=1}^{\underline{\s}+1}\lambda_j^{\mathtt{BR}}(k)c_k^{\underline{s}+2}$ equals zero, and we need to turn to the next leading term of the bias, which is
\begin{equation*}
    \epsilon_n^{\underline{\s}+2-j} \frac{\partial^{\underline{\s}+3}\Upsilon_0(\x)}{(\underline{\s}+3)!} \sum_{k=1}^{\underline{\s}+1}\lambda_j^{\mathtt{BR}}(k)c_k^{\underline{s}+3}.
\end{equation*}
Then, letting $\mathsf{B}_j^{\mathtt{BR}}(\x) = \frac{\partial^{6}\Upsilon_0(\x)}{6!} \sum_{k=1}^{4}\lambda_j^{\mathtt{BR}}(k)c_k^{6}$, the MSE-optmal step size is
\begin{equation*}
    \epsilon_{j,n}^{\mathtt{BR}} = \bigg(\frac{(2j+1)\mathsf{V}_j^{\mathtt{BR}}(\x) }{2(5-j)\mathsf{B}_j^{\mathtt{BR}}(\x)^2}\bigg)^{1/11} n^{-1/11}.
\end{equation*}
The bias and variance constants depend on unknown features of the data generating process. Specifically, $\mathsf{B}_j^{\mathtt{BR}}(\x)$ depends on the regression function $\theta_0$, the Lebesgue density of $X$, and their derivatives at $X=\x$ while $\mathsf{V}_j^{\mathtt{BR}}(\x)$ is determined by the density of $X$ and the conditional variance of the regression error $\varepsilon=Y-\theta_0(X)$ at $X=\x$. To operationalize the construction of the step size, we posit a simple parametric model:
\begin{equation*}
    \E[Y|X] = \gamma_0 + \sum_{k=1}^5 \gamma_k (X-\x_0)^k, \quad X\sim\mathrm{Normal}(\mu,\sigma^2)
\end{equation*}
where $\{\gamma_0,\gamma_1,\gamma_2,\gamma_4,\gamma_4,\gamma_5,\mu,\sigma\}$ are parameters to be estimated. Once we estimate the parameters of this reference model, we can construct a rule-of-thumb step size $\epsilon_{j,n}^{\mathtt{ROT}}$ by replacing $\mathsf{B}_j^{\mathtt{BR}}(\x)$ and $\mathsf{V}_j^{\mathtt{BR}}(\x)$ with their estimates. Note that although the bias and variance constant estimators may not be consistent for the true $\mathsf{B}_j^{\mathtt{BR}}(\x)$ and $\mathsf{V}_j^{\mathtt{BR}}(\x)$, the rate of $\epsilon_{j,n}^{\mathtt{ROT}}$ is MSE-optimal, and the numerical derivative estimator converges to $\mathcal{D}_j(\x)$ sufficiently fast to satisfy Equation \eqref{M-Rate of convergence condition} in the main paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{imsart-nameyear.bst} 
\bibliography{CJN_2024_AOS--bib}

\makeatletter\@input{xx.tex}\makeatother
\end{document}