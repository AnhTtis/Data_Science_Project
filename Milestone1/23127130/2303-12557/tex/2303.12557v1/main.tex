\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
% table
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
% math added 
\DeclareMathOperator{\E}{\mathbb{E}}
\usepackage{booktabs}
% table
\usepackage{multirow}
%subfloat
\usepackage{subfig}

%footnote
\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

% comparison table 
\usepackage{pifont}
\usepackage{xcolor}
\newcommand{\cmark}{\textcolor{green!80!black}{\ding{51}}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}
\newcommand{\pmark}{\textcolor{blue}{\ding{115}}}
%--------
% algorithm
\usepackage[linesnumbered, ruled, lined]{algorithm2e} % to write algorithm
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
% vector
\usepackage{physics}



% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

 \iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{9269} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}


%%%%%%%%% TITLE
\title{
Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with Bridge Block Reconstruction}
%Bridge-wise Reconstruction for Post-Training Quantization of Hybrid Vision Transformer: Q-HyViT
%Q-BRIDGE: Bridge-wise Reconstruction Error Minimization in Hybrid Vision Transformer}
%\title{Q-BRIDGE: Bridge-wise Reconstruction Error Minimization in Hybrid Vision Transformer with Mixed Quantization Scheme and Granularity}
%\title{Bridge-wise Reconstruction Error Minimization in Hybrid Vision Transformer with Mixed Quantization Scheme and Granularity}
%\title{Systematic Quantization of Hybrid Vision Transformer Models}
%\title{Minimizing Reconstruction Error in Hybrid Vision Transformer through Mixed Scheme and Granularity of Post-Training Quantization}

% \author{Jemin Lee$^{\S}$ Yongin Kwon$^{\S}$ Jeman Park$^{\S}$ Misun Yu$^{\S}$ Hwanjun Song$^\dagger$\thanks{Corresponding author}\\
% $^\S$ETRI \qquad \qquad \qquad \qquad $^\dagger$AWS AI Labs\\
% Daejeon South Korea \qquad \qquad \qquad \qquad  Seattle USA \\
% {\tt\small \{leejaymin,yongin.kwon,jeman,msyu\}@etri.re.kr hwanjuns@amazon.com}}
% Daejeon South Korea Seattle USA }
%{\tt\small \{leejaymin,yongin.kwon,jeman,msyu\}@etri.re.kr hwanjuns@amazon.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
%  \\
% \\
% {\tt\small }
% }

\author{Jemin Lee$^{\S}$ \quad Yongin Kwon$^{\S}$ \quad Jeman Park$^{\S}$ \quad Misun Yu$^{\S}$ \\
$^\S$ETRI \\
%Daejeon South Korea \\
{\tt\small \{leejaymin,yongin.kwon,jeman,msyu\}@etri.re.kr}
% Daejeon South Korea Seattle USA }
%{\tt\small \{leejaymin,yongin.kwon,jeman,msyu\}@etri.re.kr hwanjuns@amazon.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and Hwanjun Song$^\dagger$\thanks{Corresponding author} \\ 
%$^\dagger$AWS AI Labs \\
$^\dagger$NAVER AI Lab \\
%Seattle USA\\
%{\tt\small hwanjuns@amazon.com }
{\tt\small ghkswns91@gmail.com }
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
Recently, vision transformers (ViT) have replaced convolutional neural network models in numerous tasks, including classification, detection, and segmentation. However, the high computational requirements of ViTs hinder their widespread implementation. To address this issue, researchers have proposed efficient hybrid transformer architectures that combine convolutional and transformer layers and optimize attention computation for linear complexity. Additionally, post-training quantization has been proposed as a means of mitigating computational demands. Combining quantization techniques and efficient hybrid transformer structures is crucial to maximize the acceleration of vision transformers on mobile devices. However, no prior investigation has applied quantization to efficient hybrid transformers.
In this paper, at first, we discover that the straightforward manner to apply the existing PTQ methods for ViT to efficient hybrid transformers results in a drastic accuracy drop due to the following challenges: (i) highly dynamic ranges, (ii) zero-point overflow, (iii) diverse normalization, and (iv) limited model parameters ($<$5M). 
To overcome these challenges, we propose a new post-training quantization method, which is the first to quantize efficient hybrid vision transformers (MobileViTv1 and MobileViTv2) with a significant margin (an average improvement of 7.75\%) compared to existing PTQ methods (EasyQuant, FQ-ViT, and PTQ4ViT). We plan to release our code at \url{https://github.com/Q-HyViT}.

% In recent times, vision transformers (ViT) have supplanted convolutional neural network models in numerous tasks, such as classification, detection, and segmentation. Nevertheless, the high computational requirements of ViTs pose a significant impediment to their widespread implementation. To address this issue, researchers have proposed efficient transformer architectures that combine convolutional and transformer layers and optimize attention computation for linear complexity. Furthermore, post-training quantization for vision transformers has also been proposed as a means of mitigating computational demands. To maximize the acceleration of vision transformers on mobile devices, the combining of quantization techniques and efficient transformer structures is crucial. However, there has been no prior investigation into applying quantization to efficient transformers. 
% Our study addresses this gap by directly applying existing post-training quantization techniques to hybrid vision transformers, which poses several unique challenges, such as high dynamic range, overflow, diverse normalization, and a limited number of parameters ($<$5M). 
% We propose a new post-training quantization technique that overcomes these challenges and is the first work to quantize hybrid vision transformers (MobileViTv1 and MobileViTv2) with a significant margin compared to existing techniques. 
% Our method reduces the accuracy drop concerning MobileViTv1 and MobileViTv2. Additionally, we will soon release our code as open-source software.
\end{abstract}

%\footnote[\asterisk]{Corresponding authors}

\begin{figure*}[t]
	\centering
	\includegraphics[width=2\columnwidth]{./figures/system_overview}
	%\includegraphics[width=1\columnwidth]{./fig/overview}
	\caption{Overall quantization process of Q-HyViT on the representative structure of hybrid vision transformers, including local, global, and bridge representation.}
	\label{fig:overview}
\end{figure*}


%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Thanks to self-attention that captures the global representation and shows better generalization with a low inductive bias, vision transformers (ViT) have substituted convolutional neural network models in numerous tasks, such as classification, detection, and segmentation~\cite{han2022survey, khan2022transformers}. 
Nevertheless, the high computational requirements of ViTs pose a significant impediment to their widespread implementation. 


To democratize the ViT on resource-constrained devices, researchers have proposed a hybrid vision transformer architecture that combines convolutional and transformer layers, such as MobileViTv1~\cite{mehta2021mobilevit}, and further optimized attention computation for linear complexity, such as MobileViTv2~\cite{mehta2022separable}. 
Additionally, quantization techniques that reduce the precision of real values have been used with efficient architecture design to achieve model compression. 
Such quantization techniques can be categorized into two types: quantization-aware training (QAT) and post-training quantization (PTQ). 
Despite QAT's accuracy preservation benefits over PTQ, its adoption has been limited due to privacy concerns, resource-intensive and time-consuming retraining processes, and the need for expertise in developing model architectures for hyper-parameter tuning~\cite{kris_whitepaper2018,steven2020,PACT2018,zhang2018lq,jung2019learning,zhou2016dorefa,jacob2018,songhan2016}. 
In practice, PTQ methods have been more commonly employed due to their applicability~\cite{kris_whitepaper2018, jiang2021automated, banner_neurips2019, choukroun2019low, zhao2019improving, lee2018quantization, goncharenko2019fast, migacz20178, wu2020integer}. 
PTQ allows pre-trained models to be calibrated without the need for re-training, using only a small unlabeled dataset. 
While PTQ for CNN models has been extensively studied, research on PTQ for vision transformers has recently gained significant attention. 
PTQ for ViT has been shown to preserve the accuracy of quantized models by handling diverse activation ranges after a non-linear function. However, those works considered only pure vision transformers such as ViT~\cite{dosovitskiy2020image}, DeiT~\cite{touvron2021training}, and Swin~\cite{liu2021swin}.


To maximize the acceleration of vision transformers on mobile devices, the integration of quantization techniques and efficient transformer structures is crucial. 
However, there has been no prior investigation into applying quantization to efficient transformers. 
%Our study addresses this gap by directly applying existing post-training quantization techniques to hybrid vision transformers, which poses the following challenges,
Although the existing PTQ can be directly applied to hybrid ViT, this is non-trial because of the four key differences from the canonical ViT:
%To investigate the potential of directly applying the existing PTQ to hybrid ViT, we identify four key issues that hinder existing PTQ: 
(i) the highly dynamic activation range of the hybrid ViT makes it difficult to preserve accuracy using existing methods developed for pure vision transformers; 
(ii) the bridge block has the gap between local and global representations and the zero-point overflow problem; 
(iii) the diverse normalization techniques are used in hybrid vision transformers; and 
(iv) models with less than 5 million parameters pose a significant loss of robustness from quantization due to their relatively low number of parameters and minimal residual connections.
By these differences, it is essential to adjust the granularity and scheme for both the bridge layer and each layer simultaneously, while also determining the optimal scaling factors for quantization.

%We propose a new post-training quantization technique that overcomes these challenges and is the first work to quantize hybrid vision transformers (MobileViTv1 and MobileViTv2) with a significant margin compared to existing techniques. 
%Our method reduces the accuracy drop concerning MobileViTv1 and MobileViTv2. Additionally, we will soon release our code as open-source software.

In this paper, we present Q-HyViT, a method for minimizing quantization error in hybrid vision transformers. 
Q-HyViT achieves this objective by selecting optimal scale factors, granularity, and scheme for both the bridge block and each layer based on reconstruction error from a loss degradation perspective.
In order to implement the relevant optimization in a systematic and integrated manner, we reuse the second-order term-based reconstruction error minimization method and extend it to incorporate the bridge block.
To the best of our knowledge, this is the first work that identifies the challenges of quantization for hybrid vision transformers and proposes a unified method to mitigate errors in post-training quantization. 

% how the algorithm works

% describe evaluation results
We conducted comprehensive experiments to compare the performance of existing open-source algorithms, EasyQuant~\cite{wu2020easyquant}, FQ-ViT~\cite{lin2022fq}, and PTQ4ViT~\cite{yuan2022ptq4vit}, on the same hybrid vision transformers in order to demonstrate the preservation of accuracy degradation caused by quantization in Q-HyViT. 
The hybrid vision transformers used in the experiment are MobileViTv1~\cite{mehta2021mobilevit}, which proposes the basic hybrid structure for the first time, and MobileViTv2~\cite{mehta2022separable}, which is a representative variant transformer for efficient attention operation.

The results demonstrate that our Q-HyViT performs exceptionally well across two types of hybrid vision transformers and outperforms existing quantization methods for ViT (EasyQuant, FQ-ViT, and PTQ4ViT) by a significant margin (up to an average improvement of 7.75\%). 
Compared to FQ-ViT, we highlight that Q-HyViT achieves the state-of-the-art accuracy on hybrid vision transformers in a fully quantized manner, including non-linear operations such as softmax and diverse normalization.

%- direct implementation
%- comparison in quantization
%- comparison in a fully quantized model 
%- ablation study

% ptq4vit
% Experiments show the quantized vision transformers (ViT, DeiT, and Swin) achieve near-lossless prediction accu- racy (less than 0.5% drop at 8-bit quantization) on the ImageNet classification task.
% apq
% Our APQ-ViT revisits the process of post-training quantization
%for vision transformer and presents novel insights. Comprehensive experiments on the large-scale computer vision tasks (image classifi- cation [7] and object detection [26]) demonstrate that our APQ-ViT performs remarkably well across various transformer architectures such as ViT [9], DeiT [40], and Swin Transformer [28], and sur- passes the existing methods by convincing margins, especially in lower bit-width settings (e.g., averagely up to 5.17% improvement for classification and 24.43% for detection on W4A4). We highlight that our APQ-ViT scheme achieves state-of-the-art accuracy per- formance on various bit-width settings, and enjoys versatility on diverse architectures and vision tasks.

The primary contributions of our work are listed below.
\begin{itemize}

\item We discover that quantization of hybrid vision transformers presents unique challenges: (i) the presence of highly dynamic activation ranges, (ii) zero-point overflow in the bridge block, (iii) diverse normalization, and (iv) a parameter count of less than 5 million.

\item We present a unified method called Q-HyViT, which is based on Hessian to adjust the granularity and scheme for both the bridge layer and each layer simultaneously, while also determining the optimal scaling factors for quantization.

\item To establish a baseline for comparison, we re-implement the existing PTQ methods for pure ViT, enabling them to be applied to hybrid vision transformers. We then evaluate Q-HyViT for large-scale image classification using various model variants of efficient hybrid vision transformers.
Our experimental results demonstrate the effectiveness of Q-HyViT in preserving the accuracy of quantized hybrid vision transformers, as it achieved better performance compared to the state-of-the-art PTQ methods: FQ-ViT~\cite{lin2022fq} and PTQ4ViT~\cite{yuan2022ptq4vit}.

\end{itemize}

% 5m under 의 모델을 시도했다. (cvpr18논문 논리)
% PTQ 이야기도 넣기 
\iffalse
CNN 방법론 적용시 -> 문제점
Transformer 방법론 적용시 -> 문제점
표를 만들어서 각 케이스별로 해결법을 구조적 나열 (기존 방법들의 조합 or a little bit new)
새로운 메소드를 제안x -> 기존의 기법들을 hybrid 생태게에 맞게 동작 시킨다. 간단하게 동작 
\fi
% 5m under 의 모델을 시도했다. (cvpr18논문 논리)


\section{Related work}
The design of model architecture and quantization have received substantial attention for efficient AI. 
We categorize prior research on efficient model architecture for computer vision quantization and quantization methods and compares our study's novelty with existing quantization methods.

\subsection{Efficient architectures for computer vision}
To avoid heavy computation in CNN, a standard convolution has been replaced by separable convolutions~\cite{chollet2017xception}.
Separable convolutions have been widely used when designing light-weight CNNs including MobileNets~\cite{howard2017mobilenets,sandler2018mobilenetv2,howard2019searching}, ShuffleNetv2~\cite{ma2018shufflenet}, and MNASNet~\cite{tan2019mnasnet}, as they structurally reduce the computational complexity of CNN operations.
Despite the prevalence of these models, one major drawback is that they are spatially local and have higher inductive bias than transformers.

%-> global and local representation (shape filter, texture filter) 
To stretch model capacity with low inductive bias, a vision transformer (ViT)~\cite{dosovitskiy2020image}, which directly applies a pure transformer to treat image patches as a sequence. 
Dosovitskiy \textit{et al.}~\cite{dosovitskiy2020image} showed that it performs exceptionally well on multiple image recognition benchmarks. 
Moreover, Touvron \textit{et al.}~\cite{touvron2021training} introduced a teacher-student strategy specific to transformers, resulting in competitive convolution-free transformers trained on ImageNet only. 

% hybrid vision transformer (v1, v2)
Even though pure vision transformer models achieve performance competitive to CNNs, the majority of these models are heavy-weight.
Recently, for lightweight vision transformers, Mehta \textit{et al.} proposed MobileViTv1 that showed better accuracy than light-weight CNNs at similar number of model parameters by hybridizing convolution and transformer.
Furthermore, Mehta \textit{et al.} proposed MobileViTv1 that used a separable self-attention method to reduce attention complexity ($O(n^2)$ to $O(n)$). 
Considering these active studies on efficient model architecture design, the hybrid vision transformer will continue to replace CNNs in many areas, and its usefulness will be significant in terms of optimizing the model simultaneously.


\subsection{Model Quantization}
Quantization-aware training (QAT) is a method used to reduce the accuracy drop from a quantized model by mapping high bit precision to low bit precision during training~\cite{kris_whitepaper2018,steven2020,PACT2018,zhang2018lq,jung2019learning,zhou2016dorefa,jacob2018,songhan2016}. This method involves a retraining procedure that takes a few epochs. As a result, CNN models can be quantized in low precision representation with little to no noticeable accuracy drop, even at 2 bits. However, QAT has several limitations. Firstly, retraining is time-consuming. Secondly, the training data may not always be accessible to third-party services. Finally, hyper-parameter tuning is complicated, and even using active and continual learning approaches does not completely mitigate these limitations~\cite{nguyen2017variational,doulamis2000line,shin2017fixed,dhaliwal2018effective}. For these reasons, we focus on post-training quantization, which does not require retraining based on the training data, making it practical for rapid deployment and its usage.


PTQ methods, which convert high-precision representation bits to low-precision bits without requiring retraining steps, have been extensively studied by researchers and widely adopted in practical scenarios~\cite{kris_whitepaper2018,jiang2021automated,banner_neurips2019,choukroun2019low,nagel2019data,zhao2019improving,lee2018quantization,goncharenko2019fast,meller19a,migacz20178,wu2020integer}. 
PTQ helps in the rapid deployment of CNN models on resource-constrained devices by addressing too time-consuming and data privacy issues associated with retraining. However, PTQ leads to significant accuracy degradation, particularly in low-precision representations, and prior research has mainly focused on \texttt{int8} quantization. 
As an effort to preserve the performance of a full-precision model, recent PTQ works~\cite{adaround20,brecq21,hubara2021accurate,wei2022qdrop,wangleveraging} have suggested to reconstruction error minimization of each layer or block by adjusting the magnitude of weight rounding and searching optimal scaling factors. 

Recently, quantization for vision transformers has been studied~\cite{liu2021post,lin2022fq,yuan2022ptq4vit,apq22}.
To reduce quantization error, such research works considered the special structure of vision transformers such as multi-head attention and layer normalization. 
Nevertheless, they do not consider the unique aspects of hybrid vision transformers which are the bridge block and diverse normalization.


%Several post-training quantization methods have been developed without requiring any training or fine-tuning. 
%Yoni \textit{et al.}~\cite{choukroun2019low} propose the OMSE method, which optimizes the L2 distance between the quantized tensor and the original tensor.
%Ron \textit{et al.}~\cite{banner_neurips2019} present the ACIQ method to analytically compute the clipping range and per-channel bit allocation for NNs.
%Zhao \textit{et al.}~\cite{zhong2019ada} propose the OCS method to address the outlier channel problem, while Wang et al.~\cite{wang2020towards} introduce the Bit-Split and Stitching framework and the Error Compensated Activation Quantization method for lower-bit post-training quantization. 
%Nagel \textit{et al.}~\cite{adaround20} propose AdaRound, a weight-rounding mechanism that adapts to the data and task loss, and Cai et al.~\cite{cai2020zeroq} introduce ZeroQ, which matches the statistics in batch normalization layers and utilizes a Pareto Frontier method for mixed-precision settings. 



%Our experimental results show that there is no universal configuration that always achieves the most accurate quantization for all CNN models. Rather, the optimal quantization strategy is dependent on the specific CNN model, and a naive parameter search can be time-consuming. To address this challenge, we propose a machine learning-based approach to finding the best configurations for each CNN model, which is the first study to do so.



\iffalse
\begin{itemize}
    \item Arxiv22.12,CNN, ViT PD-Quant
    \item Multimedia, ViT APQ-ViT
    \item Block-wise reconstruction error , Transformer (N)
    \item Neurips22, PLM Towards Efficient Post-training Quantization of pre-trained Language models
    \item Module-wise reconstruction error
    \item Neurips22, CNN] NWQ Network-wise reconstruction error
    \item CNN QDrop
    \item CNN AdaQunat
    \item CNN BRECQ
    \item CNN AdaRound
\end{itemize}
\fi



\section{Preliminary}
% show problematic distribution (observation)

\subsection{Hybrid vision transformer}
Fully transformer-based models for computer vision tasks have recently lead to impressive performance improvements in image classification and object detection, and also have shown the potential of reducing the inductive bias compared with CNNs.
However, the size of vision transformer models is commonly larger than CNN models on same performance. 
While reducing model size and keeping similar performance, Mehta \textit{et al.} proposed MobileViTv1~\cite{mehta2021mobilevit} which combined convolution and transformer.
For further improvement, MobileViTv2~\cite{mehta2022separable} has been proposed to tackle a bottleneck to compute an attention map of which complexity is $O(N^2)$. 

%Inductive bias (CNN+Transformer)
%MobileViTv1
%MobileViTv2: linear attention to reduce the complexity$O(N^2)$

\subsection{Quantization for Hybrid ViT}
Uniform quantization is a commonly used method to quantize neural networks, including convolution nets. and transformers.
A hybrid vision transformer consists of convolution and transformer modules. 
As shown in Fig.~\ref{fig:overview}, quantization for hybrid vision transformers are divided into three parts: convolution, bridge block, and transformer.
In uniform quantization, the weights and input activations of convolution are evenly quantized by each scale factor as: 
\begin{equation}
\vb{x} _{q} = \mathcal{Q}(\vb{x} _{r} ) = \text{clip}(\text{round} \left(\frac{\vb{x}_{r}}{\Delta_{\vb{x}}}+zp \right), \text{min}, \text{max}),
\label{lab:quantizer}
\end{equation}
where $\vb{x}_r$ is a real value (full precision) and $\vb{x}_q$ is a quantized value. $\Delta_{\vb{x}}$ is a scaling factor that is calculated depending on the quantization scheme: asymmetric or symmetric. 
Also, $zp$ denotes the zero point and exists only when using an asymmetric scheme.


In the case of transformers, input data is first passed through a quantized embedding layer before entering the transformer blocks, which consist of a multi-head self-attention (MHSA) and a feed-forward network (FFN). 
The MHSA module computes queries $\vb{Q}$, keys $\vb{K}$, and values $\vb{V}$ with their pre-trained weights $\vb{W}$ and inputs $\vb{X}$. 

In a specific quantized transformer layer, at first, the weights of $\vb{Q}$,$\vb{K}$,$\vb{V}$ are quantized, and then multi-head self-attention is calculated using $\vb{Q}$, $\vb{K}$, and softmax normalization.
% The computation of self-attention is represented as:
% \begin{equation}
%     \text{Attention}^{Q}(\vb{Q},\vb{K},\vb{V}) = \text{softmax}^{Q} \left( \frac{\vb{Q}_{q} \times \vb{K}^{T}_{q}}{\sqrt{\vb{d}}} \right) 
%     \label{eq:basic_attention}
% \end{equation}
% In Eq.~\ref{eq:basic_attention}, $d$ denotes the dimension of values. 
After the self-attention layer, FFN takes a quantized output which is concatenated from the results of MHSA as an input.


One popular method to reduce quantization error in post-training is reconstruction error minimization~\cite{yuan2022ptq4vit,adaround20,brecq21,apq22}.
Previous works optimized task loss $\mathcal{L}=\text{Cross  Entropy}(\hat{\vb{y}},\vb{y})$, where $\hat{\vb{y}}$ represents the quantized output and $\vb{y}$ denotes the full precision output which is used as ground truth in PTQ. 
The expectation of task loss is a function of network parameters $\vb{w}$, given by $\E[\mathcal{L}(\vb{x},\vb{y},\vb{w})]$, where $\vb{x}$ denotes activation and $\vb{y}$ denotes output. 
Quantization introduces a small perturbation $\epsilon$ on the parameter $\hat{\vb{w}} = \vb{w} + \epsilon$. 
As in prior works~\cite{yuan2022ptq4vit,adaround20,brecq21,apq22}, we calculate the influence of quantization on the task loss using Taylor series expansion as follows. 
\begin{equation}
    \E[\mathcal{L}(\vb{x},\vb{y},\hat{\vb{w}})] - \E[\mathcal{L}(\vb{x},\vb{y},\vb{w})] \approx \epsilon^{\intercal} \bar{g}^{(\vb{w})} + \frac{1}{2}\epsilon^{\intercal} \bar{H}^{(\vb{w})}\epsilon
    \label{eq:taylor}
\end{equation}

Since the weight perturbation $\epsilon$ is relatively small, a second-order term of Taylor expansion can be used. 
In this equation, $\bar{g}^{(\vb{w})} = \E[\nabla_{\vb{w}}\mathcal{L}(\vb{x},\vb{y},\hat{\vb{w}})]$ is the gradient and can be ignored if the pre-trained model is well-converged. 
$\bar{H}^{(\vb{w})} = \E[\nabla_{\vb{w}}^2\mathcal{L}(\vb{x},\vb{y},\hat{\vb{w}})]$ is the Hessian matrix. 
The goal is to find a quantizer that includes optimal scaling factors or a rounding scheme to minimize the influence, given by $\min{\E[\mathcal{L}(\vb{x},\vb{y},\hat{\vb{w}})] - \E[\mathcal{L}(\vb{x},\vb{y},\vb{w})]}$. 
However, directly minimizing task loss leads to overfitting problems due to small datasets during the calibration step. 
Thus, the second-order term of the Taylor series (Eq.(\ref{eq:taylor})) is used. 
Referring to BRECQ\cite{brecq21}, to reduce computational cost, Eq.(\ref{eq:taylor}) is simplified by removing the gradient ($\bar{g}^{(\vb{w})}$) and approximating $\epsilon=\Delta \vb{w}$ to network output ($\Delta O = \hat{O}-O$) as: 
\begin{equation}
    \epsilon^{\intercal} \bar{H}^{(x)}\epsilon \approx \Delta O^{\intercal}\bar{H}^{(O)}\Delta O
    \label{eq:rem_basic}
\end{equation}
Referring to previous works\cite{yuan2022ptq4vit,adaround20,brecq21,apq22}, MSE minimization based on the squared gradient that approximates the Hessian matrix captures the trend of task loss more accurately than other metrics such as MSE, Cosine, and Pearson.

We adopt the methodology described in~\cite{yuan2022ptq4vit,apq22,wu2020easyquant} to traverse a search space of scaling factors by linearly dividing the maximum-minimum range of $\vb{w}$ and $\vb{x}$ into $n$ candidates, represented as:
\begin{equation}
\begin{aligned}
    &[\alpha \frac{MAX|\vb{w}_{l}|}{2^{k-1}}, \beta \frac{MAX|\vb{w}_{l}|}{2^{k-1}}] \\ 
    &[\alpha \frac{MAX|\vb{x}_{l}|}{2^{k-1}}, \beta \frac{MAX|\vb{x}_{l}|}{2^{k-1}}],
\end{aligned}
\label{eq:candiates}
\end{equation}
where $\alpha$ and $\beta$ are utilized to control the number of candidates generated for scaling factors.


\begin{figure}[t]
\centering
%\subfloat[][]{\label{sub:minmax_layer}\includegraphics[width=.31\textwidth]{./figures/minmax_layer_quant}} %\vfill
\subfloat[][]{\label{sub:box_layer}\includegraphics[width=.4\textwidth]{./figures/15_channel_boxplot}} \vfill
\subfloat[][]{\label{sub:hist_layer}\includegraphics[width=.4\textwidth]{./figures/17_channel_boxplot}}
\caption{Per activation channel ranges of (a) the point-wise (\texttt{stage0.conv1x1.point.act}) and (b) depth-wise convolution (\texttt{stage0.convkxk.depth.act}) in the inverted bottleneck block in MobileViTv1-xxs}
\label{fig:layer_quant}
\end{figure}

\begin{figure}[t]
\centering
%\subfloat[][]{\label{sub:minmax_channel}\includegraphics[width=.33\textwidth]{./figures/minmax_channel_quant}} %\vfill
\subfloat[][]{\label{sub:hist_channel1_quant}\includegraphics[width=.4\textwidth]{./figures/17_hist_ch1}} \vfill
\subfloat[][]{\label{sub:hist_channel5_quant}\includegraphics[width=.4\textwidth]{./figures/17_hist_ch5}}
\caption{An overlapping histogram of quantized values (blue) and real values (orange) in the two channels of depth-wise convolution (\texttt{stage0.convkxk.depth.act}) in MobileViTv1-xxs: (a) the first channel and (b) fifth channel}
\label{fig:channel_quant}
\end{figure}

\section{Challenges of hybrid ViT quantization}
\label{sec:chall}
% Describe lessons 
%% Scheme: asymmetric problem
%% granularity: activation channel wise 

In this section, we identify four critical challenges that impede the quantization of hybrid vision transformers and explain why the current quantization method is insufficient.


\subsection{Highly dynamic activation range} 
To minimize accuracy degradation caused by large dynamic activation ranges, quantization granularity is adaptively changed depending on the channel distribution of each layer. 
The distribution of subsequent activations (\texttt{stage0.conv1x1.point.act}) following the point-wise convolution operation in the InvertedBottlenect block is illustrated in Fig.\ref{fig:layer_quant}\subref{sub:box_layer}.  
In this case, \texttt{stage0.conv1x1.point.act} shows the appropriate activation distribution for layer-wise quantization because the channel-wise distribution is not diverse.

Fig.\ref{fig:layer_quant}\subref{sub:hist_layer} illustrates the activation distribution (\texttt{stage0.convkxk.depth.act}) after the depth-wise convolution operation in the InvertedBottleneck block. 
%while Fig.\ref{fig:channel_quant}\subref{sub:hist_channel_quant} shows the result of layer-wise quantization. 
The distribution of activations is highly diverse, unlike in Fig.\ref{fig:layer_quant}\subref{sub:box_layer}, posing a problem when determining the scaling factor for quantization based on the minimum and maximum values of the activation range for the entire layer. 
For that reason, while the fifth channel activation of \texttt{stage0.convkxk.depth.act} is well quantized, all the values in the first channel activation are quantized to zero as shown in Fig.\ref{fig:channel_quant}\subref{sub:hist_channel1_quant} and Fig.\ref{fig:channel_quant}\subref{sub:hist_channel5_quant}.
 
Considering large variations to other channels, adjusting the scaling factor is difficult to compensate for this difference in activation ranges in layer-wise quantization. 
To mitigate the gap among channels, changing the quantization granularity is necessary. 
Applying layer-wise quantization to the activation \texttt{stage0.convkxk.depth.act}, the signal quantization noise ratio (SQNR) is observed to be $26.00$, and the channel-wise is significantly improved to $34.31$. 
Consequently, channel-wise quantization is appropriate for \texttt{stage0.convkxk.depth.act}.

% Quantization error introduced by \texttt{stage0.conv1x1.point.act} activation in a layer-wise manner. It is because the largest scaling factors are computed by using data from channel index 5 of \texttt{stage0.conv1x1.point.act}. If the divisor is large, the dividend will be zero after the rounding operation.
\iffalse
% mobilevit-v2 100
However, in some cases (mobilevit-v2 100), coarse-grained granularity is more adequate to reduce quantization error.
Therefore, considering quantization error, granularity should be carefully selected based on a hessian-guided manner.

% twin quantization problem 
High variation of dynamic activation range leads to an unprecedented range during the calibration step with a few training samples. To this end, the twin uniform quantization scheme is not working properly. 
\fi


\begin{figure*}[t]
	\centering
	\includegraphics[width=2\columnwidth]{./figures/59_channel}
	\caption{Per activation channel ranges of convolution in bridge block of MobileViTv1-xxs} 
 %make all values of channel index 1 to zero.} 
	\label{fig:small_range_clip}
\end{figure*}

\subsection{Zero-point overflow in bridge block}
Uniform quantization includes two types: asymmetric and symmetric, each with its own advantages and disadvantages. 
In most cases, asymmetric quantization shows better accuracy than symmetric quantization. 
Also, fine-grained granularity (channel-wise) is to produce better quantization results than coarse-grained manner (layer-wise). However, this is not always true. 
We have found that a severe accuracy drop occurs in the bridge block which is the transition part between convolution and transformer layers when using an asymmetric scheme or channel-wise granularity due to highly dynamic activation ranges according to each channel and non-zero distribution.
%specifically because the distribution is positively skewed, leading to an overflow. 

As shown in Fig.~\ref{fig:small_range_clip}, the activation of the bridge block convolution shows a similar range for both the maximum and minimum values across all channels, indicating that layer-wise quantization does not lead to significant accuracy degradation. 
However, when applying channel-wise quantization to distributions such as the 16th channel in Fig.~\ref{fig:small_range_clip}, where all values are greater than zero, overflow of zero-point value for asymmetric quantization occurs (ie., the zero-point value exceeded between -128 and 127). 
As shown in Fig.\ref{fig:scheme_overflow}, the clipped zero point is used, resulting in certain values being reconstructed as a single value. 
These issues manifest differently across models, necessitating an automated approach to carefully choose granularity and scheme.




 
\begin{figure}[t]
	\centering
	\includegraphics[width=.7\columnwidth]{./figures/59_hist_ch16}
	\caption{Problematic activation channel (index 16) of convolution in bridge block due to overflow of zero point when using channel-wise manner and asymmetric scheme} 
	\label{fig:scheme_overflow}
\end{figure}

\subsection{Diverse normalization} %% Batch-Norm, I-Norm, G-Norm 
Hybrid vision transformers, unlike pure models such as CNN and Vision Transformer, employ multiple normalization techniques. 
In the case of MobileViTv1, \textit{BatchNorm} and \textit{LayerNorm} are utilized, while MobileViTv2 uses \textit{BatchNorm} and \textit{GroupNorm}.
The computation of \textit{LayerNorm} requires dynamic computation to determine the mean and variance. 
To address this, prior work~\cite{lin2022fq} introduced \textit{I-layerNorm} for integer domain computation.
In this work, we propose \textit{I-GroupNorm}, which is specifically designed for fully quantized MobileViTv2 and can also be computed under the integer domain.

% how to extend I-layerNorm to I-GroupNorm 

\subsection{Sub-5M parameter models} %% analysis depending on model size
Compared to heavy CNN models such as ResNet and VGG series, lightweight models with a relatively small number of parameters and few residual connections are more susceptible to the quantization process. This is especially true for models that have less than 5M parameters, where the quantization error is significantly higher in hybrid vision transformers. As previously mentioned, using an asymmetric scheme for an activation distribution with a minimum value of 0 or greater can cause force clamping due to the zero point overflow problem, making the accuracy very sensitive to the quantization granularity and scheme.

\section{Methodology}
In this section, we introduce Q-HyViT which is a precise post-training quantization framework for hybrid vision transformers.
As shown in Fig.~\ref{fig:overview}, we describe Q-HyViT that minimizes the quantization error of hybrid vision transformers by selecting optimal scale factors, granularity, and scheme for both the bridge block and each layer based on reconstruction error from a loss degradation perspective.

\iffalse
%We begin by presenting the approximation of loss degradation using Taylor series expansion, and then describe Q-HyViT that optimally selects the scale factor, granularity, and scheme for minimizing reconstruction error according to the bridge block or layer.
hybrid vision transformer의 양자화 에러를 최소화 하기 위해서 loss degradation 관점에서의 bridge block 또는 Layer에 맞춰서 reconstruction 에러가 최소화 되도록 스케일 팩터, granularity, scheme각각 최적으로 선택하는 방법인 Q-HyViT에 대해서 설명한다. 
% key word를 정하고 방법론을 정리 해야함 
- loss degradation을 tayler로 근사화 하고 이것을 다시 hessian으로 처리하는 방법. 그리고 그 reconsturing 수준을 layer로 할 것이냐 block으로 할 것이냐를 정의한다. 
- loss degradation을 줄이기 위해서 기존에는 rounding scheme 사용함 
- scaling factor 최적화도 시도함 
- brdige block이 basic building blocks in neural networks and reconstructs them one-by-one 보다 좋다. 
%-- AdaRound
By approximating the task loss with a Taylor series expansion, the round- ing task is posed as a quadratic unconstrained bi- nary optimization problem.
%-- FlexRound
PTQ schemes based on reconstructing each layer or block output turn out to be effective to enhance quantized model performance
Accordingly, recent works (Nagel et al., 2020; Li et al., 2021; Hubara et al., 2021; Wei et al., 2022) have suggested to reconstruct each output of layer or block by devising and learning a new weight-rounding scheme, deviating from rounding-to-nearest, as an effort to preserve the performance of a full-precision model.
-- by devising and learning a new weight-rounding scheme, deviating from rounding-to-nearest, as an effort to preserve the performance of a full-precision model.
% -- BRECQ 
Recent works like (Nagel et al., 2020) recognized the problem and analyzed the loss degradation by Taylor series expansion. Analysis of the second-order error term indicates we can reconstruct each layer output to approximate the task loss degeneration.
% -- PTQ4ViT 
We also observe that common quantization metrics, such as MSE and cosine distance, are inac- curate to determine the optimal scaling factor
we propose to use a Hessian guided metric to evaluate different scaling factors, which improves the accuracy of calibration at a small cost. To
% -- APQ 
calibration metric 
\fi


% hessian guided search in convolution
% -- loss 분석, quantized loss (sequential gradient analysis) 
\subsection{Bridge block reconstruction error minimization}
Prior post-training quantization methods for vision transformers optimize quantization task loss by using reconstruction error minimization based on the second-order metric to evaluate each candidate scaling factor and learn a weight rounding method. 
Due to the challenges in hybrid vision transformers, previous post-training quantization methods have been imprecise in dealing with the bridge block, which contains a gap between local and global representation, and the high dynamic activation range resulting from the mixed structure of CNN and transformer.


% ------- It is better to insert figures to show evidence. like loss plane.
%In an ideal scenario, a carefully designed loss term would determine how to quantize each layer with the lowest quantization loss to converge to a global optimum. 
%Nonetheless, we observed that the second-order loss terms calculated by each candidate exhibit significant variance, especially at the bridge layer.
%Consequently, we propose bridge-wise reconstruction error minimization as a post-training quantization technique. This technique optimizes calibration on a bridge-wise basis, enabling the Hessian-guided loss to perceive the quantization error of neighboring layers in a transition block from CNN to a transformer.

To address these issues, Q-HyViT integrates bridge block and layer-wise reconstruction, and determines granularity and scheme for each layer in post-training quantization. As an example, the integration of the bridge block and layer-wise reconstruction ($O^{bb}$) can be represented as follows.
\begin{align}
O^{bb} &=
\begin{cases}
    \vb{w}_{n}^{bb}\vb{w}_{n-1}^{bb} \dots \vb{w}_{1}^{bb}\vb{x}^{bb}, & \text{if a layer is in a bridge block}\\
    \vb{w}^{\ell}\vb{x}^{\ell}, & \text{otherwise } bb \text{ is equal to } \ell    
\end{cases} 
\end{align}

We have redefined the existing reconstruction approach~\cite{brecq21,yuan2022ptq4vit,apq22} not only to adopt a hybrid architecture for lightweight vision transformers with minimum quantization error but also to carefully select quantization granularity and scheme for each layer due to the hybrid architecture's distinct characteristics, which cause larger errors.
Our reconstruction equation that considers the bridge block is as follows.
\begin{equation}
\begin{aligned}
    & \min\limits_{\Delta,g,s}{\E \left[\Delta O^{(bb),\intercal},\textbf{H}^{O^{(bb)}} \Delta O^{(bb)} \right]} \approx \\  
    & \min\limits_{\Delta,g,s}{\E \left[\Delta O^{(bb),\intercal}, \textit{diag} \left((\frac{\partial \textit{L}}{\partial O_{1}^{(bb)}})^2 ,\cdots, (\frac{\partial \textit{L}}{\partial O_{|O^{bb}|}^{(bb)}})^2 \right) \Delta O^{(bb)} \right]},
    \label{eq:rem}
\end{aligned}
\end{equation}
where $bb$ is $\in [BridgeBlock, a\text{ }layer]$.
In Eq.~\ref{eq:rem}, $\Delta O^{(bb)}$ is the difference between before and after quantization outputs.
$O_{n}^{(bb)}$ indicates the $n$-th element of $O^{bb}$. the range of $n$ is from $1$ to $O_{|O^{bb}|}$.
Therefore, we find optimal scaling factors($\Delta$) $\in [1,100]$, granularity($g$) $\in [layer,channel]$, and scheme($s$) $\in [asymmetric, symmetric]$ by approximating task loss using bridge block or layerwise reconstruction error minimization based on Eq.~\ref{eq:rem}.


\subsection{Framework}
Our proposed Q-HyViT method, outlined in Algorithm~\ref{algo:qhyvit}, leverages bridge block and layer-wise reconstruction to determine optimal scaling factors, granularity, and scheme, reducing quantization errors in hybrid vision transformers.
During the calibration process, Q-HyViT calculates the output and gradient of each bridge block and layer through forward and backward propagation and then optimizes all hybrid transformer layers by reducing reconstruction error.
As we mentioned in Sec.~\ref{sec:chall}, in the hybrid vision transformers, the dynamic activation range varies widely, necessitating the need for automatic adjustment methods. 
From this perspective, this approach minimizes quantization errors to improve the second-order metric, resulting in more accurate models.


\IncMargin{1em}
\begin{algorithm}[t]
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	\Indm
	\KwIn{Hybrid vision transformer model and a few images for calibration;}
	\KwOut{Optimal scaling factors($\Delta^{\ast}$) scheme($s$) and granularity($g$);}
	\Indp
        \While{a layer$(\ell)$ is not the end of layer}{
        \tcc{full-precision outputs on each layer including final layer $(\vb{y}^{(fp32)}$)}
        $O_{\ell}^{bb} \gets$ forward propagation $(\vb{w}_{\ell}\vb{x}_{\ell})$\;
	}
         \While{a layer$(\ell)$ is not the end of layer}{
            \If {a layer$(\ell)$ in a Bridge Block}{
		      Backward propagation to get $\frac{\partial \mathcal{L}}{\partial O^{bb}}$\;
	       }
            \Else{
                 Backward propagation to get $\frac{\partial \mathcal{L}}{\partial O^{\ell}}$\;
            }
	}
        \While{a layer$(\ell)$ is not the end of layer}{
            \tcc{initialize scaling factors}
            $\Delta^{\ast}_{\vb{w}_{\ell}^{bb}},\Delta^{\ast}_{\vb{x}_{\ell}^{bb}} \gets \frac{MAX(|\vb{w}_{\ell}^{bb}|)}{2^k}, \frac{MAX(|\vb{x}_{\ell}^{bb}|)}{2^k}$  \; 
            
            \tcc{Generate candidates for scaling factors} $\Delta_{\vb{w}_{\ell}^{bb}},\Delta_{\vb{x}_{\ell}^{bb}} \gets$ Eq.~(\ref{eq:candiates})\;
            \While{Three iterations}{
                \tcc{Determine granularity and scheme}
                $g,s \gets$ Eq.~(\ref{eq:rem})\;
                \tcc{Find optimal scaling factors}
                $\Delta_{\vb{w}_{\ell}^{bb}}^{\ast} \gets$ Eq.~(\ref{eq:rem})\;
                $\Delta_{\vb{x}_{\ell}^{bb}}^{\ast} \gets$ Eq.~(\ref{eq:rem})  		
            }
        }
        \Return{$\Delta^{\ast}$ , $g$ , $s$}
	\caption{The tuning process of Q-HyViT}
	\label{algo:qhyvit}
\end{algorithm}
\DecMargin{1em}

\begin{table*}[t]
\centering
\caption{Quantization accuracy of hybrid vision transformer models. ${}^\ast$ denotes that the model was not fully quantized. Unlike the original work, softmax and layer-norm remain under floating-point}
\label{table:accuracy}
%\resizebox{\columnwidth}{!}{
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccc
} %~\cite{migacz20178,wu2020integer}
\toprule
\multicolumn{1}{c}{\textbf{Model}} &
\multicolumn{1}{c}{\textbf{\# Params.}} &
\multicolumn{1}{c}{\textbf{Type}} &
\multicolumn{1}{c}{\textbf{FP32}} & 
%\multicolumn{1}{c}{\textbf{Liu21~\cite{liu2021post}}} &
\multicolumn{1}{c}{\textbf{EasyQuant~\cite{wu2020easyquant}}} &
\multicolumn{1}{c}{\textbf{${}^\ast$FQ-ViT~\cite{lin2022fq}}} & 
\multicolumn{1}{c}{\textbf{PTQ4ViT~\cite{yuan2022ptq4vit}}} & 
\multicolumn{1}{c}{\textbf{Ours}}
\\   \midrule
MobileViTv1-xxs & 1.3M & Hybrid & 68.91 & 36.19 & 0.12 & 42.23  & 68.45 (-0.46) \\ %68.45  \\ 
% ours
MobileViTv1-xs & 2.3M & Hybrid & 74.64 & 73.13 & 0.47 & 64.45  & 74.23 (-0.41) \\ % 74.29  \\ 
% ours
MobileViTv1-s & 5.6M & Hybrid & 78.31 & 74.22 & 0.48 & 69.48  & 77.98 (-0.33) \\ %77.92 \\
% ours 
MobileViTv2-050 & 1.4M & Hybrid & 70.16 & 66.79 & 0.08 & 14.61  & 68.20 (-1.96) \\ %68.22 \\
% ours
MobileViTv2-075 & 2.8M & Hybrid & 75.62 & 62.91 & 0.11 & 66.28  & 72.52 (-3.10) \\ %72.52 \\ 
% ours
%MobileViTv2-100 & 4.3M & Hybrid & 78.09 & 69.34 & 0.12 & 61.74  & 0.1 \\ %0.1 \\ 
% ours
\bottomrule
\end{tabular}
}
\end{table*}


\begin{table}[t]
\centering
\caption{Fully quantized accuracy of hybrid vision transformer models}
\label{table:full_accuracy}
\resizebox{\columnwidth}{!}{
%\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccc
} %~\cite{migacz20178,wu2020integer}
\toprule
\multicolumn{1}{c}{\textbf{Model}} &
\multicolumn{1}{c}{\textbf{\# Params.}} &
\multicolumn{1}{c}{\textbf{Type}} &
\multicolumn{1}{c}{\textbf{FP32}} & 
\multicolumn{1}{c}{\textbf{FQ-ViT~\cite{lin2022fq}}} & 
\multicolumn{1}{c}{\textbf{Ours}}
\\   \midrule
MobileViTv1-xxs & 1.3M & Hybrid & 68.91 & 0.1 & 67.20 \\ 
% ours
MobileViTv1-xs & 2.3M & Hybrid & 74.64 & 62.2 & 73.89 \\ 
% ours
MobileViTv1-s & 5.6M & Hybrid & 78.31 & 74.94 & 77.72 \\
% ours 
MobileViTv2-050 & 1.4M & Hybrid & 70.16 & 5.00 & 68.73 \\
% ours
MobileViTv2-075 & 2.8M & Hybrid & 75.62 & 34.60 & 74.36 \\
% ours
MobileViTv2-100 & 4.3M & Hybrid & 78.09 & 0.40 & 77.13 \\ 
% ours
\bottomrule
\end{tabular}
}
\end{table}

\section{Experimental results}
In this section, we compared the proposed method with several quantization algorithms. 
As we mentioned earlier, there is no comprehensive method to quantize the hybrid vision transformer.
% environment
Therefore, we directly implemented the following open-sourced algorithms, EasyQuant~\cite{wu2020easyquant}, FQ-ViT~\cite{lin2022fq}, and PTQ4ViT~\cite{yuan2022ptq4vit}, on hybrid vision transformer.

\subsection{Accuracy evaluation}
We selected MobileViTv1~\cite{mehta2021mobilevit} and MobileViTv2~\cite{mehta2022separable} as representative hybrid vision transformer architectures.
Table~\ref{table:accuracy} shows quantization results on different model parameters and architectures.
Upon examining the table, it is evident that previous works have experienced a significant decrease in accuracy, even with 8-bit quantization, in hybrid vision transformers. 
FQ-ViT shows severe accuracy degradation due to zero-point overflow problems in the Bridge Block and layer dependencies, despite not quantizing softmax and layer norm, unlike the original version. 
PTQ4ViT and EasyQuant achieved better accuracy than FQ-ViT by exploring scaling factors for each layer using Hessian and Cosine similarity to minimize reconstruction error. 
However, for models smaller than 2M, such as extremely lightweight models, and other cases where the dynamic range of activation in hybrid vision transformers varies drastically, quantization by layer does not properly consider such changes, resulting in significant accuracy degradation. 
In contrast, our Q-HyViT achieved less than 0.5\% accuracy drop with 8-bit quantization on the MobileViTv1 series (xxs, xs, and s).

Additionally, MobileViTv2 shows more accuracy drop than MobileViTv1, potentially due to its use of linear attention in self-attention computation, which results in fewer attention maps and lower resilience after post-softmax values. 
Furthermore, we observed that larger hybrid vision transformers are less sensitive to quantization, as evidenced by the accuracy drops of MobileVitv1-xxs, MobileVitv1-xs, and MobileVitv1-s, which were 0.50, 0.41, and 0.39, respectively. 
This phenomenon is attributed to larger networks having more weights and generating more activations, making them more resilient to perturbations caused by quantization.

% result on Fully quantized models
Previous studies~\cite{liu2021post, yuan2022ptq4vit, wu2020easyquant, apq22} have refrained from quantizing softmax and layer normalization operations due to their smaller computational demand compared to matrix multiplication in terms of total FLOPS. 
Moreover, straightforward quantization of such non-linear functions may result in considerable accuracy degradation.


Nonetheless, integer-only quantization~\cite{jacob2018quantization} is important for edge and mobile devices. It is because softmax and layer normalization rise dequantization to compute them under floating-point as well as data movement on off-chip memory.
However, a fully quantized approach is necessary to alleviate significant hardware design challenges that arise from reducing off-chip level data transfer. 
In line with previous research~\cite{lin2022fq}, we applied a fully quantized approach to hybrid vision transformers. However, FQ-VIT shows poor accuracy due to its use of an asymmetric scheme with zero points, which fails to handle the high variation of activation range by adjusting quantization granularity. 
Furthermore, in the case of MobileViTv2, group normalization is utilized instead of batch and layer norms, causing the existing L2 norm-based scaling factor exploration to function inaccurately. 
Our study addresses these issues and achieves an average of 43.63\% accuracy improvement. 






%ptq4vit, 8/32 = gradient =1 0.84064
\iffalse
\begin{table*}[t]
\centering
\caption{Quantization accuracy of pure vision transformer models}
\label{table:accuracy_hybrid}
%\resizebox{\columnwidth}{!}{
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccc
} %~\cite{migacz20178,wu2020integer}
\toprule
\multicolumn{1}{c}{\textbf{Model}} &
\multicolumn{1}{c}{\textbf{\# Params.}} &
\multicolumn{1}{c}{\textbf{Type}} &
\multicolumn{1}{c}{\textbf{FP32}} & 
\multicolumn{1}{c}{\textbf{Liu21~\cite{liu2021post}}} &
\multicolumn{1}{c}{\textbf{EasyQuant~\cite{wu2020easyquant}}} &
\multicolumn{1}{c}{\textbf{FQ-ViT~\cite{lin2022fq}}} & 
\multicolumn{1}{c}{\textbf{PTQ4ViT~\cite{yuan2022ptq4vit}}} & 
\multicolumn{1}{c}{\textbf{Ours}}
\\   \midrule
%FP32 (timm, vit_base_patch16_224), vit-paper(84.15(jax), JFT-300M) 
ViT-B   & 86M   & Pure & 84.53  & ?     & ? & 83.31 & 84.25 (84.09 8/32 0.8406 8/8) & ? \\
Swin-B  & 88M   & Pure & 83.60  & -     & ? & 82.38 & -                             & 82.782 \\
DeiT-B  & 86M   & Pure & 81.85  & 80.48 & ? & 80.85 & 81.48                         & 80.820 \\
Swin-S  & M     & Pure & 83.23  & -     & ? & 82.71 & 83.10                         &  \\
DeiT-S  & M     & Pure & 79.85  & 77.47 & ? & 79.17 & 79.47                         &  \\
Swin-T  & 29M   & Pure & 81.35  & -     & ? & 80.04 & -                             & 80.458 \\ 
DeiT-T  & 5.7M  & Pure & 72.21  & -     & ? & 71.07 & -                             & 71.086 \\ 
\bottomrule
\end{tabular}
}
\end{table*}
\fi

\iffalse
\begin{table*}[t]
\centering
\caption{Accuracy}
\label{table:accuracy_pure}
%\resizebox{\columnwidth}{!}{
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccc
} %~\cite{migacz20178,wu2020integer}
\toprule
\multicolumn{1}{c}{\textbf{Model}} &
\multicolumn{1}{c}{\textbf{\# Params.}} &
\multicolumn{1}{c}{\textbf{Type}} &
\multicolumn{1}{c}{\textbf{FP32}} & 
\multicolumn{1}{c}{\textbf{Liu21~\cite{liu2021post}}} &
\multicolumn{1}{c}{\textbf{EasyQuant~\cite{wu2020easyquant}}} &
\multicolumn{1}{c}{\textbf{FQ-ViT~\cite{lin2022fq}}} & 
\multicolumn{1}{c}{\textbf{PTQ4ViT~\cite{yuan2022ptq4vit}}} & 
\multicolumn{1}{c}{\textbf{Ours}}
\\   \midrule
%FP32 (timm, vit_base_patch16_224), vit-paper(84.15(jax), JFT-300M) 
ViT-B   & 86M   & Pure & 84.53  & ?     & ? & 83.31 & 84.25 (84.09 8/32 0.8406 8/8) & ? \\
Swin-B  & 88M   & Pure & 83.60  & -     & ? & 82.38 & -                             & 82.782 \\
DeiT-B  & 86M   & Pure & 81.85  & 80.48 & ? & 80.85 & 81.48                         & 80.820 \\
Swin-S  & M     & Pure & 83.23  & -     & ? & 82.71 & 83.10                         &  \\
DeiT-S  & M     & Pure & 79.85  & 77.47 & ? & 79.17 & 79.47                         &  \\
Swin-T  & 29M   & Pure & 81.35  & -     & ? & 80.04 & -                             & 80.458 \\ 
DeiT-T  & 5.7M  & Pure & 72.21  & -     & ? & 71.07 & -                             & 71.086 \\ 
\midrule
MobileViTv1-xxs & 1.3M & Hybrid & 68.908 & - & ? & 0.00  & 0.00 & 66.706 \\ 
% oursㅅ
MobileViTv1-xs & 2.3M & Hybrid & 74.638 & - & ? & 0.00  & 0.00 & 73.874 \\ 
% ours
MobileViTv1-s & 5.6M & Hybrid & 78.314 & - & ? & 0.00  & 0.00 & 77.722 \\
% ours 
MobileViTv2-050 & 1.4M & Hybrid & 70.160 & - & ? & 0.00  & 0.00 & 68.726 \\
% ours
MobileViTv2-075 & 2.8M & Hybrid & 75.616 & - & ? & 0.00  & 0.00 & 74.364 \\ 
% ours
MobileViTv2-100 & 4.3M & Hybrid & 78.086 & - & ? & 0.00  & 0.00 & 77.128 \\ 
% ours
\bottomrule
\end{tabular}
}
\end{table*}
\fi

\subsection{Ablation study}
We performed an ablation study on the effect of the proposed techniques, namely bridge block reconstruction error minimization, mixed scheme, and mixed granularity. 
The results are presented in Table~\ref{table:ablation}, which demonstrates that our proposed methods enhance the accuracy of quantized hybrid vision transformers.


The Bridge block reconstruction error minimization method alone does not yield significant performance improvements compared to the baseline (PTQ4ViT~\cite{yuan2022ptq4vit}) when only adjusting the scaling factor based on layer-wise reconstruction.
However, adjusting not only the scaling factors, but also the granularity and scheme based on the Bridge Block, could lead to synergistic effects.
As a result, combining them together could significantly make accuracy improvements.
Excluding mixed granularity quantization significantly reduced accuracy, highlighting highly dynamic activation ranges in lightweight hybrid vision transformers as the main cause of the accuracy drop.



%ablation study
\begin{table}[t]
\centering
\caption{Ablation study of the effect of the proposed methods, bridge block, mixed granularity, and mixed scheme.
\cmark denotes that the proposed method is applied.}
\label{table:ablation}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccc%rr
}
\toprule
\multicolumn{1}{c}{\textbf{Model}}  &
\multicolumn{1}{c}{\textbf{Bridge Block}} & \multicolumn{1}{c}{\textbf{Mixed Granularity}} & \multicolumn{1}{c}{\textbf{Mixed Scheme}} & \multicolumn{1}{c}{\textbf{Accuracy}} 
\\   \midrule
MobileViTv1-xxs & \xmark & \xmark & \xmark & 42.25  \\
MobileViTv1-xxs & \cmark & \xmark & \xmark & 42.35 \\
MobileViTv1-xxs & \cmark & \cmark & \xmark & 60.22 \\
MobileViTv1-xxs & \cmark & \cmark & \cmark & 68.45 \\
\midrule
MobileViTv1-xs & \xmark & \xmark & \xmark & 64.45  \\
MobileViTv1-xs & \cmark & \xmark & \xmark & 64.62 \\
MobileViTv1-xs & \cmark & \cmark & \xmark & 71.94 \\
MobileViTv1-xs & \cmark & \cmark & \cmark & 74.23 \\
\midrule
MobileViTv1-s & \xmark & \xmark & \xmark & 69.48 \\
MobileViTv1-s & \cmark & \xmark & \xmark & 69.57 \\
MobileViTv1-s & \cmark & \cmark & \xmark & 76.93 \\
MobileViTv1-s & \cmark & \cmark & \cmark & 77.98 \\
\midrule
MobileViTv2-050 & \xmark & \xmark & \xmark & 14.61  \\
MobileViTv2-050 & \cmark & \xmark & \xmark & 17.43 \\
MobileViTv2-050 & \cmark & \cmark & \xmark & 68.20 \\
MobileViTv2-050 & \cmark & \cmark & \cmark & 68.20 \\
\midrule
MobileViTv2-075 & \xmark & \xmark & \xmark & 66.28  \\
MobileViTv2-075 & \cmark & \xmark & \xmark & 66.30 \\
MobileViTv2-075 & \cmark & \cmark & \xmark & 72.52 \\
MobileViTv2-075 & \cmark & \cmark & \cmark & 72.52 \\
% \midrule
% MobileViTv2-100 & \cmark & \xmark & \xmark & 61.21 \\
% MobileViTv2-100 & \cmark & \cmark & \xmark & 0.1 \\
% MobileViTv2-100 & \cmark & \cmark & \cmark & 0.1 \\
\bottomrule

\end{tabular}
}
\end{table}

%------------------------------------------------------------------------
\section{Conclusion}
We addressed the problem of democratizing vision transformers on resource-constrained devices by proposing a method for minimizing quantization error in hybrid vision transformers. 
The proposed method, Q-HyViT, identified the challenges of applying post-training quantization (PTQ) to hybrid vision transformers and proposed a unified method to mitigate errors in PTQ. 
Q-HyViT achieved this by selecting optimal scale factors, granularity, and scheme for both the bridge block and each layer based on reconstruction error from a loss degradation perspective.
We demonstrated the effectiveness of Q-HyViT by conducting comprehensive experiments that compared its performance with existing open-source algorithms, EasyQuant, FQ-ViT, and PTQ4ViT, on the same hybrid vision transformers. 
The results showed that Q-HyViT outperformed existing methods by a significant margin and achieved state-of-the-art accuracy performance on hybrid vision transformers in a fully quantized manner, including non-linear operations such as softmax and diverse normalization.
Finally, we contributed to the field of artificial intelligence by identifying the unique challenges of quantizing hybrid vision transformers and proposing an effective solution for minimizing quantization error. 
Our work has significant implications for democratizing vision transformers on resource-constrained devices, making it possible to use these models in real-world applications where computational resources are limited.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\newpage

\section*{Appendix}
\setcounter{section}{0}

\def\thesection{\Alph{section}}


\section{Implementation Details}
For a fair comparison, we maintain most configurations consistent with EasyQuant, PTQ4ViT, and FQ-ViT. Specifically, our settings vary depending on whether the model is fully quantized or not.

\subsection{Settings for EasyQuant and PTQ4ViT}
In EasyQuant, we quantize all operators, including fully-connected layers and matrix multiplications. To obtain the optimal scaling factors, we employ a search algorithm based on cosine distance. The search space is derived from $\alpha$ = 0.5 and $\beta$ = 1.2.

In PTQ4ViT, we adjust the hyperparameters to $\alpha$ = 0 and $\beta$ = 1.2. Similar to PTQ4ViT, this study adopts the parallel quantization method to prevent a significant accuracy drop caused by small datasets.

\subsection{Settings for FQ-ViT}
Essentially, we perform symmetric quantization on a per-channel basis for weights and asymmetric quantization on a per-layer basis for activations. To ensure a fair comparison, we set the quantization for weights to the minimum and maximum values. The hyperparameter K in the Power-of-Two Factor remains unchanged. For the calibration process, we select a sample of 1,000 images.

\subsection{Settings for Q-HyViT}
We quantize all the weights and inputs for the fully connected layers, including the first projection layer and the last head layer. Additionally, the two input matrices for the matrix multiplications in the self-attention modules are quantized. The inputs of the softmax and normalization layers are also quantized, consistent with FQ-ViT. We use 32 images for calibration, and unoptimized scaling factors are initialized with minimum and maximum values.


\iffalse
\begin{figure}[t]
\centering
\subfloat[][]{\label{sub:hist_channel1_quant}\includegraphics[width=.4\textwidth]{./figures/59_channelwise}} \vfill
\subfloat[][]{\label{sub:hist_channel5_quant}\includegraphics[width=.4\textwidth]{./figures/59_layerwise}}
\caption{An overlapping histogram of quantized values (blue) and real values (orange) in the activation of  for 1st bridge block of MobileViTv1-xxs: (left) layer-wise quantization (right) channel-wise quantization}
\label{fig:channel_quant}
\end{figure}
\fi


\begin{figure}[h]
	\centering
	\includegraphics[width=1\columnwidth]{./figures/xxs_zeropoint_overflow_6plots}
	\caption{A histogram depicting the overlap between quantized values (blue) and real values (orange) for six activation layers in the 1st, 2nd, and 3rd bridge blocks of the MobileViTv1-xxs model} 
	\label{fig:xxs_overflow_granularity}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1\columnwidth]{./figures/xs_zeropoint_overflow_6plots}
	\caption{A histogram depicting the overlap between quantized values (blue) and real values (orange) for six activation layers in the 1st, 2nd, and 3rd bridge blocks of the MobileViTv1-xs model} 
	\label{fig:xs_overflow_granularity}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1\columnwidth]{./figures/s_zeropoint_overflow_6plots}
	\caption{A histogram depicting the overlap between quantized values (blue) and real values (orange) for six activation layers in the 1st, 2nd, and 3rd bridge blocks of the MobileViTv1-s model} 
	\label{fig:s_overflow_granularity}
\end{figure}


\section{Overflow Issues in Asymmetric Channel-wise Quantization}
In this section, we demonstrate that the zero-point overflow issue arises across multiple bridge blocks within each hybrid vision transformer model.
Two problematic activations are present in every bridge block.
Both MobileViTv1 and MobileViTv2 comprise three bridge blocks.
The distribution of activations before and after quantization for the convolution operations in each bridge block is illustrated in Fig.\ref{fig:xxs_overflow_granularity}, Fig.\ref{fig:xs_overflow_granularity}, and Fig.~\ref{fig:s_overflow_granularity}.

From the three figures, we have discovered that the impact of zero point overflow diminishes as our models increase in size. 
This is due to the fact that the influence of zero point overflow decreases as the number of channels expands, such as in the cases of 64 (xxs), 96 (xs), and 144 (s).
Regardless of the model size, it is clear that clamping of specific values continues to occur in the bridge block. 
However, the accuracy drop issue is mitigated as the model size increases.
In detail, for the FQ-ViT model, the accuracy is 0.1 for the xxs model, but it achieves accuracies of 62.2 and 74.94 for the xs and s models, respectively. In other words, although the same issue arises, as the model size increases, the impact on accuracy diminishes.
The reason for this mitigation is that the number of redundant parameters increases as the size of the model increases. This is similar to why pruning works well for large models with many redundant parameters such as ResNet.

Additionally, for smaller models, we have observed that the clamping issue associated with zero point overflow can be alleviated by transitioning from channel-wise quantization to layer-wise quantization, as demonstrated in Fig.~\ref{fig:overflow_granularity}.
In the end, the reason why the zero point overflows are clamped is due to the following:
\begin{align}
-128 &> q_{min} -\frac{r_{min}}{s} \nonumber \\
0 &> -\frac{r_{min}}{s} \nonumber \\ 
0 &< \frac{r_{min}}{s}
\label{eq:zero_proof}
\end{align}
In the above equation, $q_{min}$ represents the minimum value of 8-bit quantization, which is $-128$, $s$ is the scaling factor, and $r_{min}$ refers to the minimum value of the original activation. As shown in Eq.~\ref{eq:zero_proof}, since $s$ is always a positive value, if $r_{min}$ is greater than 0, the zero point exceeds the range of $q_{min}$ and becomes clamped.
Therefore, when quantizing on a per-channel basis, if activations are composed only of values greater than or equal to $0$, it significantly causes a decrease in accuracy. 
To solve this issue, using layer-wise quantization takes into account the entire layer, including values less than or equal to 0 as $r_{min}$. 
Consequently, the zero-point value falls within the 8-bit range ($-128$ to $127$).
  
 
%8bit의 경우 -128~127까지 숫자를 표현 할 수 있으므로 
\begin{figure}[h]
	\centering
	\includegraphics[width=1\columnwidth]{./figures/59_hist_layer_channel_comparison}
	\caption{An overlapping histogram of quantized values (blue) and real values (orange) in the activation of  for 1st bridge block of MobileViTv1-xxs: (left) layer-wise quantization (right) channel-wise quantization} 
	\label{fig:overflow_granularity}
\end{figure}

\end{document}