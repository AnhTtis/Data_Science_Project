%File: anonymous-submission-latex-2024.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
%\usepackage[submission]{aaai24}  % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
%\usepackage{algorithm}
%\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
% \usepackage{newfloat}
% \usepackage{listings}
% \DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
% \lstset{%
% 	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
% 	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
% 	aboveskip=0pt,belowskip=0pt,%
% 	showstringspaces=false,tabsize=2,breaklines=true}
% \floatstyle{ruled}
% \newfloat{listing}{tb}{lst}{}
% \floatname{listing}{Listing}

% table
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
% math added 
\DeclareMathOperator{\E}{\mathbb{E}}
\usepackage{booktabs}
% table
\usepackage{multirow}
%subfloat
\usepackage{subfig}
% table and figure 
\usepackage[export]{adjustbox}

%line break
\usepackage{makecell}

\renewcommand\theadalign{bc}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}

% comparison table 
\usepackage{pifont}
\usepackage{xcolor}
\newcommand{\cmark}{\textcolor{green!80!black}{\ding{51}}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}
\newcommand{\pmark}{\textcolor{blue}{\ding{115}}}
%--------
% algorithm
\usepackage[linesnumbered, ruled, lined]{algorithm2e} % to write algorithm
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
% vector
\usepackage{physics}


%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
\nocopyright %-- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{
Q-HyViT: Post-Training Quantization for Hybrid Vision Transformers \\with Bridge Block Reconstruction}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Jemin Lee\textsuperscript{\rm 1},
    Yongin Kwon\textsuperscript{\rm 1},
    Jeman Park\textsuperscript{\rm 1},
    Misun You\textsuperscript{\rm 1},
    Sihyeong Park\textsuperscript{\rm 2},
    Hwanjun Song\textsuperscript{\rm 3}\thanks{Corresponding author}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}ETRI, \quad
    \textsuperscript{\rm 2}KETI, \quad     
    \textsuperscript{\rm 3}Naver AI Lab \\ 
    {\{leejaymin,yongin.kwon,jeman,msyu\}@etri.re.kr} , \quad
    sihyeong@keti.re.kr, \quad
    ghkswns91@gmail.com\\

%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Recently, vision transformers (ViTs) have superseded convolutional neural networks in numerous applications, including classification, detection, and segmentation. However, the high computational requirements of ViTs hinder their widespread implementation. To address this issue, researchers have proposed efficient hybrid transformer architectures that combine convolutional and transformer layers with optimized attention computation of linear complexity. Additionally, post-training quantization has been proposed as a means of mitigating computational demands. For mobile devices, achieving optimal acceleration for ViTs necessitates the strategic integration of quantization techniques and efficient hybrid transformer structures. However, no prior investigation has applied quantization to efficient hybrid transformers.
In this paper, we discover that applying existing PTQ methods for ViTs to efficient hybrid transformers leads to a drastic accuracy drop, attributed to the four following challenges:  (i) highly dynamic ranges, (ii) zero-point overflow, (iii) diverse normalization, and (iv) limited model parameters ($<$5M). 
To overcome these challenges, we propose a new post-training quantization method, which is the first to quantize efficient hybrid ViTs (MobileViTv1, MobileViTv2, Mobile-Former, EfficientFormerV1, EfficientFormerV2) with a significant margin (an average improvement of 8.32\% for 8-bit and 26.02\% for 6-bit) compared to existing PTQ methods (EasyQuant, FQ-ViT, and PTQ4ViT). We plan to release our code at \url{https://github.com/Q-HyViT}.
\end{abstract}



%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Thanks to self-attention that captures the global representation and shows better generalization with a low inductive bias, vision transformers (ViTs) have substituted convolutional neural networks (CNNs) in numerous applications, such as image classification, object detection, and instance segmentation~\cite{han2022survey, khan2022transformers}. 
Despite the great success of ViT, the high computational requirement of ViTs still remains a significant impediment to their widespread implementation. 


To democratize the ViT on resource-constrained devices, researchers have proposed a \emph{hybrid} vision transformer architectures, where convolutional and transformer layers are combined, such as MobileViTv1~\cite{mehta2021mobilevit}, or optimized attention computation is used for linear complexity, such as MobileViTv2~\cite{mehta2022separable}. 
Additionally, quantization techniques that reduce the precision of real values have been used with efficient architecture design to achieve model compression. 
Such quantization techniques can be categorized into two types: {quantization-aware training} (QAT) and {post-training quantization} (PTQ). While QAT offers advantages in preserving accuracy compared to PTQ, its adoption has been restricted due to privacy concerns, resource-intensive and time-consuming re-training process, and the requisite expertise for hyperparameter tuning in model architecture development~\cite{kris_whitepaper2018,steven2020,PACT2018,zhang2018lq,jung2019learning,zhou2016dorefa,jacob2018,songhan2016}. 

In practical setup, PTQ methods have been more commonly employed due to their high applicability~\cite{kris_whitepaper2018, jiang2021automated, banner_neurips2019, choukroun2019low, zhao2019improving, lee2018quantization, goncharenko2019fast, migacz20178, wu2020integer}. 
PTQ allows pre-trained models to be calibrated without the need for re-training, using only a small unlabeled dataset. 
PTQ for CNN models was extensively studied, and recently, research on PTQ for vision transformers has gained significant attention. 
PTQ for ViTs has been shown to preserve the accuracy of quantized models by overcoming diverse activation ranges following a non-linear function. However, these studies have solely focused on {pure} vision transformers such as ViT~\cite{dosovitskiy2020image}, DeiT~\cite{touvron2021training}, and Swin~\cite{liu2021swin}.

\iffalse
\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{./figures/overview2}
	%\includegraphics[width=1\columnwidth]{./fig/overview}
	\caption{Overall quantization process of Q-HyViT on the representative structure of hybrid vision transformers, including local, global, and bridge representation.}
	\label{fig:overview}
\end{figure}
\fi

\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{./figures/overview4}
	%\includegraphics[width=1\columnwidth]{./fig/overview}
 \vspace*{-0.6cm}
	\caption{Overall quantization process of Q-HyViT on the representative structure of hybrid vision transformers, including local, global, and bridge representation.}
	\label{fig:overview}
 \vspace*{-0.5cm}
\end{figure}

For mobile devices, achieving optimal acceleration for ViTs necessitates the fusion of quantization techniques and efficient transformer structures. 
However, no prior exploration exists regarding the application of quantization to efficient {hybrid} transformers. 
%Our study addresses this gap by directly applying existing post-training quantization techniques to hybrid vision transformers, which poses the following challenges,
Although the existing PTQ can be directly applied to hybrid ViTs, this is non-trivial because of the four key differences of them from the canonical ViTs:
%To investigate the potential of directly applying the existing PTQ to hybrid ViT, we identify four key issues that hinder existing PTQ: 
(i) \emph{the highly dynamic activation range} complicates accuracy preservation using existing methodologies for pure vision transformers; 
(ii) \emph{the existence of bridge blocks}, which serve as connectors between convolution and transformer layers, introduces a disparity between local and global representations and the issue of zero-point overflow; 
(iii) \emph{the diverse types of normalization techniques} are used in hybrid vision transformers; and 
(iv) \emph{the small-sized models with less than 5 million parameters} lead to a substantial loss of robustness during quantization, because of their limited number of parameters and minimal residual connections.
Therefore, it is imperative to simultaneously adjust the granularity and scheme of both bridge and non-bridge layers, while also identifying the optimal scaling factors for quantization.

%We propose a new post-training quantization technique that overcomes these challenges and is the first work to quantize hybrid vision transformers (MobileViTv1 and MobileViTv2) with a significant margin compared to existing techniques. 
%Our method reduces the accuracy drop concerning MobileViTv1 and MobileViTv2. Additionally, we will soon release our code as open-source software.

In this paper, we propose \textbf{Q-HyViT} in Figure \ref{fig:overview}, a tailored quantization approach for hybrid vision transformers aimed at minimizing quantization error.
Q-HyViT integrates a novel \emph{hybrid reconstruction} error minimization technique, which can determine the optimal scale factors, granularity (channel-wise or layer-wise), and quantization scheme (symmetric or asymmetric). Here, the reconstruction objective of each layer differs based on whether it is part of the bridge block\footnote{The term ``bridge block'' literally refers to the transition part between convolution and transformer blocks. The precise constitution of this block slightly differs among hybrid ViT architectures.} or not.
To implement this technique in an integrated manner, we reuse the second-order term-based reconstruction error minimization method and extend it to incorporate the bridge block.
To our knowledge, this is the \emph{first} work that identifies the challenges of quantization for hybrid vision transformers and proposes a unified method to mitigate their errors in post-training quantization. 

% how the algorithm works

% describe evaluation results
We conduct comprehensive experiments to compare Q-HyViT with existing open-source algorithms, namely EasyQuant~\cite{wu2020easyquant}, FQ-ViT~\cite{lin2022fq}, and PTQ4ViT~\cite{yuan2022ptq4vit}, on the same hybrid ViT. %The results demonstrate the preservation of accuracy degradation caused by quantization in Q-HyViT. 
The experiments use hybrid vision transformers including MobileViTv1~\cite{mehta2021mobilevit}, MobileViTv2~\cite{mehta2022separable}, Mobile-Former~\cite{chen2022mobile}, EfficientFormerV1~\cite{li2022efficientformer}, and EfficientFormerV2~\cite{li2022rethinking}, which are representative variants of efficient vision transformers. 


The results demonstrate that our Q-HyViT performs exceptionally well across five types of hybrid vision transformers and outperforms existing PTQ methods for pure ViTs (EasyQuant and PTQ4ViT) by a significant margin (up to an average improvement of 8.32\% for 8-bit and 26.02\% for 6-bit). 
Particularly, we highlight that Q-HyViT achieves state-of-the-art accuracy (an average improvement of 43.63\% for 8-bit compared with FQ-ViT) on hybrid vision transformers in \emph{full} quantization setup, including the quantization of non-linear operations, such as softmax and normalization.

%- direct implementation
%- comparison in quantization
%- comparison in a fully quantized model 
%- ablation study

% ptq4vit
% Experiments show the quantized vision transformers (ViT, DeiT, and Swin) achieve near-lossless prediction accu- racy (less than 0.5% drop at 8-bit quantization) on the ImageNet classification task.
% apq
% Our APQ-ViT revisits the process of post-training quantization
%for vision transformer and presents novel insights. Comprehensive experiments on the large-scale computer vision tasks (image classifi- cation [7] and object detection [26]) demonstrate that our APQ-ViT performs remarkably well across various transformer architectures such as ViT [9], DeiT [40], and Swin Transformer [28], and sur- passes the existing methods by convincing margins, especially in lower bit-width settings (e.g., averagely up to 5.17% improvement for classification and 24.43% for detection on W4A4). We highlight that our APQ-ViT scheme achieves state-of-the-art accuracy per- formance on various bit-width settings, and enjoys versatility on diverse architectures and vision tasks.

The primary contributions of our work are listed below.
\begin{itemize}

\item We discover that quantization of hybrid ViTs presents four unique challenges: (i) the presence of highly dynamic activation ranges, (ii) zero-point overflow in the bridge block, (iii) diverse normalization, and (iv) a parameter count of less than 5 million.

\item We propose a unified method called Q-HyViT, which is based on Hessian to adjust the granularity and scheme for bridge and non-bridge layers, while also determining the optimal scaling factors for quantization.

\item For comparison, we re-implement the existing PTQ methods enabling them to be applied to hybrid ViTs. We then evaluate Q-HyViT for large-scale image classification using five variants of efficient hybrid ViTs.
\item Our experimental results demonstrate the effectiveness of Q-HyViT in preserving the accuracy of quantized hybrid ViTs, as it outperforms the state-of-the-art PTQ methods, FQ-ViT and PTQ4ViT.

\end{itemize}

% 5m under 의 모델을 시도했다. (cvpr18논문 논리)
% PTQ 이야기도 넣기 
\iffalse
CNN 방법론 적용시 -> 문제점
Transformer 방법론 적용시 -> 문제점
표를 만들어서 각 케이스별로 해결법을 구조적 나열 (기존 방법들의 조합 or a little bit new)
새로운 메소드를 제안x -> 기존의 기법들을 hybrid 생태게에 맞게 동작 시킨다. 간단하게 동작 
\fi
% 5m under 의 모델을 시도했다. (cvpr18논문 논리)


\section{Related Work}
Model architecture design and quantization techniques have received substantial attention in the context of efficient AI. 
We categorize prior research on efficient model architectures and quantization methods.
%, followed by a comparison of our novelty against the existing studies.

\subsection{Efficient Computer Vision Architecture}
To avoid heavy computation in CNNs, standard convolutions have been replaced by separable convolutions~\cite{chollet2017xception}.
Separable convolutions have been widely used when designing light-weight CNNs including MobileNets~\cite{howard2017mobilenets,sandler2018mobilenetv2,howard2019searching}, ShuffleNetv2~\cite{ma2018shufflenet}, and MNASNet~\cite{tan2019mnasnet}, reducing the computational complexity of CNN operations.
Despite the prevalence of these models, a major drawback of them is that they are spatially local and have higher inductive bias than transformers.

%-> global and local representation (shape filter, texture filter) 
To expand model capacity while minimizing inductive bias, a vision transformer (ViT)~\cite{dosovitskiy2020image} is employed, which directly applies a pure transformer to process image patches as a sequence. 
Dosovitskiy~\textit{et~al.}~\cite{dosovitskiy2020image} showed that it performs better than CNN-based architectures on multiple image recognition benchmarks. 
Moreover, Touvron~\textit{et~al.}~\cite{touvron2021training} introduced a teacher-student strategy specific to transformers, resulting in competitive convolution-free transformers trained on ImageNet only. 

% hybrid vision transformer (v1, v2)
Even though pure vision transformer models achieve performance competitive to CNNs, the majority of these models are heavy-weight.
Recently, MobileViTv1~\cite{mehta2021mobilevit}, MobileViTv2~\cite{mehta2022separable}, EfficientFormerV1~\cite{li2022efficientformer}, EfficientFormerV2~\cite{li2022rethinking}, and Mobile-Former~\cite{chen2022mobile} have been proposed for lightweight vision transformers.
Such hybrid vision transformers show better accuracy than light-weight CNNs (MobileNet series) at a similar number of model parameters by hybridizing convolution and transformers.
%Furthermore, Mehta~\textit{et~al.} proposed MobileViTv1 that uses separable self-attention to reduce attention complexity ($O(n^2)$ to $O(n)$)~\cite{mehta2022separable}. 
%Considering these active studies on efficient model architecture design, the hybrid vision transformer will continue to replace CNNs in many areas, and its usefulness will be significant in terms of optimizing the model simultaneously.

\subsection{Model Quantization}
Quantization methods are categorized into two types: quantization-aware training (QAT) and post-training quantization (PTQ).
Although QAT methods have successfully mitigated the accuracy degradation of quantized models by mapping from high-bit to low-bit precision~\cite{kris_whitepaper2018,steven2020,PACT2018,zhang2018lq,jung2019learning,zhou2016dorefa,jacob2018,songhan2016}, their widespread adoption has been hindered due to dependencies on re-training, the necessity of a complete dataset, and sophisticated hyper-parameter tuning.

Post-training quantization methods, which convert high-precision representation bits to low-precision bits without requiring re-training steps, have been extensively studied by researchers and widely adopted in practical scenarios~\cite{jiang2021automated,choukroun2019low,nagel2019data,zhao2019improving,lee2018quantization,goncharenko2019fast,meller19a,migacz20178,wu2020integer}. 
PTQ helps in the rapid deployment of CNN models on resource-constrained devices by addressing time-consuming and data privacy issues. However, PTQ leads to significant accuracy degradation, particularly in low-precision representations, and prior research has mainly focused on \texttt{int8} quantization. 
As an effort to preserve the performance of a full-precision model, recent PTQ works~\cite{adaround20,brecq21,hubara2021accurate,wei2022qdrop,wangleveraging} have suggested to reconstruction error minimization of each layer or block by adjusting the magnitude of weight rounding and searching optimal scaling factors. 

Recently, quantization for vision transformers has been studied~\cite{liu2021post,lin2022fq,yuan2022ptq4vit,apq22,liu2023noisyquant}.
To reduce quantization error, such research works have considered the special structure of vision transformers such as multi-head attention and layer normalization. 
However, these approaches do not account for the distinctive characteristics of hybrid vision transformers, including the presence of bridge blocks and the utilization of diverse normalization techniques.


%Several post-training quantization methods have been developed without requiring any training or fine-tuning. 
%Yoni \textit{et al.}~\cite{choukroun2019low} propose the OMSE method, which optimizes the L2 distance between the quantized tensor and the original tensor.
%Ron \textit{et al.}~\cite{banner_neurips2019} present the ACIQ method to analytically compute the clipping range and per-channel bit allocation for NNs.
%Zhao \textit{et al.}~\cite{zhong2019ada} propose the OCS method to address the outlier channel problem, while Wang et al.~\cite{wang2020towards} introduce the Bit-Split and Stitching framework and the Error Compensated Activation Quantization method for lower-bit post-training quantization. 
%Nagel \textit{et al.}~\cite{adaround20} propose AdaRound, a weight-rounding mechanism that adapts to the data and task loss, and Cai et al.~\cite{cai2020zeroq} introduce ZeroQ, which matches the statistics in batch normalization layers and utilizes a Pareto Frontier method for mixed-precision settings. 



%Our experimental results show that there is no universal configuration that always achieves the most accurate quantization for all CNN models. Rather, the optimal quantization strategy is dependent on the specific CNN model, and a naive parameter search can be time-consuming. To address this challenge, we propose a machine learning-based approach to finding the best configurations for each CNN model, which is the first study to do so.



\iffalse
\begin{itemize}
    \item Arxiv22.12,CNN, ViT PD-Quant
    \item Multimedia, ViT APQ-ViT
    \item Block-wise reconstruction error , Transformer (N)
    \item Neurips22, PLM Towards Efficient Post-training Quantization of pre-trained Language models
    \item Module-wise reconstruction error
    \item Neurips22, CNN] NWQ Network-wise reconstruction error
    \item CNN QDrop
    \item CNN AdaQunat
    \item CNN BRECQ
    \item CNN AdaRound
\end{itemize}
\fi



\section{Preliminary}
% show problematic distribution (observation)

\subsection{Hybrid Vision Transformers and Bridge Blocks}
To address the inefficiencies of pure vision transformer models, MobileViTv1 was proposed as a hybrid, combining convolution and transformers to reduce model size without compromising accuracy.

\subsubsection{Variants of Hybrid Vision Transformers.} Following MobileViTv1's release, several hybrid vision transformers emerged. MobileViTv2 refines the $O(N^2)$ complexity attention map of MobileViTv1 to linear complexity. Mobile-Former integrates MobileNet and Transformer outputs. Efficient-FormerV1 employs a MetaBlock to eliminate latency-inefficient operators, while Efficient-FormerV2 reduces model parameters from V1 and introduces a design for better accuracy.

\subsubsection{Bridge Blocks.} The Bridge Block varies in its exact location depending on each hybrid vision model, but fundamentally consists of operators that adjust dimensions to facilitate transitions between local and global representations. Specifically, for MobileViTv1 and MobileViTv2, it implies convolution and reshape operators to align the input dimensions of the Transformer. In the case of Mobile-Former, it refers to operators within the Mobile-Former Block that transition bi-directionally between local and global representations.
For Efficient-FormerV1, these are operators that convert meta-blocks from 4D to 3D. In the case of Efficient-FormerV2, the bridge block consists of local and global transition operators present in the 3rd and 4th stages.

%Inductive bias (CNN+Transformer)
%MobileViTv1
%MobileViTv2: linear attention to reduce the complexity$O(N^2)$

\subsection{Hybrid Vision Transformer Quantization}
Uniform quantization is a commonly used method to quantize neural networks, including convolution networks and transformers.
As shown in Figure~\ref{fig:overview}, quantization for hybrid vision transformers is divided into 
 three parts: convolution, bridge block, and transformer.
In uniform quantization, the weights and input activations are evenly quantized by each scale factor as: 
\begin{equation}
\vb{x} _{q} = \mathcal{Q}(\vb{x} _{r} ) = \text{clip}(\text{round} \left(\frac{\vb{x}_{r}}{\Delta_{\vb{x}}}+zp \right), \text{min}, \text{max}),
\label{lab:quantizer}
\end{equation}
where $\vb{x}_r$ is a real value (full precision) and $\vb{x}_q$ is a quantized value. $\Delta_{\vb{x}}$ is a scaling factor that is calculated depending on the quantization scheme: either asymmetric or symmetric. 
Also, $zp$ denotes the zero point and exists only when using the asymmetric scheme.


In the case of transformers, input data is first passed through a quantized embedding layer before entering the transformer blocks, which consist of a multi-head self-attention (MHSA) and a feed-forward network (FFN). 
The MHSA module computes queries $\vb{Q}$, keys $\vb{K}$, and values $\vb{V}$ with their pre-trained weights $\vb{W}$ and inputs $\vb{X}$. 

In a specific quantized transformer layer, the weights of $\vb{Q}$, $\vb{K}$, $\vb{V}$ are quantized, and then multi-head self-attention is calculated using $\vb{Q}$, $\vb{K}$, and softmax normalization.
% The computation of self-attention is represented as:
% \begin{equation}
%     \text{Attention}^{Q}(\vb{Q},\vb{K},\vb{V}) = \text{softmax}^{Q} \left( \frac{\vb{Q}_{q} \times \vb{K}^{T}_{q}}{\sqrt{\vb{d}}} \right) 
%     \label{eq:basic_attention}
% \end{equation}
% In Eq.~\eqref{eq:basic_attention}, $d$ denotes the dimension of values. 
After the self-attention layer, FFN takes a quantized output which is concatenated from the results of MHSA as an input.


A popular method to reduce quantization error in post-training is reconstruction error minimization~\cite{yuan2022ptq4vit,adaround20,brecq21,apq22}.
Previous works optimized task loss $\mathcal{L}=\text{Cross  Entropy}(\hat{\vb{y}},\vb{y})$, where $\hat{\vb{y}}$ represents the quantized output and $\vb{y}$ denotes the full precision output which is used as ground truth in PTQ. 
The expectation of task loss is a function of network parameters $\vb{w}$, given by $\E[\mathcal{L}(\vb{x},\vb{y},\vb{w})]$, where $\vb{x}$ denotes activation and $\vb{y}$ denotes output. 
Quantization introduces a small perturbation $\epsilon$ on the parameter $\hat{\vb{w}} = \vb{w} + \epsilon$. 
As in prior works~\cite{yuan2022ptq4vit,adaround20,brecq21,apq22}, we calculate the influence of quantization on the task loss using Taylor series expansion as:
\begin{equation}
    \E[\mathcal{L}(\vb{x},\vb{y},\hat{\vb{w}})] - \E[\mathcal{L}(\vb{x},\vb{y},\vb{w})] \approx \epsilon^{\intercal} \bar{g}^{(\vb{w})} + \frac{1}{2}\epsilon^{\intercal} \bar{H}^{(\vb{w})}\epsilon.
    \label{eq:taylor}
\end{equation}

Since the weight perturbation $\epsilon$ is relatively small, a second-order term of Taylor expansion can be used. 
In this equation, $\bar{g}^{(\vb{w})} = \E[\nabla_{\vb{w}}\mathcal{L}(\vb{x},\vb{y},\hat{\vb{w}})]$ is the gradient and can be ignored if the pre-trained model is well-converged. 
$\bar{H}^{(\vb{w})} = \E[\nabla_{\vb{w}}^2\mathcal{L}(\vb{x},\vb{y},\hat{\vb{w}})]$ is the Hessian matrix. 
The goal is to find a quantizer that includes optimal scaling factors or a rounding scheme to minimize the influence, given by $\min{\E[\mathcal{L}(\vb{x},\vb{y},\hat{\vb{w}})] - \E[\mathcal{L}(\vb{x},\vb{y},\vb{w})]}$. 
However, directly minimizing task loss leads to overfitting problems due to small datasets during the calibration phase. 
Thus, the second-order term of the Taylor series (Eq.\eqref{eq:taylor}) is used. 
Referring to BRECQ~\cite{brecq21}, to reduce computational cost, Eq.(\eqref{eq:taylor}) is simplified by removing the gradient ($\bar{g}^{(\vb{w})}$) and approximating $\epsilon=\Delta \vb{w}$ to the network output ($\Delta O = \hat{O}-O$) as: \looseness=-1
\begin{equation}
    \epsilon^{\intercal} \bar{H}^{(x)}\epsilon \approx \Delta O^{\intercal}\bar{H}^{(O)}\Delta O.
    \label{eq:rem_basic}
\end{equation}
Referring to previous works~\cite{yuan2022ptq4vit,adaround20,brecq21,apq22}, MSE minimization based on the squared gradient that approximates the Hessian matrix captures the trend of task loss more accurately than other metrics such as MSE, Cosine, and Pearson.

We adopt the methodology described in~\cite{yuan2022ptq4vit,apq22,wu2020easyquant} to traverse a search space of scaling factors by linearly dividing the maximum-minimum range of $\vb{w}$ and $\vb{x}$ into $n$ candidates as:
\begin{equation}
\begin{aligned}
    &[\alpha \frac{MAX|\vb{w}_{l}|}{2^{k-1}}, \beta \frac{MAX|\vb{w}_{l}|}{2^{k-1}}] \\ 
    &[\alpha \frac{MAX|\vb{x}_{l}|}{2^{k-1}}, \beta \frac{MAX|\vb{x}_{l}|}{2^{k-1}}],
\end{aligned}
\label{eq:candiates}
\end{equation}
where $\alpha$ and $\beta$ are utilized to control the number of candidates generated for scaling factors.

\iffalse
\begin{figure}[t]
\centering
%\subfloat[][]{\label{sub:minmax_layer}\includegraphics[width=.31\textwidth]{./figures/minmax_layer_quant}} %\vfill
\subfloat[][]{\label{sub:box_layer}\includegraphics[width=.4\textwidth]{./figures/15_channel_boxplot}} \vfill
\subfloat[][]{\label{sub:hist_layer}\includegraphics[width=.4\textwidth]{./figures/17_channel_boxplot}}
\caption{Per activation channel ranges of (a) the point-wise (\texttt{stage0.conv1x1.point.act}) and (b) depth-wise convolution (\texttt{stage0.convkxk.depth.act}) in the inverted bottleneck block in MobileViTv1-xxs}
\label{fig:layer_quant}
\end{figure}

\begin{figure}[t]
\centering
%\subfloat[][]{\label{sub:minmax_channel}\includegraphics[width=.33\textwidth]{./figures/minmax_channel_quant}} %\vfill
\subfloat[][]{\label{sub:hist_channel1_quant}\includegraphics[width=.4\textwidth]{./figures/17_hist_ch1}} \vfill
\subfloat[][]{\label{sub:hist_channel5_quant}\includegraphics[width=.4\textwidth]{./figures/17_hist_ch5}}
\caption{An overlapping histogram of quantized values (blue) and real values (orange) in the two channels of depth-wise convolution (\texttt{stage0.convkxk.depth.act}) in MobileViTv1-xxs: (a) the first channel and (b) fifth channel}
\label{fig:channel_quant}
\end{figure}
\fi

\section{Challenges of Hybrid ViT Quantization}
\label{sec:chall}
% Describe lessons 
%% Scheme: asymmetric problem
%% granularity: activation channel wise 

Here, we identify four critical challenges (C1--C4) that impede the quantization of hybrid vision transformers and explain why the current quantization method is insufficient.


\subsection{C1: Highly Dynamic Activation Range} 
To mitigate the decline in accuracy caused by highly dynamic activation ranges, the granularity of quantization should be changed adaptively according to the channel distribution of each layer. For layers that exhibit different ranges per channel, a per-channel scaling factor could be chosen to preserve a specific layer~\cite{wu2020integer}.
Otherwise, channels exhibiting a narrow range might encounter the problem where all values are treated as zeros during the application of layer-wise quantization.

To this end, channel-wise granularity is good for highly dynamic activations across channels.
When prioritizing accuracy preservation without considering latency, opting for a fine-grained granularity at the channel level could yield the highest accuracy.
However, \emph{this does not always hold true in the hybrid vision transformers.} 
%Also, fine-grained granularity (channel-wise) is to produce better quantization results than coarse-grained manner (layer-wise). 
Applying channel-wise quantization to all the layers rather causes the scaling factors to overfit to small calibration data. This exacerbates the disparity between validation and calibration, resulting in a severe accuracy drop.
Figure~\ref{fig:gap} shows the phenomenon where scaling factors, determined on a channel-wise basis during the calibration phase, exhibit discrepancies during the validation phase.
%Second, in the calibration level, a zero distribution channel appears.
As a result, simply applying channel-wise granularity for quantization across all layers is problematic in hybrid ViTs. For layers where overfitting is an issue, applying the scaling factor on a layer-wise basis can alleviate this problem. Therefore, determining the optimal granularity of the scaling factor is a critical consideration.

\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{./figures/gap_calib_valid_stages.4.1.transformer.1.attn.out_proj}
  \vspace*{-0.7cm}
	\caption{Discrepancy in activation ranges between the calibration and validation datasets in 1st bridge block of MobileViTv2-100}
 %Problematic activation channel (index 16) of convolution in bridge block due to overflow of zero point when using channel-wise manner and asymmetric scheme} 
	\label{fig:gap}
 \vspace*{-0.1cm}
\end{figure}

\iffalse
The distribution of subsequent activations (\texttt{stage0.conv1x1.point.act}) following the point-wise convolution operation in the InvertedBottlenect block is illustrated in Figure\ref{fig:layer_quant}\subref{sub:box_layer}.  
In this case, \texttt{stage0.conv1x1.point.act} shows the appropriate activation distribution for layer-wise quantization because the channel-wise distribution is not diverse.

Figure\ref{fig:layer_quant}\subref{sub:hist_layer} illustrates the activation distribution (\texttt{stage0.convkxk.depth.act}) after the depth-wise convolution operation in the InvertedBottleneck block. 
%while Figure\ref{fig:channel_quant}\subref{sub:hist_channel_quant} shows the result of layer-wise quantization. 
The distribution of activations is highly diverse, unlike in Figure\ref{fig:layer_quant}\subref{sub:box_layer}, posing a problem when determining the scaling factor for quantization based on the minimum and maximum values of the activation range for the entire layer. 
For that reason, while the fifth channel activation of \texttt{stage0.convkxk.depth.act} is well quantized, all the values in the first channel activation are quantized to zero as shown in Figure\ref{fig:channel_quant}\subref{sub:hist_channel1_quant} and Figure\ref{fig:channel_quant}\subref{sub:hist_channel5_quant}.
 
Considering large variations to other channels, adjusting the scaling factor is difficult to compensate for this difference in activation ranges in layer-wise quantization. 
To mitigate the gap among channels, changing the quantization granularity is necessary. 
Applying layer-wise quantization to the activation \texttt{stage0.convkxk.depth.act}, the signal quantization noise ratio (SQNR) is observed to be $26.00$, and the channel-wise is significantly improved to $34.31$. 
Consequently, channel-wise quantization is appropriate for \texttt{stage0.convkxk.depth.act}.
\fi


% Quantization error introduced by \texttt{stage0.conv1x1.point.act} activation in a layer-wise manner. It is because the largest scaling factors are computed by using data from channel index 5 of \texttt{stage0.conv1x1.point.act}. If the divisor is large, the dividend will be zero after the rounding operation.
\iffalse
% mobilevit-v2 100
However, in some cases (mobilevit-v2 100), coarse-grained granularity is more adequate to reduce quantization error.
Therefore, considering quantization error, granularity should be carefully selected based on a hessian-guided manner.

% twin quantization problem 
High variation of dynamic activation range leads to an unprecedented range during the calibration step with a few training samples. To this end, the twin uniform quantization scheme is not working properly. 
\fi

\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{./figures/59_channel}
 \vspace*{-0.6cm}
	\caption{Per activation channel ranges of convolution in bridge block of MobileViTv1-xxs} 
 %make all values of channel index 1 to zero.} 
	\label{fig:small_range_clip}
  \vspace*{-0.45cm}
\end{figure}


\subsection{C2: Zero-point Overflow in Bridge Block}
Uniform quantization includes two types: asymmetric and symmetric quantization, each with its own advantages and disadvantages. 
In most cases, asymmetric quantization shows better accuracy than symmetric quantization. 
But, we found that \emph{a severe accuracy drop occurs in the bridge block when using an asymmetric scheme or channel-wise granularity} due to highly dynamic activation ranges according to each channel and non-zero distribution. This observation aligns with the prior discovery that {adopting fine-grained granularity, such as channel-wise, does not always lead to minimal quantization error.}
%specifically because the distribution is positively skewed, leading to an overflow. 

As shown in Figure~\ref{fig:small_range_clip}, the activation of the bridge block convolution shows a similar range for both the maximum and minimum values across all channels, indicating that layer-wise quantization does not lead to significant accuracy degradation. 
However, when applying channel-wise quantization to distributions such as the 2nd, 7th, 11th, and so on in Figure~\ref{fig:small_range_clip}, where all values are greater than zero, overflow of zero-point value for asymmetric quantization occurs (i.e., the zero-point value exceeded between $-128$ and $127$). 
As shown in Figure~\ref{fig:scheme_overflow}, the clipped zero point is used, resulting in certain values being reconstructed as a single value. 
These issues manifest differently across models, necessitating an automated approach to choosing granularity and scheme.




\iffalse
\begin{figure}[t]
	\centering
	\includegraphics[width=.7\columnwidth]{./figures/59_hist_ch16}
	\caption{Problematic activation channel (index 16) of convolution in bridge block due to overflow of zero point when using channel-wise manner and asymmetric scheme} 
	\label{fig:scheme_overflow}
\end{figure}
\fi 
\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{./figures/59_hist_ch_pick_6plots}
 \vspace*{-0.6cm}
	\caption{The selected problematic activation channels of convolution in bridge block of MobileViTv1-xxs due to overflow of zero point when using the channel-wise manner and asymmetric scheme} 
	\label{fig:scheme_overflow}
 \vspace*{-0.4cm}
\end{figure}
 


\subsection{C3: Quantization with Diverse Normalizations} %% Batch-Norm, I-Norm, G-Norm 
\emph{Hybrid vision transformers, unlike pure models such as CNN and ViT, employ different combinations of normalization techniques.} 
In the case of MobileViTv1, Mobile-Former, EfficientFormerV1, and EfficientFormerV2, {BatchNorm} and {LayerNorm} are utilized, while MobileViTv2 uses {BatchNorm} and {GroupNorm}.
In order to create a network that is as parameter-efficient as possible, various normalizations are used in each hybrid vision transformer. However, a method to quantize all the normalizations used has not yet been proposed. 
Unlike {BatchNorm}, the computation of {LayerNorm}, {GroupNorm}, and {InstanceNorm} requires dynamic computation to determine the mean and variance. 
When dynamic operations are handled in a floating point, additional data movement occurs in off-chip memory. 
Therefore, to minimize the inference latency, the normalization should be quantized so that as many dynamic operations as possible can be processed in the integer domain.
%To address this, prior work~\cite{lin2022fq} introduced {I-layerNorm} for integer domain computation.
%In this work, we propose \textit{I-GroupNorm}, which is specifically designed for fully quantized MobileViTv2 and can also be computed under the integer domain.
% how to extend I-layerNorm to I-GroupNorm 

\subsection{C4: Sub-5M Parameter Models} %% analysis depending on model size
Compared to heavy CNN models such as ResNet and VGG series, lightweight models with a relatively small number of parameters and few residual connections are more susceptible to the quantization process. \emph{This vulnerability is particularly pronounced in models that have fewer than 5M parameters, where the quantization error is significantly higher in hybrid vision transformers.} As previously mentioned, using an asymmetric scheme for an activation distribution with a minimum value of 0 or greater can cause force clamping due to the zero point overflow problem, making the accuracy very sensitive to the quantization granularity and scheme.

\section{Methodology}
Addressing the four identified challenges (C1--C4), we design Q-HyViT, a precise post-training quantization framework specifically tailored for hybrid ViTs. It automatically identifies optimal layer-wise strategies -- choosing granularity (channel-wise or layer-wise) for C1 and C2, quantization scheme (symmetric or asymmetric) for C2, and quantization scaling factor for C3 and C4. This is achieved by leveraging the proposed hybrid reconstruction error minimization technique in the following section, also illustrated in Figure~\ref{fig:overview}.

%the strategic selection of optimal scale factors, granularity, and scheme for both the bridge block and individual layers based on reconstruction error from a loss degradation perspective.

%As shown in Figure~\ref{fig:overview}, we describe Q-HyViT that minimizes the quantization error of hybrid vision transformers by selecting optimal scale factors, granularity, and scheme for both the bridge block and each layer based on reconstruction error from a loss degradation perspective.

\iffalse
%We begin by presenting the approximation of loss degradation using Taylor series expansion, and then describe Q-HyViT that optimally selects the scale factor, granularity, and scheme for minimizing reconstruction error according to the bridge block or layer.
hybrid vision transformer의 양자화 에러를 최소화 하기 위해서 loss degradation 관점에서의 bridge block 또는 Layer에 맞춰서 reconstruction 에러가 최소화 되도록 스케일 팩터, granularity, scheme각각 최적으로 선택하는 방법인 Q-HyViT에 대해서 설명한다. 
% key word를 정하고 방법론을 정리 해야함 
- loss degradation을 tayler로 근사화 하고 이것을 다시 hessian으로 처리하는 방법. 그리고 그 reconsturing 수준을 layer로 할 것이냐 block으로 할 것이냐를 정의한다. 
- loss degradation을 줄이기 위해서 기존에는 rounding scheme 사용함 
- scaling factor 최적화도 시도함 
- brdige block이 basic building blocks in neural networks and reconstructs them one-by-one 보다 좋다. 
%-- AdaRound
By approximating the task loss with a Taylor series expansion, the round- ing task is posed as a quadratic unconstrained bi- nary optimization problem.
%-- FlexRound
PTQ schemes based on reconstructing each layer or block output turn out to be effective to enhance quantized model performance
Accordingly, recent works (Nagel et al., 2020; Li et al., 2021; Hubara et al., 2021; Wei et al., 2022) have suggested to reconstruct each output of layer or block by devising and learning a new weight-rounding scheme, deviating from rounding-to-nearest, as an effort to preserve the performance of a full-precision model.
-- by devising and learning a new weight-rounding scheme, deviating from rounding-to-nearest, as an effort to preserve the performance of a full-precision model.
% -- BRECQ 
Recent works like (Nagel et al., 2020) recognized the problem and analyzed the loss degradation by Taylor series expansion. Analysis of the second-order error term indicates we can reconstruct each layer output to approximate the task loss degeneration.
% -- PTQ4ViT 
We also observe that common quantization metrics, such as MSE and cosine distance, are inac- curate to determine the optimal scaling factor
we propose to use a Hessian-guided metric to evaluate different scaling factors, which improves the accuracy of calibration at a small cost. To
% -- APQ 
calibration metric 
\fi


% hessian guided search in convolution
% -- loss 분석, quantized loss (sequential gradient analysis) 
\subsection{Hybrid Reconstruction Error Minimization}

Previous post-training quantization methods for ViTs have typically aimed to optimize the quantization task loss, which is achieved through the utilization of reconstruction error minimization, relying on second-order metrics to assess potential scaling factors for quantization. Furthermore, these methods incorporate a weight-rounding mechanism to enhance the overall quantization process. 
%
However, faced with the challenges specific to hybrid vision transformers, they become less effective in dealing with \emph{bridge blocks}, which contain a gap between local and global representation, and \emph{high dynamic activation ranges} resulting from the mixed structure of CNN and transformer.


% ------- It is better to insert figures to show evidence. like loss plane.
%In an ideal scenario, a carefully designed loss term would determine how to quantize each layer with the lowest quantization loss to converge to a global optimum. 
%Nonetheless, we observed that the second-order loss terms calculated by each candidate exhibit significant variance, especially at the bridge layer.
%Consequently, we propose bridge-wise reconstruction error minimization as a post-training quantization technique. This technique optimizes calibration on a bridge-wise basis, enabling the Hessian-guided loss to perceive the quantization error of neighboring layers in a transition block from CNN to a transformer.

To address these issues, Q-HyViT introduces \emph{hybrid reconstruction error minimization}, which distinguishes the reconstruction strategy based on whether a given layer is part of the bridge block or not. It then determines the appropriate granularity and quantization scheme for each layer during post-training quantization. The reconstruction objective ($O^{bb}$) of the hybrid approach can be represented as:
\begin{align}
O^{bb} &=
\begin{cases}
    \vb{w}_{n}^{bb}\vb{w}_{n-1}^{bb} \dots \vb{w}_{1}^{bb}\vb{x}^{bb}, & \text{if a layer is in a bridge block}\\
    \vb{w}^{\ell}\vb{x}^{\ell}, & \text{otherwise } bb \text{ is equal to } \ell   
\label{eq:layer_type} 
\end{cases} 
\end{align}
When a layer is part of the bridge block, the objective encompasses all the preceding layers within that bridge block.

Moreover, our hybrid reconstruction not only enables hybrid ViTs to achieve minimal quantization errors, but also determines quantization granularity and scheme automatically for each layer, using the hybrid reconstruction equation, guided by the reconstruction objective:
\begin{equation}
\begin{aligned}
    & \min\limits_{\Delta,g,s}{\E \left[\Delta O^{(bb),\intercal},\textbf{H}^{O^{(bb)}} \Delta O^{(bb)} \right]} \approx \\  
    & \min\limits_{\Delta,g,s}{\E \left[\Delta O^{(bb),\intercal}, \textit{diag} \left((\frac{\partial \textit{L}}{\partial O_{1}^{(bb)}})^2 ,\cdots, (\frac{\partial \textit{L}}{\partial O_{|O^{bb}|}^{(bb)}})^2 \right) \Delta O^{(bb)} \right]},
    \label{eq:rem}
\end{aligned}
\end{equation}

\smallskip\smallskip\smallskip
\noindent where $bb$ is $\in [BridgeBlock, a\text{ }layer]$.
In Eq.~\eqref{eq:rem}, $\Delta O^{(bb)}$ is the difference between before and after quantization outputs.
$O_{n}^{(bb)}$ indicates the $n$-th element of $O^{bb}$. the range of $n$ is from $1$ to $O_{|O^{bb}|}$.
As a result, we optimize the optimal scaling factor ($\Delta \!\in\! [1,100]$), granularity~($g\!\in\! [layer,channel]$), and scheme~($s\in [asymmetric, symmetric]$) through hybrid reconstruction error minimization.


\subsection{Q-HyViT Framework}
The proposed Q-HyViT method, as outlined in Algorithm~\ref{algo:qhyvit}, leverages hybrid reconstruction to determine optimal scaling factors, granularity, and scheme. Thus, it effectively diminishes quantization errors in hybrid ViTs.
Throughout the calibration process, Q-HyViT calculates the output and gradient of each bridge block and layer through forward and backward propagation and then optimizes all hybrid transformer layers by reducing reconstruction error.
As we mentioned, in the hybrid ViTs, the dynamic activation range varies widely, necessitating the need for automatic adjustment methods. 
From this perspective, this approach minimizes quantization errors to improve the second-order metric and ultimately leads to more accurate models.


\IncMargin{1em}
\setlength{\textfloatsep}{10pt}% Remove 
\begin{algorithm}[t]
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	\Indm
	\KwIn{Hybrid vision transformer model and a few images for calibration;}
	\KwOut{Optimal scaling factors($\Delta^{\ast}$) scheme($s$) and granularity($g$);}
	\Indp
        \While{a layer$(\ell)$ is not the end of layer}{
        \tcc{full-precision outputs on each layer including final layer $(\vb{y}^{(fp32)}$)}
        $O_{\ell}^{bb} \gets$ forward propagation $(\vb{w}_{\ell}\vb{x}_{\ell})$\;
	}
         \While{a layer$(\ell)$ is not the end of layer}{
            \If {a layer$(\ell)$ in a Bridge Block}{
		      Backward propagation to get $\frac{\partial \mathcal{L}}{\partial O^{bb}}$\;
	       }
            \Else{
                 Backward propagation to get $\frac{\partial \mathcal{L}}{\partial O^{\ell}}$\;
            }
	}
        \While{a layer$(\ell)$ is not the end of layer}{
            \tcc{initialize scaling factors}
            $\Delta^{\ast}_{\vb{w}_{\ell}^{bb}},\Delta^{\ast}_{\vb{x}_{\ell}^{bb}} \gets \frac{MAX(|\vb{w}_{\ell}^{bb}|)}{2^k}, \frac{MAX(|\vb{x}_{\ell}^{bb}|)}{2^k}$  \; 
            
            \tcc{Generate candidates for scaling factors} $\Delta_{\vb{w}_{\ell}^{bb}},\Delta_{\vb{x}_{\ell}^{bb}} \gets$ Eq.~\eqref{eq:candiates}\;
            \While{Three iterations}{
                \tcc{Determine granularity, scheme, scaling factors}
                $g,s,\Delta_{\vb{w}_{\ell}^{bb}}^{\ast},\Delta_{\vb{x}_{\ell}^{bb}}^{\ast} \gets$ Eq.~\eqref{eq:rem}\;
            }
        }
        \Return{$\Delta^{\ast}$ , $g$ , $s$}
	\caption{The tuning process of Q-HyViT}
	\label{algo:qhyvit}
\end{algorithm}
\DecMargin{1em}

%\setlength\tabcolsep{2pt}
\begin{table}[t]
\centering
\caption{A comparison of three post-training quantization methods for image classification on ImageNet-1K using five hybrid ViT architectures and bit-widths. In quantized models, softmax and layer-norm remain under floating-point. We abbreviate MobileViT as "MV", Mobile-Former as "MF", and EfficientFormer as "EF"}
\vspace*{-0.25cm}
\label{table:accuracy}
\resizebox{\columnwidth}{!}{
%\resizebox{\textwidth}{!}{
%\resizebox{11cm}{!}{
\begin{tabular}{@{\hskip0pt}l@{\hskip0pt}c@{\hskip0pt}c@{\hskip2pt}c@{\hskip2pt}c@{\hskip2pt}c@{\hskip2pt}c@{\hskip2pt}c@{\hskip2pt}c@{\hskip2pt}c@{\hskip2pt}
%\begin{tabular}{lccccccccc
} %~\cite{migacz20178,wu2020integer}
\toprule
\multirow{2}{*}{\textbf{Model}} &
\multirow{2}{*}{\textbf{\# Params.}} &
\multirow{2}{*}{\textbf{Type}} &
\multirow{2}{*}{\textbf{FP32}} & 
%\multicolumn{1}{c}{\textbf{Liu21~\cite{liu2021post}}} &
\multicolumn{2}{c}{\textbf{EasyQuant}} &
%\multirow{2}{*}{\textbf{${}^\ast$FQ-ViT~\cite{lin2022fq}}} & 
\multicolumn{2}{c}{\textbf{PTQ4ViT}} & 
\multicolumn{2}{c}{\textbf{Ours}} \\
 & & & & W8A8 & W6W6 & W8A8 & W6W6 & W8A8 & W6W6 
\\   \midrule
\midrule
MVv1-xxs & 1.3M & Hybrid & 69.0    & 36.13 & 10.17 & 37.75 & 30.80  & 68.20 & 66.33 \\ 
MVv1-xs & 2.3M & Hybrid & 74.8     & 73.16  & 55.22 &  65.52 & 62.56  & 74.31 & 73.44 \\ 
MVv1-s & 5.6M & Hybrid & 78.4    & 74.21 & 42.70 &  68.19 & 65.07  & 77.92  & 77.18 \\  
\midrule
MVv2-050 & 1.4M & Hybrid & 70.2  & 66.80 & 11.58  & 39.39 & 45.38  & 69.89 & 69.07 \\ %68.22 \\
% ours
MVv2-075 & 2.8M & Hybrid & 75.6  & 62.91 & 2.54  & 65.54 & 65.85 & 75.29 & 74.58 \\ %72.52 \\ 
% ours
MVv2-100 & 4.9M & Hybrid & 78.1 & 69.34 & 0.12 & 51.02 & 47.27  & 77.63 & 77.11 \\ %72.52 \\ 
MVv2-125 & 7.5M & Hybrid & 79.6 & 77.31 & 4.56 & 67.39 & 59.39  & 79.31 & 77.03 \\ %72.52 \\ 
MVv2-150 & 10.6M & Hybrid & 80.4 & 75.83 & 10.39 & 68.61 & 67.58  & 80.09 & 79.97 \\ %72.52 \\ 
MVv2-175 & 14.3M & Hybrid & 80.8 & 79.93 & 47.22 & 72.30 & 71.78  & 80.63 & 80.45 \\ %72.52 \\ 
MVv2-200 & 18.5M & Hybrid & 81.2 & 80.04 & 57.32 & 75.50 & 74.65 & 80.94 & 80.76 \\ %72.52 \\ 
\midrule
MF-26m & 3.2M & Hybrid & 64.0 & 28.95 & 0.12 & 58.27 & 47.25 & 61.78 & 47.25 \\ %72.52 \\ 
MF-52m & 3.5M & Hybrid & 68.7 & 62.16 & 17.29 & 67.32 & 62.01 & 67.79 & 62.01 \\ %72.52 \\ 
MF-96m & 4.6M & Hybrid & 72.8 & 53.31 & 33.68 & 71.32 & 64.72 & 71.60 & 64.72 \\ %72.52 \\ 
MF-151m & 7.6M & Hybrid & 75.2 & 4.98 & 3.49 & 73.86 & 68.16 & 74.30 & 68.16 \\ %72.52 \\ 
MF-214m & 9.4M & Hybrid & 76.7 & 72.79 & 28.32 & 75.01 & 68.24 & 75.76 & 68.24 \\ %72.52 \\ 
MF-294m & 11.4 & Hybrid & 77.9 & 74.15 & 59.55 & 76.96 & 68.24 & 76.93 & 74.48 \\ %72.52 \\ 
MF-506m & 14.0M & Hybrid & 79.3 & 78.01 & 67.14 & 75.44 & 70.13 & 75.60 & 70.13 \\ %72.52 \\ 
\midrule
EFv1-L1 & 12.3M & MetaBlock & 80.2 & 78.24 & 58.83 & 80.11 & 79.8 & 80.15 & 77.25 \\ %72.52 \\
EFv1-L3 & 31.3M & MetaBlock & 82.4 & 82.39 & 80.38 & 82.39 & 82.36 & 82.46 & 82.18 \\ %72.52 \\ 
EFv1-L7 & 82.1M & MetaBlock & 83.3 & 83.24 & 81.89 & 83.34 & 83.16 & 83.31 & 83.12 \\ %72.52 \\ 
\midrule
EFv2-S0 & 3.5M & Hybrid & 76.2 & 68.21 & 41.24 & 68.40 & 41.26 & 74.69 & 74.18 \\ %72.52 \\ 
EFv2-S1 & 6.1M & Hybrid & 79.7 & 66.42 & 2.69 & 73.44 & 73.34 & 77.56 & 77.54 \\ %72.52 \\ 
EFv2-S2 & 12.6M & Hybrid & 82.0 & 71.80 & 7.02 & 79.85 & 79.39 & 80.62 & 80.30 \\ %72.52 \\ 
EFv2-L & 26.1M & Hybrid & 83.5 & 80.34 & 3.34 & 82.46 & 82.22 & 82.80 & 82.71 \\ %72.52 \\ 
% ours
%MobileViTv2-100 & 4.3M & Hybrid & 78.09 & 69.34 & 0.12 & 61.74  & 0.1 \\ %0.1 \\ 
% ours
\bottomrule
\end{tabular}
}
\vspace*{-0.3cm}
\end{table}

\iffalse
\begin{table}[t]
\centering
\caption{A comparison of three post-training quantization methods for image classification on ImageNet-1K using five hybrid ViT architectures and bit-widths. In quantized models, softmax and layer-norm remain under floating-point.}
\label{table:accuracy}
\resizebox{\columnwidth}{!}{
%\resizebox{\textwidth}{!}{
%\resizebox{11cm}{!}{
\begin{tabular}{lccccccccc
} %~\cite{migacz20178,wu2020integer}
\toprule
\multirow{2}{*}{\textbf{Model}} &
\multirow{2}{*}{\textbf{\# Params.}} &
\multirow{2}{*}{\textbf{Type}} &
\multirow{2}{*}{\textbf{FP32}} & 
%\multicolumn{1}{c}{\textbf{Liu21~\cite{liu2021post}}} &
\multicolumn{2}{c}{\textbf{EasyQuant}} &
%\multirow{2}{*}{\textbf{${}^\ast$FQ-ViT~\cite{lin2022fq}}} & 
\multicolumn{2}{c}{\textbf{PTQ4ViT}} & 
\multicolumn{2}{c}{\textbf{Ours}} \\
 & & & & W8A8 & W6W6 & W8A8 & W6W6 & W8A8 & W6W6 
\\   \midrule
\midrule
MobileViTv1-xxs & 1.3M & Hybrid & 69.0    & 36.13 & 10.17 & 37.75 & 30.80  & 68.20 & 66.33 \\ 
MobileViTv1-xs & 2.3M & Hybrid & 74.8     & 73.16  & 55.22 &  65.52 & 62.56  & 74.31 & 73.44 \\ 
MobileViTv1-s & 5.6M & Hybrid & 78.4    & 74.21 & 42.70 &  68.19 & 65.07  & 77.92  & 77.18 \\  
\midrule
MobileViTv2-050 & 1.4M & Hybrid & 70.2  & 66.80 & 11.58  & 39.39 & 45.38  & 69.89 & 69.07 \\ %68.22 \\
% ours
MobileViTv2-075 & 2.8M & Hybrid & 75.6  & 62.91 & 2.54  & 65.54 & 65.85 & 75.29 & 74.58 \\ %72.52 \\ 
% ours
MobileViTv2-100 & 4.9M & Hybrid & 78.1 & 69.34 & 0.12 & 51.02 & 47.27  & 77.63 & 77.11 \\ %72.52 \\ 
MobileViTv2-125 & 7.5M & Hybrid & 79.6 & 77.31 & 4.56 & 67.39 & 59.39  & 79.31 & 77.03 \\ %72.52 \\ 
MobileViTv2-150 & 10.6M & Hybrid & 80.4 & 75.83 & 10.39 & 68.61 & 67.58  & 80.09 & 79.97 \\ %72.52 \\ 
MobileViTv2-175 & 14.3M & Hybrid & 80.8 & 79.93 & 47.22 & 72.30 & 71.78  & 80.63 & 80.45 \\ %72.52 \\ 
MobileViTv2-200 & 18.5M & Hybrid & 81.2 & 80.04 & 57.32 & 75.50 & 74.65 & 80.94 & 80.76 \\ %72.52 \\ 
\midrule
Mobile-Former-26m & 3.2M & Hybrid & 64.0 & 28.95 & 0.12 & 58.27 & 47.25 & 61.78 & 47.25 \\ %72.52 \\ 
Mobile-Former-52m & 3.5M & Hybrid & 68.7 & 62.16 & 17.29 & 67.32 & 62.01 & 67.79 & 62.01 \\ %72.52 \\ 
Mobile-Former-96m & 4.6M & Hybrid & 72.8 & 53.31 & 33.68 & 71.32 & 64.72 & 71.60 & 64.72 \\ %72.52 \\ 
Mobile-Former-151m & 7.6M & Hybrid & 75.2 & 4.98 & 3.49 & 73.86 & 68.16 & 74.30 & 68.16 \\ %72.52 \\ 
Mobile-Former-214m & 9.4M & Hybrid & 76.7 & 72.79 & 28.32 & 75.01 & 68.24 & 75.76 & 68.24 \\ %72.52 \\ 
Mobile-Former-294m & 11.4 & Hybrid & 77.9 & 74.15 & 59.55 & 76.96 & 68.24 & 76.93 & 74.48 \\ %72.52 \\ 
Mobile-Former-506m & 14.0M & Hybrid & 79.3 & 78.01 & 67.14 & 75.44 & 70.13 & 75.60 & 70.13 \\ %72.52 \\ 
\midrule
EfficientFormerV1-L1 & 12.3M & MetaBlock & 80.2 & 78.24 & 58.83 & 80.11 & 79.8 & 80.15 & 77.25 \\ %72.52 \\
EfficientFormerV1-L3 & 31.3M & MetaBlock & 82.4 & 82.39 & 80.38 & 82.39 & 82.36 & 82.46 & 82.18 \\ %72.52 \\ 
EfficientFormerV1-L7 & 82.1M & MetaBlock & 83.3 & 83.24 & 81.89 & 83.34 & 83.16 & 83.31 & 83.12 \\ %72.52 \\ 
\midrule
EfficientFormerV2-S0 & 3.5M & Hybrid & 76.2 & 68.21 & 41.24 & 68.40 & 41.26 & 74.69 & 74.18 \\ %72.52 \\ 
EfficientFormerV2-S1 & 6.1M & Hybrid & 79.7 & 66.42 & 2.69 & 73.44 & 73.34 & 77.56 & 77.54 \\ %72.52 \\ 
EfficientFormerV2-S2 & 12.6M & Hybrid & 82.0 & 71.80 & 7.02 & 79.85 & 79.39 & 80.62 & 80.30 \\ %72.52 \\ 
EfficientFormerV2-L & 26.1M & Hybrid & 83.5 & 80.34 & 3.34 & 82.46 & 82.22 & 82.80 & 82.71 \\ %72.52 \\ 
% ours
%MobileViTv2-100 & 4.3M & Hybrid & 78.09 & 69.34 & 0.12 & 61.74  & 0.1 \\ %0.1 \\ 
% ours
\bottomrule
\end{tabular}
}
\end{table}
\fi

\begin{table}[t]
\centering
\caption{Fully quantized accuracy of hybrid vision transformer architectures.}
\vspace*{-0.25cm}
\label{table:full_accuracy}
\resizebox{\columnwidth}{!}{
%\resizebox{\textwidth}{!}{
\begin{tabular}{@{\hskip0pt}l@{\hskip0pt}c@{\hskip0pt}c@{\hskip0pt}c@{\hskip0pt}c@{\hskip0pt}c@{\hskip0pt}c@{\hskip0pt}c@{\hskip0pt}c@{\hskip0pt}
} %~\cite{migacz20178,wu2020integer}
\toprule
\multicolumn{1}{c}{\textbf{Model}} &
\multicolumn{1}{c}{\textbf{\# Params.}} &
\multicolumn{1}{c}{\textbf{Type}} &
\multicolumn{1}{c}{\textbf{FP32}} & 
\multicolumn{1}{c}{\textbf{FQ-ViT}} & 
\multicolumn{1}{c}{\textbf{Ours}}
\\   \midrule
\midrule
MobileViTv1-xxs (MVv1-xxs) & 1.3M & Hybrid & 68.91 & 0.1 & 67.20 \\ 
% ours
MobileViTv1-xs (MVv1-xs) & 2.3M & Hybrid & 74.64 & 62.2 & 73.89 \\ 
% ours
MobileViTv1-s (MVv1-s) & 5.6M & Hybrid & 78.31 & 74.94 & 77.72 \\
% ours 
MobileViTv2-050 (MVv2-050) & 1.4M & Hybrid & 70.16 & 5.00 & 68.73 \\
% ours
MobileViTv2-075 (MVv2-075) & 2.8M & Hybrid & 75.62 & 34.60 & 74.36 \\
% ours
MobileViTv2-100 (MVv2-100) & 4.3M & Hybrid & 78.09 & 0.40 & 77.13 \\ 
% ours
\bottomrule
\end{tabular}
}
\vspace*{0.1cm}
\end{table}

\vspace*{-0.15cm}
\section{Evaluation}
We conduct an extensive comparison of Q-HyViT against various existing quantization methods. As previously discussed, there is no comprehensive method to quantize the hybrid vision transformers, so we directly implement the following open-sourced state-of-the-art quantization algorithms for pure vision transformers, namely EasyQuant~\cite{wu2020easyquant}, FQ-ViT~\cite{lin2022fq}, and PTQ4ViT~\cite{yuan2022ptq4vit}, and then apply them on five hybrid vision transformer architectures.

%In this section, we compared the proposed method with several quantization algorithms. 
%As we mentioned earlier, there is no comprehensive method to quantize the hybrid vision transformer.
%Therefore, we directly implemented the following open-sourced algorithms, EasyQuant~\cite{wu2020easyquant}, FQ-ViT~\cite{lin2022fq}, and PTQ4ViT~\cite{yuan2022ptq4vit}, on hybrid vision transformer.

\vspace*{-0.1cm}
\subsection{Accuracy Evaluation}
We select MobileViTv1 \cite{mehta2021mobilevit}, MobileViTv2 \cite{mehta2022separable}, Mobile-Former \cite{chen2022mobile}, EfficientFormerV1 \cite{li2022efficientformer}, and EfficientFormerV2~\cite{li2022rethinking} as representative hybrid vision transformer architectures.

\subsubsection{Results with Partial Quantization.}
Table \ref{table:accuracy} shows quantization results on hybird ViT architectures with varying model sizes, in terms of 8-bit and 6-bit quantization, where softmax and layer-norm remain under floating-point.

Upon examining the table, it is evident that previous works have experienced a significant decrease in accuracy, even with 8-bit quantization, in hybrid vision transformers. 
%FQ-ViT shows severe accuracy degradation due to zero-point overflow problems in the bridge block and layer dependencies, despite not quantizing softmax and layer norm, unlike the original version. 
PTQ4ViT and EasyQuant perform fairly well by exploring scaling factors for each layer using Hessian and Cosine similarity to minimize reconstruction error when the model size is large.
However, for models with fewer than 5M parameters, such as extremely lightweight models, existing quantization methods inadequately address dynamic activation changes. This leads to significant accuracy degradation, even in 8-bit settings. 
In contrast, our Q-HyViT achieves less than 1\% accuracy drop with 8-bit quantization on the sub-5M models including xxs, xs, 050, 075, 100, 26m, 52m, 96m, and S0.
In summary, Q-HyViT exhibits average improvements of 9.54\% and 7.09\% over EasyQuant and PTQ4ViT, respectively, with an 8-bit setting. Under the 6-bit setup, the improvements reach 43.39\% and 8.65\%, respectively.

%improves on EasyQuant and PTQ4ViT on average by 9.54\% and 7.09\% under an 8-bit setting, respectively, and by 43.39\% and 8.65\% under a 6-bit setting.




%ptq4vit, 8/32 = gradient =1 0.84064
\iffalse
\begin{table*}[t]
\centering
\caption{Quantization accuracy of pure vision transformer models}
\label{table:accuracy_hybrid}
%\resizebox{\columnwidth}{!}{
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccc
} %~\cite{migacz20178,wu2020integer}
\toprule
\multicolumn{1}{c}{\textbf{Model}} &
\multicolumn{1}{c}{\textbf{\# Params.}} &
\multicolumn{1}{c}{\textbf{Type}} &
\multicolumn{1}{c}{\textbf{FP32}} & 
\multicolumn{1}{c}{\textbf{Liu21~\cite{liu2021post}}} &
\multicolumn{1}{c}{\textbf{EasyQuant~\cite{wu2020easyquant}}} &
\multicolumn{1}{c}{\textbf{FQ-ViT~\cite{lin2022fq}}} & 
\multicolumn{1}{c}{\textbf{PTQ4ViT~\cite{yuan2022ptq4vit}}} & 
\multicolumn{1}{c}{\textbf{Ours}}
\\   \midrule
%FP32 (timm, vit_base_patch16_224), vit-paper(84.15(jax), JFT-300M) 
ViT-B   & 86M   & Pure & 84.53  & ?     & ? & 83.31 & 84.25 (84.09 8/32 0.8406 8/8) & ? \\
Swin-B  & 88M   & Pure & 83.60  & -     & ? & 82.38 & -                             & 82.782 \\
DeiT-B  & 86M   & Pure & 81.85  & 80.48 & ? & 80.85 & 81.48                         & 80.820 \\
Swin-S  & M     & Pure & 83.23  & -     & ? & 82.71 & 83.10                         &  \\
DeiT-S  & M     & Pure & 79.85  & 77.47 & ? & 79.17 & 79.47                         &  \\
Swin-T  & 29M   & Pure & 81.35  & -     & ? & 80.04 & -                             & 80.458 \\ 
DeiT-T  & 5.7M  & Pure & 72.21  & -     & ? & 71.07 & -                             & 71.086 \\ 
\bottomrule
\end{tabular}
}
\end{table*}
\fi

\iffalse
\begin{table*}[t]
\centering
\caption{Accuracy}
\label{table:accuracy_pure}
%\resizebox{\columnwidth}{!}{
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccc
} %~\cite{migacz20178,wu2020integer}
\toprule
\multicolumn{1}{c}{\textbf{Model}} &
\multicolumn{1}{c}{\textbf{\# Params.}} &
\multicolumn{1}{c}{\textbf{Type}} &
\multicolumn{1}{c}{\textbf{FP32}} & 
\multicolumn{1}{c}{\textbf{Liu21~\cite{liu2021post}}} &
\multicolumn{1}{c}{\textbf{EasyQuant~\cite{wu2020easyquant}}} &
\multicolumn{1}{c}{\textbf{FQ-ViT~\cite{lin2022fq}}} & 
\multicolumn{1}{c}{\textbf{PTQ4ViT~\cite{yuan2022ptq4vit}}} & 
\multicolumn{1}{c}{\textbf{Ours}}
\\   \midrule
%FP32 (timm, vit_base_patch16_224), vit-paper(84.15(jax), JFT-300M) 
ViT-B   & 86M   & Pure & 84.53  & ?     & ? & 83.31 & 84.25 (84.09 8/32 0.8406 8/8) & ? \\
Swin-B  & 88M   & Pure & 83.60  & -     & ? & 82.38 & -                             & 82.782 \\
DeiT-B  & 86M   & Pure & 81.85  & 80.48 & ? & 80.85 & 81.48                         & 80.820 \\
Swin-S  & M     & Pure & 83.23  & -     & ? & 82.71 & 83.10                         &  \\
DeiT-S  & M     & Pure & 79.85  & 77.47 & ? & 79.17 & 79.47                         &  \\
Swin-T  & 29M   & Pure & 81.35  & -     & ? & 80.04 & -                             & 80.458 \\ 
DeiT-T  & 5.7M  & Pure & 72.21  & -     & ? & 71.07 & -                             & 71.086 \\ 
\midrule
MobileViTv1-xxs & 1.3M & Hybrid & 68.908 & - & ? & 0.00  & 0.00 & 66.706 \\ 
% oursㅅ
MobileViTv1-xs & 2.3M & Hybrid & 74.638 & - & ? & 0.00  & 0.00 & 73.874 \\ 
% ours
MobileViTv1-s & 5.6M & Hybrid & 78.314 & - & ? & 0.00  & 0.00 & 77.722 \\
% ours 
MobileViTv2-050 & 1.4M & Hybrid & 70.160 & - & ? & 0.00  & 0.00 & 68.726 \\
% ours
MobileViTv2-075 & 2.8M & Hybrid & 75.616 & - & ? & 0.00  & 0.00 & 74.364 \\ 
% ours
MobileViTv2-100 & 4.3M & Hybrid & 78.086 & - & ? & 0.00  & 0.00 & 77.128 \\ 
% ours
\bottomrule
\end{tabular}
}
\end{table*}
\fi


\begin{table}[t]
\centering
\caption{Ablation study on hybrid reconstruction error minimization, where \cmark denotes that the component is considered. Results without employing all components are for FQ-ViT.}
\vspace*{-0.25cm}
\label{table:ablation}
\resizebox{\columnwidth}{!}{
%\resizebox{5cm}{!}{
\begin{tabular}{ccccc%rr
}
\toprule
\multicolumn{1}{c}{\textbf{Model}}  &
\multicolumn{1}{c}{\textbf{Scale Factor}} & \multicolumn{1}{c}{\textbf{Granularity}} & \multicolumn{1}{c}{\textbf{Quant. Scheme}} & \multicolumn{1}{c}{\textbf{Accuracy}} 
\\   \midrule
\midrule
\multirow{4}{*}{MobileViTv1-xxs} & \xmark & \xmark & \xmark & 37.75  \\
  & \cmark & \xmark & \xmark & 44.37  \\
  & \cmark & \cmark & \xmark & 59.50  \\
  & \cmark & \cmark & \cmark & 68.20  \\
\midrule
\multirow{4}{*}{MobileViTv1-xs} & \xmark & \xmark & \xmark &  65.52  \\
  & \cmark & \xmark & \xmark &  69.12 \\
  & \cmark & \cmark & \xmark &  72.00  \\
  & \cmark & \cmark & \cmark & 74.31 \\
\midrule
\multirow{4}{*}{MobileViTv1-s} & \xmark & \xmark & \xmark & 68.19 \\
  & \cmark & \xmark & \xmark & 73.02\\
  & \cmark & \cmark & \xmark & 77.01 \\
  & \cmark & \cmark & \cmark & 77.92 \\
\midrule
\multirow{4}{*}{MobileViTv2-050} & \xmark & \xmark & \xmark & 39.39 \\
  & \cmark & \xmark & \xmark & 49.62\\
  & \cmark & \cmark & \xmark & 69.89 \\
  & \cmark & \cmark & \cmark & 69.89 \\
\midrule
\multirow{4}{*}{MobileViTv2-075} & \xmark & \xmark & \xmark & 65.54 \\
  & \cmark & \xmark & \xmark & 67.24 \\
  & \cmark & \cmark & \xmark & 75.29 \\
  & \cmark & \cmark & \cmark & 75.29 \\
\midrule
\multirow{4}{*}{MobileViTv2-100} & \xmark & \xmark & \xmark & 51.02  \\
  & \cmark & \xmark & \xmark & 68.18\\
  & \cmark & \cmark & \xmark & 77.63 \\
  & \cmark & \cmark & \cmark & 77.63\\
\bottomrule
\multirow{4}{*}{MobileViTv2-125} & \xmark & \xmark & \xmark & 67.39 \\
  & \cmark & \xmark & \xmark & 75.39\\
  & \cmark & \cmark & \xmark & 79.31 \\
  & \cmark & \cmark & \cmark & 79.31\\
\bottomrule
\multirow{4}{*}{MobileViTv2-150} & \xmark & \xmark & \xmark & 68.61 \\
  & \cmark & \xmark & \xmark & 75.88\\
  & \cmark & \cmark & \xmark & 80.09 \\
  & \cmark & \cmark & \cmark & 80.09\\
\bottomrule
\multirow{4}{*}{MobileViTv2-175} & \xmark & \xmark & \xmark & 72.30 \\
  & \cmark & \xmark & \xmark & 76.81\\
  & \cmark & \cmark & \xmark & 80.63 \\
  & \cmark & \cmark & \cmark & 80.63 \\
\bottomrule
\multirow{4}{*}{MobileViTv2-200} & \xmark & \xmark & \xmark & 75.50 \\
  & \cmark & \xmark & \xmark & 77.91\\
  & \cmark & \cmark & \xmark & 80.94 \\
  & \cmark & \cmark & \cmark & 80.94 \\
\bottomrule

\end{tabular}
}
\vspace*{0.3cm}

\end{table}

Specifically, MobileViTv2 shows more accuracy drop than MobileViTv1 at the same model size, potentially due to its use of linear attention in self-attention computation, which results in fewer attention maps and lower resilience after post-softmax values.
In the case of EfficientFormerV1, there is no significant difference between the conventional method and the proposed method. The reason for this is that in the meta block, convolution and transformer layers are not used in a hybrid manner.
Furthermore, we observe that larger hybrid vision transformers are less sensitive to low-bit quantization (6-bit), as evidenced by the accuracy drops of MobileVitv1-xxs, MobileVitv1-xs, and MobileVitv1-s, which are 2.67, 1.36, and 1.22, respectively. This pattern is also observed in MobileViTv2, Mobile-Former, EfficientFormerV1, and EfficientFormerV2.
This phenomenon is attributed to larger networks having more weights and generating more activations, making them more resilient to perturbations caused by quantization.

% result on Fully quantized models
\subsubsection{Results with Full Quantization.}
Previous studies~\cite{liu2021post, yuan2022ptq4vit, wu2020easyquant, apq22} have refrained from quantizing softmax and layer normalization operations due to their smaller computational demand compared to matrix multiplication in terms of total FLOPs. 
Moreover, straightforward quantization of such non-linear functions may result in considerable accuracy degradation. Nonetheless, integer-only quantization~\cite{jacob2018quantization} is important especially for edge and mobile devices. This is due to the fact that softmax operation and layer normalization require dequantization for their computation in floating-point, as well as data movement involved in off-chip memory.
Thus, a fully quantized approach is necessary to alleviate significant hardware design challenges that arise from reducing off-chip level data transfer. 

In line with previous research~\cite{lin2022fq}, we apply a fully quantized approach, FQ-ViT, to hybrid ViTs, as summarized in Table \ref{table:full_accuracy}. Note that FQ-VIT shows very poor accuracy due to its use of an asymmetric scheme with zero points, which fails to handle the high variation of activation range by adjusting quantization granularity. 
Furthermore, in the case of MobileViTv2, group normalization is utilized instead of batch and layer norms, causing the existing L2 norm-based scaling factor exploration to function inaccurately. 
Our study addresses these issues and achieves an average of 43.63\% accuracy improvement over FQ-ViT.






\subsection{Ablation Study on Hybrid Reconstruction}
We perform an ablation study to assess the impact of utilizing the optimal selection of scale factor, granularity, and quantization scheme within the context of hybrid reconstruction error minimization. Table \ref{table:ablation} provides a summary of the ablation study results for various sizes and types of hybrid vision transformer architectures, where the results demonstrate that all the components enhance the accuracy of quantized hybrid ViTs.

When only optimizing the scaling factor with hybrid reconstruction error minimization, it does not yield significant performance improvements compared to PTQ4ViT~(baseline).
%
However, optimizing not only the scaling factors but also the granularity and scheme based on the bridge block lead to synergistic effects in general.
As a result, combining them all together leads to significant accuracy improvements.
In contrast, excluding the optimization of granularity significantly decreases the accuracy, highlighting highly dynamic activation ranges in lightweight hybrid ViTs as the main cause of the accuracy drop.


%------------------------------------------------------------------------
\section{Conclusion}

We addressed the problem of democratizing vision transformers on resource-constrained devices by proposing a method for minimizing quantization errors in hybrid vision transformers. 
The proposed method, Q-HyViT, identified the challenges of applying post-training quantization (PTQ) to hybrid vision transformers and proposed a unified method to mitigate errors in PTQ. 
Q-HyViT achieved this by selecting optimal scale factors, granularity, and scheme for both bridge and non-bridge layers based on hybrid reconstruction error minimization from a loss degradation perspective.
We demonstrated the effectiveness of Q-HyViT by conducting extensive experiments comparing it with existing several open-source algorithms, EasyQuant, FQ-ViT, and PTQ4ViT, on the same hybrid vision transformers. 
The results showed that Q-HyViT outperformed existing methods by a significant margin and achieved state-of-the-art accuracy on hybrid vision transformers in a fully quantized manner, including non-linear operations such as softmax and diverse normalization.
Finally, we contributed to the field of artificial intelligence by identifying the four unique challenges of quantizing hybrid vision transformers and proposing an effective solution for minimizing quantization error. 
%Our work has significant implications for democratizing vision transformers on resource-constrained devices, making it possible to use these models in real-world applications where computational resources are limited.





%\newpage

\section*{Appendix}
\setcounter{section}{0}

\def\thesection{\Alph{section}}


\section{Implementation Details}
For a fair comparison, we maintain most configurations consistent with EasyQuant, PTQ4ViT, and FQ-ViT. Specifically, our settings vary depending on whether the model is fully quantized or not. Additionally, the 5 hybrid vision transformer models are referred to as the official models.

\subsection{Model Download}
Except for Mobile-Former, the other four hybrid models leverage the \texttt{timm} framework~\footnote{https://github.com/huggingface/pytorch-image-models}. Meanwhile, Mobile-Former is built upon the implementation by AAboys~\footnote{https://github.com/AAboys/MobileFormer}. We successfully replicated the original accuracy using open-source codes under FP32 precision.


\subsection{Settings for EasyQuant and PTQ4ViT}
In EasyQuant, we quantize all operators, including fully-connected layers and matrix multiplications. To obtain the optimal scaling factors, we employ a search algorithm based on cosine distance. The search space is derived from $\alpha$ = 0.5 and $\beta$ = 1.2.

In PTQ4ViT, we adjust the hyperparameters to $\alpha$ = 0 and $\beta$ = 1.2. Similar to PTQ4ViT, this study adopts the parallel quantization method to prevent a significant accuracy drop caused by small datasets.

\subsection{Settings for FQ-ViT}
Essentially, we perform symmetric quantization on a per-channel basis for weights and asymmetric quantization on a per-layer basis for activations. To ensure a fair comparison, we set the quantization for weights to the minimum and maximum values. The hyperparameter K in the Power-of-Two Factor remains unchanged. For the calibration process, we select a sample of 1,000 images.

\subsection{Settings for Q-HyViT}
We quantize all the weights and inputs for the fully connected layers, including the first projection layer and the last head layer. Additionally, the two input matrices for the matrix multiplications in the self-attention modules are quantized. The inputs of the softmax and normalization layers are also quantized, consistent with FQ-ViT. We use 32 images for calibration, and unoptimized scaling factors are initialized with minimum and maximum values.

\begin{table*}[t]
\centering
\caption{Definition and precise configuration of the Bridge Block within each hybrid vision transformer}
\label{table:bridge_block}
\resizebox{\textwidth}{!}{
\begin{tabular}{lllc}
\toprule
\multicolumn{1}{c}{\textbf{Model}} &
\multicolumn{1}{c}{\textbf{Description}} &
\multicolumn{1}{c}{\textbf{Detailed Operators}} &
\multicolumn{1}{c}{\textbf{\# of Bridge Blocks}} 
\\   \midrule
\midrule
\multirow{4}{*}{MobileViTv1} & \multirow{4}{*}{\makecell{Convolution and reshape operators \\ to align the input dimensions of the Transformer
}} & \multirow{4}{*}{\texttt{\makecell{stages.2.1.convkxk.conv -> stages.2.1.conv1x1 \\ 
stages.3.1.convkxk.conv -> stages.3.1.conv1x1 \\
stages.4.1.convkxk.conv -> stages.4.1.conv1x1}}}  & \multirow{4}{*}{3} \\ 
  &  &  & \\ 
  &  &  &  \\
 &  &  &  \\  

\midrule
\multirow{7}{*}{MobileViTv2} & \multirow{7}{*}{\makecell{Convolution and reshape operators \\ to align the input dimensions of the Transformer}} & \multirow{7}{*}{\texttt{\makecell{stages.2.1.convkxk.conv ->
stages.2.1.conv1x1 \\
stages.3.1.convkxk.conv ->
stages.3.1.conv1x1 \\
stages.4.1.convkxk.conv ->
stages.4.1.conv1x1}}} & \multirow{7}{*}{3} \\ %68.22 \\
% ours
  &  &   &   \\ %72.52 \\ 
% ours
  &  &   &   \\ %72.52 \\ 
  &  &   &   \\ %72.52 \\ 
  &  &   &   \\ %72.52 \\ 
  &  &   &   \\ %72.52 \\ 
  &  &   &   \\ %72.52 \\ 
\midrule
\multirow{16}{*}{Mobile-Former} & \multirow{16}{*}{\makecell{It refers to operators within the Mobile-Former Block \\ that transition bi-directionally \\ between local and global representations}} & \multirow{16}{*}{\texttt{\makecell{features.1.local global.proj ->
features.1.global block.ffn.0 \\
features.1.global local.proj ->
features.2.conv1.0 \\
features.2.local global.proj -> 
features.2.global block.ffn.0 \\
features.2.global local.proj ->
features.3.conv1.0 \\
features.3.local global.proj -> 
features.3.global block.ffn.0  \\
features.3.global local.proj ->
features.4.conv1.0 \\
features.4.local global.proj -> 
features.4.global block.ffn.0 \\
features.4.global local.proj ->
features.5.conv1.0 \\
features.5.local global.proj ->
features.5.global block.ffn.0 \\
features.5.global local.proj ->
features.6.conv1.0 \\
features.6.local global.proj ->
features.6.global block.ffn.0 \\
features.6.global local.proj ->
features.7.conv1.0 \\
features.7.local global.proj -> 
features.7.global block.ffn.0 \\
features.7.global local.proj ->
features.8.conv1.0 \\
features.8.local global.proj -> 
features.8.global block.ffn.0}}} & \multirow{16}{*}{15} \\ %72.52 \\ 
  &  &   &   \\ %72.52 \\ 
  &  &   &   \\ %72.52 \\ 
  &  &   &   \\ %72.52 \\ 
  &  &   &   \\ %72.52 \\ 
  &  &   &   \\ %72.52 \\ 
  &  &   &   \\ %72.52 \\ 
 &  &   &   \\ %72.52 \\ 
 &  &   &   \\ %72.52 \\ 
 &  &   &   \\ %72.52 \\ 
 &  &   &   \\ %72.52 \\ 
 &  &   &   \\ %72.52 \\  
 &  &   &   \\ %72.52 \\  
 &  &   &   \\ %72.52 \\  
 &  &   &   \\ %72.52 \\  
 &  &   &   \\ %72.52 \\  
 
\midrule
\multirow{3}{*}{EfficientFormerV1} & \multirow{3}{*}{\makecell{These are operators that convert feature maps \\ 
from 4D Meta Block to 3D Meta Block}} & \multirow{3}{*}{\texttt{\makecell{stages.2.blocks.5.mlp.fc2 ->
stages.3.downsample.conv -> \\
stages.3.blocks.0.mlp.fc1}}} & \multirow{3}{*}{1} \\ %72.52 \\
  &  &   &   \\ %72.52 \\ 
  &  &   &   \\ %72.52 \\ 
\midrule
\multirow{4}{*}{EfficientFormerV2} & \multirow{4}{*}{\makecell{The bridge block consists of local and global transition \\ operators exist in the 3rd and 4th stages}}  & \multirow{4}{*}{\texttt{\makecell{stages.2.downsample.conv.conv ->
stages.2.blocks.0.mlp.fc1.conv \\
stages.3.downsample.attn.proj.conv ->
stages.3.blocks.0.mlp.fc1.conv}}} & \multirow{4}{*}{2} \\ %72.52 \\ 
  &   &   &   \\ %72.52 \\ 
  &   &   &   \\ %72.52 \\ 
  &   &   &   \\ %72.52 \\ 
\bottomrule
\end{tabular}
}
\end{table*}

\subsection{Bridge Block Details}

The term \texttt{bridge block} refers to the transitional part that connects both convolution and transformer blocks. This block's precise configuration changes significantly among hybrid ViT structures. Table~\ref{table:bridge_block} lists detailed descriptions and operator names for each model.
For MobileFormer, EfficientFormerV1, and EfficientFormerV2, the exact names of the operators that compose the bridge block vary slightly depending on the size of the model. The names shown in Table~\ref{table:bridge_block} are based on the smallest model size.




\section{Accuracy Variation in the calibration dataset}
As listed in Tables 1 (partial quantization) and 2 (full quantization), the accuracy of each model varies depending on whether the softmax and layer-norm are quantized and the number of images utilized during the calibration phase. For a fair comparison with EasyQuant and PTQ4ViT as shown in Table 1, 32 images were employed to derive the results. In the case of FQ-ViT, the calibration utilized the 1,000 images referenced in the original paper. When increasing the number of images for calibration in partial quantization from 32 to 128, there is no significant difference in accuracy, as listed in Table~\ref{table:numberofimages}.


\begin{table}[h]
\centering
\caption{Quantization results under different numbers of calibration images. Partial means that \texttt{softmax} and \texttt{layer-norm} remain in floating-point, while full means that their operators are quantized}
\label{table:numberofimages}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
\multicolumn{1}{c}{\textbf{Model}} &
\multicolumn{1}{c}{\textbf{Quant.}} &
\multicolumn{1}{c}{\textbf{\# of images}} &
\multicolumn{1}{c}{\textbf{Accuracy}} 
\\   \midrule
\multirow{4}{*}{MobileViTv1-xxs} & Partial & 32 & 68.20  \\
  & Partial & 128 & 68.18   \\
  & Full & 500 & 67.16   \\
  & Full & 1,000 & 67.20   \\
\midrule
\multirow{4}{*}{MobileViTv1-xs} & Partial & 32 & 74.31  \\
  & Partial & 128 & 74.25    \\
  & Full & 500 & 73.82   \\
  & Full & 1,000 & 73.89   \\
\midrule
\multirow{4}{*}{MobileViTv2-s} & Partial & 32 & 77.92  \\
  & Partial & 128 & 77.67   \\
  & Full & 500 & 77.69   \\
  & Full & 1,000 & 77.72   \\
\midrule
\multirow{4}{*}{MobileViTv2-050} & Partial & 32 & 69.89  \\
  & Partial & 128 & 69.89 \\
  & Full & 500 & 68.52  \\
  & Full & 1,000 & 68.73  \\
\midrule
% ours
\multirow{4}{*}{MobileViTv2-075} & Partial & 32 &  75.29 \\
  & Partial & 128 & 75.32 \\
  & Full & 500 & 74.26\\
  & Full & 1,000 & 74.36  \\
% ours
\midrule
\multirow{4}{*}{MobileViTv2-100} & Partial & 32 &  77.63 \\
  & Partial & 128 & 77.75 \\
  & Full & 500 & 77.19 \\
  & Full & 1,000 & 77.13  \\
\bottomrule
\end{tabular}
}
\end{table}




\iffalse
\begin{figure}[t]
\centering
\subfloat[][]{\label{sub:hist_channel1_quant}\includegraphics[width=.4\textwidth]{./figures/59_channelwise}} \vfill
\subfloat[][]{\label{sub:hist_channel5_quant}\includegraphics[width=.4\textwidth]{./figures/59_layerwise}}
\caption{An overlapping histogram of quantized values (blue) and real values (orange) in the activation of  for 1st bridge block of MobileViTv1-xxs: (left) layer-wise quantization (right) channel-wise quantization}
\label{fig:channel_quant}
\end{figure}
\fi


\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{./figures/xxs_zeropoint_overflow_6plots}
	\caption{A histogram depicting the overlap between quantized values (blue) and real values (orange) for six activation layers in the 1st, 2nd, and 3rd bridge blocks of the MobileViTv1-xxs model} 
	\label{fig:xxs_overflow_granularity}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{./figures/xs_zeropoint_overflow_6plots}
	\caption{A histogram depicting the overlap between quantized values (blue) and real values (orange) for six activation layers in the 1st, 2nd, and 3rd bridge blocks of the MobileViTv1-xs model} 
	\label{fig:xs_overflow_granularity}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{./figures/s_zeropoint_overflow_6plots}
	\caption{A histogram depicting the overlap between quantized values (blue) and real values (orange) for six activation layers in the 1st, 2nd, and 3rd bridge blocks of the MobileViTv1-s model} 
	\label{fig:s_overflow_granularity}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{./figures/59_hist_layer_channel_comparison}
	\caption{An overlapping histogram of quantized values (blue) and real values (orange) in the activation of  for 1st bridge block of MobileViTv1-xxs: (left) channel-wise quantization (right) layer-wise quantization} 
	\label{fig:overflow_granularity}
\end{figure}

\section{Overflow Issues in Asymmetric Channel-wise Quantization}
In this section, we demonstrate that the zero-point overflow issue arises across multiple bridge blocks within each hybrid vision transformer model.
Two problematic activations are present in every bridge block.
Both MobileViTv1 and MobileViTv2 comprise three bridge blocks.
The distribution of activations before and after quantization for the convolution operations in each bridge block is illustrated in Fig.\ref{fig:xxs_overflow_granularity}, Fig.\ref{fig:xs_overflow_granularity}, and Fig.~\ref{fig:s_overflow_granularity}.

From the three figures, we have discovered that the impact of zero point overflow diminishes as our models increase in size. 
This is due to the fact that the influence of zero point overflow decreases as the number of channels expands, such as in the cases of 64 (xxs), 96 (xs), and 144 (s).
Regardless of the model size, it is clear that clamping of specific values continues to occur in the bridge block. 
However, the accuracy drop issue is mitigated as the model size increases.
In detail, for the FQ-ViT model, the accuracy is 0.1 for the xxs model, but it achieves accuracies of 62.2 and 74.94 for the xs and s models, respectively. In other words, although the same issue arises, as the model size increases, the impact on accuracy diminishes.
The reason for this mitigation is that the number of redundant parameters increases as the size of the model increases. This is similar to why pruning works well for large models with many redundant parameters such as ResNet.

Additionally, for smaller models, we have observed that the clamping issue associated with zero point overflow can be alleviated by transitioning from channel-wise quantization to layer-wise quantization, as demonstrated in Fig.~\ref{fig:overflow_granularity}.
In the end, the reason why the zero point overflows are clamped is due to the following:
\begin{align}
-128 &> q_{min} -\frac{r_{min}}{s} \nonumber \\
0 &> -\frac{r_{min}}{s} \nonumber \\ 
0 &< \frac{r_{min}}{s}
\label{eq:zero_proof}
\end{align}
In the above equation, $q_{min}$ represents the minimum value of 8-bit quantization, which is $-128$, $s$ is the scaling factor, and $r_{min}$ refers to the minimum value of the original activation. As shown in Eq.~\ref{eq:zero_proof}, since $s$ is always a positive value, if $r_{min}$ is greater than 0, the zero point exceeds the range of $q_{min}$ and becomes clamped.
Therefore, when quantizing on a per-channel basis, if activations are composed only of values greater than or equal to $0$, it significantly causes a decrease in accuracy. 
To solve this issue, using layer-wise quantization takes into account the entire layer, including values less than or equal to 0 as $r_{min}$. 
Consequently, the zero-point value falls within the 8-bit range ($-128$ to $127$).

\bibliography{aaai24}


\end{document}


