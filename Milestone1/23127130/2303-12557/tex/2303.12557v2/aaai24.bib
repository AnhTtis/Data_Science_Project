@book{em:86,
  editor  = "Engelmore, Robert and Morgan, Anthony",
  title   = "Blackboard Systems",
  year    = 1986,
  address = "Reading, Mass.",
  publisher = "Addison-Wesley",
}

@inproceedings{c:83,
  author  = "Clancey, William J.",
  year    = 1983,
  title   = "{Communication, Simulation, and Intelligent
Agents: Implications of Personal Intelligent Machines
for Medical Education}",
  booktitle="Proceedings of the Eighth International Joint Conference on Artificial Intelligence {(IJCAI-83)}", 
  pages   = "556-560",
  address = "Menlo Park, Calif",
  publisher = "{IJCAI Organization}",
}
@inproceedings{c:84,
  author  = "Clancey, William J.",
  year    = 1984,
  title   = "{Classification Problem Solving}",
  booktitle = "Proceedings of the Fourth National 
              Conference on Artificial Intelligence",
  pages   = "45-54",
  address = "Menlo Park, Calif.",
  publisher="AAAI Press",
}
@article{r:80,
  author = {Robinson, Arthur L.},
  title = {New Ways to Make Microcircuits Smaller},
  volume = {208},
  number = {4447},
  pages = {1019--1022},
  year = {1980},
  doi = {10.1126/science.208.4447.1019},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  URL = {https://science.sciencemag.org/content/208/4447/1019},
  eprint = {https://science.sciencemag.org/content/208/4447/1019.full.pdf},
  journal = {Science},
}
@article{r:80x,
  author  = "Robinson, Arthur L.",
  year    = 1980,
  title   = "{New Ways to Make Microcircuits Smaller---Duplicate Entry}",
  journal = "Science",
  volume  =  208,
  pages   = "1019-1026",
}
@article{hcr:83,
title = {Strategic explanations for a diagnostic consultation system},
journal = {International Journal of Man-Machine Studies},
volume = {20},
number = {1},
pages = {3-19},
year = {1984},
issn = {0020-7373},
doi = {https://doi.org/10.1016/S0020-7373(84)80003-6},
url = {https://www.sciencedirect.com/science/article/pii/S0020737384800036},
author = {Diane Warner Hasling and William J. Clancey and Glenn Rennels},
abstract = {This article examines the problem of automatte explanation of reasoning, especially as it relates to expert systems. By explanation we mean the ability of a program to discuss what it is doing in some understandable way. We first present a general framework in which to view explanation and review some of the research done in this area. We then focus on the explanation system for NEOMYCIN, a medical consultation program. A consultation program interactively helps a user to solve a problem. Our goal is to have NEOMYCIN explain its problem-solving strategies. An explanation of strategy describes the plan the program is using to reach a solution. Such an explanation is usually concrete, referring to aspects of the current problem situation. Abstract explanations articulate a general principle, which can be applied in different situations; such explanations are useful in teaching and in explaining by analogy. We describe the aspects of NEOMYCIN that make abstract strategic explanations possible—the representation of strategic knowledge explicitly and separately from domain knowledge— and demonstrate how this representation can be used to generate explanations.}
}
@article{hcrt:83,
  author  = "Hasling, Diane Warner and Clancey, William J. and Rennels, Glenn R. and Test, Thomas",
  year    = 1983,
  title   = "{Strategic Explanations in Consultation---Duplicate}",
  journal = "The International Journal of Man-Machine Studies",
  volume  = 20,
  number  = 1,
  pages   = "3-19",
}
@techreport{r:86,
  author  = "Rice, James",
  year    = 1986,
  title   = "{Poligon: A System for Parallel Problem Solving}",
  type    = "Technical Report", 
  number  = "KSL-86-19", 
  institution = "Dept.\ of Computer Science, Stanford Univ.",
}
@phdthesis{c:79,
  author  = "Clancey, William J.",
  year    = 1979,
  title   = "{Transfer of Rule-Based Expertise
through a Tutorial Dialogue}",
  type    = "{Ph.D.} diss.",
  school  = "Dept.\ of Computer Science, Stanford Univ.",
  address = "Stanford, Calif.",
}
@unpublished{c:21,
  author  = "Clancey, William J.",
  title   = "{The Engineering of Qualitative Models}",
  year    = 2021,
  note    = "Forthcoming",
}
@misc{c:22,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{c:23,
  title        = "Pluto: The 'Other' Red Planet",
  author       = "{NASA}",
  howpublished = "\url{https://www.nasa.gov/nh/pluto-the-other-red-planet}",
  year         = 2015,
  note         = "Accessed: 2018-12-06"
}


@misc{Authors14,
 author = {Full Author Name},
 title = {The Frobnicatable Foo Filter},
 note = {Face and Gesture  submission ID 324. Supplied as additional material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {Full Author Name},
 title = {Frobnication Tutorial},
 note = {Supplied as additional material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {Alvin Alpher},
title = {Frobnication},
journal = {Journal of Foo},
volume = 12, 
number = 1, 
pages = {234--778}, 
year = 2002
}

@article{Alpher03,
author = {Alvin Alpher and Ferris P.~N. Fotheringham-Smythe},
title = {Frobnication Revisited},
journal = {Journal of Foo},
volume = 13, 
number = 1, 
pages = {234--778}, 
year = 2003
}

@article{Alpher04,
author = {Alvin Alpher and Ferris P.~N. Fotheringham-Smythe and Gavin Gamow},
title = {Can a Machine Frobnicate?},
journal = {Journal of Foo},
volume = 14, 
number = 1, 
pages = {234--778}, 
year = 2004
}


@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}

@article{liu2021post,
  title={Post-training quantization for vision transformer},
  author={Liu, Zhenhua and Wang, Yunhe and Han, Kai and Zhang, Wei and Ma, Siwei and Gao, Wen},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={28092--28103},
  year={2021}
}

@inproceedings{lin2022fq,
  title={FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer},
  author={Lin, Yang and Zhang, Tianyu and Sun, Peiqin and Li, Zheng and Zhou, Shuchang},
  booktitle={Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22},
  pages={1173--1179},
  year={2022}
}

@inproceedings{yuan2022ptq4vit,
  title={PTQ4ViT: Post-training Quantization for Vision Transformers with Twin Uniform Quantization},
  author={Yuan, Zhihang and Xue, Chenhao and Chen, Yiqi and Wu, Qiang and Sun, Guangyu},
  booktitle={European Conference on Computer Vision},
  pages={191--207},
  year={2022},
  organization={Springer}
}

% APQ
@inproceedings{apq22,
  title={Towards Accurate Post-Training Quantization for Vision Transformer},
  author={Ding, Yifu and Qin, Haotong and Yan, Qinghua and Chai, Zhenhua and Liu, Junjie and Wei, Xiaolin and Liu, Xianglong},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={5380--5388},
  year={2022}
}

%NoisyQuant (CVPR2023)
@inproceedings{liu2023noisyquant,
  title={NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers},
  author={Liu, Yijiang and Yang, Huanrui and Dong, Zhen and Keutzer, Kurt and Du, Li and Zhang, Shanghang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20321--20330},
  year={2023}
}


@article{wu2020easyquant,
  title={EasyQuant: Post-training quantization via scale optimization},
  author={Wu, Di and Tang, Qi and Zhao, Yongle and Zhang, Ming and Fu, Ying and Zhang, Debing},
  journal={arXiv preprint arXiv:2006.16669},
  year={2020}
}

% AdaRound 
@inproceedings{adaround20,
  title={Up or down? adaptive rounding for post-training quantization},
  author={Nagel, Markus and Amjad, Rana Ali and Van Baalen, Mart and Louizos, Christos and Blankevoort, Tijmen},
  booktitle={International Conference on Machine Learning},
  pages={7197--7206},
  year={2020},
  organization={PMLR}
}

% BRECQ
@article{brecq21,
  title={Brecq: Pushing the limit of post-training quantization by block reconstruction},
  author={Li, Yuhang and Gong, Ruihao and Tan, Xu and Yang, Yang and Hu, Peng and Zhang, Qi and Yu, Fengwei and Wang, Wei and Gu, Shi},
  journal={arXiv preprint arXiv:2102.05426},
  year={2021}
}

%AdaQuant
@inproceedings{hubara2021accurate,
  title={Accurate post training quantization with small calibration sets},
  author={Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={4466--4475},
  year={2021},
  organization={PMLR}
}
%QDrop
@article{wei2022qdrop,
  title={QDrop: randomly dropping quantization for extremely low-bit post-training quantization},
  author={Wei, Xiuying and Gong, Ruihao and Li, Yuhang and Liu, Xianglong and Yu, Fengwei},
  journal={arXiv preprint arXiv:2203.05740},
  year={2022}
}
%NWQ
@inproceedings{wangleveraging,
  title={Leveraging Inter-Layer Dependency for Post-Training Quantization},
  author={Wang, Changbao and Zheng, DanDan and Liu, Yuanliu and Li, Liang},
  booktitle={Advances in Neural Information Processing Systems}
}


%---- from Quantune ----
% ETRI jouranl
@article{astrid2018deep,
  title={Deep compression of convolutional neural networks with low-rank approximation},
  author={Astrid, Marcella and Lee, Seung-Ik},
  journal={ETRI Journal},
  volume={40},
  number={4},
  pages={421--434},
  year={2018},
  publisher={Wiley Online Library}
}

% active and continual learning
@article{nguyen2017variational,
  title={Variational continual learning},
  author={Nguyen, Cuong V and Li, Yingzhen and Bui, Thang D and Turner, Richard E},
  journal={arXiv preprint arXiv:1710.10628},
  year={2017}
}
@article{lopez2017gradient,
  title={Gradient episodic memory for continual learning},
  author={Lopez-Paz, David and Ranzato, Marc'Aurelio},
  journal={Advances in neural information processing systems},
  volume={30},
  pages={6467--6476},
  year={2017}
}
@article{doulamis2000line,
  title={On-line retrainable neural networks: improving the performance of neural networks in image analysis problems},
  author={Doulamis, Anastasios D and Doulamis, Nikolaos D and Kollias, Stefanos D},
  journal={IEEE Transactions on neural networks},
  volume={11},
  number={1},
  pages={137--155},
  year={2000},
  publisher={IEEE}
}
@inproceedings{shin2017fixed,
  title={Fixed-point optimization of deep neural networks with adaptive step size retraining},
  author={Shin, Sungho and Boo, Yoonho and Sung, Wonyong},
  booktitle={2017 IEEE International conference on acoustics, speech and signal processing (ICASSP)},
  pages={1203--1207},
  year={2017},
  organization={IEEE}
}

%XGboost applications
@article{dhaliwal2018effective,
  title={Effective intrusion detection system using XGBoost},
  author={Dhaliwal, Sukhpreet Singh and Nahid, Abdullah-Al and Abbas, Robert},
  journal={Information},
  volume={9},
  number={7},
  pages={149},
  year={2018},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@inproceedings{chen2018xgboost,
  title={XGBoost classifier for DDoS attack detection and analysis in SDN-based cloud},
  author={Chen, Zhuo and Jiang, Fu and Cheng, Yijun and Gu, Xin and Liu, Weirong and Peng, Jun},
  booktitle={2018 IEEE international conference on big data and smart computing (bigcomp)},
  pages={251--256},
  year={2018},
  organization={IEEE}
}
@article{zhang2018data,
  title={A data-driven design for fault detection of wind turbines using random forests and XGboost},
  author={Zhang, Dahai and Qian, Liyang and Mao, Baijin and Huang, Can and Huang, Bin and Si, Yulin},
  journal={Ieee Access},
  volume={6},
  pages={21020--21031},
  year={2018},
  publisher={IEEE}
}
@article{ji2019five,
  title={Five-feature model for developing the classifier for synergistic vs. Antagonistic drug combinations built by XGBoost},
  author={Ji, Xiangjun and Tong, Weida and Liu, Zhichao and Shi, Tieliu},
  journal={Frontiers in genetics},
  volume={10},
  pages={600},
  year={2019},
  publisher={Frontiers}
}
@article{ogunleye2019xgboost,
  title={XGBoost model for chronic kidney disease diagnosis},
  author={Ogunleye, Adeola and Wang, Qing-Guo},
  journal={IEEE/ACM transactions on computational biology and bioinformatics},
  volume={17},
  number={6},
  pages={2131--2140},
  year={2019},
  publisher={IEEE}
}
@article{budholiya2020optimized,
  title={An optimized XGBoost based diagnostic system for effective prediction of heart disease},
  author={Budholiya, Kartik and Shrivastava, Shailendra Kumar and Sharma, Vivek},
  journal={Journal of King Saud University-Computer and Information Sciences},
  year={2020},
  publisher={Elsevier}
}
% survey paper: XGBoost is the winning algorithm in many Kaggle competitions.
@article{sagi2018ensemble,
  title={Ensemble learning: A survey},
  author={Sagi, Omer and Rokach, Lior},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  volume={8},
  number={4},
  pages={1249},
  year={2018},
  publisher={Wiley Online Library}
}
%XGBoost book
@book{hastie2009elements,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year={2009},
  publisher={Springer Science \& Business Media}
}
%XGBoost 
@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  pages={785--794},
  year={2016}
}

%VTA Result
@incollection{moreau2018leveraging,
  title={Leveraging the vta-tvm hardware-software stack for fpga acceleration of 8-bit resnet-18 inference},
  author={Moreau, Thierry and Chen, Tianqi and Ceze, Luis},
  booktitle={Proceedings of the 1st on Reproducible Quality-Efficient Systems Tournament on Co-designing Pareto-efficient Deep Learning},
  pages={1},
  year={2018}
}

%VTA hardware
@article{moreau2019hardware,
  title={A hardware--software blueprint for flexible deep learning specialization},
  author={Moreau, Thierry and Chen, Tianqi and Vega, Luis and Roesch, Jared and Yan, Eddie and Zheng, Lianmin and Fromm, Josh and Jiang, Ziheng and Ceze, Luis and Guestrin, Carlos and others},
  journal={IEEE Micro},
  volume={39},
  number={5},
  pages={8--16},
  year={2019},
  publisher={IEEE}
}

%nGraph
@article{cyphers2018intel,
  title={Intel ngraph: An intermediate representation, compiler, and executor for deep learning},
  author={Cyphers, Scott and Bansal, Arjun K and Bhiwandiwalla, Anahita and Bobba, Jayaram and Brookhart, Matthew and Chakraborty, Avijit and Constable, Will and Convey, Christian and Cook, Leona and Kanawi, Omar and others},
  journal={arXiv preprint arXiv:1801.08058},
  year={2018}
}

% DL Compilers
% Glow
@article{rotem2018glow,
  title={Glow: Graph lowering compiler techniques for neural networks},
  author={Rotem, Nadav and Fix, Jordan and Abdulrasool, Saleem and Catron, Garret and Deng, Summer and Dzhabarov, Roman and Gibson, Nick and Hegeman, James and Lele, Meghan and Levenstein, Roman and others},
  journal={arXiv preprint arXiv:1805.00907},
  year={2018}
}
% TVM
@inproceedings{chen2018tvm,
  title={$\{$TVM$\}$: An automated end-to-end optimizing compiler for deep learning},
  author={Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
  booktitle={13th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 18)},
  pages={578--594},
  year={2018}
}


%MLPerf
@inproceedings{reddi2020mlperf,
  title={Mlperf inference benchmark},
  author={Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and others},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={446--459},
  year={2020},
  organization={IEEE}
}

% FBNetv2
@inproceedings{wan2020fbnetv2,
  title={Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions},
  author={Wan, Alvin and Dai, Xiaoliang and Zhang, Peizhao and He, Zijian and Tian, Yuandong and Xie, Saining and Wu, Bichen and Yu, Matthew and Xu, Tao and Chen, Kan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12965--12974},
  year={2020}
}
% FBNet
@inproceedings{wu2019fbnet,
  title={Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search},
  author={Wu, Bichen and Dai, Xiaoliang and Zhang, Peizhao and Wang, Yanghan and Sun, Fei and Wu, Yiming and Tian, Yuandong and Vajda, Peter and Jia, Yangqing and Keutzer, Kurt},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10734--10742},
  year={2019}
}
% CGO20, low bit kernel code generation
@inproceedings{cowan2020automatic,
  title={Automatic generation of high-performance quantized machine learning kernels},
  author={Cowan, Meghan and Moreau, Thierry and Chen, Tianqi and Bornholt, James and Ceze, Luis},
  booktitle={Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
  pages={305--316},
  year={2020}
}
%CVPR18, Google, integer-arithmetic-only
@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2704--2713},
  year={2018}
}
% Efficient kernel code generation 
@article{jain2020efficient,
  title={Efficient execution of quantized deep learning models: A compiler approach},
  author={Jain, Animesh and Bhattacharya, Shoubhik and Masuda, Masahiro and Sharma, Vin and Wang, Yida},
  journal={arXiv preprint arXiv:2006.10226},
  year={2020}
}

%google white paper (PTQ, QAT) 
@article{kris_whitepaper2018,
  title={Quantizing deep convolutional networks for efficient inference: A whitepaper},
  author={Krishnamoorthi, Raghuraman},
  journal={arXiv preprint arXiv:1806.08342},
  year={2018}
}

%---QAT-- 
@inproceedings{lin2016fixed,
  title={Fixed point quantization of deep convolutional networks},
  author={Lin, Darryl and Talathi, Sachin and Annapureddy, Sreekanth},
  booktitle={International conference on machine learning},
  pages={2849--2858},
  year={2016},
  organization={PMLR}
}
@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@inproceedings{steven2020,
  author    = {Steven K. Esser and
               Jeffrey L. McKinstry and
               Deepika Bablani and
               Rathinakumar Appuswamy and
               Dharmendra S. Modha},
  title     = {Learned Step Size quantization},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  pages={1--12},
  year      = {2020},
  timestamp = {Thu, 07 May 2020 17:11:47 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{PACT2018,
  title={Pact: Parameterized clipping activation for quantized neural networks},
  author={Choi, Jungwook and Wang, Zhuo and Venkataramani, Swagath and Chuang, Pierce I-Jen and Srinivasan, Vijayalakshmi and Gopalakrishnan, Kailash},
 booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=ryQu7f-RZ}
}
@inproceedings{zhang2018lq,
  title={Lq-nets: Learned quantization for highly accurate and compact deep neural networks},
  author={Zhang, Dongqing and Yang, Jiaolong and Ye, Dongqiangzi and Hua, Gang},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={365--382},
  year={2018}
}
@inproceedings{jung2019learning,
  title={Learning to quantize deep networks by optimizing quantization intervals with task loss},
  author={Jung, Sangil and Son, Changyong and Lee, Seohyung and Son, Jinwoo and Han, Jae-Joon and Kwak, Youngjun and Hwang, Sung Ju and Choi, Changkyu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4350--4359},
  year={2019}
}
@article{zhou2016dorefa,
  title={Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients},
  author={Zhou, Shuchang and Wu, Yuxin and Ni, Zekun and Zhou, Xinyu and Wen, He and Zou, Yuheng},
  journal={arXiv preprint arXiv:1606.06160},
  year={2016}
}
@inproceedings{jacob2018,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2704--2713},
  year={2018}
}
@inproceedings{songhan2016,
  author    = {Song Han and
               Huizi Mao and
               William J. Dally},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Deep Compression: Compressing Deep Neural Network with Pruning, Trained
               Quantization and Huffman Coding},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1510.00149},
  timestamp = {Fri, 20 Nov 2020 16:16:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/HanMD15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%QAT for integer hardware.
@inproceedings{zhao2019linear,
  title={Linear symmetric quantization of neural networks for low-precision integer hardware},
  author={Zhao, Xiandong and Wang, Ying and Cai, Xuyi and Liu, Cheng and Zhang, Lei},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

%---PTQ---
@inproceedings{cai2020zeroq,
  title={Zeroq: A novel zero shot quantization framework},
  author={Cai, Yaohui and Yao, Zhewei and Dong, Zhen and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13169--13178},
  year={2020}
}

@inproceedings{wang2020towards,
  title={Towards accurate post-training network quantization via bit-split and stitching},
  author={Wang, Peisong and Chen, Qiang and He, Xiangyu and Cheng, Jian},
  booktitle={International Conference on Machine Learning},
  pages={9847--9856},
  year={2020},
  organization={PMLR}
}

@article{zhong2019ada,
  title={Ada-tucker: Compressing deep neural networks via adaptive dimension adjustment tucker decomposition},
  author={Zhong, Zhisheng and Wei, Fangyin and Lin, Zhouchen and Zhang, Chao},
  journal={Neural Networks},
  volume={110},
  pages={104--115},
  year={2019},
  publisher={Elsevier}
}

@article{jiang2021automated,
  title={Automated Backend-Aware Post-Training Quantization},
  author={Jiang, Ziheng and Jain, Animesh and Liu, Andrew and Fromm, Josh and Ma, Chengqian and Chen, Tianqi and Ceze, Luis},
  journal={arXiv preprint arXiv:2103.14949},
  year={2021}
}
@inproceedings{banner_neurips2019,
 author = {Banner, Ron and Nahshan, Yury and Soudry, Daniel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Post training 4-bit quantization of convolutional networks for rapid-deployment},
 volume = {32},
 year = {2019}
}
@inproceedings{choukroun2019low,
  title={Low-bit Quantization of Neural Networks for Efficient Inference},
  author={Choukroun, Yoni and Kravchik, Eli and Yang, Fan and Kisilev, Pavel},
  booktitle={ICCV Workshops},
  pages={3009--3018},
  year={2019}
}
@inproceedings{nagel2019data,
  title={Data-free quantization through weight equalization and bias correction},
  author={Nagel, Markus and Baalen, Mart van and Blankevoort, Tijmen and Welling, Max},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1325--1334},
  year={2019}
}
@inproceedings{zhao2019improving,
  title={Improving neural network quantization without retraining using outlier channel splitting},
  author={Zhao, Ritchie and Hu, Yuwei and Dotzel, Jordan and De Sa, Chris and Zhang, Zhiru},
  booktitle={International conference on machine learning},
  pages={7543--7552},
  year={2019},
  organization={PMLR}
}
@article{lee2018quantization,
  title={Quantization for rapid deployment of deep neural networks},
  author={Lee, Jun Haeng and Ha, Sangwon and Choi, Saerom and Lee, Won-Jo and Lee, Seungwon},
  journal={arXiv preprint arXiv:1810.05488},
  year={2018}
}
@article{goncharenko2019fast,
  title={Fast adjustable threshold for uniform neural network quantization},
  author={Goncharenko, Alexander and Denisov, Andrey and Alyamkin, Sergey and Terentev, Evgeny},
  journal={International Journal of Computer and Information Engineering},
  volume={13},
  number={9},
  pages={495--499},
  year={2019}
}
@InProceedings{meller19a,
  title = 	 {Same, Same But Different: Recovering Neural Network Quantization Error Through Weight Factorization},
  author =       {Meller, Eldad and Finkelstein, Alexander and Almog, Uri and Grobman, Mark},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4486--4495},
  year = 	 {2019},
  editor = 	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR}
}
@inproceedings{migacz20178,
  title={8-bit inference with tensorrt},
  author={Migacz, Szymon},
  booktitle={GPU technology conference},
  volume={2},
  number={4},
  pages={5},
  year={2017}
}
@article{wu2020integer,
  title={Integer quantization for deep learning inference: Principles and empirical evaluation},
  author={Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius},
  journal={arXiv preprint arXiv:2004.09602},
  year={2020}
}

% --- MobileViT v1 ---
@article{mehta2021mobilevit,
  title={Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer},
  author={Mehta, Sachin and Rastegari, Mohammad},
  journal={arXiv preprint arXiv:2110.02178},
  year={2021}
}
% --- MobileViT v2 ---
@article{mehta2022separable,
  title={Separable self-attention for mobile vision transformers},
  author={Mehta, Sachin and Rastegari, Mohammad},
  journal={arXiv preprint arXiv:2206.02680},
  year={2022}
}

% ViT 
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
% DeiT
@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}
% Swin
@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

% lightweight CNN
@inproceedings{chollet2017xception,
  title={Xception: Deep learning with depthwise separable convolutions},
  author={Chollet, Fran{\c{c}}ois},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1251--1258},
  year={2017}
}
@article{howard2017mobilenets,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}
@inproceedings{sandler2018mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4510--4520},
  year={2018}
}
@inproceedings{howard2019searching,
  title={Searching for mobilenetv3},
  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1314--1324},
  year={2019}
}
%shufflenetv2
@inproceedings{ma2018shufflenet,
  title={Shufflenet v2: Practical guidelines for efficient cnn architecture design},
  author={Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={116--131},
  year={2018}
}
%MNasNet
@inproceedings{tan2019mnasnet,
  title={Mnasnet: Platform-aware neural architecture search for mobile},
  author={Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2820--2828},
  year={2019}
}

% vit survey
@article{han2022survey,
  title={A survey on vision transformer},
  author={Han, Kai and Wang, Yunhe and Chen, Hanting and Chen, Xinghao and Guo, Jianyuan and Liu, Zhenhua and Tang, Yehui and Xiao, An and Xu, Chunjing and Xu, Yixing and others},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={45},
  number={1},
  pages={87--110},
  year={2022},
  publisher={IEEE}
}
@article{khan2022transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={10s},
  pages={1--41},
  year={2022},
  publisher={ACM New York, NY}
}

@inproceedings{chen2022mobile,
  title={Mobile-former: Bridging mobilenet and transformer},
  author={Chen, Yinpeng and Dai, Xiyang and Chen, Dongdong and Liu, Mengchen and Dong, Xiaoyi and Yuan, Lu and Liu, Zicheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5270--5279},
  year={2022}
}

@article{li2022efficientformer,
  title={Efficientformer: Vision transformers at mobilenet speed},
  author={Li, Yanyu and Yuan, Geng and Wen, Yang and Hu, Ju and Evangelidis, Georgios and Tulyakov, Sergey and Wang, Yanzhi and Ren, Jian},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12934--12949},
  year={2022}
}

@inproceedings{li2022rethinking,
  title={Rethinking Vision Transformers for MobileNet Size and Speed},
  author={Li, Yanyu and Hu, Ju and Wen, Yang and Evangelidis, Georgios and Salahi, Kamyar and Wang, Yanzhi and Tulyakov, Sergey and Ren, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  year={2023}
}