\section{Learning Tactile Dexterity}

\subsection{Problem Formulation}
We formulate the in-hand rotation problem as a Markov Decision Process $\mathcal{M}=(\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{P})$. Here, $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{R}$ is the reward function, and $\mathcal{P}$ is the transition dynamics. $\mathcal{R}$ and $\mathcal{P}$ are unknown to the robot. The robot agent observes state $s_t$ at each step $t$ and take action $a_t = \pi(s_t)$ calculated by the current policy $\pi$, then it will receive a reward $r_t = \mathcal{R}(s_t, a_t, s_{t+1})$. The goal of the agent is to maximize the $\gamma$ discounted return $\sum_{t=0}^T\gamma^t r_t$. The definition of these elements is as follows.

\subsubsection{State}
The state of the system consists of the joint position of the Allegro hand $q_t\in\mathbb{R}^{16}$, the sensor observation $o_t \in \{0, 1\}^{16}$, the previous position target $\tilde{q}_{t}\in\mathbb{R}^{16}$, and the rotation axis $k\in\mathbb{S}^2$. Since the state at one step may not be sufficient for control, we also stack it with other 3 historical states as the input when we use an MLP as the policy network.

\subsubsection{Action} At each step, the action produced by the policy network is a relative control command $a_t\in\mathbb{R}^{16}$. A PD controller then drives the hand to reach the joint position target $\tilde{q}_{t+1} = \tilde{q}_{t} + a_t$ at the next step. However, using this target directly may lead to non-smooth finger motion, since the actions of two consecutive steps may conflict with each other. Therefore, in practice, we use an exponential moving average as the target: $\tilde{q}_{t+1} = \tilde{q}_{t} + \tilde{a}_t$, where $\tilde{a}_t = \eta a_t + (1 - \eta)\tilde{a}_{t-1}, t\geq 1$ and $\tilde{a}_0 = 0$. We find that $\eta = 0.8$ works well in the experiments. This PD controller operates at a control frequency of 10Hz both in the simulation and the real.

\subsubsection{Reward} We design a reward function that is able to make the dexterous hand rotate the object in a smooth and transferable way. The reward function used in this paper is a weighted mixture of several components:
\begin{equation}
    r_t = w_1r_{rot} + w_2r_{vel} + w_3r_{fall} + w_4r_{work} + w_5r_{torque} + w_6r_{dist}.
\end{equation}
The first term $r_{rot}$ is the rotation reward defined as the rotated angle $\Delta \theta$ of a sampled unit vector in the normal plane $\Pi$ of the rotation axis $k$:
\begin{equation}
    r_{rot} = {\rm clip} (\Delta \theta, -c_1, c_1).    
\end{equation}
The detailed calculation of $\Delta \theta$ is shown in Figure~\ref{fig:rot}. First, we sample a unit vector $v$ in $\Pi$ randomly and we may as well imagine it is attached to the object. Then we fetch its corresponding vector $v'$ at the next state and project it to $\Pi$: $v'_p =  {\rm Proj} (v', \Pi)$. $\Delta \theta\in [-\pi, \pi)$ is defined as the signed distance between $v'_p$ and $v$ with respect to the axis $k$. Note that \cite{qi2022hand} uses $\langle \omega, k\rangle$ as the rotation reward, where $\omega$ is the angular velocity returned by the simulator. Nevertheless, we find that the angular velocity provided by the simulator in our setting is very noisy since the motion of the object is very complex. As a result, using this angular velocity in the reward can usually lead to very undesirable object motion patterns, like vibrating around a specific pose. We find that using this finite difference as the reward can produce consistent rotation behavior across different runs. The second term is a penalty on the object's velocity $r_{vel} = -\Vert v_t\Vert$. This encourages the hand to rotate the object in a stable manner and increases the transferability of the trained policy. The third reward $r_{fall}$ is a negative falling penalty when the object falls out of the palm. The fourth reward $r_{work}$ penalize the work of controller, which is defined as $r_{work} = -\langle|\tau|, |\dot{q_t}|\rangle$. Here, $\tau$ is the outputted torque of the PD controller at step $t$. This penalty helps to improve the smoothness of finger motion. The fifth term $r_{torque} = -\Vert\tau\Vert$ penalizes the large torque. Finally, $r_{dist} = {\rm mean}({\rm clip} (1/(\epsilon + d(x_{tip}, x_{obj})), c_2, c_3))$ is a distance reward, which encourages the fingertip to come close to the object and interact with it.

\begin{figure}[t]
    \centering 
    \includegraphics[width=0.85\linewidth]{src/figs/4_learning_rotation_crop.pdf}
    \caption{Illustration of the calculation of rotation angle $\Delta \theta$: The object rotates alone Axis $k$ and here we visualize the rotation angle $\Delta \theta$ in the Normal Plane.}
    \label{fig:rot}
    \vspace{-0.1in}
\end{figure}
\subsubsection{Reset Strategy} We design several reset strategies to reduce unnecessary exploration and speed up the learning process. First, we reset the episode when the object deviates too much from its initial position~(i.e., the center of the palm). Moreover, we reset the episode when the major axis of the object deviates too much from the rotation axis, this reduces the exploration of an undesired rotation direction.

\subsection{Domain Randomization}
We use a wide variety of domain randomization~\cite{domain_randomization} to improve the Sim2real transfer. 


\subsubsection{Physics randomization} We randomize the object's initial position, mass, shape, and friction to ensure that the learned policy can deal with different kinds of objects. 

Moreover, we randomize the gain of the PD controller to model the uncertainty of the PD controller in real. Besides, we consider randomizing each tactile sensor. For each activated contact sensor that outputs 1, with probability $p$ we flip its output to 0. We also model the signal delay of the contact sensor by an exponential delay used in~\cite{dextreme}. 


\subsubsection{Non-physics randomization} 
We use a set of non-physics randomization to further improve the robustness of the trained policy. We inject white noises into the observation of the policy, and its outputted action to ensure that it is robust to small perturbations.

\subsection{Training Procedure}
We use the proximal policy optimization~(PPO)~\cite{ppo} algorithm to train our control policy and multilayer perceptron~(MLP) for both of the policy and value networks. We use the advantage clip threshold $\epsilon=0.2$ and the KL threshold of 0.02. We use ELU~\cite{relu} as the activation function in these networks. The policy network outputs a Gaussian distribution with a learnable state-independent standard deviation. Like~\cite{dextreme}, in order to reduce the training difficulty, we also use asymmetric observation for the policy and value network. Concretely, for the value network, we add privileged information such as the contact force over each link, the object's ground-truth pose, and physical parameters to its input. This privileged information is not accessible by the policy network. For the policy network, we only stack the current state with 3 historical states as the input. 

For the IsaacGym simulation, we set $dt=0.01667s$ with 2 simulation substeps. We use 8192 parallel environments. The action~(control target) outputted by the policy network is executed by 6 steps, corresponding to a 10Hz control frequency in real.