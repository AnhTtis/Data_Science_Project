

\section{Introduction}
Imagine we are washing the used pan in the kitchen after dinner. Suddenly, the power is cut off unexpectedly, and all the lights go out. What would we do? Most of us may stop the work, put down the pan in the sink, and then probably find our phone in the pocket to light up the way. Simple as it may seem, this sequence of actions actually requires precise execution of in-hand dexterous manipulation in the dark, where we receive no vision input for guidance. Even in normal situations with lights on, the manipulation of objects in hand often comes with heavy occlusions. Without relying on vision, we humans are still very good at feeling and manipulating objects by hand, which is made possible by the tactile~(touch) information coming from our skin. Previous studies in biology also confirm the vital importance of touch information for dexterous manipulation~\cite{human_tactile}. Can we enable robots with such dexterity with touch sensing?


Indeed, tactile sensing has been a long-standing topic in robotics. With different designs of tactile sensors, robots are able to manipulate objects more precisely using contact information~\cite{tactile_insertion, kolamuri2021improving_grasp, contact_ar} and even complete tasks in a touch-only setup ~\cite{tactile2020review,murali2020learning}. However, it is still very challenging for touch-only approaches to achieve complex and high degree-of-freedom (DOF) in-hand manipulation. While most current literature focuses on modeling precise and fine-grained contact using increasingly high-quality sensors, it introduces two challenges to in-hand manipulation: (i) Most approaches are only able to attach the expensive sensors to the finger-tips of the gripper or hands instead of covering the whole manipulator, limiting the range of tasks to perform; (ii) It often requires a large number of training samples for complex tasks, but it is hard to leverage a simulator given the Sim2Real gap is usually very large for a delicate sensor. 

In this paper, we present \textbf{Touch Dexterity}, a new system design and learning pipeline for in-hand rotation using only touching. Instead of using a few sensors on finger-tips that give high-quality patterns~\cite{yuan2017gelsight,lambeta2020digit,padmanabha2020omnitact}, we propose the alternative: Use a lot of low-cost binary force sensors (touch or no touch) attached over one side of the hand (fingertips, links, and palm) as shown in Fig.~\ref{fig:teaser} (left). Specifically, we attach the Force-Sensing Resistor (FSR) sensors, which cost around $\$12$ each on Amazon\footnote{\url{https://www.amazon.com/s?k=fsr+sensor}}, to the Allegro robot hand. Our insight is that, while one single binary force sensor cannot do much, the combination of 16 of them has a strong representation power ($2^{16}$ types of states in maximum), which might allow the robot hand to ``feel'' the object state without seeing. Importantly, the Sim2Real gap by using such a binary sensor is minimized to the extreme, which allows large-scale sample collection in simulation for training. 

With this system setup, we focus on the task of rotating an ``unseen'' object around the $x$, $y$, and $z$-axis using the multi-finger hand as shown in Fig.~\ref{fig:teaser} (right). Here ``unseen'' not only indicates there is no vision, but also means the object is not presented during training time. While this task is a simplified version of the in-hand re-orientation task, it is still very challenging as all the fingers are moving with a relatively large motion to rotate the object and prevent it from falling off the palm at the same time. We believe the same pipeline can be directly extended to more complex tasks in the future. We train our policy on multiple objects in parallel in the IsaacGym simulator~\cite{isaacgym} using Reinforcement Learning~(RL), and the learned policy can be directly deployed on the real robot manipulating diverse unseen objects. The key to achieving such generalization across objects and to the real robot is our touch sensors. Our RL policy takes both the binary touch sensing information and the robot's internal state as input and predicts the action in each time step for closed-loop control. With a large coverage over the object using the touch sensors, our hypothesis is that the policy implicitly learns to understand the 3D structure and pose of the object and perform rotation accordingly. 

In our experiments, we test the real-world system with 10 diverse objects. Our method shows surprising robustness in rotating unseen objects using only touch sensing. For example, we can rotate the rubber duck for two cycles without falling, even if it is never presented in training (last row in Fig.~\ref{fig:teaser}). We perform extensive ablations on our sensor to validate our design, including disabling all the touch sensors, disabling part of them, and using continuous signals instead of binary signals. 
