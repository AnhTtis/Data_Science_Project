\begin{figure}[t]
    \centering 
    \includegraphics[width=\linewidth]{src/figs/5_exp_object_dataset.pdf}
    \caption{The object sets used in our experiments. The full object set in the real world can be found in the supplementary material.}
    \label{fig:object}
    \vspace{-0.1in}
\end{figure}

\section{Experiments}
In this part, we compare our Touch Dexterity system to several baselines in both the simulation and the real. Specifically, we are interested in the following questions:
\begin{enumerate}
    \item \textit{How much benefit does tactile information offer compared to the baseline in training?} 
    \item \textit{Using simulation as an ideal setup, does the usage of tactile information lead to better robustness and generalization?} 
    \item \textit{How well does our tactile manipulation system perform and generalize compared with other methods in the real?}
    \item \textit{How well does tactile perception in simulation align with that in real? How does it improve performance in real?}
    % \todo{think of it. Come back to it and reorganize.}
\end{enumerate}
We answer these questions through an extensive case study on the $z$-axis rotation. Then, we demonstrate that our system can also learn the rotation skill along all the other axes.
%this part may be moved to the III.A Realworld systrm
\subsection{Experiment Setup}
\subsubsection{Object Dataset}
For the simulation experiments, we train and evaluate our policy on a set of artificial objects of common geometries, such as cuboids, cylinders, and balls. Some examples of these objects are shown in Figure~\ref{fig:object}. Despite their simplicity, their diverse geometry can be used to approximate a large set of common daily objects. For the real experiments, we bring in some unseen real-world objects like a rubber duck, lego box for evaluation as shown in Figure~\ref{fig:object}. 
\subsubsection{Evaluation Metric}
To evaluate the performance of a trained policy, we introduce the following metric as suggested by~\cite{qi2022hand}.
\begin{enumerate}
    \item \textbf{Cumulative Rotation Reward~(CRR).} We calculated the cumulative rotation reward to evaluate the rotation capability of a policy in the simulation. This metric is only used in the simulation. 
    \item \textbf{Cumulative Rotation Angle~(CRA).} We count the cumulative rotation angle~(by rounds) to evaluate the rotation capability of a policy in the real. This metric is counted by a human.
    \item \textbf{Time-to-Fall~(TTF/Duration).} We measure the time~(by seconds) of an object staying in the palm before falling down the hand. This metric can be used both in the simulation and in real. 
\end{enumerate}
\begin{figure*}[ht]
\subfigure{\includegraphics[width=\textwidth]{src/figs/5_exp_sensor_abl.pdf}}
\subfigure{\includegraphics[width=\textwidth]{src/figs/5_exp_sensitivity_abl.pdf}}
\vspace{-0.2in}
\caption{Top: Policy training curve with and without sensors. Bottom: Policy training curve with sensors of different sensitivities. The results are averaged on 3 seeds. The shaded area shows the standard deviation.}
\label{fig:curve}
\vspace{-0.4cm}
\end{figure*}


\subsection{Baselines}
In the experiments, we mainly compare our methods with the following baselines. 
\begin{enumerate}
    \item \textbf{No-Sensor.} We train a PPO policy to control the hand with no tactile information available. The only way to infer the object-hand interaction information is by comparing the current joint position to the desired, target joint position. For example, when a finger~(e.g. thumb) is pressing the top surface of a cuboid, we can observe a difference between these two quantities, indicating the existence of pressing behavior.
    \item \textbf{LS-Sensor.} We set a higher sensor activation threshold $\theta_{th}=0.2N$ and train another PPO policy. In other words, the sensors now have lower sensitivity to the contact, and we call this policy LS-Sensor. Under this setup, the hand is no longer able to sense some slight contacts.
    \item \textbf{DS-Sensor.} This policy is used for ablation purposes. Its only difference from our policy is that it will disable all the tactile input during evaluation. This is used to test to which extent the trained tactile policy uses tactile information.
\end{enumerate}
In the real-world experiments, we also introduce additional two policy baselines: 
\begin{enumerate}
    \item \textbf{Openloop Policy.} We collect several successful object rotation trajectories in the simulation and execute these trajectories on the robot. This is to study whether the considered task is complex enough.
    \item \textbf{CT-Sensor.} We train a policy that uses a continuous-valued sensor input rather than the binarized version. This is to study if using continuous signals will lead to Sim2Real difficulty.
\end{enumerate}
 Note that there also exist some vision-based dexterous manipulation baselines like~\cite{chen2022visual, dextreme}. However, we do not compare our method to theirs as they require a collection of a very large amount of visual simulation data, which takes significantly longer real-world time.




\subsection{Sim: Policy learning with different sensing capabilities.}
In this section, we study whether our tactile policy and the considered baseline policies are able to succeed in the training environments in the simulation. We study both the single and multi objects setup. We use the cuboid as the object in single-object training, which is common in the previous works~\cite{rl-openai, dextreme}. We use object set A for multi-object training. The results are shown in Figure~\ref{fig:curve}. We find that in the single object setup, both the No-Sense and LS-Sensor policies have a lower rotation reward compared with our policy. Interestingly, we find that LS-Sensor can achieve a higher duration~(TTF) compared with No-Sensor and can match that of our full system. This result suggests that tactile sensors of low sensitivity may still be useful to make the motion more secure. For the multi-object training, we find that our tactile policy outperforms the baseline policies by a large margin. The baseline policies fail completely in this case, while our policy is still able to succeed. This result indicates that using tactile information is essential to tame touch-only multi-object rotation. The failure of the LS-Sensor in the multi-object training case suggests that having a high-sensitivity sensor to sense the slightest contact is important.

\subsection{Sim: Is a tactile policy robust and generalizable?}
Though our tactile policy and the baseline policies can succeed in some cases during training, so far it remains unknown whether they are robust and generalizable. We consider a policy robust if it can perform well on the unseen physics parameter setup on the same set of objects. We consider a policy generalizable if it can perform well on an unseen set of objects. 

We test the robustness of the single-object setting. To do this, we sample from a smaller, unseen range of friction and mass parameters, and perform rollout. In this case, the object is more likely to slide in the hand, requiring the hand to manipulate it in a more careful manner. The results are shown in Table~\ref{table:abl1}. We find that there is little performance drop in our full method. However, for No-Sensor and DS-Sensor, we can observe a clear performance drop. We also find that the low-sensitivity policy LS-Sensor also performs well in the unseen physics setup. This suggests that a low-sensitivity tactile sensing ability is sufficient for the robustness of the single object rotation.

The generalization testing result is shown in Table~\ref{table:abl2}. We train the policies on object set A and test them on object set B. Since No-Sensor and LS-Sensor baseline does not work on the multi-object training setup, we only compare our method with DS-Sensor. We find that disabling the sensor input will lead to a significant performance drop both in the seen and unseen object setup. This result suggests that tactile information is indeed important for generalization.
\input{src/table_real.tex}
\input{src/table_sim.tex}

\subsection{Real: Dexterity without Vision}

We have seen that our tactile policy can achieve superior performance in the ideal simulation setup. Now, we transfer the trained tactile policy to the real robot and verify if it still offers the demonstrated benefit. In real-world experiments, we train the baselines on object sets A and B and evaluate these policies on object set C. We evaluate each method using 3 different seeds for each object. The results are shown in Table~\ref{table:real}. %The evaluation of more objects is provided in the supplementary material.

Our method can outperform all the baselines. It can not only perform rotation on the seen, artificial training objects but also generalize to unseen real-world objects like apples and tomatoes. The method with continuous tactile sensor signals also performs better than the other two methods without feedback. We observe that both the no-sensor and the open-loop policy can at most rotate the object for 180 degrees on the evaluated objects, after which they will get stuck or push the object off the palm, resulting in a failure. In addition, by studying the behavior of our policy and baselines, we find that our policy can adjust the finger motion immediately when objects get to positions that are easy to get stuck or fall. In contrast, the baselines without sensors do not have such kind of adaptive behavior. This result suggests that it is crucial to have a dose of tactile feedback in the considered in-hand object manipulation setup. By comparing the methods with continuous contact signals and binarized contact signals, we find that the latter has better performance. Even though the policy with continuous signals can perform well in some objects, it has poor generalizability and huge variance between different objects. This may be due to the huge gap in force measurement between simulation and the real world. 

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{src/figs/5_exp_last_dual_plot.pdf}
 \caption{Visualization of contact signals in 400 steps in the simulation and real-world experiments on a cuboid. We also show some typical frames during the rotation process. We can see that the contact signals in the simulation and the real world in general align. This accounts for successful Sim2Real transfer.}
\label{fig:response}
\vspace{-0.65cm}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{src/figs/5_exp_human_sensor_curve_ai.pdf}
 \caption{With the learned rotation primitives around $x$, $y$, and $z$ axis, we can perform human-robot shared control to reorient an object. In this example, a human operator uses a keyboard to rotate a cuboid around $x, y, z, y$ axes consecutively. We also visualize the contact signal throughout this 600-step process~(60 seconds). }
\label{fig:human}
\vspace{-0.4cm}
\end{figure*}


\subsection{Qualitative Analysis: Sensor Response}
To understand why Sim2Real can be successful, we conduct a case study on cuboid rotation to analyze the sensor response. We visualize two 40 seconds test trajectories recorded in the simulation and real in Figure~\ref{fig:response} during the test. It is worthwhile mentioning that different runs in real will produce different patterns, and we put more cases in the appendix. We find that the contact signals in the simulation are slightly denser (along the temporal x-axis) and richer (along the sensor y-axis) compared with that of the real. Some sensors are also more likely to be activated~(e.g., sensors 1 and 10) in the simulation, but the overall patterns of the simulation and the real are similar. This can explain why our Sim2Real transfer is successful. Moreover, when looking at local windows used by our policy~(0.4s) in simulation, we observe that there are various, diverse sensor activation patterns. We hypothesize that learning from such a diverse distribution could also help the policy to transfer to the sensor observation in the real world. 

\subsection{Ablation Study I: Importance Analysis of Sensors}
Then, we perform ablation studies of the system on the real robot to see which sensors are more important for a successful rotation. We divide the sensors into two groups: Fingertip and Palm. We disable these two groups of sensors and train two policies (No-Fingertip and No-Palm). Then we compare them to our full policy and DS-Sensor, see Table~\ref{table:abl_real}. We find that neither of the two considered policies can compare to our full policy. They achieve a similar performance as DS-Sensor, which suggests that both groups of sensors are essential for the success of in-hand object rotation.
\begin{figure}[ht]
\subfigure{\includegraphics[width=1.0\linewidth]{src/figs/5_exp_recon-crop.pdf}}

\caption{Qualitative mesh reconstruction results in simulation. When the touch information does not present, we can not infer the shape of the rotated object accurately. In contrast, our method is able to reconstruct the groundtruth object by a 20-second rotation.}
\label{fig:recon}
\vspace{-0.3cm}
\end{figure}

\input{src/table_real_ablation.tex}


\subsection{Ablation Study II: A Shape Understanding Perspective}
So far, we have seen that the tactile information is essential for successful object rotation. In this part, our goal is to understand its success from a shape understanding perspective. We study whether our tactile information can reveal the shape information of the object, which may be helpful for learning robust and adaptive rotation behavior across different objects. Specifically, we would like to see if it is possible to predict the shape of the object using the rollout of a rotation policy. For simplicity, we focus on the $z$-axis rotation of column-shaped objects. We first train a $z$-axis rotation policy on 125 different irregular, column-shaped objects. Then, we use this policy to collect 55000 policy rollouts of rotating these objects, and each of these rollouts lasts 200 control steps~(20 seconds). Next, we split these collected rollouts into a training dataset and a test dataset. The objects in the test dataset do not present in the training dataset. We train a temporal-CNN model to predict the shape of the object using the full rollout trajectory as input, and then we use the trained model to reconstruct the shape of objects in the test dataset. We compare our model to another ablated model, which discards all the tactile observation in the rollout during prediction~(by setting them to 0). The shape reconstruction mean squared error~(MSE) of our model is 0.22, while that of the ablated model is 0.45. This suggest that using tactile infomation can indeed help shape understanding. Moreover, we provide visualization of predicted object shapes are in Figure~\ref{fig:recon}. With the tactile sensors, our model can reconstruct object shape much better than the ablated model. The shape understanding results suggest that the binarized tactile information is indeed important for the robot to percept the object and interact with it in a meaningful way.



\subsection{Rotation Around Other Axes}
Besides the rotation around the $z$ axis, we also test whether our system is able to perform rotation around other axes. Here, we study the rotation around the $x$ and $y$ axes. To do so, we train our policy on the object set A and B as in the previous experiments. The results are shown in Table~\ref{table:summ_all}. We find that our system is still able to rotate most of the objects successfully, though it may have difficulty rotation some particular objects which results in lower CRAs. We observe that rotation around $x$ and $y$ axes involves many critical contacts between the object and the side of finger links. This may explain why failures can occur since the layout of our current sensor array does not support this feature. We hypothesis that a denser contact sensor array over each finger link can remedy this problem. 

The rotation around $x$, $y$, and $z$ axis provides a useful set of primitives. This enables human to use high-level commands to control the rotation behavior, as shown in Figure~\ref{fig:human}. In this example, A human operator presses the keyboard to send different rotation commands (i.e. around $x$, $y$, or $z$). The robot hand is then able to execute the desired rotation, reorienting the object to different poses. 

\input{src/table_all}
