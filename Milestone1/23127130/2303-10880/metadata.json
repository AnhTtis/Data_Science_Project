{
    "arxiv_id": "2303.10880",
    "paper_title": "Rotating without Seeing: Towards In-hand Dexterity through Touch",
    "authors": [
        "Zhao-Heng Yin",
        "Binghao Huang",
        "Yuzhe Qin",
        "Qifeng Chen",
        "Xiaolong Wang"
    ],
    "submission_date": "2023-03-20",
    "revised_dates": [
        "2023-03-23"
    ],
    "latest_version": 3,
    "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
    ],
    "abstract": "Tactile information plays a critical role in human dexterity. It reveals useful contact information that may not be inferred directly from vision. In fact, humans can even perform in-hand dexterous manipulation without using vision. Can we enable the same ability for the multi-finger robot hand? In this paper, we present Touch Dexterity, a new system that can perform in-hand object rotation using only touching without seeing the object. Instead of relying on precise tactile sensing in a small region, we introduce a new system design using dense binary force sensors (touch or no touch) overlaying one side of the whole robot hand (palm, finger links, fingertips). Such a design is low-cost, giving a larger coverage of the object, and minimizing the Sim2Real gap at the same time. We train an in-hand rotation policy using Reinforcement Learning on diverse objects in simulation. Relying on touch-only sensing, we can directly deploy the policy in a real robot hand and rotate novel objects that are not presented in training. Extensive ablations are performed on how tactile information help in-hand manipulation.Our project is available at https://touchdexterity.github.io.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.10880v1",
        "http://arxiv.org/pdf/2303.10880v2",
        "http://arxiv.org/pdf/2303.10880v3"
    ],
    "publication_venue": "Project page: https://touchdexterity.github.io"
}