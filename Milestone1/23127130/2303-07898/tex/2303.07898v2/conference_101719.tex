% Template for ICIP-2022 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx,amssymb,amsfonts}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{algorithm} 
\usepackage{algpseudocode} 

\usepackage{pifont}
\newcommand{\cmark}{\textcolor{blue}{\ding{51}}}%
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}%

%\hyphenpenalty=10000
%\exhyphenpenalty=10000

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}



% Title.
% ------
\title{AutoEnsemble: Automated Ensemble Search Framework for Semantic Segmentation Using Image Labels}
%
% Single address.
% ---------------
%\name{Erik Ostrowski}
%\address{TU Wien, Vienna, Austria\\
%	erik.ostrowski@tuwien.ac.at}
 
%\name{Muhammad Shafique}
%\address{NYU Abu Dhabi\\
%muhammad.shafique@nyu.edu}
%
% For example:
% ------------
%\address{Vienna University of Technology, Austria\\
%	erik.ostrowski@tuwien.ac.at}
%
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {Erik Ostrowski}
%	{TU Wien, Vienna, Austria\\
%	erik.ostrowski@tuwien.ac.at}
%{Muhammad Shafique}
%{New York University (NYU), Abu Dhabi, UAE\\
%muhammad.shafique@nyu.edu}


\twoauthors
{Erik Ostrowski} 
{Institute of Computer Engineering,\\
Technische Universit{\"a}t Wien (TU Wien),\\ Austria\\
erik.ostrowski@tuwien.ac.at }
{Muhammad Shafique}
{Division of Engineering,\\
New York University Abu Dhabi,\\ United Arab Emirates\\
muhammad.shafique@nyu.edu
  }






%\twoauthors{
%\name{Erik Ostrowski} 
%\address{erik.ostrowski@tuwien.ac.at\\
%Institute of Computer Engineering, Technische Universit{\"a}t Wien (TU Wien), Austria\\ }
%\name{Muhammad Shafique}
%\address{muhammad.shafique@nyu.edu\\
% Division of Engineering, New York University Abu Dhabi, United Arab Emirates\\ }
%}

\begin{document}

%\ninept
%
\maketitle
%
\begin{abstract}
A key bottleneck of employing state-of-the-art semantic segmentation networks in the real world is the availability of training labels.
Standard semantic segmentation networks require massive pixel-wise annotated labels to reach state-of-the-art prediction quality.
Hence, several works focus on semantic segmentation networks trained with only image-level annotations.
However, when scrutinizing the state-of-the-art results in more detail, we notice that although they are very close to each other on average prediction quality, different approaches perform better in different classes while providing low quality in others.
To address this problem, we propose a novel framework, AutoEnsemble, which employs an ensemble of the "pseudo-labels" for a given set of different segmentation techniques on a class-wise level.
Pseudo-labels are the pixel-wise predictions of the image-level semantic segmentation frameworks used to train the final segmentation model.
Our pseudo-labels seamlessly combine the strong points of multiple segmentation techniques approaches to reach superior prediction quality.
We reach up to 2.4\% improvement over AutoEnsemble's components.
An exhaustive analysis was performed to demonstrate AutoEnsemble's effectiveness over state-of-the-art frameworks for image-level semantic segmentation.
\end{abstract}


\begin{keywords}
Semantic Segmentaion, Weakly Supervised, Ensemble, Deep Learning, Class Activation Maps
\end{keywords}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1. SECTION INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

\begin{figure}[t]
\centering
\includegraphics[width=80mm,scale=1.0]{Figures/ICIP_examples.pdf}
\caption{Pseudo labels from (a) DRS, (b) PMM, (c) PuzzleCAM, (d) CLIMS, (e) AutoEnsemble (Ours), (f) Ground truth\label{example}}

\end{figure}

Generating high-quality semantic segmentation predictions using only models trained on image-level annotated datasets would enable a new level of applicability.
The progress of fully supervised semantic segmentation networks has already helped provide many useful tools and applications. For example, in autonomous and self-driving vehicles~\cite{AD,feng2020deep}, remote sensing~\cite{diakogiannis2020resunet, kemker2018algorithms}, facial recognition~\cite{meenpal2019facial}, agriculture~\cite{milioto2018real, barth2018data}, and in the medical field \cite{rehman2020deep,zhao2017tracking}, etc.
The downside of those fully supervised semantic segmentation networks (FSSS) is that they require large amounts of pixel-wise annotated images. 
Generating such a training set is very time-consuming and tedious work. 
For instance, one frame of the Cityscapes dataset, which contains thousands of pixel-wise frame annotations of street scenes from cities, requires more than an hour of manual user-driven annotation~\cite{CT}.
Furthermore, medical imaging and molecular biology fields require the knowledge of highly qualified and experienced individuals capable of interpreting and annotating the images. 


Therefore, to reduce the time and resources required for generating pixel-wise masks, a wide range of research works focus on developing approaches that focus on weaker kinds of supervision.
In this work, we will focus on weak supervision in the form of image-level labels.
Image-level labels give the least amount of supervision for semantic segmentation but are the easiest to acquire.

Several works already focus on image-level semantic segmentation techniques, and they consistently reach better and better high scores.
Most works are based on Class Activation Maps (CAMs)~\cite{CAM}.
CAMs localize the object by training a DNN model with classification loss and then reusing the learned weights to highlight the image areas responsible for its classification decision.
Most image-level segmentation approaches aim to improve the CAM baseline by adding additional regularizations to the classification loss or refining the CAM mask afterward.
As more and more methods emerge for improving CAM quality, state-of-the-art is usually compiled of combinations of regularizations and after-the-fact refinement.
However, when analyzing different image-level segmentation tehniques on a class-by-class basis, we observed that the differences between those approaches vary significantly on specific classes, although those methods generate predictions that reach comparable scores on average. 

Therefore, we are proposing our AutoEnsemble framework.
 In our framework, we combine the pseudo-labels of multiple image-level segmentation tehniques based on the respective class scores to generate a superset of pseudo-labels, combining the upsides of multiple different approaches.
Fig.~\ref{example} visualizes the gains possible of AutoEnsemble compared to its best component.
We perform extensive experiments on the PASCAL VOC2012 dataset~\cite{VOC} to prove the effectiveness of the proposed framework in various experimental settings and compare them with a wide range of state-of-the-art techniques to illustrate the benefits of our approach.
The \textbf{key contribution} of this work are:
\begin{enumerate}
    \item Our novel AutoEnsemble framework improves the prediction quality of the segmentation mask by combining state-of-the-art pseudo-labels and class-by-class base.
   \item  Our AutoEnsemble is not limited by the number or approach of any conventional image-level segmentation framework to combine their pseudo-labels. 
   Since the AutoEnsemble is only used for generating pseudo-labels, it will not add more computations for inference predictions.
   \item  We have presented detailed ablation studies and analysis of the results comparing AutoEnsemble to state-of-the-art methods on the VOC2012 dataset to evaluate our method's efficacy and the improvements achieved using our framework. 
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2. SECTION RELATED WORK
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}

This section discusses the current state-of-the-art image-level semantic segmentation.

PCA~\cite{AFF} trains a second network to learn pixel similarities, which generates a transition matrix combined with the CAM iteratively to refine its activation coverage.
PuzzleCAM~\cite{PUZZLE} introduces a regularization loss by sub-dividing the input image into multiple parts, forcing the network to predict image segments that contain the non-discriminative parts of an object.
CLIMS~\cite{CLIMS} trained the network by matching text labels to the correct image.
Hence, the network tries to maximize and minimize the distance between correct and wrong pairs, respectively, instead of just giving a binary classification result.
PMM~\cite{PMM} used Coefficient of Variation Smoothing to smooth the  CAMs, Proportional Pseudo-mask Generation that introduces a new metric, which highlights the importance of each class on each location, in contrast to the scores trained from the binary classifier.
Furthermore, they employed Pretended Under-fitting, which improves training with noisy labels, and Cyclic Pseudo-mask, which iteratively trains the final semantic segmentation network with its predictions.
DRS~\cite{DRS} aims to improve the image's activation area to less discriminative areas.
\cite{DRS} et al. achieve this by suppressing the attention on discriminative regions, thus guiding the attention to adjacent non-discriminative regions to generate a complete attention map of the target object.





\begin{table}[ht]
\caption{Highest score per VOC2012 class on each component of AutoEnsemble (PE) and itself.\label{check}}
\centering 
{
\begin{tabular}{p{1.2cm} p{1.2cm} p{1.2cm} p{1.2cm} p{1.2cm}}
\hline
Class&PMM&DRS&CLIMS&Puzzle\\ \hline
\begin{tabular}{p{1.2cm} p{1.2cm} p{1.2cm} p{1.2cm} p{1.2cm}} 
Bus    & \cmark  &   \xmark &   \xmark &   \xmark  \\
Car    & \cmark  &   \xmark &   \xmark &   \xmark  \\
Bottle & \xmark  &   \cmark &   \xmark &   \xmark  \\
Chair  & \xmark  &   \cmark &   \xmark &   \xmark  \\
Train  & \xmark  &   \cmark &   \xmark &   \xmark  \\
Bike   & \xmark  &   \xmark &   \cmark &   \xmark  \\
Boat   & \xmark  &   \xmark &   \cmark &   \xmark  \\
Table  & \xmark  &   \xmark &   \cmark &   \xmark  \\
Motor  & \xmark  &   \xmark &   \cmark &   \xmark  \\
Person & \xmark  &   \xmark &   \cmark &   \xmark  \\
Sofa   & \xmark  &   \xmark &   \cmark &   \xmark  \\
TV     & \xmark  &   \xmark &   \cmark &   \xmark  \\
Aero   & \xmark  &   \xmark &   \xmark &   \cmark   \\
Bird   & \xmark  &   \xmark &   \xmark &   \cmark   \\
Cat    & \xmark  &   \xmark &   \xmark &   \cmark   \\
Cow    & \xmark  &   \xmark &   \xmark &   \cmark   \\
Dog    & \xmark  &   \xmark &   \xmark &   \cmark  \\
Horse  & \xmark  &   \xmark &   \xmark &   \cmark   \\
Plant  & \xmark  &   \xmark &   \xmark &   \cmark   \\
Sheep  & \xmark  &   \xmark &   \xmark &   \cmark   \end{tabular} 
\end{tabular}}
\end{table}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3. SECTION METHOD
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Our Framework}

\begin{figure*}[ht]
\centering
\includegraphics[width=150mm,scale=0.50]{Figures/ICIP_framework.pdf} %width=80mm,scale=0.80 width=\linewidth
\caption{Overview of the AutoEnsemble framework. 
The first stage is the collection of Image-level semantic segmentation;
In the second stage, we can use a number of refinement methods to improve the mask quality; 
In the third stage, we combine the refinement masks on a class-wise basis to generate the pseudo-labels that reach the best prediction for each class;
In the final stage, we are training an FSSS with the pseudo labels\label{fig2}}\end{figure*}



Fig.~\ref{fig2} presents an overview of the AutoEnsemble framework.
We start with collecting the pseudo labels of our candidate methods.
In the next step, we can employ several refinement methods to improve the pseudo-label quality beforehand.
In our case, we used PCA~\cite{AFF} and a dense Conditional Random Field (dCRF)~\cite{CRF} for the candidates if the provided pseudo labels did not already undergo refinement methods.
Then we can combine the pseudo-labels on a class-wise basis, where we only copy the predictions of particular classes of the candidate labels to our ensemble if the candidate has a high score in that class.
Finally, we use the generated pseudo labels to train an FSSS network.
Our proposed version uses the four state-of-the-art methods introduced in the previous section, and for all of them except CLIMS, we also refine their baseline with PCA.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
PCA uses a random walk in combination with pixel affinities.
Furthermore, as common practice, we use dCRF to the PCA predictions to improve their quality.
The refinement of the pseudo labels is not limited to PCA, and any combination of additional refinement methods can be employed within our AutoEnsemble framework.

In the next step, we will evaluate the different candidate pseudo label sets on a class-wise basis and determine which candidate is used for which class for the ensemble.
The combination is done by transferring every pixel classified as class $x$ by the candidate network $n$, which has the high score on $x$, to our ensemble pseudo-labels, for every class. Furthermore, we tested a naive version, in which we ranked every class by its number of instances in the training set and then used the complete CAM of candidate $n$ from the whole image if $n$ has the high score on the highest ranked class $x$ present on the image.
The naive AutoEnsemble performed worse than our final version but still better than its components.

We excluded the \textit{background} class from our ensemble since the background is the inverse of all classes combined.
Note that we can perform this class selection method since we already assign the correct class labels to each prediction instead of using a classification network for the assignment.
Therefore, training a fully supervised semantic segmentation network with those pseudo-labels is necessary.
Nevertheless, the FSSS training guarantees that collecting multiple pseudo-label sets is a one-time effort per dataset.
Fig.~\ref{fig2} illustrates an overview of the process.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 4. SECTION EXPERIMENTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}

First, we will discuss our experimental setup.
We completed the experiments on a CentOS 7.9 Operating System executing on an Intel Core i7-8700 CPU with 16GB RAM and 2 Nvidia GeForce GTX 1080 Ti GPUs.
The CLIMS pseudo-labels were used as provided by the official Github, and we performed PCA with a ResNet50 backbone and dCRF on the pseudo-labels provided on the DRS and PMM Github.
All DeepLabV3+ results were generated using a ResNet50 backbone.
The mean Intersection-over-Union (mIoU) ratio is the evaluation metric for all experiments.
We used the PASCAL VOC2012 semantic segmentation benchmark for evaluating our framework. 
It comprises 21 classes, including a background class, and most images include multiple objects.
Following the commonly adopted experimentation protocol for semantic segmentation, we use the $10,528$ augmented images, including the image-level labels, for training.
Our model is evaluated on the validation set with $1,464$ images and the test set of $1,456$ images to ensure a constant comparison with the state-of-the-art.

\begin{table}[ht]
%\resizebox{\columnwidth}{!}{
\centering 
{%\fontsize{9pt}{9pt}\selectfont
\caption{Comparison of AutoEnsemble with state-of-the-art techniques on the VOC2012 val and test datasets. All methods were trained with DeepLabV3+ with a ResNet50 backbone for comparability. Adding more or different methods to the ensemble is possible, as those work orthogonal to the AutoEnsemble.\label{tab2}}
\begin{tabular}{lccc}
\hline
Method              & Val  & Test \\ \hline
PuzzleCAM             & 62.4 & 62.9 \\
PMM            & 64.0 & 64.1 \\
DRS         & 64.5 &  64.5 \\
CLIMS             & 65.0 &  65.4\\
AutoEnsemble-2 (Ours)         & 66.6 & 67.1 \\ % 64.72
AutoEnsemble (Ours)         & \textbf{67.4} & \textbf{67.8} \\ \hline % 65.03
\end{tabular}}

\end{table}



\begin{table*}[ht]
%\resizebox{\textwidth}{!}{
\centering
\caption{Semantic segmentation performance on the first 11 classes of the VOC2012 training dataset for the final pseudo-labels. \label{tab3}}
{\begin{tabular}{lcccccccccccccccccccccc}

\hline
Method                              & bkg         & aero        & bike        & bird        & boat        & bottle      & bus         & car         & cat         & chair       & cow \\ \hline
%PCA~\cite{AFF}                      & 88.2        & 68.2        & 30.6        & 81.1        & 49.6        & 61.0        & 77.8        & 66.1        & 75.1        & 29.0        & 66.0\\
%MCOF~\cite{MCOF}                   & 87.0        & 78.4        & 29.4        & 68.0        & 44.0        & 67.3        & 80.3        & 74.1        & 82.2        & 21.1        & 70.7\\
%Zeng et al.~\cite{ZENG}            & 90.0        & 77.4        & 37.5        & 80.7        & 61.6        & 67.9        & 81.8        & 69.0        & 83.7        & 13.6        & 79.4\\
%FickleNet~\cite{FICKLE}             & 89.5        & 76.6        & 32.6        & 74.6        & 51.5        & 71.1        & 83.4        & 74.4        & 83.6        & 24.1        & 73.4\\
%Sub-Categories~\cite{SUB}           & 88.8        & 51.6        & 30.3        & 82.9        & 53.0        & 75.8        &\textbf{88.6}& 74.8        & 86.6        &\textbf{32.4}& 79.9\\ \hline
PuzzleCAM                           & 88.6        &\underline{79.2}& 43.7        &\underline{89.0}& 61.8        & 72.1        & 83.3        & 76.0        &\underline{92.0}& 29.5        & \underline{86.0}\\
CLIMS                               & 89.8        & 71.2        &\underline{45.4}& 81.7        &\underline{70.2}& 67.6        & 84.0        & 75.7        & 90.0        & 20.3        & 84.2\\
PMM                                 & 89.5        & 76.8        & 43.9        & 88.1        & 65.8        & 76.0        &\underline{84.2}&\underline{78.0}& 91.2        & 30.6        & 84.3 \\
DRS                                 &\underline{90.1}& 78.9        & 45.3        & 85.8        & 68.4        &\underline{80.8}& 83.8        & 77.4        & 90.6        &\underline{31.5}& 84.0\\ \hline
AutoEnsemble-2 (Ours)             &90.5&\textbf{79.6}&\textbf{45.4}&\textbf{89.0}& 70.0        & 72.1        & 84.0        & 76.1        &\textbf{92.2}& 30.9        & \textbf{86.1}\\
AutoEnsemble (Ours)             &\textbf{91.0}&\textbf{79.6}&\textbf{45.4}&\textbf{89.0}& 69.7        &\textbf{81.2}&\textbf{84.2}&\textbf{78.0}&\textbf{92.2}&\textbf{31.6}& \textbf{86.1} \\ \hline
\end{tabular}}%}


\end{table*}

\begin{table*}[ht]
\centering
%\resizebox{\textwidth}{!}{

\caption{Semantic segmentation performance on the remaining 10 classes of the VOC2012 training dataset for the final pseudo-labels. \label{tab4}}
{\begin{tabular}{lcccccccccccccccccccccc}

\hline
Method                       & table          & dog           & horse       & motor        & person      & plant       & sheep       & sofa        & train       & tv          & mIoU\\ \hline
%PCA~\cite{AFF}               & 40.2           & 80.4          & 62.0        & 70.4         &\textbf{73.7}& 42.5        & 70.7        & 42.6        & 68.1        & 51.6        & 61.7\\
%MCOF~\cite{MCOF}            & 28.2           & 73.2          & 71.5        & 67.2         & 53.0        & 47.7        & 74.5        & 32.4        & 71.0        & 45.8        & 60.3\\
%Zeng et al.~\cite{ZENG}     & 23.3           & 78.0          & 75.3        & 71.4         & 68.1        & 35.2        & 78.2        & 32.5        & 75.5& 48.0        & 63.3\\
%FickleNet~\cite{FICKLE}      & 47.4           & 78.2          & 74.0        & 68.8         &\textbf{73.2}& 47.8        & 79.9        & 37.0        & 57.3        &\textbf{64.6}& 64.9\\
%Sub-Categories~\cite{SUB}    & 53.8           & 82.3          & 78.5        & 70.4         & 71.2        & 40.2        & 78.3        & 42.9        & 66.8        & 58.8        & 66.1\\\hline
PuzzleCAM                    & 44.0           & \underline{91.6} &\underline{83.1}& \underline{80.1}& 42.8        &\underline{68.9}&\underline{92.6}& 53.4        & 64.8        & 42.6        & 69.7\\
CLIMS                        &\underline{57.8}   & 86.9          & 80.9        & 80.8         &\underline{72.7}& 48.4        & 90.3        &\underline{56.5}& 68.1        &\underline{58.4}& 70.5 \\
PMM                          & 48.5           & 89.3          & 82.0        & 79.0         & 61.4        & 66.5        & 89.9        & 54.4        & 66.4        & 38.6        & 70.7 \\
DRS                          & 41.2           & 88.7          & 80.0        & 79.8         & 65.4        & 62.6        & 89.9        & 55.0        &\underline{77.0}& 41.3        &\underline{71.3}\\ \hline
AutoEnsemble-2 (Ours)      & \textbf{52.5}           &\textbf{92.0}  &\textbf{86.0}&\textbf{80.9}& 71.8        & 68.5        &\textbf{92.7}& 56.2        & 68.1        & 54.1        & 73.3\\
AutoEnsemble (Ours)      & 51.1           &91.9  &85.9&80.8& 72.3        & 68.5        &\textbf{92.7}&\textbf{56.9}&\textbf{77.1}& 52.8        & \textbf{74.2} \\ \hline
\end{tabular}}%}


\end{table*}




\subsection{Semantic Segmentation Performance on VOC2012}




Next, we compare our ensemble with its components consisting of recent works using image-level supervision Table~\ref{tab2}.
We trained all pseudo-labels with the same DeepLabV3+ model for comparability using a ResNet50 backbone.
We notice that the ensemble outperforms its component by a margin of at least $2\%$, although the individual components do not show this amount of variance between them. AutoEnsemble-2 is the ensemble of just PuzzleCAM and CLIMS, and AutoEnsemble is the ensemble of all four methods.
DRS is the best performing of its component, and the AutoEnsemble reaches a $2\%$ higher mIoU score.



Next, we present a more comprehensive analysis and class-wise mIoU breakdown for all classes in the VOC2012 training dataset.
On the one hand, we see in Table~\ref{tab2} that the difference in the average mIoU score between our four component methods the relatively small, with the lowest scoring PuzzleCAM reaching 69.7\% and the highest scoring DRS at 71.3\%.
On the other hand, the ensemble of all four methods reaches 74.1\%, and the ensemble of just PuzzleCAM and CLIMS 73.6\% achieves a significant gain compared to its components.
Although, we also notice that the gain from adding more image-level segmentation pseudo-labels to the ensemble shrinks over time and needs to be considered when choosing the component for AutoEnsemble.

Let us take a closer look at the performance of the individual components on a class-wise basis.
PMM reaches the best score only in two classes and an average $0.77\%$ improvement in those classes.
Although only reaching the highest average score in three classes, DRS provides an average $6.04\%$ improvement in those three classes.
CLIMS is the best in seven classes and achieves an average improvement of $6.04\%$ as well.
Whereas PuzzleCAM is the lowest average scoring method but reaches high scores in eight classes but improves them only by $2.62\%$ on average.
Therefore, we conclude that CLIMS and PuzzleCAM contribute the most and PMM the least to the ensemble.
Hence, we also evaluated the combination of only CLIMS and PuzzleCAM to see how much improvement we gain while combining the minimum amount of pseudo-label sets. We called this version AutoEnsemble-2.
We notice that most high scores translated to the ensemble, with only minor losses in some classes, most probably due to overlap with other classes.





\section{Conclusion}

In this paper, we have proposed our AutoEnsemble framework that combines the pseudo-labels of several image-level segmentation tehniques on a class-wise basis to leverage the strong points of its different components.
The combined pseudo labels reach at least 2\% higher mIoU scores than
its components.
Most of those gains stem from bigger variances within particular classes, as we observed that different approaches have different strengths and weaknesses.
The AutoEnsemble framework combines any number of pseudo-labels to boost the quality of the pseudo-labels for final training.
We showed that the predictions generated by the model trained with the pseudo labels of AutoEnsemble achieve state-of-the-art performance on the VOC2012 dataset, which shows its effectiveness.
\section*{Acknowledgments}
This work is part of the Moore4Medical project funded by the ECSEL Joint Undertaking under grant number H2020-ECSEL-2019-IA-876190.
\section*{Copyright}
Â© 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.
\bibliographystyle{IEEEbib}
%\bibliography{strings,refs}

\bibliography{our_framework}
%\bibliographystyle{ieeetr}





%\end{thebibliography}
\vspace{12pt}
\color{red}

\end{document}


% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{strings,refs}

\end{document}