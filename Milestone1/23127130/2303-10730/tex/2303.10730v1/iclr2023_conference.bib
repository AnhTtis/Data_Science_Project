@dataset{dataset,
  author = {Bifulco, Carlo and Piening, Brian and Bower, Tucker and Robicsek, Ari and Weerasinghe, Roshanthi and Lee, Soohee and Foster, Nick and Juergens, Nathan and Risley, Josh and Haynes, Katy and Obermeyer, Ziad},
  title = {Identifying high-risk breast cancer using digital pathology images},
  publisher = {Nightingale Open Science},
  year = {2021},
  doi = {10.48815/N5159B},
  url = {https://doi.org/10.48815/N5159B}
}

@article{dataset1,
  author = {Mullainathan, Sendhil and Obermeyer, Ziad},
  title = {Solving medicine's data bottleneck: Nightingale Open Science},
  journal = {Nature Medicine},
  year = {2022},
  month = may,
  day = {01},
  volume = {28},
  number = {5},
  pages = {897-899},
  issn = {1546-170X},
  doi = {10.1038/s41591-022-01804-4},
  url = {https://doi.org/10.1038/s41591-022-01804-4}
}


@article{bc1,
author = {Gastounioti, Aimilia and Desai, Shyam and Ahluwalia, Vinayak and Conant, Emily and Kontos, Despina},
year = {2022},
month = {02},
pages = {},
title = {Artificial intelligence in mammographic phenotyping of breast cancer risk: a narrative review},
volume = {24},
journal = {Breast Cancer Research},
doi = {10.1186/s13058-022-01509-z}
}
@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{densenet,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@article{alexnet,
  title={One weird trick for parallelizing convolutional neural networks},
  author={Krizhevsky, Alex},
  journal={arXiv preprint arXiv:1404.5997},
  year={2014}
}

@inproceedings{unet,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical image computing and computer-assisted intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@article{cgan,
  title={Conditional generative adversarial nets},
  author={Mirza, Mehdi and Osindero, Simon},
  journal={arXiv preprint arXiv:1411.1784},
  year={2014}
}
@inproceedings{googlelenet,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@inproceedings{mobilenet,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4510--4520},
  year={2018}
}

@inproceedings{inception,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}

@inproceedings{retinanet,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2980--2988},
  year={2017}
}

@article{doi:10.1126/scitranslmed.3002564,
author = {Andrew H. Beck  and Ankur R. Sangoi  and Samuel Leung  and Robert J. Marinelli  and Torsten O. Nielsen  and Marc J. van de Vijver  and Robert B. West  and Matt van de Rijn  and Daphne Koller },
title = {Systematic Analysis of Breast Cancer Morphology Uncovers Stromal Features Associated with Survival},
journal = {Science Translational Medicine},
volume = {3},
number = {108},
pages = {108ra113-108ra113},
year = {2011},
doi = {10.1126/scitranslmed.3002564},
URL = {https://www.science.org/doi/abs/10.1126/scitranslmed.3002564},
eprint = {https://www.science.org/doi/pdf/10.1126/scitranslmed.3002564},
abstract = {Automated quantification of thousands of morphologic features in microscopic images of breast cancer allows the construction of a robust prognostic model. How is a camera different from the human eye? Only the eye’s images undergo extensive secondary processing as they are interpreted by the human brain. But what if we could program a computer to do the secondary processing? A pathologist reading a cancer biopsy slide matches his or her brain’s memory of certain cancer-related features (tubules, atypical nuclei, and mitosis) against the tissue. This decades-old scoring system is still standard in most places for prognosis and treatment of cancer, despite its variability and often unreliability. Now, Beck et al. have created an automated pathologist by replacing the human brain with sophisticated image processing software and instructing it to find quantitative aspects of breast cancer tissue that predict prognosis. The software located a set of features that strongly predicted breast cancer outcome in both training and validation samples. With an image analysis protocol they termed C-Path, the authors set their program loose on a set of samples from patients in the Netherlands. From more than 6000 features, the software found a set that were associated with samples from patients who had died sooner. The key aspect of this analysis was that these features were not predefined by a pathologist as being relevant to cancer; instead, the software itself found the cancer-related features among the very large set of measurements of the image. Classifying the tissue as epithelial or stromal, an important part of cancer diagnosis, took a bit of extra work: The authors needed to provide the software with some hand-marked samples so it could learn the difference. The C-Path score yielded information above and beyond that from many other measures of cancer severity including pathology grade, estrogen receptor status, tumor size, and lymph node status. In another, completely independent group of women from Vancouver, the C-Path score was also associated with overall survival. An unexpected finding was that the features that were the best predictors of patient survival were not from the cancer itself but were from the adjacent stromal tissue. Women with worse outcomes tended to have thin cords of epithelial cells infiltrating the stroma, which resulted in high-risk stromal matrix variability scores. These patients also tended to have more inflammatory cells in the stroma (picked up as dark areas by the software). Replacing the human brain with an unbiased image processing system can extract more information from microcopy images and discover new biological aspects of cancer tissue. The morphological interpretation of histologic sections forms the basis of diagnosis and prognostication for cancer. In the diagnosis of carcinomas, pathologists perform a semiquantitative analysis of a small set of morphological features to determine the cancer’s histologic grade. Physicians use histologic grade to inform their assessment of a carcinoma’s aggressiveness and a patient’s prognosis. Nevertheless, the determination of grade in breast cancer examines only a small set of morphological features of breast cancer epithelial cells, which has been largely unchanged since the 1920s. A comprehensive analysis of automatically quantitated morphological features could identify characteristics of prognostic relevance and provide an accurate and reproducible means for assessing prognosis from microscopic image data. We developed the C-Path (Computational Pathologist) system to measure a rich quantitative feature set from the breast cancer epithelium and stroma (6642 features), including both standard morphometric descriptors of image objects and higher-level contextual, relational, and global image features. These measurements were used to construct a prognostic model. We applied the C-Path system to microscopic images from two independent cohorts of breast cancer patients [from the Netherlands Cancer Institute (NKI) cohort, n = 248, and the Vancouver General Hospital (VGH) cohort, n = 328]. The prognostic model score generated by our system was strongly associated with overall survival in both the NKI and the VGH cohorts (both log-rank P ≤ 0.001). This association was independent of clinical, pathological, and molecular factors. Three stromal features were significantly associated with survival, and this association was stronger than the association of survival with epithelial characteristics in the model. These findings implicate stromal morphologic structure as a previously unrecognized prognostic determinant for breast cancer.}}

@article{vgg,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{efficientnet,
  title={Efficientnetv2: Smaller models and faster training},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International Conference on Machine Learning},
  pages={10096--10106},
  year={2021},
  organization={PMLR}
}

@inproceedings{convnext,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11976--11986},
  year={2022}
}

@article{wideresnet,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@inproceedings{shufflenet,
  title={Shufflenet v2: Practical guidelines for efficient cnn architecture design},
  author={Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={116--131},
  year={2018}
}

@article{causalinference1,
  title={Causal machine learning for healthcare and precision medicine},
  author={Sanchez, Pedro and Voisey, Jeremy P and Xia, Tian and Watson, Hannah I and O’Neil, Alison Q and Tsaftaris, Sotirios A},
  journal={Royal Society Open Science},
  volume={9},
  number={8},
  pages={220638},
  year={2022},
  publisher={The Royal Society}
}

@article{causalinference2,
  title={A review of causality for learning algorithms in medical image analysis},
  author={Vlontzos, Athanasios and Rueckert, Daniel and Kainz, Bernhard},
  journal={arXiv preprint arXiv:2206.05498},
  year={2022}
}

@article{causalinference3,
  title={Causal machine learning: A survey and open problems},
  author={Kaddour, Jean and Lynch, Aengus and Liu, Qi and Kusner, Matt J and Silva, Ricardo},
  journal={arXiv preprint arXiv:2206.15475},
  year={2022}
}

@inproceedings{causalinference4,
  title={Estimation of Causal Effects in the Presence of Unobserved Confounding in the Alzheimer’s Continuum},
  author={P{\"o}lsterl, Sebastian and Wachinger, Christian},
  booktitle={International Conference on Information Processing in Medical Imaging},
  pages={45--57},
  year={2021},
  organization={Springer}
}

@article{causalinference5,
  title={Scalable Sensitivity and Uncertainty Analysis for Causal-Effect Estimates of Continuous-Valued Interventions},
  author={Jesson, Andrew and Douglas, Alyson and Manshausen, Peter and Meinshausen, Nicolai and Stier, Philip and Gal, Yarin and Shalit, Uri},
  journal={arXiv preprint arXiv:2204.10022},
  year={2022}
}

@inproceedings{resnext,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1492--1500},
  year={2017}
}

@inproceedings{regnet,
  title={Designing network design spaces},
  author={Radosavovic, Ilija and Kosaraju, Raj Prateek and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10428--10436},
  year={2020}
}

@inproceedings{swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}

@article{maxvit,
  title={Maxvit: Multi-axis vision transformer},
  author={Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and Yang, Feng and Milanfar, Peyman and Bovik, Alan and Li, Yinxiao},
  journal={arXiv preprint arXiv:2204.01697},
  year={2022}
}