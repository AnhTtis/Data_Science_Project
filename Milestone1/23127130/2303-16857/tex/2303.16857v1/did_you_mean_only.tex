\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}

% Remove the "review" option to generate the final version.
% \usepackage[review]{acl}
% for Arxiv version: 
\usepackage[]{acl}
\usepackage{graphicx} 
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{array}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{todonotes}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{float}
\usepackage[capitalise]{cleveref}
\usepackage{siunitx}
\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}
\usepackage{colortbl}
\definecolor{lg}{gray}{0.89}

\newcommand{\longnum}[1]{\num[group-separator={,}]{#1}}
\title{Did You Mean...? Confidence-based Trade-offs in Semantic Parsing}

\newcommand{\perc}[1]{$#1\%$}

\author{Elias Stengel-Eskin \\ Johns Hopkins University \\ {\tt{elias@jhu.edu}} \And Benjamin Van Durme \\
Johns Hopkins University \\
{\tt{vandurme@jhu.edu}}}

\begin{document}
\maketitle
\begin{abstract}
We illustrate how a calibrated model can help balance common trade-offs in task-oriented parsing. 
In a simulated annotator-in-the-loop experiment, we show that well-calibrated confidence scores allow us to balance cost with annotator load, improving accuracy with a small number of interactions. 
We then examine how confidence scores can help optimize the trade-off between usability and safety. 
We show that confidence-based thresholding can substantially reduce the number of incorrect low-confidence programs executed; however, this comes at a cost to usability.  
We propose the DidYouMean system (cf. \cref{fig:example}) which better balances usability and safety.
\end{abstract}

\section{Introduction}
\vspace{-0.5em}
Task-oriented dialogue systems \citep{gupta.s.2018, cheng.j.2020, semanticmachines2020} represent one path towards achieving the long-standing goal of using natural language as an API for controlling real-world systems by transforming user requests into executable programs, i.e. translating natural language to code.
Central to the systems' success is the ability to take rational actions in light of model uncertainty.
When model confidence is low and the system is unlikely to succeed, we might prefer it to defer action or request clarification, while when confidence is high, these actions may unnecessarily annoy a user. 
Using model confidence in this way requires it to be well-correlated with accuracy, i.e. it requires a \emph{calibrated} model.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/fig_example_vertical_short.pdf}
    \vspace{-2.5em}
    \caption{The DidYouMean system. 
    At high confidences, we simply execute the predicted parse. 
    At low confidences, DidYouMean rephrases the query based on the predicted program and asks a user to confirm the paraphrase. 
    The program is executed if the user accepts.}
    \label{fig:example}
    \vspace{-1.5em}
\end{figure}


Recent work has focused on the calibration of semantic parsing models. 
Specifically, \citet{stengel-eskin.e.2022.calibration} benchmarked the calibration characteristics of a variety of semantic parsing models, finding some of them to be well-calibrated, especially on parsing for task-oriented dialogue.
Given the relatively well-calibrated nature of these models, we first examine how they could be used in an annotation interface, with a view to balancing the trade-off between \emph{annotation cost and correctness}. 
We simulate a human-in-the-loop (HITL) experiment where high-confidence tokens are automatically annotated and low-confidence tokens trigger a dialogue with an oracle annotator, who either picks the correct token from a top-K list or manually inserts it.
With a small number of interactions, we substantially boost annotator accuracy. 

A similar trade-off exists between \emph{usability and safety} in task-oriented user interfaces.
We examine how sequence-level model confidence scores can be used to balance this trade-off by reducing the number of incorrect programs executed while also minimizing the number of follow-up user interactions and their cognitive burden. 
We find that thresholding outputs based on model confidence (i.e. rejecting outputs falling below a tuned threshold) reduces the number of incorrect programs executed by  $76\%$ compared to the baseline. 
However, this comes at a cost to usability; about half of the correctly-predicted parses are rejected as well. 
To strike a balance between safety and usability, we introduce the DidYouMean system (cf. \cref{fig:example}), which rephrases the input conditioned on the predicted parse and asks users to confirm the accuracy of the paraphrase.
In a user study, we obtain an $36\%$ improvement in usability over the thresholded system while maintaining a $58\%$ reduction in the number of incorrect programs executed. 

\vspace{-0.5em}
\section{Related Work}
\vspace{-0.5em}
Our experiments in \cref{sec:hitl} involve a predictive model for human-in-the-loop coding: similar models have been integrated into IDEs, e.g. \citet{chen.m.2021}. 
DidYouMean relates to the interactive semantic parsing domain \citep{li.f.2014, chaurasia.s.2017, su.y.2018}, where humans are included in the semantic parsing loop.
In this domain, \citet{yao.z.2019} introduce a confidence-based interactive system in which a parsing agent can ask users for clarification.
Our work follows in the same spirit, but asks the user to confirm an existing parse before execution rather than generating a question for the user to answer.  
DidYouMean also relates broadly to selective prediction, where a model is expected to abstain from making a decision at low confidence \citep{chow.c.1957, elyaniv.r.2010, varshney.n.2022, xin.j.2021, whitehead.s.2022}.
Our system moves beyond the traditional selective prediction setting by including a human-in-the-loop. 
Finally, DidYouMean shares a motivation with \citet{fang.h.2022}, who introduce a method for reliably summarizing programs. 
Their work provides post-hoc action explanations while we focus on resolving misunderstandings \emph{before} execution. 

\vspace{-0.5em}
\section{Methods}
\vspace{-0.5em}
\noindent \textbf{Datasets}
Our data is drawn from the SMCalFlow \citep{semanticmachines2020} task-oriented dialogue dataset, which contains Lisp-like programs (cf. \cref{append:model}).
We follow the same preprocessing as \citet{platanios.a.2020}, and use the SMCalFlow data splits given by  \citet{roy.s.2022}: \longnum{108753} training,  \longnum{12271} validation, and \longnum{13496} testing dialogue turns.

\noindent \textbf{Models}
We use MISO, a well-calibrated model from \citet{stengel-eskin.e.2022.calibration}. 
Rather than predict the SMCalFlow surface form, including syntactic tokens like parentheses, MISO directly predicts the underlying execution graph. 
Note that the execution graph can deterministically be ``de-compiled'' into a Lisp surface form, and vice-versa. 
The fact that MISO predicts an underlying graph rather than a surface string makes it attractive for applications which require confidence scores.
For details on MISO's architecture, see \citet{zhang.s.2019b} and \citet{stengel-eskin.e.2022smcalflow}. 

For token confidence estimation, we use the maximum probability across the output vocabulary at each timestep.
This has been shown to be a relatively robust confidence estimator in classification \citep{hendrycks.d.2016, varshney.n.2022}. 
For sequence-level scores, we follow \citet{stengel-eskin.e.2022.calibration} and take the minimum over token-level confidence scores. 

\vspace{-0.5em}
\section{Human-in-the-Loop Simulation} \label{sec:hitl}
\vspace{-0.5em}
Production datasets like SMCalFlow are constantly evolving as new functionalities are added. 
The expensive and time-consuming nature of annotating data can be mitigated by the use of predictive parsing models which suggest speculative parses for new utterances.
However, the model's output can be incorrect, especially given out-of-distribution inputs. 
We need to ensure that annotators are not introducing errors by overly trusting the model. 

If the model is well-calibrated, we can use the confidence to reduce such errors.
For example, we can alert annotators to low confidence predictions and ask them to intervene.
Using a threshold, we can prioritize time or correctness: a higher threshold would result in more annotator-model interactions, decreasing the speed but increasing the correctness of the programs -- reducing the need for debugging -- while a lower threshold would increase speed but also lower the accuracy. 

Since we do not have access to expert SMCalFlow annotators, we simulate an oracle human-in-the-loop (HITL) annotator who always provides a correct answer by using the gold annotations provided in the dataset.  
Specifically, for a given input, we decode the output tokens of a predicted program $o_0, \ldots o_n$ normally as long as predictions are confident (above a given threshold). 
If at time $t$ the confidence $p(o_t)$ falls the threshold, we attempt to match the decoded prefix $o_0, \ldots, o_{t-1}$ to the gold prefix $g_0, \ldots g_{t-1}$.
If the prefixes do not match, we count the example as incorrect. 
If they do match, we replace $o_t$ with $g_t$, the gold prediction from our oracle annotator, and continue decoding. 
We consider three metrics in this experiment: 
(1) The exact match accuracy of the decoded programs (higher is better).
(2) The percentage of total tokens for which we have to query an annotator (lower is better). 
(3) The percentage of uncertain tokens (below the threshold) for which the gold token $g_t$ is in the top 5 predictions at timestep $t$. 
Here, higher is better, as selecting a token from a candidate list is typically faster than producing the token. 

\noindent \textbf{Results and Analysis} 
\cref{fig:sim_hitl} shows our three metrics as the threshold is increased in increments of $0.1$.
We see that accuracy grows exponentially with a higher threshold, and that the percentage of tokens for which an annotator intervention is required grows at roughly the same rate. 
The exponential growth reflects the distribution of token confidences, with most tokens having high confidence. 
Finally, we see that while at low confidence, most tokens must be manually inserted, the rate at which they are chosen from the top 5 list rapidly increases with the threshold.
Thus, the increased number of annotator interactions required at higher thresholds may be offset by the fact that many of these interactions are a choice from the top-5 list. 

\vspace{-0.5em}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/sim_hitl_results.pdf}
    \vspace{-2.5em}
    \caption{Simulated annotator-in-the-loop results across increasing confidence thresholds.}
    \label{fig:sim_hitl}
    \vspace{-1em}
\end{figure}

\vspace{-0.5em}
\section{User Correction via DidYouMean} \label{sec:user}
\vspace{-0.5em}
\cref{sec:hitl} showed that token-level confidence scores can be used to balance speed and correctness in an annotation interface. 
We see a similar trade-off  between safety and usability in user interfaces using semantic parsing models. 
Here, we define safety as rejecting unsuccessful programs \emph{before executing them}. 
This strict definition is motivated by physical domains: imagine that rather than controlling a digital assistant, a user is guiding a robot via language commands (e.g. \citet{winograd.t.1972, lynch.c.2020, stengel-eskin.e.2021guiding, lynch.c.2022,  nair.s.2022}).
In this setting, actions may have irreversible consequences, so determinine safety before execution is key. 
Safety considerations need to be balanced with usability of the system: an unplugged agent would be very safe but unusable. 
To increase usability, an agent might make follow-up requests to a user, like asking for clarification or confirmation. 
The types of requests the agent makes have varying cognitive load on the user: for example, providing confirmation takes less effort than rephrasing. 

We measure how well we can reject incorrect programs \emph{before} executing them. 
Following past work in selective prediction, we measure success by coverage and risk, as well as F score w.r.t. program correctness.
Coverage is the percentage of inputs for which a program is executed and risk is the percentage of executed programs that were \emph{incorrect}.
Precision is inverse risk, and recall is the percentage of correct programs which were accepted.
We additionally consider F1 and F0.5, which upweights precision (safety) by a factor of 2. 
A low-coverage, low-risk system may be safer but have more false negatives, i.e. reject more correct programs, decreasing its usability. 
A high-coverage, high-risk system is more usable at the cost of false positives, i.e. executing incorrect programs.
We do not commit to setting an optimal threshold for this trade-off, since it is task-specific. 

We consider 3 systems.
As a baseline, we consider a system that executes everything it predicts (\textbf{accept}); this will result in the highest-possible coverage, but also high risk.  
We can also use MISO's calibrated nature to improve safety outcomes by tuning a sequence-level confidence threshold for rejecting programs (\textbf{tuned}). 
We tune on the full validation set using F1; we explore the range $[0.0,1.0)$ in increments of $0.01$, finding $0.40$ to be optimal.
Finally, we introduce the \textbf{DidYouMean} system for filtering low-confidence programs. 
For a given utterance, DidYouMean shows the user a paraphrase of the input; the user then decides to accept the parse based on this paraphrase.
This allows correctly-predicted low-confidence programs to be accepted and executed, while reducing the user load: making a binary choice to accept a paraphrase is a receptive task, while rephrasing an instruction is a more costly productive task. 


\noindent \textbf{Glossing Model} Since users are typically unfamiliar with formats like Lisp, we need to present the user with a natural language paraphrase -- or \emph{gloss} -- of the candidate parse. 
To train a glossing model, we modify \citet{roy.s.2022}'s seq2seq BenchClamp framework: rather than using the user utterance with the previous turn's context $(\mathcal{U}_0, \mathcal{A}_0, \mathcal{U}_1)$ as input and a program $\mathcal{P}$ as output, we take the context and \emph{program} $(\mathcal{U}_0, \mathcal{A}_0, \mathcal{P})$ as the input and user instruction $\mathcal{U}_1$ as the output. 
We use the BART-large architecture \citep{lewis.m.2020}. 
See \cref{append:glossing} for model details.

\noindent \textbf{DidYouMean System} When a low-confidence parse is detected, DidYouMean triggers a dialogue with the user in order to recover some usability over simply rejecting all low-confidence parses.
\cref{fig:example} shows the system workflow.
DidYouMean shows the original utterance $\mathcal{U}_1$ and the gloss $\hat{\mathcal{U}}^*$ to the user, who determines whether they are identical or not.
If they accept the gloss, we optionally re-parse the gloss $\hat{\mathcal{U}}^*$ rather than the original utterance $\mathcal{U}_1$; this can remove typos and other idiosyncracies.
We call this the \textbf{re-parsed} setting, while choosing the original prediction $\hat{\mathcal{U}}$ is the \textbf{chosen} setting. 
We predict that allowing users to accept and reject glosses will improve the balance between safety and usability (i.e. F1) over the threshold system by allowing them to accept correct low-confidence parses.
In other words, adding human interaction will allow us to achieve a balance which cannot be attained given the tradeoffs resulting from thresholding. 

\noindent \textbf{User Study} We conduct a static user study of DidYouMean using examples from the SMCalFlow validation set. 
We sample 100 MISO predictions with a minimum confidence below $0.6$ (to ensure that the set contains a number of mistakes). 
This sample is stratified across 10 equally-spaced bins with 10 samples per bin. 
MTurk annotators were shown the dialogue history, the user utterance, and the gloss, and asked to confirm that the gloss was correct. 
The template and instructions can be seen in \cref{append:template}. 
We obtained 3 judgments per example.

\noindent \textbf{Annotation Statistics} 8 annotators completed at least one judgement, with 4 completing the majority. 
All 3 annotators agreed on $79\%$ examples, indicating the task is well-formulated. 
For the remaining $21\%$, we use the majority decision to accept or reject. 
After majority voting, annotators accepted $68/100$ glosses and rejected $32$.

\noindent \textbf{Results} 
\begin{table}[ht]
    {\small
    \centering
    \begin{tabular}{lcccccc}
    \hline \hline
    Setting & Cov. $\uparrow$& Risk $\downarrow$ & FP $\downarrow$ & F1 $\uparrow$ & F0.5 $\uparrow$ \\
    \hline \hline
    Accept & 1.00 & 0.67 & 67 & 0.50 & 0.38 \\
    \hline
    Tuned & 0.32 & 0.50 & 16 & 0.49 & 0.50 \\
    Chosen & 0.68 & 0.54 & 37 &  0.61 & 0.51 \\
    Re-parsed & 0.68 & 0.41 & 28 &  0.66 & 0.62 \\
    \hline 
    \end{tabular}
    \vspace{-0.5em}
    \caption{Coverage, risk, number of false positives (FP), and F measures for accepting correct parses and rejecting incorrect parses.}
    \label{tab:user_study}
    \vspace{-1em}
    }
\end{table}
\cref{tab:user_study} shows the results of the user study. 
In addition to standard selective prediction metrics like coverage (the percentage of inputs for which a program is executed) and risk (the percentage of executed programs that are incorrect) we report the number of false positives (incorrect programs executed) and F1 and F0.5  scores.
Tuning a threshold results yields better safety outcomes than accepting everything, with lower risk.
However, this safety comes at a cost to the usability of the system; a coverage of only 0.32 indicates that only $32\%$ of inputs have their programs executed.
The ``tuned'' system's low usability is reflected in the F1 and F0.5 scores, which balance precision and recall. 
The ``chosen'' system, while better in F1, is comparable to the ``tuned'' system in F0.5, which takes both usability and safety into account but prioritizes safety at a 2:1 ratio.
Users are able to recover some usability (as measured by coverage) in this setting but also add to the risk, which is higher for ``chosen'' than ``tuned''. 
The number of incorrect programs executed increases when glosses are chosen (as compared to the tuned threshold). 
When the accepted glosses are re-parsed, we see a shift back towards a system favoring safety, with fewer incorrect programs being executed than in the ``chosen'' setting; this is reflected in a lower risk score. 
For both F1 and F0.5, the ``re-parsed'' system best balances usability and safety. 

These results show that a calibrated model can be used with a threshold to greatly improve safety, reducing the number of incorrect programs accepted by $76\%$.
DidYouMean allows users to recover some low-confidence programs by accepting and rejecting programs based on their glosses, resulting in the best aggregated scores. 
Note also that the threshold was tuned on F1 score on the entire dev set. 
This means that the F1 performance of that tuned system is as high as possible for confidence-threshold-based system. 
Thus, DidYouMean achieves a balance outside what can be achieved by tuning: simply increasing the threshold would decrease safety and result in a lower F1 than the current threshold of 0.40. 

\vspace{-0.5em}
\section{Conclusion}
\vspace{-0.5em}
We examine two common trade-offs in semantic parsing, and how a well-calibrated model can be used to balance them.
In \cref{sec:hitl} we illustrated how token-level model confidences could be used in a simulated HITL task-oriented parsing annotation task. 
Our experiments in \cref{sec:user} extended these results to sequence-level confidences and non-expert users; we found that model confidence could be used to improve the usability-safety trade-off and introduced DidYouMean, which improved usability by asking users to accept predictions. 

\section{Limitations}
Our study is limited by the models, datasets, and languages we consider.
Firstly, we examine only English datasets, limiting the impact of our results.
We also only consider one task-oriented parsing dataset, and focus on one model architecture.

We make several limiting assumptions in \cref{sec:hitl} and \cref{sec:user}.
Foremost amongst these is the assumption of access to an oracle annotator in \cref{sec:hitl}; clearly, no such annotator exists.
Our results may vary when real annotators are brought into the loop.
For one, we do not know exactly how choosing from the top-k list will compare to insertion w.r.t. speed.
We also do not know how automation bias \citep{cummings.m.2004} would affect the top-k list: given that the correct answer is often in the list, real annotators might overly rely on the list and prefer it to token insertion, resulting in incorrect programs. 

The experiments in \cref{sec:user} rely on a glossing model to translate predicted programs into natural language (NL). 
We approached with a neural Lisp-to-NL model; this has several limitations.
Neural text generation models often hallucinate outputs, i.e. generated glosses may not be faithful to their corresponding programs.
Unlike \citet{fang.h.2022}, who use a grammar-based approach for response generation, we do not assume access to a grammar but note that our method is compatible with grammar-based constraints. 
Our annotators in \cref{sec:user} face the additional challenge of interpreting and choosing glosses. 
SMCalFlow programs are nuanced and slight input variations can result in different programs.
These nuances are often obscured by the glossing model, resulting in two different programs glossing to semantically equivalent utterances; we explore this further in \cref{append:selection}. 
Annotators might mistakenly accept glosses from incorrect programs or reject correct glosses; this would be difficult to address even with a faithful translation method.


\section*{Acknowledgements}
We would like to thank  Anthony Platanios, Subhro Roy, Zhengping Jiang, Kate Sanders, Yu Su, and Daniel Khashabi for their feedback on an earlier draft. 
Elias Stengel-Eskin is supported by an NSF Graduate Research Fellowship. 

\bibliography{calibration}
\bibliographystyle{acl_natbib}

\newpage
\appendix

\section{Model and Data} \label{append:model}

\paragraph{Data} We use version 2.0 of SMCalFlow, which contains language inputs paired with Lisp-like programs. 
\cref{fig:data_ex} shows an example input and output for SMCalFlow.
Unlike many other task-oriented formalisms, SMCalFlow contains salience operators like {\tt{refer}}, which allow for deixis in the input.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/data_ex.pdf}
    \caption{Input-output example for SMCalFlow}
    \label{fig:data_ex}
\end{figure}



\paragraph{Input Representation} 
We follow \citet{stengel-eskin.e.2022smcalflow} and \citet{roy.s.2022} in using the previous dialogue turn as input if available. 
Thus, each datapoint consists of an input $X = (\mathcal{U}_0, \mathcal{A}_0, \mathcal{U}_1)$ and an output program $\mathcal{P}$, where $\mathcal{U}_0$ is the previous user utterance (if it exists), $\mathcal{A}_0$ is an automatically-generated agent response to the previous utterance, and $\mathcal{U}_1$ is the current user utterance. 
\noindent \textbf{Paraphrasing}
To find a good paraphrase of each predicted parse, we generate $N$ glosses $\hat{\mathcal{U}}_1, \ldots, \hat{\mathcal{U}}_{N}$ via beam search and take the one that yields the highest probability of the predicted parse $\hat{\mathcal{P}}$, i.e. $\hat{\mathcal{U}}^* = \text{argmax}_i\: P_{MISO}(\hat{\mathcal{P}} | \mathcal{U}_0, \mathcal{A}_0, \hat{\mathcal{U}}_i)$. 

\paragraph{Evaluating the glossing model} \label{append:glossing} Instead of evaluating the glossing model using string metrics such as BLEU \citep{papineni.k.2002} which can be noisy, we choose to evaluate the output programs using cycle-consistency. 
Specifically, we evaluated  trained models by glossing programs in the gold test set and then parsing those glosses with a fixed MISO parser. 
We explore T5-base, T5-large, BART-base, and BART-large architectures. 
All accuracy scores are reported in \cref{fig:gloss_em} along with the baseline accuracy obtained by parsing the gold test inputs. 
The best-performing gloss model is BART-large. Note that all glossing models outperform MISO without glosses. 
This can be explained by the fact that we gloss the inputs from the gold program, which we then evaluate on, allowing for information leakage. 
We also hypothesize that the gloss model, having been trained on the entire dataset of utterances, averages out many annotator-specific idiosyncracies which may make inputs hard to parse. 
This result does \emph{not} imply that glosses generated from \emph{predicted} programs would yield better performance when parsed than the user input.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/gloss_acc.pdf}
    \vspace{-2.5em}
    \caption{Exact match for programs parsed from glossed inputs from the SMCalFlow test set. Performance without glosses is overlaid in black.}
    \vspace{-1em}
    \label{fig:gloss_em}
\end{figure}


\section{User study template} \label{append:template}
\cref{fig:confirmation_template} shows the template for the confirmation HIT. 
The instructions asked users to read the paraphrase produced by the agent and determine whether the agent had correctly understood the user. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/confirmation_template.png}
    \caption{Template for the confirmation HIT.}
    \label{fig:confirmation_template}
\end{figure}
Annotators were recruited from a list of qualified annotators on Amazon Mechanical Turk and paid $\$0.05$ per example, averaging to roughly $\$16$ per hour. 

\section{Selection HIT} \label{append:selection} 
In \cref{sec:user} we presented the DidYouMean system, which allowed users to confirm or reject a potential gloss. 
The gloss was chosen from a larger set of candidates; this naturally raises the question of whether users can directly choose a gloss from the candidate list, rather than accepting or rejecting a single gloss at a time.
Here, we examine to what extent it is helpful to users to choose glosses from a list of options. 
We take a sample of validation programs for SMCalFlow, stratified across 10 evenly-spaced bins from 0.0 to 1.0. 
Note that this differs from the setting in \cref{sec:user}, where the maximum bin was 0.6. 
We then present the top predictions using nucleus sampling \citep{holtzman.a.2019}; the number of predictions depends on the confidence of the model.
Specifically, we set a cutoff of 0.85 and add predictions until the sum of the minimum token probabilities across predictions is greater than 0.85.
We limit ourselves to a maximum of 10 predictions, even if the cutoff is not reached. 

The template is seen in \cref{fig:selection_template}; each example gives the gloss as well as a rounded confidence score for the predicted program.
Annotation was run with the same group of annotators as the experiments in \cref{sec:user}; annotators were paid $\$0.11$ per example, or about $\$16$ per hour of annotation. 
Each example was annotated by a single annotator, all of whom had been vetted in a pilot annotation task. 
Annotators were instructed to help a robot named SvenBot, who had recently learned English and was not confident about its understanding, disambiguate between several options. 
The interface contained a text box where annotators could optionally manually re-write the input; this was only to be done in cases where \emph{none} of the options reflected the intended meaning of the original utterance. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/selection_template.png}
    \caption{Template for the selection HIT.}
    \label{fig:selection_template}
\end{figure}

Of the 100 examples sampled, annotators manually rewrote 7 and chose from the top list for the other 93. 
Ignoring the rewritten examples, 39 model predictions were incorrect and 54 were correct;
by choosing glosses, annotators correct 5 incorrect predictions.
However, they also inadvertently changed 4 correct predictions to incorrect glosses. 
Figure \cref{fig:selection_results} shows the exact match accuracy before and after annotators selected glosses at each confidence bin.
At low confidence, we see very minor increases on the order of a single program being corrected.
At high confidences, annotators generally have only one or two options, and are able to choose the correct one, resulting in similar performance to nucleus decoding. 
However, at medium confidences, annotators often choose the wrong gloss, leading to lower performance.
Qualitatively, these incorrect choices are primarily driven by glosses that appear to be paraphrases of each other, but in fact correspond to subtly different Lisp programs. 
% This highlights the difficulty of determining the mapping between SMCalFlow programs and their glosses. 
Taken together, these results suggest that accepting and rejecting glosses is more promising than choosing them. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/human_correction.pdf}
    \caption{Selection experiment. Annotators sometimes select accurate glosses, but often have trouble deciding between seemingly invariant glosses, lowering performance especially at medium confidences.}
    \label{fig:selection_results}
\end{figure}




\end{document}
