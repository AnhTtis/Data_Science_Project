\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{enumitem}   
\usepackage{amssymb,amsmath,amsthm}
\usepackage{nicematrix}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{accents}
%\usepackage{eufrak}
\usepackage{dsfont}
\usepackage{hyperref}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\newcommand{\LJ}[1]{\textcolor{blue}{\bf \scriptsize [LJ: #1]}}
\newcommand{\JT}[1]{\textcolor{magenta}{\bf \scriptsize [JT: #1]}}

\usepackage{ulem}
\normalem
\newcommand{\edt}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{red} \sout{#1}}}

\usepackage{cleveref}

\crefname{equation}{}{}

\title{Learning to Reconstruct Signals From Binary Measurements}
\author{JuliÃ¡n Tachella$^{1}$ and Laurent Jacques$^{2}$}


\date{%
    $^1$Laboratoire de Physique, CNRS, ENS Lyon, Lyon, France\\%
    $^2$ICTEAM, UCLouvain, Louvain-la-Neuve, Belgium\\[2ex]%
    \today
}

\newcommand{\signalset}{\mathcal{X}}
\DeclareMathOperator*{\signc}{sign}
\DeclareMathOperator*{\diam}{diam}
\newcommand{\sign}[1]{\signc\left(#1\right)}
\newcommand{\baseset}{\mathcal{X}_0}
\newcommand{\ntransf}{G}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\deltopt}{\delta_{\textrm{opt}}}
\newcommand{\covering}[1]{\mathcal{N}_{\signalset}(#1)}
\newcommand{\expectation}[1]{\mathbb{E}\{#1\}}
\newcommand{\order}[1]{\mathcal{O}(#1)}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\R}[1]{\mathbb{R}^{#1}}
\newcommand{\B}[1]{\mathcal{B}^{#1}}
\newcommand{\ball}[1]{B_{\epsilon}{(#1)}}
\newcommand{\Sp}[1]{\mathbb{S}^{#1}}
\newcommand{\C}[1]{\mathbb{C}^{#1}}
\newcommand{\vect}[1]{\text{vec}(#1)}
\newcommand{\rangeA}{\mathcal{R}_A}
\newcommand{\rangeAg}{\mathcal{R}_{A_g}}
\newcommand{\nullA}{\mathcal{N}_A}
\newcommand{\proposed}{SSBM}
\newcommand{\gwidth}[1]{w_{#1}}
\newcommand{\rk}[1]{\text{rank}(#1)}
\newcommand{\bdim}[1]{\operatorname{boxdim}\left(#1\right)}
\newcommand{\nullAg}{\mathcal{N}_{A_g}}
\newcommand{\Hilb}{\mathcal{H}}
\newcommand{\jacob}{J_{\hat{x}}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\boxdim}{boxdim}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\renewcommand{\theenumi}{\Alph{enumi}}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\newtheorem{assumption}{Assumption}
%\date{July 2022}

%% LJ shortcuts
%% Useful shortcuts we often use
\newcommand{\bs}{\boldsymbol}
\newcommand{\bb}{\mathbb}
\newcommand{\cl}{\mathcal}
\newcommand{\ts}{\textstyle}
\newcommand{\ds}{\displaystyle}
\newcommand{\ie}{\emph{i.e.}, }
\newcommand{\eg}{\emph{e.g.}, }
\newcommand{\iid}{%
  \ifmmode% math mode
  \mathrm{i.i.d.}%
  \else%
  i.i.d.\@\xspace%
  \fi%
}

\begin{document}
In this short note, I explain why the model identification bounds in the main paper also provide bounds on the error of the learned reconstruction function from measurement data alone. 

\paragraph{Lower Bound}
The oracle argument is a constructive way (although impractical) to reconstruct the samples: 
if we had oracle access to $\ntransf$ measurements of a point $x_0$ in $\signalset$ through each of the $\ntransf$ different operators, we could stack them together to obtain a larger measurement operator, defined as 
\begin{equation} \label{eq: oracle}
    \begin{bmatrix}
    y_1 \\
    \vdots \\
    y_{\ntransf}
    \end{bmatrix} = \sign{Mx_0}  \text{  with  }
    M =  \begin{bmatrix}
    A_1 \\
    \vdots \\
    A_{\ntransf}
    \end{bmatrix} \in \R{m\ntransf\times n}.
\end{equation}
We could obtain a reconstruction estimate by inverting the system, which is overcomplete if $m>n/\ntransf$, and according to~\cite{goyal1998quantized} we would obtain an error of $\mathcal{O}(1/m\ntransf)$ which is exactly the same as the lower bound for model identifiability in Section 3.1. It is clear that this is a lower bound since it is better than signal recovery bounds with the known signal set, which go as $\mathcal{O}(1/m)$.
This way of reconstructing is only practical if the signal set has dimension 0 (e.g., $\signalset=\{x_0\}$). 

\paragraph{Upper bound}
A similar argument that the one used for lower bounding the model identification error in Theorem 5 can be used to lower bound the error of the learned reconstruction function \LJ{Assuming it provides a concistent estimate}.


We would have to prove that the consistency cell of a given measurement vector associated with \emph{any} ground truth signal $x_0\in\signalset$ for some operator $A_s$, i.e.,
$$ 
S_{x_0} = \{v\in \Sp{n-1} | \; \sign{A_sv}=\sign{A_sx_0} \} 
$$
 verifies that
\begin{equation}
    \text{reconstruction error} \leq \text{radius} ( S_{x_0} \cap \hat{\signalset}) \leq \delta.
\end{equation}
with high probability w.r.t. a random draw of the operator $A_1,\dots, A_{\ntransf}$. \LJ{I'm not sure to understand what you mean by reconstruction error here for the "learned algorithm". Do you speak of an hypothetical algorithm $\hat{f}$ providing a consistent estimate? Then, by reconstruction error, do you refer to a non-uniform one, bounded for a given $x_0$?}

Equivalently, we need to show that
\begin{equation}
\label{eq:constraints rec}
\begin{cases}
\sign{A_gx_g} = \sign{A_gv} \quad  \forall g =1,\dots,\ntransf \\
\sign{A_sx_0} = \sign{A_sv}
\end{cases}
\end{equation}
holds for some 
$v\in d(x_0,\delta) $
and some $ x_0,x_1,\dots,x_{\ntransf}\in \signalset$ with probability at most $\xi$, where
$$
d(x_0,\delta) = \{ x\in \Sp{n-1} |\; \|x - x_0\|\geq \delta\} .
$$
Dropping the constraint with $g=s$ in~\cref{eq:constraints rec}, the proof should continue similarly to the one of Theorem 5 with the only difference that we cover $d(x_0,\delta)$  instead of $\Sp{n-1}\setminus\signalset_{\delta}$. 
\LJ{I see the modification of the proof you propose. But then, since $x_0$ is fixed, we target here a non-uniform error bound on the learned algo, right? Moreover, we have almost $\bb S^{n-1} \setminus \cl X_\delta \subset d(x_0,\delta)$, showing that the covering number of this last set should at least be larger than the one of the first.}
These quantities should have the same covering number decay $\approx \epsilon^{-(n-1)}$ since they are $n-1$ dimensional and they are essentially treated as the full sphere $\Sp{n-1}$, although there might be some additional technical issue with the fact that $d(x_0,\delta)$ depends on $x_0$ (and thus the full set is not a simple cross product of G times the signal set, as in the model identification case).

\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}
