\documentclass[12pt]{article}



% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\usepackage{hyperref}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{enumitem}   
\usepackage{amssymb,amsmath,amsthm}
\usepackage{nicematrix}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{accents}
\usepackage{xspace}
%\usepackage{eufrak}
\usepackage{dsfont}
\newcommand{\LJ}[1]{{\color{blue} \bf \scriptsize [LJ: #1]\par}}
\newcommand{\JT}[1]{{\color{magenta} \bf \scriptsize [JT: #1]\par}}
\usepackage[a4paper, total={7in, 9in}]{geometry}

\usepackage{ulem}
\normalem
\newcommand{\edt}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{red} \sout{#1}}}

\usepackage{cleveref}

\crefname{equation}{}{}

\DeclareMathOperator*{\signc}{sign}
\DeclareMathOperator*{\diam}{diam}
\newcommand{\sign}[1]{\signc\left(#1\right)}
\newcommand{\baseset}{\mathcal{X}_0}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\deltopt}{\delta_{\textrm{opt}}}
\newcommand{\covering}[1]{\mathcal{N}_{\mathcal{X}}(#1)}
\newcommand{\expectation}[1]{\mathbb{E}\{#1\}}
\newcommand{\order}[1]{\mathcal{O}(#1)}
\newcommand{\red}[1]{\textcolor{black}{#1}}
\newcommand{\corr}[1]{\textcolor{black}{#1}}
\newcommand{\R}[1]{\mathbb{R}^{#1}}
\newcommand{\B}[1]{\mathcal{B}^{#1}}
\newcommand{\ball}[1]{B_{\epsilon}{(#1)}}
\newcommand{\Sp}[1]{\mathbb{S}^{#1}}
\newcommand{\C}[1]{\mathbb{C}^{#1}}
\newcommand{\vect}[1]{\text{vec}(#1)}
\newcommand{\rangeA}{\mathcal{R}_A}
\newcommand{\rangeAg}{\mathcal{R}_{A_g}}
\newcommand{\nullA}{\mathcal{N}_A}
\newcommand{\concatA}{\bar{A}}
\newcommand{\proposed}{SSBM}
\newcommand{\gwidth}[1]{w_{#1}}
\newcommand{\rk}[1]{\text{rank}(#1)}
\newcommand{\bdim}[1]{\operatorname{boxdim}\left(#1\right)}
\newcommand{\nullAg}{\mathcal{N}_{A_g}}
\newcommand{\Hilb}{\mathcal{H}}
\newcommand{\jacob}{J_{\hat{x}}}
\renewcommand{\theenumi}{\Alph{enumi}}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{proposition}[theorem]{Proposition}


\newcommand{\answer}[1]{{\color{red} #1}}


\begin{document}

\section*{Reviewer 1}

\answer{Thank you for the very detailed review, we appreciate the time taken to review our work. We believe that the feedback has helped to improve the paper, in particular the exactness of the proofs. We have uploaded a revised manuscript with changes highlighted in red. We answer your comments in the text below:}

\begin{enumerate}
\item The acknowledgments mention the funding, which might be a breach of double blindness. I will refer to the editor for this point as I am not sure of the importance of this issue.

\answer{We sincerely apologize for this mistake, we have removed it in the updated manuscript.}

\item The authors write that the self-supervised algorithm presented in Section 4 is motivated by the analysis of model identification in Section 3. How is that so? Perhaps this would deserve more explanation, especially since the algorithm performs signal reconstruction, not model identification.

\answer{Learning a reconstruction function $f_{\theta}$ from measurement data alone has an implicit link with the model identification problem since the reconstruction function requires some knowledge about the signal set. As discussed in Section 2, the optimal reconstruction function  
$$
    \hat{f}(y) = \text{centroid}(S_{y} \cap \mathcal{X}).
$$
where $S_y\subset \mathbb{S}^{n-1}$ is the consistency cell associated with $y$, depends crucially on $\mathcal{X}$. 
The proposed learning algorithm builds a discrete approximation of the signal set using the reconstructed dataset, i.e.,  $\cup_{g=1}^G \tilde{\mathcal{X}}_g$ where $\tilde{\mathcal{X}}_g = f_{\theta}(\mathcal{Y}_g,A_{g})$ for $g=1,\dots,G$ and $\mathcal{Y}_g$ is the subset of (training) measurement vectors associated with the $g$th operator. From a model identification perspective, the measurement consistency loss ensures that $\mathcal{Y}_g = A_g\tilde{\mathcal{X}}_g$
for all $g=1,\dots,G$. The second loss ensures consistency across all operators, i.e., $\tilde{\mathcal{X}}_g = f_{\theta}(A_s\tilde{\mathcal{X}}_g, A_s)$ for all pairs $s, g \in \{1,\dots,G\}$, acting as a proxy for $\tilde{\mathcal{X}}_s = \tilde{\mathcal{X}}_g$. 
We have included this discussion in Section 4 of the updated manuscript. 
}


\item  Why are there so few values of $m$ presented in Figures 6-7-8? Adding more values could allow for more convincing plots, e.g. for the scaling with $m$ shown in Figure 6 - right, by taking a large maximal value of $m/n$ (which is at the moment 1.28).

\answer{We have included more values of $m$ in Figures 6 and 8 (Figures 6 and 9 in the updated manuscript), showing results for sampling ratios up to $m=4n$. The new plots follow similar trends as the results in the first submission while providing a more detailed understanding of the empirical performance of each method as a function of $m$. In particular, the scaling of the proposed self-supervised method with $m$ shown in Figure 6  is compatible with the $\mathcal{O}(\frac{1}{m}\log m)$ scaling for values of $m>n$.}

%\answer{\JT{Tbh, I don't know if adding more intermediate values of $m$ will add much information. Nonetheless, we can add an experiment for $m>1.28n$, e.g. $m=2n$ and $m=4n$.} \LJ{TODO: I guess this reviewer mainly wants higher values of $m/n$. I'd just add these two values to please the reviewer, hoping a trend will be confirmed. Doable?}}

\item  Why did the authors restrict the choice of the trade-off parameter to be \{0.1, 1, 10\}? A more thorough investigation of the optimal choice of $\alpha$ (at least in some particular cases) would be an interesting addition, as the current choice seems quite arbitrary.

\answer{We empirically observed that all the measurement consistency losses obtained better results for the simple choice $\alpha=1$ except for the logistic loss. We believe this behavior is due to the interplay between the logistic loss and the squared loss used for cross-operator consistency. We included a new experiment in Appendix E of the updated manuscript showing the performance of the proposed method with the logistic loss as a function of the trade-off parameter $\alpha$ and the undersampling ratio $m/n$. We observed that the optimal $\alpha$ decreases with $m$. Thus, in the remaining experiments, we use the simple rule $\alpha=0.1$ for $m<n$ and $\alpha=0.06$ for $m\geq n$.

We believe that a complete theoretical analysis of the optimal choice of $\alpha$ is an interesting but complex problem that is out of the scope of this paper. We leave this analysis for future work. }% \LJ{Perfect}

%\answer{\JT{I guess we can try more values of $\alpha$.}\LJ{TODO: This could smooth the curves a bit. Not sure what kind of "thorough investigation" the reviewer means. Theoretical? I think it's beyond the scope and we should say it. I'd propose a systematic sweep between, e.g., 0.01 and 100 with say 16 points. Doable?}}

\item The proofs in appendices contain some mistakes (often easy to fix) and did not always seem to receive the same attention in the writing compared to the main text. While this does not impact the results, the repetition of small mistakes or the lack of clarity can make a bad impression on a careful reader. I list these problems now:

\answer{We apologize for these mistakes. We have modified the proofs of the main theorems, addressing all of the issues raised by the reviewer. We have also reorganized some parts of the proofs and simplified some parts of the notation with the aim of rendering the proofs more accessible to the reader. }

\begin{itemize}
    \item  In the proof of Theorem 1 the value of $\epsilon$ chosen is (I think) not correct, as one obtains $\delta/\pi - \delta/2$ in the lower bound between eqs. (34) and (35). This will probably change the constants later on and should be corrected.

    \answer{We thank the reviewer for finding this mistake in the choice of $\epsilon$. We have fixed this error in the updated manuscript (see page 20) by choosing $\epsilon = \frac{\delta (4-\pi)}{8+4\pi\sqrt{n\pi/2}}$, which avoids a negative $\delta$ and results in a slight modification of the constants in the final bound: 
     $$
        \textstyle m \geq \frac{4}{\delta} \Big(2k\log\frac{30\sqrt{n}}{\delta} + \log\frac{1}{\xi} \Big)
    $$
    that holds for $\delta\leq 30 \epsilon_0 \sqrt{n}$.
    }
   %  \JT{I have fixed the proof, which now has different constants}
   %  \LJ{TODO: Not related to $n$ IMO. We have a small bug here in the change of variable (I guess the $1/\pi$ was forgotten). I get 
   % \begin{align*}
   %      &\textstyle \frac{\delta}{\pi} - (2/\pi + \sqrt{\pi n/2}) \frac{\pi\delta}{4+\pi\sqrt{2\pi n}} \\
   %      &\textstyle= \delta (\frac{1}{\pi} - \frac{1 + \frac{\pi}{4}\sqrt{2\pi n}}{2+\frac{\pi}{2}\sqrt{2\pi n}}) = \delta (\frac{1}{\pi} - \frac{1}{2}) < 0 \text{??}
   %  \end{align*}
   %  Anyway, I'd also reformat them a bit the expressions to make them nicer.}
    
    \item In the proof of Theorem 6, the equality of eq. (43) should actually be an upper bound, as the vector $v$ does not 
    depend on $g\in [G]$ there is no reason for it to be an equality.

    \answer{The reviewer is right. Moreover, the inequality was missing the symbol $\exists$ before $v$. Answering comments related to this one, by this reviewer and the others, we have also simplified and reorganized the proofs of Theorems 1 and 6 (Theorems 1 and 7 in the updated manuscript). We believe they are now easier to understand.  } %\LJ{I'd not say more.}

    %\LJ{TODO: Ambiguous notation in (43): we should write $\exists x_g \in B_\epsilon(\tilde x_g), \exists v \in B_\epsilon(\tilde v)$ instead of $\exists x_g \in B_\epsilon(\tilde x_g), v \in B_\epsilon(\tilde v)$. The reviewer may think $v$ is fixed as there is not quantifier in front of it. This should be corrected elsewhere too if needed.}

    \item I think the derivation of $\delta$ after Theorem 6 should be one of an upper bound on the minimal error $\delta$ achievable for a given value of $m$, the current presentation of a lower bound is a bit confusing. For this minimal error eq. (61) becomes an equality and the bound of eq. (63) becomes an upper bound, which is consistent (see the next point).

    \answer{We agree with this observation. We have modified the derivation to stress that the upper bound is obtained by choosing the equality in eq. (63) (equation (49) in the updated manuscript). See also the comment below for more details.}

    \item  I missed something in eq. (63): how does it follow from eq. (62)? Also eq. (62) is only valid if $ae^{b}\geq 1$ , how does this translate to the original parameters? See also the previous point for a possibly simpler presentation.  One should precise for which values of $\xi$ eq. (64) holds (even though it holds even for exponentially small in $n$ I believe).

    \answer{We have simplified the derivation of $\delta$ in the updated manuscript, avoiding altogether the use of the Lambert function. The new derivation should solve all the issues of the reviewers' comments, while (in our opinion) being easier to follow. %\LJ{I have removed the derivation of $\delta$ from these answer to gain space. Morever, the one that was here was not up-to-date anymore.} 
    
    }

    
    % \LJ{TODO: I agree. Note that the whole derivation from (54) to (63) can be turn in a few lines. Here is the result:}
    % If $\delta \geq \frac{1}{a} (\log a + b)$ (Eq. 63) with $\log a + b > 1$ (which holds since $b > \log 5 \sqrt n > 1$ for all $n \geq 1$ and $\log a > 0$ since $a = \frac{m}{4(k + \frac{n}{G})}> 1$), then 
    % $$
    % \log \delta + \delta a \geq \log \delta + \log a + b = \log (a \delta) + b\geq b, 
    % $$
    % which directly proves that (63) involves (54).
    % }}
    
    
    % \JT{Since we have $W(x)\geq \log x - \log \log x$ for $x\geq e$, we also have the (simpler) inequality of eq. (62):  $W(x) \geq \log x$.}\JT{there was also a mistake in eq. (62), it should have been $\delta \geq \frac{1}{a} \log(ae^{b}) - \frac{1}{a}\log(\log a + b)$, however this eq. will be omitted in the updated manuscript}\LJ{TODO: I'd suggest simplifying these developments from the 2-line argument I give above that does not require $W$.}
    

    % \answer{\JT{It looks that it should be $ae^b\geq e$ or equally $\log a + b \geq 1$, which gives
    % \begin{align*}
    %     \log \frac{m}{4(k+n/G)} + \log 5\sqrt{n} + \frac{1}{Gk+n} \log\frac{1}{\xi} + \frac{n}{Gk+n}\log 3 \geq 1 \\
    %     \log \xi \leq (Gk+n) (\log \frac{5\sqrt{n}m}{4(k+n/G)} + \frac{n}{Gk+n}\log 3)
    % \end{align*}
    % which holds for $\xi \leq 1$ as long as 
    % \begin{align*}
    %     (Gk+n) (\log \frac{5\sqrt{n}m}{4(k+n/G)} + \frac{n}{Gk+n}\log 3) > e \\
    % \end{align*}
    % This is true for any $m\geq \frac{4(k+n/G)}{5\sqrt{n}}$.}\LJ{TODO: See my answer above, I guess it solves everything, we just need $\log a + b > 1$ which trivially holds.}}

    \item The argument in the proof of Theorem 8 combining Corollary 10 with a union bound needs to be explained more. To me, it seems to be using Bernstein's inequality, which would need to be at least mentioned.

    \answer{We apologize for this as there were indeed a missing explanation in our first derivation. We have now modified the proof of Theorem 8 (Theorem 10 in the updated manuscript) to clarify the use of Bernstein's inequality in the step combining the results from Corollary 10 (Corollary 11 in the updated manuscript) and a union bound over the centers of the covering $Q_{\epsilon}$. 
    
    Moreover, we have simplified both the statement and the proof of this theorem, introducing explictly the failure probability. We have also completed this theorem with an additional upper bound on the expectation of the $|\sign{A \mathcal{X}}|$ that completes well, we think, the probabilist bound.   
    }
    %\JT{Laurent?}\LJ{TODO: Yes, I'll add a few lines in the proof for clarity. I guess it's too quick for the moment.}

    \item In the proof of Theorem 8, how does the bound on $Z(S)$ transfer to a bound on $|\Phi(S)|$? This is used in the proof, but not really explained. 

    \answer{Given $\Phi(\cdot) = \sign{A\cdot}$, the number of discontinuities $Z(S)$ can be used to upper bound the number of intersected cells, by noting that $|\Phi(S)|\leq 2^{Z(S)}$ for any set $S\subseteq \mathbb{S}^{n-1}$. This explanation has been included in the updated proof.} %\JT{Laurent, do you agree?} \LJ{Yes, I have also added a few hints in the proof about this.}
 
 \item In the proof of Theorem 8, should the condition $m\sqrt{n}/(sk) > e$
 be replaced by  $3m\sqrt{n}/(16sk) > e$ (the stronger)?    

 \answer{
Indeed, thank you for spotting this mistake. However, in the simplified proof this bound is not useful anymore. Note that we voluntarily introduced a few crude bounds to simplify further certain constants. %\LJ{I have simplified this answer since the proof has changed.}  
%We have corrected it in the updated manuscript. The choice of $s=\frac{6}{5}$, this requirement can be written as
%  $$
%  m > \frac{32 e k}{5\sqrt{n}} 
%  $$
% which can be simplified to 
% $$
% m > \frac{18  k}{\sqrt{n}}. 
% $$
% We have added this condition to the statement of the theorem. Nonetheless, it is worth noting that this is a very mild condition on $m$. \JT{Laurent, do you agree that we need to add this to the statement? We could also write this condition as $n\geq 81$ and $m>2k$, what do you think?}
 }
\end{itemize}
\end{enumerate}

\paragraph{Minor points}
\begin{enumerate}
\item In Table 1 one should introduce what the identification error is, since it is only introduced later in the text.

    
\answer{We have included an informal definition of the model identification error $\delta$ in the caption of Table 1 in the updated manuscript, which reads "The identification error $\delta$ corresponds to the maximal error of the optimal estimation of the signal set from binary measurement data alone". The formal mathematical definition is deferred for Section 3 and is encapsulated in Definition 3.1 in the updated manuscript.}

\item In Theorem 1, the condition on $\delta$ should be given before the condition on $m$, since the latter depends on the former. 


\answer{We agree with this observation. We have modified the statements of Theorem 1 and 6 (Theorems 1 and 7 in the updated manuscript) such that the condition on $\delta$ is provided before the one on $m$.}

\item Moreover I believe the implication that is proven is $\sign{Ax}=\sign{Au} \implies \|x-u\|\leq \delta$.

\answer{Thank you for spotting this typo. Indeed, the implication of Theorem 1 is $\sign{Ax}=\sign{Au} \implies \|x-u\|\leq \delta$.} %\LJ{Yes! ;-)}

\item Does Section 3.4 assume $k<m$? It seems so since the arguments use binomial coefficients $\binom{m}{k}$, but this condition is not written.

\answer{Indeed, we assume that $k\leq m$, we have made this assumption explicit in the updated manuscript (see page 11).}

\item Before eq. (25) it would be good to recall that in this setting $T_g$ is a representation of some group under which the set $\mathcal{X}$ is invariant.

\answer{Thank you, we have clarified this in the updated manuscript (see page 12).}
 
\item In Figure 6 (and maybe the latter figures as well), one should emphasize that the PSNR grows logarithmically with  $\|x-\hat{x}\|$ as it goes to zero, or write on the axis that the unit is in dB, to enhance readability.
 
\answer{We have modified all the figures displaying a PSNR axis to highlight that the units are in dB.}

\item In Figure 6 it seems to me that the measurement consistency actually clearly separates from the linear inverse approach for $m/n\geq 0.5$, not exactly at $m/n=1$ as stated in the text. Is there an explanation for this phenomenon?


\answer{This is an interesting observation. While we do not have a full explanation for this phenomenon, we believe that it might be due to the inductive bias of the convolutional architecture used in the experiments towards clean images. Nonetheless, it is worth noting that the performance of the measurement consistency approach is worse than the proposed SSBM approach, even for values of $m>n$ (see also the point related to the new experiments for more values of $m/n$).} %$\JT{I don't have an explanation for this phenomenon, it might be related to the inductive bias of neural networks, but I prefer not to mention this.}\LJ{I don't understand this comment and I don't understand what we wrote as I don't see this separation. Let's discuss it.}

\item In Figure 8, one should match the names of the different algorithms in the left and right figures.


\answer{We have corrected the legends in Figure 8 (Figure 9 in the revised manuscript), such that the names of the algorithms are the same in both subfigures.}

\item In the proof of Proposition 3, I think $v$ is just a non-zero element of the nullspace rather than a generator (as the nullspace might have a dimension larger than 1).

\answer{We have modified the proof (see page 8 of the updated manuscript) to mention that $v$ is just an element of the nullspace of $A$ and thus does not necessarily generate the entire subspace.} %\JT{The word \emph{generator} is used to imply that $v$ is one of the vectors spanning the subspace, and not to imply that $v$ is the only one.}

\item The bound of eq. (46) is already used in the proof of Theorem 1 without being stated, it should be first stated there.

\answer{Thank you for spotting this inconsistency. The bound in terms of Euclidean distance is introduced with the statement of Lemma 9 (Lemma 10 of the updated manuscript), and the following proofs of Theorem 1 and 6 (Theorems 1 and 7 of the updated manuscript) directly apply the bound using this distance.}

\item It would be good to add a reference for the standard covering number upper bound above eq. (48).

\answer{We have added a reference  to 
\emph{G. Pisier, The volume of convex bodies and Banach space geometry. Cambridge University Press, 1999, vol. 94.} above eq. (48) (equation ?? in the updated manuscript).

}

%\JT{Vershinin's book?}\LJ{TODO: I'd rather add Pisier "G. Pisier, The volume of convex bodies and Banach space geometry. Cambridge University Press, 1999, vol. 94"} in the updated manuscript.
\end{enumerate}


\paragraph{Other questions}
\begin{enumerate}
\item Is the bound of Proposition 4 tight (up to constant factors) for some matrices? Proposition 3 shows that if $mG<n$ then it is tight, but what about when $mG>n$?

%\JT{I realised that we can use our own Theorem 1 to answer to this question instead of referring to the 2013 onebit CS paper. Do you agree?}\LJ{Indeed, I agree}

\answer{This is an interesting question. As a special case of Theorem 1 with $mG > n$ measurements and a signal set with box-counting dimension equal to $n$, for an $mG \times n$ Gaussian random matrix $\bar A$, with high probability and by picking the minimal number of required measurements in the condition of this theorem, the largest cell of $\sign{\bar A \cdot}$ has a diameter that decays like $C\frac{n}{mG}$ up to log factors. By a standard boosting argument, it thus means that there exists a $mG \times n$ matrix $\bar A$ with the same consistency cell diameter decay. We have now inserted a remark in page 9 of the updated manuscript to explain this fact. 

Moreover, we would like to stress that (18) in Prop. 3 only provides a necessary condition on the rank of $\bar A$. If we consider a binary matrix $\bar A$ with entries in $\pm 1$, one can show that there is a cell in the related tesselation whose diameter is $\sqrt 2$. 
% In fact, for any matrix $\bar A$ whose entries are picked in a fixed finite set of values whatever the values of the dimensions $mG$ and $n$, there is a constant $\underline{\delta}>0$ such that the largest cell of the tesselation has a diameter lower bounded by $\underline{\delta}$. 
This fact is now briefly explained in Example 3.1 of the updated manuscript. }

\item In Section 3.3, can one obtain a bound using Proposition 2 or Corollary 5? If model identification is difficult then signal recovery should probably also be difficult, but maybe this is not an interesting bound?

\answer{The reviewer raises a very interesting point. Consider the optimal reconstruction error of a function learned from measurement data alone, defined as 
$$
    \hat{f}(y) = \text{centroid}(S_{y} \cap \hat{\mathcal{X}}).
$$
where $S_y$ is the consistency cell associated with the measurement vector $y$. This error must be larger than the error of a reconstruction function that has full knowledge about the signal set $\mathcal{X}$. Intuitively, if we have a large model identification error, $\hat{\mathcal{X}}$ will be a bad approximation of $\mathcal{X}$ and thus $\hat{f}$ will obtain large reconstruction errors. The following proposition formalizes this intuition, showing that the reconstruction error of $\hat{f}$ is lower bounded by the model identification error. 

\begin{proposition}
    Given $G$ operators $A_1, \ldots, A_G \in \mathbb{R}^{m \times n}$ and a set $\mathcal{X} \subset \mathbb{S}^{n-1}$ with model identification error equal to $\delta$, there exist points $x_g\in\mathcal{X}$ for $g=1,\dots,G$ such that the reconstruction error is
    $$
    \|\hat{f}( \sign{A_gx_g}) -x_g \| \geq \delta/2,
    $$
    where $\hat{f}$ is the optimal reconstruction function that can be learned from the measurement data $\{ \sign{A_g\mathcal{X}}\}_{g=1}^{G}$, as defined in \Cref{eq: optimal rec}.
\end{proposition}
\begin{proof}
 Following the definition of model identification error (Def. 3.1 in the updated manuscript), there exists a point $\hat{x}\in\hat{\mathcal{X}}$ such that $\|x-\hat{x}\| \geq \delta$ for all $x\in\mathcal{X}$. According to the construction of the inferred set $\hat{\mathcal{X}}$, there exist some $x_1,\dots,x_{G}\in\mathcal{X}$ such that $\sign{A_g\hat{x}}=\sign{A_gx_g}$ for all $g=1,\dots,G$. Therefore, for any $g\in\{1,\dots,G\}$, the diameter of the set $S_{\sign{A_gx_g}} \cap \hat{\mathcal{X}}$ is at least  $\|x_g-\hat{x}\|$ since we have that both $x_g$ and $\hat{x}$ belong to this set. As the optimal reconstruction function outputs the centroid of the cell (as defined in~\cref{eq: optimal rec}, the reconstruction error of the point $x_g$ is at least $\|x_g-\hat{x}\|/2 \geq \delta/2$.
\end{proof}

Therefore, we can use the results on model identification developed in Section 3.1 to lower bound the reconstruction error for the case where the function is learned from measurement data only. In particular, combining this result with Corollary 5 (Corollary 6 in the revised manuscript), we obtain that the (worst-case) reconstruction error should be larger than $\frac{2}{6}\frac{n}{mG}$. It is worth noting that this result also holds for the case where we have a single operator and group invariance, i.e., when $A_g=AT_g$ for $g=1,\dots,G$. 

We have included this proposition and discussion in the updated manuscript (see pages 10 and 11).}
%e can use Proposition 2 and Corollary 5 to provide lower bounds on the reconstruction error of a single vector. This observation is now provided as a remark below Cor. 5 \LJ{TODO}}  

%\JT{Laurent, could you double check that my proof is ok?}\LJ{Perfect. Don't forget reporting the clearer proof here.}
%\JT{Do they mean a lower bound on the reconstruction error? I think that might be indeed possible (using some kind of randomness in the choice of $f$).} %$\LJ{This is my understanding too. }

\end{enumerate}
\paragraph{Typos}

\begin{enumerate}
    \item Before eq. (27), the sentence ``As the first term...'' needs rewriting. 
    
    \answer{We have fixed this sentence in the updated manuscript.}
    
    \item In Figs. 9 and 10, should "each column'' be "each row''?

    \answer{No, the image of each column is observed via a different operator $A_g$ (one out of the $G$ possible ones). We have modified the caption of these figures in the updated manuscript to make this point more clear.}
    
    \item Proof of Theorem 1: "[...] such that'' should be "[...] that''.

    \answer{Thank you. We have fixed this in the updated manuscript.} 
    
    \item In eq. (50), the exponent of $\epsilon$ should be $-kG-n$ not $-kG+n$.
    \answer{Thank you for spotting this typo, we have corrected it in the updated manuscript.}
    
\item In eq. (51) a minus sign is missing in front of the log in the denominator.

\answer{This was indeed an error in our proof; thank you for finding it. We have now clarified this development.} %$\LJ{TODO}

\item In the proof of Theorem 8, $\mathcal{X}$ should be replaced by $Q$ in several places.

\answer{Indeed. This is now corrected. }% \LJ{TODO}}

\item The sentence ``for some $c>0$'' in the proof of Theorem 8 relates to nothing in the equations.
\answer{We have removed this mistake in the updated manuscript.} %\LJ{TODO}

\item In the proof of Theorem 8, there is a 2 missing in the exponent in the last equation (when using the previously derived bound on 
$|\Phi(V_\epsilon(q))|$).
\answer{Thank you for spotting this mistake. As explained above, the proof of this theorem has been updated and this previous (erroneous) bound is not useful anymore.} %\LJ{TODO}

\end{enumerate}

\section*{Reviewer 2}

\answer{We thank the reviewer for the interesting and detailed feedback. We have uploaded a revised manuscript with changes highlighted in red. }
\begin{enumerate}

\item The identification error should be actually defined, e.g. as $\min \{\delta: \mathcal{X} \subseteq \mathcal{X}_\delta \} $

\answer{Indeed, thank you. We have included a formal definition of the identification error in Section 3 of the updated manuscript (see Definition 3.1).} 

\item It might be useful to add a sentence or two after Proposition 2, to explain exactly what has been proven.

\answer{ In words, Proposition 2 shows that we can rotate any signal set $\mathcal{X}$ such that it intersects the largest consistency cell in the tesselation, obtaining a model identification error that is proportional to the maximum cell diameter. The rotation is used to remove the best-case scenarios where the signal set only intersects consistency cells that are smaller than the largest one. We have included this discussion after the proof of Proposition 2 in the updated manuscript.}

\item Proof of Prop.3:

\begin{itemize}
    \item is not necessarily a generator of the nullspace of $\bar{A}$, only an element of it

\answer{We have changed the word \emph{generator} for \emph{element} in the updated manuscript, to clarify that the vector $v$ does not generate the whole nullspace of $\bar{A}$.}

\item It'd be more intuitive to specify directly that $x$ is in the complement of $\text{Ker}(\bar{A})$, instead of the range of $\bar{A}^{\top}$ (even though it's equivalent)
 
\answer{Thank you for this suggestion. We have inserted this equivalent formulation.} %\JT{Doesn't make much difference to me, but if the reviewer wants it..}

 \item I personally would've found it more intuitive to fix $x\in \mathbb{S}^{
n-1}$ and consider $x \eta$ where $\eta\to\infty$, but this is a personal preference.
 
\answer{We agree with this interesting approach, however, we have decided to keep the proposed solution.}

\end{itemize} 
\item Experiments: You mention the "necessary condition of Proposition 3" multiple times; you should either move the $m>n/G$ inside this proposition, or (my preferred option) put this condition in a corollary and refer to it instead.
 
\answer{We have included a corollary to make this condition more clear as suggested by the reviewer (see Corollary 4 of the updated manuscript). The references appearing later on in the text refer now directly to the corollary.}

\item Proof of Theorem 1: The link from (34) to (35) could use a (simple) drawing to explain what's going on. What does the $|$ sign mean in the probabilities? In any case, I think it should be replaced by "and".


\answer{Following the comments of R1, the proof of this theorem has been updated and corrected, stating from the beginning the pursued objective and how Lemma 9 (now Lemma 11) is used. Moreover, this rewriting also avoids the use of a \emph{condition} sign "$|$" altogether. We believe the explanations are clearer now.} 

\item Derivation of $\delta$: the link from (62) to (63) is unclear, but you can get (63) by the simpler inequality $W(x)>\log x/2$ valid for any $x\in \mathbb{R}$.
 
\answer{We have simplified the derivation of $\delta$ in the updated manuscript  (see page 22 of the updated manuscript), avoiding altogether the use of the Lambert function $W(x)$. The new derivation should solve the reviewer's comment, while (in our opinion) being easier to follow.}

\item Proof of Theorem 8: I feel like the use of discontinuity points is a bit circular and non-intuitive; the argument in the proof can be summarized as 
\begin{equation*}
    \text{number of cells} \asymp \text{number of cell borders} \leq \sum_{q\in Q_{\epsilon}} \text{number of borders in $B(q,\epsilon)$}
\end{equation*}
which is more intuitive than the discontinuity argument.

\answer{We have modified the proof of Theorem 8 (Theorem 10 in the revised manuscript) to render it more clear to the reader. The updated proof contains an explicit explanation of the link between the number of discontinuities and the number of intersected cells, which was missing in the first submission and may have caused confusion. In particular, the number of discontinuities $Z(S)$ of the binary mapping can be used to upper bound the number of intersected cells $|\sign{AS}|$, by noting that $|\sign{AS}|\leq 2^{Z(S)}$ for any set $S\subseteq \mathbb{S}^{n-1}$. For completeness, we've also added an upper bound on the expectation $\mathbb E|\sign{AS}|$ that scales similarly to the probabilistic bound.} % This explanation has been included in the updated proof.}

\end{enumerate}
\section*{Reviewer 3}

\answer{Thank you for the comments about our work. We have uploaded a revised manuscript with changes highlighted in red. }

The theory/results are developed/tested without any measurement noise.

\answer{In the setting of binary measurements, noise affects the measurements by flipping the sign, as the observations can only be $-1$ or $+1$. Thus, the proposed method is particularly robust to noise, as it is only affected by noise that is large enough to produce bit flips in the measurement vector. We have included new experiments demonstrating the robustness of the proposed method to noise (please see more details in the answer to one of the points below). We have included this discussion on the impact of noise in the updated manuscript.} %\JT{I think we could add some experiments with noise, without changing the training loss, leaving for future work an analysis of 'dither'  (for the case the noise level is known).}

\paragraph{Requested Changes:}

On the first page, I'd use a different word than "incomplete" (maybe "underdetermined" or "fat") to describe the measurement matrix. The word "incomplete" has too many alternative meanings, e.g., matrix completion. I'd similarly avoid using "incomplete" to describe the measurement process.

\answer{Thank you for this suggestion. It is worth mentioning that the link with matrix completion is not wrong, since our method can be seen as a one-bit matrix completion approach where the columns of the matrix are assumed to belong to a distribution with low-dimensional support, going beyond a simple linear subspace (which is the assumption behind standard low-rank matrix completion). This point is discussed in the Related Work subsection of the manuscript.

We prefer not to use the word "fat" or "underdetermined" as they refer specifically to the properties of the matrix, and do not extrapolate well to the sensing process. For example, while \emph{incomplete measurement process} is meaningful, a \emph{fat measurement process} is not. We have also avoided the use of \emph{compressed} since it is often wrongly associated with random matrices, whereas the necessary conditions presented in this paper are also valid for deterministic matrices. Given these points, we prefer to keep the current naming.} 
%\JT{To be discussed -  the link with matrix completion is not wrong.}

I think equation (10) should be an equality between the measurements. Presently (10) states that if the measurements of two points disagree then they must be close to one another with some probability.

\answer{We agree with the reviewer, this was an undesired typo in equation (10). We have corrected it in the updated manuscript to the equality $\sign{Ax}=\sign{As}$.}


The notation in (14) seems slightly odd. Would it be clearer to write $\mathcal{X}_{\delta}=\{v\in \mathbb{S}^{n-1}| \inf_{x\in \mathcal{X}} \|x-v\|<{\delta}\}$.

\answer{Thank you for this suggestion that we have implemented.}  %\JT{This is equivalent to our definition, right? I would say thanks for the suggestion but we keep the current one.}

I think the paper would be stronger if one of the tests (e.g., Fig. 9) were rerun with varying amounts of measurement noise. At present, it's unclear how sensitive the proposed method is to noise.

\answer{We have included a new experiment evaluating the proposed algorithm with $m=274$ measurements and $G=10$ operators and different noise levels, according to the model
\begin{equation}
    y_i = \sign{A_{g_i,i}x_i + \epsilon_i}
\end{equation}
where $\epsilon_i\sim \mathcal{N}(0,I\sigma^2)$ for $i=1,\dots,N$. Figure 8 of the updated manuscript shows the performance of the SSBM algorithm for different values of $\sigma$. The learning algorithm is particularly robust to noise, obtaining a good performance for noise levels up to $\sigma=0.13$. This noise level translates to having approximately 15\% of bits flipped per measurement vector. It is worth noting that these results indicate that we can expect similarly good performances for other noise distributions (e.g., Poisson noise) for a similar average number of bit flips.}

\paragraph{Typos:} At least on my pdf reader, the Fashion MNIST and MNIST dataset descriptions are missing multiplication signs: $6\times10^5$

\answer{Thanks for spotting these typos, we have updated them in the new manuscript to $6\times 10^4$  for both MNIST and FashionMNIST (there was a typo in the case of FashionMNIST in the previous version, which has $6\times 10^4$ samples).}

%\bibliography{bibliography}
%\bibliographystyle{plain}
\end{document}


