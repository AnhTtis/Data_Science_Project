\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{enumitem}   
\usepackage{amssymb,amsmath,amsthm}
\usepackage{nicematrix}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{accents}
\usepackage{xspace}
%\usepackage{eufrak}
\usepackage{dsfont}
\usepackage{hyperref}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\newcommand{\LJ}[1]{{\color{blue} \bf \scriptsize [LJ: #1]\par}}
\newcommand{\JT}[1]{{\color{magenta} \bf \scriptsize [JT: #1]\par}}

\usepackage{ulem}
\normalem
\newcommand{\edt}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{red} \sout{#1}}}

\usepackage{cleveref}

\crefname{equation}{}{}

\title{Learning to reconstruct signals\\ from binary measurements alone}
\author{Juli√°n Tachella$^{1}$ and Laurent Jacques$^{2}$}


\date{%
    $^1$Laboratoire de Physique, CNRS, ENS Lyon, Lyon, France\\%
    $^2$ICTEAM, UCLouvain, Louvain-la-Neuve, Belgium\\[2ex]%
    \today
}

\newcommand{\signalset}{\mathcal{X}}
\DeclareMathOperator*{\signc}{sign}
\DeclareMathOperator*{\diam}{diam}
\newcommand{\sign}[1]{\signc\left(#1\right)}
\newcommand{\baseset}{\mathcal{X}_0}
\newcommand{\ntransf}{G}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\deltopt}{\delta_{\textrm{opt}}}
\newcommand{\covering}[1]{\mathcal{N}_{\signalset}(#1)}
\newcommand{\expectation}[1]{\mathbb{E}\{#1\}}
\newcommand{\order}[1]{\mathcal{O}(#1)}
\newcommand{\red}[1]{\textcolor{black}{#1}}
\newcommand{\corr}[1]{\textcolor{black}{#1}}
\newcommand{\R}[1]{\mathbb{R}^{#1}}
\newcommand{\B}[1]{\mathcal{B}^{#1}}
\newcommand{\ball}[1]{B_{\epsilon}{(#1)}}
\newcommand{\Sp}[1]{\mathbb{S}^{#1}}
\newcommand{\C}[1]{\mathbb{C}^{#1}}
\newcommand{\vect}[1]{\text{vec}(#1)}
\newcommand{\rangeA}{\mathcal{R}_A}
\newcommand{\rangeAg}{\mathcal{R}_{A_g}}
\newcommand{\nullA}{\mathcal{N}_A}
\newcommand{\concatA}{\bar{A}}
\newcommand{\proposed}{SSBM}
\newcommand{\gwidth}[1]{w_{#1}}
\newcommand{\rk}[1]{\text{rank}(#1)}
\newcommand{\bdim}[1]{\operatorname{boxdim}\left(#1\right)}
\newcommand{\nullAg}{\mathcal{N}_{A_g}}
\newcommand{\Hilb}{\mathcal{H}}
\newcommand{\jacob}{J_{\hat{x}}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\boxdim}{boxdim}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\renewcommand{\theenumi}{\Alph{enumi}}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{proposition}[theorem]{Proposition}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\newtheorem{assumption}{Assumption}
%\date{July 2022}

%% LJ shortcuts
%% Useful shortcuts we often use
\newcommand{\bs}{\boldsymbol}
\newcommand{\bb}{\mathbb}
\newcommand{\cl}{\mathcal}
\newcommand{\ts}{\textstyle}
\newcommand{\ds}{\displaystyle}
\newcommand{\ie}{\emph{i.e.}, }
\newcommand{\eg}{\emph{e.g.}, }
\newcommand{\iid}{%
  \ifmmode% math mode
  \mathrm{i.i.d.}%
  \else%
  i.i.d.\@\xspace%
  \fi%
}

\begin{document}
\maketitle



\begin{abstract}
    Recent advances in unsupervised learning have highlighted the possibility of learning to reconstruct signals from noisy and incomplete linear measurements alone. These methods play a key role in medical and scientific imaging and sensing, where ground truth data is often scarce or difficult to obtain. However, in practice measurements are not only noisy and incomplete but also quantized. 
    Here we explore the extreme case of learning from binary observations and provide necessary and sufficient conditions on the number of measurements required for identifying a set of signals from incomplete binary data. Our results are complementary to existing bounds on signal recovery from binary measurements. Furthermore, we introduce a novel self-supervised learning approach\red{, which we name \proposed,} that only requires binary data for training. \red{We demonstrate in a series of experiments with real datasets that \proposed~performs} on par with supervised learning and outperforms sparse reconstruction methods with a fixed wavelet basis by a large margin.\blfootnote{The code associated with this paper is available at \url{https://github.com/tachella/ssbm}.} 
\end{abstract}




\section{Introduction}
%\LJ{General comments:
%\begin{itemize}
%    \item I'm wondering if it would not be worth introducing $\cl X_\delta$ in the Introduction in order to better explain what we mean by approximate signal set identification.
%    \item I guess we should still improve our explanations on how knowing/estimating a signal set is connected to "learning to reconstruct", \ie to estimate $f_\theta$. After a new reading of the whole paper, I still find this unclear, and I guess reviewers could think the same. I discuss a possible storytelling in the beginning of Sec. 4.
%    \item I've changed a bit the formulation of our contributions in the end of the Introduction to make them closer to what we really show (at least to my understanding)
%    \item I have renamed the matrix $M$ stacking all measurement matrices $A_1, ..., A_G$ into $\concatA$.
%    \item I have an idea to simplify a bit the notations: writing $B(x)$ for \emph{b}inary mapping $\sign{A x}$, $B_g(x)$ for $\sign{A_g x}$, and $\bar B(x)$ for the one of $\sign{\concatA(x)}$. I didn't implement it. Note that $\hat{\cl X} = \bigcap_g [B_g^{-1} \circ B_g (\cl X)]$.   
%    \end{itemize}}
Continuous signals have to be quantized in order to be represented digitally with a limited number of bits in a computer.
In many real-world applications, such as radar~\cite{alberti1991time}, wireless sensor networks~\cite{chen2015amplitude}, and recommender systems~\cite{davenport2014onebit}, the measured data is quantized with just a few bits per observation. The extreme case of quantization corresponds to observing a single bit per measurement.
For example, single-photon detectors record the presence or absence of photons at each measurement cycle~\cite{kirmani2014first}, and recommendation systems often observe a binary measurement of users' preferences only (\eg via thumbs up or down).

The binary sensing problem is formalized as follows: we observe binary measurements $y\in \{-1,1\}^{m}$ of a signal $x\in \signalset \subset \Sp{n-1}$ with unit norm\footnote{Note that the sensing model in~\cref{eq: onebit model} provides no information about the norm of $x$, so it is commonly assumed that signals verify $\|x\|=1$. } via the following forward model
\begin{equation} \label{eq: onebit model}
    y = \sign{A x}
\end{equation}
where $A\in \R{m\times n}$ is a linear forward operator.
Recovering the signal from the measurements is an ill-posed inverse problem since there are many signals $x\in \Sp{n-1}$ that are consistent with a given measurement vector $y$. Moreover, often the measurement matrix is incomplete $m<n$, \eg as in one-bit compressed sensing~\cite{jacques2013robust}, which makes the signal recovery problem even more challenging.

It is possible to obtain a good estimation of $x$ despite the binary quantization, if the set of plausible signals $\signalset$ is low-dimensional~\cite{bourrier2014fundamental}, \ie if it occupies a small portion of the ambient space $\Sp{n-1}$. A popular approach is to assume that $\signalset$ is a single linear subspace or a union of subspaces~\cite{jacques2013robust}, imposing sparsity over a known dictionary. For example, the well-known total variation regularization assumes that the gradients of the signal are sparse~\cite{rudin1992nonlinear}. However, in real-world settings, the set of signals $\signalset$ is generally unknown, and sparsity assumptions on an arbitrary dictionary yield a loose description of the true set $\signalset$, negatively impacting the quality of reconstructions obtained under this assumption. This limitation can be overcome by learning the reconstruction mapping $y\mapsto x$ (\eg with a deep neural network) directly from $N$ pairs of measurements and associated signals---\ie a supervised learning scenario with a labeled dataset $\{(y_i,x_i)\}_{i=1}^N$ with $N$ assumed sufficiently large. While this learning-based approach generally obtains state-of-the-art performance, it is often impractical since it can be very expensive or even impossible to obtain ground-truth signals $x_i$ for training. For example, recommender systems generally do not have access to high-resolution user ratings on all items for training.


 \begin{figure}[t]
\centering
\includegraphics[width=1\textwidth]{figures/onebit_schematic.pdf}
\caption{ We propose a method for learning to reconstruct binary measurement observations, using only the binary observations themselves for training. The learned reconstruction function can discover unseen patterns in the data (in this case the clothes of fashionMNIST - see the experiments in~\Cref{sec: experiments}),  
 \red{which cannot be recognized in the standard linear reconstructions (no learning). We also provide theoretical bounds which characterize how well we can expect to learn the set of signals from binary measurement data alone.}}
\label{fig:schematic} 
\end{figure}

In this paper, we investigate the \red{problems of identifying the signal set and learning reconstruction mapping} using a dataset of binary measurements only $\{y_i\}_{i=1}^N$. 
%In this setting, if the measurement process is (over)complete $m\geq n$ and no prior information is known about $\signalset$, learning is not required, as the optimal reconstruction mapping corresponds to standard consistency reconstruction algorithms, \eg as described in~\cite{goyal1998quantized}.
In this setting, if the measurement process is incomplete $m<n$, the matrix $A$ has a non-trivial nullspace and there is no information in the measurement data about the set of signals $\signalset$ in the nullspace~\cite{chen2021equivariant}. \red{As a consequence, there is not enough information for learning the reconstruction function either. For example, the trivial pseudo-inverse reconstruction $f(y)  = A^{\top}(AA^{\top})^{-1} y$ is perfectly consistent with the  binary measurements, \ie $\sign{Af(y)}=y$, but is generally far from being optimal~\cite{boufounos2015quantization}.}

Here we show that it is still possible to (approximately) \red{identify the signal set and learn to reconstruct the binary measurements,} %\LJ{we must be sure of our terminology here, and align it with the title and abstract. Moreover, learning to reconstruct is not recovering $\signalset$ per se. We should explain this.} 
if the measurement operator varies across observations, \ie
\begin{equation}
    y_i = \sign{A_{g_i} x_i}
\end{equation} 
 where each signal $x_i$ is observed via one out of $\ntransf$ operators $g_i \in \{1,\dots,\ntransf\}$, and $i=1,\dots,N$. 
 This sensing assumption holds in various practical applications, where signals are observed through different operators (\eg recommendation systems access ratings about a different set of items for each user) or through an operator which changes through time (\eg a sensor that changes its calibration). Moreover, this assumption is also valid for the case where we obtain binary measurements via a single operator $A$, but the set $\signalset$ is known to be invariant to a group of invertible transformations $\{T_g\}_{g=1}^{\ntransf}$, such as translations or rotations. The invariance of $\signalset$ provides access to measurements associated with a set of (implicit) operators $\{A_g = AT_g\}_{g=1}^{\ntransf}$, as we have that
\begin{equation}
    y = \sign{AT_g T_g^{-1}x} = \sign{AT_g x'} 
\end{equation}
with $x'=T_g^{-1}x \in \signalset$ for all $g=1,\dots,\ntransf$.
This observation has been exploited to perform fully unsupervised learning on various linear inverse problems, such as magnetic resonance imaging and computed tomography~\cite{chen2021equivariant,chen2021robust,tachella2022sensing}. 
 
 %The two fundamental problems of signal and model identification from binary observations can be summarised as follows: 
%\begin{description}
%    \item[Signal Recovery] What error should we expect when estimating $x$ from the measurements $y=\sign{A_gx}$ under the constraint that $x$ belongs to a low-complexity signal set $\signalset$?
%    \item[Model Identification] How well can we estimate the set of signals $\signalset$ from the  binary measurement sets $\{\sign{A_g\signalset}\}_{g=1}^{G}$?
%\end{description}

The problem of recovering a signal from  binary measurements under the assumption of a known signal set has been extensively studied in the literature~\cite{goyal1998quantized,jacques2013robust,oymak2015near}. These works provide practical bounds which characterize the recovery error as a function of the number of measurements $m$ for different classes of signal sets. \red{However, they assume that the signal set is known (or that there is enough ground-truth training data to approximate it), which is not often the case in real-world scenarios. Here we investigate the best approximation of the signal set that can be obtained from the binary observations. This approximation lets us understand how well we can learn the reconstruction function from binary data.
To the best of our knowledge, the model identification problem has not been yet addressed, and we aim to provide the first answers to this problem here.} The main contributions of this paper are:
\begin{itemize}
    \item We show that for any $G$ sensing matrices $A_1,\dots, A_{\ntransf}\in \R{m\times n}$ and any dataset size $N$, there exists a signal set whose identification error (precisely defined in \Cref{sec: model ident}) from binary measurements cannot decay faster than $\mathcal{O}(\frac{n}{m\ntransf})$ when $m$ increases.
% ###Backup of the sentence modified above###: \item We show that for any $G$ sensing matrices $A_1,\dots, A_{\ntransf}\in \R{m\times n}$ \red{and any dataset size $N$,} the signal set cannot be estimated from binary measurements up to a global identification error (precisely defined in \Cref{sec: model ident}) which decays faster than $\mathcal{O}(\frac{n}{m\ntransf})$. 

    
    %\LJ{How different this claim is from the linear case? From $G$ measurement vectors, we cannot recover a continuous set anyway from linear measurements. So how this contribution could be compared to the linear case?}
    \item We prove that, if each operator $A_g$, $g \in \{1,\dots,\ntransf\}$, has iid Gaussian entries (a standard construction in one-bit compressed sensing), it is possible to estimate a $k$-dimensional\footnote{The definition of dimension used in this paper is the upper box-counting dimension defined in~\Cref{sec: signal recovery}.} signal set up to a global error of $\mathcal{O}( \frac{k + n/\ntransf}{m} \log \frac{nm}{k+n/\ntransf} )$ with high probability. %, as long as the number of operators is sufficiently large, \ie $\ntransf\approx n$.
     \item We determine the \emph{sample complexity} of the related unsupervised learning problem, \ie we find that the number of distinct binary observations for obtaining the best possible approximation of a $k$-dimensional signal set $\cl X$ is $N = \mathcal{O}\left(\ntransf(\frac{m}{k})^{k}\right)$. 
    \item We introduce a \red{Self-Supervised learning loss for training reconstruction networks from Binary Measurement data alone (\proposed),}  and show experimentally that the learned reconstruction function outperforms classical binary iterative hard thresholding~\cite{jacques2013robust} and performs on par with fully supervised learning on various real datasets. %\LJ{This contribution, in addition to being experimental, offers an implicit way to learn the signal set. This should be clarified in the text as well as in certain notations and in the cost functions. I propose some solutions later for that.}
\end{itemize}
A summary of the model identification bounds presented in this paper is shown in~\Cref{tab: summary}.
\begin{table}[t] 
\centering
\scalebox{0.9}{
\begin{tabular}{|l|c|c|c|c|c|}
\hline
%Lower bound inters. cells& - & $2^{n}(\frac{em\ntransf}{n})^{m\ntransf}$ & $2^{3 S k \log(\frac{m \sqrt n}{S k})}$ &  $2^{n}(\frac{em\ntransf}{n})^{m\ntransf}$   &      -              \\ \hline
&&&\\[-3.5mm]
Assumption on $\signalset \subseteq \Sp{n-1}$ & None                                              & None            & $\boxdim < k$ \\[1mm] \hline
&&&\\[-3.5mm] 
Assumption on $A_g\in \R{m\times n}$, $g \in [G]$ & rank $[A_1^\top, \ldots, A_G^\top] < n$                                              & None            & Gaussian \\[1mm] \hline
&&&\\[-3.5mm]
Identification error bounds   & $\delta>1$   & $\delta\gtrsim\frac{n}{m\ntransf}$ & $\delta \lesssim \frac{k+n/\ntransf}{m} \log \frac{nm}{k+n/\ntransf}$     \\[1mm]  \hline
        \small \red{Section}& \small \red{\Cref{subsec: lower bound}}      & \small \red{\Cref{subsec: lower bound}}   &  \small \red{\Cref{subsec: sufficient cond}}                    \\ \hline
\end{tabular}}
\caption{
%\LJ{I've modified a bit this table. Somehow, \Cref{prop: necessary multA}, \Cref{prop:  lowerbound} and \Cref{theo: onebit} do not directly provide these bounds. They still need to be combined with the context around to get these bounds. We could have to rephrase these propositions to clarify this.}
Summary of the global model identification error $\delta$ bounds presented in this paper, as a function of the size of the signals $n$, the number of binary measurement operators $\ntransf$ with $m$ measurements and the dimension of the signal set $k$.}
\label{tab: summary}
\end{table}

%The paper is organized as follows: 

\subsection*{Related Work}


\paragraph{Unsupervised learning in inverse problems.}
Despite providing very competitive results, most deep learning-based solvers require a supervised learning scenario, \ie they need measurements and signal pairs $\{(y_i,x_i)\}$, a labeled dataset, in order to learn the reconstruction function $y\mapsto x$. A first step to overcome this limitation is due to Noise2Noise~\cite{lehtinen2018noise2noise}, where the authors show that it is possible to learn from only noisy data if two noisy realizations of the same signal $\{(x_i+n_i,x_i+{n}_i')\}$ are available for training. This approach has been extended to linear inverse problems with pairs of measurements $\{(A_{g_i}x_i+n_i,A_{g_i'}x_i+{n}_i')\}$ ~\cite{yaman2020self,liu2020rare}.
The equivariant imaging framework~\cite{chen2021equivariant,chen2021robust} shows that learning the reconstruction function from unpaired measurement data $\{Ax_i+n_i\}$ of a single incomplete linear operator $A$ is possible if the signal model is invariant to a group of transformations. This approach can also be adapted to the case where the signal model is not invariant, but measurements are obtained via many different operators $\{A_{g_i}x_i+n_i\}$~\cite{tachella2022unsupervised}.
Necessary and sufficient conditions for learning in these settings are presented in~\cite{tachella2022sensing}, however under the assumption of linear observations (no quantization). Here we extend these results to the non-linear binary sensing problem with \red{an unsupervised dataset with multiple operators $\{\sign{A_{g_i}x_i}\}$ and $g_i\in \{1,\dots,\ntransf\}$, or with a single operator and a group-invariant signal set $\{\sign{Ax_i}\}$.}

\paragraph{Quantized and one-bit sensing.}
Reconstructing signals from one-bit compressive measurements is a well-studied problem~\cite{goyal1998quantized,jacques2013robust,oymak2015near,baraniuk2017onebit}, both in the (over)complete case $m\geq n$~\cite{goyal1998quantized}, and in the incomplete setting $m<n$, either under the assumption that the signals are sparse~\cite{jacques2013robust}, or more generally, that the signal set has small Gaussian width~\cite{oymak2015near}. Some of these results are summarized in~\Cref{sec: signal recovery}.
The theoretical bounds presented in this paper complement those of signal recovery bounds from quantized data, as they characterize the fundamental limitations of model identification from binary measurement data.

%The fundamental limitation of failing to learn a signal model from incomplete measurement data goes back to blind compressed sensing~\cite{gleichman2011blind}, for the specific case of models exploiting sparsity on an orthogonal dictionary. In order to learn the dictionary from compressed observations, 
%Gleichman and Eldar~\cite{gleichman2011blind} imposed additional constraints on the dictionary, while some subsequent papers~\cite{silva2011blind,aghagolzadeh2015new} removed these assumptions by proposing to use multiple operators $A_g$ as studied here. This paper can be seen as a generalization of such results to a much wider class of signal models.


\paragraph{One-bit matrix completion and dictionary learning.} 
%\JT{do we want to mention this? The requirement of noise is slightly confusing with our narrative.}
Matrix completion consists of inferring missing entries of a data matrix $Y = [y_1,\dots,y_N]$, whose columns can be seen as partial observations of signals $x_i$, \ie $y_i = \sign{A_{g_i}x_i}$ where the operators $A_{g_i}$ select a random subset of $m$ entries of the signal $x_i$. In order to recover the missing entries, it is generally assumed that the signals $x_i$ (the columns of $X=[x_1,\dots,x_N]$) belong to a $k$-dimensional subspace with $k\ll n$. \cite{davenport2014onebit} solve this learning problem via convex programming, and present theoretical bounds for the reconstruction error.

\cite{zayyani2015dictionary} present an algorithm that learns a dictionary (\ie a union of $k$-dimensional subspaces) from binary data alone in the overcomplete regime $m>n$.
\cite{rencker2019sparse} presents a similar dictionary learning algorithm with convergence guarantees.
In this paper, we characterize the model identification error for the larger class of low-dimensional signal sets, which includes subspaces and the union of subspaces as special cases. \red{Moreover, we propose a self-supervised method that learns the reconstruction mapping directly, avoiding an explicit definition (\eg a dictionary) of the signal set.}
%\LJ{We should also add that, for the experimental part, we do not get an explicit regularizer adjusted by $\signalset$, but rather a self-supervised reconstruction algorithm that contains the information of both $\signalset$ and the sensing context.}



\section{Signal Recovery Preliminaries}\label{sec: signal recovery}

\begin{figure}[t]
\centering
\includegraphics[width=1\textwidth]{figures/example00.pdf}
\caption{Geometry of the 1-bit signal recovery problem with $m=5$ and $n=3$. \textbf{Left:} The binary sensing operator $\sign{A\cdot}$ defines a tessellation of the sphere into multiple \emph{consistency cells}, which are defined as all vectors $x\in \Sp{2}$ associated with the same binary code. The consistency cell associated with a given measurement $y$ is shown in green. Each red line is a great circle defined by all points of $\Sp{2}$ perpendicular to one row of $A$. 
\textbf{Middle:} If the signal set consists of all vectors in the sphere, \ie $\signalset = \Sp{2}$, the center of the cell is the optimal reconstruction $\hat{f}(y)$ (depicted with a blue cross) and the recovery error (denoted by $\delta$) is given by the radius of the cell. \textbf{Right:} If the signal set (depicted in black) occupies only a small subset of $\Sp{2}$, \ie it has a small box-counting dimension, the optimal reconstruction corresponds to the center of the intersection between the signal set and the consistency cell, and the resulting signal recovery error is smaller.}
\label{fig: recovery schematic} 
\end{figure}
We begin with some basic definitions related to the one-bit sensing problem. The diameter of a set is defined as $\diam(S) = \sup_{u,v\in S} \|u-v\|$, and the radius is defined as half the diameter. Each row $a_i\in \R{n}$ in the operator $A$ divides the unit sphere $\Sp{n-1}$ into two hemispheres, \ie  $\{x\in \Sp{n-1}|\; a_i^{\top}x \geq 0\}$ and $\{x\in \Sp{n-1}|\; a_i^{\top}x < 0\}$. Considering all rows, the operator $\sign{A\cdot}$ defines a \emph{tesselation} of $\Sp{n-1}$ into  \emph{consistency cells}, where each  cell is composed of all the signals that are associated with a binary code $y$, i.e. $\{ x\in \Sp{n-1} |\; \sign{Ax} = y\}$. The radius and number of consistency cells  play an important role in the analysis of signal recovery and model identification. 
\Cref{fig: recovery schematic} illustrates the geometry of the problem for $n=3$ and $m=5$.


The problem of recovering a signal from one-bit compressed measurements with a known signal set has been well studied~\cite{goyal1998quantized,jacques2013robust,oymak2015near,baraniuk2017onebit}. These works characterize the maximum estimation error across all signals obtained by an optimal reconstruction function $\hat{f}$, \ie
\begin{equation}
\delta = \max_{x\in\signalset} \; \|x- \hat{f}( \sign{Ax})\|
\end{equation}
 as a function of the number of measurements and complexity of the signal model.
From a geometric viewpoint (see~\Cref{fig: recovery schematic}), the optimal reconstruction function with respect to the norm $\|\cdot \|$ is given by the centroid (with respect to the same norm $\|\cdot \|$) of the intersection between the consistency cell associated with the measurement $y=y(x)=\sign{Ax}$, \ie $S_y:= \{u \in \bb S^{n-1}: y=\sign{Au}\}$, and the signal set $\signalset$, i.e., 
\begin{equation} \label{eq: oracle f}
    \hat{f}(y) = \text{centroid}(S_{y} \cap \signalset).
\end{equation}
while the maximum reconstruction error is given by the intersection with maximal radius, that is
\begin{equation}
   \delta = \max_{x\in\signalset} \; \text{radius}(S_{y(x)} \cap \signalset).
\end{equation}
In the overcomplete case $m>n$, assuming that all unit vectors are plausible signals, \ie $\signalset=\Sp{n-1}$, the mean reconstruction error \corr{$\delta$ is given by the consistency cell with maximal radius, which scales as $\frac{n}{m}$ (see~\Cref{prop:  lowerbound}). The optimal rate is achieved by} measurement consistent reconstruction functions, \ie those verifying $y=\sign{Af(y)}$~\cite{goyal1998quantized}.

In the incomplete case $m<n$, non-trivial signal recovery is only possible if the set of signals occupies a low-dimensional subset of the unit sphere $\Sp{n-1}$~\cite{oymak2015near}. For example, a common assumption is that $\signalset$ is the set of $k$-sparse vectors~\cite{jacques2013robust}.
In this paper, we characterize the class of low-dimensional sets using a single intuitive descriptor, the box-counting dimension.
The upper box-counting dimension~\cite[Chapter~2]{falconer2004fractal} is defined for a compact subset $S\subset\R{n}$ as
\begin{equation}
   \bdim{S} = \lim \sup_{\epsilon\to0^{+}}  \frac{\log \mathfrak{N}(S,\epsilon)}{\log 1/\epsilon}
\end{equation}
where $\mathfrak{N}(S,\epsilon)$ is the minimum number of closed balls of radius $\epsilon$ with respect to the norm $\|\cdot\|$ that are required to cover $S$.
This descriptor has been widely adopted in the inverse problems literature~\cite{puy2017recipes,tachella2022sensing}, and it captures the complexity of various popular models, such as smooth manifolds~\cite{baraniuk2009random} and union of subspaces~\cite{blumensath2009uos,baraniuk2017onebit}. For example, the set of $(k+1)$-sparse vectors with unit norm has a box-counting dimension equal to $k$.
The upper box-counting dimension is particularly useful to obtain an upper bound on the covering number of a set: if $\bdim{\signalset}<k$, there exists a set-dependent constant $\epsilon_{0} \in (0,\frac{1}{2})$ for which
\begin{equation}
 \mathfrak{N}(\signalset,\epsilon)\leq \epsilon^{-k}
\end{equation}
holds for all $\epsilon\leq \epsilon_0$~\cite{puy2017recipes}. The following theorem exploits this fact to provide a bound on the number of measurements needed for recovering a signal with an error smaller than $\delta$ from generic binary observations:

\begin{theorem}\label{theo: signal recov boxdim}
Let $A$ be a matrix with iid entries sampled from a standard Gaussian distribution and assume that $\bdim{\signalset}<k$, \red{such that $ \mathfrak{N}(\signalset,\epsilon)\leq \epsilon^{-k}$ for all $\epsilon<\epsilon_0$ with $\epsilon_0\in (0,\frac{1}{2})$.} If the number of measurements verifies
\begin{equation}
m \geq  \tfrac{2}{\delta} \big(2k\log\tfrac{4\sqrt{n}}{\delta} + \log\tfrac{1}{\xi} \big)
\end{equation}
then for all $x,s\in \signalset$ and \red{$\delta\leq \min \{ 4\sqrt{n}\epsilon_0,\frac{1}{2}\}$}, we have that 
\begin{equation}
   \sign{Ax} \neq \sign{As} \implies \|x-s\| < \delta 
\end{equation}
with probability greater than $1-\xi$.
\end{theorem}
\noindent This result extends Theorem 2 in~\cite{jacques2013robust}, which holds for $k$-sparse sets only, to general low-dimensional sets and is included in \Cref{app: signal recovery}. 
For example, if $\signalset$ is the intersection of $L$ $(s+1)$-dimensional subspaces with the unit sphere, %$N(\signalset,\delta)\leq L(\delta/3)^{-k}$ and 
\Cref{theo: signal recov boxdim} holds with constant $\epsilon_0 = (3^{s}L)^{-\frac{1}{k-s}}$ and $k>s$~\cite[Chapter~4.2]{vershynin2018high}. %\LJ{reference?}
This theorem tells us that we can recover sparse signals from binary measurements up to an error of $$\mathcal{O}(\tfrac{k}{m}\log \tfrac{nm}{k})$$ which is sharp, up to the logarithmic factor~\cite{jacques2013robust}. Oymak and Recht~\cite{oymak2015near} present a similar result, stated in terms of the Gaussian width\footnote{The Gaussian width of a set $S$ is defined as $\mathbb{E}_s\{ \sup_{x\in S} x^{\top}s \}$ where $s$ is distributed as a standard Gaussian vector.} of the signal set instead of the box-counting dimension.
%\JT{Do we want to include our own general recovery theorem here? I think we should get an error of order $\mathcal{O}\left(\frac{k+kc_{\signalset}}{m}\log\frac{m\sqrt{n}}{k}\right)$}
 \begin{figure}[t]
\centering
\includegraphics[width=1\textwidth]{figures/onebit_example2.png}
\caption{%\LJ{Note that $\hat{\signalset}_i$ can be non-convex, in the same way than $\signalset_{\rm oracle}$ is in \Cref{fig:oracle}}
Illustration of the model identification problem from binary measurements with $n=3$, $m=4$, and $G=3$. A signal set with box-counting dimension 1 is depicted in black. The red lines define the frontiers of the consistency cells associated with operators $A_1,\dots, A_3$. \textbf{From left to right:} The signal set, the estimation of the signal set associated with $A_1,\dots,A_3$ and the overall estimate $\hat{\signalset}$.} %, \ie each red line is a great circle defined by all points of $\Sp{2}$ perpendicular to one row of a given operator.} %\LJ{As said on page 1, this is why I had to define the concept of "cell"; it's obvious for us, not necessarily for the reader}
\label{fig:illustration} 
\end{figure}

\section{Model Identification from Binary Observations} \label{sec: model ident}

% \LJ{We should cover in the text the following points, to better stress the nuance between the theoretical analysis below and what sets we can actually access in practice. 
% \begin{itemize}
%     \item Our guarantees in \Cref{sec: model ident} are equivalent to an infinite sampling of $\signalset$; \ie in practice we'll never have access to each $\hat{\signalset}_g$, but only to $\hat{\signalset}_{g,y_g} := \{v : \sign{A_gv} = y_g\} \subset \hat{\signalset}_g := \{v : \exists x \in \signalset: \sign{A_g v} = \sign{A_g x}\}$. This infinite sampling of $\signalset$ is fine, at least if we target lower bounds, the argument being then that, even in this infinite sampling context, we do still get lower bounds on the identification error.
%     \item Saying the item above differently, we must clarify that, when we say we ``observe" all elements $x$ of $\signalset$, we actually observe (and know) $\sign{B \signalset}$ (with $B$ being either $A_g$ or $M$), \ie the range of $\sign{B\cdot}$ over $\signalset$. This is the distinction that exist between the theoretical analysis of \Cref{sec: model ident}, and what we can do in practice (from \Cref{sec: experiments}), \ie observing an (unknown) sampling of $\signalset$. Remark that a reviewer could argue for our MNIST analysis that we rely on a $\boxdim \simeq 12$, while MNIST is (hopefyll) a sampled version of an unknown $\signalset$; I do not know if our analysis is so far 100\% clear on the connection between continuous sets and their sampling.   
%     \item However, in practice, we do not observe an infinity of mesurement vectors, but only $G$ such vectors, each of size $m$ (and thus a total of $mG$ bits). This means that we only observe, with these binary measurements, $G$ (unknown) signals of $\signalset$ (possibly not all distinct). Therefore, in practice, we are bound to recover a sampling of $\signalset$, \ie $\signalset_{\rm s} \subset \signalset$, and not $\signalset$ itself. 
%     \item Somehow, our final (experimental) objective is to perform an implicit identification of $\signalset$ from both an (unknown) sampling $\signalset_{\rm s}$ and a reconstruction method.
% \end{itemize}
% }


In this section, we study how well we can identify the signal set from binary measurement data associated with $\ntransf$ different measurement operators $A_1,\dots,A_{\ntransf}\in \R{m\times n}$. We focus on the problem of identifying the set $\signalset$ from the binary sets $\{\sign{A_g\signalset}\}_{g=1}^{\ntransf}$. In practice, we observe a subset of each binary set $\sign{A_g\signalset}$, \corr{however, in \Cref{subsec: complexity} we show that} the number of elements in each of these sets is controlled by the box-counting dimension of $\signalset$, which is typically low in real-world settings~\cite{hein2005intrinsic}.

We start by analyzing how the different operators provide us with information about $\signalset$. 
Each forward operator $A_g$ constrains the signal space by the following  set \begin{equation} \label{eq: constraint Ag}
    \hat{\signalset}_g = \{v \in \Sp{n-1} | \; \exists x_g\in \signalset, \;  \sign{A_gv} = \sign{A_g x_g}\}.
\end{equation}
Each set $\hat{\signalset}_g$ is thus composed of all unit vectors $v$ that are \emph{consistent} with at least one point $x_g$ of $\signalset$ according to the binary mapping $\sign{A_g\cdot}$. %, \ie $\sign{A_gv} = \sign{A_g x_g} =: y_g$. 
%Said differently, defining the concistency cell $\hat{\cl X}_{y_g,g} \subset \bb S^{n-1}$ of a given point $x \in \signalset$ as the set of all unit vectors $v$ \emph{consistent} with $x$ according to $\sign{A_g\cdot}$, then $\hat{\signalset}_g$ is the union $\cup_{x \in \signalset} \,\hat{\cl X}_{y_g,g}$ of all consistency cells defined over all points of $\signalset$. 
We thus conclude that $\hat{\signalset}_g$ is essentially a \emph{dilation} of $\signalset$---and we clearly have $\signalset \subset \hat{\signalset}_g$---whose extension is locally determined by specific cells of $\sign{A_g\cdot}$.  A three-dimensional example with $m=4$ measurements and $\ntransf=3$ operators is presented in~\Cref{fig:illustration}. Note that, for a given binary mapping $\sign{A_g\cdot}$, each cell is characterized by one binary vector in the range of this mapping, so that, as shown in this figure, all cells provide a different tesselation of $\Sp{n-1}$ whose size and dimension will play an important role in our analysis. %\LJ{The purpose of this new paragraph is to introduce the concepts of consistency and cells, which are helpful later and for \Cref{fig:illustration}.}

Since each $\hat{\signalset}_g$ is a dilation of $\signalset$, we can infer the signal set from the following intersection
\begin{equation}
    \hat{\signalset} := \bigcap_{g=1}^{\ntransf} \hat{\signalset}_g, 
\end{equation}
which can be expressed concisely as
\begin{equation} \label{eq:inferred set}
  \hat{\signalset} =  \left\{ v\in \Sp{n-1} |  \; \exists x_1,\dots,x_{\ntransf}\in \signalset, \; \;   \sign{A_gv} = \sign{A_g x_g}, \; \forall g =1,\dots, \ntransf  \right\}.
\end{equation}

Due to the binary quantization, the inferred set will be larger than the true set, \ie $\signalset \subset \hat{\signalset}$. However, we will show that it is possible to learn a slightly \emph{larger} signal set, defined in terms of a global identification error $\delta>0$, \ie the open $\delta$-\emph{tube}
\begin{equation}
 \signalset_\delta = \{ v \in \Sp{n-1} |  \;\| x- v \| < \delta, \; x\in\signalset \}
\end{equation}
such that the inferred set is contained in it, \ie $\hat{\signalset} \subset \signalset_\delta$. %\LJ{I guess you need this tube to be open for \Cref{prop: rotation} below, right?}.
For our developments to be valid, we will further assume that $\signalset$ is not too dense over $\bb S^{n-1}$ so that two tubes of $\signalset$ with two distinct radii are distinct.
\begin{assumption}
\label{ass:not-too-dense-X}
\red{The set $\signalset$ is closed} and there exists a maximal radius $0< \delta_0 < 2$ for which $\signalset_{\delta} \subsetneq \signalset_{\delta_0}$ for any $0<\delta<\delta_0$.
\end{assumption}
This assumption amounts to saying that there exists at least one open ball \corr{in $\bb S^{n-1}$} that does not belong to $\cl X \subset \bb S^{n-1}$. For instance, $\signalset = \bb S^{n-1}$ does not verify this assumption, and $\signalset = \bb S^{n-1} \cap \{x \in \bb R^n: x_1 \geq 0\}$ verifies it for $\delta_0 \leq \sqrt{2}$ since $\signalset_{\delta} = \bb S^{n-1}$ for any $\delta \geq \sqrt{2}$. 
 The next subsections provide lower and upper bounds for $\delta$. %with high probability with respect to a random sample of a set of operators $A_1,\dots, A_{\ntransf}$. 

\subsection{A Lower Bound on the Identification Error} \label{subsec: lower bound}

We first aim to find a lower bound on the best $\delta$ achievable via the following oracle argument: if we had oracle access to $\ntransf$ measurements of each point $x$ in $\signalset$ through each of the $\ntransf$ different operators, we could stack them together to obtain a larger measurement operator, defined as 
\begin{equation} \label{eq: oracle}
    \begin{bmatrix}
    y_1 \\
    \vdots \\
    y_{\ntransf}
    \end{bmatrix} = \sign{\concatA x}  \text{  with  }
    \concatA  =  \begin{bmatrix}
    A_1 \\
    \vdots \\
    A_{\ntransf}
    \end{bmatrix} \in \R{m\ntransf\times n}.
\end{equation}
This oracle measurement operator  provides a  refined approximation of the signal set, specified as 
\begin{equation}
\label{eq: constraint oracle}
    \hat{\signalset}_{\text{oracle}} = \{v \in \Sp{n-1} | \; \exists x\in \signalset, \;  \sign{ \concatA v} = \sign{\concatA x}\},
\end{equation}
which is again a dilation of $\signalset$.
%What is the best approximation of $\signalset$ we can obtain from the oracle binary measurement set $\sign{\concatA \signalset}$? 

\begin{figure}[t]
\centering
\includegraphics[width=.7\textwidth]{figures/onebit_oracle.png}
\caption{Illustration of the oracle argument in the example of~\Cref{fig:illustration}. \textbf{Left:} The signal set $\signalset\subset \Sp{2}$ is depicted in black. \textbf{Middle:} Cells intersected by the oracle system are indicated in green.  \textbf{Right:} The identified set $\hat{\signalset}$ is indicated in green, and \emph{is  larger} than the oracle counterpart.}
\label{fig:oracle} 
\end{figure}

\Cref{fig:oracle} shows an example with the oracle set $\hat{\signalset}_{\text{oracle}}$, which provides a better (or equal) approximation of the signal set than~\cref{eq:inferred set}, due to the fact that $\signalset\subset \hat{\signalset}_{\text{oracle}}\subseteq \hat{\signalset}$ by the construction of these sets.
 As the oracle estimate is composed of the cells associated with $\sign{\concatA \cdot}$ which are intersected by the signal set, the oracle approximation error depends on the diameter of the intersected cells. Given a certain oracle tesselation of $\Sp{n-1}$, the worst estimate of $\signalset$ is obtained when it intersects the largest cells in the tessellation.  The following proposition formalizes the intuition that the maximum consistency cell diameter---\ie the greatest distance separating two binary consistent vectors of $\signalset$ according to $\concatA $---serves as a lower bound on the model identification error $\delta$.

%\LJ{For \Cref{prop: rotation}, the rational behind the rotation trick should be explained. Moreover, for the proof to work, we must impose $\signalset$ close. To better define $\delta_0$ below, I've rewritten the proposition below and defined \Cref{ass:not-too-dense-X} above.}

\begin{proposition}\label{prop: rotation}
Given $\concatA \in\R{ m\ntransf \times n}$, for any set $\cl X \subset \bb S^{n-1}$ respecting \Cref{ass:not-too-dense-X} with $0<\delta_0 <2$, there exists a rotation matrix $R\in SO(n)$ such that the rotated set 
    \begin{equation}
        \signalset' = \{v\in \Sp{n-1} | v = Rx,\; x\in \signalset \} = R \signalset
    \end{equation}
    verifies $\hat{\signalset}_\text{oracle}' \not \subset \signalset'_{\delta}$ for any $\delta<\min\{d,\delta_0\}$ where $0<d<2$ is the largest cell diameter of the tesselation induced by $\sign{\concatA \cdot}$.
\end{proposition}
\begin{proof}
    Given $\delta < \delta_0$, the proof consists in choosing an appropriate rotation matrix, such that we can find a point $v$ which belongs to the oracle estimate $\hat{\signalset}_\text{oracle}'$ of the rotated set $\signalset'$, but doesn't belong to the $\delta$-tube $\signalset_{\delta}'$ of this set.  
   From \Cref{ass:not-too-dense-X} and since the $\delta$-tube $\signalset_{\delta}$ is open, there exists $x\in\signalset$ and $v \not\in \signalset_{\delta}$ such that $\|x-v\|=\delta$ 
   %\LJ{This is where we need $\signalset$ close}. 
   Let $S$ denote the largest cell in the tesselation of $\Sp{n-1}$ induced by $\sign{\concatA \cdot}$, such that $d=\text{diam}(S)$. If $\delta<d$, we can always pick a rotation $R\in SO(n)$ such that both $x' = R x$ and $v' = R v$ belong to $S$.  As $x'\in S$, $\signalset'$ intersects $S$ and we have that $ S\subseteq \hat{\signalset}_\text{oracle}'$, and thus that $v'\in \hat{\signalset}_\text{oracle}'$.
\end{proof}


In the rest of this subsection, we focus on bounding the maximum cell diameter, as it is directly related to the model identification error through \Cref{prop: rotation}. We start with the following proposition which shows that, if the stacked matrix is rank-deficient, there exist cells having the maximum possible diameter.

\begin{proposition}%[Theorem 1 in~\cite{chen2021equivariant}] 
\label{prop: necessary multA}
Consider the tessellation defined by $\sign{\concatA \cdot}$   
with $\concatA \in\R{m\ntransf\times n}$. If \begin{equation}\label{eq: rank condition}
    \rk{\concatA } < n
\end{equation} 
there are cells in the tessellation with a diameter equal to $2$.
\end{proposition}
\begin{proof}
If $\concatA $ has a rank smaller than $n$, it has a non-trivial nullspace. Let $v\in \Sp{n-1}$ be a generator of the nullspace with unit norm.
Consider a cell associated with the code $\sign{\concatA x}$ for some $x\in\R{n}$ inside the range of $\concatA ^{\top}$. The points $\frac{x+v}{\|x+v\|},\frac{x-v}{\|x-v\|}\in \Sp{n-1}$ belong to this cell since they share the same code. As $\|x\pm v\| = \sqrt{\|v\|^2+\|x\|^2}$ due to orthogonality, the distance between these two points is 
\begin{equation}
   \frac{2\|v\|}{\sqrt{\|v\|^2+\|x\|^2}} =  \frac{2}{\sqrt{1+\|x\|^2}}
\end{equation}
which tends to $2$ as $\|x\|$ goes to zero, without modifying the cell code $\sign{\concatA x}$.
\end{proof}
\noindent This proposition tells us that $n/\ntransf$ measurements are necessary in order to obtain non-trivial cell diameters, and thus to obtain a non-trivial estimation of $\signalset$. It provides a practical necessary condition for model identification, \ie that we have at least
$$m > \frac{n}{\ntransf}$$
measurements per operator.
Moreover, in practice, it is possible to compute the  rank of the stacked matrix $\concatA $ via numerical approximations.
The following theorem provides a more refined characterization of the oracle error for $m\geq n/\ntransf$:
%\begin{theorem}
%\label{prop:  lowerbound}
%Let the entries of $A_1,\dots,A_{\ntransf} \in \R{m\times n}$ be sampled from a standard Gaussian distribution.
%The  signal set estimated from the measurement sets $\{\sign{A_g\signalset} \}_{g=1}^{\ntransf}$, as defined in \cref{eq:inferred set}, verifies $\hat{\signalset}\not\subset\signalset_{\delta}$ for any
%$$\delta  \leq  \min\{ \delta_0, \frac{\log (\frac{1}{1-\xi})}{2m\ntransf \sqrt{n\frac{\pi}{2}}} \}$$
% with probability at least $1-\xi$, where $\delta_0\leq 2$ is a constant such that $\signalset_{\delta} \neq \Sp{n-1}$ for any $\delta<\delta_0$.
%\end{theorem}

\begin{proposition}
\label{prop:  lowerbound}
Consider the tessellation defined by $\sign{\concatA \cdot}$   
with $\concatA \in\R{m\ntransf\times n}$. The largest cell in the tessellation 
has a diameter of size at least $\frac{2}{3}\frac{n}{m\ntransf}$.
\end{proposition}

\begin{proof}
According to~\cite[Theorem A.7]{thao1996lower}, the maximum number of cells $C_{\concatA }$ induced by a tessellation defined by $\sign{\concatA \cdot}$ 
with $\concatA \in\R{m\ntransf\times n}$ can be upper bounded as
$$
    C_{\concatA } \leq \binom{m\ntransf}{n}2^{n}.
    $$
As $\binom{m\ntransf}{n}\leq (\frac{e m\ntransf}{n})^{n}$, we have that $C_{\concatA }\leq (\frac{2e m\ntransf}{n})^{n}$. 
We can inscribe all cells into spherical caps $S_i$\footnote{A spherical cap of radius $r$ around a point $v\in\Sp{n-1}$ is defined as $\{x\in \Sp{n-1}| \; \|x-v\|<r\}$.} of radius $\delta/2$, where $\delta$ is the maximum cell diameter. 
As shown in~\cite[Lemma 2.3]{ball1997elementary}, a spherical cap of radius $\delta/2$ has measure bounded by $\sigma(S_i) \leq (\frac{\delta}{4})^{n-1}\sigma_{n-1}$ where $\sigma_{n-1}$ is the measure of $\Sp{n-1}$. %\JT{fix this}
Since the tessellation covers the unit sphere $\Sp{n-1}$, we have that $\Sp{n-1} \subseteq \cup_{i=1}^{C_{\concatA }} S_i$ and thus
$$
\ts  \sum_{i=1}^{C_{\concatA }} \sigma( S_i) \geq \sigma_{n-1}\ \Rightarrow\ (\frac{2e m\ntransf}{n})^{n}(\frac{\delta}{4})^{n}\sigma_{n-1} \geq \sigma_{n-1}\ \Rightarrow\ \delta \geq \frac{2}{3}\frac{n}{m\ntransf}.
$$
\end{proof}
\noindent 

As stated in the following corollary, \Cref{prop:  lowerbound} shows that the model identification error cannot decrease faster with the number of measurements and operators than $\mathcal{O}(\frac{n}{m\ntransf})$, since the largest cell in any oracle tesselation has a diameter of at least $\frac{2}{3}\frac{n}{m\ntransf}$.

\begin{corollary}
    \label{cor:lower-bound-identif-error}
    Given the $G$ operators $A_1, \ldots, A_G \in \bb R^{m \times n}$, and any set $\cl X \subset \bb S^{n-1}$ respecting \Cref{ass:not-too-dense-X} with $0<\delta_0<2$, for any $0<\delta<\min(\delta_0, \frac{2}{3} \frac{n}{mG})$, there exists a rotation $R$ such that the inferred signal set $\hat{\cl X}'$ of $\cl X' = R \cl X$ is not included in $\cl X'_\delta$, \ie $\hat{\cl X}' \not\subset \cl X'_\delta$.
\end{corollary}

\begin{proof}
If $\cl X \subset \bb S^{n-1}$ respects \Cref{ass:not-too-dense-X} with $0<\delta_0<2$, and $\hat{\cl X}_{\rm oracle}$ and $\hat{\cl X}$ are the oracle set associated with $\concatA$ and the inferred set of $\cl X'$, respectively, then, as derived previously, we know that $\cl X \subset \hat{\cl X}_{\rm oracle} \subset \hat{\cl X}$. According to \Cref{prop: rotation} and \Cref{prop:  lowerbound}, there exists a rotation $R$ such that 
$\hat{\cl X}'_{\rm oracle} = R\hat{\cl X}_{\rm oracle} \not\subset \cl X'_\delta = R\cl X_\delta$ for $0<\delta<\min(\delta_0, \frac{2}{3} \frac{n}{mG})$. Therefore, from the inclusion above, we thus see that there exists $\hat{\cl X}' = R\hat{\cl X} \not\subset \cl X'_\delta$.
\end{proof}


%The constant $\delta_0$ is handles degenerate cases where $\signalset$ is too big, such as $\signalset=\Sp{n-1}$ (and thus $\delta_0=0$), and the error is trivially low since $\hat{\signalset}=\signalset$. 

\subsection{A Sufficient Condition for Model Identification} \label{subsec: sufficient cond}

We now seek a sufficient condition on the number of measurements per operator that guarantees the identification of $\signalset$ up to a global error of $\delta$. As with the sufficient conditions ensuring signal recovery (see \Cref{sec: signal recovery}), we assume that $\signalset$ is low-dimensional to provide a bound that holds with high probability if the entries of the operators are sampled from a Gaussian distribution.
\begin{theorem} \label{theo: onebit}
Given the operators $A_1,\dots,A_{\ntransf} \in \R{m\times n}$ with entries \iid as a standard Gaussian distribution, a low-dimensional signal set $\cl X$, with $\bdim{\signalset}<k$, such that $\mathfrak{N}(\signalset,\epsilon)\leq \epsilon^{-k}$ for all $\epsilon<\epsilon_0$ with $\epsilon_0\in (0,\frac{1}{2})$, and some failure probability $0 < \xi < 1$, if the number of measurements per operator verifies
\begin{equation} \label{eq: m bound}
    \textstyle m \geq \frac{4}{\delta}\, \big[(k+\frac{n}{\ntransf}) \log \frac{5\sqrt{n}}{\delta} + \frac{1}{\ntransf} \log \frac{1}{\xi} + \frac{n}{\ntransf}\log 3\big] 
\end{equation}
for \red{$0<\delta\leq \min \{ 4\sqrt{n}\epsilon_0,\frac{1}{2}\}$}, then with probability at least $1-\xi$, we have that $\hat{\signalset}\subseteq \signalset_{\delta}$. %\JT{check $\delta_0$ in the proof.}
\end{theorem}


\noindent The proof is included in \Cref{app: model ident}. \Cref{theo: onebit} provides a bound on $\delta$, \ie how precisely we can characterise the signal set $\signalset$, which we can compare with the lower bound in~\Cref{prop:  lowerbound}. From~\cref{eq: m bound} we have that (see \Cref{app: model ident} for a detailed derivation),
\begin{equation} \label{eq: main bound}
 \textstyle \delta=\mathcal{O}( \frac{k+n/\ntransf}{m} \log \frac{nm}{k+n/\ntransf} )
\end{equation}
The bound in~\cref{eq: main bound} is consistent with existing model identification bounds in the linear setting~\cite{tachella2022sensing}, which require  $m>k+n/\ntransf$ measurements per operator for uniquely identifying the signal set.

\subsection{Learning to Reconstruct}

\corr{The best reconstruction function $\hat{f}$ that can be learned from binary measurements alone can be defined as a function of the identified set $\hat{\signalset}$, as defined in~\cref{eq: oracle f}
\begin{equation}\label{eq: unsup oracle}
   \hat{f}(y) = \text{centroid}(S_{y} \cap \hat{\signalset})
\end{equation}
for a binary input $y$ with associated consistency cell $S_{y}=\{v\in\Sp{n-1} | \; \sign{Av}=y\}$ and the error is given by the radius of $S_{y} \cap \hat{\signalset}$. Unfortunately, \Cref{theo: signal recov boxdim,theo: onebit} do not automatically translate into a bound on the optimal reconstruction error of the reconstruction function defined in \eqref{eq: unsup oracle}.  \Cref{theo: onebit} implies that the optimal unsupervised reconstruction $\hat{f}(\sign{Ax})$ is at most $\mathcal{O}( \frac{k+n/\ntransf}{m} \log \frac{nm}{k+n/\ntransf} )$ away from the signal set $\signalset$, but does not guarantee that it is close to $x$. Nonetheless, we conjecture that this rate holds with high probability if the operators follow a Gaussian distribution:
\begin{conjecture} \label{conj: unsup recov}
Given binary measurements from the operators $A_1,\dots, A_{\ntransf} \in \R{m\times n}$ with entries \iid from a standard Gaussian distribution, the optimal reconstruction function defined in \eqref{eq: unsup oracle}
has a maximal reconstruction error that is upper bounded as $\mathcal{O}(\frac{k+n/\ntransf}{m} \log \frac{nm}{k+n/\ntransf})$
with high probability.
\end{conjecture}
\Cref{conj: unsup recov} hypothesizes that the optimal unsupervised reconstruction function should obtain a similar performance than the supervised one, \ie $\mathcal{O}(\frac{k}{m} \log \frac{nm}{k})$ shown in \Cref{theo: signal recov boxdim}, if the number of operators is sufficiently large, i.e., $\ntransf > n/k$. In the experiments in \Cref{sec: experiments}, we provide empirical evidence that supports this hypothesis. }

% If we have enough operators $\ntransf > n/k$, the bound in~\Cref{theo: onebit} is $\mathcal{O}( \frac{k}{m} \log \frac{nm}{k})$
% which is similar to the one for signal recovery of~\Cref{theo: signal recov boxdim}. Thus, we can expect to learn the reconstruction function \LJ{Still not clear in the text how signal set identification is related to learning a reconstruction function. I guess we should start from the fact that signal set identification is in fact an unsupervised problem, since we don't know the $x_i$, but then an element of our explanation is I guess missing to make the transition with Sec.\ref{sec: signal recovery} smoother.} from binary measurements with an error that scales as $\mathcal{O}( \frac{k}{m} \log \frac{nm}{k})$.


%but it is worse than the lower bound in~\Cref{prop:  lowerbound}, which decays inversely proportional to the number of operators $\ntransf$. 



%\textcolor{red}{I wonder if this is real limitation or it just reflects that the proof is not tight. It should be possible to investigate this empirically by fixing $m$ and varying $\ntransf$.}

%\paragraph{Comparison to signal recovery bounds from one bit measurements} It is useful to compare this result with the signal recovery bounds in~\Cref{sec: signal recovery}. If we have a large number of operators, \ie $\ntransf > n/k $, our necessary condition for unsupervised learning scales similar than the signal recovery error $\mathcal{O}(\frac{k}{m})$, and thus we can expect to learn from measurement data alone to recover signals up to a reconstruction error which scales as $\mathcal{O}(\frac{k}{m})$. %, with $n$ instead of $k$ inside the logarithm.  

%\paragraph{Comparison to model identification bounds from linear measurements}
%Tachella et al.~\cite{tachella2022sensing} show that $m>k+\frac{n}{\ntransf}$ measurements per operator are sufficient for uniquely identifying the signal set from incomplete linear observations.


\subsection{Sample complexity}\label{subsec: complexity}

We end our theoretical analysis of the unsupervised learning problem by bounding its \emph{sample complexity}, \ie we bound the number $N$ of \emph{distinct} binary measurement vectors $\{y_i\}_{i=1}^N$ that must be acquired for obtaining the best approximation of the signal set $\cl X$ from binary data. %In this subsection, we aim to upper bound how many (different) measurement vectors we need to observe to fully characterize the set $\hat{\signalset}$ defined in the previous section.

Since we observe binary vectors $y\in \{ \pm 1\}^{m}$, there is a limited number of different binary observations.
%\footnote{Note that observing multiple times the same binary measurement provides information about the signal distribution, but it does not provide additional information about the support $\signalset$ which is the main focus of this paper. \LJ{I guess this sentence will be unclear for most readers}}.
We could naively expect to observe up to $2^{m}$ different vectors per measurement operator (\ie all possible binary codes with $m$ bits), requiring at most $N\leq \ntransf2^{m}$ samples to fully characterize the best approximation of the signal set $\hat{\signalset}$ defined in~\cref{eq:inferred set}.
Fortunately, as already exploited in the proof of \Cref{prop:  lowerbound}, this upper bound can be significantly reduced if the signal set has a low box-counting dimension, as not all cells in the tessellation will be intersected by the signal set (see~\cref{fig:illustration}). We can thus obtain a better upper bound by counting the number of intersected cells, denoted as $|\sign{A\signalset}|$. 
 
If $\signalset$ is the intersection of a single $k$-dimensional subspace with the unit sphere, \cite[Theorem A.7]{thao1996lower} tells us that, for any matrix $A\in\R{m\times n}$, there are $|\sign{A\signalset}|\leq 2^{k}\binom{m}{k}$ intersected cells. More generally, if $\signalset$ is a union of $L$ subspaces, we have $|\sign{A\signalset}|\leq L 2^{k}\binom{m}{k}$. Thus, using the fact that $\binom{m}{k}\leq \left(\frac{3m}{k}\right)^k$, from the $G$ measurement operators, we can observe up to 
\begin{equation} \label{eq: n uos}
\textstyle N \leq  \ntransf L (\frac{6 m}{k})^{k}
\end{equation}
different measurement vectors. However, this result only holds for a union of subspaces having each dimension $k$. The following theorem extends this result to more general low-dimensional sets with small upper box-counting dimension.

%\LJ{I have applied the change $S \to s$ below, as $S$ is usually used for sets.}
\begin{theorem}\label{thm:max-bits-gauss}
Let the entries of $A\in \R{m\times n}$ be sampled from a standard Gaussian distribution, and let $\signalset\subseteq \R{n}$ with $\bdim{\signalset}<k$ \red{such that $ \mathfrak{N}(\signalset,\epsilon)\leq \epsilon^{-k}$ for all $\epsilon<\min(\frac{32}{3} \frac{k}{m \sqrt n} \log(\frac{3 m \sqrt n}{32 k}),\epsilon_0)$}. The cardinality of the measurement set is bounded as 
$$
\textstyle |\sign{A \signalset}|  \leq \left(\frac{m \sqrt n}{k}\right)^{8k}
$$ 
with probability at least $1 - \frac{1024}{9m^2  n}$. % \LJ{could be 1/10 instead of 3/32 ;-)}
\end{theorem}
\noindent The proof is included in \Cref{app: sample complexity}. \red{This result depends on the ambient dimension $n$ due to the application of the technical~\Cref{lemma: lemma laurent} and can be suboptimal for some signal sets. For example, the bound in~\Cref{eq: n uos} avoid this dependency for the case where $\signalset$ is a union-of-subspaces. Moreover, the squared dependency of the probability in $m$  can be increased by loosening the bound on $N$ (see the proof for more details).}
%\LJ{Acknowledge the presence of $n$ due to the Lemma x, as well as the fact that the exponent $-2$ is arbitrary and could be increased.}

Using this result, if we arbitrarily set $m_0$ such that $\frac{1024}{9m_0^2  n} = 0.01$, we thus  obtain the following upper bound on the number of different binary measurement vectors:
$$
N \leq  \ntransf \left(\frac{m \sqrt n}{k}\right)^{8k}.
$$
which holds with probability exceeding $0.99$ for any $m \geq m_0$.
Similarly to \cref{eq: n uos}, this bound scales exponentially only in the model dimension $k$ but not in the number of measurements $m$ or operators $\ntransf$. In the setting of a single operator and a $k$-dimensional invariant signal set, we have the upper bound $
N \leq  \left(\frac{m \sqrt n}{k}\right)^{8 k}$.


%\textcolor{red}{I believe this difference is an artifact of the proof, and it could be removed if we adapt~\Cref{eq: lemma laurent} to vectors $x$ in a $k$-dimensional ball $B_{\epsilon_x}(\tilde{x})$ and vectors $v$ in an $n$-dimensional ball $B_{\epsilon_v}(\tilde{v})$, such that the bound reads $d(\tilde{x}_g,\tilde{v}) - \sqrt{k\frac{\pi}{2}}\epsilon_x-\sqrt{n\frac{\pi}{2}}\epsilon_v$. In this case, we should get something like $m=\mathcal{O}(k \log \frac{\sqrt{k}}{\delta} + \frac{n}{\ntransf}\log \frac{\sqrt{n}}{\delta})$. However, while I envisage an adaptation of the prove for linear subspaces (and possibly smooth manifolds), I don't see an easy extension for more general $k$-dimensional sets. Maybe we can use some of the results in~\cite{oymak2015near}?}

%\edt{In summary, we can gather our different bounds on the following axis.
%\begin{center}
%    \includegraphics[width=.5\textwidth]{bounds_axis.png}
%\end{center}
%}


%\section*{Learning with invariance}
%We now briefly consider the setting where we have a single operator $A$ but the signal set presents some symmetry to transformations. As observed in~\cite{chen2021equivariant,tachella2022sensing}, if the signal set is invariant to a certain group of transformations $T_1,\dots,T_{\ntransf}$, we can obtain \emph{implicit} access to $\ntransf$ operators 
%\begin{equation}
%    A_g = AT_g
%\end{equation}
% for group elements $g=1,\dots,\ntransf$. This setting is significantly harder to analyse due to the correlation between the operators. In order to adapt the proof of~\Cref{theo: onebit} to this case, we would need to extend the bound in~\Cref{eq: lemma laurent} to the more general case of
% \begin{align}
% \Prob \left[ \forall x_1\in \ball{\tilde{x}_{1}},\dots,x_{\ntransf} \in \ball{\tilde{x}_{\ntransf}},  \forall v\in \ball{\tilde{v}} \;| \;  \sign{\Phi_v a} \neq \sign{\Phi_x a}  \right] 
%\end{align}
%where $\Phi_x = [x_1,\dots,x_{\ntransf}]\in \R{n\times \ntransf}$, $\Phi_v = [T_1v,\dots,T_{\ntransf}v]\in \R{n\times \ntransf}$ and the probability is computed with respect to the random Gaussian vector $a$. We can potentially partition the space of $x_1,\dots,x_{\ntransf}$ and $v$ such that we have $\rk{\Phi_x}=r_1$ and $\rk{\Phi_v}=r_2$ for all the elements in the neighbourhood (similarly to the proof in~\cite{tachella2022sensing}), and then bound the probability as a function of $r_1$ and $r_2$.
 
 
% \section*{Dithering}
% Consider the following dither model
% \begin{align}
% \begin{cases}
%     \epsilon \sim \mathcal{U}(-\xi,\xi) \\
%     y = \xi \sign{\Phi x+\epsilon}
% \end{cases}
% \end{align}
% If $\|\Phi x\|_{\infty} \leq \xi$, the model is equivalent to 
% \begin{align}
%     y_i = \begin{cases} \xi &\text{ with probability } \frac{1}{2}+\frac{t}{2\xi} \\
%   -\xi &\text{ with probability } \frac{1}{2}-\frac{t}{2\xi}
%     \end{cases}
% \end{align}
% for $i=1,\dots,m$. 
% The measurement vector has expectation $\expectation{y}=Ax$ and a diagonal covariance $\expectation{(y-Ax)(y-Ax)^{\top}}=\text{diag}(1_m\xi^2 - |Ax|^2)$ where $|\cdot|^2$ denotes the elementwise squaring operation.
% If $\|\Phi x\|_{\infty} \ll \xi$, the covariance is dominated by $\xi^2$ and the dither approximately linearizes the observations, \ie $y \approx \mathcal{N}(Ax, I\xi)$.

% \begin{equation}
%   \mathbb{E}_y \left\{ y^{\top}f(y) + \frac{\xi}{2} (f^{[\xi]}(y)-f^{[-\xi]}(y)) \right\} = \mathbb{E}_{x,y} x^{\top}f(y) 
% \end{equation}

% \begin{align}
%     \expectation{y g(y)} &= \xi(\frac{t}{2\xi} + \frac{1}{2}) g(\xi) - \xi(\frac{1}{2}-\frac{t}{2\xi}) g(-\xi) \\
%     &= \frac{t}{2} \left(g(\xi) + g(-\xi)\right) + \frac{\xi}{2} \left(g(\xi)- g(-\xi) \right)
% \end{align}
% \begin{align}
%     \expectation{t g(y)} &= (\frac{t}{2\xi} + \frac{1}{2}) tg(\xi) + t (\frac{1}{2}-\frac{t}{2\xi}) g(-\xi) \\
%     &= \frac{t}{2} \left(g(\xi) + g(-\xi)\right) + \frac{t^2}{2\xi} \left(g(\xi) - g(-\xi)\right) \\
%     &=  \expectation{y g(y)} -\frac{1}{2} (\xi-\frac{t^2}{\xi}) \left(g(\xi)- g(-\xi) \right) \\
% \end{align}
% Using a first order Taylor approximation, we get $g(\xi)- g(-\xi) \approx  2\xi \frac{\delta g}{\delta y_i} $
% \begin{equation}
%     \expectation{t g(y)} \approx \expectation{y g(y)} -  (\xi^2-t^2)\frac{\delta g}{\delta y_i}
% \end{equation}


% \begin{align}
%     \mathbb{E}_{y} \{ t_i f_i(y)\} &=  \mathbb{E}_{y_{-i}} \mathbb{E}_{y_i|y_{-i}} \{ t_i g_i(y_i) \} \\
%      & \approx \mathbb{E}_{y_{-i}}  \left\{ \mathbb{E}_{y_i}{y_i g_i(y)} - (\xi^2-t_i^2) \frac{\delta g}{\delta y_i} \right\} \\
%       & \approx \mathbb{E}_{y}  \{ y_i f_i(y) - (\xi^2-t_i^2)\frac{\delta f_i}{\delta y_i}  \}
% \end{align}
% where $g_i: \{0,1\}\mapsto \R{}$ depicts the restriction of $f_i$ to the input $i$th input (fixing the rest of inputs $y_{-1}$).

% Putting all of this together 
% \begin{equation}
%     \mathbb{E}_{y}  \{ t^{\top} f(y)\} \approx \mathbb{E}_{y}  \left\{ y^{\top} f(y) - \sum_{i=1}^{m} (\xi^2-t_i^2)\frac{\delta f_i}{\delta y_i} \right\}
% \end{equation}

%\newpage
\section{Learning Algorithms}\label{sec: algorithms}

%\LJ{
%In this section, I believe we should smooth the connection with the previous theoretical sections. Here, the learning of (a sampling of) $\cl X$ is certainly implicit. I see some ways to improve the way we derive the cost function. First, we could define the following sets (the first being the consistency cell previously defined): 
%$$
%\hat{\cl X}_{y,g} := \{v \in \bb S^{n-1}: \sign{A_g v} = y\},\ \text{for any $y$ in the range of $\sign{A_g \cdot}$},
%$$
%and an empirical version of $\hat{\cl X}$ defined by
%$$
%\hat{\cl X}^{\rm MC} := \bigcap_{i=1}^G \hat{\cl X}_{y_i,g_i}\ \subset \hat{\cl X}.
%$$
%From these sets, we can see that 
%$$
%\sum_{i=1}^{N} \mathcal{L}_{\text{MC}}\left(y_i,A_{g_i}\hat{x}_{\theta,i}\right)
%$$
%is basically a way to measure how far the $G$ evaluation of the reconstruction model $f_\theta$ on $(y_i, A_{g_i})$ is from each $\hat{\cl X}_{y_i,g_i}$, which as such tells us how far the average model is from $\hat{\cl X}^{\rm MC}$.

%I guess it would be nice to try drawing such connections, to demonstrate the rationale of our cost function, as a way to reach empirical cost functions that mimic our theoretical considerations of the previous sections. 

%\medskip
%Regarding the second term of the cost function, it seems less connected to our theoretical analysis. We can somehow define the cross-operator functional set
%$$
%\hat{\cl F}^{\rm CC} := \bigcap_i\ \{f: v_i = f(\sign{A_s v_i}, A_s), v_i = f(y_i, A_{g_i}), \forall s \neq g\}.
%$$
%so that 
%$$
%\sum_{s\neq g_i}\| \hat{x}_{\theta,i}- f_{\theta}( \sign{A_s\hat{x}_{\theta,i}},A_s) \|_2^2
%$$
%somehow measures how far $f_\theta$ is from $\hat{\cl F}^{\rm CC}$. But this does not allow us to draw a connection with the previous sections and is maybe not worth investigating.
%}

%\LJ{Something is maybe missing in our argumentation to introduce this section, \ie about the link between our theoretical analysis and the SSBM cost.
%On possible "story" we can tell is as follows. In a supervized context, where $\cl X$ is assumed generated by a distribution $p(x)$ (and $\cl X = {\rm supp}(p)$), we would want to find $f_\theta$ by solving this 
%\begin{equation}
%\label{eq:ideal-supervised-cost}
%\argmin_\theta\ \bb E_{x} \sum_{s \in [G]} \|x - f_\theta (B_s(x), A_s)\|^2,
%\end{equation}
%which is also related to the oracle context where the same $x$ is observed by all operators.

%In this problem, it is I guess clearer to say that $\cl X$ is encoded in $f_\theta$ since each point of $\cl X$ is somehow an approximate fixed point of $f_\theta (B_s(x), A_s)$ for each $s$. 


 %Since we do not have access to $p$ (and we do not observe all $x \in \cl X$ either) and since we target an unsupervised learning scenario, we opt for a self-supervised approach where we use $f_\theta$ to "sample" an approximation of points in $\cl X$ "from what we have" (\ie $\{(y_i, A_{g_i})\}_{i=1}^N$)---we thus define somehow a resampling technique.  

%This means that, given the (dynamic) resampling $\cl X(\theta) = \{\hat{x}_{\theta,i} := f_\theta(y_i, A_{g_i})\}_{i=1}^N$, the problem \eqref{eq:ideal-supervised-cost} is replaced by  
%\begin{equation}
%    \label{eq:empirical-self-supervised-cost}
%\argmin_\theta \tfrac{1}{N} \sum_{x \in \cl X(\theta)} %\sum_{s \in [G]} \|x - f_\theta (B_s(x), A_s)\|^2,
%\end{equation}
%assuming the empirical distribution $p_\theta := \frac{1}{N} \sum_{x \in \cl X(\theta)} \delta(\cdot - x)$ close to $p$ (when $\theta$ is good enough). In the %same way that \eqref{eq:ideal-supervised-cost} drives %the fact that $\cl X$ is encoded in $f_\theta$, we %could somehow say that $\hat{\cl X}_{\rm oracle}$ is %approximately encoded in encoded in the $f_\theta$ %minimizing \eqref{eq:empirical-self-supervised-cost}. I %guess this last point is also a safer justification of %what means all the costs than the difficult association %we (I?) tried to do in \eqref{eq: not MC set} and %\eqref{eq: MC set}.

%To get the SSBM cost, we could somehow say that we %split $s$ above between the case where it is different %or not from the operator index $g(x)$ of $x \in \cl %X(\theta)$, with a weighting factor $\alpha$, and that %for $s=g(x)$ we rather promote MC, as done in signal %recovery algorithms from binary measurements.   

%Therefore, SSBM reads
%$$
%\argmin_\theta \tfrac{1}{N} \sum_i \cl L_{\rm MC}(y_i, %A_{g_i} f_\theta (y_i, A_{g_i})) + \alpha \tfrac{1}{N} %\sum_{x \in \cl X(\theta)} \sum_{s \in [G]: s \neq %g(x)} \|x - f_\theta (B_s(x), A_s)\|^2,
%$$
%where the second term is both self-supervised and driven by the oracle's point of view. 

%Does all this make sense?
%}


In this section, we present a novel algorithm for learning the reconstruction function $f:(y,A)\mapsto x$ from $N$ binary measurement vectors $\{(y_i, A_{g_i})\}_{i=1}^{N}$, \red{which is motivated by the analysis in~\Cref{sec: model ident}.}
We parameterize the reconstruction function using a deep neural network with parameters $\theta\in\R{p}$.
The learned function can take into account the knowledge about the forward operator by simply applying a linear inverse at the first layer, \ie $f_{\theta}(y,A)=\tilde{f}_{\theta}(A^{\top}y)$, or using more complex unrolled optimization architectures~\cite{monga2021algorithm}. 

In the case where we observe measurements associated with $\ntransf$ different forward operators, we propose the SSBM loss 
\begin{align} \label{eq:multop loss}
    \argmin_{\theta\in \R{p}} &\; \sum_{i=1}^{N} \Big[\, \mathcal{L}_{\text{MC}}\left(y_i,A_{g_i}\hat{x}_{\theta,i}\right)\ +\ \alpha \sum_{s\neq g_i}\| \hat{x}_{\theta,i}-  f_{\theta}( \sign{A_s\hat{x}_{\theta,i}},A_s) \|_2^2\,\Big], 
\end{align}
where $\hat{x}_{\theta,i}=f_{\theta}(y_i,A_{g_i})$, 
the cost $\mathcal{L}_{\text{MC}}\left(y_i,A_{g_i}\hat{x}_{\theta,i}\right) \geq 0$ enforces \emph{measurement consistency} (MC), \ie require that $y_i=\sign{A_{g_i}\hat{x}_{\theta,i}}$, and $\alpha\in \mathbb{R}_{+}$ is a hyperparameter controlling the trade-off between the two terms involved.  
In the setting where we have a single operator, we aim to learn a reconstruction function $f_{\theta}:y\mapsto x$ (we remove the dependence of $f_{\theta}$ on $A$ to simplify the notation) via the following self-supervised loss:
\begin{align} \label{eq:ei loss}
    \argmin_{\theta\in \R{p}} &\; \sum_{i=1}^{N} \Big[\, \mathcal{L}_{\text{MC}}\left(y_i,A\hat{x}_{\theta,i}\right)  +\alpha \sum_{g=1}^{\ntransf}\| T_g\hat{x}_{\theta,i}-  f_{\theta}( \sign{A T_g\hat{x}_{\theta,i}}) \|_2^2\,\Big], 
\end{align}
where $\hat{x}_{\theta,i}=f_{\theta}(y_i)$  and $\alpha\in \mathbb{R}_{+}$. %The first term enforces measurement consistency, whereas the second term enforces equivariance of the imaging system~\cite{chen2021equivariant}, \ie $T_g f_\theta(Ax)=f_\theta(A T_gx)$ for all transformation matrices $g=1,\dots,\ntransf$.
In practice, we minimize~\Cref{eq:multop loss} by mini-batching approaches (\eg stochastic gradient descent) by using \red{sampling one out of the $\ntransf$ operators} at random per batch.
In both cases, we choose the measurement consistency term to be the logistic loss, \ie
\begin{equation}\label{eq:logistic}
\mathcal{L}_{\text{MC}}\left(y,\hat{y}\right)= \log \left(1+\exp(-y \circ \hat{y})\right)
\end{equation}
which enforces sign-consistent predictions which are far from zero, as the logistic function tends asymptotically towards zero as $|\hat{y}|\to \infty$. An empirical analysis in~\Cref{sec: experiments} shows that the logistic loss obtains the best performance across various popular consistency losses.

\paragraph{Analysis of the proposed loss} We focus on the multi-operator loss in~\cref{eq:multop loss}, although a similar analysis also holds for the equivariant setting. As the first term of the loss enforces measurement consistency, \ie requires $y_i=\sign{A_{g_i}f_{\theta}(y_i,A_{g_i})}$ for every $y_i$ in the dataset. However, in the incomplete setting $m<n$, \corr{the simple pseudo-inverse solution 
\begin{equation} \label{eq: pseudo-inverse}
f(y,A_g) = A_g^{\dagger}y
\end{equation}
with $A_g^{\dagger} = A_g^{\top}(A_gA_g^{\top})^{-1}$,}
is measurement consistent for any number of operators $\ntransf$ and training data $N$. Therefore, the first loss does not prevent learning a function $f_{\theta}(y, A_g)$ which acts independently for each operator (as if there were $\ntransf$ independent learning problems). \corr{The second loss \emph{bootstraps} the current estimates $\hat{x}_{i,\theta}=f_{\theta}(y_i,A_{g_i})$ as new ground truth references, mimicking the supervised loss
\begin{equation}
    \label{eq:empirical-self-supervised-cost}
 \sum_{i=1}^N \sum_{s=1}^{\ntransf} \|\hat{x}_{i,\theta} - f_\theta (\sign{A_s \hat{x}_{i,\theta}}, A_s)\|^2,
\end{equation}
in order to enforce consistency across operators.
Importantly, this additional loss avoids the trivial pseudo-inverse solution in~\cref{eq: pseudo-inverse}, as 
\begin{equation}
    A_g^{\dagger}y - A_s^{\dagger}\sign{A_s A_g^{\dagger}y} \neq 0
\end{equation}
for $g\neq s$ if the nullspaces of $A_g$ and $A_s$ are different, \eg if the necessary condition in~\Cref{prop: necessary multA} is verified.}

%which enforces that $f_\theta(\sign{A_gx}, A_g)=f_\theta(\sign{A_sx}, A_s)$ for all $g,s\in \{1, \dots, \ntransf \}$. The cross-operator consistency error of the pseudo-inverse solution in~\cref{eq: pseudo-inverse} is not zero if the matrices $A_g$ and $A_s$ have different nullspaces, which is a necessary condition for learning according to \Cref{prop: necessary multA}.
 



%\LJ{I'm not sure of the following paragraphs. See my comments above for an alternative justification.}\red{We can also understand the proposed losses using the construction of $\hat{\signalset}$ developed in \Cref{sec: model ident}. Any function that verifies
%$f_{\theta}(y_i,A_{g_i}) \in \{ v\in \Sp{n-1} | \; \sign{A_{g_i}v} = y_i \}$ for all the observed measurement vectors $i=1,\dots,N$ minimizes the measurement consistency loss. For example, the simple pseudo-inverse solution in \Cref{eq: pseudo-inverse} verifies this constraint. Thus, only using the measurement consistency loss (\ie $\alpha=0$) can be associated with  the following estimate of the signal set:
%\begin{equation}\label{eq: MC set}
%    \bigcup_{i=1}^N \{ v\in \Sp{n-1} | \; \sign{A_{g_i}v} = y_i \}.
%\end{equation}
%which contains all consistent cells associated with the observed measurements.
%If we observe data associated with more than one operator, \ie  $\ntransf>1$, this set is larger than the optimal estimate $\hat{\signalset}$, even with an arbitrarily large $N$, since there is no consistency across cells of different operators. %\LJ{Note sure to see this}. 
%The consistency across operators improves the measurement consistent set in~\cref{eq: MC set}, and can be associated with the estimate
%\begin{equation}
%\label{eq: not MC set}
%   \bigcap_{s=1}^\ntransf \bigcup_{i | g_i=s} \{ x\in \Sp{n-1} | \; \sign{A_{s}x} = y_i \}
%\end{equation}
%which is equivalent to the optimal one $\hat{\signalset}$ studied in \Cref{sec: model ident} when $N$ is of order $\mathcal{O}((\frac{m}{k})^k)$ as indicated in \Cref{thm:max-bits-gauss}.}


 %
%\LJ{IMO, what we can certainly say is that, $\mathcal{L}_{\text{MC}}\left(y_i,A\hat{x}_{\theta,i}\right)$ is a distance between $\hat{x}_{\theta,i}=f_{\theta}(y_i)$ and the set $\{ x\in \Sp{n-1} | \; \sign{A_{g_i}x} = y_i \}$, this distance being 0 if $\hat{x}_{\theta,i}$ belongs to this set. So, the measurement consistency loss with $\alpha=0$ measures the average of all distances of $\hat{x}_{\theta,i}$ to their set $\{ x\in \Sp{n-1} | \; \sign{A_{g_i}x} = y_i \}$. However, I don't see clearly how (24) could arise from this concept; in particular, how a distance to a union of all cells could be deduced from this average of distances. 
%}
%From the point of view of the learned reconstruction function, 




\section{Experiments} \label{sec: experiments}

For all experiments, we use measurement operators with entries sampled from a standard Gaussian distribution and evaluate the performance of the algorithms using by computing the average peak-to-signal ratio (PSNR) on a test set with $N'$ ground-truth signals, that is:
\begin{equation}
\textstyle \frac{1}{N'} \sum_{i=1}^{N'} {\rm PSNR}\Big( x'_i, f_\theta(\sign{A_{g_i} x'_i}, A_{g_i})\Big),
\end{equation}
where the PSNR is computed after normalizing the reconstructed image such that it has the same norm as the reference image, \ie 
\begin{equation}
    {\rm PSNR}(x, \hat{x}) = - 20 \log \|x -\hat{x} \frac{\|x\|}{\|\hat{x}\|} \|.
\end{equation}
%\LJ{I guess we should briefly explain (here or after) how we will assess later the quality of the learned $\theta$. below you use the ``test PSNR" which I guess is the average PSNR over a test set. Therefore, we could explain that, while the reconstruction function $f_\theta$ encodes what we have learned from $\cl X$ given its sampling $\{x_i\}_{i=1}^N \subset \cl X$ and the posed inverse problem, we assess how well $f_\theta$ encodes $\cl X$ from a test set $\{x'_i\}_{i=1}^{N'} \subset \cl X$ and estimate
%$$
%\textstyle \frac{1}{N'} \sum_{i=1}^{N'} {\rm PSNR}\Big( x'_i, f_\theta(\sign{A_{g_i} x'_i}, A_{g_i})\Big),
%$$
%or something similar with any other metric than the PSNR.
%}
We choose $f_{\theta}(y,A)=\tilde{f}_{\theta}(A^{\top}y)$ where $\tilde{f}_{\theta}$ is the U-Net network introduced in~\cite{chen2021equivariant} with trainable weights $\theta$, and used the Adam optimizer for training with learning rate $.001$ and standard hyperparameters $\beta_1=0.9$ and $\beta_2=0.99$.

\subsection{MNIST experiments}

%\LJ{In this section we should be really clear on what is $\cl X$ in the case of MNIST. Simply saying that its (effective) boxdim is about 12 is not enough. There is again the question of observing a sampling of $\cl X$ instead of $\cl X$.}

We evaluate the theoretical bounds using the MNIST dataset, which consists of greyscale images with $n=784$ pixels and whose box-counting dimension is approximately $k\approx 12$~\cite{hein2005intrinsic}. We use $6 \times 10^{4}$ images for training and $10^{3}$ for testing. 

\paragraph{Multiple operators setting.}
We start by comparing the logistic consistency loss in~\cref{eq:logistic} with the following alternatives:
\begin{itemize}
    %\item 0-1 loss: $\mathcal{L}_{\text{MC}}\left(y,\hat{y}\right)=\| y- \sign{\hat{y}}\|^2$. This loss 
    \item Standard $\ell_p$-loss, $\mathcal{L}_{\text{MC}}\left(y,\hat{y}\right)=\| y- \hat{y} \|_p^p$. As this loss is zero only if $\hat{y}=y$, it promotes sign consistency, $\sign{\hat{y}}=y$ and unit outputs $|\hat{y}|=1$. 
    \item One-sided $\ell_p$-loss, $\mathcal{L}_{\text{MC}}\left(y,\hat{y}\right)=\| \max(-y \circ \hat{y},0) \|_p^p$ where $\circ$ denotes element-wise multiplication and the $\max$ operation is performed element-wise. This loss is zero as long as $\sign{\hat{y}}=y$ regardless of the value of $|\hat{y}|$. 
\end{itemize}
For each loss we chose the best performance across trade-off parameter $\alpha\in \{0.1,1,10\}$. \Cref{fig:losses} shows the different losses and the test performance for different values of measurements using $\ntransf=10$ operators. The logistic loss obtains the best performance across all sampling regimes, whereas the one-sided $\ell_2$ loss obtains the worst results.

\begin{figure}[t]
\centering
\includegraphics[width=.8\textwidth]{figures/all_losses.pdf}
\caption{Evaluated training losses for enforcing sign measurement consistency $\sign{A\hat{x}}=y$ of reconstructions $f_{\theta}(y)=\hat{x}$. \textbf{Left:} The loss functions are shown for the case $y=1$. \textbf{Right:} Average test PSNR %\LJ{This test PSNR should be explained, see my previous comment.}
of different measurement consistency losses on the MNIST dataset with $\ntransf=10$ operators.}
\label{fig:losses}
\end{figure}

Secondly, we compare the logistic loss with the following learning schemes:
\begin{itemize}
    \item Linear inverse (no learning), defined as $\hat{x}_i= A_{g_i}^{\top}y_i$. This reconstruction can fail to be measurement consistent~\cite{goyal1998quantized}.
    \item Standard supervised learning loss, defined as $\sum_{i=1}^{N} \| x_i- f_{\theta}(y_i,A_{g_i}) \|^2.$ We also evaluate this loss together with the cross-operator consistency term in~\cref{eq:multop loss} which we denote as supervised+.
    \item Measurement consistency loss, defined as $\sum_{i=1}^{N} \mathcal{L}_{\text{MC}}\left(y_i,A_{g_i}f_{\theta}(y_i,A_{g_i}) \right)$ using the logistic loss.
    \item 
    The binary iterative hard-thresholding (BIHT) reconstruction algorithm~\cite{jacques2013robust} with a Daubechies4 orthonormal wavelet basis. The step size and sparsity level of the algorithm were chosen via grid search. It is worth noting that the best-performing sparsity level increases as the number of measurements $m$ is increased.
    \item Proposed \proposed~loss in~\cref{eq:multop loss} using the logistic loss for measurement consistency.
\end{itemize}


\begin{figure}[t]
\centering
 \begin{subfigure}{0.5\textwidth}
     \centering     \includegraphics[width=1\textwidth]{figures/all_m_G10.pdf}
     %\caption{}
\label{fig: comparison sup} 
 \end{subfigure}
 \begin{subfigure}{0.43\textwidth}
\centering\includegraphics[width=1\textwidth]{figures/bounds_check.pdf}
     %\caption{}
 \end{subfigure}
\caption{\textbf{Left:} Average test PSNR of different supervised and unsupervised algorithms on the MNIST dataset with $\ntransf=10$ operators. \textbf{Right:} The performance of the SSBM method follows closely the bounds \corr{in~\Cref{conj: unsup recov}}. }
\label{fig: allmG10} 
\end{figure}

Test PSNR values obtained for the case of $\ntransf=10$ operators are shown in the left subfigure of~\Cref{fig: allmG10}, where the PSNR in dB is plotted against $m/n$ in log-scale representation. The measurement consistency approach obtains performance similar to simply applying a linear inverse for the incomplete $m/n<1$ setting, whereas it obtains a significant improvement over the linear inverse in the overcomplete case $m/n\geq 1$. This gap can be attributed to the lack of measurement consistency of the linear reconstruction algorithm~\cite{goyal1998quantized}. The proposed loss obtains a performance that is several dBs above the linear inverse and BIHT for all sampling regimes. BIHT relies on the wavelet sparsity prior, which does not capture well enough the MNIST digits.
\proposed~performs similarly to supervised learning as the sampling ratio tends to 1, and perhaps surprisingly, it obtains slightly better performance than supervised learning for $m/n=1.28$. However, adding the cross-operator consistency loss to the supervised method (\ie the method supervised+ in~\Cref{fig: comparison sup}) performs better for all sampling regimes than \proposed. 

The right plot in \Cref{fig: allmG10} compares the performance of the \proposed~with the \corr{bounds in~\Cref{conj: unsup recov}}. These bounds behave almost linearly in this log-log plot of both the error---through the PSNR---and the log-scale representation of $m/n$. We thus observe a good agreement between the \corr{predictions in~\Cref{conj: unsup recov}} and the performance in practice.

%\footnote{The supervised+ method necessarily performs better than the proposed self-supervised method since it access strictly more information during training.}.


\Cref{fig:allmG} shows the average test PSNR and reconstructed images  obtained by the proposed self-supervised method for different values of $\ntransf$ and $m$. The method fails to obtain good reconstructions when $\ntransf=1$, as the necessary condition in~\Cref{prop: necessary multA} is not fulfilled.

\begin{figure}[t]
\centering
 \begin{subfigure}{0.45\textwidth}
     \centering     \includegraphics[width=.7\textwidth]{figures/all_Gm.pdf}
     \caption{}
 \end{subfigure}
 \hspace{5mm}
 \begin{subfigure}{0.45\textwidth}
\centering\includegraphics[width=.7\textwidth]{figures/moi.pdf}
     \caption{}
 \end{subfigure}
\caption{(\textbf{a}) Average test PSNR and (\textbf{b}) reconstructed test images of the proposed unsupervised method for different numbers of operators $G$ and measurements $m$. }
\label{fig:allmG} 
\end{figure}



\paragraph{Equivariant setting using shifts.}
We evaluate the setting of learning with a single operator by using the unsupervised equivariant objective in~\cref{eq:ei loss} with 2D shifts as the group of transformations (as the MNIST dataset is approximately shift-invariant). \Cref{fig:allm} shows the average test PSNR %\LJ{to be explained, see above.}
and reconstructed images as a function of the measurements $m$ for various algorithms. The proposed unsupervised method significantly outperforms the linear inverse, BIHT, and the measurement consistent network in all sampling regimes, and performs closely to supervised learning for $m/n>0.4$.

\begin{figure}[t]
\centering
 \begin{subfigure}{0.52\textwidth}
     \centering
     \includegraphics[width=\textwidth]{figures/all_m.pdf}
     \caption{}
 \end{subfigure}
 \hspace{1mm}
 \begin{subfigure}{0.42\textwidth}
     \centering
     \includegraphics[width=1\textwidth]{figures/all_m_equivariance4.pdf}
     \caption{}
 \end{subfigure}
\caption{(\textbf{a})  Average test PSNR and (\textbf{b}) reconstructed test images by the compared algorithms with a single operator $A$ as a function of the undersampling ratio $m/n$. }
\label{fig:allm} 
\end{figure}

\subsection{Other Datasets}
In order to demonstrate the robustness of the proposed method across datasets, we evaluate the proposed unsupervised approach on the FashionMNIST~\cite{xiao2017online}, CelebA~\cite{liu2015faceattributes} and Flowers~\cite{nilsback2008automated} datasets. The FashionMNIST dataset consists of $6\; 10^{5}$ greyscale images with $28\times 28$ pixels which are divided across $\ntransf=10$ different forward operators. As with MNIST, we use $N=6\; 10^{4}$ per operator for training and $10^{4}$ per operator for testing. For the CelebA dataset, we use $\ntransf=10$ forward operators and choose a subset of $10^{4}$ images for each operator for training and another subset of the same amount for testing. 
The Flowers dataset consists of 6149 color images for training and 1020 images for testing, all associated with the same forward operator. For both CelebA and Flowers datasets, a center crop of $128\times 128$ pixels of each color image was used for training and testing. 
\Cref{tab: datasets} shows the average test PSNR of the proposed unsupervised method, standard supervised learning, BIHT, and the linear inverse. For BIHT, we use the Daubechies4 orthonormal wavelet basis and optimize the step size and sparsity level via grid search. % sparsity = 20 for fashionMNIST
% Flowers+CelebA sparsity 100 step size 20

The self-supervised method obtains an average test PSNR which is only 1 to 2 dB below the supervised approach. \Cref{fig:fashion,fig:celebA} show reconstructed test images by the evaluated approaches for each forward operator. The proposed unsupervised method is able to provide good estimates of the images, while only having access to highly incomplete binary information. The supervised method obtains sharper images, however at the cost of hallucinating details, whereas the proposed method obtains blurrier estimates with less hallucinated details.

\begin{table}[t]
\centering \label{tab: datasets}
\scalebox{0.8}{\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Dataset      & $n$   & $m$  & $\ntransf$ & Linear Inverse & BIHT & Supervised       & \proposed (ours)        \\ \hline
FashionMNIST & 784   & 300  & 10        & $6.38\pm 0.23$ & $10.68\pm 0.31$  & $17.63\pm 0.33$  & $16.47\pm 0.22$ \\ \hline
CelebA       & 49152 & 9830 & 10         & $4.81\pm 0.32$& $16.26\pm 0.40$  & $21.59\pm 0.31$ & $19.53\pm 0.3$  \\ \hline
Flowers     & 49152 & 9830 & shifts         & $5.31\pm 0.72$ & $14.62\pm 0.92$  & $18.26 \pm 0.75$ & $16.45\pm 0.71$  \\ \hline
\end{tabular}}
\caption{Average test PSNR in dB obtained by the compared methods for the FashionMNIST, CelebA and Flowers datasets.}
\end{table}


 \begin{figure}[t]
\centering
\includegraphics[width=.8\textwidth]{figures/OneBitFashion2.pdf}
\caption{Reconstructed test images using the FashionMNIST dataset. Each column corresponds to a different forward operator.}
\label{fig:fashion} 
\end{figure}


 \begin{figure}[t]
\centering
\includegraphics[width=.9\textwidth]{figures/OneBitCelebA2.pdf}
\caption{CelebA results. Reconstructed test images using the CelebA dataset. Each column corresponds to a different forward operator.}
\label{fig:celebA} 
\end{figure}

\section{Conclusions and Future Work}

The theoretical analysis in this work characterizes the best approximation of a low-dimensional set that can be obtained from binary measurements. The model identification bounds presented here apply to a large class of signal models, as they only rely on the box-counting dimension, and complement those existing for signal recovery from binary measurements~\cite{goyal1998quantized,jacques2013robust}. Moreover, the proposed self-supervised loss provides a practical algorithm for learning to reconstruct signals from binary measurements alone, which performs closely to fully supervised learning. This work paves the way for deploying machine learning algorithms in scientific and medical imaging applications with quantized observations, where no ground-truth references are available for training.

We leave the proof of \Cref{conj: unsup recov}, and a study of the effect of noise in the observations and related dithering techniques for future work. Another avenue of future research is the extension of~\Cref{theo: onebit} for the case of operators related through the action of a group.

\section*{Acknowledgments}

Part of this research was supported by the Fonds de la Recherche Scientifique ‚Äì FNRS under Grant T.0136.20 (Project Learn2Sense).

\appendix
\section{Technical Lemmas}

%\LJ{Appendix should be split in sections, one per thm/prop proof, and reference to them should be made from the text}
\corr{We begin by introducing some technical results that play an important role in the main theorems of the paper.} We start with a result from~\cite{jacques2013robust}.
\begin{lemma}[Lemma 9 in~\cite{jacques2013robust}]\label{lemma: lemma laurent}
Given $0\leq \epsilon < 1$ and two unit vectors $\tilde{x},\tilde{v} \in \Sp{n-1}\subset \R{n}$ and $a \in \bb R^n$ with $a_{i} \sim_{\iid} \cl N(0,1)$, we have 
\begin{align} \label{eq: disk lemma}
  p_0 &= \Prob \left[ \forall x \in \ball{\tilde{x}}, \forall  v\in \ball{\tilde{v}} \;| \;  \sign{a^{\top} v} = \sign{a^{\top} x}  \right] \geq 1 - d(\tilde{x},\tilde{v}) - \sqrt{n\frac{\pi}{2}}\epsilon \\
p_1 &= \Prob \left[ \forall x \in \ball{\tilde{x}},  \forall v\in \ball{\tilde{v}} \;| \;  \sign{a^{\top} v} \neq \sign{a^{\top} x}  \right] \geq  d(\tilde{x},\tilde{v}) - \sqrt{n\frac{\pi}{2}}\epsilon.
\end{align}
where $d(\cdot,\cdot)$ denotes the angular distance.
\end{lemma}

Let $C^0(S)$ denote the set of continuous functions on the set $S$. This lemma has the following corollary:
%\LJ{Useful propositions for Prop.~\ref{thm:max-bits-gauss}}
\begin{corollary}
\label{cor:proba-discontinuous-sign-scp}
Given $\tilde{x} \in \bb S^{n-1}$, $0<\epsilon < 1/2$, $a \in \R{n}$ with $a \sim_{\iid} \cl N(0,1)$, we have 
$$
\ts \bb P\Big[\sign{a^\top \cdot} \notin C^0\big(\ball{\tilde{x}}\cap \bb S^{n-1}\big)\Big] \leq \eta(\epsilon) := \sqrt{n}\,\epsilon.
$$
\end{corollary}
\begin{proof}
%\LJ{I found this easy proof. Let's see if we keep it or not} 
The proof can be derived from the complement of the event associated with $p_0$ in \Cref{eq: disk lemma} when $\tilde{x}=\tilde{v}$. Here is, however, a simplified proof for completeness. We first observe that $\sign{a^\top \cdot}$ is discontinuous over $\ball{\tilde{x}}\cap \bb S^{n-1}$ iff $|\frac{a^\top \tilde{x}}{\|a\|}| \leq \epsilon$. Therefore, by the rotational invariance of the Gaussian distribution \red{we can choose $\tilde{x}=[1,0,\dots,0]^{\top}$} and the probability above amounts to computing 
$$
\textstyle p := \bb P[|\frac{a_1}{\|a\|}| \leq \epsilon] = \bb P[ a_1^2 \leq \epsilon^2 \|a\|^2] = \bb P[  a_1^2 \leq \frac{\epsilon^2}{(1 - \epsilon^2)} (a_2^2 + \ldots + a_n^2)] = \bb E_\xi \bb P[a_1^2 \leq \frac{\epsilon^2}{(1 - \epsilon^2)} \xi],
$$
where $\xi \sim \chi^2(n-1)$. Since $\bb P[a_1^2 \leq \frac{\epsilon^2}{(1 - \epsilon^2)} \xi] \leq \frac{\sqrt 2}{\sqrt{\pi}}  \frac{\epsilon}{\sqrt{1 - \epsilon^2}} \sqrt{\xi}$, and $\bb E_\xi \sqrt \xi \leq \sqrt{\bb E_\xi \xi} \leq \sqrt{n-1} \leq \sqrt{n}$ by Jensen, we finally get $p \leq \frac{\sqrt 2}{\sqrt \pi} \, \frac{\epsilon}{\sqrt{1 - \epsilon^2}} \sqrt n \leq \frac{2 \sqrt 2}{\sqrt \pi \sqrt 3} \epsilon \sqrt n < \epsilon \sqrt n$. 




\end{proof}


\section{Signal Recovery Proof}\label{app: signal recovery}

%We begin with the proof of \Cref{theo: signal recov boxdim}.
\begin{proof}[Proof of \Cref{theo: signal recov boxdim}]
As $\bdim{\signalset}<k$, there exist a constant $\epsilon_0\in (0,\frac{1}
{2})$ such that $\mathfrak{N}(\signalset,\epsilon)\leq \epsilon^{-k}$ for all $\epsilon\leq \epsilon_0$. Thus, there is a set $Q_{\epsilon}$ of $\epsilon^{-k}$ points, such that for every $x\in\signalset$, there exist a point $q\in Q_{\epsilon}$ which verifies $\|x-q\|<\epsilon$. Applying~\Cref{lemma: lemma laurent}, we have for any two distinct points $\tilde{q}_1,\tilde{q}_2\in Q_\epsilon$ and $a_i\in \R{n}$ drawn from a standard Gaussian distribution such that
\begin{align}
    \Prob\left[\forall q_1 \in \ball{\tilde{q}_1}, \forall q_2\in \ball{\tilde{q}_2}: \;   \sign{a_{i}^{\top} q_1} \neq \sign{a_{i}^{\top} q_2} \right] \geq   \frac{1}{\pi}\| \tilde{q}_1 - \tilde{q}_2 \| - \sqrt{n\frac{\pi}{2}}\epsilon
\end{align}
Since $\| \tilde{q}_1 - \tilde{q}_2 \|\geq \|q_1-q_2\| -2\epsilon$, for any $\delta>0$, we can write
\begin{align*}
    \Prob\left[\forall q_1 \in \ball{\tilde{q}_1}, \forall q_2\in \ball{\tilde{q}_2}: \;  \sign{a_{i}^{\top} q_1} \neq \sign{a_{i}^{\top} q_2} |\; \|q_1 - q_2 \|>\delta \right] \geq   \frac{\delta}{\pi} - (\frac{2}{\pi} +\sqrt{n\frac{\pi}{2}})\epsilon
\end{align*}
By setting $\epsilon =\frac{ \pi \delta}{4+\pi\sqrt{2\pi n}}$ and reversing the inequality we obtain
\begin{align}
    \Prob\left[\exists q_1 \in \ball{\tilde{q}_1}, \exists q_2\in \ball{\tilde{q}_2} : \;  \sign{a_{i}^{\top} q_1} = \sign{a_{i}^{\top} q_2} |\; \|q_1 - q_2\|>\delta \right] \leq 1-\delta/2
\end{align}
Extending these bound to all the rows of $A\in \R{m\times n}$ drawn from a standard Gaussian distribution, we have
\begin{align}
    \Prob\left[\exists q_1 \in \ball{\tilde{q}_1}, \exists q_2\in \ball{\tilde{q}_2} : \;  \sign{A q_1} = \sign{Aq_2} |\; \|q_1 - q_2\|>\delta  \right] \leq   (1-\delta/2)^{m}
\end{align}

Applying a union bound to all points $q\in Q_{\epsilon}$.
Since there are $\binom{|Q_\epsilon|}{2}\leq |Q_{\epsilon}|^2 \leq \epsilon^{-2k}$ pairs of points, we obtain
\begin{align}
    \Prob\left[\exists x_1,x_2 \in \signalset :\;  \sign{Ax_1} = \sign{Ax_2} | \; \|x_1-x_2\|> \delta \right] \leq (\frac{4+\pi\sqrt{2\pi n}}{ \pi \delta})^{2k}  (1-\delta/2)^{m} \\
    \leq \exp \left( 2k\log(\frac{4+\pi\sqrt{2\pi n}}{\pi\delta}) + m \log(1-\delta/2) \right)
\end{align}
Upper bounding this probability by $\xi$ and using the fact that $1-\delta/2\leq \exp(\delta/2)$ and $\frac{4+\pi\sqrt{2\pi n}}{\pi}\leq \sqrt{n}(\frac{4}{\pi}+\sqrt{2\pi})\leq 4 \sqrt{n}$, we obtain
\begin{align}
    2k\log\frac{4\sqrt{n}}{\delta} + m\frac{\delta}{2} \geq \log\xi \\
    m \geq  \frac{2}{\delta} \left(2k\log\frac{4\sqrt{n}}{\delta} + \log\frac{1}{\xi} \right)
\end{align}
for all \red{$\delta\leq 4 \epsilon_0 \sqrt{n}$.}
\end{proof}


\section{Model Identification Proof}\label{app: model ident}
 

%We continue with the proof of~\Cref{theo: onebit}.
\begin{proof}[Proof of~\Cref{theo: onebit}]
We want to prove that $\hat{\signalset}\subseteq\signalset_\delta$ holds with high probability w.r.t. a random draw of the operators $A_1,\dots, A_{\ntransf}$. Equivalently, we need to show that
\begin{equation}
    \sign{A_gx_g} = \sign{A_gv} \quad  \forall g =1,\dots,\ntransf
\end{equation}
holds for some $v\in \Sp{n-1}\setminus \signalset_\delta $ and some $x_1,\dots,x_{\ntransf}\in \signalset$ with probability at most $\xi$ with respect to a random draw of the Gaussian matrices $A_1,\dots,A_{\ntransf}$.  This proof adapts some of the procedures given in~\cite{jacques2013robust} to our specific setting. We start by bounding this probability for $\epsilon$-balls around vectors $\tilde{v}\in \Sp{n-1}\setminus \signalset$, $\tilde{x}_1,\dots,\tilde{x}_{\ntransf} \in \signalset$, that is
\begin{multline}
 p_0 = \Prob  [  \exists x_1 \in \ball{\tilde{x}_1}, \dots, \exists x_{\ntransf}\in \ball{\tilde{x}_{\ntransf}}, \exists v\in \ball{\tilde{v}} \; | \;  \forall g=1,\dots,\ntransf, \;\sign{A_{g} v} = \sign{A_{g} x_g}]
\end{multline}
As every row of each operator $A_g$ is independent, we have
\begin{align}
p_0 = \prod_{g=1}^{\ntransf} \Prob  \left[ \exists x_g \in \ball{\tilde{x}_g}, v\in \ball{\tilde{v}} \;| \; \sign{a_{g,i}^{\top} v} = \sign{a_{g,i}^{\top} x_g} \right]^{m}  \\
= \prod_{g=1}^{\ntransf}(1- \Prob\left[\forall x_g \in \ball{\tilde{x}_g}, v\in \ball{\tilde{v}} \;| \;  \sign{a_{g,i}^{\top} v} \neq \sign{a_{g,i}^{\top} x_g} \right])^{ m} \label{eq: prod}
\end{align}
Applying~\Cref{lemma: lemma laurent}, we have that 
\begin{equation}
    \Prob\left[\forall x_g \in \ball{\tilde{x}_g}, \forall v\in \ball{\tilde{v}} \;| \;  \sign{a_{g,i}^{\top} v} \neq \sign{a_{g,i}^{\top} x_g} \right] \geq d(\tilde{x}_g,\tilde{v}) - \sqrt{n\frac{\pi}{2}}\epsilon
\end{equation}
where the angular distance can be bounded by the Euclidean distance
\begin{equation}
   \pi d(\tilde{x}_g,\tilde{v}) \geq 2 \sin (\frac{\pi}{2} d(\tilde{x}_g,\tilde{v}) ) =   \| \tilde{x}_g - \tilde{v} \|\geq \delta % \| x_g  - v\| - 2\epsilon \geq \| x_g  - v\| - 2\epsilon
\end{equation}
for all $x_g \in \ball{\tilde{x}_g}$ and all $v\in\ball{\tilde{v}}$. Plugging this into~\cref{eq: prod}, we have
\begin{align}\label{eq:large g prod}
p_0 \leq (1-\frac{\delta}{\pi}+\sqrt{n\frac{\pi}{2}}\epsilon)^{m\ntransf} .
\end{align}
We can extend this result to all vectors $v\in\Sp{n-1}\setminus\signalset_\delta$ and $x_1,\dots,x_{\ntransf}\in \signalset$ by applying a union bound over a covering of the product set $\signalset^{\ntransf}\times (\Sp{n-1}\setminus\signalset_\delta)$. Since we can cover $\signalset$ with $\epsilon^{-k}$ balls with $\epsilon\leq \epsilon_0$ due to the assumption that $\bdim{\signalset}<k$, and also cover $\Sp{n-1}\setminus \signalset_{\delta}$ with 
$(3/\epsilon)^{n}$ balls, we have 
\begin{multline}
 \Prob  [  \exists x_1,x_2,\dots,x_{\ntransf} \in \signalset, \exists v\in (\Sp{n-1}\setminus \signalset_{\delta}) \; | \;  \forall g=1,\dots,\ntransf, \;\sign{A_{g} v} = \sign{A_{g} x_g} ] \\ \leq \epsilon^{-k\ntransf}(\epsilon/3)^{-n} p_0 
\end{multline}
Bounding this probability by $\xi$, we obtain the following inequality
\begin{align}
    &\epsilon^{-k\ntransf}(\epsilon/3)^{-n} (1-\frac{\delta}{\pi}+\sqrt{n\frac{\pi}{2}}\epsilon)^{m\ntransf} \leq \xi \\
    &\epsilon^{-k\ntransf+n}3^{n} (1-\frac{\delta}{\pi}+\sqrt{n\frac{\pi}{2}}\epsilon)^{m\ntransf} \leq \xi
\end{align}
Solving for $m$ and choosing $\epsilon = \sqrt{\frac{2(4-\pi)^2}{\pi^3 n}}\delta \approx 0.23\sqrt{\frac{1}{n}}\delta$ we get
\begin{equation}
    m \geq \frac{1}{\log(1-\frac{\delta}{4})} \left\{ (k+\frac{n}{\ntransf}) \log \frac{5\sqrt{n}}{\delta} + \frac{1}{\ntransf} \log \frac{1}{\xi} + \frac{n}{\ntransf}\log 3   \right\}
\end{equation}
%\JT{$\delta< \delta_0 = .23\sqrt{n} \epsilon_0 $}
for $\delta< 4\sqrt{n} \epsilon_0 $.
Finally, using the fact that $\log(1-\frac{\delta}{4})\geq \delta/4$, we obtain the desired bound,
\begin{align}
    m \geq \frac{4}{\delta} \left\{ (k+\frac{n}{\ntransf}) \log \frac{5\sqrt{n}}{\delta} + \frac{1}{\ntransf} \log \frac{1}{\xi} + \frac{n}{\ntransf}\log 3  \right\}.
\end{align}
\end{proof}



\paragraph{Derivation of $\delta$.}
The bound in~\Cref{theo: onebit}, that is
\begin{equation}
    m \geq \frac{4}{\delta} \left\{ (k+\frac{n}{\ntransf}) \log \frac{5\sqrt{n}}{\delta} + \frac{1}{\ntransf} \log \frac{1}{\xi} + \frac{n}{\ntransf}\log 3 \right\} 
\end{equation}
can be rewritten as a function of $\delta$ as 
\begin{equation} \label{eq: lambert}
    \log \delta + \delta a \geq b
\end{equation}
where 
\begin{align}
    a &= \frac{m}{4(k+\frac{n}{\ntransf})}   \\
    b &=  \log 5\sqrt{n} + \frac{1}{(\ntransf k + n)} \log \frac{1}{\xi} + \frac{n}{(\ntransf k + n)}\log 3 
\end{align}
Using this notation, the inequality in \cref{eq: lambert} can be further simplified as
\begin{align}
    \log \delta + \delta a &\geq b \\
    \delta e^{a\delta} \geq e^{b} \\
    a\delta \exp a\delta \geq a e^b \\
    a\delta \geq W(a e^b) \\
    \delta \geq \frac{1}{a} W(a e^b)
\end{align}
where the line before the last uses the fact that the inverse of $xe^{x}$ is the Lambert W function denoted as $W(\cdot)$. Since $W(x)\geq \log x - \log \log x$ for all $x\geq e$, we can write 
\begin{align}
    \delta &\geq \frac{1}{a} \log(a e^{b}) - \frac{1}{a} (\log \log a + \log b)\\
    \delta &\geq \frac{1}{a} (\log a+b) 
\end{align}
%\JT{TODO: verify if removing the loglog term is ok. It seems that Laurent removed it in 
%his analysis.} 
Finally, observing that $a\approx\frac{m}{k+\frac{n}{\ntransf}} $  and $b \approx \log n$ for large $m$, $n$ and $\ntransf$, we get
\begin{equation}
\delta = \mathcal{O} \left( \frac{k+\frac{n}{\ntransf}}{m} \log\frac{m n}{k+\frac{n}{\ntransf}}  \right)
\end{equation}
which, for $n/\ntransf \ll k$ is
\begin{equation}
\delta = \mathcal{O} \left( \frac{k}{m} \log\frac{m n}{k}  \right).
\end{equation}


\section{Sample Complexity Proof} \label{app: sample complexity}

\begin{proof}[Proof of \Cref{thm:max-bits-gauss}] %\LJ{A rough idea of the proof machinery is missing here. Hard to tell at first sight why we want to bound the number of discontinuous components.}

 \corr{We aim to bound the number of different cells associated with the binary mapping $\sign{A\cdot}$ which contain at least one element from the signal set $\signalset$, \ie $|\sign{A\signalset}|$. Our strategy consists in obtaining a global bound on the number of discontinuities of the binary mapping over the image of a covering of $\signalset$, which can then be related to the number of different cells that contain at least one element of $\signalset$. }

For $\epsilon < \epsilon_{0}$, let $\cl Q_\epsilon \subset \signalset$ be an optimal $\epsilon$ covering of $\signalset$, \ie $\signalset \subset \signalset_\epsilon + \epsilon \bb B^n$. If $\bdim{\signalset} < k$, then there exists an $\epsilon_{0}\in (0,\frac{1}{2})$ such that $|Q_\epsilon| \leq \epsilon^{-k}$ for all $\epsilon < \epsilon_{0}$. 
Let us define the binary mapping $\Phi(\cdot) := \sign{A\, \cdot}$ and the number $Z(S)$ of its discontinuous components over a set $S \subset \bb S^{n-1}$, \ie 
$$
Z(S) := \big|\{i: \Phi_i \notin C^0(S)\}\big|.
$$

%\LJ{Below, check if we propagate the new constant of \Cref{cor:proba-discontinuous-sign-scp}, paying attention that then $\epsilon \leq \min(1/2, \epsilon_{\signalset})$ for the corollary to hold.}
Combining \Cref{cor:proba-discontinuous-sign-scp} with a union bound argument and setting $t= \eta(\epsilon)$, we get that with probability exceeding 
$$
\ts 1 - |Q_\epsilon| \exp(-\frac{3}{2} \frac{m t^2}{3 \eta(\epsilon) + t}) \geq 1 - \exp(k\log(\frac{1}{\epsilon}) -\frac{3}{8} m \eta(\epsilon)),
$$
we have, for $V_\epsilon(q) := \ball{q} \cap \bb S^{n-1}$ 
$$
Z\big(V_\epsilon(q)\big) \leq 2 m \eta(\epsilon),
$$
for all $q \in \signalset_\epsilon$. Therefore, provided $\ts \eta(\epsilon)m = \epsilon m \sqrt{\frac{\pi}{2} n}\geq \frac{16}{3} k \log(\frac{1}{\epsilon})$,
the above probability exceeds $1 - \exp(-\frac{3}{16} m \eta(\epsilon))$. 

Given $s>\red{1}$, with $s=\mathcal{O}(1)$, let us set $\epsilon$ as 
$$
\ts \epsilon = \frac{16}{3} s \frac{k}{m \sqrt n} \log(\frac{3 m \sqrt n}{16 s k}), 
$$
or $\ts \eta(\epsilon)m = \frac{16}{3} s k \log(\frac{3 m \sqrt n}{16 s k})$, \red{under the assumption that $\epsilon_0 > \frac{16}{3} s \frac{k}{m \sqrt n} \log(\frac{3 m \sqrt n}{16 s k})$.}
Assuming $\frac{m \sqrt n}{s k}>e$, this means that $\log(1/\epsilon)\leq \log(\frac{3 m \sqrt n}{16 s k})$, so that the previous requirement on $\ts \eta(\epsilon)m$ holds if 
$$
\ts s k \log(\frac{3 m \sqrt n}{16 s k}) \geq k \log(\frac{3 m \sqrt n}{16 s k}),
$$
which is reached as soon as $s > 1$.

Therefore, with probability exceeding $1 - \exp(-\frac{3}{16} m \eta(\epsilon)) = 1 - \exp(- s k \log(\frac{3 m \sqrt n}{16 s k})) = 1 - (\frac{3 m \sqrt n}{16 s k})^{- s k} \geq 1 - (\frac{3 m \sqrt n}{16 s})^{- s}$, we thus know that for any $q \in Q_\epsilon$, 
$$
|\Phi(V_\epsilon(q))| \leq 2^{2 s k \log(\frac{3 m \sqrt n}{16 s k})}.
$$
Since $\epsilon^{-k} = 2^{k \log_2(1/\epsilon)} \leq 2^{ (\log 2)^{-1} k \log(\frac{3 m \sqrt n}{16 s k})}$ for some $c>0$,  
$$
\ts |\sign{A \signalset}| \leq \sum_{q \in \signalset_\epsilon} |\Phi(V_\epsilon(q))| \leq \epsilon^{-k} 2^{s k \log(\frac{3 m \sqrt n}{16 s k})} \leq 2^{((\log 2)^{-1} + 1) s k \log(\frac{3 m \sqrt n}{16 s k})} \leq \left(\frac{2m \sqrt n}{s k}\right)^{4 s k} .
$$
We conclude \red{by choosing $s=2$ to obtain the bound $|\sign{A \signalset}| \leq \left(\frac{m \sqrt n}{k}\right)^{8 k}$ with probability exceeding $1 - \frac{1024}{9 m^2 n}$ which holds as long as $\epsilon_0>\frac{32}{3} \frac{k}{m \sqrt n} \log(\frac{3 m \sqrt n}{32 k})$.}

\end{proof}


\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}
