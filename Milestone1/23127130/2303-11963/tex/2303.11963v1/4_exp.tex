\section{Experiments}
\label{sec:exp}
\begin{figure*}
\begin{center}
\includegraphics[width=.95\linewidth]{img/novelview.pdf}
\end{center}
\vspace{-0.4cm}
   \caption{\textbf{Qualitative comparison with baseline methods on Novel View Synthesis. } We compare our novel view synthesis on transparent objects with the methods that we identify as most relevant to ours, NeRF~\cite{mildenhall2020nerf}, Eikonal Field~\cite{bemana2022eikonal}, IDR~\cite{yariv2020multiview}, and PhySG~\cite{zhang2021physg}. Our method outperforms the others on the high-frequency details caused by ray refraction. }
   \vspace{-0.4cm}
\label{fig:novelviewsyn}
\end{figure*}

\subsection{Synthetic Data Evaluation}
\vspace{-0.2cm}
\medskip
\noindent\textbf{Datasets. }We use the 4 mesh objects of kitty, cow, bear, and key-mouse from~\cite{Xing2022drot, zhang2021physg}, and render each object with the smooth dielectric BSDF model with Mitsuba 3 \cite{jakob2022mitsuba3} under an environmental light emitter. For synthetic dataset evaluations we set interior IOR to 1.4723 for glass and exterior IOR to 1.00028 for air. We also create datasets with interior IOR set to 1.2, and 2.4 for ablation studies. We uniformly sample 200 camera poses on the upper hemisphere around each object following the Fibonacci lattice and randomly assign 100 each for training and testing. We obtain object masks through data pre-processing~\cite{remove_bg}.    

\vspace{-0.2cm}
\medskip
\noindent\textbf{Baseline. }As discussed in Sec.~\ref{sec:related} and shown in Tab.~\ref{tab:baseline}, no other work studies the same problem as ours, i.e., modeling refraction for transparent objects with complex geometry by neural networks for novel view and relighting synthesis. We, therefore, classify our baselines based on different tasks: \textbf{NeRF}~\cite{mildenhall2020nerf} and \textbf{Eikonal Fields}~\cite{bemana2022eikonal} on novel view synthesis; \textbf{IDR}~\cite{yariv2020multiview} on novel view synthesis and geometry reconstruction; \textbf{PhySG}~\cite{zhang2021physg} on novel view and relighting synthesis, and geometry reconstruction.  As geometry is not our aimed task to improve, extracted mesh quality is only to show that RBN effectively disentangles geometry and ray refraction on appearance. We do not include comparisons with volume-based neural relighting methods~\cite{boss2021nerd, verbin2022refnerf} as they share the same appearance model with PhySG.

\begin{figure}
\begin{center}
\includegraphics[width=0.85\linewidth]{img/relight-cow.pdf}
\end{center}
\vspace{-0.4cm}
   \caption{\textbf{Qualitative results on Relighting for synthetic datasets. } We show that our network can faithfully relight the object with unseen environment illumination, unlike PhySG~\cite{zhang2021physg}. }
\label{fig:relightsyn}
\vspace{-0.3cm}
\end{figure}

\vspace{-0.2cm}
\medskip
\noindent\textbf{Novel View Synthesis.} 
A qualitative comparison of our method and baseline methods is shown in Fig.~\ref{fig:novelviewsyn}. NeRF and Eikonal Fields model object appearance as MLP-based volume and cannot distill radiance properly around the object surface. However, when modeling refractive objects with complex geometry, it is important to locate the surface for accurate refraction direction estimation. Eikonal fields relies on user-defined bounding boxes, resulting in failure cases where the opaque scene and the refractive part cannot be separated. Meanwhile, IDR and PhySG are surface-based methods, but IDR models appearance as a light field~\cite{wood2000surface} and cannot correctly interpolate the high-frequency change of the refracted environment illumination on object appearance. PhySG uses Disney BSDF~\cite{burley2012physically} which does not work for non-opaque objects and therefore fails to correctly disentangle geometry and appearance. 

We report quantitative evaluation on novel view synthesis with metrics including PSNR, SSIM, and LPIPS~\cite{zhang2018perceptual} through testing on held-out images in Tab.~\ref{tab:quanti}. Our method significantly outperforms all of our baselines on synthesizing novel views for accurately modeling the refraction direction of each ray intersected with geometry. 

% \vspace{-0.2cm}
\medskip
\noindent\textbf{Relight Synthesis.} 
We provide a qualitative comparison of relighting synthesis in Fig.~\ref{fig:relightsyn}. As the environment map used during training is natural and unstructured unlike in prior works~\cite{lyu2020differentiable, wu2018full}, many pixels share similar radiance, but our learned refractions are not overfitted on the training illumination; they are aligned with the true refractions. We relight each scene with an unseen environment map to test the correctness of the object refraction. PhySG fails on this task as it does not model refractive material, resulting in incorrect appearance decomposition~\cite{burley2012physically}.  We report quantitative evaluation w.r.t ground truth relighting in Tab.~\ref{tab:quanti}. 


\input{table-geomsyn}


\vspace{-0.2cm}
\medskip
\noindent\textbf{Disentanglement on Geometry and Appearance.}
We evaluate our extracted geometry on synthetic datasets with ground truth mesh through the Chamfer distance metric and compare our geometry with those of surfaced-based methods. In Fig~\ref{fig:relightsyn}, the geometry of PhySG is entangled with surface appearance, i.e. the appearance under the original illumination is imprinted on the surface and raised geometry. Tab.~\ref{tab:chamfersyn} shows that IDR does better than PhySG, though still worse than ours. Our geometry and refracted appearance are better separated due to our modeling of ray refraction and optimizations. 

\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.8\linewidth]{img/ablation-ior.pdf}
\end{center}
   \caption{\textbf{Experiments on different transparent media. }We show that NEMTO works for different transparent media other than glass. The learned $\eta_\mathbf{t}$ is adaptive to different media and allows our model to synthesize faithful results.}
\label{fig:ablation-ior}
\vspace{-0.4cm}
\end{figure}



\vspace{-0.2cm}
\medskip
\noindent\textbf{Robustness to different refractive indices.}
We conducted experiments on transparent objects rendered with various IORs to showcase the robustness of our framework to IOR changes. Our approach is suitable for different types of refractive materials, as demonstrated in Fig.~\ref{fig:ablation-ior}. Note that our predicted $\eta_\mathbf{t}$ for the blending of ray refraction and reflection is also adaptive to different IOR, as shown in the case for IOR = 2.4, the reflected radiance is adequately brighter than in IOR = 1.4723 and 1.2. 

\input{table-quantitative}


\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.9\linewidth]{img/sdf-a.pdf}
\end{center}
\vspace{-0.3cm}
   \caption{\textbf{Qualitative ablation on SDF-A}. SDF-A shows that jointly optimizing refraction and geometry is prone to error. Our approach performs significantly better than this naive approach.}
\label{fig:ablation-sdfa}
\vspace{-0.4cm}
\end{figure} 




\begin{figure*}
\begin{center}
\includegraphics[width=.95\linewidth]{img/rw.pdf}
\end{center}
   \caption{\textbf{Qualitative results on image synthesis and extracted geometry for real-world data. } We compare our extracted geometry, novel view synthesis, and relighting with the extracted geometry and rendering layer of TLG~\cite{li2020through}, which restricts the light bounce within transparent media to only two bounces. }
\label{fig:relightrw}
\vspace{-0.5cm}
\end{figure*}


\begin{figure}[t]
\begin{center}
   \includegraphics[width=\linewidth]{img/ablation-loss.pdf}
\end{center}
\vspace{-0.4cm}
   \caption{\textbf{Ablation on losses for ray refraction optimizations.} Each experiment is trained with a frozen geometry network to demonstrate the effect of each loss term on ray bending. }
\label{fig:ablation-loss}
\vspace{-0.4cm}
\end{figure}

\vspace{-0.2cm}
\subsection{Ablation Studies}
\vspace{-0.2cm}
We perform the following ablation studies to demonstrate the effectiveness of Our RBN, $\mathcal{L}_{\textrm{rg}}$, and $\mathcal{L}_{\textrm{rs}}$. 

\vspace{-0.2cm}
\medskip
\noindent\textbf{Ablation on ray bending network.} We implemented a naive version of our method~\textbf{SDF-A} without using RBN. It renders transparent objects with \textit{analytical} refraction to demonstrates the effectiveness of our RBN and neural environment matting method over the use of a physically-based differentiable renderer on transparent objects. As shown in Fig.~\ref{fig:ablation-sdfa}, our method synthesizes more accurate results when jointly optimizing for geometry and light refraction, which are better disentangled. This is evident from the smoother surfaces of our method due to $\mathcal{L}_{\textrm{rs}}$. NEMTO estimated smoother surface normal than SDF-A, and gives much more faithful ray refractions.%SDF-A shows that our method is able to disentangle geometry and appearance during joint optimization of the networks.

\vspace{-0.2cm}
\medskip
\noindent\textbf{Ablation on $\mathcal{L}_{\textrm{rg}}$ and $\mathcal{L}_{\textrm{rs}}$.} For experiments on the refraction guiding and refraction smoothness loss, we fix the optimized geometry and only show different optimization results for refraction prediction. The lower part of Tab.~\ref{tab:quanti} shows quantitative evaluation that our complete architecture performs better than without each of these two loss terms. Fig.~\ref{fig:ablation-loss} compares the learned refraction from each ablation experiment: in column (b) without $\mathcal{L}_{\textrm{rg}}$, the model cannot learn the correct direction; in column (c) without $\mathcal{L}_{\textrm{rs}}$, the optimized ray refraction is around the true scope but shows wrong wave-patterned artifacts.

\vspace{-0.2cm}
\subsection{Real World Data Results}
\noindent\textbf{Datasets.} For real-world evaluation, we use 4 sets of captured images and environment maps on dog, monkey, pig, and mouse shapes from TLG ~\cite{li2020through}. As TLG only provides 10-12 images for each real-world object, we render training data with the ground truth CT-scanned meshes following the steps of synthetic datasets generation detailed in the supplementary. We do evaluations on released real-world images. 

\vspace{-0.3cm}
\medskip
\noindent\textbf{Edited Scene Synthesis. }Fig.~\ref{fig:relightrw} shows our synthesis for real-world images. Our method is able to predict accurate ray refraction for transparent objects and produces a smoother surface normal prediction on geometry extraction than TLG. TLG designed a novel differentiable rendering layer for physically-based transparent object modeling, but it only renders up to two bounces of refraction, whereas our method does not pose an upper bound on the number of ray bounces. Moreover, TLG does not work with an unknown IOR for transparent objects. Note that, although TLG claims to require only 10-12 images for testing, it requires rendering a large-scale synthetic dataset with 1.5k HDR (High Dynamic Range) environment maps for training, which is unnecessary in our case.% However, the generalization ability of TLG remains an isse.
\vspace{-0.2cm}
