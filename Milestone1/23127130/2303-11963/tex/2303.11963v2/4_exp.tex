\section{Experiments}
\label{sec:exp}
\begin{figure*}
\begin{center}
\includegraphics[width=.95\linewidth]{img/novel_view.png}
\end{center}
\vspace{-0.4cm}
   \caption{\textbf{Qualitative comparison with baseline methods on Novel View Synthesis. } We compare our novel view synthesis on transparent objects with the methods that we identify as most relevant to ours, NeRF~\cite{mildenhall2020nerf}, Eikonal Field~\cite{bemana2022eikonal}, IDR~\cite{yariv2020multiview}, and PhySG~\cite{zhang2021physg}. Our method outperforms the others on the high-frequency details caused by ray refraction. }
   \vspace{-0.4cm}
\label{fig:novelviewsyn}
\end{figure*}

\subsection{Synthetic Data Evaluation}
\vspace{-0.2cm}
\medskip
\noindent\textbf{Datasets. }We use the 4 mesh objects of kitty, cow, bear, and key-mouse from~\cite{Xing2022drot, zhang2021physg}, and render each object with the smooth dielectric BSDF model with Mitsuba 3 \cite{jakob2022mitsuba3} under an environmental light emitter. For synthetic dataset evaluations we set interior IOR to 1.4723 for glass and exterior IOR to 1.00028 for air. We also create datasets with interior IOR set to 1.2, and 2.4 for ablation studies. We uniformly sample 200 camera poses on the upper hemisphere around each object following the Fibonacci lattice and randomly assign 100 each for training and testing. We obtain object masks through data pre-processing~\cite{remove_bg}.    

\vspace{-0.2cm}
\medskip
\noindent\textbf{Baseline. }As discussed in Sec.~\ref{sec:related} and shown in Tab.~\ref{tab:baseline}, no other work studies the same problem as ours, i.e., modeling refraction for transparent objects with complex geometry by neural networks for novel view and relighting synthesis. We, therefore, classify our baselines based on different tasks: \textbf{NeRF}~\cite{mildenhall2020nerf} and \textbf{Eikonal Fields}~\cite{bemana2022eikonal} on novel view synthesis; \textbf{IDR}~\cite{yariv2020multiview} on novel view synthesis and geometry reconstruction; \textbf{PhySG}~\cite{zhang2021physg} on novel view and relighting synthesis, and geometry reconstruction.  As geometry is not our aimed task to improve, extracted mesh quality is only to show that RBN effectively disentangles geometry and ray refraction on appearance. We do not include comparisons with volume-based neural relighting methods~\cite{boss2021nerd, verbin2022refnerf} as they share the same appearance model with PhySG.

\begin{figure}
\begin{center}
\includegraphics[width=0.8\linewidth]{img/rl.png}
\end{center}
\vspace{-0.5cm}
   \caption{\textbf{Qualitative results on Relighting for synthetic datasets. } We show that our network can faithfully relight the object with unseen environment illumination, unlike PhySG~\cite{zhang2021physg}. }
\label{fig:relightsyn}
\vspace{-0.4cm}
\end{figure}

\vspace{-0.2cm}
\medskip
\noindent\textbf{Novel View Synthesis.} 
A qualitative comparison of our method and baseline methods is shown in Fig.~\ref{fig:novelviewsyn}. NeRF and Eikonal Fields model object appearance as MLP-based volume and cannot distill radiance properly around the object surface. However, when modeling refractive objects with complex geometry, it is important to locate the surface for accurate refraction direction estimation. Eikonal fields relies on user-defined bounding boxes, resulting in failure cases where the opaque scene and the refractive part cannot be separated. Meanwhile, IDR and PhySG are surface-based methods, but IDR models appearance as a light field~\cite{wood2000surface} and cannot correctly interpolate the high-frequency change of the refracted environment illumination on object appearance. PhySG uses Disney BSDF~\cite{burley2012physically} which does not work for non-opaque objects and therefore fails to correctly disentangle geometry and appearance. 

We report quantitative evaluation on novel view synthesis with metrics including PSNR, SSIM, and LPIPS~\cite{zhang2018perceptual} through testing on held-out images in Tab.~\ref{tab:quanti}. Our method significantly outperforms all of our baselines on synthesizing novel views for accurately modeling the refraction direction of each ray intersected with geometry. 

% \vspace{-0.2cm}
\medskip
\noindent\textbf{Relight Synthesis.} 
We provide a qualitative comparison of relighting synthesis in Fig.~\ref{fig:relightsyn}. As the environment map used during training is natural and unstructured unlike in prior works~\cite{lyu2020differentiable, wu2018full}, many pixels share similar radiance, but our learned refractions are not overfitted on the training illumination; they are aligned with the true refractions. We relight each scene with an unseen environment map to test the correctness of the object refraction. PhySG fails on this task as it does not model refractive material, resulting in incorrect appearance decomposition~\cite{burley2012physically}.  We report quantitative evaluation w.r.t ground truth relighting in Tab.~\ref{tab:quanti}. 


\input{table-geomsyn}


\vspace{-0.2cm}
\medskip
\noindent\textbf{Disentanglement on Geometry and Appearance.}
We evaluate our extracted geometry on synthetic datasets with ground truth mesh through the Chamfer distance metric and compare our geometry with those of surfaced-based methods. In Fig~\ref{fig:relightsyn}, the geometry of PhySG is entangled with surface appearance, i.e. the appearance under the original illumination is imprinted on the surface and raised geometry. Tab.~\ref{tab:chamfersyn} shows that IDR does better than PhySG, though still worse than ours. Our geometry and refracted appearance are better separated due to our modeling of ray refraction and optimizations. 

\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.85\linewidth]{img/ior_ab.png}
\end{center}
   \caption{\textbf{Experiments on different transparent media. }We show that NEMTO works for different transparent media other than glass. The learned $\eta_\mathbf{t}$ is adaptive to different media and allows our model to synthesize faithful results.}
\label{fig:ablation-ior}
\vspace{-0.4cm}
\end{figure}



\vspace{-0.2cm}
\medskip
\noindent\textbf{Robustness to different refractive indices.}
We conducted experiments on transparent objects rendered with various IORs to showcase the robustness of our framework to IOR changes. Our approach is suitable for different types of refractive materials, as demonstrated in Fig.~\ref{fig:ablation-ior}. Note that our predicted $\eta_\mathbf{t}$ for the blending of ray refraction and reflection is also adaptive to different IOR, as shown in the case for IOR = 2.4, the reflected radiance is adequately brighter than in IOR = 1.4723 and 1.2. 


\input{table-quantitative}


\begin{figure}
\begin{center}
   \includegraphics[width=0.9\linewidth]{img/ab-sdf.png}
\end{center}
\vspace{-0.3cm}
   \caption{\textbf{Qualitative ablation on SDF-A}. SDF-A shows that jointly optimizing refraction and geometry is prone to error. Our approach performs significantly better than this naive approach.}
\label{fig:ablation-sdfa}
\vspace{-0.4cm}
\end{figure} 




\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.9\linewidth]{img/ab-loss.png}
\end{center}
\vspace{-0.4cm}
   \caption{\textbf{Ablation on losses for ray refraction optimizations.} Each experiment is trained with a frozen geometry network to demonstrate the effect of each loss term on ray bending. }
\label{fig:ablation-loss}
\vspace{-0.4cm}
\end{figure}


\vspace{-0.2cm}
\subsection{Ablation Studies}
% \vspace{-0.2cm}
We perform the following ablation studies to demonstrate the effectiveness of Our RBN, $\mathcal{L}_{\textrm{rg}}$, and $\mathcal{L}_{\textrm{rs}}$. 

\vspace{-0.2cm}
\medskip
\noindent\textbf{Ablation on ray bending network.} We implemented a naive version of our method~\textbf{SDF-A} without using RBN. It renders transparent objects with \textit{analytical} refraction to demonstrate the effectiveness of our RBN and neural environment matting method over the use of a physically-based differentiable renderer on transparent objects. As shown in Fig.~\ref{fig:ablation-sdfa}, our method synthesizes more accurate results when jointly optimizing for geometry and light refraction, which are better disentangled. This is evident from the smoother surfaces of our method due to $\mathcal{L}_{\textrm{rs}}$. NEMTO estimated smoother surface normal than SDF-A, and gives much more faithful ray refractions.


\begin{figure}[t]
\begin{center}
   \includegraphics[width=\linewidth]{img/rw-cat.pdf}
\end{center}
   \caption{\textbf{Qualitative results on novel view synthesis for real-world captured data.} We show the novel view synthesis of NEMTO on our captured real-world dataset compared to ground truth. The rightmost column displays chosen regions from the environment map as a reference for their corresponding refractions in our synthesis.}
\label{fig:rw-cat}
% \vspace{-0.2cm}
\end{figure}


\begin{table}[t]
  \centering
    \scalebox{0.9}{
      \begin{tabular}{@{}lcccc@{}}
        \toprule
         Method & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\
        \cmidrule{2-4}
        NStudio & 24.67 & 0.79 &  0.23 \\
        \textbf{NEMTO} & \textbf{27.34} & \textbf{0.85} & \textbf{0.17} \\
        \bottomrule
        \vspace{-0.2cm}
      \end{tabular}
    }
   \caption{\textbf{Quantitative comparison between NeRFStudio~\cite{tancik2023nerfstudio} and NEMTO on novel view synthesis}. We evaluate both methods on our captured cat dataset NEMTO renders better results due to its better capture of refractions. }
  \label{tab:rw-cat}
\end{table}


\medskip
\noindent\textbf{Ablation on $\mathcal{L}_{\textrm{rg}}$ and $\mathcal{L}_{\textrm{rs}}$.} For experiments on the refraction guiding and refraction smoothness loss, we fix the optimized geometry and only show different optimization results for refraction prediction. The lower part of Tab.~\ref{tab:quanti} shows quantitative evaluation that our complete architecture performs better than without each of these loss terms. Fig.~\ref{fig:ablation-loss} compares the learned refraction from each ablation experiment: in column (b) without $\mathcal{L}_{\textrm{rg}}$, the model cannot learn the correct direction; in column (c) without $\mathcal{L}_{\textrm{rs}}$, the optimized ray refraction is around the true scope but shows wrong wave-patterned artifacts.



\subsection{Real World Data Evaluation}
\medskip
\noindent\textbf{Datasets.} For real-world evaluation, we have two sets of experiments. Firstly, we use 4 datasets of transparent objects with complex geometry (dog, monkey, pig, and mouse shape) from TLG ~\cite{li2020through}. As TLG only provides 10-12 images for each object, we render training data using the provided ground truth CT-scanned meshes and environment illuminations. Our evaluations are performed on their released real-world images. Details of all our dataset generation procedures can be found in the supplementary. 

Additionally, we captured a real-world transparent object with sufficient training images to assess the applicability of NEMTO on real-world captured data. Details of our dataset capture procedure are in the supplementary materials. 



\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{img/rl2.pdf}
% \vspace{-0.3cm}
\end{center}
   \caption{\textbf{Qualitative results on relighting synthesis for real-world captured data}. Our captured environment map is shown in the top left. NEMTO can render visually-plausible relighted transparent objects. }
\label{fig:rw-rl}
\end{figure}

 
\medskip
\noindent\textbf{Qualitative results on the captured dataset. } Fig.~\ref{fig:rw-cat} and~\ref{fig:rw-rl} show the rendered results from NEMTO trained on captured real-world data. Despite the inaccuracy in real-world camera poses and captured environment maps, NEMTO synthesizes faithful and visually-plausible novel views and relighting results. In Fig.~\ref{fig:rw-cat}, we select geometric regions that produce approximately two bounces of refraction and compare synthesized refractions with the corresponding sections on the Envmap. From these comparisons, we show that our refraction synthesis is quite realistic. 

\begin{figure*}
\begin{center}
\includegraphics[width=.95\linewidth]{img/tlg.png}
\end{center}
% \vspace{-0.3cm}
   \caption{\textbf{Qualitative results on image synthesis and extracted geometry for `rendered' real-world data. } We compare our extracted geometry, novel view synthesis, and relighting with the extracted geometry and rendering layer of TLG~\cite{li2020through}, which restricts the light bounce within transparent media to only two bounces. }
\label{fig:relightrw}
% \vspace{-0.5cm}
\end{figure*}


\medskip
\noindent\textbf{Quantitative results on the captured dataset. } To quantitatively evaluate the performance of NEMTO on the captured real-world dataset, we provide a comparison on the quality of novel view synthesis between NEMTO and the combined implementation `nerfacto' from several SOTA NeRF models~\cite{barron2021mip, barron2022mipnerf360, martin2021nerf, muller2022instant} from NeRFStudio~\cite{tancik2023nerfstudio} in Tab.~\ref{tab:rw-cat}. NEMTO renders high-frequency refraction details that are closer to ground truth than `nerfacto' which cannot work with light transmission.  


\medskip
\noindent\textbf{Qualitative results on the `rendered' real-world datasets. } 
Fig.~\ref{fig:relightrw} shows our results for real-world images trained with synthetic data. Our method is able to predict accurate ray refraction for transparent objects and produces a smoother surface normal prediction on geometry extraction than TLG. TLG designed a novel differentiable rendering layer for physically-based transparent object modeling, but it only renders up to two bounces of refraction, whereas our method does not pose an upper bound on the number of ray bounces. Moreover, TLG does not work with an unknown IOR for transparent objects. Note that, although TLG claims to require only 10-12 images for testing, it requires rendering a large-scale synthetic dataset with 1.5k HDR (High Dynamic Range) envmaps for training, which is unnecessary in our case.



