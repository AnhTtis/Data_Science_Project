\section{Related Work}
\label{sec:related}
 \input{table-compare.tex}
\medskip
\noindent\textbf{Neural Rendering.}
Neural rendering algorithms with implicit scene representation fall into two categories, volume-based and surface-based methods. Volume-based methods, e.g.\ NeRF~\cite{mildenhall2020nerf}, enable photo-realistic novel view synthesis by representing the scene as a Multilayer Perceptron (MLP) based radiance field ~\cite{boss2021nerd, verbin2022refnerf, yariv2021volume}. These methods often cannot distill radiance near the object surface, which is disadvantageous in our case as light refraction strongly relies on ray-surface intersections as prior. Surfaced-based methods~\cite{yariv2020multiview} directly optimize the underlying geometry with SDFs
and estimate object surface with higher accuracy. 

Both representations have evolved to model appearance via the rendering equation \cite{kajiya1986rendering} i.e.,\ to jointly estimate the scene geometry, appearance, and illumination of the scene using existing 2D images ~\cite{bi2020neural, boss2021nerd, verbin2022refnerf, zhang2021physg, zhang2021nerfactor, zhang2022invrender}. These methods assume that objects have opaque surfaces and light paths are non-refractive throughout the scene, and model appearance with the Disney BRDF model~\cite{burley2012physically, karis2013real}. They provide insights into solving the highly ill-posed inverse rendering problem, but cannot work for transparent objects. In our model, we design an MLP for ray refraction prediction to allow modeling of light through non-opaque objects.



\begin{figure*}
\begin{center}
\includegraphics[width=\linewidth]{img/modelarch.pdf}
\end{center}
   \caption{\textbf{Overview of NEMTO framework}. (a) \textit{Geometry Network}. For each viewing ray $\boldsymbol\rho(t) = \mathbf{o} + t \boldsymbol{\omega_\mathbf{i}}$, we query geometry network $f_\theta$ through sphere tracing for the ray-surface intersection. (b) \textit{Ray Bending Network.} We map the viewing direction $\boldsymbol{\omega_i}$ directly to the final refracted ray $\boldsymbol{\omega_\mathbf{t}}$ exiting the object geometry with surface normal $\mathbf{n}$ and intersection $\mathbf{x}$ as prior. As we use an environment map as illumination, the radiance evaluated through refraction only depends on the ray direction, not the location that the ray exits from. (c) \textit{Forward Rendering.} To render $\boldsymbol\rho(t)$,  we analytically calculate reflection direction $\boldsymbol{\omega_\mathbf{r}}$ through $\boldsymbol{\omega_i}$ and $\mathbf{n}$. We then use our physically-inspired rendering algorithm with predicted ``refractive index'' $\eta_\mathbf{t}$ and evaluate the environment map through $\boldsymbol{\omega_\mathbf{t}}$ and $\boldsymbol{\omega_\mathbf{r}}$. }
\label{fig:modelarch}
\end{figure*}


\medskip
\noindent\textbf{Environment Matting.}
Environment matting captures the reflection and refraction of environment light by transparent objects. It represents illumination as texture maps and recovers refraction through pixel-texel correspondence. With a structured backlight as background, the light path through the object in the front can be approximately computed \cite{chuang2000environment, zongker1999environment}. Inspired by traditional environment matting techniques for transparent object shape reconstruction,  Chen \etal~\cite{chen2018tom} design a deep learning network to estimate the environment matting as a refractive flow field. These above methods require a controlled dark room to capture images without ambient light. Wexler \etal~\cite{wexler2002image} develop an environment matting algorithm that works with natural scene backgrounds, but that method requires complex camera setups and structured background light. In our method, we approximate environment matting through a neural network. We directly map camera rays to refracted rays and optimize by comparing projected pixels on the environment maps to ground truth pixels. Our method does not require a complex physical setup and is more adaptive to inaccurate geometry.

\medskip
\noindent\textbf{Transparent Object Modeling.}
Forward rendering of transparent objects is well-understood given Snell's Law. Inversely rendering transparent objects and reconstructing the geometry from images, however, remains challenging. Kutulakos \etal~\cite{kutulakos2008theory} first prove that a ray path through two-interface refractive media can be recovered theoretically. Given this insight, previous methods estimating transparent geometry use controlled setups for light path acquisition such as light field probes~\cite{wetzstein2011refractive}, polarized imagery \cite{huynh2010shape}, X-ray CT scanner \cite{stets2017scene}, and transmission imaging~\cite{kim2017acquiring}.
Wu \etal~\cite{wu2018full} and Lyu \etal \cite{lyu2020differentiable} use turntables in front of static structured backlights to reconstruct geometry, but our work does not require these special setups. 
Neural networks are later introduced with analytical refraction modeling to approach the problem. Li \etal~\cite{li2020through} assume known environment illumination and IOR to only optimize for geometry with one neural network on all kinds of geometry. 
Bemana \etal~\cite{bemana2022eikonal} synthesize novel views for transparent objects with unknown IOR, but only work with simple geometry such as a sphere. Our work doesn't assume known IORs or restrict the light refraction to two bounces. By directly estimating object-specific light refractions, NEMTO synthesizes photo-realistic novel views and relighting for transparent objects on casually-captured images. 
