\vspace{-0.2cm}
\section{Method }
\label{sec:method}

\subsection{Problem Formulation }
Each input dataset contains $N$ multi-view images $\{\mathbf{I}_n\}^N_{n=1}$ and $N$ object masks $\{\mathbf{Q}_n\}^N_{n=1}$ of a transparent object with unknown IOR,  and an environment map $\Gamma:\mathbb{R}^{H\times W\times3}$ for scene illumination. Object masks and the environment map can be generated by preprocessing image data, which we detail in the supplementary. 
We assume the images are captured under static distant illumination represented by the environment map. Separately estimating illumination can compensate for entanglement between geometry and lighting, as transparent object appearance is highly correlated to scene illumination. We relax the assumptions that light bounces twice within the transparent media in Li \etal~\cite{li2020through} and do not require two reference points on each ray~\cite{kutulakos2008theory}. 
The model architecture is summarized in Figure~\ref{fig:modelarch}. 



\subsection{Geometry Network }
\label{subsec:geometry}
 We adopt implicit signed distance functions (SDFs)~\cite{yariv2020multiview} to represent object geometry due to their adaptive resolution and memory efficiency~\cite{park2019deepsdf}. Specifically, we represent the geometry as a zero-level set $z_\theta$ of an MLP neural network $f_\theta: \mathbb{R}^3 \rightarrow \mathbb{R}$ mapping a 3D location $\mathbf{x}$ to its SDF value $z \in \mathbb{R}$, $\theta$ being its optimizable weights. Concretely, $z_\theta = \{\mathbf{x} \in \mathbb{R}^3 \; | \;f_\theta(\mathbf{x}) = 0 \}$. We optimize geometry with silhouette loss through $\{\mathbf{Q}_n\}^N_{n=1}$ and regularize the neural SDF with IGR regulariztion~\cite{icml2020_2086} for a smooth and realistic object surface, detailed in Sec.~\ref{subsec:opt}. 
 
 To find the intersection point $\mathbf{x}$ between the viewing ray $\boldsymbol\rho(t) = \mathbf{o} + t \boldsymbol{\omega}_\mathbf{i}$ and implicit geometry surface, we perform ray casting through sphere tracing~\cite{hart1996sphere} starting from the intersection between  $\boldsymbol\rho(t)$ and the bounding box of the object. 
 The surface normal of an SDF at $\mathbf{x}$ is directly given by its first order derivative: $\mathbf{n}=\nabla_{\mathbf{x}}f$, therefore enabling gradient flow between normal and geometry optimization. 

\subsection{Ray bending Network }

To render each viewing ray through the transparent object in a physically-plausible manner, we design our model to be consistent with the rendering equation \cite{kajiya1986rendering} and physically-based material model. 

 \medskip
\noindent\textbf{Light Refraction and Reflection Modeling. }
To model a transparent object, 
 one can analytically calculate the light path of an incident ray through the object given the perfect specular reflection and transmission exhibited by its smooth dielectric materials~\cite{pharr2016physically}. Specifically, all scattering of radiance for a given ray shares a single outgoing direction.  
 
As shown in Fig.~\ref{fig:modelarch}. (b), for a 3D point $\mathbf{x}$ on object surface, we denote the incoming ray direction as $\boldsymbol{\omega}_\mathbf{i}$, the unit surface normal as $\mathbf{n}$, the angle between $\boldsymbol{\omega}_\mathbf{i}$ and $\mathbf{n}$ as $\beta_\mathbf{i}$. Likewise denote reflected and transmitted ray direction as $\boldsymbol{\omega}_\mathbf{r}$ and $\boldsymbol{\omega}_\mathbf{t}$ with $\beta_\mathbf{r}$ and $\beta_\mathbf{t}$. Analytically, $\beta_\mathbf{r} = \beta_\mathbf{i}$, and according to Snell's Law, 
\begin{equation}\label{eqn:transmitangle}
\eta_\mathbf{t} \sin{\beta_\mathbf{t}} = \eta_\mathbf{i} \sin{\beta_\mathbf{i}},
 \end{equation}
 where $\eta_\mathbf{i}$ and $\eta_\mathbf{t}$ are defined as the indices of refraction of the medium through which the incoming and outgoing ray travels, respectively. The analytical \textit{reflected} ray direction is expressed as: \vspace{-0.3cm}
 \begin{equation}\label{eqn:reflectdir}
\boldsymbol{\omega}_\mathbf{r} = 2(\mathbf{n}\cdot\boldsymbol{\omega}_\mathbf{i})\mathbf{n} - \boldsymbol{\omega}_\mathbf{i}, 
 \end{equation}
 while the analytical \textit{refracted} ray direction is formulated as \vspace{-0.2cm}
\begin{equation}\label{eqn:transmitdir}
\boldsymbol{\omega}_\mathbf{a} = -\frac{\eta_\mathbf{i}}{\eta_\mathbf{t}} (\boldsymbol{\omega}_\mathbf{i} - (\boldsymbol{\omega}_\mathbf{i} \cdot \mathbf{n})\mathbf{n}) - \mathbf{n}\sqrt{1 - (\frac{\eta_\mathbf{i}}{\eta_\mathbf{t}})^2 (1 - (\boldsymbol{\omega}_\mathbf{i} \cdot \mathbf{n})^2 )}.
 \end{equation}


\vspace{-0.2cm}
\medskip
\noindent\textbf{Neural Environment Matting.}
The accuracy of the analytically evaluated refracted light direction is highly correlated to the quality of the estimated geometry, which makes it difficult to simultaneously optimize the geometry and light refraction through the object. 
To overcome this issue, we discard the analytical solution $\boldsymbol{\omega}_\mathbf{a}$ for refraction modeling in Eq.~\eqref{eqn:transmitdir} and utilize a neural environment matting method. Our ray bending network (RBN) directly estimates $\boldsymbol{\omega}_\mathbf{t}$, mapping incident rays intersecting the scene object to the final refracted ray direction, thereby learning how the transparent object refracts environment light and implicitly represents the refractive index through the network.  

As shown in Fig.~\ref{fig:modelarch} (b), our modeling of light refraction passing through transparent media is expressed as a function $\mathbf{R}$, which takes as input the viewing direction $\boldsymbol{\omega}_\mathbf{i}$, the first intersection point $\mathbf{x}$ between the viewing ray $\boldsymbol\rho(t) = \mathbf{o} + t \boldsymbol{\omega}_\mathbf{i}$ and the implicit geometry surface, and the surface normal $\mathbf{n}$ of point $\mathbf{x}$. The function output is the refracted direction $\boldsymbol{\omega_\mathbf{t}}$ for the viewing ray $\boldsymbol\rho$ exiting the geometry and the ``refractive index'' $\eta_\mathbf{t}$ at $\mathbf{x}$. By incorporating $\mathbf{x}$ and $\mathbf{n}$ as priors, we can account for complex viewing effects, such as total internal reflection, which depends on the angle between the surface normal, viewing direction and the concavity of the geometry. We approximate this function using a Multi-Layer Perceptron (MLP) network $\mathbf{R}_\Theta: (\boldsymbol{\omega}_\mathbf{i}, \mathbf{x}, \mathbf{n}) \rightarrow (\boldsymbol{\omega}_\mathbf{t}, \eta_\mathbf{t})$. To handle high-frequency ray refractions, we apply positional encoding~\cite{tancik2020fourfeat} to the viewing direction and surface intersection.

Our light path modeling stems from two important intuitions inspired by traditional environment matting techniques for transparent objects: (1) we assume the scene to contain a single object that is transparent, and we assume the contribution of radiance on each ray segment exiting the object is negligible except for one main refracted ray; (2) the scene illumination will be modeled with an environment map and each ray comes from an infinite distance. From these observations, the final evaluated radiance of each camera ray that intersects with the scene object only depends on its direction upon exiting the object. 
In the next section, we present our differentiable forward rendering algorithm to work with these assumptions. 

\subsection{Forward Rendering}

The recursive hemispherical integral of the rendering equation for evaluating each viewing ray does not have a closed-form solution and has to be numerically evaluated with the Monte Carlo method~\cite{veach1998robust}. We provide an approximate evaluation of the final radiance as the combination of reflected radiance at incident ray and refracted radiance at outgoing ray through the Fresnel term. Our rendering module is differentiable and designed for physical plausibility.

For transparent objects with smooth surfaces, the view-dependent reflected radiance $L_r$ at each incident point is only dependent on the the reflected ray direction \cite{pharr2016physically, verbin2022refnerf}:\vspace{-0.2cm}
\begin{equation}\label{eqn:reflectrad}
L_r \propto \int f_r(\boldsymbol\omega_\mathbf{r}, \boldsymbol\omega_\mathbf{i}) 
L_i(\boldsymbol{\omega}_\mathbf{i})d\boldsymbol{\omega}_\mathbf{i} = E(\boldsymbol{\omega}_\mathbf{r}),\vspace{-0.3cm}
\end{equation} while $\boldsymbol\omega_\mathbf{i}$ is related to $\boldsymbol{\omega_r}$ through Eqn.~\eqref{eqn:reflectdir}. We propose the assumption that an analogous relationship exists between refracted radiance at the incident location and the final refracted ray direction. Specifically,\vspace{-0.3cm}
\begin{equation}\label{eqn:transmitrad}
L_t \propto \int f_r(\boldsymbol{\omega}_\mathbf{t}, \boldsymbol{\omega}_\mathbf{i}) 
L_i(\boldsymbol{\omega}_\mathbf{i})d\boldsymbol{\omega}_\mathbf{i} = E(\boldsymbol{\omega}_\mathbf{t}),\vspace{-0.3cm}
\end{equation}where $E: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ maps unit ray direction $\boldsymbol\omega_{(\cdot)}$ to a 3-channel RGB color. We reference the RGB value on our estimated environment map by the 2D coordinate obtained through texture mapping from the viewing direction. 

We use the Fresnel equation to compute the incident energy split between reflection and transmission, therefore the reflected and transmitted radiance is proportional to the incident ray radiance at the surface intersection. 
For unpolarized transparent objects, the Fresnel reflectance is given by\vspace{-0.3cm}
\begin{equation}\label{eqn:r}
r_{\parallel} = \frac{\eta_\mathbf{t}\cos{\beta}_\mathbf{i} - \eta_\mathbf{i}\cos{\beta}_\mathbf{t}}{\eta_\mathbf{t}\cos{\beta}_\mathbf{i} + \eta_\mathbf{i}\cos{\beta}_\mathbf{t}},
\end{equation}
% \vspace{-0.3cm}
\begin{equation}\label{eqn:p}
 r_{\perp} = \frac{\eta_\mathbf{i}\cos{\beta}_\mathbf{i} - \eta_\mathbf{t}\cos{\beta}_\mathbf{t}}{\eta_\mathbf{i}\cos{\beta}_\mathbf{i} + \eta_\mathbf{t}\cos{\beta}_\mathbf{t}},
\end{equation} where $r_{\parallel}$ gives the reflectance for parallel polarized light, and $r_{\perp}$ is the reflectance for perpendicular polarized light. Since we assume light to be unpolarized, the Fresnel reflectance $F_r$ can be analytically written as, \vspace{-0.3cm}
\begin{equation}\label{eqn:fresnel}
F_r = \frac{1}{2}(r_{\parallel}^2 + r_{\perp}^2).
\end{equation}
 Due to the conservation of energy, the energy transmittance $F_t$ is therefore given by $F_t = 1 - F_r$. 

The final radiance for a given ray $\boldsymbol{\rho}$ is then evaluated as: \vspace{-0.3cm}
\begin{equation}\label{eqn:radiance}
\hat{\mathbf{C}}(\boldsymbol\rho) = F_r\odot E(\boldsymbol{\omega}_\mathbf{r}) + \frac{\eta_\mathbf{i}^2}{\eta_\mathbf{t}^2}(1 - F_r) \odot E(\boldsymbol{\omega}_\mathbf{t}),
\end{equation}with $\odot$ denotes element-wise multiplication, and $\eta_\mathbf{i}=1.00028$ being the IOR for air.



\subsection{Optimization}
\label{subsec:opt}
As the joint optimization of geometry and light refraction of transparent objects is highly ill-posed, we enforce different priors to generate visually plausible solutions. 

For each batch, we sample the set of all pixels $P$ from the input image dataset to get $M$ patches, each of $m\times m$ neighboring pixels. Therefore a training batch contains $m^2M$ pixels we denote as $P_M \subset P$. $P_M$ can be subdivided into two non-overlapping sets of pixels $P_{M_i}$ and $P_{M_o}$ depending on whether the pixel contains the object or not, $|P_{M_o}| + |P_{M_i}| = m^2M$. Each $p_{\boldsymbol\rho} \in P_M$ is rendered by one camera ray $\boldsymbol\rho(t) = \mathbf{o} + t \boldsymbol{\omega_\mathbf{i}}$ with origin $\mathbf{o}$ and viewing direction $\boldsymbol{\omega_\mathbf{i}}$. We apply masked rendering and mask out non-intersecting rays for the loss functions. Specifically, through sphere tracing, if $\boldsymbol\rho$ hits the object surface, $p_{\boldsymbol\rho} \in P_{M_i}$, otherwise $p_{\boldsymbol{\rho}} \in P_{M_o}$ and its rendered radiance $\hat{\mathbf{C}}(\boldsymbol\rho) = 0$. 

The Pixel loss for $P_M$ is obtained through the ground truth RGB $\Tilde{\mathbf{C}}_p$ for $p_{\boldsymbol\rho} \in P_{M_i}$ and rendered radiance $\hat{\mathbf{C}}(\boldsymbol\rho)$,
\begin{equation}
\mathcal{L}_{\textrm{pix}} = \frac{1}{|P_{M_i}|}\sum_{p_\mathbf{r}\in P_{M_i}}\lVert \hat{\mathbf{C}}(\boldsymbol\rho)- \Tilde{\mathbf{C}}_p \rVert _1.
\label{eq:img_loss}
\vspace{-0.3cm} 
\end{equation}


For ray refraction estimation, we use two losses: $\mathcal{L}_{\textrm{rg}}$ for each ray $\boldsymbol\rho$ that hits the object with viewing direction $\boldsymbol\omega_\mathbf{i}$ to guide its refracted direction $\boldsymbol\omega_\mathbf{t}$ exiting the object toward the analytical solution $\boldsymbol\omega_\mathbf{a}$ obtained by Eq.~\eqref{eqn:transmitdir} through cosine similarity,
\begin{equation}
\mathcal{L}_{\textrm{rg}} = \frac{1}{|P_{M_i}|}\sum_{\boldsymbol\omega_{\mathbf{i}}: p_{\boldsymbol\rho} \in P_{M_i}}
[ 1 - \frac{\boldsymbol\omega_\mathbf{t} \cdot \boldsymbol\omega_\mathbf{a}}{\max(\lVert\boldsymbol\omega_\mathbf{t}\rVert_2 \cdot \lVert\boldsymbol\omega_\mathbf{a}\rVert_2, 0)} ]. 
\label{eq:rg_loss}
\end{equation} We employ a weight decaying strategy on $\mathcal{L}_{\textrm{rg}} $ to provide initial physically-guided supervision for RBN on refraction. Additionally, we utilize a patch-based loss term $\mathcal{L}_{\textrm{rs}} $ for ray hits to encourage locally smooth refraction directions, as we assume that most transparent objects are locally smooth given their smooth dielectric material. For each $m\times m$ patch containing only ray hits, we penalize the mean of variance on the estimated refraction directions. Along with $\mathcal{L}_{\textrm{pix}}$, the RBN can better compensate the inaccuracy on estimated geometry than strictly following analytical solution. 

For geometry optimization, we employ a silhouette loss from input masks $\{\mathbf{Q}_n\}^N_{n=1}$ and focus on non-intersecting rays. For each ray $\boldsymbol\rho: p_{\boldsymbol\rho} \in P_{M_o}$, we uniformly sample K points on $\boldsymbol\rho$ within the object bounding box and query $f_\theta$ for the minimal SDF value $z_k$ to penalize on ray miss %$\{z_k\}^K_{k=1}$ 
with the cross-entropy $\textrm{CE}_\alpha$ loss with logit parameterized by $\alpha$,
\begin{equation}
\mathcal{L}_{\textrm{sil}} = \frac{1}{|P_{M_o}|}\sum_{p_{\boldsymbol\rho}\in P_{M_o}} \textrm{CE}_\alpha(z_k).
\label{eq:mask_loss}
 \vspace{-0.3cm}
\end{equation}
 In order to impose a constraint on the learned function $f_\theta$ to ensure that it is an approximate SDF, we randomly sample $V$ points $\{\mathbf{v_i}\}^V_{i=1}$ inside the bounding box and adopt the loss by Implicit Geometric Regularization (IGR)~\cite{icml2020_2086}:
\begin{equation}
\mathcal{L}_{\textrm{e}} = \mathbb{E}_\mathbf{v}(\lVert \nabla_\mathbf{v}f_\theta \rVert_2 -1)^2 .
\label{eq:eik_loss}
\end{equation}

The overall loss function of our optimization is defined as the weighted sum of each loss with $\lambda_{(\cdot)}$ as weight terms: 
\vspace{-0.3cm}
\begin{equation}
    \begin{split}
    \mathcal{L} &= \lambda_\textrm{pix} \mathcal{L}_\textrm{pix} 
     + \lambda_{\textrm{e}} \mathcal{L}_{\textrm{e}}  + \lambda_\textrm{sil} \mathcal{L}_\textrm{sil}\\
    &+ \lambda_{\textrm{rg}} \mathcal{L}_{\textrm{rg}}  + \lambda_\textrm{rs} \mathcal{L}_\textrm{rs}.
    \label{eq:overall_loss}
    \end{split}
\end{equation}
