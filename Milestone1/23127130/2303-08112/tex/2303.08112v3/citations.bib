@article{aitchison1982statistical,
  title={The statistical analysis of compositional data},
  author={Aitchison, John},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={44},
  number={2},
  pages={139--160},
  year={1982},
  publisher={Wiley Online Library}
}

@article{alabdulmohsin2022revisiting,
  title={Revisiting neural scaling laws in language and vision},
  author={Alabdulmohsin, Ibrahim and Neyshabur, Behnam and Zhai, Xiaohua},
  journal={arXiv preprint arXiv:2209.06640},
  year={2022}
}

@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}


@inproceedings{
halawi2023overthinking,
title={Overthinking the Truth: Understanding how Language Models process False Demonstrations},
author={Halawi, Danny and Denain, Jean-Stanislas and Steinhardt, Jacob},
booktitle={Submitted to The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=em4xg1Gvxa},
note={under review}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{baldock2021deep,
  title={Deep learning through the lens of example difficulty},
  author={Baldock, Robert and Maennel, Hartmut and Neyshabur, Behnam},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10876--10889},
  year={2021}
}

@article{bansal2021revisiting,
  title={Revisiting model stitching to compare neural representations},
  author={Bansal, Yamini and Nakkiran, Preetum and Barak, Boaz},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={225--236},
  year={2021}
}

@article{belinkov2022probing,
  title={Probing classifiers: Promises, shortcomings, and advances},
  author={Belinkov, Yonatan},
  journal={Computational Linguistics},
  volume={48},
  number={1},
  pages={207--219},
  year={2022},
  publisher={MIT Press}
}

@article{black2021gpt,
  title={GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow},
  author={Black, Sid and Gao, Leo and Wang, Phil and Leahy, Connor and Biderman, Stella},
  journal={If you use this software, please cite it using these metadata},
  volume={58},
  year={2021}
}

% Non-exploitability citations
@article{de1937foresight,
  title={Foresight: Its logical laws, its subjective sources},
  author={De Finetti, Bruno and Kyburg, Henry E and Smokler, Howard E},
  journal={Breakthroughs in statistics},
  volume={1},
  pages={134--174},
  year={1937}
}

@article{garrabrant2016logical,
  title={Logical induction},
  author={Garrabrant, Scott and Benson-Tilsen, Tsvi and Critch, Andrew and Soares, Nate and Taylor, Jessica},
  journal={arXiv preprint arXiv:1609.03543},
  year={2016}
}

@article{hacking1967slightly,
  title={Slightly more realistic personal probability},
  author={Hacking, Ian},
  journal={Philosophy of Science},
  volume={34},
  number={4},
  pages={311--325},
  year={1967},
  publisher={Philosophy of Science Association}
}

@article{ramsey1926truth,
  title={Truth and probability.},
  author={Ramsey, FP},
  journal={Studies in subjective probability},
  pages={61--92},
  year={1926},
  publisher={New York: Wiley}
}

@misc{yudkowsky2007conservation,
  author = {Yudkowsky, Eliezer},
  title = {Conservation of Expected Evidence},
  year = {2007},
  howpublished = {\url{https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence}},
  note = {Accessed: March 18, 2023}
}
% end

@misc{gpt-neox-library,
  title = {{GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch}},
  author = {Andonian, Alex and Anthony, Quentin and Biderman, Stella and Black, Sid and Gali, Preetham and Gao, Leo and Hallahan, Eric and Levy-Kramer, Josh and Leahy, Connor and Nestler, Lucas and Parker, Kip and Pieler, Michael and Purohit, Shivanshu and Songz, Tri and Phil, Wang and Weinbach, Samuel},
  url = {https://www.github.com/eleutherai/gpt-neox},
  doi = {10.5281/zenodo.5879544},
  month = {8},
  year = {2021},
  version = {0.0.1},
}

@article{black2022gpt,
  title={Gpt-neox-20b: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}


@article{biderman2022datasheet,
  title={Datasheet for the {Pile}},
  author={Biderman, Stella and Bicheno, Kieran and Gao, Leo},
  year={2022},
    journal={Computing Research Repository},
    eprint={2201.07311},
    note={version 1},
    eprintprefix={arXiv},
    primaryClass={cs.CL},
    doi={10.48550/arXiv.2201.07311},
    url={https://arxiv.org/abs/2201.07311v1}
}

@article{biderman2023pythia,
  title={Pythia: a Scaling Suite for Language Model Interpretability Research},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Skowron, Aviya and Sutawika, Lintang and van der Wal, Oskar},
  journal={Computing Research Repository},
    eprint={2201.07311},
    note={version 1},
    eprintprefix={arXiv},
    primaryClass={cs.CL},
    doi={10.48550/arXiv.2201.07311},
    url={https://arxiv.org/abs/2201.07311v1},
    year={2023}
}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M and Nasrabadi, Nasser M},
  volume={4},
  number={4},
  year={2006},
  publisher={Springer}
}

@article{bojanowski2016enriching,
  title={Enriching Word Vectors with Subword Information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.04606},
  year={2016}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{breunig2000lof,
  title={LOF: identifying density-based local outliers},
  author={Breunig, Markus M and Kriegel, Hans-Peter and Ng, Raymond T and Sander, J{\"o}rg},
  booktitle={Proceedings of the 2000 ACM SIGMOD international conference on Management of data},
  pages={93--104},
  year={2000}
}

@article{chan2022causal,
  title={Causal Scrubbing: a method for rigorously testing interpretability hypotheses},
  author={Chan, Lawrence and Garriga-Alonso, Adrià and Goldowsky-Dill, Nicholas and Greenblatt, Ryan and Nitishinskaya, Jenny and Radhakrishnan, Ansh and Shlegeris, Buck and Thomas, Nate},
  journal={Alignment Forum},
  year={2022},
  url={https://bit.ly/3WRBhPD}
}

@article{csiszarik2021similarity,
  title={Similarity and Matching of Neural Network Representations},
  author={Csisz{\'a}rik, Adri{\'a}n and K{\H{o}}r{\"o}si-Szab{\'o}, P{\'e}ter and Matszangosz, {\'A}kos and Papp, Gergely and Varga, D{\'a}niel},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5656--5668},
  year={2021}
}

@inproceedings{dai-etal-2022-knowledge,
    title = "Knowledge Neurons in Pretrained Transformers",
    author = "Dai, Damai  and
      Dong, Li  and
      Hao, Yaru  and
      Sui, Zhifang  and
      Chang, Baobao  and
      Wei, Furu",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.581",
    doi = "10.18653/v1/2022.acl-long.581",
    pages = "8493--8502",
}

@article{dar2022analyzing,
  title={Analyzing transformers in embedding space},
  author={Dar, Guy and Geva, Mor and Gupta, Ankit and Berant, Jonathan},
  journal={arXiv preprint arXiv:2209.02535},
  year={2022}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{egozcue2016changing,
  title={Changing the reference measure in the simplex and its weighting effects},
  author={Egozcue, Juan Jos{\'e} and Pawlowsky-Glahn, Vera},
  journal={Austrian Journal of Statistics},
  volume={45},
  number={4},
  pages={25--44},
  year={2016}
}

@article{elazar2021amnesic,
  title={Amnesic probing: Behavioral explanation with amnesic counterfactuals},
  author={Elazar, Yanai and Ravfogel, Shauli and Jacovi, Alon and Goldberg, Yoav},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={160--175},
  year={2021},
  publisher={MIT Press}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{elhage2022toy,
  title={Toy Models of Superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  journal={arXiv preprint arXiv:2209.10652},
  year={2022}
}

@software{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@article{fan2019reducing,
  title={Reducing transformer depth on demand with structured dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  journal={arXiv preprint arXiv:1909.11556},
  year={2019}
}

@article{fort2021exploring,
  title={Exploring the Limits of Out-of-Distribution Detection},
  author={Fort, Stanislav and Ren, Jie and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:2106.03004},
  year={2021}
}

@article{gao2020pile,
  title={{The Pile}: An {800GB} Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  year={2020},
  journal={Computing Research Repository},
  eprint={2101.00027},
  note={version 1},
  eprintprefix={arXiv},
  primaryClass={cs.CL},
  doi={10.48550/arXiv.2101.00027},
  url={https://arxiv.org/abs/2101.00027v1}
}

@article{geva2020transformer,
  title={Transformer feed-forward layers are key-value memories},
  author={Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  journal={arXiv preprint arXiv:2012.14913},
  year={2020}
}

@article{geva2022transformer,
  title={Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space},
  author={Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2203.14680},
  year={2022}
}

@article{greff2016highway,
  title={Highway and residual networks learn unrolled iterative estimation},
  author={Greff, Klaus and Srivastava, Rupesh K and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1612.07771},
  year={2016}
}

@article{hase2023does,
  title={Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models},
  author={Hase, Peter and Bansal, Mohit and Kim, Been and Ghandeharioun, Asma},
  journal={arXiv preprint arXiv:2301.04213},
  year={2023}
}

@InProceedings{gehman-etal-2020-realtoxicityprompts,
    title = "{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models",
    author = "Gehman, Samuel  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Choi, Yejin  and
      Smith, Noah A.",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.301",
    doi = "10.18653/v1/2020.findings-emnlp.301",
    pages = "3356--3369",
    abstract = "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning {``}bad{''} words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",
}

@InProceedings{heinzerling2018bpemb,
  author = {Benjamin Heinzerling and Michael Strube},
  title = "{BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages}",
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year = {2018},
  month = {May 7-12, 2018},
  address = {Miyazaki, Japan},
  editor = {Nicoletta Calzolari (Conference chair) and Khalid Choukri and Christopher Cieri and Thierry Declerck and Sara Goggi and Koiti Hasida and Hitoshi Isahara and Bente Maegaard and Joseph Mariani and Hélène Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis and Takenobu Tokunaga},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {979-10-95546-00-9},
  language = {english}
  }

@article{hewitt2019designing,
  title={Designing and Interpreting Probes with Control Tasks},
  author={Hewitt, J and Liang, P},
  journal={Proceedings of the 2019 Con},
  year={2019}
}

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019}
}

@inproceedings{huang2016deep,
  title={Deep networks with stochastic depth},
  author={Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q},
  booktitle={European conference on computer vision},
  pages={646--661},
  year={2016},
  organization={Springer}
}

@article{jastrzkebski2017residual,
  title={Residual connections encourage iterative inference},
  author={Jastrz{\k{e}}bski, Stanis{\l}aw and Arpit, Devansh and Ballas, Nicolas and Verma, Vikas and Che, Tong and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.04773},
  year={2017}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{kaya2019shallow,
  title={Shallow-deep networks: Understanding and mitigating network overthinking},
  author={Kaya, Yigitcan and Hong, Sanghyun and Dumitras, Tudor},
  booktitle={International conference on machine learning},
  pages={3301--3310},
  year={2019},
  organization={PMLR}
}

@inproceedings{kornblith2019similarity,
  title={Similarity of neural network representations revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning},
  pages={3519--3529},
  year={2019},
  organization={PMLR}
}

@article{kovaleva2021bert,
  title={BERT busters: Outlier dimensions that disrupt transformers},
  author={Kovaleva, Olga and Kulshreshtha, Saurabh and Rogers, Anna and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2105.06990},
  year={2021}
}

@article{kumar2022probing,
  title={Probing classifiers are unreliable for concept removal and detection},
  author={Kumar, Abhinav and Tan, Chenhao and Sharma, Amit},
  journal={arXiv preprint arXiv:2207.04153},
  year={2022}
}

@inproceedings{laurencon2022roots,
title={The BigScience {ROOTS} Corpus: A 1.6{TB} Composite Multilingual Dataset},
author={Hugo Lauren{\c{c}}on and Lucile Saulnier and Thomas Wang and Christopher Akiki and Albert Villanova del Moral and Teven Le Scao and Leandro Von Werra and Chenghao Mou and Eduardo Gonz{\'a}lez Ponferrada and Huu Nguyen and J{\"o}rg Frohberg and Mario {\v{S}}a{\v{s}}ko and Quentin Lhoest and Angelina McMillan-Major and G{\'e}rard Dupont and Stella Biderman and Anna Rogers and Loubna Ben allal and Francesco De Toni and Giada Pistilli and Olivier Nguyen and Somaieh Nikpoor and Maraim Masoud and Pierre Colombo and Javier de la Rosa and Paulo Villegas and Tristan Thrush and Shayne Longpre and Sebastian Nagel and Leon Weber and Manuel Romero Mu{\~n}oz and Jian Zhu and Daniel Van Strien and Zaid Alyafeai and Khalid Almubarak and Vu Minh Chien and Itziar Gonzalez-Dios and Aitor Soroa and Kyle Lo and Manan Dey and Pedro Ortiz Suarez and Aaron Gokaslan and Shamik Bose and David Ifeoluwa Adelani and Long Phan and Hieu Tran and Ian Yu and Suhas Pai and Jenny Chim and Violette Lepercq and Suzana Ilic and Margaret Mitchell and Sasha Luccioni and Yacine Jernite},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=UoEw6KigkUn}
}

@article{lee2018simple,
  title={A simple unified framework for detecting out-of-distribution samples and adversarial attacks},
  author={Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{lenc2015understanding,
  title={Understanding image representations by measuring their equivariance and equivalence},
  author={Lenc, Karel and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={991--999},
  year={2015}
}

@article{li2021frequency,
  title={Frequency-aware SGD for Efficient Embedding Learning with Provable Benefits},
  author={Li, Yan and Choudhary, Dhruv and Wei, Xiaohan and Yuan, Baichuan and Bhushanam, Bhargav and Zhao, Tuo and Lan, Guanghui},
  journal={arXiv preprint arXiv:2110.04844},
  year={2021}
}

@article{li2022emergent,
  title={Emergent world representations: Exploring a sequence model trained on a synthetic task},
  author={Li, Kenneth and Hopkins, Aspen K and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2210.13382},
  year={2022}
}

@inproceedings{liu2008isolation,
  title={Isolation forest},
  author={Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
  booktitle={2008 eighth ieee international conference on data mining},
  pages={413--422},
  year={2008},
  organization={IEEE}
}

@article{mahalanobis1936generalized,
  title={On the generalized distances in statistics: Mahalanobis distance},
  author={Mahalanobis, PC},
  journal={Journal Soc. Bengal},
  volume={26},
  pages={541--588},
  year={1936}
}

@inproceedings{marelli-etal-2014-sick,
    title = "A {SICK} cure for the evaluation of compositional distributional semantic models",
    author = "Marelli, Marco  and
      Menini, Stefano  and
      Baroni, Marco  and
      Bentivogli, Luisa  and
      Bernardi, Raffaella  and
      Zamparelli, Roberto",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf",
    pages = "216--223",
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{ahdritz2022openfold,
  title={OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization},
  author={Ahdritz, Gustaf and Bouatta, Nazim and Kadyan, Sachin and Xia, Qinghui and Gerecke, William and O’Donnell, Timothy J and Berenberg, Daniel and Fisk, Ian and Zanichelli, Niccol{\`o} and Zhang, Bo and others},
  journal={bioRxiv},
  pages={2022--11},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@article{din2023jump,
  title={Jump to Conclusions: Short-Cutting Transformers With Linear Transformations},
  author={Din, Alexander Yom and Karidi, Taelin and Choshen, Leshem and Geva, Mor},
  journal={arXiv preprint arXiv:2303.09435},
  year={2023}
}



@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}


@article{meng2022mass,
  title={Mass-editing memory in a transformer},
  author={Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  journal={arXiv preprint arXiv:2210.07229},
  year={2022}
}


@article{millidge2022svd,
  title={The Singular Value Decompositions of Transformer Weight Matrices are Highly Interpretable},
  author={Millidge, Beren and Black, Sid},
  journal={LessWrong},
  year={2022},
  url={https://bit.ly/3GdbZoa}
}

@misc{nandatransformerlens2022,
    title  = {TransformerLens},
    author = {Nanda, Neel},
    url    = {https://github.com/neelnanda-io/TransformerLens},
    year   = {2022}
}

@article{nostalgebraist2020logitlens,
  title={interpreting GPT: the logit lens},
  author={nostalgebraist},
  journal={LessWrong},
  year={2020},
  url={https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}
}

@misc{nostalgebraist2021logitlens,
  title={logit lens on non-gpt2 models + extensions},
  author={nostalgebraist},
  year={2021},
  url={https://colab.research.google.com/drive/1MjdfK2srcerLrAJDRaJQKO0sUiZ-hQtA}
}

@article{olah2020zoom,
  title={Zoom in: An introduction to circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}

@article{perez2022ignore,
  title={Ignore Previous Prompt: Attack Techniques For Language Models},
  author={Perez, F{\'a}bio and Ribeiro, Ian},
  journal={arXiv preprint arXiv:2211.09527},
  year={2022}
}

@article{pimentel2022architectural,
  title={The Architectural Bottleneck Principle},
  author={Pimentel, Tiago and Valvoda, Josef and Stoehr, Niklas and Cotterell, Ryan},
  journal={arXiv preprint arXiv:2211.06420},
  year={2022}
}

@inproceedings{qendro2021early,
  title={Early exit ensembles for uncertainty quantification},
  author={Qendro, Lorena and Campbell, Alexander and Lio, Pietro and Mascolo, Cecilia},
  booktitle={Machine Learning for Health},
  pages={181--195},
  year={2021},
  organization={PMLR}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  journal={OpenAI Blog},
  year={2019}
}

@inproceedings{ravfogel2022linear,
  title={Linear adversarial concept erasure},
  author={Ravfogel, Shauli and Twiton, Michael and Goldberg, Yoav and Cotterell, Ryan D},
  booktitle={International Conference on Machine Learning},
  pages={18400--18421},
  year={2022},
  organization={PMLR}
}

@article{ravfogel2020null,
  title={Null it out: Guarding protected attributes by iterative nullspace projection},
  author={Ravfogel, Shauli and Elazar, Yanai and Gonen, Hila and Twiton, Michael and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2004.07667},
  year={2020}
}

@article{ravichander2020probing,
  title={Probing the probing paradigm: Does probing accuracy entail task relevance?},
  author={Ravichander, Abhilasha and Belinkov, Yonatan and Hovy, Eduard},
  journal={arXiv preprint arXiv:2005.00719},
  year={2020}
}

@inproceedings{roy2007effective,
  title={The effective rank: A measure of effective dimensionality},
  author={Roy, Olivier and Vetterli, Martin},
  booktitle={2007 15th European signal processing conference},
  pages={606--610},
  year={2007},
  organization={IEEE}
}

@article{sajjad2023effect,
  title={On the effect of dropping layers of pre-trained transformer models},
  author={Sajjad, Hassan and Dalvi, Fahim and Durrani, Nadir and Nakov, Preslav},
  journal={Computer Speech \& Language},
  volume={77},
  pages={101429},
  year={2023},
  publisher={Elsevier}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{scao2022bloom,
  title={{BLOOM}: A {176B}-Parameter Open-Access Multilingual Language Model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ilić, Suzana and Hesslow, Daniel and Castagné, Roman and Luccioni, Alexandra Sasha and Yvon, François and Gallé, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Benoît and Muennighoff, Niklas and del Moral, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and McMillan-Major, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Laurençon, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and et al.},
  year={2022},
  journal={Computing Research Repository},
  eprint={2211.05100},
    note={version 2},
    eprintprefix={arXiv},
    primaryClass={cs.CL},
    doi={10.48550/arXiv.2211.05100},
    url={https://arxiv.org/abs/2211.05100v2}
}

@article{schuster2022confident,
  title={Confident adaptive language modeling},
  author={Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh Q and Tay, Yi and Metzler, Donald},
  journal={arXiv preprint arXiv:2207.07061},
  year={2022}
}

@article{scholkopf2001estimating,
  title={Estimating the support of a high-dimensional distribution},
  author={Sch{\"o}lkopf, Bernhard and Platt, John C and Shawe-Taylor, John and Smola, Alex J and Williamson, Robert C},
  journal={Neural computation},
  volume={13},
  number={7},
  pages={1443--1471},
  year={2001},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{Sitikhu2019ACO,
  title={A Comparison of Semantic Similarity Methods for Maximum Human Interpretability},
  author={Pinky Sitikhu and Kritish Pahi and Pujan Thapa and Subarna Shakya},
  journal={2019 Artificial Intelligence for Transforming Business and Society (AITB)},
  year={2019},
  volume={1},
  pages={1-4}
}

@article{sra2005generalized,
  title={Generalized nonnegative matrix approximations with Bregman divergences},
  author={Sra, Suvrit and Dhillon, Inderjit},
  journal={Advances in neural information processing systems},
  volume={18},
  year={2005}
}

@article{timkey2021all,
  title={All bark and no bite: Rogue dimensions in transformer language models obscure representational quality},
  author={Timkey, William and van Schijndel, Marten},
  journal={arXiv preprint arXiv:2109.04404},
  year={2021}
}

@article{toneva2018empirical,
  title={An empirical study of example forgetting during deep neural network learning},
  author={Toneva, Mariya and Sordoni, Alessandro and Combes, Remi Tachet des and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J},
  journal={arXiv preprint arXiv:1812.05159},
  year={2018}
}

@inproceedings{tucker2021if,
  title={What if this modified that? syntactic interventions with counterfactual embeddings},
  author={Tucker, Mycal and Qian, Peng and Levy, Roger},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={862--875},
  year={2021}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{veit2016residual,
  title={Residual networks behave like ensembles of relatively shallow networks},
  author={Veit, Andreas and Wilber, Michael J and Belongie, Serge},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{xin2020deebert,
  title={DeeBERT: Dynamic early exiting for accelerating BERT inference},
  author={Xin, Ji and Tang, Raphael and Lee, Jaejun and Yu, Yaoliang and Lin, Jimmy},
  journal={arXiv preprint arXiv:2004.12993},
  year={2020}
}

@inproceedings{xu2019theory,
  title={A Theory of Usable Information under Computational Constraints},
  author={Xu, Yilun and Zhao, Shengjia and Song, Jiaming and Stewart, Russell and Ermon, Stefano},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{zhang2022opt,
  title={{OPT}: Open Pre-trained Transformer Language Models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  year={2022},
  journal={Computing Research Repository},
    eprint={2205.01068},
    note={version 4},
    eprintprefix={arXiv},
    primaryClass={cs.CL},
    doi={10.48550/arXiv.2205.01068},
    url={https://arxiv.org/abs/2205.01068v4}
}

@article{wang2022interpretability,
  title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small},
  author={Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2211.00593},
  year={2022}
}

@inproceedings{zhang2018improved,
  title={Improved adam optimizer for deep neural networks},
  author={Zhang, Zijun},
  booktitle={2018 IEEE/ACM 26th International Symposium on Quality of Service (IWQoS)},
  pages={1--2},
  year={2018},
  organization={IEEE}
}

@article{zhang2020accelerating,
  title={Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping},
  author={Zhang, Minjia and He, Yuxiong},
  journal={arXiv preprint arXiv:2010.13369},
  year={2020}
}

@inproceedings{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={International Conference on Machine Learning},
  pages={12697--12706},
  year={2021},
  organization={PMLR}
}

@incollection{zipf2013selected,
  title={Selected studies of the principle of relative frequency in language},
  author={Zipf, George Kingsley},
  booktitle={Selected Studies of the Principle of Relative Frequency in Language},
  year={2013},
  publisher={Harvard university press}
}

@techreport{openai2023gpt4,
  title = {GPT-4 Technical Report},
  author = {OpenAI},
  year = {2023},
  note = {Technical Report},
}

@misc{redpj2023,
	title={RedPajama, a project to create leading open-source models, starts by reproducing LLaMA training dataset of over 1.2 trillion tokens},
	url={https://www.together.xyz/blog/redpajama},
	author = {Together},
	month={April},
	year={2023}
}

@misc{stablevicuna2023,
	tile = {Stability AI releases StableVicuna, the AI World’s First Open Source RLHF LLM Chatbot},
	url = {https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot},
	month = {April},
	year = {2023},
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{voita2020information,
    title = "Information-Theoretic Probing with Minimum Description Length",
    author = "Voita, Elena  and
      Titov, Ivan",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.14",
    doi = "10.18653/v1/2020.emnlp-main.14",
    pages = "183--196",
    abstract = "To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates {``}the amount of effort{''} needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.",
}