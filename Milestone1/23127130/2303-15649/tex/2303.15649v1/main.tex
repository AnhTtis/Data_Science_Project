\documentclass[10pt,twocolumn,letterpaper]{article}
\pdfoutput=1
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[inline]{enumitem}
\usepackage{algorithm}
\usepackage[algo2e]{algorithm2e} 
\usepackage{cuted}
\usepackage{capt-of}
\usepackage{caption}

\usepackage{caption}
\usepackage{color}
\usepackage{subfig}
\usepackage{lipsum}
\usepackage{url}

\usepackage{diagbox} 

\usepackage{multirow}
\usepackage{booktabs} 

\usepackage{threeparttable}
\usepackage{pifont}
\usepackage{xcolor}  

\newcommand{\minisection}[1]{\vspace{0.04in} \noindent {\bf #1}\ \ }
\newcommand{\STAB}[1]{\begin{tabular}{@{}c@{}}#1\end{tabular}}
\newcommand{\lblfig}[1]{\label{fig:#1}}


\newcounter{alphasect}
\def\alphainsection{0}

\let\oldsection=\section
\def\section{%
  \ifnum\alphainsection=1%
    \addtocounter{alphasect}{1}
  \fi%
\oldsection}%

\renewcommand\thesection{%
  \ifnum\alphainsection=1% 
    \Alph{alphasect}
  \else%
    \arabic{section}
  \fi%
}%

\newenvironment{alphasection}{%
  \ifnum\alphainsection=1%
    \errhelp={Let other blocks end at the beginning of the next block.}
    \errmessage{Nested Alpha section not allowed}
  \fi%
  \setcounter{alphasect}{0}
  \def\alphainsection{1}
}{%
  \setcounter{alphasect}{0}
  \def\alphainsection{0}
}%


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi



\begin{document}

%%%%%%%%% TITLE
\title{StyleDiffusion: Prompt-Embedding Inversion for Text-Based Editing}

\author{Senmao Li$^{1}$ \quad Joost van de Weijer$^{2}$ \quad Taihang Hu$^{3}$ \\ \quad Fahad Shahbaz Khan$^{4,5}$ \quad Qibin Hou$^{6}$ \quad Yaxing Wang$^{1}$\thanks{The corresponding author.} \quad Jian Yang$^{1}$\\
	$^1${VCIP,CS, Nankai University}, $^2${Universitat Aut\`onoma de Barcelona}, $^3${Chongqing University}\\ $^4${Mohamed bin Zayed University of AI}, $^5${Linkoping University}, $^6${Nankai University}  \\
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\begin{strip}
	\vspace{-55pt}
	\centering
        \includegraphics[width=\linewidth]{abstract_compressed.pdf}    \vspace{-6mm}
        \captionof{figure}{ Our method takes as input a real image (leftmost column) and an associated caption.  We have more accurate editing capability than Null-text inversion~\cite{mokady2022null}. We manipulate the inverted image using the  editing technique P2P~\cite{hertz2022prompt}.}
        \label{fig:teaser}
	\vspace{-3pt}
\end{strip}


%%%%%%%%% ABSTRACT
\begin{abstract}

A significant research effort is focused on exploiting the amazing capacities of pretrained diffusion models for the editing of images. 
They either finetune the model,  or invert the image in the latent space of the  pretrained model.   However, they suffer from two problems: 
   (1) Unsatisfying results for selected regions, and unexpected changes in non-selected regions.     
    (2) They require careful text prompt editing where the prompt should include all visual objects in the input image. 
To address this, we propose two improvements: 
(1)  Only optimizing the input of the value linear network in the  cross-attention layers, is sufficiently powerful to reconstruct a real image.
    (2) We propose attention regularization to preserve the object-like attention maps after editing, enabling us to obtain accurate style editing without invoking significant structural changes. 
 We further improve the editing technique which is used for the unconditional branch of classifier-free guidance, as well as the conditional one as used by P2P~\cite{hertz2022prompt}. Extensive experimental prompt-editing results on a variety of images,  demonstrate qualitatively and quantitatively that our method has superior editing capabilities than existing and concurrent works.
\end{abstract}


\section{Introduction}
Large-scale models \cite{ramesh2022hierarchical,saharia2022photorealistic,rombach2021highresolution} have witnessed remarkable progress due to their exceptional realism and diversity.  Current works have studied the text-guided diffusion model for image editing. 
Based on a diffusion model generative prior, SDEdit~\cite{meng2021sdedit} adds noise to the input, then subsequently denoises the resulting image to enhance  generative image realism.
However, the generated image fails to accurately preserve the input image details. Several works~\cite{nichol2021glide, avrahami2022blended, avrahami2022blendedlatent}  
perform mask-specific image editing, enabling users to perform accurate editing.  Nevertheless, requiring additional masks for image editing makes the process less intuitive since the user must provide a prefect mask, limiting their flexibility. P2P~\cite{hertz2022prompt}  introduces prompt-to-prompt image editing by exploring  the cross attention layer, avoiding the need for additional mask information. Furthermore,  Some works focus on optimizing the textual embedding for image editing, which mainly consists of two directions: global editing~\cite{crowson2022vqgan, kwon2021clipstyler, kim2022diffusionclip} and local editing~\cite{bar2022text2live}. However, these methods still struggle to conduct complex image editing, since the used regularization is performed for the entire image.

Transferring knowledge of diffusion models to specific domains with real images has been studied. These works focus on finetuning the entire~\cite{Kawar2022ImagicTR, valevski2022unitune, ruiz2022dreambooth} or only part~\cite{kumari2022multi} of the network, aiming to manipulate a given real image,  while preserving high semantic and visual fidelity.  
Yet,  finetuning both the entire or part of the generative model with only few  examples suffers from both the cumbersome tuning of the model's weights and catastrophic forgetting~\cite{wu2018memory}. 
To overcome these shortcomings, recent works~\cite{hertz2022prompt,gal2022image,mokady2022null} prevent updating the pre-trained model. 
They focus on optimizing conditional or unconditional inputs of the cross-attention layers of the classifier-free diffusion model~\cite{ho2021classifier} (e.g., the Stable Diffusion model \cite{rombach2021highresolution}). 
Textual Inversion~\cite{gal2022image} optimizes the  textual embedding of the conditional branch given a few content-similar images. 
Null-text optimization~\cite{mokady2022null}  modifies the unconditional textual embedding  of the unconditional branch. 
However, they  suffer from the following challenges:
 \begin{enumerate*}[label=(\Roman*)]
    \item They lead to unsatisfying results for selected regions, and unexpected changes in non-selected regions. (see Figs.~\ref{fig:teaser} (\textit{Null-text})). 
     \item They require a user to provide an accurate text prompt that describes every visual object,  and the relationship between them in the input image (see Fig.~\ref{fig:perfect_prompt}). 
 \end{enumerate*}


To overcome the above-mentioned challenges, we analyze the role of the attention mechanism (and specifically the roles of keys, queries and values herein) in the diffusion process. This leads to the observation that the key dominates the output image structure (where) whereas the value determines the object style (what). Therefore, we propose to learn the textual embedding with a mapping network, namely the input of the \textit{value} linear mapping network in the cross-attention layers~\cite{bahdanau2014neural,NIPS2017_3f5ee243}. We freeze the input of the \textit{key} linear mapping network with the corresponding textual embedding of the  real image. 
Furthermore, we observe that DDIM inversion not only provides an approximate trajectory to reconstruct the real image~\cite{hertz2022prompt,mokady2022null}, but also the object-liked attention maps~(e.g., Fig.~\ref{fig:ours_woattnloss} (left corner)). Since the attention map is directly synthesized by both the key and query linear mapping networks, freezing the input of the key linear mapping contributes to preserve the well-learned attention map, which guarantees  the initial editability of the inverted image.  
While precisely reconstructing the real image, we observe that the system often outputs unsatisfying editing results (greatly adjusting the input image structure) due to less accurate attention maps.  Hence, to further improve our method, we propose an attention regularization to obtain more precise editing capabilities. 
Finally, in the widely used classifier-free diffusion model, the outputs are from both the conditional and unconditional branches with guidance scale. However, P2P only operates on the conditional branches, and ignores  the unconditional branch, which leads to less accurate editing capabilities for some cases.  Thus, we propose to perform the attention map exchange in both the unconditional branch ( called \textit{P2P+}), as well as in the conditional branch like P2P~\cite{hertz2022prompt}. This technique  enables us to obtain more accurate editing capabilities.  We build our method on Stable Diffusion~\cite{rombach2021highresolution} and experiment on a variety of images and prompt editing. See our accompanying code in  Suppl. Mat.. 

\begin{figure}[t]
    \centering
\includegraphics[width=\columnwidth]{perfect_prompt_compressed.pdf}\vspace{-2mm}
        \caption{Null-text~\cite{mokady2022null} editing results of the real image with different prompts. A good result requires  a full or perfect prompt.}
    \label{fig:perfect_prompt}\vspace{-2mm}
%\end{wrapfigure}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{ours_woattnloss2.pdf}\vspace{-2mm}
        \caption{The effect of the attention regularization.}
    \label{fig:ours_woattnloss}\vspace{-6mm}
%\end{wrapfigure}
\end{figure}


\begin{figure*}[t]
%\begin{wrapfigure}[11]{r}{90mm}
%\vspace{-2mm}
    \centering
\includegraphics[width=\textwidth]{framework_Taihang.pdf}\vspace{-4mm}
        \caption{Overview of the proposed method. (a) DDIM inversion: the diffusion process is performed to generate the latent representations: ${(\mathbf{\hat{z}}_t, \mathbf{\hat{a}}_t)} (t = 1,...,T)$, where $\mathbf{\hat{z}}_0$ is set to be the encoding of the input real image $\mathbf{x}_0$. $\mathbf{c}_0$ is the extracted textual embedding by a Clip-text Encoder with a given prompt.    (b) The proposed method: we take the input image $\mathbf{x}_0$ as input, and extract the textual embedding  $\mathbf{c}_{t-1} = M_{t-1}\left ( \mathbf{\mathbf{x}_0} \right )$, which is used to generate the value matrix $\mathbf{v}$ with the linear network $\Psi_V$. We freeze the input of   the linear network $\Psi_K$ with the given textual embedding $\mathbf{c}_0$.}\vspace{-6mm}
    \label{fig:framework}
%\end{wrapfigure}
\end{figure*}

\section{Related work}
\minisection{Transfer learning for diffusion models.}
A series of recent works investigated knowledge transfer on diffusion models~\cite{Kawar2022ImagicTR,kumari2022multi,ruiz2022dreambooth,valevski2022unitune} with one or few images. 
Recent works ~\cite{ruiz2022dreambooth,Kawar2022ImagicTR,valevski2022unitune, kumari2022multi}  either finetune the pretrained model,  or invert the image in the latent space of the  pretrained model. 
However, updating the diffusion model  unavoidably loses the text editing capability of the pre-trained diffusion model. In this paper, we focus on real image editing with a frozen diffusion model.

\minisection{GAN inversion.}
Image inversion aims to project a given real image into the latent space,  allowing users to further manipulate the image. There  exist several approaches~\cite{bermano2022state,creswell2018inverting,goetschalckx2019ganalyze,jahanian2019steerability,lipton2017precise,  xia2021gan,yeh2017semantic,zhu2016generative}  which focus on image manipulation based on pretrained GANs.  Given a target semantic attribute, they aim to manipulate the output image 
of a pretrained GAN. Several other methods~\cite{abdal2019image2stylegan,zhu2020domain} reverse a given image into the input latent space of a pretrained GAN (e.g., StyleGAN), 
and restructure the target image by optimization of the latent representation.


\minisection{Diffusion model inversion.} Diffusion-based inversion can be conducted naively by optimizing the latent representation.  \cite{dhariwal2021diffusion} shows that a given real image can be reconstructed by DDIM~\cite{song2020denoising} sampling. DDIM provides a good starting point to synthesize a given real image.  Several works~\cite{avrahami2022blendedlatent, avrahami2022blended,nichol2021glide} assume that the user provides a mask to restrict the region in which the changes are applied, achieving both meaningful editing and background preservation. P2P~\cite{hertz2022prompt} propose a mask-free editing method. However, it lead to the unexpected results when editing the real image.  Recent works investigate either the text embedding of the conditional input~\cite{gal2022image},  or the null-text optimization of the unconditional input(i.e., Null-Text Inversion~\cite{ mokady2022null}).  Although having the editing capability by combining the new prompts, 
they suffer from the following challenges:
   \begin{enumerate*}[label=(\Roman*)]
    \item It leads to unsatisfying results for the selected regions, and unexpected changes in non-selected regions.    
    \item They require careful text prompt editing where the prompt should include all visual objects in the input image.    
 \end{enumerate*}

Concurrent work~\cite{parmar2023zero} propose pix2pix-zero,  also aiming to  increase more accurate editing capabilities of the real image. However, it firstly need to compute the textual embedding direction with thousand sentencs in advances. 


\section{Method}
\subsection{Preliminary: Diffusion Model Given} 
Generally, diffusion models optimize a UNet-based denoiser  network $\epsilon_\theta$  to predict Gaussian noise $\epsilon$,  following the objective:
\begin{equation}
\min_\theta E_{\mathbf{z}_0,\epsilon \sim N(0,I),t\sim [1,T]} \left \| \epsilon-\epsilon_\theta(\mathbf{z}_t,t,\mathbf{c}) \right \|_{2}^{2}, 
\end{equation}
where $z_t$ is a noise sample according to timestamp $t\sim [1,T]$, and $T$ is the number of the timesteps.  The encoded text embedding $\mathbf{c}$ is extracted by a Clip-text Encoder $\Gamma$ with given prompt $\mathbf{p}$: $\mathbf{c} = \Gamma(\mathbf{p})$. 
In this paper, we  build on the Stable Diffusion model~\cite{rombach2021highresolution}.  It firstly trains both encoder $E$ and decoder $D$. Then the diffusion process is performed in the latent space. 
Here the encoder maps the image $\mathbf{x}$ into the latent representation  $\mathbf{z_0} = E(\mathbf{x})$, and the decoder $D$ aims to reverse the  latent representation  $\mathbf{z_0}$ into the image $\mathbf{x} = D(\mathbf{z_0})$.  The sampling process is given by:
\begin{equation}
\resizebox{0.9\hsize}{!}{$%
\mathbf{z}_{t-1} = \sqrt{\frac{\alpha_{t-1}}{\alpha_t}}\mathbf{z}_t +\left(\sqrt{\frac{1}{\alpha_{t-1}}-1}-\sqrt{\frac{1}{\alpha_t}-1}\right) \cdot \epsilon_\theta(\mathbf{z}_t,t,\mathbf{c}),
$}
\label{eq:ddim_sampling}
\end{equation}
 where  $\alpha_t$ is a scalar function.
 where  $\alpha_t$ is a scalar function that denotes the magnitude of the data.  


\minisection{DDIM inversion.} For real-image editing with a pretrained diffusion model, a given real image is to be reconstructed by finding its initial noise. 
 We use the deterministic DDIM model to perform image inversion.  
 This process is given by:
\begin{equation}
\label{eq:ddim_inversion}
\resizebox{0.9\hsize}{!}{$%
\mathbf{z}_{t+1} = \sqrt{\frac{\alpha_{t+1}}{\alpha_t}}\mathbf{z}_t \\
+ \left(\sqrt{\frac{1}{\alpha_{t+1}}-1}-\sqrt{\frac{1}{\alpha_t}-1}\right)\cdot \epsilon_\theta(\mathbf{z}_t,t,\mathbf{c}).
$}
\end{equation}
 DDIM inversion synthesizes the latent noise that produces an approximation of the input image when fed to the diffusion process. While the reconstruction based on DDIM is not sufficiently accurate,  it still provides a good starting point for training, enabling us to efficiently achieve high-fidelity  inversion~\cite{hertz2022prompt}. We use  the intermediate results of DDIM inversion to train our model, similarly as~\cite{couairon2022diffedit,mokady2022null}.


\minisection{Cross-attention.} The Stable Diffusion Model achieves text-driven image generation by feeding a prompt into the cross-attention layer.  Given both the text embedding $\mathbf{c}$ and  the image feature representation $\mathbf{f}$,  
we are able to produce the key matrix $\mathbf{k}= \Psi_K(\mathbf{c})$,  the value matrix $\mathbf{v}= \Psi_V(\mathbf{c})$ and the query matrix $\mathbf{q}= \Psi_Q(\mathbf{f})$,  via the  linear networks:  $\Psi_K, \Psi_V,\Psi_Q$. The attention maps are then  computed with:
\begin{equation}
\begin{aligned}\label{eq:atten_softmax}
\mathbf{a}= \text{Softmax}(\frac{\mathbf{q}\mathbf{k}^{T}}{\sqrt{\mathbf{d}}}),
\end{aligned}
\end{equation}
where $\mathbf{d}$ is the projection dimension of the keys and queries.  Finally, the cross-attention output is $\mathbf{\hat{f}} = \mathbf{a}\mathbf{v}$, which is then taken as input in the following  convolution layers. 


Intuitively, P2P~\cite{hertz2022prompt} performs prompt-to-prompt image editing  with cross attention control. P2P is based on the idea that the attention maps largely control where the image is drawn, and the values decide what is drawn (mainly defining the style). 
Improving the accuracy of the attention maps leads to more powerful editing capabilities~\cite{mokady2022null}.
  We experimentally observe that DDIM inversion generates satisfying attention maps (e.g., Figure~\ref{fig:ours_woattnloss} (left corner)), 
and provides a good starting point for the optimization. Next, we investigate the attention maps to guide the image inversion.


%%% JOOST
\subsection{StyleDiffusion for improved Image Inversion}

\minisection{Problem setting.} For a given real image,  our goal is to obtain more accurate editing capabilities with a frozen pretrained model. We invert a real image into a textual embedding $\mathbf{c}$ which is fed into the cross-attention layers.
Given the pair image feature $\mathbf{z}_0$ and textual embedding $\mathbf{c}_0$, 
we freeze the input of the linear network $\Psi_K$ with the provided textual embedding $\mathbf{c}_0$, namely  $\mathbf{k}= \Psi_K(\mathbf{c}_0)$. We learn the textual embedding $\mathbf{\widetilde {c}}$ for the linear network $\Psi_V$, namely $\mathbf{v}= \Psi_V(\mathbf{\widetilde {c}})$.  Instead of directly optimizing  the  textual embedding $\mathbf{\widetilde {c}}$, we introduce a mapping network to output  $\mathbf{\widetilde {c}}$.  
 We further introduce the attention  regularization to guide the optimization of the attention map, which is implicitly decided by the learned textual embedding $\mathbf{\widetilde {c}}$. In addition,  for the inverted image we further improve the editing technique which is used for the unconditional branch of classifier-free guidance, as well as the conditional one, like P2P~\cite{hertz2022prompt}. 


\begin{algorithm}[t]
\SetAlgoLined
\textbf{Require:} the features of the training images and the prompt embeddings: $\{\mathbf{z}_0,  \mathbf{c}_0 \}$.
$K=20$ is the number of training iteration for each timestep.  The mapping network $M$ with initialization parameters $\omega$.\\
\textbf{Temporary results:} With guidance scale $w=1$ for the classifier-free diffusion model, we use DDIM inversion to produce $\{ \mathbf{\hat{z}}_j, \mathbf{\hat{a}}_j\}(j=1,...,T)$.

\textbf{Output:} 
Mapping network $M$.\\
 \vspace{1mm} \hrule \vspace{1mm}
 Set guidance scale $w=7.5$; \\
 Initializing $ \mathbf{\widetilde{z}}_T \leftarrow \mathbf{\hat{z}}_T$; \\
 \For{$t=T,T-1,\ldots,1$}{
    \For{$k=0,\ldots,K-1$}{
        $ {\mathbf{a}_{t-1}, \mathbf{z}_{t-1}} \leftarrow \mathbf{\widetilde{z}}_t$;(Eqs.~\ref{eq:atten_softmax} and \ref{eq:pred_noise})\\
       % $ \mathbf{a}^i_t \leftarrow \mathbf{\widetilde{a}^i}_t$;\\
        $\omega  \leftarrow  \omega  - \eta \nabla_{\omega }\mathcal{L}$ ;(Eq.~\ref{eq:full_loss})
 }

 Synthesizing $\mathbf{\widetilde{z}}_{t-1}$;(Eq.~\ref{eq:pred_noise})
 
 }
 \textbf{Return} Mapping network $M$
\caption{Our algorithm}\label{alg:alg_ours}
\end{algorithm}

\begin{figure*}
    \centering
    \includegraphics[width=0.94\linewidth]{compa_compressed.pdf}
    \vspace{-10pt}
    \caption{Comparisons with different baselines for real images.  
        Our method, achieves realistic editing of both style and structured objects, while preserving the structure of the input image (last column).  }
    \label{fig:cat2dog}
\end{figure*}

\begin{figure*}[t]
%\begin{wrapfigure}[11]{r}{90mm}
%\vspace{-2mm}
    \centering
\includegraphics[width=\textwidth]{ablation_uncondiotion_AttentionRegu_compressed.pdf}
        \caption{
        (Left) Using additionally the attention injection in unconditional branch improves the real image editing ability of \emph{Null-text}~\cite{mokady2022null}~(\emph{P2P}). (Right) Comparison of  variants of our method. }
    \label{fig:uncondselfattn_more}
%\end{wrapfigure}
\end{figure*}

%\subsection{Our method}
\minisection{Method overview.}
As illustrated in Fig.~\ref{fig:framework} (top), 
we perform DDIM inversion~\cite{dhariwal2021diffusion,song2020denoising} to synthesize a series of latent noises $\{\mathbf{\hat{z}}_t\}$ and the attention maps $\{\mathbf{\hat{a}}_t\} (t = 1, ..., T)$. 
% 
Fig.~\ref{fig:framework} (bottom) shows that our framework consists of both a mapping network  $M_{t-1}$ and a diffusion network. 
The  mapping network $M_{t-1}$ takes  the input image $\mathbf{x}_0$ as an input, producing the textual embedding $\mathbf{c}_{t-1} = M_{t-1}\left ( \mathbf{x}_0 \right )$,  which is fed into the Value network $\Psi_V$ of the cross-attention layers.  
For a specific timestep $t$, in the order of the diffusion process $T \rightarrow 1$,  we get the diffusion network output $\epsilon_\theta(\mathbf{\widetilde{z}}_t,t-1,\mathbf{c}_0,\mathbf{c}_{t-1})$. 
Finally, we use the DDIM sampling method (i.e., Eq.~\ref{eq:ddim_sampling}) to produce the noise sample $\mathbf{z}_{t-1}$. 
Note $\mathbf{z}_{T}=\mathbf{\hat{z}}_{T}$ in the inference stage.   Our full algorithm is presented in algorithm~\ref{alg:alg_ours}.

\minisection{Training losses.}  The overall loss is a multi-task objective consisting of: (a) a \textit{reconstruction loss} and (b) a \textit{attention loss}. The reconstruction loss aims to reconstruct a given image sequentially  for a fixed number of timesteps: $\mathbf{z}_{T} \rightarrow \mathbf{z}_0$. The attention loss learns a more accurate attention map in the cross-attention layer.   

\minisection{\textit{Reconstruction Loss}.} 
Since the noise representations ($\{\mathbf{\hat{z}}_1, \cdots \mathbf{\hat{z}}_T\}$) provide an initial trajectory which is close to the real image, we train the mapping network $M_{t-1}$
to output the noise, which  is close to the noise representations ($\mathbf{\hat{z}}_t$)  with Eq.~\ref{eq:ddim_sampling}~\cite{mokady2022null}. The objective is 


\begin{equation}\label{eq:recon}
\resizebox{0.5\hsize}{!}{$%
 \mathcal{L}_{rec} = \min_{M_{t-1}}  \left \|\mathbf{\hat{z}}_{t-1}-\mathbf{z}_{t-1}\right \|^2,
 $}
\end{equation}
\begin{equation}\label{eq:pred_noise}
\resizebox{0.9\hsize}{!}{$%
 \mathbf{z}_{t-1} = \sqrt{\frac{\alpha_{t-1}}{\alpha_t}}\widetilde{\mathbf{z}}_t+\left(\sqrt{\frac{1}{\alpha_{t-1}}-1}-\sqrt{\frac{1}{\alpha_t}-1}\right) \cdot \epsilon_\theta(\widetilde{\mathbf{z}}_t,t-1, \mathbf{c}_{0}, \mathbf{c}_{t-1}),
 $}
\end{equation}
\begin{equation}\label{eq:pred_c}
\resizebox{0.9\hsize}{!}{$%
 \widetilde{\mathbf{z}}_{t} = \sqrt{\frac{\alpha_{t}}{\alpha_{t+1}}}\widetilde{\mathbf{z}}_{t+1}+\left(\sqrt{\frac{1}{\alpha_{t}}-1}-\sqrt{\frac{1}{\alpha_{t+1}}-1}\right) \cdot \epsilon_\theta(\widetilde{\mathbf{z}}_{t+1},t, \mathbf{c}_{0}, \widetilde{\mathbf{c}}_{t}),
 $}
\end{equation}
where $\mathbf{c}_{t-1}=M_{t-1}(\mathbf{x}_0)$. We learn a different mapping network $M_{t-1}$ for each timestep.  $\widetilde{\mathbf{c}}_{t}= M_{t}(\mathbf{x}_0)$ is  the learned textual embedding with the well-optimized $M_{t}$ at timestep $t$, and the  representation $\widetilde{\mathbf{z}}_{t}$ is the corresponding denoised feature.  At inference time, the initial input is $\widetilde{\mathbf{z}}_T = \hat{\mathbf{z}}_T$. 

\minisection{\textit{Attention loss.}} It is known that a more accurate attention map is positively correlated to the editability capability~\cite{mokady2022null}. 
The attention map, which is synthesized during the DDIM inversion, provides a good start point. Thus, we propose the attention regularization 
%to learn more accurate attention map 
when optimizing the mapping network $M_{t-1}$ to further improve its quality. The objective is the following:
\begin{equation}\label{eq:atten}
\begin{aligned}
 \mathcal{L}_{att} &= \min_{M}  \left \|\mathbf{\hat{a}}_{t}-\mathbf{a}_{t}\right \|^2,
 \end{aligned}
\end{equation}
where  $\mathbf{\hat{a}}_{t}$ and  $\mathbf{a}_{t}$ can be obtained with Eqs.~\ref{eq:ddim_inversion} and ~\ref{eq:atten_softmax}. 

\minisection{\textit{Full objective.}}
The full objective function of our model is:
\begin{equation}
\label{eq:full_loss}
\begin{aligned}
\mathcal{L} =   \mathcal{L}_{rec} +\mathcal{L}_{att}.
\end{aligned}
\end{equation}

When using P2P~\cite{hertz2022prompt} to edit the inverted image, it may lead to unsatisfying results. The potential reason could be that P2P only works on  the attention maps of the conditional branch, and ignores the ones of the unconditional branch. We propose \textit{P2P+} to perform additionally the attention exchange for the unconditional branch, enabling us to obtain more accurate editing capabilities. 



%\textbf{Copy from NeurIPS}

%\noindent \textbf

\begin{table}[t]
    \setlength{\tabcolsep}{1mm}
    \resizebox{\columnwidth}{!}{%
    \centering
    \footnotesize
    \setlength{\tabcolsep}{10pt}
    \begin{tabular}{|c|c|c|c|c|}
    \hline

        Metric   & Structure-dist$\downarrow$	&NS-LPIPS$\downarrow$	&Clipscore$\uparrow$\cr\cline{1-4}

      *DDIM  & 0.094	&0.3408 &\textbf{84.2$\%$ }\cr\cline{1-4}
      SDEit  & 0.044	&0.2046&80.1$\%$ 	\cr\cline{1-4}
      Null-text  & 0.028	&0.1114	&77.8$\%$ \cr\cline{1-4}
      Ours  & \textbf{0.022}&\textbf{0.0845}&79.3$\%$ \cr\cline{1-4}
    \hline 
    \end{tabular}  
    }
\caption{\small Comparison with baselines on three metrics. NS-LPIPS: non-selected LPIPS.  *DDIM: DDIM inversion with word swap.  Although \textit{DDIM with word swap} achieves the best  Clipscore, it not only changes  the background, but also modifies  the structure of the selected-region, see also Fig.~\ref{fig:cat2dog} (last three rows, fourth column) and additional results in  Suppl. Mat.~A.
}\label{tab:scores}
\end{table}

\section{Experimental setup}
\vspace{-2mm}
\minisection{Training details and datasets.} 
We use the pretrained Stable diffusion model.  We show the network details and more results on Supp.~Mat.~A. We randomly collect a real image dataset of 50 images and captions pairs (of $512 \times 512$ resolution) from Unsplash(\url{https://unsplash.com/}) and COCO~\cite{chen2015microsoft}.


\minisection{Evaluation metrics.}    \textit{Clipscore}~\cite{hessel2021clipscore} is  a metric that evaluates the quality of a pair of a prompt and an edited image. To evaluate the preservation of the structure information after editing,  we use Structure Dist~\cite{tumanyan2022splicing} to compute the  structural consistency of the edited image.  Furthermore, in this paper we aim to modify the selected region which is corresponding to the target prompt,  and preserve the non-selected region. Thus we need to evaluate the change of the non-selected region after editing. To get automatically  the non-selected region of the edited image, we use  a binary method to generate the raw mask by the attention map. Then we reverse it to get the non-selected region mask. Using the non-selected region mask, we compute the non-selected region LPIPS~\cite{zhang2018unreasonable}  between a pair of a real and edited image, named \textit{NS-LPIPS}. A lower score on NS-LPIPS means that the non-selected region is more similar to the input image.

\minisection{Baselines.}
We compare against the following baselines.
\emph{Null-text}~\cite{mokady2022null}  inverts real images with corresponding captions into the text embedding of the unconditional part of the classifier-free diffusion model.  \emph{SDEdit}~\cite{meng2021sdedit} introduces a stochastic differential equation to generate realistic images by an iterative denoising process.  \emph{Pix2pix-zero}~\cite{parmar2023zero}(concurrent work) edits the real image to find the potential direction from source to target words.   We compare with \textit{DDIM + word swap}~\cite{parmar2023zero} which performs DDIM sampling with an edited prompt generated by swapping the source word with the target. We use the official codes of the baselines  to compare with StyleDiffusion. 
We also evaluate variants of StyleDiffusion

\begin{figure}[t]
%\begin{wrapfigure}[11]{r}{90mm}
%\vspace{-2mm}
    \centering
\includegraphics[width=0.9\columnwidth]{user_study.pdf}
        \caption{ We conduct a forced-choice user study and ask subjects to select the results according to \textit{'Which figure does best preserve the input image structure and matches target prompt style?'}.}
    \label{fig:user_study}
%\end{wrapfigure}
\end{figure}
 
\section{Experiments}
\minisection{Qualitative and quantitative results.}
Fig.~\ref{fig:cat2dog}  presents a comparison between the baselines and our method. \textit{SDEit}~\cite{meng2021sdedit} fails to generate  high-quality images, such as dog or cat faces (second column).  Although \textit{Null-text} generates the target-specific images (e.g., the dog), it leads to unexpected changes in non-selected regions (e.g., the generated cat), which translates both the wall and door into a human face.  Pixel2pixel-zero~\cite{parmar2023zero} synthesizes better results, but it also  modifies the non-selected region, such as removing the plant when translating cat $\rightarrow$ dog.   The official implementation of \emph{pix2pix-zero}~\cite{parmar2023zero} provides the editing directions (e.g, cat $\leftrightarrow$ dog),  we  directly use them. Note \emph{pix2pix-zero}~\cite{parmar2023zero} firstly require that the editing directions are calculated in advance, while we do not require this.   Fig.~\ref{fig:cat2dog} (last three rows, third column) shows that \textit{DDIM with word swap} largely modifies both the background and the structure information of the foreground.  Our method successfully edits the target-specific object resulting in a high-quality image, indicating that the proposed method has more accurate editing capabilities. 

\begin{figure}[t]
%\begin{wrapfigure}[11]{r}{90mm}
\vspace{-5mm}
    \centering
\includegraphics[width=\columnwidth]{failure_compressed.pdf}\vspace{-2mm}
        \caption{Failure cases.\vspace{2mm}}
    \label{fig:failure_compressed}
    \vspace{-10mm}
%\end{wrapfigure}
\end{figure}

\begin{figure*}[t]
    \centering

\includegraphics[width=\textwidth]{oursresults_compressed.pdf} \vspace{-8mm}
        \caption{Examples of StyleDiffusion for editing with   attention injection or prompt refinement.}
          \vspace{-6mm}
    \label{fig:oursresults}
%\end{wrapfigure}
\end{figure*}


We evaluate the performance of the proposed method on the collected dataset.  As reported in Tab.~\ref{tab:scores},  in terms of both Structure distance and NS-LPIPS the proposed method achieves the best score, which indicates that we have superior capabilities to preserve the structure information. In term of Clipscore,  we get better score than Null-text (i.e., 79.3$\%$ vs 77.8$\%$), and a comparative result with SDEdit. \textit{DDIM with word swap} achieves the best score in Clipscore. We observe that \textit{DDIM with word swap} not only changes  the background, but also modifies the structure of the selected-region (see Fig.~\ref{fig:cat2dog} (last three rows, fourth column) and additional results in  Suppl. Mat.B). Note, we do not compare with pix2pix-zero~\cite{parmar2023zero} in Fig.~\ref{fig:cat2dog} (last three rows), since it needs first to compute the textual embedding directions with thousands of sentences with GPT-3~\cite{brown2020language}.
  
Furthermore, we conduct a user study and ask subjects to select the results that best matches the following statement: \textit{which figure preserves the input image structure and matches target prompt style} (Figure~\ref{fig:user_study}). We apply quadruplets comparisons (forced choice) with 23 users (30 quadruplets/user). Experiments are performed on images from the collected dataset. Fig.~\ref{fig:user_study} shows that our method considerably outperforms the other methods.

Fig~\ref{fig:oursresults}  shows that we manipulate  the inverted image with  attention injection and prompt refinement. 
For example, we translate  \textit{glasses}  into \textit{sunglasses} (~\ref{fig:oursresults} (first row)). Fig.~\ref{fig:oursresults} (last row)  we  add \textit{Chinese style} (new prompts) to the source prompt. These results indicate that our approach  manages  to invert real images with corresponding captions into the latent space,  while maintaining its powerful editing capabilities. 


\minisection{Ablation study.}  Here, we evaluate the effect of each independent contribution to our method and their combinations.

\minisection{\textit{Attention injection in the unconditional branch.}} Although P2P~\cite{hertz2022prompt} obtains satisfactory editing results with attention injection in the conditional branch, this method ignores the  attention injection in the unconditional branch.
We experimentally observe that the self-attention maps in the unconditional branch play an important role to obtain more accurate capabilities, especially when the structure changes before and after editing of the real image are relatively large,  e.g., translating \textit{bike} to \textit{motorcycle} on Fig.~\ref{fig:uncondselfattn_more} (left).  It also shows that the unconditional branch also contains a lot of useful texture and structure information, which indicates that we are able to reduce the influence of unwanted structure in the input image.

\minisection{\textit{Prompt-embedding in cross-attention layer.}} 
We evaluate variants of our method, namely
(1) learning the input prompt-embedding for the key linear network and freezing the input of the value linear network with the one provided by the user,  and (2) learning the textual embedding for both key and value linear networks. As shown in Fig.~\ref{fig:uncondselfattn_more} (right), the two variants fail to edit the image according to the target prompt. Our method successfully modifies the real image with the target prompt, and produces realistic results. 


\minisection{\textit{Attention regularization.}}  
We perform an ablation study of the attention regularization. Fig.~\ref{fig:ours_woattnloss} shows that the system ignores the partial objection information (e.g., the leg in Fig.~\ref{fig:ours_woattnloss} (first row, second column)), and learns a less accurate attention map (e.g., the leg attention map in Fig.~\ref{fig:ours_woattnloss} (second row, second column). Our method not only synthesizes high-quality images, but also learns a better attention map even comparing with the one generated by DDIM inversion (Fig.~\ref{fig:ours_woattnloss} (second row, first column)). 

\section{Conclusions and Limitations} 
We propose a new method for real image editing.  We invert the real image into the input of the value linear mapping network in the cross-attention layers, and freeze the input of the key linear mapping network with the textual embedding provided by the user. This allows to learn initial  attention maps, and an approximate trajectory to reconstruct the real image.   We introduce a new attention regularization to preserve the attention maps after editing, enabling us to obtain more accurate editing capabilities. In addition, we propose  attention injection in the unconditional branch of the classifier-free diffusion model, further improving the editing capabilities, especially when both source and target prompts have large domain shift. 

While StyleDiffusion successfully modifies the real image, it still suffers from some limitations.  Our method fails to generate satisfying images when the object of the real image has a rare pose (Fig.~\ref{fig:failure_compressed} (left)), or both the source and the target prompts have a large semantic shift (Fig.~\ref{fig:failure_compressed} (right)). 

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\clearpage
\appendix
\noindent{\Large\bf Supplementary Material}
\vspace{5pt}

\begin{alphasection}
\section{Training details}
\subsection{Network}
The mapping network consists of two sub-networks:   Encoder $E$ and  projection network $P$. In this paper, we use the encoder of the Stable Diffusion model   to initialize the Encoder $E$, and fix it. The projection network $P$ information is provided in Tab.~\ref{tab:network}. 


\begin{table*}[t]
    \setlength{\tabcolsep}{1mm}
    \resizebox{\textwidth}{!}{%
    \centering
    \footnotesize
    \setlength{\tabcolsep}{10pt}
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
        Metric   & (Input channel, Output channel)	&(Kernal size,Stride)& Input  Dimension&  Output dimension \cr\cline{1-5}
      Conv0  & (4, 77)&((2,2), 2) &(4, 64, 64)&(77, 32, 32)\cr\cline{1-5}
      % BatN0  & -	&-&	(77, 32, 32)&(77, 32, 32)\cr\cline{1-5}
      % Relu0  & -	&-	&(77, 32, 32)&	(77, 32, 32)\cr\cline{1-5}
      Conv1  & (77, 77)&((1,1), 1)&(77, 32, 32)&	(77, 32, 32)\cr\cline{1-5}
    BatN1  & -&-&(77, 32, 32)&	(77, 32, 32)\cr\cline{1-5}
      Relu1  & -&-	&(77, 32, 32)&	(77, 32, 32)\cr\cline{1-5}
      Conv2  & (77, 77)&((1, 9), 1)&(77, 32, 32)&(77, 32, 24) \cr\cline{1-5}
    \hline 
    \end{tabular}  
    }
\caption{\small Projection network $P$  information.
}\label{tab:network}
\end{table*}



\subsection{Training configure}
We use Adam~\cite{kingma2014adam} with a batch size of 1, and learning rates of 0.0001. The exponential decay rates are $( \beta_{1},\beta_{2})$ $= (0, 0.999)$. We randomly initialize the weights of the mapping network following a Gaussian distribution centered at 0 with 0.01 standard deviation. We use one Quadro RTX 3090 GPUs (24 GB VRAM) to conduct all our experiments. 

\section{Results}

\subsection{Sensitivity to prompts}
Fig.~\ref{fig:needfullprompt} shows the edited results when feeding different prompts.  We observe that Null-text~\cite{mokady2022null} is sensitive to the input prompts, and requires a very accurate prompt (e.g., Fig.~\ref{fig:needfullprompt}(first row, fifth column)) to modify successfully the inverted image with the target prompt. As shown on Fig.~\ref{fig:needfullprompt}(bottom), StyleDiffusion is robust  to the input prompts, always generate the target-like realistic images. 

\subsection{Intermediate results at inference stage}
We visualize intermediate results when editing the inverted image. As shown on Fig.~\ref{fig:zt2z0}(Top), Null-text~\cite{mokady2022null} modifies the pose or structure information at early timesteps, unexpectedly changing non-selected regions. For example, the door (i.e., a non-selected region) is translated into the tiger head. Our method preserves non-selected region, and focuses on modifying the inverted image in the selected region (e.g., the pose or structure on Fig.~\ref{fig:zt2z0}(Bottom)). 

\subsection{NS-LPIPS metric}

In this paper, we use \textit{NS-LPIPS} to evaluate changes of the LPIPS~\cite{zhang2018unreasonable} of  non-selected regions between a pair of a real and edited image. To automatically get  the non-selected region of the edited image, we use  a binary method to generate the raw mask from the attention map. Then we reverse it to get the non-selected region mask. Fig.~\ref{fig:ns-lpips} shows that the attention maps basically match the modified source prompt (e.g., dog), which indicates that using the attention map to extract non-selected regions is acceptable. 

\subsection{Prompt-embedding}



We propose two variants of our method, namely %which includes that 
(1) learning the input prompt-embedding for the key linear network and freezing the input of the value linear network with the one provided by the user,  and (2) learning the textual embedding for both key and value linear networks. Fig.~\ref{fig:inversion} presents the inversion and editing results. We observe that all variants successfully reconstruct the input images. However, they have poor editing capabilities.   Two variants (Fig.~\ref{fig:inversion} (third and fifth columns)) fail to edit the image according to the target prompt. Our method successfully modifies the real image with the target prompt, and produces realistic results (Fig.~\ref{fig:inversion} (last column)). 


\subsection{Additional results}

We show additional results in Figs.~\ref{fig:compare2}, ~\ref{fig:more_results_supp},~\ref{fig:results3},~\ref{fig:results4} and ~\ref{fig:results2}. 

\end{alphasection}


\begin{figure*}[t]
    \centering
\includegraphics[width=0.95\textwidth]
{needfullprompt_compressed.pdf}
        \caption{When using Null-text~\cite{mokady2022null} to edit real images, the quality of the input prompts is crucial for achieving successful results. We can see that Null-text  is very sensitive to the input prompt.  
        In comparison, StyleDiffusion is much more robust with respect to the input prompt. Furthermore, it has the advantage of requiring only a subset of prompts, sometimes even as minimal as a single word (second row, last column), to perform the desired editing results.}
    \label{fig:needfullprompt}
%\end{wrapfigure}
\end{figure*}

\begin{figure*}[t]
    \centering
\includegraphics[width=0.95\textwidth]
{ns-lpips-fg_compressed.pdf}
        \caption{We visualize the attention map (second column)  of the modified source prompt. We observe that the attention maps basically match the objects (third column). Thus, we invert the attention map to extract non-selected regions, which is further used to compute LPIPS. We can observe that StyleDiffusion only modifies the region which corresponds to the attention map.}
    \label{fig:ns-lpips}
%\end{wrapfigure}
\end{figure*}


\begin{figure*}[t]
    \centering
\includegraphics[width=\textwidth]
{inversion_compressed.pdf}
        \caption{Inversion and edited results of prompt-embedding for StyleDiffusion  and two variants.}
    \label{fig:inversion}
%\end{wrapfigure}
\end{figure*}

\begin{figure*}[t]
    \centering
\includegraphics[width=\textwidth]
{compare2_compressed.pdf}
        \caption{ Comparisons with different baselines for real images. Our method (last colum), achieves realistic editing of both style and
structured objects, while preserving the structure of the input image.}
    \label{fig:compare2}
%\end{wrapfigure}
\end{figure*}

\begin{figure*}[t]
    \centering
\includegraphics[width=\textwidth]
{zt2z0_compressed.pdf}
        \caption{The generated results at each timestep of the diffusion process from $\mathbf{\widetilde{z}}_T$ to $\mathbf{\widetilde{z}}_{0,edited}$ during the editing phase.
        Null-text~\cite{mokady2022null} fails to adequately distinguish between the door in the background and the tiger (top).}
    \label{fig:zt2z0}
%\end{wrapfigure}
\end{figure*}



\begin{figure*}[t]
    \centering
\includegraphics[width=\textwidth]
{more_results_supp2.pdf}
        \caption{Examples of StyleDiffusion for editing with   attention injection or prompt refinement.}
    \label{fig:more_results_supp}
%\end{wrapfigure}
\end{figure*}


\begin{figure*}[t]
    \centering
\includegraphics[width=\textwidth]
{more_results_compressed.pdf}
        \caption{Examples of StyleDiffusion for editing with   attention injection or prompt refinement.}
    \label{fig:results3}
%\end{wrapfigure}
\end{figure*}

\begin{figure*}[t]
    \centering
\includegraphics[width=\textwidth]
{results4_compressed.pdf}
        \caption{Examples of StyleDiffusion for editing with   attention injection or prompt refinement.}
    \label{fig:results4}
%\end{wrapfigure}
\end{figure*}

\begin{figure*}[t]
    \centering
\includegraphics[width=\textwidth]
{results2_compressed.pdf}
        \caption{Examples of StyleDiffusion for editing with   attention injection or prompt refinement.}
    \label{fig:results2}
%\end{wrapfigure}
\end{figure*}


\end{document}