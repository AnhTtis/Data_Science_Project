% \begin{table*}[h]
% \small 
% \centering
% \begin{tabular}{lcc}
% \hline 
%  & \multicolumn{1}{c}{BC5CDR-overall}& \multicolumn{1}{c}{BC5CDR-NER} \\
% \textbf{Model} &  P/R/F1   & P/R/F1 \\
% \multicolumn{3}{l}{baselines}\\
% end2end-span-dict\#&-/-/85.1&-/-/86.7 \\
% GENRE* & 68.63/67.83/68.23&84.99/84.00/84.49\\
% entQA \\

% \hline
% \end{tabular}
% \caption{The main results on E2E Entity Linking tasks.* GENRE reproduced by 3000 step, 8bsz, 300 warm-up with shortest name selected.\# We adopted the same version of MEDIC as TaggerOne used, and that we dismissed non-disease entity annotations contained in BC5CDR, and use a relatively loose abstract-level microF1 metric.}
% \label{tab:main}
% \small 
% \end{table*}



% \begin{table*}[h]
% \small 
% \centering
% \begin{tabular}{lccc}
% \hline 
%  & \multicolumn{1}{c}{BC5CDR-overall}& \multicolumn{1}{c}{BC5CDR-NER} & \multicolumn{1}{c}{BC5CDR-correctly predicted NER} \\
%  &  P/R/F1   & P/R/F1 & Linking Accu.\\
%  \hline
% % \multicolumn{3}{l}{GENRE+shortest name train on MeSH+test on MeSH} \\
% %  &70.92/68.71/69.80&87.59/84.86/86.20&80.97 \\
% \multicolumn{3}{l}{GENRE+shortest name train on MeSH+test on MESH$\cap$MEDIC} \\
%  &31.24/67.54/42.72&37.80/81.72/51.69&82.65 \\
% \multicolumn{3}{l}{GENRE+shortest name train on MESH$\cap$MEDIC+test on MESH$\cap$MEDIC} \\
%  &67.89/67.84/67.87&80.33/80.27/80.30&84.52 \\
% %  \multicolumn{3}{l}{GENRE+shortest name train on MESH$\cap$MEDIC+test on MeSH} \\
% %  &67.40/30.40/41.90&80.55/36.33/50.07&83.68\\
%  \hline
%  \multicolumn{3}{l}{GENRE+shortest name train on MeSH+test on MESH$\cap$MEDIC+threshold of -0.15} \\
%  &76.84/59.07/66.79&85.44/65.69/74.27&89.93 \\
%  \hline
% %  \multicolumn{3}{l}{KeBioLM+CODER+train on MeSH+test on MeSH} \\
% %  &72.21/74.84/73.5&86.47/91.05/88.70&82.42\\
%  \multicolumn{3}{l}{KeBioLM+CODER+train on MeSH+test on MESH$\cap$MEDIC} \\
%  &28.83/66.82/40.29&36.24/85.37/50.88&78.63\\
%  \multicolumn{3}{l}{KeBioLM+CODER+train on MESH$\cap$MEDIC+test on MESH$\cap$MEDIC} \\
%  &64.68/66.78/65.71&81.04/85.44/83.18&78.51\\
% %  \multicolumn{3}{l}{KeBioLM+CODER+train on MESH$\cap$MEDIC+test on MeSH} \\
% %  &62.15/28.45/39.03&81.29/38.00/51.79&75.21\\
%  \hline
%  \multicolumn{3}{l}{KeBioLM+CODER+train on MeSH+test on MESH$\cap$MEDIC+threshold of 0.8} \\
%  &81.02/64.07/71.56&90.22/71.35/79.69&89.80 \\
% \hline
% \end{tabular}
% \caption{Probings on knowledge base sensitivity.}
% \label{tab:main:probing}
% \small 
% \end{table*}

\section{Hyper-parameters}\label{app:train_hyper}

We demonstrates the hyper-parameters we used in training three EL models on MedMEntions and BC5CDR in \Cref{tab:model_train_hyper}.
All other hyper-parameters in training and inference that are not mentioned in this table are the same from the public codes and scripts of GENRE\footnote{Github repository of GENRE: \url{https://github.com/facebookresearch/GENRE}}, EntQA\footnote{Github repository of EntQA: \url{https://github.com/WenzhengZhang/EntQA}}, KeBioLM\footnote{Github repository of KeBioLM: \url{https://github.com/GanjinZero/KeBioLM}}, and CODER\footnote{Github repository of CODER: \url{https://github.com/GanjinZero/CODER}}. Models are implemented on single NVIDIA V100 GPU with 32GB memory. 

\begin{table*}[h]
\small 
\centering
\begin{tabular}{lcccc}
\toprule
& KeBioLM & GENRE & EntQA-retriever & EntQA-reader \\ 
\multicolumn{5}{c}{\textbf{BC5CDR}}\\

Train Length & 20 Epochs & 8000 Steps& 20 Epochs & 20 Epochs \\ 
Learning Rate & $1\times 10^{-5}$ & $3\times 10^{-5}$ & $2\times 10^{-6}$& $1\times 10^{-5}$\\
Warmup & 570 & 600 & 20\% & 6\%\\
Batch Size &16 & 8 & 8 & 2\\
Adam $\beta$ & (0.9,0.999) & (0.9,0.999)& (0.9,0.999)& (0.9,0.999) \\
Adam $\epsilon$ & $1\times 10^{-8}$ & $1\times 10^{-8}$& $1\times 10^{-8}$& $1\times 10^{-8}$\\
Weight Decay & 0.0 & 0.01 & 0 & 0\\
Clip Norm & 1.0 & 0.1 & - & -\\
Label Smoothing & 0.0 & 0.1 & - & -\\

\multicolumn{5}{c}{\textbf{MedMentions}}\\

Train Length & 20 Epochs & 8000 Steps & 50 Epochs & 50 Epochs\\ 
Learning Rate & $1\times 10^{-5}$ & $3\times 10^{-5}$ & $5\times 10^{-6}$& $1\times 10^{-5}$\\
Warmup & 570 & 600 & 20\% & 6\% \\
Batch Size &16 & 8 & 8 & 2\\
Adam $\beta$ & (0.9,0.999) & (0.9,0.999) & (0.9,0.999)& (0.9,0.999)\\
Adam $\epsilon$ & $1\times 10^{-8}$ & $1\times 10^{-8}$& $1\times 10^{-8}$& $1\times 10^{-8}$\\
Weight Decay & 0.0 & 0.01 & 0 & 0\\
Clip Norm & 1.0 & 0.1 & - & -\\
Label Smoothing & 0.0 & 0.1 & - & -\\


\bottomrule
\end{tabular}
\caption{The training settings for investigated models on BC5CDR and MedMentions. We leave out CODER as CODER is not further fine-tuned on downstream samples. }
\label{tab:model_train_hyper}
\small 
\end{table*}


\section{Datasets Statistics}\label{app:data_stat}

\Cref{tab:stats} shows the detailed statistics of data we used for partial KB inference. We use MeSH and MEDIC in the BC5CDR corpus\footnote{BC5CDR: \url{https://biocreative.bioinformatics.udel.edu/tasks/biocreative-v/track-3-cdr/}}. 
The BC5CDR dataset has been identified as being free of known restrictions under copyright law.
We use UMLS, MeSH and SNOMED from the 2017 AA release of UMLS.
To meet the assumption that MEDIC forms a subset of MeSH, we ditch the concepts in MEDIC that do not exist in MeSH.
And we use st21pv version of MedMentions\footnote{Github repository of MedMentions: \url{https://github.com/chanzuckerberg/MedMentions}}.
The MedMentions dataset is under CC0 licence.
We follow GenBioEL\footnote{Github repository of GenBioEL: \url{https://github.com/Yuanhy1997/GenBioEL}} for preprocessing the concepts and synonyms in the original KBs.
To meet the assumption that the partial KBs do not contain concepts out of training KB, we ditch the concepts in partial KBs that do not exist in UMLS. 

We use precision, recall, and F1 as metrics for entity linking and mention detection, and accuracy on correctly detected mentions for disambiguation performance. 
We also use the top 100 recall (R@100) to illustrate the performance of EntQA retriever.

\begin{table*}[h]
\small 
\centering
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{lccccc}
\toprule
Target KB & \#Concepts & \#Annotations & \#Annotated Concepts & \#Annot. in Train & \#Concepts in Train \\
\midrule
\multicolumn{6}{c}{\textbf{BC5CDR}}\\
MeSH &268,146 & 9,269/9,511/9,655 & 1,304/1,246/1,299 & -/7,439/7,504 & -/681/691 \\
\MEDICint & 11,209 & 4,149/4,217/4,307 & 652/595/636 & -/3,526/3,655 & -/360/390 \\
\MEDICext & 256,937 & 5,120/5,294/5,348  & 652/651/663 & -/3,913/3,849 & -/321/301  \\
\midrule
\multicolumn{6}{c}{\textbf{MedMentions}} \\
UMLS & 2,368,641 & 122,241/40,884/40,157 & 18,520/8,643/8,457 & -/9,320/9,072& -/3,659/3,590 \\
\SNOMEDint & 342,998 & 74,272/25,385/24,391 & 9,678/4,768/4 ,716 & -/4,556/4,766 & -/1,807/1,779\\
\SNOMEDext &2,025,643 &47,969/15,499/15,766&8,842/3,875/3,741&-/4,554/4,516 & -/1,852/1,811 \\
\TAint & 184,939 & 25,109/8,240/8,117 & 3,805/1,797/1,741 & -/1,822/1,741 & -/734/721 \\
\TAext & 2,183,702 &97,132/32,644/32,040  &14,715/6,846/6,716  &  -/7,498/7,330 & -/2,925/2,869 \\
\TBint & 122,433 & 14,835/4,682/4,789 & 2,382/1,104/1,100 & -/1,000/1,051 & -/454/440 \\
\TBext & 2,246,208 &107,406/36,202/35,368  &16,138/7,539/7,357  &-/8,320/8,021  & -/3,205/3,150 \\
\bottomrule
\end{tabular}}
\caption{Dataset and corresponding knowledge base statistics.}
\label{tab:stats}
\small 
\end{table*}



\section{Appendix for Redemption Methods}

\subsection{Illustrative Example}\label{app:example}
We show an entity linking result on an example from BC5CDR:

\textit{Indomethacin induced {hypotension} in {sodium and volume} depleted rats. After a single oral dose of 4 mg/kg {indomethacin} ({IDM}) to {sodium} and volume depleted rats plasma renin activity (PRA) and systolic blood pressure fell significantly within four hours.}

The entity linking results are shown in Table \ref{tab:case}. In Post-Pruning, the final results (marked blue) are those linked to a concept in the partial KB MEDIC. In Thresholding, the final results (marked blue) are those scores larger than a fix threshold, for KeBioLM+CODER is 0.8, GENRE is -0.15 and EntQA is 0.043.

\begin{table*}[h]
    \centering
    \scriptsize
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{l|ccc|cccc}
                \hline
                 \multirow{2}{*}{Methods}&\multicolumn{3}{c|}{Post-Pruning}&\multicolumn{4}{c}{Thresholding} \\
         & Mention Span & Concept & In Partial KB & Mention Span & Concept & Score & $\ge$Threshold \\
         \hline
        \multirow{6}{*}{Ke.+CO.}&(0,12)& D007213:indomethacin& False&(0,12)& C564365:ilvasc& 0.35& False \\
        &(21,32)& D007022:hypotension& \textcolor{blue}{True}&(21,32)& D007022:hypotension& 1.00& \textcolor{blue}{True} \\
        &(36,53)& D005441:fluids and secretions& False&(36,53)& D003681:water stress& 0.43& False \\
        &(105,117)& D007213:indomethacin& False&(105,117)& C564365:ilvasc& 0.35& False \\
        &(119,122)& D003922:iddm& \textcolor{blue}{True}&(119,122)& D003922:iddm& 0.94& \textcolor{blue}{True} \\
        &(127,133)& D012964:sodium& False&(127,133)& D000747:chloroses& 0.38& False \\
         \hline
         \multirow{5}{*}{GENRE}&(0,12)& D007213:amuno& False&(21,32)& D007022:hypotension& -0.067& \textcolor{blue}{True}\\
         &(21,32)& D007022:hypotension& \textcolor{blue}{True}&(36,42)& D007022:hypotension& -1.349& False \\
         &(36,42)& D012964:sodium& False &(105,117)& C563086:amc syndrome& -1.848& False \\
         &(105,117)& D007213:amuno& False &(127,133)& D007022:hypotension& -0.491& False  \\
         &(127,133)& D012964:sodium& False \\
         \hline
    \multirow{3}{*}{EntQA}&(0,12)& D007213:indomethacin& False&(0,12)& C564365:ilvasc& 0.087& \textcolor{blue}{True}  \\
        &(21,32)& D007022:hypotension& \textcolor{blue}{True}&(21,32)& D007022:hypotension& 0.098& \textcolor{blue}{True} \\
        &(127,133)& D012964:sodium& False&(127,133)& D000747:chloroses& 0.004& False \\
         \hline
    \end{tabular}
    }
    \caption{An illustrative example of Post-Pruning and Thresholding.}
    \label{tab:case}
\end{table*}

% \begin{table*}[h]
%     \centering
%     \scriptsize
%     \resizebox{0.9\textwidth}{!}{
%     \begin{tabular}{p{0.2\columnwidth}|p{0.8\columnwidth}}
%          Raw Text&Indomethacin induced{hypotension} in {sodium and volume} depleted rats. After a single oral dose of 4 mg/kg {indomethacin} ({IDM}) to {sodium} and volume depleted rats plasma renin activity (PRA) and systolic blood pressure fell significantly within four hours .\\
%          \hline
%          Post-Pruning&\textcolor{blue}{'Indomethacin', 'D007213:indomethacin', False} \\
%          &\textcolor{red}{'hypotension', 'D007022:hypotension', \textcolor{blue}{True}} \\
%          &\textcolor{blue}{'sodium and volume', 'D005441:fluids and secretions', False} \\
%          &\textcolor{blue}{'indomethacin', 'D007213:indomethacin', False} \\
%          &\textcolor{red}{'IDM', 'D003922:iddm', \textcolor{blue}{True}} \\
%          &\textcolor{blue}{'sodium', 'D012964:sodium', False} \\
%          \hline
%          Thresholding&\textcolor{blue}{'Indomethacin', 'C564365:ilvasc', 0.35} \\
%          &\textcolor{red}{'hypotension', 'D007022:hypotension', 1.00} \\
%          &\textcolor{blue}{'sodium and volume', 'D003681:'water stress', 0.43} \\
%          &\textcolor{blue}{'indomethacin', 'C564365:ilvasc', 0.35} \\
%          &\textcolor{red}{'IDM', 20, 1, 1, 'D003922', 'iddm', 0.94} \\
%          &\textcolor{blue}{'sodium', 'D000747:chloroses', 0.38} \\
%          \hline\hline
%          Post-Pruning&\textcolor{blue}{'Indomethacin', 'D007213:amuno', False} \\
%          &\textcolor{red}{'hypotension', 'D007022:hypotension', \textcolor{blue}{True}} \\
%          &\textcolor{blue}{'sodium', 'D012964:sodium', False} \\
%          &\textcolor{blue}{'indomethacin', 'D007213:amuno', False} \\
%          &\textcolor{blue}{'sodium', 'D012964:sodium', False} \\
%          \hline
%          Thresholding&\textcolor{red}{'hypotension', 'D007022:hypotension', -0.067} \\
%          &\textcolor{blue}{'sodium', 'D007022:hypotension', -1.349} \\
%          &\textcolor{blue}{'indomethacin', 'C563086:amc syndrome', -1.848} \\
%          &\textcolor{blue}{'sodium', 'D007022:hypotension', -0.491} \\
         
%     \end{tabular}
%     }
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table*}
% \subsection{Detailed on the Thresholding Score}

% We give a detailed explanation of the scores used for thresholding in the three models.




\subsection{Results on Different Datasets}\label{app:redeem_result}



\Cref{tab:main_transfer_enhance_app} shows results of the same experiments described in \Cref{sec:post-processing} on MedMentions with partial KB \SNOMEDint and \SNOMEDext.
Results on these table also supports the conclusion we provides at \Cref{sec:conclusion}.
We find thresholding and post-pruning benefit EntQA in this additional results whereas we witness a significantly performance drop in \Cref{tab:main_transfer_enhance}.
This suggests performance of thresholding and post-pruning on EntQA is different across partial KBs.
Nevertheless, we have not seen a dramatic performance boost (as those in GENRE and KeBioLM+CODER) brought by post-processing techniques on EntQA.


\begin{table*}[h]
\small 
\centering
\setlength{\tabcolsep}{1.2mm}{
\begin{tabular}{p{2mm}l|cccc|cccc}
\toprule
% &  & \multicolumn{6}{c}{BC5CDR on MeSH} \\
&& \multicolumn{4}{c}{\SNOMEDint}\vline & \multicolumn{4}{c}{\SNOMEDext} \\
&& EL-P/R&EL-F1&NER-F1&NED-Acc& EL-P/R&EL-F1&NER-F1&NED-Acc \\
\hline
\cellcolor{blue!10}& In-KB Train &53.10/26.28&\underline{35.16}&64.37&76.92&44.77/23.99&\textbf{31.24}&\textbf{67.40} & \textbf{70.12}\\
\cellcolor{blue!10}& Partial KB Inference &46.04/27.01&34.05&63.54&74.81& 36.75/23.12&28.38 & 64.53 & 68.67 \\
\cellcolor{blue!10}& \ \ +w/ Thresholding &44.92/30.01&\textbf{35.98} &\textbf{64.98}&\textbf{79.10}& 35.10/27.33&\underline{30.73}&\underline{65.73} & \underline{69.04} \\
\cellcolor{blue!10}\multirow{-4}{*}{\rotatebox[origin=c]{90}{EntQA}}& \ \ +w/ Post-pruning&44.70/28.23&34.45&\underline{64.77}&\underline{78.82}&35.48/26.92&30.61 &65.71&68.88\\
\hline
\cellcolor{blue!10}& In-KB Train &46.15/39.95&42.83&53.38&\textbf{80.21}&42.24/25.68&31.94 &44.77&\underline{71.35}\\
\cellcolor{blue!10}& Partial KB Inference &34.40/49.40&40.56&53.97 &75.14& 19.82/39.28&26.35&40.33 &65.33 \\
\cellcolor{blue!10}& \ \ +w/ Thresholding & 45.25/44.25&\underline{44.75}&\underline{56.38} &\underline{79.37}&30.45/34.09&\underline{32.17}&\underline{45.13}&71.26\\
\cellcolor{blue!10}\multirow{-4}{*}{\rotatebox[origin=c]{90}{GENRE}}& \ \ +w/ Post-pruning & 46.20/47.18&\textbf{46.68}&\textbf{59.05}&79.07& 38.27/36.56&\textbf{37.39}&\textbf{51.64}&\textbf{72.41}\\
\hline
\cellcolor{blue!10}& In-KB Train  &43.87/44.62&\underline{44.24}&\textbf{64.00}&69.13&30.62/30.36&\textbf{30.49}&\textbf{55.41}&55.02 \\
\cellcolor{blue!10}& Partial KB Inference &28.19/48.28&35.59&54.58&65.22&14.18/37.54&20.59 &39.11 &52.64\\
\cellcolor{blue!10}& \ \ +w/ Thresholding &53.48/44.64&\textbf{48.66}&\underline{61.42}&\textbf{79.23}&28.54/31.33&\underline{29.87}&44.37&\textbf{67.33}  \\
\cellcolor{blue!10}\multirow{-4}{*}{\rotatebox[origin=c]{90}{Ke.+CO.}}& \ \ +w/ Post-pruning&46.86/33.73&39.22&51.44&\underline{76.26}&23.65/36.27&28.63&\underline{50.06}&\underline{57.19}\\
\bottomrule
\end{tabular}
}
\caption{Additional Results of partial KB inference, In-KB training and two redemption methods for three investigated models. The results are evaluated on partial KB \SNOMEDint and \SNOMEDext in MedMentions.
The best performance for a model in each dataset is identified with \textbf{bold} and the second is \underline{underlined}.}
\label{tab:main_transfer_enhance_app}
\small 
\end{table*}















\begin{comment}
\section{Initial results of models in various settings}

In this appendix section, we display our initial results, which are an aggregation of results we analysis in this paper.

\begin{table*}[h]
\small 
\centering
\begin{tabular}{lcccccc}
\hline 
 Trainset/KB & Testset/KB & \multicolumn{1}{c}{Overall}&\multicolumn{1}{c}{MentionDetection/NER}& \multicolumn{1}{c}{EntityRetrieval} \\
 & &  P/R/F1   & P/R/F1 & Top100 Recall\\
 \hline
 \multicolumn{5}{l}{\textbf{EntQA}} \\
  MESH & MESH$\cap$MEDIC & 81.92/70.45/75.75 & / & 88.72 \\
  MESH & MESH$\backslash$MEDIC & 87.10/66.92/75.69 & / & 77.73 \\
  MESH$\cap$MEDIC&MESH$\cap$MEDIC & 81.27/71.34/75.98 & / & 91.09 \\
  MESH$\backslash$MEDIC&MESH$\backslash$MEDIC & 86.87/69.30/77.10 & / & 80.61 \\
  \multicolumn{5}{l}{\textbf{EntQA+Threshold}} \\
  MESH & MESH$\cap$MEDIC & 60.83/69.44/64.85 & / & 85.00 \\
  MESH & MESH$\backslash$MEDIC & 79.64/66.55/72.51 & / & 75.64 \\
  \multicolumn{5}{l}{\textbf{EntQA+PostProcessing}} \\
  MESH & MESH$\cap$MEDIC & 62.97/64.99/63.96 & / & 85.00 \\
  MESH & MESH$\backslash$MEDIC & 80.02/63.11/70.57 & / & 75.64 \\
  
  \hline
 \multicolumn{5}{l}{\textbf{GENRE}} \\
  MESH & MESH$\cap$MEDIC &31.53/68.19/43.12 &37.85/81.84/51.76  &-\\
  MESH & MESH$\backslash$MEDIC &37.55/65.33/47.69&49.07/85.38/62.32&-\\
  MESH$\cap$MEDIC&MESH$\cap$MEDIC &65.65/68.38/66.99 &77.00/80.20/78.56 &-\\
  MESH$\backslash$MEDIC&MESH$\backslash$MEDIC &69.96/62.02/65.75&90.99/80.67/85.52&-\\
  \multicolumn{5}{l}{\textbf{GENRE+Threshold}} \\
  MESH & MESH$\cap$MEDIC &76.32/59.25/66.71&82.86/64.34/72.43&-\\
  MESH & MESH$\backslash$MEDIC &69.05/56.99/62.45&82.78/68.32/74.86&-\\
  \multicolumn{5}{l}{\textbf{GENRE+PostProcessing}} \\
  MESH & MESH$\cap$MEDIC &69.31/68.59/68.95&80.34/79.50/79.92&-\\
  MESH & MESH$\backslash$MEDIC & 69.46/66.29/67.83&88.54/84.50/86.47&-\\
  
  \hline
 \multicolumn{5}{l}{\textbf{KeBioLM+CODER}} \\
 %\multirow{6}{*}{EntQA} & \multirow{4}{*}{MESH} & MESH & MESH & & 74.8 & \\
 %&  & MESH & MESH$\cap$MEDIC & & 60.2 &\\
 %&  & MESH$\cap$MEDIC & MESH & & 49.3 &\\
  MESH & MESH$\cap$MEDIC &29.24/68.38/40.96 &36.46/86.46/51.29 &-\\
  MESH & MESH$\backslash$MEDIC &42.57/80.67/55.73&50.13/94.99/65.63 &-\\
  MESH$\cap$MEDIC&MESH$\cap$MEDIC &63.98/68.47/66.15 &79.50/86.70/82.94 &-\\
  MESH$\backslash$MEDIC&MESH$\backslash$MEDIC &77.52/80.65/79.05& 91.01/94.69/92.82 &-\\
  
  \multicolumn{5}{l}{\textbf{KeBioLM+CODER+Threshold}} \\
  MESH & MESH$\cap$MEDIC &79.20/65.08/71.45&86.97/71.47/78.46 &-\\
  MESH & MESH$\backslash$MEDIC &86.32/77.04/81.41&88.37/78.87/83.35 &-\\
  \multicolumn{5}{l}{\textbf{KeBioLM+CODER+PostProcessing}} \\
  MESH & MESH$\cap$MEDIC &69.03/65.27/67.10&80.75/76.34/78.48&-\\
  MESH & MESH$\backslash$MEDIC &69.17/80.67/74.48&81.05/94.52/87.27&-\\
 %& & MESH$\cap$MEDIC & MESH & & 63.5 &\\
 %& & MESH & MESH$\cap$MEDIC & & 71.0 &\\
 %& & MESH & MESH & & 34.0 &\\
\hline
\end{tabular}
\caption{Probings on knowledge base sensitivity.}
\label{tab:main:probing}
\small 
\end{table*}

\begin{table*}[h]
\small 
\centering
\begin{tabular}{lcccccc}
\hline 
 Trainset/KB & Testset/KB & \multicolumn{1}{c}{Overall}&\multicolumn{1}{c}{MentionDetection/NER}& \multicolumn{1}{c}{EntityRetrieval}&Disambiguation \\
 & &  P/R/F1   & P/R/F1 & Top100 Recall&Accuracy\\
 \hline
 \multicolumn{5}{l}{\textbf{EntQA}} \\
  UMLS & UMLS & 45.99/23.68/31.27 & - & 57.26 &\\
 UMLS & UMLS$\cap$SNOMED & 46.04/27.01/34.05 &-& 65.86 & \\
 UMLS & UMLS$\backslash$SNOMED & 36.75/23.12/28.38 &- & 61.72 & \\
 UMLS$\cap$SNOMED & UMLS$\cap$SNOMED & 53.10/26.28/35.16 &-& 74.33 & \\
 UMLS$\backslash$SNOMED&UMLS$\backslash$SNOMED& 44.77/23.99/31.24 &-& 66.77 & \\
 
  \multicolumn{5}{l}{\textbf{EntQA+Threshold}} \\
  UMLS & UMLS$\cap$SNOMED & 44.92/30.01/35.98 &-&  58.57 & \\
 UMLS & UMLS$\backslash$SNOMED & 35.10/27.33/30.73 &- & 55.30 & \\
 
 \multicolumn{5}{l}{\textbf{EntQA+PostProcessing}} \\
  UMLS & UMLS$\cap$SNOMED & 44.70/28.23/34.45 &-& 58.57 & \\
 UMLS & UMLS$\backslash$SNOMED & 35.48/26.92/30.61 &- & 55.30 & \\
  
  \hline
 \multicolumn{5}{l}{\textbf{GENRE}} \\
 UMLS & UMLS & 42.44/43.69/43.05 &64.27/66.17/65.21&-&\\
 UMLS & UMLS$\cap$SNOMED &34.40/49.40/40.56&45.78/65.74/53.97&-& \\
 UMLS & UMLS$\backslash$SNOMED &19.82/39.28/26.35&30.34/60.13/40.33&-& \\
 UMLS$\cap$SNOMED & UMLS$\cap$SNOMED &46.15/39.95/42.83&57.53/49.80/53.38&-& \\
 UMLS$\backslash$SNOMED&UMLS$\backslash$SNOMED&42.24/25.68/31.94&59.20/35.99/44.77&-& \\
 
  \multicolumn{5}{l}{\textbf{GENRE+Threshold}} \\
  UMLS & UMLS$\cap$SNOMED & 45.25/44.25/44.75&57.01/55.75/56.38&-& \\
 UMLS & UMLS$\backslash$SNOMED &30.45/34.09/32.17&42.73/47.83/45.13&-& \\
   \multicolumn{5}{l}{\textbf{GENRE+PostProcessing}} \\
  UMLS & UMLS$\cap$SNOMED & 46.20/47.18/46.68&58.43/59.67/59.05&-& \\
 UMLS & UMLS$\backslash$SNOMED &38.27/36.56/37.39&52.85/50.49/51.64&-& \\
  \hline
 \multicolumn{5}{l}{\textbf{KeBioLM+CODER}} \\
 UMLS&UMLS&33.58/34.94/34.25&69.08/71.88/70.45&-&\\
 UMLS & UMLS$\cap$SNOMED & 28.19/48.28/35.59&43.22/74.04/54.58&-& \\
 UMLS & UMLS$\backslash$SNOMED &14.18/37.54/20.59&26.94/71.32/39.11&-& \\
 UMLS$\cap$SNOMED & UMLS$\cap$SNOMED &43.87/44.62/44.24&63.46/64.55/64.00&-& \\
 UMLS$\backslash$SNOMED & UMLS$\backslash$SNOMED &30.62/30.36/30.49&55.65/55.17/55.41&-& \\
  \multicolumn{5}{l}{\textbf{KeBioLM+CODER+Threshold}} \\
  UMLS & UMLS$\cap$SNOMED & 53.48/44.64/48.66&67.5/56.35/61.42&-&\\
 UMLS & UMLS$\backslash$SNOMED & 28.54/31.33/29.87&42.39/46.54/44.37&-&\\
  
   \multicolumn{5}{l}{\textbf{KeBioLM+CODER+PostProcessing}} \\
  UMLS & UMLS$\cap$SNOMED & 46.86/33.73/39.22&61.45/44.23/51.44&-&\\
 UMLS & UMLS$\backslash$SNOMED & 23.65/36.27/28.63&41.35/63.42/50.06&-&\\
\hline
\end{tabular}
\caption{Probings on knowledge base sensitivity. Disambiguation accuracy = Overall Precision / NER Precision.}
\label{tab:main:probing}
\small 
\end{table*}





\begin{table*}[h]
\small 
\centering
\begin{tabular}{lcccccc}
\hline 
 Trainset/KB & Testset/KB & \multicolumn{1}{c}{Overall}&\multicolumn{1}{c}{MentionDetection/NER}& \multicolumn{1}{c}{EntityRetrieval}&Disambiguation \\
 & &  P/R/F1   & P/R/F1 & Top100 Recall&Accuracy\\
 \hline
 \multicolumn{5}{l}{\textbf{EntQA}} \\
 UMLS & UMLS$\cap$T038 & 41.52/31.56/35.86 &-& 75.10 & \\
 UMLS & UMLS$\backslash$T038 & 43.43/23.24/30.28 &- & 58.54 & \\
 UMLS$\cap$T038 & UMLS$\cap$T038 &  54.63/27.52/36.60 &-&82.39& \\
 UMLS$\backslash$T038 & UMLS$\backslash$T038 & 49.29/16.69/24.93 &-&46.46& \\
 
  \multicolumn{5}{l}{\textbf{EntQA+Threshold}} \\
  UMLS & UMLS$\cap$T038 & 15.43/24.01/18.53 &-& 61.70 & \\
 UMLS & UMLS$\backslash$T038 & 38.41/20.97/27.05 &- & 56.14 & \\
 
 \multicolumn{5}{l}{\textbf{EntQA+PostProcessing}} \\
  UMLS & UMLS$\cap$T038 & 23.87/27.92/25.74 &-& 61.70 & \\
  UMLS & UMLS$\backslash$T038 & 42.09/20.17/27.27 &- & 56.14 & \\
  
  \hline
 \multicolumn{5}{l}{\textbf{GENRE}} \\
 UMLS & UMLS$\cap$T038 &17.26/49.53/25.60&22.34/64.10/33.13&-& \\
 UMLS & UMLS$\backslash$T038 &34.97/42.45/38.35&53.44/64.86/58.60&-& \\
 UMLS$\cap$T038 & UMLS$\cap$T038 &44.58/42.86/43.70&57.18/54.97/56.05&-& \\
 UMLS$\backslash$T038&UMLS$\backslash$T038&43.52/39.27/41.28&64.80/58.48/61.48&-& \\
 
  \multicolumn{5}{l}{\textbf{GENRE+Threshold}} \\
  UMLS & UMLS$\cap$T038 &30.76/42.10/35.54 &37.10/50.78/42.88&-& \\
 UMLS & UMLS$\backslash$T038 &43.07/39.43/41.17&62.29/57.01/59.54&-& \\
   \multicolumn{5}{l}{\textbf{GENRE+PostProcessing}} \\
  UMLS & UMLS$\cap$T038 & 51.19/49.43/50.29 &62.86/60.69/61.75&-& \\
 UMLS & UMLS$\backslash$T038 & 41.17/42.00/41.58&62.13/63.38/62.75&-& \\
  \hline
 \multicolumn{5}{l}{\textbf{KeBioLM+CODER}} \\
 UMLS & UMLS$\cap$T038 &9.78/50.28/16.37 &14.85/76.36/24.86&-& \\
 UMLS & UMLS$\backslash$T038 &26.52/34.59/30.02&55.26/72.07/62.56&-& \\
 UMLS$\cap$T038 & UMLS$\cap$T038 &46.13/47.31/46.71&67.88/69.61/68.73&-& \\
 UMLS$\backslash$T038 & UMLS$\backslash$T038 &32.57/33.72/33.14&67.95/70.35/69.13&-& \\
  \multicolumn{5}{l}{\textbf{KeBioLM+CODER+Threshold}} \\
  UMLS & UMLS$\cap$T038 & 56.04/45.08/49.97&68.3/54.93/60.89&-&\\
 UMLS & UMLS$\backslash$T038 & 37.17/32.91/34.91&66.45/58.85/62.42&-&\\
    
     \multicolumn{5}{l}{\textbf{KeBioLM+CODER+PostProcessing}} \\
  UMLS & UMLS$\cap$T038 & 47.75/39.69/43.35&60.44/50.24/54.87&-&\\
 UMLS & UMLS$\backslash$T038 & 31.14/34.05/32.53&63.76/69.73/66.61&-&\\
  
\hline
\end{tabular}
\caption{Probings on knowledge base sensitivity. Disambiguation accuracy = Overall Precision / NER Precision.}
\label{tab:main:probing}
\small 
\end{table*}


\begin{table*}[h]
\small 
\centering
\begin{tabular}{lcccccc}
\hline 
 Trainset/KB & Testset/KB & \multicolumn{1}{c}{Overall}&\multicolumn{1}{c}{MentionDetection/NER}& \multicolumn{1}{c}{EntityRetrieval}&Disambiguation \\
 & &  P/R/F1   & P/R/F1 & Top100 Recall&Accuracy\\
 \hline
 \multicolumn{5}{l}{\textbf{EntQA}} \\
 UMLS & UMLS$\cap$T058 & 30.01/25.56/27.61 &-& 74.28 & \\
 UMLS & UMLS$\backslash$T058 & 46.02/24.34/31.84 &- & 58.76 & \\
 UMLS$\cap$T058 & UMLS$\cap$T058 &38.15/24.22/29.63&-&81.74& \\
 UMLS$\backslash$T058 & UMLS$\backslash$T058 &55.05/17.76/26.86 &-&47.76 & \\
 
  \multicolumn{5}{l}{\textbf{EntQA+Threshold}} \\
  UMLS & UMLS$\cap$T058 & 9.98/20.87/13.41 &-& 58.02 & \\
 UMLS & UMLS$\backslash$T058 & 42.03/20.01/27.13 &- & 57.15  & \\
 
 \multicolumn{5}{l}{\textbf{EntQA+PostProcessing}} \\
 UMLS & UMLS$\cap$T058 & 14.89/22.03/17.77 &-& 58.02 & \\
 UMLS & UMLS$\backslash$T058 & 44.77/21.97/29.48 &- & 57.15 & \\
  
  \hline
 \multicolumn{5}{l}{\textbf{GENRE}} \\
 UMLS & UMLS$\cap$T058 &7.69/36.06/12.68&11.37/53.31/18.75&-& \\
 UMLS & UMLS$\backslash$T058 &40.45/44.76/42.50&59.90/66.30/62.94&-& \\
 UMLS$\cap$T058 & UMLS$\cap$T058 &30.30/33.24/31.70&41.99/46.06/43.93&-& \\
 UMLS$\backslash$T058&UMLS$\backslash$T058&43.68/42.96/43.31&64.20/63.15/63.67&-& \\
 
  \multicolumn{5}{l}{\textbf{GENRE+Threshold}} \\
  UMLS & UMLS$\cap$T058 &13.62/31.18/18.96&18.91/43.29/26.32&-& \\
 UMLS & UMLS$\backslash$T058 &47.81/42.04/44.74&67.21/59.10/62.89&-& \\
   \multicolumn{5}{l}{\textbf{GENRE+PostProcessing}} \\
  UMLS & UMLS$\cap$T058 & 38.71/34.52/36.49&52.06/46.42/49.08&-&\\
 UMLS & UMLS$\backslash$T058 &43.33/44.37/43.85&63.63/65.16/64.38& -&\\
  \hline
 \multicolumn{5}{l}{\textbf{KeBioLM+CODER}} \\
 UMLS & UMLS$\cap$T058 &4.76/41.51/8.54&7.76/67.68/13.93&-& \\
 UMLS & UMLS$\backslash$T058 &31.95/37.74/34.61&62.35/73.65/67.53&-& \\
 UMLS$\cap$T058 & UMLS$\cap$T058 &38.54/37.46/37.99&-& \\
 UMLS$\backslash$T058 & UMLS$\backslash$T058 &35.84/37.15/36.48&69.87/72.42/71.12&-& \\
  \multicolumn{5}{l}{\textbf{KeBioLM+CODER+Threshold}} \\
  UMLS & UMLS$\cap$T058 & 35.85/30.38/32.89&43.51/36.88/39.92&-&\\
 UMLS & UMLS$\backslash$T058 &42.24/35.81/38.76&71.51/60.63/65.62& -&\\
  \multicolumn{5}{l}{\textbf{KeBioLM+CODER+PostProcessing}} \\
  UMLS & UMLS$\cap$T058 & 27.7/28.92/28.3&37.96/39.63/38.78&-&\\
 UMLS & UMLS$\backslash$T058 &34.76/36.15/35.44&67.02/69.69/68.33& -&\\
  
\hline
\end{tabular}
\caption{Probings on knowledge base sensitivity. Disambiguation accuracy = Overall Precision / NER Precision.}
\label{tab:main:probing}
\small 
\end{table*}
\end{comment}