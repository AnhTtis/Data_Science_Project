There are three widely-used paradigms for entity linking: (1) \textit{NER-NED}; (2) \textit{NED-NER}; (3) \textit{Simultaneous Generation}.
We introduce representative methods for each paradigm and how methods are accommodated to partial KB inference with minimal change. 
\textbf{To be noticed, these paradigms are not aware of the KB $\mathcal{E}_1$ during partial KB inference.}
The top subgraph in \Cref{fig:method_overview} depicts the overview of the three paradigms.
We also describe how directly applying these methods to partial KB inference, which corresponds to the Direct inference method in \Cref{fig:method_overview}.
% To notice, following 
Hyper-parameters for experiments are reported in \Cref{app:train_hyper}.

\subsubsection{NER-NED}\label{sec:NER-NED}

A straightforward solution for entity linking is a two-phase paradigm that first detects the entity mentions by NER models and then disambiguates the mentions to concepts in KBs by NED models, shown in the left top subgraph of \Cref{fig:method_overview}.
We finetune a pre-trained biomedical language model for token classification as the NER model in this paradigm.
Specifically, we use KeBioLM \cite{yuan-etal-2021-improving} as our language model backbone.
We use CODER \cite{yuan2022coder} as our NED model which is a self-supervised biomedical entity normalizer pre-trained on UMLS synonyms with contrastive learning.
CODER disambiguates mentions by generating embedding from each concept synonym and recognized mentions into dense vectors and then finding the nearest concept neighbors of each mention vector by maximum inner product search (MIPS).

% For partial KB inference, the change of target KBs has no influence on KeBioLM as NER and NED are decoupled in this paradigm. 
In partial KB inference, although the NER model is not aware of the changes in KB, the NED model only needs to search for the nearest concept within a partial KB.
Smaller inference KB is challenging for the NED model. 
For a mention $m$ and its corresponding concept $e\in\mathcal{E}_1$, if $e\notin\mathcal{E}_2$, the NED model will return an incorrect or less accurate concept from $\mathcal{E}_2$. 
Since the users are only interested in concepts within $\mathcal{E}_2$, these kinds of mention $m$ should be linked as unlinkable entities (NIL).
% Therefore, in the scenario, only concepts synonyms corresponding to the partial KB are embedded to the dense vectors along with mentions.  
 




% \subsubsection{NER-NED}

% As mentioned before, a great number of research focuses on the separated subtasks NER and NED respectively.
% Therefore, a trivial and straight-forward solution for entity linking is a two-phase framework that first recognizes the entity mentions in the text then disambiguate the mentions to concepts from knowledge bases.

% Recent neural NER methods take texts $s$ as input, and use a pretrained language models to encode texts.
% Token or span classification is conducted on hidden representations for entity recognition.
% % We combine the state-of-the-art methods in the biomedical domain, KeBioLM~\cite{yuan-etal-2021-improving} and CODER~\cite{coder}, for NER and NED respectively.
% % KeBioLM is a biomedical knowledge enhanced pre-trained language model which is pre-trained using NER task on large weakly supervised ScispaCy-annotated PubMed corpora.
% % We fine-tune the pretrained language models using cross-entropy on the BIO labels 
% % $\boldsymbol{l} = \{l_1,...,l_n\}$:
% % \begin{equation*}
% %     L =-\sum_{i=1}^n\log(l_i|\boldsymbol{s}).
% % \end{equation*}
% % We use KeBioLM \cite{yuan-etal-2021-improving} as our pretrained language model.

% We take self-supervised biomedical term normalizer \cite{sapbert,coder} as our NED model.
% These self-supervised term normalizer trained with synonyms and relations of concepts from UMLS using contrastive learning.
% Self-supervised biomedical term normalizer takes the synonyms of concepts as inputs and embeds each into a common dense space and training with contrastive learning by pulling close the synonyms embedding from the same concepts while pushing away those from different concepts under cosine similarities.
% In inference, self-supervised biomedical term normalizer first generates embedding from each concept synonyms and recognized mentions into dense vectors, and then finds the nearest concept neighbors of each mention vector by maximum inner product search (MIPS).
% Formally, for a mention entity $\{x_i,..,x_j\}$ recognized by the NER model, the target concept $e_{tar}$ is given by:
% \begin{align*}
%      &e_{tar}= \arg \max_{e\in\mathcal{E}} \text{Cosine}(H(e),H(\{x_i,..,x_j\})).
% \end{align*}
% where $H(\cdot): \mathcal{V}^{m'}\to\mathcal{R}^d$ represents the mapping of CODER that embeds into language tokens into $d$-dimensional dense vectors.

\subsubsection{NED-NER}\label{sec:NED-NER}

NED-NER methods are also formatted as a  two-phase pipeline, which is shown in the middle top subgraph of \Cref{fig:method_overview}.
This paradigm first retrieves the concepts mentioned in the text, then identifies mentions based on retrieved concepts. 
This paradigm is proposed along with the method EntQA \cite{zhang2022entqa}. 
In the concept retrieval phase of EntQA, a retriever finds top-K related concepts for texts by embedding both into a common dense space using a bi-encoder, then searches nearest neighbors for texts by MIPS within the partial KB $\mathcal{E}_2$.
This phase retrieves concepts from raw texts directly and we view it as the NED phase.
Following its original setting, We initialize the retriever from BLINK \cite{wu2019scalable} checkpoints and further fine-tune the bi-encoder on our datasets with its contrastive loss functions.
In the following phase, a reader is trained to identify mentions in a question-answering fashion where mentions and concepts correspond to answers and queries respectively. 
This phase is viewed as NER.
In partial KB inference, only concepts from the partial KB  will be encoded into dense vectors for MIPS. 
% This will guarantee that retrieved concepts and final results are all from the partial KB. 

% \subsection{NED-NER}

% NED-NER methods are also formatted as two-phase pipelines for entity linking.
% Given it is unnatural and challenged to mine mentions without knowing entities in conventional methods~\cite{zhang2022entqa}, NED-NER methods first retrieve candidate entities from the knowledge base with a fast retrieval module.
% Then, a reading module takes one of candidate entities as query to identify the mention spans in context.
% This idea shares obvious common grounds with open-domain question answering (ODQA)~\cite{chen2020open,zhu2021retrieving}.
% Inspired by dense retrievers~\cite{karpukhin-etal-2020-dense,xiong2020answering} that are widely employed to retrieve relevant contexts in ODQA, \citet{zhang2022entqa} proposed EntQA to tackle entity linking with a ODQA framework.
% The first phrase of EntQA is entity retrieval, which aims to retrieve relevant entities from large scale knowledge bases given contexts that include entity mentions.
% EntQA trains a biencoder retriever between context and entity descriptions.
% It takes advantages from BLINK~\cite{wu2019scalable} by using it as initialization and further elaborate it with a multi-label variant of noise contrastive estimation.
% The retriever will retrieve top-K candidate entities from the knowledge base.
% And a reader is trained for identifying mention spans in context or reject this entity if it does not occur in context. 
% extend more about top-K candidates and reader inference


\subsubsection{Simultaneous Generation}\label{sec:generation}

In the generative paradigm for entity linking, NER and NED are achieved simultaneously, which is shown in the right top subgraph of \Cref{fig:method_overview}.
Entity linking is modeled as a sequence-to-sequence (seq2seq) task where models insert special tokens and concept names into texts with a constrained decoding technique via a Trie.
% The final results are derived by resolving generated sequences by regrex. 
We follow the detailed model design in GENRE.
Given a input text $\boldsymbol{s}$, the target sequence is built as: $s^{\text{tar}} = \{\ldots,M^B,x_i,\ldots x_j,M^E,E^B,e,E^E,\ldots\}$,
where $x_i,\ldots x_j$ are the mention tokens in $\boldsymbol{s}$, $e$ is a token sequence of the concept name, and $M^B,M^E,E^B,E^E$ are special tokens marking the beginning and ending of mentions and concepts. 
The model is trained in seq2seq fashion by maximum log-likelihood with respect to each token. 
During inference, a token prefix trie is built to constrain model only output concepts within the given KB. 
For partial KB inference, only concept names from the partial KB are added to build the prefix Trie in GENRE. 
This will ensure all entity linking results will only be referred to the partial KB.


% \subsection{Simultaneous Generation}
% Given the vast number of concepts in a biomedical knowledge base (e.g., millions in UMLS), searching for the corresponding concepts in the embedding space requires large computational resource such as memory footprint.
% In the current literature for information extraction, generative methods have advanced both NER and NED task to a new state-of-the-art.
% GENRE is a recently proposed generative entity linking method in general domain.
% In a sequence-to-sequence manner, given a input text $\boldsymbol{s}$, the target sequence is built as:
% \begin{align*}
%     s^{\text{tar}} = \{\ldots,M^B,x_i,\ldots,x_j,M^E,E^B,e,E^E,\ldots\},
% \end{align*}
% where a mention $x_i,..,x_j$ in $\boldsymbol{s}$ is marked with special tokens $M^B$ and $M^E$ at the beginning and ending respectively, then followed with tokens of the corresponding concept marked with special token $E^B$ and $E^E$ at both ends. GENRE is trained with standard cross-entropy loss with respect to each tokens.
% During inference, the model decides where to insert the special tokens $M^B$ and $M^E$ to mark the mention entity during left-to-right decoding.
% After $M^E$ is given at previous decoding step, the model transits to decoding the possible corresponding concept name in knowledge bases.
% The decoding scheme is achieved by constrained beam search with a token prefix trie.
% After the sequence decoding terminates, the results mention positions and concepts for $\boldsymbol{s}$, $(\boldsymbol{s}, \hat{i}, \hat{j}, \hat{e})$ are extracted by language regrexes. 

% One major advantage of generative approaches is they treat concepts as a special language sequences and directly generate the concepts token-by-token using prefix tries. 
% The construction and storage of the prefix tries save a large proportions of memory footprints compared with generating and saving embeddings for each concepts.

% \subsection{Knowledge Base Transfer}

% As mentioned above, a practical scenario for biomedical entity linking methods is that users only concern the mentions that are correspond to concepts from a sub-knowledge base, such as SNOMED-CT or Chemical concepts.
% We define this scenario as partial KB inference.
% To investigate the generalization of different paradigms on partial KB inference, given two knowledge bases $\mathcal{E}_1$ and $\mathcal{E}_2$, we consider the two following settings:
% \begin{enumerate}
%     \item Models trained on annotations from $\mathcal{E}_2$ and evaluated on annotations from $\mathcal{E}_1$;
%     \item Models trained on annotations from $\mathcal{E}_1$ and evaluated on annotations from $\mathcal{E}_1$.
% \end{enumerate}

% Concerning the transferrability, we may also consider a setting that models are trained on annotations from $\mathcal{E}_1$ and evaluated on annotations from $\mathcal{E}_2$. However, transferring from small knowledge bases to large knowledge bases is naturally a zero-shot problem. In biomedical domain, the model may not acquire the special knowledge to link the concept entities only from the large knowledge base. Thus the setting is out of scope for this research. 

% To measure the generalization across knowledge bases, we propose a metric ... 