% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{hiai2021relation,
  title={Relation extraction using multiple pre-training models in biomedical domain},
  author={Hiai, Satoshi and Shimada, Kazutaka and Watanabe, Taiki and Miura, Akiva and Iwakura, Tomoya},
  booktitle={Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)},
  pages={530--537},
  year={2021}
}


@article{lin2020high,
  title={High-throughput relation extraction algorithm development associating knowledge articles and electronic health records},
  author={Lin, Yucong and Lu, Keming and Chen, Yulin and Hong, Chuan and Yu, Sheng},
  journal={arXiv preprint arXiv:2009.03506},
  year={2020}
}

@inproceedings{huang2021covid,
  title={Covid-19 knowledge graph for drug and vaccine development},
  author={Huang, Lan and Guan, Hongrui and Liang, Yanchun and Guan, Renchu and Feng, Xiaoyue},
  booktitle={2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
  pages={328--333},
  year={2021},
  organization={IEEE}
}

@article{luo2022biored,
  title={BioRED: a rich biomedical relation extraction dataset},
  author={Luo, Ling and Lai, Po-Ting and Wei, Chih-Hsuan and Arighi, Cecilia N and Lu, Zhiyong},
  journal={Briefings in Bioinformatics},
  volume={23},
  number={5},
  pages={bbac282},
  year={2022},
  publisher={Oxford University Press}
}

@article{yuan2022coder,
  title={CODER: Knowledge-infused cross-lingual medical term embedding for term normalization},
  author={Yuan, Zheng and Zhao, Zhengyun and Sun, Haixia and Li, Jiao and Wang, Fei and Yu, Sheng},
  journal={Journal of biomedical informatics},
  volume={126},
  pages={103983},
  year={2022},
  publisher={Elsevier}
}

@article{liu2020self,
  title={Self-alignment pretraining for biomedical entity representations},
  author={Liu, Fangyu and Shareghi, Ehsan and Meng, Zaiqiao and Basaldella, Marco and Collier, Nigel},
  journal={arXiv preprint arXiv:2010.11784},
  year={2020}
}

@article{zhang2021knowledge,
  title={Knowledge-rich self-supervised entity linking},
  author={Zhang, Sheng and Cheng, Hao and Vashishth, Shikhar and Wong, Cliff and Xiao, Jinfeng and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  journal={arXiv preprint arXiv:2112.07887},
  year={2021}
}

@article{bodenreider2004unified,
  title={The unified medical language system (UMLS): integrating biomedical terminology},
  author={Bodenreider, Olivier},
  journal={Nucleic acids research},
  volume={32},
  number={suppl\_1},
  pages={D267--D270},
  year={2004},
  publisher={Oxford University Press}
}

@article{donnelly2006snomed,
  title={SNOMED-CT: The advanced terminology and coding system for eHealth},
  author={Donnelly, Kevin and others},
  journal={Studies in health technology and informatics},
  volume={121},
  pages={279},
  year={2006},
  publisher={IOS Press; 1999}
}

@article{yu2022bios,
  title={Bios: An algorithmically generated biomedical knowledge graph},
  author={Yu, Sheng and Yuan, Zheng and Xia, Jun and Luo, Shengxuan and Ying, Huaiyuan and Zeng, Sihang and Ren, Jingyi and Yuan, Hongyi and Zhao, Zhengyun and Lin, Yucong and others},
  journal={arXiv preprint arXiv:2203.09975},
  year={2022}
}

@article{Zhao_Liu_Zhao_Wang_2019, 
title={A Neural Multi-Task Learning Framework to Jointly Model Medical Named Entity Recognition and Normalization}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/3861}, DOI={10.1609/aaai.v33i01.3301817}, abstractNote={&lt;p&gt;State-of-the-art studies have demonstrated the superiority of joint modeling over pipeline implementation for medical named entity recognition and normalization due to the mutual benefits between the two processes. To exploit these benefits in a more sophisticated way, we propose a novel deep neural multi-task learning framework with explicit feedback strategies to jointly model recognition and normalization. On one hand, our method benefits from the general representations of both tasks provided by multi-task learning. On the other hand, our method successfully converts hierarchical tasks into a parallel multi-task setting while maintaining the mutual supports between tasks. Both of these aspects improve the model performance. Experimental results demonstrate that our method performs significantly better than state-of-theart approaches on two publicly available medical literature datasets.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Zhao, Sendong and Liu, Ting and Zhao, Sicheng and Wang, Fei}, year={2019}, month={Jul.}, pages={817-824} }

@ARTICLE{medtype2020,
       author = {{Vashishth}, Shikhar and {Joshi}, Rishabh and {Newman-Griffis}, Denis and
         {Dutt}, Ritam and {Rose}, Carolyn},
        title = "{MedType: Improving Medical Entity Linking with Semantic Type Prediction}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2020,
        month = may,
          eid = {arXiv:2005.00460},
        pages = {arXiv:2005.00460},
archivePrefix = {arXiv},
       eprint = {2005.00460},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200500460V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{krissbert,
  doi = {10.48550/ARXIV.2112.07887},
  
  url = {https://arxiv.org/abs/2112.07887},
  
  author = {Zhang, Sheng and Cheng, Hao and Vashishth, Shikhar and Wong, Cliff and Xiao, Jinfeng and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Knowledge-Rich Self-Supervision for Biomedical Entity Linking},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Derczynsk,
title = {Analysis of named entity recognition and linking for tweets},
journal = {Information Processing \& Management},
volume = {51},
number = {2},
pages = {32-49},
year = {2015},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2014.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306457314001034},
author = {Leon Derczynski and Diana Maynard and Giuseppe Rizzo and Marieke {van Erp} and Genevieve Gorrell and Raphaël Troncy and Johann Petrak and Kalina Bontcheva},
keywords = {Information extraction, Named entity recognition, Entity disambiguation, Microblogs, Twitter},
abstract = {Applying natural language processing for mining and intelligent information access to tweets (a form of microblog) is a challenging, emerging research area. Unlike carefully authored news text and other longer content, tweets pose a number of new challenges, due to their short, noisy, context-dependent, and dynamic nature. Information extraction from tweets is typically performed in a pipeline, comprising consecutive stages of language identification, tokenisation, part-of-speech tagging, named entity recognition and entity disambiguation (e.g. with respect to DBpedia). In this work, we describe a new Twitter entity disambiguation dataset, and conduct an empirical analysis of named entity recognition and disambiguation, investigating how robust a number of state-of-the-art systems are on such noisy texts, what the main sources of error are, and which problems should be further investigated to improve the state of the art.}
}

@inproceedings{kore,
author = {Hoffart, Johannes and Seufert, Stephan and Nguyen, Dat Ba and Theobald, Martin and Weikum, Gerhard},
title = {KORE: Keyphrase Overlap Relatedness for Entity Disambiguation},
year = {2012},
isbn = {9781450311564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396761.2396832},
doi = {10.1145/2396761.2396832},
abstract = {Measuring the semantic relatedness between two entities is the basis for numerous tasks in IR, NLP, and Web-based knowledge extraction. This paper focuses on disambiguating names in a Web or text document by jointly mapping all names onto semantically related entities registered in a knowledge base. To this end, we have developed a novel notion of semantic relatedness between two entities represented as sets of weighted (multi-word) keyphrases, with consideration of partially overlapping phrases. This measure improves the quality of prior link-based models, and also eliminates the need for (usually Wikipedia-centric) explicit interlinkage between entities. Thus, our method is more versatile and can cope with long-tail and newly emerging entities that have few or no links associated with them. For efficiency, we have developed approximation techniques based on min-hash sketches and locality-sensitive hashing. Our experiments on semantic relatedness and on named entity disambiguation demonstrate the superiority of our method compared to state-of-the-art baselines.},
booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
pages = {545–554},
numpages = {10},
keywords = {entity disambiguation, semantic relatedness, locality-sensitive hashing, entity relatedness},
location = {Maui, Hawaii, USA},
series = {CIKM '12}
}

@inproceedings{elq,
    title = "Efficient One-Pass End-to-End Entity Linking for Questions",
    author = "Li, Belinda Z.  and
      Min, Sewon  and
      Iyer, Srinivasan  and
      Mehdad, Yashar  and
      Yih, Wen-tau",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.522",
    doi = "10.18653/v1/2020.emnlp-main.522",
    pages = "6433--6441",
    abstract = "We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass. Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question, ELQ outperforms the previous state of the art by a large margin of +12.7{\%} and +19.6{\%} F1, respectively. With a very fast inference time (1.57 examples/s on a single CPU), ELQ can be useful for downstream question answering systems. In a proof-of-concept experiment, we demonstrate that using ELQ significantly improves the downstream QA performance of GraphRetriever.",
}

@inproceedings{ujiie-etal-2021-end,
    title = "End-to-end Biomedical Entity Linking with Span-based Dictionary Matching",
    author = "Ujiie, Shogo  and
      Iso, Hayate  and
      Yada, Shuntaro  and
      Wakamiya, Shoko  and
      Aramaki, Eiji",
    booktitle = "Proceedings of the 20th Workshop on Biomedical Language Processing",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.bionlp-1.18",
    doi = "10.18653/v1/2021.bionlp-1.18",
    pages = "162--167",
    abstract = "Disease name recognition and normalization is a fundamental process in biomedical text mining. Recently, neural joint learning of both tasks has been proposed to utilize the mutual benefits. While this approach achieves high performance, disease concepts that do not appear in the training dataset cannot be accurately predicted. This study introduces a novel end-to-end approach that combines span representations with dictionary-matching features to address this problem. Our model handles unseen concepts by referring to a dictionary while maintaining the performance of neural network-based models. Experiments using two major datasaets demonstrate that our model achieved competitive results with strong baselines, especially for unseen concepts during training.",
}

@InProceedings{medlinker,
author="Loureiro, Daniel
and Jorge, Al{\'i}pio M{\'a}rio",
editor="Jose, Joemon M.
and Yilmaz, Emine
and Magalh{\~a}es, Jo{\~a}o
and Castells, Pablo
and Ferro, Nicola
and Silva, M{\'a}rio J.
and Martins, Fl{\'a}vio",
title="MedLinker: Medical Entity Linking with Neural Representations and Dictionary Matching",
booktitle="Advances in Information Retrieval",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="230--237",
abstract="Progress in the field of Natural Language Processing (NLP) has been closely followed by applications in the medical domain. Recent advancements in Neural Language Models (NLMs) have transformed the field and are currently motivating numerous works exploring their application in different domains. In this paper, we explore how NLMs can be used for Medical Entity Linking with the recently introduced MedMentions dataset, which presents two major challenges: (1) a large target ontology of over 2M concepts, and (2) low overlap between concepts in train, validation and test sets. We introduce a solution, MedLinker, that addresses these issues by leveraging specialized NLMs with Approximate Dictionary Matching, and show that it performs competitively on semantic type linking, while improving the state-of-the-art on the more fine-grained task of concept linking (+4 F1 on MedMentions main task).",
isbn="978-3-030-45442-5"
}



@inproceedings{
zhang2022entqa,
title={Ent{QA}: Entity Linking as Question Answering},
author={Wenzheng Zhang and Wenyue Hua and Karl Stratos},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=US2rTP5nm_}
}

@inproceedings{karpukhin-etal-2020-dense,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.550",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
}

@article{xiong2020answering,
  title={Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval},
  author={Xiong, Wenhan and Li, Xiang Lorraine and Iyer, Srinivasan and Du, Jingfei and Lewis, Patrick and Wang, William Yang and Mehdad, Yashar and Yih, Wen-tau and Riedel, Sebastian and Kiela, Douwe and O{\u{g}}uz, Barlas},
  journal={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{chen2020open,
  title={Open-domain question answering},
  author={Chen, Danqi and Yih, Wen-tau},
  booktitle={Proceedings of the 58th annual meeting of the association for computational linguistics: tutorial abstracts},
  pages={34--37},
  year={2020}
}

@article{zhu2021retrieving,
  title={Retrieving and reading: A comprehensive survey on open-domain question answering},
  author={Zhu, Fengbin and Lei, Wenqiang and Wang, Chao and Zheng, Jianming and Poria, Soujanya and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2101.00774},
  year={2021}
}

@article{gillick2019learning,
  title={Learning dense representations for entity retrieval},
  author={Gillick, Daniel and Kulkarni, Sayali and Lansing, Larry and Presta, Alessandro and Baldridge, Jason and Ie, Eugene and Garcia-Olano, Diego},
  journal={arXiv preprint arXiv:1909.10506},
  year={2019}
}

@article{wu2019scalable,
  title={Scalable zero-shot entity linking with dense entity retrieval},
  author={Wu, Ledell and Petroni, Fabio and Josifoski, Martin and Riedel, Sebastian and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1911.03814},
  year={2019}
}


@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@inproceedings{chang2021diakg,
  title={DiaKG: An Annotated Diabetes Dataset for Medical Knowledge Graph Construction},
  author={Chang, Dejie and Chen, Mosha and Liu, Chaozhen and Liu, Liping and Li, Dongdong and Li, Wei and Kong, Fei and Liu, Bangchang and Luo, Xiaobin and Qi, Ji and others},
  booktitle={China Conference on Knowledge Graph and Semantic Computing},
  pages={308--314},
  year={2021},
  organization={Springer}
}

@article{reese2021kg,
  title={KG-COVID-19: a framework to produce customized knowledge graphs for COVID-19 response},
  author={Reese, Justin T and Unni, Deepak and Callahan, Tiffany J and Cappelletti, Luca and Ravanmehr, Vida and Carbon, Seth and Shefchek, Kent A and Good, Benjamin M and Balhoff, James P and Fontana, Tommaso and others},
  journal={Patterns},
  volume={2},
  number={1},
  pages={100155},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{lin2020kgnn,
  title={KGNN: Knowledge Graph Neural Network for Drug-Drug Interaction Prediction.},
  author={Lin, Xuan and Quan, Zhe and Wang, Zhi-Jie and Ma, Tengfei and Zeng, Xiangxiang},
  booktitle={IJCAI},
  volume={380},
  pages={2739--2745},
  year={2020}
}