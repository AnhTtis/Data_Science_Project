% In this section, we first review 

\paragraph{NER and NED} In biomedical and general domains, NER and NED are two extensively studied sub-fields in NLP. As mentioned, EL can be decomposed and approached by NER and NED.
NER is often considered a sequential labeling task \cite{lample2016neural}.
Neural encoders like LSTM \cite{gridach2017character,10.1093/bioinformatics/btx228,cho2019biomedical} or pretrained language models \cite{weber2021hunflair} encode input text and assign BIO/BIOES tags to each word. 
Many biomedical pretrained language models are proposed to enhance NER performances \cite{beltagy-etal-2019-scibert,peng2019transfer,lee2020biobert,gu2021domain,yuan-etal-2021-improving}.
Concerning NED, most methods embed mentions and concepts into a common dense space by language models and disambiguate mentions by nearest neighbor search \cite{dualencoder,biocom,rescnn}. 
\citet{clustering} and \citet{arboel} first rerank the disambiguation target to boost performance. To overcome the limitation of labeled NED corpus, \citet{sapbert,coder,yuan-etal-2022-generative} leverage synonyms from huge biomedical KB for zero-shot NED.
\citet{dataintegration,krissbert} use weakly supervised data generated from Wikipedia and PubMed for data augmentation. 
NER and NED are both essential components of EL.
In this work, we further explore partial KB inference by analyzing performance in these two steps and reveal how the design and order of NER and NED infer EL performance in partial KB inference.
% SapBERT \cite{sapbert} leverages the concepts synonyms in knowledge bases and pre-trains the encoders with contrastive objectives, and CODER \cite{coder,coder++} further injects relation information between concepts. 
% As biomedical knowledge bases may contain millions of concepts (e.g., UMLS), storing and searching on millions of dense embeddings requires large computational footprints. Addressing the problem, \citet{yuan-etal-2022-generative} apply a seq2seq framework and deal with NED with constrained language generation and disambiguate mentions by direct concept name generation.


% Given each detected mention, most methods embed mentions and concepts into a common dense space, and disambiguate mentions by nearest neighbor search \cite{dualencoder,biocom,rescnn}. SapBERT \cite{sapbert} leverage the synonyms for each concepts in knowledge bases and generate better concepts embeddings by contrastive learning, and CODER \cite{coder,coder++} further injects relation informations between cocnepts. \citet{clustering} and \citet{arboel} use a two phrase retrieve-then-rerank framwork to enhance the performance. \citet{dataintegration} and KRISSBERT \cite{krissbert} use weak supervised data generated from Wikipedia and PubMed to improve the performance. As biomedical knowledge bases may contain millions of concepts (e.g., UMLS), storing and searching on millions of dense embeddings requires large computational footprints. Addressing the problem, \citet{yuan-etal-2022-generative} apply a seq2seq framework and deal with NED with constrained language generation and disambiguate mentions by direct concept name generation.

\paragraph{Entity Linking} Although EL can be handled by a direct pipeline of NER and NED, there is limited research focusing on the task as a whole in biomedical. 
As EL may enjoy the mutual benefits from supervision of both subtasks, \citet{Zhao_Liu_Zhao_Wang_2019} deal with biomedical EL in a multi-task setting of NER and NED.
% and each token are classified into BIO and concept labels respectively. 
MedLinker \cite{medlinker} and \citet{ujiie-etal-2021-end} approach biomedical EL by sequentially dealing with NER and NED using a shared language model and they devise a dictionary-matching mechanism to deal with concepts absent from the training annotations. 

In the general domain, GENRE \cite{genre,mgenre} is proposed and formulated EL as a seq2seq task. They detect and disambiguate mentions with constrained language generation in an end-to-end fashion. We categorize GENRE as \textit{simultaneous-generate} EL.
% The mention positions and corresponding concepts can be extracted by generated structural languages. 
EntQA \cite{zhang2022entqa} provides a novel framework by first finding probable concepts in texts and then treating each extracted concept as queries to detect corresponding mentions in a question-answering fashion which is categorized as \textit{NED-NER} in our framework. 
\textit{Simultaneous-generate} and \textit{NED-NER} fashion are not widely examined in biomedical EL, and they interest us to examine their performances for biomedical EL and partial KB inferences.
% EntQA achieves state-of-the-art results in general domain entity linking.


% \paragraph{Biomedical Entity Linking} 
% Although entity linking is of critical importance to extract biomedical concept information from the free texts, there are limited research on this task. 
% \citet{Zhao_Liu_Zhao_Wang_2019} deal with biomedical entity linking in a multi-task setting of NER and NED, where both are treated as sequence labeling task and each token are classified into BIO and concept labels respectively. 
% They argue that such method can enjoy mutual benefits from supervisions of both tasks.
% MedType \cite{medtype2020} is a biomedical entity disambiguation module which leverage the off-the-shelf entity linking system and fine-tuned BERT. 
% MedLinker \cite{medlinker} and \citet{ujiie-etal-2021-end} approaches biomedical entity linking by sequentially deal with NER and NED using a shared language model. 
% Emphasizing the zero-shot biomedical NED problem where not all concepts have annotations in the training set, they both take advantage of textual similarity by dictionary-matching to address such problem.

% \paragraph{General Entity Linking} In general domain, ELQ \cite{elq} is proposed which embeds free texts and concepts into a common dense space by two separated encoder and then jointly detect and disambiguate mentions by inner-product scoring. Besides NED, GENRE \cite{genre,mgenre} can also detect and disambiguate mentions in one shot with constrained language generation. EntQA \cite{zhang2022entqa} proposes a novel framework by first finding all the probable concepts in texts and then treat each extracted concepts as queries to detect corresponding mentions. EntQA achieves state-of-the-art results in general domain entity linking.

\paragraph{Partial KB inference in EL}
In the biomedical domain, there is no prior work considering this setting to the best of our knowledge.
NILINKER \cite{nilinker} is the most related work which focuses on linking NIL entities out of the training KB, while ours aim to infer EL on part of the training KB and discard NIL entities.

% Besides, recent evaluation procedures in general domain EL consider the \textit{out-of-domain} performance of linking models \cite{genre,zhang2022entqa}. Compared to the \textit{in-domain} training data, \textit{out-of-domain} data contain different language styles \cite{Derczynsk} or annotations from a subset of concepts in KBs \cite{kore} which resembles the partial KB inference scenario.

% In the general domain entity linking, recent evaluation procedure \cite{genre,zhang2022entqa} considers the domain generalization of linking methods. The domain generalization indicates the annotations may only cover a small concept subsets (e.g., KORE \cite{kore}) or different language style (e.g., Derczynski \cite{Derczynsk}). However, in biomedical domain, there is no prior work challenge the linking methods with domain generalization to the best of our knowledge. 

% from one large domain to one specific small sub-domain. Concretely, methods are trained on AIDACoNLL dataset with Wikipedia as target knowledge base and evaluate the out-of-domain datasets such as .  As mentioned before, as training separate models for each different domain is tedious and may be computational infeasible, well-developed linking methods may require good domain adaption ability. 