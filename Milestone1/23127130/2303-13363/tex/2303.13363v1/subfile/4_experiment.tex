
\begin{table*}[h!]
\centering
\caption{Comparisons of on-device FL training time (seconds). 
We train a ConvNet2 model on FEMNIST for 50 training rounds and use the same local data size each round.
The marker ``$\star$'' indicates the estimated value of FedScale according to its original cost model $(\#Sample \cdot LatencyPerSample) + ModelSize/Bandwidth$, which is a fixed value for different configurations.}
% \vspace{-0.1in}
\label{tab:device-time}
\begin{tabular}{cccccccc} 
\toprule
\multirow{2}{*}{CPU Loads} & \multirow{2}{*}{Wifi Bandwidth} & \multirow{2}{*}{Batch Size} & \multicolumn{4}{c}{Running Time per FL Round (Seconds)} \\ 
\cmidrule(lr){4-7}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\oursys, Real} & \multicolumn{1}{c}{\oursys, Simula.} & FedScale, Real & FedScale, Simula. \\ 
\midrule
\multirow{3}{*}{Idle} & \multirow{3}{*}{100Mbps} & 16 & 3.58 \textasciitilde{} 3.77 & 1.99 \textasciitilde{} 3.12 & 7.24 \textasciitilde{} 7.75 & \multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}\\ 5.85$^\star$ \end{tabular}} \\
 &  & 32 & 3.42 \textasciitilde{} 3.62 & 1.70 \textasciitilde{} 2.45  & 6.33 \textasciitilde{} 6.94 &  \\
 &  & 64 & 3.31 \textasciitilde{} 3.51 & 1.54 \textasciitilde{} 2.10  & 5.84 \textasciitilde{} 6.63 &  \\ 
\cmidrule(lr){1-6}
Idle & 25Mbps & 32 & 8.66 \textasciitilde{} 9.24 & 3.31 \textasciitilde{} 4.54  & 12.28 \textasciitilde{} 13.27 &  \\
Idle & 5Mbps & 32 & 43.7 \textasciitilde{} 53.1 & 20.31 \textasciitilde{} 28.98  & 51.55 \textasciitilde{} 50.18 &  \\ 
\cmidrule(lr){1-6}
Light & 100Mbps & 32 & 3.48 \textasciitilde{} 3.54 & 3.07 \textasciitilde{} 5.76 & 6.53 \textasciitilde{} 7.44 &  \\
Moderate~ & 100Mbps & 32 & 3.66 \textasciitilde{} 4.05 & 2.12 \textasciitilde{} 3.56 & 7.84 \textasciitilde{} 8.94 &  \\
\bottomrule
\end{tabular}
% \vspace{-0.1in}
\end{table*}

\section{How \oursys benefits real FL?}
\label{exp:oursys}
In this section, we conduct experiments to show that the proposed \oursys is usable, efficient and scalable to cross-device FL for both research and deployment.


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.47\textwidth]{figs/our-scale.pdf}
	%\vspace{-0.15in}
	\caption{The scalability study of \oursys.} 
	\label{fig:our-scale}
		%\vspace{-0.2in}
\end{figure}


\subsection{Heterogeneous Device Runtime}
To demonstrate the usability and efficiency of the proposed \oursys for hetero-device runtime (introduced in Section \ref{sec:heter-runtime}), we conduct a series of experiments with \oursys on both real mobile phones (RedMi K40) and simulated Android devices, denoted as ``\oursys, Real'' and ``\oursys, Simula.'' respectively.
We vary the configurations of CPU loads (idle/light/moderate), network bandwidth (100Mbps/25Mbps/5Mbps), and batch sizes (16/32/64) to mimic diverse FL environments. 
For real mobile phones, a light CPU load indicates that a video player and a music player are running, and a moderate CPU load indicates that a large game is running and high-quality lighting effects are turning on. 
For simulated Android devices, we adjust the available CPU cores to imitate different CPU loads (4/2/1 CPU cores for idle/light/moderate CPU loads).
Meanwhile, we compare a representative FL tool, FedScale \cite{fedscale}, which also supports FL deployment (denoted as ``FedScale, Real'') and simulation (denoted as ``FedScale, Simula.'') for heterogeneous devices.


The results of the running time per FL training round are reported in Table \ref{tab:device-time}. 
Based on the results, we can conclude that \oursys outperforms FedScale in terms of both the \emph{efficiency} of real-world application and the \emph{fidelity} of simulation.
The reason is that \oursys allows users in running FL on real mobile devices based on MNN (with C++ codes), while FedScale is based on the Termux App to run Linux on Android (with Python codes).
We also note that \oursys requires only a portable app with 163 MB to run FL on the device, while the FedScale runtime takes up to 8.98 GB overhead, including a number of dependencies such as installed Linux OS, Numpy, and Pytorch.
(2) For fidelity of the simulation, we can observe that the simulation tool provided in \oursys can faithfully reflect the real-world diverse, changeable device runtimes within \textit{ranges}, under different computing and communication configurations (CPU cores, network bandwidths, and bath sizes).
By contrast, FedScale adopts a cost model to estimate the FL running time per round with a formula: $(\#Sample \cdot LatencyPerSample) + ModelSize/Bandwidth$. 
This estimation generates coarse-grained \textit{point} results and ignores factors that can directly affect the running time, such as I/O, hardware resource competition, and inaccurate pre-computation for $LatencyPerSample$ and $Bandwidth$.


% \vspace{-0.07in}
\subsection{Scalability}
\label{exp:scale}
In the proposed \oursys, we enhance the concurrent processing capability of the FL server by providing multiple processes for Messages Transmitters and Processors (denoted as \textit{parallel FL transactions}), and providing process pools for parallel gRPC sending and receiving (denoted as \textit{parallel communications}), as introduced in Section \ref{sec:server}.
To confirm the effect of these concurrency techniques in improving the scalability of \oursys, we perform a stress test for the FL server, and compare the proposed \oursys with FedScale, FederatedScope (FS), and FS equipped only with \textit{parallel FL transactions}.
We design a stress generator in which the devices skip local training and only send/receive dummy model parameters, and track the average running time per FL round.

The experimental results are shown in Figure \ref{fig:our-scale}, from which we can conclude that the proposed \oursys performs much better than FS and FedScale in terms of scalability.
In particular, as the scales of clients increase, \oursys can effectively work in scenarios with up to 100,000 FL clients.
By contrast, when using the same high-performance server (1 TB memory and 64 CPU cores with 2.5 GHz frequency), FedScale takes 1.4x $\sim$ 3.9x time consumption longer than the proposed \oursys, and FS suffers from out-of-memory (OOM) at the scale of 100,000 and also excessive time consumption at the other scales.
Besides, we can see that when ablating the two concurrency optimizations (\ie, from red line to blue and green lines), the FL times significantly increase, which verifies the necessity and effectiveness of our optimizations that improve potentially heavy competition for different hardware resources and reduce the corresponding queuing delays.
In Appendix \ref{append:exp-scale-asyn}, we will further show more benefits of these two optimizations in asynchronous FL aggregation, which involves more intense competition for server system resources than the synchronized FL.



\section{How big is the gap w.r.t device runtime and scale?}
\label{sec:exp-gap}
In this section, we use the simulation platform provided in \oursys to investigate the FL performance in \textit{hetero-device} and \textit{scalable} scenarios, by varying the device distributions and the scales of participated clients.

% \vspace{-0.07in}
\subsection{Simulation Settings}
\label{sec:exp-setting}
\subsubsection{Heterogeneous Devices.} 
We consider fairly diverse heterogeneous devices with different computation, memory, and communications capacities. Specifically, the devices can have 1$\sim$ 4 CPU cores, and CPU frequencies within 2.55 Ghz, 2.9 Ghz and 3.3 GHz. The device memory can be one of $\{256, 1024\}$ Mb. 
The network delays are one of $\{80 \sim 400,  35 \sim 200, 0\}$ in seconds, and the communication bandwidths (upload/download) can be one of $\{58,000/173,000,$ $75,000/285,000, $ $340,000/1,024,000\}$ in kbps, which are some representative configurations indicating network speeds and states of different quality such as 4G and WiFi.


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.47\textwidth]{figs/device-dist}
	% \vspace{-0.2in}
        \caption{Illustration of different device distributions.}
        \label{fig:device-dist}	
\end{figure}


Based on the above hetero-device configurations, we consider generating multiple device distributions in the simulation device pool that corresponds to different application scenarios: 
(1) \textbf{Homo-device} case: all the participated devices have the same configuration (one of the configurations provided in \oursys), which is adopted by most existing works; 
(2) \textbf{Uniform} case: the devices have diverse configurations uniformly drawn from the set of provided configurations. This case differs \textit{homo-device} in only the heterogeneous device types;
(3) We further consider the device heterogeneity in terms of quantity (number of different device types). Specifically, we draw each device configuration from a beta-binomial distribution with parameters $(\alpha=10, \beta=10)$, which we denoted as \textbf{Near-normal} case corresponding to most applications where the devices with medium capacities have dominant numbers.
Similarly, we change the parameter of beta-binomial distribution into $(\alpha=10, \beta=2)$ for \textbf{Strong-heavy} case and $(\alpha=0.2, \beta=0.2)$ for \textbf{Double-tails} case where the major devices have strong and both strong and weak capacities respectively.
We illustrate the distributions in Figure \ref{fig:device-dist}, where the x-axis indicates the 72 device combinations ordered by their capacities (we sort the capacity matrix diagonally, and thus the larger index, the stronger capacity). 
Moreover, to simulate the changes in device runtime in real-world applications (\eg, CPU contention and network signal volatility), the device pool will randomly allocate available (\ie, no FL task is executing) devices to participated clients at each FL round.


\subsubsection{FL Settings.} 
We consider the widely adopted FL algorithm FedAvg and conduct experiments on the federated datasets FEMNIST, CelebA and Twitter \cite{leaf}.
Following previous works \cite{fedem,pflbench,fedbn}, we adopt the ConvNet model with different capacities for FEMNIST and CelebA datasets, and LR model for Twitter.
We vary the number of participated clients to investigate the effect of device scales $n$ while keeping the available device rate to be $0.3n$ at each FL round.
Due to the space limitation, we present more details about the implementation and adopted hyper-parameters in Appendix \ref{append:implent-detail}.


\subsubsection{Evaluation Metrics.}
We adopt comprehensive metrics to examine the FL performance in terms of  
(a) \textbf{Prediction Accuracy.} Both globally averaged and individual prediction accuracies are considered. Specifically, we calculate the average accuracy of each client weighted by their local data size (denoted as $\overline{acc}$),  the bottom 90\% decile (denoted as $\protect\widebreve{acc}$), and the standard deviation (denoted as $\sigma_{Acc}$) among all individual accuracies. 
(b) \textbf{Training and Communication Efficiency.} We track the FL convergence round and wall-clock times in hours (denoted as $T_{conv}$) to measure training efficiency, and use total communication bytes and network traffic (comm. bytes /sec) as metrics for communication efficiency.
(c) \textbf{Client Utilization.} For each participated client, we calculate the number of contributions per unit of time as $N_{contrib}/T_{conv}$, where $N_{contrib}$ indicates the total number of times the client successfully contributed to FL (uploaded models for aggregation). We consider the mean (denoted as $\overline{uti}$) and standard deviation (denoted as $\sigma_{uti}$) for this metric among all clients to reflect the extent to which this federation unites the various participants. 
All the experiments are repeated 2 times with different random seeds.

\begin{figure*}[h!]
	\centering
	
	\subfigure[$\overline{acc}$]{
		\includegraphics[width=0.31\textwidth]{figs/sync_participated.pdf}
		\label{fig:acc-parti}
	}
	\subfigure[$\protect\widebreve{acc}$ ]{
		\includegraphics[width=0.31\textwidth]{figs/fairness_participated.pdf}
	}
	\subfigure[ $\sigma_{Acc}$]{
		\includegraphics[width=0.31\textwidth]{figs/fairness_acc_std_participated.pdf}
		\label{fig:std-acc-parti}
	}
	% \vspace{-0.15in}
	\caption{Accuracy w.r.t. increasing number of \textit{participated} clients under different hetero-device distributions on FEMNIST. }
	\label{fig:acc-scale-curve-parti}
\end{figure*}

\begin{figure*}[h!]
	\centering
	\subfigure[$\overline{acc}$]{
		\includegraphics[width=0.31\textwidth]{figs/sync_all.pdf}
		\label{fig:acc-all}
	}
	\subfigure[$\protect\widebreve{acc}$ ]{
		\includegraphics[width=0.31\textwidth]{figs/fairness_all.pdf}
		\label{fig:bottom-acc-full}
	}
	\subfigure[ $\sigma_{Acc}$]{
		\includegraphics[width=0.31\textwidth]{figs/fairness_acc_std_all.pdf}
	}
	% \vspace{-0.15in}
	\caption{Accuracy w.r.t. increasing number of \textit{all} clients under different hetero-device distributions on FEMNIST.}
	\label{fig:acc-scale-curve-full}
\end{figure*}

\subsection{Accuracy and Fairness}
We show various accuracy results ($\overline{acc}, \protect\widebreve{acc}$ and $\sigma_{acc}$) under different hetero-device distributions on FEMNIST in Figure \ref{fig:acc-scale-curve-parti} and Figure \ref{fig:acc-scale-curve-full}. The results within Figure \ref{fig:acc-scale-curve-parti} are evaluated on the participated clients (\eg, the number of evaluated clients is 450, 900, $...$), while the results within Figure \ref{fig:acc-scale-curve-full} are evaluated on all clients of the full datasets (\ie, the 3,596 clients of FEMNIST).

From these figures, we can see that
\textbf{there is a substantial accuracy gap between the homo-device case and hetero-device cases, which is more noticeable in the fairness-related metrics and at large device scales.}
Specifically, for both the participated and full clients (Figure \ref{fig:acc-parti} and Figure \ref{fig:acc-all}), the $\overline{acc}$ differences between homo- and hetero-device cases are not negligible, and the difference increases as the clients scale increases (from $-$1.6\% at the scale of 450 to 2.7\% at the scale of 3,596). 
When considering the fairness-related metrics, the differences become larger, \eg, at the scale of 3,596, there is about $7\%  \protect\widebreve{acc}$ gap in Figure \ref{fig:bottom-acc-full} and about $1.5\%$ ${\sigma}_{acc}$ gap in Figure \ref{fig:std-acc-parti}, this is because that the inconsistent training dynamics between homo- and hetero-device cases have a greater impact on individual clients, especially on those having slow response speeds.
We also find that with the increasing number of participated clients, the accuracy of homo-device setting improves by about $3\%\sim4\%$ for $\overline{acc}$, and  $6\%\sim7\%$ for $\protect\widebreve{acc}$.
However, the results of other hetero-device distributions have much smaller changes,
leaving the huge potential to improve the accuracy and practicality of FL under hetero-device and scalable scenarios.

Moreover, \textbf{both distribution differences and quantity differences in heterogeneous devices do matter.} 
Among the compared hetero-device distributions, there are large homo-hetero differences of $\overline{acc}$, $\protect\widebreve{acc}$ and ${acc}_{\sigma}$ even for the \textit{Uniform} distribution case, which differs \textit{Homo-device} case in only the participated device types.
When taking the quantity differences into consideration, the accuracy differences become larger for the other hetero-device cases, such as $2.7\%$ for $\overline{acc}$ metric on \textit{Near-normal} distribution at scale of 3,596.
Besides, the variances of results of homo-device w.r.t. different FL experiments are much larger than the ones of hetero-device cases due to the fact that all clients have the same opportunity to contribute to FL aggregation in homo-device distribution, verifying the existence and significance of the gap again.


\begin{figure*}[h!]
	\centering
	\includegraphics[width=0.31\textwidth]{figs/convergence_time.pdf}
	\includegraphics[width=0.31\textwidth]{figs/network_traffic.pdf}
	\includegraphics[width=0.31\textwidth]{figs/utilization.pdf}
	% \vspace{-0.15in}
	\caption{Performance under different hetero-device distribution w.r.t time to convergence (left), communication traffic (middle), and clients utilization (right) on FEMNIST.}
	\label{fig:sys-curve}
	%\vspace{-0.1in}
\end{figure*}

\begin{table*}[h!]
\centering
\caption{Experimental results of personalized FL algorithms on FEMNIST under \textit{\textit{Near-normal}} device distribution.}
% \vspace{-0.15in}
\label{tab:pfl-study}
\begin{tabular}{lcccccccccccc} 
\toprule
\multirow{2}{*}{Method } & \multicolumn{4}{c}{\# Clients = 225} & \multicolumn{4}{c}{\# Clients = 450} & \multicolumn{4}{c}{\# Clients = 900} \\
\cmidrule(lr){2-5}  \cmidrule(lr){6-9}  \cmidrule(lr){10-13}
& $\overline{acc}$~ & $\protect\widebreve{acc}$ & $\sigma_{acc}$ & $T_{conv}$ & $\overline{acc}$ & $\protect\widebreve{acc}$  & $\sigma_{acc}$ & $T_{conv}$ & $\overline{acc}$ & $\protect\widebreve{acc}$  & $\sigma_{acc}$ & $T_{conv}$ \\ 
\midrule
FedAvg & 70.21 &  54.23 &   13.17   & 0.42 & 74.36 &    55.21   &   13.65   & 1.47 & 75.70 &  57.27   &  13.01    & 1.89 \\ 
%\hline
FT & 72.62 &  54.05 &  16.24    & 0.76 & 75.16 &  58.82  &  15.05  & 1.72 & 75.36 &  59.38 &  15.22    & 1.96 \\ 
%\hline
FedBABU & 74.03 &  55.39    &  18.43    & 0.97 & 75.01 & 58.57   &  15.95    & 1.16 & 75.83 &  58.29   &  17.15    & 1.48 \\
\bottomrule
\end{tabular}
%\vspace{-0.1in}
\end{table*}

\subsection{FL System Efficiency}
In addition to model accuracy, we also quantify the impact of device heterogeneity and scales on the system efficiency.
Figure \ref{fig:sys-curve} demonstrates the results of the convergence wall-clock time in hours, the network traffic (communication bytes per second), and the client utilization (the contribution number per hour averaged over all clients, $\overline{uti} \pm \sigma_{uti}$).
Compared with the results for homo-device case, we observed that hetero-device cases exhibit \textbf{complex differences in the time to convergence, network traffic, and client utilization specific to device scales and device distributions}, challenging the utility of FL in real-world scenarios involving constrained time resources, constrained network traffic, and user incentive mechanisms.

Specifically, 
(1) The variance of time to convergence increases with increasing number of participated clients (e.g. $0.09$ with 450 participated clients and $1.34$ with 3,596 participated clients for \textit{Strong-heavy} case). 
On the one hand, this instability of convergence rate affects the iteration scheduling of real FL applications, as well as potentially more resource usage than budgeted. 
On the other hand, the degree of fluctuation is very different across heterogeneous distributions and scales, suggesting that different hetero-device distributions and scales amplify the differences in convergence due to the initialization of the model parameters and the aggregation dynamics (we varied the random seeds across multiple experiments), leaving an open question about how to design FL algorithms with more stable convergence under real FL scenarios.

(2) For communication load, interestingly, hetero-device cases have on average lower communication loads than homo-device case (e.g., -13.1\% for \textit{Strong-heavy} case) and on average significantly larger load variance than homo-device case (+4,989\%). 
This indicates that in real FL applications, the network traffic consumed by the entire FL server (the FL service provider) and clients (probably the individual users) in an acceptable time may be less than the estimated results with homogeneous devices. Also note that our system enables timeout re-broadcasting and over-selection mechanisms to match the real scenarios, thus some wasted traffic is also included in the reported traffic load. How to further reduce traffic waste is still under-explored, especially in conjunction with the heterogeneous device distributions and scales.

(3) For client utilization, except for the two highly biased hetero- distributions (\textit{Strong-heavy} and \textit{Double-tails}), the average utilization of the other ones is lower than that of the homo-device case. 
Besides, the utilization variance under hetero-device cases are significantly larger than those of the homo-device scenarios, especially with medium device scales, which implies that more clients are not able to contribute their uploaded messages efficiently to FL aggregation. 
It is important to improve the $\overline{uti}$ and reduce the $\sigma_{uti}$ in scalable hetero-device FL that involve incentive mechanisms \cite{zhan2020learning,zeng2021comprehensive}. 


\begin{table*}[h!]
	\centering
	\caption{Experimental results of personalized FL algorithms on CelebA under \textit{\textit{Near-normal}} device distribution.}
	% \vspace{-0.15in}
	\label{tab:pfl-study-celeba}
\begin{tabular}{lcccccccccccc} 
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{c}{\# Clients = 250} & \multicolumn{4}{c}{\# Clients = 500} & \multicolumn{4}{c}{\# Clients = 1000} \\
\cmidrule(lr){2-5}  \cmidrule(lr){6-9}  \cmidrule(lr){10-13}
 & $\overline{acc}$~ & $\protect\widebreve{acc}$ & $\sigma_{acc}$ & $T_{conv}$ & $\overline{acc}$ & $\protect\widebreve{acc}$  & $\sigma_{acc}$ & $T_{conv}$ & $\overline{acc}$ & $\protect\widebreve{acc}$  & $\sigma_{acc}$ & $T_{conv}$ \\ 
\midrule
FedAvg &  74.57 &   50.00   &  23.56 & 0.73 & 73.31 & 33.33 & 25.28 & 1.36 & 75.36 & 50.00 & 22.49 & 1.68 \\ 
%\hline
FT &  76.74    &    50.00   &   23.32   &  0.78   & 79.35 &    50.00   &   22.49  &   1.36   & 79.66 &  50.00  &   21.54  &  1.70 \\ 
%\hline

FedBABU & 77.21 & 40.00 & 24.86 & 0.77 & 79.20 & 50.00 & 21.39 & 1.44 & 80.64 & 50.00 & 25.01 & 1.73 \\
\bottomrule
\end{tabular}
\end{table*}


\begin{table*}[h!]
	\centering
	\caption{Experimental results of compression techniques on FEMNIST under \textit{\textit{Near-normal}} device distribution.}
	% \vspace{-0.15in}
	\label{tab:compress-study}
	%\small
	\begin{tabular}{lccccccccc}
		\toprule                      
		\multirow{2}{*}{Method} & \multicolumn{3}{c}{\# Clients = 225} & \multicolumn{3}{c}{\# Clients = 450} & \multicolumn{3}{c}{\# Clients = 900}  \\
		\cmidrule(lr){2-4}  \cmidrule(lr){5-7}  \cmidrule(lr){8-10}
		& $\overline{acc}$  & net. traffic & $T_{conv}$  &   $\overline{acc}$     & net. traffic &   $T_{conv}$ &  $\overline{acc}$ &   net. traffic &  $T_{conv}$  \\ \midrule
		FedAvg w/o comp.    &   70.21  &    135.08  &   0.42    &   74.36  &   106.02   &   1.47     &  75.70   &  133.34   &   1.89    \\ %\hline
		+Gzip &   70.43  &   90.24   &   0.54   &  73.75  &   93.78 &  0.98 &   75.36  &    94.62  &   2.12    \\ %\hline
		+FP16  &   69.63    &   56.23 &   0.44    &   73.99 &   64.38    &   0.78      &   74.58   & 88.71   &   1.16      \\ %\hline
		+INT8   &   65.58  &   32.39   &   0.41    &   72.63   &   42.58  &   0.63   &   74.46   &   43.29   &   1.19     \\  %\hline
		\bottomrule		
	\end{tabular}
% \vspace{-0.1in}
\end{table*}


\section{Advanced FL Features with \oursys}
\label{exp:advanced-fl-tech}
In this section, we will show several use cases of advanced FL features with \oursys, including personalization (Section \ref{exp:pfl}), compression (Section \ref{exp:compression}), and asynchronous training (Section \ref{exp:asyn}). Due to the space limitation, we present more results about the performance gap between homo- and hetero- device distributions for these studied advanced FL features in Appendix \ref{append:homo-hetero-advanced-fl}, and more results about the end-to-end evaluation when combining these studied advanced FL features in Appendix \ref{append:end2end}.
\subsection{Personalized FL (pFL)}
\label{exp:pfl}
Personalization is one of the promising directions of FL to maximize the model utility for individual clients.
To demonstrate how well \oursys supports personalized FL research, we take FedAvg as a baseline and compare it with fine-tuning (FedAvg + FT) and FedBABU \cite{oh2022fedbabu}, a SOTA pFL algorithm.
Specifically, FedBABU freezes the final classification layer during FL training, lets clients train and upload other layers of the model to the server for aggregation, and fine-tune the whole model before evaluation.
With the results of FedBABU on \oursys, we want to demonstrate the potential of \oursys for supporting more customized training/upload/aggregation behaviors in the FL personalization study.

We summarize the results in Table \ref{tab:pfl-study} and \ref{tab:pfl-study-celeba} based on heterogeneous devices following \textit{\textit{Near-normal}} distribution for FEMNIST and CelebA dataset respectively.
In general, the pFL algorithms achieve better accuracy than FedAvg (both $\overline{acc}$ and $\protect\widebreve{acc}$) on FEMNIST dataset, while larger $\sigma_{acc}$ with un-fairness.
Besides, the performance advantages of pFL algorithms over FedAvg decrease on FEMNIST as the device scale increases (\eg the $\overline{acc}$ difference between FedBABU and FedAvg is 3.82, 0.65 and 0.13 at the scale of 225, 450, 900 respectively). 
Interestingly, compared to the results on FEMNIST, the accuracy advantage of pFL algorithms over FedAvg is more significant on CelebA dataset (\eg, the absolute $\overline{acc}$ improvements are 2.64\% and 5.89\% on FEMNIST and CelebA respectively).
This discrepancy may be due to the different amount of local data (the average number of local data size is 21.4 for CelebA, which is less than FEMNIST's 226.8).
It is suggested to improve the fairness and stability for future personalized FL algorithms by collaboratively accounting for device heterogeneity and data heterogeneity.



\begin{table*}[h!]
	\centering
	\caption{Experimental results of asynchronous aggregation on FEMNIST under different device distributions.}
	% \vspace{-0.15in}
	\label{tab:asyn-study}
	%\small
	\begin{tabular}{lcccccccccccc}
		\toprule                       
		\multirow{2}{*}{Method} & \multicolumn{4}{c}{\# Clients = 225} & \multicolumn{4}{c}{\# Clients = 450} & \multicolumn{4}{c}{\# Clients = 900}  \\
		\cmidrule(lr){2-5}  \cmidrule(lr){6-9}  \cmidrule(lr){10-13}
		& $\overline{acc}$ & $\protect\widebreve{acc}$ & $\sigma_{acc}$ & $\overline{uti} \pm  \sigma_{uti}$ &   $\overline{acc}$ & $\protect\widebreve{acc}$ & $\sigma_{acc}$ &       $\overline{uti} \pm  \sigma_{uti}$  &  $\overline{acc}$ & $\protect\widebreve{acc}$ & $\sigma_{acc}$ &       $\overline{uti} \pm  \sigma_{uti}$     \\ \midrule
		Sync, \textit{Near-normal}    &   70.21  &    54.23  & 13.17  &   41.68$\pm$18.87&   74.36  &   55.21  & 13.65  &   27.63$\pm$3.98    &  75.70   &  57.27  & 13.01   &   24.33$\pm$3.72   \\ 
		Async, \textit{Near-normal}        &   80.81  &  67.65  & 10.01   &   54.93$\pm$3.81  &   77.97   & 62.50  & 11.77  &   72.84$\pm$9.14    &   78.34   & 63.01 &  14.57  &   79.41$\pm$4.31    \\ \midrule
		Sync, \textit{Strong-heavy}  &   70.74   & 54.99  &  12.66  &   46.90$\pm$12.49 &   74.23  &  57.41 & 12.89  & 42.86$\pm$6.94   &   74.83  & 56.32 &  13.31 &  32.75$\pm$5.34   \\ 
		Async, \textit{Strong-heavy}        &   77.84  & 55.88  &  14.20  &   59.02$\pm$4.70  &  73.43  &  54.55   &  13.20 &   77.16$\pm$8.69   &    74.94  &  56.25  &  16.26  &   81.32$\pm$6.95                   \\
		\bottomrule
		
	\end{tabular}
\end{table*}




\subsection{Communication Compression}
\label{exp:compression}
In real FL scenarios, communication cost is one of the most important metrics, especially in low-resource cross-device federated learning. 
To illustrate how the compression techniques affect the trade-off between the communication cost and model utility on \oursys, we conduct experiments to compare accuracies, communication cost and time to convergence of the FedAvg using vanilla gradients transmission (no compression), transmission with Gzip (lossless compression), quantized transmission in FP16 and INT8 (lossy model quantization).
The results on FEMNIST are summarized in Table \ref{tab:compress-study}.


One can see from Table \ref{tab:compress-study} that, as the number of clients increases, the accuracies of models using different communication modes also increase as expected for the \textit{\textit{Near-normal}} distribution.
The reducing network traffic effect is significant when the compression communication mode is turned on. 
Compared with the communication cost of the vanilla communication mode, Gzip reduces the 33\% cost with almost indistinguishable accuracy differences.
As for the lossy compressions, FP16 reduces about 58\% communication cost while INT8 has only 1/3 communication cost of the vanilla one.
Although lossy compression introduces slight accuracy declines, the gap between the vanilla communication mode and the lossy ones is narrowed down to only about 1\% differences when the number of clients increases to 900.
Another benefit of using lossy compression lies in the time to convergence of FL training.
We also measure the convergence time (wall time in hours) and show that the training with lossy compression usually takes less time to converge.

Overall, by comparing the result of different communication modes, we demonstrate that our \oursys can have great potential in federated learning model compression.



\subsection{Asynchronous Aggregation}
\label{exp:asyn}

Applying asynchronous aggregation in FL balances the training efficiency and model utility, since clients get out of waiting for stragglers to finish local training at each FL round.
For cross-device FL, asynchronous aggregation is particularly essential for handling the heterogeneity, however, it also brings additional challenges to the system design.
For example, compared with synchronous aggregation, applying asynchronous aggregation might create more serious resource competition for sending and receiving messages, and lead to the training time growing rapidly with the scale.
With the help of the FL Server in \oursys (Section~\ref{sec:server}), separate process pools are provided for parallel sending and receiving messages, which makes it scalable and efficient to apply asynchronous aggregation in FL.

Furthermore, we provide observations and discussions on the effectiveness and efficiency of asynchronous aggregation with different device distributions and device scales, calling for further careful adaptation towards real-world hetero-device FL.
The experimental results are reported in Table~\ref{tab:asyn-study}. Overall, we can observe that applying asynchronous aggregation achieves competitive model performance and significantly higher utilization of clients' contributions compared to those of synchronous aggregation.
When the device scale increases,  the utilization becomes higher when applying asynchronous aggregation (e.g., 59.02/77.16/81.32 for 225/450/900 participated clients under \textit{Near-normal} device distributions), while the utilization becomes lower when applying synchronous aggregation (e.g., 46.90/42.86/32.75 for 225/450/900 participated clients under \textit{Near-normal} device distributions). These results confirm the importance and necessity of asynchronous techniques towards real-world hetero-device FL.
Besides, it is worth pointing out the impact of different device distributions on the model performance (e.g., $\overline{acc}$ and $\sigma_{acc}$). When using synchronous aggregation, the model performance varies slightly (e.g., $\pm$0.53/0.13/0.87$\%$ on $\overline{acc}$ when the number of clients is 225/450/900 comparing between \textit{Near-normal} and \textit{Strong-heavy}) under different device distributions. However, there exist noticeable variances (e.g., $\pm$2.97/4.54/3.40$\%$ on $\overline{acc}$ when the number of clients is 225/450/900 comparing between \textit{Near-normal} and \textit{Strong-heavy}) in model performance when applying asynchronous aggregation under different device distributions.
These variances inspire us to call on the community to pay more attention to the hetero-device distribution of real-world FL applications for improving the usability and robustness of cross-device FL studies, and to carefully adapt some async-mode configurations (e.g., staleness toleration and aggregation goal) and develop new FL algorithms according to the hetero-device distributions.













