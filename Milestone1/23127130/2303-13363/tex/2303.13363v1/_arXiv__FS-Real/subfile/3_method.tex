
\section{The Proposed \oursys}
In this section, we describe the design and implementation of our system optimized for real-world FL.

\subsection{Design Principles}
\label{sec:design-require}
The two main characteristics of real cross-device FL (\ie, \textit{heterogeneous device} and \textit{large scale}) impose a number of unique system requirements in terms of the following aspects:
\textbf{(i) Usability and Efficiency.} The hardware and software environments of devices are very different from cloud-based high-performance servers, having various instruction sets (x86, ARM, etc.,), operating systems, and library dependencies. 
A cross-device FL system should have good usability and can conduct evaluation for a wide range of device runtimes.
Besides, devices often have limited resources, such as computing capacity, communication bandwidth, and storage.
It is critical for cross-device FL systems to efficiently conduct training, inference, and management for local models with minimal consumption of device resources.
\textbf{(ii) Scalability and Robustness.} As the participating devices can be on diverse scales, the FL server should be highly scalable, make good use of system resources, and yield a corresponding improvement in model quality and training speed as more system resources are invested. 
Moreover, in real scalable FL scenarios with heterogeneous devices, many devices are prone to be slow and disconnected as we discussed aforementioned.
How to robustly handle such devices is also one of the key challenges.
\textbf{(iii) Flexibility and Extensibility.} 
Real FL applications require the collaboration between devices and the server, which involves a large number of potentially inconsistent software stacks and programming interfaces.
Supporting flexible customization and extension of FL algorithms is therefore necessary to improve the model quality and convergence for diverse scales and scenarios.

Next we introduce how we implement the components to fulfill the above requirements (\textbf{RS}), including an easy-to-use and high-fidelity simulation platform and efficient FL device executors (Sec.\ref{sec:heter-runtime} for \textbf{RS i}); a scalability enhanced robust server (Sec.\ref{sec:server} for \textbf{RS ii}); and supports and extensions of diverse advanced FL algorithms (Sec.\ref{sec:fl-algo} for \textbf{RS iii}).

\subsection{Heterogeneous Device Runtime}
\label{sec:heter-runtime}
For the sake of usability and efficiency, we implement a dedicated high-performance FL execution engine for heterogeneous devices. 
The major modules are designed as a portable learner based on the MNN \cite{proc:osdi22:walle}, and a communication and storage manager based on the native APIs of the target OS (e.g. Android SDK).
The learner is responsible for local model training and monitoring, and can be selectively packaged and compiled into lightweight dynamic link libraries for different target hardware (e.g. x86 CPUs, ARM CPUs, and CUDA-GPU).
We leverage MNN to automatically optimize the computational graphs for the target hardware, such that the computation and memory access can be accelerated, and the binary size can be reduced. 

Besides, the communication and storage manager is responsible for sending messages to (and receiving messages from) the server and for the serialization of the data model, in which we leverage gRPC and compressed MNN models to enable high-performance network transmission.
Here we use the MNN model file as an intermediate representation of the exchanged model, which facilitates cross-platform execution and reduces development costs. With support for mainstream model formats such as onnx \cite{onnx}, Tensorflow \cite{tensorflow}, Torchscripts \cite{pytorch} and a large number of widely used operations, we can easily define computational graphs on the server side using python and various frameworks, which will be automatically and uniformly converted to MNN model on devices, reducing the programming burden for diverse devices.

Our efficiency optimization is multi-granular in terms of (1) the device learning behaviors with efficient C++ implementation and leveraging of heterogeneous computing hardware (e.g., high-performance assembly codes for different operators with the help of MNN), (2) the communication and storage manager implemented with native OS API to reduce redundant memory accesses and computational calls from other intermediate code bases and (3) the compact binary size and minimal dependency that reduce the burden on users' storage resources. In Sec.\ref{exp:oursys}, we will give some quantitative comparisons to demonstrate the efficiency of \oursys.


\subsection{Enhanced FL Server}
In real-world FL scenarios, the FL server can easily become a performance bottleneck for the whole system due to the fact that as the scale of clients increases, both the consumed resources and the number of slow or disconnected devices increase. 
In this section, we introduce the enhanced FL server in \oursys for better scalability and robustness.
\label{sec:server}
\subsubsection{Message Concurrency.}
To enhance the scalability of the system, we implement concurrency at multiple granularities. 
We choose a message-passing-based FL library FederatedScope \cite{fs} as a starting point for the server implementation, and further extend and optimize it.
By focusing on the unified message object, we can easily analyze and handle potential space-time contention and redundant resource overheads.
Specifically, we first abstract FL server behaviors into \textit{Message Transmitter} that is responsible for the transfer of messages between clients and server, and \textit{Message Processor} that is responsible for a series of FL transactions to process messages, such as model aggregation and monitoring.
We implement \textit{Message Transmitters} and \textit{Message Processors} with multiple individual processes for interleaved execution, due to the fact that they typically occupy different hardware resources (e.g., CPU, Network Interface Card and disks) and have great potential for concurrency.
Note that this design and optimization is necessary and effective because the transmission latency and processing latency of messages can vary by even several orders of magnitude in different FL and network situations.
Furthermore, for transmitters, we introduce separate process pools of configurable size for parallel gRPC sending and receiving, so that they can be flexibly and automatically scale out to efficiently handle different scales of FL participated devices.

\subsubsection{Robust Client Selection.}
The FL server needs to select the available devices and broadcast messages to them to initiate local training in each round, perform aggregation at the proper time according to the response messages, and subsequently start the next round of broadcasting. In real FL scenarios, the response times between devices can differ by order of magnitude as aforementioned.

To advance the FL process robustly and efficiently in various scenarios, we design a timeout mechanism for FL workload adaptation. 
Suppose that a server in an FL round has selected $n$ devices from $n_{ava}$ available devices to broadcast messages and expects to receive $n'$ device responses for aggregation.
Given a timeout budget $t_o$, if the number of received messages is less than $n'$ within $t_o$ time after broadcasting, we will rebroadcast the messages to other $min(n, max(n_{ava}-n, 0))$ available devices and reset the timeout timer.
Inspired by the AIMD (Additive Increase, Multiplicative Decrease) congestion control algorithm \cite{tcpaimd}, we double $t_o$ after each triggered rebroadcast and subtract $\delta_t$ from $t_o$ when no rebroadcast is triggered for $k$ consecutive rounds.
This mechanism takes into account the diverse load and dynamics of the available heterogeneous devices, allowing for efficient advancement of FL process across time and scenarios based on the overall responsiveness of the devices.
Besides, we support the over-selection mechanism on the server side, by broadcasting to $min(n_{ava}, \lfloor qn \rfloor)$ available devices, where $q$ indicates the over-selection rate (e.g. 150\%). This mechanism enhances the robustness of synchronous aggregation scenarios, while in the next section, we describe asynchronous algorithms that can make FL algorithms more efficient and robust.


\subsection{Advanced FL Techniques}
\label{sec:fl-algo}
In order to provide flexible and easy FL algorithm extensions, we design simple and unified configuration options and programming interfaces for both the cloud and the device side in \oursys.
These APIs abstract a set of expressive behaviors for common yet necessary objects in FL such as messages, models, trainers, monitors, etc., which are decoupled from the target device runtime and can be flexibly customized.
In particular, here we support and provide a number of advanced FL technology implementations that are important for scaling heterogeneous devices FL, including personalization, communication compression, and asynchronous aggregation.

(1) Personalization is important to improve the quality of client-specific models and ensure the user experience of end-intelligence applications where each device corresponds to a single real user. 
In the \textit{Device Executor} (Sec.\ref{sec:heter-runtime}), we provide device-wise configuration and interfaces such as ``set\_local\_module'', ``fine\_tune'' and implement SOTA personalized FL algorithms such as personalized fine-tuning, FedRep \cite{fedrep} and FedBABU \cite{oh2022fedbabu}.
(2) Communication compression reduces the message size to save the running time and reduce development costs (such as the fee charged for mobile traffic).
In the server's \textit{Message Transmitter} (Sec.\ref{sec:server}), we provide interfaces such as ``channel\_compress'', ``para\_compress'' and support compression techniques such as lossless gzip and deflate algorithm \cite{deutsch1996gzip}, and lossy half-precision floating point (FP16) and INT2$\sim$INT8 quantization \cite{liang2021pruning}.
(3) Asynchronous aggregations combine messages received in different FL rounds to improve training efficiency and robustness. In the server's \textit{Message Processor}, we provide convenient maintenance of message staleness and necessary interfaces such as ``stale\_aggregation'' for asynchronous algorithms, and implement the SOTA algorithm, FedBuff \cite{nguyen2022fedbuff}.



\subsection{Heterogeneous Device Simulation Platform}
\label{sec:heter-simula}
Preparing the corresponding hardwares (\eg, mobile phones) for heterogeneous runtimes to run real FL requires high costs in terms of development time, financial expense for acquiring new hardware devices and potential loss of user experience (\eg, testing with users' devices).
To accelerate real-world FL research and reduce the incurred costs, we further implement a high-fidelity and easy-to-use heterogeneous runtime simulation platform in \oursys.

As mentioned in Sec.\ref{sec:backgrond-related}, the computing, communication, and storage capabilities of the participated devices have the most direct impact on FL, thus we mainly focus on fidelity to these aspects. 
We run separate processes for the participated clients based on the Android official native simulation tool \footnote{https://developer.android.com/studio/run/emulator}, and allow users to configure the number of CPU cores, memory footprint, network types, and network latency. 
Note that with our simulation platform, each device runs in the same software environment (\eg, OS and library dependencies) as a real phone and will communicate realistically across machines via gRPC, which greatly reduces the gap between emulation and real FL.

In addition, we provide easy-to-use configurable tools to quickly start simulated heterogeneous devices and run FL at scale with a single command. To enhance the utilization of simulation host resources, we introduce a scalable, self-managing device pool that supports different types of heterogeneous runtime distributions and allocates available devices to participated clients at each FL round. 
This configuration system is comprehensive: 
At the global level, users can specify common FL settings (e.g. dataset, FL scale, network topology, FL algorithm); 
At the local client level, different personalized hyperparameters and data configurations can be specified for each client; 
At the device level, different device capabilities (e.g. CPU, network type) can be specified for each emulated device, as well as the device pool distributions (we will give some examples in Sec.\ref{sec:exp-setting}). 
Hyperparameter optimization is also supported with early stopping and easy configuration.

Based on the available host resources and specified configurations such as the number of devices and device capacity, the device pool will automatically perform auxiliary engineering actions such as Android phone startup, \oursys runtime (App) installation, port mapping, network connecting, clients' data switching and storage, failure reboot, etc., allowing users to only focus on emulation configuration and not worry about the complexity of dealing with heterogeneous devices.
Here the device allocation can be randomized to simulate the changes in device runtime (e.g., CPU contention and network bandwidth fluctuations). 
