\appendix

\section*{Appendix}


\section{Implementation Details}
\label{append:implent-detail}


\subsection{Datasets}

Our experiments are conducted on several widely used FL datasets with diverse scales and tasks.
Specifically, \textbf{FEMNIST} \cite{leaf,emnist} is for hand-written digits classification and contains 3,550 clients whose local data is partitioned by the writers. \textbf{CelebA} \cite{celeba,leaf} is for the classification of celebrities' characteristics and contains 9,343 clients whose local data is partitioned by the celebrities. \textbf{Twitter} \cite{twitter,leaf} is for sentiment classification and we adopt the subset used in \cite{pflbench}, which contains 13,203 clients whose local data is partitioned by the Twitter users. 
We randomly split the datasets into train/valid/test sets with a ratio of 6:2:2.

\iffalse
[WIP] \textbf{FedCREMA-D}, CREMA-D (Crowd-sourced Emotional Multimodal Actors Dataset), according to speaker ID, there are 91 clients. 
Actors spoke from a selection of 12 sentences. The sentences were presented using one of six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) and four different emotion levels (Low, Medium, High, and Unspecified).
(https://github.com/mohammad5994/SER-Federated-Learning/blob/main/SER\_Crema.py)
\fi

\subsection{Models and Baselines}
Following previous works \cite{leaf,federatedscope,fedem,pflbench}, we adopt CNN models for FEMNIST and CelebA datasets, and an LR model for the Twitter dataset.
Specifically, 
for FEMNIST, the training model consists of two convolution layers and two linear layers, whose dimensions are 32, 64, 1024 and 62 respectively.
For CelebA, we use the model with the same architecture as the one used for FEMNIST, but with dimensions [32, 64, 256, 2] to avoid out-of-memory in devices.
For Twitter, we use an LR model and represent the sentence features by concatenating the 50d Glove embeddings. \footnote{https://nlp.stanford.edu/data/glove.6B.zip.}

\subsection{Platform and Hyper-Parameters}
We conduct the experiments on a cluster of 10 servers, whose CPU cores are within [196, 256, 512] and the CPU frequencies are within [2.55, 2.9, 3.3] Ghz.
The codes are built upon FederatedScope in version 0.2.0, MNN in version 2.0.0, Clang in version 14.0.0, and android JDK in version 1.8.

For each adopted dataset, we vary the number of participated clients to investigate the effect of device scales $n$, while keeping the available device rate to be $0.3n$ at each FL round, i.e., the server only sends messages to $0.3n$ randomly selected clients at each FL round.
As we mentioned in Section \ref{sec:server}, there are slow and disconnected devices in real hetero-device scenarios, we thus enable the over-selection with ratio $q=1.5$ for all experiments in synchronous mode, and enable the timeout mechanism with the timeout parameters $t_o=60, \delta_t=5$.
Besides, we run FL in at most $200$ rounds, set the local run step to be 1 epoch for each device, and grid search the hyper-parameters of FL algorithms.


In synchronous aggregation, the learning rate is searched from $\{0.001,0.025, 0.05, 0.1,0.5, 1\}$. We adopt 0.1 for FEMNIST and 0.025 for CelebA. 
We train FL models for 200 training rounds with a batch size of 16 for both datasets.
In Section \ref{exp:pfl}, we locally fine-tune the models for 5 epochs for each client before evaluation. 
In asynchronous aggregation, similarly, we train the FL models for 200 training rounds with a local learning rate of 0.1. The batch size is set as 16.


\section{Additional Experiments}
\label{append:more-exp-results}

\subsection{Scalability Study for Asynchronous Aggregation}
\label{append:exp-scale-asyn}
In Section \ref{exp:scale}, we have studied the scalability of \oursys that is enhanced by two optimizations, the \textit{parallel FL transactions}, and the \textit{parallel communications}. 
In fact, these two optimizations will bring additional benefits in asynchronous aggregation FL.
We adopt the same experimental settings as Section \ref{exp:scale} to compare the training time at different FL scales, except that the FL algorithm is replaced from synchronous FedAvg to asynchronous FedBuff.
The results are shown in Figure \ref{fig:our-scale-asyn} and we do not compare to FedScale here since it does not natively support asynchronous FL.
Notably, the training time per round of FS with parallel FL transactions is 1.18x $\sim$ 4.77x longer than \oursys, while FS stills failed in the scale of 100,000 due to the OOM.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.4\textwidth]{figs/our-async-scale.pdf}
	%\vspace{-0.15in}
	\caption{The scalability study of \oursys in asynchronous aggregation FL.} 
	\label{fig:our-scale-asyn}
		%\vspace{-0.2in}
\end{figure}



\begin{table*}[ht]
    \centering
        \caption{Experimental results of personalized FL algorithms on FEMNIST under \textit{Near-Normal} and \textit{Homo-device} distribution.}

    \begin{tabular}{llcccccccccccc}
        \toprule
        \multirow{2}{*}{Distribution}&    \multirow{2}{*}{Method}& \multicolumn{4}{c}{\# Clients = 225} & \multicolumn{4}{c}{\# Clients = 450} & \multicolumn{4}{c}{\# Clients = 900} \\
        \cmidrule(lr){3-6}  \cmidrule(lr){7-10}  \cmidrule(lr){11-14}
        &   & $\overline{acc}$~ & $\protect\widebreve{acc}$ & $\sigma_{acc}$ & $T_{conv}$ & $\overline{acc}$ & $\protect\widebreve{acc}$  & $\sigma_{acc}$ & $T_{conv}$ & $\overline{acc}$ & $\protect\widebreve{acc}$  & $\sigma_{acc}$ & $T_{conv}$ \\ 
        \midrule
        \multirow{3}{*}{\textit{Near-normal}}&  FedAvg & 70.21 &  54.23 &   13.17   & 0.42 & 74.36 &    55.21   &   13.65   & 1.47 & 75.70 &  57.27   &  13.01    & 1.89 \\ 
        %\hline
        &   FT & 72.62 &  54.05 &  16.24    & 0.76 & 75.16 &  58.82  &  15.05  & 1.72 & 75.36 &  59.38 &  15.22    & 1.96 \\ 
        %\hline
        &   FedBABU & 74.03 &  55.39    &  18.43    & 0.97 & 75.01 & 58.57   &  15.95    & 1.16 & 75.83 &  58.29   &  17.15    & 1.48 \\
        \midrule
         \multirow{3}{*}{\textit{Homo-device}}&  FedAvg &   71.35&   54.96&   15.34&   0.54&   73.19   &   55.94   &   13.34   &   1.53    &   75.14   &   59.54   &   12.19   &   1.71  \\
         &  FT &   75.43&   44.50&   17.79&   0.62&   75.28&   47.02&   17.12&   1.76&   76.61&   52.81&  17.00&    1.25  \\
         &  FedBABU &   75.55&   53.12&  17.51&   0.65&   76.13
         &   51.78&   16.86&   1.72&   77.24&   53.33&   15.13&  1.64  \\
         \bottomrule
    \end{tabular}
    \label{tab:homo-hetero-diff-pfl}
\end{table*}

\begin{table*}[ht]
    \centering
        \caption{Experimental results of compression techniques on FEMNIST under \textit{Near-Normal} and \textit{Homo-device} distribution.}

    \begin{tabular}{llccccccccc}
		\toprule                      
	\multirow{2}{*}{Distribution}&	\multirow{2}{*}{Method} & \multicolumn{3}{c}{\# Clients = 225} & \multicolumn{3}{c}{\# Clients = 450} & \multicolumn{3}{c}{\# Clients = 900}  \\
		\cmidrule(lr){3-5}  \cmidrule(lr){6-8}  \cmidrule(lr){9-11}
		& & $\overline{acc}$  & net. traffic & $T_{conv}$  &   $\overline{acc}$     & net. traffic &   $T_{conv}$ &  $\overline{acc}$ &   net. traffic &  $T_{conv}$  \\ \midrule

        \multirow{4}{*}{\textit{Near-normal}}&   FedAvg w/o comp.    &   70.21  &    135.08  &   0.42    &   74.36  &   106.02   &   1.47     &  75.70   &  133.34   &   1.89    \\ %\hline
		& +Gzip &   70.43  &   90.24   &   0.54   &  73.75  &   93.78 &  0.98 &   75.36  &    94.62  &   2.12    \\ %\hline
		& +FP16  &   69.63    &   56.23 &   0.44    &   73.99 &   64.38    &   0.78      &   74.58   & 88.71   &   1.16      \\ %\hline
		& +INT8   &   65.58  &   32.39   &   0.41    &   72.63   &   42.58  &   0.63   &   74.46   &   43.29   &   1.19     \\  %\hline
        \midrule
        \multirow{4}{*}{\textit{Homo-device}}&   FedAvg w/o comp.&   71.35&   130.44&   0.54&   73.19&   110.14&   1.53&   75.14& 144.45&   1.71    \\
        &   +Gzip&   70.54&   83.45&   0.64&   72.46&   87.74&   0.72&   74.60&   87.99&  0.74    \\
        &   +FP16&   68.33&   51.95&   0.49&   72.05&   59.22&  0.75&   72.31&  78.84&  0.69    \\
        &   +INT8&   64.50&   30.74&   0.38&   69.43&   40.16&   0.67&   71.45& 50.42&  0.73    \\
        \bottomrule
    \end{tabular}
    \label{tab:homo-hetero-diff-compre}
\end{table*}


\begin{table*}[ht]
    \centering
        \caption{Experimental results of asynchronous aggregation on FEMNIST under \textit{Near-Normal} and \textit{Homo-device} distribution.}

    \begin{tabular}{llcccccccccccc}
		\toprule                       
		\multirow{2}{*}{Distribution}&\multirow{2}{*}{Method} & \multicolumn{4}{c}{\# Clients = 225} & \multicolumn{4}{c}{\# Clients = 450} & \multicolumn{4}{c}{\# Clients = 900}  \\
		\cmidrule(lr){3-6}  \cmidrule(lr){7-10}  \cmidrule(lr){11-14}
		& & $\overline{acc}$ & $\protect\widebreve{acc}$ & $\sigma_{acc}$ & $\overline{uti} \pm  \sigma_{uti}$ &   $\overline{acc}$ & $\protect\widebreve{acc}$ & $\sigma_{acc}$ &       $\overline{uti} \pm  \sigma_{uti}$  &  $\overline{acc}$ & $\protect\widebreve{acc}$ & $\sigma_{acc}$ &       $\overline{uti} \pm  \sigma_{uti}$ \\ \midrule
        \multirow{2}{*}{\textit{Near-normal}}& Sync   &   70.21  &    54.23  & 13.17  &   41.68$\pm$18.87&   74.36  &   55.21  & 13.65  &   27.63$\pm$3.98    &  75.70   &  57.27  & 13.01   &   24.33$\pm$3.72   \\ 
	&	Async       &   80.81  &  67.65  & 10.01   &   54.93$\pm$3.81  &   77.97   & 62.50  & 11.77  &   72.84$\pm$9.14    &   78.34   & 63.01 &  14.57  &   79.41$\pm$4.31    \\ 
        \midrule
        \multirow{2}{*}{\textit{Homo-device}} & Sync &   71.35&   54.13&   15.34&   46.64$\pm$8.34&   73.19&   55.94&  13.34&   25.55$\pm$3.86&   75.14&   59.54&   12.19&    33.89$\pm$5.85   \\
        & Async &   80.17&   63.87&   11.58&   89.23$\pm$7.85&   80.05&   61.55&   12.31&   81.68$\pm$16.59&   78.21&   55.03&   15.22&   44.47$\pm$5.54   \\
        \bottomrule
    \end{tabular}
    \label{tab:homo-hetero-diff-asyn}
\end{table*}



\subsection{Homo-Hetero Gap Study for Advanced FL Features}
\label{append:homo-hetero-advanced-fl}
In Section \ref{sec:exp-gap}, we empirically show that there are substantial performance gaps for FedAvg between homogeneous and heterogeneous device distributions, in terms of various aspects such as accuracy, time to convergence, network traffic, and client utilization. 
In this section, we investigate whether the gap still exists for the advanced FL features within \oursys.
Specifically, we conduct experiments using the same baseline settings we adopted in Section \ref{exp:advanced-fl-tech}, and present the results in Table \ref{tab:homo-hetero-diff-pfl} for personalization, Table \ref{tab:homo-hetero-diff-compre} for communication compression, and Table \ref{tab:homo-hetero-diff-asyn} for asynchronous FL.

From the personalization results (Table \ref{tab:homo-hetero-diff-pfl}), we observe that the runs on \textit{Near-normal} distribution usually gain lower $\overline{acc}$ while higher $\protect\widebreve{acc}$ and $\sigma_{acc}$ than the runs on \textit{Homo-device}. For example, on \textit{Near-normal} distribution, FedBABU achieves 1.12\% $\sim$ 1.52\% lower $\overline{acc}$, while 2.27\% $\sim$ 6.79 \% higher $\protect\widebreve{acc}$ than the ones on \textit{Near-normal} distribution.
This suggests an enhancing effect of device heterogeneity on the bias associated with personalization, which is more severe than that demonstrated in existing homogeneous research work.

While for the compression results (Table \ref{tab:homo-hetero-diff-compre}), in a nutshell, when using the same advanced FL algorithms and parameter settings, the runs on \textit{Near-normal} distribution gain better accuracy than the runs on \textit{Homo-device} in the vast majority of cases.
Taking the INT8 quantization as an example, on \textit{Near-normal} distribution, it gains on average 2.43\% $\overline{acc}$ higher than the ones on \textit{Homo-device} distribution, indicating the great potential of compression technology when device heterogeneity is taken into account.

As for the asynchronous FL (Table \ref{tab:homo-hetero-diff-asyn}), we can see that compared with the runs on \textit{Homo-device}, the runs on \textit{Near-normal} distribution gain lower client utilization at the small device scales ($-43.3$ and $-8.84$ $\overline{uti}$ difference when the number of clients is 225 and 450 respectively), while higher client utilization at the large device scale ($+$ 34.94 $\overline{uti}$ when the number of clients is 900). 
The training acceleration brought by asynchronous FL is more and more obvious on a larger scale under heterogeneous devices, enlightening us on the applicable scenarios and potential improvement space of asynchronous aggregation in real FL applications.


\subsection{End-to-End Evaluation}
\label{append:end2end}
In section \ref{exp:advanced-fl-tech}, we study several advanced FL techniques with \oursys respectively. 
In this section, we examine the end-to-end performance on Twitter dataset by leveraging all the introduced advanced techniques, and summarize the results in Table \ref{tab:end-to-end-res}.
Interestingly, we can find that when comparing the all-in-one method (the last line) with the methods ablating one of the three techniques (the middle three lines), all the three techniques have yielded corresponding gains in the metrics they excel at, \ie, $\overline{acc}$ for personalization, network traffic for INT8 quantization, and client utilization for asynchronous FL.
However, simple quantization still drags down the entire system (compared to the first line), as personalization at this point brings no greater improvement than the model degradation brought by the INT8 technique, which reveals that there is still much room for exploration in efficient personalization techniques.

\begin{table*}[h!]
    \centering
    \caption{The end-to-end evaluation of FS-Device on Twitter dataset under \textit{Near-normal} device distribution.}
    \label{tab:end-to-end-res}
    %\small
    \begin{tabular}{lccccccccc}
        \toprule                       
	\multirow{2}{*}{Method} & \multicolumn{3}{c}{\# Clients = 250} & \multicolumn{3}{c}{\# Clients = 500} & \multicolumn{3}{c}{\# Clients = 1,000}  \\
	\cmidrule(lr){2-4}  \cmidrule(lr){5-7}  \cmidrule(lr){8-10}
	& $\overline{acc}$ & net. traffic & $\overline{uti} \pm \sigma_{uti}$ &   $\overline{acc}$ &  net. traffic&     $\overline{uti} \pm \sigma_{uti}$  &  $\overline{acc}$ &  net. traffic&    $\overline{uti} \pm \sigma_{uti}$ \\ \midrule
    FedAvg, Syn, w/o INT8 &	61.02&	3.14&95.45  $\pm$  14.87&	62.44&	4.65&	120.84  $\pm$ 18.59&   63.93&  6.09&    123.88 $\pm$ 19.01	\\ \hline
	+FT, +Asyn&	70.26&	2.04&	131.86 $\pm$   12.59&	70.83&	2.82&   159.79 $\pm$ 15.98&    70.74&  4.58&  78.09 $\pm$  12.32  	\\ \hline
	  +FT, +INT8&	53.94&	2.16&	78.09 $\pm$	12.32&    53.93&  2.98&  99.80 $\pm$  15.92&  46.09&	3.59&  111.56 $\pm$   17.10	\\ \hline
	+INT8, +Asyn&   52.45&   1.89& 142.41 $\pm$   14.44&   53.15&   2.52&  137.31 $\pm$    17.55&  53.23   &   3.55&    142.90  $\pm$   19.91	\\ \hline		
	\textbf{+FT, +Asyn, +INT8}&	53.31&	1.91&  134.13 $\pm$    17.78&  53.57&  2.49&	 139.23 $\pm$	16.98& 51.35   &3.58 &  149.13  $\pm$ 19.24	\\    
        \bottomrule
    \end{tabular}
\end{table*}
