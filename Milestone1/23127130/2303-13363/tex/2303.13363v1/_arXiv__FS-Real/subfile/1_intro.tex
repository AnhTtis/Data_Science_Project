\section{Introduction}
\label{sec:intro}

Cross-device federated learning (FL) aims to leverage a large-scale distributed group of clients to collaboratively train high-quality machine learning models, while retaining the client data locally to devices for privacy protection \cite{fedavg, yang2019federated, advancesopenfl,konevcny2016federated, nguyen2021federated}. 
In recent years, the boom of edge intelligence and the growing demand for data privacy protection has spawned remarkable innovations in FL algorithms \cite{lim2020federated, li2020review, mothukuri2021survey}. 
However, there is still a considerable gap between cross-device FL research and practical solutions in real-world application scenarios, particularly in terms of runtime characteristics and the scale of the participated devices.
\textit{Without bridging the gap soon, the effectiveness and usability of both existing and follow-up FL research are challenged.}

To be specific, let us start by explaining why the gap exists in FL:
\textbf{(a) Homogeneous v.s. Heterogeneous Device}. Most existing studies conduct FL experiments based on homogeneous device settings: each participated device has the same computational, storage, and communication capabilities \cite{advancesopenfl, li2020federated}. 
In real-world applications, however, the device capabilities are highly heterogeneous and dynamically changing. For example, the computational power of mainstream smartphones can vary by orders of magnitude \cite{mobile-bench,wu2019machine}. 
Further, the available computational and communication resources of an FL device can change due to the competition from other apps on the same device and the spotty network connection \cite{flsurveyiot}. 
\textbf{(b) Device Scales.} Furthermore, it is highly challenging to conduct research at scale with real heterogeneous devices. Scaling heterogeneous devices requires researchers to deal with diverse device hardware and software environments to reflect real computing capacities, and build distributed communications through the network interface cards to reflect real transmission capabilities \cite{flsyssurvey}. 
Due to limited resources and complex software stacks, most existing works focus on standalone simulation studies with high-performance servers, which introduces non-negligible simulation errors compared with real scenarios. For more details, please see the observations in Section \ref{exp:oursys}.


Notably, the impact of these two key factors on recent advanced FL algorithms is non-trivial while still under-explored. 
For example, in practical FL scenarios, it is important to get a high-performance model in a short time and with few resources.
Personalized FL shows promising results to improve the performance when dealing with Non-IID data \cite{towardspfl, pflbench, zhu2021federated};
communication compression \cite{Haddadpour2020FederatedLW, konevcny2016federated2,wang2022communication} and asynchronous aggregation \cite{nguyen2022fedbuff, xie2019asynchronous, chen2020asynchronous} can effectively accelerate FL training speed by reducing traffic and increasing device utilization respectively.
However, \textit{there are inconsistent training dynamics, convergence time and trained model of FL with heterogeneous and homogeneous devices}, which are closely related to the performance of individual clients.
Furthermore, such inconsistency is exacerbated by various real-world scales of FL devices, calling for further validation of effectiveness and usability of existing solutions.

To bridge this gap, in this paper, we propose a prototyping system for real-world FL, \oursys, based on which we aim to identify major challenges in real heterogeneous device and scalable FL scenarios, providing reusable functionality and valuable insights for further FL research, development and deployment.
Specifically, \oursys contains a flexible heterogeneous device runtime, a group of efficient and scalable FL device executors and FL server, and implementations for a number of practical and advanced FL enhancement techniques with easy extensions.
(1) The \oursys runtime enables users to easily and cost-efficiently study FL performance in real FL scenarios, where the devices can have diverse scales and highly different hardware capabilities, e.g., with configurable computation and communication resources such as CPU cores and network types.
(2) Based on a computation engine optimized for edge intelligence, MNN \cite{proc:osdi22:walle}, we implement efficient lightweight executors for FL behaviors such as local training, which can be executed in real Android phones and IoT devices. 
To enhance the training efficiency and scalability, we carefully decouple and schedule the FL plan so that server-side processing behaviors such as sending, receiving, and aggregation of information can be efficiently parallelized and scaled up. 
(3) Further, we incorporate several representative personalized FL, communication compression, and asynchronous FL algorithms into \oursys, and provide simple and easily customizable programming interfaces for future extensions.

With the implemented \oursys, we conduct an extensive evaluation to demonstrate its usability, efficiency, and scalability. We first examine the performance of existing FL algorithms with different hardware device distributions and varied device scales.
We find that both distribution differences and quantity differences in heterogeneous devices bring a substantial model performance gap between homogeneous and heterogeneous scenarios, and such gap becomes more noticeable in terms of fairness-related metrics and at large device scales. These findings confirm the strong need of \oursys.
Besides, heterogeneous devices exhibit complex differences in convergence speed, network traffic and client utilization, which are specific to the device scales and device distributions, challenging the utility of FL algorithms in real-world cross-device federated learning scenarios. 
Moreover, we find advanced FL techniques, such as personalized FL, communication compression, and asynchronous aggregation, work well in most evaluated heterogeneous device cases, while the performance gain of these techniques suffers from high variance especially at large scales, calling for future attention to their usability and robustness.

Our contributions are summarized as follows: 
\begin{itemize}
	\item We propose an efficient and scalable prototype system \oursys for real-world cross-device FL, which supports heterogeneous device runtime and includes several advanced FL utility features such as personalization, communication compression, and asynchronous concurrency. 
	\item With experiments conducted on up to thousand-scale heterogeneous devices (android phones), we quantify and analyze how the underexplored factor, heterogeneous device, affects the FL performance under different scenarios and different device distributions, and point out some challenges and open issues when scaling up FL.
	\item We release the system at https://github.com/alibaba/Federated
 Scope/tree/FSreal. With the open-sourced system and provided insights, we hope that our work can greatly facilitate further research and broad applications on real-world FL scenarios that would otherwise be infeasible without a dedicated real system.
 	
\end{itemize}

