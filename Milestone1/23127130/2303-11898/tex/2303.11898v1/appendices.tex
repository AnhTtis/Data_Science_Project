\begin{appendices}

\section{Additional technical details}

\subsection{Linear blend skinning\label{appendix:lbs}}

Given a canonical mesh and its associated \emph{skeleton}, which is a collection of bones $b\in\{1,\dots,B\}$, and the pose parameters $\theta$, which are the rotations of the bone joints, linear blend skinning (LBS) allows to~\emph{pose} the canonical mesh using the pose parameters alone. In order to account for transitions between bones, each canonical vertex $\bar{\bx}$ is not assigned to a single bone, but rather~\emph{softly} attached to different bones by its \emph{skinning weights} $w(\bar{\bx})$.

Bones form a tree, where $b'=\operatorname{par}(b)$ denotes the parent of bone $b$, except for the root bone $b=1$ which has no parent.

Then, given a 3D point $\tilde{\bx}$ defined in the reference frame of bone $b$, we can express it in the reference frame of the parent bone as $\bx' = g_b(\theta) \tilde{\bx}$, where $g_b(\theta)\in SE(3)$ is the rigid transformation between the two bones.
Hence, the pose vector $\theta$ specifies three rotation angles for each transformation $g_b$ (the translation component is given by the parent bone's length); only for the root transformation $g_1$, which positions the skeleton in world space, $\theta$ specifies the translation component as well.
In order to express the 3D point in world coordinates $\bx = G_b(\theta) \tilde{\bx}$, we compose recursively the transformation towards the root:
$$
G_b(\theta) = G_{\operatorname{par}(b)}(\theta) \circ g_b(\theta),~~~G_1(\theta) = g_1(\theta).
$$

Consider now a 3D point $\bar{\bx}$ in canonical space, rigidly attached to bone $b$.
In order to pose it, we first express it relative to its bone as $\tilde{\bx} = G_b^{-1}(\theta_0)\bar{\bx}$, where $\theta_0$ is the \emph{canonical pose}.
Then, we map it to world space by as $\bx = G_b(\theta)\tilde{\bx}$.
In practice, each point $\bar{\bx}$ is attached to all bones to a degree expressed by the \emph{skinning weights} $w(\bar{\bx}) \in \mathbb{R}^B$, which are non-negative and sum to one.
The individual maps are linearly combined (called \emph{blend skinning}), resulting in the affine map
\begin{equation}\label{eq:skinning}
A(\bar{\bx}; \theta)
=
\sum_{b=1}^B
w_b(\bar{\bx})~
G_b(\theta) \circ G_b^{-1}(\theta_0).
\end{equation}
%
For simplicity, in the main paper we consider the composed affine map for each bone  $A_b(\theta) = G_b(\theta) \circ G_b^{-1}(\theta_0)$, assuming $\theta_0$ as fixed.

Finally, the corresponding posing function resulting from linear blend skinning is
\begin{equation}\label{eq:fwd}
  x = h(\bar{\bx}; \theta) = A(\bar{\bx} ; \theta) \bar{\bx}.
\end{equation}

\subsection{Customized human mesh extraction\label{appendix:mesh}}

While previous methods have used algorithms such as marching cubes to extract meshes from implicit volumetric representations, we propose to use a render-based approach instead, as our canonical model contains noise in the regions where no supervision was provided due to the threshold $\tau$. To this end, we render frames from the trained canonical factorized model, along with their depth maps and estimated foreground masks, with a circular camera trajectory around the object (in a turntable fashion). 

The depth maps are computed by measuring the depth w.r.t.\ to the current camera of the expected termination $\hat{\textbf{x}}_t$ of each camera ray, defined as 
%
\begin{equation}
    \hat{\textbf{x}}_t =\sum_{i=0,\dots,N-1} (T_i - T_{i+1}) \textbf{x}_i,
\end{equation}
%
as done in {NeRF~[21]}. Similarly, the foreground masks are obtained by computing the expected ray opacity as
%
\begin{equation}
\hat{o} =\sum_{i=0,\dots,N-1} (T_i - T_{i+1})= 1 - T_N,    
\end{equation}
%
The estimated object foreground masks are obtained by retaining the pixels where $\hat{o} > 0.5$. 

Then, each depth-map is \emph{unprojected} using the camera parameters and combined with the color render to obtain a set of 3D points, which are then fused with those from other images to obtain a dense point cloud. This point cloud is finally filtered using the foreground masks, such that only points that are classified as foreground for all images are kept. 

Next, this dense point cloud is converted into a mesh by applying the screened Poisson surface reconstruction algorithm~\cite{screenedpoission}, and simplified using an edge-collapse technique~\cite{surfacesimplif} such that the number of final faces is around ~15K, which is on the order of magnitude to the template mesh in SMPL. The surface normals for the extracted canonical mesh are mapped to the surface normals of the closest vertices of the underlying SMPL template mesh. Note that these normals are not used for shading, but only for running the screened Poisson surface reconstruction.

Finally, in order to be able to perform skinning of the extracted canonical mesh, we transfer the skinning weights from the SMPL template using the nearest vertices. The process of canonical mesh extraction is illustrated in Fig.~\ref{fig:mesh_extraction}. Note that the colors of the extracted canonical  mesh are solely used for visualization purposes, and not used in the proposed rasterization-based raymarching, which only requires the vertices and triangles of the canonical mesh to guide the raymarching on the factorized volumetric representation.

\subsection{Training losses}

We present some additional details about the losses used for training. At each training iteration, a batch $\mathcal{B}=\{\mathcal{P}_i\}_{i=1,\dots,6}$ of six image patches $\mathcal{P}$ of size $32\times 32$ is constructed, and the our proposed model is used to estimate the image color $I(u)$ for each pixel $u\in P$. Then we use three different loss terms for optimizing the proposed model: (i) a photometric loss $\mathcal{L}_{\text{rgb}}$, (ii)  a perceptual loss (LPIPS) $\mathcal{L}_{\text{lpips}}$, and, (iii) a sparsity regularization loss $\mathcal{L}_{\text{sparse}}$. Our overall loss is thus:
%
$$
\mathcal{L}(\mathcal{B}) = \alpha \mathcal{L}_{\text{rgb}}(\mathcal{B}) + \beta \mathcal{L}_{\text{lpips}}(\mathcal{B}) + \gamma \mathcal{L}_{\text{sparse}},
$$
%
with:
\begin{align}
    \mathcal{L}_{\text{rgb}}(\mathcal{B})&= \frac{1}{N} \sum_{u\in \mathcal{B}} \lVert  I(u) - I_{\text{gt}}(u) \rVert_2^2\\
    \mathcal{L}_{\text{lpips}}(\mathcal{B})&= \frac{1}{6} \sum_{\mathcal{P}
\in\mathcal{B}} \text{LPIPS}_{\text{vgg}}(I(\mathcal{P}),I_{\text{gt}}(\mathcal{P}))\\
    \mathcal{L}_{\text{sparse}}&= \frac{1}{R_\sigma\cdot HWD} \sum_{(M,v) \in \mathcal{W}} \sum_{rxyz} (Mv)^+,
\end{align}
%
where $N=6\times 32\times 32$ is the total number of pixels in the training batch, $\text{LPIPS}_{\text{vgg}}$ is the perceptual loss proposed in~\cite{lpips} using the VGG-Net feature extractor~\cite{vggnet}, and the sparsity loss is applied over all three factors $\mathcal{W} = \{
(M^{YX}, v^Z), (M^{YZ}, v^X), (M^{XZ}, v^Y)\}$ of the implicit volumetric representation of the density $\sigma$, and across all channels $R_\sigma$.
%
In addition, the coefficients $\alpha, \beta, \gamma$ which modulate each loss term vary as the training progresses, and we can therefore define them as functions of the training iteration $i=1,\dots,30\,000$:
%
\begin{align}
    \alpha(i)& = \begin{cases}
         1 - 0.8 \cdot i/10\,000 &  \text{if } i < 10\,000 \\
         0.2 & \text{otherwise} 
    \end{cases}\\ 
    \beta(i)& = \begin{cases}
         0.8 \cdot i / 10\,000 &  \text{if } i < 10\,000 \\
         0.8 & \text{otherwise} 
    \end{cases}\\
    \gamma(i)& =\begin{cases}
        0 & \text{if } i < 2\,000 \\
        8\cdot 10^{-5} & \text{if } 2\,000 \leq i < 4\,000 \\
        5\cdot 10^{-5} & \text{otherwise}
    \end{cases}
\end{align}


\begin{figure}[h!]
    \centering
    {\footnotesize
        \begin{tabular}{ccc}
        \includegraphics[width=0.24\columnwidth]{figures/mesh_extr_1.png} & \includegraphics[width=0.24\columnwidth]{figures/mesh_extr_2.png} & \includegraphics[width=0.24\columnwidth]{figures/mesh_extr_3.png} \\
        (a) Dense PCL & (b) Mesh (100K faces) & (c) Mesh (15K faces)
    \end{tabular}
    }
    \caption{\textbf{Canonical mesh extraction.} We first generate a dense point cloud (a), from which we perform screened Poisson reconstruction (b), and, finally mesh simplification to 15K faces (c).}
    \label{fig:mesh_extraction}
\end{figure}

\subsection{Proposed rasterization-guided local raymarching vs. phong shading}

As an ablation study, we show in~\cref{fig:phong} the comparison of rendering the posed template mesh with our proposed method against using standard phong shading based on vertex colors. Note that as a texture of the human is not available, many details are lost when applying phong shading directly. In addition, this rendering technique requires an artificial light source and generates unrealistic reflection effects. On the contrary, our proposed method recovers the textures details from the learnt volumetric representation, and new views can be rendered directly without any additional artificial light source.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{\columnwidth}
    \includegraphics[width=\columnwidth]{figures/raster2.png}
    \caption{Local volumetric rendering}
    \end{subfigure}
    
    \centering
    \begin{subfigure}{\columnwidth}
    \includegraphics[width=\columnwidth]{figures/raster1.png}
    \caption{Phong shading}
    \end{subfigure}
    \caption{\textbf{Comparison between rendering techniques.} We illustrate the results of the proposed local rasterization-based emission-absorption raymarching against rendering the template mesh directly with a standard phong shading.}
    \label{fig:phong}
\end{figure}

\section{Demos and source code}
\subsection{VR Mixed reality demo}

We have built a VR Mixed reality demo that demonstrates our proposed method, and rendering at 40FPS in consumer-level standalone VR headset, using the reconstructions obtained from the ZJU-mocap scenes.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/vr_mixed_reality.jpg}
    \caption{\textbf{VR Mixed reality demo.} The dynamic human is rendered using VR-passthrough at 40FPS on a standalone VR device.}
    \label{fig:vr_demo}
\end{figure}

Several recordings of these scenes playing on the VR headset, as illustrated in \Cref{fig:vr_demo}, are available in our website \url{https://real-time-humans.github.io/}. Note that the background scene is rendered in mixed reality thorough RGB passthrough using the VR headset's frontal RGB camera, giving the effect of an AR device. 


\subsection{Desktop WebGL demo}

Additionally we developed a Desktop WebGL demo that implements the proposed real-time volumetric rendering and runs inside the web browser, as illustrated in \Cref{fig:web_demo}. The demo is available in our website \url{https://real-time-humans.github.io/}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{figures/webgl.jpg}
    \caption{\textbf{WebGL desktop demo.} The demo is available in our website.}
    \label{fig:web_demo}
\end{figure}

\subsection{Source code.} We will release our source code to allow for reproducibility. It will be released under an open-source licence. The VR and WebGL demo apps will be released in binary format.

\end{appendices}