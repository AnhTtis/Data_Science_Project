{
    "arxiv_id": "2303.17152",
    "paper_title": "Mixed Autoencoder for Self-supervised Visual Representation Learning",
    "authors": [
        "Kai Chen",
        "Zhili Liu",
        "Lanqing Hong",
        "Hang Xu",
        "Zhenguo Li",
        "Dit-Yan Yeung"
    ],
    "submission_date": "2023-03-30",
    "revised_dates": [
        "2023-06-08"
    ],
    "latest_version": 2,
    "categories": [
        "cs.CV",
        "cs.LG"
    ],
    "abstract": "Masked Autoencoder (MAE) has demonstrated superior performance on various vision tasks via randomly masking image patches and reconstruction. However, effective data augmentation strategies for MAE still remain open questions, different from those in contrastive learning that serve as the most important part. This paper studies the prevailing mixing augmentation for MAE. We first demonstrate that naive mixing will in contrast degenerate model performance due to the increase of mutual information (MI). To address, we propose homologous recognition, an auxiliary pretext task, not only to alleviate the MI increasement by explicitly requiring each patch to recognize homologous patches, but also to perform object-aware self-supervised pre-training for better downstream dense perception performance. With extensive experiments, we demonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the state-of-the-art transfer results among masked image modeling (MIM) augmentations on different downstream tasks with significant efficiency. Specifically, our MixedAE outperforms MAE by +0.3% accuracy, +1.7 mIoU and +0.9 AP on ImageNet-1K, ADE20K and COCO respectively with a standard ViT-Base. Moreover, MixedAE surpasses iBOT, a strong MIM method combined with instance discrimination, while accelerating training by 2x. To our best knowledge, this is the very first work to consider mixing for MIM from the perspective of pretext task design. Code will be made available.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.17152v1",
        "http://arxiv.org/pdf/2303.17152v2"
    ],
    "publication_venue": "Accepted by CVPR 2023"
}