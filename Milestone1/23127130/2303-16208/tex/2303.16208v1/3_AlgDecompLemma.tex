\section{Our algorithmic decomposition lemma}
\label{sec:decomp}

Here we present an algorithm that constructs a decision tree of depth $d$ for a a distribution $\mcD$, and analyze its correctness and complexity. 
Throughout this section, we assume access to an oracle that gives the exact influences of variables in $f_\mcD$ or any of its restrictions. 
In the next sections we will show that the influences can be estimated from random examples for monotone distributions, and from subcube conditional examples for general distributions. 
%We will also assume an oracle for the densities of the leaves, up to accuracy $\eps \cdot 2^{-{d+1}}$ and failure probability $\delta \cdot 2^{-d}$ --- by Chernoff and union bounds this oracle can be simulated using random examples.

\begin{theorem}[Learning decision tree distributions]
    \label{thm:decompose}
    Let $\mcD$ be a distribution that is representable by a depth-$d$ decision tree.
    The algorithm $\BuildDT$ returns a depth-$d$ tree representing a distribution $\mcD'$
    such that $\TV(\mcD,\mcD') \le \eps$ w.h.p..
    Given access to a unit time influence oracle, its running time is $n \cdot (d/\eps)^{O(d)}$.
\end{theorem}

The algorithm $\BuildDT$ is an exhaustive search over a subset of depth-$d$ decision trees. 
We characterize this subset as follows: 

\begin{definition}[Everywhere $\tau$-influential]
Let $T$ be a tree and $\nu$ be an internal node with root variable $i(\nu)$. $T$ is \emph{everywhere $\tau$-influential} with respect to some $f$ if for every $\nu \in T$, we have $\Inf_{i(\nu)}(f_\nu) \ge \tau$.
\end{definition}

\begin{figure*}[t] 
  \captionsetup{width=.9\linewidth}

\begin{tcolorbox}[colback = white,arc=1mm, boxrule=0.25mm]
\vspace{3pt} 

$\BuildDT(\mcD, \pi, d, \tau)$:

\begin{itemize}%[align=left]
    \item[]\textbf{Input:} Random examples from $\mcD$, restriction $\pi$, influence oracle for $(f_\mcD)_\pi$, depth parameter $d$, influence parameter $\tau$.
    \item[]\textbf{Output:} A decision tree $T$ that minimizes $\Ex_{\bell \in T}[\Inf((f_\mcD)_{\bell})]$ among all depth-$d$, everywhere $\tau$-influential trees.
\end{itemize}
\begin{enumerate}
    \item Let $S \subseteq [n]$ be the set of variables $i$ such that $\Inf_i((f_\mcD)_\pi) \ge \tau$.
    \item If $S$ is empty or $d=0$, return the leaf labeled with $2^{|\pi|} \cdot \Pr_{\bx \sim \mcD}[\bx\text{ is consistent with }\pi]$.
    \item Otherwise: 
    \begin{enumerate}
        %\item Set $M[\pi, s] = M[\pi, s-1]$. 
        \item For each $i \in S$, let $T_i$ be the tree such that 
            \begin{align*}
                \mathrm{root}(T_i) &= x_i \\
                \textnormal{left-subtree}(T_i) &= \BuildDT(\mcD, \pi \cup \{x_i = -1\}, d-1, \tau) \\
                \textnormal{right-subtree}(T_i) &= \BuildDT(\mcD, \pi \cup \{ x_i = 1\}, d-1, \tau)
            \end{align*}
        \item Return the tree among the $T_i$'s defined above that minimizes $\Ex_{\bell \in T_i}[\Inf((f_\mcD)_{\bell})]$.
    \end{enumerate}
\end{enumerate}

% $\BuildDT(\mcD, \pi, d, \tau)$:
% \begin{itemize}%[align=left]
%     \item[]\textbf{Input:} Random examples from $\mcD$, influence oracle for $(f_\mcD)_\pi$, restriction $\pi$, depth parameter $d$, influence parameter $\tau$.
%     \item[]\textbf{Output:} A decision tree $T$ that minimizes $\Ex_{\ell \in T}[\Inf((f_\mcD)_\ell)]$ among all depth-$d$, everywhere $\tau$-influential trees.
% \end{itemize}
% \begin{enumerate}
%     \item Let $S \subseteq [n]$ be the set of variables $i$ such that $\Inf_i((f_\mcD)_\pi) \ge \tau$.
%     \violet{
%     \item If $S$ is empty or $d=0$, return the leaf labeled $2^{|\pi|} \cdot \Pr_{\bx \sim \mcD}[\bx\text{ is consistent with }\pi]$.}
%     \item Otherwise: 
%     \begin{enumerate}
%         %\item Set $M[\pi, s] = M[\pi, s-1]$. 
%         \item For each $i \in S$, let $T_i$ be the tree such that 
%             \begin{align*}
%                 \mathrm{root}(T_i) &= x_i \\
%                 \textnormal{left-subtree}(T_i) &= \BuildDT(\mcD, \pi \cup \{x_i = -1\}, d-1, \tau) \\
%                 \textnormal{right-subtree}(T_i) &= \BuildDT(\mcD, \pi \cup \{ x_i = 1\}, d-1, \tau)
%             \end{align*}
%         \item Return the tree among the $T_i$'s defined above that minimizes $\Ex_{\ell \in T_i}[\Inf((f_\mcD)_\ell)]$.
%     \end{enumerate}
% \end{enumerate}
\end{tcolorbox}

\caption{$\BuildDT$ recursively searches for the depth-$d$, everywhere $\tau$-influential tree of minimal influence at the leaves.}
\label{fig:BuildDT}
\end{figure*} 

\subsection{Correctness} 
Here we show that under the oracle assumptions described above, $\BuildDT$ returns a tree within TV distance $\eps$. The proof will rely on the following fact, which relates TV distance to the uniform $\ell_1$ error of the tree with respect to $f_\mcD$. 

\begin{fact}[TV distance = label error]
\label{fact:distance error}
     \begin{align*}
			\TV(\mcD, \mcD') &= \frac{1}{2} \cdot \|\mcD - \mcD'\|_1 \\
							 &= 2^{-(n+1)} \cdot \|2^n \mcD - 2^n \mcD'\|_1 \\
							 &= 2^{-(n+1)} \cdot \|f_\mcD - T'\|_1.
        \end{align*}
\end{fact}

First, we will show that $\BuildDT$ outputs a decision tree $T'$ with small average influence at the leaves.
Then, we will show that this implies that the uniform $\ell_1$ error of $T'$ with respect to $f_\mcD$ is small.
Correctness follows from the equivalence between $2^{-(n+1)}\| f_\mcD- T' \|_1$ and $\TV(\mcD, \mcD')$. \\

The claim that $\BuildDT$ outputs a decision tree $T'$ with small average influence at the leaves extends a lemma from \cite{BLQT21focs}, instantiated here for the metric space $\R$ equipped with the  $\ell_1$-norm:

% \begin{lemma}[Theorem 5 of \cite{BLQT21focs}]
% \label{lem:blqt}\anote{Should we just write this with $\mcY = [0, 2^n]$? It will link better with the next part}
% Let $\mathcal{Y}$ be a metric space. Let $f : \bits^n \to \mathcal{Y}$ be representable by a depth-$d$ DT $T$. Then there exists $T^\star$ such that the following are satisfied:

% \begin{enumerate}
%     \item The size and depth of $T^\star$ are at most the size and depth of $T$,
%     \item $T^\star$ is everywhere $\tau$-influential with respect to $f$,
%     \item $\mathrm{dist}_{\mathcal{Y}}(f(\bx) - T^\star(\bx)) \le d\tau$.
% \end{enumerate}
% \end{lemma}


\begin{lemma}[Theorem 5 of \cite{BLQT21focs}]
\label{lem:blqt} Let $f : \bits^n \to \R$ be representable by a depth-$d$ DT $T$. Then there exists $T^\star$ such that the following are satisfied:

\begin{enumerate}
    \item The size and depth of $T^\star$ are at most the size and depth of $T$,
    \item $T^\star$ is everywhere $\tau$-influential with respect to $f$,
    \item $2^{-n} \cdot \| f - T^\star \|_1 \le d\tau$.
\end{enumerate}
\end{lemma}

% This lemma suffices for \cite{BLQT21focs} because they have access to labelled examples, allowing them to compute $\ell_1$ error and directly search for trees that minimise it. 

In our \BuildDT, we cannot compute $\|f_\mcD - T'\|_1$ and hence cannot search for trees that minimise this error. Instead, we find trees that minimise the expected total influence at the leaves. The following lemma relates these two values:


\begin{lemma}[Expected total influence and $\ell_1$ error]\label{lem:inf_leaves_l1}
Let $f : \bits^n \to \R$ be representable by a depth-$d$ DT, and let $T'$ be any other DT. Then:

\begin{equation*}
    \Ex_{\bell \in T'} [\Inf(f_{\bell})] \leq 4d \cdot 2^{-n} \|f - T'\|_1
\end{equation*}
\end{lemma}

\begin{proof}
	Since $f$ is representable by a depth-$d$ decision tree, its maximum sensitivity (and the sensitivity of each of its leaf restrictions) must be at most $d$. Therefore, for any leaf $\ell \in T'$, \Cref{lem:inf_s_var} asserts that $\Inf(f_{\ell}) \leq  2d \cdot \Var^{(1)}(f_{\ell})$. Moreover, 
	\begin{align*}
	    \Var^{(1)}(f_{\ell}) 
	        &= \Ex_{\bx, \by \sim \mcU^n} |f_\ell(\bx) - f_\ell(\by)| \\
	        &= \Ex_{\bx, \by \sim \mcU^n} |f_\ell(\bx) - T'_\ell + T'_\ell - f_\ell(\by)| \\
	        &\leq 2\cdot \Ex_{\bx \sim \mcU^n} |f_\ell(\bx) - T'_\ell| \tag{Triangle ineq.}\\
	        &= 2\cdot \Ex_{\bx \sim \mcU^n} [|f(\bx) - T'(\bx)| \mid \bx \in \ell].
	\end{align*}
	
	Therefore, 
	\begin{align*}
	    \Ex_{\bell \in T'} [\Inf(f_{\bell})] 
	        &\leq 2d \cdot \Ex_{\bell \in T'} [\Var^{(1)}(f_{\bell})] \\
	        &\leq 4d \cdot \Ex_{\bell \in T'} \Ex_{\bx \sim \mcU^n} [|f(\bx) - T'(\bx)| \mid x \in \ell] \\
	        &= 4d \cdot \Ex_{\bx \sim \mcU^n} |f(\bx) - T'(\bx)| \\
	        &= 4d  \cdot 2^{-n} \cdot \|f(\bx) - T'(\bx)\|_1. \qedhere 
	\end{align*}	
\end{proof}

As a corollary of \Cref{lem:blqt} and \Cref{lem:inf_leaves_l1}, we get our pruning lemma stated in terms of influences:

\begin{corollary}[Pruning lemma with expected total influence at leaves]
\label{cor:influence}
Let $f : \bits^n \to \R$ be representable by a depth-$d$ DT $T$. Then there exists $T^\star$ such that the following are satisfied:

\begin{enumerate}
    \item The size and depth of $T^\star$ are at most the size and depth of $T$,
    \item $T^\star$ is everywhere $\tau$-influential with respect to $f$,
    \item $\Ex_{\bell \in T^\star}[\Inf(f_{\bell})] \leq 4d^2 \tau$.
\end{enumerate}

\end{corollary}

% \begin{corollary}[Pruning lemma for total influence at leaves]
% \label{cor:influence}
% Let $f : \bits^n \to [0, 2^n]$ be representable by a depth-$d$ DT $T$. Then there exists $T^\star$ such that the following are satisfied:

% \begin{enumerate}
%     \item The size and depth of $T^\star$ are at most the size and depth of $T$,
%     \item $T^\star$ is everywhere $\eps/8d^2$-influential with respect to $f_\mcD$,
%     \item $\Ex_{\bell \in T^\star}[\Inf((f_\mcD)_{\bell} )] \leq \eps/2$.\anote{This shouldn't have the 2 here right?} \jnote{oops, yes this should be $\eps/2$ instead of $2\eps$ and the setting of $\tau$ should fix this}
% \end{enumerate}

% \end{corollary}

% \begin{proof}
%     \violet{
%     Let $T^*$ be the tree guaranteed by applying \Cref{lem:blqt} to $f$. Then 1 and 2 follow immediately. 
%     }
% 	To prove 3, we first note that because $f$ is reprsentable by a depth-$d$ decision tree,
% 	its maximum sensitivity (and the sensitivity of each of its leaf restrictions) must be at most $d$.
% 	Then \Cref{lem:inf_s_var} asserts that	$\Ex_{\bell \in T^\star} [\Inf(f_{\bell})] \le d \cdot \Ex_{\bell \in T^\star}[\Var^{(1)}[f_{\bell}]]$.
% 	Then, for each $\ell$, we note that $\Var^{(1)}(f_\ell) \le 2\Var_\mu((f_\mcD)_\ell)$, 
% 	and that $\Var_\mu((f_\mcD)_\ell)$ minimizes $\Ex_{\bx \sim \mcU^n}|(f_\mcD)_\ell(\bx) - c|$ over all constants $c$.\anote{I think this isnt true but we can just direct triangle ineq?}  
% 	Therefore, 
% 	\[\Var^{(1)}[(f_\mcD)_\ell] \le 2\Var_\mu[(f_\mcD)_\ell] \le 2\Ex_{\bx \sim \mcU}\big [ |(f_\mcD)_\ell(\bx) - T^\star_\ell(\bx)| \big ] = 2^{-(n -1)} \cdot \|(f_\mcD)_\ell - T^\star_\ell\|_1.\]\jnote{using $2^{-(n-1)}$ instead of $2^{-(n -|\ell| - 1)}$ to go along with the notation of restricted functions, not restricted domains. This is our choice though and can be changed.}
% 	Having established that 
% 	\[\Ex_{\ell \in T^\star}[\Inf((f_\mcD)_\ell)] \le 2d \cdot \Ex_{\bell \in T^\star} \big [ 2^{-(n-1)} \cdot \|(f_\mcD)_{\bell} - T^\star_{\bell}\|_1 \big ],\]
% 	we now make use of the fact that $\Ex_{\bell \in T^\star} \big [ 2^{-n} \cdot \|(f_\mcD)_{\bell} - T^\star_{\bell}\|_1 \big ]$ is just $2^{-n} \cdot \|f_\mcD - T^\star\|_1$, 
% 	which by the third point of \Cref{lem:blqt} is at most $d\tau$.
% 	The corollary follows from our choice of $\tau = \eps/8d^2$.
% \end{proof}

We now move to show that the tree output by \BuildDT \  satisfies $2^{-n} \cdot \|f_\mcD - T'\|_1 \le \eps$.


\begin{claim}
\label{claim:low error}
Let $\mcD$ be a distribution that is representable by a depth-$d$ decision tree, and let $T$ be the output of $\BuildDT$, with $\tau = \eps/8d^2$. Then, with high probability, $2^{-n} \cdot \|f_\mcD - T \|_1 \le \eps$.
\end{claim}


\begin{proof}
    First, we claim that $T$ minimizes $\E_{\bell \in T}[\Inf((f_\mcD)_{\bell})]$ among all depth-$d$, everywhere $\tau$-influential trees.
    This claim holds by induction on $d$: since 
    \[\E_{\bell \in T}[\Inf((f_\mcD)_{\bell})] = \lfrac{1}{2}(\E_{\bell \in T_{\mathrm{left}}}[\Inf((f_\mcD)_{\bell})] + \E_{\bell \in T_{\mathrm{right}}}[\Inf((f_\mcD)_{\bell})]),\] 
    each candidate $T_i$ minimizes influence among all depth-$d$, everywhere $\tau$-influential trees with $x_i$ at the root 
    under the inductive assumption that $T_{\mathrm{left}}$ and $T_{\mathrm{right}}$ minimize influence for depth-$(d-1)$ trees. 
    Then $\BuildDT$ chooses the tree of smallest influence among all the candidate $T_i$'s,
    so it minimizes total influence at leaves among all trees in its search space of depth-$d$, $\tau$-influential trees. 
    
    Since \Cref{cor:influence} establishes the existence of a tree with average total influence at leaves $\le \eps/2$, 
    it follows that the influence $T$'s influence is also $\le \eps/2$.
    Furthermore, we may assume by standard Hoeffding bounds that with a sample size of $\poly(2^d, 1/\eps)$, each leaf's value estimate of $\E[f_\mcD(\bx)~|~\bx\text{ is consistent with }\ell] = 2^{|\ell|} \cdot \Pr_{\bx \sim \mcD}[\bx\text{ is consistent with }\ell]$ is accurate to within $\pm \eps/2$ w.h.p..  
    
    
    We can now show that $2^{-n} \cdot \|f_\mcD - T\|_1 \le \eps$. 
    Throughout this section, $x \in \ell$ will stand in as shorthand for ``$x\text{ consistent with the restriction at }\ell$''.
    \begin{align*}
    2^{-n} \cdot \|f_\mcD - T\|_1 &= 2^{-n} \cdot \sum_{x \in \bits^n} |f_\mcD(x) - T(x)|\\
					 &= 2^{-n} \cdot \sum_{\ell \in T}\sum_{x \in \ell} |(f_\mcD)_\ell(x) - T_\ell(x)| \\
					 &\le 2^{-n} \cdot \sum_{\ell \in T}\sum_{x \in \ell} |(f_\mcD)\ell(x) - \E[(f_\mcD)_\ell]| + |\E[(f_\mcD)_\ell] - T_\ell(\bx)|  \\
					 &= 2^{-n} \cdot \big(\sum_{\ell \in T} 2^{n-|\ell|} \cdot \Var_\mu((f_\mcD)_\ell) + \sum_{\ell \in T}2^{n-|\ell|} \cdot |\E[(f_\mcD)_\ell] - T_\ell|\big) \\
				     &\le \E_{\ell \in T} \Inf((f_\mcD)_\ell)) + \E_{\ell \in T} \big [|\E[(f_\mcD)_\ell(\bx)] - T_\ell| \big ] \tag{\Cref{lem:efron_stein}}\\
				     &\le \frac{\eps}{2} + \frac{\eps}{2} = \eps.\qedhere 
    \end{align*}
\end{proof}


We can now prove correctness and time bounds for $\BuildDT$.
\begin{proof}[Proof of \Cref{thm:decompose}]
Correctness follows by combining \Cref{claim:low error} and \Cref{fact:distance error}. To bound the running time, first we note that for all restrictions $\pi$, there are at most $d/\tau = 8d^3/\eps$ variables of influence at least $\tau$. This is because all restrictions of $f_\mcD$ are depth-$d$ decision trees, and for any depth-$d$ decision tree, the sum of all variable influences is at most $d$. 

Then the number of recursive calls to $\BuildDT$ is at most $(8d^3/\eps)^d$. Each call at an internal node makes $n$ calls to the unit-time influence oracle, and each call at a leaf processes $\poly(2^d, 1/\eps)$ samples to estimate the leaf label. Thus, the total running time is $n\cdot (d/\eps)^{O(d)}$, as desired.
\end{proof}



%\subsection{Relating distribution learning and function learning}\lnote{Update subsection title}

%We can relate the task of learning the DT for $\mcD$ to the regular function learning over the uniform distribution by relating the following:









% \section{Top-Down DT}
% Will write top-down algorithm in appendix too.
% In order to run something like a TopDown algorithm, we need a goal, a measure of progress towards the goal, a way to split the tree, and a guarantees that while the goal hasn't been reached, there is a way to split the tree that will make significant progress.

% In traditional DT learning of a function $f: \bits^n \to [0,1]$ we have:
% \begin{enumerate}
%     \item \textbf{Goal}: $\error(T, f) \leq \eps$.
%     \item \textbf{Measure of progress}: Impurity function $\mcG(T)$ which satisfies $\mcG(T) \geq \error(T, f)$ and starts with a bounded value. Examples: $\mcG(T) = \Ex_{\ell} [\Var_\ell] \leq 1$ or  $\mcG(T) = \Ex_{\ell} [ \Inf(f_\ell)] \leq \log s$ if $f$ is a size-$s$ DT. 
%     \item \textbf{Split}: Split leaf $\ell$ and index $i$ with highest score e.g.  $w_\ell \cdot \Delta_\ell$ (drop in impurity), $w_\ell \cdot \Ex[f(x) x_i]$, or $w_\ell \cdot  \Inf_i(f_\ell)$ (for uniform dist). The last two are equal for monotone $f$.
%     \item \textbf{Guarantee}: A guarantee that while $\error(T, f) > \eps$, there is a leaf with a high score. Example guarantees: KM weak-learning assumption $\Delta_\ell \geq \gamma^2 \eps_\ell^2$ or OSSS which gives $\max_i \Inf^{(\rho)}_i(f_\ell) \geq \Var^{(\rho)}[f_\ell]/\log s$ if $f$ is a size-$s$ DT.
% \end{enumerate}



% \begin{fact}[Relationship between bias at leaves and bias at root]
%     For any distribution $\mcD$, and $i,j \in [d]$.
%     \begin{equation*}
%         \bias_j(\mcD) = \Ex_{\bx \sim \mcD}\left[\bias_j( \mcD| \bx_i = x_i ) \right]
%     \end{equation*}
% \end{fact}

% \begin{corollary}
%     For any distribution $\mcD$ and $i \in [d]$,
%     \begin{equation*}
%         \Ex_{\bx_i \sim \mcD_i}[\bias(\mcD_{x_i = \bx_i})] = \bias(\mcD) - \bias_i(\mcD).
%     \end{equation*}
% \end{corollary}



