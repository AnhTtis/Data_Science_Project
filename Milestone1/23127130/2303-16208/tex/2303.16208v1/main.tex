\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsfonts,amsthm,epsfig}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{bm,xspace}
\usepackage{tcolorbox}
\usepackage{cancel}
\usepackage{fullpage}
\usepackage{liyang}
\usepackage{framed}
\usepackage{verbatim}
\usepackage{enumitem}
\usepackage{array}
\usepackage{multirow}
\usepackage{afterpage}
\usepackage{mathrsfs}
\usepackage{pifont} 
\usepackage{chngpage}
\usepackage[normalem]{ulem}
\usepackage{boxedminipage}
\usepackage{caption}
\usepackage{forest}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Given:}}

\usepackage{pgfplots}
\pgfplotsset{width=8cm,compat=newest}

% \newcommand{\lnote}[1]{\footnote{{\bf \color{blue}Li-Yang}: {#1}}}
% \newcommand{\jnote}[1]{\footnote{{\bf \color{red}Jane}: {#1}}}
% \newcommand{\anote}[1]{\footnote{{\bf \color{green}Ali}: {#1}}}
% \newcommand{\gnote}[1]{\footnote{{\bf \color{violet}Guy}: {#1}}}


%%%%%%%%%%%%%%%%%%%%
%
%  COLORS 
%
%%%%%%%%%%%%%%%%%%%%

\def\colorful{0}


\ifnum\colorful=1
\newcommand{\violet}[1]{{\color{violet}{#1}}}
\newcommand{\orange}[1]{{\color{orange}{#1}}}
\newcommand{\blue}[1]{{{\color{blue}#1}}}
\newcommand{\red}[1]{{\color{red} {#1}}}
\newcommand{\green}[1]{{\color{green} {#1}}}
\newcommand{\pink}[1]{{\color{pink}{#1}}}
\newcommand{\gray}[1]{{\color{gray}{#1}}}

\fi
\ifnum\colorful=0
\newcommand{\violet}[1]{{{#1}}}
\newcommand{\orange}[1]{{{#1}}}
\newcommand{\blue}[1]{{{#1}}}
\newcommand{\red}[1]{{{#1}}}
\newcommand{\green}[1]{{{#1}}}
\newcommand{\gray}[1]{{{#1}}}

\fi

%%%%%%%%%%%%%%%%%%%%
%
%  Paper-specific macros
%
%%%%%%%%%%%%%%%%%%%%

\newcommand{\bias}{\mathrm{bias}}
\newcommand{\TV}{\dist_{\mathrm{TV}}}

\newcommand{\BayesOpt}{\textsc{BayesOpt}}
\newcommand{\error}{\mathrm{error}}
%\renewcommand{\opt}{\mathsf{opt}}
\newcommand{\round}{\mathrm{round}}
\newcommand{\Sens}{\mathrm{Sens}}
\newcommand{\est}{\mathrm{est}}
\newcommand{\score}{\mathrm{Score}}
\newcommand{\Prune}{\mathrm{Prune}}
\newcommand{\BuildDT}{\textsc{BuildDT}}
\newcommand{\Search}{\textsc{Search}}
\newcommand{\UnbiasedEstimator}{\textsc{UnbiasedEstimator}}
\newcommand{\Reconstructor}{\textsc{Reconstructor}}


\newcommand{\ArbDist}{\mathcal{E}}
\newcommand{\InfEst}{\textsc{InfEst}}
\newcommand{\Lift}{\textsc{LiftLearner}}

% Macros for Shapley stuff
\def\sv{\mathrm{SV}}
\def\se{\mathrm{SE}}
\def\shap{\mathrm{Shap}}

% Misc macros
\newcommand{\paren}[1]{\left({#1}\right)}
\DeclareMathOperator*{\argmax}{arg\,max}

% Establishing creferences for enum items in a proposition environment
\newlist{enumprop}{enumerate}{1} % set up a dedicated enumeration environment
\setlist[enumprop]{label=\arabic*.,ref=\theproposition.\arabic*}
\crefalias{enumpropi}{proposition}


\makeatletter
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{
\newenvironment{rep#1}[1]{
 \def\rep@title{#2 \ref{##1}}
 \begin{rep@theorem}\itshape}
 {\end{rep@theorem}}}
\makeatother
%\theoremstyle{plain}


\newreptheorem{theorem}{Theorem}




\newcommand{\pparagraph}[1]{\bigskip \noindent {\bf {#1}}}

\newcommand{\myfig}[4]{\begin{figure}[H] \centering \includegraphics[width=#1\textwidth]{#2} \caption{#3} \label{#4} \end{figure}}




\begin{document}
\title{Lifting uniform learners via distributional decomposition  %\vspace{15pt} 
}

\author{}
\author{Guy Blanc \vspace{8pt} \\ \hspace{-5pt}{\sl Stanford}
\and \hspace{0pt} Jane Lange \vspace{8pt} \\ \hspace{-4pt}  {\sl MIT}
\and Ali Malik \vspace{8pt}\\ \hspace{-8pt} {\sl Stanford}
\and Li-Yang Tan \vspace{8pt} \\ \hspace{-8pt} {\sl Stanford}}  

\date{\vspace{15pt}\small{\today}}


\maketitle


% \begin{abstract} 
% We prove an algorithmic decomposition lemma for monotone high-dimensional distributions over $\zo^n$, showing how they can be efficiently broken down into a small number of close-to-uniform components, where ``efficiently" and ``small" scale with their inherent complexity.  A distribution $\mathcal{D}$ has {\sl decision tree complexity $d$} if its pmf can be computed by depth-$d$ decision tree; this 


% Given samples from a monotone distribution $\mathcal{D}$ whose pmf is computed by a depth-$d$ decision tree---which gives a decom the mixture of $2^d$ many uniform distrubtions over subcubes), our algorithm runs in time $\poly(n)\cdot d^{O(d)}$ and 
% \end{abstract} 

\begin{abstract}
    \input{0_abstract}    
\end{abstract}

% \footnote{\red{Keywords: PAC Learning, Semi-supervised learning, decision tree decomposition}}

\thispagestyle{empty}
\newpage 
\setcounter{page}{1}


\newcommand{\depth}{\mathrm{depth}}



\input{1_intro}
\input{2_Preliminaries}
\input{3_AlgDecompLemma}
\input{4_ComputingInf}
\input{5_Lifting}

\section*{Acknowledgements}

We thank the STOC reviewers for their detailed feedback, especially for the references to the literature on semi-supervised learning. 

Guy and Li-Yang are supported by NSF awards 1942123, 2211237, and 2224246. Jane is
supported by NSF Award CCF-2006664. Ali is supported by a graduate fellowship award from Knight-Hennessy Scholars at Stanford University.


\bibliographystyle{alpha}
\bibliography{ref}


\end{document}