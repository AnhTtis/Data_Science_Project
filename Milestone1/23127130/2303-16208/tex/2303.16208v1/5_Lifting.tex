\section{Lifting uniform distribution learners: Proof of~\Cref{thm:lift}}

In this section, we show how to lift algorithms that learn over the uniform distribution to algorithms that learn with respect to arbitrary distributions, where the sample complexity and runtime scale with the decision tree complexity of the distribution. We first define our goal formally.

\begin{definition}[Learning with respect to a class of distributions]
    \label{def:learn}
    For any concept class $\mathscr{C}$ of functions $f: \bits^n \to \zo$, $\eps, \delta > 0$, set of distributions $\mathscr{D}$ with support $\bits^n,$ and $m, d \in \N$, we say that an algorithm $\mcA$ $(\eps,\delta)$-learns $\mathscr{C}$ for distributions $\mathscr{D}$ using $m$ samples if the following holds: For any $\mathcal{D} \in \mathscr{D}$ and any $f^\star \in \mathscr{C}$, given $m$ iid samples of the form $(\bx, f^\star(\bx))$ where $\bx \sim \mathcal{D}$, $\mcA$ outputs a hypothesis $h$ satisfying
    \begin{equation*}
        \Prx_{\bx \sim \mathcal{D}}[f^\star(\bx) \neq h(\bx)] \leq \eps.
    \end{equation*}
    with probability at least $1 - \delta$.
    % Similarly, we say $\mcA$ $(\eps, \delta)$-learns $\mathscr{C}$ for distributions $\mcD$ using $m$ samples and $m'$ conditional subcube queries if, in addition to $m$ labeled samples, it also is able to request (unlabeled) subcube conditional samples from $D$, and its output has the same guarantee as above.
\end{definition}

Generally, we can think of $\delta$ as any fixed constant, as the success probability can always be boosted.
\begin{fact}[Boosting success probability]
\label{fact:boost}
    Given an algorithm $\mcA$ that $(\eps,\frac{1}{2})$-learns a concept $\mathscr{C}$ using $m$ samples, for any $\delta > 0$, we can construct an $\mcA'$ that $(1.1\eps,\delta)$-learns $\mathscr{C}$ using $m \cdot \poly(1/\eps, \log(1/\delta)) $ samples.
    % In \Cref{def:learn}, for any $\delta > 0$, the success probability can be boosted from $\frac{1}{2}$ to $1 - \delta$, while the desired accuracy is slightly decreased from $\eps$ to $1.1\eps$, with only an additional $\poly(1/\eps, \log(1/\delta))$ in the number of samples.
\end{fact}
\begin{proof}
    By repeating $\mcA$ $\log(1/\delta)$ times, we can guarantee that with probability at least $1 - \delta/2$, one of the returned hypotheses is $\eps$-close to $f^\star$. The accuracy of each of these hypothesis can be estimated to accuracy $\pm 0.05\eps$ using $O(\log(1/\delta)/\eps^2)$ random samples, and the most accurate one returned. With probability at least $1 - \delta$, that hypothesis will have at most $1.1\eps$ error.
\end{proof}


Our goal is to learn with respect to the class of all low-depth decision tree distributions. We use the following natural assumption on the concept class, which includes almost every concept class considered in the learning theory literature.

\begin{definition}[Closed under restriction]
    \label{def:closed-under-restriction}
    A concept class $\mathscr{C}$ of functions $f: \bits^n \to \zo$ is \emph{closed under restriction} if, for any $f \in \mathscr{C}$, $i \in [n]$, and $b \in \bits$, the restriction $f_{i = b}$ is also in $\mathscr{C}$.
\end{definition}

We will use \Cref{thm:decompose} to first decompose $\mcD$ into a mixture of nearly uniform distributions, and then run our learner on each of those distributions, as described in \Cref{fig:lift}.

\begin{figure}[h]

  \captionsetup{width=.9\linewidth}
\begin{tcolorbox}[colback = white,arc=1mm, boxrule=0.25mm]
\vspace{3pt} 

$\Lift(T, \mcA, S)$:  \vspace{6pt} \\
\textbf{Input:} A decision tree $T$, an algorithm for learning in the uniform distribution $\mcA$, and a random labeled sample $S$. \vspace{5pt} \\
\textbf{Output:} A hypothesis \vspace{4pt}

\ \ For each leaf $\ell \in T$ \{\vspace{-6pt}
\begin{enumerate}
    \item Let $S_\ell$ be the subset of points in $S$ that reach $\ell$.
    \item Create a set $S_{\ell}'$ consisting of points in $S_{\ell}$ but where all coordinates queried on the root-to-leaf path for $\ell$ are rerandomized independently (this makes the marginal over the input uniform).
    \item Use $\mcA$ to learn a hypothesis, $h_\ell$, with $S_{\ell}'$ as input.
\end{enumerate}
\vspace{-6pt}
\ \ \}\vspace{6pt}

\ \ Return the hypothesis that, when given an input $x$, first determines which leaf $\ell \in T$ that $x$ follows and then outputs $h_\ell(x)$.
% \begin{enumerate}
%     \item For each leaf $\ell \in T$, let $S_\ell$ be the subset of $S$ that reaches $\ell$. Use $\mcA$ to learn a hypothesis for $S_{\ell}$, denoted $h_{\ell}$.
%     \item Return the hypothesis that, when given an input $x$, first determines which leaf $\ell \in T$ that $x$ follows and then outputs $h_\ell(x)$.
% \end{enumerate}
\end{tcolorbox}
\caption{Pseudocode lifting a uniform distribution learner to one which succeeds on decision tree distributions. In this pseudocode, we assume that we have a decision tree representation which is close to the distribution, which can be accomplished using $\BuildDT$ in \Cref{fig:BuildDT}.}
\label{fig:lift}
\end{figure}

For our first result, we will assume that we already have a learner that succeeds on distributions that are sufficiently close to uniform.

\begin{definition}[Robust learners]
    For any concept class $\mathscr{C}$ of functions $f: \bits^n \to \zo$ and algorithm $\mcA$, we say that $\mcA$ $(\eps, \delta, c)$-\emph{robustly learns} $\mathscr{C}$ using $m$ samples under the uniform distribution if, for any $\eta > 0$ and the class of distributions
    \begin{equation*}
        \mathscr{D}_{\mathrm{TV, \eta}} \coloneqq \left\{\text{Distributions } \mathcal{D} \text{ over }\bits^n \text{ where } \TV(\mcU, \mathcal{D}) \leq \eta\right\},
    \end{equation*}
    $\mcA$ $(\eps + c\eta, \delta)$-learns $\mathscr{C}$ for the distributions in $\mathscr{D}_{\mathrm{TV, \eta}}$ using $m$ samples.
    % the following holds. For any distribution $\mcD$ satisfying $\TV(\mcU, \mcD) \leq \eta$ and any $f^\star \in \mathscr{C}$, given $m$ iid samples of the form $(\bx, f^\star(\bx))$ where $\bx \sim \mcD$, $\mcA$ returns some $h: \zo^n \to \zo$ that satisfies
    % \begin{equation*}
    %     \Pr_{\bx \sim \mcD}[h(\bx) \neq f^\star(\bx)] \leq \eps
    % \end{equation*}
    % with probability at least $1 - \delta$.
\end{definition}

The study of robust learners is part of a long and fruitful line of work. In particular, every learner that is robust to nasty noise \cite{BEK02} meets our definition of robust learners. 

Our result will also apply to learners that aren't explicitly robust. This is because \emph{every} learner is robust for $c = O(m)$.
\begin{proposition}
    \label{prop:auto-robust}
    For any concept class $\mathscr{C}$ and algorithm $\mcA$, is $\mcA$ $(\eps, \delta)$-learns $\mathscr{C}$ using $m$ samples under the uniform distribution, then $\mcA$ also $(\eps, \delta + \frac{1}{3}, 3m)$-robustly learns $\mathscr{C}$ using $m$ samples.
\end{proposition}
\begin{proof}
    Fix any $\eta > 0$.  Our goal is to show that $\mcA$ $(\eps + 3m \eta, \delta)$-learns $\mathscr{C}$ for distributions in $\mathscr{D}_{\TV, \eta}$. If $\eta \geq \frac{1}{3m}$, this is obviously true as any hypothesis has error $\leq 1$. We therefore need only consider $\eta < \frac{1}{3m}$. When $\mcA$ receives a sample from $\mcU^m$, it returns a hypothesis with error $\leq \eps$ with probability at least $1 - \eta$. Instead, $\mcA$ is receiving a sample from $\mcD^m$, where $\TV(\mcU, \mcD) < \frac{1}{3m}$. The success probability of any test given a sample from $\mcD^m$ rather than $\mcU^m$ can only differ by at most
    \[\TV(\mcU^m, \mcD^m) \leq m \cdot \TV(\mcU,\mcD) < \frac{1}{3}.\]
    Therefore, for $\eta < \frac{1}{3m}$, $\mcA$ succeeds wp at least $\delta + \frac{1}{3}$, as desired.
\end{proof}


We now state the main result of this section.

\begin{theorem}
    \label{thm:lift-given-DT}
    Choose any concept class $\mathscr{C}$ of functions $\bits^n \to \zo$ closed under restrictions, $\eps,\delta, c > 0$, $m,d \in \N$, and algorithm $\mcA$ that $(\eps, \delta/(2 \cdot 2^d), c)$-robustly learns $\mathscr{C}$ using $m$ samples under the uniform distribution. For any function $f^\star \in \mathscr{C}$, distribution $\mcD$ over $\bits^n$, depth-$d$ decision tree $T:\bits^n \to \R$ computing the PMF of a distribution $\mcD_T$ where
    \begin{equation*}
        \TV(\mcD, \mcD_T) \leq \frac{\eps}{c},
    \end{equation*}
     and sample size of
    \begin{equation*}
        M = m \cdot \poly\left(2^d, \frac{1}{\eps}, \log\left(\frac{1}{\delta}\right)\right).
    \end{equation*}
    Let $\bS$ a size-$M$ iid sample of labeled points $(\bx, f^\star(\bx))$ where $\bx \sim \mcD$. The output of $\Lift(T,\mcA, \bS)$ is $O(\eps)$-close to $f^\star$ w.r.t $\mcD$ with probability at least $1 - \delta$.
\end{theorem}
By \Cref{fact:boost}, an algorithm with constant failure probability could be transformed into one with the failure probability required by \Cref{thm:lift-given-DT} with only a $\poly(d, 1/\eps, \log(\delta))$ increase in the sample size. Therefore, would \Cref{thm:lift-given-DT} still holds when $\mcA$ $(\eps, \frac{1}{2}, c)$-robustly learns $\mathscr{C}$ if $\Lift$ applies \Cref{fact:boost} to boost the success probability of $\mcA$. Before proving \Cref{thm:lift-given-DT}, we show how it implies our main result.


\begin{theorem}[Lifting uniform-distribution learners, formal version of \Cref{thm:lift}]
\label{thm:lift-formal}
Choose any concept class $\mathscr{C}$ of functions $\bits^n \to \zo$ closed under restrictions, $\eps, c > 0$, $m,d \in \N$. If there is an efficient algorithm, $\mcA$, that $(\eps, \frac{1}{2})$-learns $\mathscr{C}$ using $m$ samples for the uniform distribution, then for $M = \poly(n) \cdot \left(\frac{dm}{\eps}\right)^{O(d)}$,
\begin{itemize}
    \item[$\circ$] There is an algorithm that $(\eps, \frac{1}{6})$-learns $\mathscr{C}$ using $M$ samples for monotone distributions representable by a depth-$d$ decision tree.
    \item[$\circ$] There is an algorithm that uses $M$ conditional subcube samples from $\mcD$ and $M$ random samples labeled by the target function that learns $\mathscr{C}$ to $\eps$-accuracy for arbitrary (not necessarily monotone) distributions representable by a depth-$d$ decision tree.
\end{itemize}
In both cases, the algorithm runs in time $\poly(n, M)$.
\end{theorem}
\begin{proof}[Proof of \Cref{thm:lift-formal} given \Cref{thm:lift-given-DT}]
    Using \Cref{thm:decompose}, we can learn the input distribution to TV-distance accuracy $\frac{\eps}{3m}$ with a decision tree hypothesis.  By \Cref{prop:auto-robust}, $\mcA$ $(\eps, \frac{1}{2}, 3m)$-robustly learns $\mathscr{C}$ for the uniform distribution, which can be boosted to failure probability $O(2^{-d})$ using \Cref{fact:boost}. Applying \Cref{thm:lift-given-DT} gives the desired result, with the desired runtime following from \Cref{prop:runtime-lift}.
\end{proof}

 The remainder of this section is devoted to the proof of \Cref{thm:lift-given-DT}. We'll use the following proposition.
% Before proving \Cref{thm:lift-given-DT}, a few remarks are in order. 

% \begin{enumerate}
%     \item We require that the original algorithm $\mcA$ have a failure low failure probability of $O(\delta 2^{-d})$. However, by \Cref{fact:boost}, an algorithm with constant failure probability can be transformed into one with such low failure probability with only a multiplicative $\poly(d, 1/\eps, \log(\delta))$ increase in the sample size.
%     \item Even when $\mcA$ is not explicitly robust, by applying \Cref{prop:auto-robust}, we are free to use $c = O(m)$. In this setting, we want to have $\TV(\mcD, \mcD_T) \leq O(\frac{\eps}{m})$.
% \end{enumerate}

% \begin{theorem}\gnote{will fill in details of this theorem after proof}
%     \label{thm:lift-robust-learners}
%     For any concept class $\mathscr{C}$, $\eps, \eta > 0$, and sample size $m \in \N$, suppose there is an algorithm $\mcA$ that $(m, \eps, \eta)$-robustly learns $\mathscr{C}$ under the uniform distribution. Then,
%     \begin{enumerate}
%         \item \emph{Monotone distributions}: For any $d \in \N$, there is an algorithm $\mcB$ which, for
%         \begin{equation*}
%             M = \poly \left(\left(\frac{d}{\eps \eta}\right)^d, m\right),
%         \end{equation*}
%         learns functions in $\mathscr{C}$ with respect to monotone decision tree distributions of depth at most $d$ using $M$ labeled examples.
%         \item \emph{Arbitrary distributions}:
%     \end{enumerate}
% \end{theorem}





\begin{proposition}
    \label{prop:each-leaf-many}
    For any distribution $\mcD$ over $\bits^n$, depth-$d$ decision tree $T$, $m \in \N$ and $p, \delta > 0$, as long as
    \begin{equation}
        \label{eq:M-each-leaf-many}
        M \geq O\left(\frac{d + m + \log(1/\delta)}{p}\right)
    \end{equation}
    for a random sample of $M$ points from $\mcD$, the probability there is a leaf $\ell \in T$ satisfying:
    \begin{enumerate}
        \item High weight: The probability a random sample from $\mcD$ reaches $\ell$ is at least $p$,
        \item Few samples: The number of points in the size-$M$ sample that reach $\ell$ is less than $m$
    \end{enumerate}
    is at most $\delta$.
\end{proposition}
\begin{proof}
    Fix a single leaf $\ell \in T$ with weight at least $p$. Then, the expected number of points that reach this leaf is $\mu \geq Mp$. As long as $\mu \geq 2m$, applying multiplicative Chernoff bounds,
    \begin{equation*}
        \Pr[\text{Fewer than $m$ points reach $\ell$}] \leq \exp\left(-\frac{\mu}{8} \right).
    \end{equation*}
    Union bounding over all leaves $2^d$, it is sufficient to choose an $M$ where
    \begin{equation*}
        2^d \exp\left(-\frac{\mu}{8} \right) \leq \delta.
    \end{equation*}
    This is satisfied for the $M$ from \Cref{eq:M-each-leaf-many}.
\end{proof}


We'll also use that if we have a decision tree $T$ that has learned the PMF to $\mcD$ to high accuracy, $\mcD$ restricted to the leaves of $T$ is, on average over the leaves, close to uniform.
\begin{fact}[Lemma B.4 of \cite{BLMT-boosting}]
    \label{prop:TV-distance-split}
    For any distribution $\mcD$ and decision tree $T$ computing the PMF for some distribution $\mcD_{T}$,
    \begin{equation*}
        \sum_{\text{leaves }\ell \in T} \Prx_{\bx \sim \mcD}[\bx\text{ reaches }\ell] \cdot \TV(\mcD_{\ell}, (\mcD_T)_\ell) \leq 2\cdot \TV(\mcD, \mcD_{T}).
    \end{equation*}
\end{fact}

% \begin{theorem}[]
%     Choose any concept class $\mathscr{C}$ of functions $\zo^n \to \zo$ closed under restrictions, $\eps,c > 0$ sample size $m \in \N$, and algorithm $\mcA$  that $(\eps, c)$-robustly learns $\mathscr{C}$ using $m$ samples under the uniform distribution. For any function $f^\star \in \mathscr{C}$, distribution $\mcD$ over $\zo^n$, decision tree $T:\zo^n \to \R$ computing the PMF of a distribution $\mcD_T$ where
%     \begin{equation*}
%         \TV(\mcD, \mcD_T) \leq \frac{\eps}{c},
%     \end{equation*}
%      and sample size of
%     \begin{equation*}
%         M = m \cdot \poly\left(2^d, \frac{1}{\eps}\right).
%     \end{equation*}
%     Let $\bS$ a size-$M$ iid sample of $(\bx, f^\star(\bx))$ where $\bx \sim \mcD$. The output of $\Lift(T,\mcA, \bS)$ is $O(\eps)$-close to $f^\star$ w.r.t $\mcD$ with high probability.
% \end{theorem}
\begin{proof}[Proof of \Cref{thm:lift-given-DT}]
    First, we split up the accuracy of $h = \Lift(T, \mcA, \bS)$ into the accuracy of the hypotheses $h_{\ell}$ learned at each leaf $\ell$: 
    \begin{equation*}
        \Prx_{\bx \sim \mcD}[h(\bx) \neq f^\star(\bx)] = \sum_{\ell \in T}\Prx_{\bx \sim \mcD}[\bx\text{ reaches }\ell] \cdot \Prx_{\bx \sim \mcD_\ell}[h_{\ell}(\bx) \neq f^\star(\bx)].
    \end{equation*}
    Each hypothesis $h_{\ell}$ is learned by running $\mcA$ on the sample $\bS_{\ell}'$. First, we argue that for all leaves $\ell$ with much of $\mcD$'s weight, will, whp, have $\geq m$ samples. Applying \Cref{prop:each-leaf-many} with probability at least $1 - \delta/2$, for all $\ell \in T$ with $\Prx_{\bx \sim \mcD}[\bx\text{ reaches }\ell] \geq  \frac{\eps}{2^d}$, the size of $\bS_{\ell}'$ is at least $m$. Conditioning on that being true, we have that,
    \begin{align*}
        \Prx_{\bx \sim \mcD}[h(\bx) \neq f^\star(\bx)] &\leq  \sum_{\ell \in T}\Prx_{\bx \sim \mcD}[\bx\text{ reaches }\ell] \cdot \Prx_{\bx \sim \mcD_\ell}[h_{\ell}(\bx) \neq f^\star(\bx) \mid |\bS_{\ell}'| \geq m] \\
        &\quad+  \sum_{\ell \in T}\Prx_{\bx \sim \mcD}[\bx\text{ reaches }\ell] \cdot \Ind[\Prx_{\bx \sim \mcD}[\bx\text{ reaches }\ell] \leq p] \\
        & \leq \sum_{\ell \in T}\Prx_{\bx \sim \mcD}[\bx\text{ reaches }\ell] \cdot \Prx_{\bx \sim \mcD_\ell}[h_{\ell}(\bx) \neq f^\star(\bx) \mid |\bS_{\ell}'| \geq m] + \eps,
    \end{align*}
    where the last line uses that $T$ has at most $2^d$ leaves and $p = \frac{\eps}{2^d}$.
    
    
    Each point in $\bS_{\ell}'$ is an iid sample with the input uniform over $\bits^n$ and labeled by the function $f^\star_{\ell}$. As $\mathscr{C}$ is closed under restrictions, $f^\star_{\ell} \in \mathscr{C}$. Therefore,
    \begin{equation*}
        \Prx_{\bS}\left[\left[\Prx_{\bx \sim \mcD_\ell}[h_{\ell}(\bx) \neq f^\star(\bx)\mid |\bS_{\ell}'| \geq m ]\right] \leq \eps +  c \cdot \TV(\mcD_{\ell}, \mcU)\right] \geq 1 - \frac{\delta}{2 \cdot 2^d}.
    \end{equation*}
    We union bound over all $2^d$ leaves $\ell$ and the earlier event that $| \bS_{\ell}| \geq m$ whenever $\Prx_{\bx \sim \mcD}[\bx\text{ reaches }\ell] \geq p$, we have with probability at least $1 - \delta$,
    \begin{align*}
        \Prx_{\bx \sim \mcD}[h(\bx) \neq f^\star(\bx)] & \leq \sum_{\ell \in T}\Prx_{\bx \sim \mcD}[\bx\text{ reaches }\ell]\cdot \left(2\eps + c \cdot \TV(\mcD_{\ell}, \mcU)\right)\\
        &= 2\eps + c\cdot \sum_{\ell \in T}\Prx_{\bx \sim \mcD}[\bx\text{ reaches }\ell]\cdot \TV(\mcD_{\ell}, \mcU)\\
        &= 2\eps + c\cdot \sum_{\ell \in T}\Prx_{\bx \sim \mcD}[\bx\text{ reaches }\ell]\cdot \TV(\mcD_{\ell}, (\mcD_T)_\ell)\\
        &\leq  2\eps + c\cdot 2 \cdot \TV(\mcD, \mcD_T) \tag{\Cref{prop:TV-distance-split}} \\
        & \leq 4\eps. \tag{$\TV(\mcD, \mcD_T) \leq \frac{\eps}{c}$}
    \end{align*}
    As desired, we have $\Lift$ learns an $O(\eps)$-accurate hypothesis w.h.p.
\end{proof}



\begin{proposition}[Runtime of \Lift]
    \label{prop:runtime-lift}
    Assuming unit-time calls to $\mathcal{A}$, given a depth-$d$ decision tree $T$, $\Lift(T, \mathcal{A}, S)$ runs in time $O(n2^d \cdot |S|)$.
\end{proposition}
\begin{proof}
    The number of leaves in $T$ is at most $2^d$. Since each point in $S$ is in $\bits^n$, the entire sample takes $O(n \cdot |S|)$ bits to represent. For each leaf $\ell$, it takes $O(n \cdot |S|)$ time to loop through this representation, find the points consistent with $\ell$, and create the modified dataset $S_\ell'$, and pass it into $\mcA$. Repeating this over all leaves can be done in $O(n2^d \cdot |S|)$ time.
\end{proof}


\begin{remark}[The agnostic setting]
\label{remark:agnostic}
    A popular variant of learning, as defined in \Cref{def:learn}, is the \emph{agnostic setting}. In this generalization of standard learning, rather than assume $f^\star \in \mathscr{C}$, $\mcA$ is required to output a hypothesis with error $\opt + \eps$, where $\opt$ is the minimum error of a hypothesis in $\mathscr{C}$ w.r.t $f^\star$. It's straightforward to see that \Cref{thm:lift-given-DT}, and therefore \cref{thm:lift-formal}, extend to the agnostic setting, where if the uniform distribution learning $\mcA$ succeeds in the agnostic setting, that learner is upgraded to one that succeeds for decision tree distributions in the agnostic setting. This is because, if there is an $f \in \mathscr{C}$ with error $\opt$ w.r.t. $f^\star$, then the average error of $f$ over the leaves of a tree $T$ w.r.t $f^\star$ will also be at most $\opt$.
\end{remark}