\section{Algorithms for computing distributional influences} 



% \gray{In this section, we will give estimators for computing $\Inf_i(f_\mcD)$ given uniform samples or subcube conditional samples, for non-monotone and general distributions respectively. In \Cref{sec:decomp}, we will actually require an estimator, for any restriction $\pi$, of $\Inf_i((f_\mcD)_{\ell})$. This is nearly equivalent, as by defining $\mcD' = \mcD_{\ell}$, we can use the below estimators to compute $\Inf_i(f_{\mcD_\ell})$.}

In \Cref{sec:decomp}, we assumed the ability to exactly compute influences of $f$ and its restrictions in unit time. In this section, we show how to instead estimate the influences from samples, when the distribution is monotone or when we have access to subcube conditional samples. Just as in \cite{BLQT21focs}, our proof only requires estimates to be accurate to $\pm \min(\tau/4, \eps/n)$. Letting $\InfEst_i(f)$ denote such an estimate of $\Inf_i(f)$, the pseudocode in \Cref{fig:BuildDT} and proof of \Cref{thm:decompose} is modified as follows.
\begin{enumerate}
    \item We modify $S$ to include variables $i$ such that $\InfEst_i((f_{\mcD})_\pi) \geq 3\tau/4$. Since the estimate is accurate to $\pm \tau/4$, this is guaranteed to include all variables with influence $\geq \tau$, and furthermore will only include variables with influence at least $\tau/2$. Therefore, the total size of $S$ is at most $\frac{d}{\tau/2}$, which is only a constant factor (of $2$) larger than in the proof of \Cref{thm:decompose} which assumed perfect influence oracles, and there does not affect asymptotic runtime or sample complexity.
    \item It returns the tree $T_i$ that minimizes $\Ex_{\bell \in T_i}[\sum_{i \in [n]}\InfEst_i((f_\mcD)_{\bell})]$. Since each estimate is accurate to $\eps/n$, this estimate of total influence of $(f_\mcD)_{\bell})$ will be accurate to $\pm \eps$. Then, in \Cref{claim:low error}, rather than $\BuildDT$ building a tree $T$ minimizes $\E_{\bell \in T}[\Inf((f_\mcD)_{\bell})]$ among all depth-$d$, everywhere $\tau$-influential trees, $T$ (roughly) minimizes $\E_{\bell \in T}[\sum_{i \in [n]}\InfEst_i((f_\mcD)_{\bell})]$ among all depth-$d$, everywhere $\tau$-influential trees. More formally, $T$ will either be the best depth-$d$, everywhere $\tau$-influential trees, or better, as we know its searches over all variables with $\InfEst_i((f_{\mcD})_\pi) \geq 3\tau/4$ which is guaranteed to include variables with influence $\geq \tau$, but can include more variables. Finally, since the estimates of total influence are accurate to $\pm \eps$, using $\InfEst$ rather than $\Inf$ can only incur at most $2\eps$ additive error, which is a constant factor in the analysis.
\end{enumerate}

Given any distribution $\mcD$ over $\bits^n$, we need to estimate $\Inf_i((f_\mcD)_{\ell})$ for any restriction $\ell$ of $f_\mcD$ and $i \in [n] - \ell$. In this section, we will instead show how to compute $\Inf_i(f_{\ArbDist})$ for any distribution $\ArbDist$ over $\bits^m$.  We can then use our estimators with $\ArbDist = \mcD_\ell$  to obtain the necessary answers using the following fact:

\begin{fact}
    \label{fact:inf-estimates-scaling}
    For any distribution $\mcD$ over $\bits^n$,  restriction $\ell$ of $\bits^n$, and $i \in [n] - \ell$:
    \begin{equation*}
        \Inf_i((f_\mcD)_{\ell}) = 2^{|\ell|} \cdot \Prx_{x \sim \mcD}[x \in \ell] \cdot \Inf_i(f_{\mcD_\ell}).
    \end{equation*}
\end{fact}
\begin{proof}
    Let $w_\ell = \Prx_{\bx \sim \mcD}[\bx \in \ell]$. We have: 
    \begin{align*}
        \Inf_i((f_\mcD)_{\ell}) 
            &= \Ex_{\bx \sim \mcU^n} \left[\left| f_\mcD(\bx) - f_\mcD(\bx^{\sim i}) \right | \mid x \in \ell \right] \\
            &= 2^n \cdot \Ex_{\bx \sim \mcU^n} \left[\left| \mcD(\bx) - \mcD(\bx^{\sim i}) \right | \mid x \in \ell \right] \\
            &= 2^n \cdot w_\ell \cdot \Ex_{\bx \sim \mcU^n} \left[\left| \frac{\mcD(\bx)}{w_\ell} - \frac{\mcD(\bx^{\sim i})}{{w_\ell}} \right | \mid x \in \ell \right]  \\
            &= 2^n \cdot w_\ell \cdot \Ex_{\by \sim \mcU^{n - |\ell|}} \left| \mcD_\ell(\by) - \mcD_\ell(\by^{\sim i}) \right |   \\
            &= 2^{|\ell|} \cdot w_\ell \cdot \Ex_{\by \sim \mcU^{n - |\ell|}} \left| f_{\mcD_\ell}(\by) - f_{\mcD_\ell}(\by^{\sim i}) \right |   \\
            % &= \frac{2^n}{2^{n - |\ell|}} \cdot \mcD(x \in \ell) \Ex_{\bx \sim \mcU^n} \left[\left| f_{\mcD_\ell}(\bx) - f_{\mcD_\ell}(\bx^{\sim i}) \right | \mid x \in \ell \right] \\
            % &= 2^{|\ell|} \cdot \mcD(x \in \ell) \Ex_{\bx \sim \mcU^{n-|\ell|}} \left[\left| f_{\mcD_\ell}(\bx_{\cup \ell}) - f_{\mcD_\ell}(\bx_{\cup \ell}^{\sim i}) \right | \right] \\
            % &= \Ex_{\bx \sim \mcU^{n - |\ell|}} \left[\left| f_\mcD(\bx) - f_\mcD(\bx^{\sim i}) \right |\right] \\
            % &= 2^n \cdot \Ex_{\bx \sim \mcU^{n - |\ell|}} \left[\left| \mcD(\bx) - \mcD(\bx^{\sim i}) \right |\right] \\
            % &= 2^n \cdot \mcD(x \in \ell) \cdot \Ex_{\bx \sim \mcU^{n - |\ell|}} \left[\left| \mcD_\ell(\bx) - \mcD_\ell(\bx^{\sim i}) \right |\right] \\
            % &= \frac{2^n}{2^{n - |\ell|}} \cdot \mcD(x \in \ell) \cdot \Ex_{\bx \sim \mcU^{n - |\ell|}} \left[\left| f_{\mcD_\ell}(\bx) - f_{\mcD_\ell}(\bx^{\sim i}) \right |\right] \\
            &= 2^{|\ell|} \cdot w_\ell \cdot \Inf_i ( f_{\mcD_\ell}). \qedhere
    \end{align*}
    % \begin{align*}
    %     \Inf_i((f_\mcD)_{\ell}) 
    %         &= \Ex_{\bx \sim \mcU^n} \left[\left| f_\mcD(\bx) - f_\mcD(\bx^{\sim i}) \right | \mid x \in \ell \right] \\
    %         &= \Ex_{\bx \sim \mcU^{n - |\ell|}} \left[\left| f_\mcD(\bx) - f_\mcD(\bx^{\sim i}) \right |\right] \\
    %         &= 2^n \cdot \Ex_{\bx \sim \mcU^{n - |\ell|}} \left[\left| \mcD(\bx) - \mcD(\bx^{\sim i}) \right |\right] \\
    %         &= 2^n \cdot \mcD(x \in \ell) \cdot \Ex_{\bx \sim \mcU^{n - |\ell|}} \left[\left| \mcD_\ell(\bx) - \mcD_\ell(\bx^{\sim i}) \right |\right] \\
    %         &= \frac{2^n}{2^{n - |\ell|}} \cdot \mcD(x \in \ell) \cdot \Ex_{\bx \sim \mcU^{n - |\ell|}} \left[\left| f_{\mcD_\ell}(\bx) - f_{\mcD_\ell}(\bx^{\sim i}) \right |\right] \\
    %         &= 2^{|\ell|} \cdot \mcD(x \in \ell) \cdot \Inf_i ( f_{\mcD_\ell})
    % \end{align*}
\end{proof}

    Because of \Cref{fact:inf-estimates-scaling}, for any restriction $\ell$ of depth at most $d$, to estimate $ Inf_i((f_\mcD)_{\ell})$ to accuracy $\pm \eps$, it is sufficient to estimate $\Inf_i(f_{\mcD_\ell})$ to accurate $\pm \eps/2^d$. This $2^d$ factor is dominated by the $d^{O(d)}$ term in \Cref{thm:learn-DT}, so we are free to do the later.


\subsection{Monotone distributions using samples} 

Let $\ArbDist$ be an arbitrary distribution over $\bits^m$.  If $\ArbDist$ is monotone, the influences of $f_\ArbDist$ can be efficiently computed directly from samples of $\ArbDist$, via an estimate of bias:

% In the monotone setting, the influences of $f_\mcD$ can be efficiently computed directly from samples of $\mcD$, via an estimate of bias:

% \begin{definition}[bias of a distribution]
%     For any $i \in [m]$,
%     \begin{equation*}
%         \bias_i(\ArbDist) \coloneqq \Ex_{\bx \sim \ArbDist}[(\bx_i - 1/2)].
%     \end{equation*}
% \end{definition}


\begin{lemma}[Estimating influence using bias]\label{lem:bias_inf}

If $\ArbDist$ is monotone,

\begin{equation*}
        \Inf_i(f_\ArbDist) = \Ex_{\bx \sim \ArbDist}[\bx_i].
    \end{equation*}
\end{lemma}
\begin{proof}
    Using \Cref{fact:inf_eq_corr}, 
    \begin{align*}
        \Inf_i(f_\ArbDist) 
        &=   \Ex_{\bx \sim \mcU^m}[f_\ArbDist(\bx) \cdot \bx_i]  \\ 
        &= \sum_{x \in \bits^m} 2^{-m} \ f_\ArbDist(x) \cdot x_i   \\
        &= \sum_{x \in \bits^m} \ArbDist(x) \cdot  x_i \\
        &=  \Ex_{\bx \sim \ArbDist}[ \bx_i ] . \qedhere 
    \end{align*} 
\end{proof}


As a simple application of the above and Hoeffding's inequality, we obtain the following corollary.
\begin{corollary}
[Estimating influences of monotone distributions]
\label{cor:high-accuracy-monotone}
    For any $\eps, \delta > 0$, there is an efficient algorithm that given unknown monotone distribution $\ArbDist$, computes an estimate of $\Inf_i(f_{\ArbDist})$ to accuracy $\pm \eps$ with probability at least $1 - \delta$ using $O(\log(1/\delta)/ \eps^2)$ random samples from $\ArbDist$.
\end{corollary}
Recall that for \Cref{thm:decompose}, we only need the influence estimates to be accurate to $\pm \poly(2^{-d}, \tau, \eps,1/n)$. Setting $\tau = O(\eps/d^2)$ and union bounding over $n \cdot (d/\eps)^{O(d)}$ calls to the influence oracle gives a sample complexity of $\poly(n, 2^d, \eps, \log(1/\delta))$. The running time is still dominated by the number of recursive calls.




% To estimate the bias to accuracy $\pm \tau$ with failure probability $\frac{\delta}{n \cdot (d/\eps)^d}$, it is sufficient to take the empirical bias from a sample of size $\frac{1}{\tau^2} \cdot (\log(1/\delta) + d \log (d/\eps))$, by the standard Hoeffding bound. Setting $\tau = O(\frac{\eps}{d^2})$ and union bounding over the $n \cdot (d/\eps)^{O(d)}$ calls to the influence oracle gives a sample complexity of $\poly(d, 1/\eps, \log(1/\delta))$. However, $2^{\Theta(d)}$ samples are still required to estimate the leaf densities to the required accuracy of $2^{-d}$, so the final sample complexity is $\poly(2^d, 1/\eps, \log(1/\delta))$. The running time is still dominated by the number of recursive calls.

\begin{corollary}[Sample complexity of $\BuildDT$ for monotone distributions]
The algorithm $\BuildDT(\mcD, \varnothing, d, \lfrac{\eps}{2d^2})$, given $\poly(n,2^d, 1/\eps, \log(1/\delta))$ random examples from a monotone distribution $\mcD$, runs in $\poly(n) \cdot (d/\eps)^{O(d)} \cdot \log(1/\delta)$ time and outputs a distribution within TV distance $\eps$ of $\mcD$. The algorithm fails with probability at most $\delta$.
\end{corollary}


\subsection{Beyond monotone distributions using subcube conditional sampling}
We also design an influence estimator for arbitrary distributions $\ArbDist$ over $\bits^m$ using subcube conditional sampling.

\begin{figure}[H]
  \captionsetup{width=.9\linewidth}
\begin{tcolorbox}[colback = white,arc=1mm, boxrule=0.25mm]
\vspace{3pt} 

$\InfEst(\ArbDist, i, \eps)$:  \vspace{6pt} \\
\textbf{Input:} A distribution $\ArbDist$ over $\bits^m$, coordinate $i \in [m]$, and bias parameter $\eps$. \\
\textbf{Output:} An estimate of $\Inf_i(f_{\ArbDist})$ that has bias at most $\eps$.% \vspace{4pt}

\begin{enumerate}
    \item Sample a random $\bx \sim \ArbDist$ and define the subcube
    \begin{equation*}
        S \coloneqq \{\bx\} \cup \{\bx^{\oplus i}\}
    \end{equation*}
    where $x^{\oplus i}$ is $x$ with the $i^{\text{th}}$ bit flipped.
    \item \label{step:est-p}Take $\left\lceil1/\eps^2\right\rceil$ independent samples from $\ArbDist$ conditioned on the output being in $S$, and let $p$ be the fraction of those samples that equal $\bx$.
    \item Output $ |p - (1 - p)|$.
\end{enumerate}
\end{tcolorbox}
\caption{Pseudocode for estimating the influence of a variable on a distribution's weighting function.}
\label{fig:inf-est}
\end{figure}

\begin{proposition}[{\sc InfEst} has low bias]
\label{prop:low-bias}
    For any distribution $\ArbDist$ over $\bits^m$, coordinate $i \in [m]$, and $\eps > 0$,
    \begin{equation*}
        \left|\Ex\left[\InfEst(\ArbDist, i, \eps)\right] - \Inf_i(f_{\ArbDist})\right| \leq \eps
    \end{equation*}
    where $\InfEst$ is as defined in \Cref{fig:inf-est}.
\end{proposition}

Before proving \Cref{prop:low-bias}, we note that it implies a high accuracy estimator.

\begin{corollary}[Estimating influences]
\label{cor:high-accuracy}
    For any $\eps, \delta > 0$, there is an efficient algorithm that given unknown distribution $\ArbDist$, computes an estimate of $\Inf_i(f_{\ArbDist})$ to accuracy $\pm \eps$ with probability at least $1 - \delta$ using $O(\log(1/\delta)/ \eps^4)$ subcube conditional samples from $\ArbDist$.
\end{corollary}
\begin{proof}
    The algorithm outputs the mean of $O(\log(1/\delta)/\eps^2)$ independent calls to $\InfEst(\ArbDist, i , \eps/2)$. Each call to $\InfEst(\ArbDist, i , \eps/2)$ gives an output bounded within $[0,1]$. By Hoeffing's inequality, if $\textbf{est}$ is the mean of $O(\log(1/\delta)/\eps^2)$ independent calls to $\InfEst(\ArbDist, i , \eps/2)$,
    \begin{equation*}
        \Pr\left[\left|\textbf{est} -  \Ex\left[\InfEst(\ArbDist, i, \eps/2)\right] \right| \geq \eps/2\right] \leq \delta.
    \end{equation*}
    The result then follows from triangle inequality and \Cref{prop:low-bias}.
\end{proof}

We prove that $\InfEst$ has low bias.
\begin{proof}[Proof of \Cref{prop:low-bias}]
    For each $x \in \bits^n$, let
    \begin{equation*}
        p(x) \coloneqq \frac{\ArbDist(x)}{\ArbDist(x) + \ArbDist(x^{\oplus i})}
    \end{equation*}
    be the relative weight of $x$ in the subcube containing $x$ and $x^{\oplus i}$. Then, we can rewrite the influence as: 
    \begin{align*}
        \Inf_{i}(f_{\ArbDist}) &= \sum_{x \in \bits^m} \frac{1}{2^m}\left| f_{\ArbDist}(x) - f_{\ArbDist}(x^{\sim i}) \right |\\ 
         &= \sum_{x \in \bits^m} \left| \ArbDist(x) - \ArbDist(x^{\sim i}) \right | \tag{$f_{\ArbDist}(x) = 2^m \ArbDist(x)$}\\
        &= \frac{1}{2} \cdot \sum_{x \in \bits^m}\left| \ArbDist(x) - \ArbDist(x^{\oplus i}) \right| \tag{$x^{\sim i} = x^{\oplus i}$ wp $\frac{1}{2}$, and otherwise $x^{\sim i} = x$} \\
        &= \frac{1}{2} \cdot \sum_{x \in \bits^m}\left(\ArbDist(x) + \ArbDist(x^{\oplus i})\right) \cdot\left| p(x) - p(x^{\oplus i}) \right|. \tag{definition of $p(x)$} 
    \end{align*}
    Then, using the fact that $p(x) = 1 - p(x^{\oplus i})$, and distributing the $(\ArbDist(x) + \ArbDist(x^{\oplus i}))$ term, we can write
    \begin{align*}
        \Inf_{i}(f_{\ArbDist}) &= \frac{1}{2} \sum_{x \in \bits^m}\ArbDist(x) \cdot \left|p(x) - (1 - p(x)) \right| + \ArbDist(x^{\oplus i}) \cdot \left|p(x^{\oplus i }) - (1 - p(x^{\oplus i})) \right| \\
        &= \sum_{x \in \bits^m}\ArbDist(x) \cdot \left|p(x) - (1 - p(x)) \right|
    \end{align*}
    where, in the last step, we used the fact that summing over $x \in \bits^m$ is equivalent to summing over $x^{\oplus i} \in \bits^m$. Let $\hat{p}(x)$ be the random variable for the estimate of $p(x)$ computed by $\InfEst$ step \ref{step:est-p}. We bound the bias of \InfEst.
    \begin{align*}
         \Big|\Ex\left[\InfEst(\ArbDist, i, \eps)\right] &- \Inf_i(f_{\ArbDist})\Big| \\
         &= \left|\Ex_{\bx \sim \ArbDist}\left[\left|2\hat{p}(\bx) - 1\right|\right] - \Ex_{\bx \sim \ArbDist}\left[\left|2p(\bx) - 1\right|\right] \right| \tag{$2p-1 = p - (1-p)$} \\
         &= \left|\Ex_{\bx \sim \ArbDist}\left[\left|2\hat{p}(\bx) - 1\right| - \left|2p(\bx) - 1\right|\right] \right| \tag{linearity of expectation} \\
         &\leq \Ex_{\bx \sim \ArbDist}\left[\big|\left|2\hat{p}(\bx) - 1\right| - \left|2p(\bx) - 1\right|\big|\right]  \tag{Jensen's inequality} \\
         &\leq 2\left|\Ex_{\bx \sim \ArbDist}\left[\left|\hat{p}(\bx) - p(\bx)\right|\right]\right|. \tag{$\big||a| - |b|\big| \leq |a - b|$}
    \end{align*}
    For any $x \in \bits^m$, $\hat{p}(x)$ is the average of $\left\lceil1/\eps^2\right\rceil$ random variables each which is $1$ with probability $p(x)$ and $0$ otherwise. As a result, $\Ex[\hat{p}(x)] = p(x)$, and $\Var[\hat{p}(x)] \leq \eps^2/4$. Applying Jensen's inequality, we conclude
    \begin{equation*}
        \left|\Ex\left[\InfEst(\ArbDist, i, \eps)\right] - \Inf_i(f_{\ArbDist})\right| \leq 2\sqrt{\Var[\hat{p}(x)]} \leq \eps. \qedhere 
    \end{equation*}
\end{proof}