

\section{Overview}


\subsection{Related Work}

\begin{enumerate}
    \item Ronitt Arsen Learning monotone distributions \footnote{https://arxiv.org/abs/2002.03415}
    \item $k$-junta distributions
    \item Learning mixtures of product\footnote{\url{http://www.cs.columbia.edu/~rocco/Public/feldman_odonnell_servedio_05_1.pdf}}
    \item Mixture of subcubes: \footnote{\url{https://arxiv.org/pdf/1803.06521.pdf}}
\end{enumerate}

\subsection{Stuff}


We should be able to handles $[m]^n$ rather than $\bits^n$ using an encoding reduction.

\begin{definition}[Complexity measure of a distribution]


Define DT complexity of distribution $\mcD$ as the smallest $d \in \N$ s.t. $\mcD$ can be expressed as a depth $d$ DT, where the elements at the leaves are distributed  uniformly.

\end{definition}

\begin{definition}[Disjoint subcube complexity measure of a distribution]


Define disjoint subcube complexity of distribution $\mcD$ as the smallest $k \in \N$ s.t. $\mcD$ can be expressed as the mixture of disjoint distributions that are each uniform on codimension $\leq$ k subcubes.
\end{definition}


\begin{theorem}[Hoped-for]
Let $D$ be a monotone distribution with DT-complexity $d$. Then there is an algorithm with sample complexity $\poly(n) \cdot 2^{d^2}$ that produces a hypothesis $H$ of depth $O(d^2)$, such that $\TV(D, H) \le \eps$.\footnote{And might try time $\poly(n) \cdot d^d$ and depth $d$ using pruning.}
\end{theorem}

\begin{theorem}[Stronger hope]
The above also holds for non-monotone distributions with subcube-conditional samples.
\end{theorem}


\paragraph{Applications.}

\begin{itemize}
    \item Distribution Learning / density estimation
        \begin{enumerate}
            \item Ronitt-Arsen General monotone distribution, sample complexity: $2^{n - n^{1/5}}$, time complexity: $2^{n + n^{1/5}}$
            \item Learning $k$-junta distributions in time $n^k$. This is a subclass of depth-$k$ DTs.
            \item Mixtures of product distributions/mixtures of subcubes. This is a superclass of DTs ([feldman, odonnel, sevideio]\footenote{https://www.cs.cmu.edu/~odonnell/papers/learning-mixtures.pdf} and [chen-moitra] \footnote{https://arxiv.org/abs/1803.06521})
        \end{enumerate}
    
    \item Function learning/PAC. First learn decomposition of $\mcD$ into a disjoint mixture of uniform pieces, then run uniform distribution learner on each piece. 
    
    Consequently: all uniform dist. learners are upgraded to work for simple enough non-uniform DTs (small depth DT)
\end{itemize}

\section{Notes}

\begin{definition}[Decision tree distribution]
    We say that a distribution $\mathcal{D}$ is a depth-$d$ DT if its PMF $\mcD: \pm 1 \to [0,1]$ is computable by a depth-$d$ DT. 
\end{definition}

\begin{definition}[Monotone distribution]
We furthermore say that a distribution $\mcD$ is monotone if the PMF is monotone.
\end{definition}

\begin{definition}[bias of $i$]
    For any $i \in [n]$,
    \begin{equation*}
        \bias_i(\mcD) \coloneqq \Ex_{\bx \sim \mcD}[x_i]
    \end{equation*}
\end{definition}
\begin{fact}
    \begin{equation*}
        \bias_i(\mcD) = 2^n \Ex_{\bx \sim \mcU}[\mcD(x) x_i]
    \end{equation*}
    where $\mcU$ is the uniform distribution
\end{fact}

\begin{definition}[Total bias]

    \begin{equation*}
        \bias({\mcD}) \coloneqq \sum_{i = 1}^n \bias_i(\mcD)
    \end{equation*}
\end{definition}


\subsection{Monotone stuff}
\begin{lemma}[OSSS for monotone depth-$d$]
    For any monotone depth-$d$ distribution $\mcD$, there exists an $i \in [n]$ s.t.
\begin{equation}
    \bias_i(\mcD) \geq \frac{\TV(\mcD, \mcU)}{d}
\end{equation}
where $\mcU$ is the uniform distribution.
\end{lemma}


\begin{definition}[Influence]

\begin{equation}
\Inf^{(1)}_{i}(\mcD) \coloneqq \Ex_{\bx \sim \mcU} [\left| \mcD(\bx) - \mcD(\bx^{\sim i}) \right |]
\end{equation}    
\end{definition}

\begin{definition}[Variance]
    The L1 variance of a distribution is
    \begin{equation*}
        \Var^{(1)}(\mcD) \coloneqq \Ex_{x, x' \sim \mcU}[|p(x) - p(x')|]
    \end{equation*}
\end{definition}

\begin{fact}[Bias and influence for monotone dists]
    If $\mcD$ is monotone, $\Inf^{(1)}_i(\mcD) = {2^{-n}} \ \bias_i(\mcD)$
\end{fact}
\begin{proof}
    TODO
\begin{align*}
    \Inf^{(1)}_i(\mcD) &= ...
\end{align*}
\end{proof}

\begin{fact}[Variance related to TV distance to uniform]
    \begin{equation*}
        \Var^{(1)}(\mcD) \geq 2^{-n} 
        \TV(\mcD, \mcU)
    \end{equation*}
\end{fact}

\begin{lemma}[Rubinfeld-Servidio]
    For monotone $\mcD$,
    \begin{equation*}
        \bias(\mcD) \geq \TV(\mcD, \mcU)
    \end{equation*}
\end{lemma}


\begin{fact}
    If $\mcD$ is depth-$d$, then
    \begin{align*}
        \bias(\mcD) \leq d.
    \end{align*}
\end{fact}




\begin{fact}[Relationship between bias at leaves and bias at root]
    For any distribution $\mcD$, and $i,j \in [d]$.
    \begin{equation*}
        \bias_j(\mcD) = \Ex_{\bx \sim \mcD}\left[\bias_j( \mcD| \bx_i = x_i ) \right]
    \end{equation*}
\end{fact}

\begin{corollary}
    For any distribution $\mcD$ and $i \in [d]$,
    \begin{equation*}
        \Ex_{\bx_i \sim \mcD_i}[\bias(\mcD_{x_i = \bx_i})] = \bias(\mcD) - \bias_i(\mcD).
    \end{equation*}
\end{corollary}