






\section{Preliminaries}


\paragraph{Notation.} Given an input $x \in \bits^n$, coordinate $i \in [n]$, and setting $b \in \bits$, we use $x_{i = b}$ to refer to the input $x$ with the $i^{\text{th}}$ coordinate overwritten to take the value $b$. 
Similarly, given a sequence of (coordinate, value) pairs 
% $\pi \in ([n] \times \bits)^k$, 
$\pi = \{(i_1, b_1), \ldots, (i_k, b_k) \}$, 
we use $x_{\pi}$ to represent $x$ with the coordinates in $\pi$ overwritten/inserted with their respective values. 

Given a function $f: \bits^n \to \R$,  we denote the \emph{restriction} $f_{i = b}: \bits^n \to \R$ to be the function that maps $x$ to $f(x_{i = b})$. We define the restriction $f_\pi$ analogously. 

% \violet{Similarly, given a sequence of coordinates $\pi \in [n]^k$ and corresponding values $v \in \bits^k$, we use $x_{\pi = v}$ to represent $x$ with the coordinates in $\pi$ overwritten/inserted using the values in $v$. }


\begin{definition}[Decision trees (DT)]
 A decision trees $T : \bits^n \to \R$, is a binary tree whose internal nodes query a particular coordinate, and whose leaves are labelled by values. Each instance $x \in \bits^n$ follows a unique root-to-leaf path in $T$: at any internal node, it follows either the left or right branch depending on the value of the queried coordinate, until a leaf is reached and its value is returned. 
 
\end{definition}

 The set of leaves $\ell \in \mathrm{leaves}(T)$ therefore form a partition of $\bits^n$,  with each leaf having $2^{n - |\ell|}$ elements, where $|\ell|$ is the depth of the leaf.  Every leaf $\ell$ also corresponds to a sequence of  (coordinates, value) pairs  $\pi(\ell)$ that lead to the leaf. For a function $f$, will sometimes use the shorthand $f_\ell$ to mean the restriction $f_{\pi(\ell)}$.


\begin{definition}[Decision tree distribution]
    We say that a distribution $\mathcal{D} : \bits^n \to [0,1]$ is representable by a depth-$d$ DT, if its pmf is computable by a depth-$d$ decision tree $T$. 
    % Specifically, each leaf $\ell$ has a probability $w_\ell$ and the conditional distribution of points that reach a leaf is uniform. So, for any $x$ that reaches leaf $\ell$, we have $\mcD(x) = w_\ell/|\{y : y \in \ell\}|$.
    Specifically, each leaf $\ell$ is labelled by a value $p_\ell$, so that $\mcD(x) = p_\ell$ for all $x \in \ell$. This means that the conditional distribution of all points that reach a leaf is uniform. Moreover, since $\mcD$ is a distribution, we have: $\sum_{\ell \in \mathrm{leaves}(T)} 2^{n - |\ell|} \cdot p_\ell  = 1$.
   
\end{definition}
   
   For a given leaf $\ell$, we will write  $\mcD_\ell : \bits^{n - |\ell|} \to [0,1]$ to represent the conditional distribution of $\mcD$ at the leaf $\ell$, so that for any $x \in \bits^{n - |\ell|}$, we have $\mcD_\ell(x) = \mcD(x_{\pi(\ell)}) /\Pr_{y \sim \mcD}[y \in \ell]$.
    % {
    % \color{gray}
    % We will often scale $\mcD$ up by $2^n$ since it makes our analysis easier. As such, we also define the weighting function $f_\mcD(x) \coloneqq 2^n \mcD(x)$. 
    % }
    
    

    We will often scale up the pmfs of our distributions by the domain size, since it makes our analysis easier. As such, we also define the weighting function:
    

\begin{definition}[Weighting function of distribution]
Let $\ArbDist$ be an arbitrary distribution over $\bits^m$. We define the weighting function: 
\[
f_\ArbDist(x) \coloneqq 2^m \cdot \ArbDist(x).
\]

\end{definition}




\begin{definition}[Monotone distribution]
We furthermore say that a distribution $\mcD$ is monotone if its pmf is monotone: for $x, y \in \bits^n$, if $x_i \leq y_i$ for all $i \in [n]$, then $\mcD(x) \leq \mcD(y)$.
\end{definition}



\begin{definition}[TV Distance]
    For two distributions $\mcP, \mcQ$ over a countable domain $\mcX$, we define the total variation distance:
    \begin{equation*}
        \TV(\mcP, \mcQ) = \frac{1}{2}\sum_{x \in \mcX} | \mcP(x) - \mcQ(x)|= \frac{1}{2}\| \mcP - \mcQ\|_1 .
    \end{equation*}
\end{definition}

\begin{definition}[$\ell_1$ Influence]
For any function $f : \bits^n \to \R$, the influence of the $i$-th variable on $f$ is given by:
\begin{equation*}
\Inf_{i}(f) \coloneqq \Ex_{\bx \sim \mcU^n} \big[| f(\bx) - f(\bx^{\sim i}) |\big],
\end{equation*}   
where $\bx^{\sim i}$ denotes $\bx$ with the $i$-th coordinate re-randomised. Note that the influence of a function is defined with respect to the uniform distribution over its domain.

We further define the total influence as the sum of influences over all variables:
$
\Inf(f) \coloneqq \sum_{i=1}^n \Inf_i(f).
$


\end{definition}

\begin{fact}[Influence $\equiv$ correlation for monotone functions]\label{fact:inf_eq_corr}
    Let $f : \bits^n \to \R$ be a monotone function. Then 
    \begin{equation*}
        \Inf_i(f) = \Ex_{\bx \sim \mcU^n}[f(\bx) \cdot \bx_i] .
    \end{equation*}
\end{fact}

\begin{definition}[$\ell_1$ Variance]
\label{def:variance} 
    For any function $f : \bits^n \to \R$,
    
    \begin{equation*}
        \Var^{(1)}(f) \coloneqq \Ex_{\bx, \by \sim \mcU^n} |f(\bx) - f(\by)|.
    \end{equation*}
    
    We will also sometimes use a different definition of variance, given by the mean absolute deviation of $f$:
    % Equivalently\anote{I think we don't need this def anymore, also I think they aren't equal?x}, 
    \begin{equation*}
    \Var_{\mu}(f) \coloneqq \Ex_{\bx \sim \mcU^n} |f(\bx) - \Ex[f]|.
    \end{equation*}
\end{definition}

These two definitions are equivalent, up to constant factors:

\begin{lemma}\label{lem:var_defs}
    For a function $f : \bits^n \to \R$, 
    \begin{align*}
           \Var_{\mu}(f) \leq \Var^{(1)}(f) \leq 2\Var_{\mu}(f) 
    \end{align*}    
\end{lemma}

\begin{proof}
    The second part follows immediately from the triangle inequality and the first is an application of Jensen's:
    \[ \Var_{\mu}(f) =  \Ex_{\bx \sim \mcU}\left[\Big| \Ex_{\by \sim \mcU} [f(\bx) -  f(\by)]\Big|\right]\\
        \leq  \Ex_{\bx, \by \sim \mcU}\left|  f(\bx) -  f(\by)\right|.  \qedhere \]  
    % \begin{align*}
    %     \Var_{\mu}(f) &=  \Ex_{\bx \sim \mcU}\left[\left| \Ex_{\by \sim \mcU} [f(\bx) -  f(\by)]\right|\right]\\
    %     &\leq  \Ex_{\bx, \by \sim \mcU}\left|  f(\bx) -  f(\by)\right|   \tag{Jensen's}
    % \end{align*} \
\end{proof}


\begin{definition}[Sensitivity]
For a function $f:\bits^n \to \R$ and $x \in \bits^n$, the sensitivity of $f$ at $x$ is defined to be
\[s(f,x) = \sum_{i=1}^n \Ind[f(x) \ne f(x^{\oplus i})].\] 
Furthermore, the sensitivity of $f$ is given by its maximum sensitivity over all points:
\[s(f) = \max_{x \in \bits^n} \{ s(f,x) \}.\]
\end{definition}

Note that the sensitivity of a decision tree is at most the depth of the decision tree, since any point can only be sensitive to the coordinates queried on its root-to-leaf path.

\subsection{Useful inequalities}
We present some useful inequalities for boolean functions.

\begin{lemma}[Efron-Stein]\label{lem:efron_stein}
For any function $f: \bits^n \to \R$:
\begin{align*}
    \Var^{(1)}(f) \leq \Inf(f).
\end{align*}
\end{lemma}



\begin{lemma}[Total influence and sensitivity]\label{lem:inf_s_var}
    For any function $f : \bits^n \to \R$:
    \begin{align*}
        \Inf(f) \leq 2s(f) \cdot \Var^{(1)}(f).
    \end{align*}
\end{lemma}

\begin{proof}
    Let $s = s(f)$ be the sensitivity of $f$ and consider the set, $\mathrm{snbr}(x) = \{i \in [n] \mid f(x) \ne f(x^{\oplus i})\}$. By assumption, $|\mathrm{snbr}(x)| \leq s$. We define a coupling $(\bx, \by) \sim \pi$ s.t. $\by$ is often in $\mathrm{snbr}(\bx)$,  but the marginal distributions $\pi(\bx)$ and $\pi(\by)$ are still uniform. First, sample $\bx \sim \mcU$. Then, sample $\by$ given $\bx$ as follows: for each $i \in \mathrm{snbr}(\bx)$, let $\by = \bx^{\oplus i}$ (i.e. flip the $i$-th coordinate of $\bx$) with probability $1/s$, and with the remaining $ 1 - |\mathrm{snbrs}(\bx)|/s$ probability, take $\by = \bx$. It is easy to see that the marginal distribution over $\by$ is still uniform.
    
    Unrolling the definition of influence, we have:
    \begin{align*}
        \Inf(f) 
            &= \sum_{i=1}^n \Ex_{\bx \sim \mcU} |f(\bx) - f(\bx^{\oplus i})|  \\
            &= \Ex_{\bx \sim \mcU} \left[ \sum_{i=1}^n |f(\bx) - f(\bx^{\oplus i})| \right ]  \\
            &= \Ex_{\bx \sim \mcU} \left[ \sum_{i \in \mathrm{snbr}(\bx)} |f(\bx) - f(\bx^{\oplus i})| \right ] \tag{only consider nonzero terms}\\
            &=  \Ex_{\bx \sim \mcU} \left[ s \cdot \sum_{i \in \mathrm{snbr}(\bx)} \frac{|f(\bx) - f(\bx^{\oplus i})|}{s}  \right ] \\
            &=  \Ex_{\bx \sim \pi} \left[ s \cdot \Ex_{\by \sim \pi(\cdot | \bx)} |f(\bx) - f(\by)|  \right ] \tag{definition of coupling $\pi$}\\
            &=  s \cdot\Ex_{(\bx, \by) \sim \pi} |f(\bx) - f(\by)|  \\
            &\leq  s \cdot \Ex_{(\bx, \by) \sim \pi}  |f(\bx) - \E[f]|+ s \cdot \Ex_{(\bx, \by) \sim \pi} |f(\by) - \E[f]|  \tag{triangle inequality}\\
            &=  2s \cdot \Var_\mu(f) \tag{marginal distributions of $\pi$ are uniform }\\
            &\leq  2s \cdot \Var^{(1)}(f) \tag{\Cref{lem:var_defs}}.
    \end{align*}
\end{proof}


