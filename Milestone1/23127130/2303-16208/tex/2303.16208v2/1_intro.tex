% \title{Lifting uniform learners via distributional decomposition  %\vspace{15pt} 
% }

% \author{}
% \author{Guy Blanc \vspace{8pt} \\ \hspace{-5pt}{\sl Stanford}
% \and \hspace{0pt} Jane Lange \vspace{8pt} \\ \hspace{-4pt}  {\sl MIT}
% \and Ali Malik \vspace{8pt}\\ \hspace{-8pt} {\sl Stanford}
% \and Li-Yang Tan \vspace{8pt} \\ \hspace{-8pt} {\sl Stanford}}  

% \date{\vspace{15pt}\small{\today}}


% \maketitle


% % \begin{abstract} 
% % We prove an algorithmic decomposition lemma for monotone high-dimensional distributions over $\zo^n$, showing how they can be efficiently broken down into a small number of close-to-uniform components, where ``efficiently" and ``small" scale with their inherent complexity.  A distribution $\mathcal{D}$ has {\sl decision tree complexity $d$} if its pmf can be computed by depth-$d$ decision tree; this 


% % Given samples from a monotone distribution $\mathcal{D}$ whose pmf is computed by a depth-$d$ decision tree---which gives a decom the mixture of $2^d$ many uniform distrubtions over subcubes), our algorithm runs in time $\poly(n)\cdot d^{O(d)}$ and 
% % \end{abstract} 

% \begin{abstract}
% We show how any PAC learning algorithm that works under the uniform distribution can be transformed, in a blackbox fashion, into one that works under an  arbitrary and unknown distribution~$\mathcal{D}$.  The efficiency of our transformation scales with the inherent complexity of~$\mathcal{D}$, running in $\poly(n, (md)^d)$ time for distributions over $\bits^n$ whose pmfs are computed by depth-$d$ decision trees, where $m$ is the sample complexity of the original algorithm.   For monotone distributions our transformation uses only samples from~$\mathcal{D}$, and for general ones it uses subcube conditioning samples.

% A key technical ingredient is an algorithm which, given the aforementioned access to $\mathcal{D}$, produces an {\sl optimal} decision tree decomposition of $\mathcal{D}$: an approximation of $\mathcal{D}$ as a mixture of uniform distributions over disjoint subcubes.   With this decomposition in hand, we run the uniform-distribution learner on each subcube and combine the hypotheses using the decision tree. This algorithmic decomposition lemma also yields new algorithms for learning decision tree distributions  with runtimes that exponentially improve on the prior state of the art---results of independent interest in distribution learning.     

% \footnote{\red{Keywords: PAC Learning, Semi-supervised learning, decision tree decomposition}}

% \end{abstract}

% \thispagestyle{empty}
% \newpage 
% \setcounter{page}{1}


% \newcommand{\depth}{\mathrm{depth}}

\section{Introduction} 


% \violet{The theoretical study of learning from data has traditionally fallen into two extremes.}\lnote{I'm hesitant about making such a claim.  I think we are implicitly suggesting that this is the case given what we wrote below, but I worry that making such a claim explicitly would open us up to reviewers who just throw a bunch of random counterexamples back at us.  I agree that we should find a smoother start to the paper than what I initially had though.} \anote{Yea i agree, i just wrote a random sentence as a placeholder, but I dont think we should keep it} 
A major strand of research in learning theory concerns the learning of an unknown target  function $f : \bits^n\to\zo$ with respect to an unknown source distribution $\mathcal{D}$.  This line of work has both motivated and benefited from rich connections to the study of function complexity: underlying every new learning algorithm are new structural insights about the concept class of functions that $f$ belongs to.  Our work is motivated by the question of whether structural properties of the {\sl source distribution $\mathcal{D}$} can be similarly leveraged for the design of efficient algorithms.  

%where the goal is to efficiently learn an unknown target function  $f : \bits^n \to \zo$  with respect to an unknown distribution $\mathcal{D}$ over the inputs,  has a long and rich history.%, dating from Valiant's seminal work on PAC learning \cite{Val84}. 

%One of the core focuses of this area has been to quantify the difficulty of the learning task based on various notions of complexity e.g. VC dimension, Rademacher complexity, etc.\anote{more?}
%Notably, these complexity measures have traditionally been properties of the target function $f$.  In this paper, we instead investigate an orthogonal question: 

% %\begin{quote}
%     \centering
%     \sl
%     How does the difficulty of learning scale with the  complexity of the \textbf{distribution} $\mcD$?
% \end{quote}

\paragraph{Distribution-free learning.} In Valiant's original distribution-free PAC learning model~\cite{Val84}, the target function $f$ is promised to belong to a {\sl known}, and often ``computationally simple" concept class $\mathscr{C}$, whereas no assumptions are made about the distribution $\mathcal{D}$.
% \paragraph{Distribution-free learning.} In PAC learning \cite{Val84}, the goal is to efficiently learn an unknown target function $f : \bits^n \to \zo$ with respect to an unknown distribution $\mathcal{D}$ over $\bits^n$.  The target function is promised to belong to a {\sl known} concept class $\mathscr{C}$ whereas, in contrast, no assumptions are made about $\mathcal{D}$.

Unfortunately, efficient algorithms have proven hard to come by even for simple concept classes.  Consider, for instance, the class of polynomial-size DNF formulas---the motivating example that Valiant used throughout his paper. Despite decades of effort, the current fastest algorithm for this class runs in exponential time, $\exp(\tilde{O}(n^{1/3}))$~\cite{KS04}.  For other classes, even formal hardness results have been established.  For example, under standard cryptographic assumptions it has been shown that there are no efficient algorithms for learning intersections of polynomially many halfspaces~\cite{KS09}.  

A major source of difficulty in the design of efficient learning algorithms in this model stems from the absence of assumptions about $\mathcal{D}$.  Even if the concept class $\mathscr{C}$ is assumed to be simple, this can be negated by the fact that the underlying distribution $\mathcal{D}$ is arbitrarily complex. Indeed, this is precisely the idea underlying existing hardness results for distribution-free learning. 

\paragraph{Distribution-specific learning.} This leads us to the {\sl distribution-specific} variant of PAC learning where $\mathcal{D}$ is known rather than arbitrary \cite{BenedekItai91,Natarajan92}.  For the domain
% concept classes over 
$\bits^n$, the most popular assumption takes $\mathcal{D}$ to be the uniform distribution---an assumption that opens the door to many more positive results.  For example, 
% in sharp contrast with the state of affairs in the distribution-free setting, 
there is a quasipolynomial-time algorithm for learning polynomial-size DNF formulas under the uniform distribution~\cite{Ver90}; if the learner is additionally given query access to $f$, there are even polynomial-time algorithms~\cite{Jac97,GKK08}. {{ Similarly, there is a  quasipolynomial-time algorithm for learning intersections of polynomially many halfspaces under the uniform distribution~\cite{KOS04}---a sharp contrast to the distribution-free setting where even learning intersections of {\sl two} halfspaces in subexponential time is an longstanding open problem~\cite{KS08,She13}.  Indeed, there is by now a suite of powerful techniques for the design of uniform-distribution learning algorithms, notably  ``meta-algorithms" for learning an arbitrary concept class $\mathscr{C}$ with performance guarantees that scale with its Fourier properties.  Famous examples include the algorithms for learning (approximately) low-degree functions~\cite{LMN93} and (approximately) sparse functions~\cite{KM93}, as well as their agnostic variants~\cite{KKMS08,GKK08}; see Chapter \S3 of~\cite{ODBook} for an in-depth treatment.}}


While this setting has proved to be fertile grounds for the development of sophisticated techniques and fast algorithms, it is admittedly a stylized theoretical model, since real-world data distributions often exhibit correlations and are not uniform.  For this reason, uniform-distribution learners have thus far been limited in their practical relevance.  

\subsection{This work}  These two settings, distribution-free and uniform-distribution learning, correspond to two extremes in terms of distributional assumptions.  Distribution-free algorithms make no assumptions about the distribution, but the design of such algorithms that are computationally efficient has proven correspondingly challenging.  On the other hand, we have a wealth of techniques and positive results in the uniform-distribution setting, but the guarantees of such algorithms only hold under a strong, often unrealistic assumption on the data distribution. 
Due to these vast differences, research in these two settings has mostly proceeded independently with few known connections. 

The motivation for our work is the search for fruitful middle grounds between these two extremes, where efficient algorithms can be obtained for expressive concept classes and where their guarantees hold for broad classes of distributions.  More generally, we ask:
\begin{quote}
    \emph{Can we interpolate between the two extremes via notions of {\sl distribution complexity}, and design learning algorithms whose efficiency scale with the inherent complexity of $\mathcal{D}$?}
\end{quote} 
Relatedly, can the large body of existing results and techniques for learning under the uniform distribution be lifted, ideally in a blackbox fashion, to non-uniform but nevertheless ``simple" distributions? 
\section{Our results} 

We provide affirmative answers to these questions via a natural notion of distribution complexity: 

\begin{definition}[Decision tree complexity of $\mathcal{D}$] 
\label{def:DT} 
The {\sl decision tree complexity} of a distribution $\mathcal{D}$ over $\bits^n$, denoted $\depth(\mathcal{D})$, is the smallest integer $d$ such that its probability mass function (pmf) can be computed by a depth-$d$ decision tree. 
\end{definition} 

Decision tree complexity interpolates between the uniform distribution on one extreme (depth $0$) and arbitrary distributions on the other (depth $n$).  

This notion, {{first considered by Feldman, O'Donnell, and Servedio~\cite{FOS08},}} generalizes other well-studied notions of complexity of high-dimensional discrete distributions and is itself a special cases of others.  A {\sl $d$-junta distribution} $\mathcal{D}$, introduced by Aliakbarpour, Blais, and Rubinfeld~\cite{ABR16}, is one for which there exists a set of $d$ coordinates $J \sse [n]$ such that for every assignment $\rho \in \bits^J$, the conditional distribution $\mathcal{D}_{J \leftarrow\rho}$ is  uniform.  Every $d$-junta distribution has decision tree complexity $d$, but a depth-$d$ decision tree distribution can have junta complexity as large as $2^d$.  On the other hand,  decision tree distributions are a special case of mixtures of subcubes~\cite{CM19}, since a depth-$d$ decision tree induces a partition of $\bits^n$ into $2^d$ many disjoint subcubes. Mixtures of subcubes are in turn a special case of mixtures of product distributions over discrete domains~\cite{FM99,Cry99,CGG01,FOS08}.  
 
 
 
\subsection{First main result: Lifting uniform-distribution learners}
 
 Our algorithms and analyses will use the notion of {\sl influence} of variables.  This notion is most commonly applied to boolean-valued functions, and in this work we extend its study to the pmfs of distributions: 
 
 \begin{definition}[Influence of variables on distributions]
 \label{def:inf} 
Let $\mathcal{D}$ be a distribution over $\bits^n$ and  $f_\mathcal{D}(x) = 2^n \cdot \mathcal{D}(x)$ be its pmf scaled up by the domain size.\footnote{This $2^n$ normalisation factor makes the average value of $f_\mathcal{D}$ exactly $1$, lending to a cleaner analysis.}  The {\sl influence} of a coordinate $i\in [n]$ on a distribution $\mathcal{D}$ over $\bits^n$ is the quantity 
 \[ 
 \Inf_i(f_\mathcal{D}) \coloneqq  \Ex_{\bx \sim \mcU^n}\big[ |f_\mathcal{D}(\bx)-f_\mathcal{D}(\bx^{\sim i})| \big], 
 \]
 where $\mcU^n$ denotes the uniform distribution over $\bits^n$ and $\bx^{\sim i}$ denotes $\bx$ with its $i$-th coordinate rerandomized.  The {\sl total influence} of $\mathcal{D}$ is the quantity $\Inf(f_\mathcal{D}) \coloneqq \sum_{i=1}^n \Inf_i(f_\mathcal{D}).$
 \end{definition}


With~\Cref{def:DT,def:inf} in hand we can now state our first main result.  For clarity, we first state it assuming a unit-time oracle that computes the influences of variables: 

\begin{theorem}[Lifting uniform-distribution learners; see~\Cref{thm:lift-formal} for the formal version]
\label{thm:lift} 
    For any concept class $\mathscr{C}$ of functions $f: \bits^n \to \zo$ closed under restrictions, assuming a unit-time influence oracle, if there is an algorithm for learning $\mathscr{C}$ under the uniform distribution to accuracy $\eps$ with sample complexity $m$ and running time $\poly(n,m)$, there is an algorithm for learning $\mathscr{C}$ under depth-$d$ decision tree distributions with sample complexity and running time
    \begin{equation*}
        M = \poly(n) \cdot \left(\frac{dm}{\eps}\right)^{O(d)}.
    \end{equation*}
    % For monotone distributions, the algorithm only uses random labeled samples, and for general distributions, it uses subcube conditional samples\gnote{We haven't defined these yet} from the input distribution, as well as random labeled samples.
\end{theorem}

{As an example setting of parameters,~\Cref{thm:lift} lifts a quasipolynomial-time uniform-distribution algorithm into one that still runs in quasipolynomial time, but now succeeds under any distribution with decision tree complexity $\polylog(n)$.  We note that such distributions are quite broad: the size of a depth-$d$ tree can be as large as $2^d = \mathrm{quasipoly}(n)$, corresponding to the mixture of that many subcubes.}  

The proof of~\Cref{thm:lift}   readily extends to lift {\sl agnostic} uniform-distribution  learners~\cite{Hau92,KSS94} to agnostic distribution-free ones.   As mentioned in the introduction, many algorithms for learning under the uniform distribution are  obtained through Fourier-analytic meta-algorithms.  By~\Cref{thm:lift}, we now have analogues of these meta-algorithms for distributions with low decision tree complexity.  

\paragraph{Estimating influences efficiently.} We have stated~\Cref{thm:lift} assuming that influences of variables on distributions can be computed exactly in unit time.  In the body of the paper we show how these quantities can be approximated to sufficiently high accuracy given access to $\mathcal{D}$.  For monotone distributions we show how this can be done using only samples from $\mathcal{D}$.  For general distributions, we use the notion of {\sl subcube conditioning samples}, proposed in~\cite{CRS15,BC18} and subsequently studied in~\cite{CCKLW21,CJLW21}: in this model, the algorithm specifies a subcube of $\bits^n$ and receives a draw $\bx \sim \mathcal{D}$ conditioned on $\bx$ lying in the subcube.  (For general distributions, estimating influences based only on samples is intractable as it can be easily seen to require $\Omega(\sqrt{2^n})$ many samples.) 
% \lnote{Let's add a brief justification for why subcube conditioning is necessary: Consider two distributions, uniform over $\bits^n$, and uniform over random subset of density $1/2$.  One needs  $\Omega(\sqrt{2^n})$ samples to distinguish between these two settings.  Since the influences are $0$ in the first case and $\Omega(1)$ in the second, it also requires that many samples to obtain accuracy influence estimates.}

\paragraph{A more general result.} We obtain~\Cref{thm:lift} as a corollary of a more general result which shows how every uniform-distribution learner $\mathcal{A}$ that is robust to {\sl distributional noise} can be lifted in a way that the resulting runtime depends only on $\mathcal{A}$'s noise tolerance and not its sample complexity.  \Cref{thm:lift} follows since every $m$-sample algorithm is automatically tolerant to an $O(1/m)$ amount of distributional noise.  We achieve better parameters for algorithms that are tolerant to higher amounts of noise. 


%\subsection{The key algorithmic subroutine and its analysis} 

\subsection{Second main result: Learning decision tree distributions}


Our algorithm for~\Cref{thm:lift} proceeds in a two-stage manner: we first learn the decision tree structure of $\mathcal{D}$ and then use the uniform-distribution learner to learn $f$ restricted to each of the leaves of the tree.   To carry out the first stage, we give an algorithm that learns the {\sl optimal} decision tree decomposition of a distribution $\mathcal{D}$: 


\begin{theorem}[Learning decision tree distributions]
\label{thm:learn-DT} 
    Let $\mcD$ be a distribution that is representable by a depth-$d$ decision tree.
    There is an algorithm that returns a depth-$d$ tree representing a distribution $\mcD'$
    such that $\TV(\mcD,\mcD') \le \eps$, with high probability over the draw of samples, and with running time and sample complexity $\poly(n) \cdot (d/\eps)^{O(d)}$. For monotone distributions, the algorithm only uses random samples from $\mcD$, and for general distributions, it uses subcube conditional samples.
\end{theorem}

{{\Cref{thm:learn-DT} is a result of independent interest in distribution learning.}}  Even setting aside properness, the performance guarantees of our algorithm  {{improves, quite dramatically, the prior state of the art for learning decision tree distributions and circumvents existing hardness results}}.  Aliakbarpour, Blais, and Rubinfeld~\cite{ABR16} gave an $n^{O(k)}$ time algorithm for learning $k$-junta distributions, which implies an $n^{O(2^d)}$ time algorithm for learning depth-$d$ decision tree distributions. Chen and Moitra~\cite{CM19} gave an $s^{s^3}\cdot n^{O(\log s)}$ time algorithm algorithm for learning the mixture of $s$ subcubes, which implies a $2^{2^{O(d)}}\cdot n^d$ time algorithm for learning depth-$d$ decision tree distributions.  Finally, Feldman, O'Donnell, and Servedio~\cite{FOS08} gave an $n^{O(m^3)}$ time algorithm algorithm for learning the mixture of $m$ product distributions, which implies an $n^{2^{O(d)}}$ time algorithm for learning depth-$d$ decision tree distributions.  These runtimes all have a doubly-exponential dependence on $d$, whereas ours only depends exponentially on $d$. { Note that the runtime of any  algorithm must have at least an exponential dependence on $d$ since that is the description length of a depth-$d$ decision tree.}


None of these prior algorithms are proper, and that fact that ours is is crucial to the application to~\Cref{thm:lift}: the decision tree structure specifies a decomposition of $\bits^n$ into disjoint subcubes, and it is on these subcubes that we run our uniform-distribution learner.  

\paragraph{Circumventing hardness results.} {{A novel aspect of~\Cref{thm:learn-DT} is that it sidesteps existing hardness results for learning decision tree distributions.}} \cite{FOS08} showed that the problem of learning depth-$d$ decision tree  distributions is as hard as that of learning depth-$d$ decision tree {\sl functions} under the uniform distribution. Despite significant efforts for over three decades, the current fastest algorithm for the latter problem runs in time $n^{O(d)}$~\cite{EH89} and improving on this is a  longstanding challenge of learning theory. (It contains as a special case the junta problem~\cite{BL97}---learning $k$-juntas in time better than $n^{O(k)}$---itself already a notorious open problem.) ~\Cref{thm:learn-DT} shows that this barrier can be circumvented in two different ways: by giving the algorithm access to subcube conditional samples, and by considering monotone distributions.   


\paragraph{Proof Overview of~\Cref{thm:learn-DT}.}
To describe the intuition behind~\Cref{thm:learn-DT} and the role that influence plays in its proof, we begin by considering the following elementary equations: 
\begin{align} 
\Ex_{\bb \sim \bits}\big[\Inf((f_{\mathcal{D}})_{x_i=\bb})\big] &= \Inf(f_\mathcal{D}) - \Inf_i(f_\mcD) \label{eq:influence-drops} \\
2\cdot  \TV(\mathcal{D},\mathcal{U})  & \le \Inf(f_{\mathcal{D}}),  \label{eq:influence upper bounds distance to uniformity} 
\end{align} 
where $(f_\mathcal{D})_{x_i=b}$ denotes the restriction of $f_\mathcal{D}$ by fixing $x_i$ to $b$. 
\Cref{eq:influence-drops}, which follows from the definition of influence, says that the total influence of $f_\mathcal{D}$ drops by $\Inf_i(f)$ when restricted by $x_i$.  \Cref{eq:influence upper bounds distance to uniformity}, which is a consequence of the Efron--Stein inequality, says that the total influence of a distribution upper bounds its distance to uniformity.  

Together, they  suggest a simple and natural algorithm for learning decision tree distributions: build a decision tree hypothesis for $\mathcal{D}$ greedily by iteratively querying the most influential variable.  After sufficiently many stages, the total influence of the conditional distributions at most leaves will be close to zero (by~\Cref{eq:influence-drops}), which in turn means that most leaves will be close to uniform (\Cref{eq:influence upper bounds distance to uniformity}).  Indeed, this intuition can be formalized using the techniques in this work to get an algorithm that learns depth-$d$ decision tree distributions with depth-$O(d^2)$ decision tree hypotheses in time~$2^{O(d^2)}$.  

To obtain the improved parameters of~\Cref{thm:learn-DT}, we consier a generalization of this different algorithm: instead of splitting on the single most influential variable, we consider all $O(d)$ most influential ones as candidate splits.  While this involves searching over more candidates at each split, we will show that at least one of the choices leads to a high accuracy hypothesis at depth $d$ instead of $O(d^2)$, resulting in a smaller search space of $d^{O(d)}$ instead of $2^{O(d^2)}$.  

Our approach is inspired by a recent algorithm of Blanc, Lange, Qiao, and Tan for properly learning decision tree {\sl functions} under the uniform distribution~\cite{BLQT21focs}.  Our analysis builds on and extends theirs to the setting of unsupervised learning, which poses a number of challenges that we have to overcome.  First, while highly accurate estimates of influences can be easily obtained with membership queries to the function (in our case, the pmf of $\mathcal{D}$), subcube conditioning samples  provide more limited and coarse-grained information about $\mathcal{D}$.  Our algorithms for estimating influences with subcube conditioning samples, and from samples alone for monotone distributions, could see further utility in other problems.  Second, while it is easy to estimate how close a {\sl function} is to a constant (relatedly, how close two functions are) via random sampling, the analogous problem of estimating the distance of {\sl distribution} to uniformity is an intractable problem: for a distribution over $\bits^n$, the sample complexity of estimating its distance to uniformity is $\Theta(2^n/n)$~\cite{VV11}.  There are no known improvements using subcube conditioning samples (though~\cite{BC18,CCKLW21} give efficient uniformity {\sl testers}), and the best known algorithm for monotone distributions uses $2^{n-\Theta(\sqrt{n}\log n)}$ samples~\cite{RV20}.  We sidestep this barrier by showing how total influence---for which we provide efficient estimators---can be used as a good proxy for distance to uniformity. 

%This fundamental challenge requires significant changes to 



\subsection{Other related work} 

\paragraph{Conditional samples in distribution learning and testing.}  The subcube conditioning model falls within a recent line of work on the power of {\sl conditional samples} in distribution learning and testing.  In this more general model, which was independently introduced by Chakraborty, Fischer, Goldhirsh, and Matsliah~\cite{CFGM16} and Canonne, Ron, and Servedio~\cite{CRS15}, the algorithm can specify an arbitrary subset of the domain and receive a sample conditioned on falling within this subset.  Since its introduction, a large number of works have designed conditional sample algorithms, in distribution learning and testing~\cite{Can15,FJOPS15,ACK15chasm,SSJ17,BCG19, FLV19,CJLW21,CCKLW21} and beyond~\cite{ACK15,GTZ17,GTZ18}.  Our results add to this line of work, and further reinforce the message that conditional samples (indeed, even just subcube conditional samples) can be used circumvent sample complexity lower bounds in a variety of settings. 

Other access models to distributions include queries to the pmf or cdf (the {\sl evaluation oracle} model)~\cite{BDKR05,GMV06,RS09,CR14} and giving the algorithm probability revealing samples~\cite{OS18}.


%\paragraph{Learning junta distributions.} 
\paragraph{Semi-supervised learning.}
There is extensive research in the statistical machine learning literature on leveraging unlabeled examples to improve learning.  Much of this work focuses on improving sample complexity or convergence rates of existing algorithms using additional unlabelled data \cite{pmlr-v99-gopfert19a,BdLP08}. In contrast, our work is aimed at creating \emph{computational and sample efficient} algorithms from existing ones that are only guaranteed to work under ``nice" distributions. Results of this flavour have  been studied in limited ways, e.g.~transforming a 1d-uniform learning algorithm to one that works on any 1d continuous distribution \cite{BdLP08}.



\paragraph{The work of~\cite{BOW10}.} In~\cite{BOW10}, Blais, O'Donnell, and Wimmer gave an algorithm for performing polynomial regression under arbitrary product distributions over $\bits^n$.  As an application, they showed how their algorithm can be lifted to {\sl mixtures} of product distributions via the algorithm of~\cite{FOS08} for learning mixtures of product distributions.  

Like our work, this is also an example where algorithms for learning with respect to a ``simple" distribution (the uniform distribution in our case and product distributions in~\cite{BOW10}'s case) can be lifted to more complex ones (decision tree distributions in our case and mixtures of product distributions in~\cite{BOW10}'s case), via a distribution learning algorithm that decomposes the more complex one into simple ones (\Cref{thm:learn-DT} in our case and~\cite{FOS08}'s algorithm in~\cite{BOW10}'s case).  The quantitative details of our transformations are incomparable: our algorithm for learning depth-$d$ decision tree distributions run in $\poly(n)\cdot d^{O(d)}$ time, whereas~\cite{FOS08}'s algorithm for learning the mixture of $m$ product distributions run in $n^{O(m^3)}$ time.  {{(Recall that a depth-$d$ decision tree induces a mixture of as many as $m=2^d$ product distributions.)}} 



\section{Discussion and future work}

We view our work as part of  two broader and potentially fruitful approaches to PAC learning.  Much of the progress in the field thus far has been guided by the design of efficient learning algorithms for successively more expressive concept classes, as measured according to various notions of function complexity; this was the approach advocated in Valiant's pioneering paper and other early works. For example, on one such axis we have small-width conjunctions as a special case of small juntas, which are in turn a special case of small-depth decision trees, which are in turn a special case of small-width DNFs, and so on, and the field seeks to design efficient algorithms for each of these classes.  We believe that it is equally natural to make progress along a separate dimension, with respect to various notions of distribution complexity. The overall goal can then be cast as that of learning successively more expressive concept classes with respect to successively more expressive distributions.  


Next, our algorithm is just one instantiation of a general two-stage approach to learning that is studied in the semi-supervised literature (see e.g.~\cite{BFB10}): the first gathers information about the underlying distribution, and the second exploits this distributional information to learn the target function.  It would be interesting to develop more computationally efficient examples of such a two-stage approach. More broadly, there should be much to be gained from using the insights of distribution learning to counter the difficulty of distribution-free PAC learning, the crux of which is the potential nastiness of the unknown distribution. 

Finally, looking beyond PAC learning, a similar gulf exists between uniform-distribution and distribution-free {\sl testing} of function properties. The original model of property testing was defined with respect to the uniform distribution~\cite{RS96,GGR98} and much of the ensuing research has focused on this setting, with the distribution-free variant receiving increasing attention in recent years. Can uniform-distribution testers be lifted generically to the distribution-free setting?   A concrete avenue towards such a result would be via a variant of our distribution decomposition lemma that runs in sublinear time. 



