%-------------------------------------------------------------------------
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[t]
\centering
\resizebox{2.1\columnwidth}{!}{%
\begin{tabular}{llccccccccccc}
\toprule[1.5pt]
                          &            & \multicolumn{5}{c}{Terra Incognita~\cite{beery2018recognition}}            &  & \multicolumn{5}{c}{VLCS~\cite{fang2013unbiased}}                       \\ \cline{3-7} \cline{9-13} 
Metric                    & Method                                       & Loc.100          & Loc.38           & Loc.43           & Loc.46           & Avg.             &  & Caltech101       & LabelMe          & SUN09           
                          & VOC2007          & Avg.              \\ \toprule[1pt]
\multirow{6}{*}{DEC loss $\downarrow$} & ERM~\cite{vapnik1999nature}                  & 1.014            & 0.971            & 0.998            & 1.016            & 1.000            &  & 0.991            & 0.902            & 0.987           
                          & 1.120            & 1.000             \\
                          & GroupDRO~\cite{sagawa2019distributionally}   & 0.929            & 0.996            & 1.072            & 0.969            & 0.992            &  & 0.947            & 0.907            & 1.026           & 0.942            & 0.956             \\
                          & IRM~\cite{arjovsky2019invariant}             & 0.982            & 0.965            & 0.982            & 0.939            & 0.967            &  & \underline{0.918}& \underline{0.874}&\underline{0.947} & 0.928            & \underline{0.918} \\
                          & Mixup~\cite{xu2020adversarial}               & 0.998            & 0.926            & 1.032            & 0.970            & 0.982            &  & 0.942            & 1.014            & 1.072           & 0.947            & 0.995             \\ \cline{2-13} 
                          & CGC~\cite{pillai2022consistent}              & \underline{0.382}& \underline{0.412}& \underline{0.396}& \underline{0.356}& \underline{0.387}&  & 1.000            & 0.915            & 0.992           & \underline{0.918}& 0.956             \\
                          & DRE (ours)                                   & {\bf 0.195}      & {\bf 0.250}      & {\bf 0.213}      & {\bf 0.226}      & {\bf 0.221}      &  & {\bf 0.061}      & {\bf 0.431}      & {\bf 0.421}      & {\bf 0.229}      & {\bf 0.286}       \\ \toprule[1pt]
\multirow{6}{*}{iAUC $\uparrow $}     & ERM~\cite{vapnik1999nature}                  & 0.517            & 0.644            & \underline{0.614}& \underline{0.560}& 0.584            &  & \underline{0.964}& \underline{0.787}& {\bf 0.765}           
                               & \underline{0.758}& \underline{0.819}             \\
                          & GroupDRO~\cite{sagawa2019distributionally}   & {\bf 0.678}      & 0.578            & 0.608            & 0.525            & \underline{0.597}&  & 0.928            & 0.786            & 0.643           & 0.706            & 0.766            \\
                          & IRM~\cite{arjovsky2019invariant}             & 0.489            & \underline{0.651}& 0.438            & 0.500            & 0.520            &  & 0.939            & 0.772            & 0.613           & 0.715            & 0.760             \\
                          & Mixup~\cite{xu2020adversarial}               & 0.535            & 0.471            & 0.488            & 0.450            & 0.486            &  & 0.924            & 0.767            & 0.607           & 0.641            & 0.735             \\ \cline{2-13} 
                          & CGC~\cite{pillai2022consistent}              & 0.238            & 0.250            & 0.380            & 0.322            & 0.298            &  & 0.804            & 0.581            & 0.524           & 0.618            & 0.632             \\
                          & DRE (ours)                                   & \underline{0.651}& {\bf 0.685}      & {\bf 0.633}      & {\bf 0.605}      & {\bf 0.644}      &  & {\bf 0.973}      & {\bf 0.837}      & \underline{0.752}       & {\bf 0.763}          & {\bf 0.840}             \\ \toprule[1pt]
\multirow{6}{*}{Acc (\%) $\uparrow $} & ERM~\cite{vapnik1999nature}                  & 49.8             & 42.1             & \underline{56.9} & 35.7             & 46.1             &  & 97.7             & 64.3             & 73.4            
                          & 74.6             & 77.5              \\
                          & GroupDRO~\cite{sagawa2019distributionally}   & 41.2             & 38.6             & 56.7             & 36.4             & 43.2             &  & 97.3             & 63.4             & 69.5            & 76.7             & 76.7              \\
                          & IRM~\cite{arjovsky2019invariant}             & 54.6             & 39.8             & 56.2             & 39.6             & 47.6             &  & {\bf 98.6}       & \underline{64.9} & 73.4            & \underline{77.3} & \underline{78.5}  \\
                          & Mixup~\cite{xu2020adversarial}               & \underline{59.6} & 42.2             & 55.9             & 33.9             & \underline{47.9} &  & \underline{98.3} & 64.8             & 72.1            & 74.3             & 77.4              \\ \cline{2-13} 
                          & CGC~\cite{pillai2022consistent}              & 51.8             & \underline{44.6} & 54.9             & \underline{39.8} & 47.8             &  & 97.1             & 63.2             & \underline{73.6} & 70.6             & 76.1              \\
                          & DRE (ours)                                   & {\bf 64.1}       & {\bf 48.1}       & {\bf 57.1}       & {\bf 42.8}       & {\bf 53.0}       &  & \underline{98.3} & {\bf 65.5}       & {\bf 73.8}      & {\bf 80.2}       & {\bf 79.5}        \\ \bottomrule[1.5pt]
\end{tabular}
}
\caption{
Comparison of the {\it out-of-distribution} explanation and prediction performance on {\it Terra Incognita}~\cite{beery2018recognition} and {\it VLCS}~\cite{fang2013unbiased} datasets.
The models are tested on the specified distribution and trained on all other distributions.
Our results are on the average of three trials of experiments.
We highlight the {\bf best results} and the \underline{second best} results.
Note that the Acc (\%) numbers for ERM~\cite{vapnik1999nature}, GroupDRO~\cite{sagawa2019distributionally}, IRM~\cite{arjovsky2019invariant}, and Mixup~\cite{xu2020adversarial} are from Gulrajani and Lopez-Paz~\cite{gulrajani2020search}.
}
\label{tab:terra-vlcs}
\end{table*}
%-------------------------------------------------------------------------



%-------------------------------------------------------------------------
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[t]
\centering
\resizebox{2.1\columnwidth}{!}{%
\begin{tabular}{llcccccccccc}
\toprule[1.5pt]
Metric                                                                                      & Method                             & Africa       & E. Asia       & Europe       & N. Africa       & N. America   & Oceania 
         & Russia       & S. America    & S. Asia        & Avg. \\ \toprule[1pt]
\multirow{2}{*}{DEC loss $\downarrow$}                                                      & ERM~\cite{vapnik1999nature}        & 3.69e-1      & 2.25e-1       & 9.98e-1      & 1.43e+0         & 2.14e+1      & 2.27e+0  
         & 5.56e-1      & 5.58e-1       & 4.54e-1        & 1.00e+0     \\
                                                                                            & DRE (ours)                         & {\bf 2.52e-1}& {\bf 2.11e-1} & {\bf 9.20e-2}& {\bf 1.12e-3}   & {\bf 4.86e-6}& {\bf 7.25e-1}   
         & {\bf 5.17e-6}& {\bf 6.38e-6} & {\bf 1.10e-1}  & {\bf 1.55e-1} \\ \toprule[1pt]
\multirow{2}{*}{SC $\uparrow$}                                                              & ERM~\cite{vapnik1999nature}        & 0.810        &  {\bf 0.779}  & 0.100        & -0.707          & -0.653       & 0.855  
         & -0.856       & -0.837        & -0.504         & -0.113      \\
                                                                                            & DRE (ours)                         & {\bf 0.967}  &  {\bf 0.779}  & {\bf 0.760}  & {\bf 0.135}     & {\bf 0.189}  & {\bf 0.894}   
         & {\bf 0.130}  & {\bf 0.058}   & {\bf 0.324}    & {\bf 0.471}     \\ \toprule[1pt]
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Prediction\\ Residual $\downarrow$\end{tabular}} & ERM~\cite{vapnik1999nature}        & 7.45e-4      & 1.46e-3       & 2.43e-3      & 7.10e-4         & 5.40e-4      & {\bf 1.53e-4} & 8.04e-5      & 3.55e-4       & 2.26e-3        & 9.70e-4     \\
                                                                                            & DRE (ours)                         & {\bf 6.58e-4}& {\bf 1.16e-3} & {\bf 1.95e-3}& {\bf 5.74e-4}   & {\bf 4.14e-4}& 1.56e-4 
         & {\bf 6.34e-5}& {\bf 2.94e-4} & {\bf 1.85e-3}  & {\bf 7.91e-4}     \\ \bottomrule[1.5pt]
\end{tabular}
}
\caption{
Comparison of the {\it out-of-distribution} explanation and prediction performance for short-term urbanization estimation (2000â€“2010) on the {\it Urban Land}~\cite{gao2020mapping} dataset.
Our results are on the average of three trials of experiments.
Note that a residual of 0.01 indicates a one-percentage point difference between the estimated and observed built-up land fractions.
}
\label{tab:urban}
\end{table*}
%-------------------------------------------------------------------------



To best validate the performance, we conduct a series of experiments to compare our DRE method with existing methods.
The experimental results prove that our method achieves superior explanation and prediction robustness against {\it out-of-distribution} data on a wide scope of tasks, including classification and regression tasks on image and scientific tabular data.



\subsection{Datasets and Implementation Details}
{\bf Datasets.}
We validate our method on two OOD generalization benchmark image datasets for classification and a scientific tabular dataset for regression.
(1) {\it Terra Incognita}~\cite{beery2018recognition} ($\approx$ 11K images, 10 classes) consists of four sub-datasets: Location 100, Location 38, Location 43, and Location 46.
Each sub-dataset indicates a camera trap location in the wild and can be viewed as a different distribution.
Each image in these datasets contains one single animal category ({\it e.g.,} coyote) with different illumination, backgrounds, perspective, etc.
(2) {\it VLCS}~\cite{fang2013unbiased} ($\approx$ 25K images, 5 classes) consists of four sub-datasets: Caltech101~\cite{FeiFei2004LearningGV}, LabelMe~\cite{russell2008labelme}, SUN09~\cite{choi2010exploiting}, and VOC2007~\cite{pascal-voc-2007}.
Each sub-dataset can be viewed as a different distribution.
Each image in these datasets contains one single image category ({\it e.g.,} car) with different styles and backgrounds.
(3) {\it Global National Total Amounts of Urban Land, SSP-Consistent Projections and Base Year, v1 (2000 - 2100)}~\cite{gao2020mapping} (hereinafter referred to as {\it Urban Land}).
The dataset is used for urban land prediction, and the global land area has been divided into 997,022 grid cells.
Each grid cell contains nine topographic, population, and historical urban fraction attributes.
The task is to predict the urban fraction in the year 2010.
The world is been divided into nine continental regions, each region can be viewed as a different distribution.




{\bf Implementation details.}
For all datasets, we alternately leave one distribution out as the testing set.
We split the data from each training distribution into 80\% and 20\% splits, and use the larger splits for training, the smaller splits for validation and model selection.
All models are trained using Adam~\cite{kingma2014adam} optimizer for 5,000 steps.
(1) For both of the image datasets, following the setup in~\cite{gulrajani2020search}, we use a ResNet50~\cite{he2016deep} model pretrained on ImageNet~\cite{deng2009imagenet} and fine-tune.
We freeze all batch normalization layers before fine-tuning and insert a dropout layer before the final linear layer.
We crop the images of random size and aspect ratio, resizing to 224 $\times $ 224 pixels, random horizontal flips, random color jitter, grayscaling the image with 10\% probability, and normalization using the ImageNet channel statistics.
We use learning rate=5e-5 and batch size=16.
(2) For scientific data, following the setup in~\cite{li2021deep}, we transform the tabular data into image-like data with each grid cell as a pixel.
With each land pixel as the center pixel, we densely sampled 997,022 images with size 16 $\times$ 16.
We use a standard U-Net~\cite{ronneberger2015u} model training from scratch.
The images have been augmented by random vertical and horizontal flips, rotation by $90^{\circ}$, $180^{\circ}$, $270^{\circ}$.
We use learning rate=1e-4 and batch size=256.





%-------------------------------------------------------------------------
\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{Figure_Camera_ready/Fig_VLCS.pdf}
\caption{
Grad-CAM explanations for images from {\it Bird} (left) and {\it Chair} (right) classes in {\it VLCS} dataset.
The model trained via existing methods, such as ERM~\cite{vapnik1999nature}, Mixup~\cite{xu2020adversarial}, and CGC~\cite{pillai2022consistent}, not only focuses on the objects, but also distribution-specific associations, it getting even severe on OOD data.
On the contrary, our model alleviates the reliance on {\it spurious correlations} ({\it e.g.}, background pixels), and makes consistent explanations on OOD data.
This figure is best viewed in color.
}
\label{fig:exp-vlcs}
\end{figure*}
%-------------------------------------------------------------------------




\subsection{Evaluation Metrics}
\label{sec:metric}
{\bf Distributional explanation consistency (DEC).}
We evaluate the explanations using the same explanation consistency we used for our model training.
Note that although we optimized the in-distribution explanation consistency, the explanation consistency on {\it out-of-distribution} data is still an important metric to evaluate our goal.
We mix up the OOD samples and their explanations with in-distribution samples and their explanations to calculate the DEC loss.
For better comparison, we set the average DEC loss for the model trained via standard ERM~\cite{vapnik1999nature} as 1.0 and rescale others correspondingly.
We expect the DEC loss to be lower for the robust explanations against {\it out-of-distribution} data.



{\bf Explanation fidelity (iAUC).}
% Or faithfulness introduced by~\cite{Petsiuk2018rise}.
This metric measures an increase in the probability of the predictive score as more and more pixels are introduced in a descending order of importance, where the importance is obtained from the explanation~\cite{Petsiuk2018rise, 10.5555/3491440.3491857}.
We measure the area under the insertion curve (iAUC), which accumulated the probability increase while gradually inserting the features from the original input into a reference input.
The reference input we used is an initially blurred canvas, since the blurring takes away most of the finer details of an image without exposing it to sharp edges which might introduce spurious correlations.
To compare between models, we normalize the logit of the correct class to 1.
We expect the iAUC score to be higher for a robust explanation against OOD data.



{\bf Scientific consistency (SC).}
We introduce this metric for the explanations of scientific data.
Scientific consistency means the explanations obtained are plausible and consistent with existing scientific principles~\cite{roscher2020explainable}.
We leverage explanations of domain expert verified models as the ground truth domain knowledge. 
We use it as a posteriori by cosine similarity between feature importance obtained via explanations and domain knowledge.
We expect the SC score to be higher for a robust explanation against distributional shifts.




\subsection{Evaluation on Terra Incognita}
\label{sec:evaluate_terra}
We compare our model with models trained via four representative OOD generalization methods ({\it i.e.}, ERM~\cite{vapnik1999nature}, GroupDRO~\cite{sagawa2019distributionally}, IRM~\cite{arjovsky2019invariant}, and Mixup~\cite{xu2020adversarial}), and a state-of-the-art explanation-guided learning method ({\it i.e.}, CGC~\cite{pillai2022consistent}).
We use Grad-CAM~\cite{selvaraju2017grad} to generate explanations after model training. 
% Note that the four OOD generalization methods involve no explanation constraints during training.
Quantitatively, in Tab.~\ref{tab:terra-vlcs} we report the results of explanation quality and predictive performance on OOD data.
Our model outperforms the four OOD generalization methods and CGC~\cite{pillai2022consistent} with significant improvement on all three metrics.
On average of each distribution as the testing set, our method outperforms the second-best results by 42.9\%, 7.9\%, and 5.1\% in terms of DEC loss, explanation fidelity, and prediction accuracy.
Qualitatively, in Fig.~\ref{fig:intro} we visualize the Grad-CAM explanations of ERM, GroupDRO, IRM, and our model using Location 46 as the testing distribution.
Our explanations corrected the wrongly focused explanations generated by the models of existing methods.
Rather than backgrounds, our explanations are more concentrated on the most discriminative object.
The results demonstrate the superiority of our model on explanation and prediction robustness against OOD data.





\subsection{Evaluation on VLCS}
Following the same settings in Sec.~\ref{sec:evaluate_terra}, we compare our model with ERM~\cite{vapnik1999nature}, GroupDRO~\cite{sagawa2019distributionally}, IRM~\cite{arjovsky2019invariant}, Mixup~\cite{xu2020adversarial}), and CGC~\cite{pillai2022consistent} models.
Quantitatively, as shown in Tab.~\ref{tab:terra-vlcs}, our model outperforms existing methods with significant improvement on all three metrics.
On average of each distribution as the testing set, our method outperforms the second-best results by 68.8\%, 2.6\%, and 1.0\% in terms of DEC loss, explanation fidelity, and prediction accuracy.
Qualitatively, in Fig.~\ref{fig:exp-vlcs} we visualize the Grad-CAM explanation of five existing methods and our method.
Our explanations are more concentrated on the most discriminative features of the object, and significantly alleviate the focus of background pixels on OOD data.
Furthermore, we evaluate the efficiency of our method shown in Tab.~\ref{tab:efficiency}.




\subsection{Evaluation on Urban Land}
%-------------------------------------------------------------------------
% \begin{figure}[t]
% \centering
% % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    \includegraphics[width=1.0\linewidth]{Figure/Fig_urban.pdf}

% \caption{
% The feature importance visualizations of {\it Africa} (OOD testing set) in the {\it Urban} dataset.
% The feature importance calculated by our model is more consistent with domain knowledge.
% }
% \label{fig:exp-urban}
% \end{figure}
%-------------------------------------------------------------------------
We compare our model with models trained via ERM~\cite{vapnik1999nature}.
We leverage the Input Gradient~\cite{simonyan2013deep} to generate explanations after model training, because of its fine-grained resolution and advanced explanation performance on truly continuous inputs~\cite{ross2017right}.
As shown in Tab.~\ref{tab:urban}, our model outperforms the ERM model with significant improvement on all three metrics.
On average of each continental region (distribution) as the testing set, our method outperforms the ERM method by 84.5\%, 29.2\%, and 18.5\% in terms of DEC loss, scientific consistency, and prediction residual.
Note that the higher scientific consistency would further achieve promising and valuable scientific outcomes in the downstream tasks~\cite{roscher2020explainable}.




%-------------------------------------------------------------------------
\begin{table}[t]
\centering
\begin{tabular}{lccc}
\toprule[1pt]
Method                          & \# of params. & Training Time & Acc (\%) \\ \toprule[0.5pt]
ERM~\cite{vapnik1999nature}     & 25.6M         & 18.4min       & 74.6     \\
CGC~\cite{pillai2022consistent} & 25.6M         & 28.8min       & 70.6     \\
DRE (ours)                      & 25.6M         & 33.1min       & {\bf 80.2}     \\ \bottomrule[1pt]
\end{tabular}
\caption{
Efficiency comparison on {\it VLCS}~\cite{fang2013unbiased} dataset using VOC2007~\cite{pascal-voc-2007} as testing distribution.
Our model significantly outperforms existing methods on prediction accuracy with a marginal increase in train time than the CGC~\cite{pillai2022consistent} method, no additional parameters were introduced. 
}
\label{tab:efficiency}
\end{table}
%-------------------------------------------------------------------------



%-------------------------------------------------------------------------
\begin{table}[t]
\centering
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{lccc}
\toprule[1pt]
                                                                        & DEC loss $\downarrow$          & iAUC $\uparrow$             & Acc (\%) $\uparrow$ \\ \toprule[0.5pt]
ERM~\cite{vapnik1999nature}                                             & 1.000$\pm$0.02                 & \underline{0.758}$\pm$0.01  & 74.6$\pm$1.3     \\
DRE w/o reg.                                 & 0.909$\pm$0.01                 & 0.756$\pm$0.01              & \underline{78.1}$\pm$0.9     \\
DRE w/o consist.                                   & {\bf 0.364}$\pm$0.03           & 0.698$\pm$0.02              & 71.2$\pm$2.1     \\
DRE (full)                                                              & \underline{0.773}$\pm$0.01     & {\bf 0.772}$\pm$0.01        & {\bf 80.2}$\pm$0.4     \\ \bottomrule[1pt]
\end{tabular}
}
\caption{
Ablation study on {\it VLCS}~\cite{fang2013unbiased} dataset using VOC2007~\cite{pascal-voc-2007} as testing distribution.
Although the variant (w/o explanation regularization) increases the prediction accuracy compared to ERM, it makes limited improvement in DEC loss and slightly dropped on iAUC.
The variant (w/o explanation consistency) has the most significant DEC loss decrease, but it also leads to a severe drop in iAUC and prediction accuracy.
}
\label{tab:ablation}
\end{table}
%-------------------------------------------------------------------------



\subsection{Ablation Study}
In this section, we perform ablation studies to investigate key components of our method proposed in Eq.~\ref{eq:L_con_lagrange} and Eq.~\ref{eq:L_reg}.
The empirical results are shown in Tab.~\ref{tab:ablation}

{\bf Ablation on explanation regularization.}
The variant (w/o explanation regularization) increases the predictive accuracy by 3.5\% compared to ERM, however, the it makes limited improvement in DEC loss and slightly dropped on iAUC.
This indicates limited enhancement of explanation robustness against OOD data, the model might falls into a local minimum to satisfy the {\it distributional explanation consistency} constraint.
For example, an explanation that uniformly attributes the prediction to all features.




{\bf Ablation on distributional explanation consistency.}
The variant (w/o explanation consistency) significantly improved the DEC loss by 63.6\% over the ERM model.
However, the explanation fidelity and prediction accuracy are significantly decreased by 6.0\% and 3.4\%.
This indicates that blindly encouraging the explanation sparsity would hurt the explanation and predictive performance.




\subsection{Generalize to Different Explanation Methods}
In this experiment, we compare the saliency maps of ours and the ERM model.
As shown in Fig.~\ref{fig:abl-exp}, the improved explainability of our model trained by Grad-CAM can be generalized to a variety of data-driven explanation methods.
Using Location 46 in {\it Terra Incognita} as the testing set, for the ERM model, the saliency maps of both Integrated Gradients (IG)~\cite{sundararajan2017axiomatic} and Gradient SHAP~\cite{lundberg2017unified} methods are excessively focused on background pixels, such as branch and ground.
On the contrary, the saliency maps of our model alleviate the reliance on background pixels, and clearly depicts the contour of the object.



%-------------------------------------------------------------------------
\begin{figure}[t]
\centering
% \fbox{\rule{0pt}{3in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{Figure_Camera_ready/Fig_SHAP.pdf}

\caption{
Integrated Gradients (IG)~\cite{sundararajan2017axiomatic} and Gradient SHAP~\cite{lundberg2017unified} saliency maps for OOD data from {\it Terra Incognita} dataset.
Using location 46 as the testing set, for ERM model the saliency maps of both Integrated Gradients (IG)~\cite{sundararajan2017axiomatic} and Gradient SHAP~\cite{lundberg2017unified} methods are excessively focused on background pixels, such as branch and ground.
}
\label{fig:abl-exp}
\end{figure}
%-------------------------------------------------------------------------




