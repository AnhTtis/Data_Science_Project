{\bf Explainable machine learning.}
A suite of techniques has been proposed to reveal the decision-making process of modern {\it black-box} ML models.
One direction is intrinsic to the model design and training, rendering an explanation along with its output, {\it e.g.}, attention mechanisms~\cite{vaswani2017attention} and joint training~\cite{hind2019ted, chen2019looks}.
A more popular way is to give insight into the learned associations of a model that are not readily interpretable by design, known as post-hoc methods.
Such methods usually leverage backpropagation or local approximation to offer saliency maps as explanations, {\it e.g.}, Input Gradient~\cite{simonyan2013deep}, Grad-CAM~\cite{selvaraju2017grad}, LIME~\cite{ribeiro2016should}, and SHAP~\cite{lundberg2017unified}.
Recent works have shed light on the downsides of post-hoc methods.
The gradient-based explanations ({\it e.g.}, Input Gradient) are consistent with sample-based explanations ({\it e.g.}, LIME) with comparable fidelity but have much lower computational cost~\cite{ross2017right}.
Moreover,  only the Input Gradient and Grad-CAM methods passed the sanity checks in~\cite{adebayo2018sanity}.
In our work, we incorporate gradient-based methods into optimization to calculate explanations efficiently.



%-------------------------------------------------------------------------
{\bf Out-of-distribution generalization.}
% Aiming to address the {\it distributional shifts} problems, OOD generalization tasks usually assume the training and testing data follow different distributions whose labels are available.
To generalize machine learning models from training distributions to unseen distributions, existing methods on OOD generalization can be categorized into four branches:
(1) Data augmentation. \cite{volpi2018generalizing, peng2018jointly, shankar2018generalizing, ma2022multimodal} enhance the generalization performance by increasing the diversity of data through augmentation.
(2) Distribution alignment. \cite{li2018domain, bahng2020learning, qiao2023topology} align the features across source distributions in latent space to minimize the distribution gaps.
(3) Meta learning. \cite{li2018learning, dou2019domain, peng2022out, ma2021smil} using meta-learning to facilitate fast-transferable model initialization.
(4) Invariant learning. \cite{arjovsky2019invariant, krueger2021out, peng2017reconstruction} learn invariant representations that are general and transferable to different distributions.
However, recent works~\cite{gulrajani2020search, koh2021wilds} show that the classic empirical risk minimization (ERM)~\cite{vapnik1999nature} method has comparable or even outperforms the aforementioned approaches.
We argue that this is because the existing approaches barely have constraints on explanations, the model would still recklessly absorb any correlations identified in the training data.




%-------------------------------------------------------------------------
{\bf Explanation-guided learning.}
Several recent works have attempted to incorporate explanations into model training to improve predictive performance.
\cite{rieger2020interpretations, stammer2021right} match explanations with human annotations based on domain knowledge, to alleviate the model's reliance on background pixels.
\cite{guo2019visual, wang2020self, pillai2022consistent} align explanations between spatial transformations to improve image classification and weakly-supervised segmentation.
\cite{chen2019robust, han2021explanation, cugu2022attention} synchronize the explanations of the perturbed and original samples to enhance the robustness of models.
However, acquiring ground truth explanations is prohibitively labor-intensive~\cite{wang2020self} or even impossible due to subjectivity in real-world tasks~\cite{roscher2020explainable}.
Furthermore, image transformations are insufficient to address different data types and the general {\it naturally-occurring} distributional shifts, there is no one-to-one correlation between samples from different distributions to provide supervision.
%-------------------------------------------------------------------------



% Different from the aforementioned approaches, our method introduces {\it distributional explanation consistency} constraints to regularize the model training.
% Such regularization plays an important role in narrowing the search space of explanations, {\it i.e.}, the associations that the model relies on to make predictions.
% As the result, our method would significantly alleviate the model's reliance on {\it spurious correlations}, therefore enhancing its generalization capability.
% Furthermore, our explanations are learned utilizing inter-distribution information in a self-supervised manner, no additional annotation is required.
% Probably~\cite{pillai2022consistent} is the closest to our work, since it leverages the self-supervised learning ideas, using affine transformation to generate positive pairs of explanations for contrastive learning.
% However, spatial transformation is only one kind of distributional shift, it can not be generalized to 



