%-------------------------------------------------------------------------
\begin{figure}[t]
\centering
% \fbox{\rule{0pt}{3in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{Figure_Camera_ready/Fig_intro.pdf}

\caption{
The explanations for {\it in-} and {\it out-of-distribution} data of {\it Terra Incognita}~\cite{beery2018recognition} dataset. 
% Camera locations can be viewed as distributions.
Note that GroupDRO~\cite{sagawa2019distributionally} and IRM~\cite{arjovsky2019invariant} are explicitly designed methods that can predict accurately across distributions.
Although with correct predictions, the explanations of models trained by such methods would also highlight distribution-specific associations ({\it e.g.}, tree branches) except the object. 
This leads to unreliable explanations on OOD data.
On the contrary, our model consistently focuses on the most discriminative features shared across distributions.
 % and makes accurate predictions with explanation plausibility.
}
\label{fig:intro}
\end{figure}
%-------------------------------------------------------------------------



%-------------------------------------------------------------------------
\footnotetext[1]{The source code and pre-trained models are available at: \url{https://github.com/tangli-udel/DRE}.}
%-------------------------------------------------------------------------



There has been an increasing trend to apply {\it black-box} machine learning (ML) models for high-stakes applications.
The lack of explainability of models can have severe consequences in healthcare~\cite{rudin2019stop}, criminal justice~\cite{wexler2017computer}, and other domains.
Meanwhile, ML models are inevitably exposed to unseen distributions that lie outside their training space~\cite{torralba2011unbiased, lake2017building};
% a highly accurate model on average can fail catastrophically on {\it out-of-distribution} (OOD) data due to {\it spurious correlations}~\cite{arjovsky2019invariant}.
a highly accurate model on average can fail catastrophically on {\it out-of-distribution} (OOD) data due to naturally-occurring variations, sub-populations, spurious correlations, and adversarial attacks.
% {\it distributional shifts}~\cite{quinonero2008dataset}.
For example, a cancer detector would erroneously predict samples from hospitals having different data acquisition protocols or equipment manufacturers.
Therefore, reliable explanations across distributions are crucial for the safe deployment of ML models.
However, existing works focus on the reliability of data-driven explanation methods~\cite{adebayo2018sanity, zhou2022feature} while ignoring the robustness of explanations against distributional shifts.



A question naturally arises: {\it Are data-driven explanations robust against out-of-distribution data?}
We empirically investigate this problem across different methods. 
Results of the Grad-CAM~\cite{selvaraju2017grad} explanations are shown in Fig.~\ref{fig:intro}.
We find that {\it the distributional shifts would further obscure the decision-making process due to the black-box nature of ML models}.
As shown, the explanations focus not only on the object but also spurious factors ({\it e.g.}, background pixels).
Such distribution-specific associations would yield {\it inconsistent} explanations across distributions.
Eventually, it leads to unreliable explanations ({\it e.g.}, tree branches) on OOD data.
This contradicts with human prior that the most discriminative features ought to be invariant. 



{\it How to develop robust explanations against out-of-distribution data?}
Existing works on OOD generalization are limited to data augmentation~\cite{volpi2018generalizing, qiao2020learning, shankar2018generalizing}, distribution alignment~\cite{ganin2016domain, li2018domain, bahng2020learning}, Meta learning~\cite{li2018learning, dou2019domain, qiao2021uncertainty}, or invariant learning~\cite{arjovsky2019invariant, krueger2021out}.
However, without constraints on explanations, the model would still recklessly absorb all associations found in the training data, including {\it spurious correlations}.
To constrain the learning of explanations, existing methods rely on explanation annotations~\cite{rieger2020interpretations, stammer2021right} or one-to-one mapping between image transforms~\cite{guo2019visual, pillai2022consistent, chen2019robust}.
However, there is no such mapping in general {\it naturally-occurring} distributional shifts.
Furthermore, obtaining ground truth explanation annotations is prohibitively expensive~\cite{wang2020self}, or even impossible due to subjectivity in real-world tasks~\cite{roscher2020explainable}.
To address the aforementioned limitations, we propose an end-to-end model-agnostic training framework {\it Distributionally Robust Explanations (DRE)}.
The key idea is, inspired by self-supervised learning, to fully utilize the inter-distribution information to provide supervisory signals for explanation learning.



{\it Can robust explanations benefit the model's generalization capability?}
We evaluate the proposed methods on a wide range of tasks in Sec.~\ref{sec:exp}, including the classification and regression tasks on image and scientific tabular data.
Our empirical results demonstrate the robustness of our explanations.
The explanations of the model trained via the proposed method outperform existing methods in terms of explanation consistency, fidelity, and scientific plausibility.
The extensive comparisons and ablation studies prove that our robust explanations significantly improve the model's prediction accuracy on OOD data.
As shown, the robust explanations would alleviate the model's excessive reliance on {\it spurious correlations}, which are unrelated to the causal correlations of interest~\cite{arjovsky2019invariant}.
Furthermore, the enhanced explainability can be generalized to a variety of data-driven explanation methods.




In summary, our main contributions:
\begin{itemize}
  \item We comprehensively study the robustness of data-driven explanations against naturally-occurring distributional shifts.

  \item We propose an end-to-end model-agnostic learning framework Distributionally Robust Explanations (DRE). 
  It fully utilizes inter-distribution information to provide supervisory signals for explanation learning without human annotations.

  \item Empirical results in a wide range of tasks including classification and regression on image and scientific tabular data demonstrate superior explanation and prediction robustness of our model against OOD data.
\end{itemize}


