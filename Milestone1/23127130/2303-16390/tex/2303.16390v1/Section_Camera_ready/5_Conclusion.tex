In this paper, we present comprehensive study to show that the data-driven explanations are not robust against out-of-distribution data.
To address this problem, we propose a an end-to-end model-agnostic learning framework Distributionally Robust Explanation (DRE).
The proposed method fully utilizes inter-distribution information to provide supervisory signals for explanation learning without human annotation.
We conduct extensive experiments on wide range of tasks, including the classification and regression tasks on image and scientific tabular data.
Our results demonstrate the superior of our method in terms of explanation and prediction robustness against {\it out-of-distribution} data. 
We anticipate using our robust model with solid justifications in the next efforts for further scientific knowledge discovery.