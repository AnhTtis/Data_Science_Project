\subsection{Robustness against Out-of-distribution Data}
Question: {\it Are data-driven explanations robust against out-of-distribution data?}



We empirically investigate this problem on an out-of-distribution generalization benchmark image dataset {\it Terra Incognita}~\cite{beery2018recognition} and a scientific tabular dataset {\it Urban Land}~\cite{gao2020mapping}.
For image data, we leverage the Grad-CAM~\cite{selvaraju2017grad} method to generate explanations, and leverage explanation fidelity~\cite{Petsiuk2018rise} as the metric to measure explanation quality.
Specifically, we evaluate ERM~\cite{vapnik1999nature} and two representative out-of-distribution generalization methods, GroupDRO~\cite{sagawa2019distributionally} and IRM~\cite{arjovsky2019invariant}.
For scientific tabular data, we leverage the Input Gradient~\cite{simonyan2013deep} method to generate explanations, and leverage scientific consistency (Sec.~\ref{sec:metric}) as the metric to measure explanation quality.
Qualitatively, the more explanation plausibility on OOD data, the better the explanation robustness.
Quantitatively, the higher the explanation fidelity or scientific consistency on OOD data, the better the explanation robustness.



For qualitative evaluation, we observe that even with correct predictions, the explanations on out-of-distribution data might still focus on spurious correlations. 
Fig.~\ref{fig:intro} shows the Grad-CAM explanations of models trained via ERM, GroupDRO, and IRM.
As shown, the explanations on in-distribution data would not only highlight the object, but also distribution-specific associations ({\it e.g.}, background pixels).
This eventually leads to unreliable explanations on OOD data ({\it e.g.}, tree branches).
We find that the out-of-distribution generalization methods perform even worse than the classic ERM in terms of explanation robustness.
This finding verify the results in \cite{gulrajani2020search, koh2021wilds} that ERM outperforms the out-of-distribution generalization methods from an explanation perspective.
For quantitative evaluation, we empirically verify that the explanation fidelity and scientific consistency severely dropped on OOD data.
Tab.~\ref{tab:question_1} reports the evaluation results, as shown, on the average of each distribution as the testing set, the explanation fidelity dropped by 24.9\%, 20.8\%, and 15.0\% for ERM, GroupDRO, and IRM on {\it Terra Incognita};
the scientific consistency dropped by 32.4\% for ERM on {\it Urban Land}.



To conclude, {\it data-driven explanations are not robust again out-of-distribution data.}
The distributional shifts further obscure the decision-making process due to the {\it black-box} nature of modern ML models.
% The models trained via existing methods would yield {\it inconsistent} explanations across distributions, which contradict with human prior that the most discriminative features should be invariant. 



%-------------------------------------------------------------------------
\begin{table}[t]
\centering
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{llccc}
\toprule[1pt]
\multirow{2}{*}{Dataset}         & \multirow{2}{*}{Method} & \multicolumn{2}{c}{Evaluation*} & \multirow{2}{*}{$\Delta$ $\downarrow$} \\ \cline{3-4}
                                 &                         & ID             & OOD            &                                        \\ \toprule[1pt]
\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Terra\\ Incognita~\cite{beery2018recognition}\end{tabular}} & ERM~\cite{vapnik1999nature}                     & 0.778          & 0.584          & {\bf 0.194}                                  \\
                                 & GroupDRO~\cite{sagawa2019distributionally}                & 0.742          & 0.597          & 0.145                                  \\
                                 & IRM~\cite{arjovsky2019invariant}                     & 0.612          & 0.520          & 0.092                                  \\ \toprule[0.5pt]
Urban Land~\cite{gao2020mapping}                       & ERM~\cite{vapnik1999nature}                     & 0.535          & -0.113         & {\bf 0.648}                                  \\ \bottomrule[1pt]
\end{tabular}
}
\caption{
Evaluation of the explanation quality of {\it in-distribution} (ID) and {\it out-of-distribution} data.
The results are on the average of each distribution as the testing set.
Note that the explanation quality of both image and scientific data severely dropped on OOD data.
*The reported evaluation scores are explanation fidelity~\cite{Petsiuk2018rise} ({\it Terra Incogenita}) and scientific consistency ({\it Urban Land}).
}
\label{tab:question_1}
\end{table}
%-------------------------------------------------------------------------









%-------------------------------------------------------------------------
\begin{figure*}[t]
\centering
% \fbox{\rule{0pt}{3in} \rule{0.9\linewidth}{0pt}}
\includegraphics[width=0.9\linewidth]{Figure_Camera_ready/Fig_overview.pdf}
\caption{
{\bf Overview of the proposed Distributionally Robust Explanations (DRE) method.}
Our method consists of distributional explanation consistency constraint and explanation sparsity constraint.
Firstly, we load a batch of random samples from different distributions and feed them into the model to calculate the standard task-specific objective ({\it e.g.}, Cross-entropy loss for classification tasks).
Then, we calculate the explanations of each sample {\it w.r.t.} their predictions and constrain the explanation sparsity.
Next, we pair up the samples with the same prediction but from different distributions, mixing up both samples and explanations using the same parameters.
We feed the mixed samples into the model and calculate the explanations {\it w.r.t.} their shared predictions.
Finally, we constrain the consistency between the explanation of mixed samples and the mixed explanations.
This figure is best viewed in color.
}
\label{fig:overview}
\end{figure*}
%-------------------------------------------------------------------------



\subsection{Distributionally Robust Explanations}
Question: {\it How to develop robust explanations against out-of-distribution data?}

We started by providing the formulation of the problem, then introduce a new framework {\it Distributionally Robust Explanations} (DRE) for explanation learning.



\subsubsection{Problem formulation}
\label{sec:problem_formulation}
% Typical supervised learning is defined by a pair of random variables $(X,Y)$ following an unknown joint probability distribution $P(X,Y)$.
The objective of typical supervised learning is to learn a predictor $f \in \mathcal{F}$ such that $f(x)\rightarrow y$ for any $(x, y) \sim P(X,Y)$, where $P(X,Y)$ is an unknown joint probability distribution, and $\mathcal{F}$ is a function class that is model-agnostic for a prediction task.
However, in the scenario of {\it out-of-distribution} generalization, one can not sample directly from $P(X,Y)$ due to distributional shifts.
Instead, it is assumed that we can only measure $(X, Y)$ under different environmental conditions $e$ so that data is drawn from a set of groups $\mathcal{E}_\mathrm{all}$ such that $(x, y) \sim P_e(X,Y)$.
For example, in the cancer detection task, the environmental conditions denote the latent factors ({\it e.g.}, data acquisition protocols or equipment manufacturers) that underlie different hospitals.



Informally, this assumption specifies that there should exist a function $G$ that relates the random variables $X$ and $X^e$ via $G(X,e) = X^e$.
Let $\mathcal{E}_\mathrm{train} \subsetneq \mathcal{E}_\mathrm{all}$ be a finite subset of training groups (distributions), given the task-specific objective function $\ell$ and explanation method $g(\cdot)$, our {\it Distributionally Robust Explanations} is equivalent to the following constrained problem:
%-------------------------------------------------------------------------
\begin{equation}
\tag{DRE}
\begin{aligned}
& \mathop{\mathrm{min}}\limits_{f\in \mathcal{F}} \quad \mathcal{R}(f) : = \mathbb{E}_{(x,y)\sim P_\mathrm{train}} [\ell(f(x), y)] \\
& \; \mathrm{s.t.} \quad \;\; g(x) = g(G(x, e)) \quad \forall e \in \mathcal{E}_\mathrm{all}.  
\end{aligned}
\label{eq:DRE}
\end{equation}
%-------------------------------------------------------------------------
Intuitively, we encourage the model to have invariant explanations for a sample under different environmental conditions (distributions) after optimization.

The problem in \ref{eq:DRE} is challenging to solve, since we do not have access to the set of all distributions $\mathcal{E}_\mathrm{all}$ or the underlying distribution $P(X,Y)$, and to the distribution transformation function $G$.
The alternative solutions would be:
(i) acquire the ground truth explanations for all samples;
(ii) obtain the one-to-one mapping between samples from different distributions, such as original sample and its corresponding corrupted ones~\cite{hendrycks2018benchmarking}.
However, as our discussion in Sec.~\ref{sec:relate}, the ground truth explanations and one-to-one mappings are practically unavailable in real-world tasks.



\subsubsection{Distributional Explanation Consistency}
\label{sec:DEM}
We address these challenges by explicitly designing the {\it distributional explanation consistency}.
The key idea is, inspired by self-supervised learning, leveraging the mixed explanation to provide supervisory signals for the learning of explanations.
Specifically, we first leverage the {\it distributional mixup} to achieve a simple but effective inter-distributional transformation. 
The {\it mixup}~\cite{zhang2018mixup} methods have been empirically shown to substantially improve performance and robustness to adversarial noise~\cite{qiao2021uncertainty, xu2020adversarial}.
In contrast to original {\it mixup} that mixes random pairs of samples and labels, we mix up samples with the same ground truth but from different distributions.
Denote $(\mathrm{\bf x}_e, \mathrm{\bf x}_{e'})$ as random pairs of training samples with the same ground truth $\mathrm{\bf y}$ but from different distributions, then our {\it mixup} operator can be defined as:
%-------------------------------------------------------------------------
\begin{equation}
  \mathcal{M} (\mathrm{\bf x}_e, \mathrm{\bf x}_{e'}) = \tau  \mathrm{\bf x}_e + (1-\tau  )\mathrm{\bf x}_{e'}
  \label{eq:input_mixup}
\end{equation}
%-------------------------------------------------------------------------
where $\tau \sim \mathrm {Beta}(\alpha, \alpha)$ and the {\it mixup} hyper-parameter $\alpha \in (0, +\infty)$ controls the interpolation strength, in practice we use $\alpha=0.2$.
Secondly, we {\it mixup} the explanations of the samples using the same parameters, namely:
%-------------------------------------------------------------------------
\begin{equation}
  \mathcal{M} (g(\mathrm{\bf x}_e), g(\mathrm{\bf x}_{e'})) = \tau  g(\mathrm{\bf x}_e) + (1-\tau  )g(\mathrm{\bf x}_{e'})
  \label{eq:exp_mixup}
\end{equation}
%-------------------------------------------------------------------------
Thirdly, we feed the mixed samples into the model to calculate the explanations $g(\mathcal{M}(\mathrm{\bf x}_e, \mathrm{\bf x}_{e'}))$.
Lastly, denote $\mathcal{D}$ as an arbitrary discrepancy metric, for example, $\ell_1$ distance and KL-divergence~\cite{kullback1951information}, for all $e,e' \in \mathcal{E}_\mathrm{train}$, we leverage the consistency between the mixed explanation and the explanation of the mixed sample as an alternative of~\ref{eq:DRE}:
%-------------------------------------------------------------------------
\begin{equation}
\mathop{\mathrm{min}}\limits_{f\in \mathcal{F}} \mathcal{R}(f) \;
\mathrm{s.t.} \; \mathcal{D} [g(\mathcal{M}(\mathrm{\bf x}_e, \mathrm{\bf x}_{e'})), \mathcal{M}(g(\mathrm{\bf x}_e), g(\mathrm{\bf x}_{e'}))] \le \epsilon 
\label{eq:L_con}
\end{equation}
%-------------------------------------------------------------------------
Intuitively, the mixed explanation serves as the pseudo label to guide the learning of the explanation for the mixed sample.
Note that $g(\cdot)$ is not restrictive, including any gradient-based explanation methods, such as Grad-CAM~\cite{selvaraju2017grad} and Input Gradient~\cite{simonyan2013deep}.
We leverage Karush–Kuhn–Tucker conditions~\cite{boyd2004convex} and introduce a Lagrange multiplier $\lambda$ to convert the constrained problem in Eq.~\ref{eq:L_con} into its unconstrained counterpart:
%-------------------------------------------------------------------------
\begin{equation}
\begin{aligned}
\mathop{\mathrm{min}}\limits_{f\in \mathcal{F}} \{& \mathcal{R}_\mathrm{con} (f) := \; \mathbb{E}_{(x,y)\sim P_\mathrm{train}} [\ell(f(x), y)] \\
                                                  & + \lambda \mathcal{D} [g(\mathcal{M}(\mathrm{\bf x}_e, \mathrm{\bf x}_{e'})), \mathcal{M}(g(\mathrm{\bf x}_e), g(\mathrm{\bf x}_{e'}))]\}
\end{aligned}
\label{eq:L_con_lagrange}
\end{equation}
%-------------------------------------------------------------------------




{\bf Explanation Regularization}
Our empirical results (Tab.~\ref{tab:ablation}) show that recklessly optimizing Eq.~\ref{eq:L_con_lagrange} could easily fall into a local minimum.
For example, explanations that evenly attribute the prediction to all features in the sample would be a trivial solution to satisfy the constraint.
To address this problem, we propose to further regularize the $\ell_1$-norm of the explanations. 
Let $\gamma$ be a dual variable, our overall objective is formulated as follows:
%-------------------------------------------------------------------------
\begin{equation}
\mathop{\mathrm{min}}\limits_{f\in \mathcal{F}} \{\mathcal{R}(f) := \mathcal{R}_\mathrm{con} (f)
+ \gamma [|| g(\mathrm{\bf x}_e) ||_{1} + || g(\mathrm{\bf x}_{e'}) ||_{1}] \}
\label{eq:L_reg}
\end{equation}
%-------------------------------------------------------------------------



The entire training pipeline is summarized in Alg.~\ref{alg:DEC}.
Our {\it Distributional Robust Explanations} has the following merits.
First, in contrast to existing works that rely on expensive explanation annotations~\cite{rieger2020interpretations, stammer2021right} or one-to-one mapping between image transforms~\cite{guo2019visual, wang2020self, pillai2022consistent}, the proposed method provides a supervisory signal for the explanation learning in general distributional shifts.
Second, the proposed method fully utilizes the inter-distribution information which guarantees distributional invariance in a more continuous latent space.
Third, the proposed method does not involve additional parameters, it can be considered as an add-on module for training a robust model without changing the model architecture.
This enables DRE for robust explanation and prediction against distributional shifts as shown in Sec.~\ref{sec:exp}.



%-------------------------------------------------------------------------
\begin{algorithm}[t]
\caption{The proposed Distributionally Robust Explanations (DRE).}
\LinesNumbered
\label{alg:DEC}
\KwIn{Data of $\mathcal{E}_\mathrm{train}$; Step size $\eta$}
\KwOut{Learned model parameters $\theta$}
\While{not converged}{
     Sample $(\mathrm{\bf x}_e, \mathrm{\bf y})\sim P_e(X,Y)$  $\; \forall e \in \mathcal{E}_\mathrm{train}$ \\
     Sample $(\mathrm{\bf x}_{e'}, \mathrm{\bf y})\sim P_{e'}(X,Y)$  $\; \forall e' \in \mathcal{E}_\mathrm{train}$ \\
     Calculate $\mathcal{R}(f)$ via Eq.~\ref{eq:L_reg} \\
     Update $\theta$ via $\theta^{t+1} = \theta^{t} - \eta^{t}\nabla \mathcal{R}(f) $
}
\end{algorithm}
%-------------------------------------------------------------------------




\subsection{Improve Model's Generalization Capability}
Question: {\it Can robust explanations benefit the model's generalization capability?}


We empirically evaluate this problem on two out-of-distribution generalization benchmark image datasets ({\it Terra Incognita}~\cite{beery2018recognition} and {\it VLCS}~\cite{fang2013unbiased}) and a scientific tabular dataset ({\it Urban Land}~\cite{gao2020mapping}).
Tab.~\ref{tab:question_3} shows the results of prediction performance on OOD data.
For image datasets, on average of each distribution as the testing set, our method outperforms the ERM~\cite{vapnik1999nature} results by 6.9\% and 2.0\% in terms of prediction accuracy.
For scientific tabular data, on average of each continental region as the testing set, our method significantly outperforms the baseline results by 18.5\% in terms of the prediction residual.



Therefore, {\it the robust explanations against OOD data can benefit the model's generalization capability}.
Recklessly minimizing training error would leads machines to absorb all the correlations found in training data~\cite{arjovsky2019invariant}.
We argue that our robust explanations alleviate the excessive reliance of the model on {\it spurious correlations}.
Our method constrains the model to rely on invariant causal correlations and leads to better generalization capability.


%-------------------------------------------------------------------------
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[t]
\centering
\begin{tabular}{llc}
\toprule[1pt]
Dataset                          & Method     & Evaluation* \\ \toprule[1pt]
\multirow{2}{*}{Terra Incognita~\cite{beery2018recognition}} & ERM        & 46.1\%      \\
                                 & DRE (ours) & {\bf 53.0\%}     \\ \toprule[0.5pt]
\multirow{2}{*}{VLCS~\cite{fang2013unbiased}}            & ERM        & 77.5\%      \\
                                 & DRE (ours) & {\bf 79.5\%}      \\ \toprule[0.5pt]
\multirow{2}{*}{Urban Land~\cite{gao2020mapping}}      & ERM        & 9.70e-4     \\
                                 & DRE (ours) & {\bf 7.91e-4}     \\ \bottomrule[1pt]
\end{tabular}
\caption{
Evaluation of predictive performance on {\it out-of-distribution} data.
The results are on the average of each distribution as the testing set.
Note that our method outperforms ERM on {\it Terra Incogenita}, {\it VLCS}, and {\it Urban Land}.
*The reported evaluation scores are Accuracy ({\it Terra Incogenita}, {\it VLCS}) and Prediction Residual ({\it Urban Land}).
}
\label{tab:question_3}
\end{table}
%-------------------------------------------------------------------------