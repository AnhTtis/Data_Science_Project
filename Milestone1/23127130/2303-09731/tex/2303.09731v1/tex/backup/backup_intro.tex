\section{Introduction}\label{sec:Introduction}
Due to the rapid development of deep learning, several safety-critical applications have adopted deep neural networks (DNNs) to improve their performances \cite{edward2016doctorai,jiaxuan2019anAutomated}. As one of the safety-critical applications, autonomous vehicles also benefit from it by deploying different DNNs in their self-driving systems, e.g., implementing object detectors in the perception modules to predict the category and the location of each object around autonomous vehicles \cite{yulong2019advObj,gregory2019lasernet}. Because the point clouds (PCs) generated by LiDARs (Light Detection and Ranging) contains richer location information than the images generated by cameras, most existing self-driving systems, such as Baidu's Apollo Go\footnote{https://apollo.auto/} and Google's Waymo One\footnote{https://waymo.com/waymo-one/}, set LiDARs as the main sensors and rely on the predictions of \textit{3D object detectors} \cite{alex2019pointpillars,shaoshuai2019pointrcnn,shaoshuai2020pvrcnn,gregory2019lasernet}, one kind of models which take PCs as models' inputs and predicts the category and the location of each object nearby.

However, recent works find that these LiDAR-based object detectors in autonomous vehicles are vulnerable to some \textit{adversarial attacks} \cite{yulong2019advObj,kaichen2021robust}, which changes the predicted category or location of 3D object detectors by perturbing original points or adding new points in the input PCs, i.e. appearing attacks. Appearing attacks aim at forging inexistent vehicles in detectors' predictions, which may cause the emergency braking, and further lead to some serious threats such as the traffic jam or the crash accident in the real world  \cite{hocheol2017illusion,yulong2019advSensor,jiachen2020towards}. Meanwhile, existing defense methods have limitations to defend against most existing appearing attacks, which will be introduced in Sec.\ref{sec:Limitation} in detailed.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\columnwidth]{figs/physical_equipment.png}
    \caption{The physical equipment for existing appearing attacks.}
    \label{fig:appearing_attack_equipment}
\end{figure}

\noindent\textbf{Our Work.} To generally defend against existing appearing attacks, we first summarize two important properties of existing appearing attacks. On the one hand, the limitations below restrict the effect of appearing attacks in the real world, and behind them there is the inimitable relation between the depth and the point density of real vehicles. In terms of the attacker's goal, the fake vehicles created by appearing attack have to be located nearby the victim autonomous vehicle. Otherwise, the victim will have enough time to re-route and bypass the fake vehicles. Consequently, the appearing attack may not lead to the real-world threats as the attacker wishes. Meanwhile, constraint to the attacker's physical equipment, which is a combination of a photodiode, a delay component, a laser transmitter and a lens shown in Fig. \ref{fig:appearing_attack_equipment}, the attacker can only forge limited amounts of points for appearing attack during one scan of the LiDAR. Therefore, it is impossible for the attackers fully copy the PC corresponding to a complete vehicle, and parse it nearby the victim by firing laser pulses.

On the other hand, the existing appearing attacks focus on increasing the objectness scores, which will directly affect the existence of objects in the 3D detector's predictions, of the fake vehicles. In other words, appearing attacks only imitate the fake vehicles as similar as the real vehicles in general, without caring about the local differences between them. Therefore, there remains distinguishable difference between the fake vehicles and the real vehicles in the local.

In this paper, we propose a local objectness predictor to predict the objectness scores, which represent the probability of a part of an object inside, for the divided, equal-sized space, e.g., voxels or pixels, of input space. We can thus distinguish the fake vehicles and the real vehicles by these objectness scores.

However, the depth-density relation is hard to model due to its complexity. For example, most of objects with smaller depth have larger point density, while those objects blocked by others have smaller depth and point density at the same time. Consequently, we re-generate input PCs’ feature vectors by explicitly embedding the depth of each point, which substantially improve the modeling capability compared with using the original input feature. Although, there are some defense methods such as CARLO and SVF \cite{jiachen2020towards} which are also proved effective by experiments, we argue that they are in essence heuristic algorithms based on point density. First, they are designed for one or several specific appearing attacks, while the generalization of them remains unproved. Besides, they haven't explicitly made use of the depth to distinguish real objects and fake objects.

Meanwhile, in the context of object detection, there is no explicit annotation for supervising the training of objectness detector. However, Chen et al. suggest that one single part of input space has already contained rich semantic information for a PC model to predict its related object's category and location \cite{qi2020object}. Inspired by their work, we construct the supervision task by taking the features of input points inside a pillar as input, and whether the pillar intersects with any bounding box of real objects as $0/1$ label. Technically, we use our local objectness predictor to predict objectness score for each pillar intersected with a predicted object's bounding box, and determine whether the object is real by voting of these objectness scores. 

Furthermore, we reveal that our local objectness predictor is model-agnostic: the performance of our local objectness predictor is robust to the change of the structure of itself or the structure of the 3D object detector. From this perspective, our local objectness predictor is more effective than the traditional defense methods.

To understand the impact of our defense, we further implement most existing appearing attacks against three start-of-the-art 3D object detectors: PointPillars \cite{alex2019pointpillars}, PointRCNN \cite{shaoshuai2019pointrcnn} and PV-RCNN \cite{shaoshuai2020pvrcnn}. We compare their performance with and without our local objectness predictors, and find that: 
%with appearing attack, the detectors' precision can increase from $[data0]!!!$ to $[data1]!!!$ with our our local objectness predictors, while the detectors' precision can also increase from $[data2]!!!$ to $[data3]!!!$ with our our local objectness predictors in an un-attack situation.
with our our local objectness predictors, the 3D object detectors' precision on vehicles can increase from $28\sim66\%$ to $60\sim74\%$ under attack, while their precision on vehicles also increase from $73\sim79\%$ to $77\sim81\%$ in normal circumstances.
The results validate that our local objectness predictors can improve the robust of 3D object detectors to appearing attack without harming their normal performance.

% data0->不防御的情况下，受攻击的3D实体检测模型的precision范围
% data1->防御的情况下，受攻击的3D实体检测模型的precision范围
% data2->不防御的情况下，3D实体检测模型的正常precision范围
% data3->防御的情况下，3D实体检测模型的正常precision范围

\noindent\textbf{Our Contributions.} In summary, we mainly make the following key contributions:
\begin{itemize}
    \item We systematically inspect the existing appearing attacks and observe their common limitations. On the one hand, existing appearing attacks can't forge the relation between the depth and the point density of real vehicles. On the other hand, they mainly aim at increasing the confidence scores of the whole fake vehicles, regardless of the difference in small separated parts.
    \item Based on the discovery above, we propose a local objectness predictor as a general defense method for existing appearing attacks. Our local objectness predictor learns the depth-density relation of real objects, and discriminate the small parts of fake objects and real objects by predicting their objectness scores.
    \item We evaluate our defense against most existing appearing attacks on many start-of-the-art 3D object detectors, which strongly validates that our defense is efficient without side effects. For example, our local objectness predictors can increase a 3D object detectors' precision in normal circumstances or under attack, regardless of the structure of the 3D object detector.
\end{itemize}