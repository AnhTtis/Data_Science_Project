\section{Experiments} \label{sec:Experiment}
In this section, we deploy our local objectness predictor on three state-of-the-art 3D object detectors to evaluate their performances in normal circumstances \louis{as well as} under appearing attacks. Besides, we further compare the defense effect of our local objectness predictor with other existing defenses against appearing attacks. Below, we describe the setup of experiments and report the results.

\subsection{Setup}\label{sec:Experiment:setup}
\paragraph{Victim 3D Object Detectors} We choose Pointpillars \cite{alex2019pointpillars}, PointRCNN \cite{shaoshuai2019pointrcnn} and PV-RCNN \cite{shaoshuai2020pvrcnn} as the victim 3D object detectors. We adopt the \louis{pre-trained} 3D object detectors, which are trained on the KITTI dataset \cite{andreas2012kitti}, supported by OpenPCDet\footnote{https://github.com/open-mmlab/OpenPCDet/}, an open-source testing platform for 3D object detection. Because these 3D objects detectors may return some objects with a low confidence score in prediction, which may significantly injure their performance, we only consider the objects with confidence scores larger than a threshold $C$ in their prediction to keep their precision in a normal level. For Pointpillars and PointRCNN, we set $C=0.5$; for PV-RCNN, we set $C=0.7$.

\paragraph{Attack Methods} We implement following three different appearing attacks to deceive the victim 3D object detectors:
\begin{itemize}
    \item \textbf{the Variant of Adv-LiDAR \cite{yulong2019advSensor}} Because Adv-LiDAR is specially designed for the LiDAR-based 3D object detector of Baidu's Apollo, it is unsuitable to directly implement it to attack other 3D object detectors. Thus, we follow Adv-LiDAR's idea and propose the variant of ADV-LiDAR: randomly inject a certain amount of the points into a specified zone, and use FGSM to increase the confidence scores of those fake object related to these points.
    \item \textbf{Sun's Work \cite{jiachen2020towards}} This attack forge objects by duplicating the real objects with limited points and transforming them to the position near the victim autonomous vehicle. 
    \item \textbf{Yang's Work \cite{kaichen2021robust}} This attack forge objects by increasing the confidence scores of fake objects and enlarging their bounding boxex with gradient descent  methods \footnote{In the original paper, this attack mainly aim at PointRCNN in the white-box scenario. We follow this settings in our experiments.}.
\end{itemize}

\paragraph{Other Defense Methods} \louis{Besides} our LOP, we also implement SRS, SOR and CARLO as comparison. The details of these defense methods are introduced in Sec.\ref{sec:Limitation}. Especially, due to the large time cost and computation of deploying SVF and re-training the victim 3D object detectors, we do not evaluate the effect of SVF in the experiments. Besides, CARLO and SVF are specially designed for the same appearing attack, thus we believe the results of CARLO can be representative of SVF in a certain extent.

\subsection{Results}\label{sec:Experiment:results}
\subsubsection{Without Attacks}\label{sec:Experiment:results:woAtk}
\noindent\textbf{Experiment Settings.} We first evaluate the performance of our LOP in normal circumstance. We implement our LOP on three state-of-the-art 3D object detectors, and evaluate their precision on the validating samples of KITTI dataset. Without loss of generality, we choose two different point-wise PC classifier, PointNet and DGCNN, as the our LOP, and choose five different values as the boundary values $B$ of our LOP. Table \ref{tab:normal_pre} presents the precision of these 3D object detectors in normal circumstance.

\begin{table*}[htbp]
  \centering
  \caption{The precision of 3D object detectors with and without LOP on clean samples. "PointNet" and "DGCNN" refers to the model structure of our LOP, and $B$ is the boundary value of our LOP, which is used to distinguish real objects and forged objects as the description in Sec.\ref{sec:method:voting}.}
\scalebox{0.9}{
    \begin{tabular}{c|ccc|ccc|ccc}
    \toprule
    % Model 
    & \multicolumn{3}{c|}{PointPillar} & \multicolumn{3}{c|}{PointRCNN} & \multicolumn{3}{c}{PV-RCNN} \\
    \cmidrule{2-10}
    % Object Type 
    & Car   & pedestrian & Cyclist & Car   & pedestrian & Cyclist & Car   & pedestrian & Cyclist \\
    \midrule
    None  & 78.99\% & 63.63\% & 59.11\% & 75.04\% & 47.08\% & 56.87\% & 73.12\% & 53.34\% & 62.17\% \\
    \midrule
    % Ours(PointNet, B=0.2) & 80.12\% & 63.99\% & 60.09\% & 76.49\% & 50.70\% & 58.66\% & 74.83\% & 54.14\% & 62.91\% \\
    % Ours(PointNet, B=0.3) & 80.46\% & 64.00\% & 59.91\% & 77.15\% & \textbf{50.94\%} & 58.93\% & 75.49\% & \textbf{54.16\%} & 63.30\% \\
    Ours(PointNet, B=0.4) & 81.04\% & \textbf{64.27\%} & 61.34\% & 78.05\% & \textbf{50.49\%} & 61.59\% & 76.50\% & \textbf{53.80\%} & 63.17\% \\
    Ours(PointNet, B=0.5) & 81.77\% & 63.45\% & 61.50\% & 79.29\% & 49.63\% & 61.92\% & 77.85\% & 53.13\% & 63.13\% \\
    Ours(PointNet, B=0.6) & \textbf{82.38\%} & 63.47\% & \textbf{62.11\%} & \textbf{80.03\%} & 49.43\% & \textbf{63.87\%} & \textbf{78.53\%} & 53.00\% & \textbf{64.99\%} \\
    \midrule
    % Ours(DGCNN, B=0.2) & 80.88\% & 64.71\% & 60.44\% & 77.56\% & 51.04\% & 61.03\% & 76.43\% & \textbf{54.40\%} & 63.94\% \\
    % Ours(DGCNN, B=0.3) & 81.41\% & 64.42\% & 60.51\% & 78.36\% & \textbf{51.31\%} & 61.72\% & 77.04\% & 51.16\% & 64.35\% \\
    Ours(DGCNN, B=0.4) & 82.20\% & \textbf{64.73\%} & 63.56\% & 79.44\% & \textbf{51.21\%} & 63.74\% & 78.30\% & \textbf{53.77\%} & 64.93\% \\
    Ours(DGCNN, B=0.5) & 83.04\% & 64.42\% & 64.89\% & 80.75\% & 49.59\% & 65.14\% & 79.61\% & 52.49\% & 66.80\% \\
    Ours(DGCNN, B=0.6) & \textbf{83.90\%} & 64.44\% & \textbf{66.45\%} & \textbf{81.52\%} & 49.42\% & \textbf{67.24\%} & \textbf{80.34\%} & 52.43\% & \textbf{68.95\%} \\
    \bottomrule
    \end{tabular}
}
  \label{tab:normal_pre}
\end{table*}

\noindent\textbf{Result \& Analysis.} As we can see from Table \ref{tab:normal_pre}, these three 3D objectors detectors all benefit from our LOP: their precision on cars increase $1.13\sim7.12\%$ with our LOP, and their precision on cyclists increase $0.74\sim10.37\%$ with our LOP. Though their precision on pedestrians may degrade due to our LOP, the scale of degradation is at most $2.18\%$, which is far less than the improvement LOP bring in other aspects. We believe that there are two reasons caused the degradation of precision on pedestrians. On the one hand, the point density and the depth of pedestrians are usually less than cars and cyclists. It increases the difficulty of the depth-density relation modeling, which is the key to the performance of our LOP. On the other hand, the performance of 3D object detectors on pedestrians are usually worse than their performance on cars and cyclists due to the difficulty of recognizing small objects, which increases the difficulty of distinguish real objects and forged objects for our LOP. Besides, existing appearing attacks focus on forging car, so it is not necessary to implement LOP to eliminate forged pedestrians. As a summary, our LOP can increase the performance of 3D object detectors in most aspects, and there also exists valid way to avoid the latent damage caused by our LOP.

\subsubsection{Under Attacks}\label{sec:Experiment:results:uAtk}
\noindent\textbf{Experiment Settings.} Next, we evaluate the performance of our LOP against existing appearing attacks. We follow the settings in Sec.\ref{sec:Experiment:results:woAtk}, and implement three existing appearing attacks to generate adversarial samples based on the validating samples of KITTI dataset. We then evaluate the attack success rate (ASR) of forged objects and the precision of the 3D object detectors on these adversarial samples \footnote{Besides the forged objects crafted by appearing attacks, there also remains some normal objects in adversarial samples.}. Table \ref{tab:attack_asr} shows the ASR of three existing appearing attacks on 3D object detectors, and Table \ref{tab:attack_pre} presents the precision of three different 3D object detectors under existing appearing attacks.

\begin{table*}[htbp]
  \centering
  \caption{The precision of 3D object detectors with and without LOP on adversarial samples. "None" refers to no attack, "Physical" refers to the black-box appearing attack proposed in Sun's work, "FGSM" refers to the variant of Adv-LiDAR , and 'Roadside' refers to the white-box appearing attack proposed in Yang's work. Here, three attacks are described in Sec.\ref{sec:Experiment:setup} in details.}
\scalebox{0.9}{
    \begin{tabular}{c|ccc|cccc|ccc}
    \toprule
          & \multicolumn{3}{c}{PointPillar} & \multicolumn{4}{c|}{PointRCNN} & \multicolumn{3}{c}{PV-RCNN} \\
\cmidrule{2-11}          & None  & Physical & FGSM  & None  & Physical & FGSM  & Roadside & None  & Physical & FGSM \\
    \midrule
    None  & 78.99\% & 71.89\% & 66.61\% & 75.04\% & 61.82\% & 55.27\% & 50.47\% & 73.12\% & 64.06\% & 28.08\% \\
    \midrule
    % Ours(PointNet, B=0.2) & 80.12\% & 73.51\% & 69.06\% & 76.49\% & 64.65\% & 66.72\% & 62.65\% & 74.83\% & 66.11\% & 47.52\% \\
    % Ours(PointNet, B=0.3) & 80.46\% & 74.31\% & 70.12\% & 77.15\% & 66.40\% & 68.77\% & 65.32\% & 75.49\% & 67.35\% & 50.23\% \\
    Ours(PointNet, B=0.4) & 81.04\% & 75.79\% & 72.08\% & 78.05\% & 69.14\% & 71.33\% & 68.80\% & 76.50\% & 69.20\% & 54.58\% \\
    Ours(PointNet, B=0.5) & 81.77\% & 77.65\% & 74.60\% & 79.29\% & 73.09\% & 74.33\% & 72.87\% & 77.85\% & 72.82\% & 60.33\% \\
    Ours(PointNet, B=0.6) & \textbf{82.38\%} & \textbf{79.23\%} & \textbf{76.08\%} & \textbf{80.03\%} & \textbf{75.45\%} & \textbf{76.13\%} & \textbf{74.99\%} & \textbf{78.53\%} & \textbf{74.69\%} & \textbf{61.17\%} \\
    \midrule
    % Ours(DGCNN, B=0.2) & 80.88\% & 74.33\% & 69.91\% & 77.56\% & 64.84\% & 69.06\% & 65.58\% & 76.43\% & 67.07\% & 51.25\% \\
    % Ours(DGCNN, B=0.3) & 81.41\% & 75.47\% & 71.30\% & 78.36\% & 66.91\% & 71.23\% & 68.45\% & 77.04\% & 68.46\% & 54.11\% \\
    Ours(DGCNN, B=0.4) & 82.20\% & 77.23\% & 73.51\% & 79.44\% & 69.85\% & 73.86\% & 72.28\% & 78.30\% & 71.00\% & 58.57\% \\
    Ours(DGCNN, B=0.5) & 83.04\% & 79.18\% & 76.10\% & 80.75\% & 73.83\% & 76.60\% & 75.83\% & 79.61\% & 74.19\% & 63.49\% \\
    Ours(DGCNN, B=0.6) & \textbf{83.90\%} & \textbf{80.83\%} & \textbf{78.08\%} & \textbf{81.52\%} & \textbf{75.92\%} & \textbf{78.11\%} & \textbf{77.66\%} & \textbf{80.34\%} & \textbf{75.99\%} & \textbf{64.30\%} \\
    \bottomrule
    \end{tabular}
}
  \label{tab:attack_pre}
\end{table*}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/ASR_single.png}
    \caption{The ASR of three appearing attacks on different 3D object detectors with and without LOP.}
    \label{fig:asr_single}
\end{figure}

\noindent\textbf{Result \& Analysis.} As we can see from Fig. . 

\subsubsection{Compared with Other Defenses}\label{sec:Experiment:results:comp}
Furthermore, we compare the performance of our LOP with other existing defense methods in normal circumstance and under attacks. We implement three existing appearing attacks to generate adversarial samples based on the validating samples of KITTI dataset. Table [TODO:precision] presents the precision of three different 3D object detectors in normal circumstance, and Table [TODO:precision] presents the precision of three different 3D object detectors under appearing attacks.

\begin{table*}[htbp]
  \centering
  \caption{The precision of 3D object detectors with different defenses on clean samples. $M$, $K$ and $r$ refers to the super-parameters of these three defenses which is introduced in Sec.\ref{sec:Limitation} in details.}
\scalebox{0.9}{
    \begin{tabular}{c|ccc|ccc|ccc}
    \toprule
          & \multicolumn{3}{c|}{PointPillar} & \multicolumn{3}{c|}{PointRCNN} & \multicolumn{3}{c}{PV-RCNN} \\
\cmidrule{2-10}          & Car   & pedestrian & Cyclist & Car   & pedestrian & Cyclist & Car   & pedestrian & Cyclist \\
    \midrule
    None  & 78.99\% & 63.63\% & 59.11\% & 75.04\% & 47.08\% & 56.87\% & 73.12\% & 53.34\% & 62.17\% \\
    SRS (M=500) & 79.14\% & 63.22\% & 59.24\% & 74.75\% & 51.00\% & 56.34\% & 73.23\% & 53.50\% & 63.82\% \\
    \midrule
    SOR (k=2) & 78.91\% & 62.40\% & 58.44\% & 74.49\% & 45.46\% & 55.72\% & 72.92\% & 52.62\% & 61.95\% \\
    SOR(k=10) & 78.86\% & 61.60\% & 58.00\% & 73.88\% & 44.05\% & 55.32\% & 72.94\% & 52.18\% & 60.57\% \\
    \midrule
    CARLO(LPD, r=0.6) & 75.06\% & 6.37\% & 12.64\% & 73.10\% & 10.23\% & 15.23\% & 70.08\% & 7.46\% & 14.29\% \\
    CARLO(LPD, r=0.7) & 77.81\% & 11.99\% & 19.94\% & 74.35\% & 15.85\% & 22.64\% & 72.22\% & 13.03\% & 22.50\% \\
    CARLO(LPD, r=0.8) & 78.00\% & 21.40\% & 27.12\% & 74.32\% & 23.26\% & 29.00\% & 72.41\% & 20.56\% & 28.59\% \\
    \midrule
    CARLO(FSD, r=0.6) & 75.77\% & 39.47\% & 41.75\% & 73.07\% & 35.44\% & 38.46\% & 69.93\% & 36.78\% & 40.92\% \\
    CARLO(FSD, r=0.7) & 77.56\% & 53.27\% & 55.38\% & 74.38\% & 42.22\% & 54.21\% & 71.66\% & 47.34\% & 58.50\% \\
    CARLO(FSD, r=0.8) & 78.31\% & 63.29\% & 58.25\% & 74.87\% & 46.87\% & 56.81\% & 72.37\% & 53.07\% & 61.44\% \\
    \midrule
    Ours(PointNet, B=0.5) & 81.77\% & 63.45\% & 61.50\% & 79.29\% & \textbf{49.63\%} & 61.92\% & 77.85\% & \textbf{53.13\%} & 63.13\% \\
    Ours(PointNet, B=0.6) & 82.38\% & 63.47\% & 62.11\% & 80.03\% & 49.43\% & 63.87\% & 78.53\% & 53.00\% & 64.99\% \\
    Ours(DGCNN, B=0.5) & 83.04\% & 64.42\% & 64.89\% & 80.75\% & 49.59\% & 65.14\% & 79.61\% & 52.49\% & 66.80\% \\
    Ours(DGCNN, B=0.6) & \textbf{83.90\%} & \textbf{64.44\%} & \textbf{66.45\%} & \textbf{81.52\%} & 49.42\% & \textbf{67.24\%} & \textbf{80.34\%} & 52.43\% & \textbf{68.95\%} \\
    \bottomrule
    \end{tabular}%
}
  \label{tab:addlabel}%
\end{table*}%

[TODO:Table and Result Analyze]

\subsection{Simulated Driving Test}\label{sec:Experiment:results:simulation}
\noindent\textbf{Experiment Settings.} [TODO]

[TODO]

\noindent\textbf{Result \& Analysis.} [TODO]
