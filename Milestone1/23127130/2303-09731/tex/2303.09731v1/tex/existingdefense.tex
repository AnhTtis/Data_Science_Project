% \subsection{Limitations of Previous Defenses}
\subsection{Previous Defenses}
\label{sec:Limitation}

\noindent$\bullet$\textbf{ Rationale behind Defenses by Elimination}. To eliminate the forged vehicles crafted by appearing attacks, a defense would unavoidably remove a small ratio of detected real objects from the prediction of 3D object detectors. However, we argue this would hardly cause as substantial damages to the ADS as the \lyf{mistake} of detecting forged vehicles. It is mainly because: (i) As described in the attacker's goal, obstacles which appear near the ADS take the most decisive effect on the vehicle's future planning. Therefore, incorrect elimination of a real obstacle far from this vehicle may have limited influence on the decision-making of the ADS \cite{jiachen2020towards}. (ii) In ADS, the \textit{multi-object tracking} (MOT) module which follows the perception module will take the predictions from the LiDAR-based object detectors as input, maintain and predict the trajectories of objects nearby \cite{xinshuo2020AB3DMOT,hsu2021P3DMOT,wenhan2021motReview}. By design, MOT usually creates an object trajectory for a newly predicted object which is constantly detected for $6$ frames, while removes an overdue object trajectory which is continuously unmatched with any predicted objects for $60$ frames in a common visual perception system \cite{ji2018onlineMOT} of $30$ FPS. This mechanism guarantees that it is much easier for an ADS to create a fake object in its perception \lyf{due to} a successful appearing attack than forgetting a real object, due to the occasional misprediction of the LiDAR-based detector itself or the incorrect elimination of some real objects by such a defense.


Therefore, it is reasonable to tolerate a small ratio of false alarms from defenses by elimination and recognize the importance of defending against appearing attacks by slightly trading the recall of LiDAR-based object detectors. 
% For more background on MOT, please refer to Appendix \ref{appendix:mot}. 
However, the existing defense methods which are possibly against appearing attacks remain limitations in their design, so it is hard for them to maintain good performance in different scenes. To make it clear, we further analyze these defense methods and discuss their limitations accordingly.

\noindent$\bullet$\textbf{ Limitations of Universal Defenses.}
SRS (Simple Random Sampling) and SOR (Statistical Outlier Removal) are two universal defense methods for PC models. They are both unaware of attacks and against adversarial attacks by removing suspect points in input PC.

\noindent\textbf{(1) SRS.} SRS is in essence a random method regardless of any auxiliary information \cite{hang2019dupnet}. Formally speaking, given a raw input PC $X$ with $n$ points, SRS will randomly sample $M(M<n)$ points from $X$ by $
P(X)=\{\mathbb I_x|x\in X,\ \mathbb I_x\sim Bernoulli(0.5)\}, % \label{equal:bernoulli}
$
where $\mathbb I_x$ indicates the existence of each point $x$ in $X$.

\noindent\textbf{(2) SOR.} For a raw input PC $X$, SOR computes the average of the $k$-nearest neighbors' (kNN) distances for each point in $X$, and counts the mean $\mu$ and the standard deviation $\sigma$ of these distances. Then, it recognize those points which fall outside the range of $[\mu-\alpha\cdot\sigma,\mu+\alpha\cdot\sigma]$ as noises and removes them from $X$, where $\alpha=1.1$ is its hyper-parameter \cite{hang2019dupnet}.
% \noindent\textbf{(2) SOR.} Due to the measurement errors of LiDARs, there may be some sparse outliers in its generated PC, which can be seen as the noise in input. 
% Therefore, Rusu et al. suggest a statistical defense method, sparse outlier removal, to remove these outliers \cite{radu2008towards}.
% For a raw input PC $X$, sparse outlier removal computes the nearest \louis{neighbour's distance} for each point in $X$, and counts the mean $\mu$ and the standard deviation $\sigma$ of these distances. Then, it removes those points which fall outside the range of $[\mu-\alpha\cdot\sigma,\mu+\alpha\cdot\sigma]$, where $\alpha=1.1$ is its hyper-parameter.
% Inspired by Rusu's work, Zhou et al. suggest a similar defense method, SOR, which chooses the average of the $k$-nearest neighbors' (kNN) distances instead of the nearest neighbour's distance as the metric \cite{hang2019dupnet}.
% Though Zhou et al. also introduce DUP-Net as a defense method in their work, it can be seen as the combination of SOR and an upsampling network, which is mainly used for data augmentation and denoising.
% Therefore, we consider SOR and DUP-Net as identical defense methods, and evaluate SOR as the representative in our experiments.

% These universal defenses are all mainly applied on simple PC models such as 3D classifiers. Their performance on 3D object detectors remains unevaluated. 
% Meanwhile, a recent work proposes an adversarial attack method, JGBA (Joint Gradient Based Attack), to operate the prediction of 3D classifiers against, and it succeeds under the defense of SOR and DUP-Net \cite{chengcheng2020efficient}.
% This further implies that existing universal defenses are ineffective to defend against adversarial attacks.

\noindent$\bullet$\textbf{ Limitations of Specific Defenses.} 
CARLO (oCclusion-Aware hieRarchy anomaLy detectiOn), SVF (Sequential View Fusion) and Shadow-Catcher are three specific heuristic defense methods for 3D object detectors.
They both specify the attack as a black-box appearing attack proposed in Sun's work \cite{jiachen2020towards}, and perform defense by removing suspect points in input PC or deleting suspect objects in the final prediction.

\noindent\textbf{(1) CARLO.} CARLO is a heuristic defense algorithm proposed by Sun et al. to detect the cars forged by their black-box appearing attack \cite{jiachen2020towards}.
For each object predicted by the 3D object detectors, CARLO computes an anomalous ratio $r$ in one of the following two ways:
(1) \textbf{FSD (Free Space Detection)}, which defines $r=\sum_{c\in S^c}FC(c)/|S^c|$, where $S^c$ is a set including all the cells in this object's bounding box, and $FC(c)$ is a $0/1$ function indicating whether there are input points in the cell $c$; and (2) \textbf{LPD (Laser Penetration Detection)}, which defines $r=|{S\downarrow}^p|/|{S\downarrow}^p\bigcup S^p\bigcup {S\uparrow}^p|$, where the superscript $p$ indicates the corresponding set is composed of points. Specifically, ${S\downarrow}^p$ contains the input points in the space behind this object, $S^p$ is contains the input points inside this object's bounding box, and ${S\uparrow}^p$ contains the input points in the space between this object and the LiDAR.
% \begin{itemize}
%     \item \textbf{FSD (Free Space Detection)} In this way, the ratio $r=\frac{\sum_{c\in S^c}FC(c)}{|S^c|}$, where $S^c$ is a set including all the cells in this object's bounding box, and $FC(c)$ is a $0/1$ function indicating whether there are input points in the cell $c$.
%     \item \textbf{LPD (Laser Penetration Detection)} In this way, the ratio $r=\frac{|{S\downarrow}^p|}{|{S\downarrow}^p\bigcup S^p\bigcup {S\uparrow}^p|}$, where the superscript $p$ indicates the corresponding set is composed of points. Specifically, ${S\downarrow}^p$ contains the input points in the space behind this object, $S^p$ is contains the input points inside this object's bounding box, and ${S\uparrow}^p$ contains the input points in the space between this object and the LiDAR.
% \end{itemize}

Then, CARLO compares all these $r$ with a fixed threshold $R$. For those objects with $r>R$, CARLO recognizes them as fake objects and erases them from the prediction.

\noindent\textbf{(2) SVF.} Similarly, SVF is another defense algorithm suggested by Sun et al., but its key is more similar to SOR: removing outliers from the raw input PC.
As an extra end-to-end network, SVF turns the raw input PC into front-view (FV) representation and uses LU-Net\cite{pierre2019lunet}, a PC segmenter, to calculate a segmentation score for each point.
SVF then concatenates these scores with their related points' input features to re-generate the input PC, and passes this augmented PC to the 3D object detector as input.

\noindent\textbf{(3) Shadow-Catcher.} As our concurrent work, Shadow-Catcher \cite{zhongyuan2021shadowcatcher} also exploits the physical law to improve the robustness of the 3D object detectors in self-driving system. However, Shadow-Catcher is mainly based on hand-crafted rules to determine the forged obstacles, while our work proposes the first learning-based defense scheme to model the complicated physical relation between the depth and density of real objects for defensive purposes. Specifically, Shadow-Catcher computes an anomaly score for each detected object based on the distances of the points inside its bounding box to four key lines related to its bounding box, then compare this score with a presetting threshold to determine whether the perceived obstacle is forged. 
   
As a final remark, most of the previous defenses are initially designed for mitigating specific appearing attacks.
% , regardless of the existence of other potential attacks. 
In this sense, the performance of previous defenses against each popular attack remains unjustified in a systematic way, which we accomplish in our evaluation. 