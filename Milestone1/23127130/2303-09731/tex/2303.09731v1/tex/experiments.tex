\section{Evaluation and Analysis} \label{sec:Experiment}
% In this section, we deploy our LOP on three state-of-the-art 3D object detectors to evaluate their performances in normal circumstances \louis{as well as} under appearing attacks. 
% Besides, we further compare the defense effect of our LOP with other existing defenses against appearing attacks. Below, we describe the setup of experiments and report the results.




\subsection{Overview of Evaluation}\label{sec:Experiment:setup}
\noindent\textbf{(1) Victim Models.} We choose three mainstream LiDAR-based object detectors: \textbf{PointPillars} \cite{alex2019pointpillars}, \textbf{PointRCNN} \cite{shaoshuai2019pointrcnn} and \textbf{PV-RCNN} \cite{shaoshuai2020pvrcnn} as the victim models. Specifically, we adopt the implementation of these three object detectors available on an open-source project OpenPCDet \cite{OpenPCDet}, each of which is normally trained on KITTI dataset \cite{andreas2012kitti} to achieve the near state-of-the-art performance. 

% OpenPCDet is an open-source testing platform for 3D object detection.  % As the 3D object detectors may predict some objects with a low confidence score in prediction, we follow the convention in \cite{joseph2016yolo,shaoshuai2019pointrcnn,shaoshuai2020pvrcnn} and only consider the objects with confidence scores larger than a threshold $C$ in their prediction to keep their precision at a normal level. 

\noindent\textbf{(2) Attack Methods.} We implement three popular appearing attacks which can be roughly categorized into \textit{white-box} and \textit{black-box} attacks. In the former case, the attacker has full access to the victim 3D detector, including the architecture and the parameters, while the latter only accesses the detector as a black-box prediction API. Specifically, the attacks are
    
    \noindent \textbullet \textit{\ A Variant of Adv-LiDAR} \cite{yulong2019advSensor}  (\textit{abbrev.} \textbf{Baseline}, \textit{white-box}): Because Adv-LiDAR is specially designed for attacking Baidu's Apollo \cite{Apollo}, it could hardly be directly transferred to attack other 3D object detectors \cite{jiachen2020towards}. Therefore, following its main idea, we implement a variant of ADV-LiDAR by randomly injecting a certain number of points into a specified zone, and using FGSM \cite{ian2015explaining} to increase the confidence scores of those forged cars related to these points.

    \noindent \textbullet \textit{\ Yang's Work} \cite{kaichen2021robust}  (\textit{abbrev.} \textbf{Roadside}, \textit{white-box}): This attack forges cars by 3D printing a small and specifically-designed object, increasing their confidence scores and category scores of label car, and enlarging their bounding boxes with gradient descent. 
    \xqf{Since this attack will generate the adversarial points and then turn them into a physical object, we will only deploy the first part for our experiments.}
    In their original work, this attack mainly aims at breaking PointRCNN with a white-box access. We thus also follow the settings in our experiments.
    
   \noindent \textbullet \textit{\ Sun's Work} \cite{jiachen2020towards}  (\textit{abbrev.} \textbf{Physical}, \textit{black-box}): This attack forges cars by duplicating real cars, which contains a limited number of points due to either inter-occlusion or intra-occlusion. The PC of the fake car is then transformed to a front-near position of the victim ADS. 
   
   
\noindent\textbf{(3) Baseline Defenses.} We implement \textbf{SRS}, \textbf{SOR}, \textbf{CARLO} and \textbf{Shadow-Catcher} which we have introduced in Section \ref{sec:Limitation} as the baseline defenses.
We do not consider SVF because it relies on retraining the whole 3D object detector itself, and causes much more time and computation cost compared with other baseline defenses as well as ours (Section \ref{sec:Experiment:results:overhead}).
% Also, as CARLO and SVF are specially designed for the same attack in \cite{jiachen2020towards}, we argue that the results of CARLO can partly represent that of SVF.


\noindent\textbf{(4) Metrics.}
We choose three different metrics to evaluate the performance of our proposed defense and other defenses:

    \noindent{\textbullet\ \textbf{Precision}} measures the proportion of the real objects in the prediction results. In the context of defense, the decrease in precision reflects whether these defenses would harm the original performance of the victim model.
    Following  \cite{alex2019pointpillars,shaoshuai2019pointrcnn,shaoshuai2020pvrcnn}, we first choose a certain threshold $C_{\text{conf}}$ for each 3D object detector, and remove those predicted objects with confidence scores less than $C_{\text{conf}}$. 
    Then we calculate the 2D IoU on the x-y plane between the bounding boxes of each remaining predicted object and the ground-truth objects. A predicted object is true positive, if its maximal IoU value surpasses another certain threshold $C_{\text{IoU}}$ and the predicted category is identical with the ground-truth; otherwise, the predicted object is a false positive prediction.
    For PointPillars and PointRCNN, we set $C_{\text{conf}}=0.5,\ C_{\text{IoU}}=0.5$; for PV-RCNN, we set $C_{\text{conf}}=0.7,\ C_{\text{IoU}}=0.5$.


    \noindent{\textbullet\ \textbf{Average Precision (AP)}} is a comprehensive metric over the precision and the recall of the detection results. Specifically, AP is the average value of precision when the recall is larger than different specific values, which can be represented as
\begin{align}
AP=\frac{1}{11}\sum_{r\in\{0,0.1,\hdots,1.0\}}{max}_{r'\geq r}\text{Precision}@(\text{Recall}= r')
\end{align}

    \noindent{\textbullet\ \textbf{Attack Success Rate (ASR)}} measures the ratio of the number of forged cars detected by the victim 3D object detector over the total number of attack attempts, which directly reflects the performance of these defenses. A more effective defense should result in a lower ASR. 

\noindent\textbf{(5) Implementation of LOP.} We choose two off-the-shelf point-wise PC classification architectures, PointNet \cite{charles2017PN} and DGCNN \cite{yue2019DGCNN}, to instantiate the LOP. For those hyper-parameters of LOP described in Section \ref{sec:OurDefense}, we set $M_{pc}=1024,\ T_{\text{IoU}}=1\times{10}^{-6},\ \alpha_{fl}=1,\ \gamma_{fl}=2,\ \beta=1\times{10}^{-3}$.

\input{table/attack_comp_table.tex}

\subsection{Comparison with Baselines}
\subsubsection{Attack Scenarios}
\label{sec:Experiment:results:comp_attack}
First, we evaluate the performance of our defense against \xqf{recent} appearing attacks. We implement three recent appearing attacks to generate adversarial examples against the three mainstream 3D object detectors based on the KITTI's validation set. We evaluate the ASR of these appearing attacks along with the AP and the precision of these detectors under attacks.
Besides the forged cars crafted by appearing attacks, there also remains some normal objects in the adversarial examples, which are considered in the AP and precision metrics.
Table \ref{tab:comp_map_atk_0} and Table \ref{tab:comp_map_atk} show the AP of the detectors on cars when equipped with different defenses,
and Fig.\ref{fig:comp_atk_pre_asr} and Fig.\ref{fig:comp_atk_pre_asr_0} plots the defense effectiveness (y-axis, in terms of $1-\text{ASR}$) and the precision on cars (x-axis) of different defenses.

\noindent\textbf{Results \& Analysis.} As we can see from Table \ref{tab:comp_map_atk_0}, Table \ref{tab:comp_map_atk}, Fig.\ref{fig:comp_atk_pre_asr} and Fig.\ref{fig:comp_atk_pre_asr_0}, compared with SRS and CARLO, our defense simultaneously achieves higher defense effectiveness and the victim models under guard have higher AP and precision on cars. 
For example, under \xqf{recent} appearing attacks, the PointRCNN equipped with the LOP keeps AP on cars over $70\%$ and precision on cars over $72\%$, while AP on cars is always less than $70\%$ and precision on cars is always less than $63\%$ when deploying SRS or CARLO on the PointRCNN.
Compared with SOR, although in some cases our defense has slightly lower defense effectiveness (the margin is less than $5\%$), it always results in higher AP and precision on cars under attacks. Compared with Shadow-Catcher, although in some cases our defense has slightly lower AP on cars, it always results in higher precision on cars and better defense effectiveness under attacks.

% In summary, our defense is effective against existing appearing attacks and incurs only slightly decrease on the normal performance of the victim detectors.

From a different perspective, we observe that other defenses only perform well when protecting certain models against specific attack techniques. For example,
SOR performs better when protecting PointPillars, while CARLO performs better when defending against \textit{Physical}. In contrast, the LOP performs well independent of the structure of the 3D object detector or the undergoing appearing attack, which implies that our proposed defense is more general than other defenses.


\subsubsection{Benign Scenarios}
\label{sec:Experiment:results:normal}
Then, we evaluate the performance of the victim models under guard on clean samples to measure the performance overhead brought by different defenses.
Table \ref{tab:comp_normal_pre_0} and Table \ref{tab:comp_normal_pre} present the AP and precision of them in the normal circumstances.

\input{table/normal_comp_table.tex}

%  As is observed, the improvements is substantially larger than the improvement brought by other defenses.

\noindent\textbf{Results \& Analysis.} As Table \ref{tab:comp_normal_pre_0} and Table \ref{tab:comp_normal_pre} show, compared with existing defenses, the performance of 
% the three victim models 
these detectors
has less degradation in the normal cases when equipped with the LOP than with other defenses. For example, the AP and precision of 
% 3D object detectors 
detectors
equipped with other defenses both decrease in most cases, while for these 3D object detectors equipped with the LOP, the AP on cars even increases by $0.33\sim1.71\%$, and the precision on cars increases by $2.78\sim7.12\%$. Although Shadow-Catcher has slightly higher AP on cars than the LOP, 
% the detectors with Shadow-Catcher usually has lower precision than those detectors with LOP. Besides, 
considering the defensive advantages of LOP under different appearing attacks, our proposed defense may be more suitable for practical ADS, due to the performance-robustness balance when the detector is equipped with LOP.
% the choice of its threshold will significantly affect the performance of Shadow-Catcher and lead to a large trade-off between the performance and the robustness of the detector deployed with it, while \cite{zhongyuan2021shadowcatcher} only suggest to choose the value of it based on experience, which is harder to implement in the real world.}

% Although performance degradation is observed for the precision on pedestrians, the degradation is controlled below $2.2\%$. We infer the main reason of degradation is two-fold:
% On the one hand, the performance of the victim models on pedestrians is usually worse than that on cars and cyclists, which is a long-standing pain point for all existing object detectors \cite{alex2019pointpillars,shaoshuai2019pointrcnn,shaoshuai2020pvrcnn}.
% Therefore, it is also hard for the LOP to precisely detect the fake pedestrian based on the local pillars.
% On the other hand, the point density and the depth of pedestrians are usually smaller than cars and cyclists, which brings additional challenges in modeling the depth-density relation, a key to the performance of LOP. 

We further analyze why our proposed defense would even increase the performance of the victim models on cars in normal cases, while existing defenses would not: 
(i) The LOP mainly learns the semantic and spatial features of real objects, while other defenses focus on recognizing fake objects. (ii) The bounding boxes of cars are much larger than that of other objects, which means that there are enough samples corresponding to components of cars provided for the LOP to learn. In summary, our proposed defense incurs almost no damage on the normal performance of the victim models and may sometimes even improve the performance due to its finer granularity modeling of the obstacles. 
% In Appendix \ref{appendix:hyper}, we further experiment with the hyper-parameters of LOP, which validate that the model structure will not affect the performance of LOP, while the LOP's performance may be affected by the value of $B$ in a certain degree.
In Appendix \ref{appendix:hyper}, we further experiment with the hyper-parameters of LOP, which validate that the model structure will not affect the performance of LOP.
% The 3D object detectors with our LOP have higher precision on cars and cyclists than those detectors with other defenses, regardless of the detector structure.
% Though detectors with SRS perform better than those with our LOP on the prediction of pedestrians, SRS is considered as the baseline of these defenses in our experiment.
% Furthermore, SRS has limited influence on defending against existing appearing attacks, which will be reported in Section \ref{sec:Experiment:results:comp_attack}.
% Should analyze why LOP will improve the normal performance of a model



% Similarly, we evaluate the performance of these detectors under both normal circumstance and attacks. 

\subsubsection{Overhead Analysis}
\label{sec:Experiment:results:overhead}
Next, we evaluate the overhead in the preparation stage. Except for our LOP and SVF, other defense methods do not introduce additional learning modules and therefore no training is required in the preparation stage. Table \ref{tab:EXP4-1} compares the time overhead of LOP and SVF during the preparation phase. Table \ref{tab:EXP4-2} reports the time and the space overhead of the inference phase of each defense. We conduct the experiments with $5$ repetitive tests on each case, and report the mean and the standard deviation as the final results. 

\input{table/overhead_table.tex}

\noindent$\bullet$\textbf{ Results \& Analysis.} As Table \ref{tab:EXP4-1} shows, the time overhead of SVF in the preparation phase is much more higher than that of LOP. It is mainly because SVF requires the retraining of the whole 3D object detectors from scratch, while the training task of LOP only involves a PC-based binary classifier, a much easier learning task compared with that of SVF. More importantly, once LOP is trained, it can be combined with different defense targets to provide the defense, while SVF has to retrain each target.

Meanwhile, Table \ref{tab:EXP4-2} shows, LOP incurs slightly more time and space overheads than most of the statistical defenses, which 
can be further reduced by 
some optimization techniques. For example, to simplify the implementation, we split the whole input space into pillars and use LOP to predict their objectness score during the split in this experiment. However, there is not necessary to check all pillars in the real case. We can identify the pillars which not only intersects with the predicted bounding boxes but also contains points, and only predict their objectness scores to reduce the total times of calculations. Furthermore, we can combine parts of these pillars into a batch and uses LOP to predict in a parallel way for further acceleration.
% a more parallel implementation of our defense. For example, the prediction over different pillars can be paralleled in our defense. 
In Section \ref{sec:Experiment:results:real}, we 
% further optimize the implementation in
follow the optimization mentioned above to deploy LOP in
the end-to-end self-driving system and reduce the time overhead caused by LOP to less than $10$ms per detection, which has almost no influence on the real-time requirement of the self-driving system. 

% In summary, compared with the heavy-weight defense SVF, our LOP has much lower time overhead during the preparation phase, while, compared with the post-processing defenses, LOP only has a slightly higher time and space overhead in both modular and end-to-end testing, which, from our perspective, is an acceptable trade-off considering the advantages in robustness enhancement.


\subsection{Adaptive Attacks}
\label{sec:Experiment:results:adaptive}
In this part, we evaluate whether our defense is robust against an adaptive attacker who knows the existence of LOP and in the worst case has the access to the structure and the parameters of our LOP. In this almost worst-case threat model, it is possible for the adversary to attempt to bypass our defense during the generation of forged objects. As the \textit{Physical} attack in \cite{jiachen2020towards} requires no training stage in its generation, we choose to modify the \textit{Baseline} attack, i.e., the attack in \cite{yulong2019advSensor}, which we refer to as the \textit{Baseline} attack throughout this response letter, into an adaptive attack against our defense. Specifically, we propose to generate the adversarial point cloud by simultaneously optimizing the original appearing attack objective and maximizing the score of the crafted object under LOP. To enhance the performance of the Baseline attack, we further replace the FGSM algorithm by PGD. Table \ref{tab:EXP1} reports the ASR of the adaptive attacks on the three 3D object detectors when LOP is deployed or not, along with the AP and the precision of the detectors on cars under the adaptive attack. 

\input{table/adaptive_attack_table.tex}

\noindent$\bullet$\textbf{ Results \& Analysis.} As we can see from Table \ref{tab:EXP1}, our LOP performs well when defending against the adaptive attacks above. Both the PointNet-based and the DGCNN-based LOP can reduce the ASR of the adaptive attacks by a large margin, while only a slight loss of performance on clean samples is observed. For example, when defending PointPillars, the ASR is reduced from $45\%$ to $12\%$ with the DGCNN-based LOP, while the decrease of AP is by less than $4\%$. From our perspective, the result may be because the orthogonality between the original attack target and the intention of bypassing LOP, which brings challenges for optimizing two different loss function at the same time.  In summary, LOP has certain robustness against even the worst-case adaptive attack where the attack has a full white-box access to the defense module. 


\subsection{System Integration}
\label{sec:Experiment:results:real}
 To evaluate the system-level usefulness of our proposed defense, we implement the PointNet-based LOP in Baidu's Apollo 6.0.0 system
 \xqf{in the optimized way described in Section \ref{sec:Experiment:results:overhead}}
 , and conduct both the modular and the closed-loop control evaluation in two simulation environments in normal driving scenarios and against the \textit{Physical} attack. We release the implementation details
 % and the code 
 in \cite{Apollo_LOP}.

\noindent$\bullet$\textbf{ Experimental Settings.} In the experiments, we construct two different scenarios (e.g., Single Lane Road and Borregas Ave) with random traffic in the LGSVL simulator to evaluate LOP's performance in the end-to-end system. 
% As the modular testing results, 
Table \ref{tab:EXP6} reports the ASR of the Physical attack on Apollo 6.0.0, together with the precision and the time cost of the 3D object detectors in Apollo's perception module when LOP is deployed or not, and 
% As the closed-loop control testing results,
Fig.\ref{fig:apollo_LOP} illustrates the detection results in an end-to-end driving test 
% (i.e., closed-loop control with Perception, Prediction, Localization and Planning modules enabled) 
when the system is deployed without or with LOP, and shows a snapshot of the attacking scenario
% in the \textit{Borregas Ave} map 
in the experiments. 


\input{table/e2e_table.tex}

%%%%%%%% BEGIN APOLLO
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/apollo_visualize_4.png}
    \vspace{-0.15in}
    \caption{The simulating scenario and the Dreamview of Apollo 6.0.0 without and with our LOP under Physical attack.}
    \label{fig:apollo_LOP}
\end{figure*}
%%%%%%%% END APOLLO

\noindent$\bullet$\textbf{ Results \& Analysis.} As Table \ref{tab:EXP6} shows, LOP effectively defends against appearing attacks in the end-to-end Apollo 6.0.0, with a slight proportion of time overhead (less than $10$ms). As Fig.\ref{fig:apollo_LOP}(b) shows, the \textit{Physical} attack can successfully fools Apollo's perception module, and remains existent in the Dreamview even after the processing of MOT. This confirms our argument that appearing attacks is easier to be mounted in practical scenarios than disappearing attacks. In the Dreamview view of Fig.\ref{fig:apollo_LOP}(c), with the help of our LOP, the forged object is eliminated from Apollo's perception during the evaluation (with ASR$=0\%$), while the real obstacles remain intact in the perception of the ADS. Therefore, the driving trajectory of the ADS with LOP remains normal and safe during the full driving test. Besides, LOP only incurs a $9.12ms$ overhead on the running time of the 3D detection pipeline on average and slightly brings down the FPS from $29.97$ to $23.54$, which still satisfies the real-time requirement of a physical self-driving system \cite{yulan2021deep}.  

% Moreover, we select previous forged objects, which can successfully fool the perception module of Apollo for at least one frame, to further determine whether they would lead to a potential car crash in different traces by showing the crash rate. The crash rate is calculated based on the assumption that there is a car moving behind the self-driving vehicle at a constant speed, which means some specific events of the Apollo' planning may lead to the crash between the imaginary car and the self-driving vehicle, such as the sudden brake. We observe that the crash rate of the Apollo without LOP is $13.33\%$ $(2/15)$, while, with LOP, the crash rate is reduced to $0.00\%$ $(0/15)$. We provide the Dreamview snapshot and the details of these experiments in \xqf{Appendix \ref{appendix:system}}. Therefore, combined with the comprehensive evaluation results on the KITTI benchmark, our end-to-end experiments further validate the system-level usefulness of our proposed defense in terms of the improved system robustness, and the acceptable overhead on the running time and the normal driving performance.

Moreover, we use the previously forged objects, which can successfully fool the perception module of Apollo for at least one frame, to further test whether they would lead to a potential harsh braking in different traces. Specifically, we measure whether the self-driving vehicle would do sudden braking, 
which is shown as it decelerating to 0 km/h in less than 1 second, 
to calculate the \textit{harsh braking rate}, i.e., the ratio of the test cases where the self-driving vehicle suddenly brakes when there is no real obstacle in front of it. We observe that the harsh braking rate of the Apollo without LOP is $13.33\%$ $(2/15)$, while, with LOP, the harsh braking rate is reduced to $0.00\%$ $(0/15)$. We provide the Dreamview snapshot and the details of these experiments in \xqf{Appendix \ref{appendix:system}}. Therefore, combined with the comprehensive evaluation results on the KITTI benchmark, our end-to-end experiments further validate the system-level usefulness of our proposed defense in terms of the improved system robustness, and the acceptable overhead on the running time and the normal driving performance.

% Furthermore, we also evaluate the effectiveness of our defense with an experimental vehicle for automated driving, the D-KIT Advanced \cite{DKit}, where real-world PCs are captured by the Velodyne 128 LiDAR of it when the ADS drives in a closed environment.
% with 5 moving obstacles. 
% We report and analyze the experimental results in Appendix \ref{app:realworld}, which validates the practical usefulness of our defense from a different aspect.  
