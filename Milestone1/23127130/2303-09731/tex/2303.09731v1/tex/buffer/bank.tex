In contrast, in an appearing attack the attacker can choose the victim to fire the laser and forge non-existent cars as he/she wishes. Meanwhile, it is hard for others to notice this attack because there is almost no evidence left in the accident scene.

\xqf{Although due to the attack goal, it is meaningless to deploy LOP to defend against disappearing attacks, it is still worthwhile to evaluate the performance of LOP under mis-categorization attacks. We evaluate the performance of 3D object detectors under mis-categorization attacks in Appendix \ref{appendix:miscategorization}, and prove that LOP is also robustness against such attacks.}

% \noindent\textbf{Threats of Backdoor Attacks.}
\xqf{At last, }Backdoor attack is another threatening attack class to DNN models, which injects one or multiple backdoors into the victim model by modifying the training data \cite{chen2017targeted,turner2019labelconsitent}, the parameters \cite{jacob2020backdoorCNN,adnan2020TBT} or the network structure \cite{ruixiang2020embarrass}. After the injection, the adversary can activate his/her specified model misbehaviors by feeding special inputs containing the related trigger pattern. However, to implement a backdoor attack on the victim ADS is unrealistic. For example, the attacker requires the permission of the victim's self-driving system, which only belongs to a few administrators or security operators.


% On one hand, 
% to implement a disappearing attack, the attacker has to put the printed objects on the road or near some objects before the attack happens,
% which means the attack can not choose the victim ADS.
% On the other hand, after the first accident of disappearing attack, a printed object may be destroyed and the people nearby may realize that there is probably some problems with the printed object, so it is hard for the attacker to use the same printed object to cause the second accident.

% Besides, with such permission, the attacker is able to directly modify the outputs of any DNN models, or even any modules in the self-driving system.
% Besides, even with such permission, the attacker has no need to backdoor is able to modify any modules in the self-driving system, or even the outputs of DNN models directly.
% Therefore, we believe that it is hard for an attacker to acquire the permission, and,
% once getting the permission, the attacker does not need to implement such a backdoor attack because he/she can already manipulate the victim as his/her wish.

% If the input of target model doesn't activate any triggers, the target model will return the normal prediction as usual.
% Otherwise, the target model will return a specific prediction which is linked with the activated trigger by the attacker.

% In 2017, Chen et al. firstly apply backdoor attack in machine learning, and suggest a \louis{fixed-pattern} backdoor attack: the trigger in the target model will be activated by a fixed pattern in the input \cite{chen2017targeted}.
% Turner et al. suggest not to change the labels of backdoor samples \louis{added} into the target model's training set, and improve the invisibility of backdoor attack during the training stage of target model \cite{turner2019labelconsitent}.
% On the other hand, Li et al. and Pang et al. suggest dynamic-pattern backdoor attacks, where the trigger in the target model will be activated by different patterns for different inputs, and improve the invisibility of backdoor attack during the inference stage of target model \cite{li2019invisible,pang2020atale}.
% The studies above prove that backdoor attacks can manipulate the target model's output on specific tasks, without harming the model's performance on its main task or being noticed by others.


% he/she doesn't need to implement backdoor attack while he/she is able to manipulate the victim as his/her wish. 

% \noindent\textbf{Limitations and Future Directions.} 
% Although there may be potential adaptive attacks designed to break the LOP in the future, we argue that it is hard to implement in the real world. It is mainly because: 
% (i) Even the open-sourced ADS such as Apollo \cite{Apollo} keeps some small DNN models as black boxes. For example, there is a black-box DNN model used to locate the ADS in Apollo. The structure of this model is stored as an executable and linking file (*.so), and the parameter of this model is stored as an encrypted binary file (*.bin), which are difficult for attackers to decipher. Thus, it is also possible to protect the LOP by encrypting.
% (ii) The output of LOP is only a binary value to indicate the object is real or fake. Once the LOP is accessed as a black-box executable, it is generally hard for attacker to efficiently generate adversarial examples against our defense via optimization-based techniques.
% \morinop{(add discussion on adaptive attacks)}