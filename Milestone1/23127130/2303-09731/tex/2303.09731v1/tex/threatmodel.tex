\section{Security Settings}\label{sec:ThreatModel}
% In this section, we first define the security settings of an appearing attack. Then, we review the existing attack and defense techniques. Finally, we further discuss the limitations of existing defenses in countering existing appearing attacks.

\subsection{Threat Model}
\noindent$\bullet$\textbf{ Attacker's Goal}. In general, the direct goal of an appearing attack is to forge fake cars, in the detection results of the LiDAR-based object detector in ADS. To refine the attack goal above, we first analyze the following two attack scenarios of an appearing attack. 

\noindent\textbf{Attack Scenario 1.} \textit{(On the Highways)}  As shown in the top part (a) of Fig.\ref{fig:atk_scenario}, an attacker can spoof the LiDAR of the victim ADS when it passes by. Detecting a forged car at the immediate front, the victim will make a stop decision and decrease its speed to 0 km/h within seconds. The unpredictable emergency braking may leave no reaction time for other vehicles behind. This may lead to a rear-end collision or even more severe traffic accidents.

\noindent\textbf{Attack Scenario 2}. \textit{(At the Traffic Lights)} Similarly, as shown in the bottom part (b) of Fig.\ref{fig:atk_scenario}, the attacker conducts LiDAR spoofing when the victim ADS stops at the red light. By forging a fake car ahead, the victim will keep immobile even after the traffic signal turns green, blocking other vehicles behind and causing a traffic jam. 



As the two attack scenarios show, to cause a real-world threat, the forged cars are required to be not only recognized by LiDAR-based object detectors with sufficiently large confidence scores, but also close enough to result in the re-routing of the victim. Therefore, we further refine the attack goal to expect the cars to be forged in a close distance to the victim. Specifically, in this work we require a forged car to be within a $5\sim{10}$ meters to the victim to pose a sufficient threat \cite{yulong2019advSensor, jiachen2020towards}.



\noindent$\bullet$\textbf{ Attacker's Capability.}
Following the threat model in recent attacks \cite{yulong2019advSensor, jiachen2020towards}, our defense mainly aims at mitigating an attacker satisfying the following threat model: 

\noindent \textbf{Assumption 1.} \textit{(Prior Knowledge)} The attacker knows the architecture and the parameters of the LiDAR-based object detector deployed on the victim ADS (i.e.,  \textit{white-box}).

% It is feasible because most ADS are open-sourced on third-party platforms (e.g., GitHub), where the attacker can audit the source code to figure out the details of the target model. 

\noindent \textbf{Assumption 2.} \textit{(Number of Added Points)} The attacker can inject at most 200 points (according to \cite{jiachen2020towards}) into the input PC of the victim 3D object detector in one scan of LiDAR.

% , the 200 points are the upper bound on the capability of existing physical equipment for LiDAR spoofing in one scan.

\noindent \textbf{Assumption 3.} \textit{(Features of Added Points)} The attacker is allowed to inject points at any location and with arbitrary light intensity, which is imposed for a more generic defense.

% . Although there exists a bounded feasible region for LiDAR spoofing due to the capability of physical equipment, we relax this requirement

\noindent$\bullet$\textbf{ Attack Process.} Before the attack starts, the attacker deploys a physical equipment to receive the lasers emitted by the victim ADS's LiDAR, and shoot lasers back to the LiDAR. 
% The details of this equipment are described in Appendix \ref{appendix:physical_equipment}. 
% With the support of this equipment, the attacker can inject a certain number of points within a certain area into the LiDAR's generated PC as he/she wishes. 
Later, the LiDAR-based 3D object detectors of the victim takes the infected PC and predicts a non-existent car. 
Finally, the victim re-routes to avoid the non-existent car, which may lead to severe collision accidents.


\subsection{Recent Appearing Attacks}
\label{sec:attack_review}
Next, we review the recent appearing attacks on LiDAR-based object detectors. As one of the earliest work, Shin et al. propose a spoofing attack by randomly injecting points into a certain area regardless of the LiDAR-based object detectors of the victim ADS, which is sufficient to forge a non-existent car \cite{hocheol2017illusion}. Inspired by Shin's work, Cao et al. standardize the attack pipeline of adversarial spoofing attack, and propose an appearing attack, Adv-LiDAR, which aims at breaking Apollo's detection system \cite{yulong2019advSensor}. By modeling the preprocessing and postprocessing modules in Apollo's LiDAR-based object detector, Adv-LiDAR successfully uses traditional adversarial attack technology to forge non-existent cars. However, Sun et al. later prove that other 3D object detectors such as PointPillars and PointRCNN will not be affected by the the adversarial samples generated by Adv-LiDAR, and then suggest a more general black-box appearing attack based on the intrinsic physical nature of LiDARs \cite{jiachen2020towards}. Also, another attack by Yang et al. shares the same attack goal as the above appearing attacks but uses a different attack process and physical equipment \cite{kaichen2021robust}. Specifically, they use a physical object which is specially designed to tempt the 3D object detector to predict itself as a car with a falsely enlarged bounding box and therefore fabricate a non-existent part of this object in the model's perception. For completeness, we also cover this attack into the appearing attacks in experiments. 

% Despite the different attack strategies, we observe that most of them rely solely on increasing the confidence score of the fake cars in the detection results for car forgery, which may leave a clear differences between local parts of a real and a forged cars (\S\ref{sec:method:lop}).
