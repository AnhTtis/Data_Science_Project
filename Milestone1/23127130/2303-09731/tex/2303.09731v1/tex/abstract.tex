\begin{abstract}
Automated driving systems rely on 3D object detectors to recognize possible obstacles from LiDAR point clouds. However, recent works show the adversary can forge non-existent cars in the prediction results with a few fake points (i.e., \textit{appearing attack}). By removing statistical outliers, existing defenses are however designed for specific attacks or biased by predefined heuristic rules. Towards more comprehensive mitigation, we first systematically inspect the mechanism of 
% previous
\xqf{recent}
appearing attacks: Their common weaknesses are observed in crafting fake obstacles which (i) have obvious differences in the local parts compared with real obstacles and (ii) violate the physical relation between depth and point density. 

In this paper, we propose a novel plug-and-play defensive module which works by side of a trained LiDAR-based object detector to eliminate forged obstacles where a major proportion of local parts have low \textit{objectness}, i.e., to what degree it belongs to a real object. At the core of our module is a \textit{local objectness predictor}, which explicitly incorporates the depth information to model the relation between depth and point density, and predicts each local part of an obstacle with an \textit{objectness} score. Extensive experiments show, our proposed defense eliminates at least $70\%$ cars forged by three known appearing attacks in most cases, while, for the best previous defense, less than $30\%$ forged cars are eliminated. Meanwhile, under the same circumstance, our defense incurs less overhead for AP/precision on cars compared with existing defenses. Furthermore, We validate the effectiveness of our proposed defense 
% on real-world PC data obtained from the D-KIT Advanced with Velodyne-128 in a closed road environment and 
on simulation-based closed-loop control driving tests in the open-source system of Baidu's Apollo.
% while the overhead on the normal precision of the models is less than $3\%$ by average.
% The results show that our local objectness predictors can increase 3D object detectors' precisions from $[data0]!!!$ to $[data1]!!!$ under appearing attacks.

%%  is deployed with state-of-the-art 3D object detectors
% data0->不防御的情况下，受攻击的3D实体检测模型的precision范围
% data1->防御的情况下，受攻击的3D实体检测模型的precision范围



% strengthens the robustness of a trained LiDAR-based object detector by eliminating forged objects. Specifically, we construct 

% 摘要提纲：
% 1、自动驾驶系统依赖3D实体检测模型基于点云检测周围的实体，但现在已经证明了对抗性攻击可以有效攻击3D实体检测模型
% 2、appearing attack作为其中一种对抗性攻击，能导致3D实体检测模型检测到不存在的实体，进而导致自动驾驶小车紧急制动造成交通阻塞甚至是追尾事故
% 3、现有的防御研究都是针对其中一种或几种appearing attack设计，缺乏泛化性
% 4、我们研究了现存的几种appearing attack，发现了它们所具有的相同特点，并利用这一点设计了防御appearing attack的伪造实体检测模型。
% 5、我们通过实验发现，使用了我们的伪造实体检测模型后，3D实体检测模型的precision在无攻击场景和有攻击场景中都明显提升
% while the precision on cars is only degraded by less than $3\%$ compared with their precision in normal circumstance in most cases.
% Our extensive evaluation validate that, compared with the normal circumstance, when our proposed defense is deployed with $3$ state-of-the-art 3D object detectors, and at least $70\%$ of the fake obstacles forged by $3$ popular appearing attacks are eliminated,
% while the precision on cars is only degraded by less than $3\%$ in most cases. 
\if 0 

Based on our observation that 

a local objectness predictors to distinguish the local differences between the fake objects and the real objects and avoid appearing attacks. We design the local objectness predictors based on two common properties of recent appearing attacks revealed by us. First, recent appearing attacks are limited by attacker's goal and attacker's capability. Behind these limitations, there is an invariant relation between the depth and the point density of real cars. Second, recent appearing attacks focus on forging the fake car which is similar as a real car in general, regardless of the local differences between them. 
\fi
\end{abstract}
%to handle the complexity of real-world driving scenes. 