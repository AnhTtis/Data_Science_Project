\section{Background}\label{sec:Background}
\noindent\textbf{Basics of LiDAR.} As one of the main sensors deployed in an automated driving system (ADS), a LiDAR (Light Detection and Ranging) scans the surrounding environment and generates a point cloud (PC) $X=\{(x_i,y_i,z_i,int_i)\}\in R^{n\times 4}$, including $n$ points with $(x_i,y_i,z_i)$ as $i$-th point's location and $int_i$ as $i$-th point's intensity, during each detection \cite{LiDAR, hocheol2017illusion}. 
Technically, the LiDAR first emits a laser ray consecutive in both horizontal and vertical directions, which then captures the reflected lasers, records their time of flight and light intensity, and further computes the depth and 3D coordinate of the points related to these reflected lasers. 
Finally, the LiDAR collects these information to generate the raw PC, which represents the object surfaces in the surrounding environment, and sends this raw PC to the ADS for downstream processing. 

% Because the time of flight and light intensity of reflected lasers are hardly affected by the change of object surface texture, the traditional image-based adversarial attack methods have limited effects on these LiDAR-based 3D object detectors. On the other words, LiDARs are more robust to the change of environment and the image-based adversarial attacks than cameras. This is primarily why most commercial self-driving systems, including Google's Waymo One \cite{Waymo} and Baidu's Apollo \cite{Apollo}, mainly rely on the sensor data (e.g., PC) generated from LiDAR for obstacle perception.

\noindent\textbf{3D Object Detectors.}
% Options: Draw a picture to introduce the meanings of bounding box, pillar and voxel
DNN-based 3D object detectors empower modern ADS for perceiving and detecting objects in the surrounding environment (i.e., \textit{obstacle perception}).
Technically, a 3D object detector usually takes PC as the input and returns the category and \textit{bounding box}, a rectangle or cuboid which bounds the detected object to represent its location in a PC, of each perceived object \cite{yulan2021deep}. 
In most cases, 3D object detectors can be regarded as the combination of three modules: the preprocessing, the backbone and the prediction modules. 

A typical preprocessing module first divides the points of PC into a number of sets (e.g., voxels or pillars) based on specific rules and then calculates the statistical information \cite{alex2019pointpillars,yin2018VoxelNet}, or uses DNN models, such as PointNet \cite{charles2017PN} or DGCNN \cite{yue2019DGCNN}, to generate the feature vectors for each point \cite{shaoshuai2019pointrcnn}. Then,
the backbone module implemented with 2D/3D convolutional neural networks (CNN) \cite{yann1989backpropagation, shuiwang20103DCNN} extracts the PC's features and generates the global feature map. Finally, the prediction module in one-stage 3D object detectors like VoxelNet \cite{yin2018VoxelNet} and PointPillars \cite{alex2019pointpillars} directly predicts the bounding box and category of each obstacle based on the global feature map. Differently, in two-stage 3D object detectors like PointRCNN \cite{shaoshuai2019pointrcnn} and PV-RCNN \cite{shaoshuai2020pvrcnn}, the prediction module predicts the proposal bounding boxes of objects based on the global feature map and generate a local feature map for each object based on the combination of the global feature map and the related proposal bounding boxes at the first stage, and then the final bounding box and category of each obstacle based on each local feature map at the second stage.

% Considering the direct effect on the future
% travel plan,
% % \louis{driving plan, }
% the location prediction % from the detection results of a LiDAR-based object detector
% of the detection results
% is much more important than the predicted category at the self-driving level. The key to predicting the bounding box is the spatial information of the environment, which is stored explicitly in PC while implicitly in images. In other words, a PC always contains more spatial information than an image, which explains why the bounding box prediction from a LiDAR-based object detector is usually considered more reliable than that from an image-based object detector \cite{joseph2016yolo, shaoqing2015fasterrcnn, wei2016ssd}.


\noindent\textbf{Adversarial Example.} In general, given a machine learning model $F$ and a normal sample $x$ with label $y$, an adversarial example $x'$ is generated from $x$ by adding a slight perturbation to mislead the victim model's prediction while causing no modification to either the model's architecture or the parameters \cite{Szegedy2014IntriguingPO,abdullah2020advpc,yue2020onisomety}.
According to the attack goal, an adversarial example can be further categorized into untargeted and targeted. By definition,
an untargeted attack aims at misleading the victim model into $F(x')\neq y$, while a targeted attack aims at misleading the victim model into $F(x')=y'$, where $y'$ is the target label specified by attacker. 
According to \cite{nicholas2017towards}, the targeted adversarial attack can be further represented as the optimization problem:
% \begin{align}
% & {argmin}_{x'}||x-x'||_p \label{equal:CW} \\ 
% \text{s.t.}\quad & F(x')=y'\text{ and } x' \in X \notag
% \end{align}
\vspace{-1mm}
\begin{align}
& {argmin}_{x'}||x-x'||_p & \text{s.t.} \quad F(x')=y'\text{ and } x' \in X \label{equal:CW}
\end{align}
\vspace{-1mm}
where the objective $\min\|x - x'\|$ restricts the region of perturbation (i.e., attack budget) and $X$ denotes the input space. In the context of ADS, to cause severe safety issues, several recent adversarial attacks focus on conducting LiDAR spoofing to forge a non-existent object in the detection results of a LiDAR-based object detector, or called \textit{appearing attacks}, on which we provide a detailed review in Section \ref{sec:attack_review}.

% A very recent line of works showed that PC models, including those 3D object detectors are vulnerable to \louis{adversarial attacks}. Xiang et al. suggested the first adversarial attack that \louis{against} PC models \cite{chong2019generating}. Tsai et al. and Hamdi et al. both focused on the transferability of adversarial samples, and suggested two different attack methods that the adversarial samples generated by them can affect the prediction of multiple PC classifiers at the same time \cite{abdullah2020advpc,yue2020onisomety}.