\section{Defense with Local Objectness Predictor}\label{sec:OurDefense}
\begin{figure*}[htpb]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/pipeline.png}
    % \caption{The pipeline of our local objectness predictor. The red boxes represent the ground truths of original training sample, the green box represent the predicted objects of 3D object detectors, and the blue box represent the equal-size pillars separated from input space.}
    \caption{The pipeline of our proposed defense. The input space is split into a number of equal-sized pillars (in the form of blue boxes). The red box in \raisebox{.5pt}{\textcircled{\raisebox{-1pt} {1}}} represents the bounding box of a ground-truth \louis{object during training}, while the green box in \louis{\raisebox{.5pt}{\textcircled{\raisebox{-1pt} {3}}}} represents
    \louis{that of a predicted object from the 3D object detector during testing.}
    % the predicted bounding box of a test object from the 3D object detector.
    }
    \label{fig:local_objectness_detector}
\end{figure*}

% In this section, we propose a local objectness predictor \louis{(LOP)} to detect the fake objects forged by existing appearing attacks.


% \subsection{Defensive Insights.}
% Our defensive insights are derived from a closer look at the attack process of an appearing attack. 

% \noindent\textbf{Attack Process.} Before the attack starts, the attacker deploys the physical equipment shown in Fig.\ref{fig:appearing_attack_equipment}. Once the photodiode receives the lasers emitted by the victim autonomous vehicle's LiDAR, it activates the delay component, which then controls the laser transmitter to emit lasers to the LiDAR for spoofing. In other words, the delay component determines the travel of time of emitted lasers, the laser transmitter determines the light intensity of emitted lasers, and the lens determines the direction of emitted lasers, the attacker can inject a certain amount of points within a certain area as his/her wish. Later, the LiDAR-based 3D object detectors of the victim takes the infected PC and predicts an inexistent vehicle as the attacker wishes. 


% The process is full physical and is therefore restricted by physical laws. 
% % Finally, the victim changes its future travel plan to avoid the inexistent vehicle, \louis{which might lead to} the traffic jam or even crash accidents.

% As is shown 


\noindent\textbf{Methodology Overview.} As shown in Fig.\ref{fig:local_objectness_detector}, the pipeline of our proposed defense can be divided into three stages: training sample generation, objectness predictor construction and fake object elimination.
In the training sample generation stage, we construct a learning task for our local objectness predictor (LOP), which consists of pairs of \louis{points inside a small local pillar and its corresponding} objectness label, annotated in a fully self-supervised way without additional annotation except for a standard training dataset for LiDAR-based object detectors.
Then, in the objectness predictor construction stage, we train the LOP to learn to predict the objectness score \louis{for each pillar}, i.e., the confidence of whether a \louis{local part} belongs to a real object.
% the possibility that indicates the existence of a small part of real object, for input PC. 
Finally, in the fake object elimination stage, we use our trained LOP to predict an objectness score for \louis{each small pillar} intersected with the bounding \louis{boxes of the predicted objects,}
and determine whether these objects are real by majority voting.
Below, we elaborate on the insights and the technical designs in each stage of our defense.

\subsection{Training Sample Generation}
\label{sec:method:preparation}


\subsubsection{Insight: Global Objectness $\neq$ Local Objectness} 
By inspecting the design of recent appearing attacks, we observe that most attacks focus on increasing the confidence scores of the forged obstacles, which represents the possibility of the detected object to be real.
Equivalently, according to our definition of objectness, the confidence score can be explained as a \textit{global objectness score} related with the predicted obstacle to some extent.
As most LiDAR-based object detectors by design keep those objects with higher confidence, or global objectness scores, in their final predictions, increasing confidence scores is the most direct way for the attacker to successfully forge a non-existent obstacle. 
However, to increase the global objectness score of a forged obstacle does not necessarily lead to a higher objectness score for each local part.
With the following experiments, we observe that most of the recent appearing attacks have ignored 
\louis{the local difference, i.e. the spatial distance of two corresponding subsets, between a real and a forged obstacle,}
% the differences \louis{of local parts between the real and} their forged objects, 
which leaves an exploitable \louis{trace} for the defender.

\noindent$\bullet$\textbf{ A Pilot Study.} As the description in Section \ref{sec:ThreatModel}, the mainstream appearing attacks all focus on forging cars, so we mainly validate the above observation on cars.
We first randomly sample one real car
from the training set of KITTI \cite{andreas2012kitti} and $1,000$ forged cars crafted by three mainstream appearing attacks \cite{yulong2019advSensor, jiachen2020towards, kaichen2021robust} (later described in Section \ref{sec:Experiment}).
Next, we translate the interior points of each ground-truth car and each forged car into its local coordinate system, rotated by the lead angle to the identical orientation.
Then, for the point set $S$ of the real car and $S'$ of each forged car, we measure the distance between them by using the chamfer distance\cite{chong2019generating} and the average square L2 distance of kNN as metrics. 
% The details of these two popular metrics are shown in Appendix \ref{appendix:metrics}.
% We then calculate the differences on PC between the real vehicle and each forged vehicle with the following two different measurements:

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/local_global.png}
    \caption{The local and global differences \louis{of PCs between} real and forged cars (\scriptsize{The grey bars inside denote the overlapping region.}).}
    \label{fig:local_global}
\end{figure}

Specifically, we collect points belonging to the real car as $S_R$. For each forged car, we first collect points belonging to it as $S_F$.
Then, we split the point space into equal-sized pillars $p_j$ (as in Fig.\ref{fig:local_objectness_detector}), and generate \louis{a point subset} $S_{F,j}= S_F \cap p_j$ for each pillar.
Finally, we calculate the global difference and local difference as follows:
\begin{equation}
    D_{\text{global}} = D(S_R,S_F) \label{equal:global}
    % & D_{max\_local} = max\{D(S_R,S_{F, j})\} & \label{equal:max_local}
\end{equation}
\begin{equation}
    D_{\text{avg\_local}} = \frac{1}{|\{S_{F,j}\}|}\sum D(S_R,S_{F,j}) \label{equal:avg_local}
\end{equation}
\begin{equation}
    D_{\text{half\_max\_local}} = \frac{1}{N_{\text{half}}}\text{Top}_{N_{\text{half}}}(\{D(S_R,S_{F,j})\}) \label{equal:half_max_local}
\end{equation}
where $S_R,S_F$ denote the two specific point sets defined above, $S_{F,j}$ denotes the point sets gathered from the separated pillars of $S_F$, $D\in\{D_C,D_k\}$ denotes the metric that we use to measure distance between two point sets,
$N_{\text{half}} = \lceil |\{S_{F,j}\}|/2 \rceil$ is half of the number of point subsets $S_{F,j}$,
and $\text{Top}_{k}(V)$ denotes the sum of the largest $k$ values in $V$.
% and $Top_{cnt}(V)$ is a function that calculate the sum of first $cnt$ large value in $V$.

As shown in Fig.\ref{fig:local_global}, the local differences of the forged cars are usually larger than the global differences in both
% two different metrics above
chamfer distance and average \louis{square} L2 distance of kNN 
(with all p-value less than $1.0\times 10^{-11}$ in Kolmogorov-Smirnov tests).
We further compare
the local difference and global difference
% $D_{\text{local}}$ and $D_{\text{global}}$
for each forged car, and find that if we choose $D_{\text{avg\_local}}$ as the local difference, there are $55.7\%$ forged cars have larger local difference on the chamfer distance metric, and $54.5\%$ forged cars have larger local difference on the average \louis{square} L2 distance of kNN metric.
If we choose $D_{\text{half\_max\_local}}$ as the local difference, $87.4\%$ forged cars have larger local difference on the chamfer distance metric, and $87.5\%$ forged cars have larger local difference on the average \louis{square} L2 distance of kNN metric.
\louis{Similar results are observed when we repeat the experiment above on several other real cars randomly sampled from the training set of KITTI.}

In summary, the experimental results imply that \textit{the local features do provide the defender with a \louis{trace} to distinguish between the real and forged cars.}
In fact, our insight also conforms to a recent work on enhancing the precision of LiDAR-based object detectors \cite{qi2020object}, where they suggest that with an appropriate strategy of spatial division, one small part of real objects can also contain rich enough spatial and semantic information to predict the category, bounding box and confidence score of its related object. 

% [Kolmogorov-Smirnov Test Result]
% avg_local:
% KstestResult(statistic=0.164, pvalue=1.8715496208415e-12)
% KstestResult(statistic=0.384, pvalue=2.0919998989157213e-66)
% half_max_local:
% KstestResult(statistic=0.307, pvalue=2.6373240994946432e-42)
% KstestResult(statistic=0.544, pvalue=2.0738160340030527e-136)

\subsubsection{Technical Designs} 
% \noindent$\bullet$ \textbf{ Technical Designs.} 
To facilitate the modeling of local object features, in the first stage we prepare a dataset ${D}_\text{obj}$ consisting of pairs of \louis{points in each pillar} from ground-truth objects \louis{and} an automatically annotated objectness label based on a standard training dataset for LiDAR-based object detectors (e.g., KITTI \cite{andreas2012kitti}).
Formally, we denote the training dataset as $D = \{(X_t, \{\mathbf{b}_k\}_{k=1}^{N_t})\}_{t=1}^{N}$, where $N_t$ denotes the number of ground-truth objects in the PC $X_t$, and $\mathbf{b}_k$ denotes the bounding box of the $k$-th ground-truth object in $X_t$.
First, we split the full $L\times W\times H$ 3D region which covers the input point clouds into a number of pillars $\{p_j\}$ with an equal size $l \times {w} \times {H}$, where $l=1m,w=1m$ in our implementation.
Then for each pillar $p_j$, we generate an input-output pair, which can be represented as $(pc_j,obj_j)$, as follows:

\noindent\textbf{Generating Input $pc_j$.} We directly collect the inside points of each pillar from the input PC $X_t$ to form the input feature $pc_j$, i.e., $pc_j = X_t \cap {p_j}$, \louis{composed of a batch of points' features $x_i$ inside $p_j$}.
To normalize the generated input, we constrain the size of $pc_j$ as $M_{pc}$, where $M_{pc}$ is a fixed hyper-parameter. For those $pc_j$ with a larger size, we randomly sample $M_{pc}$ interior points as its input. Otherwise, $pc_j$ is padded with $\vec{0}$ until the size constraint is satisfied.

\noindent\textbf{Generating Label $obj_j$.} We first calculate the 2D Intersection over Union (IoU), the ratio of the area of intersection region over that of union region, between $p_j$ and each ground-truth bounding box $\mathbf{b}_k$ on the x-y plane.
\louis{For each pillar $p_j$,} we keep the maximal IoU value over all ground-truth bounding boxes.
Finally, we compare the maximal IoU value with a fixed threshold $T_{\text{IoU}}$.
If this value is greater than $T_{\text{IoU}}$, we annotate $obj_j = 1$ to indicate that the pillar $p_j$ contains a local part of a real object,
or \louis{$obj_j=0$ otherwise.}
Iterating over all the PC inputs with the pillars, we finish the collection of the training set $D_\text{obj} = \{(pc_j, obj_j)\}$.
As an analogy to the training task of masked word prediction for pretrained language models \cite{jacob2019bert}, this process works in a fully self-supervised manner without any additional information.


\subsection{Objectness Predictor Construction}
\label{sec:method:lop}

\subsubsection{Insight: The Inimitable Depth-Density Law}
Meanwhile, we find that, because recent appearing attacks are designed to cause threats in the real world, they are inevitably limited by certain physical constraints imposed by both the attacker's goal and the attack apparatus.
As is introduced in Section \ref{sec:Limitation}, there exist physical upper bounds on the number of added points and the permissible distance between a fake object and LiDAR for recent appearing attacks.
Behind these two limitations, we find that the capability of recent appearing attacks is inherently restricted by the \textit{depth-density} law \cite{depth_density}:
with existing technology and methods, it is hard to imitate \louis{the real-world objects' relation} between the \textit{depth}, i.e. the distance between this object and the LiDAR, and the \textit{point density}, i.e. the ratio of the number of input points inside this object's bounding box over the volume of its bounding box.

\noindent$\bullet$\textbf{ A Pilot Study.} 
Similar to the reason introduced in Section \ref{sec:method:preparation}, we mainly validate the above observation on cars here.
We first randomly sample $1,000$ real cars from the training set of KITTI and $1,000$ forged cars crafted by the mainstream appearing attacks described in Section \ref{sec:Experiment}.
Then, we calculate the depth and point density for these objects based on their bounding boxes and the related points.
As shown in Fig.\ref{fig:depth_density}, the point density of real cars is approximately inversely proportional to their depth.
In contrast, the point density of the forged cars seems to be independent of the depth: they can have small depth and small point density simultaneously, while this seldom happens for real cars. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/depth_density.png}
    \caption{The distribution chart of the depth-density relation.
    \lyf{The blue points represent normal cars, the orange crosses represent forged cars, and the red rectangle shows the confounding region of the two.}
    }
    \label{fig:depth_density}
\end{figure}


Though differences exist between real and forged cars in terms of the depth-density relation, it is still hard to directly distinguish them by heuristic algorithms.
Due to the complexity of real-world environments, there exists the confounding region in the depth-density relation \louis{distribution} (highlighted in Fig.\ref{fig:depth_density}), which is mainly caused by some real cars occluded by others, with smaller depth and point density at the same time.
Besides, the complexity is further increased by errors such as the noise in LiDAR perception and the deficiency of attack equipment.
In other words, it can be improper to explicitly filter out any detected object based on the hand-crafted rules.
As a data-driven approach, we alternatively \louis{encourage} the LOP to actively learn to model the complicated depth-density relation of real objects, by further incorporating the depth information explicitly into the input feature of each pillar we derive in the first stage.

\subsubsection{Technical Designs}
At this stage, we augment the input features in our prepared training dataset $D_\text{obj}$ with the depth information.
Specifically, for each generated training sample $(pc_j, obj_j)$ in $D_\text{obj}$,
\louis{we expand the feature of each point in $pc_j$ from an original $4$-dim vector $x_i = (x,y,z,int)$ into a $7$-dim one $x_i' = (dx,dy,x,y,z,int,dep)$, where}
\louis{% $(x,y,z)$ is the point's 3D coordinates,
$(dx,dy)$ is the point's 2D relative coordinates to the center of its corresponding pillar in the x-y plane,
% $int$ is the point's intensity
and $dep=\sqrt{x^2+y^2+z^2}$ is the point's depth.} In our preliminary, we also experimented with an alternative design with no depth information explicit in the input feature. The practice would result in a LOP which is much less effective in distinguishing the forged objects from the real ones than using our current solution.

% \noindent$\bullet$ \textbf{ Technical Designs.} At this stage, we augment the input feature in our prepared training dataset $D_\text{obj}$ with the depth information. Specifically, for each generated training sample $(x_j, y_j)$ in $D_\text{obj}$, we expand the feature vector of each point in $x_j$ from $4$-dim feature vector $(x,y,z,int)$ into a $7$-dim feature vector $(dx,dy,x,y,z,int,dep)$, where $(x,y,z)$ represents the point's 3D coordinates, $(dx,dy)$ represents the point's 2D relative coordinates to the center of its corresponding pillar in x-y plane, $int$ represents the point's intensity and $dep=\sqrt{x^2+y^2+z^2}$ represents the point's depth.


To adaptively learn the depth-density relation for distinguishing real and forged cars or other obstacles, we implement the LOP $O$ with the architecture of an off-the-shelf backbone PC classifier (e.g., PointNet \cite{charles2017PN} or DGCNN \cite{yue2019DGCNN}),
considering their validated performance on many downstream 3D tasks.
Note that the \textit{negative samples} in $D_\text{obj}$, i.e. the generated samples with $obj_j=0$, are much more than the \textit{positive samples}, i.e. the generated samples with $obj_j=1$.
Thus, we delete a part of negative samples in random to keep data balance and ensure that the ratio of positive samples and negative samples does not exceed $1:1.5$.
To further alleviate the data imbalance problem, we also adopt the idea of focal loss \cite{tsungyi2017focalloss} in the learning objective of LOP:
\begin{align}
FL(p,y)=-\alpha_{fl}(1-p_y)^{\gamma_{fl}} log(p_y) \label{equal:focalLoss}
\end{align}
where the positive constants $\alpha_{fl},\gamma_{fl}$ ($\gamma_{fl}>1$) are the hyper-parameters of the focal loss, which are set by following the best practices in \cite{tsungyi2017focalloss}. Besides, $p_y$ is the probability of the $y$-th class returned by the predictor.



%%%%%%% BEGIN ASR-Precision Map
\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/pre_ASR_comp.png}
    \caption{The relation graph of defense effect (1-ASR) and precision on cars of PointPillars under attacks. 
    % \lyf{The defenses are represented in different colors, while the attacks are represented in different shapes.}
    "PointNet" and "DGCNN" refers to LOP's structure, \louis{with a boundary value $B$} used to distinguish real and fake objects as the description in Section \ref{sec:method:voting}."LPD" and "FSD" are two strategies for CARLO to calculate the anomalous ratio, and $M,\ k,\ R$, Threshold are the hyper-parameters of other defenses, which are all described in Section \ref{sec:Limitation}.
    }
    \label{fig:comp_atk_pre_asr}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/pre_ASR_comp_0.png}
    \caption{The relation graph of defense effect and precision on cars of (a) PointRCNN and (b) PV-RCNN under attacks.
    }
    \label{fig:comp_atk_pre_asr_0}
\end{figure}
%%%%%%%% END ASR-Precision Map

\subsection{Fake Object Elimination}
\label{sec:method:voting}
Finally, we leverage the LOP to calculate the objectness score for each pillar intersected with predicted objects, and determine whether these objects are real by a majority voting among the pillars. Specifically, we first divide the detection space into
% We first divide the detection space into
\louis{equal-sized pillars, translate the input PC into a series of point subsets inside these pillars and then augment their features, similarly to the former two stage.}

% turn them into input PCs similarly to the sample generating stage and the training stage.
Then we use the LOP to calculate a $0/1$ objectness score for each pillar.
%%%% Technically speaking, these objectness scores are shared among all the predicted objects.
For each object in the prediction of the 3D object detector,
we search for those pillars \louis{whose} 2D IoU on the x-y plane between itself and the predicted object's bounding box is greater than a specified threshold 
$\beta$,
and calculate the sum of their objectness scores \louis{as well as the ratio of this sum over the total number of related pillars.}
Finally, we recognize those objects with the ratio
% of this sum over the number of counted pillars
less than or equal to a boundary value $B$ as fake objects, and eliminate them from the prediction. 
% For more details, please refer to Algorithm \ref{alg:detecting} in Appendix \ref{appendix:FOE}. 
% The pseudo-code of this stage is described in Appendix \ref{appendix:stage3_code}.