\section{Introduction}\label{sec:Introduction}
% In the past decade, deep learning empowers ma.ny real-world application in, e.g., finance \cite{Heaton2016DeepLF}, healthcare \cite{Esteva2017DermatologistlevelCO} and transportation \cite{edward2016doctorai,jiaxuan2019anAutomated}, 
In automated driving systems (ADS), multiple deep neural networks (DNNs) are jointly deployed to provide key functionalities of localization, perception and planning, stimulating the recent development of automated transportation \cite{marco2016MLinTrack,sina2020practical,yunli2021MLEnabled}. The robustness of each DNN module is of key importance to the security of the whole ADS. A typical example is the \textit{perception} module, which relies on a vector of \textit{object detectors}, based on multiple sources like cameras and LiDARs \cite{LiDAR}, to predict the categories and locations of the obstacles around the ADS \cite{yulong2019advObj, gregory2019lasernet}. 
As the LiDAR point clouds (PCs) contain richer location information than the images from cameras, most commercial ADS, including Google's Waymo One \cite{Waymo, waymo_report} and Baidu's Apollo \cite{Apollo, apollo_code}, set LiDARs as the main sensors and rely on the detection results of \textit{LiDAR-based object detectors} for obstacle perception  \cite{alex2019pointpillars,shaoshuai2019pointrcnn,shaoshuai2020pvrcnn,gregory2019lasernet}.

%%%%%%% BEGIN ATTACK SCENARIO
\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/atk_scenario.png}
    \caption{Appearing attacks on LiDAR-based object detectors in ADS can cause severe traffic accidents by forging cars.}
    \label{fig:atk_scenario}
\end{figure}
%%%%%%%% END ATTACK SCENARIO


Differently using PC as the model input, LiDAR-based object detectors still share the common vulnerability against \textit{adversarial examples} \cite{Szegedy2014IntriguingPO,yulong2019advObj,kaichen2021robust}. In general, the attacker can spoof the LiDAR sensors with a limited number of perturbed/crafted  points to mislead the detector's prediction.
As a popular attack class, the \textit{appearing attack} aims at forging non-existent cars in the detection results to cause traffic jams and emergency braking \cite{hocheol2017illusion,yulong2019advSensor} (Fig.\ref{fig:atk_scenario}).
Despite the severity, existing defenses \cite{jiachen2020towards,qi2020object, zhongyuan2021shadowcatcher} either have strong prior assumptions on the undergoing attacks, or are biased by predefined heuristic rules, insufficient for handling complex driving scenarios (\S\ref{sec:Limitation}).

% %%%%%%%%%% BEGIN GENERAL DEFENSE
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.45\textwidth]{figs/general_defense_2.png}
%     \caption{The mechanism of our plug-and-play defense.}
%     \label{fig:general_defense}
% \end{figure}
% %%%%%%%%%%%% END GENERAL DEFENSE

\noindent\textbf{Our Work.} In this paper, we propose a novel plug-and-play defense for 3D object detectors, which, instead of constructing a more robust model,
adopts a \textit{local objectness predictor} (LOP) module to detect and eliminate forged obstacles from the original detection results. 
% (Fig.\ref{fig:general_defense}).
In general, our LOP is designed as a point-wise PC classifier \cite{charles2017PN++,yangyan2018PCNN,charles2017PN,yue2019DGCNN} which learns to predict each local part of a detected object with an \textit{objectness} score, i.e., the confidence of whether the local part belongs to a real object.
By systematizing \xqf{recent} appearing attacks, we develop the following defensive insights: 
\begin{enumerate}[leftmargin=*]
    \item \xqf{Recent} appearing attacks focus on increasing the confidence score of a fake detection result without considering the local difference between a real and a forged obstacle. Although an increased confidence score enhances the possibility of a non-existent obstacle to be detected by a 3D object detector, most appearing attacks leave the fake and the real obstacles locally distinguishable when inspected at the granularity of pillars or voxels (\S\ref{sec:method:preparation}) 
    \item Constrained by the physical capability of attack apparatus, appearing attacks are usually unable to forge a fake obstacle without violating some physical laws,
    especially the inimitable relation between the depth and the point density of real obstacles \cite{depth_density}.
    To pose real-world threats, the forged obstacles have to be close to the victim ADS, because otherwise they can be easily bypassed after the victim's re-routing.
    % \louis{In order to realize the attack, the forged obstacles have to be close to the victim ADS because otherwise they could be easily bypassed after the victim's re-routing.}
    % In terms of the attacker's goal, the forged obstacles have to be located near the victim ADS. Otherwise, the victim ADS can easily re-route and bypass the fake obstacles.
    Yet, constrained by the attack apparatus (e.g., a laser transmitter \cite{jiachen2020towards}), the attacker can only forge a limited number of points near the victim during one scan of the LiDAR, which could hardly reach the normal point density of a real car at a close distance (\S\ref{sec:method:lop}). 
    % \xqf{Besides, though there exists a defense method \cite{zhongyuan2021shadowcatcher} which also discovered a similar physical law between the depth and the point density of real obstacles, they decided to turn it into a heuristic design based on their observation, and we further prove that it is not robust enough compared with LOP in the later experiments (\S\ref{sec:Experiment:results:comp_attack}).}

\end{enumerate}

Concurrent to our defense, Hau et al. \cite{zhongyuan2021shadowcatcher} also notices the importance of the physical law in detecting forged obstacles, and presents a set of hand-crafted rules to eliminate the anomaly. Our work steps further by showing stronger robustness can be achieved if we exploit learning-based techniques to model the complicated physical laws. In fact, modeling the relation between the depth and the point density is rather challenging with hand-crafted rules. For example, although most of the real cars with smaller depth tend to have larger point density, those real cars occluded by others may also have smaller depth and point density simultaneously (Fig.\ref{fig:depth_density}). 
To address this challenge, we implement the LOP as a DNN-based point-wise PC classification model and explicitly incorporate the depth information of each point into its feature vector.
This substantially improves the modeling capability compared with using the original input feature for statistical outlier detection. 


Moreover, another technical challenge is the lack of no explicit annotation available for supervising the training of LOP in standard 3D object detection datasets. 
Inspired by a recent observation that a single part of the input already contains rich semantic information for a PC model to predict its related object's category and location \cite{qi2020object},
we construct a self-supervised learning task where the LOP learns to predict whether a pillar intersects with any bounding box of real objects based on the features of its inside points.
During the detection, we first divide the input 3D space into equal-sized pillars, then the LOP predicts an objectness score for each pillar intersected with a predicted object's bounding box. By majority voting on the local objectness predictions, our defense determines whether the object is real or fake (\S\ref{sec:method:voting}). 


% For experiments, we extensively evaluate our proposed defense against three known appearing attacks on three mainstream LiDAR-based object detectors, i.e., PointPillars \cite{alex2019pointpillars}, PointRCNN \cite{shaoshuai2019pointrcnn} and PV-RCNN \cite{shaoshuai2020pvrcnn} on the KITTI dataset \cite{andreas2012kitti} and on real-world PC data we collect from a driving test of the D-KIT Advanced with Velodyne-128 \cite{DKit} in a closed road environment. Moreover, we empirically validate that the effectiveness of our proposed LOP is robust to both the architecture design of the LOP and the type of the LiDAR-based object detector under guard, which further implies our defense is more general-purpose than existing defenses (\S\ref{sec:Experiment}).


% % with and without our LOP under different
% \louis{attacks}.
% Our results show that with the deployment of our LOP, \louis{at least} $70\%$ of the fake cars forged by existing appearing attacks are eliminated,
% while there are only less than $30\%$ forged cars are eliminated for the best previous defense in certain cases

% while the precision on cars is only degraded by less than $3\%$ in most cases,  which is far more less than other defenses. Furthermore, we also validate our defense on real-world PC data obtained from a D-KIT Advanced car with Velodyne-128 in a closed road environment.

% data0->不防御的情况下，受攻击的3D实体检测模型的precision范围
% data1->防御的情况下，受攻击的3D实体检测模型的precision范围
% data2->不防御的情况下，3D实体检测模型的正常precision范围
% data3->防御的情况下，3D实体检测模型的正常precision范围

\noindent\textbf{Our Contributions.} In summary, the key contributions of this work are as follows:

\noindent\textbullet\ 
% We for the first time inspect and systematize the common limitations of existing appearing attacks, 
We systematize the limitations of 
% existing 
\xqf{recent} appearing attacks in violating the physical invariants and propose a learning-based defense to detect the forged obstacles with anomaly in the relation between the depth and the point density for the mainstream LiDAR-based object detectors.
% which inspire us to derive a general defense method for a generic set of mainstream LiDAR-based object detectors under multiple attack techniques.

\noindent\textbullet\ We propose the design of our local objectness predictor (LOP) which learns to predict the confidence of whether a local object part belongs to a real object, and allows plug-and-play integration with different defense targets for enhancing robustness against popular appearing attacks. 

% Our LOP can be constructed with no external annotation and achieves effectiveness independent of the LOP architecture and the type of the LiDAR-based object detector under guard.

\noindent\textbullet\  Extensive evaluation on mainstream 3D detectors (i.e., PointPillars \cite{alex2019pointpillars}, PointRCNN \cite{shaoshuai2019pointrcnn} and PV-RCNN \cite{shaoshuai2020pvrcnn}) on the KITTI dataset \cite{andreas2012kitti} and on real-world PC data we collect from a driving test of the D-KIT Advanced with Velodyne-128 \cite{DKit} validate the advantages of our proposed defense under three popular attacks. For example, with the same-level trade-off in model utility, our proposed defense eliminates at least $70\%$ cars forged by most appearing attacks, while the best baseline method only eliminates the forged ones less than $30\%$.

\noindent\textbullet\ Moreover, we empirically validate that the effectiveness of our proposed LOP is robust to the architecture design of the LOP, the type of the defense target (including fusion models) which further implies our defense is more general-purpose than existing defenses. Besides, we also provide a preliminary study on the robustness of LOP against adaptive attacks. 

\noindent\textbullet\ 
% Meanwhile, we also validate our proposed defense on PC data gathered with a real car driven by ADS 
We further implement and evaluate the effectiveness of LOP in Apollo 6.0.0, an end-to-end open-source self-driving system, with closed-loop control in the LGSVL simulation tests, which validates the system-level usefulness of our proposed defense in both benign and adversarial scenarios.



% Meanwhile, under the same circumstances, our defense brings uniformly less decrease in AP/precision on cars than other defenses.
% {For example, our LOP increases a 3D object detectors' precision under normal circumstances or under attack.}