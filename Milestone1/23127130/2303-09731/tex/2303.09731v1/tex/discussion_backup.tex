\section{Discussion}\label{sec:Discussion}
Our current defense mainly focuses on appearing attacks, which form a popular attack class on LiDAR-based object detectors in ADS. Below, we discuss other attack classes which may threaten the detector's security and why our current work chooses the appearing attack as the main research target.


\noindent\textbf{\xqf{Other Potential Attacks}} \xqf{Fisrt of all, }disappearing attacks on ADS also belong to the family of adversarial examples \cite{dawn2018physical,shang2018shapeshifter,yue2019seeing}.
\xqf{However, }different from appearing attacks, a disappearing attack aims at hiding the existing objects from the prediction results of the victim 3D object detector. To accomplish this purpose, in a disappearing attack the adversary usually generates an object by 3D printing, and puts it on the road or near some objects to conduct the attack. 
Technically, the specific shape of the printed object is optimally chosen to guarantee the target detector would not recognize it or its neighbouring object in the prediction results, leading to its disappearance in the detector's output.

In existing literature, Cao et al. propose the first disappearing attack on ADS, and successfully hide the printed objects from the LiDAR-based detection system of Baidu's Apollo by modeling its preprocessing and postprocessing phases into differentiable functions \cite{yulong2019advObj}.
Furthermore, Tu et al. suggest a more general disappearing attack which breaks the state-of-the-art 3D object detectors including PointPillars and PointRCNN, and hide the car on which the printed object is positioned from the model's prediction results \cite{james2020physically}.
Recently, Cao et al. suggest a more powerful disappearing attack, MSF-ADV, which fools the image-based 2D object detectors and LiDAR-based 3D object detectors at the same time, and causes the fusion-based detection system of Baidu's Apollo to ignore the existence of the printed objects \cite{yulong2021invisible}.

\xqf{Besides, mis-categorization attacks are also one of the threatening adversarial attacks to ADS. A mis-categorization attack can be seen as the combination of a disappearing attack and a appearing attack, it aims at changing the predicted class of the target objects in the prediction results of the victim 3D object detector. In a certain degree, a mis-categorization attack can also be seen as the variation of a disappearing attack, since they both use the similar methods to manipulate the victim's prediction results to an existed objects.}

Compared with appearing attacks, we argue that a disappearing attack \xqf{or a mis-categorization attack are} less threatening in the real world because {they are both untargeted and single-shot} at the self-driving level.
\xqf{For both two attacks, }the attacker has to put a printed object on the road or near some objects in preparation, which means he/she cannot choose the victim ADS during the attack.
Moreover, the printed object can only take effect once because it might be destroyed or recognized by the people nearby after the first accident of each attack.
% On one hand, 
% to implement a disappearing attack, the attacker has to put the printed objects on the road or near some objects before the attack happens,
% which means the attack can not choose the victim ADS.
% On the other hand, after the first accident of disappearing attack, a printed object may be destroyed and the people nearby may realize that there is probably some problems with the printed object, so it is hard for the attacker to use the same printed object to cause the second accident.
In contrast, in an appearing attack the attacker can choose the victim to fire the laser and forge non-existent cars as he/she wishes. Meanwhile, it is hard for others to notice this attack because there is almost no evidence left in the accident scene.

\xqf{Although due to the attack goal, it is meaningless to deploy LOP to defend against disappearing attacks, it is still worthwhile to evaluate the performance of LOP under mis-categorization attacks. We evaluate the performance of 3D object detectors under mis-categorization attacks in Appendix \ref{appendix:miscategorization}, and prove that LOP is also robustness against such attacks.}

% \noindent\textbf{Threats of Backdoor Attacks.}
\xqf{At last, }Backdoor attack is another threatening attack class to DNN models, which injects one or multiple backdoors into the victim model by modifying the training data \cite{chen2017targeted,turner2019labelconsitent}, the parameters \cite{jacob2020backdoorCNN,adnan2020TBT} or the network structure \cite{ruixiang2020embarrass}. After the injection, the adversary can activate his/her specified model misbehaviors by feeding special inputs containing the related trigger pattern. However, to implement a backdoor attack on the victim ADS is unrealistic. For example, the attacker requires the permission of the victim's self-driving system, which only belongs to a few administrators or security operators.
% Besides, with such permission, the attacker is able to directly modify the outputs of any DNN models, or even any modules in the self-driving system.
% Besides, even with such permission, the attacker has no need to backdoor is able to modify any modules in the self-driving system, or even the outputs of DNN models directly.
% Therefore, we believe that it is hard for an attacker to acquire the permission, and,
% once getting the permission, the attacker does not need to implement such a backdoor attack because he/she can already manipulate the victim as his/her wish.

% If the input of target model doesn't activate any triggers, the target model will return the normal prediction as usual.
% Otherwise, the target model will return a specific prediction which is linked with the activated trigger by the attacker.

% In 2017, Chen et al. firstly apply backdoor attack in machine learning, and suggest a \louis{fixed-pattern} backdoor attack: the trigger in the target model will be activated by a fixed pattern in the input \cite{chen2017targeted}.
% Turner et al. suggest not to change the labels of backdoor samples \louis{added} into the target model's training set, and improve the invisibility of backdoor attack during the training stage of target model \cite{turner2019labelconsitent}.
% On the other hand, Li et al. and Pang et al. suggest dynamic-pattern backdoor attacks, where the trigger in the target model will be activated by different patterns for different inputs, and improve the invisibility of backdoor attack during the inference stage of target model \cite{li2019invisible,pang2020atale}.
% The studies above prove that backdoor attacks can manipulate the target model's output on specific tasks, without harming the model's performance on its main task or being noticed by others.


% he/she doesn't need to implement backdoor attack while he/she is able to manipulate the victim as his/her wish. 

\xqf{\noindent\textbf{Other Potential Defenses.}
Except those defense methods we mentioned in Section \ref{sec:Limitation}, the fusion strategy is also a choice to defend against appearing attacks. Different from most existing defenses, fusion strategy can also improve the performance of the object detectors by taking images as the auxiliary inputs. Furthermore, \cite{Hallyburton2021SecurityAO} has already proved that the Physical attack can not break most of the fusion model such as FCN, FPN and AVOD.}

\xqf{However, according to \cite{yulan2021deep}, the detection frequency of existing fusion models is usually lower than $15$ FPS, and we are afraid but current fusion models may not be suitable for real-time self-driving system due to the efficiency bottleneck. Besides, we argue that fusion models should be viewed as also one of the defense targets instead of the comparison groups to our proposed defense, since they are also by essence detectors. Due to the plug-in nature of LOP, we are further able to equip the fusion models with LOP to defend against appearing attacks. Furthermore, we evaluate the performance of one of the fusion models with our proposed defense under different appearing attacks in Appendix \ref{appendix:fusion}, and prove that LOP empirically improves the robustness of the fusion models.}

\noindent\textbf{Limitations and Future Directions.} 
% Although there may be potential adaptive attacks designed to break the LOP in the future, we argue that it is hard to implement in the real world. It is mainly because: 
% (i) Even the open-sourced ADS such as Apollo \cite{Apollo} keeps some small DNN models as black boxes. For example, there is a black-box DNN model used to locate the ADS in Apollo. The structure of this model is stored as an executable and linking file (*.so), and the parameter of this model is stored as an encrypted binary file (*.bin), which are difficult for attackers to decipher. Thus, it is also possible to protect the LOP by encrypting.
% (ii) The output of LOP is only a binary value to indicate the object is real or fake. Once the LOP is accessed as a black-box executable, it is generally hard for attacker to efficiently generate adversarial examples against our defense via optimization-based techniques.
% \morinop{(add discussion on adaptive attacks)}

\xqf{Although our proposed defense performs well in most cases in our experiments, there still exists some limitations of LOP to be solved:
(i) With the development of laser technology, the upper bound on the number of fake points generated by the attacker's equipment may keep increasing. In the future, the attacker may be capable to forge a car by copying all the points of a real car, which was almost impossible for LOP or other potential defenses to distinguish the difference.
(ii) By analyzing the false positives detected by LOP, we found that LOP performs not good enough on discriminating distant objects. Despite this, the existence of MOT module guarantees that erasing a real object in a few times will not harm too much on tracking its trajectory, and the influence of these distant objects on the route planning of ADS in a short time is limited. The further visualization and analysis of the main factor of these false positives are also shown in Appendix \ref{appendix:FP}.}


In this situation, it is almost impossible for a single LiDAR-based object detector and also the LOP to distinguish real and forged objects as their PCs are completely identical. Therefore, we believe that the fusion of multiple object detectors,
which combines multi-modal information from different sensor sources (e.g., cameras, LiDARs, radars) and different DNN models for a comprehensive prediction \cite{xiaozhi2017mv3d, gregory2019lasernet++, anas2021seqFusion},
is a promising way to alleviate the threats from the appearing attack, the disappearing attack or more unknown threats against object detectors. However, we should bear in mind that fusion is simply a mechanism which ensembles the capability of multiple DNN models and no more than such a mechanism. On one hand, to break the fused detector, the attacker needs to manipulate the outputs of all the component detectors, which means he/she has to modify the sampled data of multiple sensors towards the same attack goal without raising other modules' suspicion. On the other hand, to improve the robustness of each single component in the complex ADS is still meaningful and urging.  
For example, Cao et al. have already successfully broken a fused detector which combines image-based YOLO and Apollo's LiDAR-based object detector by disappearing attacks\cite{yulong2021invisible}.
% However, Cao et al. have already successfully broken a fused detector fusing YOLO and Apollo's LiDAR-based object detector \cite{yulong2021invisible},
% which proves that fusion is not the final solution to these latent attacks for ADS.
% Thus, we believe that our work also makes a little contribution to the security improvement of ADS.
Therefore, how to improve the robustness of a single object detector is crucial to the security of ADS, on which our current work focuses.

% Therefore, we believe that in order to eliminate the threat from appearing attack, other adversarial attacks or other attacks against object detectors, the most suitable way is to fuse multiple object detectors which take sampled data from different sensors, e.g. cameras, LiDARs and radars, as model's input \cite{xiaozhi2017mv3d, gregory2019lasernet++, anas2021seqFusion}.