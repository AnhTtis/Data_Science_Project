Dear Shepherd,

Thank you very much for your and other reviewers' suggestions on our recent submission!

First of all, we are very sorry for our carelessness and all the typos and grammar errors appeared in the system integration part of the Section 5.4 and the Appendix H. We have carefully revised these parts again, and we promise that there will be no more typos or grammar errors in these parts.

Besides, we are sorry that we don't clearly describe the details of our system integration experiments in our submission, such as how we perform these experiments, how we calculate the crash rate and why we perform the system-level experiments in such way. Specifically, to perform these experiments, we first select 5 different fake PCs which are located in front of the self-driving vehicle and can successfully forged the perception module of Apollo or those 3D object detectors for at least 1 frame in the previous experiments. Meanwhile, we use Apollo to record 3 traces of the self-driving vehicle moving in different maps under the manual control. Then, we inject the selected fake PCs into all the frames of these traces to generate 15 different poisoned traces. 

During the experiments, we run Apollo 6.0.0 system with and without LOP in the poisoned traces and visualize the future routes generated by the planning module of Apollo, which are shown as the green rectangles in front of the self-driving vehicle. We also assume that there is a car moving behind the self-driving vehicle at a constant speed as the potential crash target. The generated future routes represent the moving trajectories of the self-driving vehicle under the control of Apollo, which can help us determine whether the self-driving vehicle will suddenly brake in the corresponding circumstances, and indirectly indicate a potential crash.

As shown in the part (a) of Fig.13, the generated future route is extended to the crossroads, which means the self-driving vehicle will moving normally and stop at a red light under the instructions of the Apollo with our LOP. Therefore, we consider this situation as the ``no accident''. Meanwhile, as shown in the part (b) of Fig.13, the generated future route disappears for a while, which means the self-driving vehicle will falsely brake in the middle of the road, and crash with the imaginary car moving behind it as we mentioned above. Therefore, we consider this situation as the ``car crash''. We calculates the proportion of the ``car crash'' in the 15 poisoned traces as the crash rate, and report it in the Section 5.4.

The main reason why we calculate the crash rate in such a complex way is that we are facing some problems in bridging the LGSVL simulator and the Apollo, which are difficult to solve in a short time. Specifically, the regular bridging methods will directly send the ground-truth detection results of objects nearby the self-driving vehicle to Apollo. It means that it is impossible for appearing attacks to successfully forge objects, since the Apollo will bypass the perception module during the simulation.

Moreover, we think the details of these experiments are a little bit redundancy for our paper, so we incorporate the explanation above in the Appendix H of our submission instead. 

We provide the draft and the diff file which highlights the difference between this submission and the previous submission in the attachments. If you have any further suggestions on our system-level experiments or other parts of our work, please let us know. Thank you again for your valuable help and constructive suggestions:)  

Best Regards!