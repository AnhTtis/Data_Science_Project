% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.2 distribution.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[%
 reprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-2}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines
\usepackage{tabularx}

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}
\usepackage{float}

\newtheorem{theorem}{Theorem}[section]

\DeclareMathOperator{\SO}{SO}
\DeclareMathOperator{\vrelu}{VReLU}
\DeclareMathOperator{\trelu}{TReLU}
\DeclareMathOperator{\pfn}{PFN}
\DeclareMathOperator{\btn}{BTN}
\newcommand{\jhat}{{\ensuremath{\hat{\text{\textbf{\j}}}}}}
\let\barvec\vec
\renewcommand{\vec}[1]{{\mathbf{#1}}}

%%%% constants to define based on experiments!
\newcommand{\relativeimprovement}{\ensuremath{2.3\times}}
\newcommand{\nparticle}{30}
\newcommand{\embeddingdim}{3}
\newcommand{\pfnfeaturestotal}{12}
\newcommand{\phiwidth}{128}
\newcommand{\fwidth}{128}
\newcommand{\ntrain}{1M}
\newcommand{\nvalsig}{100k}
\newcommand{\nvalbg}{500k}

\begin{document}

\preprint{APS/123-QED}

\title{Rethinking $\SO(3)$-equivariance with Bilinear Tensor Networks}% Force line breaks with \\
%\thanks{A footnote to the article title}%



\author{Chase Shimmin}
\affiliation{
Department of Physics, Yale University% with \\
}%

\author{Zhelun Li}
%\email{zhelun.li@mail.mcgill.ca}
\affiliation{%
Department of Physics, McGill University
}%

\author{Ema Smith}
\affiliation{
Department of Physics, Yale University
}
\affiliation{%
Department of Physics, Cornell University
}%

\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}

Many datasets in scientific and engineering applications are comprised of objects which have specific geometric structure.
A common example is data which inhabits a representation of the group $\SO(3)$\  of 3D rotations: scalars, vectors, tensors, \textit{etc}.
One way for a neural network to exploit prior knowledge of this structure is to enforce $\SO(3)$-equivariance throughout its layers, and several such architectures have been proposed.
While general methods for handling arbitrary $\SO(3)$ representations exist, they computationally intensive and complicated to implement.
We show that by judicious symmetry breaking, we can efficiently increase the expressiveness of a network operating only on vector and order-2 tensor representations of $\SO(2)$.
We demonstrate the method on an important problem from High Energy Physics known as \textit{b-tagging}, where particle jets originating from b-meson decays must be discriminated from an overwhelming QCD background.
In this task, we find that augmenting a standard architecture with our method results in a \relativeimprovement\ improvement in rejection score.
\end{abstract}

%\keywords{Suggested keywords}%Use showkeys class option if keyword
                              %display desired
\maketitle

%\tableofcontents


\section{Introduction}\label{sec:intro}

In many Machine Learning (ML) applications, at least some of the data of interest have specific geometric structure.
For example, position measurements from LiDAR imaging, the configuration of atoms in molecular potentials, and measurements of particle momenta are all cases where the data are naturally represented as spatial 3-vectors.
However, classical Neural Network (NN) architectures are not well suited to this sort of data; for instance, the standard Multi Level Perceptron would require that all information, spatial or otherwise, must be collapsed into a flat list of features as input to the network.
In this case, the spatial nature of the data, while not lost, is not communicated \textit{a priori} nor enforced \textit{post hoc}.

More recently, developments in the field of Representation Learning have shown that \textit{equivariant} NNs are a natural way to accommodate structured data, and in many cases lead to substantially improved algorithms.
Very informally, a function (such as a NN) is called equivariant if the output transforms similarly to the input.

Convolutional Neural Networks (CNNs), are the prototypical example of this.
CNNs exploit the fact that image data can be most naturally represented as data on a discrete 2-dimensional grid.
This data structure is associated with the representation of the group of discrete translations.
The standard CNN layer takes advantage of this by operating on input grid (pixel) data with discrete translation operations, and returning outputs on a similar grid structure.
Because the output of each layer has the same representational structure as the input, it is straightforward to build very deep representations without destroying the prior spatial structure of the data, simply by stacking CNN layers.
The result, of course, is that CNNs have completely revolutionized the field of computer vision.

We specifically consider the case of continuous scalar and 3-dimensional vector point data, as may be encountered in many point-cloud datasets.
For these data, the natural group associated with their representation is $\SO(3)$, the set of 3D rotations.
Therefore, one strategy to incorporate this structure into a neural architecture is to enforce equivariance \textit{w.r.t.} $\SO(3)$, and several such architectures have been proposed~\cite{Thomas2018TensorFN,Kondor2018ClebschGordanNA,Weiler20183DSC}.
In general, these approaches achieve equivariance either by defining a spherical convolutional operation~\cite{Weiler20183DSC,Thomas2018TensorFN}, or by constraining the network's operations to maintain strict representational structure~\cite{Kondor2018ClebschGordanNA,Anderson2019CormorantCM}.

Our method follows the latter approach, but in a much simpler way.
Rather than concerning ourselves with arbitrary $(2\ell + 1)$-dimensional representations, we consider only a few physically relevant representations: scalars, vectors, and order-2 tensors.
For these three representations, it is straightforward to enumerate the options for linear neuron layers.
We also want our network to be able to exchange information between different representations.
The Clebsh-Gordon theory prescribed in other methods provides the most general method for projecting arbitrary tensor products between representations back into irreducible representations,
However, once again we take a similar approach, and instead introduce a simple \textit{Tensor Bilinear Layer}, a subset of the CG space that consists of commonly known and physically intuitive operations, such as the vector dot product and cross product.

Importantly, we propose a novel method that allows us to relax equivariance requirements when an axial symmetry is present, by allowing the global $\SO(3)$ symmetry to be locally broken down to $\SO(2)$.
These looser conditions allow us to design of models that enforce only the instantaneously relevant equivariance, and allows the network to learn more expressive functions at each layer.
We show that this kind of equivariant neuron is generally only possible with the introduction of order-2 tensor representations, but we provide an efficient implementation for vector-valued networks that constructs only the minimal tensors required.

To illustrate a real-world application to data with an axial symmetry, we introduce a common problem from the field of High Energy Physics (HEP), described in Sec.~\ref{sec:bjets}.
In Sec.~\ref{sec:elements}, we describe the modular elements of our method, from which a wide variety of neural architectures may be composed.
In Sec.~\ref{sec:arch}, we describe a specific architecture based on Deep~Sets~\cite{Zaheer2017DeepS} which will serve as a baseline model, and we illustrate how to adapt this architecture using our approach.
In Sec.~\ref{sec:exp}, we describe the simulated data used for training and evaluation, and describe the results of a progressive implementation of the modules developed herein.
Finally, we offer concluding remarks in Sec.~\ref{sec:conclusion}.

\section{B-jet Identification at LHC Experiments}
\label{sec:bjets}

In HEP experiments, such as \textsc{ATLAS}~\cite{ATLAS:2008xda} and \textsc{CMS}~\cite{CMS:2008xjf} at CERN, \textit{b-jets} are a crucial signal for studying rare phenomena and precision physics at the smallest scales of nature.
A \textit{jet} is a collimated spray of hadronic particles originating from energetic quarks or gluons produced in high energy particle collisions.
A \textit{b-jet} is a jet which specifically originates from a $b$-quark; when these quarks hadronize, they form metastable B-mesons which travel some distance from the collision origin before decaying, emitting particles from a secondary, displaced vertex.

Charged particles originating from these vertices are measured with tracking detectors and are often referred to as \textit{tracks}.
Due to the displacement of the secondary vertex, when track trajectories originating from B-meson decays are extrapolated backwards, they are generally not incident to the origin.
Therefore, we instead measure the distance to the point of closest approach; this is often referred to as the \textit{track impact parameter}, which is a 3-vector quantity that we denote with $\vec{a}$.

In most applications, only the transverse and longitudinal components, $d_0$ and $z_0$, of this impact parameter are examined~\cite{ATLAS:2021piz}.
The magnitude of these projections is the most distinctive feature that indicates whether a particular jet originated from a $b$-quark.

The inspiration for this work was the observation that the physical processes which govern how particles within a jet are produced and propagated are largely invariant with respect to rotations about the \textit{jet axis}, denoted $\jhat$.
$\jhat$ is the unit vector in the direction of the aggregate jet's momentum vector.
On the other hand the standard $b$-tagging observables $d_0$ and $z_0$ have no well-defined transformation rule under rotations, \textit{i.e.} they are not part of a covariant representation.

Previous works~\cite{Shimmin:2021pkm} have demonstrated that networks which exploit this natural $\SO(2)$ symmetry can greatly improve performance, but these methods all essentially rely on reducing the problem to vectors in a 2-dimensional plane.
In order to obtain an equivariant representation in the case of $b$-jets, we must consider the full 3-dimensional structure of the impact parameter, which transforms as a vector under general rotations $\vec{a} \overset{R}{\rightarrow} R\vec{a}$.
In addition to the 3-dimensional impact parameter $\vec{a}$, we also have information about the track's momentum $\vec{p}$ and various scalar quantities such as the particle's charge, energy, and a limited identification of the particle type.

In the next section, we will describe modular neural elements that can solve this problem, by allowing a network to admit a global $\SO(3)$ symmetry which preserves the scalar and vector representations, while also breaking $\SO(3)$ down to the more physically appropriate $\SO(2)$ whenever possible.

\section{Network Elements}\label{sec:elements}

Our proposed method depends on four modular elements, described in detail in the following subsections.
The overall strategy begins by mirroring what has proved to work for NNs in general: we interleave simple linear (or affine) layers with nonlinear activation functions, in order to learn powerful models.
For an equivariant network, we first need to identify a set of linear equivariant maps suitable for the symmetry at hand.
In our case, we come up with two sets of such maps: a global $\SO(3)$-equivariant affine layer, and a locally $\SO(2)_\jhat$-equivariant linear layer.

Since we also require our network to mix between its scalar, vector, and tensor representations, we introduce an equivariant \textit{bilinear} layer.
Lastly, we define $\SO(3)$ equivariant nonlinear activations for each output representation.

In Sec.~\ref{sec:arch}, we demonstrate how to combine these elements into a complete neural architecture.
This architecture is based on the Deep Sets~\cite{Zaheer2017DeepS} architecture suitable for variable-length, permutation-invariant data.
Given the modularity and strictly defined input and output representations of each layer, we anticipate that these elements could be used to augment other neural architectures such as convolutional, graph, and transformer as well.

\subsection{$\SO(3)$-equivariant Affine Layers}\label{subsec:affine}

A well-known way to ensure equivariance \textit{w.r.t.} any group is to broadcast the neural action across the representational indices of the data~\cite{Wood1996RepresentationTA,Finzi2021APM}.
That is, the neural weight matrix simply forms linear combinations of the features in their representation space.
In general, it is helpful to add a bias term, but care must be taken to select one that preserves equivariance.

The simplest example of this is for a collection of $F$ scalar input features, $\{ s_i \}$, mapping to a collection of $K$ output features.
The scalar has no representational indices, so this simply amounts to the standard affine\footnote{Also referred to as a Dense or Linear layer.} network layer
\begin{equation}
    y_i = W_{ij} s_j + b_i \,
\end{equation}
where the learnable parameters $W_{ij}$ and $b_i$ are the neural weights and bias terms, respectively.

In the vector case, we may generalize to
\begin{equation}
    \vec{y}_i = W_{ij} \vec{v}_j \,;\ \ \vec{b}_i = 0 \,.
\end{equation}
Here, the equivariance condition for vector-valued functions $f(R\vec{v}) = Rf(\vec{v})$ implies that $R\vec{b} = \vec{b}$ for arbitrary rotation $R$; hence, the bias vector must be zero and the operation is strictly linear.

Finally, we consider the analogous case for order-2 tensors:
\begin{equation}
    Y_i = W_{ij} T_j + B_i \,;\ \ B_i = b_i I \,,
\end{equation}
where again we have learnable scalar parameters $b_i$.
In this case, the equivariance condition is $f(RTR^T) = Rf(T)R^T$, which implies that $RBR^T = B$, \textit{i.e.} $B$ must commute with arbitrary $R$.
Therefore, $B$ must be proportional to the identity tensor $I$.

\subsection{$\SO(2)_{\jhat}$-equivariant Linear Layers}\label{ssec:so2layer}

The above layers are purely isotropic in $\SO(3)$.
However, as discussed in Sec.~\ref{sec:intro}, for our problem we have prior knowledge that the distribution is symmetric about a specific axis.
At worst, having only isotropic operations can over-regularize the network by imposing too much structure, and at best it might be harder for the network to spontaneously learn about the axial symmetry.
We therefore consider the most general linear map is equivariant \textit{w.r.t.} the axial symmetry.
Since this is a lesser degree of symmetry, the network should have greater freedom in choosing linear maps.

\subsubsection{Vector Case}
Let $\jhat$ be a unit vector (indicating the direction of the overall jet's momentum in our example application) which is instantaneously fixed per batch input.
The rotations about this axis define a proper subgroup $S_\jhat \subset \SO(3)$ where we identify $S_\jhat \cong \SO(2)$.
We therefore refer to this subgroup as $\SO(2)_\jhat \subset \SO(3)$; the distinction being that $\SO(2)_\jhat$ fixes a representation on $\mathbb{R}^3$ which depends on $\jhat$.

The set of all linear $\SO(2)_\jhat$-equivariant maps is exactly the set of matrices $A$ which commute with arbitrary $R_\jhat \in \SO(2)_\jhat$, which are of the form
\begin{equation}
\label{eq:so2-equivariant-linear}
    A = (a \jhat \jhat^T + b (I - \jhat \jhat^T)) R'_{\jhat}(\phi) \,,
\end{equation}
for arbitrary learnable parameters $\bar{\theta}=(a,b,\phi)$.
The first two terms represent anisotropic scaling in the directions parallel and perpendicular to $\jhat$, respectively.
The third term represents any other arbitrary rotation about the $\jhat$ axis, parameterized by a single angle $\phi$.

Because $A$ commutes with all $R_\jhat \in \SO(2)_\jhat$, the linear layer defined by
\begin{equation}
    \vec{y}_i = A(\bar\theta_{ij}) \vec{v}_j \,
\end{equation}
is $\SO(2)_\jhat$-equivariant, for arbitrary parameters $\bar\theta_{ij}$.

\subsubsection{Tensor Case}
In order for a tensor-valued linear map $L$ to be equivariant, we require that $L(R_{\jhat} T R_{\jhat}^T) = R_{\jhat} (LT) R_{\jhat}^T$.
Note that in the case of $\SO(3)$, the only option is for $L$ to be proportional to the identity.
Without loss of generality, we may assume the order-4 tensor $L$ can be written as a sum of terms $A \otimes B$ for some order-2 tensors $A,B$.
The tensor product acts on an order-2 tensor $T$ as $(A\otimes B)T := A T B^T$. Taking $L$ to be of this form (up to linear combinations), the equivariance condition reads ${A (R_\jhat T R_\jhat^T) B^T = R_\jhat (A T B^T) R_\jhat^T}$.
This is satisfied when both $A$ and $B$ commute with $R_\jhat$; we have already identified the set of such matrices in Eq.~\ref{eq:so2-equivariant-linear}. Therefore, we define the action of the tensor-valued $\SO(2)_\jhat$ linear layer by:
\begin{equation}
    Y_i = A(\bar\theta_{ij}) T_j B^T(\bar\varphi_{ij}) \,,
\end{equation}
where the six parameters $(\bar\theta, \bar\varphi)$ per connection parameterize the tensors $A,B$ which are of the form of Eq.~\Ref{eq:so2-equivariant-linear}.

\subsection{Tensor Bilinear Operations}\label{ssec:bilinearOp}

So far we have provided two means for working with data in the $\SO(3)$ scalar, vector, and order-2 tensor representations.
However, we also desire a means for allowing information between the different representations to be combined and mixed.

The most general approach to this is addressed by Clebsh-Gordon theory~\cite{Kondor2018ClebschGordanNA}.
But a more straightforward way to allow such mixing is to create bilinear combinations, between different representations as well as within representations.

The operations considered are enumerated schematically in Fig.~\ref{fig:bilinear}.
In order to form these terms, the bilinear layer requires that the scalar, vector, and tensor inputs $(s, \vec{v}, T)$ all have the same size, $2F$, in their feature dimension, and that the size is a multiple of two.
We then split the features into groups of two: $s_a = \{s_i\}_{i=1..F}$, $s_b = \{s_i\}_{i=F+1..2F}$, and define similarly $\vec{v}_{a,b}$ and $T_{a,b}$.

After effecting all of the options from Fig.~\ref{fig:bilinear}, the layer returns scalar, vector, and tensor outputs with $3F$ features each.


%\begin{table}[]
%    \centering
%    \begin{tabular}{|c||c||c|}
%    \hline
%    Scalar Resultant & Vector Resultant & Tensor Resultant \\ \hline \hline
%    
%    $s_a s_b$ & $(s_a + s_b)(\vec{v}_a + \vec{v}_b)$ & $(s_a+s_b)(T_a+T_b)$ \\ \hline
%    
%    $\vec{v}_a \cdot \vec{v}_b$ &  $\vec{v}_a \times \vec{v}_b$ & $\vec{v}_a \vec{v}_b^T$ \\ \hline
%    
%    $\langle T_a, T_b\rangle_F$ & $(T_a + T_b)(\vec{v}_a + \vec{v}_b)$ & $T_a T_b$ \\ \hline
%    \end{tabular}
%    \caption{The set of bilinear operations of each resultant type, constructed from scalar $s_{a,b}$, vector $\vec{v}_{a,b}$ and tensor $T_{a,b}$ inputs.}
%    \label{tab:bilinears}
%\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/bilinear_diagram.pdf}
    \caption{A schematic diagram of the bilinear layer with mixing between different representations.}
    \label{fig:bilinear}
\end{figure}

\subsection{$\SO(3)$-equivariant Nonlinear Activations}

For the scalar features, any function is automatically equivariant. Therefore, for these features we use the well-known ReLU\cite{Nair2010RectifiedLU} activation function, although any alternative nonlinearity would also work.

In the vector and tensor cases, care must be taken to ensure equivariance.
For the vector case, we state a simple theorem\cite{jog_2015}:
\begin{theorem}
\label{thm:vector-isotropic}
For any vector-valued function ${f: \mathbb{R}^3 \rightarrow \mathbb{R}^3}$ which satisfies $f(R\vec{x})=Rf(\vec{x})$ for all $R\in\SO(3)$, there exists a scalar function $\tilde{f}$ such that
\[
f(\vec{x}) = \tilde{f}(|x|) \hat{\mathbf{x}} \,,
\]
where $\hat{\mathbf{x}} = \vec{x}/|x|$ when $|x|>0$ and $\hat{\mathbf{x}}=\mathbf{0}$ otherwise.
\end{theorem}
In other words, we may chose an arbitrary, nonlinear function $\tilde{f}$ which acts only on the vector magnitude, and the layer must leave the direction of the input unchanged.
This leaves many possibilities; after some experimentation, we found the following activation, which we call Vector ReLU ($\vrelu$), works well:
\begin{equation}
    \vrelu(\vec{v}) := \begin{cases}
        \ \ \vec{v} & |v| < 1 \\
        \vec{v}/|v| & \text{else}
    \end{cases} \,.
\end{equation}
The $\vrelu$ activation is analogous to the standard rectified linear unit, except that the transition from linear to constant happens at a fixed positive magnitude rather than zero.
We found that in particular, the saturating aspect of $\vrelu$ greatly helps to stabilize training, as otherwise the vector features tend to coherently grow in magnitude, leading to exploding gradients.


For the order-2 tensor case, we note here that the tensor analog to Theorem~\ref{thm:vector-isotropic} is much more nuanced\cite{jog_2015}, and in general depends on three principal invariants $\mathcal{I}_1, \mathcal{I}_2, \mathcal{I}_3$.
For simplicity, we define the Tensor ReLU ($\trelu$) completely analogously to the vector case, and leave a more complete analysis of tensor nonlinearities to future work:
\begin{equation}
    \trelu(T) := \begin{cases}
        T & ||T||_F < 1 \\
        T/||T||_f & \text{else}
    \end{cases} \,.
\end{equation}

\section{Benchmark Architectures}\label{sec:arch}

We now have defined the four modular elements which provide the appropriate equivariant operations.
In order to evaluate the practical effects of these modules, we define a benchmark architecture that is based on the Deep Sets architecture\cite{Zaheer2017DeepS}, also referred to as a Particle Flow Network (PFN)~\cite{PF} in the field of HEP. The PFN is a commonly-used architecture for this sort of problem in real-world applications such as at the \textsc{ATLAS} experiment\cite{ATLAS:2021piz}.

We will first define the standard PFN architecture, which will serve as our baseline in experiments.
Then, we describe a modified version at the module level using the analogous equivariant operations in place of the standard neural network layers.

\subsection{Particle Flow Network}
\label{subsec:pfn}

The basic structure of the PFN that will serve as our baseline is of the form:
\begin{equation}
\label{eq:pfn}
    \pfn(\{p_k\}) = F\left( \sum_{k=1}^P \Phi(p_k) \right) \,.
\end{equation}
where $\Phi: \mathbb{R}^F \rightarrow \mathbb{R}^L$ and $F: \mathbb{R}^L \rightarrow Y$ are arbitrary continuous functions parameterized by neural networks.
$L$ is the dimension of the latent embedding space in which the particles are aggregated.
$Y$ represents the relevant output space for the task at hand; in our case, we will consider $Y=[0,1]$, the interval representing the class prediction of the network, where $0$ corresponds to background (QCD) and $1$ corresponds to signal (b-jets).

The input features $\{p_k\}$ represent the observed track particles within the jet.
These features include:
\begin{itemize}
    \item The jet 3-momentum in detector coordinates, $(p_T^{(J)}, \eta^{(J)}, \phi^{(J)})$
    \item The 3-momentum of each particle track in \textit{relative} detector coordinates, $(p_T^{k}, \Delta\eta^k, \Delta\phi^k)$
    \item The track impact parameters of each particle $(d_0^k, z_0^k)$
    \item The particle's charge $q$ and particle type class \{electron, muon, hadron\}
\end{itemize}
For each jet, we allow up to $\nparticle$ particle tracks; inputs with fewer than $\nparticle$ particles are padded with zeros.
Since there is only one jet 3-momentum per input, we repeat this value over the particle axis and concatenate its features along with the rest of the per-particle features.
The discrete particle type feature is passed to an embedding layer with output dimension $\embeddingdim$.
Therefore, after concatenating all features, the input to the PFN is of shape $(*, P, F)$ where $P=\nparticle$ is the particle dimension and $F=\pfnfeaturestotal$ is the feature dimension.

The subnetworks $\Phi$ and $F$ are simple fully-connected neural networks.
$\Phi$ consists of two hidden layers with $\phiwidth$ units each, and ReLU activation.
The output layer of $\Phi$ has $L$ units and no activation applied.

The $F$ network consists of three hidden layers with $\fwidth$ units each and ReLU activations.
The final output layer has two units with no activation, in order to train with a categorical cross entropy objective.

\subsection{Bilinear Tensor Network}

We now adapt the basic PFN architecture and promote it to what we call a Bilinear Tensor Network (BTN).
The overall architecture is of the same form as Eq.~\ref{eq:pfn}; we will simply modify the detailed implementation of the $\Phi$ and $F$ sub-networks.

The first change is that the network now takes as input features which belong strictly to one of the three $\SO(3)$ representations: scalar, vector, or order-2 tensor:
\begin{equation}
    BTN(\{ (s, \vec{v}, T )_k \}) = F\left( \sum_{k=1}^P \Phi(s_k, \vec{v}_k, T_k) \right)
\end{equation}
In general, the number of features in any of the representation channels are independent.
The features for the BTN experiments include:
\begin{itemize}
    \item The jet 3-momentum in Cartesian coordinates $(p^{(J)}_x, p^{(J)}_y, p^{(J)}_z)$
    \item The 3-momentum of each particle track $\vec{p}^k$
    \item The 3-position of the track's point of closest approach to the origin $\vec{a}^k$
    \item The charge and particle type of each track, as described in Sec.~\ref{subsec:pfn}
\end{itemize}
As before, we replicate the jet momentum across the particle index, and we embed the particle type into \embeddingdim\ dimensions, resulting in $F_s=4$ scalar and $F_v=3$ vector features.
Since there are no observed tensor features for particle tracks, we synthesize an initial set of features to act as a starting point for the tensor operations.
This is done by taking the outer product between all combinations of the three available vector features, resulting in $F_t=9$ features.

We now have $\Phi: \mathbb{R}^{F_s \times 3F_v \times 9F_t} \rightarrow \mathbb{R}^{L\times 3L \times 9L}$, where $F_s, F_v, F_t$ are the number of scalar, vector, and tensor inputs, respectively.
A single layer of $\Phi$ is formed as shown in Fig.~\ref{fig:BTN}, by combining in sequence the Affine, $\SO(2)_\jhat$-Linear, Bilinear, and Nonlinear modules described in Sec.~\ref{sec:elements}.
The network consists of two hidden and one output layer.
Each hidden Affine layer of the $\Phi$ network contains $2F=128$ features per representation, which results in $3F=192$ features after the Bilinear layer.
The output of the $\Phi$ sub-network had $L$ features, and there is no Bilinear or Nonlinear layers applied.

The $F$ network is built similarly to the $\Phi$ network, except that it has three hidden tensor layers.
In lieu of an output layer, after the hidden tensor layers, the $F$ network computes the square magnitude of each vector and tensor feature, in order to create a final set of $3\times3F$ scalar invariant features.
The scalar features are concatenated, passed through two more hidden layers with 128 units each and ReLU activations, and finally to an output layer with two units and no activation.


\begin{figure}
\centering
\includegraphics[scale=0.4]{figures/btn_particleflow.pdf}
\caption{A schematic diagram of the DeepSets-adapated Bilinear Tensor Network with. }
\label{fig:BTN}
\end{figure}


\section{Experiments}\label{sec:exp}
\subsection{Simulated Data}

To train b-tagging algorithms, we must use Monte Carlo simulated particle collision events, as this is the only way to get sufficiently accurate ground truth labels.
The optimization these algorithms is commonly studied by experiments such as \textsc{ATLAS} and \textsc{CMS}.
However, these experiments use highly detailed proprietary detector simulation software, and only limited amounts data are available for use outside of the collaborations.~\cite{cmsopendata}
There are also some community-generated datasets available for benchmarking~\cite{Qu:2022mxj}.
Unfortunately, none of these publically available datasets contain the key information that our method leverages for its unique equivariant approach.
Specifically, our model requires the full 3-dimensional displacement vector of each track's impact parameter, whereas the existing datasets only retain the transverse and longitudinal projections $d_0$ and $z_0$.

Therefore, we have created a new dataset for b-jet tagging benchmarks, to be made publically available.
The data is generated using standard Monte Carlo tools from the HEP community.

We begin by generating inclusive QCD and $t\bar{t}$ events for background and signal, respectively, using \textsc{Pythia8}\cite{Bierlich2022ACG}. \textsc{Pythia} handles sampling the matrix element of the hard processes at $\sqrt{s}=13 TeV$, the parton shower, and hadronization.
The hadron-level particles are then passed \textsc{Delphes}\cite{Favereau2013DELPHES3A}, a fast parametric detector simulator which is configured to emulate the CMS\cite{CMS:2008xjf} detector at the LHC.

After detector simulation, jets are formed from reconstructed EFlow objects using the anti-$k_T$~\cite{Cacciari:2011ma,Cacciari:2005hq} clustering algorithm with radius parameter $R=0.5$.
Only jets with $p_T > 90 \text{GeV}$ are considered.
For the signal sample, we additionally only consider jets which are truth-matched to a B-meson.
Finally, the highest-$p_T$ jet is selected and the track and momentum features are saved to file.

The training dataset consists of a balanced mixture of signal and background with a total of \ntrain\ jets.
The validation and test datasets contain \nvalsig\ signal jets each.
Due to the high degree of background rejection observed, we must generate a substantially larger sample of background events for accurate test metrics, so the validation and test datasets contain \nvalbg\ background jets each.


\subsection{Results}
To quantify the performance of our model, we consider the following metrics in our experiments. First, the loss function used in the training is sparse categorical cross entropy, which is also used in the validation dataset. We also consider the area under the ROC curve (AUC) as an overall measure of the performance in signal efficiency and background rejection. We also consider the background rejection at fixed efficiency points of $70\%$ and $85\%$, labeled by $R_{70}$ and $R_{85}$, respectively.
Background rejection is defined as the reciprocal of the false positive rate at the specified true positive rate.

A comparison of ROC curves for the best-performing models of each class (baseline, vector, and tensor) are shown in Fig.~\ref{fig:ROC}.
A summary of a variety of experiments is given in Table~\ref{tab:scores}.
The numbers in the table represent the median test score over 10 training runs, where the test score is always recorded at the epoch with the lowest validation loss.
The quoted uncertainties for the rejections are the inter-quartile range.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/ROC_curves.pdf} \\
    \includegraphics[width=0.9\linewidth]{figures/ROC_curves_relative.pdf}
    \caption{The top figure shows the test ROC curves for the best training run from each model class.
    The vertical dotted lines at 60\%, 70\%, 77\%, and 85\% efficiency correspond to the standard operating points for the \textsc{ATLAS} b-jet tagging algorithm~\cite{ATLAS:2019bwq}.
    The bottom figure shows the fractional improvement of each model relative to the baseline.
    }
    \label{fig:ROC}
\end{figure}

\begin{table*}[]
    \centering
    \begin{tabular*}{0.7\textwidth}{@{\extracolsep{\fill}}l|ccccc}
    Model & AUC & $\mathrm{R}_{70}$ & Impr.($\mathrm{R}_{70}$) & $\mathrm{R}_{85}$ & Impr.($\mathrm{R}_{85}$) \\
    \hline
    \hline
    
    Baseline & $0.9920$ &
    $298 \pm 85$ & -- &
    $85 \pm 16$ & -- \\
    
    \hspace{2ex}+Aug & $0.9906$ &
    $193 \pm 23$ & $-35\%$ &
    $66 \pm 11$ & $-22\%$ \\
    
    \hline
    \hline
    
    Vector & $0.9904$ &
    $166 \pm 21$ & $-44\%$ &
    $61 \pm 5$ & $-28\%$ \\
    
    \hspace{2ex}+Bilinear &  $0.9933$ &
    $346 \pm 142$ & $+16\%$ &
    $103 \pm 28$ & $+21\%$ \\
    
    \hspace{4ex}+$\SO(2)$ & $0.9943$ &
    $584 \pm 138$ & $+96\%$ &
    $150 \pm 26$ & $+76\%$ \\
    
    \hline
    
    Tensor & $0.9859$ &
    $149 \pm 10$ & $-50\%$ &
    $56 \pm 4$ & $-35\%$ \\
    
    \hspace{2ex}+Bilinear & $0.9945$ &
    $625 \pm 89$ & $+110\%$ &
    $158 \pm 17$ & $+86\%$ \\
    
    \hspace{4ex}+$\SO(2)$ & $\mathbf{0.9946}$ &
    $\mathbf{674 \pm 137}$ & $\mathbf{+126\%}$ &
    $\mathbf{164 \pm 18}$ & $\mathbf{+92\%}$ \\
    
    \end{tabular*}
    \caption{Test metrics for training experiments on progressive model architectures.
    \textit{Vector} and \textit{Tensor} correspond to the base vector and tensor models, including only the affine and nonlinearity modules. The results from the addition of bilinear layers is indicated by ``+BiL'', while ``+$\SO(2)$'' indicates the further addition of $\SO(2)$-linear layers.
    R$_{70}$ and R$_{85}$ indicate the test rejection at 70\% and 85\% signal efficiency, respectively.
    The percentage relative improvement in these metrics is also shown.
    Values shown are the median result over 10 training runs, per model type; errors quoted on rejection figures are the inter-quartile range.
    }
    \label{tab:scores}
\end{table*}

\subsection{Discussion}

Table~\ref{tab:scores} shows that the family of models with only vector representations can indeed improve over the baseline, provided that we include at least the bilinear layer allowing the vector and scalar representations to mix.
Moreover we find that adding the the $\SO(2)$ linear operations gives the vector network access to a minimal set of order-2 tensors, $R_\jhat$, $\jhat \jhat^T$, and $I$ to enable it to exploit the axial symmetry of the data.

In the case of the tensor family of models, there is a less substantial improvement when adding the $\SO(2)$ linear layer.
We expect that this is because the network with only bilinear operations is, at least in theory, able to learn the relevant operations on its own.
Nonetheless, there is some improvement when adding this layer, so it would be reasonable to include both unless computational constraints are a concern.

Finally, we note that neither family of models performs even as well as the baseline, when no bilinear operations are allowed.
This clearly demonstrates the effectiveness of a network which can mix $\SO(3)$ representations.

\section{Conclusion}
\label{sec:conclusion}
In this work, we have introduced four modules of neural network architecture that allow for the preservation of $\SO(3)$ symmetry.  The Bilinear Tensor Network (BTN) shows promising results in our dataset, yielding up to \relativeimprovement\ improvement in background rejection, compared to the Particle Flow baseline model. In our approach, the BTN outputs a scalar which is rotationally invariant. However, it is also possible to obtain a covariant output by simply not apply the scalar pooling operations.

Moreover, we show that second-order tensor representations are required in order to exploit a locally-resticted class of equivariance with respect to the axial rotations $\SO(2)_\jhat$.
When computational constraints are a concern, it is possible to recover most of the performance of the Bilinear Tensor Network, by restricting it to a faster Bilinear Vector Network with the appropriate $\SO(2)$ equivariant linear layer.

While the example application demonstrated here is of particular interest to the field of HEP, we expect our method can have great impact in other ares where vector-valued point cloud data is used.
Finally, we note that while we demonstrated the modular elements of the TBN on a simple Deep Sets / PFN type network, it should also be possible to use these modules for creating equivariant Graph and attention based networks.

\section{Acknowledgements}

We extend special thanks Dan Guest, who sparked the original idea for this project during a discussion with CS, and also provided helpful feedback on details about the methods and concerns of the LHC flavor tagging community.
We also thank Paul Tipton for giving support and feedback at all stages of this project.
We are grateful to
Alexander Bogatskii,
Arianna Garcia Caffaro,
Ian Moult,
Nathan Suri,
who provided helpful feedback on the method and the early manuscript.

Training experiments were made possible by the Grace GPU cluster
operated by Yale Center for Research Computing.
CS is supported by grant DE-SC0017660 funded by the U.S. Department of Energy, Office of Science

\newpage


% \begin{acknowledgments}
% We wish to acknowledge the support of the author community in using
% REV\TeX{}, offering suggestions and encouragement, testing new versions,
% \dots.
% \end{acknowledgments}

% \appendix

% \section{Appendixes}

% The \nocite command causes all entries in a bibliography to be printed out
% whether or not they are actually referenced in the text. This is appropriate
% for the sample file to show the different styles of references, but authors
% most likely will not want to use it.
% \nocite{*}


\bibliographystyle{unsrt}
\bibliography{apssamp}% Produces the bibliography via BibTeX.

\end{document}
%
% ****** End of file apssamp.tex ******
