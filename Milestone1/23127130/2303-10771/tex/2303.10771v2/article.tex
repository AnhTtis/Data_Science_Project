%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%%                                                                 %%
%%        Dictionary-based model reduction for state estimation    %%
%%                   Alexandre Pasco and Anthony Nouy              %%
%%                                                                 %%
%%                                                                 %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper,10pt,oneside]{article}
%
% This is a basic set of packages.  Feel free to use as many packages
% as you want, only the generated PDF will be submitted in the end.
%
\usepackage[utf8]{inputenc}
\usepackage[margin=25.4mm]{geometry}
\usepackage{titlesec}
\usepackage{amsmath,amssymb}
\usepackage{xcolor,graphicx}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsopn}
\usepackage[caption=false]{subfig}
\usepackage{bbm}
\usepackage{hyperref}
\hypersetup{hidelinks}
\usepackage[capitalize,nameinlink]{cleveref}

%
% title
%
\title{%
Dictionary-based model reduction for state estimation}

\author{%
  A. Nouy${}^{1}$,
  A. Pasco${}^{1}$}
%
\date{\medskip%
  \small %
  ${}^1$  Centrale Nantes, Nantes Université, Laboratoire de Mathématiques Jean Leray\\
  \texttt{\{Anthony.Nouy,Alexandre.Pasco\}@ec-nantes.fr}
  }

%%%% My shortchut commands
\newcommand\innerp[2]{\langle #1, #2 \rangle}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}} 
\newcommand{\argmax}{\mathop{\mathrm{argmax}}} 

\newcommand*{\hermtrans}{^{\mathsf{T}}}
\newcommand{\metricmat}{\mathbf{R}_{\mathrm{U}}}
%%%%

%%%% Theorems
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}% 
\newtheorem{corollary}[theorem]{Corollary}%

\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\newtheorem{definition}{Definition}%


\begin{document}

\maketitle
%\thispagestyle{fancy}


\begin{abstract}
We consider the problem of state estimation from $m$ linear measurements, where the state $u$ to recover is an element of the manifold $\mathcal{M}$ of solutions of a parameter-dependent equation. The state is estimated using a prior knowledge on $\mathcal{M}$ coming from model order reduction. Variational approaches based on linear approximation of $\mathcal{M}$, such as PBDW, yields a recovery error limited by the Kolmogorov $m$-width of $\mathcal{M}$. 
To overcome this issue, piecewise-affine approximations of $\mathcal{M}$ have also be considered, that consist in using a library of linear spaces among which one is selected by minimizing some distance to $\mathcal{M}$. 
In this paper, we propose a state estimation method relying on dictionary-based model reduction, where a space is selected from a library generated by a dictionary of snapshots, using a distance to the manifold. The selection is performed among a set of candidate spaces obtained from the path of a $\ell_1$-regularized least-squares problem. 
Then, in the framework of parameter-dependent operator equations (or PDEs) with affine parameterizations, we provide an efficient offline-online decomposition based on randomized linear algebra, that ensures efficient and stable computations while preserving theoretical guarantees.
\end{abstract}

\paragraph{Keywords.} inverse problem, model order reduction, sparse approximation, randomized linear algebra.

\paragraph{MSC Classification.}
65M32, % Numerical methods for inverse problems for initial value and initial-boundary value problems involving PDEs
62J07, % Ridge regression; shrinkage estimators (Lasso)
60B20, % Random matrices (probabilistic aspects)


\section{Introduction}
\label{sec:introduction}

This paper is concerned with state estimation (or inverse) problems that consist in the approximation (or recovery) of an element $u$ of a Hilbert space  $U$ using linear measurements and the additional knowledge that $u$ is the solution of a parameter-dependent equation \begin{equation}
\label{equ:general parametric problem}
    \mathcal{F}\big(u(\xi), \xi \big) = 0,
\end{equation}
for some unknown parameter $\xi$ in a parameter set $\mathcal{P}$. In other words, the sought state $u$ is an element of the so called solution manifold 
\begin{equation}
\label{equ:solution manifold}
    \mathcal{M}:= \{ u(\xi) : \xi \in\mathcal{P} \} \subset U,
\end{equation}
and we aim to approximate $u$ by using $m$ measurements $\ell_1(u)$, ..., $\ell_m(u)$, where the $\ell_i$ are continuous linear forms on $U$, along with some prior knowledge on $\mathcal{M}$ coming from model order reduction (MOR). 
%Knowing the measurements is equivalent to knowing the orthogonal projection of $u$ onto  ...

A variational approach called parameterized-background data-weak (PBDW) was proposed in \cite{madayParameterizedbackgroundDataweakApproach2015}. It relies on the assumption that $\mathcal{M}$ can be well approximated by a low-dimensional linear space $V$, which can be obtained by, e.g., principal component analysis or greedy algorithms. The PBDW state estimate is then obtained as the sum of an element from the \emph{background space} $V$ and a correction  
in the \emph{observation space} $W$ generated by the Riesz representers of the linear forms $\ell_i$. 
However, the background space and the measurements should be compatible for ensuring stability of the estimation, especially in the case of noisy observations \cite{madayPBDWStateEstimation2015}. More precisely, 
stability is related to the alignment of spaces  $V$ and $W$.
There are two main approaches to overcome this issue. The first is to select carefully an observation space that is adapted to the given space $V$, e.g., by selecting the linear forms from a dictionary using a greedy algorithm \cite{binevGreedyAlgorithmsOptimal2018}. In practice, this approach may not perform well if the number and type of measurements (e.g., location of sensors) are restricted. 
The second approach is to consider a different prior knowledge for $\mathcal{M}$, by constructing a space $V$ adapted to the measurements, e.g., using the optimization procedure from  \cite{cohenOptimalReducedModel2020a}, or 
 by taking advantage of a wider knowledge coming from the construction of $V$ by a greedy algorithm, as proposed in \cite{binevDataAssimilationReduced2017,herzetPerformanceGuaranteesVariational2020}. In the case of noisy observations, the authors in \cite{taddeiAdaptiveParametrizedBackgroundDataWeak2017} propose a  regularized version of PBDW, that consists in considering elements from a bounded subset of the background space $V$.

The above approaches are all based on linear approximation, which may poorly perform in cases where the manifold $\mathcal{M}$ can not be well approximated by a single  linear space of small dimension, that is characterized by a slow decay of its Kolmogorov $n$-width 
\begin{equation}
\label{equ:kolmogorov m width}
    d_n(\mathcal{M})_U := 
    \inf_{\text{dim} V = n }
    \sup_{u \in\mathcal{M}} \min_{v \in V}
    \|u - v \|_U,
\end{equation}
which is the best worst-case error we can expect when approximating   $\mathcal{M}$ by a $n$-dimensional subspace $V$ in $U$. 

Hence, there are incentives to consider non-linear approximations of $\mathcal{M}$, like the piecewise-affine approximation proposed in \cite{cohenNonlinearReducedModels2022}, which relies on a library of  subspaces among which one space is selected by minimizing some  distance to the manifold, or the  approach based on manifold approximation \cite{cohenNonlinearApproximationSpaces2022a}.  

\subsection{Contributions and outline}

Our first contribution is a state estimation strategy using dictionary-based reduced order models   \cite{balabanovRandomizedLinearAlgebra2021, kaulmannOnlineGreedyReduced2013}. It consists in using a library of background low-dimensional spaces generated by a large dictionary $\mathcal{D}_K$ of $K$ snapshots in $\mathcal{M}$. Our approach is an adaptation of the approach from    
\cite{cohenNonlinearReducedModels2022} to a setting where we have access to a combinatorial number $\binom{K}{n}$ of candidate background spaces of low dimension $n$. A benchmark error for this  model order reduction method is the nonlinear Kolmogorov width from  
\cite{temlyakovNonlinearKolmogorovWidths1998} defined by
\begin{equation}
\label{equ:non linear kolmogorov n width}
    d_n(\mathcal{M}, N)_U := \inf_{\#\mathcal{L}_n^N=N} 
    \sup_{u\in\mathcal{M}} 
    \min_{V \in \mathcal{L}_n^N}   \min_{v \in V}
    \|u - v \|_U,
\end{equation}
where the infimum is taken over all libraries $\mathcal{L}_n^N$ of $N$ subspaces of dimension $n$, with  $N = \binom{K}{n}$, that is $N \sim K^n$ in the regime $K \gg n.$ 
With this dictionary-based approach, it is expected to  select a low-dimensional  background space among a huge candidate set, yielding a good approximation of  $\mathcal{M}$ and a good stability of the estimation problem in the case where only a few measurements are available. 

 Our second contribution is to provide an efficient offline-online decomposition for our dictionary-based approach, in the framework of parameter-dependent operator equations (or PDEs) with affine parametrization. Indeed, classical offline precomputations for residual-based quantities often lead to prohibitive offline costs when it comes to dictionary-based approaches, as pointed out in \cite{kaulmannOnlineGreedyReduced2013, balabanovRandomizedLinearAlgebra2021}. They also tend to be sensible to round-off errors. For solving these issues, and following \cite{balabanovRandomizedLinearAlgebra2019}, we rely on randomized linear algebra (RLA), that allows for efficient and stable computations, with theoretical guarantees. 

The outline is as follows.  
First in \cref{sec:the onespace problem} we recall the original PBDW approach from \cite{madayParameterizedbackgroundDataweakApproach2015}, which was called the one-space problem in \cite{binevDataAssimilationReduced2017}. We also recall the error bounds for this approach, as well as its limitations.

Then in \cref{sec:the multi-space problem} we describe a general multi-space problem, in which the background space $V$ is selected adaptively among a library $\mathcal{L}_n^N$ containing $N$ subspaces of dimension at most $n$. The space is selected using a surrogate distance $\mathcal{S}$ to the manifold $\mathcal{M}$ and the associated recovery error is controlled, under certain assumptions, by the best recovery error among $\mathcal{L}_n^N$. This approach was introduced in \cite{cohenNonlinearReducedModels2022} for a specific choice of library. Here, it is presented in a general setting. 

In \cref{sec:randomized multi-space problem}, we present a randomized version of the selection method for the general multi-space problem, based on RLA techniques for the estimation of $\mathcal{S}$. We prove that our randomized approach provides a priori error bounds similar to the non-randomized approach with high (user-defined) probability. This approach makes feasible an offline-online decomposition, with reasonable offline costs, low online costs and robustness to round-off errors. These computational aspects are developed in the next section.

In \cref{sec:dictionary approach for inverse problem}, we consider a dictionary-based multi-space problem, in which the library contains all the subspaces generated by $n$ elements of some dictionary $\mathcal{D}_K$ of size $K$. More precisely, for computational reasons, we use adaptive libraries obtained from the path of solutions of $\ell_1$-regularized optimization problems. In the framework of parametric operator equations (or PDEs), the randomized selection from the previous section allow us to select efficiently a good subspace among those generated, with an associated error  controlled by the best error among the adaptive library.

Finally in \cref{sec:numerical examples} we test our approach on two different numerical examples. First a classical thermal-block diffusion problem, then an advection-diffusion problem.


\subsection{Setting and notations}
Instead of referring to the measurements $\ell_1(u)$, ..., $ \ell_m(u)$, we will equivalently  consider the orthogonal projection $w = P_W u $ of the state $u$ onto the observation space 
\begin{equation}
\label{equ:observation space def}
    W := \text{span} \{ R_U^{-1} \ell_i: 1\leq i\leq m \} \subset U,
\end{equation}
where $R_U : U \rightarrow U'$ is the Riesz map. We assume (w.l.o.g.) that the linear forms are linearly independent, which implies that the space $W$ is of dimension $m$. The problem of state estimation is equivalent  to finding a recovery map $A : W \to U$ which associates to an observation $w \in W$ an element  $A(w) \in U$. 

Our methodology is valid for real or complex Hilbert spaces, but for the sake of simplicity, we restrict the presentation to the real case. 
In practice, although most of our results are valid for a general Hilbert setting, the space $U$ is of finite  dimension $\mathcal{N}$. In the context of PDEs, the space arises from some discretization (e.g., based on finite elements, finite volumes...) and its dimension $\mathcal{N}$ is usually very large to ensure high fidelity towards the true solution. The space $U$ and its dual $U'$ are then identified with  $\mathbb{R}^{\mathcal{N}}$, respectively endowed with inner products 
$\langle\cdot, \cdot\rangle_U = \langle\cdot, \metricmat \cdot \rangle_{\ell_2}$ and $\langle\cdot, \cdot\rangle_{U'} = \langle \cdot, \metricmat^{-1} \cdot \rangle_{\ell_2}$, where $\langle \cdot, \cdot \rangle$ denotes the canonical $\ell_2$-inner product in $\mathbb{R}^{\mathcal{N}}$, and where $\metricmat\in\mathbb{R}^{\mathcal{N}\times \mathcal{N}}$ is the positive definite symmetric  matrix associated with the Riesz map $R_U$. Discrete representations of operators and vectors will be systematically written with bold notations.


\section{The one-space problem, or PBDW}
\label{sec:the onespace problem}

The one-space problem was initially called Parameterized-Background Data Weak (PBDW) and formulated in \cite{madayParameterizedbackgroundDataweakApproach2015}. We denote by $A_V : W \rightarrow W+V$ the corresponding recovery map for some background space $V$. In this section, we consider that the only knowledge we have on $\mathcal{M}$ is that it is well approximated by a $n$-dimensional space $V_n$, with
\begin{equation*}
    \text{dist}(V_n, \mathcal{M}) = \sup_{u\in \mathcal{M}} \min_{v\in V_n} \Vert u - v \Vert \leq \varepsilon_n.
\end{equation*}
Note that for the one-space problem, the precision $\varepsilon_n$ is not required to be known to compute the state estimate, while it is the case in the nested multi-space problem in \cref{subsec:nested multi-space}.

\subsection{Variational formulation}
\label{subsec:variational formulation}

The PBDW problem was first formulated as a variational problem. For an observation $w = P_{W} u$, the recovery $A_{V_n}(w)$ is obtained by selecting the background term $v^*\in V_n$ which requires the smallest update $\eta^*\in W$. In other words,
\begin{align}
\label{equ:pbdw ls formulation}
 A_{V_n}(w) := v^* + \eta^* \quad \text{with} \quad \left\{\begin{aligned}
    &v^* := \arg\min_{v\in V_n} \|P_{W} (u - v)\|_U \\
     &\eta^* := P_{W} (u - v^*)
    \end{aligned}\right..
\end{align}
In a finite dimensional setting, with $U = \mathbb{R}^{\mathcal{N}}$, the problem \eqref{equ:pbdw ls formulation} has the following  algebraic form:
\begin{align}
\label{equ:pbdw ls formulation algebraic}
A_{V_n}(w) := v^* + \eta^* \quad \text{with} \quad \left\{
\begin{aligned}
%    & {A}_{V_n}(w) := \mathbf{V v^* + W \boldsymbol{\eta}^*}, \\
    & v^* = \mathbf{V} \mathbf{v}^*, \quad \mathbf{v}^* := \arg\min_{\mathbf{v} \in \mathbb{R}^n} 
    \|\mathbf{C} \mathbf{v} - \mathbf{w}\|_{2},& \\
    & \eta^* =\mathbf{W} \boldsymbol{\eta}^*, \quad \boldsymbol{\eta}^* := \mathbf{w} - \mathbf{C} \mathbf{v}^*,&
\end{aligned}
\right.
\end{align}
where $\mathbf{V} \in \mathbb{R}^{\mathcal{N} \times n}$ and $\mathbf{W} \in \mathbb{R}^{\mathcal{N} \times m}$ are the matrices whose columns form an orthonormal basis of $V_n$ and $W$ respectively, $\mathbf{C = W\hermtrans \metricmat V} \in \mathbb{R}^{m\times n}$ is the cross gramian-matrix, and $\mathbf w \in \mathbb{R}^{m}$ is such that $w = \mathbf{W w}$. Problem \eqref{equ:pbdw ls formulation} is well-posed if and only if $V_n ~\cap~ W^{\perp} = \{0\}$. This is equivalent to 
$\sigma_{\text{min}}(\mathbf{C})>0$ with $n\leq m$. For the PBDW estimation, we assume that this condition is satisfied.
  The analysis from \cite{binevDataAssimilationReduced2017} then ensures the error bound
\begin{equation}
\label{equ:pbdw error bound}
    \|u - A_{V_n}(w)\|_U \leq \mu(V_n, W) \varepsilon_n,
\end{equation}
where $\mu(V_n, W)$ is the inverse of the smallest singular value $\beta(V_n, W)$ of the projection operator $P_{W}$ restricted to $V_n$. These constants are defined by
\begin{align}
\label{equ:pbdw infsup constants}
\begin{aligned}
    \mu(V_n, W) &:= \sup_{\eta \in W^{\perp}} \frac{\|\eta\|_U}{\|\eta - P_{V_n} \eta\|_U} 
    = \sigma_{\text{min}}(\mathbf C)^{-1}, \\
    \beta(V_n, W) &:= \inf_{v \in V_n} \frac{\|P_{W} v\|_U}{\|v\|_U} 
    = \inf_{v \in V_n} \sup_{w \in W} \frac{\langle v, w \rangle_U}{\|v\|_U \|w\|_U} 
    = \sigma_{\text{min}}(\mathbf C).
\end{aligned}
\end{align}

\subsection{Geometric formulation and optimality}
\label{subsec:Geometric formulation and optimality}

The geometric interpretation of the problem described in \cref{subsec:variational formulation} was provided and analysed in \cite{binevDataAssimilationReduced2017}. In the one-space problem, the manifold can be considered as the  cylinder $\mathcal{K}^n$ defined by
\begin{equation}
\label{equ:definition cylinder around manifold}
    \mathcal{K}^n := \{ v \in U : \text{dist}(v, V_n) \leq \varepsilon_n \}.
\end{equation}
Given an observation $w = P_W u $, all the knowledge we have about $u$ is that $u \in \mathcal{K}^n_w$, where 
\begin{equation}
\label{equ:observation ellipsoid}
    \mathcal{K}^n_w := \mathcal{K}^n \cap  U_w, \quad 
    U_w := w + W^{\perp}.
\end{equation}
The optimal recovery map, in the sense of the worst case error in $\mathcal{K}^n$, is the one mapping $w$ to the Chebyshev center of $\mathcal{K}^n_w$, where the Chebyshev center is defined for any bounded set $\mathcal{X} \subset U$ by
\begin{equation}
\label{equ:Chebyshev center}
    \text{cen}(\mathcal X) := \argmin_{v \in U} \inf \{ r: \mathcal X \subset \overline{\mathcal{B}(v, r)} \},
\end{equation}
which is well defined and unique. It has been shown that $A_{V_n}$ is the optimal map when the manifold is the cylinder $\mathcal{K}^n$. In other words $A_{V_n}(w) = \text{cen}(\mathcal{K}^n_{w})$ and
\begin{equation}
    A_{V_n} = \min_{A:W \rightarrow U} \sup_{u \in \mathcal{K}^n} \|u - A(P_W u)\|_U,
\end{equation}
where the minimum is taken over all linear maps from $W$ to $U$. In \Cref{fig:pbdw geometric} we illustrate  this geometric formulation. 


\begin{figure}[!ht]
    \centering
    \subfloat{\label{fig:a}\includegraphics[page=1, width=0.4\textwidth]{FIGURES_Dictionary_MOR_for_state_estimation.pdf}}
    \hspace{2cm}
    \subfloat{\label{fig:b}\includegraphics[page=2, width=0.21\textwidth]{FIGURES_Dictionary_MOR_for_state_estimation.pdf}}
    \caption[PBDW geometric]{\footnotesize Geometric interpretation of the PBDW recovery for one observation $w=P_W u$, with dim$(W)=2$ and dim$(V)=1$.}
    \label{fig:pbdw geometric}
\end{figure}

The one-space problem  may have bad performances when the background space $V_n$ is not well aligned with the observation space $W$, which leads to a bad $\mu(V_n, W)$, or when the manifold cannot be well approximated by a single linear space, which leads to a bad $\varepsilon_n$. The approximation by a single subspace is inherently limited by the Kolmogorov $m$-width via the bound $\varepsilon_n \geq d_m(\mathcal{M})_U$ because $n \leq m$.

\section{The multi-space problem}
\label{sec:the multi-space problem}

Several approaches have been proposed for addressing the issues of the one-space approach \cite{cohenOptimalReducedModel2020a, herzetPerformanceGuaranteesVariational2020,cohenNonlinearReducedModels2022}.
These are \emph{multi-space} approaches that consist in exploiting a library of $N$ spaces $V_1$, ..., $V_N$.

First in \cref{subsec:nested multi-space} we present the multi-space approach as first described in \cite{binevDataAssimilationReduced2017}. Then in \cref{subsec:general approach} we present the multi-space approach proposed in \cite{cohenNonlinearReducedModels2022}, with a selection method based on some surrogate distance to the manifold, and recall the associated oracle error bound which holds not only in the piecewise-affine framework considered in \cite{cohenNonlinearReducedModels2022}. This piecewise affine framework is considered in \cref{subsec:piecewise multi-space}. Finally in \cref{subsec:parametric pdes}, we focus on the case where problem \eqref{equ:general parametric problem} is a parameter-dependent PDE, which provides (under suitable conditions) a natural residual-based surrogate distance.


\subsection{Nested multi-space}
\label{subsec:nested multi-space}
The library corresponding to the multi-space approach from \cite{binevDataAssimilationReduced2017} is constituted of a nested sequence of subspaces with increasing approximation powers. In this subsection, we consider a library composed of $N = n \leq m$ nested subspaces such that 
\begin{equation*}
    V_1 \subset \cdots \subset V_n,
    \quad \text{dim}(V_j) = j,
    \quad \text{dist}(\mathcal{M}, V_j) \leq \varepsilon_j,
    \quad 1\leq j\leq n.
\end{equation*}
For an observation $w\in W$ we consider the compact set
\begin{equation}
\label{equ:multi-space observed ellipsoid}
    \mathcal{K}^{\text{mult}}_w := U_w \cap \mathcal{K}^{\text{mult}},
    \quad \mathcal{K}^{\text{mult}} := \bigcap_{j=1}^n \mathcal{K}^j, \quad U_w := w + W^{\perp}.
\end{equation}
where the cylinders $\mathcal{K}^j$ for $1\leq j\leq n$ are defined by \eqref{equ:definition cylinder around manifold}, as for the one-space problem. The optimal recovery map for the worst case error would be the map $w \mapsto \text{cen}(\mathcal{K}^{\text{mult}}_w)$. However, contrary to the one-space problem, this optimal map is not computable in general. In \cite{binevDataAssimilationReduced2017}, the authors propose to select any map $A^{\text{mult}}$ such that 
\begin{equation}
\label{equ:nested multi-space map 1}
    A^{\text{mult}}(w) \in \mathcal{K}^{\text{mult}}_w.
\end{equation}
They showed that any map satisfying \eqref{equ:nested multi-space map 1} satisfies the oracle error bound
\begin{equation}
\label{equ:nested multi-space map 1 error bound}
    \sup_{u\in\mathcal K^{\text{mult}}} \|u - A^{\text{mult}}(P_W u)\| \leq 
    \sqrt{2} \min_{1\leq j \leq n} \mu(V_j, W) \varepsilon_j.
\end{equation}
They also propose numerical algorithms to compute such a map. It is important to note that the proposed algorithms require to know the widths $(\varepsilon_j)_{1\leq j\leq n}$ to be performed, whereas it is not the case for the one-space problem. This approach allows to take advantage of the best compromise possible between $\varepsilon_j$ and $\mu(V_j, W)$ among all the available spaces. However it is not well adapted to problems with slow decay of Kolmogorov $n$-width, similarly as the one-space problem. 


We end this subsection by citing the works from \cite{herzetPerformanceGuaranteesVariational2020} which also consider the nested multi-space framework. The authors propose a recovery map $A$ such that $A(w) \in V_n \cap \mathcal{K}^{\text{mult}}$ minimizes the norm of the update term $ w - P_W A(w)$ . Note that \cite{herzetPerformanceGuaranteesVariational2020} also extends the multi-space problem to the case $m<n$ and provides error bounds, ensuring stability when increasing the knowledge with $n-m$ new spaces with better approximation power, although the error bounds provided are not sharp. We also cite the works from \cite{cohenOptimalReducedModel2020a} in which the background space is obtained via a convex optimization problem.

\subsection{General approach}
\label{subsec:general approach}

The general multi-space approach consists in using not a single space $V_n$ as background, as for the initial PBDW approach, but rather a library 
\begin{equation*}
    \mathcal{L}_n^N := \{V_1, \cdots, V_N\}
\end{equation*}
of $N$ subspaces of dimension at most $n$. We suppose that
\begin{equation*}
    n_k := \text{dim}(V_k) \leq n \leq m,
    \hspace{5mm} 1\leq k\leq N.
\end{equation*}
The benchmark error for this non-linear model order reduction method is the library-based non linear Kolmogorov $n$-width from \cite{temlyakovNonlinearKolmogorovWidths1998} defined by \eqref{equ:non linear kolmogorov n width}.
For some $N=N(n)$, the library width $d_n(\mathcal{M}, N(n))_U$ may have a (much) faster decay with $n$  than $d_n(\mathcal{M})_U$, so that a small error may be obtained with spaces of low dimension $n$, which can be crucial for our state estimation problem. 
%In particular, \cite{temlyakovNonlinearKolmogorovWidths1998} identifies $N(n) \sim n^{\nu n}$, for some $\nu \geq 1$, as a case of interest. 
To each subspace is associated the one-space recovery map
\begin{equation}
\label{equ:one map in multi-space}
    A_k := A_{V_k}.
\end{equation}
For a given observation $w$, it was proposed in \cite{cohenNonlinearReducedModels2022} to select one of those maps by minimizing some surrogate distance to the manifold $\mathcal{S}(\cdot, \mathcal{M})$. The idea is to select $k^* = k^*_{\mathcal{S}}(w)$ whose associated one-space recovery is the closest to the manifold, in the sense that it minimizes $\mathcal{S}(\cdot, \mathcal{M})$. This recovery is then denoted as $A^{\text{mult}}_{\mathcal{S}}(w)$. In other words,
\begin{equation}
\label{equ:multi-space space selection}
    A^{\text{mult}}_{\mathcal{S}}(w) := A_{k^*}(w)
    \quad\text{with} \quad 
    k^* \in \argmin_{1\leq k\leq N} ~ \mathcal{S}(A_k (w), \mathcal{M}).
\end{equation}
The best choice for $\mathcal{S}$ would be the distance based on the $\|\cdot \|_U$ norm. However, this approach may not be computationally feasible in most practical cases where $\mathcal{N}$ is large and elements in $\mathcal{M}$ are expensive to compute or store. Still, assuming that $\mathcal{S}(\cdot, \mathcal{M})$ controls the true distance to $\mathcal{M}$, interesting bounds can be shown. Let us assume that there exist constants $0< c \leq C <  + \infty$ such that
\begin{equation}
\label{equ:distance controled by surrogate}
    c ~ \text{dist}(v,\mathcal{M}) 
    \leq \mathcal{S}(v,\mathcal{M})
    \leq C ~ \text{dist}(v,\mathcal{M}).
\end{equation}
Then in \cite[Theorem 3.4]{cohenNonlinearReducedModels2022}, the authors manage to control the error of the selected estimate by the best recovery error among the library. Although they work with a library built by a piecewise affine model reduction approach, their result still holds for a general library. They introduce and analyse the constant $\mu(\mathcal{M}, W)$ defined by
\begin{subequations}\label{equ:general stability constant}
\begin{equation}
    \mu(\mathcal{M}, W) := \frac{1}{2} \sup_{\sigma >0} 
    \frac{\delta_{\sigma} - \delta_0}{\sigma}
    \end{equation}
with 
\begin{equation}
    \delta_{\sigma} := \max_{w\in W} \text{diam}(\mathcal{M}_{\sigma}\cap U_w)
    \quad \text{and} \quad 
    \mathcal{M}_{\sigma} := \big\{
    v \in U : \text{dist}(v, \mathcal{M}) \leq \sigma
    \big\},
\end{equation}
\end{subequations}
where $\mathcal{M}_\sigma$ is the $\sigma$-fattening of $\mathcal{M}$. The constant $\mu(\mathcal{M}, W)$ reflects the stability of the recovery problem, independently on the method used for the approximation of $\mathcal{M}$. In other words, it reflects how well $\mathcal{M}$ and $W$ are aligned. The preferable case is when $\delta_0=0$. It is equivalent to $P_W$ being injective on $\mathcal{M}$. In this case, the error on $A_{\mathcal{S}}^{\text{mult}}$ is truly controlled by the best possible error within the library, as shown in \cref{prop:general oracle bound}.

\begin{proposition}
\label{prop:general oracle bound}
Assume that $\mu(\mathcal{M}, W) < \infty$, then for all $u\in\mathcal{M}$,
\begin{equation}
\label{equ:general oracle bound}
    \| u - A^{\text{mult}}_{\mathcal{S}}(P_W u) \|_U 
    \leq \delta_0 + 2 \kappa \mu(\mathcal{M}, W) 
    \min_{1\leq k \leq N} \|u - A_{k}(P_W u) \|_U,
\end{equation}
where $\kappa := \frac{C}{c}$.
\end{proposition}
\begin{proof}
    Let us pick the $k_{\text{best}}$-th subspace from $\mathcal{L}_n^N$ as the space associated to the best one-space recovery,
    \[
    k_{\text{best}}=\argmin_{1 \leq k \leq N} \| u - A_k(w) \|_U.
    \]
    Using \eqref{equ:distance controled by surrogate} and the definition of $k^*$ from \eqref{equ:multi-space space selection}, it follows that
    \[
    \text{dist}(A_{k^*}(w), \mathcal{M}) 
    \leq \frac{1}{c} \mathcal{S}(A_{k^*}(w), \mathcal{M}) 
    \leq \frac{1}{c} \mathcal{S}(A_{k_{\text{best}}}(w), \mathcal{M}) 
    \leq \frac{C}{c} \text{dist}(A_{k_{\text{best}}}(w), \mathcal{M}), 
    \]
    that is 
    \[\text{dist}(A_{k^*}(w), \mathcal{M}) \leq \kappa \lambda,
    \quad \text{with}\quad  
    \lambda := \text{dist}(A_{k_{\text{best}}}(w), \mathcal{M}).
    \]
    Hence, since $A_{k^*}(w) \in U_w$ by definition \eqref{equ:pbdw ls formulation},
    $A_{k^*}(w) \in \mathcal{M}_{\kappa \lambda} \cap U_w$. Since $u$ also lies in this set, the recovery error using $A^{\textup{mult}}_{\mathcal{S}} = A_{k^*}$ is bounded via
    \[
    \| u - A^{\textup{mult}}_{\mathcal{S}}(w) \|_U 
    \leq \text{diam}(\mathcal{M}_{\kappa \lambda} \cap U_w)
    \leq \delta_{\kappa \lambda}.
    \]
    We now distinguish the cases $\lambda=0$ and $\lambda>0$. 
    If $\lambda=0$, we have $\| u - A^{\textup{mult}}_{\mathcal{S}}(w) \|_U \leq \delta_0$, which   implies \eqref{equ:general oracle bound}. If $\lambda >0$,  using  
    \[
    \delta_{\kappa \lambda} %= \delta_0 + 2\kappa\lambda \frac{\delta_{\kappa \lambda}}{2\kappa \lambda}
    \leq \delta_0 + 2\kappa \lambda \mu(\mathcal{M}, W),
    \]
    and $\lambda \leq \|u - A_{k_{\text{best}}}(w) \|_U$ (since $u\in \mathcal{M}$), we also obtain  \eqref{equ:general oracle bound}. 
\end{proof}

This general library-based approach allows to perform one-space recovery in background spaces of low dimension, hence potentially good $\mu(V_{k^*}, W)$, while ensuring good approximation power of the selected background space. However, as stated in \cite{balabanovRandomizedLinearAlgebra2021}, some problems require a very large number $N$ of spaces to ensure good precisions $(\varepsilon_k)_{1\leq k\leq N}$. Hence testing each of the recovery maps $A_k$ for every new observation $w\in W$ can be a computational burden online. Our dictionary-based approach described in \cref{sec:dictionary approach for inverse problem}, as well as  our random sketching approach in \cref{subsec:randomized selection criterion}, aim to circumvent this issue.


\subsection{Piecewise multi-space}
\label{subsec:piecewise multi-space}

This subsection is consecrated to the case where the spaces in the library are obtained via partitioning of the parameter space $\mathcal{P}$ and the manifold $\mathcal{M}$, such that
\begin{equation}
\label{equ:partition of P and M}
    \mathcal{P} = \bigcup_{k=1}^N \mathcal{P}^{(k)},
    \hspace{5mm}
    \mathcal{M} = \bigcup_{k=1}^N \mathcal{M}^{(k)},
    \hspace{5mm} 
    \mathcal{M}^{(k)} = \{u(\xi):\xi\in\mathcal{P}^{(k)}\}.
\end{equation}
The piecewise multi-space approach  then consists in building $N$  spaces with moderate dimension, where the space $V_k$ approximates the piece $\mathcal{M}^{(k)}$.  The so called \textit{hp}-refinement methods, as in \cite{eftangHpCertifiedReduced2010, eftangHpCertifiedReduced2011}, are examples of such nonlinear methods for model order reduction. Recovery algorithms using such reduced spaces were considered in \cite{cohenNonlinearReducedModels2022}, and it was the initial framework for the general approach described in \cref{subsec:general approach}. They also covered the case of a library of affine spaces, but we do not extend on it in the present paper. In this subsection, we consider a library
\begin{equation*}
    \mathcal{L}_n^N = \{V_k : 1\leq k \leq N \}
    \quad  \text{with} \quad 
    \text{dim}(V_k) = n_k \leq n \leq m,
\end{equation*}
with the one-space constants associated to each sub-manifold,
\begin{equation*}
\begin{gathered}
    \varepsilon_k := \text{dist}( \mathcal{M}^{(k)}, V_k ),
    \quad \mu_k := \mu(V_k, W),
    \quad 1\leq k\leq N.
\end{gathered}
\end{equation*}
In this framework, using the general approach described in \cref{subsec:general approach}, it is possible to ensure error bounds depending on the quantity $\mathcal{S}$ used for model selection. If $\mathcal{S}$ satisfies equation \eqref{equ:distance controled by surrogate}, then for $\sigma := \max_{1\leq k\leq N} \varepsilon_k \mu_k$ we have the error bound \cite[Theorem 3.2]{cohenNonlinearReducedModels2022}
\begin{equation}
\label{equ:piecewise affine error bound}
    \|u - A^{\text{mult}}_{\mathcal{S}}(P_W u) \|_U \leq
    \delta_{\kappa\sigma},
\end{equation}
with $\delta_{\kappa\sigma}$ defined in \eqref{equ:general stability constant}. This piecewise multi-space approach may suffer from computational problems when the required number $N$ of subspaces becomes too large to satisfy a desired precision. Moreover, it is sensitive to the parametrization of the initial problem \eqref{equ:general parametric problem}, hence it may require some preliminary reformulation  of  the problem   before the generation of  background spaces. Both of those issues may be circumvented by the dictionary-based approach proposed in \cref{sec:dictionary approach for inverse problem}.

\subsection{Parameter-dependent operator equations}
\label{subsec:parametric pdes}

In this subsection we assume that the parametric problem \eqref{equ:general parametric problem} is actually a parameter-dependent  
operator equation (or PDE)
\begin{equation}
\label{equ:parametric pde}
    B(\xi) u(\xi) = f(\xi)
\end{equation}
where $B(\xi): U \to U'$ is the operator and $f(\xi) \in U'$ the right-hand side. In this framework, it is common to consider a residual-based error as a surrogate to the error in the norm $\|\cdot\|_U$. We define for all $v\in U$,
\begin{subequations}\label{equ:S2 and residual norm definition}
\begin{equation}
    \mathcal{S} (v, \mathcal{M}) = \min_{\xi \in \mathcal{P}} \mathcal{R}(v , \xi),
    \quad
    \end{equation}
    with
\begin{equation}    
    \mathcal{R}(v ,  \xi) := \| r(v , \xi) \|_{U'} , \quad    
    r(v,\xi) := B(\xi) v - f(\xi) \in U', 
    \quad\forall \xi\in\mathcal{P}.
\end{equation}
\end{subequations}
For the quantity $\mathcal{S}$ to be equivalent to the error $\| u -v \|_U$, and in order to derive error bounds, some assumptions are required on the operator $B(\xi)$. 
In the case where $B(\xi)$ is linear, we assume that its singular values are uniformly bounded in $\xi$, that is 
\begin{equation}
\label{equ:fom infsup constants}
    0 < c \leq \min_{v\in U} \frac{\| B(\xi) v \|_{U'} }{\|v\|_U}
    \leq \max_{v \in U} \frac{\| B(\xi) v \|_{U'} }{\|v\|_U}
    \leq C < \infty,
\end{equation}
for all $\xi \in \mathcal{P}$, for some constants $c$ and $C$. This implies that 
 for all $v \in U$,
\begin{subequations}
\label{equ:residual distance bounds}
\begin{equation}
    c \| v - u(\xi) \|_U 
    \leq \mathcal{R}(v,\xi) 
     \leq  C  \|v - u(\xi)\|_U, 
    \quad  \forall \xi\in\mathcal{P}, 
    \end{equation}
    and
 \begin{equation}   
    c~ \text{dist}(v,\mathcal{M}) 
    \leq  \mathcal{S}(v,\mathcal{M})
    \leq  C~  \text{dist}(v,\mathcal{M}).
\end{equation}
\end{subequations}
Moreover, under additional assumptions on $B$ and $f$, it is possible to compute $\mathcal{R}(v,\cdot)$ in an online efficient way, i.e. with costs independent on $\mathcal{N}$, by taking advantage of the fact that $v$ lies in a low-dimensional subspace. Furthermore, computing $\mathcal{S}$ does not require to know any element in $\mathcal{M}$, whereas the true distance does. We provide a more detailed discussion on the computational aspects in \cref{subsec:offline-online decomposition}. 
\begin{remark}
    In \cite{cohenNonlinearReducedModels2022} the authors proposed another residual-based nonlinear recovery map, which is
    \[
       A(w)  = \argmin_{v\in w + W^{\perp}} \min_{\xi\in\mathcal{P}} \mathcal{R}(v,\xi),
    \]
    which yields an error $\Vert A(w) - u \Vert_U \le \delta_0$ in our noiseless setting. In some cases, as when $B(\xi)$ is linear and $\xi \mapsto B(\xi)$ and $ \xi \mapsto f(\xi)$ are affine functions of $\xi$, the function $(v,\xi)\mapsto \mathcal{R}(v,\xi)^2$ is convex and quadratic in each component, hence the previous problem can be tackled with an alternating minimization procedure. Numerical experiments showed very good results for large enough number of measurements $m$. However, it requires solving online about $m$ high dimensional problems, which is prohibitive for large $\mathcal{N}$.
\end{remark}
\begin{remark}
The properties \eqref{equ:residual distance bounds} also hold in the case of a  Lipschitz continuous and strongly monotone nonlinear operator $B(\xi)$, where $c$ is a uniform lower bound of the strong monotonicity constant and $C$ is a uniform bound of the Lipschitz constant. However, in a nonlinear setting, additional efforts are required for obtaining efficient online and offline computations. 
\end{remark}


\begin{remark}
    A preconditioned residual could be used to obtain a better condition number $\kappa$. Using preconditioners to better estimate the $U$-norm in a model order reduction setting  was proposed in \cite{zahmInterpolationInverseOperators2016, balabanovPreconditionersModelOrder2021}.
\end{remark}


\section{Randomized multi-space approach}
\label{sec:randomized multi-space problem}

First in \cref{subsec:randomized linear algebra} we recall some required preliminaries on randomized linear algebra. Then in \cref{subsec:randomized selection criterion}, we propose to use a sketched (or randomized) version of the selected criterion from \cref{subsec:parametric pdes} in the framework of parameter-dependent operator equations. We show that, with a rather small sketch and high probability, an oracle bound similar to the non-sketched approach holds.

\subsection{Randomized linear algebra}
\label{subsec:randomized linear algebra}

A basic task of RLA is to approximate norms and inner products of high dimensional vectors, by 
operating on lower dimensional vectors obtained through a \emph{random embedding} or \emph{random sketching}.  Such embedding does not only reduce the computational time for evaluating norms and inner products, but also reduce the memory consumption since only the embedded vectors need to be stored. 

We first recall the notion of $U \rightarrow \ell_2$ embedding for a subspace $Y \subset U$, which is a generalisation from \cite{balabanovRandomizedLinearAlgebra2019} of the notion of $\ell_2$ embedding for a subspace introduced in \cite{woodruffComputationalAdvertisingTechniques2014}. Let $\mathbf{\Theta} \in \mathbb{R}^{k \times \mathcal{\mathcal{N}}}$ be a random matrix, where $k$ is the embedding dimension, that represents a linear map $\Theta$ from $(U,\langle\cdot,\cdot\rangle_U)$ to $(\mathbb{R}^{k},\langle\cdot,\cdot\rangle)$ $U$ to $\ell_2$. This map allows to define (sketched) semi-inner products
\begin{equation}
\label{equ:rla semi inner products}
    \langle \cdot, \cdot \rangle_U^{\Theta} = 
    \langle \mathbf{\Theta} \cdot, \mathbf{\Theta} \cdot \rangle
    \hspace{5mm}\text{and}\hspace{5mm}
    \langle \cdot, \cdot \rangle_{U'}^{\Theta} = 
    \langle \mathbf{\Theta} \metricmat^{-1} \cdot, \mathbf{\Theta} \metricmat^{-1} \cdot \rangle, 
\end{equation}
and associated   semi-norms $\|\cdot\|_U^{\Theta}$ and $\|\cdot\|_{U'}^{\Theta}$ respectively. We now define formally the notion of subspace embedding. 
\begin{definition}
\label{def:subspace embedding def}
$\mathbf{\Theta}$ is called a $U\rightarrow \ell_2$ $\epsilon$-subspace embedding for $Y$, with  $\epsilon\in [0,1)$, if 
\begin{equation}
\label{equ:subspace embedding def}
 \Big\vert \|v\|_U ^2 - \|v\|_U^{\Theta~2} \Big\vert 
 \leq \epsilon \|v\|_U ^2, \quad \forall v \in Y.
\end{equation}
\end{definition}
It can be shown that if $\mathbf{\Theta}$ is a $U\rightarrow \ell_2$ $\epsilon$-subspace embedding for $Y$ then $\mathbf{\Theta} \metricmat^{-1}$ is a $U'\rightarrow \ell_2$ $\epsilon$-subspace embedding for $Y'$. Hence, both $\| \cdot \|_U$ and $\|\cdot\|_{U'}$ are well approximated by their   sketched versions. 

This definition is a property of $\mathbf{\mathbf{\Theta}}$ for a specific subspace $Y$. We are interested in the probability of failure for the random embedding $\mathbf{\Theta}$ to be a $U\rightarrow \ell_2$ $\epsilon$-subspace embedding for any $d$-dimensional subspace $Y$ of $U$. With this in mind, the notion of oblivious embedding is defined.
\begin{definition}
\label{def:oublivous embedding def}
$\mathbf{\Theta}$ is called a $(\epsilon, \delta, d)$ oblivious $U\rightarrow \ell_2$ subspace embedding if for any $d$-dimensional subspace $Y$ of $U$,
\begin{equation}
\label{equ:oublivous embedding def}
    \mathbb{P}(\mathbf{\Theta} \text{ is a } U \rightarrow \ell_2 ~\epsilon\text{-subspace embedding for } Y) 
    \geq 1-\delta.
\end{equation}
\end{definition}

Depending on the type of embedding considered, a priori conditions on $k$ can be obtained to ensure $\mathbf{\Theta}$ to be a $(\epsilon, \delta, d)$ oblivious $U \rightarrow \ell_2$ subspace embedding. For example, for a Gaussian embedding $\mathbf{\Omega}$ (with i.i.d. Gaussian entries with zero mean and variance $k^{-1}$), it has been shown in \cite{balabanovRandomizedLinearAlgebra2019} that if $0<\epsilon<0.572$ and 
\begin{equation}
\label{equ:bound gaussian embedding size}
    k \geq 7.87 \epsilon^{-2} (6.9d + \log(\delta^{-1}))
\end{equation} 
then $\mathbf{\Omega}$ is a $(\epsilon, \delta, d)$ oblivious $\ell_2 \rightarrow \ell_2$ subspace embedding. Then, by considering $\mathbf{\Theta} = \mathbf{\Omega} \mathbf{Q}$ with $\mathbf{Q}\in\mathbb{R}^{s\times \mathcal{N}}$ such that $\mathbf{Q}\hermtrans \mathbf{Q} = \metricmat$, it follows that $\mathbf{\Theta}$ is a $(\epsilon, \delta, d)$ $U \rightarrow \ell_2$ subspace embedding. Note that \eqref{equ:bound gaussian embedding size} is a sufficient condition, which may be very conservative. Indeed, numerical experiments from \cite{balabanovRandomizedLinearAlgebra2019, balabanovRandomizedLinearAlgebra2021} showed that random sketching may perform well for much smaller dimension $k$.
It takes $\mathcal{O}(k\mathcal{N})$ operations to sketch a vector with a Gaussian embedding, which can be prohibitive. A solution is to consider structured embeddings allowing to sketch vectors with only $\mathcal{O}(\mathcal{N}\log(\mathcal{N}))$ operations. It is for example the case for the partial subsampled randomized Hadamard transform (P-SRHT) described in \cite{troppImprovedAnalysisSubsampled2011}. Sufficient conditions on $k$ are also available but they are very conservative. However,   numerical experiments from \cite{balabanovRandomizedLinearAlgebra2019, balabanovRandomizedLinearAlgebra2021} showed performances similar to the Gaussian embedding for a given dimension $k$. A good practice is to use composed embeddings, for example $\mathbf{\Theta}=\mathbf{\Theta}_2 \mathbf{\Theta}_1$ with $\mathbf{\Theta}_1$ a moderate sized P-SRHT embedding and $\mathbf{\Theta}_2$ a small sized Gaussian embedding.

\begin{remark}
\label{rema:implicite semi inner product matrix}
    In practice, the matrix $\mathbf{Q}$ can be obtained via (sparse) Cholesky factorization of $\metricmat$, but it can also be any rectangular matrix such that $\mathbf{Q}\hermtrans \mathbf{Q} = \metricmat$, as pointed out in \cite[Remark 2.7]{balabanovRandomizedLinearAlgebra2019}. Such matrix may be obtained by Cholesky factorizations of small matrices, which are easy to compute. This is especially important for large scale problems.
\end{remark}

\subsection{Randomized selection criterion for parameter-dependent operator equations}
\label{subsec:randomized selection criterion}

Within the framework of \cref{subsec:parametric pdes}, we propose to replace the function $\mathcal{S}$ by a surrogate quantity  $\mathcal{S}^{\Theta}$, which is a sketched version of $\mathcal{S}$ defined for all $v\in U$ by
\begin{subequations}
\label{equ:definition sketched S2 and sketched residual norm}
\begin{equation}
    \mathcal{S}^{\Theta} (v, \mathcal{P}) 
    := \min_{\xi \in \mathcal{P}} \mathcal{R}^{\Theta}(v, \xi), 
    \end{equation}
    with 
\begin{equation}
    \mathcal{R}^{\Theta}(v , \xi) 
    := \big\| r(v,\xi) \big\|_{U'}^{\Theta} = \big\|B(\xi)v - f(\xi) \big\|_{U'}^{\Theta}, 
    \quad \forall \xi\in\mathcal{P}.
\end{equation}
\end{subequations}
Let us now discuss how bounds similar to \eqref{equ:residual distance bounds} can be obtained. 
In a general setting where no particular structure is assumed for $B$ or $f$,  $\mathcal{S}^{\Theta}$ can be approximated on a finite (possible very large) set $\mathcal{\widetilde P} \subset \mathcal{P}$. 
\begin{proposition}
\label{prop:sketched distance bounded by distance when finite manifold}
    Assume that $\#\mathcal{\widetilde P}<\infty$. If $\mathbf{\Theta}$ is a $(\epsilon, \#\mathcal{\widetilde P}^{-1} \delta, 1)$ oblivious $U\rightarrow \ell_2$ subspace embedding, then for any $v\in U$, with probability at least $1-\delta$ we have
    \begin{equation}
    \label{equ:sketched distance bounded by distance when finite manifold}
        \sqrt{1-\epsilon} ~\mathcal{S}(v, \mathcal{\widetilde P}) 
        \leq \mathcal{S}^{\Theta}(v, \mathcal{\widetilde P}) 
        \leq \sqrt{1+\epsilon} ~\mathcal{S}(v, \mathcal{\widetilde P}).
    \end{equation}
\end{proposition}
\begin{proof}
    For any $v\in U$ and $\xi\in\mathcal{\widetilde P}$, span$\{r(v,\xi)\}$ is a $1$-dimensional space, thus $\Theta$ is a subspace embedding for this space with probability greater than $1 - \# \mathcal{\widetilde P}^{-1} \delta$. Considering a union bound for the probability of failure for each element in $\mathcal{\widetilde P}$ gives the expected result.
\end{proof}
Computational efficiency can be obtained if we assume that $B(\xi)$ is linear and that $B(\xi)$ and $f(\xi)$ admit  affine representations
\begin{equation}
\label{equ:fom affine representation}
    B(\xi) = B^{(0)} + \sum_{q=1}^{m_B} \theta_q^{B}(\xi) B^{(q)},
    \quad 
    f(\xi) = f^{(0)} + \sum_{q=1}^{m_{f}} \theta_q^{f}(\xi) f^{(q)}.
\end{equation}
where $\theta^B_q : \mathcal{P} \rightarrow \mathbb{R}$ and $\theta^{f}_q : \mathcal{P} \rightarrow \mathbb{R}$ are called the affine coefficients, and where $B^{(q)} : U \rightarrow U'$ and $f^{(q)}\in U'$ are parameter-independent and called the affine terms. Such representations allow to do precomputations (offline) independently of $\xi$ using the affine terms, and then to rapidly evaluate (online) the expansion of a parameter dependent quantity. This leads to online-efficient computation of the residual norm. Those   affine representations may be naturally obtained from the initial problem, such as in the numerical examples in \cref{sec:numerical examples}, or may be obtained by using for example the empirical interpolation method (EIM) presented in \cite{madayGeneralMultipurposeInterpolation2009}. 

\begin{proposition}
\label{prop:surrogate distance equivalence affine representation}
Assume the affine representations of $B$ and $f$ from \eqref{equ:fom affine representation}. If $\mathbf{\Theta}$ is a $(\epsilon, \delta, m_B + m_{f}+1)$ oblivious $U \rightarrow \ell_2$ subspace embedding, then for any $v \in U$, with probability at least $1-\delta$ we have
\begin{equation}
\label{equ:sketched res dist bounded by res dist}
    \sqrt{1-\epsilon} ~\mathcal{S}(v, \mathcal{P}) 
    \leq \mathcal{S}^{\Theta}(v, \mathcal{P}) 
    \leq \sqrt{1+\epsilon} ~\mathcal{S}(v, \mathcal{P}).
\end{equation}
\end{proposition}

\begin{proof}
    For any $v \in U$ and $\xi\in\mathcal{P}$, the residual can be written in a form with separated variables,
    \begin{equation*}
        r(v, \xi) = 
        G(v) \theta(\xi) - g(v) ,
    \end{equation*}
    with
    \begin{equation}
    \label{equ:separated variable formulation}
    \begin{gathered}
        G(v) := \Big( 
        B^{(1)} v \mid \cdots \mid B^{(m_B)} v \mid
        -b^{(1)} \mid \cdots \mid -b^{(m_f)}
        \Big) ,
        \\
        \theta(\xi) = \Big( 
        \theta^B_1(\xi) \mid \cdots \mid \theta^B_{m_B}(\xi) \mid
        \theta^f_1(\xi) \mid \cdots \mid \theta^f_{m_f}(\xi)
        \Big)^T,
        \\
        g(v) := f^{(0)} - B^{(0)} v.
    \end{gathered}
    \end{equation}
    In other words, for a given $v\in U$, $r(v, \xi)$ lies in a low dimensional subspace, spanned by $g(v)$ and the columns of $G(v)$, independently from $\xi$. This subspace is of dimension at most $m_B+m_f+1$. Hence, if $\mathbf{\Theta}$ is a $(\epsilon, \delta, m_B + m_f+1)$ oblivious $U \rightarrow \ell_2$ subspace embedding, then for a fixed $v\in U$, with probability at least $1-\delta$,
    \begin{equation*}
        \sqrt{1-\epsilon}~ \mathcal{R}(v, \xi)
        \leq \mathcal{R}^{\Theta}(v, \xi) 
        \leq \sqrt{1+\epsilon}~ \mathcal{R}(v, \xi),
        \hspace{5mm} \forall \xi \in \mathcal{P}.
    \end{equation*}
    Taking the infimum  over $\mathcal{P}$ yields the desired result. 
    \end{proof}

Now that we can ensure a control of $\mathcal{S}^{\Theta}$ by $\mathcal{S}$ with high probability, we can ensure the following error bounds  by using the same reasoning as for obtaining \cref{prop:general oracle bound}. 
\begin{corollary}
\label{coro:dic-based multi-space sketched error bound 2}
Assume that $\mu(\mathcal{M}, W) < \infty$. If the assumptions of either \cref{prop:sketched distance bounded by distance when finite manifold} or \cref{prop:surrogate distance equivalence affine representation} are satisfied, then for any $u \in \mathcal{M}$, with probability at least $1-N \delta,$ we have
\begin{equation}
\label{equ:dic-based multi-space sketched error bound 2}
    \| u - A^{\textup{mult}}_{\mathcal{S}^{\Theta}}(w) \|_U 
    \leq \delta_0 + 2 \kappa^{\Theta} \mu(\mathcal{M}, W) 
    \min_{1\leq k \leq N} \|u - A_{k}(w) \|_U,
\end{equation}
with $\kappa^{\Theta} := \kappa \sqrt{1+\epsilon} / \sqrt{1-\epsilon}$.
\end{corollary}

The key point of this random sketching approach is that we can use a sketch of rather low size while ensuring a desired precision with  high probability. Indeed, in view of \eqref{equ:bound gaussian embedding size}, we can satisfy the assumptions of respectively \cref{prop:sketched distance bounded by distance when finite manifold} or \cref{prop:surrogate distance equivalence affine representation}
with a sketch of size
\begin{equation*}
    k=\mathcal{O}\Big(\epsilon^{-2} \log(\#\mathcal{\widetilde P}^{-1}\delta^{-1})\Big)
    \hspace{5mm} \text{or} \hspace{5mm}
    k=\mathcal{O}\Big(\epsilon^{-2} \big( (m_B + m_f) + \log(\delta^{-1}) \big)\Big).
\end{equation*}
This will allow us to manipulate only some low dimensional quantities during the online stage, while requiring reasonable offline costs and ensuring robustness to round-off errors. This is discussed in more details in \cref{subsec:offline-online decomposition}

\section{Dictionary approach for inverse problem}
\label{sec:dictionary approach for inverse problem}

It has been stated in \cite{balabanovRandomizedLinearAlgebra2021} that the size of the library required for piecewise linear (or affine) reduced modeling may become too large, resulting in prohibitive computational costs. The authors then proposed a new dictionary-based approach for forward problems, where low dimensional approximation spaces are obtained from a sparse selection of vectors in a dictionary. We propose to use a similar approach for inverse problems, more especially for the background term. 

Note that a dictionary-based approach for inverse problems has already been considered in \cite{binevGreedyAlgorithmsOptimal2018}, but for observation space selection. The authors considered a fixed background space $V$ and proposed a greedy algorithm to select a good observation space $W$ spanned by vectors from a dictionary. On  the other hand, our work focuses on building a good background $V$ for a fixed $W$.

In \cref{subsec:dictionary-based approximation} we summarize the works and motivations from \cite{balabanovRandomizedLinearAlgebra2021} concerning the dictionary-based approximation for highly nonlinear manifolds. Then in \cref{subsec:dictionary based multi-space} we present a dictionary-based multi-space recovery problem and introduce a $\ell_1$-regularized optimization problem to build adaptively a library of spaces, and we provide the associated oracle bounds. The solution method generates a whole path of candidate spaces, and in \cref{subsec:model selection} we propose to select one of these spaces using the approach from \cref{subsec:general approach} or \cref{subsec:randomized selection criterion}, in the framework of parameter-dependent  operator equations (or PDEs). Finally, in \cref{subsec:offline-online decomposition}, in the same framework, we show that our randomized approach allows to select efficiently a good state estimate, with low online cost and reasonable offline cost.

\subsection{Dictionary-based approximation}
\label{subsec:dictionary-based approximation}

Let $\mathcal{D}_K = \{ v^{(1)},\cdots, v^{(K)} \}$ be a dictionary of $K$ vectors in $U$ and let 
 $\mathcal{L}_n(\mathcal{D}_K)$ be the library defined by
\begin{equation}
    \mathcal{L}_n(\mathcal{D}_K) := \Big\{ \sum_{k=1}^K x_k v^{(k)} 
    : ~(x_k)_{k=1}^K \in\mathbb{R}^{K},~ \|x\|_0 \leq n \Big\},
\end{equation}
which contains all subspaces spanned by at most $n$ vectors of $\mathcal{D}_K$. The benchmark error for the approximation of $\mathcal{M}$ is the dictionary-based $n$-width proposed in \cite{balabanovRandomizedLinearAlgebra2021} defined as
\begin{equation}
\label{equ:dictionary r width}
    \sigma_n(\mathcal{M}, K) := \inf_{\# \mathcal{D}_K=K}
    \sup_{u\in \mathcal{M}}
    \min_{V \in \mathcal{L}_n(\mathcal{D}_K)}
    \min_{v\in V}\|u- v\|_U.
\end{equation}
The use of the dictionary-based approximation presents three main advantages over methods based on a partition of the parameter space, such as the piece-wise affine approach from \cref{subsec:piecewise multi-space}. Firstly, a library $\mathcal{L}_n(\mathcal{D}_K)$ using a dictionary of $K\geq nM$ vectors has at least the same approximation power as a library $\mathcal{L}_n^M$ containing $M$ subspaces of dimension $n$. The reversed statement is not true since such a library $\mathcal{L}_n(\mathcal{D}_K)$ can generate up to $ \mathcal{O}(\binom{K}{n})$ different subspaces. 

Secondly, assuming that $\mathcal{M}=\{\sum_{i=1}^l u^{(i)}(\xi): \xi\in\mathcal{P}\}$ is obtained by summing elements 
from  several manifolds $\mathcal{M}^{(i)} = \{ u^{(i)}(\xi): \xi \in \mathcal{P} \}$, then \cite[Corollary 4.2]{balabanovRandomizedLinearAlgebra2021} ensures the preservation of the decay of the dictionary-based $n$-width, that is: 
\begin{subequations}
\begin{equation}
    \text{if } \sigma_n(\mathcal{M}^{(i)}, cn^{\nu}) \leq Cn^{-\alpha}
    \text{ then }
    \sigma_n(\mathcal{M}, cl^{1-\nu}n^{\nu}) \leq Cl^{1+\alpha}n^{-\alpha}, 
    \end{equation}
    or
\begin{equation}
\text{if } \sigma_n(\mathcal{M}^{(i)}, cn^{\nu}) \leq Ce^{-\gamma n^{\beta}}
    \text{ then }
    \sigma_n(\mathcal{M}, cl^{1-\nu}n^{\nu}) \leq Cle^{-\gamma (n/l)^{\beta}}, 
\end{equation}
\end{subequations}
with $\nu \geq 1$ and some constants $c,C,\alpha,\beta,\gamma>0$. To obtain similar results with the nonlinear $(N,n)$-width from equation \eqref{equ:non linear kolmogorov n width}, the number $N$ of subspaces needs to be $N \sim n^{l\nu}$, while the dictionary $n$-width requires $K \sim n^{\nu}$ vectors. Note that a dictionary of size $K \sim n^\nu$ may generate a number of $n$-dimensional spaces of the order of $\binom{K}{n} \sim n^{\nu n}$, that corresponds to the particular regime studied in \cite{temlyakovNonlinearKolmogorovWidths1998} for the nonlinear Kolmogorov width \eqref{equ:non linear kolmogorov n width}.

Thirdly, it does not require any particular parameterization of the initial parametric problem, whereas it can be a central issue for methods based on a partition of the parameter space.

\subsection{Dictionary-based multi-space approach}
\label{subsec:dictionary based multi-space}

In this section, following the idea from \cite{balabanovRandomizedLinearAlgebra2021}, we consider a dictionary $\mathcal{D}_K$ containing $K$ normalized vectors from a finite dimensional  space $U$,
\begin{equation}
%\label{equ:dictionary multi-space dictionary}
    \mathcal{D}_K := \{v^{(k)}: 1\leq k\leq K\} 
    \hspace{5mm} \text{with} \hspace{5mm} \|v^{(k)}\|_U = 1, \hspace{5mm} 1\leq k\leq K,
\end{equation}
whose elements are represented by the columns of the matrix $\mathbf{V} \in \mathbb{R}^{\mathcal{N} \times K}$, which is not necessarily orthogonal. The library of spaces with dimension less than $m$ produced by the dictionary is 
\[
\mathcal{L}_m^N = \mathcal{L}_m(\mathcal{D}_K)
\quad \text{with} \quad
N  = \mathcal{O}(\binom{K}{m}).
\]
In practical situations, the dictionary is typically composed of a large number of (re-scaled) snapshots from $\mathcal{M}$. We will assume that $K\gg m$. Since the library $\mathcal{L}_m(\mathcal{D}_K)$ is too large to  be fully explored, we propose to find the background space $V$ by minimizing a $\ell_1$-regularized (sparsity-inducing) version of the problem \eqref{equ:pbdw ls formulation algebraic}:
\begin{equation}
\label{equ:lasso}
    \mathbf{x^*} \in \argmin_{\mathbf{x} \in \mathbb{R}^K}
    \frac{1}{2} \|\mathbf{C x - w}\|_2^2 + \alpha \|\mathbf{x}\|_1,
\end{equation}
with $\mathbf{C = W\hermtrans \metricmat V} \in \mathbb{R}^{m\times K}$ and a regularization parameter $\alpha >0$. This minimization problem is known as the LASSO problem, whose properties  have been studied in, e.g., \cite{tibshiraniLASSOProblemUniqueness2013, rauhutInterpolationWeightedMinimization2015, zhangNecessarySufficientConditions2015}.   However, the condition required for problem \eqref{equ:lasso} to admit a unique solution, with error bounds, are not available in our general setting.

Still, we can define a recovery map based on this problem. To do so, we use the well-known Least-Angle Regression (LAR) algorithm \cite{efronLeastAngleRegression2004} adapted for LASSO, also called the LARS/homotopy algorithm. It computes a deterministic solution to the LASSO problem, via an adapted forward selection of the elements  from the dictionary. We use the support $\Lambda_{\alpha}(w)$ of a solution $\mathbf{x}^* \in $ to generate a space $V=V_{\alpha}(w)$, with
\begin{align}
\begin{aligned}
    &V_{\alpha}(w) := \text{span} \{ v^{(i)} : i\in \Lambda_{\alpha}(w) \}, \quad 
    \Lambda_{\alpha}(w) := \big\{i\in \{1, \hdots, K\} : \mathbf{x}_i^* \neq 0 \big\}, \\
    &\mathbf{x^*} = \text{LARS}_{\alpha}(w),
\end{aligned}
\end{align}
where LARS$_{\alpha}(w)$ is the output of the LARS algorithm with regularization parameter $\alpha$. An important feature of the LARS algorithm is that the final output $\mathbf{x}^*$ contains at most $m$ active components, i.e. $\# \Lambda_{\alpha}(w) \leq m$. Thus we introduce a new well-defined recovery map $A^{\text{dic}}_{\alpha}$ defined by
\begin{equation}
\label{equ:dictionary lars formulation}
    A^{\text{dic}}_{\alpha}(w) := A_{V_{\alpha}(w)}(w).
\end{equation}
Note that the relation between the space $V_{\alpha}(w)$ and an observation $w$ is non linear, hence the map $A^{\text{dic}}_{\alpha}$ is also non linear. Another interesting feature of the LARS$_{\alpha}$ algorithm is that it actually provides a whole (finite) family of spaces $(V_{\alpha}(w))_{\alpha \geq \alpha_0}$, which is a subset of the large library $\mathcal{L}_m(\mathcal{D}_K)$. Let us denote by $N_{\alpha_0}$ the size of this family. Thus we obtain all the associated state estimates $(A^{\text{dic}}_{\alpha}(w))_{\alpha \geq \alpha_0}$ with only one call of the algorithm, hence with no (significant) supplementary online costs. We exploit this aspect in \cref{subsec:model selection}.


\begin{remark}
\label{rmk:lars path size}
In practice, for numerical stability reasons, the algorithm is stopped either when $\alpha=\alpha_0$ or when $N_{\alpha} \ge C K$ for some fixed $C$. In the latter case, this corresponds to a $\alpha > \alpha_0$. In numerical experiments, we will take $\alpha_0$ equal to machine precision.
\end{remark}

\begin{remark}
\label{rmk:lars stopping criterion}
One may use a modified version of the LARS algorithm by adding a stopping criterion on the number of active components, i.e. $\|\mathbf{x}^*\|_0 \leq n \leq m$. By doing so, one can consider a smaller library $\mathcal{L}_n(\mathcal{D}_K)$ with potentially a better compromise between approximation of $\mathcal{M}$ and stability.
\end{remark}

\begin{remark}
\label{rmk:lars direct output}
In the case of noisy observations, it may be relevant to use the solution of the LARS algorithm directly as the background term, setting $ v^* = \mathbf{V}\mathbf{x}^*$. The noisy setting will be investigated in a subsequent work. 
%topic, which would induce more regularity of $A^{\text{dic}}_{\alpha}(w)$ w.r.t $\alpha$. This approach, or even the elastic-net approach may be used  in the case of noisy observations. However, numerical experiments in our case tend to show similar performances in the noiseless framework.
\end{remark}

\subsection{Model selection}
\label{subsec:model selection}

The dictionary-based multi-space approach relies on some regularization parameter $\alpha$.  A first way to select its value is to use an offline statistical validation approach. There are two drawbacks to this approach. The first is that it requires to know a sufficiently large set of snapshots in $\mathcal{M}$. This could be circumvented by considering an approximate manifold  obtained from some MOR method, which would be similar to a noisy framework.  
The second drawback is that there is no guarantee that a single $\alpha$ will perform well for every observation.
This requires further analysis. 

Then we propose a second way to select $\alpha$, based on the selection criterion described in \cref{subsec:general approach}. We define
\begin{equation}
\label{equ:alpha online}
    \alpha_{\mathcal{S}}(w) := \argmin_{\alpha \geq \alpha_0} \mathcal{S}(A^{\text{dic}}_{\alpha}(w), \mathcal{M}).
\end{equation}
Although this approach does not test every subspaces in $\mathcal{L}_m(\mathcal{D}_K)$, the results from \cref{prop:general oracle bound} and \cref{coro:dic-based multi-space sketched error bound 2} still hold, but with the smaller, adaptive library $(V_{\alpha}(w))_{\alpha>\alpha_0}$ generated by the LARS$_{\alpha_0}$ algorithm. This leads us to the following error bound.

\begin{corollary}
\label{coro:LARS error bound}
Assume that $\mu(\mathcal{M}, W) < \infty$. Then for all $u\in\mathcal{M}$,
\begin{equation}
    \|u - A^{\textup{dic}}_{\alpha_{\mathcal{S}}}(P_W u)\|_U 
    \leq \delta_0 + 2\kappa\mu(\mathcal{M}, W) 
    \min_{\alpha>\alpha_0} \| u - A^{\textup{dic}}_{\alpha}(P_W u) \|_U.
\end{equation}
\end{corollary}


Although there are several advantages to the dictionary-based approach, it is important to note that we can not control the   constant $\mu(V,W)$ for every space $V\in\mathcal{L}_m(\mathcal{D}_K)$, hence we can not bound the best recovery error contrary to the 
 piecewise multi-space setting described in \cref{subsec:piecewise multi-space}, with a smaller number of spaces involved.

\subsection{Parameter-dependent operator equations: offline-online decomposition}
\label{subsec:offline-online decomposition}

In this subsection, we consider the same framework as in \cref{subsec:parametric pdes}, in other words problem \eqref{equ:general parametric problem} is actually a parameter-dependent operator equation with inf-sup stable linear operator. We first state the randomized version of \cref{coro:LARS error bound} in the following corollary.

\begin{corollary}
\label{coro:LARS sketched error bound}
Assume that $\mu(\mathcal{M}, W) < \infty$. If assumptions of either \cref{prop:sketched distance bounded by distance when finite manifold} or \cref{prop:surrogate distance equivalence affine representation} are verified, then for any $u \in \mathcal{M}$, with probability at least $1-N_{\alpha_0}\delta$, we have
\begin{equation}
    \|u - A^{\textup{dic}}_{\alpha_{\mathcal{S}^{\Theta}}}(P_W u)\|_U 
    \leq \delta_0 + 2\kappa^{\Theta}\mu(\mathcal{M}, W) 
    \min_{\alpha>\alpha_0} \| u - A^{\textup{dic}}_{\alpha}(P_W u) \|_U
\end{equation}
\end{corollary}

Let us now discuss on how to efficiently compute $\mathcal{S}^{\Theta}$ when the affine decomposition \eqref{equ:fom affine representation} is assumed. In a nutshell, it is done by performing precomputation during an offline stage, independently on the unknown state $u\in\mathcal{M}$ to recover. We denote by $\mathbf{U}$ the matrix whose columns span the observation space and the dictionary vectors,
\begin{equation}
    \mathbf{U := \big( ~W\mid V~\big)} \in \mathbb{R}^{\mathcal{N}\times (m+K)}.
\end{equation}
Any dictionary-based recovery can then be expressed as $\mathbf{v}=\mathbf{Ua}$ with $\mathbf{a}\in\mathbb{R}^{m+K}$. In the proof of \cref{prop:surrogate distance equivalence affine representation}, we observed that the computation of $\mathcal{S}$ can be written as a constrained, possibly non linear, least-squares problem. It is also the case for $\mathcal{S}^{\Theta}$,  since 
\begin{equation}
\label{equ:sketched distance ls formulation}
    \mathcal{S}^{\Theta}(v, \mathcal{M}) = \min_{\xi\in\mathcal{P}}
    \| G(v) \theta(\xi) - g(v) \|_{U'}^{\Theta}
    = \min_{\xi\in\mathcal{P}}
    \| \mathbf{G}_K^{\Theta}(\mathbf{a}) \theta(\xi) - \mathbf{g}_K^{\Theta}(\mathbf{a}) \|
\end{equation}
with  
\begin{equation}
\label{equ:separated variable formulation reduced sketched}
    \mathbf{G}_K^{\Theta}(\mathbf{a}) := 
    \mathbf{\Theta}\metricmat^{-1} \mathbf{G}_K(\mathbf{a}),
    \quad  
    \mathbf{g}_K^{\Theta}(\mathbf{a}) := \mathbf{\Theta}\metricmat^{-1} \mathbf{g}_K(\mathbf{a}),
\end{equation}
which are the sketched versions of  
\begin{equation}
\label{equ:separated variable formulation reduced}
\begin{gathered}
    \mathbf{G}_K(\mathbf{a}) := \Big( 
    \mathbf{B}^{(1)} \mathbf{Ua} \mid \cdots \mid \mathbf{B}^{(m_B)} \mathbf{Ua} \mid
    -\mathbf{f}^{(1)} \mid \cdots \mid -\mathbf{f}^{(m_f)}
    \Big), \\
    \mathbf{g}_K(\mathbf{a}) := \mathbf{f}^{(0)} - \mathbf{B}^{(0)} \mathbf{Ua},
\end{gathered}
\end{equation}
associated to ${G}$ and ${g}$ defined in \eqref{equ:separated variable formulation}. The important point is that we can precompute  $\mathbf{G}^{\Theta}_K(\cdot)$ and $\mathbf{g}_K^{\Theta}(\cdot)$ independently on $\mathbf{a}$. More precisely, we compute offline $\mathbf{\Theta} \metricmat^{-1} \mathbf{B}^{(i)}\mathbf{U}$ for $0\leq i\leq m_B$ and $\mathbf{\Theta} \metricmat^{-1} \mathbf{f}^{(j)}$ for $0\leq j\leq m_f$. Using some structured embedding, the global offline cost is then typically
\[
    \mathcal{O}\big( ~( m_BK + m_f )~ \mathcal{N}\log(\mathcal{N})~ \big).
\]
Then, for a given observation, we perform the LARS algorithm to obtain a family of $N_{\alpha_0}$ state estimates, where we assume that $N_{\alpha_0}=\mathcal{O}(K)$ in view of \cref{rmk:lars path size}, with online cost $\mathcal{O}(m^2 K)$ \cite{efronLeastAngleRegression2004}. Now, for each of the vectors $\mathbf{a}$ produced, we can efficiently evaluate $\mathbf{G}^{\Theta}_K(\mathbf{a})$ and $\mathbf{g}_K^{\Theta}(\mathbf{a})$ by performing small matrix-vector products, with matrices of size $k\times (m+K)$. More precisely, since the dictionary-based approach produces a sparse background term, the matrices used are only of size $k\times 2m$. Therefore, the online preparation of \eqref{equ:sketched distance ls formulation} for every recoveries produced costs $\mathcal{O}(km(m_B+m_f)K)$. In summary, if we denote by $C_{ls}$ the cost for  solving a single (nonlinear) least-squares problem \eqref{equ:sketched distance ls formulation}, the total online cost to obtain $A_{\mathcal{S}^{\Theta}}(w)$ is 
\[
    \mathcal{O}\big( 
    \underbrace{\vphantom{C_{ls}} m^2 K}_{\textup{LARS}} + 
    \underbrace{\vphantom{C_{ls}} km(m_B+m_f)K}_{\textup{prepare K l.s.}} + 
    \underbrace{C_{ls}K}_{\textup{solve K l.s.}} 
    \big).
\]
Note that if $\theta(\xi) = \xi\in\mathbb{R}^d$, or more generally if $\theta$ is an affine function of $\xi$, then problem \eqref{equ:sketched distance ls formulation} is a simple linear least-squares, which can be solved at cost $C_{ls} = \mathcal{O}(kd^2)$. Otherwise, the cost depends on the global optimization procedure performed. We recall that, in view of \eqref{equ:bound gaussian embedding size}, we have typically 
\[
    k = \mathcal{O}\Big(\epsilon^{-2} \big( (m_B + m_f) + \log(K\delta^{-1})\big)\Big),
\]
with $\epsilon$ the precision of the sketch and $\delta$ the probability of failure from \cref{def:oublivous embedding def}. We recall that both are user-defined.

The advantage of considering $\mathcal{S}^{\Theta}$ instead of $\mathcal{S}$ is that it yields a small least-squares system, which is efficiently prepared online, whereas doing a similar procedure with $\mathcal{S}$ leads to a large least-squares system of size $\mathcal{N}\times (m_B+m_f)$. A common approach to compute the residual norm efficiently is to expand offline $\|r(\mathbf{Ua}, \cdot)\|^2_{U'}$ as 
\begin{equation*}
    \|r(\mathbf{Ua};\xi)\|_{U'}^2 = 
    \mathbf{a}\hermtrans \mathbf{M}_1 (\xi) \mathbf{a}
    - 2 \mathbf{a}\hermtrans \mathbf{M}_2(\xi)
    + \mathbf{M}_3(\xi),
    \quad \forall \xi \in \mathcal{P},
\end{equation*}
where $\mathbf{M}_1: \mathcal{P} \rightarrow \mathbb{R}^{(K+m)\times (K+m)}$, $\mathbf{M}_2 : \mathcal{P} \rightarrow \mathbb{R}^{(K+m)}$ and $\mathbf{M}_3 : \mathcal{P} \rightarrow \mathbb{R}$ all admit affine representations, with respectively $m_B^2$, $m_B m_f$ and $m_f^2$ affine terms. Although this expression is theoretically exact, in practice the high number of affine terms makes this approach more sensitive to round-off errors, as pointed out in \cite{casenaveAccurateOnlineefficientEvaluation2014, balabanovRandomizedLinearAlgebra2019, buhrNumericallyStablePosteriori2014}, and the associated offline cost scales as 
\[
    \mathcal{O}\big( ~( m_BK + m_f )^2~ \mathcal{N}~ \big)
\]
which is   prohibitive since we aim for large $K$. Another approach proposed \cite{buhrNumericallyStablePosteriori2014} is to compute an orthonormal basis of the space in which the residual lies. This approach is more stable than the previous one, but comes with a similar prohibitive offline cost. Note that another approach is proposed in \cite{casenaveAccurateOnlineefficientEvaluation2014,giraldiWeaklyIntrusiveLowRank2019}, where different empirical interpolation methods (EIM) are proposed to approximate the residual norm. 
%However, the number of interpolation points for this method is not properly known a priori, hence we will not develop on this approach.

\section{Numerical examples}
\label{sec:numerical examples}

To illustrate the performances of our dictionary-based multi-space approach, we provide two numerical examples where problem \eqref{equ:general parametric problem} is a parametric PDE defined on some open, regular, bounded domain $\Omega \subset \mathbb{R}^2$. The solution space is the Hilbert space $\mathcal{H}_{\Gamma_D}^1(\Omega) = \{u \in \mathcal{H}^1(\Omega) : u =0 \; \text{on} \; \Gamma_D\}$, where $\Gamma_D$ is the  boundary where an homogeneous Dirichlet condition is applied. The problem is discretized using  Lagrange $\mathbb{P}_1$ finite elements with a regular triangular mesh, leading to a finite dimensional space $U$ that we equip with the natural norm  $\|\cdot\|_U = \| \nabla \cdot \|_{\mathcal{L}^2(\Omega)}$ in $\mathcal{H}_{\Gamma_D}^1(\Omega)$.

We consider local radial integral sensors, approximated in the finite element space, such that
\begin{equation}
\label{equ:thermal block sensors}
    \ell_i(u) \simeq \int_{\Omega} u(x) 
    \exp \Big( -\frac{\|x-x^{(i)}_{\text{loc}} \|^2}{\sigma^2}\Big) dx,
    \quad  1\leq i\leq m,
\end{equation}
where $x^{(i)}_{\text{loc}} \in\Omega$ is the location of the $i$-th sensor and $\sigma$ represent the filter width of the sensor. 

To approximate the residual-based distance $\mathcal{S}$ with $\mathcal{S}^{\Theta}$, we consider random embeddings of the form $\mathbf{\Theta}=\mathbf{\Omega} \mathbf{Q}$ with $\mathbf{\Omega}\in\mathbb{R}^{k\times \mathcal{N}}$ a random embedding and $\mathbf{Q}\in\mathbb{R}^{\mathcal{N}\times \mathcal{N}}$ a matrix such as $\mathbf{Q\hermtrans Q}=\metricmat$, obtained by Cholesky factorisation. Note that in both examples, the quantity $\mathcal{S}^{\Theta}(\cdot, \mathcal{P})$ can be exactly computed by solving a constrained linear least-squares system.

We will compare the performances of three types of recovery maps on a test set of $N_{\text{test}}=500$ snapshots for various dictionary sizes $K$. More precisely, we compare the recovery relative error, $ \|u-A(P_W u)\|_U / \|u\|_U$. The first one is the best multi-space recovery map from \cref{subsec:nested multi-space} with a library containing $m$ nested subspaces obtained by performing a POD on the $K$ first vectors of $\mathcal{M}_{\text{prior}}$, which is a discrete set of snapshots. The second one is the dictionary-based recovery map $A^{\text{dic}}_{\alpha}$ with randomized selection criterion,  where $\alpha=\alpha_{\mathcal{S}^{\Theta}}$. The third one is the best dictionary-based recovery map $A^{\text{dic}}_{\alpha}$, with optimal space selected  among the spaces obtained by the LARS algorithm. For both dictionary-based recoveries, the dictionary $\mathcal{D}_K$ used is composed of the $K$ first (normalized) snapshots from $\mathcal{M}_{\text{prior}}$.

In \cref{subsec:thermal block} we consider a diffusion problem, built with the pymor library \cite{milkPyMORGenericAlgorithms2016}, which provides many standard tools for model order reduction. Then in \cref{subsec:advection-diffusion} we consider an advection-diffusion problem, built using the DOLFINx package \cite{loggDOLFINAutomatedFinite2010}. Finally in \cref{subsec:observations} we summarize our main observations. 

The implementation has been made via an open-source python development. The implementation of the sparse Cholesky factorization is from the scikit-sparse package, which includes a wrapper for the CHOLMOD package \cite{chenAlgorithm887CHOLMOD2018}. The implementation of the Hadamard transform is from the FALCONN library \cite{andoniPracticalOptimalLSH2015}. The implementation of the LARS algorithm is from the SPAMS toolbox library \cite{mairalOnlineDictionaryLearning2009}.




\subsection{Thermal block}
\label{subsec:thermal block}

In this subsection we consider a classical thermal block problem with Dirichlet boundary conditions. It models a heat diffusion in a 2D domain $\Omega = (0,1)^2$ composed of $9$ blocks $\Omega_1, \hdots, \Omega_9$ of equal sizes and various thermal conductivities $\xi_1, \cdots, \xi_9$. The heat comes from a constant volumic source term equal to $1$ and homogeneous Dirichlet boundary conditions are imposed on $\Gamma_D = \partial \Omega$. The problem we consider can be written as
\begin{align}
\label{equ:thermal block}
\left\{\begin{aligned}
    -\nabla \cdot \big(\kappa\nabla u\big) =~& 1 &\text{in } &\Omega, \\
    u =~& 0 &\text{on } &\Gamma_D, \\
    \kappa =~& \xi_i &\text{in } &\Omega_i, ~1\leq i\leq 9,
\end{aligned}\right.
\end{align}
where the diffusion coefficients $\xi_i$ are drawn independently in $[\frac{1}{10}, 1]$ with a log-uniform distribution, leading to a parameter set $\mathcal{P} = [\frac{1}{10}, 1]^{9}$. The maximal finite element mesh element diameter is $2^{-6}$, leading to $\mathcal{N}=8~321$ degrees of freedom. This leads to the simple affine representations 
\begin{equation}
\label{equ:fom thermal}
    \mathbf{B}(\xi) = \mathbf{B}^{(0)} + \sum_{i=1}^9 \xi_i \mathbf{B}^{(i)},
    \hspace{5mm}
    \mathbf{f}(\xi) = \mathbf{f}.
\end{equation}
We consider $64$ sensors uniformly placed  at positions $ (\frac{i}{9}, \frac{j}{9})_{1\leq i,j \leq 8}$, with a sensor width $\sigma=2^{-6}$. The problem can be visualized in \Cref{fig:thermal block problem}.

\begin{figure}[!ht]
    \centering
    \includegraphics[page=3, width=1\textwidth]{FIGURES_Dictionary_MOR_for_state_estimation.pdf}
    \caption[Thermal block problem]{\footnotesize The thermal block problem. On the left, the geometry of the problem, with sensors locations (crosses). On the middle, the (normalized) Riesz representer of a sensor. On the right, an example of snapshot.}
    \label{fig:thermal block problem}
\end{figure}

We consider $3$ different values for $m$: $64$, $36$ and $9$. For $m=64$, all the sensors are used, for $m=36$ only sensors at positions $(\frac{i}{9}, \frac{j}{9})_{i,j \in \{1,2,4,5,7,8\}}$ are used, and for $m=9$ only sensors at positions $(\frac{i}{9}, \frac{j}{9})_{i,j \in \{1,4,7\}}$ are used. We computed a set of snapshots $\mathcal{M}_{\text{prior}}$ of size $5000$ to form our prior knowledge on the manifold $\mathcal{M}$. We test the performances of our dictionary-based approach with dictionaries of size $K \in \{ 100,200,500,1000,2000,5000 \}$. We choose $\mathbf{\Omega}$ as a Gaussian embedding with $k=100$ rows. The numerical results are summarized in \cref{fig:thermal block results}.


\begin{figure}[!ht]
    \centering
    \includegraphics[page=5, width=1\textwidth]{FIGURES_Dictionary_MOR_for_state_estimation.pdf}
    \caption[Thermal block results]{\footnotesize Evolution of the recovery errors in $U$-norm, on $500$ test snapshots, with growing dictionary sizes. We compare the PBDW recovery based on the best adaptive POD truncation (red), the dictionary-based recovery with randomized selection criterion (blue), as well as the best one produced by the LARS algorithm (cyan). The dotted lines are the $10\%$ and $90\%$ error quantiles, and the full line is the mean error.}
    \label{fig:thermal block results}
\end{figure}




\subsection{Advection-diffusion}
\label{subsec:advection-diffusion}

In this subsection we consider an advection diffusion problem with multiple transport phenomena,   inspired from \cite{balabanovRandomizedLinearAlgebra2021}. It models a heat diffusion and advection in a 2D domain $\Omega = \mathcal{B}(0,1.5) \setminus \bigcup_{i=1}^5 \Omega_i$, with $\Omega_i=\mathcal{B}(x^{(i)}, 0.1)$ a ball of radius $0.1$ centered at $x^{(i)}=(\cos(2i\pi/5), \sin(2i\pi/5))$, for $1\leq i\leq 5$. A constant source term is considered on the circular subdomain $\Omega_S = \mathcal{B}(0,0.1)$. The (parametric) advection field $\mathcal{V}$ is a sum of $5$ potential fields around the domains $\Omega_i$. The unknown temperature field $u$ satisfies the equations
\begin{align}
\label{equ:advection diffusion problem}
\left\{\begin{aligned}
    -\kappa \Delta u  + \mathcal{V}(\xi) \cdot \nabla u =~& \frac{100}{\pi} \mathbbm{1}_{\Omega_S} &\text{in } &\Omega, \\
    u =~& 0 &\text{on } &\Gamma_D, \\
    n \cdot \nabla u =~& 0 &\text{on } &\Gamma_N,
\end{aligned}\right.
\end{align}
with a diffusion coefficient $\kappa = 0.01$ and the advection field
\[
    \mathcal{V}(\xi) = 
    \sum_{i=1}^5 \frac{1}{\|x-x^{(i)}\|} \Big(
    \xi_i e_r(x^{(i)}) 
    + \xi_{i+5} e_{\theta}(x^{(i)})
    \Big),
\]
where $e_r(x^{(i)})$ and $e_{\theta}(x^{(i)})$ are the basis vectors of the polar coordinate system centered at $x^{(i)}$. Dirichlet boundary conditions are imposed on $\Gamma_D = \partial \mathcal{B}(0,1.5)$ and homogeneous Neumann conditions are imposed on $\Gamma_N = \Omega \setminus \Gamma_D$. The parameter $\xi$ is chosen uniformly in $\mathcal{P}=[-1,-\frac{1}{2}]^5 \times [-2, -1]^5$. The problem is discretized using Lagrange $\mathbb{P}_1$ finite elements on a triangular mesh refined around the pores, leading to  $\mathcal{N} = 152~297$ degrees of freedom. The operator and right-hand admit the  affine representations 
\begin{equation}
\label{equ:fom advection diffusion}
    \mathbf{B}(\xi) = \mathbf{B}^{(0)} + \sum_{i=1}^{10} \xi_i \mathbf{B}^{(i)},
    \hspace{5mm}
    \mathbf{f}(\xi) = \mathbf{f}.
\end{equation}
We consider $101$ sensors. A first sensor is placed at point $(0,0)$, and for $1\leq j\leq 4$,  $10\times j$ sensors are placed uniformly on a circle of radius $0.2\times j$ centered at $(0,0)$. The sensor width is chosen as $\sigma=2.10^{-2}$. The problem can be visualized in \Cref{fig:advection diffusion problem}. 

\begin{figure}[!ht]
    \centering
    \includegraphics[page=4, width=1\textwidth]{FIGURES_Dictionary_MOR_for_state_estimation.pdf}
    \caption[Advection diffusion problem]{\footnotesize The advection diffusion problem. On the left, the geometry of the problem, with sensors locations (crosses) and advection fields (circular arrows). On the middle, the (normalized) Riesz representer of the central sensor. On the right, an example of snapshot.}
    \label{fig:advection diffusion problem}
\end{figure}

We consider 3 different values for $m$: $101$, $61$ and $31$. For $m=101$ every sensors are used, for $m=61$ we dropped the sensors of the outer circle, and for $m=31$ we dropped the sensors of the two outer circles. We computed a set $\mathcal{M}_{\text{prior}}$ of $2048$ snapshots to form our prior knowledge on the manifold $\mathcal{M}$. We test the performances of our dictionary-based approach with dictionaries of size $K \in \{ 256,512,1024,2048 \}$. We choose $\mathbf{\Omega} = \mathbf{\Omega}_2 \mathbf{\Omega}_1$ with $\mathbf{\Omega}_1$ a P-SRHT embedding with $k'=16~263$ rows and $\mathbf{\Omega}_2$ a Gaussian embedding with $k=850$ rows. The numerical results are summarized in \cref{fig:advection diffusion results}.


\begin{figure}[!ht]
    \centering
    \includegraphics[page=6, width=1\textwidth]{FIGURES_Dictionary_MOR_for_state_estimation.pdf}
    \caption[Advection diffusion results]{\footnotesize Evolution of the recovery errors in $U$-norm, on $500$ test snapshots, with growing dictionary sizes. We compare the PBDW recovery based on the best adaptive POD truncation (red), the dictionary-based recovery with randomized selection criterion (blue), as well as the best one produced by the LARS algorithm (cyan). The dotted lines are the $10\%$ and $90\%$ error quantiles, and the full line is the mean error.}
    \label{fig:advection diffusion results}
\end{figure}

\subsection{Observations}
\label{subsec:observations}

From the numerical results of the previous subsections, we draw three main observations. Firstly, our dictionary-based approach with randomized selection criterion  outperformed the best possible POD-based PBDW recovery, even with reasonable sizes of dictionary. Secondly, increasing the latter improves global performances. Secondly, increasing the number of measurements $m$ improves the relative performance gain of both the best recovery map $A^{\text{dic}}_{\alpha}$ and the recovery map $A^{\text{dic}}_{\alpha_{\mathcal{S}^{\Theta}}}$, compared to the best POD-based multispace recovery. Thirdly, the relative difference of performances between the two dictionary-based recoveries decreases when $m$ grows. This may be explained by an increase of $\mu(\mathcal{M}, W)$ in \cref{coro:LARS error bound}.

Finally, it is important to note that, in \cref{subsec:advection-diffusion}, with the rather large $\mathcal{N}$ and $K$ involved, the randomized approach from \cref{sec:randomized multi-space problem} was crucial to efficiently evaluate online $\mathcal{S}^{\Theta}$ while requiring a moderate offline cost, which is not possible with $\mathcal{S}$. On the other hand, in \cref{subsec:thermal block},  $\mathcal{N}$ was rather small and we could compute $\mathcal{S}$ online by solving the least-squares system of size $\mathcal{N} \times d$ with rather low computational cost.


\section{Conclusions}
\label{sec:conclusions}

In this work, we proposed a dictionary-based model reduction approach for inverse problems, similar to what already exists for direct problems, with a near-optimal subspace selection based on some surrogate distance to the solution manifold, among the solutions of a path of $\ell_1$-regularized problems. We focused on the framework of parameter-dependent operator equations with affine parameterization, for which we provided an efficient procedure based on randomized linear algebra, ensuring stable computation while preserving theoretical guarentees.

Future work shall study the performances of our approach in a noisy observations framework. One may also consider a bi-dictionary approach, in which both the observation space and the background space would be selected adaptively with a dictionary-based approach.

\bibliographystyle{plain}  
\bibliography{article}
%\input article.bbl

\end{document}
