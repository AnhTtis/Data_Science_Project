
%
\title{Supplementary to Laplacian Segmentation Networks Improve Epistemic Uncertainty Quantification}
%
\titlerunning{Supplementary Laplacian Segmentation Networks}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{}
%
\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{}
%
\maketitle              % typeset the header of the contribution
\vspace*{-\baselineskip}
\vspace*{-\baselineskip}
%

%
%
%

\section{Implementation and Training Details}

Training configurations are provided in Table \ref{training_configs}. All models were trained with the Adam optimizer.  The U-net backbone was constructed with feature maps of size $8, 16, 32, 64, 128$. Uncertainty measures were approximated from $50$ samples from the posterior and $20$ samples from the logit distribution. 

\vspace*{-\baselineskip}
\begin{table}
% \scriptsize
\begin{center}
  \caption{Implementation and Training Details.  Dropout models for the ISIC dataset were trained with a $0.0005$ learning rate to improve convergence. }
\label{training_configs}
   \vspace{.5em}
  \centering
  \begin{tabular}{p{30mm}p{20mm}|p{20mm}|p{20mm}}
    \toprule
    
    & \multicolumn{3}{c}{Dataset}\\
    \cmidrule{2-4}
    % \cmidrule{6-9}
    % \cmidrule{10-12}
    
    
    Configuration       & ISIC & Prostate & Brats \\
    \hline
   Epochs & 60 & 150 & 600 \\
   \hline
    Batch Size  & 32 & 10 & 32  \\
    \hline
    Learning Rate   & 0.001* & 0.001    & 0.001  \\
    \bottomrule
  \end{tabular}

   \vspace{.5em}
  \centering
 
\vspace*{-\baselineskip}
\vspace*{-\baselineskip}
\end{center}
\end{table}
\vspace*{-\baselineskip}


\section{Fast Hessian Approximation}

Consider a neural network (\textsc{nn}) $f_\theta:\mathcal{X}\rightarrow\mathcal{Y}$ with $L$ layers. The parameter $\theta = (\theta_1, \dots, \theta_L) \in\Theta$ is the concatenation of the parameters for each layer $i \in \{1,...,L \}$. The \textsc{nn} $f_\theta=
f^{(L)}_{\theta_L}\circ f^{(L-1)}_{\theta_{L-1}} 
\circ\,\dots\,\circ 
f^{(2)}_{\theta_2} \circ f^{(1)}_{\theta_1}$ is a composition of $L$ functions $f^{(L)},f^{(L-1)},\dots,f^{(1)}$, where $f^{(i)}$ is parametrized by $\theta_{i}$. Let $x_0\in\mathcal{X}$ be the input and  $x_i:=f^{(i)}_{\theta_i}(x_{i-1})$ for $i=1,\dots,L$, such that the \textsc{nn} output is $x_L\in\mathcal{Y}$. We define the \emph{diagonal} operator $\mathcal{D}: 
\mathbb{R}^{m\times m}
\rightarrow
\mathbb{R}^{m\times m} $ on quadratic matrices as
\[
[\mathcal{D}(M)]_{ij}
:=
\left\{
\begin{array}{ll}
    M_{ij} & \text{ if } i=j \\
    0 & \text{ if } i\not=j
\end{array}
\right.
\qquad
\forall i,j=1,\dots,m.
\]
The \textbf{Jacobian} $J_\theta f_\theta(x_0)$ of the \textsc{nn} %w.r.t.\@ the parameter 
has a layer block structure, block $i$ is %$\theta_i$ of layer $i$ as
\begin{equation*}\label{eq:jacobian_chain_rule}
J_{\theta_i}f_{\theta}(x_0)
=
J_{\theta_i}
\left(
        f^{(i)}_{\theta_i}
        \circ\dots\circ
        f^{(L)}_{\theta_L}
    \right)
(x_{i-1}) 
=
\left(
    \prod_{j=L}^{i+1} 
    J_{x_{j-1}}f^{(j)}_{\theta_j}(x_{j-1})
\right)
J_{\theta_i}f^{(i)}_{\theta_i}(x_{i-1}).
\end{equation*}

The Laplace approximation requires the Hessian \textbf{H} of the loss w.r.t. the parameters $\nabla^2_{\theta} \mathcal{L}(f_\theta(x_0))
\in\mathbb{R}^{|\theta|\times|\theta|}$. Using the chain rule it holds, that
\begin{equation*}\label{eq:hessian_chain_rule}
\underbrace{\nabla^2_{\theta} \mathcal{L}(f_\theta(x_0))}_{=:H_\theta}
=
\underbrace{
J_{\theta}f_\theta(x_0)^\top
    \cdot 
    \nabla^2_{x_L}\mathcal{L}(x_L) 
    \cdot
J_{\theta}f_\theta(x_0)
}_{=:\textsc{ggn}_\theta}
+
\sum_{o=1}^{|x_L|} 
    [\nabla_{x_L}\mathcal{L}(x_L)]_o \cdot
    \nabla^2_{\theta} [f_\theta(x_0)]_o,
\end{equation*}
where $[v]_o$ refers to the $o$-th component of vector $v$ and $|v|$ to its length. We can write the diagonal block $\textsc{ggnb}^{(i)}_\theta=J_{\theta_i}f_\theta(x_0)^\top \textbf{H}^{\mathcal{L}} J_{\theta_i}f_\theta(x_0)$ of the $i$-th layer as

\begin{equation}\label{eq:ggn_chain_rule}
\begin{aligned}
    \textsc{ggnb}^{(i)}_\theta
    = &
    J_{\theta_i}f_{\theta}(x_0)^\top \cdot \textbf{H}^{\mathcal{L}} \cdot J_{\theta_i}f_{\theta}(x_0)
\end{aligned}
\end{equation}

From this expression, plus the chain rule expansion of the Jacobian, we can build an efficient backpropagation-like algorithm to compute $\textsc{ggnb}_\theta$, it start from $\textbf{H}^{\mathcal{L}}$ and then iterated backward over layers. The same holds for the diagonal approximation, which we refer to as $\textsc{ggnd}_\theta:=\mathcal{D}(\textsc{ggn}_\theta)=\mathcal{D}(\textsc{ggnb}_\theta)$. This approach already scales linearly in the number of parameter $|\theta|$.
On top of that, the diagonal backpropagation approximates the diagonal of the Generalized Gauss-Newton matrix. It is defined, for each layer $i$, by adding a diagonalization operator in between each Jacobian product, marked \textcolor{BrickRed}{red} in Algorithm 1. Without this extra operator the algorithm would return the exact diagonal.

\vspace*{-\baselineskip}
\begin{algorithm}[H]
\caption{Computation of $\textsc{db}_\theta$}\label{alg:exact_backprop_diag}
\begin{algorithmic}
\State $M$ = $\textbf{H}^{\mathcal{L}}$
\For{$j=L,L-1,\dots,1$}
\State $\textsc{db}^{(j)}_\theta$ = $\mathcal{D}\left(J_{\theta_j}f^{(j)}_{\theta_j}(x_{j-1})^\top \cdot M \cdot J_{\theta_j}f^{(j)}_{\theta_j}(x_{j-1})\right)$
\State $M$ = $\textcolor{BrickRed}{\mathcal{D}}\textcolor{BrickRed}{\Big(} J_{x_{j-1}}f^{(j)}_{\theta_j}(x_{j-1})^\top \cdot M \cdot J_{x_{j-1}}f^{(j)}_{\theta_j}(x_{j-1})\textcolor{BrickRed}{\Big)}$
\EndFor
\State $\textsc{db}_\theta$ = $(\textsc{db}^{(1)}_\theta, \dots, \textsc{db}^{(L)}_\theta)$
\State \textbf{return} $\textsc{db}_\theta$
\end{algorithmic}
\end{algorithm}
\vspace*{-\baselineskip}

\textbf{Proposition.} For an autoencoder network, the memory requirement of the Algorithm scale \emph{linearly} both in number of parameter and in number of pixels.
\begin{proof}
    The bottlenecks are the storage of the matrixes $\textsc{db}^{(j)}_\theta
\in\mathbb{R}^{|\theta_j|}$ and $
M
\in\mathbb{R}^{|x_{j-1}|}
$ at each step $j$
\end{proof}

\paragraph{Skip-connections}
For any given submodule $g_\theta$, a skip-connection layer $\textsc{sc}(g)_\theta$ is defined as $x  \longmapsto (g_\theta(x), x)$. The Jacobian with respect to the parameter is the same as the Jacobian of the $g_\theta$ while the Jacobian with respect to the input is $
    J_x \textsc{sc}(g)_\theta(x) 
    =
    \left(\begin{array}{c}
        J_x g_\theta(x) \\ \hline
        \mathbb{I}
    \end{array}\right)
    \in\mathbb{R}^{(O+I)\times I}$. 

\textbf{Proposition.} If $M$ is diagonal, then one step of Alg 1 can be computed as
\begin{equation*}\label{eq:skipconn_recursive_def}
    \mathcal{D}\left( J_x \textsc{SC}(g)_\theta(x)^\top \cdot M \cdot J_x \textsc{SC}(g)_\theta(x) \right)
    =
    \mathcal{D}\left( J_x g_\theta(x)^\top M_{11} J_x g_\theta(x) \right)
    + \mathcal{D} (M_{22}).
\end{equation*}
\begin{proof}
    Let $
    M
    =
    \begin{pmatrix}
    M_{11} & M_{12} \\
    M_{21} & M_{22}
    \end{pmatrix}$ and then
    
\begin{align*}
    J_x \textsc{SC}(g)_\theta(x)^\top \cdot &\,M \cdot J_x \textsc{SC}(g)_\theta(x) 
     =
    \left(\begin{array}{c|c}
        J_x g_\theta(x)^\top &
        \mathbb{I}
    \end{array}\right)
    \begin{pmatrix}
    M_{11} & M_{12} \\
    M_{21} & M_{22}
    \end{pmatrix}
    \left(\begin{array}{c}
        J_x g_\theta(x) \\ \hline
        \mathbb{I}
    \end{array}\right) \\
    & =
    J_x g_\theta(x)^\top M_{11} J_x g_\theta(x)
    + M_{12} J_x g_\theta(x)
    + J_x g_\theta(x)^\top M_{21}
    + M_{22}
\end{align*}
\end{proof}


%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}

\begin{comment}
    

{\small
\bibliographystyle{splncs04}
\bibliography{references}
}

\end{comment}


