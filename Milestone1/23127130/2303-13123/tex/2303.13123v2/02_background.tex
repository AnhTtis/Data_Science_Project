While several different sources and taxonomies of uncertainties have been proposed \cite{gawlikowski2021,ditlevsen2008}, the Bayesian framework \cite{bishop2006,kendall2017} distinguishes between two of them: aleatoric and epistemic. These can be derived directly from the Bayesian model average (BMA) predictive distribution
\begin{equation}
  \label{pred_dist}
   p(y \vert x, D) = \int \underbrace{p(y \vert x,\theta)}_\text{likelihood} \underbrace{p(\theta \vert D)}_\text{posterior} \diff \theta,
\end{equation}
where $(x, y)$ is an input-output pair, $D$ is the training data and $\theta$ the model parameters. The Shannon entropy $\text{H}(\cdot)$ of the predictive distribution is a common measure of total predictive uncertainty \cite{houlsby2011bayesian}. This \emph{predictive entropy} can be decomposed into the \emph{expected entropy} as a measure of aleatoric uncertainty, and the \emph{mutual information}, $\mathbf{I}$, representing epistemic uncertainty \cite{kendall2017}: 
\begin{equation}
  \label{uncertainty_mi}
   \underbrace{\text{H}(p(y \vert x, D))}_\text{predictive entropy} =  \underbrace{\mathbb{E}_{p(\theta \vert D)}[\text{H}(p(y \vert x,\theta))]}_\text{expected entropy - aleatoric} +  \underbrace{\mathbf{I}[p(y, \theta \vert x,D)]}_\text{mutual information - epistemic} \text{\cite{schweighofer2023}}. 
\end{equation}

Aleatoric uncertainty represents noise or variations that arise from ambiguities in the data, e.g., vague tumor boundaries resulting from gradual tissue infiltration. Epistemic uncertainty, in this framework measured as mutual information $\mathbf{I}$, quantifies the degree to which the model itself should be trusted. However, recent works  
caution against using this decomposition by demonstrating behavioral incoherences of mutual information \cite{schweighofer2023,schweighofer2023quantification,pmlr-v216-wimmer23a}, calling for new formulations to calculate epistemic uncertainty. Table \ref{table_measures} lists the measures included in our study, such as expected pairwise KL-divergence (EPKL) \cite{schweighofer2023}: a recently proposed method which uses pairwise comparisons between the predictive distributions of possible models and weights to calculate epistemic uncertainty. We also consider the pixel-wise variance of mean predictions averaged over samples from the posterior distribution, a measure we call Pixel Variance. Prior work \cite{kahl2024values,mucsanyi2024benchmarking} suggests there is no universal method for uncertainty estimation. 
Thus, the measure of magnitude of a targeted uncertainty type becomes a design choice that needs to be selected for the dataset and task at hand.
\begin{table}
    \begin{center}
    \caption{Overview of uncertainty measures with targeted uncertainty types.}
    \label{table_measures}
    \begin{tabular}{|l|l|l|}
        \hline
        Targeted Type &  Uncertainty Measure & Definition\\
        \hline
        Predictive  &  Predictive Entropy & $H(\mathbb{E}_{q(\theta \vert D)}[p(y \vert x, \theta)])$\\
        Aleatoric  &  Expected Entropy & $\mathbb{E}_{q(\theta \vert D)}[H(p(y \vert x, \theta))]$\\
        Epistemic  & Mutual Information & $I(p(y,\theta \vert x, D))$\\
        Epistemic  & Expected Pairwise KL & $\mathbb{E}_{q(\theta \vert D)}[\mathbb{E}_{q(\Tilde{\theta} \vert D)}[D_{\text{KL}}(p(y \vert x, \theta) \vert \vert p(y \vert x, \Tilde{\theta}) )]]$\\
        Epistemic  & Pixel Variance & $\text{Var}_{q(\theta \vert D)}[\textrm{sigmoid}(\mu_{\theta})]$\\
        \hline
    \end{tabular}
    %\vspace*{-\baselineskip}
    \end{center}
\end{table}

Kendall et al. \cite{kendall2017} recommend jointly modelling aleatoric and epistemic uncertainty for regression and classification tasks in computer vision. Prior methods that model both types of uncertainty typically combine Mean-Variance networks with diagonal covariance matrices and Dropout \cite{kendall2017} or utilize Gaussian-Process based convolutional layers \cite{popescu2021distributional}. Recent works, however, focus on modeling either one or the other component \cite{kohl2018probunet,monteiro2020}. Generally, aleatoric uncertainty techniques rely on mixing deterministic segmentation architectures with generative components \cite{baumgartner2019phiseg,kohl2019hierarchical,selvan2020uncertainty}. Common epistemic modeling techniques incorporate dropout, ensembles and multi-head models \cite{kendall2017,blundell2016ensemble,lee2016stochastic,rupprecht2017learning}.

Fairly evaluating the performance of uncertainty estimation techniques on real-world tasks is challenging due to combinatorial design factors. Kahl et al. \cite{kahl2024values} address this issue by providing a framework which standardizes evaluations for uncertainty estimation methods. Our work coheres to this framework in the sense that we determine how Prediction Models, Uncertainty Measures and Aggregation strategies impact uncertainty estimation on OOD detection tasks for a fixed U-net architecture.
Our results add insight to the recent discussion which calls into question the suitability of common uncertainty measures \cite{schweighofer2023,schweighofer2023quantification,pmlr-v216-wimmer23a}.