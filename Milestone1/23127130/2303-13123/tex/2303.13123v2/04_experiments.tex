Our method validation is based on the ValUES framework by \cite{kahl2024values} for evaluating segmentation uncertainty. All benchmark models are constructed by combining an aleatoric and an epistemic component, using the same U-net backbone architecture for comparability. As aleatoric components we consider mean predictions of a U-net and mean-variance predictions of SSN, with diagonal and low-rank covariance matrices. The epistemic components are implemented as Ensembles, MC-Dropout and our post-hoc Laplace approximation. For the nine \textit{method combinations} we calculate the following uncertainty measures: Predictive Entropy, Expected Entropy, Mutual Information, Expected Pairwise KL (EPKL) \cite{schweighofer2023} and Pixel Variance. With exception of the EPKL all measures yield pixel-wise uncertainty heatmaps. We consider sum aggregation as well as a patch based strategy to take the uncertainty from pixel to image level. Patch aggregation sums uncertainties within a $10^2$ sliding window across the image, selecting the patch with the highest uncertainty as the image-level score. Table \ref{table_measures} lists the calculated uncertainty measures along their targeted uncertainty type and their definition.

\begin{figure*}[t]
    \begin{center}
    \includegraphics[width=0.9\linewidth]{figure_discussion.pdf}
    %\vspace*{-\baselineskip}
    %\vspace*{-\baselineskip}
    \end{center}
       \caption{\textbf{Left} - OOD performance measured by AUROC across epistemic components for Mutual Information (MI), Expected Pairwise KL (EPKL) and Pixel Variance (PV). Models using Laplace Approximations with EPKL and PV reach highest AUROC values on average. \textbf{Right} - Uncertainty Measures for predictive and aleatoric uncertainty perform on par indicating weak disentanglement.   }
    \label{fig:auroc_discussion}
    \vspace*{-\baselineskip}
\end{figure*}

All experiments are conducted on three datasets: the ISIC19 skin lesion dataset \cite{combalia2019bcn20000,codella2018skin,tschandl2018ham10000}, the BRATS dataset \cite{menze2014multimodal,bakas2017advancing,bakas2018identifying} and the first Prostate segmentation task from the QUBIQ 2021 challenge\footnote{https://qubiq21.grand-challenge.org}.
For the ISIC19 dataset we assign three OOD datasets, representing distribution shifts: Derm-Skin (DERM), Clin-Skin (CLINIC) \cite{Pacheco2020} and the PAD-UFES-20 dataset \cite{pacheco2020pad}. The Derm-Skin dataset contains 1,565 images of healthy skin, cropped out of the ISIC dataset. The Clin-Skin dataset contains 723 images showing healthy skin gathered from social networks. The PAD-UFES-20 dataset contains 1570 photos of skin lesions collected from smartphone cameras. For the BRATS and Prostate datasets, we follow the experimental setup of \cite{fuchs2021practical} and augment the images with Motion, Spike, Ghosting and Noise artifacts, which are regularly observed in MR images. For training and implementation details we refer to the supplementary material.

% \subsection{Out-of-distribution detection on image level}

Image Level OOD detection can be viewed as a binary classification task across the ID test set and all OOD test sets. We therefore calculate the Area Under the Receiver Operating Characteristic Curve (AUROC) for all method combinations, uncertainty measures and aggregation strategies to assess how well different combinations can separate OOD from ID images. The AUROCs were calculated with \texttt{sklearn} \cite{scikit-learn} using per image ground truth binary labels (0-ID, 1-OOD) versus uncertainty scores as target predictions.
Figure~\ref{fig:auroc_experiment} shows the mean AUROC values for different method combinations and uncetainty measures across datasets. Note that for the method combinations, which use a U-net as an aleatoric component, the EPKL is not defined since it uses a KL divergence term, which is again not defined for two Dirac measures. For the Prostate and ISIC dataset, combinations of the LSN with EPKL and PV yield the highest AUROC values. On the BRATS dataset method combinations that use the Laplace approximation range after models using Dropout. We find that over all datasets method combinations that use the Laplace approximation for their parameter distribution yield the best OOD capabilities when combined with Pixel Variance and EPKL as shown in the left plot of Figure~\ref{fig:auroc_discussion}.

\begin{figure*}[t]
    \begin{center}
    \includegraphics[width=0.8\linewidth]{mean_auroc_per_dataset.pdf}
    \end{center}
       \caption{\textbf{OOD performance} measured by AUROC across models, marginalized over aggregation strategies, for Mutual Information (MI), Expected Pairwise KL (EPKL) and Pixel Variance (PV). LSN models with EPKL and PV reach highest AUROC values for Prostate and ISIC respectively. }
    \label{fig:auroc_experiment}
    %\vspace*{-\baselineskip}
\end{figure*}

Additionally, we evaluate how well certain distribution shifts are detected by calculating the mean epistemic uncertainty assigned to a given OOD test set and normalizing it by the uncertainty assigned to the respective ID test set obtained from our ID dataset split. This compares how different distribution shifts influence the absolute values of epistemic uncertainty assigned, providing intuition on each method's sensitivity to OOD data. In Table \ref{uncertainty_ratios} we display which method combinations on the EPKL measure are able to assign 5\% and 10\% more uncertainty to the OOD set. Since the EPKL measure does not require aggregation over pixels, we can compare the method combinations directly on all datasets without bias introduced by a aggregation strategy. We find that the LSN model can detect most distribution shifts in terms of assigning a higher EPKL. The method, however, fails to identify Random Motion augmentations as ID, which have been used to augment images during training. 

%\vspace*{-\baselineskip}
\begin{table}
\scriptsize
\begin{center}
  \caption{OOD datasets with uncertainty measured by EPKL compared to ID datasets. A orange check (\greencheck) signifies an average uncertainty that is at least 5\% higher than the ID dataset, while a green check (\yellowcheck) indicates a 10\% or greater increase in uncertainty. For the ID Motion dataset an orange check (\greencheck \textcolor{BurntOrange}{*}) signifies an average uncertainty that is at most 5\% lower than the ID dataset. }
\label{uncertainty_ratios}
   %\vspace{.5em}
  \centering
  \begin{tabular}{lllll|llll|lll|ll}
    \toprule
    
    & \multicolumn{4}{c}{Prostate} & \multicolumn{4}{c}{Brats}& \multicolumn{3}{c}{ISIC}\\
    \cmidrule{2-5}
    \cmidrule{6-9}
    \cmidrule{10-12}
    
    
    Model       & Motion & Ghost & Spike & Blur & Motion & Ghost & Spike & Blur & Clin & Derm & Padufes &\greencheck &\yellowcheck \\
    \midrule
    Ensemble SSN (diag.)  & \xmark  & \greencheck \yellowcheck & \greencheck \yellowcheck & \xmark & \xmark & \greencheck \yellowcheck & \greencheck \yellowcheck& \xmark & \greencheck \yellowcheck & \greencheck \yellowcheck  & \greencheck \yellowcheck & 7& 7\\
    Ensemble SSN  & \xmark  & \greencheck \yellowcheck & \greencheck & \xmark & \xmark & \greencheck \yellowcheck & \greencheck \yellowcheck  & \greencheck \yellowcheck & \greencheck \yellowcheck & \greencheck \yellowcheck  & \greencheck \yellowcheck &8&7\\
    \midrule
    Dropout SSN (diag.)       & \xmark  & \greencheck \yellowcheck& \greencheck \yellowcheck& \xmark & \xmark & \greencheck \yellowcheck  & \greencheck \yellowcheck & \xmark & \greencheck \yellowcheck & \greencheck \yellowcheck  &\greencheck \yellowcheck &7&7\\
    Dropout SSN      & \greencheck \textcolor{BurntOrange}{*}  & \greencheck \yellowcheck& \greencheck \yellowcheck& \xmark & \xmark & \greencheck \yellowcheck  & \greencheck \yellowcheck & \xmark & \xmark & \xmark  & \greencheck \yellowcheck &5&6\\
    \midrule 
    LSN (diag.)    & \greencheck \textcolor{BurntOrange}{*} & \greencheck & \xmark & \xmark & \xmark & \greencheck \yellowcheck  & \greencheck \yellowcheck & \xmark & \greencheck \yellowcheck & \greencheck \yellowcheck  & \greencheck \yellowcheck &7&5\\
    \textbf{LSN}      & \xmark  & \greencheck \yellowcheck & \greencheck \yellowcheck & \greencheck & \xmark & \greencheck \yellowcheck & \greencheck \yellowcheck & \greencheck \yellowcheck & \greencheck \yellowcheck & \greencheck \yellowcheck  & \greencheck \yellowcheck &\textbf{9}&\textbf{8}\\
    \bottomrule
  \end{tabular}

   %\vspace{.5em}
  \centering
 
%\vspace*{-\baselineskip}
%\vspace*{-\baselineskip}
\end{center}
\end{table}
%\vspace*{-\baselineskip}
%\vspace*{-\baselineskip}
