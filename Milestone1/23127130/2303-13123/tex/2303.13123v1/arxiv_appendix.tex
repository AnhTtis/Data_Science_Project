


\section{Appendix}
\subsection{Implementation and Training Details}
\label{appendix:training_details}
All architectures were trained using the Adam optimizer \citep{kingma2014adam} and its default PyTorch configuration. For models with Dropout the learning rate was 0.005, for all other models the learning rate was 0.0001. We used a batch size of 32 and trained all models for 60 epochs. These hyperparameters were found with a grid search on the validation set. Computations were performed on an internal GPU cluster. To retrieve epistemic uncertainties we sample $50$ samples from the parameter distributions given by Dropout and the post-hoc Laplace approximation.

\subsection{Fast Hessian Approximation}
\label{appendix:hessian_details}
The aim of this section is to clarify the following sequence of approximations for the Hessian $H_\theta $
\[
H_\theta 
\,\overset{(1)}{\approx}\,
\textsc{ggn}_\theta
\,\overset{(2)}{\approx}\,
\textsc{ggnd}_\theta
\,\overset{(3)}{\approx}\,
\textsc{db}_\theta
\]
where 
$\textsc{ggn}_\theta$ and $\textsc{ggnd}_\theta$ refer to the Generalized Gau√ü Newton matrix and its Diagonal \citep{botev2020} and $\textsc{db}_\theta$ is the matrix found by the diagonal backpropagation algorithm \citep{miani_2022_neurips}. Each of the above approximations trade exactness of the found solution for speed of its computation. Specifically, the approximation $(1)$ yields a matrix $\textsc{ggn}_\theta$ that is always positive semi-definite. Its diagonal given by $(2)$ is commonly used for the Laplace approximation in neural networks, since its Inverse is computationally feasible, often referred to as diagonal Laplace. In the following we develop the terminology needed to explain our contribution in extending the diagonal backpropagation algorithm $\textsc{db}_\theta$ to nested skip-connection layers.

\subsection{Notation}
Consider a neural network (\textsc{nn}) $f_\theta:\mathcal{X}\rightarrow\mathcal{Y}$ with $L$ layers. The parameter $\theta = (\theta_1, \dots, \theta_L) \in\Theta$ is the concatenation of the parameters for each layer $i \in \{1,...,L \}$. The \textsc{nn} is a composition of $L$ functions $f^{(1)},f^{(2)},\dots,f^{(L)}$, where $f^{(i)}$ is parametrized by $\theta_{i}$.
\[
f_\theta
:=
f^{(L)}_{\theta_L}\circ f^{(L-1)}_{\theta_{L-1}} 
\circ\,\dots\,\circ 
f^{(2)}_{\theta_2} \circ f^{(1)}_{\theta_1}.
\]
Since we need explicit access to the intermediate values, we call the input $x_0\in\mathcal{X}$ and iteratively define $x_i:=f^{(i)}_{\theta_i}(x_{i-1})$ for $i=1,\dots,L$, such that the \textsc{nn} output is $x_L\in\mathcal{Y}$. This notation can be visually presented as
\begin{align*}
\mathcal{X}
& \overset{f_\theta}{\xrightarrow{\hspace*{11.3cm}}}
    \mathcal{Y} 
\\
x_0 
& \underset{f^{(1)}_{\theta_1}}{\xrightarrow{\hspace*{1cm}}} x_1 \longrightarrow 
\quad\dots\quad 
\longrightarrow 
x_{i-1} \underset{f^{(i)}_{\theta_i}}{\xrightarrow{\hspace*{1cm}}} x_i \longrightarrow 
\quad\dots\quad 
\longrightarrow
x_{L-1} \underset{f^{(L)}_{\theta_L}}{\xrightarrow{\hspace*{1cm}}} 
x_L
\end{align*}

We highlight that there is no restriction of what a layer $f^{(i)}$ can be, in particular, it can be a composition of functions itself, translating to an object of the \emph{sequential} class in PyTorch. This recursive structure of the function, which is replicated by its implementation in PyTorch, will be key in extending the diagonal backpropagation method to nested skip-connections.

We define the \emph{diagonal} operator $\mathcal{D}$ for a quadratic matrix $M$ of size $m\in\mathbb{N}^+$
\[
\mathcal{D}: 
\mathbb{R}^{m\times m}
\rightarrow
\mathbb{R}^{m\times m}
\]
by
\[
[\mathcal{D}(M)]_{ij}
:=
\left\{
\begin{array}{ll}
    M_{ij} & \text{ if } i=j \\
    0 & \text{ if } i\not=j
\end{array}
\right.
\qquad
\forall i,j=1,\dots,m
\]
Note that this is a trace-preserving operator, a property that will be inherited by our final approximation. Moreover, note that applying this operator induces a systematic bias, specifically it decreases the Von Neumann entropy of the matrix. This is due to a smoothing effect on the eigenspectrum.

\subsection{Jacobian of the Neural Network}
We are interested in the Jacobian $J_\theta f_\theta(x_0)$ of the \textsc{nn} with respect to the parameter $\theta$. Each column of the Jacobian is the derivative of the output vector w.r.t.\@ a single parameter. We can then group the parameters (i.e. columns) layer by layer
\begin{align*}
J_\theta f_\theta(x_0) 
& = 
\left(\begin{array}{c|c|c|c|c}
    & & & &\\
    J_{\theta_1}f_\theta(x_0) &
    \,\dots\, &
    J_{\theta_i}f_\theta(x_0) &
    \,\dots\, &
    J_{\theta_L}f_\theta(x_0) \\
    & & & &
\end{array}\right)
\\
& = 
\left(\begin{array}{c|c|c|c|c}
    & & & & \\
    J_{\theta_1}
    \left(
        f^{(1)}_{\theta_1}
        \circ\dots\circ
        f^{(L)}_{\theta_L}
    \right)
    (x_0) &
    \,\dots\, &
    J_{\theta_i}
    \left(
        f^{(i)}_{\theta_i}
        \circ\dots\circ
        f^{(L)}_{\theta_L}
    \right)
    (x_{i-1}) &
    \,\dots\, &
    J_{\theta_L}f^{(L)}_{\theta_L}(x_{L-1}) \\
    & & & &
\end{array}\right),
\end{align*}
where the second equality comes from the fact that each layer only depends on its respective parameters, i.e.
\[ 
J_{\theta_j} f^{(i)}_{\theta_i} (x_{i-1}) = 0 
\quad \text{ if }i\not=j.
\]

Exploiting this block-structure, we can focus on a single layer Jacobian $J_{\theta_i}f_{\theta}(x_0)$, and concatenate them afterwards. With the chain rule we get 
\begin{equation}\label{eq:jacobian_chain_rule}
J_{\theta_i}f_{\theta}(x_0)
=
J_{\theta_i}
\left(
        f^{(i)}_{\theta_i}
        \circ\dots\circ
        f^{(L)}_{\theta_L}
    \right)
(x_{i-1}) 
=
\left(
    \prod_{j=L}^{i+1} 
    J_{x_{j-1}}f^{(j)}_{\theta_j}(x_{j-1})
\right)
J_{\theta_i}f^{(i)}_{\theta_i}(x_{i-1}).
\end{equation}

The intuition for the chain rule is that the Jacobian $J_{\theta_i}f_{\theta}(x_0)$ for layer $i$ is the composition of the Jacobians w.r.t.\@ the \emph{input} $J_{x_{j-1}}f^{(j)}_{\theta_j}(x_{j-1})$ of subsequent layers $j=L,L-1,\dots,i+2,i+1$, times the Jacobian w.r.t.\@ the \emph{parameters} $J_{\theta_i}f^{(i)}_{\theta_i}(x_{i-1})$ of the specific layer $i$. Thus, we can reuse computation for one layer to improve the computation of other layers, specifically the product of Jacobians w.r.t.\@ the input.


\subsection{Generalized Gauss Newton method}
For the Laplace approximation of a given loss function $\mathcal{L}:\mathcal{Y}\rightarrow\mathbb{R}$ we need to compute the Hessian of the composition of \textsc{nn} and loss with respect to the parameters $\nabla^2_{\theta} \mathcal{L}(f_\theta(x_0))
\in\mathbb{R}^{|\theta|\times|\theta|}$. In the following we denote the length of vector $v$ by $|v|$. According to the chain rule it holds, that
\begin{equation}\label{eq:hessian_chain_rule}
\underbrace{\nabla^2_{\theta} \mathcal{L}(f_\theta(x_0))}_{=:H_\theta}
=
\underbrace{
J_{\theta}f_\theta(x_0)^\top
    \cdot 
    \nabla^2_{x_L}\mathcal{L}(x_L) 
    \cdot
J_{\theta}f_\theta(x_0)
}_{=:\textsc{ggn}_\theta}
+
\sum_{o=1}^{|x_L|} 
    [\nabla_{x_L}\mathcal{L}(x_L)]_o \cdot
    \nabla^2_{\theta} [f_\theta(x_0)]_o,
\end{equation}
where $[v]_o$ refers to the $o$-th component of vector $v$. In this sense, the Generalized Gauss-Newton matrix $\textsc{ggn}_\theta$ is commonly used as a positive-definite approximation of the full Hessian $H_\theta$. The positive definiteness follows from the positive definiteness of the Hessian of the loss function with respect to the \textsc{nn} output $H^{\mathcal{L}}:=\nabla^2_{x_L}\mathcal{L}(x_L) \in\mathbb{R}^{|x_L|\times|x_L|}$, which holds for common losses like Mean Squared Error or Cross-Entropy.

Consider a layer-by-layer block structure of the Generalized Gauss-Newton
\[
\textsc{ggn}_\theta
=
\left(\begin{array}{cccccc}
    J_{\theta_1}f_\theta(x_0)^\top
        H^{\mathcal{L}}
    J_{\theta_1}f_\theta(x_0)
        & 
        J_{\theta_1}f_\theta(x_0)^\top
            H^{\mathcal{L}}
        J_{\theta_2}f_\theta(x_0)
        &
        & \dots 
        &
        & 
        J_{\theta_1}f_\theta(x_0)^\top
            H^{\mathcal{L}}
        J_{\theta_L}f_\theta(x_0)
        \\ \\%\hline
    J_{\theta_2}f_\theta(x_0)^\top
        H^{\mathcal{L}}
    J_{\theta_1}f_\theta(x_0)
        & 
        J_{\theta_2}f_\theta(x_0)^\top
            H^{\mathcal{L}}
        J_{\theta_2}f_\theta(x_0)
        & 
        &
        &
        & \\ %
    & & & & & \\ %
    \vdots 
        & 
        &
        & \,\ddots\, 
        &
        & \\ %
    & & & & & \\ %
    J_{\theta_L}f_\theta(x_0)^\top
        H^{\mathcal{L}}
    J_{\theta_1}f_\theta(x_0)
        & 
        &
        &
        & 
        & 
        J_{\theta_L}f_\theta(x_0)^\top
            H^{\mathcal{L}}
        J_{\theta_L}f_\theta(x_0)
\end{array}\right).
\]

Its \emph{block-diagonal} approximation
\[
\textsc{ggnb}_\theta
=
\left(\begin{array}{cccccc}
    J_{\theta_1}f_\theta(x_0)^\top
        H^{\mathcal{L}}
    J_{\theta_1}f_\theta(x_0)
        & 
        0
        &
        & \dots 
        &
        & 
        0
        \\ \\%\hline
    0
        & 
        J_{\theta_2}f_\theta(x_0)^\top
            H^{\mathcal{L}}
        J_{\theta_2}f_\theta(x_0)
        & 
        &
        &
        & \\ %
    & & & & & \\ %
    \vdots 
        & 
        &
        & \,\ddots\, 
        &
        & \\ %
    & & & & & \\ %
    0
        & 
        &
        &
        & 
        & 
        J_{\theta_L}f_\theta(x_0)^\top
            H^{\mathcal{L}}
        J_{\theta_L}f_\theta(x_0)
\end{array}\right)
\]
is usually considered as a cheaper-to-compute approximation of the full $\textsc{ggn}_\theta$. According to chain rule in Eq. \eqref{eq:jacobian_chain_rule}, we can explicitly write the diagonal block $\textsc{ggnb}^{(i)}_\theta=J_{\theta_i}f_\theta(x_0)^\top H^{\mathcal{L}} J_{\theta_i}f_\theta(x_0)$ of the $i$-th layer as
\begin{align}\label{eq:ggn_chain_rule}
    \textsc{ggnb}^{(i)}_\theta
    & =
    J_{\theta_i}f^{(i)}_{\theta_i}(x_{i-1})^\top
    \left(
        \prod_{j=i+1}^{L} 
        J_{x_{j-1}}f^{(j)}_{\theta_j}(x_{j-1})^\top
    \right)
    \cdot H^{\mathcal{L}} \cdot
    \left(
        \prod_{j=L}^{i+1} 
        J_{x_{j-1}}f^{(j)}_{\theta_j}(x_{j-1})
    \right)
    J_{\theta_i}f^{(i)}_{\theta_i}(x_{i-1}).
\end{align}
This can be rearranged as a sequence of $(J^\top M J)$-like operators iteratively applied to $H^{\mathcal{L}}$, as
\begin{align}\label{eq:ggn_chain_rule_rearranged}
    \textsc{ggnb}^{(i)}_\theta
    & =
    J_{\theta_i}f^{(i)}_{\theta_i}(x_{i-1})^\top
    \Bigg(
        J_{x_{i}}f^{(i+1)}_{\theta_{i+1}}(x_{i})^\top
        \bigg(
        J_{x_{i+1}}f^{(i+2)}_{\theta_{i+2}}(x_{i+1})^\top
        \Big(
            \quad\dots\quad
            \big(
            J_{x_{L-1}}f^{(L)}_{\theta_L}(x_{L-1})^\top \cdot H^{\mathcal{L}} \cdot
            \\ \nonumber & \qquad\qquad \cdot
            J_{x_{L-1}}f^{(L)}_{\theta_L}(x_{L-1})
            \big)
            \quad\dots\quad
        \Big)
        J_{x_{i}}f^{(i+1)}_{\theta_{i+1}}(x_{i})
        \bigg)
        J_{x_{i+1}}f^{(i+2)}_{\theta_{i+2}}(x_{i+1})
    \Bigg)
    J_{\theta_i}f^{(i)}_{\theta_i}(x_{i-1}).
\end{align}

From this expression, we can build an efficient backpropagation-like algorithm to compute $\textsc{ggnb}_\theta$.

\begin{algorithm}[H]
\caption{Computation of $\textsc{ggnb}_\theta$ (exact backpropagation)}\label{alg:exact_backprop}
\begin{algorithmic}
\State $M$ = $H^{\mathcal{L}}$
\For{$j=L,L-1,\dots,1$}
\State $\textsc{ggnb}^{(j)}_\theta$ = $J_{\theta_j}f^{(j)}_{\theta_j}(x_{j-1})^\top \cdot M \cdot J_{\theta_j}f^{(j)}_{\theta_j}(x_{j-1})$
\State $M$ = $J_{x_{j-1}}f^{(j)}_{\theta_j}(x_{j-1})^\top \cdot M \cdot J_{x_{j-1}}f^{(j)}_{\theta_j}(x_{j-1})$
\EndFor
\State $\textsc{ggnb}_\theta$ = $(\textsc{ggnb}^{(1)}_\theta, \dots, \textsc{ggnb}^{(L)}_\theta)$
\State \textbf{return} $\textsc{ggnb}_\theta$
\end{algorithmic}
\end{algorithm}

Note the two memory bottlenecks of this algorithm: after each step $j$ the involved matrices have sizes
\[
\textsc{ggnb}^{(j)}_\theta
\in\mathbb{R}^{|\theta_j|\times|\theta_j|}
\qquad\qquad
M
\in\mathbb{R}^{|x_{j-1}|\times|x_{j-1}|}.
\]
The former one is commonly avoided by just computing the diagonal of each block $\textsc{ggnb}^{(j)}_\theta$, and thus just computing the diagonal of the Generalized Gauss-Newton matrix, which we can refer to as $\textsc{ggnd}_\theta:=\mathcal{D}(\textsc{ggn}_\theta)=\mathcal{D}(\textsc{ggnb}_\theta)$. Using this approach together with a Laplace approximation of the loss landscape is called \emph{diagonal Laplace} and scales linearly in the number of parameter $|\theta|$. 

\begin{algorithm}[H]
\caption{Computation of $\textsc{ggnd}_\theta$ (exact backpropagation)}\label{alg:exact_backprop_diag}
\begin{algorithmic}
\State $M$ = $H^{\mathcal{L}}$
\For{$j=L,L-1,\dots,1$}
\State $\textsc{ggnd}^{(j)}_\theta$ = $\mathcal{D}\left(J_{\theta_j}f^{(j)}_{\theta_j}(x_{j-1})^\top \cdot M \cdot J_{\theta_j}f^{(j)}_{\theta_j}(x_{j-1})\right)$
\State $M$ = $J_{x_{j-1}}f^{(j)}_{\theta_j}(x_{j-1})^\top \cdot M \cdot J_{x_{j-1}}f^{(j)}_{\theta_j}(x_{j-1})$
\EndFor
\State $\textsc{ggnd}_\theta$ = $(\textsc{ggnd}^{(1)}_\theta, \dots, \textsc{ggnd}^{(L)}_\theta)$
\State \textbf{return} $\textsc{ggnd}_\theta$
\end{algorithmic}
\end{algorithm}
The memory bottlenecks of this modified version of Eq. \eqref{alg:exact_backprop} are
\[
\textsc{ggnd}^{(j)}_\theta
\in\mathbb{R}^{|\theta_j|}
\qquad\qquad
M
\in\mathbb{R}^{|x_{j-1}|\times|x_{j-1}|}.
\]

However, the quadratic dependence on the size of $x_j$ is still potentially problematic. In an encoder-decoder architecture, as in our case, the maximum of this size is realized in the input $x_0$ and output $x_L$. For image-based networks, this size corresponds to the number of pixels. With \emph{reasonable} resources this backpropagation algorithm is feasible for images with size $28\times28$, but it became infeasible for larger images. In order to deal with high-resolution images we need to avoid the quadratic dependency on the number of pixels in the output.


\subsection{Fast diagonal backprop}

The diagonal backpropagation computes an approximation of the Generalized Gauss-Newton. Inspired by Eq. \eqref{eq:ggn_chain_rule_rearranged}, it is defined, for each layer $i=1,\dots,L$, as
\begin{align}
    \textsc{db}^{(i)}_\theta
    & =
    J_{\theta_i}f^{(i)}_{\theta_i}(x_{i-1})^\top
    \mathcal{D}\Bigg(
        J_{x_{i}}f^{(i+1)}_{\theta_{i+1}}(x_{i})^\top
        \mathcal{D}\bigg(
        J_{x_{i+1}}f^{(i+2)}_{\theta_{i+2}}(x_{i+1})^\top
        \mathcal{D}\Big(
            \quad\dots\quad
            \mathcal{D}\big(
            J_{x_{L-1}}f^{(L)}_{\theta_L}(x_{L-1})^\top \cdot \mathcal{D}(H^{\mathcal{L}}) \cdot
            \\ \nonumber & \qquad\qquad \cdot
            J_{x_{L-1}}f^{(L)}_{\theta_L}(x_{L-1})
            \big)
            \quad\dots\quad
        \Big)
        J_{x_{i}}f^{(i+1)}_{\theta_{i+1}}(x_{i})
        \bigg)
        J_{x_{i+1}}f^{(i+2)}_{\theta_{i+2}}(x_{i+1})
    \Bigg)
    J_{\theta_i}f^{(i)}_{\theta_i}(x_{i-1}).
\end{align}

As before, we can build an efficient backpropagation-like algorithm from this expression to compute $\textsc{db}_\theta$.

\begin{algorithm}[H]
\caption{Computation of $\textsc{db}_\theta$ (approximated backpropagation)}\label{alg:approx_backprop}
\begin{algorithmic}
\State $M$ = $H^{\mathcal{L}}$
\For{$j=L,L-1,\dots,1$}
\State $\textsc{db}^{(j)}_\theta$ = $\mathcal{D}\left(J_{\theta_j}f^{(j)}_{\theta_j}(x_{j-1})^\top \cdot M \cdot J_{\theta_j}f^{(j)}_{\theta_j}(x_{j-1})\right)$
\State $M$ = $\mathcal{D}\left( J_{x_{j-1}}f^{(j)}_{\theta_j}(x_{j-1})^\top \cdot M \cdot J_{x_{j-1}}f^{(j)}_{\theta_j}(x_{j-1})\right)$
\EndFor
\State $\textsc{db}_\theta$ = $(\textsc{db}^{(1)}_\theta, \dots, \textsc{db}^{(L)}_\theta)$
\State \textbf{return} $\textsc{db}_\theta$
\end{algorithmic}
\end{algorithm}
The memory bottlenecks of this algorithm are
\[
\textsc{db}^{(j)}_\theta
\in\mathbb{R}^{|\theta_j|}
\qquad\qquad
M
\in\mathbb{R}^{|x_{j-1}|}
\]
thus allowing for linear scaling in both parameter and number of pixels in the output.

A key aspect, that enables this algorithm to be so efficient, is the simultaneous diagonalization and computation of the Jacobian product. Instead of first computing the full matrix and then discarding all non-diagonal elements, we directly compute only the diagonal elements of the products. However, this means that this operation needs to be implemented separately for every type of layer. Moreover, all diagonal matrices are stored implicitly, i.e. diagonal matrices are stored as vectors of the diagonal entries. 

\subsection{Skip-connections}
In PyTorch, objects of the class \emph{module} can be nested, allowing to create classes in a tree-like structure. All modules, but the root of such a tree are usually referred to as \emph{submodules}. It's then possible to form sequential neural networks by composing different \emph{submodules}, each representing a (potentially parametric) function as part of the architecture. For a given submodule $g_\theta:\mathbb{R}^I\rightarrow\mathbb{R}^O$, a skip-connection layer $\textsc{sc}(g)_\theta$ concatenates the submodule with the identity. It is defined as
\begin{align}
    \textsc{sc}(g)_\theta
    : \mathbb{R}^I &\longrightarrow \mathbb{R}^{O+I} \\ \nonumber
    x & \longmapsto (g_\theta(x), x)
\end{align}
Note that this is a submodule itself, and its parameter $\theta$ is the same as the parameter of the used submodule $g_\theta$.

The Jacobian with respect to the parameter is the same as the Jacobian of the submodule
\begin{equation}
    J_\theta \textsc{sc}(g)_\theta(x) = J_\theta g_\theta(x) \in\mathbb{R}^{O\times|\theta|}
\end{equation}
while the Jacobian with respect to the input is
\begin{equation}
    J_x \textsc{sc}(g)_\theta(x) 
    =
    \left(\begin{array}{c}
        J_x g_\theta(x) \\ \hline
        \mathbb{I}
    \end{array}\right)
    \in\mathbb{R}^{(O+I)\times I},
\end{equation}
where $\mathbb{I}\in\mathbb{R}^{I\times I}$ is the identity matrix of the same size as the input.

In order to perform the fast diagonal backpropagation through this kind of layer we need to compute the $\mathcal{D}(J^\top \cdot M\cdot J)$ operator efficiently. The one with respect to the parameter is straightforward: being the Jacobian identical to the one of the submodule $g$, we simply need to call the operator implemented on the submodule. The one with respect to the input, on the other hand, is slightly different. We can exploit the same block structure on the matrix $M\in\mathbb{R}^{(O+I)\times(O+I)}$ as
\begin{equation}
    M
    =
    \begin{pmatrix}
    M_{11} & M_{12} \\
    M_{21} & M_{22}
    \end{pmatrix},
    \qquad\qquad
    \text{where }
    M_{11}\in\mathbb{R}^{O\times O},
    M_{22}\in\mathbb{R}^{I\times I}.
\end{equation}
We can then explicity write
\begin{align*}
    J_x \textsc{SC}(g)_\theta(x)^\top \cdot M \cdot J_x \textsc{SC}(g)_\theta(x) 
    & =
    \left(\begin{array}{c|c}
        J_x g_\theta(x)^\top &
        \mathbb{I}
    \end{array}\right)
    \begin{pmatrix}
    M_{11} & M_{12} \\
    M_{21} & M_{22}
    \end{pmatrix}
    \left(\begin{array}{c}
        J_x g_\theta(x) \\ \hline
        \mathbb{I}
    \end{array}\right) \\
    & =
    J_x g_\theta(x)^\top M_{11} J_x g_\theta(x)
    + M_{12} J_x g_\theta(x)
    + J_x g_\theta(x)^\top M_{21}
    + M_{22}
\end{align*}
and thus, assuming $M$ is already in diagonal form, which implies $M_{12}=0$, $M_{21}=0$, we have
\begin{equation}\label{eq:skipconn_recursive_def}
    \mathcal{D}\left( J_x \textsc{SC}(g)_\theta(x)^\top \cdot M \cdot J_x \textsc{SC}(g)_\theta(x) \right)
    =
    \mathcal{D}\left( J_x g_\theta(x)^\top M_{11} J_x g_\theta(x) \right)
    + \mathcal{D} (M_{22}).
\end{equation}
This means that we can efficiently perform this backpropagation by calling the same operator on the submodule $g$, which returns $\mathcal{D}\left( J_x g_\theta(x)^\top M_{11} J_x g_\theta(x) \right)$ and then adding the diagonal $\mathcal{D} (M_{22})$.

\subsubsection{Recursiveness}
We highlight that this efficient implementation builds on a recursive structure, which unlocks several possibilities. 
The idea of the library is that every module has implemented the two backpropagation operators: $\mathcal{D}(J_x^\top \cdot M\cdot J_x)$ with respect to the input and $\mathcal{D}(J_\theta^\top \cdot M\cdot J_\theta)$ with respect to the parameter. Most importantly, these operators have to share the same syntax across all modules. For basic modules such as \textsc{linear}, \textsc{conv2d}, \textsc{relu}, \textsc{tanh}, these operators are hardcoded with tensor operations. The Skip-connection module however, has these operators coded with a recursive call on the submodule, as in Eq. \eqref{eq:skipconn_recursive_def}. In the Sequential module the two operators are implemented with a  backpropagation-like structure, as in algorithm \eqref{alg:approx_backprop}, with a recursive call on all the submodules in the sequence (in reverse order in the for loop).

In our work we use this to implement the skip-connections of the U-net \citep{ronneberger2015}. The submodule of a skip-connection is a sequential, which contains convolutions and other layers, as well as another skip-connection with the same structure.


