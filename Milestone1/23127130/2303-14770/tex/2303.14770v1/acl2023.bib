@string{ acmtois = "ACM Trans. Information Systems" }
@string{ acmtods = "ACM Trans. Database Systems" }
@string{ acmtalg = "ACM Trans. Algorithms" }
@string{ cacm = "CACM" }
@string{ adcs = "Proc. Aust. Doc. Comp. Symp." }
@string{ cikm = "Proc. CIKM" }
@string{ compsurv = "ACM Comp. Surv." }
@string{ dcc = "Proc. DCC" }
@string{ esa = "Proc. ESA" }
@string{ ecir = "Proc. ECIR" }
@string{ icde = "Proc. ICDE" }
@string{ ipm = "Inf. Proc. \& Man." }
@string{ sigir = "Proc. SIGIR" }
@string{ spe = "Soft. Prac. \& Exp." }
@string{ wsdm = "Proc. WSDM" }
@string{ parco = "Proc. Parallel Comput. Conf." }
@string{ infoscale = "Proc. Conf. Scalable Information Systems" }
@string{ laweb = "Proc. LA-WEB" }
@string{ irj = "Information Retrieval" }
@string{ www = "Proc. WWW" }
@string{ kdd = "Proc. KDD" }
@string{ icdm = "Proc. ICDM" }
@string{ vldb = "Proc. VLDB" }
@string{ pvldb = "PVLDB" }
@string{ acsc= "Proc. ACSC" }
@string{ acmcsurv = "ACM Computing Surveys" }
@string{ acmpress = "ACM Press" }
@string{ focs = "Proceedings of the Annual Symposium on Foundations of Computer Science" }
@string{ sea = "Proceedings of the International Symposium on Experimental Algorithms" }
@string{ soda = "Proceedings of the ACM-SIAM symposium on Discrete algorithms" }
@string{ spire = "Proceedings of the International Symposium on String Processing and Information Retrieval" }
@string{ acl = "Proceedings of the Annual Meeting of the Association for Computational Linguistics" }
@string{ tacl = "Transactions of the Association for Computational Linguistics" }
@string{ eacl = "Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics" }
@string{ aclhlt = "Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies" }
@string{ emnlp = "Proceedings of the Conference on Empirical Methods in Natural Language Processing" }
@string{ lrec = "Proceedings of the Conference on Language Resources and Evaluation" }
@string{ wmt = "Proceedings of the Workshop on Statistical Machine Translation" }
@string{ interspeech = "Proceedings of INTERSPEECH"}
@string{ icslp = "Proceedings of the International Conference of Spoken Language Processing"}
@string{ emnlp-conll = "Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning" }
@string{ icassp = "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing"}
@string{ asru = "Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop"}
@string{ swat = "Proceedings of the Annual Symposium Switching and Automata Theory"}
@string{ fsttcs = "Proceedings of Foundations of Software Technology and Theoretical Computer Science"}
@string{ cpm = "Proceedings of the Annual Symposium on Combinatorial Pattern Matching"}

@techreport{zhang2006vogel,
  title={Suffix Array and its
Applications in Empirical Natural Language Processing},
  author={Zhang, Ying and Vogel, Stephan},
  year=2006,
  institution={CMU, Pittsburgh PA}
}

% Golomb Coding
@inproceedings{church2007compressing,
  title={Compressing Trigram Language Models With {Golomb} Coding.},
  author={Church, Kenneth and Hart, Ted and Gao, Jianfeng},
  booktitle=emnlp,
  year={2007}
}

% FB 
@article{Chen:2015,
  title={Strategies for Training Large Vocabulary Neural Language Models},
  author={Chen, Welin and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1512.04906},
  year={2015}
}

% 1billion
@article{Chelba:2014,
  title={One billion word benchmark for measuring progress in statistical language modeling},
  author={Chelba, Ciprian and Mikolov, Tomas and Schuster, Mike and Ge, Qi and Brants, Thorsten and Koehn, Phillipp and Robinson, Tony},
  journal={arXiv preprint arXiv:1312.3005},
  year={2014}
}

@article{Shareghi:2016TACL,
	author = {Shareghi, Ehsan  and Petri, Matthias  and Haffari, Gholamreza  and Cohn, Trevor },
	title = {Fast, Small and Exact: Infinite-order Language Modelling with Compressed Suffix Trees},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {4},
	year = {2016},
	issn = {2307-387X},
	pages = {477--490}
}


% emnlp2015
@inproceedings{Shareghi:2015,
  author={Shareghi, Ehsan and Petri, Matthias and Haffari, Gholamreza and Cohn, Trevor},
  title={Compact, Efficient and Unlimited Capacity: Language Modeling with Compressed Suffix Trees},
  booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing},
  year={2015},
}

% Europarl 
@inproceedings{koehn2005europarl,
  title={Europarl: A parallel corpus for statistical machine translation},
  author={Koehn, Philipp},
  booktitle={Proceedings of the Machine Translation summit},
  year={2005},
}

% Largest - Common Crawl
@inproceedings{Buck:2014,
  title={N-gram counts and language models from the common crawl},
  author={Buck, Christian and Heafield, Kenneth and van Ooyen, Bas},
  booktitle={Proceedings of the Language Resources and Evaluation Conference},
  year={2014}
}

% DAC
@inproceedings{brisaboa2009directly,
  title={Directly addressable variable-length codes},
  author={Brisaboa, Nieves R and Ladra, Susana and Navarro, Gonzalo},
  booktitle={Proceedings of the International Symposium on String Processing and Information Retrieval},
  year={2009},
}

% RRR
@inproceedings{raman2002succinct,
  title={Succinct indexable dictionaries with applications to encoding k-ary trees and multisets},
  author={Raman, Rajeev and Raman, Venkatesh and Rao, S Srinivasa},
  booktitle={Proceedings of the thirteenth annual ACM-SIAM Symposium on Discrete algorithms},
  year={2002},
}

@article{He:2015,
  author    = {Hua He and
               Jimmy Lin and
               Adam Lopez},
  title     = {Gappy Pattern Matching on GPUs for On-Demand Extraction of Hierarchical
               Translation Grammars},
  journal   = tacl,
  volume    = {3},
  pages     = {87--100},
  year      = {2015}
}

@phdthesis{Lopez:2008,
  author       = {Adam Lopez}, 
  title        = {Machine Translation by Pattern Matching}, 
  school       = {University of Maryland},
  year         = 2008
}

@inproceedings{Callisoni-Burch:2005,
  author    = {Chris Callison{-}Burch and
               Colin J. Bannard and
               Josh Schroeder},
  title     = {Scaling Phrase-Based Statistical Machine Translation to Larger Corpora
               and Longer Phrases},
  booktitle = acl, 
  year      = {2005}
}

@article{DBLP:journals/corr/abs-2208-11981,
  author    = {Nigel H. Collier and
               Fangyu Liu and
               Ehsan Shareghi},
  title     = {On Reality and the Limits of Language Data},
  journal   = {CoRR},
  volume    = {abs/2208.11981},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2208.11981},
  eprinttype = {arXiv},
  eprint    = {2208.11981},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2208-11981.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{Zhang:2005, 
AUTHOR = {Ying Zhang and Stephan Vogel}, 
TITLE = {An Efficient Phrase-to-Phrase Alignment Model for Arbitrarily Long Phrase and Large Corpora}, 
BOOKTITLE = eacl,
year=2005
}

% Randomised LM with Perfect Hash
@inproceedings{levenberg2009stream,
  title={Stream-based randomised language models for {SMT}},
  author={Levenberg, Abby and Osborne, Miles},
  booktitle= emnlp,
  year={2009},
}

% Randomised LM
@inproceedings{Talbot:2007,
  author    = {David Talbot and
               Miles Osborne},
  title     = {Randomised Language Modelling for Statistical Machine Translation},
  booktitle = acl, 
  year      = {2007}
}

@inproceedings{Watanabe:2009,
  author    = {Taro Watanabe and
               Hajime Tsukada and
               Hideki Isozaki},
  title     = {A Succinct N-gram Language Model},
  booktitle = acl,
  year      = {2009}
}

@inproceedings{Sorensen:2011,
title = {Unary Data Structures for Language Models},
author  = {Jeffrey Sorensen and Cyril Allauzen},
year  = 2011,
booktitle = interspeech,
}

@inproceedings{Germann:2009,
 author = {Germann, Ulrich and Joanis, Eric and Larkin, Samuel},
 title = {Tightly Packed Tries: How to Fit Large Models into Memory, and Make Them Load Fast, Too},
 booktitle = {Proceedings of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing},
 year = {2009},
} 


@inproceedings{Chazelle:2004,
 author = {Chazelle, Bernard and Kilian, Joe and Rubinfeld, Ronitt and Tal, Ayellet},
 title = {The Bloomier Filter: An Efficient Data Structure for Static Support Lookup Tables},
 booktitle = soda,
 year = {2004},
 pages = {30--39}
} 

@article{Bengio:2003,
 author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
 title = {A neural probabilistic language model},
 journal = {Journal of Machine Learning Research},
 issue_date = {3/1/2003},
 volume = {3},
 year = {2003},
 pages = {1137--1155}
} 

@inproceedings{Mikolov:2010,
  author    = {Tomas Mikolov and
               Martin Karafi{\'{a}}t and
               Luk{\'{a}}s Burget and
               Jan Cernock{\'{y}} and
               Sanjeev Khudanpur},
  title     = {Recurrent neural network based language model},
  booktitle = interspeech,
  pages     = {1045--1048},
  year      = {2010}
}

@book{Rabiner:1993,
author = {Lawrence Rabiner and Biing-Hwang Juang},
title = {Fundamentals of speech recognition},
publisher = {Prentice-Hall},
year = {1993}
}

@book{Koehn:2010,
author = {Philipp Koehn},
title = {Statistical Machine Translation},
publisher = {Cambridge University Press},
year = {2010},
}

@article{Russo:2011,
  author    = {L. Russo and G. Navarro and A. Oliveira},
  title     = {Fully-Compressed Suffix Trees},
  journal   = acmtalg,
  volume    = {7},
  number    = {4},
  pages     = {article 53},
  year      = {2011}
}



@techreport{Burrows:1994,
  author = {Michael Burrows and David Wheeler},
  title = {A block sorting lossless data compression algorithm},
	institution = {Digital Equipment Corporation Systems Research Center},
  year = 1994,
  number = 124
}

@article{Ferragina:2008,
  author    = {Paolo Ferragina and
               Rodrigo Gonz{\'{a}}lez and
               Gonzalo Navarro and
               Rossano Venturini},
  title     = {Compressed text indexes: From theory to practice},
  journal   = {{ACM} J. of Exp. Algorithmics},
  volume    = {13},
  year      = {2008},
  doi       = {10.1145/1412228.1455268}
}

@article{Ferragina:2007,
  title = "Compressed Representations of Sequences and Full-Text Indexes",
  author = "P. Ferragina and G. Manzini and V. M{\"a}kinen and G. Navarro",
  journal = "ACM Trans. on Algorithms",
  year = 2007,
  volume = 3,
  number = 2,
  pages = "article 20"
}

@inproceedings{Munro:1996,
  author    = {Ian Munro},
  title     = {Tables},
  booktitle = fsttcs,
  pages     = {37--42},
  year      = {1996},
  doi       = {10.1007/3-540-62034-6_35}
}

@article{Kurtz:1999,
  author    = {Stefan Kurtz},
  title     = {Reducing the space requirement of suffix trees},
  journal   = {Softw., Pract. Exper.},
  volume    = {29},
  number    = {13},
  pages     = {1149--1171},
  year      = {1999},
  doi       = {10.1002/(SICI)1097-024X(199911)29:13<1149::AID-SPE274>3.0.CO;2-O}
}

@article{Juha:2006,
  author    = {Juha K{\"{a}}rkk{\"{a}}inen and
               Peter Sanders and
               Stefan Burkhardt},
  title     = {Linear work suffix array construction},
  journal   = {J. {ACM}},
  volume    = {53},
  number    = {6},
  pages     = {918--936},
  year      = {2006},
  url       = {http://doi.acm.org/10.1145/1217856.1217858},
  doi       = {10.1145/1217856.1217858},
  timestamp = {Tue, 06 Mar 2007 14:22:11 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/jacm/KarkkainenSB06},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Manber:1993,
  author    = {Udi Manber and
               Eugene W. Myers},
  title     = {Suffix Arrays: {A} New Method for On-Line String Searches},
  journal   = {{SIAM} Journal on Computing},
  volume    = {22},
  number    = {5},
  pages     = {935--948},
  year      = {1993},
  doi       = {10.1137/0222058}
}

@article{brown2020language,
  title={Language Models Are Few-shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@online{openwebtext,
    author    = "Aaron Gokaslan and Vanya Cohen",
    title     = "Openwebtext corpus",
    year      = {2019},
    url       = "http://web.archive.org/save/http://Skylion007.github.io/OpenWebTextCorpus",
}

@article{Ukkonen:1995,
  author    = {Esko Ukkonen},
  title     = {On-Line Construction of Suffix Trees},
  journal   = {Algorithmica},
  volume    = {14},
  number    = {3},
  pages     = {249--260},
  year      = {1995},
  doi       = {10.1007/BF01206331}
}

@inproceedings{Weiner:1973,
  author    = {Peter Weiner},
  title     = {Linear Pattern Matching Algorithms},
  booktitle = swat,
  year      = {1973},
  doi       = {10.1109/SWAT.1973.13}
}

% Compression algorithms
@article{nv-csurv07,
  title={Compressed full-text indexes},
  author={Navarro, Gonzalo and M{\"a}kinen, Veli},
  journal=compsurv,
  volume={39},
  number={1},
  pages={2},
  year={2007},
  publisher={ACM}
}

@article{n-csurv14,
  title = "Spaces, Trees and Colors: The Algorithmic Landscape of  Document Retrieval on Sequences",
  author = "G. Navarro",
  journal = compsurv,
  year = 2014,
  volume = 46,
  number = "4.52",
}

@article{sadakane2007compressed,
  title={Compressed suffix trees with full functionality},
  author={Sadakane, Kunihiko},
  journal={Theory of Computing Systems},
  volume={41},
  number={4},
  pages={589--607},
  year={2007},
  publisher={Springer}
}

@article{s-jda07,
  author    = {K. Sadakane},
  title     = {Succinct data structures for flexible text retrieval systems},
  journal   = jda,
  volume    = {5},
  number    = {1},
  pages     = {12--22},
  year      = {2007}
}


@article{s-tcs07,
  author    = {Kunihiko Sadakane},
  title     = {Compressed Suffix Trees with Full Functionality},
  journal   = {Theory Comput. Syst.},
  volume    = {41},
  number    = {4},
  pages     = {589--607},
  year      = {2007},
  doi       = {10.1007/s00224-006-1198-x}
}

@article{s-jalg03,
  author    = {K. Sadakane},
  title     = {New text indexing functionalities of the compressed suffix arrays},
  journal   = jalg,
  volume    = {48},
  number    = {2},
  pages     = {294--313},
  year      = {2003}
}



@inproceedings{kkp-dcc14,
  author    = {J. K{\"{a}}rkk{\"{a}}inen and
               D. Kempa and
               S. J. Puglisi},
  title     = {Hybrid Compression of Bitvectors for the {FM}-Index},
  booktitle = dcc,
  pages     = {302--311},
  year      = {2014}
}


% Ukkonen
@article{ukkonen1995line,
  title={On-line construction of suffix trees},
  author={Ukkonen, Esko},
  journal={Algorithmica},
  volume={14},
  number={3},
  pages={249--260},
  year={1995},
  publisher={Springer}
}

% T-MPHR Lossy Compression
@inproceedings{guthrie2010storing,
  title={Storing the web in memory: Space efficient language models with constant time retrieval},
  author={Guthrie, David and Hepple, Mark},
  booktitle=emnlp,
  year={2010}
}

@inproceedings{ggv03soda,
	author = "R. Grossi and A. Gupta and J. S. Vitter",
	title = "High-order entropy-compressed text indexes",
	booktitle = soda,
	year = 2003,
}

@inproceedings{j89focs,
  author    = {Guy Jacobson},
  title     = {Space-efficient Static Trees and Graphs},
  booktitle = focs,
  year      = {1989},
  hidecrossref  = {DBLP:conf/focs/FOCS30},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}


@inproceedings{w-swat73,
  author    = {Peter Weiner},
  title     = {Linear Pattern Matching Algorithms},
  booktitle = swat,
  year      = {1973},
  doi       = {10.1109/SWAT.1973.13}
}

@article{n-jda14,
  author    = {Gonzalo Navarro},
  title     = {Wavelet trees for all},
  journal   = {Journal of Discrete Algorithms},
  volume    = {25},
  pages     = {2--20},
  year      = {2014},
  url       = {http://dx.doi.org/10.1016/j.jda.2013.07.004},
  doi       = {10.1016/j.jda.2013.07.004},
  timestamp = {Wed, 02 Apr 2014 13:53:55 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/jda/Navarro14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}





@article{DBLP:journals/corr/abs-2101-00027,
  author    = {Leo Gao and
               Stella Biderman and
               Sid Black and
               Laurence Golding and
               Travis Hoppe and
               Charles Foster and
               Jason Phang and
               Horace He and
               Anish Thite and
               Noa Nabeshima and
               Shawn Presser and
               Connor Leahy},
  title     = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  journal   = {CoRR},
  volume    = {abs/2101.00027},
  year      = {2021},
  url       = {https://arxiv.org/abs/2101.00027},
  eprinttype = {arXiv},
  eprint    = {2101.00027},
  timestamp = {Thu, 14 Oct 2021 09:16:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2101-00027.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{fm00focs,
  author    = {Paolo Ferragina and
               Giovanni Manzini},
  title     = {Opportunistic Data Structures with Applications},
  booktitle = focs,
  year      = {2000},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  pdf       = {ferragina_manzini-opportunistic_data_structures_with_applications.pdf}	
}

@InProceedings{Zhu_2015_ICCV,
    title = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
    author = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
    booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
    month = {December},
    year = {2015}
}

@inproceedings{Gog:2014,
  author    = {Simon Gog and
               Timo Beller and
               Alistair Moffat and
               Matthias Petri},
  editor    = {Joachim Gudmundsson and
               Jyrki Katajainen},
  title     = {From Theory to Practice: Plug and Play with Succinct Data Structures},
  booktitle = {Experimental Algorithms - 13th International Symposium, {SEA} 2014,
               Copenhagen, Denmark, June 29 - July 1, 2014. Proceedings},
  series    = {Lecture Notes in Computer Science},
  volume    = {8504},
  pages     = {326--337},
  publisher = {Springer},
  year      = {2014},
  url       = {https://doi.org/10.1007/978-3-319-07959-2\_28},
  doi       = {10.1007/978-3-319-07959-2\_28},
  timestamp = {Tue, 14 May 2019 10:00:42 +0200},
  biburl    = {https://dblp.org/rec/conf/wea/GogBMP14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}


@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}
@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}
@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{liu2019roberta,
  title={Roberta: A Robustly Optimized Bert Pretraining Approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}


@article{Trieu:2018,
  author    = {Trieu H. Trinh and
               Quoc V. Le},
  title     = {A Simple Method for Commonsense Reasoning},
  journal   = {CoRR},
  volume    = {abs/1806.02847},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.02847},
  eprinttype = {arXiv},
  eprint    = {1806.02847},
  timestamp = {Mon, 13 Aug 2018 16:46:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-02847.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{Gokaslan:2019,
  title={{OpenWebText} corpus},
  author={Aaron Gokaslan and Vanya Cohen},
  url={http://web.archive.org/save/http://Skylion007.github.io/OpenWebTextCorpus},
  year={2019}
}



@inproceedings{DBLP:conf/iclr/RaePJHL20,
  author    = {Jack W. Rae and
               Anna Potapenko and
               Siddhant M. Jayakumar and
               Chloe Hillier and
               Timothy P. Lillicrap},
  title     = {Compressive Transformers for Long-Range Sequence Modelling},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=SylKikSYDH},
  timestamp = {Thu, 07 May 2020 17:11:48 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/RaePJHL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{DBLP:conf/iclr/SaxtonGHK19,
  author    = {David Saxton and
               Edward Grefenstette and
               Felix Hill and
               Pushmeet Kohli},
  title     = {Analysing Mathematical Reasoning Abilities of Neural Models},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=H1gR5iR5FX},
  timestamp = {Thu, 25 Jul 2019 14:25:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/SaxtonGHK19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/lrec/Tiedemann16,
  author    = {J{\"{o}}rg Tiedemann},
  editor    = {Nicoletta Calzolari and
               Khalid Choukri and
               Thierry Declerck and
               Sara Goggi and
               Marko Grobelnik and
               Bente Maegaard and
               Joseph Mariani and
               H{\'{e}}l{\`{e}}ne Mazo and
               Asunci{\'{o}}n Moreno and
               Jan Odijk and
               Stelios Piperidis},
  title     = {Finding Alternative Translations in a Large Corpus of Movie Subtitle},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources
               and Evaluation {LREC} 2016, Portoro{\v{z}}, Slovenia, May 23-28, 2016},
  publisher = {European Language Resources Association {(ELRA)}},
  year      = {2016},
  url       = {http://www.lrec-conf.org/proceedings/lrec2016/summaries/62.html},
  timestamp = {Mon, 19 Aug 2019 15:22:59 +0200},
  biburl    = {https://dblp.org/rec/conf/lrec/Tiedemann16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@misc{nagel2016cc,
  title={{CC-News}},
  author={Nagel, Sebastian},
  url={http://web.archive.org/save/http://commoncrawl.org/2016/10/news-dataset-available},
  year={2016}
}


@inproceedings{Weiner1973,
  author    = {Peter Weiner},
  title     = {Linear Pattern Matching Algorithms},
  booktitle = {14th Annual Symposium on Switching and Automata Theory, Iowa City,
               Iowa, USA, October 15-17, 1973},
  pages     = {1--11},
  publisher = {{IEEE} Computer Society},
  year      = {1973},
  url       = {https://doi.org/10.1109/SWAT.1973.13},
  doi       = {10.1109/SWAT.1973.13},
  timestamp = {Wed, 15 Dec 2021 10:47:19 +0100},
  biburl    = {https://dblp.org/rec/conf/focs/Weiner73.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{chen2022program,
  title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
  journal={arXiv preprint arXiv:2211.12588},
  year={2022}
}

@inproceedings{gehman-etal-2020-realtoxicityprompts,
    title = "{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models",
    author = "Gehman, Samuel  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Choi, Yejin  and
      Smith, Noah A.",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.301",
    doi = "10.18653/v1/2020.findings-emnlp.301",
    pages = "3356--3369",
    abstract = "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning {``}bad{''} words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",
}

@inproceedings{dziri-etal-2022-origin,
    title = "On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?",
    author = "Dziri, Nouha  and
      Milton, Sivan  and
      Yu, Mo  and
      Zaiane, Osmar  and
      Reddy, Siva",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.387",
    doi = "10.18653/v1/2022.naacl-main.387",
    pages = "5271--5285",
    abstract = "Knowledge-grounded conversational models are known to suffer from producing factually invalid statements, a phenomenon commonly called hallucination. In this work, we investigate the underlying causes of this phenomenon: is hallucination due to the training data, or to the models? We conduct a comprehensive human study on both existing knowledge-grounded conversational benchmarks and several state-of-the-art models. Our study reveals that the standard benchmarks consist of {\textgreater} 60{\%} hallucinated responses, leading to models that not only hallucinate but even amplify hallucinations. Our findings raise important questions on the quality of existing datasets and models trained using them. We make our annotations publicly available for future research.",
}



@article{kirk2021bias,
  title={Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models},
  author={Kirk, Hannah Rose and Jun, Yennie and Volpin, Filippo and Iqbal, Haider and Benussi, Elias and Dreyer, Frederic and Shtedritski, Aleksandar and Asano, Yuki},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={2611--2624},
  year={2021}
}

@article{carlini2022quantifying,
  title={Quantifying memorization across neural language models},
  author={Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  journal={arXiv preprint arXiv:2202.07646},
  year={2022}
}

@article{razeghi2022impact,
  title={Impact of pretraining term frequencies on few-shot reasoning},
  author={Razeghi, Yasaman and Logan IV, Robert L and Gardner, Matt and Singh, Sameer},
  journal={arXiv preprint arXiv:2202.07206},
  year={2022}
}


@article{lewis2020question,
  title={Question and answer test-train overlap in open-domain question answering datasets},
  author={Lewis, Patrick and Stenetorp, Pontus and Riedel, Sebastian},
  journal={arXiv preprint arXiv:2008.02637},
  year={2020}
}

@article{magar2022data,
  title={Data Contamination: From Memorization to Exploitation},
  author={Magar, Inbal and Schwartz, Roy},
  journal={arXiv preprint arXiv:2203.08242},
  year={2022}
}

@inproceedings{tanzer2022memorisation,
  title={Memorisation versus Generalisation in Pre-trained Language Models},
  author={T{\"a}nzer, Michael and Ruder, Sebastian and Rei, Marek},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={7564--7578},
  year={2022}
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}


@article{jang2022can,
  title={Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts},
  author={Jang, Joel and Ye, Seonghyeon and Seo, Minjoon},
  journal={arXiv preprint arXiv:2209.12711},
  year={2022}
}

@inproceedings{paik-etal-2021-world,
    title = "{T}he {W}orld of an {O}ctopus: {H}ow {R}eporting {B}ias {I}nfluences a {L}anguage {M}odel{'}s {P}erception of {C}olor",
    author = "Paik, Cory  and
      Aroca-Ouellette, St{\'e}phane  and
      Roncone, Alessandro  and
      Kann, Katharina",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.63",
    doi = "10.18653/v1/2021.emnlp-main.63",
    pages = "823--835",
    abstract = "Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that reporting bias, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can mitigate this issue. To accomplish this, we 1) generate the Color Dataset (CoDa), a dataset of human-perceived color distributions for 521 common objects; 2) use CoDa to analyze and compare the color distribution found in text, the distribution captured by language models, and a human{'}s perception of color; and 3) investigate the performance differences between text-only and multimodal models on CoDa. Our results show that the distribution of colors that a language model recovers correlates more strongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that reporting bias negatively impacts and inherently limits text-only training. We then demonstrate that multimodal models can leverage their visual training to mitigate these effects, providing a promising avenue for future research.",
}

@InProceedings{Hamborg2017,
  author     = {Hamborg, Felix and Meuschke, Norman and Breitinger, Corinna and Gipp, Bela},
  title      = {news-please: A Generic News Crawler and Extractor},
  year       = {2017},
  booktitle  = {Proceedings of the 15th International Symposium of Information Science},
  location   = {Berlin},
  doi        = {10.5281/zenodo.4120316},
  pages      = {218--223},
  month      = {March}
}

@inproceedings{roller-etal-2021-recipes,
    title = "Recipes for Building an Open-Domain Chatbot",
    author = "Roller, Stephen  and
      Dinan, Emily  and
      Goyal, Naman  and
      Ju, Da  and
      Williamson, Mary  and
      Liu, Yinhan  and
      Xu, Jing  and
      Ott, Myle  and
      Smith, Eric Michael  and
      Boureau, Y-Lan  and
      Weston, Jason",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.24",
    doi = "10.18653/v1/2021.eacl-main.24",
    pages = "300--325",
    abstract = "Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills: providing engaging talking points, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.",
}

@book{Rajaraman&Ullman,
author = {Rajaraman, Anand and Ullman, Jeffrey David},
title = {Mining of Massive Datasets},
year = {2011},
isbn = {1107015359},
publisher = {Cambridge University Press},
address = {USA}
}

@inproceedings{broder1997resemblance,
  title={On the resemblance and containment of documents},
  author={Broder, Andrei Z},
  booktitle={Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)},
  pages={21--29},
  year={1997},
  organization={IEEE}
}
@ARTICLE{908981,  author={Cohen, E. and Datar, M. and Fujiwara, S. and Gionis, A. and Indyk, P. and Motwani, R. and Ullman, J.D. and Yang, C.},  journal={IEEE Transactions on Knowledge and Data Engineering},   title={Finding interesting associations without support pruning},   year={2001},  volume={13},  number={1},  pages={64-78},  doi={10.1109/69.908981}}

@inproceedings{koehn-etal-2007-moses,
    title = "{M}oses: Open Source Toolkit for Statistical Machine Translation",
    author = "Koehn, Philipp  and
      Hoang, Hieu  and
      Birch, Alexandra  and
      Callison-Burch, Chris  and
      Federico, Marcello  and
      Bertoldi, Nicola  and
      Cowan, Brooke  and
      Shen, Wade  and
      Moran, Christine  and
      Zens, Richard  and
      Dyer, Chris  and
      Bojar, Ond{\v{r}}ej  and
      Constantin, Alexandra  and
      Herbst, Evan",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P07-2045",
    pages = "177--180",
}

@inproceedings{zellers-etal-2019-hellaswag,
    title = "{H}ella{S}wag: Can a Machine Really Finish Your Sentence?",
    author = "Zellers, Rowan  and
      Holtzman, Ari  and
      Bisk, Yonatan  and
      Farhadi, Ali  and
      Choi, Yejin",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1472",
    doi = "10.18653/v1/P19-1472",
    pages = "4791--4800",
}

@inproceedings{Bisk2020,
  author = {Yonatan Bisk and Rowan Zellers and
            Ronan Le Bras and Jianfeng Gao
            and Yejin Choi},
  title = {PIQA: Reasoning about Physical Commonsense in
           Natural Language},
  booktitle = {Thirty-Fourth AAAI Conference on
               Artificial Intelligence},
  year = {2020},
}

@inproceedings{mihaylov-etal-2018-suit,
    title = "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",
    author = "Mihaylov, Todor  and
      Clark, Peter  and
      Khot, Tushar  and
      Sabharwal, Ashish",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1260",
    doi = "10.18653/v1/D18-1260",
    pages = "2381--2391",
}