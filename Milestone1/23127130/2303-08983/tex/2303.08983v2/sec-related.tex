\section{Related work}
We build on top of the well-known Knowledge Distillation 
framework~\citep{bucilua2006model,ba2014deep,hinton2015distilling}, the 
effectiveness of which has been extensively 
studied~\citep{cho2019efficacy,stanton2021does}. Numerous variants of KD have 
been proposed, including feature distillation~\citep{ji2021show,zhang2020improve}, 
iterative distillation~\citep{mirzadeh2020improved,yang2019snapshot}, and
self-distillation~\citep{xie2020self,mobahi2020self,furlanello2018born,ji2021refine}.
Label smoothing, an effective regularizer and related to KD, is particularly 
related to our work when interpreted as
augmenting the output space~\citep{yuan2020revisiting,shen2021label}.

Closely related to our work, investigating and improving the accuracy on the \IN{} dataset has attracted 
much interest lately.
\citet{beyer2020we} eliminated erroneous labeled examples in the training with 
reference to a strong classifier. In \citet{shankar2020evaluating}, \IN{} 
dataset evaluation was revisited and alternative test sets were released.  
Relabel~\citet{yun2021re} proposed storing multiple labels on various regions 
of an image using a teacher. FKD~\citet{shen2021fast} further pushed this 
direction by caching the predictions of a strong teacher but with a limited 
augmentation.
Similarly, in \citet{ridnik2022solving}, the architecture-independent 
generalization of KD was exploited to propose a unified scheme for training with \IN{} 
seamlessly without any hyperparameter tuning or per-model training recipes.
\citet{liu2022meta} identified the temperature hyperparameter in KD as an 
important factor limiting benefits of stronger augmentations and teachers, and 
proposed an adaptive scheme to dynamically set the temperature during training. 
Distilling feature maps and probability distributions between the random pair 
of original images and their MixUp images was proposed to guide the network to 
learn cross-image knowledge \citet{pouransari2021extracurricular,yang2022mixskd}.
For self-supervised learning, \citet{kim2022generalized} adapted modern 
image-based regularizations with KD to improve the contrastive loss with some 
supervision. Our work has also been inspired by~\citep{beyer2022knowledge} 
where they proposed imitating the teacher on severe augmentations and train for 
thousands of epochs.
With our proposed DR strategy, we significantly reduce the 
cost of function matching by storing a few samples and reusing them for longer 
training.



%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%

%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%

%
%

%
%
%
%
%
%

%
%
%
%
%
