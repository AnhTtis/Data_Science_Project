\section{Conclusion}\label{sec:conclusion}
%
%

%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%

We go beyond the conventional online knowledge distillation and introduce 
Dataset Reinforcement (DR) as a general offline strategy.
%
%
%
%
Our investigation unwraps tradeoffs in finding generalizable reinforcements 
controlled by the difficulty of augmentations and we propose ways to balance.


We study the choice of the teacher (more than {100} SOTA large models and 
ensembles), augmentation (4 more than prior work), and their impact on 
a diverse collection of models (25 architectures), especially for long training 
(up to 1000 epochs). We demonstrate significant improvements (up to 20\%) in 
robustness, calibration and transfer (in/out of distribution classification, 
segmentation, and detection).  Our novel method of training and fine-tuning on 
doubly reinforced datasets (e.g., \INp{} to \CIFARp{}) demonstrates new 
possibilities of DR as a generic strategy.  We also study ideas that were not 
used in \INp{}, including curriculums, mixing augmentations and more in the 
appendix.

The proposed DR strategy is only an example of the large category of ideas 
possible within the scope of dataset reinforcement.  Our desiderata would also 
be satisfied by methods that expand the training data, especially in limited 
data domains, using strong generative foundation models.



%
%
%

%
%
%
%

\vspace{-4mm}
\paragraph{Limitations}
Limitations of the teacher can potentially transfer through dataset 
reinforcement.
For example, over-confident biased teachers should not be used and diverse 
ensembles are preferred.
Human verification of the reinforcements is also a solution.
Note that original labels are unmodified in reinforced datasets and can be used
in curriculums.
Our robustness and transfer learning evaluations consistently show better 
transfer and generalization for \INp{} models likely because of lower bias 
of the teacher ensemble trained on diverse data.

%
%

%
%
%
%
%


