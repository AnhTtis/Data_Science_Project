
%
%
%
%
%


%


%

%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%

%
%
%

%
%

We propose \emph{Dataset Reinforcement}, a strategy to improve a dataset once 
such that the accuracy of any model architecture trained on the reinforced 
dataset is improved at no additional training cost for users.
We propose a Dataset Reinforcement strategy based on data augmentation and 
knowledge distillation.
Our generic strategy is designed based on extensive analysis across CNN- and 
transformer-based models and performing large-scale study of distillation with 
state-of-the-art models with various data augmentations.
We create a reinforced version of the \IN{} training dataset, called 
\INp{}, as well as reinforced datasets \CIFARp{}, \Flowersp{}, and 
\Foodp{}.
Models trained with \INp{} are more accurate, robust, and calibrated, and 
transfer well to downstream tasks (e.g., segmentation and detection).
As an example, the accuracy of ResNet-50 improves by 1.7\% on the \IN{} 
validation set, 3.5\% on \IN{}V2, and 10.0\% on \IN{}-R.
Expected Calibration Error (ECE) on the \IN{} validation set is also reduced by 9.9\%.
Using this backbone with Mask-RCNN for 
object detection on MS-COCO, the mean average precision improves by 0.8\%. We 
reach similar gains for MobileNets, ViTs, and Swin-Transformers. For 
MobileNetV3 and Swin-Tiny, we observe significant improvements on \IN{}-R/A/C 
of \textbf{up to 20\% improved robustness}.
Models pretrained on \INp{} and fine-tuned on \CIFARp{}, \Flowersp{}, and 
\Foodp{}, reach up to 3.4\% improved accuracy.
The code, datasets, and pretrained models are available at
\href{https://github.com/apple/ml-dr}{https://github.com/apple/ml-dr}.
%
%

%
%
%
%
%
%
%
%
%
%
