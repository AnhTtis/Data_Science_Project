\clearpage

\section{Full results of training on \IN{} and \INp{}, compared with 
Knowledge Distillation}\label{sec:imagenet_plus_full}

\Cref{tab:imagenet_plus_full} provides the full results of training with 
\INp{} compared with \IN{} and Knowledge Distillation (KD). We choose 
\RRCRARE{} that provides a balanced trade-off across architectures and training 
durations, and call it \INp{}.
Results in \cref{tab:imagenet_plus_full} are without some state-of-the-art 
training features that are further improved in \cref{tab:imagenet_plus_cvnets}.  
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\Cref{tab:imagenet_plus_cvnets} provides improved results using 
state-of-the-art training recipes from the CVNets 
library~\citep{mehta2022cvnets}. We use the exact same \INp{} variant and 
only write a new dataset class in CVNets, further confirming our minimal code 
change claim. We note the training changes that help each model:
\begin{itemize}[leftmargin=*]
        \vspace{-2mm}
    \itemsep0em
    \item Higher resolution training: EfficientNets, ViT-Base, Swin-Base. We 
        observe that \INp{} reinforcements are resolution independent and 
        provide improvements even if the resolution is different from the one 
        used to generate them.
    \item Variable resolution with variable batch size training (VBS): ViTs, 
        EfficientNets, Swin.
    \item Mixed-precision: ViTs, Swin.
    \item Multi-node training: EfficientNets (resolution larger than 224).
    \item Exponential Model Averaging (EMA): MobileViTs.
    \item New results for MobileViT.
\end{itemize}

\begin{table*}[thb!]
    \centering
    \begin{subtable}[b]{.79\columnwidth}
        \centering
        \resizebox{0.63\columnwidth}{!}{
            \input{tables/tables_plus/imagenet_table6_E150_vs_timm_accuracy.tex}
        }
        \caption{150 epochs}
        \label{tab:imagenet_plus_table6_e150}
    \end{subtable}
    \vfill
    \begin{subtable}[b]{.79\columnwidth}
        \centering
        \resizebox{0.63\columnwidth}{!}{
            \input{tables/tables_plus/imagenet_table6_E300_vs_timm_accuracy.tex}
        }
        \caption{300 epochs}
        \label{tab:imagenet_plus_table6_e300}
    \end{subtable}
    \vfill
    \begin{subtable}[b]{.79\columnwidth}
        \centering
        \resizebox{0.6\columnwidth}{!}{
            \input{tables/tables_plus/imagenet_table_E1000_vs_timm_accuracy.tex}
        }
        \caption{1000 epochs}
        \label{tab:imagenet_plus_table6_e1000}
    \end{subtable}
    \caption{Comparison of training different models using knowledge 
    distillation and different \IN{}/\INp{} datasets. Subscripts show the 
    improvement on top of the \IN{} accuracy. We highlight the best accuracy 
    on each row from our proposed datasets and any number that is within $0.2$ 
    of the best.
    Knowledge distillation results are not reported for E=1000 
    (\cref{tab:imagenet_plus_table6_e1000}) as it is computationally very 
    expensive.}
    \label{tab:imagenet_plus_full}
\end{table*}


%
%
%
%
%
%

%
%
%
%
%
%
%
%

%
%
%
%
%
%

\begin{table*}[thb!]
    \centering
    \begin{subtable}[b]{.77\columnwidth}
        \centering
        \resizebox{0.59\columnwidth}{!}{
            \input{tables/tables_cvnets/imagenet_table0_E150_vs_timm_accuracy.tex}
        }
        \caption{150 epochs}
        \label{tab:imagenet_plus_cvnets_e150}
    \end{subtable}
    \vfill
    \begin{subtable}[b]{.79\columnwidth}
        \centering
        \resizebox{0.59\columnwidth}{!}{
            \input{tables/tables_cvnets/imagenet_table0_E300_vs_timm_accuracy.tex}
        }
        \caption{300 epochs}
        \label{tab:imagenet_plus_cvnets_e300}
    \end{subtable}
    \vfill
    \begin{subtable}[b]{.79\columnwidth}
        \centering
        \resizebox{0.59\columnwidth}{!}{
            \input{tables/tables_cvnets/imagenet_table0_E1000_vs_timm_accuracy.tex}
        }
        \caption{1000 epochs}
        \label{tab:imagenet_plus_cvnets_e1000}
    \end{subtable}
    \caption{\textbf{Improved results with state-of-the-art training recepies 
    in CVNets library.} Subscripts show the improvement on top of the \IN{} 
    accuracy.  We highlight the best accuracy on each row from our proposed 
    datasets and any number that is within $0.2$ of the best.
    }
    \label{tab:imagenet_plus_cvnets}
\end{table*}



\subsection{Aggregated improvements of \INp{} across models}
To better demonstrate the scale of accuracy improvements, we plot the results 
of training on \INp{} (\RRCRARE{}) from \cref{tab:imagenet_plus_full} in 
\cref{fig:figs_delta_ra_re_300} (300 epochs).
\RRCRARE{} balances the tradeoff between various architectures.
Given prior knowledge of architecture characteristics or enough training 
resources, we can select the dataset optimal for any architecture.
\Cref{fig:figs_delta_max} shows the best accuracy achieved for each model when 
we train on all 4 of our reinforced datasets for 300 epochs (maximum of the 
four numbers).  We observe that alternative reinforced datasets can provide 
1-2\% additional improvement, especially for light-weight CNNs and 
Transformers. In practice, given the knowledge of the complexity of the model 
architecture, one can decide to use alternative datasets (\RRC{} for 
light-weight and \RRCMsRs for heavy-weight models or Transformers). Otherwise, 
given additional compute resources, one can train on all 4 datasets and choose 
the best model according to the validation accuracy.

\begin{figure*}[thb!]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/figs_delta/RRC+RA_RE.pdf}
        \caption{\INp{} (RRC+RA/RE)}
        \label{fig:figs_delta_ra_re_300}
        \vspace{-6mm}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/figs_delta/max.pdf}
        \caption{Maximum of 4 \INp{} variants}
        \label{fig:figs_delta_max}
    \end{subfigure}
    \vspace{-4mm}
    \centering
    \caption{\textbf{\INp{} training improves validation accuracy} compared 
        with \IN{} training (Epochs=$300$).
    To train models using the \INp{} dataset, we use the same publicly-available \IN{} training recipes with \emph{no hyperparameter tuning} on \INp{}.
    We use the same hyperparameters 
    tuned for \IN{} with \emph{no hyperparameter tuning} on \INp{}.  
    \INp{} provides a balanced tradeoff with more improvements for 
    Heavy-weight CNNs and Transformers.
    \Cref{fig:figs_delta_max}:
    further improvements are achieved using the best out of our 4 proposed 
    datasets (See \cref{tab:imagenet_plus_table6_e300} for details).}
\end{figure*}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\newpage
\clearpage
\section{Expanded study on what is a good teacher?}
In this section we provide additional results and studies such as super 
ensembles as teachers.

\subsection{Additional results of knowledge distillation with pretrained Timm 
models}\label{sec:timm_distill_appendix}
\Cref{fig:timm_distill_e150} (E=150) complements \cref{fig:timm_distill_e300} 
(E=300) demonstrating the validation accuracy using knowledge distillation for 
a variety of teachers from the Timm library~\citep{rw2019timm}.
\Cref{tab:timm_distill_all} shows the results in detail. For both 150 and 300 
epoch training durations, we observe that ensembles of the state-of-the-art 
models in the Timm library perform best as the teachers across different 
student architectures. We choose the IG-ResNext ensemble for dataset 
reinforcement.

\begin{figure*}[thb!]
\begin{center}
    \begin{subfigure}[t]{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/figs_distill/imagenet_MobileNet-v3_E150_vs_timm_accuracy.pdf}
        \caption{MobileNetV3}\label{fig:imagenet_MobileNetv3_distill_e150}
    \end{subfigure}
    \begin{subfigure}[t]{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/figs_distill/imagenet_ResNet-50_E150_vs_timm_accuracy.pdf}
        \caption{ResNet50}\label{fig:imagenet_R50_distill_e150}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/figs_distill/imagenet_ViT-Small_E150_vs_timm_accuracy.pdf}
        \caption{ViT-Small}\label{fig:imagenet_ViTSmall_distill_e150}
    \end{subfigure}
    \hfill
    \hfill
    \begin{minipage}[t]{0.17\linewidth}
        \vspace{-3.5cm}
        \begin{subfigure}[t]{\textwidth}
            \includegraphics[width=\textwidth]{figures/figs_distill/legend.pdf}
        \end{subfigure}
    \end{minipage}
\end{center}
    \vspace{-0.5cm}
    \caption{Knowledge distillation accuracy of representative student 
    architectures (ResNet-50, ViT-Small, MobileNetV3) for pretrained teachers 
    from Timm library. We train for 150 epochs and \cref{fig:timm_distill_e300} 
    shows results for 300 epoch training.}\label{fig:timm_distill_e150}
\end{figure*}

\begin{table*}[htb!]
\centering
\resizebox{0.7\textwidth}{!}{
\input{tables/tables_distill/imagenet_all_vs_timm_accuracy.tex}
    }
    \caption{Effect of distillation from pretrained teachers (Timm library) on 
    the performance of MobileNetV3-large, ResNet-50, ViT-Small trained for 
    $150$ and $300$ epochs. This table includes the details of 
    \cref{fig:timm_distill_e150,fig:timm_distill_e300}.}
\label{tab:timm_distill_all}
\end{table*}


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\newpage
\clearpage
\subsection{Super ensembles on \IN{}}\label{sec:super_ensembles}

It is common to limit the number of models in an ensemble to less than 10 
members and typically only $4$. The reason is partly that larger ensembles are 
more expensive to evaluate at test time as well as training with knowledge 
distillation. Dataset reinforcement allows us to consider expensive teachers 
such as super ensembles with significantly more than 10 members. In 
\cref{tab:cifar100_super_ensembles_diverse_distill} we present results for 
super ensembles on \CIFAR{} and in \cref{fig:imagenet_superensemble} we 
present results on \IN{}. On \CIFAR{} we create super ensembles by training 
{128} models in parallel for ResNet-18, ResNet-50, and ResNet-152 
architectures.  To increase diversity, we train models with 16 choices of 
enable/disable {4} augmentations (CutMix, MixUp, RandAugment, and Label 
Smoothing) and train with $8$ different random seeds for each choice. In total 
we train $8\times 16=128$ models.  
\cref{tab:cifar100_super_ensembles_diverse_acc} shows the accuracy of the super 
ensembles while \cref{tab:cifar100_super_ensembles_diverse_distill} shows the 
accuracy of distillation with the super ensembles. We observe that the best 
student accuracy is achieved with the largest ensemble 128xR152.
Interestingly, super ensembles of small models (128xR50) are better than 
standard ensembles of large models (10xR152).
With super ensembles we achieve strong accuracies for ResNet-50 at 86.30 and 
ResNet-152 at 87.03.

We also consider super ensembles for \IN{} using dataset reinforcement.  
Knowledge distillation with super ensembles of larger than 10 members on 
\IN{} becomes challenging and resource demanding.
\cref{fig:imagenet_superensemble} shows the validation accuracy of the ensemble 
and \cref{fig:imagenet_superensemble_distill} shows the accuracy of student 
training with the reinforced \IN{} dataset using the super ensemble.
We observe that the entropy and confidence of the teacher on the validation set 
are not more correlated with the distillation accuracy than the accuracy of the 
validation teacher.  In particular, large ensembles are more accurate but not 
necessarily better teachers. In summary, we observe that the optimal ensemble 
size for KD is around 4.
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%



%
%
%
%
%
%
\begin{table*}[thb!]
\centering
\parbox{0.6\linewidth}{
\resizebox{0.6\columnwidth}{!}{
\begin{tabular}{l cccccc}
    \toprule[1.5pt]
    %
    & \textbf{10xR18} & \textbf{128xR18} & \textbf{10xR50} & \textbf{128xR50} & \textbf{10x152} & \textbf{128xR152}
    \\ 
    \midrule[1.25pt]
    ResNet-18         & 83.57 & 84.24 & 83.43 & 84.07 & 83.65 & {\bf 84.25}\\
    ResNet-50         & 84.40 & 85.16 & 84.33 & 86.38 & 86.03 & {\bf 86.30}\\
    ResNet-152        & 85.01 & 85.74 & 85.00 & 86.80 & 86.85 & {\bf 87.03}\\
    \bottomrule[1.5pt]
\end{tabular}
    }
\caption{Distillation on \CIFAR{} with super diverse ensembles.}
    \label{tab:cifar100_super_ensembles_diverse_distill}
}
\hfill
%
\parbox{0.35\linewidth}{
\resizebox{0.35\columnwidth}{!}{
\begin{tabular}{l cc}
    \toprule[1.5pt]
    %
    & \textbf{Single best} & \textbf{Ensemble (128x)}
    \\ \midrule[1.25pt]
    ResNet-18         & 81.57 & 85.88 \\
    ResNet-50         & 83.43 & 87.29 \\
    ResNet-152        & 84.44 & 87.92 \\
    \bottomrule[1.5pt]
\end{tabular}
    }
\caption{Accuracy of super diverse ensembles on \CIFAR{}.}
\label{tab:cifar100_super_ensembles_diverse_acc}
}
\end{table*}



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%




\begin{figure*}[thb!]
\begin{center}
    \includegraphics[width=\textwidth]{figures/figs_superensemble/imagenet_superensemble.png}
\end{center}
    \vspace{-0.5cm}
    \caption{\IN{} accuracy of super ensembles as the size of the ensemble 
    is increased. OR means the ensemble is created from diverse augmentation 
    choices while Best means only the random seed is different between models.
    %
    }\label{fig:imagenet_superensemble}
\end{figure*}

\begin{figure}[thb!]
\begin{center}
    %
    \includegraphics[width=0.3\linewidth]{figures/figs_superensemble/imagenet_superensemble_distill.png}
    %
\end{center}
    \vspace{-0.5cm}
    \caption{\IN{} super ensemble distillation accuracy for ResNet-50 
    facilitated by dataset reinforcement.}\label{fig:imagenet_superensemble_distill}
\end{figure}




%
%
%
%
%

\newpage
\clearpage
\section{Expanded study on reinforcing 
\IN{}}\label{sec:reinforce_imagenet_sup}
In this section, we provide ablations on the number and type of augmentations 
using a single relatively cheap teacher (ConvNext-Base-IN22FT1K) that still 
provides comparatively good improvements across all students.


\subsection{What is the best combination of augmentations for
reinforcement?}\label{sec:aug_difficulty_sup}

To recap, using our selected teachers from \cref{sec:good_teacher}, we 
investigate the choice of augmentations for dataset reinforcement.
Utilizing Fast Knowledge Distillation~\citep{shen2021fast}, we store the sparse 
outputs of a teacher on multiple augmentations.
For efficiency, we store top 10 probabilities predicted by the teacher, along 
with the augmentation parameters and reapply augmented images in the data 
loader of the student.
We observe that light-weight CNNs perform best on easier reinforcements while 
transformers perform best with difficult reinforcements. We balance this 
tradeoff using a mid-difficulty reinforcement.


We refer to the combination of baseline augmentations fixed resize, random crop 
and horizontal flip by CropFlip. In addition, we consider the following 
augmentations for dataset reinforcement: Random-Resize-Crop (\RRC{}), 
MixUp~\citep{zhang2017mixup} and CutMix~\citep{yun2019cutmix} (\Mixing{}), and 
RandomAugment~\citep{cubuk2020randaugment} and RandomErase (\RARE{}).  We also 
combine \Mixing{} with \RARE{} and refer to it as \MsRs.  We add all 
augmentations on top of \RRC{} and for clarity add + as shorthand for 
{\RRC{}$+$}.  Except for mixing augmentations, reapplying all augmentations has 
zero overhead compared to standard training with the same augmentations.  For 
mixing augmentations, our current implementation has approximately $30\%$ 
wall-clock time overhead because of the extra load time of mixing pairs stored 
with each reinforced sample. We discuss efficient alternatives in 
\cref{sec:library}.  Our balanced solution, \RRCRARE{}, does not use mixing and 
has zero overhead.

\begin{figure*}[t!]
\centering
    \begin{subfigure}[t]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/figs_augs/imagenet_MobileNet-v3_E150_accuracy.pdf}
        \caption{Light-weight CNN 
        (MobileNetV3)}\label{fig:imagenet_MobileNetv3_augs_e150}
    \end{subfigure}
     \hfill
    \begin{subfigure}[t]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/figs_augs/imagenet_ResNet-50_E150_accuracy.pdf}
        \caption{Heavy-weight CNN 
        (ResNet-50)}\label{fig:imagenet_R50_augs_e150}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/figs_augs/imagenet_ViT-Small_E150_accuracy.pdf}
        \caption{Transformer 
        (ViT-Small)}\label{fig:imagenet_ViTSmall_augs_e150}
    \end{subfigure}
    \hfill
    \begin{minipage}[t]{0.14\linewidth}
        \vspace{-3.5cm}
        \begin{subfigure}[t]{\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/figs_augs/legend.pdf}
        \end{subfigure}
    \end{minipage}
    \vspace{-3mm}
    \caption{\textbf{Light-weight CNNs prefer easy while Transformers prefer 
    difficult reinforcements and we balance the tradeoff.}
    \IN{} validation accuracy of three representative architectures trained 
    on reinforcements of \IN{}. We use ConvNext-Base-IN22FT1K as the teacher 
    and train for 150 epochs.  The x-axis is the number of augmentations stored 
    per original sample in the \IN{} training set.
    In favor of dataset reinforcement, we observe that training with $25-50$ 
    samples provides similar gains to training with more samples.
    The baseline augmentation is Fixed Resize-RandomCrop and horizontal flip 
    (CropFlip). In addition we consider the following augmentations for 
    reinforcement:
    Random-Resize-Crop and horizontal flip (\RRC{}), MixUp and CutMix 
    (\Mixing{}), RandomAugment/RandomErase (\RARE{}) and Mixing+RA/RE 
    (\MsRs{}).  We add these augmentations on top of \RRC{} and for clarity add 
    + as shorthand for {\RRC{}$+$}.
    }\label{fig:aug_difficulty}
    \vspace{-4mm}
\end{figure*}

\Cref{fig:aug_difficulty} shows the accuracy of various models trained on 
reinforced datasets. We observe that the light-weight CNN performs best with 
\RRC{} as the most simple augmentation after CropFlip while the transformer 
performs best with the most difficult set of reinforcements in \RRCMsRs{}. This 
observation matches the standard state-of-the-art recipes for training these 
models. At the same time, we observe that \RRCRARE{} provides nearly the best 
performance for all models without the extra overhead of mixing methods in our 
implementation.

Consistent across three models and reinforcements, we observe that even though 
we train for 150 epochs, at most $25-50$
different augmentations of each training sample
is enough to achieve the 
best accuracy for almost all methods. This gives at least $\times 3$ reduction 
in the number of samples we can take advantage of given a fixed training 
budget. Based on this observation and following \citet{beyer2022knowledge}, in 
\cref{sec:long_train} we train models for up to 1000 epochs while reinforce 
datasets with 400 augmentation samples.


%
%
%
%
%
%
%
%
%
%

\subsection{Augmentation: invariance vs imitation}\label{sec:invariance}
Data augmentation is crucial to train generalizable models in various domains. 
The key objective is to make the model invariant to content-preserving 
transformations. In knowledge distillation, however, it is not clear whether the student benefits more from being invariant to data augmentations as in \cref{eqn:invariance} or from imitating teacher's variations on augmented data as in \cref{eqn:imitation}. The training objective for each case is as follows:
\begin{equation}\label{eqn:invariance}
\text{(Invariance)}\hspace{5mm}
    \min_{\theta}\, \mathbb{E}_{\vx \sim \mathcal{D}, \hat{\vx} \sim \mathcal{A}(\vx)} \mathcal{L}(f_{\theta}(\hat{\vx}), g({\color{blue} \vx})) 
\end{equation}
\begin{equation}\label{eqn:imitation}
\text{(Imitation)}\hspace{5mm}
    \min_{\theta}\, \mathbb{E}_{\vx \sim \mathcal{D}, \hat{\vx} \sim \mathcal{A}(\vx)} \mathcal{L}(f_{\theta}(\hat{\vx}), g({\color{blue} \hat{\vx}}))
\end{equation}
where, $\mathcal{D}$ is the training dataset, $\mathcal{A}$ is augmentation function, $f_{\theta}$ is the student model parameterized with $\theta$, $g$ is the teacher model, and $\mathcal{L}$ is the loss function between student and 
teacher outputs.

In \cref{fig:invariance}, we compare the above training objectives for a wide 
range of augmentations in computer vision. For most augmentations, we observe 
imitation is more effective than invariance. This is consistent with 
observation in \citet{beyer2022knowledge}.  Therefore, in our setup we use 
augmentations only for imitation (and not invariance).
\begin{figure}[thb!]
    \centering
    \includegraphics[width=.5\linewidth]{figures/invariance.pdf}
    \vspace{-2mm}
    \caption{\IN{} top-1\% accuracy improvement when distilling knowledge 
    from a ConvNext (Base-IN22FT1K) teacher to a ViT-tiny student using 
    a single augmentation with training objectives in \cref{eqn:invariance} and 
    \cref{eqn:imitation}. No augmentation top-1\% accuracy is 
    $54\%$.\label{fig:invariance}}
    \vspace{-4mm}
\end{figure}



\subsection{Library size: Can we limit the mixing pairs?}\label{sec:library}
Mixing augmentations have the extra overhead of the load time for the 
corresponding pair in each mini-batch. Standard training does not have such an 
overhead because the mixing is performed on random pairs within a mini-batch.  
In dataset reinforcement, the pairs that have been matched in the reinforcement 
phase are limited to the number of samples stored and do not always appear in 
the same mini-batch during the student training time. This means, we have to 
load the matching pair for every sample in the mini-batch that doubles the data 
load time and becomes an overhead for CPU-bound models. This overhead in the 
smallest models we consider is at most 30\%. Even though much lower than the 
cost of knowledge distillation, it is still more than our desiderata would 
allow.

We consider an alternative where the pairing is done only with a library of 
selected samples from the training set. The library can be loaded in the memory 
once and reduce the additional cost incurred during the training.
\cref{fig:library} shows the performance as we vary the library size. Even 
a relatively large library does not cover the accuracy drop caused by the 
reduced randomness in the mixing. The reason is that to reduce the cost, we can 
only have one augmentation per sample in the library which reduces the 
randomness from the mixing substantially and negatively affects knowledge 
distillation.

\begin{figure*}[htb!]
\begin{center}
    \begin{subfigure}[t]{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/figs_library/imagenet_MobileNet-v3_E150_accuracy.pdf}
        \caption{MobileNetV3}\label{fig:imagenet_MobileNetv3_library_e150}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/figs_library/imagenet_ResNet-50_E150_accuracy.pdf}
        \caption{ResNet50}\label{fig:imagenet_R50_library_e150}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/figs_library/imagenet_ViT-Small_E150_accuracy.pdf}
        \caption{ViT-Small}\label{fig:imagenet_ViTSmall_library_e150}
    \end{subfigure}
    \hfill
    \begin{minipage}[t]{0.17\linewidth}
        \vspace{-4.2cm}
        \begin{subfigure}[t]{\textwidth}
            \includegraphics[width=\textwidth]{figures/figs_library/legend.pdf}
        \end{subfigure}
    \end{minipage}
\end{center}
    \vspace{-0.5cm}
    \caption{\textbf{Library of mixing pairs reduces wall-clock overhead for 
    mixing methods but negatively impacts accuracy.} We plot the validation 
    accuracy for models trained with \INp{} as we vary the library size.  
    The teacher is ConvNext-Base-IN22FT1K. See \cref{sec:library} for details.  
    (E=150)}\label{fig:library}
\end{figure*}

We also consider variations of mixing in \cref{fig:aug_difficulty_extra}. We 
consider two variations: Double-mix and Self-mix. In double-mix, for every 
augmented pair, we store two outputs with two sets of mixing coefficients. This 
means for every mini-batch we can load half the mini-batch along with a random 
pair for each sample, perform the stored augmentation on each and get two 
different mixed samples. As a result the overhead is zero.  Second alternative, 
self-mix, mixes every image only by itself. As such, there is no data load 
time, but there is still an extra overhead of preprocessing the input twice.
\cref{fig:aug_difficulty_extra} shows that neither of the considered 
alternatives provide a better tradeoff compared with \RRCRARE{}. Therefore, we use \RRCRARE{} in our paper and call it \INp{}.

\begin{figure*}[t!]
\begin{center}
    \begin{subfigure}[t]{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/figs_augs/imagenet_MobileNet-v3_E150_accuracy_extra.pdf}
        \caption{Light-weight CNN 
        (MobileNetV3)}\label{fig:imagenet_MobileNetv3_augs_e150_extra}
    \end{subfigure}
    \begin{subfigure}[t]{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/figs_augs/imagenet_ResNet-50_E150_accuracy_extra.pdf}
        \caption{Heavy-weight CNN 
        (ResNet-50)}\label{fig:imagenet_R50_augs_e150_extra}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/figs_augs/imagenet_ViT-Small_E150_accuracy_extra.pdf}
        \caption{Transformer 
        (ViT-Small)}\label{fig:imagenet_ViTSmall_augs_e150_extra}
    \end{subfigure}
    \hfill
    \begin{minipage}[t]{0.17\linewidth}
        \vspace{-3.5cm}
        \begin{subfigure}[t]{\textwidth}
            \includegraphics[width=\textwidth]{figures/figs_augs/legend_extra.pdf}
        \end{subfigure}
    \end{minipage}
\end{center}
    \vspace{-0.5cm}
    \caption{\textbf{Alternative Mixing Augmentations.}
    Accuracy of three representative architectures trained on reinforcements of 
    \IN{}. We use ConvNext-Base-IN22FT1K as the teacher and train for 150 
    epochs. The x-axis is the number of augmentations stored per original 
    sample in the \IN{} training set. Augmentations used are Fixed 
    Resize-RandomCrop and horizontal flip (CropFlip),
    Random-Resize-Crop and horizontal flip (\RRC{}), MixUp and CutMix (Mixing), 
    RandomAugment/RandomErase (RA/RE) and Mixing+RA/RE (M*+R*).  Alterantive 
    mixing augmentations: Double-mix (Dmix) and Self-mix 
    (Smix).}\label{fig:aug_difficulty_extra}
\end{figure*}

\subsection{What is the best curriculum of 
reinforcements?}\label{sec:curriculum}
\begin{figure*}[thb!]
    \centering
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/figs_curr/MobileNet-v3.pdf}
        \caption{Light-weight CNN 
        (MobileNetV3)}\label{fig:curriculum-RRC_Mixing}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/figs_curr/ResNet-50.pdf}
        \caption{Heavy-weight CNN (ResNet-50)}\label{fig:curriculum-RRC}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/figs_curr/ViT-Small.pdf}
        \caption{Transformer (ViT-Small)}\label{fig:curriculum-RRC_RA_RE}
    \end{subfigure}
    \vspace{-3mm}
    \caption{
        \textbf{Tradeoff in augmentation difficulty can be further reduced with 
        curriculums.}
        Using random samples of the optimal augmentation is the best (`All').
        If we have access to one reinforcement dataset or we want to only keep 
        a subset of data for efficiency, then it is better to use `easy' 
        curriculums for CNN based architectures and `hard' curriculums for 
        Transformers.  Full table in 
        \cref{sec:curriculum_extra}.}\label{fig:curriculum}
    \vspace{-4mm}
\end{figure*}



The one-time cost of reinforcing a dataset allows us to generate as much useful 
information as we need and store it for future use. An example is various 
metrics that can be used to devise learning curriculums that adapt to specific 
students. In this section we consider a set of initial curriculums we get for 
free with our dataset reinforcement strategy. Specifically, the output of the 
teacher on each sample also incorporates the confidence of the teacher on its 
prediction. We can use the confidence or the entropy of its predictions to make 
curriculums.

Given $\pp\in\R^{c}$ the set of predicted probabilities of the teacher for $c$ 
classes, we define confidence as $\max \pp_j$. For every sample, we order its 
augmentations by the confidence value from $0$ to \#samples. During training, 
at each iteration we only sample from a range of augmentations with indices 
between $[a, b]$, where $0\leq a,b < \text{\#samples}$. We devise curriculums 
by smoothly changing $a,b$ during the training using a cosine function between 
specified values of initial and final values for $a,b$.

\cref{fig:curriculum} shows the performance of various Easy, Hard, and All 
curriculums. Easy curriculums start from $[0, 10]$ (the $10\%$ easiest 
samples), hard samples start from $[90, 100]$ (the $10\%$ hardest samples), and 
All curriculums start from $[0, 100]$ (all the samples). We observe that the 
curriculum provides an alternative knob to control the difficulty of 
reinforcements that we can use adaptively during the training of the student.  
For example, the best performance of the light-weight CNN is with \RRC{} 
combined with the All curriculum, but similar performance can be achieved with 
\RRCRARE{} combined with an Easy curriculum. Similarly, the transformer achieves 
its best performance with \RRCMsRs{} combined with the All curriculum, while 
a similar performance can be achieved with \RRCMixing{} and a Hard curriculum.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

In \cref{sec:optimal_augs}, we study various objectives for choosing most 
useful samples during the reinforcement process. We consider storing on the 
most informative samples according to a number of metrics such as entropy, 
loss, and clustering. We make similar observations to the behaviour of 
curriculums that the objectives that increase hardness benefit the transformer 
while the easy objectives benefit the light-weight {CNN}.

\subsection{Additional details of curriculums}\label{sec:curriculum_extra}

We study reinforcements on curriculums shown in 
\cref{fig:curriculum_illustration}. \Cref{tab:effect_curr} provides the full 
results for the effect of dataset reinforcement curriculums.  We summarized 
these results in \cref{fig:curriculum} where we compared `*$\rightarrow$all' curriculums 
that end with `all' of the data. We observe that the beginning of the 
curriculum has much more impact on the generalization than the end of the 
curriculum. We observe that `all$\rightarrow$*' curriculums perform the best while 
`hard$\rightarrow$*' curriculums perform near optimal for ViT-Small and `easy$\rightarrow$*' performs
best for MobileNetV3-Large. At the same time, we observe clearly that the hard 
and easy curriculums result in significantly worse generalization when used to 
train the opposite architecture, i.e., `easy$\rightarrow$*' for ViT-Small and `hard$\rightarrow$*' 
for MobileNetV3-Large. This result clearly demonstrates the tradeoff in the 
architecture independent generalization controlled by the difficulty of 
reinforcements.


\begin{figure}[thb!]
\begin{center}
    %
    \includegraphics[width=0.5\linewidth]{figures/figs_curr/curriculum_illustration.pdf}
    %
\end{center}
    \vspace{-0.5cm}
    \caption{\textbf{Illustration of curriculums.} The x-axis shows the 
    percentage of training epochs while the y-axis shows the index of 
    augmentation samples in percentages as we order them from easy to hard by 
    the confidence of the teacher.  Highlighted regions show the subset of 
    indices of reinforcements to uniformly draw from at each epoch.  
    }\label{fig:curriculum_illustration}
    \vspace{-4mm}
\end{figure}

\begin{table*}[thb!]
    \centering
    \resizebox{\columnwidth}{!}{
        \input{tables/tables_curriculum/imagenet_curr_all_E150.tex}
    }
    \caption{\textbf{The effect of curriculum.} We observe that the beginning 
    of the curriculum has much more impact on the generalization than the end 
    of the curriculum (accuracy within the groups of three rows is similar).  
    Accuracies within 0.2\% of the best accuracy in each column are 
    highlighted.}
    \label{tab:effect_curr}
    \vspace{-4mm}
\end{table*}



\subsection{Optimal augmentation sample selection}\label{sec:optimal_augs}

We discussed that augmentations used to reinforce the dataset are sampled from 
a pool of augmentation operations and that we apply the augmentations with 
a predetermined application probability. The setup of dataset reinforcement 
allows us to optimize for the most informative augmentation samples. For 
example, we can generate a large set of candidate augmentations and choose 
a subset with maximum or minimum values of ad-hoc metrics. We considered 
selecting samples according to metrics such as confidence, entropy, and loss.  
Given $\pp\in\R^{c}$, the set of predicted probabilities of the teacher for $c$ 
classes, we define confidence as $\max \pp_j$, the entropy as $-\sum_j 
\pp_j\log\pp_j$, and the loss as $-\log\pp_{y}$ where $y$ is the ground-truth 
label. To encourage diversity, we also considered selecting samples based on 
the clustering of the predicted probability vectors by performing KMeans on 
$\pp$ vectors of the candidates and selecting one sample per cluster.  
\Cref{fig:optimal_augs} shows the performance of a subset of sample selection 
methods we considered. 

Generally we observe that max-entropy/min-confidence objectives demonstrate 
similar behaviors better than min-entropy/max-confidence. So we only show the 
min-confidence variant. We observe that overall random samples (blue lines) 
provides the best validation accuracy if used with the right augmentations 
(\RRCRARE{} for light-weight CNNs and \RRCMsRs{} for transformers).  Using 
min-confidence (orange) with \RRCMsRs{} (dashed orange), leads to similar 
generalization on transformers while hurting the generalization on CNNs. This 
matches our observations with the complexity of augmentations and curriculums 
that transformers prefer difficult samples. We observe that diversified samples 
using KMeans clustering (green) provide similar behavior to random samples 
(blue) while for transformers provide more consistent improvements at varying 
number of samples (dashed green compared with dashed blue). We identify this 
potential for future work and investigate reinforced datasets with random 
samples in the rest of the paper. Note that the curriculums are 
a generalization of the objective-based metrics that are adaptive to the 
student (See \cref{sec:curriculum} and \cref{sec:curriculum_extra}).

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



\begin{figure*}[b!]
\begin{center}
    \begin{subfigure}[t]{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/figs_optimal/imagenet_MobileNet-v3_E150_accuracy.pdf}
        \caption{MobileNetV3}\label{fig:imagenet_MobileNetv3_optimal_e150}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/figs_optimal/imagenet_ResNet-50_E150_accuracy.pdf}
        \caption{ResNet50}\label{fig:imagenet_R50_optimal_e150}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/figs_optimal/imagenet_ViT-Small_E150_accuracy.pdf}
        \caption{ViT-Small}\label{fig:imagenet_ViTSmall_optimal_e150}
    \end{subfigure}
    \hfill
    \begin{minipage}[t]{0.17\linewidth}
        \vspace{-2.9cm}
        \begin{subfigure}[t]{\textwidth}
            \includegraphics[width=\textwidth]{figures/figs_optimal/legend.pdf}
        \end{subfigure}
    \end{minipage}
\end{center}
    \vspace{-0.5cm}
    \caption{\textbf{Optimal augmentation sample selection.} \INp{} accuracy 
    for varying objectives and number of samples. The teacher is ConvNext 
    (Base-IN22FT1K)  (E=150)}\label{fig:optimal_augs}
\end{figure*}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%






%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\newpage
\clearpage
\section{Additional pretraining/finetuning/transfer learning 
results}\label{sec:transfer_full}
In \cref{tab:transfer_cls_full} we provide results for various combinations of 
pretraining and fine-tuning on reinforced/non-reinforced datasets. We observe 
that the best results are achieved when both pretraining and fine-tuning are 
done using reinforced datasets. We also observe that the improvement is 
significant compared to when only one of the pretraining/fine-tuning datasets 
is reinforced. The idea of training and fine-tuning on multiple reinforced 
datasets is unique to dataset reinforcement and would be challenging to 
replicate with standard data augmentations or knowledge distillation.

We train models for 100, 400, 1000 epochs on \CIFAR{}, \Food{} and 1000, 
4000, and 10000 epochs on \Flowers{} and report the best accuracy for each 
model. Models pretrained/fine-tuned on non-reinforced datasets tend to overfit 
at longer training while models trained on reinforced datasets benefit from 
longer training.

For future tasks and datasets,  additional task-specific information could be 
considered as reinforcements. For example, an object detection dataset can be 
further reinforced using the teacher's uncertainty on bounding boxes, occlusion 
estimate, and border uncertainty.  Multi-modal models such as CLIP are an 
immediate future work that can provide variety of additional training signal 
based on the relation to an anchor text.




\begin{table}[tbh!]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{llcc|cc|cc}
            \toprule[1.5pt]
            \multirow{2}{*}{\textbf{Model}} 
            & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Pretraining dataset}}} 
            & \multicolumn{6}{c}{\textbf{Fine-tuning dataset}}  \\
            \cmidrule[1.25pt]{3-8}
             &  & \CIFAR{} & \CIFARp{} & \Flowers{} & \Flowersp{} & \Food{} & \Foodp{}\\
             \midrule[1.25pt]
             \multirow{2}{*}{MobileNetV3-Large}
             & None & 80.2 & 83.6 & 68.8 & 87.5 & 85.1 & 88.2\\
             & \IN{} &  84.4 & 87.2 & 92.5 & 94.1 & 86.1 & 89.2 \\
             & \INp{} (Ours) &  86.0 & \textbf{87.5} & 93.7 & \textbf{95.3} & 86.6 &\textbf{89.5} \\
             \midrule
             \multirow{2}{*}{ResNet-50}
             & None & 83.8 & 85.0 & 87.3 & 85.0 & 89.1 & 90.2\\
             & \IN{} & 88.4 & 89.5 &  93.6 & 94.9 & 90.0 & 91.8\\
             & \INp{} (Ours) & 88.8 & \textbf{89.8} & 95.0 & \textbf{96.3} & 90.5 & \textbf{92.1}\\
             \midrule
             \multirow{2}{*}{SwinTransformer-Tiny}
             & None & 35.0 & 82.2 & 78.3 & 72.5 & 89.6 & 90.9\\
             & \IN{} & 90.6 & 90.7 & 96.3 & 96.5 &  92.3 & 92.7\\
             & \INp{} (Ours) & 90.9 & \textbf{91.2} & 96.6 & \textbf{97.0} & \textbf{93.0} & \textbf{92.9}\\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{\textbf{Pretraining/Finetuning/Transfer learning for fine-grained 
    object classification.}}
    \label{tab:transfer_cls_full}
    \vspace{-4mm}
\end{table}

\section{Full table of calibration results}\label{sec:val_calib_error_full}

In \cref{tab:val_calib_error_full} we provide the full results for 
\cref{fig:val_calib_error}. We see observe that validation ECE of \INp{} 
pretrained models is lower than \IN{} pretrained models.

\begin{table}[htb!]
    \centering
    \resizebox{1.0\textwidth}{!}{
        \begin{tabular}{lcccccccc}
            \toprule[1.5pt]
            \textbf{Model}
            & \textbf{Method}
            & \textbf{Epochs}
            & \textbf{Train ECE}
            & \textbf{Val ECE}
            & \textbf{ECE gap}
            & \textbf{Train Error}
            & \textbf{Val Error}
            & \textbf{Error gap}
            \\
             \midrule[1.25pt]
             \multirow{5}{*}{MobileNetV3-Large}
&\IN{}               &  300    &      0.1503 &    0.0727 &    0.0776 &        0.0934 &      0.2509 &      0.1575 \\
&\INp{}              &  300    &      0.0339 &    0.0309 &    0.0030 &        0.1400 &      0.2298 &      0.0898 \\
&\IN{}               &  1000   &      0.1489 &    0.0608 &    0.0881 &        0.0599 &      0.2491 &      0.1891 \\
&\INp{}              &  1000   &      0.0312 &    0.0323 &    0.0011 &        0.1218 &      0.2206 &      0.0988 \\
&KD                     &  300    &      0.0303 &    0.0297 &    0.0006 &        0.1550 &      0.2358 &      0.0808 \\
\midrule
             \multirow{5}{*}{ResNet-50}
&\IN{}               &  300    &      0.1938 &    0.1513 &    0.0425 &        0.1239 &      0.2122 &      0.0883 \\
&\INp{}              &  300    &      0.0263 &    0.0362 &    0.0098 &        0.1115 &      0.1944 &      0.0829 \\
&\IN{}               &  1000   &      0.1887 &    0.1348 &    0.0539 &        0.0906 &      0.2036 &      0.1130 \\
&\INp{}              &  1000   &      0.0241 &    0.0360 &    0.0119 &        0.0936 &      0.1830 &      0.0894 \\
&KD                     &  300    &      0.0250 &    0.0339 &    0.0089 &        0.1065 &      0.1846 &      0.0781 \\
\midrule
             \multirow{5}{*}{SwinTransformer-Tiny}
&\IN{}               &  300    &      0.1084 &    0.0663 &    0.0421 &        0.0734 &      0.1910 &      0.1176 \\
&\INp{}              &  300    &      0.0201 &    0.0381 &    0.0180 &        0.0818 &      0.1698 &      0.0880 \\
&\IN{}               &  1000   &      0.1042 &    0.0522 &    0.0519 &        0.0421 &      0.1905 &      0.1484 \\
&\INp{}              &  1000   &      0.0195 &    0.0397 &    0.0203 &        0.0743 &      0.1621 &      0.0877 \\
&KD                     &  300    &      0.0206 &    0.0379 &    0.0173 &        0.0958 &      0.1701 &      0.0742 \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{Full calibration error and validation error for \cref{fig:val_calib_error}.}
    \label{tab:val_calib_error_full}
    \vspace{-4mm}
\end{table}




\section{Cost of dataset reinforcement}
\label{sec:cost}
In \cref{sec:aug_difficulty_sup}, we observe that similar accuracy to knowledge 
distillation is reached with $\times 3$ fewer samples than the number of target 
epochs. This reduces the reinforcement cost.
\INp{} took 2080 mins to generate using 64xA100 GPUs which is highly 
parallelizable and similar to training ResNet-50 for 300 epochs on 8xA100 GPUs.  
The parallelization is another significant advantage to knowledge distillation 
because samples are reinforced independently while knowledge distillation 
requires following a trajectory on training samples.
For \CIFAR{}, \Flowers{}, and \Food{}, the reinforcement took 90, 40, and 
120 minutes respectively. With pretrained teachers and extrapolating our 
\INp{} observations, we can reinforce any new dataset and the cost is 
performing inference using the teacher on the dataset for
approximately $\times 3$ fewer samples than the maximum intended training epochs.  
This is a one-time cost that is amortized over many uses.

%
%
%
We provide storage cost analysis for \INp{} in 
\cref{tab:imagenet_plus_specs}. Note that for variants with mixing, the storage 
of \RRCRARE{} parameters doubles because each reinforcement consists of 
augmentations for a pair. The proposed \INp{} variant, \RRCRARE{}, does not 
have that doubling cost. Also note that the storage can be further reduced 
using compression methods. For example, \INp{} \RRCRARE{} with the 
compression from Python's Joblib with compression level 3 can be reduced to 
55GBs instead of 61GBs. Even more compression is possible by reducing the 
number of stored logits for the teacher and more aggressive compression 
methods.

The storage cost for \CIFARp{}, \Flowersp{}, and \Foodp{} uses the same set 
of formula given the number of samples that amounts to approximately 4.8, 1.0, 
and 7.3GBs in basic compressed form as in \cref{tab:imagenet_plus_specs}.
We have not explored reducing the size of these datasets significantly as it is 
not a significant overhead for small datasets. For larger datasets such as 
\IN{}, the reinforcement overhead is much smaller relative to the original 
dataset size because the bulk of the dataset is taken by the inputs while our 
reinforcements only store the outputs.


We provide the breakdown of training time on MobileNetV3-Large, ResNet-50, and 
SwinTransformer-Tiny in \cref{tab:train_time}.
Except for mixing augmentations, reapplying all augmentations has zero overhead 
compared to standard training with the same augmentations.  For mixing 
augmentations, our current implementation has approximately $30\%$ time 
overhead because of the extra load time of mixing pairs stored with each 
reinforced sample. This overhead only translate to extra wall-clock for very 
small models where the bottleneck is on the CPU rather than {GPU}.
We discuss efficient alternatives in \cref{sec:library}.  Our balanced 
solution, \RRCRARE{}, does not use mixing and has zero overhead.

%
%
%
%
%
%
%

\begin{table}[htb!]
    \centering
    \resizebox{0.45\textwidth}{!}{
        \begin{tabular}{llccc}
            \toprule[1.5pt]
            \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{3}{c}{\textbf{Training Epochs}}  \\
            \cmidrule[1.25pt]{3-5}
            & & \textbf{150} & \textbf{300} & \textbf{1000} \\
             %
             \midrule[1.25pt]
             \multirow{2}{*}{MobileNetV3-Large} & \IN{} & $1.00\times$ & $1.00\times$ & $1.00\times$ \\
             & \INp{} (Ours) & $1.13\times$ & $1.12\times$ & $1.12\times$ \\
             \midrule
             \multirow{2}{*}{ResNet-50} & \IN{} & $1.00\times$ & $1.00\times$ & $1.00\times$ \\
             & \INp{} (Ours) & $1.04 \times$ & $1.02 \times$ & $0.97\times$ \\
             \midrule
             \multirow{2}{*}{SwinTransformer-Tiny} & \IN{} & $1.00\times$ & $1.00\times$ & $1.00\times$ \\
              & \INp{} (Ours) & $0.99\times$ & $0.99\times$ & $0.99\times$ \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{\textbf{Training time for different models using \INp{} is 
    similar to \IN{} dataset}. Full results in 
    \cref{sec:imagenet_plus_full}.  }
    \label{tab:train_time}
    \vspace{-4mm}
\end{table}




\newpage
\clearpage
\section{Hyperparameters and implementation details}\label{sec:hparams}
We follow \cite{mehta2022cvnets, wightman2021resnet} and use  state-of-the-art 
recipes, including optimizers, hyperparameters, and learning. The details are 
provided in \cref{tab:train_hparams}. Because of resource limitations, we train 
EfficientNet-B3/B4 with KD using batch size $512$. Overall, we use the same 
hyperparameters on \IN{} and \INp{} with the exception of the data 
augmentations that are removed from the training on \INp{} based on our 
observations in \cref{sec:invariance}. For KD, we use the KL loss with 
temperature $1.0$ (no mixing with the cross-entropy loss) and shrink the weight 
decay by $10\times$.

In \cref{tab:train_hparams_cvnets} we provide hyperparameters for training with CVNets. For higher resolution and variable resolution training, we use the same metadata in \INp{} to create a random crop then resize it to the target resolution instead of the base resolution of 224.
In \cref{tab:det_seg_hparams} we provide hyperparameters for training Detection/Segmentation models.
In \cref{tab:transfer_hparams} we provide hyperparameters for transfer learning on \CIFAR{}/\Flowers{}/\Food{} datasets.

%
%
%
%
%
%


%
%
%
%
%
%

%
%
%
%
%
%
%


\begin{table}[h!]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{ll cccccc cccc}
            \toprule[1.5pt]
           \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\bfseries Training Method} & \multicolumn{6}{c}{\bfseries Optimization Hyperparams} & \multicolumn{4}{c}{\bfseries Data augmentation methods} \\ 
           \cmidrule[1.25pt]{3-8}
           \cmidrule[1.25pt]{9-12}
            & & \textbf{Optimizer} & \textbf{Batch Size} & \textbf{LR} & \textbf{Warmup} & \textbf{Weight Decay} & \textbf{Label Smoothing}
            & \textbf{RandAugment} & \textbf{Random Erase} ($p$) & \textbf{MixUp} ($\alpha$) & \textbf{CutMix} ($\alpha$)\\
           \midrule[1.25pt]
            \multirow{3}{*}{\bfseries MobileNetV1}
            & \IN{}  & SGD+Mom=0.9 & 1024 & 0.8 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & \xmark & \xmark & \xmark & \xmark \\
            & \INp{} & SGD+Mom=0.9 & 1024 & 0.8 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & \xmark & \xmark & \xmark & \xmark \\
            & KD        & SGD+Mom=0.9 & 1024 & 0.8 & 3 & $4.0\mathrm{e}{-6}$ & \xmark & \xmark & \xmark & \xmark & \xmark \\
            \midrule
            \multirow{3}{*}{\bfseries MobileNetV2}
            & \IN{}  & SGD+Mom=0.9 & 1024 & 0.4 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & \xmark & \xmark & \xmark & \xmark \\
            & \INp{} & SGD+Mom=0.9 & 1024 & 0.4 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & \xmark & \xmark & \xmark & \xmark \\
            & KD        & SGD+Mom=0.9 & 1024 & 0.4 & 3 & $4.0\mathrm{e}{-6}$ & \xmark & \xmark & \xmark & \xmark & \xmark \\
            \midrule
            \multirow{3}{*}{\bfseries MobileNetV3}
            & \IN{}  & SGD+Mom=0.9 & 1024 & 0.4 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & \xmark & \xmark & \xmark & \xmark \\
            & \INp{} & SGD+Mom=0.9 & 1024 & 0.4 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & \xmark & \xmark & \xmark & \xmark \\
            & KD        & SGD+Mom=0.9 & 1024 & 0.4 & 3 & $4.0\mathrm{e}{-6}$ & \xmark & \xmark & \xmark & \xmark & \xmark \\
            \midrule
            \multirow{3}{*}{\bfseries ResNet}
            & \IN{}  & SGD+Mom=0.9 & 1024 & 0.4 & 5 & $1.0\mathrm{e}{-4}$ & \cmark & \cmark & \cmark (0.25) & \cmark (0.2) & \cmark (1.0) \\
            & \INp{} & SGD+Mom=0.9 & 1024 & 0.4 & 5 & $1.0\mathrm{e}{-4}$ & \cmark & \xmark & \xmark & \xmark & \xmark \\
            & KD        & SGD+Mom=0.9 & 1024 & 0.4 & 5 & $1.0\mathrm{e}{-5}$ & \xmark & \cmark & \cmark (0.25) & \cmark (1.0) & \cmark (1.0) \\
            \midrule
            \multirow{3}{*}{\bfseries EfficientNet-B2}
            & \IN{}  & SGD+Mom=0.9 & 1024 & 0.4 & 5 & $4.0\mathrm{e}{-5}$ & \cmark & \cmark & \cmark (0.25) & \cmark (0.2) & \cmark (1.0) \\
            & \INp{} & SGD+Mom=0.9 & 1024 & 0.4 & 5 & $4.0\mathrm{e}{-5}$ & \cmark & \xmark & \xmark & \xmark & \xmark \\
            & KD        & SGD+Mom=0.9 & 1024 & 0.4 & 5 & $4.0\mathrm{e}{-6}$ & \xmark & \cmark & \cmark (0.25) & \cmark (1.0) & \cmark (1.0) \\
            \midrule
            \multirow{3}{*}{\bfseries EfficientNet-B3}
            & \IN{}  & SGD+Mom=0.9 & 1024 & 0.4 & 5 & $4.0\mathrm{e}{-5}$ & \cmark & \cmark & \cmark (0.25) & \cmark (0.2) & \cmark (1.0) \\
            & \INp{} & SGD+Mom=0.9 & 1024 & 0.4 & 5 & $4.0\mathrm{e}{-5}$ & \cmark & \xmark & \xmark & \xmark & \xmark \\
            & KD        & SGD+Mom=0.9 & 512 & 0.2 & 5 & $4.0\mathrm{e}{-6}$ & \xmark & \cmark & \cmark (0.25) & \cmark (1.0) & \cmark (1.0) \\
            \midrule
            \multirow{3}{*}{\bfseries EfficientNet-B4}
            & \IN{}  & SGD+Mom=0.9 & 1024 & 0.4 & 5 & $4.0\mathrm{e}{-5}$ & \cmark & \cmark & \cmark (0.25) & \cmark (0.2) & \cmark (1.0) \\
            & \INp{} & SGD+Mom=0.9 & 1024 & 0.4 & 5 & $4.0\mathrm{e}{-5}$ & \cmark & \xmark & \xmark & \xmark & \xmark \\
            & KD        & SGD+Mom=0.9 & 512 & 0.4 & 5 & $4.0\mathrm{e}{-6}$ & \xmark & \cmark & \cmark (0.25) & \cmark (1.0) & \cmark (1.0) \\
            \midrule
            \multirow{3}{*}{\bfseries ViT}
            & \IN{}  & AdamW (0.9, 0.999) & 1024 & 0.001 & 5 & 0.05 & \cmark & \cmark & \cmark (0.25) & \cmark (0.2) & \cmark (1.0) \\
            & \INp{} & AdamW (0.9, 0.999) & 1024 & 0.001 & 5 & 0.05 & \cmark & \xmark & \xmark & \xmark & \xmark \\
            & KD        & AdamW (0.9, 0.999) & 1024 & 0.001 & 5 & 0.005 & \xmark & \cmark & \cmark (0.25) & \cmark (1.0) & \cmark (1.0) \\
            \midrule
            \multirow{3}{*}{\bfseries SwinTransformer}
            & \IN{}  & AdamW (0.9, 0.999) & 1024 & 0.001 & 5 & 0.05 & \cmark & \cmark & \cmark (0.25) & \cmark (0.2) & \cmark (1.0) \\
            & \INp{} & AdamW (0.9, 0.999) & 1024 & 0.001 & 5 & 0.05 & \cmark & \xmark & \xmark & \xmark & \xmark \\
            & KD        & AdamW (0.9, 0.999) & 1024 & 0.001 & 5 & 0.005 & \xmark & \cmark & \cmark (0.25) & \cmark (1.0) & \cmark (1.0) \\
           \bottomrule[1.5pt]
        \end{tabular}
    }
    \caption{\textbf{Hyperparameters used for training different models.} We use cosine learning rate schedule to zero.}
    \label{tab:train_hparams}
\end{table}



\begin{table}[h!]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{ll cccccccc c}
            \toprule[1.5pt]
           \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\bfseries Training Method} & \multicolumn{8}{c}{\bfseries Optimization Hyperparams} & \multirow{2}{*}{\bfseries Data augmentation methods} \\ 
           \cmidrule[1.25pt]{3-10}
            & & \textbf{Optimizer} & \textbf{Batch Size} & \textbf{LR} & \textbf{Warmup} & \textbf{Weight Decay} & \textbf{Mixed Precision} & \textbf{Resolution} & \textbf{Grad. Clip} & \\
           \midrule[1.25pt]
            \multirow{2}{*}{\bfseries MobileNetV1}
            & \IN{}  & SGD+Mom=0.9 & 1024 & 0.8 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & 224 & \xmark & LS+RRC+HF\\
            & \INp{} & SGD+Mom=0.9 & 1024 & 0.8 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & 224 & \xmark & \xmark\\
            \midrule
            \multirow{2}{*}{\bfseries MobileNetV2}
            & \IN{}  & SGD+Mom=0.9 & 1024 & 0.4 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & 224 & \xmark & LS+RRC+HF\\
            & \INp{} & SGD+Mom=0.9 & 1024 & 0.4 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & 224 & \xmark & \xmark\\
            \midrule
            \multirow{2}{*}{\bfseries MobileNetV3}
            & \IN{}  & SGD+Mom=0.9 & 2048 & 0.4 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & 224 & \xmark & LS+RRC+HF\\
            & \INp{} & SGD+Mom=0.9 & 2048 & 0.4 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & 224 & \xmark & \xmark\\
            \midrule
            \multirow{2}{*}{\bfseries MobileNetViT}
            & \IN{}  & AdamW (0.9, 0.999) & 1024 & 0.002 & 20 & $0.01$ & \cmark & VBS(160, 320, 256) & \xmark & LS+RRC+HF\\
            & \INp{} & AdamW (0.9, 0.999) & 1024 & 0.002 & 20 & $0.01$ & \cmark & VBS(160, 320, 256) & \xmark & \xmark\\
            \midrule
            \multirow{2}{*}{\bfseries ResNet}
            & \IN{}  & SGD+Mom=0.9 & 1024 & 0.4 & 5 & $1.0\mathrm{e}{-4}$ & \cmark & 224 & \xmark & LS+RRC+HF+RA+RE+MU+CM\\
            & \INp{} & SGD+Mom=0.9 & 1024 & 0.4 & 5 & $1.0\mathrm{e}{-4}$ & \cmark & 224 & \xmark & \xmark\\
            \midrule
            \multirow{2}{*}{\bfseries EfficientNet-B2}
            & \IN{}  & SGD+Mom=0.9 & 2048 & 0.8 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & VBS(144, 432, 288) & \xmark & LS+RRC+HF+RA+RE+MU+CM\\
            & \INp{} & SGD+Mom=0.9 & 2048 & 0.8 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & VBS(144, 432, 288) & \xmark & \xmark\\
            \midrule
            \multirow{2}{*}{\bfseries EfficientNet-B3}
            & \IN{}  & SGD+Mom=0.9 & 2048 & 0.8 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & VBS(150, 450, 300) & \xmark & LS+RRC+HF+RA+RE+MU+CM\\
            & \INp{} & SGD+Mom=0.9 & 2048 & 0.8 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & VBS(150, 450, 300) & \xmark & \xmark\\
            \midrule
            \multirow{2}{*}{\bfseries EfficientNet-B4}
            & \IN{}  & SGD+Mom=0.9 & 2048 & 0.8 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & VBS(190, 570, 380) & \xmark & LS+RRC+HF+RA+RE+MU+CM\\
            & \INp{} & SGD+Mom=0.9 & 2048 & 0.8 & 3 & $4.0\mathrm{e}{-5}$ & \cmark & VBS(190, 570, 380) & \xmark & \xmark\\
            \midrule
            \multirow{2}{*}{\bfseries ViT-Tiny}
            & \IN{}  & AdamW (0.9, 0.999) & 2048 & 0.002 & 10 & 0.05 & \cmark & 224 & \cmark (1.0) & LS+RRC+HF+RA+RE+MU+CM\\
            & \INp{} & AdamW (0.9, 0.999) & 2048 & 0.002 & 10 & 0.05 & \cmark & 224 & \cmark (1.0) & \xmark\\
            \midrule
            \multirow{2}{*}{\bfseries ViT-Small/Base}
            & \IN{}  & AdamW (0.9, 0.999) & 2048 & 0.002 & 10 & 0.2 & \cmark & 224 & \cmark (1.0) & LS+RRC+HF+RA+RE+MU+CM\\
            & \INp{} & AdamW (0.9, 0.999) & 2048 & 0.002 & 10 & 0.2 & \cmark & 224 & \cmark (1.0) & \xmark\\
            \midrule
            \multirow{2}{*}{\bfseries ViT-Base $\uparrow$384}
            & \IN{}  & AdamW (0.9, 0.999) & 2048 & 0.002 & 20 & 0.2 & \cmark & VBS(192, 576, 384) & \cmark (1.0) & LS+RRC+HF+RA+RE+MU+CM\\
            & \INp{} & AdamW (0.9, 0.999) & 2048 & 0.002 & 20 & 0.2 & \cmark & VBS(192, 576, 384) & \cmark (1.0) & \xmark\\
            \midrule
            \multirow{2}{*}{\bfseries SwinTransformer}
            & \IN{}  & AdamW (0.9, 0.999) & 1024 & 0.001 & 20 & 0.05 & \cmark & 224 & \cmark (5.0) & LS+RRC+HF+RA+RE+MU+CM\\
            & \INp{} & AdamW (0.9, 0.999) & 1024 & 0.001 & 20 & 0.05 & \cmark & 224 & \cmark (5.0) & \xmark\\
            \midrule
            \multirow{2}{*}{\bfseries SwinTransformer-Base $\uparrow$384}
            & \IN{}  & AdamW (0.9, 0.999) & 1024 & 0.001 & 20 & 0.05 & \cmark & VBS(192, 576, 384) & \cmark (5.0) & LS+RRC+HF+RA+RE+MU+CM\\
            & \INp{} & AdamW (0.9, 0.999) & 1024 & 0.001 & 20 & 0.05 & \cmark & VBS(192, 576, 384) & \cmark (5.0) & \xmark\\
           \bottomrule[1.5pt]
        \end{tabular}
    }
    \caption{\textbf{Hyperparameters used for training different models in 
    CVNets.} LS: Label Smoothing with 0.1, RRC: Random-Resize-Crop, HF: Horizontal Flip, VBS(min-res, max-res, crop-size): Variable Batch Sampler with variable resolution. RA: RandAugment, RE: Random Erase with 0.25, MU: MixUp with alpha
0.2, CM: CutMix with alpha 0.1. We use cosine-learning rate schedule to 0.}
    \label{tab:train_hparams_cvnets}
\end{table}


\begin{table}[h!]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{ll cccccccccc c}
            \toprule[1.5pt]
           \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\bfseries Training Method} & \multicolumn{10}{c}{\bfseries Optimization Hyperparams} & \multirow{2}{*}{\bfseries Data augmentation methods} \\ 
           \cmidrule[1.25pt]{3-12}
            & & \textbf{Optimizer} & \textbf{Epochs} & \textbf{Batch Size} & \textbf{LR} & \textbf{BackBone LR Mul.} & \textbf{Warmup iter.} & \textbf{Weight Decay} & \textbf{Mixed Precision} & \textbf{Resolution} & \textbf{Grad. Clip} & \\
           \midrule[1.25pt]
            \multirow{2}{*}{\bfseries MobileNetV3-Large}
            & Detection    & SGD+Mom=0.9 & 36 & 64 & multi-step-lr(0.1, [24, 33])  & 0.1 & 500 & $4.0\mathrm{e}{-5}$ & \xmark & VBS(512, 1280, 1024) & \xmark & \xmark\\
            & Segmentation & SGD+Mom=0.9 & 50 & 16 & cosine-lr(0.02, 0.0001)       & 0.1 & 0   & $1.0\mathrm{e}{-4}$ & \xmark & 512 & \xmark & RC+RSSR+RR+PD+RG\\
            \midrule
            \multirow{2}{*}{\bfseries ResNet-50}
            & Detection    & SGD+Mom=0.9 & 100 & 64 & multi-step-lr(0.1, [60, 84]) & 0.1 & 500 & $4.0\mathrm{e}{-5}$ & \xmark & VBS(512, 1280, 1024) & \xmark & \xmark\\
            & Segmentation & SGD+Mom=0.9 & 50  & 16 & cosine-lr(0.02, 0.0001)      & 0.1 & 500 & $4.0\mathrm{e}{-5}$ & \xmark & 512 & \xmark & RC+RSSR+RR+PD+RG\\
            \midrule
            \multirow{2}{*}{\bfseries SwinTransformer-Tiny}
            & Detection    & SGD+Mom=0.9 & 100 & 64 & multi-step-lr($6.0\mathrm{e}{-4}$, [60, 84])        & 1.0 & 500 & 0.05 & \xmark & VBS(512, 1280, 1024) & \xmark & \xmark\\
            & Segmentation & SGD+Mom=0.9 & 50  & 16 & cosine-lr($6.0\mathrm{e}{-4}$, $1.0\mathrm{e}{-6}$) & 0.1 & 500 & 0.05 & \xmark & 512 & \xmark & RC+RSSR+RR+PD+RG\\
           \bottomrule[1.5pt]
        \end{tabular}
    }
    \caption{\textbf{Hyperparameters of detection/segmentation using CVNets.} RC: Random Crop, RSSR: Random Short-Size Resize, RR: Random Rotate by maximum 10 degrees angle. VBS(min-res, max-res, crop-size): Variable Batch Sampler with variable resolution. PD: Photometric Distortion, RG: Random Gaussian noise}
    \label{tab:det_seg_hparams}
\end{table}

\begin{table}[h!]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{ll ccccc c}
            \toprule[1.5pt]
           \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\bfseries Pretrained} & \multicolumn{5}{c}{\bfseries Optimization Hyperparams} & \multirow{2}{*}{\bfseries Data augmentation methods} \\ 
           \cmidrule[1.25pt]{3-7}
            & & \textbf{Optimizer} & \textbf{Batch Size} & \textbf{LR} & \textbf{Warmup} & \textbf{Weight Decay} & \\
           \midrule[1.25pt]
            \multirow{3}{*}{\bfseries MobileNetV3-Large}
            & \xmark & SGD+Mom=0.9 & 256 & 0.2   & 0 & $5.0\mathrm{e}{-4}$ & \xmark \\
            & \cmark & SGD+Mom=0.9 & 256 & 0.002 & 0 & $5.0\mathrm{e}{-4}$ & \xmark \\
            & \cmark+ & SGD+Mom=0.9 & 256 & 0.002 & 0 & $5.0\mathrm{e}{-4}$ & \xmark \\
            \midrule
            \multirow{3}{*}{\bfseries ResNet-50}
            & \xmark & SGD+Mom=0.9 & 256 & 0.2   & 0 & $5.0\mathrm{e}{-4}$ & RA+MU+CM\\
            & \cmark & SGD+Mom=0.9 & 256 & 0.002 & 0 & $5.0\mathrm{e}{-4}$ & RA+MU+CM\\
            & \cmark+ & SGD+Mom=0.9 & 256 & 0.002 & 0 & $5.0\mathrm{e}{-4}$ & \xmark\\
            \midrule
            \multirow{3}{*}{\bfseries SwinTransformer-Tiny}
            & \xmark & AdamW (0.9, 0.999) & 256 & 0.0001 & 5 & 0.05 & RA+MU+CM\\
            & \cmark & AdamW (0.9, 0.999) & 256 & 0.00001 & 5 & 0.05 & RA+MU+CM\\
            & \cmark+ & AdamW (0.9, 0.999) & 256 & 0.00001 & 5 & 0.05 & \xmark\\
           \bottomrule[1.5pt]
        \end{tabular}
    }
    \caption{\textbf{Hyperparameters used for \CIFAR{}/\Flowers{}/\Food{}.} We use cosine learning rate schedule to zero. We resize the inputs for all datasets to 224 including \CIFAR{} where we pad the input by 16. We also use label smoothing.}
    \label{tab:transfer_hparams}
\end{table}

\newpage
\clearpage

\section{CLIP, ViT, and Mixed Architecture Teachers}\label{sec:clip_vit_mix}

In this section, we evaluate the effectiveness of CLIP-pretrained models 
fine-tuned on \IN{} as teachers.  We evaluate various ensembles teachers 
mixed with non-CILP pretrained teachers and a variety of ViT-based models. We 
provide the model names in \cref{tab:imagenet_plus_clip_vit_mixed_names}.
\Cref{tab:imagenet_plus_clip_vit_mixed} shows the accuracy of various student 
models trained on reinforced datasets with our selection of ensembles. We 
observe 1) Ensembles are consistently better teachers 2) CLIP-pretrained 
teachers are at best on-par with the IG-ResNext ensemble 3) ViT-based teachers 
are not good teachers for CNN-based models, regardless of their training 
method.

\begin{table*}[htb!]
\centering
\resizebox{0.95\textwidth}{!}{
    \input{tables/tables_clip_vit_mixed/models}
}
\caption{CLIP, ViT, Mixed architecture teacher ensemble names.}
\label{tab:imagenet_plus_clip_vit_mixed_names}
\end{table*}

\begin{table*}[thb!]
    \centering
    \begin{subtable}[b]{.95\textwidth}
        \centering
        \resizebox{\columnwidth}{!}{
            \input{tables/tables_clip_vit_mixed/table0_E150_vs_timm_accuracy.tex}
        }
        \caption{150 epochs}
        \label{tab:imagenet_plus_clip_vit_mixed_e150}
    \end{subtable}
    \vfill
    \begin{subtable}[b]{.95\columnwidth}
        \centering
        \resizebox{\columnwidth}{!}{
            \input{tables/tables_clip_vit_mixed/table0_E300_vs_timm_accuracy.tex}
        }
        \caption{300 epochs}
        \label{tab:imagenet_plus_clip_vit_mixed_e300}
    \end{subtable}
    \vfill
    \begin{subtable}[b]{.95\columnwidth}
        \centering
        \resizebox{\columnwidth}{!}{
            \input{tables/tables_clip_vit_mixed/table0_E1000_vs_timm_accuracy.tex}
        }
        \caption{1000 epochs}
        \label{tab:imagenet_plus_clip_vit_mixed_e1000}
    \end{subtable}
    \caption{\textbf{CLIP, ViT, Mixed architecture teachers.} Subscripts show 
    the improvement on top of the \IN{} accuracy.  We highlight the best 
    accuracy on each row from our proposed datasets and any number that is 
    within $0.2$ of the best.
    }
    \label{tab:imagenet_plus_clip_vit_mixed}
\end{table*}



