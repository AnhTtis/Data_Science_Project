\section{Dataset Reinforcement}\label{sec:method}
%
%
%
%
%
%
%
%
%
%
%
%
%


Our proposed strategy for dataset reinforcement (DR) is efficiently combining 
knowledge distillation and data augmentation to generate an enhanced dataset.
We precompute and store the output of a strong pretrained model on multiple 
augmentations per sample as reinforcements. The stored outputs are more 
informative and useful for training compared with ground truth labels.  This 
approach is related to prior works, such as Fast Knowledge Distillation 
(FKD)~\citep{shen2021fast}) and ReLabel~\citep{yun2021re}, that aim to improve 
the labels.
Beyond these works, our goal is to find generalizable reinforcements that 
improve the accuracy of any architecture.
First we perform a comprehensive study to find a strong teacher 
(\cref{sec:good_teacher})  then find generalizable reinforcements on \IN{} 
(\cref{sec:aug_difficulty}). To demonstrate the generality of our strategy and 
findings, we further reinforce \CIFAR{}, \Flowers{}, and \Food{} 
(\cref{sec:other_datasets}).

The reinforced dataset consists of the original dataset plus the reinforcement 
meta data for all training samples.  During the reinforcement process, for each 
sample a fixed number of reinforcements is generated using parametrized 
augmentation operations and evaluating the teacher predictions. To save 
storage, instead of storing the augmented images, the augmentation parameters 
are stored alongside the sparsified output of the teacher. As a result, the 
extra storage needed is only a fraction of the original training set for large 
datasets.
Using our reinforced dataset has no computational overhead on training, 
requires no code change, and provides improvements for various architectures.


%
%
%
%


%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{What is a good teacher?}\label{sec:good_teacher}


\begin{figure*}[t]
\begin{center}
    \begin{subfigure}[t]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/figs_distill/imagenet_MobileNet-v3_E300_vs_timm_accuracy.pdf}
        \caption{Light-weight CNN 
        (MobileNetV3)}\label{fig:imagenet_MobileNetv3_distill_e300}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/figs_distill/imagenet_ResNet-50_E300_vs_timm_accuracy.pdf}
        \caption{Heavy-weight CNN 
        (ResNet-50)}\label{fig:imagenet_R50_distill_e300}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/figs_distill/imagenet_ViT-Small_E300_vs_timm_accuracy.pdf}
        \caption{Transformer (ViT-Small) 
        }\label{fig:imagenet_ViTSmall_distill_e300}
    \end{subfigure}
    \hfill
    \begin{minipage}[t]{0.14\linewidth}
        \centering
        \vspace*{-3.5cm}
        \begin{subfigure}[t]{\textwidth}
            \includegraphics[width=\textwidth]{figures/figs_distill/legend.pdf}
        \end{subfigure}
    \end{minipage}
\end{center}
    \vspace{-5mm}
    \caption{\textbf{Knowledge Distillation with models and ensembles from Timm 
    library.} We observe the validation accuracy of students
    saturates or drops as the accuracy of teachers within an architecture 
    family increases.
    We also observe that ensembles (marked by asterisks) are better teachers.
    Ensemble of IG-ResNext models performs best as teachers across student 
    architectures.
    ERM (Empirical Risk Minimization) is standard training without knowledge 
    distillation.
    Similar results for 150 epoch training in \cref{fig:timm_distill_e150}.
    }\label{fig:timm_distill_e300}
    \vspace{-4mm}
\end{figure*}


Knowledge distillation (KD) refers to training a student model using the 
outputs of a teacher 
model~\citep{bucilua2006model,ba2014deep,hinton2015distilling}.  The training 
objective is as follows:
\begin{equation}
    \min_{\theta}\, \mathbb{E}_{\vx \sim \mathcal{D}, \hat{\vx} \sim 
    \mathcal{A}(\vx)} \mathcal{L}(f_{\theta}(\hat{\vx}), g(\hat{\vx}))\,,
\end{equation}
where, $\mathcal{D}$ is the training dataset, $\mathcal{A}$ is augmentation 
function, $f_{\theta}$ is the student model parameterized with $\theta$, $g$ is 
the teacher model, and $\mathcal{L}$ is the loss function between student and 
teacher outputs. Throughout this paper, we use the KL loss without 
a temperature hyperparameter and no mixing with the cross-entropy loss.
We teach the student to imitate the output of the teacher on all augmentations 
consistent with \citep{beyer2022knowledge}.
%

It is common to use a fixed teacher because repeating experiments and selecting 
the best teacher is expensive~\citep{beyer2022knowledge,gou2021knowledge}.
The teacher is often selected based on the state-of-the-art test accuracy of 
available pretrained models.  However, it has been observed that most accurate 
models do not necessarily appear to be the best 
teachers~\citep{cho2019efficacy,mirzadeh2020improved}.  Ensemble models on the 
other hand, have been shown to be promising teachers from the early work of 
\citet{bucilua2006model} until recent works in various 
domains~\citep{chebotar2016distilling,you2017learning,shen2020meal,stanton2021does} 
and with techniques to boost the their 
performance~\citep{shen2019meal,fakoor2020fast,malinin2019ensemble}. None of 
these works have comprehensively studied finding the best teacher along with 
the necessary augmentations that result in consistent improvements over 
multiple student architectures.
%
%
%
%

To understand what makes a good teacher to reinforce datasets, we perform 
knowledge distillation with a variety of pretrained models in the Timm 
library~\citep{rw2019timm} distilled to three representative student 
architectures MobileNetV3-large~\citep{howard2019searching}, 
ResNet-50~\citep{he2016deep}, and ViT-Small~\citep{dosovitskiy2020image}.
MobileNetV3 represents light-weight CNNs that often prefer easier training.  
ResNet-50 represents heavy-weight CNNs that can benefit from difficult training 
regimes but do not heavily rely on it because of their implicit inductive bias 
of the architecture.
ViT-small represents the transformer architectures that have less implicit bias 
compared with CNNs and learn better in the presence of complex and difficult 
datasets.
We consider various families of models as teachers including ResNets (34--152 
and type d variants)~\citep{he2016deep}, ConvNeXt family pretrained on the 
\IN{}-22K and fine-tuned on \IN{}-1K~\citep{liu2022convnet}, DeiT-3 
pretrained on the \IN{}-21K and fine-tuned on \IN{}-1K, IG-ResNext 
pretrained on the Instagram dataset~\citep{mahajan2018exploring}, EfficientNets 
with Noisy Student training~\citep{xie2020self}, and Swin-TransformersV2 
pretrained with and without \IN{}-22K and fine-tuned on 
\IN{}-1K~\citep{liu2022swin}.
This collection covers a variety of vision transformers and CNNs  pretrained on 
a wide spectrum of dataset sizes.
We train all students with $224\times 224$ inputs and follow 
\citep{beyer2022knowledge} to match the resolution of teachers optimized to 
take larger inputs by passing the large crop to the teacher and resize it to 
$224\times 224$ for the student.

We present the accuracies of students trained for 300 epochs as a function of 
the teacher accuracy in~\cref{fig:timm_distill_e300}.
Focusing first on the single (non-ensemble) networks (marked by circles),  
consistent with prior work, we observe that the most accurate models are not 
usually the best teachers~\citep{mirzadeh2020improved}.
For CNN model families (ResNets, EfficientNets, ResNexts, and ConvNeXts), the 
student accuracy is generally correlated with the teacher accuracy.
%
%
When increasing the teacher accuracy, the student first improves but then it 
starts to saturate or even drops with the most accurate member of the family.  
Vision Transformers (Swin-Transformers, and DeiT-3) as teachers do not show the 
same trend as the accuracy of the students flattens across different teachers.  
Recently,\citet{liu2022meta} suggested that temperature tuning can help in KD 
from larger teachers. We do not adopt such hyperparameter tuning strategies in 
favor of architecture-independence and generalizability of dataset 
reinforcement.

On the other side, ensembles of state-of-the-art models (marked by asterisks) 
are consistently better teachers compared with any individual member of the 
family.  We create 4-member ensembles of the best models from IG-ResNexts, 
ConvNeXts, and DeiT3 to cover CNNs, vision transformers, and extra data models.  
We find IG-ResNext teacher to provide a balanced improvement across all 
students. IG-ResNext models are also trained with $224\times 224$ inputs while, 
for example, the best teacher from EfficientNet-NS family, EfficientNet-L2-NS, 
performs best at larger resolutions that is significantly more expensive to 
train with.

One of the benefits of dataset reinforcement paradigm is that the teacher can 
be expensive to train and use as long as we can afford to run it \emph{once} on 
the target dataset for reinforcement. Also, the process of dataset 
reinforcement is highly parallelizable because performing the forward-pass on 
the teacher to generate predictions on multiple augmentations does not depend 
on any state or any optimization trajectory. For these reasons, we also 
considered significantly scaling knowledge distillation to super large 
ensembles with up to 128 members.  We discuss our findings in 
\cref{sec:super_ensembles}.
Full table of accuracies for this section are in 
\cref{sec:timm_distill_appendix}.

%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%




\subsection{\INp{}: What is the best combination of 
reinforcements?}\label{sec:aug_difficulty}
%

\begin{table*}[t!]
    \centering
    \resizebox{1.8\columnwidth}{!}{
        \begin{tabular}{lcccc}
            \toprule[1.5pt]
            &
            \multirow{2}{*}{\bfseries \shortstack[l]{Sparse teacher prob.}} &
            \multirow{2}{*}{\bfseries \shortstack[l]{Random Resize Crop \\
            + Horizontal Flip}} &
            \multirow{2}{*}{\bfseries \shortstack[l]{Random Augment \\
            + Random Erase}} &
            \multirow{2}{*}{\bfseries \shortstack[l]{MixUp + CutMix}} \\
            \\
            \midrule[1.25pt]
            \textbf{\INp{} variant} & All & All & +RA/RE, +M*+R* & +Mixing, M*+R*\\
            \textbf{Apply probability} &
            1 & 1, 0.5 & 1, 0.25 & 0.5, 0.5\\
            \textbf{Parameters} &
            $10\times$ (Index, Prob) &
            $4\times$ Coords + Flip bit &
            $2\times$ (Op Id, Magnitude) +
            $4\times$ Coords &
            (Img Id, $\lambda$) +
            (Img Id, $4\times$ Coords) \\
            \textbf{Storage space (in bytes)} &
            $10\times (2\times 4)$ &
            $4\times 4 + 1$ &
            $2\times 2\times 4 + 4\times 4$ &
            $2\times 4 + (1+4)\times 4$\\
            %
            %
            \textbf{Total storage space ($400$ samples per image)} &
            38 GB & 8  GB & 15  GB & 13  GB\\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{\textbf{Additional storage in \INp{} variants}.  Total additional 
    storage for \INp{} (\RRCRARE{}) is 61 GBs.}
    \label{tab:imagenet_plus_specs}
    %
    %
    %
    %
    %
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\end{table*}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[b]{2\columnwidth}
        \centering
        \begin{tabular}{cccc}
            \hspace{8mm} \scalebox{0.75}{MobileNetV1}
            & \hspace{8mm} \scalebox{0.75}{MobileNetV2}
            & \hspace{8mm} \scalebox{0.75}{MobileNetV3} 
            & \hspace{3mm} \scalebox{0.85}{\textbf{\INp{} Variants}}  \\[-1mm]
            %
             \raisebox{-0.5\height}{\includegraphics[width=0.24\columnwidth]{figures/figs_delta_full/mobilenetv1.pdf}} 
             & \raisebox{-0.5\height}{\includegraphics[width=0.24\columnwidth]{figures/figs_delta_full/mobilenetv2.pdf}} 
             & \raisebox{-0.5\height}{\includegraphics[width=0.24\columnwidth]{figures/figs_delta_full/mobilenetv3.pdf}}
             & \hspace{4mm} \raisebox{-0.25\height}{\includegraphics[width=0.16\columnwidth]{figures/figs_delta_full/legend.pdf}}
        \end{tabular}
        \caption{Light-weight CNNs}
        \label{fig:light_cnn_imagenet_delta}
    \end{subfigure}
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    \vfill
    \begin{subfigure}[b]{\columnwidth}
        \centering
        %
        \begin{tabular}{cc}
            \hspace{8mm} \scalebox{0.75}{ResNet} & \hspace{8mm} 
            \scalebox{0.75}{EfficientNet}\\[-1mm]
             \includegraphics[width=0.48\columnwidth]{figures/figs_delta_full/resnet.pdf} 
             & \includegraphics[width=0.48\columnwidth]{figures/figs_delta_full/efficientnet.pdf}
        \end{tabular}
        %
        \vspace{-2mm}
        \caption{Heavy-weight CNNs}
        \label{fig:heavy_cnn_imagenet_delta}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\columnwidth}
        \centering
        %
        \begin{tabular}{cc}
            \hspace{8mm} \scalebox{0.75}{ViT} & \hspace{8mm} 
            \scalebox{0.75}{SwinTransformer}\\[-1mm]
             \includegraphics[width=0.48\columnwidth]{figures/figs_delta_full/vit.pdf} 
             & \includegraphics[width=0.48\columnwidth]{figures/figs_delta_full/swin.pdf}
        \end{tabular}
        %
        \vspace{-2mm}
        \caption{Transformers}
        \label{fig:transformer_imagenet_delta}
    \end{subfigure}
    \vspace{-2mm}
    \caption{\textbf{Improvements across architectures with \INp{} variants 
    compared with \IN{}.}  Top-1 accuracy of different models on the \IN{} 
    validation set consistently improves when trained with the proposed 
    datasets as compared to the standard \IN{} training set (Epochs=150).  Our 
    proposed dataset variant with \RRCRARE{}, \textbf{\INp{}}, provides 
    balanced improvements of 1-4\% across architectures. Further improvements 
    with longer training (300-1000 epochs) in \cref{tab:long_train_effect}.
    %
    %
    %
    }\label{fig:imagenet_delta_150}
    \vspace{-4mm}
\end{figure*}


In this section, we introduce \INp{}, a reinforcement of \IN{}.  We 
create \INp{} using the IG-ResNext ensemble (\cref{sec:good_teacher}).  
Following \citep{shen2021fast}, we store top $10$ sparse probabilities for 
$400$ augmentations per training sample in the \IN{} dataset 
\cite{deng2009imagenet}.
We consider the following augmentations: Random-Resize-Crop (\RRC{}), 
MixUp~\citep{zhang2017mixup} and CutMix~\citep{yun2019cutmix} (\Mixing{}), and 
RandomAugment~\citep{cubuk2020randaugment} and RandomErase (\RARE{}).
We also combine \Mixing{} with \RARE{} and refer to it as \MsRs.  We add all 
augmentations on top of \RRC{} and for clarity add + as shorthand for 
{\RRC{}$+$}. We provide a summary of the reinforcement data stored for each 
\INp{} variant in \cref{tab:imagenet_plus_specs}.
%

\vspace{-4mm}
\paragraph{Models} We study light-weight CNN-based (MobileNetV1 \cite{howard2017mobilenets}/ V2 
\cite{sandler2018mobilenetv2}/ V3\cite{howard2019searching}), heavy-weight 
CNN-based (ResNet \cite{he2016deep} and EfficientNet 
\cite{tan2019efficientnet}), and transformer-based (ViT 
\cite{dosovitskiy2020image} and SwinTransformer \cite{liu2021swin}) models. We 
follow \cite{mehta2022cvnets, wightman2021resnet} and use  state-of-the-art 
recipes, including optimizers, hyperparameters, and learning schedules, 
specific to each model on the \IN{}. We perform \textbf{no hyperparameter 
tuning specific to \INp{}} and achieve improvements with the same setup as 
\IN{} for all models.

\vspace{-4mm}
\paragraph{Better accuracy}
We evaluate the performance of each model in terms of top-1 accuracy on the 
\IN{} validation set.
\Cref{fig:imagenet_delta_150} compares the performance of different models 
trained using \IN{} and \INp{} datasets.  
\cref{fig:light_cnn_imagenet_delta} shows that light-weight CNN models do not 
benefit from difficult reinforcements. This is expected because of their 
limited capacity. On the other side, both heavy-weight CNN 
(\cref{fig:heavy_cnn_imagenet_delta}) and transformer-based 
(\cref{fig:transformer_imagenet_delta}) models benefit from difficult 
reinforcements (\RRCMixing{}, \RRCRARE{}, and \RRCMsRs{}). However, 
transformer-based models deliver best performance with the most difficult 
reinforcement (\RRCMsRs{}). This concurs with previous works that show 
transformer-based models, unlike CNNs, benefit from more data regularization as 
they do not have inductive 
biases~\citep{dosovitskiy2020image,touvron2021training}.  
 
Overall, \textbf{\RRCRARE{}} provides a balanced trade-off between performance 
and model size across different models.  Therefore, in the rest of this paper, 
we use \RRCRARE{} as our reinforced dataset and call it \textbf{\INp{}}. In 
the rest of the paper, we show results for three models that spans different 
model sizes and architecture designs (MobileNetV3-Large, ResNet-50, and 
SwinTransformer-Tiny).

%
%
%
%
%
%
%
%
%

We note that our observations are consistent across different architectures and 
recommend to see \cref{sec:imagenet_plus_full} for comprehensive results on 25 
architectures.
We provide expanded ablation studies in \cref{sec:reinforce_imagenet_sup}
using a cheaper teacher, ConvNext-Base-IN22FT1K. For example, we find 
1) The number of stored samples can be $3\times$ fewer than intended training 
epochs,
2) Additional augmentations on top of \INp{} are not useful.
3) Tradeoff in reinforcement difficulty can be further reduced with
curriculums.
4) Curriculums are better than various sample selection methods at the time of 
reinforcing the dataset.
We provide all hyperparameters and training recipes in \cref{sec:hparams}.



\subsection{\CIFARp{}, \Flowersp{}, \Foodp{}: How to reinforce other 
datasets?}
\label{sec:other_datasets}

We reinforced \IN{} due to its popularity and effectiveness as a pretraining 
dataset for other tasks (e.g., object detection).  Our findings on \IN{} are 
also useful for reinforcing other datasets and reduce the need for exhaustive 
studies.  Specifically, we suggest the following guidelines:
1) use ensemble of strong teachers trained on large diverse data
2) balance reinforcement difficulty and model complexity.

In this section, we extend dataset reinforcement to three other datasets, 
\CIFAR{}~\citep{krizhevsky2009learning}, 
\Flowers{}~\citep{nilsback2008automated}, and \Food{}~\citep{bossard14},
with 50K, 1K, and 75K training data respectively.
%
We build a teacher for each dataset by fine-tuning \INp{} pretrained 
ResNet-152 that reaches the accuracy of 90.6\%, 96.6\%, and 91.8\%, respectively.
By repeating fine-tuning 4 times, we get three teacher ensembles of 
4xResNet-152.
Next we generate reinforcements using similar augmentations to \INp{}, that 
is \RRCRARE{}. We store 800, 8000, and 800 augmentations per original sample.  
After that, we train various models on the reinforced data at similar training 
time to standard training. To achieve the best performance, we use pretrained 
models on \IN{}/\INp{} and fine-tune on each dataset for varying epochs 
up to 1000, 10000, and 1000 (for \CIFAR{}, \Flowers{}, and \Food{}, 
respectively) and report the best result.

\Cref{tab:transfer_cls_mobilenetv3} shows that MobileNetV3-Large pretrained and 
fine-tuned with reinforced datasets reaches up to 3\% better accuracy. We 
observe that pretraining and fine-tuning on reinforced datasets together give 
the largest improvements. We provide results for other models in
\cref{sec:transfer_full}.



\begin{table}[t!]
    \centering
    \resizebox{0.95\columnwidth}{!}{
        \begin{tabular}{lcc|cc|cc}
            \toprule[1.5pt]
            \multirow{2}{*}{\textbf{Pretraining Dataset}} 
            & \multicolumn{2}{c}{\textbf{\CIFAR{}}}
            & \multicolumn{2}{c}{\textbf{\Flowers{}}}
            & \multicolumn{2}{c}{\textbf{\Food{}}}  \\
            \cmidrule[1.25pt]{2-7}
             & \textbf{Orig.} & \textbf{+}
             & \textbf{Orig.} & \textbf{+}
             & \textbf{Orig.} & \textbf{+}\\
             \midrule[1.25pt]
             None & 80.2 & 83.6 & 68.8 & 87.5 & 85.1 & 88.2\\
             \IN{} &  84.4 & 87.2 & 92.5 & 94.1 & 86.1 & 89.2 \\
             \INp{} (Ours) &  86.0 & \textbf{87.5} & 93.7 & \textbf{95.3} & 86.6 &\textbf{89.5} \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{\textbf{Pretraining and fine-tuning on reinforced datasets is up 
    to  
    3.4\% better than using non-reinforced datasets.}
    Top-1 accuracy on the test set for MobileNetV3-Large is shown.
    On \Food{}, 86.1\% is improved to 89.5\%, demonstrating composition of 
    reinforced datasets.}
    \label{tab:transfer_cls_mobilenetv3}
    \vspace{-4mm}
\end{table}


