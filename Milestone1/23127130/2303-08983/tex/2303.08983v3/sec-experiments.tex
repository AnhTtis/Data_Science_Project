\section{Experiments}\label{sec:experiments}
\paragraph{Baseline methods} We compare the performance of models trained using 
\INp{} with the following baseline methods: (1) 
\emph{KD}~\citep{hinton2015distilling,beyer2022knowledge} (Online 
distillation): A standard knowledge distillation method with strong teacher 
models and model-specific augmentations, (2)  
\emph{MEALV2}~\citep{shen2020meal} (Fine-tuning distillation): Distill 
knowledge to student with good initialization from multiple teachers, (3) 
\emph{FunMatch}~\citep{beyer2022knowledge} (Patient online distillation): 
Distill for significantly many epochs with strong augmentations, (4) 
\emph{ReLabel}~\citep{yun2021re} (Offline label-map distillation): Pre-computes 
global label maps from the pre-trained teacher, and (5) 
\emph{FKD}~\citep{shen2021fast} (Offline distillation): Pre-computes soft 
labels using multi-crop knowledge distillation. We consider FKD as the baseline 
approach for dataset reinforcement.
%

\vspace{-4mm}
\paragraph{Longer training}\label{sec:long_train}
Recent works have shown that models trained for few epochs (e.g.,\ 100 epochs) 
are sub-optimal and their performance improves with longer training 
\cite{wightman2021resnet,dosovitskiy2020image,touvron2021training}. Following these works, we train 
different models at three epoch budgets, i.e., 150, 300, and 1000 epochs, using 
both \IN{} and \INp{} datasets.  \Cref{tab:long_train_effect} shows  
models trained with \INp{} dataset consistently deliver better accuracy in 
comparison to the ones trained on \IN{}.
%
An epoch of \INp{} consists of exactly one random reinforcement per sample 
in \IN{}.


%

\begin{table}[t!]
    \centering
    \resizebox{0.95\columnwidth}{!}{
        \begin{tabular}{llccc}
            \toprule[1.5pt]
            \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Dataset}} 
            & \multicolumn{3}{c}{\textbf{Training Epochs}}  \\
            \cmidrule[1.25pt]{3-5}
             & & \textbf{150} & \textbf{300} & \textbf{1000} \\
             \midrule[1.25pt]
             \multirow{2}{*}{MobileNetV3-Large} & \IN{} & 74.7 & 74.9 & 75.1 \\
             & \INp{} (Ours) & \textbf{76.2} & \textbf{77.0} &  \textbf{77.9} \\
             \midrule
             \multirow{2}{*}{ResNet-50} & \IN{} & 77.4 &  78.8 & 79.6 \\
             & \INp{} (Ours) & \textbf{79.6} & \textbf{80.6} & \textbf{81.7} \\
             \midrule
             \multirow{2}{*}{SwinTransformer-Tiny} & \IN{} & 79.9 & 80.9 &  80.9 \\
              & \INp{} (Ours) & \textbf{82.0} & \textbf{83.0} & \textbf{83.8} \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{\textbf{\INp{} models consistently outperform \IN{} models 
        when trained for longer}. Top-1 accuracy on the \IN{} validation set 
        is shown. An epoch has the same number of iterations for 
        \IN{}/\INp{}.}
    \label{tab:long_train_effect}
    \vspace{-4mm}
\end{table}

\vspace{-4mm}
\paragraph{Training and reinforcement time} \Cref{tab:long_train_effect} shows 
\INp{} improves the performance of various models. A natural question 
that arises is: \emph{Does \INp{} introduce computational overhead when 
training models?}
On average, training MobileNetV3-Large, ResNet-50, and SwinTransformer-Tiny is 
$1.12\times$, $1.01\times$, and $0.99\times$ the total training time on 
\IN{}. The extra time for MobileNetV3 is because there is no data 
augmentations in our baseline.
%
\INp{} took 2205 GPUh to generate using 64xA100 GPUs, which is highly 
parallelizable.
For comparison, training ResNet-50 for 
%
300 epochs on 8xA100 GPUs takes 206 GPUh.
The reinforcement generation is a one-time cost that is amortized over many 
uses.  The time to reinforce other datasets and the storage is discussed in 
\cref{sec:cost}.

\begin{table}[b!]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{llccccc}
            \toprule[1.5pt]
            \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Dataset}} & \textbf{Offline} & \textbf{Random } & \multirow{2}{*}{\textbf{Epochs}} & \multirow{2}{*}{\textbf{Accuracy}}   \\
             &  & \textbf{KD?} & \textbf{Init.?} &  &   \\
             \midrule[1.25pt]
              & \IN{}~\cite{howard2019searching} & NA & \cmark & 600 & 75.2 \\
             MobileNetV3& FunMatch~\citep{beyer2022knowledge}* & \xmark & \cmark & 1200 & 76.3\\
             -Large & MEALV2~\citep{shen2020meal} & \xmark & \xmark & 180 & 76.9 \\
             & \INp{} (Ours) & \cmark & \cmark & 300 & \textbf{77.0}\\
             \midrule[1pt]
             \multirow{6}{*}{ResNet-50} & \IN{} \cite{wightman2021resnet} & NA & \cmark & 600 & 80.4 \\
             & ReLabel~\citep{yun2021re} & \cmark & \cmark & 300 & 78.9\\
             %
             %
             & FKD~\citep{shen2021fast} & \cmark & \cmark & 300 & 80.1\\
             %
             & MEALV2~\citep{shen2020meal} & \xmark & \xmark & 180 & 80.6\\
             & \INp{} (Ours) & \cmark & \cmark & 300 & 80.6\\
             & \INp{} (Ours) & \cmark & \cmark & 1000 & \textbf{81.7}\\
             & FunMatch~\citep{beyer2022knowledge}* & \xmark & \cmark & 1200 & \textbf{81.8}\\
             %
             %
             \midrule[0.5pt]
             ResNet-101
             & \IN{} \cite{wightman2021resnet} & NA & \cmark & 1000 & 81.5 \\
%
             \midrule[1pt]
             \multirow{4}{*}{ViT-Tiny} & \IN{} \cite{touvron2021training} & NA & \cmark & 300 & 72.2 \\
              & DeiT \cite{touvron2021training} & \xmark & \cmark & 300 & 74.5 \\
             & FKD~\citep{shen2021fast} & \cmark & \cmark & 300 & 75.2\\
             %
             & \INp{} (Ours) & \cmark & \cmark & 300 & \textbf{75.8}\\
             %
             %
             \midrule[1pt]
             \multirow{3}{*}{ViT-Small}
             & \IN{}~\cite{touvron2021training} & NA & \cmark & 300 & 79.8 \\
             & DeiT~\cite{touvron2021training} & \xmark & \cmark & 300 & 81.2 \\
             & \INp{} (Ours) & \cmark & \cmark & 300 & \textbf{81.4}\\
             \midrule[1pt]
             %
             %
             %
             %
             \multirow{3}{*}{ViT-Base$\uparrow$384}
             & \IN{}~\citep{touvron2021training} & NA & \cmark & 300 & 83.1\\
             & DeiT~\cite{touvron2021training} & \xmark & \cmark & 300 & 83.4 \\
             & \INp{} (Ours) & \cmark & \cmark & 300 & \textbf{84.5}\\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{\textbf{Comparison with state-of-the-art methods on the \IN{} 
    validation set.} Models trained with \INp{} dataset deliver similar or 
    better performance than existing methods. Importantly, unlike online KD 
    methods (e.g., FunMatch or DeiT), \INp{} does not add computational 
    overhead to standard \IN{} training (\cref{fig:wall_clock}). Here, NA 
    denotes standard supervised \IN{} training with no online/offline KD.
    $\uparrow$384 denotes training at 384 resolution. An epoch has the same 
    number of iterations for \IN{}/\INp{}.}
    \label{tab:comparison_to_literature}
    \vspace{-4mm}
\end{table}

\vspace{-4mm}
\paragraph{Comparison with state-of-the-art methods} 
\Cref{tab:comparison_to_literature} compares the performance of models trained 
with \INp{} and existing methods. We make following observations: (1) Compared 
to the closely related method, i.e., FKD, models trained using \INp{} deliver 
better accuracy. (2) We achieve comparable results to online distillation 
methods (e.g., FunMatch), but with fewer epochs and faster training 
(\cref{fig:wall_clock}). (3) Small variants of the same family trained with 
\INp{} achieve similar performance to larger models  trained with \IN{} 
dataset. For example, ResNet-50 (81.7\%) with \INp{} achieves similar 
performance  as ResNet-101 with \IN{} (81.5\%). We observe similar phenomenon 
across other models, including light-weight CNN models. This enables replacing 
large models with smaller variants in their family for faster inference across 
devices, including edge devices, without sacrificing accuracy.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%


%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\subsection{Transfer Learning}
\label{sec:transfer} 
To evaluate the transferability of models 
pre-trained using \INp{} dataset, we evaluate on following tasks: (1) 
semantic segmentation with DeepLabv3 \cite{chen2017rethinking} on the ADE20K 
dataset~\citep{zhou2019semantic}, (2) object detection with Mask-RCNN 
\cite{he2017mask} on the MS-COCO dataset~\citep{lin2014microsoft}, and (3) 
fine-grained classification on the \CIFAR{}~\citep{krizhevsky2009learning}, 
\Flowers{}~\citep{nilsback2008automated}, and \Food{}~\citep{bossard14} 
datasets.



\Cref{tab:transfer_det_seg,tab:transfer_cls} show models trained on the 
\INp{} dataset have better transferability properties as compared to the 
\IN{} dataset across different tasks (detection, segmentation, and 
fine-grained classification). To analyze the isolated impact of \INp{} in 
this section, the fine-tuning datasets are not reinforced. We present all 
combinations of training with reinforced/non-reinforced pretraining/fine-tuning 
datasets in \cref{sec:transfer_full}.


%



%
%


\begin{table}[t!]
    \centering
    \resizebox{0.8\columnwidth}{!}{
        \begin{tabular}{llcc}
            \toprule[1.5pt]
            \multirow{2}{*}{\textbf{Model}} 
            & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Pretraining dataset}}}
            & \multicolumn{2}{c}{\textbf{Task}}  \\
            \cmidrule[1.25pt]{3-4}
             & & ObjDet & SemSeg \\
             \midrule[1.25pt]
             \multirow{2}{*}{MobileNetV3-Large} & \IN{} & 35.5 & 37.2 \\
             & \INp{} (Ours) & \textbf{36.1} & \textbf{38.5} \\
             \midrule
             \multirow{2}{*}{ResNet-50} & \IN{} & 42.2 &  42.8 \\
             & \INp{} (Ours) & \textbf{42.5} & \textbf{44.2} \\
             \midrule
             \multirow{2}{*}{SwinTransformer-Tiny} & \IN{} & 45.8 & 41.2
             \\
              & \INp{} (Ours) & \textbf{46.5} & \textbf{42.5} \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{\textbf{Transfer learning for object detection and semantic 
    segmentation}. For object detection (ObjDet), we report standard mean 
    average precision on MS-COCO dataset while for sementic segmentation 
    (SemSeg), we report mean intersection accuracy on ADE20K dataset. Task 
    datasets are not reinforced.}
    \label{tab:transfer_det_seg}
    \vspace{-4mm}
\end{table}


\subsection{Robustness analysis} 
%
\label{sec:robustnes}
To evaluate the robustness of different models 
trained using the \INp{} dataset, we evaluate on three subsets of the 
\IN{}V2 dataset \citep{recht2019imagenet}, which is specifically designed to 
study the robustness of models trained on the \IN{} dataset. We also 
evaluate \IN{} models on other distribution shift datasets, 
\IN{}-A~\citep{hendrycks2021nae}, \IN{}-R~\citep{hendrycks2021many}, 
\IN{}-Sketch~\citep{wang2019learning}, ObjectNet~\citep{barbu2019objectnet}, 
and \IN{}-C~\citep{hendrycks2019robustness}.
We measure the top-1 accuracy except for \IN{}-C. On \IN{}-C, we measure 
the mean corruption error (mCE) and report 100 minus {mCE}.

\cref{tab:imagenetv2_accuracy} shows that models trained using \INp{} 
dataset are up to 20\% more robust.  Overall, these robustness results 
in conjunction with results in
\cref{tab:long_train_effect} highlight the effectiveness of the proposed 
dataset.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\begin{table*}[t!]
    \centering
    \resizebox{0.95\textwidth}{!}{
        \begin{tabular}{llcccccccc|c}
            \toprule[1.5pt]
            \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Dataset}} 
            & \multicolumn{3}{c}{\textbf{\IN{}-V2}}
            & \multirow{2}{*}{\textbf{\IN{}-A}} 
            & \multirow{2}{*}{\textbf{\IN{}-R}} 
            & \multirow{2}{*}{\textbf{\IN{}-Sketch}} 
            & \multirow{2}{*}{\textbf{ObjectNet}} 
            & \multirow{2}{*}{\textbf{\IN{}-C}}
            & \multirow{2}{*}{\textbf{Avg.}}\\
            %
            %
            %
            \cmidrule[1.25pt]{3-5}
             & & V2-A & V2-B & V2-C\\
             \midrule[1.25pt]
             \multirow{2}{*}{MobileNetV3-Large} & \IN{} & 71.5 & 62.9 & 76.8 
             & 4.5 & 32.4 & 20.6 & 32.8 & 21.8 & 30.4\\
             & \INp{} (Ours) & \textbf{75.1} & \textbf{66.3} 
             &  \textbf{80.5} & \textbf{7.6} & \textbf{42.0} & \textbf{29.0} 
             & \textbf{38.1} & \textbf{32.0} & \textbf{37.1}\\
             \midrule
             \multirow{2}{*}{ResNet-50} & \IN{} & 76.3 &  67.4 & 81.3
             & 11.9 & 38.1 & 27.4 & 41.6 & 33.2 & 37.9\\
             & \INp{} (Ours) & \textbf{79.3} & \textbf{71.3} 
             & \textbf{83.8}
             & \textbf{15.1} & \textbf{48.1} & \textbf{34.9} & \textbf{46.8} 
             & \textbf{39.0} & \textbf{43.7}\\
             \midrule
             \multirow{2}{*}{SwinTransformer-Tiny} & \IN{} & 77.0 & 69.3 
             &  81.6 & 21.0 & 37.7 & 25.4 & 40.5 & 36.9 & 39.6\\
              & \INp{} (Ours) & \textbf{81.5} & \textbf{74.1} 
              & \textbf{85.3} & \textbf{30.2} & \textbf{58.0}$^*$ 
              & \textbf{40.8} & \textbf{50.6} & \textbf{46.6} & \textbf{51.1}\\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{\textbf{\INp{} models are up to 20\% more robust on \IN{} 
    distribution shifts}. All models are trained for 
    1000 epochs.  We report on \IN{}V2 variations Threshold-0.7 (V2-A), 
         Matched-Frequency (V2-B), and Top-Images (V2-C). We report accuracy on 
         all datasets except for \IN{}-C where we report 100 minus mCE
         metric. $^*$ Largest improvement.}
    \label{tab:imagenetv2_accuracy}
    \vspace{-2mm}
    %
\end{table*}




%

%

%
%

\begin{table}[b!]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{llccc}
            \toprule[1.5pt]
            \multirow{2}{*}{\textbf{Model}} 
            & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Pretraining dataset}}} 
            & \multicolumn{3}{c}{\textbf{Fine-tuning dataset}}  \\
            \cmidrule[1.25pt]{3-5}
            & & \CIFAR{} & \Flowers{} & \Food{}\\
             \midrule[1.25pt]
             \multirow{2}{*}{MobileNetV3-Large}
             & \IN{} &  84.4 & 92.5 & 86.1  \\
             & \INp{} (Ours) &  \textbf{86.0} & \textbf{93.7}  
             & \textbf{86.6}  \\
             \midrule
             %
             %
             %
             %
             %
             %
             %
             \multirow{2}{*}{ResNet-50}
             & \IN{} & 88.4 &  93.6 & 90.0 \\
             & \INp{} (Ours) & \textbf{88.8} & \textbf{95.0} & \textbf{90.5} 
             \\
             \midrule
             \multirow{2}{*}{SwinTransformer-Tiny}
             & \IN{} & 90.6 & 96.3 &  92.3 \\
             & \INp{} (Ours) & \textbf{90.9} & \textbf{96.6} & \textbf{93.0} 
             \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{\textbf{Transfer learning for fine-grained object classification.} 
    Only pretraining dataset is reinforced and fine-tuning datasets are not 
    reinforced.  Reinforced pretraining/fine-tuning results in 
    \cref{tab:teaser_results}.}
    \label{tab:transfer_cls}
    \vspace{-4mm}
\end{table}

\subsection{Calibration: Why are \INp{} models robust and transferable?}
\label{sec:calibration}
To understand why \INp{} models are significantly more robust than \IN{} 
models we evaluate their Expected Calibration Error 
(ECE)~\citep{kumar2019verified} on the validation set.  
\cref{fig:val_calib_error} shows that \INp{} models are well-calibrated and 
significantly better than \IN{} models. This matches recent observations 
about ensembles that out-of-distribution robustness is better for 
well-calibrated models~\citep{kumar2022calibrated}. Full calibration results are presented in 
\cref{sec:val_calib_error_full}.

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/figs_calib_plus/val_calib_error.pdf}
       \vspace{-4mm}
    \caption{\textbf{\INp{} models are well-calibrated}.
    We plot the Expected Calibration Error (ECE) on the \IN{} validation set 
    over the validation error (normalized by 100 to range $[0, 1]$) for 
    MobileNetV3/ResNet-50/Swin-Tiny architectures trained for 300 and 1000 
    epochs on \IN{} and \INp{}.  \INp{} models are significantly more 
    calibrated, even matching or better than their teacher (IG-ResNext 
    Ensemble). We also observe that the IG-ResNext model is one of the best 
    calibrated models on the validation set from our pool of teachers.
    }
    \label{fig:val_calib_error}
    %
\end{figure}

\subsection{Comparison with FKD and ReLabel.}

We reproduce FKD and ReLabel with our training recipe as well as regenerate the 
dataset of FKD.
We compare the accuracy on \IN{} validation and its distribution shifts as 
well as the cost of dataset generation/storage.  We train models for 300 
epochs.
%
%

\vspace{-4mm}
\paragraph{Training recipe}
We report results of training with our code on the released datasets of 
ReLabel and {FKD}.
In addition to reproducing FKD results by training on their released dataset of 
500-sample per image, we also reproduce their dataset using our code and their 
teacher.  \cref{tab:fkd_comparison} verifies that our improvements are due to 
the superiority of \INp{}, not any other factors such as the training recipe.
Our \INp{}-RRC is also closely related to FKD as it uses the same set of 
augmentations (random-resized-crop and horizontal flip) but together with our 
optimal teacher (4xIG-ResNext).  We observe that \INp{}-RRC achieves better 
results than FKD but still lower than \INp{} 
(\cref{tab:imagenet_plus_table6_e1000,fig:imagenet_delta_150}).

\vspace{-4mm}
\paragraph{Generation/Storage Cost} We provide comparison of generation/storage 
costs in \cref{tab:fkd_comparison}. In our reproduction, generating FKD's data 
takes 2260 GPUh, slightly more than \INp{} because their teacher processes 
inputs at the larger resolution of $475\times 475$ compared to our resolution 
of $224\times 224$.

\vspace{-4mm}
\paragraph{\INp{}-Small} We subsampled \INp{} into a variant that is {10.6} 
GBs, comparable to prior work. We reduce the number of samples per image to 
{100} and store teacher probabilities with top-5 sparsity. If not subsampled 
from \INp{}, generating \INp{}-Small would take half the time of FKD (200 
samples) while still comparable in accuracy to \INp{}.  Note that \INp{} is 
more general-purpose and preferred, especially for long training.

%
%
%
%




\begin{table*}[tbh!]
    \vspace{-2mm}
    \centering
    \resizebox{0.95\textwidth}{!}{
        \begin{tabular}{ccccc|cc|cc|c|cc|cc}
            \toprule[1.5pt]
\textbf{Dataset} & \textbf{Our} & \textbf{Our} & \multicolumn{2}{c}{\textbf{Optimal}} & \textbf{Top-K} & \textbf{Num.} & \multicolumn{2}{c}{\textbf{Storage (GBs)}} & \textbf{Gen. Time} & \multicolumn{2}{c}{\textbf{ResNet-50}}& \multicolumn{2}{c}{\textbf{Swin-Tiny}} \\
& \textbf{Gen.} &\textbf{Train} & \textbf{Teacher} & \textbf{Aug.} &&\textbf{Samples}& Raw &GZIP& \textbf{(GPUh)}& IN & IN-OOD & IN & IN-OOD\\
ReLabel                    & \xmark & \cmark & \xmark & \xmark & 5 &   1 &          10.7 &  4.8&          10 & 79.5 & 45.7 & 81.2 & 48.2\\
FKD                        & \xmark & \cmark & \xmark & \xmark & 5 & 200 &          13.6 &  8.9&     904$^*$ & 79.8 & 45.0 & 82.0 & 48.7\\
FKD                        & \xmark & \cmark & \xmark & \xmark & 5 & 500 &          34.0 & 22.0&     2260$^*$& 80.1 & 45.0 & 82.2 & 48.9\\
FKD                        & \cmark & \cmark & \xmark & \xmark &10 & 400 &          46.3 & 33.4&        1808 & 79.8 & 45.0 & 82.1 & 49.0\\
\midrule[1.25pt]
\INp{}-RRC                 & \cmark & \cmark & \cmark & \xmark &10 & 400 &          46.3 & 33.4&         1993 & 80.3 & 46.5 & 82.4 & 51.0\\
\INp{}-Small  & \cmark & \cmark & \cmark & \cmark & 5 & 100 
            &  10.6& 5.6 & 551& \textbf{80.6} & \textbf{48.9} 
            & \textbf{82.9} & \textbf{54.6}\\
\INp{}                     & \cmark & \cmark & \cmark & \cmark &10 & 400 &          61.5 & 37.5&         2205 & \textbf{80.6} & \textbf{49.1} & \textbf{83.0} & \textbf{54.7}\\
\bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{\textbf{Comparison with Relabel and FKD. Up to 5.6\% better than 
    FKD on ImageNet-OOD}, the average of \IN{}-V2/A/R/S/O/C accuracies.  
    Highlighted accuracies are within 0.2\% of the best. Compared with prior 
    work, we use an optimal teacher (4xIG-ResNext) and optimal combination of 
    augmentations (RRC+RA/RE). $^*$ Our estimates.}
    \label{tab:fkd_comparison}
    %
\end{table*}

\subsection{CLIP-pretrained Teachers}

In this section, we evaluate the performance of CLIP-pretrained 
models~\citep{radford2021learning} fine-tuned on \IN{} as teachers. This 
study complements our large-scale study of teachers in \cref{sec:good_teacher} 
where we evaluated more than {100} SOTA large models and ensembles.  
\Cref{tab:imagenet_plus_clip_vit_mixed_short} compares an ensemble of {4} 
CLIP-pretrained models to our selected ensemble of 4 IG-ResNext models as well 
as a mixture of ResNext, ConvNext, CLIP-ViT, and ViT (abrv. RCCV) models (See 
\cref{sec:clip_vit_mix} for the model names). We generate new \INp{} 
variants and train various architectures for {1000} epochs on each dataset. We 
observe that \INp{} with our previously selected IG-ResNext ensemble is 
superior to CLIP-pretrained and mixed-architecture teachers across 
architectures.
The CLIP variant provides near the maximum gain on Swin-Tiny and mixing it with 
IG-ResNext reduces the gap on CNNs.
%
%
%
%


\begin{table}[tbh!]
    %
    \centering
    \resizebox{0.98\columnwidth}{!}{
        \input{tables/tables_clip_vit_mixed/table0_E1000_vs_timm_accuracy_short.tex}
    }
    \caption{\textbf{Our selected IG-ResNext ensemble is superior to 
    CLIP-pretrained ensembles.} We reinforce \IN{} dataset with an ensemble 
    of CLIP-pretrained models as well as a mixture of multiple architectures 
    and train various models for 1000 epochs.
    Subscripts show the improvement on top of the \IN{} accuracy.
    $^*$ Our chosen \INp{} variant.
    }
    \label{tab:imagenet_plus_clip_vit_mixed_short}
    \vspace{-5mm}
\end{table}
