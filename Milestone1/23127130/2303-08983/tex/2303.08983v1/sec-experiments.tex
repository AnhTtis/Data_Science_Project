\section{Experiments}\label{sec:experiments}
\paragraph{Baseline methods} We compare the performance of models trained using 
ImageNet+ with following baseline methods: (1) 
\emph{KD}~\citep{hinton2015distilling,beyer2022knowledge} (Online 
distillation): A standard knowledge distillation method with strong teacher 
models and model-specific augmentations, (2)  
\emph{MEALV2}~\citep{shen2020meal} (Fine-tuning distillation): Distill 
knowledge to student with good initialization from multiple teachers, (3) 
\emph{FunMatch}~\citep{beyer2022knowledge} (Patient online distillation): 
Distill for significantly many epochs with strong augmentations, (4) 
\emph{ReLabel}~\citep{yun2021re} (Offline label-map distillation): Pre-computes 
global label maps from the pre-trained teacher, and (5) 
\emph{FKD}~\citep{shen2021fast} (Offline distillation): Pre-computes soft 
labels using multi-crop knowledge distillation. We consider FKD as the baseline 
approach for dataset reinforcement.
%

\vspace{-4mm}
\paragraph{Longer training}\label{sec:long_train}
Recent works have shown that models trained for few epochs (e.g.,\ 100 epochs) 
are sub-optimal and their performance improves with longer training 
\cite{wightman2021resnet,dosovitskiy2020image,touvron2021training}. Following these works, we train 
different models at three epoch budgets, i.e., 150, 300, and 1000 epochs, using 
both ImageNet and ImageNet+ datasets.  \Cref{tab:long_train_effect} shows  
models trained with ImageNet+ dataset consistently deliver better accuracy in 
comparison to the ones trained on ImageNet.
%
An epoch of ImageNet+ consists of exactly one random reinforcement per sample 
in ImageNet.


%

\begin{table}[t!]
    \centering
    \resizebox{0.95\columnwidth}{!}{
        \begin{tabular}{llccc}
            \toprule[1.5pt]
            \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Dataset}} 
            & \multicolumn{3}{c}{\textbf{Training Epochs}}  \\
            \cmidrule[1.25pt]{3-5}
             & & \textbf{150} & \textbf{300} & \textbf{1000} \\
             \midrule[1.25pt]
             \multirow{2}{*}{MobileNetV3-Large} & ImageNet & 74.7 & 74.9 & 75.1 \\
             & ImageNet+ (Ours) & \textbf{76.2} & \textbf{77.0} &  \textbf{77.9} \\
             \midrule
             \multirow{2}{*}{ResNet-50} & ImageNet & 77.4 &  78.8 & 79.6 \\
             & ImageNet+ (Ours) & \textbf{79.6} & \textbf{80.6} & \textbf{81.7} \\
             \midrule
             \multirow{2}{*}{SwinTransformer-Tiny} & ImageNet & 79.9 & 80.9 &  80.9 \\
              & ImageNet+ (Ours) & \textbf{82.0} & \textbf{83.0} & \textbf{83.8} \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{\textbf{ImageNet+ models consistently outperform ImageNet models 
        when trained for longer}. Top-1 accuracy on the ImageNet validation set 
        is shown. An epoch has the same number of iterations for 
        ImageNet/ImageNet+.}
    \label{tab:long_train_effect}
    \vspace{-4mm}
\end{table}

\vspace{-4mm}
\paragraph{Training and reinforcement time} \Cref{tab:long_train_effect} shows 
that ImageNet+ improves the performance of different models. A natural question 
that arises is: \emph{Does ImageNet+ introduce computational overhead when 
training models?}
On average, training MobileNetV3-Large, ResNet-50, and SwinTransformer-Tiny is 
$1.12\times$, $1.01\times$, and $0.99\times$ the total training time on 
ImageNet. The extra time for MobileNetV3 is because there is no data 
augmentations in our baseline.
ImageNet+ took 2080 mins to generate using 64xA100 GPUs which is highly 
parallelizable and similar to training ResNet-50 for 300 epochs on 8xA100 GPUs.
This is a one-time cost that is amortized over many uses.
The time to reinforce other datasets and the storage is discussed in 
\cref{sec:cost}.

\begin{table}[b!]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{llccccc}
            \toprule[1.5pt]
            \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Dataset}} & \textbf{Offline} & \textbf{Random } & \multirow{2}{*}{\textbf{Epochs}} & \multirow{2}{*}{\textbf{Accuracy}}   \\
             &  & \textbf{KD?} & \textbf{Init.?} &  &   \\
             \midrule[1.25pt]
              & ImageNet \cite{howard2019searching} & NA & \cmark & 600 & 75.2 \\
             MobileNetV3& FunMatch~\citep{beyer2022knowledge}* & \xmark & \cmark & 1200 & 76.3\\
             -Large & MEALV2~\citep{shen2020meal} & \xmark & \xmark & 180 & 76.9 \\
             & ImageNet+ (Ours) & \cmark & \cmark & 300 & \textbf{77.0}\\
             \midrule[1pt]
             \multirow{6}{*}{ResNet-50} & ImageNet \cite{wightman2021resnet} & NA & \cmark & 600 & 80.4 \\
             & ReLabel~\citep{yun2021re} & \xmark & \cmark & 300 & 78.9\\
             %
             %
             & FKD~\citep{shen2021fast} & \cmark & \cmark & 300 & 80.1\\
             %
             & MEALV2~\citep{shen2020meal} & \xmark & \xmark & 180 & 80.6\\
             & ImageNet+ (Ours) & \cmark & \cmark & 300 & 80.6\\
             & ImageNet+ (Ours) & \cmark & \cmark & 1000 & \textbf{81.7}\\
             & FunMatch~\citep{beyer2022knowledge}* & \xmark & \cmark & 1200 & \textbf{81.8}\\
             %
             %
             \midrule[0.5pt]
             ResNet-101
             & ImageNet \cite{wightman2021resnet} & NA & \cmark & 1000 & 81.5 \\
%
             \midrule[1pt]
             \multirow{4}{*}{ViT-Tiny} & ImageNet \cite{touvron2021training} & NA & \cmark & 300 & 72.2 \\
              & DeiT \cite{touvron2021training} & \xmark & \cmark & 300 & 74.5 \\
             & FKD~\citep{shen2021fast} & \cmark & \cmark & 300 & 75.2\\
             %
             & ImageNet+ (Ours) & \cmark & \cmark & 300 & \textbf{75.8}\\
             %
             %
             \midrule[1pt]
             \multirow{3}{*}{ViT-Small}
             & ImageNet~\cite{touvron2021training} & NA & \cmark & 300 & 79.8 \\
             & DeiT~\cite{touvron2021training} & \xmark & \cmark & 300 & 81.2 \\
             & ImageNet+ (Ours) & \cmark & \cmark & 300 & \textbf{81.4}\\
             \midrule[1pt]
             %
             %
             %
             %
             \multirow{3}{*}{ViT-Base$\uparrow$384}
             & ImageNet~\citep{touvron2021training} & NA & \cmark & 300 & 83.1\\
             & DeiT~\cite{touvron2021training} & \xmark & \cmark & 300 & 83.4 \\
             & ImageNet+ (Ours) & \cmark & \cmark & 300 & \textbf{84.5}\\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{\textbf{Comparison with state-of-the-art methods on the ImageNet 
    validation set.} Models trained with ImageNet+ dataset deliver similar or 
    better performance than existing methods. Importantly, unlike online KD 
    methods (e.g., FunMatch or DeiT), ImageNet+ does not add computational 
    overhead to standard ImageNet training (\cref{fig:wall_clock}). Here, NA 
    denotes standard supervised ImageNet training with no online/offline KD.
    $\uparrow$384 denotes training at 384 resolution. An epoch has the same 
    number of iterations for ImageNet/ImageNet+}
    \label{tab:comparison_to_literature}
    \vspace{-4mm}
\end{table}

\vspace{-4mm}
\paragraph{Comparison with state-of-the-art methods} 
\Cref{tab:comparison_to_literature} compares the performance of different 
models trained with ImageNet+ and existing methods. We make following 
observations: (1) Compared to the closely related method, i.e., FKD, models 
trained using ImageNet+ deliver better accuracy. (2) We achieve comparable 
results to online distillation methods (e.g., 
FunMatch), but with fewer epochs and faster training 
(\cref{fig:wall_clock}). (3) Small variants of the same family trained with 
ImageNet+ achieve similar performance to larger models  trained with ImageNet 
dataset. For example, ResNet-50 (81.7\%) with ImageNet+ achieves similar performance  as 
ResNet-101 with ImageNet (81.5\%). We observe similar phenomenon across other 
models, including light-weight CNN models. This enables replacing large models 
with smaller variants in their family for faster inference across devices, 
including edge devices, without sacrificing accuracy.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%


%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{Transfer Learning}
\label{sec:transfer} 
To evaluate the transferability of models 
pre-trained using ImageNet+ dataset, we evaluate on following tasks: (1) 
semantic segmentation with DeepLabv3 \cite{chen2017rethinking} on the ADE20K 
dataset~\citep{zhou2019semantic}, (2) object detection with Mask-RCNN 
\cite{he2017mask} on the MS-COCO dataset~\citep{lin2014microsoft}, and (3) 
fine-grained classification on the CIFAR-100~\citep{krizhevsky2009learning}, 
Flowers-102~\citep{nilsback2008automated}, and Food-101~\citep{bossard14} 
datasets.



\cref{tab:transfer_det_seg,tab:transfer_cls} show models trained on the 
ImageNet+ dataset have better transferability properties as compared to the 
ImageNet dataset across different tasks (detection, segmentation, and 
fine-grained classification). To analyze the isolated impact of ImageNet+ in 
this section, the fine-tuning datasets are not reinforced. We present all 
combinations of training with reinforced/non-reinforced pretraining/fine-tuning 
datasets in \cref{sec:transfer_full}.


%



%
%


\begin{table}[t!]
    \centering
    \resizebox{0.8\columnwidth}{!}{
        \begin{tabular}{llcc}
            \toprule[1.5pt]
            \multirow{2}{*}{\textbf{Model}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Dataset}}}
            & \multicolumn{2}{c}{\textbf{Task}}  \\
            \cmidrule[1.25pt]{3-4}
             & & ObjDet & SemSeg \\
             \midrule[1.25pt]
             \multirow{2}{*}{MobileNetV3-Large} & ImageNet & 35.5 & 37.2 \\
             & ImageNet+ (Ours) & \textbf{36.1} & \textbf{38.5} \\
             \midrule
             \multirow{2}{*}{ResNet-50} & ImageNet & 42.2 &  42.8 \\
             & ImageNet+ (Ours) & \textbf{42.5} & \textbf{44.2} \\
             \midrule
             \multirow{2}{*}{SwinTransformer-Tiny} & ImageNet & 45.8 & 41.2
             \\
              & ImageNet+ (Ours) & \textbf{46.5} & \textbf{42.5} \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{\textbf{Transfer learning for object detection and semantic 
    segmentation}. For object detection (ObjDet), we report standard mean 
    average precision on MS-COCO dataset while for sementic segmentation 
    (SemSeg), we report mean intersection accuracy on ADE20K dataset. Task 
    datasets are not reinforced.}
    \label{tab:transfer_det_seg}
    \vspace{-4mm}
\end{table}


\subsection{Robustness analysis} 
%
\label{sec:robustnes}
To evaluate the robustness of different models 
trained using the ImageNet+ dataset, we evaluate on three subsets of the 
ImageNetV2 dataset \citep{recht2019imagenet}, which is specifically designed to 
study the robustness of models trained on the ImageNet dataset. We also 
evaluate ImageNet models on other distribution shift datasets, 
ImageNet-A~\citep{hendrycks2021nae}, ImageNet-R~\citep{hendrycks2021many}, 
ImageNet-Sketch~\citep{wang2019learning}, ObjectNet~\citep{barbu2019objectnet}, 
and ImageNet-C~\citep{hendrycks2019robustness}.
We measure the top-1 accuracy except for ImageNet-C. On ImageNet-C we measure 
the mean corruption error (mCE) and report 100 minus {mCE}.

\cref{tab:imagenetv2_accuracy} shows that models trained using ImageNet+ 
dataset are up to 10\% more robust models.  Overall, these robustness results 
in conjunction with results in
\cref{tab:long_train_effect} highlight the effectiveness of the proposed 
dataset.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\begin{table*}[t!]
    \centering
    \resizebox{0.95\textwidth}{!}{
        \begin{tabular}{llcccccccc}
            \toprule[1.5pt]
            \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Dataset}} 
            & \multicolumn{3}{c}{\textbf{ImageNet-V2}}
            & \multirow{2}{*}{\textbf{ImageNet-A}} 
            & \multirow{2}{*}{\textbf{ImageNet-R}} 
            & \multirow{2}{*}{\textbf{ImageNet-Sketch}} 
            & \multirow{2}{*}{\textbf{ObjectNet}} 
            & \multirow{2}{*}{\textbf{ImageNet-C}}\\
            \cmidrule[1.25pt]{3-5}
             & & V2-A & V2-B & V2-C\\
             \midrule[1.25pt]
             \multirow{2}{*}{MobileNetV3-Large} & ImageNet & 71.5 & 62.9 & 76.8 
             & 4.5 & 32.4 & 20.6 & 32.8 & 21.8 \\
             & ImageNet+ (Ours) & \textbf{75.1} & \textbf{66.3} 
             &  \textbf{80.5} & \textbf{7.6} & \textbf{42.0} & \textbf{29.0} 
             & \textbf{38.1} & \textbf{32.0} \\
             \midrule
             \multirow{2}{*}{ResNet-50} & ImageNet & 76.3 &  67.4 & 81.3
             & 11.9 & 38.1 & 27.4 & 41.6 & 33.2 \\
             & ImageNet+ (Ours) & \textbf{79.3} & \textbf{71.3} 
             & \textbf{83.8}
             & \textbf{15.1} & \textbf{48.1} & \textbf{34.9} & \textbf{46.8} 
             & \textbf{39.0} \\
             \midrule
             \multirow{2}{*}{SwinTransformer-Tiny} & ImageNet & 77.0 & 69.3 
             &  81.6 & 21.0 & 37.7 & 25.4 & 40.5 & 36.9 \\
              & ImageNet+ (Ours) & \textbf{81.5} & \textbf{74.1} 
              & \textbf{85.3} & \textbf{30.2} & \textbf{58.0} & \textbf{40.8} 
              & \textbf{50.6} & \textbf{46.6} \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{\textbf{ImageNet+ models are up to 10\% more robust on ImageNet 
    distribution shifts}. All models are trained for 
    1000 epochs.  We report on ImageNetV2 variations Threshold-0.7 (V2-A), 
         Matched-Frequency (V2-B), and Top-Images (V2-C). We report accuracy on 
         all datasets except for ImageNet-C. For ImageNet-C we report 100 minus 
         mCE metric.}
    \label{tab:imagenetv2_accuracy}
    \vspace{-4mm}
    %
\end{table*}




%

%

%
%

\begin{table}[b!]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{llccc}
            \toprule[1.5pt]
            \multirow{2}{*}{\textbf{Model}} 
            & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Pretraining dataset}}} 
            & \multicolumn{3}{c}{\textbf{Fine-tuning dataset}}  \\
            \cmidrule[1.25pt]{3-5}
            & & CIFAR-100 & Flowers-102 & Food-101\\
             \midrule[1.25pt]
             \multirow{2}{*}{MobileNetV3-Large}
             & ImageNet &  84.4 & 92.5 & 86.1  \\
             & ImageNet+ (Ours) &  \textbf{86.0} & \textbf{93.7}  
             & \textbf{86.6}  \\
             \midrule
             %
             %
             %
             %
             %
             %
             %
             \multirow{2}{*}{ResNet-50}
             & ImageNet & 88.4 &  93.6 & 90.0 \\
             & ImageNet+ (Ours) & \textbf{88.8} & \textbf{95.0} & \textbf{90.5} 
             \\
             \midrule
             \multirow{2}{*}{SwinTransformer-Tiny}
             & ImageNet & 90.6 & 96.3 &  92.3 \\
             & ImageNet+ (Ours) & \textbf{90.9} & \textbf{96.6} & \textbf{93.0} 
             \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{\textbf{Transfer learning for fine-grained object classification.} 
    Only pretraining dataset is reinforced and fine-tuning datasets are not 
    reinforced.  Reinforced pretraining/fine-tuning results in 
    \cref{tab:teaser_results}.}
    \label{tab:transfer_cls}
    \vspace{-4mm}
\end{table}

\subsection{Calibration: Why are ImageNet+ models robust and transferable?}
\label{sec:calibration}
To understand why ImageNet+ models are significantly more robust than ImageNet 
models we evaluate their Expected Calibration Error 
(ECE)~\citep{kumar2019verified} on the validation set.  
\cref{fig:val_calib_error} shows that ImageNet+ models are well-calibrated and 
significantly better than ImageNet models. This matches recent observations 
about ensembles that out-of-distribution robustness is better for 
well-calibrated models~\citep{kumar2022calibrated}. Full calibration results are presented in 
\cref{sec:val_calib_error_full}.

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/figs_calib_plus/val_calib_error.pdf}
       \vspace{-4mm}
    \caption{\textbf{ImageNet+ models are well-calibrated}.
    We plot the Expected Calibration Error (ECE) on the ImageNet validation set 
    over the validation error (normalized by 100 to range $[0, 1]$) for 
    MobileNetV3/ResNet-50/Swin-Tiny architectures trained for 300 and 1000 
    epochs on ImageNet and ImageNet+.  ImageNet+ models are significantly more 
    calibrated, even matching or better than their teacher (IG-ResNext 
    Ensemble). We also observe that the IG-ResNext model is one of the best 
    calibrated models on the validation set from our pool of teachers.
    }
    \label{fig:val_calib_error}
    %
\end{figure}


