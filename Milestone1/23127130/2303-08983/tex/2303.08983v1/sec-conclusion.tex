\section{Conclusion}\label{sec:conclusion}
%
%

%
%
%
%
%
%
%


We go beyond the conventional online knowledge distillation and introduce 
Dataset Reinforcement (DR) as a general offline strategy. We re-brand data 
augmentation and knowledge distillation as two exemplary tools for dataset 
reinforcement, exploit their advantages, comprehensively study the factors that 
make a good teacher, and investigate a variety of augmentations in knowledge 
distillation. Our investigation unwraps tradeoffs in finding generalizable 
reinforcements controlled by the difficulty of augmentations and we propose 
ways to balance.
The proposed DR strategy is only an example of the large category of ideas 
possible within the scope of dataset reinforcement.  Our desiderata would also 
be satisfied by methods that further expand the training data, especially in 
limited data domains, using strong generative foundation models.

%
%
%

%
%
%
%

\vspace{-4mm}
\paragraph{Limitations.}
Limitations of the teacher can potentially transfer through dataset 
reinforcement.
For example, over-confident biased teachers should not be used and diverse 
ensembles are preferred.
Human verification of the reinforcements is also a solution.
Note that original labels are unmodified in reinforced datasets and can be used
in curriculums.
Our robustness and transfer learning evaluations consistently show better 
transfer and generalization for ImageNet+ models likely because of lower bias 
of the teacher ensemble trained on diverse data.
Another direction is to use more accurate teachers trained on
large multi-modal data, e.g., CLIP (future work).

%
%
%
%
%


