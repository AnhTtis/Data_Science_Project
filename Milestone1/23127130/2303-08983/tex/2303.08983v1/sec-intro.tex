\section{Introduction}\label{sec:intro}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/figs_epochs/imagenet_RRC+RARE_accuracy_annotated.pdf}
       \vspace{-4mm}
    \caption{\textbf{Reinforced ImageNet, ImageNet+, improves accuracy at 
    similar iterations/wall-clock.}  ImageNet validation accuracy of ResNet-50 
    is shown as a function of training duration with (1) ImageNet dataset, (2) 
    knowledge distillation (KD), and (3) ImageNet+ dataset (ours).  Each point 
    is a full training with epochs varying from 50-1000.
    %
    An epoch has the same number of iterations for ImageNet/ImageNet+.
    %
    }
    \label{fig:wall_clock}
    \vspace{-4mm}
    %
\end{figure}
\begin{table}[t]
    \centering
    \resizebox{1.0\columnwidth}{!}{
        %
        %
        %
        %
        %
        %
        %
        %
        %
        %
        %
        %
        %
        %
        %
        %
        %
        \begin{tabular}{lcccccc}
            \toprule[1.5pt]
            \multirow{2}{*}{\textbf{Model}}
            & \textbf{+Data}
            & \textbf{+Reinforced}
            & \multirow{2}{*}{\textbf{ImageNet}} 
            & \multirow{2}{*}{\textbf{CIFAR-100}} 
            & \multirow{2}{*}{\textbf{Flowers-102}} 
            & \multirow{2}{*}{\textbf{Food-101}} \\
            & \textbf{Augmentation}
            & \textbf{Dataset(s)}\\
             \midrule[1.25pt]
             \multirow{2}{*}{MobileNetV3-Large}
             & \xmark & \xmark & 75.8 & 84.4 & 92.5 & 86.1\\
             & \xmark & \cmark & \textbf{77.9} & \textbf{87.5} & \textbf{95.3} & \textbf{89.5}\\
             \midrule
             \multirow{4}{*}{ResNet-50}
             %
             %
             & RandAugment & \xmark & 80.4 & 88.4 & 93.6 & 90.0\\
             & AutoAugment & \xmark & 80.2 & 87.9 & 95.1 & 89.0\\
             & TrivialAugWide & \xmark & 80.4 & 87.9 & 94.8 & 89.3\\
             & \xmark & \cmark & \textbf{82.0} & \textbf{89.8} & \textbf{96.3} & \textbf{92.1}\\
             \midrule
             \multirow{2}{*}{SwinTransformer-Tiny}
             & RandAugment & \xmark & 81.3 & 90.7 & 96.3 & 92.3 \\
             & \xmark & \cmark & \textbf{84.0} & \textbf{91.2} & \textbf{97.0} & \textbf{92.9}\\
            \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{\textbf{Training/fine-tuning on reinforced datasets improve 
    accuracy for a variety of architectures.} We reinforce each dataset 
    \emph{once} and train multiple models with similar cost as training on the 
    original dataset.
    For datasets other than ImageNet, we fine-tune ImageNet/ImageNet+ 
    pretrained models.
    %
    %
    Dataset reinforcement significantly benefits from efficiently reusing the 
    knowledge of a teacher.
         %
%
    %
%
    %
         }
    \label{tab:teaser_results}
    \vspace{-5mm}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/DR_illustration_wide}
       \vspace{-8mm}
    \caption{\textbf{Illustration of Dataset Reinforcement.}
    Data augmentation and knowledge distillation are common approaches to 
    improving accuracy.
    Dataset reinforcement combines the benefits of both by bringing
    the advantages of large models trained on large datasets to other datasets 
    and models.
    Training of new models with a reinforced dataset is as fast as 
    training on the original dataset for the same total iterations.
    Creating a reinforced dataset is a one-time process (e.g., ImageNet to ImageNet+)
    the cost of which is amortized over repeated uses.
    }\label{fig:illustration_wide}
    \vspace{-4mm}
\end{figure*}




With the advent of the CLIP~\citep{radford2021learning}, the machine 
learning community got increasingly interested in massive datasets whereby 
the models are trained on hundreds of millions of samples
which is orders of magnitude larger than the conventional 
ImageNet~\citep{deng2009imagenet} with 1.2M samples.
%
At the same time, models have gradually grown larger in multiple domains~\citep{alabdulmohsin2022revisiting}.
In computer vision, the state-of-the-art models
have upwards of $300$M parameters according to the 
{Timm}~\citep{rw2019timm} library
(e.g., {BEiT}~\citep{beit}, {DeiT III}~\citep{Touvron2022DeiTIR}, 
{ConvNeXt}~\citep{liu2022convnet}) and process inputs at up to $800\times 800$ 
resolution (e.g., {EfficientNet-L2-NS}~\citep{xie2020self}).
Recent multi-modal vision-language models have up to 1.9B parameters (e.g., 
{BeiT-3}~\citep{wang2022image}).

%
%
%
%
%
%
%
%
%

%
%
%
%

On the other side, there is a significant demand for small models that satisfy 
stringent hardware requirements.
Additionally, there are plenty of tasks with small datasets that are 
challenging to scale because cost of collecting and annotating new data.
We seek to bridge this gap and bring the benefits of large models to any large, 
medium, or small-scale dataset.
We use knowledge from large 
models~\citep{radford2021learning,dosovitskiy2020image,bommasani2021opportunities} 
to enhance the training of new models.
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%

In this paper, we introduce {\it Dataset Reinforcement (DR)\/} as a strategy 
that improves the accuracy of models through reinforcing the training dataset.  
Compared to the original training data, a method for dataset reinforcement 
should satisfy the following desiderata:
\begin{itemize}[leftmargin=*]
%
\itemsep0em
\item {\bf No overhead for users}: Minimal increase in the computational cost 
    of training a new model for similar total iterations (e.g., similar 
    wall-clock time and CPU/GPU utilization).
\item {\bf Minimal changes in user code and model}: Zero or minimal 
    modification to the training code and model architecture for the users of 
    the reinforced dataset (e.g., only the dataset path and the data loader need to 
    change).
\item {\bf Architecture independence}: Improve the test accuracy across variety of model architectures.
%
    %
\end{itemize}

To understand the importance of the DR desiderata, let us discuss two common 
methods for performance improvements: data augmentation and knowledge 
distillation. Illustration in \cref{fig:illustration_wide} compares these 
methods and our strategy for dataset reinforcement.

Data augmentation is crucial to the improved performance of machine learning 
models. Many state-of-the-art vision models~\citep{he2016deep, 
huang2017densely,howard2019searching} use the standard Inception-style 
augmentation \cite{szegedy2015going} (i.e., random resized crop and random 
horizontal flipping) for training.
In addition to these standard augmentation methods, recent models~\citep{touvron2021training,liu2021swin} also incorporate mixing augmentations (e.g., MixUp~\citep{zhang2017mixup} and 
CutMix~\citep{yun2019cutmix}) and automatic augmentation methods (e.g., RandAugment~\citep{cubuk2020randaugment} and
AutoAugment~\citep{cubuk2018autoaugment}) to generate new data. However, data 
augmentation fails to satisfy all the desiderata as it does not provide 
architecture independent generalization. For example, light-weight CNNs perform 
best with standard Inception-style augmentations~\citep{howard2019searching} 
while vision transformers~\citep{touvron2021training,liu2021swin} prefer 
a combination of standard as well as advanced augmentation methods.


Knowledge distillation (KD) refers to the training of a student model by 
matching the output of a teacher model~\citep{hinton2015distilling}. KD has 
consistently been shown to improve the accuracy of new models independent of 
their architecture significantly more than data 
augmentations~\citep{touvron2021training}.
However, knowledge distillation is expensive as it requires performing the 
inference (forward-pass) of an often significantly large teacher model at every 
training iteration.
KD also requires modifying the training code to perform two forward passes on 
both the teacher and the student. As such, KD fails to satisfy minimal overhead 
and code change desiderata.

%
%
%
%
%


%
%
%
%
%

This paper proposes a dataset reinforcement strategy that exploits the 
advantages of both knowledge distillation and data augmentation by removing the 
training overhead of KD and finding generalizable data augmentations.
Specifically, we introduce the \emph{ImageNet+} dataset that provides 
a balanced trade-off between accuracies on a variety of models and has the same 
wall-clock as training on ImageNet for the same number of iterations 
(\cref{fig:wall_clock,tab:teaser_results}).
To train models using the ImageNet+ dataset, one only needs to change a few 
lines of the user code to use a modified data loader that reinforces every 
sample loaded from the training set.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\vspace{-3mm}
\paragraph{Summary of contributions:}
\begin{itemize}[leftmargin=*]
        \vspace{-2mm}
    \itemsep0em
    \item We present a comprehensive large scale study of knowledge 
        distillation from 80 pretrained state-of-the-art models and their 
        ensembles.
        We observe that ensembles of state-of-the-art models 
        trained on massive datasets generalize across student architectures 
        (\cref{sec:good_teacher}).
    \item We reinforce ImageNet by efficiently storing the knowledge of a strong 
        teacher on a variety of augmentations. We investigate the generalizability 
        of various augmentations for dataset reinforcement and find a tradeoff 
        controlled by the reinforcement difficulty and model complexity 
        (\cref{sec:aug_difficulty}). This tradeoff can further be alleviated 
        using curriculums based on the reinforcements (\cref{sec:curriculum}).
    \item We introduce ImageNet+, a reinforced version of ImageNet, that 
        provides 1-4\% improvement in accuracy for a variety of architectures 
        in short as well as long training.
        We show that ImageNet+ pretrained models result in 0.6-0.8 improvements in 
        mAP for detection on MS-COCO and 
        0.3-1.3\% improvement in mIoU for segmentation on ADE-20K 
        (\cref{sec:transfer}).
    \item
        Similarly, we create CIFAR-100+, Flowers-102+, and Food-101+ and 
        demonstrate their effectiveness for fine-tuning 
        (\cref{sec:other_datasets}).
        ImageNet+ pretrained models fine-tuned on CIFAR-100+, Flowers-102+, and 
        Food-101+ show up to 3\% improvement in transfer learning on CIFAR-100, 
        Flowers-102, and Food-101.
    \item To further investigate this emergent transferablity we study 
        robustness and calibration of the ImageNet+ trained models. They reach 
        up to 10\% improvement on a variety of OOD datasets,  ImageNet-(V2, A, 
        R, C, Sketch), and ObjectNet (\cref{sec:robustnes}).  We also show that 
        models trained on ImageNet+ are well calibrated compared to their 
        non-reinforced alternatives (\cref{sec:calibration}).
\end{itemize}
%

We will publish ImageNet+, CIFAR-100+, Flowers-102+, and Food-101+ datasets, 
and open source the code to reinforce new datasets.

%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
