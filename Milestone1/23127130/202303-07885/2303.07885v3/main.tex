\documentclass[journal,twoside,web]{ieeecolor}
%\overrideIEEEmargins
\let\labelindent\relax 
\let\proof\relax 
\let\endproof\relax 
%
\usepackage{lcsys}
\usepackage{cite}

%
\usepackage{amsmath,amssymb,amsthm,graphics,epsfig,times,enumitem,float,booktabs}
\usepackage{hyperref,stmaryrd,comment,filecontents,bm} 
\hypersetup{colorlinks=true, linkcolor=blue, filecolor=blue, urlcolor=blue,citecolor=blue}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage[dvipsnames]{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup{font=small}
\captionsetup[sub]{font=scriptsize,labelfont=small}
%\captionsetup{belowskip=-7pt}
%\setlength{\abovedisplayskip}{-1pt}
%\setlength{\belowdisplayskip}{4.8pt}

%
\usepackage{times,mathptm}
\usepackage{tikz,tikz-3dplot}
\usetikzlibrary{arrows,shapes,calc}
\usetikzlibrary{calc,intersections,through,backgrounds}
\usetikzlibrary{positioning}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepackage{pgfplotstable}
\usepgfplotslibrary{units}
%  
%% THEOREM, LEMMA ....
\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem Statement}
\newtheorem*{problem*}{Problem Statement}
\newtheorem{assumption}{Assumption}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example} 
%  
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\tb}[1]{\textcolor{black}{#1}}
\newcommand{\diag}[1]{\textrm{diag}\{#1\}}
\newcommand{\col}[1]{\textrm{col}\{#1\}}
\newcommand{\row}[1]{\textrm{row}\{#1\}}
\newcommand{\idm}[1]{\mathbf{I}_{#1}}
\newcommand{\zdm}[1]{\mathbf{0}_{#1}}
\newcommand{\odm}[1]{\mathbf{1}_{#1}} 
\newcommand{\cufrac}[2]{\tfrac{\partial #1}{\partial #2}}
\newcommand{\PVRcomment}[1]{\textcolor{black}{#1}}
%
\DeclareMathOperator*{\argmin}{arg\,min}
\renewcommand{\qedsymbol}{$\blacksquare$}
\newcommand{\PVR}[1]{\textcolor{black}{#1}}
\newcommand{\AG}[1]{\textcolor{black}{#1}}
% 
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{ }{ }

% Symbol Definitions
\def\B{{\mathbb B}}
\def\w{{\widetilde w}}
\def\t{\tau}
\def \tb {\textcolor{blue}}
\def \tr {\textcolor{red}}
\def\inpr#1{\left\langle #1\right\rangle}
\def\Bf{\mathbf}
\def\x{\mathbf{x}}
\def\z{\mathbf{z}}
\def\B{\mathit{B}}
\def\Bs{\mathsf B}
\def\mR{\mathsf R}
\def\R{{\mathbb R}}
\def\u{\mathbf u}
\def\v{\mathbf v}
\def\H{\mathit H}
\def\C{\mathcal C}
\def\T{\mathsf T}
\def\A{\mathbf A}
\def\V{\mathcal V}

\begin{document}
	\title{Optimal Role Assignment for Multiplayer Reach-Avoid Differential Games in 3D Space}
	\author{Abinash Agasti, Puduru Viswanadha Reddy, \IEEEmembership{Member, IEEE}, Bharath Bhikkaji
		\thanks{A. Agasti, P. V. Reddy and B. Bhikkaji are with the Department of Electrical Engineering, Indian Institute of Technology-Madras, Chennai, 600036, India.  
			(e-mail: abinashagasti@outlook.com, vishwa@ee.iitm.ac.in, bharath@ee.iitm.ac.in)}} 
	
	\maketitle
	\thispagestyle{empty}
	\begin{abstract} 
		In this article an $n$-pursuer versus $m$-evader reach-avoid differential game in 3D space is studied. A team of evaders aim to \textit{reach} a stationary target while \textit{avoiding} capture by a team of pursuers. The multiplayer scenario is formulated in a differential game framework. This article provides an optimal solution for the particular case of $n=m=1$ and extends it to a more general scenario of $n\geq m$ via an optimal role assignment algorithm based on a linear program. Consequently, the barrier surface and the Value of the game are analytically characterized providing optimal strategies of the players in state feedback form.
	
	\end{abstract}
	
	\begin{IEEEkeywords}Reach-Avoid Differential Games; Optimal Control; Autonomous Systems
	\end{IEEEkeywords}

	\section{Introduction}   Multiplayer reach-avoid differential games (MRADG) are mathematical abstractions for interactions in which a team of evaders strives to reach a predetermined goal while avoiding an adversarial team of pursuers. Such interactions arise in various real-world scenarios, including safe motion/path planning, region protection in the presence of hostile agents, dynamic collision avoidance and robotic herding \cite{tomlin_2015,karaman_frazzoli_2011,oyler_2016,paranjape_2018}.
	
	Two player reach-avoid games involving one pursuer and one evader are commonly solved using a differential game framework \cite{isaacs_1965}. This approach involves solving Game of Kind to obtain a reach-avoid set, and then solving the Hamilton-Jacobi-Isaacs partial differential equation (HJI-PDE) to determine the optimal strategies in state feedback form. However, using this approach is challenging to solve an MRADG, which requires determining optimal strategies in state feedback form and optimal assignments of pursuers to evaders. This results in a hybrid decision problem that involves both discrete and continuous variables, and the complexity of this problem scales exponentially as the number of players in the game increases, making it practically infeasible in real-world settings.
	
	In the recent years, within the differential games literature, there have been attempts \cite{tomlin_2017,tomlin_2018,yan_assignment_2020,yan_matching_2022,garcia_2021} to address the hybrid nature of the problem arising in MRADGs. In \cite{tomlin_2017}, the authors design computationally efficient algorithms by approximating the two player  solution and using a maximum matching strategy for assignment. Zhou et al. in \cite{tomlin_2018} aim to reduce the computational burden through approximated solutions of the HJI-PDE. In \cite{yan_assignment_2020}, the authors analytically characterize the barrier but do not provide optimal strategies.  In \cite{garcia_2021}, Garcia et al. obtain optimal strategies in feedback form and provide a complete analytical solution to the game. However, determining the optimal assignment involves enumerating all feasible assignments relevant to the game, resulting in an exponential increase in complexity with the number of players. All the above mentioned works study MRADGs where players interact in 2D space.	In \cite{yan_matching_2022}, the authors analyze an MRADG in 3D space and present an analytical solution for a subgame with multiple pursuers and a single evader. They then use this solution as a building block to develop a polynomial-time approximation for an NP-hard assignment algorithm.  
	
	This letter presents a closed-form optimal solution to a class of MRADGs involving $m$ evaders and $n$ pursuers interacting in 3D space. Our work is motivated by military and surveillance applications that require aerial swarms to operate in adversarial environments \cite{Chung:18}. By allowing for unequal player speeds and the possibility of a subset of evaders being faster than a subset of pursuers, we create a more realistic scenario. In the two-player case ($m=n=1$), we solve the Game of Kind and Game of Degree, thereby generalizing the work of \cite{garcia_2020}, where players were assumed to have equal speeds in a similar scenario. For the MRADG ($n \geq m\geq 1$), we provide a linear programming (LP) based optimal assignment of pursuers to evaders by assigning no more than one pursuer to an evader. We note, searching through this restricted space of assignments is of factorial time complexity. 
	The novelty of our approach lies in providing a cost-benefit framework for obtaining the optimal assignments. 
	%\textcolor{red}{Our solution is optimal in the restricted space of assignments enumerating whose possibilities still results in a factorial time complexity.} 
	We derive the Value function that satisfies the HJI-PDE to obtain the  optimal strategies of the players in state feedback form.
	
	\begin{comment} 
	The contribution of this letter is to provide an optimal solution to an MRADG that involves $m$ evaders and $n$ pursuers interacting in a 3D space. Our work is motivated by military and surveillance applications  involving aerial swarms operating adversarial environments \cite{Chung:18}. We allow for unequal player speeds and the possibility of a subset of evaders being faster than a subset of pursuers, creating a more realistic scenario. For the two-player case ($m=n=1$), we solve the games of kind and degree, extending the work of \cite{garcia_2020}, where players were assumed to have equal speeds in a similar scenario. For the MRADG ($n \geq m\geq 1$), by imposing certain restrictions on the possible assignments of pursuers to evaders, we provide a linear programming (LP) based optimal assignment of pursuers to evaders. The novelty of our work lies in providing a cost-benefit framework for obtaining the optimal assignment. Moreover, we perform sensitivity analysis of the perceived cost to a pursuer for failing to capture an evader on the outcome of the game. Within the restricted space of assignments, our solution is optimal and not an approximation. Further, we derive the Value function that satisfies the HJI-PDE to obtain the optimal strategies of the players in state feedback form.
	\end{comment}
	
	The letter is organized as follows. The problem formulation of the MRADG is stated in Section \ref{sec:prelim}. Section \ref{sec:1v1} considers the particular case of one pursuer against one evader. Using this result, an assignment technique is proposed in Section \ref{sec:multiplayer} solve the MRADG analytically. Section \ref{sec:num} provides a few numerical illustrations, and finally conclusions are drawn in Section \ref{sec:conc} along with citing some future directions.
	
	
	%%%
	%%%
	%%%
	\section{Preliminaries and Problem Formulation}
	\label{sec:prelim}
	We consider a multi-player reach-avoid differential game  (MRADG) consisting of $n$ pursuers and $m$ evaders. A player in the evading team is denoted by $E_i$, $i\in M:=\{1,\cdots,m\}$, and one in the pursuing team by $P_j$, $j\in N:=\{1,\cdots,n\}$. The players are assumed to be holonomic, and interact in  3-dimensional Euclidean space. The evaders aim to reach a stationary target, while the pursuers desire to prevent this outcome. We assume (without loss of generality) that  the target is located at the origin. The position vector or state of $E_i$ and $P_j$ are denoted respectively by $\x_{E_i} :=(x_{E_i} ,y_{E_i} ,z_{E_i} )\in \mathbb{R}^3$ and $\x_{P_j} :=(x_{P_j} ,y_{P_j} ,z_{P_j} )\in \mathbb{R}^3$  for   $i\in M$  and $j\in N$. The global state space of the differential game is denoted by $\x:=(\x_E,\x_P)\in \mathbb R^{3(m+n)}$, where $\x_E=\{\x_{E_i}:i\in M\}$ and $\x_P=\{\x_{P_j}:j\in N\}$.  We denote the controls of the players $E_i$ and $P_j$ respectively by $\u_{E_i}=(u_{x_i},u_{y_i},u_{z_i})$  and $\v_{P_j}=(v_{x_j},v_{y_j},v_{z_j})$  for $i\in M$ and $j\in N$. The joint team controls for the evaders and pursuers are denoted by $\u:=\{\u_{E_i}:i\in M\}$ and $\v:=\{\v_{P_j}:j\in N\}$ respectively. We assume that players $E_i$ and $P_j$ move with constant speeds $U_i> 0$ and $V_j> 0$ respectively. There is no further assumption on the speeds of the players, and hence, the speed ratios $\alpha_{ij}=U_i/V_j$ can take any positive real value for all $i\in M$ and $j\in N$. Consequently, the admissible control sets of $E_i$ and $P_j$  are given respectively by $\{\u_{E_i}\in \mathbb R^3:||\u_{E_i}|| =U_i \}$ and $\{\v_{P_j}\in \mathbb R^3:||\v_{P_j}|| =V_j \}$, where $||.||$ denotes the Euclidean norm. The players have simple motion dynamics given by
	% 
	\begin{equation}
	\begin{aligned}
		&\dot{x}_{E_i}(t) =u_{x_i}(t),~	\dot{y}_{E_i}(t)=u_{y_i}(t), ~	\dot{z}_{E_i}(t)=u_{z_i}(t), \\
		&\dot{x}_{P_j}(t)=v_{x_j}(t),~ \dot{y}_{P_j}(t)=v_{y_j}(t),~\dot{z}_{P_j}(t)=v_{z_j}(t)
		\label{eq:multi_dynamics}
	\end{aligned} 
	\end{equation}
	with  initial positions  
	$\x_{E_i}(0)=(x_{E_{i_0}},y_{E_{i_0}},z_{E_{i_0}})$ and    $\x_{P_j}(0)=(x_{P_{j_0}},y_{P_{j_0}},z_{P_{j_0}})$ for $i\in M$ and $j\in N$.  The global initial state of the system is denoted by $\x_0\in \mathbb R^{3(m+n)}$.
	 \par
	The MRADG considered in this paper is characterized by two termination criteria: either all the evaders are captured by the pursuer team or at least one of the evaders reaches the origin. We assume point capture, that is, an evader is said to have been captured when its state vector coincides with that of a pursuer, and an evader is said to have reached the target when its state vector coincides with the origin. After capturing an evader, the speed of the pursuer drops to zero, and both the pursuer and the evader cease to remain active in the game. To represent the terminal criteria, we first define binary variables $\{\mu_{ij}\in \{0,1\},~i\in M,~j\in N\}$, where $\mu_{ij}=1$ when $P_j$ is assigned to capture $E_i$, and $0$ otherwise. 
	
		
 	Consequently, the termination set for the game is given by
	\begin{equation}
		\T=\T_E\cup\T_P, \label{eq:termination}
	\end{equation}
	where
	\begin{equation}
		\begin{aligned}
			\T_E=\{ \x\in\R^{3(m+n)}~\big|~\exists i\in M\ \text{ such that  }
			\vert\vert\x_{E_i} \vert\vert=0\},
		\end{aligned}
	\end{equation}
	represents the game outcome when  atleast one of the evaders reaches the target, and
	\begin{equation}
		\begin{aligned}
			\T_P=\{ \x\in\R^{3(m+n)}~\big|~\forall i\in M\ \exists j\in N, 	\text{ such that}& \\
			\mu_{ij}=1 \text{ and }	\vert\vert \x_{P_j}-\x_{E_i} \vert\vert=0&\},
		\end{aligned}
	\end{equation}
	represents the outcome when all the evaders are captured by the pursuer team. Further, the associated termination time is calculated as $t_f:=\inf\{t\in\R^+: \x(t)\in\T\}$.  As there are two outcomes in MRADG,  
	Game of Kind needs to be solved to partition the global state space into winning regions for the pursuer  and evader teams. Let $\B:\R^{3(m+n)}\shortrightarrow\R$ denote the barrier function whose zero level set constructs the barrier surface of the MRADG given by
	\begin{equation}
		\Bs(\x):=\{\x~|~\B(\x)=0\}, \label{eq:barrier_surface}
	\end{equation}
	which  partitions $\R^{3(m+n)}$ into the following two sets
	\begin{equation}
		\mR_P:=\{\x ~|~\B(\x)>0 \}, \quad  \mR_E:=\{\x ~|~\B(\x)<0\}, \label{eq:partition_sets}
	\end{equation}
	where $\mR_P$ and $\mR_E$ denote winning regions for the pursuer and the evader teams respectively. The optimal strategies of the players within their winning regions are obtained by solving the Game of Degree (GoD). Let the subscript $f$ denote a quantity at terminal time. Starting in the region $\mR_E$, consider the cost function given by 
	\begin{equation}
		J(\u(.),\v(.);\x_0)=-\sum\limits_{i\in\tilde{M}}\min\limits_{j\in \tilde{N}_i}\vert\vert\x_{P_{j_f}}\vert\vert, \label{eq:multi_Ecost}
	\end{equation}
	where $\tilde{M}=\{i\in M:\vert\vert \x_{E_{i_f}}\vert\vert =0\}$, $\tilde{N}_i=\{j\in N:\mu_{ij}=1\}$. 
		This cost accumulates the individual costs corresponding to all the evaders $E_i$ reaching the target at the terminal time. 
		The individual cost is in turn given by the closest distance of all pursuers assigned to $E_i$, from the target. The negative sign conventionally allows the pursuers to be the maximizing team for the cost in \eqref{eq:multi_Ecost}. This convention is also applied in the cost considered when starting in the region $\mR_P$, given by
 
	\begin{equation}
		J(\u(.),\v(.);\x_0)=\sum\limits_{i\in M}\vert\vert\x_{E_{i_f}}\vert\vert. \label{eq:multi_Pcost}
	\end{equation}
	 This cost function again represents an accumulation of individual costs corresponding to the terminal distance of each evader from the target.
	The optimal payoff in this game, referred to as the Value of the game, is defined as
	\begin{equation}
		\V(\x_0):=\min\limits_{\u(.)}\max\limits_{\v(.)}J(\u(.),\v(.);\x_0) \label{eq:valfun}
	\end{equation} 
	subject to \eqref{eq:multi_dynamics} and \eqref{eq:termination}, where $\u(.)$ and $\v(.)$ are the teams' state feedback strategies. 
	
	\begin{problem*}
Clearly, the solution of an MRADG consists of two steps. The first step involves an assignment problem, which is combinatorial in nature, and the second step involves computation of the barrier function and the Value of the game. In this paper, we seek to solve this co-design problem through a computationally efficient assignment scheme. Then, we aim to provide an analytical characterization of the barrier function and the Value function, to obtain optimal strategies of the players in feedback form.
		\end{problem*}
	%
	
	We state the following preliminary result which will be used throughout the reminder of the paper.
		%
		\begin{theorem}
			Consider the MRADG described by  \eqref{eq:multi_dynamics},    \eqref{eq:multi_Ecost} and \eqref{eq:multi_Pcost}. The optimal headings are constant and the optimal trajectories are straight lines. \label{theorem:prelim}
		\end{theorem}
		%
		\begin{proof}
			%The proof follows from the simple motion dynamics \eqref{eq:multi_dynamics} and Mayer type cost functions \eqref{eq:multi_Ecost} and \eqref{eq:multi_Pcost}. For more details, please refer to the online supplementary material \cite{Abinash:23}.
			As the cost functions  \eqref{eq:multi_Ecost} and \eqref{eq:multi_Pcost} are of terminal type, the  Hamiltonian associated with the saddle-point problem in \eqref{eq:valfun}
			is written as 
			\begin{equation}
				\begin{aligned}
					\H &=\sum\limits_{i\in M}\langle \lambda_{E_i},\dot\x_{E_i}\rangle+\sum\limits_{j\in N} \langle\lambda_{P_j},\dot\x_{P_j}\rangle\\
					&=\sum\limits_{i\in M} \langle \lambda_{E_i},\u_i\rangle+\sum\limits_{j\in N} \langle \lambda_{P_j},\v_j\rangle,
				\end{aligned}
			\end{equation}
			where  $\lambda=(\lambda_{E_1},...,\lambda_{E_m},\lambda_{P_1},...,\lambda_{P_n})^T\in\R^{3(m+n)}$ denotes the costate vector. As the Hamiltonian is independent of $\x$   the costate dynamics is obtained as $\dot\lambda=-\frac{\partial}{\partial \x}\H=0$. So, the costate vector remain constant under optimal play. The saddle-point controls must satisfy $\min_{\u}\max_{\v}H=0$ and thus depend on the costates. As the costates are constant, this implies that the optimal controls also remain constant.  Consequently, the optimal trajectories are straight lines. 
	\end{proof}
	%%%%
	% 
	%%%%
	\section{1 versus 1 Differential Game}
	\label{sec:1v1}
In this section, we will analyze a reach-avoid differential game with one pursuer and one evader, referred to as the \texttt{1v1} RADG for brevity. To keep the notation consistent with MRADG, we label the evader and the pursuer as $E_i$ and $P_j$ respectively. The computation of the barrier function involves finding the Apollonius sphere, that is the locus of all the points which can be reached by the players at the same time. The desired locus is given by $\frac{\vert\vert \x-\x_{P_j}\vert\vert^2}{v_{P_j}^2}=\frac{\vert\vert \x-\x_{E_i}\vert\vert^2}{v_{E_i}^2} \Rightarrow \alpha_{ij}^2\vert\vert \x-\x_{P_j}\vert\vert^2=\vert\vert \x-\x_{E_i}\vert\vert^2$.  Upon simplification, the Apollonius sphere is calculated as
		\begin{equation}
			(x-x_{c_{ij}})^2+(y-y_{c_{ij}})^2+(z-z_{c_{ij}})^2=r_{c_{ij}}^2, \label{eq:apsphere_E}
		\end{equation}
		where $x_{c_{ij}}=\frac{x_{E_i}-\alpha_{ij}^2x_{P_j}}{1-\alpha_{ij}^2}$, $y_{c_{ij}}=\frac{y_{E_i}-\alpha_{ij}^2y_{P_j}}{1-\alpha_{ij}^2}$, $z_{c_{ij}}=\frac{z_{E_i}-\alpha_{ij}^2z_{P_j}}{1-\alpha_{ij}^2}$, and $r_{c_{ij}}=\frac{\alpha_{ij}}{1-\alpha_{ij}^2}||\x_{P_j}-\x_{E_i}||$. The next result is concerned with the computation of the barrier function.
		\begin{lemma}
			The   function $\B_{ij}(\x):\R^6\shortrightarrow\R$ defined by 
			\begin{equation}
				\B_{ij}(\x)=R_{E_i}^2-\alpha_{ij}^2R_{P_j}^2, \label{eq:1v1_barrier}
			\end{equation}
			where $R_{E_i}=\vert\vert x_{E_i}\vert\vert$ and $R_{P_j}=\vert\vert x_{P_j}\vert\vert$, 
			is a barrier function for the \texttt{1v1} RADG. 
		\end{lemma}
		\begin{proof}
		If $\B_{ij}(\x)>0$ for $\x\in\R^6$, then $R_{E_i}^2>\alpha_{ij}^2R_{P_j}^2 \Rightarrow R_{E_i}^2/U_i^2>{R_{P_j}^2}/{V_j^2}$. The pursuer can thus reach the origin faster than the evader, hence this condition characterizes the pursuer winning region $\mR_P$. On the other hand, if $\B_{ij}(\x)<0$, then the evader can reach the origin faster than the pursuer, hence this condition characterizes the evader winning region $\mR_E$. If $\B_{ij}(\x)=0$, both players can reach the origin simultaneously. The barrier surface given by \eqref{eq:barrier_surface} characterizes this tie situation.
		\end{proof}
		Using the barrier function \eqref{eq:1v1_barrier}, we provide optimal strategies of the players in their respective winning
		regions in the next two results.  
		\begin{theorem}
			Consider the \texttt{1v1} RADG with $\x\in\mR_P$ and $\alpha_{ij}<1$. The Value function is $\C^1$ and it is the solution of the HJI-PDE. The Value function is given by 
			\begin{equation}
				\V_{ij}(\x)=R_{c_{ij}}-r_{c_{ij}}, \label{eq:value_Pout}
			\end{equation}
			where $R_{c_{ij}}=\vert\vert \x_{c_{ij}} \vert\vert$, with $\x_{c_{ij}}=(x_{c_{ij}},y_{c_{ij}},z_{c_{ij}})$,  %$\sqrt{x_{c_{ij}}^2+y_{c_{ij}}^2+z_{c_{ij}}^2}$, 
			and $x_{c_{ij}},\ y_{c_{ij}},\ z_{c_{ij}}$, and $r_{c_{ij}}$ are given by \eqref{eq:apsphere_E}. \label{theorem:1v1_varspeed}
		\end{theorem}
		\begin{proof}
			Since $\x\in\mR_P$ and $\alpha_{ij}<1$, the evader is located inside the Apollonius sphere with the target lying outside it. As both the players reach the Apollonius sphere at the same time, the evader must choose the point on the sphere that minimizes its distance from the origin, considering the cost specified by \eqref{eq:multi_Pcost}. The coordinates of this point can be determined as the solution of the following  equality constrained optimization problem  
			\begin{equation}
				\begin{aligned}
					\min\ &x^2+y^2+z^2,\\ \label{eq:opt_sphere}
					\text{subject to } (x-x_{c_{ij}}&)^2+(y-y_{c_{ij}})^2+(z-z_{c_{ij}})^2=r_{c_{ij}}^2.
				\end{aligned}
			\end{equation}
			Taking the Lagrange multiplier associated with the equality constraint as $\delta$, the first order necessary condition is obtained as  $
			x+\delta(x-x_{c_{ij}})=0$, $
			y+\delta(y-y_{c_{ij}})=0$, and $ 
			z+\delta(z-z_{c_{ij}})=0$.
			The desired optimal point is obtained as $I=(x^*,y^*,z^*)=\frac{\delta}{1+\delta}(x_{c_{ij}},y_{c_{ij}},z_{c_{ij}})$. Since $I$ must lie on  the Apollonius sphere, it satisfies a quadratic equation given by $(x_{c_{ij}}^2+y_{c_{ij}}^2+z_{c_{ij}}^2)\left(1-\frac{\delta}{1+\delta}\right)^2=r_{c_{ij}}^2$. Solving for $\delta$, we get $1-\tfrac{\delta}{1+\delta}=1\mp\tfrac{r_{c_{ij}}}{R_{c_{ij}}}$. 
			One solution corresponds to the point on the sphere farthest from the origin, while the other solution provides the candidate interception point, given by:
			\begin{equation}
				I=(x^*,y^*,z^*)=\left( 1-\tfrac{r_{c_{ij}}}{R_{c_{ij}}}\right) (x_{c_{ij}},y_{c_{ij}},z_{c_{ij}}).
			\end{equation}
			The payoff of the game when the evader and the pursuer choose to head straight towards the interception point (as a result of Theorem \ref{theorem:prelim}) yields a guess of the Value function given by
			\begin{equation}
				\V(\x)=||I||=\left( 1-\tfrac{r_{c_{ij}}}{R_{c_{ij}}}\right)\sqrt{x_{c_{ij}}^2+y_{c_{ij}}^2+z_{c_{ij}}^2}=R_{c_{ij}}-r_{c_{ij}} \label{eq:value_guess_Pout}
			\end{equation} 
			It is now imperative to verify that this candidate Value function satisfies the HJI equation. The partial derivatives of $\V_{ij}$ can be written as follows: 
			\begin{equation}
				\begin{aligned}
					\begin{bmatrix}\cufrac{\V_{ij}}{x_{E_i}}&\cufrac{\V_{ij}}{y_{E_i}}&\cufrac{\V_{ij}}{z_{E_i}}\end{bmatrix}&=\tfrac{1}{1-\alpha_{ij}^2}\left(\tfrac{\x_{c_{ij}}}{R_{c_{ij}}} -\tfrac{\alpha_{ij}^2}{1-\alpha_{ij}^2}\tfrac{\x_{E_i}-\x_{P_j}}{r_{c_{ij}}}\right), \\
					\begin{bmatrix}\cufrac{\V_{ij}}{x_{P_j}}&\cufrac{\V_{ij}}{y_{P_j}}&\cufrac{\V_{ij}}{z_{P_j}}\end{bmatrix}&=\tfrac{\alpha_{ij}^2}{1-\alpha_{ij}^2}\left(-\tfrac{\x_{c_{ij}}}{R_{c_{ij}}} +\tfrac{1}{1-\alpha_{ij}^2}\tfrac{\x_{E_i}-\x_{P_j}}{r_{c_{ij}}}\right).
				\end{aligned}
				\label{eq:gradV_compact}
			\end{equation}
			%
			The HJI equation associated with the saddle-point problem \eqref{eq:valfun} is given by
			%
			\begin{multline}
				\min\limits_{\u_i}\max\limits_{\v_j}\left(\cufrac{\V_{ij}}{x_{E_i}} u_{x_i}+\cufrac{\V_{ij}}{y_{E_i}}u_{y_i}+\cufrac{\V_{ij}}{z_{E_i}}u_{z_i}\right. \\
				\left. +\cufrac{\V_{ij}}{x_{P_j}}v_{x_j}+\cufrac{\V_{ij}}{y_{P_j}}v_{y_j}+\cufrac{\V_{ij}}{z_{P_j}}v_{z_j}\right)=0. \label{eq:ME1}
			\end{multline}
			%
			Then, the optimal controls are obtained in feedback form as
			\begin{equation}
				\begin{aligned}
					\u_i^*&=-\tfrac{U_i}{\rho_{E_i}}\begin{bmatrix}
						\cufrac{\V_{ij}}{x_{E_i}}&\cufrac{\V_{ij}}{y_{E_i}}&\cufrac{\V_{ij}}{z_{E_i}}
					\end{bmatrix},\\  \v_j^*&=\tfrac{V_j}{\rho_{P_j}}\begin{bmatrix}
						\cufrac{\V_{ij}}{x_{P_j}}&\cufrac{\V_{ij}}{y_{P_j}}& \cufrac{\V_{ij}}{z_{P_j}}
					\end{bmatrix},\label{eq:opt_con}
				\end{aligned}
			\end{equation}
			where $\rho_{E_i}=\sqrt{\cufrac{\V_{ij}}{x_{E_i}}^2+\cufrac{\V_{ij}}{y_{E_i}}^2+\cufrac{\V_{ij}}{z_{E_i}}^2}$ and $\rho_{P_j}=\sqrt{\cufrac{\V_{ij}}{x_{P_j}}^2+\cufrac{\V_{ij}}{y_{P_j}}^2+\cufrac{\V_{ij}}{z_{P_j}}^2}$. Substituting the optimal controls \eqref{eq:opt_con} in the HJI equation \eqref{eq:ME1}, we get
			\begin{equation}
				-\alpha_{ij}\rho_{E_i}+\rho_{P_j}=0. \label{eq:ME2}
			\end{equation}
			Next, we verify if the proposed Value function \eqref{eq:value_Pout}
			indeed satisfies the equation \eqref{eq:ME2}. 
			Using \eqref{eq:gradV_compact}, we get
			%
			\begin{align*}
			\alpha_{ij}^2\rho_{E_i}^2&=\alpha_{ij}^2\left( \cufrac{\V_{ij}}{x_{E_i}}^2+\cufrac{\V_{ij}}{y_{E_i}}^2+\cufrac{\V_{ij}}{z_{E_i}}^2\right) \\ 
			&=\tfrac{\alpha_{ij}^2}{(1-\alpha_{ij}^2)^2}\left[1-\tfrac{2\alpha_{ij}^2}{1-\alpha_{ij}^2}\tfrac{\langle \x_{c_{ij}},\x_{EP} \rangle}{R_{c_{ij}}r_{c_{ij}}}+\alpha_{ij}^2 \right],  \\
			\rho_{P_j}^2&=\cufrac{\V_{ij}}{x_{P_j}}^2+\cufrac{\V_{ij}}{y_{P_j}}^2+\cufrac{\V_{ij}}{z_{P_j}}^2 \\
			&=\tfrac{\alpha_{ij}^4}{(1-\alpha_{ij}^2)^2}\left[1-\tfrac{2}{1-\alpha_{ij}^2}\tfrac{\langle \x_{c_{ij}},\x_{EP} \rangle}{R_{c_{ij}}r_{c_{ij}}}+\tfrac{1}{\alpha_{ij}^2} \right] 
			= \alpha_{ij}^2\rho_{E_i}^2.
		\end{align*} 
		 Hence, the function defined by \eqref{eq:value_Pout} satisfies the HJI equation \eqref{eq:ME2} and is, therefore, the Value of the \texttt{1v1} RADG with $\x\in\mR_P$ and $\alpha_{ij}<1$. The optimal strategies of the players in feedback form are obtained as \eqref{eq:opt_con}.
		\end{proof}
		\begin{theorem}Consider the \texttt{1v1} RADG with $\x\in\mR_E$ and $\alpha_{ij}<1$. The Value function is $\C^1$ and it is the solution of the HJI-PDE. The Value function is given by  
			\begin{equation}
				\V_{ij}(\x)=R_{P_{j}}-\frac{R_{E_{i}}}{\alpha_{ij}}, \label{eq:valueE}
			\end{equation}
			where $R_{P_{j}}=\vert\vert \x_{P_j} \vert\vert$, and $R_{E_{i}}=\vert\vert \x_{E_i} \vert\vert$.
			\label{theorem:1v1_varspeed_E}
		\end{theorem}
			\begin{proof}
			Since $\x\in\mR_E$ and $\alpha_{ij}<1$, both the evader and the target are located inside the Apollonius sphere. Considering the cost specified in \eqref{eq:multi_Ecost}, as the target lies in the dominance region of the evader, both the players must head straight to the target (as a result of \ref{theorem:prelim}). In such a scenario, the time taken by the evader to reach origin is given by $t_{E_i}=\tfrac{\sqrt{x_{E_i}^2+y_{E_i}^2+z_{E_i}^2}}{U_i}=\tfrac{R_{E_i}}{U_i}$.
			%	\begin{equation}
				%		t_{E_i}=\frac{\sqrt{x_{E_i}^2+y_{E_i}^2+z_{E_i}^2}}{U_i}=\frac{R_{E_i}}{U_i}
				%	\end{equation}
			In this time, $P_j$ covers a distance of $V_jt_{E_i}$ along the direction towards the origin. Hence, the terminal location of $P_j$ is given by 		$\x_{P_{j_f}}=\x_{P_j}-V_jt_{E_i}\tfrac{\x_{P_j}}{R_{P_j}}$, and the payoff is given by this distance $||\x_{P_{j_f}}||$. This payoff now forms a guess for the Value function given by 
			\begin{equation}
				\begin{aligned}
				\V_{ij}(\x)&=\left|\left|\x_{P_j}-V_jt_{E_i }\tfrac{\x_{P_j}}{R_{P_j}}\right|\right|\\
				&=\left[\vert\vert \x_{P_j} \vert\vert^2 -2\tfrac{R_{E_i}}{\alpha_{ij}}\left\langle \x_{P_j},\tfrac{\x_{P_j}}{R_{P_j}}\right\rangle+\tfrac{R_{E_i}^2}{\alpha_{ij}^2}\left\vert\left\vert \tfrac{\x_{P_j}}{\alpha_{ij}} \right\vert\right\vert^2 \right]^{1/2}\\
				&=R_{P_j}-\frac{R_{E_i}}{\alpha_{ij}}.
				\end{aligned}
			\label{eq:valueE_cand}
			\end{equation}
			\begin{comment}
				\begin{equation}
					\begin{aligned}
						\V(\x)&=\vert\vert\x_{P_{j_f}}\vert\vert=\left\vert\left\vert\x_{P_j}-V_jt_{E_i}\frac{\x_{P_j}}{R_{P_j}} \right\vert\right\vert\\
						&=\left[\vert\vert \x_{P_j} \vert\vert^2 -2\frac{R_{E_i}}{\alpha_{ij}}\left\langle \x_{P_j},\frac{\x_{P_j}}{R_{P_j}}\right\rangle+\frac{R_{E_i}^2}{\alpha_{ij}^2}\left\vert\left\vert \frac{\x_{P_j}}{\alpha_{ij}} \right\vert\right\vert^2 \right]^{1/2}\\
						&=R_{P_j}-\frac{R_{E_i}}{\alpha}.
					\end{aligned}
					\label{eq:value_guess_E}
				\end{equation}
			\end{comment}
			It is now imperative to verify that this candidate Value function satisfies the HJI equation. The partial derivatives of $\V_{ij}$ can be written as follows: 
			\begin{comment}
				\begin{equation}
					\begin{aligned}
						V_{x_{E_i}}=-\frac{1}{\alpha}\frac{x_{E_i}}{R_{E_i}} \qquad V_{x_{P_j}}=\frac{x_{P_j}}{R_{P_j}}\\
						V_{y_{E_i}}=-\frac{1}{\alpha}\frac{y_{E_i}}{R_{E_i}} \qquad V_{y_{P_j}}=\frac{y_{P_j}}{R_{P_j}}\\
						V_{z_{E_i}}=-\frac{1}{\alpha}\frac{z_{E_i}}{R_{E_i}} \qquad V_{z_{P_j}}=\frac{z_{P_j}}{R_{P_j}}
					\end{aligned}
				\end{equation}
			\end{comment}
			\begin{equation}
				\begin{aligned}
					\begin{bmatrix}\cufrac{\V_{ij}}{x_{E_i}}&\cufrac{\V_{ij}}{y_{E_i}}&\cufrac{\V_{ij}}{z_{E_i}}\end{bmatrix}&=-\tfrac{1}{\alpha_{ij}}\tfrac{\x_{E_i}}{R_{E_i}},\\
					\begin{bmatrix}\cufrac{\V_{ij}}{x_{P_j}}&\cufrac{\V_{ij}}{y_{P_j}}&\cufrac{\V_{ij}}{z_{P_j}}\end{bmatrix}&=\tfrac{\x_{P_j}}{R_{P_j}}.
				\end{aligned}
			\end{equation}
			The HJI equation associated with the saddle-point problem \eqref{eq:valfun} is given by
			\begin{multline}
				\min\limits_{\u_i}\max\limits_{\v_j}\left(\cufrac{\V_{ij}}{x_{E_i}} u_{x_i}+\cufrac{\V_{ij}}{y_{E_i}}u_{y_i}+\cufrac{\V_{ij}}{z_{E_i}}u_{z_i}\right. \\
				\left. +\cufrac{\V_{ij}}{x_{P_j}}v_{x_j}+\cufrac{\V_{ij}}{y_{P_j}}v_{y_j}+\cufrac{\V_{ij}}{z_{P_j}}v_{z_j}\right)=0. \label{eq:ME1E}
			\end{multline}
			Thus, the optimal controls in terms of the Value function can be written as 
			\begin{equation}
				\begin{aligned}
					\u_i^*&=-\tfrac{U_i}{\rho_{E_i}}\begin{bmatrix}
						\cufrac{\V_{ij}}{x_{E_i}}&\cufrac{\V_{ij}}{y_{E_i}}&\cufrac{\V_{ij}}{z_{E_i}}
					\end{bmatrix},\\  \v_j^*&=\tfrac{V_j}{\rho_{P_j}}\begin{bmatrix}
						\cufrac{\V_{ij}}{x_{P_j}}&\cufrac{\V_{ij}}{y_{P_j}}& \cufrac{\V_{ij}}{z_{P_j}}
					\end{bmatrix}. \label{eq:opt_conE}
				\end{aligned}
			\end{equation}
			where $\rho_{E_i}=\sqrt{\cufrac{\V_{ij}}{x_{E_i}}^2+\cufrac{\V_{ij}}{y_{E_i}}^2+\cufrac{\V_{ij}}{z_{E_i}}^2}$ and $\rho_{P_j}=\sqrt{\cufrac{\V_{ij}}{x_{P_j}}^2+\cufrac{\V_{ij}}{y_{P_j}}^2+\cufrac{\V_{ij}}{z_{P_j}}^2}$. Substituting the optimal controls \eqref{eq:opt_conE} in the HJI equation \eqref{eq:ME1E}, we get
			\begin{equation}
				-\alpha_{ij}\rho_{E_i}+\rho_{P_j}=0. \label{eq:ME2E}
			\end{equation}
			Next, we verify if the proposed Value function \eqref{eq:valueE}
			indeed satisfies the equation \eqref{eq:ME2E}. The LHS is given by
			\begin{align*}
					%&-\alpha_{ij}\rho_{E_i}+\rho_{P_j}\\
					% &=-\alpha_{ij}\sqrt{\V_{x_{E_i}}^2+\V_{y_{E_i}}^2+\V_{z_{E_i}}^2}+\sqrt{\V_{x_{P_j}}^2+\V_{y_{P_j}}^2+\V_{z_{P_j}}^2}\\
					&=-\alpha_{ij}\sqrt{\tfrac{1}{\alpha_{ij}^2}\left[\left(\tfrac{x_{E_i}}{R_{E_i}} \right)^2+\left( \tfrac{y_{E_i}}{R_{E_i}}\right)^2+\left( \tfrac{z_{E_i}}{R_{E_i}}\right)^2 \right] }\\
					&\qquad \qquad +\sqrt{\left(\tfrac{x_{P_j}}{R_{P_j}} \right)^2+\left( \tfrac{y_{P_j}}{R_{P_j}}\right)^2+\left( \tfrac{z_{P_j}}{R_{P_j}}\right)^2}\\
					&=-\alpha_{ij}\tfrac{1}{\alpha_{ij}}+1=0.
				\end{align*}
		 Hence, the function defined by \eqref{eq:valueE} satisfies the HJI equation \eqref{eq:ME2E} and is, therefore, the Value of the \texttt{1v1} RADG with $\x\in\mR_P$ and $\alpha_{ij}<1$. The optimal strategies of the players in feedback form are obtained as \eqref{eq:opt_conE}.
		\end{proof}
		\begin{remark}
		Theorems \ref{theorem:1v1_varspeed} and \ref{theorem:1v1_varspeed_E} generalize the analysis of the \texttt{1v1} 3D RADG presented in \cite{garcia_2020}. The previous work considered a specific case with $\alpha_{ij}=1$ and thus a plane for interception point calculations instead of an Apollonius sphere.
	\end{remark}
	%%%%
	\section{Multiplayer Differential Game}
	\label{sec:multiplayer}
	In this section, we analyze a class of MRADGs using the results obtained from the previous section. Specifically, we solve the co-design problem by providing an optimal assignment scheme and an analytical characterization of the barrier and the Value of the game in the puruser team's winning region. 
	The class of MRADGs considered in this paper is characterized by the following assumption on players' interactions.
	\begin{assumption} 
		\begin{enumerate}[label=(\roman*)]
			\item \label{itm:assumitem1}  A pursuer can capture at most one evader, and an evader is pursued by one pursuer.
			\item \label{itm:assumitem2} The pursuing team is atleast as large as the evading team.% that is, $n\geq m$.
			\item \label{itm:assumitem3} The pursuers commit to their assignments throughout the duration of the game.
		\end{enumerate}
		\label{assum:interactions}
	\end{assumption} 
	%
	Item \ref{itm:assumitem1} is a crucial assumption in our paper. Using this, we show that there exists a linear-programming based optimal assignment scheme for matching the pursuers with   evaders. Item \ref{itm:assumitem2} is a natural consequence of Item \ref{itm:assumitem1}, because if $n<m$ then, by Item \ref{itm:assumitem1}, at least $m-n$ evaders cannot be assigned to a pursuer, thus making the game outcome trivial.
	%at least $m-n$ evaders can reach the target, thus making the game outcome (evader team wins) trivial. 
	Item \ref{itm:assumitem3} implies that the pursuers strictly commit to their assignments throughout the game \textit{i.e.}, $\mu_{ij}(t)=\mu_{ij}(0)$. 
	Note that we do not make the assumption that all pursuers are faster than all evaders, as is often assumed in the existing literature.
	\par	
	Next, following Assumption \ref{assum:interactions} Item \ref{itm:assumitem1} we consider the set of all assignments with \texttt{1v1} matchings %to ensure the assignment of all evaders,
	 as follows:
	\begin{definition} \label{def:aval}
		The set of all \emph{feasible} assignments is denoted by
		\begin{align}
			\Sigma:=\big\{\bm{\mu}\in\{0,1\}^{m\times n}~\big|~
			\bm\mu.\bm 1_n=\bm1_m,~ \bm\mu^T.\bm 1_m\leq \bm1_n \big\}, \label{eq:feasibleset}
		\end{align}
		where $\bm1_k$ denotes a column one vector of size $k$. Denote $\mu_{ij}$ as the element in $i^{th}$ row and $j^{th}$ column of $\bm\mu$. Associated with every $\bm\mu\in\Sigma$, we define 
		\begin{equation}
			A_\mu:=\big\{ ij\in M\times N~\big|~\mu_{ij}=1 \big\}.
		\end{equation}
%		indicating the pursuer assigned to each evader by the assignment $\bm\mu$. 
Further, we denote the set of all \emph{probabilistic feasible} assignments by
		\begin{align}
			\Gamma:=\big\{\bm{\gamma}\in[0,1]^{m\times n}~\big|~\bm\gamma.\bm 1_n=\bm1_m,~ \bm\gamma^T.\bm 1_m\leq \bm1_n \big\}. \label{eq:feasiblepset}
		\end{align}
	Denote $\bm\gamma_{ij}$ as the element in $i^{th}$ row and $j^{th}$ column of $\bm\gamma$. The payoff received by the pursuer $P_j$ when matched with an evader $E_i$ is now defined as
%	We further define the payoff received by the pursuer $P_j$ when matched with an evader $E_i$  as 
	\begin{align*}
	a_{ij}(\x_{E_i},\x_{P_j})= \begin{cases} \mathcal V_{ij}(\x_{E_i},\x_{P_j}),&  B_{ij}(\x_{E_i},\x_{P_j})\geq 0,~ \alpha_{ij}\leq1  \\ -L,&  \text{otherwise}. \end{cases}
	\end{align*}
	 If an evader $E_i$ is assigned to a pursuer $P_j$, at least as fast as $E_i$, and $(\x_{E_i},\x_{P_j})$ lies outside the evader winning region of the induced \texttt{1v1} RADG. In this case, the pursuer $P_j$ receives a payoff equal to the Value of the game obtained from \eqref{eq:value_Pout} for $\alpha_{ij}<1$ (for $\alpha_{ij}=1$, the payoff follows from \cite{garcia_2020}). In any other case, the pursuer incurs a cost equal to $L>0$. \par
	 Under a feasible assignment $\bm\mu\in\Sigma$, as defined in \eqref{eq:feasibleset}, there can only be one pursuer who is matched to an evader $E_i$. The contribution of this evader's capture to the pursuer team can be written as 
	\begin{align}
		\Psi_i((\x_{E_i},\x_P),\bm{\mu}):=\sum_{j\in N} a_{ij}(\x_{E_i},\x_{P_j})\mu_{ij}.
		\label{eq:payoffcontribution}
	\end{align}
	Now, the team payoff of the pursuers under the assignment $\bm\mu$ is given by 
	\begin{equation}
		\Psi(\x,\bm\mu):=\sum\limits_{i\in M}\Psi_i\left((\x_{E_i},\x_{P_j}),\bm\mu\right).
	\end{equation}
	Note that $a_{ij}$ and $\V_{ij}$ are used further in the text with the implicit assumption of dependence on $\x_{E_i}$ and $\x_{P_j}$. Let $\hat N_i:=\{j\in N:\B_{ij}\geq0,\ \alpha_{ij}\leq1\}$, using which we define the maximum possible payoff generated by the pursuer team by
	\begin{align}
		L^\star:=\sum_{i\in M}~ \max_{j\in \hat N_i} \mathcal V_{ij}(\x_{E_i},\x_{P_j}). \label{eq:lowerboundcost}
	\end{align}
	\end{definition}
	%
	\begin{remark}
	 In \eqref{eq:lowerboundcost}, $\max_{j\in \hat{N}_i} \mathcal V_{ij}(\x_{E_i},\x_{P_j})$ denotes the best payoff generated by a pursuer from among all the pursuers (faster than $E_i$) that can capture $E_i$.  Then, $L^\star$ denotes the payoff that a pursuer team could generate if every evader is assigned to a pursuer who can generate the best possible payoff. However, such an assignment need not be feasible.
	\end{remark} 
	\begin{theorem}
	The linear programming problem defined by
	\begin{equation}
		\bm\gamma^\star:=\argmax\limits_{\bm\gamma\in\Gamma} \Psi(\x,\bm\gamma)
		\label{eq:lpsolution}
	\end{equation}
	satisfies $\bm\gamma^\star\in\Sigma$, that is, $\bm\gamma^\star$ is a feasible assignment. Further, if $L>L^\star$, then the optimal assignment $\bm\gamma^\star$ ensures that the least number of evaders reach the target without being intercepted.
	\label{thm:lp}
\end{theorem}
\begin{proof}
	We note that the  linear program given by \eqref{eq:lpsolution} is a  form of Shapley-Shubik assignment game \cite{shapley_1971}, which  has been shown to have integer solutions (see  \cite{shapley_1971,dantzig_1963})
	, thus resulting in $\bm\gamma^\star\in\Sigma$.
	Assume there exists an assignment $\bm\mu\in\Sigma$ that allows $m_\mu$ evaders to reach the target without interception. We claim that if $L>L^\star$, the optimal assignment $\bm\gamma^\star$ allows at most $m_\mu$ evaders to reach the target. Assume to the contrary that the optimal assignment allows $m_{\bm\gamma}$ evaders to reach the target with $m_{\bm\gamma}>m_\mu$. 
	%Define $A_\mu:=\{ij\in M\times N:\mu_{ij}=1\}$ and $A_{\gamma^\star}:=\{ij\in M\times N:\bm\gamma_{ij}^\star=1\}$. Further, let
	Define $A_\mu^C:=\{ij\in A_\mu:a_{ij}\geq0\}$ and $A_{\bm\gamma^\star}^C:=\{ij\in A_{\bm\gamma^\star}:a_{ij}\geq0\}$. Then, the assignments $\bm\mu$ and $\bm\gamma^\star$ yield payoffs $\Psi(\x,\bm\mu)=\sum\limits_{ij\in A_\mu}a_{ij}$ and $\Psi(\x,\bm\gamma^\star)=\sum\limits_{ij\in A_{\bm\gamma^\star}}a_{ij}$ for the pursuer team respectively. Now, consider 
	\begin{align*}
			&\Psi(\x,\bm\mu)-\Psi(\x,\bm\gamma^\star)=\sum\limits_{ij\in A_\mu}a_{ij}-\sum\limits_{ij\in A_{\bm\gamma^\star}}a_{ij}\\
			&=\Big( \sum\limits_{ij\in A_\mu^C}a_{ij}+\sum\limits_{ij\in A_\mu\setminus A_\mu^C}a_{ij} \Big) -\Big( \sum\limits_{ij\in A_{\bm\gamma^\star}^C}a_{ij}+\sum\limits_{ij\in A_{\gamma^\star}\setminus A_{\bm\gamma^\star}^C}a_{ij} \Big) \\
			&=\Big( \sum\limits_{ij\in A_\mu^C}a_{ij}-\sum\limits_{ij\in A_{\bm\gamma^\star}^C}a_{ij} \Big)+(m_{\bm\gamma}-m_\mu)L.
	\end{align*}
	By the definition of $L^\star$, we have that $0\leq\sum\limits_{ij\in A_\mu^C}a_{ij}\leq L^\star\ \forall\bm\mu\in\Sigma$, and since $\bm\gamma^\star\in\Sigma$, we have $\sum\limits_{ij\in A_\mu^C}a_{ij}-\sum\limits_{ij\in A_{\gamma^\star}^C}a_{ij}\geq -L^\star$. By assumption $m_{\bm\gamma}>m_\mu$, and thus $m_{\bm\gamma}-m_\mu\geq1$ as $m_{\bm\gamma}$ and $m_\mu$ are both integers. Hence, we have that $\Psi(\x,\bm\mu)-\Psi(\x,\bm\gamma^\star)>0$ which is a contradiction to the fact that $\bm\gamma^\star$ optimizes the LP given by \eqref{eq:lpsolution}. Further, since the chosen $\bm\mu\in\Sigma$ is arbitrary, $m_{\bm\gamma}\geq m_\mu,\ \forall\bm\mu\in\Sigma$.
\end{proof}
\begin{remark}
In Theorem \ref{thm:lp}, using a cost-benefit framework, we formulated the pursuer team's task assignment problem as a Shapley-Shubik assignment game \cite{shapley_1971}   given by \eqref{eq:lpsolution}.  The obtained linear programming based optimal assignment \eqref{eq:lpsolution}  is agnostic to the outcome of the game.  If $L$, the cost of failing to capture an evader is low, the optimal assignment may allow more evaders to reach the target, even if another assignment allows fewer evaders to win. Additionally, $L^\star$ serves as a conservative lower bound for $L$.
\end{remark}
Next, using the optimal assignment \eqref{eq:lpsolution} we provide the barrier function for the MRADG in the following theorem.
	\begin{theorem} Let Assumption \ref{assum:interactions} hold and $L>L^\star$.
		The function $\B(\x):\R^{3(m+n)}\shortrightarrow\R$ defined by 
		\begin{equation}
			B(\x)=\min\limits_{i\in M}\Psi_i((\x_{E_i},\x_P),\bm{\gamma}^\star), \label{eq:nm_barrier}
		\end{equation}
		is a barrier function for the MRADG, where $\bm\gamma^\star\in \Sigma$ satisfies \eqref{eq:lpsolution}. Further, the barrier function partitions the global state space into pursuer and evader winning regions as in \eqref{eq:partition_sets}. 
		\label{thm:multibarrier}
	\end{theorem}
	\begin{proof}
		Consider $\x\in\R^{3(m+n)}$ and $\B(\x)>0$. We claim that $a_{ij}>0,\ \forall\ ij\in A_{\bm\gamma^\star}$. Assume to the contrary that there exists $\bar{i}\bar{j}\in A_{\bm\gamma^\star}$ such that $a_{\bar i\bar j}\leq0$. By Definition \ref{def:aval}, it is possible either if $a_{\bar i\bar j}=-L$ or $a_{\bar i\bar j}=0$ resulting in $\Psi_{\bar i}((\x_{E_{\bar i}},\x_P),\bm\gamma^\star)=\sum_{j\in N}a_{\bar i j}\bm\gamma_{\bar i j}^\star\leq0$. Thus, $\B(\x)\leq0$, resulting in a contradiction. Hence, every pursuer assigned to an evader can intercept it before the target is reached. Further, the constraint $\bm\gamma^\star.\bm 1_n=1_m$ ensures that every evader has been assigned to some pursuer. From the previous two arguments, it is evident that the pursuing team wins, and $\B(\x)>0$ characterizes the winning region for the pursuers $\mR_P$.\par 
		Now, consider the case when $\B(\x)<0$. There exists $\bar i\in M$ such that $\Psi_{\bar i}((\x_{E_{\bar i}},\x_P),\bm\gamma^\star)=\sum_{j\in N}a_{\bar i j}\bm\gamma^\star_{\bar ij}<0$. This is again possible only if $a_{\bar i\bar j}<0$ for some $\bar j\in N$ with $\bar i\bar j\in A_{\bm\gamma^\star}$. This implies that, under the assignment $\bm\gamma^\star$, at least one evader reaches the target before interception. Therefore, using Theorem \ref{thm:lp}, every $\bm\mu\in\Sigma$ allows at least one evader to reach the target. Thus, $\B(\x)<0$ characterizes the winning region for the evaders $\mR_E$. \par
		Finally, consider the case when $\B(\x)=0$. This implies that $\forall ij\in A_{\bm\gamma^\star}$, we have $a_{ij}\geq0$ and there is at least one $\bar i\bar j\in A_{\bm\gamma^\star}$ such that $a_{\bar i\bar j}=0$. A particular $a_{\bar i\bar j}$ can be zero only if the Value of the associated \texttt{1v1} game is zero, which is only possible when $P_{\bar j}$ intercepts $E_{\bar i}$ precisely at the target. This corresponds to a tie situation, and hence characterizes the barrier surface. 
	\end{proof}
	Having solved the Game of Kind for MRADG, starting from any point $\x\in \R^{3(n+m)}$ in the global state space it is possible to determine which team wins the game. Next, in the pursuer team's winning region, a Game of Degree is formulated in the following theorem.
	\begin{theorem} Let Assumption \ref{assum:interactions} hold and $L>L^\star$.
		Consider the  MRADG   for $\x\in\mR_P$. The Value function is $C^1$ (except at the dispersal surfaces i.e., there are multiple optima for the LP in \eqref{eq:lpsolution}) and it is the solution of the HJI-PDE. The Value function is given by 
		\begin{equation}
			\V(\x)= \Psi(\x,\bm\gamma^\star), \label{eq:multi_value}
		\end{equation}
		where $\bm\gamma^\star\in \Sigma$ satisfies \eqref{eq:lpsolution}. The optimal strategies of all the players are obtained from \eqref{eq:opt_con}.
	\end{theorem}
	\begin{proof}
		Since $\x\in\mR_P$, we have $\B(\x)>0$. This results in $a_{ij}>0,\ \forall ij\in A_{\bm\gamma^\star}$ from Theorem \ref{thm:multibarrier}. But, by Definition \ref{def:aval}, $a_{ij}>0$ only when $a_{ij}=\V_{ij}$. Thus, the proposed Value function \eqref{eq:multi_value} can be written as 
		\begin{equation}
			\begin{aligned}
				\V(\x)=\Psi(\x,\bm\gamma^\star)=\sum\limits_{ij\in A_{\bm\gamma^\star}}a_{ij}=\sum\limits_{ij\in A_{\bm\gamma^\star}}\V_{ij}. \label{eq:multi_value_simplify}
			\end{aligned}
		\end{equation}
		The proposed Value of the multiplayer game is hence posed as a sum of the individual pairwise games dictated by the optimal assignment. We must now verify that the proposed Value function \eqref{eq:multi_value} satisfies the HJI equation:
		\begin{equation*}
			\begin{aligned}
				&\min\limits_{\u}\max\limits_{\v}\langle \nabla \V(\x),f(\x,\u,\v)\rangle\\
				%			&=\min\limits_{\u(.)}\max\limits_{\v(.)}\langle \nabla V(\x),[\u\ \v]^T\rangle\\
				&=\min\limits_{\u}\max\limits_{\v}\sum\limits_{ij\in A_{\gamma^\star}}\Big\langle\Big[\tfrac{\partial \V_{ij}}{\partial\x_{E_i}}\ \tfrac{\partial\V_{ij}}{\partial\x_{P_j}}\Big],[\u_i\ \v_j] \Big\rangle \\
				&=\sum\limits_{ij\in A_{\gamma^\star}} \min\limits_{\u_i}\max\limits_{\v_j}\left[\cufrac{\V_{ij}}{x_{E_i}} u_{x_i}+\cufrac{\V_{ij}}{y_{E_i}}u_{y_i}\right. \\
				&\left.+\cufrac{\V_{ij}}{z_{E_i}}u_{z_i}+\cufrac{\V_{ij}}{x_{P_j}}v_{x_j}+\cufrac{\V_{ij}}{y_{P_j}}v_{y_j}+\cufrac{\V_{ij}}{z_{P_j}}v_{z_j}\right]=0.\\
			\end{aligned}
		\end{equation*}
		The separability of the expression in terms of the individual controls of all the players allows the interchange of the summation with the minmax operator. Hence, the Value function proposed in \eqref{eq:multi_value} satisfies the HJI equation and provides the solution to the MRADG. The closed loop optimal controls can also be obtained from the Value function as given in \eqref{eq:opt_con}.\par 
		 Finally, the singular surface defined by $\Psi(\x,\bm\gamma_1^\star)=...=\Psi(\x,\bm\gamma_k^\star)$ forms the dispersal surface, where $\bm\gamma_1^\star,...,\bm\gamma_k^\star$ solve the LP in \eqref{eq:lpsolution}. In such a scenario, each team has multiple equally optimal strategies. To move away from the dispersal surface, each team can choose an assignment scheme randomly from the available optimal options $\{\bm\gamma_1^\star,...,\bm\gamma_k^\star\}$ at the instant the game begins. After an infinitesimal amount of time, the state moves out of the dispersal surface and the optimal strategies become fixed. However, this initial choice may favor one team and disadvantage their opponents.
	\end{proof}
\begin{remark}
Despite restricting the space of feasible assignments to $\Sigma$, defined in \eqref{eq:feasibleset}, a brute-force search spans through $^nP_m$ possibilities and results in a factorial time complexity. Formulating the assignment problem as an LP enables an analytic characterization of the barrier surface and the Value of the game while significantly reducing the computational burden.
%Formulating the assignment problem as an LP, thus reduces this computational burden significantly. Furthermore, the LP formulation enables us to find an analytic characterization of both the barrier surface and the Value of the game thus achieving both optimality and computational efficiency. 
\end{remark}
	%%%%% 
	\section{Numerical Illustrations}
	\label{sec:num}
	We consider a few representative MRADG examples to illustrate our solution approach. The computations were done in MATLAB 2022b on a workstation PC with a Core i9-13900K processor and a memory of 128GB.
	\begin{example}
		\label{ex:1}
	 We illustrate the efficiency of our linear programming based optimal assignment by using the built-in \texttt{linprog} function. We compare our results with a brute-force algorithm that spans over all $^nP_m$ possible assignments; see Table \ref{tab:Table1}. This table shows the average time required to compute assignments via brute-force and linear programming (in seconds) for different values of $n$ and $m$. The last column shows the required computational time for coordinate and distance calculations (in milliseconds). Although the simplex algorithm has an exponential time complexity, it can significantly outperform a brute-force method in most real-world cases, as evident in Table \ref{tab:Table1}.  
	\begin{comment}
		\begin{table}[h]
			\caption{Average computation time for various multiplayer cases. \footnotesize{NA indicates that the set of feasible assignments cannot be generated by the brute-force method due to memory limitations.}}
			\label{tab:Table1}
			\centering
			{\setlength{\tabcolsep}{5pt} 
				\renewcommand{\arraystretch}{1} { 
					\begin{tabular}{| m{4em} | m{5em} | m{5em} | m{5em} |}
						\hline 
						Player number ($n,m$) & Brute Force Assignment(s) & LP-Based Assignment(s) &Coordinates/ Distance(ms) \\ 
						\toprule
						3,3 & 0.0105 & 0.0018 & 0.0074\\
						\hline
						%5,4 & 0.0529 & 0.0021 & 0.0087\\
						%\hline 
						7,5 & 0.0761 & 0.0023 & 0.0106 \\
						\hline
						%8,8 & 0.0946 & 0.0033 & 0.0161 \\
						%\hline 
						%9,6 & 0.1478 & 0.0023 & 0.0148\\
						%\hline
						10,8 & 0.9792 & 0.0022 & 0.0172\\
						\hline
						11,7 & 11.0177 & 0.0019 & 0.0176\\
						\hline 
						12,10 & 287.7911 & 0.0020 & 0.0220\\
						\hline 
						20,15 & NA & 0.0020 & 0.0328 \\
						\hline 
						50,40 & NA & 0.0043 & 0.0886\\
						\hline 
						100,100 & NA & 0.0223 & 0.1964\\
						\bottomrule
					\end{tabular}
			} }
		\end{table} 
	\end{comment}
	\begin{table}[h]
		\caption{Average computation time for various multiplayer cases. NA indicates that the set of feasible assignments cannot be generated by the brute-force method due to memory limitations.}
		\label{tab:Table1}
		\centering
		{\setlength{\tabcolsep}{5pt} 
			\renewcommand{\arraystretch}{1} { 
				\begin{tabular}{| m{4em} | m{5em} | m{5em} | m{5em} |}
					\hline 
					Player number ($n,m$) & Brute Force Assignment(s) & LP-Based Assignment(s) &Coordinates/ Distance(ms) \\ 
					\toprule
					3,3 & 0.0188 & 0.0019 & 0.0074\\
					\hline
					%5,4 & 0.0812 & 0.0022 & 0.0129\\
					%\hline 
					7,5 & 0.0953 & 0.0022 & 0.0118 \\
					\hline
					%8,8 & 0.1296 & 0.0021 & 0.0315 \\
					%\hline 
					%9,6 & 0.1442 & 0.0022 & 0.0135\\
					%\hline
					10,8 & 0.3439 & 0.0018 & 0.0178\\
					\hline
					11,7 & 0.3264 & 0.0021 & 0.0182\\
					\hline 
					12,10 & 59.561 & 0.0020 & 0.0285\\
					\hline 
					20,15 & NA & 0.0020 & 0.0306 \\
					\hline 
					50,40 & NA & 0.0039 & 0.0787\\
					\hline 
					100,100 & NA & 0.0231 & 0.2032\\
					\bottomrule
				\end{tabular}
		} }
	\end{table} 
	\end{example}
	\begin{example}
		Consider an MRADG with the origin as the target with 3 pursuers and 3 evaders. The pursuers are situated at  $\x_{P_1}=[-6.77,-2.95,0.01],\ \x_{P_2}=[-3.34,-3.96,-3.33]$ and $\x_{P_3}=[4.76,-13.35,-0.61]$, while the evaders are situated at $\x_{E_1}=[4.92,-7.91,4.43],\ \x_{E_2}=[-8.07,2.73,-5.91]$ and $\x_{E_3}=[-6.73,-10.65,-12.49]$. The speeds of the pursuers are $V_{P_1}=1.71,\ V_{P_2}=2.23$ and $V_{P_3}=2.28$, while those of the evaders are $U_{E_1}=1.69,\ U_{E_2}=1.01$ and $U_{E_3}=1.84$. The pursuing team wins in this particular case and the optimal assignment is given by $E_1-P_2,\ E_2-P_1,\ E_3-P_3$. The resulting Value of the game is $18.63$. The optimal trajectories for all the agents can be seen in Figure \ref{fig:optimal}. Note that in this setting there is a subset of pursuers who are not superior to all the evaders, namely $P_1$ is slower than $E_3$. Furthermore, $E_1$ cannot be captured by $P_3$  and $E_3$ cannot be captured by $P_1$. 
	 This particular example also serves as an illustration for the sensitivity of the assignment to the value of $L$. If $L>L^\star=23.84$, then the optimal assignment is as given above. However, for values of $L\leq L^\star$ there is no guarantee on the nature of the optimal assignment. If for example, $L=1$, then the optimal assignment provided by the LP in \eqref{eq:lpsolution} allots $E_1-P_3,\ E_2-P_1,\ E_3-P_2$. Despite obtaining a payoff larger than the Value of the game, the assignment fails as $E_1$ and $E_2$ are assigned to pursuers who cannot capture them. Thus, $L$ must be large enough to ensure that the pursuer team goal is kept as a priority. Here, we also illustrate the robustness of the optimal state feedback strategies. Supposing that the evaders decided not to follow the obtained optimal strategies and instead decided to head straight to the origin with the pursuers committed to their optimal strategies, then the ensuing nonoptimal play results in a higher Value of $20.26$ for the game, favoring the pursuers. These trajectories are shown in Figure \ref{fig:nonoptimal}.
	\end{example}
	\begin{figure}[h]
		\centering
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
 	\includegraphics[scale=.5]{FigureOPT_new.pdf}		
			\caption{Optimal Play}
			\label{fig:optimal}
		\end{subfigure}  
		\begin{subfigure}[h]{0.24\textwidth}
			\centering 
	 	\includegraphics[scale=.5]{FigureNONOPT_new.pdf}
			\caption{Nonoptimal Play}
			\label{fig:nonoptimal}
		\end{subfigure}
		\caption{Trajectories of a 3v3 Reach-Avoid Game}
		\label{fig:num_ill}
	\end{figure}
	\begin{example}
		Now, consider a reach-avoid game with $3$ pursuers and $2$ evaders with $\x_{P_1}=[1,0,0],\ \x_{P_2}=[1,0,0.5],\ \x_{P_3}=[1,0,-0.5],\ \x_{E_1}=[0.75,1,0]$ and $\x_{E_2}=[0.75,-1,0]$. All the evaders are assumed to have a speed of 0.5 units while the pursuers have unit speed. Among the $6$ feasible assignments, $4$ achieve the optimal Value and hence the state belongs to a dispersal surface. Thus at the beginning of the game, there are $4$ equally optimal solutions for the players given by the assignments $[E_1-P_3,\ E_2-P_1]$, $[E_1-P_2,\ E_2-P_1]$, $[E_1-P_1,\ E_2-P_3]$ and $[E_1-P_1,\ E_2-P_2]$ resulting in a Value of $1.5398$. The optimal trajectories in all these cases are shown in Figure \ref{fig:dispersal}.
	\end{example} 
	\begin{figure}[h]  
		\begin{subfigure}[h]{.175\textwidth} 
			\includegraphics[scale=0.5]{disp_figure_1.pdf}
			\label{fig:dispersal1}
		\end{subfigure} \hspace{20pt}
		\begin{subfigure}[h]{.175\textwidth} 
			\includegraphics[scale=0.5]{disp_figure_2.pdf}
			\label{fig:dispersal2}
		\end{subfigure} \\
		\begin{subfigure}[h]{.175\textwidth} 
			\includegraphics[scale=0.5]{disp_figure_3.pdf}
			\label{fig:dispersal3}
		\end{subfigure} \hspace{20pt}
		\begin{subfigure}[h]{.175\textwidth} 
			\includegraphics[scale=0.5]{disp_figure_4.pdf}
			\label{fig:dispersal4}
		\end{subfigure} 
		\caption{Optimal assignments for dispersal surface}
		\label{fig:dispersal} 
	\end{figure}
	
	%%%%%
	%%%%%
	\section{Conclusion and Future Work}
	\label{sec:conc}
	The paper discusses a reach-avoid differential game in 3D space with $n$ pursuers and $m$ evaders, where $n\geq m \geq 1$. We propose an optimal task assignment algorithm based on linear programming to provide optimal trajectories for all players while satisfying the HJI-PDE. Currently, our method assigns a maximum of one pursuer to one evader. However, assigning multiple pursuers to one evader can improve the game Value for the pursuing team by restricting the evader's reachable space. Therefore, future work can explore this direction. 	

	\bibliographystyle{IEEEtran}
	\bibliography{main}
	
\end{document}
