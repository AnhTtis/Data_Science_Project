\documentclass[12pt]{article}
%\documentclass{beamer}
%\usetheme{Madrid}
%\usetheme{AnnArbor}
%\usetheme{JuanLesPins}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{makeidx}
%\usepackage{natbib}
\usepackage{subcaption}
\usepackage{float}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{makeidx}
\usepackage{lineno}
%\linenumbers
%\usepackage{natbib}
%\usepackage{subcaption}
%\usepackage{float}
%\usepackage{placeins}
%\usepackage[section]{placeins}
%\usepackage[utf8]{inputenc}
%\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage[russian]{babel}
    




\linewidth=8.5in
\def\doublespace{\parskip 3pt plus 1.2pt

     \baselineskip 18pt plus 1pt minus .5pt

     \lineskip 2pt plus 1pt \lineskiplimit 5pt}

\newcommand{\be}{\begin{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\ee}{\end{equation}}
\newcommand{\eea}{\end{eqnarray}}
\def \blank{\mbox{}}
\def\d{\mbox{$\partial$}}
\def\de{\mbox{$d_E$}}
\def\dw{\mbox{$d_W$}}
\def\dl{\mbox{$d_L$}}
\def\da{\mbox{$d_A$}}
\def\hb{\mbox{$\hbar$}}
\def\Re{\mbox{\,Re\,}}
\def\Im{\mbox{\,Im\,}}
\def\dw{\mbox{$d_W$}}
\def\nb{\mbox{$N_B$}}
\def\A{\mbox{\bf A}}
\def\a{\mbox{\bf a}}
\def\B{\mbox{\bf B}}
\def\C{\mbox{\bf C}}
\def\J{\mbox{\bf J}}
\def\M{\mbox{\bf M}}
\def\P{\mbox{\bf P}}
\def\x{\mbox{{\bf x}}}
\def\f{\mbox{{\bf f}}}
\def\q{\mbox{{\bf q}}}
\def\c{\mbox{{\bf c}}}
\def\ol{\mbox{$\bar{\lambda}$}}
\def\Q{\mbox{{\bf Q}}}
\def\O{\mbox{{\bf O}}}
\def\R{\mbox{{\bf R}}}
\def\r{\mbox{{\bf r}}}
\def\S{\mbox{{\bf S}}}
\def\L{\mbox{$L$}}
\def\Rd{\mbox{$R^d$}}
\def\DF{\mbox{{\bf DF}}}
\def\DS{\mbox{{\bf DS}}}
\def\Df{\mbox{{\bf Df}}}
\def\DG{\mbox{{\bf DG}}}
\def\DH{\mbox{{\bf DH}}}
\def\DFL{\mbox{$\DF^L$}}
\def\DGL{\mbox{$\DG^L$}}
\def\OSL{\mbox{{\bf OSL}}}
\def\T{\mbox{{\bf $T$}}}
\def\X{\mbox{{\bf X}}}
\def\TD{\mbox{{\bf TD}}}
\def\KN{\mbox{{\bf \Y}}}
\def\Ec{\mbox{{$\cal E$}}}
\def\Pc{\mbox{{$\cal P$}}}
\def\Ac{\mbox{{$\cal A$}}}
\def\l{\mbox{$\lambda$}}
\def\g{\mbox{{\bf g}}}
\def\G{\mbox{{\bf G}}}
\def\k{\mbox{{\bf k}}}
\def\cals{\mbox{{\bf S}}}
\def\oma{\mbox{$\omega_A$}}
\def\om{\mbox{$\omega$}}
\def\eps{\mbox{$\epsilon$}}
\def\Y{\mbox{$\bf{Y}$}}
\def\Z{\mbox{$\bf{Z}$}}
\def\W{\mbox{$\bf{W}$}}
\def\V{\mbox{$\bf{V}$}}
\def\y{\mbox{$\bf{y}$}}
\def\v{\mbox{$\bf{v}$}}
\def\ynn{\mbox{$\bf{y}^{NN}$}}
\def\w{\mbox{$\bf{w}$}}
\def\W{\mbox{$\bf{W}$}}
\def\u{\mbox{$\bf{u}$}}
\def\z{\mbox{$\bf{z}$}}
\def\e{\mbox{$\bf{e}$}}
\def\p{\mbox{$\bf{p}$}}
\def\s{\mbox{$\bf{s}$}}
\def\del{ \mbox{\boldmath{$\Delta$}} }
\def\delt{ \mbox{\boldmath{$\delta$}} }
\def\domega{ \mbox{$\delta\omega$}}
\def\xint{\mbox{$\displaystyle\int d^3x \,$}}
\def\f{\mbox{$\bf{f}$}}
\def\F{\mbox{$\bf{F}$}}
\def\E{\mbox{$\bf{E}$}}
\def\D{\mbox{$\bf{D}$}}
\def\H{\mbox{$\bf{H}$}}
\def\h{\mbox{$\bf{h}$}}
\def\G{\mbox{$\bf{G}$}}
\def\U{\mbox{$\bf{U}$}}
\def\J{\mbox{$\bf{J}$}}
\def\X{\mbox{$\bf{X}$}}
\def\Y{\mbox{$\bf{Y}$}}
\def\ca{\mbox{Ca$^{2+}$}}
\def\Dg{\mbox{$\Delta g$}}
\def\K{\mbox{$\bf{K}$}}
\def\I{\mbox{$\bf{I}$}}
\def\btheta{\mbox{$\boldsymbol{\theta}$}}
\def\bpsi{\mbox{$\boldsymbol{\psi}$}}
\def\bsig{\mbox{$\boldsymbol{\sigma}$}}
\def\bchi{\mbox{$\boldsymbol{\chi}$}}
\def\bXi{\mbox{$\boldsymbol{\Xi}$}}
\def\bXi{\mbox{$\boldsymbol{\Xi}$}}
\def\btau{\mbox{$\boldsymbol{\tau}$}}
\def\bvarphi{\mbox{$\boldsymbol{\varphi}$}}
\def\bpi{\mbox{$\boldsymbol{\pi}$}}
\def\bgam{\mbox{$\boldsymbol{\gamma}$}}
\def\boldeta{\mbox{$\boldsymbol{\eta}$}}
\def\Beta{\mbox{$\boldsymbol{\beta}$}}
\def \sig {\mbox{$\sigma$}}
\def \normw {\mbox{$|w|$}}
\def \tf {\mbox {$[t_0,t_{final}]$}}
\def \lf {\mbox {$[l_0,l_F]$}}
\def \btm {\mbox{{\small $\beta_{max}$}}}
\def \px {\mbox{$P(\X)$}}


\begin{document}



\begin{center}

{\Large {\bf Data Driven Regional Weather Forecasting: Example using the Shallow Water Equations}}\\
\vspace{0.25in} 

Randall Clark, Luke C. Fairbanks~\footnote{Corresponding Author: lfairban@ucsd.edu}, Ramon E. Sanchez, Pacharadech Wacharanan,\\
Department of Physics\\
University of California San Diego\\
La Jolla, CA 92093, USA\\
\bigskip
\bigskip 

Henry D. I. Abarbanel\\
%\vspace{0.25in}
Department of Physics\\
and\\
Marine Physical Laboratory (Scripps Institution of Oceanography)\\
University of California San Diego\\
La Jolla, CA 92093,USA\\
habarbanel@ucsd.edu\\
%https://orcid.org/0000-0002-4690-6081 \\
https://orcid.org/0000-0002-4690-6081\\
\vspace{0.25in}
\today\\
\vspace{0.2in}
\end{center}

\newpage
%{\small
%\tableofcontents
%\listoffigures}
%\newpage

\section*{Abstract}

Using data alone, without knowledge of underlying physical models, nonlinear discrete time regional forecasting dynamical rules are constructed employing well tested methods from applied mathematics and nonlinear dynamics. Observations of environmental variables such as wind velocity, temperature, pressure, etc allow the development of forecasting rules that predict the future of these variables only. A regional set of observations with appropriate sensors allows one to forgo standard considerations of spatial resolution and uncertainties in the properties of detailed physical models. Present global or regional models require specification of details of physical processes globally or regionally, and the ensuing, often heavy, computational requirements provide information of the time variation of many quantities not of interest locally. In this paper we formulate the construction of data driven forecasting (DDF) models of geophysical processes and demonstrate how this works within the familiar example of a ‘global’ model of shallow water flow on a mid-latitude $\beta$ plane. A sub-region, where observations are made, of the global flow is selected. A discrete time dynamical forecasting system is constructed from these observations. DDF forecasting accurately predicts the future of observed variables. %We include a detailed discussion of the DDF protocols.
 
\newpage
\section{Introduction}


%Review~\cite{tang98}

In geophysical models for global and regional weather or climate forecasting, solutions of the Navier-Stokes equations, expressing conservation of mass, and momentum, are accompanied by a thermodynamic equation describing energy conservation. An equation of state relating the thermodynamic state variables to each other is required as are further parameterizations to represent unresolved Physics below the model grid scale. Cloud moisture dynamics is a critical example of the latter. 

Numerical solutions for these partial differential equations (PDEs) using, for example, a finite difference method~\cite{press07,olver2020} lead to formulations with a very large number of ordinary differential equations (ODEs) at a global set of spatial grid points. 
In global models the number of these degrees of freedom (ODEs) may range from 10$^8$ to 10$^{10}$ while for regional models this may still be large, say 10$^6$ to 10$^7$. Even then the resolution of the largest operational GCMs is today about 9 km in the horizontal~\cite{stevens2019}. Models with scales down to 1.4 km are being developed and tested~\cite{wedi20}, and the computational challenges grow as these are realized.

If one's interest is in forecasting the weather or climate only in a selected region, another point of view may be employed. This builds the relevant dynamics using only observations of the state variables (pressure, velocities, temperatures, ...) one wishes to forecast. Knowledge of the forcing of the system at the location of the observations is required, but this is already estimated in the formulation of the global models~\cite{echam5,ecmwfbasics}.

Working from data alone avoids uncertainties in initial conditions for the ODEs, uncertain physical features of the models and their boundary conditions, and the like. It also circumvents the growing computational complexity as the spatial resolution of big models is increased.

Not surprisingly, one loses something in a formulation that bypasses knowledge of the fundamental physical dynamical equations of the problem. %The most striking item one relinquishes is detailed knowledge of the state of the atmosphere or ocean at points where observations are not available. 
At the same time the computations for forecasting observables only is enormously simplified compared to calculating a full set of physical properties in a region or globally.

The subject of this paper is a method, which we call Data Driven Forecasting (DDF).  It is a combination of applied mathematical tools that were well developed some time ago~\cite{hardy71,micc86,broom88,schab95,powell02,buhmann09} with the essential Physics of how one can learn properties of nonlinear dynamical systems through observations of a subset of the dynamical variables of that system~\cite{takens81,eckmann85,embed91,abar96,kantz04}.

\subsection{Recent Work in Using Machine Learning Methods in Weather and Climate Modeling}

The interest in using data driven modeling in earth systems problems is both intense and productive. It is well documented in a recent theme issue of the Proceedings of the Royal Society A entitled ``Machine Learning for weather and climate modeling'' (The articles can be accessed directly at www.bit.ly/TransA-2194)~\cite{roysoc2021}


%Currently there is a large amount of interest in applying machine learning techniques to geophysical  problems. 

Efforts are directed toward replacing large numerical weather forecasting GCMs with data trained networks (machine learning or ML)~\cite{shi2015convolutional}~\cite{comrie1997comparing}~\cite{krasnopolsky2002neural}~\cite{geer2021}~\cite{dueben2018challenges}. While there is success in these investigations, there remains hesitancy in the climate and weather forecasting community to openly accept these methods as they are viewed as black boxes that have tenuous relation to the physics that they model other than the data that is provided to them~\cite{schultz2021can}. It is for reasons like this that there have been efforts in ML research to investigate methods with a large focus on the efficacy and trustworthiness of ML tools~\cite{mackowiak2021generative}~\cite{huang2020survey}. The ML community in weather forecasting have gone an additional step further by implementing hybrid models that are machine learning devices that include physical constraints in some form~\cite{wandel2020learning}~\cite{grover2015deep}~\cite{daw2017physics}. There have been studies showing that weather forecasting tools that account for physical constraints (typically in their loss functions) show improvement over ML tools that utilize restricted knowledge of the underlying physics~\cite{kashinath2021physics}. In summary, there is both an interest and benefit to the hybridization of physics and ML models~\cite{dwp2021,bocquet2021}.

In this paper we follow a complementary but different path in realizing nonlinear, dynamical forecasting rules based on observed data alone. We call it data driven forecasting or DDF. We argue that the DDF modeling strategy we describe in this paper embodies the hybridization of physics and ML in a way not typically performed to reap the forecasting benefits while maintaining a high degree of transparency in how it actually operates.

DDF, as will be shown in its formulation and developed explicitly in a simple, familiar geophysical model incorporates both the physics of the underlying model and ML tools to create an update rule for forecasting. Unlike many hybridization ML models, it does not enforce a physics constraint in its training function. The training of the ML model proceeds as standard regularized ridge regression. The update rule for DDF is a sum of radial basis functions and physically inspired terms which include, but are not limited to, forcing terms and polynomial variables in the original model. The separation of the forcing of the intrinsic geophysical fluid dynamics is both a feature of the underlying fluid dynamical equations and makes testing of the generalization to innovative forcing straightforward. 

As we use DDF in this paper, a subset of the degrees of freedom of a `global' model, which for purposes of illustration and simplicity is taken as a one layer shallow water flow on a $\beta$-plane, are considered observed. Then using the familiar method of time delay embedding~\cite{takens81,abar96}, we introduce the physics of the global flow into the regional forecast. It is clear that this paper represents a proof of principle in using DDF and  addressing how the method scales to large, increasingly realistic models is still to be accomplished. In using the DDF forecasting formulation on field data there is a requirement for comparison of computational complexity to mainstream GCMs that is an important part of continuing work.

The use of the tools from nonlinear dynamics employed in our work, in particular the time delay embedding method for building model appropriate state spaces when only a subset of the state variables is observed~\cite{takens81,abar96,kantz04}, have been the subject of many years of basic and applied studies for a number of years. One goal of this paper is to demonstrate how this view can lead to accurate regional forecasting without the need for solutions of large GCMs. This paper may be viewed a only a start, a proof of principle actually, in that interesting direction.

Regional models of different types have been investigated in the past~\cite{knighton2019potential,nieves2021predicting,xiao2019spatiotemporal,kartal2023assessment}. There also have been other phase space embedding approaches shown to be useful when only a subset of the dynamical variables are observed~\cite{ouala2020learning,gottwald2021combining,young2022deep,abar96}
%,kazem2013support,frank2010activity} 
that perform a similar task of reconstructing the dynamics of their respective system. In the context of the SWE we seek to compare our application of DDF to models already applied to the SWE~\cite{yildiz2021learning,chen2022predicting} and the results we show later, while not an apples to apples comparison as the specific SWE data sets differ, show DDF's resiliency to noise, its accuracy over time, and adaptability to study under reduced dimensional observations.



\subsection{Plan for this Paper}
\begin{enumerate}
\item To begin we describe the DDF method and its forecasting goals in the context of a simple fluid dynamics problem of geophysical interest: shallow water flow (SWE)~\cite{jiang94,pedlosky1986,Vallis17}. 

\item The discussion of the SWE example illustrates the key ideas in a concrete example and provides us the basis for a general consideration of the problem of learning from observations alone how to forecast the future development of those observations. 

\item We then return to the SWE example to show explicitly how the DDF method is implemented and performs.

\item A Summary and Discussion completes the main body of the paper.

\item An Appendix contains many of the details describing how one estimates the parameters in a DDF model. The discussion is more general than the simple geophysical examples in the body of the paper and may be widely useful beyond this paper.



\end{enumerate}

\section{Shallow Water Flow on a $\beta$ plane used to illustrate DDF}

To illustrate the main ideas in this paper we begin with a familiar example which, though simple, illustrates the ideas in a useful context. The example is shallow water flow on a $\beta$ plane, and this is introduced as the {\it global} model of a geophysical flow. This is two dimensional flow with state or dynamical variables of velocities in the x and y direction and the height of the fluid: $\{v_1(\r,t),v_2(\r.t),\zeta(\r,t)\} = \{u(\r,t),v(\r,t),\zeta(\r,t)\}$. $\r = \{x,y\}$. The partial differential equations of shallow water flow are solved on a two dimensional global grid in a finite difference approximation for the state variables $\{v_1(i,j,t),v_2(i,j,t),\zeta(i,j,t)\}$ with the integers $(i,j)$ denoting locations on the grid. 

We then select a subgrid as the region where we record observations of the state variables $\{v_1(I,J,t),v_2(I,J,t),\zeta(I,J,t)\}$ with the integers $(I,J)$ denoting locations on the regional subgrid. 

In this paper we address how data from these regional measurements alone, without knowledge of the underlying dynamical equations or knowledge of the states of the global system outside the subgrid, can allow us to forecast the states of the regional variables $\{v_1(I,J,t),v_2(I,J,t),\zeta(I,J,t)\}$.


In applying the methods developed here one needs only measurements of the state variables we wish to forecast at selected spatial locations in a sub-region. There is no `grid' where we must place the sensors for the desired observations. 

We do not require information about the state of the system outside the region. The forcing of the fluid in the selected region is required. That is already in the global formulation of the problem~\cite{echam5,ecmwfbasics}. %With these observed data, we construct a discrete time dynamical forecasting system for the observed states alone.
\subsection{Fluid Equations on a Grid; The Global Model}

We initiate our considerations with the one layer Shallow Water Equations (SWE) on a mid-latitude $\beta$ plane~\cite{jiang94,pedlosky1986,Vallis17}.  The plane has coordinates $\r= \{x,y\}$, and the dynamical variables of the flow are the velocities in the x and the y directions $\{v_1(\r,t) = u(\r,t), v_2(\r,t) = v(\r,t)\}$ and the fluid height $\zeta(\r,t)$. $\nabla_{\perp} = (\frac{\partial \blank}{\partial x},\frac{\partial \blank}{\partial y})$. 

The SWE take the form
\bea 
&& \frac{\partial \zeta(\r,t)}{\partial t}  + \nabla_{\perp}(\zeta(\r,t)\v(\r,t)) = 0 \nonumber %-\hat{z}\cdot curl [\frac{\btau(\r,t)}{f(\r)}]   
 \\
%&& \frac{\partial T(\r,t)}{\partial t} +  \nabla_{\perp}(T(\r,t)\v(\r,t) = \kappa \nabla^2_{\perp} T(\r,t) + Q(\r,t), \nonumber \\
&& \frac{\partial \v(\r,t)}{\partial t} + \v(\r,t)\cdot \nabla_{\perp} \v(\r,t) + f(\r) \hat{z} \times \v(\r,t) = -g \nabla_{\perp} \zeta(\r,t))  \nonumber \\
&& + A \nabla^2_{\perp} \v(\r,t) - \eps \v(\r,t) + {\cal F}(\r,t).
\label{globalswe1}
\eea
The pressure is given by the hydrostatic relation $p(\r,z,t) = g\rho(\zeta(\r,t) - z); p(\r,\zeta(\r,t), t) = 0$. Body forces driving the fluid are ${\cal F}(\r,t) = [{\cal F}_1(\r,t), {\cal F}_2(\r,t)].$

We solve the SWE on a $n_x \times n_y$ grid $\{x = x_0 + i \Delta x, y = y_0 + j \Delta y\};\;i = 0,1,..., n_x -1;\;j = 0,1,..., n_y-1$ using, for example,  periodic boundary conditions
\be 
v(0,y,t) = v(L_x,y,t) ;\;u(x,0,t) = u(x,L_y,t);\;
L_x = n_x \Delta x;\;L_y = n_y \Delta y.
\ee

The fluid density is constant and chosen to be $\rho = 1 kg/m^3$. $f(\r) = f_0 + \beta y$ is the local rotation of the earth, A is an effective kinematic viscosity, $\eps$ is a Rayleigh friction coefficient.



The solution on this grid comprises our {\em global} dynamics. There are $D_G = 3( n_x \times n_y)$ ODEs for the states $\S(i,j,t) = \{v_1(i,j,t),v_2(i,j,t),\zeta(i,j,t)\}$:
\be
\frac{d\S(i,j,t)}{dt} = \F_{i,j}(\S(i,j,t), \theta) + [{\cal F}(i,j,t),0].
\label{global}
\ee
$\theta$ are fixed parameters in the global system. $\F_{i,j}(\S, \theta)$ is the vector field of the $D_G$ (global) nonlinear differential equations for the shallow water flow. 

\subsection{Dynamics on a Subgrid; The Regional Model}

Next select a subgrid, {\bf our region}, where we make measurements. The locations in the observation region are denoted by $\R = \{X_0 + I \Delta x, Y_0+  J \Delta y\};\;I = 0,1,2,...,N_x-1, J = 0,1,2,...,N_y-1\}$; $N_x \le n_x, N_y \le n_y$. It is in this region that we collect observations at a subset of the full complement of state variables ${\S}(\r,t)$. 

These observations 
are $\O(\R,t) = \O(I,J,t) = \{v_1(I,J,t), v_2(I,J,t), \zeta(I,J,t)\}$
which satisfy
\bea
&&\frac{d \O(\R,t)}{dt} = F_{\R}({\S}(\r,t),\theta) + [{\cal F}(\R,t),0], \nonumber \\
&&\frac{d \O(I,J,t)}{dt} = F_{I,J}({\S}(i,j,t),\theta) + [{\cal F}(I,J,t),0]
\label{regional}
\eea
The number of regional observed variables is $D_R = 3 (N_x \times N_y) \le D_G$. $F_{\R}({\S}(\r,t),\theta) = F_{I,J}({\S},\theta)$ is the (global) vector field restricted to the region $\R$. It is a function of the states of the global dynamics $\S(i,j,t)$.

Using data from observations on the $\O(\R,t)$, without knowledge of the vector field $F_{I,J}({\S}(i,j,t),\theta)$, we want to construct a discrete time dynamical rule which takes $\O(\R,t)$ forward in time; $\O(\R,t) \to \O(\R,t + \Delta t) = \O(\R,t + h)$. This discrete time dynamical map is our forecasting system for the region. 

Observations are made at $N_O$ times $t_n = t_0 + n h; n = 0,1,..., N_O-1$ giving us $\O(\R,t_n) = \O(\R,n)$ at all those times. These observations form a trajectory in $D_R \le D_G$ dimensional space. We are interested in the situation where $D_R < D_G$, giving us observations in a subregion of the global dynamics. For purposes of explaining the steps in the construction of a regional forecasting system we will first consider the case $D_R = D_G$. A return to $D_R < D_G$ will follow that discussion.

Now we integrate the regional dynamical equation Eq. (\ref{regional}) over the interval $[t_n, t_n + h] = [t_n, t_{n+1}]$. This gives us the {\it flow} of the $D_R$ dimensional dynamical system which we find to be
\bea
&&\O(\R,t_n+h) = \O(\R, n+1) = \O(\R,n) \nonumber \\
&&+ \int_{t_n}^{t_n + h} dt' \, \biggl \{F_{\R}({\S}(\r,t'),\theta) + [{\cal F}(\R,t'),0] \biggr \},
\label{flow}
\eea

As we do not know the vector field  $F_{I,J}({\S}(\r,t),\theta)$, we must ${\bf represent}$ the integral over it in some manner. The integral over the external forces on the fluid in the subregion, $\R$, $[{\cal F}(\R,t),0]$ we can approximate, because, to solve the original global problem, we were required to specify how the fluid was driven both regionally and globally~\cite{echam5,ecmwfbasics}. The forcing of the fluid is an aspect of the flow that is not intrinsic to the fluid properties themselves, and it is important to observe that this external forcing is additive. Essentially these are just Newton's equations of motion.

Using whatever approximation to the integral over $\F_{I,J}({\S}(\r,t'),\theta)$ one wishes~\cite{olver17} that establishes it as dependent on $\S(\r,t)$, so the resulting dynamical rule for moving forward in time is explicit, we arrive at a representation of the flow as the discrete time map ($1 \le i \le n_x, 1 \le j \le n_y:\; 1 \le I \le N_x \le n_x, 1 \le J \le N_y \le n_y.$)
\bea
&&\O(I,J,n+1) = \O(I,J,n) + \f_{I,J}(\S(i,j,n), \bchi) \nonumber \\
&&+ \frac{h}{2} \biggl[ [{\cal F}(I,J,n),0] + [{\cal F}(I,J,n+1),0] \biggr ].
\label{discrete}
\eea 
%{\bf Nota bene.  The dynamical map could be implicit--again refer to Olver--then each time step $\O(\R,t) \to \O(\R,t + \Delta t)$ involves solving a nonlinear problem. We have not explored this alternative. This stuff goes in the discussion}

Although we do not use it in our construction of DDF models, we could have chosen Equation (\ref{discrete}) to have $\S(i,j,n+1)$ on the right hand side as we represent the integral over the vector field $\F_{I,J}(\S(i,j,t))$ in Equation (\ref{flow})~\cite{olver17}. The dynamical discrete time map would them be {\it implicit} and using it would require the solution of a nonlinear problem at each step.

The trapezoidal rule was used to approximate the integral over the known forces which should be quite adequate as the measurements are known only at intervals of size h. The $\bchi$ are constants to be estimated in the representation of the flow of the discrete time dynamics. 
We call $\f_{I,J}(\S(i,j,n),\bchi)$ the vector field of the discrete time flow restricted to the regional variables. 


\begin{table}
\begin{centering}
\begin{tabular}{|c|c|c|} \hline
Parameter & Symbol & Value \\
\hline
Fluid Density & $\rho$ &1 kg/m$^3$\\
Coriolis Parameter & $f_0 + \beta y$ & $f_0 = 10^{-5} 1/s$\\
& & $\beta = 10^{-12} 1/m\,s$ \\
Effective Viscosity& A& 100 m$^2$/s \\
Rayleigh friction  & $\eps$ &10$^{-8}$ 1/s \\
Gravity &g&9.8 m/s$^2$ \\
Time Step & h& 6 min\\
Resting Fluid Depth& $H_0$& 50 m\\
Grid Resolution &$\Delta x = \Delta y$ & 100 km \\
Domain Extent &$L_x = n_x \Delta x; L_y = n_y \Delta y$ & \\
Body Forcing & [${\cal F}_1(\r,t), {\cal F}_2(\r,t)]$ &  $[F_1(\r,t), 0]  \; m/s^2$\\
&  $F_1(\r,t) = - F_0 \cos(\frac{2 \pi y}{L_y})$ & $F_0 = 10^{-5} m/s^2$\\
Number of Centers & $N_c$&1000\\
\hline
%\label{params}
\end{tabular}
\caption{Table of Parameters and Symbols in the shallow water flow.  For the RBF representation we have additional parameters $\bchi$ used in the solution to the DDF flow vector field. These depend on the particular problem, e.g. which subregion one selects. The $\bchi$ include the RBF Shape Factor, $\sigma$,  the time Time Delay $\tau = T_h h; (T_h = \mbox{integer})$,  the Embedding Dimension, $D_E$, and the Tikhonov Regularization Parameter, B. We have used $N_c$ = 1000.}
\end{centering}
\end{table}


\subsection{Proceeding in Two Steps}

As noted earlier, we proceed now in two steps:
\begin{enumerate}
\item First we select $D_R = D_G$ putting us in the situation where we observe all components of the global dynamics, but in which we do not know the equations of these global dynamics but only have the data.

\item Second we move to the case of interest in this paper when $D_R < D_G$ where we are also required to find a way to introduce information about the global dynamical flow into our rule $\O(\R,t) \to \O(\R,t + \Delta t)$, Equation (\ref{discrete}).


\end{enumerate}

\subsubsection{First Step, $D_R = D_G$; Introducing Radial Basis Functions}

This is the case where we observe all of the global set of state variables ${\S}(\r,n)$. This is not of central interest to us, but it is nonetheless very instructive for making our next step.

The discrete time dynamics appears in this instance as 
\bea
&&{\S}(i,j,n+1) = {\S}(i,j,n) + \f_{i,j}(\S(r,n), \bchi) \nonumber \\
&&+ \frac{h}{2} \biggl[ [{\cal F}(\r,n),0] + [{\cal F}(\r,n+1),0] \biggr ],
\eea
and we need a representation of the discrete time vector field $\f_{i,j}(\S(i,j,n), \bchi)$ of the flow.

This flow vector field $\f({\S}, \bchi)$ is a set of $D_G$ functions on the $\S \in {\cal R}^{D_G}$. There are many ways to represent such functions of many variables. We can think of the observed samples as points of information about a distribution $\f(\S, \bchi)$ and ask that the representation give us an interpolating function among the observed point locations $\S(\r,t_n) = \S(\r,n)$.

We select a method of representing this function using radial basis functions (RBFs)~\cite{hardy71,micc86,broom88,schab95,powell02,buhmann09}. In using this method we use a K-means clustering algorithm~\cite{du2006neural} to select  $N_c $ of all the $N_O$ observed locations in $\cals$ space. These are called centers. The flow vector field is then given in components by
\be
f_a(\cals,\bchi) = \sum_{k=1}^K c_{ak} p_k(\cals) + \sum_{q=1}^{N_c} w_{aq} \bpsi((\cals - \cals(q))^2, \sigma), 
 a = 1,2,...,D_G 
%&& \mbox{and we impose the constraints~\cite{schab95}} \nonumber \\
%&& \sum_{q=1}^{N_c} w_{aq} p_k(\cals(q)) = 0; \;\; k = 1,2, ..., K
\label{rbfrep1}
\ee

In this $p_k(\cals)$ is a polynomial of degree k. The radial basis 
functions $\bpsi((\cals - \cals(q))^2, \sigma)$ are sensitive to the distribution samples, the centers $\cals(q)$, over a range $\sigma$. 

There are a multitude of choices for the RBF $\bpsi((\cals - \cals(q))^2, \sigma)$, and we have investigated the use of two. The Gaussian
\be
\bpsi_G((\cals - \cals(q))^2, \sigma) = \exp[-R(\cals - \cals(q))^2];\; R = \frac{1}{2\sigma^2},
\ee
and the multiquadric of Hardy~\cite{hardy71}
\be
\bpsi_{MQ}((\cals - \cals(q))^2, \sigma) = \sqrt{(\cals - \cals(q))^2 + \sigma^2}.
\ee

%The representation Eq. (\ref{rbfrep}) was developed to be an accurate interpolating function among the observations $\cals(n)$ is the space of the states $\S$. 
The constants $\bchi = \{c_{ak},w_{aq}\}$ are determined by the linear algebra problem
\bea
&&S_a(n+1) = S_a(n) + \sum_{k=1}^K c_{ak} p_k(\cals(n)) + \sum_{q=1}^{N_c} w_{aq} \bpsi((\cals(n) - \cals(q))^2, \sigma) \nonumber \\
&&+ \frac{h}{2} \biggl[ [{\cal F}(\r,n),0] + [{\cal F}(\r,n+1),0] \biggr ], 
\eea
for $n = 0,1,2,...,N_O - 1$ and $a = 1,2, ..., D_G$.

\subsubsection{Second Step, $D_R < D_G$; Utilizing Time Delay Embedding}

Turning back to the question of developing a dynamical map for the regional subset $\O(\R,n)$ of our dynamical variables, we see that the observations are a projection from dimension $D_G \to D_R < D_G$. To proceed we require a space of state variables equivalent to the full state space of the $\S(\r,t)$, so we must `unproject' the $\O(\R,t)$ to a space equivalent to $\S(\r,t)$.

A dynamical method for accomplishing this is well analyzed in the nonlinear dynamics literature. It rests on the fact that as the observed quantities move from some time $t - \tau$ to time t, they depend on all of the state variables $\S(\r,t)$ as seen in Equation (\ref{discrete}). Using time delays of the observed regional variables provides us the desired information on the unobserved state variables.

This suggests creating a $D_E$-dimensional time delay embedding space~\cite{takens81,eckmann85,embed91,abar96,kantz04} with vectors of dimension $D_{TD} = D_R D_E$  {\small
\be
\TD(t) = [\O(\R,t),\O(\R,t- \tau), \O(\R,t-2\tau),...,\O(\R,t - (D_E-1)\tau].
\label{TDdef}
\ee
}
This vector of time delays depends only on the observed quantities in the region labeled by $\R$ and their time delays. It is through those time delays that $\TD$ inherits information about the dynamics outside the region $\R$.

Using this time delay vector in the observed dynamics gives us 
\bea
&&\O(\R,n+1) = \O(\R,n) + \nonumber \\
&& \f_{\R}(\TD(n), \bchi) + \frac{h}{2} \biggl[ [{\cal F}(\R,n),0] + [{\cal F}(\R,n+1),0] \biggr ].
\label{discretetd}
\eea 

It is useful to write this in components. The $\O(\R,t)$ are $D_R$ dimensional $\O(\R,t) = \{O_{\alpha}(t)\};\;\alpha = 1,2,...,D_R$.  The dynamical map for the regional observables becomes
\bea
&&O_{\alpha}(n+1) = O_{\alpha}(n) + \nonumber \\
&&f_{\alpha}(\TD(n),\bchi) + \frac{h}{2} \biggl[ [{\cal F}(\R,n),0] + [{\cal F}(\R,n+1),0] \biggr ]_{\alpha}.
\label{obsmap}
\eea

This informs us that we will need a representation for each of the $D_R$ components of the observed regional variables. The DDF parameters we need to estimate using the observed data now include $\bchi = \{w_{\alpha q}, c_{\alpha j}, \sigma, B, D_E, T_h\}$.



\section{Estimating $\bchi$ by Minimizing C($\bchi$)}



The $\O(\R,t)$ are $D_R$ dimensional $\O(\R,t) = \{O_{\alpha}(t)\};\;\alpha = 1,2,...,D_R$.  The dynamical map for the regional observables is
{\small 
\bea
&&O_{\alpha}(n+1) = O_{\alpha}(n) + \nonumber \\
&&f_{\alpha}(\TD(n),\bchi) + \frac{h}{2} \biggl[ [{\cal F}(\R,n),0] + [{\cal F}(\R,n+1),0] \biggr ]_{\alpha} \nonumber \\
&&\TD(t) = [\O(\R,t),\O(\R,t- \tau),...,\O(\R,t - (D_E-1)\tau].
\label{obsmap1}
\eea}

To estimate the parameters $\bchi$ in the DDF forecasting function, we form the cost function $C(\bchi)$
and minimize this objective function with respect to the elements of $\bchi = \{w_{\alpha q},c_{\alpha j}, R, B, \tau = hT_h, D_E\}$.
\bea
&&
C(\bchi) = \sum^{N_O}_{N_c+1} \biggl \{[\O(n+1) - \O(n) - 
\f(\TD(n),\bchi) \nonumber \\
&&- \frac{h}{2} \biggl[ [{\cal F}(\R,n),0] + [{\cal F}(\R,n+1),0] \biggr ] \biggr \}^2.
\label{costchi1}
\eea

In Equation (\ref{costchi1})
\be
f_{\alpha}(\TD(n),\bchi) = \sum_{q=1}^{N_c} w_{\alpha q} \bpsi((\TD(n)-\TD^c(q))^2, \sigma) + \sum_{j=1}^{D_R} c_{\alpha j} \O_j(n)
\label{rbfrep22}
\ee


Even though we can choose any RBF for $\bpsi$, the $C(\bchi)$ is always linear in the weights $w_{\alpha q}, c_{\alpha j}$, enabling the use of the linear algebra of Ridge Regression or Tikhonov regularization~\cite{press07} in estimating them.



The other elements of $\bchi$ enter the minimization of $C(\bchi)$ nonlinearly.  There are many excellent ways to search for values for these using ideas noted in the Appendix. We utilized a rather coarse grained grid search to identify good forecasts, then refined the search in those regions. This works well, as the results we present below demonstrate. Nonetheless, we recommend more efficient methods. Again, see the Appendix.




 % fig 1
\begin{figure}
    \centering
    \includegraphics[width=1.05\linewidth,height=0.75\linewidth]{fig1.jpg}
%\includegraphics[width=0.49\linewidth,height=0.45\linewidth]{regionalddfforecasting10x10.jpg}
%\includegraphics[width=0.49\linewidth,height=0.45\linewidth]{10x10random10.jpg}
%\includegraphics[width=0.49\linewidth,height=0.45\linewidth]{regional2x2offcenter.jpg}
 \caption{Overall Scheme of Regional DDF Forecasting in the context of the SWE. Red dots are grid points of the overall dynamical system, Eq. (\ref{global}). Blue dots are the location of the``regional sensors" where fluid height $\zeta(\r,t)$ and fluid velocity $\v(\r,t) = \{v_1(\r,t), v_2(\r,t) \}$ are recorded. If there are $N_R$ regional sensor points then there are $D_R = 3N_R$ regional measured time series. The spatial locations for the sensors are denoted $\R$; these are the blue locations on the grid. In this Figure $n_x \times n_y = 30;\;N_R = 7$. In this example there would then be $D_R = 3N_R = 21$ observed regional time series out of $D_G = 3 (n_x \times n_y) = 90$ global time series. $D_R < D_G$.}
   \label{overall1}
\end{figure}
\newpage

%\section{General Formulation}

%State variables measured at each regional location $D_S$, 
%$D_R = D_S(N_x N_y)$ and total state variables of the problem $D_s$ means $D_G = D_s(n_x  n_y)$




\section{Results from the Example of the SWE on a $\beta$ Plane} \label{results}



\subsection{Clustered Sensor Region; 3x3 Corner}

%fig 2
\begin{figure}
    \centering
  %  \includegraphics[width=1.05\linewidth,height=0.75\linewidth]{regionalddfforecasting10x10.jpg}
\includegraphics[width=1.05\linewidth,height=0.75\linewidth]{regionalddfforecasting10x10.jpg}
%\includegraphics[width=0.49\linewidth,height=0.45\linewidth]{10x10random10.jpg}
%\includegraphics[width=0.49\linewidth,height=0.45\linewidth]{regional2x2offcenter.jpg}
    \caption{Shallow Water Equations on a $ (n_x =10) \times (n_y = 10)$ grid. $D_G = 3(n_x \times n_y)$. Red dots are grid points of the overall global dynamical system, Eq. (\ref{global}), a set of Shallow Water Equations on a $\beta$ plane. Blue dots are the location of the "regional sensors" where fluid height $\zeta(\r,t)$ and fluid velocity $\v(\r,t)$ are recorded. There are  $(N_x = 3) \times (N_y = 3) = 9$ regional sensor locations in this example, and $D_R = 3(N_x \times N_y) = 27$ measured time series. The spatial locations of the sensors are denoted $\R$; these are the blue locations on the grid. In this Figure $n_x \times n_y = 100$, and $D_G = 3(n_x \times n_y) = 300$. We have 27 observed regional time series out of 300 global time series. $D_R < D_G$.}
    \label{overall}
\end{figure}

%fig 3
\begin{figure}
 \centering
    \includegraphics[width=0.47\linewidth,height=0.45\linewidth]{swe3x3corneru11.jpg}
     \includegraphics[width= 0.47\linewidth,height=0.45\linewidth]{swe3x3cornerv11_2.jpg}
         \includegraphics[width=0.51\linewidth,height=0.45\linewidth]{swe3x3cornerz11.jpg}
  %\includegraphics[width=0.47\linewidth,height=0.45\linewidth]{u11swe10x103x3cornerde_10.jpg}
        \includegraphics[width=0.47\linewidth,height=0.45\linewidth]{u11swe10x103x3cornerde_15.jpg}
 \caption{ SWE on a $10 \times 10$ grid. The sensor region is comprised of 9 locations, blue dots in Figure (\ref{overall}), in a 3x3 corner location. The time delay parameters here are $D_E = 20, \tau = 20 \Delta t$ with $\Delta t = h = 0.1 hr$. {\bf Top Left Panel} We display the data and the DDF forecast for the x-velocity $u(1,1,t)$.   {\bf Top Right Panel} We display the data and the DDF forecast for the y-velocity $v(1,1,t)$.  {\bf Bottom Left Panel} We display the data and the DDF forecast for the y-velocity $\zeta(1,1,t)$. The fluid rest height is $H_0 = 50$m. {\bf Bottom Right Panel } This shows the result for DDF forecast for the x-velocity $u(1,1,t)$ with $D_E = 15$. Contrast this with the result in the {\bf Top Left Panel} where $D_E$ = 20.}
\label{3x3u11}
\end{figure}





% fig 4
%\begin{figure}
 %\centering
%   \includegraphics[width=1.05\linewidth,height=0.75\linewidth]{swe3x3cornerv11.jpg}
 %\caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 9 locations, blue dots in Figure (\ref{overall}), in a 3x3 corner location. We display the data and the DDF forecast for the y-velocity $v(1,1,t)$.  The time delay parameters here are $D_E = 20, \tau = 20 \Delta t$ with $\Delta t = h = 0.1 hr$.}
%    \label{3x3v11}
%\end{figure}

% fig 5
%\begin{figure}
% \centering
%    \includegraphics[width=1.05\linewidth,height=0.75\linewidth]{swe3x3cornerz11.jpg}
 %\caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 9 locations, blue dots in Figure (\ref{overall}), in a 3x3 corner location. We display the data and the DDF forecast for the fluid height $\zeta(1,1,t)$.  The fluid rest height is $H_0 = 50$m. The time delay parameters here are $D_E = 20, \tau = 20 \Delta t$ with $\Delta t = 0.1 hr$.}
 %   \label{3x3z11}
%\end{figure}

%fig 6
%\begin{figure}
 %\centering
%    \includegraphics[width=0.95\linewidth,height=0.6\linewidth]{u11swe10x103x3cornerde_10.jpg}
      %  \includegraphics[width=0.95\linewidth,height=0.6\linewidth]{u11swe10x103x3cornerde_15.jpg}
      %\includegraphics[width=0.95\linewidth,height=0.6\linewidth]{z11swe10x103x3cornerde_10.jpg}
%includegraphics[width=0.49\linewidth,height=0.45\linewidth]{swe3x3cornerv11.jpg}
%\includegraphics[width=0.49\linewidth,height=045\linewidth]{swe3x3cornerz11.jpg}
% \caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 9 locations, blue dots in Figure (\ref{overall}), in a 3x3 corner location. We display the data and the DDF forecast for the x-velocity $u(1,1,t)$.   $\tau = 20 \Delta t$ with $\Delta t = h = 0.1 hr$. {\bf Top Panel}  $D_E = 10$ {\bf Bottom Panel}  $D_E = 15$. The results with $D_E = 20$ are in Figure (\ref{3x3u11}).}
   % \label{3x3u11de1015}
%\end{figure}



% fig7

%\begin{figure}
% \centering
% \includegraphics[width=0.95\linewidth,height=0.6\linewidth]{z11swe10x103x3cornerde_10.jpg}
%  \includegraphics[width=0.95\linewidth,height=0.6\linewidth]{z11swe10x103x3cornerde_15.jpg}
% \caption{{\small SWE on a $10 \times 10$ grid. The sensor region is comprised of 9 locations, blue dots in Figure (\ref{overall}), in a 3x3 corner location.  We display the data and the DDF forecast for the fluid height $\zeta(1,1,t)$. The fluid rest height is $H_0 = 50$m. $ \tau = 20 \Delta t$ with $\Delta t = h = 0.1 hr$. {\bf Top Panel} $D_E = 10$ {\bf Bottom Panel}  $D_E = 15$.} The results with $D_E = 20$ are in Figure (\ref{3x3z11}).}
 %   \label{3x3z11de1015}
%\end{figure}

The shallow water equations were solved using the method of Sadourny~\cite{sadourny75} with forcing, viscous dissipation, Coriolis forces, and Rayleigh friction as indicated in Equation (\ref{globalswe1}). We used periodic boundary conditions.

%The parameters in the SWE dynamics were selected to be $\Delta t = 0.1 hr$, A = 100 $m^2/s$, $\eps$ = 10$^{-8}$ 1/s, 
%$f_0$ = 1.0 x 10$^{-5}$ 1/s, $\beta$ = 1.0 x 10$^{-12}$ 1/(m s). The forcing was taken to be ${\cal F}(\r,t) = [-F_0 \cos(\frac{2 \pi r_2}{L_y}), 0]$ and $F_0$ = 1.0 x 10$^{-5}$ $m^2/s$. $\Delta x$ = $\Delta y =100 km$, so $L_x = L_y = 1000 km$ in this calculation. As $H_0$ is 50 m, this is certainly shallow water.

The global grid is $n_x = n_y =10$, so we solved 300 ODEs to generate the time series for the state variables $\S(\r,t)$ on this grid. A regional sub grid with $N_x = N_y = 3$ was selected, and on this regional subgrid we `measured' $\{u(\R,t), v(\R,t), \zeta(\R,t)\}$ to form the $D_R = 27$ dimensional observation vectors $\O(\R,t)$. The regional sensor locations are shown in Figure (\ref{overall}). We generated $N_O = 15,000$ time steps of size 0.1 hr. $N_c = 1000$ centers were selected from these data.These were used in a Polynomial plus Gaussian RBF, Equation (\ref{rbfrep1}). 1000 hr of these data were used to train the RBF representation of the discrete time flow vector field, the 500 hr of forecasts were made of the 9 regional state variables.



In the estimation of the parameters we found B = 1.0 x10$^{-9}$, R = 
1.0 x 10$^{-6}$, so $\sigma \approx 707$, $D_E = 20$ and $\tau = 20 h = 2 hr$ gave the best forecasts. See Figure (\ref{3x3u11}) for the data and the DDF Forecast for $u(1,1,t), v(1,1,t),
 \mbox{and} \, \zeta(1,1,t)$ with $D_E = 20$.

We note that values of the embedding dimension $10 \le D_E \le 20$ gave forecasts of more or less equal quality. In Figure (\ref{3x3u11}) we display the x-velocity $u(1,1,t)$ for $D_E = 10$ and $D_E = 15$ respectively.

%For comparison, the RMS error in the DDF forecast of $u(1,1,t)$ for $D_E = 10$ is 0.034 m/s, for $D_E = 15$ is 0.028 m/s, and for $D_E = 20$ is 0.02 m/s. Any of these is quite acceptable as the dynamic range of $u(1,1,t)$ over the prediction window is $\approx 0.8$ m/s. 

\newpage


\begin{table}
\begin{centering}
\begin{tabular}{|c|c|c|} \hline
State Variable & $D_E$ & RMS Error \\
\hline
u(1,1,t) & 10 & 0.034 m/s \\
u(1,1,t) & 15 & 0.028  m/s\\
u(1,1,t) &  20  & 0.02  m/s\\
$\zeta(1,1,t)$ & 10  & 0.37 m \\
$\zeta(1,1,t)$  & 15  & 0.37 m \\
$\zeta(1,1,t)$ & 20 & 0.32 m \\
\hline
%\label{params}
\end{tabular}
\caption{3x3 Corner example, Figure (\ref{overall}). RMS error in DDF forecasting of the state variable $u(1,1,t)$ and $\zeta(1,1,t)$ as we vary $D_E$. These are evaluated in the prediction region; N = 5000h = 500 hr.}
\end{centering}
\end{table}


\vspace{-0.6in}
The RMS error is evaluated as
 \bea
&&RMS(u) = \sqrt{\frac{1}{N}\sum_{n=1}^N\biggl [ u_{Data}(1,1,n) - u_{DDF}(1,1,n)\biggr ]^2}\; m/s , \nonumber \\
&&RMS(\zeta) = \sqrt{\frac{1}{N}\sum_{n=1}^N\biggl [ \zeta_{Data}(1,1,n) - \zeta_{DDF}(1,1,n)\biggr ]^2}\; m,
\eea
and the results are in Table 2.



In the underlying mathematical theorem associated with time delay embedding~\cite{takens81,eckmann85,embed91,abar96,kantz04} there is no restriction on the time delay $\tau$. From a geometrical result one knows that if the dimension of $\TD(n)$, namely $D_E$ is large enough, then the unprojection we wish to achieve is accomplished. The results of~\cite{embed91} indicate that if the dimension is larger than $2D_A + 1$, with $D_A$ the information dimension of the strange attractor, the unprojection will work.  We do not know the $D_A$ of the global SWE model, but if it is near 300 or so, then $D_E \approx 10-20$ is a consistent with this.

In the next Figure, we compare the DDF regional forecasts with the `data' comprised of the solutions to the 300 SWE ODEs at the regional grid points. In this first set of results, we choose the sensor region to be comprised of small clusters of sites.

In Figure (\ref{3x3u11}) we display the x-velocity $u(1,1,t)$ at the regional grid location (1,1) as forecast by the DDF dynamics trained as indicated. We make the same comparison for the y-velocity $v(1,1,t)$ at the (1,1) regional grid location. and the same comparison is made for the height of the fluid $\zeta(1,1,t)$. The height of the fluid when at rest is $H_0 = 50 m$.




\subsection{Region With Sparse, Distributed Sensors}

In the next set of outcomes for DDF regional forecasting, we now select our sensors to be disperesed over the `global' region.  Figure ({\ref{sparsegridmap}) shows ten sensors dispered among the 100 grid locations of the global dynamical regime. The senor locations were selected at random among the 100 possible sites available on the `global' grid. The main purpose of this example is to demonstrate that the sensor sites where $\{v_1(\R,t), v_2(\R,t), \zeta(\R,t)\}$ are observed,  in the SWE example,  need not all be contiguous.

The sucess of DDF forecasting using sensors in broadly dispered sensor region $\R$ suggests one could use the strategy to forecast in a quite broad geographical sub-region of a global dynamical system.

%fig 4
\begin{figure}
    \centering
    \includegraphics[width=1.05\linewidth,height=0.75\linewidth]{10x10random10.png}
    \caption{Sparse Regional Sensor Locations. SWE on a $10 \times 10$ grid. The sensor region, indicated by blue dots, is comprised of 10 locations selected at random among the 100 global grid points. }
    \label{sparsegridmap}
\end{figure}





% fig 5
\begin{figure}
 \centering
    \includegraphics[width=0.47\linewidth,height=0.45\linewidth]{swe10x10dataddfu81.jpg}
     \includegraphics[width=0.47\linewidth,height=0.45\linewidth]{swe10x10dataddfv55.jpg}
        \includegraphics[width=0.47\linewidth,height=0.45\linewidth]{swe10x10dataddfz22.jpg}
 \caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 10 locations, blue dots in Figure (\ref{sparsegridmap}), selected at random among the 100 global grid points. We display the data and the DDF forecast for the x-velocity $u(8,1,t)$. {\bf Upper Left Panel} We display the data and the DDF forecast for the x-velocity $u(8,1,t)$. {\bf Upper Right Panel} We display the data and the DDF forecast for the y-velocity $v(5,5,t)$. {\bf Bottom Panel} We display the data and the DDF forecast for the fluid height $\zeta(2,2,t)$. The fluid rest height is $H_0 = 50$m. The time delay parameters here are $D_E = 20, \tau = 20 h$; $h = 0.1 hr$. }
    \label{sparseu81}
%\end{figure}

%fig 10
%\begin{figure}
% \centering
  %  \includegraphics[width=1.05\linewidth,height=0.75\linewidth]{swe10x10dataddfv55.jpg}
% \caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 10 locations, blue dots in Figure (\ref{sparsegridmap}), selected at random among the 100 global grid points. We display the data and the DDF forecast for the y-velocity $v(5,5,t)$.  The time delay parameters here are $D_E = 20, \tau = 17 \Delta t$ with $\Delta t = 0.1 hr$.}
 %   \label{sparsev55}
\end{figure}

% fig 11
%\begin{figure}
% \centering
 %   \includegraphics[width=1.05\linewidth,height=0.75\linewidth]{swe10x10dataddfz63.jpg}
 %\caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 10 locations, blue dots in Figure (\ref{sparsegridmap}), selected at random among the 100 global grid points. We display the data and the DDF forecast for the fluid height $\zeta(6,3,t)$.  The fluid rest height is $H_0 = 50$m. The time delay parameters here are $D_E = 20, \tau = 17 \Delta t$ with $\Delta t = 0.1 hr$.}
 %   \label{sparsez63}
%\end{figure}

\subsection{Off Center; 2x2 Region}

Our final example is a 2x2 subregion randomly selected to be off center in the global 10x10 grid. This is shown in Figure (\ref{2x2offcenter}).


%fig 6
\begin{figure}
\centering
\includegraphics[width=1.05\linewidth,height=0.75\linewidth]{regional2x2offcenter.jpg}
\caption{SWE global 10x10 grid. Regional grid is 2x2 and is located off center within the global grid.}
\label{2x2offcenter}
\end{figure}

%fig 7
\begin{figure}
 \centering
    \includegraphics[width=0.49\linewidth,height=0.45\linewidth]{swe10x102x2offcenteru57.jpg}
     \includegraphics[width=0.49\linewidth,height=0.45\linewidth]{swe10x102x2offcenterv57.jpg}
       \includegraphics[width=0.49\linewidth,height=0.45\linewidth]{swe10x102x2offcenterz57.jpg}
 \caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 4 locations, blue dots in Figure (\ref{2x2offcenter}), in a 2x2 off center location. The sensor region is comprised of 4 locations, blue dots in Figure (\ref{sparsegridmap}), in a 2x2 off center location. The time delay parameters here are $D_E = 20, \tau = 17 \Delta t$ with $\Delta t = 0.1 hr$. {\bf Top Left Panel} We display the data and the DDF forecast for the x-velocity $u(5,7,t)$.   {\bf Top Right Panel} We display the data and the DDF forecast for the y-velocity $v(5,7,t)$. {\bf Bottom Panel} We display the data and the DDF forecast for the fluid height $\zeta(5,7,t)$.  The fluid rest height is $H_0 = 50$m.}
    \label{2x2u11}
\end{figure}

%fig 14
%\begin{figure}
% \centering
%    \includegraphics[width=1.05\linewidth,height=0.75\linewidth]{swe10x102x2offcenterv57.jpg}
% \caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 4 locations, blue dots in Figure (\ref{sparsegridmap}), in a 2x2 off center location. We display the data and the DDF forecast for the y-velocity $v(5,7,t)$.  The time delay parameters here are $D_E = 20, \tau = 17 \Delta t$ with $\Delta t = 0.1 hr$.}
%    \label{2x2v11}
%\end{figure}

%fig 15
%\begin{figure}
% \centering
  %  \includegraphics[width=1.05\linewidth,height=0.75\linewidth]{swe10x102x2offcenterz57.jpg}
 %\caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 4 locations, blue dots in Figure (\ref{sparsegridmap}), in a 2x2 off center location. We display the data and the DDF forecast for the fluid height $\zeta(5,7,t)$.  The fluid rest height is $H_0 = 50$m.  The time delay parameters here are $D_E = 20, \tau = 17 \Delta t$ with $\Delta t = 0.1 hr$.}
%  \label{2x2z11}
%\end{figure}





%fig 16
%\begin{figure}
% \centering
  %  \includegraphics[width=1.05\linewidth,height=0.75\linewidth]{swe10x102x2offcenterz68.jpg}
 %\caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 4 locations, blue dots in Figure (\ref{sparsegridmap}), in a 2x2 off center location. We display the data and the DDF forecast for the fluid height $\zeta(6,8,t)$.  The fluid rest height is $H_0 = 50$m. The time delay parameters here are $D_E = 20, \tau = 17 \Delta t$ with $\Delta t = 0.1 hr$.}
 %   \label{2x2z22}
%\end{figure}

%\newpage

\section{Addressing Noisy Data}



Our twin experiment examples using `global data' from a shallow water flow have heretofore implicitly assumed we had data with a very high signal to noise ratio. In this section we take these very clean data and add Gaussian noise before constructing a DDF model of the regional dynamics.

We used the 10x10 SWE data and focused on the 3x3 corner region. For each of the 27 time series in this region we added Gaussian noise with zero mean and variance $S\sigma^2_{\O}$. $\sigma^2_{\O}$ is the variance of the signal $\O(\R,t)$ in the sensor region.  In this configuration the signal to noise ratio is $S/N = 1/S$ or in dB $S/N = 10 \log_{10}[1/S]$. For small S the data is essentially noise free, as S approaches and exceeds unity, the noise level slowly overcomes the signal.

In Figure (\ref{v11s0_001}) we show the y-velocity v(1,1,t), data and DDF Forecast for S = 0.001. Next we display the fluid height $\zeta(1,1,t)$ for the same noise level is shown as the data and the DDF Forecast.
We next display the the y-velocity $v(1,1,t))$ data and DDF Forecast when $S = 0.01$, and then the data and DDF Forecast for the fluid height $\zeta(1,3,t)$ are shown when $S = 0.1$.

As S is increased beyond this, we see significant degradation in the DDF Forecast relative to the data. In Figure (\ref{rmse}), {\bf Left Panel}, S is now unity, and the DDF Forecast, as can be seen has become much less accurate. This noise level corresponds to a signal to noise ratio of 0 dB.

We summarize the robustness to added noise in the data in Figure (\ref{rmse}), {\bf Right Panel}, where the RMS error in the x-velocity is shown for the range of noise level S we considered.

%\vspace{-0.5in}

%fig 8
\begin{figure}
 \centering
    \includegraphics[width=0.49\linewidth,height=0.43\linewidth]{v11swe10x103x3cornerS0_001.jpg}
      \includegraphics[width=0.49\linewidth,height=0.43\linewidth]{z11swe10x103x3cornerS0_001.jpg}
       \includegraphics[width=0.49\linewidth,height=0.43\linewidth]{v13swe10x103x3cornerS0_01.jpg}
          \includegraphics[width=0.49\linewidth,height=0.43\linewidth]{z13swe10x103x3corners0_1.jpg}
           % \includegraphics[width=0.52\linewidth,height=0.43\linewidth]{u32swe10x103x3corners1.jpg}
 \caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 9 locations, blue dots in Figure (\ref{overall}), in a 3x3 off corner location. S = 0.001; S/N = 30 dB. {\bf Top Left Panel} We display the y-velocity u(1,1,t) data and DDF forecast.  {\bf Top Right Panel} We display the fluid height $\zeta(1,1,t)$ data and DDF forecast.  {\bf Bottom Left Panel} S = 0.01; S/N = 20 dB. We display the y-velocity v(1,3,t) data and DDF forecast. {\bf Bottom Right Panel}  S = 0.1; S/N = 10 dB. We display the fluid height $\zeta(1,3,t)$ data and DDF forecast.  }
    \label{v11s0_001}
\end{figure}

%fig 18
%\begin{figure}
 %\centering
%   \includegraphics[width=1.05\linewidth,height=0.75\linewidth]{z11swe10x103x3cornerS0_001.jpg}
% \caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 9 locations, blue dots in Figure (\ref{overall}), in a 3x3 off corner location. We display the fluid height $\zeta(1,1,t)$ data and DDF forecast.  S = 0.001; S/N = 30 dB.}
 %   \label{z11S0_001}
%\end{figure}

%fig 19
%\begin{figure}
 %\centering
%    \includegraphics[width=1.05\linewidth,height=0.75\linewidth]{v13swe10x103x3cornerS0_01.jpg}
% \caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 9 locations, blue dots in Figure (\ref{overall}), in a 3x3 off corner location. We display the y-velocity v(1,3,t) data and DDF forecast. S = 0.01; S/N = 20 dB.}
%    \label{v13s0_01}
%\end{figure}

%fig 20
%\begin{figure}
% \centering
  %  \includegraphics[width=1.05\linewidth,height=0.75\linewidth]{z13swe10x103x3corners0_1.jpg}
% \caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 9 locations, blue dots in Figure (\ref{overall}), in a 3x3 off corner location. We display the xfluid height $\zeta(1,3,t)$ data and DDF forecast.  S = 0.1; S/N = 10 dB.}
   % \label{z13s0_1}
%\end{figure}

%fig 21
%\begin{figure}
 %\centering
 %   \includegraphics[width=1.05\linewidth,height=0.75\linewidth]{u32swe10x103x3corners1.jpg}
% \caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 9 locations, blue dots in Figure (\ref{overall}), in a 3x3 off corner location. We display the x-velocity u(3,2,t) data and DDF forecast. S = 1; S/N = 0 dB. At S = 1 the DDF Forecast has degraded substantially.}
%   \label{u32s1}
%\end{figure}

%fig 22
%\begin{figure}
% \centering
%    \includegraphics[width=1.05\linewidth,height=0.75\linewidth]{u12swe10x103x3corners10.jpg}
% \caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 9 locations, blue dots in Figure (\ref{overall}), in a 3x3 off corner location. We display the x-velocity u(7,0,t) data and DDF forecast.  The time delay parameters here are $D_E = 20, \tau = 50 \Delta t$ with $\Delta t = 0.1 hr$. S = 0.32; S/N = 5 dB.}
   % \label{u12s10}
%\end{figure}

%fig 9
\begin{figure}
 \centering
  \includegraphics[width=0.49\linewidth,height=0.5\linewidth]{u32swe10x103x3corners1.jpg}
    \includegraphics[width=0.49\linewidth,height=0.5\linewidth]{log10rmsswedata.jpg}
 \caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 9 locations, blue dots in Figure (\ref{overall}), in a 3x3 off corner location. {\bf Left Panel}   S = 1; S/N = 0 dB. We display the x-velocity u(3,2,t) data and DDF forecast.  At S = 1 the DDF Forecast has degraded substantially. {\bf Right Panel} We display the RMS error in the x-velocity as a function of S over the range we considered.}
    \label{rmse}
\end{figure}

%fig 24
%\begin{figure}
% \centering
%    \includegraphics[width=1.05\linewidth,height=0.75\linewidth]{u70s3_2.jpg}
% \caption{SWE on a $10 \times 10$ grid. The sensor region is comprised of 9 locations, blue dots in Figure (\ref{overall}), in a 3x3 off corner location. We display the x-velocity u(7,0,t) data and DDF forecast.  The time delay parameters here are $D_E = 20, \tau = 50 \Delta t$ with $\Delta t = 0.1 hr$. S = 3.2; S/N = -5 dB.}
   % \label{u70s3_2}
%\end{figure}

\newpage
\section{Average {\em Regional}  Error of Fluid Heights $<\zeta>_{\R}$(t)}
We now discuss our forecasting results in a context that benchmarks their accuracy for future comparison with future machine learning models of a similar nature. We will take the three existing regional SWE forecasting examples we have examined in the text above, and quantify their accuracy through the use of an average percent error equation, Equation (\ref{avzeta}), averaging the height $\zeta$ over a region $\R$ at each forecast time in the validation window. 
%Unlike the typical percent error formula, this one will not be the difference between ground truth and predicted values divided by ground truth, rather it will be the absolute error divided by maximum range of possible wave heights. Also note that this percent error equation will only account for the values of water height and disregard the velocity in the u and v directions as precedent has set the height as a quantity of greater interest \cite{yildiz2021learning}. 

We define the regional average percent error as:

%\begin{equation}
%APE(n) = \frac{1}{D_z}\sum_i^{D_z} \frac{|\boldsymbol{O}(\boldsymbol{R}_i,n)_z-\boldsymbol{TD}(\boldsymbol{R}_i,n))_z|}{|\text{Largest Obs. Height} - \text{Smallest Obs. Height}|}*100\text{\%}
%\end{equation}

\be
<\zeta>_{{\bf R}}(t) = \frac{100}{\mbox{dim}\,{\bf R}}\sum_{{\bf R}} \frac{|\zeta(\R,t)_{\mbox{data}} - {\zeta}_{\mbox{DDF Forecast}}(\R,t)|}{|\mbox{Max} \, \zeta_{\mbox{data}}  - \mbox{Min } \, \zeta_{\mbox{data}}|}
\label{avzeta}
\ee



%fig 10
\begin{figure}
 \centering
    \includegraphics[width=0.49\linewidth,height=0.45\linewidth]{3x3cornerzeta.jpg}
     \includegraphics[width=0.49\linewidth,height=0.45\linewidth]{10x10randomzeta.jpg}
      \includegraphics[width=0.49\linewidth,height=0.45\linewidth]{2x2offcenterzeta.jpg}
    \caption{Using Equation (\ref{avzeta}) we evaluated $<\zeta>_{\R}(t)$ for each of the examples treated in the text.
    {\bf Upper Left Panel} 3x3 Corner Observation Region, Figure (\ref{overall}) {\bf Upper Right Panel} 10x10 Random Observation Region, Figure (\ref{sparsegridmap}) {\bf Bottom Panel} 2x2 Off Center Observation Region, Figure (\ref{2x2offcenter})}
    \label{avzetacases}
\end{figure}
 
$\mbox{dim}\, {\bf R}$ is the number of observed $\zeta$ across the region $\R$. ${\zeta}_{\mbox{DDF Forecast}}(\R,t)$ is achieved using the time delay method featured in earlier sections. $<\zeta>_{{\bf R}}(t)$ is evaluated at each observation time $t_n = t_0 + n\Delta t$ in the forecast time window..


The maximum and minimum observed $\zeta$ values are taken across the entire region $\R$, and are not location specific. The $<\zeta>_{\R}(t)$ we display in Figure (\ref{avzetacases}) show quite accurate results across a long time in the validation or forecasting window. From data alone, DDF can build a model of the shallow water equation output at the regional sensor locations that forecasts up to 500 hours with an average percent error in $\zeta$ that is less than 10\% as shown in the 3 by 3 and sparse examples.

This metric of forecasting performance is nearly the same as~\cite{yildiz2021learning}, and both provide
an informative look at the performance of a ML construction. The work of~\cite{yildiz2021learning} analyzes an instatiation of the SWE with neither forcing nor dissipation~\cite{stuart2016}. 

The ``dimension reduction'~\cite{yildiz2021learning} is achieved via a proper orthogonal decomposition approach (another name for principal component analysis~\cite{press07}), a linear method, and not by using time delay embedding on the regional observations as discussed in this paper. This differs from our work, but the performance meteric is in the same spirit as we discuss here.
%The motivation for choosing the denominator as the difference between the largest wave height and lowest wave height comes from the fact that we view the error DDF produces as originating from trying to follow the attractor in a set volume of phase space. The choice of largest minus smallest height sets the scale of DDFs operating range and thusly allows for the most relevant scaling of error that something like the magnitude of the height would fail to achieve.

%he average percent errors we display in our figure below show quite accurate results across a long period of time (up to 500 hours). From data lone, DDF can reconstruct a model of the shallow water equations that forecast up to 500 hours long with an average percent error in height less than 10\% as shown in the 3 by 3 and sparse examples.


 %       \caption{These are the average percent error graphs for the 2 by 2, 3 by 3, and sparse grid tests done previously. The error for each of these graphs account for the height only and compare the absolute difference in DDF predicted height with the true height which is then divided by the maximum observed height and minimum observed height across all observed dimensions. This percent error is then averaged across all observed height dimensions.}
    %    \label{fig:three graphs}
%\end{figure}




\newpage

\section{Summary and Discussion}

\subsection{General Remarks}

We have introduced a method for building a nonlinear discrete time forecasting system for observed state variables in geophysical dynamical settings where the underlying model of the dynamics is not known. The method relies on observed data to train model parameters in a representation of the unknown dynamical rules. 
%The representation has parameters that are trained using the observed data. 
Time series of the data are considered as samples of the distribution in state space visited by the trajectories of the selected physical variables,  and the representation of the vector field of the dynamical flow uses well tested interpolating basis functions to give information among the observed samples. 

The method for representing the unknown dynamics of the physical flow adopted in our work uses weighted linear combinations of radial basis functions (RBFs)~\cite{hardy71, micc86,broom88,schab95,powell02,buhmann09}, and the estimation of the weights is a linear algebra problem. This linear algebra operations require a Tikhonov regularization,  also known as ridge regression~\cite{gruber98,press07,vanw21}.  These methods and algorithms implementing them are well tested and widely discussed in the literature.

Selection of observations in a region means that the full state space of the dynamics is not sampled. This entails a projection from the full state space to the subspace of selected observables.  Construction of an equivalent state space to the unknown physical state space is, in effect, an {\bf unprojection} from the observed subspace. This is  accomplished using time delay embedding widely analyzed in the nonlinear dynamics literature~\cite{takens81,eckmann85,embed91,abar96,kantz04}.

The Physics of using time delays comes from the fact that the temporal evolution of the observed state variables depends on the full set of (unknown) state variables; see Equation (\ref{global}) and Equation (\ref{regional}).  Over a time delay the full set of state variables is in operation, and the information about the unobserved state variables is conveyed to the observed variables through this feature of the dynamics.

The use of time delay embedding introduces at least two additional quantities to be estimated by data: the value of the time delay $\tau = hT_h$ and the number of required time delays $D_E$. If there are many time scales in the physical system of interest, many time delays maybe useful along with different numbers of delayed observables $D_E$~\cite{judd95,hirata06}.

These considerations led us to formulate the DDF task for forecasting the future of $D_R$ dimensional observed state variables $\O(\R,t)$ located in a spatial region $\R$ as


\bea
&&\O(\R,n+1) = \O(\R,n) + \nonumber \\
&& \f_{\R}(\TD(n), \bchi) + \frac{h}{2} \biggl[ [{\cal F}(\R,n),0] + [{\cal F}(\R,n+1),0] \biggr ].
\label{discretetd1}
\eea 

The $\O(\R,t)$ are $D_R$ dimensional $\O(\R,t) = \{O_{\alpha}(t)\};\;\alpha = 1,2,...,D_R$.  The dynamical map for the regional observables,  written in components, is seen to be
\bea
&&O_{\alpha}(n+1) = O_{\alpha}(n) + \nonumber \\
&&f_{\alpha}(\TD(n),\bchi) + \frac{h}{2} \biggl[ [{\cal F}(\R,n),0] + [{\cal F}(\R,n+1),0] \biggr ]_{\alpha}.
\label{obsmap11}
\eea
in which $\TD(t_n) = \TD(n)$ are time delay vectors of dimension $D_ED_R$
\be
\TD(t) = [\O(\R,t),\O(\R,t- \tau), \O(\R,t-2\tau),...,\O(\R,t - (D_E-1)\tau)],
\ee
and $\bchi = \{w_{\alpha q},c_{\alpha j}, \sigma, B, D_E, T_h \}$; $\tau = h T_h$.

As noted this implies that a representation of the vector field for the flow of each observed quantity is required. We recognize that depending on the number of state variables one chooses to observe and the number of geographical locations at which one observes them,  the computational burden required for DDF forecasting may increase substantially.  It is unlikely to match the requirements for a full scale regional or global model as presently implemented.

The regional forcing of the system is given as ${\cal F}(\R,t_n) = {\cal F}(\R,n)$. The external forcing must be given in a DDF formulation just as it is required when we present a detailed physical model of the dynamics~\cite{echam5,ecmwfbasics}. 

It is important that the forcing driving the global dynamics enters in an additive fashion in many physical systems, fluid dynamics among them, so the intrinsic properties of the global or regional system is separated from the forces that drive the system into motion.  Once the intrinsic aspects of the dynamics is encoded in the representation of the required flow vector fields,  it should respond to any presentation of forcing as does a detailed model of the dynamical system.

\subsection{The Specific Results in this Paper}

In the example of DDF regional forecasting we investigated in this paper we took as our global dynamics the PDEs of shallow water flow realized on a rectangular mid-latitude plane.  Clearly this is a useful but major reduction in complexity compared to the collection and set of data from real field observations. We have found it instructive to begin in this manner, and the results illustrate the issues to be encountered in a practical application of the formulation.

In this framework we demonstrated in a number of scenarios that observations of a subset of `global' shallow water flows can be use to build a discrete time flow $\O(\R,t) \to \O(\R,t+h)$ allowing for accurate forecasting beyond a temporal domain where data has been previously collected.

We also noted that in the choice of time delay embedding parameters $\{D_E, \tau = T_h h\}$ there is a range of $D_E$ over which excellent forecasting can be achieved.

By adding Gaussian noise to the SWE data, we found a rather robust ability to make accurate DDF Forecasts in a region of the `global' model until about S = 1, or 0 dB signal to noise.

\subsection{Looking Forward}

In investigating the examples we presented we were required to generate our own data, performing what is often called a {\it twin experiment}~\cite{abar2021} at a choice of grid points used to approximate solutions to the SWE PDFs.  As one proceeds to using our results to guide DDF weather forecasting,  at no stage do we approximately solve some physical PDEs on a grid of some spatial resolution, we have no restrictions on where the regional measurements may be performed.


Equally important to note is that the information about the actual dynamics of the overall system of interest are not encoded into physical properties of a model such as effective viscosities at a selected spatial resolution or parametrizations of sub-grid scale dynamics, etc.

The observed information is now located in the estimated parameters $\bchi$ of the vector field representations and in the time delay reconstruction of a space equivalent to the full state space of the system.  That is, the information in the data remains as it was, but it is redistributed to a different representation of the global dynamics than is encountered in the physical equations of the global problem.

In working with observed data, there are no grid points. These are only introduced to aid in the solution of the PDEs of a physical model.  The locations where one sets out sensors are totally up to the users. The discrete time predictive models of observed quantites summarized in Equation (\ref{obsmap1}) are built with out concern for the spatial resolution of grid points in a physical model, without specifying the dynamics of subgrid scale quantities, such as cloud dynamics, and setting aside similar concerns in the formulation of detailed models of the atmosphere and ocean.

As we pointed out earlier, the gains in using DDF models, must be recognized as having been achieved by relinquishing many of the details about earth systems processes that are captured by detailed physically based PDEs for earth systems processes. In DDF only the observed state variables can be forecast.
When that is what actually wishes to know, it provides another way to sample data on observed quantities and forecast them.



%This remark implies that a predictive dynamical model of turbulent fluid behavior, in the laboratory or in the field,  at large Reynolds number could be accomplished with DDF forecasting without the need for finer and finer spatial grids on which an accurate evaluation of the underlying PDEs is performed.

\section*{Acknowledgements}
We acknowledge support the Office of Naval Research, Grants N00014-20-1-2580 and N00014-19-1-2522. Discussions with Erik Bollt, Daniel Gauthier, and Steve Penny were central to pursuing this work.
\newpage 
\section{Appendix}

\subsection{Performing DDF on Observed Data}

The purpose of this section is to provide the reader with more detail on how the implementation of DDF works in practice and to identify what steps would need to be taken by the reader in order to implement DDF to fit their own data, observed or numerically generated for a twin experiment. Clearly the first step is to collect the data, and we assume that is where we start.

%The process of performing DDF begins with obtaining a set of data, either generated computationally or observed experimentally; DDF offers a flexible way to forecast the future behavior of the system based on these data. 

In order to implement DDF some choices must be made, the first choice will be in selecting a function representation of the flow vector field. We have tested Taylor Series Representations, RBF's, and RBF's plus polynomial. While the best choice will be dependent upon the system being studied, we've found general success with various representations. 

In the coming sections we'll discuss different options and offer some guidance on choosing a representation for a data set. After choosing a function representation, we must choose a way to select centers for the RBF's, choose an appropriate time delay embedding dimension and time delay, and finally perform a hyper parameter search.

\subsection{Designing a Representation of the Discrete Time Flow Vector Field}
To start every problem in DDF we must first choose a function 
representation, $\f(\S(\r,t_n),\bchi)$, for the data set we are studying. The way we choose to represent vector fields for a discrete time map will have the largest impact on the predictive power of DDF, we've found that this choice carries far more weight than a good choice of hyper-parameters or an apt selection of centers. 

This paper has only focused on the use of RBF's, specifically the Gaussian RBF plus the first order polynomial of each observed dimension. 
For the shallow water flow data we found that using using polynomial terms in our $\f(\S(\r,t_n),\bchi)$ representation to match the polynomial terms in the actual SWEs and letting the Gaussian RBF's capture the rest of the behavior of the SWEs works quite well. 

We will go on to discuss more details of RBF's, we want to note that other choices exist; these choices include but are not limited to Multilayer Perceptrons, Hermite Interpolation, or any other numerical tool for interpolation. One item to keep in mind is that the RBF expansion is linear in the RBF weights, allowing us to use linear algebra to estimate them. 
%Should the reader opt to use a different function representation, they should be aware if they will still be able to use Ridge Regression or if they must use a different method of minimizing the cost function (which would almost definitely be harder and slower than Ridge Regression).

\subsubsection{Choosing a Radial Basis Function}
The Radial Basis Function (RBF) was invented by Hardy~\cite{hardy71} for interpolation among observed samples of a function of multivariate variables. There is now an extensive literature on RBF's and their effectiveness as interpolation functions. In this literature, many different choices for the form of the RBF have been found to work~\cite{hardy71,micc86,broom88,schab95,powell02,buhmann09}.

The parameters in our choice of RBF $\f(\TD(r,n),\bchi$), appearing in Equation (\ref{obsmap}), are estimated as follows

The $\O(\R,t)$ are $D_R$ dimensional $\O(\R,t) = \{O_{\alpha}(t)\};\;\alpha = 1,2,...,D_R$.  The dynamical map for the regional observables is
{\small 
\bea
&&O_{\alpha}(n+1) = O_{\alpha}(n) + \nonumber \\
&&f_{\alpha}(\TD(n),\bchi) + \frac{h}{2} \biggl[ [{\cal F}(\R,n),0] + [{\cal F}(\R,n+1),0] \biggr ]_{\alpha} \nonumber \\
&&\TD(t) = [\O(\R,t),\O(\R,t- \tau), \O(\R,t-2\tau),...,\O(\R,t - (D_E-1)\tau].
\label{obsmap2}
\eea}

To estimate the parameters $\bchi$ we minimize the objective function with respect to $\bchi$.
%\begin{equation}
%\boldsymbol{S}(\boldsymbol{r},n+1) = \boldsymbol{S}(\boldsymbol{r},n) + \boldsymbol{f}(\boldsymbol{S}(\boldsymbol{r},n),\boldsymbol{\chi})
%\end{equation}
\bea
&&
C(\bchi) = \sum^{N_T}_{N_c+1} \biggl \{[\O(n+1) - \O(n) - 
\f(\TD(n),\bchi) \nonumber \\
&&- \frac{h}{2} \biggl[ [{\cal F}(\R,n),0] + [{\cal F}(\R,n+1),0] \biggr ] \biggr \}^2.
\label{costchi}
\eea

In Equation (\ref{costchi})
\be
f_{\alpha}(\TD(n),\bchi) = \sum_{q=1}^{N_c} w_{\alpha q} \bpsi((\TD(n)-\TD^c(q))^2, \sigma) + \sum_{j=1}^{D_R} c_{\alpha j} \O_j(n)
\label{rbfrep2}
\ee



Even though we can choose any RBF for $\bpsi$, the $C(\bchi)$ is always linear in the weights $w_{\alpha q}, c_{\alpha j}$, enabling the use of the linear algebra of Ridge Regression or Tikhonov regularization in estimating them.

%There are many different choices of RBF's described in the applied math literature; here is a list of a common few \cite{wu2012using}:
%\begin{equation}
%\begin{split}
%    &\psi(\boldsymbol{S}-\boldsymbol{S}(q),R) = e^{-R(\boldsymbol{S}-\boldsymbol{S}(q))^2}, \text{Gaussian}           \\
%    &\psi(\boldsymbol{S}-\boldsymbol{S}(q),\sigma,\alpha) = (\sigma^2+(\boldsymbol{S}-\boldsymbol{S}(q))^2)^\alpha, 0<\alpha<1 , \text{MultiQuadric} \\
   % &\psi(\boldsymbol{S}-\boldsymbol{S}(q),\sigma,\alpha) = \frac{1}{(\sigma^2+(\boldsymbol{S}-\boldsymbol{S}(q))^2)^\alpha}, \alpha > 0, \text{Inverse MultiQuadric}        \\
%    &\psi(\boldsymbol{S}-\boldsymbol{S}(q)) = \boldsymbol{S}-\boldsymbol{S}(q), \text{Linear}         \\
   % &\psi(\boldsymbol{S}-\boldsymbol{S}(q)) = (\boldsymbol{S}-\boldsymbol{S}(q))^2ln(\boldsymbol{S}-\boldsymbol{S}(q)), \text{Thin-Plate Spline}      \\
%    &\psi(\boldsymbol{S}-\boldsymbol{S}(q),\sigma,\theta) = \frac{1}{1+e^{\frac{\boldsymbol{S}-\boldsymbol{S}(q)}{\sigma^2}-\theta}}, \text{logistic function}
%\end{split}
%\end{equation}

In working with our example of the SWE, we found that the Gaussian RBF $\bpsi_G((\TD(n)-\TD^c(q))^2$ plus (1st order) polynomials in $\TD(n)$ to be more effective than $\bpsi_G((\TD(n)-\TD^c(q))^2$  alone. %In the discussion below we will show how we implement this form.

To have a simplified notation for solving the regularized linear algebra problem, let us call
\be
\KN(n) = \O(n+1) - \O(n) - \frac{h}{2} \biggl[ [{\cal F}(\R,n),0] + [{\cal F}(\R,n+1),0] \biggr ],
\ee
the vector of RBF coefficients,
\be
\P = \{w_{\alpha q}, c_{\alpha j}\},
\ee
and the RBF + Polynomial terms are the matrix $\M$.

Then the regularized cost function is given by~\cite{press07}
\be
C_B(\bchi) = \sum_{n} \biggl[ \KN(n) - \P \M(n)\biggr ]^2 + B\P^T\P.
\label{costchib}
\ee
T indicates the transpose.

The minimization of $C_B(\bchi)$ with respect to $\P$, gives us the regularized solution
\be
\P = \KN \cdot \M^T \frac{1}{(\M\M^T + B\I)}.
\ee

After we introduce time delay embedding, the DDF parameters are $\bchi = \{w_{\alpha q}, c_{\alpha j}, \sigma, B, T_h , D_E\}$.
%Variation of $C(\bchi)$ with respect to the $\P$ yields the discrete time predictive models we call DDF, Equation (\ref{obsmap1}).

\subsubsection{Including Polynomial Terms in Eq. (\ref{rbfrep2})}
In this paper we have found the most effective representation for SWE to be RBF representations with the inclusion of a polynomial term; the polynomial terms are in the $\TD(n)$. We retain only first order terms in this paper~\cite{schab95,powell02}. 

%The vector field approximation would take the following general form:
%\begin{equation}
%\boldsymbol{f}(\boldsymbol{S}(\boldsymbol{r},n),\boldsymbol{\chi}) = \sum_{q=1}^{N_c} \boldsymbol{\chi}_{q} \psi(||\boldsymbol{S}(\boldsymbol{r},n)-\boldsymbol{S}(q)||) + \sum_{i=1}^{\hat{m}} \boldsymbol{\chi}_{N_c+i} p_i(\boldsymbol{S}(\boldsymbol{r},n))
%\end{equation}
%Where we let the $p_i$, $i=1,2,3,...,\hat{m}$, be the basis of polynomial space. In testing, 
%We added the first order polynomial term to match expected first order terms in the state variables of the actual SWEs and found a substantial boost in predictive power. This supports confirming our guidelines that the function representation should resemble the true dynamics as closely as possible. In other data sets, lacking 1st order terms in their dynamics, when polynomial terms are added, the change in predictive power is negligible in our testing.

With a general set of observations where we might have no particular insight into the dynamics, one should add the polynomials present in a general formulations of the problem~\cite{schab95,powell02}.

\subsubsection{How to choose Centers}
The choice of centers, $\TD^c(q)$ vector in the RBF, will be dictated by the data points in the training set. %The centers are typically either directly chosen to be existing vectors from the data points in the training window, or are determined algorithmically. 

We had initial success taking every $n^{th}$ data point in the training window to be a center, this method is quick, but it may poorly saturate regions of low density and is prone to overlapping centers and to leaving empty space. 

We recommend taking the approach of trying to saturate all important areas of space densely with centers to provide DDF with the most accurate update rule in those regions. With this in mind, we have opted to use K-means clustering to choose our centers for us. While there may be more advanced strategies, we have found K-means to work well enough, and we used it in finding all of the results in the present paper~\cite{du2006neural}.

Another aspect to consider is the number of centers. The general rule of thumb is that more centers is better, but this comes at the cost of both memory and computational time. For the results in this paper, typically around 1000 centers were used. 

One strategy could be to do preliminary testing with fewer centers and increase the number of centers. 
%and redo the grid search in a smaller region previously shown to work well. 
The fewer centers one can get by with, the better, as they will dramatically improve the computational time using the DDF map over the use of excessive centers.

\subsubsection{Time Delay Embedding (TDE)}

When studying physical systems, it is often very difficult or completely impossible to measure all observable quantities of the system. There is information that is lost in these unobserved quantities that must be recaptured through the use of time delay embedding. When studying the sub-regions of the shallow water flows as global data or some region of the atmosphere/oceans, we take full advantage of TDE in DDF to recapture the dynamical information of the system. 

For the purpose of this paper, we took subregions of a grid generated by the SWE, and we required a choice of a time delay and dimension size for TDE. 

To find an effective number of time delay dimensions, $D_E$, Taken's Theorem~\cite{takens81} guarantees that the attractor can be reconstructed in as few as $2D_A+1$ dimensions, where $D_A$ is the box counting fractal dimension dimension of the original state space. However, by calculating the number of false nearest neighbors~\cite{abar96} at each $D_E$ value, we can get an estimate of the ideal number of time delay dimensions. %By increasing the number of time delay dimensions, we reconstruct the state space and reveal that previous nearest neighbors in the data set were false nearest neighbors. 
As this reconstruction proceeds, we eventually reach a point where the number of false nearest neighbors drops to a negligibly low number. This number of time delay dimensions serves as our estimate for $D_E$.

The time delay $\tau$ is conveniently taken to be a multiple of the observed temporal step h in the data collection. $\tau = T_h h$. Also the embedding dimension $D_E$ for $\TD(t)$ is always an integer. So in minimizing the cost function $C(\bchi)$ we must search a grid of integers $\{T_h,D_E\}$ along with a search in continuous variables $\{w_{\alpha q}, B ,\sigma^2 = \frac{1}{2R}\}$.

%For more information on how to choose a time delay and dimension size we recommend the following reference \cite{abar96}; after being given a good initial reference point, some grid sweeping was performed to fine tune these two hyper parameters.\\

%To implement TDE into DDF there are a few clarifications that must be made. 
%We perform TDE in DDF by creating delays $\TD(t)$, for the observed data. , 
%and $\boldsymbol{TD'}(n)=[\boldsymbol{S}_1(\boldsymbol{r},n),\boldsymbol{S}_2(\boldsymbol{r},n),...,\boldsymbol{S}_{D_E}(\boldsymbol{r},n)]$, for the DDF forecasted data, and forecasting on the leading S vector, $S_1$, where each S is just a $D_R$ dimensional vector of the observed dimensions of the system; the delayed S vectors will not be forecasted for they follow the leading term, $S_1$, in accordance to their set time delay:
%\begin{equation}
%\boldsymbol{TD}(t)=[\boldsymbol{O}(\boldsymbol{R},t),\boldsymbol{O}(\boldsymbol{R},t-\tau),...,\boldsymbol{O}(\boldsymbol{R},t-\tau(D_E-1))]
%\end{equation}
%\begin{equation}
%\boldsymbol{TD'}(n)=[\boldsymbol{S}_1(\boldsymbol{r},n),\boldsymbol{S}_2(\boldsymbol{r},n),...,\boldsymbol{S}_{D_E}(\boldsymbol{r},n)]
%\end{equation}
%\begin{equation}
%\boldsymbol{TD'}(n)=[\boldsymbol{S}_1(\boldsymbol{r},n),\boldsymbol{S}_1(\boldsymbol{r},n-\tau),...,\boldsymbol{S}_1(\boldsymbol{r},n-\tau*(D_E-1)]
%\end{equation}

To identify an appropriate time delay we begin with the suggestion of~\cite{fraser86,fraser89} to use the first minimum of the average mutual information (AMI) between components of the time delay state vector $\TD(t)$ to identify independent coordinates in time delay space. Qualitatively the AMI is a nonlinear correlation function and is used as a guide to an order of magnitude for the delay $\tau$. Similarly the use of false nearest neighbors (FNN)~\cite{abar96} provides a guide to an approximate estimate for $D_E$.

Our choice of parameters in the DDF model is done by minimizing the cost function $C(\bchi)$, Eq. (\ref{costchi1}), which represents the quality of a forecast. This is, in detail, a different criterion than the familiar AMI and FNN criteria, but the latter gives on a sensible place to initiate searches for the parameters $\bchi$.

We do not need to separately forecast all components of $\TD(t)$. If we forecast only $\O(t)$, the remaining components are forecast as well. We do require the full time delay vectors in the arguments of the RBF.

%For a physical system of $D_R$ dimensions, the RBF's will receive $D_RD_E$ inputs, an input for each dimension and each time delay of that dimension. The polynomial term in the function representation was chosen to only account for the leading term and not the time delays of the leading term dimensions. This has been illustrated below:
%\begin{equation}
%\boldsymbol{f}(\boldsymbol{TD'}(n),\boldsymbol{\chi}) = \sum_{q=1}^{N_c} \boldsymbol{\chi}_{q} \psi(||\boldsymbol{TD'}(n)-\boldsymbol{TD'}(q)||) + \sum_{i=1}^{\hat{m}} \boldsymbol{\chi}_{N_c+i} p_i(\boldsymbol{S}_1(\boldsymbol{r},n))
%\end{equation}
%To summarize, one will be training a $D_RD_E$ dimensional system, but only using the predictions for the first $D_R$ dimensions using the representation in Equation (\ref{rbfrep}). The other delay dimensions will simply repeat the values of the leading term (with their respective time delays applied). 


\subsubsection{Finding the hyper-parameters R and B in $\bchi$}

When performing DDF we are faced with the choice of picking a good value for hyper-parameters that affect our training and forecasting. For the case of the Gaussian RBF we have to pick an R value for the coefficient in the exponent of the RBF and a B value for the regularization term in Ridge Regression. To find a good value of B and R, it is typically easiest to perform the brute force method of grid searching across orders of magnitude of R and B; once a good result is found, further sweeps can be conducted with greater precision if greater accuracy is desired. Alternatively, one method the reader could use would be to use a genetic algorithm to find a precise result after performing a wide grid sweep (this is discussed in the Future Improvements to DDF section under Differential Evolution).

It is the experience of the authors that if a large grid sweep across many orders of magnitude of R and B fail to provide any good results that the next best course of action would be to take another look at the function representation and try something new. We typically start with a basic order of magnitude sweep for both B and R starting both values at 1e-8 and going up to 1e+1 (the best value for B and R are very data set dependent, so initial large sweeps are always necessary). If the function representation is not working well, or is a bad fit for the data, then our experience has taught us that it is a fruitless endeavour to keep grinding away at testing more and more hyper-parameter values.

\subsection{Programming DDF}
We did all of our testing and programming in Python. Here we will offer a few, hopefully, helpful suggestions for the reader in performing their own DDF calculations as well as provide sample code.

\subsubsection{PreBuilt DDF Code in Python}
Here is a link to the code we used (written and tested in Python) that have been published in GitHub. It includes the python scripts and Jupyter Notebooks used to get the results in this paper, it also includes a few examples as well as some information as to how we solve the SWE to generate the data we use~\cite{clarkgithub}
%\href{https://github.com/RandarserousRex/Driven_SWE_DDF}{DDFCode}

%\href{https://github.com/RandarserousRex/SWE-DDF}{SWE DDF Method Git Hub (Includes both TDE and non TDE methods)}

\subsubsection{Memory Management}
An important note to consider is the size of the matrices involved in training, for they can grow to the size of gigabytes. For example, the largest matrix involved in training is the $\M$ matrix consisting of all values of the RBF's at all times, an N by $N_c$, can have a length of up to 25,000 data points with as many as 5,000 centers. Typically we use float64 for all our values resulting this matrix using $25,000*5,000*8/1e9=1$ Gigabyte, then we'll need another gigabyte for its transpose. This could be a limiting factor if one is running multiple tests in parallel on a CPU or on a cluster with limited storage.

\subsubsection{Parallelizing DDF}
This section serves less as a guide and more as a suggestion and an obvious disclaimer to take advantage of the ability to run DDF code in parallel. The grid searching method discussed in the "Finding Hyper Parameters" section of the appendix lends itself very readily to parallel programming. Even the Differential Evolution method that is discussed in the Future Improvements to DDF section can have it's trials ran in parallel. Note that a single trial of DDF doesn't have much potential for parallel operations as the training is just matrix multiplication and the forecasting is a step by step process that relies of the result of the previous step's calculation.

\subsection{Future Improvements to DDF}
DDF isn't perfect and there is still much left to be explored with this method. There are still many RBF's that we never tested that could potentially work better than the Gaussian we've had success with. There are also smarter ways of choosing 
hyper-parameters other than brute force grid searching, such as using genetic algorithms like Differential Evolution.

\subsubsection{Centers and RBF's}
We took for granted that the obvious choices for DDF worked out as well as they did, choosing Gaussian RBF's and K-means clustering to put together our DDF experiments were ideas commonly used in the RBF literature~\cite{wu2012using}; however, they are far from the only good ideas there and others could possibly work better.

The centers could also be chosen by emphasizing places where the first or second derivative are greatest; this idea could work well because it could saturate regions where not many data points exist.~\cite{sanchez1995second}\\

\subsubsection{Differential Evolution}
Differential Evolution is a genetic algorithm that could be implemented into DDF to improve our ability to create DDF models of observed systems by more precisely picking out sets of hyper parameters. It works by initiating a parent set of 
hyper-parameters from a user defined uniform distribution. From there, new "children" sets of hyper parameters are made from combining the parents in a algorithmic way and comparing the new forecast that is made to the old one. A user defined cost function is used to compare the the children to their parents, as generations go by, parents will gradually be replaced by children until all the parents converge on a minimum in the cost function. If the user is able to define a clever cost function, they could get a result that hope to surpass those from simple grid searching. The full method is described in ~\cite{storn1997differential}.



\newpage
\begin{thebibliography}{10}

\bibitem{abar96}
H.~D.~I. Abarbanel.
\newblock {\em The Analysis of Observed Chaotic Data}.
\newblock Springer-Verlag, New York, 1996.

\bibitem{abar2021}
H.~D.~I. Abarbanel.
\newblock {\em The Statistical Physics of Data Assimilation and Machine
  Learning}.
\newblock Cambridge University Press, 2022.

\bibitem{bocquet2021}
J.~Brajard, A.~Carrassi, M.~Bocquet, and L.~Bertino.
\newblock Combining data assimilation and machine learning to infer unresolved
  scale parametrization.
\newblock {\em Phil. Trans. R. Soc. A}, 379:20200086, 2021.

\bibitem{broom88}
D.~S. Broomhead and David Lowe.
\newblock Multivariable functional interpolation and adaptive networks.
\newblock {\em Complex Systems}, 2:321-- 355, 1988.

\bibitem{buhmann09}
Martin~D. Buhmann.
\newblock {\em Radial Basis Functions (Theory and Implementations)}.
\newblock Cambridge University Press, 2009.

\bibitem{roysoc2021}
Matthew Chantry, Hannah Christensen, Peter Dueben, and Tim Palmer.
\newblock Machine learning for weather and climate modelling.
\newblock {\em Philosphical Transactions of the Royal Society A}, 379, 2021.

\bibitem{chen2022predicting}
Xiaoqian Chen, Balasubramanya~T Nadiga, and Ilya Timofeyev.
\newblock Predicting shallow water dynamics using echo-state networks with
  transfer learning.
\newblock {\em GEM-International Journal on Geomathematics}, 13(1):20, 2022.

\bibitem{clarkgithub}
Randall Clark.
\newblock https://github.com/RandarserousRex/Driven\_SWE\_DDF.

\bibitem{comrie1997comparing}
Andrew~C Comrie.
\newblock Comparing neural networks and regression models for ozone
  forecasting.
\newblock {\em Journal of the Air \& Waste Management Association},
  47(6):653--663, 1997.

\bibitem{daw2017physics}
Arka Daw, Anuj Karpatne, William~D Watkins, Jordan~S Read, and Vipin Kumar.
\newblock Physics-guided neural networks (pgnn): An application in lake
  temperature modeling.
\newblock In {\em Knowledge-Guided Machine Learning}, pages 353--372. Chapman
  and Hall/CRC, 2017.

\bibitem{du2006neural}
Ke-Lin Du and Madisetti N.~S. Swamy.
\newblock {\em Neural networks in a softcomputing framework}.
\newblock Springer Science \& Business Media, 2006.

\bibitem{dueben2018challenges}
Peter~D Dueben and Peter Bauer.
\newblock Challenges and design choices for global weather and climate models
  based on machine learning.
\newblock {\em Geoscientific Model Development}, 11(10):3999--4009, 2018.

\bibitem{eckmann85}
J.-P. Eckmann and David Ruelle.
\newblock Ergodic theory of chaos and strange attractors.
\newblock {\em Reviews of Modern Physics}, 57:617--656, 1985.

\bibitem{fraser89}
A.~M. Fraser.
\newblock {\em Information Theory and Strange Attractors}.
\newblock PhD thesis, University of Texas, Austin, 1989.

\bibitem{fraser86}
A.~M. Fraser and H.~L. Swinney.
\newblock Independent coordinates for strange attractors from mutual
  information.
\newblock {\em Physical Review A}, 33:1134--1140, 1986.

\bibitem{geer2021}
Alan~J. Geer.
\newblock Learning earth system models from observations: machine learning or
  data assimilation?
\newblock {\em Phil. Trans. R. Soc. A}, 379:20200089, 2021.

\bibitem{gottwald2021combining}
Georg~A Gottwald and Sebastian Reich.
\newblock Combining machine learning and data assimilation to forecast
  dynamical systems from noisy partial observations<? a3b2 show [editpick]?>.
\newblock {\em Chaos: An Interdisciplinary Journal of Nonlinear Science},
  31(10):101103, 2021.

\bibitem{grover2015deep}
Aditya Grover, Ashish Kapoor, and Eric Horvitz.
\newblock A deep hybrid model for weather forecasting.
\newblock In {\em Proceedings of the 21th ACM SIGKDD international conference
  on knowledge discovery and data mining}, pages 379--386, 2015.

\bibitem{gruber98}
Marvin Gruber.
\newblock {\em Improving Efficiency by Shrinkage: The James–Stein and Ridge
  Regression Estimators}.
\newblock Boca Raton: CRC Press, 1998.

\bibitem{hardy71}
Rolland~L. Hardy.
\newblock Multiquadric equations of topography and other irregular surfaces.
\newblock {\em Journal of Geophysical Research}, 76:1905--1915, 1971.

\bibitem{hirata06}
Yoshito Hirata, Hideyuki Suzuki, and Kazuyuki Aihara.
\newblock Reconstructing state spaces from multivariate data using variable
  delays.
\newblock {\em Physical Review E}, 74:026202, 2006.

\bibitem{huang2020survey}
Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese
  Thamo, Min Wu, and Xinping Yi.
\newblock A survey of safety and trustworthiness of deep neural networks:
  Verification, testing, adversarial attack and defence, and interpretability.
\newblock {\em Computer Science Review}, 37:100270, 2020.

\bibitem{jiang94}
Shi Jiang, Fei-Fei Jin, and Michael Ghil.
\newblock Multiple equilibria, periodic, and aperiodic solutions in a
  wind-driven, double-gyre, shallow-water model.
\newblock {\em Journal of Physical Oceanography}, 25:764--786, 1994.

\bibitem{judd95}
Kevin Judd and Alistair Mees.
\newblock On selecting models for nonlinear time series.
\newblock {\em Physica D}, 82:426--444, 1995.

\bibitem{kantz04}
H.~Kantz and T.~Schreiber.
\newblock {\em Nonlinear Time Series Analysis, 2nd ed.}
\newblock Cambridge University Press, Cambridge, UK, 2004.

\bibitem{kartal2023assessment}
Serkan Kartal.
\newblock Assessment of the spatiotemporal prediction capabilities of machine
  learning algorithms on sea surface temperature data: A comprehensive study.
\newblock {\em Engineering Applications of Artificial Intelligence},
  118:105675, 2023.

\bibitem{kashinath2021physics}
K~Kashinath, M~Mustafa, A~Albert, JL~Wu, C~Jiang, S~Esmaeilzadeh,
  K~Azizzadenesheli, R~Wang, A~Chattopadhyay, A~Singh, A.~Manepalli,
  D.~Chirila, R.~Yu, R.~Walters, B.~White, H.~Xiao, H.~A. Tchelepi, P.~Marcus,
  A.~Anandkumar, P.~Hssanzadeh, and Prabhat.
\newblock Physics-informed machine learning: case studies for weather and
  climate modelling.
\newblock {\em Philosophical Transactions of the Royal Society A},
  379(2194):20200093, 2021.

\bibitem{knighton2019potential}
James Knighton, Geoff Pleiss, Elizabeth Carter, Steven Lyon, M~Todd Walter, and
  Scott Steinschneider.
\newblock Potential predictability of regional precipitation and discharge
  extremes using synoptic-scale climate information via machine learning: An
  evaluation for the eastern continental united states.
\newblock {\em Journal of Hydrometeorology}, 20(5):883--900, 2019.

\bibitem{krasnopolsky2002neural}
Vladimir~M Krasnopolsky, Dmitry~V Chalikov, and Hendrik~L Tolman.
\newblock A neural network technique to improve computational efficiency of
  numerical oceanic models.
\newblock {\em Ocean Modelling}, 4(3-4):363--383, 2002.

\bibitem{mackowiak2021generative}
Radek Mackowiak, Lynton Ardizzone, Ullrich Kothe, and Carsten Rother.
\newblock Generative classifiers as a basis for trustworthy image
  classification.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2971--2981, 2021.

\bibitem{micc86}
Charles~A. Micchelli.
\newblock Interpolation of scattered data: Distance matrices and conditionally
  positive definite functions.
\newblock {\em Constructive Approximation}, 2:11--22, 1986.

\bibitem{nieves2021predicting}
Veronica Nieves, Cristina Radin, and Gustau Camps-Valls.
\newblock Predicting regional coastal sea level changes with machine learning.
\newblock {\em Scientific Reports}, 11(1):1--6, 2021.

\bibitem{olver17}
Peter~J. Olver.
\newblock Nonlinear ordinary differential equations.
\newblock Technical report, University of Minnesota, 2017.
\newblock http://www-users.math.umn.edu/\textasciitilde olver/.

\bibitem{olver2020}
Peter~J. Olver.
\newblock {\em Introduction to Partial Differential Equations}.
\newblock Springer Science+Business Media, 2020.
\newblock Undergraduate Texts in Mathematics.

\bibitem{ouala2020learning}
Said Ouala, Duong Nguyen, Lucas Drumetz, Bertrand Chapron, Ananda Pascual,
  Fabrice Collard, Lucile Gaultier, and Ronan Fablet.
\newblock Learning latent dynamics for partially observed chaotic systems.
\newblock {\em Chaos: An Interdisciplinary Journal of Nonlinear Science},
  30(10):103121, 2020.

\bibitem{pedlosky1986}
Joseph Pedlosky.
\newblock {\em Geophysical Fluid Dynamics, 2$^{nd}$ Edition}.
\newblock Springer Verlag, 1986.
\newblock ISBN 0-387-96387-1.

\bibitem{powell02}
M.~D.~J. Powell.
\newblock Radial basis function methods for interpolation to functions of many
  variables.
\newblock {\em Fifth Hellenic-European Conference on Computer Mathematics and
  its Applications; Athens, September 2001}, 2002.

\bibitem{press07}
William~H. Press, Saul~A. Teukolsky, William~T. Vetterling, and Brian~P.
  Flannery.
\newblock {\em Numerical recipes: the art of scientific computing, 3$rd$
  Edition}.
\newblock Cambridge University Press, 2007.

\bibitem{echam5}
E.~Roeckner, G.~Bauml, L.~Bonaventura, R.~Brokopf, M.~Esch, M.~Giorgetta,
  S.~Hagemann, I.~Kirchner, L.~Komblueh, E.~Manzini, A.~Rhodin, U.~Schlese,
  U.~Schulzweida, and A.~Tompkins.
\newblock The atmospheric general circulation model echam5, part1, model
  description.
\newblock Technical Report 349, The Max Planck Institute for Meteorology,
  Hamburg, 2003.

\bibitem{sadourny75}
Robert Sadourny.
\newblock The dynamics of finite-difference models of the shallow water
  equations.
\newblock {\em Journal of the Atmospheric Sciences}, 32:680--689, 1975.

\bibitem{sanchez1995second}
V~David S{\'a}nchez~A.
\newblock Second derivative dependent placement of rbf centers.
\newblock {\em Neurocomputing}, 7(3):311--317, 1995.

\bibitem{embed91}
T.~J. Sauer, J.~A. Yorke, and M.~Casdagli.
\newblock Embedology.
\newblock {\em Journal of Statistical Physics}, 65:579--616, 1991.

\bibitem{schab95}
Robert Schaback.
\newblock Error estimates and condition numbers for radial basis function
  interpolation.
\newblock {\em Advances in Computational Mathematics}, 3:251--264, 1995.

\bibitem{schultz2021can}
Martin~G Schultz, Clara Betancourt, Bing Gong, Felix Kleinert, Michael
  Langguth, Lukas~Hubert Leufen, Amirpasha Mozaffari, and Scarlet Stadtler.
\newblock Can deep learning beat numerical weather prediction?
\newblock {\em Philosophical Transactions of the Royal Society A},
  379(2194):20200097, 2021.

\bibitem{shi2015convolutional}
Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and
  Wang-chun Woo.
\newblock Convolutional lstm network: A machine learning approach for
  precipitation nowcasting.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{ecmwfbasics}
ECMWF Staff.
\newblock Ifs documentation-cy47r3; part iii: Dynamics and numerical
  procedures.
\newblock Technical report, European Center for Medium Range Weather
  Forecasting, 2021.
\newblock https://www.ecmwf.int/en/publications/ifs-documentation.

\bibitem{stevens2019}
Bjorn Stevens, Masaki Satoh, Ludovic Auger, Joachim Biercamp, Christopher~S.
  Bretherton, Xi~Chen, Peter Duben, Falko Judt, Marat Khairoutdinov, Daniel
  Klocke, Chihiro Kodama, Luis Kornblueh, Shian-Jiann Lin, Philipp
  Philipp~Neumann, William~M. Putman, Niklas Rober, Ryosuke Shibuya, Benoit
  Vanniere, Pier~Luigi Vidale, Nils Wedi, and Linjiong Zhou.
\newblock Dyamond: the dynamics of the atmospheric general circulation modeled
  on non-hydrostatic domains.
\newblock {\em Progress in Earth and Planetary Science}, 6:61--78, 2019.

\bibitem{stuart2016}
A.~L. Stewart and P.~J. Dellar.
\newblock An energy and potential enstrophy conserving numerical scheme for the
  multi-layer shallow water equations with complete coriolis force.
\newblock {\em J Comput Phys.}, 313:99--120, 2016.

\bibitem{storn1997differential}
Rainer Storn and Kenneth Price.
\newblock Differential evolution--a simple and efficient heuristic for global
  optimization over continuous spaces.
\newblock {\em Journal of global optimization}, 11(4):341--359, 1997.

\bibitem{takens81}
F.~Takens.
\newblock Detecting strange attractors in turbulence.
\newblock {\em Lecture Notes in Math.}, 898:366--381, 1981.

\bibitem{Vallis17}
Geoffrey~K. Vallis.
\newblock {\em Atmosphere and Ocean Fluid Dynamics: Fundamentals and
  Large-Scale Circulation}.
\newblock Cambridge University Press, 2017.
\newblock doi: 10.101.1017/9781107588417.

\bibitem{vanw21}
Wessel~N. van Wieringen.
\newblock Lecture notes on ridge regression,; version 0.40.
\newblock {\em https://arxiv.org/1509.09169}, 2021.

\bibitem{wandel2020learning}
Nils Wandel, Michael Weinmann, and Reinhard Klein.
\newblock Learning incompressible fluid dynamics from scratch--towards fast,
  differentiable fluid models that generalize.
\newblock {\em arXiv preprint arXiv:2006.08762}, 2020.

\bibitem{dwp2021}
Duncan Watson-Parris.
\newblock Machine learning for weather and climate are worlds apart.
\newblock {\em Phil. Trans. R. Soc. A}, 379:20200098, 2021.

\bibitem{wedi20}
Nils~P. Wedi, Inna Polichtchouk, Peter Dueben, Valentine~G. Anantharaj, Peter
  Bauer, Souhail Boussetta, Philip Browne, Willem Deconinck, Wayne Gaudin, Ioan
  Hadade, Sam Hatfield, Olivier Iffrig, Philippe Lopez, Pedro Maciel, Andreas
  Mueller, Sami Saarinen, Irina Sandu, Tiago Tiago~Quintino, and Frederic
  Vitart.
\newblock A baseline for global weather and climate simulations at 1 km
  resolution.
\newblock {\em Journal of Advances in Modeling Earth Systems},
  12:e2020MS002192, 2020.

\bibitem{wu2012using}
Yue Wu, Hui Wang, Biaobiao Zhang, and K-L Du.
\newblock Using radial basis function networks for function approximation and
  classification.
\newblock {\em International Scholarly Research Notices}, 2012, 2012.

\bibitem{xiao2019spatiotemporal}
Changjiang Xiao, Nengcheng Chen, Chuli Hu, Ke~Wang, Zewei Xu, Yaping Cai, Lei
  Xu, Zeqiang Chen, and Jianya Gong.
\newblock A spatiotemporal deep learning model for sea surface temperature
  field prediction using time-series satellite data.
\newblock {\em Environmental Modelling \& Software}, 120:104502, 2019.

\bibitem{yildiz2021learning}
S{\"u}leyman Y{\i}ld{\i}z, Pawan Goyal, Peter Benner, and B{\"u}lent
  Karas{\"o}zen.
\newblock Learning reduced-order dynamics for parametrized shallow water
  equations from data.
\newblock {\em International Journal for Numerical Methods in Fluids},
  93(8):2803--2821, 2021.

\bibitem{young2022deep}
Charles~D Young and Michael~D Graham.
\newblock Deep learning delay coordinate dynamics for chaotic attractors from
  partial observable data.
\newblock {\em arXiv preprint arXiv:2211.11061}, 2022.

\end{thebibliography}
%\bibliography{regional_linenumbers01_28_23.bbl}
%\bibliographystyle{plain}
%\bibliographystyle{alpha}
%\bibliographystyle{copernicus}
\end{document}