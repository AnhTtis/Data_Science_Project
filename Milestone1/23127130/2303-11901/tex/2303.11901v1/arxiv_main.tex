\documentclass[final]{etna}%{siamltex} %[final,leqno,letterpaper]
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath,amssymb}
\usepackage{mathtools}
\usepackage[dvipsnames]{xcolor}
\usepackage{soul} % for the command \hl
\usepackage{adjustbox}
%\usepackage{lineno}
\usepackage{graphicx}
\usepackage{subcaption}

\renewcommand{\algorithmicrequire}{\textbf{Input: }}
\renewcommand{\algorithmicensure}{\textbf{Output: }}



\newcommand{\Atilde}{\widetilde{A}}
\newcommand{\Ahat}{\widehat{A}}
\newcommand{\btilde}{\tilde{b}}
\newcommand{\Cbar}{\bar{C}}
\newcommand{\ctilde}{\tilde{c}}
\newcommand{\Gbar}{\bar{G}}
\newcommand{\Hbar}{\bar{H}}
\newcommand{\Lbar}{\bar{L}}
\newcommand{\Rbar}{\bar{R}}
\newcommand{\rbar}{\bar{r}}
\newcommand{\sbar}{\bar{s}}
\newcommand{\rtilde}{\tilde{r}}
\newcommand{\Ubar}{\bar{U}}
\newcommand{\Vhat}{\widehat{V}}
\newcommand{\Vbar}{\bar{V}}
\newcommand{\vbar}{\bar{v}}
\newcommand{\Zbar}{\bar{Z}}
\newcommand{\zbar}{\bar{z}}
\newcommand{\Zhat}{\widehat{Z}}
\newcommand{\wbar}{\bar{w}}
\newcommand{\ybar}{\bar{y}}
\newcommand{\xbar}{\bar{x}}
\newcommand{\xhat}{\widehat{x}}

\newcommand{\betabar}{\bar{\beta}}



\setbibdata{1}{xx}{46}{2017} %First page/Last page/volume/year



\title{The stability of split-preconditioned FGMRES in four precisions\thanks{%
%Received... Accepted... Published online on... Recommended by....
%Version of \cred{\today}. 
Work supported by ERC Starting Grant No. 101075632, and the Exascale Computing Project (17-SC-20-SC), a collaborative
effort of the U.S. Department of Energy Office of Science and the National Nuclear Security Administration. The
first author was additionally supported by Charles University Research program no. UNCE/SCI/023.
}}

\author{Erin Carson\footnotemark[2]
        \and Ieva Dau\v{z}ickait\.{e}\footnotemark[2]}

\shorttitle{Mixed precision FGMRES} 
\shortauthor{E.~Carson AND I.~Dau\v{z}ickait\.{e}}



\begin{document}

\maketitle


\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\footnotetext[2]{Faculty of Mathematics and Physics, Charles University, Sokolovsk\'{a} 366/84, 180 00, Prague, Czech Republic. email: carson@karlin.mff.cuni.cz, dauzickaite@karlin.mff.cuni.cz.}


\begin{abstract}
We consider the split-preconditioned FGMRES method in a mixed precision framework, in which four potentially different precisions can be used for computations with the coefficient matrix, application of the left preconditioner, and application of the right preconditioner, and the working precision. Our analysis is  applicable to general preconditioners. We obtain bounds on the backward and forward errors in split-preconditioned FGMRES. Our analysis further provides insight into how the various precisions should be chosen; a suitable selection guarantees a backward error on the order of the working precision. 
\end{abstract}

\begin{keywords}
mixed precision, FGMRES, iterative methods, roundoff error, split-preconditioned
\end{keywords}

\begin{AMS}
65F08, 65F10, 65F50, 65G50, 65Y99
\end{AMS}


\section{Introduction}
We consider the problem of solving a linear system of equations
\begin{equation}\label{eq:Ax=b}
    Ax=b,
\end{equation}
where $A \in \mathbb{R}^{n \times n}$ is nonsymmetric and $x,b \in \mathbb{R}^n$. When $A$ is large and sparse, the iterative generalised minimal residual method (GMRES) or its flexible variant (FGMRES) are often used for solving \eqref{eq:Ax=b}; see, for example, \cite{saad2003iterative}. 
In these and other Krylov subspace methods, preconditioning is an essential ingredient. Given a preconditioner $P = M_L M_R$, the problem \eqref{eq:Ax=b} is transformed to
\begin{align}
   M_L^{-1} A M_R^{-1} \tilde{x} = \, &  M_L^{-1} b, \label{eq:Ax=b_split_precond}\\
   \textrm{where } M_R^{-1} \tilde{x} = \, & x. \nonumber
\end{align}
Note that a particular strength of FGMRES is that it allows the right preconditioner to change throughout the iterations. Although for simplicity, we consider the case here where the preconditioners are static, our results can be easily extended to allow dynamic preconditioning. 


The emergence of mixed precision hardware has motivated work in developing mixed precision algorithms for matrix computations; see, e.g., the recent surveys \cite{abdelfattah2021survey,higham_mary_2022}. Modern GPUs offer double, single, half, and even quarter precision, along with specialized tensor core instructions; see, e.g., \cite{h100}. The use of lower precision can offer significant performance improvements, although this comes at a numerical cost. With fewer bits, we have a greater unit roundoff and a smaller range of representable numbers. The goal is thus to selectively use low precision in algorithms such that performance is improved without adversely affecting the desired numerical properties. 


Mixed precision variants of GMRES and FGMRES with different preconditioners have been proposed and analyzed in multiple papers. 
Arioli and Duff \cite{arioli2009using} analyzed a two-precision variant of FGMRES in which the right-preconditioner is constructed using an LU decomposition computed in single precision and applied in either single or double precision, and other computations are performed in double precision. They proved that in this setting, a backward error on the order of double precision in attainable. 
%
The authors of \cite{lindquist2020improving} develop a mixed precision variant of left-preconditioned GMRES in a mix of single and double precisions, requiring only a few operations to be performed in double precision. Their numerical experiments show that they can obtain a backward error to the level of double precision. 
%
Vieubl\'{e} \cite{vieuble_thesis} proved the backward stability of left-preconditioned GMRES in four precisions, following the earlier works \cite{carson2017new} and \cite{h:21} which analyzed left-preconditioned GMRES in two and three precisions, respectively. In general, different precisions can be used for computing the preconditioner, matrix-vector products with $A$, matrix-vector products or solves with the general preconditioner(s), and the remaining computations. 
We refer the readers to the recent surveys \cite{abdelfattah2021survey,higham_mary_2022} for other examples.

The stability of split-preconditioned GMRES and FGMRES has not been analysed in either uniform or mixed precision even though the structure of some problems makes it desirable to construct and apply a split-preconditioner rather than left or right ones. The work \cite{arioli2007note} showed that uniform precision FGMRES with a specific right-preconditioner is backward stable while this is not the case for GMRES and that FGMRES is more robust than GMRES. We thus focus on split-preconditioned FGMRES in this paper and develop a mixed precision framework allowing for four potentially different precisions for the following operations:
computing matrix-vector products with $A$, applying the left-preconditioner $M_L$, applying the right-preconditioner $M_R$, and all other computations. FGMRES computes a series of approximate solutions $x_k$ from Krylov subspaces to \eqref{eq:Ax=b}. The Arnoldi method is employed to generate the basis for the Krylov subspaces like in GMRES, but FGMRES stores the right-preconditioned basis as well. The particular algorithm is shown in Algorithm~\ref{alg:fgmres}. Our analysis considers general preconditioners, only requiring an assumption on the error in applying its inverse to a vector, and is thus widely applicable. 

The paper is outlined as follows. We bound the backward errors in Section~\ref{sec:finite_prec_analysis} while also providing guidance for setting the four precisions such that backward error to the desired level is attainable. To make the results of the analysis more concrete, in Section~\ref{sec:example_LU_precond} we bound the quantities involved for the example of LU preconditioners, and then present a set of numerical experiments on both synthetic problems and problems from SuiteSparse \cite{SuiteSparse}. In Section~\ref{sec:conclusions} we make concluding remarks.

\begin{algorithm}
\caption{Split-preconditioned FGMRES for solving \eqref{eq:Ax=b} in four precisions}\label{alg:fgmres}
\algorithmicrequire  matrix $A \in \mathbb{R}^{n \times n}$, right hand side $b \in \mathbb{R}^n$, preconditioner $P = M_L M_R$,  maximum number of iterations \textit{maxit}, convergence tolerance $\tau$, precisions $u$, $u_A$, $u_L$ and $u_R$\\
\algorithmicensure approximate solution $x_k$
\begin{algorithmic}
\State  \textrm{initialize } $x_0$; 
\State $ r_0 = M_L^{-1} b - M_L^{-1}Ax_0$; \Comment{$u_A$, $u_L$ and $u$}
\State $\beta = \Vert r_0 \Vert$; $v_1 = r_0 / \beta$; \Comment{$u$}  
\State  $k=0$; $it = 0$; \textit{convergence} = \textit{false}
\While {\textit{convergence} = \textit{false} and $it < maxit$}
\State $k = k +1$; $it = it +1$ \Comment{$u$}
\State $z_k = M_R^{-1} v_k$ \Comment{$u_R$}
\State $w = M_L^{-1} A z_k$ \Comment{$u_A$ and $u_R$}
\For {$i = 1,\dots,k$}
\State $h_{i,k} = v_i^T w$ \Comment{$u$} 
\State $w = w - h_{i,k} v_i$ \Comment{$u$} 
\EndFor
\State $h_{k+1,k} = \Vert w \Vert$ \Comment{$u$} 
\State $Z_k = [z_1, \dots, z_k]$; $H_k = \{h_{i,j}\}_{1 \leq i \leq j+1; 1 \leq j \leq k}$
\State $y_k = \textrm{arg} \min_y \Vert \beta e_1 - H_k y \Vert$ \Comment{$u$} 
\If {$\Vert \beta e_1 - H_k y_k \Vert \leq \tau  \beta $}%\Vert b - A x_0 \Vert
\State $x_k = x_0 + Z_k y_k $ and $r = b - Ax_k$ \Comment{$u$ and $u_A$} 
\State \textit{convergence} = \textit{true}
\Else
\State $v_{k+1} = w/h_{k+1,k}$; $V_{k+1} = [v_1, \dots, v_{k+1}]$  \Comment{$u$} 
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}



\section{Finite precision analysis of FGMRES in four precisions}\label{sec:finite_prec_analysis}
From the Rigal-Gaches Theorem (see \cite[Theorem 7.1]{high:ASNA2}), the normwise relative backward error is given by 
\[
\min\{\varepsilon:(A+\Delta A)x_k = b+\Delta b, \|\Delta A\|\leq \varepsilon \|A\|, \|\Delta b\|\leq \varepsilon\|b\| \} = \frac{\| r_k\|}{\|A\|\|x_k\|+\|b\|},
\]
where $r_k=b-Ax_k$. We aim to bound this quantity when $x_k$ is the approximate solution produced by Algorithm~\ref{alg:fgmres}. To account for various ways in which the preconditioner can be computed and some constraints on $A$ resulting in the need for different precisions, we assume that
\begin{itemize}
    \item computations with $A$ are performed in precision with unit roundoff $u_A$;
    \item computations with $M_L$ are performed in precision with unit roundoff $u_L$;
    \item computations with $M_R$ are performed in precision with unit roundoff $u_R$;
    \item the precision for other computations (the working precision) has unit roundoff $u$.
\end{itemize}
Using the approach in \cite{vieuble_thesis}, we assume that the application of $M_L^{-1}$ and $M_R^{-1}$ can be computed in a way such that
\begin{align}
    fl(M_L^{-1} w_j) = & M_L^{-1} w_j + \Delta M_{L,j} w_j, \quad  \lvert \Delta M_{L,j} \rvert \leq c(n) u_L E_{L,j}, \label{eq:Ml_assump} \\
    fl(M_R^{-1} w_j) = & M_R^{-1} w_j + \Delta M_{R,j} w_j, \quad  \lvert \Delta M_{R,j} \rvert \leq c(n) u_R E_{R,j}, \label{eq:Mr_assump}
\end{align}
where $fl(\cdot)$ denotes the quantity computed in floating point arithmetic, $E_{L,j}$ and $E_{R,j}$ have positive entries, $w_j \in \mathbb{R}^n$, and $c(n)$ is a constant that depends on $n$ only. We define
\begin{equation*}
    \Atilde \coloneqq M_L^{-1} A \quad \text{ and } \quad \btilde \coloneqq M_L^{-1} b
\end{equation*}
and assume that matrix-vector products with $\Atilde$ can be computed so that
\begin{equation*}
    fl(\Atilde z_j) = (M_{L,j}^{-1} +  \Delta M_{L,j}) (A +  \Delta A_j) z_j.
\end{equation*}
Denoting
\begin{equation*}
    u_A \psi_{A,j} = \frac{\Vert M_L^{-1} \Delta A_j z_j \Vert}{\Vert \Atilde \Vert \Vert z_j \Vert} \quad \text{ and } \quad   u_L \psi_{L,j} = \frac{\Vert \Delta M_{L,j} A z_j\Vert}{\Vert \Atilde \Vert \Vert z_j \Vert},
\end{equation*}
where here and in the rest of the paper $\Vert \cdot \Vert$ denotes the 2-norm, and ignoring the second order terms, we can write 
\begin{gather*}
    fl(\Atilde z_j) \approx \Atilde z_j + f_j,\\
   \text{where } \Vert f_j \Vert \leq (u_A \psi_{A,j} + u_L \psi_{L,j})\Vert \Atilde \Vert \Vert z_j \Vert.
\end{gather*}

In the following, a standard error analysis approach is used, e.g., \cite{high:ASNA2}, and  we closely follow the analysis in \cite{arioli2007note} and \cite{arioli2009using}. The analysis is performed in the following stages:
\begin{enumerate}
    \item Bounding the computed quantities in the modified Gram-Schmidt (MGS) algorithm that returns $C^{(k)} = \begin{bmatrix} \btilde - \Atilde x_0 &  \Atilde Z_k \end{bmatrix} = V_{k+1} R_k$, where $ V_k^T V_k =   I_k$, $R_k = \, \begin{bmatrix} \beta e_1 & H_k \end{bmatrix}$ and $e_1 = \begin{bmatrix} 1 & 0 & \dots & 0\end{bmatrix}^T$.
    \item Solving the least-squares problem 
    \begin{equation}\label{eq:LS_problem}
       y_k = \textrm{arg} \min_y \Vert \beta e_1 - H_k y \Vert
    \end{equation}
    via QR employing Givens rotations and analysing its residual.
    \item Computing $x_k = x_0 + Z_k y_k$.
    \item Bounding $\Vert y_k \Vert$.
\end{enumerate}
Throughout the paper computed quantities are denoted with bars, that is, $\Cbar^{(k)}$ is the computed $C^{(k)}$, 
and $\kappa(A) = \Vert A \Vert \Vert A^{\dagger} \Vert$ is the 2-norm condition number of $A$. The second order terms in $u_A$, $u_L$, $u_R$, and $u$ are ignored. We drop the subscripts $j$ for $E_{L,j}$, $E_{R,j}$, $\Delta M_{L,j}$, $\Delta M_{R,j}$, $ \Delta A_j $, $\psi_{A,j}$ and $\psi_{L,j}$ and replace these quantities by their maxima over all $j$. $\kappa(B)$ is the 2-norm condition number of $B$. It is assumed that no overflow or underflow occurs. We present the main result here and refer the reader to Appendix~\ref{app:backward_err_proof} for the proof.


\begin{theorem}\label{th:backward_error}
Let $\xbar_k$ be the approximate solution to \eqref{eq:Ax=b_split_precond} computed by Algorithm~\ref{alg:fgmres}. Under the assumptions \eqref{eq:Ml_assump}, \eqref{eq:Mr_assump}, 
\begin{gather}
    2.12(n+1)u < 0.01 \quad \text{and} \quad c_0(n) u \kappa(C^{(k)}) < 0.1 \ \forall k, \label{eq:assumptions_nu_ukappaC} \\
     \lvert \sbar_k \rvert < 1 - u \quad \forall k, \label{eq:assumption_sk}
\end{gather}
where $c_0(n) = 18.53 n^{3/2}$ and $\sbar_k$ are the sines computed for the Givens rotations, and
\begin{equation}\label{eq:rho_def_assump}
    \rho \coloneqq 1.3 c_{13}(n,k) \Vert M_R \Vert \left( u \Vert \Zbar_k \Vert + u_R \Vert E_R \Vert \right) < 1,
\end{equation}
the residual for the left-preconditioned system is bounded by
\begin{equation}\label{eq:residual_bound}
\Vert \btilde - \Atilde  \xbar_k  \Vert \lesssim   \frac{1.3 c(n,k)}{1 - \rho} \left( \zeta_1 + \zeta_2 \right),
\end{equation}
where
\begin{gather}
    \zeta_1 \coloneqq \left(u + u_L \Vert E_L M_L \Vert \right) \Vert \btilde \Vert , \quad\text{and} \nonumber \\
    \zeta_2 \coloneqq \left( u + u_A \psi_A + u_L \psi_L \right) \Vert \Atilde \Vert 
 \left( \Vert \Zbar_k \Vert \Vert M_R(\xbar_k - \xbar_0) \Vert + \Vert \xbar_0 \Vert \right) \label{eq:zeta2_def}
\end{gather}
and the normwise relative backward error for the left-preconditioned system %$\Atilde \coloneqq M_L^{-1} A $ and $\btilde \coloneqq M_L^{-1} b$ 
is bounded by
\begin{equation}\label{eq:backward_error}
   \frac{ \Vert \btilde - \Atilde  \xbar_k  \Vert}{\Vert \btilde \Vert + \Vert \Atilde \Vert \Vert \xbar_k  \Vert} \lesssim   \frac{1.3 c(n,k)}{1 - \rho} \zeta,
\end{equation}
where
\begin{equation}\label{eq:zeta}
    \zeta \coloneqq \frac{\zeta_1 + \zeta_2 }{\Vert \btilde \Vert + \Vert \Atilde \Vert \Vert \xbar_k  \Vert }.
\end{equation}

\end{theorem}
We expect \eqref{eq:backward_error} to be dominated by $\zeta_2$, mainly due to the term $\Vert \Zbar_k \Vert \Vert M_R(\xbar_k - \xbar_0) \Vert$. As observed in \cite{arioli2007note,arioli2009using} and in our experiments (Sections~\ref{sec:numerics_dense} and \ref{sec:numerics_sparse}),  $\Vert \Zbar_k \Vert$ remains small in early iterations, but can be large if many iterations are needed for convergence. We expect the quantity $\Vert M_R(\xbar_k - \xbar_0) \Vert$ to aid in partially mitigating the size of $\Vert \Zbar_k \Vert$, so that $\zeta_2$ still gives good guarantees for the backward error. Note that if we were to obtain $\Vert \Atilde \Vert \Vert \xbar_k  \Vert $ in $\zeta_2$ by using $\Vert M_R(\xbar_k - \xbar_0) \Vert \leq \Vert M_R \Vert (\Vert \xbar_k \Vert + \Vert \xbar_0 \Vert)$, then we would introduce the term $\Vert M_R \Vert \Vert \Zbar_k \Vert$. Depending on the preconditioner, $\Vert M_R \Vert$ can be close to $\Vert A \Vert$ and for some problems $\Vert M_R \Vert \Vert \Zbar_k \Vert$ can grow rapidly, thus making \eqref{eq:backward_error} a large overestimate. We comment on how \eqref{eq:residual_bound} compares with other bounds for FGMRES available in the literature in the following section. The quantities $\psi_A$, $\psi_L$ and the role of different precisions are discussed in Section~\ref{sec:precisions}.

Bound \eqref{eq:backward_error} can be formulated with respect to the the original system, that is, without the left preconditioner, using inequalities $\Vert b - A  \xbar_k  \Vert \leq \Vert M_L \Vert \Vert \btilde - \Atilde  \xbar_k  \Vert$ and $\Vert b \Vert + \Vert A \Vert \Vert \xbar_k  \Vert \geq  (\Vert \btilde \Vert + \Vert \Atilde \Vert \Vert \xbar_k  \Vert )/\Vert M_L^{-1} \Vert$. We state the bound in the following corollary.   
\begin{corollary}
If the conditions in Theorem~\ref{th:backward_error} are satisfied, then the normwise relative backward error for the system \eqref{eq:Ax=b} is bounded by
\begin{equation*}
    \frac{ \Vert b - A  \xbar_k  \Vert}{\Vert b \Vert + \Vert A \Vert \Vert \xbar_k  \Vert} \lesssim   \frac{1.3 c(n,k)}{1 - \rho} \zeta \kappa(M_L).
\end{equation*}
\end{corollary}
The condition number of the left preconditioner weakens the result, yet for some preconditioners $\kappa(M_L)$ can be expected to be small, for example when $LU$ decomposition is used and $M_L=L$. We also note that a small backward error with respect to the preconditioned system and small $\kappa(\Atilde)$ implies small backward error with respect to the original system; see Section~\ref{sec:forward_error} for an explanation.


\subsection{Comparison with existing bounds}
We wish to compare our result with bound (5.6) in \cite{arioli2007note} for FGMRES with a general right-preconditioner and bound (3.32) in \cite{arioli2009using} for FGMRES right-preconditioned with $LU$ factorization computed in single precision. We set $M_L = I$, $u = u_A$, then $u_L = 0$ and $u_A \psi_A = u$. The bound \eqref{eq:residual_bound} becomes
\begin{equation*}
   \Vert b - A  \xbar_k  \Vert \lesssim \frac{1.3 c(n,k)u }{1 - \rho}  \left( \Vert b \Vert + \Vert A \Vert  \left( \Vert \Zbar_k \Vert \Vert M_R(\xbar_k - \xbar_0) \Vert + \Vert \xbar_0 \Vert \right) \right).
\end{equation*}
We thus recover the bound (5.6) in \cite{arioli2007note}, but ignoring the term $u^2 \Vert \xbar_0 \Vert$ and with a slightly different $\rho$. If we further set $\Gamma = \frac{\Vert M_R \Vert}{\Vert A  \Vert}$ and use $\Vert M_R(\xbar_k - \xbar_0) \Vert \leq \Vert M_R \Vert (\Vert \xbar_k \Vert + \Vert \xbar_0 \Vert)$, then our bound becomes 
\begin{equation*}
   \Vert b - A  \xbar_k  \Vert \lesssim \frac{1.3 c(n,k) u}{1 - \rho} \left( \Vert b \Vert + \Vert A \Vert \left( \Vert \xbar_k \Vert + \Vert \xbar_0 \right) \Vert \left( 1+ \Gamma \Vert A \Vert \Vert \Zbar_k \Vert  \right) \right).
\end{equation*}
The main aspect in which this bound differs from (3.32) in \cite{arioli2009using} is that in \cite{arioli2009using} the term $\Gamma \Vert A  \Vert \Vert \Zbar_k \Vert $ is controlled by a factor depending on $u_R$ and the precision in which the LU decomposition used as $M_R$ is computed. This comes from substitutions that rely on the specific $M_R$ when bounding $\Vert \ybar_k \Vert$. Thus, when more information on $M_R$ is available, reworking the bound for $\Vert \ybar_k \Vert$ may result in an improved bound. 

\subsection{Choosing the precisions}\label{sec:precisions}
We provide guidance on how the precisions should be set when the target backward error is of order $u$. In our experiments we observe that the achievable backward error is determined by $u + u_A \psi_A + u_L \psi_L$ and we hence ignore the term $\Vert \Zbar_k \Vert \Vert M_R(\xbar_k - \xbar_0) \Vert$ in this section. We also note that because of the structure of the former term, we do not expect the backward error to be reduced by setting $u_A$ or $u_L$ so that $u_A \psi_A \ll u$ or $u_L \psi_L \ll u$. The aim is thus to have $u \approx u_A \psi_A \approx u_L \psi_L$ in \eqref{eq:zeta2_def}. 
\begin{itemize}
    \item $u_A$. The precision for computations with $A$ should be chosen so that $u_A \approx u / \psi_A$. Numerical experiments with left-preconditioned GMRES in \cite{vieuble_thesis} show that for large $\kappa(A)$ and $\kappa(M_L)$ the quantity $\psi_A$ can be large and is driven by $\kappa(M_L)$. In such situations $u_A \ll u$ may be required. If, on the other hand, $\kappa(M_L)$ is small, then setting $u_A \approx u$ may be sufficient.  
    \item $u_L$. Guidance for setting $u_L$ comes from balancing $u \approx u_A \psi_A \approx u_L \psi_L$ and $u \approx u_L \Vert E_L M_L \Vert$. Based on the first expression, $u_L \approx u_A \psi_A / \psi_L$. Vieubl\'{e} argues that $\psi_L \leq \psi_A$ is likely, and if $\kappa(A)$ and $\kappa(M_L)$ are large then we may observe $\psi_L \ll \psi_A$ \cite{vieuble_thesis}. In these cases we can set $u_L \geq u_A$ and $u_L \gg u_A$, respectively. The quantity $\Vert E_L M_L \Vert$ depends on $M_L$ and the error in computing matrix-vector products with $M_L^{-1}$, which may be large for an ill conditioned $M_L$. In this case thus we may require $u_L \approx u$, which is consistent with the guidance for setting $u_A \ll u$.
    \item $u_R$. Our insight on $u_R$ comes from the sufficient, but not necessary condition \eqref{eq:rho_def_assump} (see Sections~\ref{sec:numerics_dense} and \ref{sec:numerics_sparse} for examples). It requires that $\Vert M_R \Vert \Vert E_R \Vert \ll u_R^{-1} $. $E_R$ depends on the forward error of matrix-vector products with $M_R$. If $\kappa(M_R)$ is large, we may need a small $u_R$ for the condition to be satisfied. When $\Vert M_R \Vert $ and $\kappa(M_R)$ are small, a large value for $u_R$ may suffice. Note that these comments take into account the backward error only and not the FGMRES iteration count. 
\end{itemize}
  
\subsection{Forward error}\label{sec:forward_error}
A rule of thumb says that the forward error can be bounded by multiplying the backward error by the condition number of the coefficient matrix; see, for example, \cite{high:ASNA2}. 
Using \eqref{eq:backward_error} thus gives the bound
\begin{equation}\label{eq:forw_err_left_precond}
    \frac{\Vert x - \xbar_k \Vert}{\Vert x \Vert} \leq  \frac{1.3 c(n,k)}{1 - \rho}\zeta \kappa(\Atilde),
\end{equation}
where $x$ is the solution to \eqref{eq:Ax=b_split_precond} and $\xbar_k$ is the output of FGMRES. 
Note that the bound depends on the condition number of the left-preconditioned matrix $\Atilde$. If $\kappa(\Atilde)$ and $\zeta$ are small, then the forward error is small too, and thus $\xbar_k \approx x$. Then $b - A\xbar_k = A(x - \xbar_k)$ is small and implies a small backward error with respect to the original system as previously noted.

The forward error bound can also be formulated with respect to the split-preconditioned matrix $\Ahat \coloneqq M_L^{-1} A M_R^{-1}$ as follows
\begin{equation}\label{eq:forw_err_split_prec}
    \frac{\Vert x - \xbar_k \Vert}{\Vert x \Vert} \leq  \frac{1.3c(n,k)}{1 - \rho} \zeta \kappa(\Ahat) \kappa(M_R).
\end{equation}
The bounds \eqref{eq:forw_err_left_precond} and \eqref{eq:forw_err_split_prec} suggest that guaranteeing a small forward error requires controlling the backward error and constructing the preconditioners so that either $\kappa(\Atilde)$ or both $\kappa(\Ahat)$ and $ \kappa(M_R)$ are small. If $A$ is ill conditioned, then achieving a small $\kappa(\Atilde)$ in \eqref{eq:forw_err_left_precond} requires an $M_L$ with a high condition number. Note that in this case, as discussed in the previous section, we may have to set $u_A \ll u$ and can get away with $u_L \gg u_A$. The bound \eqref{eq:forw_err_split_prec} indicates that if we achieve a small $\kappa(\Ahat)$ at the price of $\kappa(M_R) \approx \kappa(A)$ then we cannot guarantee a smaller forward error than when no preconditioning is used because unpreconditioned FGMRES is equivalent to unpreconditioned GMRES in uniform precision with backward error bounded by $\frac{cnu}{1-cnu}$, where $c$ is a constant \cite{paige2006modified}.

\section{Example: LU preconditioner}\label{sec:example_LU_precond}
We supplement the theoretical analysis in the previous section with an example. Assume that an approximate LU decomposition of $A$ is computed, for example in a low precision, and the computed factors $\Lbar$ and $\Ubar$ are used for preconditioning, i.e., $M_L = \Lbar$ and $M_R = \Ubar$. 
In Algorithm~\ref{alg:fgmres}, products with $A$ are computed in precision $u_A$ and hence
\begin{equation}\label{eq:bound_psiA}
    \psi_{A,j} = \frac{\Vert M_{L}^{-1} \Delta A_j z_j\Vert}{u_A \Vert \Atilde \Vert \Vert z_j \Vert} \leq \ctilde_1(n) \frac{ \Vert \lvert \Lbar^{-1} \rvert \lvert A \rvert \Vert \Vert z_j \Vert }{ \Vert \Lbar^{-1} A \Vert  \Vert z_j \Vert} = \ctilde_1(n) \frac{ \Vert \lvert \Lbar^{-1} \rvert \lvert A \rvert \Vert }{ \Vert \Lbar^{-1} A \Vert },
\end{equation}
where $\ctilde_i(n)$ is a constant that depend on $n$. We expect $\Vert \lvert \Lbar^{-1} \rvert \lvert A \rvert \Vert / \Vert \Lbar^{-1} A \Vert$ to be moderate for many systems and in this case setting $u_A = u$ may be sufficient.

We apply $M_L$ by solving a triangular system $\Lbar w_j = (A +  \Delta A_j) z_j$ via substitution in precision $u_L$. From standard results we know that the computed $\wbar_j$ satisfies
\begin{gather*}
    (\Lbar + \Delta L_j) \wbar_j = (A +  \Delta A_j) z_j, \\
    \text{where } \lvert  \Delta L_j \rvert \leq \ctilde_2(n) u_L \lvert  \Lbar \rvert.
\end{gather*}
Thus 
\begin{equation*}
\Delta M_{L,j} = \Lbar^{-1} - (\Lbar + \Delta L_j)^{-1} \approx \Lbar^{-1} \Delta L_j \Lbar^{-1}.    
\end{equation*}
We use this to bound $\psi_{L,j}$ as
\begin{equation}\label{eq:bound_psiL}
    \psi_{L,j} =  \frac{\Vert \Delta M_{L,j} A z_j\Vert}{u_L \Vert \Atilde \Vert \Vert z_j \Vert} \approx \frac{\Vert \Lbar^{-1} \Delta L_j \Lbar^{-1} A z_j\Vert}{u_L \Vert \Lbar^{-1} A \Vert \Vert z_j \Vert} \leq  \frac{\Vert \Lbar^{-1}   \Delta L_j \Vert \Vert \Lbar^{-1}  A \Vert \Vert z_j \Vert}{u_L \Vert \Lbar^{-1} A \Vert \Vert z_j \Vert}  \leq  \ctilde_3(n) \kappa_2(\Lbar).
\end{equation}
The bound \eqref{eq:bound_psiL} is obtained using the bound on the forward error of solving a triangular system. In general such systems are solved to high accuracy and thus we expect \eqref{eq:bound_psiL} to be a large overestimate. Note that bounds \eqref{eq:bound_psiA} and \eqref{eq:bound_psiL} hold for every $j$.


\subsection{Numerical example: synthetic systems}\label{sec:numerics_dense}
We perform numerical experiments in MATLAB R2021a\footnote{The code is available at \url{https://github.com/dauzickaite/mpfgmres/}.} using a setup similar to an example from \cite{arioli2009using}. An $n \times n$ coefficient matrix $A=UDV$ is constructed by generating random orthogonal $n \times n$ matrices $U$ and $V,$ and setting $D$ to be diagonal with elements $10^{-c(j-1)/(n-1)}$ for $j=1,2,\dots,n$. The condition number of $A$ is $10^c$ and we vary its value. The right hand side $b$ is a random vector with uniformly distributed entries. The preconditioner is computed as a low precision LU factorization. Namely, for $c \in \{1,2,\dots,5 \}$ we use $[\Lbar,\Ubar] = lu(mp(A,4))$, where $mp(\cdot,4)$ calls the Advanpix Multiprecision Computing Toolbox \cite{advanpix} and simulates precision accurate to four decimal digits; note that this has a smaller unit roundoff than IEEE half precision (see Table~\ref{tab:ieee_param} for the unit roundoff values). For $c \in \{6,7,\dots,10 \}$ we compute LU factorization in single precision using the built-in MATLAB single precision data type. We set $M_L = \Lbar$ and $M_R = \Ubar$, and $E_L = u_L \vert \Lbar^{-1}\vert  \vert \Lbar \vert \vert \Lbar^{-1}\vert $ and $E_R = u_R \vert \Ubar^{-1} \vert  \vert \Ubar \vert \vert \Ubar^{-1}\vert$. The left-preconditioner can slightly reduce the condition number of the coefficient matrix whereas the split-preconditioner achieves a high reduction (Table~\ref{tab:cond_no_different_c}). 

\begin{table}[]
 \caption{Unit roundoff $u$ for IEEE floating point arithmetics.}
    \centering
    \begin{tabular}{l|c}
        Arithmetic &  $u$  \\
         \hline
        fp16 (half)  &  $2^{-11} \approx 4.88 \times 10^{-4}$ \\
        fp32 (single) & $2^{-24} \approx 5.96 \times 10^{-8}$ \\
        fp64 (double) & $2^{-53} \approx 1.11 \times 10^{-16}$ \\
        fp128 (quadruple) & $2^{-113} \approx 9.63 \times 10^{-35}$
    \end{tabular}
    \label{tab:ieee_param}
\end{table}


We set the working precision $u$ to double. Bounds for $\psi_A$ in Table~\ref{tab:cond_no_different_c} indicate that there is no need for $u_A < u$, thus we choose $u_A = u$. The preconditioners are applied using all combinations of half, single, double, and quadruple precisions. Half precision is simulated via the \textit{chop} function \cite{HighamChop}, and Advanpix is used for quadruple precision. We expect $\kappa(M_L)$ to be a large overestimate for $\psi_L$. $\kappa(M_R)$ suggests that the condition $\rho <1$ in \eqref{eq:rho_def_assump} should be satisfied with $u_R$ set to any of the four precisions, except half for large $c$ values. The solver tolerance $\tau$ (see Algorithm~\ref{alg:fgmres}) is set to $4u$ and we use $x_0 = 0$. For the unpreconditioned system, FGMRES converges in 200 iterations when $c=1$ and does not converge for other $c$ values.

We show results for $c=5$ for all precision combinations (Figure~\ref{fig:dense_c5}), and for all $c$ values with $u_L$ set to single and $u_R$ set to double (Table~\ref{tab:results_uL_single}), and $u_L$ set to double and $u_R$ set to single (Table~\ref{tab:results_uR_single}). We report the relative backward error (BE) of the original problem, that is,
\begin{equation*}
    \frac{\Vert b - A \xbar_k \Vert}{\Vert b \Vert + \Vert A \Vert \Vert \xbar_k \Vert}
\end{equation*}
and compute the dominant part of the backward error bound $\zeta$ (as defined in \eqref{eq:zeta}).

From Figure~\ref{fig:dense_c5}, we can see that the achievable backward error and subsequently the forward error depends on $u_L$. As expected from theory, $u_R$ does not affect the achievable backward error, however $u_R$ influences the iteration count. Setting $u_L$ to half results in extra iterations when $c=1$, $c=2$ and $c=6$ (not shown). Note that setting $u_L$ to quadruple, and $u_R$ to double or quadruple does not give any benefit. As mentioned, the backward error bound \eqref{eq:backward_error} is dominated by $\zeta_2$. From Tables~\ref{tab:results_uL_single} and \ref{tab:results_uR_single}, we can see that the quantity $\Vert \Zbar_k \Vert \Vert M_R(\xbar_k - \xbar_0) \Vert$ can become large, but it stays of the order of $\Vert \xbar_k \Vert$ or close to it (not shown) and thus $\zeta$ gives a good estimate of the backward error. The increase in the forward error compared to the backward error is well estimated by $\kappa(\Atilde)$, whereas $\kappa(\Ahat) \kappa(M_R)$ is an overestimate. From Figure~\ref{fig:dense_c5}, we see that the condition $\rho <1$ is sufficient, but not necessary as it is not satisfied when $c=5$ and $u_R$ is set to half.


\begin{table}[]
    \caption{Synthetic problems. Condition numbers of the unpreconditioned and preconditioned coefficient matrices and preconditioners, and bounds for $\psi_A$ and $\psi_L$ ($\kappa(M_L)$ is also the bound for $\psi_L$). The preconditioners are computed in precision accurate to four decimal digits for $c<6$ and in single precision for $c\geq 6$.}
    \centering
    \begin{tabular}{c|c|c|c|c|c|c}
c & $\kappa(A)$	& $\kappa(\Atilde)$	&$\kappa(\Ahat)$ & $\kappa(M_R)$ & $\kappa(M_L)$	& $\psi_A$ bound \\	
\hline
1 & 10&	$2.69 \times 10^2$&	1.06 &	$2.69 \times 10^2$	& $3.05 \times 10^2$ &	$4.85 \times 10^1$\\
 2 & $10^2$	& $4.80\times 10^2$	& 1.15	& $4.80 \times 10^2$	& $3.23 \times 10^2$	& $6.30 \times 10^1$ \\
 3 & $10^3$& 	$1.69 \times 10^3$& 	1.66	& $1.69 \times 10^3$ &	$3.09 \times 10^2$	& $7.27 \times 10^1$ \\
 4 & $10^4$	& $1.40 \times 10^4$ & 	$1.06 \times 10^1$	& $1.43 \times 10^4$	& $2.79 \times 10^2$	& $1.64 \times 10^2$\\
 5 & $10^5$	& $7.73 \times 10^4$ &	$3.25 \times 10^2$	& $1.10 \times 10^5$	& $2.85 \times 10^2$	& $1.18 \times 10^2$ \\
 \hline
 6 & $10^6$ &	$4.68 \times 10^5$	& $1.14$	& $4.69 \times 10^5$	& $2.64 \times 10^2$& 	$2.54 \times 10^2$ \\
 7 & $10^7$ & $2.76 \times 10^6$ & $2.34$ & $2.74 \times 10^6$ & $3.50 \times 10^2$ & $1.80 \times 10^2$ \\
 8 & $10^8$ & $3.98 \times 10^7$ & $7.00 \times 10^1$ & $4.04 \times 10^7$ & $3.41 \times 10^2$ & $1.03 \times 10^2$\\
9 & $10^9$ & $3.19  \times 10^8$ & $5.44 \times 10^3$ & $4.78 \times 10^8$ & $3.95 \times 10^2$ & $1.37 \times 10^2$ \\
10 & $10^{10}$ & $4.49 \times 10^9$ & $5.88 \times 10^4$ & $5.72 \times 10^8$ & $3.06 \times 10^2$ & $1.46 \times 10^2$
    \end{tabular}
    \label{tab:cond_no_different_c}
\end{table}

\begin{figure}
    \centering
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[width=\linewidth]{BE_ADgamma1_c5.eps}
  \caption{BE}
\end{subfigure}\hspace{2pc}
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[width=\linewidth]{FE_ADgamma1_c5.eps}
  \caption{FE}
\end{subfigure}
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[width=\linewidth]{BEbound_ADgamma1_c5_ZMxkmx0.eps}
  \caption{$\zeta$}
\end{subfigure}\hspace{2pc}
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[width=\linewidth]{it_ADgamma1_c5.eps}
  \caption{iterations}
\end{subfigure} 
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[trim={0 0 0 3},clip,width=\linewidth]{rho_ADgamma1_c5.eps}%[width=\linewidth]{rho_ADgamma1_c5.eps}
  \caption{$\rho$}
\end{subfigure}
    \caption{Synthetic problem, $c=5$. BE is the relative backward error and FE is the relative forward error, $\zeta$ is as defined in \eqref{eq:zeta}, and $\rho$ is as defined in \eqref{eq:rho_def_assump}.}
    \label{fig:dense_c5}
\end{figure}

\begin{table}[]
    \caption{Synthetic problems with $u_L$ single and $u_R$ double. IC denotes the iteration count, BE is the relative backward error and FE is the relative forward error. For $c=10$, the solver is terminated at 200 iterations without satisfying the convergence criteria.}
    \centering
     \scalebox{0.7}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
c	& IC & BE &	FE	&	$\zeta$&	$\Vert \Zbar_k \Vert \Vert M_R(\xbar_k - \xbar_0) \Vert$	& $\psi_A$&	$\psi_L$	& $\rho$ \\
\hline
1 & 6 & $2.45 \times 10^{-7}$& 	$1.61 \times 10^{-6}$	&$2.91 \times 10^{-7}$& $ 5.04 \times 10^1$	& 1.21	& 2.22 &	$1.44 \times 10^{-15}$ \\
2	& 7 & $3.82 \times 10^{-8}$	& $1.31 \times 10^{-6}$ & $9.34 \times 10^{-8}$	& $2.92 \times 10^2$ 	& 2.08	& $7.46 \times 10^{-1}$	& $3.59 \times 10^{-15}$\\
3	& 9 & $5.76 \times 10^{-9}$	& $1.46 \times 10^{-6}$		& $ 2.56 \times 10^{-8}$&	$2.53 \times 10^3$&	3.79	&$1.88  \times 10^{-1}$	& $1.51 \times 10^{-14}$ \\
4	& 15 &$5.45 \times 10^{-10}$	& $1.27 \times 10^{-6}$	&	$ 3.14 \times 10^{-9}$	& $ 2.30 \times 10^4$	& 3.55	& $2.19  \times 10^{-2}$	& $1.46 \times 10^{-13}$\\
5 & 	35 & $1.03 \times 10^{-10}$&	$2.59 \times 10^{-6}$	& $ 2.51 \times 10^{-9}$&	$  8.39 \times 10^5$	&4.58	&$4.28  \times 10^{-3}$ &	$2.66 \times 10^{-12}$\\
 \hline
6	& 7 & $6.37 \times 10^{-12}$	& $1.58 \times 10^{-6}$	&  $ 5.30 \times 10^{-11}$	& $ 1.35 \times 10^6$& 	4.81	& $5.08 \times 10^{-4}$	& $4.88 \times 10^{-12}$ \\
7 & 11 & $5.88  \times 10^{-13}$ & $9.77  \times 10^{-7}$ & $ 8.25 \times 10^{-12}$ & $1.42 \times 10^7$ & 8.09 & $6.88  \times 10^{-5}$ & $3.28 \times 10^{-11}$\\
8 & 21 & $7.11 \times 10^{-14}$ & $8.24 \times 10^{-7}$ & $1.72 \times 10^{-12}$ & $2.71 \times 10^8$ & 6.08 & $6.95 \times 10^{-6}$ & $8.31 \times 10^{-10}$ \\
9 & 92 & $ 7.97 \times 10^{-14}$ & $8.82 \times 10^{-6}$ & $1.32 \times 10^{-11}$ & $4.02 \times 10^{10}$ & 6.82 & $3.33 \times  10^{-6}$ & $4.48  \times 10^{-8}$ \\
10 & 200 & $1.21 \times 10^{-13} $ & $2.09 \times 10^{-4}$ & $2.30 \times 10^{-11}$ & $5.95 \times 10^{11}$ & 8.28 & $3.68 \times  10^{-6}$ & $6.97 \times 10^{-8}$
    \end{tabular}
    }
    \label{tab:results_uL_single}
\end{table}

\begin{table}[]
    \caption{Synthetic problems with $u_L$ double and $u_R$ single. IC denotes the iteration count, BE is the relative backward error and FE is the relative forward error. For $c=10$, the solver is terminated at 200 iterations without satisfying the convergence criteria.}
    \centering
     \scalebox{0.7}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
 c & IC & BE &	FE	& 	$\zeta$&	$\Vert \Zbar_k \Vert \Vert M_R(\xbar_k - \xbar_0) \Vert$	& $\psi_A$&	$\psi_L$	& $\rho$ \\
\hline
1	& 6 & $5.14 \times 10^{-16}$&	$ 3.04 \times 10^{-15}$		& $1.17\times 10^{-15}$	&$ 5.05 \times 10^1$ &	1.26	& 2.48	&$1.29 \times 10^{-9}$\\
2	& 7 &$1.20 \times 10^{-16}$	& $3.61 \times 10^{-15}$&	$ 7.69 \times 10^{-16}$&	$ 2.92 \times 10^2$ &	1.66 &	$6.33 \times 10^{-1}$&	$3.06 \times 10^{-9}$\\
3	& 9 & $8.62 \times 10^{-17}$	& $2.07 \times 10^{-14}$ & $ 1.19 \times 10^{-15}$&	$2.53 \times 10^3$	& 3.46	& $2.45 \times 10^{-1}$&	$1.31 \times 10^{-8}$ \\
4 &	15 &$6.91 \times 10^{-17}$&	$1.36 \times 10^{-13}$		& $ 1.12 \times 10^{-15}$	& $ 2.30 \times 10^4$&	3.19	& $2.44 \times 10^{-2}$	& $1.05 \times 10^{-7}$ \\
5	& 34 &  $1.42 \times 10^{-16}$	& $2.45 \times 10^{-12}$	& 	$5.20 \times 10^{-15}$	& $ 8.20\times 10^5$&	3.86	& $5.87 \times 10^{-3}$	& $6.88 \times 10^{-7}$ \\
 \hline
6	& 7 & $5.17 \times 10^{-17}$	& $8.85 \times 10^{-12}$&		$ 9.97 \times 10^{-16}$	& $1.31 \times 10^6$	& 4.28 &	$7.65 \times 10^{-4}$	&$3.81 \times 10^{-6}$ \\
7 & 11 & $4.97 \times 10^{-17}$ & $6.91 \times 10^{-11}$ & $1.59 \times 10^{-15}$ & $1.40 \times 10^7$ & 6.23 & $1.11 \times 10^{-4}$ & $2.40 \times 10^{-5}$\\
8 & 21 & $6.58 \times 10^{-17}$ & $5.29 \times 10^{-10}$ & $2.41  \times 10^{-15}$ & $2.29 \times 10^8$ & 5.18 & $1.71 \times 10^{-5}$ & $3.38 \times 10^{-4}$\\
9 & 158 & $2.91 \times 10^{-16}$ & $4.98 \times 10^{-8}$ & $7.65 \times 10^{-14}$ & $4.28 \times 10^{10}$ & 8.79 & $7.92\times 10^{-6} $ & $3.16 \times 10^{-3}$\\
10 & 200 & $4.69 \times 10^{-16}$ & $5.71 \times 10^{-7}$ & $8.81 \times 10^{-14}$ & $5.24 \times 10^{11}$ & 7.62 & $7.24 \times 10^{-6}$ & $4.38 \times 10^{-3}$
    \end{tabular}
    }
    \label{tab:results_uR_single}
\end{table}



\subsection{Numerical example: application problems}\label{sec:numerics_sparse}
We perform experiments with some ill-conditioned problems from the SuiteSparse collection \cite{SuiteSparse}; see Table~\ref{tab:sparsesuit_problems}. We generate the right hand side vector $b$ in the same way as for the synthetic problems. The preconditioner is computed as in the previous section, but in single precision, and the matrix $A$ is converted to a full matrix due to the lacking single precision sparse matrix-vector product functionality in MATLAB. Note that the split-preconditioner reduces the condition number to the theoretical minimum or close to it, except for the problem with the highest $\kappa(A)$. However $\kappa(\Atilde)$ and $\kappa(M_R)$ are of the order of $\kappa(A)$ and thus we expect to see its effect on the forward error. We set $u_A$ and $u$ to double based on the $\psi_A$ bound; $u_L$ and $u_R$ are set as in the previous section. Unpreconditioned FGMRES does not converge in $n$ iterations for any of the problems.

For all problems except \textit{rajat14}, $M_R$ is singular with respect to $u_R$ set to half. This may be amended by using scaling strategies when computing the preconditioner, see, for example, \cite{HighamSqueeze}. We observe similar tendencies (Figures~\ref{fig:sparse_rajat14_arc130} and \ref{fig:sparse_wets0132_fs1833}) as for the synthetic problems, however here we can achieve smaller backward error and for \textit{fs\_183\_3} the backward error is $\mathcal{O}(u)$ even with $u_L$ set to half. Note that for application problems setting $u_L$ to a low precision results in iteration-wise slower convergence. The term $\Vert \Zbar_k \Vert \Vert M_R(\xbar_k - \xbar_0) \Vert$ grows as for the synthetic problems, but is balanced by $\Vert \xbar_k \Vert$; see Tables~\ref{tab:sparsesuit_uL_single} and \ref{tab:sparsesuit_uR_single}.

In all of our application and synthetic examples, $\psi_A/\psi_L$ does not become large enough to allow setting $u_L > u_A$ without it affecting the backward error. However for \textit{fs\_183\_3} both $\psi_A$ and $\psi_L$ are small enough that we can set $u_A$ and $u_L$ to single and expect $\mathcal{O}(u)$ backward error. Numerical experiments confirm this even though the backward and forward errors become slightly larger compared to $u_A$ set to double (not shown).   


\begin{table}[]
\caption{As in Table~\ref{tab:cond_no_different_c}, but for SuiteSparse problems.}
    \centering
       \scalebox{0.9}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
 problem	& $n$ & $\kappa(A)$ &$\kappa(\Atilde)$ &	$\kappa(\Ahat)$&	$\kappa(M_R)$	&$\kappa(M_L)$	& $\psi_A$ bound\\
 \hline
rajat14	& 180 &$3.22 \times 10^8$	&$1.44 \times 10^8$& 1.01	& $1.44 \times 10^8$& 	$9.72 \times 10^1$&	1.13\\
arc130 & 130 &	$6.05 \times 10^{10}$&	$6.05 \times 10^{10}$&	1.00	& $6.05 \times 10^{10}$& 	2.64&	1.00\\
west0132 & 132 &	$4.21\times 10^{11}$&	$2.20 \times 10^{11}$	& 1.12	& $2.20 \times 10^{11}$	&7.49 & 1.00 \\
fs\_183\_3	& 183 & $3.27 \times 10^{13}$&	$2.39 \times 10^{13}$	& 1.00	& $2.39 \times 10^{13}$	& $4.73 \times 10^1$	& $6.74 \times 10^{-1}$ \\
    \end{tabular}
    }
    \label{tab:sparsesuit_problems}
\end{table}


\begin{table}[]
\caption{As in Table~\ref{tab:results_uL_single}, but for SuiteSparse problems: $u_L$ is set to single, $u_R$ is set to double. }
    \centering
       \scalebox{0.7}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
problem & IC & BE &	FE	& 	$\zeta$&	$\Vert \Zbar_k \Vert \Vert M_R(\xbar_k - \xbar_0) \Vert$	& $\psi_A$&	$\psi_L$	& $\rho$ \\
\hline
rajat14	& 3	& $7.89 \times 10^{-13}$	& $1.84 \times 10^{-6}$	&	$ 1.18 \times 10^{-11}$&	$5.14 \times 10^2$&	$1.42 \times 10^{-2}$&	$1.06 \times 10^{-4}$	&$3.11 \times 10^{-11}$\\
arc130	& 3	& $1.73 \times 10^{-18}$& 	$2.14 \times 10^{-8}$&	$1.41 \times 10^{-16}$&	$ 1.02\times 10^{6}$	&$5.99 \times 10^{-6}$&	$4.80 \times 10^{-10}$	&$4.02 \times 10^{-6}$\\
west0132	& 4	& $2.30 \times 10^{-17}$	& $2.93 \times 10^{-6}$	&	$4.84 \times 10^{-16}$	&$6.46 \times 10^4$	& $1.87 \times 10^{-5}$& 	$6.05 \times 10^{-9}$	&$3.00 \times 10^{-7}$\\
fs\_183\_3	& 3 & 	$2.41 \times 10^{-20}$&	$1.31 \times 10^{-8}$&	$ 1.31 \times 10^{-16}$&	$ 1.53 \times 10^{5}$&	$9.38 \times 10^{-12}$&	$5.32 \times 10^{-13}$	& $9.17 \times 10^{-4}$\\
    \end{tabular}
    }
    \label{tab:sparsesuit_uL_single}
\end{table}

\begin{table}[]
    \caption{As in Table~\ref{tab:results_uR_single}, but for SuiteSparse problems: $u_L$ is set to double, $u_R$ is set to single.}
    \centering
     \scalebox{0.7}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
problem & IC & BE &	FE	& 	$\zeta$&	$\Vert \Zbar_k \Vert \Vert M_R(\xbar_k - \xbar_0) \Vert$	& $\psi_A$&	$\psi_L$	& $\rho$ \\
\hline
rajat14	& 3	& $1.25 \times 10^{-19}$	& $6.19 \times 10^{-15}$	&	$1.19 \times 10^{-16}$	& $2.97 \times 10^2$ &	$7.46 \times 10^{-3}$&	$1.18 \times 10^{-4}$	&$1.15 \times 10^{-6}$\\
arc130	& 5&	$3.43 \times 10^{-22}$&	$1.83 \times 10^{-16}$	&	$ 1.11\times 10^{-16}$&	$1.02 \times 10^{6}$&	$2.14 \times 10^{-6}$	&$5.51 \times 10^{-11}$	& $6.49 \times 10^{-4}$\\
west0132 &	5&	$1.48 \times 10^{-21}$	& $4.80 \times 10^{-15}$	&	$ 1.13 \times 10^{-16}$	& $ 6.39\times 10^4$	& $1.98 \times 10^{-5}$&	$6.16 \times 10^{-9}$&	$2.04 \times 10^{-1}$\\
fs\_183\_3	& 3	& $1.52 \times 10^{-27}$&	$ 1.05 \times 10^{-15}$	&	$1.25 \times 10^{-16}$&	$ 1.47 \times 10^{5}$&	$1.87 \times 10^{-11}$&	$1.71 \times 10^{-12}$&	$1.38$ \\
    \end{tabular}
    }
    \label{tab:sparsesuit_uR_single}
\end{table}


\begin{figure}
    \centering
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[width=\linewidth]{BE_rajat14.eps}
  \caption{BE, rajat14}
\end{subfigure}\hspace{2pc}
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[width=\linewidth]{BE_arc130.eps}
  \caption{BE, arc130}
\end{subfigure}
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[width=\linewidth]{FE_rajat14.eps}
  \caption{FE, rajat14}
\end{subfigure}\hspace{2pc}
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[width=\linewidth]{FE_arc130.eps}
  \caption{FE, arc130}
\end{subfigure}
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[width=\linewidth]{it_rajat14.eps}
  \caption{iterations, rajat14}
\end{subfigure}\hspace{2pc}
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[width=\linewidth]{it_arc130.eps}
  \caption{iterations, arc130}
\end{subfigure}
    \caption{SuiteSparse problems rajat14 and arc130. BE is the relative backward error and FE is the relative forward error.}
    \label{fig:sparse_rajat14_arc130}
\end{figure}

\begin{figure}
    \centering
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[width=\linewidth]{BE_west0132.eps}
  \caption{BE, west0132}
\end{subfigure}\hspace{2pc}
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[width=\linewidth]{BE_fs1833.eps}
  \caption{BE, fs\_183\_3}
\end{subfigure}
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[width=\linewidth]{FE_west0132.eps}
  \caption{FE, west0132}
\end{subfigure}\hspace{2pc}
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[width=\linewidth]{FE_fs1833.eps}
  \caption{FE, fs\_183\_3}
\end{subfigure}
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[width=\linewidth]{it_west0132.eps}
  \caption{iterations, west0132}
\end{subfigure}\hspace{2pc}
\begin{subfigure}[t]{0.45\linewidth}
  \centering
 \includegraphics[width=\linewidth]{it_fs1833.eps}
  \caption{iterations, fs\_183\_3}
\end{subfigure}
    \caption{SuiteSparse problems west0132 and fs\_183\_3. BE is the relative backward error and FE is the relative forward error.}
    \label{fig:sparse_wets0132_fs1833}
\end{figure}



\section{Concluding remarks}\label{sec:conclusions}

In light of great community focus on mixed precision computations, we analyzed a variant of split-preconditioned FGMRES that allows using different precisions for computing matrix-vector products with the coefficient matrix $A$ (unit roundoff $u_A$), left-preconditioner $M_L$ (unit roundoff $u_L$), right-preconditioner $M_R$ (unit roundoff $u_R$), and other computations (unit roundoff $u$). A backward error of a required level can be achieved by controlling these precisions. 

Our analysis and numerical experiments show that  
the precision for applying $M_L$ must be chosen in relation to $u$, $u_A$, and the required backward and forward errors, because $u_L$ heavily influences the achievable backward error.  
We can be more flexible when choosing $u_R$ as it does not influence the backward error directly. Our analysis holds under a sufficient but not necessary assumption on $u_R$ in relation to $M_R$. As long as $M_R$ is not singular in precision $u_R$ (note that scaling strategies may be used to ensure this), setting $u_R$ to a low precision is sufficient. Very low precisions $u_L$ and $u_R$ may delay the convergence iteration wise, yet setting $u_L \leq u$ or $u_R \leq u$ does not improve the convergence in general.

We observe that the forward error is determined by the backward error and the condition number of the left-preconditioned coefficient matrix. This motivates concentrating effort on constructing an appropriate left-preconditioner when aiming for a small forward error: the preconditioner should reduce the condition number sufficiently and needs to be applied in a suitably chosen precision.

\appendix
\section{Proof of Theorem~\ref{th:backward_error}}\label{app:backward_err_proof}
The analysis closely follows \cite{arioli2007note} and \cite{arioli2009using}, and thus we provide the important results for each stage rather than the step-by-step analysis. 

\subsection{Left preconditioner} We start by accounting for the effect of $M_L$. 

\subsubsection{Stage 1: MGS} In this stage, we use precisions $u$, $u_A$ and $u_L$. MGS is applied to 
\begin{equation*}
\Cbar^{(k)} = \begin{bmatrix} fl(M_L^{-1} \rbar_0) &  fl(\Atilde \Zbar_k) \end{bmatrix}.    
\end{equation*}
MGS returns an upper triangular $\Rbar_k$ and there exists an orthonormal $\Vhat_{k+1}$, that is $\Vhat_{k+1}^T \Vhat_{k+1} = I_{k+1}$, such that
\begin{gather}
 \begin{bmatrix} \btilde - \Atilde \xbar_0  &  \Atilde \Zbar_k  \end{bmatrix} \vspace{-1pt} + \vspace{-1pt}  \begin{bmatrix} f_1 + f_2 +f_3 & F_k^{(1)} + F_k^{(2)} \end{bmatrix}\vspace{-1pt}  = \vspace{-1pt} \Vhat_{k+1} \Rbar, \nonumber \\
 \Vert  f_1 \Vert  \leq  (u_A \psi_A + u_L \psi_L) \Vert \Atilde \Vert \Vert  \xbar_0 \Vert , \label{eq:f1_def}\\
\Vert f_2 \Vert \leq  c_1(n) u_L \Vert E_L M_L \Vert \Vert  \btilde \Vert + u \left( \Vert \btilde \Vert + (1+ u_A \psi_A + u_L \psi_L) \Vert \Atilde \Vert \Vert \xbar_0 \Vert \right), \label{eq:f2_bound} \\
 \Vert f_3 \Vert \leq c_2 (n) u \left( \Vert \btilde - \Atilde \xbar_0  \Vert  + (u_A \psi_A + u_L \psi_L) \Vert \Atilde \Vert \Vert \xbar_0 \Vert +  c_1(n) u_L \Vert E_L M_L \Vert \Vert  \btilde \Vert\right), \label{eq:f3_bound} \\
 \Vert F_k^{(1)} \Vert \leq  (u_A \psi_A + u_L \psi_L) \Vert \Atilde \Vert \Vert \Zbar_k \Vert, \label{eq:Fk1_bound}\\
 \Vert F_k^{(2)} \Vert \leq c_3(n,k) u\left( \Vert \Atilde \Zbar_k  \Vert +   (u_A \psi_A + u_L \psi_L) \Vert \Atilde \Vert \Vert \Zbar_k \Vert \right).  \label{eq:Fk2_bound}
\end{gather}
Here $f_1$ is the error in computing the matrix vector product $\Atilde \xbar_0$ and $f_2$ accounts for computing $M_L^{-1}b$ and adding it to the computed $\Atilde \xbar_0$. Error $F_k^{(1)}$ comes from computing $\Atilde \Zbar_k$. $f_3$ and $ F_k^{(2)}$ arise in the MGS process.


\subsubsection{Stage 2: Least squares} The least squares problem is solved using precision $u$. 

From the analysis of \cite{arioli2007note}, under assumptions \eqref{eq:assumptions_nu_ukappaC} and \eqref{eq:assumption_sk} the norm of the residual of the least-squares problem \eqref{eq:LS_problem}
\begin{equation*}
    \alpha_k = \Vert \betabar e_1 + g^{[k]} - (\Hbar_k +\Delta \Hbar_k) \ybar_k  \Vert
\end{equation*}
monotonically converges to zero for a finite $k \leq n$. 
We can express $\alpha_k$ in the following way: 
\begin{equation}
    \alpha_k =  \Vert \btilde - \Atilde \xbar_0 + \delta \rtilde_0 - \Atilde (\Zbar_k + \Zhat_k ) \ybar_k  \Vert,\label{eq:alphak_ZZ}
\end{equation}
where
\begin{gather}
    \delta \rtilde_0 = f_1 + f_2 + f_3  + \Vhat_{k+1} g^{[k]}, \label{eq:delta_r0_def} \\
    \Zhat_k = \Atilde^{-1} \left( F_k^{(1)} +  F_k^{(2)} + \Vhat_{k+1} \Delta \Hbar_k \right), \label{eq:Zhat_k_def}\\
    \Vert g^{[k]} \Vert \leq c_5(k) u \Vert \btilde - \Atilde \xbar_0 \Vert + c_5(k) u (u_A \psi_A + u_L \psi_L) \Vert \Atilde \Vert \Vert \xbar_0 \Vert + c_6(n,k) u u_L \Vert E_L M_L \Vert \Vert  \btilde \Vert, \label{eq:gk_bound_fin}\\
    \Vert \Delta \Hbar_k  \Vert \leq c_4(k) u \Vert \Atilde \Zbar_k \Vert + c_7(n,k) u (u_A \psi_A + u_L \psi_L) \Vert \Atilde \Vert \Vert \Zbar_k \Vert.\label{eq:deltaHk_bound}
\end{gather}

\subsubsection{Stage 3: Computing $\xbar_k$} When certain conditions on the residual norm are satisfied, precision $u$ is used to compute $x_k$ as
\begin{gather}
    \xbar_k = \xbar_0 + \Zbar_k \ybar_k + \delta x_k, \label{eq:xbar_k} \\
    \Vert \delta x_k \Vert \leq c_8(k) u \Vert \Zbar_k \Vert \Vert \ybar_k \Vert + u \Vert \xbar_0 \Vert. \label{eq:bound_deltax_k} % + \mathcal{O}(u^2)
\end{gather}
Using this to eliminate $\Zbar_k \ybar_k$ in \eqref{eq:alphak_ZZ}, then applying the reverse triangle inequality to bound $\Vert \btilde - \Atilde  \xbar_k  \Vert$ and bounding $\Vert \delta \rtilde_0 \Vert$, $\Vert \Atilde \delta x_k \Vert$ and $\Vert \Atilde  \Zhat_k \ybar_k \Vert$ gives
\begin{align}
     \Vert \btilde - \Atilde  \xbar_k  \Vert
 \leq & \, c_{11}(n,k) \Big(\big( u + (1+u)( u_A \psi_A + u_L \psi_L)\big) \Vert \Atilde \Vert \left(  \Vert \xbar_0 \Vert + \Vert \Zbar_k \Vert \Vert \ybar_k \Vert \right) \nonumber\\
 +  & \, \left( u + u_L (1+u)\Vert E_L M_L \Vert\right) \Vert  \btilde \Vert \Big). \label{eq:residual_norm_with_yk}
\end{align}
We eliminate $\Vert \ybar_k \Vert $ from the bound in the following section.



\subsection{Right preconditioner} We now extend the analysis to account for the effect of applying $M_R$. Under assumption \eqref{eq:Mr_assump} $\Zbar_k$ is computed such that
\begin{equation*}
    \Zbar_k = M_R^{-1} \Vbar_k + \Delta M_R \Vbar_k, 
\end{equation*}
where $\Vert \Delta M_R \Vert \leq c_{12}(n) u_R \Vert E_R \Vert$. 
Then we can obtain
\begin{align*}
\Vert \ybar_k \Vert \leq & \, 1.3   \left( \Vert  M_R (\xbar_k - \xbar_0) \Vert +  \Vert M_R \Vert \Vert \delta x_k \Vert + \Vert M_R \Vert \Vert \Delta M_R \Vbar_k \Vert \Vert \ybar_k \Vert \right) \\
\leq & \, 1.3 c_{13}(n,k)  \left( \Vert M_R( \xbar_k - \xbar_0) \Vert +  u \Vert M_R \Vert \Vert \Zbar_k \Vert \Vert \ybar_k \Vert + u \Vert M_R \Vert \Vert \xbar_0 \Vert +  u_R \Vert M_R \Vert \Vert E_R \Vert \Vert \ybar_k \Vert \right).
\end{align*}
Under assumption \eqref{eq:rho_def_assump}
\begin{equation*}
    \Vert \ybar_k \Vert \leq \frac{1.3 c_{13}(n,k)}{1 - \rho} \left( \Vert M_R (\xbar_k - \xbar_0) \Vert + u \Vert M_R \Vert \Vert \xbar_0 \Vert \right).
\end{equation*}
Using this in \eqref{eq:residual_norm_with_yk} 
and dropping the terms $u^2$, $u u_L$ and $u u_A$ gives the required result.


\bibliographystyle{siam}
\bibliography{paper}

\end{document}