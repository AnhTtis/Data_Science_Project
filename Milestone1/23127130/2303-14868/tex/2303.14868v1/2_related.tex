We are interested in data reconstruction attacks in the setting of FL under secure aggregation (SA)~\cite{bonawitz2017practical}. Under SA, a server cannot gain access to any individual client's updates. Participating clients encrypt their updates such that only after a server aggregates them will it have access to an unencrypted aggregate update. This section discusses prior work in data reconstruction attacks and their applicability toward this challenging scenario.

\noindent
\textbf{Gradient inversion.} An attacker with access to the model parameters and an honest individual gradient performs a gradient inversion attack by initializing random dummy data and minimizing the difference between the gradient computed by the dummy data and the ground truth gradient. Many works have looked to improve reconstruction through zero-shot label restorations~\cite{yin2021see,zhao2020idlg,geng2021towards}, regularizers~\cite{geiping2020inverting}, or the use of multiple initialization seeds~\cite{yin2021see}. However, they cannot scale to aggregation because the computational complexity scales with an increasing number of images as $\mathcal{O}(n\times dim_{input})$~\cite{kariyappa2022cocktail}, where $n$ is the total number of images. For FL, $n$ is the batch size $\times$ number of clients.

\noindent
\textbf{Analytic attacks.} Analytic attacks involve parameter manipulation~\cite{wen2022fishing,pasquini2021eluding} or attempting to dis-aggregate the gradient by observing multiple training rounds~\cite{lam2021gradient}. While these methods work in the aggregate setting, they are not scalable towards an increasing number of clients. \cite{wen2022fishing} can only attack a single training image within a single round and~\cite{pasquini2021eluding} can only attack a single client. ~\cite{lam2021gradient} can support an increasing number of clients, but requires additional side-channel information not required for FL and additionally can require hundreds or thousands of training rounds to succeed. It also relies on optimization, so if the client batch size is larger, reconstruction quality will diminish. 

\noindent
\textbf{Linear layer leakage.} A sub-class of analytic methods is linear layer leakage attacks~\cite{fowl2022robbing,boenisch2021curious}. These attacks function with an inserted module that is typically two FC layers (linear layers) at the start of the model architecture. The attacks are then able to use the gradients of the first FC layer to directly recover the inputs to the layer. Since the FC layer is placed at the start of the architecture, the inputs are the training images themselves. Specifically, if an image activates a neuron in an FC layer, the image can be reconstructed as 
\vspace{0 pt}
\begin{equation}\label{eq:1}
    x = \frac{\delta L}{\delta W^i} / \frac{\delta L}{\delta B^i}
\vspace{-1 pt}
\end{equation}
\noindent
where $x$ is the recovered image and $\frac{\delta L}{\delta W^i}, \frac{\delta L}{\delta B^i}$ are the weight and bias gradient of the activated neuron~\cite{phong2017privacy}.

These recovered images are near-exact reconstructions. However, if multiple images activate the same neuron, the reconstructed image becomes a combination of these images. Prior work has proposed binning~\cite{fowl2022robbing} and trap weights~\cite{boenisch2021curious} to prevent collision of activated neurons between different images. Trap weights aim to create a sparse activation by initializing the FC layer weights as half negative and positive, with a slightly larger negative magnitude. Under binning, the weights of the FC layer are set such that they measure an aspect of the image, such as the image brightness or pixel intensity. A ReLU activation is used and the neuron biases increase (negatively) so that subsequent neurons allow fewer images to activate them. For any neuron, if only one image has it as the activated neuron with the largest cut-off bias, we can reconstruct the image as
\vspace{-1 pt}
\begin{equation}\label{eq:5}
    x^i = (\frac{\delta L}{\delta W^i} - \frac{\delta L}{\delta W^{i+1}}) / (\frac{\delta L}{\delta B^i} - \frac{\delta L}{\delta B^{i+1}})
\vspace{-1 pt}
\end{equation}
where $x^i$ is the reconstructed image, $i$ is the activated neuron, and $i+1$ is the neuron with the next largest cut-off that was not activated. This method can scale to larger number of clients or batch size while maintaining a high leakage rate by increasing the number of units in the FC layer~\cite{fowl2022robbing,our2022mandrake}. However, this scalability comes at the cost of an increasing model size and becomes much worse under aggregation, as the number of images increases multiplicatively with the batch size and number of clients.

Another similar method uses blind-source separation of an FC layer~\cite{kariyappa2022cocktail} to reconstruct images. This method can support only reconstructions up to 1024 images and, in the context of FL, is a small scale attack and is not particularly applicable to scaling in FL. The size overhead added by the method is not insignificant, as an FC layer added to the start of the model for attacking a batch size of 1024 will be a minimum of a 768MB model size overhead.

The size overhead added by scaling these methods is a fundamental problem. With~\cite{fowl2022robbing,boenisch2021curious}, these methods treat aggregation attacks the same as individual client attacks, evident through the statement that \textit{"given an aggregated gradient update, we always reconstruct as discussed in [the methodology section]"}~\cite{fowl2022robbing}. ~\cite{kariyappa2022cocktail} falls under similar thinking, applying their attack on aggregate updates as simply the same attack on a larger batch size. Another work~\cite{qian2021minimal} discusses how a full batch can be recovered as long as the number of units is larger than the total number of images. While many attacks have not been applied to aggregation yet, it is clear that there is no key difference in the perspective of applying attacks to aggregation compared to individual updates.

Our work is mainly focused on linear layer leakage attacks, but the applicability will be relevant to other methods as they explore large-scale attacks on aggregation. For example, while optimization attacks still do not have a good method of scaling to aggregation due to an increasing computational complexity, a dual problem has been shown where multiple solutions exist for a single update~\cite{zhang2022survey}. If future work discusses model modifications in an increased width or depth of the model to reconstruct larger numbers of images, our work will be relevant. This is also likely since prior work has already begun discussing the relationship between model size/depth and reconstruction ability~\cite{geiping2020inverting}. If computational complexity is not a problem, these same ideas will be used for attack scalability.

In the next section, we will discuss why the prior work perspective on attacking aggregate gradients as a single large batch is a problem and how it leads to large resource overheads in linear layer leakage. We will show how the design of \name with separate scaling between the number of clients and batch size uses the correct perspective and allows us to use sparsity to decrease overhead.