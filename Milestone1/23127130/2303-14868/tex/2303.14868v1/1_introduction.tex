\vspace{-1mm}
% Training deep neural networks requires large amounts of data, and while the amount of user data is growing at an unprecedented rate, the privacy of data is of utmost importance. 
Federated learning (FL)~\cite{mcmahan2017communication} has been hailed as a privacy-preserving method of training. 
% applies machine learning in a distributed setting, where a single round of training will involve a server sending a model to clients (edge devices) and 
FL involves multiple clients which train their model on their private data before sending the update back to a server. The promise is that FL will keep the client data private from all (server as well as other clients) as the update cannot be used to infer information about client training data.

However, many recent works have shown that client gradients are not truly privacy preserving. Specifically, data reconstruction attacks~\cite{fowl2022robbing,boenisch2021curious,yin2021see,geiping2020inverting,kariyappa2022cocktail,zhu2019deep,pasquini2021eluding,wen2022fishing} use a model update to directly recover the private training data. These methods typically consist of gradient inversion~\cite{yin2021see,geiping2020inverting,zhu2019deep} and analytic attacks~\cite{boenisch2021curious,fowl2022robbing,kariyappa2022cocktail,lam2021gradient,pasquini2021eluding,wen2022fishing}. Gradient inversion attacks observe an honest client gradient and iteratively optimizes randomly initialized dummy data such that the resulting gradient becomes closer to the honest gradient. The goal is that dummy data that creates a similar gradient will be close to the ground truth data. These methods have shown success on smaller batch sizes, but fail when batch sizes become too large. Prior work has shown that reconstruction on ImageNet is possible up to a batch size of 48, although the reconstruction quality is low~\cite{yin2021see}. Analytic attacks cover a wide range of methods. Primarily, they use a malicious modification of model architecture and parameters~\cite{wen2022fishing, pasquini2021eluding}, linear layer leakage methods~\cite{fowl2022robbing,boenisch2021curious}, observe updates over multiple training rounds~\cite{lam2021gradient}, or treat images as a blind-source separation problem~\cite{kariyappa2022cocktail}. However, most of these approaches fail when secure aggregation is applied~\cite{bonawitz2017practical,elkordy2022much,so2021lightsecagg,secagg_so2021securing,9712310}. Particularly, when a server can only access the updates aggregated across hundreds or thousands of training images, the reconstruction process becomes very challenging. Gradient inversion attacks are impossible without additional model modifications or training rounds. This is where linear layer leakage attacks~\cite{fowl2022robbing,boenisch2021curious} have shown their superiority.

This sub-class of analytic data reconstruction attacks is based on the server crafting maliciously modified models that it sends to the clients. In particular, the server uses a fully-connected (FC) layer to leak the input images. Compared to any other attack, linear layer leakage attacks are the only methods able to scale to an increasing number of clients or batch size, maintaining a high total leakage rate. This is done by continually increasing the size of an FC layer used to leak the images. For example, with 100 clients and a batch size of 64 on CIFAR-100, an attacker can leak $77.2\%$ of all images % (4938 out of 6400) 
in a single training round using an inserted FC layer of size 25,600. In this case, the number of units in the layer is $4\times$ the number of total images, and maintaining this ratio when the number of clients or batch size increases allows the attack to still achieve roughly the same leakage rate. Despite the potential of linear layer leakage, however, an analysis of the limits of its scalability in FL has been missing till date. 

In this work, we dive into this question and explore the potential of scaling linear layer leakage attacks to secure aggregation. We particularly highlight the challenges in resource overhead corresponding to memory, communication, and computation, which are the primary restrictions of cross-device FL. We discover that while SOTA attacks can maintain a high leakage rate regardless of aggregation size, the overhead is massive. With 1,000 clients and a batch size of 64, maintaining the same leakage rate as before would result in the added layers increasing the model size by 6GB. There would also be an added computation time of 21.85s for computing the update for a single batch (size 64), % on a CPU, 
a $10\times$ overhead compared to a baseline ResNet-50. This is a massive problem for resource-constrained FL where clients have limited communication or computation budgets.

However, this problem arises from an incorrect perspective from prior work where they treat the attack on an aggregate update the same as an individual client update. Specifically, we argue that it is critical to treat an aggregation attack not as an attack on a single large update, but as individual client updates combined together. In the context of linear layer leakage, this is the difference between separating the scaling of the attack between batch size and the number of clients or scaling to all images together. 
% However, the application of this idea becomes less clear when secure aggregation is applied.

Following this, we use the attack \name~\cite{our2022mandrake} with sparsity in the added parameters between the convolutional output and the FC layer to highlight the difference in model size compared to prior SOTA. The addition can decrease the added model size by over 327$\times$ and decrease computation time by 3.34$\times$ compared to SOTA attacks while achieving the same total leakage rate. For a batch size of 64 and 1000 clients participating in training, the sparse \name module adds only a little over 18MB to the model while leaking $77.8\%$ of the total data % ($49798$ images) 
in a single training round (comparable to other SOTA attacks). 

We discuss other fundamental challenges for linear layer leakage including the resource overhead of leaking larger input data sizes. We also discuss that sparsity in the client update fundamentally cannot be maintained through secure aggregation and the client still accrues a communication overhead when sending the update back to the server. All other aspects of resource overhead such as communication cost when the server sends the model to the client, computation time, and memory size, are decreased through sparsity.

Our contributions are as follows:
\vspace{-5 pt}
\begin{itemize}[noitemsep]
    \item We show the importance of sparsity in maintaining a small model size overhead when scaling to a large number of clients and the incorrect perspective prior work has had when treating the aggregate update as a single large update. By using sparsity with \name and attacking 1000 clients with a batch size of 64, the added model size is only 18.33 MB. Compared to SOTA attacks, this is a decrease in over $327\times$ in size and also results in a decreased computation time by $3.3\times$ while maintaining the same leakage rate.
    
    \item We show the fundamental challenge of linear layer leakage attacks for scaling attacks towards leaking larger input image sizes and the resulting resource overhead added.
    
    \item We show the problem of maintaining sparsity in secure aggregation when the encryption mask is applied, which adds to the communication overhead when clients send updates back to the server.
\end{itemize}