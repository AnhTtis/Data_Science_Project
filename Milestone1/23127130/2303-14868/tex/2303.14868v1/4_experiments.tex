\begin{figure}[!t]
\vspace*{-0.5mm}
\begin{center}
\includegraphics[width=1.0\columnwidth,trim={0 0 0 10mm},clip]{images/model_size_all_inserted_module.pdf}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:model-size-all} Comparing the model size of the dense and sparse tensor attack with Robbing the Fed on the downsampled Tiny ImageNet dataset with a client batch size of 64. The model sizes are given when achieving a total leakage rate of $77\%$. At 1000 clients, the sparse representation is $327.33\times$ smaller than Robbing the Fed.}
\vspace*{-5 mm}
\end{figure}
% \begin{figure}[!t]
% % \vspace*{-2.5mm}
% \begin{center}
% \includegraphics[width=0.9\columnwidth]{images/model_size_sparse.png}
% \end{center}
% \vspace*{-5mm}
% \caption{\label{fig:model-size-sparse} The model size grows from the sparse tensor attack as the number of clients increases. The sparse tensor size itself does not change, however, the extra parameters added by the convolutional kernels create a small size increase.}
% \vspace*{-4mm}
% \end{figure}

% \begin{figure}[!t]
% \vspace*{2mm}
%      \centering
%      \begin{subfigure}[b]{0.49\columnwidth}
%          \centering
%          \includegraphics[width=1.0\columnwidth]{images/model_size_all_inserted_module.png}
%          \caption{Ground truth}
%          \label{fig:model-size-all}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.49\columnwidth}
%          \centering
%          \includegraphics[width=1.0\columnwidth]{images/model_size_sparse.png}
%          \caption{Reconstruction}
%          \label{fig:model-size-sparse}
%      \end{subfigure}
% \vspace*{-2mm}
% \caption{\label{fig:model-size-both} Comparing the model size of the dense and sparse tensor attack with Robbing the Fed on the downsampled Tiny ImageNet dataset with a client batch size of 64 (a). The model size is given when achieving the same total leakage rate of $76\%$. At 1000 clients, the sparse representation is $314.78\times$ smaller than Robbing the Fed. (b) shows the model growth of the sparse method.}
% \vspace*{-5mm}
% \end{figure}

We evaluate our attack in the secure aggregation FL setting. We are particularly focused on the resource costs in terms of model size and computation overhead added by linear layer leakage attacks when scaling to larger numbers of clients. We primarily compare three attacks: our attack using dense tensors, our attack using the sparse tensor representation, and Robbing the Fed~\cite{fowl2022robbing} the prior SOTA linear layer attack. For all experiments, we use a client batch size of 64 with a varied number of clients in aggregation. We use a $|\text{FC layer}|$ $4\times$ the number of images. Using the binning method of ~\cite{fowl2022robbing}, both our method and Robbing the Fed achieve the same total leakage rate on the Tiny ImageNet dataset~\cite{le2015tiny}. Using a single training round with 1000 clients, \name leaks $76.9\%$ (49,209) images, and Robbing the Fed leaks $76.5\%$ (48,992) of the total 64,000 images. For additional examples of reconstructed images and the leakage rate for other datasets, we refer to the supplementary material.

We first show the model size comparison for each of the methods on a downsampled Tiny ImageNet dataset ($32\times32\times3$), comparing the model size trend with increasing clients. We show the overhead added to standard vision models from each method. In Section~\ref{sec:experiments-larger-ims} we also look at the size overhead of the inserted modules on the MNIST ($28\times28\times1$)~\cite{lecun1998mnist}, CIFAR-100 ($32\times32\times3$)~\cite{krizhevsky2009learning}, Tiny ImageNet ($64\times64\times3$), and ImageNet ($256\times256\times3$)~\cite{russakovsky2015imagenet} datasets. We use this section to highlight the difficulties in scaling with larger input image sizes.

When using larger input image sizes with a large number of clients, the FC layer size of Robbing the Fed grows too large for memory. As a result, we use the downsampled Tiny ImageNet dataset for these comparisons. We run the attacks on a CPU compared to a GPU, focusing on the resource restrictions of cross-device FL. For the computation overhead, we compare the additional time required to compute the model gradients when compared to a baseline ResNet-50~\cite{he2016deep} from PyTorch. We place the extra layers at the start of the architecture.

Finally, we experimentally show the binning methodology of Robbing the Fed~\cite{fowl2022robbing} is more effective than trap-weights~\cite{boenisch2021curious} in terms of mutual information. 

\begin{table}[!t]
\begin{center}
\small
\begin{tabular}{|l|c|cc|}
\hline
                                                                       & \textbf{\begin{tabular}[c]{@{}c@{}}Model size \\ (MB)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Sparse \\ attack\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Robbing \\ the Fed\end{tabular}} \\ \hline
% \textbf{\begin{tabular}[c]{@{}l@{}}MobileNet v3 Large\\  v3 Large\end{tabular}} & 20.9161                                                             & 0.8759                                                            & 275.7182                                                            \\
\textbf{MobileNet v3 (L)} & 20.9161                                                             & 87.65\%                                                            & 28690.76\%                                                           \\
\textbf{ResNet-18}                                                     & 44.5919                                                             & 41.11\%                                                            & 13457.57\%                                                            \\
\textbf{ResNet-50}                                                     & 97.4923                                                             & 18.80\%                                                            & 6155.35\%                                                             \\
\textbf{Inception v3}                                                  & 103.6120                                                            & 17.69\%                                                            & 5791.79\%                                                             \\
\textbf{VGG-11}                                                        & 506.8334                                                            & 3.62\%                                                            & 1184.02\%                                                             \\ \hline
\end{tabular}
\end{center}
\vspace*{-4mm}
\caption{\label{tab:model-size-comparison} Model size overhead added from the attacks with 1000 clients and a batch size of 64 on Tiny ImageNet compared to vision models. The overhead added by the sparse representation attack ($18.33$MB) is significantly smaller than Robbing the Fed ($6000.99$MB) and achieves the same leakage rate.}
\vspace*{-3mm}
\end{table}
\vspace{-4 pt}
\subsection{Model size}
We start with a discussion on the model size. For these experiments, we use PyTorch's sparse COO (Coordinate format) tensor representation~\cite{paszke2019pytorch}. This format stores the non-zero values in indices and values tensor. The size of the sparse tensor in bytes is
\vspace{-4 pt}
\begin{equation}\label{eq:7}
    size = (dim \cdot 8 + data size) \cdot |\{w_N \mid w_N\neq0\}|
\vspace{-4 pt}
\end{equation}
following PyTorch's sparse tensor memory consumption. The tensor dimensions are $dim=2$ and the data size is $4$ bytes for our model.

When the ratio of the number of neurons to images is $4:1$, the attack methods achieve $~77\%$ total leakage rate on the Tiny ImageNet dataset (small randomness coming from batch images selection). In the case of Robbing the Fed, this is achieved when the $|\text{FC layer}| = (\text{num clients})\cdot(\text{batch size})\cdot 4$. Our method achieves this with a fixed $|\text{FC layer}|=256$ by increasing the number of convolutional kernels by 3 for each client.

Figure~\ref{fig:model-size-all} shows the model size overhead (MB) added by the 3 methods with a fixed leakage rate and a varying number of clients. At 3 clients, the sparse representation is nearly the same size as Robbing the Fed ($99.994\%$). At 5 clients, the sparse and dense (update size with SA sent back to server) representations are the same size. As the number of clients grows, both dense weights and Robbing the Fed quickly grow in size, while the sparse representation remains virtually the same. While the method of Robbing the Fed is able to achieve the same total leakage rate, the number of parameters is roughly double the dense weights attack. With 1000 clients, Robbing the Fed is $327.33\times$ larger than the sparse tensor attack. Between $1-1000$ clients, the size overhead of the sparse representation increases from 18.04MB to 18.33MB. The small size increase comes from the convolutional kernel parameters and biases.

Table~\ref{tab:model-size-comparison} shows the percentage overhead added by the sparse tensor attack and Robbing the Fed on several standard vision models. There are 1000 clients in aggregation with a batch size of 64. The sparse tensor representation adds a significantly smaller overhead ($18.33$MB) compared to Robbing the Fed ($6000.99$MB) while achieving the same leakage rate. Even with a large model like VGG-11, Robbing the Fed adds a massive model size overhead increase of $1184.0\%$, while the sparse attack only adds $3.6\%$.

We note that using a compressed sparse row (CSR) tensor representation results in a model size overhead of only roughly $\frac{2}{3}$ compared to the COO representation. At 1000 clients, the size added using sparse CSR is only $12.33$MB. However, this sparse tensor representation is currently in the beta phase of PyTorch, so we only use sparse COO tensors for the experimental comparisons. 

\begin{figure}[!t]
\vspace*{-2.5mm}
\begin{center}
\includegraphics[width=1.0\columnwidth,trim={2mm 0 2mm 10mm},clip]{images/train_time_all_resnet50.pdf}
\end{center}
\vspace*{-5mm}
\caption{\label{fig:resnet50-comp} Computational overhead in training time added to a ResNet-50 on a CPU from attacks. At 1000 clients, the sparse tensor method adds a 6.5s overhead while Robbing the Fed adds a 21.8s overhead.}
\vspace*{-5mm}
\end{figure}

\subsection{Computation overhead}
We compare the computational overhead added by the linear layer leakage attacks through a comparison of the time to compute an update for an individual client. This includes the time for a forward pass, loss computation, and gradient computation on a client batch. The baseline model we use is a ResNet-50. The vanilla model uses 2.14 seconds for the update computation on a batch of 64 images.

Figure~\ref{fig:resnet50-comp} shows the time required for the update computation for all three attacks with a varying number of clients and a batch size of 64. With 100 clients, using sparse weights adds a $34\%$ (0.73s) time overhead, dense weights adds a $55\%$ (1.17s) overhead, and Robbing the Fed adds a $67\%$ (1.43s) overhead. At 1000 clients, the overhead is $305\%$ (6.54s), $714\%$ (15.30s), and $1019\%$ (21.85s) respectively. With 1000 clients, the sparse attack adds $3.34\times$ less computational overhead compared to Robbing the Fed.

Much work is going into sparse matrix/tensor optimization~\cite{bell2008efficient, williams2007optimization, dalton2015optimizing, zhao2018bridging}. While these experiments give a brief snapshot of the potential computational differences between methods, we note that as sparse tensor implementations improve, the computation overhead of the sparse weights will continue to decrease.

\begin{table}[!t]
\footnotesize
\begin{center}
\begin{tabular}{|l|c|ccc|}
\hline
                                                                                            & \textbf{Clients} & \textbf{\begin{tabular}[c]{@{}c@{}}Robbing\\ the Fed\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Dense\\ weights\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Sparse\\ weights\end{tabular}} \\ \hline
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}MNIST\\ (28x28x1)\end{tabular}}}         & 100              & 153.2                                                             & 77.3                                                            & 4.6                                                              \\
                                                                                            & 1000             & 1532.2                                                            & 766.4                                                           & 4.6                                                              \\ \hline
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}CIFAR-100\\ (32x32x3)\end{tabular}}}     & 100              & 600.1                                                             & 303.0                                                           & 18.0                                                             \\
                                                                                            & 1000             & 6001.0                                                            & 3003.3                                                          & 18.3                                                             \\ \hline
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Tiny ImageNet\\ (64x64x3)\end{tabular}}} & 100              & 2400.1                                                            & 1212.1                                                          & 72.1                                                             \\
                                                                                            & 1000             & 24001.0                                                           & 12012.4                                                         & 72.4                                                             \\ \hline
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}ImageNet\\ (256x256x3)\end{tabular}}}    & 100              & 38400.9                                                           & 19392.8                                                         & 1152.8                                                           \\
                                                                                            & 1000             & 384001.7                                                          & 192193.1                                                        & 1153.1                                                           \\ \hline
\end{tabular}
\end{center}
\vspace*{-5mm}
\caption{\label{tab:input-image-size} Comparison of model size overhead (MB) using different datasets with batch size 64 and 100 and 1000 clients. At 1000 clients on ImageNet, the sparse representation adds a 1.1GB overhead while Robbing the Fed adds 375GB.}
\vspace*{-6mm}
\end{table}

\subsection{Larger image sizes}
\label{sec:experiments-larger-ims}
\vspace{-1mm}
We revisit the model size to show the overhead added for different image sizes. As discussed in Section~\ref{sec:methodology}, the fundamental requirement of linear layer leakage is to be able to store all image pixels in the gradients. As a result, the input image size directly ties to the model overhead added by the attack. Table~\ref{tab:input-image-size} shows the overhead added from the dense and sparse tensor representation attacks along with Robbing the Fed on several different input image sizes. Results are shown for 100 and 1000 clients with a batch size of 64.

As the input image size increases, so does the size overhead from the inserted module. This size increase trend is (near) directly proportional to the change in input image size. For example, the difference in image size between Tiny ImageNet and ImageNet is $(256\cdot256\cdot3) / (64\cdot64\cdot3)=16$. We see that the size overhead difference for Robbing the Fed is also $38,400.9/2400.1\approx 16$. This scaling property also exists with dense and sparse tensor representations.

The model size overhead added, particularly for the larger image sizes, is extremely large. For Robbing the Fed and the dense weight representation, for 1000 clients on ImageNet, the size overhead reaches 375GB and 188GB respectively. By comparison, the sparse tensor setting is much better for attack scalability, creating a little over 1GB in size overhead for 1000 clients.

These experiments highlight a problem with the model size overhead for current linear layer leakage methods when working with larger input sizes. The need to store image pixels in the gradients means that larger images inherently create larger size overheads. This in turn results in overheads in all aspects of memory, communication, and computation for the clients, and practically, these overheads are too large for FL. For the malicious server, one solution would be to use pooling operations prior to leaking the images. While this method will result in reconstructing downsampled images, leaking full-resolution large-sized images, especially with aggregation, is unrealistic. This fundamental limitation applies to all current linear layer leakage methods. Sparsity can significantly decrease the size overhead, but once the input images become large enough the attacks become infeasible on reasonable-sized devices.

\vspace*{-1mm}
\subsection{Leakage in terms of mutual information}
\vspace{-1mm}
We focus on the differences between binning~\cite{fowl2022robbing} and trap-weights~\cite{boenisch2021curious} in terms of mutual information using the neural estimator proposed in~\cite{belghazi2018mine}. Compared to leakage rate which only considers the number of reconstructed images, the mutual information ratio is a finer-grained metric --- it also captures the information leakage that cannot be reconstructed directly into individual images due to images activating the same neurons and thus ignored by leakage rate.
% Additional details on how the mutual information is estimated are discussed in the supplements.

\begin{figure}[!t]
\vspace*{-7mm}
\begin{center}
\includegraphics[width=1.0\columnwidth,trim={0 0 0 0},clip]{images/MI_leakage_vs_imprint_RtF_WtC.pdf}
\end{center}
\vspace*{-6mm}
\caption{\label{fig:MI_leakage_ratio} Comparison of the percentage of the information leaked into the gradient $I(x^{input}_k;g)$ that is recovered through reconstruction $I(x^{input}_k;x_k)$ based on the FC layer size when using binning and trap weights. MNIST dataset with a batch size = 10 is used.}
\vspace*{-6mm}
\end{figure}
Figure~\ref{fig:MI_leakage_ratio} shows that the power of the image reconstruction increases for both the binning and trap weights in terms of the percentage of leaked information as the FC layer size increases. Figure~\ref{fig:MI_leakage_ratio} also shows that for all FC layer sizes, the leakage from trap weights~\cite{boenisch2021curious} is lower than binning~\cite{fowl2022robbing}.
