\twocolumn
\section{Leakage rate and reconstructions}
Table~\ref{tab:leakage-rate} gives the leakage rate on the MNIST~\cite{lecun1998mnist}, CIFAR-100~\cite{krizhevsky2009learning}, and Tiny ImageNet~\cite{le2015tiny} datasets using sparse \name and Robbing the Fed~\cite{fowl2022robbing} as discussed in the main paper. We use a batch size of 64 with 100 clients, and the ratio of FC size to batch size is 4:1 (256 unit FC layer). The leakage rate on CIFAR-100 and Tiny ImageNet are roughly the same for both methods. Sparse \name~\cite{our2022mandrake} has a slightly lower leakage rate than Robbing the Fed on MNIST.

Figure~\ref{fig:single_client_tinyimagenet} shows the ground truth and reconstructions for a single, random client with a batch of 64 on Tiny ImageNet using the sparse \name attack. 50 images were leaked from the client.

\begin{table}[!t]
\begin{center}
\begin{tabular}{|l|cc|}
\hline
                       & \textbf{\begin{tabular}[c]{@{}c@{}}Sparse \\ \name \end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Robbing\\ the Fed\end{tabular}} \\ \hline
\textbf{CIFAR-100}     & 77.5\% (4957)                                                              & 77.1\% (4931)                                                             \\
\textbf{MNIST}         & 71.0\% (4546)                                                              & 75.1\% (4803)                                                             \\
\textbf{Tiny ImageNet} & 77.8\% (4978)                                                              & 77.7\% (4970)                                                             \\ \hline
\end{tabular}
\end{center}
\vspace*{-4mm}
\caption{\label{tab:leakage-rate} Total leakage rate of sparse \name and Robbing the Fed on various datasets. For all three datasets, 100 aggregated clients and batch size of 64 were used (6400 total images).}
\vspace*{-5mm}
\end{table}


\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=0.97\textwidth]{images/tiny-imagenet-ground-truth.png}
         \caption{Ground truth}
         \label{fig:ground-truth-tiny-image}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=0.97\textwidth]{images/tiny-imagenet-reconstructed.png}
         \caption{Reconstruction}
         \label{fig:reconstruction-tiny-image}
     \end{subfigure}
% \includegraphics[width=0.5\columnwidth]{Plots-Images/Leaked-images-cifar100.png}
\vspace*{-1mm}
\caption{\label{fig:single_client_tinyimagenet} Reconstructed images from Tiny ImageNet from a random client with a batch size of 64 using \name. The ground truth images (a) are shown on the left and the reconstructed images (b) are shown on the right. Any empty boxes within the reconstructed images indicate that reconstruction failed due to an overlap of image activations.}
% \vspace*{-5mm}
\end{figure*}


\section{Trap weights under FL}
\begin{figure}[!t]
\begin{center}
\includegraphics[width=1.0\columnwidth,trim={3mm 0 3mm 10mm},clip]{images/wtc_comparison_rtf.pdf}
\end{center}
\vspace*{-6mm}
\caption{\label{fig:wtc-comparison} Leakage rate using trap weights (TW) for a batch size of 64 on Tiny ImageNet and varying the FC layer size ratio and number of clients. The leakage rate decreases with an increasing number of clients even if the ratio of FC size to total number of images remains the same. Binning (Robbing the Fed) has a higher leakage rate at all scales of FC size.}
\vspace*{-4 mm}
\end{figure}

We show the leakage rate using the trap weights attack~\cite{boenisch2021curious} for different FC layer sizes on the downsampled Tiny ImageNet (32x32x3) dataset. We tune the scaling factor between 0.90 and 0.99 (step size of 0.01) to find the highest leakage rate, vary the FC layer ratio (FC layer size = batch size $\times$ num. clients $\times$ FC size ratio), and report the average over 10 runs. We apply this on several numbers of clients and Figure~\ref{fig:wtc-comparison} shows the results compared to binning~\cite{fowl2022robbing}, which has roughly the same leakage rate regardless of the number of clients. Even while maintaining the same ratio between the FC layer size and total number of images, the leakage rate when using trap weights decreases as the number of clients increases.

We note that by using sparsity, trap weights are able to overcome this scalability problem. However, since the binning method of Robbing the Fed achieves a higher leakage rate for all FC layer size ratios, it is still a better choice.



\section{Sparse variant of Robbing the Fed}
The sparse variant of Robbing the Fed (RtF)~\cite{fowl2022robbing} is a method introduced in addition to their baseline in order to apply the attack in the FedAVG setting. The "sparsity" mentioned in Section 4.3 of the RtF paper is discussing how to create activations in the fully-connected (FC) layer such that images should only activate a single neuron instead of a set of neurons. However, this does {\em not} reduce the resource usage added from the attack, which is what we address. With the main change being in the activation function, the same fundamental method as the baseline is used with aggregated updates and the FC layer size still needs to scale to compensate for the total number of images. These layers added to the model will still be fully dense with non-zero parameters.

\section{Evaluating information leakage using mutual information}
In practice, the amount of leaked information is typically quantified as the number of images a malicious server reconstructs (leaked). However, the reconstructions from the attack module can also leak some additional information that is not counted in the leakage rate. For example, while reconstructions of images can overlap, an observer can still obtain information about the training data (e.g., a malicious server who sees an overlap of digits 2, 3, and 8 might be able to identify that an 8 is in the reconstruction). In Section 4, we compared how much information was leaked to the server under a varying FC layer size using either the binning and trap weights method of linear layer leakage attacks, since \name is able to use both.

We used the MNIST dataset for these experiments, and in order to measure the amount of information leaked into the gradient and the amount of information the server was able to reconstruct out of it, we compare the mutual information between: (1) the data batch $x^{input}_k$ at user $i$ and the aggregate gradient $g$ of the attack at the server; (2) the data batch $x^{input}_k$ and the reconstructions $x_k$ at the server for user $k$. Note that by the data processing inequality, we have that:
\begin{equation}\label{eq:ratio}
    \frac{I(x^{input}_k;x_k)}{I(x^{input}_k;g)} \leq 1.    
\end{equation}
since the leaked images were reconstructed only using the gradient. In order to compute the mutual information terms in~\eqref{eq:ratio}, we use the Mutual Information Neural Estimator (MINE) which is the SOTA method~\cite{belghazi2018mine} to estimate the mutual information between two random vectors. For each FC layer size, we sampled 20,000 random batches of the users' data and used each to compute the aggregate gradient $g$ and reconstructed images for a single user $i$. These 20,000 samples were used by MINE to estimate mutual information.

This same procedure was repeated multiple times in order to get multiple mutual information estimates and the average ratio was reported.

\section{FedAVG}
Unlike the gradients of the FC layers, the gradients of the convolutional layer are not necessary for the data reconstruction attack. A malicious server can then send a maliciously crafted model which would freeze the parameters of the convolutional layer to prevent changes from occurring over the local iterations of FedAVG. 
