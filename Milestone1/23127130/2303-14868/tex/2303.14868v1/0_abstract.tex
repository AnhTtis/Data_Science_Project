\iffalse
Federated learning aims to collaboratively train a neural network while offering the promise of user data privacy. Clients participating in each round train the global model on their private training data and send the update to the server. Prior work has shown that the individual updates are susceptible to data reconstruction attacks, allowing attackers to directly recover the user data from the gradient. Secure aggregation has been used to address this weakness, maintaining that a server only has access to the aggregate update. Optimization attacks completely fail under this setting and most analytical attacks result in a small leakage scale or require multiple training rounds. Only linear layer leakage methods are able to scale to aggregation, achieving a high leakage rate regardless of the number of clients participating in secure aggregation. However, this is done through increasing the size of an injected fully-connected (FC) layer, which while offering attack scalability, adds resource overhead to the participating clients. In particular, memory, communication, and computational overhead added by the attack prevents practicality in federated learning, which is a fundamental problem for these methods. Particularly, we argue that the perspective of viewing the aggregate update as combining separate individual client updates is critical to solving the resource problem. We further show that the use of sparsity in linear layer leakage attacks alleviates these resource overheads, decreasing the model size overhead by over 327$\times$ and the computation time by 3.34$\times$ compared to state-of-the-art while maintaining a $77\%$ total leakage rate even with $1000$ clients in aggregation. Finally, we discuss the fundamental challenges that remain in regards to input size and the problems with application of sparsity with secure aggregation.
\fi

Secure aggregation promises a heightened level of privacy in federated learning, maintaining that a server only has access to a decrypted {\em aggregate} update. Within this setting, linear layer leakage methods are the only data reconstruction attacks able to scale and achieve a high leakage rate regardless of the number of clients or batch size. This is done through increasing the size of an injected fully-connected (FC) layer. However, this results in a resource overhead which grows larger with an increasing number of clients. We show that this resource overhead is caused by an incorrect perspective in all prior work that treats an attack on an aggregate update in the same way as an individual update with a larger batch size. Instead, by attacking the update from the perspective that aggregation is combining multiple individual updates, this allows the application of sparsity to alleviate resource overhead. We show that the use of sparsity can decrease the model size overhead by over 327$\times$ and the computation time by 3.34$\times$ compared to SOTA while maintaining equivalent total leakage rate, 77\% even with $1000$ clients in aggregation.