\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{subfigure}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{14} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ificcvfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Application-Driven AI Paradigm for Person Counting in Various Scenarios}

% \author{Minjie Hua \quad Yibing Nan \quad Shiguo Lian\\
% CloudMinds Technologies Inc., Beijing, China\\
% % Institution1 address\\
% {\tt\small \{michael.hua, charlie.nan, scott.lian\}@cloudminds.com}

% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Yibing Nan$^*$\\
%CloudMinds Technologies\\
%% First line of institution2 address\\
%{\tt\small charlie.nan@cloudminds.com}
%\and
%Shiguo Lian\\
%CloudMinds Technologies\\
%{\tt\small scott.lian@cloudminds.com}
%}

\author{Minjie Hua\\
China Unicom\\
{\tt\small huamj5@chinaunicom.cn}
\and
Yibing Nan\\
China Unicom\\
{\tt\small nanyb5@chinaunicom.cn}
\and
Shiguo Lian\\
China Unicom\\
{\tt\small sg\_lian@163.com}
}

\maketitle
%\thispagestyle{empty}


%%%%%%%%% ABSTRACT
\begin{abstract}
%This paper presents a novel approach for predicting the falls of people in advance from monocular video. First, all persons in the observed frames are detected and tracked with the coordinates of their body keypoints being extracted meanwhile. A keypoints vectorization method is exploited to eliminate irrelevant information in the initial coordinate representation. Then, the observed keypoint sequence of each person is input to the pose prediction module adapted from sequence-to-sequence(seq2seq) architecture to predict the future keypoint sequence. Finally, the predicted pose is analyzed by the falls classifier to judge whether the person will fall down in the future. The pose prediction module and falls classifier are trained separately and tuned jointly using Le2i dataset, which contains 191 videos of various normal daily activities as well as falls performed by several actors. The contrast experiments with mainstream raw RGB-based models show the accuracy improvement of utilizing body keypoints in falls classification. Moreover, the precognition of falls is proved effective by comparisons between models that with and without the pose prediction module.

Person counting is considered as a fundamental task in video surveillance. However, the scenario diversity in practical applications makes it difficult to exploit a single person counting model for general use. Consequently, engineers must preview the video stream and manually specify an appropriate person counting model based on the scenario of camera shot, which is time-consuming, especially for large-scale deployments. In this paper, we propose a person counting paradigm that utilizes a scenario classifier to automatically select a suitable person counting model for each captured frame. First, the input image is passed through the scenario classifier to obtain a scenario label, which is then used to allocate the frame to one of five fine-tuned models for person counting. Additionally, we present five augmentation datasets collected from different scenarios, including side-view, long-shot, top-view, customized and crowd, which are also integrated to form a scenario classification dataset containing 26323 samples. In our comparative experiments, the proposed paradigm achieves better balance than any single model on the integrated dataset, thus its generalization in various scenarios has been proved.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}\label{sec:intro}

% Falls are a major cause of fatal injury especially for the elderly and create a serious obstruction for independent living~\cite{FALL}. Therefore, falls prediction is one of the most meaningful applications for elderly caring and home monitoring \etc. The precognition of falls is the prerequisite for follow-up preventions and early warning, which will largely decrease the risks of falling accident. Although mainstream human action recognition (HAR) algorithms can be trained to recognize falls as one of the action classes, most of them utilize raw RGB information for classification and are not capable of predicting falls in advance.

Person counting is a common application in many video surveillance tasks such as visitor analysis, traffic monitoring and abnormality recognition. In general, the two mainstream techniques for person counting are detection-based methods and density-based methods. The former detects the whole or part of a person's body and count the number of detected bounding boxes as the final prediction, while the latter generates a density map and sums up all the pixel values to produce the person counting estimation.

Over the past few years, a multitude of algorithms based on convolutional neural networks (CNN) have been developed to handle various real-world scenarios. For most mid-range scenarios where people are captured from a side view, widely-used object detection methods such as YOLO~\cite{YOLO,YOLOv2,YOLOv3,YOLOv4} and SSD~\cite{SSD}, pre-trained on MSCOCO~\cite{MSCOCO}, already satisfy the requirements. However, in some challenging scenarios \eg long-shot images with tiny person targets, top-view images where only heads are visible, or specific places where people are wearing customized suits, additional datasets are necessary for fine-tuning the detector. Furthermore, when facing the scenario of crowd, the accuracy of almost all person detectors drops significantly due to severe body occlusion that impedes feature extraction of individuals. Compared to detection-based approaches, density-based methods~\cite{DensityCount,ShanghaiTech,CSRNet,Bayesian,DMCount,P2PNet} perform better under the crowd scenarios.

% However, the following characteristics make fall distinct from other actions: 1) Fall is highly relevant to the status of body keypoints, \ie, the body skeleton of a fallen person is significantly different. 2) Unlike smoking and handshaking, fall does not involve interactions with objects or other people. 3) Fall is an accident rather than a daily activity. So it's expected to `foresee' its happening and alert emergency as soon as possible.
% % \begin{enumerate}
% %  \item Falls is highly relevant to the status of body keypoints, \ie, the body skeleton of a fallen person is obviously different from others.
% %  \item Unlike smoking and handshaking, falls is generally not involved in the interaction with objects or other people.
% %  \item Falls is not a normal action, but an accident. So it's expected to predict its happening in advance and alert emergency as soon as possible.
% % \end{enumerate}

% According to 1) and 2), our falls classifier gives prediction using human body keypoints instead of raw RGB information in the video frames. The essence is to decrease the dimensionality of features with valuable cues preserved. With regard to 3), we introduce a pose prediction module adapted from sequence-to-sequence (seq2seq)~\cite{SEQ2SEQ} to predict future keypoint sequence based on the observed keypoint sequence. By analyzing future pose, the falls classifier is able to give early prediction.

Although solutions can be found on a case-by-case basis, experienced engineers must review all video streams in advance and manually assign the most suitable person counting model to each camera device in practical applications. This process is time-consuming and cumbersome, especially as the deployment scale expands. Moreover, when the backstage manager controls the camera by moving, rotating or zooming, it can cause a shift in the captured scene, potentially resulting in imprecise output from the predetermined model.

% It is well known that mainstream keypoints detection models like OpenPose~\cite{OPENPOSE} and AlphaPose~\cite{ALPHAPOSE} represent each extracted keypoint by its coordinate in the image. However, coordinate representation involves the body's absolute position and scale, which contribute little to action classification. Consequently, we propose a keypoints vectorization method to transform coordinates to a feature vector, in which only the direction information remains.

In this paper, we propose a unified framework that concatenates a scenario classifier and a person counting module. The scenario classifier is a ResNet-50~\cite{ResNet} network, and the person counting module contains five fine-tuned models prepared for corresponding scenarios: (i) side-view, where people are captured from a side view in mid-range, (ii) long-shot, where person bodies appear tiny, (iii) top-view, where people are captured from an overhead camera, (iv) customized, where people are wearing specific suits (we use protective suits as an example, which are commonly observed in healthcare institutions), and (v) crowd, where numerous people appear in the image. For (i)-(iv), we utilized YOLOv5, an improved version of YOLOv4~\cite{YOLOv4}, as the person counting model. For (v), we employ DM-Count~\cite{DMCount} as the person counting model. Additionally, we enhance the robustness of the aforementioned models in specific scenarios by introducing five augmentation datasets. These datasets are also integrated to form a scenario classification dataset used to train the scenario classifier. Further details about these datasets are provided in Section~\ref{subsec:data}.

% Since the pose prediction module already encodes temporal features, the falls classifier can focus on the spatial cues in the predicted body pose. To train the falls classifier, we re-annotated Le2i dataset~\cite{LE2I} to tag each frame a label. This operation largely increases the amount of training data, which makes the falls classifier converge better.

% %\begin{figure} [tbp]
% %  \centering
% %  \subfigure[]{
% %    \centering
% %    \label{fig:case:a} %% label for first subfigure
% %    \begin{minipage}[b]{0.15\textwidth}%\textwidth %0.46  %\linewidth
% %      \centering
% %      \includegraphics[width=1\linewidth]{figs/fig_1_a.png}%0.2
% %    \end{minipage}}
% %  \subfigure[]{
% %    \centering
% %    \label{fig:case:b}
% %    \begin{minipage}[b]{0.15\textwidth}
% %      \centering
% %      \includegraphics[width=1\linewidth]{figs/fig_1_b.png}
% %    \end{minipage}}
% %  \subfigure[]{
% %    \centering
% %    \label{fig:case:c} %% label for first subfigure
% %    \begin{minipage}[b]{0.15\textwidth}%\textwidth %0.46  %\linewidth
% %      \centering
% %      \includegraphics[width=1\linewidth]{figs/fig_1_c.png}%0.2
% %    \end{minipage}}
% %  \caption{Some cases with small obstacles in autonomous driving (a), patrol robot (b) and blind guidance (c).}
% %  \label{fig:case}
% %\end{figure}

% The main contributions of this paper are as follows:
% \begin{itemize}
%   \item We proposed an end-to-end falls prediction model, which consists of a seq2seq-based pose prediction module and a falls classifier.
%   \item We developed a keypoints vectorization method to extract salient features from coordinate representation.
%   \item We evaluated our model by comparisons with mainstream HAR networks, and self-comparison between the pose prediction module enabled and disabled.
% \end{itemize}

The main contributions of this work are three-fold:
\begin{itemize}
  \item We propose an application-driven paradigm that automatically selects proper person counting model for the input image based on the scenario classification result.
  \item We introduce five augmentation datasets to enhance models in specific scenarios, together with a classification dataset to train the scenario classifier.
  \item We conduct comparative experiments to demonstrate the generalization of our proposed framework.
\end{itemize}

% The rest of the paper is organized as follows: Section~\ref{sec:relat} reviews the related work on falls detection, action recognition and prediction. The proposed network is presented in Section~\ref{sec:sys}. In Section~\ref{sec:exp}, the dataset and experiments are described. Finally, Section~\ref{sec:concl} gives a conclusion.

The rest of the paper is structured as follows: Section~\ref{sec:relat} reviews the related work on person detection and crowd counting. Section~\ref{sec:sys} presents the proposed framework and datasets. In Section~\ref{sec:exp}, we describe the experiments and demonstrate the results. Finally in Section~\ref{sec:concl}, we draw a conclusion.

\section{Related Work}\label{sec:relat}

% \subsection{Falls Detection}
\subsection{Person Detection}

% Many early solutions for falls detection relied on the wearable devices equipped by the person to be monitored. Bourke \etal proposed a method in~\cite{BO07} to detect falls based on peak acceleration. Narayanan \etal developed a distributed falls management system, which is capable of real-time falls detection using a waist-mounted triaxial accelerometer~\cite{NM07}. Bianchi \etal enhanced previous falls detection system with an extra barometric pressure sensor in ~\cite{BF09,BF10} and found that the algorithm incorporating pressure information achieved higher accuracy.

Traditional person detection methods relied on handcrafted features, \eg deformable part models (DPM) using histograms of oritented gradients (HOG) features~\cite{DPM1,DPM2} and decision forests models using integral channel features (ICF)~\cite{DF1,DF2,DF3}. With the great success of AlexNet~\cite{AlexNet} in ImageNet~\cite{ImageNet} competition, CNN-based methods became mainstream in computer vision tasks like image classification and object detection.

% Although wearable device based falls detection methods are computationally efficient and insensitive to the environment, they require users to wear corresponding sensors for data collection. This limitation causes huge inconvenience to the users, and also makes it expensive for widespread application. Moreover, these algorithms suffer from a high false-detection rate due to sudden acceleration change in daily activities such as squat and run \etc.

R-CNN~\cite{RCNN} pioneered to introduce CNN into object detection by implementing the two-stage architecture, \ie a proposal stage and a downstream classification stage. Regarding to the intolerable computational complexity for R-CNN to classify each proposal separately, Fast R-CNN~\cite{FastRCNN} optimized the inference time by executing CNN on the whole image at the beginning to extract features shared by all proposals. Faster R-CNN~\cite{FasterRCNN} replaced the external proposal modules in Fast R-CNN with a region proposal network (RPN), integrating the two stages into an end-to-end framework which could be trained jointly.

% Later, vision-based solutions were developed to bring more user-friendly experience. Ma \etal~\cite{MX14} proposed an approach that extracted curvature scale space (CSS) features of human silhouettes from a depth camera and represented each action by a bag of CSS words (BoCSS), which was then classified by the extreme learning machine (ELM) to identify falls. Stone \etal presented solutions for falls recognition using gait parameters in~\cite{SS11} and five handcrafted features in~\cite{SS14}. And both were based on the foreground extracted from the Kinect. Besides, Quero \etal adopted non-invasive thermal vision sensors in~\cite{QJ18} to detect falls using thermal images.

One-stage detection methods completely abandoned the proposal stage. YOLO~\cite{YOLO} divided the final feature map into $S\times S$ grid cells and predicted the center coordinate, width and height of the object bounding box in each cell. However, this approach had disadvantage in detecting small objects and clustered objects within a single cell. This issue was resolved in YOLOv2~\cite{YOLOv2} by adopting anchor boxes, whose scales and aspect ratios were set a priori by performing k-means clustering~\cite{Kmeans} on the training dataset. YOLOv3~\cite{YOLOv3} introduced residual connections, which was first presented in ResNet~\cite{ResNet}, to construct a deeper darknet-53 network, and proposed feature pyramid networks (FPN) for higher recall on smaller objects. YOLOv4~\cite{YOLOv4} embedded advanced techniques in the backbone, neck and head of original YOLOv3 network, and deployed bag of freebies including mosaic data augmentation, self-adversarial training, and CIoU loss function, together with bag of specials including Mish activation function, cross mini-Batch normalization and drop-block regularization. Just a few weeks later, Jocher~\etal released YOLOv5 implemented on PyTorch~\cite{PyTorch} aiming to further improve accessibility and achieve greater balance between the effectiveness and efficiency. Similar to YOLO serials, SSD~\cite{SSD} also used anchor boxes with a variety of aspect ratios. The difference is that SSD applied anchor boxes on multiple feature maps with different resolutions, thus being able to detect objects of diverse scales.

% To the best of our knowledge, there was a lack of monocular vision based algorithms specialized for falls detection. However, most of the HAR models would support falls recognition after being trained on the dataset including fall as one of the actions. For example, the UCF-101 dataset~\cite{UCF101} contains 13320 videos divided into 101 action categories but without falls. By contrast, the HMDB-51~\cite{HMDB}, another large-scale dataset containing 6849 videos, takes `fall on the floor' as one of the 51 action classes.

Person detection can be achieved by training the aforementioned models on datasets taking 'person' as one of the categories, such as MSCOCO~\cite{MSCOCO}, Pascal VOC~\cite{PascalVOC} and CityPersons~\cite{CityPersons}. In these public datasets, the majority of persons were captured from the frontal or side view with an optimal shot range. In recent years, overhead person detection gained significant attention, and models and datasets specifically designed for detecting persons from a top-view perspective were introduced in several studies~\cite{TOPVIEW, TopviewData}. However, there was a lack of open-source datasets customized to detect persons in long-range fields and those wearing protective suits, which motivated us to collect and utilize such datasets in our experiments.

\begin{figure*}[tbp]%
\centering
\includegraphics[width=1\linewidth]{architecture.png}
\caption{The workflow of our proposed paradigm. The input image is first passed to the scenario classifier to obtain a scenario label. Based on this label, the image is allocated to one of five person counting models, which produce the final prediction on the number of persons in the image.}
\label{fig:workflow}
\end{figure*}

% \subsection{Action Recognition and Prediction}
\subsection{Crowd Counting}

% As investigated in~\cite{SURVEY}, convolutional neural networks (CNN) and temporal modeling are the two major variables for action recognition. Karpathy \etal \cite{HAR2D} pioneered to introduce CNN in HAR problem by finetuning a general image recognition model pre-trained on ImageNet~\cite{IMAGENET} using UCF-101~\cite{UCF101}. However, The inability of utilizing temporal information became a severe disadvantage of 2D CNN. This issue was resolved by the 3D CNN proposed in~\cite{C3D,Conv3D}. By performing 3D convolutions, the 3D CNN was capable of extracting features from both spatial and temporal dimensions, thereby capturing the motion information in multiple adjacent frames. As for the temporal modeling methods like Two-Stream~\cite{TWOSTREAM} and TSN~\cite{TSN}, the main idea was to extract spatial features using 2D convolutions and encode temporal information by recurrent neural networks(RNN).

Early approaches to crowd counting relied on detecting persons, heads or upper bodies~\cite{detCrowd1,detCrowd2,detCrowd3}. However, these detection-based methods suffered from severe occlusions, especially in the dense crowds. Later, researchers developed regression-based frameworks~\cite{regCrowd1,regCrowd2,regCrowd3,regCrowd4} to avoid the detection shortcomings. However, the model was hard to converge without the supervision of head localization annotations in the training process. Recently, methods based on density map estimation~\cite{DensityCount,ShanghaiTech,CSRNet,Bayesian,DMCount,P2PNet} outperformed the aforementioned detection-based and regression-based approaches and became the mainstream solution for crowd counting problem.

% However, all the aforementioned models predict only one label for each video, even when multiple actions are existing in the meantime. As shown in Fig.~\ref{fig:multiact}, although two people are presenting different actions, the TSN model~\cite{TSN} trained on UCF-101 dataset~\cite{UCF101} just predict `swing' with the man's fall being ignored.

Due to the difficulty of delineating the spatial extent for each person in crowd scenes, existing crowd counting datasets~\cite{ShanghaiTech,UCFCC50,UCFQNRF,NWPU} only mark each person with a single dot on the head or forehead. Consequently, the ground truth density map generated from the annotations is a sparse binary matrix, while the predicted density map is a dense real-value matrix. However, directly measuring the discrepancy between the sparse binary and dense real-value matrices with a loss function can make the network hard to converge.

% \begin{figure}[htbp]%
% \centering
% \includegraphics[width=1\linewidth]{multi_actions.png}
% \caption{An example of multiple actions presented in the meantime. TSN model trained on UCF-101 dataset gives the prediction `swing'. However, the man's fall is ignored because only one label will be predicted for a video.}
% \label{fig:multiact}
% \end{figure}

% Action recognition models must `watch' the entire video to give prediction. But in some cases like incomplete data or the requirement of early alarm, predicting the future action based on partial clip is necessary. Early classification and motion prediction are two routes of action prediction towards different goals: Given $t_{\text{obs}}$ observed frames $(f_1, f_2, ..., f_{t_{\text{obs}}})$, early classification tries to infer the label $y$ in advance, while motion prediction tries to produce future motions in next $t_{\text{pred}}$ frames $(f_{t_{\text{obs}}+1}, f_{t_{\text{obs}}+2}, ..., f_{t_{\text{obs}}+t_{\text{pred}}})$.

Therefore, a crucial challenge for all density map estimation methods is to effectively utilize the dot annotations. One common approach is to convert each annotated dot into a Gaussian blob, creating a 'pseudo ground truth' that is more balanced. Most prior methods \eg DensityCount~\cite{DensityCount}, MCNN~\cite{ShanghaiTech} and CSRNet~\cite{CSRNet} adopted this idea. However, the kernel widths used for the Gaussian blobs may not accurately reflect the size of people's heads in the image, which can significantly impact the network's performance. Another approach is to design a reasonable loss function. For example, a Bayesian loss was proposed in~\cite{Bayesian} to transform the annotation map into $N$ smoothed density maps, where each pixel value is the posterior probability of the corresponding annotation dot. Recently, the DMCount~\cite{DMCount} model used optimal transport (OT) and total variation (TV) loss to measure the similarity between the normalized predicted density map and the normalized ground truth density map. Without introducing Gaussian smoothing operations, DMCount has been shown to outperform the aforementioned Gaussian-based methods.

% The work in~\cite{MS16} designed novel ranking losses to learn activity progressing in LSTMs for early classification. Kong \etal adopted an auto-encoder to reconstruct missing features from observed frames by learning from the complete action videos~\cite{KT17}. In \cite{KG18}, a mem-LSTM model was proposed to store several hard-to-predict samples and a variety of early observations in order to improve the prediction performance at early stage.

% In recent years, motion prediction attracted much attention. Fragkiadaki \etal proposed the encoder-recurrent-decoder (ERD) model~\cite{ERD} that extended long short term memory (LSTM)~\cite{LSTM} to jointly learn representations and their dynamics. Jain \etal proposed structural-RNN (SRNN) based on spatiotemporal graphs (st-graphs) in \cite{SRNN} to capture the interactions between the human and the objects. Martinez \etal modified standard RNN models in sampling-based loss and residual architectures for better motion prediction~\cite{CVPR17}. Tang \etal proposed modified highway unit (MHU) to filter the still keypoints and adopted gram matrix loss in~\cite{IJCAI18} for long-term motion prediction. For the same purpose, Li \etal proposed convolutional encoding module (CEM) in~\cite{CVPR18} to learn the correlations between each keypoint, which is hard for an RNN model.

\section{Methodology}\label{sec:sys}

% As illustrated in Fig.~\ref{fig:flow}, the proposed obstacle detection and avoidance system contains several steps: RGB-D based two-stage semantic segmentation, morphological processing, local destination setting and path planning. The two-stage semantic segmentation transforms the input RGB-D image to a raw binary image, which is then smoothed in morphological processing. As a result, the module generates a calibrated binary image where every pixel is labelled as either ground or obstacle. Then the binary image is passed to the obstacle avoidance module to determine a destination and a walkable path. The whole process works repeatedly during robot¡¯s moving.

\subsection{Overview of the Paradigm}\label{subsec:overview}

% The problem to be solved in this paper is formulated as follow: Given $t_{\text{obs}}$ observed frames $(f_1, f_2, ..., f_{t_{\text{obs}}})$, we try to predict whether the human(s) in the video will fall down in next $t_{\text{pred}}$ frames (\ie, from $f_{t_{\text{obs}} + 1}$ to $f_{t_{\text{obs}} + t_{\text{pred}}}$).

% The skeleton framework of our model is presented in Fig.~\ref{fig:workflow}. The input is a sequence of observed frames. We first adopted OpenPose~\cite{OPENPOSE} to extract keypoints coordinates of human(s) from each observed frame. The bounding boxes of detected persons were passed to DeepSort~\cite{DEEPSORT}, a tracking algorithm, to cluster body keypoints belonging to the same person in different frames. As a result, the $i$-th person was corresponding to a sequence of observed keypoints $\mathbf{K}^i_{obs}=\left(\mathbf{k}^i_1, \mathbf{k}^i_2, ..., \mathbf{k}^i_{t_{\text{obs}}} \right)$, where $\mathbf{k}^i_j$ included the keypoints coordinates of the $i$-th person in frame $j$.

The architecture of our proposed paradigm is shown in Fig.~\ref{fig:workflow}. First, the input image is passed to the scenario classifier, which classifies the image into one of five scenario categories as defined in Section~\ref{sec:intro}. After that, the image is fed to the person counting module containing five models fine-tuned on the corresponding augmentation dataset. The person counting module automatically allocate an appropriate model to the image based on its scenario label. Specifically, for (i)-(iv), we apply a fine-tuned YOLOv5 model, named YOLOv5(i) to YOLOv5(iv), to detect and count persons in the image. For (v), we adopt the DM-Count model to estimate the density map, and output the final count prediction by summing up all the pixel values in the map.

% Based on the observation that fall is highly correlated to the relative position between body keypoints, we exploited a keypoints vectorization method to extract salient features from coordinate representation. The transformed sequence of the $i$-th person was denoted $\overline{\mathbf{K}^i_{obs}}=\left(\overline{\mathbf{k}^i_1}, \overline{\mathbf{k}^i_2}, ..., \overline{\mathbf{k}^i_{t_{\text{obs}}}} \right)$. Then, the pose prediction module adapted from seq2seq architecture~\cite{SEQ2SEQ} was used to predict body poses in next $t_{\text{pred}}$ frames. Considering that applying excessive LSTM units made the network hard to converge, we encoded several consecutive keypoints vectors in one LSTM unit to shorten the lengths of both encoder and decoder LSTM layers. Moreover, shorter sequences also suppressed the mean-pose problem caused by long-term prediction~\cite{IJCAI18,CVPR18}.

% After that, $\overline{\mathbf{k}^i_{t_{\text{obs}}+t_{\text{pred}}}}$, the future keypoints vector of the $i$-th person at frame $f_{t_{\text{obs}} + t_{\text{pred}}}$, was predicted and used for classification. The falls classifier adopted fully connected network and was trained on re-annotated Le2i~\cite{LE2I} dataset, in which each frame was labeled either `fall' or `no fall'. Combining the pose prediction module and falls classifier, our model was capable of predicting falls in advance.

\subsection{Scenario Classifier}\label{subsec:scenecls}

% The keypoints coordinates extracted by OpenPose~\cite{OPENPOSE} cannot reflect the correlation between different keypoints, and suffered from the effects of body skeleton's absolute position and scale. With the motivation that the same pose should be represented by the same keypoints vector, we exploited the following keypoints vectorization method.

The scenario classifier is a fundamental ResNet-50~\cite{ResNet} network. The input layer accepts an image with size of $224\times224\times3$, followed by a $7\times7\times64$ convolution layer, a $3\times3$ max pooling layer and four groups of bottleneck residual blocks. The first block contains three repeated units composed by three convolution layers with kernel size of $1\times1\times64$, $3\times3\times64$ and $1\times1\times256$. The second block contains four repeated units composed by three convolution layers with kernel size of $1\times1\times128$, $3\times3\times128$ and $1\times1\times512$. The third block contains six repeated units composed by three convolution layers with kernel size of $1\times1\times256$, $3\times3\times256$ and $1\times1\times1024$. The fourth block contains three repeated units composed by three convolution layers with kernel size of $1\times1\times512$, $3\times3\times512$ and $1\times1\times2048$. After all the convolution layers, an average pooling layer, a 5d fully connection layer and a softmax layer are applied to predict the scenario category.

% As we know, the 18 keypoints of MS COCO~\cite{MSCOCO} are nose, neck, left and right shoulders, elbows, wrists, hips, knees, ankles, eyes and ears. Since we focused on the body keypoints, 5 face keypoints (\ie nose, left and right eyes and ears) were ignored. For 13 concerned ones, we transformed their coordinates to vectors connecting with corresponding adjacent keypoints. As illustrated in Fig.~\ref{fig:kptvec}, the left and right shoulders are connected to the neck, the left/right elbow is connected to left/right shoulder, and the left/right wrist is connected to left/right elbow. Similarly, the left and right hips are connected to the neck, the left/right knee is connected to left/right hip, and the left/right ankle is connected to left/right knee. As a result, 12 keypoints vectors were constructed from the coordinates information. Finally, we normalized all the vectors to unit length.

% Formally, recall that the observed keypoints sequence of the $i$-th person was $\mathbf{K}^i_{obs}=\left(\mathbf{k}^i_1, \mathbf{k}^i_2, ..., \mathbf{k}^i_{t_{\text{obs}}} \right)$, where
% \begin{equation}\label{eq:transk}
% \begin{aligned}
%   \mathbf{k}^i_j=\left(x^i_{j,1}, y^i_{j,1}, x^i_{j,2}, y^i_{j,2}, ..., x^i_{j,18}, y^i_{j,18} \right).
% \end{aligned}
% \end{equation}
% $(x^i_{j,m}, y^i_{j,m})$ denoted the $m$-th keypoint coordinate of the $i$-th person in frame $j$. For the $p$-th connection pointing from the $l$-th keypoint to the $r$-th keypoint, the keypoints vector $\left(\overline{x^i_{j,p}}, \overline{y^i_{j,p}} \right)$ was calculated by:
% \begin{equation}\label{eq:transfv}
% \begin{aligned}
%   \left( {\overline {x_{j,p}^i} ,\overline {y_{j,p}^i} } \right) = {\textstyle{{\left( {x_{j,r}^i - x_{j,l}^i,y_{j,r}^i - y_{j,l}^i} \right)} \over {\sqrt {{{\left( {x_{j,r}^i - x_{j,l}^i} \right)}^2} + {{\left( {y_{j,r}^i - y_{j,l}^i} \right)}^2}} }}}.
% \end{aligned}
% \end{equation}
% After 12 keypoints vectors were constructed, they were then concatenated to form  $\overline{\mathbf{k}^i_j}$ as follows:
% \begin{equation}\label{eq:transkv}
% \begin{aligned}
%   \overline{\mathbf{k}^i_j}=\left(\overline{x^i_{j,1}}, \overline{y^i_{j,1}}, \overline{x^i_{j,2}}, \overline{y^i_{j,2}}, ..., \overline{x^i_{j,12}}, \overline{y^i_{j,12}} \right).
% \end{aligned}
% \end{equation}

% The transformation from $\mathbf{k}^i_j$ to $\overline{\mathbf{k}^i_j}$ eliminates the absolute position and scale of body skeleton, and preserves direction information between adjacent keypoints. It not only ensures that the same body pose has the same representation, but also extracts salient features for better falls classification.

% \begin{figure}[tbp]%
% \centering
% \includegraphics[width=0.7\linewidth]{kpt_vec.png}
% \caption{The illustration of the keypoints vectorization method. The arrows indicate 12 vectors constructed from the coordinates of 13 body keypoints. All the vectors are normalized to unit length with only direction information remains.}
% \label{fig:kptvec}
% \end{figure}

\subsection{Person Counting Module}\label{subsec:pcm}

% The authors in~\cite{SEQ2SEQ} proposed a seq2seq network, which was applied to machine translation at first and achieved excellent performance. Later, they introduced this architecture to conversational modeling in~\cite{CONVER}. In analogy to mapping a sentence from one language to another in machine translation, the conversational model maps a query sentence to a response sentence. Generally, the seq2seq framework uses an LSTM~\cite{LSTM} layer to encode the input sentence to a vector of a fixed dimensionality, and then another LSTM layer to decode the target sentence from the vector. This encoder-decoder architecture is widely used in sequence mapping problems such as machine translation~\cite{SEQ2SEQ}, conversation modeling~\cite{CONVER} and even video caption~\cite{VID2TEXT} because of its powerful capabilities.

The person counting module includes five models that are automatically executed based on the scenario label of the input image. These models are fine-tuned YOLOv5 and DM-Count models, each enhanced for a specific scenario.

Four YOLOv5 models, referred to as YOLOv5(i) to YOLOv5(iv), are used in scenarios (i)-(iv) to detect and count the number of persons based on their bodies or heads. The YOLOv5 model consists of a backbone to aggregate and form image features at different granularity levels, a neck to mix and combine image features, and a head to consume features from the neck and predict the objectness score, class probability, and bounding box coordinates for each anchor box at multiple scales and aspect ratios. The anchor boxes are pre-defined shapes and sizes that cover different parts of the image.

% Inspired by the great success of seq2seq network in the sequence mapping problems, we implemented a seq2seq-based pose prediction module to predict body poses in the future $t_{\text{pred}}$ frames. Formally, recall that the observed keypoints vector sequence of the $i$-th person was $\overline{\mathbf{K}^i_{obs}}=\left(\overline{\mathbf{k}^i_1}, \overline{\mathbf{k}^i_2}, ..., \overline{\mathbf{k}^i_{t_{\text{obs}}}} \right)$. The pose prediction module was designed to generate future keypoints vector sequence $\overline{\mathbf{K}^i_{pred}}=\left(\overline{\mathbf{k}^i_{t_{\text{obs}} + 1}}, \overline{\mathbf{k}^i_{t_{\text{obs}} + 2}}, ..., \overline{\mathbf{k}^i_{t_{\text{obs}} + t_{\text{pred}}}} \right)$.

For scenario (v), the DM-Count is used to estimate the density map for person counting. It adopts VGG-19~\cite{VGG} as the backbone network and employs OT and TV loss instead of traditional Gaussian smoothing operations to avoid hurting the realness of the ground truth. Specifically, the OT loss measures the similarity between the predicted and ground truth density maps, and the TV loss is added to further enhance the smoothness of the predicted density map.

% \begin{figure*}[htbp]%
% \centering
% \includegraphics[width=1\linewidth]{seq2seq_arch.png}
% \caption{The architecture of our seq2seq-based pose prediction module, which is composed of an encoder (colored green), and a decoder (colored blue). Each LSTM unit in the encoder parses an observed keypoints vector (visualized in the figure to present more intuitive result) and produces a hidden vector. The first unit in the decoder accepts the last hidden vector from the encoder and generates the first prediction. Latter units receive the previous prediction and produce a new one. Note that the vector packing is reflected in the figure.}
% \label{fig:seqarch}
% \end{figure*}

% As illustrated in Fig.~\ref{fig:seqarch}, the pose prediction module is composed of two LSTM~\cite{LSTM} layers as the encoder and decoder respectively. The encoder analyzes the sequence of observed keypoints vectors with each LSTM unit parsing one keypoints vector. The hidden vector calculated by the previous unit is passed to the next one. In the decoder, the keypoints vector is generated one at a step. The first decoder LSTM unit accepts the last hidden vector from the encoder and outputs the first prediction. Latter units take the previous outcome as input and produce a new one. As all the LSTM units in the decoder complete predictions, the output keypoints sequence will be generated.

% Recall the mechanism of LSTM layer: Assume that the input sequence is $\left( {{\mathbf{x}}_{1}},{{\mathbf{x}}_{2}},...,{{\mathbf{x}}_{m}} \right)$, the $t$-th LSTM unit updates the states based on the states at $t-1$:
% \begin{eqnarray}\label{eq:state}
% \begin{aligned}
% & {{\mathbf{i}}_{t}} = \sigma \left( {{\mathbf{W}}_{i}}\left[ {{\mathbf{h}}_{t-1}},{{\mathbf{x}}_{t}} \right]+{{\mathbf{b}}_{i}} \right) \\
% & {{\mathbf{f}}_{t}} = \sigma \left( {{\mathbf{W}}_{f}}\left[ {{\mathbf{h}}_{t-1}},{{\mathbf{x}}_{t}} \right]+{{\mathbf{b}}_{f}} \right) \\
% & {{\mathbf{o}}_{t}} = \sigma \left( {{\mathbf{W}}_{o}}\left[ {{\mathbf{h}}_{t-1}},{{\mathbf{x}}_{t}} \right]+{{\mathbf{b}}_{o}} \right) \\
% & \widetilde{{{\mathbf{c}}_{t}}} = \tanh \left( {{\mathbf{W}}_{c}}\left[ {{\mathbf{h}}_{t-1}},{{\mathbf{x}}_{t}} \right]+{{\mathbf{b}}_{c}} \right) \\
% & {{\mathbf{c}}_{t}} = {{\mathbf{f}}_{t}}\odot {{\mathbf{c}}_{t-1}}+{{\mathbf{i}}_{t}}\odot \widetilde{{{\mathbf{c}}_{t}}} \\
% & {{\mathbf{h}}_{t}} = {{\mathbf{o}}_{t}}\odot \tanh \left( {{\mathbf{c}}_{t}} \right),
% \end{aligned}
% \end{eqnarray}
% where $\sigma$ denotes the sigmoid function, $\tanh$ is the hyperbolic tangent function, $\odot$ denotes the element-wise multiplication, ${{\mathbf{i}}_{t}}$, ${{\mathbf{f}}_{t}}$, ${{\mathbf{o}}_{t}}$, ${{\mathbf{c}}_{t}}$ and ${{\mathbf{h}}_{t}}$ represent input gate, forget gate, output gate, cell state and hidden state of the $t$-th LSTM unit respectively. $\mathbf{W}$ and $\mathbf{b}$ are trainable weights.

% We attempted to apply $t_{\text{obs}}$ LSTM units to the encoder and $t_{\text{pred}}$ LSTM units to the decoder (\ie, each LSTM unit only deals with a single keypoints vector). However, we found that as the increment of $t_{\text{obs}}$ and $t_{\text{pred}}$, the network was getting harder to converge. To mitigate the negative effect of long LSTM layer, we packed every $n_p$ consecutive keypoints vectors in one for decreasing the length of keypoints sequence and the number of LSTM units. Formally, the packed sequence of $\overline{\mathbf{K}^i_{obs}}=\left(\overline{\mathbf{k}^i_1}, \overline{\mathbf{k}^i_2}, ..., \overline{\mathbf{k}^i_{t_{\text{obs}}}} \right)$ was $\widetilde{\mathbf{K}^i_{obs}}=\left(\widetilde{\mathbf{k}^i_1}, \widetilde{\mathbf{k}^i_2}, ..., \widetilde{\mathbf{k}^i_{t_{\text{obs}}/n_p}} \right)$, where
% \begin{equation}\label{eq:concat}
% \begin{aligned}
%   \widetilde{\mathbf{k}^i_j}=\overline{\mathbf{k}^i_{n_p(j-1)+1}}\oplus \overline{\mathbf{k}^i_{n_p(j-1)+2}}\oplus ... \oplus \overline{\mathbf{k}^i_{n_pj}},
% \end{aligned}
% \end{equation}
% where $\oplus$ denotes the concatenation of two vectors. If there are no enough keypoints vectors for the last package (it will always happen when $n_p$ is not divisible by $t_{\text{obs}}$), zero-paddings are filled to its tail for the dimensional equality. Correspondingly, since the vectors in output sequence are also packed, they need to be unpacked before classification to obtain the future pose at each frame.

% Benefiting from the vector packing technique, the pose prediction module required less time to get convergent in the training phase, and the mean-pose problem raised in~\cite{IJCAI18,CVPR18} was also suppressed.

% \subsection{Falls Classifier}\label{subsec:cls}

% The falls classifier was trained to perform inference on the future keypoints vector at frame $f_{t_{\text{obs}} + t_{\text{pred}}}$. Considering that $\overline{\mathbf{k}^i_{t_{\text{obs}}+t_{\text{pred}}}}$ only contained 24 features, we simply adopted a traditional fully connected neural network for classification. The input layer was embedded with 24 neurons to fit the dimensionality of the input vector. And we set up five hidden layers containing 96, 192, 192, 96, 24 neurons respectively. The output layer with 2 neurons was used to give the final prediction: `fall' or `no fall'.

%\begin{equation}\label{eq:a2}
%\begin{aligned}
%  {{a}_{2}}=f({{k}_{2}}\cdot \min (w,h))
%\end{aligned}
%\end{equation}
%\begin{equation}\label{eq:a3}
%\begin{aligned}
%  {{a}_{3}}=f({{k}_{3}}\cdot \min (w,h))
%\end{aligned}
%\end{equation}


%\begin{figure}[htbp]%
%\centering
%\includegraphics[width=1\linewidth]{figs/fig_5.png}
%\caption{Morphological processing on binarized semantic segmentation result.}
%\label{fig:morlo}
%\end{figure}
%\begin{figure} [htbp]
%  \centering
%  \subfigure[]{
%    \centering
%    \label{fig:morlo:a} %% label for first subfigure
%    \begin{minipage}[b]{0.23\textwidth}%\textwidth %0.46  %\linewidth
%      \centering
%      \includegraphics[width=1\linewidth]{figs/fig_5_a.png}%0.2
%    \end{minipage}}
%  \subfigure[]{
%    \centering
%    \label{fig:morlo:b}
%    \begin{minipage}[b]{0.23\textwidth}
%      \centering
%      \includegraphics[width=1\linewidth]{figs/fig_5_b.png}
%    \end{minipage}}
%  \caption{Morphological processing results from binarized semantic segmentation maps. (a) indoor scenario. (b) outdoor scenario.}
%  \label{fig:morlo}
%\end{figure}


\section{Experiments}\label{sec:exp}

\subsection{Dataset Overview}\label{subsec:data}

% We trained and evaluated our model on Le2i falls detection dataset~\cite{LE2I}, which consists of 191 videos captured under four different scenes: home, coffee room, office and lecture room. The frame rate is 25 frames per second and the resolution is 320$\times$240 pixels. In each video, an actor performs various of normal activities and falls (fall might be absent in several videos). The official annotations provide the falling-start frame stamp and the falling-end frame stamp for each video. If there is no fall in a video, both frame stamps will be marked as 0 in its annotation file.

To train and validate our five person counting models and scenario classifier, we constructed five augmentation datasets, which were also integrated to form the scenario classification dataset. Each augmentation dataset was collected from a specific scenario as defined in Section~\ref{sec:intro}. The side-view dataset was mainly collected from public streets, parks, and offices \etc, captured by cameras at a height of 3-5 meters, with each person annotated with a body bounding box. The long-shot dataset was collected from cameras placed at far distances from the subjects, such as surveillance cameras on highways, ferries, and squares \etc, with each person also annotated with a body bounding box. The top-view dataset was collected from various sources, including a public overhead person detection dataset~\cite{TopviewData}, with the annotations being head bounding boxes. The protective suit dataset, as an example of customized scenario, was mostly collected from hospitals, health centers, and medical waste rooms, with the dataset characterized by data augmentation on people wearing protective suits and annotated with body bounding boxes. Lastly, the crowd counting dataset was randomly selected from ShanghaiTech~\cite{ShanghaiTech}, UCF-CC50~\cite{UCFCC50}, UCF-QNRF~\cite{UCFQNRF} and NWPU~\cite{NWPU}. with each person marked with a dot on the head. For the scenario classification dataset, we combined all the images from the five augmentation datasets and labeled each image with a tag indicating the corresponding scenario. Additionally, we inferred the number of persons in each image from the raw annotations, and calculated the maximum, minimum and average values for each augmentation dataset and the integrated dataset. The statistics are presented in Table~\ref{tb:statis}.

\begin{table}[tbp]
\center
  \caption{The statistics of five augmentation datasets and the integrated dataset. The columns from left to right show the dataset name, data scale, and the maximum, minimum, and average number of persons respectively.}
  \label{tb:statis}
\begin{center}
\begin{tabular}{c|c|c|c|c}
\hline
Dataset & Scale & Max. & Min. & Avg.\\
\hline
Side-View & 5648 & 70 & 0 & 6.6\\
Long-Shot & 5305 & 16 & 0 & 2.7\\
Top-View & 4632 & 18 & 0 & 3.3\\
Protective-Suit & 5904 & 6 & 0 & 1.8\\
Crowd & 4834 & 12865 & 34 & 743.8\\
Integrated & 26323 & 12865 & 0 & 138.1\\
\hline
\end{tabular}
\end{center}
\end{table}

% For the requirement of tagging a label for each frame, we first looked through all the videos and manually annotated an extra getting-up frame stamp for each one. If there is no fall appears, this value will be set to 0. And if the actor does not get up until the video ends, this value will be set to the last frame. Suppose that the frame stamps of falling start, falling end and getting up are denoted $S_{fs}$, $S_{fe}$, $S_{gu}$ respectively, we attempted three automatic frame-annotation principles in our experiments:
% \begin{enumerate}
%   \item Frames between $S_{fs}$ and $S_{fe}$ are labeled `fall';
%   \item Frames between $S_{fs}$ and $S_{gu}$ are labeled `fall';
%   \item Frames between $S_{fe}$ and $S_{gu}$ are labeled `fall'.
% \end{enumerate}
% And all the excluded frames are labeled `no fall'. The first principle only regarded the falling proceeding as fall, which means `no fall' will be annotated to a fallen person. Under this principle, the trained falls classifier could not recognize falls normally. The second one resulted in a high false positive rate because many `precursors' of falling event will be predicted as `fall' even when they are not leading to a real fall. The third principle achieved great balance by labeling `fall' after the actor was already in the fallen state. So we finally adopted this principle for frame annotations.

To train the person counting models, each augmentation dataset is randomly divided into a training set and a validation set at a ratio of 8:2. As for the training of the scenario classifier, the division of the scenario classification dataset keeps consistent with that of the augmentation datasets for the convenience of joint network evaluations, which will be explained in Section~\ref{subsec:eval}.

\begin{table}[tp]
\center
  \caption{The evaluation of the scenario classifier. The one-vs-rest strategy is used to calculate precision, recall, F1-score and support for each class, as well as the macro-average across all classes.}
  \label{tb:cls_eval}
\begin{center}
\begin{tabular}{c|c|c|c|c}
\hline
Class & Prec. & Rec. & F1-score & Support\\
\hline
Side-View & 0.70 & 0.75 & 0.72 & 1130\\
Long-Shot & 0.87 & 0.82 & 0.85 & 1061\\
Top-View & 0.93 & 0.98 & 0.95 & 926\\
Protective-Suit & 0.73 & 0.70 & 0.71 & 1181\\
Crowd & 0.87 & 0.86 & 0.87 & 967\\
Macro-Average & 0.81 & 0.81 & 0.81 & 5265\\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{figure}[tp]%
\centering
\includegraphics[width=1\linewidth]{confusion_matrix.png}
\caption{The confusion matrix of scenario classifier evaluation. Each grid in the matrix corresponds to a combination of ground truth and prediction, with the horizontal and vertical axes representing the ground truth and prediction respectively.}
\label{fig:cm}
\end{figure}

\subsection{Experiments Setup}

\begin{table*}[htbp]
\center
  \caption{Comparisons on the networks with and without the scenario classifier. The first five rows report the results of fixed person counting models without using the scenario classifier, while the last row shows the performance of the model automatically selected by the scenario classifier. The experiments are conducted on five separate datasets and the integrated dataset, using MAE and RMSE as evaluation metrics.}
  \label{tb:cross_eval}
\begin{center}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{Model} & \multicolumn{2}{c|}{Side-View} & \multicolumn{2}{c|}{Long-Shot} & \multicolumn{2}{c|}{Top-View} & \multicolumn{2}{c|}{Protective-Suit} & \multicolumn{2}{c|}{Crowd} & \multicolumn{2}{c}{Integrated}\\  \cline{2-13}
                       & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE\\
\hline
YOLOv5(i) & \textbf{0.39} & \textbf{0.94} & 1.02 & 1.44 & 1.24 & 1.57 & 0.76 & 1.02 & 613.6 & 721.3 & 113.4 & 309.1\\
YOLOv5(ii) & 0.68 & 1.13 & \textbf{0.36} & \textbf{0.80} & 1.21 & 1.48 & 0.91 & 1.12 & 583.0 & 622.5 & 107.7 & 266.8\\
YOLOv5(iii) & 1.88 & 2.31 & 1.38 & 1.61 & \textbf{0.44} & \textbf{0.89} & 1.42 & 1.69 & 624.7 & 736.4 & 115.8 & 315.6\\
YOLOv5(iv) & 0.44 & 0.91 & 1.09 & 1.49 & 1.26 & 1.57 & \textbf{0.28} & \textbf{0.69} & 616.2 & 725.8 & 113.8 & 311.1\\
DM-Count & 7.56 & 10.37 & 13.23 & 18.63 & 6.45 & 8.77 & 6.84 & 10.1 & \textbf{96.4} & \textbf{153.8} & 24.7 & \textbf{66.9}\\
\textbf{Automatic} & 0.41 & 0.93 & 0.58 & 1.01 & 0.66 & 1.23 & 0.54 & 0.92 & 108.6 & 168.1 & \textbf{20.4} & 72.0\\
\hline
\end{tabular}
\end{center}
\end{table*}

% We implemented our model on a workstation with double Nvidia 1080Ti GPUs. The seq2seq-based pose prediction module and the falls classifier were trained separately and tuned jointly.

The experiment projects are implemented on a workstation equipped with double Nvidia 3080Ti GPUs. The scenario classifier and five person counting models are trained separately. For the scenario classifier, we utilize cross-entropy as the loss function and Adam~\cite{ADAM} as the optimizer, with a learning rate of 0.001. The person counting module is built on YOLOv5 and DM-Count. During the training phase of YOLOv5, the loss function is a combination of bounding box regression loss, objectness loss and classification loss, and is optimized using stochastic gradient descent (SGD) with an initial learning rate of 0.01 and momentum of 0.937. To train DM-Count, we use Adam with a learning rate of $1\times 10^{-5}$ to optimize the overall loss function, which combined counting loss, OT loss and TV loss.

% To train the pose prediction module, we preprocessed all videos in the Le2i dataset. For each video, we utilized OpenPose to extract the actor's keypoints coordinates frame by frame, and transformed them to keypoints vectors with the method proposed in Section~\ref{subsec:vector}. However, due to the effect of illuminance or camera perspective \etc, the actor's keypoints might be partially or completely missed. We dealt with the partial missing by setting vectors connecting undetected keypoint to $(0, 0)$ in the vectorization step. For the complete missing, the corresponding frames would be discarded. To ensure the coherence of keypoints sequence, consecutive 10 discarded frames would break the sequence into two segments. Then we filtered out the sequences containing less than 10 frames and finally obtained 139 sequences. The maximum, minimum and average frames of these sequences are 1773, 13 and 241.26 respectively.

% During the training phase, the network loaded all the processed keypoints sequences and acquired training samples according to the configurations of $t_{\text{obs}}$ and $t_{\text{pred}}$. For each sequence, all sub-sequences with length of $t_{\text{obs}} + t_{\text{pred}}$ frames were segmented and used as valid training samples. The former $t_{\text{obs}}$ frames were input to the encoder of the pose prediction module, and the latter $t_{\text{pred}}$ ones were regarded as ground truth. After the network completed inference, the mean square error (MSE) was applied to calculate the loss between ground truth and the predicted sequence. And the network was optimized using Adam~\cite{ADAM} algorithm. In our experiments, the learning rate was set to 0.001.

During the inference phase of YOLOv5, all the generated proposals are first filtered by non-maximum suppression (NMS) with intersection over union (IoU) threshold of 0.45 and confidence threshold of 0.6. The model then outputs the number of reserved bounding boxes as the final prediction. For DM-Count, the model produces a density map, and the sum of all the pixel values in the map is rounded to estimate the number of persons in the image.

% To evaluate performance of the pose prediction module trained with different selections of $t_{\text{obs}}$, $t_{\text{pred}}$ and $n_p$ mentioned in Section~\ref{subsec:ppm}, we exploited mean cosine similarity (MCS) as the metrics. Specifically, suppose that there are $m$ test samples, the ground truth and the predicted sequence of the $i$-th sample are $\mathbf{g}^i$ and $\mathbf{p}^i$ respectively. Note that $\mathbf{g}^i$ and $\mathbf{p}^i$ both contain $t_{\text{pred}}$ keypoints vectors ($\mathbf{p}^i$ has been unpacked). The $j$-th vector of $\mathbf{g}^i$ and $\mathbf{p}^i$ are denoted $\mathbf{g}^i_j$ and $\mathbf{p}^i_j$. Then the MCS was calculated by:
% \begin{equation}\label{eq:mcs}
% \begin{aligned}
% 	{\text{MCS}} = \frac{1}{m}\sum\limits_{i = 1}^m {{{C}^i}},
% \end{aligned}
% \end{equation}
% where
% \begin{equation}\label{eq:ci}
% \begin{aligned}
% 	{{C}^i} = \frac{1}{{{t_{{\text{pred}}}}}}\sum\limits_{j = 1}^{{t_{{\text{pred}}}}} {\frac{{{\mathbf{g}}_j^i \cdot {\mathbf{p}}_j^i}}{{\left\| {{\mathbf{g}}_j^i} \right\|\left\| {{\mathbf{p}}_j^i} \right\|}}}.
% \end{aligned}
% \end{equation}

% \begin{figure}[tbp]%
% \centering
% \includegraphics[width=1\linewidth]{seq2seq_param.png}
% \caption{The MCS comparisons among several parameter configurations. We first selected five combinations of $t_{\text{obs}}$ and $t_{\text{pred}}$, which is denoted by ($t_{\text{obs}}$, $t_{\text{pred}}$) in the figure, then trained and evaluated the pose prediction module with different $n_p$.}
% \label{fig:seqparam}
% \end{figure}

% We tried several configurations of $t_{\text{obs}}$, $t_{\text{pred}}$ and $n_p$ to see the effect to the pose prediction module. It can be seen from Fig.~\ref{fig:seqparam} that observing 10 frames was insufficient for the network to give accurate predictions, especially for long predictions. Using 25 frames to predict 25 frames and using 50 frames to predict 50 frames both resulted in high MCS. Generally, the network was capable of predicting future 50 frames based on 25 observed frames with a tolerable MCS decrease. And $n_p = 5$ showed the best performance among all the candidate values. Hoping that the pose prediction module could predict further future poses with acceptable performance for earlier falls warning, we adopted $t_{\text{obs}} = 25$, $t_{\text{pred}} = 50$ and $n_p = 5$ in the deployment.

% With respect to the training of falls classifier described in~\ref{subsec:cls}, we mapped each keypoints vector to the label of corresponding frame. Samples with less than 8 detected body keypoints were discarded because they contained too many null features. (In the inference phase, these samples would be prejudged as `unknown' before the classification.) All the valid vector-label pairs were used to train the network and cross-entropy function was adopted to calculate the loss. Adam~\cite{ADAM} was still selected as the optimizer.

\subsection{Evaluations}\label{subsec:eval}

The evaluations are designed to consider two aspects: the performance of the trained scenario classifier, and the effect of the scenario classifier on the overall network.

To evaluate the performance of our trained scenario classifier, we input all 5265 validation samples into the model and analyze the predictions. We adopt the one-vs-rest strategy to convert our multi-class problem into a series of binary ones. For each class, all the remaining classes are treated as the negative class, enabling us to use binary classification metrics such as precision, recall, and F1-score in a multi-class problem. Given the relatively balanced class distribution in our dataset, we use the macro average (the arithmetic mean of all metrics across classes) to reflect the performance of the scenario classifier on the entire validation dataset. The evaluation result and the confusion matrix are presented in Table~\ref{tb:cls_eval} and Fig.~\ref{fig:cm}. It is observed that the scenario classifier produces generally fair predictions. However, some scenarios, such as side-view and protective-suit, as well as long-shot and crowd, are more frequently mislabeled.

Regarding the effect of the scenario classifier, we compare the networks with and without the scenario classifier. When using the scenario classifier, the person counting model is automatically selected based on the scenario classification result. Conversely, when not using the scenario classifier, the person counting model is fixed to one of the five person counting models. The evaluations are performed on the five augmentation datasets and the integrated dataset. We adopt MAE and RMSE to measure the difference between ground truth and the model's prediction. Recall that MAE and RMSE are calculated as follows:

\begin{equation}\label{eq:metrics}
\begin{aligned}
\begin{array}{l}
    {\text{MAE}} = \frac{1}{n}\sum_{i=1}^{n}\left | {y}_{i}-\hat{y}_{i}  \right |\\
    \\
    {\text{RMSE}} = \sqrt{\frac{1}{n}\sum_{i=1}^{n} \left ( {y}_{i}-\hat{y}_{i} \right )^{2} }
\end{array}
\end{aligned}
\end{equation}
where $n$ is the number of validation samples, ${y}_{i}$ and $\hat{y}_{i}$ are the ground truth and prediction of the $i$-th sample respectively. As shown in Table~\ref{tb:cross_eval}, each person counting model performs best on a specific scenario but produces higher deviation on other datasets. In comparison, with automatic model selection by the scenario classifier, our network achieves a great balance on the integrated dataset.

% Evaluations are designed concerning about the following questions: 1) Whether our falls classifier utilizing body keypoints information outperforms mainstream raw RGB based models. 2) What effects will the pose prediction module bring to the accuracy of falls classification.
%\begin{enumerate}
%  \item Whether our falls classifier utilizing body keypoints information outperforms raw RGB based models.
%  \item What effects will the pose prediction module bring to the accuracy of falls classification.
%\end{enumerate}

% \subsubsection{Body Keypoints \vs Raw RGB}
% We conducted comparisons between our falls classifier and popular raw RGB-based HAR models including C3D~\cite{C3D}, Two-Stream~\cite{TWOSTREAM} and TSN~\cite{TSN} \etc. However, considering that these models just predict one label to a complete video rather than a single frame, we converted the annotations from frame-label pairs to clip-label pairs.

% The main idea was to segment 3-second (\ie 75 frames) clips from the original video and labeled `fall' to those including the whole falling proceeding and `no fall' to those not involving falls at all. The choice of 3-second length is based on the statistic that the average duration of falling proceeding among all the videos is 1.26 seconds, which ensures the acquirement of positive samples. Formally, recall that $S_{fs}$, $S_{fe}$ and $S_{gu}$ are three frame stamps that mark the falling start, falling end and getting up respectively. Suppose a clip is segmented from the video's $S_l$-th frame to $S_r$-th frame ($S_r-S_l=75$), its annotation is decided by:
% \begin{equation}\label{eq:slsr}
% \begin{aligned}
% \begin{array}{l}
% {S_l} \le {S_{fs}}\text{ and } S_{fe} \le {S_r} \le {S_{gu}}\quad\text{labeled `fall'}\\
% {S_r} \le {S_{fs}}\,\text{ or }\;  {S_l} \ge {S_{gu}}\qquad\;\;\:\text{labeled `no fall'}
% \end{array}
% \end{aligned}
% \end{equation}

% To avoid ambiguities, clips with partial falling proceeding were excluded. We finally obtained 9549 samples, which were randomly divided into a training set and a test set with the ratio of 7:3. We finetuned C3D (3 nets), Two-Stream and TSN (2 modalities) on the training set and evaluated them on the test set. For each clip, our falls classifier directly classified the keypoints vector from the frame $S_r$. The limitation of ${S_r} \le {S_{gu}}$ in Eq.~\eqref{eq:slsr} ensured the label consistency between each clip and its last frame.

% \begin{table}[bp]
% \center
%   \caption{Comparisons between mainstream RGB-based models and our keypoints-based model on falls classification problem}
%   \label{tb:comprgb}
% \begin{center}
% \begin{tabular}{l|c|c|c|c}
% \hline
% \qquad Model & Acc. & Prec. & Rec. & F1\\
% \hline
% C3D~\cite{C3D} & 89.4\% & 66.1\% & 87.9\% & 0.755\\
% Two-Stream~\cite{TWOSTREAM} & 91.6\% & 71.6\% & 91.0\% & 0.801\\
% TSN~\cite{TSN} & 94.6\% & 79.8\% & 94.5\% & 0.866\\
% Ours & \textbf{97.8\%} & \textbf{90.8\%} & \textbf{98.3\%} & \textbf{0.944}\\
% \hline
% \end{tabular}
% \end{center}
% \end{table}

% The test set consists of 2865 samples, including 531 positive samples (fall) and 2334 negative samples (no fall). We adopted accuracy, precision, recall and F1-score to evaluate each model. As illustrated in Table~\ref{tb:comprgb}, our keypoints-based classifier showed better performance than mainstream raw RGB-based models, which proved the salience of body keypoints features for falls classification problem.

\begin{figure*}[tbp]%
\centering
\includegraphics[width=1\linewidth]{result.png}
\caption{Visualization results of our experiments. The input images are picked from five augmentation datasets and each image is processed by all the five person counting models. For YOLOv5 models, we render the detected body/head bounding boxes on the image. For DM-Count model, we overlay the generated heatmap on the original image. And the counting result is remarked on each output image. The last column indicates the actual and the predicted scenario labels of the image, along with the ground truth and final prediction of person counting for each image. In case (a)-(i), the scenario classifier successfully allocates the input image to the correct person counting model, while in case (j), the input image is mislabeled thus passed to a model that is not the most appropriate.}
\label{fig:result}
\end{figure*}

% \subsubsection{Effects of the Pose Prediction Module}

% The pose prediction module was evaluated by self comparisons with this module enabled (denoted as ${Model}_\text{pred+cls}$) and disabled (denoted as ${Model}_\text{cls}$).

% The first experiment was designed to compare the classification results between predicted keypoints and directly observed ones. Specifically, while predicting the label of the $i$-th frame in a video, ${Model}_\text{pred+cls}$ utilized frames from $i-74$ to $i-50$ (25 frames) to predict the keypoints vectors of frames from $i-49$ to $i$ (50 frames). Then the predicted keypoints vector of frame $i$ was input to falls classifier to produce a label. As for ${Model}_\text{cls}$, it was directly given the keypoints vector of the $i$-th frame for classification.

% \begin{table}[tbp]
% \center
%   \caption{Comparison between ${Model}_\text{cls}$ and ${Model}_\text{pred+cls}$}
%   \label{tb:compred1}
% \begin{center}
% \begin{tabular}{l|c|c|c|c}
% \hline
% \qquad Model & Acc. & Prec. & Rec. & F1\\
% \hline
% ${Model}_\text{cls}$ & \textbf{99.2\%} & \textbf{95.1\%} & \textbf{98.8\%} & \textbf{0.970}\\
% ${Model}_\text{pred+cls}$ & 98.7\% & 92.6\% & 98.0\% & 0.952\\
% \hline
% \end{tabular}
% \end{center}
% \end{table}

% The comparison result is presented in Table~\ref{tb:compred1}. In accordance with expectations, the pose prediction module brought a reduction to the accuracy of falls classification.

% However, ${Model}_\text{pred+cls}$ is capable of predicting falls in advance. For a fairer comparison, we conducted another experiment to enable ${Model}_\text{cls}$ to `foresee' the falls by early annotation. Specifically, the labels of the falling proceeding (\ie, from frame $S_{fs}$ to $S_{fe}$) were modified to `fall' in the training of ${Model}_\text{cls}$. As shown in Table~\ref{tb:compred2}, compared to the pose prediction method, early annotation improves the recall at the cost of an obvious drop on the precision, which is caused by the significant increase of false positive.

% \begin{table}[tbp]
% \center
%   \caption{Comparison between ${Model}_\text{cls}$ and ${Model}_\text{pred+cls}$ with modified annotations}
%   \label{tb:compred2}
% \begin{center}
% \begin{tabular}{l|c|c|c|c}
% \hline
% \qquad Model & Acc. & Prec. & Rec. & F1\\
% \hline
% ${Model}_\text{cls}$ & 98.1\% & 87.7\% & \textbf{99.7\%} & 0.933\\
% ${Model}_\text{pred+cls}$ & \textbf{98.7\%} & \textbf{92.6\%} & 98.0\% & \textbf{0.952}\\
% \hline
% \end{tabular}
% \end{center}
% \end{table}

\subsection{Visualization Results}

% We present several experimental results in Fig.~\ref{fig:result}. In most scenarios, our model can correctly predict the falling event in advance. However, (d) and (e) show two failure cases caused by different reasons. In (d), many keypoints are undetected by OpenPose due to the invisibility of the person's upper body under that camera perspective. As a consequence, the pose prediction module produces absurd keypoints sequence providing insufficient features for classification. (Recall that vectors with less than 8 detected keypoints will be prejudged as `unknown' before the falls classifier). While in (e), no `precursor' of fall is discovered in the observed frame sequence, which is beyond the ability of the pose prediction module to predict the future falls.

As illustrated in Fig.~\ref{fig:result}, we visualize the inference results of person counting models on several samples to demonstrate the performance of our proposed paradigm. Specifically, we select two validation images from each augmentation dataset and apply all the person counting models for prediction. For YOLOv5(i), (ii) and (iv), we render body bounding boxes on the image, while for YOLOv5(iii), we render head bounding boxes. For the DM-Count model, we first blur the estimated density map with a Gaussian kernel with size of $5\times5$ to generate a heatmap, which is then overlaid onto the original image for better visualization. We also remark the counting number on the left-bottom of each output image. From these results, we observe that each person counting model shows its strengths and weaknesses in specific scenarios. Moreover, the existence of scenario classifier raises the possibility for the input image to be processed by the appropriate person counting model.

%\begin{figure}[htbp]%
%\centering
%\includegraphics[width=1\linewidth]{figs/fig_8.png}
%\caption{Outdoor obstacle segmentation results of different models. (a) The input image. (b) Ground truth of semantic segmentation. (c) Result of PSPNet. (d) Result of DeepLabv3+. (e) Result of ours.}
%\label{fig:outdoor}
%\end{figure}

%\begin{figure}[htbp]%
%\centering
%\includegraphics[width=1\linewidth]{figs/fig_9.png}
%\caption{Segmentation performance of our method on consecutive frames. (a) The input image. (b) and (c) are segmentation result without and with temporal consistency supervision. (d) and (e) are the obstacle detection result of (b) and (c).}
%\label{fig:cons}
%\end{figure}

%\begin{figure}[tbp]%
%\centering
%\includegraphics[width=1\linewidth]{figs/fig_10.png}
%\caption{Obstacle avoidance results for indoor and outdoor scenarios. (a) image (b) segmentation (c) morphological processing (d) path planning}
%\label{fig:res}
%\end{figure}

\section{Conclusion}\label{sec:concl}

% In this work, we propose a model combining a pose prediction module and a falls classifier for the precognition of falls. The pose prediction module generates future keypoints vector sequence based on the observation. Then the falls classifier takes the predicted keypoints vector as input and judges whether it's a fall. Evaluations has proved the superiority of keypoints features for falls classification and the effectiveness of the pose prediction module.

In this work, we propose an AI paradigm specially designed for person counting task, which takes into account the scenario in which the image is captured. The proposed architecture consists of a scenario classifier and a person counting module containing four YOLOv5 models and a DM-Count model, each fine-tuned for a specific scenario. The scenario classifier is proved effective in allocating the input image to one of five person counting models based on its scenario label. The YOLOv5-based models count the persons by detecting bodies or heads and counting the number of bounding boxes, while the DM-Count produces an estimation by generating a density map and summing up all the pixel values. Our paradigm outperforms any single predetermined model on the integrated validation dataset, demonstrating its generalization in various scenarios.


{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
