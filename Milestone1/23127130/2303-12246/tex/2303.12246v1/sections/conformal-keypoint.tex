%!TEX root = ../main.tex

\section{Conformal Keypoint Detection}
\label{sec:keypoint:conformal}
In this section, we apply the ICP framework in Section~\ref{sec:pre:icp} to the problem of semantic keypoint detection. 

{\bf Setup}. Denote by $x \in \Real{H\times W\times 3}$ an RGB image picturing an object, by $\vy = (y_1,\dots,y_K) \in \Real{2} \times \dots \times \Real{2} := \calY$ the groundtruth locations of $K$ semantic keypoints of the object. We partition a given dataset $\{z_{i}:=(x_{i},\vy_{i})\}_{i=1}^l$ 
% containing pairs of images and keypoint annotations. We partition the dataset 
into a proper training set (of size $m$) and a calibration set (of size $n$).
We follow the three steps in Section~\ref{sec:pre:icp} to perform ICP.

{\bf Training}. We choose 
% one of the first and most popular approaches for training a network that can predict semantic keypoints: 
the heatmap approach in~\cite{pavlakos17icra-semantic,schmeckpeper22jfr-semantic} as the prediction function: given an image $x$, \cite{schmeckpeper22jfr-semantic} outputs a set of heatmaps $\vf(x) = (f(x)_1,\dots,f(x)_K)$, where each $f(x)_{k} \in \Delta^{HW}:=\{v \in \mathbb{R}^{HW}_{+}\mid \sum_{i}^{HW} v_i = 1\}$ predicts the probability distribution of the $k$-th keypoint lying on each pixel of the image.\footnote{The heatmap in the original paper~\cite{pavlakos17icra-semantic} is not a valid probability distribution as it contains negative values and do not sum up to $1$. We remove the negative values and normalize it to be a valid probability distribution.} For convenience, we use $q^j \in \Real{2}$ to denote the $j$-th pixel location in $x$ and $f(x)_k^j \in \mathbb{R}_{+}$ to denote the probability of the $k$-th keypoint lying on $q^j$. Let $\sigma_k$ be the index permutation that sorts $f(x)_k$ in nonincreasing order, \ie, $f(x)_k^{\sigma_k(1)} \geq \dots \geq f(x)_k^{\sigma_k(HW)}$. As we will soon show, choosing the heatmap approach leads to simple and intuitive designs of the nonconformity function.

{\bf Conformal calibration}. 
% We conformalize the heatmap by designing a nonconformity function. 
We design the following nonconformity function 
\bea\label{eq:maxnonconformity}
r(\vy,\vf(x)) = \max\{ \phi(y_k,f(x)_k) \}_{k=1}^K
\eea
that uses $\phi$ to score each keypoint and then selects the maximum score. This design considers the worst keypoint detection performance of $\vf$. We provide two designs of $\phi$ below.

{\emph{(a) Peak}}. Shorthand $p_k = f(x)_k^{\sigma_k(1)}$ as the peak probability in the $k$-th heatmap and $q_k = q^{\sigma_k(1)}$ as the pixel location attaining the peak probability, we design
\begin{equation}\label{eq:peak}
\phipeak(y_k, f(x)_k) = p_k \norm{y_k - q_k} \tag{peak}
\end{equation}
which computes the error between the true keypoint location $y_k$ and the most probable keypoint location $q_k$ and scales the error by the peak probability $p_k$. $\phipeak$ describes nonconformity because it becomes larger when the network $\vf$ is \emph{confidently wrong} (both $\norm{y_k - q_k}$ and $p_k$ are large), implying the sample is highly nonconforming. 

{\emph{(b) Covariance}}. Let $\barq_k = \sum_{j=1}^J f(x)_k^{\sigma_k(j)} q^{\sigma_k(j)}$ be the expected location of the top-$J$ most likely detections for the $k$-th keypoint, and $\Sigma_k = \sum_{j=1}^J f(x)_k^{\sigma_k(j)} \cdot (q^{\sigma_k(j)} - \barq_k)(q^{\sigma_k(j)} - \barq_k)\tran$ as the covariance, we design
\begin{equation}\label{eq:cov}
\phicov(y_k, f(x)_k) = (y_k - \barq_k)\tran \Sigma_k\inv (y_k - \barq_k) \tag{cov}
\end{equation}
which computes the squared Mahalanobis distance~\cite{mahalanobis36nisi-mahalanobis} from the groundtruth $y_k$ to the top-$J$ keypoint detections (represented by the mean $\barq_k$ and covariance $\Sigma_k$).\footnote{We only choose the top-$J$ ($J=100$) most likely detections on the heatmap because the heatmap can be quite noisy in practice.} A larger Mahalanobis distance indicates more abnormality of the heatmap $f(x)_k$ (compared to the groundtruth $y_k$)~\cite{geun00cs-multivariate}, and hence implies higher nonconformity.

Using the nonconformity function \eqref{eq:maxnonconformity} with \eqref{eq:peak} or \eqref{eq:cov}, we compute the nonconformity scores of the calibration set and sort them as: $\alpha_{\pi(1)} \geq \dots \geq \alpha_{\pi(n)}$.

{\bf Conformal prediction}. Given an error rate $\epsilon \in (0,1)$, we first find $\alpha_{\pi(\floor{(n+1)\epsilon})}$. Then, according to the ICP set definition~\eqref{eq:icpcompute} and our nonconformity function~\eqref{eq:maxnonconformity}, we output the ICP set for a new $x_{l+1}$ as
\bea\label{eq:generalkpticp}
 & \Feps(x_{l+1}) \nonumber \\
\hspace{-4mm} = & \hspace{-3mm} \{\vy \in \calY \mid \max\{ \phi(y_k,f(x_{l+1})) \}_{k=1}^K \leq \alpha_{\pi(\floor{(n+1)\epsilon})} \}  \nonumber \\
\hspace{-4mm} = & \{\vy \in \calY \mid \phi(y_k,f(x_{l+1})) \leq \alpha_{\pi(\floor{(n+1)\epsilon})}, \forall k \},
\eea
where we used $\max\{\phi_1,\dots,\phi_K\} \leq \alpha$ if and only if $\phi_k \leq \alpha$ for any $k$. Insert~\eqref{eq:peak} into~\eqref{eq:generalkpticp}, we have $\Fepsball(x_{l+1})$ as
\begin{equation}\label{eq:icp-ball} \tag{ball}
\cbrace{ \vy \in \calY \mid \norm{y_k - q_{l+1,k}} \leq \frac{ \alpha_{\pi(\floor{(n+1)\epsilon})} }{p_{l+1,k}}, \forall k},
\end{equation}
which defines --for the $k$-th keypoint-- a ball centered at $q_{l+1,k}$ (the most likely detection) with a radius inversely proportional to $p_{l+1,k}$ and proportional to $\alpha_{\pi(\floor{(n+1)\epsilon})}$. Similarly, insert~\eqref{eq:cov} into~\eqref{eq:generalkpticp}, we have $\Fepsellipse(x_{l+1})$ as
\begin{equation}\label{eq:icp-ellipse} \tag{ellipse}
\hspace{-4mm}\cbrace{ \vy \in \calY \mid (y_k - \barq_{l+1,k})\tran \frac{\Sigma_{l+1,k}\inv }{\alpha_{\pi(\floor{(n+1)\epsilon})}} (y_k - \barq_{l+1,k}) \leq 1, \forall k},
\end{equation}
which defines --for the $k$-th keypoint-- an ellipse centered at $\barq_{l+1,k}$ (the expected location of the top-$J$ detections) with an area proportional to $\det(\Sigma_{l+1,k})$ and $\alpha_{\pi(\floor{(n+1)\epsilon})}$.\footnote{The area of  $(x-\mu)\tran A (x-\mu) \leq 1$ is proportional to $\det(A\inv)$.} From \eqref{eq:icp-ball} and \eqref{eq:icp-ellipse}, we observe that the prediction sets become larger when (i) the heatmaps are uncertain, \ie, the peak probability is low or the covariance matrix has large determinant; and (ii) the heatmaps perform poorly on the calibration set, leading to a large $\alpha_{\pi(\floor{(n+1)\epsilon})}$.

{\bf Connections to geometric vision}. Our nonconformity function bears similarity to the \emph{residual} function in geometric vision~\cite{hartley03book-geometry,antonante21tro-outlier,chin18eccv-robust}. For example, the \eqref{eq:peak} and \eqref{eq:cov} functions are similar to the (weighted) reprojection error~\cite{hartley03book-geometry}, and the ``$\max$'' in~\eqref{eq:maxnonconformity} can be connected to seminal work on optimizing the $\ell_{\infty}$ norm~\cite{kahl08tpami-multiple}.

{\bf Outlier-robust nonconformity}? One potential issue of the nonconformity function~\eqref{eq:maxnonconformity} is that a \emph{single} {outlier} can inflate the score and the calibration quantile $\alpha_{\pi(\floor{(n+1)\epsilon})}$ and lead to conservative prediction sets (\eg, when $\vf$ predicts $K-1$ keypoints perfectly but misses one keypoint). A potential remedy in geometric vision is to use robust cost functions~\cite{black96ijcv-unification,yang20ral-gnc,barron19cvpr-general}. Therefore, a natural question is whether ``robustifying'' the nonconformity function \eqref{eq:maxnonconformity} can lead to better prediction sets. Here we focus on only robustifying $\phi$ in \eqref{eq:maxnonconformity} and provide a negative answer.

\begin{proposition}[Invariance of ICP]\label{prop:invariance}
Let $\rho: \mathbb{R}_+ \mapsto \mathbb{R}_+$ be any monotonically increasing function. Fixing the calibration set and error rate $\epsilon$, the nonconformity function
\bea\label{eq:robustnonconformity}
r_{\rho}(\vy, \vf(x)) = \max\{ \rho(\phi(y_k,f(x)_k)) \}_{k=1}^K
\eea
leads to the same ICP set as \eqref{eq:maxnonconformity}.
\end{proposition}
% \begin{proof} Let $\{ \alpha_{i}  \}_{i=1}^n$ be the calibration scores obtained by applying $r$ in \eqref{eq:maxnonconformity} to the calibration set, and $\{ \alpha^{\rho}_i \}_{i=1}^{n}$ be the scores obtained by applying $r_{\rho}$ in \eqref{eq:robustnonconformity}. Observe that $\alpha^{\rho}_i = \rho(\alpha_i)$ because $\rho$ being monotonically increasing implies $\max \circ \rho = \rho \circ \max$ (``$\circ$'' describes function composition). As a result, it follows that $\alpha^{\rho}_{\pi(\floor{(n+1)\epsilon})} = \rho(\alpha_{\pi(\floor{(n+1)\epsilon})})$.
% Let $\Feps_{\rho}$ be the ICP set due to $r_{\rho}$ for a given $\epsilon$, we have
% \bea
% \Feps_{\rho} = \{ \vy \in \calY \mid  \max\{ \rho(\phi(y_k,f(x)_k)) \}_{k=1}^K \leq \alpha^{\rho}_{\pi(\floor{(n+1)\epsilon})}  \} \nonumber \\
% = \{ \vy \in \calY \mid  \rho( \max\{ \phi(y_k,f(x)_k) \}_{k=1}^K ) \leq \rho(\alpha_{\pi(\floor{(n+1)\epsilon})} ) \} \nonumber\\
% = \{ \vy \in \calY \mid \max\{ \phi(y_k,f(x)_k) \}_{k=1}^K \leq \alpha_{\pi(\floor{(n+1)\epsilon})} \}, \nonumber 
% \eea
% where the last set is $\Feps$, the ICP set induced by $r$.
% \end{proof}
The proof of Proposition~\ref{prop:invariance} is presented in Supplementary Material. We conclude that common robust costs, such as $\ell_1$, Huber, Geman-McClure, and Barron's adaptive kernel~\cite{black96ijcv-unification,barron19cvpr-general} (which are monotonically increasing on $[0,+\infty]$) cannot change the ICP sets by robustifying the individual score $\phi$. However, it remains an open question whether changing the ``$\max$'' operation in \eqref{eq:maxnonconformity} can give rise to better ICP sets. For instance, replacing ``$\max$'' with ``$\sum$'' in \eqref{eq:maxnonconformity} and using the Geman-McClure robust cost $\rho(\phi) = \frac{\phi^2}{1 + \phi^2}$ with $\phi = \phipeak$ results in the following ICP set 
\bea
\cbrace{ \vy \in \calY \mid \sum_{k=1}^K \frac{p_k^2 \norm{y_k - q_k}^2}{1 + p_k^2 \norm{y_k - q_k}^2} \leq \alpha_{\pi(\floor{(n+1)\epsilon})} }
\eea
that does not admit a geometric interpretation that is as simple and intuitive as the \eqref{eq:icp-ball} and \eqref{eq:icp-ellipse} sets introduced before. In fact, it is indeed the simplicity of \eqref{eq:icp-ball} and \eqref{eq:icp-ellipse} that enables us to propagate the uncertainty in keypoints to the object pose, as we will show in the next section.




% {\bf Heatmap-based keypoint detector}. Given an RGB image $x \in \Real{H \times W \times 3}$, heatmap-based methods~\cite{pavlakos17icra-semantic,schmeckpeper22jfr-semantic,lin22icra-single} output $f(x) \in \Delta^{HW} := \{ v \in \Real{HW} \mid v_{i} \geq 0, \sum_{i}^{HW} v_{i} = 1 \}$ where $f(x)_{i} \geq 0$ indicates the probability of the keypoint on the $i$-th pixel (we vertically concatenate all pixels). Let $\pi(\cdot)$ denote an index permutation that sorts $f(x)$ as $f(x)_{\pi(1)} \geq \dots \geq f(x)_{\pi(HW)}$.
% % \footnote{In practice, heatmaps directly coming out of neural networks are often not constrained to be nonnegative and sum up to one~\cite{schmeckpeper22jfr-semantic}. In this case, we offset and normalize the heatmaps.} 

% {\bf Nonconformity function}. It is tempting to treat keypoint detection as a classification problem and adopt popular nonconformity functions designed for classification (\eg the one in~\cite{romano20neurips-classification,angelopoulos21iclr-uncertainty}). However, in the Supplementary Material we show the resulting prediction sets are loose and hard to interpret. This motivates us to design the following three nonconformity functions.

% {\bf (I) Peak}. Let $q^\star \in \Real{2}$ be the pixel location with maximum probability $p^\star := f(x)_{\pi(1)}$. We design
% \begin{equation}\label{eq:score-peak}
% r(y, f(x)) = p^\star \norm{y - q^\star}. \tag{Peak}
% \end{equation}
% According to \eqref{eq:icpcompute}, we have the following inductive conformal prediction set
% \begin{equation}\label{eq:icp-heatmap-peak}
% \Feps(x_{l+1}) = \cbrace{y \in \calY \ \middle\vert\ \norm{y - q^\star_{l+1}} \leq {\alpha_{\pi(\floor{(n+1)\epsilon})}}/{p^\star_{l+1}} }. \tag{ICP-Peak}
% \end{equation}
% This ICP set is a ball centered at $q^\star_{l+1}$ with a radius \emph{inversely} proportional to $p^\star_{l+1}$. Intuitively, when $p^\star_{l+1}$ is small, \ie $f(x)$ is uncertain, the ball is enlarged to account for higher uncertainty.
    
% {\bf (II) Variance}. Let $q_i \in \Real{2}$ be the $i$-th pixel location. Compute $\bar{q} = \sum_{i=1}^{K} f(x)_{\pi(i)} \cdot q_{\pi(i)}$ as the expected location of the top-$K$ keypoints, and $\gamma^2 = \sum_{i=1}^{K} f(x)_{\pi(i)} \cdot \Vert q_{\pi(i)} - \bar{q} \Vert^2$ as the ``variance'' (we use $K=100$ pixels because the heatmap can be quite noisy). We design
% \begin{equation}\label{eq:score-variance}
% r(y,f(x)) = {\norm{y - \bar{q}}}/{\gamma}. \tag{Var}
% \end{equation}
% According to \eqref{eq:icpcompute}, we have the inductive conformal prediction set
% \begin{equation}\label{eq:icp-heatmap-var}
% \Feps(x_{l+1}) = \cbrace{ y \in \calY \ \middle\vert\ \norm{y - \bar{q}_{l+1}} \leq \gamma_{l+1} \alpha_{\pi(\floor{(n+1)\epsilon})}  }. \tag{ICP-Var}
% \end{equation}
% This ICP set is a ball centered at $\bar{q}_{l+1}$ with a radius proportional to the ``standard deviation'' $\gamma_{l+1}$. Intuitively, when the heatmap is spread out and $f(x)$ has higher uncertainty, the ball becomes larger. 
    
% {\bf (III) Covariance}. Compute the expected top-$K$ keypoint location $\bar{q}$ as before. Then compute the \emph{covariance matrix} $\Sigma = \sum_{i=1}^{K} f(x)_{\pi(i)} \cdot (q_{\pi(i)} - \bar{q})(q_{\pi(i)} - \bar{q})\tran$.
% We design
% \begin{equation}\label{eq:score-cov}
% r(y,f(x)) = (y - \bar{q})\tran \Sigma\inv (y-\bar{q}) \tag{Cov}.
% \end{equation}
% According to \eqref{eq:icpcompute}, we have the inductive conformal prediction set
% \begin{equation}\label{eq:icp-heatmap-cov}
% \Feps(x_{l+1}) = \cbrace{y \in \calY \ \middle\vert\ (y-\bar{q}_{l+1})\tran {\Sigma_{l+1}\inv} (y-\bar{q}_{l+1}) \leq \alpha_{\pi(\floor{(n+1)\epsilon})} }.\tag{ICP-Cov}
% \end{equation}
% This ICP set is an ellipse centered at $\bar{q}_{l+1}$. The eigenvectors of ${\Lambda}_{l+1} := \Sigma\inv_{l+1} / \alpha_{\pi(\floor{(n+1)\epsilon})}$ point in the directions of the principal axes, while the eigenvalues are $1/a^2$ and $1/b^2$, with $a\leq b$ the lengths of the semi-axes. Similar to~\eqref{eq:icp-heatmap-var}, the ellipse gets larger if the heatmap has higher uncertainty. Different from~\eqref{eq:icp-heatmap-var}, the ellipse better captures nonuniform uncertainty, as we show in Section~\ref{sec:experiments}. 



