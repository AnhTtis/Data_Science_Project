%!TEX root = ../main.tex

\section{Inductive Conformal Prediction}
\label{sec:pre:icp}
Given a set $\{ z_i = ( x_i, y_i ) \}_{i=1}^l$ with observation $x_i \in \calX$ and label $y_i \in \calY$ such that each $z_i \in \calZ := \calX \times \calY $ is drawn i.i.d. from an \emph{unknown} distribution on $\calZ$, inductive conformal prediction (ICP) provides 
% a simple yet powerful framework to learn 
a \emph{set prediction} $\Feps(x) \subseteq \calY$, parameterized by an error rate $0 < \epsilon <1$, such that given a new sample $z_{l+1} = (x_{l+1},y_{l+1})$ satisfying an \emph{exchangeability} condition (elaborated in Theorem~\ref{thm:icp-validity}), we have
\bea\label{eq:icpmiscoverage}
\probof{ y_{l+1} \in \Feps(x_{l+1}) } \geq 1-\epsilon, 
\eea
\ie, the prediction set $\Feps$ guarantees to contain the true label $y_{l+1}$ with probability at least $1-\epsilon$. 

% In order to achieve the probabilistic coverage in~\eqref{eq:icpmiscoverage}, ICP performs the following three steps.

{\bf Training}. We start by dividing the dataset into a \emph{proper training set} $\{ z_1,\dots,z_m \}$ and a \emph{calibration set} $\{ z_{m+1},\dots,z_{l} \}$. We shorthand $n = l - m$ as the size of the calibration set.
We learn a prediction function $f: \calX \rightarrow \tcalY$ from the proper training set using \emph{any} architecture, which allows us to fully exploit the power of modern deep learning. The prediction space $\tcalY$ can be the same as the label space $\calY$, or can contain auxiliary information such as a heuristic notion of uncertainty (\eg, softmax scores in classification or a heatmap in the case of keypoint detection). 

{\bf Conformal calibration}. 
% Leveraging the learned $f$, 
We define a \emph{nonconformity} function $S: \calZ^{m} \times \calZ \rightarrow \Real{}$ to measure how well a given sample $z = (x,y)$ \emph{conforms} to the proper training set. A popular instance of $S$ leverages the learned prediction $f$:
\bea \label{eq:nonconformity}
S\parentheses{\cbrace{z_1,\dots,z_m},(x,y)} \stackrel{\eg}{=} r(y,f(x)),
\eea
where $r: \calY \times \tcalY \rightarrow \Real{}$ is a measure of disagreement between the label $y$ and the prediction $f(x)$. For example, consider $\calY = \tcalY = \Real{}$, one can design $r(y,f(x)) = \abs{y - f(x)}$: if $(x,y)$ poorly conforms to the training set, $f$ will incur large errors.   
While the function $S$ can be arbitrary (\eg, a learnable neural network~\cite{stutz22iclr-learnconformal}), \eqref{eq:nonconformity} is a convenient definition since $f$ is implicitly dependent on $\{z_i\}_{i=1}^m$ and $r$ can incorporate domain-specific knowledge.
We then compute the nonconformity scores on the calibration set as $\alpha_i = r(y_i,f(x_i)), i = m+1,\dots,l$,
and sort them in \emph{nonincreasing} order $\alpha_{\pi(1)}\geq\dots \geq \alpha_{\pi(n)}$, where $\pi(i) \in \{m+1,\dots,l\}$ is an index permutation.
 % (offset by $m$).

{\bf Conformal prediction}. Given a new observation $x_{l+1}$ (with an unknown $y_{l+1}$) and a user-specified $\epsilon \in (0,1)$, we compute the inductive conformal prediction (ICP) set as
\bea\label{eq:icpcompute}
\Feps \parentheses{x_{l+1}} = \cbrace{y \in \calY \mid \alpha^y \leq \alpha_{\pi(\floor{(n+1)\epsilon})}},
\eea
where $\alpha^y = r(y,f(x_{l+1}))$
is the nonconformity score of the new sample when fixing the true label to be $y$. In other words, the ICP set~\eqref{eq:icpcompute} outputs the set of all labels that make the nonconformity score of the new sample no greater than $\alpha_{\pi(\floor{(n+1)\epsilon})}$ -- the $\floor{(n+1)\epsilon}$-th largest nonconformity score in the calibration set. 
% By doing so, ICP ensures that there are at least $\floor{(n+1)\epsilon}$ samples in the calibration set that are less conforming than the new sample. 
We have the following result stating the probabilistic coverage of the ICP set~\eqref{eq:icpcompute}.
% provides a valid statistical coverage of the true label $y_{l+1}$.

\begin{theorem}[Validity of ICP Coverage {\cite{vovk05book-conformal,lei18jasa-conformal,vovk12acml-icpconditional}}] \label{thm:icp-validity}
If $z_{m+1},\dots,z_l$, $z_{l+1} = (x_{l+1},y_{l+1})$ are exchangeable, \ie, their distribution is invariant under permutation, then
\bea\label{eq:icpvalidity}
1 - \epsilon \leq \probof{y_{l+1} \in \Feps(x_{l+1})} \leq 1 - \epsilon + 1/(n+1)
\eea
for any $\epsilon \in (0,1)$. Furthermore, when conditioned on the calibration set, calling $h = \floor{(n+1)\epsilon}$, we have
\begin{equation}\label{eq:beta}
\hspace{-4mm}\probof{y_{l+1}\!\in\!\Feps(x_{l+1})\!\mid\!\{z_{m+1},\dots,z_l\}}\!\sim\!\mathrm{Beta}(n+1\!-\!h,h).
\end{equation}
\end{theorem}
A few remarks are in order about Theorem~\ref{thm:icp-validity}.
First, asking $z_{m+1},\dots,z_l,z_{l+1}$ to be exchangeable is weaker than asking them to be independent. However, this assumption typically fails when the calibration set is a single video sequence, where the image frames $\{z_{m+1},\dots,z_l\}$ are temporally correlated~\cite{luo21arxiv-conformalsafety}. Fortunately, as we detail in Section~\ref{sec:experiments}, the way the LineMOD Occlusion dataset~\cite{brachmann14eccv-linemodocc} was collected makes the exchangeability condition easily satisfied, which also suggests best practices to make the exchangeability condition hold in computer vision. 
Second, the lower bound in~\eqref{eq:icpvalidity} can be intuitively proved because under exchangeability, $\alpha_{l+1} := r(y_{l+1},f(x_{l+1}))$ --the nonconformity score of the new sample with the true label-- is \emph{exchangeable} with the nonconformity scores of the calibration samples, and hence \emph{equally likely} to fall in anywhere between the scores $\{ \alpha_{\pi(i)}\}_{i=1}^n$. Consequently, $\probof{y_{l+1} \in \Feps(x_{l+1})} = \probof{\alpha_{l+1} \leq \alpha_{\pi(\floor{(n+1)\epsilon})}} = 1 - \floor{(n+1)\epsilon}/(n+1) \geq 1 - \epsilon$. The upper bound in \eqref{eq:icpvalidity} states that $1-\epsilon$ is not overly conservative (indeed tight if $n$ is large). 
Lastly, the probabilistic guarantee in \eqref{eq:icpvalidity} is \emph{marginal} over the randomness of the calibration set, meaning if one chooses an infinite number of calibration sets,  the \emph{average} empirical coverage will converge to $1-\epsilon$. This, however, implies that the empirical coverage given one calibration set is a random variable that fluctuates as the Beta distribution~\eqref{eq:beta}. Fig.~\ref{fig:beta-distribution} plots the Beta distribution at $\epsilon=0.1$ with different sizes of the calibration set. We observe that as $n$ increases the empirical coverage becomes more concentrated at $1-\epsilon$. Our experiments show that even with a small ($n=200$) calibration set, the empirical coverage is close to, and mostly higher than, $1-\epsilon$.

% \begin{proposition}[Conditional Validity of ICP {\cite{vovk12acml-icpconditional}}] \label{prop:icp-conditional-validity}
% \red{To be filled out}
% \end{proposition}


% Proposition~\ref{prop:icp-validity} states that, if the new observation $z_{l+1}$ is exchangeable with the calibration set (which is a weaker condition than requiring $z_{l+1}$ is jointly i.i.d. with the calibration set), then no matter which prediction function $f$ has been learned from the proper training set, and which function $A$ has been chosen to compute the nonconformity score, we have at least $1-\epsilon$ confidence that the ICP $\Feps$ defined in \eqref{eq:icp} contains the true label. Of course, the caveat here is that the quality of the learned prediction function $f$ and the nonconformity function $A$ will decide the conservativeness of the ICP $\Feps$. For example, if $f$ has poor predictive power, then the set $\Feps$ may be arbitrarily large so that it tells little information about the true label $y$. \red{Fortunately, as we will show in experiments, with modern deep learning architectures for learning $f$, we can obtain ICPs that are both confident and tight.}

\input{sections/fig-icp-overview.tex}