%!TEX root = ../main.tex

\input{sections/fig-coverage-and-bound}
\input{sections/table-ape-recall-lmo}

\section{Experiments}
\label{sec:experiments}

We test our approach on the LineMOD Occlusion (\lmo) dataset~\cite{brachmann14eccv-linemodocc} to (i) justify the exchangeability assumption (Theorem~\ref{thm:icp-validity}) and suggest best practices for applying conformal prediction; (ii) evaluate the empirical coverage of the {\purse} and verify the correctness of Theorem~\ref{thm:icp-validity}, and (iii) compute the worst-case error bounds and demonstrate tightness or looseness. We also (iv) show that the average pose achieves better or similar accuracy as other approaches.

{\bf Implementation and runtime}. We set $T=1000$ in {\ransag}; use OpenGV~\cite{kneip14icra-opengv} for \pthreep and \pnp; and add a redundant $\norm{t} \leq 5$ in~\eqref{eq:purse} to ensure bounded translation. All procedures are implemented in Python except SDP relaxations are implemented in Matlab. The runtime of {\ransag} is comparable to {\ransac} and below one second. The runtime of computing~\eqref{eq:pose2purse} via SDPs is around $8$ seconds on a workstation with 2.2GHz AMD CPUs. The (second-order) SDP relaxations are almost always exact. 
% It is possible to compute a faster (in $0.1$ second) but looser upper bound for~\eqref{eq:pose2purse} by using a first-order SDP relaxation.

{\bf Dataset and exchangeability}. The \lmo dataset contains $1214$ test images capturing $8$ different objects on a table, of which $200$ images were chosen by BOP19'20~\cite{hodan18eccv-bop}. We use the $200$ images for calibration and the entire $1214$ images for testing. As mentioned in Section~\ref{sec:pre:icp}, if the dataset was collected as a single video sequence under natural motion (\eg, a straight line), then the exchangeability assumption would fail. However, \cite{hinterstoisser12accv-linemod} described the data collection:
\begin{quote}
\vspace{-2mm}  
In order to guarantee a well distributed pose space sampling of the dataset pictures, we \emph{uniformly} divided the upper hemisphere of the objects into \emph{equally distant} pieces and took \emph{at most one image per piece}. As a result, our sequences provide \emph{uniformly distributed views} ...
\end{quote}
\vspace{-2mm}  
which indicates the $1214$ images are independent (\cf~\cite[Figs.~5-6]{hinterstoisser12accv-linemod}) and therefore exchangeable. This demonstrates a good example for data collection --to equally divide the parameter space and collect one observation per division-- so the guarantees offered by conformal prediction are valid. 


{\bf Empirical coverage}. Our approach conformalizes the heatmaps~\cite{schmeckpeper22jfr-semantic} as~\eqref{eq:icp-ball} or~\eqref{eq:icp-ellipse}. The implementation\footnote{\url{https://github.com/yufu-wang/6D_Pose}} of~\cite{schmeckpeper22jfr-semantic} uses either groundtruth or Faster RCNN~\cite{ren15neurips-frcnn} bounding boxes, giving four variants of our approach: groundtruth box plus~\eqref{eq:icp-ball} or~\eqref{eq:icp-ellipse} (labels: \gtball, \gtellipse), and Faster RCNN box plus~\eqref{eq:icp-ball} or~\eqref{eq:icp-ellipse} (labels: \frcnnball, \frcnnellipse). Fig.~\ref{fig:coverage-and-bound} left column shows the empirical coverage (\ie, the percentage of images whose groundtruth poses lie in~\eqref{eq:purse}) of all four variants with $\epsilon=0.1$ and $\epsilon=0.4$. We see the empirical coverage is around $90\%$ when $\epsilon=0.1$ and around $60\%$ when $\epsilon=0.4$, for all $8$ objects. Though the empirical coverage can deviate from $1-\epsilon$, it generally stays within $\pm 5\%$ and mostly goes above $1-\epsilon$, which is encouraging given that our calibration set only has size $n=200$. Fig.~\ref{fig:methodoverview} (b) plots examples of the prediction sets. More examples are shown in the Supplementary Material. 


{\bf Worst-case error bounds}. Fig.~\ref{fig:coverage-and-bound} middle column plots the worst-case rotation error bound ($x$-axis) vs. the actual rotation error between the average pose and the groundtruth ($y$-axis) for our approach using the {\gtball} setup (results for {\gtellipse}, {\frcnnball} and {\frcnnellipse} are similar and provided in the Supplementary Material). First, when the {\purse} covers the groundtruth (blue circles), the rotation error bound is always larger than the actual error (\ie, the blue circles never cross the $y=x$ diagonal). Second, when the error rate is increased from $\epsilon=0.1$ to $\epsilon=0.4$, we observe a shift of the blue circles towards $y=x$, indicating the error bounds get tightened. Third, our bounds are reasonably tight for most test images (\ie, the bottom-left cluster of blue circles) especially when $\epsilon=0.4$. However, they can become overly conservative (\ie, the line of blue circles on the right-side boundary) due to the keypoint prediction sets become too large. Fig.~\ref{fig:coverage-and-bound} right column plots similar results for the translation. The Supplementary Material gives a more detailed analysis of this conservatism, wherein we also solve~\eqref{eq:pose2purse} for multiple samples computed by {\ransag}, choose the minimum bound, and compare them with those obtained for the average pose (\cf Remark~\ref{rmk:bestbound}). 

{\bf Accuracy of the average pose}. We compare the accuracy of our average pose with other methods according to the 2D projection metric (an estimation is correct if the mean reprojection error is below $5$ pixels). Table~\ref{table:accuracy} shows: (i) our average pose achieves significantly better success rates when using groundtruth bounding boxes, and similar success rates when using Faster RCNN; (ii) the accuracy of the average pose increases when $\epsilon$ increases. 