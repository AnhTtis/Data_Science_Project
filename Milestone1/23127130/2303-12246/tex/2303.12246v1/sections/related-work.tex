%!TEX root = ../main.tex

\section{Related Work}
\label{sec:related-work}

{\bf Image-based object pose estimation}. We categorize object pose estimation into two paradigms: \emph{single-stage} and \emph{two-stage}. The latter first detects 2D-3D correspondences and then estimates the object pose via solving a \pnp problem, while the former produces poses without intermediate correspondences. (i) \emph{Single-stage}. Early methods perform pose estimation via template matching~\cite{huttenlocher93pami-comparing,gu10eccv-discriminative,hinterstoisser11pami-gradient}. Recently, deep learning-based approaches such as PoseNet~\cite{kendall15iccv-posenet} and PoseCNN~\cite{xiang18rss-posecnn} applied CNNs to directly regress poses. A major challenge of pose regression is the nonlinearity of 3D rotations, and motivated formulating regression as classification~\cite{su15iccv-render,tulsiani15cvpr-viewpoints,sundermeyer18eccv-implicit} or designing better rotation representations~\cite{zhou19cvpr-continuity,labbe20eccv-cosypose}. It is also popular to predict multiple pose hypotheses followed by voting~\cite{liebelt08cvpr-independent,sun10eccv-depth,michel17cvpr-global}. (ii) \emph{Two-stage}. Early research used handcrafted features~\cite{lowe99iccv-sift,rothganger06ijcv-3d,lepetit05cgv-monocular} to establish 2D-3D correspondences and focused on developing algorithms for solving {\pnp}. Notable algorithms include the minimal solver \pthreep~\cite{gao03pami-p3p,kneip11cvpr-p3p} and variants of the nonminimal solver \pnp~\cite{kneip14eccv-upnp,lepetit09ijcv-epnp,olsson06icpr-optimal,yang20cvpr-perfect}. Outliers (\ie, wrong correspondences) motivated robust estimation based on \ransac~\cite{fischler81acm-ransac}, graduated non-convexity~\cite{yang20ral-gnc,black96ijcv-unification,blake87book-visual}, branch-and-bound~\cite{jiao20iros-globally,li09iccv-consensus,campbell17iccv-globally}, or semidefinite relaxations~\cite{yang22pami-certifiably}. Unreliable correspondences soon became the bottleneck and learned correspondences have been predominant. Learned correspondences can be \emph{sparse} or \emph{dense}. Sparse methods define a handful of keypoints
and predict locations of the keypoints via direct regression~\cite{rad17iccv-bb8,tekin18cvpr-yolo}, probabilistic heatmap~\cite{pavlakos17icra-semantic,oberweger18eccv-heatmap}, or voting~\cite{peng19cvpr-pvnet}. Dense methods~\cite{brachmann16cvpr-uncertainty,li19iccv-cdpn,zakharov19iccv-dpod,park19iccv-pix2pose,hodan20cvpr-epos,wang21cvpr-gdrnet} regress for each object pixel the coordinates of its corresponding 3D point.
% , and hence can thought of treating every point as a potential keypoint.
Recent literature focus on end-to-end training via differentiating {\pnp}~\cite{brachmann18cvpr-learning,chen20cvpr-backproppnp,campbell20eccv-blindpnp,iwase21iccv-repose,chen22cvpr-epropnp}. Both single-stage and two-stage methods perform well on standard benchmarks~\cite{hodan18eccv-bop}, but a crucial feature that is missing, especially when deploying computer vision algorithms in safety-critical applications, is that these methods do not provide \emph{provably correct} uncertainty quantification and \emph{formal} error bounds \wrt the groundtruth (for either the correspondences or the poses). In this paper, we provide rigorous guarantees by applying conformal prediction to an existing keypoint detection method (the heatmap~\cite{pavlakos17icra-semantic}) and leveraging old and new techniques in computer vision to derive formal error bounds. 
% Although we only perform a case study, the methodology can be generalized to other existing pose estimation methods.

{\bf Conformal prediction in computer vision}. Conformal prediction~\cite{vovk05book-conformal} is a statistical machinery that offers provably correct finite-sample uncertainty quantification without assumptions on the data distribution or the prediction model (\ie, offering a set prediction, instead of a point prediction, that guarantees probabilistic coverage of the groundtruth). \emph{Inductive conformal prediction}~\cite{papadopoulos08chapter-icp} is the most popular variant of conformal prediction because it does not require retraining of the prediction models~\cite{lei13jasa-distribution,angelopoulos21arxiv-gentle,angelopoulos21iclr-conformal}.
% , and hence rises to be one of the most exciting research areas in statistics and machine learning. 
Applying conformal prediction to computer vision, however, is still in its infancy. Existing works focus on image classification~\cite{romano20neurips-classification,angelopoulos21iclr-conformal}, tumor segmentation~\cite{wieslander20jbhi-tumor,angelopoulos22arxiv-conformalriskcontrol,bates21jacm-rcps}, and bounding box detection~\cite{li22arxiv-towards,de22csrs-object,angelopoulos21arxiv-gentle}, which are classification or low-dimensional regression problems. Inspired by these works, our unique contributions in this paper are: (i) we apply conformal prediction to keypoints detection, a high-dimensional regression problem; (ii) we design new nonconformity functions and discuss their connections with classical geometric vision; and (iii) we develop algorithms that propagate the uncertainty after conformal prediction to form prediction sets of 6D poses, which are nonlinear and nonconvex manifold objects. 



{\bf Performance guarantees}. Pose estimation from 2D-2D, 2D-3D, and 3D-3D correspondences are foundational problems in computer vision textbooks~\cite{hartley03book-geometry,barfoot17book-state,ma04book-invitation,szeliski22book-computer} and typically boil down to formulating and solving mathematical optimization problems.
% \footnote{Such formulations typically start with a generative model that the groundtruth must satisfy given noiseless measurements, and seek an estimation that minimizes the violations of the generative model (\eg, nonlinear and robust least squares)~\cite{matei06pami-estimation}. It is clear that the optimal solutions of the formulated problem contain the groundtruth when the measurements are noiseless, which, however, rarely happens in practice.} 
Benchmarking on simulated and real datasets has been a widely adopted standard for testing different formulations and solvers. However, empirical performance can be misleading without theoretical guarantees. A striking fact is that, though error analysis is an important topic in applied math~\cite{candes06cpam-stable,klivans18colt-efficient,diakonikolas22arxiv-list} and control theory~\cite{milanese91automatica-optimal,soderstrom07automatica-errors,mazzaro04cdc-set}, there is very limited literature in computer vision that reason about \emph{worst-case estimation errors} between the optimal solution and the groundtruth. A popular heuristic relies on the inverse of the Hessian at an optimal solution, which provides the \emph{Cramer-Rao lower bound} on the covariance of the solution (for linear regression this coincides with the covariance)~\cite[Section B.6]{szeliski22book-computer} and thus cannot \emph{upper bound} the estimation errors. Recent works~\cite{rosen19ijrr-sesync,yang20tro-teaser,carlone22arxiv-estimation} derived error bounds for a few geometric vision problems. However, the bounds either depend on uncheckable assumptions and cannot be computed~\cite{rosen19ijrr-sesync,yang20tro-teaser}, or build on machinery (\eg, sum-of-squares proof~\cite{moitra20book-sos,barak16course-proofs}) that only applies to estimators based on moment relaxations~\cite{carlone22arxiv-estimation}, which are still computationally expensive in practice~\cite{yang22mp-inexact}. In this paper, we develop the first kind of efficiently computable error bounds that only require the assumption of \emph{exchangeability} (which comes from conformal prediction). We justify this assumption on our test dataset and numerically show our bounds can be tight for a subset of the test problems.



