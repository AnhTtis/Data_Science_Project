%!TEX root = ../main.tex

\section{Supplementary Experiments}

\input{sections/supp-fig-relax_order}

\subsection{Ablation: Relaxation Order}
In the main document, we briefly described that we applied second-order semidefinite relaxations to compute the worst-case error bounds in~\eqref{eq:pose2purse} and reported that the average runtime is around $8$ seconds on an ordinary workstation. Here we justify the choice of second-order relaxations by showing that first-order relaxations, although much faster (average runtime is about $0.1$ seconds), lead to much looser upper bounds for the optimization~\eqref{eq:pose2purse}. 

To help the reader better understand the approach, we first give a very short introduction to semidefinite relaxations for polynomial optimization problems (POPs). We refer the reader to~\cite[Section 2]{yang22pami-certifiably} for a detailed introduction.  

{\bf Polynomial optimization problems} (POPs) are problems of the following general formulation
\bea
\min_{x \in \Real{n}} & p(x) \\
\subject & h_i(x) = 0, i=1,\dots,l_h,\\
& g_j(x) \geq 0, j=1,\dots,l_g
\eea 
where $p,\{h_i\}_{i=1}^{l_h}, \{ g_j\}_{j=1}^{l_g}$ are all polynomial functions in $x \in \Real{n}$. Notice that if we denote $s = [\vectorize{R}\tran,t\tran]\tran \in \Real{12}$, it is clear that the cost function of~\eqref{eq:pose2purse} is a polynomial in $s$ when fixing a particular $\lambda$ (we can add a minus sign to the cost of~\eqref{eq:pose2purse} so that we convert ``$\max$'' to ``$\min$''). The constraints for~\eqref{eq:pose2purse} is $(R,t) \in \Seps$ where $\Seps$ has the form in~\eqref{eq:purse}. We claim that the~\eqref{eq:purse} can be described by a set of polynomial equalities and inequalities. This is because (i) the rotation constraint $R \in \SOthree$ can be described by $15$ quadratic equality constraints~\cite{yang22pami-certifiably}; (ii) the quadratic constraints in~\eqref{eq:purse} are already polynomial constraints; and (iii) the linear inequalities $b_k\tran s > 0$ can be equivalently written as $b_k\tran s \geq \epsilon$ for a small $\epsilon > 0$ (note that $b_k\tran s$ is the depth of the 3D keypoints, so it makes sense to enforce they are larger than, say $\epsilon = 0.001$). We conclude that computing the worst-case error bounds~\eqref{eq:pose2purse} is a POP.

{\bf Semidefinite relaxations} are a powerful tool to approximate (or even exactly compute) the \emph{global optimal} solutions for (generally nonconvex) POPs. In particular, Lasserre's hierarchy of moment and sums-of-squares relaxations~\cite{lasserre01global} provides a systematic approach to design such semidefinite relaxations. In particular, Lasserre's hierarchy relaxes a POP into a hierarchy of convex semidefinite programs (SDPs) of increasing size. Each relaxation, at a so-called \emph{relaxation order}, in this hierarchy can be solved in polynomial time and provides a valid lower bound for the POP (if the POP aims to maximize, as in~\eqref{eq:pose2purse}, then a valid upper bound is provided). Moreover, under mild technical conditions, the lower (upper) bounds of these relaxations coincide with the global optimum of the original POP, in which case we say the relaxation is \emph{exact}, or \emph{tight}.

{\bf First-order vs. second-order relaxations}. The minimum relaxation order for the POP~\eqref{eq:pose2purse} is $1$, since all the polynomials in~\eqref{eq:pose2purse} have degree at most $2$ (in general, the minimum relaxation order for a POP is $\lceil d/2 \rceil$, where $d$ is the maximum degree of the polynomials defining a POP). In practice we choose a second-order relaxation instead of a first-order relaxation because first-order relaxations give loose upper bounds for problem~\eqref{eq:pose2purse}. Fig.~\ref{fig:relax-order} plots the worst-case error bounds computed by solving the first-order relaxation of~\eqref{eq:pose2purse} under the same {\gtball} setup. Compared to Fig.~\ref{fig:coverage-and-bound} middle and right columns, we clearly see that solving the first-order relaxation produces overly conservation upper bounds for~\eqref{eq:pose2purse}. For example, when $\epsilon=0.1$, solving the first-order relaxation never produces a rotation error bound that is below $100^\circ$, while in Fig.~\ref{fig:coverage-and-bound} we see a cluster of blue circles near the bottom left corner indicating tight bounds.

One nice property of applying semidefinite relaxations is that we get a certificate of global optimality when the relaxation is indeed exact. Such certificates typically come in the form of a rank-one optimal SDP solution, or a relative suboptimality gap (\cf \cite[eq. (24)]{yang22pami-certifiably}), which indicates exactness of the relaxation when the value is numerically zero (loosely speaking, a relative suboptimality gap of $\epsilon\%$ means that the global optimum of the SDP is at most $\epsilon$ percentage away from the global optimum of the POP). When we solve second-order relaxations of problem~\eqref{eq:pose2purse} under the {\gtball} setup with $\lambda = 1$, we obtain a relative suboptimality gap that is below $10^{-3}$ (resp. $10^{-6}$) for $99.02\%$ (resp. $72.51\%$) of the $8784$ test problems, indicating that the second-order relaxation is sufficient to obtain (approximately) globally optimal solutions for problem~\eqref{eq:pose2purse}.

\subsection{Qualitative ICP Sets}

Fig.~\ref{fig:methodoverview}(b) shows circular and elliptical examples of the ICP sets. Fig.~\ref{fig:heatmap-qualitative} provides more examples of the ICP sets with $\epsilon=0.1$ and $\epsilon=0.4$. Notice how the ICP sets become smaller when $\epsilon$ increases.

\input{sections/supp-fig-heatmap-qualitative}


\subsection{Worst-case Error Bounds under {\gtellipse}, {\frcnnball}, and {\frcnnellipse} setups}
Fig.~\ref{fig:coverage-and-bound} middle and right columns (from the main document) show the worst-case error bounds (computed from~\eqref{eq:pose2purse}) of the average pose under the {\gtball} setup. Fig.~\ref{fig:other-error-bounds} shows the worst-case error bounds under the {\gtellipse}, {\frcnnball}, and {\frcnnellipse} setups, which are qualitatively similar to Fig.~\ref{fig:coverage-and-bound}. Notice that the blue circles never cross the $y=x$ diagonal, indicating our bounds are always valid when the \purse contains the groundtruth pose.

\input{sections/supp-fig-error_bounds}

\subsection{A Closer Look at the Conservative Error Bounds}
The reader may have noticed two unusual results in the experiments on \lmo. First, the success rate on eggbox is consistently lower than other categories in our methods and other baselines (\eg, PVNet achieves $8.43\%$ success rate on eggbox, while the second lowest success rate is $55.37\%$). Second, the worst-case error bounds can be overly conservative, \eg, having $180^\circ$ rotation error bounds. It turns out both unusual results can be explained by the same reason: a labelling discrepancy in the {\lmo} dataset about eggbox.

We noticed the low success rate on eggbox across all baseline methods and contacted the authors of~\cite{schmeckpeper22jfr-semantic}, who encountered the same problem. One author told us ``\emph{I think this is a mistake or discrepancy in the 6DoF annotations of the dataset itself. As it [the eggbox] is considered a symmetric object, annotators for LMOD might not have consistently annotate it}''. Though it is possible to revise the nonconformity score for symmetric objects, the manually chosen keypoints by~\cite{schmeckpeper22jfr-semantic} break the symmetry. Therefore we decided to leave this discrepancy as is because it does not affect our probabilistic guarantees.
% even though the eggbox is actually not completely symmetric. 

This labeling discrepancy, however, does translate to \emph{ conservative} prediction sets for the eggbox, in order to contain the (wrong) groundtruth at the desired probability. Fig.~\ref{supp:fig:eggbox} shows the eggbox prediction sets are \emph{one order of magnitude} larger than the other categories, leading to worst-case rotation error bounds being mostly $180^\circ$ (because the \purse is large enough to cover the entire $\SOthree$). {This indeed shows the advantage of our framework}: \emph{the user will see the large uncertainty produced by our algorithm and be alerted}!

Finally, because the \purse is too large, \ransag essentially 
returns a random sample in $\SOthree$, which has zero probability being close to the (wrong) groundtruth. Hence, a $0\%$ success rate makes sense. 

\begin{figure}
    \includegraphics*[width=0.49\linewidth]{ball_size.pdf}
    \hfill
    \includegraphics*[width=0.4\linewidth]{bound.pdf}
    \vspace{-3mm}
    \caption{Left: cumulative distribution (CDF) of the average radius of prediction sets (under \gtball).
    Right: CDF of the worst-case rotation error bounds; \emph{eggbox} error bounds are mostly $180^\circ$. \label{supp:fig:eggbox}}
    \vspace{-4mm}
 \end{figure}

\subsection{Best Worst-case Error Bounds from Samples (Remark~\ref{rmk:bestbound})}

In Remark~\ref{rmk:bestbound}, we discussed that since solving~\eqref{eq:pose2purse} can provide worst-case error bounds for any pose estimator, the natural question is to ask if we can find better pose estimators (than the average pose computed from {\ransag}) with tighter worst-case error bounds, which boils down to solving the minimax problem in~\eqref{eq:bestbound}. However, problem~\eqref{eq:bestbound} is much more challenging to solve than~\eqref{eq:pose2purse}, and to the best of our knowledge, there is no efficient way to obtain a globally optimal solution. We think a good future research direction may be to explore methods in~\cite{pineda22neurips-theseus} or~\cite{nie17siopt-bilevel} for solving~\eqref{eq:bestbound}.

In this section, we provide a very preliminary study to explore if~\eqref{eq:bestbound} can indeed offer us tighter error bounds. Towards this goal, we randomly select $M=5$ pose samples $\{(R_i,t_i)\}_{i=1}^M$ from the results of {\ransag} (recall \ransag not only returns an average pose, but also returns a set of poses), and compute
\bea\label{eq:sampleworstbound}
\underline{d}^2_{\epsilon,\lambda} = \min \cbrace{  {d}^2_{i, \epsilon,\lambda} = \max_{(R,t) \in \Seps} \lambda \Fnorm{R - R_i}^2 + (1-\lambda) \norm{ t - t_i}^2  }_{i=1}^M,
\eea
which first solves~\eqref{eq:pose2purse} (inner ``$\max$'' in~\eqref{eq:sampleworstbound}) for each $(R_i,t_i)$ and then selects the minimum (tightest) error bounds. Note that we still apply a second-order SDP relaxation when computing the error bounds for each $(R_i,t_i)$ since~\eqref{eq:pose2purse} is nonconvex. 

Fig.~\ref{fig:avg-vs-smp} plots the cumulative distribution functions (CDF) of the error bounds under the {\gtball} setup with $\epsilon=0.1$. The blue curves plot the CDF of the error bounds computed for the average pose, while the red curves plot the CDF of the error bounds computed from solving~\eqref{eq:sampleworstbound}. We can see that solving~\eqref{eq:sampleworstbound} does slightly improve the tightness of the translation bounds (while the rotation bounds are very close). Considering that we only select the minimum error bounds from $M=5$ samples, we conjecture solving the minimax problem can give us much tighter error bounds, and we leave this as an exciting future research.

\input{sections/supp-fig-avg_vs_smp}