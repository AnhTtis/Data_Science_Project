\section{Methodology}
An overview of our natural language-assisted sign language recognition (NLA-SLR) framework is shown in Figure \ref{fig:framework}. Our framework mainly consists of three parts: 1) data pre-processing which generates video-keypoint pairs as network inputs (Section~\ref{sec:data}); 2) a video-keypoint network (VKNet) which takes video-keypoint pairs of various temporal receptive fields as inputs for vision feature extraction (Section~\ref{sec:vknet}); 3) a head network (Section~\ref{sec:head}) containing a language-aware label smoothing branch (Section~\ref{sec:lang_lbsm}) and an inter-modality mixup branch (Section~\ref{sec:vl_mixup}). We empirically find that Mixup~\cite{zhang2018mixup} can be applied on both RGB videos and keypoint heatmap sequences, which will be described in Section~\ref{sec:intra}.


% \vspace{-2mm}
\subsection{Data Pre-Processing}
\label{sec:data}
Sign languages are visual languages which adopt handshape, facial expression, and body movement to convey information.
% 
To more effectively model sign languages, we propose to model human body keypoints besides RGB videos to enhance the robustness of visual representations. 

Concretely, given a temporally cropped video $\boldsymbol{V} \in \mbb{R}^{T\times H_{V} \times W_{V} \times 3}$ with $T=64$ frames \cite{li2020transferring} and a spatial resolution of $H_V=W_V=224$, we use HRNet~\cite{sun2019deep} trained on COCO-WholeBody~\cite{jin2020whole} to estimate its 63 keypoints (11 for upper body, 10 for mouth, and 42 for two hands) per frame. The keypoints of the $t$-th frame are represented as a heatmap $\boldsymbol{K}_t\in\mathbb{R}^{H_K \times W_K \times K}$, where $H_K=W_K=112$ denote the height and width of the heatmap, and $K=63$ is the keypoint number. The elements within the heatmap $\boldsymbol{K}_t$ are generated by a Gaussian function: $\boldsymbol{K}_t[i,j,k] = \exp (-[(i-x_t^k)^2 + (j-y_t^k)^2] / 2\sigma^2)$, where $(i, j)$ represents the spatial index, $k$ is the keypoint index, $(x_t^k, y_t^k)$ denotes the coordinate of the $k$-th estimated keypoint of the $t$-th frame, and $\sigma=4$ controls the scale of the keypoints. We repeatedly generate the heatmaps for all frames and stack them along the temporal dimension into a keypoint heatmap sequence $\boldsymbol{K} \in \mathbb{R}^{T \times H_K \times W_K \times K}$. Now the 64-frame training sample is processed as a video-keypoint pair denoted as $(\boldsymbol{V}_{64}, \boldsymbol{K}_{64})$. Finally, we temporally crop a 32-frame counterpart $(\boldsymbol{V}_{32}, \boldsymbol{K}_{32})$ and feed it along with the 64-frame video-keypoint pair $(\boldsymbol{V}_{64}, \boldsymbol{K}_{64})$ into the VKNet to extract more robust vision features, which will be described in the next section.



\subsection{Video-Keypoint Network}
\label{sec:vknet}
\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{figures/VKNetV2.pdf}
\vspace{-7mm}
\caption{Our VKNet consists of two sub-networks, VKNet-64 and VKNet-32, which take video-keypoint pairs with different temporal receptive fields as inputs and output a set of vision features via global average pooling (GAP) layers. Within the VKNet, bidirectional lateral connections \cite{duan2022revisiting} are applied to the outputs of the first four S3D blocks (B1-B4) for video-video, keypoint-keypoint, and video-keypoint information exchange.}
\vspace{-5mm}
\label{fig:vknet}
\end{figure}

An illustration of the proposed video-keypoint network (VKNet) is shown in Figure~\ref{fig:vknet}. VKNet is composed of two sub-networks, namely VKNet-32 and VKNet-64, which take $(\boldsymbol{V}_{32}, \boldsymbol{K}_{32})$ and $(\boldsymbol{V}_{64}, \boldsymbol{K}_{64})$ as inputs, respectively. The network architectures of VKNet-32 and VKNet-64 are identical—either has a two-stream architecture consisting of a video encoder and a keypoint encoder. Since we denote keypoints as heatmaps, it is feasible to utilize any existing convolutional neural networks to extract keypoint features. In this work, S3D \cite{xie2018rethinking} with five blocks (B1--B5) is served as our video/keypoint encoder due to its excellent accuracy-speed trade-off. In our implementation, VKNet-32 (VKNet-64) is composed of two separate S3D networks with bidirectional lateral connections \cite{duan2022revisiting} applied to the outputs of the first four blocks (B1--B4). Specifically, VKNet-32 (VKNet-64) takes RGB video $\boldsymbol{V}_{32}$ ($\boldsymbol{V}_{64}$) and keypoint heatmap sequence $\boldsymbol{K}_{32}$ ($\boldsymbol{K}_{64}$) as inputs to extract the video feature $\boldsymbol{f}_{32}^{V}$ ($\boldsymbol{f}_{64}^{V}$) and the keypoint feature $\boldsymbol{f}_{32}^{K}$ ($\boldsymbol{f}_{64}^{K}$), respectively. 
We further concatenate $\boldsymbol{f}_{32}^{V}$ ($\boldsymbol{f}_{64}^{V}$) and $\boldsymbol{f}_{32}^{K}$ ($\boldsymbol{f}_{64}^{K}$) to generate $\boldsymbol{f}_{32}$ ($\boldsymbol{f}_{64}$) as the output of VKNet-32 (VKNet-64). The final feature $\boldsymbol{f}$ extracted by VKNet is the concatenation of $\boldsymbol{f}_{32}$ and $\boldsymbol{f}_{64}$.



It is worth mentioning that VKNet-32 and VKNet-64 are not two independent networks, we also introduce bidirectional lateral connections \cite{duan2022revisiting} to the corresponding encoders of the same input modality for video-video and keypoint-keypoint information exchange.



\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{figures/vl_mixup_V2.pdf}
\vspace{-7mm}
\caption{
The architecture of our head network. Language-aware label smoothing generates soft labels whose smoothing weights are the normalized semantic similarities between the ground truth and remaining glosses within the sign vocabulary. Inter-modality mixup generates inter-modality features and the corresponding labels to maximize the signs' separability in a latent space.
Integration between FC1 and FC2 can further boost SLR performance.
}
\vspace{-5mm}
\label{fig:vl_mixup}
\end{figure}


\subsection{Head Network}
\label{sec:head}
Figure~\ref{fig:vl_mixup} illustrates our head network, which is composed of a language-aware label smoothing branch and an inter-modality mixup branch.

\vspace{-4mm}
\subsubsection{Language-Aware Label Smoothing}
\vspace{-2mm}
\label{sec:lang_lbsm}
The classical label smoothing \cite{szegedy2016rethinking, he2019bag} was first proposed as a regularization technique to alleviate overfitting and make the model more adaptable.
% alleviate the negative effects raised by the mislabeled training data. 
Specifically, given a training sample belonging to the $b$-th class, label smoothing replaces the one-hot label with a soft label $\boldsymbol{y} \in \mathbb{R}^{N}$ which is defined as:
\begin{equation}
\label{equ:vani_lbsm}
\boldsymbol{y}[i]=
\begin{cases}
    1-\epsilon \ &\text{if}\  i=b, \\
    \epsilon/(N-1) \  &\text{otherwise},
\end{cases}
\end{equation}
where $\epsilon$ is a small constant (\eg, 0.2) and $N$ denotes the class number.

The vanilla label smoothing uniformly distributes $\epsilon$ to $N-1$ negative terms while the implicit semantics contained in glosses (sign labels) are ignored. In Section~\ref{sec:intro}, we discuss the phenomenon that visually indistinguishable signs (VISigns) may have similar semantic meanings (\textit{finding-1}). Motivated by this finding, we present a novel regularization strategy termed language-aware label smoothing, which assigns biased smoothing weights on the basis of semantic similarities of glosses to ease the training.

%
\noindent\textbf{Gloss Features.} Gloss is identified by a word which is associated with the sign’s semantic meaning. Thus any word representation learning framework can be adopted to extract gloss features for semantic similarity assessment. Concretely, given a sign vocabulary containing $N$ glosses, we leverage fastText~\cite{mikolov2018advances} pretrained on Common Crawl to extract a 300-$d$ feature for each gloss. We use $\boldsymbol{E}\in\mathbb{R}^{N \times 300}$ to denote the $N$ gloss features.

\noindent\textbf{Language-Aware Label Smoothing and Loss Function.} As shown in Figure~\ref{fig:vl_mixup}, given a training sample whose label is the $b$-th gloss, we first use fastText to extract its gloss feature $\boldsymbol{e} \in \mathbb{R}^{300}$. Then we compute the cosine similarities of the $b$-th gloss and all $N$ glosses within the sign vocabulary by $\boldsymbol{s}=\|\boldsymbol{E}\|_2 \|\boldsymbol{e}\|_2^T \in \mathbb{R}^{N}$, where $\|\cdot\|_2$ denotes row-wise L2-norm. The proposed language-aware label smoothing generates a soft label $\boldsymbol{y} \in \mathbb{R}^{N}$ as:
\begin{equation}
\label{equ:lang_lbsm}
\boldsymbol{y}[i]=
\begin{cases}
    1-\epsilon \ &\text{if}\  i=b, \\
    \epsilon \cdot \frac{\exp{(\boldsymbol{s}[i]/\tau)}}{\sum_{i=1,i\neq b}^N \exp{(\boldsymbol{s}[i]/\tau)}} \  &\text{otherwise},
\end{cases}
\end{equation}
where $\tau$ denotes a temperature parameter \cite{chen2020simple}.
The classification loss $\mathcal{L}_{CLS}$ is a simple cross-entropy loss applied on the prediction and soft label $\boldsymbol{y}$.




\vspace{-4mm}
\subsubsection{Inter-Modality Mixup}
\vspace{-2mm}
\label{sec:vl_mixup}
In Section~\ref{sec:intro}, we observe that VISigns may have distinct semantic meanings (\textit{finding-2}), motivating us to make use of the semantic meanings of glosses to maximize signs' separability in the latent space. To achieve the goal, as shown in Figure~\ref{fig:vl_mixup}, we introduce the inter-modality mixup, which generates the inter-modality features by combining the vision feature and gloss features to predict the corresponding inter-modality labels.
%

\noindent\textbf{Inter-Modality Mixup and Loss Function.} Given the vision feature $\boldsymbol{f}\in \mathbb{R}^{D}$ extracted by our VKNet and the gloss features $\boldsymbol{E} \in \mathbb{R}^{N\times 300}$ encoded by the fastText, we first use a fully-connected (FC) layer to map $\boldsymbol{E}$ to the dimension of $N \times D$. After that, we integrate the vision feature $\boldsymbol{f}$ and the mapped gloss features $\bar{\boldsymbol{E}}$ via a broadcast addition operation into the inter-modality features $\boldsymbol{F} = \boldsymbol{f} + \bar{\boldsymbol{E}} \in\mathbb{R}^{N \times D}$. The $n$-th row of $\boldsymbol{F}$ (denoted as $\boldsymbol{F}^n$), which is the combination of the vision feature (whose corresponding ground truth is the $b$-th gloss) and the $n$-th gloss feature, is associated with the inter-modality labels $\boldsymbol{y}^n \in \mathbb{R}^{N}$:
\begin{equation}
\boldsymbol{y}^n[i] = 
\begin{cases}
    0.5 \quad \text{if}\  i=b~\text{or}~i=n, \\
    0 ~~~\quad \text{otherwise}.
\end{cases}
\end{equation}
Note that as a special case, we set $\boldsymbol{y}^n[b]=1.0$ when $n=b$. Then we feed $\boldsymbol{F}^n$ into a classification layer to generate its prediction $\boldsymbol{p}^n \in (0,1)^{N}$, and use cross-entropy loss to approximate $\boldsymbol{y}^n$:
\begin{equation}
    \mathcal{L}_{IMM}^{n} = -\sum_{i=1}^{N}\boldsymbol{y}^n[i]\text{log}(\boldsymbol{p}^n[i]).
\end{equation}
Similarly, we could obtain the predictions of $N$ inter-modality features and their corresponding labels. The overall loss of inter-modality mixup is the average of $N$ cross-entropy losses:
\begin{equation}
    \mathcal{L}_{IMM} = \frac{1}{N}\sum_{n=1}^{N}\mathcal{L}_{IMM}^{n}.
\end{equation}
It is worth noting that $\mathcal{L}_{IMM}$ is an auxiliary loss and we drop the inter-modality mixup branch in the inference stage.

\noindent\textbf{Boost Sign Language Recognition via the Integrated Classification Layer.} As shown in Figure \ref{fig:vl_mixup}, we term the classification layer in the language-aware label smoothing branch and inter-modality mixup branch as FC1 and FC2, respectively. Though the inter-modality mixup only attends the training, the well-optimized FC2 contains implicit knowledge of recognizing signs with the help of language information. This inspires us to integrate FC2 into FC1 to boost sign language recognition. Concretely, the parameters of the FC1 are updated by a weighted sum of its own parameters and the FC2's parameters at each iteration, which can be formulated as:
\begin{equation}
\label{equ:ema}
\begin{split}
% 
\theta_1, \theta_2 &\leftarrow \text{optimizer}(\theta_1, \theta_2, \nabla_{\theta_1}\mathcal{L}, \nabla_{\theta_2}\mathcal{L}, \eta) \\
\theta_1 &\leftarrow \mu\theta_1 + (1-\mu)\theta_2,
\end{split}
\end{equation}
where $\theta_1$ and $\theta_2$ denote the parameters of FC1 and FC2, respectively, $\mathcal{L}$ is the overall loss of the head network introduced in Section \ref{sec:overall_loss}, $\eta$ is the learning rate, and $\mu$ controls the contribution of $\theta_2$.



\begin{table*}[t]
\setlength\tabcolsep{4pt}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc|cc}
\toprule
\multirow{3}{*}{Method} & \multicolumn{4}{c|}{MSASL1000} & \multicolumn{4}{c|}{MSASL500} & \multicolumn{4}{c|}{MSASL200} & \multicolumn{4}{c}{MSASL100} \\
\cmidrule(){2-17}
& \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c|}{Per-class} & \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c|}{Per-class} & \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c|}{Per-class} & \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c}{Per-class} \\
& Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 \\

\midrule
I3D \cite{I3D} & -- & -- & 57.69 & 81.08 & -- & -- & 72.50 & 89.80 & -- & -- & 81.97 & 93.79 & -- & -- & 81.76 & 95.16 \\
I3D+BLSTM \cite{I3D,lstm} & 40.99 & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & 72.07 & -- & -- & -- \\
ST-GCN \cite{yan2018spatial} & 36.03 & 59.92 & 32.32 & 57.15 & -- & -- & -- & -- & 52.91 & 76.67 & 54.20 & 77.62 & 59.84 & 82.03 & 60.79 & 82.96 \\
BSL (multi-crop) \cite{albanie2020bsl} & 64.71 & 85.59 & 61.55 & 84.43 & -- & -- & -- & --  & -- & --  & -- & --  & -- & --  & -- & -- \\
TCK$\dagger$ \cite{li2020transferring} & -- & -- & -- & -- & -- & -- & -- & -- & 80.31 & 91.82 & 81.14 & 92.24 & 83.04 & 93.46 & 83.91 & 93.52 \\
HMA \cite{hu2021hand} & 69.39 & 87.42 & 66.54 & 86.56 & -- & -- & -- & -- & 85.21 & 94.41 & 86.09 & 94.42  & 87.45 & 96.30  & 88.14 & 96.53 \\
BEST \cite{best} & 71.21 & 88.85 & 68.24 & 87.98 & -- & -- & -- & -- & 86.83 & 95.66 & 87.45 & 95.72 & 89.56 & 96.96 & 90.08 & 97.07 \\
SignBERT$\dagger$ \cite{hu2021signbert} & 71.24 & 89.12 & 67.96 & 88.40 & -- & -- & -- & -- & 86.98 & 96.39 & 87.62 & 96.43 & 89.56 & 97.36 & 89.96 & 97.51 \\
\midrule

NLA-SLR (Ours) & 72.56 & 89.12 & 69.86 & 88.48 & 81.62 & 93.09 & 81.36 & 93.39 & 88.74 & 96.17 & 89.23 & 96.38 & 90.49 & 97.49 & 91.04 & 97.92 \\
NLA-SLR (Ours, 3-crop) & \tbf{73.80} & \tbf{89.65} & \tbf{70.95} & \tbf{89.07} & \tbf{82.90} & \tbf{93.46} & \tbf{83.06} & \tbf{93.54} & \tbf{89.48} & \tbf{96.69} & \tbf{89.86} & \tbf{96.93} & \tbf{91.02} & \tbf{97.89} & \tbf{91.24} & \tbf{98.19} \\
\bottomrule
\end{tabular}}
\vspace{-3mm}
\caption{Comparison with previous works on MSASL. The results of I3D, I3D+BLSTM, and ST-GCN are reproduced by \cite{joze2019ms}, \cite{adaloglou2021comprehensive}, and \cite{hu2021signbert}, respectively. BSL achieves multi-crop inference by sliding a window with a stride of 8 frames. ($\dagger$denotes methods using extra data.)}
\vspace{-5mm}
\label{tab:sota_msasl}
\end{table*}


\vspace{-4mm}
\subsubsection{Overall Loss}
\vspace{-2mm}
\label{sec:overall_loss}
The loss $\mathcal{L}$ of the head network is the sum of the classification loss $\mathcal{L}_{CLS}$ and the inter-modality mixup loss $\mathcal{L}_{IMM}$ with a trade-off hyper-parameter $\gamma$: $\mathcal{L} = \mathcal{L}_{CLS} + \gamma \mathcal{L}_{IMM}$.
Note that we apply the head network to each vision feature in Figure \ref{fig:vknet} independently, and the overall loss for the whole model is the sum of the loss of each head network.

\subsection{Intra-Modality Mixup}
\label{sec:intra}
We empirically find that Mixup~\cite{zhang2018mixup} is helpful for sign language recognition. In contrast to the traditional Mixup which is applied to images and videos, we adopt the Mixup regularization on both RGB videos and keypoint heatmap sequences. For a distinction with our proposed Inter-Modality Mixup, we term the classical Mixup as Intra-Modality Mixup in our work.





