\section{More Implementation Details}
\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
     \centering
     \includegraphics[width=0.8\textwidth]{figures/bilateral_a.pdf}
     \caption{Lateral connection from the 64-frame video encoder to the 64-frame keypoint encoder. We use a 2D convolution layer with a kernel size of $3\times 3$ and a stride of 2 to match the spatial resolutions. The same holds true for the 32-frame input.}
     \label{fig:bilat_A}
    \end{subfigure}
    \quad\quad
    \begin{subfigure}[b]{0.45\textwidth}
     \centering
     \includegraphics[width=0.8\textwidth]{figures/bilateral_b.pdf}
     \caption{Lateral connection from the 64-frame keypoint encoder to the 64-frame video encoder. We use a 2D transposed convolution layer with a kernel size of $3\times 3$ and a stride of 2 to match the spatial resolutions. The same holds true for the 32-frame input.}
     \label{fig:bilat_B}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.45\textwidth}
     \centering
     \includegraphics[width=0.8\textwidth]{figures/bilateral_c.pdf}
     \caption{Lateral connection from the 64-frame video encoder to the 32-frame one. We use a 1D convolution layer with a kernel size of 3 and a stride of 2 to match the temporal resolutions. The same holds true for the keypoint encoder.}
     \label{fig:bilat_C}
    \end{subfigure}
    \quad\quad
    \begin{subfigure}[b]{0.45\textwidth}
     \centering
     \includegraphics[width=0.8\textwidth]{figures/bilateral_d.pdf}
     \caption{Lateral connection from the 32-frame video encoder to the 64-frame one. We use a 1D transposed convolution layer with a kernel size of 3 and a stride of 2 to match the temporal resolutions. The same holds true for the keypoint encoder.}
     \label{fig:bilat_D}
    \end{subfigure}

    \caption{Illustration of the lateral connections. Note that we split bidirectional lateral connections into unidirectional ones for better illustration.}
    \label{fig:bilat}
\end{figure*}

\noindent\textbf{Bidirectional Lateral Connections.}
We apply bidirectional lateral connections \cite{duan2022revisiting} to the first four S3D blocks for video-video, keypoint-keypoint, and video-keypoint information exchange.
For video-keypoint connections (dashed lines in Figure \textcolor{red}{4} in the main paper), since the input spatial resolutions of the video and keypoint encoder are $224\times224$ and $112\times112$, respectively, we use 2D convolution (Figure \ref{fig:bilat_A}) and transposed convolution layers (Figure \ref{fig:bilat_B}) with a stride of 2 and a kernel size of $3\times3$ to match the spatial resolutions.
For video-video and keypoint-keypoint connections (dotted dashed lines in Figure \textcolor{red}{4} in the main paper), due to the input length difference, we use 1D convolution (Figure \ref{fig:bilat_C}) and transposed convolution layers (Figure \ref{fig:bilat_D}) with a stride of 2 and a kernel size of 3 to match the temporal resolutions. Figure~\ref{fig:bilat} shows the bidirectional lateral connections.

\begin{figure*}[t]
\centering
\includegraphics[width=0.7\linewidth]{figures/kp_illustration.pdf}
\caption{Illustration of the keypoints (11 upper body keypoints, 10 mouth keypoints, and 42 hand keypoints) used in our VKNet.}
\label{fig:kp_illustration}
\end{figure*}

\noindent\textbf{Keypoint Illustration.} We show the keypoints used in our VKNet in Figure \ref{fig:kp_illustration}. The keypoints are estimated by HRNet~\cite{sun2019deep} trained on COCO-WholeBody~\cite{jin2020whole}.
We use a subset of keypoints including 11 upper body keypoints, 10 mouth keypoints, and 42 hand keypoints.


\section{More Experiments}
\noindent\tbf{Head Choices for Inter-Modality Mixup.}
By default, we apply our inter-modality mixup on all head networks.
To validate the effectiveness of this setting, we further conduct experiments on only applying it on partial heads.
We categorize the head networks into three groups: video heads with input features $(\boldsymbol{f}_{64}^V, \boldsymbol{f}_{32}^V)$, keypoint heads with input features $(\boldsymbol{f}_{64}^K, \boldsymbol{f}_{32}^K)$, and joint heads with input features $(\boldsymbol{f}_{64}, \boldsymbol{f}_{32}, \boldsymbol{f})$. See Figure \textcolor{red}{4} in the main paper for their definitions.
Table \ref{tab:abl_head} shows that applying the inter-modality mixup on either one group of heads outperforms the baseline, and our default setting, applying the inter-modality mixup on all heads, achieves the best performance.

\begin{table}[t]
% \setlength\tabcolsep{4pt}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ccc|cc|cc}
\toprule
\multirow{2}{*}{Video} & \multirow{2}{*}{Keypoint} & \multirow{2}{*}{Joint} & \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c}{Per-class} \\
& & & Top-1 & Top-5 & Top-1 & Top-5 \\

\midrule
& & & 59.56 & 90.10 & 56.77 & 89.33 \\
\checkmark & & & 60.42 & 91.07 & 57.62 & 90.37 \\
& \checkmark & & 60.08 & 90.62 & 57.27 & 89.76 \\
& & \checkmark & 59.83 & 90.72 & 56.88 & 90.11 \\
\checkmark & \checkmark & & 60.56 & 91.24 & 57.87 & 90.37 \\
\checkmark & \checkmark & \checkmark & \tbf{61.05} & \tbf{91.45} & \tbf{58.05} & \tbf{90.70} \\
\bottomrule
\end{tabular}
}
% \vspace{-2mm}
\caption{Ablation studies on applying inter-modality mixup on different types of head networks.}
\label{tab:abl_head}
% \vspace{-2mm}
\end{table}


\begin{table}[ht]
\setlength\tabcolsep{3pt}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{cccc|cc|cc}
\toprule
\multirow{2}{*}{Upper Body} & \multirow{2}{*}{Hand} & \multirow{2}{*}{Mouth} & \multirow{2}{*}{\#Keypoints} &  \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c}{Per-class} \\
& & & & Top-1 & Top-5 & Top-1 & Top-5 \\

\midrule
\checkmark & & & 11 & 21.37 & 50.66 & 19.78 & 49.00 \\
\checkmark & \checkmark & & 53 & 48.54 & 81.45 & 45.52 & 79.94 \\
& \checkmark & \checkmark & 52 & 48.64 & 81.83 & 45.64 & 80.36 \\
\checkmark & \checkmark & \checkmark & 63 & \tbf{49.10} & \tbf{82.00} & \tbf{46.18} & \tbf{80.71}\\

\bottomrule
\end{tabular}
}
\caption{Ablation study on keypoint selection.}
\label{tab:abl_kp}
%\vspace{-2mm}
\end{table}

\noindent\textbf{Keypoint Selection.} We utilize HRNet \cite{sun2019deep} trained on COCO-WholeBody \cite{jin2020whole} to estimate 63 keypoints (11 for upper body, 42 for hands, and 10 for mouth) per frame.
As shown in Table \ref{tab:abl_kp}, we validate the effectiveness of each keypoint group by training several single-stream keypoint encoders.
Only using upper body keypoints yields the lowest top-1 accuracy (21.37\%).
Employing hand keypoints significantly improves the top-1 accuracy by 27.17\%.
This result is also consistent to the fact that sign languages mainly convey information by signers' hand movement.
Finally, the mouth keypoints also have a positive effect since signers usually mouth the words during signing.


\begin{table}[t]
% \setlength\tabcolsep{4pt}
\centering
% \resizebox{\linewidth}{!}{
\begin{tabular}{ccc|cc|cc}
\toprule
\multirow{2}{*}{V-V} & \multirow{2}{*}{K-K} & \multirow{2}{*}{V-K} & \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c}{Per-class} \\
& & & Top-1 & Top-5 & Top-1 & Top-5 \\

\midrule
& & & 56.85 & 86.87 & 53.34 & 85.60 \\
\checkmark & & & 57.12 & 87.11 & 54.21 & 85.94 \\
\checkmark & \checkmark & & 57.16 & 87.56 & 54.03 & 86.54 \\
\checkmark & \checkmark & \checkmark & \tbf{57.19} & \tbf{88.29} & \tbf{54.35} & \tbf{87.49} \\
\bottomrule
\end{tabular}
% }
\caption{Ablation studies on different types of bidirectional lateral connections. (V-V: video-video; K-K: keypoint-keypoint; V-K: video-keypoint.)}
\label{tab:abl_lat}
\end{table}


\noindent\textbf{Bidirectional Lateral Connections.} Within the VKNet, we apply bidirectional lateral connections \cite{duan2022revisiting} for video-video, keypoint-keypoint, and video-keypoint information exchange. See Figure \textcolor{red}{4} in the main paper for their illustration.
As shown in Table \ref{tab:abl_lat}, each type of bidirectional lateral connections has a positive effect on model performance, and our default setting, using all of the three types of the lateral connections, can achieve the best performance.


\begin{table}[t]
% \setlength\tabcolsep{4pt}
\centering
% \resizebox{\linewidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
\multirow{2}{*}{Method} &  \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c}{Per-class} \\
& Top-1 & Top-5 & Top-1 & Top-5 \\

\midrule
SlowFast & 56.81 & 87.60 & 53.69 & 86.68 \\
VKNet & \tbf{57.19} & \tbf{88.29} & \tbf{54.35} & \tbf{87.49} \\

\bottomrule
\end{tabular}
% }
\caption{Comparison between SlowFast and our VKNet.}
\label{tab:abl_slowfast}
%\vspace{-2mm}
\end{table}

\noindent\textbf{VKNet vs. SlowFast.} Our VKNet consists of two sub-networks, VKNet-64 and VKNet-32, to jointly model video-keypoint pairs with different temporal receptive fields.
The results in Table \textcolor{red}{4} in the main paper suggest that modeling different video-keypoint pairs with varied temporal receptive fields improves the model generalization capability. One network that is related to our VKNet is SlowFast \cite{feichtenhofer2019slowfast}, which consists of two streams taking RGB videos with low/high frame rate as inputs while having a fixed temporal receptive field.
For a fair comparison between SlowFast and our VKNet, we replace the ``temporal crop" operation in Figure \textcolor{red}{3} in the main paper with ``temporal sampling", \ie, sampling a 32-frame pair from the 64-frame one with a stride of 2 frames, to mimic the SlowFast.
As shown in Table \ref{tab:abl_slowfast}, our VKNet can consistently outperform SlowFast on all of the four metrics, showing that VKNet is a stronger backbone for sign language recognition.


\begin{table}[t]
% \setlength\tabcolsep{4pt}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
\multirow{2}{*}{Method} &  \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c}{Per-class} \\
& Top-1 & Top-5 & Top-1 & Top-5 \\

\midrule
Contrastive Learning & 59.90 & 91.28 & 57.23 & 90.59 \\
Inter-Modality Mixup & \tbf{61.05} & \tbf{91.45} & \tbf{58.05} & \tbf{90.70} \\

\bottomrule
\end{tabular}
}
\caption{Comparison between contrastive learning and our inter-modality mixup.}
\label{tab:abl_contras}
%\vspace{-2mm}
\end{table}

\noindent\textbf{Inter-Modality Mixup vs. Contrastive Learning.} Our inter-modality mixup blends vision and language features to better maximize the separability of signs.
Its effectiveness is shown in Table \textcolor{red}{6} in the main paper. One work that is related to our inter-modality mixup is CLIP \cite{clip}, which jointly trains an image encoder and a text encoder with a contrastive loss by maximizing the cosine similarity of positive image-text pairs while minimizing the similarity of negative pairs.
Following the practice in CLIP, we replace our inter-modality mixup loss $\mathcal{L}_{IMM}$ with a contrastive loss between the vision feature $\boldsymbol{f}$ and gloss features $\bar{\boldsymbol{E}}$.
As shown in Table \ref{tab:abl_contras}, our inter-modality mixup can consistently outperform the contrastive learning method on all of the four metrics. The results demonstrate that our inter-modality mixup is a more effective approach to exploit semantic information contained in glosses.
% since it enables a direct connection between vision and language information in the feature space.


\begin{table}[t]
% \setlength\tabcolsep{4pt}
\centering
% \resizebox{\linewidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
\multirow{2}{*}{Method} &  \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c}{Per-class} \\
& Top-1 & Top-5 & Top-1 & Top-5 \\

\midrule
Word2vec \cite{word2vec} & 60.63 & 91.14 & 57.53 & 90.42 \\
GloVe \cite{glove} & 60.81 & 90.90 & 57.73 & 90.27 \\
FastText \cite{mikolov2018advances} & \tbf{61.05} & \tbf{91.45} & \tbf{58.05} & \tbf{90.70} \\
BERT \cite{kenton2019bert} & 60.11 & 90.83 & 57.15 & 90.05 \\

\bottomrule
\end{tabular}
% }
\caption{Comparison among different word representation learning methods.}
\label{tab:abl_word}
%\vspace{-2mm}
\end{table}

\noindent\textbf{Word Representation Learning Methods.} We adopt fastText \cite{mikolov2018advances} as our default gloss feature extractor. Here we investigate other alternatives as shown in Table~\ref{tab:abl_word}. Word2vec \cite{word2vec} and GloVe \cite{glove} are two classical word representation learning methods which are widely-adopted in NLP community. They perform comparably to each other that GloVe achieves better results on the top-1 accuracy while word2vec is superior regarding to the top-5 accuracy. As an improvement of word2vec, fastText leads to better results on all of the four metrics. Finally, we also utilize an advanced model, BERT-base \cite{kenton2019bert}, to extract word representations by averaging the outputs of the last layer. However, it performs worse than all the other methods since it is not dedicated to word representation learning.



\section{Visualization}
\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{figures/word_sim.pdf}
\caption{Visualization of gloss feature similarities. We adopt fastText to extract gloss features.}
\label{fig:word_sim}
\end{figure}

\noindent\textbf{Gloss Feature Similarity.} The gloss feature similarities play a key role in our language-aware label smoothing.
We select several glosses from the vocabulary and visualize the cosine similarities between their gloss features as a heatmap in Figure \ref{fig:word_sim}.
We can see that the similarity matrix can roughly reflect the semantic similarities between glosses.
For example, the pairs: (``adapt", ``change"), (``city", ``community"), (``cold", ``winter"), (``speech", ``oral"), and (``accent", ``voice"), have high similarities, which are consistent to human understanding.


\begin{figure*}[t]
     \centering
     \begin{subfigure}[t]{0.95\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/heatmaps_wlasl.pdf}
         \caption{WLASL2000.}
         \label{fig:hmap_wlasl}
     \end{subfigure}

     \begin{subfigure}[t]{0.95\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/heatmaps_msasl.pdf}
         \caption{MSASL1000.}
         \label{fig:hmap_msasl}
     \end{subfigure}
     
     \begin{subfigure}[t]{0.95\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/heatmaps_nmf.pdf}
         \caption{NMFs-CSL.}
         \label{fig:hmap_nmf}
     \end{subfigure}

\caption{Visualizations for the randomly selected frames and their corresponding keypoint heatmaps estimated by HRNet.}
\label{fig:hmap}
\end{figure*}

\noindent\textbf{Keypoint Heatmaps.} As shown in Figure \ref{fig:hmap}, we visualize the keypoint heatmaps extracted by HRNet \cite{sun2019deep} by randomly selecting six frames of three signers from the test sets of WLASL2000, MSASL1000, and NMFs-CSL, respectively.
We can clearly see that the heatmaps are robust to signer appearances, background variations, hand positions, and palm orientations.


\begin{figure*}[t]
     \centering
     \begin{subfigure}[t]{0.95\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/qual_res_A.pdf}
         \caption{VISigns with \textit{similar} semantic meanings.}
         \label{fig:qual_res_A}
     \end{subfigure}

     \begin{subfigure}[t]{0.95\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/qual_res_B.pdf}
         \caption{VISigns with \textit{distinct} semantic meanings.}
         \label{fig:qual_res_B}
     \end{subfigure}

\caption{Qualitative results on WLASL2000. (Here for NLA-SLR, we do not use intra-modality mixup for a fair comparison. The ground-truth gloss is highlighted in \textcolor{red}{red}.)}
\label{fig:qual_res}
\end{figure*}





\section{Qualitative Results}
As shown in Figure \ref{fig:qual_res}, we conduct qualitative analysis for our NLA-SLR.
We find that compared with VKNet (baseline), our NLA-SLR can well classify visually indistinguishable signs (VISigns) with either similar or distinct meanings. 
As shown in Figure \ref{fig:qual_res_A}, our NLA-SLR can successfully distinguish (``doctor", ``nurse") and (``calculate", ``multiply"), which are VISigns with similar semantic meanings, whereas the baseline, VKNet, fails to classify them.
Besides, as shown in Figure \ref{fig:qual_res_B}, our NLA-SLR can also recognize VISigns with distinct semantic meanings: (``champion", ``mushroom") and (``friend", ``medicine").
We owe these success to the two proposed techniques: language-aware label smoothing and inter-modality mixup.


\section{Social Impact and Limitation}
Sign language is the primary communication method among the deaf community.
Thus, research on sign language recognition can help bridge the communication gap between the normal-hearing and hearing-impaired people.

The proposed method is data-driven.
Thus, the model performance may be affected by the biases in the training data.
Besides, our backbone relies on pre-extracted keypoints; inaccurate keypoint estimation may hurt the model performance.
We believe that stronger keypoint estimators may further improve sign language recognition in the future.