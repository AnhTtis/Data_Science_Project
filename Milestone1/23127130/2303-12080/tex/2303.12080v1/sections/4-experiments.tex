\section{Experiments}
\begin{table*}[t]
\setlength\tabcolsep{4pt}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc|cc}
\toprule
\multirow{3}{*}{Method} & \multicolumn{4}{c|}{WLASL2000} & \multicolumn{4}{c|}{WLASL1000} & \multicolumn{4}{c|}{WLASL300} & \multicolumn{4}{c}{WLASL100} \\
\cmidrule(){2-17}
& \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c|}{Per-class} & \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c|}{Per-class} & \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c|}{Per-class} & \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c}{Per-class} \\
& Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 \\

\midrule
OpenHands \cite{selvaraj2022openhands} & 30.60 & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
PSLR \cite{tunga2021pose} & -- & -- & -- & -- & -- & -- & -- & -- & 42.18 & 71.71 & -- & -- & 60.15 & 83.98 & -- & -- \\
I3D \cite{I3D} & 32.48 & 57.31 & -- & -- & 47.33 & 76.44 & -- & -- & 56.14 & 79.94 & -- & -- & 65.89 & 84.11 & -- & -- \\
ST-GCN \cite{yan2018spatial} & 34.40 & 66.57 & 32.53 & 65.45 & -- & -- & -- & -- & 44.46 & 73.05 & 45.29 & 73.16 & 50.78 & 79.07 & 51.62 & 79.47 \\
Fusion-3 \cite{hosain2021hand} & 38.84 & 67.58 & -- & -- & 56.68 & 79.85 & -- & -- & 68.30 & 83.19 & -- & -- & 75.67 & 86.00 & -- & -- \\
BSL (multi-crop) \cite{albanie2020bsl} & 46.82 & 79.36 & 44.72 & 78.47 & -- & -- & -- & --  & -- & --  & -- & --  & -- & --  & -- & -- \\
HMA \cite{hu2021hand} & 51.39 & 86.34 & 48.75 & 85.74 & -- & -- & -- & --  & -- & --  & -- & --  & -- & --  & -- & -- \\
TCK$\dagger$ \cite{li2020transferring} & -- & -- & -- & -- & -- & -- & -- & -- & 68.56 & 89.52 & 68.75 & 89.41 & 77.52 & 91.08 & 77.55 & 91.42 \\
BEST \cite{best} & 54.59 & 88.08 & 52.12 & 87.28 & -- & -- & -- & -- & 75.60 & 92.81 & 76.12 & 93.07 & 81.01 & 94.19 & 81.63 & 94.67 \\
SignBERT$\dagger$ \cite{hu2021signbert} & 54.69 & 87.49 & 52.08 & 86.93 & -- & -- & -- & -- & 74.40 & 91.32 & 75.27 & 91.72 & 82.56 & 94.96 & 83.30 & 95.00 \\
SAM-SLR* (5-crop) \cite{jiang2021skeleton} & 58.73 & 91.46 & 55.93 & \tbf{90.94} & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
SAM-SLR-v2* (5-crop) \cite{jiang2021sign} & 59.39 & 91.48 & 56.63 & 90.89 & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
\midrule

NLA-SLR (Ours) & 61.05 & 91.45 & 58.05 & 90.70 & 75.11 & \tbf{94.62} & 75.07 & \tbf{94.70} & 86.23 & \tbf{97.60} & 86.67 & \tbf{97.81} & 91.47 & \tbf{96.90} & 92.17 & \tbf{97.17} \\
NLA-SLR (Ours, 3-crop) & \tbf{61.26} & \tbf{91.77} & \tbf{58.31} & 90.91 & \tbf{75.64} & \tbf{94.62} & \tbf{75.72} & 94.65 & \tbf{86.98} & \tbf{97.60} & \tbf{87.33} & \tbf{97.81} & \tbf{92.64} & \tbf{96.90} & \tbf{93.08} & \tbf{97.17} \\

\bottomrule
\end{tabular}}
\vspace{-3mm}
\caption{Comparison with previous works on WLASL. The results of I3D and ST-GCN are reproduced by \cite{li2020word} and \cite{hu2021signbert}, respectively. BSL achieves multi-crop inference by sliding a window with a stride of 8 frames. ($\dagger$denotes methods using extra data. *denotes methods using much more modalities than ours, \eg, optical flow, depth map, and depth flow.)}
\label{tab:sota_wlasl}
\vspace{-5mm}
\end{table*}


\subsection{Datasets and Evaluation Metrics}
\noindent\textbf{Datasets.}
We evaluate our method on three public sign language recognition datasets: MSASL \cite{joze2019ms}, WLASL \cite{li2020word}, and NMFs-CSL \cite{hu2021global}.
\textit{MSASL} is an American sign language (ASL) dataset with a vocabulary size of 1,000. It consists of 16,054, 5,287, and 4,172 samples in the training, development (dev), and test set, respectively. It also released three subsets consisting of only the top 500/200/100 most frequent glosses.
\textit{WLASL} is the latest ASL dataset with a larger vocabulary size of 2,000. It consists of 14,289, 3,916, and 2,878 samples in the training, dev, and test set, respectively. Similar to MSASL, it also released three subsets consisting of 1,000/300/100 frequent glosses.
\textit{NMFs-CSL} is a challenging Chinese sign language (CSL) dataset involving many fine-grained non-manual features (NMFs). It consists of 25,608 and 6,402 samples in the training and test set with a vocabulary size of 1,067. However, since the dataset owners only provide label indexes instead of glosses, we cannot apply inter-modality mixup on it, and we have to replace our language-aware label smoothing with the vanilla one.


\noindent\tbf{Evaluation Metrics.}
Following \cite{hu2021signbert, hu2021hand, jiang2021sign}, we report both per-instance and per-class accuracy, which denote the average accuracy over instances and classes, on the test sets. Note that since NMFs-CSL is a balanced dataset, \ie, each class contains equal amount of samples, we only report per-instance accuracy on it.


\subsection{Implementation Details}
\noindent\tbf{Training Details and Hyper-parameters.}
The S3D backbone within VKNet-64/32 is first pretrained on Kinetics-400 \cite{kay2017kinetics}.
Then we separately pretrain the video and keypoint encoder within VKNet-64/32 on SLR datasets.
Finally, our VKNet is initialized with the pretrained VKNet-64 and VKNet-32.
Data augmentations include spatial cropping with a range of [0.7-1.0] and temporal cropping.
We adopt identical data augmentations for both RGB videos and heatmap sequences to maintain spatial and temporal consistency.
Unless otherwise specified, we set $\lambda \sim Beta(0.8, 0.8)$ for intra-modality mixup \cite{zhang2018mixup}, and $\epsilon=0.2$ and $\tau=0.5$ in Eq. \ref{equ:lang_lbsm}.
Similar to \cite{grill2020bootstrap}, we gradually increase $\mu$ in Eq. \ref{equ:ema} such that greater gradients of FC1 come from $\mathcal{L}_{CLS}$ in the late training stage since only FC1 is used during inference.
Specifically, $\mu=1-(1-\mu_{base})\cdot(\cos{(\pi m/M)}+1)/2$, where $\mu_{base}=0.99$, $m$ is the current epoch, and $M$ is the maximum number of epochs.
For the same reason, we gradually decrease the weight of $\mathcal{L}_{IMM}$ by $\gamma=(\cos{(\pi m/M)}+1)/2$.
The whole model is trained with a batch size of 32 for 100 epochs.
We use a cosine annealing schedule and an Adam optimizer \cite{adam} with a weight decay of $1e-3$ and an initial learning rate of $1e-3$.


\noindent\tbf{Inference.}
%----move to supplementary materials---
%We experimentally average the prediction probabilities of each head network on WLASL and NMFs-CSL, while only average the prediction probabilities of FC1 of each joint head network (corresponding to the three joint features in Section~\ref{sec:vknet}) for MSASL.
%----------------------------------------
We report results of single-crop and 3-crop inference for a comparison with state-of-the-art methods \cite{albanie2020bsl, jiang2021skeleton, jiang2021sign}. All ablation studies are conducted in the setting of single-crop inference. For 3-crop inference, we temporally crop videos at the start, middle, end of the raw video, and the average prediction is served as the final prediction.
%As an option, we repeat the above process on three temporally cropped clips, \ie, 3-crop, at the start, middle, end of the raw video, respectively, and average the prediction probabilities of each clip as the final outputs.
More details are in the supplementary materials.


\subsection{Comparison with State-of-the-art Methods}
\begin{table}[t]
% \setlength\tabcolsep{4pt}
\centering
\resizebox{0.75\linewidth}{!}{
\begin{tabular}{l|cc}
\toprule
Method & Top-1 & Top-5 \\

\midrule
% 
I3D$^\Diamond$ \cite{I3D} & 64.4 & 88.0 \\
TSM$^\Diamond$ \cite{lin2019tsm} & 64.5 & 88.7 \\
Slowfast$^\Diamond$ \cite{feichtenhofer2019slowfast} & 66.3 & 86.6 \\
GLE-Net \cite{hu2021global} & 69.0 & 88.1 \\
HMA \cite{hu2021hand} & 75.6 & 95.3 \\
SignBERT$\dagger$ \cite{hu2021signbert} & 78.4 & 97.3 \\
BEST \cite{best} & 79.2 & 97.1 \\
\midrule

NLA-SLR (Ours) & 83.4 & 98.3 \\
NLA-SLR (Ours, 3-crop) & \tbf{83.7} & \tbf{98.5} \\

\bottomrule
\end{tabular}
}
\vspace{-3mm}
\caption{Comparison with previous works on NMFs-CSL. ($^\Diamond$methods reproduced by GLE-Net. $\dagger$methods using extra data.)}
\label{tab:sota_nmf}
\vspace{-5mm}
\end{table}


\noindent\tbf{MSASL.}
Table \ref{tab:sota_msasl} shows a comprehensive comparison between other methods and ours on all the sub-splits of MSASL.
Our approach outperforms the previous best method SignBERT \cite{hu2021signbert}, which utilizes extra data, by 2.56\%/2.50\%/1.46\% on the 1,000/200/100 sub-splits regarding the top-1 accuracy, respectively.

\noindent\tbf{WLASL.}
We evaluate our method on all the sub-splits of WLASL as shown in Table \ref{tab:sota_wlasl}.
The previous state-of-the-art method, SAM-SLR-v2 \cite{jiang2021sign}, proposes a heavy multi-modal ensemble framework, which involves many modalities including RGB videos, keypoints, optical flow, depth map, and depth flow.
However, our method significantly outperforms SAM-SLR-v2 by 1.87\%/1.68\% in terms of the per-instance/class top-1 accuracy while using much fewer modalities (only RGB videos and keypoints).

\noindent\tbf{NMFs-CSL.}
Finally, as shown in Table \ref{tab:sota_nmf}, our approach also outperforms the previous best method BEST \cite{best} by a large margin (83.7\% \vs 79.2\% on top-1 accuracy).


\subsection{Ablation Studies}
We conduct ablation studies on WLASL following \cite{li2020transferring, jiang2021sign} due to its large vocabulary size.

\noindent\tbf{VKNet.}
We first validate the effectiveness of our backbone, VKNet. As shown in Table \ref{tab:abl_vknet}, two-stream models, VKNet-32/64, can significantly outperform single-stream models, Video/Keypoint-32/64, which validates the effectiveness of modeling both videos and keypoints.
Besides, 64-frame models can consistently outperform 32-frame ones as expected since longer inputs can provide more information for the model to classify sign videos.
However, our VKNet performs better than a single 64-frame model, VKNet-64, especially on the top-5 accuracy, which implies that the 64-frame and 32-frame inputs can complement each other and the difference of the temporal receptive fields can bring more knowledge to model training.


\begin{table}[t]
% \setlength\tabcolsep{4pt}
\centering
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c}{Per-class} \\
& Top-1 & Top-5 & Top-1 & Top-5 \\

\midrule
Video-32 & 45.73 & 81.10 & 42.69 & 79.90\\
Keypoint-32 & 46.66 & 79.95 & 43.81 & 78.49\\
VKNet-32 & 52.95 & 85.75 & 50.26 & 84.50\\
\midrule
Video-64 & 51.15 & 83.43 & 48.14 & 82.20\\
Keypoint-64 & 49.10 & 82.00 & 46.18 & 80.71\\
VKNet-64 & 56.95 & 87.00 & 54.13 & 86.05\\
\midrule
VKNet & \tbf{57.19} & \tbf{88.29} & \tbf{54.35} & \tbf{87.49}\\
\bottomrule
\end{tabular}
}
\vspace{-3mm}
\caption{Ablation studies on VKNet.}
\label{tab:abl_vknet}
\vspace{-5mm}
\end{table}




\noindent\tbf{Major Components of NLA-SLR.}
As shown in Table \ref{tab:abl_main}, we study the effects of the major components of our NLA-SLR framework: language-aware label smoothing (Lang-LS) and sign mixup (ensemble of the intra- and inter-modality mixup).
First, Lang-LS can improve the performance of the baseline, VKNet, by 1.22\%/1.11\% on the top-1 and top-5 accuracy, respectively, which validates the effectiveness of language-aware soft labels.
Besides, more performance gain comes from sign mixup, which significantly improves the top-1 accuracy from 57.19\% to 60.32\%.
% 
Finally, using both Lang-LS and sign mixup along with the VKNet can achieve the best performance: 61.05\%/91.45\% on the top-1 and top-5 accuracy, respectively.
Note that both of the major components introduce negligible extra cost: Lang-LS simply replace the one-hot labels with the language-aware soft labels; sign mixup merely introduces two extra fully-connected layers (one for mapping gloss features and the other one for auxiliary training) for each head network, and both of them are dropped during inference.

\begin{table}[t]
\setlength\tabcolsep{4pt}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cc|cc|cc}
\toprule
\multirow{2}{*}{VKNet} & \multirow{2}{*}{Lang-LS} & \multirow{2}{*}{Sign Mixup} & \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c}{Per-class} \\
& & & Top-1 & Top-5 & Top-1 & Top-5 \\

\midrule
\checkmark & & & 57.19 & 88.29 & 54.35 & 87.49 \\
\checkmark & \checkmark & & 58.41 & 89.40 & 55.74 & 88.67 \\
\checkmark & & \checkmark & 60.32 & 90.86 & 57.55 & 90.06 \\
\checkmark & \checkmark & \checkmark & \tbf{61.05} & \tbf{91.45} & \tbf{58.05} & \tbf{90.70} \\

\bottomrule
\end{tabular}
}
\vspace{-3mm}
\caption{Ablation studies for the major components of NLA-SLR. (Lang-LS: language-aware label smoothing.)}
\label{tab:abl_main}
\vspace{-3mm}
\end{table}


\begin{table}[t]
\setlength\tabcolsep{4pt}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{cc|cc|cc}
\toprule
\multicolumn{2}{c|}{Sign Mixup} & \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c}{Per-class} \\
Intra-Modality & Inter-Modality & Top-1 & Top-5 & Top-1 & Top-5 \\

\midrule
& & 58.41 & 89.40 & 55.74 & 88.67 \\
\checkmark & & 59.56 & 90.10 & 56.77 & 89.33 \\
& \checkmark & 59.66 & 90.10 & 56.72 & 89.20 \\
\checkmark & \checkmark & \tbf{61.05} & \tbf{91.45} & \tbf{58.05} & \tbf{90.70} \\

\bottomrule
\end{tabular}
}
\vspace{-3mm}
\caption{Ablation studies on sign mixup which is composed of intra-modality and inter-modality mixup.}
\label{tab:abl_signmixup}
\vspace{-3mm}
\end{table}

\noindent\textbf{Sign Mixup.}
Our sign mixup is composed of two parts: intra-modality mixup, which extends the vanilla mixup \cite{zhang2018mixup} to keypoint heatmaps, and inter-modality mixup, which aims to maximize the signs' separability with the help of language information.
As shown in Table \ref{tab:abl_signmixup}, either intra- or inter-modality mixup can improve the performance by more than 1\% on the top-1 accuracy.
In addition, intra- and inter-modality mixup are compatibleâ€”using both mixup techniques surpasses using either one of them.



\begin{table}[t]
\setlength\tabcolsep{4pt}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ccc|cc|cc}
\toprule
Auxiliary & Inte- & Loss & \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c}{Per-class} \\
Classifier & gration & Weight Decay & Top-1 & Top-5 & Top-1 & Top-5 \\

\midrule
& & & 59.56 & 90.10 & 56.77 & 89.33 \\
\checkmark & & & 59.87 & 90.31 & 57.07 & 89.57 \\
\checkmark & \checkmark & & 60.84 & 91.07 & 57.99 & 90.28 \\
\checkmark & \checkmark & \checkmark & \tbf{61.05} & \tbf{91.45} & \tbf{58.05} & \tbf{90.70} \\

\bottomrule
\end{tabular}
}
\vspace{-3mm}
\caption{Ablation studies for inter-modality mixup.}
\label{tab:abl_lang}
\vspace{-5mm}
\end{table}

\noindent\tbf{Inter-Modality Mixup.}
% 
As shown in Table \ref{tab:abl_lang}, we first study the effects of the auxiliary classifier, FC2 in Figure \ref{fig:vl_mixup}.
It only slightly improves the performance (0.31\% on top-1 accuracy).
Most performance gain (almost 1\% on the top-1 accuracy) comes from the integration of the two classifiers (FC1 and FC2 as described in Section~\ref{sec:vl_mixup}). The reason is that it enables the natural language information to propagate from FC2 to FC1, which is the primary classifier during inference.
Finally, the loss weight decay strategy of $\mathcal{L}_{IMM}$ also has a positive effect since it assures that more gradients for FC1 come from $\mathcal{L}_{CLS}$ in the late training stage.

\noindent\tbf{Language-aware Label Smoothing.}
We conduct a comprehensive comparison between the vanilla label smoothing and our language-aware label smoothing (Lang-LS) by varying the smoothing parameter $\epsilon$ from 0.1 to 0.3.
As shown in Table \ref{tab:abl_lbsm}, our Lang-LS consistently outperforms the vanilla one regardless of the value of $\epsilon$.
The results suggest that for SLR models, assigning biased smoothing weights to the soft labels on the basis of gloss feature similarities (Eq. \ref{equ:lang_lbsm}) is a stronger regularization technique than the uniform distribution in the vanilla label smoothing (Eq. \ref{equ:vani_lbsm}).



\begin{table}[t]
\centering
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{c|c|cc|cc}
\toprule
\multirow{2}{*}{$\epsilon$} & \multirow{2}{*}{Type} & \multicolumn{2}{c|}{Per-instance} & \multicolumn{2}{c}{Per-class} \\
& & Top-1 & Top-5 & Top-1 & Top-5 \\

\midrule
\multirow{2}{*}{0.1} & Vanilla & 59.83 & 90.72 & 56.90 & 90.10 \\
& Language & 60.15 & 91.35 & 57.30 & 90.68 \\
\midrule
\multirow{2}{*}{0.2} & Vanilla & 60.11 & 91.00 & 57.09 & 90.34 \\
& Language & \tbf{61.05} & \tbf{91.45} & \tbf{58.05} & \tbf{90.70} \\
\midrule
\multirow{2}{*}{0.3} & Vanilla & 60.01 & 90.97 & 57.01 & 90.12 \\
& Language & 60.49 & 91.31 & 57.44 & 90.67 \\

\bottomrule
\end{tabular}
}
\vspace{-3mm}
\caption{Comparison between the vanilla and language-aware label smoothing.}
\label{tab:abl_lbsm}
\vspace{-4mm}
\end{table}


\begin{table}[t]
% \setlength\tabcolsep{4pt}
\centering
% \scriptsize
\resizebox{\linewidth}{!}{
\begin{tabular}{l|lc|c|c}
\toprule
Method & VS-S & VS-D & Non-VS & Overall \\
% \#samples/percentage & 101/3.51 & 428/14.87 & 2,349/81.62 & 2,878/100.00 \\

\midrule
VKNet & 50.50 & 48.13 & 59.13 & 57.19 \\
+Lang-LS & 64.36 & 50.93 & 59.51 & 58.41 \\
+Lang-LS, Inter-Mixup & \textbf{65.35} & \textbf{56.07} & \textbf{60.07} & \textbf{59.66} \\

\bottomrule
\end{tabular}
}
\vspace{-3mm}
\caption{Quantitative results over VISigns. We report top-1 accuracy on WLASL2000. 
(VS-S/D: VISigns with similar/distinct semantic meanings.)
}
\label{tab:visign}
\vspace{-5mm}
\end{table}


\noindent \textbf{Presence and Quantitative Results of VISigns.}
To identify the VISigns appeared in the testing set, we first use our baseline model, VKNet, to get the highest prediction score $p_1$ (classified as gloss $g_1$) and the second highest prediction score $p_2$ (classified as gloss $g_2$) for each sample. Then we calculate the difference $\delta = p_1 - p_2$. If $\delta \leq 0.1$, we regard $g_1$ and $g_2$ as potential VISigns. Next, we calculate the gloss similarity $s$ of $g_1$ and $g_2$ via FastText. If $s\ge0.5$, we consider $g_1$ and $g_2$ as VS-S, otherwise, they are considered as VS-D. Finally, we invite native signers to filter out wrong cases. As a result, for WLASL with a vocabulary size of 2000, we get 101 instances covering 64 VS-S, 428 instances covering 270 VS-D, and 2349 instances covering 1666 non-VISigns (non-VS), respectively. As shown in Table \ref{tab:visign}, Lang-LS and Inter-Mixup yield the highest performance gains for VS-S (50.50 $\rightarrow$ 64.36) and VS-D (50.93 $\rightarrow$ 56.07), respectively, demonstrating that the improvements of our method derive from handling VISigns.