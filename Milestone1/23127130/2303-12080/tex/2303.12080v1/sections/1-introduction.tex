\vspace{-5mm}
\section{Introduction}
\label{sec:intro}
Sign languages are the primary languages for communication among deaf communities. On the one hand, sign languages have their own linguistic properties as most natural languages \cite{sandler2006sign, adaloglou2021comprehensive, yin2022mlslt}. On the other hand, sign languages are visual languages that convey information by the movements of the hands, body, head, mouth, and eyes, making them completely separate and distinct from natural languages \cite{stmc, zuo2022c2slr, chen2022simple}. This work dedicates to sign language recognition (SLR), which requires models to classify the isolated signs from videos into a set of glosses\footnote{Gloss is a unique label for a single sign. Each gloss is identified by a word which is associated with the signâ€™s semantic meaning.}. Despite its fundamental capacity of recognizing signs, SLR has a broad range of applications including sign spotting~\cite{varol2022scaling, momeni2022automatic, li2020transferring}, sign video retrieval~\cite{duarte2022sign}, sign language translation~\cite{li2020tspnet, shi2022open, chen2022simple}, and continuous sign language recognition~\cite{chen2022simple, adaloglou2021comprehensive}.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/teaser_A2.pdf}
         \caption{VISigns may have \textit{similar} semantic meanings.}
         \label{fig:teaser_A}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/teaser_B.pdf}
         \caption{VISigns may have \textit{distinct} semantic meanings.}
         \label{fig:teaser_B}
     \end{subfigure}
    %  \hfill
     \vspace{-2mm}
    \caption{Vision neural networks are demonstrated to be less effective to recognize visually indistinguishable signs (VISigns)~\cite{albanie2020bsl, li2020word, joze2019ms}. We observe that VISigns may have similar or distinct semantic meanings, inspiring us to leverage this characteristic to facilitate sign language recognition as illustrated in Figure~\ref{fig:teaser_vl}.}
    \vspace{-6mm}
    \label{fig:teaser}
\end{figure}


\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.43\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/teaser_D.pdf}
         \caption{Language-aware label smoothing.}
         \label{fig:teaser_D}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.51\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/teaser_E.pdf}
         \caption{Inter-modality mixup.}
         \label{fig:teaser_E}
     \end{subfigure}
    %  \hfill
     \vspace{-3mm}
    \caption{We incorporate natural language modeling into sign language recognition to promote recognition capacity. (a) Language-aware label smoothing generates a soft label for each training video, whose smoothing weights are the normalized semantic similarities of the ground truth gloss and the remaining glosses within the sign language vocabulary. (b) Inter-modality mixup yields the blended features (denoted by orange rectangles) with the corresponding mixed labels to maximize the separability of signs in a latent space.}
    \vspace{-5mm}
    \label{fig:teaser_vl}
\end{figure*}

Since the lexical items of sign languages are defined by the handshape, facial expression, and movement, the combinations of these visual ingredients are restricted inherently, yielding plenty of visually indistinguishable signs termed VISigns. VISigns are those signs with similar handshape and motion but varied semantic meanings. We show two examples (``Cold'' \vs ``Winter'' and ``Table'' \vs ``Afternoon'') in Figure~\ref{fig:teaser}. Unfortunately, vision neural networks are demonstrated to be less effective to accurately recognize the VISigns~\cite{albanie2020bsl, li2020word, joze2019ms}. Due to the intrinsic connections between sign languages and natural languages, the glosses, \ie, labels of signs, are semantically meaningful in contrast to the one-hot labels used in traditional classification tasks~\cite{kay2017kinetics, imagenet}. 
%
Thus, although the VISigns are challenging to be classified from the vision perspective, their glosses provide
serviceable semantics, which is, however, less taken into consideration in previous works~\cite{hu2021hand, hu2021signbert, hu2021global, li2020transferring, li2020word, joze2019ms, jiang2021skeleton, jiang2021sign}. Our work is built upon the following two findings. 




\textit{Finding-1: VISigns may have similar semantic meanings (Figure~\ref{fig:teaser_A}).}
Due to the observation that VISigns may have higher visual similarities, assigning hard labels to them may hinder the training since it is challenging for vision neural networks to distinguish each VISign apart. 
A straightforward way to ease the training is to replace the hard labels with soft ones as in well-established label smoothing~\cite{szegedy2016rethinking, he2019bag}.
However, how to generate proper soft labels is non-trivial.
The vanilla label smoothing~\cite{szegedy2016rethinking, he2019bag} assigns equal smoothing weights to all negative terms, which ignores the semantic information contained in labels.
In light of the \textit{finding-1} that VISigns may have similar semantic meanings and the intrinsic connections between sign languages and natural languages, we consider the semantic similarities among the glosses when generating soft labels. 
Concretely, for each training video, we adopt an off-the-shelf word representation framework, \ie, fastText~\cite{mikolov2018advances}, to pre-compute the semantic similarities of its gloss and the remaining glosses within the sign language vocabulary. 
Then we can properly generate a soft label for each training sample whose smoothing weights are the normalized semantic similarities. 
In this way, negative terms with similar semantic meanings to the ground truth gloss are assigned higher values in the soft label.
As shown in Figure~\ref{fig:teaser_D}, we term this process as language-aware label smoothing, which injects prior knowledge into the training.


\textit{Finding-2: VISigns may have distinct semantic meanings (Figure~\ref{fig:teaser_B}).} Although the VISigns are challenging to be classified from the vision perspective, the semantic meanings of their glosses may be distinguishable according to \textit{finding-2}. This inspires us to combine the vision features and gloss features to drive the model towards maximizing signs' separability in a latent space. Specifically, given a sign video, we first leverage our proposed backbone to encode its vision feature and the well-established fastText~\cite{mikolov2018advances} to extract the feature of each gloss within the sign language vocabulary. Then we independently integrate the vision feature and each gloss feature to produce a blended representation, which is further fed into a classifier to approximate its mixed label. We refer to this procedure as inter-modality mixup as shown in Figure~\ref{fig:teaser_E}. We empirically find that our inter-modality mixup significantly enhances the model's discriminative power. 


Our contributions can be summarized as follows:
\vspace{-2mm}
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{3pt}
    \item We are the first to incorporate natural language modeling into sign language recognition based on the discovery of VISigns. Language-aware label smoothing and inter-modality mixup are proposed to take full advantage of the linguistic properties of VISigns and semantic information contained in glosses.
    \item We take into account the unique characteristic of sign languages and present a novel backbone named video-keypoint network (VKNet), which not only models both RGB videos and human keypoints, but also derives knowledge from sign videos of various temporal receptive fields.
    \item Our method, termed natural language-assisted sign language recognition (NLA-SLR), achieves state-of-the-art performance on the widely-used SLR datasets including MSASL~\cite{joze2019ms}, WLASL~\cite{li2020word}, and NMFs-CSL~\cite{hu2021global}.
\end{itemize}


\begin{figure*}[t]
\centering
\includegraphics[width=0.98\linewidth]{figures/frameworkV3.pdf}
\vspace{-3mm}
\caption{An overview of our NLA-SLR. Given a training video, we temporally crop a 64-frame clip \cite{li2020transferring} and use HRNet~\cite{sun2019deep} trained on COCO-WholeBody~\cite{jin2020whole} to estimate its keypoint sequence which is represented by a set of heatmaps, yielding a 64-frame video-keypoint pair. Then we temporally crop a 32-frame counterpart and feed it along with the 64-frame pair into our proposed VKNet (Figure~\ref{fig:vknet}) to extract the vision feature. The head network (Figure~\ref{fig:vl_mixup}) has a two-branch architecture consisting of a language-aware label smoothing branch and an inter-modality mixup branch. We only retain the VKNet and the classification layer in the head network for inference.}
\vspace{-5mm}
\label{fig:framework}
\end{figure*}

