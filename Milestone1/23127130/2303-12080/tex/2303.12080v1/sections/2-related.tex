\vspace{-4mm}
\section{Related Works}
\noindent\textbf{Sign Language Recognition.} Sign language recognition (SLR) is a fundamental task in the field of sign language understanding.
Feature extraction plays a key role in an SLR model.
% 
Most recent SLR works \cite{jiang2021sign, jiang2021skeleton, hu2021signbert, hu2021hand, li2020transferring, li2020word, joze2019ms, hu2021global, stmc, zuo22_interspeech, vac} adopt CNN-based architectures, \eg, I3D \cite{I3D} and R3D \cite{qiu2017learning}, to extract vision features from RGB videos.
In this work, we adopt S3D \cite{xie2018rethinking} as the backbone of our VKNet due to its excellent accuracy-speed trade-off.

However, RGB-based SLR models may suffer from the large variation of video backgrounds. 
As a complement, some SLR works \cite{jiang2021skeleton, jiang2021sign, hu2021hand, hu2021signbert, chentwo} explore to jointly model RGB videos and keypoints.
For example, SAM-SLR \cite{jiang2021skeleton} uses graph convolutional networks (GCNs) to model pre-extracted keypoints.
HMA \cite{hu2021hand} and SignBERT \cite{hu2021signbert} propose to decode 3D hand keypoints from RGB videos.
A common deficiency of these works is that they need a dedicated network to model keypoints.
In this work, we represent keypoints as a sequence of heatmaps~\cite{duan2022revisiting, chentwo} so that the keypoint encoder of our VKNet can share the identical architecture with the video encoder.

To enable mini-batch training, previous works \cite{jiang2021sign, jiang2021skeleton, hu2021signbert, hu2021hand, li2020transferring, li2020word} crop fixed-length clips from raw videos as model inputs.
However, the model may overfit to the training videos of fixed temporal receptive fields.
In contrast, our VKNet is trained on videos with varied temporal receptive fields to improve its generalization capability.



\noindent\textbf{Word Representation Learning.}
Word2vec \cite{word2vec} and GloVe \cite{glove} are two classical word representation learning frameworks in the field of NLP.
Based on word2vec, fastText \cite{mikolov2018advances} improves word representations with several modifications including the use of sub-word information \cite{bojanowski2017enriching} and position independent features \cite{mnih2013learning}.
Although some advanced language models, \eg, BERT \cite{kenton2019bert}, can also be used to extract word representations, they are computationally intensive and are not dedicated to word representation learning.
In this paper, we adopt the lightweight but effective fastText, which is also used in a recent sign language translation work \cite{yin2021simulslt}, to pre-compute gloss (word) representations.


\noindent\textbf{Vision-Language Models.}
Recently, a majority of vision-language models \cite{clip, align, yao2022filip, gu2022wukong} learn visual representations on large-scale image-text pairs.
Among them, CLIP \cite{clip} is the pioneer to jointly optimize an image encoder and a text encoder through a contrastive loss. 
% 
Besides, the pre-trained CLIP can be generalized to various downstream tasks, \eg, semantic segmentation \cite{xu2022groupvit, li2021language, xu2021simple}, object detection \cite{du2022learning, rao2022denseclip}, image classification~\cite{zhou2022learning,huang2022unsupervised}, and style transfer \cite{patashnik2021styleclip, kwon2022clipstyler}.
In this work, we exploit the implicit knowledge included in glosses (sign labels), which is distinct from previous works on vision-language modeling.


\noindent\textbf{Multi-label Classification.} Real-world objects may have multiple semantic meanings, which motivates research on multi-label classification \cite{ridnik2021asymmetric, ke2022hyperspherical, zhang2013review, rajeswar2022multi, kim2022large} requiring models to map inputs to multiple possible labels.
Although the VISigns may be associated with the multi-label classification problem, most widely-adopted SLR datasets \cite{li2020word, joze2019ms, hu2021global} are singly labeled.
In this work, we deal with the VISigns by incorporating language information included in glosses.