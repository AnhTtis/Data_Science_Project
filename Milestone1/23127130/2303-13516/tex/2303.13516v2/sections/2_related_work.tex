\section{Related Work}
\myparagraph{Text-to-image synthesis} has advanced significantly since the seminal works~\cite{zhu2007text,mansimov2015generating}, thanks to improvements in model architectures~\cite{zhang2017stackgan, zhu2019dm,tao2020df,xu2018attngan,huang2022multimodal,dhariwal2021diffusion,wu2022nuwa,kang2023scaling,sauer2023stylegan,ding2022cogview2}, generative modeling techniques~\cite{reed2016generative, ho2020denoising,rombach2022high,saharia2022photorealistic,balaji2022ediffi,nichol2021glide,chang2023muse}, and availability of large-scale datasets~\cite{schuhmann2021laion}. Current methods can synthesize high-quality images with remarkable generalization ability, capable of composing different instances, styles, and concepts in unseen contexts. However, as these models are often trained on copyright images, it learns to mimic various artist styles~\cite{somepalli2022diffusion,shan2023glaze} and other copyrighted content~\cite{carlini2023extracting}. In this work, we aim to modify the pretrained models to prevent the generation of such images. %
To remove data from pre-trained GANs, Kong~\etal~\cite{kong2022data} add the redacted data to fake data,  apply standard adversarial loss, and show results on MNIST and CIFAR. Unlike their method, which requires time-consuming model re-training on the entire dataset,  our method can efficiently remove concepts without going through the original training set. Furthermore, we focus on large-scale text-based diffusion models. \nupur{Recent work of Schramowski et al.~\cite{schramowski2022safe} modify the inference process to prevent certain concepts from being generated. But we aim to ablate the concept from the model weights. Concurrent with our work, Gandikota~\etal~\cite{gandikota2023erasing} aims to remove concepts using a score-based formulation. The reader is encouraged to review their work. %
}

\myparagraph{Training data memorization and unlearning.} Several works have studied training data leaking~\cite{shokri2017membership,carlini2019secret,carlini2021extracting,carlini2022quantifying}, which can pose a greater security and privacy risk, especially with the use of web-scale uncurated datasets in deep learning. Recent works~\cite{somepalli2022diffusion,carlini2023extracting} have also shown that text-to-image models are susceptible to generating exact or similar copies of the training dataset for certain text conditions. Another line of work in machine unlearning~\cite{cao2015towards,ginart2019making,golatkar2020eternal,golatkar2021mixed,nguyen2020variational,bourtoule2021machine,tanno2022repairing,sekhari2021remember} explores data deletion at user's request after model training. %
However, existing unlearning methods~\cite{golatkar2020eternal,tanno2022repairing} typically require calculating information, such as Fisher Information Matrix, 
making them computationally infeasible for large-scale models with billions of parameters trained on billions of images. In contrast, our method can directly update model weights and ablate a target concept as fast as five minutes. 


\myparagraph{Generative model fine-tuning and editing.} Fine-tuning aims to adapt the weights of a pretrained generative model
to new domains~\cite{wang2018TransferGAN,noguchi2019SB,wang2019MineGAN,mo2020FreezeD,zhao2020leveraging,li2020few,ojha2021few,zhao2020differentiable,karras2020training,liu2020towards,gu2021lofgan,nitzan2022mystyle}, downstream tasks~\cite{wang2022pretraining,rombach2022high,zhang2023adding}, and test images~\cite{bau2020semantic,roich2022pivotal,pan2021exploiting,kawar2022imagic,hertz2022prompt,parmar2023zero}. %
Several recent works also explore fine-tuning text-to-image models to learn personalized or unseen concepts~\cite{kumari2022multi,gal2022image,ruiz2022dreambooth,gal2023designing} given a few exemplar images. Similarly, model editing~\cite{bau2020rewriting,wang2022rewriting,gal2022stylegan,wang2021sketch,nitzan2023domain,meng2022locating,mitchell2021fast,meng2022mass} aims to modify specific model weights based on users' instructions to incorporate new computational rules or new visual effects.
Unlike the above approaches, our method reduces the possible space by ablating specific concepts in the pretrained model.

