\section{Introduction}

Large-scale text-to-image models have demonstrated remarkable ability in synthesizing photorealistic images~\cite{ramesh2022hierarchical,nichol2021glide,saharia2022photorealistic,rombach2022high,yu2022scaling,chang2023muse}. In addition to algorithms and compute resources, this technological advancement is powered by the use of massive datasets scraped from web~\cite{schuhmann2021laion}. %
Unfortunately, the datasets often consist of copyrighted materials, the artistic oeuvre of creators, and personal photos~\cite{somepalli2022diffusion,carlini2023extracting,shan2023glaze}.  

We believe that every creator should have the right to \emph{opt out} from large-scale models at any time for any image they have created.  However, fulfilling such requests poses new computational challenges, as re-training a model from scratch for every user request can be computationally intensive. Here, we ask -- \emph{How can we prevent the model from generating such content? How can we achieve it efficiently without re-training the model from scratch? How can we make sure that the model still preserves related concepts?}   




These questions motivate our work on ablation (removal) of concepts from text-conditioned diffusion models~\cite{rombach2022high,stablediffusionlink}.
We perform concept ablation by modifying generated images for the target concept ($\c^*$) to match a broader \anchor concept ($\c$), e.g., overwriting {\menlo Grumpy Cat} with {\menlo cat} or {\menlo Van Gogh} paintings with {\menlo painting} as shown in \reffig{teaser}. Thus, given the text prompt, {\menlo painting of olive trees in the style of Van Gogh}, generate a normal painting of olive trees even though the text prompt consists of {\menlo Van Gogh}. Similarly, prevent the generation of specific instances/objects like {\menlo Grumpy Cat} and generate a random cat given the prompt. 


Our method aims at modifying the conditional distribution of the model given a target concept $p_{\Phi}(\x|\c^*)$ to match a distribution $p(\x|\c)$ defined by the \anchor concept $\c$. This is achieved by minimizing the Kullbackâ€“Leibler divergence between the two distributions. We propose two different target distributions that lead to different training objectives. In the first case, we fine-tune the model to match the model prediction between two text prompts containing the target and corresponding anchor concepts, e.g., {\menlo A cute little Grumpy Cat} and {\menlo A cute little cat}. In the second objective, the conditional distribution $p(\x|\c)$ is defined by the modified text-image pairs of: a target concept prompt, paired with images of \anchor concepts, e.g., the prompt {\menlo a cute little Grumpy Cat} with a random cat image. We show that both objectives can effectively ablate concepts.  %

We evaluate our method on 16 concept ablation tasks, including specific object instances,  artistic styles,  and memorized images, using various evaluation metrics. Our method can successfully ablate target concepts while minimally affecting closely related surrounding concepts that should be preserved (e.g., other cat breeds when ablating {\menlo Grumpy Cat}). Our method takes around five minutes per concept. %
Furthermore, we perform an extensive ablation study regarding different algorithmic design choices, such as the objective function variants, the choice of parameter subsets to fine-tune, the choice of \anchor concepts, the number of fine-tuning steps, and the robustness of our method to misspelling in the text prompt. %
Finally, we show that our method can ablate multiple concepts at once and discuss the current limitations. Our \href{https://github.com/nupurkmr9/concept-ablation} {code}, data, and models are available at \url{https://www.cs.cmu.edu/~concept-ablation/}.  





