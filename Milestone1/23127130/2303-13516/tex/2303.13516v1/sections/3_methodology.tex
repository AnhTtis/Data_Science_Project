\section{Method}\label{sec:method}
Here, we first provide a brief overview of text-to-image diffusion models~\cite{sohl2015deep,ho2020denoising} in \refsec{diffusion}. We then propose  our concept ablation formulation and explore two variants in \refsec{objective}. Finally, in \refsec{details}, we discuss the training details for each type of ablation task. %

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/method.pdf}
    \vspace{-10pt}
    \caption{{\textbf{Overview}. We update model weights to modify the generated image distribution on the target concept, e.g., {\menlo Grumpy Cat}, to match an anchor distribution, e.g., {\menlo Cat}. We propose two variants. \textit{Left:} The anchor distribution is generated by the model itself, conditioned on the anchor concept. \textit{Right:} The anchor distribution is defined by the modified pairs of $<$target prompt, \anchor image$>$. An input image $\x$ is generated with \anchor concept $\c$. Adding randomly sampled noise $\epsilon$ results in noisy image $\x_t$ at time-step $t$. Target prompt $\c^*$ is produced by appropriately modifying $\c$. In experiments, we find the model-based variant to be more effective.
    }}
    \lblfig{method}
    \vspace{-15pt}
\end{figure*}


\subsection{Diffusion Models}
\label{sec:diffusion}
Diffusion models~\cite{sohl2015deep} learn to reverse a forward Markov chain process where noise is gradually added to the input image over multiple timesteps $t \in [0, T]$. The noisy image $\x_t$ at any time-step $t$ is given by $ \sqrt{\alpha_t}\x_{0} + \sqrt{1 - \alpha_t}\epsilon$, where $\x_0$ is a random real image, and $\alpha_t$ determines the strength of gaussian noise $\epsilon$ and decreases gradually with timestep such that $\x_T \smallsim N(0,I)$. The denoising network $\Phi(\x_t, \c,  t)$ is trained to denoise the noisy image to obtain $\x_{t-1}$, and can also be conditioned on other modalities such as text $\c$. %
The training objective can be reduced to predicting the noise $\epsilon$:  
\begin{equation}
    \begin{aligned}
     \mathcal{L}(\x,\c) = \mathbb{E}_{\epsilon,\x,\c, t } [w_t||\epsilon - \Phi (\x_t, \c, t) ||],\\
    \end{aligned}\label{eq:loss_diffusion}
\end{equation}
where $w_t$ is a time-dependent weight on the loss. To synthesize an image during inference, given the text condition $\c$, we iteratively denoise a Gaussian noise image $\x_T \smallsim N(0, I)$ for a fixed number of timesteps~\cite{song2020denoising,lu2022dpm}. 



\subsection{Concept Ablation}
\label{sec:objective}

We define concept ablation as the task of preventing the generation of the desired image corresponding to a given target concept that needs to be ablated. As re-training the model on a new dataset with the concept removed is impractical, this becomes a challenging task. We need to ensure that editing a model to ablate a particular concept doesn't affect the model performance on other closely related concepts. 


\myparagraph{A na\"ive approach.} Our first attempt is to simply maximize the diffusion model training loss~\cite{tanno2022repairing,kong2022data} on the text-image pairs for the target concept while imposing regularizations on the weights.
Unfortunately, this method leads to worse results on close surrounding concepts of the target concept. We compare our method with this baseline in \refsec{main_results} (\reffig{mse_kldiv_compare}) and show that it performs sub-optimally.


\myparagraph{Our formulation.}
As concept ablation prevents the generation of the target concept, thus the question arises: what should be generated instead? In this work, we assume that the user provides the desired \anchor concept, e.g., {\menlo Cat} for {\menlo Grumpy Cat}. The \anchor concept overwrites the target concept and should be a superset or similar to the target concept. Thus, given a set of text prompts $\{ \c^* \}$ describing the target concept, we aim to match the following two distributions via Kullbackâ€“Leibler (KL) divergence:  
\vspace{-2pt}
\begin{equation}
    \begin{gathered}
     \arg\min_{\hat{\Phi}} \mathcal{D_{KL}}(p(\x_{(0 ...T)}|\c) || p_{\hat{\Phi}}(\x_{(0 ...T)} | \c^*)),
    \end{gathered}\label{eq:loss}
\end{equation}
where $p(\x_{(0 ...T)}|\c)$ is some target distribution on the $\{ \x_t \}$, $\t \in [0,T]$, defined by the \anchor concept $\c$ and $p_{\hat{\Phi}}(\x_{(0 ...T)} | \c^*)$ is the model's distribution for the target concept. Intuitively, we want to associate text prompts $\{ \c^* \}$ with the images corresponding to \anchor prompts $\{ \c \}$. Defining different \anchor concept distributions leads to different objective functions, as we discuss next. 

To accomplish the above objective, we first create a small dataset that consists of $(\x, \c, \c^*)$ tuple, where $\c$ is a random prompt for the \anchor concept, $\x$ is the generated image with that condition, and $\c^*$ is modified from $\c$ to include the target concept. For example, if $\c$ is {\menlo photo of a cat}, $\c^*$ will be {\menlo photo of a Grumpy Cat}, and $\x$ will be a generated image with text prompt $\c$. For brevity, we use the same notation $\x$ to denote these generated images. 


\myparagraph{Model-based concept ablation}.
Here, we match the distribution of the target concept $p_{\hat{\Phi}}(\x_{(0 ...T)} | \c^*)$ to the pretrained model's distribution  $p_{\Phi}(\x_{(0 ...T)}| \c)$ given the \anchor concept. The fine-tuned network should have a similar distribution of generated images given $\c^*$ as that of $\c$, which can be expressed as minimizing the KL divergence between the two. This is similar to the standard diffusion model training objective, except the target distribution is defined by the pretrained model instead of training data. \refeq{loss} can be expanded as
\begin{equation}
    \begin{gathered}
     \arg\min_{\hat{\Phi}} \sum_{t=1}^{T} \mathop{\mathbb{E}}_{p_{\Phi}(\x_0...\x_T|\c)} \Big[ \log \frac{p_{\Phi}(\x_{t\text{-}1}|\x_{t},\c)}{p_{\hat{\Phi}}(\x_{t\text{-}1}|\x_{t},\c^*)} \Big]
    \end{gathered}\label{eq:loss_model_expandkl}
\end{equation}
where the noisy intermediate latent $\x_t \sim p_{\Phi}(\x_t| \c)$,
$\Phi$ is the original network, 
and $\hat{\Phi}$ is the new network we aim to learn. %
We can optimize the KL divergence by minimizing the following equivalent objective:  
\vspace{-5pt}
\begin{equation}
    \begin{gathered}
     \arg \min_{\hat{\Phi}} \mathbb{E}_{\epsilon,\x_t,\c^*, \c, t } [w_t||\Phi (\x_t, \c, t) - \hat{\Phi} (\x_t, \c^*, t) ||], 
    \end{gathered}\label{eq:loss_model}
    \vspace{-5pt}
\end{equation}
where we show the full derivation in \refapp{loss_objective}. We initialize $\hat{\Phi}$ with the pretrained model. Unfortunately, optimizing the above objective requires us to sample from $p_{\Phi}(\x_t|\c)$ and keep copies of two large networks $\Phi$ and $\hat{\Phi}$, which is time and memory-intensive. 
To bypass these, we sample $\x_t$ using the forward diffusion process and assume that the model remains similar for the \anchor concept during fine-tuning. Therefore we use the network $\hat{\Phi}$  with $stopgrad$ to get the \anchor concept prediction. Thus, our final training objective is
\vspace{-5pt}
\begin{equation}
    \begin{aligned}
     \mathcal{L}_{\text{model}}(\x, \c, \c^*)=\mathbb{E}_{\epsilon,\x,\c^*, \c, t } [w_t||\hat{\Phi} (\x_t, \c, t).\text{sg()} - \\  \hat{\Phi} (\x_t, \c^*, t) ||],
    \end{aligned}\label{eq:loss_approx}
    \vspace{-5pt}
\end{equation}
where $\x_t=\sqrt{\alpha_t}\x + \sqrt{1 - \alpha_t}\epsilon$. As shown in \reffig{method} (left), this objective minimizes the difference in the model's prediction given the target prompt and \anchor prompt. \nupur{It is also possible to optimize the approximation to reverse KL divergence, and we discuss it in \refsec{other_experiments}}.


\myparagraph{Noise-based concept ablation.}
Alternatively, we can redefine the ground truth text-image pairs as $<$a target concept text prompt, the generated image of the corresponding \anchor concept text prompt$>$, e.g., $<${\menlo photo of Grumpy Cat}, random cat image$>$. We fine-tune the model on these redefined pairs with the standard diffusion training loss:
\vspace{-5pt}
\begin{equation}
    \begin{aligned}
     \mathcal{L}_{\text{noise}}(\x, \c, \c^*)=\mathbb{E}_{\epsilon,\x,\c^*, t } [w_t||\epsilon - \hat{\Phi}(\x_t, \c^*, t) ||], \\
    \end{aligned}\label{eq:loss_data}
    \vspace{-5pt}
\end{equation}
where the generated image $\x$ is sampled from conditional distribution $p_{\Phi}(\x| \c)$. We then create the noisy version $\x_t= \sqrt{\alpha_t}\x + \sqrt{1 - \alpha_t}\epsilon$. 
As shown in \reffig{method}, the first objective (\refeq{loss_approx}) aims to match the model's predicted noises, while the second objective (\refeq{loss_data}) aims to match the Gaussian noises $\epsilon$. We evaluate the above two objectives in \refsec{expr}.




\myparagraph{Regualization loss.}
We also add the standard diffusion loss on $(\x,\c)$ \anchor concept pairs as a regularization~\cite{ruiz2022dreambooth,kumari2022multi}. Thus, our final objective is $\lambda \mathcal{L}(\x, \c) + \mathcal{L}(\x, \c, \c^*) $, where the losses are as defined in \refeq{loss_diffusion} and \ref{eq:loss_approx} (or \ref{eq:loss_data}) respectively. We require regularization loss as the target text prompt can consist of the \anchor concept, e.g., {\menlo Cat} in {\menlo Grumpy Cat}. 


\myparagraph{Parameter subset to update.} We experiment with three variations where we fine-tune different network parts: (1) \textit{Cross-Attention}: fine-tune key and value projection matrices in the diffusion model's U-Net~\cite{kumari2022multi}, (2) \textit{Embedding}: fine-tune the text embedding in the text transformer~\cite{gal2022image}, and (3) \textit{Full Weights}: fine-tune all parameters of the U-Net~\cite{ruiz2022dreambooth}.  


\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/compare_methods_all.pdf}
    \vspace{-22pt}
    \caption{{\textbf{Comparison of different learning objectives.} The $\methodmodel$ concept ablation converges faster than the $\methoddata$ variant while maintaining better performance on surrounding concepts. Maximizing the loss on the target concept dataset leads to the deterioration of surrounding concepts (top row). 
    }}
    \lblfig{mse_kldiv_compare}
    \vspace{-16pt}
\end{figure}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/instance_ablation_weights.pdf}
    \vspace{-18pt}
    \caption{{\textbf{Quantitative evaluation for ablating instances (top row) and styles (bottom row).} We show the performance of our final $\methodmodel$ concept ablation method across training steps and on updating different subsets of parameters. All metrics are averaged across four target concepts. Both embedding and cross-attention fine-tuning converge early. Fine-tuning cross-attention layers performs slightly worse for surrounding concepts but remains more robust to small spelling mistakes (third column). 
    }}
    \lblfig{kldiv_compare}
    \vspace{-10pt}
\end{figure*}


\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/instance_ablation.pdf}
    \vspace{-18pt}
    \caption{\textbf{ Qualitative samples when ablating specific object instances.} We show samples from different variations of our method in each row. The $\methoddata$ method performs worse on {\menlo Nemo} and {\menlo R2D2} instances compared to the $\methodmodel$ variant. With the $\methodmodel$ variant, fine-tuning different subsets of parameters perform comparably to each other. As shown in \reffig{kldiv_compare} (third column) and \reffig{results_spell_mistake}, fine-tuning only the embedding is less robust to small spelling mistakes.}
    \label{fig:results_instance}
    \vspace{-18pt}
\end{figure*}



\subsection{Training Details}\lblsec{training_details}
\label{sec:details}



\myparagraph{Instance.} Given the target and the \anchor concept, such as {\menlo Grumpy Cat} and {\menlo Cat}, we first use ChatGPT~\cite{chatgpt} to generate $200$ random prompts $\{\c \}$ containing the \anchor concept. %
We generate $1,000$ images from the pretrained diffusion model using the $200$ prompts and replace the word {\menlo Cat} with {\menlo Grumpy Cat} to get target text prompts $\{\c^*\}$. %


\myparagraph{Style.} When removing a style, we use generic painting styles as the \anchor concept. We use clip-retrieval~\cite{clip_retrieval} to obtain a set of text prompts $\c$ similar to the word {\menlo painting} in the CLIP feature space. We then generate $1000$ images from the pretrained model using the $200$ prompts. To get target prompts $\{\c^*\}$, we append {\menlo in the style of $\{$target style$\}$ } and similar variations to \anchor prompts $\c$. 


\myparagraph{Memorized images.} %
Recent methods for detecting training set memorization can identify both the memorized image and corresponding text prompt  $\c^*$~\cite{carlini2023extracting}. We then use ChatGPT to generate five \anchor prompts $\{\c\}$ that can generate similar content as the memorized image. In many cases, these anchor prompts still generate the memorized images. Therefore, we first generate several more paraphrases of the anchor prompts using chatGPT and include the three prompts that lead to memorized images often into target prompts and ten prompts that lead to memorized images least as anchor prompts. Thus $\c^*$ and $\c$ for ablating the target memorized image consists of four and ten prompts, respectively. We then similarly generate $1000$ images using the anchor prompts and use image similarity metrics~\cite{pizzi2022self,carlini2023extracting} to filter any memorized images generated in case and use the remaining ones for training. 




