\documentclass{article}

\input{commands}
\usepackage{PRIMEarxiv}
\usepackage{amsmath}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks=true, linkcolor=black]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Welfare Maximization Algorithm for Solving Budget-Constrained Multi-Component POMDPs }
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Welfare Maximization Algorithm for Solving Budget-Constrained Multi-Component POMDPs
%%%% Cite as
%%%% Update your official citation here when published 
}

\author{
  Manav Vora\\
  Department of Aerospace Engineering \\
  University of Illinois Urbana-Champaign \\
  \texttt{mkvora2@illinois.edu} \\
  %% examples of more authors
   \And
  Pranay Thangeda \\
  Department of Aerospace Engineering \\
  University of Illinois Urbana-Champaign \\
  \texttt{pranayt2@illinois.edu} \\
    \And
  Michael N. Grussing \\
   Engineer Research and Development Center \\
  US Army Corps of Engineers \\
  \texttt{michael.n.grussing@erdc.dren.mil} \\
     \And
  Melkior Ornik \\
  Department of Aerospace Engineering \\
  University of Illinois Urbana-Champaign \\
  \texttt{mornik@illinois.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\maketitle


\begin{abstract}
Partially Observable Markov Decision Processes (POMDPs) provide an efficient way to model real-world sequential decision making processes. Motivated by the problem of maintenance and inspection of a group of infrastructure components with independent dynamics, this paper presents an algorithm to find the optimal policy for a multi-component budget-constrained POMDP. We first introduce a budgeted-POMDP model (\textit{b-POMDP}) which enables us to find the optimal policy for a POMDP while adhering to budget constraints. Next, we prove that the value function or maximal collected reward for a special class of b-POMDPs is a concave function of the budget for the finite horizon case. Our second contribution is an algorithm to calculate the optimal policy for a multi-component budget-constrained POMDP by finding the optimal budget split among the individual component POMDPs. The optimal budget split is posed as a welfare maximization problem and the solution is computed by exploiting the concavity of the value function. We illustrate the effectiveness of the proposed algorithm by proposing a maintenance and inspection policy for a group of real-world infrastructure components with different deterioration dynamics, inspection and maintenance costs. We show that the proposed algorithm vastly outperforms the policies currently used in practice.
\end{abstract}


% keywords can be removed
\keywords{POMDPs \and Budget \and Welfare \and Optimization \and Infrastructure}


\section{Introduction}
Sequential decision-making is an integral component of many real world problems like machine maintenance, structural inspection and autonomous robotics \cite{Cassandra2003ASO}. Markov Decision Processes (MDPs) have provided an efficient framework to model and solve such problems while accounting for the corresponding uncertainty \cite{putterman}. POMDPs are a generalized version of MDPs, allowing for more uncertainty to be accounted for in the form of partial observability of the state of the system \cite{astrompomdp}. However, finding optimal policies for POMDPs is much more computationally intensive as compared to MDPs and has been proven to be PSPACE-complete \cite{complexitypomdp}. Synthesis of optimal policies for POMDPs is a classical problem and many algorithms have been proposed for the same \cite{finitestatecontroller, pruning, heuristicVI, NIPS2010_edfbe1af}.

This paper considers optimal planning for a class of structured budget-constrained POMDPs. This setting is motivated by infrastructure maintenance planning -- a widely studied problem \cite{mishra,infra,hcrl} that involves finding the optimal policy for maintenance and inspection, of an infrastructure component or a group of components within a certain budget\cite{deeprl,subway}. For simplicity of planning, the stochastic dynamics of multi-component systems are modelled using a POMDP \cite{intinspect,maintplan}. Also, the dynamics of individual components are assumed to be independent of each other \cite{ANDRIOTIS2021107551}. We will thus model such a setting by multi-component budget-constrained POMDPs where the transition probabilities of the individual component POMDPs are decoupled from each other. Thus we can say that the individual component POMDPs are weakly-coupled in the sense that they are only connected by the shared total budget of the multi-component POMDP. 

% The efficiency of POMDPs in maintenance and inspection scheduling and planning has been studied in depth in \cite{intinspect,maintplan}. In real world multi-component infrastructure management problems, the components are considered to be independent of each other in terms of behaviour \cite{ANDRIOTIS2021107551}. Also since, these problems involve operating on a certain budget \cite{deeprl,subway}, we consider the problem of solving large state space, weakly-coupled budget-constrained POMDPs. 

An algorithm for solving cost-constrained POMDPs has been proposed in \cite{CC-POMCP}. However, this algorithm becomes computationally infeasible for multi-component POMDPs with very large state spaces. A POMDP-based solution for optimal maintenance and inspection of structures using Dynamic Bayesian Networks is presented in \cite{optmaint}. However, in addition to computational infeasibility, this algorithm does not account for budget constraints. Optimal allocation for MDPs has been studied in \cite{stochasticranking}. The paper models the statistical ranking and selection problem as an MDP and derives an approximately optimal allocation policy using value function approximation. In our work, we study optimal budget allocation for multi-component POMDPs, for infrastructure management. A method for solving budget-constrained MDPs has been presented in \cite{cmdps}. The paper introduces a budgeted-MDP model which includes the budget as an implicit part of the state. This is because the paper uses a cost function, similar to Constrained MDPs \cite{cmdp1}, to keep a track of the cost incurred by the policy. Hence, this algorithm cannot be directly extended to POMDPs because the partial observability of the state would cause a violation of the budget constraint in some cases. 

In this work, we propose a computationally efficient algorithm for optimal policy synthesis of a multi-component budget-constrained POMDP.
% by splitting it into individual component POMDPs through a priori allocation of the total budget. 
Our contributions here are:
\begin{itemize}
    \item we introduce a b-POMDP model to facilitate strict adherence to budget constraints in POMDPs,
    \item we obtain an approximately optimal policy for multi-component POMDPs by finding the optimal budget split among the individual component POMDPs.
\end{itemize}
The b-POMDP model includes the total cost incurred upto a time instant $k$ explicitly as a part of the state vector. We show that the value function for a particular class of b-POMDPs is a concave function of the budget. Next, we compute the optimal policy for the individual component POMDPs by modeling them as b-POMDPs and using an online solver like POMCP \cite{NIPS2010_edfbe1af}. Doing so gives us the approximate maximal total reward collected by the policy in terms of the budget allocated to the b-POMDP. We use these rewards to calculate the optimum distribution of the total budget among the individual component b-POMDPs and find the approximately optimal policy of the multi-component POMDP. The budget splitting is posed as a welfare maximization problem constrained by the total budget of the multi-component POMDP. The concave nature of the value function renders this as a convex optimization problem, thus guaranteeing a global optimum. We demonstrate the utility of the proposed algorithm by finding optimal maintenance and inspection policies for multiple components of a realistic general administrative building, subject to a budget. Based on this real data, we show that our algorithm vastly outperforms the policy currently used in practice.


\section{Preliminaries and Background}
\label{sec:prelim}
In this section, we provide background on Partially Observable Markov Decision Processes for sequential decision-making with stochastic dynamics. We start by defining the notation used in the paper. 

Given a finite set $\actions$, $|\actions|$ denotes its cardinality and $\Delta(\actions)$ denotes the set of all probability distributions over the set $\actions$. Notation $\mathbb{N}_0$ denotes the set of natural numbers including 0 i.e. $\mathbb{N}_0 = \{0,1,2, \ldots\}$. The symbols $\lfloor . \rfloor$ and $\lceil . \rceil$ denote the floor  and ceiling functions respectively.

\subsection{Partially Observable Markov Decision Process}
A discrete-time finite-horizon POMDP \cite{pomdpcassandra, Braziunas-POMDPsurvey} $M$ is specified by the 8-tuple $(\states, A, \pi, T, \Omega, O, R, H)$, where $\states$ denotes a finite set of states, $A$ denotes a finite set of actions, $\pi : \states \rightarrow A$ denotes the policy which specifies the action to take in a given state $s$ and $T :  \states\times A \rightarrow \Delta(\states)$ denotes the transition probability function, where $\Delta(\states)$ is the space of probability distributions over $\states$. Furthermore, $\Omega$ denotes a finite set of observations and $O : \Omega\times\states\times A \rightarrow \Delta(\Omega)$ denotes the observation probability function where $\Delta(\Omega)$ is analogous to $\Delta(\states)$. Finally, $R : \states \times A \rightarrow [0, R_\tss{max}]$ denotes the reward function and $H \in \mathbb{N}_0$ denotes the finite planning horizon.

For the above POMDP, at each time step, the environment is in some state $s \in \states$ and the agent interacts with the environment by taking an action $a \in A$. Doing so results in the environment transitioning to a new state $\bar{s} \in \states$ in the next time step with probability $T(s,a,\bar{s})$. Simultaneously, the agent receives an observation $o \in \Omega$ regarding the state of the environment with probability $O(o|\bar{s},a)$ which depends on the new state of the environment and the action taken by the agent. In a POMDP the agent doesn't have access to the true state of the environment. However, the agent can update it's belief about the true state of the environment using this observation. The agent also receives a reward $R(s,a)$. 

The problem of optimal policy synthesis for a finite-horizon POMDP is that of choosing a sequence of actions which maximizes the expected total reward. 
% $\EE[\sum_{t=0}^{H}r_t]$ where $r_t$ is the reward earned at time instant $t$.  
% Hence the optimal behavior may often include actions which are taken simply because they improve the agent's belief about the true state. After reaching $s^{\prime}$, the agent receives observation $o \in \Omega$ with probability $O(o|s^{\prime},a)$. Let the belief $b$ be a probability distribution over $\states$. Then, $b(s)$ denotes the belief state and the agent updates the belief state \cite{pruning} according to Bayes' rule. The updated belief state $b^\prime(s^\prime)$ is given by:
% \begin{equation*}
%     b^{\prime}(s^{\prime}) = \eta O(o|s^{\prime},a)\sum_{s \in \states}T(s^{\prime}|s,a)b(s)
% \end{equation*}
% where $\eta$ is a normalizing constant and is defined as:
% % and is defined as:
% \begin{equation*}
%     \eta = \frac{1}{\sum_{s^\prime \in \states}O(o|s^\prime,a)\sum_{s \in \states}T(s^\prime|s,a)b(s)}.
% \end{equation*}

\section{Problem Formulation}
\label{sec:problem}
We consider a multi-component POMDP, which is a collection of $n$ component POMDPs. The component POMDPs are weakly-coupled in the sense that they have independent transition probabilities and are connected only by the shared total budget. In this paper, we consider optimal policy synthesis for POMDPs with budget, i.e., each action incurs a cost and the total cost incurred by the optimal policy is limited by the budget. We first formally define the multi-component POMDP with a budget and then define the problem of finding the optimal policy for such a POMDP.

\subsection{Multi-Component Decoupled POMDP with Shared Budget}
\label{subsec:multicomponent}
For a multi-component POMDP, the state space $\states \subseteq \mathbb{N}_0^n$ is given by $\states = \states^1 \times \states^2 \times \ldots \times \states^n$. The state space $\states^i \in \mathbb{N}_0$, for component $i$, is given by $\states^i = \{0,1,2, \ldots, s_{max}\}$, where $s_{max} \in \mathbb{N}_0$.The state $s_k \in \states$, at time step $k$ is given by $s_k = \{s^1_k,s^2_k, \ldots, s^n_k\}$ where $s^i_k \in \states^i$ represents the state of component $i$ at time step $k$. 
% Also, for some $s_{max} \in \states$, $s^i_k \leq s_{max}$ for all $i \in \{1,2,\ldots,n\}$ and for all $k \in \mathbb{N}_0$. 
The action space is given by 
\begin{equation*}
    A = \prod_{i = 1}^n A^i,
\end{equation*}
where the action space $A^i$ for component $i$ is given by $A^i = \{d^i,q^i,m^i\}$. Action $d^i$ lets the component move to a new state according to the transition probabilities. The action $q^i$ provides an observation which is equal to the next state $\bar{s}^i$ of the component and action $m^i$ drives the component state to $s_{max}$. The transition probability function for the multi-component POMDP for $s,\bar{s} \in \states$ and $a \in A$ is given by
\begin{equation*}
    T(s,a,\bar{s}) = \prod_{i=1}^n T^i(s^i,a^i,\bar{s}^i).
\end{equation*}
$T^i$ denotes the transition probability function for component $i$ and is defined as
\begin{equation}
    T^i(s,a,\bar{s}^i) = \begin{cases}
    1,& \text{if } \bar{s}^i = s_{max} \text{ and } a^i = m^i, \\
    p^i(s^i,a^i,\bar{s}^i),& \text{if } \bar{s}^i \leq s^i \text{ and } a^i \in \{d^i, q^i\},\\
    1,& \text{if } \bar{s}^i = 0 = s^i \text{ and } a^i \in A^i,\\
    0,& \text{otherwise}.
    \end{cases}
\end{equation}
The probability $p^i(s^i,a^i,\bar{s}^i)$ is chosen according to a probability distribution specific to component $i$. From the above equation, it can be observed that $0$ is an absorbing state. 

The observation space is given by $\Omega = \states \cup \{e\}$, where $e \in \mathbb{N}_0$ is an observation that does not provide any information regarding the true state of the system, i.e., $e \notin \states^i$ for all $i \in \{1,2,\ldots,n\}$. The observation function for the multi-component POMDP is given by 
\begin{equation*}
    O(\bar{s},a,o) = \prod_{i=0}^n O^i(\bar{s}^i,a^i,o^i).
\end{equation*}
Here, $O^i$ is the observation probability function for component $i$ and is defined as
\begin{equation*}
    O^i(\bar{s}^i,a^i,o^i) = \begin{cases}
        1,& \text{if } o^i = \bar{s}^i \text{ and } a^i \in \{q^i, m^i\} \\
        1,& \text{if } o^i = e \text{ and } a^i = d^i \\
        0,& \text{otherwise}.
    \end{cases}
\end{equation*}
% The reward function for the multi-component POMDP for $s \in \states$ and $a \in A$ is given by:
% \begin{equation*}
%     R(s,a) = \sum_{i=1}^n R^i(s^i,a^i), \forall i \in \{1,2,\ldots,n\},
% \end{equation*}
% where the reward function $R^i$ for component $i$ is defined as:
% \begin{equation*}
%     R^i(s^i,a^i) = \begin{cases}
%         r_1^i > 0,& \text{if } s^i > 0, \\
%         r_2^i = 0,& \text{if } s^i = 0,
%         \end{cases}
% \end{equation*}
% with $r_1^i, r_2^i \in \mathbb{R}$.

For each component $i$, each action $d^i,q^i$ and $m^i$ incurs a cost $c_d^i, c_q^i$ and $c_m^i$ respectively, against a total budget $B$. 
% Assume that the number of $d^i,i^i$ and $m^i$ actions taken for component $i$ for a horizon $H$ are $n^i_d,n^i_i$ and $n^i_m$ respectively. Then, the total cost incurred for the multi-component POMDP for the horizon is given by:
% \begin{equation}
%     C_H = \sum_{i=1}^n (n^i_dc^i_d + n^i_ic^i_i + n^i_mc^i_m) \label{cost}
% \end{equation}
% The multi-component POMDP also has a total budget $B$ which can be used for taking actions.

\subsection{Problem Statement}
For a multi-component POMDP, given by the formulation in the previous section, we consider the problem of finding an optimal policy $\pi^*$ which maximizes the time before reaching the absorbing state. Mathematically, $\pi^*$ maximizes $t$ such that $s_t > 0$. Furthermore, for a horizon of length $H$, $\pi^*$ should be such that the total cost incurred for the multi-component POMDP does not exceed the total budget. 
% Assume that, for a horizon $H$, the number of $d^i,i^i$ and $m^i$ actions taken, according to the optimal policy, for component $i$  are $n^i_d,n^i_i$ and $n^i_m$ respectively. Then
% \begin{equation*}
%     C_H = \sum_{i=1}^n (n^i_dc^i_d + n^i_ic^i_i + n^i_mc^i_m) \leq B,
% \end{equation*}
We propose an approximately optimal solution for the above problem through a two-step approach. In the first step, we will solve a single-component POMDP for any given budget. In the second step, we will partition the total budget $B$ into budgets for each individual component.

% for $n$ POMDPs with decoupled transition probabilities. We have n agents, one for each POMDP, which independently choose one of three possible actions at each time instant. Each agent incurs a specific cost for performing a specific action. Also, there is a forbidden state (say 0) associated with each POMDP which the agent must try to avoid. The goal of each agent is to minimize the time spent in the forbidden state or equivalently maximize the time spent in feasible states. However, the agents must choose the optimal strategy in a way such that the total cost incurred by all the agents is less than or equal to a certain total budget for a particular planning horizon. 

% For this problem, let $\states \subseteq \mathbb{N}$ and let $\actions = \{\text{d}, \text{i}, \text{m}\}$ where $d$ is an action such that the agent does nothing and let's the environment move to a new state according to the transition probabilities. The action $i$ allows the agent to get the true value of the state of the environment and the action $m$ makes the environment transition to state $s = s_{max} \in \states$ where $s_{max}$ is the maximum possible value in $\states$. Also, for the $d$ and $i$ actions the environment transitions in such a way that the value of the new state reached is always less than or equal to the value of the previous state i.e. $s^{\prime} \leq s$. Thus, the transition function for the problem is:

%  The agent receives the exact value of the state of the system as observation when it performs action $i$. Also, it receives the true state as observation on performing action $m$ because this action deterministically makes the system transition to $s_{max}$. For action $d$, the agent receives some observation $e$ which is not a part of the state space $\states$. Hence for this problem, $\Omega$ is $\states \cup \{e\}$. The observation probability function $O$ is :

% Let the total budget for a planning horizon be $B$ and the total cost incurred by the agents be $C$. The reward function $R(s,a)$ for this problem is defined as:
% \begin{equation*}
%     R(s,a) = 
% \end{equation*}
% where $|r_1| \ll |r_2|$. As can be clearly seen, the agents receive a small positive reward for each time the system is in a feasible state and a larger negative reward for each time the system is in the forbidden state.


\section{Solution Approach}
\label{sec:solution}
In this section, we detail our methodology for solving the problem of optimal policy synthesis of a multi-component POMDP. First, we introduce the b-POMDP model and discuss how to solve a single b-POMDP. Next, we discuss why the value function for such a POMDP is a concave function of the budget. Finally, we present our proposed method for finding the optimal policy for an $n$-component POMDP by computing the optimal split of the total budget, among the individual component POMDPs.

\subsection{Budgeted-POMDP Model (b-POMDP)}
\label{subsec:budgeted-POMDP}

Our main goal is to find an optimal policy for a POMDP while adhering to a total budget for actions. The budgeted-MDP model in \cite{cmdps} tracks the incurred cost using a cost function similar to Constrained MDPs \cite{cmdp1}. This model can't be extended directly to a POMDP because the partial observability of the state would lead to budget violation in some cases. Hence, we introduce a new b-POMDP model. In a b-POMDP, the budget constraint is incorporated by augmenting the total cost incurred upto time instant k, to each state of the state space. Thus, if we consider a single component POMDP with a total budget $B$, the modified state at time instant $k$ according to the b-POMDP formulation is given by $(s_k,c_k)$ where $s_k$ is as defined in the previous section, for $i = 1$. We assume that unlike $s_k$, $c_k$ is completely observable at all time instants. The transition function for the cost component of the state is given by:
\begin{equation*}
    T_c(c^{\prime}|c,a) = \begin{cases}
    1,& \text{if } c^{\prime} = c+c_m \text{ and } a = m \\
    1,& \text{if } c^{\prime} = c+c_q \text{ and } a = q \\
    1,& \text{if } c^{\prime} = c+c_d \text{ and } a = d \\
    0,& \text{ otherwise }.
    \end{cases}
\end{equation*}
The transition function for the overall b-POMDP is:
\begin{equation}
    T^\prime((s,c),a,(\bar{s},c^\prime)) = T(s,a, \bar{s})T_c(c,a,c^{\prime}),
\end{equation}
where $T(s,a,\bar{s})$ is defined in Section \ref{subsec:multicomponent}.
The new formulation prevents the policy from violating the budget at any time instant $k$. This is done 
% Furthermore, since we now have the cost incurred explicitly as part of the state vector, we can prevent the policy from taking actions which would result in $c_k > B$ at any time instant $k$. Such a modification can be easily performed 
by making $c_k > min\{B-c_m,B-c_i\}$ an absorbing state, similar to $s=0$. The reward function for the b-POMDP is given by:
\begin{equation*}
    R^\prime((s,c),a) = \begin{cases}
        r_1 > 0,& \text{if } s > 0, \\
        r_2 = 0,& \text{if } s = 0.
        \end{cases}
\end{equation*}
% Hence, we can say that the optimal policy $\pi^*$ maximizes the total collected reward obtained for a horizon $H$.
% Hence, the new state vector for the POMDP for component $i$ is a tuple of values given by $(s_k^i, c_k^i)$ where $s_k^i$ is as defined in the previous section. We assume that unlike $s_k^i$, $c_k^i$ is completely observable at all time instants. The transition function for the cost component of the state vector of component $i$ is as follows:
% \begin{equation*}
%     T(c^{\prime,i}|c^i,a) = \begin{cases}
%     1,& \text{if } c^{\prime,i} = c+c_m^i \text{ and } a = m^i \\
%     1,& \text{if } c^{\prime,i} = c+c_i^i \text{ and } a = i^i \\
%     1,& \text{if } c^{\prime,i} = c+c_d^i \text{ and } a = d^i
%     \end{cases}
% \end{equation*}
% where $c_m^i$, $c_i^i$ and $c_d^i$ are the costs incurred for performing the $m^i$, $i^i$ and $d^i$ actions respectively. Furthermore, since we now have the cost incurred explicitly as part of the state vector, we can prevent the policy from taking actions which would violate the budget constraint at any time instant.

To find an optimal policy for a b-POMDP, we use the method of Monte-Carlo Planning in POMDPs (POMCP \cite{NIPS2010_edfbe1af}). POMCP is an online planning algorithm for large POMDPs, which combines a Monte-Carlo update of the agentâ€™s belief with a Monte-Carlo tree search for the best action from the current belief state. 
% Each node of the tree represents a belief state and action pair and at each node, POMCP simulates a rollout by selecting actions according to a policy and updating the belief state based on the observations. 
For a b-POMDP, the maximal collected reward (value function) obtained using POMCP will be a function of the budget $B$ associated with it. We will now prove that this value function is concave in the budget for a special subclass of our overall problem.

\subsection{Proof for Concavity of Optimal Value Function}
\label{subsec:proof}
Consider an MDP with state space $\states_{MDP} = \{0,1,2 \ldots s_{max}\}$, where $s_{max} \in \mathbb{N}_0$ and action space is $A_{MDP} = \{m,d\}$. The state of the system decreases by a value $d_0 \in \mathbb{N}_0$ unless we perform action $m$. The transition probability function is defined as:
\begin{equation*}
    T(s^{\prime}|s,a) = \begin{cases}
    1,& \text{if } s^{\prime} = s_{max} \text{ and } a = m \\
    1,& \text{if } s^{\prime} = s-d_0 \text{ and } a = d\\
    1,& \text{if } s^{\prime} = 0 = s \text{ and } a \in A_{MDP}\\
    0,& \text{otherwise,}
    \end{cases}
\end{equation*}
From the above transition function, we can clearly see that state $0$ is an absorbing state. 
Also, $d_0$ is a decrease in the state value. The cost for the $d$ action is $c_d = 0$ and the cost for the $m$ action is $c_m > 0$. For simplicity, assume that $c_m = 1$. Let the available budget be denoted by $b \in \mathbb{N}_0$. This means that we can perform the action $m$ at most $b$ times. 

The reward function is similar to our original problem with a constant positive reward $r$ for all states $s > 0$ and a zero reward for $s=0$.

The value function for an MDP is the total expected reward collected by the optimal policy. For a b-POMDP, the value function is a function of the state value and the available budget. Let $V_H(s,b)$ represent the value function of the state and budget for a given horizon with $H$ steps to go.

The following lemma will be used for proving the concavity of the value function for the above mentioned MDP with respect to the budget.

\begin{lemma} \label{lemma:1}
For a given budget $b$ and horizon $H$, the value function $V_H(s_0,b)$ is an increasing function of the state $s_0$, i.e., for two states $s_0$ and $s_0^\prime$ such that $s_0 < s_0^\prime$, the following holds:
\begin{equation*}
    V_H(s_0^\prime,b) \geq V_H(s_0,b).
\end{equation*}
\end{lemma}
\begin{proof}
% For a horizon $H$, the value function is given by the Bellman equation \cite{bellman} as:
% \begin{equation*}
%     V_H(s_0,b) = r(s_0,a) + V_{H-1}(s^\prime,b^\prime),
% \end{equation*}
% where $s_0,s^\prime \in \states_{MDP}$ and $a \in A_{MDP}$. From the definition of the reward function, for the same $a$ and $s^\prime$, we have:
% \begin{equation*}
%     r(s_0,a) = \begin{cases}
%         r,& \text{ if } s_0 > 0,\\
%         -r,& \text{ if } s_0 = 0.
%     \end{cases}
% \end{equation*}
% Using this we can directly say that for a given 
Let $\pi^*$ be a policy that generates the value function $V_H(s_0,b)$, in the sense of maximizing the expected total return over the horizon $H$ given the initial state $s_0$ and budget $b$. Thus, we have $V_H^{\pi^*}(s_0,b) = V_H(s_0,b)$. If we start at state $s_0^\prime > s_0$, then following the same policy $\pi^{*}$ the expected return will be at least as good as that for $s_0$. We can therefore say that:
\begin{equation*}
    E\left[\sum_{t=0}^{H} r_t \Big\vert s_0^\prime, \pi^{*}, b\right] \geq E\left[\sum_{t=0}^{H} r_t \Big\vert s_0, \pi^*, b\right].
\end{equation*}
Hence, we can say that for a given budget $b$ and horizon $H$, the optimal policy for $s_0^\prime$ is atleast as good as the optimal policy for $s_0$ and hence:
\begin{equation*}
    V_H(s_0^\prime,b) \geq V_H^{\pi^*}(s_0^\prime,b) \geq V_H^{\pi^*}(s_0,b) = V_H(s_0,b) \quad \forall s_0^\prime > s_0
\end{equation*}
\end{proof}
\begin{lemma}\label{lemma:2}
    For a given initial state $s_0$ and horizon $H$, the value function $V_H(s_0,b)$ is an increasing function of the budget $b$, i.e., for two budgets $b$ and $b^\prime$ such that $b < b^\prime$, the following holds:
\begin{equation*}
    V_H(s_0,b^\prime) \geq V_H(s_0,b).
\end{equation*}
\end{lemma}
\begin{proof}
    Let $\pi^*$ be a policy that generates the value function $V_H(s_0,b)$, in the sense of maximizing the expected total return over the horizon $H$ given the initial state $s_0$ and budget $b$. The same policy will also be optimal for the same initial state $s_0$ but budget $b^\prime$. This is because following the same policy will result in the same trajectory of states for both budgets, but with a higher budget at each time step for $b^\prime$. Thus, we have
    \begin{equation*}
         E\left[\sum_{t=0}^{H} r_t \Big\vert s_0, \pi^{*}, b^\prime\right] \geq E\left[\sum_{t=0}^{H} r_t \Big\vert s_0, \pi^*, b\right].
    \end{equation*}
    Hence, the value function generated by this policy with budget $b^\prime$ will be at least as good as the value function generated by the same policy with budget $b$. Since this holds for any optimal policy, it follows that:
\begin{equation*}
    V_H(s_0,b^\prime) \geq V_H(s_0,b) \quad \forall s_0, b^\prime > b
\end{equation*}
\end{proof}

\begin{lemma}\label{lemma:3}
    For a given initial state $s_0 > d_0$ and horizon $H$, if $b > 0$, the optimal action to take at time $t=0$ is $d$.
\end{lemma}
\begin{proof}
    We will compare four cases:
\begin{enumerate}
    \item We first take an action m and then action o. Doing so yields the maximal collected reward $V_{H}^{md}$ given by 
    \begin{equation}
    V_{H}^{md}(s_0,b) = 2r + V_{H-2}(s_{max}-d_0,b-1). \label{mo}
    \end{equation}
    \item We first take an action o and then action m. Doing so yields the maximal collected reward $V_{H+1}^{dm}$ given by
    \begin{equation}
    V_{H}^{dm}(s_0,b) = 2r + V_{H-2}(s_{max},b-1).\label{om}
    \end{equation}
    \item We first take an action m and then action m. Doing so yields the maximal collected reward $V_{H}^{mm}$ given by
    \begin{equation}
    V_{H}^{mm}(s_0,b) = 2r + V_{H-2}(s_{max},b-2).\label{mm}
    \end{equation}
    \item We first take an action o and then action o. Doing so yields the maximal collected reward $V_{H+1}^{dd}$ given by
    \begin{equation}
    V_{H}^{dd}(s_0,b) = 2r + V_{H-2}(s_0-2d_0,b).\label{oo}
    \end{equation}
\end{enumerate}
Using Lemma \ref{lemma:1}, we know that the value function is an increasing function of the state for the same budget. Also from \ref{lemma:2} we know that the value function is an increasing function of the budget for the same state. Using these results and \eqref{mo}, \eqref{om} and \eqref{mm}, we get that 
\begin{equation}
    V_{H}^{dm} = \max\{V_{H}^{md}, V_{H}^{dm}, V_{H}^{mm}\}. \label{o_optimal}
\end{equation}
We don't need to compare $V_{H}^{dd}(s_0,b)$ because \eqref{o_optimal} is sufficient to prove that for $s_0 > d_0$, action $o$ is the optimal initial action.
\end{proof}

\begin{lemma}\label{lemma:4}
    For a given initial state $0 < s_0 \leq d_0$ and horizon $H$, if $b > 0$, the optimal action to take at time $t=0$ is $m$.
\end{lemma}
\begin{proof}
    We will consider two cases:
    \begin{enumerate}
        \item We take action $d$ in the first step. Doing so yields the maximal collected reward $V_H^o$ given by
        \begin{equation}
            V^d_{H}(s_0,b) = r + V_{H-1}(0,b), \label{ofirst}\\
        \end{equation}
        \item We take action $m$ in the first step. Doing so yields the maximal collected reward $V_H^m$ given by
        \begin{equation}
            V^m_{H}(s_0,b) = r + V_{H-1}(s_{max},b-1), \label{mfirst}
        \end{equation}
    \end{enumerate}
Clearly from \eqref{ofirst} and \eqref{mfirst}, taking action $d$ in the first step leads to the absorbing state and hence we get a total reward of $r$. Taking action $m$ gives the same reward $r$ and drives the state to $s_{max}$. This implies that the system will not reach the absorbing state for at least one more step and hence leads to higher total reward. Thus, we have
\begin{equation*}
    V^m_{H}(s_0,b) > V^d_{H}(s_0,b).
\end{equation*}
Hence, we can say that if $b > 0$, action $m$ is the optimal action to perform at $t=0$ when $0 < s_0 \leq d_0$.
\end{proof}
% $$V_H(s_0,b) = \max_{\pi} E\left[\sum_{t=0}^{H} r_t \Big\vert s_0, \pi, b\right]$$

% Now, consider the initial state $s_0^\prime$ such that $s_0 < s_0^\prime$. Let $\pi^{**}$ be a policy that generates the same trajectory of states and rewards as $\pi^*$, but starting from the initial state $s_1$ instead of $s_0$. Since the trajectory generated by $\pi^{**}$ is the same, we have:

% $$E\left[\sum_{t=0}^{H} r_t \Big\vert s_1, \pi^{**}, b\right] = E\left[\sum_{t=0}^{H} r_t \Big\vert s_0, \pi^*, b\right]$$

% Therefore, $\pi^{**}$ is also an optimal policy for the initial state $s_1$ and budget $b$, and hence we have:

% $$V_H(s_1,b) = \max_{\pi} E\left[\sum_{t=0}^{H} r_t \Big\vert s_1, \pi, b\right] \geq E\left[\sum_{t=0}^{H} r_t \Big\vert s_1, \pi^{**}, b\right] = E\left[\sum_{t=0}^{H} r_t \Big\vert s_0, \pi^*, b\right] = V_H(s_0,b)$$

% Thus, we have shown that $V_H(s_0,b) \leq V_H(s_1,b)$ for any two initial states $s_0$ and $s_1$ such that $s_0 < s_1$. Therefore, the optimal value function is monotonically increasing in the initial state when the budget is constant.


% \begin{lemma}
%     At state $s_0 = 0$, action $m$ is the optimal action to take at time $t=0$ if $b > 0$.
% \end{lemma}
% \begin{proof}
%     Let the value function corresponding to taking action $o$ in the first step be denoted by $V^o_{H+1}(s_0,b)$ and for taking action $m$ in the first step be denoted by $V^m_{H+1}(s_0,b)$. Then, we have:
% \begin{gather*}
%     V^o_{H+1}(s_0,b) = r(s_0,o) + V_H(0,b)\\
%     V^m_{H+1}(s_0,b) = r(s_0,m) + V_H(s_{max},b-1).
% \end{gather*}
% \end{proof}

% \begin{lemma} \label{lemma:s>d}
% For a given horizon length $H$, if $b > 0$, the optimal action to take at time $t=0$ is $o$ if $s_0 > d$.
% \end{lemma}
% \begin{proof}
    
% \end{proof}

\begin{theorem} \label{theorem:1}
The value function for the MDP, $V_H(s_0,b)$, is concave in the budget $b$ for any horizon length $H \in \mathbb{N}_0$ and initial state $s_0 \in \states_{MDP}$. More specifically:
\begin{itemize}
    \item For $s_0 > d_0$, $V_H(s_0,b)$ is constant  with respect to the budget for all $b \geq \lfloor H/2 \rfloor$, where $\lfloor . \rfloor$ represents the floor function, 
    \item For $s_0 \leq d_0$, $V_H(s_0,b)$ is constant with respect to the budget for all $b \geq \lceil H/2 \rceil$, where $\lceil . \rceil$ represents the ceiling function,
    \item For budget values $b$,$b^\prime$ and $b^{\prime \prime}$ such that $b^{\prime\prime} = b^\prime + 1 = b + 2$ and $b \geq 0$, we have:
    \begin{equation}
        V_H(s_0,b^{\prime \prime}) - V_H(s_0,b^\prime) \leq V_H(s_0,b^{\prime}) - V_H(s_0,b), \label{relation}
    \end{equation}
\end{itemize}
\end{theorem}

\begin{proof}
%     It is trivial to prove that $V_H(s_0,b)$ is an increasing function of the budget $b$. Consider two budget values $b$ and $b^\prime$ such that $b<b^\prime$. Consider an optimal policy for initial state $s_0$ and budget $b$ which results in $V_H(s_0,b)$. The same policy will also be optimal for the same initial state $s_0$ but budget $b^\prime$. This is because following the same policy will result in the same trajectory of states for both budgets, but with a higher budget at each time step for $b^\prime$. Hence, the value function generated by this policy with budget $b^\prime$ will be at least as good as the value function generated by the same policy with budget $b$. Since this holds for any optimal policy, it follows that:
% \begin{equation*}
%     V_H(s_0,b^\prime) \geq V_H(s_0,b) \quad \forall s_0, b^\prime > b
% \end{equation*}

First, we will show that the claims hold for a horizon of length 0. Then, using induction we will prove that they hold for a horizon of length $H$. 

Consider a horizon of length 0. For this case we have:
\begin{equation*}
    V_0(s_0,b) = \begin{cases}
        r,& \text{ if } s_0 > 0\\
        0,& \text{ if } s_0 = 0,
    \end{cases}
\end{equation*}
where $b \geq 0$. Clearly, $V_0(s_0,b)$ is constant in the budget for all $b \geq 0$ and thus, the first two claims hold. Also, for all values of $s_0 > 0$ we can say that:
\begin{equation*}
    V_0(s_0,b^{\prime\prime}) - V_0(s_0,b^\prime) = 0 \leq V_0(s_0,b^\prime) - V_0(s_0,b),
\end{equation*}
where $b^{\prime\prime} = b^\prime + 1 = b + 2$ and $b > 0$. 
% We also have that $V_0(s_0,b)$ is constant for all $b  \geq 0$ and a given $s_0 \in \states$.

Now, for a horizon of length $H$, assume the following:
\begin{itemize}
    \item For $s_0 > d_0$, $V_H(s_0,b)$ is constant  with respect to the budget for all $b \geq \lfloor H/2 \rfloor$, where $\lfloor . \rfloor$ represents the floor function, 
    \item For $s_0 \leq d_0$, $V_H(s_0,b)$ is constant with respect to the budget for all $b \geq \lceil H/2 \rceil$, where $\lceil . \rceil$ represents the ceiling function,
    \item Relation \eqref{relation} holds true for $b \geq 0$.
\end{itemize}
As can be clearly seen, the above assumptions hold true for $H=0$ as we proved previously. Now, assume that they hold for some $H > 0$ and consider a horizon of length $H+1$. We will prove that the assumptions hold true for horizon $H+1$ when $s_0>d_0$ and when $s_0 \leq d_0$.

% First we will consider $s_0 > d$: To prove that action $o$ is the optimal action to be performed in the first step, we will compare 3 cases:
% \begin{enumerate}
%     \item The optimal action in the first step is $m$ and the optimal action in the step after that is $o$. Then we have:
%     \begin{equation}
%     V_{H+1}^{mo}(s_o,b) = 2r + V_{H-1}(s_{max}-d,b-1). \label{mo}
%     \end{equation}
%     \item The optimal action in the first step is $o$ and the optimal action in the step after that is $m$. Then we have:
%     \begin{equation}
%     V_{H+1}^{om}(s_o,b) = 2r + V_{H-1}(s_{max},b-1).\label{om}
%     \end{equation}
%     \item The optimal action in the first step is $m$ and the optimal action in the step after that is also $m$. Then we have:
%     \begin{equation}
%     V_{H+1}^{mm}(s_o,b) = 2r + V_{H-1}(s_{max},b-2).\label{mm}
%     \end{equation}
% \end{enumerate}
% First let's consider the case when we start at state $s_{max}$. Let's say the first optimal action is $m$ and the optimal action in the step after that is $o$. Then we get:
% \begin{equation}
%     V_{H+1}^{mo}(s_{max},b) = 2r + V_{H-1}(s_{max}-d,b-1). \label{mo}
% \end{equation}
% Now consider the case when the first optimal action is $o$ and the optimal action in the step after that is $m$. Then we get:
% \begin{equation}
%     V_{H+1}^{om}(s_{max},b) = 2r + V_{H-1}(s_{max},b-1).\label{om}
% \end{equation}
% % If $s_{max}$ is such that $s_{max} = nd$, where $n>1$, then both \eqref{mo} and \eqref{om} will yield the same value and hence taking action $m$ first followed by $o$ is the same as taking $o$ first followed by $m$. 
% Finally, consider the case where the first optimal action is $m$ and the optimal action in the next step is also $m$. Then, we get:
% \begin{equation}
%     V_{H+1}^{mm}(s_{max},b) = 2r + V_{H-1}(s_{max},b-2).\label{mm}
% \end{equation}
% Using Lemma \ref{lemma:1}, we know that the value function is an increasing function of the state for the same budget. Also, as we proved previously, the value function is an increasing function of the budget for the same state. Using these results and \eqref{mo}, \eqref{om} and \eqref{mm}, we get that $V_{H+1}^{om} = max\{V_{H+1}^{mo}, V_{H+1}^{om}, V_{H+1}^{mm}\}$. Hence, we can conclude that for $s_0 > d$, action $o$ is the optimal initial action.

First we will consider $s_0 > d_0$. Using Lemma \eqref{lemma:3} we know that, if $b > 0$, action $d$ is the optimal action to take at time $t=0$ when $s_0 > d_0$. Hence for horizon $H+1$, if $s_0 > d_0$, the value function is given by:
\begin{equation}
    V_{H+1}(s_0,b) = r + V_H(s_0-d_0,b), \quad \forall b \geq 0 \label{V:s>d}
\end{equation}
We will now consider two cases:
\begin{enumerate}
    \item Case 1 : $s_0-d_0 > d_0$. In this case, using the results for horizon $H$, we get that $V_{H+1}(s_0,b)$ becomes constant in budget for all $b \geq \lfloor H/2 \rfloor$. Using the properties of the floor function, we have:
    \begin{equation*}
        \lfloor (H+1)/2 \rfloor \geq \lfloor H/2 \rfloor.
    \end{equation*}
    \item Case 2 : $s_0-d_0 \leq d_0$. In this case, using the results for horizon $H$, we get that $V_{H+1}(s_0,b)$ becomes constant in budget for all $b \geq \lceil H/2 \rceil$. Using the properties of the ceiling and floor functions, we have:
    \begin{equation*}
    \lceil H/2 \rceil = \lfloor (H+1)/2 \rfloor.
\end{equation*}
\end{enumerate}
Thus we can say that $V_{H+1}(s_0,b)$ becomes constant with respect to the budget for all $b \geq \lfloor (H+1)/2 \rfloor$. Also, if we consider three budget values $b^{\prime\prime} = b^\prime + 1 = b + 2$ such that $b \geq 0$, we have:
\begin{equation*}
    % \begin{aligned}
    V_{H+1}(s_0,b^{\prime\prime}) - V_{H+1}(s_0,b^\prime) = V_H(s_0-d_0,b^{\prime\prime}) - V_H(s_0-d,b^\prime)) \leq V_H(s_0-d_0,b^\prime) - V_H(s_0-d_0,b),
    % \end{aligned}
\end{equation*}
where the inequality is due to the assumption that relation \eqref{relation} holds holds for horizon $H$. Also, using \eqref{V:s>d}, we know that :
\begin{equation*}
    V_{H+1}(s_0,b^\prime) - V_{H+1}(s_0,b) = V_H(s_0-d_0,b^\prime) - V_H(s_0-d_0,b)
\end{equation*}
Hence, from this we can say that:
\begin{equation*}
    V_{H+1}(s_0,b^{\prime\prime}) - V_{H+1}(s_0,b^\prime) \leq V_{H+1}(s_0,b^\prime) - V_{H+1}(s_0,b),
\end{equation*}
and thus, relation \eqref{relation} holds true for horizon $H+1$ when $s_0 > d_0$.

%   Now, let us consider $s_0 \leq d$. We will first prove that action $m$ is the optimal action to take at time $t=0$ if $b > 0$. 

%   For a horizon $H+1$, if $s_0 \leq d$, we can either take action $o$ or action $m$ in the first step. Using the Bellman equation \cite{bellman}, we have:
%   \begin{gather}
%       V^o_{H+1}(s_0,b) = r + V_H(0,b), \label{ofirst}\\
%       V^m_{H+1}(s_0,b) = r + V_H(s_{max},b-1), \label{mfirst}
%   \end{gather}
%   where $V^o_{H+1}(s_0,b)$ and $V^m_{H+1}(s_0,b)$ represent the value functions corresponding to taking action $o$ and $m$, respectively, at time $t=0$. Since we know that $s=0$ is an absorbing state and yields a reward of $0$, using \eqref{ofirst} we have:
%   \begin{equation*}
%       V^o_{H+1}(s_0,b) = r
%   \end{equation*}
% This is equivalent to saying that the only and hence optimal policy for $V_H(0,b)$ is to take the action $o$ for $H$ steps.  Also, using previous results, we know that action $o$ is the optimal action to take when $s_0 > d$. Assume that $s_{max} = nd$ where $n = \eta H$ such that $0<\eta<1$. Then, if we are state $s_{max}$, we will reach state $s=0$ in $\lceil n \rceil$ steps even if we always take action $o$. Thus, if we apply the optimal policy for $V_H(0,b)$ to $V_H(s_{max},b-1)$, using \eqref{mfirst} we get:
%   \begin{equation*}
%       V^m_{H+1}(s_0,b) = (\lceil n \rceil + 2)r,
%   \end{equation*}
% i.e., $V^m_{H+1}(s_0,b)$ is atleast as good as $V^o_{H+1}(s_0,b)$. Thus we can say that:
% \begin{equation*}
%     V^m_{H+1}(s_0,b) \geq V^o_{H+1}(s_0,b).
% \end{equation*}
% Hence, if $b > 0$, action $m$ is the optimal action to perform at time $t=0$ if $s_0 \leq d$.
%   For a horizon $H=0$, the value function is:
%   \begin{equation*}
%       V_0(s_0,b) = \begin{cases}
%           -r,& \text{if } s_0 = 0 \\
%           r,& \text{if } 0<s_0\leq d.
%       \end{cases}
%   \end{equation*}
%   Clearly, the value function is independent of the action we choose and hence we can say that action $m$ is the optimal action to take in the first step for horizon $H=0$ when $s_0 \leq d$. Now, for a horizon $H$, assume that action $m$ is the optimal action to take at time $t=0$ for $s_0 \leq d$. We will prove that this assumption is true for a horizon $H+1$ and hence using induction, true for any $H\geq 0$. Using our assumption, the value function for horizon $H$ is given by:
%   \begin{equation*}
%       V_H(s_0,b) = \begin{cases}
%           -r + V_{H-1}(s_{max},b-1),& \text{if } s_0 = 0\\
%           r + V_{H-1}(s_{max},b-1),& \text{if } 0<s_0\leq d
%       \end{cases}
%   \end{equation*}
%   Consider a horizon of length $H+1$. We can take either action $o$ or $m$ at time $t=0$. Let the value functions for these two cases be denoted by $V^o$ and $V^m$ respectively. Then, we have:
%   \begin{gather}
%       V^o_{H+1}(s_0,b) = r + V_H(0,b), \label{vo}\\
%       V^m_{H+1}(s_0,b) = r + V_H(s_{max},b-1). \label{vm}
%   \end{gather}
%   Using the value function for horizon $H$ we know that $V_H(0,b) = -r + V_{H-1}(s_{max},b-1)$. Therefore, from \eqref{vo}, we get:
%   \begin{equation}
%       V^o_{H+1}(s_0,b) = V_{H-1}(s_{max},b-1), \label{vo1}
%   \end{equation}
%   Also, using previous results, we know that for $s_0 > d$, action $o$ is the optimal action. Assume that $s_{max} = nd$ where $n = \eta H$ such that $0<\eta<1$. Hence, using \eqref{vm}:
%   \begin{equation}
%       V^m_{H+1}(s_0,b) = 2r + V_H(s_{max}-d,b-1). \label{vm1}
%   \end{equation}
% If we are state $s_{max}$, we will reach state $s=0$ in $\lceil n \rceil$ steps if we always take action $o$.  Also, using the result for optimal policy for $s_0 > d$ and the assumption for $s_0 \leq d$, we can say that the optimal policy for \eqref{vo1} is to take action $o$ for $\lceil n \rceil - 1$ steps and then take action $m$. Thus we get:
%   \begin{equation*}
%       V^o_{H+1}(s_0,b) = (\lceil n \rceil)r + V_{H-\lceil n \rceil+1}(s_{max},b-2)
%   \end{equation*}
%   If we apply the same policy to \eqref{vm1}, we get:
%   \begin{equation*}
%       V^m_{H+1}(s_0,b) = (\lceil n \rceil)r + V_{H - \lceil n \rceil + 1}(s_{max},b-2)
%   \end{equation*}
%   Hence, we can say that for a horizon $H+1$, taking action $m$ in the first step is at least as good as taking action $o$ in the first step, i.e.:
%   \begin{equation*}
%       V^m_{H+1}(s_0,b) \geq V^o_{H+1}(s_0,b)
%   \end{equation*}
%   Thus, we can say that action $m$ is the optimal action to take at time $t=0$ for a horizon of $H+1$. Hence, using induction we have that for any horizon $H \geq 0$, action $m$ is the optimal action to take in the first step when $s_0 \leq d$.
  % \begin{enumerate}
  %     \item The optimal action in the first step is $o$ and the optimal action in the step after that is also $o$. Then we have:
  %   \begin{equation}
  %   V_{H+1}^{oo}(s_0,b) = V_{H-1}(0,b). \label{oo1}
  %   \end{equation}
  %   \item The optimal action in the first step is $o$ and the optimal action in the step after that is $m$. Then we have:
  %   \begin{equation}
  %   V_{H+1}^{om}(s_0,b) = V_{H-1}(s_{max},b-1).\label{om1}
  %   \end{equation}
  %   \item The optimal action in the first step is $m$ and the optimal action in the step after that is $o$. Then we have:
  %   \begin{equation}
  %   V_{H+1}^{mo}(s_o,b) = 2r + V_{H-1}(s_{max}-d,b-1).\label{mo1}
  %   \end{equation} 
  % \end{enumerate}
  
%   For $b > 0$ we have two possible actions $m$ and $o$. Let the value function corresponding to taking action $o$ in the first step be denoted by $V^o_{H+1}(s_0,b)$ and for taking action $m$ in the first step be denoted by $V^m_{H+1}(s_0,b)$. Then, we have:
% \begin{gather*}
%     V^o_{H+1}(s_0,b) = r(s_0,o) + V_H(0,b)\\
%     V^m_{H+1}(s_0,b) = r(s_0,m) + V_H(s_{max},b-1).
% \end{gather*}
% Intuitively, the optimal action to take at $s_0 = 0$ is $m$ because it would prevent us from getting a negative reward for staying in state $0$. Thus, we have $V_H(0,b) = -r + V_{H-1}(s_{max},b-1)$. Also since we know from previous results that action $o$ is optimal for $s_o > d$, we have $V_H(s_{max},b-1) = r + V_{H-1}(s_{max}-d,b-1)$. Hence, overall we have:
% \begin{gather}
%     V^o_{H+1}(s_0,b) =  V_{H-1}(s_{max},b-1), \label{vo}\\
%     V^m_{H+1}(s_0,b) = 2r + V_{H-1}(s_{max}-d,b-1) \label{vm}.
% \end{gather}
% Let us assume that $s_{max} = nd$ where $n = \eta H$ with $0 < \eta <1$. Now, for a horizon $H-1$, if we assume that the optimal policy for being in state $s_{max}$ with budget $b-1$ is to take action $o$ for $n-1$ steps and then take the $m$ action, using \eqref{om1} and \eqref{mo1} we get:
% \begin{gather*}
%     V^{om}_{H+1}(s_0,b) = (n-1)r + V_{H-n-2}(s_{max},b-2) \\
%     V^{mo}_{H+1}(s_0,b) = (n-1)r + V_{H-n-2}(s_{max},b-2).
% \end{gather*}
% Hence, we can say that the value function $V^{mo}_{H+1}(s_0,b)$ is atleast as good as $V^{om}_{H+1}(s_0,b)$ and thus:
% \begin{equation*}
%     V^{mo}_{H+1}(s_0,b) \geq V^{om}_{H+1}(s_0,b).
% \end{equation*}
% Similarly, assume that for a horizon of $H-1$, the optimal policy for being in state 0 with budget $b$ is to take  action $m$ in the first step and then take action $o$ for the next $n-1$ steps. Then, using \eqref{oo1}:
% \begin{equation*}
%     V^{oo}_{H+1}(s_0,b) = 
% \end{equation*}
% Using \eqref{vo}, we have that $V^o_{H+1}(s_0,b) = (n-1)r + V_{H-n-2}(s_{max},b-2)$. Now, if we apply to same policy to starting state $s_{max}-d$ instead of $s_{max}$, we get:
% \begin{equation*}
%     V_{H-1}(s_{max}-d,b-1) = (n-3)r + V_{H-n-2}(s_{max},b-2).
% \end{equation*}
% Using \eqref{vm}, we have that $V^m_{H+1}(s_0,b) = (n-1)r + V_{H-n-2}(s_{max},b-2)$. Hence, we can say that the optimal policy for $V^m_{H+1}(s_0,b)$ is atleast as good as $V^o_{H+1}(s_0,b)$ and thus:
% \begin{equation*}
%     V^m_{H+1}(s_0,b) \geq V^o_{H+1}(s_0,b).
% \end{equation*}
% Therefore, action $m$ is the optimal action to take when $s_0 \leq d$ and $b > 0$.

Using Lemma \eqref{lemma:4} we know that, if $b > 0$, action $m$ is the optimal action to take at time $t=0$ when $0 < s_0 \leq d_0$. Hence for horizon $H+1$, if $0 < s_0 \leq d_0$, the value function is given by:
\begin{equation}
    V_{H+1}(s_0,b) = \begin{cases}
        r + V_H(s_0-d_0,b),& \text{if } b = 0\\
        r + V_H(s_{max},b-1),& \text{if } b > 0.
    \end{cases}, \label{V:s<d}
\end{equation}
Note that if $s_0 = 0$, then we are in the absorbing state and we get $V(0,b) = 0$ for all $b \geq 0$. Thus, we can say that $V(0,b)$ is constant with respect to budget for all $b \geq \lceil (H+1)/2 \rceil$.

 Now, for $b > 0$, using \eqref{V:s<d}  and the results for horizon $H$, we get that $V_{H+1}(s_0,b)$ becomes constant in budget for all $b-1 \geq \lfloor H/2 \rfloor$ or $b \geq \lfloor H/2 \rfloor + 1$. From the properties of the floor and ceiling functions, we know that:
 \begin{equation*}
     \lfloor H/2 \rfloor + 1 = \lceil (H+1)/2 \rceil
 \end{equation*}
 % $\lfloor H/2 \rfloor + 1  \lceil H/2 \rceil + 1$ (equality holds when $H$ is even). Also,
 % \begin{equation*}
 %     \lceil H/2 \rceil + 1 = \lceil (H+1)/2 \rceil
 % \end{equation*}
 Hence, we can say that for a horizon $H+1$ and $s_0 \leq d_0$, the value function $V_{H+1}(s_0,b)$ becomes constant in b for all $b \geq \lceil (H+1)/2 \rceil$. 
 
Now, to prove that relation \eqref{relation} holds for $s_0 \leq d_0$, we will consider two cases:
\begin{enumerate}
    \item Consider three budget values $b^{\prime\prime} = b^\prime + 1 = b + 2$ such that $b>0$. Then, using \eqref{V:s<d} we have:
    \begin{equation*}
        V_{H+1}(s_0,b^{\prime\prime}) - V_{H+1}(s_0,b^\prime) = V_H(s_{max},b^{\prime\prime}-1) - V_H(s_{max},b^\prime-1)) \leq V_H(s_{max},b^\prime-1) - V_H(s_{max},b-1),
    \end{equation*}
    where the inequality is due to the assumption that relation \eqref{relation} holds for horizon $H$. Also, using \eqref{V:s<d} we know that:
    \begin{equation*}
        V_{H+1}(s_0,b^\prime) - V_{H+1}(s_0,b) = V_H(s_{max},b^\prime-1) - V_H(s_{max},b-1).
    \end{equation*}
    Hence, we using the above equations we can say that:
    \begin{equation*}
        V_{H+1}(s_0,b^{\prime\prime}) - V_{H+1}(s_0,b^\prime) \leq V_{H+1}(s_0,b^\prime) - V_{H+1}(s_0,b)
    \end{equation*}
    \item Consider three budget values $b^{\prime\prime} = b^\prime + 1 = b + 2$ such that $b=0$. If we are at $s = s_{max}$, we will reach $s=0$ in $\lceil \frac{s_{max}}{d_0} \rceil$ steps if we take only action $o$ repeatedly. We will now consider two cases:
    \begin{enumerate}
        \item Case 1 :  $\lceil \frac{s_{max}}{d_0} \rceil < H$.
        Using \eqref{V:s<d} we have:
    \begin{gather*}
        V_{H+1}(s_0,b) = r + V_H(0,b) = r\\
        V_{H+1}(s_0,b^\prime) = r + V_H(s_{max},b^\prime-1) = (\lceil \frac{s_{max}}{d_0} \rceil + 1)r\\
        V_{H+1}(s_0,b^{\prime\prime}) = r + V_H(s_{max},b^{\prime\prime}-1) = (2\lceil \frac{s_{max}}{d_0} \rceil)r.
    \end{gather*}
    Using the above equations, we have:
    \begin{gather*}
        V_{H+1}(s_0,b^{\prime\prime}) - V_{H+1}(s_0,b^\prime) = (\lceil \frac{s_{max}}{d_0} \rceil-1)r\\
        V_{H+1}(s_0,b^\prime) - V_{H+1}(s_0,b) = (\lceil \frac{s_{max}}{d_0} \rceil)r,
    \end{gather*}
    and thus we can say that:
    \begin{equation*}
        V_{H+1}(s_0,b^{\prime\prime}) - V_{H+1}(s_0,b^\prime) \leq V_{H+1}(s_0,b^\prime) - V_{H+1}(s_0,b)
    \end{equation*}

        \item Case 2 : $\lceil \frac{s_{max}}{d_0} \rceil \geq H$. Using \eqref{V:s<d} we have:
        \begin{gather*}
        V_{H+1}(s_0,b) = r + V_H(0,b) = r\\
        V_{H+1}(s_0,b^\prime) = r + V_H(s_{max},b^\prime-1) = (H + 1)r\\
        V_{H+1}(s_0,b^{\prime\prime}) = r + V_H(s_{max},b^{\prime\prime}-1) = (H + 1)r.
    \end{gather*}
    Using the above equations, we have:
    \begin{gather*}
        V_{H+1}(s_0,b^{\prime\prime}) - V_{H+1}(s_0,b^\prime) = 0\\
        V_{H+1}(s_0,b^\prime) - V_{H+1}(s_0,b) = Hr,
    \end{gather*}
    and thus we can say that:
    \begin{equation*}
        V_{H+1}(s_0,b^{\prime\prime}) - V_{H+1}(s_0,b^\prime) \leq V_{H+1}(s_0,b^\prime) - V_{H+1}(s_0,b)
    \end{equation*}
    \end{enumerate}
    
\end{enumerate}
 
 Hence, we can say that relation \eqref{relation} holds for horizon $H$ when $s_0 \leq d_0$. 

 

 Thus, using induction, we have shown that for any horizon of length $H \geq 0$ the following holds:
 \begin{itemize}
     \item The value function becomes constant with respect to the budget for $b \geq \lfloor H/2 \rfloor$ if $s_0 > d_0$ and $b \geq \lceil H/2 \rceil$ if $s_0 < d_0$.
     \item The increase in the value function decreases with increase in the budget.
 \end{itemize}
\end{proof}

% \textbf{Theorem:}  First, we will prove that the value function $V_H(s_0, b)$ for an initial state $s_0 \in \states$, budget $b \in \mathbb{N}_0$ and horizon length $H \in \mathbb{N}_0$ is an increasing function of the budget. Next, using mathematical induction we will prove that:
% \begin{itemize}
%     \item $V_H(s_0,b)$ becomes constant with respect to the budget after a certain budget value which depends on $c_m$ and $H$.
%     \item For budget values $b$,$b^\prime$ and $b^{\prime \prime}$ such that $b < b^\prime \leq b^{\prime\prime}$ and $b^\prime \geq \frac{b + b^{\prime\prime}}{2}$ , we have:
%     \begin{equation}
%         V_H(s_0,b^{\prime \prime}) - V_H(s_0,b^\prime) \leq V_H(s_0,b^{\prime}) - V_H(s_0,b), \label{relation}
%     \end{equation}
% \end{itemize}

% \textbf{Proof:} It is trivial to prove that $V_H(s_0,b)$ is an increasing function of the budget $b$. Consider two budget values $b$ and $b^\prime$ such that $b<b^\prime$. Consider an optimal policy for initial state $s_0$ and budget $b$ which results in $V_H(s_0,b)$. The same policy will also be optimal for the same initial state $s_0$ but budget $b^\prime$. This is because following the same policy will result in the same trajectory of states for both budgets, but with a higher budget at each time step for $b^\prime$. Hence, the value function generated by this policy with budget $b^\prime$ will be at least as good as the value function generated by the same policy with budget $b$. Since this holds for any optimal policy, it follows that:
% \begin{equation*}
%     V_H(s_0,b^\prime) \geq V_H(s_0,b) \quad \forall s_0, b^\prime > b
% \end{equation*}
 
 % Then, for the same initial state $s_0$, $V_H(s_0,b) \leq V_H(s_0,b^\prime)$ because following the same optimal policy which generates $V_H(s_0,b)$, but starting from budget $b^\prime > b$ will generate the exact same trajectory of states as for budget $b$ but will always have a higher available budget, $b^\prime - b$, at each time step.
 

% We will now prove the other two claims. First, we will show that the claims hold for a horizon of length 0. Then, using induction we will prove that they hold for a horizon of length $H$. 

% Consider a horizon of length 0. For this case we have:
% \begin{equation*}
%     V_0(s_0,b) = \begin{cases}
%         r,& \text{ if } s_0 > 0\\
%         -r,& \text{ if } s_0 = 0,
%     \end{cases}
% \end{equation*}
% where $b \geq 0$. Clearly, $V_0(s_0,b)$ is constant in the budget for all $b \geq 0$ and hence for a horizon of length $H=0$ and for all values of $s_0 > 0$ we can say that:
% \begin{equation*}
%     V_0(s_0,b^{\prime\prime}) - V_0(s_0,b^\prime) = 0 \leq V_0(s_0,b^\prime) - V_0(s_0,b),
% \end{equation*}
% where $b<b^\prime \leq b^{\prime\prime}$. 
% % We also have that $V_0(s_0,b)$ is constant for all $b  \geq 0$ and a given $s_0 \in \states$.

% Now, for a horizon of length $H$, assume the following:
% \begin{itemize}
%     \item For $s_0 > d$, $V_H(s_0,b)$ is constant  with respect to the budget for all $b \geq \lfloor H/2 \rfloor$, where $\lfloor . \rfloor$ represents the floor function, 
%     \item For $s_0 \leq d$, $V_H(s_0,b)$ is constant with respect to the budget for all $b \geq \lceil H/2 \rceil$, where $\lceil . \rceil$ represents the ceiling function,
%     \item Relation \eqref{relation} holds true.
% \end{itemize}
% As can be clearly seen, the above assumptions hold true for $H=0$ as we proved previously. Now, assume that they hold for some $H > 0$ and consider a horizon of length $H+1$. First we'll prove that, if $b > 0$, the optimal action to take at time $t=0$ is $m$ if $s_0 \leq d$ and $o$ if $s_0 > d$.


% Clearly, $V_H(0,b) \leq V_H(s_{max},b-1)$ because we will incur a negative reward for being in state $s=0$. Hence, action $m$ is the optimal action to take when $s_0 \leq d$ and $b > 0$.

% Consider $s_0 > d$: First let's consider the case when we start at state $s_{max}$. Let's say the first optimal action is $m$ and the optimal action in the step after that is $o$. Then we get:
% \begin{equation}
%     V_{H+1}^{mo}(s_{max},b) = 2r + V_{H-1}(s_{max}-d,b-1). \label{mo}
% \end{equation}
% Now consider the case when the first optimal action is $o$ and the optimal action in the step after that is $m$. Then we get:
% \begin{equation}
%     V_{H+1}^{om}(s_{max},b) = 2r + V_{H-1}(s_{max},b-1).\label{om}
% \end{equation}
% If $s_{max}$ is such that $s_{max} = nd$, where $n>1$, then both \eqref{mo} and \eqref{om} will yield the same value and hence taking action $m$ first followed by $o$ is the same as taking $o$ first followed by $m$. 
% Finally, consider the case where the first optimal action is $m$ and the optimal action in the next step is also $m$. Then, we get:
% \begin{equation}
%     V_{H+1}^{mm}(s_{max},b) = 2r + V_{H-1}(s_{max},b-2).\label{mm}
% \end{equation}
% For the same value of budget, we can say from intuition that the value function is an increasing function of the state. Also, for the same state, the value function is an increasing function of the budget. Hence, using \eqref{mo}, \eqref{om} and \eqref{mm}, we get that $V_{H+1}^{mm} \leq V_{H+1}^{mo} \leq V_{H+1}^{om}$. Hence, we can conclude that at $s_{max}$, action $o$ is the optimal initial action.

% Note that we do not consider the case of taking action $o$ followed by another $o$ action because we just need to prove that taking action $o$ in the first step is optimal and hence the inequality derived above is sufficient.

% The same process can be repeated to show that the derived inequality holds true for any $s_0 > d$ such that $s_0 - d > d$. Hence, we can say that action $o$ is the optimal action to take in the first step for all $s_0 > d$ such that $s_0-d > d$.

% Using the above results for horizon $H+1$, if $s_0 > d$, the value function is given by:
% \begin{equation*}
%     V_{H+1}(s_0,b) = r(s_0,o) + V_H(s_0-d,b), \quad \forall b \geq 0
% \end{equation*}
% Clearly, $V_{H+1}$ is the sum of instantaneous reward for being in state $s_0$ and the optimal value function for horizon $H$ corresponding to the case of $s_0 \leq d$. Thus, using the results for horizon $H$, we get that $V_{H+1}(s_0,b)$ becomes constant in budget for all $b \geq \lceil H/2 \rceil$. Using the properties of the ceiling and floor functions, we have:
% \begin{equation*}
%     \lceil H/2 \rceil = \lfloor (H+1)/2 \rfloor
% \end{equation*}
% Hence, we can say that for a horizon $H+1$ and $s_0 > d$, the value function $V_{H+1}(s_0,b)$ becomes constant in b for all $b \geq \lfloor (H+1)/2 \rfloor$. Also, if we consider three budget values $b < b^\prime < b^{\prime\prime}$ such that $b^\prime \geq \frac{b + b^{\prime\prime}}{2}$, we have:
% \begin{aligned*}
%     V_{H+1}(s_0,b^{\prime\prime}) - V_{H+1}(s_0,b^\prime) &= V_H(s_0,b^{\prime\prime}) - V_H(s_0,b^\prime))\\
%     & \leq V_H(s_0,b^\prime) - V_H(s_0,b),
% \end{aligned*}
% where the inequality is due to the assumption that relation \eqref{relation} holds holds for horizon $H$. Hence, from this we can say that relation \eqref{relation} holds true for horizon $H+1$ when $s_0 > d$.
% % If $b > 0$, we can take either action $m$ or $o$. Thus, for an initial state $d < s_0 < s_{max}$, we have:
% % \begin{gather}
% %     V^o_{H+1}(s_0,b) = r(s_0,o) + V_H(s_0-d,b), \label{eqo1}\\
% %     V^m_{H+1}(s_0,b) = r(s_0,m) + V_H(s_{max},b-1), \label{eqm1}
% % \end{gather}
% Consider the case : $s_0-d \leq d$. For $b > 0$, we can take either action $m$ or $o$. Thus we have:
% \begin{gather}
%     V^o_{H+1}(s_0,b) = r(s_0,o) + V_H(s_0-d,b), \label{eqo1}\\
%     V^m_{H+1}(s_0,b) = r(s_0,m) + V_H(s_{max},b-1), \label{eqm1}
% \end{gather}
% In this case, using the previous results for $s_0 < d$, we know that $m$ is the optimal action since $s_0 -d \leq d$. Hence $V_H(s_0-d,b) = r(s_0-d,m) + \gamma V_{H-1}(s_{max}, b-1)$. Also, as proved above, taking action $o$ is optimal when at $s_{max}$. From this we have $V_H(s_{max},b-1) = r(s_{max},o) + \gamma V_{H-1}(s_{max}-d,b-1)$. From intuition, we can say that $V_{H-1}(s_{max} - d, b-1) \leq V_{H-1}(s_{max}, b-1)$ since for the same value of budget, we are at a higher starting state for the latter. Thus, using \eqref{eqo1} and \eqref{eqm1} we have $V^o_{H+1}(s_0,b) \geq V^m_{H+1}(s_0,b)$ and we can say that taking action $o$ at time $t=0$ is optimal.

% Also, for both \eqref{eqo1} and \eqref{eqm1}, the instantaneous reward is the same because we are starting in the same $s_0$. For \eqref{eqo1}, the value function $V_H$ becomes constant and attains it's maximum with respect to the budget for $b \geq \lceil H/2 \rceil$ and for \eqref{eqm1}, the value function attains it's maximum with respect to the budget at $b-1 \geq \lfloor H/2 \rfloor$ or $b \geq \lfloor H/2 \rfloor + 1$. Clearly, for all values of $H$, we have $\lceil H/2 \rceil \leq \lfloor H/2 \rfloor + 1$ where the equality holds for odd values of $H$. This means that $V_{H+1}$ attains its maximum value faster for \eqref{eqo1} as compared to \eqref{eqm1}. Hence, we can say that for $s_0 > d$, it is optimal to take the $o$ action at the first step irrespective of the budget.

% Hence, we can say that action $o$ is the optimal action to take in the first step for all $s_0 > d$.

% Consider the case: $s_0 -d > d$.
% Let's assume that for a given initial state $s_0$ such that $s_0-d > d$ and $b > 0$, action $m$ is the optimal initial action when we start at state $s_0 - d$ i.e. $V^m_{H+1}(s_0,b) \geq V^o_{H+1}(s_0,b)$. Using the above assumption, we get:
% \begin{gather}
%     V_H(s_0-d,b) = r(s_0-d,m) +  V_{H-1}(s_{max},b-1), \label{assump1}\\
%     V_H(s_{max},b-1) = r(s_{max},0) + V_{H-1}(s_{max}-d,b-1). \label{assump2}
% \end{gather}
% where we get \eqref{assump2} using the result that action $o$ is optimal when we are at state $s_{max}$. We know that for any horizon, the value function $V(s,b)$ is an increasing function of the budget. Hence, from \eqref{assump1} and \eqref{assump2} we can say that $V_H(s_0-d,b) \geq V_H(s_{max},b-1)$. Using this relation and equations \eqref{eqo1} and \eqref{eqm1}, we have that $V^o_{H+1}(s_0,b) \geq V^m_{H+1}(s_0,b)$. This contradicts our assumption and hence our assumption was wrong. Thus, action $o$ is the optimal initial action for a given state $s_0$ such that $s_0-d > d$. We can similarly prove this for all possible such values of $s_0$.


% Again, if $b = 0$, we are forced to take the $o$ action since we don't have enough budget to perform a single maintenance action. If $b > 0$, we can take either action $m$ or $o$. Thus, we have:
% \begin{gather}
%     V^o_{H+1}(s_0,b) = r(s_0,o) + \gamma V_H(s_0-d,b), \label{eqo1}\\
%     V^m_{H+1}(s_0,b) = r(s_0,m) + \gamma V_H(s_{max},b-1), \label{eqm1}
% \end{gather}
% Since, we know that when $s_0 \leq d$, the optimal action is $m$, $V_H(s_0-d,b) = r(s_0-d,m) + \gamma V_{H-1}(s_{max}, b-1)$. Also, if we are already at $s_{max}$, intuitively action $o$ is the optimal action. Therefore, $V_H(s_{max},b-1) = r(s_{max},o) + \gamma V_{H-1}(s_{max} - d, b-1)$. From intuition, we can say that $V_{H-1}(s_{max} - d, b-1) \leq V_{H-1}(s_{max}, b-1)$ since for the same value of budget, we are at a higher starting state for the latter. Hence we can say that when $s_0 > d$, action $o$ is the optimal action.



% Using the above results for horizon $H+1$, if $s_0 > d$, the value function is given by:
% \begin{equation*}
%     V_{H+1}(s_0,b) = r(s_0,o) + V_H(s_0-d,b), \text{ } \forall \text{ } b \geq 0
% \end{equation*}
% Clearly, $V_{H+1}$ is the sum of instantaneous reward for being in state $s_0$ and the optimal value function for horizon $H$ corresponding to the case of $s_0 \leq d$. Thus, using the results for horizon $H$, we get that $V_{H+1}(s_0,b)$ becomes constant in budget for all $b \geq \lceil H/2 \rceil$. Using the properties of the ceiling and floor functions, we have $\lceil H/2 \rceil \leq \lfloor H/2 \rfloor + 1$ (equality holds when $H$ is odd). Also, using the properties of the floor function we have:
% \begin{equation*}
%     \lfloor H/2 \rfloor + 1 = \lfloor(H+2)/2 \rfloor = \lfloor (H+1)/2 \rfloor
% \end{equation*}
% Hence, we can say that for a horizon $H+1$ and $s_0 > d$, the value function $V_{H+1}(s_0,b)$ becomes constant in b for all $b \geq \lfloor (H+1)/2 \rfloor$.
% \begin{equation*}
%     V_{H+1}(s_0,b_1) \leq V_{H+1}(s_0,b_2) \leq \text{. . . . . . } \leq V_{H+1}(s_0,b_{\lceil H/2 \rceil +1})
% \end{equation*}
% Now, using the property of the ceiling function, $\lceil x \rceil + 1$ = $\lceil x+1 \rceil$ for $x \geq 0$. Hence $\lceil H/2 \rceil + 1 = \lceil (H+2)/2 \rceil = \lceil (H+1)/2 \rceil + 1$. Hence the above equation can be re-written as:
% \begin{equation*}
%     V_{H+1}(s_0,b_1) \leq V_{H+1}(s_0,b_2) \leq \text{. . . . . . } \leq V_{H+1}(s_0,b_{\lceil (H+1)/2 \rceil + 1})
% \end{equation*}
% The above inequality implies that the value function becomes constant for $b \geq \lceil (H+1)/2 \rceil$.
% Also, we know that:
% \begin{equation*}
%     \lceil (H+1)/2 \rceil = \begin{cases}
%         \lfloor (H+1)/2 \rfloor,& \text{if } \frac{H+1}{2} \in \mathbb{Z} \\
%         \lfloor (H+1)/2 \rfloor + 1,& \text{if } \frac{H+1}{2} \notin \mathbb{Z}
%     \end{cases}
% \end{equation*}
% Hence, we can say that the value function becomes constant for $b \geq \lfloor (H+1)/2 \rfloor$. Thus for horizon $H+1$ with $s_0 > d$, we have:
% \begin{equation*}
%     V_{H+1}(s_0,b_1) \leq V_{H+1}(s_0,b_2) \leq \text{. . . . . . } \leq V_{H+1}(s_0,b_{\lfloor (H+1)/2 \rfloor + 1})
% \end{equation*}
% with $b_i \in C_{i-1}$ for $i < \lfloor (H+1)/2 \rfloor +1$ and $b_{\lfloor (H+1)/2 \rfloor +1} \in C_{k>\lfloor (H+1)/2 \rfloor}$ i.e. or in other words, for a horizon of $H+1$ the optimal value function $V_{H+1}$ increases with b and becomes constant for $b \geq \lfloor (H+1)/2 \rfloor$. 

\begin{corollary}
The proof of theorem \ref{theorem:1} and lemma \ref{lemma:2} implies the concavity of the value function with the budget, i.e., if $b^\prime = \alpha b + (1-\alpha)b^{\prime\prime}$ for some $\alpha \in [0,1]$, then we have
\begin{equation}
    V_H(s_0,b^\prime) = \alpha V_H(s_0,b) + (1-\alpha)V_H(s_0,b^{\prime\prime}) \label{eq:concavity}
\end{equation}
\end{corollary}
  
% Note that the above proof also holds true for budget values $b < b^\prime < b^{\prime\prime}$ such that $b^\prime = \frac{b+b^{\prime\prime}}{2}$.

% This proof can be directly extended to non-deterministic cases where the decrease $d$ will be replaced by the expected decrease $\mathbb{E}[d]$.
This proof can be easily extended to any general $c_m \in \mathbb{R}^+$ by scaling the costs and budget with $1/c_m$. Furthermore, this proof works under heavy technical assumptions of full observability and deterministic transitions. However, we empirically observe that that same property is often true for general systems (partially observable and stochastic) and we believe the same proof approach could work, and we leave it for future work.

 % We can also prove a similar result for POMDPs by performing the analysis on the value function for the belief-state MDP. Hence, we can say that the optimal value function for our POMDP is a concave function of the budget $b$.

 We will now use this concavity property to obtain the optimal budget split among the component POMDPs of an $n$-component POMDP. Doing so would provide the approximately optimal policy for the individual component POMDPs and hence the $n$-component POMDP.


% Consider an MDP with state space $\states$ such that $s \leq s_{max}$ $ \forall s \in  \states$. The action space is $\actions = \{m,o\}$ and the transition probability function is defined as:
% \begin{equation*}
%     T(s^{\prime}|s,a) = \begin{cases}
%     1,& \text{if } s^{\prime} = s_{max} \text{ and } a = \text{m} \\
%     1,& \text{if } s^{\prime} = s-d \text{ and } a = \text{o}\\
%     0,& \text{otherwise}
%     \end{cases}
% \end{equation*}
% Here, d is a decrease in the state value such that $d > \frac{s_{max}}{2}$. The cost for the $o$ action is $c_o = 0$ and the cost for the $m$ action is $c_m > 0$. The reward function is similar to our original problem with a positive reward $r(s,a)$ for all states $s > 0$ and an equal negative reward for $s=0$.

% Let $V_H$ represents the value function for a given horizon length with $H$ steps to go. Then we prove that for all values of $H$, the following relations hold:
% \begin{itemize}
%     \item The optimal value function $V_H(s_0,b)$, for an initial state $s_0$ and budget $b$ increases with increase in budget.
%     \item For a horizon of $H$, $V_H(s_0,b)$ becomes constant with respect to the budget for $b \geq (\lfloor H/2 \rfloor)c_m$ if $s_0 > d$ and for $b \geq (\lceil H/2 \rceil)c_m$ if $s_0 \leq d$.
%     \item The increase in $V_H(s_0,b)$, with the budget, decreases with increasing $b$ i.e. for all $s_0 \in \states$ and budget values $b,b^\prime$ and $b^{\prime\prime}$, where $b < b^\prime < b^{\prime\prime}$, we have:
%     \begin{equation*}
%         V_H(s_0,b^{\prime\prime} - V_H(s_0,b^\prime) \leq V_H(s_0,b^\prime) - V_H(s_0,b)
%     \end{equation*}
% \end{itemize}
% In summary, we show that the optimal value function is an increasing function of the budget $b$, where the magnitude of increase decreases with increasing budget, and it saturates to a maximum value at some value of $b$ which depends on the horizon. This implies that the optimal value function can be approximated by a concave function of the budget.

% We use the method of mathematical induction to prove the above mentioned properties. First, we show that these properties hold true for a horizon of length 1. Next, we assume that these properties are true for a horizon of length $H$. Finally, we show that the optimal value function for a horizon of length $H+1$ also has the same properties by manipulating the Bellman equation given by :
% \begin{equation*}
%     V_{H+1}(s_0,b) = r(s_0,a) + \gamma V_H(s^\prime, b^\prime)
% \end{equation*}
% where $s_0,s \in \states$, $s \in \actions$ and $s^\prime \text{ and } b^\prime$ are decided based on $s_0 \text{ and } a$. Also, since the optimal value function is the total expected return for a given horizon, we can say that the expected return can also be approximated by a concave function of the budget.

% This proof can be directly extended to the belief state MDP for our problem (where the states will be replaced by the belief states and the decrease $d$ will become the expected decrease $\mathbb{E}[d]$) and hence can be used to prove concavity of the optimal value function for the partially observable case. A detailed version of the proof can be found in the arxiv document \textbf{THIS}.
\subsection{Optimal Policy Synthesis for Multi-Component POMDP}
Consider a multi-component POMDP with $n$ components and budget $B$ as described in Section ~\ref{subsec:multicomponent}. The size of the state space is $(|\states|)^n$ where $|\states|$ is the size of the state space of each component POMDP. Also, the size of the total action space is $3^n$. Directly applying a POMDP solver to such a large state and action space may not be computationally feasible. Hence, we propose an algorithm which decouples the $n$ component POMDPs by allocating a portion of the total budget to each of them prior to the beginning of the system run. We then compute the  approximate value function for each component POMDP as a function of the budget and then using that, obtain the optimal split of the total budget. 

% If we consider $n$ such POMDPs operating under a total budget of $B$ and if we assume that the state space of each POMDP is of the size $|\states|$, then the size of the total state space for $n$ POMDPs will be $|\states|^n$ and the size of the total action space will be $3^n$. The POMCP algorithm cannot be applied directly to such exponentially large state and action spaces. Hence, we propose an algorithm which computes the optimal policy for each of the $n$ POMDPs by calculating the optimal split of the total budget among all the POMDPs. 

Given a total budget $B$, we assume that the $i^{th}$ component POMDP is alloted a budget $b_i$ from the total budget. Hence,
\begin{equation}
    b_1 + b_2 + \ldots + b_n = B \label{eq1}
\end{equation}
We now have $n$ independent POMDPs, where each POMDP has its own total budget. We formulate each of them as a b-POMDP and solve each b-POMDP using the POMCP algorithm as discussed in Section ~\ref{subsec:budgeted-POMDP}. 
% The optimal policy and hence the maximal collected reward for the $i^{th}$ component b-POMDP will depend on the budget $b_i$ allocated to it. 
Let the maximal collected reward, for component $i$, obtained using the POMCP algorithm, for a given initial state $s_0$ and horizon $H$, be denoted by $V^i_H(s_0,b_i)$. We can then find the optimal budget split among the $n$ b-POMDPs by solving a welfare maximization problem. Welfare maximization is the concept of maximizing the overall well-being or welfare of a society, and is achieved by maximizing some measure of social welfare (e.g. maximal collected reward). We thus maximize the total maximal collected reward, for all components, with respect to $b_i$ while adhering to the constraint in \eqref{eq1}, i.e.,
\begin{equation}
\begin{aligned}
&\max_{b_i} \sum_{i = 1}^{n}V^i_H(s_0,b_i)\\
&\text{s.t.} \sum_{i = 1}^{n}b_i = B. \label{maximize}
\end{aligned}
\end{equation}
Using the results we proved in Section \ref{subsec:proof}, we know that $V_H(s_0,b_i)$ is a concave function of $b_i$ in the special case mentioned in Section \ref{subsec:proof} and emperically observe it to be concave in general. The welfare maximization problem then becomes a constrained convex optimization problem. Hence, it can be solved easily and is guaranteed to have a global optimum. The solution to \eqref{maximize} provides the optimal budget allocation for each b-POMDP which in-turn gives us the optimal policy for all $n$ component POMDPs. Let $\pi_i : \states^i \rightarrow A^i$ be a policy for component $i$ with budget $b_i$ obtained using the POMCP algorithm. Then, we define the overall policy, for the $n$-component POMDP, $\pi : \states \rightarrow A$ by $$\pi = \prod_{i=1}^n \pi_i.$$
While such a policy is naturally not guaranteed to be generally optimal on the multi-component POMDP, it provably satisfies the budgetary constraints and performs well in practice. To illustrate its performance on real data, we now move to the implementation and evaluation section. 
% We compare the perfomance of the proposed algorithm with a realistic baseline for a multi-component infrastructure management scenario.
% We cannot directly use the POMCP algorithm for n POMDPs owing to the exponentially large state space ($|\S|^n$) and action space ($3^n$). Hence, we propose an algorithm that optimally splits the total budget $B$ among the n POMDPs and then finds the optimal policy for each POMDP with the corresponding allocated budget. 

% Our proposed heuristic that balances between exploring locations with high value and exploring locations with high \textit{expected} value while also ensuring minimum viability of the mission. Given that the agent makes decisions based on the distribution of reward values at different states, the key to maximizing the total reward in our scenario would be to consider both of the following objectives: (i) explore potentially valuable locations that either likely to have a high true expected value or provide us with information about the sampling value at many other similar locations, and (ii) exploit the current estimates by sampling at locations where, with high confidence, we have a higher expected value than other uncertain locations. The first objective ensures that we maximize the long term total collected reward where as the second objective ensures that we maximize the collected sampling value in the short term and meet the minimum mission requirements before a potential failure.

% As discussed in the previous section, selecting the sampling locations solely on the basis of expected reward values is highly influenced by the informative prior selected for the expected values and may lead to sub-optimal policies. Given that the agent has access to the entire probability distribution of the reward values, it can exploit the uncertainty in the distribution to direct the exploration process. Specifically, at each decision step $k, k \in [H]$, we use the following metric as the reward of each sampling location:

% $$
% \tilde{R}(s_i) = \mu_{s_i} + \alpha \left(\frac{k}{H} \right) \sigma_{s_i}
% $$

% where $H$ is the horizon of the sampling MDP $M_\tss{sampling}$, $\alpha > 0$ is a tunable parameter, $\mu_i$ and $\sigma_i = \Sigma(i,i)$ are the expected value and the standard deviation of the reward distribution at location corresponding to the state $s_i$. Intuitively, the above reward function ensures that the agent samples conservatively in the early stages of the sampling process by sampling at locations with high expected sampling value. With time, as the agent collects more samples thereby ensuring that minimum mission requirements are met, it is incentivized to explore remaining uncertain location by providing optimistic estimates of the reward at those locations. 

% Algorithm \ref{alg:heuristic} summarizes the steps involved in finding the optimal policy using the proposed heuristic.

% \begin{algorithm}
%     \caption{\emph{EndOptimism}: Optimistic Heuristic}\label{alg:heuristic}
%     \begin{algorithmic}[1]
    
%     \State \textbf{input:} Markov decision process $(\states, \actions, P, R, H, s_o)$ 
%     \State \textbf{initialize:} $t = 0, s_t = s_o, \tilde{V}^*_H(\cdot) = 0$
%     \Repeat
%         \State $\tilde{R}(s^i) = \mu_{i} + \alpha \left(\frac{t}{H} \right) \sigma_{i} \ \forall \ s^i  \in \states$
%         \ForAll{$i = H-1, H-2, \ldots, t$}
%             \ForAll{$s \in \states$}
%                 \State $\tilde{V}^*_i(s) =  \max\limits_a \left[ \tilde{R}(s) + \EE_{s'}(\tilde{V}^*_{i+1}(s')) \right]$
%                 \State $\tilde{\pi}^{*}_i(s) = \argmax\limits_{a} \left[ \tilde{R}(s) + \EE_{s'} (\tilde{V}^*_{i+1}(s')) \right]$
%             \EndFor
%         \EndFor
%         \State $f(\bm{X} \vert X_{s_t} = \delta_{s_t}) = \mathcal{N}(\overline{\bm{\mu}},\overline{\bm{\Sigma}})$
%         \State $R(s_t) = 0$
%         \State $s_{t+1} \sim P( \cdot | s_{t}, \tilde{\pi}^*_t(s_{t}))$
%         \State $t = t + 1$
%     \Until{$t = H-1$}
    
%     \end{algorithmic}
%     \end{algorithm}

% Now that we proposed the heuristic for selecting the sequence of sampling locations to maximize the total collected sampling value, we now proceed to demonstrate its utility on two case studies.

% \begin{figure}[tb]
%     \centering
%     \includegraphics[scale=0.6]{figures/grid_values.pdf}
%     \caption{Distribution of total sampling value obtained by running 10000 simulations of the grid-world environment with 50 candidate sampling locations and a failure rate of 0.003.}
%     \label{fig:gridvalue}
%  \end{figure}

% Table \ref{tab:europa} shows the results from the experiment comparing the baseline approach to the proposed approach for different failure rates. While the performance of the baseline approach and the proposed approach are comparable for the lower failure rate scenario, the proposed approach performs significantly better for the higher failure rate scenario which is consistent with the results of the previous experiment.

% \begin{table}[htb]
% 	\caption{Average total collected sampling value comparison}
% 	\label{tab:europa}
%   \setlength{\tabcolsep}{.5em}
%   \centering
%   \begin{tabular}{c|cc}https://www.overleaf.com/project/638b81208f97ce673eebc537
%     Failure Rate $\lambda$ & Baseline  & Proposed \\
%     \midrule
%     0.001 & 8.2 & 8.6  \\
%     0.005 & 3.5 & 6.4 \\
%   \end{tabular}
% \end{table}

% \iffalse
% \section{Numerical Example}
% We consider an example which involves finding the optimal maintenance and inspection policy for a group of 5 buildings, where each building has it's own deterioration dynamics. We apply our algorithm to the problem to compute the optimal budget to be allocated to each building for it's inspection and repair. This is done by first computing the maximum survival time for a particular horizon for each building for different values budget allocated to it and then approximating this maximum survival time as a function of budget from the data. Then we perform the maximization as discussed in Section \ref{sec:solution} to find the optimal allocation of the total budget between the buildings. The optimal policy thus obtained is then compared with a realistic baseline policy. Finally, we show the scalability and efficiency of our algorithm by performing the same simulations for a larger number of buildings.
% \subsection{5 Building Problem}
% We perform simulations in Julia using the POMPDs.jl package \cite{egorov2017pomdps}
% Let's consider a group of 5 buildings, each with their own deterioration dynamics. We have 5 agents, one for each building and our goal is to compute the optimal policy for each agent. The state vector for each building is $(h,c)$ where $h$ denotes the health of the building and it takes integer values from 0 to 100. $c$ denotes the cost incurred by the agent upto a certain time instant. Each agent has 3 possible actions and the action space for each agent is given by $\actions = \{\text{maintain, inspect, do-nothing}\}$. The costs for the maintain, inspect and do-nothing actions are denoted by $c_m, c_i$ and $c_d$ respectively. To make our simulations as realistic as possible we use the weibull distribution to model the transition probability function for each building:
% \begin{equation*}
%     T(h^{\prime}|h,a) = \begin{cases}
%     1,& \text{if } h^{\prime} = 100 \text{ and } a = \text{maintain} \\
%     f(h,\lambda,k),& \text{if } h^{\prime} \leq h \text{ and } a \neq \text{maintain}\\
%     0,& \text{otherwise}
%     \end{cases}
% \end{equation*}
% where the probability density function $f$ is given by:
% \begin{equation*}
%     f(h,\lambda,k)= \frac{k}{\lambda}\left(\frac{h}{\lambda}\right)^{k-1}e^{-(h/\lambda)^{k}}
% \end{equation*} with shape factor $k > 0$ and scale factor $\lambda > 0$. Each building has different values for both the scale factor as well as the shape factor. The observation function, reward function and transition function for the cost component are the same as described in previous sections. 

% The total budget for 5 buildings is taken to be 2000. The reward model is the same for all 5 buildings and is given by:
% \begin{equation*}
%     R(h,c,a) = \begin{cases}
%     3,& \text{if } h > 0 \text{ and } c \leq 2000 \\
%     -20,& \text{if } h = 0 \text{ and } c \leq 2000\\
%     -100,& \text{if } c > 2000
%     \end{cases}
% \end{equation*}
% The other simulation parameters used for the buildings are as listed in Table \ref{tab:param} below. Also, for each POMDP, the time horizon is taken to be 100 and the discount factor is 0.95.


% \begin{table}[!ht]
%     \centering
%     \begin{tabular}{cccccc}
%         \toprule
%         Parameter & Build 1 & Build 2 & Build 3 & Build 4 & Build 5 \\
%         \midrule
%         $\lambda$ & 1 & 1 & 1 & 1 & 1 \\
%         $k$ & 1 & 1.49 & 5 & 10 & 50 \\
%         $c_m$ & 10 & 10 & 10 & 10 & 10 \\
%         $c_i$ & 2 & 2 & 2 & 2 & 2 \\
%         $c_d$ & 0 & 0 & 0 & 0 & 0 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Simulation parameters for different buildings}
%     \label{tab:param}
% \end{table}

% % \begin{table}[h]
% % \caption{Example table with 6 columns}
% % \label{tab:example}
% % \centering
% % \begin{tabular}{|c|c|c|c|c|c|} 
% % \hline
% %  Column 1 & Column 2 & Column 3 & Column 4 & Column 5 & Column 6 \\ 
% % \hline
% %  1 & 2 & 3 & 4 & 5 & 6 \\ 
% % \hline
% %  7 & 8 & 9 & 10 & 11 & 12 \\ 
% % \hline
% % \end{tabular}
% % \end{table}
% \subsection{Solving the 5 POMDPs and Plots of survival probability}
% From the total budget $B = 2000$, we assume that component $i$ is allocated a budget $b_i$ and hence we have:
% \begin{equation*}
%     \sum_{i = 1}^{5}b_i = 2000
% \end{equation*}
% For each component, we define a quantity called survival probability which is simply the number of time steps for which the component $i$ is in a feasible state divided by the total time horizon which is 100. Let survival probability of component $i$ for a particular value of budget be denoted by $p_i$. Then we have:
% \begin{equation*}
%     p_i = \frac{\text{Number of times agent gets a reward of 3}}{100}
% \end{equation*}
% We now apply the POMCP algorithm to solve the POMDP for each component $i$ for and record the optimal values of $p_i$ achieved for budget values $b_i \in [0,500]$ where $b_i$ is an integer. Next, we plot $p_i$ as a function of $b_i$ for each component $i$ as shown below in FIGURE. All simulations are performed using the POMCP solver and QuickPOMDP interface of the POMDPS.jl framework \cite{egorov2017pomdps}.


% \subsection{Function Approximation (Symbolic Regression + Plots for exponential approximation)}
% The data generated from the plot for survival probability vs the budget for each building is used to estimate the underlying function. To do this, we use the gplearn library in Python to perform symbolic regression and find the best fit function. Symbolic regression is a machine learning technique that aims to find the mathematical expression which is the best fit for a given dataset, both in terms of accuracy and simplicity. No particular model is provided as a starting point for symbolic regression. Instead, initial expressions are formed by randomly combining mathematical building blocks such as mathematical operators, analytic functions, constants, and state variables. Upon doing this regression analysis, we find that the best fit for our survival probability data is a function of the form $\frac{a}{x^b + c}$ and the second best fit is an exponential function of the form $ae^{-bx} + c$ where the variable $x$ represents the budget allocated. Clearly the exponential function is concave in $x$ and would thus lead to a well-defined solution for the budget maximization problem given by equation \ref{maximize}. 

% Hence we approximate the survival probability as an exponential function of the allocated budget and estimate the parameters $a,b,c$ for each component as shown in the FIGURE below.

% \subsection{Maximisation w.r.t. budget and plots for final budget distribution}
% The maximization problem presented in equation \ref{maximize} is the same as minimizing the negative of the objective function with respect to the same constraints. Mathematically,
% \begin{equation}
% \begin{aligned}
% &\max_{b_i} \sum_{i = 1}^{n}-f_i(b_i)\\
% &\text{s.t.} \sum_{i = 1}^{n}b_i = B, \label{minimize}
% \end{aligned}
% \end{equation}
% Since the function $f_i$ is exponential in $b_i$, the above problem becomes a convex optimization problem with linear constraints and hence has a well defined minimum which in-turn gives a well defined maximum for equation \ref{maximize}.

% We use the CVXPY optimization library in Python to solve the convex minimization problem given by \ref{minimize} to find the optimal values of $b_1,b_2,b_3,b_4 \text{ and }b_5$ and hence obtain the optimal split of the total budget among the components. The histogram and the pie chart in the FIGURE below represent the individual values of budget allocated and the fraction of the total budget allocated respectively. 

% \subsection{Comparison with a baseline policy}
% In this section, the optimal policy and budget split generated by our algorithm is compared to a baseline policy which is used in maintenance and inspection of components in real life. According to the baseline policy, a maintain action is taken only when the component is close to the failure state i.e. 0. The plot in the FIGURE below shows the survival probability as a function of the budget when the agent performs actions according to this baseline policy. Table \ref{tab:compare} below consists of values of survival probability achieved by our algorithm and the baseline policy for some budget values. Clearly, the survival probability values achieved are higher, for same budget values, for our algorithm as compared to the baseline policy. 

% \begin{table}[!ht]
%     \centering
%     \begin{tabular}{ccc}
%         \toprule
%         Budget Allocated & Proposed Algorithm & Baseline Policy \\
%         \midrule
%         $\lambda$ & 1 & 1 \\
%         $k$ & 1 & 1.49 \\
%         $c_m$ & 10 & 10 \\
%         $c_i$ & 2 & 2 \\
%         $c_d$ & 0 & 0 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Comparison of survival probability values}
%     \label{tab:compare}
% \end{table}

% \subsection{Same algorithm for more buildings}
% In this section, we show the scalability of our proposed algorithm by solving the same optimization problem for MORE number of components. The FIGURES below show the plots for survival probability, function approximation and the optimal budget split. We also compare our algorithm to the baseline discussed in the previous section and clearly observe that our algorithm outperforms the baseline. 
% \fi

\section{Implementation and Evaluation}
In this section we illustrate the utility of the proposed approach for multi-component decision making with budgetary-constraints. In particular, we compare the policy described above with existing approaches on a scenario of multi-component building management. Our implementation utilizes the POMDP.jl \cite{egorov2017pomdps} Julia package for efficiently solving the budgeted-POMDPs using POMCP, as well as CVXPY \cite{diamond2016cvxpy} for solving the convex optimization formulation of the budget allocation problem. The initial budget-split for solving the budget allocation problem is chosen randomly while satisfying the constraint of \eqref{eq1}.

We model the components that comprise a typical administration building with a total size of 10,000 sq. ft. The building comprises multiple components such as lighting systems, roofing components, boilers, and carpeting, where each component's cost of replacement and inspection are based on empirically derived industry averages. Each component's health is defined by the Condition Index (CI) \cite{grussing2006condition}, which takes values between 0 and 100. The condition deteriorates stochastically over time, depending on various factors, and can only be observed through explicit inspections, which incur a cost. The component fails when the CI reaches below a \textit{failure threshold}, which we assume to be 0. Components can also be replaced, restoring their CI to its full value.

The building is associated with an average maintenance budget of \$2,200,000 for a given period of interest. Using historic CI data for each component, we synthesize the transition probabilities of their corresponding Partially Observable Markov Decision Processes (POMDPs). We consider 20 sustainable components from the building, with replacement costs ranging from 0.15\% to 3\% of the total budget and inspection costs ranging from 0.01\% to 0.03\% of the total budget. We scale the total budget to 10,000 units and appropriately scale the replacement and inspection costs of all components while ensuring that they are rounded to the nearest integers. The decision-maker's objective is to maximize the  time until failure of the components by effectively allocating the budget among the components and taking replacement and inspections when needed. As in Section IV.C, we model this objective as a POMDP by assigning a reward of 1 when the CI is greater than the failure threshold and 0 otherwise, and modeling the state of 0 health and budget exhaustion as absorbing states. For our experiments, we consider simulations with a horizon of up to 100 decision steps with a 1 year step size. 

\subsection{Maintenance Policy Synthesis}
In this section we compare the maintenance and inspection policies obtained from the proposed POMDP-based model with a realistic baseline approach. In the baseline approach, a building manager typically schedules component inspection at a regular interval and the true health of the component is only obtained at these regular intervals. In the absence of an inspection, the CI of a component at a given time step is estimated to be the most probable CI state as determined by its CI transition dynamics. The baseline policy used in this section replaces the component if its estimated CI is less than a pre-determined threshold. 

We use \textit{time-to-failure} (TTF), defined as the number of simulation steps until failure, as the performance metric. We run experiments to calculate the TTF for each component by averaging the values obtained over 5 independent simulations with 100 maximum possible simulation steps. We set the maximum tree depth for POMCP rollouts to 50 and use a UCB exploration constant of 10. In the baseline policy, we inspect the CI every 5 steps and replace the component if the estimated CI is below 15. 
Figure \ref{fig:ttf-budget} shows the simulation results comparing the TTF obtained for different budget values using the baseline and the proposed approach. The proposed approach provides a clear advantage over baseline strategy over the entire range of budget values for all 20 components, irrespective of the replacement costs. 
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{ttfr_vs_budget_mini.png}
  \caption{Comparison of the \textcolor{blue}{proposed} and \textcolor{red}{baseline} approaches using time-to-failure for a range of budget values. (a) Overall results obtained by averaging over all components. (b) Results for the Air Handling Unit component with a replacement cost of 250 units. (c) Results for the Lighting Equipment component with a replacement cost of 24 units.}
  \label{fig:ttf-budget}
  \vspace{-0.2cm}
\end{figure}

Figure \ref{fig:component_history} shows sample CI histories for the same component obtained from simulations using the proposed approach and the baseline policy. The proposed approach takes inspection and replacement actions only when deemed necessary based on the latest belief estimate and the potential loss of value due to an inaccurate estimate or due to not taking a replacement action. Such a behavior holds true for every component without any component specific parameter tuning. On the other hand, the estimated state from the baseline policy based on the most-probable transition may not always be the same as the real transition, ultimately resulting in early failures. Although it is possible to enhance the baseline by incorporating component-specific parameters and budget-aware heuristics, our experiments indicate that its performance still lags behind the proposed approach, particularly when the budget is tightly constrained.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{component_history.png}
  \caption{Sample condition index (CI) histories illustrating the performance of the proposed policy when compared to the baseline for the Boiler component with a replacement cost of 45 units, an inspection cost of 1 unit, and a total budget of 500 units. (a) CI history using proposed approach showing failure at 80 time steps. (b) Baseline approach failing at 39 time steps.}
  \label{fig:component_history}
\end{figure}
\vspace{-0.4cm}
\subsection{Budget Allocation}
We demonstrate the effectiveness of our proposed budget allocation approach by comparing it to a baseline that depends on two component properties: (i) mean-time-to-failure (MTTF) which is the expected number of steps a component takes for its condition index to go below the failure threshold when starting from maximum possible condition index, and (ii) the replacement cost of the component. The baseline allocation is proportional to ratio of the component's replacement cost and MTTF.

We quantify the performance of the budget allocation algorithms by running 20 independent simulations over all the components using the allocated budgets and calculating the overall TTF for the building. To ensure fairness, we compare both budget allocation algorithms by running simulations using policies obtained by the same decision making strategy: the POMCP-based approach. 
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{allocation_mini.png}
  \vspace{-0.5cm}
  \caption{Comparison of baseline and proposed budget allocation approaches for the all 20 components for an overall budget of 10,000 units.}
  \label{fig:allocation}
  \vspace{-0.2cm}
\end{figure}
Figure \ref{fig:allocation} summarizes the TTF results from the baseline and the proposed approaches for all components in the building. The proposed allocation approach achieves an overall TTF of 1510, outperforming the baseline that achieves an overall TTF of 1355. Hence the proposed approach maximizes the overall TTF in accordance with the objective defined by \eqref{maximize}. Analyzing individual component data we observe that instances where the proposed approach underperforms the baseline exhibit only slight differences in TTF values. In contrast, when the proposed approach outperforms the baseline, we observe a significant improvement. Note that the maximal TTF of 100 is achieved by both strategies for 50\% of components, the proposed strategy performs better for 35\% of the components, and the baseline performs better only for 15\% of the components.

\begin{table}[!h]
    \centering
    \vspace{7pt}
    \begin{tabular}{c c}
        \toprule
        Number of Components & Time (mean $\pm$ std. dev. of 7 runs)\\
        \midrule
        5 & $333 ms \pm 21.4 ms$\\
        10 & $412 ms \pm 37.1 ms$ \\
       20 & $552 ms \pm 31.6 ms$ \\
        \bottomrule
    \end{tabular}
    \vspace{0.5cm}
    \caption{Comparison of time taken to find optimal budget split among 5, 10 and 20 components respectively, for a total budget of 10000 units.}
    \vspace{-0.8cm}
    \label{tab:time}
\end{table}
The results in Table \ref{tab:time} present the time taken to solve the optimization problem given by \eqref{maximize}. As can be clearly observed, the solution time increases with an increase in the number of components. However, the values are always of the order of milliseconds.

\section{Conclusions and Future Work}
In this paper, we formulated the problem of optimal policy synthesis for a multi-component POMDP with a budget, in the sense of maximizing the time before reaching an absorbing state. We first introduced a b-POMDP model to facilitate optimal planning in POMDPs while adhering to budget constraints. Next, we showed that the value function or maximal collected reward for b-POMDPs, under significant assumptions of full observability and deterministic transitions, is a concave function of the budget. We then presented an algorithm to find the optimal budget split among the component POMDPs of an $n$-component POMDP with a given total budget $B$. The budget-splitting problem was posed as a welfare maximization problem. The concavity of the maximal collected reward, with respect to the budget, makes the problem a convex optimization problem. The experimental evaluations of the proposed algorithm, in an infrastructure component management scenario, verify its effectiveness in terms of performance. Performing simulations on real data, we observe that our algorithm vastly outperforms the policies currently used in practice. 

There are two possible directions of future work. While the proposed approach makes it possible to compute an approximately optimal policy in a feasible amount of time, the computational cost of using the POMCP algorithm is still high. Our first direction of work is to reduce this cost by incorporating a learning framework so as eliminate repeated runs of POMCP. Finally, the budget allocation scheme is fixed in the sense that the budget-split is done before the start of the planning horizon. The second direction of work is to consider other efficient budget allocation methods. A sequential algorithm for optimal computing budget allocation is presented in \cite{futurework1}. Applying this algorithm to our problem may result in more accurate budget allocation. Similarly, another method which can be explored is the allocating method presented in \cite{futurework2}. Also, following these methods may allow us to generalize our algorithm for cases where the value function does not satisfy the concavity property. Furthermore, another direction is to derive an optimal dynamic budget allocation scheme to account for change in transition probabilities of the component states during the planning horizon and also account for a cyclical budget.

\section*{Acknowledgments}
This work was supported through a cooperative agreement between the University of Illinois Urbana-Champaign and the the United States Army Engineer Research and Development Center. We acknowledge the kind help of Trevor Betz, Louis Bartels, Ryan Smith, and Zachary Sunberg in the discussions leading to this paper.

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}
