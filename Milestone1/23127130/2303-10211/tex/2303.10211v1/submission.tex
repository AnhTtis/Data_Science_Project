\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{arydshln}
\usepackage{xr}
\usepackage{cases}
\usepackage{amsthm}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\title{ASymReg: Robust symmetric image registration using anti-symmetric formulation and deformation inversion layers}


\author{
 Joel Honkamaa \\
  Department of Computer Science\\
  Aalto University
  %% examples of more authors
   \And
 Pekka Marttinen \\
  Department of Computer Science\\
  Aalto University 
}

\begin{document}
\maketitle
\begin{abstract}
Deep learning based deformable medical image registration methods have emerged as a strong alternative for classical iterative registration methods. However, the currently published deep learning methods do not fulfill as strict symmetry properties with respect to the inputs as some classical registration methods, for which the registration outcome is the same regardless of the order of the inputs. While some deep learning methods label themselves as symmetric, they are either symmetric only a priori, which does not guarantee symmetry for any given input pair, or they do not generate accurate explicit inverses. In this work, we propose a novel registration architecture which by construction makes the registration network anti-symmetric with respect to its inputs. We demonstrate on two datasets that the proposed method achieves state-of-the-art results in terms of registration accuracy and that the generated deformations have accurate explicit inverses.
\end{abstract}

\section{Introduction}

In this work we study deformable medical image registration using deep learning. In medical image registration one typically seeks to find a mapping anatomically connecting coordinate systems of two images. Typically deep learning is applied to image registration by teaching a registration network which given two images as an input directly outputs a candidate for the coordinate mapping. The predicted coordinate mappings are often called deformations. In this work we limit ourselves to unsupervised intra-modality registration, that is, we have no ground truth deformations and the images to be registered are of a same modality. Intra-modality registration is required e.g., when brain MRI images from different patients are deformed to an atlas, or when analyzing breathing cycle of a patient using multiple images at different times.

One often aims for the predicted deformations to be invertible and that the inverse deformation can be found easily in practice. Most commonly in deep learning applications this is achieved using the stationary velocity field (SVF) formulation \cite{arsigny2006log}, e.g \citet{krebs2018unsupervised, krebs2019learning, niethammer2019metric, shen2019networks, shen2019region, mok2020fast}. Invertibility itself can also encouraged using specific loss functions for that purpose, e.g by penalizing negative determinants \cite{mok2020fast}. Often used term related to inveritibility is diffeomorphic which refers to a method which should generate diffeomorphisms, continuous invertible deformations.

Symmetricity as a characteristic of a registration algorithm was used by \citet{avants2008symmetric} in the context of Large Deformation Diffeomorphic Metric Mapping (LDDMM) to refer to algorithms for which registration outcome does not depend on the order of the inputs. In the LDDMM context both the forward and the inverse deformation can be generated from a flow and the outcome not depending on the input order refers to the generated flow being identical up to it's direction changing. Hence symmetric methods are actually anti-symmetric: changing the order of the inputs inverts the output. However, the term symmetricity is not used consistently across the registration literature, e.g. while few recent deep learning methods label themselves as symmetric \cite{mok2020fast, estienne2021mics}, they are not symmetric in a similarly strict sense.

A related concept is inverse-consistency: A registration method is said to be inverse consistent if changing the order of the inputs results in inverting the predicted deformation \citep{thirion1998image, christensen2001consistent}. The definition is in principle identical to the definition of anti-symmetricity given earlier but the term inverse-consistency is usually used in a context where the forward and inverse deformations are generated separately and are not guaranteed by construct to be inverses of each other. Recent deep learning methods try to achieve this type of anti-symmetricity by e.g, directly estimating the inverses of the predicted deformations \cite{zhang2018inverse}, or by using cycle-consistency constraints \cite{kim2019unsupervised, mahapatra2019training, gu2020pair}. However, inverse consistency as a property is inferior to symmetricity as used by \citet{avants2008symmetric} since symmetric methods are inverse-consistent by design.

In this work we develop a deep learning registration method which is symmetric in a very strict sense: Firstly, it generates deformations in both directions guaranteed by construct to be accurate inverses of each other, and secondly, it is invariant to the the input order up to only swapping the predicted forward and inverse deformations. In addition to the nice theoretical properties, we show that the method produces state-of-the-art results. The method is based on a novel anti-symmetric multi-resolution formulation and to implement it in practice we develop a memory efficient deformation inversion layer based on relatively well-known fixed point iteration formula \cite{chen2008simple} and recent advances in Deep Equilibirum models \cite{bai2019deep, duvenaud2020deep}.  We name the algorithm \textit{ASymReg} after it's anti-symmetrical properties.

\begin{figure*}
\centering
\includegraphics[width=1.0\textwidth]{figures/deformation_example.pdf}
\vskip -0.05in
\caption{\textbf{Example deformation produced by the method.} Composition of the forward and the inverse deformations is shown on the right demonstrating the accuracy of the inverse. Only one 2D slice is shown of the 3D deformation. The visualized deformation is from the LPBA40 experiment E3.}
\label{fig:example_deformation}
\vskip -0.1in
\end{figure*}

\section{Background}

\subsection{Diffeomorphic registration}

Among classical registration methods Large Deformation Diffeomorphic Metric Mapping (LDDMM) \cite{cao2005large} framework has proven to be very powerful method for generating diffeomorphic deformations but has not seen much use in unsupervised deep learning context due to the computational cost. Instead more simple stationary velocity field (SVF) formulation \cite{arsigny2006log} has been very popular \cite{krebs2018unsupervised, krebs2019learning, niethammer2019metric, shen2019networks, shen2019region, mok2020fast}. In SVF formulation the final deformation is obtained by integrating stationary velocity field over itself over unit time. Under relatively mild continuity constraints for the velocity field this should result in diffeomorphism.

Relatively popular classical method for generating invertible deformations has been one by \citet{choi2000injectivity, rueckert2006diffeomorphic}. The basic idea is that one finds rather limiting constraint under which a deformation is diffeomorphic but small and the final deformation is the result of composition of such small deformations. Since diffeomorphisms form a group under composition, also the final deformation is diffeomorphic.

Note that the practical implementation of SVF is close to the method by \citet{rueckert2006diffeomorphic} as the velocity field is almost always integrated using scaling and squaring. In scaling and squaring the velocity field is first scaled down by a power of two and then interpreting the scaled down version as a small deformation, repeatedly composed with itself to provide the final deformation. The basic idea is hence the same: composition of small deformations.

\begin{figure*}[t]
\centering
\includegraphics[width=0.8\textwidth]{figures/architecture_overview.pdf}
\caption{\textbf{Overview of the proposed architecture.} Multi-resolution features are first extracted from the inputs $x_1$ and $x_2$. Output deformations $\phi$ and $\phi^{-1}$ are built recursively from the multi-resolution features using the anti-symmetric deformation updates described in Equation \ref{eq:anti-symmetric_deformation_update} and visualized in Figure \ref{fig:anti-symmetric_update}. The architecture is anti-symmetric with respect to the inputs and the final deformation is obtained in both directions. The brain images are from the OASIS dataset \cite{marcus2007open}}
\vskip -0.1in
\end{figure*}

\subsection{Symmetric registration}

The term "symmetric registration method" as used in the paper on the classical registration method Symmetric Normalization (SyN) \cite{avants2008symmetric} includes basically two properties. Firstly, the registration result should not vary (up to the flow direction) if the input order is changed, and secondly, one should be able to obtain accurate inverse deformations as guaranteed by Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework \cite{cao2005large} in their context.

In our literary survey we found two deep learning registration methods claiming to be symmetric, SYMNet by \citet{mok2020fast} and MICS by \citet{estienne2021mics}. The approaches are indeed more symmetric than many registration methods which treat one of the images as moving and the other one as fixed. However, neither of the methods fulfill similarly strict symmetricity properties as SyN.

SYMNet treats both input orders equally a priori and produces analytically invertible deformations using the stationary velocity field framework \cite{arsigny2006log}. However, the final trained network might predict a different deformation after reversing the input order. In SYMNet model two stationary velocity fields are produced which are integrated in opposite directions until up to time $0.5$ and the final deformation is then produced by composition.

MICS on the other hand fulfills the other symmetricity requirement as the registration results are equal (up to the directions being swapped) if one reverts the order of the inputs. However, their method does not provide a way to compute accurate inverses and the predicted forward and inverse transformations are only encouraged to be actual inverses of each other using an inverse consistency loss.

\subsection{Deep Equilibrium networks}

Deep Equilibrium networks have emerged as a prominent alternative to traditional explicit layers in deep learning \cite{bai2019deep, bai2020multiscale, duvenaud2020deep}. Output of a Deep Equilibrium layer is defined by a solution to a fixed point equation with the basic intuition being that instead of having multiple sequential layers, one can apply the same layer repeatedly until convergence. The main theoretical innovation is that one can use any solver for solving the fixed point equation since the gradient can be back-propagated based on implicit differentiation given the fixed point solution. The framework is also very memory efficient as only the fixed point solution needs to be stored for the backward pass.

The basic idea is to define a layer using a fixed point mapping, say $g$. In the most simple case the fixed point mapping will take two arguments, one of which is the input to the layer. The task is then to find a solution such that
$$
z = g(x, z)
$$
holds where $x$ is the input and $z$ is the output of the layer. By an implicit function theorem one can assume an existence of a solution mapping in the neighborhood of the solution $z$ which given an input $x$ outputs another solution. Let us denote this mapping by $z^*$. By implicit differentiation one can then obtain the following formula for the Jacobian of $z^*$

$$
\frac{\partial z^*}{\partial x} = \left( I - \frac{\partial f(z^*, x)}{\partial z^*} \right)^{-1}\frac{\partial f(z^*, x)}{\partial x}.
$$

The vector-Jacobian product of $z^*$ needed for neural network back-propagation can be calculated from this using another fixed point equation without need for fully computing the Jacobians. For details see, e.g., the tutorial by \citet{duvenaud2020deep}.

\section{Methods}

\begin{figure*}
\label{fig:anti-symmetric_update}
\centering
\includegraphics[width=0.5\columnwidth]{figures/anti-symmetric_update.pdf}
\caption{\textbf{Anti-symmetric deformation update.} Figure visualizes the anti-symmetric deformation update described in Equation \ref{eq:anti-symmetric_deformation_update}. The compositions are done by linear interpolation. Updates of the inverse mappings $(\phi^{(k)}_{1\to2})^{-1}$ and $(\phi^{(k)}_{2\to1})^{-1}$ are not shown and their results are only required for computing the full deformations at the end.}
\end{figure*}

In deformable image registration given a non-aligned image pair $x_1, x_2: \mathbb{R}^n\to \mathbb{R}^k$ one seeks to find a mapping $\phi: \mathbb{R}^n\to\mathbb{R}^n$ connecting the image coordinate systems, often called a deformation. Here $n$ is the dimensionality of the image, e.g $n = 3$ for three dimensional medical images, and $k$ is the number of channels in the image, e.g. $k = 3$ for a RGB-image. In pratice we only have samples of the images and to get a continuous representation we have to interpolate. In this work we use linear interpolation. The deformation $\phi$ should deform the image $x_1$ such that in some sense $x_1 \circ \phi \approx x_2$. In medical context $\approx$ usually refers to anatomical correspondence. In this work we limit ourselves to intra-modality registration, that is, the images $x_1$ and $x_2$ are of the same type, e.g. both are MRI images taken with a similar sequence.

Similarly to many works applying neural networks to image registration, we want to learn a neural network $f$ that can take two images and directly output the deformation connecting the image coordinates.

Further, given a deformation $\phi$ we denote the corresponding displacement field as $d(\phi) := \phi - \mathrm{I}$ where $\mathrm{I}$ is the identity mapping. Displacement field describes the relative movement of each image location when it is deformed.

\subsection{Anti-symmetric formulation}

Assuming that the generated deformations are invertible, a neural network $f$ doing intra-modality registration should ideally have the following properties for any input images $x_1, x_2$:

\begin{numcases}{}
    f(x_1, x_2) = f(x_2, x_1)^{-1}\label{eq:anti-symmetricity}\\
    f(x_i, x_i) = \mathrm{I}\label{eq:identity_for_identic_inputs}
\end{numcases}
where $\mathrm{I}$ is the identity mapping. The first property is often called anti-symmetricity.

Our core idea is to define the architecture of the neural network $f$ such that both of the properties are fulfilled by construct. In short, given some neural network $\tilde{f}$ which outputs invertible deformations, one can define $f$ as
\begin{equation}\label{eq:anti-symmetric_formulation}
f(x_1, x_2) := \tilde{f}(x_1, x_2) \circ \tilde{f}(x_2, x_1)^{-1}
\end{equation}
and as a result the properties \eqref{eq:anti-symmetricity} and \eqref{eq:identity_for_identic_inputs} hold exactly. We call this \textit{anti-symmetric formulation}.

Applying the formulation naively would double the computational cost. Hence we propose to encode features from both the inputs separately before feeding them to the deformation extraction network following Equation \ref{eq:anti-symmetric_formulation}. Separately extracting features has been successfully used in recent image registration methods \cite{estienne2021mics, young2022superwarp}

\subsection{Deformation inversion layer}

\begin{table*}
\centering
\setlength\tabcolsep{3pt}
\small
\caption{\textbf{Mean results for the OASIS experiments E1 and E2.} The values are computed on the test set with the standard deviation shown in the parenthesis. VoxelMorph and cLapIRN do not predict inverse deformations and hence the inverse-consistency error can not be computed for them. Determinant standard deviation and inverse-consistency-error are omitted for the ASymReg model trained with the non-affinely-aligned data since they are not meaningfully comparable to the other results. The omitted values were 0.18 (0.019) and 1.2 (2.8) respectively.}
\vskip 0.15in
\begin{tabular}{lccccc}
\hline
 Model                               & Dice $\uparrow$ & HD95 $\downarrow$ & $|J_{\phi}|_{\leq 0} \downarrow$ & $\operatorname{std}(|J_{\phi}|) \downarrow$ & $||\phi \circ \phi^{-1}||^2 \downarrow$ \\
 \hline SYMNet                       &  0.788 (0.029)  &    2.15 (0.54)    &            \textbf{0.3} (0.7)             &                \textbf{0.42} (0.035)                 &             0.0067 (0.0039)             \\
 SYMNet without anti-folding         &  0.787 (0.029)  &    2.17 (0.56)    &           86 (33)            &                0.44 (0.040)                 &             0.0092 (0.0046)             \\
 VoxelMorph                          &  0.803 (0.031)  &    2.08 (0.57)    &         3360 (2287)          &                0.46 (0.030)                 &                    -                    \\
 cLapIRN                             &  0.812 (0.027)  &    1.93 (0.50)    &         37505 (7662)         &                0.52 (0.030)                 &                    -                    \\
 \hline ASymReg                      &  \textbf{0.817} (0.025)  &    1.85 (0.43)    &            10 (4.2)            &                \textbf{0.42} (0.031)                 &            \textbf{0.0058} (0.00079)             \\
 \hdashline ASymReg (non-affinely-aligned data) &  0.810 (0.024)  &    \textbf{1.81} (0.49)    &            1.5 (1.4)             &                -                 &                -                \\
\hline
\end{tabular}
\label{table:results_oasis}
\vskip -0.1in
\end{table*}

To use Equation \ref{eq:anti-symmetric_formulation} one has to be able to invert the deformation $\tilde{f}(x_2, x_1)$. For that, one could, e.g., use the SVF framework but we propose a different method.

As shown by \citet{chen2008simple}, given a displacement field $d(\phi)$ of some deformation $\phi$, the corresponding displacement field of the inverse deformation $d(\phi^{-1})$ can in many cases be solved by an iteration with the following fixed point equation:
$$
d(\phi^{-1})(x) = -d(\phi)(x + d(\phi^{-1})(x)).
$$
We propose to use this fixed-point iteration as part of a neural network architecture and call the resulting component \textit{deformation inversion layer}.

Following the recent works on Deep Equilibirium Models we use Anderson Acceleration \cite{walker2011anderson} for solving the fixed point equation and use the memory-effecient back-propagation \cite{bai2019deep, duvenaud2020deep} straregy which significantly reduces memory usage. With large volumetric data the memory saving can be significant. In our use case the inverse can be solved usually with $3$ to $6$ iterations such that the maximum error is less than $10^{-2}$ voxels for the whole volume.

\citet{chen2008simple} prove that the fixed point algorithm is guaranteed to converge under Lipschitz condition. However, during experimentation we found that in practice especially when using Anderson acceleration the iteration converges for a wider set of deformations. However, the algorithm still does not converge for all relevant deformations which is part of the reason for the multi-resolution approach presented next.

\subsection{Multi-resolution architecture}\label{sec:multi-resolution}

As the final architecture we propose a novel multi-resolution approach for which the anti-symmetricity conditions \ref{eq:anti-symmetricity} and \ref{eq:identity_for_identic_inputs} hold exactly. Additionally the multi-resolution approach allows us to limit the displacements for individual deformations by a hard constraint making the deformations more easily invertible by the deformation inversion layers. The final invertible and potentially large deformation is then composed of the small invertible deformations, similarly to \citet{rueckert2006diffeomorphic}.

In practice we start by extracting separately multi-resolution features from the input images. The encoder is shared for both of the inputs and can be any multi-resolution feature extractor and we use a ResNet \cite{he2016deep} style convolutional network. The step can be executed efficiently by concatenating the two images over the batch dimension before feeding them to the network.

After extracting the multi-resolution features, the deformation is incrementally updated at each resolution using the extracted features. Instead of using the original features for extracting the deformation at each stage we deform the features by the deformation learned so far. For the properties \eqref{eq:anti-symmetricity} and \eqref{eq:identity_for_identic_inputs} to hold, we deform the features to intermediate coordinates between the two images similarly to e.g. SyN \cite{avants2008symmetric} or SYMNet \cite{mok2020fast}.

Let us now denote the multi-resolution features as $z^{(k)}_1$ and $z^{(k)}_2$ where $k \in \{0, \dots, K - 1\}$ denotes the resolution level starting from the highest resolution. Going backwards, at each stage the resolution is doubled and for each resolution we have a separate deformation extraction network $\tilde{f}^{(k)}$ which first extracts a displacement field in the same resolution as the features but then upsamples it to the full image resolution. We use prefiltered cubic spline interpolation \cite{ruijters2012gpu} for the upsampling. Spline interpolation can be implemented effeciently using transposed convolutions \citep{de2019deep}. The deformation is upsampled before inverting it for the inverses to be accurate in the final resolution.

\begin{table*}
\centering
\setlength\tabcolsep{3pt}
\small
\caption{\textbf{Mean performance metrics for the OASIS experiments E1 and E2.} Standard deviation is shown in the parenthesis. Inference time and inference memory usage were measured on NVIDIA GeForce RTX 3090. The images in the non-affinely-aligned dataset have $3.8$ times more voxels resulting in significantly larger inference time and memory usage for the ASymReg model trained with it.}
\vskip 0.15in
\begin{tabular}{lccc}
\hline
 Model                               & Inference Time (s) $\downarrow$ & Inference Memory (GB) $\downarrow$ & \# parameters (M) $\downarrow$ \\
 \hline SYMNet                       &          \textbf{0.10} (0.0010)          &                \textbf{1.6}                 &              \textbf{0.9}               \\
 SYMNet without anti-folding         &          \textbf{0.10} (0.0011)          &                \textbf{1.6}                 &              \textbf{0.9}               \\
 VoxelMorph                          &         0.17 (0.00078)          &                5.4                 &              1.3               \\
 cLapIRN                             &         0.11 (0.00085)          &                4.1                 &              1.2               \\
 \hline ASymReg                      &          0.47 (0.032)           &                2.9                 &              1.2               \\
 \hdashline ASymReg (non-affinely-aligned data) &           2.0 (0.022)           &                11.1                &              2.5               \\
\hline
\end{tabular}
\label{table:performance_results}
\vskip -0.1in
\end{table*}

To be more precise, we propose to recursively go backward updating four deformations at each resolution: $\phi^{(k)}_{1\to2}$ deforming $x_1$ half way towards $x_2$, $\phi^{(k)}_{2\to1}$ deforming $x_2$ half way towards $x_1$, and also their inverses. Initially at level $K$ these are identity mappings. Denoting the upsampled forward and reverse neural network predictions as
$
\psi_F^{(k)} := \tilde{f}^{(k)}(
    z^{(k)}_1 \circ \phi_{1\to2}^{(k + 1)},
    z^{(k)}_2 \circ \phi_{2\to1}^{(k + 1)}
)$ and
$
\psi_R^{(k)} := \tilde{f}^{(k)}(
    z^{(k)}_2 \circ \phi_{2\to1}^{(k + 1)},
    z^{(k)}_1 \circ \phi_{1\to2}^{(k + 1)}
)
$ we then perform four updates:
\begin{equation}\label{eq:anti-symmetric_deformation_update}
    \begin{cases}
        \phi^{(k)}_{1\to2} =
            \phi^{(k + 1)}_{1\to2}
            \circ \psi_F^{(k)}
            \circ (\psi_R^{(k)})^{-1}\\
        \phi^{(k)}_{2\to1} =
            \phi^{(k + 1)}_{2\to1}
            \circ \psi_R^{(k)}
            \circ (\psi_F^{(k)})^{-1}\\
        (\phi^{(k)}_{1\to2})^{-1} =
            \psi_R^{(k)}
            \circ (\psi_F^{(k)})^{-1}
            \circ (\phi^{(k + 1)}_{1\to2})^{-1}\\
        (\phi^{(k)}_{2\to1})^{-1} =
            \psi_F^{(k)}
            \circ (\psi_R^{(k)})^{-1}
            \circ (\phi^{(k + 1)}_{2\to1})^{-1}
    \end{cases}
\end{equation}
The update is visualized in Figure \ref{fig:anti-symmetric_update}.

The full deformations between the images can be obtained as:
\begin{equation}
    \begin{cases}
        \phi^{(k)} =
            \phi^{(k)}_{1\to2}
            \circ (\phi^{(k)}_{2\to1})^{-1}\\
        (\phi^{(k)})^{-1} =
            \phi^{(k)}_{2\to1}
            \circ (\phi^{(k)}_{1\to2})^{-1}\\
    \end{cases}
\end{equation}
The final deformation is then $\phi^{(0)}$. Later we will omit the index $k$ altogether to mean the deformation from the final stage.

\begin{proposition}\label{proposition:anti-symmetricity}
The properties \ref{eq:anti-symmetricity} and \ref{eq:identity_for_identic_inputs} hold for the proposed multi-resolution architecture.
\end{proposition}

\begin{proof}\renewcommand{\qedsymbol}{}
See Appendix~\ref{appendix:anti-symmetricity_proof}
\end{proof}

We formulate the networks $\tilde{f}^{(k)}$ such that for some $h^{(k)}$ they can be written as $\tilde{f}^{(k)}(z^{(k)}_1, z^{(k)}_2) = h(z^{(k)}_1 - z^{(k)}_2, z^{(k)}_1 + z^{(k)}_2)$ since \citet{young2022superwarp} argue that this way the correct displacement is easier to extract from the features.

At each resolution before upsampling we limit the displacement fields predicted within $\tilde{f}^{(k)}$ to the range $]-\gamma, \gamma[$ (in voxel coordinates) where $\gamma \in \mathbb{R}^+$ using $\gamma \times \operatorname{Tanh}$ function. We use value $\gamma = 0.15$. The chosen constraint was concluded to provide enough freedom while resulting in almost always invertible final deformations. We also experimented with value $\gamma = 0.3$ which also performed well but chose the $\gamma=0.15$ for the final experiments. More details on the hyper parameter tuning can be found in Appendix \ref{appendix:validation_set_results}. Note that the actual deformation at each stage is a composition of $4$ such small deformations and that small displacements at lower resolutions correspond to larger displacements at higher resolutions. Also, one can optionally predict an affine deformation before the full displacement fields. Affine registration stage is then just one more anti-symmetric update before the other updates. We use affine parametrization by \citet{kaji2016concise}. 

\subsection{Training}

\begin{table*}
\centering
\setlength\tabcolsep{3pt}
\small
\caption{\textbf{Mean results for the LPBA40 experiment E3.} The values computed on the test set with the standard deviation shown in the parenthesis. VoxelMorph and cLapIRN do not predict inverse deformations and hence the inverse-consistency error can not be computed for them.}
\vskip 0.15in
\begin{tabular}{lccccc}
\hline
 Model                       & Dice $\uparrow$ & HD95 $\downarrow$ & $|J_{\phi}|_{\leq 0} \downarrow$ & $\operatorname{std}(|J_{\phi}|) \downarrow$ & $||\phi \circ \phi^{-1}||^2 \downarrow$ \\
 \hline SYMNet               &  0.669 (0.033)  &    6.79 (0.69)    &            \textbf{0.7} (1.7)             &                0.34 (0.037)                 &             0.0082 (0.011)              \\
 SYMNet without anti-folding &  0.664 (0.034)  &    6.88 (0.72)    &            14 (9.1)            &                0.36 (0.039)                 &             0.0088 (0.0097)             \\
 VoxelMorph                  &  0.676 (0.032)  &    6.72 (0.68)    &         7163 (8412)          &                0.34 (0.035)                 &                    -                    \\
 cLapIRN                     &  0.714 (0.019)  &    5.93 (0.43)    &         2951 (1087)          &                \textbf{0.26} (0.019)                 &                    -                    \\
 \hline ASymReg              &  \textbf{0.716} (0.017)  &    \textbf{5.91} (0.39)    &            1.8 (1.5)             &                0.29 (0.013)                 &            \textbf{0.0028} (0.00059)             \\
\hline
\end{tabular}
\label{table:results_lpba40}
\vskip -0.1in
\end{table*}

We train the model in an unsupervised end-to-end manner similarly to the most of unsupervised registration methods by using a similarity loss and a deformation regularization loss. Similarity losses encourage deformed images to be similar to the target images and regularity losses aim to ensure that the predicted deformations have desirable properties such as smoothness.

In practice for similarity we use local normalized cross-correlation with window width 7 and for regularization we use $L^2$ gradient penalty on the displacement fields based on estimating the gradients as differences between neighboring voxels, both identically to VoxelMorph \cite{balakrishnan2019voxelmorph}

We apply losses in both directions to maintain the symmetry of the method. One could also apply the losses in the intermediate coordinates and avoid having to build the full deformations during the training. However, the current strategy was chosen since we expect that applying the losses in the original image coordinates gives the best results.

The final loss can be written as:
\begin{equation}
\begin{aligned}
    \mathcal{L} &=
    \operatorname{NCC}(x_1 \circ \phi,\ x_2) +
    \operatorname{NCC}(x_2 \circ \phi^{-1},\ x_1)\\
    &+ \lambda * \left[
        \operatorname{Grad}(d(\phi)) + \operatorname{Grad}(d(\phi^{-1}))
    \right].
\end{aligned}
\end{equation}
Here $\operatorname{NCC}$ is the local normalized cross-correlation loss and $\operatorname{Grad}$ is the gradient loss.

\section{Experiments}

We evaluate the method in subject-to-subject registration using two datasets: OASIS dataset consisting of 414 T1-weighted brain MRI images \cite{marcus2007open} as pre-processed for Learn2Reg challenge \cite{hoopes2021hypermorph, hering2022learn2reg} and LPBA40 dataset consisting of 40 brain MRI images \cite{shattuck2008construction}. Pre-processing for both datasets includes bias field correction, normalization, cropping. For OASIS dataset we use affinely pre-aligned images and for LPBA40 dataset we use rigidly pre-aligned images. Additionally we conduct one experiment with non-affinely aligned OASIS data with resolution $255 \times 255 \times 255$ to see how well our method performs with larger initial displacements. Voxel sizes of the affinely aligned and the non-affinely-aligned datasets are the same.

We split the OASIS dataset into $255$, $20$ and $139$ images for training, validation, and testing respectively. The split is different from the one used in Learn2Reg challenge since the test set is not currently available but sizes correspond to the splits used by \citet{mok2020fast, mok2020large, mok2021conditional}. We use all image pairs for testing and validation giving us $9591$ pairs for testing and $190$ pairs for validation. For the affinely-aligned OASIS experiment we crop the images to $144 \times 192 \times 160$ resolution. 

We split LPBA40 dataset to $25$, $5$ and $10$ images for training, validation, and testing respectively. This leaves us with $10$ pairs for validation and $45$ for testing. We crop the LPBA40 images to $160 \times 192 \times 160$ resolution.

In summary, we conduct three experiments:
\begin{enumerate}
    \item[E1.] Affinely aligned OASIS dataset
    \item[E2.] Non-affinely-aligned OASIS dataset. For this experiment we train only our model.
    \item[E3.] LPBA40 dataset
\end{enumerate}

\subsection{Evaluation metrics}

We evaluate the registration accuracy using the segmentations included in the two datasets. OASIS dataset includes automatic segmentations of $35$  brain structures and LPBA40 datasets includes manual segmentations of $56$ brain structures. We use two metrics: Dice score (Dice) and 95\% quantile of the Hausdorff distances (HD95) between the segmentations of each structure. These metrics were also used in Learn2Reg challenge \cite{hering2022learn2reg}. Dice score measures overlap of the segmentations and Hausdorff distance measures distance between the surfaces of the segmentations. The comparisons are done between the segmentations of the source images deformed by the generated deformations and the segmentations of the target images.

In addition, as is the standard procedure, we evaluate regularity of the generated deformations by local Jacobian determinant based metrics. We count the number voxels with negative determinant ($|J_{\phi}|_{\leq 0}$), and compute the standard deviation of the determinant ($\operatorname{std}(|J_{\phi}|)$), both over the whole volumes. Negative determinant means that the deformations is not locally invertible at that location and standard deviation of the determinant describes smoothness of the deformation. We estimate the gradients required for estimating the determinants using central finite differences.

To measure the accuracy of the generated inverses we measure mean squared error of the composition of the forward and the inverse mappings ($||\phi \circ \phi^{-1}||^2$). We label the metric inverse-consistency error.

If a method predicts both forward and inverse deformation we take the mean of the computed metrics between the forward and inverse predictions. Also the inverse-consistency error is computed both ways, although not shown explicitly in the formula above.

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{figures/dice_scores_oasis.pdf}
\vskip -0.1in
\caption{\textbf{Individual brain structure dice scores for the OASIS experiment E1.} Boxplot shows performance of each of the compared methods on each of the brain structures in the OASIS dataset. Algorithms from left to right in each group: ASymReg, cLapIRN, VoxelMorph, SYMNet}
\label{fig:dice_results_oasis}
\vskip -0.1in
\end{figure*}

\subsection{Baseline methods}

We compare our method against VoxelMorph \cite{balakrishnan2019voxelmorph}, SYMNet \cite{mok2020fast}, and conditional LapIRN (cLapIRN) \cite{mok2020large, mok2021conditional}. VoxelMorph has emerged as a standard baseline in deep learning based unsupervised registration and was hence chosen. With SYMNet we are interested in how well our method can generate invertible deformations and how accurate the generated inverse deformations are compared to SVF based methods. cLapIRN was chosen since it was the best performing method on OASIS dataset in Learn2Reg 2021 challenge \cite{hering2022learn2reg}. We used the official implementations provided online for training the algorithms and modified them for our datasets.
\footnote{\href{https://github.com/voxelmorph/voxelmorph}{https://github.com/voxelmorph/voxelmorph}}
\footnote{\href{https://github.com/cwmok/Fast-Symmetric-Diffeomorphic-Image-Registration-with-Convolutional-Neural-Networks}{https://github.com/cwmok/Fast-Symmetric-Diffeomorphic-Image-Registration-with-Convolutional-Neural-Networks}}
\footnote{\href{https://github.com/cwmok/Conditional_LapIRN/}{https://github.com/cwmok/Conditional\_LapIRN/}}.

SYMNet uses anti-folding loss which penalizes negative determinant. Since the loss is a separate technique and could be used with any of the methods, we also trained a SYMNet model without anti-folding loss (SYMNet without anti-folding). That way it provides an comparison on how well the vanilla SVF framework can generate invertible deformations in comparison to our method.

\subsection{Hyperparameters}

\begin{figure*}[t]
\vskip 0.2in
\centering
\includegraphics[width=1.0\textwidth]{figures/dice_scores_lpba40.pdf}
\vskip -0.1in
\caption{\textbf{Individual brain structure dice scores for the LPBA40 experiment E3.} Boxplot shows performance of each of the compared methods on each of the brain structures in the LPBA40 dataset. Algorithms from left to right in each group: ASymReg, cLapIRN, VoxelMorph, SYMNet}
\label{fig:dice_results_lpba40}
\vskip -0.2in
\end{figure*}

We use value $\lambda = 1.0$ for regularization. In the experiment E1 we use $5$ resolution levels and in the experiments E2 and E3 we use $6$ resolution levels. Additionally in the experiment E2 we predict an affine transformation which we omit from the deformation regularization.

We train VoxelMorph with losses and regularization weight identical to our method. For SYMNet we use directly the hyperparameter values provided in the paper \cite{mok2020fast}. For cLapIRN we perform a hyperparameter optimization of the regularization weight on the validation sets for which more details can be found in Appendix \ref{appendix:clapirn_hyperparameter}.

We use the default amount of convolution features for the baseline methods except that for VoxelMorph we double the amount of features as that was suggested for subject-to-subject registration tasks in the original paper \cite{balakrishnan2019voxelmorph}.

\subsection{Implementation}

Our proposed method is implemented in PyTorch \citep{pytorch} and the code is provided as part of the supplementary materials. The implementation allows for easily reproducing the results for our method. The evaluation methods and our part of the data pre-processing are also included in the codebase. The repository will be made public upon acceptance.

\subsection{Results}

The results on the test set are shown in Tables \ref{table:results_oasis} and \ref{table:results_lpba40}, and Figures \ref{fig:dice_results_oasis} and \ref{fig:dice_results_lpba40}. Model performance statistics are shown in Table \ref{table:performance_results}.

The proposed method performs very well on both datasets with dice score and HD95 equal to or better than all the baselines. In addition, the generated deformations have very little folding and are equal to or even improve upon the SVF-based SYMNet on inverse-consistency error.

The model trained with non-affinely-aligned OASIS data in the experiment E2 performs equally well to the model trained with affinely aligned data demonstrating the that the method is capable of accurately registering images with large initial misalignments.

Inference time of the method is slightly larger that that of the compared methods. On the other hand, the method produces deformations in both directions in that time whereas VoxelMorph and cLapIRN would require a separate inference. Also, half a second runtime is still very fast and restrictive only in the most time-critical real time use cases. In terms of inference memory usage our method is very competitive.

\section{Conclusions}

We have proposed a novel image registration architecture with the desirable theoretical property of anti-symmetricity. Due to the developed multi-resolution formulation the method is capable of accurately registering images even with large intial misalignments. To implement the theoretical ideas in practice, we developed a new neural network component \textit{deformation inversion layer}. The model is easily end-to-end trainable and does not require tedious multi-stage training strategies. In the experiments we have demonstrated that the method achieves state-of-the-art registration accuracy.

\bibliographystyle{agsm.bst}
\bibliography{references}

\newpage
\appendix
\onecolumn

\section{Hyperparameter optimization details}\label{appendix:validation_set_results}

\begin{table*}
\centering
\setlength\tabcolsep{3pt}
\small
\caption{Hyperparameter optimization results for our method calculated on the OASIS validation set. The chosen configuration was $\lambda = 1.0$, $\gamma = 0.15$, and $K=5$. HD95 metric is not included due to relatively high computational cost.}
\vskip 0.15in
\begin{tabular}{c|cc|cccc}
\hline
 $\lambda$  & $\gamma$ & $K$ & Dice $\uparrow$ & $|J_{\phi}|_{\leq 0} \downarrow$ & $\operatorname{std}(|J_{\phi}|) \downarrow$ & $||\phi \circ \phi^{-1}||^2 \downarrow$ \\
 \hline 1.0 &   0.15   &  5  &  0.820 (0.034)  &            10.9 (4.1)            &                0.42 (0.023)                 &            0.0057 (0.00070)             \\
    1.5     &   0.15   &  5  &  0.817 (0.034)  &            0.4 (0.6)             &                0.38 (0.019)                 &            0.0037 (0.00044)             \\
    2.0     &   0.15   &  5  &  0.814 (0.035)  &            0.0 (0.1)             &                0.35 (0.018)                 &            0.0026 (0.00033)             \\
 \hline 1.0 &   0.3    &  4  &  0.823 (0.034)  &           65.9 (14.5)            &                0.43 (0.024)                 &            0.0083 (0.00096)             \\
    1.5     &   0.3    &  4  &  0.819 (0.034)  &            5.1 (2.6)             &                0.39 (0.020)                 &            0.0052 (0.00060)             \\
    2.0     &   0.3    &  4  &  0.815 (0.035)  &            0.4 (0.6)             &                0.36 (0.018)                 &            0.0038 (0.00041)             \\
\hline
\end{tabular}



\label{table:hyperparameter_optimization_oasis}
\vskip -0.1in
\end{table*}

\begin{table*}[b]
\centering
\setlength\tabcolsep{3pt}
\small
\caption{Hyperparameter optimization results for our method calculated on the LPBA40 validation set. The chosen configuration was $\lambda = 1.0$, $\gamma = 0.15$, $K=7$, and $\text{Affine} = \text{No}$.  Inference time was measured on NVIDIA GeForce RTX 3090.}
\vskip 0.15in
\begin{tabular}{cccc|cccccc}
\hline
 $\lambda$  & $\gamma$ & $K$ & Affine & Dice $\uparrow$ & HD95 $\downarrow$ & $|J_{\phi}|_{\leq 0} \downarrow$ & $\operatorname{std}(|J_{\phi}|) \downarrow$ & $||\phi \circ \phi^{-1}||^2 \downarrow$ & Inference time (s) $\downarrow$ \\
 \hline 1.0 &   0.15   &  5  &   No   &  0.710 (0.016)  &    6.03 (0.41)    &            1.8 (1.2)             &                0.28 (0.0098)                &            0.0029 (0.00090)             &          0.50 (0.0089)          \\
    1.0     &   0.15   &  6  &   No   &  0.717 (0.014)  &    5.82 (0.35)    &            0.8 (0.9)             &                0.28 (0.0086)                &            0.0027 (0.00069)             &          0.56 (0.0056)          \\
    1.0     &   0.15   &  7  &   No   &  0.719 (0.012)  &    5.80 (0.30)    &            1.4 (1.4)             &                0.27 (0.0090)                &            0.0026 (0.00075)             &          0.65 (0.0056)          \\
    1.0     &   0.15   &  5  &  Yes   &  0.711 (0.017)  &    5.99 (0.45)    &            1.4 (0.9)             &                0.28 (0.0098)                &            0.0026 (0.00071)             &          0.50 (0.0052)          \\
    1.0     &   0.15   &  6  &  Yes   &  0.718 (0.013)  &    5.79 (0.29)    &            1.3 (1.0)             &                0.28 (0.0093)                &            0.0026 (0.00070)             &          0.57 (0.0045)          \\
    1.0     &   0.3    &  4  &   No   &  0.706 (0.014)  &    6.09 (0.36)    &            13.7 (3.9)            &                0.29 (0.011)                 &             0.0036 (0.0012)             &          0.49 (0.0039)          \\
    1.0     &   0.3    &  5  &   No   &  0.718 (0.013)  &    5.87 (0.35)    &            11.2 (4.4)            &                0.29 (0.010)                 &             0.0037 (0.0011)             &          0.56 (0.0079)          \\
    1.0     &   0.3    &  6  &   No   &  0.721 (0.014)  &    5.74 (0.31)    &            10.2 (4.8)            &                0.29 (0.010)                 &            0.0035 (0.00071)             &          0.63 (0.0093)          \\
\hline
\end{tabular}
\label{table:hyperparameter_optimization_lpba40}
\vskip -0.1in
\end{table*}


We experimented on validation set with different hyperparameters during the development. While the final results on test set are computed only for one chosen configuration, the results on validation set might still be of interest for the reader. Results of these experiments for the OASIS dataset are shown in Table \ref{table:hyperparameter_optimization_oasis} and for the LPBA40 dataset in Table \ref{table:hyperparameter_optimization_lpba40}.

For the OASIS dataset we experimented with two configurations of number of resolution levels $K$ and maximum displacements $\gamma$. With both of these configurations we tested three different values for the regularization weight $\lambda$.

For the LPBA40 dataset we experimented with 8 configurations of number of resolution levels $K$, maximum displacements $\gamma$, and whether to predict an affine transformation, but used the regularization weight value $\lambda = 1.0$ for all of them.

Smaller maximum displacement was chosen for both of the experiments since the resulting deformations had better properties with only small decrease in accuracy.

\section{Hyperparameter optimization details for cLapIRN}\label{appendix:clapirn_hyperparameter}

For cLapIRN baseline we used the regularization parameter value $\overline{\lambda} = 0.05$ for the OASIS dataset and value $\overline{\lambda} = 0.1$ for the LPBA40 dataset where $\overline{\lambda}$ is used as in the paper presenting the method \cite{mok2021conditional}. The value was chosen based on the validation set results shown in Tables \ref{table:clapirn_hyperparameter_optimization_oasis} and \ref{table:clapirn_hyperparameter_optimization_lpba40}.
\begin{table*}
\centering
\setlength\tabcolsep{3pt}
\small
\caption{Regularization parameter optimization results for cLapIRN calculated on the OASIS validation set. Here $\overline{\lambda}$ refers to the normalized regularization weight of the gradient loss of cLapIRN and should be in range $[0,\ 1]$. Value $\overline{\lambda} = 0.05$ was chosen since it resulted in clearly the highest Dice score. HD95 metric is not included due to relatively high computational cost.}
\vskip 0.15in
\begin{tabular}{c|ccc}
\hline
 $\overline{\lambda}$ & Dice $\uparrow$ & $|J_{\phi}|_{\leq 0} \downarrow$ & $\operatorname{std}(|J_{\phi}|) \downarrow$ \\
     \hline 0.01      &  0.812 (0.034)  &        92458.2 (10898.8)         &                0.75 (0.040)                 \\
         0.05         &  0.817 (0.034)  &         37875.5 (6698.5)         &                0.53 (0.026)                 \\
         0.1          &  0.812 (0.035)  &         14688.6 (3993.9)         &                0.41 (0.019)                 \\
         0.2          &  0.798 (0.038)  &         2337.8 (1271.4)          &                0.30 (0.013)                 \\
         0.4          &  0.769 (0.042)  &           22.5 (33.9)            &                0.18 (0.0088)                \\
         0.8          &  0.727 (0.049)  &            0.0 (0.2)             &                0.10 (0.0051)                \\
         1.0          &  0.711 (0.052)  &            0.0 (0.0)             &               0.081 (0.0043)                \\
\hline
\end{tabular}

\label{table:clapirn_hyperparameter_optimization_oasis}
\vskip -0.1in
\end{table*}
\begin{table*}[b]
\centering
\setlength\tabcolsep{3pt}
\small
\caption{Regularization parameter optimization results for cLapIRN calculated on the LPBA40 validation set. Here $\overline{\lambda}$ refers to the normalized regularization weight of the gradient loss of cLapIRN and should be in range $[0,\ 1]$. Value $\overline{\lambda} = 0.1$ was chosen due to the best overall performance.}
\vskip 0.15in
\begin{tabular}{c|cccc}
\hline
 $\overline{\lambda}$ & Dice $\uparrow$ & HD95 $\downarrow$ & $|J_{\phi}|_{\leq 0} \downarrow$ & $\operatorname{std}(|J_{\phi}|) \downarrow$ \\
     \hline 0.01      &  0.714 (0.014)  &    5.86 (0.35)    &         36607.9 (5827.7)         &                0.43 (0.017)                 \\
         0.05         &  0.715 (0.014)  &    5.87 (0.35)    &         11515.6 (2605.3)         &                0.32 (0.013)                 \\
         0.1          &  0.714 (0.014)  &    5.88 (0.36)    &          2565.8 (866.8)          &                0.25 (0.010)                 \\
         0.2          &  0.709 (0.015)  &    5.92 (0.38)    &           129.1 (74.0)           &                0.18 (0.0085)                \\
         0.4          &  0.698 (0.017)  &    6.03 (0.42)    &            0.1 (0.2)             &                0.13 (0.0070)                \\
         0.8          &  0.678 (0.019)  &    6.23 (0.48)    &            0.0 (0.0)             &               0.085 (0.0062)                \\
         1.0          &  0.671 (0.021)  &    6.31 (0.51)    &            0.0 (0.0)             &               0.073 (0.0061)                \\
\hline
\end{tabular}
\label{table:clapirn_hyperparameter_optimization_lpba40}
\vskip -0.1in
\end{table*}

\section{Proof of Proposition \ref{proposition:anti-symmetricity}}\label{appendix:anti-symmetricity_proof}
\begin{proof}
Throughout the proof we see the deformations as functions of the features extracted from the images. We use induction. Assume that for any $z_1$ and $z_2$ at level $k+1$ the following holds: $\phi^{(k + 1)}_{1\to2}(z_1, z_2) = \phi^{(k + 1)}_{2\to1}(z_2, z_1)$. For level $K$ it holds trivially since $\phi^{(K)}_{1\to2}$ and $\phi^{(K)}_{2\to1}$ are identity mappings. Using the induction assumption we have at level $k$:
\begin{align*}
\psi_F^{(k)}(z_1, z_2) &= \tilde{f}^{(k)}(
    z^{(k)}_1 \circ \phi_{1\to2}^{(k + 1)}(z_1, z_2),\ 
    z^{(k)}_2 \circ \phi_{2\to1}^{(k + 1)}(z_1, z_2)
)\\ &= \tilde{f}^{(k)}(
    z^{(k)}_1 \circ \phi_{2\to1}^{(k + 1)}(z_2, z_1),\ 
    z^{(k)}_2 \circ \phi_{1\to2}^{(k + 1)}(z_2, z_1)
) = \psi_R^{(k)}(z_2, z_1)
\end{align*}
Then we can finalize the induction step:
\begin{align*}
\phi^{(k)}_{1\to2}(z_1, z_2) &= \phi^{(k + 1)}_{1\to2}(z_1, z_2)
    \circ \psi_F^{(k)}(z_1, z_2)
    \circ \psi_R^{(k)}(z_1, z_2)^{-1}\\
&= \phi^{(k + 1)}_{2\to1}(z_2, z_1)
    \circ \psi_R^{(k)}(z_2, z_1)
    \circ \psi_F^{(k)}(z_2, z_1)^{-1} = \phi^{(k)}_{2\to1}(z_2, z_1)
\end{align*}
From this follows that the full deformation generation is anti-symmetric
\begin{align*}
\phi^{(k)}(z_1, z_2) &= \phi^{(k)}_{1\to2}(z_1, z_2) \circ \phi^{(k)}_{2\to1}(z_1, z_2)^{-1}\\
&= \phi^{(k)}_{2\to1}(z_2, z_1) \circ \phi^{(k)}_{1\to2}(z_2, z_1)^{-1}\\
&= \left[\phi^{(k)}_{1\to2}(z_2, z_1) \circ \phi^{(k)}_{2\to1}(z_2, z_1)^{-1}\right]^{-1} =\phi^{(k)}(z_2, z_1)^{-1}
\end{align*}
and that registering an image to itself results in the identity mapping
\begin{align*}
\phi^{(k)}(z_1, z_1) &= \phi^{(k)}_{1\to2}(z_1, z_1) \circ \phi^{(k)}_{2\to1}(z_1, z_1)^{-1}\\
&= \phi^{(k)}_{1\to2}(z_1, z_1) \circ \phi^{(k)}_{1\to2}(z_1, z_1)^{-1} = I.
\end{align*}

Since the properties hold with respect to the features $z_1$ and $z_2$, they also hold with respect to the input images since the features are extracted from the images independently using the same network.
\end{proof}

\end{document}
