\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage{amsmath}
\usepackage[preprint]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}

\usepackage{zref-xr}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{natbib}
\usepackage[pdftex]{graphicx}
\usepackage{cases}
\usepackage{arydshln}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{bm}
\zxrsetup{toltxlabel}

\zexternaldocument*{build/submission}

\renewcommand{\thesection}{\Roman{section}} 
\renewcommand{\thesubsection}{\thesection.\Alph{subsection}}
% Hack for making figures Say \figurename S\thefigure, e.g. Figure S1: 
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\theequation}{A\arabic{equation}}

\newcommand{\expnumber}[2]{{#1}\mathrm{e}{#2}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}} %Citation-related commands


\title{Appendices\\SITReg: Multi-resolution architecture for symmetric, inverse consistent, and topology preserving image registration using deformation inversion layers}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Joel Honkamaa \\
  Department of Computer Science\\
  Aalto University\\
  Aalto, Finland\\
  \texttt{joel.honkamaa@aalto.fi}\\
  % examples of more authors
  \And
  Pekka Marttinen \\
  Department of Computer Science\\
  Aalto University\\
  Aalto, Finland\\
  \texttt{pekka.marttinen@aalto.fi}\\
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}

\maketitle
\suppressfloats[t]

\section{Architectural details}\label{appendix:implementation_details}

The neural networks $u^{(k)}$ used for predicting deformations (in Equation \ref*{eq:update_deformation_formula}) consist of the following components (in order):
\begin{enumerate}
    \item Concatenation of the two inputs along the channel dimension. Before concatenation we reparametrize the features as subtraction of the inputs and the sum of the inputs as suggested by \citet{young2022superwarp}.
    \item Two convolutions with some non-linear activation after each of the convolutions.
    \item Convolution with kernel of spatial size $1$
    \item $\gamma \times \operatorname{Tanh}$ function where the $\gamma \in \mathbb{R}^+$ is a scaling factor defining maximum displacement. We used the value $\gamma = 0.15$ (voxels) in the experiments. The voxel size is voxel size of each resolution (lower resolution levels have larger voxels).
    \item Upsampling with prefiltered cubic spline interpolation \citep{ruijters2012gpu} to the full resolution. Spline interpolation can be implemented effeciently using transposed convolutions \citep{de2019deep}. The deformation is upsampled since it needs to be inverted and otherwise the generated inverse would not be accurate in the full resolution.
\end{enumerate}

We equate the resulting displacement field with the deformation. Limiting the range of values with the scaled $\operatorname{Tanh}$ activation is important since by that we ensure that individual predicted deformation are invertible, which in turn ensures topology preservation and convergence of the fixed point iteration. The chosen constraint was concluded to provide enough freedom while resulting in almost everywhere invertible final deformations (the performance is similar to the SVF \citep{arsigny2006log} based methods). We also experimented with value $\gamma = 0.3$ which also performed well but chose the $\gamma=0.15$ for the final experiments. More details on this can be found in Appendix \ref{appendix:hyperparameter_selection}. Note that since the displacements are in the voxel size of each resolution level, the displacements at lower resolutions correspond to larger displacements at the full resolution (scaling is performed during the upsampling).

\section{Hyperparameter selection details}\label{appendix:hyperparameter_selection}

We experimented on validation set with different hyperparameters during the development. While the final results on test set are computed only for one chosen configuration, the results on validation set might still be of interest for the reader. Results of these experiments for the OASIS dataset are shown in Table \ref{appendix-table:hyperparameter_optimization_oasis} and for the LPBA40 dataset in Table \ref{appendix-table:hyperparameter_optimization_lpba40}.

For the OASIS dataset we experimented with two configurations of number of resolution levels $K$ and maximum displacements $\gamma$. With both of these configurations we tested three different values for the regularization weight $\lambda$.

For the LPBA40 dataset we experimented with 8 configurations of number of resolution levels $K$, maximum displacements $\gamma$, and whether to predict an affine transformation, but used the regularization weight value $\lambda = 1.0$ for all of them.

Smaller maximum displacement was chosen for both of the experiments since the resulting deformations had better properties with only small decrease in accuracy.

\begin{table}
\centering
\setlength\tabcolsep{3pt}
\scriptsize
\caption{Hyperparameter optimization results for our method calculated on the OASIS validation set. The chosen configuration was $\lambda = 1.0$, $\gamma = 0.15$, and $K=5$. HD95 metric is not included due to relatively high computational cost. Here $\gamma$ corresponds to the scaling factor described in Appendix \ref{appendix:implementation_details}}
\begin{tabular}{cccccccc}
\toprule
\multicolumn{3}{c}{Hyperparameters}  & \multicolumn{1}{c}{Accuracy} & \multicolumn{2}{c}{Deformation regularity} & \multicolumn{2}{c}{Consistency}\\
\cmidrule(r){1-3} \cmidrule(r){4-4} \cmidrule(r){5-6} \cmidrule(r){7-8}
$\lambda$  & $\gamma$ & $K$ & Dice $\uparrow$ & $|J_{\phi}|_{\leq 0} \downarrow$ & $\operatorname{std}(|J_{\phi}|) \downarrow$ &       Cycle $\downarrow$        &      Inverse $\downarrow$       \\
 \midrule 1.0    &   0.15   &  5  & $0.820 (0.034)$ &           $11. (5.5)$            &               $0.42 (0.025)$                & $\expnumber{5.5}{-3} (\expnumber{5.8}{-4})$ & $\expnumber{5.5}{-3} (\expnumber{5.8}{-4})$ \\
    1.5    &   0.15   &  5  & $0.817 (0.035)$ &          $0.43 (0.84)$           &               $0.38 (0.022)$                & $\expnumber{3.5}{-3} (\expnumber{3.3}{-4})$ & $\expnumber{3.5}{-3} (\expnumber{3.3}{-4})$ \\
    2.0    &   0.15   &  5  & $0.814 (0.035)$ &         $0.0053 (0.072)$         &               $0.35 (0.020)$                & $\expnumber{2.4}{-3} (\expnumber{2.2}{-4})$ & $\expnumber{2.4}{-3} (\expnumber{2.2}{-4})$ \\
    1.0    &   0.3    &  4  & $0.823 (0.034)$ &           $66. (19.)$            &               $0.43 (0.026)$                & $\expnumber{8.0}{-3} (\expnumber{8.2}{-4})$ & $\expnumber{8.0}{-3} (\expnumber{8.2}{-4})$ \\
    1.5    &   0.3    &  4  & $0.819 (0.035)$ &           $5.1 (3.6)$            &               $0.39 (0.022)$                & $\expnumber{4.9}{-3} (\expnumber{4.8}{-4})$ & $\expnumber{4.9}{-3} (\expnumber{4.8}{-4})$ \\
    2.0    &   0.3    &  4  & $0.815 (0.035)$ &          $0.42 (0.91)$           &               $0.36 (0.020)$                & $\expnumber{3.6}{-3} (\expnumber{3.1}{-4})$ & $\expnumber{3.6}{-3} (\expnumber{3.1}{-4})$ \\
\bottomrule
\end{tabular}
\label{appendix-table:hyperparameter_optimization_oasis}
\end{table}

\begin{table}
\centering
\setlength\tabcolsep{3pt}
\scriptsize
\caption{Hyperparameter optimization results for our method calculated on the LPBA40 validation set. The chosen configuration was $\lambda = 1.0$, $\gamma = 0.15$, $K=7$, and $\text{Affine} = \text{No}$. Here $\gamma$ corresponds to the scaling factor described in Appendix \ref{appendix:implementation_details}}
\begin{tabular}{ccccccccccc}
\toprule
\multicolumn{4}{c}{Hyperparameters}  & \multicolumn{2}{c}{Accuracy} & \multicolumn{2}{c}{Deformation regularity} & \multicolumn{2}{c}{Consistency}\\
\cmidrule(r){1-4} \cmidrule(r){5-6} \cmidrule(r){7-8} \cmidrule(r){9-10}
$\lambda$  & $\gamma$ & $K$ & Affine & Dice $\uparrow$ & HD95 $\downarrow$ & $|J_{\phi}|_{\leq 0} \downarrow$ & $\operatorname{std}(|J_{\phi}|) \downarrow$ &       Cycle $\downarrow$        &      Inverse $\downarrow$       \\
 \midrule 1.0 &   0.15   &  5  &   No   & $0.710 (0.016)$ &   $6.03 (0.42)$   &           $1.8 (1.9)$            &               $0.28 (0.018)$                & $\expnumber{2.7}{-3} (\expnumber{3.4}{-4})$ & $\expnumber{2.7}{-3} (\expnumber{3.4}{-4})$ \\
    1.0     &   0.15   &  6  &   No   & $0.717 (0.014)$ &   $5.82 (0.35)$   &           $0.80 (1.1)$           &               $0.28 (0.017)$                & $\expnumber{2.6}{-3} (\expnumber{3.0}{-4})$ & $\expnumber{2.6}{-3} (\expnumber{3.0}{-4})$ \\
    1.0     &   0.15   &  7  &   No   & $0.719 (0.013)$ &   $5.80 (0.32)$   &           $1.4 (1.8)$            &               $0.27 (0.017)$                & $\expnumber{2.4}{-3} (\expnumber{3.0}{-4})$ & $\expnumber{2.4}{-3} (\expnumber{3.0}{-4})$ \\
    1.0     &   0.15   &  5  &  Yes   & $0.711 (0.017)$ &   $5.99 (0.45)$   &           $1.4 (1.3)$            &               $0.28 (0.018)$                & $\expnumber{2.5}{-3} (\expnumber{3.2}{-4})$ & $\expnumber{2.5}{-3} (\expnumber{3.2}{-4})$ \\
    1.0     &   0.15   &  6  &  Yes   & $0.718 (0.013)$ &   $5.79 (0.31)$   &           $1.4 (1.4)$            &               $0.28 (0.017)$                & $\expnumber{2.5}{-3} (\expnumber{3.2}{-4})$ & $\expnumber{2.5}{-3} (\expnumber{3.2}{-4})$ \\
    1.0     &   0.3    &  4  &   No   & $0.706 (0.014)$ &   $6.09 (0.38)$   &           $14. (5.0)$            &               $0.29 (0.019)$                & $\expnumber{3.2}{-3} (\expnumber{4.8}{-4})$ & $\expnumber{3.2}{-3} (\expnumber{4.8}{-4})$ \\
    1.0     &   0.3    &  5  &   No   & $0.718 (0.013)$ &   $5.87 (0.36)$   &           $11. (5.3)$            &               $0.29 (0.019)$                & $\expnumber{3.5}{-3} (\expnumber{4.6}{-4})$ & $\expnumber{3.5}{-3} (\expnumber{4.6}{-4})$ \\
    1.0     &   0.3    &  6  &   No   & $0.721 (0.014)$ &   $5.74 (0.33)$   &           $10. (6.1)$            &               $0.29 (0.018)$                & $\expnumber{3.5}{-3} (\expnumber{4.6}{-4})$ & $\expnumber{3.5}{-3} (\expnumber{4.6}{-4})$ \\
\bottomrule
\end{tabular}
\label{appendix-table:hyperparameter_optimization_lpba40}
\end{table}

With the raw OASIS dataset without pre-alignment we used $6$ resolution levels, together with an affine transformation prediction stage before the other deformation updates. We omitted the predicted affine transformation from the deformation regularization.

\section{Hyperparameter selection details for baselines}\label{appendix:baseline_hyperparameter}

For cLapIRN baseline we used the regularization parameter value $\overline{\lambda} = 0.05$ for the OASIS dataset and value $\overline{\lambda} = 0.1$ for the LPBA40 dataset where $\overline{\lambda}$ is used as in the paper presenting the method \citep{mok2021conditional}. The values were chosen based on the validation set results shown in Tables \ref{appendix-table:clapirn_hyperparameter_optimization_oasis} and \ref{appendix-table:clapirn_hyperparameter_optimization_lpba40}.
\begin{table}
\centering
\setlength\tabcolsep{3pt}
\scriptsize
\caption{Regularization parameter optimization results for cLapIRN calculated on the OASIS validation set. Here $\overline{\lambda}$ refers to the normalized regularization weight of the gradient loss of cLapIRN and should be in range $[0,\ 1]$. Value $\overline{\lambda} = 0.05$ was chosen since it resulted in clearly the highest Dice score. HD95 metric is not included due to relatively high computational cost.}
\begin{tabular}{cccccc}
\toprule
Hyperparameters & \multicolumn{1}{c}{Accuracy} & \multicolumn{2}{c}{Deformation regularity} & Consistency\\
\cmidrule(r){1-1} \cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-6}
$\overline{\lambda}$ & Dice $\uparrow$ & $|J_{\phi}|_{\leq 0} \downarrow$ & $\operatorname{std}(|J_{\phi}|) \downarrow$ &       Cycle $\downarrow$     \\
\midrule
     0.01      & $0.812 (0.034)$ &   $\expnumber{9.2}{4} (\expnumber{1.1}{4})$   &               $0.75 (0.040)$                & $\expnumber{1.7}{0} (\expnumber{1.5}{-1})$  \\
         0.05         & $0.817 (0.034)$ &   $\expnumber{3.8}{4} (\expnumber{6.7}{3})$   &               $0.53 (0.026)$                & $\expnumber{1.2}{0} (\expnumber{1.3}{-1})$  \\
         0.1          & $0.812 (0.035)$ &   $\expnumber{1.5}{4} (\expnumber{4.0}{3})$   &               $0.41 (0.019)$                & $\expnumber{8.9}{-1} (\expnumber{1.1}{-1})$ \\
         0.2          & $0.798 (0.038)$ &   $\expnumber{2.3}{3} (\expnumber{1.3}{3})$   &               $0.30 (0.013)$                & $\expnumber{6.0}{-1} (\expnumber{8.3}{-2})$ \\
         0.4          & $0.769 (0.042)$ &   $\expnumber{2.2}{1} (\expnumber{3.4}{1})$   &               $0.18 (0.0088)$               & $\expnumber{3.5}{-1} (\expnumber{4.4}{-2})$ \\
         0.8          & $0.727 (0.049)$ &  $\expnumber{1.6}{-2} (\expnumber{2.2}{-1})$  &               $0.10 (0.0051)$               & $\expnumber{2.5}{-1} (\expnumber{3.8}{-2})$ \\
         1.0          & $0.711 (0.052)$ & $\expnumber{0.0}{0.0} (\expnumber{0.0}{0.0})$ &              $0.081 (0.0043)$               & $\expnumber{2.3}{-1} (\expnumber{3.8}{-2})$ \\
\bottomrule
\end{tabular}

\label{appendix-table:clapirn_hyperparameter_optimization_oasis}
\end{table}
\begin{table}
\centering
\setlength\tabcolsep{3pt}
\scriptsize
\caption{Regularization parameter optimization results for cLapIRN calculated on the LPBA40 validation set. Here $\overline{\lambda}$ refers to the normalized regularization weight of the gradient loss of cLapIRN and should be in range $[0,\ 1]$. Value $\overline{\lambda} = 0.1$ was chosen due to the best overall performance.}
\begin{tabular}{cccccc}
\toprule
Hyperparameters & \multicolumn{2}{c}{Accuracy} & \multicolumn{2}{c}{Deformation regularity} & Consistency\\
\cmidrule(r){1-1} \cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-6}
$\overline{\lambda}$ & Dice $\uparrow$ & HD95 $\downarrow$ & $|J_{\phi}|_{\leq 0} \downarrow$ & $\operatorname{std}(|J_{\phi}|) \downarrow$ &       Cycle $\downarrow$     \\
\midrule 0.01      & $0.714 (0.014)$ &   $5.86 (0.35)$   &   $\expnumber{3.7}{4} (\expnumber{5.8}{3})$   &               $0.43 (0.017)$                & $\expnumber{9.9}{-1} (\expnumber{2.2}{-1})$ \\
         0.05         & $0.715 (0.014)$ &   $5.87 (0.35)$   &   $\expnumber{1.2}{4} (\expnumber{2.6}{3})$   &               $0.32 (0.013)$                & $\expnumber{8.0}{-1} (\expnumber{2.1}{-1})$ \\
         0.1          & $0.714 (0.014)$ &   $5.88 (0.36)$   &   $\expnumber{2.6}{3} (\expnumber{8.7}{2})$   &               $0.25 (0.010)$                & $\expnumber{6.6}{-1} (\expnumber{2.1}{-1})$ \\
         0.2          & $0.709 (0.015)$ &   $5.92 (0.38)$   &   $\expnumber{1.3}{2} (\expnumber{7.4}{1})$   &               $0.18 (0.0085)$               & $\expnumber{4.9}{-1} (\expnumber{1.9}{-1})$ \\
         0.4          & $0.698 (0.017)$ &   $6.03 (0.42)$   &  $\expnumber{5.0}{-2} (\expnumber{2.2}{-1})$  &               $0.13 (0.0070)$               & $\expnumber{3.6}{-1} (\expnumber{1.9}{-1})$ \\
         0.8          & $0.678 (0.019)$ &   $6.23 (0.48)$   & $\expnumber{0.0}{0.0} (\expnumber{0.0}{0.0})$ &              $0.085 (0.0062)$               & $\expnumber{3.0}{-1} (\expnumber{1.9}{-1})$ \\
         1.0          & $0.671 (0.021)$ &   $6.31 (0.51)$   & $\expnumber{0.0}{0.0} (\expnumber{0.0}{0.0})$ &              $0.073 (0.0061)$               & $\expnumber{3.0}{-1} (\expnumber{1.9}{-1})$ \\
\bottomrule
\end{tabular}
\label{appendix-table:clapirn_hyperparameter_optimization_lpba40}
\end{table}

We trained VoxelMorph with losses and regularization weight identical to our method and for SYMNet we used hyperparameters directly provided by \citet{mok2020fast}. We used the default number of convolution features for the baselines except for VoxelMorph we doubled the number of features, as suggested for subject-to-subject registration by \citet{balakrishnan2019voxelmorph}.

\section{Proof of Proposition \ref*{proposition:anti-symmetricity}}\label{appendix:anti-symmetricity_proof}
\subsection{Inverse consistent by construct}
\begin{proof}
Inverse consistency by construct follows directly from Equation \ref*{eq:final_deformation}:
\begin{align*}
f_{1\to2}(x_1, x_2) &= f_{1\to1.5}^{(0)}(x_1, x_2) \circ f_{2\to1.5}^{(0)}(x_1, x_2)^{-1}\\
&= \left(f_{2\to1.5}^{(0)}(x_1, x_2) \circ f_{1\to1.5}^{(0)}(x_1, x_2)^{-1}\right)^{-1}\\
&= f_{2\to1}(x_1, x_2)^{-1}
\end{align*}
\end{proof}
As discussed in Section \ref*{sec:intro}, inverse consistency error is not exactly zero even for earlier methods guaranteeing inverse consistency by construct, and the same is true here. Here the remaining error can be in principle from three sources: limited spatial sampling resolution, deformation inversion layer not converging, or lack of invertibility of deformations. However, as shown in Appendix \ref{appendix:deformation_inversion_iteration_counts}, the error caused by fixed point iteration not converging is tiny. Lack of invertibility should also not be a large source of error since as shown by the main experiments, the deformations are almost everywhere invertible. Hence the remaining inverse consistency error should be mostly caused by limited spatial sampling resolution, conclusion supported by the main experiments, where error of similar magnitude to that of diffeomorphic SVF framework was obtained.

\subsection{Symmetric by construct}
\begin{proof}
We use induction. Assume that for any $x_1$ and $x_2$ at level $k+1$ the following holds: $f^{(k + 1)}_{1\to1.5}(x_1, x_2) = f^{(k + 1)}_{2\to1.5}(x_2, x_1)$. For level $K$ it holds trivially since $f^{(K)}_{1\to1.5}(x_1, x_2)$ and $f^{(K)}_{2\to1.5}(x_1, x_2)$ are defined as identity deformations. For the proof we view $z_1^{(k)}$, $z_2^{(k)}$, and $U^{(k)}$ as functions of the input images, although not explicitly marked in the main paper. Using the induction assumption we have at level $k$:
\begin{equation*}
z_1^{(k)}(x_1, x_2) = h^{(k)}(x_1) \circ f^{(K)}_{1\to1.5}(x_1, x_2) = h^{(k)}(x_1) \circ f^{(K)}_{2\to1.5}(x_2, x_1) = z_2^{(k)}(x_2, x_1)\
\end{equation*}
Then also:
\begin{align*}
U^{(k)}(x_1, x_2) &= u^{(k)}(z_1^{(k)}(x_1, x_2), z_2^{(k)}(x_1, x_2)) \circ u^{(k)}(z_2^{(k)}(x_1, x_2), z_1^{(k)}(x_1, x_2))^{-1}
\\
&= u^{(k)}(z_2^{(k)}(x_2, x_1), z_1^{(k)}(x_2, x_1)) \circ u^{(k)}(z_1^{(k)}(x_2, x_1), z_2^{(k)}(x_2, x_1))^{-1}\\
&=  \left[u^{(k)}(z_1^{(k)}(x_2, x_1), z_2^{(k)}(x_2, x_1)) \circ u^{(k)}(z_2^{(k)}(x_2, x_1), z_1^{(k)}(x_2, x_1))^{-1}\right]^{-1}\\
&=  U^{(k)}(x_2, x_1)^{-1}\\
\end{align*}

Then we can finalize the induction step:
\begin{align*}
f^{(k)}_{1\to1.5}(x_1, x_2) &= f^{(k + 1)}_{1\to1.5}(x_1, x_2) \circ U^{(k)}(x_1, x_2)\\
&= f^{(k + 1)}_{2\to1.5}(x_2, x_1) \circ U^{(k)}(x_2, x_1)^{-1} = f^{(k)}_{2\to1.5}(x_2, x_1)
\end{align*}

From this follows that the method is symmetric by construct:
\begin{align*}
f_{1\to2}(x_1, x_2) &= f_{1\to1.5}^{(0)}(x_1, x_2) \circ f_{2\to1.5}^{(0)}(x_1, x_2)^{-1}\\
&= f_{2\to1.5}^{(0)}(x_2, x_1) \circ f_{1\to1.5}^{(0)}(x_2, x_1)^{-1} = f_{2\to1}(x_2, x_1)
\end{align*}
Unlike inverse consistency by construct, this relation holds exactly.
\end{proof}

\section{Fixed point iteration convergence in deformation inversion layers}\label{appendix:deformation_inversion_iteration_counts}

\begin{figure}[b]
\centering
\includegraphics[width=1.0\textwidth]{figures/inversion_iteration_counts.pdf}
\caption{\textbf{Number of fixed point iterations required for convergence in deformation inversion layers with the model trained on OASIS dataset.} The stopping criterion for the fixed point iteration was maximum displacement error within the whole volume reaching below one hundredth of a voxel. All deformation inversions for the whole OASIS test set are included.}
\label{appendix-fig:deformation_inversion_iteration_counts}
\end{figure}

We conducted an experiment on the fixed point iteration convergence in the deformation inversion layers with the model trained on OASIS dataset. The results can be seen in Figure \ref{appendix-fig:deformation_inversion_iteration_counts}. The main result was that in the whole OASIS test set of 9591 pairs not a single deformation required more than 7 iterations for convergence. Deformations requiring 7 iterations were only $0.13\%$ of all the deformations and a significant majority of the deformations ($96\%$) required $3$ to $5$ iterations. In all the experiments, including this one, the stopping criterion for the iterations was maximum displacement error within the whole volume reaching below one hundredth of a voxel, which is a very small error. In conclusion, lack of convergence of the fixed point iteration in the deformation inversion layer does not introduce practically relevant error.

\section{Deformation inversion layer memory usage}\label{appendix:memory}

We conducted an experiment on the memory usage of the deformation inversion layer compared to the stationary velocity field (SVF) framework \citep{arsigny2006log} since SVF framework could also be used to implement the suggested architecture in practice.

With the SVF framework one could slightly simplify the deformation update Equation \ref{eq:update_deformation_formula} to the form
\begin{equation}\label{appendix-eq:svf_symmetric_update}
    U^{(k)} := \exp(u^{(k)}(z_1^{(k)}, z_2^{(k)}) - u^{(k)}(z_2^{(k)}, z_1^{(k)}))
\end{equation}
where $\exp$ is the SVF integration (corresponding to Lie algebra exponentiation), and $u^{(k)}$ now predicts an auxiliary velocity field. We compared memory usage of this to our implementation, and used the implementation by \citet{dalca2018unsupervised} for SVF integration.

The results are shown in Table \ref{appendix-table:svf_deformation_inversion_layer_memory_comparison}. Our version implemented using the deformation inversion layer requires $5$ times less data to be stored in memory for the backward pass compared to the SVF integration. The peak memory usage during the inversion is also slightly lower. The memory saving is due to the memory efficient back-propagation through the fixed point iteration layers, which requires only the final inverted volume to be stored for backward pass. Since our architecture requires two such operations for each resolution level ($U^{(k)}$ and its inverse), the memory saved during training is significant.

NOTE: In the main main text we stated a $29$ times smaller memory usage for our method. This result was based on an initial analysis that did not take into account the simplification in Equation \ref{appendix-eq:svf_symmetric_update}. The results reported here were obtained after the main paper submission deadline, and they will be incorporated in the main text in the revised version.

\begin{table}[h]
\centering
\setlength\tabcolsep{3pt}
\scriptsize
\caption{\textbf{Memory usage comparison between deformation inversion layer and stationary velocity field (SVF) based implementations.} The comparison is between executing Equation \ref{eq:update_deformation_formula} using deformation inversion layers and executing Equation \ref{appendix-eq:svf_symmetric_update} using SVF integration implementation by \citet{dalca2018unsupervised}. Between passes memory usage refers to the amount memory needed for storing values between forward and backward passes, and peak memory usage refers to the peak amount of memory needed during forward and backward passes. A volume of shape $(256, 256, 256)$ with $32$ bit precision was used. We used $7$ scalings and squarings for the SVF integration.}
\vskip 0.15in
\begin{tabular}{lcc}
\toprule
Method & Between passes memory usage (GB) $\downarrow$ & Peak memory usage (GB) $\downarrow$\\
\midrule
Deformation inversion layer & $\bm{0.5625}$ & $\bm{3.9375}$\\
SVF integration & $2.8125$ & $4.125$\\
\bottomrule
\end{tabular}
\label{appendix-table:svf_deformation_inversion_layer_memory_comparison}
\end{table}

\section{Additional results}\label{appendix:additonal_results}

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{figures/dice_scores_oasis.pdf}
\caption{\textbf{Individual brain structure dice scores for the OASIS experiment.} Boxplot shows performance of each of the compared methods on each of the brain structures in the OASIS dataset. Algorithms from left to right in each group: SITReg, cLapIRN, VoxelMorph, SYMNet (original)}
\label{appendix-fig:dice_scores_oasis}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{figures/dice_scores_lpba40.pdf}
\caption{\textbf{Individual brain structure dice scores for the LPBA40 experiment.} Boxplot shows performance of each of the compared methods on each of the brain structures in the LPBA40 dataset. Algorithms from left to right in each group: SITReg, cLapIRN, VoxelMorph, SYMNet (original)}
\label{appendix-fig:dice_scores_lpba40}
\end{figure}

Figures \ref{appendix-fig:dice_scores_oasis} and \ref{appendix-fig:dice_scores_lpba40} visualize dice scores for individual anatomical regions for both OASIS and LPBA40 datasets. VoxelMorph and SYMNet perform systematically worse than our method, while cLapIRN and our method perform very similarly on most regions.

Figure \ref{appendix-fig:detailed_deformation_example} visualizes how the deformation is being gradually updated during the multi-resolution architecture.
\begin{figure}
\centering
\includegraphics[width=0.96\textwidth]{figures/detailed_deformation_example.pdf}
\caption{\textbf{Visualization of deformation being gradually updated.} Each $f_{1\to2}^{(k)}(x_1, x_2)$ corresponds to the full deformation after resolution level $k$. The example is from the OASIS experiment.}
\label{appendix-fig:detailed_deformation_example}
\end{figure}
\clearpage
\section{Dataset details}\label{appendix:dataset_details}

We split the OASIS dataset into $255$, $20$ and $139$ images for training, validation, and testing. The split differs from the Learn2Reg challenge since the test set is not available, but sizes correspond to the splits used by \citet{mok2020fast, mok2020large, mok2021conditional}. We used all image pairs for testing and validation, yielding $9591$ test and $190$ validation pairs. For the affinely-aligned OASIS experiment we cropped the images to $144 \times 192 \times 160$ resolution. Images in raw OASIS dataset have resolution $256 \times 256 \times 256$ and we did not crop the images.

We split the LPBA40 into $25$, $5$ and $10$ images for training, validation, and testing. This leaves us with $10$ pairs for validation and $45$ for testing. We cropped the LPBA40 images to $160 \times 192 \times 160$ resolution.

\section{Details on statistical significance}\label{appendix:statistical_significance}

We computed statistical significance of the results comparing the test set predictions of the trained models with each other. We measured the statistical significance using permutation test, and in practice sampled $10000$ permutations. In Figures \ref{table:results_oasis} and \ref{table:results_lpba40} all the improvements denoted with asterisk ($^*$) obtained very small p-value with not a single permutation (out of the $10000$) giving larger mean difference than the one observed.

To establish for certain the relative performance of the methods with respect to the tight metrics, one should train multiple models per method with different random seeds. However, our claim is not that the developed method improves the results with respect to a single tight metric but rather that the overall performance is better by a clear margin (see Section \ref{sec:results}).

\section{Clarifications on symmetry, inverse consistency, and topology preservation}\label{appendix:property_examples}

Here we provide examples of symmetry, inverse consistency and lack of topology preservation to further clarify how the terms are used in the paper.

Since symmetry and inverse consistency are quite similar properties, their exact difference might be unclear. Examples of registration methods that are \textit{inverse consistent by construct but not symmetric} are many deep learning frameworks applying the stationary velocity field \citep{arsigny2006log} approach, e.g,  \citep{dalca2018unsupervised, krebs2018unsupervised, krebs2019learning, mok2020fast}. All of them use a neural network to predict a velocity field for an ordered pair of input images. The final deformation is then produced via Lie algebra exponentiation of the velocity field, that is, by integrating the velocity field over itself over unit time. Details of the exponentiation are not important here but the operation has an interesting property: By negating the velocity field to be exponentiated, the exponentiation results in inverse deformation. Denoting the Lie algebra exponential by $\exp$, and using notation from Section \ref{sec:intro}, we can define such methods as
\begin{equation}
    \begin{cases}
        f_{1\to2}(x_1, x_2) &:= \exp(g(x_1, x_2))\\
        f_{2\to1}(x_2, x_1) &:= \exp(-g(x_1, x_2))
    \end{cases}
\end{equation}
where $g$ is the learned neural network predicting the velocity field. As a result, the methods are inverse consistent since $\exp(g(x_1, x_2)) = \exp(-g(x_1, x_2))^{-1}$ (accuracy is limited by spatial sampling resolution). However, by changing the order of inputs to $g$, there is no guarantee that $g(x_1, x_2) = -g(x_2, x_1)$ and hence such methods are not symmetric by construct.

MICS \citep{estienne2021mics} is an example of a method which is \textit{symmetric by construct but not inverse consistent}. MICS is composed of two components: encoder, say $E$, and decoder, say $D$, both of which are learned. The method can be defined as
\begin{equation}
    \begin{cases}
        f_{1\to2}(x_1, x_2) &:= D(E(x_1, x_2) - E(x_2, x_1))\\
        f_{2\to1}(x_1, x_2) &:= D(E(x_2, x_1) - E(x_1, x_2)).
    \end{cases}
\end{equation}
As a result, the method is symmetric by construct since $f_{1\to2}(x_1, x_2) = f_{2\to1}(x_2, x_1)$ holds exactly. However, there is no architectural guarantee that $f_{1\to2}(x_1, x_2)$ and $f_{2\to1}(x_2, x_1)$ are inverses of each other, and the paper proposes to encourage that using a loss function. In the paper they use such components in multi-steps manner, and as a result the overall architecture is no longer symmetric.

Lack of topology preservation means in practice that the predicted deformation folds on top of itself. An example of such deformation is shown in Figure \ref{appendix-fig:folding_deformation_example}.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{figures/folding_deformation_example.pdf}
\caption{\textbf{Visualization of a 2D deformation which is not topology preserving.} The deformation can be seen folding on top of itself.}
\label{appendix-fig:folding_deformation_example}
\end{figure}


\clearpage

\def\UrlBreaks{\do\/\do-}
\bibliography{references}


\end{document}