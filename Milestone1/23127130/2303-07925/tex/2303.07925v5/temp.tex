



\subsection{Comparison with Numerai Meta Model}

Model performances obtained from the optimised XGBoost Incremental Learning model are compared with the Meta Model, which is an aggregation of the predictions from participants. There are two different methods to measure model performances. The first method is to \textbf{directly} compare the performances of predictions obtained from the incremental learning model to the Meta Model. The second method is to \textbf{indirectly} compare the performance of an \textbf{ensemble} consisting of $50\%$ incremental learning model and $50\%$ Meta Model to the Meta Model. The first method can identify models that are good as standalone models. The second method can identify models that are good within \textbf{ensemble}. These models might not have the best performances on a standalone basis but are still important for the Meta Model as they give orthogonal(uncorrelated) predictive signals to the known ones, which is demonstrated by the improvement of different portfolio metrics within the ensemble. 

Tables \ref{table:XGBEnsemble500}, \ref{table:XGBEnsemble5000}, \ref{table:XGBEnsemble50000}, \ref{table:XGBEnsembleComplexity5000}, \ref{table:XGBEnsembleComplexity5000}, compare the performance of the above XGBoost models trained with different number of boosting rounds with the Meta Model from Era 888 to Era 1050 \footnote{The Numerai Meta Model data starts at Era 888}. All XGBoost models have a higher Mean Corr than the Meta Model. The equal-weighted ensemble of Meta Model and XGBoost Model have a better performance in all metrics (Mean Corr, Sharpe and Calmar), even though the improvement is limited for XGBoost models with more than 5000 boosting rounds. The XGBoost models are both good models on a \textbf{standalone} and \textbf{ensemble} basis. 

%% Do not always need larget models 
XGBoost models with 5000 and 50000 boosting rounds have similar Mean Corr. The largest XGBoost models have a slightly higher Calmar ratio. Whether spending 10 times more computational resources to train larger XGBoost models is economical depends on the utility function of the computational costs of researchers. 

%% Complex Models are better in complmentary in nature 
XGBoost models without early stopping slightly under-performed XGBoost models with early stopping on a standalone basis, but these models can improve the Ensemble model performances more.  A possible reason is due to most participants do not train large XGBoost models without early stopping. Other than constraints in computational resources, the unconscious bias stemming from classical viewpoint on machine learning leads to most participants applied different regularisation techniques in model training. 

%% Alpha Decay 
The Meta Model is determined by the collective behaviour of participants. The improvement of using XGBoost models without early stopping might decay over time as participants started to shift towards over-parameterised models after the publication of this paper. 


\begin{table}[!ht]
    \centering
    \caption{XGBoost models with different retrain periods (500 boosting rounds)}
    \begin{tabular}{|l|l|l|l|}
    \hline
        Method  & Mean Corr  & Sharpe  & Calmar  \\ \hline
        XGBoost Model  & 0.0227  & 1.0274  & 0.2239 \\ \hline
        Meta Model  & 0.0198  & 0.9563  & 0.3345 \\ \hline
        Ensemble  & 0.0235  & 1.0536  & 0.3250 \\ \hline
    \end{tabular}
    \label{table:XGBEnsemble500}
\end{table}


\begin{table}[!ht]
    \centering
    \caption{XGBoost models with different retrain periods (5000 boosting rounds)}
    \begin{tabular}{|l|l|l|l|}
    \hline
        Method  & Mean Corr  & Sharpe  & Calmar  \\ \hline
        XGBoost Model  & 0.0253  & 1.1430  & 0.3795 \\ \hline
        Meta Model  & 0.0198  & 0.9563  & 0.3345 \\ \hline
        Ensemble  & 0.0253  & 1.1500  & 0.3997 \\ \hline
    \end{tabular}
    \label{table:XGBEnsemble5000}
\end{table}



\begin{table}[!ht]
    \centering
    \caption{XGBoost models with different retrain periods (50000 boosting rounds)}
    \begin{tabular}{|l|l|l|l|}
    \hline
        Method  & Mean Corr  & Sharpe  & Calmar  \\ \hline
        XGBoost Model  & 0.0254  & 1.1581  & 0.4264 \\ \hline
        Meta Model  & 0.0198  & 0.9563  & 0.3345 \\ \hline
        Ensemble  & 0.0255  & 1.1741  & 0.4100 \\ \hline
    \end{tabular}
    \label{table:XGBEnsemble50000}
\end{table}



\begin{table}[!ht]
    \centering
    \caption{XGBoost models with different model complexities (5000 boosting rounds)}
    \begin{tabular}{|l|l|l|l|}
    \hline
        Method  & Mean Corr  & Sharpe  & Calmar  \\ \hline
        XGBoost Model  & 0.0248  & 1.1757  & 0.4001 \\ \hline
        Meta Model  & 0.0198  & 0.9563  & 0.3345 \\ \hline
        Ensemble  & 0.0263  & 1.2162  & 0.4994 \\ \hline
    \end{tabular}
    \label{table:XGBEnsembleComplexity5000}
\end{table}


\begin{table}[!ht]
    \centering
    \caption{XGBoost models with different model complexities (50000 boosting rounds)}
    \begin{tabular}{|l|l|l|l|}
    \hline
        Method  & Mean Corr  & Sharpe  & Calmar  \\ \hline
        XGBoost Model  & 0.0247  & 1.1556  & 0.4428 \\ \hline
        Meta Model  & 0.0198  & 0.9563  & 0.3345 \\ \hline
        Ensemble  & 0.0261  & 1.2211  & 0.5068 \\ \hline
    \end{tabular}
    \label{table:XGBEnsemble50000}
\end{table}



%% Is optimisation always useful? 
Unlike the previous work \cite{wong2023dynamic} which demonstrates \textbf{dynamic} learning can improve prediction models based on known statistical rules, applying \textbf{dynamic} learning iteratively on models with variability due to randomness on feature and data sampling cannot generate predictions better than simple averages. The findings are in direct contrary to empirical evidences of model stacking on \textbf{stationary} data, where model stacking are usually considered to have low risk of over-fitting. 

Some design choices, such as to use shallow trees instead of deep trees in a GBDT model will be generally true for different datasets. These design choices can be set with human expertise and do not require hyper-parameter optimisation. For design choices which are reasonably known to hold for models \textbf{globally} regardless of distribution shifts in data, such as the data sampling ratio of decision trees in XGBoost models, hyper-parameter optimisation can be performed using early observations of data and then used in the rest of the incremental learning pipeline. For design choices that are not certain to have \textbf{global} optimum, such as the number of boosting rounds of XGBoost models (model complexity) or the choice of machine learning models, applying \textbf{dynamic} optimisation techniques such as deep incremental learning can learn model parameters/weights that adapt to changes in the data stream. For design choices due to randomness or other hyper-parameters that do not demonstrate any pattern over observed history, a simple average over possible choice is preferred to other statistical/machine-learning-based optimisation methods. 

By considering simple averaging as a degenerate case of non-negative Bayesian regression model, the design choice of whether to apply machine-learning based methods in second layer onward can be formulated as a model/hyper-parameter optimisation problem within the deep incremental learning framework presented in this paper. 

%% Recommendations for non-stationary datasets 
For modelling non-stationary data streams, the philosophy of "less is more" can be used. Bootstrap techniques like data/feature sub-sampling are often as effective as feature Engineering and synthetic data generation methods. Simple rule-based methods for model selection/stacking are more robust compared to ML-based methods under distribution changes. If non-linearity is required, simpler models such as GBDT or random forests are better than deep learning methods. The simple trick of applying equal-weighted ensembles over different model designs will often work as good as sophisticated hyper-parameter optimisation methods. %This viewpoint is supported by recent findings that design spaces for hyper-parameter optimisation are often over-engineered such that random searches can perform well \cite{zimmer-tpami21a}. 



\begin{comment}
    T-test is performed using this calculator: https://www.graphpad.com/quickcalcs/ttest2/
\end{comment}