
\documentclass{article}
\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[a4paper, margin=2cm]{geometry}
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder


% Figures and Tables
\usepackage{graphicx}
\usepackage[most]{tcolorbox}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{longtable}


% Mathematics 
\usepackage{amsmath,amssymb,amsfonts}

\usepackage{textcomp}

\usepackage{verbatim}

\usepackage{xcolor}
\usepackage{url}
\usepackage{xr-hyper}
\usepackage{hyperref}
\usepackage{pdfpages} 
\usepackage{bm}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{array}
\newcolumntype{L}{>{\arraybackslash}m{3cm}}

\usepackage{fullpage}
\usepackage{rotating}
\usepackage{stmaryrd}
\usepackage{proof}


%% Tikz 
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{decorations.pathmorphing} % noisy shapes
\usetikzlibrary{fit}% fitting shapes to coordinates
\usetikzlibrary{backgrounds}	

% Theorem 
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}

% Algorithms 
\usepackage[ruled]{algorithm2e}
\usepackage{algorithmic} %% Used in Chapter 1 


% References 
\usepackage[style=numeric,sorting=none,firstinits=true]{biblatex}
\addbibresource{references.bib}



%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Numerai Paper Series 2}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}

  
%% Title
\title{Robust Incremental learning pipeline for temporal tabular ranking task, case study with Numerai data science tournament
%%%% Cite as
%%%% Update your official citation here when published 
\thanks{\textit{\underline{Citation}}: 
\textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Thomas Wong \\
  Imperial College London \\
  London\\
  \texttt{mw4315@ic.ac.uk} \\
  %% examples of more authors
   \And
  Mauricio Barahona \\
  Imperial College London \\
  London\\
  \texttt{m.barahona@imperial.ac.uk} \\
}


\begin{document}
\maketitle


\begin{abstract}
In this paper, we present a robust incremental learning pipeline for temporal tabular ranking tasks. Using commonly used tabular and time-series prediction models as building blocks, our pipeline can adapt to distributional shifts in data and perform dynamic feature engineering and hyper-parameter selections. A key advantage of our method is that no specialised deep learning architectures or data-dependent feature engineering methods are required. Using the Numerai tournament datasets as an example, we then explore different hypothesis on model complexity and diversity for incremental learning tasks under adverse situations such as regime changes, fat-tailed distributions and low signal-to-noise ratios. We provide an in-depth empirical study of the interaction between model complexity and model performances. To demonstrate the universal approximation property of our pipeline, we demonstrate our pipeline is able to recover most of the performances of the Ensemble model (Meta Model) of the participants in the tournament and over some percentages of explained variance. Our pipeline provides a tool for replicating black-box predictions from other machine learning methods and understanding the uniqueness of newly proposed machine learning methods.  
\end{abstract}


% keywords can be removed
\keywords{Machine Learning, Time-Series Prediction, }

\section{Introduction} 
\label{section:overview}

\paragraph{Overview of incremental temporal tabular ranking task}

The prediction task studied in this paper is an incremental ranking problem where the items to be ranked are provided through a data stream, and the data distribution is non-stationary. The data given is in the format of a temporal tabular dataset. A key challenge of modelling non-stationary data is \textbf{model drifts} in the learning process, which is defined as the drop of out-of-sample performances of prediction models when the models learnt relationships from the training set that significant differs from those in the test set. 


\begin{definition}[Temporal Tabular Datasets]
\label{def:temptable}
A temporal tabular dataset is a collection of matrices $\{ X_i, y_i \}_{1 \leq i \leq T}$ collected over time eras 1 to $T$. Each matrix $X_i$ represents data available at era $i$ with shape $N_i \times M$, where $N_i$ is the number of data samples in era $i$ and $M$ is the number of features describing the samples. $y_i$ are the targets corresponding to the features $X_i$, which can be single dimensional or multi-dimensional. Note that the features definition are fixed throughout the eras, in the sense that the same computational formula is used to compute the features in each week. On the other hand, the number of data samples $N_i$ does not have to be constant across time. For practical purposes, the number of data samples in each era are bounded.
%% Data Lag
Unlike standard online learning problems, where the data arrived can be used to update machine learning models \text{right after} prediction is made, there is a \textbf{fixed} and known time lag for the targets from an era to be known. In this paper, it is defined as \textbf{data embargo}. For example, for a dataset with a data embargo of $5$, at era $X$ the features of era $X$ and would be known and then a prediction of the ranking of items at that time could be made. The targets of era $X$ is only known know at era $X+5$, which can then be used to calculate the quality of predictions according to a suitable chosen metric. 
%% Scoring Metric
In this paper, a variant of \textbf{Spearman's} rank correlation metric, which is simply obtained by the Pearson's correlation on the \textbf{rank transformed} items and the actual rankings (scaled between 0 to 1) is used for model evaluation. 

For many applications, the temporal tabular dataset can grow to \textbf{infinite} size. 
\end{definition}

For general tabular datasets, the features can be in many formats, such as numerical, ordinal or categorical. With suitable pre-processing techniques, features can be transformed into equal-sized or Gaussian binned numerical(ordinal) values. The datasets used in this paper are already standardised and cleaned into bins. While the features are given in discrete bins, researchers might be tempted to use one-hot encoding to create "categorical" features, as they might want to capture the non-linearity effects of features. However, this procedure is not supported by the data creation process for the financial datasets considered \footnote{The datasets considered in this paper consists of factors which capture known economic effects in predicting stock prices, such as momentum, these factors are continuous measures with natural ordering.}. 

There are two approaches in modelling the temporal tabular dataset. First is to apply standard tabular machine learning models as usual, taken into account the temporal order of data during cross-validation. The second is to model the \textbf{derived} time-series defined in \ref{def:derivedts}.


\begin{definition}[Derived Time Series]
\label{def:derivedts}
Time series can be derived from the temporal tabular data with the following procedure. In particular, only transformations that can be applied to each data era \textbf{independently} are considered here. A major limitation of this assumption is that it precludes the use of feature engineering methods such as Auto-Encoders which would required data samples across eras. However this assumption can make sure there is no look-ahead bias in the pipeline as these derived time series would be used to build time-series models. 

The transformation can be defined as dimension reduction transformation applied on the matrix a single slice of tabular features at era $i$ with its targets to a one-dimensional tensor, which formally defined as $f(X,y): (\mathbb{R}^{N_i \times M}, \mathbb{R}^{N_i \times 1}) \mapsto \mathbb{R}^{M})$ where $X$ and $y$ are the features and targets, $N_i$ is the number of observations at era $i$, $M$ is the number of features. Examples of transformation that satisfy the above definition includes obtaining the weights of Ridge regression applied on $(X,y)$ and concatenating the correlations calculated between each feature in $X$ with $y$ independently. This procedure is defined as deriving the \textbf{feature performances} of the temporal tabular data problem. 
\end{definition} 


%% Transmutation between data 
Any machine learning pipeline can be considered as a sequence of transformations between different tensors. By focusing on data in the format of tabular and time-series only, 4 different basic transformations are defined which underpins the incremental learning pipeline introduced below.

\begin{enumerate}
    \item Transformation from tabular data to tabular 
    \begin{itemize}
        \item Standard tabular machine learning models which transforms the given feature target pair $(X,y)$ into $y'$ where the first(data) dimension of $X,y,y'$ are equal dimensions and $y$ and $y'$ matches all dimensions. The transformations is performed \textbf{point-wise}, where at inference each item can be predicted independently. There are many examples for this class of machine learning models, including (and not limited to) gradient boosting decision trees, multi-layer perceptron networks. 
        \item List-wise tabular machine learning models which do the above transformation but on the \textbf{whole} list at a time. For a temporal tabular dataset, this means the item rankings are predicted \textbf{all at once}. Examples include various list-wise models for learn-to-rank problems \cite{li2020learning}. 
    \end{itemize}
    \item Transformation from tabular data to time-series
    \begin{itemize}
        \item Deriving time series using the procedure defined in \ref{def:derivedts}. For the purpose of temporal ranking problem, the time-series output cannot be used directly as the prediction of the ranking tasks and therefore this transformation is not that last step of the pipeline
    \end{itemize}   
    \item Transformation from time-series to tabular data 
    \begin{itemize}
        \item Feature Engineering methods for time-series data such as Signature transforms \cite{Terry22} and Random Fourier transforms \cite{Sutherland15} which transforms a slice of time-series into a single dimensional tensor which captures the characteristics of the time-series. 
    \end{itemize}       
    \item Transformation from time-series to time-series 
    \begin{itemize}
        \item Sequence models which are common in deep learning, such as LSTM \cite{HochSchm97} and Transformers \cite{Bryan19}. These models are not the focus of this paper as our task is to obtain ranking predictions on the \textbf{tabular} data rather than predicting the feature performances. 
    \end{itemize}
\end{enumerate}

\begin{comment}
    Add a picture showing the 4 transformations on a square 
\end{comment}

The above transformations presents multiple ways to transform data formats within our machine learning pipeline. In particular, any transformations starting with tabular features and ending with tabular targets can be used as predictions.

Apart from standard tabular models, \textbf{factor-timing} models can be used. The features are first transformed into a time-series, capturing the history of feature performances and then use the ranking of predicted feature performances to formulate a factor-timing portfolio. The time-series model for predicting feature performances could be any of the above mentioned methods. 


\paragraph{How incremental learning is different from traditional machine learning} 

%% Incremental Learning 1: Regeneration of model parameters, the same doctor but in different forms 
The incremental nature of the prediction problem allows us to challenge the traditional assumptions of machine learning problem, in particular, a (single-pass) cross validation which splits the data into \textbf{fixed} training,validation and test periods is not the most suitable framework to deal with the stream nature of data. Instead, an incremental learning procedure should be used to retrain and/or update model parameters. Under this procedure, a model is represented by a continuous stream of parameters instead of a single set of parameters. The procedure also add new hyper-parameters, training size and retrain frequency to the machine learning model. In practice, these hyper-parameters are more often selected based on computational resources available rather than optimised. The size of memory will limit the maximum training size and the amount of GPU or other processing units available will limit how often the models are retrained/updated. 

% For example, consider a dataset with 20 years of history. At Year 11, we can first use data from the first 10 years history of data to build a model and then we retrain a model once per year. So by the end of Year 20, we have 10 different models trained with overlapping training data, where each training data consists of 10 years of history that is known at the point of model training. 

When retrain frequency is greater than 1, there is a extra degree of freedom in when to start to incremental learning procedure. For example, if models are retrained every 10-th era, then starting the incremental learning procedure at Era 1, Era 2, ... to Era 10 would gives 10 different training procedures using different training sets. This choice could have a non negligible impact on prediction performances \cite{hoffstein2020rebalance} for non-stationary datasets. 


%% Incremental Learning 2: perpetual prediction machine 
The clear distinction between features, targets and predictions are also blurred in the incremental learning setting, as predictions from each model can be used as additional features in building other models. New targets can also be created by subtracting against the predictions made. This suggests model training should be considered as a \textbf{multi-step} problem instead of a \text{single-step} problem. 

As an example, consider an incremental learning problem with a data lag of 1, then in figure \ref{fig:gantt},  three models are trained under the incremental learning framework. The training period of each model is fixed to 4. At Era 6, information up to Week 4 (where both the features and targets are known) are used to train Model 1 and then obtain predictions from Era 6 on-wards. At Era 11, information up to Week 9 are used for model training. Features between Week 6 and Week 9 are combined with predictions from Model 1 to  train a new model (Model 2). Similarly, at Era 16,  Model 3 is trained using data from Era 11 to Era 14, which consists of the original given features, predictions from Model 1 and Model 2. 


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figure/chapter3/gantt.png}
    \caption{Example of reusing model predictions in an incremental learning pipeline}
    \label{fig:gantt}
\end{figure}

There are various benefits in reusing model predictions. 

Firstly, it provides a hierarchical structure of models, which each model can be interpreted as an improvement/adaption of the previous ones to distributional shifts in data and the prediction quality of each model can be inspected independently. 

Moreover, reusing model predictions introduced a feedback learning loop where model predictions corrects itself in an incremental manner. This can be considered as a primitive example of reinforcement learning which assumes the machine learning model itself has made zero impact on the environment. 

%Finally, it allows machine learning models to process an infinite stream of data with a finite memory size. By limiting to train each model using the original features and predictions from (a fixed number of) previous models \textbf{only} for a training set with fixed window size, the size of training set is capped as both the number of features and observations are bounded. Information from previous eras can be passed indirectly to the latest machine learning models. 


%% Incremental Learning 3: Parallel universes of model representations
The above incremental learning pipeline can be applied to not just a \textbf{single} machine learning model, but a \textbf{collection} of machine learning models in parallel, which is defined as \textbf{layers} of models. The notion is borrowed from MLP, where each node within a layer is now an independently trained model. 

A major difference between our incremental learning pipeline from MLP is that training is done in a single forward pass rather than by back propagation. This allows us to train very complicated pipelines even with little resources, as there is no need to put the whole model in (distributed) memory to pass gradients between layers. As each model within a layer can be trained independently, training models \textbf{within} a layer becomes an embarrassingly parallel task when multiple GPUs can be used.  The program code of the pipeline are also easier to maintain as there is no need to use specialised software packages to distribute data between GPUs.

Recent work in deep learning research suggests back-propagation is not strictly necessary for model training \cite{Hinton22}, Deep Regression Ensemble (DRE) \cite{Kelly22deep} is an example where back propagation is not used to train a deep network. The model is built by training multiple layers of ridge regression models with random feature projections. The model can be considered as a multi-layer perceptron network where some layers have frozen weights. The new incremental pipeline presented is significantly \textbf{different} from DRE as no restrictions are made on the machine learning methods employed.


%% give me a pivot I can leverage the earth
\subsection{Practical considerations of incremental learning pipeline} 
Unlike standard machine learning problems where model training is done offline, where memory and computational time is usually not a major consideration in model design, incremental learning pipelines would be limited by both the memory and computational time during inference and online retrain/update of model parameters. The exact requirements varied between problems and therefore it is not possible to offer an one-size-fit-all solution.

All models in this paper are trained on a single CUDA-enabled GPU of 10GB memory for model training. While this assumption precludes the use of very advanced deep learning methods, incremental learning models(pipelines) can be built for any model complexity/expressiveness using commonly used tabular and time-series models as building blocks. The pipeline can be designed to process an infinite data stream of temporal tabular data effectively without ever growing memory consumption, by caching only the latest values in the incremental learning process. In other words, an arbitrarily complex model using a \textbf{fixed} amount of computational resources can be built given sufficient amount of computational time. 



\section{Machine Learning Models for Time Series Datasets} 
\label{section:ML-TS}

In the following, a simple definition of a time-series model for one-step ahead predictions is introduced.

% Extract features 
\paragraph{Multi-variate time-series}
A multi-variate time-series $X$ of $T$ steps and $N$ channels can be represented as $X = (\bm{x}_1, \bm{x}_2, \dots, \bm{x}_i, \dots, \bm{x}_T) $, with $1 \leq i \leq T$ and each vector $\bm{x}_i \in \mathbb{R}^N$ represents the values of the $N$ channels at time $i$. The number of channels of the time series is assumed to be fixed throughout time with regular and synchronous sampling, i.e. the values in each vector from multiple channels arrive at the same time at a fixed frequency. 

\begin{definition}[Time Series Model] 
Given a time series $X_T \in \mathbb{R^{T\times M}} $ where $T$ is size of time dimension and $M$ is the number of features. A (one-step) ahead time-series model is a function $f: \mathbb{R^{T\times M}} \mapsto \mathbb{R}^M$. It can be interpreted also as a dimensionality reduction method across the time dimension. 
\end{definition}

In practice, the function $f$ is often learn by training statistical/machine learning models using future values of $X_T$, which is obtained by shifting the values of $X_T$ across the time-dimension. 


\subsection{Factor Timing Models}

In the following the \textbf{time series} prediction task considered above is connected to the \textbf{tabular} ranking task. In particular, factor-timing models are created based on predicted rankings of features from the time series models as shown in Algorithm \ref{alg:factor-timing}. The raw predicted values from the time-series models are converted into normalised rankings which can be used as weights of a linear factor-timing model. Within an incremental learning pipeline, a new factor-timing model is trained at each time-step using the latest historical values of the feature importance time-series. 
 
\begin{algorithm}[hbt!]
\caption{Factor Timing model }\label{alg:factor-timing}
\KwIn{At prediction era $t$, predicted values from time series model $\hat{y}_t \in \mathbb{R}^d$, temporal tabular dataset $X_t \in \mathbb{R}^{N_t \times d}$ where $d$ is the number of features}
\KwOut{factor timing model $\hat{z}_t \in \mathbb{R}^{N_t}$ }
Calculate normalised ranking of features $\hat{r}_t$ from predictions of the time series model 
\begin{equation*}
    \hat{r}_t = \text{rank}(\hat{y}_t) - 0.5
\end{equation*}
where rank is the function which calculates the percentile rank of a values within a vector.  $\hat{r}_t$ are ranged between $-0.5$ and $0.5$ \\
Apply truncation to the normalised ranking of features if needed, given upper bound $u$ and lower bound $l$ on the normalised rankings, $-0.5 < l < u <0.5$
\begin{equation*}
    r_t = \max(\min(\hat{r}_t,u),l)
\end{equation*} 
Calculate linear factor-timing predictions $\hat{z}_t = X_t r_t $ 
\end{algorithm}


\subsection{Statistical Rules}
\label{section:stats}

Simple statistical rules can be applied be on each feature time-series \textbf{independently} to summarise the history of feature performance. 

\begin{definition}[Cumulative Mean/Variance]
Given an univariate time series $X = (X_1,X_2,\dots,X_t,\dots)$, the cumulative mean of the time-series at time $t$ is defined as 
\begin{equation*}
    \bar{X}_t = \frac{1}{t} \sum_{i=1}^t X_i
\end{equation*}
Similarly, cumulative variance of the time-series at time $t$ is defined as 
\begin{equation*}
    \sigma_{X,t} = \frac{1}{t} \sum_{i=1}^t (X_i - \bar{X}_t)^2 
\end{equation*}
\end{definition}

\begin{definition}[Cumulative Sharpe(Information) Ratio]
The cumulative Sharpe ratio is defined as the ratio of cumulative mean and cumulative variance. 
  \begin{equation*}
    S_t = \frac{\bar{X}_t}{\sigma_{X,t}}
\end{equation*}  
\end{definition}

Efficient algorithms \cite{WelfordB.P.1962NoaM} can be used to calculate above quantities for the whole time-series in $\mathcal{O}(N)$ time where $N$ is the length of the time-series. The algorithm also only requires constant memory so that it can be used to process data streams that never ends. 

In this paper, factor-timing models formed by the above two statistical rules (Cumulative Mean and Cumulative Sharpe) are used as benchmark against other time-series prediction models. 

\begin{comment}
    There are many other valid statsitical rules that can proposed to summarise time-series. Indeed, chart(technical) analysis that has been popular in finance can be also interpreted as a statistical rule under our framework. The biggest limitation of using the statistical rules is that they have to manually crafted and there is no solid mathematical theories backing the use of those. Therefore, we only pick two simple examples to illustrate the idea. 
\end{comment}



\subsection{Feature Engineering Models}
\label{section:feature-eng-multi}

Feature engineering methods can be applied to multi-variate time series so that tabular features are obtained.  

\paragraph{Feature extraction}
Feature extraction methods are defined as functions that map the two-dimensional time-series $X \in \mathbb{R}^{T \times N}$ to a one-dimensional feature space $f(X) \in \mathbb{R}^K$ where $K$ is the number of features. Feature extraction methods reduce the dimension and noise in time-series data. With feature extraction methods, traditional machine learning models such as gradient boosting decision trees can be used without relying on advanced neural network architectures such as Recurrent Neural Networks (RNN), Long-Short-Term-Memory (LSTM) Networks or Transformers \cite{Bryan19}. 

%% Look-back windows
\paragraph{Look-back windows}
For time series which can potentially grow with infinite size (such as price data in finance), a look-back window is used to restrict the data size when calculating features from time series. To avoid look-ahead bias, features that represent the state of time-series at time $i$ can only be calculated using values obtained up to time $i$, which is $(\bm{x}_1, \bm{x}_2, \dots, \bm{x}_i)$. In most use cases, data collected more recently often have more importance than data collected from a more distant past. Therefore, analysis is often restricted to use the most recent $k$ data points only, which are $(\bm{x}_{i-k}, \bm{x}_{i+1-k}, \dots, \bm{x}_i)$. This represents the state of the time series at time $i$ with a look-back window of size $k$. Feature extraction methods are applied on data within the look-back window only. In practice, multiple look-back windows are used to extract features corresponding to short-term and long-term price trends. At each time $i$, features extracted with different look-back windows are concatenated to represent the state of the time series.  

To model multivariate time series effectively, better methods which can be applied on multiple time-series in parallel are needed. In this study, both deterministic and random transformations are considered to create tabular features for the prediction task. 

% Signatures
\subsubsection{Signature Transforms} 
Signature transforms \cite{Lyons07, Chevyrev16, Terry22} are \textbf{deterministic} transformations based on rough path theory which can be used to extract features from multi-variate time series. Signature transforms are applied on continuous paths. A path $X$ is defined as a continuous function from a finite interval $[a,b]$ to $\mathbb{R}^d$ with $d$ the dimension of the path. $X$ can be parameterised in coordinate form as $X_t = (X_t^1,X_t^2,\dots,X_t^d)$ with each $X_t^i$ being a single dimensional path. 

% Iterated Integrals 
For each index $ 1 \leq i \leq d$, the increment of $i$-th coordinate of path at time $t \in [a,b]$, $S(X)_{a,t}^i$, is defined as 
\begin{equation*}
    S(X)_{a,t}^i = \int_{a<s<t} \mathrm{d}X_s^i = X_t^i - X_a^i
\end{equation*}
As $S(X)_{a,\cdot}^i$ is also a real-valued path, the integrals can be calculated iteratively. A $k$-fold iterated integral of $X$ along the indices $i_1,\dots,i_k$ is defined as 
\begin{equation*}
    S(X)_{a,t}^{i_1,\dots,i_k} = \int_{a<t_k<t} \dots \int_{a<t_1<t_2}   \mathrm{d}X_{t_1}^{i_1}  \dots \mathrm{d}X_{t_k}^{i_k} 
\end{equation*}

% Definition of Signature 
The Signature of a path $X: [a,b] \mapsto \mathbb{R}^d$, denoted by $S(X)_{a,b}$, is defined as the infinite series of all iterated integrals of $X$, which can be represented as follows 
\begin{align*}
    S(X)_{a,b} &= (1, S(X)_{a,b}^1, \dots, S(X)_{a,b}^d,  S(X)_{a,b}^{1,1}, \dots ) \\
                &=  \bigoplus_{n=1}^{\infty} S(X)_{a,b}^n
\end{align*}

An alternative definition of signature as the response of an exponential nonlinear system is given in \cite{Terry22}. 

% Log Signature 
Log Signature can be computed by taking the logarithm on the formal power series of Signature. No information is lost as it is possible to recover the (original) Signature from Log Signature by taking the exponential \cite{Chevyrev16,Terry22}. Log Signature provides a more compact representation of the time series than Signature. 
\begin{equation*}
    log S(X)_{a,b} =  \bigoplus_{n=1}^{\infty}  \frac{(-1)^{(n-1)}}{n} S(X)_{a,b}^{\bigotimes n} 
\end{equation*}


%%% Theoretical properties of signatures 
%%% Multiplicative Functional 
%%% Universal Property of Signature in predictions (flexible) 
Signatures can be computed efficiently using the Python package signatory \cite{kidger2021signatory}. The signature is a multiplicative functional in which Chen's identity holds. This allows quick computation of signatures on overlapping slices in a path. Signatures provide a unique representation of a path which is invariant under reparameterisation \cite{Chevyrev16, Terry22}. Rough Path Theory suggests the signature of a path is a good candidate set of linear functionals which captures the aspects of the data necessary for forecasting. In particular, continuous functions of paths are approximately linear on signatures \cite{pmlr-v130-lemercier21a}. This can be considered as a version of universal approximation theorem \cite{cybenko1989approximation} for signature transforms. 


%% Interpretation of Signatures
% Level 1 Signature corresponds to the difference of two series (tail-head). When log price series are given as input, it corresponds to log return 
% Basic Statistical features can be recovered from signatures 

\paragraph{Limitations for Signature in High Dimensional Datasets}
The number of signatures and log-signatures increases exponentially with the number of channels. For time series with a large number of channels, random sampling can be applied to select a small number ($5 < N < 20$) of time-series with replacement from the original time series which signature transforms are applied on. The random sampling can be repeated for a given number of times to generate representative features of the whole multivariate time series. Similar ideas are considered in \cite{James20}, in which random projections on the high dimensional time series is used to reduce dimensionality before applying signature transforms.  

%% Lookback window
Let $\tilde{X}$ be a multivariate time series with $T$ time-steps and $d$ dimensional features, denote $\tilde{X}_s \in \mathbb{R}^d$ be the observation of the time series at timestep $s$. Procedure \ref{alg:lookback} cab be used to obtain paths, which are slices of time-series with different lookback windows. Random Signature transforms \ref{alg:randomsig} can then be used to compute the signature of the path, which summarises the information of the time series. 

\begin{algorithm}[hbt!]
\caption{Lookback Window Slicing}\label{alg:lookback}
\KwIn{time series $\tilde{X} \in \mathbb{R}^{T \times d}$, lookback $\delta$}
\KwOut{paths $X_t \in \mathbb{R}^{t \times d}$}
\For{$1 \leq t \leq T$}{
    Set start of slice $s_1 = \max(1, t - \delta)$ \;
    Set end of slice $s_2 = t$ \;
    $X_t = (\tilde{X}_{s_1},\tilde{X}_{s_1+1}, \dots, \tilde{X}_{s_2}) $ \;
}
\end{algorithm}


%% Random Signature Transform Algorithms 
\begin{algorithm}[hbt!]
\caption{Random Signature Transform}\label{alg:randomsig}
\KwIn{path $X_t \in \mathbb{R}^{t \times d}$, level of signature $L$, number of channels $C$, number of feature sets $p$,} 
where $d > C$ \;
\KwOut{log signatures $s_t \in \mathbb{R}^{pN}$ }
Define $N= \text{Number of Log Signatures of a path with } C \text{ channels up to level }  L $ \;
\For{$1 \leq i \leq p$}{
    Sample with replacement $C$ Columns from $X_t$, defined as $\tilde{X}^i_t$ \;
    Compute the Log Signatures $s_t^i \in \mathbb{R}^N$ of $\tilde{X}^i_t$ \;
}
Combine all log signatures $s_t = (s_t^1, \dots, s_t^p)$ 
\end{algorithm}


\subsubsection{Random Fourier Transforms} 
Random Fourier Transforms is used in \cite{kelly2022virtue} to model return of financial price time series.
A price time series $P_t \in \mathbb{R}^T$ is first transformed into a return series $X_t \in \mathbb{R}^{T \times d} $ by taking the percentage change of price at different lookback intervals. Let $\delta_1, \dots, \delta_d \in \mathbb{N}$ be a given a list of lookback intervals, 
\begin{equation*}
    X_{t,\delta_i} = \frac{P_t - P_{t-\delta_i}}{P_{t-\delta_i}}
\end{equation*}
where $1 \leq t \leq T$ and $1 \leq i \leq d$ 

Random Fourier Transforms is then applied on the return series at each time step as in Algorithm \ref{alg:rft}. The key idea is to approximate a mixture model of Gaussian kernels with trigonometric functions \cite{Sutherland15}.  

%% Random Fourier Transform Algorithms 
\begin{algorithm}[hbt!]
\caption{Random Fourier Transform \cite{kelly2022virtue}}\label{alg:rft}
\KwIn{signal vector $x_t \in \mathbb{R}^d$, number of features sets $p$, }
\KwOut{transformed vector $s_t \in \mathbb{R}^{14p}$ }
\For{$1 \leq i \leq p$}{
    Sample $w_i \sim \mathcal{N}(0, I_{d\times d})$ \;
    Set grid $(\gamma_i)_{i=1}^14 = (0.1, 0.5, 1, 2, 4, 8, 16, 0.1, 0.5, 1, 2, 4, 8, 16)$ \;
    \For{$1 \leq j \leq 7$}{
        Set $ s_{t,14i+j} = \frac{1}{\sqrt{7p}} \sin(\gamma_j w_i^T x_t)$
    }
    \For{$8 \leq j \leq 14$}{
        Set $ s_{t,14i+j} = \frac{1}{\sqrt{7p}} \cos(\gamma_j w_i^T x_t)$
    }
}
\end{algorithm}


\subsubsection{Ridge Regression}
Applying the above feature engineering methods, tabular features that have sizes greater than the number of observations are created. This results in an over-parameterised model where regularisation is required as there are multiple models that can perfectly fit the data. In this paper, ridge regression, which is a standard choice of linear regression models with L2-regularisation is used. Ridge regression has closed form solutions and there are efficient implementation which can calculate ridge regression models with different L2-regularisation on the same dataset. 


\subsubsection{Selection of Look-back Window} 
For all the feature engineering methods mentioned above, a look-back window needs to be selected. Choosing different sizes of look-back window corresponds to extracting the dynamics of feature performances at different time-scales in the data stream. While in theory the lookback can be set to "infinite", which means using all the available history of the time series at the time of prediction, this is rarely done in practical implementation as it would requiring an arbitrage large amount of memory to store the data. By using a fixed size lookback window, the amount of memory required can be limited for the incremental learning prediction pipeline during deployment. 


\subsection{Deep Learning Models}

Transformer \cite{Transformer17}, which is the state-of-the-art deep learning based model for sequence modelling is used as benchmark against other factor-timing models. The implementation in DARTS \cite{DARTS} is used. 

Standard parameters for the architectures of different deep learning models given in DARTS are used. The standard number of training epochs is fixed to a maximum of 100 epochs with early stopping applied on the \textbf{training set}, where training process is stop if Mean-Squared Error loss does not improve by $0.001$ for 20 rounds. A validation data set is not used. 

Similar to the feature engineering models above, a lookback window (which corresponds to the input sequence dimension) needs to be selected for each model along with the training size (which can be fixed or using all the valid feature/target pairs created since the start of time-series). Again, using a fixed size of lookback and training sizes can limit the amount of computational resources required for model training/inference and is therefore the recommended approach. 

\paragraph{Transformer}
A summary on how to use transformer architecture in time-series modelling to be added. 


\section{Machine Learning Models for Tabular Datasets} 
\label{section:tabular-inc}

\subsection{Gradient Boosting Models} 

Gradient Boosting Decision Trees (GBDT) is a standard tool for tabular data modelling. There are many efficient implementations, for example XGBoost \cite{XGBoost}, LightGBM \cite{LightGBM} and CatBoost \cite{CatBoost}. While in theory gradient boosting can be applied to any kind of base learners, such as shallow networks, there are no Python implementations that are as well-maintained as the three packages just mentioned. In this paper XGBoost is used as it offers good GPU acceleration and flexible feature sub-sampling at node level. 


\paragraph{Gradient Boosting Algorithm} 

Gradient Boosting is generic algorithm for combining base learners in a sequential manner to obtain better predictions. A common choice of base learners would be decision trees. The aim of (gradient) boosting is to reduce \textbf{bias} of the ensemble learner. The aim is different to that of bagging, which fits multiple independent base learners at the same time, and the ensemble learner has a lower \textbf{variance} than each base learners. For practical implementations, bagging and boosting can often be used together. For example, in XGBoost, multiple trees (forests) can be fit in each boosting round. However empirical experience (and many textbooks \cite{pml1Book}) suggests boosting works better. Therefore, only a single tree is trained in each boosting round. Algorithm \ref{alg:gradient-boosting} shows the pseudo-code for the general gradient boosting for a regression problem. 

\begin{algorithm}[htb!]
Given $N$ data samples $(\mathbf{x_i}, y_i), 1 
\leq i \leq N$ with the aim to find an increasing better estimate $\hat{f}(\mathbf{x})$ of the minimising function $f(x)$ which minimise the loss $\mathcal{L}(f)$ between targets and predicted values. $\mathcal{L}(f) = \sum_i l(y_i,f(\mathbf{x_i})) $ where $l$ is a given loss function such as mean square losses for regression problems. Function $f$ is restricted to the class of additive models $f(\mathbf{x}) = \sum_{k=1}^K w_k h(\mathbf{x},	\bm{\alpha_k})$ where $h(\cdot,\bm{\alpha})$ is a weak learner with parameters $\bm{\alpha}$ and $w_k$ are the weights. \\

Initialise  $f_0(\mathbf{x}) = \arg \min_{\bm{\alpha_0}} \sum_{i=1}^N l(y_i, h(\mathbf{x_i}, 	\bm{\alpha_0}))$  \\

\For{k = 1 : K}{
    Compute the gradient residual using $g_{ik} = - \left [ \frac{\partial l(y_i, f_{k-1}(\mathbf{x_i})) }{\partial f_{k-1}(\mathbf{x_i}) } \right ]  $ \\
    Use the weak learner to compute $\bm{\alpha_k}$ which minimises $ \sum_{i=1}^N (g_{ik} - h(\mathbf{x_i},	\bm{\alpha_k}))^2 $  \\
    Update with learning rate $\lambda$ $f_k(\mathbf{x}) = f_{k-1}(\mathbf{x}) + \lambda h(\mathbf{x}, \bm{\alpha_k}) $ \\ 
}
\textbf{Return} $f(\mathbf{x}) = f_K(\mathbf{x})$ \\
\caption{Gradient boosting algorithm \cite{FriedmanJeromeH.2001GfaA, B_hlmann_2007} }
\label{alg:gradient-boosting}
\end{algorithm} 


\paragraph{XGBoost Implementation}

XGBoost \cite{XGBoost} modifies the above "standard" gradient boosting algorithms with approximation algorithms in split finding. Instead of finding the best(exact) split by searching over all possible split points on all the features, a histogram is constructed where splitting is based on percentiles of features. XGBoost supports two different grow policies for the leaf nodes, where nodes closest to the root are split (depth-wise) or the nodes with highest change of loss function is split (loss-guide). The default tree growing policy is depth-wise and performs better in most of benchmark studies. XGBoost also supports L1 and L2 regularisation of model weights. Other standard model regularisation techniques such as limiting the maximum depth of trees and the minimum number of data samples in a leaf node are also supported. 


% Train 1, get N for free for GBDT models 
\paragraph{Model Snapshots}

Unlike neural networks, it is not strictly necessary to use early stopping in model training for GBDT models. For GBDT models, it is easy to extract model snapshots, defined as the model parameters captured at different part of the training process. This can be done without any additional memory costs. (XGBoost and CatBoost supports the use part of the trees during inference by giving the start and end iteration.). 

For example, for any GBDT model, model snapshots from a single trained model can be obtained as follows. Always starting with the first tree, the number of trees to be used (the ending tree) are set to be $10\%,20\%,\dots,100\%$ of the number of boosting rounds. This trivially gives 10 different GBDT models representing different model complexities while using the training time of a \textbf{single} model only. Therefore, instead of using hyper-parameter optimisation to select the number of trees, simply set the number of boosting rounds to a large number and then set a small learning rate, so that learning process does not converge too early. 

\paragraph{Feature Neutralisation} 

Feature neutralisation \cite{ThomasW23} can be used to remove linear effects from the machine learning model. Let $y \in \mathbb{R}^n$ be the output from a machine learning model, and $X \in \mathbb{R}^{n \times m}$ be the input matrix of features. For a given neutralisation strength $\beta, 0 \leq \beta \leq 1$, the neutralised predicted ranking $\hat{y}$ is calculated as $ \hat{y} = y - \beta \, X X^\dagger y$, where $X^\dagger$ is the pseudo-inverse of $X$. 

Feature neutralisation is applied \textbf{after} the predictions are obtained from the trees. The calculation of gradient (and hessian) updates in the GBDT is independent of the feature neutralisation applied. 

\begin{comment}
    \paragraph{Incremental Updates} 
    GBDT models supports incremental update of model parameters by different. However when it is easy to train a new model from scratch, then within the incremental learning framework, it is always better to reuse 
\end{comment}


\subsection{Deep Learning Models} 

In this paper, deep learning is defined as any machine learning model that make use of \textbf{multiple} layers of artificial neural networks in training. PyTorch \cite{Paszke_PyTorch_An_Imperative_2019} and Tensorflow \cite{Abadi_TensorFlow_Large-scale_machine_2015} are the most commonly used Python packages for training deep learning models. Back-propagation is the standard but not the only method to learn the model parameters. 


\begin{comment}
    There is no need to exactly replicate the eval func as loss func for training. Training can be always be based on approximation of (in fact you do not even need the exact gradient/hessians to apply Newton method and it does not hurt much during optimisation). If there is an easy/smooth alternative that is easier to implement, use it instead! 
\end{comment}


\paragraph{Feature Neutralisation and Loss Function} 

Pearson correlation calculated on the whole \textbf{era} of target and predictions is used as the loss function at each training epoch. Feature neutralisation is applied from the outputs of network architecture. The neutralised predictions are further standardised to zero mean and unit norm. The negative of Pearson correlation of the standardised predictions and targets is then used as the loss function to train the network parameters. In particular, the feature neutralisation is an integrated part of the training process, as the gradient updates and model parameters are based on the neutralised predictions instead of the raw predictions.  

%For Spearman correlation, similar procedures is used except we calculating correlation based on soft ranking of predictions and targets \cite{blondel2020fast}. 


\paragraph{Training process}

PyTorch Lightning \cite{Falcon_PyTorch_Lightning_2019} is used to build neural network models as it supports modular design and allows rapid prototyping. 

The learning rate of neural networks is found by the Learning Rate Finder over a parameter grid of $(1e-3,0.1)$. Early Stopping is applied based on the validation set based on a given number of rounds (patience). The batch size of neural network is set to be the size of each era. The Adam optimiser in PyTorch with the default settings for the learning rate schedule is used. L2-regularisation on the model weights are also applied. Gradient clipping is also be applied to prevent the gradient explosion problem for correlation-based loss functions. 


\paragraph{Architecture}

The network architecture consists of two parts, firstly an "Feature Engineering" part which consists of multiple feature engineering blocks and then the "funnel" part which gradually refine the features to output. 

Each feature engineering block has an Auto-Encoder like structure, where the number of features are unchanged after passing each block. Setting a neuron scale ratio less than 1 corresponds to the case of introducing bottleneck to the network architecture so to learn a latent representation of data in a lower dimensional space. Setting a neuron scale ratio greater than 1 corresponds to the case of introducing random combinations of features which are then refined during the model training process. 

Funnel architecture, as used in \cite{Zimmer_Auto-PyTorch_Tabular_Multi-Fidelity_2021} is an effective way to define the neuron sizes in a network for different input feature sizes. Algorithm \ref{alg:funnel} shows how to create the funnel architecture. 

Each network layer is followed by a ReLU activation layer and dropout layer where $10\%$ of weights are randomly zeroed. %Batch Normalisation is not used. 


\begin{algorithm}[hbt!]
\caption{Feature Engineering network architecture}
\label{alg:encoding}
\KwIn{Input feature size $M$, Number of encoding layers $L$, neuron scale ratio $r$}
\KwOut{Sequential Feature Engineering Network Architecture}
\For{$1 \leq l \leq L$}{
    Encoding Layer $l$: Linear layer $(M,M*r)$ 
    Decoding Layer $l$: Linear layer $(M*r,M)$ 
}
\end{algorithm}

\begin{algorithm}[hbt!]
\caption{Funnel network architecture}
\label{alg:funnel}
\KwIn{Input feature size $M$, Output feature size $K$, Number of intermediate layers $L$, neuron scale ratio $r$}
\KwOut{Sequential Funnel Network Architecture}
Input Layer: Linear layer (M, $M*r$) \\
\For{$1 \leq l \leq L$}{
    Intermediate Layer $l$: Linear layer $(M*r^{l}, M*r^{l+1})$ 
}
Output Layer: Linear layer $(M*r^{L+1},K)$ \\
\end{algorithm}



\section{Incremental learning pipeline}

An incremental learning pipeline can be built using the tabular and factor-timing models as components. Algorithm \ref{alg:incremental-ml}, outlines the overall structure of the incremental learning pipeline, which is a multi-layer model with regular updates over time. 

Each factor-timing and tabular model is trained in an incremental manner as described in Section \ref{section:overview}, which model parameters are updated as new data arrives. 

\begin{algorithm}[hbt!]
\caption{Incremental Learning Pipeline}
\label{alg:incremental-ml}

\KwIn{Temporal Tabular Dataset $\{ X_i, y_i \}_{1 \leq i \leq T}$, number of layers $L$, the number of models within each layer $(K_1, \dots, K_L)$, training size $a$, data embargo $b$, learning rate $\eta$}
\KwOut{Layered Predictions from each layer $1 \leq l \leq L$, $ \hat{y}_j^l $ where $j$ consists of the eras after the start of model predictions given below}

Build the feature importance time-series using the given features and targets $\{ X_i, y_i \}$ for each era \\
\For{$1 \leq l \leq L$}{
    Calculate the start of model predictions for layer $l$ as $1+l(a+b)$ \\ 
    Prepare Features $\{ X_j^l \}$ where $ (l-1)(a+b) \le j \leq (l-1)(a+b) + a $ where $X_j^l$ consists of the given temporal tabular datasets $X_j$ and predictions from previous layers. The exact composition can be changed for each problem \\
    Calculate the average predictions $\hat{y}_i$ from the previous layer (if there is one), otherwise set $\hat{y}_i=0$ \\
    Update target with gradient boosting formula: $y_i = y_i - \eta \hat{y}_i$ \\ 
    \For{$1 \leq k \leq K_L$}{
    Perform Data and Feature Sub-sampling \\
    Train component models using features $\{ X_j^l, y_j \}$ \\
    }
    Build the feature importance time-series for the next layer using predictions from the models trained in the current layer starting at era $1+l(a+b)$ 
}
\end{algorithm}

%% As Above, So Below 
\paragraph{Self-Similarity nature of pipeline}

The incremental learning pipeline demonstrates self-similarity at various \textbf{scales}. In particular, the overall pipeline shared similar structure with each of its components. 

The incremental learning pipeline can be interpreted as a gradient boosted model with $\sum_{i=1}^L K_i$ base learners trained with $L$ boosting rounds, where $K_i$ base learners are trained in the $i$-th boosting round. Ideas from Bagging and Boosting are integrated within the pipeline. Each layer consists of multiple models trained in parallel as in bagging, so that variance is reduced by combining predictions from different models within a layer. Boosting is achieved by adjusting the target based on predictions from previous layers for training in the following layers if needed. Therefore, if each building block is set to be a GBDT model, the overall pipeline would then apply gradient boosting at both pipeline and component level.

The incremental learning pipeline can also be interpreted as neural network model where each node is now a machine learning model instead of a parameter. As predictions from previous layers can be used as features for the following layers, this simulates the residual connections in some neural network architecture. Each layer in the pipeline will refine the prediction as neural networks. Therefore, if each building is set to be a neural network, the overall pipeline would then a neural network of neural networks. 

The self-similarity structure can be extended repeatedly by interpreting the incremental learning pipeline as a base learner for another incremental learning pipeline. 


% One is All, All is One 
\paragraph{Trees and Neural Networks are alike}

Gradient Boosting Decision Trees (GBDT) and neural networks (Example: Multi-layer perceptron) are usually considered as two distinct class of machine learning models. However recent researches, such as soft decision trees \cite{NODE} and transformers \cite{Transformer17} suggests it is possible to make neural network models more tree like. Similarly, replacing the base learner in GBDT models with weakly trained shallow nets can make GBDT models more neural like. 

A better way to understand machine learning models is to place each model in a \textbf{continuous} spectrum of model density, from sparse to dense representations/structures. Trees are sparse models. Neural networks are dense models. The choice of model depends on the nature of input features. In general, sparse models are good for unstructured data or categorical features. Dense models are good for structured data (Example: images,text) or numerical features. The binned ordinal features for the temporal ranking task is somewhere in between, making both trees and networks good choices of models. 

The incremental learning pipeline can combine any machine learning models within each layer, and no assumptions are made on the nature of model. Each layer will be non-binary in nature. As discussed above, the incremental learning pipeline itself also share properties of gradient boosting models and neutral networks. 


\paragraph{Adaptive nature of pipeline} 

The incremental learning pipeline supports \textbf{dynamic} model training, as parameters of each component model is updated regularly to adapt to distributional shifts in data. Under traditional machine learning framework, hyper-parameters of machine learning models are selected by cross-validations. A big limitation of using cross-validations for incremental learning problems is that the optimal hyper-parameters based on a \textbf{single} test period might not work in future. This is replaced by \textbf{dynamic} hyper-parameter optimisation in the incremental learning pipeline. Predictions from previous layers based on different model hyper-parameters are combined in the next layer, which is called as model stacking layer. For example, with the use of a shallow MLP, the parameters of the MLP can be interpreted as the weighting for each set of hyper-parameters. Weights of the neural network can be regularised as a mechanism of soft selection of model hyper-parameters. Soft hyper-parameter selection are also related to Bayesian methods of learning the regularisation hyper-parameter of regression models. Instead of attempting to derive the posterior distributions of the model hyper-parameters, which is difficult when there are no closed form solutions, the weight parameters in the model stacking layer can be considered as something equivalent to the posterior distribution. 

%Residual connections between layers allow passing information across time (without look-ahead bias). This further allow the incremental learning pipeline to iteratively improve its predictions. 


\paragraph{Universal Approximation} 

It is well-known that Multi-layer perceptron (MLP) and gradient boosting decision trees (GBDT) models have the universal function approximation property \cite{cybenko1989approximation} for \textbf{fixed} tabular datasets. Deep Learning models for sequences, such as LSTM \cite{schafer2006recurrent} are also shown to have the universal function approximation property for any dynamical systems. It can be shown that the above incremental learning pipeline also have the universal approximation property for the underlying stochastic processes that drives the infinite data generation of the temporal tabular datasets. 

The universal approximation provides a theoretical guarantee that if there is an infinite amount of computational resources then the above pipeline can be used for any modelling tasks of temporal tabular datasets. In reality, a wide range of heuristics is applied to simplify the pipeline design with our finite amount of computational resources so that the approximated pipeline can be as close to the theoretical optimal as possible. 



\subsection{Connections with common machine learning tasks}

The incremental learning pipeline is a complete end-to-end autoML tool which transforms the given features into predictions. Different machine learning tasks, such as feature engineering and model stacking are integrated within the pipeline as follows. A key feature is that tabular models such as GBDT or MLP can be used repeatedly in different layers of the pipeline to achieve the purpose of different machine learning tasks, which previously considered as separate tasks. In particular, even with just two building blocks, GBDT and MLP models, very complicated and effective incremental learning pipelines can be built. 

\paragraph{Feature Engineering/Selection} 

Feature Engineering is an important part of modelling tabular data. A wide range of standard feature engineering methods, including random non-linear transforms \cite{Horn19} and data-based methods such as feature-tools \cite{James15} are proposed to learn higher order feature relationships. Standard tabular models are then trained on the original features with the newly created features. More recent methods would use deep learning methods, in particular sequential attention for feature selection and engineering, examples include TabNet\cite{Arik_Pfister_2021} and TabTransformer \cite{TabTransformer}. For these models, feature engineering and tabular data modelling is performed in a single model. A major limitation of these models is high (active) memory consumption during model training. Another limitation is that many feature engineering methods are developed for \textbf{static} datasets where for a set of relationships learn on \textbf{fixed} training set. Unless these feature engineering components are retrained regularly as the downstream tabular models, the learnt relationships might not hold over distribution shifts of data. 

Most feature engineering methods applied on tabular data is a transformation from tabular features to another set of tabular features and not much different than a collection of weakly trained tabular models. Under the incremental learning framework, predictions from shallow trees and networks in the early layers can be considered as new features generated for downstream layers. It is not strictly necessary to make a distinction between feature engineering and tabular modelling. Training models in a multi-step layered structure avoids the look-ahead bias issue naturally. 

Feature Selection can be considered as a special case of feature engineering where the feature engineering transformation is a Boolean mask.


\paragraph{Feature Neutralisation} 

Feature Neutralisation and its dynamic variant (Dynamic Feature Neutralisation) introduced in the previous Numerai study \cite{ThomasW23} can also be reformulated under the incremental learning framework as follows. 

The usefulenss of feature neutralisation is based on the assumption that unconstrained model training methods will result in model predictions that can be explained by a small subset of features, which is defined as feature concentration risks. While these models work well under data regimes that are similar to the training set, these models will suffer when there are distributional shifts in data. 

Feature neutralisation, which can be used to limit both the maximal feature concentration risks and (linear)-dependencies of model on the selected features can mitigate the impact of data distributional shifts by both spreading the model risk across a wider range of features and focusing more on the non-linear relationships between features. Dynamic feature neutralisation suggests for data streams with distributional shifts, the optimal subset of features for neutralisation is \textbf{dynamic} and rather than \textbf{fixed}.  

A key issue remained unresolved in the previous study \cite{ThomasW23} is that the optimal degree of feature neutralisation and the size of subset of features to be neutralised is given by (good) heuristics. Clearly, these parameters should be learn from the dataset and adjusted according to data distribution shifts. Under the incremental learning pipeline, models predictions undergoing different feature neutralisation process can be combined as soft selection of model hyper-parameters.



\paragraph{Advanced Architectures neural networks} 

There is no consensus of whether the No Free Lunch Theorem \cite{} is relevant in machine learning research. In particular, for \textbf{static} machine learning problems, many advanced architectures of neural networks did outperform basic multi-layer perceptron networks \cite{Arik_Pfister_2021,NODE,TabTransformer,}. This observation is also validated in many bench-marking studies on neural network architectures. But at the same time there are researchers that suggests the contrary when running the models using different hyper-parameters or on \textbf{different} datasets \cite{Arlind21,Leo22,Shwartz21}. As a result, the decision of whether to use more advanced architectures in place of a standard MLP remains an open problem. 

% No man ever steps in the same river twice, for it's not the same river and he's not the same man. 
For the temporal tabular ranking problem, the only purpose is to generate high quality and robust forecast for \textbf{future} using models trained on \textbf{past} data only. A good performance based on \textbf{historical} data is not relevant if the performances cannot carry on into the future. An inherent limitation in research design for incremental learning tasks is that all the results are reported are simply a \textbf{snapshot} of the infinite model evaluation process. When there are significant distribution shifts in data, conclusions from previous researcher can become invalid. 

%% Interpretability 
Interpretability is also an important consideration when choosing machine learning method. Deep learning methods with advanced architectures had a inherent black-box problem as it is difficult to interpret the weights of neural networks. In the incremental learning pipeline, the output of each component can be evaluated independently and compared to inspect the changes of prediction quality in different layers. This provides a transparent view into the usefulness of each component and layer. 


\paragraph{Model Stacking/Selection}

Stacking is a simple but highly effective techniques to combine different machine learning model predictions. A closely related problem in finance would be portfolio optimisation, where a convex optimisation is solved at each time-step to find the linear combination of assets or strategies that maximise risk-adjusted return.

Under the incremental learning framework, model stacking can be performed as \textbf{prediction} level and not just at \textbf{model} level, which means the predicted rankings from each model are combined and not just assign a global weight for each model at each era. Instead of considering model stacking as a \textbf{separate} step to model training, model stacking can be considered as an extra layer in the incremental learning pipeline as in soft hyper-parameter selection. 


\section{Applications of the incremental learning pipeline to Numerai competition}

\subsection{Numerai Dataset} 
\label{section:numerai-sunshine-dataset} 


\paragraph{Features and Targets}
The v4.1 of the Numerai dataset \cite{numerai-datav4.1}, which consists of a total of 1586 features is used for analysis in this paper. The dataset is of the format of a Temporal Tabular dataset. The dataset contains multiple targets, which represents normalised stock returns by different statistical methods. 'target-cyrus-v4-20' is used for scoring the trained models. 

To make sure some machine learning models can be trained on a \textbf{single} GPU, the number of input features needs to be reduced.
Two different methods are used for feature selection. The first one is by random sampling and the second one is by taking the median over correlated features. 

For tabular models, a random sample of features can be drawn from the 1586 features. This is a generic method that can be applied to any tabular datasets. 

A different method is applied to reduce dimensionality of data when calculating the feature importance's time-series. This method make use of the observed feature correlation structure in the Numerai dataset. The features that demonstrate high multicollinearity are grouped and replaced with their Median (called as `Group X Median'). A reason to use Median instead of Mean is to preserve the binned data structures (as each feature are binned values between $-2$ to $2$ after making the mean as zero). 

The features from Numerai can be separated into two different sets. One of the set consists of 1445 features which can be grouped in 289 groups by the multicollinearity criteria as these features have a high correlation (>0.9) consistently in different eras. Each group consists of an equal number of 5 features. These features demonstrate a consistent relationship over different data eras. This set of 1445 features is called \textbf{Set A}. The other set consists of 141 features that cannot be grouped by the multicollinearity criteria. This set of 141 features is called  \textbf{Set B} in the rest of the chapter. For features in \textbf{Set A}, the median of each group is computed. This group of 289 Median features and the \textbf{Set B} features, in total 430 features are then used to calculate the \textbf{feature performances} time-series, which are then used to build a factor timing portfolio described as above. 


\paragraph{Data Lag}
Setting the data lag for predictions would depend on the robustness of the data pipeline from Numerai. The theoretical minimal for the scoring target to resolve is 5 weeks (4 weeks of market data and 1 week for data processing) but some participants has pointed out it can take up to 6 weeks to resolve the target. To take account into both the data lag from Numerai and time for participants to train models, a conservative data lag of 15 weeks is used for running the pipelines below. 


\paragraph{Scoring Function} 

Numerai calculates a variant of Pearson correlation score with the following formula \cite{numerai-corr}.

Let $y_p$ be the predictions ranked between 0 and 1, $y_t$ be the targets centred between -0.5 and 0.5, $\Phi(\cdot)$ be the Cumulative Distribution function of standard Gaussian, $\textbf{sgn}(\cdot)$ and $\textbf{abs}(\cdot)$ be the element-wise sign and absolute value function, then Numerai Corr $\rho_n$ is given by 
\begin{align*}
    y_g &= \Phi^{-1}(y_p) \\
    y_{g15} &= \textbf{sgn}(y_g) \cdot \textbf{abs}(y_g)^{1.5} \\
    y_{t15} &= \textbf{sgn}(y_t) \cdot \textbf{abs}(y_t)^{1.5} \\
    \rho_n &= \textbf{Corr}(y_{g15},y_{t15})
\end{align*}

The purpose of applying a power transformation is that emphasis the contribution from the highest and lowest predictions. 



\paragraph{Hypothesis on the data generation process} 

The feature creation process of the Numerai dataset is proprietary and therefore there is no official documentation to guide data scientists in interpreting what the features would be. In the following, a hypothesis is proposed to explain the correlation structure between features identified above. 

Set A consists of 1445 features representing 289 firm-specific characteristics calculated with 5 different time-series smoothing techniques. Set B consists of 141 smart-beta factors which represents market and industry wide effects, see \cite{JensenKellyPedersen2022} for an example of smart-beta features in global stock market.

The data generation process is closely related to the investment aim of Numerai (and other Long/Short Equity Market Neutral hedge funds). The following table \ref{table:hedgefunds} provides an non-exhaustive overview of different sources of return for market participants in the stock market. 

The market return, defined as beta within the Capital Asset Pricing Model (CAPM) is the most basic source of return market participants can obtain. It has nearly zero capacity constraints as the strategy itself (weights of stocks in an index and the re-balance schedule) is known to all market participants. It is also the largest source of return (before leverage) one can obtain from the market. However, the market return is also the most volatile and highly dependent on economic regimes (which is very difficult to predict). 

Smart-beta factors, for example the Fama-French factors \cite{} are return factors based on well-established economic theories and empirical observations in the global stock market. In fact, a factor-timing portfolio applied on the Numerai dataset can replicate the returns of the smart-beta factors that are present in the Numerai data generation process. As only linear modelling techniques are required to replicate this source of returns, it is defined as \textbf{linear} factors.  

Applying machine learning methods on the Numerai dataset can uncover \textbf{non-linear} relationships between the smart-beta factors and alpha sources identified by Numerai. The aim of the data science competition is for data scientists to build predictions that capture return from this source, while minimising contribution from market and (linear) smart beta factors.  

Arbitrage is another source of return that is captured by High Frequency Trading (HFT) firms. As Numerai is trading at mid-frequency (where the average of stock holding period is from 7-90 days), arbitrage-like strategies are not directly relevant to Numerai as it does not fit the investment aim of their fund. However, Numerai will employ trade execution optimisation techniques to minimise the impact of HFT firms in eroding their strategy performances. 


\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
        ~ & Expected Return & Capacity & Volatility \\ \hline
        Market & Largest & No constrains & Highest \\ \hline
        Linear(Smart-beta) & Large & Some constrains & High \\ \hline
        Nonlinear(Machine-Learning) & Small & Constrained & Low \\ \hline
        Arbitrage & Smallest & Very constrained & Lowest \\ \hline
    \end{tabular}
    \caption{Sources of return from stock market}
    \label{table:hedgefunds}
\end{table}


\section{Training incremental learning pipelines with Numerai data}

A two-step process is used to train incremental learning pipelines with Numerai dataset. The first step is to use data before Era 800 for the hyper-parameter optimisation of tabular and factor-timing models. After that the incremental learning pipeline is ran to get online predictions from Era 800 on-wards. 


\subsection{Hyper-parameter optimisation for tabular models} 

For different tabular models introduced in section \ref{section:tabular-inc}, hyper-parameter optimisation is performed using data before Era 800. The training and validation set is data between Era 1 and Era 600, with  $25\%$ of data as validation set. Due to memory constraints, data sub-sampling  is applied during model training. $25\%$ of the eras in the training period is used with sampling performed at regular intervals. The performance of the models between Era 601 and Era 800 (evaluation period) is then used to select hyper-parameters for the tabular models. The Mean Corr and Sharpe Ratio of the prediction ranking correlation in the evaluation period is reported. Details of computing these performance metrics can be found in \cite{ThomasW23}. Due to memory issues for training neural network models, a random feature selection process is used to select $50\%$ of the 1586 features at the start of each model process. 





\subsection{Incremental Learning pipeline for XGBoost models} 

The above grid searchers on different tabular and time-series models suggest XGBoost is the best model. 

The XGBoost models with the optimised design choices (based on data up to Era 800) are used in incremental learning pipelines with different training sizes and retrain frequency. The start point of incremental learning pipeline is set to Era 801 and end point is set to Era 1050. 

Train size is how many eras of data are used to create the training and validation set. Retrain frequency represents how often the model parameters are updated with the latest data. A retrain frequency of 100 means the model is retrained every 100th era. 


%% When Train size is 585 
\paragraph{Adjusting the retrain frequency} 

The train size of models are fixed to be 585 as above. The retrain frequency is set to be 200,100,50 respectively. 

Table \ref{table:XGBInc1} lists the performances of the XGBoost models trained with the same optimised hyper-parameters between Era 601 and Era 1050. 


\paragraph{Adjusting the train size of incremental pipeline} 

Changing the train size from 585 to 785 provides a totally different model performance ranking is observed. 

Table \ref{table:XGBInc2} lists the performances of the XGBoost models trained with the same optimised hyper-parameters between Era 801 and Era 1050. 


\paragraph{Trade-offs in model updates for non-stationary data} 




\section{The effect of model complexity}
\label{section:paradigm-ML} 

\begin{comment}
    Some abstract theory stuff 
    
    Given an incremental learning pipeline and an infinte stream of data. Assuming the data generation process is known, and the parameters sets of an incremental learning model(pipeline) follows a dynamic system in some very high dimensional space, what properties can be derived at limit of infinite time? What would be the convegence (if there are any) of model parameters over different incremental learning processes? In particular, can the point-in-time forecasts be theorectical optimal for any incremental learning model build using base model that supports universal approximation theorem. 
\end{comment}

Traditional machine learning suggests there exists an optimal model complexity where the trade-off of bias and variance is optimal (for a loss function that behaves like the Mean-Squared error). However, model machine learning suggests using an over-parameterised model might improve performance in test set even model training is already saturated (where training loss is close to zero). The improvement is significant in cases where model specification is incomplete and low signal-to-noise ratio in the given features. This counter-intuitive phenomenon is explored in different research papers \cite{Hastie19,NakkiranPreetum2021Dddw,Teresa22} for both theoretical and empirical of a \textbf{single} machine learning model. The two approaches are summarised into Hypothesis 1 and 2. 

Two new hypothesis (Hypothesis 3 and 4) are added on top of the existing ones to extend the discussion to the case from an \textbf{ensemble} of machine learning models. The hypothesis suggests when there is a collection of machine learning models, the uniqueness of predictions is equally important as performances of individual models. 



\begin{itemize}
    \item Hypothesis 1: (Classical Approach): There exists an optimal model complexity which can found by performing by bootstrapping-like procedures, such as cross-validation. 
    \item Hypothesis 2: (Modern Approach): Over-parameterised model will outperform the optimal model under classical approach. 
    \item Hypothesis 3: (Closed System): For models obtained by the above two approaches using well-defined training processes, prediction model performances and \textbf{predictions} would converge to stable equilibrium, which is characterised by the dataset/feature quality, such as model completeness and signal-to-noise ratio of features. 
    \item Hypothesis 4: (Open System): For models obtained by the above two approaches, both would achieve similar performances and therefore it is not meaningful to use cross-validation schemes to select model complexity based on performances. However, different modelling approaches can return a diverse set of predictions which can be combined to lower variance. 
\end{itemize}


\subsection{Definitions of Model Complexity} 

As suggested in \cite{Hastie19,NakkiranPreetum2021Dddw,Teresa22}, there is no single measure of model complexity as it depends on multiple factors in the training process, such as learning rates, data size, signal-to-noise ratio of the dataset and whether the models are well-specified or mis-specified. 

From a practical point of view, computational time is a very good proxy of model complexity, since more complicated models would require more computational resources. The following lists the parameters that are highly influential to the training time of models for the models used in Section \ref{section:ML-TS}. These will be used as a measure of model complexity in the following discussion. 

\begin{itemize}
    \item Feature Engineering based Time Series Models: The feature complexity ratio, defined as the ratio of the number of features over the size of data observations
    \item Gradient Boosting Decision Trees: The number of boosting rounds 
    \item Neural Networks: The number of training epochs 
\end{itemize}






\subsection{Comparison with Numerai Meta Model}

Model performances obtained from the optimised XGBoost Incremental Learning pipeline is compared with the Meta Model described in Numerai. 

\begin{comment}
    
\end{comment}



\section{Appendix} 

\subsection{Hypothesis testing for incremental learning tasks}

In the following, a new statistical method for conducting hypothesis testing in incremental learning tasks is proposed. The new method can mitigate some of the limitations of reporting model performances based on \textbf{snapshots} of data. 

Given two sets of \textbf{static} data, standard hypothesis tests (paired sample t-test) can be used to test if means of the two sets are equal. he test is extended to \textbf{dynamic} datasets as follows. 

Consider the problem where two \textbf{similar} prediction models A and B for a temporal ranking prediction tasks are compared, which is common when two models are trained from the same machine learning model family with different hyper-parameters. From the incremental learning pipeline, a stream of model performances can be obtained. A delta series, defined as the difference between model performances can then be calculated. Taking the difference will cancel the shared effects of the two models and allow us to focus on the effects of hyper-parameters on model performances. 

Under the assumption where observations of the difference series is independent \footnote{There are many empirical evidence to suggest that auto-correlation between observations have an impact when using different model performance measures.} and followed a Gaussian distribution \footnote{Many of the datasets considered in this paper demonstrated fat-tailed behaviour which violates this assumption.}, then under the null hypothesis where the two models have the same mean performances, the cumulative performance of the delta series will follow a Brownian motion. The variance of this Brownian motion will be determined empirically from the data. 

Now consider at a fixed time-step $1 < t < T$ for a delta series with total length $T$. Under the null hypothesis the p-value of the observed value of delta series at time $t$ are calculated by comparing with a suitable Gaussian distribution. (The distribution used here can be replaced by any other valid distributions if fat-tailed behaviour is to be modelled). Doing so repeatedly at multiple time-steps will obtain \textbf{multiple} p-values determined incrementally which would then needs to be corrected by multiple test adjustments such as Bonferroni correction \cite{}. 





\newpage 
\section{Acknowledgments}
This was was supported in part by the Wellcome Trust under Grant 108908/B/15/Z and by the EPSRC under grant EP/N014529/1 funding the EPSRC Centre for Mathematics of Precision Healthcare at Imperial. 


%Bibliography
\newpage
\printbibliography


\end{document}
