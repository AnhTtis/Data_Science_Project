\section{Introduction} \label{sec:introduction}

As for the study of human-like compositional generalization ability, compositional zero-shot learning (CZSL)~\cite{Misra:red-wine-to-red-tomato,Li:symmetry-and-group,Purushwalkam:task-driven-modular-networks} studies to recognize unseen \textit{compositions} at test time, while states and objects (\textit{i.e.}, \textit{primitives}) are presented in seen \textit{compositions} during training.
Rather than learning to associate images with such state-object compositions from scratch~\cite{Naeem:CGE,Mancini:Co-CGE}, recent efforts~\cite{Nayak:CSP,Xu:PromptCompVL,Lu:DFSP} focus on adapting pre-trained vision-language models (VLMs), \textit{e.g.}, CLIP~\cite{Radford:CLIP}. 
Since CZSL datasets provide only compositional labels (\textit{e.g.}, ``\textit{red}''+``\textit{wine}'') instead of complete sentences (\textit{e.g.}, ``\textit{A glass of wine placed on the table}'') used in pre-training, to bridge the gap without fine-tuning the entire model, prompts~\cite{Lester:prompt-tuning,Liu:prompt-tuning} are constructed by appending a simple prefix like ``\textit{a photo of}'' before the composed state-object labels.
Prior methods~\cite{Nayak:CSP,Xu:PromptCompVL,Lu:DFSP} have replaced the fixed prompt tokens with learnable tokens that are directly optimized during fine-tuning.
By designing prompt tuning solutions for compositions, existing methods can efficiently contrive a cross-modal alignment between images and compositional labels, thereby unlocking the potential compositional generalization capability of VLMs.


\begin{figure}[!t]
  \centering
   \includegraphics[width=\linewidth]{figure/multi-path-latest.pdf}
   \vspace{-7mm}
   \caption{
   \textbf{Graphical comparison of the existing paradigm and the proposed \MP paradigm.}
   }
   \label{fig:multi-path}
   \vspace{-5mm}
\end{figure}


However, due to the lack of independent and explicit \textit{primitive modeling}, these methods have suffered from two challenges:
\textbf{(1)} the full leveraging of pre-trained knowledge fails, since a large amount of cross-modal information is not tied to the compositions, but related to the single primitive.
\textbf{(2)} the difficulty of generalizing to unseen compositions is increased, since the model easily over-rely on a limited number of seen compositions.
To overcome the issues with a particular focus on the universality of the solution, 
we propose a novel \textbf{\MP} paradigm for CZSL with VLMs. 
As shown in \cref{fig:multi-path}, our paradigm emphasizes the joint modeling of the state, object, and composition without redundant assumptions about the specific implementations.
Different from previous methods that depend only on the estimated composition probability, 
\MP paradigm integrates the predictions of all semantic components for the final decision.
Following the accessible paradigm, we present an outstanding implementation \textbf{\method}\footnote{``Troika'' is a traditional harness driving combination, using three horses abreast, usually pulling a sleigh.}.
On the language side, 
\method constructs branch-specific prompts that inject learnable priors for describing the context of specific target classes.
And on the vision side, 
while introducing parameter-efficient adaptation,
\method decomposes primitive visual features for individual recognition.

Moreover, for calibrating the bias between semantically similar multi-modal representations, we further devise a \textbf{\CMT} module into \method.
The motivation is that compared to diverse visual presentations, learning only a fixed prompt representation is intuitively insufficient to match all corresponding images from different domains (\cref{fig:CMT}).
By selecting and integrating the most semantically relevant visual features, the module pulls the originally static prompt representation towards the visual content.
As shown in \cref{tab:ablation-fusion}, while the basic \method has already achieved state-of-the-art (SOTA), the incorporation of the \CMT module leads to a significant improvement.
Follow-up researches are also free to try other traction ways as the module and \method are decoupled.



Three popular benchmark datasets MIT-States~\cite{Isola:MIT-States}, UT-Zappos~\cite{Yu:UT-Zappos}, and C-GQA~\cite{Naeem:CGE} are used for comparisons.
Experiments show that on the closed-world setting, \method exceeds the current state-of-the-art methods by up to \textbf{+7.4\%} HM and \textbf{+5.7\%} AUC.
And on the more challenging open-world setting, \method still surpasses the best CLIP-based method by up to \textbf{+3.8\%} HM and \textbf{+2.7\%} AUC.
We also conduct abundant ablations to verify the effectiveness of all component elements of \method.
In summary, the main contributions of our work are four-fold:
\begin{itemize}[labelsep=0.4em, leftmargin=1em,itemindent=0em]
	\item We propose a novel \MP paradigm for CZSL with VLMs, which explicitly constructs vision-language alignments for the state, object, and composition.
    The paradigm is flexible enough to derive new approaches.
	\item Based on the paradigm, we implement a model named \method that effectively aligns the branch-specific prompt representations and decomposed visual features.
    \item We further design a \CMT module for \method that adaptively adjusts the prompt representation depending on the visual content.
    \item We conduct extensive experiments on three CZSL benchmark datasets to show that \method achieves the SOTA performance on both closed-world and open-world settings.
\end{itemize}

\begin{figure}[!t]
  \centering
   \includegraphics[width=\linewidth]{figure/CMT_v3.pdf}
   \vspace{-7mm}
   \caption{
   \textbf{An example of the \CMT module.}
   The commonly learned prompt of ``\textit{red}'' may be further away (compared to ``\textit{black}'') from individual images with the same concept, and
   the module reduces such mismatches by adaptively pulling the prompt representation towards the current visual content.
   }
   \label{fig:CMT}
   \vspace{-5mm}
\end{figure}

