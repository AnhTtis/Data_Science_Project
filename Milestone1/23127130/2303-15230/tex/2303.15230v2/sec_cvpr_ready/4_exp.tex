



\section{Experiments}

\subsection{Experimental Setup}


\paragraph{Datasets.}
We experiment with three real-world CZSL benchmarks: MIT-States~\cite{Isola:MIT-States}, UT-Zappos~\cite{Yu:UT-Zappos}, and C-GQA~\cite{Naeem:CGE}.
We follow the split suggested by previous works~\cite{Purushwalkam:task-driven-modular-networks,Nayak:CSP}, and summarize detailed statistics in~\cref{tab:dataset_stats}.


\begin{table}[!t]
\tablestyle{5pt}{1.0}
\setlength\tabcolsep{1.9pt}
\def\w{20pt} 
\scalebox{1}{
    \begin{tabular}{lcc|cc|ccc|ccc}
          &       &       & \multicolumn{2}{c|}{\textbf{Training}} & \multicolumn{3}{c|}{\textbf{Validation}} & \multicolumn{3}{c}{\textbf{Test}} \\
    \textbf{Dataset} & $\mathcal{S}$ & $\mathcal{O}$ & $\mathcal{C}^{se}$ & $\mathcal{X}$ & $\mathcal{C}^{se}$ & $\mathcal{C}^{us}$ & $\mathcal{X}$ & $\mathcal{C}^{se}$ & $\mathcal{C}^{us}$ & $\mathcal{X}$ \\
    \shline
    MIT-States~\cite{Isola:MIT-States} & 115   & 245   & 1262  & 30k   & 300   & 300   & 10k   & 400   & 400   & 13k \\
    UT-Zappos~\cite{Yu:UT-Zappos} & 16    & 12    & 83    & 23k   & 15    & 15    & 3k    & 18    & 18    & 3k \\
    C-GQA~\cite{Naeem:CGE} & 413     & 674     & 5592    & 27k    & 1252    & 1040    & 7k    & 888     & 923     & 5k \\
    \end{tabular}%
    } \vspace{-2mm}
  \caption{
  \textbf{Statistics of three datasets in our experiments.} 
  The number of elements in each set is reported.
  }
  \label{tab:dataset_stats}%
  \vspace{-5mm}
\end{table}%






\begin{table*}[!t]    %
\tablestyle{5pt}{1.0}
\setlength\tabcolsep{1pt}
\def\w{20pt} 
\scalebox{1}{
    \begin{tabular}{lcccc|cccc|cccc}
    \multirow{2}[0]{*}{\textbf{Method}} & \multicolumn{4}{c|}{\textbf{MIT-States}} & \multicolumn{4}{c|}{\textbf{UT-Zappos}} & \multicolumn{4}{c}{\textbf{C-GQA}} \\[-1.5pt]
          & S     & U     & HM     & AUC   & S     & U     & HM     & AUC   & S     & U     & HM     & AUC \\
    \shline
    \multicolumn{13}{c}{ \demph{ \it{\textbf{Closed-world} Results} } }\\
    \hline
    CLIP~\cite{Radford:CLIP}  & 30.2  & 46.0  & 26.1  & 11.0  & 15.8  & 49.1  & 15.6  & 5.0   & 7.5   & 25.0  & 8.6   & 1.4 \\
    CoOp~\cite{Zhou:CoOp}  & 34.4  & 47.6  & 29.8  & 13.5  & 52.1  & 49.3  & 34.6  & 18.8  & 20.5  & 26.8  & 17.1  & 4.4 \\
    CSP~\cite{Nayak:CSP}   & 46.6  & 49.9  & 36.3  & 19.4  & 64.2  & 66.2  & 46.6  & 33.0  & 28.8  & 26.8  & 20.5  & 6.2 \\
    PromptCompVL~\cite{Xu:PromptCompVL} & 48.5  & 47.2  & 35.3  & 18.3  & 64.4  & 64.0  & 46.1  & 32.2  & - & - & - & - \\
    DFSP(i2t)~\cite{Lu:DFSP} & 47.4  & 52.4  & 37.2  & 20.7  & 64.2  & 66.4  & 45.1  & 32.1  & 35.6  & 29.3  & 24.3  & 8.7 \\
    DFSP(BiF)~\cite{Lu:DFSP} & 47.1  & 52.8  & 37.7  & 20.8  & 63.3  & 69.2  & 47.1  & 33.5  & 36.5  & 32.0  & 26.2  & 9.9 \\
    DFSP(t2i)~\cite{Lu:DFSP} & 46.9  & 52.0  & 37.3  & 20.6  & 66.7  & 71.7  & 47.2  & 36.0  & 38.2  & 32.0  & 27.1  & 10.5 \\
    \rowcolor[rgb]{ .949,  .949,  .949} \textbf{\method (Ours)} & \textbf{49.0}\tiny{$\pm$0.4} & \textbf{53.0}\tiny{$\pm$0.2} & \textbf{39.3}\tiny{$\pm$0.2} & \textbf{22.1}\tiny{$\pm$0.1} & \textbf{66.8}\tiny{$\pm$1.1} & \textbf{73.8}\tiny{$\pm$0.6} & \textbf{54.6}\tiny{$\pm$0.5} & \textbf{41.7}\tiny{$\pm$0.7} & \textbf{41.0}\tiny{$\pm$0.2} & \textbf{35.7}\tiny{$\pm$0.3} & \textbf{29.4}\tiny{$\pm$0.2} & \textbf{12.4}\tiny{$\pm$0.1} \\
    \hline\\[-2.4ex]
    \multicolumn{13}{c}{ \demph{ \it{\textbf{Open-world} Results} } }\\
    \hline
    CLIP~\cite{Radford:CLIP}  & 30.1  & 14.3  & 12.8  & 3.0   & 15.7  & 20.6  & 11.2  & 2.2   & 7.5   & 4.6   & 4.0   & 0.27 \\
    CoOp~\cite{Zhou:CoOp}  & 34.6  & 9.3   & 12.3  & 2.8   & 52.1  & 31.5  & 28.9  & 13.2  & 21.0  & 4.6   & 5.5   & 0.70 \\
    CSP~\cite{Nayak:CSP}   & 46.3  & 15.7  & 17.4  & 5.7   & 64.1  & 44.1  & 38.9  & 22.7  & 28.7  & 5.2   & 6.9   & 1.20 \\
    PromptCompVL~\cite{Xu:PromptCompVL} & 48.5  & 16.0  & 17.7  & 6.1   & 64.6  & 44.0  & 37.1  & 21.6  & - & - & - & - \\
    DFSP(i2t)~\cite{Lu:DFSP} & 47.2  & 18.2  & 19.1  & 6.7   & 64.3  & 53.8  & 41.2  & 26.4  & 35.6  & 6.5   & 9.0   & 1.95 \\
    DFSP(BiF)~\cite{Lu:DFSP} & 47.1  & 18.1  & 19.2  & 6.7   & 63.5  & 57.2  & 42.7  & 27.6  & 36.4  & 7.6   & 10.6  & 2.39 \\
    DFSP(t2i)~\cite{Lu:DFSP} & 47.5  & 18.5  & 19.3  & 6.8   & \textbf{66.8} & 60.0  & 44.0  & 30.3  & 38.3  & 7.2   & 10.4  & 2.40 \\
    \rowcolor[rgb]{ .949,  .949,  .949} \textbf{\method (Ours)} & \textbf{48.8}\tiny{$\pm$0.4} & \textbf{18.7}\tiny{$\pm$0.1} & \textbf{20.1}\tiny{$\pm$0.1} & \textbf{7.2}\tiny{$\pm$0.1} & 66.4\tiny{$\pm$1.0}  & \textbf{61.2}\tiny{$\pm$1.0} & \textbf{47.8}\tiny{$\pm$1.3} & \textbf{33.0}\tiny{$\pm$1.0} & \textbf{40.8}\tiny{$\pm$0.2} & \textbf{7.9}\tiny{$\pm$0.2} & \textbf{10.9}\tiny{$\pm$0.3} & \textbf{2.70}\tiny{$\pm$0.1} \\
    \end{tabular}%
  }\vspace{-2mm}
  \caption{
  \textbf{Main results on three benchmarks.} 
  All methods use a CLIP ViT-L/14 backbone.
  For our \method, we report the average performance on 5 random seeds with standard error.}
  \label{tab:main-SOTA}%
  \vspace{-5mm}
\end{table*}%

\vspace{-5mm}
\paragraph{Metrics.}
We follow the evaluation protocol of previous works~\cite{Purushwalkam:task-driven-modular-networks,Naeem:CGE,Tian:IVR}, where a calibration bias trades off between the prediction scores of seen and unseen pairs at test time.
While varying the candidate bias from $-\infty$ to $+\infty$, a curve can be drawn with the accuracy of seen and unseen pairs.
To quantify the overall performance on both seen and unseen pairs, we compute the area under the curve (\textbf{AUC}) and find the point with the best harmonic mean (\textbf{HM}) between the seen and unseen accuracy.
We also report the best seen accuracy (\textbf{S}) by adjusting the bias to $-\infty$, and the best unseen accuracy (\textbf{U}) by adjusting the bias to $+\infty$.


\vspace{-5mm}
\paragraph{Implementation Details.}
We implement \method with a pre-trained CLIP ViT-L/14 model in PyTorch~\cite{Paszke:PyTorch}.
The model is trained and evaluated on an NVIDIA A100 GPU.
For the open-world evaluation, we follow the post-training calibration method~\cite{Nayak:CSP} to filter out infeasible compositions.
More details can be found in the supplementary material.

\subsection{Main Results}


For a fair comparison, we primarily compare with CLIP-based methods using the same CLIP ViT-L/14 backbone.
In particular, pre-trained CLIP~\cite{Radford:CLIP}, CoOp~\cite{Zhou:CoOp}, CSP~\cite{Nayak:CSP}, PromptCompVL~\cite{Xu:PromptCompVL}, and all versions of DFSP~\cite{Lu:DFSP} are considered.
For comparisons with more baselines involved, please refer to 
the supplementary material.







In \cref{tab:main-SOTA}, we report both closed-world and open-world results.
On the \textbf{closed-world} setting, our \method exceeds the previous SOTA methods on MIT-States, UT-Zappos, and C-GQA.
Specifically, relative to existing methods, \method improves the HM by +2.0\%, +7.4\%, +2.3\%, and the AUC by +1.5\%, +5.7\%, +1.9\% respectively on three datasets.
And \method also achieves the best seen and unseen accuracies on these datasets.
On the \textbf{open-world} setting, our \method also achieves the SOTA results on the three datasets in terms of almost all metrics.
The only exception is that the best seen accuracy of \method is 0.4\% lower than DFSP(t2i)~\cite{Lu:DFSP}.
However, \method outperforms the existing methods by +0.8\%, +3.8\%, +0.5\% in terms of the HM, and by +0.4\%, +2.7\%, +0.3\% in terms of the AUC on the three datasets, indicating that our \method achieves a more consistent and comprehensive performance.

\subsection{Ablation Study} \label{sec:ablation-study}

To empirically show the effectiveness of our framework design, we conduct extensive experiments and report the closed-world results for the ablation study.


\begin{table}[!t]
\tablestyle{5pt}{1.0}
\setlength\tabcolsep{4pt}
\def\w{20pt} 
\scalebox{1}{
    \begin{tabular}{ccc|cccc|cccc}
    \multicolumn{3}{c|}{\textbf{Branch}} & \multicolumn{4}{c|}{\textbf{UT-Zappos}} & \multicolumn{4}{c}{\textbf{C-GQA}} \\
    $c$  & $s$ & $o$ & S     & U     & HM    & AUC   & S     & U     & HM    & AUC \\
    \shline
    \multicolumn{11}{c}{ \demph{ \it{Training + Inference} } }\\
    \hline
    \rowcolor[rgb]{ .949,  .949,  .949} \cmark & \cmark & \cmark & 66.8  & 73.8  & \textbf{54.6} & \textbf{41.7} & \textbf{41.0} & \textbf{35.7} & \textbf{29.4} & \textbf{12.4} \\    
    \cmark &       &       & 66.8  & \textbf{74.5}  & 49.9  & 37.7  & 39.6  & 34.3  & 28.9  & 11.6 \\
          & \cmark & \cmark & 67.9  & 69.4  & 47.0  & 35.7  & 35.8  & 20.2  & 19.1  & 5.9 \\
    \cmark & \cmark &       & 63.4  & 73.9 & 51.1  & 38.6  & 40.5  & 34.4  & 28.8  & 11.8 \\
    \cmark &       & \cmark & 67.2  & 73.1  & 49.7  & 37.7  & 39.5  & 33.8  & 28.8  & 11.5 \\
    \hline
    \multicolumn{11}{c}{ \demph{ \it{Inference} } }\\
    \hline
    \rowcolor[rgb]{ .949,  .949,  .949} \cmark & \cmark & \cmark & 66.8  & 73.8  & \textbf{54.6} & \textbf{41.7} & \textbf{41.0} & \textbf{35.7} & \textbf{29.4} & \textbf{12.4} \\        
    \cmark &       &       & \textbf{68.2} & 72.1  & 53.0  & 41.0  & 39.6  & 34.0  & 28.3  & 11.5 \\
          & \cmark & \cmark & 66.4  & 68.8  & 46.5  & 34.5  & 36.9  & 20.7  & 19.8  & 6.3 \\
    \cmark & \cmark &       & 66.3  & 70.0  & 53.7  & 39.9  & 40.7  & 33.7  & 28.8  & 11.7 \\
    \cmark &       & \cmark & \textbf{68.2} & 72.9  & 52.0  & 40.4  & 40.0  & 34.4  & 28.4  & 11.7 \\
    \end{tabular}%
  } \vspace{-2mm}
  \caption{\textbf{Ablation on the \MP paradigm.}
  The best results are obtained by keeping all three branches in both the training and inference phases.
  }
  \label{tab:ablation-branch}%
  \vspace{-7mm}
\end{table}%

\vspace{-5mm}
\paragraph{Ablation on Multi-Path Paradigm.}
In \cref{tab:ablation-branch}, we remove one or more specific branches at a time to prove that all branches in the \MP paradigm contribute.
Specifically, two scenarios are considered:
(1) \textbf{Training + Inference}, which refers to simultaneously eliminating the effects of the corresponding branches in both training and inference phases, \textit{i.e.}, removing the corresponding losses from \cref{eq:overall_loss} and the corresponding probabilities from \cref{eq:inference}.
(2) \textbf{Inference}, which refers to eliminating the effects of the corresponding branches only during inference, leaving the training loss unchanged.
Several observations in the table are worth highlighting:
(1) In both scenarios, removing the composition branch results in the greatest drop in performance, illustrating the importance of the branch for learning compositionality.
(2) Generally, removing the branches only during inference achieves a better result than removing them in both phases, indicating that all loss items have a positive impact.  %
(3) Keeping all branches in both scenarios leads to the best HM and AUC.
Note that some cases on UT-Zappos achieve a higher best seen or unseen accuracy, which only means that they might be better in unrealistic extremes.
And their worse HM and AUC suggest that removing branches would introduce instability.

\begin{table}[!t]
\tablestyle{5pt}{1.0}
\setlength\tabcolsep{3pt}
\def\w{20pt} 
\scalebox{1}{
    \begin{tabular}{cc|cccc|cccc}
          &       & \multicolumn{4}{c|}{\textbf{UT-Zappos}} & \multicolumn{4}{c}{\textbf{C-GQA}} \\
    \textbf{Prefix} & \textbf{Vocab.} & S     & U     & HM    & AUC   & S     & U     & HM    & AUC \\
    \shline
    \rowcolor[rgb]{ .949,  .949,  .949} $c|s|o$ & $cso$   & \textbf{66.8} & \textbf{73.8} & \textbf{54.6} & \textbf{41.7} & \textbf{41.0} & \textbf{35.7} & \textbf{29.4} & \textbf{12.4} \\
    $cso$   & $cso$   & \textbf{66.8} & 72.8  & 54.0  & 41.1  & 39.6  & 32.9  & 28.7  & 11.3 \\
    $c|so$  & $cso$   & 66.2  & 72.9  & 54.5  & 41.4  & 39.8  & 33.7  & 28.9  & 11.6 \\
    $c|s|o$ & $c|s|o$ & 67.2  & 72.5  & 52.9  & 40.8  & 39.9  & 33.0  & 28.9  & 11.5 \\
    \end{tabular}%
  } \vspace{-3mm}
  \caption{\textbf{Ablation on individual prefixes and shared vocabulary.}
  Branches separated by ``$|$'' do not share the corresponding prompt parameters.
  }
  \label{tab:ablation-prompt}%
  \vspace{-2mm}
\end{table}%

\begin{table}[!t]
\tablestyle{5pt}{1.0}
\setlength\tabcolsep{4pt}
\def\w{20pt} 
\scalebox{1}{
    \begin{tabular}{c|cccc|cccc}
    \multirow{2}[1]{*}{\textbf{\method}} & \multicolumn{4}{c|}{\textbf{UT-Zappos}} & \multicolumn{4}{c}{\textbf{C-GQA}} \\
          & S     & U     & HM    & AUC   & S     & U     & HM    & AUC \\
    \shline
    \rowcolor[rgb]{ .949,  .949,  .949} w/ CMT & \textbf{66.8} & \textbf{73.8} & \textbf{54.6} & \textbf{41.7} & \textbf{41.0} & \textbf{35.7} & \textbf{29.4} & \textbf{12.4} \\
    w/o CMT & 64.4  & 70.7  & 51.9  & 37.8  & 38.5  & 33.2  & 27.9  & 11.0 \\
    \end{tabular}%
  } \vspace{-3mm}
  \caption{\textbf{Ablation on the \CMT module.} 
  }
  \label{tab:ablation-fusion}%
  \vspace{-5mm}
\end{table}%

\vspace{-5mm}
\paragraph{Ablation on Prefix and Vocabulary Design.}
In \cref{tab:ablation-prompt}, based on the current design of \method (the top row), we first allow all three branches to share the prefix parameters (the second row), as well as allowing only the state and object branches to share the prefix parameters (the third row). 
We can observe that separating the prefixes of the composition branch and the primitive branches leads to higher HM and AUC.
Moreover, maintaining an individual prefix for each branch achieves the best results for all metrics.
The results show that it is necessary to inject branch-specific prior knowledge into the prefix parameters.
We also attempt to build an individual primitive vocabulary for each branch (the bottom row), which results in a significant drop in performance.
We attribute this to a disruption of the semantic dependency modeling.
As a conclusion, jointly optimizing the shared vocabulary parameters from multiple paths contributes to the compositional learning.

\vspace{-5mm}
\paragraph{Ablation on Cross-Modal Traction Module.}
In \cref{tab:ablation-fusion}, we validate the effectiveness of our \CMT module by removing it from \method.
We can observe that equipping \method with the \CMT module boosts the HM by 2.1\% and the AUC by 2.65\% in average.
This illustrates that by effectively calibrating cross-modal deviations, the adaptive traction improves the accuracy.

To qualitatively evaluate whether the \CMT module indeed exploits the semantically similar patch features, we also visualize the attention weights of several test samples from MIT-States in \cref{fig:attn_visualization}.
We can observe that the patches that are closer to the label semantics receive more attention,
which also means that they contribute more to the cross-modal traction.

\begin{figure}[!t]
  \centering
   \includegraphics[width=0.98\linewidth]{figure/attn_visualization_half.pdf}
   \vspace{-2mm}
   \caption{
   \textbf{Visualization analysis of the \CMT module.}
   We show the original image and the visualization result in pairs.
   The brighter the patch, the greater its role in the traction.
   }
   \label{fig:attn_visualization}
   \vspace{-4mm}
\end{figure}



\begin{table*}[!t]    %
\tablestyle{5pt}{1.0}
\setlength\tabcolsep{4pt}
\def\w{20pt} 
\scalebox{1}{
    \begin{tabular}{l|cccc|cccc|cccc}
    \multirow{2}[1]{*}{\textbf{Visual Tuning}} & \multicolumn{4}{c|}{\textbf{MIT-States}} & \multicolumn{4}{c|}{\textbf{UT-Zappos}} & \multicolumn{4}{c}{\textbf{C-GQA}} \\
          & S     & U     & HM    & AUC   & S     & U     & HM    & AUC   & S     & U     & HM    & AUC \\
    \shline
    None  & 48.3  & 50.8  & 37.5  & 20.6  & 62.7  & 70.7  & 50.3  & 36.2  & 34.8  & 29.5  & 24.2  & 8.5 \\
    Full  & 41.7  & 36.3  & 28.7  & 12.2  & 48.9  & 57.4  & 34.4  & 19.1  & \textbf{44.5} & \textbf{36.5} & \textbf{31.8} & \textbf{14.1} \\
    Bias~\cite{Cai:ft-bias}  & 48.6  & \uline{52.4}  & \uline{38.8}  & \uline{21.7}  & \textbf{66.8} & 70.4  & 51.1  & 38.1  & 37.4  & 32.9  & 27.0  & 10.3 \\
    Proj~\cite{Jia:VPT}  & 47.9  & 51.6  & 38.4  & 20.9  & 63.9  & \uline{71.4}  & 52.3  & 38.9  & 35.5  & 29.1  & 24.5  & 8.7 \\
    Partial~\cite{Jia:VPT} & \textbf{49.9} & 51.3  & 38.0  & 21.4  & \uline{65.1}  & 70.8  & \uline{53.9}  & \uline{39.3}  & 38.4  & 33.3  & 28.1  & 11.1 \\
    Prompt~\cite{Jia:VPT} & 48.9  & 51.3  & 38.1  & 21.3  & 65.0  & 71.2  & 51.1  & 38.0  & 36.7  & 30.6  & 26.1  & 9.6 \\
    \rowcolor[rgb]{ .949,  .949,  .949} \textbf{Adapter}~\cite{Houlsby:Adapter} & \uline{49.0}  & \textbf{53.0} & \textbf{39.3} & \textbf{22.1} & \textbf{66.8} & \textbf{73.8} & \textbf{54.6} & \textbf{41.7} & \uline{41.0}  & \uline{35.7}  & \uline{29.4}  & \uline{12.4} \\
    \end{tabular}%
  } \vspace{-2mm}
  \caption{
  \textbf{Ablation on visual tuning strategy.}
  Best results are displayed in \textbf{boldface}, and second best results are \uline{underlined}.
  }
  \label{tab:ablation-visual-tuning}%
  \vspace{-1mm}
\end{table*}%










\begin{figure*}[!t]
  \centering
   \includegraphics[width=\linewidth]{figure/qualitative_results.pdf}
   \vspace{-6mm}
   \caption{
   \textbf{Qualitative results.}
   We show top-1 predictions for randomly selected cases from MIT-States (the top row) and C-GQA (the bottom row).
   The complete \method correctly predicts the examples of five cols on the left, and fails on the examples of three cols on the right.
   Predictions when removing the \MP paradigm (\textit{i.e.}, w/o MP) or the \CMT module (\textit{i.e.}, w/o CMT) are also reported.
   \textcolor{rightcolor}{Green} denotes the correct prediction and \textcolor{wrongcolor}{red} denotes the wrong prediction.
   }
   \label{fig:qualitative_results}
   \vspace{-3mm}
\end{figure*}

\vspace{-5mm}
\paragraph{Ablation on Visual Tuning Strategy.} \label{sec:ablation-visual-tuning}
In \cref{tab:ablation-visual-tuning}, we compare the following popular tuning strategies for the pre-trained image encoder:
(1) \textbf{None}: freeze the encoder without updating its parameters.
(2) \textbf{Full}: fully update all parameters of the encoder.
(3) \textbf{Bias}~\cite{Cai:ft-bias,Zaken:ft-bias}: fine-tune only the bias terms.
(4) \textbf{Proj}~\cite{Jia:VPT}: fine-tune only the last linear projection layer $g^{proj}$.
(5) \textbf{Partial}~\cite{Jia:VPT}: fine-tune only the last block of the Transformer inside the encoder.
(6) \textbf{Prompt}~\cite{Jia:VPT}: fine-tune only the trainable prompt tokens inserted into the token sequence $\mathbf{X}$.
(7) \textbf{Adapter}~\cite{Houlsby:Adapter,Chen:AdaptFormer}: fine-tune only the adapter modules inserted into the Transformer inside the encoder, which is currently applied by \method.

We here highlight some observations from the table:
(1) Without adopting any tuning strategies for the image encoder, our approach still outperforms existing CLIP-based methods on most datasets, demonstrating the effectiveness of our proposed innovations including the multi-patch paradigm and \CMT mechanism.
(2) Although fully fine-tuning the image encoder achieves the best results on C-GQA, it hurts the performance on MIT-States and UT-Zappos to even underperform freezing the encoder.
Since C-GQA has much more training classes than the other two datasets, the observation suggests that fully fine-tuning the large pre-trained model easily overfits the training data, which results in poor generalization.
We note that this conclusion is consistent with existing studies~\cite{Sung:overfitting,Nayak:CSP}.
(3) All parameter-efficient tuning strategies, including those tune part of the original parameters (Bias, Proj, Partial) and tune additional parameters (Prompt, Adapter), significantly boost the performance compared to freezing the image encoder.
(4) Our applied Adapter achieves the most best results while its performance remains in the top two, indicating the superiority of our method design.



\subsection{Qualitative Results}

In \cref{fig:qualitative_results}, we visualize some qualitative results for both seen and unseen compositions, where the showed cases are randomly sampled from the test set of MIT-States and C-GQA datasets.
We report the predictions of the complete \method
and the models that remove the \MP paradigm or the \CMT module.
It can be observed that benefiting from both two innovations, \method recognizes the compositions with higher accuracy.    %
Taking the 5th case in the top row as an example, %
while the incomplete methods may be confused by the color and material presented by the object, the complete \method can focus on details such as shape, surface texture, and even local regions containing lens for comprehensive reasoning.
We also show some failure cases, %
where the entanglement of visual features places extreme demands on combinatorial understanding.
However, the proposed \MP paradigm enables the complete \method to correctly identify part of the contained primitives.
For the cases of complete prediction errors, although different from the provided labels, we find that the predictions can also interpret the content of these images.
This indicates the effectiveness of our \method from another perspective beyond the metrics.

