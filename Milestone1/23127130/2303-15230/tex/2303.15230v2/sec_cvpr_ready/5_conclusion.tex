\section{Conclusion}

In this paper, we explore the universal solution of adapting pre-trained VLMs for the downstream CZSL task.
We first propose a novel and flexible \MP paradigm that requires the simultaneous and explicit modeling of the state, object, and composition.
On top of that, follow-up researches can easily generate various new approaches with different multi-modal features.
And we develop a method named \method, which implements the paradigm with 
sophisticated
designs on both the language and vision sides.
We also present a \CMT module to improve \method by calibrating the bias between the static prompt representation and diverse visual content.
Both closed-world and open-world results on three benchmarks illustrate the superiority of \method, and extensive ablations also demonstrate the effectiveness of each component.
We hope that our work can inspire future research on exploiting foundational VLMs for compositional learning.

\noindent \textbf{Acknowledgement}
This work was supported by STI 2030â€”Major Projects (2022ZD0208800), NSFC General Program (Grant No. 62176215).
This work was supported by Alibaba Group through Alibaba Research Intern Program.


