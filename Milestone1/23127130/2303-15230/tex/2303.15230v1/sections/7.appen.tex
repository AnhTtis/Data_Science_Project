\appendix
\renewcommand\thefigure{A\arabic{figure}}
\renewcommand\thetable{A\arabic{table}}  
\renewcommand\theequation{A\arabic{equation}}
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}
\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}

\section*{Appendix}

\section{Experimental Details} \label{sec:exp_details}

In this section, we give more details about the architecture, training and evaluation for reference.

\subsection{Visual Adapter}

Here we detail how we introduce Adapter~\cite{Houlsby:Adapter} into the Transformer-based image encoder.
Specifically, we insert small learnable modules (\textit{i.e.}, adapters) after the multi-head self-attention layer and the feed-forward network inside each Transformer block.
Given an input feature $\mathbf{x} \in \mathbb{R}^{d}$, the adapter module uses a down-projection with the parameter matrix $\mathbf{W}^{down} \in \mathbb{R}^{d \times r}$ to project the feature to a lower-dimensional space specified by the bottleneck dimension $r$ ($r \ll d$), followed a nonlinear activation function $\sigma(\cdot)$, and an up-projection with the parameter matrix $\mathbf{W}^{up} \in \mathbb{R}^{r \times d}$.
Adopting a residual connection design, the overall computation of the adapter module is defined as
{\setlength\abovedisplayskip{2mm}
\setlength\belowdisplayskip{2mm}
\begin{align}
\text{Adapter}(\mathbf{x})=\mathbf{x}+\sigma(\mathbf{x}\mathbf{W}^{down})\mathbf{W}^{up},\nonumber
\end{align}}%
where $\sigma(\cdot)$ is implemented as GELUs~\cite{Hendrycks:GELU}.
Keeping the original image encoder frozen, we only optimize the parameters of these inserted adapters during training.

\subsection{Hyperparameters}

\cref{tab:hyperparam} lists the hyperparameters that differ on each dataset and are determined with the validation performance.
For other hyperparameters, the CLIP's pre-trained word embeddings of ``a photo of'' are used to initialize all three prefixes.
For Adapter inserted into the image encoder, the bottleneck dimension $r$ is set to 64, and the dropout rate is set to 0.1.
In the cross-modal traction module, the feed-forward network first expands the dimension of the input features to $4\times$ its original value, and then shrinks it back.
The number of attention heads $h$ is 12, and the dimension of the single-head attention $d^{attn}$ is 64.
And The strength parameter vector $\boldsymbol{\lambda}$ is filled with the scalar value 0.1 for initialization.
During training, we use the Adam~\cite{Kingma:Adam} optimizer and decay the learning rate of all trainable parameters by 0.5 every 5 epochs.


\begin{table}[!t]
\tablestyle{5pt}{1.0}
\setlength\tabcolsep{1.6pt}
\def\w{20pt} 
  \caption{\textbf{Hyperparameters for different datasets.}
  }\vspace{-2mm}
\scalebox{0.98}{
    \begin{tabular}{l|c|c|c}
    \textbf{Hyperparameter} & \textbf{MIT-States} & \textbf{UT-Zappos} & \textbf{C-GQA} \\
    \shline
    Learning rate & $10^{-4}$ & $2.5\times10^{-4}$ & $1.25\times10^{-5}$ \\
    Batch size & 64    & 64   & 64 \\
    Number of epochs & 10    & 15   & 15 \\
    Attribute dropout rate & 0.3   & 0.3   & 0 \\
    CMT dropout rate & 0.1   & 0     & 0 \\
    Weight decay & $10^{-5}$ & $10^{-5}$ & $10^{-5}$ \\
    Number of CMT layers $N$ & 3     & 2     & 2 \\
    Coefficients $\alpha^c, \alpha^s, \alpha^o$ & 1, 1, 1 & 1, 1, 1 & 1, 0.1, 0.1 \\
    \end{tabular}%
  }
  \label{tab:hyperparam}%
\end{table}%

\subsection{Feasibility Calibration for Open-World Setting}

Following~\cite{Mancini:Co-CGE,Nayak:CSP}, we apply the post-training feasibility calibration to filter out infeasible compositions that might be present in the open-world evaluation.
The calibration assumes that similar objects share similar states while dissimilar objects are unlikely to share states.
Therefore, given a candidate pair $c=\langle s, o \rangle$, similarities between the objects can be computed as
{\setlength\abovedisplayskip{2mm}
\setlength\belowdisplayskip{2mm}
\begin{align}
{\rho}_{o}(s, o) = \mathop{\max}\limits_{\hat{o} \in \mathcal{O}^{se}} \frac{\phi(o) \cdot \phi(\hat{o})}{\| \phi(o) \| \| \phi(\hat{o}) \|},\nonumber
\end{align}}%
where 
$\mathcal{O}^{se}$ is the object set that contains those paired with the state $s$ in seen compositions.
And $\phi(\cdot)$ is an embedding function that maps the primitive to a pre-trained embedding, which is implemented with GloVe embeddings~\cite{Pennington:Glove}.
We also compute similarities between the states as ${\rho}_{s}(s, o)$ in the same way.
Next, the feasibility score for the composition $(s, o)$ can be computed by combining the two similarities with a mean pooling function $\mu$:
{\setlength\abovedisplayskip{2mm}
\setlength\belowdisplayskip{2mm}
\begin{align}
{\rho}(s, o) = \mu({\rho}_{o}(s, o), {\rho}_{s}(s, o)).\nonumber
\end{align}}%

Finally, by only considering compositions above a threshold $T$, infeasible compositions can be filtered out.
And the inference of \method now becomes
{\setlength\abovedisplayskip{2mm}
\setlength\belowdisplayskip{2mm}
\begin{align}
\hat{c} = \mathop{\arg \max}\limits_{c_{i,j} \in \mathcal{C}^{tgt}, {\rho}(s, o) > T} \left(\widetilde{p}(c_{i,j}|x)\right),\nonumber
\end{align}}%
where the threshold $T$ is calibrated based on the performance on the validation set.




\section{Hyperparameter Sensitivity Analysis}

In this section, we vary some key hyperparameters to examine how sensitive the proposed \method is to them.

\begin{figure}[!t]
  \centering
   \includegraphics[width=\linewidth]{figure/HP_prim_loss_AUC.pdf}
   \vspace{-7mm}
   \caption{%
   \textbf{Sensitivity analysis on loss weighting coefficients $\alpha^s$ and $\alpha^o$.}
   }
   \label{fig:prim-loss-weight}
   \vspace{-2mm}
\end{figure}

\begin{figure}[!t]
  \centering
   \includegraphics[width=\linewidth]{figure/HP_Adapter_dim_AUC.pdf}
   \vspace{-7mm}
   \caption{%
   \textbf{Sensitivity analysis on Adapter bottleneck dimension $r$.}
   }
   \label{fig:adapter-dim}
   \vspace{-5mm}
\end{figure}

\vspace{-5mm}
\paragraph{Loss weighting coefficients $\alpha^s$ and $\alpha^o$.}
In \cref{fig:prim-loss-weight}, after fixing the weighting coefficient of the composition branch $\alpha^c$ as 1, we vary the loss coefficients on the state and object branches, \textit{i.e.}, $\alpha^s$ and $\alpha^o$.
While setting $\alpha^s$ and $\alpha^o$ as 1 can achieve the best result on MIT-States and UT-Zappos, a smaller value as 0.1 is better on C-GQA, where learning in a composable way deserves a higher priority in more complex scenarios.
Another observation is that setting $\alpha^s$ and $\alpha^o$ as 10 leads to a significant drop, 
which shows that it is detrimental to give the primitive branches a higher status than the composition branch during training.



\vspace{-5mm}
\paragraph{Adapter bottleneck dimension $r$.}
In \cref{fig:adapter-dim}, we vary the bottleneck dimension $r$ of the introduced adapters.
For all three datasets, 64 is an optimal choice for $r$.
And a higher $r$ may cause a performance crash on smaller datasets like UT-Zappos due to over-fitting.

\vspace{-5mm}
\paragraph{Number of CMT module layers $N$.}
In \cref{fig:CMT-layer}, a 2-layer \CMT module is optimal for C-GQA, while setting $N$ as 3 is better for the other two datasets.
Although not affecting its leadership over baseline methods, it is observed that further deepening the module may result in a loss of performance for \method.

\begin{figure}[!t]
  \centering
   \includegraphics[width=\linewidth]{figure/HP_fusion_layer_AUC.pdf}
   \vspace{-7mm}
   \caption{%
   \textbf{Sensitivity analysis on the number of \CMT module layers $N$.}
   }
   \label{fig:CMT-layer}
   \vspace{-2mm}
\end{figure}

\vspace{-5mm}
\paragraph{Initialization of $\boldsymbol{\lambda}$.}
In \cref{fig:lambda-initialization}, we first vary the initialization value of the trainable parameter vector $\boldsymbol{\lambda}$, which controls the strength of the cross-modal traction.
On all three datasets, initializing $\boldsymbol{\lambda}$ with 0.1 achieves the highest AUC, and steadily increasing the initialization value leads to a continuous decline in performance.
We attribute this phenomenon to the fact that aggressive traction may destroy the cross-modal alignment already established by pre-training.
Therefore, a larger initialization value for $\boldsymbol{\lambda}$ increases the difficulty of optimization.















\begin{table*}[!t]    %
\tablestyle{5pt}{1.0}
\setlength\tabcolsep{7pt}
\def\w{20pt} 
  \caption{%
  \textbf{Sensitivity analysis on initialization of prefixes.}
  }\vspace{-2mm}
\scalebox{1}{
    \begin{tabular}{lll|cc|cc|cc}
    \multicolumn{3}{c|}{\textbf{branch}} & \multicolumn{2}{c|}{\textbf{MIT-States}} & \multicolumn{2}{c|}{\textbf{UT-Zappos}} & \multicolumn{2}{c}{\textbf{C-GQA}} \\
    \multicolumn{1}{c}{$c$} & \multicolumn{1}{c}{$s$} & \multicolumn{1}{c|}{$o$} & HM    & AUC   & HM    & AUC   & HM    & AUC \\
    \shline
    \rowcolor[rgb]{ .949,  .949,  .949} ``a photo of'' & ``a photo of'' & ``a photo of'' & \textbf{39.3} & 22.1  & 54.6  & 41.7  & 29.4  & \textbf{12.4} \\
    ``a photo of'' & ``the object is'' & ``the object is'' & 39.2  & \textbf{22.4} & \textbf{55.6} & 40.9  & \textbf{29.7} & 12.0 \\
    ``the object is'' & ``the object is'' & ``the object is'' & 38.7  & 21.7  & 52.8  & 41.0  & 28.5  & 11.5 \\
    ``a photo of'' & ``the state of the object is'' & ``the class of the object is'' & 39.1  & 22.2  & 54.5  & \textbf{42.7} & 28.7  & 11.6 \\
    ``a photo of'' & ``the state is'' & ``the class is'' & 38.8  & 21.4  & 53.2  & 38.5  & 29.6  & 12.0 \\
    \end{tabular}%
  } \vspace{-2mm}
  \label{tab:ctx-initialization}%
\end{table*}%

\begin{figure}[!t]
  \centering
   \includegraphics[width=\linewidth]{figure/HP_lambda_AUC.pdf}
   \vspace{-7mm}
   \caption{%
   \textbf{Sensitivity analysis on initialization value of $\boldsymbol{\lambda}$.}
   }
   \label{fig:lambda-initialization}
   \vspace{-5mm}
\end{figure}

\vspace{-4mm}
\paragraph{Prefix initialization.}
In \cref{tab:ctx-initialization}, we report the results of trying several combinations of initialization for prompt prefixes from different branches, which confirm that \method might be marginally sensitive to the prefix initialization.
In our experiments, we have selected a simplest combination as the default initialization for convenience.

\section{Additional Ablation Study on Backbone}


\begin{table*}[!t]    %
\tablestyle{5pt}{1.0}
\setlength\tabcolsep{6pt}
\def\w{20pt}
  \caption{%
  \textbf{Ablation on backbone architecture of CLIP.}
  }\vspace{-2mm}
\scalebox{1}{
    \begin{tabular}{lc|cccc|cccc|cccc}
          &       & \multicolumn{4}{c|}{\textbf{MIT-States}} & \multicolumn{4}{c|}{\textbf{UT-Zappos}} & \multicolumn{4}{c}{\textbf{C-GQA}} \\
    \textbf{Method} & \textbf{Backbone} & S     & U     & HM    & AUC   & S     & U     & HM    & AUC   & S     & U     & HM    & AUC \\
    \shline
    CLIP~\cite{Radford:CLIP}  & ViT-B/32 & 25.1  & 39.1  & 21.4  & 7.5   & 9.6   & 42.4  & 10.0  & 2.4   & 7.3   & 22.1  & 7.4   & 1.2 \\
    CSP~\cite{Nayak:CSP}   & ViT-B/32 & 36.4  & 42.5  & 28.6  & 12.4  & 57.1  & 57.3  & 39.3  & 24.2  & 30.1  & 23.4  & 19.4  & 5.7 \\
    \rowcolor[rgb]{ .949,  .949,  .949} \textbf{\method (Ours)}  & ViT-B/32 & \textbf{39.5} & \textbf{42.8} & \textbf{30.5} & \textbf{13.9} & \textbf{60.5} & \textbf{67.4} & \textbf{47.3} & \textbf{32.3} & \textbf{36.3} & \textbf{27.2} & \textbf{24.4} & \textbf{8.4} \\
    \hline
    CLIP~\cite{Radford:CLIP}  & ViT-L/14 & 30.2  & 46.0  & 26.1  & 11.0  & 15.8  & 49.1  & 15.6  & 5.0   & 7.5   & 25.0  & 8.6   & 1.4 \\
    CSP~\cite{Nayak:CSP}   & ViT-L/14 & 46.6  & 49.9  & 36.3  & 19.4  & 64.2  & 66.2  & 46.6  & 33.0  & 28.8  & 26.8  & 20.5  & 6.2 \\
    \rowcolor[rgb]{ .949,  .949,  .949} \textbf{\method (Ours)}  & ViT-L/14 & \textbf{49.0} & \textbf{53.0} & \textbf{39.3} & \textbf{22.1} & \textbf{66.8} & \textbf{73.8} & \textbf{54.6} & \textbf{41.7} & \textbf{41.0} & \textbf{35.7} & \textbf{29.4} & \textbf{12.4} \\
    \end{tabular}%
  } \vspace{-2mm}
  \label{tab:ablation-backbone}%
\end{table*}%

\cref{tab:ablation-backbone} compares \method with other methods when using different ViT-based CLIP backbones.
Note that only CLIP~\cite{Radford:CLIP} and CSP~\cite{Nayak:CSP} are included in the comparison, as other CLIP-based methods have not reported the results with different backbones.
We can observe that our \method consistently outperforms the compared methods, and a larger backbone leads to better performance.

\section{Comparison with Existing Compositional Zero-Shot Learning Methods} \label{sec:full_comparison}

In this section, we present a more comprehensive comparison of our \method to demonstrate its superiority.

\vspace{-4mm}
\paragraph{Baselines.}
We compare \method with both CLIP-based methods~\cite{Radford:CLIP,Zhou:CoOp,Nayak:CSP,Xu:PromptCompVL,Lu:DFSP} and existing CZSL methods~\cite{Nagarajan:attributes-as-operators,Misra:red-wine-to-red-tomato,Purushwalkam:task-driven-modular-networks,Li:symmetry-and-group,Mancini:CompCos,Naeem:CGE,Mancini:Co-CGE,Li:SCEN,Anwaar:CVGAE,Karthik:KG-SP,Khan:CAPE} with a pre-trained ResNet-18~\cite{He:ResNet} backbone.
For CompCos~\cite{Mancini:CompCos} and Co-CGE~\cite{Mancini:Co-CGE}, we report the results of which version of the models according to the experimental setup, \textit{i.e.}, the closed-world version for the closed-world setting, and the open-world version for the open-world setting.

\vspace{-4mm}
\paragraph{Results.}
\cref{tab:closed-world-SOTA-full} reports the closed-world results and \cref{tab:open-world-SOTA-full} reports the open-world results.
Credit to the transferred pre-trained knowledge, we find that CLIP-based methods significantly outperform other CZSL methods in both settings.
And our proposed \method achieves the state-of-the-art performance in all cases.

\begin{table*}[!t]    %
\tablestyle{5pt}{1.0}
\setlength\tabcolsep{1pt}
\def\w{20pt} 
  \caption{\textbf{Closed-world results.} 
  For our \method, we report the average performance on 5 random seeds with standard error.}\vspace{-2mm}
\scalebox{1}{
    \begin{tabular}{lcccc|cccc|cccc}
    \multirow{2}[1]{*}{\textbf{Method}} & \multicolumn{4}{c|}{\textbf{MIT-States}} & \multicolumn{4}{c|}{\textbf{UT-Zappos}} & \multicolumn{4}{c}{\textbf{C-GQA}} \\
          & S     & U     & HM     & AUC   & S     & U     & HM     & AUC   & S     & U     & HM     & AUC \\
    \shline
    AoP~\cite{Nagarajan:attributes-as-operators}   & 14.3  & 17.4  & 9.9   & 1.6   & 59.8  & 54.2  & 40.8  & 25.9  & 17.0  & 5.6   & 5.9   & 0.7 \\
    LE+~\cite{Misra:red-wine-to-red-tomato}   & 15.0  & 20.1  & 10.7  & 2.0   & 53.0  & 61.9  & 41.0  & 25.7  & 18.1  & 5.6   & 6.1   & 0.8 \\
    TMN~\cite{Purushwalkam:task-driven-modular-networks}   & 20.2  & 20.1  & 13.0  & 2.9   & 58.7  & 60.0  & 45.0  & 29.3  & 23.1  & 6.5   & 7.5   & 1.1 \\
    SymNet~\cite{Li:symmetry-and-group} & 24.2  & 25.2  & 16.1  & 3.0   & 49.8  & 57.4  & 40.4  & 23.4  & 26.8  & 10.3  & 11.0  & 2.1 \\
    CompCos~\cite{Mancini:CompCos} & 25.3  & 24.6  & 16.4  & 4.5   & 59.8  & 62.5  & 43.1  & 28.1  & 28.1  & 11.2  & 12.4  & 2.6 \\
    CGE~\cite{Naeem:CGE}   & 28.7  & 25.3  & 17.2  & 5.1   & 56.8  & 63.6  & 41.2  & 26.4  & 28.1  & 10.1  & 11.4  & 2.3 \\
    Co-CGE~\cite{Mancini:Co-CGE} & 27.8  & 25.2  & 17.5  & 5.1   & 58.2  & 63.3  & 44.1  & 29.1  & 29.3  & 11.9  & 12.7  & 2.8 \\
    SCEN~\cite{Li:SCEN}  & 29.9  & 25.2  & 18.4  & 5.3   & 63.5  & 63.1  & 47.8  & 32.0  & 28.9  & 12.1  & 12.4  & 2.9 \\
    CVGAE~\cite{Anwaar:CVGAE} & 28.5  & 25.5  & 18.2  & 5.3   & 65.0  & 62.4  & 49.8  & 34.6  & 28.2  & 11.9  & 13.9  & 2.8 \\
    CAPE~\cite{Khan:CAPE} & 30.5  & 26.2  & 19.1  & 5.8   & 60.4  & 67.4  & 45.5  & 31.3  & 32.9  & 15.6  & 16.3  & 4.2 \\
    \hline
    CLIP~\cite{Radford:CLIP}  & 30.2  & 46.0  & 26.1  & 11.0  & 15.8  & 49.1  & 15.6  & 5.0   & 7.5   & 25.0  & 8.6   & 1.4 \\
    CoOp~\cite{Zhou:CoOp}  & 34.4  & 47.6  & 29.8  & 13.5  & 52.1  & 49.3  & 34.6  & 18.8  & 20.5  & 26.8  & 17.1  & 4.4 \\
    CSP~\cite{Nayak:CSP}   & 46.6  & 49.9  & 36.3  & 19.4  & 64.2  & 66.2  & 46.6  & 33.0  & 28.8  & 26.8  & 20.5  & 6.2 \\
    PromptCompVL~\cite{Xu:PromptCompVL} & 48.5  & 47.2  & 35.3  & 18.3  & 64.4  & 64.0  & 46.1  & 32.2  & - & - & - & - \\
    DFSP(i2t)~\cite{Lu:DFSP} & 47.4  & 52.4  & 37.2  & 20.7  & 64.2  & 66.4  & 45.1  & 32.1  & 35.6  & 29.3  & 24.3  & 8.7 \\
    DFSP(BiF)~\cite{Lu:DFSP} & 47.1  & 52.8  & 37.7  & 20.8  & 63.3  & 69.2  & 47.1  & 33.5  & 36.5  & 32.0  & 26.2  & 9.9 \\
    DFSP(t2i)~\cite{Lu:DFSP} & 46.9  & 52.0  & 37.3  & 20.6  & 66.7  & 71.7  & 47.2  & 36.0  & 38.2  & 32.0  & 27.1  & 10.5 \\
    \rowcolor[rgb]{ .949,  .949,  .949} \textbf{\method (Ours)} & \textbf{49.0}\tiny{$\pm$0.4} & \textbf{53.0}\tiny{$\pm$0.2} & \textbf{39.3}\tiny{$\pm$0.2} & \textbf{22.1}\tiny{$\pm$0.1} & \textbf{66.8}\tiny{$\pm$1.1} & \textbf{73.8}\tiny{$\pm$0.6} & \textbf{54.6}\tiny{$\pm$0.5} & \textbf{41.7}\tiny{$\pm$0.7} & \textbf{41.0}\tiny{$\pm$0.2} & \textbf{35.7}\tiny{$\pm$0.3} & \textbf{29.4}\tiny{$\pm$0.2} & \textbf{12.4}\tiny{$\pm$0.1} \\
    \end{tabular}%
    } \vspace{-2mm}
  \label{tab:closed-world-SOTA-full}%
\end{table*}%


\begin{table*}[!t]    %
\tablestyle{5pt}{1.0}
\setlength\tabcolsep{1pt}
\def\w{20pt}
\caption{\textbf{Open-world results.} 
  For our \method, we report the average performance on 5 random seeds with standard error.}\vspace{-2mm}
\scalebox{1}{
    \begin{tabular}{lcccc|cccc|cccc}
    \multirow{2}[1]{*}{\textbf{Method}} & \multicolumn{4}{c|}{\textbf{MIT-States}} & \multicolumn{4}{c|}{\textbf{UT-Zappos}} & \multicolumn{4}{c}{\textbf{C-GQA}} \\
          & S     & U     & HM     & AUC   & S     & U     & HM     & AUC   & S     & U     & HM     & AUC \\
    \shline
    AoP~\cite{Nagarajan:attributes-as-operators}   & 16.6  & 5.7   & 4.7   & 0.7   & 50.9  & 34.2  & 29.4  & 13.7  & - & - & - & - \\
    LE+~\cite{Misra:red-wine-to-red-tomato}   & 14.2  & 2.5   & 2.7   & 0.3   & 60.4  & 36.5  & 30.5  & 16.3  & 19.2  & 0.7   & 1.0   & 0.08 \\
    TMN~\cite{Purushwalkam:task-driven-modular-networks}   & 12.6  & 0.9   & 1.2   & 0.1   & 55.9  & 18.1  & 21.7  & 8.4   & - & - & - & - \\
    SymNet~\cite{Li:symmetry-and-group} & 21.4  & 7.0   & 5.8   & 0.8   & 53.3  & 44.6  & 34.5  & 18.5  & 26.7  & 2.2   & 3.3   & 0.43 \\
    CompCos~\cite{Mancini:CompCos} & 25.4  & 10.0  & 8.9   & 1.6   & 59.3  & 46.8  & 36.9  & 21.3  & 28.4  & 1.8   & 2.8   & 0.39 \\
    CGE~\cite{Naeem:CGE}   & 29.6  & 4.0   & 4.9   & 0.7   & 58.8  & 46.5  & 38.0  & 21.5  & 28.3  & 1.3   & 2.2   & 0.30 \\
    Co-CGE~\cite{Mancini:Co-CGE} & 26.4  & 10.4  & 10.1  & 2.0   & 60.1  & 44.3  & 38.1  & 21.3  & 28.7  & 1.6   & 2.6   & 0.37 \\
    KG-SP~\cite{Karthik:KG-SP} & 28.4  & 7.5   & 7.4   & 1.3   & 61.8  & 52.1  & 42.3  & 26.5  & 31.5  & 2.9   & 4.7   & 0.78 \\
    CVGAE~\cite{Anwaar:CVGAE} & 27.3  & 9.9   & 10.0  & 1.8   & 58.6  & 48.4  & 41.7  & 22.2  & 26.6  & 2.9   & 6.4   & 0.7 \\
    \hline
    CLIP~\cite{Radford:CLIP}  & 30.1  & 14.3  & 12.8  & 3.0   & 15.7  & 20.6  & 11.2  & 2.2   & 7.5   & 4.6   & 4.0   & 0.27 \\
    CoOp~\cite{Zhou:CoOp}  & 34.6  & 9.3   & 12.3  & 2.8   & 52.1  & 31.5  & 28.9  & 13.2  & 21.0  & 4.6   & 5.5   & 0.70 \\
    CSP~\cite{Nayak:CSP}   & 46.3  & 15.7  & 17.4  & 5.7   & 64.1  & 44.1  & 38.9  & 22.7  & 28.7  & 5.2   & 6.9   & 1.20 \\
    PromptCompVL~\cite{Xu:PromptCompVL} & 48.5  & 16.0  & 17.7  & 6.1   & 64.6  & 44.0  & 37.1  & 21.6  & - & - & - & - \\
    DFSP(i2t)~\cite{Lu:DFSP} & 47.2  & 18.2  & 19.1  & 6.7   & 64.3  & 53.8  & 41.2  & 26.4  & 35.6  & 6.5   & 9.0   & 1.95 \\
    DFSP(BiF)~\cite{Lu:DFSP} & 47.1  & 18.1  & 19.2  & 6.7   & 63.5  & 57.2  & 42.7  & 27.6  & 36.4  & 7.6   & 10.6  & 2.39 \\
    DFSP(t2i)~\cite{Lu:DFSP} & 47.5  & 18.5  & 19.3  & 6.8   & \textbf{66.8} & 60.0  & 44.0  & 30.3  & 38.3  & 7.2   & 10.4  & 2.40 \\
    \rowcolor[rgb]{ .949,  .949,  .949} \textbf{\method (Ours)} & \textbf{48.8}\tiny{$\pm$0.4} & \textbf{18.7}\tiny{$\pm$0.1} & \textbf{20.1}\tiny{$\pm$0.1} & \textbf{7.2}\tiny{$\pm$0.1} & 66.4\tiny{$\pm$1.0}  & \textbf{61.2}\tiny{$\pm$1.0} & \textbf{47.8}\tiny{$\pm$1.3} & \textbf{33.0}\tiny{$\pm$1.0} & \textbf{40.8}\tiny{$\pm$0.2} & \textbf{7.9}\tiny{$\pm$0.2} & \textbf{10.9}\tiny{$\pm$0.3} & \textbf{2.70}\tiny{$\pm$0.1} \\
    \end{tabular}%
  } \vspace{-2mm}
  \label{tab:open-world-SOTA-full}%
\end{table*}%
