\vspace{-1mm}
\section{Rethinking the Paradigm}

In this section, we first formalize the CZSL task and the visual feature extraction of CLIP~\cite{Radford:CLIP} (\cref{sec:preliminaries}).
Then, we introduce how previous works adapt CLIP by adjusting prompts for image-composition alignments (\cref{sec:framework-revisit}).
Finally, we present a \MP paradigm to guide the construction of VLM-based pipelines (\cref{sec:multi-path-paradigm}).

\subsection{Preliminaries} \label{sec:preliminaries}


\paragraph{CZSL Task Formulation.} 
Given the 
state set $\mathcal{S} = \{ s_1, s_2, \dots, s_{|\mathcal{S}|} \}$ and object set $\mathcal{O} = \{ o_1, o_2, \dots, o_{|\mathcal{O}|} \}$ as the primitive concepts, where $|\cdot|$ denotes the number of elements in the set, the compositional label space $\mathcal{C}$ is defined as
their Cartesian product,
\textit{i.e.}, $\mathcal{C} = \mathcal{S} \times \mathcal{O}$.
And the set of the seen and unseen compositions, denoted as $\mathcal{C}^{se}$ and $\mathcal{C}^{us}$, are the two disjoint subsets of $\mathcal{C}$, \textit{i.e.}, $\mathcal{C}^{se} \cap \mathcal{C}^{us} = \emptyset$.
To learn a model %
that assigns compositional labels from the target set $\mathcal{C}^{tgt}$ to the input images, a training set $\mathcal{T} = \{ (x_i, c_i) | x \in \mathcal{X}, c \in \mathcal{C}^{se} \}$ is provided, where $\mathcal{X}$ denotes the image space.    %
In the closed-world setting, the target set is defined as $\mathcal{C}^{tgt} = \mathcal{C}^{se} \cup \mathcal{C}^{us}$, where only the known composition space is considered.
And in the open-world setting, 
the target set
is all possible permutations of the state-object compositions, \textit{i.e.}, $\mathcal{C}^{tgt} = \mathcal{C}$. %


\vspace{-4mm}
\paragraph{Visual Feature Extraction.} 
Given the input image $x \in \mathbb{R}^{H \times W \times C}$, 
the image encoder $E_v$, implemented with ViT~\cite{Dosovitskiy:ViT}, first splits it into $N^p = HW/P^2$ non-overlapping patches, where $(P,P)$ is the resolution of each patch.
The patches are projected to form a sequence of patch tokens together with a pre-trained \texttt{[CLS]} token, where the pre-trained position embeddings are also added to preserve positional information.
Then, the encoder $E_v$ updates the token sequence $\mathbf{X} \in \mathbb{R}^{(N^p+1) \times d^{in}_v}$ with self-attention-based blocks.
Finally,
a single linear layer $g^{proj}$ with parameters $\mathbf{W}_g \in \mathbb{R}^{d^{in}_v \times d}$ projects the output \texttt{[CLS]} token, where $d$ is the dimension of the cross-modal latent space.
And the projected token $\mathbf{x}^{\texttt{CLS}} \in \mathbb{R}^{d}$ serves as the image representation.


\begin{figure}[!t]
  \centering
   \includegraphics[width=\linewidth]{figure/prompt_comparison-latest.pdf}
   \vspace{-7mm}
   \caption{%
   \textbf{Graphical comparison of prompts constructed by prior methods and \method.}
   \textcolor{red}{Red} tokens are trainable.
   }
   \label{fig:prompt_comparison}
   \vspace{-5mm}
\end{figure}


\begin{figure*}[!t]
  \centering
   \includegraphics[width=\linewidth]{figure/method-latest-v2.pdf}
   \vspace{-5mm}
   \caption{%
   \textbf{Overview of the proposed \method.}
   }
   \label{fig:method}
   \vspace{-4mm}
\end{figure*}

\subsection{A Revisit of Existing Framework} \label{sec:framework-revisit}

Taking a closer look into existing CLIP-based works~\cite{Nayak:CSP,Xu:PromptCompVL,Lu:DFSP}, we found that all of them build a single cross-modal alignment for inference, which yields the recognition probability $p(c_{i,j}|x)$ given the input image $x$ and the candidate pair $c_{i,j}=\langle s_i, o_j \rangle$. Since the frozen CLIP backbone has provided a well-established vision-language alignment, 
an essential step for these methods is to construct appropriate prompts for compositional labels.
Commonly,
initializing with the pre-trained embeddings from CLIP,
a new primitive \textit{vocabulary} $\mathbf{V} = [\mathbf{V}^{\mathcal{S}}, \mathbf{V}^{\mathcal{O}}] \in \mathbb{R}^{(|\mathcal{S}|+|\mathcal{O}|) \times d^{in}_t}$ is first built for all states and objects, where $d^{in}_t$ is the dimension of each vocabulary token.
Then,
a natural language \textit{prefix} such as ``\textit{a photo of}'' is transformed into tokens with the pre-trained embeddings.
Different from the prompt format adopted in the inference of CLIP, the CZSL methods~\cite{Nayak:CSP} append the prefix tokens to the state-object composition instead of the
class
placeholder, acquiring the prompt $\mathbf{P}_{i,j} = [\mathbf{p}_1, \dots, \mathbf{p}_m, \mathbf{v}^s_i, \mathbf{v}^o_j ]$ for the pair $c_{i,j}$, where $\{\mathbf{p}_1, \dots, \mathbf{p}_m\} \in \mathbb{R}^{m \times d^{in}_t}$ are the prefix tokens, and $\mathbf{v}^s_i, \mathbf{v}^o_j$ are the vocabulary tokens of $s_i$ and $o_j$.
By feeding $\mathbf{P}_{i,j}$ into the text encoder $E_t$, the prompt representation $\mathbf{t}_{i,j} \in \mathbb{R}^{d}$ is acquired to compute $p(c_{i,j}|x)$ by cosine similarity with image representation $\mathbf{x}^{\texttt{CLS}}$.
While earlier works~\cite{Nayak:CSP,Xu:PromptCompVL} simply turn the primitive vocabulary or prefix tokens from fixed to trainable, 
DFSP~\cite{Lu:DFSP} further decomposes the prompt representations into state and object ones to provide more supervision for training. 
However, the inference still relies on a single path for estimating the composition probability.




\subsection{Generalizing in Multi-Path Paradigm} \label{sec:multi-path-paradigm}


As already discussed in the introduction, 
existing methods still suffer from limitations in knowledge transfer and generalization.
We believe that the issue stems from the applied single-path paradigm, and thus present a novel \MP paradigm.
An intuitive comparison of the existing and proposed paradigms is illustrated in \cref{fig:multi-path}.
Crucially, the \MP paradigm requires a recognition branch for each of the three semantic components, \textit{i.e.}, state, object, and composition.
These branches are essentially cross-modal alignments that independently unearth specific knowledge from large-scale vision-language pre-training.


Specifically, during training, three branches can collectively optimize the parameters in a multi-task learning~\cite{Zhang:multi-task-learning} manner.
And for inference, the prediction results of state and object can be incorporated to assist the composition branch, thereby mitigating the excessive bias towards seen compositions. %
Without further implementation constraints,
the flexibility of the \MP paradigm allows for the derivation of new methods, which can freely exploit various multi-modal features extracted by the powerful VLMs.



























\section{Troika: An Implementation of Multi-Path Paradigm}

In this section, we first detail how the proposed \method
develops a range of instantiations, following the \MP paradigm.
After that, we introduce the plug-and-play \CMT module for \method.
An overview of \method is illustrated in \cref{fig:method}.

\subsection{Instantiations}

\paragraph{Learning Prompt Representations.} 
Since the composition prompt can only elicit information related to seen compositions from VLMs, \method respectively conducts prompts for the state, object, and composition branch.
Compared to the DFSP~\cite{Lu:DFSP} approach that decomposes the text features into state and object ones, 
individual prompts for different semantic components activate the backbone from input, thus maximizing the exploitation of pre-trained knowledge.
Moreover, as the semantic roles of the target classes on each branch are different, it is natural to introduce different priors through special contexts.
Therefore, while maintaining the same primitive vocabulary as a cue of semantic compositionality, we employ an independent prompt prefix for each branch of \method.
For each state-object pair $c_{i,j}=\langle s_i, o_j \rangle$, the state prompt $\mathbf{P}^s_i$, object prompt $\mathbf{P}^o_j$, and composition prompt $\mathbf{P}^c_{i,j}$ can be constructed as
{\setlength\abovedisplayskip{1mm}
\setlength\belowdisplayskip{2mm}
\begin{align}
\mathbf{P}^s_i &= [ \mathbf{p}^s_1, \dots, \mathbf{p}^s_m, \mathbf{v}^s_i ], \\
\mathbf{P}^o_j &= [ \mathbf{p}^o_1, \dots, \mathbf{p}^o_m, \mathbf{v}^o_j ], \\
\mathbf{P}^c_{i,j} &= [ \mathbf{p}^c_1, \dots, \mathbf{p}^c_m, \mathbf{v}^s_i, \mathbf{v}^o_j ],
\end{align}}%
where $\{\mathbf{p}^s_1, \dots, \mathbf{p}^s_m\}$, $\{\mathbf{p}^o_1, \dots, \mathbf{p}^o_m\}$, and $\{\mathbf{p}^c_1, \dots, \mathbf{p}^c_m\}$ are the learnable state prefix, object prefix, and composition prefix, respectively.
These fully trainable prompts are then fed into the text encoder $E_t$ to obtain the prompt representation for each branch, formulated as
{
\setlength\abovedisplayskip{2mm}
\setlength\belowdisplayskip{2mm}
\begin{align}
\mathbf{t}^s_i = E_t(\mathbf{P}^s_i),\quad \mathbf{t}^o_j = E_t(\mathbf{P}^o_j),\quad \mathbf{t}^c_{i,j} = E_t(\mathbf{P}^c_{i,j}).
\end{align}}

\vspace{-5mm}
\paragraph{Learning Visual Representations.} 
While existing methods~\cite{Nayak:CSP,Xu:PromptCompVL,Lu:DFSP} directly apply the frozen image encoder, 
we first trial several easy-to-implement and effective PETL techniques.
Based on the experimental results, we finally introduce Adapter~\cite{Houlsby:Adapter} to adapt the image encoder without updating its original parameters.
These small neural modules (adapters) can also be freely replaced by more complicated techniques to pursue further enhancements.
Then, to establish cross-modal alignment on each branch, specific visual features for the composition, state, and object should be extracted.
Still treating the image representation $\mathbf{x}^{\texttt{CLS}}$ as the composition visual representation $\mathbf{x}^c$, we introduce the state disentangler $D^s$ and object disentangler $D^o$ to decompose the state and object visual features $\mathbf{x}^s$ and $\mathbf{x}^o$
as
{
\setlength\abovedisplayskip{2mm}
\setlength\belowdisplayskip{2mm}
\begin{align}
\mathbf{x}^s = D^s(\mathbf{x}^{\texttt{CLS}}),\quad \mathbf{x}^o = D^o(\mathbf{x}^{\texttt{CLS}}),\quad \mathbf{x}^c = \mathbf{x}^{\texttt{CLS}},
\end{align}}%
where $D^s$ and $D^o$ are implemented with two individual MLPs.
This simple structure provides the necessary non-linear mapping to resolve primitive-specific features from entangled global features.

\vspace{-5mm}
\paragraph{Training.} 
Given the prompt representations and visual features specified with different branches, the probability of assigning labels of the state $s_i$, object $o_j$, and composition $c_{i,j}$ to the image can be computed separately as
{\setlength\abovedisplayskip{2mm}
\setlength\belowdisplayskip{2mm}
\begin{align} 
p(s_i|x) &= \frac{\exp(\mathbf{x}^s \cdot \mathbf{t}^s_i / \tau)}{\sum^{|\mathcal{S}|}_{k=1}\exp(\mathbf{x}^s \cdot \mathbf{t}^s_k / \tau)}, \\
p(o_j|x) &= \frac{\exp(\mathbf{x}^o \cdot \mathbf{t}^o_j / \tau)}{\sum^{|\mathcal{O}|}_{k=1}\exp(\mathbf{x}^o \cdot \mathbf{t}^o_k / \tau)}, \\
p(c_{i,j}|x) &= \frac{\exp(\mathbf{x}^c \cdot \mathbf{t}^c_{i,j} / \tau)}{\sum^{|\mathcal{C}^{tgt}|}_{k=1}\exp(\mathbf{x}^c \cdot \mathbf{t}^c_k / \tau)},
\end{align}}%
where $\tau \in \mathbb{R}$ is the pre-trained temperature parameter from CLIP.
In each branch, the cross-entropy loss encourages the model to explicitly recognize the corresponding semantic role, described as
{\setlength\abovedisplayskip{2mm}
\setlength\belowdisplayskip{2mm}
\begin{align}
\mathcal{L}^s = - \frac{1}{|\mathcal{X}|} \sum_{x \in \mathcal{X}} \log p(s|x), \\
\mathcal{L}^o = - \frac{1}{|\mathcal{X}|} \sum_{x \in \mathcal{X}} \log p(o|x), \\
\mathcal{L}^c = - \frac{1}{|\mathcal{X}|} \sum_{x \in \mathcal{X}} \log p(c|x).
\end{align}}
Therefore, the overall training loss $\mathcal{L}_{all}$ is defined as
{\setlength\abovedisplayskip{2mm}
\setlength\belowdisplayskip{2mm}
\begin{align} \label{eq:overall_loss}
\mathcal{L}_{all} = \alpha^s \mathcal{L}^s + \alpha^o \mathcal{L}^o + \alpha^c \mathcal{L}^c,
\end{align}}%
where $\alpha^s, \alpha^o, \alpha^c \in \mathbb{R}$ are weighting coefficients to balance the influences of different losses.
Note that we have omitted the weight decay here for simplicity.

\vspace{-5mm}
\paragraph{Inference.} 
During inference, the primitive predictions are combined to complement the composition prediction, thus promoting a more robust recognition system.
Formally, the integrated composition probability $\widetilde{p}(c_{i,j}|x)$ is defined as
{\setlength\abovedisplayskip{2mm}
\setlength\belowdisplayskip{2mm}
\begin{align} \label{eq:inference}
\widetilde{p}(c_{i,j}|x) = p(c_{i,j}|x) + p(s_i|x) \cdot p(o_j|x).
\end{align}}%
And \method predicts the most likely composition as
{\setlength\abovedisplayskip{2mm}
\setlength\belowdisplayskip{2mm}
\begin{align} \label{eq:predict}
\hat{c} = \mathop{\arg \max}\limits_{c_{i,j} \in \mathcal{C}^{tgt}} \left(\widetilde{p}(c_{i,j}|x)\right).
\end{align}}

\begin{table*}[!t]    %
\tablestyle{5pt}{1.0}
\setlength\tabcolsep{1pt}
\def\w{20pt} 
  \caption{%
  \textbf{Main results on three benchmarks.} 
  All methods use a CLIP ViT-L/14 backbone.
  For our \method, we report the average performance on 5 random seeds with standard error.
  \vspace{-3mm}}
\scalebox{1}{
    \begin{tabular}{lcccc|cccc|cccc}
    \multirow{2}[0]{*}{\textbf{Method}} & \multicolumn{4}{c|}{\textbf{MIT-States}} & \multicolumn{4}{c|}{\textbf{UT-Zappos}} & \multicolumn{4}{c}{\textbf{C-GQA}} \\[-1.5pt]
          & S     & U     & HM     & AUC   & S     & U     & HM     & AUC   & S     & U     & HM     & AUC \\
    \shline
    \multicolumn{13}{c}{ \demph{ \it{\textbf{Closed-world} Results} } }\\
    \hline
    CLIP~\cite{Radford:CLIP}  & 30.2  & 46.0  & 26.1  & 11.0  & 15.8  & 49.1  & 15.6  & 5.0   & 7.5   & 25.0  & 8.6   & 1.4 \\
    CoOp~\cite{Zhou:CoOp}  & 34.4  & 47.6  & 29.8  & 13.5  & 52.1  & 49.3  & 34.6  & 18.8  & 20.5  & 26.8  & 17.1  & 4.4 \\
    CSP~\cite{Nayak:CSP}   & 46.6  & 49.9  & 36.3  & 19.4  & 64.2  & 66.2  & 46.6  & 33.0  & 28.8  & 26.8  & 20.5  & 6.2 \\
    PromptCompVL~\cite{Xu:PromptCompVL} & 48.5  & 47.2  & 35.3  & 18.3  & 64.4  & 64.0  & 46.1  & 32.2  & - & - & - & - \\
    DFSP(i2t)~\cite{Lu:DFSP} & 47.4  & 52.4  & 37.2  & 20.7  & 64.2  & 66.4  & 45.1  & 32.1  & 35.6  & 29.3  & 24.3  & 8.7 \\
    DFSP(BiF)~\cite{Lu:DFSP} & 47.1  & 52.8  & 37.7  & 20.8  & 63.3  & 69.2  & 47.1  & 33.5  & 36.5  & 32.0  & 26.2  & 9.9 \\
    DFSP(t2i)~\cite{Lu:DFSP} & 46.9  & 52.0  & 37.3  & 20.6  & 66.7  & 71.7  & 47.2  & 36.0  & 38.2  & 32.0  & 27.1  & 10.5 \\
    \rowcolor[rgb]{ .949,  .949,  .949} \textbf{\method (Ours)} & \textbf{49.0}\tiny{$\pm$0.4} & \textbf{53.0}\tiny{$\pm$0.2} & \textbf{39.3}\tiny{$\pm$0.2} & \textbf{22.1}\tiny{$\pm$0.1} & \textbf{66.8}\tiny{$\pm$1.1} & \textbf{73.8}\tiny{$\pm$0.6} & \textbf{54.6}\tiny{$\pm$0.5} & \textbf{41.7}\tiny{$\pm$0.7} & \textbf{41.0}\tiny{$\pm$0.2} & \textbf{35.7}\tiny{$\pm$0.3} & \textbf{29.4}\tiny{$\pm$0.2} & \textbf{12.4}\tiny{$\pm$0.1} \\
    \hline\\[-2.4ex]
    \multicolumn{13}{c}{ \demph{ \it{\textbf{Open-world} Results} } }\\
    \hline
    CLIP~\cite{Radford:CLIP}  & 30.1  & 14.3  & 12.8  & 3.0   & 15.7  & 20.6  & 11.2  & 2.2   & 7.5   & 4.6   & 4.0   & 0.27 \\
    CoOp~\cite{Zhou:CoOp}  & 34.6  & 9.3   & 12.3  & 2.8   & 52.1  & 31.5  & 28.9  & 13.2  & 21.0  & 4.6   & 5.5   & 0.70 \\
    CSP~\cite{Nayak:CSP}   & 46.3  & 15.7  & 17.4  & 5.7   & 64.1  & 44.1  & 38.9  & 22.7  & 28.7  & 5.2   & 6.9   & 1.20 \\
    PromptCompVL~\cite{Xu:PromptCompVL} & 48.5  & 16.0  & 17.7  & 6.1   & 64.6  & 44.0  & 37.1  & 21.6  & - & - & - & - \\
    DFSP(i2t)~\cite{Lu:DFSP} & 47.2  & 18.2  & 19.1  & 6.7   & 64.3  & 53.8  & 41.2  & 26.4  & 35.6  & 6.5   & 9.0   & 1.95 \\
    DFSP(BiF)~\cite{Lu:DFSP} & 47.1  & 18.1  & 19.2  & 6.7   & 63.5  & 57.2  & 42.7  & 27.6  & 36.4  & 7.6   & 10.6  & 2.39 \\
    DFSP(t2i)~\cite{Lu:DFSP} & 47.5  & 18.5  & 19.3  & 6.8   & \textbf{66.8} & 60.0  & 44.0  & 30.3  & 38.3  & 7.2   & 10.4  & 2.40 \\
    \rowcolor[rgb]{ .949,  .949,  .949} \textbf{\method (Ours)} & \textbf{48.8}\tiny{$\pm$0.4} & \textbf{18.7}\tiny{$\pm$0.1} & \textbf{20.1}\tiny{$\pm$0.1} & \textbf{7.2}\tiny{$\pm$0.1} & 66.4\tiny{$\pm$1.0}  & \textbf{61.2}\tiny{$\pm$1.0} & \textbf{47.8}\tiny{$\pm$1.3} & \textbf{33.0}\tiny{$\pm$1.0} & \textbf{40.8}\tiny{$\pm$0.2} & \textbf{7.9}\tiny{$\pm$0.2} & \textbf{10.9}\tiny{$\pm$0.3} & \textbf{2.70}\tiny{$\pm$0.1} \\
    \end{tabular}%
  }\vspace{-2mm}
  \label{tab:main-SOTA}%
\end{table*}%

\subsection{Cross-Modal Traction} \label{sec:cross-modal-traction}






Although inheriting the rich cross-modal understanding of VLMs, discrepancies may still exist between semantically similar vision-language representations.
Given the same semantic concept, the static and monotonous prompt representation naturally fails to be commonly optimal for all input images that 
come
from a plentiful distribution.
This issue becomes more serious in the additional state and object branches, as the visual content of the same primitive changes considerably when paired with different primitives. %
Therefore,
we further develop a \CMT module for \method.
The module adaptively shifts the prompt representation to accommodate the content diversity and diminish the cross-modal discrepancies.
In this process, relevant patch features serve as the guidance to avoid noise from semantic-agnostic sub-regions interfering with the traction.


Specifically, the \CMT module is composed of a stack of $N$ blocks, and in each block, we first consider a scaled dot product attention mechanism~\cite{Vaswani:Transformer} with the prompt representation attending to all patch tokens.
Given the input prompt representation $\mathbf{t}$ that comes from an arbitrary branch, we first acquire the patch tokens $\mathbf{X}^p \in \mathbb{R}^{N^p \times d}$ after projecting them with the linear layer $g^{proj}$.
Then, the query, key and value can be derived as
{\setlength\abovedisplayskip{2mm}
\setlength\belowdisplayskip{2mm}
\begin{align}
\mathbf{q} = \mathbf{t} \mathbf{W}_q,\quad \mathbf{K} = \mathbf{X}^p \mathbf{W}_K,\quad \mathbf{V} = \mathbf{X}^p \mathbf{W}_V,
\end{align}}%
where $\mathbf{W}_q, \mathbf{W}_K, \mathbf{W}_V \in \mathbb{R}^{d \times d^{attn}}$ are the parameter matrices, and $d^{attn}$ is the dimension of the single-head attention.
The dot product attention gives relevance weights from $\mathbf{t}$ to each patch token, which are used to aggregate the value-projected patch tokens as 
{\setlength\abovedisplayskip{2mm}
\setlength\belowdisplayskip{2mm}
\begin{align}
\overline{\mathbf{t}} = \text{Attention}(\mathbf{q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left( \frac{\mathbf{q}\mathbf{K}^{\top}}{\sqrt{d^{attn}}}\right)\mathbf{V}.
\end{align}}%
In practice, a multi-head design with $h=d/d^{attn}$ parallel attention heads is naturally introduced to diversify representation subspaces.
After the attention layer, a feed-forward network $\text{FFN}$, implemented as a MLP, is introduced as
{\setlength\abovedisplayskip{2mm}
\setlength\belowdisplayskip{2mm}
\begin{align}
\widetilde{\mathbf{t}} = \text{FFN}(\overline{\mathbf{t}}) = \sigma(\overline{\mathbf{t}}\mathbf{W}_1 + \mathbf{b}_1)\mathbf{W}_2 + \mathbf{b}_2,
\end{align}}%
where $\mathbf{W}_1, \mathbf{W}_2$ are parameter matrices, $\mathbf{b}_1, \mathbf{b}_2$ are bias terms, and $\sigma(\cdot)$ is a nonlinear activation function.
Note that for both the attention and the feed-forward network, we omit the residual connections around them for simplicity.
Then, we can update the prompt representation as
{\setlength\abovedisplayskip{2mm}
\setlength\belowdisplayskip{2mm}
\begin{align}
\mathbf{t} \leftarrow \mathbf{t} + \boldsymbol{\lambda} \cdot \widetilde{\mathbf{t}},
\end{align}}%
where $\boldsymbol{\lambda} \in \mathbb{R}^d$ is a trainable parameter vector controlling the strength of the cross-modal traction in each dimension.
$\widetilde{\mathbf{t}}$ can be viewed as a traction that pulls $\mathbf{t}$ towards the visual content.
And the whole module can be seamlessly inserted into each branch before calculating the cross-modal matching probability.
In practice, all three branches share the same module to reduce the parameter overhead.
