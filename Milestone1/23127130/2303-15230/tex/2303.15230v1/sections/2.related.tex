\vspace{-3mm}
\section{Related Work}


\paragraph{Compositional Zero-Shot Learning (CZSL).}
Aiming to recognize 
unseen state-object pairs at test time while each semantic primitive exists in training samples, early CZSL efforts can be broadly divided into two lines.
One line of works~\cite{Misra:red-wine-to-red-tomato,Purushwalkam:task-driven-modular-networks,Naeem:CGE,Mancini:Co-CGE,Anwaar:CVGAE} learns the combined state-object semantic representation for both seen and unseen compositions with a transformation function, \textit{e.g.}, a multi-layer perceptron (MLP)~\cite{Misra:red-wine-to-red-tomato} or a graph convolutional network~\cite{Naeem:CGE}.
Another line of works~\cite{Li:symmetry-and-group,Mancini:CompCos,Karthik:KG-SP,Li:SCEN,Tian:IVR} learns two individual classifiers to identify state and object separately from the image features.
While both lines build the connection between visual features and compositional labels from scratch,
recent works focus on transferring the encyclopedic knowledge from pre-trained VLMs.
CSP~\cite{Nayak:CSP} first 
adapts
the CLIP model~\cite{Radford:CLIP} by replacing the classes in textual prompts with trainable state and object tokens.
PromptCompVL~\cite{Xu:PromptCompVL} creates a fully learnable soft prompt including the prefix, state, and object.
The latest DFSP~\cite{Lu:DFSP} proposes a cross-modal decomposed fusion module to learn more expressive image features by fusing with the decomposed language features.
In this work, by proposing the \MP paradigm for CZSL with VLMs, we highlight the importance of jointly and explicitly modeling the state, object, and composition.
The paradigm is flexible to generate new methods, and we provide an implementation \method that successfully demonstrates its potential.





\vspace{-5mm}
\paragraph{Parameter-Efficient Transfer Learning (PETL).}
PETL refers to updating only a small number of pre-trained or additional parameters during fine-tuning~\cite{He:unify-PET,Ding:delta-tuning}, 
which reduces the training and storage burdens.
As a popular PETL technique,
\textit{prompt tuning}~\cite{Liu:prompt-tuning,Lester:prompt-tuning,Liu:P-Tuning-deep} optimizes learnable tokens inserted into the input token sequence while freezing the backbone.
CLIP-based CZSL methods~\cite{Nayak:CSP,Xu:PromptCompVL,Lu:DFSP} continue the vein of prompt tuning by tuning both the inserted prefix and the primitive vocabulary tokens, 
adapting the encoder to downstream semantics.
While following the \MP paradigm to establish three independent identification branches, 
our \method constructs branch-specific prompts with individual prefixes and a shared primitive vocabulary.
\cref{fig:prompt_comparison} illustrates the differences in prompt design between \method and existing methods, and experimental results in \cref{sec:ablation-study} show that our design benefits from modeling prior knowledge specified with semantic roles.



In this work, we also attempt to introduce PETL techniques to the vision side, implemented as Adapter~\cite{Houlsby:Adapter}.
Since only tuning the text encoder while ignoring the image encoder is naturally considered insufficient, the studies of adapting both encoders for multi-modal tasks including image recognition~\cite{Zang:UPT,Khattak:MaPLe} and video-text retrieval~\cite{Jiang:Cross-Modal-Adapter,Huang:VoP} have recently emerged.
On these tasks, completely adapting both encoders has been proved to mutually promote the alignment of vision-language modalities.
For the first time, we empirically demonstrate that it is equally valid for CZSL.
