\documentclass[jcp, floatfix, nobibnotes, reprint, superscriptaddress]{revtex4-1}
\usepackage{docs}%
\usepackage{siunitx}
\usepackage{amsmath}
\usepackage{booktabs}
\DeclareSIUnit[number-unit-product = {\,}]
\cal{cal}
\DeclareSIUnit\kcal{\kilo\cal}
\DeclareSIUnit[number-unit-product = {\,}]
\Btu{Btu}
\DeclareSIUnit[number-unit-product = {\,}]
\Fahr{\degree F}
\DeclareSIUnit[number-unit-product = {\,}]
%\usepackage{subcaption}
\usepackage{bm}%
%\usepackage{subcaption}
\usepackage[colorlinks=true,linkcolor=black]{hyperref}%
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{tabularx} % provides a column type called "X" that should satisfy your professed need to have several equal-width columns

\usepackage{dcolumn}
\usepackage{epstopdf}
\usepackage{afterpage}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{dsfont}
\usepackage{sidecap}
\usepackage{epstopdf}

\usepackage{siunitx} % align numbers in tables w.r.t. decimal point
\usepackage{color, colortbl}

\definecolor{Gray}{gray}{0.9}

\epstopdfDeclareGraphicsRule{.gif}{png}{.png}{convert gif:#1 png:\OutputFile}
\AppendGraphicsExtensions{.gif}

%\usepackage{booktabs,caption,fixltx2e}
%\usepackage[flushleft]{threeparttable}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\newcommand{\Danish}[1]{\textrm{\textcolor{blue}{Danish: #1}}}
\newcommand\atsign{@}

\setcitestyle{super}
%\usepackage[super]{natbib}
%https://tex.stackexchange.com/questions/8467/superscript-citations-without-brackets-with-the-revtex4-1-document-class
%this makes it superscript but no brackets so far

\begin{document}
\setstretch{1.0}


\title[]{Quantum machine learning at record speed:  \\
Many-body distribution functionals as compact representations}


%Many Body Distribution Functional (Density) Representations for Quantum Machine Learning}

\author{Danish Khan}

\affiliation{Department of Chemistry, University of Toronto, St. George Campus, Toronto, ON, Canada}
\affiliation{Vector Institute for Artificial Intelligence, Toronto, ON, M5S 1M1, Canada}

\author{Stefan Heinen}
\affiliation{Vector Institute for Artificial Intelligence, Toronto, ON, M5S 1M1, Canada}

\author{O. Anatole von Lilienfeld}
\email{anatole.vonlilienfeld@utoronto.ca}
\affiliation{Vector Institute for Artificial Intelligence, Toronto, ON, M5S 1M1, Canada}
\affiliation{Departments of Chemistry, Materials Science and Engineering, and Physics, University of Toronto, St. George Campus, Toronto, ON, Canada}
\affiliation{Machine Learning Group, Technische Universit\"at Berlin and Institute for the Foundations of Learning and Data, 10587 Berlin, Germany}
\begin{abstract}

The feature vector mapping used to represent chemical systems
is a  key factor governing the superior data-efficiency of
kernel based quantum machine learning (QML) models applicable throughout chemical compound space. 
%For kernel ML models, a key factor underpinning both their accuracy and computational cost is the form of the feature vector mapping used to represent the chemical system.
%In principle, a feature vector of the order $4N$ for any $N$ atom system should be sufficient. 
Unfortunately, the most accurate representations require a high dimensional feature mapping, thereby imposing a considerable computational burden on model training and use.
We introduce compact yet accurate, linear scaling QML representations based on atomic Gaussian many-body distribution functionals (MBDF), and their derivatives. % and their frequencies.
%Our representation significantly reduces the feature vector dimensionality and, by extension, the computational cost of the resulting models. 
%Herein we tackle this problem by using functionals of atomic Gaussian distributions to form efficient atomic representations that significantly reduce this dimensionality.
Weighted density functions (DF) of MBDF values are used as global representations which are constant in size, i.e.~invariant with respect to the number of atoms. 
%We further generate a density function of this atomic representation that does not scale with the number of atoms to test how well this information can be compressed into a compact form.
We report predictive performance and training data efficiency that is close to state of the art for two diverse datasets of organic molecules, QM9 and QMugs.
Generalization capability has been investigated for atomization energies, HOMO-LUMO eigenvalues and gap, internal energies at 0 K, zero point vibrational energies, dipole moment norm, static isotropic polarizability, and heat capacity as encoded in QM9. 
MBDF based QM9 performance lowers the optimal Pareto front spanned between sampling and training cost to compute node minutes,~effectively sampling chemical compound space with chemical accuracy at a speed of $\sim 37$ molecules per core second.
%We also show that when using non-compact representations, the train/test time of kernel models is restricted by the kernel evaluation step which depends on the representation size.
%Furthermore, since the train/test time of kernel models is restricted by the kernel evaluation step, it depends on the representation size.
%We show that using our compact representation this bottleneck can be removed at $\sim$ 100k training set size for both small and large molecules.
\end{abstract}
\maketitle
\section{Introduction}
Modern data-driven statistical Machine Learning (ML) models have emerged as powerful tools over the past decade for inferring quantum mechanical observables throughout chemical compound space, without explicitly solving electronic Schrödinger equations\cite{CM, QMLessayAnatole, felix_google}.
Similar success was obtained for ML based interatomic potentials and force-fields\cite{Rabitz1996,NN_Tucker2006,Behler-Parrinello_NN,GAP,PES_NN,ANI-1}
as well as electronic structure modeling throughout Chemical Compound Space (CCS)\cite{ML4Kieron2012,ML4Graeme2012}. 
For an entire set of extensive in-depth reviews on these and other related ML applications, we refer the reader to the recent special issue in Chemical Reviews.\cite{ceriotti2021editorial, ceriotti_clementi_anatole_jcp2021}
%ML models are surrogate which all have in common that they exploit similarities between query and training instances.
%Given sufficient training data, Quantum Machine Learning (QML) models\cite{QMLessayAnatole}
%applicable across CCS have been shown to reach the accuracy of popular quantum chemistry methods such as density functional theory (DFT) for many molecular properties while incurring orders of magnitude less computational cost\cite{felix_google}.
Various aspects in the development of ML model architecture and training protocols have proven to be essential for data-efficiency. In particular, the molecular representation is known to have a strong impact on the performance of similarity based ML models, such as kernel ridge regression (KRR)\cite{FourierDesc., desc_role_Scheffler, physics-inspired-reps-ceriotti}.  
This is not surprising as the representation controls the information about the systems, how its weighed and the consistency of these Quantum Machine Learning (QML) models with ab-\textit{initio} methods~\cite{Ramakrishnan_vonLilienfeld_2015}.
These representations are non-linear mappings of the atomistic systems to a suitable Hilbert Space where a statistical regression model can easily be applied. 
The Hilbert Space constraint applies due to the requirement of measuring similarity in terms of an inner product\cite{Vapnik1998}.
These mappings should have some desirable features of which the most important are i) uniqueness such that systems with different properties necessarily possess different representations\cite{FourierDesc.}and 
ii) invariance with respect to transformations that leave the target property invariant, such as global translations, rotations, and atomic index permutations of the same chemical elements. Other desirable features include
iii) an analytical and continuous form of the representation function,
iv) differentiability with respect to nuclear coordinates, charges, number of electrons, and number of atoms,
v) as general as the Hamiltonian, 
vi) computationally efficient evaluation,
vii) compact or even constant size, 
limiting  the computational cost for larger systems\cite{const_size}. 
\\
Due to their critical role, many representations have been introduced and investigated within the context of atomistic simulation\cite{acsf,OM,mbtr,wacsf, ace,pip,wst,mtp,nice,gaussian_moments,kresse2020}. 
For recent comprehensive reviews, the reader is referred to Refs.~\cite{reps_review_Rupp,physics-inspired-reps-ceriotti}.
These representations can either describe the molecule as a whole (global) or each atom (local or atomic) separately. 
For the sake of brevity we restricted most of our comparative benchmarks within this study to  the following representations which are commonly used to train QML models throughout CCS:
Faber-Christensen-Huang-Lilienfeld (FCHL19) representation\cite{fchl19,fchl18}, 
Smooth Overlap of Atomic Positions (SOAP)\cite{soap},
spectrum of London and Axilrod–Teller–Muto potentials (SLATM)\cite{amons_slatm}, 
atom index sorted Coulomb matrix (CM)\cite{CM}, 
and its vectorized form, Bag of Bonds (BOB)\cite{bob}. Other representations/models tested are mentioned in the data and code section.
\\
While these representations satisfy most of the aforementioned qualities, seen the immense size of CCS, a more compact and scalable representation would still be desirable. 
Formally, the number of degrees of freedom of any material or molecule would prescribe usage of a $4M$ dimensional feature vector (3 spatial coordinates and one nuclear charge coordinate for each of the atoms). 
However, all aforementioned representations when using optimal hyperparameters require a higher dimensional feature vector mapping in order to be accurate and training-data-efficient at regression tasks, and some (e.g. CM or BOB) even scale quadratically with $M$.
While verbosity facilitates the inclusion of invariances, 
the $4M$ degrees of freedom suggest that the same performance can be obtained using more compact representations.
This is especially an issue for kernel based ML models, where the size of the representation directly affects the distance/kernel evaluation time\cite{Vapnik1998, fchl19}. 
Although the scaling for kernel inversion\cite{gpr_rasmussen} is larger ($\propto \mathcal{O}(n^{3})$ for Cholesky solvers), 
for highly data-efficient (i.e. efficient in training data) QML models it is the kernel generation and evaluation that consumes the most compute time as demonstrated later.
The kernel evaluation pre-factor becomes even worse when using atomic (or local) representations in conjunction with a suitable local kernel\cite{local_kernels_ceriotti}.
Obvious solutions by reducing finite local cutoffs within the representation come at the expense of reducing the predictive power, or, conversely, increasing training data needs. As such, a computationally efficient yet accurate solution is desirable.  
\\\\
Herein, we propose a methodology for generating representations that minimize feature size.
We use functionals of many-body distributions (MBDFs) and their derivatives to encode any local chemical environment of any atom in an integrated compact fashion.
The representations thus generated preserve the system's various symmetries (translation, rotation, atom index invariance), and can be tailored to the physical property of interest through scaling functions. 
MBDFs are easily extendable to include higher order many-body interactions with minimal increase in size. 
In the current formulation, while including three-body interactions, MBDF scales as $5M$. 
We further tackle the issue of storing this information in a manner that remains invariant to the number of atoms in a molecule. 
We do this by generating a discretized density (DF) function of MBDF values, and using it as a global molecular representation.
Using two diverse datasets the performance of MDBF is tested against aforementioned SOAP and FCHL19, which are state-of-the-art, as well as SLATM, BOB, CM representations and a few other QML models
mentioned later.
Lastly, we explore the bottleneck cross-over from kernel evaluation to inversion. 
%MBDF based QML models, kernel inversion becomes the new computational bottleneck with respect to model evaluation efficiency. 
%explore why such a compact representation could be attractive given the aforementioned, pre-existing library of well performing and elegant molecular representations. 








\section{Theory and Discussion}
\subsection{Many-Body Distribution Functionals}
The starting point of our local representation is a smoothed atomic density in the internal coordinates which can be constructed using atomic coordinates.

An analytical and continuous distribution over the pair-wise internal coordinate defined as the inter-atomic distances (pair correlation function), is easily built using Gaussian probability density functions (PDFs) centered at each inter-atomic distance with respect to an atom $i$:
\begin{align}
    \rho_{i}(r,R_{ij})=\frac{1}{\sigma_{r}\sqrt{2\pi}}\sum_{j\neq i}^{M}Z_{j}\exp{\left(-{\frac{(r-R_{ij})^{2}}{\sigma_{r}}}\right)}
    \label{eq:rdf}
\end{align}
where $\rho_{i}(r,R_{ij})$ is the normalized distribution with atom $i$ as the origin, $\sigma_{r}$ is the Gaussian length-scale (or variance) parameter, $M$ denotes the total number of atoms in the system, $R_{ij}$ denotes the inter-atomic distance between atoms $i$ and $j$ and $Z_{j}$ is the nuclear charge. 
Scaling by nuclear charges defines elemental identities, but could also be done in other ways e.g. having different length-scale parameters $\sigma_{r}$ for each unique chemical element, or multiple dimensions such as period and group specifications~\cite{Elpasolite_2016}. 
In a similar fashion, a continuous distribution (triplet correlation function) over the 3-body internal coordinate, inter-atomic angles  $\theta$, is defined as:
\begin{align}
    \rho_{i}(\theta,\theta_{jik})=\frac{1}{\sigma_{\theta}\sqrt{2\pi}}\sum_{j\neq i}^{M}\sum_{k\neq j}^{M}Z_{j}Z_{k}~\exp{\left(-\frac{(\theta-\theta_{jik})^{2}}{\sigma_{\theta}}\right)}, \nonumber
    \\
    \theta_{jik} = \cos^{-1}\frac{(\mathbf{R}_{i}-\mathbf{R}_{j})^{T}(\mathbf{R}_{i}-\mathbf{R}_{j})}{R_{ij}^{2}}
    \label{eq:adf}
\end{align}
where $\theta_{jik}$ is the inter-atomic angle centered on atom $i$.
This can be generalized to define a continuous distribution, or correlation function, over any $m$-body internal coordinate $\tau$.
\\
We define the MBDF representation as functionals of these $m$-body distributions. 
With the 2- and 3-body distributions defined above, each atom can then be represented by the zeroth-order functionals:
\begin{align}
    F_{0}^{(2)}[i] =\int_{0}^{r_{c}} dr~g_{0}(r,R_{ij})~\rho_{i}(r,R_{ij})
    \label{eq:F02}
\end{align}
\begin{align}
    F_{0}^{(3)}[i] =\int_{0}^{\pi}d\theta~ g_{0}(\theta,R_{ij},R_{jk},R_{ik})~\rho_{i}(\theta,\theta_{jik}) 
    \label{eq:F03}
\end{align}
where $r_{c}$ denotes the radial cut-off distance, and $g_{0}(r,R_{ij})$ and $g_{0}(\theta,R_{ij},R_{jk},R_{kj})$ denote 2- and 3-body weighting functions. 
Note that when the weighting functions $g_{0}(r,R_{ij})$ and $g_{0}(\theta,R_{ij},R_{jk},R_{kj})$ correspond to suitable 2 and 3-body potentials, the functionals $F_{0}^{(2)}$, $F_{0}^{(3)}$ become the average of the corresponding 2, 3-body interactions weighted by the pair and triplet correlation functions $\rho_{i}(r,R_{ij})$, $\rho_{i}(\theta,\theta_{jik})$, respectively. 
These functionals then form a coarse approximation to the average 2, 3-body interactions experienced by a chemical species. Furthermore, we exploit the advantage of using the infinite differentiability of Gaussian PDFs to define higher order functionals such as:

\begin{align}
    F_{1}^{(2)}[i] =\int_{0}^{r_{c}} dr~g_{1}(r,R_{ij})~\frac{\partial \rho_{i}}{\partial r }(r,R_{ij})
    \label{eq:F12}
\end{align}
\begin{align}
    F_{1}^{(3)}[i] =\int_{0}^{\pi} d\theta~g_{1}(\theta,R_{ij},R_{jk},R_{ik})~\frac{\partial \rho_{i}}{\partial \theta }(\theta,\theta_{jik}) 
\end{align}

with potentially different weighting functions $g_{1}(r)$, $g_{1}(\theta,R_{ij},R_{jk},R_{kj})$. 
The derivative functionals are useful since the functional of any arbitrary distribution is not unique. 
These functionals also encode the change in the $m$-body distribution in an atom's local neighborhood. 
For any $n$-th derivative of the 2-body distribution we can define the functional:
\begin{align}
    F_{n}^{(2)}[i] =\int_{0}^{r_{c}}dr~g_{n}(r) ~\partial_{r}^{n}\rho_{i}(r,R_{ij}), \nonumber
    \\
    \partial_{r}^{n}\rho_{i}(r) = \frac{\partial^{n} \rho_{i}}{\partial r^{n}}(r,R_{ij})
    \label{eq:Fn2}
\end{align}
where $g_{n}(r)$ is, again, a suitable radial weighting function.
Generalizing this to all internal coordinates, a functional $F_{n}^{(m)}[i]$ can be defined over the $n$-th derivative of any $m$-body distribution function centered at atom $i$:
\begin{multline}
    F_{n}^{(m)}[i] =\int_{0}^{\tau_{c}} d\tau~g_{n}(\tau) ~\partial_{\tau}^{n}\rho_{i}(\tau)\\
    = \int_{0}^{\tau_{c}}d\tau~g_{n}(\tau) ~H_{n}(\tau)~\rho_{i}(\tau)
    \label{eq:Fnm}
\end{multline}
where $\tau$ denotes the $m$-body internal coordinate, $\rho_{i}(\tau)$ is the $m$-body distribution function w.r.t atom $i$, $g_{n}(\tau)$ is the weighting function for the $n$-th derivative of $\rho_{i}(\tau)$ and $H_{n}(\tau)$ denotes the Hermite polynomial of degree $n$. 
The Hermite polynomials arise due to the use of Gaussian PDFs and allow convenient computation of $n$ derivatives of the distribution function at any point $\tau$.\\


We note here that an alternative way to describe a (bounded) distribution would be through a moment expansion of the form:
\begin{align}
    G^{(m)}[i] = \int_{0}^{\tau_{c}}d\tau~ \tau^{m} g_{m}(\tau) \rho_{i}(\tau)
    \label{eq:Fnm_moments}
\end{align}
where $G^{(m)}[i]$ denotes the $m$-th moment of the distribution centered at atom $i$. 
The set of $m$ moments $G^{(m)}[i]$ would then form the local representation of the atom $i$. 
These moments can also be evaluated by placing a set of Gaussians (or any basis functions) on each atom $i$, and then evaluating the moments w.r.t each atomic position within a radial cutoff:
\begin{align}
    G^{(m)}[i] = \int_{0}^{\mathbf{r}_{c}}d\mathbf{r}~|\mathbf{r}-\mathbf{R_{i}}|^{m}g_{m}(\mathbf{r})\sum_{j\epsilon S(\mathbf{r}_{c})}\exp{\left(-\frac{||\mathbf{r}-\mathbf{R_{j}}||_{2}^{2}}{2\sigma^{2}}\right)}
    \label{eq:moments_general}
\end{align}
where $|.|$ is any metric and $j\epsilon S(\mathbf{r}_{c})$ implies that the summation is over all atoms within a sphere of radius $\mathbf{r}_{c}$ centered at atom $i$.
Throughout this work, however, we use the derivative formalism in eq. \eqref{eq:Fnm}, since our numerical tests indicated superior performance. 
%with the note that the moments expansion will be assessed in a future work.
\\

We have tested multiple weighting functions to identify the best combination. In particular, since these functionals correspond to correlation function averages of $m$-body interactions, we have tested the Harmonic, Morse\cite{morse}, Lennard-Jones\cite{lennard-jones2} potentials and simple decaying functions (power laws, exponential, gaussian decays and their combinations) for 2-body terms. 
For 3-body terms we have tested Cosine Harmonic,  Axilrod-Teller\cite{axilrod_teller}, Stillinger-Weber\cite{stillinger-weber}  potentials and a scaled Fourier series.
Through trial and error combined with cross-validated hyper-parameter optimization, we have identified the following 5 suitable functionals corresponding to 2 and 3-body distributions:
\begin{align}
    F_{0}^{(2)}[i]=&
    \int_{0}^{r_{c}}
    dr~ \left[e^{-\eta r}-\frac{\sqrt{2}\pi}{10(r+1)^{3}}\right]~\rho_{i}(r,R_{ij})
    \label{eq:F02}\\
    F_{1}^{(2)}[i]
    =&\frac{\sqrt{2}\pi}{10}
    \int_{0}^{r_{c}}dr~
    \frac{H_{1}(r-R_{ij})}{(r+1)^{6}}~\rho_{i}(r,R_{ij})
    \label{eq:F12}\\
    F_{2}^{(2)}[i]
    =&\int_{0}^{r_{c}}
    dr~e^{-\alpha r} H_{2}(r-R_{ij}) 
    ~\rho_{i}(r,R_{ij})
    \label{eq:F22}\\
    F_{0}^{(3)}[i]
    =&\int_{0}^{\pi}
    d\theta~\frac{\sum_{n=0}^{3} a_{n}\cos(n\theta)}{(R_{ij}R_{jk}R_{ik})^{4}}~\rho_{i}(\theta,\theta_{jik}) 
    \label{eq:F03}\\
    \begin{split}
    F_{1}^{(3)}[i]
    =&\int_{0}^{\pi}d\theta~
    \left[\frac{1+\cos(\theta)\cos(\theta_{kji})\cos(\theta_{ikj})}{(R_{ij}R_{jk}R_{ik})^{4}}\right]\\
    &\times H_{1}(\theta-\theta_{jik})~\rho_{i}(\theta,\theta_{jik}) 
    \end{split}
\label{eq:F13}
\end{align}
where $\eta$, $\alpha$, $a_{n}$ and the various power laws are all hyperparameters of the representation. 
Note the scaling functions of MBDF contributions $F_0^{(2)}, F_1^{(2)},F_0^{(3)},F_1^{(3)}$ being respectively reminiscent of Buckingham type-potential, softened London dispersion potential, Fourier series scaled by Lennard-Jones repulsion and Axilrod-Teller-Muto potential scaled by Lennard-Jones repulsion. 
The specific reason as to why this particular selection of weighting functions has proven advantageous will be subject of future research. 

Figure \ref{fig:fig2} shows the effect of each functional on the learning capacity of MBDF for the task of predicting atomization energies of the QM9\cite{qm9} dataset.
\begin{figure}[h!]
          \centering          
          \includegraphics[width=\columnwidth]{figs/Functionals_progression_local.jpeg}
          \caption{MBDF based QML learning curves using concatenated increasingly higher order and body functionals (Eqs.~(\ref{eq:F02}-\ref{eq:F13})). Mean absolute error (MAE) of predicting atomization energies of the QM9~\cite{qm9} dataset is shown as a function of training set size $N$.                    }
     \label{fig:fig2}
 \end{figure}
It is apparent that both the derivative and many-body terms improve the learning capacity, albeit by different magnitudes. 

Throughout our testing on the QM7\cite{qm7}, QM9\cite{qm9} and QMugs\cite{QMugs} datasets, we have found these 2, 3-body functionals to be sufficient at discriminating between all of the molecular structures. 
Cases where the 2-body information does not suffice include homometric pairs, 
as already discussed many years ago~\cite{FourierDesc.}.
And even 3- and 4-body information does not suffice for some cases, as recently discussed in Ref.~\cite{incompleteness_ceriotti}. 
We note here that, whenever necessary, arbitrarily higher order derivative and many-body information could also be included in MBDFs at minimal increase in size, i.e.~one additional term per order (See eq. \eqref{eq:Fnm}).
In particular, we believe that the inclusion of the 4-body term as a functional of the dihedrals could further improve the learning capacity for conformational isomers. 
Inclusion of 4-body information has been shown to result in further improvements of learning curves~\cite{communication_bing}.
%It would also allow the representation to discriminate between molecular conformers only differing in torsional angles, a feature that is lacked by most other representations (with finite cutoffs) discussed earlier \cite{incompleteness_ceriotti}.
Also note that the size of MBDF is invariant to the cutoffs used which can be raised to arbitrarily higher values while employing a suitable long-range functional (hence increasing the farsightedness of the representation) without affecting the kernel evaluation cost. 
We further note that other weighting functions and response terms could also be useful for QML models of other physical observables such as dipole moments, vibrational frequencies, heat capacities etc.
%Due to the central role of the energy in terms of the Hamiltonian, and for the sake of brevity, we have restricted our function space search to the weighting functions mentioned earlier in this proof-of-principle study. 


\subsection{Density of functionals}
MBDF is a local representation and its size  scales linearly with the number of atoms in the system.
%This scaling can be eliminated by transforming
In order to eliminate this scaling we can transform to the frequency space of MBDF functional values.
The frequencies can be evaluated by normalizing the functional values to lie within an arbitrary range and then using, for e.g., kernel density estimation\cite{parzen_density}. 
For a finite set of MBDF functional values $\{X_{i}\}_{i=1}^{5M}$ over the range $[a,b]$, a "smooth histogram" of their frequencies can be constructed by placing a set of kernel functions $K$ at each point $X_{i}$:
\begin{align}
    f(u) = \frac{1}{5M}~\sum_{i=1}^{5M}~K(u,X_{i})
    \label{eq:kde}
\end{align}
where $f(u)$ gives the density of the samples at any point $u ~\epsilon ~[a,b]$. 
If the set $\{X_{i}\}_{i=1}^{5M}$ are the MBDF functional values for any molecule, their distribution density can be evaluated using eq. \eqref{eq:kde}.
The density $f(u)$ can then be used as a global molecular representation whose size is independent of the number of MBDF functional values, and number of atoms by extension. The (dis-)similarity between two molecules $A$ and $B$ can be evaluated as, e.g., the l2-distance:
\begin{align}
    d(A,B)^2 = \int_{-c}^{c} du~|f_{A}(u)-f_{B}(u)|^{2}
    \label{eq:dist_kde}
\end{align}
where $[-c,c]$ is the normaliztion range chosen as $[-10,10]$ in our work. The form of the density function used in our work is:
\begin{align}
    f_{A}(u) = \frac{1}{5M}~\sum_{i=1}^{5M}~\frac{\sqrt{|X_{i}|}}{\sqrt{2\pi}\sigma_{b}}~\exp{\left(-\frac{(u-X_{i})^2}{2\sigma_{b}^{2}}\right)}
    \label{eq:kde2}
\end{align}
where $X_{i}$ are MBDF functionals for molecule $A$, $\sigma_{b}$ is the variance of the Gaussian function and is a hyperparameter (also called bandwidth). Comparing with eq. \eqref{eq:kde}, the function $K$ is defined as: 
\begin{align}
    K(u,X_{i}) := \frac{\sqrt{|X_{i}|}}{\sqrt{2\pi}\sigma_{b}}~\exp{\left(-\frac{(u-X_{i})^2}{2\sigma_{b}^{2}}\right)}
    \label{eq:ku}
\end{align}
Note that this is a divergence\cite{amari2000methods} but not a kernel function because it is asymmetric: 
\begin{align}
    K(x,y) = &\frac{\sqrt{|y|}}{\sqrt{2\pi}\sigma_{b}}~\exp{\left(-\frac{(x-y)^2}{2\sigma_{b}^{2}}\right)}
    \nonumber
    \\
    \begin{split}
     \neq &\frac{\sqrt{|x|}}{\sqrt{2\pi}\sigma_{b}}~\exp{\left(-\frac{(x-y)^2}{2\sigma_{b}^{2}}\right)} = K(y,x)
    \end{split}
    \label{eq:div}
\end{align}
This function is used because it weights the MBDF functional frequencies by the functional value itself resulting in the distance measurement (eq. \eqref{eq:dist_kde}) being weighted by the difference in functional values.
Another advantage of this function is that it eliminates the frequency of null values (or "ghost atoms") within the MBDF representation which might be present due to the procedure of zero-padding\cite{CM}. 
\\
In our work we generate a separate density function $f(u)$ for each of the 5 MBDF functionals in eq. (11-15), and for each unique chemical element present in the dataset.
These are then concatenated to form the global representation of the molecule.
Alternatively, it could be done by using multivariate Gaussian functions for the density estimation. Let $\mathbf{x}_{i}$ denote the 5-dimensional vector of MBDF functional values (eq. 11-15) for any atom $i$ in molecule $A$. Then the multivariate density function $f_{A}(\mathbf{u})$ of this molecule can be evaluated as :
\begin{align}
    f_{A}(\mathbf{u}) = \frac{1}{M}~\sum_{i=1}^{M}~K(\mathbf{u},\mathbf{x}_{i})
    \label{eq:vector_kde}
\end{align}
\begin{align}
    f_{A}(\mathbf{u}) = \frac{1}{M}\sum_{i=1}^{M}\frac{(\mathbf{x}_{i}^{T}\mathbf{x}_{i})^{1/4}}{\sqrt{2\pi}\sigma_{b}}\exp{\left(-\frac{(\mathbf{u}-\mathbf{x}_{i})^{T}(\mathbf{u}-\mathbf{x}_{i})}{2\sigma_{b}^{2}}\right)}
    \label{eq:vector_kde2}
\end{align}
The l2-distance between molecules $A$ and $B$ then takes the form:
\begin{multline}
    d(A,B)^2 = \int d\mathbf{u}~(f_{A}(\mathbf{u})-f_{B}(\mathbf{u}))^{T}(f_{A}(\mathbf{u})-f_{B}(\mathbf{u}))
    \label{eq:dist_vector_kde}
\end{multline}
where the integral is over the normalization region.
Since the former method generates a more compact representation we have chosen to work with it. The abbreviation DF (Density of functionals) will be used throughout for this global representation.\\

\subsection{Numerical analysis}
The DF method allows generating a representation that does not scale with the number of atoms in the system. 
However, in order to use it as a feature vector the density functions have to be discretized.
Through convergence testing we have set the grid spacing to 0.2 throughout our work.
However, we note that this grid spacing could be changed for a different data-set in order to achieve the desirable accuracy vs computational cost trade-off. 
Furthermore, DF corresponds to a flattened feature vector which can be used with global kernels (or other ML methods), and which exhibits superior performance when compared to a straightforward concatenation of all MBDF rows (see Figure \ref{fig:flat_mbdf}).
The flattened MBDF representation is generated by sorting the MBDF matrix of each molecule by the row norm, and then flattening the matrix by concatenation of the rows to vectorize it.\cite{CM}
\begin{figure}[htb]
          \centering
          \includegraphics[width=\columnwidth]{figs/flat_mbdf.jpeg}
     \caption{QML learning curves for MBDF with local kernel, DF (global), and flattened MBDF (global and sorted by row norm). Mean absolute error (MAE) of predicting atomization energies of the
            QM9~\cite{qm9} data-set as a function of training set size N.
          }
    \label{fig:flat_mbdf}
 \end{figure}
 
Figure 3 shows molecular fingerprints generated using the 1 and 5 functional DF representations for three diverse and relevant organic  molecules (glucose, uric acid, and testosterone) on the same grid. 
For each molecule, a distinct fingerprint is obtained, with peak-positions depending  on the local chemical environment of each atom. Consequently, peaks of atoms with chemically similar environments are located closer to each other.
Peak heights encode both number and type (because of the density estimate being weighted) of chemical environments [See Eq.~\ref{eq:ku}]. 
Figure 3 demonstrates that for molecules with increasing size, corresponding DF based fingerprints will grow in magnitude, not in size.
In the SI, we also show how DF fingerprints distinguish conformational isomers, as exemplified for the chair and boat conformations of cyclohexane.

\begin{figure*}[htb]
          \centering
          \includegraphics[width=\linewidth]{figs/glucose_uric_testosterone2.jpeg}\label{fig:fig3a}
          \caption{Two-body (left) and three-body (right) DF versions (Eqs.~\ref{eq:F02}, \ref{eq:F03}) representations for Glucose, Uric Acid, and Testosterone. 
          }
 \end{figure*}


\section{Methods and Data}
\subsection{Kernel ridge regression}
The ML method that we focus on, and use throughout this work, is the supervised learning method called Kernel ridge regression\cite{Vapnik1998, gpr_rasmussen} (KRR). This method has been covered extensively earlier\cite{CM,fchl18,fchl19,felix_google,QMLessayAnatole} so we skip the details here.

The kernel functions\cite{Deisenroth2020,anatolebook} we use in our work along with global representations are the Gaussian kernel,
\begin{align}
    k(\mathbf{x}_I, \mathbf{x}_J) = \exp{\left( -\frac{\vert \vert \mathbf{x}_I - \mathbf{x}_J \vert \vert^{2}_{2}}{2 \sigma^2} \right)}
    \label{eq:kernel_gaussian}
\end{align}
and Laplacian kernel,
\begin{align}
    k(\mathbf{x}_I, \mathbf{x}_J) = \exp{\left( -\frac{\vert \vert \mathbf{x}_I - \mathbf{x}_J \vert \vert_{1}}{ \sigma} \right)}
    \label{eq:kernel_laplacian}
\end{align}
where $\mathbf{x}_I$ denotes the representation vector of molecule $I$.
\\
The kernel function used for the local representations FCHL19, SOAP and MBDF is a summation of atomic kernels:
\begin{align}
    k(\mathbf{M}_I, \mathbf{M}_J) = \sum_{a\epsilon i}\sum_{b\epsilon j} k^{l}(\mathbf{x}_{Ia}, \mathbf{x}_{Jb})
    \label{eq:kernel_local}
\end{align}
with the local Gaussian kernel:
\begin{align}
    k^{l}(\mathbf{x}_{Ia}, \mathbf{x}_{Jb}) = \mathbf{\delta}_{Z_{a},Z_{b}}  \exp{\left( -\frac{\vert \vert \mathbf{x}_{Ia} - \mathbf{x}_{Jb} \vert \vert^{2}_{2}}{2 \sigma^2} \right)}~
    \label{eq:local_gaussian}
\end{align}
or the local Laplacian kernel:
\begin{align}
    k^{l}(\mathbf{x}_{Ia}, \mathbf{x}_{Jb}) =  \mathbf{\delta}_{Z_{a},Z_{b}}  \exp{\left( -\frac{\vert \vert \mathbf{x}_{Ia} - \mathbf{x}_{Jb} \vert \vert_{1}}{ \sigma} \right)}~
    \label{eq:local_laplacian}
\end{align}
where $\mathbf{M}_I$ denotes the representation matrix of molecule $I$, $\mathbf{x}_{Ia}$ denotes the representation vector of atom $a$ within molecule $I$ and $\mathbf{\delta}_{Z_{a},Z_{b}} $ denotes a Kronecker Delta over the nuclear charges $Z_{a},Z_{b}$ which restricts the similarity measurement between atoms of the same chemical element\cite{fchl19}.
Other kernel functions will also be tested in the future.\cite{wasserstein,metric_learning,local_kernels_ceriotti}
\\
Throughout this study, we evaluate performance of ML methods through learning curves for the task of predicting physical properties of molecular systems. Learning curves quantify the model prediction error $\varepsilon$ (often measured as mean absolute error (MAE)) against the number of training samples $N$ and are key to understand the efficiency of ML models. 
It is generally known\cite{Vapnik1998,vapnik1994learningcurves,StatError_Muller1996} that they are linear on a log-log scale,
\begin{align}
    \log{ \left( \varepsilon \right) }   \approx I +  S \log{(N)}~
    \label{eq:log_learning}
\end{align}
where $I$ is the initial error and $S$ is the slope indicating model improvement given more training data. 
We also note that according to the central limit theorem the distribution of the errors $\epsilon$ approaches the normal distribution with standard deviation $\frac{\sigma}{\sqrt{N}}$, and mean 0 as $N\xrightarrow{} \infty$. Hence eq.~\eqref{eq:log_learning} becomes :
\begin{align}
    \log{ \left( \varepsilon \right) }   \approx \log(\sigma) -  \frac{1}{2} \log{(N)}
    \label{eq:log_learning_slope_half}
\end{align}
%Therefore, the expected slope $S$ of such a learning curve corresponds to $-\frac{1}{2}$.

\subsection{Hyperparameter Optimization} 
The current form of the representations has been optimized for Kernel based learning models. 
It depends on the weighting functions used and a number of hyperparameters which include variances ($\sigma_r, \sigma_\theta$) of the Gaussian PDFs, weighting function hyperparameters mentioned in eq. \eqref{eq:F02}-\eqref{eq:F13}, and bandwidth $\sigma_{b}$ for the DF representation. 
The hyperparameter optimization was done on a random subset of two thousand molecules from the QM7 dataset\cite{qm7}, and then kept fixed for all other data-sets. 
We note that further improvements might be possible if they had been
optimized simultaneously on all data-sets. 
The weighting functions $g_{n}(\tau)$ in eq. \eqref{eq:Fnm} 
were chosen by straightforward screening of the functions mentioned earlier. 
The optimization minimized the atomization energy prediction errors on the QM7 subset using Gaussian Process (GP) based Bayesian Optimization (BOpt)\cite{gpr_rasmussen}.
Starting with a Gaussian prior, the method fits a posterior distribution over the objective function using successive function evaluations. 
The posterior distribution is then used to construct an efficient acquisition function which can be optimized using, for instance, a quasi-Newton method to determine the next query point. 
Table 1 shows the optimized hyperparameter values used throughout this work. 
\begin{table}
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 Parameter & Value  \\ 
 \hline
 $\sigma_{r}$ & 1  \\ 
 $\sigma_{\theta}$ & 2  \\
 $\eta$ & 10.8 \\
 $\alpha$ & 1.5 \\
 $a_{0}, a_{1}, a_{2}, a_{3}$ & 3, 100, -200, -164 \\
 $\sigma_{b}$ & 0.07 \\
 $r_{c}$ (Å) & 6 \\
 \hline
\end{tabular}
\caption{MBDF and DF hyperparameters after optimization on atomization energies of QM7~\cite{qm7} subset.}
\end{center}
\end{table}

We used the scikit-optimize\cite{scikit-optimize} implementation of GP based BOpt using the default Mat\'ern kernel with unit variance and the limited memory BFGS optimizer\cite{lbfgs} for the acquisition function. 
In order to enable comparison to other representations (such as FCHL or SOAP) that rely on distance cutoffs, we have chosen to set inter-atomic distance cutoff $r_{c}$ for MBDF  to 6 Å  throughout this work.
We note that larger cutoffs for MBDF would not change its size. 

All MBDF functionals were evaluated using the trapezoidal numerical integration method.
The grid spacing for discretizing DF densities has been set to 0.2 throughout our work as noted earlier.
The bandwidth $\sigma_{b}$ = 0.07 was found to work well on the QM7 subset however it is recommended to be screened once (in the range $\sim$ [0.01,1]) along with the grid-spacing when using with other datasets.
The Numpy\cite{numpy} and Numba\cite{numba} libraries are used in the representation generation code.
\subsection{Data and Code}
The QM9 dataset\cite{qm9} consists of $\sim$134k small organic molecules with up to 9 heavy atoms (C, N, O, F). The calculations were performed at the B3LYP/6-31G(2df,p)\cite{b3,lyp,631g2dfp} level of theory.\\
QMugs is a dataset containing $\sim$665k biologically and pharmacologically relevant drug-like molecules.
It consists of larger molecules than QM9 with up to 100 heavy atoms (C, N, O, F, P, S, Cl, Br, or I) per molecule.
The training and predictions were performed on the DFT ($\omega$B97X-D/def2-SVP)\cite{wb97xd,def2} values reported in the dataset.
The QMugs subsets we used for Figure \ref{fig:qmugs} were drawn at random and consist of 20k molecules. 
Throughout, we used zero-padding for all representations studied in order to accommodate training and test molecules smaller than the maximum present in the data. 

In order to keep the FCHL19 and SOAP kernel evaluations computationally tractable, 
we have (a) 
restricted ourselves to a maximum 100 atoms per QMugs-molecule,
and (b)
reduced the default hyperparameters of the FCHL19 and SOAP representations to nRs2 = 12, nRs3 = 10, $r_{cut}$ = 6$\mathrm{\AA}$ and $n_{max}$ = $l_{max}$ = 3, $\sigma$ = 0.1, $r_{cut}$ = 6$\mathrm{\AA}$, respectively. 
For consistency, we used the same parameters for all other results reported in this article.
Note that the latter choice of hyperparameters negligibly deteriorates the predictive accuracy for QM9 (as assessed below and when comparing to the published prediction errors on QM9).
For FCHL19 and SOAP based prediction errors reported here within for QMugs, it could still be that the accuracy could improve further if these parameters were optimized. 

Figure \ref{fig:qm9} also includes results for QML models based on the 2- (k = 2) and 3-body (k = 3) Many-Body Tensor Representations (MBTR)\cite{mbtr} and a variant\cite{mbsf} of the Atom Centered Symmetry Functions\cite{acsf} (ACSF) as implemented in the QMLcode library\cite{qml}. 
The MBTR representations were generated with the same hyperparameters as those used in Ref.\cite{reps_review_Rupp} for the 10k training point on the QM9 dataset.

Throughout our work, the FCHL19, SLATM and ACSF representations were generated using the QMLcode library\cite{qml},
SOAP was generated using the Dscribe library\cite{dscribe} with the default spherical gaussian type orbitals as the radial basis functions, 
MBTR was generated using the qmmlpack library\cite{qmmlpack} and SchNet\cite{schnet} was built and trained using the SchNetPack\cite{schnetpack} libray.
For FCHL19, SOAP and ACSF we employed the local Gaussian (eq. \eqref{eq:local_gaussian}), for MBDF we use the local Laplacian (eq. \eqref{eq:local_laplacian}), for SLATM, MBTR and DF we use the global Gaussian (eq. \eqref{eq:kernel_gaussian}) and for CM, BOB we use the global Laplacian (eq. \eqref{eq:kernel_laplacian}) kernels respectively.
These choices were made based on the best performances for each representation.
All kernel evaluations were performed using the QMLcode library.

\begin{figure*}[htb]
          \centering           
          \includegraphics[width=\columnwidth]{figs/qm9_atomization2.jpeg}
          \includegraphics[width=\columnwidth]{figs/pareto100k.jpeg}
          \caption{MBDF/DF performance and comparison on atomization energies from QM9 data set (drawn at random from $\sim$134k organic molecules)~\cite{qm9}. 
          a) (Left) Mean absolute error (MAE) of prediction as a function of training set size for representations CM~\cite{CM}, BOB~\cite{bob}, MBTR~\cite{mbtr}, SLATM~\cite{amons_slatm}, PI~\cite{PI_townsend}, ACSF~\cite{acsf,mbsf}, FCHL19~\cite{fchl19}, SOAP~\cite{soap}. Numbers in legend denote representation size (feature vector dimensions), G and L denote Global and Local kernels, respectively.
          b) (Right) Timing for training and testing as a function of training set size  
          required for making chemically accurate (MAE = 1 kcal/mol) predictions on 100k molecules. 
          Blue, red, and green points indicate local kernels, global kernels, and neural network, respectively. Dashed gray line corresponds to the optimal Pareto front.
          For SchNet~\cite{schnet}, an Nvidia RTX 3070 GPU has been used. All other timings 
          were evaluated on a compute node equipped
          with a 24-core AMD EPYC 7402P @ 3.3 GHz CPU and 512 GB RAM. 
          Timings for FCHL18~\cite{fchl18}, BOB, CM are estimated using smaller kernels 
          (not taking into account kernel inversion). 
          Asterisk denotes representations with reduced hyperparameters used in this work. 
          $N$ values for ACSF, MBTR, BOB, CM estimated via extrapolation. Numbers in brackets denote year of publication. 
          }
          %ssand 16-core AMD Ryzen 9 7950x @5.7 GHz CPU.}
     \label{fig:qm9}
 \end{figure*}

 \begin{figure}[htb]
          \centering           
          \includegraphics[width=\columnwidth]{figs/qmugs_atomization2.jpeg}
          \caption{MBDF/DF performance and comparison to CM~\cite{CM}, BOB~\cite{bob}, SLATM~\cite{amons_slatm}, FCHL19~\cite{fchl19}, and SOAP~\cite{soap} representation based atomization energy estimates using the QMugs ($\sim$667k organic molecules with up to one hundred heavy atoms) data set~\cite{QMugs}. Training and testing data drawn at random. 
          Prediction mean absolute errors (MAE) on holdout test set of 4k molecules shown as a function of training set size. Numbers in legend denote representation size (feature vector dimensions), G and L denote Global and Local kernels respectively. Shaded region denotes the standard deviation across 4 different learning curves (except for FCHL19 and SOAP for which only one learning curve was tractable). }
     \label{fig:qmugs}
 \end{figure}

\section{Results}
\subsection{Atomization Energies}
\subsubsection{QM9} 
Figure \ref{fig:qm9} a) shows learning curves for QM9 and the size (dimension of feature vector) of the representation arrays in the legend. 
For the task of predicting atomization energies, local representations have previously been shown to be more efficient\cite{fchl19,fchl18}. 
Results for the local representations FCHL19 and SOAP are closely reproduced, and reach chemical accuracy after training on less than 10k molecules~\cite{fchl19}.
Among the global representations, SLATM has previously also been shown\cite{fchl19,amons_slatm} to perform quite well reaching chemical accuracy after training on $\sim$9k molecules although it shows a smaller slope at larger training sizes. 
This is closely followed by MBDF which reaches chemical accuracy after training on $\sim$14k molecules ($\sim$10\% of the dataset). 
The global DF representation also performs decently well reaching chemical accuracy at $\sim$32k training data. 
The local ACSF representation shows a larger offset but a better slope and it reaches chemical accuracy at $\sim$50k training set size.
We note here that for consistency with the other representations used in this work, we did not optimize the hyperparameters of the MBTR representation on every training point, 
%as was done in Ref.\cite{reps_review_Rupp}, 
but rather kept them fixed throughout.
Only the KRR hyperparameters were optimized at each training set size as with all other representations used here.
The 3-body MBTR reaches chemical accuracy at $\sim$60k training set size while the 2-body MBTR performs better than the other 2-body representations, BOB and CM.
We have also included the recently introduced constant size Persistence Images\cite{PI_townsend}(PI) representation for comparison. 
%unsure whether the default hyperparameters of this representation are suited to learning ground state physical properties of molecules.

Note that MBDF has the smallest size, requiring only 5 numbers per atom (145 dimensions for QM9 molecules).
By contrast, other local representations such as FCHL19, SOAP require $\sim$400 numbers per atom, while ACSF uses a 150 dimensional feature vector per atom. 
Encouragingly, and despite its compact size, MBDF outperforms most of the other larger representations with the exception of SOAP, SLATM, and FCHL.
We note here that while the size of the global DF representation is larger than MBDF, it utilises a global kernel implying training and prediction cost invariance with respect to system size.
%The MBDF representation uses a local kernel which requires a larger number of distance evaluations.

This compactness of the representation translates into faster ML model evaluation timings.
This is shown in Figure~\ref{fig:qm9} b) which plots the trade-off between training and prediction timings vs. training data needs for reaching mean absolute prediction errors of atomization energies of 1 kcal/mol.
%Figure 3 also shows the trade-off between accuracy and computational cost.
%On the x-axis we plot the number of training samples required to make chemically accurate predictions on a held out test set consisting of 100k molecules.
%The y-axis plots the total time required by each ML model for training and prediction on the test set.
We note that there are only three representations located on the Pareto-front, 
FCHL18~\cite{fchl18}, SLATM~\cite{amons_slatm}, and MBDF [this work]. 
As noted earlier, local kernels based on representations such as FCHL19, SOAP, or ACSF exhibit very good training data efficiency,  but this comes at the expense of a larger computational overhead.
The exception is the local MBDF based kernel which achieves the fastest training  timing of $\sim$0.1 compute node minutes (14k training molecules) due to its compact size.
Predictions on 100k QM9 molecules using the local MBDF kernel are made in $\sim$1.8 compute node minutes which translates to an unprecedented chemically accurate navigation rate of $\sim$921 molecules/second.
SLATM being on the Pareto front, and the DF representation, both represent fast global kernel based KRR models. While requiring more training data than SLATM in order to reach chemical accuracy, 
 DF has the advantage that it is largely invariant with respect to system size (see below). 
For the sake of completeness, Fig.~\ref{fig:qm9} b) also includes results for SchNet\cite{schnet} Neural Network which was trained on a GPU. 
The reported timing for SchNet refers to 3000 epochs of training on 20k molecules and predictions of 100k molecules taking about 3 h: 27 min and 0 h: 2 min : 15 sec, respectively.
As such, the prediction times of MBDF based kernel model on a CPU even outperforms a deep neural network on a GPU. 
Numerical results for porting KRR results based on FCHL19 to GPUs~\cite{browning2022gpu} would suggest that it seems likely this advantage would also grow for MBDF once reimplemented in CUDA. 


\subsubsection{QMugs}
We have tested the generalizability of our method to larger systems and more diverse chemistries using the QMugs~\cite{QMugs} data set. 
Figure \ref{fig:qmugs} shows the atomization energy learning curves.
Due to the large variety in the dataset, the predictive error is larger for all representations than their QM9 counterparts even when predicting on a much smaller test set.
MBDF reaches $~\sim$2 kcal/mol prediction error after training on 16k molecules. 
This is better than the QML based neural network predictions published in Ref.~\cite{atz2022delta}, 
and similar to the $\Delta$-QML numbers they also report. 
In terms of speed, generating the local MBDF kernel for training and testing on 20k molecules on this dataset takes $\sim$1.8 compute node mins (see below) which corresponds to a navigation rate of $\sim$185 molecules/second.
By comparison, this is substantially faster than the GPU based prediction rates of approximately 50 and 5 molecules per second for the direct and
$\Delta$-learning (using GFN2-xTB~\cite{gfn2xtb}) based ML models, respectively using the convolutional neural network reported in Ref.~\cite{atz2022delta}.
Only SLATM and FCHL19 exhibit lower off-set than MBDF, 
while the performance for SOAP and DF is similar, albeit slightly worse than MBDF. 
As mentioned before, however, in order to make FCHL19 and SOAP tractable, we
have dramatically reduced the hyper parameters impact. 
In particular, we believe that the learning efficiency of SOAP for QMugs is 
being reduced due to the use of small basis sets ($n_{max}$~=~$l_{max}$ = 3).
%With a larger basis set we expect it to perform similarly well as FCHL19.
Note that no representation reaches chemical accuracy within 16k training molecules, 
indicating that QMugs possesses substantially more chemical diversity than QM9. 
%This indicates that a more sophisticated selection of the training data could be beneficial in such scenarios\cite{sml_dom}.

%Similar trends in accuracy as QM9 are seen here with FCHL19 performing the best followed by SLATM and MBDF.
%The local representations, again, perform better with FCHL19 achieving the best performance followed by MBDF.
%The global DF representation also shows competetitive accuracy given that its size changes the least when moving to these larger molecules.
%The large disparity between CM/BOB and the other representations is, again, likely due to their lack of 3-body terms. 
In terms of representation sizes, 
MBDF again remains the smallest representation since it still requires only 5 dimensions per atom regardless of the chemical composition. 
However, being a local representation,
on average its size increases $\sim$ 3.4 times compared to QM9.
FCHL19 and SOAP, on the other hand, now
require more than 1000 dimensions to represent each atom for this larger dataset. 
CM, BOB, FCHL19, and SOAP show larger than 10 fold increase in the representation size compared to QM9, followed by SLATM which shows an increase of $\sim$ 6.6 times. 
This results in a considerable increase in the train/test time ({\em vide infra}) 
which precludes the straightforward application of these representations to the entire QMugs dataset.
The DF representation changes the least in size since it does not formally scale with number of atoms but only with number of different chemical elements. 
Consequently, its size doubles compared to QM9 since  a separate density function is generated for each unique chemical element in the dataset using eq. \eqref{eq:kde2} as mentioned earlier.

Figure 2 in the SI shows learning curves for the QM7b\cite{qm7b,970m_qm7b} atomization energies and the
size (dimension of feature vector) of the representation
arrays in the legend.
Similar trends in performance and representation size noted so far are observed on this dataset as well.

\subsection{Timings}
Figure \ref{fig:fig6} shows scaling plots of kernel evaluation timings across both QM9 and QMugs datasets for various representations and as a function of training set size.
As one would expect, the off-set increases systematically with increasing representation and training molecule size.
More specifically, for the larger molecules of QMugs, 
the FCHL19 and SOAP kernel evaluations become rapidly 
intractable very quickly with the 16k kernel 
($\sim$2 kcal/mol prediction error, Fig.~\ref{fig:qmugs}) 
already taking an entire compute node day.
Encouragingly and in stark contrast, DF, being a size invariant representation, 
shows hardly any change in computational overhead when moving from the small QM9 molecules to the much larger QMugs molecules.

For context, the time required in the kernel inversion step for each training kernel is shown as well.
The bottleneck crossover from kernel generation ($\mathcal{O}(n^2)$) to the inversion step ($\mathcal{O}(n^3)$) occurs rather late. 
When using Cholesky decomposition it occurs for MBDF at $N \sim$ 64k training molecules. For less compact representations (for SLATM and larger) the same cross-over occurs at training set sizes that exceed $\sim$1 M. 
As demonstrated above, chemical accuracy is already achieved at substantially smaller training set sizes. 
Consequently and contrary to popular belief, for any of the more modern and accurate representations, kernel inversion does not constitute a bottleneck. 

 \begin{figure}[htb]
          \centering           
          \includegraphics[width=\columnwidth]{figs/cho_time_extrapolated2.jpeg}
          \caption{Compute node time required for kernel inversion and evaluation 
           as a function training set sizes $N$ drawn from QM9 (squares) and QMugs (diamonds) datasets. QML results shown for SOAP~\cite{soap}, FCHL19~\cite{fchl19}, SLATM~\cite{amons_slatm}, and MBDF and DF. 
           Dotted lines indicate extrapolation (using quadratic (kernel evaluation) and cubic (kernel inversion) polynomial fits).  G and L denote Global and Local kernels, respectively. Compute node: 24-core AMD EPYC 7402P @ 3.3 GHz CPU with 512 GB RAM.
          }
     \label{fig:fig6}
 \end{figure}



Table 2 reports the kernel evaluation and representation generation timings for both the QM9 dataset (130k molecules) and the QMugs subset (20k molecules) used in our work.
It can be seen that MBDF reduces the local kernel evaluation timings from days to a few minutes for both small and large molecules.
For the representation generation step we note that our code is currently written in Python and uses the Numba\cite{numba} library and could be further optimized with a low-level implementation.
However, the current timings as well do not affect the overall QML model cost much given the kernel evaluation bottleneck.
\begin{table*}[htb]
\begin{tabular}{l|
S[table-number-alignment = right]
S[table-number-alignment = right]
r|
S[table-number-alignment = right]
S[table-number-alignment = right]
r}
& \multicolumn{3}{|c|}{\textbf{QM9 130k Molecules}} & \multicolumn{3}{c}{\textbf{QMugs 20k Molecules}} \\
\hline
Representation   & \multicolumn{1}{l}{$t_\mathrm{rep}$ [min]} &  \multicolumn{1}{l}{$t_\mathrm{kernel}$ [min]}   &  \multicolumn{1}{l|}{Dimension} & \multicolumn{1}{l}{$t_\mathrm{rep}$ [min]}  &  \multicolumn{1}{l}{$t_\mathrm{kernel}$ [min]} &  \multicolumn{1}{l}{Dimension}\\ 
\hline
CM$^{a}$ (G)      &  0.186              & 2.862            &      435       & 0.012               & 1.146             &    5050 \\
BoB$^{a}$ (G)     &  0.216              & 7.296            &     1128       & 1.362               & 3.396             &   18528 \\
SLATM$^{a}$ (G)   &  18.60              & 86.32            &    11960       & 15.76               & 14.58             &   79045 \\
FCHL19$^{a}$ (L)  &  0.846              & 1071             &    10440       & 1.764               & 1566              &  122000 \\
SOAP$^{b}$ (L)    &  0.216              & 1873             &    13920       & 0.246               & 2925              &  186000 \\
\rowcolor{Gray}
\textbf{MBDF (L)} &  1.626              & 11.81            & 145            & 4.182               & 1.848             & 500 \\
\rowcolor{Gray}
\textbf{DF (G)}   &  2.262              & 12.16            & 2500           &  2.442              & 0.996             & 5000\\
\hline
\end{tabular}
\caption{Compute node times for generating representations  ($t_\mathrm{rep}$) and kernel matrices ($t_\mathrm{kernel}$) for 130k molecules from the QM9 dataset and 20k molecules from the QMugs dataset. 
Global and Local kernels are again denoted by (G) and (L) respectively. Representations with superscript $a$ were generated with QMLcode\cite{qml} library and $b$ with the Dscribe\cite{dscribe} library. 
Compute node: 24-core AMD EPYC 7402P @ 3.3 GHz CPU and 512 GB RAM.}
\label{table}
\end{table*}

\subsection{Performance for molecular quantum properties}
\begin{figure*}[htb]
          \centering           
          \includegraphics[width=\linewidth]{figs/qm9_all_props_mbf_new.jpeg}
          \caption{Learning curves for various representations in QML models of highest occupied molecular orbital (HOMO), lowest unoccupied molecular orbital (LUMO) eigenvalues, HOMO-LUMO gap ($\Delta \epsilon$), internal energy at 0 K ($U_{0}$), dipole moment norm ($|\boldsymbol{\mu}|$), static isotropic polarizability ($\alpha$), zero point vibrational energy (ZPVE), and heat capacity ($C_{v}$) from the QM9 dataset. G and L denote Global and Local kernels, respectively.
          }
     \label{fig:other_props}
 \end{figure*}

We assessed the generalization capacities of MBDF/DF on physical properties other than atomization energies.
Figure \ref{fig:other_props} shows the learning curves for the task of predicting 8 important molecular quantum properties from the QM9 dataset. 
These include the highest occupied molecular
orbital (HOMO), lowest unoccupied molecular orbital
(LUMO) eigenvalues and the HOMO-LUMO gap, internal energy at 0 K ($U_{0}$),
dipole moment norm ($|\boldsymbol{\mu}|$), static isotropic polarizability ($\alpha$), zero point vibrational energy
(ZPVE) and heat capacity ($C_{v}$).
Due to substantial computational costs, the KRR hyper-parameters were not optimized at each training size for the FCHL19 and SOAP representations, and we picked the same parameters as those used for learning atomization energies in figure \ref{fig:qm9}.
We reproduce earlier trends among intensive and extensive properties when using local/global kernels~\cite{op_response_anders,felix_google}. 
MBDF and DF match this trend: They perform better on extensive and on intensive properties, respectively.

Again, note that the performance on these properties of MBDF/DF could be further improved by including different functionals suited to the property, or by augmenting them with response terms as was done for the FCHL19 representation.\cite{op_response_anders}
It would also be interesting to see how the learning capacities across these different physical properties is affected by the inclusion of higher order functional terms. 

\section{Conclusion}
We have introduced ultra-compact atomic local many body distribution functional (MBDF) and global density of functionals (DF) representations for use within kernel ridge regression based Quantum Machine Learning (QML) models for rapid sampling of chemical compound space. 
MBDF/DF can accurately describe any atom/molecule using a minimal number of discrete elements and thereby reduce QML training and prediction times by multiple orders of magnitude for small and large molecules.
MBDF and DF correspond to functionals of analytical weighted many body distributions in interatomic distances and angles, as well as their derivatives. 
Chemical identity is encoded as a prefactor to the atomic functionals. 
DF is a weighted density estimations of MBDF, i.e.~a global molecular fingerprint that is invariant with respect to number of atoms (not number of chemical elements).

We have demonstrated predictive power close to state-of-the-art for a variety of quantum physical properties, as encoded in the QM9 dataset~\cite{qm9}. 
On the QM9 dataset MBDF reaches a MAE of atomization energies of only 0.8 kcal/mol after training on 32k molecules while using only 5 dimensions to describe an atom. 
Regarding different molecular properties, it is beneficial to use DF representation along with a global kernel for intensive properties, and MBDF along with a local kernel for the extensive properties.
MBDF and DF generalize also well to other compound spaces, as evinced for the  chemically more diverse QMugs dataset~\cite{QMugs} consisting of substantially larger molecules: After training on 16k molecules, MBDF reaches a peak MAE for atomization energies of 2.1 kcal/mol while still using only 5 dimensions per atom. 
Corresponding training and prediction times for both data-sets is $\sim$2 compute node time minutes for MBDF based models. 
Furthermore, our results indicate that using MBDF/DF brings the train/test timings of kernel based ML models to their "lower-bound" as imposed by the kernel inversion bottleneck for both small and large molecules.

We have analyzed the comparative performance for the sampling cost vs.~training set size needs to reach chemical accuracy for predicting atomization 
energies in the QM9 data-set. 
MBDF has extended the corresponding optimal Pareto front towards minimal time requirements.
%whereas FCHL19 reaches an accuracy of 1.49 kcal/mol while requiring 1220 elements to describe the atoms, with the cutoff radii still at 6 Å. 

While the numerical evidence collected is indicative of a surprising effectiveness of MBDF, it is clear that the truncations used in this work may lead to lack of uniqueness. However, neither within QM9 nor within the QMugs subset we sample, have we encountered a case where using this small set of functionals maps two different molecular structures to the same feature vector.
Furthermore, the likelihood of uniqueness can easily be increased by inclusion of  higher order derivatives and many-body terms. 
More rigorous numerical analysis would be desirable to quantify this trend.


%Our compact global representation shows peak performances on the two datasets that are better than all the other global representations tested in our work. 

Thanks to its numerical efficiency, we believe that this approach holds great promise for further accelerating the virtual discovery of novel molecules and materials. 
Furthermore, this framework provides a possible solution to the general problem of unfavourable scaling due to i) inclusion of higher order many body and long range physics and ii) applying these ML models to larger molecules with greater chemical diversity. 
Future work will deal with extension of the representations as described to deal with various types of conformers, and an assessment of its sensitivity to changes in molecular structures.


\section{Acknowledgments}
D.K. is thankful for discussions with B.~Huang, S.~Krug, D.~Lemm, M.~Sahre, and J.~Weinreich. 
O.A.v.L. has received funding from the European Research Council (ERC) 
under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 772834).
O.A.v.L. has received support as the Ed Clark Chair of Advanced Materials and as a Canada CIFAR AI Chair.

\section{Supplementary material}
See supplementary material for QM7b\cite{qm7b} atomization energy learning curves and a figure showing the transition of cyclohexane from chair to boat conformation alongside the response by its DF fingerprint. 
It also contains heat maps showing ideal KRR hyperparameters to be used with the MBDF representation and kernel PCAs of some molecules from the QM7b dataset.

\section{Code and Data Availability}
Python script for generating the MBDF and DF representations is available at \url{https://github.com/dkhan42/MBDF}. Train/test split of the QM7 dataset used for optimizing representation hyperparameters, other
data and code for reproducing the results in this study and some tools for performing KRR are openly available at \url{https://github.com/dkhan42/QML}.
\bibliographystyle{apsrev4-1}
\bibliography{literature.bib}
\end{document}