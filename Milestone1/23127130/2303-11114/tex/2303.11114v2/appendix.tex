
\appendix
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}

\section*{Appendix}

In this additional document, we describe more details of \ours in \cref{sec:more_details_supp}, including the details of token encoding-decoding algorithms (\cref{subsec:pseudocode_supp}), the visualization of Emb-noise augmented tokens (\cref{subsec:embnoise_vis_supp}). We also include additional experimental results in \cref{sec:more_exp_supp}, including the hyperparameter details (\cref{subsec:hyperparam_exp_supp}), the full experimental results of Table 2 (\cref{subsec:full_exp_supp}), the additional results on storage-efficient pre-training (\cref{subsec:storage_efficient_pt_exp_supp}), exploring other tokenizers (\cref{subsec:more_tokenizers_exp_supp}), and robustness benchmarks (\cref{subsec:robustness_exp_supp}).

\section{More Details for \ours}
\label{sec:more_details_supp}

\subsection{Pseudo-code for Token Encoding-Decoding}
\label{subsec:pseudocode_supp}

\begin{algorithm}[b]
\caption{An algorithm for token encoding}\label{alg:encoding_supp}
\begin{algorithmic}[1]
\Require A sequence of tokens $T = [t_1, \ldots, t_N]$, the bits for the storage $M$
\State $L_{T} \gets [\phi]$
\Comment{Initialize an empty list for tokens}
\State $L_\text{idx} \gets [\phi]$
\Comment{Initialize an empty list for start indices}
\State $j \gets 0$
\While{$i \leq$ N}
\If{$t_i \geq 2^M$}
    \State $L_{T}$\texttt{.append} ($2^M$)
    \State $L_{T}$\texttt{.append} ($t_i - 2^M$)
    \State $j \gets j + 2$
\Comment{Assume $t_i < 2^{M+1}$ for simplicity}
\Else
    \State $L_{T}$\texttt{.append} ($t_i$)
    \State $j \gets j + 1$
\EndIf
\State $L_\text{idx}$\texttt{.append} ($j$)
\State $i \gets i + 1$
\EndWhile \\
\textbf{Return:} \texttt{Huffman encoding}~($L_{T}, L_\text{idx}$)
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
\caption{An algorithm for token decoding}\label{alg:decoding_supp}
\begin{algorithmic}[1]
\Require A compressed bytestring $L_{T}\prime$ from \cref{alg:encoding_supp}
\State $L_T = [t_0, \ldots, t_N], L_\text{idx} \gets$ \qquad \qquad \qquad \qquad \, \texttt{Huffman deencoding}~($L_{T}^\prime, L_\text{idx}^\prime$)
\State $T \gets [\phi]$
\State $i \gets 0$
\While{$L_\text{idx}$ is not empty}
\State $j \gets L_\text{idx}$\texttt{.pop} (0)
\State $k \gets i$
\While{$k \leq j$}
\If{$t_k \geq M$}
\State $T$\texttt{.append} ($t_k + t_{k+1}$)
\State $k \gets k + 2$
\Else
\State $T$\texttt{.append} ($t_k$)
\State $k \gets k + 1$
\EndIf
\EndWhile
\State $i \gets j$
\EndWhile \\
\textbf{Return:} $T$
\end{algorithmic}
\end{algorithm}

\cref{alg:encoding_supp} and \cref{alg:decoding_supp} describe the psuedo-codes for the proposed token encoding and decoding. Here, we assume $\max t_i < 2^{M+1}$ for the simplicity. For example, in our main experiments, each token belongs to 391 classes and we set $M=8$, hence, $\max t_i = 391 < 2^{8+1} = 512$. If the number of token classes is larger than $2^{M+1}$, then our algorithm can be naturally extended by repeating line 6-7 in \cref{alg:encoding_supp}. By this simple algorithm, we achieved a nearly optimal compression ratio (1.11 kB vs. 1.08 kB per image) where almost 0.63 smaller than the 16-bit encoding (2.0 kB per image). Note that, we use the native \texttt{gzip} library to perform Huffman encoding and decoding for simplicity.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/noise_supp.pdf}
    \caption{\small {\bf Emb-Noise visualization.} ``Reconstruction'' denotes the reconstructed image by the ViT-VQGAN decoder from the extracted tokens. ``Full-size noise'' is a random noise whose size is equivalent to the embedding vectors.}
    \label{fig:emb_noise_decode_overview_supp}
\end{figure}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/channelwise.png}
    \caption{\small {\bf Channel-wise modification visualization.} We present ViT-VQGAN decoded images obtained by adding a constant to each of the 32 channels in codebook vectors.}
    \label{fig:emb_noise_decode_details_supp}
\end{figure*}

\subsection{ViT-VQGAN decoded images for Emb-Noise}
\label{subsec:embnoise_vis_supp}

\cref{fig:emb_noise_decode_overview_supp} and \cref{fig:emb_noise_decode_details_supp} show the visualization examples of the Emb-Noise augmented tokens and the tokens without augmentation. We use the ViT-VQGAN decoder for visualization. We observe that our Emb-Noise can make meaningful distortions on the decoded images.

\section{Additional Experimental Results}
\label{sec:more_exp_supp}

\subsection{Hyperparameter details}
\label{subsec:hyperparam_exp_supp}

\cref{tab:hyperparams_supp} shows the full list of hyperparameters used in our experiments. All hyperparameters are for the ViT-B backbones. In the table, Token IN-1k corresponds to \ours (ImageNet-1k) in Table 2, Token IN-21k PT corresponds to token pre-training in Table 3, and Token FT and Image FT correspond to token and image fine-tuning in Table 3, respectively. For other backbones and datasets, we only adjust the learning rate as the maximum learning rate showing a stable convergence (\eg, We use 0.15 for ResNet and ViT-S uses the same learning rate as ViT-B).

\begin{table*}[t]
\footnotesize
\centering
\begin{tabular}{lrrrrr}
\toprule
Methods             & DeiT IN-1k \cite{deit}      & Token IN-1k       & Token IN-21k PT & Token IN-1k FT & Image IN-1k FT \\ \midrule
Epochs              & 300             & 300              & 270     & 100           & 100           \\ \midrule
Batch size          & 1024            & 1024             & 2048    & 4096          & 512           \\
Optimizer           & AdamW           & AdamW            & AdamW   & AdamW         & AdamW         \\
Learning rate       & 0.0005 x $\frac{bs}{512}$ & 0.00075 x $\frac{bs}{512}$ & 0.0015  & 0.00001       & 0.0005        \\
Learning rate decay & cosine          & cosine           & cosine  & \bnomark      & cosine        \\
Weight decay        & 0.05            & 0.1              & 0.02    & 0.1           & 0.05          \\
Warmup epochs       & 5               & 5                & 5       & 5             & 5             \\
Label smoothing     & 0.1             & 0.1              & 0.1     & 0.1           & 0.1           \\
Dropout             & \bnomark        & \bnomark         & \bnomark  & \bnomark  & \bnomark        \\
Stoch. Depth        & 0.1             & 0.1              & 0.1     & 0.15          & 0.1           \\
Gradient Clip       & \bnomark        & \bnomark         & \bnomark  & \bnomark  & \bnomark        \\ \midrule
Cutmix prob.        & 1               & 1                & 1       & 1             & 1             \\
Mixup prob.         & 0.8             & 0                & 0       & 0             & 0.8           \\
RandAug             & 9 / 0.5         & -                & -       & -             & 9 / 0.5       \\
Repeated Aug        & \byesmark       & -                & -       & -             & \bnomark      \\
Erasing prob.       & 0.25            & -                & -       & -             & 0             \\
EDA prob.           & -               & 0.25 (RS) / 0.25 (SR)             & 0       & 0             & -             \\
Emb-Noise prob.     & -               & 0.5              & 0.5     & 0.5           & -             \\
\bottomrule
\end{tabular}
\vspace{.5em}
\caption{\small {\bf Hyperparamters for \ours and DeiT-B.} All hyperparameters are for the ViT-B backbone. DeiT IN-1k is the same as the original DeiT paper (baseline).}
\label{tab:hyperparams_supp}
\end{table*}


\subsection{The full experimental results}
\label{subsec:full_exp_supp}

We report the full experimental results in \cref{tab:main_results_supp}. Details are the same as Table 2. 

\begin{table}[ht]
\small
\begin{tabular}{lcccc}
\toprule
Pre-trained on              & Flowers & Cars & iNat18 & iNat19 \\
\midrule
Pixel (IN-1k)               & 98.0       & 91.8         & 73.0   & 77.7   \\
Token (IN-1k)               & 93.5       & 79.7         & 43.1   & 50.1   \\
Token (IN-21k, IN-1k)       & 98.7       & 84.5         & 50.1   & 58.3  \\
\bottomrule
\label{tab:otherdatasets}
\end{tabular}
\caption{\small {\bf Other datasets.} We report the top-1 accuracies on diverse fine-grained datasets achieved by \ours. We tested \ours on the Flowers \cite{flowers102}, StanfordCars \cite{stanfordcars}, iNaturalist (iNat)-18 \cite{inaturalist18} and iNat-19 \cite{inaturalist19} datasets.}
\end{table}


\begin{table*}[ht]
\small
\centering
\begin{tabular}{lllllll}
\toprule
Tokenizer & Training dataset & Quantiztation & Voca size (\# of valid voca) & PS & FID & ViT-S (SeiT) Acc \\ \midrule
VQGAN & ImageNet & Vector quantization & 1024 (454) & 16 & 7.94 & 75.3 \\
VQGAN & ImageNet & Vector quantization & 16384 (971) & 16 & 4.98 & 76.9 \\
VQGAN & OpenImages & Gumbel quantization & 8192 (2886) & 8 & 1.49 & 79.1 \\
VQGAN & OpenImages & Vector quantization & 256 (256) & 8 & 1.49 & \textbf{81.8} \\
ViT-VQGAN & ImageNet & Vector quantization & 8192 (391) & 8 & 1.28 & 77.3 \\
\bottomrule
\end{tabular}
\vspace{.5em}
\caption{\small {\bf Exploring other tokenizers.} Various ViT-S (\ours) results on the ImageNet-100 benchmark are shown. We compare various VQGAN tokenizers with ViT-VQGAN by varying the quantization methods (Gumbel softmax vs. vector quantization) the vocabulary size, the valid vocabulary size (the number of classes actually used for the ImageNet-1k training dataset), and the patch size (PS).}
\label{tab:other_tokenizers_supp}
\end{table*}


\begin{table*}[ht]
\small
    \centering
    \begin{tabular}{ccccccc}
    \toprule
    Model & Data format & Clean & Gauss. Noise & Gauss. Blur & ImageNet-R & Sketch \\ \midrule
    ViT-S (DeiT) & Pixels & 79.9  & 75.1 {\footnotesize (6.0\%)} & 73.4 {\footnotesize (8.1\%)} & 28.8 {\footnotesize (63.9\%)} & 29.9 {\footnotesize (62.6\%)} \\ \midrule
    ViT-S (Weak Aug) & Pixels & 78.0 & 64.7 {\footnotesize \textbf{(17.1\%)}} & 66.8 {\footnotesize (14.4\%)} & 20.8 {\footnotesize (73.4\%)} & 18.1 {\footnotesize (76.8\%)} \\
    ViT-S (SeiT, ours) & Tokens & 74.0  & 60.8 {\footnotesize (17.3\%)} & 65.3 {\footnotesize \textbf{(11.2\%)}} & 26.0 {\footnotesize \textbf{(64.6\%)}} & 23.0 {\footnotesize \textbf{(68.7\%)}} \\
    \bottomrule
    \end{tabular}
    \vspace{.5em}
    \caption{\small \textbf{Robustness evaluation.} We show the clean and robust accuracies against corruptions and domain shifts of each model trained on ImageNet-1k. The performance drops are put in parentheses (lower is better) for robust accuracies.}
    \label{tab:robustness}
\end{table*}

\subsection{Other datasets}
\label{subsec:other_datasets}

The performances of \ours on various datasets are reported in Table \ref{tab:otherdatasets}.
We tokenize the datasets and then fine-tune a model (ViT-B) with the tokenized dataset, using token-trained model weights. The token-trained models weights, Token (IN-1k) and Token (IN-21k, IN-1k) achieve top-1 accuracies of 74.0\%, 81.1\% on ImageNet-1k, respectively.
The pixel counterpart is fine-tuned on pixel target datasets from the pixel pre-trained model; Pixel (IN-1k), showing 81.8\% top-1 accuracy on IN-1k.
We followed the pixel-training recipes of DeiT \cite{deit}. Although we do not modify the training recipe for tokens, the results verify the possibility of SeiT on those datasets.


\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{figures/adversarial_robustness.pdf}
    \caption{\small {\bf Adversarial robustness of DeiT and SeiT by varying $\varepsilon$.} $\varepsilon=0$ denotes the clean accuracy.}
    \label{fig:advattack_supp}
\end{figure}


\subsection{Three-stage storage-efficient pre-training}
\label{subsec:storage_efficient_pt_exp_supp}

Following BeiT v2 \cite{peng2022beit2}, we extend our storage-efficient pre-training in three stages, namely, 21k token pre-training $\rightarrow$ 1k token pre-training $\rightarrow$ 1k image fine-tuning. For simplicity, we directly fine-tune the ``21k token pre-trained and 1k token fine-tuned model'' (\ie, 81.1\% model in Table 3) on the image pixels with the same optimization hyperparameter of the image fine-tuned model. As a result, we have 82.8\% top-1 accuracy, slightly better than the original two-staged training strategy (+0.2\% than 82.6\%).

\subsection{Exploring other tokenizers}
\label{subsec:more_tokenizers_exp_supp}

In this subsection, we explore other tokenizers rather than ViT-VQGAN \cite{vitvqgan}, \eg, VQGAN \cite{vqgan}. We employ four VQGAN models from the official repository\footnote{\url{https://github.com/CompVis/taming-transformers}}, ImageNet-trained VQGAN with patch size 16 and vocabulary size 1024, ImageNet-trained VQGAN with patch size 16 and vocabulary size 16384, OpenImages \cite{openimages}-trained VQGAN with patch size 8 and vocabulary size 256, and OpenImages \cite{openimages}-trained VQGAN with patch size 8 and vocabulary size 8192. Here, the last VQGAN model is trained with the Gumbel softmax \cite{gumbel1,gumbel2} quantization, instead of the original vector quantization by VQ-VAE \cite{vqvae}. Here, we slightly change our Stem-Adopter from 4$\times$4 Conv with stride 2 to 2$\times$2 Conv with stride 1 for tokenizers with patch size 16.

In \cref{tab:other_tokenizers_supp}, we report the ViT-S (\ours) top-1 accuracy on the ImageNet-100 benchmark by varying the choice of tokenizers. We also report the reported ImageNet-1k validation FID score of each tokenizer. In the table, we observe that the top-1 accuracy of \ours follows the generation quality (FID) if we use the same quantization method (\eg, vector quantization). The ViT-VQGAN shows the best FID (1.28) as well as the best ImageNet performance with \ours (77.3). While the Gumbel quantized VQGAN achieves the best performance, in practice, we use ViT-VQGAN due to two reasons. First, the storage efficiency: 2886 valid codes need 1.5 times more storage than 391 valid codes. Second, Although the OpenImages \cite{openimages}-trained VQGAN shows better quality, it needs to be trained on a large-scale external dataset. We did not use the OpenImages-trained VQGAN for a fair comparison with other ImageNet-1k-only training methods.

\subsection{Robustness benchmarks}
\label{subsec:robustness_exp_supp}

We compare ViT-S models trained on ImageNet-1k with different training strategies using robustness benchmarks. We employ three scenarios: (1) noise and blur scenario (2) domain shift scenario (3) adversarial attack scenario. For the first scenario, we add Gaussian noise and Gaussian blur to the validation images. We use ImageNet-R \cite{imagenet-r} and Sketch-ImageNet \cite{sketchimagenet} for testing the robustness against domain shifts. Finally, we use a weak version of AutoAttak \cite{autoattack} for measuring adversarial robustness.

As the original DeiT is trained on strong augmentation, such as RandAugment or 3-Augment, we also compare our method with ``weak augmented'' ViT-S, where it only employs resized random crop (RRC) and CutMix \cite{yun2019cutmix}. Our assumption is that because the pixel-trained models are sensitive to imperceptible details, they will be less robust than our approach in noise or adversarial attack scenarios. However, on the other hand, because our method relies on the encoding power of the pre-trained tokenizer, if the employed tokenizer is not a robust feature extractor, our method could be more vulnerable than pixel-trained counterparts.

\cref{tab:robustness} shows the results of the first and the second scenarios. Here, we observe two important findings. First, when we use the same augmentations with the same strength (ViT-S Weak Aug vs. ViT-S SeiT), \ours shows smaller performance drops on both noise scenarios and domain shift scenarios. On the other hand, when we use strong pixel-level augmentations, the pixel-trained counterpart outperforms our approach. It implies that the key to the input pixel robustness depends on the pixel-level augmentations with severe distortions as observed by previous studies \cite{chun2019icmlw, taori2020measuring}. However, because our method uses only tokens, not pixels directly, investigating how to explore pixel-level distortion augmentations on the token level will be an open question and an interesting future research direction.

We also compare the adversarial robustness of DeiT-S and SeiT-S. We employ the APGD (a step size-free version of PGD attack \cite{pgd}) with cross-entropy loss and DLR loss, following AutoAttack \cite{autoattack}. Because \ours employs discrete non-differentiable representations in the computational graph, we employ the straight-through estimator (STE) \cite{ste} to estimate the non-differentiable gradients, following Athalye \etal \cite{athalye2018obfuscated}. We also evaluate the non-quantized version of the quantizer (\ie, omitting the vector quantization process, but using the extracted feature by the encoder directly to the ViT input), but we empirically observe that attacking the non-quantized version cannot drop the performance at all. Instead, we use the STE, also used during the training as well as the previous extensive robustness study \cite{athalye2018obfuscated}. We compare the attacked accuracies of DeiT and SeiT by varying $\varepsilon$ (a control parameter for the attack intensity) from 0 to 8 in \cref{fig:advattack_supp}. We observe that \ours shows almost neglectable performance drops even under the strongest attack (showing 73.95 for $\varepsilon = 8$ where 73.98 for $\varepsilon = 0$), where DeiT shows 2.8\% top-1 accuracy.

However, we should be careful to interpret \cref{fig:advattack_supp}; it could be due to a strong obfuscated gradient effect \cite{athalye2018obfuscated} that cannot be detected by a naive straight-through estimator. Moreover, our method could be vulnerable to the codebook attack by changing the token indices directly, not by perturbing the pixels. However, as an efficient and natural adversarial attack on discrete domains is still an open problem \cite{zhang2020adversarial} (\eg, altering indices as imperceptible to humans but sensitive to machines --- only a small index change can make a huge semantic gap, such as replacing ``huge'' in the previous sentence to ``neglectable''), we leave the investigation of advanced adversarial attack methods for \ours beyond straight-through estimator as future work.



\input{main_table_supp}

