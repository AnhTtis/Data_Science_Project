Capturing high-quality photos in low illumination is a fundamental yet challenging task. Increasing the ISO improves visibility; however, it also increases sensor noise. Longer exposure times improve the image but require a tripod to avoid camera motion and motion blur. Methods like enabling flash or image editing also present their own challenges. Low exposure image enhancement helps generate low-light scenes as if they were captured with a longer exposure time. It enables fast low illumination photography without a tripod. As shown in Fig. \ref{fig:canon_eg1}(a), the images captured in such settings possess a high degree of noise and color distortion. Existing single image denoising methods \cite{dabov2007image, plotz2017benchmarking} perform poorly and fail to correct color distortions in low light. 
\newpage 
An alternative is to merge a burst of short exposure images to reduce noise \cite{hasinoff2016burst,liu2014fast,mildenhall2018burst}. However, the burst images must be aligned for the camera and object motion, and aligning them in low-light conditions is a challenge. LSID, \cite{chen2018learning} a recent deep learning based method enhances raw low-light images by performing denoising, color improvement, and demosaicing, all with a single lightweight model. Despite the success of recent methods, there are two persisting challenges:

\begin{figure*}[t]
    \centering
    \label{fig:canon_eg1}
    \includegraphics[width=\linewidth, clip, trim=0cm 0.75cm 0cm 0.661cm]{Images/21BMVC_fig1_v2.png}
    \caption{Qualitative comparison between different methods with an input to the model from the Canon dataset (left-most). The models are: LSID trained on (a) full 161 Sony source images, (b) only 6 Canon images, and (c) full Canon dataset. (d) 6 Canon images and 161 Sony images with proposed few-shot domain adaptation method (discussed in section \ref{sec:method}).}
    \vspace{-0.3cm}
\end{figure*}

%Deep learning-based techniques generate high-quality results as compared to traditional non-learning methods and their success can be attributed to the large labeled train data in the raw domain. Raw data is uncompressed and minimally manipulated information directly from the digital camera sensors. Thus, raw data captured with a camera from one manufacturer differs from that of another camera manufacturer. Many cameras have their own proprietary raw image format to save sensor data.

% thus have different 'shot' noise (photon arrival statistics) and 'read' noise (readout imprecision) \cite{hasinoff2014photon} at high ISO amplification.

\textbf{Domain Shift}: CNNs are heavily data-sensitive; Since raw images captured with cameras from different manufacturers exhibit variations in color-space and noise characteristics, a model trained with one camera's raw data performs sub-optimally on another camera's raw data (see Table \ref{tab:baseline}). Hence, there exists a domain shift across different camera raw domains. In this work, we consider each camera as a separate domain. As seen from the Fig. \ref{fig:canon_eg1}, cross-camera domain performance is poor as there are color distortions (green patches on the wall) and loss of finer details (missing cat's whiskers) due to the shift across camera domains. %As we show in Table \ref{tab:baseline}, an LSID model trained on Nikon camera data performs sub-optimally on Sony and Canon camera datasets. Similarly, a model trained on Canon camera data performs well only for Canon raw images Fig. \ref{fig:canon_eg1} (b) shows an example of a model trained with Sony camera raw data and tested on Canon camera raw data. 

\textbf{Tedious to collect labeled data}: Collecting a large dataset of short-exposure and long-exposure raw image pairs for each camera is a difficult task. There must be no object motion, and to avoid camera misalignment, a tripod is necessary to capture long-exposure images (typically 10 seconds or more). Further, a smartphone or an IR remote is required to trigger the camera to avoid camera shake arising from physically pressing the camera click button. Thus, capturing a large-scale labeled dataset for different cameras is an arduous task.

To address the above mentioned challenges, we propose a paradigm shift for the low-light raw image enhancement task using few-shot learning and domain adaptation. We use a large collection of existing source camera labeled data to improve the performance and generate the output in the target domain by transferring the task onto a new target camera dataset with only a few labeled samples. In summary, our contributions are as follows:
% We address these challenges by designing a novel, simple, and lightweight approach for the low-light raw image enhancement task by adapting from an existing camera domain with large labeled data (source domain) to a new camera domain with few labeled data (target domain). In this work, we propose a paradigm shift for the low-light raw image enhancement task in few-shot learning and domain adaptation. We use a large collection of existing source camera labeled data to improve the performance and transfer the task onto a new target camera dataset with only a few labeled samples and generate the output in the target domain. In summary, our contributions are as follows:
% \begin{itemize}
\begin{enumerate}[label=(\roman*),wide,itemindent=1em,noitemsep,topsep=0pt,itemsep=0pt]
\item To the best of our knowledge, we propose the first few-shot domain adaptation method for low-light raw image enhancement. 
\item We show that, with less than ten labeled samples from the target domain, our approach can outperform a model trained with a complete target domain dataset. 
\item We present experiments and ablations to illustrate the effectiveness of our method.
\item We present a new Nikon camera dataset with short-exposure and long-exposure ground truth raw image pairs for the benefit of the research community.
% \end{itemize}
\end{enumerate}

% % Please add the following required packages to your document preamble:
% \begin{table}[t]
% \centering
% \setlength\tabcolsep{2pt}
% \caption{Comparison between LSID models trained with one source camera dataset and tested on other target camera datasets.}%\cite{chen2018learning}
% \label{tab:baseline}
% \scalebox{0.85}{
% \begin{tabular}{lcccccc}
% \hline
% Testing ($\rightarrow$) & \multicolumn{2}{c}{Sony} & \multicolumn{2}{c}{Nikon} & \multicolumn{2}{c}{Canon} \\ \hline
% Training ($\downarrow$) & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\ \hline
% Sony  & \textbf{28.50  } & \textbf{0.774}   & 25.90  & 0.693   & 27.41   & 0.845   \\
% Nikon  & 19.95   & 0.481   & \textbf{30.74 } & \textbf{0.913}  & 24.34  &   0.767  \\
% Canon  & 18.51   & 0.542   & 23.27   & 0.847   & \textbf{32.32  } & \textbf{0.899}   \\ \hline
% \end{tabular}
% }
% \end{table}

%--------------------------------------------------------------------------------------------------------------------

