% \textbf{I would like to see a motivation why the models trained on the full Canon and
% Nikon datasets underperform the models trained with a domain transfer (e.g.
% Figure5). This might suggests the proposed algorithm is indeed doing a great
% job, but since it is counter-intuitive, it deserves an expanded discussion.}
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{figure*}[t]
\centering
\subfigure{\includegraphics[width=\linewidth]{Images/Nikon-Canon-136-2.pdf}}
\caption{Qualitative comparison of results for $k$=1,2 and 4 for Sony as source and Nikon as target, and $k$=1,3 and 6 for Sony as source and Canon as target for choosing the value of $k$.}
\label{fig:canon136}
\end{figure*}

\begin{table}[t]
\centering
\caption{Comparison with baselines trained with Sony raw data as source and fine-tuned with (1/2/4-shot) Nikon camera raw images and (1/3/6-shot) Canon camera raw images. We compare with HDRCNN \cite{EKDMU17}, Unprocess \cite{Brooks_2019_CVPR} and LSID \cite{chen2018learning}.}
\label{tab:finetunenikon}
\label{tab:finetunecanon}
\scalebox{0.71}{
\begin{tabular}{c|c}
    \begin{tabular}{|c|cccccc}
        \hline
        Method & \multicolumn{6}{c}{Sony source w/ Nikon target} \\ \hline
        \multirow{1}{*}{$k$ ($\rightarrow$)} & \multicolumn{2}{c}{1} & \multicolumn{2}{c}{2} & \multicolumn{2}{c}{4} \\
        \hline
        & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM  \\ \hline
        HDRCNN & 11.47 & 0.648 & 12.36 & 0.578 & 12.87 & 0.627 \\ 
        Unprocess & 18.90 & 0.710 & 21.91 & 0.690 & 25.63 & 0.761 \\ 
        LSID & 22.38 & 0.746 & 25.75 & 0.874 & 27.93 & 0.899 \\ 
        Proposed & \textbf{25.27} & \textbf{0.860} & \textbf{28.06} & \textbf{0.909} & \textbf{30.30} & \textbf{0.913} \\ 
        \hline
    \end{tabular}
    &
    \begin{tabular}{cccccc|}
        \hline
        \multicolumn{6}{c|}{Sony source w/ Canon target} \\ \hline
        \multicolumn{2}{c}{1} & \multicolumn{2}{c}{3} & \multicolumn{2}{c|}{6} \\ 
        \hline
        PSNR & SSIM & PSNR & SSIM & PSNR & SSIM  \\ \hline
        15.07 & 0.593 & 15.89 & 0.627 & 16.14 & 0.633 \\ 
        16.97 & 0.609 & 22.15 & 0.671 & 27.20 & 0.733\\ 
        \textbf{25.88} & \textbf{0.792} & 28.38 & 0.831 & 28.85 & 0.819 \\ 
        24.29 & 0.623 & \textbf{28.78} & \textbf{0.841} & \textbf{33.22} & \textbf{0.896} \\
        \hline
    \end{tabular}
\end{tabular}
}
\end{table}

\begin{table}[t]
\centering
\caption{Comparison with recent low-light image enhancement methods. Ours is the first few-shot domain adaptation method, hence, we first train the baselines on the Sony camera dataset and then fine-tune with (4-shot) Nikon camera or (6-shot) Canon camera datasets.}
\label{tab:recent}
\scalebox{0.8}{
\begin{tabular}{@{}lccccccc@{}}
\hline
\multirow{1}{*}{Method} & \multicolumn{2}{c}{Sony Source} & \multicolumn{2}{c}{Sony w/ Nikon} & \multicolumn{2}{c}{Sony w/ Canon} \\ \hline
& PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\
%  & & & Canon & Nikon \\
%  & & & Target & Target \\
\hline
%  & HDRCNN \cite{EKDMU17} &  12.64 & 11.78 & \\
DeepUPE \cite{Wang_2019_CVPR} & 14.58 & 0.256 & 13.42 & 0.266 & 13.81 & 0.285\\
MIRNet \cite{Zamir2020MIRNet} & 15.24 & 0.414 & 14.18 & 0.458 & 13.24 & 0.397 \\
KnD \cite{zhang2019kindling} & 17.15 & 0.313 & 15.04 & 0.226 & 17.24 & 0.432 \\
HDRCNN \cite{EKDMU17} & 17.39 & 0.491 & 12.87 & 0.627 & 16.14 & 0.633 \\
KnD++ \cite{zhang2021beyond} & 23.03 & 0.579 & 19.38 & 0.471 & 21.25 & 0.434 \\
Unprocess \cite{Brooks_2019_CVPR} &  27.83 & 0.700 & 25.63 & 0.761 & 27.20 & 0.733 \\
LSID \cite{chen2018learning} &  28.50 & 0.774 & 27.93 & 0.899 & 29.36 & 0.829 \\
% & DeepUPE \cite{Wang_2019_CVPR} &  29.13 & & \\
Proposed &  - & - & \textbf{30.30} & \textbf{0.913} & \textbf{33.22} & \textbf{0.896} \\ \hline
\end{tabular}
}
\end{table}

Table \ref{tab:datasets} lists the total number of labeled raw image pairs in the train set to be 161 images for the Sony dataset \cite{chen2018learning}, 53 images for our Nikon dataset, and 44 images for the Canon dataset \cite{CanonLSID}. We train our model for three different numbers of labeled target data: For Nikon target, we use $k$=1,2, and 4. For Canon target, we use $k$=1,3, and 6. We choose the $k$ based on the different exposure ratios available for the dataset. As reported in Table \ref{tab:datasets}, the Nikon camera dataset has two exposure ratios; hence we use two images per ratio leading to a total of four images. Similarly, the Canon camera dataset has three ratios; hence we use two images per ratio leading to a total of six images. We observe that using two images per exposure ratio in the target domain is sufficient to outperform all baselines (Refer Table \ref{tab:nikon}). A qualitative guideline for choosing the value of $k$ is in Fig.(\ref{fig:canon136}). For each $k$, we run three separate experiments, each with a different set of $k$ labeled target images. We report the average and 95\% variance margin computed across three different sets for the Nikon dataset and for the Canon dataset (Table \ref{tab:canon}). We compare with an LSID model trained with only $k$-target data and an LSID model trained with the full target camera data. % for a fair comparison.

\textbf{Quantitative Evaluation:} For Nikon dataset as target, our 4-shot approach achieves 30.30dB PSNR, which is on par with the full target dataset ($k$=53) trained LSID model. Our method outperforms only $k$-shot trained model by 2.25dB PSNR (Table \ref{tab:nikon}). Similarly, for Canon dataset as target, our 6-shot approach outperforms the full dataset ($k$=44) trained LSID model by 0.9dB PSNR, and $k$-shot trained LSID model by 3.86dB PSNR (Table \ref{tab:canon}). Since ours is the first few-shot domain adaptation method, we quantitatively compare our method with baselines by training them on the full Sony source dataset and then fine-tuning them in a few-shot manner on the Nikon or Canon target datasets (Table \ref{tab:finetunecanon}). We also compare with recent low-light enhancement methods in Table \ref{tab:recent} and show that our method outperforms all $k$-shot fine-tuned baselines in PSNR and SSIM for the Nikon and Canon target datasets. We show quantitative results for our method trained with Sony as source and four OnePlus camera images or four Google Pixel camera images as target in Table \ref{tab:oneplus}.
% \begin{table}[t]
% \centering
% \caption{Comparison with baselines trained with Sony raw data as source and fine-tuned with (1/2/4-shot) Nikon camera raw images and (1/3/6-shot) Canon camera raw images. We compare with HDRCNN \cite{EKDMU17}, Unprocess \cite{Brooks_2019_CVPR} and LSID \cite{chen2018learning}.}
% \label{tab:finetunenikon}
% \label{tab:finetunecanon}
% \scalebox{0.71}{
% \begin{tabular}{c|c}
%     \begin{tabular}{|c|cccccc}
%         \hline
%         Method & \multicolumn{6}{c}{Sony source w/ Nikon target} \\ \hline
%         \multirow{1}{*}{$k$ ($\rightarrow$)} & \multicolumn{2}{c}{1} & \multicolumn{2}{c}{2} & \multicolumn{2}{c}{4} \\
%         \hline
%         & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM  \\ \hline
%         HDRCNN & 11.47 & 0.648 & 12.36 & 0.578 & 12.87 & 0.627 \\ 
%         Unprocess & 18.90 & 0.710 & 21.91 & 0.690 & 25.63 & 0.761 \\ 
%         LSID & 22.38 & 0.746 & 25.75 & 0.874 & 27.93 & 0.899 \\ 
%         Proposed & \textbf{25.27} & \textbf{0.860} & \textbf{28.06} & \textbf{0.909} & \textbf{30.30} & \textbf{0.913} \\ 
%         \hline
%     \end{tabular}
%     &
%     \begin{tabular}{cccccc|}
%         \hline
%         \multicolumn{6}{c|}{Sony source w/ Canon target} \\ \hline
%         \multicolumn{2}{c}{1} & \multicolumn{2}{c}{3} & \multicolumn{2}{c|}{6} \\ 
%         \hline
%         PSNR & SSIM & PSNR & SSIM & PSNR & SSIM  \\ \hline
%         15.07 & 0.593 & 15.89 & 0.627 & 16.14 & 0.633 \\ 
%         16.97 & 0.609 & 22.15 & 0.671 & 27.20 & 0.733\\ 
%         \textbf{25.88} & \textbf{0.792} & 28.38 & 0.831 & 28.85 & 0.819 \\ 
%         24.29 & 0.623 & \textbf{28.78} & \textbf{0.841} & \textbf{33.22} & \textbf{0.896} \\
%         \hline
%     \end{tabular}
% \end{tabular}
% }
% \end{table}

As discussed in section \ref{sec:intro}, capturing a low-light raw image dataset is difficult, and different cameras have different color-space and noise distributions, hence there is a need for a domain adaptation method that can tranfer the task from source to a target domain in a few-shot setting. Despite the high complexity of the task, our method outperforms all baselines with a lightweight model because the abundant source data helps to learn the low-light enhancement task in the source+$k$-shot setting successfully as compared to using only target domain data. While the $k$-shot samples help to predict the output in the target domain, the task is transferred successfully from the large source domain to the target camera domain.

% Capturing a low-light raw image dataset is difficult, and different cameras have their own color-space and noise distributions, hence there is a need for domain adaptation that can transfer the task in a few-shot setting across camera sensor domains. Due to the high complexity of the task, we believe our method outperforms all baselines because the abundant source data (Sony dataset has three times more images than Nikon/Canon dataset) helps to learn the low-light enhancement task in the source+$k$-shot setting successfully as compared to using only target domain data. While the $k$-shot samples help to predict the output in the target domain, the task is transferred successfully from the large source domain to the target camera domain.

% We also report the results for other baseline methods by training them on the full Sony source dataset and then fine-tuning them on the Nikon target dataset in a $k$-shot manner (Refer Table \ref{tab:recent} and \ref{tab:finetunenikon}). 

% \textbf{Comparison with Baselines}:
% \label{subsec:baselines}
% In Table \ref{tab:recent}, we show a quantitative comparison of our method with recent low-light enhancement methods. We train the respective baselines with the full Sony source dataset and then fine-tune them on the Canon or Nikon target datasets in a $k$-shot manner. Our method outperforms all other $k$-shot fine-tuned baselines both in terms of PSNR and SSIM for the Nikon and Canon camera target datasets.


% To understand the generalization capacity of our approach, we trained our model with images captured by OnePlus as the target image set. The number of target images used was four, and the Sony dataset was used as source image. An illustrative output is shown in the figure \ref{fig:oneplus}. Using smartphone camera images as target data, our approach can generate very good quality output images from very low-light data.

% \subsection{Qualitative Evaluation} \label{subsec:qual}
\textbf{Qualitative Evaluation:} \label{subsec:qual} In Fig. \ref{fig:nikon_eg1}, we qualitatively compare the results from our method and baselines when trained with Sony as source and Nikon (top row) or Canon (bottom row) as target. As highlighted by the zoomed-in regions and red arrows, the baseline results have several artifacts in terms of noise and color. Although the LSID model trained with full target data performs better than the $k$-shot model (for Nikon or Canon as target), it is still sub-par compared to our method's results. We also show qualitative results for our method trained with Sony as source and four OnePlus camera images (Fig. \ref{fig:oneplus}(a)) or four Google Pixel camera images (Fig. \ref{fig:pixel}(b)) as target to demonstrate the effectiveness of our method on low-cost smartphone camera data, which typically have higher noise severity in low-light. 
% HDRCNN \cite{EKDMU17}, Unprocess \cite{Brooks_2019_CVPR}, $k$-shot LSID, and full-dataset LSID methods
\begin{figure}
\begin{tabular}{cc}
\includegraphics[page=1, width=6.15cm, clip, trim=0cm 3.5cm 0cm 0cm]{Images/OnePlus-Results.pdf}&\hspace{-3.5mm}
\includegraphics[page=1, width=6.15cm, clip, trim=0cm 3.5cm 0cm 0cm]{Images/Pixel-Results.pdf}\\
(a)&(b)
\end{tabular}
\caption{Results for Sony as source with (a) OnePlus 5 as target (b) Google Pixel as target.}
\label{fig:oneplus}
\label{fig:pixel}
\vspace{-0.5cm}
\end{figure}

\begin{table}[t]
\centering
\caption{Quantitative evaluation of Sony as source with OnePlus or Pixel as target. The LSID model trained with full OnePlus dataset ($k$=12) achieves 28.88dB PSNR and 0.708 SSIM and when trained with full Pixel dataset ($k$=10) attains 27.95dB PSNR and 0.759 SSIM. (Note: test set for OnePlus=38 images and Pixel=18 images).}
\label{tab:oneplus}
\label{tab:pixel}
\scalebox{0.71}{
\begin{tabular}{c|c}
    \begin{tabular}{|c|cccccc}
        \hline
        Method & \multicolumn{6}{c}{Sony source w/ OnePlus target} \\ \hline
        \multirow{1}{*}{$k$ ($\rightarrow$)} & \multicolumn{2}{c}{1} & \multicolumn{2}{c}{2} & \multicolumn{2}{c}{4} \\
        \hline
        & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM  \\ \hline
        LSID & 22.54 & 0.617 & 25.87 & 0.651 & 26.69 & 0.670 \\ 
        Proposed & \textbf{25.10} & \textbf{0.633} & \textbf{26.47} & \textbf{0.650} & \textbf{27.77} & \textbf{0.712} \\ 
        % LSID (Full) & \multirow{6}{*}{28.89  0.708} \\
        \hline
    \end{tabular}
    &
   \begin{tabular}{cccccc|}
        \hline
        \multicolumn{6}{c|}{Sony source w/ Pixel target} \\ \hline
        \multicolumn{2}{c}{1} & \multicolumn{2}{c}{2} & \multicolumn{2}{c|}{4} \\
        \hline
        PSNR & SSIM & PSNR & SSIM & PSNR & SSIM  \\ \hline
        13.86 & 0.296 & 21.91 & 0.695 & 27.62 & 0.760 \\ 
        \textbf{18.60} & \textbf{0.635} & \textbf{24.98} & \textbf{0.753} & \textbf{29.60} & \textbf{0.782} \\ 
        % LSID (Full) & \multirow{6}{*}{28.89  0.708} \\
        \hline
    \end{tabular}
\end{tabular}
}
\end{table}


\begin{table}[t]
\centering
% 1 & Finetuning only last layers of LSID &  27.30 $\pm$0.60\\
% 2 & Finetuning all layers of LSID &  27.93 $\pm$0.66\\
\caption{Ablation study for training with Sony source and 4-shot Nikon as target dataset. The table reports mean PSNR/SSIM over three different runs. See section \ref{sec:ablation} for details.}
\label{tab:ablation}
\scalebox{0.85}{
\begin{tabular}{llcc}
\hline
No. & \multicolumn{1}{c}{Details} & \multicolumn{1}{c}{PSNR} & \multicolumn{1}{c}{SSIM} \\ \hline
% 1 & LSID trained w/ full Sony &  28.50 & 0.774 \\
% 2 & CIE-XYZ Input & 27.16 & 0.894 \\
% 3 & Finetuning last layer of LSID &  27.30 & 0.886\\
% 4 & Finetuning all layers of LSID &  27.93 & 0.899\\
1 & Separate encoder and decoder &  28.62 & 0.867\\
2 & Combined encoder & 29.20 & 0.890 \\
% 7 & LAB Input &  27.90 & 0.827 \\
% 5 & With discriminator loss &  28.64\\
% 6 & Pseudo-labeling &  29.44 & 0.907\\
% 3 & 8-bit RGB input & 27.88 & 0.812 \\
% 4 & LAB input &  27.90 & 0.827\\
3 & Proposed w/ Source $\ell_1$ loss & 27.14 & 0.807\\
% 5 & Color channel swapping & 28.36  \\
4 & Proposed w/o Source SSIM loss &  29.38 & 0.902\\
5 & Proposed & \textbf{30.30} & \textbf{0.913} \\ \hline
\end{tabular}
}
\end{table}

% We experiment with raw low-light images taken with smartphone cameras - Google Pixel and OnePlus 5, to investigate the performance of our proposed approach on raw images captured with low-cost smartphone camera sensors, which typically have higher noise severity in low-light conditions. We show qualitative results for our method trained with full Sony camera data as source and four Google Pixel camera images as target (Fig. \ref{fig:pixel}) and four OnePlus camera images as target (Fig. \ref{fig:oneplus}). We also show results for an LSID model trained with Sony as source and fine-tuned with the respective $k$-shot smartphone camera images.

% --------------------------------------------------------------------------------------------------------------------
% \textbf{Discussion:} As motivated in section \ref{sec:intro}, capturing a low-light raw image dataset is difficult, and since different cameras have their own color-space and noise distributions, there is a need for domain adaptation that can effectively transfer the task in a few-shot setting for camera sensor domains. Due to the high complexity of the task, we believe that our method outperforms all baselines because the abundant source data (Sony dataset has three times more images than Nikon/Canon dataset) helps to learn the extreme low-light image enhancement task in the source+$k$-shot model successfully as compared to using only target domain data. While the $k$-shot samples help to predict the output in the target domain, the task is transferred successfully from the large source domain to the target camera domain.

% TODO Are we discussing the reason why RAW performs well than sRGB in all methods in general ? We can mention this as a reason why we experimented much with RAW input.

% --------------------------------------------------------------------------------------------------------------------

% \begin{table}[t]
% \centering
% \caption{Comparison of our method with few-shot fine-tuned baselines trained with Sony raw as source and Canon raw as target.}
% \label{tab:finetunecanon}
% \scalebox{0.91}{
% \begin{tabular}{@{}ccccccc@{}}
% \hline
% \multicolumn{1}{c}{Method} & \multicolumn{6}{c}{Sony source w/ Canon target} \\ \hline
% \multirow{1}{*}{$k$ ($\rightarrow$)} & \multicolumn{2}{c}{1} & \multicolumn{2}{c}{3} & \multicolumn{2}{c}{6} \\ \hline
% & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM  \\ \hline
% HDRCNN \cite{EKDMU17} & 15.07 & 0.593 & 15.89 & 0.627 & 16.14 & 0.633 \\ 
% Unprocess \cite{Brooks_2019_CVPR} & 16.97 & 0.609 & 22.15 & 0.671 & 27.20 & 0.733\\ 
% LSID \cite{chen2018learning} & \textbf{25.88} & \textbf{0.792} & 28.38 & 0.831 & 28.85 & 0.819 \\ 
% Proposed & 24.29 & 0.623 & \textbf{28.78} & \textbf{0.841} & \textbf{33.22} & \textbf{0.896} \\ \hline
% \end{tabular}
% }
% \end{table}

% --------------------------------------------------------------------------------------------------------------------

% \begin{table}[t]
% \centering
% \caption{Comparison of our method with few-shot fine-tuned baselines trained with Sony raw as source and Nikon raw as target.}
% \label{tab:finetunenikon}
% \begin{tabular}{@{}ccccccc@{}}
% \hline
% \multicolumn{1}{c}{Method} & \multicolumn{6}{c}{Sony source w/ Nikon target} \\ \hline
% \multirow{1}{*}{$k$ ($\rightarrow$)} & \multicolumn{2}{c}{1} & \multicolumn{2}{c}{2} & \multicolumn{2}{c}{4} \\ \hline
% & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM  \\ \hline
% HDRCNN \cite{EKDMU17} & 11.47 & 0.648 & 12.36 & 0.578 & 12.87 & 0.627 \\ 
% Unprocess \cite{Brooks_2019_CVPR} & 18.90 & 0.710 & 21.91 & 0.690 & 25.63 & 0.761 \\ 
% LSID \cite{chen2018learning} & 22.38 & 0.746 & 25.75 & 0.874 & 27.93 & 0.899 \\ 
% Proposed & \textbf{25.27} & \textbf{0.860} & \textbf{28.06} & \textbf{0.909} & \textbf{30.30} & \textbf{0.913} \\ \hline
% \end{tabular}
% \end{table}
% --------------------------------------------------------------------------------------------------------------------
\textbf{Ablation Study:} \label{sec:ablation}
We discuss relevant ablations for our proposed method trained with Sony as source and 4-shot Nikon as target (Refer Table \ref{tab:ablation}). The details are as follows:
%\begin{itemize}[wide,itemindent=2.5em,noitemsep,topsep=0pt,itemsep=0pt]
\begin{itemize}[wide,itemindent=0.5em,noitemsep,topsep=0pt,itemsep=0pt]
% \item \textit{CIE-XYZ Input}: In the processing of a RAW image to sRGB, there is an intermediate conversion to the CIE-XYZ common color space before applying non-linear post-processing steps. We formulate this experiment as a CIE-XYZ to sRGB conversion to validate the requirement for domain adaptation for the extreme low-light image enhancement task. An LSID model trained with Sony data as source and fine-tuned on 4 Nikon camera images attains a PSNR of 27.16 and SSIM of 0.894 which is lesser than the proposed approach.
\item \textit{Separate Encoder and Decoder}: Transfer learning methods performed sub-optimally primarily due to the color bias. Hence, we trained separate encoders, a shared U-net, and separate camera-specific decoders to learn different camera color spaces individually. This approach obtained a PSNR of 28.62dB and SSIM of 0.867, and the results had visible color gaps between the model’s output and ground truth.
\item \textit{Combined Encoders}: To verify camera-specific denoising by the encoders, we used a shared encoder followed by a shared $\mathbb{N}$ network. But, we noticed the colors of the target domain's output to be dominated by the large source domain data. We obtain a PSNR of 29.20dB and SSIM of 0.890, which is better than fine-tuning LSID, but the colors of the target domain’s output validation images were dominated by the source training images.
% \item \textit{LAB input}: We trained the proposed model using input images in the LAB color space to investigate color quality performance. We used separate models to train for the L-channel and the a,b-channels. The $L_1$ and $\mathcal{L}_{SSIM}$ loss are used for the L-channel; only $L_{1}$ loss was used for the a,b-channels. We obtain L and ab outputs separately from the model during inference time and convert them to the corresponding RGB image. The outputs showed higher color loss and an increase in noise obtaining a PSNR of 27.90dB and SSIM of 0.827.
\item \textit{Proposed w/ Source $\mathcal{\ell}_{1}$}: Training with a strong supervision loss for the source domain strongly influenced the colors of the target domain's output images.
\item \textit{Proposed w/o $\mathcal{L}_{SSIM}$}: From previous ablations, we observe the source domain influencing the target output image color. Hence, we separate only the encoder while merging the U-net and the decoder to get ($\mathbb{N}$). We experimented with several loss functions, including combinations of $\ell_{2}$ loss, grayscale SSIM, gradient loss, $\ell_1$ loss, and cosine similarity loss. We found that using cosine similarity loss and SSIM loss for the source domain, and $\ell_{1}$ loss \cite{zhao2017loss} for the target led to better preservation of color and structure information. 
\item \textit{The $\mathcal{D}$ network for SSIM loss}: As discussed in section \ref{sec:method}, type-casting the source output from 16-bit to 8-bit space will still possess domain specific details. The SSIM loss is used to compare the brightness and structural details but not the color quality (cosine similarity is for color). Thus, following the camera ISP processing steps, we train a 16-to-8-bit conversion U-net model ($\mathcal{D}$ in Fig. 2(b)) to convert the 16-bit data to 8-bit data. The $\mathcal{D}$ network is trained to perform the following non-linear operations: White balancing, Gamma correction, Quantization and JPEG compression. From experiments, we observe that a U-net is necessary to learn all the above mentioned non-linear operations. 
% \item \textit{The $\mathcal{D}$ network and SSIM loss.} As discussed in section \ref{sec:pipeline}, type-casting the source output from 16-bit to 8-bit space will still possess domain specific details. The SSIM loss is used to compare the brightness and structural details but not the color quality (cosine similarity is for color). Thus, following the camera ISP processing steps, we train a 16-to-8-bit conversion U-net model ($\mathcal{D}$ in Fig. \ref{fig:prop_overview}) to convert the 16-bit data to 8-bit data. The $\mathcal{D}$ network is trained to perform the following non-linear operations: White balancing, Gamma correction, Quantization and JPEG compression. 
\end{itemize}

% \begin{enumerate}[wide,itemindent=1em,noitemsep,topsep=0pt,itemsep=0pt]
%\item \textit{Fine-tuning only last layers of LSID}: We fine-tune the pre-trained LSID model with target data to understand how well the task can be transferred to different camera domain. Here, we fine-tune only the last layer of the pre-trained Sony model using four random images from the Nikon training set. We repeated this step 3 times for different sets to get the average performance. All the fine-tuned models were then tested using the Nikon validation set. The average PSNR value was 27.30 on the validation set. %There seemed to be many artifacts on output images \ref{fig:ablation_eg1}. 
    % \begin{figure}[t]
    % \begin{center}
    % \includegraphics[width=0.99\linewidth]{figures/fine-tune.png}
    % \end{center}
    %   \caption{Output of Fine-tuned Sony model using 4 nikon images. The artifacts are encircled with red color.}
    % \label{fig:ablation_eg1}
    % \end{figure}
    
% \item \textit{Fine-tuning all layers of LSID}: We also fine-tuned all the layers of the pre-trained Sony model using 4 randomly chosen Nikon training images. Outputs from the fine-tuned model were unable to capture the color distribution of the target Nikon camera. The color of the output images was affected by the color distribution of the source Sony camera. These transfer learning methods \cite{transferlearning} gave a better performance than just using Sony pre-trained model for image enhancement, which gave a 25.90 PSNR value on the validation set. We have used $L_1$ loss for both the fine-tuning training.

% \textbf{Separate encoders and decoder}: Transfer learning methods performed sub-optimally primarily due to the color bias. Hence, we trained separate encoders and separate camera-specific decoders in addition to the shared U-net to learn the different camera color spaces individually.

% In every training epoch, we did following four training,
% \begin{enumerate}
% \item update the weights of source encoder and decoder. 
% \item update the weights of source pipeline (source encoder,common denoiser,source decoder)
% \item update the weights of target encoder and decoder. 
% \item update the weights of source pipeline (target encoder,common denoiser,target decoder)
% \end{enumerate}

% Though this approach gave a better performance than fine-tuned models in terms of PSNR, there were still visible color gaps between model output and ground truth. Along with this color bias problem, we found that trying to learn the image enhancement task using this approach is quite time-consuming.% as there are a lot of trainings involved.

% \textbf{Combined encoders}: To verify the camera-specific denoising performed by the encoders, we merged the source and target encoders. But, we noticed the colors of the target domain's output validation images to be dominated by the large number of source training images (Refer Fig. 6) % \ref{fig:newablation_eg2}).

% \textbf{Proposed approach with Source $\mathcal{L}_{1}$}: Training with a strong supervision loss for the source domain strongly influenced the colors of the target domain's output images.

% \item \textit{With discriminator loss}: In this ablation, we introduced a discriminator loss \cite{goodfellow2014generative} along with $\mathcal{L}_{SSIM}$ and $\mathcal{L}_{CS}$ losses for the source domain. We first train a discriminator separately to classify each pixel of the input belonging to either source or target class. Then, the pre-trained discriminator is used as a loss function to encourage the model to predict the output in the target domain. In addition to difficulty in training the discriminator, the output contained structural and color distortions.

% \item \textit{Pseudo-labeling}: Following \cite{lee2013pseudo}, we tried training a model with few labeled and many unlabeled images from the target domain along with labeled source domain data. We generate the output by a pre-trained Sony model for the unlabeled target data and call them pseudo-labels. Then, we train the model with source and target labeled and unlabeled data. For target unlabeled data, we compute the error between the prediction and the pseudo-labels using $\mathcal{L}_{SSIM}$. Though it performs second-best compared to the proposed architecture, the results were still influenced by color statistics in the source domain, which resulted in 29.44dB PSNR.

% \item \textit{8-bit RGB input}: Instead of training with 4-channel Bayer input, we trained a model with 3-channel 8-bit RGB input. We found the presence of noise in the output images, and there was loss of color and detail in output as well.
% \item \textit{LAB input}: In this experiment, we trained the model using the LAB color space input image to improve the color quality performance. We used separate models to train for L-channel and the a,b-channels. $L_1$ and $\mathcal{L}_{SSIM}$ loss is used to train L-channel; only L1 loss was used to train the ab-channels. We obtain L and ab outputs individually from models during inference time and convert them to the corresponding RGB image. The output of this model had more loss of color as compared to other models.

% \item \textit{Color channel swapping}: In all the ablation studies, we have been using very few target images and many source images to perform image enhancement for the target domain. In this ablation, we increased the number of target images by a simple data augmentation technique. We generated more target images by swapping the RGB color channel of the few target images. This training set up was similar to the previous one, where we trained the model with 8-bit RGB input. We did not find much of a performance boost in this method; the outputs still had noise traces.

% \textbf{Proposed approach without $\mathcal{L}_{SSIM}$}: From the previous ablations, we observe the source domain influencing the target output image color. Hence, we separate only the encoder while merging the shared U-net and the decoder ($\mathbb{N}$). We experimented with various loss functions including combinations of $L_{2}$ loss, grayscale SSIM, gradient loss, $L_1$ loss, and cosine similarity loss. We found that using a combination of cosine similarity loss and SSIM loss for the source domain and the $L_{1}$ loss\cite{zhao2017loss} for the target led to better preservation of color and structural information. 
%When both source and target pipelines were trained with $L_{1}$ loss, some of the outputs were affected by the color bias.

% \item \textit{Proposed Approach}: We found that a combination of 16-bit cosine similarity loss and 8-bit SSIM loss for source camera and L1 loss for target pipeline produces better outputs from different experiments. To get an 8-bit SSIM loss, we used a pre-trained converted before loss calculation. In terms of performance (PSNR/SSIM), this approach outperforms other baseline experiments. We also noticed that this approach's outputs have more noise reduction than other methods and even have a correct color transformation during inference.
% \end{enumerate}

% \textbf{Effect of camera ISO.} The images in our proposed dataset were captured with ISO 100-200. Other datasets were captured with ISO between 50-200. The range of ISO does not affect the performance of our approach due to the wide range of exposure ratios used to train our model. A single encoder is sufficient to handle all ISO values.

% \textbf{The $\mathcal{D}$ network and SSIM loss.} As discussed in section \ref{sec:pipeline}, type-casting the source output from 16-bit to 8-bit space will still possess domain specific details. The SSIM loss is used to compare the brightness and structural details but not the color quality (cosine similarity is for color). Thus, following the camera ISP processing steps, we train a 16-to-8-bit conversion U-net model ($\mathcal{D}$ in Fig. \ref{fig:prop_overview}) to convert the 16-bit data to 8-bit data. The $\mathcal{D}$ network is trained to perform the following non-linear operations: White balancing, Gamma correction, Quantization and JPEG compression. 

% From experiments, we observe that a U-net is necessary to learn all the above mentioned non-linear operations. Although $\mathcal{D}$ learns camera specific operations, it can be used to convert other camera data to 8-bit without re-training because most camera specific operations do not affect the structural detail of the image and we only use SSIM loss on $\mathcal{D}$’s output to compare structural consistency. $\mathcal{D}$ network trained on one camera data works well even for other cameras. To verify, we trained a $\mathcal{D}$ network on Sony camera data and tested on Canon camera data without re-training and obtained an SSIM score of 0.985 while the $\mathcal{D}$ network trained on Canon camera data achieved an SSIM of 0.990.


% \begin{figure}[t]
% \label{fig:newablation_eg2}
% \begin{center}
% \includegraphics[width=8cm]{3_BMVC/Images/approach_3_LR.png}
% \end{center}
%   \caption{Qualitative example from (a) model trained with combined encoders ablation experiment, (b) Proposed approach. \textcolor{red}{Move this figure to Supple.}}
% \end{figure}

% \begin{figure}[t]
% \begin{center}
% \includegraphics[width=\linewidth, clip, trim=0cm 3.7cm 0cm 0cm]{3_BMVC/Images/OnePlus-Results.pdf}
% \caption{Results for Sony as source and OnePlus 5 as target.}
% \label{fig:oneplus}
% \end{center}
% \end{figure}

% \begin{figure}[t]
% \begin{center}
% \includegraphics[page=1, width=8.3cm, clip, trim=0cm 3.7cm 0cm 0cm]{3_BMVC/Images/Pixel-Results.pdf}
% \caption{Results for Sony as source and Google Pixel as target.}
% \label{fig:pixel}
% \end{center}
% \end{figure}
