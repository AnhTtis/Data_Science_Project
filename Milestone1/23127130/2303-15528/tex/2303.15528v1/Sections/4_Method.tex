% \begin{table}[t]
% \centering
% \caption{Details of the datasets used in our work.}
% \label{tab:datasets}
% \begin{tabular}{lccc}
% \hline
% Datasets & \begin{tabular}[c]{@{}c@{}}Exposure\\ Ratios\end{tabular} & \begin{tabular}[c]{@{}c@{}}Training\\ images\end{tabular} & \begin{tabular}[c]{@{}c@{}}Validation\\ images\end{tabular} \\ \hline
% Sony \cite{chen2018learning} & 90,150,300 & 161 &  36\\
% Nikon &  100, 300 & 53 &  24 \\
% Canon \cite{CanonLSID} & 50, 150, 300 & 44 &  21\\ \hline
% \end{tabular}
% \end{table}
%--------------------------------------------------------------------------------------------------------------------
% \begin{figure*}
% \begin{center}
% \includegraphics[width=\textwidth, clip, trim=0cm 15.55cm 0.75cm 0.05cm]{figures/FDA-LSID.png}
% \end{center}
%   \caption{Proposed few-shot domain adaptation model architecture.}
% \label{fig:model}
% \end{figure*}
% % \begin{figure*}
% % \begin{center}
% % \includegraphics[scale=0.42]{figures/convert.png}
% % \end{center}
% %   \caption{Source camera specific 16-to-8-bit converter.}
% % \label{fig:convert}
% % \end{figure*}
\begin{figure}[t]
\begin{tabular}{cc}
\includegraphics[page=1, width=5.8cm]{Images/nikon.png}&\hspace{+1mm}
\includegraphics[page=1, clip, trim=0.6cm 16.75cm 7.5cm 0.15cm, scale=0.9]{Images/block_diagram.pdf}\\
(a)&(b)
\end{tabular}
\caption{(a) Example short-exposure and long-exposure image pairs from the Nikon dataset. The short exposure images are almost entirely dark whereas the long-exposure images have immense scene information. (b) Overview of our few-shot domain adaptation method.}
\label{fig:nikon}
\label{fig:prop_overview}
\end{figure}
With a noisy raw image captured with low-exposure time (i.e., shutter speed) as input, our CNN-based approach is trained to predict a clean long-exposure sRGB output of the same scene. The input is multiplied by an exposure factor calculated by the ratio of output and input exposure times. For example, to generate a 10-second long exposure output, the input 0.1-second low exposure image must be multiplied by 100. As a result of this operation, along with illumination, the noise is also amplified proportionally. Since we multiply the factor in the unprocessed raw domain and expect the output in the sRGB domain, the network must learn camera hardware-specific enhancement as well as its entire ISP pipeline (lens correction, demosaicing, white balancing, color manipulation, tone curve application, color space transform, and Gamma correction). Thus, a model trained on one specific camera data (source domain) does not translate similar performance to a different camera (target domain), hence the domain gap. In this paper, we propose to transfer the enhancement task from large labeled source data and generate output in the target domain using few labeled target data.

\textbf{Problem formulation}: We denote source domain ($\mathbf{S}$) with input short-exposure images as $\{S_n\}$ and corresponding long-exposure ground truth as $\widehat{\mathbf{S}}\!=\!\widehat{S}_n$, $\forall n=1,\cdots,N$. Similarly, the target domain ($\mathbf{T}$) consists of input images $\{T_m\}$ and corresponding ground truth, $\widehat{\mathbf{T}}\!=\!\widehat{T}_m$, $\forall m=1,\cdots,M$. Note that $N$ is much greater than $M$, $N\gg M$. With both $\mathbf{S}$ and $\mathbf{T}$ as input, we train a CNN model ($\mathbb{N}$) to generate enhanced long-exposure output ($\widetilde{\mathbf{S}}$ and $\widetilde{\mathbf{T}}$). Our method is illustrated in Fig. \ref{fig:prop_overview}(b) with the source and target training pipelines. It is an end-to-end trainable deep network that takes the raw sensor arrays as input and performs image enhancement utilizing the source data for few-shot domain adaptation to the target data.

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\linewidth]{3_BMVC/Images/Nikon-Results-1.pdf}
%     \caption{Qualitative comparison with methods tested on Nikon target images. (b) HDRCNN and (c) Unprocess are trained on Sony source and fine-tuned on 4-Nikon target images, LSID with (d) 4-Nikon target images and (e) full ($k$=53) Nikon training dataset, (f) Our few-shot domain adaptation approach with 4-Nikon target images and 161 Sony source images.}
%     \label{fig:nikon_eg1}
% \end{figure*}
% \begin{table}[t]
% \centering
% \caption{Quantitative comparison of Sony as source and Nikon as target dataset. The improvement of proposed method over only $k$ shot trained model is shown in brackets. The LSID model trained with full Nikon dataset ($k$=53) achieves 30.74dB PSNR and 0.803 SSIM.}
% \label{tab:nikon}
% \scalebox{0.735}{
% \begin{tabular}{@{}lccc|ccc@{}}
% \hline
%  & \multicolumn{3}{c|}{PSNR} & \multicolumn{3}{c}{SSIM} \\ \hline
% $k$ ($\rightarrow$) & 1 & 2 & 4 & 1 & 2 & 4 \\ \hline
% \begin{tabular}[c]{@{}l@{}}LSID\\ (only $k$ target)\end{tabular} & 23.20 $\pm$ 3.06 & 27.27 $\pm$ 0.384 & 28.05 $\pm$ 1.53 & 0.679 $\pm$ 0.172 & 0.819 $\pm$ 0.031 & 0.864 $\pm$ 0.0111 \\ \hline
% \begin{tabular}[c]{@{}l@{}}Proposed\\ ($k$ target + source)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{25.27} $\pm$ 0.58\\ (+2.07)\end{tabular} &
% \begin{tabular}[c]{@{}c@{}}\textbf{28.06} $\pm$ 0.671\\ (+0.79)\end{tabular} &
% \begin{tabular}[c]{@{}c@{}}\textbf{30.30} $\pm$ 0.52\\ (+2.25)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{0.860} $\pm$ 0.010\\ (+0.181)\end{tabular} &
% \begin{tabular}[c]{@{}c@{}}\textbf{0.909} $\pm$ 0.0028\\ (+0.090)\end{tabular} &
% % \begin{tabular}[c]{@{}c@{}}\textbf{0.913} $\pm$ 0.006\\ (+0.049)\end{tabular} \\ \midrule
% % \begin{tabular}[c]{@{}l@{}}LSID\\ (full target, $k$ = 53)\end{tabular} & \multicolumn{3}{c|}{30.74} & \multicolumn{3}{c}{0.803} \\ \bottomrule
% \begin{tabular}[c]{@{}c@{}}\textbf{0.913} $\pm$ 0.006\\ (+0.049)\end{tabular} \\ \hline
% \end{tabular}
% }
% \end{table}

\textbf{Encoders}: \label{sec:pipeline} The significant domain gap between the source and target domains necessitates the extraction of separate and independent features from each domain before processing with a shared enhancement network ($\mathbb{N}$). Hence, we use a source encoder ($\mathcal{E}_S$) and a target encoder ($\mathcal{E}_T$). We first pack the input raw sensor arrays into a four-channel vector (for Bayer arrays from Sony, Nikon, and Canon cameras) and subtract the black level (reference voltage). Then, the packed array is multiplied by the exposure ratio factor and passed as input to the respective domain encoder. It should be noted that the exposure ratio factors need not be the same between the source and the target domain (See Table \ref{tab:datasets}). For the encoder network, we use three convolutional layers with $\{16,32,64\}$ filters and 3$\times$3 kernel size. 

\textbf{Enhancement Network}: The source and target domain encoder features are passed separately to a shared common enhancement network, $\mathbb{N}$. By having a common enhancement network, the large pool of source data helps to improve the enhancement quality of $\mathbb{N}$, while the few target samples ensure that the output is in the target domain. We use U-Net architecture for the enhancement network. Further, the network has a pixel shuffle layer to convert 12-channel prediction to 16-bit three channel sRGB output. The objective of $\mathbb{N}$ is to enhance, denoise, perform other ISP operations (AWB, color manipulation, etc.), and finally demosaicking to generate an sRGB output. $\mathbb{N}$ generates enhanced output $\widetilde{\mathbf{T}}$ for the target domain data as, $\widetilde{\mathbf{T}} = \mathbb{N}\big(\mathcal{E}_T(\mathbf{T}) \big)$. Similarly, $\widetilde{\mathbf{S}}$ for the source domain as, $\widetilde{\mathbf{S}} = \mathbb{N}\big(\mathcal{E}_S(\mathbf{S}) \big)$.
%\footnote{Please refer to the supplementary material for detailed network definition.}

\textbf{Losses}: For the target domain, we compute the $\ell_1$ loss between the prediction ($\widetilde{\mathbf{T}}$) and the ground truth ($\widehat{\mathbf{T}}$) as, $\mathcal{L}_{target} = \ell_1\big(\widetilde{\mathbf{T}},\widehat{\mathbf{T}} \big)$. The source domain loss consists of two components: cosine similarity loss and SSIM loss. We compute cosine similarity between $\widetilde{\mathbf{S}}$ and $\widehat{\mathbf{S}}$ as, 
$
    \mathcal{L}_{CS}(\widetilde{\mathbf{S}},\widehat{\mathbf{S}})= 1 -  \frac{{\widetilde{\mathbf{S}} \cdotp \widehat{\mathbf{S}}}}{\|\widetilde{\mathbf{S}}\|\times\|\widehat{\mathbf{S}}\|}
$.
% \begin{equation}
%     \mathcal{L}_{CS}(\widetilde{\mathbf{S}},\widehat{\mathbf{S}})= 1 -  \frac{{\widetilde{\mathbf{S}} \cdotp \widehat{\mathbf{S}}}}{\|\widetilde{\mathbf{S}}\|\times\|\widehat{\mathbf{S}}\|}
% \end{equation}
Cosine similarity loss is weak supervision for the source domain and is used instead of $\ell_1$ loss since $N\gg M$, and using a strong supervision loss like $\ell_1$ optimizes for pixel values to train $\mathbb{N}$, making the network predict the output in the source domain even for target domain input. Cosine similarity loss ensures that the prediction and the ground truth are in a similar direction. Hence with $\mathcal{L}_{CS}$, $\mathbb{N}$ can still perform enhancement while predicting in target domain even for source domain input. Further, when trained with Sony as source and 4-shot Nikon as target (Table \ref{tab:ablation}) with $L_1$ loss for the source, we obtain only 27.14dB PSNR for target domain validation, whereas using $\mathcal{L}_{CS}$ loss for source achieves 30.30dB PSNR.

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\linewidth]{3_BMVC/Images/Canon-Results-1.pdf}
%     \caption{Qualitative comparison with methods tested on Canon target images. (b)HDRCNN and (c) Unprocess are trained on Sony source and fine-tuned on 6-Canon target images, LSID with (d) 6-Canon target images and (e) full ($k$=44) Canon training dataset, (f) Proposed few-shot domain adaptation approach with 6-Canon target images and 161 Sony source images.}
%     \label{fig:canon_eg2}
% \end{figure*}

\begin{figure*}[t]
\centering
\subfigure{\includegraphics[width=\linewidth]{Images/Nikon-Results-1.pdf}}\\ \vspace{-1.65\baselineskip}
\subfigure{\includegraphics[width=\linewidth]{Images/Canon-Results-1.pdf}}
\caption{Qualitative comparison with methods tested on Nikon (top row) and Canon (bottom row) target images. (a) Input after multiplying by exposure factor, results from (b) HDRCNN and (c) Unprocess methods are after training on full Sony source and fine-tuning on $k$-shot target images, LSID with (d) $k$-shot target images and (e) full target training dataset ($k$=53 for Nikon and $k$=44 for Canon), (f) Proposed few-shot domain adaptation method with 161 Sony source images and 4-shot Nikon (top row) and 6-Canon (bottom row) target images.}
\label{fig:nikon_eg1}
\label{fig:canon_eg2}
\end{figure*}

% \begin{table}[t]
% \centering
% \caption{Quantitative comparison of Sony as source and Canon as target dataset. The improvement of proposed method over only $k$ shot trained model is shown in brackets. The LSID model trained with full Canon dataset ($k$=44) attains 32.32dB PSNR and 0.899 SSIM.}
% \label{tab:canon}
% \scalebox{0.73}{
% \begin{tabular}{@{}lccc|ccc@{}}
% \hline
%  & \multicolumn{3}{c|}{PSNR} & \multicolumn{3}{c}{SSIM} \\ \hline
% $k$ ($\rightarrow$) & 1 & 3 & 6 & 1 & 3 & 6 \\ \hline
% \begin{tabular}[c]{@{}l@{}}LSID\\ (only $k$ target)\end{tabular} & 21.54 $\pm$ 2.89 & 26.9 $\pm$ 2.37 & 29.36 $\pm$ 0.763 & 0.588 $\pm$ 0.182 & 0.785 $\pm$ 0.0051 & 0.829 $\pm$ 0.0073 \\ \hline
% % \begin{tabular}[c]{@{}l@{}}Proposed\\ ($k$ target + source)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{24.29} $\pm$ 3.16\\ (+2.75)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{28.78} $\pm$ 3.54\\ (+1.8)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{33.22} $\pm$ 0.45\\ (+3.86)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{0.623} $\pm$ 0.0074\\ (+0.035)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{0.841} $\pm$ 0.0335\\ (+0.056)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{0.896} $\pm$ 0.015\\ (+0.067)\end{tabular} \\ \midrule
% % \begin{tabular}[c]{@{}l@{}}LSID\\ (full target, $k$ = 45)\end{tabular} & \multicolumn{3}{c|}{32.32} & \multicolumn{3}{c}{0.899} \\ \bottomrule
% \begin{tabular}[c]{@{}l@{}}Proposed\\ ($k$ target + source)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{24.29} $\pm$ 3.16\\ (+2.75)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{28.78} $\pm$ 3.54\\ (+1.8)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{33.22} $\pm$ 0.45\\ (+3.86)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{0.623} $\pm$ 0.0074\\ (+0.035)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{0.841} $\pm$ 0.0335\\ (+0.056)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{0.896} $\pm$ 0.015\\ (+0.067)\end{tabular} \\ \hline
% \end{tabular}
% }
% \end{table}

From experiments (in section \ref{sec:exp}), we find better enhancement (in terms of PSNR) using the structural similarity index measure (SSIM) \cite{wang2004image} to compute perceived degradation and preserve the spatial structure in the source output with respect to the ground truth. We do not use SSIM directly on the 16-bit data as that causes the source data to heavily influence the domain adaptation since the source dataset is much larger. Hence, we apply SSIM in JPEG compressed 8-bit domain, where the structural domain difference is less. Since type-casting the 16-bit data to 8-bit will still possess domain-specific details, we train a 16-to-8-bit U-net model ($\mathcal{D}$ in Fig. \ref{fig:prop_overview}) to convert the output from 16-bit to post-processed 8-bit representation. 

The $\mathcal{D}$ network is trained to perform the following non-linear operations: White balancing, Gamma correction, Quantization, and JPEG compression. Even after JPEG compression, the prediction may have traces of source domain specific color information. Further, the SSIM loss is a strong pixel-wise supervision, and in order to avoid the source domain from heavily influencing $\mathbb{N}$, we compute SSIM loss only in grayscale space, not in RGB color space. Also, it follows the intuition that the structure and edge information of a scene will remain the same across images captured with different cameras, while the color space representation may vary. We find that without SSIM loss for the source, we obtain 29.38dB PSNR on target domain validation, whereas using SSIM loss achieves 30.30dB PSNR (Table \ref{tab:ablation}). For computing the SSIM loss, the ground truth ($\widehat{\mathbf{S}}$) is also converted offline to post-processed 8-bit data ($\widehat{\mathbf{S}}_{PP}$) using the rawpy post process function. Hence, the loss is obtained by computing SSIM loss between $\mathcal{D}(\widetilde{\mathbf{S}})$ and $\widehat{\mathbf{S}}_{PP}$,
$
    \mathcal{L}_{SSIM} = 1 - SSIM\Big(\mathcal{D}(\widetilde{\mathbf{S}}), \widehat{\mathbf{S}}_{PP}\Big)
$.
% \begin{equation}
%     \mathcal{L}_{SSIM} = 1 - SSIM\Big(\mathcal{D}(\widetilde{\mathbf{S}}), \widehat{\mathbf{S}}_{PP}\Big)
% \end{equation}
In Fig. \ref{fig:prop_overview}, the top branch guided by the deep red arrows shows the entire source camera training pipeline. It should be noted that $\mathcal{D}$ is used only to compute the loss but not in inference. Finally, we use the sum of cosine similarity loss ($\mathcal{L}_{CS}$) as well as the SSIM loss calculated in the 8-bit domain as the total loss for the source camera pipeline: $\mathcal{L}_{source} = \mathcal{L}_{CS} + \mathcal{L}_{SSIM}$. The total loss is the sum of target and source domain losses: $\mathcal{L}_{total}=\mathcal{L}_{target}+\mathcal{L}_{source}$.
% Source and target models are trained jointly. An
% epoch consists of 161 batches, with one source patch and
% one target domain patch per batch. Both source and target
% patches (of size 512 512, lines 478-480) are cropped at a
% random location from a randomly chosen source and target
% image.

% For the proposed method, we use the respective short-exposure raw sensor data as input to the source ($\mathbf{S}$) and target ($\mathbf{T}$) encoder networks. We first pack the input raw sensor arrays into a four-channel vector (for Bayer arrays from Sony, Nikon, and Canon cameras), subtract the black level (reference voltage), and multiply the input with the exposure ratio. There is one input for the source camera encoder ($\mathcal{E}_S$) and one for the target camera encoder ($\mathcal{E}_T$) in every training step. We pass the output from these camera-specific encoders through a shared $\mathbb{N}$ network to allow the model to learn both camera-specific and camera invariant properties. The output of the $\mathbb{N}$ network is a 12-channel image with half the spatial resolution. % , comprising of a U-net \cite{ronneberger2015u} followed by three CNN layers, 

% \begin{figure}[t]
%     \centering
%     \label{fig:prop_overview}
%     \includegraphics[page=1, clip, trim=0.6cm 16.75cm 7.5cm 0.15cm, scale=0.9]{3_BMVC/Images/block_diagram.pdf}
%     \caption{Overview of our few-shot domain adaptation model. }
% \end{figure}


% \subsection{Source and Target camera pipeline}\label{sec:pipeline}
% The source and target pipelines are trained jointly in an end-to-end manner. An epoch consists of 161 batches, with one source domain patch and one target domain patch per batch 
%given as input to the model. Both source and target patches are $512\times512$ random crops. % are cropped at a random location from a random source and target domain image at each train step.

% \textbf{Source camera pipeline.}
% \label{subsec:source}
% The packed raw input arrays from the source domain are passed to the source encoder, which learns camera-specific parameters to obtain an intermediate representation. We find visible denoising performance from experiments with the encoder heads, and subsequent analysis suggests effective learning of the camera's non-uniform noise model in extreme low-light conditions. The output from the source encoder is passed through the shared $\mathbb{N}$ network to obtain the source output feature maps, $\widetilde{\mathbf{S}}$. Bayer conversion with a sub-pixel layer \cite{shi2016real} unpacks the 12-channel data into a full resolution sRGB image.

% We compute the standard Cosine Similarity loss ($\mathcal{L}_{CS}$) between the source domain output ($\widetilde{\mathbf{S}}$) and the corresponding source domain long-exposure ground-truth image ($\widehat{\mathbf{S}}$),
% % \begin{equation}
% % \mathcal{L}_{CS}(\widetilde{\mathbf{S}},\widehat{\mathbf{S}})= 1 -  \frac{{\widetilde{\mathbf{S}} \cdotp \widehat{\mathbf{S}}}}{\bf \text{max}( \sqrt{({\bf \tilde{S}})^2} \cdotp \sqrt{({\bf \widehat{S}})^2})}
% % \end{equation}
% \begin{align}
%     \widetilde{\mathbf{S}} &= \mathbb{N}\big(\mathcal{E}_S(\mathbf{S}) \big), &    \mathcal{L}_{CS}(\widetilde{\mathbf{S}},\widehat{\mathbf{S}})&= 1 -  \frac{{\widetilde{\mathbf{S}} \cdotp \widehat{\mathbf{S}}}}{\|\widetilde{\mathbf{S}}\|\times\|\widehat{\mathbf{S}}\|}
% \end{align}
% % \begin{equation}
% % \mathcal{L}_{CS}(\widetilde{\mathbf{S}},\widehat{\mathbf{S}})= 1 -  \frac{{\widetilde{\mathbf{S}} \cdotp \widehat{\mathbf{S}}}}{\|\widetilde{\mathbf{S}}\|\times\|\widehat{\mathbf{S}}\|}
% % \end{equation}
  
% Cosine similarity loss is a weak supervision for the source domain and is used instead of $\ell_1$ loss since $N\gg M$, and using a strong supervision loss like $\ell_1$ optimizes for pixel values to train $\mathbb{N}$, making the network predict the output in the source domain even for target domain input. Cosine similarity loss ensures that the prediction and the ground truth are in a similar direction. Hence with $\mathcal{L}_{CS}$, $\mathbb{N}$ can still perform enhancement while predicting in target domain even for source domain input. Further, when trained with Sony as source and 4-shot Nikon as target (Table \ref{tab:ablation}) with $L_1$ loss for the source, we obtain only 27.14dB PSNR for target domain validation, whereas using $\mathcal{L}_{CS}$ loss for source achieves 30.30dB PSNR.

% From experiments (discussed in section \ref{sec:exp}), we find better enhancement (in terms of PSNR) using the structural similarity index measure (SSIM) \cite{wang2004image} to compute perceived degradation and preserve the spatial structure in the source output with respect to the ground truth. We do not use SSIM directly on the 16-bit data as that causes the source data to heavily influence the domain adaptation since the source dataset is much larger. Thus, we apply SSIM in JPEG compressed 8-bit domain, where the structural domain difference is less. Since type-casting the 16-bit data to 8-bit will still possess domain-specific details, we train a 16-to-8-bit U-net model ($\mathcal{D}$ in Fig. \ref{fig:prop_overview}) to convert the output from 16-bit to post-processed 8-bit representation. We find that without SSIM loss for the source, we obtain 29.38dB PSNR on target domain validation, whereas using SSIM loss achieves 30.30dB PSNR (Table \ref{tab:ablation}).

% % (we discuss the converter in section \ref{sec:ablation})

% For computing the SSIM loss, the ground truth ($\widehat{\mathbf{S}}$) is also converted offline to post-processed 8-bit data ($\widehat{\mathbf{S}}_{PP}$) using the rawpy post process function. Hence, the loss is obtained by computing SSIM loss between $\mathcal{D}(\widetilde{\mathbf{S}})$ and $\widehat{\mathbf{S}}_{PP}$.
% \begin{equation}
%     \mathcal{L}_{SSIM} = 1 - SSIM\Big(\mathcal{D}(\widetilde{\mathbf{S}}), \widehat{\mathbf{S}}_{PP}\Big)
% \end{equation}
% In Fig. \ref{fig:prop_overview}, the top branch guided by the deep red arrows shows the entire source camera training pipeline. Finally, we use the sum of cosine similarity loss ($\mathcal{L}_{CS}$) as well as the SSIM loss calculated in the 8-bit domain as the total loss for the source camera pipeline. 
% \begin{equation}
%     \mathcal{L}_{source} = \mathcal{L}_{CS} + \mathcal{L}_{SSIM}
% \end{equation}


% % \begin{figure*}[t]
% %     \centering
% %     \includegraphics[width=17.4cm, height=4.75cm]{ICCV/figures/Nikon-Results-1.pdf}
% %     \caption{Qualitative comparison between different methods tested on images from the Nikon dataset (target). The models are trained on (b) only 4 Nikon images, (c) full Nikon training dataset, and (d) 4 Nikon images and 161 Sony images with our proposed approach.}
% %     \label{fig:nikon_eg1}
% % \end{figure*}

% \textbf{Target camera pipeline.} 
% \label{subsec:target}
% We pass the packed target input raw data to the target encoder ($\mathcal{E}_T$). We then pass the encoded feature maps through the $\mathbb{N}$ network and then through the Bayer converter and calculate the target pipeline's loss in the 16-bit space. From several experiments with various loss functions, we have found the best image enhancement for the target pipeline is achieved with the $\ell_{1}$ loss between the predicted ($\widetilde{\mathbf{T}}$) and ground truth ($\widehat{\mathbf{T}}$),
% \begin{align}
%     \widetilde{\mathbf{T}} &= \mathbb{N}\big(\mathcal{E}_T(\mathbf{T}) \big), &    \mathcal{L}_{target} &= \ell_1\big(\widetilde{\mathbf{T}},\widehat{\mathbf{T}} \big)
% \end{align}
