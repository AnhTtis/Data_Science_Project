\begin{figure*}[t]
\centering
\scalebox{0.9}{
\subfigure{\includegraphics[width=\linewidth]{Images/stats-2.pdf}}
}
\caption{First-order and second order statistics for an image captured with Nikon and Canon cameras. (a) Histogram of intensities, (b) Histogram of spatial derivatives of intensity, (c) Joint histogram of responses from convolution filters.}
\label{fig:canon136}
\vspace{-0.4cm}
\end{figure*}

\begin{table}
\footnotesize
\centering
\setlength\tabcolsep{3pt}
\begin{minipage}[b]{0.52\textwidth}
\centering
\caption{Comparing LSID models trained with a complete source camera dataset and tested on respective target camera datasets.}%\cite{chen2018learning}
\label{tab:baseline}
% \tablewidth=\textwidth
\scalebox{0.95}{
\begin{tabular}{lcccccc}
\hline
Testing ($\rightarrow$) & \multicolumn{2}{c}{Sony} & \multicolumn{2}{c}{Nikon} & \multicolumn{2}{c}{Canon} \\ \hline
Training ($\downarrow$) & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\ \hline
Sony \cite{chen2018learning}  & \textbf{28.50  } & \textbf{0.774}   & 25.90  & 0.693   & 27.41   & 0.845   \\
Nikon  & 19.95   & 0.481   & \textbf{30.74 } & \textbf{0.803}  & 24.34  &   0.767  \\
Canon \cite{CanonLSID}  & 18.51   & 0.542   & 23.27   & 0.847   & \textbf{32.32  } & \textbf{0.899}   \\ \hline
\end{tabular}
}
% \caption{This is a ver very very long caption which doesn't overwrites the text on the right side of the paper.}
% \label{tab:accuracy} 
\end{minipage}%
\hfill
\begin{minipage}[b]{0.42\textwidth}
\centering
\caption{Details of the datasets used in our work.}
\label{tab:datasets}
\scalebox{0.95}{
\begin{tabular}{lccc}
    \hline
    \multirow{2}{*}{Datasets} & Exposure & Training & Testing \\
    & Ratios & Images & Images \\
    \hline
    Sony \cite{chen2018learning} & 90,15,300 & 161 & 36\\
    Nikon & 100,300 & 53 & 24\\
    Canon \cite{CanonLSID} & 50,150,300 & 44 & 21 \\ \hline
    \end{tabular}
}
%  \caption{Speed up for the parallel solution of the trivial problem, 16
% Threads on Dual Xeon E-2690.} 
%  \label{tab:ompdiff}
\end{minipage}
\vspace{-0.5cm}
\end{table}

\textbf{Low-light Image Enhancement}: Existing low-light image enhancement methods require paired low/well-lit image scenes in RGB space and assume the images to be captured with minimal noise. While such methods capture global information suitably, their performance in extreme low-light conditions is sub-par. The histogram equalization method is useful for increasing the dynamic range in a global context and is sub-optimal for extreme low-light enhancement \cite{chen2018image}. Retinex \cite{park2017low} methods assume that the images have sufficient information to map the reflectance and enhance the low-light image. Similarly, \cite{Wang_2019_CVPR} formulates the illumination estimation for enhancing underexposed images akin to expert retouched ground truth without image-to-image regression. Methods such as EnlightenGAN \cite{jiang2021enlightengan}, a generative model with an attention U-Net \cite{ronneberger2015u}, \cite{xu2020learning} for decomposition and enhancement and MIRNet \cite{Zamir2020MIRNet} for attention aggregation have performed well in sRGB low-light enhancement. %While there are interesting developments in the HDR domain \cite{wu2018deep, cai2018learning}, the works are different from enhancing static low-light images. While such works have shown improved enhancement, their performance and relevance to extreme low-light images with significant noise and color distortion are yet to be explored in the raw image domain. 

\textbf{Single Image Denoising}: Non-deep methods for single image denoising either use engineered features \cite{simoncelli1996noise, rudin1992nonlinear}, or assume the noise model to be uniform and additive. Such assumptions in parametric methods are unsuitable for real-world low-light enhancement. Non-parametric methods depend on sparse image priors such as smoothness and self-similarity \cite{gu2014weighted, mairal2009non, loza2013automatic, dabov2007image} and are more expressive than parametric methods. BM3D \cite{dabov2007image, plotz2017benchmarking}, a non-blind denoising method requiring noise and color model information, has outperformed deep methods in accuracy and noise robustness; however, it is prone to over smoothing in low light conditions. Several deep methods leveraging CNN based advancements have been proposed for denoising \cite{zhang2017beyond, jain2008natural, xie2012image, zhang2018ffdnet, ulyanov2018deep}. Autoencoders \cite{xie2012image, lore2017llnet} and MLPs \cite{burger2012image} have shown sub-par performance on real-world raw sensor noise. Unprocess \cite{Brooks_2019_CVPR} learns the denoising pipeline and relevant photometric parameters by `unprocessing' the image and is different from our goal to learn the camera's color and noise model effectively.
% BM3D performs non-blind denoising requiring noise and color model information and is also prone to over-smoothing in low-light. 
% Most deep learning based methods have been evaluated on synthetic random noise rather than on real-world data. Joint denoising and demosaicing \cite{gharbi2016deep} are designed for general noise models and are not well suited for noise in the raw domain.

% \textbf{Deep Learning Methods.} Several deep learning methods leveraging Convolutional Neural Network (CNN) based advancements have been proposed for denoising and image enhancement \cite{zhang2017beyond, jain2008natural, xie2012image, zhang2018ffdnet, ulyanov2018deep}. Deep methods such as autoencoders \cite{lore2017llnet}, multi-layer perceptrons \cite{burger2012image}, and inpainting based denoising autoencoders \cite{xie2012image} have also been investigated. Still, their performance on real-world raw low light image datasets have been sub-par. One interesting direction was investigated in \cite{Brooks_2019_CVPR}, where the noise model was learned from sRGB images, and a subsequently learned denoising pipeline was applied to obtain the enhanced image. While their work explored the learning of relevant photometric parameters in 'unprocessing' and then processing the image, our goal in effectively learning the camera's color and noise models is different. We propose a few-shot domain adaptation method across camera sensors (Bayer filter array) and ISO settings.

\textbf{Few-shot Domain Adaptation}: Few-shot learning \cite{sung2018learning, finn2017model, sun2019meta, prabhakar2021labeled} and Domain adaptation \cite{bousmalis2017unsupervised, rozantsev2018beyond} techniques are well explored in the context of many computer vision tasks. Several few-shot domain adaptation works \cite{sahoo2018meta,motiian2017few}  use few labeled samples with many unlabeled samples in the target domain for image classification. Similarly, DA-FSL \cite{zhao2021domain} is a few-shot domain adaptive prototypical learning method for recognition. The meta-learning paradigm \cite{casas2019few} has also shown great promise in image denoising but depends on prior noise models to partially represent real noise. To the best of our knowledge, there has been no prior investigation of few-shot DA in inverse-imaging for raw camera domains. % Few-shot learning \cite{sung2018learning, finn2017model, sun2019meta} and Domain Adaptation \cite{bousmalis2017unsupervised, rozantsev2018beyond} methods have performed sub-optimally for the joint task of adapting to a poorly represented target with domain-shift in extreme low-light conditions where the noise and distortion are severe.

As noted in \cite{Brooks_2019_CVPR}, different camera sensors exhibit different noise models, and the process of capturing short-exposure and corresponding long-exposure raw images in low-light conditions is expensive and time-consuming. While \cite{wang2020practical} has proposed efficient low-light enhancement, it is only for one type of smartphone camera. As a step toward tackling these challenges, we introduce the first of its kind few-shot domain adaptation and enhancement method for low-light conditions in the raw domain that is lightweight and highly effective.

%  One other caveat pointed out in \cite{jiang2021enlightengan} is that in working with low-light image enhancement in the raw domain, the model learns the pipeline inclusive of color transformations, demosaicing, and denoising and primarily aims to alleviate the artefacts from the long-exposure ground truth images.
