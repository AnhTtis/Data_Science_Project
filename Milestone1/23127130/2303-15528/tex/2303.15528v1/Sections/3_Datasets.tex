% \subsection{Source and target datasets} 
\label{section:fsc}
\textbf{Datasets}: We use the Sony camera dataset \cite{chen2018learning} for our source training pipeline. We expect the diverse, high-quality low-light scenes from this dataset to aid few-shot domain adaptation performance in terms of the color spaces and noise model learned by our method. 
\begin{table}[t]
\centering
\caption{Quantitative comparison of Sony as source with Nikon or Canon as target. The improvement of our method over only $k$ shot trained model is in brackets. The LSID model trained with full Nikon dataset ($k$=53) achieves 30.74dB PSNR and 0.803 SSIM and when trained with full Canon dataset ($k$=44) attains 32.32dB PSNR and 0.899 SSIM (see Table \ref{tab:baseline}).}
\label{tab:nikon}
\label{tab:canon}
\scalebox{0.735}{
\begin{tabular}{@{}lccc|ccc@{}}
\thickhline
\textbf{Nikon as target} & \multicolumn{3}{c|}{PSNR} & \multicolumn{3}{c}{SSIM} \\ \hline
$k$ ($\rightarrow$) & 1 & 2 & 4 & 1 & 2 & 4 \\ \hline
\begin{tabular}[c]{@{}l@{}}LSID\\ (only $k$ target)\end{tabular} & 23.20 $\pm$ 3.06 & 27.27 $\pm$ 0.384 & 28.05 $\pm$ 1.53 & 0.679 $\pm$ 0.172 & 0.819 $\pm$ 0.031 & 0.864 $\pm$ 0.011 \\ \hline
\begin{tabular}[c]{@{}l@{}}Proposed\\ ($k$ target + source)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{25.27} $\pm$ 0.58\\ (+2.07)\end{tabular} &
\begin{tabular}[c]{@{}c@{}}\textbf{28.06} $\pm$ 0.671\\ (+0.79)\end{tabular} &
\begin{tabular}[c]{@{}c@{}}\textbf{30.30} $\pm$ 0.52\\ (+2.25)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{0.860} $\pm$ 0.010\\ (+0.181)\end{tabular} &
\begin{tabular}[c]{@{}c@{}}\textbf{0.909} $\pm$ 0.003\\ (+0.090)\end{tabular} &
% \begin{tabular}[c]{@{}c@{}}\textbf{0.913} $\pm$ 0.006\\ (+0.049)\end{tabular} \\ \midrule
% \begin{tabular}[c]{@{}l@{}}LSID\\ (full target, $k$ = 53)\end{tabular} & \multicolumn{3}{c|}{30.74} & \multicolumn{3}{c}{0.803} \\ \bottomrule
\begin{tabular}[c]{@{}c@{}}\textbf{0.913} $\pm$ 0.006\\ (+0.049)\end{tabular} \\ \hline \hline
\textbf{Canon as target} &  &  &  &  &  &  \\ \hline
$k$ ($\rightarrow$) & 1 & 3 & 6 & 1 & 3 & 6 \\ \hline
\begin{tabular}[c]{@{}l@{}}LSID\\ (only $k$ target)\end{tabular} & 21.54 $\pm$ 2.89 & 26.9 $\pm$ 2.37 & 29.36 $\pm$ 0.763 & 0.588 $\pm$ 0.182 & 0.785 $\pm$ 0.005 & 0.829 $\pm$ 0.007 \\ \hline
% \begin{tabular}[c]{@{}l@{}}Proposed\\ ($k$ target + source)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{24.29} $\pm$ 3.16\\ (+2.75)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{28.78} $\pm$ 3.54\\ (+1.8)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{33.22} $\pm$ 0.45\\ (+3.86)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{0.623} $\pm$ 0.0074\\ (+0.035)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{0.841} $\pm$ 0.0335\\ (+0.056)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{0.896} $\pm$ 0.015\\ (+0.067)\end{tabular} \\ \midrule
% \begin{tabular}[c]{@{}l@{}}LSID\\ (full target, $k$ = 45)\end{tabular} & \multicolumn{3}{c|}{32.32} & \multicolumn{3}{c}{0.899} \\ \bottomrule
\begin{tabular}[c]{@{}l@{}}Proposed\\ ($k$ target + source)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{24.29} $\pm$ 3.16\\ (+2.75)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{28.78} $\pm$ 3.54\\ (+1.8)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{33.22} $\pm$ 0.45\\ (+3.86)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{0.623} $\pm$ 0.007\\ (+0.035)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{0.841} $\pm$ 0.033\\ (+0.056)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{0.896} $\pm$ 0.015\\ (+0.067)\end{tabular} \\ \thickhline
\end{tabular} }
\end{table}

% \begin{table}
% \begin{center}
% \begin{tabular}{|l|c|c|c|}
% \hline
% Camera & Array & \#Train & \#Test & Ratios \\
% \hline\hline
% Sony & Bayer & 161 & 37 & 100\\
% Sony & Bayer & 161 & 37 & 100, 250, 300\\
% Sony & Bayer & 161 & 37 & 100, 250, 300\\
% \hline
% \end{tabular}
% \end{center}
% \caption{Dataset Info. }
% \end{table}

% \begin{figure*}
% \begin{center}
% \includegraphics[scale=0.4]{figures/fuji_nikon.png}
% \includegraphics[scale=0.4]{figures/fuji_nikon.png}
% \end{center}
%   \caption{Proposed few-shot domain adaptation model architecture.}
% \label{fig:model}
% \end{figure*}

%---------------------------------------------------------------------------------------------------------------
% \begin{figure}[t]
% \begin{center}
% \includegraphics[width=0.7\linewidth]{3_BMVC/Images/nikon.png}
% \end{center}
%   \caption{Examples of short-exposure and long-exposure image pairs from the Nikon dataset. Note that the short exposure images are almost entirely dark whereas the long-exposure images possess immense scene-related information.}
% \label{fig:nikon}
% \end{figure}

For few-shot domain adaptation, we work with very few (< 10) target camera images in every training experiment. We use the open-source Canon camera low-light raw image dataset \cite{CanonLSID} and a new Nikon camera dataset that we have compiled and make available with this work for our target camera training pipeline. We do not investigate burst denoising or the `lucky imaging' phenomenon. Hence, we only take the first short-exposure raw image for each scene from the Sony dataset and use the 161 images for our source camera training pipeline. Note that the Canon dataset has eight different ratios with close ranges such that they can be put into three buckets of ratios: 50, 150, and 300 (Table \ref{tab:datasets}). % We also show qualitative results for low-cost smartphone camera sensors - Google Pixel and OnePlus 5. 
%, to evaluate our approach on severe noise from low-cost smartphone camera sensors. %We also modify the Canon camera dataset to increase the number of training images by transferring a few validation images to the train set. This allows our baselines to learn the camera parameters in a better manner while we stick to few-shot training. 

% We also experiment with raw low-light images taken with smartphone cameras - Google Pixel and OnePlus 5, to investigate the performance of our proposed approach on raw images captured with low-cost smartphone camera sensors, which typically have higher noise severity in low-light conditions.

\textbf{Nikon camera dataset}:
\label{section:nikon}
We have compiled a dataset of raw low-light images captured with a Nikon D5600 camera to train the proposed few-shot domain adaptation architecture. The Nikon dataset consists of short-exposure images captured at $\nicefrac{1}{3}$ or $\nicefrac{1}{10}$ seconds and corresponding ground-truth long-exposure images captured at 10 or 30 seconds in the NEF format. For uniformity, there are two short-exposure images for every long-exposure image such that the exposure ratio (ratio of exposure time between the ground-truth long-exposure image and the input short-exposure image) is 100 and 300, respectively. Similar to \cite{chen2018learning}, we mount the camera on sturdy tripods and use appropriate camera settings to capture the static scenes using a smartphone app. The images captured include 129 short-exposure and 65 long-exposure ground-truth images of indoor and outdoor low-light scenes (sub lux). %Our aim in compiling the Nikon camera raw image dataset is to capture realistic low-light scenes and investigate few-shot domain adaptation with a dataset containing few images. %As discussed in section \ref{sec:related}, several related works compare results with datasets containing as few as only up to 10 images, and we seek to replicate these experimental setups. 

\textbf{Training Setup:} \label{sec:train}
% \textbf{Training $\mathcal{D}$}: The 16-to-8 bit converter used for the source pipeline is a fully-convolutional U-net \cite{ronneberger2015u} network architecture, trained with the ground-truth image pairs of 16-bit and 8-bit representations. We use the $L_{2}$ loss for training the converter and train it for 4000 epochs. We use the trained 16-to-8-bit converter for our model training.
We train the source and target pipelines simultaneously in an end-to-end manner. As discussed in section \ref{sec:pipeline}, we use the respective short-exposure raw images as the input to each of the encoders. We first randomly crop a $512\times512$ image patch and augment it with random-flip and random-rotate. We then subtract the black level and multiply the input raw image with the exposure ratio. We use an initial learning rate of $10^{-4}$ up to 2000 epochs and then reduce it by a factor of 10 for every 1000 epochs thereafter. We use the Adam \cite{kingma2014adam} optimizer for the 8-bit SSIM loss and Cosine Similarity loss ($\mathcal{L}_{source}$) for the source pipeline, and the $\mathcal{L}_{target}$ loss for the target pipeline.

We train the model for 4000 epochs (same as \cite{chen2018learning}) but observe the loss saturating at lower epochs prompting us to employ early stopping. Since the large source domain has 161 images, every epoch has 161 train steps. As we jointly train the source and target pipelines, for every epoch, we use 161 randomly cropped source patches obtained from the 161 source domain raw images and 161 randomly cropped target patches from only $k$-images in the target domain. We find that training our proposed method for up to 2500 epochs is sufficient to obtain the best results and reproduce the results in this paper for few-shot domain adaptation.

The source SSIM loss is calculated in the 8-bit space after passing the output from the shared $\mathbb{N}$ through the 16-to-8-bit converter. The cosine similarity loss for the source domain and $L_{1}$ loss for the target domain are computed in the 16-bit sRGB space. The exposure ratio is computed and provided to the network. At inference time, we use the full-scale raw target image as input to the target camera pipeline and obtain the enhanced target sRGB image. % in the 16-bit sRGB space.
% \begin{table}[t]
%     \centering
%     \caption{Details of the datasets used in our work.}
%     \label{tab:datasets}
%     \scalebox{0.85}{
%     \begin{tabular}{lccc}
%     \hline
%     \multirow{2}{*}{Datasets} & Exposure & Training & Validation \\
%     & Ratios & Images & Images \\
%     \hline
%     Sony \cite{chen2018learning} & 90,15,300 & 161 & 36\\
%     Nikon & 100,300 & 53 & 24\\
%     Canon \cite{CanonLSID} & 50,150,300 & 44 & 21 \\ \hline
%     \end{tabular}
%     }
%     \vspace{-2.5mm}
% \end{table}

% The images in our proposed dataset was captured with ISO 100-200. Other datasets were captured with ISO between 50-200. The range of ISO does not affect the model's performance due to the wide range of amplification factors used to train the model. A single encoder is sufficient to handle all ISO values.

% \begin{table}[t]
% \centering
% \begin{tabular}{lccc}
% \hline
% Datasets & \begin{tabular}[c]{c}Exposure\\ Ratios\end{tabular} & \begin{tabular}[c]{c}Training\\ images\end{tabular} & \begin{tabular}[c]{c}Validation\\ images\end{tabular} \\ \hline
% Sony & 90,150,300 & 161 &  36\\ %\cite{chen2018learning}
% Nikon &  100, 300 & 53 &  24 \\
% Canon & 50, 150, 300 & 44 &  21\\ \hline %\cite{CanonLSID}
% \end{tabular}
% \caption{Details of the datasets used in our work.}
% \label{tab:datasets}
% \end{table}
