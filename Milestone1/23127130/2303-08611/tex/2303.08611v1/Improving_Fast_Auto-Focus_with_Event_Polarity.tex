\documentclass{optica-article}
\pdfoutput=1
\journal{opticajournal} % for journals or Optica Open

\articletype{Research Article}

\usepackage{lineno}
\usepackage{subcaption}
% \usepackage[labelformat=simple]{subcaption}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{multirow,booktabs}
\usepackage{makecell}

\renewcommand\thesubfigure{(\alph{subfigure})}

%%%% packages added by Lei
\newcommand{\Lei}[1]{\textcolor{red}{\textbf{[Lei:~}\textit{#1}\textbf{]}}}
\newcommand{\MYQ}[1]{\textcolor{blue}{\textbf{[MYQ:~}\textit{#1}\textbf{]}}}
\newcommand{\GDY}[1]{\textcolor{brow}{\textbf{[GDY:~}\textit{#1}\textbf{]}}}
\newcommand{\BYH}[1]{\textcolor{orange}{\textbf{[BYH:~}\textit{#1}\textbf{]}}}
\usepackage{soul}
% \pagestyle{plain} % TODO: 提交论文的时候注释掉！页码
\usepackage{siunitx}


% \linenumbers % Turn off line numbering for Optica Open preprint submissions.


\begin{document}

\title{Improving Fast Auto-Focus with Event Polarity}

\author{Yuhan Bao,\authormark{1,2} Lei Sun,\authormark{1,2} Yuqin Ma,\authormark{1,2} Diyang Gu,\authormark{1,2} and Kaiwei Wang\authormark{1,2,*}}

\address{\authormark{1}State Key Laboratory of Modern Optical Instrumentation, Zhejiang University, Hangzhou 310027, China\\
\authormark{2}National Engineering Research Center of Optical Instrumentation, Zhejiang University, Hangzhou 310027, China\\}


\email{\authormark{*}wangkaiwei@zju.edu.cn} %% email address is required


\begin{abstract*} 
Fast and accurate auto-focus in adverse conditions remains an arduous task. The paper presents a polarity-based event camera auto-focus algorithm featuring high-speed, precise auto-focus in dark, dynamic scenes that conventional frame-based cameras cannot match. 
Specifically, the symmetrical relationship between the event polarities in focusing is investigated, and the event-based focus evaluation function is proposed based on the principles of the event cameras and the imaging model in the focusing process.
Comprehensive experiments on the public EAD dataset show the robustness of the model. Furthermore, precise focus with less than one depth of focus is achieved within 0.004 seconds on our self-built high-speed focusing platform. The dataset and code will be made publicly available. 
\end{abstract*}

%%%%%%%%%%%%%%%%%%%%%%%%%%  body  %%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Focusing is an indispensable part of any camera. Conventional frame-based camera auto-focus (AF) relies on sharpness analysis of images within a certain exposure time to get the optimal focus position, which makes it difficult to complete focusing in extremely dark and high-speed motion scenes. In recent years, the event camera or Dynamic Vision Sensor (DVS) shows superior performance in temporal resolution (>500Hz) and dynamic resolution (>130dB). Its unique asynchronous event output mechanism allows for a very high response time (order of \si{\micro\second}), meanwhile greatly reducing the bandwidth requirements (compared to the image matrix output of frame-based cameras). Therefore, it has been widely used in object tracking~\cite{RN43, RN44}, Simultaneous Localization and Mapping (SLAM)~\cite{RN46}, visual scene image reconstruction~\cite{RN47}, image motion deblurring~\cite{sun2022event,sun2023event}, human pose estimation~\cite{chen2022efficient}, and many other visual applications.
% Event cameras can be useful in scenarios where conventional frame-based cameras are not up to the task. 
Likewise, all the advantages of event cameras show their potential in fast AF under adverse conditions like dark, fast-moving, and high dynamic range scenes.
However, event camera AF remains an issue for the reason that the criteria for determining whether an event camera is in focus based on its output information have not been well investigated.

The unique asynchronous output of event cameras makes it impossible to rely on the contrast~\cite{RN48}, statistics~\cite{RN50}, or frequency~\cite{RN49}-based algorithms of traditional frame-based cameras for focusing. Recently, event cameras with integrated conventional frame-based cameras (Active Pixel Sensor, APS) come out, such as the DAVIS~\cite{RN7, RN8} series from IniVation, which means users can determine whether the event camera is in focus or not by the synchronized frame output. However, the limited spatial resolution and relatively high-noise characteristics of the APS, make auto-focus challenging. In addition, such a focusing method does not take advantage of the high temporal resolution characteristic of event cameras, making the focusing process as slow as with traditional frame-based cameras. Another event camera focusing idea is to use the output of the event by the event camera over a period of time to accumulate into event frames and analyze their contrast, which is a recommended way for DAVIS focusing~\cite{RN52}. However, this is quite time-consuming and requires a specific focus board (e.g., Siemens Star) to complete a focus session, resulting in restricted application scenarios. Recently, some event-rate-based methods have provided new ideas for event camera AF~\cite{RN53}. Previous event-based AF algorithms obtain the focus position by finding the position with the highest event rate (ER) in the whole focus sequence, which sometimes leads to focus errors of 5 to 6 depths of focus. Such an error comes mainly from three reasons: (1) positive and negative event polarity rate peaks separation, as shown in Fig.~\ref{fig:Intro} (b), (2) the imbalance of positive and negative event polarity ratio, and (3) the strong coupling relationship between ER and scene optical flow. Such a large focusing error is unacceptable for vision tasks, so a faster and more accurate event camera AF algorithm needs to be developed.

During the focusing process, we find that the brightness changes from blurry to sharp and from sharp to blurry are opposite for the same feature point. Since event cameras can respond to changes in brightness in the scene (darkening triggers negative events and brightening triggers positive events), the polarity of the events generated by the same feature should be \textbf{opposite} before and after the focus position, as shown in Fig.~\ref{fig:Intro} (a). Based on such a \textbf{polarity symmetry} phenomenon, we stack the events of different polarities into two event polarity rate (EPR) sequences and construct a focusing evaluation function to evaluate the symmetry of the sequences. Fig.~\ref{fig:Intro} (b) shows the focus result of (a) given by our proposed "Polarity-based auto-focus" (\texttt{PBF}) method, leveraging the symmetry of EPR sequences. Fig.~\ref{fig:Intro} (c) shows the focus result given by the state-of-art algorithm "event-based golden search" (EGS)~\cite{RN53}, leveraging the highest ER.
% \begin{figure}[ht!]
% \centering\includegraphics[width=13cm]{Figure/fig1.png}
% \caption{Sample caption (Fig. 2,}\label{img1}
% \end{figure}
\begin{figure}[htbp]
    \centering
    \vspace{-5pt}
    \includegraphics[width=11cm]{Intro.pdf}
    \vspace{-7pt}
    \caption{Focus result comparision of proposed PBF method and EGS method in a given scenary. (a) Event polarity cumulative images of blurry to sharp (left) and sharp to blurry (right) and the corresponding frame image (middle). Green: negative events, red: positive events. (b): The focus result of (a) using our proposed \texttt{PBF} method. Focus error: 36 \si{\micro\meter} (fewer than 1 depth of focus). (c): The focus result of (a) using EGS algorithm~\cite{RN53}. Focus error: -158 \si{\micro\meter} (equal to 3 depths of focus).}
    % \vspace{-7pt}
    \label{fig:Intro}
\end{figure}

In summary, we primarily offer the four contributions listed below:
\begin{itemize}
    \item From the imaging principle of the focusing process and the mechanism of event camera signal generation, we elucidate the properties of the EPR variations in the focusing process.
    \item We derived a universal event camera auto-focusing algorithm, the \textbf{P}olarity-\textbf{B}ased Auto-\textbf{F}ocus (\texttt{PBF}), from the correlation between focus position and EPR. This algorithm can be applied to most event camera devices. And the codes are available in Code 1 (Ref.~\cite{PBFcode}).
    
    \item Extensive experiments are conducted on the EAD event camera dataset, as we show in Dataset 1 (Ref.~\cite{EADnpy}), which includes challenging scenarios such as low light, rapid motion, and high-speed changes. The proposed \texttt{PBF} outperforms the state-of-the-art "event-based golden search" (EGS) algorithm by \textbf{17.5\%} in RMSE accuracy and \textbf{185 times faster} on average in processing speed.

    
    \item We assembled an event camera high-speed focusing (EvHF) dataset featuring scenes with varying lighting conditions and motion states, as shown in Dataset 2 (Ref.~\cite{highspeeddataset}), on which our proposed \texttt{PBF} achieved a focusing error of less than 1 depth of focus with a processing time of fewer than \textbf{0.004 s}, which outperforms EGS by \textbf{5 times} in MAE accuracy and \textbf{36 times} in average speed.
    
\end{itemize}
The rest of the article is organized as follows. Section~\ref{section:2} presents related works, including the state-of-art algorithm EGS. In Section~\ref{section:3}, we analyze in detail the sources of event camera signals during the focusing process and the intrinsic connection between the focus position and EPR. Then, we introduce the processing of the EPR sequences and  present our \texttt{PBF} algorithm. Section~\ref{section:4} shows the results of our proposed algorithm on the simulation dataset, the public dataset EAD and our high-speed focusing dataset. Finally, the conclusion and discussion of this paper are provided in the rest sections.


\section{Related works}
\label{section:2}
\subsection{Conventional AF methods}
Conventional AF methods can be broadly divided into those based on distance measurement and imaging information. Early AF technology mainly used the principle of distance measurement. By measuring the distance from the target to the camera lens and combining it with the known lens focal length information, the corresponding image length can be obtained, so as to adjust the imaging system to reach the on-focus state. However, distance measurement requires additional system components to emit and receive a beam of light or sounds, which increases the cost of AF systems, so most current AF systems are based on the signals (contrast, phase) received when focusing. The focus evaluation function is used to calculate the image sharpness quality for a series of images in the focusing process, and then combined with the search algorithm to find the focus position or direction, and control the motor to drive the lens to move until the image is sharp~\cite{RN1}. However, dynamic and extremely dark scenes pose a great challenge to the above signal-based methods because of the need for a certain exposure time to obtain imaging signals.
\subsection{Event-based AF}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=12cm]{related_works.pdf}
    \vspace{-5pt}
    \caption{(a) and (b) demonstrate the focus result using EGS algorithm~\cite{RN53}. Green dashed line: the focus timestamp predicted by the EGS. Black dashed line: ground truth. (a): "chinese light static" in EAD dataset, which shows an imbalance in EPR sequences. (b): "cactus dark shake" in EAD dataset, which shows the impact of violent motion.}
    \label{fig:related works}
\end{figure}


In the past, the approach to AF based on event information has often been to integrate events in time and analyze event accumulated frames, which is quite similar to conventional frame-based AF ways. Recently, S. Lin et al.~\cite{RN53} found the relationship between ER and focus evaluation function. According to event cameras’ working principle, event cameras are inherently more responsive to edges in the scene with high contrast or image gradients while the traditional frame-based AF methods also leverage the high contrast properties of images in focus. For the entire focusing process, the contrast on the image plane is greatest near the focus position, and thus the event camera outputs the most events as well. Based on the above ideas, they proposed using ER as an event-based focus measure. However, calculating the ER requires the selection of a certain time interval length $\Delta t$. Due to the presence of event camera noise, picking different $\Delta t$ brings very different results for the maximum position of the ER. Therefore, they used a dichotomous lookup algorithm called Event-based Golden Search (EGS) to find the time point with the highest ER during the whole focusing process. This goes some way to reducing the event noise inherent in event cameras and the additional events from scene motion. Using the algorithm, they conducted experiments in a variety of scenarios where it was difficult to accomplish AF with conventional frame-based cameras, such as low illumination, camera motion, and scene motion, all of which have yielded relatively good results. However, their algorithm does not meet the real-time requirements in many cases. Even though their algorithms are of O($N_e$) algorithmic complexity, they require several seconds of processing time for a dataset with millions of event points due to a series of limitations such as caching. 
%Such processing speed is difficult to accept if a shallow depth-of-field lens is used and the object distance is rapidly changing. 

In addition, because of ignoring event polarity information, the algorithm tends to find the peak of a specific event polarity when the number of event polarities is significantly out of balance and the peaks of the EPR sequences are separated by a large distance, such as “chinese light static” in EAD dataset, as shown in Fig.~\ref{fig:related works} (a). The focus position found by the EGS algorithm based on the overall maximum ER has a large error with the ground truth position, (-191.0 \si{\micro\meter}, approximately 5 depths of focus). Besides, when in a dynamic state (e.g., camera motion), the EGS algorithm sometimes finds the timestamp with the largest motion amplitude instead of that with the greatest image sharpness. As shown in Fig.~\ref{fig:related works} (b), the error is -236 \si{\micro\meter}, approximately 6 depths of focus. 

According to Lin's derivation, the ER is proportional to the product of the image gradient and the image optical flow. The image gradient $\nabla I(\boldsymbol{x},t)$ is the term closely related to the degree of focus, and using the ER as the focus evaluation function requires the optical flow term $\boldsymbol{v}(\boldsymbol{x},t)$ to be constant, which is usually difficult to achieve.
\begin{equation}
    \label{eq1}
    ER(\boldsymbol{x},t,\Delta t) =\frac{\nabla I(\boldsymbol{x},t)}{C}\cdot \boldsymbol{v}(\boldsymbol{x},t)
\end{equation}

The work of Z. Ge~\cite{RN55} et al. is closest to our idea, which applies the event camera in AF for small depth-of-focus microscopes. Interestingly, they found that events were mainly triggered symmetrically at both sides near the focal plane, the event polarities were different between the two sides and events were almost \textbf{undetectable} at the focus position. This appears to be the exact opposite of the EGS algorithm idea introduced above, which regards the position with the highest ER as the focus position. In reality, however, such opposing views are both reasonable, given the differences in their experimental conditions. Ge’s work focuses on the static scene without optical flow, while Lin's work is focused on more general situations, such as handheld shooting and scenes in constant optical flow.

Both of their schemes get relatively good results in constrained scenes, while a universal event camera AF algorithm does not want to be constrained by any scene limitations (e.g., stationary or uniform motion). In the next section, the principle of event generation during focusing is elaborated in detail to give a universal event camera AF model.


\section{Proposed method}
\label{section:3}
\subsection{Principle of event camera}
\label{subsection:31}

Event cameras respond to brightness changes and output events in an asynchronous manner. For each pixel, once the log of intensity changes above a set threshold $C$, it emits an event signal and assigns positive or negative polarity $p$.

\begin{equation}
\label{eq2}
p{(x,y,t_i)}=\left\{
\begin{aligned}
+1 & , & L(x,y,t_i)-L(x,y,t_{i-1})>C, \\
-1 & , & L(x,y,t_i)-L(x,y,t_{i-1})<-C,
\end{aligned}
% , L(x,y,t)=\log(I(x,y,t)).
\right.
\end{equation}
 where $t_i$ indicates the current timestamp, while $t_{i-1}$ indicates the last timestamp when an event is triggered.
As shown in Fig.~{\ref{fig: Priciple}} (a), the projection of a rigid edge moves from left to right, and the pixels near the edge trigger events of different polarities (green for negative, red for positive). (b) shows the brightness change of the pixel during the movement. At this time point, the brightness is raised above the threshold due to the white checkerboard on the left side passing by the pixel marked with a yellow box, triggering a series of positive red events; after some time, the next edge passes by the pixel and the brightness decreases, triggering a number of green negative events. 


\begin{figure}[ht!]
\vspace{-5pt}
\centering\includegraphics[width=9cm]{Event_camera_principle_and_optical_principle.pdf}
\vspace{-7pt}
\caption{Event camera principle and optical principle. (a) and (b) illustrate the working principle of the event camera. The checkerboard grid in (a) moves from left to right, triggering events on a pixel (yellow square) at the edge. (b) shows the brightness change of the pixel and the corresponding event sequence. (c) shows the ideal imaging model of a thin lens in an event camera. DVS and APS sensors share the same sensor plane.}
\vspace{-8pt}
\label{fig: Priciple}
\end{figure}


Due to device manufacturing issues, the actual threshold value for each pixel triggering event is not necessarily the set value $C$. Even the thresholds for a specific pixel triggering positive and negative events are not always the same, thus resulting in the polarity imbalance phenomenon shown in Fig.~\ref{fig:related works} (a). 
% Such an  inconsistent threshold can be described by the following correction equation, where $c(x,y)$ is the pixel-wise threshold value, and $b(x,y)$ is the pixel-wise bias value~\cite{RN56}.
% \begin{equation}
% \label{eq3}
% p{(x,y,t)}=\left\{
% \begin{aligned}
% +1 & , & L(x,y,t_i)-L(x,y,t_{i-1})>c(x,y)+b(x,y), \\
% -1 & , & L(x,y,t_i)-L(x,y,t_{i-1})<-c(x,y)+b(x,y).
% \end{aligned}
% % , L(x,y,t)=\log(I(x,y,t)).
% \right.
% \end{equation}

Differences in positive and negative event thresholds often pose significant problems in the task of visual reconstruction of event information~\cite{RN57}. In fact, polarity threshold compensation should be considered in any application that considers event polarity. In the post-processing of event data, the problem of different positive and negative thresholds can be compensated by normalizing the sequence of positive and negative event rates. 

In the next subsection, the sources of events generated by the event camera during focusing are described.
\subsection{Source of events during focusing}
\label{subsection:32}
In general, events arise from changes in the brightness of the sensor plane. More precisely, in the scene of AF, the brightness change comes from 4 sources: (1) changes in the convolutional blur kernel during focusing, (2) focus breathing, (3) optical flow caused by motion, and (4) unexpected noise due to devices. In the following four subsections, each of these four sources is elaborated on.
\subsubsection{Blur kernel changing during focusing}
\label{subsubsection:321}
Fig.~\ref{fig: Priciple} (c) shows the ideal imaging
model of a thin lens and the ideal imaging equation is as follows:
\begin{equation}
\label{eq4}
\frac{1}{u}+\frac{1}{v}=\frac{1}{f},
\end{equation}
where $u$ represents the object distance, $f$ represents the focal length of the thin lens, and $v$ represents the ideal image distance.

The event camera and frame-based sensors share the same sensor plane, so the event camera's focus judgment is consistent with the frame-based camera. Due to the gap between the sensor plane and the image plane, the ideal image point on the image plane is blurry on the sensor plane, and the radius of the dispersion circle is $r$. According to the simple geometric similarity relationship, the radius of the dispersion circle $r$ can be derived:
\begin{equation}
\label{eq5}
r={\frac{\left| \Delta v \right|}{2v}}D,
\end{equation}
where $\Delta v$ represents the defocus amount and $D$ represents the exit pupil of the lens.

In the case of incoherent light illumination, the point spread function of a circular aperture is the square of a first-order Bessel function. Due to the complexity of the Bessel function, a two-dimensional Gaussian function is often used for approximation in practical applications:
\begin{equation}
\label{eq6}
h(x,y,a)=\frac{1}{2\pi a^2}\exp{({-\frac{x^2+y^2}{2a^2}})},a=kr,
\end{equation}
where $k$ is a constant scaling factor related to the pixel size.


\begin{figure}[ht!]
 \vspace{-5pt}
\centering\includegraphics[width=10cm]{Change_of_log_light_intensity.pdf}
 \vspace{-5pt}
\caption{Change of log light intensity near the edge during focusing and triggered events. (a): The edge feature changes during focusing. (b): The change of relative log intensity of the investigated pixel row, where the red and blue dots indicate the positive and negative events triggered in the process. (c): The relative log intensity change of the pixel closest to the edge, and the corresponding event.
 }\label{fig: blur kernel changing}
 \vspace{-8pt}
\end{figure}

Fig.~\ref{fig: blur kernel changing} (a) shows a sharp edge changing during focusing. (b) shows the change in brightness and the triggered events of the investigated pixel row when $\Delta v$ changes from -30 to +30. (c) shows the relative brightness change of the pixel closest to the edge. In the ideal case, the polarity of events triggered by a single pixel before and after the focus position exhibits perfect symmetry.

When considering a large region of interest (ROI), the effect of log intensity changes of all pixels within the ROI needs to be accumulated. From the derivation of Eq.~(\ref{eq5}) and Eq.~(\ref{eq6}), the convolution kernel size $a$ is proportional to the absolute value of the defocus amount $\Delta v$. The ideal image distance is $v_0$, then the defocus amount $\Delta v= v-v_0$. The positive event rate is denoted as PER and the negative event rate as NER. Then Eqs.~(\ref{7a}), (\ref{7b}) and (\ref{7c}) are derived.

\begin{subequations}
\label{eq7}
\begin{align}
PER(v)=\sum_{\frac{\partial}{\partial v}L(x_i,y_i,v)>0,(x_i,y_i)\in ROI}\frac{\partial}{\partial v}L(x_i,y_i,v), \label{7a}\\
NER(v)=\sum_{\frac{\partial}{\partial v}L(x_i,y_i,v)<0,(x_i,y_i)\in ROI}\frac{\partial}{\partial v}L(x_i,y_i,v),  \label{7b} \\
L(x_i,y_i,v)=\log(I\otimes h(x_i,y_i,\left| v-v_0\right|). \label{7c}
\end{align}
\end{subequations}
According to Eq.~(\ref{7c}), the log intensity values $L(x_i,y_i,v)$ of a given pixel are equal when $v$ is symmetric about $v_0$. After accumulating the intensity changes for all pixels, the PER sequence and the NER sequence should also be symmetrically distributed about $v_0$. The simulated example in Fig.~\ref{fig:branches example} provides a more intuitive view of the symmetry relationship of the EPR, where (a) shows the process of focusing on a branch, and the red box indicates the ROI. Since the differential is replaced with a difference in the $v$-direction here, and the threshold is not infinitesimal, the rounding error leads to an imperfect symmetric curve in Fig.~\ref{fig:branches example} (b). 

\begin{figure}[ht!]
\centering\includegraphics[width=11cm]{Branches.pdf}
\caption{EPR changes of branch silhouettes in focusing. (a): The frame image of branch silhouettes during focusing. (b) The curve of EPR in the red box (ROI) during focusing, which shows symmetry.
 }\label{fig:branches example}
 \end{figure}

 To summarize this subsection, (1) the image change during the focusing process can be described by the change in the size of the Gaussian blur kernel. (2) The size of the Gaussian blur kernel is proportional to the defocus amount $\Delta v$. (3) The EPR of a pixel is proportional to the log intensity change rate of that pixel. And (4) the log intensity changing is symmetrically about the focus position $v_0$, and subsequently, the PER and NER curve should be symmetrically about $v_0$.

\subsubsection{Focus breathing}
The field of view (FOV) of a lens slightly changes during focusing~\cite{RN61}, which is called focus breathing. It introduces additional optical flow, which points to the center of the image due to the axial symmetry of the optical system. Such changes are nearly linearly related to the defocus amount~\cite{Goodsell:22}. Consequently, when the image plane moves at a uniform speed, the resulting optical flow is also constant. According to Eq.~(\ref{eq1}), the events caused by the breathing effect peak when the image gradient is at its maximum, i.e., on focus. In fact, events generated by the breathing effect are necessary for the EGS algorithm in static scenes because of the necessary constant optical flow $\boldsymbol{v}(\boldsymbol{x})$.

For our method, the events brought about by this constant optical flow do not affect the symmetry of the PER and NER, because these events remain symmetric about the focus position, referring to Eq.~(\ref{eq1}) and Eq.~(\ref{eq7}). 

Actually, the main problem caused by the breathing effect is that the same feature is located at different pixels at different image distances. When a larger ROI is considered for focusing, since we accumulate the entire ROI-generating events as a whole as illustrated in Eq.~(\ref{eq7}), the feature movement due to the breathing effect does not interfere with the focusing task. The breathing effect only needs to be considered when reaching a pixel-level focus judgment. 

\subsubsection{Optical flow caused by motion}
\label{subsubsection:323}
The events caused by dynamic scenes and camera motion are the biggest interference in event camera AF. In a dynamic scene (such as "cactus dark shake" in the EAD dataset shown in Fig.~\ref{fig:related works} (b)), the ER changes dramatically due to the drastic round trip camera movement, which means that the EGS algorithm is not robust to these irregular jitters, since Eq.~(\ref{eq1}) requires a constant optical flow $\boldsymbol{v}(\boldsymbol{x})$.
Eq.~(\ref{eq1}) and Eq.~(\ref{eq7}) show the two main sources of events in the focusing process: optical flow due to motion in the scene (from both camera and object), and the brightness changes due to focusing. The change in the defocus amount $\Delta v$ can also be considered as a motion as well as the optical flow. The optical flow is perpendicular to the optical axis while the focusing is along the optical axis. 

In the real scene, when the focusing motion is fast enough, such as with the ultrasonic motors widely used in digital cameras, the events generated by optical flow are much fewer than those generated by focusing, which means Eq.~(\ref{eq7}) better describes the output characteristics of events in high-speed focusing scenarios. In fact, the application of event cameras in AF is precisely to pursue ultra-fast focusing, where the optical flow motion can be seen as stationary during the focusing done in a very short time (e.g., several milliseconds). Therefore, the events brought by the optical flow are considered as noise in our method. This event noise constitutes a random high-frequency part of the overall EPR sequence, and it can be filtered out by certain filtering methods.



\subsubsection{Noise}
\label{subsubsection:324}
On event cameras with integrated APS, bursts of DVS events can be caused by the capture of an APS frame. These event burst spikes are conspicuous in Fig.~\ref{fig:related works} (a). In addition, in extremely dark environments, the event camera exhibits significant dark noise.
Due to the noise, it's impossible to just use the ER algorithm in ~\cite{RN53} or the "M-shaped valley" algorithm in ~\cite{RN55}. 

Different types of noise have different characteristics in the time and frequency domains. The dark noise is prevalent throughout the EPR sequence and has a relatively fixed high-frequency spectral component; the event surge brought about by the APS frame output is related to the APS frame rate, generally 40Hz to 50Hz; the event noise brought about by scene motion or irregular jitter varies in both the time and frequency domains.


\subsection{Polarity-based auto-focus algorithm}
\label{subsction:33}
\subsubsection{EPR sequence preprocess}
\label{subsubsction:331}

Referring to Eq.~(\ref{eq7}), since the image distance $v$ can be regarded as varying uniformly during the focusing process, the events generated by the focusing motion occupy the low-frequency part of the PER and NER sequences. And through the discussion in Section~\ref{subsubsection:324}, the noise is usually distributed in the high-frequency part of the sequence, and its spectrum is changing in the time domain.

Compared with the traditional Fourier transform, the wavelet transform combines the characteristics of frequency domain and time domain analysis and fits scenarios where the signal spectrum varies in the time domain. Due to the complex time-frequency domain properties of EPR sequences, wavelet transform is more suitable for the EPR sequence preprocess. 


\begin{figure}[ht!]
\vspace{-5pt}
\centering\includegraphics[width=12cm]{Wavelet_filtered_event_polarity_rate.pdf}
\caption{Wavelet filtered EPR. (a) EPR of crossroad dark static in EAD dataset, with a lot of noise. (c) EPR after wavelet filtering, showing symmetry. (b) and (d) indicate the scaling function $\phi$ and wavelet function $\psi$ of used wavelet ‘dmey’.}\label{fig: Wavelet result}
\vspace{-5pt}
\end{figure}


The event data in EAD dataset contains a non-negligible amount of the noise described in Sections \ref{subsubsection:323} and \ref{subsubsection:324} because of the slow focusing speed (1$mm$/$s$). As Fig.~\ref{fig: Wavelet result} (a) shows, the curve exhibits violent oscillations because of the noise mentioned in Section~\ref{subsubsection:324}: the dark noise makes up the highest frequency component; the APS noise is the 40Hz fixed interval spikes; and a blue spike noise brought by the scene motion appears at 3.6 s.
We use the "dmey" wavelet shown in Fig.~\ref{fig: Wavelet result} (b) and (d) to perform 6-layer wavelet noise reduction on the raw sequence, removing all the high-frequency components and retaining only the last low-frequency component "cA6". The filtered sequence is shown in Fig.~\ref{fig: Wavelet result} (c). It can be seen that the low-frequency components of the PER and NER sequences show symmetry consistent with the theoretical analysis in Section \ref{subsubsection:321}.


\subsubsection{Focus evaluation function}
\label{subsubsection:332}
After applying wavelet filtering, the EPR sequence has been effectively purged of noise interference. However, the task of determining the focus position remains. As expounded upon in Section \ref{subsubsection:321}, the EPR distribution is symmetric with respect to the focus position. In order to quantitatively characterize this symmetry and ascertain the focus timestamp of the sequence, a mean square error (MSE) evaluation function similar to the convolution operation is constructed.

Given a denoised PER sequence $P(v)$ and a denoised NER sequence $N(v), v\in(v_1,v_2)$. Firstly, the domain of definition of $v$ is shifted to $(0,v_2-v_1)$,
\begin{subequations}
\label{eq8}
\begin{align}
P'(v)=P(v+v_1),\label{8a}\\
N'(v)=N(v+v_1).\label{8b} 
\end{align}
\end{subequations}
The mean square error (MSE) function is defined as:
\begin{equation}
\label{eq9}
MSE(a)=\left\{
\begin{aligned}
\frac{1}{a}\int_{0}^{a}(N'(v)-P'(a-v))^2dv&,&0<a\leq v_2-v_1, \\
\frac{1}{2(v_2-v_1)-a}\int_{a-v_2+v_1}^{v_2-v_1}(N'(v)-P'(a-v))^2dv&,&v_2-v_1<a<2(v_2-v_1).
\end{aligned}
% , L(x,y,t)=\log(I(x,y,t)).
\right.
\end{equation}
The argument of minimal MSE function is related to the focus position:
\begin{equation}
\label{eq10}
    v^*=\frac{1}{2}\mathop{\arg\min}\limits_{a}(MSE(a))+v_1.
\end{equation}
The \texttt{PBF} MSE function can be decomposed into three distinct steps: (1) reversing the sequence of one polarity, (2) aligning the reversed sequence with the other sequence of different lengths, and (3) computing the MSE between the overlapping regions of the two sequences. The MSE function reaches its minimum value when the reversed sequence is optimally aligned with the other sequence, at which point the parameter $a$ of the MSE function is equal to twice the center of symmetry of the original sequences.


To ensure the convolution MSE function accurately reflects the entire sequence, a hyperparameter investigation factor, $k$, is introduced when the overlap is too small in the \texttt{PBF} algorithm. Additionally, to address the imbalance between positive and negative event amounts in Fig.\ref{fig:related works} (a), the PER and NER sequences are normalized. By discretizing Eqs. (\ref{eq8}), (\ref{eq9}), and (\ref{eq10}), and accounting for the instability caused by short overlap lengths, 
we derived the final \texttt{PBF} algorithm.


\begin{algorithm}
\DontPrintSemicolon
\KwData{positive event rate sequence $P[v]$, negative event rate sequence $N[v]$,sequence definition domain $v\in(v_1,v_2)$, MSE investigation factor $k$.}
\KwResult{Optimal focus position $v^*$.}
\Begin{
$P[v] \longleftarrow wavelet denoise(P[v])$\;
$N[v] \longleftarrow wavelet denoise(N[v])$\;
$P[v] \longleftarrow normalize(P[v])$\;
$N[v] \longleftarrow normalize(N[v])$\;
$P'[v] \longleftarrow P[v+v_1]$\;
$N'[v] \longleftarrow N[v+v_1]$\;
\For{$a=k(v_2-v_1)$ $\to$ $(2-k)(v_2-v_1)$ }{
\If{$a<v_2-v_1$}{
$MSE[a]=\frac{1}{a}\sum_{1}^{a}(N'[v]-P'[a-v])^2$\;
}
\Else{
$MSE[a]=\frac{1}{2(v_2-v_1)-a}\sum_{a-(v_2-v_1)+1}^{v_2-v_1}(N'[v]-P'[a-v])^2$\;
}
}
$v^*=\frac{1}{2}\mathop{\arg\min}\limits_{a}(MSE[a])+v_1$
}
\caption{Polarity-based auto-focus algorithm (\texttt{PBF}).}
\label{algorithm1}
\end{algorithm}


% The validation of the \texttt{PBF} algorithm on large datasets is elaborated in the next section.


\section{Experiments}
\label{section:4}
In this section, 
We conducted a simulation experiment to validate the theory outlined in Section~\ref{section:3}. Subsequently, the \texttt{PBF} algorithm is evaluated on the real-world dataset EAD~\cite{RN54} and compared with the currently state-of-art algorithm EGS~\cite{RN53} in Section~\ref{subsection:42}. Then, we test high-speed AF on our collected EvHF dataset in Section~\ref{subsection:43}. Finally, extensive ablation experiments are conducted in Section~\ref{subsec:ablation} to demonstrate the effectiveness of the proposed methods in our algorithm.

% firstly, a simulation experiment is performed to prove the reliability of our theory mentioned in Section~\ref{section:3}. Subsequently, the \texttt{PBF} algorithm is verified on the public dataset EAD~\cite{RN54} including various real-world scenes, and compared with the currently state-of-art algorithm EGS~\cite{RN53} of event camera AF. Then, the high-speed AF of the event camera is also tested on our own hardware. Finally, some ablation experiments are conducted to demonstrate the effectiveness of the methods proposed in our algorithm.

\subsection{Simulation experiment}
\label{subsection:41}
The event generation model presented in Section~\ref{subsection:31} and the mechanism of events generated in the focusing process discussed in Section~\ref{subsection:32} are used to develop a MATLAB script, as shown in Code 2 (Ref.~\cite{simulationcode}), that simulates the EPR sequences generated during the actual focusing process. To determine the focus position, we applied our \texttt{PBF} algorithm to the simulated data.

\textbf{Simulation experiment setup.} Our simulation model first reads in a light intensity image $I$, convolves it with Gaussian convolution kernel of different sizes, and obtains a series of blurry images under different defocus amounts. According to the definition of event camera brightness, we log map these light intensity images $I$ to the brightness image $L$ and stack them in the order of focusing process (the convolution kernel changes from large to small, and then from small to large). Besides, a new image $L_{old}$ is created to store the last trigger brightness of each pixel, and the initial value is the first one of the blurry image stack. For each pixel in the ROI, when Eq.~(\ref{eq2}) is satisfied, an event signal is triggered, and the brightness of the pixel is saved to $L_{old}$. And if $\Delta L$ is a multiple of the thresholds, the number of events is multiple accordingly, consistent with other event camera simulators~\cite{hu2021v2e}. Then, we accumulate the number of positive and negative events triggered in the whole ROI during a given time interval $\Delta t$ to obtain the EPR time sequences.
% (5) Create a new image $L_{old}$ to store the last trigger brightness, and the initial value is the first brightness images $L$.

\textbf{Qualitative Analysis.} 
% The EPR sequences obtained in simulation experiments do not differ much from those obtained from real experiments. With such simulation experiments, a very rich dataset can be generated. 
We show two qualitative examples in Fig.~\ref{fig: simulation}. The first row shows a sequence of focusing on a number with 100\% contrast. From the EPR curve in (c), the EGS algorithm finds the position in the sequence where the sum of PER and NER is greatest, marked by the green dashed line. Fig.~\ref{fig: simulation} (a) shows the corresponding image of the EGS result. On the contrary, the second row shows one of the cases where EGS performs relatively well. The red box represents the ROI for this case, ensuring that each feature inside it is sharp. In this case, the peaks of PER and NER sequences are not too far apart and are close to the ground truth position. Therefore, the event-rate-based method can also achieve an error within a depth of focus. In either example, the sequences of PER and NER are symmetrically distributed about the ground truth position and the \texttt{PBF} algorithm result (yellow dashed line in (c)) is extremely close to the ground truth and the corresponding images are sharp enough (shown in (b)), which proves the reliability of the \texttt{PBF} principle.

\begin{figure}[ht!]
\centering\includegraphics[width=12cm]{Simulation.pdf}
\caption{Qualitative results of the simulated experiment. (a): Results of  EGS. (Top: $\Delta v$ = -221\si{\micro\meter}, 8 depths of focus. Bottom: $\Delta v$ = -27\si{\micro\meter}, 1 depth of focus).  (b): Results of \texttt{PBF} image (on focus). (c): EPR sequences. Dashed line: green for EGS result, yellow for PBF result, and black for ground truth. }
% Green dashed line: EGS result, black dashed line: ground truth, yellow dashed line: \texttt{PBF} result.}
\label{fig: simulation}
\end{figure}

From the discussion of these two examples, it can be found that the distance separating the EPR peaks and the steepness of the peaks significantly affect the performance of the EGS algorithm in event camera focusing. In our simulation experiments, we find that the ER peaks are closer to the ground truth position for images with finer details, such as the second row of Fig.~\ref{fig: simulation}, while for images with clear boundaries and high contrast, such as the tree branches in Fig.~\ref{fig:branches example} and the figures in the first row of Fig.~\ref{fig: simulation}, the EPR peaks are farther away from the ground truth position. These intuitively demonstrate the shortcomings of the EGS algorithm. For the \texttt{PBF} algorithm, on the other hand, the symmetric distribution property of event polarities is universal, regardless of the texture and contrast of the scene selected.

The simulation experiments only consider events brought about by the change in the size of the convolution kernel during the focusing process, and can hardly mimic the noise in the real world. Therefore, in the next few subsections, we conduct more experiments on real event camera datasets to verify the robustness of our theory and algorithms.

\subsection{The Event-based AF Dataset (EAD)}
\label{subsection:42}
To better evaluate the performance of the \texttt{PBF} algorithm in the real-world event camera focusing process, we conduct experiments on public event camera AF dataset EAD~\cite{RN54} and compare it with the state-of-art algorithm EGS~\cite{RN53}. The EAD dataset uses the commercial event camera DAVIS 346 Color (resolution 346$\times$260) with a Fujifilm D17x7.5B-YN1 motorized zoom lens~\cite{RN54}, but the exact focal length of each sequence is unknown. The dataset includes different illumination levels (0.7 Lux to 23.35K lux), multiple real scenes (still and motion), and camera shake/still scenarios. 
This dataset is available in \cite{RN54}, and we also provide the numpy files of it in Dataset 1 (Ref.~\cite{EADnpy}). % 接收后删除此句

\textbf{Algorithm hyperparameters setting.} EPR time interval is selected to 1 $\si{\milli\second}$. Regarding the EPR wavelet filter parameters, we choose 'dmey' as our base function, perform 6-layer decomposition and take only the low-frequency component cA6. The MSE investigation factor $k$ is selected to 0.5.

\textbf{Overall results for the EAD dataset.} In Table~\ref{tab:addlabel}, we make comparisons of ER, EGS~\cite{RN53} and \texttt{PBF} in the EAD dataset and evaluate the overall algorithm properties using the mean absolute error (MAE) and root mean square error (RMSE). 
% Table generated by Excel2LaTeX from sheet 'Sheet1'

\begin{table}[htbp]
  \centering
  \scriptsize
  \caption{Comparisons of ER, EGS, and \texttt{PBF} in EAD dataset. Our method outperforms in dark scenes, light static scenes and overall. For each item, the \textbf{best} result is in \textbf{bold}.}
    \begin{tabular}{c|c|c|c|c|c|c}
    \hline
    \multirow{2}[4]{*}{Method} & \multirow{2}[4]{*}{Metric} & \multicolumn{5}{c}{Scene Type} \\
\cline{3-7}          &       & \multicolumn{2}{c|}{Light} & \multicolumn{2}{c|}{Dark} & \multirow{2}[1]{*}{Total} \\
\cline{3-6}          &       & Static & Dynamic & Static & Dynamic &  \\
    \hline
    \multirow{2}[1]{*}{ER($\Delta t$ =0.055)} & MAE   & 98.6  & 105.3  & 59.3  & 87.4  & 87.6  \\
\cline{2-7}          & RMSE  & 112.8  & 119.2  & 73.2  & 113.9  & 106.4  \\
    \hline
    \multirow{2}[1]{*}{ER($\Delta t$ =0.065)} & MAE   & 91.4  & 74.3  & 58.0  & 67.1  & 72.7  \\
\cline{2-7}          & RMSE  & 107.3  & 87.6  & 71.9  & 103.7  & 93.7  \\
    \hline
    \multirow{2}[1]{*}{EGS} & MAE   & 77.3  & \textbf{29.0}  & \textbf{54.1}  & 64.9  & 56.3  \\
\cline{2-7}          & RMSE  & 98.9  & \textbf{33.4}  & 71.7  & 96.7  & 79.7  \\
    \hline
    \hline
    \multirow{2}[1]{*}{\texttt{PBF}} & MAE   & \textbf{47.8}  & 64.0  & 54.3  & \textbf{39.0}  & \textbf{52.5} \\
\cline{2-7}          & RMSE  & \textbf{51.8}  & 87.3  & \textbf{65.1}  & \textbf{52.2}  & \textbf{65.7} \\
    \hline
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%

The proposed \texttt{PBF} algorithm outperforms the other event-based methods in the dataset as a whole and in all scenes except light dynamic scenes. The EGS algorithm performs better in the light dynamic scenes due to the fact that the focusing speed used in the EAD dataset is too slow compared to the camera motion, resulting in far more events generated by the motion than by focusing itself, which leads to a larger deviation in \texttt{PBF}. At the same time, EGS achieves better results because the optical flow of the images remains constant in these sequences, satisfying the assumption of the constant optical flow of the EGS algorithm.


It is worth noting that for focusing tasks, any focusing error of less than one depth of focus is acceptable. For the lens used in the EAD dataset, the depth of focus is about 50 \si{\micro\meter}. The evaluation of the algorithm robustness needs to focus more on the short board data, e.g., scenarios where the error exceeds 4-5 depths of focus. Compared to the EGS algorithm, which produces a maximum error of 236 µm (about 5 depths of focus) in the EAD dataset, our \texttt{PBF} algorithm produces an error of only 149 µm (about 3 depths of focus) in the worst case. 

In terms of running speed, the running time of the EGS algorithm on different sequences varies greatly (4\si{\milli\second} to 12s, average 2.6s), proportional to the number of events in the sequence. However, for our algorithm, the elapsed time is about 0.014s on every sequence. On average, our algorithm is 185 times faster than the speed of the EGS algorithm on the whole EAD dataset.

\begin{figure}[ht!]
\centering\includegraphics[width=10cm]{EAD_analysis.pdf}
\caption{Analysis on "chinese light static" sequence in EAD dataset. (a): APS frame images corresponding to the timestamps. (left to right: EGS result, \texttt{PBF} result, ground truth). (b): The symmetric EPR sequences after wavelet de-noise and polarity normalization.}
\label{fig: EAD_analysis}
\end{figure}

\textbf{Qualitative Analysis.} The sequences shown in Fig.~\ref{fig:related works} (a) are selected as an example to qualitatively compare the \texttt{PBF} algorithm with the EGS algorithm in the EAD dataset.

The "chinese light static" raw sequence of the EAD dataset shows a large difference between PER and NER, as shown in Fig.~\ref{fig:related works} (a). Fig.~\ref{fig: EAD_analysis} (b) shows the EPR sequences after wavelet transform and normalization mentioned in Algorithm~\ref{algorithm1}. Wavelet filtering eliminates the dense spikes in the raw sequences, making the variation of EPR with image distance $v$ changing more intuitive. The normalized PER and NER sequences show a clear symmetry with the ground truth timestamp. In this sequence, the \texttt{PBF} obtains a focus error of -77 \si{\micro\meter} (nearly 1 depth of focus) compared to the -191 \si{\micro\meter} focus error of the EGS (equivalent to about 4 depths of focus), as shown by the dashed lines in Fig.~\ref{fig: EAD_analysis} (b). 


\subsection{Event camera high-speed AF dataset (EvHF)}
\label{subsection:43}
While the EAD dataset offers a variety of scenes for testing focusing algorithms, the image plane movement speed during its focusing process is only \textbf{1\si{\milli\meter}/\si{\second}}. Such a slow focusing speed is not representative of real-world applications. Furthermore, the EAD dataset contains many scenarios where different parts of the scene have different focus positions, making it difficult to establish unambiguous ground truth. To evaluate the robustness of our \texttt{PBF} and EGS algorithms at a more realistic focusing motor speed of \textbf{10\si{\milli\meter}/\si{\second}}, which is typical of current digital camera autofocus (AF) motors, we collected our own Event camera high-speed AF (EvHF) dataset.

% \textbf{System setup.} We use a DAVIS346 mono camera equipped with a 35mm focal length, F1.4, C-Mount lens (depth of focus: 60 \si{\micro\meter}). We fix the camera on a ProScan III displacement stage from Prior Scientific, fix the lens on the optical stage, adjust the optical axis of the lens to the center of the frame of the DAVIS camera, and ensure that the optical axis is perpendicular to the sensor plane. 
% The C-Mount lens is not screwed to the camera, resulting in a gap at the interface. So we use suitable shading measures to avoid stray light from the gap into the sensor surface.

\textbf{Hardware.} 
We use a DAVIS346 mono camera with a 35mm focal length, F1.4, C-Mount lens (depth of focus: 60 \si{\micro\meter}) for data acquisition. The camera is mounted on a ProScan III displacement stage from Prior Scientific, and the lens is fixed on the optical stage. We adjust the optical axis of the lens to the center of the DAVIS camera frame and ensure that it is perpendicular to the sensor plane.
Due to the absence of screws to attach the C-Mount lens to the camera, there is a gap at the interface. To prevent stray light from entering the sensor surface through this gap, we implement appropriate shading measures.

\textbf{Ground truth collection.} 
% We first move the displacement stage slowly and use the traditional contrast-based focus function~\cite{RN50} to analyze the sharpness of the APS frame image given by the DAVIS camera to find the focus position and record it. Subsequently, during the formal focusing process, whenever we receive displacement data from the stage, we send a high level to the DAVIS346, allowing the displacement to be associated with the timestamp of the event camera.
To determine the focus position, we initially move the displacement stage slowly and employ a traditional contrast-based focus function~\cite{RN50} to analyze the sharpness of the APS frame image obtained from the DAVIS camera. The focus position is recorded for future reference. During the formal focusing process, we associate the displacement data from the stage with the timestamp of the event camera by sending a high level to the DAVIS346 whenever we receive displacement data from the stage. To obtain a reasonable focus evaluation, it is necessary to ensure that all features inside ROI have the same depth. So we choose to focus on an LCD placed perpendicular to the optical axis. The LCD screen is placed in an extremely dark scene to minimize the effect of external light reflecting off the surface.


\textbf{Dataset.} By playing static or dynamic features on the screen and adjusting the brightness of the screen, 16 sets of focus data are obtained as shown in Table~\ref{tab:addlabel_2and3}. We provide the raw data as well as numpy files in Dataset 2 (Ref.~\cite{highspeeddataset}).

\textbf{Dynamic scenes.}
The "focusboard" and "panda" sequences simulate irregular camera motion to mimic the effect of handheld shots. In the "street" sequence, scene motion is used to create a realistic portrayal of a moving vehicle. The most challenging sequence is the "numbers" sequence, which features scrolling numbers at 30 frames per second (fps). This sequence shows a significant challenge due to the high-speed numbers variation, which causes numerous noisy events output.

\textbf{Algorithm hyperparameters settings.} 
% (1) EPR acquisition part. EPR time interval $\Delta t$: 1\si{\milli\second} (corresponding displacement interval: 10\si{\micro\meter}). (2) EPR wavelet filter parameters. Base function: Discrete Meyer wavelet ‘dmey’; Discrete wavelet transform hierarchy: 6 (take only the low-frequency component cA6). (3) MSE investigation factor $k$:0.5.
The hyperparameters used for the experiments on the EvHF dataset are the same as those used for the EAD dataset, shown in Section~\ref{subsection:42}.



% Combined with Table 2 and 3'
\begin{table}[htbp]
  \centering
  \scriptsize
  \caption{Comparisons of \texttt{PBF}, EGS, and the ablation in the EvHF dataset. (Errors \textbf{beyond} one depth of focus are in \textbf{bold}).}
    \begin{tabular}{c|c|c|c|c|c}
    \hline
    \multirow{1}[4]{*}{Scene} & \multirow{1}[4]{*}{Sequence name} & \multirow{1}[4]{*}{\texttt{PBF} error/\si{\micro\meter}} & \multirow{1}[4]{*}{EGS error/\si{\micro\meter}} & \multicolumn{2}{c}{Ablation error/\si{\micro\meter}} \\
\cline{5-6}          &       &       &       & without filtering & without MSE \\
    \hline
    \multirow{3}[5]{*}{Dark Dynamic} & D\_D\_focusboard & 53    & -62   & \textbf{73}    & \textbf{-107} \\
\cline{2-6}          & D\_D\_numbers & 22    & \textbf{613}   & \textbf{-643}  & \textbf{572} \\
\cline{2-6}          & D\_D\_panda & 13    & -28   & 13    & -47 \\
\cline{2-6}          & D\_D\_street & 41    & -61   & \textbf{101}   & \textbf{-99} \\
    \hline
    \multirow{3}[5]{*}{Light Dynamic} & L\_D\_focusboard & 58    & \textbf{-212}  & \textbf{73}    & \textbf{-197} \\
\cline{2-6}          & L\_D\_numbers & 6     & \textbf{231}   & 6     & \textbf{216} \\
\cline{2-6}          & L\_D\_panda & 57    & -26   & 67    & -53 \\
\cline{2-6}          & L\_D\_street & 26    & -53   & 21    & \textbf{-109} \\
    \hline
    \multirow{3}[5]{*}{Dark Static} & D\_S\_focusboard & 53    & \textbf{-221}  & \textbf{93}    & -57 \\
\cline{2-6}          & D\_S\_numbers & 21    & \textbf{599}   & 61    & \textbf{571} \\
\cline{2-6}          & D\_S\_panda & 26    & 27    & 46    & -14 \\
\cline{2-6}          & D\_S\_street & 31    & -59   & 61    & \textbf{-79} \\
    \hline
    \multirow{3}[5]{*}{Light Static} & L\_S\_focusboard & 58    & \textbf{-192}  & \textbf{93}    & \textbf{-107} \\
\cline{2-6}          & L\_S\_numbers & 16    & \textbf{188}   & -4    & \textbf{146} \\
\cline{2-6}          & L\_S\_panda & 38    & \textbf{-148}  & 38    & \textbf{-152} \\
\cline{2-6}          & L\_S\_street & 21    & \textbf{-75}   & 21    & \textbf{-99} \\
    \hline
    \hline
    \multicolumn{2}{c|}{MAE/\si{\micro\meter}} & 34 & \textbf{175} & \textbf{88} & \textbf{164} \\
    \hline
    \multicolumn{2}{c|}{Average time/\si{\milli\second} } & 4 & 146 & 3 & 1 \\
    \hline
    \end{tabular}%
  \label{tab:addlabel_2and3}%
\end{table}%

\textbf{Overall results for the EvHF dataset.} We evaluate both EGS algorithm and our proposed \texttt{PBF} algorithm on EvHF dataset, and the detailed results are shown in Table~\ref{tab:addlabel_2and3}. In terms of focusing tasks, errors within one depth of focus (60 \si{\micro\meter}) are considered acceptable. Therefore, any results beyond one depth of focus error are marked in bold. The \texttt{PBF} algorithm has achieved an error within 1 depth of focus for all sequences in the dataset, while the EGS algorithm shows a worst error of 10 depths of focus (613 \si{\micro\meter}). On average, the PBF algorithm has an MAE of 34 \si{\micro\meter} on the EvHF dataset, while the EGS is 175 \si{\micro\meter}, which is a 5-fold improvement in accuracy in the high-speed scenario. In terms of speed, our algorithm is 36 times faster than the speed of the EGS algorithm on average, referring to the last row of Table~\ref{tab:addlabel_2and3}.

To provide a visual comparison of the effectiveness of the \texttt{PBF} algorithm and the EGS algorithm, some examples from the dataset are elaborated on in the next subsection.


\begin{figure}[ht!]
\centering\includegraphics[width=11cm]{Highspeed.pdf}
\caption{Comparison of the qualitative results from \texttt{PBF} algorithm and EGS algorithm in EvHF dataset. (a): "Numbers" in dark static scene. (b): "Numbers" in dark dynamic scene. The jump of the number from "59" to "1" occurs around the EGS timestamp. (c): "Street" in dark dynamic scene. In each sub-figure, above: APS images corresponding to the timestamps, below: EPR plot. %The red and blue curves represent PER and NER sequences, respectively. And the black, orange, and green dashed lines indicate GT timestamps, \texttt{PBF} focus timestamps, and EGS focus timestamps, respectively. 
}\label{fig: High-speed result}
\end{figure}

\textbf{Qualitative Analysis.} 
Fig.~\ref{fig: High-speed result} (a) shows the EPR sequences when focusing a number '41' in a dark stationary environment. As can be seen, in the dark environment, there are generally more positive events than negative events due to the different trigger thresholds. Since the goal of the EGS algorithm is to find the position with the highest ER in the whole focusing sequence, it faithfully finds the peak position of the ER (i.e., the sum of PER and NER), which is 599 \si{\micro\meter} (equivalent to 10 depths of focus) away from the true focus position, resulting in an unacceptably blurry picture. The \texttt{PBF} algorithm, based on the symmetric relationship of PER and NER, obtains a focusing error (21 \si{\micro\meter}) of less than one depth of focus (60 \si{\micro\meter}). At the same time, due to wavelet denoising, the high-frequency noise prevalent in the EPR sequence does not interfere with the determination of the center of symmetry of the sequence. 

When examining the dataset "numbers" in the dark dynamic scene shown in Fig.~\ref{fig: High-speed result} (b), the \texttt{PBF} algorithm gives fairly stable results, while the EGS algorithm gives an error of nearly 10 depths of focus (613 \si{\micro\meter}). From the EPR graph, there are peaks of PER and NER near the green EGS timestamp. According to the APS images, such peaks come from the jump of the number from "59" to "1". Such a large change in brightness brings more than the number of events near the focus position, so the EGS algorithm based on the ER evaluation fails in this case. The \texttt{PBF} algorithm, on the other hand, maintains robustness to such sudden event surges by exploiting the symmetric nature of the PER and NER in the focusing process. In the dark high-speed focus scenario, the APS sensor of DAVIS is able to capture only 8 images at a frame rate of about 40 fps for the focus motion completed in 0.2 s, which is not sufficient for the traditional contrast-based method of focus evaluation. And, due to the high-speed motion of the scene, image trailing occurs during the APS exposure time of about 10 $\si{\milli\second}$, as in the image pointing to GT timestamp shown in Fig.~\ref{fig: High-speed result} (b), which also means that it is not feasible to use the traditional image focus evaluation method.

The EGS algorithm obtains a relatively good result in the scenario shown in Fig.~\ref{fig: High-speed result} (c). As can be seen from the EPR graph, the peak position of the sum of PER and NER in this scenario is close to the GT position (-61 µm), thus barely meeting the requirements. The \texttt{PBF} algorithm still faithfully finds the center of symmetry position of the PER and NER curves, and thus the error with the GT position is less than one depth of focus. From the simulation experiments in Section~\ref{subsection:41}, it can be learned that the distance between the PER and NER peaks separation is related to the texture and contrast of the scene.



\textbf{Algorithm speed comparison. }In high-speed focusing scenarios, the speed of the algorithm is as important an evaluation indicator as the accuracy. The computational complexity of EGS algorithm $O(N_e)$, which means that the algorithm time consumption is linear to the number of events. In addition, the actual program runtime is consumed heavily in cache reads and writes because of the need to repeatedly access tens of millions of event data. 
In contrast, the original data used by the \texttt{PBF} algorithm is two integer arrays with a length of hundreds or thousands. Such arrays of EPR are available synchronously during the event data acquisition. The running time of the \texttt{PBF} algorithm is mainly spent on the EPR filtering and \texttt{PBF} MSE calculation, which can be done in a few milliseconds even on a single-threaded CPU.

Taking the sequence shown in Fig.~\ref{fig: High-speed result} (a) as an example, the total number of events is 885,024, the length of the EPR sequence is 243, the running time of the Matlab code of the EGS algorithm is 293 \si{\milli\second}, while the running time of the python code of the \texttt{PBF} algorithm is 4 \si{\milli\second}. In the case of a higher number of events, the EGS algorithm time consumption increases linearly with the number of events, while the \texttt{PBF} algorithm time consumption is almost constant.

\subsection{Ablation study.}
\label{subsec:ablation}
The proposed \texttt{PBF} algorithm consists of two important parts: the filtering based on wavelet transform, and the focus position determination based on the \texttt{PBF} MSE evaluation function. We conduct ablation experiments on these two parts separately, and the overall results are shown in the last two columns of Table~\ref{tab:addlabel_2and3}. All the ablation experiments are conducted on our EvHF dataset in Section~\ref{subsection:43}. In the first part of the ablation experiment, we remove the wavelet filter module and use the original un-denoised EPR sequences for MSE computation. In the second part of the ablation experiment, we keep the wavelet filter module but use the ER algorithm to make a focus evaluation.

% \textbf{Experiment setup.} In the first part of the ablation experiment, we remove the wavelet filter module from our algorithm and use the original un-denoised EPR sequences for MSE computation, and the hyperparameters used in the experiments are consistent with the ones mentioned in Section~\ref{subsection:43}. In the second part of the ablation experiment, we keep the wavelet filter module and reduce the number of filter layers to 2 in order to maintain the characteristics of the sequences; subsequently, we use the ER evaluation algorithm in the EGS algorithm to perform the focus evaluation of the \textbf{event rate} sequence. 

\textbf{Quantitative results.} The last column of Table~\ref{tab:addlabel_2and3} shows the quantitative result of ablation experiments. The ablation of the filtering step only significantly affects some sequences with a low signal-to-noise ratio, (e.g., "numbers" in dark dynamic scene), while the others still perform well. However, when ablating the core \texttt{PBF} MSE evaluation module, poor results are obtained in most cases.
% % Table generated by Excel2LaTeX from sheet 'ablation(filter)'
% \begin{table}[ht!]
%   \centering
%   \scriptsize
%   \caption{Results of ablation experiments (Errors beyond one depth of focus are marked bold)}
%     \begin{tabular}{c|c|c|c|c}
%     \hline
%     \multirow{2}[4]{*}{Scene} & \multirow{2}[4]{*}{feature} & \multirow{2}[4]{*}{\texttt{PBF} error /\si{\micro\meter}} & \multicolumn{2}{c}{Ablation error/ \si{\micro\meter}} \\
% \cline{4-5}          &       &       & without filtering & without MSE evaluation \\
%     \hline
%     \multirow{4}[8]{*}{Dark Dynamic} & focusboard & 53    & \textbf{73}    & \textbf{-107} \\
% \cline{2-5}          & numbers & 22    & \textbf{-643}  & \textbf{572} \\
% \cline{2-5}          & panda & 13    & 13    & -47 \\
% \cline{2-5}          & street & 41    & \textbf{101}   & \textbf{-99} \\
%     \hline
%     \multirow{4}[8]{*}{Light Dynamic} & focusboard & 58    & \textbf{73}    & \textbf{-197} \\
% \cline{2-5}          & numbers & 6     & 6     & \textbf{216} \\
% \cline{2-5}          & panda & 57    & \textbf{67}    & -53 \\
% \cline{2-5}          & street & 26    & 21    & \textbf{-109} \\
%     \hline
%     \multirow{4}[8]{*}{Dark Static} & focusboard & 53    & \textbf{93}    & -57 \\
% \cline{2-5}          & numbers & 21    & 61    & \textbf{571} \\
% \cline{2-5}          & panda & 26    & 46    & -14 \\
% \cline{2-5}          & street & 31    & 60.9  & \textbf{-79} \\
%     \hline
%     \multirow{4}[8]{*}{Light Static} & focusboard & 58    & \textbf{93}    & \textbf{-107} \\
% \cline{2-5}          & numbers & 16    & -4    & \textbf{146} \\
% \cline{2-5}          & panda & 38    & 38    & \textbf{-152} \\
% \cline{2-5}          & street & 21    & 21    & \textbf{-99} \\
%     \hline
%     \end{tabular}%
%   \label{tab:addlabel3}%
% \end{table}%

\textbf{Qualitative Analysis.} We still choose the "numbers" sequence in the dark dynamic scene shown in Fig.~\ref{fig: High-speed result} (b) to analyze the results of the ablation experiment. 

The wavelet filtering de-noise process plays an important role in improving the robustness. After the ablation of the de-noise process, the events brought by the high-speed changing numbers generate noise with a frequency of 30Hz in the EPR sequence, as shown in Fig.~\ref{fig:ablation}. Without the filtering operation, the MSE evaluation function of the \texttt{PBF} algorithm is no longer a single valley function and therefore generates unpredictable errors in finding the symmetry center. Near the 
% green  % modified due to the change in fig. 9
purple dashed line, the positive and negative events caused by the scene motion also show some symmetry on the time axis because of the 30Hz scene movement, and the MSE evaluation function reaches a global minimum at this position, thus producing erroneous results.

The \texttt{PBF} MSE evaluation function is the core part of our algorithm. When this part is ablated, even if the noise in the ER sequence is filtered using the wavelet transform, the results obtained using the ER maximum-based method do not differ much from those obtained by the EGS algorithm, referring to Table~\ref{tab:addlabel_2and3}. The reason for such a result is the same as the EGS algorithm. In fact, the EGS algorithm's dichotomous lookup method also plays the same function as wavelet transform de-noising.
% \begin{figure}[ht!]
%     \centering\includegraphics[width=0.9\linewidth]{ablation.pdf}
%     \caption{Results of the ablation experiment (data sequence "numbers" in dark dynamic scene). The green dashed line, black dashed line, and orange dashed line indicates the Ablation timestamp, GT timestamp, and \texttt{PBF} timestamp respectively. (a) Wavelet filtering ablation result. (b) \texttt{PBF} MSE evaluation ablation result.}\label{fig:ablation}
% \end{figure}

\begin{figure}[ht!]
    \centering\includegraphics[width=11cm]{ablation_simplified.pdf}
    \caption{Results of the ablation experiment (data sequence "numbers" in dark dynamic scene). The dashed line: black for the ground truth, orange for \texttt{PBF} result, purple for the result ablating filtering, and green for the result ablating MSE evaluation.}\label{fig:ablation}
\end{figure}







\section{Conclusion}
In this paper, we (1) explain the mechanism of events output in the AF process, (2) find the relationship between the EPR sequence and the focus position, and (3) derive the Polarity-based auto-focus (\texttt{PBF}) algorithm. We verify our proposed algorithm on the event camera AF public EAD dataset and compare it with the state-of-art event camera AF algorithm event-based golden search (EGS). Our \texttt{PBF} algorithm shows higher accuracy and faster speed. We also collect our event camera high-speed focusing (EvHF) dataset featuring scenes with
varying lighting conditions and motion states. The
supplement experiments on our dataset verify the robustness and real-time performance of our proposed \texttt{PBF} algorithm in high-speed focus scenarios. The experimental results provide compelling evidence that the proposed \texttt{PBF} algorithm can ensure highly accurate focusing performance, achieving errors below 1 depth of focus across diverse scenes with varying brightness and motion states.

\section{Discussion}
% Currently, due to the limitation of the spatial resolution of the event camera device, the experiment utilizes the event information of all pixels. Theoretically, at higher resolution, we can select the ROI at different positions in the image plane for focus evaluation separately. Knowing the lens parameters and the image distance, the distance from the object side to the lens can be known through the object-image relationship, which realizes the three-dimensional spatial recovery based on the focus variation method. When examining a smaller ROI, e.g., one pixel, the feature shift due to the breathing effect has to be considered, as discussed in Section~\ref{subsubsection:323}.
Currently, due to the limited spatial resolution of the event camera device, the experiment employs event information from all pixels. In theory, higher resolution would allow for selecting regions of interest (ROIs) at different positions in the image plane for separate focus evaluation. Utilizing knowledge of the lens parameters and image distance, the distance from the object side to the lens can be determined via the object-image relationship, enabling three-dimensional spatial recovery based on the focus variation method. However, when examining a smaller ROI, such as a single pixel, the effect of feature shift due to breathing must be taken into consideration, as discussed in Section~\ref{subsubsection:323}.


\begin{backmatter}
\bmsection{Funding} This work is supported by the National Key R\&D Program of China, under Grant No. 2022YFF0705500, National Key R\&D Program of China (2022YFB3206000).

% \bigskip
% \bmsection{Acknowledgments} 


\bigskip
\bmsection{Disclosures}
The authors declare no conflicts of interest.

\bigskip
\bmsection{Data availability} Data underlying the results presented in this paper are available 
in Ref.~\cite{PBFcode,EADnpy,highspeeddataset,simulationcode}.
% in Ref.~\cite{PBFcode}, ~\cite{EADnpy}, ~\cite{highspeeddataset} and ~\cite{simulationcode}.



\end{backmatter}


%%%%%%%%%%%%%%%%%%%%%%% References %%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%% If using BibTeX:
\bibliography{PBF}


\end{document}