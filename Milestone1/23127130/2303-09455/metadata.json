{
    "arxiv_id": "2303.09455",
    "paper_title": "Learning Cross-lingual Visual Speech Representations",
    "authors": [
        "Andreas Zinonos",
        "Alexandros Haliassos",
        "Pingchuan Ma",
        "Stavros Petridis",
        "Maja Pantic"
    ],
    "submission_date": "2023-03-14",
    "revised_dates": [
        "2023-03-17"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL",
        "cs.CV",
        "cs.LG",
        "cs.SD",
        "eess.AS"
    ],
    "abstract": "Cross-lingual self-supervised learning has been a growing research topic in the last few years. However, current works only explored the use of audio signals to create representations. In this work, we study cross-lingual self-supervised visual representation learning. We use the recently-proposed Raw Audio-Visual Speech Encoders (RAVEn) framework to pre-train an audio-visual model with unlabelled multilingual data, and then fine-tune the visual model on labelled transcriptions. Our experiments show that: (1) multi-lingual models with more data outperform monolingual ones, but, when keeping the amount of data fixed, monolingual models tend to reach better performance; (2) multi-lingual outperforms English-only pre-training; (3) using languages which are more similar yields better results; and (4) fine-tuning on unseen languages is competitive to using the target language in the pre-training set. We hope our study inspires future research on non-English-only speech representation learning.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.09455v1"
    ],
    "publication_venue": null
}