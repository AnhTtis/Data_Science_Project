%%
%% Copyright 2007-2020 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version. The latest version of this license is in
%% http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

 \documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx,dblfloatfix}
\usepackage{textcomp,hyperref}
\usepackage{subcaption}
\hypersetup{colorlinks,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black}

\usepackage{xcolor,algorithm}
\usepackage{multirow,makecell}

\journal{Artificial Intelligence in Medicine}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%% addressline={},
%% city={},
%% postcode={},
%% state={},
%% country={}}
%% \fntext[label3]{}

\title{Exploiting 4D CT Perfusion for segmenting infarcted areas in patients with suspected acute ischemic stroke \tnoteref{bmdlab}}
\tnotetext[bmdlab]{All authors are with the BioMedical Data analysis group (\url{https://www.uis.no/en/bmdlab})}
%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%% addressline={},
%% city={},
%% postcode={},
%% state={},
%% country={}}
%%
%% \affiliation[label2]{organization={},
%% addressline={},
%% city={},
%% postcode={},
%% state={},
%% country={}}

\author[uis]{Luca Tomasetti\corref{cor1}}
\ead{luca.tomasetti@uis.no}
\author[uis]{Kjersti Engan}
\ead{kjersti.engan@uis.no}
\author[uis,sus]{Liv Jorunn H\o{}llesli}
\ead{liv.j.hollesli@uis.no}
\author[uis,sus]{Kathinka D{\ae}hli Kurz}
\ead{kathinka.kurz@uis.no}
\author[uis]{Mahdieh Khanmohammadi}
\ead{mahdieh.khanmohammadi@uis.no}

\affiliation[uis]{organization={Department of Electrical Engineering and Computer Science, University of Stavanger},%Department and Organization
city={Stavanger},
postcode={4021},
country={Norway}}
\affiliation[sus]{organization={Stavanger Medical Imaging Laboratory (SMIL), Department of Radiology, Stavanger University Hospital},%Department and Organization
city={Stavanger},
postcode={4019},
country={Norway}}
\cortext[cor1]{Corresponding author}

\begin{abstract}
%% Text of abstract
Precise and fast prediction methods for ischemic areas (core and penumbra) in acute ischemic stroke (AIS) patients are of significant clinical interest: they play an essential role in improving diagnosis and treatment planning.
Computed Tomography (CT) scan is one of the primary modalities for early assessment in patients with suspected AIS.
CT Perfusion (CTP) is often used as a primary assessment to determine stroke location, severity, and volume of ischemic lesions.
Current automatic segmentation methods for CTP mostly use already processed 3D color maps conventionally used for visual assessment by radiologists as input. Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+time input, where the spatial information over the volume is ignored. 
In this paper, we investigate different methods to utilize the entire 4D CTP as input to fully exploit the spatio-temporal information. 
This leads us to propose a novel 4D convolution layer.
Our comprehensive experiments on a local dataset comprised of 152 patients divided into three groups show that our proposed models generate more precise results than other methods explored.
A Dice Coefficient of 0.70 and 0.45 is achieved for penumbra and core areas, respectively.
The code is available on \href{https://github.com/Biomedical-Data-Analysis-Laboratory/4D-mJ-Net.git}{GitHub}.
\end{abstract}

%%Graphical abstract
%%\begin{graphicalabstract}
%\includegraphics{grabs}
%%\end{graphicalabstract}

%%Research highlights
%%\begin{highlights}
%%\item Research highlight 1
%%\item Research highlight 2
%%\end{highlights}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
4D Convolution \sep Acute Ischemic Stroke \sep Computed Tomography Perfusion \sep Deep Neural Network.
%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{intro}

An acute ischemic stroke (AIS) generally occurs if a portion of the supplying arteries of the brain is occluded by a blood clot and prevents the regular flow of oxygen-rich blood in the brain tissue.
If collaterals are lacking or insufficient, this can lead to ischemic areas. 
There are two different types of ischemic areas: 1) penumbra, areas where the tissue is still vital but critically hypoperfused \citep{astrup1981thresholds}; and 2) infarct core, referring to irreversibly damaged areas.
If blood flood is not restored timely, penumbra regions may develop rapidly into infarct core areas.
Therefore, a fast and accurate understanding of ischemic areas to plan the treatment and tailor further procedures to every single patient is fundamental.

The recommended modalities for diagnostic imaging in AIS patients are Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) \citep{european2008guidelines}.
In the initial stages of an acute ischemic stroke, CT Perfusion (CTP) has been proven to be a fast and beneficial tool for evaluating both diagnosis and prognosis \citep{campbell2013ct}.
A CTP is a four-dimensional (4D) spatio-temporal examination to assess the passage of blood in the brain.
It is performed by acquiring a series of three-dimensional (3D) CT scans of a specific portion of the brain during contrast agent injection.
By using an iodinated contrast agent, density changes in the brain tissue over time can be analyzed. 
The shape and height of the time density curve are dependent on the perfusion of the brain tissue. \citep{kurz2016radiological}.

The raw 4D CTP contains a vast amount of data; hence, detecting ischemic strokes can be challenging for neuroradiologists, primarily due to the need for urgent and precise diagnosis and treatment.  
To overcome this challenge, doctors rely on a set of color-coded parametric maps (PMs) generated from the 4D CTP scan, as a diagnostic tool and for treatment decisions.
The generated PMs reduce the temporal dimension and produce 3D volumes.
The set of PMs normally used is: $\text{CBF}, \text{CBV}, \text{TMAX}, \text{TTP}, \text{MIP}$.
CBF and CBV correspond to cerebral blood flow and volume maps. 
TMAX is the time to maximum map; TTP represents the time to peak map, while MIP gives the map for the maximum intensity projection.
Although PMs provide helpful information about ischemic brain tissue, extracting them from the 4D CTP scans limits the spatio-temporal information only to specific subsets of information \citep{soltanpour2022using}.
Several methods \citep{d2015time,bivard2014defining} have used thresholding techniques to predict the ischemic areas from the PMs.
However, simple thresholding approaches over-simplify the complexity in AIS \citep{nielsen2018prediction}.

In the past years, Deep Neural Networks (DNNs), and especially Convolutional Neural Networks (CNNs), have been successfully applied in numerous medical applications: image classification tasks \citep{chen2021annotation,wetteland2019multiclass,neel2022quantifying,fuster2022invasive,kanwal2022devil}; automatic video analysis \citep{chakraborty2013video,meinich2020activity} and activity recognition using body sensors \citep{akbulut2020wearable,amft2008recognition}.
Automatic image segmentation adopting U-Net structure \citep{ronneberger2015u} and its numerous variants have produced innovative outcomes for several applications \citep{isensee2021nnu, zhou2018unet++, wetteland2020multiscale, khan2022rms, kamnitsas2017efficient}.
Several DNNs have been proposed for AIS applications to predict and segment only the infarct core regions using CT studies in combination with PMs derived from CTP scans as input. % \citep{clerigues2019acute,abulnaga2018ischemic, kasasbeh2019artificial, tureckova2018isles, soltanpour2019ischemic, soltanpour2021improvement}.
\citet{clerigues2019acute} adopted a 2D asymmetric residual encoder-decoder to segment stroke lesions from CT and PMs.
\citet{abulnaga2018ischemic} proposed a modified CNN with pyramid pooling and a focal loss, providing global and local contextual information stacking CT slices and PMs for the model's input.
\citet{kasasbeh2019artificial} implemented a CNN with a set of PMs as input for infarct core segmentation.
\citet{rava2021investigation} investigated a modified U-Net with multiple PMs as input to segment infarct tissue.
Soltanpour et al. developed two models using modified U-Nets taking in input a set of PMs derived from CTP. \citep{soltanpour2019ischemic,soltanpour2021improvement}. 
However, a general problem with these methods is the use of heavily pre-processed information made for visual interpretation (i.e. PMs) rather than taking advantage of the entire 4D CTP scans.

DNNs are more suitable for discovering information from raw data.
Nevertheless, relying on raw data (directly exploiting the temporal and spatial dimensions) is scarcely explored in the literature for AIS applications.
To the best of our knowledge, few studies proposed DNN models with promising results, exploiting the temporal dimension to assess ischemic stroke lesions using CTP images \citep{soltanpour2022using, amador2021stroke, robben2020prediction, tomasetti2020cnn, amador2022predicting}.
\citet{soltanpour2022using} utilized 4D CTP images to create 2D matrices, in which each row is a voxel, and each column is a time point. 
The 2D matrices are used as input for a model that shows encouraging results in differentiating healthy tissue from the ischemic core.
\citet{robben2020prediction} proposed a DNN that predicts the final infarct core volume directly from 4D raw CTP plus patients metadata (treatment parameters).
Their proposed architecture relies on a series of 3D Convolution layers; the input is a list of 4D CTP scans sampled at different resolutions.  
Their method presents satisfactory segmentation results; however, the quality of the ground truth images is debatable since they rely on non-contrast CT (NCCT) follow-up images acquired after 24 hours and five days.
It has been reported that infarcted core lesions can grow after 24 hours in NCCT measurements \citep{bucker2017associations}.
\citet{amador2021stroke} designed a framework, based on Temporal Convolution Network, for predicting AIS ischemic core from 4D CTP studies. Due to memory constraints, they independently processed each 2D slice of the 4D CTP dataset. In their recent work, \citet{amador2022predicting} also proposed an extension of their model where 3D+time tensors of the ipsilateral stroke hemisphere are used as input to predict only the ischemic core.

The aforementioned segmentation methods rely on ground truth labels obtained from either Diffusion-weighted imaging (DWI) or NCCT obtained hours or days after the patient's admission.
All these studies emphasize the potential of 4D data in AIS prediction. 
However, an appropriate method is still required to simultaneously handle the spatio-temporal information for segmenting both the ischemic core and penumbra regions. 
An understanding of the extension of the penumbra during the first stages of the ischemic stroke is crucial for treatment decision \citep{murphy2006identification,tomasetti2021machine}.
To the best of our knowledge, Tomasetti et al. was the first research group to segment both \emph{core} and \emph{penumbra} regions using machine learning \citep{tomasetti2021machine} or DNN approaches \citep{tomasetti2020cnn,tomasetti2022multi}.

In this paper, we present and investigate three novel models to segment the two ischemic regions (core and penumbra), where the input is the entire 4D CTP scans arranged in different ways to exploit the spatio-temporal nature of the data. 
We compare all models with previous work based on PMs \citep{tomasetti2022multi} and slice-by-slice CTP \citep{tomasetti2020cnn}, and two methods proposed by \citet{amador2021stroke, amador2022predicting}.

%\subsection{Contributions}

The main contributions of this work can be summarized in four points:
\begin{enumerate}
%\item We propose three different architectures to segment both core and penumbra regions. 
\item We develop a novel 4D convolution layer that processes the 4D raw data. %The convolution layer is created using existing layers from a publicly available framework\footnote{\url{https://keras.io/}}.
\item We implement the 4D convolution layer in a novel DNN model, called \emph{4D mJ-Net}, to segment core and penumbra areas from 4D CTP scans.
%develop a DNN model, called \emph{4D mJ-Net}, that takes the totality of the 4D CTP scans as input and 4D convolution layers in the structure. 
\item We extend previous methods \citep{amador2021stroke,tomasetti2020cnn} to be used directly with 4D CTP scans as input.
% \item We introduce a DNN method, called \emph{3D+time mJ-Net}, which uses a list of 2D+time CTP scans as input to utilize the spatio-temporal nature of the data.
% \item We extend the model proposed by \citet{amador2021stroke} to input a list of 3D CTP scans and name it \emph{3D-TCN}.
\item To assess the results, we use manual annotations obtained by two expert neuroradiologists from the 4D CTP data upon patients' admission. We also demonstrate the feasibility of our proposed methods by comparing their performances with existing models that rely on different inputs.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data Material}\label{material}

A section of the brain is repeatedly scanned during the passage of 40 ml iodine-containing contrast agent (Omnipaque 350 mg/ml) and 40 ml isotonic saline in a cubital vein with a flow rate of 6 ml/s to highlights changes in the tissue; the scan delay was four seconds.
Each brain slice contains a fixed number of time points $t_{\text{max}}$ representing the temporal dimension.
The first twenty time points are acquired every 1s, while the remaining ten images are every 1.5s.
The width and height of each image are $512\times512$ pixels with a resolution of 0.4258 mm/pixel.
The slice thickness is 5mm.

CTP scans from 152 patients collected between January 2014 and August 2020 formed the dataset.
137 of these patients had an AIS with a visible perfusion deficit.
During the diagnostic workup, the remaining 15 patients were admitted with suspected strokes but were determined not to have suffered from a stroke episode after the diagnostic workup.
The patients were divided into the following groups: 77 patients with large vessel occlusion (LVO), 60 patients with non-large vessel occlusion (Non-LVO), and the remaining 15 patients without ischemic stroke (WIS).

Based on CT angiography, large vessel occlusion (LVO) was defined as occlusion of any of the following arteries: the internal carotid artery, M1 and proximal M2 segment of the middle cerebral artery, A1 segment of the anterior cerebral artery, P1 segment of the posterior cerebral artery, basilar artery, and vertebral artery occlusion.
Non-LVO was defined as patients with perfusion deficit with more distal artery occlusion or with perfusion deficit without visible artery occlusion.

The dataset was randomly split into a training, validation, and holdout set.
The percentage of the three subsets (LVO, Non-LVO, WIS) is equally distributed among the sets, as shown in Table \ref{tab:division}.

\begin{table}[ht]
\caption{Division in training, validation, and holdout dataset.}
\label{tab:division}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|c|c|c|c}
\Xhline{3\arrayrulewidth}
& \textbf{Training (\#; \%)} & \textbf{Validation (\#; \%)} & \textbf{Holdout (\#; \%)} & \textbf{Tot. (\#; \%)} \\
\Xhline{3\arrayrulewidth}
\textbf{LVO} & 42; 54.5 & 16; 20.8 & 19; 24.7 & 77; 50.6 \\
\hline
\textbf{Non-LVO} & 36; 60 & 13; 21.7 & 11; 18.3 & 60; 30.5 \\
\hline
\textbf{WIS} & 9; 60 & 3; 20 & 3; 20 & 15; 9.8 \\
\Xhline{3\arrayrulewidth}
\textbf{Total} & 87; 58.6 & 32; 19.7 & 33; 21.7 & 152; 100 \\
\hline
\end{tabular}
}
\end{table}

\subsection{Ground truth}\label{gt}
The manual annotations are based on the entire CT dataset, including the parametric maps derived from CTP. MRI performed during the first days after hospital admission was also utilized. Details for the manual annotations have been described \citep{tomasetti2021machine} previously.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background theory \& existing methods}
Together with the proposed approaches (details in Sec. \ref{sec:propmet}), we implemented and compared the \emph{2D-TCN} \citep{amador2021stroke} and the \emph{mJ-Net} \citep{tomasetti2020cnn} to validate the performances of our models.
Additionally, we compare the models with a method that uses a set of PMs as input \citep{tomasetti2022multi}.
In the remainder of the paper, we call this architecture \emph{Multi-input PMs}.
\begin{figure*}[h!]
\centering
\centerline{\includegraphics[width=.76\linewidth]{images/inputs_2.pdf}}
\caption{Visual comparison of the input for each implemented model.
Every 4D CTP patient's study $V \in \mathbb{R}^{(X \times Y \times Z \times T)}$ undergoes a series of pre-processing steps to enhance each CTP scan (details in Algorithm \ref{alg}).
Approach 1 (\emph{Multi-input PMs}) and 2 (\emph{mJ-Net})  are our methods implemented in \citep{tomasetti2022multi} and  \citep{tomasetti2020cnn}, respectively.
Approach 1 accepts a list of PMs generated from a CTP study in input instead of $\widetilde{V}$.
Approach 3 (\emph{2D-TCN}) follows the model proposed by \citet{amador2021stroke}.
Approach 4 (\emph{3D-TCN-SE}) implements the approached proposed by \citet{amador2022predicting}.
The resulting 4D tensor is fed to one of the approaches.
The proposed approach 5 (\emph{3D-TCN}), 6 (\emph{3D+time mJ-Net}), and 7 (\emph{4D mJ-Net}) take in input the entire 4D CTP  processed data $\widetilde{V}$.
}
\label{fig:inputs}
\end{figure*}

\subsection{Notation}

\begin{table*}[ht]
\caption{List of formal notations used in the paper.}
\label{tab:notation}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c||c|c}
\textbf{Notation} & \textbf{Description} & \textbf{Notation} & \textbf{Description} \\
\hline
$t = [t_j | \forall j \in \{1,2,\cdots, t_{\text{max}}\}]$ & List of all the time points. & $t_{\text{max}}$ & \makecell{ Last time point in the \\ time dimension. } \\ \hline
$z = [z_i | \forall i \in \{1,2,\cdots, z_{\text{max}}\}]$ & List of all the slices. & $z_{\text{max}}$ & \makecell{Last slice in the \\ depth dimension.} \\ \hline
$\mathcal{I} = \{i-1,i,i+1\}$ & \makecell{Set of indexes $i$, plus its \\ neighbours $i-1$ and $i+1$.} & $z_I = \{z_{i-1},z_i,z_{i+1}\}$ & \makecell{Set of slices: \\ $ z_{i-1},z_i,z_{i+1} $.} \\ \hline
$V \in \mathbb{R}^{(X \times Y \times Z \times T)} $ & 4D \textbf{raw} CTP input. & \makecell{$\mathcal{C} = \{$healthy brain,\\ penumbra, core$\}$} & Set of classes. \\ \hline
$I^{t_j}_{z_i} \in \mathbb{R}^{(X \times Y)}$ & \makecell{2D brain slice $z_i$ \\ at time point $t_j$.} & $P_{z_i} \in \mathbb{R}^{(X \times Y)}$ & \makecell{2D probability output \\ of brain slice $z_i$.} \\ \hline
$\widetilde{\cdot}$ & \makecell{Input \textbf{after}\\ \textbf{pre-processing steps}.} & $\bar{\cdot}$ & \makecell{\textbf{List} of inputs} \\ \hline
$\varphi(\cdot)$ & Concatenation function. & $\widehat{\cdot} $ & \makecell{\textbf{Concatenated} inputs after \\ passing through $\varphi(\cdot)$.} \\ \hline
$\bar{V}^{t}_{z_i} = [ \widetilde{I}_{z_i}^{t_j} | \forall t_j \in t ] \in \mathbb{R}^{(X \times Y)}$ & \makecell{List of 2D images $\widetilde{I}_{z_i}$ \\ for all the time points $t$. \\ Input for \emph{2D-TCN} (Sec. \ref{tcn}).} &
$\widehat{V}_{z_i}^t =\varphi(\widetilde{I}_{z_i}^{t_j} | \forall t_j \in t) \in \mathbb{R}^{(X \times Y \times T)}$ & \makecell{2D+time volume of slice $z_i$. \\ Input for \emph{mJ-Net} (Sec. \ref{25dmj}).} \\ \hline
$\widehat{V}^{t_j}_{z_\mathcal{I}} = \varphi(\widetilde{V}^{t_j}_{z_{i-1}},\widetilde{V}^{t_j}_{z_i},\widetilde{V}^{t_j}_{z_{i+1}}) \in \mathbb{R}^{(X \times Y \times Z)}$ & \makecell{3D volume of slices $z_\mathcal{I}$ \\ for a time point $t_j$.} & $\bar{V}^{t}_{z_\mathcal{I}} = [\widehat{V}^{t_j}_{z_\mathcal{I}} | \forall t_j \in t] \in \mathbb{R}^{(X \times Y \times Z)}$ &\makecell{ List of 3D volumes of slices $z_\mathcal{I}$ \\ for all the time points $t$. \\ Input for \emph{3D-TCN} (Sec. \ref{3dtcn}).} \\ \hline
$\bar{V}_{z_\mathcal{I}}^t = [\widehat{V}_{z_{i-1}}^t,\widehat{V}_{z_i}^t,\widehat{V}_{z_{i+1}}^t] \in \mathbb{R}^{(X \times Y \times Z \times T)}$ & \makecell{ List of 2D+time volumes \\ for slices $z_\mathcal{I}$. Input \\ for \emph{3D+time mJ-Net} (Sec. \ref{3dmj}). } &$\widehat{V}^t_{z_\mathcal{I}} = \varphi(\widehat{V}_{z_{i-1}}^t,\widehat{V}_{z_i}^t,\widehat{V}_{z_{i+1}}^t) \in \mathbb{R}^{(X \times Y \times Z \times T)}$ & \makecell{4D Tensor of slices $z_\mathcal{I}$ \\ over all the time points $t$. \\ Input for \emph{4D mJ-Net} (Sec. \ref{4dmj}).} \\ \hline
\end{tabular}
}
\end{table*}

Table \ref{tab:notation} presents the various formal notations adopted in the remainder of the paper.
Let the data obtained from a CTP scan be defined as a 4D tensor $V \in \mathbb{R}^{(X \times Y \times Z \times T)}$.
After a series of pre-processing steps (details in Sec. \ref{sec:preproc}), we define the 4D tensor as $\widetilde{V} \in \mathbb{R}^{(X \times Y \times Z \times T)}$.
The four dimensions of a CTP scan are defined as width ($X$), height ($Y$), depth ($Z$), and time ($T$).
The list of time points in the time dimension is given by $t = [t_j | \forall j \in \{1,2,\cdots, t_{\text{max}}\}]$, where $t_{\text{max}}$ is the last time point of the list.
We indicate how the time dimension is adopted in the various inputs via the notation superscript.
Furthermore, we define $z = [z_i | \forall i \in \{1,2,\cdots, z_{\text{max}}\}]$ as the list of brain slices in the depth dimension, where $z_{\text{max}}$ corresponds to the last slice.
We illustrate how the depth dimension is being used in the inputs through the notation subscript.
Fig. \ref{fig:inputs} displays the input combination of all the techniques.

All methods return a 3D output, segmenting the images $P_{z_i}$ slice-by-slice.
The segmented 2D image $P_{z_i}$ corresponds to a brain slice $z_i$ at index $i$.
The predicted image $P_{z_i}$ contains brain tissue segmented with the classes $\mathcal{C}$ (if any): healthy brain, penumbra, and core.

\subsection{Pre-processing steps}\label{sec:preproc}

\begin{algorithm}[t]
\caption{Pre-processing steps for one single patient study} \label{alg}
\begin{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE 4D CTP scan: $V(x,y,z,t)$
\STATE $\text{ref} \gets V(x,y,z,t_1)_{\text{HU}}$ \COMMENT{Get the $1^{\text{st}}$ time point image as frame of reference}
\FOR {$j=2$ to $t_\text{max}$}
\STATE Co-register $V(x,y,z,t_j)$ using $\text{ref}$ as frame of reference.
\ENDFOR
\STATE $V(x,y,z,t)_{\text{HU}} \gets $ ConvertToHU($V(x,y,z,t)$)
\STATE $\dot{V}(x,y,z,t) \gets $ BrainExtraction($V(x,y,z,t)_{\text{HU}}$) \COMMENT{The brain extraction function is designed by \citet{najm2019automated}}
\STATE $\ddot{V}(x,y,z,t) \gets $ GammaCorrection($\dot{V}(x,y,z,t)$)
\STATE $z_{\text{high}} \gets $ GetSliceWithHighestIntensityValue($\ddot{V}(x,y,z,t)$)
\STATE $\text{bins} \gets 2^{16}$
\STATE $\widetilde{V}(x,y,z_{\text{high}},t), \mathcal{T}_{z_{\text{high}}} \gets $ HistEq($\ddot{V}(x,y,z_{\text{high}},t),\text{bins}$) \COMMENT{$\mathcal{T}_{z_{\text{high}}}$ is the grayscale transformation for $z_{\text{high}}$}
%\STATE $\text{maxVal} \gets $ Max($\widehat{V}(x,y,z_{\text{high}},t)$)
%\STATE $\text{minVal} \gets $ Min($\widehat{V}(x,y,z_{\text{high}},t)$)
%\STATE $\widehat{V}(x,y,z_{\text{high}},t) \gets $ Normalize($\widehat{V}(x,y,z_{\text{high}},t),\text{maxVal},\text{minVal}$)
\STATE $\widetilde{V}(x,y,z_{\text{high}},t) \gets \frac{\widetilde{V}(x,y,z_{\text{high}},t) - \text{mean}(\widetilde{V}(x,y,z_{\text{high}},t))}{\sigma(\widetilde{V}(x,y,z_{\text{high}},t))}$ \COMMENT{Standardization of the data}
\FOR {$i=1$ to $z_\text{max}$}
\IF {$i \neq z_{\text{high}}$}
\STATE $\widetilde{V}(x,y,z_i,t) \gets $ HistEq($\ddot{V}(x,y,z_i,t), \mathcal{T}_{z_{\text{high}}}$)
\STATE $\widetilde{V}(x,y,z_i,t) \gets \frac{\widetilde{V}(x,y,z_i,t) - \text{mean}(\widetilde{V}(x,y,z_i,t))}{\sigma(\widetilde{V}(x,y,z_i,t))}$
\ENDIF
\ENDFOR
\RETURN Processed 4D CTP scan: $\widetilde{V}(x,y,z,t)$
\end{algorithmic}
\end{algorithm}

The 4D CTP dataset underwent a series of pre-processing steps to extract brain tissue from the raw CTP scans.
The raw CTP studies are saved as DICOM files.
The pre-processing steps (details in Algorithm \ref{alg}) can be summarized as follow: 
\begin{enumerate}
    \item Co-registration of all the images in the 4D CTP scan using the first time point image as the frame of reference.
    \item All the registered CTP scans were converted into Hounsfield unit (HU) values to have a known quantitative scale to describe radiodensity efficiently.
    To calculate the HU value for a voxel $V$ with a rescale slope (RS) and a rescale intercept (RI) extracted from the DICOM header: $V(x,y,z,t)_{\text{HU}} = V(x,y,z,t) \cdot \text{RS} + \text{RI}$  % with an average linear attenuation coefficient $\mu$:
    % $$
    % V(x,y,z,t)_{\text{HU}} = 1000 \cdot \frac{\mu - \mu_{\text{water}}}{\mu_{\text{water}} - \mu_{\text{air}}}
    % $$
    \item Brain extraction of CT studies plays an essential role in stroke imaging research \citep{smith2002fast,najm2019automated}. An automatic brain extraction method designed by \citet{najm2019automated} was selected for this purpose due to its proven efficiency with CT datasets and public availability.
    \item Gamma correction and histogram equalization were also performed after step 3.
    \item Finally, standardization on the enhanced 4D tensor is applied to normally distribute the data. 
\end{enumerate}

The input for all the methods (except for the \emph{Multi-input PMs}'s approach) follows the same pre-processing steps.
These steps were performed to improve the quality of the images by enhancing the contrast.


\subsection{Existing methods}\label{sec:exmet}

\subsubsection{Approach 1: Multi-input PMs}\label{multipms}
The \emph{Multi-input PMs} model was proposed in \citep{tomasetti2022multi}.
The input for the architecture is a list of PMs for each brain slice $z_i$:
$\text{PMs}_{z_i} = [\text{CBF}_{z_i}, \text{CBV}_{z_i}, \text{TMAX}_{z_i}, \text{TTP}_{z_i}, \text{MIP}_{z_i}]$.
An example of the input for the model is given in Fig. \ref{fig:inputs}.

\subsubsection{Approach 2: mJ-Net}\label{25dmj}
The \emph{mJ-Net} approach was proposed in \citep{tomasetti2020cnn}.
The input for \emph{mJ-Net} is a 2D+time volume of the same brain slice $z_i$ at index $i$ over all the time points $t$: $\widehat{V}_{z_i}^t (x,y) =\varphi(\tilde{I}_{z_i}^{t_j}(x,y) | \forall t_j \in t)$.
We define the dimension of this input as 2D+time; The first dimension of the input is time.
The input $\widehat{V}_{z_i}^t$ can be seen as a series of concatenated images $\tilde{I}_{z_i}^{t_j}$ of the same brain slice $z_i$ over the totality of the time points $t_j \in t$.
An example of the input for the model is given in Fig. \ref{fig:inputs}.

The loss function used for this method is the soft Dice Coefficient loss (SDCL) \citep{milletari2016v}.
The SDCL is a modified version of the Dice Coefficient score mainly used in medical domains where the classes to predict are highly unbalanced due to a small region of interest compared to the background of the scans.
The SDCL can be written as:
\begin{align*}
\text{SDCL}(x,y) = \sum_c^{\mathcal{C}} \left(1 - \frac{2 \sum_i^{M\times N} x_{i,c} y_{i,c}}{\sum_i^{M\times N} x_{i,c}^2 + \sum_i^{M\times N} y_{i,c}^2}\right)
\end{align*}
where ${M\times N}$ is the number of pixels of the image, while $x_{i,c}$ and $y_{i,c}$ are the $i$th predicted pixel and the corresponding ground truth pixel for a class $c$.
The first section of the model contains 3D-Conv layers to extract information from the temporal dimension, while the second part follows the classic U-Net structure \citep{ronneberger2015u}.
For more details about the \emph{mJ-Net} approach, see \citep{tomasetti2020cnn}.

\subsubsection{Approach 3: 2D Temporal Convolutional Network}\label{tcn}

\begin{figure}[h!]
\centering
\begin{minipage}[b]{\columnwidth}
\centering
\centerline{\includegraphics[width=\columnwidth]{images/2D_TCN.pdf}}
\subcaption{\emph{2D-TCN} architecture proposed by \citet{amador2021stroke}.}
\label{fig:tcn:2d}
\end{minipage}
\begin{minipage}[b]{\columnwidth}
\centering
\centerline{\includegraphics[width=\columnwidth]{images/3D_TCN_SE.pdf}}
\subcaption{\emph{3D-TCN-SE} proposed by \citet{amador2022predicting}}
\label{fig:tcn:3d}
\end{minipage}
\caption{Visual comparison between (a) the \emph{2D-TCN} architecture proposed by \citet{amador2021stroke}, and (b) the \emph{3D-TCN-SE} implemented by \citet{amador2022predicting}.}
\label{fig:tcn}
\end{figure}

For comparison reasons, we implemented the method proposed by \citet{amador2021stroke}.
A general overview of the architecture can be seen in Fig. \ref{fig:tcn:2d}.
They proposed a Temporal Convolutional Network (TCN), which has been shown to outperform conventional neural networks in different tasks \citep{bai2018empirical}.
Moreover, a TCN has a lower memory requirement for training than other Recurrent Neural Networks \citep{bai2018empirical}.
We call this architecture \emph{2D-TCN} in the remainder of the paper.

The \emph{2D-TCN} was trained with the exact implementation as the original work: Adam as the optimizer \citep{kingma2014adam}, a batch size of 1, and a step-based learning rate decay.
The \emph{2D-TCN} model receives the 4D CTP scan in input.
The 4D input is processed as a list of 2D brain slices $z_i$ for each time point $t$.
Thus, the actual input for the \emph{2D-TCN} is a list $\bar{V}^{t}_{z_i} = [ \tilde{I}_{z_i}^{t_j} | \forall t_j \in t ]$ of 2D images $\tilde{I}_{z_i}^{t_j}$ over the same brain slice $z_i$ at index $i$ for a specific time point $t_j$.
%Each 2D image in the list $\bar{V}^t_{z_i}(x,y)$ represents the same brain slice $z_i$ over a distinct time point.
The list $\bar{V}^{t}_{z_i}$ contains all the time points of the brain slice $z_i$.
An example of the input for the model is given in Fig. \ref{fig:inputs}.
Every 2D input image of the list $ \tilde{I}_{z_i}^{t_j}$ at time point $t_j$ is fed to a 2D encoder $E^{t_j}_{\text{2D-TCN}}$ to extract features in the latent space.
Each $E^{t_j}_{\text{2D-TCN}}$ encoder returns a ($4\times4\times \text{Ch}$) feature vector, where $\text{Ch}$ corresponds to the number of channels.
The architecture merges the low-level feature vectors across the different $t_j$ time points to capture the spatio-temporal information.
The merged feature vector $\text{ETOT}_{\text{2D-TCN}} = [E^{t_j}_{\text{2D-TCN}} | \forall t_j \in t]$ is used as input to the TCN, which yields a one-dimensional vector $O_{\text{2D-TCN}}$ of 64 elements.
Finally, a decoder takes the $O_{\text{2D-TCN}}$ and generates a final 2D image $P_{z_i}(x,y)$.
%As the loss function, we implemented the Dice Coefficient loss (DCL), the same proposed in the original paper.
%The loss can be seen as:
The Dice Coefficient loss (DCL), same as the original paper, was implemented as the loss function as follows: 
$$
\text{DCL}(x,y) = \sum_c^\mathcal{C} (1 - D(x_c,y_c))
$$
where $x_c$ and $y_c$ are the predicted segmentation and the ground truth images for a specific class $c$, respectively.
For more details about the \emph{2D-TCN} approach, see \citep{amador2021stroke}.

\subsection{Approach 4: 3D Temporal Convolutional Network Single Encoder}\label{sec:3dtcnse}
We implemented a similar method from \citet{amador2022predicting}. We call this approach \emph{3D-TCN-SE} due to using a single encoder (SE).
Fig. \ref{fig:tcn:3d} shows a simplified version of the proposed architecture, emphasizing the input difference between the \emph{2D-TCN} and this model.
The SDCL is used as the loss function of the models.

The input for the \emph{3D-TCN-SE} is a list of $t$ 3D volumes $\bar{V}^{t}_{z_\mathcal{I}} = [\widehat{V}^{t_j}_{z_\mathcal{I}} | \forall t_j \in t] $, as shown in Fig. \ref{fig:inputs}.
Each 3D input volume in the list $\widehat{V}^{t_j}_{z_\mathcal{I}}(x,y)$ corresponds to the concatenation of the $i$th brain slice $z_i$ plus its neighbouring slices $z_{i-1}$ and $z_{i+1}$ over a specific time point $t_j$.
The \emph{3D-TCN-SE} approach uses a single encoder $E_{\text{3D-TCN}}$ for all the elements in the input list. 
Moreover, the \emph{3D-TCN-SE} model is trained with the entire brain images for comparison reasons and not with just the ipsilateral hemisphere, as in the original paper.

\subsection{Convolution in many dimensions}\label{4dconv}

In the reminder of this section, let define $I(x,y,z,t) \in \mathbb{R}^4, \mathcal{H}(w,h,d,p) \in \mathbb{R}^4$ as a 4D tensor and a 4D kernel, respectively. 
The $x$ and $w$ indicate the width of the 4D structures; $y$ and $h$ express the height dimension; $z$ and $d$ define the depth dimension, while $t$ and $p$ represent the time dimension of the 4D structures.

Like a 3D Convolution (3D-Conv) can be represented as the sum of multiple 2D Convolution (2D-Conv) along the depth dimension, a 4D Convolution (4D-Conv) operation can be represented as the sum of multiple 3D-Conv along the temporal dimension \citep{myronenko20194d}.

Let define a 2D-Conv $g''(x,y)$, where a 2D input image $I(x,y) \in \mathbb{R}^2$ is convolved with a 2D kernel $\mathcal{H}(w,h) \in \mathbb{R}^2$ as:
\begin{align*}
g''(x,y) &= \mathcal{H}(w,h) \circledast I(x,y) \\
&= \sum_{i=0}^{w-1} \sum_{j=0}^{h-1} \mathcal{H}(i,j) I(x+\widetilde{w}-i,y+\widetilde{h}-j)
\end{align*}
where $\circledast$ is the convolution operation.
$\widetilde{h} \equiv \lfloor \frac{1}{2}(h-1) \rfloor,\widetilde{w} \equiv \lfloor \frac{1}{2}(w-1) \rfloor$ correspond to the half-width, and half-height of the kernel $\mathcal{H}$.

Thereafter, let us define a 2D-Conv $g''(x,y,z)$ with a 2D kernel $\mathcal{H}(w,h) \in \mathbb{R}^2$ and a 3D input $I(x,y,z) \in \mathbb{R}^3$. Since the convolution operation is performed slice by slice over the third dimension, the 3D input can be seen as a list of 2D input $I(x,y,z) = \{ I(x,y,z_m) | \forall m \in \{1, \dots, z_\text{max}\} \}$, where $I(x,y,z_m) \in \mathbb{R}^2$ are the coordinates $(x,y)$ at slice $z_m$:
\begin{align*}
g''(x,y,z) &= \mathcal{H}(w,h) \circledast I(x,y,z)  \\
g''(x,y,z_m) &= \sum_{i=0}^{w-1} \sum_{j=0}^{h-1} \mathcal{H}(i,j) I(\tilde{x},\tilde{y}, z_m) \; \forall m \in \{1, \dots, z_\text{max}\}
\end{align*}
where $ \tilde{x} \equiv x+\widetilde{w}-i$ and $\tilde{y} = y+\widetilde{h}-j$.
Fig. \ref{fig:convex:2d} presents a visual representation of the 2D-Conv $g''(x,y,z)$ with a 3D input and a 2D kernel.

% \begin{figure}[ht!]
% \centering
% \centerline{\includegraphics[width=\linewidth]{images/convexample.pdf}}
% \caption{Example of a 2D-Conv with a 3D input and a 2D kernel.
% No padding is applied for this example.}
% \label{fig:something}
% \end{figure}

\begin{figure}[h!]
\begin{minipage}[b]{.48\columnwidth}
\centerline{\includegraphics[width=\columnwidth]{images/convexample_2d.pdf}}
\subcaption{2D-Conv with a 3D input and a 2D kernel}
\label{fig:convex:2d}
\end{minipage}
\begin{minipage}[b]{.48\columnwidth}
\centerline{\includegraphics[width=\columnwidth]{images/convexample_3d.pdf}}
\subcaption{3D-Conv with a 3D input and a 3D kernel}
\label{fig:convex:3d}
\end{minipage}
\caption{Visual examples of (a) 2D-Conv and (b) 3D-Conv. Both examples utilize a 3D input, and no padding is applied.
Each box corresponds to a pixel value.}
\label{fig:convex}
\end{figure}

Furthermore, let define a 3D-Conv operation $g'''(x,y,z)$ of a 3D kernel $\mathcal{H}(w,h,d) \in \mathbb{R}^3$ and a 3D input volume $I(x,y,z) \in \mathbb{R}^3$ as:
\begin{align*}
g'''(x,y,z) = \mathcal{H}(w,h,d) \circledast I(x,y,z) \\
= \sum_{k=0}^{d-1} \sum_{i=0}^{w-1}\sum_{j=0}^{h-1} \mathcal{H}(i,j,k) I(x+\widetilde{w}-i, y+\widetilde{h}-j, z+\widetilde{d}-k)
\end{align*}
where $\widetilde{d} \equiv \lfloor \frac{1}{2}(d-1) \rfloor$ is the half-depth of $\mathcal{H}$. Fig. \ref{fig:convex:3d} displays an example of a 3D-Conv with a 3D input and 3D kernel. 
If we define the 3D input $I(x,y,z)$ as before, then the output of a 3D-Conv $g'''(x,y,z)$ can be rewritten as the sum of multiple 2D-Conv operations over the third dimension:
\begin{align*}
g'''(x,y,z) &= \mathcal{H}(w,h,d) \circledast I(x,y,z) \\
&= \sum_{k=0}^{d-1} \mathcal{H}(w,h,k)  I(x,y,z+\widetilde{d}-k) \\
&= \sum_{k=0}^{d-1} g''(x,y,z+\widetilde{d}-k)
\end{align*}
where $\mathcal{H}(w,h,k) \in \mathbb{R}^2$ is a 2D kernel at index $k$, while $I(x,y,z+\widetilde{d}-k) \in \mathbb{R}^2$ is a 2D image at slice $z+\widetilde{d}-k$ where $[z+\widetilde{d}-k \in z_m | \forall m \in \{1, \dots, z_\text{max}\}]$.
The output dimension is defined as: $\text{dim}(g''') = \text{dim}(I) - \text{dim}(\mathcal{H}) + 1$; thus, if the $\text{dim}(I) \equiv \text{dim}(\mathcal{H})$, then $\text{dim}(g''') = 1$.

Moreover, let define a 3D-Conv operation $g'''(x,y,z,t)$ of a 3D kernel $\mathcal{H}(w,h,d) \in \mathbb{R}^3$ with a 4D input tensor $I(x,y,z,t) \in \mathbb{R}^4$. The 4D tensor $I(x,y,z,t)$ can be seen as a list of $I(x,y,z,t) = [ I(x,y,z,t_n)  | \forall n \in \{1, \dots, t_\text{max}\} ]$, where each element in the list corresponds to the coordinates $(x,y,z)$ of a $t_n$ element in the temporal dimension. 
Then, the 3D-Conv operation $g'''(x,y,z,t)$ can be seen as: 
\begin{align*}
g'''(x,y,z,t) = \mathcal{H}(w,h,d) \circledast I(x,y,z,t) \\
= \sum_{k=0}^{d-1} \sum_{i=0}^{h-1} \sum_{j=0}^{w-1} \mathcal{H}(i,j,k) I(\tilde{x},\tilde{y}, \tilde{z}, t_n) \; \forall n \in \{1, \dots, t_\text{max}\}
\end{align*}
where $ \tilde{x} \equiv x+\widetilde{w}-i$, $\tilde{y} = y+\widetilde{h}-j$, and $\tilde{z} \equiv z+\widetilde{d}-k$.

If we use a 2D+time kernel $\mathcal{H}(w,h,p) \in \mathbb{R}^3$ and we define $I(x,y,z,t)$ as a list of $I(x,y,z,t) = [ I(x,y,z_m,t) | \forall m \in \{1, \dots, z_\text{max}\} ]$, where each element in the list corresponds to the coordinates $(x,y,t)$ of a slice $z_m$, then the 3D-Conv operation $g'''(x,y,z,t)$ can be rewritten as:
\begin{align*}
g'''(x,y,z,t) &= \mathcal{H}(w,h,p) \circledast I(x,y,z,t) \\
g'''(x,y,z_m,t) &= \mathcal{H}(w,h,p) \circledast I(x,y,z_m,t) \; \forall m \in \{1, \dots, z_\text{max}\} \\
&= \sum_{l=0}^{p-1} \mathcal{H}(w,h,l) I(x,y,z_m,t+\widetilde{p}-l) \; \forall m \in \{1, \dots, z_\text{max}\}
\end{align*}
where $\widetilde{p} \equiv \lfloor \frac{1}{2}(p-1) \rfloor$ is the temporal dimension of the kernel $\mathcal{H}$ halved.

A 4D-Conv $g''''(x,y,z,t)$ of a 4D input $I(x,y,z,t) \in \mathbb{R}^4$ and a 4D kernel $\mathcal{H}(w,h,d,p) \in \mathbb{R}^4$ can be defined as:
\begin{align*}
g''''(x,y,z,t) = \mathcal{H}(w,h,d,p) \circledast I(x,y,z,t) \\
= \sum_{l=0}^{p-1} \sum_{k=0}^{d-1} \sum_{i=0}^{w-1}\sum_{j=0}^{h-1} \mathcal{H}(i,j,k,l) I(\tilde{x},\tilde{y},\tilde{z},\tilde{t})
\end{align*}
 where $ \tilde{x} \equiv x+\widetilde{w}-i$, $\tilde{y} = y+\widetilde{h}-j$, $\tilde{z} \equiv z+\widetilde{d}-k$, and $\tilde{t} \equiv t+\widetilde{p}-l$.

Finally, in a similar way, we define a 4D-Conv $g''''(x,y,z,t)$ and a 4D kernel $\mathcal{H}(w,h,d,p)$ as the sum of multiple 3D-Conv over a specific dimension, i.e., the third dimension.
The 4D kernel $\mathcal{H}(w,h,d,p)$ can be seen as a list of $\mathcal{H}(w,h,k,p) | \forall k \in d-1$, where $\mathcal{H}(w,h,k,p)$ is a 2D+time volume of the $k$th slice over the entire $p$ elements in the temporal dimension:
\begin{align*}
g''''(x,y,z,t) &= H(w,h,d,p) \circledast I(x,y,z,t) \\
&= \sum_{k=0}^{d-1}  \mathcal{H}(w,h,k,p) I(x,y,z+\widetilde{d}-k,t) \\
&= \sum_{k=0}^{d-1} g'''(x,y,z+\widetilde{d}-k,t) \\
\end{align*}
where $I(x,y,z+\widetilde{d}-k,t) \in \mathbb{R}^3$ is a 2D+time volume at slice $z+\widetilde{d}-k$ over the totality $t$ element in the temporal dimension, and $[z+\widetilde{d}-k \in z_m | \forall m \in \{1, \dots, z_\text{max}\}]$

\begin{figure}[h!]
\centering
\centerline{\includegraphics[width=\linewidth]{images/details_Conv4D.pdf}}
\caption{Visual representation of a 4D-Conv layer.
The input is $\widehat{V}^t_{z_\mathcal{I}} = \varphi(\widehat{V}_{z_{i-1}}^t,\widehat{V}_{z_i}^t,\widehat{V}_{z_{i+1}}^t) \in \mathbb{R}^{(X \times Y \times Z \times T)}$.
A series of 3D-Conv operations are calculated over a 4D input.
Several groups $(G_{i-1}, G_i, G_{i+1})$ are generated, one for each volume involved in the operation.
}
\label{fig:4Ddetails}
\end{figure}

Fig. \ref{fig:4Ddetails} presents an overview of the proposed 4D-Conv layer.
The input for our 4D-Conv layer is a 4D tensor $\widehat{V}^t_{z_\mathcal{I}}$: the 2D+time volume of the $i$th brain slice $\widehat{V}^t_{z_i}$ over all the time points $t$, and the two 2D+time volumes of the neighboring brain slices ($\widehat{V}^t_{z_{i-1}}$ and $\widehat{V}^t_{z_{i+1}}$).
The 4D-Conv layer generates three 3D-Conv groups $(G_{i-1}, G_i, G_{i+1})$, one for each volume slice involved in the operation.

In each group $G_i$, several 3D-Conv layers are created.
All convolution layers in each group shared the weights.
Each 3D-Conv layer is used for a single input volume.
The number of layers depends on the legal subscript indexes, i.e., for the group $G_{i-1}$, there are two 3D-Conv layers since the legal subscript indexes are $\{i-1,i\}$: the indexes are given by the current volume slice $z_{i-1}$ and the only neighboring volume slice $z_i$.
The output of each group is a set of feature volumes that are summed together.
The resulting feature volumes are stacked together to keep the same dimension as the input.


\section{Proposed 4D methods}\label{sec:propmet}
In this paper, we develop three novel deep learning (DL) approaches that accommodate 4D input data.
They are called \emph{3D-TCN}, \emph{3D+time mJ-Net}, and \emph{4D mJ-Net}.

All these methods use the entire 4D CTP scan as input; the main difference lies in how the 4D input is processed.
The \emph{3D-TCN} is based on a 2D Temporal Convolutional Network (\emph{2D-TCN}) \citep{amador2021stroke}, modified to receive a list of 3D input volumes.
The \emph{3D+time mJ-Net} inputs a list of 2D+time brain volumes from a CTP dataset, while the \emph{4D mJ-Net} uses the entire 4D structure of a CTP dataset as input.
Fig. \ref{fig:comparison} shows a comparison of these architectures with their respective inputs.

\begin{figure}[h!]
\centering
\begin{minipage}[b]{\columnwidth}
\centering
\centerline{\includegraphics[width=.6\columnwidth]{images/comparisontcn.pdf}}
\subcaption{\emph{3D-TCN} architecture (Sec. \ref{3dtcn})}
\label{fig:comparison:tcn}
\end{minipage}
\begin{minipage}[b]{\columnwidth}
\centering
\centerline{\includegraphics[width=.6\columnwidth]{images/comparison3d.pdf}}
\subcaption{\emph{3D+time mJ-Net} architecture (Sec. \ref{3dmj}}
\label{fig:comparison:3d}
\end{minipage}
\begin{minipage}[b]{\columnwidth}
\centering
\centerline{\includegraphics[width=.6\columnwidth]{images/comparison4d.pdf}}
\subcaption{\emph{4D mJ-Net} architecture (Sec. \ref{4dmj}}
\label{fig:comparison:4d}
\end{minipage}
\caption{Visual comparison between (a) the \emph{3D-TCN}, (b) the \emph{3D+time mJ-Net},  and (c) \emph{4D mJ-Net} architectures.}
\label{fig:comparison}
\end{figure}

\subsection{Approach 5: 3D Temporal Convolutional Network}\label{3dtcn}

We extend the architecture proposed by \citet{amador2021stroke} for our application to exploit further the information in the depth dimension.
In the remainder of the paper, we call our architecture \emph{3D-TCN}.
The main differences between the proposed \emph{3D-TCN} and the \emph{3D-TCN-SE} (Sec. \ref{sec:3dtcnse}) rely on the usage of a 3D encoder for each input element, instead of a 3D single encoder, plus the possibility to segment both core and penumbra regions, in comparison with segmenting only the core areas.
The SDCL is used as the loss function of the models.

The input for the \emph{3D-TCN} is the same as the input for the \emph{3D-TCN-SE} (details in Sec. \ref{sec:3dtcnse}).
As described in Sec. \ref{tcn}, the \emph{3D-TCN} architecture feeds each element of the input list $\widehat{V}^{t_j}_{z_\mathcal{I}}$ at time point $t_j$ to a specific 3D encoder $E^{t_j}_{\text{3D-TCN}}$ to extract low-level features.
Each $E^{t_j}_{\text{3D-TCN}}$ encoder returns a $(4\times4\times C)$ feature vector, where $C$ corresponds to the number of channels.
Each feature vector is merged to create a single input $\text{ETOT}_{\text{3D-TCN}} = [E^{t_j}_{\text{3D-TCN}} | \forall t_j \in t]$.
The $\text{ETOT}_{\text{3D-TCN}}$ is used in the TCN, which generates a one-dimensional vector $O_{\text{3D-TCN}}$ of 64 elements.
The TCN's output $O_{\text{3D-TCN}}$ is then given in input to the decoder to create the final predicted 2D image $P_{z_i}(x,y)$ of a brain slice $z_i$ at index $i$.

\subsection{Approach 6: 3D+time mJ-Net}\label{3dmj}

\begin{figure*}[h!]
\centering
\centerline{\includegraphics[width=.9\linewidth]{images/mJ-Net_3D.pdf}}
\caption{Illustration of the \emph{3D+time mJ-Net} model. The list of 2D+time input $\bar{V}^t_{z_\mathcal{I}}(x,y) = [\widehat{V}_{z_{i-1}}, \widehat{V}_{z_i}, \widehat{V}_{z_{i+1}}]$ is trained in parallel, where $z_\mathcal{I} = \{z_{i-1},z_i,z_{i+1}\}$.
The output is a 2D image $P_{z_i}(x,y)$.
}
\label{fig:3darch}
\end{figure*}

We propose a model, called \emph{3D+time mJ-Net}, which is an extension of the work of \citet{tomasetti2020cnn}.
The proposed model inputs a list of 2D+time matrices. 
Fig. \ref{fig:3darch} illustrates the model's architecture.
The dimension of this input is defined as 3D+time.
An example of the input for the model is given in Fig. \ref{fig:inputs}.
Each element of the input list coincides with a possible input for the \emph{mJ-Net} (details in Sec. \ref{25dmj}).
The input for the \emph{3D+time mJ-Net} is defined as $\bar{V}_{z_\mathcal{I}}^t (x,y) = [\widehat{V}_{z_{i-1}}^t,\widehat{V}_{z_i}^t,\widehat{V}_{z_{i+1}}^t]$, where the notation $(\bar{\cdot})$ describes a list of inputs, while the $\widehat{\cdot}$ describes a concatenated variable.
$\bar{V}_{z_\mathcal{I}}^t$ consists of a list of 2D+time volumes, where $z_\mathcal{I} = \{z_{i-1},z_i,z_{i+1}\}$ is a set of brain slices containing the $i$th slice $z_i$ analyzed and its neighboring slices $z_{i-1}$ and $z_{i+1}$.
In case that the index $i$ corresponds to the first (or last) brain slice, $\widehat{V}^t_{z_{i-1}}$ (and equivalently $\widehat{V}^t_{z_{i+1}}$) is set equal to $\widehat{V}^t_{z_{i}}$.

The SDCL is used as the loss function of the model.
Every 2D+time volume from the input list $\widehat{V}_{z_i}^t(x,y)$ is trained separately in the model through a series of encoders ($E_{z_{i-1}}, E_{z_{i}}, E_{z_{i+1}}$) composed of 3D-Conv and 2D-Conv layers.
Each section is independent of the other: there are no shared weights in the convolution layers.
The model's architecture is illustrated in Fig. \ref{fig:3darch}.

The convolution section of the \emph{3D+time mJ-Net}'s model uses a series of 3D-Conv layers to extract information from the inputs and reduce the time dimension.
Each block in this section contains two 3D-Conv layers with batch normalization layers, a max-pooling layer, and a dropout layer with a rate set to $50\%$.
After reducing the time dimension, four convolution blocks were constructed to reduce the width and height dimensions.
Each block incorporates two 2D-Conv layers with batch normalization and a max-pooling layer with a pool size of (1,2,2).
The final block of each section also contains a dropout layer with a rate set to $50\%$.
Lately, the three output sections have been concatenated before the deconvolution part.
In the decoder section, Attention layers \citep{oktay2018attention} and 2D Upsampling layers were implemented.
Attention layers benefit the architecture by focusing on target structures and help increase the segmentation performances. 
Each block of this section contains a concatenation layer for merging the output from each section in the convolution part together with the output of the previous block.
Moreover, two 2D-Conv layers with batch normalization were added to each block before an Attention layer plus a 2D Upsampling layer.

With the sole exception of the last convolution layer, each convolution layer uses a kernel of dimension 3 and a Leaky ReLU activation function \citep{graham2014spatially} with the parameter $\alpha=1/3$.
The first max-pooling layer of each block in the convolution section has a pool size of (2,1,1) to reduce the first dimension by a factor of 2.
The second max-pooling layer uses a pool size of (3,1,1), while the third max-pooling layer has a pool size of (5,1,1).
The selection of these pool sizes is due to reducing the time dimension.
The remaining max-pooling layers have a pool size of (2,1,1).
The Attention layers utilize a kernel of dimension 3 and a Leaky ReLU activation function.
The 2D Upsampling layers have an upsampling factor of 2.
The last convolution layer has a kernel of 1 and a Softmax activation function to produce a probability score for every class.


\subsection{Approach 7: 4D mJ-Net}\label{4dmj}

We propose another model called \emph{4D mJ-Net}.
We introduce this method to avoid the three distinct paths to process the 4D data from the previous architecture (Sec. \ref{3dmj}).
In order to limit the amount of input data fed to the model simultaneously, we still use a sliding window technique over the depth dimension, using three consecutive brain slices at a time.
Fig. \ref{fig:arch} represents an illustration of the model.
Like the \emph{3D+time mJ-Net} model, also this approach is an extension of the work of \citet{tomasetti2020cnn}.
The approach takes in input a 4D tensor $\widehat{V}^t_{z_\mathcal{I}} = \varphi(\widehat{V}_{z_{i-1}}^t,\widehat{V}_{z_i}^t,\widehat{V}_{z_{i+1}}^t)$.

The 4D input tensor $\widehat{V}^t_{z_\mathcal{I}}$ contains both the time dimension and the neighboring slices of the $i$th brain slice.
The $\varphi(\cdot)$ defines a concatenation function.
The $\widehat{V}^t_{z_\mathcal{I}}$ is a concatenation of a 2D+time volume $\widehat{V}^t_{z_{i}}$ of a brain slice $z_i$ at index $i$ over all the time points $t$ together with its neighbouring 2D+time volumes $\widehat{V}^t_{z_{i-1}},\widehat{V}^t_{z_{i+1}}$.
The \emph{4D mJ-Net} model can be considered an early-fusion approach since the 4D input tensors $\widehat{V}_{z_{i-1}}^t,\widehat{V}_{z_i}^t,\widehat{V}_{z_{i+1}}^t$ are concatenated before being fed to the encoder's model $E$.

\begin{figure*}[ht!]
\centering
\centerline{\includegraphics[width=\linewidth]{images/mJ-Net_4D.pdf}}
\caption{Illustration of the model architecture.
The 4D input $\widehat{V}^t_{z_\mathcal{I}} = \varphi(\widehat{V}_{z_{i-1}}^t,\widehat{V}_{z_i}^t,\widehat{V}_{z_{i+1}}^t)$ is the concatenation of a 2D+time volume $\widetilde{V}^t_{z_{i}}$ of a brain slice $z_i$ at index $i$ over all the time points $t$ plus its neighbouring brain slice volumes ($\widetilde{V}^t_{z_{i-1}},\widetilde{V}^t_{z_{i+1}}$).
}
\label{fig:arch}
\end{figure*}

The proposed \emph{4D mJ-Net} model is a combination of both \emph{3D+time mJ-Net} (Sec. \ref{3dmj}) and \emph{mJ-Net} (Sec. \ref{25dmj}).
The proposed approach uses the same input type that the \emph{3D+time mJ-Net} exploits.
However, rather than a list of 2D+time volumes, the model concatenates the input into a single 4D tensor of dimensions ($X \times Y \times Z \times T$).

Unlike 1D, 2D, and 3D Convolution layers, 4D Convolution layers are not available in public DL frameworks (i.e. Keras\footnote{\url{https://keras.io/}} or PyTorch\footnote{\url{https://pytorch.org/}}).
Thus, for this model, we implemented a novel 4D-Conv layer (details in Sec. \ref{4dconv}) which uses the convolutional layers defined in the public DL frameworks to replicate a 4D convolution operation.

The architecture of the \emph{4D mJ-Net} is displayed in Fig. \ref{fig:arch}.
The structure of the model follows the \emph{mJ-Net}: first, the architecture reduces the extra input dimensions (time and depth) and subsequentially analyzes the other dimensions.
Three blocks or 4D-Conv layers are implemented to evaluate and reduce the time dimension.
The last layer of each block diminishes by a factor equal to the stride value in the time dimension.
The stride values are 2, 3, and 5, respectively, for each block.
The output of the 4D-Conv layers is a tensor where the temporal dimension has been squeezed and reduced; information are extrapolated from the temporal dimension.
Thus the output resulting from the 4D-Conv layers contains only three dimensions ($X \times Y \times Z$) plus the channel dimension.
3D-Conv layers are implemented to reduce the depth dimension $Z$ and produce a 2D vector ($X \times Y$) plus the channel dimension.

The rest of the model's structure follows the classic U-Net architecture \citep{ronneberger2015u} with a series of 2D-Conv and Transpose layer blocks plus skip connection.
All 2D-Conv Transpose layers utilize a kernel with shape ($2\times2$).
Every max pooling layer, 3D-Conv, and 2D-Conv layers use the same parameters described in Sec. \ref{3dmj}.
Two MonteCarlo dropout layers \citep{gal2016dropout} are added at the end of the 4D and 2D Convolution blocks. The rate was set to $50\%$.
These layers were added to reduce uncertainties in the final predictions.
The last convolution layer has a kernel of ($1\times1$) and a Softmax activation function to produce a probability score for every class.

A weighted categorical cross-entropy loss \citep{van2019multiclass} was the loss function implemented for this method.
The loss can be written as:
\begin{align*}
\text{WCC}(x,y) = \sum_c^{\mathcal{C}} \sum_i^{M\times N} (y_{i,c} \log x_{i,c}) \cdot (w_{i,c} y_{i,c})
\end{align*}
where $w_{i,c}$ corresponds to the weight of the $i$th pixel for a class $c \in \mathcal{C}$; $x_{i,c}$ is the $i$th predicted pixel, and $y_{i,c}$ is the corresponding ground truth pixel.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Implementation details}\label{details}
All the methods mentioned in Sec. \ref{sec:exmet} and Sec. \ref{sec:propmet} utilize Adam as the optimizer \citep{kingma2014adam} with a learning rate of 0.0003 and a step-based decay rate of 0.95 every ten epochs.
The batch size is set to 1. 
An early stopping function  is called if there is no decrement in the validation loss after 25 epochs.
During training, L1 and L2 regularizations are applied in the kernels, plus a max norm constraint is also applied in the kernel and bias weights.
All experiments were implemented in Python using Keras (2.3.1) with Tensorflow as the backend and trained using an NVIDIA Tesla V100 GPU (32 GB memory).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments \& Results}\label{res}
We assess the proposed methods on a local dataset of CTP scans from 152 patients.
The patients were divided into LVO, Non-LVO, and WIS groups.
All experiments are performed with the same training set and evaluated over the validation set (details in Table \ref{tab:division}).
The holdout set is used only to make predictions with the best models with CTP scans that the methods have not seen before.
Since the Non-LVO group has smaller infarcted areas than the LVO patients, we set a higher penalty for every misclassification of penumbra and core classes for this sub-group during training.

\begin{table*}[]
\caption{ \textbf{Experiment results for the validation set}. Values in bold exhibit the best results for each column and each class.
Mean results plus standard deviation for Hausdorff Distance (HD) and $\Delta V$ are presented. Moreover, the Dice Coefficient (DC) is also presented.
Results are for the penumbra and core areas divided by the distinct patient groups (LVO, Non-LVO, WIS, and all).
Note that for the DC, higher values are better ($\Uparrow$), while for HD and $\Delta V$ lower values are preferable ($\Downarrow$).
}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|cccccccccc}
\Xhline{3\arrayrulewidth}
\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{DC} $\Uparrow$} & \multicolumn{3}{c|}{\textbf{HD} $\Downarrow$} & \multicolumn{4}{c}{\textbf{$\Delta V$ (ml)} $\Downarrow$} \\ \cline{2-11}
& \multicolumn{1}{c|}{\textbf{LVO}} & \multicolumn{1}{c|}{\textbf{Non-LVO}} & \multicolumn{1}{c|}{\textbf{All}} & \multicolumn{1}{c|}{\textbf{LVO}} & \multicolumn{1}{c|}{\textbf{Non-LVO}} & \multicolumn{1}{c|}{\textbf{All}} & \multicolumn{1}{c|}{\textbf{LVO}} & \multicolumn{1}{c|}{\textbf{Non-LVO}} & \multicolumn{1}{c|}{\textbf{WIS}} & \textbf{All} \\ \cline{2-11}
\Xhline{3\arrayrulewidth}
& \multicolumn{10}{c}{\textbf{Penumbra}} \\ \hline
\emph{Multi-input PMs} \citep{tomasetti2022multi} & \textbf{0.72} & 0.31 & 0.50 & \multicolumn{1}{|c}{\textbf{5.3$\pm$1.4}} & \textbf{2.3$\pm$1.4} & \textbf{3.7$\pm$2.2} & \multicolumn{1}{|c}{26.0$\pm$35.0} &6.5$\pm$14.0& 1.0$\pm$1.7 & \textbf{16.3$\pm$28.3} \\ \hline
\emph{mJ-Net} \citep{tomasetti2020cnn} & 0.67 & 0.48 & 0.65 & \multicolumn{1}{|c}{6.6$\pm$1.9} & 5.8$\pm$1.4 & 6.3$\pm$1.6 & \multicolumn{1}{|c}{\textbf{25.5$\pm$20.0}} & 24.7$\pm$29.2 & 45.5$\pm$39.1 & 27.2$\pm$26.0 \\ \hline
\emph{2D-TCN} \citep{amador2021stroke} & 0.13 & 0.03 & 0.10 &\multicolumn{1}{|c}{9.4$\pm$1.0} & 8.8$\pm$1.4 & 9.1$\pm$1.4 & \multicolumn{1}{|c}{81.3$\pm$65.6} & 80.6$\pm$57.8 & 131.6$\pm$93.1 & 86.0$\pm$66.5 \\ \hline
\emph{3D-TCN-SE} \citep{amador2022predicting} & 0.25 & 0.05 & 0.15 & \multicolumn{1}{|c}{14.1$\pm$1.1} & 15.3$\pm$0.94 & 14.8$\pm$1.3 & \multicolumn{1}{|c}{497.9$\pm$157.1} & 559.3$\pm$90.4 & 624.6$\pm$118.7 & 533.1$\pm$137.3\\ \hline 
\emph{3D-TCN} (Sec. \ref{3dtcn}) & 0.26 & 0.04 & 0.18 &\multicolumn{1}{|c}{9.8$\pm$0.9} & 10.1$\pm$1.2 & 9.9$\pm$2.5 & \multicolumn{1}{|c}{85.3$\pm$64.0} & 145.7$\pm$51.4 & 164.2$\pm$43.8 & 114.2$\pm$65.3 \\ \hline
\emph{3D+time mJ-Net} (Sec.\ref{3dmj}) &\textbf{ 0.72 } & 0.53 & \textbf{0.70} & \multicolumn{1}{|c}{6.0$\pm$1.3} & 4.3$\pm$1.9 & 5.0$\pm$2.0 & \multicolumn{1}{|c}{35.1$\pm$36.1} & 18.3$\pm$26.3 & 2.7$\pm$2.6 & 25.7$\pm$32.4 \\ \hline
\emph{4D mJ-Net} (Sec. \ref{4dmj}) & 0.68 & \textbf{0.62} & 0.67 &\multicolumn{1}{|c}{ \textbf{5.3$\pm$1.4}} & \multicolumn{1}{c}{2.9$\pm$1.5} & 3.9$\pm$2.2 & \multicolumn{1}{|c}{41.4$\pm$37.2} & \textbf{6.1$\pm$6.3} & \textbf{0.0$\pm$0.0} & 24.3$\pm$32.9\\ \hline
\Xhline{3\arrayrulewidth}
& \multicolumn{10}{c}{\textbf{Core}} \\ \hline
\emph{Multi-input PMs} \citep{tomasetti2022multi} & 0.37 & 0.21 & 0.27 & \multicolumn{1}{|c}{\textbf{2.4$\pm$1.7}} & \textbf{0.7$\pm$0.6} & \textbf{1.5$\pm$1.6}& \multicolumn{1}{|c}{\textbf{4.4$\pm$7.0}} & \textbf{0.5$\pm$0.7} & \textbf{0.0$\pm$0.0}& \textbf{2.5$\pm$5.5} \\ \hline
\emph{mJ-Net} \citep{tomasetti2020cnn} & \textbf{0.47} & 0.41 & \textbf{0.45} & \multicolumn{1}{|c}{3.3$\pm$1.5} & 1.9$\pm$0.8 & 2.6$\pm$1.5 & \multicolumn{1}{|c}{5.5$\pm$4.9} & 1.0$\pm$1.2 & 1.0$\pm$1.1 & 3.4$\pm$4.3 \\ \hline
\emph{2D-TCN} \citep{amador2021stroke} & 0.03 & 0.01 & 0.02 &\multicolumn{1}{|c}{4.3$\pm$1.6} & 3.4$\pm$1.3 & 3.8$\pm$1.6 & \multicolumn{1}{|c}{11.8$\pm$13.3} & 8.1$\pm$8.2 & 11.0$\pm$11.2 & 10.3$\pm$11.4 \\ \hline
\emph{3D-TCN-SE} \citep{amador2022predicting} & 0.0 & 0.0 & 0.0 &\multicolumn{1}{|c}{2.6$\pm$2.1} & 1.0$\pm$0.9 & 1.8$\pm$1.9 & \multicolumn{1}{|c}{12.7$\pm$15.6} & 1.9$\pm$2.8 & \textbf{0.0$\pm$0.0} & 7.8$\pm$12.8 \\ \hline 
\emph{3D-TCN} (Sec. \ref{3dtcn}) & 0.03 & 0.01& 0.03 & \multicolumn{1}{|c}{3.2$\pm$1.7} & 1.9$\pm$1.0 & 2.5$\pm$1.6 & \multicolumn{1}{|c}{12.0$\pm$14.3} & 1.9$\pm$2.1 & 2.4$\pm$1.9 & 7.3$\pm$11.6 \\ \hline
\emph{3D+time mJ-Net} (Sec.\ref{3dmj}) &0.46 & 0.35 &\textbf{0.45} & \multicolumn{1}{|c}{2.5$\pm$1.7} & 0.9$\pm$0.8 & 1.7$\pm$1.6 & \multicolumn{1}{|c}{8.1$\pm$10.6} & 1.3$\pm$1.6 & \textbf{0.0$\pm$0.0} & 4.8$\pm$8.5 \\ \hline
\emph{4D mJ-Net} (Sec. \ref{4dmj}) & 0.42 & \textbf{0.52} &0.42 & \multicolumn{1}{|c}{3.6$\pm$2.1} & 1.0$\pm$0.9 & 2.3$\pm$2.2 & \multicolumn{1}{|c}{25.9$\pm$37.0} & 1.4$\pm$2.2 & \textbf{0.0$\pm$0.0}& 14.3$\pm$29.6 \\ \hline
\end{tabular}}
\label{tab:fff}
\end{table*}


\subsection{Evaluation metrics}
Three evaluation metrics are used to assess the various experiments' models.
The Dice Coefficient ($\text{DC}$), the Hausdorff Distance (HD) \citep{birsan2005one}, and the absolute difference in the volumes ($\Delta V$).
We employ the DC to compare the model predictions with the ground truth segmentations.
The DC  between two segmentations $x$ and $y$ is given by the following equation:
\begin{align*}
\text{DC}(x,y) = 2\frac{|x \cap y|}{|x|+|y|} 
\end{align*}
where the range for the DC is $[0, 1]$; thus a $\text{DC}(x,y)=1$ corresponds to a perfect match between the prediction $x$ and ground truth $y$ segmentations.
The HD measures how two subsets are distant from each other.
The range value for the HD is $[0, \infty]$.

The absolute difference in the volumes $\Delta V$ between the prediction volume $V_x$ and the ground truth volume $V_y$ can be expressed as:
\begin{align*}
\Delta V(V_y,V_x) = |V_y- V_x|
\end{align*}
The range for $\Delta V$ is $[0, \infty]$, and $\Delta V(V_y,V_x) = 0$ represents a perfect match between the two volumes.
The $\Delta V(V_y, V_x)$ is an essential evaluation metric for the WIS group due to the lack of ground truth segmentations in this group.
The other metrics are not suitable for understanding how the predictions will be since the ground truth will always be empty.

The best scenario for a model is to produce high DC with low HD and $\Delta V$: this implies a strong correlation between the predicted areas and the ground truth regions.
If the results show high $\Delta V$ (or HD) with low DC, an over-segmentation of the ischemic areas is perceived.
On the other hand, promising outcomes of $\Delta V$ (or HD) with mediocre DC results might imply an under-segmentation of the predicted regions.

\subsection{Comparison with other methods}
The proposed \emph{3D+time mJ-Net}, \emph{4D mJ-Net}, and \emph{3D-TCN} methods are compared with alternative models: the \emph{2D-TCN} \citep{amador2021stroke}, the \emph{mJ-Net} \citep{tomasetti2020cnn}, the \emph{3D-TCN-SE} \citep{amador2022predicting}, and the \emph{Multi-input PMs} \citep{tomasetti2022multi}.

Table \ref{tab:fff} presents the results of the evaluation metrics over the validation set.
Results are presented for each group distinctly (LVO, Non-LVO, WIS, and all) to highlight the strengths and weaknesses of each model over the various groups composing the dataset.
An extensive number of experiments are performed for all the analyzed models.
However, to present a fair comparison among the various models, we only introduce the methods with a combination of parameters that yield the best results omitting the other combinations tested during experiments.
Qualitative comparison results of random brain slices extracted from the validation set are provided in Fig. \ref{fig:results}.

\begin{figure*}[h!]
\centering
\centerline{\includegraphics[width=\linewidth]{images/results.pdf}}
\caption{Qualitative comparisons for the tested models. The brain slices are taken randomly from distinct patients from the validation set, divided by group (left to right). Each row represents the results for each model involved in the study.}
\label{fig:results}
\end{figure*}


\subsection{Inter-observer variability}
Two expert neuroradiologists ($\text{NR}_1$, $\text{NR}_2$) manually annotated the scans of 33 randomly selected patients: 19 from the LVO group, 11 from the Non-LVO, and 3 from the WIS subset.
The manual annotation images were generated using the same criteria endorsed for creating the ground truth images, as explained in Sec. \ref{gt}.
An investigation of the inter-observer variability between $\text{NR}_1$, $\text{NR}_2$, and the two best proposed models is presented in Table \ref{tab:iovr}.
All evaluation results presented in Table \ref{tab:fff} and Table \ref{tab:iovr} are reported as mean $\pm$ standard deviation, with the only exception of the Dice Coefficient, which is an aggregation, for all patients, of the predicted pixels.


\begin{table*}[]
\caption{\textbf{Inter-observer variability results for test set}. Values are presented for the two best proposed architectures (\emph{3D+time mJ-Net}, \emph{4D mJ-Net}) and for two expert neuroradiologists ($\text{NR}_1$, $\text{NR}_2$) over the test set. 
Note that for the DC, higher values are better ($\Uparrow$), while for HD and $\Delta V$ lower values are preferable ($\Downarrow$).
}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|cccccccccc}
\Xhline{3\arrayrulewidth}
\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{DC} $\Uparrow$} & \multicolumn{3}{c|}{\textbf{HD} $\Downarrow$} & \multicolumn{4}{c}{\textbf{$\Delta V$ (ml)} $\Downarrow$} \\ \cline{2-11}
& \multicolumn{1}{c|}{\textbf{LVO}} & \multicolumn{1}{c|}{\textbf{Non-LVO}} & \multicolumn{1}{c|}{\textbf{All}} & \multicolumn{1}{c|}{\textbf{LVO}} & \multicolumn{1}{c|}{\textbf{Non-LVO}} & \multicolumn{1}{c|}{\textbf{All}} & \multicolumn{1}{c|}{\textbf{LVO}} & \multicolumn{1}{c|}{\textbf{Non-LVO}} & \multicolumn{1}{c|}{\textbf{WIS}} & \textbf{All} \\ \cline{2-11}
\Xhline{3\arrayrulewidth}
& \multicolumn{10}{c}{\textbf{Penumbra}} \\ \hline
\emph{3D+time mJ-Net} (Sec. \ref{3dmj}) & 0.71 & 0.52 & 0.70 &\multicolumn{1}{|c}{6.0$\pm$1.2} & 3.4$\pm$1.3 & 4.8$\pm$2.0 & \multicolumn{1}{|c}{36.7$\pm$36.4} & 6.5$\pm$4.9 & 10.1$\pm$3.1 & 24.2$\pm$31.1 \\ \hline
\emph{4D mJ-Net} (Sec. \ref{4dmj}) & 0.69 & 0.53 & 0.68 &\multicolumn{1}{|c}{5.5$\pm$1.2} & 2.1$\pm$1.1 & 3.9$\pm$2.2 & \multicolumn{1}{|c}{34.1$\pm$30.6} & \textbf{5.3$\pm$6.6} & \textbf{0.0$\pm$0.0} & 21.4$\pm$27.7 \\ \hline
$\text{NR}_1$ vs $\text{NR}_2$ & \textbf{0.80}& \textbf{0.67} & \textbf{0.79} & \multicolumn{1}{|c}{\textbf{5.1$\pm$1.0}} & \textbf{1.9$\pm$1.4} & \textbf{3.6$\pm$2.2} & \multicolumn{1}{|c}{\textbf{33.3$\pm$27.7}} & 5.5$\pm$9.2 & \textbf{0.0$\pm$0.0} & \textbf{21.0$\pm$25.9} \\ \hline
\Xhline{3\arrayrulewidth}
& \multicolumn{10}{c}{\textbf{Core}} \\ \hline
\emph{3D+time mJ-Net} (Sec. \ref{3dmj}) & 0.27 & 0.12 & 0.27 &\multicolumn{1}{|c}{3.7$\pm$1.9} & 0.6$\pm$1.0 & 2.3$\pm$2.2 & \multicolumn{1}{|c}{14.6$\pm$18.2} & 0.9$\pm$2.3 & \textbf{0.0$\pm$0.0} & 8.7$\pm$15.4 \\ \hline
\emph{4D mJ-Net} (Sec. \ref{4dmj}) & 0.35 & 0.30 & 0.35 &\multicolumn{1}{|c}{4.0$\pm$1.8} & 0.7$\pm$1.2 & 2.6$\pm$2.3 & \multicolumn{1}{|c}{21.2$\pm$31.7} & 1.8$\pm$4.5 & \textbf{0.0$\pm$0.0} & 12.8$\pm$25.9 \\ \hline
$\text{NR}_1$ vs $\text{NR}_2$ & \textbf{0.55} & \textbf{0.33} & \textbf{0.54} & \multicolumn{1}{|c}{\textbf{3.2$\pm$1.4}} & \textbf{0.5$\pm$0.9} & \textbf{2.0$\pm$1.8} & \multicolumn{1}{|c}{\textbf{5.6$\pm$4.3}} &\textbf{ 0.7$\pm$1.9} & \textbf{0.0$\pm$0.0} & \textbf{3.5$\pm$4.2} \\ \hline
\end{tabular}}
\label{tab:iovr}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}\label{disc}
Early detection and intervention in AIS patients are of vital importance \citep{advani2017golden, meretoja2014stroke, meretoja2017endovascular}, and mechanical thrombectomy plays a central role in patients with LVO as resistance to intravenous thrombolysis increases with more proximal occlusions.
In this study, we have proposed different architectures to utilize the 4D CTP input to use the spatio-temporal information better than in existing approaches.
We suggest to expand the \emph{mJ-Net} and show two ways of segmenting ischemic areas in patients suspected for AIS.
In addition, we expand another method (\emph{3D-TCN}) for comparison reasons.
We use the entire raw 4D CTP data and feed different combinations of it as input to our proposed approaches to prevent possible loss of spatio-temporal information. Studying the data as an independent volume and neglecting its spatio-temporal nature can lead to the loss of relevant information. All proposed approaches return a series of 2D segmented images as output, later stacked together to produce a 3D volume. Using 2D images as output is less computationally expensive and less memory intensive than using 3D volumetric data as output \citep{singh20203d}.

Few studies have adopted 4D datasets in DNN models to detect ischemic stroke \citep{soltanpour2022using, amador2021stroke, robben2020prediction, amador2022predicting}. This has roots in the high computational complexity of 4D data and the lack of ground truth for the whole set.
The limitations that these approaches encounter are as follows: 1) the quality of their ground truth images is debatable since they are acquired from follow-up DWI or NCCT studies; 2) datasets used for the training and evaluation take into account only a subset of the entire population; 3) segmentations are only performed on the core areas, excluding penumbra regions. Furthermore, in \citet{amador2022predicting}, the analyses were limited only to the ipsilateral hemisphere.

We include data from all patients, regardless of stroke severity, to train our models. Rather than entrusting ground truth images from follow-up DWI or NCCT studies that are usually taken 24 hours or several days after the onset of stroke, our proposed methods were trained with ground truth images obtained from the first captured CTP PMs and follow-up scans.
The ground truth includes manual annotations of the two ischemic areas, penumbra and core \citep{tomasetti2021machine}.


We use three evaluation metrics to assess the models' performances: DC, HD, and $\Delta V$ compared with our previously developed algorithms and other state-of-the-art algorithms. 
Results in Table \ref{tab:fff} demonstrate that increasing the input dimension benefits achieving more precise segmentation, especially for the Non-LVO and WIS groups, regardless of the class. Thus, when a smaller portion of the brain is affected, the whole dataset's usage helps achieve better segmentation results. 

Visual results of random validation brain slices are shown in Fig. \ref{fig:results}, where we can see that our proposed approaches (\emph{3D+time mJ-Net}, \emph{4D mJ-Net}) are less prone to over-segment, especially in the Non-LVO and WIS groups.
It is reported that LVO cases are less common compared to Non-LVO. On average, LVOs are estimated to represent around 30\% of all cases of AIS \citep{lakomkin2019prevalence, malhotra2017ischemic, dozois2017plumber}.
Thus, an architecture that can accurately segment patients in the Non-LVO group can be valuable in a real-life scenario. Nonetheless, patients with LVO represent a clinically significant proportion of patients presenting with AIS, especially considering the grim natural course of the disease.

%% Discussion for the three mJ-Net models 
The results presented in Table \ref{tab:fff} indicate that all \emph{mJ-Net} models have improved where the input data dimension has increased, regardless of the patients' group. 
Fig. \ref{fig:results} shows that using 2D+time input for the \emph{mJ-Net} \citep{tomasetti2020cnn} led to over-segmentation of penumbra class in separate brain tissue sections, brain slice 4-6.
The visual results for the Non-LVO and WIS groups highlight the limitations of this model: the over-segmentation of the penumbra regions might affect the usage in a real-life scenario, and an overestimation of the penumbra area can generate uncertainties for treatment decisions.

%% Discussion for 3D+time mJ-Net
Adding depth as an extra dimension to the input of models (\emph{3D+time mJ-Net} and \emph{4D mJ-Net}) determines an increment in the performances for both classes in the three patient groups.
A significant increase is noticeable for the DC metric in the Non-LVO group, regardless of the class.
An essential difference between these two architectures is how they exploit their structures' input.
The \emph{3D+time mJ-Net} is considered a late-fusion approach as the data sources are used independently and fused close to decision-making.
Statistical results presented for the \emph{3D+time mJ-Net} show promising general performances for the LVO group. However, an under-estimation of the core class, regardless of the patient group, can be noticed both from the visual results in Fig. \ref{fig:results} and the low HD metric.
The \emph{3D+time mJ-Net} can precisely segment ischemic regions with large areas, as shown by the first three brain slices in Fig. \ref{fig:results}.
This can also be evinced from the high DC score achieved for the LVO group for both classes (Table \ref{tab:fff}).

%% Discussion for 4D mJ-Net
The \emph{4D mJ-Net} model fuses the data before they are fed to the network. Visual results in Fig. \ref{fig:results} and values in Table \ref{tab:fff} indicate that the \emph{4D mJ-Net} model segments the penumbra class more precisely compared to the other approaches that use raw CTP as their input. This good performance follows in all patient groups. The \emph{4D mJ-Net} achieved the highest DC metric for both core and penumbra regions in patients with Non-LVO. 
The \emph{4D mJ-Net} showed high precision in detecting small ischemic areas, as shown in sample brain slices 3 to 5 in Fig. \ref{fig:results}.
Furthermore, the \emph{4D mJ-Net} model can correctly predict no ischemic regions in WIS patients, as demonstrated by the results for the $\Delta V$ in the WIS group. However, it over-segments the core class in patients with LVO. This means that including the complete spatio-temporal information of the data and following an early fusion approach leads to better prediction in Non-LVO and WIS groups, where small areas are of interest. 

%% Discussion for TCN
Models based on TCN in general showed poor results statistically in Table \ref{tab:fff} and visually in Fig. \ref{fig:results}.  They extremely over-segment the penumbra class and poorly segment the core class. 
The original \emph{2D-TCN} and \emph{3D-TCN-SE} were designed to segment only one class, the ischemic core. This can explain the poor performance of segmenting the two classes. 
Besides, the \emph{3D-TCN-SE} model in the original research is trained to use only the ipsilateral hemisphere. The model's training was done over both hemispheres for a fair comparison. This can be the cause of over-segmentation in penumbra regions.  


%% Discussion for the SF_G model (PMs as input)
The \emph{Multi-input PMs} model \citep{tomasetti2022multi} as the name indicates takes parametric maps, pre-processed data obtained form CTP scans. The experiment results of this model show a high DC value for the penumbra class in the LVO group, as also seen in the first three brain slices of Fig. \ref{fig:results}.
This highlights that this method presents satisfactory results for large ischemic areas.
However, when the region's volume is small or vacant, the predictions are not optimal: see brain slices 4 and 6 in Fig. \ref{fig:results}.
This approach gives the best HD and the $\Delta V$ metrics for the core and penumbra classes, regardless of the patient group. Although HD and $\Delta V$ values are encouraging, DC values show an under-segmentation in the core class and the penumbra class for the Non-LVO set. Using PMs derived from CTP scans limits the machine to only learn from specific pre-processed information. 


%% Test results discussion
The inter-observer variability results, highlighted in Table \ref{tab:iovr}, show encouraging outcomes for the proposed methods in relation to the results achieved by the two expert neuroradiologists ($\text{NR}_1$, $\text{NR}_2$).
Similar statistic values can be observed between $\text{NR}_1$ vs. $\text{NR}_2$ and the \emph{4D mJ-Net} for the core class in connection with the Non-LVO group.
The same results as the neuroradiologists were achieved by the \emph{4D mJ-Net} for the $\Delta V$ in the WIS group.
The proposed \emph{3D+time mJ-Net} model produces higher results for the DC compared to the \emph{4D mJ-Net} in association with the penumbra class. However, the results for the core regions are far from satisfactory.
The inter-observer variability outcomes for the HD and $\Delta V$ highlight substantial similarity among the proposed approaches and the neuroradiologists, except for the core class in connection with the Non-LVO group.
The difference can be due to an over-segmentation of this particular class, which can be highly complex to detect for the models considering its small size.

\subsection{Common limitations}
All the assessed approaches have faced general limitations. The images used during the training of each model are from CT scanners of the same vendor. This causes a lack of diversity in the data. 
The annotations used as ground truth surround the essential ischemic regions (penumbra and core), but they do not represent the areas perfectly. They might leave out small parts of the core spread into the penumbra tissue and parts of the penumbra misclassified as healthy brain tissue \citep{tomasetti2020cnn}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}
Fast and precise diagnosis and treatment are of vital importance in AIS patients.
In this paper, we proposed to use 4D CTP as input to extract spatio-temporal information for segmenting core and penumbra areas in patients with AIS. . This is proposed primarily by expanding the \emph{mJ-Net} in two different ways.
Furthermore, we introduced a novel 4D-Conv layer to exploit spatio-temporal information.
Two of our approaches achieved satisfactory results for all the classes involved.
We used the entire 4D CTP dataset of all patients and compared models using different input types. We demonstrated that relying only on images derived from the CTP scans (i.e., PMs) or on a restricted number of dimensions (i.e., 2D, 2D+time, 3D) limits the prediction accuracy in DNN-based approaches.
Moreover, we segmented both penumbra and core regions in ischemic brain tissue since an accurate and fast understanding of both is essential for medical doctors, especially during the first stages of the event.

Further studies with larger datasets, including images from different vendors and various acquisition parameters, are still needed to validate our methods. Due to complex and time-consuming work for manual annotations, using unsupervised neural networks is encouraged. 

\section*{Conflict of interest statement}
We declare that we do not have any commercial or associative interest that represents a conflict of interest in connection with the work
submitted.

\section*{Acknowledgements}
This study was supported and approved by the Regional ethic committee project 2012/1499.

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% For citations use:
%% \citet{<label>} ==> Jones et al. [21]
%% \citep{<label>} ==> [21]
%%

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
\bibliographystyle{elsarticle-num-names}
\bibliography{./strings.bib}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%%\begin{thebibliography}{00}

%% \bibitem[Author(year)]{label}
%% Text of bibliographic item

%%\bibitem[ ()]{}

%%\end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-num-names.tex'.
