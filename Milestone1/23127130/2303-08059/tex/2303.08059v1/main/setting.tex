\vspace{-0.55cm}
\section{Setting}
\label{sec:setting}

We consider a finite episodic \emph{reward-free} MDP $\cM = \left(\cS, \cA, H, \{p_h\}_{h\in[H]}, s_1\right)$, where $\cS$ is the set of states of size $S$, $\cA$ is the set of actions of size $A$, $H$ is the number of steps in one episode, $p_h(s'|s,a)$ is the probability transition from state~$s$ to state~$s'$ by performing action $a$ in step $h$. And $s_1$ is the fixed initial state.

\paragraph{Policy \& value functions} A general policy $\pi =(\psi_h)_{h\in[H]}$ is a collection of function $\psi_h:\, (\cS\times\cA\times[0,1])^{h-1}\times \cS\times[0,1] \to \cA$ that maps an history $I_h=(s_1,a_1,u_1,\ldots,s_{h-1},a_{h-1},u_{h-1})$ where $u_h$ are i.i.d. uniformly distributed on the unit interval, a state $s_h$ and an auxiliary independent uniformly distributed random variable $u_h$ to an action $a_h= \psi_h(I_h,s_h,u_h)$. A policy is Markovian if the action depends only on the previous state and the auxiliary noise $a_h=\psi_h(s_h,u_h)$. In this case the policy can be represented as $\pi=(\pi_h)_{h\in[H]}$ a collection of mappings from states to probability distributions over actions $\pi_h : \cS \to \simplex_\cA$ for all $h\in [H]$ where $a_h=\psi_h(s_h,u_h)\sim \pi_h(s_h)$.
% A \emph{stochastic} policy $\pi=(\pi_h)_{h\in[H]}$ is a collection of mappings from states to probability distributions over actions $\pi_h : \cS \to \simplex_\cA$ for all $h\in [H]$, where each $\pi_h$ maps each state to a distribution over actions.
Furthermore, $p_{h} f(s, a) \triangleq \E_{s' \sim p_h(\cdot | s, a)} \left[f(s')\right]$   denotes the expectation operator with respect to the transition probabilities $p_h$ and
$(\pi_h g)(s) \triangleq \pi_h g(s) \triangleq \E_{a \sim \pi_h(s)}[g(s,a)]$ denotes the composition with policy~$\pi$ at step $h$. Also, for any distribution over actions $\pi \in \simplex_\cA$ define $\pi g(s) \triangleq \E_{a \sim \pi}[g(s,a)]$.
% \todoPi{Check if we need that latter.}
% The value functions of $\pi$, denoted by $V_h^\pi$, as well as the optimal value functions, denoted by $\Vstar_h$ are given by the Bellman respectively  optimal Bellman equations
% \begin{small}
% \begin{align*}
% 	Q_h^{\pi}(s,a) &= r_h(s,a) + p_h V_{h+1}^\pi(s,a) & V_h^\pi(s) &= \pi_h Q_h^\pi (s)\\
%   Q_h^\star(s,a) &=  r_h(s,a) + p_h V_{h+1}^\star(s,a) & V_h^\star(s) &= \max_a Q_h^\star (s, a)
% \end{align*}
% \end{small}%
% \!where by definition, $V_{H+1}^\star \triangleq V_{H+1}^\pi \triangleq 0$. 



\paragraph{Visitation distribution} The \emph{state-action visitation distribution} of policy $\pi$ at step $h$ is denoted by $d_h^\pi$, where $d_h^\pi(s,a)$ is the probability of reaching the state-action pair $(s,a)$ at step $h$ after policy $\pi$.

\paragraph{Visitation polytopes} All the admissible collection of visitation distributions belong to the following polytope 
{\small
\begin{align*}
\cK_p &\triangleq \Big\{d=(d_h)_{h\in[H]}: \sum_{a\in\cA} d_1(s,a) = \ind\{s=s_1\}\ \forall s\in\cS\,   \\
\sum_{a\in\cA}& d_{h+1}(s,a) =\!\!\!\!\! \sum_{(s',a')\in\cS\times\cA}\!\!\!\!\! p_h(s|s',a') d_h(s',a')\ \forall s\in\cS, \forall h\geq 1\Big\}\,.
\end{align*}}
\!\!We also denote by $\cK$ the set of collections of probability distributions over state-action pairs without the constraint to be a valid visitation distribution, that is 
{\small
\begin{align*}
\cK \triangleq \Big\{d=(d_h)_{h\in[H]}&: d_h(s,a) \geq 0,\  \forall (h,s,a)\in[H]\times\cS\times\cA\,   \\
&\sum_{(s,a)\in\cS\times\cA} d_{h}(s,a) = 1,\ \forall h\in[H]\Big\}\,.
\end{align*}}

\paragraph{Trajectory distribution} We denote by {\small$\cT\triangleq (\cS\times\cA)^H = \big\{(s_1,a_1,\ldots,s_H,a_H)\!:\, (s_h,a_h)\in\cS\times\cA,\, \forall h\in[H]\big\}$} the set of all possible trajectories. The probability to generate the trajectory $m=(s_1,a_1,\ldots,s_H,a_H)$ with he policy $\pi$ is {\small$q^\pi(m) \triangleq \pi(a_1|s_1)\prod_{h=2}^H p_{h-1}(s_h|s_{h-1},a_{h-1}) \pi_h(a_h|s_h) $}. 
Note that the visitation distribution at step $h$ of policy $\pi$ is a marginal of the trajectory distribution $d_h^\pi(s,a) = \E_{(s_1,a_1,\ldots,s_H,a_H) \sim q^\pi} [\ind\{(s,a)=(s_h,a_h)\}]$.



\paragraph{Counts and empirical transition probability} the number of times the state action-pair $(s,a)$ was visited in step $h$ in the first $t$ episodes are $n_h^{t}(s,a) \triangleq  \sum_{i=1}^{t} \ind{\left\{(s_h^i,a_h^i) = (s,a)\right\}}$. Next, we define $n_h^{t}(s'|s,a) \triangleq \sum_{i=1}^{t} \ind{\big\{(s_h^i,a_h^i,s_{h+1}^i) = (s,a,s')\big\}}$ the number of transitions from $s$ to $s'$ at step $h$. %\todoRemi{Shall we write this $n_h^{t}(s,a,s')$ instead?}
The empirical distribution is defined as $\hp^{\,t}_h(s'|s,a) = n^{\,t}_h(s'|s,a) / n^{\,t}_h(s,a)$ if $n_h^t(s,a) >0$ and $\hp^{\,t}_h(s'|s,a) \triangleq 1/A$ for all $s'\in \cS$ else.

\paragraph{Additional notation} For $n\in\N_{+}$ we define the set $[n]\triangleq \{1,\ldots,n\}$. For $n\in\N_{+}$ we denote by $\Delta_n$ the probability simplex of dimension $n$. For elements $(p,q)\in\Delta_n$ the entropy of $p$ is denoted by $\cH(p) \triangleq -\sum_{i\in[n]} p_i\log p_i$ and the Kullback-Leibler divergence between $p$ and $q$ by $\KL(p,q) \triangleq \sum_{i\in[n]} p_i \log(p_i/q_i)$. For a number $x$ and any two number $m < M$ define $\clip(x, m, M) \triangleq \max(\min(x, M), m)$.
%The vector of dimension $N$ with all entries one is $\bOne^N \triangleq  (1,\ldots,1)$.  is an pseudo-empirical measure defined as $\up^{\,t}_h(s,a) \triangleq \upn^{\,t}_h(s'|s,a) / \upn^{\,t}_h(s,a)$ 




