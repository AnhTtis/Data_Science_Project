\section{Trajectory entropy}
\label{sec:trajectory_entropy}


In this section we focus on another type of entropy, the trajectory entropy, that can be efficiently maximized. The entropy of paths of a (Markovian) stochastic process is introduced by \citet{ekroot1993entropy}. It quantifies the randomness of realizations with fixed initial and final states. Later it was extended \citep{savas2019entropy}  to realizations that reach a certain set of states, rather than a fixed final state.   This type of entropy is also closely related to the so-called entropy rate of a stochastic process.

\paragraph{Trajectory entropy} We define the trajectory entropy of a policy $\pi$ as the entropy of a trajectory generated with the policy $\pi$ 
\[
\TE(q^\pi) \triangleq \cH(q^\pi) = \sum_{m\in\cT} q^\pi(m) \log\frac{1}{q^\pi(m)}\,.
\]

We denote by $\pistarTE\in\argmax_{\pi} \TE(q^\pi)$ a policy that maximizes the trajectory entropy. 

\paragraph{Maximum trajectory entropy exploration} MTEE differs from MVEE only in the choice of entropy. In particular an algorithm $((\pi^t)_{t\in\N_+},\tau,\hpi)$ for MTEE is also a combination of a time dependent policy $(\pi^t)_{t\in\N_+}$, a stopping rule $\tau$, and a decision rule $\hpi$. 
\begin{definition}
(PAC algortihm for MTEE) An algorithm $((\pi^t)_{t\in\N},\tau,\hpi)$ is $(\epsilon,\delta)$-PAC for MTEE if 
\[
\P\left( \TE\big(q^{\pistarTE}\big)- \TE(q^{\hpi}) \leq \epsilon \right) \leq 1-\delta\,.
\]
\end{definition}

\vspace{-0.25cm}
As noted by \citet{eysenbach2019if}, MTEE can  also be connected to a prediction game. In this game, the forecaster-player aims to predict the whole trajectory that the sampler-player will generate. Remark that predicting the trajectory implies to predict, in particular, the visited state-action pairs but the reverse is not true in general \footnote{Indeed $d_h^\pi$ are only the marginals of $q^\pi$.}. We could then apply the same strategy as in Section~\ref{sec:visitation_entropy} to solve MTEE. Nevertheless, for trajectory entropy, there is a more direct way to proceed.

\paragraph{Entropy regularized Bellman equations} One big difference between MVEE and MTEE is that the optimal policy can be obtained by solving regularized Bellman equations. Indeed, thanks to the chain rule for the entropy, the trajectory entropy of a policy $\pi$ is $\TE(d^\pi) = V_1^\pi(s_1)$ and the maximum trajectory entropy is $\TE\big(d^{\pistarTE}\big)  = \Vstar_1(s_1)$ where the value functions $V^\pi$ and $\Vstar$ satisfy
{\small
\begin{align*}
	Q_h^{\pi}(s,a) &= \cH\big(p_h(s,a)\big) + p_h V_{h+1}^\pi(s,a) \,,\\
    V_h^\pi(s) &= \pi_h Q_h^\pi (s) +\cH\big(\pi_h(s)\big)\,,\\
  Q_h^\star(s,a) &=  \cH\big(p_h(s,a)\big) + p_h V_{h+1}^\star(s,a) \,,\\
  V_h^\star(s) &= \max_{\pi\in\Delta_A}  \pi Q_h^\star (s) + \cH(\pi)\,,
\end{align*}}

\vspace{-0.4cm}
\!where by definition, $V_{H+1}^\star \triangleq V_{H+1}^\pi \triangleq 0$. In particular, the maximum trajectory entropy policy is given by $\pistarTE_h(s) = \argmax_{\pi\in\Delta_A} (\pi\Qstar(s)+\cH(\pi))$.
It can be computed explicitly via $\pistarTE_h(a|s) = \exp\!\big(\Qstar_h(s,a) -\Vstar_h(s)\big)$ as well as the optimal value function $\Vstar_h(s) = \log\left(\sum_{a\in\cA} \rme^{\Qstar_h(s,a)}\right)$.

We now describe our algorithm \algMTEE for MTEE. Since one only needs to solve  regularized Bellman equations to obtain a maximum trajectory entropy policy, we can use an algorithm of the same flavor as the ones proposed for best policy identification \citep{tirinzoni2022optimistic, menard2021fast, kaufmann2020adaptive, dann2019policy}. In particular, \algMTEE is close to the \BPIUCRL algorithm by \citet{kaufmann2020adaptive} and can be characterized by the following rules.


\paragraph{Sampling rule} As sampling rule we use an optimistic policy $\pi^{t+1}$ obtained by optimistic planning in the regularized MDP
{\small
\begin{align}
  \uQ_h^t(s,a) &=  \clip\Big(\cH\big(\hp_h(s,a)\big)+b_h^{\cH,t}(s,a) \nonumber\\
  &\quad+ \hp^t_h \uV_{h+1}^t(s,a)+ b_h^{p,t}(s,a),0,\log(SA)H\Big)\,,\nonumber\\
  \uV_h^t(s) &= \max_{\pi\in\Delta_A}  \pi \uQ_h^t (s) + \cH(\pi)\,,\label{eq:optimistic_planning_TE}\\
  \pi_h^{t+1}(s) &= \argmax_{\pi\in\Delta_A}  \pi \uQ_h^t (s) + \cH(\pi)\nonumber\,,
\end{align}}
with $\uV^t_{H+1} = 0$ by convention  where $b^{\cH,t}$, $b^{p,t}$ are bonuses for the entropy  and the transition probabilities, respectively. Precisely we use bonuses of the form 
{\scriptsize
\begin{align*}b_h^{\cH,t}(s,a) &=  \sqrt{\frac{2 \beta^{\cH}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + \min\left(\frac{\beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)},\, \log(S) \!\!\right)\,,\\
b_h^{p,t}(s,a) &= \sqrt{\frac{2H^2\log^2(SA) \beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}}\,,
\end{align*}}
for some functions $\beta^{\cH}, \beta^{\KL}$ and $\beta^{\conc}$.

\paragraph{Stopping and decision rule} To define the stopping rule, we first recursively build an upper-bound on the difference between the value of the optimal policy and the value of the current policy $\pi^{t+1}$, 
{\scriptsize
\begin{align}
    W_h^t(s,a) &=  4H^2 \log(SA) \frac{\beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} + \left( \!1\! +\! \frac{1}{H}\right) \hp^{\,t}_h G^{t}_{h+1}(s,a)\,,\nonumber\\
    G^{t}_{h}(s) &= \min\!\bigl( \pi_h^{t+1} W_h^t(s)+ \!\frac{1}{2} \big(\uV^t_h(s) - \lV^t_h(s)\!\big)^2\!\!,\, \log(SA)H \bigl)\,,\label{eq:upper_bound_gap}
\end{align}\\}

\vspace{-1.2cm}
where $\lV^t$ is a lower-bound on the optimal value function defined in Appendix~\ref{app:regularized_mdp} and $G_{H+1}^t = 0$ by convention.
Then the stopping time $\tau= \inf\{ t \in \N : G^t_{1}(s_1)  \leq \varepsilon \}$ corresponds to the first episode when this upper-bound is smaller than $\epsilon$. At this episode we return the Markovian policy $\hpi = \pi^{\tau+1}$.

\begin{algorithm}[h!]
\centering
\caption{\algMTEE}
\label{alg:UCBVIEnt}
\begin{algorithmic}[1]
  \STATE {\bfseries Input:} Target precision $\epsilon$, target probability $\delta$, threshold functions $\beta^{\cH},\beta^{p}$, $\beta^{\conc}$.
      \WHILE{ true}
       \STATE Compute $\pi^{t}$ by optimistic planning with \eqref{eq:optimistic_planning_TE}.
       \STATE Compute bound on the gap $G_1^{t-1}(s_1)$ with \eqref{eq:upper_bound_gap}.
       \STATE \textbf{if} $G_1^{t-1}(s_1)\leq \epsilon$ \textbf{then break}
      \FOR{$h \in [H]$}
        \STATE Play $a_h^t\sim \pi_h^t(s^t_h)$
        \STATE Observe $s_{h+1}^t\sim p_h(s_h^t,a_h^t)$
      \ENDFOR
    \STATE{ Update counts $n^t$, transition estimates $\hp^t$ and episode number $t\gets t+1$.}
   \ENDWHILE
   \STATE Output policy $\hpi = \pi^t$.
\end{algorithmic}
\end{algorithm}
The complete procedure is described in Algorithm~\ref{alg:UCBVIEnt}. We now state our main theoretical result for \algMTEE. We prove that for the well calibrated threshold functions $\beta^{\cH}, \beta^{\KL}$ and $\beta^{\conc},$ the \algMTEE is  $(\epsilon, \delta)$-PAC for MTEE and  provide a
high-probability upper bound on its sample complexity. The next result is proved in Appendix~\ref{app:regularized_mdp}.
\begin{theorem}
\label{th:ucbvi_sample_complexiy}
Let $\beta^{\KL}, \beta^{\conc}$ and $\beta^{\cH}$ be defined in Lemma~\ref{lem:proba_master_event} of Appendix~\ref{app:regularized_mdp}. Fix  $\epsilon>0$ and $\delta\in(0,1),$ then the \algMTEE algorithm is $(\epsilon,\delta)$-PAC for MTEE.   Moreover, the optimal policy is given by $\hpi = \pi^{\tau+1}$ where 
\[
\tau = \tcO\left(\frac{H^6S^2A}{\epsilon}\right).
\]
with probability at least $1-\delta.$
Here $\tcO$ hides poly-logarithmic factors in $\epsilon, \delta, H,S,A$. See Theorem~\ref{th:mtee_sample_complexity} in Appendix~\ref{app:regularized_mdp} for a precise bound.
\end{theorem}



\paragraph{Space and time complexity} Since \UCBVIEnt is a model based algorithm, its space-complexity is of order $\cO(HS^2A)$ whereas its time-complexity for one episode is of order $\cO(HS^2A)$ because of the optimistic planning.

\vspace{0.25cm}
\begin{remark} 
\label{rem:regularized_mdp}
(On solving regularized MDPs) Interestingly, our approach for MTEE can be adapted to solve entropy-regularized MDPs. For a reward functions $(r_h)_{h\in[H]}$ and regularization parameter $\lambda>0,$ consider the regularized Bellman equations 
{\small
\begin{align*}
    Q^{\pi}_{\lambda,h}(s,a) &= r_h(s,a) + p_h V^{\pi}_{\lambda,h+1}(s,a)\,,\\
    V^\pi_{\lambda,h}(s) &= \pi_h Q^\pi_{\lambda,h}(s) + \lambda \cH\big(\pi_h(s)\big)\,,\\
    \Qstar_{\lambda,h}(s,a) &= r_h(s,a) + p_h \Vstar_{\lambda,h+1}(s,a)\,,\\
    \Vstar_{\lambda,h}(s) &= \max_{\pi\in\simplex_A} \pi\Qstar_{\lambda,h}(s) + \lambda \cH(\pi)
\end{align*}}

where $V_{\lambda, H+1}^\pi = \Vstar_{\lambda, H+1} = 0$. Note that these are the Bellman equations used by \SoftQlearning \citep{fox2016taming,schulman2017equivalence,haarnoja2017reinforcement} and \SAC \citep{haarnoja2018soft} algorithms. We are interested in the best policy identification for this regularized MDP. That is finding an algorithm that will output an $\epsilon$-optimal policy $\hpi$ such that with probability $1-\delta$, it holds $\Vstar_{\lambda,1}(s_1) - V^{\hpi}_{\lambda,1}(s_1) \leq \epsilon$ after a minimal number $\tau$ of trajectories sampled from the MDP $\cM^r = (\cS,\cA, H ,(p_h)_{h\in[H]},(r_h)_{h\in[H]},s_1)$. 
By using  similar sampling  and stopping rules as in \algMTEE, we get an algorithm for BPI in the entropy-regularized MDP that also enjoys the fast rate of order $\tcO\big(H^6S^2A/(\epsilon \lambda)\big)$. Refer to Appendix~\ref{app:regularized_mdp} for precise statements and proofs.

We observe that the sample complexity for solving the regularized MDP is strictly smaller\footnote{For small enough $\epsilon$.} than the sample complexity for solving the original MDP. Indeed, one needs at least $\tcO(H^3SA/\epsilon^2)$ trajectory \citep{domingues2021episodic} to learn a policy $\pi$ which value in the (unregularized) MDP is $\epsilon$-close to the optimal value. Nevertheless, regularizing the MDP introduces a bias in the value function. Precisely we have for all $\pi$, $0\leq V^\pi_{\lambda,1}(s_1)-  V^\pi_1(s_1) \leq \tcO(\lambda H)$ where 
$V^\pi_1(s_1)$ is the value function of $\pi$ at the initial state and MDP $\cM^r$. Thus, to solve BPI in $\cM^r$ through BPI in the regularized MDP, one needs to take $\lambda = \tcO(1/(H\epsilon))$, leading to a sample complexity of order $\tcO(H^7S^2A/\epsilon^2)$. In particular, our fast rate for BPI in regularized MDP does not contradict the lower bound for BPI in the original MDP. However, our analysis shows that regularization is an effective way to trade-off bias for sample complexity.
\end{remark}


\paragraph{Visitation entropy vs trajectory entropy}
We can compare the visitation entropy and the trajectory entropy with
\[
\TE(q^\pi)\leq  \underbrace{\KL(q^\pi,\otimes_{h=1}^H d^\pi_h) + \TE(q^\pi)}_{\VE(d^\pi)} \leq H \TE(q^\pi)\,,
\]
see Lemma~\ref{lem:comparison_ent_traj_visit} in Appendix~\ref{app:technical} for a proof. Note also that in general the visitation distributions of an optimal policy for maximum trajectory entropy will be less 'spread' than the one of an optimal policy for MTEE, see Section~\ref{sec:experiments} for an example. In particular one can prove that the optimal policy for MTEE is the uniform policy if the transitions are deterministic, see Lemma~\ref{lem:MTEE_deterministic} of Appendix~\ref{app:technical}.







%\todopp{I am not sure I actually used the contraction property of the entropy for the 2nd inequality, but rather a "grouping" property of the entropy (for me contraction is something like H(X|Y)<=H(X)). If the grouping argument stated in appendix is fine, then we should change the word "contraction" by "grouping" here, otherwise we have to change the proof.}





\subsection{Proof sketch}
In this section we sketch the proof of Theorem~\ref{th:mtee_sample_complexity}. 

\paragraph{Properties of entropy}

We start from analysing several properties of the entropy function. First, we notice that the well-known log-sum-exp function is a convex conjugate to the negative entropy defined only on the probability simplex \cite{boyd2004convex}
{\small
\[
    F(x) \triangleq \log\left( \sum_{a \in \cA} \rme^{x_a} \right) = \max_{\pi \in \simplex_A} \langle \pi, x \rangle + \cH(\pi)
\]}
% \todoPi{This is not the usual convex conjugate since we are optimizing on the simplex.}
% \todoDa{Added note that we define entropy on the the probability simplex, thus it is a correct formula for conjugate.}
and extend its action to $Q$-functions
{\small
\[
    F(Q)(s) \triangleq \max_{\pi \in \simplex_A} \pi Q(s) + \cH(\pi). 
\]
}
This definition is useful because we can rewrite the optimal value function for MTEE and $\uV^t$ as follows
\begin{equation}\label{eq:V_star_using_F}
    \Vstar_{h}(s) = F(\Qstar_h)(s), \quad \uV^t_h(s) = F(\uQ^t_h)(s).
\end{equation}
Additionally, we notice that the gradient of $F$ is equal to the soft-max policy that maximizes the expressions above
\[
    \pistar_h(s) = \nabla F(\Qstar_h)(s) , \quad \pi^{t+1}_h(s) = \nabla F(\uQ^t_h)(s),
\]
and, moreover, since the negative entropy $-\cH(\pi)$ is $1$-strongly convex with respect to $\ell_1$ norm, gradients of $F$ is $1$-Lipschitz with respect to $\ell_\infty$ norm by the properties of the convex conjugate \cite{kakade2009duality}. Combining the gradient properties with  the smoothness definition to $\Qstar$ and $\uQ$ we obtain
\begin{small}
\begin{align}\label{eq:F_smooth_Q}
    \begin{split}
    F(\Qstar_h)(s) &\leq F(\uQ^t_h)(s) + \pi^{t+1}_h\left( \Qstar_h- \uQ^t_h\right)(s) \\
    &+ \frac{1}{2} \norm{ \Qstar_h - \uQ^t_h}^2_\infty(s),
    \end{split}
\end{align}\end{small}
where \begin{small}$\norm{ \Qstar_h - \uQ^t_h}_\infty(s) = \max_{a\in\cA} \vert \Qstar_h(s,a) -\uQ^t_h(s,a) \vert$.\end{small}
% \todoPi{We can say directly that since $\cH$ is strongly convex then it's convex conjugate is smooth. Btw why it is not 1 the smoothness constant (inverse of the strongly convex)?}
% \todoDa{As I remember, strong convexity defined as $\mu$ such that $f(y) \geq f(x) - \langle \nabla f(x), y-x \rangle + \frac{\mu}{2} \norm{y-x}^2$ and then $\mu$ is strong convexity constant (for entropy $\mu=1$ is from Pinsker's inequality), and the smoothness is also defined with $1/2$-constant. Added a reference with exact formulations. }
% \todoPi{Maybe we can just state this last inequality and avoid to describe precisely all the properties of entropy to save space?}
% \todoDa{Removed the smoothness definition, but I think that showing representation of V-functions and policies using F is crucial.}

\paragraph{Bound on the policy error}

Next we apply the key inequality \eqref{eq:F_smooth_Q} to analyze the error between the optimal policy and a policy $\pi^{t+1}$. Using~\eqref{eq:V_star_using_F}
\begin{small}
\begin{align*}
    \Vstar_h(s) - V^{\pi^{t+1}}_h(s) &\leq \uV^t_h(s) - V^{\pi^{t+1}}_h(s)  + \pi^{t+1}_h \big(\Qstar_h - \uQ^t_h \big)(s) \\
    &+ \frac{1}{2} \max_{a \in \cA} \left( \uQ^t_h(s,a) - \Qstar_h(s,a) \right)^2.
\end{align*}
\end{small}

\vspace{-0.4cm}
Next, by definition of $\pi^{t+1}$  we have
\begin{small}
$
    \uV^t_h(s) = \cH(\pi^{t+1}_h(s)) +  \pi^{t+1}_h \uQ^t_h(s),
$
\end{small}
therefore by the regularized Bellman equations
\begin{small}
\begin{align}\label{eq:policy_error_ineq}
    \begin{split}
    \Vstar_h(s) - V^{\pi^{t+1}}_h(s) & \leq \pi^{t+1}_h p_h \left( \Vstar_{h+1} - V^{\pi^{t+1}}_{h+1} \right)(s) \\
    &+\frac{1}{2} \max_{a \in \cA} \left( \uQ^t_h(s,a) - \Qstar_h(s,a) \right)^2,
    \end{split}
\end{align}\\
\end{small}

\vspace{-0.9cm}
and, finally, since our estimate $\uQ$ is optimistic on some high-probability event by construction of bonuses, we can show 
\begin{small}$
    \max\limits_{a \in \cA} \left( \uQ^t_h(s,a) - \Qstar_h(s,a) \right)^2 \leq \big(\uV^t_h(s) - \lV^t_h(s)\big)^2,
$\end{small}
where $\lV^t_h$ is a lower confidence bound on $\Qstar_h$. In other words, in the regularized setting we are able to propagate the policy error from step $h$ to step $h+1$ by the cost of only a squared over-estimation error. It is the most crucial part of the proof because this square allows us to make all the first-order terms of type $1/\sqrt{n^t_h(s,a)}$  the second-order terms of type $1/n^t_h(s,a)$ and obtain fast rates.

To finalize all these formulas to the computable decision rule, we notice that we can exchange $p_h$ with $\hp^t_h$ in the right-hand side of \eqref{eq:policy_error_ineq} by Lemma~\ref{lem:reg_directional_concentration} paying the second-order term and $(1+1/H)$-factor. Overall, we obtain
\begin{small}
\begin{equation}\label{eq:policy_error_via_gap}
    \Vstar_1(s_1) - V^{\pi^{t+1}}_1(s_1) \leq G^t_1(s_1)
\end{equation}\\
\end{small}

\vspace{-0.9cm}
for $G^t_h(s)$ defined in \eqref{eq:upper_bound_gap}. This inequality automatically justifies that \UCBVIEnt is $(\varepsilon,\delta)$-PAC of MTEE because for $\hpi = \pi^{\tau+1},$ we have $G^{\tau}_1(s_1) \leq \varepsilon$, and \eqref{eq:policy_error_via_gap} for $t=\tau$ concludes the statement.

% \begin{remark}
% Another key difference between our approach and approaches to identifying the best policy in ordinary MDPs \citep{menard2021fast, kaufmann2020adaptive, dann2019policy} is that smoothing the log-sum-exp function allows us to define the upper bounds on the policy error not for the difference in $Q$-functions, but only for the policy error.
% \end{remark}

\begin{remark}
Another key difference between our approach and BPI is that smoothness of the log-sum-exp function allows us to define the upper bounds on the policy error not in terms of the difference of $Q$-functions, but of value-functions.
\end{remark}


% \paragraph{Sample complexity}
% \todoDe{May give too many details in this paragraph ...}
% To derive an upper bound on sample complexity we have to provide an upper bound on $G^t_1(s_1)$. To do it, we can change  $\hp^t_h$ back to $p_h$ with a second-order penalty and by rolling out conditional expectations
% \[
%     G^t_1(s_1) \leq \E_{\pi^{t+1}}\biggl[ \sum_{h=1}^H \frac{\tcO(H^2S)}{n^t_h(s_h,a_h)} + \frac{\rme^2}{2}(\uV^t_h - \lV^t_h)^2(s_h)  \Big| s_1 \biggl].
% \]
% Next we analyze the second term. The difference $\uV^t_h - \lV^t_h$ is a standard object in the analysis of \UCBVI-like algorithms and could be upper bounded as follows
% \begin{small}
%     \begin{align*}
%         (\uV^t_h &- \lV^t_h)(s_h) \leq \sqrt{\E_{\pi^{t+1}}\left[ \sum_{h'=h}^H \frac{\tcO(H^3)}{n^t_{h'}(s_{h'},a_{h'})} \bigg| s_h\right]} \\
%         &+ \tcO(H) \cdot \E_{\pi^{t+1}}\left[ \sum_{h'=h}^H \min\left(\frac{\tcO(HS)}{n^t_{h'}(s_{h'},a_{h'})}, 1\right) \bigg| s_h\right].
%     \end{align*}\\\end{small}
% In the setting of unregularized MDPs the term that is similar to the first one in the upper bound above has the leading role because it scales as sum of $1/\sqrt{n^t_h(s,a)}$ and leads to $\tcO(1/\varepsilon^2)$ sample complexity, In our case, this term becomes the second-order one too that leads to improvement in sample complexity.
    
% Taking the square and applying Jensen's inequality several times we can obtain
% \[
%     G^t_1(s_1) \leq \E_{\pi^{t+1}}\left[ \sum_{h=1}^H \frac{\tcO(H^5 S)}{n^t_h(s,a)} \bigg| s_1  \right].
% \]
% Notice that for all $t < \tau$ we have $G^t_1(s_1) > \varepsilon$, thus summing up the inequality above for all $t < \tau$
% \[
%     (\tau-1)\varepsilon \leq  \sum_{h,s,a} \sum_{t=1}^{\tau-1} d^{\pi^{t+1}}_h(s,a) \frac{\tcO(H^5 S)}{n^t_h(s,a)}  = \tcO\left( H^6 S^2 A \right),
% \]
% that yields the fast rate for the MTEE problem.

