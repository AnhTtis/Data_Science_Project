\section{Introduction}\label{sec:intro}



In reinforcement learning (RL), an agent interacts with an environment  aiming to maximize the sum of rewards returned by the environment. When the reward signal is very sparse or completely absent, the agent may experience long periods without any feedback. In these periods exploration is the main challenge. 

This work studies the problem of efficient \emph{exploration in the absence of rewards}. Approaches to solve this problem can be roughly cast into three main groups: The \emph{bonus-based exploration} where the agent maximizes self-defined bonuses or intrinsic rewards collected along trajectory \citep{schmidhuber1991possibility,oudeyer2007intrisic,bellemare2016unifying}. Typically these bonuses are related to the variances of  error-signals from some auxiliary tasks, such as learning the transition probability distributions \citep{schmidhuber1991possibility,chentanez2004intrinsically, pathak2017curiosity,savas2019entropy},
learning the optimal value function for all the possible rewards \citep{jin2020reward-free,kaufmann2020adaptive,menard2021fast}, learning random generated features \citep{burda2019exploration}. A second approach is the \emph{goal-conditioned exploration} where the agent learns to navigate to self-assigned states (or goals). A common goal-selection rule for this class of algorithms is to select as goals the states at the frontier of the visited states \citep{lim2012autonomous,tarbouriech2020improved, escoffet2019goexplore}. Other selection-goal rules include reaching each state a fixed number of times \citep{tarbouriech2021provably} or going to states where the estimation error for the transition probabilities is large \citep{tarbouriech2020active}. The third approach, which so far received bit less attention, is the \emph{maximum entropy exploration} \citep{hazan2019provably,lee2019efficient,mutti2021task}. That is, we want a policy leading to the as uniform as possible visitation distribution of all state-action pairs. Such goal is usually achieved by maximizing  entropy-like functionals and this is what we study.

In this work, we focus on environments modeled by an episodic, finite, reward-free Markov Decision Process (MDP) with $S$ states, $A$ actions, horizon $H$ and step-dependent transitions. We consider two types of entropy:  the \emph{visitation entropy} and  the \emph{trajectory entropy}. The visitation entropy of a policy is defined as the sum of the entropies of the visitation distributions induced by the given policy at each step. The trajectory entropy of a policy is given by the entropy of a trajectory, generated when one follows the given policy and seen as one random variable on the corresponding path space. We study maximum entropy exploration under the $(\epsilon,\delta)$-PAC framework, that is, we want to learn, with probability $1-\delta,$ a policy leading to  $\epsilon$-optimal maximum visitation or trajectory entropy
and  using as few as possible interactions with the environment. 

\paragraph{Visitation entropy} \citet{hazan2019provably} study maximum visitation entropy\footnote{Note that \citet{hazan2019provably} consider a slightly different entropy; see Remark~\ref{rem:hazan_entropy_vs_us}.} exploration (MVEE) in the more general framework of convex MDPs where the agent wants to maximize a convex function of the visitation distribution. The authors in  \citet{hazan2019provably} propose to apply the Frank-Wolfe algorithm \citep{frank1956algorithm} to a smoothed version of the visitation entropy. Their algorithm, \MaxEnt\footnote{In this work we refer to \MaxEnt as the algorithm by \citet{hazan2019provably} applied to the visitation entropy and not to the reverse entropy as initially proposed by the authors.}, has a sample complexity of order\footnote{We adapt rates from the $\gamma$-discounted setting by replacing $1/(1-\gamma)$ with $H$. To take into account step-dependent transitions we multiply the first order term by $H^2$.} $\tcO(H^4S^2A/\epsilon^3+S^3/\epsilon^6)$, that is, it needs to sample that  number of trajectories in order to find an $\epsilon$-optimal policy for MVEE.
Later, \citet{cheung2019exploration} obtains a better rate of order $\tcO(H^4S^2A/\epsilon^2+H^3 S/\epsilon^3)$ for the \TocUCRL algorithm. Then, building on the ideas introduced by \citet{abernethy2017frankwolfe}, \citet{zahavy2021reward} reinterpret the \MaxEnt algorithm as a method to compute the equilibrium of a particular game induced by the Legendre-Fenchel transform of the smoothed entropy. Using this new point of view, they propose the \MetaEnt algorithm\footnote{We call \MetaEnt the specialization of their general Meta-algorithm to the special case of MVEE. Note that \MaxEnt, \TocUCRL, \MetaEnt could be seen as variations of the same algorithm. We use different names to distinguish, at least, the associated sample complexity.} with a sample complexity of order $\tcO(H^4S^2A/(\delta^2\epsilon^2)+H^3S/\epsilon^3)$. 
In this work, building on the ideas by \citet{grunwald2002game}, we draw a connection between MVEE and another game. In this game, a \emph{forecaster-player tries to predict the state-action pairs visited by a sampler-player} who aims at \emph{surprising the forecaster-player by visiting not well predicted state-action pairs}. We propose the \EntGame algorithm that tackles MVEE by solving this prediction game. We prove that \EntGame has a sample complexity of order $\tcO(H^4S^2A/\epsilon^2+HSA/\epsilon)$, thus improving over the previous rate in terms of its  dependence of $\epsilon$, see Table~\ref{tab:sample_complexity}. The key technical point leading to this improvement is that, contrary to the previous algorithms, \EntGame does not need to estimate accurately the visitation distribution of a policy at each iteration but only needs \emph{one trajectory generated by following this policy}. 


\paragraph{Trajectory entropy} The second problem we consider is maximum trajectory entropy exploration (MTEE). 
The entropy of paths of a (Markovian) stochastic process was first introduced in \citet{ekroot1993entropy}. Intuitively maximizing the trajectory entropy of an MDP minimizes the predictability of paths. Also there is a close connection between MTEE and regularized RL, a very popular approach in practical applications of RL.
%Even if trajectory entropy is a natural choice we show that an optimal policy for trajectory entropy will %not induced a visitation distribution as `spread' as the one obtained with a policy optimal for visitation %entropy.\footnote{See Section~\ref{sec:trajectory_entropy}.} However, as we will see the connection %between trajectory entropy and regularized RL justifies our interest for MTEE. 

Contrary to MVEE, the optimal policy for MTEE can easily be obtained by solving \emph{entropy-regularized Bellman equations} with \emph{entropy of the transition probabilities as rewards}. Leveraging this observation, one can proceed in a similar way as for the best policy identification\footnote{Where in this problem the goal is to identify the optimal policy of a given MDP (equipped with a reward function).} (BPI, \citealt{fiechter1994efficient}). Precisely, we propose the \algMTEE algorithm that computes a policy solving  optimistic version of the aforementioned Bellman equations and uses it to interact with the environment. The algorithm stops when some upper confidence bound on the difference between the maximum trajectory entropy and the trajectory entropy of the current policy is small enough. Interestingly, we prove that \algMTEE enjoys a sample complexity of order $\tcO(H^6S^2A/\epsilon)$. The key technical ingredients to obtain such fast rate are  exploitation of the \emph{smoothness introduced by the regularization} and the use of the explicit form of the optimal policy. 


% showing a separation between MTEE and other exploration problem such that reward-free exploration (RFE) where the optimal sample complexity is of order $\tcO(H^6S^2A/\epsilon)$. 

\paragraph{Regularized MDPs} Notably we can adapt\footnote{That is replace the entropy of the transition probability by an arbitrary reward function.} our algorithm for MTEE to best policy identification in regularized MDPs \citep{neu2017unified, geist2019theory}. Especially, we consider the same entropy-regularized MDPs and associated Bellman equations as in \SoftQlearning \citep{fox2016taming,schulman2017equivalence,haarnoja2017reinforcement} or \SAC\citep{haarnoja2018soft}, see Remark~\ref{rem:regularized_mdp}.
We show that a variation of \algMTEE has a sample complexity of order $\tcO(H^6S^2A/(\epsilon\lambda))$ for BPI in regularized MDPs, where $\lambda$ is the regularization parameter. In particular, it exhibits a \emph{statistical separation between BPI in regularized MDP and BPI in the original MDP} since in this case the optimal sample complexity is of order  $\tcO(H^3SA/\epsilon^2)$ \citep{menard2021fast,domingues2021episodic}. Thus, our analysis shows that regularization is an effective way to trade-off bias for sample complexity.




We highlight our main contributions:
\vspace{-0.2cm}
\begin{itemize}[itemsep=-2pt,leftmargin=6pt]
    \item We propose the \EntGame algorithm for MVEE with a sample complexity of order $\tcO(H^4S^2A/\varepsilon^2)$ thus significantly improving the existing complexity bounds  for MVEE.
    \item We introduce the new MTEE setting for exploration and provide the \algMTEE algorithm for MTEE with a sample complexity of order $\tcO(H^6 S^2 A/\varepsilon)$. Up to our knowledge, this is the first time that a fast rate (in $1/\varepsilon$) is obtained thanks to regularization.
    \item We adapt the \algMTEE algorithm to solve the entropy-regularized MDPs with a sample complexity of order $\tcO(H^6S^2A/(\epsilon\lambda))$ where $\lambda$ is the regularization parameter. 
\end{itemize}



% \todoDa{Ideas on general structure, similar to \cite{menard2021fast}:}
% \begin{itemize}
%     \item Maximum entropy: two different formulations
%     \item Visitation entropy. Connection to a general convex MDP setting, refinement of meta-algorithm for $\tcO\left(\frac{H^3 S^2 A}{\varepsilon^2}\right)$ complexity for MaxEnt.
%     \item Trajectory entropy. Bellman equations. Connection to regularized MDPs, soft Q-learning thought the lens of new objective function. Two algorithms: regularization-agnostic $\tcO\left(\frac{H^3 S A}{\varepsilon^2} \right)$ and regularization-aware $\tcO \left(\frac{H^6 S^2 A}{\varepsilon} \right)$.
% \end{itemize}





\begin{table}[t]
\setlength{\tabcolsep}{4pt}
\centering
\label{tab:sample_complexity}
\begin{tabular}{@{}lcl}\toprule
\textbf{Algorithm} & \textbf{Setting} & \textbf{Sample complexity} \\
\midrule
\MaxEnt~{\tiny{\citep{hazan2019provably}}} & \multirow{4}{*}{MVEE} & {\scriptsize$\tcO(H^4S^2A/\epsilon^3\!+\!S^3/\epsilon^6)$} \\
\TocUCRL~{\tiny{\citep{cheung2019exploration}}} & &{\scriptsize$\tcO(H^4S^2A/\epsilon^2\!+\!H^3 S/\epsilon^3)$} \\
\MetaEnt~{\tiny{\citep{zahavy2021reward}}} &  & {\scriptsize$\tcO(H^4S^2A/(\delta^2\epsilon^2)\!+\!H^3 S/\epsilon^3)$} \\
 \rowcolor[gray]{.90} \algMVEE~{\tiny(this paper)} &  & {\scriptsize$\tcO(H^4S^2A/\epsilon^2\!+\!HSA/\epsilon)$} \\
\midrule 
 \rowcolor[gray]{.90} \algMTEE~{\tiny (this paper)} & MTEE & {\scriptsize$\tcO(H^6S^2A/\epsilon)$} \\
\bottomrule
\end{tabular}
\caption{Sample complexities for MVEE and MTEE. We convert rates from the $\gamma$-discounted setting by replacing $1/(1-\gamma)$ with $H$ or from the infinite horizon setting by replacing the diameter with $H$. To take into account step-dependent transitions we multiply the first order term by $H^2$. {\scriptsize(For \MetaEnt since they do not precisely specify the cost for estimating a visitation distribution we use the same $1/\epsilon^3$ term as for \TocUCRL.)}}
% \todoPi{Is it just $1/\epsilon^3$ for the second order term for \citet{zahavy2021reward}?}
% \todoDa{They do not consider the complexity of density estimation and I guess that it should be like the second term from \citet{hazan2019provably}, but anyway it will be at least $1/\varepsilon^2$ per each step (and the number of steps is $1/\varepsilon$). We can leave remark like that. }}
%\todom{mention \url{https://arxiv.org/pdf/1905.06466.pdf} - perhaps add to this table too}
\end{table}
