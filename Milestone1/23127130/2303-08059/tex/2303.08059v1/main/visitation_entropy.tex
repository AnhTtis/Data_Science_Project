\section{Visitation entropy}
\label{sec:visitation_entropy}
In this section we focus on maximizing the visitation entropy defined below.

\paragraph{Visitation entropy} We define the visitation entropy of a policy $\pi$ denoted by  as the sum of the visitation distribution entropies at each steps
\vspace{-0.3cm}
\[
\VE(d^\pi) \triangleq \sum_{h=1}^H \cH(d_h^\pi)\,\cdot
\]
We denote by $\pistarVE\in\argmax_{\pi} \VE(d^\pi)$ a policy that maximizes the visitation entropy. 

% %\todom{can we say which one is preferable? was it easier for them
% %to show something of their AVG version?}
% %\todopp{I am not sure that the visitation entropy of \citet{hazan2019provably} is what is stated here. Maybe I am wrong, but from my understanding, it should be$\frac{1}{H}\sum_{(h,s,a)} d_h^\pi(s,a) \log\frac{1}{\frac{1}{H}\sum_{h} d_h^\pi(s,a)}$. The inequality is still true by concavity though}
% \todoPi{
% %What is the difference between the entropy you wrote and the one below it seems they are the same. 
% For me the difference between averaging the visitation distribution or not is a matter of do we treat each step state action $(h,s,a)$ separately or we only consider only $(s,a)$. The second option makes more sens in the discounted setting where the transition does not depends on the step and there is a natural limit visitation distribution. In the episodic case is not clear if we want to 'merge' all the visitation distributions}
% %\todopp{Bellow, inside the log, the sum is also over $s,a$ which I think is not correct.}
% %\todoPi{Yes you are right!}
% \todom{Thanks Pierre for noticing this! Given this 'discrepancy' of Hazan's defintion, can does it maka Hazan's less natural, since it is not $\sum_{i\in[n]} p_i\log p_i$ for the same $p$, if I understood correctly? 
% And this is something that we can remark? (I'm fishing here for some reason to justify why we do not follow Hazan 100\%)} 
% \todopp{Does our result hold for both entropies? If, for the Hazan's one, we take as rewards $-\log\big(\frac{1}{H}\sum_{h}d_h^t(s,a)\big)$ in \algMVEE}
% \end{remark}

\paragraph{Maximum visitation entropy exploration} In MVEE the agent interacts with the reward-free MDP $\cM$ as follows. At the beginning of episode $t$, the agent picks a policy $\pi^t$ based only on the transitions collected up to episode $t-1$. Then a new reward-free trajectory is sampled following the policy~$\pi^t$ and observed by the agent. At the end of each episode the agent can decide to stop collecting new data, according to a random stopping time $\tau$, the stopping rule, and outputs a (general) policy $\hpi$ based on the observed transitions. An agent for MVEE is therefore made of a triplet $((\pi^t)_{t\in\N},\tau,\hpi)$. 
%\todom{why is $((\pi^t)_{t\in\N},\tau,\hpi)$ and agent? }
%\todoPi{Non-Markovian output}
\begin{definition} (PAC algorithm for MVEE) An algorithm $((\pi^t)_{t\in\N},\tau,\hpi)$ is $(\epsilon,\delta)$-PAC for MVEE if 
{\small\[
\P\Big( \VE\big(d^{\pistarVE}\big) - \VE(d^{\hpi}) \leq \epsilon\Big) \geq 1-\delta.
\]}
\end{definition}
Our goal is to design an algorithm that is $(\epsilon,\delta)$-PAC for MVEE with as sample complexity $\tau$ as small as possible.

\subsection{MVEE by solving game}
\label{sec:MVEE_game}

Following the general framework of \citet{hazan2019provably, zahavy2021reward}, it is possible to solve MVEE by applying the Frank-Wolfe algorithm to a smoothed version of the visitation entropy. Interestingly, \citet{abernethy2017frankwolfe} showed that this procedure is equivalent to computing the Nash equilibrium of a particular game induced by the Legendre-Fenchel transform of the smoothed entropy. In fact, as noted by \citet{grunwald2002game}, there exists another game naturally linked to MVEE, stated next.

\paragraph{Prediction game} Maximum visitation entropy is the value of the following prediction game 
\begin{align*}
    \max_{d\in\cK_p} \VE(d) &= \max_{d\in\cK_p} \min_{\bd\in\cK}\sum_{(h,s,a)} d_h(s,a) \log \frac{1}{\bd_h(s,a)}\\
    &=  \min_{\bd\in\cK} \max_{d\in\cK_p} \sum_{(h,s,a)} d_h(s,a) \log \frac{1}{\bd_h(s,a)}\CommaBin
\end{align*}
see Lemma~\ref{lem:prediction_game} in Appendix~\ref{app:technical} for a proof.
This game can be interpreted as follows. On the one hand, the min player, or forecaster player, tries to predict which state-action pairs the max player will visit to minimize $\KL(d_h,\bd_h)$.  On the other hand, the max player, or sampler player, is rewarded for visiting state-action pairs that the forecaster player did not predict correctly.


We now describe the algorithm \algMVEE\ for MVEE. In this algorithm, we let a forecaster player and a sampler player compete for $T$ episodes long. Let us first define the two players.
\paragraph{Forecaster-player} As forecaster-player we use the Mixture-Forecaster for a logarithmic loss, see Section~9 in \citep{cesabianchi2006prediction}. Fix a prior count $n_0$ and their sum $t_0 = S A n_0$. The forecaster-player predicts at episode $t$ the distributions $\bd^t\in\cK$ with $\bd_h^t(s,a) = \bn_h^{t-1}(s,a) /(t+t_0)$ where the pseudo counts are $ \bn_h^t(s,a) = n_h^t(s,a)+n_0$ and $n_h^t(s,a)$ the counts of state-action pairs visited by the sampler-player.
Note that $\bd_h^t$ can be seen as the posterior mean under a Dirichlet distribution on $\cS\times\cA.$ 
% \todopp{Do we put a remark here saying that the Hazan's entropy case can be dealt using the AVG over h of the $\bd_h^t(s,a)$ ?}
% \todoPi{Explain the Bayesian interpretation}



\paragraph{Sampler-player} As sampler-player we choose the optimistic best-response. Define the optimistic Bellman equations 
{\small
\begin{align}
\begin{split}\label{eq:optimistic_planning_VE}
\uQ_h^t(s,a) &=  \log\frac{1}{\bd_h^{t+1}(s,a)} + \hp_h^{\,t} \uV^t_{h+1}(s,a) +b_h^t(s,a) \\
\uV_h^t(s) &= \clip\big( \max_{a\in\cA}\uQ_h^t(s,a), 0, \log(t/n_0+SA)H\big) 
\end{split}
\end{align}\\}
where  $V_{H+1}^t = 0$ and $b_h^t$ are some Hoeffding-like bonuses defined in \eqref{eq:sampler_exploration_bonus} of Appendix~\ref{app:visitation_entropy_proofs}. The sampler player then plays $d^{\pi^{t+1}}$ where $\pi^{t+1}$ is greedy with respect to the optimistic Q-values, that is, $\pi_h^{t+1}(s) \in\argmax_{\pi\in\Delta_A} \pi \uQ_h^{t}(s)$. 

\paragraph{Sampling rule} At each episode $t$ the policy $\pi^t$ of the sampler-player is used as a sampling rule to generate a new trajectory.

\paragraph{Decision rule} After $T$ episodes we output a non-Markovian policy $\hpi$ defined as the mixture of the policies $\{\pi^t\}_{t\in[T]}$, that is, to obtain a trajectory from $\hpi$ we first sample uniformly at random $t\in[T]$ and then follow the policy $\pi^t$. Note that the visitation distribution of $\hpi$ is exactly the average $d^{\hpi} = (1/T)\sum_{t\in[T]} d^{\pi^t}$. 



Remark that the stopping rule of \algMVEE is deterministic and equals to $\tau = T$. The complete procedure is detailed in Algorithm~\ref{alg:ourMVEE}.

\begin{algorithm}[h!]
\centering
\caption{\algMVEE}
\label{alg:ourMVEE}
\begin{algorithmic}[1]
  \STATE {\bfseries Input:} Number of episodes $T$, prior counts $n_0$.
      \FOR{$t \in[T]$}
      \STATE \textcolor{blue}{\# Forecaster-player}
      \STATE Update pseudo counts $\bn_h^{t-1}(s,a)$ and predict $\bd_h^t(s,a)$. 
      \STATE \textcolor{blue}{\# Sampler-player}
      \STATE Compute $\pi^t$ by optimistic planning \eqref{eq:optimistic_planning_VE} with rewards $\log\big(1/ \bd_h^t(s,a)\big)$.
    \STATE \textcolor{blue}{\# Sampling}
      \FOR{$h \in [H]$}
        \STATE Play $a_h^t\sim \pi_h^t(s_h^t)$
        \STATE Observe $s_{h+1}^t\sim p_h(s_h^t,a_h^t)$
      \ENDFOR
    \STATE{ Update counts and transition estimates.}
   \ENDFOR
   \STATE Output $\hpi$ the uniform mixture of $\{\pi^t\}_{t\in[T]}$.
\end{algorithmic}
\end{algorithm}




\begin{theorem}
\label{th:MVEE_sample_complexity}
Fix  $\epsilon > 0,$  $\delta\in(0,1)$ and $n_0=1.$ Then under the choice 
\[
T = \tcO\left( \frac{H^4 S^2 A}{\varepsilon^2} + \frac{HSA}{\varepsilon} \right)
\]
the algorithm \algMVEE is $(\epsilon,\delta)$-PAC. See Theorem~\ref{th:MVEE_sample_complexity_full} in Appendix~\ref{app:visitation_entropy_proofs} for a precise bound.
\end{theorem}
Thus the sample complexity of \algMVEE is of order $\tcO(H^4S^2A/\epsilon^2)$. In particular, this result significantly improves over the previous rate for MTEE, see Table~\ref{tab:sample_complexity}. Note that, by using Bernstein-like bonuses~\citep{azar2017minimax} instead of Hoeffding-like ones for the sampler-player would gives a sample complexity of order $\tcO(H^3S^2A/\epsilon^2)$ saving one factor $H$. 

\paragraph{Space and time complexity} Since \algMVEE relies on a model-based algorithm for the sampler-player, its space complexity is of order $\cO(HS^2A)$. Because of the value iteration performed by the sampler-player, the time-complexity of one iteration of \algMVEE is of order $\cO(HS^2A)$.

\begin{remark}
\label{rem:hazan_entropy_vs_us} Note that our definition of the visitation entropy slightly differs from the one considered by \citet{hazan2019provably}. Indeed, their definition, translated to the episodic setting, is the entropy of the average of the visitation distributions which is an upper bound on the average of the entropies by concavity of the entropy 
{\small
\[
\cH\left(\frac{1}{H} \sum_{h=1}^H d_h^\pi\right)\geq \frac{1}{H}\VE(d^\pi)\,.\]\vspace{-0.25cm}\\
} 
Even if both definitions make sense in the episodic setting, we think ours is slightly more appropriate in the case of step-dependent transition probabilities. 
Indeed, in this case we want visitation distributions to be close to the uniform distribution over state-action pairs \emph{for all steps}.
 Nevertheless, \EntGame can be adapted to optimize the visitation entropy  used in \citet{hazan2019provably} simply by predicting $\bd_h^t(s,a) = \sum_{h'=1}^H \bn_{h'}^{t-1}(s,a)/(H(t+t_0))$ for the forecaster-player. We conjecture that the sample complexity of this adaptation of \EntGame for the alternative entropy is again of order $\tcO(HS^2A/\epsilon^2)$.
\end{remark}

\paragraph{Comparison with \MaxEnt  and \MetaEnt} All three algorithms, \algMVEE, \MetaEnt\citep{zahavy2021reward}, \MaxEnt\citep{hazan2019provably} rely on the same principle of computing, implicitly or explicitly, the equilibrium of a well chosen game and deduce from it an optimal policy for MVEE. One first difference between \algMVEE and its competitors lies in the choice of the game. While \MetaEnt, \MaxEnt consider the game induced by the Legendre-Fenchel conjugate of a smoothed visitation entropy \citep{zahavy2021reward}, \algMVEE leverages the prediction game which looks more natural for MVEE. One advantage of using this game, is that it allows to avoid the need to smooth the visitation entropy because it is done implicitly by the forecaster-agent with the pseudo-counts. % In fact,  they have to choose a more conservative equivalent of prior count of order $n_0=\cO(\sqrt{T})$ whereas $n_0=1$ is enough for \algMVEE.
More importantly, \MaxEnt and \MetaEnt both needs to accurately estimate at each episode the visitation distributions of the sampler-player $d_h^{\pi^{t}}$, leading to an extra $1/\epsilon^3$ term in the sample complexity. Whereas \algMVEE  needs one trajectory from $\pi^t$ since it only involves the estimation of the averages $1/T \sum_{t=1}^T d_h^{\pi^t}$.
