\vspace{-0.3cm}
\section{Conclusion}
\label{sec:conclusion}

In this work we studied MVEE for which we provided the \EntGame algorithm with a sampling complexity significantly smaller than the existing complexity  rates. We also introduced the MTEE problem where the optimal policy can be found using the dynamic programming. We proposed the \UCBVIEnt algorithm for MTEE that can be adapted to BPI in regularized MDPs. We proved that, in both cases, \UCBVIEnt and its variant enjoy a fast rate. In particular, we observed a statistical separation between BPI in regularized MDP and in the original MDP.

This work opens the following interesting future research directions:

\vspace{-0.2cm}
\paragraph{Optimal rates for MVEE and MTEE} We are still lacking lower bounds for MTEE and MVEE problems enabling us to determine the optimal rates, especially what the number of states $S$ and the horizon $H$ is concerned. Note that one cannot apply directly the usual lower-bounds techniques for these two problems because of the  entropy  regularization  in both cases. In particular, we conjecture that the optimal rate for MVEE is also of order $\tcO(\text{poly}(H,S,A)/\epsilon)$.

\vspace{-0.1cm}
\paragraph{Optimal rate for entropy-regularized RL} It would be interesting to obtain the optimal rate for BPI in a regularized MDP. In particular to recover the optimal rate for BPI in the original MDP by tuning the regularization parameter $\lambda$. We conjecture that the optimal rate is $\tcO(H^2SA/(\lambda\epsilon))$ for BPI in entropy-regularized MDP. 

\vspace{-0.1cm}
\paragraph{Other types of entropies} Our methodology can be applied to other types of entropies and even to other regularization penalties. One interesting case would be the goal-conditioned trajectory entropy (see \citet{savas2019entropy}) where one considers only process realizations that reach a certain set of states at time $H$.  This entropy can be applied to goal-conditioned RL. 