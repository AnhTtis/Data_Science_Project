\section{Regularized Bellman Equations}\label{app:reg_bellman_eq}

In this section we provide complete proofs for regularized Bellman Equations in the general setting. Let $\Phi \colon \Delta_{\cA} \to \R$ be a strictly convex function.

Then we can define the regularized value function as follows
\begin{equation}\label{eq:reg_value_func}
    V^{\pi}_{\lambda,h}(s) \triangleq \E_\pi\left[ \sum_{h'=h}^H r_{h'}(s_{h'}, a_{h'}) -\lambda \Phi(\pi_h(s_{h'}))  \mid s_h = s \right].
\end{equation}
Notably, for a specific choice of rewards $r_h(s,a) = \cH(p_h(s,a))$, the regularizer is equal to the negative entropy $\Phi(\pi) = -\cH(\pi)$, and $\lambda = 1$ we have $V^{\pi}_{\lambda,1}(s_1) = \TE(q^\pi)$. In more general setting let $r_h(s,a)$ be equal to the sum of deterministic reward and $\lambda \cH(p_h(s,a))$. In this case we have $V^{\pi}_{\lambda,1}(s_1) = V^\pi_1(s_1) + \lambda\TE(q^\pi)$ in terms of a usual non-regularized value function.

Let us define an entropy action-value function as follows
\begin{equation}\label{eq:reg_q_func}
    Q^{\pi}_{\lambda,h}(s,a) \triangleq \E_{\pi}\left[ r_h(s_h,a_h) + \sum_{h'=h+1}^H \left[  r_{h'}(s_{h'}, a_{h'}) - \lambda \Phi(\pi_h(s_{h'})) \right] \mid (s_h,a_h) = (s,a) \right].
\end{equation}

Additionally, we define an optimal entropy-regularized value functions a follows
\[
    \Vstar_{\lambda,h}(s) \triangleq \max_{\pi} V^{\pi}_{\lambda,h}(s), \quad \Qstar_{\lambda,h}(s,a) \triangleq \max_{\pi} Q^{\pi}_{\lambda,h}(s,a) \quad \forall (s,a,h) \in \cS \times \cA \times [H]
\]

\subsection{Proof of Entropy-Regularized Bellman Equations}\label{app:proof_entropic_bellman_eq}

\begin{theorem}[Regularized Bellman Equations]
    For any stochastic policy $\pi$ the following decomposition of the entropy-regularized value function holds
    \begin{align}\label{eq:reg_bellman_equation}
    \begin{split}
        Q^{\pi}_{\lambda,h}(s,a) &= r_h(s,a) + p_h V^{\pi}_{\lambda,h+1}(s,a), \\
        V^{\pi}_{\lambda, h}(s) &= \pi_h Q^{\pi}_{\lambda,h}(s) - \lambda \Phi(\pi_h(s)).
    \end{split}
    \end{align}

    Moreover, for optimal $Q$- and $V$-functions we have
    \begin{align}\label{eq:opt_reg_bellman_equation}
    \begin{split}
        \Qstar_{\lambda,h}(s,a) &= r_h(s,a) + p_h \Vstar_{\lambda,h+1}(s,a), \\
        \Vstar_{\lambda,h}(s) &= \max_{\pi \in \Delta_{\cA}}\left\{ \pi \Qstar_h(s) - \lambda \Phi(\pi) \right\}.
    \end{split}
    \end{align}
\end{theorem}
\begin{remark}
    For the case of interest $\Phi(\pi) = - \cH(\pi)$ the expression for the $V$-function allows the closed-form formula by a well-known LogSumExp smooth maximum approximation
    \[
        \Vstar_{\lambda,h}(s) = \lambda \log\left( \sum_{a \in \cA} \exp\left( 
        \frac{1}{\lambda} \Qstar_{\lambda,h}(s,a) \right) \right),
    \]
    and as $\lambda \to 0$ we see that entropy-regularized value function tends to a usual value function without regularization.
\end{remark}
\begin{proof}
    We proceed by induction. For $h=H+1$ the equation is trivial. By definition and tower property of conditional expectation
    \begin{align*}
        Q^{\pi}_{\lambda,h}(s,a) &= r_h(s,a) + \E\left[ \sum_{t=h+1}^{H} r_t(s_t, a_t) - \lambda \Phi(\pi_t(s_t)) \biggl| s_h=s, a_h=a \right] \\
        &= r(s,a) + \E\left[ \E\left[\sum_{t=h+1}^{H} r_t(s_t, a_t) - \lambda \Phi(\pi_t(s_t)) \biggl| s_{h+1}\right] \biggl| s_h=s, a_h=a \right] \\
        &= r_h(s,a) + p_h V^{\pi}_{\lambda, h+1}(s,a).
    \end{align*}
    Next we provide the second Bellman equation by tower property and the definition of the regularized $Q$-function
    \begin{align*}
        V^{\pi}_{\lambda,h}(s) &=  - \lambda \Phi(\pi_h(s)) + \E\left[ r_h(s_h, a_h) + \sum_{t=h+1}^{H} r_t(s_t, a_t) - \lambda \Phi(\pi_t(s_t))  \biggl| s_h=s \right] \\
        &= \pi_h Q^{\pi}_{\lambda,h}(s) - \lambda \Phi(\pi_h(s)).
    \end{align*}

    For optimal Bellman equation we proceed by induction. For $h=H+1$ the equation is also trivial. By Bellman equations
    \[
        \Qstar_{\lambda,h}(s,a) = \max_{\pi} \left\{ r_h(s,a) + p_h V^{\pi}_{\lambda,h+1}(s,a) \right\} = r_h(s,a) + p_h \Vstar_{\lambda,h+1}(s,a),
    \]
    and, finally
    \[
        \Vstar_{\lambda,h}(s) = \max_{\pi_1,\ldots,\pi_H \in \simplex_{\cA}}\left\{ \pi_h \Qstar_{\lambda,h}(s)  - \lambda \Phi(\pi_h(s))\right\} = \max_{\pi \in \Delta_{\cA}}\left\{ \pi \Qstar_{\lambda,h}(s) - \lambda \Phi(\pi) \right\}.
    \]
\end{proof}



\subsection{A Bellman-type Equations for Variance}\label{app:Bellman_variance}
For a stochastic policy $\pi$ we define Bellman-type equations for the variances as follows
\begin{align*}
  \Qvar_{\lambda,h}^{\pi}(s,a) &\triangleq \Var_{p_h}{V_{\lambda,h+1}^{\pi}}(s,a) + p_h \Vvar^{\pi}_{\lambda,h+1}(s,a)\\
  \Vvar_{\lambda,h}^{\pi}(s) &\triangleq \Var_{\pi_h}{Q^{\pi}_{\lambda,h}}(s) + \pi_h \Qvar^{\pi}_{\lambda,h} (s)\\
  \Vvar_{\lambda,H+1}^{\pi}(s)&\triangleq0,
\end{align*}
where $\Var_{p_h}(f)(s,a) \triangleq \E_{s' \sim p_h(\cdot | s, a)} \big[(f(s')-p_h f(s,a))^2\big]$ denotes the \emph{variance operator over transitions} and $\Var_{\pi_h}(f)(s) \triangleq \E_{a' \sim \pi_h(s)}\big[ (f(s,a') - \pi_h f(s))^2 \big]$ denoted the \emph{variance operator over the policy}.
 In particular, the function $s \mapsto \Vvar_{\lambda,1}^{\pi}(s)$ represents the average sum of the local variances $\Var_{p_h}{V_{\lambda, h+1}^{\pi}}(s,a)$ and $ \Var_{\pi_h}{Q^{\pi}_{\lambda,h}}(s) $ over a trajectory following the policy $\pi$, starting from $(s, a)$. Indeed, the definition above implies that
 \[\Vvar_{\lambda,1}^{\pi}(s_1) = \sum_{h=1}^H \sum_{s\in\cS} d_h^\pi(s) \Var_{\pi_h}{Q^{\pi}_{\lambda,h}}(s) +  \sum_{h=1}^H\sum_{s,a} d_h^\pi(s,a) \Var_{p_h}(V_{\lambda,h+1}^{\pi})(s,a).
 \]
 The lemma below shows that we can relate the global variance of the cumulative reward over a trajectory to the average sum of local variances.
\begin{lemma}[Law of total variance]\label{lem:law_of_total_variance}  For any stochastic policy $\pi$ and for all $h\in[H]$,
\begin{align*}
   \Qvar_{\lambda,h}^{\pi}(s,a) &= \E_\pi\!\left[  \left(\sum_{h'=h}^H \left( r_{h'}( s_{h'},a_{h'}) -\lambda \Phi(\pi_{h'}(s_{h'})) \right) - \left( Q_{\lambda,h}^{\pi}(s_h,a_h) -\lambda \Phi(\pi_h(s_h)) \right) \right)^{\!\!2}\middle| (s_h,a_h)=(s,a) \right], \\
  \Vvar_{\lambda,h}^{\pi}(s) &= \E_\pi\!\left[  \left(\sum_{h'=h}^H \left( r_{h'}( s_{h'},a_{h'}) -\lambda \Phi(\pi_{h'}(s_{h'})) \right) - V_{\lambda,h}^{\pi}(s_h) \right)^{\!\!2}\middle| s_h=s \right].
\end{align*}
\end{lemma}
\begin{proof}
	We proceed by induction. The statement in Lemma~\ref{lem:law_of_total_variance} is trivial for $h=H+1$. We now assume that it holds for $h+1$ and prove that it also holds for $h$. For this purpose, we compute
	\begin{align*}
		&
		%\Qvar_h^\pi(s,a) =
		\E_\pi\left[\!
		 \left(\sum_{h'=h}^H \left( r_{h'}( s_{h'},a_{h'}) -\lambda \Phi(\pi_{h'}(s_{h'})) \right) -  \left( Q_{\lambda,h}^{\pi}(s_h,a_h) -\lambda \Phi(\pi_h(s_h))\right) \right)^{\!\!2}
		 \middle| (s_h,a_h) \right] \\
		& =
		\E_\pi\left[\! \left( V_{\lambda, h+1}^{\pi}(s_{h+1}) - p_h V_{\lambda, h+1}^{\pi}(s_h,a_h) + \sum_{h'=h+1}^H \left( r_{h'}( s_{h'},a_{h'}) -\lambda \Phi(\pi_{h'}(s_{h'})) \right) - V_{\lambda, h+1}^{\pi}(s_{h+1})\right)^{\!\!2}
		\middle| (s_h,a_h) \right] \\
		& =
		\E_\pi\left[\!
			\left(  V_{\lambda, h+1}^{\pi}(s_{h+1}) - p_h V_{\lambda, h+1}^{\pi}(s_h,a_h) \right)^{\!\!2}
		\middle| (s_h,a_h) \right] \\
		&
		+ \E_\pi\left[\!
		\left( \sum_{h'=h+1}^H \left( r_{h'}( s_{h'},a_{h'}) -\lambda \Phi(\pi_{h'}(s_{h'})) \right) - V_{\lambda, h+1}^{\pi}(s_{h+1})\right)^{\!\!2}
		\middle| (s_h,a_h) \right] \\
		& + 2 \E_\pi\left[\!
			\left(  \sum_{h'=h+1}^H \left( r_{h'}( s_{h'},a_{h'}) -\lambda \Phi(\pi_{h'}(s_{h'})) \right) - V_{\lambda, h+1}^{\pi}(s_{h+1}) \right)
			\left( V_{\lambda, h+1}^{\pi}(s_{h+1}) - p_h V_{\lambda, h+1}^{\pi}(s_h,a_h) \right)
		\middle| (s_h,a_h) \right].
	\end{align*}
	The definition of  $V_{\lambda,h+1}^\pi(s_{h+1})$ implies that \[\E_\pi\!\left[  \sum_{h'=h+1}^H \left( r_{h'}( s_{h'},a_{h'}) -\lambda \Phi(\pi_{h'}(s_{h'})) \right) - V_{\lambda, h+1}^{\pi}(s_{h+1})	\middle| s_{h+1} \right] = 0.\]
	Therefore, the tower property of conditional expectation gives us
	\begin{align*}
		&\E_\pi\left[\!
		 \left(\sum_{h'=h}^H \left( r_{h'}( s_{h'},a_{h'}) -\lambda \Phi(\pi_{h'}(s_{h'})) \right) -  \left( Q_{\lambda,h}^{\pi}(s_h,a_h) -\lambda \Phi(\pi_h(s_h))\right) \right)^{\!\!2}
		 \middle| (s_h,a_h) \right]  \\ 
        &= \E_\pi\left[\!
			\left(  V_{\lambda, h+1}^{\pi}(s_{h+1}) - p_h V_{\lambda, h+1}^{\pi}(s_h,a_h) \right)^{\!\!2}
		\middle| (s_h,a_h) \right] \\
		&
		+ \E\left[\! \E_\pi\left[
		\left( \sum_{h'=h+1}^H \left( r_{h'}( s_{h'},a_{h'}) -\lambda \Phi(\pi_{h'}(s_{h'})) \right) - V_{\lambda, h+1}^{\pi}(s_{h+1})\right)^{\!\!2}
		\middle| s_{h+1} \right] \middle| (s_h,a_h) \right] \\
		&  = \Var_{p_h}{V_{\lambda, h+1}^{\pi}}(s_h,a_h) + p_h \Vvar^{\pi}_{\lambda,h+1}(s_h,a_h) = \Qvar_{\lambda,h}^{\pi}(s_h,a_h)
	\end{align*}
	where in the third equality we used the inductive hypothesis and the definition of $\sigma V_{h+1}^{\pi}$. To prove the second equation we use the entropy-regularized Bellman equations
    \begin{align*}
        & \E_\pi\!\left[  \left(\sum_{h'=h}^H \left( r_{h'}( s_{h'},a_{h'}) -\lambda \Phi(\pi_{h'}(s_{h'})) \right) - V_{\lambda,h}^{\pi}(s_h) \right)^{\!\!2}\middle| s_h=s \right] \\
        &= \E_\pi\!\left[  \left(\sum_{h'=h}^H \left( r_{h'}( s_{h'},a_{h'}) -\lambda \Phi(\pi_{h'}(s_{h'})) \right) - (Q_{\lambda,h}^{\pi}(s_h, a_h) - \lambda \Phi(\pi_{h}(s_h)) ) \right)^{\!\!2}\middle| s_h=s \right] \\
        &+ 2\E_\pi\!\left[  \left(\sum_{h'=h}^H \left( r_{h'}( s_{h'},a_{h'}) -\lambda \Phi(\pi_{h'}(s_{h'})) \right) - (Q_{\lambda,h}^{\pi}(s_h, a_h) - \lambda \Phi(\pi_{h}(s_h)) )\right) \left( \pi_h (Q_{\lambda,h}^{\pi}(s_h) - (Q_{\lambda,h}^{\pi}(s_h, a_h)  \right)\middle| s_h=s \right] \\
        &+ \E_{\pi}\!\left[\left( \pi_h Q_{\lambda,h}^{\pi}(s_h) - Q_{\lambda,h}^{\pi}(s_h, a_h)  \right)^{2} \middle| s_h=s\right].
    \end{align*}
    By definition of $Q^{\pi}_{\lambda,h}$ we have
    \[
        \E_\pi\left[ \left(\sum_{h'=h}^H \left( r_{h'}( s_{h'},a_{h'}) -\lambda \Phi(\pi_{h'}(s_{h'})) \right) - (Q_{\lambda,h}^{\pi}(s_h, a_h) - \lambda \Phi(\pi_{h}(s_h)) )\right) \middle| (s_h,a_h) = (s,a) \right] = 0,
    \]
    thus, by the tower property
    \begin{align*}
        &\E_\pi\!\left[  \left(\sum_{h'=h}^H \left( r_{h'}( s_{h'},a_{h'}) -\lambda \Phi(\pi_{h'}(s_{h'})) \right) - V_{\lambda,h}^{\pi}(s_h) \right)^{\!\!2}\middle| s_h=s \right] = \pi_h \Qvar^{\pi}_{\lambda,h}(s_h) + \Var_{\pi_h} Q^{\pi}_{\lambda, h}(s_h) = \Vvar^{\pi}_{\lambda, h}(s_h).
    \end{align*}
\end{proof}
