%!TEX root = ../BayesUCBVI.tex
\section{Technical Lemmas}
\label{app:technical}

\subsection{Entropy properties}

\begin{lemma}
\label{lem:prediction_game}
For any reward-free MDP $\cM = (\cS, \cA, \{p_h\}_{h\in[H]}, s_1)$ the following holds
\begin{align*}
    \max_{d\in\cK_p} \VE(d) &= \max_{d\in\cK_p} \min_{\bd\in\cK}\sum_{(h,s,a)} d_h(s,a) \log \frac{1}{\bd_h(s,a)}\\
    &=  \min_{\bd\in\cK} \max_{d\in\cK_p} \sum_{(h,s,a)} d_h(s,a) \log \frac{1}{\bd_h(s,a)}
\end{align*}
\end{lemma}
\begin{proof}
The first equality is due to 
\begin{align*}
    \VE(d)=\sum_{h=1}^H \cH(d_h) = \sum_{h=1}^H \cH(d_h) +\KL(d_h,d_h)
= \min_{\bd\in\cK}
\sum_{h=1}^H (\cH(d_h) +\KL(d_h,\bd_h))
= \min_{\bd\in\cK}
\sum_{(h,s,a)} d_h(s,a) \log \frac{1}{\bd_h(s,a)}
\end{align*}
and the second equality uses Sion's theorem \citep{sion1958general} with the fact that $\cK,\cK_p$ are compact convex sets and $(d,\bd) \mapsto -\sum_{h,s,a} d_h(s,a) \log \bd_h (s,a)$ is concave-convex.
\end{proof}

\begin{lemma}
\label{lem:comparison_ent_traj_visit}
For any reward-free MDP $\cM = (\cS, \cA, \{p_h\}_{h\in[H]}, s_1)$ and any policy $\pi$ it holds
\begin{align*}
\TE(q^\pi)\leq \VE(d^\pi)\leq H \TE(q^\pi).
\end{align*}
\end{lemma}
\begin{proof}
The first inequality is a result of the non-negativity of mutual information, specifically the difference $\VE(d^\pi)-\TE(q^\pi)$ is equal to the Kullback-Leibler divergence between $q^\pi$ and the product distribution $\otimes_{h=1}^H d^\pi_h$, 
\begin{align*}
\VE(d^\pi)&=
\sum_{s,a,h}d^\pi_h(s,a) 
 \log\frac{1}{d^\pi_h(s,a)}
\\&= 
 \sum_{s,a,h}~\sum_{m=(s_1,a_1,\dots,s_H,a_H)\in\cT}\ind\{(s,a)=(s_h,a_h)\} q^\pi(m) 
 \log\frac{1}{d^\pi_h(s,a)}
 \\&= \sum_{h=1}^H~\sum_{m=(s_1,a_1,\dots,s_H,a_H)\in\cT} q^\pi(m)\log\frac{1}{d^\pi_h(s_h,a_h)} 
 \\&= \sum_{m=(s_1,a_1,\dots,s_H,a_H)\in\cT} q^\pi(m)\log\frac{1}{\prod_{h=1}^H d^\pi_h(s_h,a_h)} 
 \\&= \KL(q^\pi,\otimes_{h=1}^H d^\pi_h) + \TE(q^\pi) \\&\geq \TE(q^\pi).
\end{align*}
The second inequality is simply a consequence of the 
monotonicity of logarithm
\begin{align*}
\TE(q^\pi) &= \sum_{m\in\cT} q^\pi(m) \log\frac{1}{q^\pi(m)} 
\\&=
 \sum_{s,a,h}\frac{1}{H}\sum_{m=(s_1,a_1,\dots,s_H,a_H)\in\cT}\ind\{(s,a)=(s_h,a_h)\} q^\pi(m) \log\frac{1}{q^\pi(m)}
 \\&\geq
 \sum_{s,a,h}\frac{1}{H}\sum_{m=(s_1,a_1,\dots,s_H,a_H)\in\cT}\ind\{(s,a)=(s_h,a_h)\} q^\pi(m) \log\frac{1}{\sum_{m'=(s'_1,a'_1,\dots,s'_H,a'_H)\in\cT}\ind\{(s,a)=(s'_h,a'_h)\} q^\pi(m')}
 \\&=
  \frac{1}{H} \VE(d^\pi),
\end{align*}
where we used that $\log(1/x) = -\log(x)$ is monotonically decreasing, and 
\[
    \sum_{m'=(s'_1,a'_1,\dots,s'_H,a'_H)\in\cT}\ind\{(s,a)=(s'_h,a'_h)\} q^\pi(m') \geq q^\pi(m)
\]
for any $m = (s_1,a_1,\dots,s_H,a_H)$ such that $(s_h,a_h) = (s,a)$.
\end{proof}


\begin{lemma}
\label{lem:MTEE_deterministic}
In a deterministic MDP, i.e. the transition probability distributions are deterministic, the maximum trajectory entropy policy is the uniform policy over actions
\[
\pistarTE_h(s,a) =1/A\,.
\]
\end{lemma}
\begin{proof}
    The proof is straightforward by using the Bellman equations for MTEE. By induction over the step $h$ we prove that $\pistarTE_h(s,a)=1/A$ and $\Vstar_h(s) = (H+1-h)\log(A)$. Assume the induction hypothesis at step $h+1$. Then we get $\Qstar_h(s,a) = 0 + p_h \Vstar_{h+1}(s,a) = (H-h)\log(A)$ which yields $\pistarTE_h(s,a)=1/A$ and $\Vstar_h(s) = \log\big(\sum_{a\in\cA} A^{H-h}\big) = (H+1-h)\log(A)$. The base case $h=H$ is immediate.
    \end{proof}


\subsection{Counts to pseudo-counts}
\label{app:count}
Here we state Lemma~8 and Lemma~9 by \citet{menard2021fast}.
\begin{lemma}\label{lem:cnt_pseudo} On event $\cE^{\text{\normalfont cnt}}$,  for any $\beta(\delta, \cdot)$ such that $x \mapsto \beta(\delta,x)/x$ is non-increasing for $x\geq 1$,  $x \mapsto \beta(\delta,x)$ is non-decreasing $\forall  h \in [H], (s,a) \in \cS \times \cA$,
\[ \forall t \in \N^\star, \ \frac{\beta(\delta, n_h^t(s,a))}{n_h^t(s,a)}\wedge 1 \leq 4 \frac{\beta(\delta, \bar n_h^t(s,a))}{\bar n_h^t(s,a)\vee 1}\cdot\]
\end{lemma}
% \begin{proof}
% As event $\cE^{\mathrm{\normalfont cnt}}$ holds, we know that for all $t < \tau$,
% \begin{align*}n_{\ell}^{t}(s,a) \geq \frac{1}{2}\bar n_{\ell}^{t}(s,a) - \beta^{\cnt}(\delta).
% \end{align*}
% We now distinguish two cases. First, if $\beta^{\cnt}(\delta) \leq \tfrac{1}{4}\bar n_{\ell}^{t}(s,a)$, then \[\frac {\beta(n_{\ell}^t(s,a), \delta)} {n_{\ell}^t(s,a)}\wedge 1 \leq \frac {\beta(n_{\ell}^t(s,a), \delta)} {n_{\ell}^t(s,a)}\leq \frac {\beta\left(\tfrac{1}{4}\bar n_{\ell}^{t}(s,a), \delta\right)} {\tfrac{1}{4}\bar n_{\ell}^{t}(s,a)} \leq 4   \frac {\beta\left(\bar n_{\ell}^{t}(s,a), \delta\right)} {\bar n_{\ell}^{t}(s,a) \vee 1}\CommaBin\]
% where we used that $x \mapsto \beta(x,\delta)/x$ is non-increasing for $x\geq 1$,  $x \mapsto \beta(x,\delta)$ is non-decreasing, and $\beta^{\cnt}(\delta) \geq 1$. Second,
% if $\beta^{\cnt}(\delta) > \tfrac{1}{4}\bar n_{\ell}^{t}(s,a)$, a simple derivation gives that
% \[\frac {\beta(n_{\ell}^t(s,a), \delta)} {n_{\ell}^t(s,a)}\wedge 1\leq   1 < 4 \frac{\beta^{\cnt}(\delta)}{\bar n_{\ell}^{t}(s,a) \vee 1} \leq 4 \frac{\beta(\bar n_{\ell}^{t}(s,a), \delta)}{\bar n_{\ell}^{t}(s,a) \vee 1}\CommaBin\]
% where we used that $1 \leq \beta^{\cnt}(\delta)\leq \beta(0,\delta)$ and $x \mapsto \beta(x,\delta)$ is non-decreasing.
% \end{proof}


\begin{lemma}
	\label{lem:sum_1_over_n}
	 For $T\in\N^\star$ and $(u_t)_{t\in\N^\star},$ for a sequence where  $u_t\in[0,1]$ and $U_t \triangleq \sum_{l=1}^t u_\ell$, we get
	\[
		\sum_{t=0}^T \frac{u_{t+1}}{U_t\vee 1} \leq 4\log(U_{T+1}+1).
	\]
\end{lemma}
% \begin{proof}
% 	Notice that	\begin{align*}
% 		\sum_{t=0}^T \frac{u_{t+1}}{U_t\vee 1} &\leq 4 \sum_{t=0}^T \frac{u_{t+1} }{2U_t + 2} \\
% 		&\leq  4\sum_{t=0}^T \frac{U_{t+1}-U_{t}}{U_{t+1} + 1}\\
% 		&\leq 4\sum_{t=0}^T \int_{U_t}^{U_{t+1}} \frac{1}{x+1} \mathrm{d}x\\
% 		& = 4\log(U_{T+1}+1).
% 	\end{align*}
% \end{proof}

\subsection{On the Bernstein inequality}
\label{app:Bernstein}
We restate here a Bernstein-type inequality by \citet{talebi2018variance}.
\begin{lemma}[Corollary 11 by \citealp{talebi2018variance}]\label{lem:Bernstein_via_kl}
Let $p,q\in\simplex_{S},$ where $\simplex_{S}$ denotes the probability simplex of dimension $S$. For all functions $f:\ \cS\mapsto[0,b]$ defined on $\cS$,
\begin{align*}
	p f - q f &\leq  \sqrt{2\Var_{q}(f)\KL(p,q)}+\frac{2}{3} b \KL(p,q)\\
  q f- p f &\leq  \sqrt{2\Var_{q}(f)\KL(p,q)}\,.
\end{align*}
where use the expectation operator defined as $pf \triangleq \E_{s\sim p} f(s)$ and the variance operator defined as
$\Var_p(f) \triangleq \E_{s\sim p} \big(f(s)-\E_{s'\sim p}f(s')\big)^2 = p(f-pf)^2.$
\end{lemma}
% \begin{proof}
% We only prove that
% \begin{align*}
% 	q f - p f &\leq  \sqrt{2\Var_{q}(f)\alpha}+\frac{2}{3}b \alpha
% \end{align*}
% as an upper bound on $p f - q f$ can be obtained analogically. We assume that $q f > p f,$ otherwise the inequality is trivially true. Furthermore, without loss of generality, we consider that $0 \leq f(s) \leq 1$ for all $s\in\cS$: when $f$ is bounded by $b$, we can apply the inequality to $f/b$ and by homogeneity, we get the desired general version.
% %by \todoMi{and by? .... maybe by?}
% %
% %homogeneity that for all $s\in\cS$, $0 \leq f(s) \leq 1$.
% Using the variational formula for the Kullback-Leibler divergence, we have
% \begin{align*}
% 	\KL(p,q) &= \sup_{g \in \R^S} p g - \log(q e^{g})\\
% 	&\geq \sup_{\lambda\geq 0} \lambda(q f-pf) - \log\left(q e^{\lambda(q f -f)}\right),
% \end{align*}
% where we chose $g \triangleq \lambda (q f -f)$ and used that $pqf = qf$ since $qf$ is a scalar. Given  that the mapping $u \to (e^u -u -1)/u^2$ is non-decreasing in $u$ and that $q f -f(s) \leq 1,$ we obtain
% \[
% e^{\lambda \big(q f -f(s)\big)} - \lambda\big( q f -f(s) \big) - 1 \leq \big(q f - f(s)\big)^2 (e^\lambda -\lambda-1).
% \]
% Furthermore, by taking the expectation of the previous inequality with respect to $q$, using the property $\log(1+x)\leq x,$ and define function $\phi(\lambda) \triangleq e^\lambda -\lambda -1,$ we get
% \begin{align*}
% 	\log(q e^{\lambda(q f-f)}) \leq \log\big(1+  \Var_{q}(f) \phi(\lambda)\big)\leq  \Var_{q}(f) \phi(\lambda).
% \end{align*}
% Now using the previous inequality in the variational formula above with the fact that the convex conjugate of $\phi$ is for $u\geq 0$, $\sup_{\lambda\geq 0}\lambda u -\phi(\lambda) \triangleq h(u) \geq u^2/\left(2(1+u/3)\right)$, yields
% \begin{align*}
% 	\KL(p,q) &\geq \sup_{\lambda\geq 0} \lambda(q f-pf) -  \Var_{q}(f) \phi(\lambda)\\
% 	&=  \Var_{q}(f) h\left( \frac{q f-pf}{ \Var_{q}(f)}  \right)\\
% 	&\geq \frac{(q f-pf)^2}{2\left(\Var_{q}(f)+(q f-pf)/3\right)}\cdot
% \end{align*}
% Since by assumption $\KL(p,q) \leq \alpha$, we get
% \[
% 2\alpha\left(\Var_{q}(f)+(q f-pf)/3\right)- (q f-pf)^2\geq 0.
% \]
% Hence $qf - pf$ is upper bounded by the positive root of the polynomial in the left hand side,
% \[\frac{\alpha}{3} + \sqrt{2\alpha \Var_q(f) + \frac{\alpha^2}{9}} \leq \sqrt{2\alpha \Var_q(f)} + \frac{2}{3}\alpha,\]
% using that $\sqrt{x+y} \leq \sqrt{x} + \sqrt{y}$.
% \end{proof}
% \todoAlex{I think that we don't need this lemma}
% \todoPi{Indeed I checked my notes and I do not use it...}
% \begin{lemma}
%   \label{lem:Bernstein_via_kinf} Let $p,q\in\Sigma_S$, and a functions $f:\ \cS\mapsto[0,b]$ defined on $\cS$, then
%   \[
%   pf -qf \leq \sqrt{4\Var_{p}(f)\Kinf(p,qf,f)}+4 b \Kinf(p,qf,f)\,.
%   \]
% \end{lemma}
% \begin{proof}
% Let $\tq\in\Sigma_S$ such that $\tq f \geq q f$ thus $\KL(p,\tq) \geq \Kinf(p,qf,f)$. If such probability distribution does not exists then the result is trivially true. Thanks to Lemma~\ref{lem:Bernstein_via_kl} we obtain
% \[
% pf -qf  = pf -\tq f \leq  \sqrt{2\Var_{\tq}(f)\KL(p,\tq)}+\frac{2}{3} b \KL(p,\tq)\,.
% \]
% Lemma~\ref{lem:switch_variance_bis} allows us to conclude
% \begin{align*}
%   pf -qf &\leq  \sqrt{4\Var_{p}(f)\KL(p,\tq)}+4 b \KL(p,\tq)\\
%   &= \sqrt{4\Var_{p}(f)\Kinf(p,qf,f)}+4 b \Kinf(p,qf,f)\,.
% \end{align*}
% \end{proof}

\begin{lemma}
\label{lem:switch_variance_bis}
Let $p,q\in\simplex_{S}$ and a function $f:\ \cS\mapsto[0,b]$, then
\begin{align*}
  \Var_q(f) &\leq 2\Var_p(f) +4b^2 \KL(p,q)\,,\\
  \Var_p(f) &\leq 2\Var_q(f) +4b^2 \KL(p,q).
\end{align*}
\end{lemma}
% \begin{proof}
% Let $\tp$ be the distribution of the pair of random variables $(X,Y)$ where $X,Y$ are i.i.d.\,according to the distribution $p$. Similarly, let $\tq$ be the distribution of the pair of random variables $(X,Y)$ where $X,Y$ are i.i.d.\,according to distribution $q$. Since Kullback–Leibler divergence is additive for independent distributions, we know that
% \[\KL(\tp,\tq) = 2\KL(p,q) \leq 2\KL(p,q).\]
% Using Lemma~\ref{lem:Bernstein_via_kl} for the function $g(x,y) = (f(x)-f(y))^2$ defined on $\cS^2$, such that  $0\leq g\leq b^2,$ we get
% \begin{align*}
%   |\tp g- \tq g| &\leq \sqrt{4\Var_{\tq}(g) \KL(p,q)} + \frac{4}{3}b^2 \KL(p,q)\\
%   &\leq  \sqrt{4 b^2 \KL(p,q) \tq g  } + \frac{4}{3}b^2 \KL(p,q)\\
%   &\leq \frac{1}{2} \tq g + \frac{10}{3} b^2 \KL(p,q)\,,
% \end{align*}
% where in the last line we used $2\sqrt{xy}\leq x +y$ for $x,y\geq 0$. In particular we obtain
% \begin{align*}
%   \tp g &\leq \frac{3}{2} \tq g + \frac{10}{3} b^2\KL(p,q)\\
%   \tq g &\leq 2 \tp g + \frac{20}{3}b^2 \KL(p,q) \,.
% \end{align*}
% To conclude, it remains to note that
% \[ \tp g = 2\Var_p(f) \text{ and } \tq g = 2\Var_q(f). \]
% \end{proof}

\begin{lemma}
	\label{lem:switch_variance}
	For $p,q\in\simplex_{S}$, for $f,g:\cS\mapsto [0,b]$ two functions defined on $\cS$, we have that
	\begin{align*}
 \Var_p(f) &\leq 2 \Var_p(g) +2 b p|f-g|\quad\text{and} \\
 \Var_q(f) &\leq \Var_p(f) +3b^2\|p-q\|_1,
\end{align*}
where we denote the absolute operator by $|f|(s)= |f(s)|$ for all $s\in\cS$.
\end{lemma}
% \begin{proof}
% First note that
% \[
% \Var_p(f) = p(f-g+ g-p g + p g- p f)^2 \leq 2 p(f-g - p f + p g)^2 +2 p(g-p g)^2 = 2\Var_p(f-g)+2\Var_p(g).
% \]
% From the above we can immediately conclude the proof of the first inequality with
% \[
% \Var_p(f-g) \leq p(f-g)^2 \leq b p|f-g|,
% \]
% where we used that for all $s\in\cS$, $0\leq |f(s)-g(s)| \leq b$. For the second inequality, using the Hölder inequality,
% \begin{align*}
% 	\Var_q(f) &= pf^2 - (pf)^2 +(q-p)f^2 + (pf)^2 -(qf)^2 \\
% 	&\leq \Var_p(f) + b^2\|p-q\|_1 +2b^2\|p-q\|_1 \\
% 	&\leq \Var_p(f) +3 b^2 \|p-q\|_1.
% \end{align*}
% \end{proof}
