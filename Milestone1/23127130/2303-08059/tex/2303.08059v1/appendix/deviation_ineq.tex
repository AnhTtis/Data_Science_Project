%!TEX root = ../BayesUCBVI.tex
\section{Deviation Inequalities}
\label{app:deviation_ineq}
% We define the following favorable events: $\cE^\star$ where the empirical means of the optimal value functions are close to the true ones, $\cE^\cnt$ the event where the pseudo-counts are close to their expectation, and $\cE^\bias$ the event where we control the deviation of the martingale of the bias of the upper bounds on the optimal value function,
% \begin{align*}
%   \cE^\star &\triangleq \Bigg\{\forall t \in \N, \forall h \in [H], \forall (s,a)\in\cS\times\cA:
%     \Kinf(\hp_h^t(s,a),p_h \Vstar_{h+1}(s,a), \Vstar_{h+1}) \leq  \frac{\beta(\delta,n_h^t(s,a))}{n_h^t(s,a)}\Bigg\}\,,\\
%  \cE^{\cnt} &\triangleq  \left\{ \forall t \in \N, \forall h\in [H],\forall (s ,a)\in\cS\times\cA:\ n_h^t(s,a) \geq \frac{1}{2}\bar n_h^t(s,a)-\beta^{\cnt}(\delta)  \right\}\,,\\
%  \cE^{\bias} &\triangleq \Bigg\{\forall t \in \N, \forall h \in [H], \forall (s,a)\in\cS\times\cA:\\
% &\qquad\sum_{k=1}^t \ind_{\{(s_h^t,a_h^t)=(s,a)\}}(\tp_h^k -p_h)( \uV_{h+1}^{k-1}-\Vstar_{h+1})(s,a) \leq\\
% &\qquad\qquad\sqrt{2 \beta(\delta,n_h^t(s,a))\sum_{k=1}^t \ind_{\{(s_h^t,a_h^t)=(s,a)\}} \Var_{p_h}( \uV_{h+1}^{k-1}-\Vstar_{h+1}) (s,a)} + 3 H \beta(\delta,n_h^t(s,a))\Bigg\}\\
% \cE^{\conc} &\triangleq \Bigg\{\forall t \in \N, \forall h \in [H], \forall (s,a)\in\cS\times\cA: \\
% &\qquad|(\hp_h^t -p_h) \Vstar_{h+1}(s,a)| \leq \sqrt{2 \Var_{p_h}(\Vstar_{h+1})(s,a)\beta(\delta,n_h^t(s,a))} + 3 H \frac{\beta(\delta,n_h^t(s,a))}{n_h^t(s,a)}\\
% &\qquad|(\hp_h^t -p_h) (\Vstar_{h+1})^2(s,a)| \leq 5\sqrt{ H^2\Var_{p_h}(\Vstar_{h+1})(s,a)\beta(\delta,n_h^t(s,a))} + 9 H^2 \frac{\beta(\delta,n_h^t(s,a))}{n_h^t(s,a)}
% \Bigg\}
% \end{align*}
% We also introduce the intersection of these events, $\cG \triangleq \cE \cap \cE^{\cnt}\cap \cE^\star,$ and the intersection of only the first two events, $\cF \triangleq \cE \cap \cE^{\cnt}$ . Note that the event $\cF$ is independent of the reward function $r$. We  prove that for the right choice of the functions $\beta$ the above events hold with high probability.
% \begin{lemma}
% \label{lem:proba_master_event}
% For the following choices of functions $\beta,$
% \begin{align*}
%   \beta(n,\delta) &\triangleq   \log(3SAH/\delta) + S\log \left(8e(n+1)\right),\\
%   \beta^\cnt(\delta) &\triangleq \log\left(3SAH/\delta\right), \quad \text{and}\\
%   \betastar(n,\delta) &\triangleq \log(3SAH/\delta) + \log\left(8e(n+1)\right)\,,\\
% \end{align*}
% it holds that
% \[
% \P(\cE)\geq 1-\delta, \qquad \P(\cE^{\cnt})\geq 1-\delta,  \qquad \text{and} \qquad \P(\cE^\star)\geq 1-\delta\,.
% \]
% In particular, $\P(\cG) \geq 1-\delta$ and $\P(\cF) \geq 1-\delta$.
% \end{lemma}
% \begin{proof}
% First, by Theorem~\ref{th:max_ineq_categorical}, we have that
% \[
% \P(\cE)\geq 1-\frac{\delta}{3}\cdot
% \]
% Second, by Theorem~\ref{th:bernoulli-deviation}, we have that
% \[
% \P(\cE^{\cnt})\geq 1-\frac{\delta}{3}\cdot
% \]
% Finally, by Theorem~\ref{th:bernstein}, we have that
% \[
% \P(\cE^\star)\geq 1-\frac{\delta}{3}\cdot
% \]
% Applying a union to the above three inequalities, we conclude that
% \[
% \P(\cG)\geq 1-\delta \qquad \P(\cE)\geq 1-\delta\,.
% \]
% \noindent \emph{Remark}: Note that we can order %\begin{align*}
%   $1\leq \beta^{\cnt}(\delta)\leq \betastar(n,\delta) \leq \beta(n,\delta).$
% %\end{align*}
% \end{proof}


\subsection{Deviation inequality for categorical distributions}

Next, we state the deviation inequality for categorical distributions by \citet[Proposition 1]{jonsson2020planning}.
Let $(X_t)_{t\in\N^\star}$ be i.i.d.\,samples from a distribution supported on $\{1,\ldots,m\}$, of probabilities given by $p\in\simplex_{m-1}$, where $\simplex_{m-1}$ is the probability simplex of dimension $m-1$. We denote by $\hp_n$ the empirical vector of probabilities, i.e., for all $k\in\{1,\ldots,m\},$
 \[
 \hp_{n,k} \triangleq \frac{1}{n} \sum_{\ell=1}^n \ind\left\{X_\ell = k\right\}.
 \]
 Note that  an element $p \in \simplex_{m-1}$ can be seen as an element of $\R^{m-1}$ since $p_m = 1- \sum_{k=1}^{m-1} p_k$. This will be clear from the context. 
%  We denote by $H(p)$ the (Shannon) entropy of $p\in\Sigma_m$,
%  \[
%  H(p) = \sum_{k=1}^m p_k \log\left(\frac{1}{p_k}\right)\cdot
%  \]
 \begin{theorem} \label{th:max_ineq_categorical}
 For all $p\in\simplex_{m-1}$ and for all $\delta\in[0,1]$,
 \begin{align*}
     \P\left(\exists n\in \N^\star,\, n\KL(\hp_n, p)> \log(1/\delta) + (m-1)\log\left(e(1+n/(m-1))\right)\right)\leq \delta.
 \end{align*}
\end{theorem}



\subsection{Deviation inequality for Shannon entropy}

 We denote by $\cH(p)$ the (Shannon) entropy of $p\in \simplex_{m-1}$,
 \[
    \cH(p) \triangleq \sum_{k=1}^m p_k \log\left(\frac{1}{p_k}\right) .
 \]
We will follow the ideas of \citet{paninski2003estimation}.
 
\begin{theorem}\label{th:entropy_concentration}
    For all $p \in \simplex_{m-1}$ and for all $\delta \in[0,1]$
    \[
        \P\left[ \vert \cH(\hp_n) - \cH(p) \vert \geq  \sqrt{\frac{2 \log^2(n) \cdot \log(2/\delta)}{n}} + \left( \frac{(m-1) \log(\rme (1 + n/(m-1))) + 1}{n} \wedge \log(m) \right) \right] \leq \delta.
    \]
    Moreover,
    \[
        \P\left[ \exists n:  \vert \cH(\hp_n) - \cH(p) \vert \geq  \sqrt{\frac{2 \log^2(n) \cdot (\log(2/\delta) + \log(n(n+1)))}{n}} + \left( \frac{(m-1) \log(\rme (1 + n/(m-1))) + 1}{n} \wedge \log(m) \right)\right] \leq \delta.
    \]
\end{theorem}
\begin{proof}
    We start from application of McDiarmid's inequality to entropy by \citet{antos2001convergence}.
    For all $p \in \Delta_{m-1}$ with probability at least $1-\delta$ we have
    \[
        \P\left[ \vert \cH(\hat p_n) - \E[\cH(\hat p_n)]  \vert \geq \sqrt{\frac{2 \log^2(n) \cdot \log(2/\delta)}{n}}  \right] \leq \delta.
    \]
    To relate $\E[\cH(\hp_n)]$ and $\cH(p)$ we use the following observation
    \[
        \cH(\hp_n) - \cH(p) = - \KL(\hp_n, p) + \sum_{k: p_k > 0} (\hp_{n,k} - p_k) \log(1/p_k),
    \]
    therefore by taking expectation we have
    \[
        \E[\cH(\hp_n)] - \cH(p) = - \E[\KL(\hp_n, p)].
    \]
    In the following our analysis differs from \cite{paninski2003estimation} since we obtain a direct estimate on the KL-divergence using Theorem~\ref{th:max_ineq_categorical},
    \begin{align*}
        \E[n \KL(\hp_n,p)] &= \int_{0}^\infty \P[n \KL(\hp_n, p) > t]\rmd t \leq (m-1) \log(\rme(1 + n/(m-1))) + \int_0^\infty \rme^{-t} \rmd t.
    \end{align*}
    At the same time we have a trivial bound that concludes the first statement
    \[
        \E[\cH(\hp^n)] - \cH(p) \leq \log(m).
    \]

    To show the second statement of Theorem~\ref{th:entropy_concentration}, we apply the first part with $\delta'(n) = \delta/( n(n+1))$,
    \[
        \P\left[ \vert \cH(\hp_n) - \cH(p) \vert \geq  \sqrt{\frac{2 \log^2(n) \cdot \log(2/\delta'(n))}{n}} + \left( \frac{(m-1) \log(\rme (1 + n/(m-1))) + 1}{n} \wedge \log(m) \right)\right] \leq \frac{\delta}{n(n+1)},
    \]
    thus by union bound over $n \in \N$ we conclude the statement.
\end{proof}

% \subsection{Deviation inequality for categorical weighted sum}
% %  Let $(X_t)_{t\in\N^\star}$ be i.i.d.\,samples from a distribution supported on $\{1,\ldots,m\}$, of probabilities given by $p\in\Sigma_m$, where $\Sigma_m$ is the probability simplex of dimension $m-1$. We denote by $\hp_n$ the empirical vector of probabilities, i.e., for all $k\in\{1,\ldots,m\},$
% %  \[
% %  \hp_{n,k} = \frac{1}{n} \sum_{\ell=1}^n \ind\left\{X_\ell = k\right\}.
% %  \]
% %  Note that  an element $p \in \Sigma_m$ can be seen as an element of $\R^{m-1}$ since $p_m = 1- \sum_{k=1}^{m-1} p_k$. 
%  We fix a function $f: \{1,\ldots,m\} \mapsto [0,b]$ and recall the definition of the minimal Kullback-Leibler divergence for $p\in\simplex_{m-1}$  and $u\in\R$
%  \[
% \Kinf(p,u,f) = \inf\left\{  \KL(p,q): q\in\simplex_{m-1}, qf \geq u\right\}\,.
%  \]
% As the Kullback-Leibler divergence this quantity admits a variational formula.
% \begin{lemma}[Lemma 18 by \citet{garivier2018kl}]
% \label{lem:var_form_Kinf} For all $p \in \simplex_{m-1}$, $u\in [0,b)$,
% \[
% \Kinf(p,u,f) = \max_{\lambda \in[0,1]} \E_{X\sim p}\left[ \log\left( 1-\lambda \frac{f(X)-u}{b-u}\right)\right]\,,
%  \]
%  moreover if we denote by $\lambda^\star$ the value at which the above maximum is reached, then
%  \[
%    \E_{X\sim p} \left[\frac{1}{1-\lambda^\star\frac{f(X)-u}{b-u}}\right] \leq 1\,.
%  \]
% \end{lemma}
% \begin{remark} Contrary to \citet{garivier2018kl} we allow that $u=0$ but in this case Lemma~\ref{lem:var_form_Kinf} is trivially true, indeed
%   \[
%   \Kinf(p,0,f) =  0  = \max_{\lambda \in[0,1]} \E_{X\sim p}\left[ \log\left( 1-\lambda \frac{f(X)}{b}\right)\right]\,.
%    \]
% \end{remark}

% We are now ready to state the deviation inequality for the $\Kinf$ which is a self-normalized version of Proposition~13 by \citet{garivier2018kl}.
%  \begin{theorem} \label{th:max_ineq_kinf}
%  For all $p\in\simplex_{m-1}$ and for all $\delta\in[0,1]$,
%  \begin{align*}
%      \P\big(\exists n\in \N^\star,\, n\Kinf(\hp_n, pf, f)> \log(1/\delta) + 3\log(e\pi(1+2n))\big)\leq \delta.
%  \end{align*}
% \end{theorem}

%  \begin{proof}
% First if $pf=b$ then $f(k)=b$ for all $k$ such that $p_k>0$. In this case $\Kinf(\hp_n, pf, f)=0$ for all $n$ and the result is trivially true. We thus assume now that $pf<b$.

% The proof is a combination of the one of Proposition~13 by \citet{garivier2018kl} and the method of mixtures. We first define the martingale
% \[
% M_n^\lambda = \exp\left(\sum_{\ell=1}^n \log\left(1-\lambda \frac{f(X_\ell)-pf}{b-pf}\right)\right)\,,
% \]
% with the convention $M_0^\lambda= 1$. Indeed if we denote by $\cF_n = \sigma(X_1,\ldots,X_n)$ the information available at time $n$, we have
% \begin{align*}
%   \E\left[M_n^\lambda|\cF_{n-1}\right] = \E\left[1-\lambda\frac{f(X_n)-pf}{b-pf}\right] M_{n-1}^\lambda = M_{n-1}^\lambda\,.
% \end{align*}
% We fix a real number $\gamma_j = 1/(2j)$ for $j\in\N^*$and let $S_j$ be the set
% \[
% S_j= \Bigg\{ \frac{1}{2}-\Bigg\lfloor\frac{1}{2\gamma_j}\Bigg\rfloor\gamma_j, \dots,\frac{1}{2}-\gamma_j,\,\frac{1}{2},\,\frac{1}{2}+\gamma_j,\dots,\frac{1}{2}+\Bigg\lfloor\frac{1}{2\gamma_j}\Bigg\rfloor\gamma_j \Bigg\}\,.
% \]
% The cardinality of this set $S_j$ is bounded by $1 + 2j$. We choose a prior on $\lambda$ the mixture of uniform distribution over this grid: $6/\pi^2\sum_{j=1}^{\infty} 1/j^2 \cU(S_j)$. Thus we consider the integrated martingale
%  \begin{align}
%      M_n &= \frac{6}{\pi^2}\sum_{j=1}^{\infty} \frac{1}{j^2} \sum_{\lambda \in S_j} \frac{1}{|S_j|}M_n^{\lambda} \nonumber\\
%      &\geq \frac{6}{\pi^2 n^2 |S_n|} \max_{\lambda \in S_n}M_n^{\lambda}\nonumber\\
%      &\geq \frac{6}{\pi^2 (1+2n)^3}\max_{\lambda \in S_n}M_n^{\lambda}\,.\label{eq:lb_mixture_kinf}
%  \end{align}
% Lemma~\ref{lem:regularity_ln_lambda} below
% indicates that for all $\lambda\in[0,1]$, there exists a $\lambda'\in S_n$ such that for all $x\in[0,b]$,
% \begin{equation}
% \label{eq:2gamma}
% \log\!\Bigg(1-\lambda \, \frac{x-p f}{b-p f } \Bigg)\leq 2\gamma_n
% + \log\!\Bigg(1- \lambda' \frac{x-p f}{b-p f} \Bigg)\,.
% \end{equation}
% Now, a combination of
% the variational formula of Lemma~\ref{lem:var_form_Kinf}
% and of the inequality~\eqref{eq:2gamma} yields a finite maximum as an upper bound on $\Kinf(\hp_n,pf,f)$
% \begin{align*}
% \Kinf(\hp_n,pf,f)
% & = \max_{0\leq \lambda\leq 1} \frac{1}{n}\sum_{\ell=1}^{n} \log\!\Bigg(1-\lambda \frac{X_\ell-pf}{b-pf} \Bigg) \\
% & \leq 2 \gamma_n + \max_{\lambda' \in S_n} \frac{1}{n}\sum_{k=1}^{n}\log\!\Bigg(1-\lambda' \frac{X_\ell-pf}{b-pf} \Bigg)\,.
% \end{align*}
% Thanks to the definition of the martingale $M_n^\lambda$ we obtain
% \[
% \max_{\lambda \in S_n}M_n^{\lambda} \geq e^{-2n\gamma_n} e^{n\Kinf(\hp_n,pf,f)}= e^{-1} e^{n\Kinf(\hp_n,pf,f)}\,.
% \]
% Combining this inequality with \eqref{eq:lb_mixture_kinf} yields
% \[
%  M_n  \geq  \frac{6}{e\pi^2 (1+2n)^3}e^{n\Kinf(\hp_n,pf,f)}\,.
% \]
% Since for any supermartingale we have that
%  \begin{equation}\P\left(\exists n \in \N : M_n > 1/\delta\right) \leq \delta \cdot \E[M_0],\label{eq:supermartingale}\end{equation}
%  which is a well-known property of the method of mixtures \citep{de2004self}, we conclude that
%  \[
%  \P\left(\exists n\in \N^\star,\, n\Kinf(\hp_n,pf,f)> \log(1/\delta)+ 3\log(e\pi(1+2n))\right) \leq \delta\,.
%  \]
% \end{proof}

% \begin{lemma}[Lemma 19 by \citealp{garivier2018kl} and comment below]
% For all $\lambda, \lambda'\in[0,1]$ such that either $\lambda \leq \lambda'\leq 1/2$ or $1/2\leq \lambda'\leq \lambda$, for all real numbers $c\leq 1$,
% \begin{equation*}
% \log(1-\lambda c)-\log(1-\lambda' c)\leq 2|\lambda-\lambda'|\,.
% \end{equation*}
% \label{lem:regularity_ln_lambda}
% \end{lemma}

\subsection{Deviation inequality for sequence of Bernoulli random variables}

Below, we state the deviation inequality for Bernoulli distributions by \citet[Lemma F.4]{dann2017unifying}.
Let $\mathcal F_t$ for $t\in\N$ be a filtration and $(X_t)_{t\in\N^\star}$ be a sequence of Bernoulli random variables with $\P(X_t = 1 | \mathcal F_{t-1}) = P_t$ with $P_t$ being $\mathcal F_{t-1}$-measurable and $X_t$ being $\mathcal F_{t}$-measurable.

\begin{theorem}\label{th:bernoulli-deviation}
	For all $\delta>0$,
	\begin{align*}
	\P \left(\exists n : \,\, \sum_{t=1}^n X_t < \sum_{t=1}^n P_t / 2 -\log\frac{1}{\delta}  \right) \leq \delta.
	\end{align*}
\end{theorem}

% \begin{proof}
% 	$P_t - X_t$ is a martingale difference sequence with respect to the filtration $\mathcal F_t$.
% 	Since $X_t$ is nonnegative and has finite second moment, we have for any $\lambda > 0$ that
% 	$\E\left[e^{-\lambda (X_t - P_t)} | \mathcal F_{t-1} \right] \leq e^{\lambda^2 P_t / 2}$ (Exercise 2.9, \citealp{boucheron2013concentration}).
% 	Hence, we have
% 	\begin{align}
% 	\E\left[ e^{\lambda(P_t - X_t) - \lambda^2 P_t / 2} | \mathcal F_{t-1} \right] \leq 1
% 	\end{align}
% 	and by setting $\lambda \triangleq 1$, we see that
% 	\begin{align}
% 	M_n = e^{\sum_{t=1}^n (-X_t + P_t / 2)}
% 	\end{align}
% 	is a supermartingale.
% 	Therefore by Markov's inequality,
% 	\begin{align}
% 	\P \left( \sum_{t=1}^n (-X_t + P_t / 2) \geq \log\frac{1}{\delta} \right)
% 	=
% 	\P \left( M_n \geq \frac{1}{\delta} \right)
% 	\leq {\delta} \E[M_n] \leq \delta
% 	\end{align}
% 	which gives us
% 	\begin{align}
% 	\P \left( \sum_{t=1}^n X_t \leq \sum_{t=1}^n P_t / 2 -\log\frac{1}{\delta}\right)
% 	\leq \delta
% 	\end{align}
% 	for a fixed $n$.
% 	We define now the stopping time $\tau \triangleq \min\{ t \in \mathbb N \, : \, M_t > \frac{1}{\delta} \}$ and the sequence $\tau_n = \min\{ t \in \mathbb N \, : \, M_t > \frac{1}{\delta} \vee t \geq n \}$.
% 	Applying the convergence theorem for nonnegative supermartingales
% 	(Theorem~5.2.9 by \citealp{durrett2010probability}), we get that $\lim_{t \rightarrow
% 		\infty} M_t$ is well-defined almost surely. Therefore, $M_\tau$ is
% 	well-defined even when $\tau = \infty$.
% 	By the optional stopping theorem for nonnegative supermartingales (Theorem
% 	5.7.6 by \citealp{durrett2010probability}), we have $\E[M_{\tau_n}] \leq \E[M_0] \leq
% 	1$ for all $n$ and applying Fatou's lemma, we obtain
% 	$\E[M_\tau] = \E[\lim_{n \rightarrow \infty} M_{\tau_n}] \leq \lim \inf_{n \rightarrow \infty} \E[M_{\tau_n}] \leq 1$.
% 	Using Markov's inequality, we can finally bound
% 	\begin{align}
% 	\P\left( \exists n:\,\, \sum_{t=1}^n X_t < \frac 1 2 \sum_{t=1}^n P_t - \log\frac{1}{\delta} \right)
% 	\leq
% 	\P ( \tau < \infty) \leq \P ( M_{\tau} > \frac{1}{\delta})
% 	\leq \delta \E[M_{\tau}] \leq \delta.
% 	\end{align}
% \end{proof}


\subsection{Deviation inequality for bounded distributions}
Below, we state the self-normalized Bernstein-type inequality by \citet{domingues2020regret}. Let $(Y_t)_{t\in\N^\star}$, $(w_t)_{t\in\N^\star}$ be two sequences of random variables adapted to a filtration $(\cF_t)_{t\in\N}$. We assume that the weights are in the unit interval $w_t\in[0,1]$ and predictable, i.e. $\cF_{t-1}$ measurable. We also assume that the random variables $Y_t$  are bounded $|Y_t|\leq b$ and centered $\EEc{Y_t}{\cF_{t-1}} = 0$.
Consider the following quantities
\begin{align*}
		S_t \triangleq \sum_{s=1}^t w_s Y_s, \quad V_t \triangleq \sum_{s=1}^t w_s^2\cdot\EEc{Y_s^2}{\cF_{s-1}}, \quad \mbox{and} \quad W_t \triangleq \sum_{s=1}^t w_s
\end{align*}
and let $h(x) \triangleq (x+1) \log(x+1)-x$ be the CramÃ©r transform of a Poisson distribution of parameter~1.

\begin{theorem}[Bernstein-type concentration inequality]
  \label{th:bernstein}
	For all $\delta >0$,
	\begin{align*}
		\PP{\exists t\geq 1,   (V_t/b^2+1)h\left(\!\frac{b |S_t|}{V_t+b^2}\right) \geq \log(1/\delta) + \log\left(4e(2t+1)\!\right)}\leq \delta.
	\end{align*}
  The previous inequality can be weakened to obtain a more explicit bound: if $b\geq 1$ with probability at least $1-\delta$, for all $t\geq 1$,
 \[
 |S_t|\leq \sqrt{2V_t \log\left(4e(2t+1)/\delta\right)}+ 3b\log\left(4e(2t+1)/\delta\right)\,.
 \]
\end{theorem}

% \subsection{Deviation inequality for Dirichlet distribution}
% Below we provide the Bernstein-type inequality for weighted sum of Dirichlet distribution, using a result on upper bound on tails of Dirichlet boundary crossing (see Lemma~\ref{lem:upper_bound_dbc}).

% \begin{lemma}\label{lem:kinf_attains}
%      For any $p \in \simplex_m$, $f \colon \{0,\ldots,m\} \to [0,b]$ such that $f(0) = b$, $p_0 > 0$, and $\mu \in (p f, b)$ there exists a measure  $q \in \simplex_m$ such that $p \ll q $, $q f = \mu$ and \(\Kinf(p, \mu, f) = \KL(p, q)\).
% \end{lemma}
% \begin{proof}
%     By the variational form of $\Kinf$ (Lemma~\ref{lem:var_form_Kinf})
%     \[ 
%         \Kinf(p, \mu, f) = \max_{\lambda \in [0,1]} \E_{X \sim p}\left[ \log\left(1 - \lambda \frac{f(X) - \mu}{b - \mu} \right) \right] = \E_{X \sim p}\left[ \log\left(1 - \lambda^\star \frac{f(X) - \mu}{b - \mu} \right) \right].
%     \]
%     Note that $\PP{f(X) = b} > 0$ implies $\lambda^\star < 1$. Jensen's inequality and $\mu > pf$ imply $\lambda^\star > 0$. It is easy to check that $\lambda^\star$ satisfies
%     \[
%         \E\left[ \frac{1}{1 - \lambda^\star (f(X) - \mu)/(b-\mu)} \right] = \sum_{j=0}^m \frac{p(j)}{1 - \lambda^\star (f(j) - \mu)/(b-\mu)} = 1,
%     \]
%     and 
%     \begin{equation}\label{eq: kinf_attains1}
%         \E\left[ \frac{f(X) - \mu}{1 - \lambda^\star (f(X) - \mu)/(b-\mu)} \right] = \sum_{j=0}^m \frac{p(j) (f(j) - \mu)}{1 - \lambda^\star (f(j) - \mu)/(b-\mu)} = 0.
%     \end{equation}
%     Define $q(j) = \frac{p(j)}{1 - \lambda^\star (f(j) - \mu)/(b-\mu)}, j = 0, \ldots, m$, and let $q = (q_0, \ldots, q_m)$. Clearly, $q \in \simplex_m$, $qf = \mu$ by \eqref{eq: kinf_attains1} and $p \ll q$. Moreover,
%     \[
%         \Kinf(p, \mu, f) = \E_{X \sim p}\left[ \log\left(1 - \lambda^\star \frac{f(X) - \mu}{b - \mu} \right) \right] = \E_{p}\left[ \log \frac{\rmd p}{\rmd q}\right] = \KL(p,q).
%     \]
% \end{proof}


% \begin{lemma}\label{lem:bernstein_dirichlet}
%      For any $\alpha = (\alpha_0, \alpha_1, \ldots, \alpha_m) \in \N^{m+1}$ define  $\up \in \simplex_{m}$ such that $\up(\ell) = \alpha_\ell/\ualpha, \ell = 0, \ldots, m$, where $\ualpha = \sum_{j=0}^m \alpha_j$. Then for any $f \colon \{0,\ldots,m\} \to [0,b]$ such that $f(0) = b$ and $\delta \in (0,1)$
%      \[
%         \P_{w \sim \Dir(\alpha)}\left[wf \geq \up f +  2 \sqrt{ \frac{ \Var_{\up}(f) \log(1/\delta)}{\ualpha}} + \frac{2b\sqrt 2  \cdot \log(1/\delta)}{\ualpha} \right] \leq \delta.
%      \]
% \end{lemma}
% \begin{proof}
% Fix $\delta \in (0,1)$ and let $\mu \in (\up f,b)$ be such that
% \[
%     \Kinf(\up, \mu, f) = \ualpha^{-1} \log (1/\delta). 
% \]
% Note that such $\mu$ exists. Indeed, it follows from the continuity of $\Kinf$ w.r.t. the second argument, see \citet[Theorem 7]{honda2010asymptotically}. By Lemma~\ref{lem:kinf_attains} there exists $q$ such that $\up \ll q $, $q f = \mu$ and $\KL(\up, q) = \ualpha^{-1} \log (1/\delta)$. By Lemma \ref{lem:upper_bound_dbc}   
% \begin{equation}
% \label{eq: bernstein 1}
%  \P_{w \sim \Dir(\alpha)}[wf \geq qf] = \P_{w \sim \Dir(\alpha)}[wf \geq \mu] \leq \exp\left( -\ualpha \Kinf(\up, \mu, f) \right) = \delta.   
% \end{equation}
% By Lemma \ref{lem:Bernstein_via_kl}
% \[
% q f  - \up f \le \sqrt{2\Var_{q}(f)\KL(\up ,q)}.
% \]
% By Lemma \ref{lem:switch_variance_bis},  $\Var_q(f) \leq 2\Var_{\up}(f) +4b^2 \KL(\up ,q)$.
% The last two inequalities and \eqref{eq: bernstein 1} imply that
% \begin{equation*}
%   \P_{w \sim \Dir(\alpha)}\left[wf - \up f \geq \sqrt{4 \Var_{\up}(f) \KL(\up ,q)} + 2 b \sqrt 2 \cdot      \KL(\up,q) \right] \le \delta. 
% \end{equation*}
  
% \end{proof}

