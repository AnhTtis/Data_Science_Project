\section{Notation}\label{app:notations}


\begin{table}[h!]
	\centering
	\caption{Table of notation use throughout the paper}
	\begin{tabular}{@{}l|l@{}}
		\toprule
		{Notation} & \thead{Meaning} \\ \midrule
	$\cS$ & state space of size $S$\\
	$\cA$ & action space of size $A$\\
	$H$ & length of one episode\\
    $s_1$ & initial state \\ 
    $\tau$ & stopping time \\
    $\cT$ & trajectory space, $\cT \triangleq (\cS \times \cA)^H$ \\
    $\varepsilon$ & desired accuracy of solving the problem \\
    $\delta$ & desired upper bound on failure probability \\
    $\lambda$ & regularization parameter in Regularized MDPs (see Appendix~\ref{app:regularized_mdp}). \\
    \hline 
	$p_h(s'|s,a)$ & probability transition \\
    $r_h(s,a)$ & reward function \\
    $d^{\pi}_h(s,a)$ & state-action visitation distribution (state-action occupancy measure) at step $h$ for the policy $\pi$ \\
    $q^\pi(m)$ & visitation probability of trajectory $m \in \cT$ by policy $\pi$ \\
    $\cK_p$ & polytope of all admissible state-action visitation distributions \\
    $\cK$ &  polytope of all admissible distributions over state-actions, $\cK \triangleq (\simplex_{SA})^H$ \\
	$\VE(d^\pi)$ & visitation entropy $\VE(d^\pi) \triangleq \sum_{h=1}^H \cH(d^\pi_h)$ for $d^\pi \in \cK_p$, \\
    $\pistarVE$ & policy that maximizes $\VE(d^\pi)$, a solution to the MVEE problem \\
    $\TE(q^\pi)$ & trajectory entropy $\TE(q^\pi) \triangleq \cH(q^\pi)$ \\
    $\pistarTE$ & policy that maximizes $\TE(q^\pi)$, a solution to the MTEE problem \\
	\hline
	$n_0$ & number of prior visits for the forecaster-player in \EntGame  \\
    $t_0$ & total number of prior visits \\ 
	$s^{\,t}_h$ & state that was visited at $h$ step during $t$ episode \\
	$a^{\,t}_h$ & action that was picked at $h$ step during $t$ episode \\
	$n_h^t(s,a)$ & number of visits of state-action $n_h^t(s,a) = \sum_{k = 1}^t  \ind{\left\{(s_h^k,a_h^k) = (s,a)\right\}}$\\
	$n_h^t(s'|s,a)$ & number of transition to $s'$ from state-action $n_h^t(s'|s,a) = \sum_{k = 1}^t  \ind{\left\{(s_h^k,a_h^k, s_{h+1}^k) = (s,a,s')\right\}}$. \\
	$\upn_h^t(s,a)$ & pseudo number of visits of state-action $\upn_h^t(s,a)=n_h^t(s,a)+n_0$\\
	$\hp_h^{\,t}(s'|s,a)$ & empirical probability transition $\hp_h^{\,t}(s'|s,a) = n_h^t(s'|s,a) / n_h^t(s,a)$ \\
	\hline
    $\bd^t_h(s,a)$ & predicted distribution by the forecaster-player in \EntGame $\bd^t_h(s,a) \triangleq \upn^{t-1}_h(s,a) / (t + t_0)$ \\
    $\uQ_h^t(s,a)$,$\uV_h^t(s,a)$ & for \EntGame: upper bound on the optimal Q/V-functions in a MDP with rewards $\log(1/\bd^{t+1}_h(s,a)))$\\
    \hline 
    $Q^{\pi}_h(s,a)$, $V^{\pi}_h(s,a)$ & Q- and V-functions for the MTEE problem \\
    $\Qstar_h(s,a)$, $\Vstar_h(s,a)$ & optimal Q- and V-function for the MTEE problem \\
    $\uQ^t_h(s,a)$, $\uV^t_h(s,a)$ & for \UCBVIEnt: the upper bound on the optimal Q/V-functions for the MTEE problem \\
    $\lQ^t_h(s,a)$, $\lV^t_h(s,a)$ & for \UCBVIEnt: the lower bound on the optimal Q/V-functions for the MTEE problem \\
    $Q^{\pi}_{\lambda,h}(s,a)$, $V^{\pi}_{\lambda,h}(s,a)$ & Q- and V-functions in a regularized MDP \\
    $\Qstar_{\lambda,h}(s,a)$, $\Vstar_{\lambda,h}(s,a)$ & optimal Q- and V-function in a regularized MDP \\
    \hline
    $\R_+$ & non-negative real numbers  \\
    $\N_+$ & positive natural numbers \\
    $[n]$ & set $\{1,2,\ldots, n\}$\\
    $\rme$ & Euler's number \\
    $\simplex_d$ & $d+1$-dimensional probability simplex: $\simplex_d \triangleq \{x \in \R_{+}^{d}: \sum_{j=1}^{d} x_j = 1 \}$ \\ 
    $\simplex_{\cX}$ & set of distributions over a finite set $\cX$ : $\simplex_\cX = \simplex_{\vert \cX \vert}$. \\
    $\cH(p)$ & Shannon entropy for $p \in \simplex_{\cX}$, $\cH(p) \triangleq \sum_{i \in \cX} p_i \log(1/p_i)$ \\
    $\clip(x,m,M)$ & clipping procedure $\clip(x,m,M) \triangleq \max(\min(x,M), m)$ \\
    \bottomrule
	\end{tabular}
\end{table}



Let $(\Xset,\Xsigma)$ be a measurable space and $\Pens(\Xset)$ be the set of all probability measures on this space. For $p \in \Pens(\Xset)$ we denote by $\E_p$ the expectation w.r.t. $p$. For random variable $\xi: \Xset \to \R$ notation $\xi \sim p$ means $\operatorname{Law}(\xi) = p$. We also write $\E_{\xi \sim p}$ instead of $\E_{p}$.  For any $p, q \in \Pens(\Xset)$ the Kullback-Leibler divergence $\KL(p, q)$ is given by
$$
\KL(p, q) = \begin{cases}
\E_{p}[\log \frac{\rmd p}{\rmd q}], & p \ll q \\
+ \infty, & \text{otherwise}
\end{cases} 
$$
For any $p \in \Pens(\Xset)$ and $f: \Xset \to \R$, $p f = \E_p[f]$. In particular, for any $p \in \simplex_d$ and $f: \{0, \ldots, d\}   \to  \R$, $pf =  \sum_{\ell = 0}^d f(\ell) p(\ell)$. Define $\Var_{p}(f) = \E_{s' \sim p} \big[(f(s')-p f)^2\big] = p[f^2] - (pf)^2$. For any $(s,a) \in \cS$, transition kernel $p(s,a) \in \Pens(\cS)$ and $f \colon \cS \to \R$ define $pf(s,a) = \E_{p(s,a)}[f]$ and $\Var_{p}[f](s,a) = \Var_{p(s,a)}[f]$. For any $s\in \cS$, policy $\pi(s) \in \Pens(\cS)$ and $f \colon \cS \times \cA \to \R$ define $\pi f(s) = \E_{a \sim \pi(s)}[f(s,a)]$ and $\Var_{\pi} f(s) = \Var_{a \sim \pi(s)}[f(s,a)]$.

For a MDP $\cM$,a policy $\pi$ and a sequence of function $f_{h}$ define \(\E_{\pi}[ \sum_{h'=h}^H f(s_{h'}, a_{h'}) | s_h] \) as a conditional expectation of $\sum_{f(s_{h'}, a_{h'})}$ with respect to the sigma-algebra $\cF_h = \sigma\{ (s_{h'}, a_{h'}) | h' \leq h \}$, where for any $h\in[H]$ we have $a_h \sim \pi(s_h), s_{h+1} \sim p_h(s_h, a_h)$.

We write $f(S,A,H,\varepsilon) = \cO(g(S,A,H,\varepsilon,\delta))$ if there exist $ S_0, A_0, H_0, \varepsilon_0, \delta_0$ and constant $C_{f,g}$ such that for any $S \geq S_0, A \geq A_0, H \geq H_0, \varepsilon < \varepsilon_0, \delta < \delta_0, f(S,A,H,T,\delta) \leq C_{f,g} \cdot g(S,A,H,T,\delta)$. We write $f(S,A,H,\varepsilon,\delta) = \tcO(g(S,A,H,\varepsilon,\delta))$ if $C_{f,g}$ in the previous definition is poly-logarithmic in $S,A,H,1/\varepsilon,1/\delta$.

%For $\lambda > 0$ we define $\Exponential(\lambda)$ as an exponential distribution with a parameter $\lambda$. For $k, \theta > 0$ define $\Gamma(k,\theta)$ as a gamma-distribution with a shape parameter $k$ and a rate parameter $\theta$. For set $\Xset$ such that $\vert \Xset \vert < \infty$ define $\Unif(\Xset)$ as a uniform distribution over this set. In particular, $\Unif[N]$ is a uniform distribution over a set $[N]$.


% For independent (resp. i.i.d.) random variables $\xi_\ell \mysim p_\ell$ (resp. $\xi_\ell \mysimiid p$), $\ell = 1, \ldots, d$, we will write $\E_{\xi_\ell \mysim p_\ell}$ (resp. $\E_{\xi_\ell \mysimiid p}$), to denote expectation w.r.t. product measure on $(\Xset^d, \Xsigma^{\otimes d})$.