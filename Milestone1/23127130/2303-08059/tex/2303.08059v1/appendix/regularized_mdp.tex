\section{Sample Complexity for Regularized MDPs}\label{app:regularized_mdp}

In this section we describe the general setting of regularized MDPs, not only entropy-regularized.

\subsection{Preliminaries}

First we define class of regularizers we are interested in. For more exposition on this definition, see \cite{bubeck2015convex}. 
\begin{definition}\label{def:mirror_map}
    Let $\Phi \colon \simplex_{\cA} \to \R$ be a proper closed strongly-convex function. We will call $\Phi$ a mirror-map if the following holds 
\begin{itemize}
    \item $\Phi$ is $1$-strongly convex with respect to norm $\norm{\cdot}$;
    \item $\nabla \Phi$ takes all possible values in $\R^{\cA}$;
    \item $\nabla \Phi$ diverges on the boundary of $\simplex_{\cA}$: $\lim_{x \in \partial \simplex_{\cA}} \norm{\nabla \Phi(x)} = +\infty$;
\end{itemize}
\end{definition}

We explain three main examples of a such regularizers.
\begin{itemize}
    \item The negative Shannon entropy $\Phi(\pi) = -\cH(\pi)$ for $\cH(\pi) = \sum_{a\in\cA} \pi_a \log\left(\frac{1}{\pi_a}\right)$ satisfies the Definition~\ref{def:mirror_map} for $\ell_1$-norm;
    \item The negative Tsallis entropy $\Phi(\pi) = - \frac{1}{q}\cT_{q}(\pi)$ for $\cT_q(\pi) = \frac{1}{q - 1} \left(1 - \sum_{a\in\cA} \pi_a^{q} \right)$ satisfied the Definition~\ref{def:mirror_map} for $\ell_2$ norm for every $q\in(0,1)$. In particular, $q=0.5$ corresponds to the choice by \citet{grill2019planning} in Appendix E that is tightly connected to the UCB algorithm;
    \item For any other fixed policy $\pi'\in\simplex_{\cA}$ we can choose $\Phi(\pi) = \KL(\pi, \pi') = \sum_{a\in\cA} \pi_a \log\left( \frac{\pi_a}{\pi'_a}\right)$ that inherits all the properties from the choice of the negative entropy.
\end{itemize}


Let $\cM = (\cS, \cA, \{p_h\}_{h\in[H]}, \{r_h\}_{h\in[H]}, s_1)$ be a finite-horizon MDP, where $r_h(s,a)$ is a deterministic reward function. For simplicity we assume that $0 \leq r_h(s,a) \leq r_{\max}$ for any $(h,s,a) \in [H] \times \cS \times \cA$. 

Then we can define entropy-augmented rewards as follows
\[
    r_{\mu,h}(s,a) = r_h(s,a) + \mu \cH(p_h(s,a)).
\]
This definition is required to cover the following case of practical interest. Let $\mu = \lambda$ and $\Phi(\pi) = -\cH(\pi)$, then we obtain the following representation for the $\lambda$-regularized value function
\[
    V^{\pi}_{\lambda,1}(s_1) = V^\pi_1(s_1) + \lambda \TE(q^\pi),
\]
where $V^\pi_1(s_1)$ is a usual value function for a MDP $\cM$. For $r_{\max} = 0$  and $\mu=\lambda=1$ we recover just a trajectory entropy $V^{\pi}_{\lambda,1}(s_1) = \TE(q^\pi)$.



Next we define a convex conjugate to $\lambda \Phi$ as $F_\lambda \colon \R^{\cA} \to \R$
\[
    F_\lambda (x) = \max_{\pi \in \simplex_{\cA}} \{ \langle \pi, x \rangle - \lambda \Phi(\pi) \}
\]
and, with a sight abuse of notation extend the action of this function to the $Q$-function as follows
\[
    \Vstar_{\lambda,h}(s) = F_{\lambda}(\Qstar_{\lambda,h})(s) = \max_{\pi \in \Delta_{\cA}}\left\{ \pi \Qstar_{\lambda,h}(s) - \lambda \Phi(\pi) \right\}.
\]

Thanks to the fact that $\Phi$ satisfies Definition~\ref{def:mirror_map}, we have exact formula for the optimal policy by Fenchel-Legendre transform
\[
    \pi^\star_h(s) = \argmax_{\pi \in \Delta_{\cA}}\left\{ \pi \Qstar_{\lambda,h}(s) - \lambda \Phi(\pi) \right\} = \nabla F_\lambda(\Qstar_{\lambda,h}(s,\cdot)).
\]
Notice that we have $\nabla F_\lambda(\Qstar_{\lambda,h}(s,\cdot)) \in \simplex_{\cA}$ since the gradient of $\Phi$ diverges on the boundary of $\simplex_{\cA}$. For entropy regularization this formula become the softmax function.

Finally, it is known that the smoothness property of $F_{\lambda}$ plays a key role in reduced sample complexity for planning in regularized MDPs \cite{grill2019planning}. For our general setting we have that since $\lambda \Phi$ is $\lambda$-strongly convex with respect to $\norm{\cdot}$, then $F_{\lambda}$ is $1/\lambda$-strongly smooth with respect to a dual norm $\norm{\cdot}_*$
\[
    F_\lambda(x) \leq F_\lambda(x') + \langle \nabla F_\lambda(x'), x-x' \rangle + \frac{1}{2\lambda} \norm{x - x'}_*^2.
\]
Let us define $\Rphi$ as a maximal possible value of $\vert \Phi\vert$. Without loss of generality assume that $\Phi \leq 0$. In this case we define $\Rmax = r_{\max} + \mu \log(S) + \lambda \Rphi $ as an upper bound of an about of reward obtain at the one step. By this definition we have $0 \leq V^{\pi}_{\lambda,h}(s) \leq H \Rmax$ for any $h\in[H], s \in \cS$ and any policy $\pi$.

Also, since all norms in $\R^\cA$ are equivalent, we define a constant $r_A$ that defined for a dual norm $\norm{\cdot}_*$ as follows
\[
    \norm{\cdot }_* \leq r_A \cdot \norm{\cdot}_\infty.
\]
For example, for $\ell_2$-norm $r_A = \sqrt{A}$ and for $\ell_1$-norm $r_A = A$. In the case $\Phi = -\cH$ we have $r_A = 1$ since the entropy is $1$-strongly convex with respect to a $\ell_1$-norm, thus the dual norm is exactly a $\ell_\infty$-norm.

The rest of this section is devoted to obtain the sample complexity guarantees for \UCBVIEnt algorithm with two different choices of the stopping rule:
\begin{itemize}
    \item Regularization-aware stopping rule \eqref{eq:def_tau_lambda} with the gap notion \eqref{eq:def_gap_lambda}. In this case we can prove Theorem~\ref{th:reg_aware_sample_complexity} that gives $\tcO\left(\frac{H^6S^2A}{\lambda \varepsilon}\right)$ sample complexity guarantee ignoring $\Rmax$,$r_A$ and poly-logarithmic factors;
    \item Regularization-agnostic stopping rule \eqref{eq:def_tau_agnostic} with the gap notion \eqref{eq:def_gap_agnostic}. In this case Theorem~\ref{th:reg_agnostic_sample_complexity} gives us $\tcO\left(\frac{H^3SA}{\varepsilon^2} + \frac{H^3 S^2A}{\varepsilon}\right)$ sample complexity guarantee ignoring $\Rmax$ and poly-logarithmic factors. Notably, this sample complexity result does depend directly on $\lambda$ and $r_A$.
\end{itemize}
As a corollary of these general results, we obtain algorithm for the MTEE problem with sample complexity $\tcO\left( H^6 S^2 A / \varepsilon \right)$ by taking as a regularizer negative entropy,  $\lambda = \mu = 1$ and $r_{\max} = 0$.

\newpage
\subsection{Concentration Events}
Following the ideas of \cite{menard2021fast}, we define the following concentration events. 

Let $\beta^{\KL}, \beta^{\conc}, \beta^{\cnt}, \beta^{\cH}: (0,1) \times \N \to \R_{+}$ be some functions defined later on in Lemma \ref{lem:proba_master_event}. We define the following favorable events
\begin{align*}
\cE^{\KL}(\delta) &\triangleq \Bigg\{ \forall t \in \N, \forall h \in [H], \forall (s,a) \in \cS\times\cA: \quad \KL(\hp^{\,t}_h(s,a), p_h(s,a)) \leq \frac{\beta^{\KL}(\delta, n^{\,t}_h(s,a))}{n^{\,t}_h(s,a)} \Bigg\},\\
\cE^{\conc}_H(\delta) &\triangleq \Bigg\{\forall t \in \N, \forall h \in [H], \forall (s,a)\in\cS\times\cA: \\
&\qquad|(\hp_h^t -p_h) \Vstar_{\lambda, h+1}(s,a)| \leq \sqrt{\frac{2 H^2 \Rmax^2 \beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} \Bigg\},\\
\cE^{\conc}_B(\delta) &\triangleq \Bigg\{\forall t \in \N, \forall h \in [H], \forall (s,a)\in\cS\times\cA: \\
&\qquad|(\hp_h^t -p_h) \Vstar_{\lambda,h+1}(s,a)| \leq \sqrt{2 \Var_{p_h}(\Vstar_{\lambda,h+1})(s,a)\frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} + 3 H \Rmax \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}\Bigg\},\\
\cE^{\cnt}(\delta) &\triangleq \Bigg\{ \forall t \in \N, \forall h \in [H], \forall (s,a) \in \cS\times\cA: \quad n^t_h(s,a) \geq \frac{1}{2} \upn^t_h(s,a) - \beta^{\cnt}(\delta) \Bigg\},\\
\cE^{\cH}(\delta) &\triangleq \Bigg\{\forall t \in \N, \forall h \in [H], \forall (s,a)\in\cS\times\cA: \\
&\qquad  \vert \cH(\hp^t_h(s,a)) - \cH(p_h(s,a)) \vert \leq  \sqrt{\frac{2 \beta^{\cH}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + \left(\frac{\beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} \wedge \log(S) \right)
\Bigg\}.
\end{align*}
We also introduce two intersections of these events of interest, $\cG_H(\delta) \triangleq \cE^{\KL}(\delta) \cap \cE^{\conc}_H(\delta) \cap \cE^{\cnt}(\delta) \cap \cE^{\cH}(\delta)$ and $\cG_B(\delta) \triangleq \cE^{\KL}(\delta) \cap \cE^{\conc}_B(\delta) \cap \cE^{\cnt}(\delta) \cap \cE^{\cH}(\delta)$. We  prove that for the right choice of the functions $\beta^{\KL}, \beta^{\conc},\beta^{\cnt}, \beta^{\cH}$ the above events hold with high probability.
\begin{lemma}
\label{lem:proba_master_event}
For any $\delta \in (0,1)$ and for the following choices of functions $\beta,$
\begin{align*}
    \beta^{\KL}(\delta, n) & \triangleq \log(4SAH/\delta) + S\log\left(\rme(1+n) \right), \\
    \beta^{\conc}(\delta, n) &\triangleq \log(4SAH/\delta) + \log(4\rme n(2n+1)) ,\\
    \beta^{\cnt}(\delta) &\triangleq \log(4SAH/\delta), \\
    \beta^{\cH}(\delta,n) &\triangleq \log^2(n)\left(\log(4SAH/\delta) + \log(n(n+1))\right),
\end{align*}
it holds that
\begin{align*}
\P[\cE^{\KL}(\delta)]&\geq 1-\delta/4, \qquad \P[\cE^{\conc}_H(\delta)]\geq 1-\delta/4, \qquad \P[\cE^{\conc}_B(\delta)]\geq 1-\delta/4, \\
\P[\cE^\cnt(\delta)]&\geq 1-\delta/4,  \qquad \P[\cE^{\cH}(\delta)]\geq 1-\delta/4.
\end{align*}
In particular, $\P[\cG_H(\delta)] \geq 1-\delta$ and $\P[\cG_B(\delta)] \geq 1-\delta$.
\end{lemma}
\begin{proof}
Applying Theorem~\ref{th:max_ineq_categorical} and the union bound over $h \in [H], (s,a) \in \cS \times \cA$ we get $\P[\cE^{\KL}(\delta)]\geq 1-\delta/4$. 

Let us call the empirical model constructed based on $n$ sampled as $\hp^{[n]}_h$. Let us fix $n,s,a,h$. Then by Azuma-Hoeffding inequality we have
\[
    \P\left[ \vert [\hp^{[n]}_h - p_h]\Vstar_{\lambda, h+1}(s,a) \vert > \sqrt{\frac{2 H^2 \Rmax^2\beta^{\conc}(\delta,n)}{n}} \right] \leq \frac{\delta}{4SAH \cdot 2n(n+1)}.
\]
Union bound over $n,s,a,h$ concludes show that $\P[\cE^{\conc}_H(\delta)] \geq 1 - \delta/4$.

Next, Theorem~\ref{th:bernstein} and the union bound over $h \in [H], (s,a) \in \cS \times \cA$ yield $\P[\cE^{\conc}_B(\delta)]\geq 1 - \delta/4$. By Theorem~\ref{th:bernoulli-deviation} and union bound,  $\P[\cE^{\cnt}(\delta)]\geq 1 - \delta/4$.  Finally, by Theorem~\ref{th:entropy_concentration} and union bound over $(s,a,h) \in \cS \times \cA \times [H]$ $\P[\cE^{\cH}(\delta)]\geq 1-\delta/4$. The union bound over four prescribed events concludes $\P[\cG_H(\delta)] \geq 1 - \delta$ and $\P[\cG_B(\delta)] \geq 1 - \delta$.
\end{proof}


\begin{lemma}\label{lem:reg_directional_concentration}
      Assume conditions of Lemma \ref{lem:proba_master_event}. Then on event $\cE^{\KL}(\delta)$, for any $f \colon \cS \to [0, H \Rmax]$, $t \in \N, h \in [H], (s,a) \in \cS \times \cA$,
      \begin{align*}
            [p_h - \hp_h^t]f(s,a) &\leq \frac{1}{H} \hp^t_h f(s,a) + 2H\Rmax \left(\frac{2H \beta^{\KL}(\delta, n^{\,t}_h(s,a))}{n^{\,t}_h(s,a)} \wedge 1 \right), \\
            [\hp_h^t -p_h]f(s,a) &\leq \frac{1}{H} p_h f(s,a) + 2H\Rmax \left(\frac{2H \beta^{\KL}(\delta, n^{\,t}_h(s,a))}{n^{\,t}_h(s,a)} \wedge 1 \right). 
      \end{align*}
\end{lemma}
\begin{proof}
    Let us start from the first statement.  We apply the second inequality of Lemma~\ref{lem:Bernstein_via_kl} and Lemma~\ref{lem:switch_variance_bis} to obtain
    \begin{align*}
        [p_h - \hp_h^t]f(s,a) &\leq \sqrt{2\Var_{p_h}[f](s,a) \cdot \KL(\hp_h^t, p_h) } \\
        &\leq 2\sqrt{\Var_{\hp^t_h}[f](s,a) \cdot \KL(\hp_h^t, p_h) } +  3 H \Rmax \KL(\hp^t_h, p_h).
    \end{align*}
    Since $0 \leq f(s) \leq  H\Rmax$ we get
    \[
        \Var_{\hp^t_h}[f](s,a) \leq \hp^t_h[f^2](s,a) \leq  H\Rmax \cdot \hp^t_h f(s,a).
    \]
    Finally, applying $2\sqrt{ab} \leq a+b, a, b \geq 0$, we obtain the following inequality
    \begin{align*}
        (\hp_h^t -p_h)f(s,a) &\leq \frac{1}{H} \hp^t_h f(s,a) + 4H^2\Rmax \KL(\hp_h^t, p_h).
    \end{align*}
    Definition of $\cE^{\KL}(\delta)$ implies the part of the statement. At the same time we have a trivial bound since $f(s) \in [0, H \Rmax]$
    \[
        [p_h - \hp^t_h] f(s,a) \leq 2H\Rmax \leq \frac{1}{H} \hp^t_h f(s,a) + 2H\Rmax.
    \]

    To prove the second statement, apply the first inequality of Lemma~\ref{lem:Bernstein_via_kl} and proceed similarly.
\end{proof}


\begin{lemma}\label{lem:empirical_bernstein}
    Assume conditions of Lemma \ref{lem:proba_master_event} and assume that $\beta^{\conc}_B(\delta) \leq \beta^{\KL}(\delta)$. Then conditioned on event $\cG_B(\delta)$, for any $U \colon \cS \to [0, H \Rmax]$, $t \in \N, h \in [H], (s,a) \in \cS \times \cA$,
      \begin{align*}
            \vert (\hp_h^t -p_h)\Vstar_{\lambda,h+1}(s,a) \vert &\leq 3 \sqrt{\Var_{\hp^t_{h+1}}(U)(s,a) \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} + \frac{9H^2\Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} \\
            &+ \frac{1}{H} \hp^t_h \vert U - \Vstar_{\lambda,h+1} \vert(s,a).
      \end{align*}
\end{lemma}
\begin{proof}
    First, we apply the definition of event $\cE^{\conc}(\delta)$
    \[
        \vert (\hp_h^t -p_h)\Vstar_{\lambda,h+1}(s,a) \vert \leq \sqrt{2 \Var_{p_h}(\Vstar_{\lambda,h+1})(s,a)\frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} + 3 H \Rmax \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}.
    \]
    Next we apply Lemma~\ref{lem:switch_variance_bis} and Lemma~\ref{lem:switch_variance} and obtain
    \begin{align*}
        \Var_{p_h}(\Vstar_{\lambda,h+1})(s,a) &\leq 2 \Var_{\hp^t_h}(\Vstar_{\lambda,h+1})(s,a) + 4H^2 \Rmax^2 \KL(\hp^t_h(s,a), p_h(s,a)) \\
        &\leq 4\Var_{\hp^t_{h+1}}(U)(s,a) + 4H\Rmax \hp^t_h \vert U - \Vstar_{\lambda,h+1} \vert(s,a) +  4H^2 \Rmax^2 \KL(\hp^t_h(s,a), p_h(s,a)).
    \end{align*}
    Thus, by inequality $\sqrt{a+b} \leq \sqrt{a} + \sqrt{b}$.
    \begin{align*}
        \vert (\hp_h^t -p_h)\Vstar_{\lambda,h+1}(s,a) \vert &\leq 3 \sqrt{\Var_{\hp^t_{h+1}}(U)(s,a) \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} + 3 \sqrt{H\Rmax \hp^t_h \vert U - \Vstar_{\lambda,h+1}\vert(s,a) \cdot \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} \\
        &+ 3H\Rmax\sqrt{\KL(\hp^t_h(s,a), p_h(s,a)) \cdot \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} + 3H\Rmax \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}.
    \end{align*}
    By inequality $2\sqrt{ab} \leq a+b$ we have
    \[
        3 \sqrt{H\Rmax \hp^t_h \vert U - \Vstar_{\lambda,h+1}\vert(s,a) \cdot \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} \leq \frac{1}{H}\hp^t_h \vert U - \Vstar_{\lambda,h+1}\vert(s,a) + \frac{9 H^2 \Rmax\beta^{\conc}(\delta,n_h^t(s,a))}{4 n^t_h(s,a)}.
    \]
    By the definition of the event $\cE^{\KL}(\delta)$ and the fact $\beta^{\conc }(\delta) \leq \beta^{\KL}(\delta)$ we have
    \[
        \sqrt{\KL(\hp^t_h(s,a), p_h(s,a)) \cdot \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} \leq \frac{\beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)}.
    \]
\end{proof}

\subsection{Confidence Intervals}

Similar to \citet{azar2017minimax,Zanette19Euler,menard2021fast}, we define the upper confidence bound for the optimal regularized  Q-function of two types: with Hoeffding bonuses and with Bernstein bonuses. 

Let us define empirical estimate of entropy-augmented rewards as follows
\[
    \hat r^t_{\mu,h}(s,a) = r_h(s,a) + \mu \cH(\hp^t_h(s,a)).
\]
Then we have the following sequences defined as follows
\begin{align*}
    \uQ^{t}_{h}(s,a) &= \clip\left( \hat r^t_{\mu,h}(s,a) + \hp^t_h \uV^t_h(s,a) + b^{p,t}_h(s,a) + \mu b^{\cH, t}_h(s,a),0,H\Rmax \right) \\
    \pi^{t+1}_h(s) &= \max_{\pi \in \simplex_\cA} \{ \pi \uQ^t_h(s) -\lambda \Phi(\pi)\}, \\
    \uV^t_h(s) &= \cH(\pi^{t+1}_h(s)) + \pi^{t+1}_h \uQ^t_h(s)  \\
    \uV^t_{H+1}(s) &= 0,
\end{align*}
and the lower confidence bound as follows
\begin{align*}
    \lQ^{t}_{h}(s,a) &= \clip\left( \hat r^t_{\mu,h}(s,a) + \hp^t_h \lV^t_h(s,a) - b^{p,t}_h(s,a) - \mu b^{\cH, t}_h(s,a),0,H \Rmax\right) \\
    \lV^t_h(s) &= \max_{\pi \in \simplex_\cA }\{ \pi \lQ^t_h(s) - \lambda\Phi(\pi)  \} \\
    \lV^t_{H+1}(s) &= 0,
\end{align*}
where we have two types of transition bonuses that will be specified before use. The Hoeffding bonuses defined as follows
\begin{align}\label{eq:hoeffding_transition_bonuses}
    b^{p,t}_h(s,a) &\triangleq \sqrt{\frac{2H^2\Rmax^2 \beta^{\conc}(\delta, n^t_h(s,a))]}{n^t_h(s,a)}},
\end{align}
and the Bernstein bonuses are defined as follows
\begin{align}\label{eq:bernstein_transition_bonuses}
    \begin{split}
        b^{p,t}_h(s,a) &\triangleq b^{B,t}_h(s,a) + b^{\corr,t}_h(s,a),\\
        b^{B,t}_h(s,a) &\triangleq 3\sqrt{\Var_{\hp^t_h}(\uV^t_{h+1})(s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + \frac{9H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)}, \\
        b^{\corr,t}_h(s,a) &\triangleq \frac{1}{H} \hp^t_h(\uV^t_{h+1} - \lV^t_{h+1})(s,a).
    \end{split}
\end{align}
The entropy bonuses are defined bellow
\begin{align}\label{eq:entropy_bonuses}
    b^{\cH, t}_h(s,a) &\triangleq \sqrt{\frac{2 \beta^{\cH}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + \left(\frac{\beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} \wedge \log(S) \right).
\end{align}

\begin{theorem}\label{th:reg_confidence_intervals}
    Let $\delta \in (0,1)$. Assume Hoeffding bonuses \eqref{eq:hoeffding_transition_bonuses}. Then on event $\cG_H(\delta)$ for any $t \in \N$, $(h,s,a) \in [H]\times \cS \times \cA$ it holds
    \[
        \lQ^t_h(s,a) \leq \Qstar_{\lambda,h}(s,a) \leq \uQ^t_h(s,a), \qquad \lV^t_h(s) \leq \Vstar_{\lambda,h}(s) \leq \uV^{t}_h(s,a).
    \]
    The same inequalities hold on event $\cG_B(\delta)$ if we assume Bernstein bonuses \eqref{eq:bernstein_transition_bonuses}.
\end{theorem}
\begin{proof}
    Proceed by induction over $h$. For $h = H+1$ the statement is trivial. Now we assume that inequality holds for any $h' > h$ for a fixed $h \in [H]$. Fix a timestamp $t \in \N$ and a state-action pair $(s,a)$ and assume that $\uQ^t_h(s,a) < H\Rmax$, i.e. no clipping occurs. Otherwise the inequality $\Qstar_{\lambda,h}(s,a) \leq \uQ^t_h(s,a)$ is trivial. In particular, it implies $n^t_h(s,a) > 0$.

    In this case by Bellman equations \eqref{eq:opt_reg_bellman_equation} we have
    \begin{align*}
        [\uQ^t_h - \Qstar_{\lambda,h}](s,a) &= \underbrace{r_h(s,a) + \mu \cH(\hp^t_h(s,a)) - r_h(s,a) - \mu \cH(p_h(s,a)) + \mu b^{\cH,t}_h(s,a)}_{T_1} \\
        &+ \underbrace{\hp^t_h \uV^t_{h+1}(s,a) - p_h \Vstar_{\lambda,h+1}(s,a) + b^{p,t}_h(s,a)}_{T_2}.
    \end{align*}
    By the definition of event $\cE^{\cH}(\delta)$ that is subset of both $\cG_H(\delta)$ and $\cG_B(\delta)$ we have $T_1 \geq 0$. 
    To show that $T_2 \geq 0$, we start from induction hypothesis
    \begin{align*}
        T_2 &\geq [\hp^t_h - p_h] \Vstar_{\lambda,h+1}(s,a) + b^{p,t}_h(s,a).
    \end{align*}
    In the case of Hoeffding bonuses the inequality $T_2 \geq 0$ automatically holds from the definition of $\cE^{\conc}_{H}(\delta) \subseteq \cG_H(\delta)$.
    
    In the case of Bernstein bonuses we apply Lemma~\ref{lem:empirical_bernstein} with $U = \uV^t_{h+1}$ and definition of transition bonuses we have 
    \[
        T_2 \geq - \frac{1}{H} \hp^t_h \vert \uV^t_{h+1} - \Vstar_{\lambda, h+1} \vert (s,a) + \frac{1}{H} \hp^t_h [\uV^t_{h+1} - \lV^t_{h+1}](s,a).
    \]
    By induction hypothesis we have $\uV^t_{h+1}(s) \geq  \Vstar_{\lambda, h+1}(s) \geq \lV^t_{h+1}(s)$, thus $T_2 \geq 0$.

    To prove the second inequality on $Q$-function, we assume $\lQ^t_h(s,a) > 0$ and, as a consequence, $n^t_h(s,a) > 0$. Thus we have
    \begin{align*}
        [\lQ^t_h - \Qstar_{\lambda,h}](s,a) = \underbrace{\cH(\hp^t_h(s,a)) - \cH(p_h(s,a)) - b^{\cH,t}_h(s,a)}_{T_1'} + \underbrace{\hp^t_h \lV^t_{h+1}(s,a) - p_h \Vstar_{\lambda, h+1}(s,a) - b^{p,t}_h(s,a)}_{T_2'}.
    \end{align*}
    Again, by the definition of event $\cE^{\cH}(\delta)$ we have $T_1' \leq 0$ and, by induction hypothesis
    \[
        T_2' \leq [\hp^t_h - p_h] V^{\cH,\star}_{h+1}(s,a) - b^{p,t}_h(s,a).
    \]
    In the case of Hoeffding bonuses $T_2'\leq 0$ be event $\cE^{\conc}_H(\delta)$. In the case of Bernstein bonuses we again apply Lemma~\ref{lem:empirical_bernstein} with $U = \uV^t_{h+1}$
    \[
        T_2' \leq \frac{1}{H} \hp^t_h \vert \uV^t_{h+1} - \Vstar_{\lambda,h+1} \vert (s,a) - \frac{1}{H} \hp^t_h[\uV^t_{h+1} - \lV^t_{h+1}](s,a).
    \]
    We conclude the statement by induction hypothesis for $h' = h+1$.

    Finally, we have to show the inequality for $V$-functions. To do it, we use the fact that $V$-functions are computed by $F_{\lambda}$ applied to $Q$-functions
    \[
        \lV^t_h(s) = F_{\lambda}(\lQ^t_h)(s), \quad \Vstar_{\lambda,h}(s) = F_{\lambda}(\Qstar_{\lambda,h})(s), \quad \uV^t_h(s) = F_{\lambda}(\uQ^t_h)(s).
    \] 
    Notice that $\nabla F_\lambda$ takes values in a probability simplex, thus, all partial derivatives of $F_\lambda$ are non-negative and therefore $F_\lambda$ is monotone in each coordinate. Thus, since $\lQ^t_h(s,a) \leq \Qstar_{\lambda,h}(s,a) \leq \uQ^t_h(s,a)$, we have the same inequality $\lV^t_h(s) \leq \Vstar_{\lambda,h}(s) \leq \uV^t_h(s)$.
\end{proof}


\subsection{Regularization-Aware Stopping Rule}
In this section we provide guarantees for the \textit{regularization-aware gap} that highly depends on the parameter $\lambda$. The stopping rule defined with this notion of the gap for $\lambda = \Omega(1)$ allows to obtain algorithm with $\tcO(1/\varepsilon)$ sample complexity.


Let us define new regularization-aware gap recursively. We start from $G^{t}_{\lambda,H+1} \triangleq 0$ and
\begin{align}\label{eq:def_gap_lambda}
    \begin{split}
        W^t_{\lambda,h}(s,a) &=  \left( 1 + \frac{1}{H}\right)\hp^t_h G^{t}_{\lambda,h+1}(s) + \frac{4H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} \\
        G^{t}_{\lambda,h}(s) &= \clip\biggl( \pi^{t+1}_h W^t_{\lambda,h}(s) + \frac{r_A^2}{2\lambda} \left(\uV^t_h(s) - \lV^t_h(s)\right)^2,
        0, H\Rmax \biggl)
    \end{split}
\end{align}
and the corresponding stopping time
\begin{align}\label{eq:def_tau_lambda}
    \tau_\lambda = \inf\{ t \in \N : G^t_{\lambda, 1 }(s_1)  \leq \varepsilon \}.
\end{align}

The next lemma justifies this choice of the stopping rule.
\begin{lemma}\label{lem:reg_aware_stopping_rule}
    Assume the choice of Hoeffding bonuses \eqref{eq:hoeffding_transition_bonuses} and let the event $\cG_H(\delta)$ defined in Lemma~\ref{lem:proba_master_event} holds. Then for any $t\in\N$, $s\in \cS, h \in [H]$ 
    \[
        \Vstar_{\lambda,h}(s) - V^{ \pi^{t+1}}_{\lambda,h}(s) \leq G^{t}_{\lambda,h}(s).
    \]
\end{lemma}
\begin{proof}
    Let us proceed by induction. For $h=H+1$ the statement is trivial. Assume that for any $h' > h$ the statement holds. Also assume that $G^t_{\lambda,h}(s) < H \Rmax$, otherwise the inequality on the policy error holds trivially. In particular, it holds that $n^t_h(s,a) > 0$ for all $a \in \cA$.

    We can start analysis from understanding the policy error by applying the smoothness of $F_\lambda$.    
    \begin{align*}
        \Vstar_{\lambda,h}(s) - V^{\pi^{t+1}}_{\lambda,h}(s) &= F_\lambda(\Qstar_{\lambda,h}(s, \cdot)) - \left(\pi^{t+1}_h Q^{\pi^{t+1}}_{\lambda,h}(s, \cdot)  -\lambda \Phi(\pi^{t+1}_h(s)) \right) \\
        &\leq F_\lambda(\uQ^{t}_h)(s) + \langle \nabla F_\lambda(\uQ^{t}_h(s,\cdot)), \Qstar_{\lambda,h}(s,\cdot) - \uQ^t_h(s,\cdot)  \rangle + \frac{1}{2\lambda} \norm{\uQ^t_h - \Qstar_{\lambda,h}}_*^2(s) \\
        &- \left(\pi^{t+1}_h Q^{\pi^{t+1}}_{\lambda,h}(s, \cdot)  -\lambda \Phi(\pi^{t+1}_h(s)) \right).
    \end{align*}
    Next we recall that
    \[
         \pi^{t+1}_h(s) = \nabla F(\uQ^{t}_h(s,\cdot)), \quad F(\uQ^{t}_h)(s)  =  \pi^{t+1}_h \uQ^{t}_h(s) - \lambda \Phi(\pi^{t+1}_h(s)),
    \]
    thus we have
    \[
        F(\uQ^t_h)(s) - \left( \pi^{t+1}_h Q^{\pi^{t+1}}_{\lambda,h}(s, \cdot) - \lambda \Phi(\pi^{t+1}_h(s))  \right) = \pi^{t+1}_h [ \uQ^t_h - Q^{\pi^{t+1}}_{\lambda,h}](s)
    \]
    and, by Bellman equations
    \begin{align*}
        \Vstar_{\lambda,h}(s) - V^{\pi^{t+1}}_{\lambda,h}(s) &\leq \pi^{t+1}_h \left[ \Qstar_{\lambda,h} - Q^{\pi^{t+1}}_{\lambda,h} \right] (s) + \frac{1}{2\lambda} \norm{\uQ^t_h - \Qstar_{\lambda,h}}_*^2(s) \\
        &\leq \pi^{t+1}_h p_h \left[ \Vstar_{\lambda,h+1} - V^{\pi^{t+1}}_{\lambda,h+1} \right] (s) + \frac{1}{2\lambda} \norm{\uQ^t_h - \Qstar_{\lambda,h}}_*^2(s).
    \end{align*}
    By induction hypothesis we have
    \[
        \Vstar_{\lambda,h}(s) - V^{\pi^{t+1}}_{\lambda,h}(s) \leq \pi^{t+1}_h p_h G^{t}_{\lambda,h+1}(s) + \frac{1}{2\lambda} \norm{\uQ^t_h - \Qstar_{\lambda,h}}^2_*(s).
    \]
    Next, we apply Lemma~\ref{lem:reg_directional_concentration}
    \[
        p_h^t G_{\lambda, h+1}(s) = \hp^t_hG_{\lambda, h+1}(s) +  [p_h - \hp^t_h] G^{t}_{\lambda,h+1}(s) \leq \left(1 + \frac{1}{H}\right) \hp^t_h G^{t}_{\lambda,h+1}(s) + \frac{4H^2 \Rmax\beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} = W^t_{\lambda,h}(s,a),
    \]
    thus
    \begin{align*}
        \Vstar_{\lambda,h}(s) - V^{\pi^{t+1}}_{\lambda,h}(s) &\leq \pi^{t+1}_h W^t_{\lambda,h}(s) + \frac{1}{2\lambda} \norm{\uQ^t_h - \Qstar_{\lambda,h}}^2_*(s).
    \end{align*}
    Next we are going to bound $\ell_\infty$ of norm and upper bound this difference by something simpler. First, by Theorem~\ref{th:reg_confidence_intervals}
    \begin{align*}
        \max_{a \in \cA}[\uQ^t_h - \Qstar_{\lambda,h} ]^2(s,a) &= \left( \max_{a \in \cA}[\uQ^t_h - \Qstar_{\lambda,h}](s,a) \right)^2,
    \end{align*}
    and, therefore
    \begin{align*}
         \max_{a \in \cA}[\uQ^t_h - \Qstar_{\lambda,h}](s,a) &= \max_{q \in \simplex_{\cA}}q[\uQ^t_h - \Qstar_{\lambda,h}](s)  \\
         &= \max_{q \in \simplex_{\cA}}\left\{ q\uQ^t_h  - \lambda \Phi(q) - q\Qstar_{\lambda,h} + \lambda \Phi(q) \right\} \\
         &\leq \uV^t_h(s) - \Vstar_{\lambda,h}(s) \leq \uV^t_h(s) - \lV^t_h(s).
    \end{align*}
    Finally, we replace $\norm{\cdot}_*$ by $\norm{\cdot}_\infty$ and obtain
    \begin{align*}
        \Vstar_{\lambda,h}(s) - V^{\pi^{t+1}}_{\lambda,h}(s) &\leq \pi^{t+1}_h W^t_{\lambda,h}(s) + \frac{r_A^2}{2\lambda} \left(\uV^t_h(s) - \lV^t_h(s)\right)^2 = G^{t}_{\lambda,h}(s).
    \end{align*}
\end{proof}


\begin{theorem}\label{th:reg_aware_sample_complexity}
    Let $\varepsilon > 0$,$\delta \in (0,1)$, $S\geq 2$ and $\lambda \leq H^3 S r^2_A$. Then $\UCBVIEnt$ algorithm with Hoeffding bonuses and a regularization-aware stopping rule $\tau_\lambda$ is $(\varepsilon,\delta)$-PAC for the best policy identification in regularized MDPs. 
    
    Moreover, the stopping time $\tau_\lambda$ is bounded as follows
    % \[
    %     \tau_\lambda \leq 1 + \frac{256\rme^4 H^6 S A r^2_A \Rmax^2(\log(3SAH/\delta) + S(1+L)) \cdot L^3}{\varepsilon\lambda},
    % \]
    \[
        \tau_\lambda = \cO\left( \frac{H^6SA r_A^2\Rmax^2 \cdot (\log(SAH/\delta) + SL) \cdot L^3}{\varepsilon \lambda} \right)
    \]
    % where $L = 3\log(54A + \sqrt{18AB})= \cO(\log(\nicefrac{SAH\Rmax}{\varepsilon\lambda}) + \log\log(SAH/\delta) )$, where $A = \nicefrac{256\rme^4 H^6 S A \Rmax^2}{\lambda \varepsilon}$ and $B = \log(3SAH/\delta) + S$. 
    where $L = \cO(\log(\nicefrac{SAH\Rmax}{\varepsilon\lambda}) + \log\log(SAH/\delta))$.
\end{theorem}
\begin{proof}
    To show that $\UCBVIEnt$ is $(\varepsilon,\delta)$-PAC we notice that on event $\cG_H(\delta)$ for $\hpi = \pi^{\tau}$ by Lemma~\ref{lem:reg_aware_stopping_rule}
    \[
        \Vstar_{\lambda,1}(s_1) - V^{\hat \pi}_{\lambda,1}(s_1) \leq G^{\tau_{\lambda}}_1(s_1) \leq \varepsilon,
    \]
    and the event $\cG_H(\delta)$ holds with probability at least $1-\delta$. Next we show that the sample complexity is bounded by the mentioned quantity.


    \textbf{Step 1. Bound for $G^{t}_{\lambda,1}(s_1)$}
    First, we start from bounding $W^t_{\lambda,h}(s,a)$ and $G^t_{\lambda, h}(s)$. By Lemma~\ref{lem:reg_directional_concentration} we can define the following upper bound for $W^t_{\lambda,h}(s,a)$
    \begin{align*}
        W^t_{\lambda,h}(s,a) \leq \left( 1 + \frac{2}{H}\right)p_h G^{t}_{\lambda, h+1}(s,a) + \frac{8H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)}
    \end{align*}
    Therefore we obtain
    \begin{align*}
        G^t_{\lambda, h}(s)  \leq \E_{\pi^{t+1}}\left[ \left( 1 + \frac{2}{H}\right) G^{t}_{\lambda, h+1}(s_{h+1}) + \frac{8H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s_h,a_h))}{n^t_h(s_h,a_h)} + \frac{r_A^2}{2\lambda} \left(\uV^t_h(s_h) - \lV^t_h(s_h)\right)^2 \bigg| s_h = s\right] ,
    \end{align*}
    By rolling out this expression
    \begin{align*}
        G^t_{\lambda, 1}(s_1) &\leq \E_{\pi^{t+1}} \left[ \sum_{h=1}^H \left( 1 + \frac{2}{H} \right)^h \frac{8H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s_h,a_h))}{n^t_h(s_h,a_h)} + \left( 1 + \frac{2}{H} \right)^h  \frac{r_A^2}{2\lambda} \left(\uV^t_h(s_h) - \lV^t_h(s_h)\right)^2 \bigg| s_1 \right].
    \end{align*}
    Using the fact that $(1+2/H)^h \leq \rme^2$, we have
    \begin{align*}
        G^t_{\lambda, 1}(s_1) &\leq \underbrace{8\rme^2 H^2 \Rmax \E_{\pi^{t+1}}\left[\sum_{h=1}^H \frac{\beta^{\KL}(\delta, n^t_h(s_h,a_h))}{n^t_h(s_h,a_h)} \right]}_{\termA} + \frac{\rme^2 r^2_A}{2\lambda} \underbrace{\E_{\pi^{t+1}}\left[ \sum_{h=1}^H \left( \uV^t_h(s_h) - \lV^t_h(s_h) \right)^2 \right]}_{\termB}.
    \end{align*}
    The analysis of the term $\termA$ follows \cite{menard2021fast}: we switch counts to pseudocounts by Lemma~\ref{lem:cnt_pseudo} and obtain
    \[
        \termA \leq 32\rme^2 H^2 \Rmax \sum_{h=1}^H \sum_{(s,a) \in \cS \times \cA} d^{\pi^{t+1}}_h(s,a)  \frac{\beta^{\KL}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1}. 
    \]
    For the term $\termB$ we start from expression for one term. First, we have
    \[
        \uV^t_h(s_h) - \lV^t_h(s_h) \leq 2 \pi^{t+1}_h b^{p,t}_h(s_h) + 2\mu \pi^{t+1}_h b^{\cH, t}_h(s_h) + \pi^{t+1}_h \hp^t_h [\uV^t_{h+1} - \lV^t_{h+1}](s_h).
    \]
    By Lemma~\ref{lem:reg_directional_concentration}
    \begin{align*}
        \uV^t_h(s_h) - \lV^t_h(s_h) &\leq 2 \pi^{t+1}_h b^{p,t}_h(s_h) + 2\mu \pi^{t+1}_h b^{\cH, t}_h(s_h) + \pi^{t+1}_h 2H\Rmax \left(\frac{2H \beta^{\KL}(\delta, n^{\,t}_h(s,\cdot))}{n^{\,t}_h(s,\cdot)} \wedge 1 \right) \\
        &+ \left(1 + \frac{1}{H} \right) \pi^{t+1}_h p_h [\uV^t_{h+1} - \lV^t_{h+1}](s_h).
    \end{align*}
    By definition of Hoeffding bonuses \eqref{eq:hoeffding_transition_bonuses} and entropy bonuses \eqref{eq:entropy_bonuses} 
    \begin{align*}
        \mu b^{\cH,t}_h(s_h) &\leq \mu \log(S) \left( \sqrt{\frac{2 \beta^{\cH}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + \left(\frac{\beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} \wedge 1 \right)  \right) \\
        &\leq 2H\Rmax \sqrt{\frac{2 \beta^{\cH}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + H\Rmax \left(\frac{2H \beta^{\KL}(\delta, n^{\,t}_h(s,\cdot))}{n^{\,t}_h(s,\cdot)} \wedge 1 \right). 
    \end{align*}
    Notice that $\beta^{\KL}(\delta, n) \leq \log^2(n) \cdot \beta^{\conc}(\delta, n)$, thus, rolling out this recursion and using the fact that $(1+1/H)^h \leq \rme$ for $h \leq H$, we have
    \begin{align*}
        \uV^t_h(s_h) - \lV^t_h(s_h) &\leq 4\rme H\Rmax \cdot \E_{\pi^{t+1}} \Biggl[ \sum_{h'=h}^H \sqrt{\frac{2\log^2(n^t_{h'}(s_{h'}, a_{h'}))\beta^{\conc}(\delta, n^t_{h'}(s_{h'}, a_{h'})) }{n^t_{h'}(s_{h'}, a_{h'})}} \\
        &+  \left(\frac{2H \beta^{\KL}(\delta, n^{\,t}_{h'}(s_{h'},a_{h'}))}{n^{\,t}_{h'}(s_{h'},a_{h'})} \wedge 1 \right)   \bigg| s_h \Biggl].
    \end{align*}
    By Lemma~\ref{lem:cnt_pseudo} and Jensen inequality we have
    \begin{align*}
        \uV^t_h(s_h) - \lV^t_h(s_h) &\leq 2\rme H^{3/2}\Rmax \sqrt{ \E_{\pi^{t+1}} \left[ \sum_{h'=h}^H  \frac{2\log^2(\upn^t_{h'}(s_{h'}, a_{h'})) \beta^{\conc}(\delta, \upn^t_{h'}(s_{h'}, a_{h'})) }{\upn^t_{h'}(s_{h'}, a_{h'}) \vee 1} | s_h \right]} \\
        &+ 2\rme H \Rmax \E_{\pi^{t+1}}\left[\sum_{h'=h}^H  \left( \frac{2H\beta^{\KL}(\delta, \upn^t_{h'}(s_{h'},a_{h'}))}{\upn^t_{h'}(s_{h'},a_{h'}) \vee 1} \wedge 1 \right) \big| s_h \right].
    \end{align*}
    By inequality $(a+b)^2 \leq 2a^2 + 2b^2$ 
    \begin{align*}
        \termB &= \frac{\rme^2 r_A^2}{\lambda} \cdot \E_{\pi^{t+1}}\left[ \sum_{h=1}^H (\uV^t_h(s_h) - \lV^t_h(s_h))^2 \bigg| s_1 \right]  \\
        &\leq \frac{8\rme^4 H^3 \Rmax^2 r_A^2}{\lambda} \underbrace{\E_{\pi^{t+1}}\left[\sum_{h=1}^H \E_{\pi^{t+1}}\left[\sum_{h'=h}^H \frac{2\log^2(\upn^t_{h'}(s_{h'}, a_{h'}))\beta^{\conc}(\delta, \upn^t_{h'}(s_{h'}, a_{h'})) }{\upn^t_{h'}(s_{h'}, a_{h'}) \vee 1} \bigg| s_h \right] \bigg| s_1 \right]}_{\termC} \\
        &+ \frac{8\rme^4 H^2 \Rmax^2 r_A^2}{\lambda}  \underbrace{\E_{\pi^{t+1}}\left[\sum_{h=1}^H \left( \E_{\pi^{t+1}}\left[  \sum_{h'=h}^H  \left( \frac{2H\beta^{\KL}(\delta, \upn^t_h(s_{h'},a_{h'}))}{\upn^t_{h'}(s_{h'},a_{h'}) \vee 1} \wedge 1 \right)\bigg| s_h \right]  \right)^2 \bigg| s_1 \right]}_{\termD}.
    \end{align*}
    For term $\termC$ we apply the telescoping property of conditional expectation and obtain the following bound
    \begin{align*}
        \termC &= \E_{\pi^{t+1}}\left[\sum_{h=1}^H \sum_{h'=h}^H \frac{2\log^2(\upn^t_{h'}(s_{h'}, a_{h'})) \beta^{\conc}(\delta, \upn^t_{h'}(s_{h'}, a_{h'})) }{\upn^t_{h'}(s_{h'}, a_{h'}) \vee 1}  \bigg| s_1 \right] \\
        &\leq H \E_{\pi^{t+1}}\left[\sum_{h=1}^H \frac{2\log^2(\upn^t_{h}(s_{h}, a_{h})) \beta^{\conc}(\delta, \upn^t_{h}(s_{h}, a_{h}))}{\upn^t_{h}(s_{h}, a_{h}) \vee 1}  \bigg| s_1 \right].
    \end{align*}
    For term $\termD$ we first apply Jensen's inequality and the telescoping property of conditional expectation
    \[
        \termD \leq \E_{\pi^{t+1}}\left[ \sum_{h=1}^H \left( \sum_{h'=h}^H  \left( \frac{2H\beta^{\KL}(\delta, \upn^t_h(s_{h'},a_{h'}))}{\upn^t_{h'}(s_{h'},a_{h'}) \vee 1} \wedge 1 \right) \right)^2 \bigg| s_1 \right].
    \]
    Notice that the expression under the square maximizes at $h=1$. Thus, applying Jensen's inequality
    \[
        \termD \leq H^3 \E_{\pi^{t+1}}\left[ \left( \sum_{h=1}^H \frac{1}{H}   \left( \frac{2H\beta^{\KL}(\delta, \upn^t_h(s_{h'},a_{h'}))}{\upn^t_{h'}(s_{h'},a_{h'}) \vee 1} \wedge 1 \right) \right)^2 \bigg| s_1 \right] \leq H^2 \E_{\pi^{t+1}}\left[ \sum_{h=1}^H  \left( \frac{2H\beta^{\KL}(\delta, \upn^t_h(s_{h'},a_{h'}))}{\upn^t_{h'}(s_{h'},a_{h'}) \vee 1} \wedge 1 \right)^2 \bigg| s_1 \right].
    \]
    Finally, representing maximum as a product of two equivalent terms and using two different upper bounds based on minimum operation we have
    \[
        \termD \leq 2H^3 \E_{\pi^{t+1}}\left[ \sum_{h=1}^H  \frac{\beta^{\KL}(\delta, \upn^t_h(s_{h},a_{h}))}{\upn^t_{h}(s_{h},a_{h}) \vee 1} \bigg| s_1 \right].
    \]
    The final bound for an initial gap follows
    \begin{align*}
        G^t_{\lambda, 1}(s_1) &\leq 32\rme^2 H^2 \Rmax \sum_{h=1}^H \sum_{(s,a) \in \cS \times \cA} d^{\pi^{t+1}}_h(s,a)  \frac{\beta^{\KL}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1} \\
        &+ \frac{16\rme^4 H^4 \Rmax^2 r_A^2}{\lambda} \sum_{h=1}^H \sum_{(s,a) \in \cS \times \cA} d^{\pi^{t+1}}_h(s,a)  \frac{\log^2(\upn^t_{h}(s, a)) \beta^{\conc}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1} \\
        &+ \frac{16\rme^4 H^5 \Rmax^2 r_A^2}{\lambda} \sum_{h=1}^H  \sum_{(s,a) \in \cS \times \cA} d^{\pi^{t+1}}_h(s,a) \frac{\beta^{\KL}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1} .
    \end{align*}

    \textbf{Step 2. Sum over $t < \tau_{\lambda}$.} Assume $\tau_{\lambda} > 0$. In the case $\tau_{\lambda} = 0$ the bound is trivially true. Notice that for any $t < \tau_{\lambda}$ we have
    \[
        G^t_{\lambda,1}(s_1) > \varepsilon,
    \]
    thus, summing upper bounds on $G^t_{\lambda,1}(s_1)$ over all $t < \tau_{\lambda}$ we have
    \begin{align*}
        \varepsilon (\tau_{\lambda} - 1) < \sum_{t=1}^{\tau_{\lambda}-1} G^{t}_{\lambda,1}(s_1) &\leq 32\rme^2 H^2 \Rmax \sum_{h=1}^H \sum_{(s,a) \in \cS \times \cA} \sum_{t=1}^{\tau_{\lambda}-1}d^{\pi^{t+1}}_h(s,a)  \frac{\beta^{\KL}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1} \\
        &+ \frac{16\rme^4 H^4 \Rmax^2 r_A^2}{\lambda} \sum_{h=1}^H \sum_{(s,a) \in \cS \times \cA} \sum_{t=1}^{\tau_{\lambda}-1} d^{\pi^{t+1}}_h(s,a)  \frac{\log^2(\upn^t_{h}(s, a))  \beta^{\conc}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1} \\
        &+ \frac{16\rme^4 H^5 \Rmax^2 r_A^2}{\lambda} \sum_{h=1}^H  \sum_{(s,a) \in \cS \times \cA}  \sum_{t=1}^{\tau_{\lambda}-1} d^{\pi^{t+1}}_h(s,a)   \frac{\beta^{\KL}(\delta, \upn^t_h(s,a)) }{\upn^t_h(s,a) \vee 1}.
    \end{align*}

    Notice that $\beta^{\KL}(\delta,\cdot)$ and $\beta^{\conc}(\delta, \cdot)$ are monotone and maximizes at $\tau_{\lambda}$, and $d^{\pi^{t+1}}_h(s,a) = \upn^{t+1}_h(s,a) - \upn^t_h(s,a)$. Thus, applying Lemma~\ref{lem:sum_1_over_n}, we have
    \begin{align*}
        \varepsilon(\tau_{\lambda} - 1) &< 128\rme^2 H^3 SA \Rmax \cdot  \beta^{\KL}(\delta, \tau_{\lambda}-1) \log(\tau_{\lambda})   \\
        &+ \frac{64\rme^4 H^5 SA \Rmax^2 r_A^2 \cdot \beta^{\conc}(\delta, \tau_{\lambda}-1) \log^3(\tau_{\lambda})}{\lambda} \\
        &+ \frac{64\rme^4 H^6 SA \Rmax^2 r_A^2 \cdot \beta^{\KL}(\delta, \tau_{\lambda} - 1) \log(\tau_{\lambda})}{\lambda}.
    \end{align*}
    Notice that since $S>2$ we have $\beta^{\KL}(\delta, \tau_{\lambda} - 1) \geq \beta^{\conc}(\delta, \tau_{\lambda}-1)$, thus the third term dominates the second one. Assume that $\lambda \leq H^3 S r^2_A$. Then by definition of $\beta^{\KL}$
    \[
        \varepsilon(\tau_{\lambda}-1) \leq \frac{256\rme^4 H^6 S A r^2_A \Rmax^2}{\lambda} \cdot (\log(3SAH/\delta) + S \log(\rme \tau_{\lambda})) \cdot \log^3(\tau_{\lambda}).
    \]

    \textbf{Step 3. Solving the recurrence.}
    Define $A = \nicefrac{192\rme^4 H^6 S A r^2_A \Rmax^2}{\lambda \varepsilon}$ and $B = \log(3SAH/\delta) + S$. Our goal is to upper bound solutions to the following inequality
    \[
        \tau \leq 1 + A (S\log(\tau) + B) \cdot \log^3(\tau).
    \]
    First we obtain loose solution by using inequality $\log(\tau) \leq \tau^\beta / \beta$ that holds for any $\tau \geq 1$. Taking $\beta = 1/3$ inside the brackets and $\beta=1/9$ outside we have
    \[
        \tau \leq 1 + 9A(3S\cdot \tau^{1/3} + B ) \cdot \tau^{1/3}.
    \]
    Also we may assume that $\tau \geq 2$, thus $1 \leq \tau/2$ and we achieve
    \[
        \tau^{2/3} \leq 18A (3S \tau^{1/3} + B).
    \]
    Solving this quadratic inequality in $\tau^{1/3}$, we have
    \[
        \tau \leq \left(\frac{54AS + \sqrt{(54AS)^2 + 72AB}}{2} \right)^3 \leq \left( 54AS + \sqrt{18 AB} \right)^{3}.
    \]
    Define $L = 3 \log\left( 54AS + \sqrt{18AB} \right)$. Then we can easily upper bond the initial inequality as follows
    \[
        \tau \leq 1 + A(B + SL) L^3.
    \]
\end{proof}

After this general result we state the bound for the MTEE problem that was stated in the main text.
\begin{theorem}\label{th:mtee_sample_complexity}
    For all $\varepsilon > 0$ and $\delta \in (0,1)$ the \UCBVIEnt algorithm is $(\varepsilon,\delta)$-PAC for MTEE. Moreover, with probability at least $1-\delta$
    \[
        \tau \leq \cO\left( \frac{H^6 SA \log^2(SA) \cdot (\log(SAH/\delta) + SL) L^3}{\varepsilon} \right),
    \]
    where $L = \log(SAH/\varepsilon) + \log\log(SAH/\delta)$.
\end{theorem}
\begin{proof}
    Fix $\Phi(\pi) = -\cH(\pi), \mu=\lambda=1$ and $r_{\max} = 0$. Since $\cH(\pi)$ is $1$-strongly convex with respect to $\ell_1$-norm, we have $r_A = 1$. Also we automatically have $\Rmax = \log(SA)$. In this setting, Theorem~\ref{th:reg_aware_sample_complexity} yields the desired statement.
\end{proof}
\begin{remark}
    Note that the obtained sample complexity corresponds only to the second-order term in the complexity of the algorithm \UCBVIBPI for identifying the best policy \cite{menard2021fast}. Improving $S$-dependence turns out to be a major challenge for the MTEE problem, as it requires reducing the second-order bonus within the expression for the entropy bonus $b^{\cH,t}_h(s,a)$, which scales with $S/n^t_h(s,a)$ and therefore leads to $\tcO(HS ^2 A /\varepsilon)$ sample complexity. However, if we assume that we have access to the true entropy of the transitions, it is possible to improve the $S$-dependence by applying the $Q$-learning type algorithm \cite{jin2018is,zhang2020advantage}.
\end{remark}


\subsection{Regularization-Agnostic Stopping Rule}

In this section we provide guarantees for the so-called \textit{regularization-agnostic gap}: this notion of gap does not influenced by regularization except the changing of the range of value functions and basically mimics \UCBVIBPI algorithm by \citet{menard2021fast} in definition of the similar gap.

Let us define $G^t_{H+1}(s,a) \triangleq 0$ for all $s,a$ and
\begin{align}\label{eq:def_gap_agnostic}
    \begin{split}
    G^t_h(s,a) &\triangleq \clip\biggl( 2 b^{B,t}_h(s,a) + \frac{4 H^2\Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} + 2 \mu b^{\cH, t}_h(s,a) + \left(1 + \frac{3}{H} \right) \hp^t_h \left[\pi^{t+1}_{h+1} G^t_{h+1}\right](s,a), \\
    &\qquad 0, H\Rmax \biggl),
    \end{split}
\end{align}
where $b^{B,t}_h(s,a)$ is defined in \eqref{eq:bernstein_transition_bonuses}. For this notion of the gap we can define the stopping rule as follows
\begin{align}\label{eq:def_tau_agnostic}
    \tau = \min\{ t \in \N : \pi^{t+1}_1 G^t_1(s_1) \leq \varepsilon \}.
\end{align}

The lemma below justifies this choice of the stopping time.
\begin{lemma}\label{lem:reg_agnostic_stopping_rule}
    Assume the choice of Bernstein bonuses \eqref{eq:bernstein_transition_bonuses} and let the event $\cG_B(\delta)$ defined in Lemma~\ref{lem:proba_master_event} holds. Then for all $t \in \N$ we have
    \[
        \Vstar_{\lambda,1}(s_1) - V^{\pi^{t+1}}_{\lambda,1}(s_1) \leq \pi^{t+1}_1 G^t_1(s_1).
    \]
\end{lemma}
\begin{proof}
    Following \cite{menard2021fast}, we start by defining the following quantities
    \begin{align*}
        \tQ^t_h(s,a) &\triangleq \clip\left( \hat r^t_{\mu,h}(s,a) + \hp^t_h \tV^t_{h+1}(s,a) - b^{p,t}_h(s,a) - \mu b^{\cH,t}_h(s,a) , 0, r_{\mu,h}(s,a) + p_h \tV^t_{h+1}(s,a) \right),  \\
        \tV^t_h(s) &\triangleq  \pi^{t+1}_h \tQ^t_h(s) - \lambda \Phi(\pi^{t+1}_h(s)), \\
        \tV^t_{H+1}(s) &\triangleq 0.
    \end{align*}
    By Theorem~\ref{th:reg_confidence_intervals} and Lemma~\ref{lem:tQ_properties} we have
    \begin{align*}
        \Vstar_{\lambda,1}(s_1) - V^{\pi^{t+1}}_{\lambda,1}(s_1) &\leq \uV^{t}_1(s_1) - V^{\pi^{t+1}}_{\lambda,1}(s_1) \leq \uV^{t}_1(s_1) - \tV^t_1(s_1) \\
        &=  \pi^{t+1}_1 \uQ^t_1(s_1) - \lambda \Phi(\pi^{t+1}_1(s_1))- \pi^{t+1}_1 \tQ^t_1(s_1) + \lambda \Phi(\pi^{t+1}_1(s_1)) = \pi^{t+1}_1 [\uQ^t_1 - \tQ^t_1](s_1).
    \end{align*}
    Therefore, it is enough to show that for any $(h,s,a) \in [H] \times \cS \times \cA$
    \[
        [\uQ^t_h - \tQ^t_h](s,a) \leq G^t_h(s,a), \qquad  [\uV^t_h - \tV^t_h](s) \leq \pi^{t+1}_h G^t_h(s).
    \]
    Proceed by backward induction over $h$. The case $h=H+1$ is trivial, thus we may assume that the statement holds for any $h' > h$ for a fixed $h$. Also fix $(s,a) \in \cS \times \cA$.

    Notice that if $G^t_h(s,a) = H\Rmax$, then the inequality is trivially true. Therefore we may assume that $G^t_h(s,a) < H\Rmax$ and, consequently, $n^t_h(s,a) > 0$. Now we have to separate cases.
    \paragraph{First case.}
    In this case we have $\tQ^t_h(s,a) = r_{\mu,h}(s,a) + p_h \tV^t_{h+1}(s,a)$, i.e. maximal clipping occurs. Therefore
    \begin{align*}
        \uQ^t_h(s,a) - \tQ^t_h(s,a) &= r_h(s,a) + \mu \cH(\hp^t_h(s,a)) + \mu b^{\cH,t}_h(s,a) - r_h(s,a) - \mu\cH(p_h(s,a)) \\
        &+ \hp^t_h \uV^t_{h+1}(s,a) - p_h \tV^t_{h+1}(s,a) + b^{p,t}_h(s,a).
    \end{align*}
    By the definition of the event $\cE^{\cH}(\delta) \subseteq \cG_B(\delta)$ we have
    \[
        \mu \cH(\hp^t_h(s,a)) + \mu b^{\cH,t}_h(s,a)  - \mu\cH(p_h(s,a)) \leq 2\mu b^{\cH,t}_h(s,a),
    \]
    for the next term we have
    \[
        \hp^t_h \uV^t_{h+1} - p_h \tV^t_{h+1}(s,a) = \hp^t_h [\uV^t_{h+1} - \tV^t_{h+1}(s,a)] + [\hp^t_h - p_h] \Vstar_{\lambda,h+1} (s,a) + [p_h - \hp^t_h][ \Vstar_{\lambda,h+1}- \tV^t_{h+1} ](s,a).
    \]
    By induction hypothesis 
    \[
        \hp^t_h [\uV^t_{h+1} - \tV^t_{h+1}(s,a)] \leq \hp^t_h [ \pi^{t+1}_{h+1} G^t_{h+1}](s,a).
    \]
    Next we apply Lemma~\ref{lem:empirical_bernstein} with $U = \uV^t_{h+1}(s,a)$ and Theorem~\ref{th:reg_confidence_intervals}
    \[
        [\hp^t_h - p_h] \Vstar_{\lambda,h+1}(s,a) \leq b^{p,t}_h(s,a).
    \]
    Finally, we apply Lemma~\ref{lem:reg_directional_concentration} and obtain
    \[
        [p_h - \hp^t_h][\Vstar_{\lambda,h+1}- \tV^t_{h+1} ](s,a) \leq \frac{1}{H} \hp^t_h[\Vstar_{\lambda,h+1} - \tV^t_{h+1} ](s,a) + \frac{4 H^2 \Rmax \cdot \beta^{\KL}(\delta,n^t_h(s,a))}{n^t_h(s,a)}.
    \]
    Summing all these bounds up, we have
    \begin{align*}
        \uQ^t_h(s,a) - \tQ^t_h(s,a) &\leq \hp^t_h [ \pi^{t+1}_{h+1} G^t_{h+1}](s,a) + 2\mu b^{\cH,t}_h(s,a) + 2b^{p,t}_h(s,a) \\
        &+ \frac{1}{H} \hp^t_h[ \Vstar_{\lambda, h+1}- \tV^t_{h+1} ](s,a) + \frac{4 H^2 \Rmax \cdot \beta^{\KL}(\delta,n^t_h(s,a))}{n^t_h(s,a)}.
    \end{align*}
    Notice that by Theorem~\ref{th:reg_confidence_intervals} and the induction hypothesis 
    \[
        \frac{1}{H} \hp^t_h[\Vstar_{\lambda, h+1}- \tV^t_{h+1} ](s,a) \leq \frac{1}{H} \hp^t_h[\uV^{t}_{h+1}- \tV^t_{h+1} ](s,a) \leq  \frac{1}{H} \hp^t_h [ \pi^{t+1}_{h+1} G^t_{h+1}](s,a),
    \]
    and by decomposing the transition bonus \eqref{eq:bernstein_transition_bonuses} to Bernstein bonus and correction term and applying Lemma~\ref{lem:tQ_properties}
    \begin{align*}
        b^{p,t}_h(s,a) &= b^{B,t}_h(s,a) + \frac{1}{H} \hp^t_h[\uV^t_{h+1} - \lV^t_{h+1}](s,a) \leq  b^{B,t}_h(s,a) + \frac{1}{H} \hp^t_h[\uV^{t}_{h+1}- \tV^t_{h+1} ](s,a) \\
        &\leq  b^{B,t}_h(s,a) + \frac{1}{H}\hp^t_h [ \pi^{t+1}_{h+1} G^t_{h+1}](s,a),
    \end{align*}
    thus
    \begin{align*}
        \uQ^t_h(s,a) - \tQ^t_h(s,a) &\leq \left(1 + \frac{3}{H} \right)\hp^t_h [ \pi^{t+1}_{h+1} G^t_{h+1}](s,a) + 2 \mu b^{\cH,t}_h(s,a) + 2b^{B,t}_h(s,a) \\
        &+ \frac{4 H^2 \Rmax \cdot \beta^{\KL}(\delta,n^t_h(s,a))}{n^t_h(s,a)} = G^t_h(s,a).
    \end{align*}
    
    \paragraph{Second case.} In this case we have $\tQ^t_h(s,a) = \hat r^t_{\lambda,h}(s,a) + \hp^t_h \tV^t_{h+1}(s,a) - b^{p,t}_h(s,a) - \mu b^{\cH,t}_h(s,a)$. Thus

    \[
        \uQ^t_h(s,a) - \tQ^t_h(s,a) \leq 2 \mu b^{\cH,t}_h(s,a) + 2 b^{B,t}_h(s,a) + \hp^t_h[\uV^{t}_{h+1} - \tV^t_{h+1}](s,a) + \frac{2}{H}\hp^t_h[\uV^{t}_{h+1} - \lV^t_{h+1}](s,a).
    \]
    By Lemma~\ref{lem:tQ_properties} and induction hypothesis we have 
    \[
        \uQ^t_h(s,a) - \tQ^t_h(s,a) \leq 2\mu b^{\cH,t}_h(s,a) + 2 b^{B,t}_h(s,a) + \left(1 + \frac{2}{H} \right)\hp^t_h [\pi^{t+1}_{h+1} G^t_{h+1}](s,a)  \leq G^t_h(s,a).
    \]

    \paragraph{Conclusion.} From the two cases above we conclude
    \[
        [\uQ^t_h - \tQ^t_h](s,a) \leq G^t_h(s,a).
    \]
    Moreover, we have
    \[
        \uV^t_h(s) - \tV^t_h(s) = \pi^{t+1}_h \uQ^t_h(s) -\lambda\Phi(\pi^{t+1}_h(s))  \pi^{t+1}_h \tQ^t_h(s) + \lambda\Phi(\pi^{t+1}_h(s))= \pi^{t+1}_h [\uQ^t_h - \tQ^t_h](s) \leq \pi^{t+1}_h G^t_h(s).
    \]
    The last inequality concludes the statement of Lemma~\ref{lem:reg_agnostic_stopping_rule}.
\end{proof}


\begin{lemma}\label{lem:tQ_properties}
    Under the choice of Bernstein bonuses \eqref{eq:bernstein_transition_bonuses}, on event $\cG_B(\delta)$ for any $t \in \N$ and any $(h,s,a) \in [H] \times \cS \times \cA$
    \[
        \tQ^t_h(s,a) \leq \min\{ Q^{\pi^{t+1}}_{\lambda,h}(s,a), \lQ^t_h(s,a) \}, \qquad
        \tV^t_h(s) \leq \min\{ V^{\pi^{t+1}}_{\lambda,h}(s), \lV^t_h(s) \}.
    \]
\end{lemma}
\begin{proof}
    Proceed by backward induction over $h$. The case $h=H+1$ is trivially true, assume that the statement holds for any $h' > h$ for a fixed $h$. Also let us fix $t,s,a$.  By induction hypothesis we have
    \[
        \tQ^t_h(s,a) \leq r_{\mu,h}(s,a) + p_h \tV^t_{h+1} \leq r_{\mu,h}(s,a) + p_h V^{\pi^{t+1}}_{\lambda, h+1}(s,a) = Q^{\pi^{t+1}}_{\lambda,h}(s,a).
    \]
    In the same manner
    \begin{align*}
        \tQ^t_h(s,a) &\leq \hat r^t_{\mu,h}(s,a) + \hp^t_h \tV^t_{h+1} - b^{p,t}_h(s,a) - \mu b^{\cH,t}_h(s,a) \\
        &\leq \hat r^t_{\mu,h}(s,a) + \hp^t_h \lV^t_{h+1} - b^{p,t}_h(s,a) - \mu b^{\cH,t}_h(s,a) = \lQ^t_h(s,a).
    \end{align*}
    Next, we prove the same inequalities for $V$-functions
    \[
        \tV^t_h(s) = \pi^{t+1}_h(s) \tQ^t_h(s,a) - \lambda \Phi(\pi^{t+1}_h(s))\leq  \pi^{t+1}_h(s) Q^{\pi^{t+1}}_{\lambda,h}(s,a) -\lambda \Phi(\pi^{t+1}_h(s)) = V^{\pi^{t+1}}_{\lambda,h}(s),
    \]
    and
    \begin{align*}
        \tV^t_h(s) = \pi^{t+1}_h \tQ^t_h(s)  -\lambda \Phi(\pi^{t+1}_h(s)) \leq  \pi^{t+1}_h \lQ^{t}_h(s) -\lambda \Phi(\pi^{t+1}_h(s)) \leq \max_{\pi \in \simplex_{\cA}}\left\{ \pi \lQ^{t}_h(s) -\lambda \Phi(\pi)\right\} = \lV^t_h(s).
    \end{align*}
\end{proof}

After defining a proper quantity for a stopping rule we may proceed with the final proof for sample-complexity of the presented algorithm \UCBVIEnt.

\begin{theorem}\label{th:reg_agnostic_sample_complexity}
    Let $\delta \in (0,1)$. Then $\UCBVIEnt$ algorithm with Bernstein bonuses \eqref{eq:bernstein_transition_bonuses} and a regularization-agnostic stopping rule $\tau$ is $(\varepsilon,\delta)$-PAC for the best policy identification in regularized MDPs. 
    
    Moreover, with probability at least $1-\delta$ the stopping time $\tau$ is bounded as follows
    \[
        \tau = \cO\left( \frac{H^3SA \Rmax^2 \log(SAH/\delta) L^4}{\varepsilon^2} + \frac{H^3SA (\log(SAH/\delta) + SL) \cdot L}{\varepsilon} \right),
    \]
    where $L = \cO(\log(SAH\Rmax/\varepsilon)) + \log\log(SAH/\delta))$.
\end{theorem}
\begin{proof}
    Notice that if $\tau = 0$, then our sample complexity bound is trivial, thus we assume that $\tau > 0$.
    Let us start from deriving an upper bound for $G^t_h(s,a)$ for $t < \tau, h \in [H], (s,a) \in \cS \times \cA$. 
    \begin{align*}
        G^t_h(s,a) &\leq 2 b^{B,t}_h(s,a) + 2 \mu b^{\cH,t}_h(s,a) + \frac{4 H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} + \left(1 + \frac{3}{H} \right) \hp^t_h [\pi^{t+1}_{h+1} G^t_{h+1}](s,a) \\
        &\leq 6\sqrt{\Var_{\hp^t_h}[\uV^t_{h+1}](s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + 2\mu \sqrt{\frac{2\beta^{\cH}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + \frac{23 H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)}  \\
        &+ \left(1 + \frac{3}{H} \right)[\hp^t_h - p_h] [\pi^{t+1}_{h+1} G^t_{h+1}](s,a) + \left(1 + \frac{3}{H} \right)p_h [\pi^{t+1}_{h+1} G^t_{h+1}](s,a).
    \end{align*}
    By Lemma~\ref{lem:reg_directional_concentration}
    \[
        [\hp^t_h - p_h] [\pi^{t+1}_{h+1} G^t_{h+1}](s,a) \leq \frac{1}{H} p_h [\pi^{t+1}_{h+1} G^t_{h+1}](s,a) + \frac{4 H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)}.
    \]
    Also we have to replace the variance of the empirical model with the real variance of the value function for $\pi^{t+1}$ in order to apply the law of total variance (Lemma~\ref{lem:law_of_total_variance}). 

    Apply Lemma~\ref{lem:switch_variance} and Lemma~\ref{lem:switch_variance_bis}
    \begin{align*}
        \Var_{\hp^t_h}[\uV^t_{h+1}](s,a) &\leq 2\Var_{p_h}[\uV^t_{h+1}](s,a) + \frac{4H^2\Rmax^2\beta^{\KL}(\delta,n^t_h(s,a))}{n^t_h(s,a)} \\
        &\leq 4 \Var_{p_h}[V^{ \pi^{t+1}}_{\lambda, h+1}](s,a) + 2H\Rmax p_h [ \uV^t_{h+1} - V^{\pi^{t+1}}_{\lambda, h+1}](s,a) + \frac{4H^2\Rmax^2\beta^{\KL}(\delta,n^t_h(s,a))}{n^t_h(s,a)} .
    \end{align*}
    In the proof of Lemma~\ref{lem:reg_agnostic_stopping_rule} it was proven that
    \[
        [ \uV^t_{h+1} - V^{\pi^{t+1}}_{\lambda, h+1}](s) \leq \pi^{t+1}_{h+1} G^t_{h+1}(s),
    \]
    thus, combining with an inequality $\sqrt{a+b} \leq \sqrt{a} + \sqrt{b}$
    \begin{align*}
        6\sqrt{\Var_{\hp^t_h}[\uV^t_{h+1}](s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} &\leq 12 \sqrt{\Var_{p_h}[V^{\pi^{t+1}}_{\lambda,h+1}](s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} \\
        &+ 6\sqrt{ p_h[\pi^{t+1}_{h+1} G^t_{h+1}](s,a) \frac{2H\Rmax\beta^{\conc}(\delta,n^t_h(s,a))}{n^t_h(s,a)}}\\
        &+\frac{12 H \Rmax \beta^{\KL}(\delta, n^t_h(s,a)}{n^t_h(s,a)} 
    \end{align*}
    To bound the second term, we use inequality $2\sqrt{ab} \leq a + b$
    \[
        6\sqrt{ p_h[\pi^{t+1}_{h+1} G^t_{h+1}](s,a) \frac{2H\Rmax\beta^{\conc}(\delta,n^t_h(s,a))}{n^t_h(s,a)}} \leq \frac{3}{H}p_h[\pi^{t+1}_{h+1} G^t_{h+1}](s,a) + \frac{3H^2\Rmax\beta^{\conc}(\delta,n^t_h(s,a))}{n^t_h(s,a)}.
    \]
    Finally, we have the following bound on $G^t_{h}(s,a)$
    \begin{align*}
        G^t_h(s,a) &\leq 12 \sqrt{\Var_{p_h}[V^{\pi^{t+1}}_{\lambda, h+1}](s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + 2\mu\sqrt{\frac{2\beta^{\cH}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} \\
        &+ \frac{54 H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} + \left(1 + \frac{10}{H}\right) p_h[\pi^{t+1}_{h+1} G^t_{h+1}](s,a).
    \end{align*}
    Notice that his inequality could be rewritten in the following form
    \begin{align*}
        G^t_h(s,a) &\leq \E_{\pi^{t+1}}\biggl[12 \sqrt{\Var_{p_h}[V^{\pi^{t+1}}_{\lambda,h+1}](s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + 2\mu\sqrt{\frac{2\beta^{\cH}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} \\
        &+ \frac{54 H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} + \left(1 + \frac{10}{H} \right) G^t_{h+1}(s_{h+1},a_{h+1}) \biggl| (s_h,a_h) = (s,a) \biggl ],
    \end{align*}
    thus by rolling out we have
    \begin{align*}
        \pi^{t+1}_1 G^t_1(s_1) &\leq \E_{\pi^{t+1}}\biggl[ \underbrace{12 \sum_{h=1}^H \left(1 + \frac{10}{H}\right)^{h} \sqrt{\Var_{p_h}[V^{\pi^{t+1}}_{\lambda,h+1}](s_h,a_h) \frac{\beta^{\conc}(\delta, n^t_h(s_h,a_h))}{n^t_h(s_h,a_h)}}}_{\termA} \\
        &+ \underbrace{2\mu\sum_{h=1}^H \left(1 + \frac{10}{H}\right)^{h} \sqrt{\frac{2\beta^{\cH}(\delta, n^t_h(s_h,a_h))}{n^t_h(s_h,a_h)}}}_{\termB} \\
        &+ \underbrace{\sum_{h=1}^H \left(1 + \frac{10}{H}\right)^{h} \frac{54 H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s_h,a_h))}{n^t_h(s_h,a_h)}}_{\termC} \biggl| s_1\biggl ],
    \end{align*}
    where $(1+10/H)^h \leq \rme^{10}$ for any $h \in [H]$. Now we bound each term separately. 

    \paragraph{Term $\termA$.} To bound this term, we apply Cauchy-Schwarz inequality
    \begin{align*}
        \termA &\leq 12 \rme^{10} \sum_{(h,s,a) \in [H] \times \cS \times \cA} d^{\pi^{t+1}}_h(s,a) \sqrt{\Var_{p_h}[V^{\pi^{t+1}}_{\lambda,h+1}](s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} \\
        &\leq 12 \rme^{10} \sqrt{\sum_{(h,s,a) \in [H] \times \cS \times \cA} d^{\pi^{t+1}}_h(s,a) \Var_{p_h}[V^{\pi^{t+1}}_{\lambda,h+1}](s,a)} \cdot \sqrt{\sum_{(h,s,a) \in [H] \times \cS \times \cA} d^{\pi^{t+1}}_h(s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}}.
    \end{align*}
    For the first multiplier we apply the law of total variance (Lemma~\ref{lem:law_of_total_variance})
    \begin{align*}
        \sum_{h,s,a} d^{\pi^{t+1}}_h(s,a) \Var_{p_h}[V^{\pi^{t+1}}_{\lambda,h+1}](s,a) &\leq \sum_{h,s,a} d^{\pi^{t+1}}_h(s,a) \Var_{p_h}[V^{\pi^{t+1}}_{\lambda,h+1}](s,a) + \sum_{h,s}d^{\pi^{t+1}}_h(s) \Var_{\pi^{t+1}_h}[Q^{\pi^{t+1}}_{\lambda,h}](s) \\
        &= \Vvar^{\cH, \pi^{t+1}}_1(s_1) \leq H^2\Rmax^2.
    \end{align*}
    Therefore, 
    \[
        \termA \leq 24\rme^{10} H\Rmax\sqrt{\sum_{(h,s,a) \in [H] \times \cS \times \cA} d^{\pi^{t+1}}_h(s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}}.
    \]

    \paragraph{Term $\termB$.} For this term we may apply Jensen's inequality
    \begin{align*}
        \termB &\leq 2\mu H\rme^{10} \E_{\pi^{t+1}} \left[ \frac{1}{H}\sum_{h=1}^H \sqrt{\frac{2\beta^{\cH}(\delta, n^t_h(s_h,a_h))}{n^t_h(s_h,a_h)}} \bigg| s_1 \right] \leq \mu \sqrt{8H}\rme^{10}\sqrt{\sum_{h,s,a} d^{\pi^{t+1}}_h(s,a) \frac{\beta^{\cH}(\delta, n^t_h(s,a))}{n^t_h(s,a)}}.
    \end{align*}

    By summing up and replacing counts by pseudo-counts by Lemma~\ref{lem:cnt_pseudo} we obtain
    \begin{align*}
        \pi^{t+1}_1 G^t_1(s_1) &\leq 48\rme^{10} H\Rmax\sqrt{\sum_{(h,s,a) \in [H] \times \cS \times \cA} d^{\pi^{t+1}}_h(s,a) \frac{\beta^{\conc}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1}} \\
        &+ 4\mu \rme^{10} \sqrt{2H}\sqrt{\sum_{h,s,a} d^{\pi^{t+1}}_h(s,a) \frac{\beta^{\cH}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1}} \\
        &+  4\rme^{10} H^2 \Rmax  \sum_{h,s,a} d^{\pi^{t+1}}_h(s,a)  \frac{\beta^{\KL}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1}.
    \end{align*}

    The last step is to notice that for $t < \tau$ we have $\pi^{t+1}_1 G^t_1(s_1) \geq \varepsilon$, thus summing over all $t < \tau$ we have
    \begin{align*}
        (\tau-1) \varepsilon &\leq 48\rme^{10} H\Rmax \sum_{t=1}^{\tau-1}\sqrt{\sum_{(h,s,a) \in [H] \times \cS \times \cA} d^{\pi^{t+1}}_h(s,a) \frac{\beta^{\conc}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1}} \\
        &+ 4\mu \rme^{10} \sqrt{2H} \sum_{t=1}^{\tau-1}\sqrt{\sum_{h,s,a} d^{\pi^{t+1}}_h(s,a) \frac{\beta^{\cH}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1}} \\
        &+  4\rme^{10} H^2 \Rmax  \sum_{t=1}^{\tau-1} \sum_{h,s,a} d^{\pi^{t+1}}_h(s,a)  \frac{\beta^{\KL}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1}.
    \end{align*}

    Also we notice that $\beta^{\KL}(\delta, \cdot), \beta^{\conc}(\delta, \cdot), \beta^{\cH}(\delta, \cdot)$ are monotone, thus we may replace $\upn^t_h(s,a)$ with a stopping time $\tau$. Thus, by Jensen's inequality
    \begin{align*}
        (\tau-1) \varepsilon &\leq 48\rme^{10} H\Rmax \sqrt{(\tau-1) \cdot \beta^{\conc}(\delta, \tau-1)}\sqrt{\sum_{t=1}^{\tau-1} \sum_{(h,s,a) \in [H] \times \cS \times \cA} d^{\pi^{t+1}}_h(s,a) \frac{1}{\upn^t_h(s,a)\vee 1} } \\
        &+ 4\mu \rme^{10} \sqrt{2H \beta^{\cH}(\delta, \tau) \cdot (\tau-1)} \sqrt{\sum_{t=1}^{\tau-1} \sum_{h,s,a} d^{\pi^{t+1}}_h(s,a) \frac{1}{\upn^t_h(s,a) \vee 1}} \\
        &+  4\rme^{10} H^2 \Rmax\beta^{\KL}(\delta, \tau-1)  \sum_{t=1}^{\tau-1} \sum_{h,s,a} d^{\pi^{t+1}}_h(s,a)  \frac{1}{\upn^t_h(s,a) \vee 1}.
    \end{align*}
    Furthermore, notice 
    \[
        \sum_{t=1}^{\tau-1} d^{\pi^{t+1}}_h(s,a)  \frac{1}{\upn^t_h(s,a) \vee 1} = \sum_{t=1}^{\tau-1} \frac{\upn^{t+1}_h(s,a) - \upn^t_h(s,a)}{\upn^t_h(s,a) \vee 1},
    \]
    thus Lemma~\ref{lem:sum_1_over_n} is applicable:
        \begin{align*}
        (\tau-1) \varepsilon &\leq 96\rme^{10} \Rmax \sqrt{(\tau-1)H^3SA \log(\tau) \cdot \beta^{\conc}(\delta, \tau-1)} \\
        &+ 2\mu\rme^{10} \sqrt{2H^2 SA \log^3(\tau) \beta^{\cH}(\delta, \tau) \cdot (\tau-1)} \\
        &+  8\rme^{10} H^3SA \log(\tau) \Rmax\beta^{\KL}(\delta, \tau-1).
    \end{align*}
    By the definitions of $\beta^{\KL}, \beta^{\conc}, \beta^{\cH}$ we have the following inequality
    \begin{align*}
        (\tau-1) \varepsilon &\leq 96\rme^{10} \Rmax \sqrt{(\tau-1)H^3SA \log(\tau) \cdot (\log(16SAH/\delta) + 2 \log(\rme \tau)) } \\
        &+ 12\mu\rme^{10} \sqrt{H^2 SA (\tau-1) \log^3(\tau) \cdot (\log(4SAH/\delta) + 2\log(\rme \tau) ) } \\
        &+  16\rme^{10} H^3SA \log(\tau) \Rmax (\log(4SAH/\delta) + S\log(\rme \tau)).
    \end{align*}
    Under assumption $\tau \geq 2$ we can proceed with the further simplifications
    \begin{align}\label{eq:tau_inequality}
        \begin{split}
            \tau \varepsilon &\leq 216\rme^{10} \Rmax \sqrt{\tau H^3SA \log^3(\tau) \cdot (\log(16 SAH/\delta) + 2 \log(\rme \tau)) } \\
        &+  32\rme^{10} H^3SA \log(\tau) \Rmax (\log(16SAH/\delta) + S\log(\rme \tau)).
        \end{split}
    \end{align}

    Let us define the following constants
    \[
        A = 216\rme^{10} \Rmax \cdot \sqrt{\frac{H^3SA}{\varepsilon^2}}, \quad B =\log(16 SAH/\delta), \quad C = \frac{32\rme^{10} \cdot H^3 SA \Rmax }{\varepsilon}.
    \]
    Then inequality~\eqref{eq:tau_inequality} has the following form
    \[
        \tau \leq A\sqrt{\tau(B + 2\log(\rme \tau)) \cdot \log^3(\tau)} + C(B + S \log(\rme \tau))\log \tau.
    \]
    First, we obtain a loose inequality on $\tau$. Let us use the inequality $\log(x) \leq x^\beta / \beta$ for any $x > 0, \beta > 0$ with different $\beta$ for each logarithm
    \begin{align*}
        \tau &\leq A \sqrt{216\tau (B + 4(\rme \tau)^{1/4}) \tau^{1/2}}+ 4C(B + 8S/3 (\rme \tau)^{3/8}) \tau^{1/4} \\
        \Rightarrow \tau^{3/4} &\leq \tau^{3/8} \left( 6A\sqrt{6(B + 4 \rme^{1/4})} + 12CS \rme^{3/8} \right) + 4CB.
    \end{align*}
    
    Notice that the solution to the inequality $x^2 \leq ax + b$ could be upper-bounded as follows
    \[
        x \leq \frac{a + \sqrt{a^2 + 4b}}{2} \leq a + \sqrt{b},
    \]
    thus
    \[
        \tau^{3/8} \leq \left( 6A\sqrt{6(B + 4 \rme^{1/4})} + 12CS \rme^{3/8} \right) + 2\sqrt{CB}.
    \]
    Define $L = 8/3 \log\left( 6A\sqrt{6(B + 4 \rme^{1/4})} + 12CS \rme^{3/8} + 2\sqrt{CB}\right) = \cO(\log(SAH\Rmax/\varepsilon) + \log\log(SAH/\delta) )$ and we have $\log(\tau) \leq L$. Then we have that the solution to \eqref{eq:tau_inequality} is a subset of solutions to
    \[
        \tau \leq A\sqrt{\tau(B + 2(1+L)) \cdot L^3} + C(B + S(1+L))L,
    \]
    solving this inequality we obtain the bound
    \[
        \tau \leq 2A^2(B + 2(1+L))L^3 + 2CB(S(1+L))L.
    \]    
\end{proof}

