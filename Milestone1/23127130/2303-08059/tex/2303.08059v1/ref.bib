@inproceedings{chentanez2004intrinsically,
 author = {Chentanez, Nuttapong and Barto, Andrew and Singh, Satinder},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {L. Saul and Y. Weiss and L. Bottou},
 pages = {},
 publisher = {MIT Press},
 title = {Intrinsically Motivated Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2004/file/4be5a36cbaca8ab9d2066debfe4e65c1-Paper.pdf},
 volume = {17},
 year = {2004}
}




@InProceedings{tarbouriech2020active,
  title = 	 {Active Model Estimation in Markov Decision Processes},
  author =       {Tarbouriech, Jean and Shekhar, Shubhanshu and Pirotta, Matteo and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
  booktitle = 	 {Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI)},
  pages = 	 {1019--1028},
  year = 	 {2020},
  editor = 	 {Peters, Jonas and Sontag, David},
  volume = 	 {124},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {03--06 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v124/tarbouriech20a/tarbouriech20a.pdf},
  url = 	 {https://proceedings.mlr.press/v124/tarbouriech20a.html},
  abstract = 	 {We study the problem of efficient exploration in order to learn an accurate model of an environment, modeled as a Markov decision process (MDP). Efficient exploration in this problem requires the agent to identify the regions in which estimating the model is more difficult and then exploit this knowledge to collect more samples there. In this paper, we formalize this problem, introduce the first algorithm to learn an $\epsilon$-accurate estimate of the dynamics, and provide its sample complexity analysis. While this algorithm enjoys strong guarantees in the large-sample regime, it tends to have a poor performance in early stages of exploration. To address this issue, we propose an algorithm that is based on maximum weighted entropy, a heuristic that stems from common sense and our theoretical analysis. The main idea here is to cover the entire state-action space with the weight proportional to the noise in their transition functions. Using a number of simple domains with heterogeneous noise in their transitions, we show that our heuristic-based algorithm outperforms both our original algorithm and the maximum entropy algorithm in the small sample regime, while achieving similar asymptotic performance as that of the original algorithm. }
}



@inproceedings{tarbouriech2021provably,
 author = {Tarbouriech, Jean and Pirotta, Matteo and Valko, Michal and Lazaric, Alessandro},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {7611--7624},
 publisher = {Curran Associates, Inc.},
 title = {A Provably Efficient Sample Collection Strategy for Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2021/file/3e98410c45ea98addec555019bbae8eb-Paper.pdf},
 volume = {34},
 year = {2021}
}



@article{tirinzoni2022optimistic,
  author    = {Andrea Tirinzoni and
               Aymen Al Marjani and
               Emilie Kaufmann},
  title     = {Optimistic {PAC} Reinforcement Learning: the Instance-Dependent View},
  journal   = {CoRR},
  volume    = {abs/2207.05852},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2207.05852},
  doi       = {10.48550/arXiv.2207.05852},
  eprinttype = {arXiv},
  eprint    = {2207.05852},
  timestamp = {Tue, 19 Jul 2022 17:45:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2207-05852.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@article{schulman2017equivalence,
  author    = {John Schulman and
               Pieter Abbeel and
               Xi Chen},
  title     = {Equivalence Between Policy Gradients and Soft Q-Learning},
  journal   = {CoRR},
  volume    = {abs/1704.06440},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.06440},
  eprinttype = {arXiv},
  eprint    = {1704.06440},
  timestamp = {Mon, 03 Sep 2018 12:15:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SchulmanAC17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{fox2016taming,
  author    = {Roy Fox and
               Ari Pakman and
               Naftali Tishby},
  editor    = {Alexander Ihler and
               Dominik Janzing},
  title     = {Taming the Noise in Reinforcement Learning via Soft Updates},
  booktitle = {Proceedings of the Thirty-Second Conference on Uncertainty in Artificial
               Intelligence, {UAI} 2016, June 25-29, 2016, New York City, NY, {USA}},
  publisher = {{AUAI} Press},
  year      = {2016},
  url       = {http://auai.org/uai2016/proceedings/papers/219.pdf},
  timestamp = {Mon, 05 Dec 2022 15:54:38 +0100},
  biburl    = {https://dblp.org/rec/conf/uai/FoxPT16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{neu2017unified,
  author    = {Gergely Neu and
               Anders Jonsson and
               Vicen{\c{c}} G{\'{o}}mez},
  title     = {A unified view of entropy-regularized Markov decision processes},
  journal   = {CoRR},
  volume    = {abs/1705.07798},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.07798},
  eprinttype = {arXiv},
  eprint    = {1705.07798},
  timestamp = {Mon, 13 Aug 2018 16:47:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/NeuJG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{lee2019efficient,
  author    = {Lisa Lee and
               Benjamin Eysenbach and
               Emilio Parisotto and
               Eric P. Xing and
               Sergey Levine and
               Ruslan Salakhutdinov},
  title     = {Efficient Exploration via State Marginal Matching},
  journal   = {CoRR},
  volume    = {abs/1906.05274},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.05274},
  eprinttype = {arXiv},
  eprint    = {1906.05274},
  timestamp = {Sat, 23 Jan 2021 01:14:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-05274.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{mutti2021task,
  author    = {Mirco Mutti and
               Lorenzo Pratissoli and
               Marcello Restelli},
  title     = {Task-Agnostic Exploration via Policy Gradient of a Non-Parametric
               State Entropy Estimate},
  booktitle = {Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2021, Thirty-Third Conference on Innovative Applications of Artificial
               Intelligence, {IAAI} 2021, The Eleventh Symposium on Educational Advances
               in Artificial Intelligence, {EAAI} 2021, Virtual Event, February 2-9,
               2021},
  pages     = {9028--9036},
  publisher = {{AAAI} Press},
  year      = {2021},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/17091},
  timestamp = {Sat, 05 Jun 2021 18:11:55 +0200},
  biburl    = {https://dblp.org/rec/conf/aaai/MuttiPR21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{frank1956algorithm,
author = {Frank, Marguerite and Wolfe, Philip},
title = {An algorithm for quadratic programming},
journal = {Naval Research Logistics Quarterly},
volume = {3},
number = {1-2},
pages = {95-110},
doi = {https://doi.org/10.1002/nav.3800030109},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800030109},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800030109},
year = {1956}
}




@article{cheung2019exploration,
  author    = {Wang Chi Cheung},
  title     = {Exploration-Exploitation Trade-off in Reinforcement Learning on Online
               Markov Decision Processes with Global Concave Rewards},
  journal   = {CoRR},
  volume    = {abs/1905.06466},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.06466},
  eprinttype = {arXiv},
  eprint    = {1905.06466},
  timestamp = {Tue, 28 May 2019 12:48:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-06466.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{escoffet2019goexplore,
  author    = {Adrien Ecoffet and
               Joost Huizinga and
               Joel Lehman and
               Kenneth O. Stanley and
               Jeff Clune},
  title     = {Go-Explore: a New Approach for Hard-Exploration Problems},
  journal   = {CoRR},
  volume    = {abs/1901.10995},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.10995},
  eprinttype = {arXiv},
  eprint    = {1901.10995},
  timestamp = {Sun, 03 Feb 2019 14:23:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-10995.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{oudeyer2007intrisic,
  author    = {Pierre{-}Yves Oudeyer and
               Fr{\'{e}}d{\'{e}}ric Kaplan and
               Verena Vanessa Hafner},
  title     = {Intrinsic Motivation Systems for Autonomous Mental Development},
  journal   = {{IEEE} Trans. Evol. Comput.},
  volume    = {11},
  number    = {2},
  pages     = {265--286},
  year      = {2007},
  url       = {https://doi.org/10.1109/TEVC.2006.890271},
  doi       = {10.1109/TEVC.2006.890271},
  timestamp = {Tue, 12 May 2020 16:51:08 +0200},
  biburl    = {https://dblp.org/rec/journals/tec/OudeyerKH07.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{colas2022autotelic,
  author    = {C{\'{e}}dric Colas and
               Tristan Karch and
               Olivier Sigaud and
               Pierre{-}Yves Oudeyer},
  title     = {Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement
               Learning: {A} Short Survey},
  journal   = {J. Artif. Intell. Res.},
  volume    = {74},
  pages     = {1159--1199},
  year      = {2022},
  url       = {https://doi.org/10.1613/jair.1.13554},
  doi       = {10.1613/jair.1.13554},
  timestamp = {Mon, 25 Jul 2022 17:24:45 +0200},
  biburl    = {https://dblp.org/rec/journals/jair/ColasKSO22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{tarbouriech2020improved,
 author = {Tarbouriech, Jean and Pirotta, Matteo and Valko, Michal and Lazaric, Alessandro},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {11273--11284},
 publisher = {Curran Associates, Inc.},
 title = {Improved Sample Complexity for Incremental Autonomous Exploration in MDPs},
 url = {https://proceedings.neurips.cc/paper/2020/file/81e793dc8317a3dbc3534ed3f242c418-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{rosenberg2019online,
  author    = {Aviv Rosenberg and
               Yishay Mansour},
  editor    = {Kamalika Chaudhuri and
               Ruslan Salakhutdinov},
  title     = {Online Convex Optimization in Adversarial Markov Decision Processes},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {5478--5486},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/rosenberg19a.html},
  timestamp = {Sat, 22 Jun 2019 09:10:29 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/RosenbergM19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{basserrano2021logistic,
  author    = {Joan Bas{-}Serrano and
               Sebastian Curi and
               Andreas Krause and
               Gergely Neu},
  editor    = {Arindam Banerjee and
               Kenji Fukumizu},
  title     = {Logistic Q-Learning},
  booktitle = {The 24th International Conference on Artificial Intelligence and Statistics,
               {AISTATS} 2021, April 13-15, 2021, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {130},
  pages     = {3610--3618},
  publisher = {{PMLR}},
  year      = {2021},
  url       = {http://proceedings.mlr.press/v130/bas-serrano21a.html},
  timestamp = {Wed, 14 Apr 2021 18:58:38 +0200},
  biburl    = {https://dblp.org/rec/conf/aistats/Bas-SerranoC0N21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{zimin2013online,
  author    = {Alexander Zimin and
               Gergely Neu},
  editor    = {Christopher J. C. Burges and
               L{\'{e}}on Bottou and
               Zoubin Ghahramani and
               Kilian Q. Weinberger},
  title     = {Online learning in episodic Markovian decision processes by relative
               entropy policy search},
  booktitle = {Advances in Neural Information Processing Systems 26: 27th Annual
               Conference on Neural Information Processing Systems 2013. Proceedings
               of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States},
  pages     = {1583--1591},
  year      = {2013},
  url       = {https://proceedings.neurips.cc/paper/2013/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/ZiminN13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{peters2010relative,
author = {Peters, Jan and M\"{u}lling, Katharina and Alt\"{u}n, Yasemin},
title = {Relative Entropy Policy Search},
year = {2010},
publisher = {AAAI Press},
abstract = {Policy search is a successful approach to reinforcement learning. However, policy improvements often result in the loss of information. Hence, it has been marred by premature convergence and implausible solutions. As first suggested in the context of covariant policy gradients (Bagnell and Schneider 2003), many of these problems may be addressed by constraining the information loss. In this paper, we continue this path of reasoning and suggest the Relative Entropy Policy Search (REPS) method. The resulting method differs significantly from previous policy gradient approaches and yields an exact update step. It works well on typical reinforcement learning benchmark problems.},
booktitle = {Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence},
pages = {1607–1612},
numpages = {6},
location = {Atlanta, Georgia},
series = {AAAI'10}
}


@inproceedings{haarnoja2017reinforcement,
author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
title = {Reinforcement Learning with Deep Energy-Based Policies},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1352–1361},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}


@InProceedings{haarnoja2018soft,
  title = 	 {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author =       {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1861--1870},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf},
  url = 	 {https://proceedings.mlr.press/v80/haarnoja18b.html},
  abstract = 	 {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.}
}




@book{cesabianchi2006prediction,
  added-at = {2019-07-29T00:00:00.000+0200},
  author = {Cesa-Bianchi, Nicolò and Lugosi, Gábor},
  biburl = {https://www.bibsonomy.org/bibtex/262edbe6d002d919a93e22e6e36ed3c78/dblp},
  ee = {https://www.wikidata.org/entity/Q59538584},
  interhash = {1170748dfb0bbdd1a80fce936f0fe46b},
  intrahash = {62edbe6d002d919a93e22e6e36ed3c78},
  isbn = {978-0-511-54692-1},
  keywords = {dblp},
  pages = {I-XII, 1-394},
  publisher = {Cambridge University Press},
  timestamp = {2019-07-30T11:47:45.000+0200},
  title = {Prediction, learning, and games.},
  year = 2006
}




@article{sion1958general,
author = {Maurice Sion},
title = {{On general minimax theorems.}},
volume = {8},
journal = {Pacific Journal of Mathematics},
number = {1},
publisher = {Pacific Journal of Mathematics, A Non-profit Corporation},
pages = {171 -- 176},
year = {1958},
doi = {pjm/1103040253},
URL = {https://doi.org/}
}




@article{eysenbach2019if,
  author    = {Benjamin Eysenbach and
               Sergey Levine},
  title     = {If MaxEnt {RL} is the Answer, What is the Question?},
  journal   = {CoRR},
  volume    = {abs/1910.01913},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.01913},
  eprinttype = {arXiv},
  eprint    = {1910.01913},
  timestamp = {Wed, 09 Oct 2019 14:07:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-01913.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{grunwald2002game,
  author    = {Peter D. Gr{\"{u}}nwald and
               A. Philip Dawid},
  title     = {Game theory, maximum generalized entropy, minimum discrepancy, robust
               Bayes and Pythagoras},
  booktitle = {Proceedings of the 2002 {IEEE} Information Theory Workshop, {ITW}
               2002, 20-25 October 2002, Bangalore, India},
  pages     = {94--97},
  publisher = {{IEEE}},
  year      = {2002},
  url       = {https://doi.org/10.1109/ITW.2002.1115425},
  doi       = {10.1109/ITW.2002.1115425},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
  biburl    = {https://dblp.org/rec/conf/itw/GrunwaldD02.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@misc{jin2018qlearning,
      title={Is Q-learning Provably Efficient?}, 
      author={Chi Jin and Zeyuan Allen-Zhu and Sebastien Bubeck and Michael I. Jordan},
      year={2018},
      eprint={1807.03765},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wang2019optimism,
      title={Optimism in Reinforcement Learning with Generalized Linear Function Approximation}, 
      author={Yining Wang and Ruosong Wang and Simon S. Du and Akshay Krishnamurthy},
      year={2019},
      eprint={1912.04136},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{russo2019worstcase,
      title={Worst-Case Regret Bounds for Exploration via Randomized Value Functions}, 
      author={Daniel Russo},
      year={2019},
      eprint={1906.02870},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ishfaq2021randomized,
      title={Randomized Exploration for Reinforcement Learning with General Value Function Approximation}, 
      author={Haque Ishfaq and Qiwen Cui and Viet Nguyen and Alex Ayoub and Zhuoran Yang and Zhaoran Wang and Doina Precup and Lin F. Yang},
      year={2021},
      eprint={2106.07841},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chernozhukov2019improved,
      title={Improved Central Limit Theorem and bootstrap approximations in high dimensions}, 
      author={Victor Chernozhukov and Denis Chetverikov and Kengo Kato and Yuta Koike},
      year={2019},
      eprint={1912.10529},
      archivePrefix={arXiv},
      primaryClass={math.ST}
}

@misc{hao2019bootstrapping,
      title={Bootstrapping Upper Confidence Bound}, 
      author={Botao Hao and Yasin Abbasi-Yadkori and Zheng Wen and Guang Cheng},
      year={2019},
      eprint={1906.05247},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{domingues2021kernel,
  title = 	 {Kernel-Based Reinforcement Learning: A Finite-Time Analysis},
  author =       {Domingues, Omar Darwiche and M\'enard, Pierre and Pirotta, Matteo and Kaufmann, Emilie and Valko, Michal},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2783--2792},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/domingues21a/domingues21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/domingues21a.html},
  abstract = 	 {We consider the exploration-exploitation dilemma in finite-horizon reinforcement learning problems whose state-action space is endowed with a metric. We introduce Kernel-UCBVI, a model-based optimistic algorithm that leverages the smoothness of the MDP and a non-parametric kernel estimator of the rewards and transitions to efficiently balance exploration and exploitation. For problems with $K$ episodes and horizon $H$, we provide a regret bound of $\widetilde{O}\left( H^3 K^{\frac{2d}{2d+1}}\right)$, where $d$ is the covering dimension of the joint state-action space. This is the first regret bound for kernel-based RL using smoothing kernels, which requires very weak assumptions on the MDP and applies to a wide range of tasks. We empirically validate our approach in continuous MDPs with sparse rewards.}
}


@article{filippi2010optimism,
   title={Optimism in reinforcement learning and Kullback-Leibler divergence},
   url={http://dx.doi.org/10.1109/ALLERTON.2010.5706896},
   DOI={10.1109/allerton.2010.5706896},
   journal={2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
   publisher={IEEE},
   author={Filippi, Sarah and Cappe, Olivier and Garivier, Aurelien},
   year={2010},
   month={Sep}
}



@InProceedings{riou20bandit,
  title = 	 {Bandit Algorithms Based on Thompson Sampling for Bounded Reward Distributions},
  author =       {Riou, Charles and Honda, Junya},
  booktitle = 	 {Proceedings of the 31st International Conference  on Algorithmic Learning Theory},
  pages = 	 {777--826},
  year = 	 {2020},
  editor = 	 {Kontorovich, Aryeh and Neu, Gergely},
  volume = 	 {117},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {08 Feb--11 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v117/riou20a/riou20a.pdf},
  url = 	 {https://proceedings.mlr.press/v117/riou20a.html},
  abstract = 	 {We focus on a classic reinforcement learning problem, called a multi-armed bandit, and more specifically in the stochastic setting with reward distributions bounded in $[0,1]$. For this model, an optimal problem-dependent asymptotic regret lower bound has been derived. However, the existing algorithms achieving this regret lower bound all require to solve an optimization problem at each step, inducing a large complexity. In this paper, we propose two new algorithms, which we prove to achieve the problem-dependent asymptotic regret lower bound. The first one, which we call Multinomial TS, is an adaptation of Thompson Sampling for Bernoulli rewards to multinomial reward distributions whose support is included in $\{0, \frac{1}{M}, …, 1\}$. This algorithm achieves the regret lower bound in the case of multinomial distributions with the aforementioned support, and it can be easily generalized to bounded reward distributions in $[0, 1]$ by randomly rounding the observed rewards. The second algorithm we introduce, which we call Non-parametric TS, is a randomized algorithm but not based on the posterior sampling in the strict sense. At each step, it computes an average of the observed rewards with random weight. Not only is it asymptotically optimal, but also it performs very well even for small horizons.}
}

@misc{baudry2021optimality,
      title={From Optimality to Robustness: Dirichlet Sampling Strategies in Stochastic Bandits}, 
      author={Dorian Baudry and Patrick Saux and Odalric-Ambrym Maillard},
      year={2021},
      eprint={2111.09724},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{agrawal2020posterior,
      title={Posterior sampling for reinforcement learning: worst-case regret bounds}, 
      author={Shipra Agrawal and Randy Jia},
      year={2020},
      eprint={1705.07041},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{osband2013more,
      title={(More) Efficient Reinforcement Learning via Posterior Sampling}, 
      author={Ian Osband and Daniel Russo and Benjamin Van Roy},
      year={2013},
      eprint={1306.0940},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{litvak2005smallest,
title = {Smallest singular value of random matrices and geometry of random polytopes},
journal = {Advances in Mathematics},
volume = {195},
number = {2},
pages = {491-523},
year = {2005},
issn = {0001-8708},
doi = {https://doi.org/10.1016/j.aim.2004.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0001870804002750},
author = {A.E. Litvak and A. Pajor and M. Rudelson and N. Tomczak-Jaegermann},
keywords = {Random matrices, Random polytopes, Singular values, Deviation inequalities},
abstract = {We study the behaviour of the smallest singular value of a rectangular random matrix, i.e., matrix whose entries are independent random variables satisfying some additional conditions. We prove a deviation inequality and show that such a matrix is a “good” isomorphism on its image. Then, we obtain asymptotically sharp estimates for volumes and other geometric parameters of random polytopes (absolutely convex hulls of rows of random matrices). All our results hold with high probability, that is, with probability exponentially (in dimension) close to 1.}
}

@article{lasserre2020simple,
author = {Lasserre, Jean-Bernard},
year = {2020},
month = {08},
pages = {},
title = {Simple formula for integration of polynomials on a simplex},
volume = {61},
journal = {BIT Numerical Mathematics},
doi = {10.1007/s10543-020-00828-x}
}

@phdthesis{dirksen2015sections, title={Sections of simplices and cylinders: Volume formulas and estimates}, url={https://macau.uni-kiel.de/receive/diss_mods_00018308}, abstractNote={We investigate sections of simplices and generalized cylinders. We are interested in the volume of sections of these bodies with affine subspaces and give formulas and estimates for these volumes. For the regular n-simplex we state a general formula to compute the volume of the intersection with some k-dimensional subspace. A formula for central hyperplane sections was given by S. Webb. He also showed that the hyperplane through the centroid containing n-1 vertices gives the maximal volume. We generalize the formula to arbitrary dimensional sections that do not necessarily have to contain the centroid. And we show that, for a prescribed small distance of a hyperplane to the centroid, still the hyperplane containing n-1 vertices is volume maximizing. The proof also yields a new and short argument for Webb’s result. The minimal hyperplane section is conjectured to be the one parallel to a face. We show that this hyperplane section is indeed minimal for dimensions n=2,3,4 and that it is a local minimum in general. Using results by Brehm e.a. we compute the average hyperplane section volume. For k-dimensional sections we give an upper bound. Finally we modify our volume formula to compute the section volume of irregular simplices. As an application we show that in odd dimensions larger than 4 there exist irregular simplices whose maximal section is not a face. A generalized cylinder is the Cartesian product of a n-dimensional cube and a m-dimensional ball of radius r. We study the behavior of the hyperplane section volume depending on the radius of the cylinder. First we show for the 3-dimensional cylinder that always a truncated ellipse gives the maximal volume. This is done by elementary geometric considerations and calculus. For the generalized cylinder we use the Fourier transform to derive an explicit formula. Then we estimate this by Hölder’s inequality. Finally it remains to prove an integral inequality that is similar to the inequality of K. Ball for the cube.}, author={Dirksen, Hauke Carl-Erwin}, year={2015} }



@book {olver1997asymptotics,
    AUTHOR = {Olver, Frank W. J.},
     TITLE = {Asymptotics and special functions},
    SERIES = {AKP Classics},
      NOTE = {Reprint of the 1974 original [Academic Press, New York;
              MR0435697 (55 \#8655)]},
 PUBLISHER = {A K Peters, Ltd., Wellesley, MA},
      YEAR = {1997},
     PAGES = {xviii+572},
      ISBN = {1-56881-069-5},
   MRCLASS = {41-02 (33Cxx 41A60 65D20)},
  MRNUMBER = {1429619},
}

@misc{tiapkin2022dirichlet,
      title={From Dirichlet to Rubin: Optimistic Exploration in RL without Bonuses}, 
      author={Daniil Tiapkin and Denis Belomestny and Eric Moulines and Alexey Naumov and Sergey Samsonov and Yunhao Tang  and Michal Valko and Pierre Menard},
      year={2022},
}

@book{puterman1994,
author = {Puterman, Martin L.},
doi = {10.2307/2983520},
file = {:Users/xuedong/Library/Application Support/Mendeley Desktop/Downloaded/Puterman - 1994 - Markov Decision Processes - Discrete Stochastic Dynamic Programming.pdf:pdf},
isbn = {3175723993},
issn = {09641998},
pages = {666},
publisher = {John Wiley {\&} Sons, Inc},
title = {{Markov Decision Processes - Discrete Stochastic Dynamic Programming}},
url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887},
year = {1994}
}

@misc{rlberry,
    author = {Domingues, Omar Darwiche and Flet-Berliac, Yannis and Leurent, Edouard and M{\'e}nard, Pierre and Shang, Xuedong and Valko, Michal},
    doi = {10.5281/zenodo.5544540},
    month = {10},
    title = {{rlberry - A Reinforcement Learning Library for Research and Education}},
    url = {https://github.com/rlberry-py/rlberry},
    year = {2021}
}


@inproceedings{garivier2011the,
  title = 	 {The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond},
  author = 	 {Garivier, Aur\'{e}lien and Capp\'{e}, Olivier},
  booktitle = 	 {Proceedings of the 24th Annual Conference on Learning Theory},
  pages = 	 {359--376},
  year = 	 {2011},
  editor = 	 {Kakade, Sham M. and von Luxburg, Ulrike},
  volume = 	 {19},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Budapest, Hungary},
  month = 	 {09--11 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v19/garivier11a/garivier11a.pdf},
  url = 	 {https://proceedings.mlr.press/v19/garivier11a.html},
  abstract = 	 {This paper presents a finite-time analysis of the KL-UCB algorithm, an online, horizon-free  index policy for stochastic bandit problems.  We prove two distinct results: first, for arbitrary bounded rewards, the KL-UCB algorithm  satisfies a uniformly better regret bound than UCB and its variants; second, in the special case of  Bernoulli rewards, it reaches the lower bound of Lai and Robbins.  Furthermore, we show that simple adaptations of the KL-UCB algorithm are also optimal for  specific classes of (possibly unbounded) rewards, including those generated from exponential  families of distributions.  A large-scale numerical study comparing KL-UCB with its main competitors (UCB, MOSS,  UCB-Tuned, UCB-V, DMED) shows that KL-UCB is remarkably efficient and stable, including for short time horizons. KL-UCB is also the only method that always performs better  than the basic UCB policy.  Our regret bounds rely on deviations results of independent interest which are stated and proved  in the Appendix. As a by-product, we also obtain an improved regret bound for the standard UCB  algorithm.}
}



@misc{menard2021ucb,
      title={UCB Momentum Q-learning: Correcting the bias without forgetting}, 
      author={Pierre M\'enard and Omar Darwiche Domingues and Xuedong Shang and Michal Valko},
      year={2021},
      eprint={2103.01312},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@inproceedings{russo2014learning,
 author = {Russo, Daniel and Van Roy, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning to Optimize via Information-Directed Sampling},
 url = {https://proceedings.neurips.cc/paper/2014/file/301ad0e3bd5cb1627a2044908a42fdc2-Paper.pdf},
 volume = {27},
 year = {2014}
}

@article{qian2020concentration,
  author    = {Jian Qian and
               Ronan Fruit and
               Matteo Pirotta and
               Alessandro Lazaric},
  title     = {Concentration Inequalities for Multinoulli Random Variables},
  journal   = {CoRR},
  volume    = {abs/2001.11595},
  year      = {2020},
  url       = {https://arxiv.org/abs/2001.11595},
  eprinttype = {arXiv},
  eprint    = {2001.11595},
  timestamp = {Mon, 03 Feb 2020 11:21:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2001-11595.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{badia2020never,
abstract = {We propose a reinforcement learning agent to solve hard exploration games by learning a range of directed exploratory policies. We construct an episodic memory-based intrinsic reward using k-nearest neighbors over the agent's recent experience to train the directed exploratory policies, thereby encouraging the agent to repeatedly revisit all states in its environment. A self-supervised inverse dynamics model is used to train the embeddings of the nearest neighbour lookup, biasing the novelty signal towards what the agent can control. We employ the framework of Universal Value Function Approximators (UVFA) to simultaneously learn many directed exploration policies with the same neural network, with different trade-offs between exploration and exploitation. By using the same neural network for different degrees of exploration/exploitation, transfer is demonstrated from predominantly exploratory policies yielding effective exploitative policies. The proposed method can be incorporated to run with modern distributed RL agents that collect large amounts of experience from many actors running in parallel on separate environment instances. Our method doubles the performance of the base agent in all hard exploration in the Atari-57 suite while maintaining a very high score across the remaining games, obtaining a median human normalised score of 1344.0{\%}. Notably, the proposed method is the first algorithm to achieve non-zero rewards (with a mean score of 8,400) in the game of Pitfall! without using demonstrations or hand-crafted features.},
archivePrefix = {arXiv},
arxivId = {2002.06038},
author = {Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Piot, Bilal and Kapturowski, Steven and Tieleman, Olivier and Arjovsky, Mart{\'{i}}n and Pritzel, Alexander and Bolt, Andew and Blundell, Charles},
booktitle = {International Conference on Learning Representations},
eprint = {2002.06038},
month = {feb},
title = {{Never Give Up: Learning directed exploration Strategies}},
url = {https://arxiv.org/abs/2002.06038},
year = {2020}
}


@article{efron1979bootstrap,
author = {B. Efron},
title = {Bootstrap Methods: Another Look at the Jackknife},
volume = {7},
journal = {The Annals of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {1 -- 26},
keywords = {bootstrap, discriminant analysis, error rate estimation, jackknife, Nonlinear regression, nonparametric variance estimation, Resampling, subsample values},
year = {1979},
doi = {10.1214/aos/1176344552},
URL = {https://doi.org/10.1214/aos/1176344552}
}



@inproceedings{fruit2018efficient,
  title={Efficient bias-span-constrained exploration-exploitation in reinforcement learning},
  author={Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro and Ortner, Ronald},
  booktitle={International Conference on Machine Learning},
  pages={1578--1586},
  year={2018},
  organization={PMLR}
}


@article{thompson1933on,
    author = {Thompson, William R},
    title = {On the likelihood that one unknown probability exceeds another in view of the evidence of two samples},
    journal = {Biometrika},
    volume = {25},
    number = {3-4},
    pages = {285-294},
    year = {1933},
    month = {12},
    issn = {0006-3444},
    doi = {10.1093/biomet/25.3-4.285},
    url = {https://doi.org/10.1093/biomet/25.3-4.285},
    eprint = {https://academic.oup.com/biomet/article-pdf/25/3-4/285/513725/25-3-4-285.pdf},
}





@InProceedings{kveton2019garbage,
  title = 	 {Garbage In, Reward Out: Bootstrapping Exploration in Multi-Armed Bandits},
  author =       {Kveton, Branislav and Szepesvari, Csaba and Vaswani, Sharan and Wen, Zheng and Lattimore, Tor and Ghavamzadeh, Mohammad},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3601--3610},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/kveton19a/kveton19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/kveton19a.html},
  abstract = 	 {We propose a bandit algorithm that explores by randomizing its history of rewards. Specifically, it pulls the arm with the highest mean reward in a non-parametric bootstrap sample of its history with pseudo rewards. We design the pseudo rewards such that the bootstrap mean is optimistic with a sufficiently high probability. We call our algorithm Giro, which stands for garbage in, reward out. We analyze Giro in a Bernoulli bandit and derive a $O(K \Delta^{-1} \log n)$ bound on its $n$-round regret, where $\Delta$ is the difference in the expected rewards of the optimal and the best suboptimal arms, and $K$ is the number of arms. The main advantage of our exploration design is that it easily generalizes to structured problems. To show this, we propose contextual Giro with an arbitrary reward generalization model. We evaluate Giro and its contextual variant on multiple synthetic and real-world problems, and observe that it performs well.}
}



@article{yang2021exploration,
  author    = {Tianpei Yang and
               Hongyao Tang and
               Chenjia Bai and
               Jinyi Liu and
               Jianye Hao and
               Zhaopeng Meng and
               Peng Liu},
  title     = {Exploration in Deep Reinforcement Learning: {A} Comprehensive Survey},
  journal   = {CoRR},
  volume    = {abs/2109.06668},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.06668},
  eprinttype = {arXiv},
  eprint    = {2109.06668},
  timestamp = {Tue, 21 Sep 2021 17:46:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-06668.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{tang2017exploration,
 author = {Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Xi Chen, OpenAI and Duan, Yan and Schulman, John and DeTurck, Filip and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {\#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2017/file/3a20f62a0af1aa152670bab3c602feed-Paper.pdf},
 volume = {30},
 year = {2017}
}




@article{osband2017gaussian,
  author    = {Ian Osband and
               Benjamin Van Roy},
  title     = {Gaussian-Dirichlet Posterior Dominance in Sequential Learning},
  journal   = {CoRR},
  volume    = {abs/1702.04126},
  year      = {2017},
  url       = {http://arxiv.org/abs/1702.04126},
  eprinttype = {arXiv},
  eprint    = {1702.04126},
  timestamp = {Mon, 13 Aug 2018 16:46:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/OsbandR17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{hinton2012neural,
  title={Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  journal={Cited on},
  volume={14},
  number={8},
  pages={2},
  year={2012}
}


@article{watkins1992q,
author = {Watkins, Chris J. and Dayan, Peter},
journal = {Machine Learning},
number = {3-4},
pages = {279--292},
title = {{Q-learning}},
url = {https://link.springer.com/content/pdf/10.1007/BF00992698.pdf},
volume = {8},
year = {1992}
}



@inproceedings{zhang2020advantage,
author = {Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
title = {Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the reinforcement learning problem in the setting of finite-horizon episodic Markov Decision Processes (MDPs) with S states, A actions, and episode length H. We propose a model-free algorithm UCB-ADVANTAGE and prove that it achieves \~{O}(√H2SAT) regret where T = KH and K is the number of episodes to play. Our regret bound improves upon the results of [Jin et al., 2018] and matches the best known model-based algorithms as well as the information theoretic lower bound up to logarithmic factors. We also show that UCB-ADVANTAGE achieves low local switching cost and applies to concurrent reinforcement learning, improving upon the recent results of [Bai et al., 2019].},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1274},
numpages = {10},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}


@article{osband2015bootstrap,
  author    = {Ian Osband and
               Van Roy, Benjamin},
  title     = {Bootstrapped Thompson Sampling and Deep Exploration},
  journal   = {CoRR},
  volume    = {abs/1507.00300},
  year      = {2015},
  url       = {http://arxiv.org/abs/1507.00300},
  eprinttype = {arXiv},
  eprint    = {1507.00300},
  timestamp = {Mon, 13 Aug 2018 16:48:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/OsbandR15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{pacchiano2021towards,
  title = 	 {Towards tractable optimism in model-based reinforcement learning},
  author =       {Pacchiano, Aldo and Ball, Philip and Parker-Holder, Jack and Choromanski, Krzysztof and Roberts, Stephen},
  booktitle = 	 {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1413--1423},
  year = 	 {2021},
  editor = 	 {de Campos, Cassio and Maathuis, Marloes H.},
  volume = 	 {161},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {27--30 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v161/pacchiano21a/pacchiano21a.pdf},
  url = 	 {https://proceedings.mlr.press/v161/pacchiano21a.html},
  abstract = 	 {The principle of optimism in the face of uncertainty is prevalent throughout sequential decision making problems such as multi-armed bandits and reinforcement learning (RL). To be successful, an optimistic RL algorithm must over-estimate the true value function (optimism) but not by so much that it is inaccurate (estimation error). In the tabular setting, many state-of-the-art methods produce the required optimism through approaches which are intractable when scaling to deep RL. We re-interpret these scalable optimistic model-based algorithms as solving a tractable noise augmented MDP. This formulation achieves a competitive regret bound: $\tilde{\mathcal{O}}( |\mathcal{S}|H\sqrt{|\mathcal{A}| T } )$ when augmenting using Gaussian noise, where $T$ is the total number of environment steps. We also explore how this trade-off changes in the deep RL setting, where we show empirically that estimation error is significantly more troublesome. However, we also show that if this error is reduced, optimistic model-based RL algorithms can match state-of-the-art performance in continuous control problems.}
}



@inproceedings{azizzadenesheli2018efficient,
  author    = {Kamyar Azizzadenesheli and
               Emma Brunskill and
               Animashree Anandkumar},
  title     = {Efficient Exploration Through Bayesian Deep Q-Networks},
  booktitle = {2018 Information Theory and Applications Workshop, {ITA} 2018, San
               Diego, CA, USA, February 11-16, 2018},
  pages     = {1--9},
  publisher = {{IEEE}},
  year      = {2018},
  url       = {https://doi.org/10.1109/ITA.2018.8503252},
  doi       = {10.1109/ITA.2018.8503252},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
  biburl    = {https://dblp.org/rec/conf/ita/Azizzadenesheli18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{nikolov2019information,
  author    = {Nikolay Nikolov and
               Johannes Kirschner and
               Felix Berkenkamp and
               Andreas Krause},
  title     = {Information-Directed Exploration for Deep Reinforcement Learning},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=Byx83s09Km},
  timestamp = {Thu, 25 Jul 2019 14:26:00 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/NikolovKBK19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@article{mnih2015humanlevel,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}



@incollection{mnih2013playing,
  title = {Playing Atari With Deep Reinforcement Learning},
  author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
  booktitle = {NIPS Deep Learning Workshop},
  year = {2013}
}

@article{choshen2018dora,
  author    = {Leshem Choshen and
               Lior Fox and
               Yonatan Loewenstein},
  title     = {{DORA} The Explorer: Directed Outreaching Reinforcement Action-Selection},
  journal   = {CoRR},
  volume    = {abs/1804.04012},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.04012},
  eprinttype = {arXiv},
  eprint    = {1804.04012},
  timestamp = {Mon, 13 Aug 2018 16:46:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-04012.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{haber2018learning,
 author = {Haber, Nick and Mrowca, Damian and Wang, Stephanie and Fei-Fei, Li F and Yamins, Daniel L},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning to Play With Intrinsically-Motivated, Self-Aware Agents},
 url = {https://proceedings.neurips.cc/paper/2018/file/71e63ef5b7249cfc60852f0e0f5bf4c8-Paper.pdf},
 volume = {31},
 year = {2018}
}



@InProceedings{pathak2017curiosity,
  title = 	 {Curiosity-driven Exploration by Self-supervised Prediction},
  author =       {Deepak Pathak and Pulkit Agrawal and Alexei A. Efros and Trevor Darrell},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2778--2787},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/pathak17a/pathak17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/pathak17a.html},
  abstract = 	 {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent’s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.}
}



@article{achiam2017surprise,
  author    = {Joshua Achiam and
               Shankar Sastry},
  title     = {Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1703.01732},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.01732},
  eprinttype = {arXiv},
  eprint    = {1703.01732},
  timestamp = {Mon, 13 Aug 2018 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/AchiamS17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{schmidhuber1991possibility,
  title={A possibility for implementing curiosity and boredom in model-building neural controllers},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={Proc. of the international conference on simulation of adaptive behavior: From animals to animats},
  pages={222--227},
  year={1991}
}


@inproceedings{burda2019exploration,
  author    = {Yuri Burda and
               Harrison Edwards and
               Amos J. Storkey and
               Oleg Klimov},
  title     = {Exploration by random network distillation},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=H1lJJnR5Ym},
  timestamp = {Thu, 25 Jul 2019 14:25:55 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/BurdaESK19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{bellemare2016unifying,
 author = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Unifying Count-Based Exploration and Intrinsic Motivation},
 url = {https://proceedings.neurips.cc/paper/2016/file/afda332245e2af431fb7b672a68b659d-Paper.pdf},
 volume = {29},
 year = {2016}
}


@article{osband2019deep,
  author  = {Ian Osband and Benjamin Van Roy and Daniel J. Russo and Zheng Wen},
  title   = {Deep Exploration via Randomized Value Functions},
  journal = {Journal of Machine Learning Research},
  year    = {2019},
  volume  = {20},
  number  = {124},
  pages   = {1-62},
  url     = {http://jmlr.org/papers/v20/18-339.html}
}


@inproceedings{osband2016deep,
 author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Exploration via Bootstrapped DQN},
 url = {https://proceedings.neurips.cc/paper/2016/file/8d8818c8e140c64c743113f563cf750f-Paper.pdf},
 volume = {29},
 year = {2016}
}




@inproceedings{fortunato2018noisy,
  author    = {Meire Fortunato and
               Mohammad Gheshlaghi Azar and
               Bilal Piot and
               Jacob Menick and
               Matteo Hessel and
               Ian Osband and
               Alex Graves and
               Volodymyr Mnih and
               R{\'{e}}mi Munos and
               Demis Hassabis and
               Olivier Pietquin and
               Charles Blundell and
               Shane Legg},
  title     = {Noisy Networks For Exploration},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=rywHCPkAW},
  timestamp = {Thu, 25 Jul 2019 14:25:43 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/FortunatoAPMHOG18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{russo2019worst,
 author = {Russo, Daniel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Worst-Case Regret Bounds for Exploration via Randomized Value Functions},
 url = {https://proceedings.neurips.cc/paper/2019/file/451ae86722d26a608c2e174b2b2773f1-Paper.pdf},
 volume = {32},
 year = {2019}
}



@InProceedings{osband16generalization,
  title = 	 {Generalization and Exploration via Randomized Value Functions},
  author = 	 {Osband, Ian and Roy, Benjamin Van and Wen, Zheng},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2377--2386},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/osband16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/osband16.html},
  abstract = 	 {We propose randomized least-squares value iteration (RLSVI) – a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions. We explain why versions of least-squares value iteration that use Boltzmann or epsilon-greedy exploration can be highly inefficient, and we present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish an upper bound on the expected regret of RLSVI that demonstrates near-optimality in a tabula rasa learning context. More broadly, our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning: synthesizing efficient exploration and effective generalization.}
}




@InProceedings{osband2017why,
  title = 	 {Why is Posterior Sampling Better than Optimism for Reinforcement Learning?},
  author =       {Ian Osband and Van Roy, Benjamin},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2701--2710},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/osband17a/osband17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/osband17a.html},
  abstract = 	 {Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms existing algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an $\tilde{O}(H\sqrt{SAT})$ Bayesian regret bound for PSRL in finite-horizon episodic Markov decision processes. This improves upon the best previous Bayesian regret bound of $\tilde{O}(H S \sqrt{AT})$ for any reinforcement learning algorithm. Our theoretical results are supported by extensive empirical evaluation.}
}

@InProceedings{bai21principled,
  title = 	 {Principled Exploration via Optimistic Bootstrapping and Backward Induction},
  author =       {Bai, Chenjia and Wang, Lingxiao and Han, Lei and Hao, Jianye and Garg, Animesh and Liu, Peng and Wang, Zhaoran},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {577--587},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/bai21d/bai21d.pdf},
  url = 	 {https://proceedings.mlr.press/v139/bai21d.html},
  abstract = 	 {One principled approach for provably efficient exploration is incorporating the upper confidence bound (UCB) into the value function as a bonus. However, UCB is specified to deal with linear and tabular settings and is incompatible with Deep Reinforcement Learning (DRL). In this paper, we propose a principled exploration method for DRL through Optimistic Bootstrapping and Backward Induction (OB2I). OB2I constructs a general-purpose UCB-bonus through non-parametric bootstrap in DRL. The UCB-bonus estimates the epistemic uncertainty of state-action pairs for optimistic exploration. We build theoretical connections between the proposed UCB-bonus and the LSVI-UCB in linear setting. We propagate future uncertainty in a time-consistent manner through episodic backward update, which exploits the theoretical advantage and empirically improves the sample-efficiency. Our experiments in MNIST maze and Atari suit suggest that OB2I outperforms several state-of-the-art exploration approaches.}
}



@misc{xiong2021nearoptimal,
      title={Near-Optimal Randomized Exploration for Tabular MDP}, 
      author={Zhihan Xiong and Ruoqi Shen and Qiwen Cui and Simon S. Du},
      year={2021},
      eprint={2102.09703},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{obsband2013more,
 author = {Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {(More) Efficient Reinforcement Learning via Posterior Sampling},
 url = {https://proceedings.neurips.cc/paper/2013/file/6a5889bb0190d0211a991f47bb19a777-Paper.pdf},
 volume = {26},
 year = {2013}
}

@inproceedings{agrawal2020posterior,
 author = {Agrawal, Shipra and Jia, Randy},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Optimistic posterior sampling for reinforcement learning: worst-case regret bounds},
 url = {https://proceedings.neurips.cc/paper/2017/file/3621f1454cacf995530ea53652ddf8fb-Paper.pdf},
 volume = {30},
 year = {2017}
}



@inproceedings{osband18randomized,
 author = {Osband, Ian and Aslanides, John and Cassirer, Albin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Randomized Prior Functions for Deep Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2018/file/5a7b238ba0f6502e5d6be14424b20ded-Paper.pdf},
 volume = {31},
 year = {2018}
}



@InProceedings{kveton19a,
  title = 	 {Garbage In, Reward Out: Bootstrapping Exploration in Multi-Armed Bandits},
  author =       {Kveton, Branislav and Szepesvari, Csaba and Vaswani, Sharan and Wen, Zheng and Lattimore, Tor and Ghavamzadeh, Mohammad},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3601--3610},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/kveton19a/kveton19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/kveton19a.html},
  abstract = 	 {We propose a bandit algorithm that explores by randomizing its history of rewards. Specifically, it pulls the arm with the highest mean reward in a non-parametric bootstrap sample of its history with pseudo rewards. We design the pseudo rewards such that the bootstrap mean is optimistic with a sufficiently high probability. We call our algorithm Giro, which stands for garbage in, reward out. We analyze Giro in a Bernoulli bandit and derive a $O(K \Delta^{-1} \log n)$ bound on its $n$-round regret, where $\Delta$ is the difference in the expected rewards of the optimal and the best suboptimal arms, and $K$ is the number of arms. The main advantage of our exploration design is that it easily generalizes to structured problems. To show this, we propose contextual Giro with an arbitrary reward generalization model. We evaluate Giro and its contextual variant on multiple synthetic and real-world problems, and observe that it performs well.}
}



@inproceedings{szita2008,
author = {Szita, Istv\'{a}n and L\H{o}rincz, Andr\'{a}s},
title = {The Many Faces of Optimism: A Unifying Approach},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390288},
doi = {10.1145/1390156.1390288},
abstract = {The exploration-exploitation dilemma has been an intriguing and unsolved problem within the framework of reinforcement learning. "Optimism in the face of uncertainty" and model building play central roles in advanced exploration methods. Here, we integrate several concepts and obtain a fast and simple algorithm. We show that the proposed algorithm finds a near-optimal policy in polynomial time, and give experimental evidence that it is robust and efficient compared to its ascendants.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1048–1055},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}



@InProceedings{kaufmann12,
  title = 	 {On Bayesian Upper Confidence Bounds for Bandit Problems},
  author = 	 {Kaufmann, Emilie and Cappe, Olivier and Garivier, Aurelien},
  booktitle = 	 {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {592--600},
  year = 	 {2012},
  editor = 	 {Lawrence, Neil D. and Girolami, Mark},
  volume = 	 {22},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {La Palma, Canary Islands},
  month = 	 {21--23 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v22/kaufmann12/kaufmann12.pdf},
  url = 	 {https://proceedings.mlr.press/v22/kaufmann12.html},
  abstract = 	 {Stochastic bandit problems have been analyzed from two different perspectives: a frequentist view, where the parameter is a deterministic unknown quantity, and a Bayesian approach, where the parameter is drawn from a prior distribution.  We show in this paper that methods derived from this second perspective prove optimal when evaluated using the frequentist cumulated regret as a measure of performance. We give a general formulation for a class of Bayesian index policies that rely on quantiles of the posterior distribution. For binary bandits, we prove that the corresponding algorithm, termed Bayes-UCB, satisfies finite-time regret bounds that imply its asymptotic optimality.  More generally, Bayes-UCB appears as an unifying framework for several variants of the UCB algorithm addressing different bandit problems (parametric multi-armed bandits, Gaussian bandits with unknown mean and variance, linear bandits). But the generality of the Bayesian approach makes it possible to address more challenging models. In particular, we show how to handle linear bandits with sparsity constraints by resorting to Gibbs sampling.}
}


@article{rubin1981bayesian,
  title={The bayesian bootstrap},
  author={Rubin, Donald B},
  journal={The annals of statistics},
  pages={130--134},
  year={1981},
  publisher={JSTOR}
}



@article{hjort1991bayesian,
  title={Bayesian and empirical Bayesian bootstrapping},
  author={Hjort, Nils Lid},
  journal={Preprint series. Statistical Research Report http://urn. nb. no/URN: NBN: no-23420},
  year={1991},
  publisher={Matematisk Institutt, Universitetet i Oslo}
}


@inproceedings{baudry2021optimality,
  title={From Optimality to Robustness: Dirichlet Sampling Strategies in Stochastic Bandits},
  author={Baudry, Dorian and Saux, Patrick and Maillard, Odalric-Ambrym},
  booktitle={Neurips 2021},
  year={2021}
}


@InProceedings{riou20a,
  title = 	 {Bandit Algorithms Based on Thompson Sampling for Bounded Reward Distributions},
  author =       {Riou, Charles and Honda, Junya},
  booktitle = 	 {Proceedings of the 31st International Conference  on Algorithmic Learning Theory},
  pages = 	 {777--826},
  year = 	 {2020},
  editor = 	 {Kontorovich, Aryeh and Neu, Gergely},
  volume = 	 {117},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {08 Feb--11 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v117/riou20a/riou20a.pdf},
  url = 	 {https://proceedings.mlr.press/v117/riou20a.html},
  abstract = 	 {We focus on a classic reinforcement learning problem, called a multi-armed bandit, and more specifically in the stochastic setting with reward distributions bounded in $[0,1]$. For this model, an optimal problem-dependent asymptotic regret lower bound has been derived. However, the existing algorithms achieving this regret lower bound all require to solve an optimization problem at each step, inducing a large complexity. In this paper, we propose two new algorithms, which we prove to achieve the problem-dependent asymptotic regret lower bound. The first one, which we call Multinomial TS, is an adaptation of Thompson Sampling for Bernoulli rewards to multinomial reward distributions whose support is included in $\{0, \frac{1}{M}, …, 1\}$. This algorithm achieves the regret lower bound in the case of multinomial distributions with the aforementioned support, and it can be easily generalized to bounded reward distributions in $[0, 1]$ by randomly rounding the observed rewards. The second algorithm we introduce, which we call Non-parametric TS, is a randomized algorithm but not based on the posterior sampling in the strict sense. At each step, it computes an average of the observed rewards with random weight. Not only is it asymptotically optimal, but also it performs very well even for small horizons.}
}



@inproceedings{dann2019policy,
  title={Policy certificates: Towards accountable reinforcement learning},
  author={Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={1507--1516},
  year={2019},
  organization={PMLR}
}


@inproceedings{talebi2018variance,
  title={Variance-Aware Regret Bounds for Undiscounted Reinforcement Learning in MDPs},
  author={Talebi, Mohammad Sadegh and Maillard, Odalric-Ambrym},
  booktitle={Algorithmic Learning Theory},
  pages={770--805},
  year={2018}
}


@inproceedings{sidford2018near,
author = {Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin F. and Ye, Yinyu},
booktitle = {Neural Information Processing Systems},
title = {{Near-optimal time and sample complexities for solving discounted Markov decision process with a generative model}},
url = {https://arxiv.org/pdf/1806.01492.pdf},
year = {2018}
}





@inproceedings{agarwal2020model,
author = {Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
booktitle = {Conference on Learning Theory},
title = {{Model-based reinforcement learning with a generative model is minimax optimal}},
url = {https://arxiv.org/pdf/1906.03804.pdf},
year = {2020}
}



@inproceedings{bartlett2009regal,
author = {Bartlett, Peter L. and Tewari, Ambuj},
booktitle = {Uncertainty in Artificial Intelligence},
keywords = {bandits},
mendeley-tags = {bandits},
title = {{REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs}},
url = {https://arxiv.org/pdf/1205.2661.pdf},
year = {2009}
}



@inproceedings{auer2009near,
author = {Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
booktitle = {Neural Information Processing Systems},
title = {{Near-optimal regret bounds for reinforcement learning}},
url = {https://papers.nips.cc/paper/3401-near-optimal-regret-bounds-for-reinforcement-learning.pdf},
year = {2009}
}



@article{azar2013minimax,
abstract = {We consider the problems of learning the optimal action-value function and the optimal policy in discounted-reward Markov decision processes (MDPs). We prove new PAC bounds on the sample-complexity of two well-known model-based reinforcement learning (RL) algorithms in the presence of a generative model of the MDP: value iteration and policy iteration. The first result indicates that for an MDP with N state-action pairs and the discount factor $\gamma${\^{a}}̂̂[0,1) only O(Nlog(N/$\delta$)/((1-$\gamma$)3 $\epsilon$ 2)) state-transition samples are required to find an $\epsilon$-optimal estimation of the action-value function with the probability (w.p.) 1-$\delta$. Further, we prove that, for small values of $\epsilon$, an order of O(Nlog(N/$\delta$)/((1-$\gamma$)3 $\epsilon$ 2)) samples is required to find an $\epsilon$-optimal policy w.p. 1-$\delta$. We also prove a matching lower bound of $\Theta$(Nlog(N/$\delta$)/((1-$\gamma$)3 $\epsilon$ 2)) on the sample complexity of estimating the optimal action-value function with $\epsilon$ accuracy. To the best of our knowledge, this is the first minimax result on the sample complexity of RL: the upper bounds match the lower bound in terms of N, $\epsilon$, $\delta$ and 1/(1-$\gamma$) up to a constant factor. Also, both our lower bound and upper bound improve on the state-of-the-art in terms of their dependence on 1/(1-$\gamma$). {\textcopyright} 2013 The Author(s).},
author = {Azar,  Mohammad Gheshlaghi and Munos, R{\'{e}}mi and Kappen, Hilbert J.},
file = {:Users/valkom/Library/Application Support/Mendeley Desktop/Downloaded/Gheshlaghi Azar et al. - 2013 - Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model bounds on.pdf:pdf},
journal = {Machine Learning},
number = {3},
pages = {325--349},
title = {{Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model}},
url = {https://hal.archives-ouvertes.fr/hal-00831875},
volume = {91},
year = {2013}
}


@article{zhang2020task-agnostic,
author = {Zhang, Xuezhou and Ma, Yuzhe and Singla, Adish},
journal = {arXiv preprint: arXiv:2006.09497},
title = {{Task-agnostic exploration in reinforcement learning}},
url = {https://arxiv.org/pdf/2006.09497.pdf},
year = {2020}
}


@article{wang2020on,
archivePrefix = {arXiv},
arxivId = {cs.LG/2006.11274},
author = {Wang, Ruosong and Du, Simon S and Yang, Lin F and Salakhutdinov, Ruslan},
eprint = {2006.11274},
journal = {arXiv preprint arXiv:2006.11274},
primaryClass = {cs.LG},
title = {{On reward-free reinforcement learning with linear function approximation}},
url = {https://arxiv.org/pdf/2006.11274.pdf},
year = {2020}
}



@article{garivier2019explore,
author = {Garivier, Aur{\'{e}}lien and M{\'{e}}nard, Pierre and Stoltz, Gilles},
journal = {Mathematics of Operations Research},
number = {2},
pages = {377--399},
title = {{Explore first, exploit next: The true shape of regret in bandit problems}},
url = {https://arxiv.org/pdf/1602.07182.pdf},
volume = {44},
year = {2019}
}



@book{boucheron2013concentration,
author = {Boucheron, St{\'{e}}phane and Lugosi, G{\'{a}}bor and Massart, Pascal},
publisher = {Oxford University Press},
title = {{Concentration inequalities}},
url = {https://www.hse.ru/data/2016/11/24/1113029206/Concentration inequalities.pdf},
year = {2013}
}


@article{jonsson2020planning,
abstract = {We propose MDP-GapE, a new trajectory-based Monte-Carlo Tree Search algorithm for planning in a Markov Decision Process in which transitions have a finite support. We prove an upper bound on the number of calls to the generative models needed for MDP-GapE to identify a near-optimal action with high probability. This problem-dependent sample complexity result is expressed in terms of the sub-optimality gaps of the state-action pairs that are visited during exploration. Our experiments reveal that MDP-GapE is also effective in practice, in contrast with other algorithms with sample complexity guarantees in the fixed-confidence setting, that are mostly theoretical.},
author = {Jonsson, Anders and Kaufmann, Emilie and M{\'{e}}nard, Pierre and Domingues, Omar Darwiche and Leurent, Edouard and Valko, Michal},
journal = {arXiv preprint arXiv:2006.05879},
month = {jun},
title = {{Planning in markov decision processes with gap-dependent sample complexity}},
url = {http://arxiv.org/abs/2006.05879},
year = {2020}
}





@InProceedings{domingues2020regret,
  title = 	 {Kernel-Based Reinforcement Learning: A Finite-Time Analysis},
  author =       {Domingues, Omar Darwiche and Menard, Pierre and Pirotta, Matteo and Kaufmann, Emilie and Valko, Michal},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2783--2792},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/domingues21a/domingues21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/domingues21a.html},
  abstract = 	 {We consider the exploration-exploitation dilemma in finite-horizon reinforcement learning problems whose state-action space is endowed with a metric. We introduce Kernel-UCBVI, a model-based optimistic algorithm that leverages the smoothness of the MDP and a non-parametric kernel estimator of the rewards and transitions to efficiently balance exploration and exploitation. For problems with $K$ episodes and horizon $H$, we provide a regret bound of $\widetilde{O}\left( H^3 K^{\frac{2d}{2d+1}}\right)$, where $d$ is the covering dimension of the joint state-action space. This is the first regret bound for kernel-based RL using smoothing kernels, which requires very weak assumptions on the MDP and applies to a wide range of tasks. We empirically validate our approach in continuous MDPs with sparse rewards.}
}





@article{lattimore2016end,
  title={The end of optimism? an asymptotic analysis of finite-armed linear bandits},
  author={Lattimore, Tor and Szepesvari, Csaba},
  journal={arXiv preprint arXiv:1610.04491},
  year={2016}
}


@InProceedings{kaufmann2020adaptive,
  title = 	 {Adaptive Reward-Free Exploration},
  author =       {Kaufmann, Emilie and M{\'e}nard, Pierre and Darwiche Domingues, Omar and Jonsson, Anders and Leurent, Edouard and Valko, Michal},
  booktitle = 	 {Proceedings of the 32nd International Conference on Algorithmic Learning Theory},
  pages = 	 {865--891},
  year = 	 {2021},
  editor = 	 {Feldman, Vitaly and Ligett, Katrina and Sabato, Sivan},
  volume = 	 {132},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--19 Mar},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v132/kaufmann21a/kaufmann21a.pdf},
  url = 	 {https://proceedings.mlr.press/v132/kaufmann21a.html}
}



@inproceedings{hazan2019provably,
author = {Hazan, Elad and Kakade, Sham and Singh, Karan and Soest, Abby Van},
booktitle = {International Conference on Machine Learning},
title = {{Provably efficient maximum entropy exploration}},
url = {https://arxiv.org/pdf/1812.02690.pdf},
year = {2019}
}


@inproceedings{strehl2006pac,
  title={PAC model-free reinforcement learning},
  author={Strehl, Alexander L and Li, Lihong and Wiewiora, Eric and Langford, John and Littman, Michael L},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={881--888},
  year={2006}
}

@inproceedings{ostrovski2017count,
  title={Count-based exploration with neural density models},
  author={Ostrovski, Georg and Bellemare, Marc G and van den Oord, A{\"a}ron and Munos, R{\'e}mi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2721--2730},
  year={2017},
  organization={JMLR. org}
}

@article{schrittwieser2020mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group}
}


@InProceedings{bas2020logistic,
  title = 	 { Logistic Q-Learning },
  author =       {Bas-Serrano, Joan and Curi, Sebastian and Krause, Andreas and Neu, Gergely},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3610--3618},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/bas-serrano21a/bas-serrano21a.pdf},
  url = 	 {https://proceedings.mlr.press/v130/bas-serrano21a.html},
  abstract = 	 { We propose a new reinforcement learning algorithm derived from a regularized linear-programming formulation of optimal control in MDPs. The method is closely related to the classic Relative Entropy Policy Search (REPS) algorithm of Peters et al. (2010), with the key difference that our method introduces a Q-function that enables efficient exact model-free implementation. The main feature of our algorithm (called QREPS) is a convex loss function for policy evaluation that serves as a theoretically sound alternative to the widely used squared Bellman error. We provide a practical saddle-point optimization method for minimizing this loss function and provide an error-propagation analysis that relates the quality of the individual updates to the performance of the output policy. Finally, we demonstrate the effectiveness of our method on a range of benchmark problems. }
}


@article{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado},
  journal={Advances in neural information processing systems},
  volume={23},
  pages={2613--2621},
  year={2010}
}

@inproceedings{van2016deep,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  year={2016}
}



@article{bellemare2013arcade,
  title={The arcade learning environment: An evaluation platform for general agents},
  author={Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={47},
  pages={253--279},
  year={2013}
}


@misc{gajane2019autonomous,
    title={Autonomous exploration for navigating in non-stationary CMPs},
    author={Pratik Gajane and Ronald Ortner and Peter Auer and Csaba Szepesvari},
    year={2019},
    eprint={1910.08446},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@inproceedings{lim2012autonomous,
  title={Autonomous exploration for navigating in mdps},
  author={Lim, Shiau Hong and Auer, Peter},
  booktitle={Conference on Learning Theory},
  pages={40--1},
  year={2012}
}



@article{cohen2020near,
  title={Near-optimal Regret Bounds for Stochastic Shortest Path},
  author={Cohen, Alon and Kaplan, Haim and Mansour, Yishay and Rosenberg, Aviv},
  journal={arXiv preprint arXiv:2002.09869},
  year={2020}
}

@article{tarbouriech2019no,
  title={No-Regret Exploration in Goal-Oriented Reinforcement Learning},
  author={Tarbouriech, Jean and Garcelon, Evrard and Valko, Michal and Pirotta, Matteo and Lazaric, Alessandro},
  journal={arXiv preprint arXiv:1912.03517},
  year={2019}
}



@inproceedings{hren2008optimistic,
	title = {{Optimistic planning of deterministic systems}},
	year = {2008},
	booktitle = {European Workshop on Reinforcement Learning},
	author = {Hren, Jean-Francois and Munos, Rémi}
}

@inproceedings{coquelin2007bandit,
	title = {{Bandit algorithms for tree search}},
	year = {2007},
	booktitle = {Uncertainty in Artificial Intelligence},
	author = {Coquelin, Pierre-Arnaud and Munos, Rémi},
	url = {https://arxiv.org/pdf/1408.2028.pdf}
}

@inproceedings{Zanette19Euler,
  author    = {Andrea Zanette and
               Emma Brunskill},
  title     = {Tighter Problem-Dependent Regret Bounds in Reinforcement Learning
               without Domain Knowledge using Value Function Bounds},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               (ICML)},
  year      = {2019}
}

@inproceedings{hamrick2019combining,
title={Combining Q-Learning and Search with Amortized Value Estimates},
author={Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Tobias Pfaff and Theophane Weber and Lars Buesing and Peter W. Battaglia},
booktitle={International Conference on Learning Representations},
year={2020}
}

@article{kearns02E3,
  author    = {Michael J. Kearns and
               Satinder P. Singh},
  title     = {Near-Optimal Reinforcement Learning in Polynomial Time},
  journal   = {Machine Learning},
  volume    = {49},
  number    = {2-3},
  pages     = {209--232},
  year      = {2002}
}


@book{cover2006elements,
author = {Cover, Thomas M. and Thomas, Joy A.},
publisher = {John Wiley {\&} Sons},
title = {{Elements of information theory}},
url = {https://www.amazon.com/Elements-Information-Theory-Telecommunications-Processing/dp/0471241954},
year = {2006}
}


@article{de2004self,
author = {de la Pe{\~{n}}a, Victor H. and Klass, Michael J. and Lai, Tze Leung},
journal = {Annals of probability},
pages = {1902--1933},
title = {{Self-normalized processes: {E}xponential inequalities, moment bounds and iterated logarithm laws}},
url = {https://arxiv.org/pdf/math/0410102.pdf},
volume = {32},
year = {2004}
}




@inproceedings{garivier2011kl,
  title={The KL-UCB algorithm for bounded stochastic bandits and beyond},
  author={Garivier, Aur{\'e}lien and Capp{\'e}, Olivier},
  booktitle={Proceedings of the 24th annual conference on learning theory},
  pages={359--376},
  year={2011}
}

@Article{KLUCBJournal,
  Title                    = {{{K}ullback-{L}eibler upper confidence bounds for optimal sequential allocation}},
  Author                   = {Capp{\'e}, O. and Garivier, A. and Maillard, O-A. and Munos, R. and Stoltz, G.},
  Journal                  = {Annals of Statistics},
  Year                     = {2013},
  Pages                    = {1516--1541},
  Volume                   = {41(3)}
}

@inproceedings{dann2017unifying,
author = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
booktitle = {Neural Information Processing Systems},
 year={2017},
title = {{Unifying {PAC} and regret: Uniform {PAC} bounds for episodic reinforcement learning}},
url = {https://arxiv.org/pdf/1703.07710.pdf}
}


@inproceedings{jin2020reward-free,
abstract = {Exploration is widely regarded as one of the most challenging aspects of reinforcement learning (RL), with many naive approaches succumbing to exponential sample complexity. To isolate the challenges of exploration, we propose a new "reward-free RL" framework. In the exploration phase, the agent first collects trajectories from an MDP {\$}\backslashmathcal{\{}M{\}}{\$} without a pre-specified reward function. After exploration, it is tasked with computing near-optimal policies under for {\$}\backslashmathcal{\{}M{\}}{\$} for a collection of given reward functions. This framework is particularly suitable when there are many reward functions of interest, or when the reward function is shaped by an external agent to elicit desired behavior. We give an efficient algorithm that conducts {\$}\backslashtilde{\{}\backslashmathcal{\{}O{\}}{\}}(S{\^{}}2A\backslashmathrm{\{}poly{\}}(H)/\backslashepsilon{\^{}}2){\$} episodes of exploration and returns {\$}\backslashepsilon{\$}-suboptimal policies for an arbitrary number of reward functions. We achieve this by finding exploratory policies that visit each "significant" state with probability proportional to its maximum visitation probability under any possible policy. Moreover, our planning procedure can be instantiated by any black-box approximate planner, such as value iteration or natural policy gradient. We also give a nearly-matching {\$}\backslashOmega(S{\^{}}2AH{\^{}}2/\backslashepsilon{\^{}}2){\$} lower bound, demonstrating the near-optimality of our algorithm in this setting.},
archivePrefix = {arXiv},
arxivId = {2002.02794},
author = {Jin, Chi and Krishnamurthy, Akshay and Simchowitz, Max and Yu, Tiancheng},
booktitle = {International Conference on Machine Learning},
eprint = {2002.02794},
file = {:Users/valkom/Library/Application Support/Mendeley Desktop/Downloaded/Jin et al. - 2020 - Reward-Free Exploration for Reinforcement Learning.pdf:pdf},
title = {{Reward-free exploration for reinforcement learning}},
url = {http://arxiv.org/abs/2002.02794},
year = {2020}
}


@inproceedings{jin2018is,
author = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, S{\'{e}}bastien and Jordan, Michael I.},
booktitle = {Neural Information Processing Systems},
file = {:Users/valkom/Library/Application Support/Mendeley Desktop/Downloaded/Jin et al. - 2018 - Is Q-learning Provably Efficient.pdf:pdf},
title = {{Is Q-learning provably efficient?}},
url = {https://arxiv.org/pdf/1807.03765.pdf},
year = {2018}
}




@article{garivier2018kl,
  title={KL-UCB-switch: optimal regret bounds for stochastic bandits from both a distribution-dependent and a distribution-free viewpoints},
  author={Garivier, Aur{\'e}lien and Hadiji, H{\'e}di and Menard, Pierre and Stoltz, Gilles},
  journal={arXiv preprint arXiv:1805.05071},
  year={2018}
}

@book{puterman1994markov,
address = {New York, NY},
author = {Puterman, Martin L.},
howpublished = {Hardcover},
isbn = {0471619779},
publisher = {John Wiley {\&} Sons},
title = {{Markov decision processes: Discrete stochastic dynamic programming}},
url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887},
year = {1994}
}



@Book{SurveyRemiMCTS,
  Title                    = {From bandits to Monte-Carlo Tree Search: The optimistic principle applied to optimization and planning.},
  Author                   = {Munos, R.},
  Publisher                = {Foundations and Trends in Machine Learning},
  Year                     = {2014},
  Number                   = {1},
  Volume                   = {7}
}

@inproceedings{SmoothCruiser19,
  author    = {Jean{-}Bastien Grill and
               Omar Darwiche Domingues and
               Pierre M{\'{e}}nard and
               R{\'{e}}mi Munos and
               Michal Valko},
  title     = {Planning in entropy-regularized Markov decision processes and games},
  booktitle = {Neural Information Processing Systems},
  year      = {2019}
}

@inproceedings{Huang17StructuredBAI,
  author    = {Ruitong Huang and
               Mohammad M. Ajallooeian and
               Csaba Szepesv{\'{a}}ri and
               Martin M{\"{u}}ller},
  title     = {Structured Best Arm Identification with Fixed Confidence},
  booktitle = {International Conference on Algorithmic Learning Theory (ALT)},
  year      = {2017}
}


@InProceedings{Kocsis06UCT,
  Title                    = {Bandit Based Monte-carlo Planning},
  Author                   = {Kocsis, Levente and Szepesv\'{a}ri, Csaba},
  Booktitle                = {Proceedings of the 17th European Conference on Machine Learning (ECML)},
  Year                     = {2006}
}


@article{even2006action,
author = {Even-Dar, Eyal and Mannor, Shie and Mansour, Yishay},
journal = {Journal of Machine Learning Research},
pages = {1079--1105},
title = {{Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems}},
url = {https://jmlr.csail.mit.edu/papers/volume7/evendar06a/evendar06a.pdf},
volume = {7},
year = {2006}
}


@book{BanditBook,
author = {Lattimore, Tor and Szepesvari, Csaba},
publisher = {Cambridge University Press},
title = {{Ba
@inproceedinndit Algorithms}},
year = {2019}
}

@Article{Aueral02,
  Title                    = {{Finite-time analysis of the multiarmed bandit problem}},
  Author                   = {Auer, P. and Cesa-Bianchi, N. and Fischer, P.},
  Journal                  = {Machine Learning},
  Year                     = {2002},
  Number                   = {2},
  Pages                    = {235--256},
  Volume                   = {47},
  Publisher                = {Springer}
}

@article{jaksch2010near,
author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
journal = {Journal of Machine Learning Research},
keywords = {bandits},
mendeley-tags = {bandits},
pages = {1563--1600},
title = {{Near-optimal regret bounds for reinforcement learning}},
url = {http://www.jmlr.org/papers/volume11/jaksch10a/jaksch10a.pdf},
volume = {99},
year = {2010}
}

@inproceedings{Tolpin12SRMCTS,
  author    = {David Tolpin and
               Solomon Eyal Shimony},
  title     = {{MCTS} Based on Simple Regret},
  booktitle = {Proceedings of the Twenty-Sixth {AAAI} Conference on Artificial Intelligence,
               July 22-26, 2012, Toronto, Ontario, Canada.},
  year      = {2012}
}

@inproceedings{Pepels14SimpleMCTS,
  author    = {Tom Pepels and
               Tristan Cazenave and
               Mark H. M. Winands and
               Marc Lanctot},
  title     = {Minimizing Simple and Cumulative Regret in Monte-Carlo Tree Search},
  booktitle = {Third Workshop on Computer Games (CGW)},
  pages     = {1--15},
  year      = {2014}
}

@article{Feldman14BRUE,
  author    = {Zohar Feldman and
               Carmel Domshlak},
  title     = {Simple Regret Optimization in Online Planning for Markov Decision
               Processes},
  journal   = {Journal of Artifial Intelligence Research},
  volume    = {51},
  pages     = {165--205},
  year      = {2014}
}



@article{Kearns02SS,
  author    = {Michael J. Kearns and
               Yishay Mansour and
               Andrew Y. Ng},
  title     = {A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov
               Decision Processes},
  journal   = {Machine Learning},
  volume    = {49},
  number    = {2-3},
  pages     = {193--208},
  year      = {2002}
}

@InProceedings{STOP14,
  Title                    = {Optimistic Planning in Markov Decision Processes using a generative model},
  Author                   = {Szorenyi, B. and Kedenburg, G. and Munos, R.},
  Booktitle                = {Advances in Neural Information Processing Systems (NIPS)},
  Year                     = {2014}
}

@Book{SuttonBarto98,
  Title                    = {Reinforcement Learning: an Introduction},
  Author                   = {Sutton, R. and Barto, A.},
  Publisher                = {MIT press},
  Year                     = {1998},

  Owner                    = {emilie},
  Timestamp                = {2016.11.07}
}


@article{AlphaZero,
  author    = {David Silver and
               Thomas Hubert and
               Julian Schrittwieser and
               Ioannis Antonoglou and
               Matthew Lai and
               Arthur Guez and
               Marc Lanctot and
               Laurent Sifre and
               Dharshan Kumaran and
               Thore Graepel and
               Timothy P. Lillicrap and
               Karen Simonyan and
               Demis Hassabis},
  title     = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  journal   = {Science},
  volume    = {362},
  issue = {6419},
  page = {1140-1144},
  year      = {2018},
}

@inproceedings{dann2015sample,
author = {Dann, Christoph and Brunskill, Emma},
booktitle = {Neural Information Processing Systems},
title = {{Sample complexity of episodic fixed-horizon reinforcement learning}},
url = {https://arxiv.org/pdf/1510.08906.pdf},
year = {2015}
}

@article{MuZero,
  author    = {Julian Schrittwieser and
               Ioannis Antonoglou and
               Thomas Hubert and
               Karen Simonyan and
               Laurent Sifre and
               Simon Schmitt and
               Arthur Guez and
               Edward Lockhart and
               Demis Hassabis and
               Thore Graepel and
               Timothy P. Lillicrap and
               David Silver},
  title     = {Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  journal   = {arXiv:1911.08265},
  year      = {2019}
}


@Article{SurveyMCTS12,
  Title                    = {A Survey of Monte Carlo Tree Search Methods},
  Author                   = {Browne, C. and Powley, E. and Whitehouse, D. and Lucas, S. and Cowling, P. and Rohlfshagen, P. and Tavener, S. and Perez, D. and Samothrakis, S. and Colton, S.},
  Journal                  = {IEEE Transactions on Computational Intelligence and AI in games,},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {1-49},
  Volume                   = {4}
}


@InProceedings{TrailBlazer16,
  Title                    = {Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning},
  Author                   = {Grill, J.-B. and Valko, M. and Munos, R.},
  Booktitle                = {Neural Information Processing Systems (NIPS)},
  Year                     = {2016}
}



@inproceedings{gabillon2012best,
  title={Best arm identification: A unified approach to fixed budget and fixed confidence},
  author={Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3212--3220},
  year={2012}
}



@inproceedings{azar2017minimax,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.05449v2},
author = {Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'{e}}mi},
booktitle = {International Conference on Machine Learning},
eprint = {arXiv:1703.05449v2},
file = {:Users/valkom/Library/Application Support/Mendeley Desktop/Downloaded/Azar, Osband, Munos - 2017 - Minimax regret bounds for reinforcement learning.pdf:pdf},
title = {{Minimax regret bounds for reinforcement learning}},
url = {https://arxiv.org/pdf/1703.05449.pdf},
year = {2017}
}



@inproceedings{azar2012on,
author = {Azar, Mohammad Gheshlaghi and Munos, R{\'{e}}mi and Kappen, Bert},
booktitle = {International Conference on Machine Learning},
title = {{On the sample complexity of reinforcement learning with a generative model}},
url = {https://arxiv.org/pdf/1206.6461.pdf},
year = {2012}
}


@inproceedings{kearns1998finite-sample,
author = {Kearns, Michael J. and Singh, Satinder P.},
booktitle = {Neural Information Processing Systems},
title = {{Finite-sample convergence rates for Q-learning and indirect algorithms}},
url = {http://papers.neurips.cc/paper/1531-finite-sample-convergence-rates-for-q-learning-and-indirect-algorithms.pdf},
year = {1998}
}



@inproceedings{fiechter1994efficient,
author = {Fiechter, Claude-Nicolas},
booktitle = {Conference on Learning Theory},
title = {{Efficient reinforcement learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=7F5F8FCD1AA7ED07356410DDD5B384FE?doi=10.1.1.49.8652\&rep=rep1\&type=pdf},
year = {1994}
}

@inproceedings{Fiechter97,
  author    = {Claude{-}Nicolas Fiechter},
  title     = {Expected Mistake Bound Model for On-Line Reinforcement Learning},
  booktitle = {Proceedings of the Fourteenth International Conference on Machine Learning (ICML)},
  year      = {1997}
}

@phdthesis{kakade2013on,
author = {Kakade, Sham},
school = {University College London},
title = {{On the sample complexity of reinforcement learning}},
url = {https://homes.cs.washington.edu/~sham/papers/thesis/sham_thesis.pdf},
year = {2003}
}



@InProceedings{filippi2010optimism,
  Title                    = {{Optimism in Reinforcement Learning and {K}ullback-{L}eibler Divergence}},
  Author                   = {Filippi, S. and Capp{\'e}, O. and Garivier, A.},
  Booktitle                = {{Allerton Conference on Communication, Control, and Computing}},
  Year                     = {2010},
}

@article{Brafman02RMAX,
  author    = {Ronen I. Brafman and
               Moshe Tennenholtz},
  title     = {{R-MAX} - {A} General Polynomial Time Algorithm for Near-Optimal Reinforcement
               Learning},
  journal   = {Journal of Machine Learning Research},
  volume    = {3},
  pages     = {213--231},
  year      = {2002}
}

@inproceedings{Strehl06DelayedQL,
  author    = {Alexander L. Strehl and
               Lihong Li and
               Eric Wiewiora and
               John Langford and
               Michael L. Littman},
  title     = {{PAC} model-free reinforcement learning},
  booktitle = {Proceedings of the Twenty-Third International Conference on Machine Learning (ICML},
  year      = {2006}
}

@article{Strehl08MBIE,
  author    = {Alexander L. Strehl and
               Michael L. Littman},
  title     = {An analysis of model-based Interval Estimation for Markov Decision
               Processes},
  journal   = {Journal of Computer and System Sciences},
  volume    = {74},
  number    = {8},
  pages     = {1309--1331},
  year      = {2008}
}

@inproceedings{bubeck2010open,
  title={Open Loop Optimistic Planning},
  author={Bubeck, S and Munos, R},
  booktitle={Conference on Learning Theory},
  year={2010}
}


@inproceedings{leurent2019practical,
	title={Practical Open-Loop Optimistic Planning},
	author={Edouard Leurent and Odalric-Ambrym Maillard},
	year={2019},
	booktitle={Proceedings of the 19th European Conference on Machine Learning and Principles and Practice (ECML-PKDD)}
}

@inproceedings{busoniu2012optimistic,
  title={Optimistic planning for Markov decision processes},
  author={Busoniu, Lucian and Munos, R{\'e}mi},
  booktitle={Artificial Intelligence and Statistics},
  pages={182--189},
  year={2012}
}

@incollection{NIPS2015_5668,
title = {Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning},
author = {Mohamed, Shakir and Jimenez Rezende, Danilo},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2125--2133},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5668-variational-information-maximisation-for-intrinsically-motivated-reinforcement-learning.pdf}
}

@misc{montufar2016information,
    title={Information Theoretically Aided Reinforcement Learning for Embodied Agents},
    author={Guido Montufar and Keyan Ghazi-Zahedi and Nihat Ay},
    year={2016},
    eprint={1605.09735},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@article {PMID:22791268,
	Title = {An information-theoretic approach to curiosity-driven reinforcement learning},
	Author = {Still, Susanne and Precup, Doina},
	DOI = {10.1007/s12064-011-0142-z},
	Number = {3},
	Volume = {131},
	Month = {September},
	Year = {2012},
	Journal = {Theory in biosciences = Theorie in den Biowissenschaften},
	ISSN = {1431-7613},
	Pages = {139—148},
	URL = {https://doi.org/10.1007/s12064-011-0142-z},
}

@incollection{NIPS2004_2552,
title = {Intrinsically Motivated Reinforcement Learning},
author = {Nuttapong Chentanez and Andrew G. Barto and Satinder P. Singh},
booktitle = {Advances in Neural Information Processing Systems 17},
editor = {L. K. Saul and Y. Weiss and L. Bottou},
pages = {1281--1288},
year = {2005},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2552-intrinsically-motivated-reinforcement-learning.pdf}
}

@inproceedings{zanette2019tighter,
author = {Zanette, Andrea and Brunskill, Emma},
booktitle = {International Conference on Machine Learning},
title = {{Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds}},
url = {https://arxiv.org/pdf/1901.00210.pdf},
year = {2019}
}

@book{Boucheron2013,
	TITLE = {{Concentration inequalities : a non asymptotic theory of independence}},
	AUTHOR = {Boucheron, St{\'e}phane and Lugosi, Gabor and Massart, Pascal},
	URL = {https://hal.inria.fr/hal-00942704},
	PUBLISHER = {{Oxford University Press}},
	PAGES = {481},
	YEAR = {2013},
	HAL_ID = {hal-00942704},
	HAL_VERSION = {v1},
}

@book{durrett2010probability,
author = {Durrett, Rick},
edition = {4},
isbn = {978-0-521-76539-8},
publisher = {Cambridge University Press},
series = {Cambridge Series in Statistical and Probabilistic Mathematics},
title = {{Probability: Theory and Examples}},
url = {https://services.math.duke.edu/{~}rtd/PTE/PTE5{\_}011119.pdf},
year = {2010}
}



@article{lasserre2020simple,
author = {Lasserre, Jean-Bernard},
year = {2020},
month = {08},
pages = {},
title = {Simple formula for integration of polynomials on a simplex},
volume = {61},
journal = {BIT Numerical Mathematics},
doi = {10.1007/s10543-020-00828-x}
}

@phdthesis{dirksen2015sections, title={Sections of simplices and cylinders: Volume formulas and estimates}, url={https://macau.uni-kiel.de/receive/diss_mods_00018308}, abstractNote={We investigate sections of simplices and generalized cylinders. We are interested in the volume of sections of these bodies with affine subspaces and give formulas and estimates for these volumes. For the regular n-simplex we state a general formula to compute the volume of the intersection with some k-dimensional subspace. A formula for central hyperplane sections was given by S. Webb. He also showed that the hyperplane through the centroid containing n-1 vertices gives the maximal volume. We generalize the formula to arbitrary dimensional sections that do not necessarily have to contain the centroid. And we show that, for a prescribed small distance of a hyperplane to the centroid, still the hyperplane containing n-1 vertices is volume maximizing. The proof also yields a new and short argument for Webb’s result. The minimal hyperplane section is conjectured to be the one parallel to a face. We show that this hyperplane section is indeed minimal for dimensions n=2,3,4 and that it is a local minimum in general. Using results by Brehm e.a. we compute the average hyperplane section volume. For k-dimensional sections we give an upper bound. Finally we modify our volume formula to compute the section volume of irregular simplices. As an application we show that in odd dimensions larger than 4 there exist irregular simplices whose maximal section is not a face. A generalized cylinder is the Cartesian product of a n-dimensional cube and a m-dimensional ball of radius r. We study the behavior of the hyperplane section volume depending on the radius of the cylinder. First we show for the 3-dimensional cylinder that always a truncated ellipse gives the maximal volume. This is done by elementary geometric considerations and calculus. For the generalized cylinder we use the Fourier transform to derive an explicit formula. Then we estimate this by Hölder’s inequality. Finally it remains to prove an integral inequality that is similar to the inequality of K. Ball for the cube.}, author={Dirksen, Hauke Carl-Erwin}, year={2015} }



@book {olver1997asymptotics,
    AUTHOR = {Olver, Frank W. J.},
     TITLE = {Asymptotics and special functions},
    SERIES = {AKP Classics},
      NOTE = {Reprint of the 1974 original [Academic Press, New York;
              MR0435697 (55 \#8655)]},
 PUBLISHER = {A K Peters, Ltd., Wellesley, MA},
      YEAR = {1997},
     PAGES = {xviii+572},
      ISBN = {1-56881-069-5},
   MRCLASS = {41-02 (33Cxx 41A60 65D20)},
  MRNUMBER = {1429619},
}


@inproceedings{honda2010asymptotically,
  added-at = {2013-02-19T00:00:00.000+0100},
  author = {Honda, Junya and Takemura, Akimichi},
  biburl = {https://www.bibsonomy.org/bibtex/2b5226653f70c05722e4f8529cd73b3c3/dblp},
  booktitle = {COLT},
  editor = {Kalai, Adam Tauman and Mohri, Mehryar},
  ee = {http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#page=75},
  interhash = {1b5a7fad600cbc3d260b38f5dacc6a63},
  intrahash = {b5226653f70c05722e4f8529cd73b3c3},
  isbn = {978-0-9822529-2-5},
  keywords = {dblp},
  pages = {67-79},
  publisher = {Omnipress},
  timestamp = {2013-02-20T11:39:32.000+0100},
  title = {An Asymptotically Optimal Bandit Algorithm for Bounded Support Models.},
  url = {http://dblp.uni-trier.de/db/conf/colt/colt2010.html#HondaT10},
  year = 2010
}


@article{borwein2007uniform,
author = {Borwein, Jonathan (Jon) and Chan, O-Yeat},
year = {2007},
month = {01},
pages = {},
title = {Uniform bounds for the complementary incomplete Gamma function},
volume = {12},
journal = {Mathematical Inequalities and Applications},
doi = {10.7153/mia-12-10}
}

@article{qi2010bounds,
author = {Qi, Feng},
year = {2010},
month = {03},
pages = {Article ID 493058, 84 pages},
title = {Bounds for the Ratio of Two Gamma Functions},
volume = {2010},
journal = {Journal of Inequalities and Applications},
doi = {10.1155/2010/493058}
}

@book {fedoryuk1977metod,
    AUTHOR = {Fedoryuk, M. V.},
     TITLE = {Metod perevala},
 PUBLISHER = {Izdat. ``Nauka'', Moscow},
      YEAR = {1977},
     PAGES = {368},
  MRCLASS = {30A84 (41A60)},
  MRNUMBER = {0507923},
MRREVIEWER = {E. Riekstins},
}

@book{evans2018measure,
  title={Measure theory and fine properties of functions},
  author={Evans, Lawrence C and Garzepy, Ronald F},
  year={2018},
  publisher={Routledge}
}

@ARTICLE{ferrante2021binomialtails,  author={Ferrante, Guido Carlo},  journal={IEEE Transactions on Information Theory},   title={Bounds on Binomial Tails With Applications},   year={2021},  volume={67},  number={12},  pages={8273-8279},  doi={10.1109/TIT.2021.3115044}}

@article{cerone2007special,
 ISSN = {14528630, 2406100X},
 URL = {http://www.jstor.org/stable/43666039},
 abstract = {The Steffensen inequality and bounds for the Čebyšev functional are utilised to obtain bounds for some classical special functions. The technique relies on determining bounds on integrals of products of functions. The above techniques are used to obtain novel and useful bounds for the Bessel function of the first kind, the Beta function, and the Zeta function.},
 author = {P. Cerone},
 journal = {Applicable Analysis and Discrete Mathematics},
 number = {1},
 pages = {72--91},
 publisher = {University of Belgrade, Serbia},
 title = {SPECIAL FUNCTIONS: APPROXIMATIONS AND BOUNDS},
 volume = {1},
 year = {2007}
}


@article{alfers1984normal,
  title={A normal approximation for beta and gamma tail probabilities},
  author={Duncan Alfers and Hermann Dinges},
  journal={Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und Verwandte Gebiete},
  year={1984},
  volume={65},
  pages={399-420},
  url={https://link.springer.com/content/pdf/10.1007/BF00533744.pdf}
}

@book{reed1975ii,
  title={II: Fourier Analysis, Self-Adjointness},
  author={Reed, M. and Simon, B.},
  isbn={9780125850025},
  lccn={75182650},
  series={Methods of Modern Mathematical Physics},
  url={https://books.google.ru/books?id=Kz7s7bgVe8gC},
  year={1975},
  publisher={Elsevier Science}
}


@inproceedings{zhang2020taskagnostic,
 author = {Zhang, Xuezhou and Ma, Yuzhe and Singla, Adish},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {11734--11743},
 publisher = {Curran Associates, Inc.},
 title = {Task-agnostic Exploration in Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/8763d72bba4a7ade23f9ae1f09f4efc7-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{zahavy2021reward,
title={Reward is enough for convex {MDP}s},
author={Tom Zahavy and Brendan O'Donoghue and Guillaume Desjardins and Satinder Singh},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=ELndVeVA-TR}
}


@inproceedings{abernethy2017frankwolfe,
 author = {Abernethy, Jacob D and Wang, Jun-Kun},
 booktitle = {Neural Information Processing Systems},
 title = {On {Frank-Wolfe} and Equilibrium Computation},
 url = {https://proceedings.neurips.cc/paper/2017/file/7371364b3d72ac9a3ed8638e6f0be2c9-Paper.pdf},
 year = {2017}
}


@book{hazan2022introduction,
  title={Introduction to Online Convex Optimization, second edition},
  author={Hazan, E.},
  isbn={9780262370127},
  series={Adaptive Computation and Machine Learning series},
  url={https://books.google.ru/books?id=yqtaEAAAQBAJ},
  year={2022},
  publisher={MIT Press}
}


@InProceedings{zhang2021rewardfree,
  title = 	 {Near Optimal Reward-Free Reinforcement Learning},
  author =       {Zhang, Zihan and Du, Simon and Ji, Xiangyang},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12402--12412},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zhang21e/zhang21e.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zhang21e.html},
  abstract = 	 {We study the reward-free reinforcement learning framework, which is particularly suitable for batch reinforcement learning and scenarios where one needs policies for multiple reward functions. This framework has two phases: in the exploration phase, the agent collects trajectories by interacting with the environment without using any reward signal; in the planning phase, the agent needs to return a near-optimal policy for arbitrary reward functions. %This framework is suitable for batch RL setting and the setting where there are multiple reward functions of interes We give a new efficient algorithm, \textbf{S}taged \textbf{S}ampling + \textbf{T}runcated \textbf{P}lanning (\algoname), which interacts with the environment at most $O\left( \frac{S^2A}{\epsilon^2}\poly\log\left(\frac{SAH}{\epsilon}\right) \right)$ episodes in the exploration phase, and guarantees to output a near-optimal policy for arbitrary reward functions in the planning phase, where $S$ is the size of state space, $A$ is the size of action space, $H$ is the planning horizon, and $\epsilon$ is the target accuracy relative to the total reward. Notably, our sample complexity scales only \emph{logarithmically} with $H$, in contrast to all existing results which scale \emph{polynomially} with $H$. Furthermore, this bound matches the minimax lower bound $\Omega\left(\frac{S^2A}{\epsilon^2}\right)$ up to logarithmic factors. Our results rely on three new techniques : 1) A new sufficient condition for the dataset to plan for an $\epsilon$-suboptimal policy % for any totally bounded reward function ; 2) A new way to plan efficiently under the proposed condition using soft-truncated planning; 3) Constructing extended MDP to maximize the truncated accumulative rewards efficiently.}
}


@InProceedings{rosenberg2019online,
  title = 	 {Online Convex Optimization in Adversarial {M}arkov Decision Processes},
  author =       {Rosenberg, Aviv and Mansour, Yishay},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5478--5486},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/rosenberg19a/rosenberg19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/rosenberg19a.html},
  abstract = 	 {We consider online learning in episodic loop-free Markov decision processes (MDPs), where the loss function can change arbitrarily between episodes, and the transition function is not known to the learner. We show $\tilde{O}(L|X|\sqrt{|A|T})$ regret bound, where $T$ is the number of episodes, $X$ is the state space, $A$ is the action space, and $L$ is the length of each episode. Our online algorithm is implemented using entropic regularization methodology, which allows to extend the original adversarial MDP model to handle convex performance criteria (different ways to aggregate the losses of a single episode) , as well as improve previous regret bounds.}
}



@InProceedings{he2022nearoptimal,
  title = 	 { Near-optimal Policy Optimization Algorithms for Learning Adversarial Linear Mixture MDPs },
  author =       {He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {4259--4280},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v151/he22a/he22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/he22a.html},
  abstract = 	 { Learning Markov decision processes (MDPs) in the presence of the adversary is a challenging problem in reinforcement learning (RL). In this paper, we study RL in episodic MDPs with adversarial reward and full information feedback, where the unknown transition probability function is a linear function of a given feature mapping, and the reward function can change arbitrarily episode by episode. We propose an optimistic policy optimization algorithm POWERS and show that it can achieve $\tilde{O}(dH\sqrt{T})$ regret, where $H$ is the length of the episode, $T$ is the number of interaction with the MDP, and $d$ is the dimension of the feature mapping. Furthermore, we also prove a matching lower bound of $\tilde{\Omega}(dH\sqrt{T})$ up to logarithmic factors. Our key technical contributions are two-fold: (1) a new value function estimator based on importance weighting; and (2) a tighter confidence set for the transition kernel. They together lead to the nearly minimax optimal regret. }
}

@article{vanhandel2016probability,
  added-at = {2019-06-09T20:31:32.000+0200},
  author = {van Handel, Ramon},
  biburl = {https://www.bibsonomy.org/bibtex/2a29d65d7d2e770511b492d5b617be0c0/kirk86},
  description = {APC550.pdf},
  interhash = {28060bf77784608166771ffb67c37d49},
  intrahash = {a29d65d7d2e770511b492d5b617be0c0},
  keywords = {book probability stats theory},
  timestamp = {2019-06-09T20:31:44.000+0200},
  title = {Probability in High Dimensions},
  url = {https://web.math.princeton.edu/~rvan/APC550.pdf},
  year = 2016
}



@InProceedings{jin2021efficiently,
  title = 	 {Efficiently Solving {MDP}s with Stochastic Mirror Descent},
  author =       {Jin, Yujia and Sidford, Aaron},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4890--4900},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/jin20f/jin20f.pdf},
  url = 	 {https://proceedings.mlr.press/v119/jin20f.html},
  abstract = 	 {We present a unified framework based on primal-dual stochastic mirror descent for approximately solving infinite-horizon Markov decision processes (MDPs) given a generative model. When applied to an average-reward MDP with $A_{tot}$ total actions and mixing time bound $t_{mix}$ our method computes an $\epsilon$-optimal policy with an expected $\widetilde{O}(t_{mix}^2 A_{tot} \epsilon^{-2})$ samples from the state-transition matrix, removing the ergodicity dependence of prior art. When applied to a $\gamma$-discounted MDP with $A_{tot}$ total actions our method computes an $\epsilon$-optimal policy with an expected $\widetilde{O}((1-\gamma)^{-4} A_{tot} \epsilon^{-2})$ samples, improving over the best-known primal-dual methods while matching the state-of-the-art up to a $(1-\gamma)^{-1}$ factor. Both methods are model-free, update state values and policies simultaneously, and run in time linear in the number of samples taken. We achieve these results through a more general stochastic mirror descent framework for solving bilinear saddle-point problems with simplex and box domains and we demonstrate the flexibility of this framework by providing further applications to constrained MDPs.}
}



@InProceedings{menard2021fast,
  title = 	 {Fast active learning for pure exploration in reinforcement learning},
  author =       {M\'enard, Pierre and Domingues, Omar Darwiche and Jonsson, Anders and Kaufmann, Emilie and Leurent, Edouard and Valko, Michal},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {7599--7608},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/menard21a/menard21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/menard21a.html},
  abstract = 	 {Realistic environments often provide agents with very limited feedback. When the environment is initially unknown, the feedback, in the beginning, can be completely absent, and the agents may first choose to devote all their effort on \emph{exploring efficiently.} The exploration remains a challenge while it has been addressed with many hand-tuned heuristics with different levels of generality on one side, and a few theoretically-backed exploration strategies on the other. Many of them are incarnated by \emph{intrinsic motivation} and in particular \emph{explorations bonuses}. A common choice is to use $1/\sqrt{n}$ bonus, where $n$ is a number of times this particular state-action pair was visited. We show that, surprisingly, for a pure-exploration objective of \emph{reward-free exploration}, bonuses that scale with $1/n$ bring faster learning rates, improving the known upper bounds with respect to the dependence on the horizon $H$. Furthermore, we show that with an improved analysis of the stopping time, we can improve by a factor $H$ the sample complexity in the \emph{best-policy identification} setting, which is another pure-exploration objective, where the environment provides rewards but the agent is not penalized for its behavior during the exploration phase.}
}

@misc{suggala2019online,
  doi = {10.48550/ARXIV.1903.08110},
  
  url = {https://arxiv.org/abs/1903.08110},
  
  author = {Suggala, Arun Sai and Netrapalli, Praneeth},
  
  keywords = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Online Non-Convex Learning: Following the Perturbed Leader is Optimal},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{jaggi2013revisiting,
author = {Jaggi, Martin},
year = {2013},
month = {01},
pages = {},
title = {Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization},
volume = {28},
journal = {JMLR: Workshop and Conference Proceedings}
}

@Article{beck2017linearly,
author={Beck, Amir
and Shtern, Shimrit},
title={Linearly convergent away-step conditional gradient for non-strongly convex functions},
journal={Mathematical Programming},
year={2017},
month={Jul},
day={01},
volume={164},
number={1},
pages={1-27},
abstract={We consider the problem of minimizing the sum of a linear function and a composition of a strongly convex function with a linear transformation over a compact polyhedral set. Jaggi and Lacoste-Julien (An affine invariant linear convergence analysis for Frank-Wolfe algorithms. NIPS 2013 Workshop on Greedy Algorithms, Frank-Wolfe and Friends, 2014) show that the conditional gradient method with away steps --- employed on the aforementioned problem without the additional linear term --- has a linear rate of convergence, depending on the so-called pyramidal width of the feasible set. We revisit this result and provide a variant of the algorithm and an analysis based on simple linear programming duality arguments, as well as corresponding error bounds. This new analysis (a) enables the incorporation of the additional linear term, and (b) depends on a new constant, that is explicitly expressed in terms of the problem's parameters and the geometry of the feasible set. This constant replaces the pyramidal width, which is difficult to evaluate.},
issn={1436-4646},
doi={10.1007/s10107-016-1069-4},
url={https://doi.org/10.1007/s10107-016-1069-4}
}


@article{pena2019polytope,
author = {Pe\~{n}a, Javier and Rodr\'{\i}guez, Daniel},
title = {Polytope Conditioning and Linear Convergence of the Frank–Wolfe Algorithm},
journal = {Mathematics of Operations Research},
volume = {44},
number = {1},
pages = {1-18},
year = {2019},
doi = {10.1287/moor.2017.0910},

URL = { 
    
        https://doi.org/10.1287/moor.2017.0910
    
    

},
eprint = { 
        https://doi.org/10.1287/moor.2017.0910 
}
,
    abstract = { It is well known that the gradient descent algorithm converges linearly when applied to a strongly convex function with Lipschitz gradient. In this case, the algorithm’s rate of convergence is determined by the condition number of the function. In a similar vein, it has been shown that a variant of the Frank–Wolfe algorithm with away steps converges linearly when applied to a strongly convex function with Lipschitz gradient over a polytope. In a nice extension of the unconstrained case, the algorithm’s rate of convergence is determined by the product of the condition number of the function and a certain condition number of the polytope. We shed new light on the latter type of polytope conditioning. In particular, we show that previous and seemingly different approaches to define a suitable condition measure for the polytope are essentially equivalent to each other. Perhaps more interesting, they can all be unified via a parameter of the polytope that formalizes a key premise linked to the algorithm’s linear convergence. We also give new insight into the linear convergence property. For a convex quadratic objective, we show that the rate of convergence is determined by a condition number of a suitably scaled polytope. }
}

@misc{lacoste-julien2015global,
  doi = {10.48550/ARXIV.1511.05932},
  
  url = {https://arxiv.org/abs/1511.05932},
  
  author = {Lacoste-Julien, Simon and Jaggi, Martin},
  
  keywords = {Optimization and Control (math.OC), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences, G.1.6; I.2.6, 90C52, 90C90, 68T05},
  
  title = {On the Global Linear Convergence of Frank-Wolfe Optimization Variants},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}




@article{pena2016on,
author = {Pe\~{n}a, Javier and Rodr\'{\i}guez, Daniel and Soheili, Negar},
title = {On the von Neumann and Frank--Wolfe Algorithms with Away Steps},
journal = {SIAM Journal on Optimization},
volume = {26},
number = {1},
pages = {499-512},
year = {2016},
doi = {10.1137/15M1009937},

URL = { 
    
        https://doi.org/10.1137/15M1009937
    
    

},
eprint = { 
    
        https://doi.org/10.1137/15M1009937
    
    

}
,
    abstract = { The von Neumann algorithm is a simple coordinate-descent algorithm to determine whether the origin belongs to a polytope generated by a finite set of points. When the origin is in the interior of the polytope, the algorithm generates a sequence of points in the polytope that converges linearly to zero. The algorithm's rate of convergence depends on the radius of the largest ball around the origin contained in the polytope. We show that under the weaker condition that the origin is in the polytope, possibly on its boundary, a variant of the von Neumann algorithm that includes away steps generates a sequence of points in the polytope that converges linearly to zero. The new algorithm's rate of convergence depends on a certain geometric parameter of the polytope that extends the above radius but is always positive. Our linear convergence result and geometric insights also extend to a variant of the Frank--Wolfe algorithm with away steps for minimizing a convex quadratic function over a polytope. }
}



@article{paninski2003estimation,
    author = {Paninski, Liam},
    title = "{Estimation of Entropy and Mutual Information}",
    journal = {Neural Computation},
    volume = {15},
    number = {6},
    pages = {1191-1253},
    year = {2003},
    month = {06},
    abstract = "{We present some new results on the nonparametric estimation of entropy and mutual information. First, we use an exact local expansion of the entropy function to prove almost sure consistency and central limit theorems for three of the most commonly used discretized information estimators. The setup is related to Grenander's method of sieves and places no assumptions on the underlying probability measure generating the data. Second, we prove a converse to these consistency theorems, demonstrating that a misapplication of the most common estimation techniques leads to an arbitrarily poor estimate of the true information, even given unlimited data. This “inconsistency” theorem leads to an analytical approximation of the bias, valid in surprisingly small sample regimes and more accurate than the usual  formula of Miller and Madow over a large region of parameter space. The two most practical implications of these results are negative: (1) information estimates in a certain data regime are likely contaminated by bias, even if “bias-corrected” estimators are used, and (2) confidence intervals calculated by standard techniques drastically underestimate the error of the most common estimation methods.Finally, we note a very useful connection between the bias of entropy estimators and a certain polynomial approximation problem. By casting bias calculation problems in this approximation theory framework, we obtain the best possible generalization of known asymptotic bias results. More interesting, this framework leads to an estimator with some nice properties: the estimator comes equipped with rigorous bounds on the maximum error over all possible underlying probability distributions, and this maximum error turns out to be surprisingly small. We demonstrate the application of this new estimator on both real and simulated data.}",
    issn = {0899-7667},
    doi = {10.1162/089976603321780272},
    url = {https://doi.org/10.1162/089976603321780272},
    eprint = {https://direct.mit.edu/neco/article-pdf/15/6/1191/815550/089976603321780272.pdf},
}



@article{antos2001convergence,
author = {Antos, András and Kontoyiannis, Ioannis},
title = {Convergence properties of functional estimates for discrete distributions},
journal = {Random Structures \& Algorithms},
volume = {19},
number = {3-4},
pages = {163-193},
keywords = {functional estimation, entropy estimation, rates of convergence, match lengths},
doi = {https://doi.org/10.1002/rsa.10019},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rsa.10019},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rsa.10019},
abstract = {Abstract Suppose P is an arbitrary discrete distribution on acountable alphabet ��. Given an i.i.d. sample (X1,…,Xn) drawnfrom P, we consider the problem of estimating the entropy H(P) or some other functional F=F(P) of the unknown distribution P. We show that, for additive functionals satisfying mild conditions (including the cases of the mean, the entropy, and mutual information), the plug-in estimates of F are universally consistent. We also prove that, without further assumptions, no rate-of-convergence results can be obtained for any sequence of estimators. In the case of entropy estimation, under a variety of different assumptions, we get rate-of-convergence results for the plug-in estimate and for a nonparametric estimator based on match-lengths. The behavior of the variance and the expected error of the plug-in estimate is shown to be in sharp contrast to the finite-alphabet case. A number of other important examples of functionals are also treated in some detail. © 2001 John Wiley \& Sons, Inc. Random Struct. Alg., 19: 163–193, 2001},
year = {2001}
}




@InProceedings{geist2019theory,
  title = 	 {A Theory of Regularized {M}arkov Decision Processes},
  author =       {Geist, Matthieu and Scherrer, Bruno and Pietquin, Olivier},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2160--2169},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/geist19a/geist19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/geist19a.html},
  abstract = 	 {Many recent successful (deep) reinforcement learning algorithms make use of regularization, generally based on entropy or Kullback-Leibler divergence. We propose a general theory of regularized Markov Decision Processes that generalizes these approaches in two directions: we consider a larger class of regularizers, and we consider the general modified policy iteration approach, encompassing both policy iteration and value iteration. The core building blocks of this theory are a notion of regularized Bellman operator and the Legendre-Fenchel transform, a classical tool of convex optimization. This approach allows for error propagation analyses of general algorithmic schemes of which (possibly variants of) classical algorithms such as Trust Region Policy Optimization, Soft Q-learning, Stochastic Actor Critic or Dynamic Policy Programming are special cases. This also draws connections to proximal convex optimization, especially to Mirror Descent.}
}


@misc{kozuno2022kl,
  doi = {10.48550/ARXIV.2205.14211},
  
  url = {https://arxiv.org/abs/2205.14211},
  
  author = {Kozuno, Tadashi and Yang, Wenhao and Vieillard, Nino and Kitamura, Toshinori and Tang, Yunhao and Mei, Jincheng and Ménard, Pierre and Azar, Mohammad Gheshlaghi and Valko, Michal and Munos, Rémi and Pietquin, Olivier and Geist, Matthieu and Szepesvári, Csaba},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {KL-Entropy-Regularized RL with a Generative Model is Minimax Optimal},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{domingues2021episodic,
  title={Episodic reinforcement learning in finite mdps: Minimax lower bounds revisited},
  author={Domingues, Omar Darwiche and M{\'e}nard, Pierre and Kaufmann, Emilie and Valko, Michal},
  booktitle={Algorithmic Learning Theory},
  pages={578--598},
  year={2021},
  organization={PMLR}
}

@Article{hazan2007logarithmic,
author={Hazan, Elad
and Agarwal, Amit
and Kale, Satyen},
title={Logarithmic regret algorithms for online convex optimization},
journal={Machine Learning},
year={2007},
month={Dec},
day={01},
volume={69},
number={2},
pages={169-192},
abstract={In an online convex optimization problem a decision-maker makes a sequence of decisions, i.e., chooses a sequence of points in Euclidean space, from a fixed feasible set. After each point is chosen, it encounters a sequence of (possibly unrelated) convex cost functions. Zinkevich (ICML 2003) introduced this framework, which models many natural repeated decision-making problems and generalizes many existing problems such as Prediction from Expert Advice and Cover's Universal Portfolios. Zinkevich showed that a simple online gradient descent algorithm achieves additive regret{\$}O({\backslash}sqrt{\{}T{\}}){\$}, for an arbitrary sequence of T convex cost functions (of bounded gradients), with respect to the best single decision in hindsight.},
issn={1573-0565},
doi={10.1007/s10994-007-5016-8},
url={https://doi.org/10.1007/s10994-007-5016-8}
}

@article{bubeck2015convex,
author = {Bubeck, S\'{e}bastien},
title = {Convex Optimization: Algorithms and Complexity},
year = {2015},
issue_date = {11 2015},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {8},
number = {3–4},
issn = {1935-8237},
url = {https://doi.org/10.1561/2200000050},
doi = {10.1561/2200000050},
abstract = {This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by the seminal book of Nesterov, includes the analysis of cutting plane methods, as well as accelerated gradient descent schemes. We also pay special attention to non-Euclidean settings relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with FISTA to optimize a sum of a smooth and a simple non-smooth term, saddle-point mirror prox Nemirovski's alternative to Nesterov's smoothing, and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, mini-batches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods.},
journal = {Found. Trends Mach. Learn.},
month = {nov},
pages = {231–357},
numpages = {127}
}



@inproceedings{grill2019planning,
 author = {Grill, Jean-Bastien and Darwiche Domingues, Omar and Menard, Pierre and Munos, Remi and Valko, Michal},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Planning in entropy-regularized Markov decision processes and games},
 url = {https://proceedings.neurips.cc/paper/2019/file/50982fb2f2cfa186d335310461dfa2be-Paper.pdf},
 volume = {32},
 year = {2019}
}

@book{boyd2004convex, place={Cambridge}, title={Convex Optimization}, DOI={10.1017/CBO9780511804441}, publisher={Cambridge University Press}, author={Boyd, Stephen and Vandenberghe, Lieven}, year={2004}}

@misc{van2014probability,
  title={Probability in high dimensions. Manuscript, 2014},
  author={van Handel, Ramon},
  year={2014}
}

@article{savas2019entropy,
  title={Entropy maximization for Markov decision processes under temporal logic constraints},
  author={Savas, Yagiz and Ornik, Melkior and Cubuktepe, Murat and Karabag, Mustafa O and Topcu, Ufuk},
  journal={IEEE Transactions on Automatic Control},
  volume={65},
  number={4},
  pages={1552--1567},
  year={2019},
  publisher={IEEE}
}

@article{ekroot1993entropy,
  title={The entropy of Markov trajectories},
  author={Ekroot, Laura and Cover, Thomas M},
  journal={IEEE Transactions on Information Theory},
  volume={39},
  number={4},
  pages={1418--1421},
  year={1993},
  publisher={IEEE}
}

@article{kakade2009duality,
  title={On the duality of strong convexity and strong smoothness: Learning applications and matrix regularization},
  author={Kakade, Sham and Shalev-Shwartz, Shai and Tewari, Ambuj and others},
  journal={Unpublished Manuscript, http://ttic. uchicago. edu/shai/papers/KakadeShalevTewari09. pdf},
  volume={2},
  number={1},
  pages={35},
  year={2009}
}

@incollection{sutton1990integrated,
title = {Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming},
editor = {Bruce Porter and Raymond Mooney},
booktitle = {Machine Learning Proceedings 1990},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {216-224},
year = {1990},
isbn = {978-1-55860-141-3},
doi = {https://doi.org/10.1016/B978-1-55860-141-3.50030-4},
url = {https://www.sciencedirect.com/science/article/pii/B9781558601413500304},
author = {Richard S. Sutton},
abstract = {This paper extends previous work with Dyna, a class of architectures for intelligent systems based on approximating dynamic programming methods. Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned model of the world. In this paper, I present and show results for two Dyna architectures. The Dyna-PI architecture is based on dynamic programming's policy iteration method and can be related to existing AI ideas such as evaluation functions and universal plans (reactive systems). Using a navigation task, results are shown for a simple Dyna-PI system that simultaneously learns by trial and error, learns a world model, and plans optimal routes using the evolving world model. The Dyna-Q architecture is based on Watkins's Q-learning, a new kind of reinforcement learning. Dyna-Q uses a less familiar set of data structures than does Dyna-PI, but is arguably simpler to implement and use. We show that Dyna-Q architectures are easy to adapt for use in changing environments.}
}

@inproceedings{cesabianchi2017boltzmann,
author = {Cesa-Bianchi, Nicol\`{o} and Gentile, Claudio and Lugosi, G\'{a}bor and Neu, Gergely},
title = {Boltzmann Exploration Done Right},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Boltzmann exploration is a classic strategy for sequential decision-making under uncertainty, and is one of the most standard tools in Reinforcement Learning (RL). Despite its widespread use, there is virtually no theoretical understanding about the limitations or the actual benefits of this exploration scheme. Does it drive exploration in a meaningful way? Is it prone to misidentifying the optimal actions or spending too much time exploring the suboptimal ones? What is the right tuning for the learning rate? In this paper, we address several of these questions for the classic setup of stochastic multi-armed bandits. One of our main results is showing that the Boltzmann exploration strategy with any monotone learning-rate sequence will induce suboptimal behavior. As a remedy, we offer a simple non-monotone schedule that guarantees near-optimal performance, albeit only when given prior access to key problem parameters that are typically not available in practical situations (like the time horizon T and the suboptimality gap Δ). More importantly, we propose a novel variant that uses different learning rates for different arms, and achieves a distribution-dependent regret bound of order K log2 T/Δ and a distribution-independent bound of order √KT log K without requiring such prior knowledge. To demonstrate the flexibility of our technique, we also propose a variant that guarantees the same performance bounds even if the rewards are heavy-tailed.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6287–6296},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}