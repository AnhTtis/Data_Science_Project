\section{Trajectory Entropy}
\label{sec:trajectory_entropy}


In this section we focus on another type of entropy, the trajectory entropy, that can be efficiently maximized. The entropy of paths of a (Markovian) stochastic process is introduced by \citet{ekroot1993entropy}. It quantifies the randomness of realizations with fixed initial and final states. Later it was extended \citep{savas2019entropy}  to realizations that reach a certain set of states, rather than a fixed final state.   This type of entropy is also closely related to the so-called entropy rate of a stochastic process.

\paragraph{Trajectory entropy} We define the trajectory entropy of a policy $\pi$ as the entropy of a trajectory generated with the policy $\pi$ 
\[
\TE(q^\pi) \triangleq \cH(q^\pi) = \sum_{m\in\cT} q^\pi(m) \log\frac{1}{q^\pi(m)}\,.
\]

We denote by $\pistarTE\in\argmax_{\pi} \TE(q^\pi)$ a policy that maximizes the trajectory entropy. 

\paragraph{Maximum trajectory entropy exploration} MTEE differs from MVEE only in the choice of entropy. In particular an algorithm $((\pi^t)_{t\in\N_+},\tau,\hpi)$ for MTEE is also a combination of a time dependent policy $(\pi^t)_{t\in\N_+}$, a stopping rule $\tau$, and a decision rule $\hpi$. 
\begin{definition}
(PAC algortihm for MTEE) An algorithm $((\pi^t)_{t\in\N},\tau,\hpi)$ is $(\epsilon,\delta)$-PAC for MTEE if 
\[
\P\left( \TE\big(q^{\pistarTE}\big)- \TE(q^{\hpi}) \leq \epsilon \right) \leq 1-\delta\,.
\]
\end{definition}

\vspace{-0.25cm}
As noted by \citet{eysenbach2019if}, MTEE can  also be connected to a prediction game. In this game, the forecaster-player aims to predict the whole trajectory that the sampler-player will generate. Remark that predicting the trajectory implies to predict, in particular, the visited state-action pairs but the reverse is not true in general \footnote{Indeed $d_h^\pi$ are only the marginals of $q^\pi$.}. We could then apply the same strategy as in Section~\ref{sec:visitation_entropy} to solve MTEE. Nevertheless, for trajectory entropy, there is a more direct way to proceed.

\paragraph{Entropy regularized Bellman equations} One big difference between MVEE and MTEE is that the optimal policy can be obtained by solving regularized Bellman equations. Indeed, thanks to the chain rule for the entropy, the trajectory entropy of a policy $\pi$ is $\TE(d^\pi) = V_1^\pi(s_1)$ and the maximum trajectory entropy is $\TE\big(d^{\pistarTE}\big)  = \Vstar_1(s_1)$ where the value functions $V^\pi$ and $\Vstar$ satisfy
{\small
\begin{align*}
	Q_h^{\pi}(s,a) &= \cH\big(p_h(s,a)\big) + p_h V_{h+1}^\pi(s,a) \,,\\
    V_h^\pi(s) &= \pi_h Q_h^\pi (s) +\cH\big(\pi_h(s)\big)\,,\\
  Q_h^\star(s,a) &=  \cH\big(p_h(s,a)\big) + p_h V_{h+1}^\star(s,a) \,,\\
  V_h^\star(s) &= \max_{\pi\in\Delta_A} \{ \pi Q_h^\star (s) + \cH(\pi)\}\,,
\end{align*}}

\vspace{-0.4cm}
\!where by definition, $V_{H+1}^\star \triangleq V_{H+1}^\pi \triangleq 0$. In particular, the maximum trajectory entropy policy is given by $\pistarTE_h(s) = \argmax_{\pi\in\Delta_A} (\pi\Qstar(s)+\cH(\pi))$.
It can be computed explicitly via $\pistarTE_h(a|s) = \exp\!\big(\Qstar_h(s,a) -\Vstar_h(s)\big)$ as well as the optimal value function $\Vstar_h(s) = \log\left(\sum_{a\in\cA} \rme^{\Qstar_h(s,a)}\right)$. We refer to Appendix~\ref{app:reg_bellman_eq} for a complete derivation.



We now describe our algorithm \RFExploreEnt, the description of \algMTEE is postponed to Appendix~\ref{app:regularized_mdp}.  The idea of the algorithm is rather simple: since we need to solve regularized Bellman equations  to obtain a maximum trajectory entropy policy, we can 1) find a \textit{preliminary exploration policy $\pi^{\mathrm{mix}}$} allowing one to construct  estimates of the transition probabilities which are uniformly  good when computing expectations of arbitrary bounded functions over all policies (see Lemma~\ref{lem:sampling_square_value_error_bound}), and 2) solve the regularized Bellman equations based on the estimated model. A similar approach is used in reward-free exploration \citep{jin2020reward-free,kaufmann2020adaptive,menard2021fast}, and, in particular, our algorithm is close to \RFExplore by \citet{jin2020reward-free}. However, the key difference is that in the presence of regularization a much smaller number of transitions (trajectories) needs to be collected in order to obtain a high quality policy.

\paragraph{Exploration phase}

This phase is devoted to learn a simple (non-Markovian) preliminary exploration policy $\pi^{\mathrm{mix}}$ that could be used to construct a accurate enough estimates of transition probabilities. This policy is obtained, as in \RFExplore, by learning for each state $s$ and step $h$, a policy that reliably reaches state $s$ at step $h$. This can be done by running for $N_0$ iterations any regret minimization algorithm, e.g. \EULER \citep{zanette2019tighter}, for the sparse reward function putting reward one at state $s$ at step $h$ and zero otherwise. The policy $\pi^{\mathrm{mix}}$ is defined as the uniform mixture of the aforementioned policies. Then the policy $\pi^{\mathrm{mix}}$ is used to collect $N$ fresh independent trajectories from the MDP.


\paragraph{Planning phase}
For the planning phase, the agent builds a transition model
\begin{equation}\label{eq:empirical_model}
    \hp_h(s'|s,a) = \begin{cases}
        \frac{n_h(s'|s,a)}{n_h(s,a)} & n_h(s,a) > 0 \\
        \frac{1}{S} & n_h(s,a) = 0
    \end{cases}\,,
\end{equation}
where $n_h(s,a)$ is the number of visits of the state-action pair $(s,a)$ at step $h$ for these $N$ sampled trajectories.
The final policy is a solution to the empirical regularized Bellman equations
{\small
\begin{align}\label{eq:empirical_entropy_bellman}
    \begin{split}
      \hQ_h^\star(s,a) &=  \cH\big(\hp_h(s,a)\big) + \hp_h \hV_{h+1}^\star(s,a) \,,\\
      \hV_h^\star(s) &= \max_{\pi\in\Delta_A}\left\{  \pi \hQ_h^\star (s) + \cH(\pi) \right\}\,,\\
      \hpi_h(s) &= \argmax_{\pi\in\Delta_A}\left\{  \pi \hQ_h^\star (s) + \cH(\pi)\right\} \,.
    \end{split}
\end{align}}

The complete procedure is described in Algorithm~\ref{alg:RFExploreEnt}. We now prove that, for the well-calibrated choice of $N$ and $N_0$ of order $\tcO(\poly(S,A,H)/\varepsilon)$, the \RFExploreEnt algorithm is $(\epsilon,\delta)$-PAC for MTEE and provide an upper bound on its sample complexity. For the proof we refer to Appendix~\ref{app:fast_rates_regularized}.

\begin{theorem}
    The algorithm \RFExploreEnt with parameters $N_0 = \Omega\left( \frac{H^7 S^3 A \cdot L^3}{\varepsilon}\right)$ and $N = \Omega\left( \frac{ H^6 S^3 A  L^5 }{\varepsilon}\right)$ is $(\varepsilon,\delta)$-PAC for the MTEE problem, where $L = \log(SAH/(\varepsilon \delta))$. Its total sample complexity $SHN_0 + N$ is bounded by
    \[
        \tcO\left( \frac{H^8 S^4 A}{\varepsilon} \right).
    \]
\end{theorem}


\begin{algorithm}[h!]
\centering
\caption{\RFExploreEnt}
\label{alg:RFExploreEnt}
\begin{algorithmic}[1]
  \STATE {\bfseries Input:} Target precision $\epsilon$, target probability $\delta$, number of episodes for simple exploration policy $N_0$, number of sampled trajectories $N$.
    \FOR{$(s',h') \in \cS \times [H]$}
        \STATE Form rewards $r_h(s,a) = \ind\{ s=s', h=h'\}$.
        \STATE Run \EULER \citep{zanette2019tighter} with rewards $r_h$ over $N_0$ episodes and collect all policies $\Pi_{s',h'}$.
        \STATE Modify $\pi \in \Pi_{s',h'}:\ \pi_{h'}(a|s') = 1/A$ for all $a\in \cA$.
    \ENDFOR
    \STATE Construct a uniform mixture policy $\pi^{\mathrm{mix}}$ over all $\{\pi\in\Pi_{s,h} : (s,h) \in \cS \times [H] \}$.
    \STATE Sample $N$ independent trajectories $(z_n)_{n\in[N]}$ following $\pi^{\mathrm{mix}}$ in the original MDP.
    \STATE Construct from $(z_n)_{n\in[N]}$ an empirical model $\hp_h$ as in \eqref{eq:empirical_model}.
   \STATE Output policy $\hpi$ as a solution to \eqref{eq:empirical_entropy_bellman}.
\end{algorithmic}
\end{algorithm}


\begin{remark} 
\label{rem:regularized_mdp}
(On solving regularized MDPs) Interestingly, our approach for MTEE can be adapted to solve entropy-regularized MDPs. For a reward functions $(r_h)_{h\in[H]}$ and regularization parameter $\lambda>0,$ consider the regularized Bellman equations 
{\small
\begin{align*}
    Q^{\pi}_{\lambda,h}(s,a) &= r_h(s,a) + p_h V^{\pi}_{\lambda,h+1}(s,a)\,,\\
    V^\pi_{\lambda,h}(s) &= \pi_h Q^\pi_{\lambda,h}(s) + \lambda \cH\big(\pi_h(s)\big)\,,\\
    \Qstar_{\lambda,h}(s,a) &= r_h(s,a) + p_h \Vstar_{\lambda,h+1}(s,a)\,,\\
    \Vstar_{\lambda,h}(s) &= \max_{\pi\in\simplex_A} \pi\Qstar_{\lambda,h}(s) + \lambda \cH(\pi),
\end{align*}}
\!where $V_{\lambda, H+1}^\pi = \Vstar_{\lambda, H+1} = 0$. Note that these are the Bellman equations used by \SoftQlearning \citep{fox2016taming,schulman2017equivalence,haarnoja2017reinforcement} and \SAC \citep{haarnoja2018soft} algorithms. We are interested in the best policy identification for this regularized MDP. That is finding an algorithm that will output an $\epsilon$-optimal policy $\hpi$ such that with probability $1-\delta$, it holds $\Vstar_{\lambda,1}(s_1) - V^{\hpi}_{\lambda,1}(s_1) \leq \epsilon$ after a minimal number $\tau$ of trajectories sampled from the MDP $\cM^r = (\cS,\cA, H ,(p_h)_{h\in[H]},(r_h)_{h\in[H]},s_1)$. 
By using similar exploration and planning phases as in \RFExploreEnt, we get an algorithm for BPI in the entropy-regularized MDP that also enjoys the fast rate of order $\tcO\big(H^8S^4A/(\epsilon \lambda)\big)$. Moreover, this algorithm could be used for more general types of regularization and even in reward-free setting with the same order of the sample complexity.
Refer to Appendix~\ref{app:regularized_mdp}-\ref{app:fast_rates_regularized} for precise statements and proofs.

We observe that the sample complexity for solving the regularized MDP is strictly smaller\footnote{For small enough $\epsilon$.} than the sample complexity for solving the original MDP. Indeed, one needs at least $\tcO(H^3SA/\epsilon^2)$ trajectory \citep{domingues2021episodic} to learn a policy $\pi$ which value in the (unregularized) MDP is $\epsilon$-close to the optimal value. Nevertheless, regularizing the MDP introduces a bias in the value function. Precisely we have for all $\pi$, $0\leq V^\pi_{\lambda,1}(s_1)-  V^\pi_1(s_1) \leq \tcO(\lambda H)$ where 
$V^\pi_1(s_1)$ is the value function of $\pi$ at the initial state and MDP $\cM^r$. Thus, to solve BPI in $\cM^r$ through BPI in the regularized MDP, one needs to take $\lambda = \tcO(1/(H\epsilon))$, leading to a sample complexity of order $\tcO(H^9S^4A/\epsilon^2)$. In particular, our fast rate for BPI in regularized MDP does not contradict the lower bound for BPI in the original MDP. However, our analysis shows that regularization is an effective way to trade-off bias for sample complexity.
\end{remark}


\paragraph{Visitation entropy vs trajectory entropy}
We can compare the visitation entropy and the trajectory entropy with
\[
\TE(q^\pi)\leq  \underbrace{\KL(q^\pi,\otimes_{h=1}^H d^\pi_h) + \TE(q^\pi)}_{\VE(d^\pi)} \leq H \TE(q^\pi)\,,
\]
where $\otimes_{h=1}^H d^\pi_h$ is a product measure, see Lemma~\ref{lem:comparison_ent_traj_visit} in Appendix~\ref{app:technical} for a proof. Note also that in general the visitation distributions of an optimal policy for maximum trajectory entropy will be less 'spread' than the one of an optimal policy for MTEE, see Section~\ref{sec:experiments} for an example. In particular one can prove that the optimal policy for MTEE is the uniform policy if the transitions are deterministic, see Lemma~\ref{lem:MTEE_deterministic} of Appendix~\ref{app:technical}.

% \textcolor{red}{
% Regarding the difference in computational hardness, the MVEE problem is challenging even in the case of known transition probabilities. When relying on the dual representation \citep{zimin2013online}, it requires solving a high-dimensional convex optimisation problem, whereas solving the MTEE problem for a known MDP requires only simple dynamic programming. Thus, we may expect that the MVEE problem is more computationally challenging than the MTEE problem.
% }




\subsection{Proof Sketch}\label{sec:proof_mtee}
In this section we sketch the proof of Theorem~\ref{th:mtee_sample_complexity}. 

\paragraph{Properties of entropy}

We start from analysing several properties of the entropy function. First, we notice that the well-known log-sum-exp function is a convex conjugate to the negative entropy defined only on the probability simplex \cite{boyd2004convex}
%{\small
\[
    F(x) \triangleq \log\left( \sum_{a \in \cA} \rme^{x_a} \right) = \max_{\pi \in \simplex_A} \langle \pi, x \rangle + \cH(\pi)
\]
and extend its action to $Q$-functions
\[
    F(Q)(s) \triangleq \max_{\pi \in \simplex_A} \pi Q(s) + \cH(\pi). 
\]
This definition is useful because we can rewrite the optimal value function for MTEE in real and empirical model as follows
\begin{equation}\label{eq:V_star_using_F}
    \Vstar_{h}(s) = F(\Qstar_h)(s), \quad \hV^{\hpi}_h(s) = F(\hQ^{\hpi}_h)(s).
\end{equation}
Additionally, we notice that the gradient of $F$ is equal to the soft-max policy that maximizes the expressions above
\[
    \pistar_h(s) = \nabla F(\Qstar_h)(s) , \quad \hpi_h(s) = \nabla F(\hQ^{\hpi}_h)(s),
\]
and, moreover, since the negative entropy $-\cH(\pi)$ is $1$-strongly convex with respect to $\ell_1$ norm, gradients of $F$ is $1$-Lipschitz with respect to $\ell_\infty$ norm by the properties of the convex conjugate \cite{kakade2009duality}. Combining the gradient properties with  the smoothness definition to $\Qstar$ and $\hQ^{\hpi}$ we obtain
\begin{small}
\begin{align}\label{eq:F_smooth_Q}
    \begin{split}
    F(\Qstar_h)(s) &\leq F(\hQ^{\hpi}_h)(s) + \hpi_h\left( \Qstar_h- \hQ^{\hpi}_h\right)(s) \\
    &+ \frac{1}{2} \norm{ \Qstar_h - \hQ^{\hpi}_h}^2_\infty(s),
    \end{split}
\end{align}\end{small}
where \begin{small}$\norm{ \Qstar_h - \hQ^{\hpi}_h}_\infty(s) = \max_{a\in\cA} \vert \Qstar_h(s,a) -\hQ^{\hpi}_h(s,a) \vert$.\end{small}

\paragraph{Bound on the policy error}

Next we apply the key inequality \eqref{eq:F_smooth_Q} to analyze the error between the optimal policy and policy $\hpi$. Using~\eqref{eq:V_star_using_F} yields
\begin{small}
\begin{align*}
    \Vstar_h(s) - V^{\hpi}_h(s) &\leq \hV^{\hpi}_h(s) - V^{\hpi}_h(s)  + \hpi_h \big(\Qstar_h - \hQ^{\hpi}_h \big)(s) \\
    &+ \frac{1}{2} \max_{a \in \cA} \left( \hQ^{\hpi}_h(s,a) - \Qstar_h(s,a) \right)^2.
\end{align*}
\end{small}

\vspace{-0.4cm}
Next, by definition of $\hpi$  we have
\begin{small}
$
    \hV^{\hpi}_h(s) = \cH(\hpi_h(s)) +  \hpi_h \hQ^{\hpi}_h(s),
$
\end{small}
therefore by the regularized Bellman equations
\begin{small}
\begin{align*}
    \begin{split}
    \Vstar_h(s) - V^{\hpi}_h(s) & \leq \hpi_h p_h \left( \Vstar_{h+1} - V^{\hpi}_{h+1} \right)(s) \\
    &+\frac{1}{2} \max_{a \in \cA} \left( \hQ^{\hpi}_h(s,a) - \Qstar_h(s,a) \right)^2.
    \end{split}
\end{align*}\\
\end{small}

\vspace{-0.8cm}
Finally, rolling out this expression we have
\begin{small}
\begin{align*}
    \Vstar_1(s_1) - V^{\hpi}_1(s_1) \leq \frac{1}{2}\E_{\hpi}\left[ \sum_{h=1}^H \max_{a \in \cA} \left( \hQ^{\hpi}_h - \Qstar_h \right)^2(s_h,a)\right]\,.
\end{align*}\\
\end{small}
Next we may notice that in the generative model setting\footnote{When there is a sampling oracle for each state-action pair.} there is available results that tells us that $\tcO(1/\varepsilon)$ samples are enough to obtain $\norm{\hQ^{\hpi}_h - \Qstar_h}_\infty \lesssim \sqrt{\epsilon}$ \citep{azar2013minimax}, and we can conclude the statement. However, in the online setup the situation is more complicated, and we apply reward-free techniques developed by \citet{jin2020reward-free} to obtain a "surrogate" of the generative model.

% an estimate of the model, using $\tcO(1/\epsilon)$ samples, that will guarantee that $\hQ^{\hpi}_h(s,a) - \Qstar_h(s,a) \lessim \sqrt{\epsilon}$. It is very easy to show in the setting of
% and, finally, 
% \begin{small}$
%     \max\limits_{a \in \cA} \left( \uQ^t_h(s,a) - \Qstar_h(s,a) \right)^2 \leq \big(\uV^t_h(s) - \lV^t_h(s)\big)^2,
% $\end{small}
% where $\lV^t_h$ is a lower confidence bound on $\Qstar_h$. In other words, in the regularized setting we are able to propagate the policy error from step $h$ to step $h+1$ by the cost of only a squared over-estimation error. It is the most crucial part of the proof because this square allows us to make all the first-order terms of type $1/\sqrt{n^t_h(s,a)}$  the second-order terms of type $1/n^t_h(s,a)$ and obtain fast rates.

% To finalize all these formulas to the computable decision rule, we notice that we can exchange $p_h$ with $\hp^t_h$ in the right-hand side of \eqref{eq:policy_error_ineq} by Lemma~\ref{lem:reg_directional_concentration} paying the second-order term and $(1+1/H)$-factor. Overall, we obtain
% \begin{equation}\label{eq:policy_error_via_gap}
%     \Vstar_1(s_1) - V^{\pi^{t+1}}_1(s_1) \leq G^t_1(s_1)
% \end{equation}
% for $G^t_h(s)$ defined in \eqref{eq:upper_bound_gap}. This inequality automatically justifies that \UCBVIEnt is $(\varepsilon,\delta)$-PAC of MTEE because for $\hpi = \pi^{\tau+1},$ we have $G^{\tau}_1(s_1) \leq \varepsilon$, and \eqref{eq:policy_error_via_gap} for $t=\tau$ concludes the statement.


% \begin{remark}
% Another key difference between our approach and BPI is that smoothness of the log-sum-exp function allows us to define the upper bounds on the policy error not in terms of the difference of $Q$-functions, but of value-functions.
% \end{remark}


% \paragraph{Sample complexity}
% \todoDe{May give too many details in this paragraph ...}
% To derive an upper bound on sample complexity we have to provide an upper bound on $G^t_1(s_1)$. To do it, we can change  $\hp^t_h$ back to $p_h$ with a second-order penalty and by rolling out conditional expectations
% \[
%     G^t_1(s_1) \leq \E_{\pi^{t+1}}\biggl[ \sum_{h=1}^H \frac{\tcO(H^2S)}{n^t_h(s_h,a_h)} + \frac{\rme^2}{2}(\uV^t_h - \lV^t_h)^2(s_h)  \Big| s_1 \biggl].
% \]
% Next we analyze the second term. The difference $\uV^t_h - \lV^t_h$ is a standard object in the analysis of \UCBVI-like algorithms and could be upper bounded as follows
% \begin{small}
%     \begin{align*}
%         (\uV^t_h &- \lV^t_h)(s_h) \leq \sqrt{\E_{\pi^{t+1}}\left[ \sum_{h'=h}^H \frac{\tcO(H^3)}{n^t_{h'}(s_{h'},a_{h'})} \bigg| s_h\right]} \\
%         &+ \tcO(H) \cdot \E_{\pi^{t+1}}\left[ \sum_{h'=h}^H \min\left(\frac{\tcO(HS)}{n^t_{h'}(s_{h'},a_{h'})}, 1\right) \bigg| s_h\right].
%     \end{align*}\\\end{small}
% In the setting of unregularized MDPs the term that is similar to the first one in the upper bound above has the leading role because it scales as sum of $1/\sqrt{n^t_h(s,a)}$ and leads to $\tcO(1/\varepsilon^2)$ sample complexity, In our case, this term becomes the second-order one too that leads to improvement in sample complexity.
    
% Taking the square and applying Jensen's inequality several times we can obtain
% \[
%     G^t_1(s_1) \leq \E_{\pi^{t+1}}\left[ \sum_{h=1}^H \frac{\tcO(H^5 S)}{n^t_h(s,a)} \bigg| s_1  \right].
% \]
% Notice that for all $t < \tau$ we have $G^t_1(s_1) > \varepsilon$, thus summing up the inequality above for all $t < \tau$
% \[
%     (\tau-1)\varepsilon \leq  \sum_{h,s,a} \sum_{t=1}^{\tau-1} d^{\pi^{t+1}}_h(s,a) \frac{\tcO(H^5 S)}{n^t_h(s,a)}  = \tcO\left( H^6 S^2 A \right),
% \]
% that yields the fast rate for the MTEE problem.

