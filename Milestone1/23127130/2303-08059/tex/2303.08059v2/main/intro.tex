\section{Introduction}\label{sec:intro}



In reinforcement learning (RL), an agent interacts with an environment  aiming to maximize the sum of rewards returned by the environment. When the reward signal is very sparse or completely absent, the agent may experience long periods without any feedback. In these periods exploration is the main challenge. 

This work studies the problem of efficient \emph{exploration in the absence of rewards}. Approaches to solve this problem can be roughly cast into three main groups: The \emph{bonus-based exploration} where the agent maximizes self-defined bonuses or intrinsic rewards collected along trajectory \citep{schmidhuber1991possibility,oudeyer2007intrisic,bellemare2016unifying}. Typically these bonuses are related to the variances of  error-signals from some auxiliary tasks, such as learning the transition probability distributions \citep{schmidhuber1991possibility,chentanez2004intrinsically, pathak2017curiosity,savas2019entropy},
learning the optimal value function for all the possible rewards \citep{jin2020reward-free,kaufmann2020adaptive,menard2021fast}, learning random generated features \citep{burda2019exploration}. A second approach is the \emph{goal-conditioned exploration} where the agent learns to navigate to self-assigned states (or goals). A common goal-selection rule for this class of algorithms is to select as goals the states at the frontier of the visited states \citep{lim2012autonomous,tarbouriech2020improved, escoffet2019goexplore}. Other selection-goal rules include reaching each state a fixed number of times \citep{tarbouriech2021provably} or going to states where the estimation error for the transition probabilities is large \citep{tarbouriech2020active}. The third approach, which has received relatively less attention thus far, is the \emph{maximum entropy exploration} \citep{hazan2019provably,lee2019efficient,mutti2020intristically,mutti2021task}. This approach involves learning a policy that aims to achieve a visitation distribution over state-action pairs that is as uniform as possible. One specific application of this approach is in unsupervised pretraining, where it helps to obtain a better initial policy \cite{seo21state,zhang2021exploration,mutti2022unsupervised}. To achieve this goal, the approach focuses on maximizing entropy-like functionals, which is the main focus of our study.


% \textcolor{red}{ The third approach, which so far received bit less attention, is the \emph{maximum entropy exploration} \citep{hazan2019provably,lee2019efficient,mutti2020intristically,mutti2021task}; where a policy leading to the as uniform as possible visitation distribution over state-action pairs is learned. In particular, obtaining such policy is useful as a unsupervised pretraining phase to obtain better initial policy \cite{seo21state,zhang2021exploration,mutti2022unsupervised}}. Such goal is usually achieved by maximizing  entropy-like functionals and this is what we study. 

In this work, we focus on environments modeled by an episodic, finite, reward-free Markov Decision Process (MDP) with $S$ states, $A$ actions, horizon $H$ and step-dependent transitions. We consider two types of entropy:  the \emph{visitation entropy} and  the \emph{trajectory entropy}. The visitation entropy of a policy is defined as the sum of the entropies of the visitation distributions induced by the given policy at each step. The trajectory entropy of a policy is given by the entropy of a trajectory, generated when one follows the given policy and seen as one random variable on the corresponding path space. We study maximum entropy exploration under the $(\epsilon,\delta)$-PAC framework, that is, we want to learn, with probability $1-\delta,$ a policy leading to  $\epsilon$-optimal maximum visitation or trajectory entropy
and  using as few as possible interactions with the environment. 

\paragraph{Visitation entropy} \citet{hazan2019provably} study maximum visitation entropy\footnote{Note that \citet{hazan2019provably} consider a slightly different entropy; see Remark~\ref{rem:hazan_entropy_vs_us}.} exploration (MVEE) in the more general framework of convex MDPs where the agent wants to maximize a convex function of the visitation distribution. The authors in  \citet{hazan2019provably} propose to apply the Frank-Wolfe algorithm \citep{frank1956algorithm} to a smoothed version of the visitation entropy. Their algorithm, \MaxEnt\footnote{In this work we refer to \MaxEnt as the algorithm by \citet{hazan2019provably} applied to the visitation entropy and not to the reverse entropy as initially proposed by the authors.}, has a sample complexity of order\footnote{We adapt rates from the $\gamma$-discounted setting by replacing $1/(1-\gamma)$ with $H$. To take into account step-dependent transitions we multiply the first order term by $H^2$.} $\tcO(H^4S^2A/\epsilon^3+S^3/\epsilon^6)$, that is, it needs to sample that  number of trajectories in order to find an $\epsilon$-optimal policy for MVEE.
Later, \citet{cheung2019exploration} obtains a better rate of order $\tcO(H^4S^2A/\epsilon^2+H^3 S/\epsilon^3)$ for the \TocUCRL algorithm. Then, building on the ideas introduced by \citet{abernethy2017frankwolfe}, \citet{zahavy2021reward} reinterpret the \MaxEnt algorithm as a method to compute the equilibrium of a particular game induced by the Legendre-Fenchel transform of the smoothed entropy. Using this new point of view, they propose the \MetaEnt algorithm\footnote{We call \MetaEnt the specialization of their general Meta-algorithm to the special case of MVEE. Note that \MaxEnt, \TocUCRL, \MetaEnt could be seen as variations of the same algorithm. We use different names to distinguish, at least, the associated sample complexity.} with a sample complexity of order $\tcO(H^4S^2A/(\delta^2\epsilon^2)+H^3S/\epsilon^3)$. 
In this work, building on the ideas by \citet{grunwald2002game}, we draw a connection between MVEE and another game. In this game, a \emph{forecaster-player tries to predict the state-action pairs visited by a sampler-player} who aims at \emph{surprising the forecaster-player by visiting not well predicted state-action pairs}. We propose the \EntGame algorithm that tackles MVEE by solving this prediction game. We prove that \EntGame has a sample complexity of order $\tcO(H^4S^2A/\epsilon^2+HSA/\epsilon)$, thus improving over the previous rate in terms of its  dependence of $\epsilon$, see Table~\ref{tab:sample_complexity}. The key technical point leading to this improvement is that, contrary to the previous algorithms, \EntGame does not need to estimate accurately the visitation distribution of a policy at each iteration but only needs \emph{one trajectory generated by following this policy}. Moreover, we propose \regalgMVEE, the regularized version of \algMVEE, that achieves sample complexity of order $\tcO(H^2SA/\epsilon^2)$, additionally improving the previous rates in $S$. The main technique behind this improvement is exploiting a strong connection between visitation entropy and regularization in MDPs. As a result, we have shown that \textit{MVEE is statistically strictly simpler than reward-free exploration} \citep{jin2020reward-free}. 


\paragraph{Trajectory entropy} The second problem we consider is maximum trajectory entropy exploration (MTEE). 
The entropy of paths of a (Markovian) stochastic process was first introduced in \citet{ekroot1993entropy}. Intuitively maximizing the trajectory entropy of an MDP minimizes the predictability of paths. Also there is a close connection between MTEE and regularized RL, a very popular approach in practical applications of RL.
%Even if trajectory entropy is a natural choice we show that an optimal policy for trajectory entropy will %not induced a visitation distribution as `spread' as the one obtained with a policy optimal for visitation %entropy.\footnote{See Section~\ref{sec:trajectory_entropy}.} However, as we will see the connection %between trajectory entropy and regularized RL justifies our interest for MTEE. 

Contrary to MVEE, the optimal policy for MTEE can easily be obtained by solving \emph{entropy-regularized Bellman equations} with \emph{entropy of the transition probabilities as rewards}. Leveraging this observation, one can proceed in a similar way as for the best policy identification\footnote{Where in this problem the goal is to identify the optimal policy of a given MDP (equipped with a reward function).} (BPI, \citealt{fiechter1994efficient}). Precisely, we propose two algorithms. The first one, \UCBVIEnt is the simplest one and computes a policy by solving optimistic version of the aforementioned Bellman equations and using it to interact with the environment. The algorithm stops when an upper confidence bound on the difference between the maximum trajectory entropy and the trajectory entropy of the current policy is small enough. The second algorithm, \RFExploreEnt, is an adaptation of the  reward-free exploration  by \citet{jin2020reward-free} to our setting. This algorithm has two phases. In the first phase, we compute a \textit{preliminary exploration policy} which is then used to generate independent trajectories (data). In the second phase, a nearly optimal MTEE policy is obtained by solving the empirical Bellman equations  with transitions estimated from the data collected  in the first phase.


% Precisely, we propose the \RFExpressEnt algorithm that first explores MDP in reward-free fashion and then computes a policy solving optimistic version of the aforementioned Bellman equations and uses it to interact with the environment. The algorithm stops when some upper confidence bound on the difference between the maximum trajectory entropy and the trajectory entropy of the current policy is small enough. 
Interestingly, we prove that \RFExploreEnt enjoys a sample complexity of order $\tcO(\poly(S,A,H)/\epsilon)$. The key technical ingredients to obtain such fast rate are exploitation of the \emph{smoothness introduced by the regularization} and the use of the explicit form of the optimal policy.


% showing a separation between MTEE and other exploration problem such that reward-free exploration (RFE) where the optimal sample complexity is of order $\tcO(H^6S^2A/\epsilon)$. 

\paragraph{Regularized MDPs} Notably we can adapt\footnote{That is replace the entropy of the transition probability by an arbitrary reward function.} our algorithms for MTEE to best policy identification in regularized MDPs \citep{neu2017unified, geist2019theory}. Especially, we consider the same entropy-regularized MDPs and associated Bellman equations as in \SoftQlearning \citep{fox2016taming,schulman2017equivalence,haarnoja2017reinforcement} or \SAC\citep{haarnoja2018soft}, see Remark~\ref{rem:regularized_mdp}.
We show that a variation of \RFExploreEnt has a sample complexity of order $\tcO(\poly(S,A,H)/(\epsilon\lambda))$ for BPI and reward-free exploration in regularized MDPs, where $\lambda$ is the regularization parameter. In particular, it exhibits a \emph{statistical separation between BPI in regularized MDP and BPI in the original MDP} since in this case the optimal sample complexity is of order  $\tcO(H^3SA/\epsilon^2)$ \citep{menard2021fast,domingues2021episodic}. Thus, our analysis shows that regularization is an effective way to trade-off bias for sample complexity. Additionally, we show how to use entropy regularization to obtain a theoretically faster version of \EntGame algorithm.



We highlight our main contributions:
%\vspace{-0.2cm}
\begin{itemize}[itemsep=-2pt,leftmargin=6pt]
    \item We propose the \EntGame algorithm for MVEE with a sample complexity of order $\tcO(H^4S^2A/\varepsilon^2)$ thus significantly improving the existing complexity bounds  for MVEE.
    \item We introduce the new MTEE setting for exploration and provide two algorithm: the \algMTEE algorithm for MTEE with a sample complexity of order $\tcO(H^3 S A/\varepsilon^2)$, and \RFExploreEnt with a sample complexity of order $\tcO(\poly(S,A,H)/\varepsilon)$. Up to our knowledge, this is the first time that a fast rate (in $1/\varepsilon$) is obtained thanks to regularization.
    \item We adapt \algMTEE and \RFExploreEnt to solve the entropy-regularized MDPs with a sample complexity of order $\tcO(H^3SA/\epsilon^2)$ and $\tcO(\poly(S,A,H)/(\lambda \varepsilon))$ correspondingly, where $\lambda$ is the regularization parameter. 
    \item We combine \EntGame algorithm with regularization techniques, resulting in a new algorithm \regalgMVEE. This algorithm improves a sample complexity of \EntGame to $\tcO(H^2SA/\varepsilon^2)$ and shows statistical separation of MVEE from reward-free exploration.
\end{itemize}



% \todoDa{Ideas on general structure, similar to \cite{menard2021fast}:}
% \begin{itemize}
%     \item Maximum entropy: two different formulations
%     \item Visitation entropy. Connection to a general convex MDP setting, refinement of meta-algorithm for $\tcO\left(\frac{H^3 S^2 A}{\varepsilon^2}\right)$ complexity for MaxEnt.
%     \item Trajectory entropy. Bellman equations. Connection to regularized MDPs, soft Q-learning thought the lens of new objective function. Two algorithms: regularization-agnostic $\tcO\left(\frac{H^3 S A}{\varepsilon^2} \right)$ and regularization-aware $\tcO \left(\frac{H^6 S^2 A}{\varepsilon} \right)$.
% \end{itemize}





\begin{table}[t]
\setlength{\tabcolsep}{4pt}
\centering
\label{tab:sample_complexity}
\begin{tabular}{@{}lcl}\toprule
\textbf{Algorithm} & \textbf{Setting} & \textbf{Sample complexity} \\
\midrule
{\scriptsize\MaxEnt}~{\tiny{\citep{hazan2019provably}}} & \multirow{5}{*}{MVEE} & {\scriptsize$\tcO(H^4S^2A/\epsilon^3\!+\!S^3/\epsilon^6)$} \\
{\scriptsize\TocUCRL}~{\tiny{\citep{cheung2019exploration}}} & &{\scriptsize$\tcO(H^4S^2A/\epsilon^2\!+\!H^3 S/\epsilon^3)$} \\
{\scriptsize\MetaEnt}~{\tiny{\citep{zahavy2021reward}}} &  & {\scriptsize$\tcO(H^4S^2A/(\delta^2\epsilon^2)\!+\!H^3 S/\epsilon^3)$} \\
 \rowcolor[gray]{.90} {\scriptsize\algMVEE}~{\tiny(this paper)} &  & {\scriptsize$\tcO(H^4S^2A/\epsilon^2\!+\!HSA/\epsilon)$} \\
 \rowcolor[gray]{.90} {\scriptsize\regalgMVEE}~{\tiny(this paper)} &  & {\scriptsize$\tcO(H^2SA/\epsilon^2\!+\!H^8S^4A/\epsilon)$} \\
\midrule 
 \rowcolor[gray]{.90} {\scriptsize\algMTEE}~{\tiny (this paper)} & \multirow{1}{*}{MTEE}  & {\scriptsize$\tcO(H^3SA/\epsilon^2 + H^3S^2A/\varepsilon)$} \\
 \rowcolor[gray]{.90} {\scriptsize\RFExploreEnt}~{\tiny (this paper)} & & {\scriptsize$\tcO(H^8S^4A/\epsilon)$} \\
\bottomrule
\end{tabular}
\caption{Sample complexities for MVEE and MTEE. We convert rates from the $\gamma$-discounted setting by replacing $1/(1-\gamma)$ with $H$ or from the infinite horizon setting by replacing the diameter with $H$. To take into account step-dependent transitions we multiply the first order term by $H^2$. {\scriptsize(For \MetaEnt since they do not precisely specify the cost for estimating a visitation distribution we use the same $1/\epsilon^3$ term as for \TocUCRL.)}}
% \todoPi{Is it just $1/\epsilon^3$ for the second order term for \citet{zahavy2021reward}?}
% \todoDa{They do not consider the complexity of density estimation and I guess that it should be like the second term from \citet{hazan2019provably}, but anyway it will be at least $1/\varepsilon^2$ per each step (and the number of steps is $1/\varepsilon$). We can leave remark like that. }}
%\todom{mention \url{https://arxiv.org/pdf/1905.06466.pdf} - perhaps add to this table too}
\end{table}
