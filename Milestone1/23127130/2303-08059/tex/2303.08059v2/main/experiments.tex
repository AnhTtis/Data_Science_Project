\vspace{-0.3cm}
\section{Experiments}
\label{sec:experiments}


\begin{figure}
    \centering
    \vspace{-8pt}
    
    \includegraphics[width=0.95\linewidth]{figures/occupancies_doublechain_crop.pdf}

    \vspace{-10pt}
    \caption{Number of state visits for $N=100000$ samples in Double Chain MDP with $S=31$ states, $A=2$ actions, a horizon $H=20$ and a $0.1$ probability of moving to the opposite direction.} \vspace{-10pt}
    \label{fig:double_chain_main}
\end{figure}

%\vspace{-60pt}

In this section we report experimental results on simple tabular MDP for presented algorithms and show the difference between visitation and trajectory entropies. In particular, we compare \EntGame and \UCBVIEnt algorithms with (a) random agent that takes all actions uniformly at random, (b) an optimal MVEE policy computed by solving the convex program, and (c) an optimal MTEE policy computed by solving the regularized Bellman equations. As an MDP, we choose a stochastic environment called Double Chain as considered by \citet{kaufmann2020adaptive}. 

Since the transition kernel for this environment is stage-homogeneous, for \EntGame and \UCBVIEnt algorithms we joint counters over the different steps $h$. In particular, it changes the objective of the \EntGame algorithm to the objective considered by \citet{hazan2019provably} that makes more sense in the stage-independent setting \footnote{See Remark~\ref{rem:hazan_entropy_vs_us}.}.

In Figure~\ref{fig:double_chain_main} we present the number of state visits for our algorithms and baselines during $N=100000$ interactions with the environment. For \UCBVIEnt algorithm the procedure was separated on two stages: at first we learn MDP with $N$-sample budget and extract the final policy, and then plot the number of visits for the final policy during another $N$ samples. 

In particular, we see that since this environment is almost deterministic the optimal MTEE policy is almost coincides with a random policy. Notably, the policy induced by \UCBVIEnt is more uniform over states because of $1/\sqrt{n^t(s,a)}$ bonuses, that make our algorithm close to \RFUCRL \cite{kaufmann2020adaptive}. Also we see that the optimal MVEE policy is the most uniform over states, that makes it an appropriate target for the pure exploration problem. For more details and additional experiments we refer to Appendix~\ref{app:experiments}.




% Empirical visitation distribution plot (as Fig 1.b and Fig 3.b by \citet{kaufmann2020adaptive}) for the algorithms 
% \begin{itemize}
% \item Random policy
% \item \RFUCRL by \citet{kaufmann2020adaptive}
% \item \algMVEE
% \item \algMTEE
% \end{itemize}
% in the environment 
% \begin{itemize}
%     \item doublechain
%     \item gridworld
% \end{itemize}