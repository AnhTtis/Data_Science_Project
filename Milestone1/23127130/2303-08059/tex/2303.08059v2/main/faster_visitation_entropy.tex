\vspace{-0.25cm}
\section{Faster Rates for Visitation Entropy}
\label{sec:faster_rates_mvee}


In this section, we show how to combine the regularization techniques developed in Section~\ref{sec:trajectory_entropy} with \EntGame algorithm presented in Section~\ref{sec:MVEE_game}.

The new algorithm \regalgMVEE is based on exactly the same game-theoretical framework as \EntGame, but uses a \textit{regularized sampler player} instead of the usual one.


\paragraph{Regularized sampler-player} For the sampler player, we shall take advantage of strong convexity of the visitation entropy. Beforehand, we construct an estimate of the model $\{ \hp_h \}_{h\in[H]}$ by reward-free exploration, using $HSN_0$ samples to compute a policy $\pi^{\mathrm{mix}}$ and $N$ samples to estimate transitions. Next, define the empirical regularized Bellman equations 
{\small
\begin{align*}
\begin{split}
\hQ_h^t(s,a) &=  \log\left(\frac{1}{\bd_h^{t+1}(s)}\right) + \hp_h \hV^t_{h+1}(s,a),\\
\hV_h^t(s) &= \max_{\pi\in\Delta_{\cA}}\{ \pi \hQ_h^t(s) + \cH(\pi) \},
\end{split}
\end{align*}\\}
where  $\hV_{H+1}^t = 0$. The sampler player then follows the distribution $d^{\pi^{t+1}}$ where $\pi^{t+1}$ is greedy with respect to the regularized empirical Q-values, that is, $\pi_h^{t+1}(s) \in\argmax_{\pi\in\Delta_A}\{ \pi\hQ_h^t(s) + \cH(\pi) \}$.


\begin{theorem}
\label{th:fast_MVEE_sample_complexity}
Fix $\epsilon > 0$ and $\delta\in(0,1)$. For $n_0=1,$ 
\begin{small}
\[
    N_0 = \Omega\left( \frac{H^7 S^3 A  \cdot L^3}{\varepsilon}\right),
    \quad
    N = \Omega\left( \frac{ H^6 S^3 A L^5 }{\varepsilon}\right), 
    \]
\end{small}
\!and \begin{small}
\[
    T = \Omega\left( \frac{H^3 S A L^3}{\varepsilon^2} + \frac{H^2 S^2 A^2 L^2}{\varepsilon}\right).
\]
\end{small}
\!with $L = \log(SAH/\delta\varepsilon)$ the algorithm \regalgMVEE is $(\epsilon,\delta)$-PAC. The total sample complexity is equal to $SH \cdot N_0 + N + T,$ that is,
\begin{small}
\[
    \tau = \tcO\left( \frac{H^2 SA}{\varepsilon^2} + \frac{H^8 S^4 A}{\varepsilon} \right).
\]
\end{small}
\end{theorem}

\vspace{-0.2cm}
\!Thus, the sample complexity of \regalgMVEE is of order $\tcO(H^2SA/\epsilon^2)$ for $\varepsilon$ large enough. In particular, this result significantly improves over  the previous rates for MVEE, see Table~\ref{tab:sample_complexity}. Moreover, this result shows a rate separation between reward-free exploration \citep{jin2020reward-free}, where the established lower bound on sample complexity $\Omega(H^3S^2A/\varepsilon^2)$ scales with $S^2$, and the visitation entropy maximization problem. 

\paragraph{Proof idea} The main proof idea is to exploit not just strong convexity of the visitation entropy with respect to Euclidean distance but its strong convexity \textit{with respect to trajectory entropy} \citep{bauschke2016descent}. It allows us to use entropy regularization as described in Section~\ref{sec:proof_mtee} for the sampler player resulting in an averaged regret less than $\varepsilon$ for only $\tcO(\poly(S,A,H)/\varepsilon)$ samples. Thus, the density estimation error becomes the leading term in the full error decomposition. For more details refer to Appendix~\ref{app:reg_visitation_entropy_proofs}.
