\section{Additional Experiments}\label{app:experiments}


In this section we provide details on experiments and additional experiments. The code for experiments could be found by the following link: \url{https://github.com/d-tiapkin/max-entropy-exploration}.

First, we describe our baselines in details.
\begin{itemize}
    \item Random policy $\pi_h(a|s) = 1/A$ for any $h\in[H], (s,a) \in \cS \times \cA$;
    \item Optimal MVEE policy. First we compute the solution to the following convex program
    \begin{align*}
        \max_{d \in \cK_p} \sum_{(s,a) \in \cS \times \cA} -\left( \frac{1}{H} \sum_{h=1}^H d_h(s,a) \right) \log \left(\frac{1}{H} \sum_{h=1}^H d_h(s,a) \right),
    \end{align*}
    where $\cK_p$ is a polytope of admissible visitation distributions defined in Section~\ref{sec:setting}. This problem exactly corresponds to the setting visitation entropy by \citet{hazan2019provably}. Then we compute the optimal MVEE policy by normalization of the visitation distribution $\pi_h(a|s) = \frac{d_h(s,a)}{\sum_{a\in\cA} d_h(s,a)}$.
    \item Optimal MTEE policy that was computed by solving regularized Bellman equations with $\lambda = 1$ and the entropy of transition kernels as rewards.
\end{itemize}
The last two baselines requires the knowledge of the true transition kernel $p_h(s'|s,a)$. Additionally, we perform experiments with \RFUCRL algorithm by \citet{kaufmann2020adaptive} but the final visitation numbers were very close to the visitation numbers of \EntGame algorithm and we do not report them. The data collection procedure for \EntGame and \UCBVIEnt is following.
\begin{itemize}
    \item For \EntGame algorithm we report visitation counts of states of the algorithm during the learning. It is an appropriate choice since this counts corresponds to the empirical density $\rd^T_h(s,a) = \frac{1}{T} \sum_{t=1}^T \ind\{ (s^t_h, a^t_h) = (s,a)\}$ which converges to $d^{\hpi}_h(s,a) = \frac{1}{T} \sum_{t=1}^T d^{\pi^t}_h(s,a)$.
    \item For \UCBVIEnt algorithm we have to separate stages. At the first stage the algorithm interacts with environment to learn the final policy during $N$ interactions with the environment, and all the plots present the visitations counts only for the final policy during another $N$ interactions when the policy is fixed.
\end{itemize}

\begin{remark}
    We do not compare \EntGame with \MaxEnt and \TocUCRL because of their similarity. The only difference in the these algorithms is the way how density estimation is performed. More precisely, in the \MaxEnt algorithm the average of state-action visitation distributions of all the policies played is estimated from scratch at the end of each epoch with additional trajectories. In \EntGame we also estimate this average but with all trajectories collected until now and without need of additional fresh trajectories. Thus, \EntGame can be seen as a version of \MaxEnt that reuses past trajectories instead of collecting new ones. The latter feature is helpful in practice but will not change the rate. That is why we decided not to include \MaxEnt in the experiments. Similar comments hold for \TocUCRL.
\end{remark}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.48\linewidth]{figures/occupancies_mvee.pdf}
    \includegraphics[width=0.48\linewidth]{figures/occupancies_mtee.pdf}
    \caption{Number of state visits for $N=100000$ samples in the Double Chain MDP for \EntGame and \UCBVIEnt algorithms with and without bonuses.}
    \label{fig:double_chain_no_bonus}
\end{figure}

\paragraph{Double Chain.} The experiment described in Section~\ref{sec:experiments} was preformed on Double Chain environment described by \citet{kaufmann2020adaptive}. This MDP consists of states $\cS = \{0, \ldots, L -1\}$, where $L$ is the length of the chain, the actions $\cA = \{l,r\}$ which corresponds to the transition to the left (action $l$) or to the right (action $r$). Additionally, while taking the actions, there is $10\%$ probability of moving to the opposite direction.  The agent starts at the middle of the chain $s_1 = (L-1)/2$. For experiments we run each algorithm to collect $N=100000$ samples during episodes of horizon $H=20$ and then report the average and confidence intervals over $48$ random seeds.



In Section~\ref{sec:experiments} we conclude that the exploration bonuses have the important role in the state visitations of \UCBVIEnt algorithm and make it close to \RFUCRL by \citet{kaufmann2020adaptive}. As an ablation study we preform the same experiments for algorithms without bonuses. The results are presented in Figure~\ref{fig:double_chain_no_bonus}.

In particular, we see that \EntGame algorithm without bonuses converges to the optimal MVEE policy in terms of the visitation densities that is slightly more spread than \EntGame algorithm with bonuses. Notice that algorithm has its own exploration mechanism since the rewards are equal to $\log((t+SA)/(n^t(s,a)+1))$ that is tightly connected to exploration bonuses. \UCBVIEnt algorithm also has its own exploration method that induced by soft-max policies. In particular, soft-max policies are tightly connected to the Boltzmann exploration with step-size equal to $1$ \cite{sutton1990integrated}. However, it is well-known \cite{cesabianchi2017boltzmann} that the Boltzmann exploration with fixed step-size did not provide efficient way to explore.


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.48\linewidth]{figures/occupancies_doublechain_resampling.pdf}
    \caption{Number of state visits for $N=50000$ samples in the Double Chain MDP with resampling for the \UCBVIEnt algorithm with and without bonuses.}
    \label{fig:double_chain_resampling}
\end{figure}

\paragraph{Double Chain with Resampling} To test the exploration mechanism of \UCBVIEnt without bonuses, we slightly modify Double Chain environment: now the visitation of the left end of the chain leads to uniform resampling over all states. The result is presented in Figure~\ref{fig:double_chain_resampling}. 

In particular, we observe that in this situation \UCBVIEnt algorithm without bonuses still acts like a random policy due to low exploration for the ends of the chain, whereas the optimal MTEE policy visits the left part of the chain more often. This observation imply that the additional exploration mechanism for \UCBVIEnt algorithm is required and the exploration problem in regularized MDPs is non-trivial.

\paragraph{GridWorld} As an additional experiment to verify our findings we perform additional experiments on the environment Grid World as it presented in Figure~\ref{fig:gridworld}. The state space is a set of discrete points in a $21 \times 21$ grid. For each state there are 4 possible actions: left, right, up or down, and for each action there is a $5\%$ probability to move to the wrong direction. The initial state $s_1$ is the middle of the grid. For this experiment we use $N=60000$ samples and report the average over $12$ random seeds.


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/occupancies_gridworld.pdf}
    \caption{Number of state visits for $N=60000$ samples in the GridWorld MDP for \EntGame and \UCBVIEnt algorithms with and without bonuses.}
    \label{fig:gridworld}
\end{figure}

Here we see the similar effect as on simpler environment: the \UCBVIEnt algorithm produces slightly less "spread" policy than \EntGame or \RFUCRL algorithms. It is connected to the fact that in this case the limiting MTEE policy of \UCBVIEnt is again almost uniform policy due to near-deterministic structure of the MDP. Additionally, we remark that in this case the optimal MVEE policy is much harder to compute due to numerical issues, however, the \EntGame algorithm without bonuses produces slightly more uniform distribution that coincides with the effect we observe during experiments with a Double Chain environment.