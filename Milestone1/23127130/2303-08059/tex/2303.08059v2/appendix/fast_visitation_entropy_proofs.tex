\section{Faster Rates for Visitation Entropy}
\label{app:reg_visitation_entropy_proofs}

\subsection{Algorithm description}
Let us start from the description of the modified algorithm \regalgMVEE. It has a similar game-theoretical foundation as it aims at solving the following minimax game
\begin{align*}
    \max_{d\in\cK_p} \VE(d) &= \max_{d\in\cK_p} \min_{\bd\in\cK}\sum_{(h,s,a)} d_h(s,a) \log \frac{1}{\bd_h(s,a)} =  \min_{\bd\in\cK} \max_{d\in\cK_p} \sum_{(h,s,a)} d_h(s,a) \log \frac{1}{\bd_h(s,a)}.
\end{align*}
As for usual \algMVEE, there are two players in the  game. On the one hand, the min player, or forecaster player, tries to predict which state-action pairs the max player will visit to minimize $\KL(d_h,\bd_h)$.  On the other hand, the max player, or sampler player, is rewarded for visiting state-action pairs that the forecaster player did not predict correctly.

We now describe the algorithm \regalgMVEE\ for MVEE. In this algorithm, we let a forecaster player and a sampler player compete for $T$ episodes long. Let us first define the two players.
\paragraph{Forecaster-player} The forecaster player remains exactly the same as for usual \algMVEE algorithm, see the corresponding section in the main text.

\paragraph{Regularized Sampler-player} For the sampler player we exploit strong convexity of visitation entropy. The running time of the sampler player will be divided onto two stages, as it was done in \RFExploreEnt algorithm.

\paragraph{Exploration phase}

Before the start of the game, the sampler-player uses some preprocessing time in order to explore the environment to learn a simple (non-markovian) preliminary exploration policy $\pi^{\mathrm{mix}}$. This policy is used to construct an accurate enough estimates of transition probabilities. This policy is obtained, as in \RFExplore and \RFExploreEnt, by learning for each state-action pair $(s,ah)$, a policy that reliably reaches this action pair $(s,a)$ at step $h$. This can be done by running any regret minimization algorithm for the sparse reward function putting reward one at state $s$ at step $h$ and zero otherwise. The policy $\pi^{\mathrm{mix}}$ is defined as the mixture of the aforementioned policies.

\paragraph{Planning phase}

The second phase is starting during the running time of the algorithm. Since \RFExploreEnt algorithm is essentially reward-free in a sense of working with an arbitrary reward functions.

For each episode $t$ during the game we define the empirical regularized Bellman equations 
\begin{align}
\begin{split}\label{eq:empirical_regularized_planning_VE}
\hQ_h^t(s,a) &=  \log\frac{1}{\bd_h^{t+1}(s)} + \hp_h^{\,t} \hV^t_{h+1}(s,a) \\
\hV_h^t(s) &= \max_{\pi \in \simplex_{\cA}}\{ \pi\hQ_h^t(s,a) + \cH(\pi) \},
\end{split}
\end{align}
where  $\hV_{H+1}^t = 0$. The sampler player then follows $d^{\pi^{t+1}}$ where $\pi^{t+1}$ is greedy with respect to the regularized Q-values, that is, $\pi_h^{t+1}(s) \in\argmax_{\pi\in\Delta_A} \{ \pi\hQ_h^t(s) + \cH(\pi) \}$. This choice of sampler player will be clear in the analysis below. 

\paragraph{Sampling rule} At each episode $t,$ the policy $\pi^t$ of the sampler-player is used as a sampling rule to generate a new trajectory.

\paragraph{Decision rule} After $T$ episodes we output a non-Markovian policy $\hpi$ defined as the mixture of the policies $\{\pi^t\}_{t\in[T]}$, that is, to obtain a trajectory from $\hpi$ we first sample uniformly at random $t\in[T]$ and then follow the policy $\pi^t$. Note that the visitation distribution of $\hpi$ is exactly the average $d^{\hpi} = (1/T)\sum_{t\in[T]} d^{\pi^t}$. 



Remark that the stopping rule of \regalgMVEE is deterministic and equals to $\tau = T$. The complete procedure is detailed in Algorithm~\ref{alg:regMVEE}.



\begin{algorithm}[h!]
\centering
\caption{\regalgMVEE}
\label{alg:regMVEE}
\begin{algorithmic}[1]
  \STATE {\bfseries Input:} Number of episodes $T$, number of exploration episodes $N_0$, number of transition samples $N$, prior counts $n_0$.
  \STATE \textcolor{blue}{\# Preliminary exploration}
  \FOR{$(s',h') \in \cS \times [H]$}
        \STATE Form rewards $r_h(s,a) = \ind\{ s=s', h=h'\}$.
        \STATE Run \EULER \citep{zanette2019tighter} with rewards $r_h$ over $N_0$ iterates and collect all policies $\Pi_{s',h'}$.
        \STATE Modify $\pi \in \Pi_{s',h'}:\ \pi_{h'}(a|s') = 1/A$ for all $a\in \cA$.
    \ENDFOR
    \STATE Construct a uniform mixture policy $\pi^{\mathrm{mix}}$ over all $\{ \Pi_{s,h} : (s,h) \in \cS \times [H] \}$.
    \STATE Sample $N$ independent trajectories $\{z_n\}_{n\in[N]}$ using  $\pi^{\mathrm{mix}}$ in the original MDP.
    \STATE Construct from $\{z_n\}_{n\in[N]}$ the estimates $\hp_h$ as in \eqref{eq:hp_construction}.
      \FOR{$t \in[T]$}
      \STATE \textcolor{blue}{\# Forecaster-player}
      \STATE Update pseudo counts $\bn_h^{t-1}(s,a)$ and predict $\bd_h^t(s,a)$. 
      \STATE \textcolor{blue}{\# Sampler-player}
      \STATE Compute $\pi^t$ by regularized planning \eqref{eq:empirical_regularized_planning_VE} with rewards $\log\big(1/ \bd_h^t(s)\big)$ and entropy regularization.
    \STATE \textcolor{blue}{\# Sampling}
      \FOR{$h \in [H]$}
        \STATE Play $a_h^t\sim \pi_h^t(s_h^t)$
        \STATE Observe $s_{h+1}^t\sim p_h(s_h^t,a_h^t)$
      \ENDFOR
    \STATE{ Update counts and transition estimates.}
   \ENDFOR
   \STATE Output $\hpi$ the uniform mixture of $\{\pi^t\}_{t\in[T]}$.
\end{algorithmic}
\end{algorithm}



\subsection{Analysis}


We first define the regrets of each players obtained by playing $T$ times the games. For the forecaster-player, for any $\bd\in\cK,$ we define 
\[
\regret_{\fore}^T(\bd) \triangleq \sum_{t=1}^T \sum_{h,s,a} \td_h^t(s,a) \left(\log\frac{1}{\bd_h^t(s,a)} -\log\frac{1}{\bd_h(s,a)}\right)
\]
where $\td_h^t(s,a) \triangleq \ind\big\{(s_h^t,a_h^t)=(s,a)\big\}$ is a sample from $d_h^{\pi^t}(s,a)$.
Similarly for the sampler-player, for any $d\in\cK_p,$ we define a \textit{regularized regret}
\begin{small}
\[
\regret_{\samp}^T(d)\triangleq \sum_{t=1}^T \left( \sum_{h,s,a} \big[ d_h(s,a) - d_h^{\pi^t}(s,a) \big] \log\frac{1}{\bd_h^t(s,a)} - \sum_{h,s} \left[ d_h(s) \KL(\pi(s), \bar{\pi}^t_h(s)) -  d^{\pi^t}_h(s) \KL(\pi^t_h(s), \bar{\pi}^t_h(s)) \right] \right)\,,
\]
\end{small}
\!where corresponding policies are defined as $\pi_h(a|s) = d_h(s,a) / d_h(s)$ and $\bar{\pi}^t_h(a|s) = \bd^t_h(s,a) / \bd^t_h(s)$ for $d_h(s) = \sum_{a} d_h(s,a)$ and $\bd^t_h(s) = \sum_{a} \bd^t_h(s,a)$.

Recall that the visitation distribution of the policy $\pi$ returned by \regalgMVEE is the average of the visitation distributions of the sampler-player 
$d_h^{\hpi}(s,a) = \hd^{\,T}_h(s,a) \triangleq (1/T) \sum_{t=1}^T d_h^{\pi^t}(s,a)$.  We also denote by $\rd^T_h(s,a)\triangleq (1/T) \sum_{t=1}^T \td^t(s,a)$ the average of the 'sample' visitation distributions.

We now relate the difference between the optimal visitation entropy and the visitation entropy of the outputted policy $\hpi$ to the regrets of the two players. 
Indeed, using $\cH(p) = \sum_{i\in[n]} p_i \log(1/q_i) -\KL(p,q)$ for all $(p,q)\in(\Delta_n)^2$ and
\begin{align*}
    \KL(d^{\pistar}_h, \bd_h^t) &= \sum_{s,a} d^{\pistar}_h(s,a) \log\left( \frac{d^{\pistar}_h(s,a)}{\bd_h^t(s,a)} \right) \\
    &= \sum_{s} d^{\pistar}_h(s) \log\left( \frac{d^{\pistar}_h(s)}{\bd_h^t(s)} \right) + \sum_{s} d^{\pistar}_h(s) \sum_{a} \pistar_h(a|s) \log\left( \frac{\pistar_h(a|s)}{\bar{\pi}_h^t(a|s)} \right) \\
    &\geq \sum_{s} d^{\pistar}_h(s) \KL(\pistar_h(s), \bar{\pi}^t_h(s))\,.
\end{align*}    
This inequality could be treated as a strong convexity of visitation entropy with respect to trajectory entropy since $\KL(d^{\pistar}_h, \bd^t_h)$ is a \textit{Bregman divergence} with respect to $\VE$, and the final average of $\KL(\pistar_h(s), \bpi^t_h(s))$ is a Bregman divergence with respect to $\TE$ (up to linearities).

Applying this inequality, we have
\begin{align*}
T\big(\VE(d^{\pistar}) -\VE(d^{\hpi})\big) &\leq \sum_{t=1}^T \left( \sum_{h,a,s} d_h^{\pistar}(s,a) \log\frac{1}{\bd_h^t(s,a)} - \sum_{h,s}  d_h^{\pistar}(s) \KL(\pistar_h(s), \bar{\pi}^t_h(s))  \right) \\
& \quad- \sum_{t=1}^T \td^t_h(s,a) \log\frac{1}{\rd_h^{\,T}(s,a)} + T\big(\VE(\rd^T) - \VE(\hd^T)\big)\\
& \leq \regret_{\samp}^T(d^{\pistar})+ \underbrace{\sum_{t=1}^T \sum_{h,s,a} \big(d_h^{\pi^t}(s,a) - \td_h^t(s,a) \big) \log\frac{1}{\bd_h^t(s,a)}}_{\mathrm{Bias}_1} \\
& \quad + \regret_{\fore}^T(\rd^T) + \underbrace{T\big(\VE(\rd^T) - \VE(\hd^T)\big)}_{\mathrm{Bias}_2}\,.
\end{align*}
It remains to upper bound each terms separately in order to obtain a bound on the gap. Notably, only the sampler player result changes in comparison to \algMVEE.


\subsection{Regret of the Sampler-Player}

We start from introducing new notation. Let $\cM_t = (\cS, \cA, \{ p_h \}_{h\in[H]}, \{r^t_h\}_{h\in[H]}, s_1)$ be a sequence of entropy-regularized MDPs where reward defined as $r^t_h(s,a) = \log(1/ \bd^t_h(s))$. Define $Q^{\pi, t}_h(s,a)$ and $V^{\pi, t}_h(s,a)$ as a action-value and value functions of a policy $\pi$ on a MDP $\cM_t$. Notice that the value-function of initial state in this case could be written as follows (see Appendix~\ref{app:reg_bellman_eq})
\begin{align*}
    V^{\pi,t}_1(s_1) &= \sum_{h,s,a} d^{\pi}_h(s,a) \log\left( \frac{1}{\bd^t_h(s,a)} \right) - \sum_{h,s} d^\pi_h(s) \KL(\pi_h(s), \bar{\pi}^t_h(s)) \\
    &= \sum_{h,s,a} d^\pi_h(s,a) \log\left( \frac{1}{\bd^t_h(s)} \right) + \sum_{h,s} d^\pi_h(s) \cH(\pi_h(s)),
\end{align*}
also see Appendix~\ref{app:regularized_mdp} for more exposition. Therefore, the regret for the sampler-player could be rewritten in the terms of the regret for this sequence of entropy-regularized MDPs
\[
    \regret_{\samp}^T(d^\pi) =  \sum_{t=1}^T V^{\pi,t}_1(s_1) - V^{\pi^t,t}_1(s_1).
\]

We notice that our approach does not gives a regret minimizer algorithm in a classical sense, however analysis shows us that we can control the sum of policy error with respect to \textit{any} reward function.

\begin{lemma}\label{lem:reg_regret_sampler}
    Let $N_0 = \Omega\left( \frac{H^7 S^3 A  \cdot \log^2(T+SA) \cdot L}{\varepsilon}\right)$ and $N = \Omega\left( \frac{ H^6 S^3 A \log^2(T+SA) L^3 }{\varepsilon}\right).$ Then with probability at least $1-\delta/2,$ the regret of the sampler player is bounded as
    \[
        \regret_{\samp}^T(d^{\pistar}) \leq \varepsilon/2 \cdot T
    \]
    after 
    \[
         \tcO\left( \frac{H^8 S^4 A}{\varepsilon} \right)
    \]
    episodes of pure exploration.
\end{lemma}
\begin{proof}
    From  Corollary~\ref{cor:rf_explore_ent_rf_sample_complexity} under the choice of parameters $\lambda = 1, \kappa = 0$ and reward function $r^t_h(s,a) = \log(1/\bd^t_h(s))$ for each iteration, that is bounded by $\log(T+SA)$, we have that for any reward function the sub-optimality gap is bounded by $\varepsilon/2$. The total number of episodes of pure exploration is equal to $N_0SH + N$.
\end{proof}





\subsection{Proof of Theorem~\ref{th:fast_MVEE_sample_complexity}}

We state the version of this theorem with all prescribed dependencies factors.
\begin{theorem}\label{th:fast_MVEE_sample_complexity_full}
Fix some $\epsilon > 0$ and $\delta\in(0,1)$. Then for $n_0=1,$ 
\[
    N_0 = \Omega\left( \frac{H^7 S^3 A  \cdot L^3}{\varepsilon}\right),
    \quad
    N = \Omega\left( \frac{ H^6 S^3 A L^5 }{\varepsilon}\right), 
    \quad
    T = \Omega\left( \frac{H^2 S A L^3}{\varepsilon^2} + \frac{H^2 S^2 A^2 L^2}{\varepsilon}\right)
    \]
with $L = \log(SAH/\delta\varepsilon),$ the algorithm \regalgMVEE is $(\epsilon,\delta)$-PAC. Its total sample complexity is equal to $SH \cdot N_0 + N + T,$ that is,
\[
    \tau = \tcO\left( \frac{H^2 SA}{\varepsilon^2} + \frac{H^8 S^4 A}{\varepsilon} \right).
\]
\end{theorem}
\begin{proof}
    We start from writing down the decomposition defined in the beginning of the appendix
    \[
        T(\VE(d^{\pistarVE}) - \VE(d^{\hpi})) \leq \regret_{\samp}^T(d^{\pistarVE}) + \regret_{\fore}^T(\rd^T) + \mathrm{Bias}_1 + \mathrm{Bias}_2.
    \]
    By Lemma~\ref{lem:reg_regret_sampler} with probability at least $1-\delta/2$ it holds
    \[
        \regret_{\samp}^T(d^{\pistarVE}) \leq \varepsilon T/2.
    \]
    By Lemma~\ref{lem:regret_forecaster} 
    \[
        \regret_{\fore}^T(\rd^T) \leq HSA\log\big(\rme(T+1)\big).
    \]
    By Lemma~\ref{lem:bias_terms} with probability at least $1-\delta/2$
    \[
         \mathrm{Bias}_1 + \mathrm{Bias}_2 \leq 3\log(SAT)\left(\sqrt{TH\log(4/\delta)} + H\sqrt{SAT\log(3T)} \right).
    \]
    By union bound all these inequalities hold simultaneously with probability at least $1-\delta$. Combining all these bounds we get
    \begin{align*}
        T(\VE(d^{\pistarVE}) - \VE(d^{\hpi})) &\leq  3\log(SAT)\left(\sqrt{TH\log(4/\delta)} + H\sqrt{SAT\log(3T)} \right) \\
        &+ HSA\log(\rme (T+1)) + \varepsilon T / 2.
    \end{align*}
    Therefore, it is enough to choose $T$ such that $\VE(d^{\pistarVE}) - \VE(d^{\hpi})$ is guaranteed to be less than $\varepsilon$. In this case \regalgMVEE become automatically $(\varepsilon,\delta)$-PAC. It is equivalent to find a maximal $T$ such that
    \begin{align*}
        \varepsilon T/2 &\leq 3\log(SAT)\left(\sqrt{TH\log(4/\delta)} + H\sqrt{SAT\log(3T)} \right) +  HSA \log(\rme (T+1))).
    \end{align*}
    and add $1$ to it. We start from obtaining a loose bound to eliminate logarithmic factors in $T$.
    
    First, we assume that $T \geq 1$, thus $T+1 \leq 2T$. Additionally, let us use inequality $\log(x) \leq x^{\beta}/\beta$ for any $x > 0$ and $\beta > 0$. We obtain
    \[
        \varepsilon T \leq 48(SAT)^{1/8}\left(\sqrt{T^{3/2} H\log(4/\delta)} + H\sqrt{4 SAT^{3/2}} \right) +  16/7 \cdot HSA (2\rme T)^{7/8}
    \]
    that could be relaxed as follows
    \[
         \varepsilon T^{1/8} \leq 48 (SA)^{1/8} (H \log(4/\delta)^{1/2} + H (SA)^{5/8} + 11 HSA
    \]
    thus we can define $\gamma = 8 \log\left( (48(SA)^{1/8} (H \log(4/\delta)^{1/2} + H (SA)^{5/8} + 11 HSA)/\varepsilon \right) = \cO(L)$ for which $\log(T) \leq \gamma$. Therefore
    \[ 
        \varepsilon T/2 \leq 3 (\log(SA) + \gamma) \sqrt{T} \left( \sqrt{H\log(4/\delta)} + \sqrt{SAH^2 (\log(3) + L)} \right) + HSA (1 + 2\gamma).
    \]
    Solving this quadratic inequality, we obtain the minimal required $T$ to guarantee $\VE(d^{\pistarVE}) - \VE(d^{\hpi}) \leq \varepsilon$. In particular,
    \[
        T = \Omega\left( \frac{H^2 S A L^3}{\varepsilon^2} + \frac{H^2 S^2 A^2 L^2}{\varepsilon}\right).
    \]
\end{proof}