\section{Proofs for Visitation Entropy}
\label{app:visitation_entropy_proofs}

We first define the regrets of each players obtained by playing $T$ times the games. For the forecaster-player, for any $\bd\in\cK$ we define 
\[
\regret_{\fore}^T(\bd) \triangleq \sum_{t=1}^T \sum_{h,s,a} \td_h^t(s,a) \left(\log\frac{1}{\bd_h^t(s,a)} -\log\frac{1}{\bd_h(s,a)}\right)
\]
where $\td_h^t(s,a) \triangleq \ind\big\{(s_h^t,a_h^t)=(s,a)\big\}$ is a sample from $d_h^{\pi^t}(s,a)$.
Similarly for the sampler-player, for any $d\in\cK_p$ we define 
\[
\regret_{\samp}^T(d)\triangleq \sum_{t=1}^T \sum_{h,s,a} \big(d_h(s,a) - d_h^{\pi^t}(s,a) \big) \log\frac{1}{\bd_h^t(s,a)}\,.
\]

Recall that the visitation distribution of the policy $\pi$ returned by \algMVEE is the average of the visitation distributions of the sampler-player 
$d_h^{\hpi}(s,a) = \hd^{\,T}_h(s,a) \triangleq (1/T) \sum_{t=1}^T d_h^{\pi^t}(s,a)$.  We also denote by $\rd^T_h(s,a)\triangleq (1/T) \sum_{t=1}^T \td^t(s,a)$ the average of the 'sample' visitation distributions.

We now relate the difference between the optimal visitation entropy and the visitation entropy of the outputted policy $\hpi$ with the regrets of the two players. Indeed, using $\cH(p) = \sum_{i\in[n]} p_i \log(1/q_i) -\KL(p,q) \leq  \sum_{i\in[n]} p_i \log(1/q_i)$ for all $(p,q)\in(\Delta_n)^2$, we obtain 
\begin{align*}
T\big(\VE(d^{\pistar}) -\VE(d^{\hpi})\big) &\leq \sum_{t=1}^T \sum_{h,a,s} d_h^{\pistar}(s,a) \log\frac{1}{\bd_h^t(s,a)}  - \td^t_h(s,a) \log\frac{1}{\rd_h^{\,T}(s,a)} + T\big(\VE(\rd^T) - \VE(\hd^T)\big)\\
& = \regret_{\samp}^T(d^{\pistar})+ \underbrace{\sum_{t=1}^T \sum_{h,s,a} \big(d_h^{\pi^t}(s,a) - \td_h^t(s,a) \big) \log\frac{1}{\bd_h^t(s,a)}}_{\mathrm{Bias}_1} + \regret_{\fore}^T(\rd^T) \\
&\quad+ \underbrace{T\big(\VE(\rd^T) - \VE(\hd^T)\big)}_{\mathrm{Bias}_2}\,.
\end{align*}
It remains to upper bound each terms separately in order to obtain a bound on the gap. We first bound the two regrets terms. The first bias term is  martingale and can easily be bounded with a deviation inequality, whereas for the second one we introduce just instrumentally smoothing of the entropy.

\subsection{Regret of the Forecaster-Player}

We prove in this section a regret-bound for the mixture forecaster.
\begin{lemma}
\label{lem:regret_forecaster}
For $n_0=1$, for any $\bd\in\cK$ it holds almost surely 
\[
\regret_{\fore}^T(\bd) \leq  HSA\log\big(\rme(T+1)\big) - T\sum_{h=1}^H\KL(\rd_h^T,\bd_h)\,.
\]
\end{lemma} 
\begin{proof}
We will bound the regret at step $h$,
\[
\regret^T_{\fore,h}(\bd) \triangleq \sum_{t=1}^T \sum_{s,a} \td_h^t(s,a) \left(\log\frac{1}{\bd_h^t(s,a)} -\log\frac{1}{\bd_h(s,a)}\right)
\]
and then sum the upper bounds. Recall 
\[
    \bd^t_h(s,a) = \frac{n^{t-1}_h(s,a) + 1}{t-1 + SA},
\]
and for $(s,a) = (s^t_h, a^t_h)$ and any $t\in[T], h \in [H]$ we have $n^{t-1}_h(s,a) + 1 = n^t_h(s,a)$. Since $n_0 = 1$, we have $\bd^t_h(s^t_h,a^t_h) = n^t_h(s^t_h, a^t_h) / (SA + t-1)$. Armed with this observation we can rewrite the regret as follows 
\begin{align*}
    \regret^T_{\fore,h}(\bd) &= -T\KL(\rd_h^T,\bd_h) - T\cH(\rd_h^T) -\sum_{t=1}^T \log\big( \bd_h^t(s_h^t,a_h^t)\big)\\
    &= -T\KL(\rd_h^T,\bd_h) - T\cH(\rd_h^T) -\log\left( \prod_{t=1}^T \bd_h^t(s_h^t,a_h^t)\right).
\end{align*}
Then we have an explicit formula for the product of $\bd^t_h$
\begin{align*}
    \prod_{t=1}^T \bd^t_h(s^t_h, a^t_h) &= \prod_{t=1}^T \frac{n^t_h(s^t_h, a^t_h)}{SA + t-1} = \frac{(SA-1)!}{(SA+T-1)!} \prod_{(s,a)\in \cS \times \cA}  [n^T_h(s, a)]! \\
    &= \frac{1}{\binom{T}{(n_h^T(s,a))_{(s,a)\in\cS\times\cA}}}\frac{1}{\binom{T+SA-1}{SA-1}}\\
    &\geq \exp\left(-T\cH(\rd_h^T) - (T+SA-1)\cH\left(\frac{SA-1}{T+SA-1}\right)\right)
\end{align*}
where in the last inequality we used Theorem 11.1.3 by \citet{cover2006elements} and overload the entropy notation $\cH(p) =- p\log(p) - (1-p)\log(1-p)$ for $p\in[0,1]$.
 Putting all together we get
\[
\regret^T_{\fore,h}(\bd) \leq (T+SA-1)\cH\left(\frac{A-1}{T+A-1}\right)-T\KL(\rd_h^T,\bd_h)\,.
\]
Bounding the entropic term
\begin{align*}
(T+SA-1)\cH\left(\frac{SA-1}{T+SA-1}\right) &= (SA-1)\log\frac{T+SA-1}{SA-1}+T\log\frac{T+SA-1}{T}\\
&\leq(SA-1)\log\frac{T+SA-1}{SA-1}+T\log\left(1+\frac{SA-1}{T}\right)\\
&\leq (SA-1)\log\frac{\rme(T+SA-1)}{SA-1}\\
&\leq SA\log\big(\rme(T+1)\big)\,,
\end{align*}
and summing over $h$ allows us to conclude.
\end{proof}

\subsection{Regret of the Sampler-Player}

We start from introducing new notation. Let $\cM_t = (\cS, \cA, \{ p_h \}_{h\in[H]}, \{r^t_h\}_{h\in[H]}, s_1)$ be a sequence of MDPs where reward defined as follows $r^t_h(s,a) = \log(1/ \bd^t_h(s,a))$. Define $Q^{\pi, t}_h(s,a)$ and $V^{\pi, t}_h(s,a)$ as a action-value and value functions of a policy $\pi$ on a MDP $\cM_t$. Notice that the value-function of initial state in this case could be written as follows
\[
    V^{\pi,t}_1(s_1) = \sum_{h,s,a} d^{\pi}_h(s,a) \log\left( \frac{1}{\bd^t_h(s,a)} \right)
\]
therefore, the regret for the sampler-player could be rewritten in the terms of the regret for this sequence of MDPs
\[
    \regret_{\samp}^T(d^\pi) =  \sum_{t=1}^T V^{\pi,t}_1(s_1) - V^{\pi^t,t}_1(s_1).
\]
Since the rewards are changing in each episode and depending on the full history on interaction during previous episodes, we have to handle more uniform approach as in usual \UCBVI proofs \cite{azar2017minimax}.


\paragraph{Concentration}

Let $\alpha^{\KL}, \alpha^{\cnt}: (0,1) \times \R_{+} \to \R_{+}$ be some functions defined later on in Lemma \ref{lem:sampler_proba_master_event}. We define the following favorable events
\begin{align*}
\cE^{\KL}(\delta) &\triangleq \Bigg\{ \forall t \in \N, \forall h \in [H], \forall (s,a) \in \cS\times\cA: \quad \KL(\hp^{\,t}_h(s,a), p_h(s,a)) \leq \frac{\alpha^{\KL}(\delta, n^{\,t}_h(s,a))}{n^{\,t}_h(s,a)} \Bigg\},\\
\cE^{\cnt}(\delta) &\triangleq \Bigg\{ \forall t \in \N, \forall h \in [H], \forall (s,a) \in \cS\times\cA: \quad n^t_h(s,a) \geq \frac{1}{2} \upn^t_h(s,a) - \alpha^{\cnt}(\delta) \Bigg\},
\end{align*}
\begin{lemma}\label{lem:sampler_proba_master_event}
For any $\delta \in (0,1)$ and for the following choices of functions $\alpha,$
\begin{align*}
    \alpha^{\KL}(\delta, n)  \triangleq \log(2SAH/\delta) + S\log\left(\rme(1+n) \right),  \quad
    \alpha^{\cnt}(\delta) \triangleq \log(2SAH/\delta), 
\end{align*}
it holds that
\begin{align*}
\P[\cE^{\KL}(\delta)] \geq 1-\delta/2, \qquad \P[\cE^\cnt(\delta)]\geq 1-\delta/2
\end{align*}
In particular, $\P[\cG(\delta)] \geq 1-\delta$.
\end{lemma}
\begin{proof}
Applying Theorem~\ref{th:max_ineq_categorical} and the union bound over $h \in [H], (s,a) \in \cS \times \cA$ we get $\P[\cE^{\KL}(\delta)]\geq 1-\delta/2$.  By Theorem~\ref{th:bernoulli-deviation} and union bound,  $\P[\cE^{\cnt}(\delta)]\geq 1 - \delta/2$. The union bound yields $\P[\cG(\delta)] \geq 1- \delta$.
\end{proof}

\paragraph{Optimism}
Next we define the exploration bonuses $b^t_h(s,a)$ for the sampler-player for $n_0 = 1$
\begin{equation}\label{eq:sampler_exploration_bonus}
    b^t_h(s,a) = \sqrt{\frac{2 H^2 \log^2(t+SA) \cdot \alpha^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)}}
\end{equation}


\begin{lemma}\label{lem:sampler_optimism}
    For any $t \in [T]$ and any policy $\pi$, the following holds on event $\cG(\delta)$
    \[
        \uQ^t_h(s,a) \geq Q^{\pi, t+1}_h(s,a), \qquad \uV^t_h(s) \geq V^{\pi, t+1}_h(s).
    \]
\end{lemma}
\begin{proof}
    Proceed by backward induction over $h$. For $h = H+1$ the statement trivially holds. Next we assume that the statement holds for any $h' > h$. Then we have by induction hypothesis and HÃ¶lder's inequality
    \begin{align*}
        \uQ^t_h(s,a) - Q^{\pi,t+1}_h(s,a) &= \hp^t_h \uV^t_{h+1}(s,a) - p_h V^{\pi,t+1}_h(s,a) + b^t_h(s,a)\\
        & \geq [\hp^t_h - p_h] V^{\pi,t+1}_h(s,a) + b^t_h(s,a) \geq - \norm{V^{\pi,t+1}_h}_\infty \norm{\hp^t_h - p_h}_1 + b^t_h(s,a).
    \end{align*}
    The fact that $\norm{V^{\pi,t+1}_h}_\infty \leq H \log(t+SA)$, Pinsker's inequality and the definition of the event $\cE^{\KL}(\delta)$ yields
    \[
        \norm{V^{\pi,t+1}_h}_\infty \norm{\hp^t_h - p_h}_1 \leq H\log(t+SA) \sqrt{\frac{2\alpha^{\KL}(\delta,n^t_h(s,a))}{n^t_h(s,a)}}  = b^t_h(s,a)
    \]
    that shows $\uQ^t_h(s,a) - Q^{\pi,t+1}_h(s,a) \geq 0$. The inequality on $V$-functions could be derived as follows
    \[
        \uV^t_h(s) \geq \pi \uQ^t_h(s) \geq \pi Q^{\pi,t+1}_h(s) = V^{\pi,t+1}_h(s).
    \]
\end{proof}


\paragraph{Regret Bound}

\begin{lemma}\label{lem:regret_sampler}
    Let $\pi$ be any fixed policy. Then for any $\delta \in(0,1)$ with probability at least $1-\delta$ the following holds
    \[
        \regret_{\samp}^T(d^\pi)  \leq 10\log(T+SA)\sqrt{2H^4 S A T \cdot \left( \log(2SAH/\delta) + S\log(\rme T)\right)\log(T)}.
    \]
\end{lemma}
\begin{proof}
    Assume that the event $\cG(\delta)$ holds.   By Lemma~\ref{lem:sampler_optimism} for any $t \in [T]$ and $h\in[H]$ we have
    \[
        V^{\pi, t}_t(s_h) - V^{\pi^t, t}_h(s^t_h) \leq \uV^{t-1}_h(s^t_h) - V^{\pi^t, t}_h(s^t_h) = \pi^t_h(\uQ^{t-1}_h - Q^{\pi^t, t}_h)(s),
    \]
    thus we can define $\delta^t_h(s,a) = \uQ^{t-1}_h(s,a) - Q^{\pi^t, t}_h(s,a)$ and upper bound the regret as follows
    \[
        \regret_{\samp}^T(d^\pi) \leq \sum_{t=1}^T \pi^t_1 \delta^t_1(s_1).
    \]

    Next we analyze $\delta^t_h(s^t_h)$. By the same argument as in Lemma~\ref{lem:sampler_optimism}
    \[
        \delta^t_h(s,a) = [\hp^{t-1}_h - p_h] \uV^{t-1}_{h+1}(s,a) + b^t_h(s,a) + p_h[\uV^{t-1}_{h+1} - V^{\pi^t, t}_{h+1}](s,a) \leq 2b^{t-1}_h(s,a) + p_h\pi^{t}_{h+1} [\uQ^{t-1}_{h+1} - Q^{\pi^t, t}_{h+1}](s,a)
    \]
    that could be rewritten as follows
    \[
        \delta^t_h(s,a) \leq \E_{\pi^t}\left[ 2b^{t-1}_h(s,a) + \delta^t_{h+1}(s_{h+1}, a_{h+1}) | (s_h, a_h) = (s,a)\right],
    \]
    thus, rolling out the initial bound on regret we have
    \[
        \regret_{\samp}^T(d^\pi) \leq H\log(T+SA) \sum_{t=1}^{T-1} \E_{\pi^t}\left[ \sum_{h=1}^H 2\sqrt{\frac{2\alpha^{\KL}(\delta, n^t_h(s_h, a_h))}{n^t_h(s_h,a_h)} \wedge 1} \bigg| s_1 \right] + H\log(T+SA).
    \]
    By Lemma~\ref{lem:cnt_pseudo} and Jensen's inequality we have
    \[
        \regret_{\samp}^T(d^\pi) \leq 5 H^{3/2}\log(T+SA) \sqrt{2T} \sqrt{ \sum_{h,s,a} \sum_{t=1}^{T-1} d^{\pi^t}_h(s,a)  \frac{\alpha^{\KL}(\delta, \upn^t_h(s, a)) }{\upn^t_h(s,a) \vee 1}}.
    \]
    Notice that $d^{\pi^t}_h(s,a) = \upn^{t+1}_h(s,a) - \upn^t_h(s,a)$ and $\alpha^{\KL}(\delta, \upn^t_h(s,a)) \leq \alpha^{\KL}(\delta, T-1)$. Combined with Lemma~\ref{lem:sum_1_over_n} it implies
    \[
        \regret_{\samp}^T(d^\pi) \leq 10\log(T+SA)\sqrt{2H^4 S A T \cdot \left( \log(2SAH/\delta) + S\log(\rme T)\right)\log(T)}.
    \]

    Finally, the fact that $\P[\cG(\delta)] \geq 1 -\delta$ concludes the statement of theorem.
\end{proof}
\begin{remark}
    It is possible to improve the $H$-dependence by introducing Bernstein-type bonuses, however, we are focused on improvement in a dependence in $\varepsilon^{-1}$ and leave this regret bound as simple as possible.
\end{remark}


\subsection{Bias Terms}

\begin{lemma}\label{lem:bias_terms}
    Let $\delta \in (0,1)$ and $n_0 = 1$. Then with probability at least $1-\delta$ the following two bounds hold
    \begin{align*}
        \mathrm{Bias}_1 &\triangleq \sum_{t=1}^T \sum_{h,s,a} \big(d_h^{\pi^t}(s,a) - \td_h^t(s,a) \big) \log\frac{1}{\bd_h^t(s,a)} \leq  \log(T+SA) \sqrt{2TH \log(2/\delta)} \\
        \mathrm{Bias}_2 &\triangleq T(\VE(\rd^T) - \VE(\hd^T)) \leq \log(SAT)\left(\sqrt{2TH\log(2/\delta)} + 3H\sqrt{SAT\log(3T)} \right).
    \end{align*}
\end{lemma}
\begin{proof}
    Let us define the lexicographic order on the set $[T] \times [H]$ with an additional convention $(t,0) = (t-1, H)$.
    
    Then we can define a filtration $\cF_{t,h} = \sigma\left\{ (s^{t'}_{h'}, a^{t'}_{h'}) \ \forall t \leq t,  \forall h' \in [H] \} \cup \{ (s^t_{h'}, a^t_{h'})\  \forall h' \leq h \} \right\}$ that consists of the all history of interactions of the \algMVEE algorithm with an environment up to the $h$-th step of the episode $t$. The most important fact is that $\pi^t$ and $\bd^t_h(s,a)$ are $\cF_{t,h-1}$-measurable for $h > 1$ and $\cF_{t-1,H}$-measurable for $h=1$. 
    
    Therefore,  for any $t \in [T], h \in [H]$
    \[
        \E\left[ \sum_{s,a} (d_h^{\pi^t}(s,a) - \td_h^t(s,a)) \log \frac{1}{\bd_h^t(s,a)} \bigg| \cF_{t,h-1} \right] = 0.
    \]
    Therefore $X_{t,h} = \sum_{s,a} (d_h^{\pi^t}(s,a) - \td_h^t(s,a)) \log \frac{1}{\bd_h^t(s,a)}$ is a martingale-difference sequence adapted to the filtration $\cF_{t,h}$. Also we notice that a.s. the following bound holds
    \[
        \vert X_{t,h} \vert \leq \log(T+SA)
    \]

    All these facts combined with Azuma-Hoeffding inequality implies that with probability at least $1-\delta/2$
    \[
        \mathrm{Bias}_1 = \sum_{t=1}^T \sum_{h=1}^H X_{t,h} \leq \log(T+SA) \sqrt{2TH \log(2/\delta)}.
    \]

    To show the second part of the statement we notice that
    \[
        \mathrm{Bias}_2 = T \sum_{h=1}^H (\cH(\rd^T_h) - \cH(\hd^T_h)).
    \]
    Let us introduce the smoothed entropy as it was done by \citet{hazan2019provably}. 
    \[
        \forall d \in \simplex_{SA}: \cH_\sigma(d) = \sum_{s,a} d(s,a) \log \frac{1}{d(s,a) + \sigma}.
    \]
    The key difference with our approach and approach of \citet{hazan2019provably} that we need the smoothing only instrumentally to provide a bound on $\mathrm{Bias}_2$.
    
    It is easy to see that $\cH_\sigma$ is concave and, moreover $d \in \simplex_{SA}$
    \[
        \vert \cH(d) - \cH_\sigma(d)  \vert \leq \sum_{s,a} d(s,a) \log \frac{d(s,a) +\sigma}{d(s,a) }  \leq \sigma SA,
    \]
    where we used inequality $\log(1+x) \leq x$ for all $x \geq 0$, and also for $\sigma < \rme^{-1}$
    \[
        \norm{ \nabla \cH_\sigma(d) }_\infty = \sup_{x \in (0,1)} \left\vert \log(x + \sigma) + \frac{x}{x + \sigma} \right\vert \leq \log(\sigma^{-1}).
    \]
    By replacing an entropy with a smoothed entropy
    \[
        \mathrm{Bias}_2 \leq T\sum_{h\in H} (\cH_\sigma(\rd^T_h) - \cH_\sigma(\hd^T_h)) + 2\sigma \cdot TSAH.
    \]
    To analyze the first term we use that $\cH_\sigma$ is concave, therefore
    \[
        \sum_{h=1}^H \cH_\sigma(\rd^T_h) - \cH_\sigma(\hd^T_h) \leq \sum_{h=1}^H \langle \nabla \cH_\sigma(\hd^T_h), \rd^T_h - \hd^T_h \rangle = \frac{1}{T} \sum_{s,a}\sum_{t=1}^T \sum_{h=1}^H (\td_h^t(s,a) - d_h^{\pi^t}(s,a)) \cdot \nabla \cH_\sigma(\hd^T_h)(s,a)
    \]

     For this term situation is more involved than for $\mathrm{Bias}_1$ because $\hd^T_h$ is dependent on all generated policies. Therefore we have to preform uniform bounds. Define $\cW = \{ w \in \R^{HSA} \mid \vert w_h(s,a) \vert \leq 1\} $ as a unit $\ell_\infty$-ball in $\R^{HSA}$. Then we have
     \[
        T\sum_{h=1}^H \cH_\sigma(\rd^T_h) - \cH_\sigma(\hd^T_h) \leq \log(\sigma^{-1}) \cdot \sup_{w \in \cW } \sum_{t=1}^T \sum_{h=1}^H \left( \sum_{s,a} (\td_h^t(s,a) - d_h^{\pi^t}(s,a)) \cdot w_h(s,a) \right).
     \]
     
     Define $N(\varepsilon, \norm{\cdot}_\infty, \cW)$ as $\epsilon$-covering number for a set $\cW$ with $\ell_\infty$-norm as a distance, and $\cW_\varepsilon$ as a minimal $\varepsilon$-net. Next we can use the well-known result on upper bound on the covering number (e.g. see Exercise 5.5 by \citet{van2014probability})
    \[
        N(\epsilon, \norm{\cdot}_\infty, \cW) \leq \left(\frac{3}{\varepsilon}\right)^{SAH},
    \]
    and replace our maximization problem with the maximization over $\varepsilon$-net
    \[
        \sup_{w \in \cW } \sum_{t=1}^T \sum_{h=1}^H \left( \sum_{s,a} (\td_h^t(s,a) - d_h^{\pi^t}(s,a)) \cdot w_h(s,a) \right) \leq \sup_{\hat w \in \cW_\varepsilon } \sum_{t=1}^T \sum_{h=1}^H \left( \sum_{s,a} (\td_h^t(s,a) - d_h^{\pi^t}(s,a)) \cdot \hat w_h(s,a) \right) + \varepsilon TH.
    \]
    For any fixed $\hat w \in \cW_{\varepsilon}$ we apply Azuma-Hoeffding inequality exactly in the same manner as in the bound for $\mathrm{Bias}_1$-term. We have that with probability at least $1-\delta/(2N_\varepsilon)$ for $N = N(\varepsilon, \norm{\cdot}_\infty, \cW)$ we have
    \[
        \sum_{t=1}^T \sum_{h=1}^H \left( \sum_{s,a} (\td_h^t(s,a) - d_h^{\pi^t}(s,a)) \cdot \hat w_h(s,a) \right) \leq \sqrt{2TH\log(2 N_{\varepsilon} / \delta)}.
    \]
    Thus, by union bound we have with probability at least $1-\delta/2$
    \[
         \sup_{w \in \cW } \sum_{t=1}^T \sum_{h=1}^H \left( \sum_{s,a} (\td_h^t(s,a) - d_h^{\pi^t}(s,a)) \cdot w_h(s,a) \right) \leq \sqrt{2TH(\log(2 / \delta) + SAH\log(3/\varepsilon)) } + \varepsilon TH.
    \]
    Taking $\varepsilon = 1/T$ we have
    \[
        \mathrm{Bias}_2 \leq \log(\sigma^{-1})(\sqrt{2TH(\log(2 / \delta) + SAH\log(3T)) } + H) + \sigma SATH.
    \]
    Next we choose $\sigma = 1/SAT$ and by inequality $\sqrt{a+b} \leq \sqrt{a} +\sqrt{b}$ obtain
    \[
        \mathrm{Bias}_2 \leq \log(SAT)\left(\sqrt{2TH\log(2/\delta)} + 3H\sqrt{SAT\log(3T)} \right).
    \]
\end{proof}

\subsection{Proof of Theorem~\ref{th:MVEE_sample_complexity}}

We state the version of this theorem with all prescribed dependencies factors.
\begin{theorem}\label{th:MVEE_sample_complexity_full}
For all $\epsilon > 0$ and $\delta\in(0,1)$. For $n_0=1$ and 
\[
T \geq 1 + \frac{648 (\log(SA) + L)H^4 SA \cdot (\log(4SAH/\delta) + S + L) \cdot L}{\varepsilon^2} + \frac{2 HSA(2+L)}{\varepsilon}
\]
for $L = 9 \log\left( 1010 \sqrt{H^4 S^{8/3} A^{8/3} \log(4SAH/\delta)} / \varepsilon \right)$ the algorithm \algMVEE is $(\epsilon,\delta)$-PAC.
\end{theorem}
\begin{proof}
    We start from writing down the decomposition defined in the beginning of the appendix
    \[
        T(\VE(d^{\pistarVE}) - \VE(d^{\hpi})) \leq \regret_{\samp}^T(d^{\pistarVE}) + \regret_{\fore}^T(\rd^T) + \mathrm{Bias}_1 + \mathrm{Bias}_2.
    \]
    By Lemma~\ref{lem:regret_sampler} with probability at least $1-\delta/2$ it holds
    \[
        \regret_{\samp}^T(d^{\pistarVE}) = 10\log(T+SA)\sqrt{2H^4 S A T \cdot \left( \log(4SAH/\delta) + S\log(\rme T)\right)\log(T)}
    \]
    By Lemma~\ref{lem:regret_forecaster} 
    \[
        \regret_{\fore}^T(\rd^T) \leq HSA\log\big(\rme(T+1)\big).
    \]
    By Lemma~\ref{lem:bias_terms} with probability at least $1-\delta/2$
    \[
         \mathrm{Bias}_1 + \mathrm{Bias}_2 \leq 3\log(SAT)\left(\sqrt{TH\log(4/\delta)} + H\sqrt{SAT\log(3T)} \right).
    \]
    By union bound all these inequalities hold simultaneously with probability at least $1-\delta$. Combining all these bounds we get
    \[
        T(\VE(d^{\pistarVE}) - \VE(d^{\hpi})) \leq 18 \log(SAT) \sqrt{ H^4 SAT (\log(4SAH/\delta) + S \log(\rme T)) \log(T) } + HSA\log(\rme (T+1)).
    \]
    Therefore, it is enough to choose $T$ such that $\VE(d^{\pistarVE}) - \VE(d^{\hpi})$ is guaranteed to be less than $\varepsilon$. In this case \algMVEE become automatically $(\varepsilon,\delta)$-PAC.
    
    It is equivalent to find a maximal $T$ such that
    \[
        \varepsilon T \leq 18 \log(SAT) \sqrt{ H^4 SAT (\log(4SAH/\delta) + S \log(\rme T)) \log(T) } + HSA\log(\rme (T+1))
    \]
    and add $1$ to it.

    
    We start from obtaining a loose bound to eliminate logarithmic factors in $T$.
    
    First, we assume that $T \geq 1$, thus $T+1 \leq 2T$. Additionally, let us use inequality $\log(x) \leq x^{\beta}/\beta$ for any $x > 0$ and $\beta > 0$. We obtain
    \begin{align*}
        \varepsilon T &\leq 18 \frac{(SAT)^{1/3}}{1/3} \sqrt{ H^4 S^2 AT \log(4SAH/\delta) \frac{(\rme T)^{1/18}}{1/18} \frac{T^{1/18}}{1/18} } + HSA \frac{(2\rme T)^{8/9}}{8/9}\\
        &\leq T^{8/9} \left( 1010 \sqrt{H^4 S^2 A^2 \log(2SAH/\delta)} \right),
    \end{align*}
    thus we can define $L = 9 \log\left( 1010 \sqrt{H^4 S^{8/3} A^{8/3} \log(4SAH/\delta)} / \varepsilon \right)$ for which $\log(T) \leq L$. Thus we have
    \[
        \varepsilon T \leq 18 (\log(SA) + L) \sqrt{H^4 SAT(\log(4SAH/\delta) + S + L) L} + HSA(2+ L).
    \]
    Solving this quadratic inequality, we obtain the minimal required $T$ to guarantee $\VE(d^{\pistarVE}) - \VE(d^{\hpi}) \leq \varepsilon$. In particular,
    \[
        T \geq 1 + \frac{648 (\log(SA) + L)H^4 SA \cdot (\log(4SAH/\delta) +S + L) \cdot L}{\varepsilon^2} + \frac{2 HSA(2+L)}{\varepsilon}.
    \]
\end{proof}