%!TEX root = ../BayesUCBVI.tex
\section{Deviation Inequalities}
\label{app:deviation_ineq}
% We define the following favorable events: $\cE^\star$ where the empirical means of the optimal value functions are close to the true ones, $\cE^\cnt$ the event where the pseudo-counts are close to their expectation, and $\cE^\bias$ the event where we control the deviation of the martingale of the bias of the upper bounds on the optimal value function,
% \begin{align*}
%   \cE^\star &\triangleq \Bigg\{\forall t \in \N, \forall h \in [H], \forall (s,a)\in\cS\times\cA:
%     \Kinf(\hp_h^t(s,a),p_h \Vstar_{h+1}(s,a), \Vstar_{h+1}) \leq  \frac{\beta(\delta,n_h^t(s,a))}{n_h^t(s,a)}\Bigg\}\,,\\
%  \cE^{\cnt} &\triangleq  \left\{ \forall t \in \N, \forall h\in [H],\forall (s ,a)\in\cS\times\cA:\ n_h^t(s,a) \geq \frac{1}{2}\bar n_h^t(s,a)-\beta^{\cnt}(\delta)  \right\}\,,\\
%  \cE^{\bias} &\triangleq \Bigg\{\forall t \in \N, \forall h \in [H], \forall (s,a)\in\cS\times\cA:\\
% &\qquad\sum_{k=1}^t \ind_{\{(s_h^t,a_h^t)=(s,a)\}}(\tp_h^k -p_h)( \uV_{h+1}^{k-1}-\Vstar_{h+1})(s,a) \leq\\
% &\qquad\qquad\sqrt{2 \beta(\delta,n_h^t(s,a))\sum_{k=1}^t \ind_{\{(s_h^t,a_h^t)=(s,a)\}} \Var_{p_h}( \uV_{h+1}^{k-1}-\Vstar_{h+1}) (s,a)} + 3 H \beta(\delta,n_h^t(s,a))\Bigg\}\\
% \cE^{\conc} &\triangleq \Bigg\{\forall t \in \N, \forall h \in [H], \forall (s,a)\in\cS\times\cA: \\
% &\qquad|(\hp_h^t -p_h) \Vstar_{h+1}(s,a)| \leq \sqrt{2 \Var_{p_h}(\Vstar_{h+1})(s,a)\beta(\delta,n_h^t(s,a))} + 3 H \frac{\beta(\delta,n_h^t(s,a))}{n_h^t(s,a)}\\
% &\qquad|(\hp_h^t -p_h) (\Vstar_{h+1})^2(s,a)| \leq 5\sqrt{ H^2\Var_{p_h}(\Vstar_{h+1})(s,a)\beta(\delta,n_h^t(s,a))} + 9 H^2 \frac{\beta(\delta,n_h^t(s,a))}{n_h^t(s,a)}
% \Bigg\}
% \end{align*}
% We also introduce the intersection of these events, $\cG \triangleq \cE \cap \cE^{\cnt}\cap \cE^\star,$ and the intersection of only the first two events, $\cF \triangleq \cE \cap \cE^{\cnt}$ . Note that the event $\cF$ is independent of the reward function $r$. We  prove that for the right choice of the functions $\beta$ the above events hold with high probability.
% \begin{lemma}
% \label{lem:proba_master_event}
% For the following choices of functions $\beta,$
% \begin{align*}
%   \beta(n,\delta) &\triangleq   \log(3SAH/\delta) + S\log \left(8e(n+1)\right),\\
%   \beta^\cnt(\delta) &\triangleq \log\left(3SAH/\delta\right), \quad \text{and}\\
%   \betastar(n,\delta) &\triangleq \log(3SAH/\delta) + \log\left(8e(n+1)\right)\,,\\
% \end{align*}
% it holds that
% \[
% \P(\cE)\geq 1-\delta, \qquad \P(\cE^{\cnt})\geq 1-\delta,  \qquad \text{and} \qquad \P(\cE^\star)\geq 1-\delta\,.
% \]
% In particular, $\P(\cG) \geq 1-\delta$ and $\P(\cF) \geq 1-\delta$.
% \end{lemma}
% \begin{proof}
% First, by Theorem~\ref{th:max_ineq_categorical}, we have that
% \[
% \P(\cE)\geq 1-\frac{\delta}{3}\cdot
% \]
% Second, by Theorem~\ref{th:bernoulli-deviation}, we have that
% \[
% \P(\cE^{\cnt})\geq 1-\frac{\delta}{3}\cdot
% \]
% Finally, by Theorem~\ref{th:bernstein}, we have that
% \[
% \P(\cE^\star)\geq 1-\frac{\delta}{3}\cdot
% \]
% Applying a union to the above three inequalities, we conclude that
% \[
% \P(\cG)\geq 1-\delta \qquad \P(\cE)\geq 1-\delta\,.
% \]
% \noindent \emph{Remark}: Note that we can order %\begin{align*}
%   $1\leq \beta^{\cnt}(\delta)\leq \betastar(n,\delta) \leq \beta(n,\delta).$
% %\end{align*}
% \end{proof}


\subsection{Deviation Inequality for Categorical Distributions}

Next, we state the deviation inequality for categorical distributions by \citet[Proposition 1]{jonsson2020planning}.
Let $(X_t)_{t\in\N^\star}$ be i.i.d.\,samples from a distribution supported on $\{1,\ldots,m\}$, of probabilities given by $p\in\simplex_{m-1}$, where $\simplex_{m-1}$ is the probability simplex of dimension $m-1$. We denote by $\hp_n$ the empirical vector of probabilities, i.e., for all $k\in\{1,\ldots,m\},$
 \[
 \hp_{n,k} \triangleq \frac{1}{n} \sum_{\ell=1}^n \ind\left\{X_\ell = k\right\}.
 \]
 Note that  an element $p \in \simplex_{m-1}$ can be seen as an element of $\R^{m-1}$ since $p_m = 1- \sum_{k=1}^{m-1} p_k$. This will be clear from the context. 
%  We denote by $H(p)$ the (Shannon) entropy of $p\in\Sigma_m$,
%  \[
%  H(p) = \sum_{k=1}^m p_k \log\left(\frac{1}{p_k}\right)\cdot
%  \]
 \begin{theorem} \label{th:max_ineq_categorical}
 For all $p\in\simplex_{m-1}$ and for all $\delta\in[0,1]$,
 \begin{align*}
     \P\left(\exists n\in \N^\star,\, n\KL(\hp_n, p)> \log(1/\delta) + (m-1)\log\left(e(1+n/(m-1))\right)\right)\leq \delta.
 \end{align*}
\end{theorem}



\subsection{Deviation Inequality for Shannon Entropy}

 We denote by $\cH(p)$ the (Shannon) entropy of $p\in \simplex_{m-1}$,
 \[
    \cH(p) \triangleq \sum_{k=1}^m p_k \log\left(\frac{1}{p_k}\right) .
 \]
We will follow the ideas of \citet{paninski2003estimation}.
 
\begin{theorem}\label{th:entropy_concentration}
    For all $p \in \simplex_{m-1}$ and for all $\delta \in[0,1]$
    \[
        \P\left[ \vert \cH(\hp_n) - \cH(p) \vert \geq  \sqrt{\frac{2 \log^2(n) \cdot \log(2/\delta)}{n}} + \left( \frac{(m-1) \log(\rme (1 + n/(m-1))) + 1}{n} \wedge \log(m) \right) \right] \leq \delta.
    \]
    Moreover,
    \[
        \P\left[ \exists n:  \vert \cH(\hp_n) - \cH(p) \vert \geq  \sqrt{\frac{2 \log^2(n) \cdot (\log(2/\delta) + \log(n(n+1)))}{n}} + \left( \frac{m \log(\rme (1 + n))}{n} \wedge \log(m) \right)\right] \leq \delta.
    \]
\end{theorem}
\begin{proof}
    We start from application of McDiarmid's inequality to entropy by \citet{antos2001convergence}.
    For all $p \in \Delta_{m-1}$ with probability at least $1-\delta$ we have
    \[
        \P\left[ \vert \cH(\hat p_n) - \E[\cH(\hat p_n)]  \vert \geq \sqrt{\frac{2 \log^2(n) \cdot \log(2/\delta)}{n}}  \right] \leq \delta.
    \]
    To relate $\E[\cH(\hp_n)]$ and $\cH(p)$ we use the following observation
    \[
        \cH(\hp_n) - \cH(p) = - \KL(\hp_n, p) + \sum_{k: p_k > 0} (\hp_{n,k} - p_k) \log(1/p_k),
    \]
    therefore by taking expectation we have
    \[
        \E[\cH(\hp_n)] - \cH(p) = - \E[\KL(\hp_n, p)].
    \]
    In the following our analysis differs from \cite{paninski2003estimation} since we obtain a direct estimate on the KL-divergence using Theorem~\ref{th:max_ineq_categorical},
    \begin{align*}
        \E[n \KL(\hp_n,p)] &= \int_{0}^\infty \P[n \KL(\hp_n, p) > t]\rmd t \leq (m-1) \log(\rme(1 + n/(m-1))) + \int_0^\infty \rme^{-t} \rmd t.
    \end{align*}
    At the same time we have a trivial bound that concludes the first statement
    \[
        \E[\cH(\hp^n)] - \cH(p) \leq \log(m).
    \]

    To show the second statement of Theorem~\ref{th:entropy_concentration}, we apply the first part with $\delta'(n) = \delta/( n(n+1))$,
    \begin{small}
    \[
        \P\left[ \vert \cH(\hp_n) - \cH(p) \vert \geq  \sqrt{\frac{2 \log^2(n) \cdot \log(2/\delta'(n))}{n}} + \left( \frac{(m-1) \log(\rme (1 + n/(m-1))) + 1}{n} \wedge \log(m) \right)\right] \leq \frac{\delta}{n(n+1)},
    \]
    \end{small}
    thus by union bound over $n \in \N$ we conclude the statement.
\end{proof}

\subsection{Deviation Inequality for Sequence of Bernoulli Random Variables}

Below, we state the deviation inequality for Bernoulli distributions by \citet[Lemma F.4]{dann2017unifying}.
Let $\mathcal F_t$ for $t\in\N$ be a filtration and $(X_t)_{t\in\N^\star}$ be a sequence of Bernoulli random variables with $\P(X_t = 1 | \mathcal F_{t-1}) = P_t$ with $P_t$ being $\mathcal F_{t-1}$-measurable and $X_t$ being $\mathcal F_{t}$-measurable.

\begin{theorem}\label{th:bernoulli-deviation}
	For all $\delta>0$,
	\begin{align*}
	\P \left(\exists n : \,\, \sum_{t=1}^n X_t < \sum_{t=1}^n P_t / 2 -\log\frac{1}{\delta}  \right) \leq \delta.
	\end{align*}
\end{theorem}



\subsection{Deviation Inequality for Bounded Distributions}
Below, we state the self-normalized Bernstein-type inequality by \citet{domingues2020regret}. Let $(Y_t)_{t\in\N^\star}$, $(w_t)_{t\in\N^\star}$ be two sequences of random variables adapted to a filtration $(\cF_t)_{t\in\N}$. We assume that the weights are in the unit interval $w_t\in[0,1]$ and predictable, i.e. $\cF_{t-1}$ measurable. We also assume that the random variables $Y_t$  are bounded $|Y_t|\leq b$ and centered $\EEc{Y_t}{\cF_{t-1}} = 0$.
Consider the following quantities
\begin{align*}
		S_t \triangleq \sum_{s=1}^t w_s Y_s, \quad V_t \triangleq \sum_{s=1}^t w_s^2\cdot\EEc{Y_s^2}{\cF_{s-1}}, \quad \mbox{and} \quad W_t \triangleq \sum_{s=1}^t w_s
\end{align*}
and let $h(x) \triangleq (x+1) \log(x+1)-x$ be the CramÃ©r transform of a Poisson distribution of parameter~1.

\begin{theorem}[Bernstein-type concentration inequality]
  \label{th:bernstein}
	For all $\delta >0$,
	\begin{align*}
		\PP{\exists t\geq 1,   (V_t/b^2+1)h\left(\!\frac{b |S_t|}{V_t+b^2}\right) \geq \log(1/\delta) + \log\left(4e(2t+1)\!\right)}\leq \delta.
	\end{align*}
  The previous inequality can be weakened to obtain a more explicit bound: if $b\geq 1$ with probability at least $1-\delta$, for all $t\geq 1$,
 \[
 |S_t|\leq \sqrt{2V_t \log\left(4e(2t+1)/\delta\right)}+ 3b\log\left(4e(2t+1)/\delta\right)\,.
 \]
\end{theorem}


\subsection{Deviation Inequalities for Expectation over Sampling Measure}

In this section we describe two inequalities that shows the deviations of quantities of empirical transition kernels constructed by an independent samples from some base distribution over states. Let $\{z_k\}_{k\in[N]}$ be a set of independent trajectories using a fixed policy $\pi$ in the original MDP: $\forall i \in [H]: a_i \sim \pi(s_i), s_{i+1} \sim p(s_i, a_i)$, and let $\mu_h(s,a) \triangleq d^\pi_h(s,a)$ be its state-action visitation distribution. 

Using this data, we construct an estimates of transition probabilities $\{\hp_h\}_{h\in[H]}$: for each state-action-step triple $(s,a,h)$ we define $n_h(s,a)$ as a number of visits in these $N$ trajectories:
\[
    n_h(s,a) = \sum_{k=1}^N \ind\{ (s^k_h, a^k_h) = (s,a) \} \quad  n_h(s'|s,a) = \sum_{k=1}^N \ind\{ (s^k_h, a^k_h, s^k_{h+1}) = (s,a,s') \},
\]
where $z_k = (s^k_1, a^k_1, \ldots, s^k_H, a^k_H, s^k_{H+1})$, and then the model constructed in a usual way as a maximum likelihood estimate
\[
    \hp_h(s'|s,a) = \begin{cases}
        \frac{n_h(s'|s,a)}{n_h(s,a)} & n_h(s,a) > 0 \\
        \frac{1}{S} & n_h(s,a) = 0
    \end{cases}\,.
\]

\begin{lemma}\label{lem:sampling_entropy_bound}
    Suppose that $\hp_h$ is the empirical transitions formed using $N$ independent trajectories $\{z_k\}_{k\in[N]}$ sampled independently using policy $\pi$ in the original MDP. Then we probability at least $1-\delta$ the following holds
    \[
        \forall h \in [H]: \E_{(s,a) \sim \mu_{h}}\left[ (\cH(\hp_{h}(s,a)) - \cH(p_{h}(s,a)))^2 \right] \leq \frac{12S^2 A \log^2(SN) \cdot \log(4SAH/\delta)}{N},
    \]
    where $\mu_h(s,a) = d^\pi_h(s,a)$.
\end{lemma}
\begin{proof}
    Define $n_h(s,a)$ be a number of samples from a fixed state-action pair $s,a$. Then by Theorem~\ref{th:entropy_concentration} we have with probability at least $1-\delta/2$ for all triples $(s,a,h) \in \cS \times \cA \times [H]$
    \[
        (\cH(\hp_{h}(s,a)) - \cH(p_{h}(s,a)))^2 \leq \frac{2 \log^2(n_h(s,a)) \cdot \log\left( 4SAH/\delta \right)  }{n_h(s,a)} + \frac{S \log(S) \log(n_h(s,a)) }{n_h(s,a)}.
    \]
    From other point of view, we have a trivial upper bound $\log^2(S)$. Thus, we obtain
    \[
        (\cH(\hp_{h}(s,a)) - \cH(p_{h}(s,a)))^2 \leq \frac{2 \log^2(S n_h(s,a)) \cdot \log\left( 4SAH/\delta \right) + S \log(S) \log(n_h(s,a) }{n_h(s,a)} \wedge 1.
    \]
    Next we define the following event
    \[
        \cE^{\cnt}(\delta) = \left\{ \forall (s,a,h) \in \cS \times \cA \times [H]:  n_h(s,a) \geq \frac{N}{2} \mu_h(s,a) - \log(2SAH/\delta) \right\}.
    \]
    By Theorem~\ref{th:bernoulli-deviation} it holds with probability at least $1-\delta/2$, then we can apply Lemma~\ref{lem:cnt_pseudo} and obtain the following bound with probability at least $1-\delta$ by union bound
    \[
        (\cH(\hp_{h}(s,a)) - \cH(p_{h}(s,a)))^2 \leq 4\frac{2 \log^2(SN) \cdot \log\left( 4SAH/\delta \right) + S \log(S) \log(N)}{(N \mu_h(s,a)) \vee 1}.
    \]
    Thus, taking expectation we have
    \begin{align*}
        \E_{(s,a) \sim \mu_{h}}\left[ (\cH(\hp_{h}(s,a)) - \cH(p_{h}(s,a)))^2 \right] &\leq \sum_{s,a} \frac{\mu_h(s,a)}{(N \mu_h(s,a)) \vee 1} \cdot 4\left(2\log^2(SN) \log(4SAH/\delta) + S \log(S)\log(N)\right) \\
        &\leq \frac{12S^2A \cdot \log^2(SN) \cdot \log(4SAH/\delta)}{N}.
    \end{align*}
\end{proof}

\begin{lemma}\label{lem:sampling_square_value_error_bound}[Lemma C.2 by \citealt{jin2020reward-free}]
      Suppose that $\hp_h$ is the empirical transitions formed using $N$ independent trajectories $\{z_k\}_{k\in[N]}$ sampled independently using policy $\pi$ in the original MDP.  Then there is an absolute constant $C > 0$ such that with probability at least $1-\delta$ the following holds for all $h \in [H]$
    \begin{align*}
        \max_{G \colon \cS \to [0,H \Rmax]} \max_{\nu \colon \cS \to \cA} &\E_{(s,a) \sim \mu_{h}}\left[ \big(\left[ \hp_{h'} - p_{h'}  \right] G(s,a) \big)^2 \ind\{\nu(s) = a \} \right] \leq  \frac{CH^2 \Rmax^2 S}{N} \log\left( \frac{AH\Rmax N}{\delta} \right),
    \end{align*}
    where $\mu_h(s,a) = d^\pi_h(s,a)$.
\end{lemma}