\section{Sample Complexity for MTEE and Regularized MDPs}\label{app:regularized_mdp}

In this section we describe the general setting of regularized MDPs, not only entropy-regularized.

\subsection{Preliminaries}

First we define class of regularizers we are interested in. For more exposition on this definition, see \citet{bubeck2015convex}. 
\begin{definition}\label{def:mirror_map}
    Let $\Phi \colon \simplex_{\cA} \to \R$ be a proper closed strongly-convex function. We will call $\Phi$ a mirror-map if the following holds 
\begin{itemize}
    \item $\Phi$ is $1$-strongly convex with respect to norm $\norm{\cdot}$;
    \item $\nabla \Phi$ takes all possible values in $\R^{\cA}$;
    \item $\nabla \Phi$ diverges on the boundary of $\simplex_{\cA}$: $\lim_{x \in \partial \simplex_{\cA}} \norm{\nabla \Phi(x)} = +\infty$;
\end{itemize}
\end{definition}

We explain three main examples of a such regularizers.
\begin{itemize}
    \item The negative Shannon entropy $\Phi(\pi) = -\cH(\pi)$ for $\cH(\pi) = \sum_{a\in\cA} \pi_a \log\left(\frac{1}{\pi_a}\right)$ satisfies the Definition~\ref{def:mirror_map} for $\ell_1$-norm;
    \item The negative Tsallis entropy $\Phi(\pi) = - \frac{1}{q}\cT_{q}(\pi)$ for $\cT_q(\pi) = \frac{1}{q - 1} \left(1 - \sum_{a\in\cA} \pi_a^{q} \right)$ satisfied the Definition~\ref{def:mirror_map} for $\ell_2$ norm for every $q\in(0,1)$. In particular, $q=0.5$ corresponds to the choice by \citet{grill2019planning} in Appendix E that is tightly connected to the UCB algorithm;
    \item For any other fixed policy $\pi'\in\simplex_{\cA}$ we can choose $\Phi(\pi) = \KL(\pi, \pi') = \sum_{a\in\cA} \pi_a \log\left( \frac{\pi_a}{\pi'_a}\right)$ that inherits all the properties from the choice of the negative entropy.
\end{itemize}


Let $\cM = (\cS, \cA, \{p_h\}_{h\in[H]}, \{r_h\}_{h\in[H]}, s_1)$ be a finite-horizon MDP, where $r_h(s,a)$ is a deterministic reward function. For simplicity we assume that $0 \leq r_h(s,a) \leq r_{\max}$ for any $(h,s,a) \in [H] \times \cS \times \cA$. 

Then we can define entropy-augmented rewards as follows
\[
    r_{\kappa,h}(s,a) = r_h(s,a) + \kappa \cH(p_h(s,a)).
\]
This definition is required to cover the following case of practical interest. Let $\kappa = \lambda$ and $\Phi(\pi) = -\cH(\pi)$, then we obtain the following representation for the $\lambda$-regularized value function
\[
    V^{\pi}_{\lambda,1}(s_1) = V^\pi_1(s_1) + \lambda \TE(q^\pi),
\]
where $V^\pi_1(s_1)$ is a usual value function for a MDP $\cM$. For $r_{\max} = 0$  and $\kappa=\lambda=1$ we recover just a trajectory entropy $V^{\pi}_{\lambda,1}(s_1) = \TE(q^\pi)$.



Next we define a convex conjugate to $\lambda \Phi$ as $F_\lambda \colon \R^{\cA} \to \R$
\[
    F_\lambda (x) = \max_{\pi \in \simplex_{\cA}} \{ \langle \pi, x \rangle - \lambda \Phi(\pi) \}
\]
and, with a sight abuse of notation extend the action of this function to the $Q$-function as follows
\[
    \Vstar_{\lambda,h}(s) = F_{\lambda}(\Qstar_{\lambda,h})(s) = \max_{\pi \in \Delta_{\cA}}\left\{ \pi \Qstar_{\lambda,h}(s) - \lambda \Phi(\pi) \right\}.
\]

Thanks to the fact that $\Phi$ satisfies Definition~\ref{def:mirror_map}, we have exact formula for the optimal policy by Fenchel-Legendre transform
\[
    \pi^\star_h(s) = \argmax_{\pi \in \Delta_{\cA}}\left\{ \pi \Qstar_{\lambda,h}(s) - \lambda \Phi(\pi) \right\} = \nabla F_\lambda(\Qstar_{\lambda,h}(s,\cdot)).
\]
Notice that we have $\nabla F_\lambda(\Qstar_{\lambda,h}(s,\cdot)) \in \simplex_{\cA}$ since the gradient of $\Phi$ diverges on the boundary of $\simplex_{\cA}$. For entropy regularization this formula become the softmax function.

Finally, it is known that the smoothness property of $F_{\lambda}$ plays a key role in reduced sample complexity for planning in regularized MDPs \cite{grill2019planning}. For our general setting we have that since $\lambda \Phi$ is $\lambda$-strongly convex with respect to $\norm{\cdot}$, then $F_{\lambda}$ is $1/\lambda$-strongly smooth with respect to a dual norm $\norm{\cdot}_*$
\[
    F_\lambda(x) \leq F_\lambda(x') + \langle \nabla F_\lambda(x'), x-x' \rangle + \frac{1}{2\lambda} \norm{x - x'}_*^2.
\]
Let us define $\Rphi$ as a maximal possible value of $\vert \Phi\vert$. Without loss of generality assume that $\Phi \leq 0$. In this case we define $\Rmax = r_{\max} + \kappa \log(S) + \lambda \Rphi $ as an upper bound of an about of reward obtain at the one step. By this definition we have $0 \leq V^{\pi}_{\lambda,h}(s) \leq H \Rmax$ for any $h\in[H], s \in \cS$ and any policy $\pi$.

Also, since all norms in $\R^\cA$ are equivalent, we define a constant $r_A$ that defined for a dual norm $\norm{\cdot}_*$ as follows
\begin{equation}\label{eq:norm_equivalence}
    \norm{\cdot }_* \leq r_A \cdot \norm{\cdot}_\infty.
\end{equation}
For example, for $\ell_2$-norm $r_A = \sqrt{A}$ and for $\ell_1$-norm $r_A = A$. In the case $\Phi = -\cH$ we have $r_A = 1$ since the entropy is $1$-strongly convex with respect to a $\ell_1$-norm, thus the dual norm is exactly a $\ell_\infty$-norm.

The rest of this section is devoted to obtain the sample complexity guarantees for \UCBVIEnt algorithm with \textit{a regularization-agnostic stopping rule} \eqref{eq:def_tau_agnostic} with the gap notion \eqref{eq:def_gap_agnostic}. In this case Theorem~\ref{th:reg_agnostic_sample_complexity} gives us $\tcO\left(\frac{H^3SA}{\varepsilon^2} + \frac{H^3 S^2A}{\varepsilon}\right)$ sample complexity guarantee ignoring $\Rmax$ and poly-logarithmic factors. Notably, this sample complexity result does depend directly on $1/\lambda$, so small regularization does not affect the complexity.

In Section~\ref{app:fast_rates_regularized} we present another algorithm \RFExploreEnt, based on ideas of reward-free exploration, that achieves sample complexity of order $\tcO(\poly(S,A,H)/(\lambda\varepsilon)$. As a particular example, it yields an algorithm for the MTEE problem with sample complexity $\tcO\left( \poly(S,A,H) / \varepsilon \right)$ by taking as a regularizer negative entropy,  $\lambda = \kappa = 1$ and $r_{\max} = 0$. Moreover, in Section~\ref{app:reg_visitation_entropy_proofs} we apply this algorithm to achieve sample complexity of order $\tcO(H^2 S A /\varepsilon^2)$ for the maximum visitation entropy exploration problem.

\subsection{\UCBVIEnt Algorithm}

In this section we describe \UCBVIEnt algorithm to solve MTEE problem, however as we show in the proofs, it is capable to work with general regularized MDPs.

We now describe our algorithm \algMTEE for MTEE. Since one only needs to solve  regularized Bellman equations to obtain a maximum trajectory entropy policy, we can use an algorithm of the same flavor as the ones proposed for best policy identification \citep{tirinzoni2022optimistic, menard2021fast, kaufmann2020adaptive, dann2019policy}. In particular, \algMTEE is close to the \BPIUCRL algorithm by \citet{kaufmann2020adaptive} and can be characterized by the following rules.


\paragraph{Sampling rule} As sampling rule we use an optimistic policy $\pi^{t+1}$ obtained by optimistic planning in the regularized MDP
\begin{align}
  \uQ_h^t(s,a) &=  \clip\Big(\cH\big(\hp_h(s,a)\big)+b_h^{\cH,t}(s,a) + \hp^t_h \uV_{h+1}^t(s,a)+ b_h^{p,t}(s,a),0,\log(SA)H\Big)\,,\nonumber\\
  \uV_h^t(s) &= \max_{\pi\in\Delta_A}  \pi \uQ_h^t (s) + \cH(\pi)\,,\label{eq:optimistic_planning_TE}\\
  \pi_h^{t+1}(s) &= \argmax_{\pi\in\Delta_A}  \pi \uQ_h^t (s) + \cH(\pi)\nonumber\,,
\end{align}
with $\uV^t_{H+1} = 0$ by convention  where $b^{\cH,t}$, $b^{p,t}$ are bonuses for the entropy  and the transition probabilities, respectively. Precisely, we use bonuses of the form 
\begin{align*}
        b_h^{\cH,t}(s,a) &=  \sqrt{\frac{2 \beta^{\cH}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + \min\left(\frac{\beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)},\, \log(S) \!\!\right)\,,\\
        b^{p,t}_h(s,a) &\triangleq b^{B,t}_h(s,a) + b^{\corr,t}_h(s,a),\\
        b^{B,t}_h(s,a) &\triangleq 3\sqrt{\Var_{\hp^t_h}(\uV^t_{h+1})(s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + \frac{9H^2 \log(SA) \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)}, \\
        b^{\corr,t}_h(s,a) &\triangleq \frac{1}{H} \hp^t_h(\uV^t_{h+1} - \lV^t_{h+1})(s,a).
\end{align*}
for some functions $\beta^{\cH}, \beta^{\KL}$ and $\beta^{\conc}$ and $\lV^t$ is a lower confidence bound on the optimal value function defined in Appendix~\ref{app:conf_intervals_ucbvient}.



\paragraph{Stopping and decision rule} To define the stopping rule, we first recursively build an upper-bound on the difference between the value of the optimal policy and the value of the current policy $\pi^{t+1}$, 
\begin{align}\label{eq:upper_bound_gap}
    \begin{split}
    G^t_h(s,a) \triangleq \clip\biggl( 2 b^{B,t}_h(s,a) + 2 b^{\cH, t}_h(s,a) &+ \frac{4 H^2 \log(SA) \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} \\
    & + \left(1 + \frac{3}{H} \right) \hp^t_h \left[\pi^{t+1}_{h+1} G^t_{h+1}\right](s,a),  0, H\Rmax \biggl),
    \end{split}
\end{align}

where $\lV^t$ is a lower-bound on the optimal value function defined in Appendix~\ref{app:conf_intervals_ucbvient} and $G_{H+1}^t(s,a) = 0$ by convention.
Then the stopping time $\tau= \inf\{ t \in \N : \pi^{t+1} G^t_{1}(s_1)  \leq \varepsilon \}$ corresponds to the first episode when this upper-bound is smaller than $\epsilon$. At this episode we return the Markovian policy $\hpi = \pi^{\tau+1}$.

\begin{algorithm}
\centering
\caption{\algMTEE}
\label{alg:UCBVIEnt}
\begin{algorithmic}[1]
  \STATE {\bfseries Input:} Target precision $\epsilon$, target probability $\delta$, threshold functions $\beta^{\cH},\beta^{p}$, $\beta^{\conc}$.
      \WHILE{ true}
       \STATE Compute $\pi^{t}$ by optimistic planning with \eqref{eq:optimistic_planning_TE}.
       \STATE Compute bound on the gap $G_1^{t-1}(s,a)$ with \eqref{eq:upper_bound_gap}.
       \STATE \textbf{if} $\pi^t G_1^{t-1}(s_1)\leq \epsilon$ \textbf{then break}
      \FOR{$h \in [H]$}
        \STATE Play $a_h^t\sim \pi_h^t(s^t_h)$
        \STATE Observe $s_{h+1}^t\sim p_h(s_h^t,a_h^t)$
      \ENDFOR
    \STATE{ Update counts $n^t$, transition estimates $\hp^t$ and episode number $t\gets t+1$.}
   \ENDWHILE
   \STATE Output policy $\hpi = \pi^t$.
\end{algorithmic}
\end{algorithm}
The complete procedure is described in Algorithm~\ref{alg:UCBVIEnt}. We now state our main theoretical result for \algMTEE. We prove that for the well calibrated threshold functions $\beta^{\cH}, \beta^{\KL}$ and $\beta^{\conc},$ the \algMTEE is  $(\epsilon, \delta)$-PAC for MTEE and  provide a
high-probability upper bound on its sample complexity. 
\begin{theorem}
\label{th:ucbvi_sample_complexiy}
Let $\beta^{\KL}, \beta^{\conc}$ and $\beta^{\cH}$ be defined in Lemma~\ref{lem:proba_master_event} of Appendix~\ref{app:regularized_mdp}. Fix  $\epsilon>0$ and $\delta\in(0,1),$ then the \algMTEE algorithm is $(\epsilon,\delta)$-PAC for MTEE.   Moreover, the optimal policy is given by $\hpi = \pi^{\tau+1}$ where 
\[
\tau = \tcO\left(\frac{H^3SA}{\epsilon^2} + \frac{H^3S^2A}{\varepsilon}\right).
\]
with probability at least $1-\delta.$
Here $\tcO$ hides poly-logarithmic factors in $\epsilon, \delta, H,S,A$.
\end{theorem}

See Theorem~\ref{th:mtee_sample_complexity} in Appendix~\ref{app:regularized_mdp} for a precise bound and a proof. Basically, this result is a simple corollary of the general result on regularized MDPs.


\paragraph{Space and time complexity} Since \UCBVIEnt is a model based algorithm, its space-complexity is of order $\cO(HS^2A)$ whereas its time-complexity for one episode is of order $\cO(HS^2A)$ because of the optimistic planning.



\subsection{Concentration Events}
Following the ideas of \cite{menard2021fast}, we define the following concentration events. 

Let $\beta^{\KL}, \beta^{\conc}, \beta^{\cnt}, \beta^{\cH}: (0,1) \times \N \to \R_{+}$ be some functions defined later on in Lemma \ref{lem:proba_master_event}. We define the following favorable events
\begin{align*}
\cE^{\KL}(\delta) &\triangleq \Bigg\{ \forall t \in \N, \forall h \in [H], \forall (s,a) \in \cS\times\cA: \quad \KL(\hp^{\,t}_h(s,a), p_h(s,a)) \leq \frac{\beta^{\KL}(\delta, n^{\,t}_h(s,a))}{n^{\,t}_h(s,a)} \Bigg\},\\
\cE^{\conc}(\delta) &\triangleq \Bigg\{\forall t \in \N, \forall h \in [H], \forall (s,a)\in\cS\times\cA: \\
&\qquad|(\hp_h^t -p_h) \Vstar_{\lambda,h+1}(s,a)| \leq \sqrt{2 \Var_{p_h}(\Vstar_{\lambda,h+1})(s,a)\frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} + 3 H \Rmax \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}\Bigg\},\\
\cE^{\cnt}(\delta) &\triangleq \Bigg\{ \forall t \in \N, \forall h \in [H], \forall (s,a) \in \cS\times\cA: \quad n^t_h(s,a) \geq \frac{1}{2} \upn^t_h(s,a) - \beta^{\cnt}(\delta) \Bigg\},\\
\cE^{\cH}(\delta) &\triangleq \Bigg\{\forall t \in \N, \forall h \in [H], \forall (s,a)\in\cS\times\cA: \\
&\qquad  \vert \cH(\hp^t_h(s,a)) - \cH(p_h(s,a)) \vert \leq  \sqrt{\frac{2 \beta^{\cH}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + \left(\frac{\beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} \wedge \log(S) \right)
\Bigg\}.
\end{align*}
We also introduce two intersections of these events of interest, $\cG(\delta) \triangleq \cE^{\KL}(\delta) \cap \cE^{\conc}_B(\delta) \cap \cE^{\cnt}(\delta) \cap \cE^{\cH}(\delta)$. We  prove that for the right choice of the functions $\beta^{\KL}, \beta^{\conc},\beta^{\cnt}, \beta^{\cH}$ the above events hold with high probability.
\begin{lemma}
\label{lem:proba_master_event}
For any $\delta \in (0,1)$ and for the following choices of functions $\beta,$
\begin{align*}
    \beta^{\KL}(\delta, n) & \triangleq \log(4SAH/\delta) + S\log\left(\rme(1+n) \right), \\
    \beta^{\conc}(\delta, n) &\triangleq \log(4SAH/\delta) + \log(4\rme n(2n+1)) ,\\
    \beta^{\cnt}(\delta) &\triangleq \log(4SAH/\delta), \\
    \beta^{\cH}(\delta,n) &\triangleq \log^2(n)\left(\log(4SAH/\delta) + \log(n(n+1))\right),
\end{align*}
it holds that
\begin{align*}
\P[\cE^{\KL}(\delta)]&\geq 1-\delta/4, \qquad\qquad \P[\cE^{\conc}(\delta)]\geq 1-\delta/4, \\
\P[\cE^\cnt(\delta)]&\geq 1-\delta/4,  \qquad \P[\cE^{\cH}(\delta)]\geq 1-\delta/4.
\end{align*}
In particular, $\P[\cG(\delta)] \geq 1-\delta$.
\end{lemma}
\begin{proof}
Applying Theorem~\ref{th:max_ineq_categorical} and the union bound over $h \in [H], (s,a) \in \cS \times \cA$ we get $\P[\cE^{\KL}(\delta)]\geq 1-\delta/4$.  Next, Theorem~\ref{th:bernstein} and the union bound over $h \in [H], (s,a) \in \cS \times \cA$ yield $\P[\cE^{\conc}(\delta)]\geq 1 - \delta/4$. By Theorem~\ref{th:bernoulli-deviation} and union bound,  $\P[\cE^{\cnt}(\delta)]\geq 1 - \delta/4$.  Finally, by Theorem~\ref{th:entropy_concentration} and union bound over $(s,a,h) \in \cS \times \cA \times [H]$ $\P[\cE^{\cH}(\delta)]\geq 1-\delta/4$. The union bound over four prescribed events concludes $\P[\cG(\delta)] \geq 1 - \delta$.
\end{proof}


\begin{lemma}\label{lem:reg_directional_concentration}
      Assume conditions of Lemma \ref{lem:proba_master_event}. Then on event $\cE^{\KL}(\delta)$, for any $f \colon \cS \to [0, H \Rmax]$, $t \in \N, h \in [H], (s,a) \in \cS \times \cA$,
      \begin{align*}
            [p_h - \hp_h^t]f(s,a) &\leq \frac{1}{H} \hp^t_h f(s,a) + 2H\Rmax \left(\frac{2H \beta^{\KL}(\delta, n^{\,t}_h(s,a))}{n^{\,t}_h(s,a)} \wedge 1 \right), \\
            [\hp_h^t -p_h]f(s,a) &\leq \frac{1}{H} p_h f(s,a) + 2H\Rmax \left(\frac{2H \beta^{\KL}(\delta, n^{\,t}_h(s,a))}{n^{\,t}_h(s,a)} \wedge 1 \right). 
      \end{align*}
\end{lemma}
\begin{proof}
    Let us start from the first statement.  We apply the second inequality of Lemma~\ref{lem:Bernstein_via_kl} and Lemma~\ref{lem:switch_variance_bis} to obtain
    \begin{align*}
        [p_h - \hp_h^t]f(s,a) &\leq \sqrt{2\Var_{p_h}[f](s,a) \cdot \KL(\hp_h^t, p_h) } \\
        &\leq 2\sqrt{\Var_{\hp^t_h}[f](s,a) \cdot \KL(\hp_h^t, p_h) } +  3 H \Rmax \KL(\hp^t_h, p_h).
    \end{align*}
    Since $0 \leq f(s) \leq  H\Rmax$ we get
    \[
        \Var_{\hp^t_h}[f](s,a) \leq \hp^t_h[f^2](s,a) \leq  H\Rmax \cdot \hp^t_h f(s,a).
    \]
    Finally, applying $2\sqrt{ab} \leq a+b, a, b \geq 0$, we obtain the following inequality
    \begin{align*}
        (\hp_h^t -p_h)f(s,a) &\leq \frac{1}{H} \hp^t_h f(s,a) + 4H^2\Rmax \KL(\hp_h^t, p_h).
    \end{align*}
    Definition of $\cE^{\KL}(\delta)$ implies the part of the statement. At the same time we have a trivial bound since $f(s) \in [0, H \Rmax]$
    \[
        [p_h - \hp^t_h] f(s,a) \leq 2H\Rmax \leq \frac{1}{H} \hp^t_h f(s,a) + 2H\Rmax.
    \]

    To prove the second statement, apply the first inequality of Lemma~\ref{lem:Bernstein_via_kl} and proceed similarly.
\end{proof}


\begin{lemma}\label{lem:empirical_bernstein}
    Assume conditions of Lemma \ref{lem:proba_master_event} and assume that $\beta^{\conc}(\delta) \leq \beta^{\KL}(\delta)$. Then conditioned on event $\cG(\delta)$, for any $U \colon \cS \to [0, H \Rmax]$, $t \in \N, h \in [H], (s,a) \in \cS \times \cA$,
      \begin{align*}
            \vert (\hp_h^t -p_h)\Vstar_{\lambda,h+1}(s,a) \vert &\leq 3 \sqrt{\Var_{\hp^t_{h+1}}(U)(s,a) \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} + \frac{9H^2\Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} \\
            &+ \frac{1}{H} \hp^t_h \vert U - \Vstar_{\lambda,h+1} \vert(s,a).
      \end{align*}
\end{lemma}
\begin{proof}
    First, we apply the definition of event $\cE^{\conc}(\delta)$
    \[
        \vert (\hp_h^t -p_h)\Vstar_{\lambda,h+1}(s,a) \vert \leq \sqrt{2 \Var_{p_h}(\Vstar_{\lambda,h+1})(s,a)\frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} + 3 H \Rmax \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}.
    \]
    Next we apply Lemma~\ref{lem:switch_variance_bis} and Lemma~\ref{lem:switch_variance} and obtain
    \begin{align*}
        \Var_{p_h}(\Vstar_{\lambda,h+1})(s,a) &\leq 2 \Var_{\hp^t_h}(\Vstar_{\lambda,h+1})(s,a) + 4H^2 \Rmax^2 \KL(\hp^t_h(s,a), p_h(s,a)) \\
        &\leq 4\Var_{\hp^t_{h+1}}(U)(s,a) + 4H\Rmax \hp^t_h \vert U - \Vstar_{\lambda,h+1} \vert(s,a) +  4H^2 \Rmax^2 \KL(\hp^t_h(s,a), p_h(s,a)).
    \end{align*}
    Thus, by inequality $\sqrt{a+b} \leq \sqrt{a} + \sqrt{b}$.
    \begin{align*}
        \vert (\hp_h^t -p_h)\Vstar_{\lambda,h+1}(s,a) \vert &\leq 3 \sqrt{\Var_{\hp^t_{h+1}}(U)(s,a) \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} \\
        &+ 3 \sqrt{H\Rmax \hp^t_h \vert U - \Vstar_{\lambda,h+1}\vert(s,a) \cdot \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} \\
        &+ 3H\Rmax\sqrt{\KL(\hp^t_h(s,a), p_h(s,a)) \cdot \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} \\
        &+ 3H\Rmax \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}.
    \end{align*}
    By inequality $2\sqrt{ab} \leq a+b$ we have
    \[
        3 \sqrt{H\Rmax \hp^t_h \vert U - \Vstar_{\lambda,h+1}\vert(s,a) \cdot \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} \leq \frac{1}{H}\hp^t_h \vert U - \Vstar_{\lambda,h+1}\vert(s,a) + \frac{9 H^2 \Rmax\beta^{\conc}(\delta,n_h^t(s,a))}{4 n^t_h(s,a)}.
    \]
    By the definition of the event $\cE^{\KL}(\delta)$ and the fact $\beta^{\conc }(\delta) \leq \beta^{\KL}(\delta)$ we have
    \[
        \sqrt{\KL(\hp^t_h(s,a), p_h(s,a)) \cdot \frac{\beta^{\conc}(\delta,n_h^t(s,a))}{n_h^t(s,a)}} \leq \frac{\beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)}.
    \]
\end{proof}

\subsection{Confidence Intervals}\label{app:conf_intervals_ucbvient}

Similar to \citet{azar2017minimax,zanette2019tighter,menard2021fast}, we define the upper confidence bound for the optimal regularized  Q-function of two types: with Hoeffding bonuses and with Bernstein bonuses. 

Let us define empirical estimate of entropy-augmented rewards as follows
\[
    \hat r^t_{\kappa,h}(s,a) = r_h(s,a) + \kappa \cH(\hp^t_h(s,a)).
\]
Then we have the following sequences defined as follows
\begin{align*}
    \uQ^{t}_{h}(s,a) &= \clip\left( \hat r^t_{\kappa,h}(s,a) + \hp^t_h \uV^t_h(s,a) + b^{p,t}_h(s,a) + \kappa b^{\cH, t}_h(s,a),0,H\Rmax \right) \\
    \pi^{t+1}_h(s) &= \max_{\pi \in \simplex_\cA} \{ \pi \uQ^t_h(s) -\lambda \Phi(\pi)\}, \\
    \uV^t_h(s) &= \cH(\pi^{t+1}_h(s)) + \pi^{t+1}_h \uQ^t_h(s)  \\
    \uV^t_{H+1}(s) &= 0,
\end{align*}
and the lower confidence bound as follows
\begin{align*}
    \lQ^{t}_{h}(s,a) &= \clip\left( \hat r^t_{\kappa,h}(s,a) + \hp^t_h \lV^t_h(s,a) - b^{p,t}_h(s,a) - \kappa b^{\cH, t}_h(s,a),0,H \Rmax\right) \\
    \lV^t_h(s) &= \max_{\pi \in \simplex_\cA }\{ \pi \lQ^t_h(s) - \lambda\Phi(\pi)  \} \\
    \lV^t_{H+1}(s) &= 0,
\end{align*}
where the Bernstein bonuses for transitions are defined as follows
\begin{align}\label{eq:bernstein_transition_bonuses}
    \begin{split}
        b^{p,t}_h(s,a) &\triangleq b^{B,t}_h(s,a) + b^{\corr,t}_h(s,a),\\
        b^{B,t}_h(s,a) &\triangleq 3\sqrt{\Var_{\hp^t_h}(\uV^t_{h+1})(s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + \frac{9H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)}, \\
        b^{\corr,t}_h(s,a) &\triangleq \frac{1}{H} \hp^t_h(\uV^t_{h+1} - \lV^t_{h+1})(s,a).
    \end{split}
\end{align}
The entropy bonuses are defined bellow
\begin{align}\label{eq:entropy_bonuses}
    b^{\cH, t}_h(s,a) &\triangleq \sqrt{\frac{2 \beta^{\cH}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + \left(\frac{\beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} \wedge \log(S) \right).
\end{align}

\begin{theorem}\label{th:reg_confidence_intervals}
    Let $\delta \in (0,1)$. Assume Bernstein bonuses \eqref{eq:bernstein_transition_bonuses}. Then on event $\cG(\delta)$ for any $t \in \N$, $(h,s,a) \in [H]\times \cS \times \cA$ it holds
    \[
        \lQ^t_h(s,a) \leq \Qstar_{\lambda,h}(s,a) \leq \uQ^t_h(s,a), \qquad \lV^t_h(s) \leq \Vstar_{\lambda,h}(s) \leq \uV^{t}_h(s,a).
    \]
\end{theorem}
\begin{proof}
    Proceed by induction over $h$. For $h = H+1$ the statement is trivial. Now we assume that inequality holds for any $h' > h$ for a fixed $h \in [H]$. Fix a timestamp $t \in \N$ and a state-action pair $(s,a)$ and assume that $\uQ^t_h(s,a) < H\Rmax$, i.e. no clipping occurs. Otherwise the inequality $\Qstar_{\lambda,h}(s,a) \leq \uQ^t_h(s,a)$ is trivial. In particular, it implies $n^t_h(s,a) > 0$.

    In this case, by Bellman equations \eqref{eq:opt_reg_bellman_equation} we have
    \begin{align*}
        [\uQ^t_h - \Qstar_{\lambda,h}](s,a) &= \underbrace{r_h(s,a) + \kappa \cH(\hp^t_h(s,a)) - r_h(s,a) - \kappa \cH(p_h(s,a)) + \kappa b^{\cH,t}_h(s,a)}_{T_1} \\
        &+ \underbrace{\hp^t_h \uV^t_{h+1}(s,a) - p_h \Vstar_{\lambda,h+1}(s,a) + b^{p,t}_h(s,a)}_{T_2}.
    \end{align*}
    By the definition of event $\cE^{\cH}(\delta)$ that is subset of $\cG(\delta)$ we have $T_1 \geq 0$. 
    To show that $T_2 \geq 0$, we start from induction hypothesis
    \begin{align*}
        T_2 &\geq [\hp^t_h - p_h] \Vstar_{\lambda,h+1}(s,a) + b^{p,t}_h(s,a).
    \end{align*}
    Next we apply Lemma~\ref{lem:empirical_bernstein} with $U = \uV^t_{h+1}$ and definition of transition bonuses we have 
    \[
        T_2 \geq - \frac{1}{H} \hp^t_h \vert \uV^t_{h+1} - \Vstar_{\lambda, h+1} \vert (s,a) + \frac{1}{H} \hp^t_h [\uV^t_{h+1} - \lV^t_{h+1}](s,a).
    \]
    By induction hypothesis we have $\uV^t_{h+1}(s) \geq  \Vstar_{\lambda, h+1}(s) \geq \lV^t_{h+1}(s)$, thus $T_2 \geq 0$.

    To prove the second inequality on $Q$-function, we assume $\lQ^t_h(s,a) > 0$ and, as a consequence, $n^t_h(s,a) > 0$. Thus we have
    \begin{align*}
        [\lQ^t_h - \Qstar_{\lambda,h}](s,a) = \underbrace{\cH(\hp^t_h(s,a)) - \cH(p_h(s,a)) - b^{\cH,t}_h(s,a)}_{T_1'} + \underbrace{\hp^t_h \lV^t_{h+1}(s,a) - p_h \Vstar_{\lambda, h+1}(s,a) - b^{p,t}_h(s,a)}_{T_2'}.
    \end{align*}
    Again, by the definition of event $\cE^{\cH}(\delta)$ we have $T_1' \leq 0$ and, by induction hypothesis
    \[
        T_2' \leq [\hp^t_h - p_h] V^{\cH,\star}_{h+1}(s,a) - b^{p,t}_h(s,a).
    \]
    We again apply Lemma~\ref{lem:empirical_bernstein} with $U = \uV^t_{h+1}$
    \[
        T_2' \leq \frac{1}{H} \hp^t_h \vert \uV^t_{h+1} - \Vstar_{\lambda,h+1} \vert (s,a) - \frac{1}{H} \hp^t_h[\uV^t_{h+1} - \lV^t_{h+1}](s,a).
    \]
    We conclude the statement by induction hypothesis for $h' = h+1$.

    Finally, we have to show the inequality for $V$-functions. To do it, we use the fact that $V$-functions are computed by $F_{\lambda}$ applied to $Q$-functions
    \[
        \lV^t_h(s) = F_{\lambda}(\lQ^t_h)(s), \quad \Vstar_{\lambda,h}(s) = F_{\lambda}(\Qstar_{\lambda,h})(s), \quad \uV^t_h(s) = F_{\lambda}(\uQ^t_h)(s).
    \] 
    Notice that $\nabla F_\lambda$ takes values in a probability simplex, thus, all partial derivatives of $F_\lambda$ are non-negative and therefore $F_\lambda$ is monotone in each coordinate. Thus, since $\lQ^t_h(s,a) \leq \Qstar_{\lambda,h}(s,a) \leq \uQ^t_h(s,a)$, we have the same inequality $\lV^t_h(s) \leq \Vstar_{\lambda,h}(s) \leq \uV^t_h(s)$.
\end{proof}



\subsection{Regularization-Agnostic Stopping Rule}

In this section we provide guarantees for the so-called \textit{regularization-agnostic gap}: this notion of gap does not influenced by regularization except the changing of the range of value functions and basically mimics \UCBVIBPI algorithm by \citet{menard2021fast} in definition of the similar gap.

Let us define $G^t_{H+1}(s,a) \triangleq 0$ for all $s,a$ and
\begin{align}\label{eq:def_gap_agnostic}
    \begin{split}
    G^t_h(s,a) &\triangleq \clip\biggl( 2 b^{B,t}_h(s,a) + \frac{4 H^2\Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} + 2 \kappa b^{\cH, t}_h(s,a) + \left(1 + \frac{3}{H} \right) \hp^t_h \left[\pi^{t+1}_{h+1} G^t_{h+1}\right](s,a), \\
    &\qquad 0, H\Rmax \biggl),
    \end{split}
\end{align}
where $b^{B,t}_h(s,a)$ is defined in \eqref{eq:bernstein_transition_bonuses}. For this notion of the gap we can define the stopping rule as follows
\begin{align}\label{eq:def_tau_agnostic}
    \tau = \min\{ t \in \N : \pi^{t+1}_1 G^t_1(s_1) \leq \varepsilon \}.
\end{align}

The lemma below justifies this choice of the stopping time.
\begin{lemma}\label{lem:reg_agnostic_stopping_rule}
    Assume the choice of Bernstein bonuses \eqref{eq:bernstein_transition_bonuses} and let the event $\cG(\delta)$ defined in Lemma~\ref{lem:proba_master_event} holds. Then for all $t \in \N$ we have
    \[
        \Vstar_{\lambda,1}(s_1) - V^{\pi^{t+1}}_{\lambda,1}(s_1) \leq \pi^{t+1}_1 G^t_1(s_1).
    \]
\end{lemma}
\begin{proof}
    Following \cite{menard2021fast}, we start by defining the following quantities
    \begin{align*}
        \tQ^t_h(s,a) &\triangleq \clip\left( \hat r^t_{\kappa,h}(s,a) + \hp^t_h \tV^t_{h+1}(s,a) - b^{p,t}_h(s,a) - \kappa b^{\cH,t}_h(s,a) , 0, r_{\kappa,h}(s,a) + p_h \tV^t_{h+1}(s,a) \right),  \\
        \tV^t_h(s) &\triangleq  \pi^{t+1}_h \tQ^t_h(s) - \lambda \Phi(\pi^{t+1}_h(s)), \\
        \tV^t_{H+1}(s) &\triangleq 0.
    \end{align*}
    By Theorem~\ref{th:reg_confidence_intervals} and Lemma~\ref{lem:tQ_properties} we have
    \begin{align*}
        \Vstar_{\lambda,1}(s_1) - V^{\pi^{t+1}}_{\lambda,1}(s_1) &\leq \uV^{t}_1(s_1) - V^{\pi^{t+1}}_{\lambda,1}(s_1) \leq \uV^{t}_1(s_1) - \tV^t_1(s_1) \\
        &=  \pi^{t+1}_1 \uQ^t_1(s_1) - \lambda \Phi(\pi^{t+1}_1(s_1))- \pi^{t+1}_1 \tQ^t_1(s_1) + \lambda \Phi(\pi^{t+1}_1(s_1)) = \pi^{t+1}_1 [\uQ^t_1 - \tQ^t_1](s_1).
    \end{align*}
    Therefore, it is enough to show that for any $(h,s,a) \in [H] \times \cS \times \cA$
    \[
        [\uQ^t_h - \tQ^t_h](s,a) \leq G^t_h(s,a), \qquad  [\uV^t_h - \tV^t_h](s) \leq \pi^{t+1}_h G^t_h(s).
    \]
    Proceed by backward induction over $h$. The case $h=H+1$ is trivial, thus we may assume that the statement holds for any $h' > h$ for a fixed $h$. Also fix $(s,a) \in \cS \times \cA$.

    Notice that if $G^t_h(s,a) = H\Rmax$, then the inequality is trivially true. Therefore we may assume that $G^t_h(s,a) < H\Rmax$ and, consequently, $n^t_h(s,a) > 0$. Now we have to separate cases.
    \paragraph{First case.}
    In this case we have $\tQ^t_h(s,a) = r_{\kappa,h}(s,a) + p_h \tV^t_{h+1}(s,a)$, i.e. maximal clipping occurs. Therefore
    \begin{align*}
        \uQ^t_h(s,a) - \tQ^t_h(s,a) &= r_h(s,a) + \kappa \cH(\hp^t_h(s,a)) + \kappa b^{\cH,t}_h(s,a) - r_h(s,a) - \kappa\cH(p_h(s,a)) \\
        &+ \hp^t_h \uV^t_{h+1}(s,a) - p_h \tV^t_{h+1}(s,a) + b^{p,t}_h(s,a).
    \end{align*}
    By the definition of the event $\cE^{\cH}(\delta) \subseteq \cG(\delta)$ we have
    \[
        \kappa \cH(\hp^t_h(s,a)) + \kappa b^{\cH,t}_h(s,a)  - \kappa\cH(p_h(s,a)) \leq 2\kappa b^{\cH,t}_h(s,a),
    \]
    for the next term we have
    \[
        \hp^t_h \uV^t_{h+1} - p_h \tV^t_{h+1}(s,a) = \hp^t_h [\uV^t_{h+1} - \tV^t_{h+1}(s,a)] + [\hp^t_h - p_h] \Vstar_{\lambda,h+1} (s,a) + [p_h - \hp^t_h][ \Vstar_{\lambda,h+1}- \tV^t_{h+1} ](s,a).
    \]
    By induction hypothesis 
    \[
        \hp^t_h [\uV^t_{h+1} - \tV^t_{h+1}(s,a)] \leq \hp^t_h [ \pi^{t+1}_{h+1} G^t_{h+1}](s,a).
    \]
    Next we apply Lemma~\ref{lem:empirical_bernstein} with $U = \uV^t_{h+1}(s,a)$ and Theorem~\ref{th:reg_confidence_intervals}
    \[
        [\hp^t_h - p_h] \Vstar_{\lambda,h+1}(s,a) \leq b^{p,t}_h(s,a).
    \]
    Finally, we apply Lemma~\ref{lem:reg_directional_concentration} and obtain
    \[
        [p_h - \hp^t_h][\Vstar_{\lambda,h+1}- \tV^t_{h+1} ](s,a) \leq \frac{1}{H} \hp^t_h[\Vstar_{\lambda,h+1} - \tV^t_{h+1} ](s,a) + \frac{4 H^2 \Rmax \cdot \beta^{\KL}(\delta,n^t_h(s,a))}{n^t_h(s,a)}.
    \]
    Summing all these bounds up, we have
    \begin{align*}
        \uQ^t_h(s,a) - \tQ^t_h(s,a) &\leq \hp^t_h [ \pi^{t+1}_{h+1} G^t_{h+1}](s,a) + 2\kappa b^{\cH,t}_h(s,a) + 2b^{p,t}_h(s,a) \\
        &+ \frac{1}{H} \hp^t_h[ \Vstar_{\lambda, h+1}- \tV^t_{h+1} ](s,a) + \frac{4 H^2 \Rmax \cdot \beta^{\KL}(\delta,n^t_h(s,a))}{n^t_h(s,a)}.
    \end{align*}
    Notice that by Theorem~\ref{th:reg_confidence_intervals} and the induction hypothesis 
    \[
        \frac{1}{H} \hp^t_h[\Vstar_{\lambda, h+1}- \tV^t_{h+1} ](s,a) \leq \frac{1}{H} \hp^t_h[\uV^{t}_{h+1}- \tV^t_{h+1} ](s,a) \leq  \frac{1}{H} \hp^t_h [ \pi^{t+1}_{h+1} G^t_{h+1}](s,a),
    \]
    and by decomposing the transition bonus \eqref{eq:bernstein_transition_bonuses} to Bernstein bonus and correction term and applying Lemma~\ref{lem:tQ_properties}
    \begin{align*}
        b^{p,t}_h(s,a) &= b^{B,t}_h(s,a) + \frac{1}{H} \hp^t_h[\uV^t_{h+1} - \lV^t_{h+1}](s,a) \leq  b^{B,t}_h(s,a) + \frac{1}{H} \hp^t_h[\uV^{t}_{h+1}- \tV^t_{h+1} ](s,a) \\
        &\leq  b^{B,t}_h(s,a) + \frac{1}{H}\hp^t_h [ \pi^{t+1}_{h+1} G^t_{h+1}](s,a),
    \end{align*}
    thus
    \begin{align*}
        \uQ^t_h(s,a) - \tQ^t_h(s,a) &\leq \left(1 + \frac{3}{H} \right)\hp^t_h [ \pi^{t+1}_{h+1} G^t_{h+1}](s,a) + 2 \kappa b^{\cH,t}_h(s,a) + 2b^{B,t}_h(s,a) \\
        &+ \frac{4 H^2 \Rmax \cdot \beta^{\KL}(\delta,n^t_h(s,a))}{n^t_h(s,a)} = G^t_h(s,a).
    \end{align*}
    
    \paragraph{Second case.} In this case we have $\tQ^t_h(s,a) = \hat r^t_{\lambda,h}(s,a) + \hp^t_h \tV^t_{h+1}(s,a) - b^{p,t}_h(s,a) - \kappa b^{\cH,t}_h(s,a)$. Thus

    \[
        \uQ^t_h(s,a) - \tQ^t_h(s,a) \leq 2 \kappa b^{\cH,t}_h(s,a) + 2 b^{B,t}_h(s,a) + \hp^t_h[\uV^{t}_{h+1} - \tV^t_{h+1}](s,a) + \frac{2}{H}\hp^t_h[\uV^{t}_{h+1} - \lV^t_{h+1}](s,a).
    \]
    By Lemma~\ref{lem:tQ_properties} and induction hypothesis we have 
    \[
        \uQ^t_h(s,a) - \tQ^t_h(s,a) \leq 2\kappa b^{\cH,t}_h(s,a) + 2 b^{B,t}_h(s,a) + \left(1 + \frac{2}{H} \right)\hp^t_h [\pi^{t+1}_{h+1} G^t_{h+1}](s,a)  \leq G^t_h(s,a).
    \]

    \paragraph{Conclusion.} From the two cases above we conclude
    \[
        [\uQ^t_h - \tQ^t_h](s,a) \leq G^t_h(s,a).
    \]
    Moreover, we have
    \[
        \uV^t_h(s) - \tV^t_h(s) = \pi^{t+1}_h \uQ^t_h(s) -\lambda\Phi(\pi^{t+1}_h(s))  \pi^{t+1}_h \tQ^t_h(s) + \lambda\Phi(\pi^{t+1}_h(s))= \pi^{t+1}_h [\uQ^t_h - \tQ^t_h](s) \leq \pi^{t+1}_h G^t_h(s).
    \]
    The last inequality concludes the statement of Lemma~\ref{lem:reg_agnostic_stopping_rule}.
\end{proof}


\begin{lemma}\label{lem:tQ_properties}
    Under the choice of Bernstein bonuses \eqref{eq:bernstein_transition_bonuses}, on event $\cG(\delta)$ for any $t \in \N$ and any $(h,s,a) \in [H] \times \cS \times \cA$
    \[
        \tQ^t_h(s,a) \leq \min\{ Q^{\pi^{t+1}}_{\lambda,h}(s,a), \lQ^t_h(s,a) \}, \qquad
        \tV^t_h(s) \leq \min\{ V^{\pi^{t+1}}_{\lambda,h}(s), \lV^t_h(s) \}.
    \]
\end{lemma}
\begin{proof}
    Proceed by backward induction over $h$. The case $h=H+1$ is trivially true, assume that the statement holds for any $h' > h$ for a fixed $h$. Also let us fix $t,s,a$.  By induction hypothesis we have
    \[
        \tQ^t_h(s,a) \leq r_{\kappa,h}(s,a) + p_h \tV^t_{h+1} \leq r_{\kappa,h}(s,a) + p_h V^{\pi^{t+1}}_{\lambda, h+1}(s,a) = Q^{\pi^{t+1}}_{\lambda,h}(s,a).
    \]
    In the same manner
    \begin{align*}
        \tQ^t_h(s,a) &\leq \hat r^t_{\kappa,h}(s,a) + \hp^t_h \tV^t_{h+1} - b^{p,t}_h(s,a) - \kappa b^{\cH,t}_h(s,a) \\
        &\leq \hat r^t_{\kappa,h}(s,a) + \hp^t_h \lV^t_{h+1} - b^{p,t}_h(s,a) - \kappa b^{\cH,t}_h(s,a) = \lQ^t_h(s,a).
    \end{align*}
    Next, we prove the same inequalities for $V$-functions
    \[
        \tV^t_h(s) = \pi^{t+1}_h(s) \tQ^t_h(s,a) - \lambda \Phi(\pi^{t+1}_h(s))\leq  \pi^{t+1}_h(s) Q^{\pi^{t+1}}_{\lambda,h}(s,a) -\lambda \Phi(\pi^{t+1}_h(s)) = V^{\pi^{t+1}}_{\lambda,h}(s),
    \]
    and
    \begin{align*}
        \tV^t_h(s) = \pi^{t+1}_h \tQ^t_h(s)  -\lambda \Phi(\pi^{t+1}_h(s)) \leq  \pi^{t+1}_h \lQ^{t}_h(s) -\lambda \Phi(\pi^{t+1}_h(s)) \leq \max_{\pi \in \simplex_{\cA}}\left\{ \pi \lQ^{t}_h(s) -\lambda \Phi(\pi)\right\} = \lV^t_h(s).
    \end{align*}
\end{proof}

After defining a proper quantity for a stopping rule we may proceed with the final proof for sample-complexity of the presented algorithm \UCBVIEnt.

\begin{theorem}\label{th:reg_agnostic_sample_complexity}
    Let $\delta \in (0,1)$. Then $\UCBVIEnt$ algorithm with Bernstein bonuses \eqref{eq:bernstein_transition_bonuses} and a regularization-agnostic stopping rule $\tau$ is $(\varepsilon,\delta)$-PAC for the best policy identification in regularized MDPs. 
    
    Moreover, with probability at least $1-\delta$ the stopping time $\tau$ is bounded as follows
    \[
        \tau = \cO\left( \frac{H^3SA \Rmax^2 \log(SAH/\delta) L^4}{\varepsilon^2} + \frac{H^3SA (\log(SAH/\delta) + SL) \cdot L}{\varepsilon} \right),
    \]
    where $L = \cO(\log(SAH\Rmax/\varepsilon)) + \log\log(SAH/\delta))$.
\end{theorem}
\begin{proof}
    Notice that if $\tau = 0$, then our sample complexity bound is trivial, thus we assume that $\tau > 0$.
    Let us start from deriving an upper bound for $G^t_h(s,a)$ for $t < \tau, h \in [H], (s,a) \in \cS \times \cA$. 
    \begin{align*}
        G^t_h(s,a) &\leq 2 b^{B,t}_h(s,a) + 2 \kappa b^{\cH,t}_h(s,a) + \frac{4 H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} + \left(1 + \frac{3}{H} \right) \hp^t_h [\pi^{t+1}_{h+1} G^t_{h+1}](s,a) \\
        &\leq 6\sqrt{\Var_{\hp^t_h}[\uV^t_{h+1}](s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + 2\kappa \sqrt{\frac{2\beta^{\cH}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + \frac{23 H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)}  \\
        &+ \left(1 + \frac{3}{H} \right)[\hp^t_h - p_h] [\pi^{t+1}_{h+1} G^t_{h+1}](s,a) + \left(1 + \frac{3}{H} \right)p_h [\pi^{t+1}_{h+1} G^t_{h+1}](s,a).
    \end{align*}
    By Lemma~\ref{lem:reg_directional_concentration}
    \[
        [\hp^t_h - p_h] [\pi^{t+1}_{h+1} G^t_{h+1}](s,a) \leq \frac{1}{H} p_h [\pi^{t+1}_{h+1} G^t_{h+1}](s,a) + \frac{4 H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)}.
    \]
    Also we have to replace the variance of the empirical model with the real variance of the value function for $\pi^{t+1}$ in order to apply the law of total variance (Lemma~\ref{lem:law_of_total_variance}). 

    Apply Lemma~\ref{lem:switch_variance} and Lemma~\ref{lem:switch_variance_bis}
    \begin{align*}
        \Var_{\hp^t_h}[\uV^t_{h+1}](s,a) &\leq 2\Var_{p_h}[\uV^t_{h+1}](s,a) + \frac{4H^2\Rmax^2\beta^{\KL}(\delta,n^t_h(s,a))}{n^t_h(s,a)} \\
        &\leq 4 \Var_{p_h}[V^{ \pi^{t+1}}_{\lambda, h+1}](s,a) + 2H\Rmax p_h [ \uV^t_{h+1} - V^{\pi^{t+1}}_{\lambda, h+1}](s,a) + \frac{4H^2\Rmax^2\beta^{\KL}(\delta,n^t_h(s,a))}{n^t_h(s,a)} .
    \end{align*}
    In the proof of Lemma~\ref{lem:reg_agnostic_stopping_rule} it was proven that
    \[
        [ \uV^t_{h+1} - V^{\pi^{t+1}}_{\lambda, h+1}](s) \leq \pi^{t+1}_{h+1} G^t_{h+1}(s),
    \]
    thus, combining with an inequality $\sqrt{a+b} \leq \sqrt{a} + \sqrt{b}$
    \begin{align*}
        6\sqrt{\Var_{\hp^t_h}[\uV^t_{h+1}](s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} &\leq 12 \sqrt{\Var_{p_h}[V^{\pi^{t+1}}_{\lambda,h+1}](s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} \\
        &+ 6\sqrt{ p_h[\pi^{t+1}_{h+1} G^t_{h+1}](s,a) \frac{2H\Rmax\beta^{\conc}(\delta,n^t_h(s,a))}{n^t_h(s,a)}}\\
        &+\frac{12 H \Rmax \beta^{\KL}(\delta, n^t_h(s,a)}{n^t_h(s,a)} 
    \end{align*}
    To bound the second term, we use inequality $2\sqrt{ab} \leq a + b$
    \[
        6\sqrt{ p_h[\pi^{t+1}_{h+1} G^t_{h+1}](s,a) \frac{2H\Rmax\beta^{\conc}(\delta,n^t_h(s,a))}{n^t_h(s,a)}} \leq \frac{3}{H}p_h[\pi^{t+1}_{h+1} G^t_{h+1}](s,a) + \frac{3H^2\Rmax\beta^{\conc}(\delta,n^t_h(s,a))}{n^t_h(s,a)}.
    \]
    Finally, we have the following bound on $G^t_{h}(s,a)$
    \begin{align*}
        G^t_h(s,a) &\leq 12 \sqrt{\Var_{p_h}[V^{\pi^{t+1}}_{\lambda, h+1}](s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + 2\kappa\sqrt{\frac{2\beta^{\cH}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} \\
        &+ \frac{54 H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} + \left(1 + \frac{10}{H}\right) p_h[\pi^{t+1}_{h+1} G^t_{h+1}](s,a).
    \end{align*}
    Notice that his inequality could be rewritten in the following form
    \begin{align*}
        G^t_h(s,a) &\leq \E_{\pi^{t+1}}\biggl[12 \sqrt{\Var_{p_h}[V^{\pi^{t+1}}_{\lambda,h+1}](s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} + 2\kappa\sqrt{\frac{2\beta^{\cH}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} \\
        &+ \frac{54 H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s,a))}{n^t_h(s,a)} + \left(1 + \frac{10}{H} \right) G^t_{h+1}(s_{h+1},a_{h+1}) \biggl| (s_h,a_h) = (s,a) \biggl ],
    \end{align*}
    thus by rolling out we have
    \begin{align*}
        \pi^{t+1}_1 G^t_1(s_1) &\leq \E_{\pi^{t+1}}\biggl[ \underbrace{12 \sum_{h=1}^H \left(1 + \frac{10}{H}\right)^{h} \sqrt{\Var_{p_h}[V^{\pi^{t+1}}_{\lambda,h+1}](s_h,a_h) \frac{\beta^{\conc}(\delta, n^t_h(s_h,a_h))}{n^t_h(s_h,a_h)}}}_{\termA} \\
        &+ \underbrace{2\kappa\sum_{h=1}^H \left(1 + \frac{10}{H}\right)^{h} \sqrt{\frac{2\beta^{\cH}(\delta, n^t_h(s_h,a_h))}{n^t_h(s_h,a_h)}}}_{\termB} \\
        &+ \underbrace{\sum_{h=1}^H \left(1 + \frac{10}{H}\right)^{h} \frac{54 H^2 \Rmax \beta^{\KL}(\delta, n^t_h(s_h,a_h))}{n^t_h(s_h,a_h)}}_{\termC} \biggl| s_1\biggl ],
    \end{align*}
    where $(1+10/H)^h \leq \rme^{10}$ for any $h \in [H]$. Now we bound each term separately. 

    \paragraph{Term $\termA$.} To bound this term, we apply Cauchy-Schwarz inequality
    \begin{align*}
        \termA &\leq 12 \rme^{10} \sum_{(h,s,a) \in [H] \times \cS \times \cA} d^{\pi^{t+1}}_h(s,a) \sqrt{\Var_{p_h}[V^{\pi^{t+1}}_{\lambda,h+1}](s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}} \\
        &\leq 12 \rme^{10} \sqrt{\sum_{(h,s,a) \in [H] \times \cS \times \cA} d^{\pi^{t+1}}_h(s,a) \Var_{p_h}[V^{\pi^{t+1}}_{\lambda,h+1}](s,a)} \cdot \sqrt{\sum_{(h,s,a) \in [H] \times \cS \times \cA} d^{\pi^{t+1}}_h(s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}}.
    \end{align*}
    For the first multiplier we apply the law of total variance (Lemma~\ref{lem:law_of_total_variance})
    \begin{align*}
        \sum_{h,s,a} d^{\pi^{t+1}}_h(s,a) \Var_{p_h}[V^{\pi^{t+1}}_{\lambda,h+1}](s,a) &\leq \sum_{h,s,a} d^{\pi^{t+1}}_h(s,a) \Var_{p_h}[V^{\pi^{t+1}}_{\lambda,h+1}](s,a) + \sum_{h,s}d^{\pi^{t+1}}_h(s) \Var_{\pi^{t+1}_h}[Q^{\pi^{t+1}}_{\lambda,h}](s) \\
        &= \Vvar^{\cH, \pi^{t+1}}_1(s_1) \leq H^2\Rmax^2.
    \end{align*}
    Therefore, 
    \[
        \termA \leq 24\rme^{10} H\Rmax\sqrt{\sum_{(h,s,a) \in [H] \times \cS \times \cA} d^{\pi^{t+1}}_h(s,a) \frac{\beta^{\conc}(\delta, n^t_h(s,a))}{n^t_h(s,a)}}.
    \]

    \paragraph{Term $\termB$.} For this term we may apply Jensen's inequality
    \begin{align*}
        \termB &\leq 2\kappa H\rme^{10} \E_{\pi^{t+1}} \left[ \frac{1}{H}\sum_{h=1}^H \sqrt{\frac{2\beta^{\cH}(\delta, n^t_h(s_h,a_h))}{n^t_h(s_h,a_h)}} \bigg| s_1 \right] \leq \kappa \sqrt{8H}\rme^{10}\sqrt{\sum_{h,s,a} d^{\pi^{t+1}}_h(s,a) \frac{\beta^{\cH}(\delta, n^t_h(s,a))}{n^t_h(s,a)}}.
    \end{align*}

    By summing up and replacing counts by pseudo-counts by Lemma~\ref{lem:cnt_pseudo} we obtain
    \begin{align*}
        \pi^{t+1}_1 G^t_1(s_1) &\leq 48\rme^{10} H\Rmax\sqrt{\sum_{(h,s,a) \in [H] \times \cS \times \cA} d^{\pi^{t+1}}_h(s,a) \frac{\beta^{\conc}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1}} \\
        &+ 4\kappa \rme^{10} \sqrt{2H}\sqrt{\sum_{h,s,a} d^{\pi^{t+1}}_h(s,a) \frac{\beta^{\cH}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1}} \\
        &+  4\rme^{10} H^2 \Rmax  \sum_{h,s,a} d^{\pi^{t+1}}_h(s,a)  \frac{\beta^{\KL}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1}.
    \end{align*}

    The last step is to notice that for $t < \tau$ we have $\pi^{t+1}_1 G^t_1(s_1) \geq \varepsilon$, thus summing over all $t < \tau$ we have
    \begin{align*}
        (\tau-1) \varepsilon &\leq 48\rme^{10} H\Rmax \sum_{t=1}^{\tau-1}\sqrt{\sum_{(h,s,a) \in [H] \times \cS \times \cA} d^{\pi^{t+1}}_h(s,a) \frac{\beta^{\conc}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1}} \\
        &+ 4\kappa \rme^{10} \sqrt{2H} \sum_{t=1}^{\tau-1}\sqrt{\sum_{h,s,a} d^{\pi^{t+1}}_h(s,a) \frac{\beta^{\cH}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1}} \\
        &+  4\rme^{10} H^2 \Rmax  \sum_{t=1}^{\tau-1} \sum_{h,s,a} d^{\pi^{t+1}}_h(s,a)  \frac{\beta^{\KL}(\delta, \upn^t_h(s,a))}{\upn^t_h(s,a) \vee 1}.
    \end{align*}

    Also we notice that $\beta^{\KL}(\delta, \cdot), \beta^{\conc}(\delta, \cdot), \beta^{\cH}(\delta, \cdot)$ are monotone, thus we may replace $\upn^t_h(s,a)$ with a stopping time $\tau$. Thus, by Jensen's inequality
    \begin{align*}
        (\tau-1) \varepsilon &\leq 48\rme^{10} H\Rmax \sqrt{(\tau-1) \cdot \beta^{\conc}(\delta, \tau-1)}\sqrt{\sum_{t=1}^{\tau-1} \sum_{(h,s,a) \in [H] \times \cS \times \cA} d^{\pi^{t+1}}_h(s,a) \frac{1}{\upn^t_h(s,a)\vee 1} } \\
        &+ 4\kappa \rme^{10} \sqrt{2H \beta^{\cH}(\delta, \tau) \cdot (\tau-1)} \sqrt{\sum_{t=1}^{\tau-1} \sum_{h,s,a} d^{\pi^{t+1}}_h(s,a) \frac{1}{\upn^t_h(s,a) \vee 1}} \\
        &+  4\rme^{10} H^2 \Rmax\beta^{\KL}(\delta, \tau-1)  \sum_{t=1}^{\tau-1} \sum_{h,s,a} d^{\pi^{t+1}}_h(s,a)  \frac{1}{\upn^t_h(s,a) \vee 1}.
    \end{align*}
    Furthermore, notice 
    \[
        \sum_{t=1}^{\tau-1} d^{\pi^{t+1}}_h(s,a)  \frac{1}{\upn^t_h(s,a) \vee 1} = \sum_{t=1}^{\tau-1} \frac{\upn^{t+1}_h(s,a) - \upn^t_h(s,a)}{\upn^t_h(s,a) \vee 1},
    \]
    thus Lemma~\ref{lem:sum_1_over_n} is applicable:
        \begin{align*}
        (\tau-1) \varepsilon &\leq 96\rme^{10} \Rmax \sqrt{(\tau-1)H^3SA \log(\tau) \cdot \beta^{\conc}(\delta, \tau-1)} \\
        &+ 2\kappa\rme^{10} \sqrt{2H^2 SA \log^3(\tau) \beta^{\cH}(\delta, \tau) \cdot (\tau-1)} \\
        &+  8\rme^{10} H^3SA \log(\tau) \Rmax\beta^{\KL}(\delta, \tau-1).
    \end{align*}
    By the definitions of $\beta^{\KL}, \beta^{\conc}, \beta^{\cH}$ we have the following inequality
    \begin{align*}
        (\tau-1) \varepsilon &\leq 96\rme^{10} \Rmax \sqrt{(\tau-1)H^3SA \log(\tau) \cdot (\log(16SAH/\delta) + 2 \log(\rme \tau)) } \\
        &+ 12\kappa\rme^{10} \sqrt{H^2 SA (\tau-1) \log^3(\tau) \cdot (\log(4SAH/\delta) + 2\log(\rme \tau) ) } \\
        &+  16\rme^{10} H^3SA \log(\tau) \Rmax (\log(4SAH/\delta) + S\log(\rme \tau)).
    \end{align*}
    Under assumption $\tau \geq 2$ we can proceed with the further simplifications
    \begin{align}\label{eq:tau_inequality}
        \begin{split}
            \tau \varepsilon &\leq 216\rme^{10} \Rmax \sqrt{\tau H^3SA \log^3(\tau) \cdot (\log(16 SAH/\delta) + 2 \log(\rme \tau)) } \\
        &+  32\rme^{10} H^3SA \log(\tau) \Rmax (\log(16SAH/\delta) + S\log(\rme \tau)).
        \end{split}
    \end{align}

    Let us define the following constants
    \[
        A = 216\rme^{10} \Rmax \cdot \sqrt{\frac{H^3SA}{\varepsilon^2}}, \quad B =\log(16 SAH/\delta), \quad C = \frac{32\rme^{10} \cdot H^3 SA \Rmax }{\varepsilon}.
    \]
    Then inequality~\eqref{eq:tau_inequality} has the following form
    \[
        \tau \leq A\sqrt{\tau(B + 2\log(\rme \tau)) \cdot \log^3(\tau)} + C(B + S \log(\rme \tau))\log \tau.
    \]
    First, we obtain a loose inequality on $\tau$. Let us use the inequality $\log(x) \leq x^\beta / \beta$ for any $x > 0, \beta > 0$ with different $\beta$ for each logarithm
    \begin{align*}
        \tau &\leq A \sqrt{216\tau (B + 4(\rme \tau)^{1/4}) \tau^{1/2}}+ 4C(B + 8S/3 (\rme \tau)^{3/8}) \tau^{1/4} \\
        \Rightarrow \tau^{3/4} &\leq \tau^{3/8} \left( 6A\sqrt{6(B + 4 \rme^{1/4})} + 12CS \rme^{3/8} \right) + 4CB.
    \end{align*}
    
    Notice that the solution to the inequality $x^2 \leq ax + b$ could be upper-bounded as follows
    \[
        x \leq \frac{a + \sqrt{a^2 + 4b}}{2} \leq a + \sqrt{b},
    \]
    thus
    \[
        \tau^{3/8} \leq \left( 6A\sqrt{6(B + 4 \rme^{1/4})} + 12CS \rme^{3/8} \right) + 2\sqrt{CB}.
    \]
    Define $L = 8/3 \log\left( 6A\sqrt{6(B + 4 \rme^{1/4})} + 12CS \rme^{3/8} + 2\sqrt{CB}\right) = \cO(\log(SAH\Rmax/\varepsilon) + \log\log(SAH/\delta) )$ and we have $\log(\tau) \leq L$. Then we have that the solution to \eqref{eq:tau_inequality} is a subset of solutions to
    \[
        \tau \leq A\sqrt{\tau(B + 2(1+L)) \cdot L^3} + C(B + S(1+L))L,
    \]
    solving this inequality we obtain the bound
    \[
        \tau \leq 2A^2(B + 2(1+L))L^3 + 2CB(S(1+L))L.
    \]    
\end{proof}


After this general result we state the bound for the MTEE problem that was stated in the main text.
\begin{theorem}\label{th:mtee_sample_complexity}
    For all $\varepsilon > 0$ and $\delta \in (0,1)$ the \UCBVIEnt algorithm is $(\varepsilon,\delta)$-PAC for MTEE. Moreover, with probability at least $1-\delta$
    \[
        \tau \leq \cO\left( \frac{H^3 SA \log^2(SA) \log(SAH/\delta) \cdot L^4}{\varepsilon^2} + \frac{H^3 SA(\log(SAH/\delta) + SL) \cdot L}{\varepsilon} \right),
    \]
    where $L = \log(SAH/\varepsilon) + \log\log(SAH/\delta)$.
\end{theorem}
\begin{proof}
    Fix $\Phi(\pi) = -\cH(\pi), \kappa=\lambda=1$ and $r_{\max} = 0$. Since $\cH(\pi)$ is $1$-strongly convex with respect to $\ell_1$-norm, we have $r_A = 1$. Also we automatically have $\Rmax = \log(SA)$. In this setting, Theorem~\ref{th:reg_agnostic_sample_complexity} yields the desired statement.
\end{proof}

