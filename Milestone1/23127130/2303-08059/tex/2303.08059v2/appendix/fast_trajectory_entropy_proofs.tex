\section{Fast Rates for MTEE and Regularized MDPs}\label{app:fast_rates_regularized}

In this section we describe an algorithm that will achieve $\tcO(\poly(S,A,H)/\varepsilon)$ sample complexity for regularized MDPs. Additionally, we show that this algorithm could be used for reward-free exploration under regularization.

\subsection{\RFExploreEnt Algorithm}

We leverage the reward-free exploration approach by \citet{jin2020reward-free}. Our algorithm is split into two phases: the first phase is devoted to reward-free exploration, and on the second phase the collected samples are used to build estimates of transition probabilities and entropy of transitions. The main idea is that regularization allows us to collect much smaller number of samples to control the policy error.


\paragraph{Exploration phase} We first learn a policy that visit uniformly the MDP. To this aim for each state $s'\in\cS$ at each step $h'\in[H]$ we build the reward that put one on this state at step $h$ and zeros everywhere else $r_h(s,a) = \ind\{(s,h)=(s',h')\}$. We note that the reward function does not depend on action taken.
We then run the \EULER algorithm for $N_0$ episodes in the MDP equipped with the reward $r$ and denote by $\tilde{\Pi}_{s',h'}$ the set of $N_0$ policies used by \EULER to interact with the MDP. We modify this set of policies by forcing to act uniformly at the goal state $s'$ into the set 

\[\Pi_{s',h'} = \Bigg\{\pi'_{h}(a|s) = \begin{cases} 1/A &\text{if } s=s', h=h'\\ \pi_h(s,a)&\text{else} \end{cases}:\ \pi\in\tilde{\Pi}_{s',h'}\Bigg\}\,.\]
We define the (non-Markovian) policy $\pi^{\mathrm{mix}}$ as the uniform mixture of the policies $\{\pi\in\Pi_{s,h}, (s,h)\in\cS\times[H]\}$ we just constructed. As proved by \citet{jin2020reward-free} the policy $\pi^{\mathrm{mix}}$ is built such that it will visit almost uniformly all the states that can be reached in the MDP from the initial state. Before precising this property we need to introduce the notion of significant state.

\begin{definition}\label{def:significant_states}
    A state $s$ at step $h$ is called $\varepsilon'$-significant if there exists a policy $\pi$ such that the visitation probability of $s$ under policy $\pi$ is greater than $\varepsilon'$:
    \[
        \max_{\pi} d^\pi_h(s) \geq \varepsilon'.
    \]
    The set of all $\varepsilon'$-significant state-step pairs is called $S_{\varepsilon'}$.
\end{definition}
We reproduce here the result by \citet{jin2020reward-free} that shows that the policy $\pi^{\mathrm{mix}}$ will visit any significant state with a large enough probability.
\begin{theorem}[Theorem 3.3 by \citealt{jin2020reward-free}]\label{th:rf_explore_sampling} There exists an absolute constant $c > 0$ such that for any $\varepsilon' > 0$ and $\delta \in (0,1)$, if we set the parameter $N_0 \geq c S^2 A H^4 L/\varepsilon'$ where $L = \log(SAH/(\delta \varepsilon'))$, then with probability at least $1-\delta/3$ the following event holds
\[
    \cE^{\RFExplore}(\delta, \varepsilon')  = \left\{ \forall (s,h) \in S_{\varepsilon'}, \forall a \in \cA, \forall \pi:  \frac{d^\pi_h(s,a)}{\mu_h(s,a)} \leq 2 SAH \right\},
\]
where we denote the visitation distribution of policy $\pi^{\mathrm{mix}}$ by $\mu_h(s,a)= d^{\pi^{\mathrm{mix}}}_h(s,a)$.
\end{theorem}
\begin{remark} Note that the space complexity of \RFExploreEnt is very large since we need to store all the intermediate policies in order to construct $\pi^{\mathrm{mix}}$.
\end{remark}

This policy $\pi^{\mathrm{mix}}$ is then used to collect $N$ new independent trajectories $(z_n)_{n\in[N]}$ where $z_n=  (s^n_1,a^n_1,\ldots,s^n_H, a^n_H, s^n_{H+1})$ by following the policy $\pi^{\mathrm{mix}}$ in the original MDP. Next define the set $\cD= \{(s^n_h,a^n_h,s^n_{h+1}) ,h\in[H], n \in[N]\}$ consisting of the transitions in the sampled trajectories.




% \todoDa{Add sample complexity information}
% \todoPi{TODO Put the definition of the event in the theorem. And define $\pi^{\mathrm{mix}}$ and $\mu_h(s,a) = d_h^{\pi^{\mathrm{mix}}}(s,a)$.}
% \db{what is $N_0$ ? how is it related to complexity of the algorithm ?}
% \todoDa{Parameter of the algorithm, will be additional $SHN_0$ (basically, it is a number of iteration of Euler algorithm with reward that will be $1$ on visitation of state-step pair $(s,h)$)}
% \db{What does it mean ``$N$ trajectories $\{ z_n\}_{n=1}^N$ sampled i.i.d. from a distribution $\mu$'' ? what are the values of $z_n$, pairs ?}
% \todoDa{It is trajectories: $z_n = (s_1, a_1, \ldots, s_H, a_H, s_{H+1})$. $\mu_h$ is a marginal distribution for state-action pairs of a mixture policy by algorithm of \cite{jin2020reward-free}. }
% \todoPi{Ok maybe we need more context from \citet{jin2020reward-free} before giving the theorem. Then we can talk about the datset only in terms of transitions (and rescale quantities by $H$.}
% \db{yes, more context would be good. otherwise many things are not clear, e.g. mixture policy }


\paragraph{Planning phase} Given the transitions collected in the exploration phase we estimate the transition probability distributions and then plan in the estimated MDP with the Bellman equations for MTEE to obtain a maximum trajectory entropy policy.

Using the dataset $\cD$, we construct estimates of transition probabilities $\{\hp_h\}_{h\in[H]}$. We first define the number of visits of a state action pair $(s,a)$ at step $h$ and the number of transitions for $(s,a)$ at step $h$ to a states $s'$ observed in the dataset $\cD$,
\[
    n_h(s,a) = \sum_{n=1}^N \ind\{ (s^n_h, a^n_h) = (s,a) \} \quad  n_h(s'|s,a) = \sum_{n=1}^N \ind\{ (s^n_h, a^n_h, s^n_{h+1}) = (s,a,s') \},
\]
The transitions are estimated using the maximum likelihood method:
\begin{align}\label{eq:hp_construction}
    \hp_h(s'|s,a) = \begin{cases}
        \frac{n_h(s'|s,a)}{n_h(s,a)} & n_h(s,a) > 0 \\
        \frac{1}{S} & n_h(s,a) = 0
    \end{cases}\,.
\end{align}
% \db{how exactly ? because the trajectories are obtained using iid data, you get constant conditional probabilities or ?}
% \todoDa{We can treat $(s_h,a_h)$ generated from any policy $\pi$ as $(s_h,a_h) \sim d^\pi_h$ i.i.d. over trajectories because trajectories are independent. So, at each layer we will have independent sample}
% \db{Does it mean that $(s_h,a_h)$ are independent for different $h$ ? I still do not understand how you  recover the conditional probabilities $s_{h+1}$ given $s_h$ and $a_h$ from $d^\pi_h.$  }
% \todoDa{They will be dependent of course. If you know policy and $d^\pi_h$, you cannot reconstruct the model (at least it is not clear to me), but why it is needed? We can reconstruct the model from the joint distribution over trajectories: $q^{\pi}(m) = \pi_1(s_1) \prod_{i=2}^H p_{i-1}(s_{i+1}|s_i, a_i)$. I don't understand what is unclear there.}
% and solve regularized Bellman equations with respect to this policy. 
% For a given policy $\pi$, we call $\hQ^\pi_h(s,a)$ the Q-values computed with estimated transitions and estimated entropy of the transitions. They could be defined by the following version of Bellman equations
Given these estimates, we can define an empirical version of the regularized Bellman equations for MTEE
\begin{align}\label{eq:hQ_definition}
    \begin{split}
        \hQ^{\pi}_{\lambda,h}(s,a) &= r_h(s,a) + \kappa \cH(\hp_h(s,a)) + \hp_h \hV^{\pi}_{\lambda, h+1}(s,a) \\
        \hV^{\pi}_{\lambda, h}(s) &= \pi \hQ^{\pi}_{\lambda, h}(s) - \lambda \Phi(\pi).
    \end{split}
\end{align}

Then the output policy $\hpi$ is the solution to the optimal regularized Bellman equations
\begin{align}\label{eq:hQ_opt_definition}
    \begin{split}
        \hQ^{\star}_{\lambda,h}(s,a) &= r_h(s,a) + \kappa \cH(\hp_h(s,a)) + \hp_h \hV^{\star}_{\lambda, h+1}(s,a) \\
        \hV^{\star}_{\lambda, h}(s) &= \max_{\pi}\left\{ \pi \hQ^{\star}_{\lambda, h}(s) - \lambda \Phi(\pi) \right\} \\
        \hpi_h(s) &= \argmax_{\pi}\left\{ \pi \hQ^{\star}_{\lambda, h}(s) - \lambda \Phi(\pi) \right\}.
    \end{split}
\end{align}
We call this algorithm \RFExploreEnt. Notably, we can extend this algorithm to the setting of the changing rewards by solving \eqref{eq:hQ_opt_definition} with new reward functions $r_h(s,a)$. The detailed description of the algorithm is presented in Algorithm~\ref{alg:RFExploreEnt}. The only difference between our algorithm and \RFExplore by \citet{jin2020reward-free} is the use of a smaller number of trajectories $N$ and solving regularized Bellman equations instead of usual one.


\subsection{Concentration Events}

In this section we describe all required concentration events.

Let $\beta^{\conc}\colon (0,1) \times \N \to \R_{+}$ and $\beta^{\cnt} \colon (0,1) \to \R_+$ be some functions defined later on in Lemma \ref{lem:fast_traj_proba_master_event}. We define the following favorable events
\begin{align*}
\cE^{\conc}(N,\delta) &\triangleq \Bigg\{ \forall h\in [H], \forall G \colon \cS \to [0, H\Rmax], \forall \nu \colon \cS \to \cA: \\
&\qquad\quad \E_{(s,a) \sim \mu_{h}}\left[ \left(\left[ \hp_{h'} - p_{h'}  \right] G(s,a) \right)^2 \ind\{\nu(s) = a \} \right] \leq  \frac{CH^2 \Rmax^2 S \cdot \beta^{\conc}(\delta, N)}{N}\Bigg\}\,,\\
\cE^{\cH}(N,\delta) &\triangleq \Bigg\{\forall h \in [H]: \E_{(s,a) \sim \mu_{h}}\left[ (\cH(\hp_{h}(s,a)) - \cH(p_{h}(s,a)))^2 \right] \leq \frac{12S^2 A \log^2(SN) \cdot \beta^{\cH}(\delta)}{N}
\Bigg\}\,,
\end{align*}
where $C$ is a some absolute constant.
We also introduce two intersections of these events of interest and $\cE^{\RFExplore}(\delta)$, defined in Theorem~\ref{th:rf_explore_sampling}: $\cG(N,\delta,\varepsilon') \triangleq \cE^{\conc}(N,\delta) \cap \cE^{\cH}(\delta) \cap \cE^{\RFExplore}(\delta, \varepsilon')$. We  prove that for the right choice of the functions $\beta^{\conc}, \beta^{\cH}$ the above events hold with high probability.
\begin{lemma}
\label{lem:fast_traj_proba_master_event}
For any $\delta \in (0,1), \varepsilon' > 0, N \in \N$ and for the following choices of functions $\beta,$
\begin{align*}
    \beta^{\conc}(\delta, N) &\triangleq \log(3AH\Rmax N/\delta),\\
    \beta^{\cH}(\delta) &\triangleq \log(12SAH/\delta),
\end{align*}
it holds that
\begin{align*}
 \P[\cE^{\conc}(\delta)]\geq 1-\delta/3, \qquad \P[\cE^{\cH}(\delta)]\geq 1-\delta/3.
\end{align*}
In particular, $\P[\cG(\delta, N,\varepsilon')] \geq 1-\delta$.
\end{lemma}
\begin{proof}
    Holds from an application of Lemma~\ref{lem:sampling_square_value_error_bound}, Lemma~\ref{lem:sampling_entropy_bound}, Theorem~\ref{th:rf_explore_sampling} and union bound.
\end{proof}


\subsection{Sample Complexity Proof}

In this section we provide the sample complexity result of \RFExploreEnt algorithm in the simple BPI setting and in the reward free setting.

\begin{theorem}\label{th:rf_explore_ent_sample_complexity}
    Algorithm \RFExploreEnt with parameters $N_0 = \Omega\left( \frac{H^7 S^3 A r_A^2  \cdot \Rmax^2 \cdot L}{\varepsilon \lambda}\right)$ and $N \geq \Omega\left( \frac{ H^6 S^3 A r_A^2 \Rmax^2 L^3 }{\varepsilon \lambda}\right)$ is $(\varepsilon,\delta)$-PAC for the best policy identification in regularized MDPs, where $L = \log(SAH/(\varepsilon \lambda \delta))$. The sample complexity is bounded by
    \[
        \tcO\left( \frac{H^8 S^4 A r_A^2 \Rmax^2}{\varepsilon \lambda} \right).
    \]
\end{theorem}
\begin{proof}
    Let us start from exploiting the strong convexity of the regularizer. This property is given by Lemma~\ref{lem:policy_error_decomposition}
    \[
        \Vstar_{\lambda,1}(s_1) - V^{\hpi}_{\lambda,1}(s_1) \leq \frac{r_A^2}{2\lambda} \sum_{h=1}^H \E_{\hpi}\left[\max_{a\in \cA } \left( \hQ^{\hpi}_{\lambda,h} - \Qstar_{\lambda,h} \right)^2(s_h,a) \middle| s_1 \right].
    \]
    Next we study each separate term in this decomposition. By the definition of $\hpi$ and $\pistar$ we have
    \[
    \hQ^{\pistar}_{\lambda,h}(s,a) - Q^{\pistar}_{\lambda,h}(s,a) \leq \hQ^{\hpi}_{\lambda,h}(s,a) - \Qstar_{\lambda,h}(s,a) \leq \hQ^{\hpi}_{\lambda,h}(s,a) - Q^{\hpi}_{\lambda,h}(s,a),
    \]
    thus
    \[
        \left( \hQ^{\pistar}_{\lambda,h}(s,a) - Q^{\pistar}_{\lambda,h}(s,a)\right)^2 \leq \max\left\{ \left( \hQ^{\hpi}_{\lambda,h}(s,a) - \Qstar_{\lambda,h}(s,a) \right)^2, \left(\hQ^{\hpi}_{\lambda,h}(s,a) - Q^{\hpi}_{\lambda,h}(s,a) \right)^2  \right\},
    \]
    and by an inequality $\max\{a,b\} \leq a+b$ for positive $a,b$ we have
    \begin{align*}
        \left( \hQ^{\hpi}_{\lambda,h}(s,a) - \Qstar_{\lambda,h}(s,a) \right)^2 &\leq \left(\hQ^{\hpi}_{\lambda,h}(s,a) - Q^{\hpi}_{\lambda,h}(s,a) \right)^2  + \left( \hQ^{\pistar}_{\lambda,h}(s,a) - Q^{\pistar}_{\lambda,h}(s,a)  \right)^2.
    \end{align*}
    Therefore, the policy error decomposes as follows
    \begin{align*}
        \Vstar_{\lambda,1}(s_1) - V^{\hpi}_{\lambda,1}(s_1) &\leq \frac{r_A^2}{2\lambda}\sum_{h=1}^H \biggl( \E_{\hpi}\left[ \max_{a\in \cA}\left(\hQ^{\hpi}_{\lambda,h}(s_h,a) - Q^{\hpi}_{\lambda,h}(s_h,a) \right)^2  \mid s_1 \right] \\
        &\qquad\qquad + \E_{\hpi}\left[ \max_{a\in \cA}\left( \hQ^{\pistar}_{\lambda,h}(s_h,a) - Q^{\pistar}_{\lambda,h}(s_h,a)  \right)^2  \mid s_1 \right]\biggl).
    \end{align*}
    Next we assume that the event $\cG(N,\delta,\varepsilon')$ holds for the values $N$ and $\varepsilon'$ that will be specified later.  Then Lemma~\ref{lem:q_square_bound} applied $2H$ times yields
    \begin{align*}
        \Vstar_{\lambda,1}(s_1) - V^{\hpi}_{\lambda,1}(s_1) &\leq \frac{r_A^2}{\lambda}\biggl( \frac{48 S^3 H^4 A \Rmax^2 \log^2(N) \log(12SAH/\delta)}{N} \\
        &+ \frac{4C H^6 \Rmax^2 S^2 A \cdot (\log(3AH\Rmax/\delta) + \log(N))}{N} + 2 S H^3 \Rmax^2 \cdot \varepsilon' \biggl).
    \end{align*}
    Next we take
    \[
        \varepsilon' = \frac{\lambda \varepsilon}{4r_A^2 S H^3 \Rmax^2},
    \]
    that requires to take $N_0 \geq \frac{cH^7 S^3 A r_A^2  \cdot \Rmax^2 \cdot L}{\varepsilon \lambda}$ for an absolute constant $c > 0$ and $L = \log(SAH/\delta) + \log(1/(\varepsilon \lambda))$. This yields $\tcO(H^8 S^4 A r_A^2 \Rmax^2 / (\varepsilon \lambda))$ sample complexity of the first phase, since we need $N_0$ samples for each $(s,h) \in \cS \times [H]$. Under this choice, we have
    \[
        \Vstar_{\lambda,1}(s_1) - V^{\hpi}_{\lambda,1}(s_1) \leq \varepsilon/2 + \frac{r^2_A}{\lambda N} \cdot \left( (48 + 4C) S^3 A H^6 \Rmax^2 \log^2(N) \cdot \log(12SAH \Rmax/\delta) \right).
    \]
    To make the second part smaller than $\varepsilon$, we have to analyze the following inequality
    \[
        \log^2(N) / N \cdot B \leq \varepsilon
    \]
    and upper bound its minimal solution that we will call $N^\star$. To do it, we first use a simple numeric bound $\log(N) \leq 4 N^{1/4}$ and obtain a simple estimate $N \geq 256 B^2 / \varepsilon^2$, thus the minimal solution $N^\star \leq 256 B^2 / \varepsilon^2$.
    Therefore, we can assume that $\log(N^\star) \leq 2\log(16B/\varepsilon) = \cO(\log(SAH\Rmax/\varepsilon + \log(1/\delta))$, thus taking
    \[
        N \geq N^\star = \Omega\left( \frac{ H^6 S^3 A r_A^2 \Rmax^2 \log^2(SAH\Rmax/\varepsilon + \log(1/\delta)) \cdot \log(SAH \Rmax/\delta) }{\varepsilon \lambda}\right).
    \]
    is enough to guarantee that the policy error is smaller than $\varepsilon$.
\end{proof}

Notice that in the proof we do not rely on one particular reward function, since the only we need is conditioning on event $\cG(\delta)$ that does not depend on the particular reward function.
\begin{corollary}\label{cor:rf_explore_ent_rf_sample_complexity}
    Algorithm \RFExploreEnt for a  choice $N_0 = \Omega\left( \frac{H^7 S^3 A r_A^2  \cdot \Rmax^2 \cdot L}{\varepsilon \lambda}\right)$ and $N \geq \Omega\left( \frac{ H^6 S^3 A r_A^2 \Rmax^2 L^3 }{\varepsilon \lambda}\right)$ outputs $\varepsilon$-optimal policies for an arbitrary number of reward functions in regularized MDPs. The sample complexity is bounded by
    \[
        \tcO\left( \frac{H^8 S^4 A r_A^2 \Rmax^2}{\varepsilon \lambda} \right).
    \]
\end{corollary}

Finally, we provide a formal proof for application of this algorithm to the MTEE problem, that is a simple application of the results above.
\begin{theorem}\label{th:mtee_fast_rates}
    Algorithm \RFExploreEnt with parameters $N_0 = \Omega\left( \frac{H^7 S^3 A \cdot L^3}{\varepsilon}\right)$ and $N = \Omega\left( \frac{ H^6 S^3 A  L^5 }{\varepsilon}\right)$ is $(\varepsilon,\delta)$-PAC for the MTEE problem, where $L = \log(SAH/(\varepsilon \delta))$. The total sample complexity $SHN_0 + N$ is bounded by
    \[
        \tcO\left( \frac{H^8 S^4 A}{\varepsilon} \right).
    \]
\end{theorem}
\begin{proof}
    Fix $\Phi(\pi) = -\cH(\pi), \kappa = \lambda = 1$ and $r_{\max} = 0$. By 1-strong convexity of $-\cH(\pi)$ with respect to $\ell_1$-norm, its dual is 1-strongly convex with respect to $\ell_\infty$ norm, yielding $r_A = 1$. Also we have $\Rmax = \log(SA)$, thus by \thmref{th:rf_explore_ent_sample_complexity} we conclude the statement.
\end{proof}


\subsection{Technical Lemmas}


\begin{lemma}\label{lem:policy_error_decomposition}
    Let $\pi$ be a greedy policy with respect to regularized Q-values $\uQ_{\lambda,h}(s,a): \pi_h(s) = \argmax_{\pi} \{ \pi \uQ_{\lambda,h}(s) - \lambda \Phi(\pi) \}$. Then the following error decomposition holds
    \[
        \Vstar_{\lambda,1}(s_1) - V^{\pi}_{\lambda,1}(s_1) \leq \frac{r_A^2}{2\lambda}\E_{\pi}\left[ \sum_{h=1}^H \max_{a\in \cA } \left( \uQ_{\lambda,h} - \Qstar_{\lambda,h} \right)^2(s_h,a) \mid s_1 \right],
    \]
    where $r_A$ is a constant defined in \eqref{eq:norm_equivalence}.
\end{lemma}
\begin{proof}
    First, we formulate the statement dependent of $h$
    \begin{equation}\label{eq:policy_error_h}
        \Vstar_{\lambda,h}(s) - V^{\pi}_{\lambda,h}(s) \leq \frac{r_A^2}{2\lambda}\E_{\pi}\left[ \sum_{h'=h}^H \max_{a\in \cA } \left( \uQ_{\lambda,h'} - \Qstar_{\lambda,h'} \right)^2(s_{h'},a) \mid s_h=s \right].
    \end{equation}
    Notice that for $h=1$ and $s=s_1$ the initial statement is recovered. We proceed by induction over $h$. The initial case $h=H+1$ is trivial, next we assume that the statement \eqref{eq:policy_error_h} is true for any $h' > h$.

    We start the analysis from understanding the policy error by applying the smoothness of $F_\lambda$ for any $h$.
    \begin{align*}
        \Vstar_{\lambda,h}(s) - V^{\pi}_{\lambda,h}(s) &= F_\lambda(\Qstar_{\lambda,h}(s, \cdot)) - \left(\pi_h Q^{\pi}_{\lambda,h}(s, \cdot)  -\lambda \Phi(\pi_h(s)) \right) \\
        &\leq F_\lambda(\uQ_h)(s) + \langle \nabla F_\lambda(\uQ_h(s,\cdot)), \Qstar_{\lambda,h}(s,\cdot) - \uQ_h(s,\cdot)  \rangle + \frac{1}{2\lambda} \norm{\uQ_h - \Qstar_{\lambda,h}}_*^2(s) \\
        &- \left(\pi_h Q^{\pi}_{\lambda,h}(s, \cdot)  -\lambda \Phi(\pi_h(s)) \right).
    \end{align*}
    Next we recall that
    \[
         \pi_h(s) = \nabla F(\uQ_h(s,\cdot)), \quad F(\uQ_h)(s)  =  \pi_h \uQ_h(s) - \lambda \Phi(\pi_h(s)),
    \]
    thus we have
    \[
        F(\uQ_h)(s) - \left( \pi_h Q^{\pi}_{\lambda,h}(s, \cdot) - \lambda \Phi(\pi_h(s))  \right) = \pi_h [ \uQ_h - Q^{\pi}_{\lambda,h}](s)
    \]
    and, by Bellman equations
    \begin{align*}
        \Vstar_{\lambda,h}(s) - V^{\pi}_{\lambda,h}(s) &\leq \pi_h \left[ \Qstar_{\lambda,h} - Q^{\pi}_{\lambda,h} \right] (s) + \frac{1}{2\lambda} \norm{\uQ_h - \Qstar_{\lambda,h}}_*^2(s) \\
        &\leq \pi_h p_h \left[ \Vstar_{\lambda,h+1} - V^{\pi}_{\lambda,h+1} \right] (s) + \frac{1}{2\lambda} \norm{\uQ_h - \Qstar_{\lambda,h}}_*^2(s).
    \end{align*}
    Applying norm equivalence \eqref{eq:norm_equivalence} we have
    \[
        \Vstar_{\lambda,h}(s) - V^{\pi}_{\lambda,h}(s) \leq \E_{\pi}\left[ \frac{r_A^2}{2\lambda} \norm{\uQ_h - \Qstar_{\lambda,h}}_\infty^2(s_h) + \Vstar_{\lambda,h+1}(s_{h+1}) - V^{\pi}_{\lambda,h+1}(s_{h+1})  \mid s_h = s\right].
    \]
    By induction hypothesis we conclude the statement.
\end{proof}


\begin{lemma}\label{lem:q_square_bound}
    For any policy $\pi$ the following holds on event $\cG(N,\delta,\varepsilon')$ defined in Lemma~\ref{lem:fast_traj_proba_master_event} for any $h \in [H]$
    \begin{align*}
        \E_{\hpi}\left[ \max_{a\in \cA} \left( \hQ^{\pi}_{\lambda,h} - Q^\pi_{\lambda,h} \right)^2(s_h,a) \mid s_1 \right] &\leq \frac{48 S^3 H^3 A \Rmax^2 \log^2(N) \beta^{\cH}(\delta)}{N} + \frac{4C H^5 \Rmax^2 S^2 A \cdot \beta^{\conc}(\delta,N)}{N} \\
        &+ 2 S H^2 \Rmax^2 \cdot \varepsilon'.
    \end{align*}
\end{lemma}
\begin{proof}
    By performance-difference Lemma~\ref{lm:performance_difference} and form of the rewards stated in \eqref{eq:hQ_definition} we have for any $(s,a,h)\in \cS \times\cA\times[H]$
    \begin{align*}
        \hQ^{\pi}_{\lambda,h}(s,a) - Q^\pi_{\lambda,h}(s,a) &= \kappa \E_{\pi}\left[ \sum_{h'=h}^H \cH(\hp_{h'}(s_{h'},a_{h'})) - \cH(p_{h'}(s_{h'}, a_{h'})) \mid (s_h,a_h) = (s,a) \right] \\
        &+ \E_{\pi}\left[ \sum_{h'=h}^H \left[ \hp_{h'} - p_{h'}\right] \hV^{\pi}_{\lambda, h'+1}(s_{h'}, a_{h'}) \mid (s_h,a_h) = (s,a)\right].
    \end{align*}
    
    Next we analyze all required expectation for one fixed value $h\in[H]$. By Jensen's inequality and a simple algebraic inequality $(a+b)^2 \leq 2a^2 + 2b^2$
    \begin{align*}
        \left(\hQ^{\pi}_{\lambda,h}(s,a) - Q^\pi_{\lambda,h}(s,a)\right)^2 &\leq 2\kappa^2 \E_{\pi}\left[ \left(\sum_{h'=h}^H \cH(\hp_{h'}(s_{h'},a_{h'})) - \cH(p_{h'}(s_{h'}, a_{h'})\right)^2 \bigg| (s_h,a_h) = (s,a) \right] \\
        &+ 2\E_{\pi}\left[ \left( \sum_{h'=h}^H \left[ \hp_{h'} - p_{h'}\right] \hV^{\pi}_{\lambda, h'+1}(s_{h'}, a_{h'})\right)^2 \bigg| (s_h,a_h) = (s,a)\right].
    \end{align*}
    By Cauchyâ€“Schwarz inequality we have the final form
    \begin{align}
        \begin{split}\label{eq:performance_difference_dq}
        \left(\hQ^{\pi}_{\lambda,h}(s,a) - Q^\pi_{\lambda,h}(s,a)\right)^2 &\leq 2H \E_{\pi}\biggl[ \sum_{h'=h}^H \kappa^2\left(\cH(\hp_{h'}(s_{h'}, a_{h'})) - \cH(p_{h'}(s_{h'}, a_{h'})) \right)^2 \\
        &+ \sum_{h'=h}^H \left(\left[ \hp_{h'} - p_{h'}\right] \hV^{\pi}_{\lambda, h'+1}\right)^2(s_{h'}, a_{h'}) \mid (s_h,a_h) = (s,a) \biggl]
        \end{split}
    \end{align}
    To connect this conditional expectation in the right-hand side of \eqref{eq:performance_difference_dq} with conditional expectation over $\hpi,$ we define the following policy
    \[
        \tpi_{h'}(a|s) = \begin{cases}
            \hpi_{h'}(a|s) & h' < h \\
            \ind\{ a = \argmax_{a\in \cA} \left\{ \left(\hQ^{\pi}_h(s,a) - Q^\pi_h(s,a)\right)^2  \right\} & h' = h \\
            \pi_h(a|s) & h' > h.
        \end{cases}
    \]
    Since we do not change the policy $\pi$ for steps greater than $h$, we can replace $\pi$ with $\tpi$ in \eqref{eq:performance_difference_dq}. Additionally, since this policy is equal to $\hpi$ for the first $h-1$ steps, the distribution $d^{\hpi}_h(s_h)$ is equal to $d^{\tpi}_h(s_h)$:
    \begin{align*}
        d^{\hpi}_h(s_h) &= \sum_{s_1,a_1,\ldots,s_{h-1},a_{h-1}} \hpi_1(a_1|s_1) \left( \prod_{h'=1}^{h-1} p_{h'-1}(s_{h'}|s_{h'-1},a_{h'-1}) \hpi_{h'}(a_{h'}|s_{h'}) \right) \cdot p_{h-1}(s_h|s_{h-1},a_{h-1}) \\
        &= \sum_{s_1,a_1,\ldots,s_{h-1},a_{h-1}} \tpi_1(a_1|s_1) \left( \prod_{h'=1}^{h-1} p_{h'-1}(s_{h'}|s_{h'-1},a_{h'-1}) \tpi_{h'}(a_{h'}|s_{h'}) \right) \cdot p_{h-1}(s_h|s_{h-1},a_{h-1}) = d^{\tpi}_h(s_h).
    \end{align*}
    Therefore
    \begin{align*}
        \E_{\hpi}\left[ \max_{a\in \cA} \left( \hQ^{\pi}_{\lambda,h} - Q^\pi_{\lambda,h} \right)^2(s_h,a) \mid s_1 \right] &= \sum_{s} d^{\hpi}_h(s) \max_{a\in \cA}\left( \hQ^{\pi}_{\lambda,h} - Q^\pi_{\lambda,h} \right)^2(s,a) \\
        &=\sum_{s} d^{\tpi}_h(s) \max_{a\in \cA}\left( \hQ^{\pi}_{\lambda,h} - Q^\pi_{\lambda,h} \right)^2(s,a) \\
        &=  \E_{\tpi}\left[ \max_{a\in \cA} \left( \hQ^{\pi}_{\lambda,h} - Q^\pi_{\lambda,h} \right)^2(s_h,a) \mid s_1 \right] = \E_{\tpi}\left[  \left( \hQ^{\pi}_{\lambda,h} - Q^\pi_{\lambda,h} \right)^2(s_h,a_h) \mid s_1 \right].
    \end{align*}
    Next we show that in \eqref{eq:performance_difference_dq} we can make a change of policy from $\pi$ to $\tpi$. It is enough to show that the required marginal distributions are equal for all $h' \geq h$, i.e. for any $(s,a) \in \cS\times \cA$ it holds $\P_{\pi}[(s_{h'},a_{h'}) | (s_h,a_h)] = \P_{\tpi}[(s_{h'},a_{h'}) | (s_h,a_h)]$. For $h'=h$ this probability is an indicator on $(s_h,a_h)$, so it does not depend on policy. For the general case $h'>h$ we can use Markov property and imply
    \begin{align*}
        \P_{\pi}[(s_{h'},a_{h'}) | (s_h,a_h)] &= \sum_{(s_{h+1},a_{h+1},\ldots,s_{h'-1},a_{h'-1})} \prod_{i=h}^{h'-1} \pi_{i+1}(a_{i+1}|s_{i+1}) p_{i}(s_{i+1} | s_i, a_i) \\
        &= \sum_{(s_{h+1},a_{h+1},\ldots,s_{h'-1},a_{h'-1})} \prod_{i=h}^{h'-1} \tpi_{i+1}(a_{i+1}|s_{i+1}) p_{i}(s_{i+1} | s_i, a_i) 
        &= \P_{\tpi}[(s_{h'},a_{h'}) | (s_h,a_h)].
    \end{align*}
    Therefore, we can make change of measure in \eqref{eq:performance_difference_dq} and obtain
    \begin{align*}
        \E_{\hpi}\left[ \max_{a\in \cA} \left( \hQ^{\pi}_{\lambda,h} - Q^\pi_{\lambda,h} \right)^2(s_h,a) \mid s_1 \right]
        &\leq 2H \E_{\tpi} \biggl[ \E_{\tpi}\left[\sum_{h'=h}^H \kappa^2\left(\cH(\hp_{h'}(s_{h'}, a_{h'})) - \cH(p_{h'}(s_{h'}, a_{h'})) \right)^2   \mid s_h, a_h\right] \\
        &\quad+ \E_{\tpi}\left[\sum_{h'=h}^H  \left(\left[ \hp_{h'} - p_{h'}\right] \hV^{\pi}_{\lambda, h'+1}\right)^2(s_{h'}, a_{h'})\mid s_h,a_h\right]\ \bigg|\ s_1 \biggl].
    \end{align*}
    By the properties of conditional expectation we can eliminate the inner expectation and obtain the following upper bound
    \begin{align*}
        \E_{\hpi}\left[ \max_{a\in \cA} \left( \hQ^{\pi}_{\lambda,h} - Q^\pi_{\lambda,h} \right)^2(s_h,a) \mid s_1 \right] &\leq 2H\kappa^2 \sum_{h'=h}^H \E_{\tpi}\left[ \left(\cH(\hp_{h'}(s_{h'}, a_{h'})) - \cH(p_{h'}(s_{h'}, a_{h'})) \right)^2  \ \big|\ s_1 \right] \\
        &+ 2H \sum_{h'=h}^H \E_{\tpi}\left[ \left(\left[ \hp_{h'} - p_{h'}\right] \hV^{\pi}_{\lambda, h'+1}\right)^2(s_{h'}, a_{h'}) \ \bigg|\ s_1\right].
    \end{align*}
    Applying Lemma~\ref{lem:change_measure} and the definition of $\cE^{\cH}(\delta)$ we have
    \begin{align*}
        \E_{\tpi}\left[ \left(\cH(\hp_{h'}(s_{h'}, a_{h'})) - \cH(p_{h'}(s_{h'}, a_{h'})) \right)^2  \mid s_1 \right] &\leq 2SAH \E_{(s,a) \sim \mu_{h'}}\left[ \left(\cH(\hp_{h'}(s, a)) - \cH(p_{h'}(s, a)) \right)^2  \right] + S \log^2(S) \varepsilon' \\
        &\leq \frac{24 S^3 H A \log^2(SN) \beta^{\cH}(\delta)}{N} + S \log^2(S) \varepsilon'.
    \end{align*}

    In the same way by Lemma~\ref{lem:change_measure} and the definition of event $\cE^{\conc}(N,\delta)$
    \begin{align*}
        \E_{\tpi}\left[ \left(\left[ \hp_{h'} - p_{h'}\right] \hV^{\pi}_{\lambda, h'+1}\right)^2(s_{h'}, a_{h'}) \mid s_1\right] &\leq 2SAH\E_{(s,a) \sim \mu_{h'}}\left[ \left(\left[ \hp_{h'} - p_{h'}\right] \hV^{\pi}_{\lambda, h'+1}\right)^2(s, a)\right] + SH^2 \Rmax^2 \varepsilon' \\
        &\leq \frac{2C H^3 \Rmax^2 S^2 A \cdot \beta^{\conc}(\delta, N) }{N} + SH^2 \Rmax^2 \varepsilon'.
    \end{align*}
    Combining these two upper bounds, we have
    \begin{align*}
        \E_{\hpi}\left[ \max_{a\in \cA} \left( \hQ^{\pi}_{\lambda,h} - Q^\pi_{\lambda,h} \right)^2(s_h,a) \mid s_1 \right] &\leq \frac{48 S^3 H^3 A \kappa^2 \log^2(SN) \beta^{\cH}(\delta)}{N} + \frac{4C H^5 \Rmax^2 S^2 A \cdot \beta^{\conc}(\delta,N)}{N} \\
        &+ S (H^2 \Rmax^2 + \kappa^2 \log^2(S))\varepsilon'.
    \end{align*}
    Since $\kappa^2 \log^2(s) \leq \Rmax^2$ and $H \geq 1$, we conclude the statement.
\end{proof}

\begin{lemma}\label{lem:change_measure}
    For any bounded function $f \colon \cS \times \cA \to \R_+, f(s,a) \leq B$ for any policy $\pi$ and step $h$ on event $\cE^{\RFExplore}(\delta, \varepsilon')$ the following holds
    \[
        \E_{\pi}\left[ f(s_h,a_h) | s_1 \right] \leq 2SAH\E_{(s,a) \sim \mu_h}\left[ f(s,a) \right] + BS \varepsilon'.
    \]
\end{lemma}
\begin{proof}
    Recall $S_{\varepsilon', h}$ be a set of all $\varepsilon'$-significant states (see Definition~\ref{def:significant_states}) at step $h$. Then we can rewrite this expectation as follows
    \[
        \E_{\pi}\left[ f(s_h,a_h) | s_1 \right] = \sum_{a \in \cA, s \in S_{\varepsilon', h}} d^\pi_h(s,a) f(s,a) + \sum_{a \in \cA, s \not \in S_{\varepsilon', h}} d^\pi_h(s,a) f(s,a).
    \]
    For the first sum by Theorem~\ref{th:rf_explore_sampling} we have $d^\pi_h(s,a) \leq 2SAH \mu_h(s,a)$, thus
    \[
        \sum_{a \in \cA, s \in S_{\varepsilon', h}} d^\pi_h(s,a) f(s,a)  \leq 2SAH \sum_{(s,a) \in \cS \times \cA} \mu_h(s,a) f(s,a) = 2SAH \E_{(s,a) \sim \mu_h}\left[ f(s,a) \right].
    \]
    For the second sum we apply $f(s,a) \leq B$ and the fact that for all states that are not $\varepsilon'$-significant under any policy $d^\pi_h(s) \leq \varepsilon'$:
    \[
         \sum_{a \in \cA, s \not \in S_{\varepsilon', h}} d^\pi_h(s,a) f(s,a) \leq B  \sum_{a \in \cA, s \not \in S_{\varepsilon', h}} d^\pi_h(s,a) = B \sum_{s \not \in S_{\varepsilon', h}} d^\pi_h(s) \leq BS \varepsilon'.
    \]
\end{proof}