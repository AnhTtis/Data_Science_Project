{
  "2106-00661": {
    "title": "Reward is enough for convex MDPs",
    "authors": [
      "Tom Zahavy",
      "Brendan O'Donoghue",
      "Guillaume Desjardins",
      "Satinder Singh"
    ],
    "submission_date": "2021-06-01",
    "revised_dates": [],
    "arxiv_id": "2106.00661",
    "venue": "Neural Information Processing Systems",
    "year": 2021
  },
  "2010-03531": {
    "title": "Episodic Reinforcement Learning in Finite MDPs: Minimax Lower Bounds Revisited",
    "authors": [
      "O. D. Domingues",
      "Pierre M'enard",
      "E. Kaufmann",
      "Michal Valko"
    ],
    "submission_date": "2020-10-07",
    "revised_dates": [],
    "arxiv_id": "2010.03531",
    "venue": "International Conference on Algorithmic Learning Theory",
    "year": 2020
  },
  "2006-05879": {
    "title": "Planning in Markov Decision Processes with Gap-Dependent Sample Complexity",
    "authors": [
      "Anders Jonsson",
      "E. Kaufmann",
      "Pierre M'enard",
      "O. D. Domingues",
      "Edouard Leurent",
      "Michal Valko"
    ],
    "submission_date": "2020-06-10",
    "revised_dates": [],
    "arxiv_id": "2006.05879",
    "venue": "Neural Information Processing Systems",
    "year": 2020
  },
  "2004-10019": {
    "title": "Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition",
    "authors": [
      "Zihan Zhang",
      "Yuanshuo Zhou",
      "Xiangyang Ji"
    ],
    "submission_date": "2020-04-21",
    "revised_dates": [],
    "arxiv_id": "2004.10019",
    "venue": "Neural Information Processing Systems",
    "year": 2020
  },
  "2002-02794": {
    "title": "Reward-Free Exploration for Reinforcement Learning",
    "authors": [
      "Chi Jin",
      "A. Krishnamurthy",
      "Max Simchowitz",
      "Tiancheng Yu"
    ],
    "submission_date": "2020-02-07",
    "revised_dates": [],
    "arxiv_id": "2002.02794",
    "venue": "International Conference on Machine Learning",
    "year": 2020
  },
  "1910-01913": {
    "title": "If MaxEnt RL is the Answer, What is the Question?",
    "authors": [
      "Benjamin Eysenbach",
      "S. Levine"
    ],
    "submission_date": "2019-09-25",
    "revised_dates": [],
    "arxiv_id": "1910.01913",
    "venue": "arXiv.org",
    "year": 2019
  },
  "1906-05274": {
    "title": "Efficient Exploration via State Marginal Matching",
    "authors": [
      "Lisa Lee",
      "Benjamin Eysenbach",
      "Emilio Parisotto",
      "E. Xing",
      "S. Levine",
      "R. Salakhutdinov"
    ],
    "submission_date": "2019-06-12",
    "revised_dates": [],
    "arxiv_id": "1906.05274",
    "venue": "arXiv.org",
    "year": 2019
  },
  "1905-06466": {
    "title": "Exploration-Exploitation Trade-off in Reinforcement Learning on Online Markov Decision Processes with Global Concave Rewards",
    "authors": [
      "Wang Chi Cheung"
    ],
    "submission_date": "2019-05-15",
    "revised_dates": [],
    "arxiv_id": "1905.06466",
    "venue": "arXiv.org",
    "year": 2019
  },
  "1901-10995": {
    "title": "Go-Explore: a New Approach for Hard-Exploration Problems",
    "authors": [
      "Adrien Ecoffet",
      "Joost Huizinga",
      "J. Lehman",
      "Kenneth O. Stanley",
      "J. Clune"
    ],
    "submission_date": "2019-01-30",
    "revised_dates": [],
    "arxiv_id": "1901.10995",
    "venue": "arXiv.org",
    "year": 2019
  },
  "1901-00210": {
    "title": "Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds",
    "authors": [
      "A. Zanette",
      "E. Brunskill"
    ],
    "submission_date": "2019-01-01",
    "revised_dates": [],
    "arxiv_id": "1901.00210",
    "venue": "International Conference on Machine Learning",
    "year": 2019
  },
  "1812-02690": {
    "title": "Provably Efficient Maximum Entropy Exploration",
    "authors": [
      "Elad Hazan",
      "S. Kakade",
      "Karan Singh",
      "A. V. Soest"
    ],
    "submission_date": "2018-12-06",
    "revised_dates": [],
    "arxiv_id": "1812.02690",
    "venue": "International Conference on Machine Learning",
    "year": 2018
  },
  "1811-03056": {
    "title": "Policy Certificates: Towards Accountable Reinforcement Learning",
    "authors": [
      "Christoph Dann",
      "Lihong Li",
      "Wei Wei",
      "E. Brunskill"
    ],
    "submission_date": "2018-11-07",
    "revised_dates": [],
    "arxiv_id": "1811.03056",
    "venue": "International Conference on Machine Learning",
    "year": 2018
  },
  "1810-12894": {
    "title": "Exploration by Random Network Distillation",
    "authors": [
      "Yuri Burda",
      "Harrison Edwards",
      "A. Storkey",
      "Oleg Klimov"
    ],
    "submission_date": "2018-09-27",
    "revised_dates": [],
    "arxiv_id": "1810.12894",
    "venue": "International Conference on Learning Representations",
    "year": 2018
  },
  "1807-03765": {
    "title": "Is Q-learning Provably Efficient?",
    "authors": [
      "Chi Jin",
      "Zeyuan Allen-Zhu",
      "Sébastien Bubeck",
      "Michael I. Jordan"
    ],
    "submission_date": "2018-07-10",
    "revised_dates": [],
    "arxiv_id": "1807.03765",
    "venue": "Neural Information Processing Systems",
    "year": 2018
  },
  "1807-03223": {
    "title": "Entropy Maximization for Markov Decision Processes Under Temporal Logic Constraints",
    "authors": [
      "Y. Savas",
      "Melkior Ornik",
      "Murat Cubuktepe",
      "M. Karabag",
      "U. Topcu"
    ],
    "submission_date": "2018-07-09",
    "revised_dates": [],
    "doi": "10.1109/TAC.2019.2922583",
    "arxiv_id": "1807.03223",
    "venue": "IEEE Transactions on Automatic Control",
    "year": 2018
  },
  "1803-01626": {
    "title": "Variance-Aware Regret Bounds for Undiscounted Reinforcement Learning in MDPs",
    "authors": [
      "M. S. Talebi",
      "Odalric-Ambrym Maillard"
    ],
    "submission_date": "2018-03-05",
    "revised_dates": [],
    "arxiv_id": "1803.01626",
    "venue": "International Conference on Algorithmic Learning Theory",
    "year": 2018
  },
  "1705-10257": {
    "title": "Boltzmann Exploration Done Right",
    "authors": [
      "N. Cesa-Bianchi",
      "C. Gentile",
      "Gergely Neu",
      "G. Lugosi"
    ],
    "submission_date": "2017-05-29",
    "revised_dates": [],
    "arxiv_id": "1705.10257",
    "venue": "Neural Information Processing Systems",
    "year": 2017
  },
  "1705-07798": {
    "title": "A unified view of entropy-regularized Markov decision processes",
    "authors": [
      "Gergely Neu",
      "Anders Jonsson",
      "V. Gómez"
    ],
    "submission_date": "2017-05-22",
    "revised_dates": [],
    "arxiv_id": "1705.07798",
    "venue": "arXiv.org",
    "year": 2017
  },
  "1704-06440": {
    "title": "Equivalence Between Policy Gradients and Soft Q-Learning",
    "authors": [
      "John Schulman",
      "P. Abbeel",
      "Xi Chen"
    ],
    "submission_date": "2017-04-21",
    "revised_dates": [],
    "arxiv_id": "1704.06440",
    "venue": "arXiv.org",
    "year": 2017
  },
  "1703-05449": {
    "title": "Minimax Regret Bounds for Reinforcement Learning",
    "authors": [
      "M. G. Azar",
      "Ian Osband",
      "R. Munos"
    ],
    "submission_date": "2017-03-16",
    "revised_dates": [],
    "arxiv_id": "1703.05449",
    "venue": "International Conference on Machine Learning",
    "year": 2017
  },
  "1703-07710": {
    "title": "Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning",
    "authors": [
      "Christoph Dann",
      "Tor Lattimore",
      "E. Brunskill"
    ],
    "submission_date": "2017-03-01",
    "revised_dates": [],
    "arxiv_id": "1703.07710",
    "venue": "Neural Information Processing Systems",
    "year": 2017
  },
  "1702-08165": {
    "title": "Reinforcement Learning with Deep Energy-Based Policies",
    "authors": [
      "Tuomas Haarnoja",
      "Haoran Tang",
      "P. Abbeel",
      "S. Levine"
    ],
    "submission_date": "2017-02-27",
    "revised_dates": [],
    "arxiv_id": "1702.08165",
    "venue": "International Conference on Machine Learning",
    "year": 2017
  },
  "1606-01868": {
    "title": "Unifying Count-Based Exploration and Intrinsic Motivation",
    "authors": [
      "Marc G. Bellemare",
      "S. Srinivasan",
      "Georg Ostrovski",
      "T. Schaul",
      "D. Saxton",
      "R. Munos"
    ],
    "submission_date": "2016-06-06",
    "revised_dates": [],
    "arxiv_id": "1606.01868",
    "venue": "Neural Information Processing Systems",
    "year": 2016
  },
  "1512-08562": {
    "title": "Taming the Noise in Reinforcement Learning via Soft Updates",
    "authors": [
      "Roy Fox",
      "Ari Pakman",
      "Naftali Tishby"
    ],
    "submission_date": "2015-12-28",
    "revised_dates": [],
    "arxiv_id": "1512.08562",
    "venue": "Conference on Uncertainty in Artificial Intelligence",
    "year": 2015
  },
  "1206-6461": {
    "title": "On the Sample Complexity of Reinforcement Learning with a Generative Model",
    "authors": [
      "M. G. Azar",
      "R. Munos",
      "H. Kappen"
    ],
    "submission_date": "2012-06-26",
    "revised_dates": [],
    "arxiv_id": "1206.6461",
    "venue": "International Conference on Machine Learning",
    "year": 2012
  },
  "2106-01946": {
    "title": "Convex optimization",
    "authors": [
      "Stephen P. Boyd",
      "L. Vandenberghe"
    ],
    "submission_date": "2010-02-01",
    "revised_dates": [],
    "doi": "10.1002/9780470050118.ecse383",
    "arxiv_id": "2106.01946",
    "venue": "Computer Vision",
    "year": 2010
  }
}